   Block Wiedemann algorithm      Block Wiedemann algorithm   The block Wiedemann algorithm for computing kernel vectors of a matrix over a finite field is a generalisation of an algorithm due to Don Coppersmith .  Coppersmith's algorithm  Let M be an    n  ×  n      n  n    n\times n    square matrix over some finite field F, let    x  base     subscript  x  base    x_{\mathrm{base}}   be a random vector of length n, and let    x  =   M   x  base        x    M   subscript  x  base      x=Mx_{\mathrm{base}}   . Consider the sequence of vectors    S  =   [  x  ,   M  x   ,    M  2   x   ,  …  ]       S   x    M  x      superscript  M  2   x   normal-…     S=\left[x,Mx,M^{2}x,\ldots\right]   obtained by repeatedly multiplying the vector by the matrix M; let y be any other vector of length n, and consider the sequence of finite-field elements     S  y   =   [   y  ⋅  x   ,    y  ⋅  M   x   ,    y  ⋅   M  2    x  …   ]        subscript  S  y     normal-⋅  y  x      normal-⋅  y  M   x      normal-⋅  y   superscript  M  2    x  normal-…      S_{y}=\left[y\cdot x,y\cdot Mx,y\cdot M^{2}x\ldots\right]     We know that the matrix M has a minimal polynomial ; by the Cayley–Hamilton theorem we know that this polynomial is of degree (which we will call    n  0     subscript  n  0    n_{0}   ) no more than n. Say      ∑   r  =  0    n  0      p  r    M  r     =  0        superscript   subscript     r  0     subscript  n  0       subscript  p  r    superscript  M  r     0    \sum_{r=0}^{n_{0}}p_{r}M^{r}=0   . Then      ∑   r  =  0    n  0     y  ⋅   (    p  r    (    M  r   x   )    )     =  0        superscript   subscript     r  0     subscript  n  0     normal-⋅  y     subscript  p  r      superscript  M  r   x      0    \sum_{r=0}^{n_{0}}y\cdot(p_{r}(M^{r}x))=0   ; so the minimal polynomial of the matrix annihilates the sequence   S   S   S   and hence    S  y     subscript  S  y    S_{y}   .  But the Berlekamp–Massey algorithm allows us to calculate relatively efficiently some sequence     q  0   …   q  L        subscript  q  0   normal-…   subscript  q  L     q_{0}\ldots q_{L}   with      ∑   i  =  0   L     q  i    S  y    [   i  +  r   ]     =   0   ∀  r          superscript   subscript     i  0    L      subscript  q  i    subscript  S  y    delimited-[]    i  r        0   for-all  r      \sum_{i=0}^{L}q_{i}S_{y}[{i+r}]=0\forall r   . Our hope is that this sequence, which by construction annihilates    y  ⋅  S     normal-⋅  y  S    y\cdot S   , actually annihilates   S   S   S   ; so we have      ∑   i  =  0   L     q  i    M  i   x    =  0        superscript   subscript     i  0    L      subscript  q  i    superscript  M  i   x    0    \sum_{i=0}^{L}q_{i}M^{i}x=0   . We then take advantage of the initial definition of   x   x   x   to say     M    ∑   i  =  0   L     q  i    M  i    x  base      =  0        M    superscript   subscript     i  0    L      subscript  q  i    superscript  M  i    subscript  x  base      0    M\sum_{i=0}^{L}q_{i}M^{i}x_{\mathrm{base}}=0   and so     ∑   i  =  0   L     q  i    M  i    x  base        superscript   subscript     i  0    L      subscript  q  i    superscript  M  i    subscript  x  base      \sum_{i=0}^{L}q_{i}M^{i}x_{\mathrm{base}}   is a hopefully non-zero kernel vector of   M   M   M   .  The block Wiedemann algorithm  The natural implementation of sparse matrix arithmetic on a computer makes it easy to compute the sequence S in parallel for a number of vectors equal to the width of a machine word – indeed, it will normally take no longer to compute for that many vectors than for one. If you have several processors, you can compute the sequence S for a different set of random vectors in parallel on all the computers.  It turns out, by a generalization of the Berlekamp–Massey algorithm to provide a sequence of small matrices, that you can take the sequence produced for a large number of vectors and generate a kernel vector of the original large matrix. You need to compute      y  i   ⋅   M  t     x  j        normal-⋅   subscript  y  i    superscript  M  t     subscript  x  j     y_{i}\cdot M^{t}x_{j}   for some     i  =   0  …   i  max     ,    j  =   0  …   j  max     ,   t  =   0  …   t  max         formulae-sequence    i    0  normal-…   subscript  i       formulae-sequence    j    0  normal-…   subscript  j        t    0  normal-…   subscript  t         i=0\ldots i_{\max},j=0\ldots j_{\max},t=0\ldots t_{\max}   where     i  max   ,   j  max   ,   t  max       subscript  i     subscript  j     subscript  t      i_{\max},j_{\max},t_{\max}   need to satisfy     t  max   >    d   i  max    +   d   j  max    +   O   (  1  )          subscript  t        d   subscript  i       d   subscript  j       O  1      t_{\max}>\frac{d}{i_{\max}}+\frac{d}{j_{\max}}+O(1)   and    y  i     subscript  y  i    y_{i}   are a series of vectors of length n; but in practice you can take    y  i     subscript  y  i    y_{i}   as a sequence of unit vectors and simply write out the first    i  max     subscript  i     i_{\max}   entries in your vectors at each time t .  References  Villard's 1997 research report ' A study of Coppersmith's block Wiedemann algorithm using matrix polynomials ' (the cover material is in French but the content in English) is a reasonable description.  Thomé's paper ' Subquadratic computation of vector generating polynomials and improvement of the block Wiedemann algorithm ' uses a more sophisticated FFT -based algorithm for computing the vector generating polynomials, and describes a practical implementation with i max = j max = 4 used to compute a kernel vector of a 484603×484603 matrix of entries modulo 2 607 −1, and hence to compute discrete logarithms in the field GF (2 607 ).  "  Category:Numerical linear algebra   