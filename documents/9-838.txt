   Observed information      Observed information   In statistics , the observed information , or observed Fisher information , is the negative of the second derivative (the Hessian matrix ) of the "log-likelihood" (the logarithm of the likelihood function ). It is a sample-based version of the Fisher information .  Definition  Suppose we observe random variables      X  1   ,  …  ,   X  n       subscript  X  1   normal-…   subscript  X  n     X_{1},\ldots,X_{n}   , independent and identically distributed with density f ( X ; θ), where θ is a (possibly unknown) vector. Then the log-likelihood of the parameters   θ   θ   \theta   given the data     X  1   ,  …  ,   X  n       subscript  X  1   normal-…   subscript  X  n     X_{1},\ldots,X_{n}   is      ℓ   (  θ  |   X  1   ,  …  ,   X  n   )   =   ∑   i  =  1   n   log  f   (   X  i   |  θ  )      fragments  ℓ   fragments  normal-(  θ  normal-|   subscript  X  1   normal-,  normal-…  normal-,   subscript  X  n   normal-)     superscript   subscript     i  1    n    f   fragments  normal-(   subscript  X  i   normal-|  θ  normal-)     \ell(\theta|X_{1},\ldots,X_{n})=\sum_{i=1}^{n}\log f(X_{i}|\theta)   .  We define the observed information matrix at    θ  *     superscript  θ     \theta^{*}   as       𝒥   (   θ  *   )    =   -       ∇   ∇  ⊤    ℓ    (  θ  )    |    θ  =   θ  *            𝒥   superscript  θ        evaluated-at      normal-∇   superscript  normal-∇  top    normal-ℓ   θ     θ   superscript  θ         \mathcal{J}(\theta^{*})=-\left.\nabla\nabla^{\top}\ell(\theta)\right|_{\theta=%
 \theta^{*}}       = -     \left. \left( \begin{array}{cccc}  \tfrac{\partial^2}{\partial \theta_1^2}  &  \tfrac{\partial^2}{\partial \theta_1 \partial \theta_2}  &  \cdots  &  \tfrac{\partial^2}{\partial \theta_1 \partial \theta_n} \\  \tfrac{\partial^2}{\partial \theta_2 \partial \theta_1}  &  \tfrac{\partial^2}{\partial \theta_2^2}  &  \cdots  &  \tfrac{\partial^2}{\partial \theta_2 \partial \theta_n} \\  \vdots &  \vdots &  \ddots &  \vdots \\  \tfrac{\partial^2}{\partial \theta_n \partial \theta_1}  &  \tfrac{\partial^2}{\partial \theta_n \partial \theta_2}  &  \cdots  &  \tfrac{\partial^2}{\partial \theta_n^2} \\  \end{array} \right) \ell(\theta) \right|_{\theta = \theta^*}  In many instances, the observed information is evaluated at the maximum-likelihood estimate . 1  Fisher information  The Fisher information     ℐ   (  θ  )       ℐ  θ    \mathcal{I}(\theta)   is the expected value of the observed information given a single observation   X   X   X   distributed according to the hypothetical model with parameter   θ   θ   \theta   :       ℐ   (  θ  )    =   E   (   𝒥   (  θ  )    )          ℐ  θ     normal-E    𝒥  θ      \mathcal{I}(\theta)=\mathrm{E}(\mathcal{J}(\theta))   .  Applications  In a notable article, Bradley Efron and David V. Hinkley  2 argued that the observed information should be used in preference to the expected information when employing normal approximations for the distribution of maximum-likelihood estimates .  See also   Fisher information matrix  Fisher information metric   References  "  Category:Information theory  Category:Statistical terminology  Category:Estimation theory     Dodge, Y. (2003) The Oxford Dictionary of Statistical Terms , OUP. ISBN 0-19-920613-9 ↩  ↩     