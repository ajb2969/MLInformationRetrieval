   Observed information      Observed information   In statistics , the observed information , or observed Fisher information , is the negative of the second derivative (the Hessian matrix ) of the "log-likelihood" (the logarithm of the likelihood function ). It is a sample-based version of the Fisher information .  Definition  Suppose we observe random variables      X  1   ,  â€¦  ,   X  n       subscript  X  1   normal-â€¦   subscript  X  n     X_{1},\ldots,X_{n}   , independent and identically distributed with density f ( X ;Â Î¸), where Î¸ is a (possibly unknown) vector. Then the log-likelihood of the parameters   Î¸   Î¸   \theta   given the data     X  1   ,  â€¦  ,   X  n       subscript  X  1   normal-â€¦   subscript  X  n     X_{1},\ldots,X_{n}   is      â„“   (  Î¸  |   X  1   ,  â€¦  ,   X  n   )   =   âˆ‘   i  =  1   n   log  f   (   X  i   |  Î¸  )      fragments  â„“   fragments  normal-(  Î¸  normal-|   subscript  X  1   normal-,  normal-â€¦  normal-,   subscript  X  n   normal-)     superscript   subscript     i  1    n    f   fragments  normal-(   subscript  X  i   normal-|  Î¸  normal-)     \ell(\theta|X_{1},\ldots,X_{n})=\sum_{i=1}^{n}\log f(X_{i}|\theta)   .  We define the observed information matrix at    Î¸  *     superscript  Î¸     \theta^{*}   as       ğ’¥   (   Î¸  *   )    =   -       âˆ‡   âˆ‡  âŠ¤    â„“    (  Î¸  )    |    Î¸  =   Î¸  *            ğ’¥   superscript  Î¸        evaluated-at      normal-âˆ‡   superscript  normal-âˆ‡  top    normal-â„“   Î¸     Î¸   superscript  Î¸         \mathcal{J}(\theta^{*})=-\left.\nabla\nabla^{\top}\ell(\theta)\right|_{\theta=%
 \theta^{*}}       = -     \left. \left( \begin{array}{cccc}  \tfrac{\partial^2}{\partialÂ \theta_1^2}  &Â Â \tfrac{\partial^2}{\partialÂ \theta_1Â \partialÂ \theta_2}  &Â Â \cdots  &Â Â \tfrac{\partial^2}{\partialÂ \theta_1Â \partialÂ \theta_n}Â \\  \tfrac{\partial^2}{\partialÂ \theta_2Â \partialÂ \theta_1}  &Â Â \tfrac{\partial^2}{\partialÂ \theta_2^2}  &Â Â \cdots  &Â Â \tfrac{\partial^2}{\partialÂ \theta_2Â \partialÂ \theta_n}Â \\  \vdotsÂ &  \vdotsÂ &  \ddotsÂ &  \vdotsÂ \\  \tfrac{\partial^2}{\partialÂ \theta_nÂ \partialÂ \theta_1}  &Â Â \tfrac{\partial^2}{\partialÂ \theta_nÂ \partialÂ \theta_2}  &Â Â \cdots  &Â Â \tfrac{\partial^2}{\partialÂ \theta_n^2}Â \\  \end{array} \right) \ell(\theta) \right|_{\theta = \theta^*}  In many instances, the observed information is evaluated at the maximum-likelihood estimate . 1  Fisher information  The Fisher information     â„   (  Î¸  )       â„  Î¸    \mathcal{I}(\theta)   is the expected value of the observed information given a single observation   X   X   X   distributed according to the hypothetical model with parameter   Î¸   Î¸   \theta   :       â„   (  Î¸  )    =   E   (   ğ’¥   (  Î¸  )    )          â„  Î¸     normal-E    ğ’¥  Î¸      \mathcal{I}(\theta)=\mathrm{E}(\mathcal{J}(\theta))   .  Applications  In a notable article, Bradley Efron and David V. Hinkley  2 argued that the observed information should be used in preference to the expected information when employing normal approximations for the distribution of maximum-likelihood estimates .  See also   Fisher information matrix  Fisher information metric   References  "  Category:Information theory  Category:Statistical terminology  Category:Estimation theory     Dodge, Y. (2003) The Oxford Dictionary of Statistical Terms , OUP. ISBN 0-19-920613-9 â†©  â†©     