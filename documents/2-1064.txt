   Runge‚ÄìKutta methods      Runge‚ÄìKutta methods   In numerical analysis , the Runge‚ÄìKutta methods are an important family of implicit and explicit iterative methods, which are used in temporal discretization for the approximation of solutions of ordinary differential equations . These techniques were developed around 1900 by the German mathematicians C. Runge and M. W. Kutta .  See the article on numerical methods for ordinary differential equations for more background and other methods. See also List of Runge‚ÄìKutta methods .  The Runge‚ÄìKutta method  One member of the family of Runge‚ÄìKutta methods is often referred to as " RK4 ", " classical Runge‚ÄìKutta method " or simply as " the Runge‚ÄìKutta method ".  Let an initial value problem be specified as follows.         y  Àô   =   f   (  t  ,  y  )     ,    y   (   t  0   )    =   y  0     .     formulae-sequence     normal-Àô  y     f   t  y         y   subscript  t  0     subscript  y  0      \dot{y}=f(t,y),\quad y(t_{0})=y_{0}.     Here, y is an unknown function (scalar or vector) of time t which we would like to approximate; we are told that    y  Àô     normal-Àô  y    \dot{y}   , the rate at which y changes, is a function of t and of y itself. At the initial time    t  0     subscript  t  0    t_{0}   the corresponding y -value is    y  0     subscript  y  0    y_{0}   . The function f and the data    t  0     subscript  t  0    t_{0}   ,    y  0     subscript  y  0    y_{0}   are given.  Now pick a step-size h >0 and define      y   n  +  1      subscript  y    n  1     \displaystyle y_{n+1}     for n =¬†0,¬†1,¬†2,¬†3,¬†.¬†.¬†.¬†, using      k  1     subscript  k  1    \displaystyle k_{1}    1   (Note: the above equations have different but equivalent definitions in different texts). 2    Here    y   n  +  1      subscript  y    n  1     y_{n+1}   is the RK4 approximation of    y   (   t   n  +  1    )       y   subscript  t    n  1      y(t_{n+1})   , and the next value (    y   n  +  1      subscript  y    n  1     y_{n+1}   ) is determined by the present value (    y  n     subscript  y  n    y_{n}   ) plus the weighted average of four increments, where each increment is the product of the size of the interval, h , and an estimated slope specified by function f on the right-hand side of the differential equation.       k  1     subscript  k  1    k_{1}   is the increment based on the slope at the beginning of the interval, using   y   y   {y}   , ( Euler's method ) ;      k  2     subscript  k  2    k_{2}   is the increment based on the slope at the midpoint of the interval, using    y  +    h  2    k  1        y      h  2    subscript  k  1      {y}+\tfrac{h}{2}k_{1}   ;      k  3     subscript  k  3    k_{3}   is again the increment based on the slope at the midpoint, but now using    y  +    h  2    k  2        y      h  2    subscript  k  2      {y}+\tfrac{h}{2}k_{2}   ;      k  4     subscript  k  4    k_{4}   is the increment based on the slope at the end of the interval, using    y  +   h   k  3        y    h   subscript  k  3      {y}+hk_{3}   .   In averaging the four increments, greater weight is given to the increments at the midpoint. If   f   f   f   is independent of   y   y   y   , so that the differential equation is equivalent to a simple integral, then RK4 is Simpson's rule . 3  The RK4 method is a fourth-order method, meaning that the local truncation error is on the order of     O   (   h  5   )       O   superscript  h  5     O(h^{5})   , while the total accumulated error is order    O   (   h  4   )       O   superscript  h  4     O(h^{4})   .  Explicit Runge‚ÄìKutta methods  The family of explicit Runge‚ÄìKutta methods is a generalization of the RK4 method mentioned above. It is given by        y   n  +  1    =    y  n   +   h    ‚àë   i  =  1   s     b  i    k  i        ,       subscript  y    n  1       subscript  y  n     h    superscript   subscript     i  1    s      subscript  b  i    subscript  k  i         y_{n+1}=y_{n}+h\sum_{i=1}^{s}b_{i}k_{i},   where        k  1   =   f   (   t  n   ,   y  n   )     ,       subscript  k  1     f    subscript  t  n    subscript  y  n       k_{1}=f(t_{n},y_{n}),\,           k  2   =   f   (    t  n   +    c  2   h    ,    y  n   +   h   (    a  21    k  1    )     )     ,       subscript  k  2     f      subscript  t  n      subscript  c  2   h       subscript  y  n     h     subscript  a  21    subscript  k  1          k_{2}=f(t_{n}+c_{2}h,y_{n}+h(a_{21}k_{1})),\,           k  3   =   f   (    t  n   +    c  3   h    ,    y  n   +   h   (     a  31    k  1    +    a  32    k  2     )     )     ,       subscript  k  3     f      subscript  t  n      subscript  c  3   h       subscript  y  n     h       subscript  a  31    subscript  k  1       subscript  a  32    subscript  k  2           k_{3}=f(t_{n}+c_{3}h,y_{n}+h(a_{31}k_{1}+a_{32}k_{2})),\,          ‚ãÆ   normal-‚ãÆ   \vdots             k  s   =   f   (    t  n   +    c  s   h    ,    y  n   +   h   (     a   s  1     k  1    +    a   s  2     k  2    +  ‚ãØ  +    a   s  ,   s  -  1      k   s  -  1      )     )     .       subscript  k  s     f      subscript  t  n      subscript  c  s   h       subscript  y  n     h       subscript  a    s  1     subscript  k  1       subscript  a    s  2     subscript  k  2    normal-‚ãØ     subscript  a   s    s  1      subscript  k    s  1            k_{s}=f(t_{n}+c_{s}h,y_{n}+h(a_{s1}k_{1}+a_{s2}k_{2}+\cdots+a_{s,s-1}k_{s-1})).    4   (Note: the above equations have different but equivalent definitions in different texts). 5    To specify a particular method, one needs to provide the integer s (the number of stages), and the coefficients a ij (for 1 ‚â§ j i'' (for i = 1, 2, ..., s ) and c i (for i = 2, 3, ..., s ). The matrix [ a ij ] is called the Runge‚ÄìKutta matrix , while the b i and c i are known as the weights and the nodes . 6 These data are usually arranged in a mnemonic device, known as a Butcher tableau (after John C. Butcher ):       0          c  2     subscript  c  2    c_{2}             c  3     subscript  c  3    c_{3}            ‚ãÆ   normal-‚ãÆ   \vdots             c  s     subscript  c  s    c_{s}            The Runge‚ÄìKutta method is consistent if          ‚àë   j  =  1    i  -  1     a   i  j     =     c  i     for   i   =  2   ,   ‚Ä¶  ,  s    .     formulae-sequence        superscript   subscript     j  1      i  1     subscript  a    i  j        subscript  c  i   for  i        2     normal-‚Ä¶  s     \sum_{j=1}^{i-1}a_{ij}=c_{i}\ \mathrm{for}\ i=2,\ldots,s.   There are also accompanying requirements if one requires the method to have a certain order p , meaning that the local truncation error is O( h p +1 ). These can be derived from the definition of the truncation error itself. For example, a two-stage method has order 2 if b 1 + b 2 = 1, b 2 c 2 = 1/2, and a 21 = c 2 . 7  In general, if an explicit   s   s   s   -stage Runge-Kutta method has order   p   p   p   , then    s  ‚â•  p      s  p    s\geq p   , and if    p  ‚â•  5      p  5    p\geq 5   , then    s  >  p      s  p    s>p   . 8 The minimum   s   s   s   required for an explicit   s   s   s   -stage Runge-Kutta method to have order   p   p   p   is an open problem. Some values which are known are 9        p    1    2    3    4    5    6    7    8       min  s     1    2    3    4    6    7    9    11        p  1  2  3  4  5  6  7  8      s   1  2  3  4  6  7  9  11     \begin{array}[]{c|cccccccc}p&1&2&3&4&5&6&7&8\\
 \hline\min s&1&2&3&4&6&7&9&11\end{array}     Examples  The RK4 method falls in this framework. Its tableau is: 10       0      1/2      1/2      1         A slight variation of "the" Runge‚ÄìKutta method is also due to Kutta in 1901 and is called the 3/8-rule. 11 The primary advantage this method has is that almost all of the error coefficients are smaller than the popular method, but it requires slightly more FLOPs (floating point operations) per time step. Its Butcher tableau is given by:       0      1/3      2/3      1         However, the simplest Runge‚ÄìKutta method is the (forward) Euler method , given by the formula     y   n  +  1    =    y  n   +   h  f   (   t  n   ,   y  n   )          subscript  y    n  1       subscript  y  n     h  f    subscript  t  n    subscript  y  n        y_{n+1}=y_{n}+hf(t_{n},y_{n})   . This is the only consistent explicit Runge‚ÄìKutta method with one stage. The corresponding tableau is:       style="border-right:1px solid; border-bottom:1px solid;" 0   style="border-bottom:1px solid;"       1     Second-order methods with two stages  An example of a second-order method with two stages is provided by the midpoint method        y   n  +  1    =    y  n   +   h  f   (    t  n   +    1  2   h    ,    y  n   +    1  2   h  f   (   t  n   ,   y  n   )     )      .       subscript  y    n  1       subscript  y  n     h  f      subscript  t  n       1  2   h       subscript  y  n       1  2   h  f    subscript  t  n    subscript  y  n           y_{n+1}=y_{n}+hf\left(t_{n}+\frac{1}{2}h,y_{n}+\frac{1}{2}hf(t_{n},y_{n})%
 \right).   The corresponding tableau is:       0      1/2         The midpoint method is not the only second-order Runge‚ÄìKutta method with two stages; there is a family of such methods, parameterized by Œ±, and given by the formula        y   n  +  1    =    y  n   +   h   (     (   1  -    1   2  Œ±      )   f   (   t  n   ,   y  n   )    +     1   2  Œ±     f   (    t  n   +   Œ±  h    ,    y  n   +   Œ±  h  f   (   t  n   ,   y  n   )     )     )      .       subscript  y    n  1       subscript  y  n     h        1    1    2  Œ±     f    subscript  t  n    subscript  y  n         1    2  Œ±    f      subscript  t  n     Œ±  h       subscript  y  n     Œ±  h  f    subscript  t  n    subscript  y  n             y_{n+1}=y_{n}+h\bigl((1-\tfrac{1}{2\alpha})f(t_{n},y_{n})+\tfrac{1}{2\alpha}f(%
 t_{n}+\alpha h,y_{n}+\alpha hf(t_{n},y_{n}))\bigr).    12  Its Butcher tableau is       0         Œ±   Œ±   \alpha            In this family,    Œ±  =   1  2       Œ±    1  2     \alpha=\tfrac{1}{2}   gives the midpoint method and    Œ±  =  1      Œ±  1    \alpha=1   is Heun's method . 13  Usage  As an example, consider the two-stage second-order Runge‚ÄìKutta method with Œ± = 2/3, also known as Ralston method . It is given by the tableau       0      2/3         with the corresponding equations      k  1     subscript  k  1    \displaystyle k_{1}     This method is used to solve the initial-value problem        y  ‚Ä≤   =    tan   (  y  )    +  1    ,     y  0   =  1   ,   t  ‚àà   [  1  ,  1.1  ]        formulae-sequence     superscript  y  normal-‚Ä≤       y   1     formulae-sequence     subscript  y  0   1     t   1  1.1       y^{\prime}=\tan(y)+1,\quad y_{0}=1,\ t\in[1,1.1]   with step size h = 0.025, so the method needs to take four steps.  The method proceeds as follows:            t  0   =  1   :      normal-:     subscript  t  0   1   absent    t_{0}=1\colon                y  0   =  1       subscript  y  0   1    y_{0}=1              t  1   =  1.025   :      normal-:     subscript  t  1   1.025   absent    t_{1}=1.025\colon              y  0   =  1       subscript  y  0   1    y_{0}=1           k  1   =  2.557407725       subscript  k  1   2.557407725    k_{1}=2.557407725              y  1   =    y  0   +   h   (     1  4    k  1    +    3  4    k  2     )     =   1.066869388  ¬Ø          subscript  y  1      subscript  y  0     h        1  4    subscript  k  1        3  4    subscript  k  2             normal-¬Ø  1.066869388      y_{1}=y_{0}+h(\tfrac{1}{4}k_{1}+\tfrac{3}{4}k_{2})=\underline{1.066869388}              t  2   =  1.05   :      normal-:     subscript  t  2   1.05   absent    t_{2}=1.05\colon              y  1   =  1.066869388       subscript  y  1   1.066869388    y_{1}=1.066869388           k  1   =  2.813524695       subscript  k  1   2.813524695    k_{1}=2.813524695              y  2   =    y  1   +   h   (     1  4    k  1    +    3  4    k  2     )     =   1.141332181  ¬Ø          subscript  y  2      subscript  y  1     h        1  4    subscript  k  1        3  4    subscript  k  2             normal-¬Ø  1.141332181      y_{2}=y_{1}+h(\tfrac{1}{4}k_{1}+\tfrac{3}{4}k_{2})=\underline{1.141332181}              t  3   =  1.075   :      normal-:     subscript  t  3   1.075   absent    t_{3}=1.075\colon              y  2   =  1.141332181       subscript  y  2   1.141332181    y_{2}=1.141332181           k  1   =  3.183536647       subscript  k  1   3.183536647    k_{1}=3.183536647              y  3   =    y  2   +   h   (     1  4    k  1    +    3  4    k  2     )     =   1.227417567  ¬Ø          subscript  y  3      subscript  y  2     h        1  4    subscript  k  1        3  4    subscript  k  2             normal-¬Ø  1.227417567      y_{3}=y_{2}+h(\tfrac{1}{4}k_{1}+\tfrac{3}{4}k_{2})=\underline{1.227417567}              t  4   =  1.1   :      normal-:     subscript  t  4   1.1   absent    t_{4}=1.1\colon              y  3   =  1.227417567       subscript  y  3   1.227417567    y_{3}=1.227417567           k  1   =  3.796866512       subscript  k  1   3.796866512    k_{1}=3.796866512               y  4   =    y  3   +   h   (     1  4    k  1    +    3  4    k  2     )     =   1.335079087  ¬Ø    .         subscript  y  4      subscript  y  3     h        1  4    subscript  k  1        3  4    subscript  k  2             normal-¬Ø  1.335079087      y_{4}=y_{3}+h(\tfrac{1}{4}k_{1}+\tfrac{3}{4}k_{2})=\underline{1.335079087}.        The numerical solutions correspond to the underlined values.  Adaptive Runge‚ÄìKutta methods  The adaptive methods are designed to produce an estimate of the local truncation error of a single Runge‚ÄìKutta step. This is done by having two methods in the tableau, one with order   p   p   p   and one with order    p  -  1      p  1    p-1   .  The lower-order step is given by        y   n  +  1   *   =    y  n   +   h    ‚àë   i  =  1   s     b  i  *    k  i        ,       subscript   superscript  y      n  1       subscript  y  n     h    superscript   subscript     i  1    s      subscript   superscript  b    i    subscript  k  i         y^{*}_{n+1}=y_{n}+h\sum_{i=1}^{s}b^{*}_{i}k_{i},   where the    k  i     subscript  k  i    k_{i}   are the same as for the higher-order method. Then the error is        e   n  +  1    =    y   n  +  1    -   y   n  +  1   *    =   h    ‚àë   i  =  1   s     (    b  i   -   b  i  *    )    k  i       ,         subscript  e    n  1       subscript  y    n  1     subscript   superscript  y      n  1            h    superscript   subscript     i  1    s        subscript  b  i    subscript   superscript  b    i     subscript  k  i         e_{n+1}=y_{n+1}-y^{*}_{n+1}=h\sum_{i=1}^{s}(b_{i}-b^{*}_{i})k_{i},   which is    O   (   h  p   )       O   superscript  h  p     O(h^{p})   . The Butcher tableau for this kind of method is extended to give the values of    b  i  *     subscript   superscript  b    i    b^{*}_{i}   :       0          c  2     subscript  c  2    c_{2}             c  3     subscript  c  3    c_{3}            ‚ãÆ   normal-‚ãÆ   \vdots             c  s     subscript  c  s    c_{s}                The Runge‚ÄìKutta‚ÄìFehlberg method has two methods of orders 5 and 4. Its extended Butcher tableau is:       0      1/4      3/8      12/13      1      1/2             However, the simplest adaptive Runge‚ÄìKutta method involves combining Heun's method , which is order 2, with the Euler method , which is order 1. Its extended Butcher tableau is:       0      1             The error estimate is used to control the step size.  Other adaptive Runge‚ÄìKutta methods are the Bogacki‚ÄìShampine method (orders 3 and 2), the Cash‚ÄìKarp method and the Dormand‚ÄìPrince method (both with orders 5 and 4).  Nonconfluent Runge‚ÄìKutta methods  A Runge‚ÄìKutta method is said to be nonconfluent  14 if all the       c  i   ,  i   =  1   ,   2  ,  ‚Ä¶  ,  s      formulae-sequence      subscript  c  i   i   1    2  normal-‚Ä¶  s     c_{i},\,i=1,2,\ldots,s   are distinct.  Implicit Runge‚ÄìKutta methods  All Runge‚ÄìKutta methods mentioned up to now are explicit methods . Explicit Runge‚ÄìKutta methods are generally unsuitable for the solution of stiff equations because their region of absolute stability is small; in particular, it is bounded. 15 This issue is especially important in the solution of partial differential equations .  The instability of explicit Runge‚ÄìKutta methods motivates the development of implicit methods. An implicit Runge‚ÄìKutta method has the form        y   n  +  1    =    y  n   +   h    ‚àë   i  =  1   s     b  i    k  i        ,       subscript  y    n  1       subscript  y  n     h    superscript   subscript     i  1    s      subscript  b  i    subscript  k  i         y_{n+1}=y_{n}+h\sum_{i=1}^{s}b_{i}k_{i},   where         k  i   =   f   (    t  n   +    c  i   h    ,    y  n   +   h    ‚àë   j  =  1   s     a   i  j     k  j       )     ,   i  =   1  ,  ‚Ä¶  ,  s     .     formulae-sequence     subscript  k  i     f      subscript  t  n      subscript  c  i   h       subscript  y  n     h    superscript   subscript     j  1    s      subscript  a    i  j     subscript  k  j            i   1  normal-‚Ä¶  s      k_{i}=f\left(t_{n}+c_{i}h,y_{n}+h\sum_{j=1}^{s}a_{ij}k_{j}\right),\quad i=1,%
 \ldots,s.    16  The difference with an explicit method is that in an explicit method, the sum over j only goes up to i ‚àí 1. This also shows up in the Butcher tableau: the coefficient matrix    a   i  j      subscript  a    i  j     a_{ij}   of an explicit method is lower triangular. In an implicit method, the sum over j goes up to s and the coefficient matrix is not triangular, yielding a Butcher tableau of the form 17          c  1      a  11      a  12     ‚Ä¶     a   1  s         c  2      a  21      a  22     ‚Ä¶     a   2  s        ‚ãÆ    ‚ãÆ    ‚ãÆ    ‚ã±    ‚ãÆ       c  s      a   s  1       a   s  2      ‚Ä¶     a   s  s            b  1      b  2     ‚Ä¶     b  s      =     ùêú    A          ùêõ  ùêì             subscript  c  1    subscript  a  11    subscript  a  12   normal-‚Ä¶   subscript  a    1  s       subscript  c  2    subscript  a  21    subscript  a  22   normal-‚Ä¶   subscript  a    2  s      normal-‚ãÆ  normal-‚ãÆ  normal-‚ãÆ  normal-‚ã±  normal-‚ãÆ     subscript  c  s    subscript  a    s  1     subscript  a    s  2    normal-‚Ä¶   subscript  a    s  s      absent   subscript  b  1    subscript  b  2   normal-‚Ä¶   subscript  b  s       ùêú  A    absent   superscript  ùêõ  ùêì       \begin{array}[]{c|cccc}c_{1}&a_{11}&a_{12}&\dots&a_{1s}\\
 c_{2}&a_{21}&a_{22}&\dots&a_{2s}\\
 \vdots&\vdots&\vdots&\ddots&\vdots\\
 c_{s}&a_{s1}&a_{s2}&\dots&a_{ss}\\
 \hline&b_{1}&b_{2}&\dots&b_{s}\\
 \end{array}=\par
 \begin{array}[]{c|c}\mathbf{c}&A\\
 \hline&\mathbf{b^{T}}\\
 \end{array}     The consequence of this difference is that at every step, a system of algebraic equations has to be solved. This increases the computational cost considerably. If a method with s stages is used to solve a differential equation with m components, then the system of algebraic equations has ms components. This can be contrasted with implicit linear multistep methods (the other big family of methods for ODEs): an implicit s -step linear multistep method needs to solve a system of algebraic equations with only m components, so the size of the system does not increase as the number of steps increases. 18  Examples  The simplest example of an implicit Runge‚ÄìKutta method is the backward Euler method :        y   n  +  1    =    y  n   +   h  f   (    t  n   +  h   ,   y   n  +  1    )      .       subscript  y    n  1       subscript  y  n     h  f      subscript  t  n   h    subscript  y    n  1         y_{n+1}=y_{n}+hf(t_{n}+h,y_{n+1}).\,     The Butcher tableau for this is simply:        1    1         1        1  1    absent  1     \begin{array}[]{c|c}1&1\\
 \hline&1\\
 \end{array}     This Butcher tableau corresponds to the formulae         k  1   =    f   (    t  n   +  h   ,    y  n   +   h   k  1     )    and      y   n  +  1    =    y  n   +   h   k  1       ,     formulae-sequence     subscript  k  1      f      subscript  t  n   h      subscript  y  n     h   subscript  k  1       and       subscript  y    n  1       subscript  y  n     h   subscript  k  1        k_{1}=f(t_{n}+h,y_{n}+hk_{1})\quad\text{and}\quad y_{n+1}=y_{n}+hk_{1},     which can be re-arranged to get the formula for the backward Euler method listed above.  Another example for an implicit Runge‚ÄìKutta method is the trapezoidal rule . Its Butcher tableau is:        0    0    0      1     1  2      1  2           1  2      1  2         0  0  0    1    1  2     1  2     absent    1  2     1  2      \begin{array}[]{c|cc}0&0&0\\
 1&\frac{1}{2}&\frac{1}{2}\\
 \hline&\frac{1}{2}&\frac{1}{2}\\
 \end{array}     The trapezoidal rule is a collocation method (as discussed in that article). All collocation methods are implicit Runge‚ÄìKutta methods, but not all implicit Runge‚ÄìKutta methods are collocation methods. 19  The Gauss‚ÄìLegendre methods form a family of collocation methods based on Gauss quadrature . A Gauss‚ÄìLegendre method with s stages has order 2 s (thus, methods with arbitrarily high order can be constructed). 20 The method with two stages (and thus order four) has Butcher tableau:          1  2   -    1  6    3        1  4       1  4   -    1  6    3           1  2   +    1  6    3         1  4   +    1  6    3        1  4           1  2      1  2             1  2       1  6     3       1  4       1  4       1  6     3           1  2       1  6     3         1  4       1  6     3       1  4     absent    1  2     1  2      \begin{array}[]{c|cc}\frac{1}{2}-\frac{1}{6}\sqrt{3}&\frac{1}{4}&\frac{1}{4}-%
 \frac{1}{6}\sqrt{3}\\
 \frac{1}{2}+\frac{1}{6}\sqrt{3}&\frac{1}{4}+\frac{1}{6}\sqrt{3}&\frac{1}{4}\\
 \hline&\frac{1}{2}&\frac{1}{2}\end{array}    21  Stability  The advantage of implicit Runge‚ÄìKutta methods over explicit ones is their greater stability, especially when applied to stiff equations . Consider the linear test equation y ' = Œª y . A Runge‚ÄìKutta method applied to this equation reduces to the iteration     y   n  +  1    =   r   (   h  Œª   )    y  n         subscript  y    n  1      r    h  Œª    subscript  y  n      y_{n+1}=r(h\lambda)\,y_{n}   , with r given by        r   (  z  )    =   1  +   z   b  T     (   I  -   z  A    )    -  1    e    =    det   (    I  -   z  A    +   z  e   b  T     )     det   (   I  -   z  A    )      ,          r  z     1    z   superscript  b  T    superscript    I    z  A      1    e                 I    z  A      z  e   superscript  b  T          I    z  A         r(z)=1+zb^{T}(I-zA)^{-1}e=\frac{\det(I-zA+zeb^{T})}{\det(I-zA)},    22  where e stands for the vector of ones. The function r is called the stability function . 23 It follows from the formula that r is the quotient of two polynomials of degree s if the method has s stages. Explicit methods have a strictly lower triangular matrix A , which implies that det( I ‚àí zA ) = 1 and that the stability function is a polynomial. 24  The numerical solution to the linear test equation decays to zero if | r ( z ) |  If the method has order p , then the stability function satisfies     r   (  z  )    =    e  z   +   O   (   z   p  +  1    )           r  z      superscript  e  z     O   superscript  z    p  1        r(z)=\textrm{e}^{z}+O(z^{p+1})   as    z  ‚Üí  0     normal-‚Üí  z  0    z\to 0   . Thus, it is of interest to study quotients of polynomials of given degrees that approximate the exponential function the best. These are known as Pad√© approximants . A Pad√© approximant with numerator of degree m and denominator of degree n is A-stable if and only if m ‚â§ n ‚â§ m + 2. 25  The Gauss‚ÄìLegendre method with s stages has order 2 s , so its stability function is the Pad√© approximant with m = n = s . It follows that the method is A-stable. 26 This shows that A-stable Runge‚ÄìKutta can have arbitrarily high order. In contrast, the order of A-stable linear multistep methods cannot exceed two. 27  B-stability  The A-stability concept for the solution of differential equations is related to the linear autonomous equation     y  ‚Ä≤   =   Œª  y        superscript  y  normal-‚Ä≤     Œª  y     y^{\prime}=\lambda y   . Dahlquist proposed the investigation of stability of numerical schemes when applied to nonlinear systems which satisfies a monotonicity condition. The corresponding concepts were defined as G-stability for multistep methods (and the related one-leg methods) and B-stability (Butcher, 1975) for Runge‚ÄìKutta methods. A Runge‚ÄìKutta method applied to the non-linear system     y  ‚Ä≤   =   f   (  y  )         superscript  y  normal-‚Ä≤     f  y     y^{\prime}=f(y)   , which verifies     ‚ü®    f   (  y  )    -   f   (  z  )     ,   y  -  z   ‚ü©   <  0           f  y     f  z      y  z    0    \langle f(y)-f(z),y-z\rangle<0   , is called B-stable , if this condition implies     ‚à•    y   n  +  1    -   z   n  +  1     ‚à•   ‚â§   ‚à•    y  n   -   z  n    ‚à•        norm     subscript  y    n  1     subscript  z    n  1       norm     subscript  y  n    subscript  z  n       \|y_{n+1}-z_{n+1}\|\leq\|y_{n}-z_{n}\|   for two numerical solutions.  Let   B   B   B   ,   M   M   M   and   Q   Q   Q   be three    s  √ó  s      s  s    s\times s   matrices defined by        B  =   diag   (   b  1   ,   b  2   ,  ‚Ä¶  ,   b  s   )     ,    M  =     B  A   +    A  T   B    -   b   b  T      ,   Q  =     B   A   -  1     +    A   -  T    B    -    A   -  T    b   b  T    A   -  1         .     formulae-sequence    B   diag   subscript  b  1    subscript  b  2   normal-‚Ä¶   subscript  b  s      formulae-sequence    M        B  A      superscript  A  T   B      b   superscript  b  T        Q        B   superscript  A    1        superscript  A    T    B       superscript  A    T    b   superscript  b  T    superscript  A    1          B=\operatorname{diag}(b_{1},b_{2},\ldots,b_{s}),\,M=BA+A^{T}B-bb^{T},\,Q=BA^{-%
 1}+A^{-T}B-A^{-T}bb^{T}A^{-1}.     A Runge‚ÄìKutta method is said to be algebraically stable  28 if the matrices   B   B   B   and   M   M   M   are both non-negative definite. A sufficient condition for B-stability  29 is   B   B   B   and   Q   Q   Q   are non-negative definite.  Derivation of the Runge‚ÄìKutta fourth-order method  In general a Runge‚ÄìKutta method of order   s   s   s   can be written as:        y   t  +  h    =    y  t   +   h  ‚ãÖ    ‚àë   i  =  1   s     a  i    k  i      +   ùí™   (   h   s  +  1    )      ,       subscript  y    t  h       subscript  y  t    normal-‚ãÖ  h    superscript   subscript     i  1    s      subscript  a  i    subscript  k  i        ùí™   superscript  h    s  1        y_{t+h}=y_{t}+h\cdot\sum_{i=1}^{s}a_{i}k_{i}+\mathcal{O}(h^{s+1}),   where:       k  i   =   f   (    y  t   +   h  ‚ãÖ    ‚àë   j  =  1   s     Œ≤   i  j     k  j       ,    t  n   +    Œ±  i   h    )         subscript  k  i     f      subscript  y  t    normal-‚ãÖ  h    superscript   subscript     j  1    s      subscript  Œ≤    i  j     subscript  k  j          subscript  t  n      subscript  Œ±  i   h        k_{i}=f\left(y_{t}+h\cdot\sum_{j=1}^{s}\beta_{ij}k_{j},t_{n}+\alpha_{i}h\right)   are increments obtained evaluating the derivatives of    y  t     subscript  y  t    y_{t}   at the   i   i   i   -th order.  We develop the derivation 30 for the Runge‚ÄìKutta fourth-order method using the general formula with    s  =  4      s  4    s=4   evaluated, as explained above, at the starting point, the midpoint and the end point of any interval    (  t  ,   t  +  h   )     t    t  h     (t,t+h)   , thus we choose:         Œ±  i      Œ≤   i  j          Œ±  1   =  0       Œ≤  21   =   1  2          Œ±  2   =   1  2        Œ≤  32   =   1  2          Œ±  3   =   1  2        Œ≤  43   =  1         Œ±  4   =  1            subscript  Œ±  i    subscript  Œ≤    i  j         subscript  Œ±  1   0      subscript  Œ≤  21     1  2         subscript  Œ±  2     1  2       subscript  Œ≤  32     1  2         subscript  Œ±  3     1  2       subscript  Œ≤  43   1        subscript  Œ±  4   1    missing-subexpression      \begin{array}[]{|l|l|}\hline\alpha_{i}&\beta_{ij}\\
 \hline\alpha_{1}=0&\beta_{21}=\frac{1}{2}\\
 \alpha_{2}=\frac{1}{2}&\beta_{32}=\frac{1}{2}\\
 \alpha_{3}=\frac{1}{2}&\beta_{43}=1\\
 \alpha_{4}=1&\\
 \hline\end{array}     and     Œ≤   i  j    =  0       subscript  Œ≤    i  j    0    \beta_{ij}=0   otherwise. We begin by defining the following quantities:      y   t  +  h   1     subscript   superscript  y  1     t  h     \displaystyle y^{1}_{t+h}   where     y   t  +   h  /  2    1   =      y  t   +   y   t  +  h   1    2         subscript   superscript  y  1     t    h  2          subscript  y  t    subscript   superscript  y  1     t  h     2     y^{1}_{t+h/2}=\dfrac{y_{t}+y^{1}_{t+h}}{2}   and     y   t  +   h  /  2    2   =      y  t   +   y   t  +  h   2    2         subscript   superscript  y  2     t    h  2          subscript  y  t    subscript   superscript  y  2     t  h     2     y^{2}_{t+h/2}=\dfrac{y_{t}+y^{2}_{t+h}}{2}   If we define:      k  1     subscript  k  1    \displaystyle k_{1}   and for the previous relations we can show that the following equalities holds up to    ùí™   (   h  2   )       ùí™   superscript  h  2     \mathcal{O}(h^{2})   :      k  2     subscript  k  2    \displaystyle k_{2}   where:        d   d  t    f   (   y  t   ,  t  )    =     ‚àÇ   ‚àÇ  y    f   (   y  t   ,  t  )     y  Àô   t    +    ‚àÇ   ‚àÇ  t    f   (   y  t   ,  t  )     =     f  y    (   y  t   ,  t  )    y  Àô    +    f  t    (   y  t   ,  t  )     :=    y  ¬®   t             d    d  t    f    subscript  y  t   t             y    f    subscript  y  t   t    subscript   normal-Àô  y   t           t    f    subscript  y  t   t               subscript  f  y     subscript  y  t   t    normal-Àô  y       subscript  f  t     subscript  y  t   t       assign     subscript   normal-¬®  y   t      \frac{d}{dt}f(y_{t},t)=\frac{\partial}{\partial y}f(y_{t},t)\dot{y}_{t}+\frac{%
 \partial}{\partial t}f(y_{t},t)=f_{y}(y_{t},t)\dot{y}+f_{t}(y_{t},t):=\ddot{y}%
 _{t}   is the total derivative of   f   f   f   with respect to time.  If we now express the general formula using what we just derived we obtain:      y   t  +  h      subscript  y    t  h     \displaystyle y_{t+h}     and comparing this with the Taylor series of    y   t  +  h      subscript  y    t  h     y_{t+h}   around    y  t     subscript  y  t    y_{t}   :      y   t  +  h      subscript  y    t  h     \displaystyle y_{t+h}     we obtain a system of constraints on the coefficients:      {          a  ;    +  b   +  c  +  d    =  1               1  2    b   +     1  2    c   +  d   =    1  2                 1  4    c   +     1  2    d    =    1  6                1  4    d   =    1  24           cases  absent     a      b   c  d    1   absent          1  2   b       1  2   c   d     1  2    absent          1  4   c       1  2   d      1  6    absent        1  4   d     1  24      \begin{cases}&a+b+c+d=1\\
 &\frac{1}{2}b+\frac{1}{2}c+d=\frac{1}{2}\\
 &\frac{1}{4}c+\frac{1}{2}d=\frac{1}{6}\\
 &\frac{1}{4}d=\frac{1}{24}\end{cases}   which when solved gives     a  =   1  6    ,    b  =   1  3    ,    c  =   1  3    ,   d  =   1  6         formulae-sequence    a    1  6     formulae-sequence    b    1  3     formulae-sequence    c    1  3      d    1  6        a=\frac{1}{6},b=\frac{1}{3},c=\frac{1}{3},d=\frac{1}{6}   as stated above.  See also   Dynamic errors of numerical methods of ODE discretization  Euler's method  List of Runge‚ÄìKutta methods  Numerical ordinary differential equations  PottersWheel ‚Äì Parameter calibration in ODE systems using implicit Runge‚ÄìKutta integration  Runge‚ÄìKutta method (SDE)   Notes  References    .   .   .   .   .   .   .   (see Chapter 6).   .   .   .    .   .   . Also, Section 17.2. Adaptive Stepsize Control for Runge-Kutta .   .   .   .   External links    On line calculator for Runge-Kutta methods by www.mathstools.com  Runge‚ÄìKutta 4th-Order Method  Runge Kutta Method for O.D.E.'s  DotNumerics: Ordinary Differential Equations for C# and VB.NET ‚Äî Initial-value problem for nonstiff and stiff ordinary differential equations (explicit Runge‚ÄìKutta, implicit Runge‚ÄìKutta, Gear's BDF and Adams‚ÄìMoulton).  GafferOnGames ‚Äî A physics resource for computer programmers   "  Category:Numerical differential equations  Category:Runge‚ÄìKutta methods     ; ‚Ü©  , ,  and  leave out the factor h in the definition of the stages. ,  and  use the y -values as stages. ‚Ü©  ‚Ü©  ‚Ü©   ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  refer to ‚Ü©  ‚Ü©   ‚Ü©  ‚Ü©  ; ‚Ü©   ‚Ü©  ‚Ü©  ‚Ü©   ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  This result is due to . ‚Ü©  ‚Ü©  ‚Ü©  PDF reporting this derivation ‚Ü©     