<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="827">Slope One</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Slope One</h1><hr/>

<p><strong>Slope One</strong> is a family of algorithms used for <a href="collaborative_filtering" title="wikilink">collaborative filtering</a>, introduced in a 2005 paper by Daniel Lemire and Anna Maclachlan.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Arguably, it is the simplest form of non-trivial <a href="item-item_collaborative_filtering" title="wikilink">item-based collaborative filtering</a> based on ratings. Their simplicity makes it especially easy to implement them efficiently while their accuracy is often on par with more complicated and computationally expensive algorithms.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> They have also been used as building blocks to improve other algorithms.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a><a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a><a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> They are part of major open-source libraries such as <a href="Apache_Mahout" title="wikilink">Apache Mahout</a> and <a class="uri" href="Easyrec" title="wikilink">Easyrec</a>.</p>
<h2 id="item-based-collaborative-filtering-of-rated-resources-and-overfitting">Item-based collaborative filtering of rated resources and overfitting</h2>

<p>When ratings of items are available, such as is the case when people are given the option of ratings resources (between 1 and 5, for example), collaborative filtering aims to predict the ratings of one individual based on his past ratings and on a (large) database of ratings contributed by other users.</p>

<p><strong>Example</strong>: Can we predict the rating an individual would give to the new Celine Dion album given that he gave the Beatles 5 out of 5?</p>

<p>In this context, item-based collaborative filtering <a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> predicts the ratings on one item based on the ratings on another item, typically using <a href="linear_regression" title="wikilink">linear regression</a> (

<math display="inline" id="Slope_One:0">
<semantics>
<mrow>
<mrow>
<mi>f</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mrow>
<mi>a</mi>
<mi>x</mi>
</mrow>
<mo>+</mo>
<mi>b</mi>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<times></times>
<ci>f</ci>
<ci>x</ci>
</apply>
<apply>
<plus></plus>
<apply>
<times></times>
<ci>a</ci>
<ci>x</ci>
</apply>
<ci>b</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   f(x)=ax+b
  </annotation>
</semantics>
</math>

). Hence, if there are 1,000 items, there could be up to 1,000,000 linear regressions to be learned, and so, up to 2,000,000 regressors. This approach may suffer from severe <a class="uri" href="overfitting" title="wikilink">overfitting</a><a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> unless we select only the pairs of items for which several users have rated both items.</p>

<p>A better alternative may be to learn a simpler predictor such as 

<math display="inline" id="Slope_One:1">
<semantics>
<mrow>
<mrow>
<mi>f</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mi>x</mi>
<mo>+</mo>
<mi>b</mi>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<times></times>
<ci>f</ci>
<ci>x</ci>
</apply>
<apply>
<plus></plus>
<ci>x</ci>
<ci>b</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   f(x)=x+b
  </annotation>
</semantics>
</math>

: experiments show that this simpler predictor (called Slope One) sometimes outperforms<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> linear regression while having half the number of regressors. This simplified approach also reduces storage requirements and latency.</p>

<p>Item-based collaborative filtering is just one form of <a href="collaborative_filtering" title="wikilink">collaborative filtering</a>. Other alternatives include user-based collaborative filtering where relationships between users are of interest, instead. However, item-based collaborative filtering is especially scalable with respect to the number of users.</p>
<h2 id="item-based-collaborative-filtering-of-purchase-statistics">Item-based collaborative filtering of purchase statistics</h2>

<p>We are not always given ratings: when the users provide only binary data (the item was purchased or not), then Slope One and other rating-based algorithm do not apply. Examples of binary item-based collaborative filtering include Amazon's <a href="http://doi.ieeecomputersociety.org/10.1109/MIC.2003.1167344">item-to-item</a> patented algorithm<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> which computes the cosine between binary vectors representing the purchases in a user-item matrix.</p>

<p>Being arguably simpler than even Slope One, the Item-to-Item algorithm offers an interesting point of reference. Let us consider an example.</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">

<p>Sample purchase statistics</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">

<p>Customer</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">

<p>John</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">

<p>Mark</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">

<p>Lucy</p></td>
</tr>
</tbody>
</table>

<p>In this case, the cosine between items 1 and 2 is:</p>

<p>
<math display="inline" id="Slope_One:2">
<semantics>
<mrow>
<mfrac>
<mrow>
<mrow>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo>,</mo>
<mn>0</mn>
<mo>,</mo>
<mn>0</mn>
<mo stretchy="false">)</mo>
</mrow>
<mo>⋅</mo>
<mrow>
<mo stretchy="false">(</mo>
<mn>0</mn>
<mo>,</mo>
<mn>1</mn>
<mo>,</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mrow>
<mrow>
<mo>∥</mo>
<mrow>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo>,</mo>
<mn>0</mn>
<mo>,</mo>
<mn>0</mn>
<mo stretchy="false">)</mo>
</mrow>
<mo>∥</mo>
</mrow>
<mrow>
<mo>∥</mo>
<mrow>
<mo stretchy="false">(</mo>
<mn>0</mn>
<mo>,</mo>
<mn>1</mn>
<mo>,</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
<mo>∥</mo>
</mrow>
</mrow>
</mfrac>
<mo>=</mo>
<mn>0</mn>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<divide></divide>
<apply>
<ci>normal-⋅</ci>
<vector>
<cn type="integer">1</cn>
<cn type="integer">0</cn>
<cn type="integer">0</cn>
</vector>
<vector>
<cn type="integer">0</cn>
<cn type="integer">1</cn>
<cn type="integer">1</cn>
</vector>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="latexml">norm</csymbol>
<vector>
<cn type="integer">1</cn>
<cn type="integer">0</cn>
<cn type="integer">0</cn>
</vector>
</apply>
<apply>
<csymbol cd="latexml">norm</csymbol>
<vector>
<cn type="integer">0</cn>
<cn type="integer">1</cn>
<cn type="integer">1</cn>
</vector>
</apply>
</apply>
</apply>
<cn type="integer">0</cn>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \frac{(1,0,0)\cdot(0,1,1)}{\|(1,0,0)\|\|(0,1,1)\|}=0
  </annotation>
</semantics>
</math>

,</p>

<p>The cosine between items 1 and 3 is:</p>

<p>
<math display="inline" id="Slope_One:3">
<semantics>
<mrow>
<mfrac>
<mrow>
<mrow>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo>,</mo>
<mn>0</mn>
<mo>,</mo>
<mn>0</mn>
<mo stretchy="false">)</mo>
</mrow>
<mo>⋅</mo>
<mrow>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo>,</mo>
<mn>1</mn>
<mo>,</mo>
<mn>0</mn>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mrow>
<mrow>
<mo>∥</mo>
<mrow>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo>,</mo>
<mn>0</mn>
<mo>,</mo>
<mn>0</mn>
<mo stretchy="false">)</mo>
</mrow>
<mo>∥</mo>
</mrow>
<mrow>
<mo>∥</mo>
<mrow>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo>,</mo>
<mn>1</mn>
<mo>,</mo>
<mn>0</mn>
<mo stretchy="false">)</mo>
</mrow>
<mo>∥</mo>
</mrow>
</mrow>
</mfrac>
<mo>=</mo>
<mfrac>
<mn>1</mn>
<msqrt>
<mn>2</mn>
</msqrt>
</mfrac>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<divide></divide>
<apply>
<ci>normal-⋅</ci>
<vector>
<cn type="integer">1</cn>
<cn type="integer">0</cn>
<cn type="integer">0</cn>
</vector>
<vector>
<cn type="integer">1</cn>
<cn type="integer">1</cn>
<cn type="integer">0</cn>
</vector>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="latexml">norm</csymbol>
<vector>
<cn type="integer">1</cn>
<cn type="integer">0</cn>
<cn type="integer">0</cn>
</vector>
</apply>
<apply>
<csymbol cd="latexml">norm</csymbol>
<vector>
<cn type="integer">1</cn>
<cn type="integer">1</cn>
<cn type="integer">0</cn>
</vector>
</apply>
</apply>
</apply>
<apply>
<divide></divide>
<cn type="integer">1</cn>
<apply>
<root></root>
<cn type="integer">2</cn>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \frac{(1,0,0)\cdot(1,1,0)}{\|(1,0,0)\|\|(1,1,0)\|}=\frac{1}{\sqrt{2}}
  </annotation>
</semantics>
</math>


,</p>

<p>Whereas the cosine between items 2 and 3 is:</p>

<p>
<math display="inline" id="Slope_One:4">
<semantics>
<mrow>
<mfrac>
<mrow>
<mrow>
<mo stretchy="false">(</mo>
<mn>0</mn>
<mo>,</mo>
<mn>1</mn>
<mo>,</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
<mo>⋅</mo>
<mrow>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo>,</mo>
<mn>1</mn>
<mo>,</mo>
<mn>0</mn>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mrow>
<mrow>
<mo>∥</mo>
<mrow>
<mo stretchy="false">(</mo>
<mn>0</mn>
<mo>,</mo>
<mn>1</mn>
<mo>,</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
<mo>∥</mo>
</mrow>
<mrow>
<mo>∥</mo>
<mrow>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo>,</mo>
<mn>1</mn>
<mo>,</mo>
<mn>0</mn>
<mo stretchy="false">)</mo>
</mrow>
<mo>∥</mo>
</mrow>
</mrow>
</mfrac>
<mo>=</mo>
<mfrac>
<mn>1</mn>
<mn>2</mn>
</mfrac>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<divide></divide>
<apply>
<ci>normal-⋅</ci>
<vector>
<cn type="integer">0</cn>
<cn type="integer">1</cn>
<cn type="integer">1</cn>
</vector>
<vector>
<cn type="integer">1</cn>
<cn type="integer">1</cn>
<cn type="integer">0</cn>
</vector>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="latexml">norm</csymbol>
<vector>
<cn type="integer">0</cn>
<cn type="integer">1</cn>
<cn type="integer">1</cn>
</vector>
</apply>
<apply>
<csymbol cd="latexml">norm</csymbol>
<vector>
<cn type="integer">1</cn>
<cn type="integer">1</cn>
<cn type="integer">0</cn>
</vector>
</apply>
</apply>
</apply>
<apply>
<divide></divide>
<cn type="integer">1</cn>
<cn type="integer">2</cn>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \frac{(0,1,1)\cdot(1,1,0)}{\|(0,1,1)\|\|(1,1,0)\|}=\frac{1}{2}
  </annotation>
</semantics>
</math>

.</p>

<p>Hence, a user visiting item 1 would receive item 3 as a recommendation, a user visiting item 2 would receive item 3 as a recommendation, and finally, a user visiting item 3 would receive item 1 (and then item 2) as a recommendation. The model uses a single parameter per pair of item (the cosine) to make the recommendation. Hence, if there are <em>n</em> items, up to <em>n(n-1)/2</em> cosines need to be computed and stored.</p>
<h2 id="slope-one-collaborative-filtering-for-rated-resources">Slope one collaborative filtering for rated resources</h2>

<p>To drastically reduce <a class="uri" href="overfitting" title="wikilink">overfitting</a>, improve performance and ease implementation, the <strong>Slope One</strong> family of easily implemented Item-based Rating-Based <a href="collaborative_filtering" title="wikilink">collaborative filtering</a> algorithms was proposed. Essentially, instead of using linear regression from one item's ratings to another item's ratings (

<math display="inline" id="Slope_One:5">
<semantics>
<mrow>
<mrow>
<mi>f</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mrow>
<mi>a</mi>
<mi>x</mi>
</mrow>
<mo>+</mo>
<mi>b</mi>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<times></times>
<ci>f</ci>
<ci>x</ci>
</apply>
<apply>
<plus></plus>
<apply>
<times></times>
<ci>a</ci>
<ci>x</ci>
</apply>
<ci>b</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   f(x)=ax+b
  </annotation>
</semantics>
</math>

), it uses a simpler form of regression with a single free parameter (

<math display="inline" id="Slope_One:6">
<semantics>
<mrow>
<mrow>
<mi>f</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mi>x</mi>
<mo>+</mo>
<mi>b</mi>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<times></times>
<ci>f</ci>
<ci>x</ci>
</apply>
<apply>
<plus></plus>
<ci>x</ci>
<ci>b</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   f(x)=x+b
  </annotation>
</semantics>
</math>

). The free parameter is then simply the average difference between the two items' ratings. It was shown to be much more accurate than linear regression in some instances,<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> and it takes half the storage or less.</p>
<figure><b>(Figure)</b>
<figcaption>Simplicity diagram.png</figcaption>
</figure>

<p><strong>Example</strong>:</p>
<ol>
<li>User A gave a 1 to Item I and an 1.5 to Item J.</li>
<li>User B gave a 2 to Item I.</li>
<li>How do you think User B rated Item J?</li>
<li>The Slope One answer is to say 2.5 (1.5-1+2=2.5).</li>
</ol>

<p>For a more realistic example, consider the following table.</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">

<p>Sample rating database</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">

<p>Customer</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">

<p>John</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">

<p>Mark</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">

<p>Lucy</p></td>
</tr>
</tbody>
</table>

<p>In this case, the average difference in ratings between item B and A is (2+(-1))/2=0.5. Hence, on average, item A is rated above item B by 0.5. Similarly, the average difference between item C and A is 3. Hence, if we attempt to predict the rating of Lucy for item A using her rating for item B, we get 2+0.5 = 2.5. Similarly, if we try to predict her rating for item A using her rating of item C, we get 5+3=8.</p>

<p>If a user rated several items, the predictions are simply combined using a weighted average where a good choice for the weight is the number of users having rated both items. In the above example, we would predict the following rating for Lucy on item A:</p>

<p>
<math display="inline" id="Slope_One:7">
<semantics>
<mrow>
<mfrac>
<mrow>
<mrow>
<mn>2</mn>
<mo>×</mo>
<mn>2.5</mn>
</mrow>
<mo>+</mo>
<mrow>
<mn>1</mn>
<mo>×</mo>
<mn>8</mn>
</mrow>
</mrow>
<mrow>
<mn>2</mn>
<mo>+</mo>
<mn>1</mn>
</mrow>
</mfrac>
<mo>=</mo>
<mfrac>
<mn>13</mn>
<mn>3</mn>
</mfrac>
<mo>=</mo>
<mn>4.33</mn>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<and></and>
<apply>
<eq></eq>
<apply>
<divide></divide>
<apply>
<plus></plus>
<apply>
<times></times>
<cn type="integer">2</cn>
<cn type="float">2.5</cn>
</apply>
<apply>
<times></times>
<cn type="integer">1</cn>
<cn type="integer">8</cn>
</apply>
</apply>
<apply>
<plus></plus>
<cn type="integer">2</cn>
<cn type="integer">1</cn>
</apply>
</apply>
<apply>
<divide></divide>
<cn type="integer">13</cn>
<cn type="integer">3</cn>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<cn type="float">4.33</cn>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \frac{2\times 2.5+1\times 8}{2+1}=\frac{13}{3}=4.33
  </annotation>
</semantics>
</math>
</p>

<p>Hence, given <em>n</em> items, to implement Slope One, all that is needed is to compute and store the average differences and the number of common ratings for each of the <em>n</em><sup>2</sup> pairs of items.</p>
<h2 id="algorithmic-complexity-of-slope-one">Algorithmic complexity of Slope One</h2>

<p>Suppose there are <em>n</em> items, <em>m</em> users, and <em>N</em> ratings. Computing the average rating differences for each pair of items requires up to <em>n(n-1)/2</em> units of storage, and up to <em>m n</em><sup>2</sup> time steps. This computational bound may be pessimistic: if we assume that users have rated up to <em>y</em> items, then it is possible to compute the differences in no more than <em>n</em><sup>2</sup>+<em>my</em><sup>2</sup>. If a user has entered <em>x</em> ratings, predicting a single rating requires <em>x</em> time steps, and predicting all of his missing ratings requires up to (<em>n-x</em>)<em>x</em> time steps. Updating the database when a user has already entered <em>x</em> ratings, and enters a new one, requires <em>x</em> time steps.</p>

<p>It is possible to reduce storage requirements by partitioning the data (see <a href="Partition_(database)" title="wikilink">Partition (database)</a>) or by using sparse storage: pairs of items having no (or few) corating users can be omitted.</p>
<h2 id="footnotes">Footnotes</h2>

<p>"</p>

<p><a class="uri" href="Category:Collaboration" title="wikilink">Category:Collaboration</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Daniel Lemire, Anna Maclachlan, <a href="http://arxiv.org/abs/cs/0702144">Slope One Predictors for Online Rating-Based Collaborative Filtering</a>, In SIAM Data Mining (SDM'05), Newport Beach, California, April 21–23, 2005.<a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3">Fidel Cacheda, Victor Carneiro, Diego Fernandez, and Vreixo Formoso. 2011. <a href="http://portal.acm.org/citation.cfm?id=1921593">Comparison of collaborative filtering algorithms: Limitations of current techniques and proposals for scalable, high-performance recommender systems</a>. ACM Trans. Web 5, 1, Article 2<a href="#fnref3">↩</a></li>
<li id="fn4">Pu Wang, HongWu Ye, <a href="http://doi.ieeecomputersociety.org/10.1109/IIS.2009.71">A Personalized Recommendation Algorithm Combining Slope One Scheme and User Based Collaborative Filtering</a>, IIS '09, 2009.<a href="#fnref4">↩</a></li>
<li id="fn5">DeJia Zhang, [<a class="uri" href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp">http://ieeexplore.ieee.org/stamp/stamp.jsp?tp</a>=&amp;arnumber;=5209738&amp;isnumber;=5209636 An Item-based Collaborative Filtering Recommendation Algorithm Using Slope One Scheme Smoothing], ISECS '09, 2009.<a href="#fnref5">↩</a></li>
<li id="fn6">Min Gaoa, Zhongfu Wub, and Feng Jiang, Userrank for item-based collaborative filtering recommendation, Information Processing Letters Volume 111, Issue 9, 1 April 2011, pp. 440-446.<a href="#fnref6">↩</a></li>
<li id="fn7">Mi, Zhenzhen and Xu, Congfu, A Recommendation Algorithm Combining Clustering Method and Slope One Scheme, Lecture Notes in Computer Science 6840, 2012, pp. 160-167.<a href="#fnref7">↩</a></li>
<li id="fn8">Danilo Menezes, Anisio Lacerda, Leila Silva, Adriano Veloso, and Nivio Ziviani. 2013. Weighted slope one predictors revisited. In Proceedings of the 22nd international conference on World Wide Web companion (WWW '13 Companion). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 967-972.<a href="#fnref8">↩</a></li>
<li id="fn9">Sun, Z., Luo, N., Kuang, W., One real-time personalized recommendation systems based on Slope One algorithm, FSKD 2011, 3, art. no. 6019830, 2012 pp. 1826-1830.<a href="#fnref9">↩</a></li>
<li id="fn10">Gao, M., Wu, Z., Personalized context-aware collaborative filtering based on neural network and slope one, LNCS 5738, 2009, pp. 109-116<a href="#fnref10">↩</a></li>
<li id="fn11">Slobodan Vucetic, Zoran Obradovic: Collaborative Filtering Using a Regression-Based Approach. Knowl. Inf. Syst. 7(1): 1-22 (2005)<a href="#fnref11">↩</a></li>
<li id="fn12">Badrul M. Sarwar, George Karypis, Joseph A. Konstan, John Riedl: Item-based collaborative filtering recommendation algorithms. WWW 2001: 285-295<a href="#fnref12">↩</a></li>
<li id="fn13"></li>
<li id="fn14"></li>
<li id="fn15">Greg Linden, Brent Smith, Jeremy York, "Amazon.com Recommendations: Item-to-Item Collaborative Filtering," IEEE Internet Computing, vol. 07, no. 1, pp. 76-80, Jan/Feb, 2003<a href="#fnref15">↩</a></li>
<li id="fn16"></li>
</ol>
</section>
</body>
</html>
