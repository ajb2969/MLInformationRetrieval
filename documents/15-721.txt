   Rectifier (neural networks)      Rectifier (neural networks)   In the context of artificial neural networks , the rectifier is an activation function defined as       f   (  x  )    =   max   (  0  ,  x  )          f  x     0  x     f(x)=\max(0,x)     where x is the input to a neuron. This activation function has been argued to be more biologically plausible 1 than the widely used logistic sigmoid (which is inspired by probability theory ; see logistic regression ) and its more practical 2 counterpart, the hyperbolic tangent . The rectifier is, , the most popular activation function for deep neural networks . 3  A unit employing the rectifier is also called a rectified linear unit ( ReLU ). 4  A smooth approximation to the rectifier is the analytic function       f   (  x  )    =   ln   (   1  +   e  x    )          f  x       1   superscript  e  x       f(x)=\ln(1+e^{x})     which is called the softplus function. 5 The derivative of softplus is      f  ′    (  x  )    =    e  x   /   (    e  x   +  1   )    =   1  /   (   1  +   e   -  x     )             superscript  f  normal-′   x      superscript  e  x      superscript  e  x   1           1    1   superscript  e    x         f^{\prime}(x)=e^{x}/(e^{x}+1)=1/(1+e^{-x})   , i.e. the logistic function.  Rectified linear units find applications in computer vision using deep neural nets . 6  Variants  Noisy ReLUs  Rectified linear units can be extended to include Gaussian noise , making them noisy ReLUs, giving 7       f   (  x  )    =   max   (  0  ,   x  +   𝒩   (  0  ,   σ   (  x  )    )     )          f  x     0    x    𝒩   0    σ  x         f(x)=\max(0,x+\mathcal{N}(0,\sigma(x)))     Noisy ReLUs have been used with some success in restricted Boltzmann machines for computer vision tasks. 8  Leaky ReLUs  Leaky ReLUs allow a small, non-zero gradient when the unit is not active. 9       f   (  x  )    =   {     x      if  x   >  0        0.01  x     otherwise            f  x    cases  x      if  x   0     0.01  x   otherwise     f(x)=\begin{cases}x&\mbox{if }x>0\\
 0.01x&\mbox{otherwise}\end{cases}     Advantages   Biological plausibility: One-sided, compared to the antisymmetry of tanh .  Sparse activation: For example, in a randomly initialized networks, only about 50% of hidden units are activated (having a non-zero output).  Efficient gradient propagation: No vanishing gradient problem or exploding effect.  Efficient computation: Only comparison, addition and multiplication.   For the first time in 2011, 10 the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units, compared to sigmoid function or similar activation functions, allow for faster and effective training of deep neural architectures on large and complex datasets.  Potential problems   Non-differentiable at zero: however it is differentiable at any point arbitrarily close to 0.   See also   Softmax function  Sigmoid function  Tobit model   References  "  Category:Artificial neural networks     ↩  ↩  ↩   C. Dugas, Y. Bengio, F. Bélisle, C. Nadeau, R. Garcia, NIPS'2000, (2001), Incorporating Second-Order Functional Knowledge for Better Option Pricing ↩   ↩   Andrew L. Maas, Awni Y. Hannun, Andrew Y. Ng (2014). Rectifier Nonlinearities Improve Neural Network Acoustic Models ↩      