   Quantile regression      Quantile regression   Quantile regression is a type of regression analysis used in statistics and econometrics. Whereas the method of least squares results in estimates that approximate the conditional mean of the response variable given certain values of the predictor variables, quantile regression aims at estimating either the conditional median or other quantiles of the response variable.  Advantages and applications  Quantile regression is desired if conditional quantile functions are of interest. One advantage of quantile regression, relative to the ordinary least squares regression, is that the quantile regression estimates are more robust against outliers in the response measurements. However, the main attraction of quantile regression goes beyond that. Different measures of central tendency and statistical dispersion can be useful to obtain a more comprehensive analysis of the relationship between variables. 1  In ecology , quantile regression has been proposed and used as a way to discover more useful predictive relationships between variables in cases where there is no relationship or only a weak relationship between the means of such variables. The need for and success of quantile regression in ecology has been attributed to the complexity of interactions between different factors leading to data with unequal variation of one variable for different ranges of another variable. 2  Another application of quantile regression is in the areas of growth charts, where percentile curves are commonly used to screen for abnormal growth. 3 4  Mathematics  The mathematical forms arising from quantile regression are distinct from those arising in the method of least squares . The method of least squares leads to a consideration of problems in an inner product space , involving projection onto subspaces, and thus the problem of minimizing the squared errors can be reduced to a problem in numerical linear algebra . Quantile regression does not have this structure, and instead leads to problems in linear programming that can be solved by the simplex method .  History  The idea of estimating a median regression slope, a major theorem about minimizing sum of the absolute deviances and a geometrical algorithm for constructing median regression was proposed in 1760 by Ruƒëer Josip Bo≈°koviƒá , a Jesuit Catholic priest from Dubrovnik. 5 6 Median regression computations for larger data sets are quite tedious compared to the least squares method, for which reason it has historically generated a lack of popularity among statisticians, until the widespread adoption of computers in the latter part of the 20th century.  Quantiles  Let   Y   Y   Y   be a real valued random variable with cumulative distribution function      F  Y    (  y  )   =  P   (  Y  ‚â§  y  )      fragments   subscript  F  Y    fragments  normal-(  y  normal-)    P   fragments  normal-(  Y   y  normal-)     F_{Y}(y)=P(Y\leq y)   . The   œÑ   œÑ   \tau   th quantile of Y is given by        Q  Y    (  œÑ  )    =    F  Y   -  1     (  œÑ  )    =   inf   {  y  :     F  Y    (  y  )    ‚â•  œÑ   }             subscript  Q  Y   œÑ      superscript   subscript  F  Y     1    œÑ         infimum   conditional-set  y       subscript  F  Y   y   œÑ        Q_{Y}(\tau)=F_{Y}^{-1}(\tau)=\inf\left\{y:F_{Y}(y)\geq\tau\right\}   where     œÑ  ‚àà   [  0  ,  1  ]    .      œÑ   0  1     \tau\in[0,1].     Define the loss function as      œÅ  œÑ    (  y  )    =   |   y   (   œÑ  -   ùïÄ   (  y  <  0  )     )    |          subscript  œÅ  œÑ   y       y    œÑ   subscript  ùïÄ   fragments  normal-(  y   0  normal-)         \rho_{\tau}(y)=|y(\tau-\mathbb{I}_{(y<0)})|   , where   ùïÄ   ùïÄ   \mathbb{I}   is an indicator variable . A specific quantile can be found by minimizing the expected loss of    Y  -  u      Y  u    Y-u   with respect to   u   u   u   : 7         min  ùë¢   E   (    œÅ  œÑ    (   Y  -  u   )    )    =     min  ùë¢    (   œÑ  -  1   )     ‚à´   -  ‚àû   u     (   y  -  u   )   d   F  Y    (  y  )      +   œÑ    ‚à´  u  ‚àû     (   y  -  u   )   d   F  Y    (  y  )        .         u    E     subscript  œÅ  œÑ     Y  u          u      œÑ  1     superscript   subscript        u       y  u   d   subscript  F  Y   y       œÑ    superscript   subscript   u         y  u   d   subscript  F  Y   y        \underset{u}{\min}E(\rho_{\tau}(Y-u))=\underset{u}{\min}(\tau-1)\int_{-\infty}%
 ^{u}(y-u)dF_{Y}(y)+\tau\int_{u}^{\infty}(y-u)dF_{Y}(y).     This can be shown by setting the derivative of the expected loss function to 0 and letting    q  œÑ     subscript  q  œÑ    q_{\tau}   be the solution of       0  =     (   1  -  œÑ   )     ‚à´   -  ‚àû    q  œÑ     d   F  Y    (  y  )      -   œÑ    ‚à´   q  œÑ   ‚àû    d   F  Y    (  y  )        .      0        1  œÑ     superscript   subscript         subscript  q  œÑ      d   subscript  F  Y   y       œÑ    superscript   subscript    subscript  q  œÑ        d   subscript  F  Y   y        0=(1-\tau)\int_{-\infty}^{q_{\tau}}dF_{Y}(y)-\tau\int_{q_{\tau}}^{\infty}dF_{Y%
 }(y).   This equation reduces to       0  =     F  Y    (   q  œÑ   )    -  œÑ    ,      0       subscript  F  Y    subscript  q  œÑ    œÑ     0=F_{Y}(q_{\tau})-\tau,   and then to         F  Y    (   q  œÑ   )    =  œÑ   .         subscript  F  Y    subscript  q  œÑ    œÑ    F_{Y}(q_{\tau})=\tau.     Hence    q  œÑ     subscript  q  œÑ    q_{\tau}   is   œÑ   œÑ   \tau   th quantile of the random variable Y.  Example  Let   Y   Y   Y   be a discrete random variable that takes values 1,2,..,9 with equal probabilities. The task is to find the median of Y, and hence the value    œÑ  =  0.5      œÑ  0.5    \tau=0.5   is chosen. The expected loss, , is      œÑ  =  0.5      œÑ  0.5    \tau=0.5   is a constant, it can be taken out of the expected loss function (this is only true if     L   (  3  )    ‚àù     ‚àë   i  =  1   2   -   (   i  -  3   )    +    ‚àë   i  =  3   9    (   i  -  3   )     =   [    (   2  +  1   )   +   (   0  +  1  +  2  +  ‚Ä¶  +  6   )    ]   =  24.       proportional-to    L  3        superscript   subscript     i  1    2     i  3      superscript   subscript     i  3    9     i  3           delimited-[]      2  1     0  1  2  normal-‚Ä¶  6          24.     L(3)\propto\sum_{i=1}^{2}-(i-3)+\sum_{i=3}^{9}(i-3)=[(2+1)+(0+1+2+...+6)]=24.   ). Then, at u =3,        (  3  )   -   (  6  )    =   -  3         3  6     3     (3)-(6)=-3     Suppose that u is increased by 1 unit. Then the expected loss will be changed by      L   (  5  )    ‚àù     ‚àë   i  =  1   4   i   +    ‚àë   i  =  0   4   i    =  20   ,       proportional-to    L  5       superscript   subscript     i  1    4   i     superscript   subscript     i  0    4   i         20     L(5)\propto\sum_{i=1}^{4}i+\sum_{i=0}^{4}i=20,   on changing u to 4. If, u =5, the expected loss is      0.5  /  9      0.5  9    {0.5/9}   and any change in u will increase the expected loss. Thus u =5 is the median. The Table below shows the expected loss (divided by    œÑ  =  0.5      œÑ  0.5    \tau=0.5   ) for different values of u .      u   1   2   3   4   5   6   7   8   9     Expected loss   36   29   24   21   20   21   24   29   36     Intuition  Consider    q  œÑ     subscript  q  œÑ    q_{\tau}   and let q be an initial guess for      -   0.5    ‚à´   -  ‚àû   q     (   y  -  q   )   d   F  Y    (  y  )       +   0.5    ‚à´  q  ‚àû     (   y  -  q   )   d   F  Y    (  y  )       .          0.5    superscript   subscript        q       y  q   d   subscript  F  Y   y        0.5    superscript   subscript   q         y  q   d   subscript  F  Y   y       -0.5\int_{-\infty}^{q}(y-q)dF_{Y}(y)+0.5\int_{q}^{\infty}(y-q)dF_{Y}(y).   . The expected loss evaluated at q is         ‚à´   -  ‚àû   q    1  d   F  Y    (  y  )     -    ‚à´  q  ‚àû    1  d   F  Y    (  y  )      .        superscript   subscript        q     1  d   subscript  F  Y   y      superscript   subscript   q       1  d   subscript  F  Y   y      \int_{-\infty}^{q}1dF_{Y}(y)-\int_{q}^{\infty}1dF_{Y}(y).   In order to minimize the expected loss, we move the value of q a little bit to see whether the expect loss will rise or fall. Suppose we increase q by 1 unit. Then the change of expected loss would be       F  Y    (  q  )        subscript  F  Y   q    F_{Y}(q)     The first term of the equation is    1  -    F  Y    (  q  )        1     subscript  F  Y   q     1-F_{Y}(q)   and second term of the equation is      F  Y    (  q  )    <  0.5         subscript  F  Y   q   0.5    F_{Y}(q)<0.5   . Therefore the change of expected loss function is negative if and only if   œÑ   œÑ   \tau   , that is if and only if q is smaller than the median. Similarly, if we reduce q by 1 unit, the change of expected loss function is negative if and only if q is larger than the median.  In order to minimize the expected loss function, we would increase (decrease) L ( q ) if q is smaller (larger) than the median, until q reaches the median. The idea behind the minimization is to count the number of points (weighted with the density) that are larger or smaller than q and then move q to a point where q is larger than   œÑ   œÑ   \tau   % of the points.  Sample quantile  The       q  ^   œÑ   =    arg min   q  ‚àà  R      ‚àë   i  =  1   n     œÅ  œÑ    (    y  i   -  q   )       ,       subscript   normal-^  q   œÑ        q  R   arg min     superscript   subscript     i  1    n      subscript  œÅ  œÑ      subscript  y  i   q        \hat{q}_{\tau}=\underset{q\in R}{\mbox{arg min}}\sum_{i=1}^{n}\rho_{\tau}(y_{i%
 }-q),   sample quantile can be obtained by solving the following minimization problem        Q   Y  |  X     (  œÑ  )    =   X   Œ≤  œÑ           subscript  Q   fragments  Y  normal-|  X    œÑ     X   subscript  Œ≤  œÑ      Q_{Y|X}(\tau)=X\beta_{\tau}        Y   Y   Y   th conditional quantile function is    Œ≤  œÑ     subscript  Œ≤  œÑ    \beta_{\tau}   . Given the distribution function of      Œ≤  œÑ   =    arg min   Œ≤  ‚àà   R  k     E   (    œÅ  œÑ    (   Y  -   X  Œ≤    )    )     .       subscript  Œ≤  œÑ        Œ≤   superscript  R  k    arg min   E     subscript  œÅ  œÑ     Y    X  Œ≤        \beta_{\tau}=\underset{\beta\in R^{k}}{\mbox{arg min}}E(\rho_{\tau}(Y-X\beta)).   ,   Œ≤   Œ≤   \beta   can be obtained by solving         Œ≤  œÑ   ^   =    arg min   Œ≤  ‚àà   R  k       ‚àë   i  =  1   n    (    œÅ  œÑ    (    Y  i   -   X  Œ≤    )    )      .       normal-^   subscript  Œ≤  œÑ         Œ≤   superscript  R  k    arg min     superscript   subscript     i  1    n      subscript  œÅ  œÑ      subscript  Y  i     X  Œ≤         \hat{\beta_{\tau}}=\underset{\beta\in R^{k}}{\mbox{arg min}}\sum_{i=1}^{n}(%
 \rho_{\tau}(Y_{i}-X\beta)).     Solving the sample analog gives the estimator of      min     Œ≤  +   ,   Œ≤  -   ,   u  +   ,   u  -    ‚àà    R   2  k    √ó   R  +   2  n        {    œÑ   1  n    ‚Ä≤     u  +    +    (   1  -  œÑ   )    1  n    ‚Ä≤     u  -     |      X   (    Œ≤  +   -   Œ≤  -    )    +   u  +    -   u  -    =  Y   }    ,           superscript  Œ≤     superscript  Œ≤     superscript  u     superscript  u        superscript  R    2  k     superscript   subscript  R      2  n         conditional-set      œÑ   superscript   subscript  1  n    normal-‚Ä≤     superscript  u         1  œÑ    superscript   subscript  1  n    normal-‚Ä≤     superscript  u              X     superscript  Œ≤     superscript  Œ≤       superscript  u      superscript  u     Y      \underset{\beta^{+},\beta^{-},u^{+},u^{-}\in R^{2k}\times R_{+}^{2n}}{\min}%
 \left\{\tau 1_{n}^{{}^{\prime}}u^{+}+(1-\tau)1_{n}^{{}^{\prime}}u^{-}|X(\beta^%
 {+}-\beta^{-})+u^{+}-u^{-}=Y\right\},   .       Œ≤  j  +   =   max   (   Œ≤  j   ,  0  )         superscript   subscript  Œ≤  j        subscript  Œ≤  j   0     \beta_{j}^{+}=\max(\beta_{j},0)     Computation  The minimization problem can be reformulated as a linear programming problem       Œ≤  j  -   =   -   min   (   Œ≤  j   ,  0  )          superscript   subscript  Œ≤  j          subscript  Œ≤  j   0      \beta_{j}^{-}=-\min(\beta_{j},0)     where       u  j  +   =   max   (   u  j   ,  0  )         superscript   subscript  u  j        subscript  u  j   0     u_{j}^{+}=\max(u_{j},0)   ,      u  j  -   =   -   min   (   u  j   ,  0  )      .       superscript   subscript  u  j          subscript  u  j   0      u_{j}^{-}=-\min(u_{j},0).   ,    œÑ  ‚àà   (  0  ,  1  )       œÑ   0  1     \tau\in(0,1)   ,     Œ≤  ^   œÑ     subscript   normal-^  Œ≤   œÑ    \hat{\beta}_{\tau}     Simplex methods 8 or interior point methods 9 can be applied to solve the linear programming problem.  Asymptotic properties  For      n    (     Œ≤  ^   œÑ   -   Œ≤  œÑ    )    ‚Üí  ùëë   N   (  0  ,   œÑ   (   1  -  œÑ   )    D   -  1     Œ©  x    D   -  1     )    ,        n      subscript   normal-^  Œ≤   œÑ    subscript  Œ≤  œÑ     d  normal-‚Üí   N   0    œÑ    1  œÑ    superscript  D    1     subscript  normal-Œ©  x    superscript  D    1        \sqrt{n}(\hat{\beta}_{\tau}-\beta_{\tau})\overset{d}{\rightarrow}N(0,\tau(1-%
 \tau)D^{-1}\Omega_{x}D^{-1}),   , under some regularity conditions,    D  =   E   (    f  Y    (   X  Œ≤   )   X   X  ‚Ä≤    )        D    E     subscript  f  Y     X  Œ≤   X   superscript  X  normal-‚Ä≤       D=E(f_{Y}(X\beta)XX^{\prime})   is asymptotically normal :        Œ©  x   =   E   (    X  ‚Ä≤   X   )     .       subscript  normal-Œ©  x     E     superscript  X  normal-‚Ä≤   X      \Omega_{x}=E(X^{\prime}X).   where      a  >  0      a  0    a>0   and    œÑ  ‚àà   [  0  ,  1  ]       œÑ   0  1     \tau\in[0,1]     Direct estimation of the asymptotic variance-covariance matrix is not always satisfactory. Inference for quantile regression parameters can be made with the regression rank-score tests or with the bootstrap methods. 10  Equivariance  See invariant estimator for background on invariance or see equivariance .  Scale equivariance  For any       Œ≤  ^    (  œÑ  ;   a  Y   ,  X  )    =   a   Œ≤  ^    (  œÑ  ;  Y  ,  X  )     ,         normal-^  Œ≤    œÑ    a  Y   X      a   normal-^  Œ≤    œÑ  Y  X      \hat{\beta}(\tau;aY,X)=a\hat{\beta}(\tau;Y,X),   and       Œ≤  ^    (  œÑ  ;   -   a  Y    ,  X  )    =   -   a   Œ≤  ^    (   1  -  œÑ   ;  Y  ,  X  )      .         normal-^  Œ≤    œÑ      a  Y    X        a   normal-^  Œ≤      1  œÑ   Y  X       \hat{\beta}(\tau;-aY,X)=-a\hat{\beta}(1-\tau;Y,X).         Œ≥  ‚àà   R  k       Œ≥   superscript  R  k     \gamma\in R^{k}         œÑ  ‚àà   [  0  ,  1  ]       œÑ   0  1     \tau\in[0,1]     Shift equivariance  For any       Œ≤  ^    (  œÑ  ;   Y  +   X  Œ≥    ,  X  )    =     Œ≤  ^    (  œÑ  ;  Y  ,  X  )    +  Œ≥    .         normal-^  Œ≤    œÑ    Y    X  Œ≥    X         normal-^  Œ≤    œÑ  Y  X    Œ≥     \hat{\beta}(\tau;Y+X\gamma,X)=\hat{\beta}(\tau;Y,X)+\gamma.   and   A   A   A         p  √ó  p      p  p    p\times p     Equivariance to reparameterization of design  Let    œÑ  ‚àà   [  0  ,  1  ]       œÑ   0  1     \tau\in[0,1]   be any       Œ≤  ^    (  œÑ  ;  Y  ,   X  A   )    =    A   -  1     Œ≤  ^    (  œÑ  ;  Y  ,  X  )     .         normal-^  Œ≤    œÑ  Y    X  A        superscript  A    1     normal-^  Œ≤    œÑ  Y  X      \hat{\beta}(\tau;Y,XA)=A^{-1}\hat{\beta}(\tau;Y,X).   nonsingular matrix and   h   h   h           h   (    Q   Y  |  X     (  œÑ  )    )    ‚â°    Q   h   (  Y  )   |  X     (  œÑ  )     .        h     subscript  Q   fragments  Y  normal-|  X    œÑ       subscript  Q   fragments  h   fragments  normal-(  Y  normal-)   normal-|  X    œÑ     h(Q_{Y|X}(\tau))\equiv Q_{h(Y)|X}(\tau).     Invariance to monotone transformations  If    Y  =   ln   (  W  )        Y    W     Y=\ln(W)   is a nondecreasing function on 'R , the following invariance property applies:        Q   Y  |  X     (  œÑ  )    =   X   Œ≤  œÑ           subscript  Q   fragments  Y  normal-|  X    œÑ     X   subscript  Œ≤  œÑ      Q_{Y|X}(\tau)=X\beta_{\tau}     Example (1):  Let      Q   W  |  X     (  œÑ  )    =   exp   (   X   Œ≤  œÑ    )           subscript  Q   fragments  W  normal-|  X    œÑ       X   subscript  Œ≤  œÑ       Q_{W|X}(\tau)=\exp(X\beta_{\tau})   and      E   (   ln   (  Y  )    )    ‚â†   ln   (   E   (  Y  )    )     .       normal-E    Y       normal-E  Y      \operatorname{E}(\ln(Y))\neq\ln(\operatorname{E}(Y)).   , then     Y  c   =   max   (  0  ,  Y  )         superscript  Y  c     0  Y     Y^{c}=\max(0,Y)   . The mean regression does not have the same property since     Q   Y  |  X    =   X   Œ≤  œÑ         subscript  Q   fragments  Y  normal-|  X      X   subscript  Œ≤  œÑ      Q_{Y|X}=X\beta_{\tau}     Bayesian methods for Quantile Regression  Because quantile regression does not normally assume a parametric likelihood for the conditional distributions of Y|X, the Bayesian methods work with a working likelihood. A convenient choice is the asymmetric Laplacian likelihood, 11 because the mode of the resulting posterior under a flat prior is the usual quantile regression estimates. The posterior inference, however, must be interpreted with care. Yang and He 12 showed that one can have asymptotically valid posterior inference if the working likelihood is chosen to be the empirical likelihood.  Censored Quantile Regression  If the response variable is subject to censoring, the conditional mean is not identifiable without additional distributional assumptions, but the conditional quantile is often identifiable. For recent work on censored quantile regression, see: Portnoy 13 and Wang and Wang 14  Example (2):  Let      Q    Y  c   |  X     (  œÑ  )    =   max   (  0  ,   X   Œ≤  œÑ    )           subscript  Q   fragments   superscript  Y  c   normal-|  X    œÑ     0    X   subscript  Œ≤  œÑ       Q_{Y^{c}|X}(\tau)=\max(0,X\beta_{\tau})   and $Q_{Y|X}=X\beta_{\tau}$ , then $Q_{Y^{c}|X}(\tau)=\max(0,X\beta_{\tau})$ . This is the censored quantile regression model: estimated values can be obtained without making any distributional assumptions, but at the cost of computational difficulty, 15 some of which can be avoided by using a simple three step censored quantile regression procedure as an approximation. 16  For random censoring on the response variables, the censored quantile regression of Portnoy (2003) provides consistent estimates of all identifiable quantile functions based on reweighting each censored point appropriately.  Implementations  Statistical software packages, such as R , Eviews (ver. 6), Stata (via qreg ), gretl , SAS through proc quantreg (ver. 9.2) and proc quantselect (ver. 9.3), Vowpal Wabbit (via --loss_function quantile ), and RATS include implementations of quantile regression. Several R packages implement quantile regression using different methods: quantreg package by Roger Koenker, gbm , and quantregForest .  References  Further reading     "  Category:Regression analysis     ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©   ‚Ü©     ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©     