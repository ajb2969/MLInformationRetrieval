


Hinge loss




Hinge loss

 In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for "maximum-margin" classification, most notably for support vector machines (SVMs).1 For an intended output 
 
 
 
  and a classifier score 
 
 
 
 , the hinge loss of the prediction 
 
 
 
  is defined as



Note that 
 
 
 
  should be the "raw" output of the classifier's decision function, not the predicted class label. E.g., in linear SVMs, 
 
 
 
 .
It can be seen that when 
 
 
 
  and 
 
 
 
  have the same sign (meaning 
 
 
 
  predicts the right class) and 
 
 
 
 , the hinge loss 
 
 
 
 , but when they have opposite sign, 
 
 
 
  increases linearly with 
 
 
 
  (one-sided error).
Extensions
While SVMs are commonly extended to multiclass classification in a one-vs.-all or one-vs.-one fashion,2 there exists a "true" multiclass version of the hinge loss due to Crammer and Singer,3 defined for a linear classifier as4



In structured prediction, the hinge loss can be further extended to structured output spaces. Structured SVMs with margin rescaling use the following variant, where 
 
 
 
  denotes the SVM's parameters, 
 
 
 
  the joint feature function, and 
 
 
 
  the Hamming loss:



Optimization
The hinge loss is a convex function, so many of the usual convex optimizers used in machine learning can work with it. It is not differentiable, but has a subgradient with respect to model parameters 
 
 
 
  of a linear SVM with score function 
 
 
 
  that is given by



 However, since the derivative of the hinge loss at 
 
 
 
  is non-deterministic, smoothed versions may be preferred for optimization, such as Rennie and Srebro's5



or the quadratically smoothed



suggested by Zhang.6 The modified Huber loss is a special case of this loss function with 
 
 
 
 .7
References
"
Category:Loss functions Category:Support vector machines



↩
↩
↩
↩
↩
↩





