   Hinge loss      Hinge loss   In machine learning , the hinge loss is a loss function used for training classifiers . The hinge loss is used for "maximum-margin" classification, most notably for support vector machines (SVMs). 1 For an intended output    t  =   ±  1       t    normal-±  1     t=±1   and a classifier score   y   y   y   , the hinge loss of the prediction   y   y   y   is defined as       ℓ   (  y  )    =   max   (  0  ,   1  -   t  ⋅  y    )          normal-ℓ  y     0    1   normal-⋅  t  y       \ell(y)=\max(0,1-t\cdot y)     Note that   y   y   y   should be the "raw" output of the classifier's decision function, not the predicted class label. E.g., in linear SVMs,    y  =    𝐰  ⋅  𝐱   +  b       y     normal-⋅  𝐰  𝐱   b     y=\mathbf{w}\cdot\mathbf{x}+b   .  It can be seen that when   t   t   t   and   y   y   y   have the same sign (meaning   y   y   y   predicts the right class) and     |  y  |   ≥  1        y   1    |y|\geq 1   , the hinge loss     ℓ   (  y  )    =  0        normal-ℓ  y   0    \ell(y)=0   , but when they have opposite sign,    ℓ   (  y  )       normal-ℓ  y    \ell(y)   increases linearly with   y   y   y   (one-sided error).  Extensions  While SVMs are commonly extended to multiclass classification in a one-vs.-all or one-vs.-one fashion, 2 there exists a "true" multiclass version of the hinge loss due to Crammer and Singer, 3 defined for a linear classifier as 4       ℓ   (  y  )    =   max   (  0  ,    1  +    max   t  ≠  y      𝐰  t   𝐱     -    𝐰  y   𝐱    )          normal-ℓ  y     0      1    subscript     t  y       subscript  𝐰  t   𝐱        subscript  𝐰  y   𝐱       \ell(y)=\max(0,1+\max_{t\neq y}\mathbf{w}_{t}\mathbf{x}-\mathbf{w}_{y}\mathbf{%
 x})     In structured prediction , the hinge loss can be further extended to structured output spaces. Structured SVMs with margin rescaling use the following variant, where   𝐲   𝐲   \mathbf{y}   denotes the SVM's parameters,   φ   φ   φ   the joint feature function, and   Δ   normal-Δ   Δ   the Hamming loss :      ℓ   (  𝐲  )       normal-ℓ  𝐲    \displaystyle\ell(\mathbf{y})     Optimization  The hinge loss is a convex function , so many of the usual convex optimizers used in machine learning can work with it. It is not differentiable , but has a subgradient with respect to model parameters   𝐰   𝐰   \mathbf{w}   of a linear SVM with score function    y  =   𝐰  ⋅  𝐱       y   normal-⋅  𝐰  𝐱     y=\mathbf{w}\cdot\mathbf{x}   that is given by        ∂  ℓ    ∂   w  i     =   {      -   t  ⋅   x  i          if  t   ⋅  y   <  1       0    otherwise              normal-ℓ      subscript  w  i      cases     normal-⋅  t   subscript  x  i        normal-⋅    if  t   y   1   0  otherwise     \frac{\partial\ell}{\partial w_{i}}=\begin{cases}-t\cdot x_{i}&\text{if }t%
 \cdot y<1\\
 0&\text{otherwise}\end{cases}     However, since the derivative of the hinge loss at     t  y   =  1        t  y   1    ty=1   is non-deterministic, smoothed versions may be preferred for optimization, such as Rennie and Srebro's 5       ℓ   (  y  )    =   {        1  2    -   t  y          if   t  y   ≤  0   ,          1  2      (   1  -   t  y    )   2          if   0   <   t  y   ≤  1   ,       0       if   1   ≤   t  y              normal-ℓ  y    cases      1  2     t  y        if  t  y   0       1  2    superscript    1    t  y    2          if  0     t  y        1    0      if  1     t  y       \ell(y)=\begin{cases}\frac{1}{2}-ty&\text{if}~{}~{}ty\leq 0,\\
 \frac{1}{2}(1-ty)^{2}&\text{if}~{}~{}0     or the quadratically smoothed      ℓ   (  y  )   =   1   2  γ    max    (  0  ,  1  -  t  y  )   2      fragments  ℓ   fragments  normal-(  y  normal-)      1    2  γ      superscript   fragments  normal-(  0  normal-,  1   t  y  normal-)   2     \ell(y)=\frac{1}{2\gamma}\max(0,1-ty)^{2}     suggested by Zhang. 6 The modified Huber loss is a special case of this loss function with    γ  =  2      γ  2    \gamma=2   . 7  References  "  Category:Loss functions  Category:Support vector machines     ↩  ↩  ↩  ↩  ↩  ↩      