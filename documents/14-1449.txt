   Hinge loss      Hinge loss   In machine learning , the hinge loss is a loss function used for training classifiers . The hinge loss is used for "maximum-margin" classification, most notably for support vector machines (SVMs). 1 For an intended output    t  =   Â±  1       t    normal-Â±  1     t=Â±1   and a classifier score   y   y   y   , the hinge loss of the prediction   y   y   y   is defined as       â„“   (  y  )    =   max   (  0  ,   1  -   t  â‹…  y    )          normal-â„“  y     0    1   normal-â‹…  t  y       \ell(y)=\max(0,1-t\cdot y)     Note that   y   y   y   should be the "raw" output of the classifier's decision function, not the predicted class label. E.g., in linear SVMs,    y  =    ğ°  â‹…  ğ±   +  b       y     normal-â‹…  ğ°  ğ±   b     y=\mathbf{w}\cdot\mathbf{x}+b   .  It can be seen that when   t   t   t   and   y   y   y   have the same sign (meaning   y   y   y   predicts the right class) and     |  y  |   â‰¥  1        y   1    |y|\geq 1   , the hinge loss     â„“   (  y  )    =  0        normal-â„“  y   0    \ell(y)=0   , but when they have opposite sign,    â„“   (  y  )       normal-â„“  y    \ell(y)   increases linearly with   y   y   y   (one-sided error).  Extensions  While SVMs are commonly extended to multiclass classification in a one-vs.-all or one-vs.-one fashion, 2 there exists a "true" multiclass version of the hinge loss due to Crammer and Singer, 3 defined for a linear classifier as 4       â„“   (  y  )    =   max   (  0  ,    1  +    max   t  â‰   y      ğ°  t   ğ±     -    ğ°  y   ğ±    )          normal-â„“  y     0      1    subscript     t  y       subscript  ğ°  t   ğ±        subscript  ğ°  y   ğ±       \ell(y)=\max(0,1+\max_{t\neq y}\mathbf{w}_{t}\mathbf{x}-\mathbf{w}_{y}\mathbf{%
 x})     In structured prediction , the hinge loss can be further extended to structured output spaces. Structured SVMs with margin rescaling use the following variant, where   ğ²   ğ²   \mathbf{y}   denotes the SVM's parameters,   Ï†   Ï†   Ï†   the joint feature function, and   Î”   normal-Î”   Î”   the Hamming loss :      â„“   (  ğ²  )       normal-â„“  ğ²    \displaystyle\ell(\mathbf{y})     Optimization  The hinge loss is a convex function , so many of the usual convex optimizers used in machine learning can work with it. It is not differentiable , but has a subgradient with respect to model parameters   ğ°   ğ°   \mathbf{w}   of a linear SVM with score function    y  =   ğ°  â‹…  ğ±       y   normal-â‹…  ğ°  ğ±     y=\mathbf{w}\cdot\mathbf{x}   that is given by        âˆ‚  â„“    âˆ‚   w  i     =   {      -   t  â‹…   x  i          if  t   â‹…  y   <  1       0    otherwise              normal-â„“      subscript  w  i      cases     normal-â‹…  t   subscript  x  i        normal-â‹…    if  t   y   1   0  otherwise     \frac{\partial\ell}{\partial w_{i}}=\begin{cases}-t\cdot x_{i}&\text{if }t%
 \cdot y<1\\
 0&\text{otherwise}\end{cases}     However, since the derivative of the hinge loss at     t  y   =  1        t  y   1    ty=1   is non-deterministic, smoothed versions may be preferred for optimization, such as Rennie and Srebro's 5       â„“   (  y  )    =   {        1  2    -   t  y          if   t  y   â‰¤  0   ,          1  2      (   1  -   t  y    )   2          if   0   <   t  y   â‰¤  1   ,       0       if   1   â‰¤   t  y              normal-â„“  y    cases      1  2     t  y        if  t  y   0       1  2    superscript    1    t  y    2          if  0     t  y        1    0      if  1     t  y       \ell(y)=\begin{cases}\frac{1}{2}-ty&\text{if}~{}~{}ty\leq 0,\\
 \frac{1}{2}(1-ty)^{2}&\text{if}~{}~{}0     or the quadratically smoothed      â„“   (  y  )   =   1   2  Î³    max    (  0  ,  1  -  t  y  )   2      fragments  â„“   fragments  normal-(  y  normal-)      1    2  Î³      superscript   fragments  normal-(  0  normal-,  1   t  y  normal-)   2     \ell(y)=\frac{1}{2\gamma}\max(0,1-ty)^{2}     suggested by Zhang. 6 The modified Huber loss is a special case of this loss function with    Î³  =  2      Î³  2    \gamma=2   . 7  References  "  Category:Loss functions  Category:Support vector machines     â†©  â†©  â†©  â†©  â†©  â†©      