<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1891">Speeded up robust features</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Speeded up robust features</h1>
<hr/>

<p>In <a href="computer_vision" title="wikilink">computer vision</a>, <strong>Speeded Up Robust Features (SURF)</strong> is a local <a href="feature_detection_(computer_vision)" title="wikilink">feature detector</a> and descriptor that can be used for tasks such as <a href="object_recognition" title="wikilink">object recognition</a> or registration or classification or <a href="3D_reconstruction" title="wikilink">3D reconstruction</a>. It is partly inspired by the <a href="scale-invariant_feature_transform" title="wikilink">scale-invariant feature transform</a> (SIFT) descriptor. The standard version of SURF is several times faster than SIFT and claimed by its authors to be more robust against different image transformations than SIFT.</p>

<p>To detect interest points, SURF uses an integer approximation of the <a href="blob_detection#The_determinant_of_the_Hessian" title="wikilink">determinant of Hessian</a> <a href="blob_detection" title="wikilink">blob detector</a>, which can be computed with 3 integer operations using a precomputed <a href="integral_image" title="wikilink">integral image</a>. Its feature descriptor is based on the sum of the <a href="Haar-like_features" title="wikilink">Haar wavelet</a> response around the point of interest. These can also be computed with the aid of the integral image.</p>

<p>SURF descriptors can be used to locate and recognize objects, people or faces, to make 3D scenes, to track objects and to extract points of interest.</p>

<p>SURF was first presented by <a href="Herbert_Bay" title="wikilink">Herbert Bay</a> et al. at the 2006 <a href="European_Conference_on_Computer_Vision" title="wikilink">European Conference on Computer Vision</a>. An application of the algorithm is patented in the United States.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="overview">Overview</h2>

<p>SURF is a detector and a descriptor for points of interest in images where the image is transformed into coordinates, using the multi-resolution pyramid technique. Is to make a copy of the original image with Pyramidal Gaussian or Laplacian Pyramid shape and obtain image with the same size but with reduced bandwidth. Thus a special blurring effect on the original image, called Scale-Space is achieved. This technique ensures that the points of interest are scale invariant.</p>
<h2 id="algorithm-and-features">Algorithm and features</h2>

<p>The SURF algorithm is based on the same principles and steps as SIFT but details in each step are different. The algorithm has three main parts: interest point detection, local neighborhood description and matching.</p>
<h3 id="interest-point-detection">Interest point detection</h3>

<p>The SIFT approach uses cascaded filters to detect scale-invariant characteristic points, where the difference of Gaussians (DoG) is calculated on rescaled images progressively. In SURF, square-shaped filters are used as an approximation of <a href="Gaussian_smoothing" title="wikilink">Gaussian smoothing</a>. <a href="Image_filtering" title="wikilink">Filtering</a> the image with a square is much faster if the <a href="integral_image" title="wikilink">integral image</a> is used, which is defined as:</p>

<p>

<math display="inline" id="Speeded_up_robust_features:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>S</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo>,</mo>
     <mi>y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>0</mn>
     </mrow>
     <mi>x</mi>
    </msubsup>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mi>j</mi>
       <mo>=</mo>
       <mn>0</mn>
      </mrow>
      <mi>y</mi>
     </msubsup>
     <mrow>
      <mi>I</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>i</mi>
       <mo>,</mo>
       <mi>j</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>S</ci>
     <interval closure="open">
      <ci>x</ci>
      <ci>y</ci>
     </interval>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">0</cn>
       </apply>
      </apply>
      <ci>x</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>j</ci>
         <cn type="integer">0</cn>
        </apply>
       </apply>
       <ci>y</ci>
      </apply>
      <apply>
       <times></times>
       <ci>I</ci>
       <interval closure="open">
        <ci>i</ci>
        <ci>j</ci>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S(x,y)=\sum_{i=0}^{x}\sum_{j=0}^{y}I(i,j)
  </annotation>
 </semantics>
</math>

</p>

<p>The sum of the original image within a rectangle can be evaluated quickly using the integral image, requiring four evaluations at the corners of the rectangle.</p>

<p>SURF uses a blob detector based on the <a href="Hessian_matrix" title="wikilink">Hessian matrix</a> to find points of interest. The <a class="uri" href="determinant" title="wikilink">determinant</a> of the Hessian matrix is used as a measure of local change around the point and points are chosen where this determinant is maximal. In contrast to the Hessian-Laplacian detector by Mikolajczyk and Schmid, SURF also uses the determinant of the Hessian for selecting the scale, as it is done by Lindeberg. Given a point p=(x, y) in an image I, the Hessian matrix H(p, σ) at point and scale σ, is defined as follows:</p>

<p>

<math display="inline" id="Speeded_up_robust_features:1">
 <semantics>
  <mrow>
   <mrow>
    <mi>H</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>p</mi>
     <mo>,</mo>
     <mi>σ</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>(</mo>
    <mtable>
     <mtr>
      <mtd columnalign="center">
       <mrow>
        <msub>
         <mi>L</mi>
         <mrow>
          <mi>x</mi>
          <mi>x</mi>
         </mrow>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>p</mi>
         <mo>,</mo>
         <mi>σ</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mtd>
      <mtd columnalign="center">
       <mrow>
        <msub>
         <mi>L</mi>
         <mrow>
          <mi>x</mi>
          <mi>y</mi>
         </mrow>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>p</mi>
         <mo>,</mo>
         <mi>σ</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="center">
       <mrow>
        <msub>
         <mi>L</mi>
         <mrow>
          <mi>x</mi>
          <mi>y</mi>
         </mrow>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>p</mi>
         <mo>,</mo>
         <mi>σ</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mtd>
      <mtd columnalign="center">
       <mrow>
        <msub>
         <mi>L</mi>
         <mrow>
          <mi>y</mi>
          <mi>y</mi>
         </mrow>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>p</mi>
         <mo>,</mo>
         <mi>σ</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mtd>
     </mtr>
    </mtable>
    <mo>)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>H</ci>
     <interval closure="open">
      <ci>p</ci>
      <ci>σ</ci>
     </interval>
    </apply>
    <matrix>
     <matrixrow>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>L</ci>
        <apply>
         <times></times>
         <ci>x</ci>
         <ci>x</ci>
        </apply>
       </apply>
       <interval closure="open">
        <ci>p</ci>
        <ci>σ</ci>
       </interval>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>L</ci>
        <apply>
         <times></times>
         <ci>x</ci>
         <ci>y</ci>
        </apply>
       </apply>
       <interval closure="open">
        <ci>p</ci>
        <ci>σ</ci>
       </interval>
      </apply>
     </matrixrow>
     <matrixrow>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>L</ci>
        <apply>
         <times></times>
         <ci>x</ci>
         <ci>y</ci>
        </apply>
       </apply>
       <interval closure="open">
        <ci>p</ci>
        <ci>σ</ci>
       </interval>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>L</ci>
        <apply>
         <times></times>
         <ci>y</ci>
         <ci>y</ci>
        </apply>
       </apply>
       <interval closure="open">
        <ci>p</ci>
        <ci>σ</ci>
       </interval>
      </apply>
     </matrixrow>
    </matrix>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(p,\sigma)=\begin{pmatrix}L_{xx}(p,\sigma)&L_{xy}(p,\sigma)\\
L_{xy}(p,\sigma)&L_{yy}(p,\sigma)\end{pmatrix}
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Speeded_up_robust_features:2">
 <semantics>
  <mrow>
   <msub>
    <mi>L</mi>
    <mrow>
     <mi>x</mi>
     <mi>x</mi>
    </mrow>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>p</mi>
    <mo>,</mo>
    <mi>σ</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>L</ci>
     <apply>
      <times></times>
      <ci>x</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <interval closure="open">
     <ci>p</ci>
     <ci>σ</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L_{xx}(p,\sigma)
  </annotation>
 </semantics>
</math>

 etc. are the second-order derivatives of the grayscale image.</p>

<p>The box filter of size 9×9 is an approximation of a Gaussian with σ=1.2 and represents the lowest level (highest spatial resolution) for blob-response maps.</p>
<h4 id="scale-space-representation-and-location-of-points-of-interest">Scale-space representation and location of points of interest</h4>

<p>The interest points can be found in different scales, partly because the search for correspondences often requires comparison images where they are seen at different scales. The scale space is usually realized as an image pyramid. Images are repeatedly smoothed with a Gaussian filter, then they are subsampled to get the next higher level of the pyramid. Therefore, several floors or stairs with various measures of the masks are calculated:</p>

<p>

<math display="inline" id="Speeded_up_robust_features:3">
 <semantics>
  <mrow>
   <msub>
    <mi>σ</mi>
    <mtext>approx</mtext>
   </msub>
   <mo>=</mo>
   <mrow>
    <mtext>Current filter size</mtext>
    <mo>*</mo>
    <mrow>
     <mo>(</mo>
     <mfrac>
      <mtext>Base Filter Scale</mtext>
      <mtext>Base Filter Size</mtext>
     </mfrac>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>σ</ci>
     <mtext>approx</mtext>
    </apply>
    <apply>
     <times></times>
     <mtext>Current filter size</mtext>
     <apply>
      <divide></divide>
      <mtext>Base Filter Scale</mtext>
      <mtext>Base Filter Size</mtext>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma_{\text{approx}}=\text{Current filter size}*\left(\frac{\text{Base %
Filter Scale}}{\text{Base Filter Size}}\right)
  </annotation>
 </semantics>
</math>

</p>

<p>The scale space is divided into a number of octaves, where an octave refers to a series of response maps of covering a doubling of scale. In SURF, the lowest level of the scale space is obtained from the output of the 9×9 filters.</p>

<p>Scale spaces are implemented by applying box filters of different sizes. Therefore, the scale space is analyzed by up-scaling the filter size rather than iteratively reducing the image size. The output of the above 9×9 filter is considered as the initial scale layer, to which we will refer as scale s=1.2 (corresponding to Gaussian derivatives with σ=1.2). The following layers are obtained by filtering the image with gradually bigger masks, taking into account the discrete nature of integral images and the specific structure of filters. Specifically, this results in filters of size 9×9, 15×15, 21×21, 27×27, etc. In order to localize interest points in the image and over scales, non-maximum suppression in a 3×3×3 neighborhood is applied. The maxima of the determinant of the Hessian matrix are then interpolated in scale and image space with the method proposed by Brown et al. Scale space interpolation is especially important in this case, as the difference in scale between the first layers of every octave is relatively large.</p>
<h3 id="local-neighborhood-descriptor">Local neighborhood descriptor</h3>

<p>The goal of a descriptor is to provide a unique and robust description of an image <a href="Feature_(computer_vision)" title="wikilink">feature</a>, e.g. by describing the intensity distribution of the pixels within the neighborhood of the point of interest. Most descriptors are computed thus in a local manner; hence, a description is obtained for every point of interest identified previously.</p>

<p>The dimensionality of the descriptor has direct impact on both its computational complexity and point-matching robustness/accuracy. A short descriptor may be more robust against appearance variations, but may not offer sufficient discrimination and thus give too many false positives.</p>

<p>The first step consists of fixing a reproducible orientation based on information from a circular region around the interest point. Then we construct a square region aligned to the selected orientation and extract the SURF descriptor from it.</p>

<p>Furthermore, there is also an upright version of SURF (called U-SURF) that is not invariant to image rotation and therefore faster to compute and better suited for application where the camera remains more or less horizontal.</p>
<h4 id="orientation-assignment">Orientation assignment</h4>

<p>In order to achieve rotational invariance, the orientation of the point of interest needs to be found. The Haar wavelet responses in both x- and y-directions within a circular neighbourhood of radius 

<math display="inline" id="Speeded_up_robust_features:4">
 <semantics>
  <mrow>
   <mn>6</mn>
   <mi>s</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <cn type="integer">6</cn>
    <ci>s</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   6s
  </annotation>
 </semantics>
</math>

 around the point of interest are computed, where 

<math display="inline" id="Speeded_up_robust_features:5">
 <semantics>
  <mi>s</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>s</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s
  </annotation>
 </semantics>
</math>

 is the scale at which the point of interest was detected. The obtained responses are weighed by a Gaussian function centered at the point of interest, then plotted as points in a two-dimensional space, with the horizontal response in the abscissa and the vertical response in the ordinate. The dominant orientation is estimated by calculating the sum of all responses within a sliding orientation window of size π/3. The horizontal and vertical responses within the window are summed. The two summed responses then yield a local orientation vector. The longest such vector overall defines the orientation of the point of interest. The size of the sliding window is a parameter which has to be chosen carefully to achieve a desired balance between robustness and angular resolution.</p>
<h4 id="descriptor-based-on-the-sum-of-haar-wavelet-responses">Descriptor based on the sum of Haar wavelet responses</h4>

<p>To describe the region around the point, a square region is extracted centred around the interest point and oriented along the orientation as selected in the previous section. The size of this window is 20s.</p>

<p>The interest region is split up into smaller 4x4 square sub-regions, and for each one, the Haar wavelet responses are extracted at 5x5 regularly spaced sample points. The responses are weighted with a Gaussian (to offer more robustness for deformations, noise and translation).</p>
<h3 id="matching">Matching</h3>

<p>By comparing the descriptors obtained from different images, matching pairs can be found.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Scale-invariant_feature_transform" title="wikilink">Scale-invariant feature transform</a> (SIFT)</li>
<li><a href="GLOH" title="wikilink">Gradient Location and Orientation Histogram</a></li>
<li><a href="LESH" title="wikilink"> LESH - Local Energy based Shape Histogram</a></li>
<li><a href="Blob_detection" title="wikilink">Blob detection</a></li>
<li><a href="Feature_detection_(computer_vision)" title="wikilink">Feature detection (computer vision)</a></li>
</ul>
<h2 id="references">References</h2>
<references>
</references>
<ul>
<li>Herbert Bay, Tinne Tuytelaars, and Luc Van Gool, "<a href="http://www.vision.ee.ethz.ch/~surf/eccv06.pdf">Speeded Up Robust Features</a>", ETH Zurich, Katholieke Universiteit Leuven</li>
<li>Andrea Maricela Plaza Cordero,Jorge Luis Zambrano Martínez, " <a href="http://41jaiio.sadio.org.ar/sites/default/files/6_EST_2012.pdf">Estudio y Selección de las Técnicas SIFT, SURF y ASIFT de Reconocimiento de Imágenes para el Diseño de un Prototipo en Dispositivos Móviles</a>" , 15º Concurso de Trabajos Estudiantiles, EST 2012</li>
<li>A. M. Romero and M. Cazorla, "<a href="http://rua.ua.es/dspace/bitstream/10045/23389/1/2009_Romero_Cazorla_Workshop_Agentes_Fisicos_Caceres.pdf">Comparativa de detectores de característicasvisuales y su aplicación al SLAM</a>", X Workshop de agentes físicos, Setiembre 2009, Cáceres</li>
<li>P. M. Panchal, S. R. Panchal, S. K. Shah, "<a href="http://www.ijircce.com/upload/2013/april/21_V1204057_A%20Comparison_H.pdf">A Comparison of SIFT and SURF</a> ", International Journal of Innovative Research in Computer and Communication Engineering Vol. 1, Issue 2, April 2013</li>
</ul>
<ul>
<li>Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool <a href="http://www.vision.ee.ethz.ch/~surf/papers.html">"SURF: Speeded Up Robust Features"</a>, Computer Vision and Image Understanding (CVIU), Vol. 110, No. 3, pp. 346–359, 2008</li>
<li><a href="http://www.chrisevansdev.com">Christopher Evans "Notes on the OpenSURF Library", MSc Computer Science, University of Bristol</a></li>
<li>Jan Knopp, Mukta Prasad, Gert Willems, Radu Timofte, and Luc Van Gool, "<a href="http://www.vision.ee.ethz.ch/~timofter/publications/Knopp-ECCV-2010.pdf">Hough Transform and 3D SURF for Robust Three Dimensional Classification</a>", European Conference on Computer Vision (ECCV), 2010</li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.vision.ee.ethz.ch/~surf">Website of SURF: Speeded Up Robust Features</a></li>
<li><a href="http://www.vision.ee.ethz.ch/~surf/eccv06.pdf">First publication of Speeded Up Robust Features (2006)</a></li>
<li><a href="http://glorfindel.mavrinac.com/~aaron/school/pdf/bay06_surf.pdf">Revised publication of SURF (2008)</a></li>
</ul>

<p>"</p>

<p><a href="Category:Feature_detection_(computer_vision)" title="wikilink">Category:Feature detection (computer vision)</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
</ol>
</section>
</body>
</html>
