<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1482">Distribution learning theory</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Distribution learning theory</h1>
<hr/>

<p>The <strong>distributional learning theory</strong> or <strong>learning of probability distribution</strong> is a framework in <a href="computational_learning_theory" title="wikilink">computational learning theory</a>. It has been proposed from <a href="Michael_Kearns_(computer_scientist)" title="wikilink">Michael Kearns</a>, <a href="Yishay_Mansour" title="wikilink">Yishay Mansour</a>, <a href="Dana_Ron" title="wikilink">Dana Ron</a>, <a href="Ronitt_Rubinfeld" title="wikilink">Ronitt Rubinfeld</a>, <a href="Robert_Schapire" title="wikilink">Robert Schapire</a> and <a href="Linda_Sellie" title="wikilink">Linda Sellie</a> in 1994 <a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> and it was inspired from the <a href="PAC-learning" title="wikilink">PAC-framework</a> introduced by <a href="Leslie_Valiant" title="wikilink">Leslie Valiant</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>In this framework the input is a number of samples drawn from a distribution that belongs to a specific class of distributions. The goal is to find an efficient algorithm that, based on these samples, determines with high probability the distribution from which the samples have been drawn. Because of its generality this framework it has been used in a large variety of different fields like <a href="machine_learning" title="wikilink">machine learning</a>, <a href="approximation_algorithms" title="wikilink">approximation algorithms</a>, <a href="applied_probability" title="wikilink">applied probability</a> and <a class="uri" href="statistics" title="wikilink">statistics</a>.</p>

<p>This article explains the basic definitions, tools and results in this framework from the theory of computation point of view.</p>
<h2 id="basic-definitions">Basic Definitions</h2>

<p>Let 

<math display="inline" id="Distribution_learning_theory:0">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle X
  </annotation>
 </semantics>
</math>

 be the support of the distributions that we are interested in. As in the original work of Kearns et. al.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> if 

<math display="inline" id="Distribution_learning_theory:1">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle X
  </annotation>
 </semantics>
</math>

 is finite it can be assumed without loss of generality that 

<math display="inline" id="Distribution_learning_theory:2">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">{</mo>
     <mn>0</mn>
     <mo>,</mo>
     <mn>1</mn>
     <mo stretchy="false">}</mo>
    </mrow>
    <mi>n</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>X</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <set>
      <cn type="integer">0</cn>
      <cn type="integer">1</cn>
     </set>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle X=\{0,1\}^{n}
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Distribution_learning_theory:3">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle n
  </annotation>
 </semantics>
</math>


 is the number of bits that have to be used in order to represent any 

<math display="inline" id="Distribution_learning_theory:4">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>∈</mo>
   <mi>X</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>y</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle y\in X
  </annotation>
 </semantics>
</math>

. We focus in probability distributions over 

<math display="inline" id="Distribution_learning_theory:5">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle X
  </annotation>
 </semantics>
</math>

.</p>

<p>There are two possible representations of a probability distribution 

<math display="inline" id="Distribution_learning_theory:6">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

 over 

<math display="inline" id="Distribution_learning_theory:7">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle X
  </annotation>
 </semantics>
</math>

.</p>
<ul>
<li><strong>probability distribution function (or evaluator)</strong> an evaluator 

<math display="inline" id="Distribution_learning_theory:8">
 <semantics>
  <msub>
   <mi>E</mi>
   <mi>D</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>E</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle E_{D}
  </annotation>
 </semantics>
</math>


 for 

<math display="inline" id="Distribution_learning_theory:9">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

 takes as input any 

<math display="inline" id="Distribution_learning_theory:10">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>∈</mo>
   <mi>X</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>y</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle y\in X
  </annotation>
 </semantics>
</math>

 and outputs a real number 

<math display="inline" id="Distribution_learning_theory:11">
 <semantics>
  <mrow>
   <msub>
    <mi>E</mi>
    <mi>D</mi>
   </msub>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>y</mi>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>E</ci>
     <ci>D</ci>
    </apply>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <ci>y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle E_{D}[y]
  </annotation>
 </semantics>
</math>

 which denotes the probability that of 

<math display="inline" id="Distribution_learning_theory:12">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle y
  </annotation>
 </semantics>
</math>

 according to 

<math display="inline" id="Distribution_learning_theory:13">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>


, i.e. 

<math display="inline" id="Distribution_learning_theory:14">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>E</mi>
     <mi>D</mi>
    </msub>
    <mrow>
     <mo stretchy="false">[</mo>
     <mi>y</mi>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>Pr</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mrow>
      <mi>Y</mi>
      <mo>=</mo>
      <mi>y</mi>
     </mrow>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>E</ci>
      <ci>D</ci>
     </apply>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <ci>y</ci>
     </apply>
    </apply>
    <apply>
     <ci>Pr</ci>
     <apply>
      <eq></eq>
      <ci>Y</ci>
      <ci>y</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle E_{D}[y]=\Pr[Y=y]
  </annotation>
 </semantics>
</math>

 if 

<math display="inline" id="Distribution_learning_theory:15">
 <semantics>
  <mrow>
   <mi>Y</mi>
   <mo>∼</mo>
   <mi>D</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <ci>Y</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle Y\sim D
  </annotation>
 </semantics>
</math>

.</li>
</ul>
<ul>
<li><strong>generator</strong> a generator 

<math display="inline" id="Distribution_learning_theory:16">
 <semantics>
  <msub>
   <mi>G</mi>
   <mi>D</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>G</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle G_{D}
  </annotation>
 </semantics>
</math>

 for 

<math display="inline" id="Distribution_learning_theory:17">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

 takes as input a string of truly random bits 

<math display="inline" id="Distribution_learning_theory:18">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle y
  </annotation>
 </semantics>
</math>


 and outputs 

<math display="inline" id="Distribution_learning_theory:19">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>G</mi>
     <mi>D</mi>
    </msub>
    <mrow>
     <mo stretchy="false">[</mo>
     <mi>y</mi>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>∈</mo>
   <mi>X</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>G</ci>
      <ci>D</ci>
     </apply>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <ci>y</ci>
     </apply>
    </apply>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle G_{D}[y]\in X
  </annotation>
 </semantics>
</math>

 according to the distribution 

<math display="inline" id="Distribution_learning_theory:20">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

. Generator can be interpreted as a routine that simulates sampling from the distribution 

<math display="inline" id="Distribution_learning_theory:21">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

 given a sequence of fair coin tosses.</li>
</ul>

<p>A distribution 

<math display="inline" id="Distribution_learning_theory:22">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

 is called to have a polynomial generator (respectively evaluator) if its generator (respectively evaluator) exists and can be computed in polynomial time.</p>

<p>Let 

<math display="inline" id="Distribution_learning_theory:23">
 <semantics>
  <msub>
   <mi>C</mi>
   <mi>X</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>C</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C_{X}
  </annotation>
 </semantics>
</math>


 a class of distribution over X, that is 

<math display="inline" id="Distribution_learning_theory:24">
 <semantics>
  <msub>
   <mi>C</mi>
   <mi>X</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>C</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C_{X}
  </annotation>
 </semantics>
</math>

 is a set such that every 

<math display="inline" id="Distribution_learning_theory:25">
 <semantics>
  <mrow>
   <mi>D</mi>
   <mo>∈</mo>
   <msub>
    <mi>C</mi>
    <mi>X</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>D</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>C</ci>
     <ci>X</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D\in C_{X}
  </annotation>
 </semantics>
</math>

 is a probability distribution with support 

<math display="inline" id="Distribution_learning_theory:26">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle X
  </annotation>
 </semantics>
</math>

. The 

<math display="inline" id="Distribution_learning_theory:27">
 <semantics>
  <msub>
   <mi>C</mi>
   <mi>X</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>C</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C_{X}
  </annotation>
 </semantics>
</math>

 can also be written as 

<math display="inline" id="Distribution_learning_theory:28">
 <semantics>
  <mi>C</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>C</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C
  </annotation>
 </semantics>
</math>


 for simplicity.</p>

<p>Before defining learnability its necessary to define good approximations of a distribution 

<math display="inline" id="Distribution_learning_theory:29">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

. There are three ways to measure the distance between two distribution. The three more common possibilities are</p>
<ul>
<li><a href="Kullback-Leibler_divergence" title="wikilink">Kullback-Leibler divergence</a></li>
</ul>
<ul>
<li><a href="Total_variation" title="wikilink">Total variation</a> distance</li>
</ul>
<ul>
<li><a href="Kolmogorov–Smirnov_test" title="wikilink">Kolmogorov distance</a></li>
</ul>

<p>The strongest of these distances is the <a href="Kullback-Leibler_divergence" title="wikilink">Kullback-Leibler divergence</a> and the weakest is the <a href="Kolmogorov–Smirnov_test" title="wikilink">Kolmogorov distance</a>. This means that for any pair of distributions 

<math display="inline" id="Distribution_learning_theory:30">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Distribution_learning_theory:31">
 <semantics>
  <msup>
   <mi>D</mi>
   <mo>′</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>D</ci>
    <ci>normal-′</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D^{\prime}
  </annotation>
 </semantics>
</math>

 :</p>

<p>

<math display="block" id="Distribution_learning_theory:32">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>K</mi>
     <mi>L</mi>
    </mrow>
    <mo>-</mo>
    <mrow>
     <mi>d</mi>
     <mi>i</mi>
     <mi>s</mi>
     <mi>t</mi>
     <mi>a</mi>
     <mi>n</mi>
     <mi>c</mi>
     <mi>e</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>D</mi>
      <mo>,</mo>
      <msup>
       <mi>D</mi>
       <mo>′</mo>
      </msup>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>≥</mo>
   <mrow>
    <mrow>
     <mi>T</mi>
     <mi>V</mi>
    </mrow>
    <mo>-</mo>
    <mrow>
     <mi>d</mi>
     <mi>i</mi>
     <mi>s</mi>
     <mi>t</mi>
     <mi>a</mi>
     <mi>n</mi>
     <mi>c</mi>
     <mi>e</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>D</mi>
      <mo>,</mo>
      <msup>
       <mi>D</mi>
       <mo>′</mo>
      </msup>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>≥</mo>
   <mrow>
    <mrow>
     <mi>K</mi>
     <mi>o</mi>
     <mi>l</mi>
     <mi>m</mi>
     <mi>o</mi>
     <mi>g</mi>
     <mi>o</mi>
     <mi>r</mi>
     <mi>o</mi>
     <mi>v</mi>
    </mrow>
    <mo>-</mo>
    <mrow>
     <mi>d</mi>
     <mi>i</mi>
     <mi>s</mi>
     <mi>t</mi>
     <mi>a</mi>
     <mi>n</mi>
     <mi>c</mi>
     <mi>e</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>D</mi>
      <mo>,</mo>
      <msup>
       <mi>D</mi>
       <mo>′</mo>
      </msup>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <geq></geq>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>K</ci>
       <ci>L</ci>
      </apply>
      <apply>
       <times></times>
       <ci>d</ci>
       <ci>i</ci>
       <ci>s</ci>
       <ci>t</ci>
       <ci>a</ci>
       <ci>n</ci>
       <ci>c</ci>
       <ci>e</ci>
       <interval closure="open">
        <ci>D</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>D</ci>
         <ci>normal-′</ci>
        </apply>
       </interval>
      </apply>
     </apply>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>T</ci>
       <ci>V</ci>
      </apply>
      <apply>
       <times></times>
       <ci>d</ci>
       <ci>i</ci>
       <ci>s</ci>
       <ci>t</ci>
       <ci>a</ci>
       <ci>n</ci>
       <ci>c</ci>
       <ci>e</ci>
       <interval closure="open">
        <ci>D</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>D</ci>
         <ci>normal-′</ci>
        </apply>
       </interval>
      </apply>
     </apply>
    </apply>
    <apply>
     <geq></geq>
     <share href="#.cmml">
     </share>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>K</ci>
       <ci>o</ci>
       <ci>l</ci>
       <ci>m</ci>
       <ci>o</ci>
       <ci>g</ci>
       <ci>o</ci>
       <ci>r</ci>
       <ci>o</ci>
       <ci>v</ci>
      </apply>
      <apply>
       <times></times>
       <ci>d</ci>
       <ci>i</ci>
       <ci>s</ci>
       <ci>t</ci>
       <ci>a</ci>
       <ci>n</ci>
       <ci>c</ci>
       <ci>e</ci>
       <interval closure="open">
        <ci>D</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>D</ci>
         <ci>normal-′</ci>
        </apply>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   KL-distance(D,D^{\prime})\geq TV-distance(D,D^{\prime})\geq Kolmogorov-%
distance(D,D^{\prime})
  </annotation>
 </semantics>
</math>

</p>

<p>Therefore for example if 

<math display="inline" id="Distribution_learning_theory:33">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>


 and 

<math display="inline" id="Distribution_learning_theory:34">
 <semantics>
  <msup>
   <mi>D</mi>
   <mo>′</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>D</ci>
    <ci>normal-′</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D^{\prime}
  </annotation>
 </semantics>
</math>

 are close with respect to <a href="Kullback-Leibler_divergence" title="wikilink">Kullback-Leibler divergence</a> then they are also close with respect to all the other distances.</p>

<p>Next definitions hold for all the distances and therefore the symbol 

<math display="inline" id="Distribution_learning_theory:35">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo>,</mo>
    <msup>
     <mi>D</mi>
     <mo>′</mo>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>d</ci>
    <interval closure="open">
     <ci>D</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>D</ci>
      <ci>normal-′</ci>
     </apply>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle d(D,D^{\prime})
  </annotation>
 </semantics>
</math>

 denotes the distance between the distribution 

<math display="inline" id="Distribution_learning_theory:36">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

 and the distribution 

<math display="inline" id="Distribution_learning_theory:37">
 <semantics>
  <msup>
   <mi>D</mi>
   <mo>′</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>D</ci>
    <ci>normal-′</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D^{\prime}
  </annotation>
 </semantics>
</math>

 using one of the distances that we describe above. Although learnability of a class of distributions can be defined using any of these distances, applications refer to a specific distance.</p>

<p>The basic input that we use in order to learn a distribution is an number of samples drawn by this distribution. For the computational point of view the assumption is that such a sample is given in a constant amount of time. So it's like having access to an oracle 

<math display="inline" id="Distribution_learning_theory:38">
 <semantics>
  <mrow>
   <mi>G</mi>
   <mi>E</mi>
   <mi>N</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>G</ci>
    <ci>E</ci>
    <ci>N</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle GEN(D)
  </annotation>
 </semantics>
</math>


 that returns a sample from the distribution 

<math display="inline" id="Distribution_learning_theory:39">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

. Sometimes the interest is, apart from measuring the time complexity, to measure the number of samples that have to be used in order to learn a specific distribution 

<math display="inline" id="Distribution_learning_theory:40">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

 in class of distributions 

<math display="inline" id="Distribution_learning_theory:41">
 <semantics>
  <mi>C</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>C</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C
  </annotation>
 </semantics>
</math>

. This quantity is called <strong>sample complexity</strong> of the learning algorithm.</p>

<p>In order for the problem of distribution learning to be more clear consider the problem of supervised learning as defined in.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> In this framework of <a href="statistical_learning_theory" title="wikilink">statistical learning theory</a> a training set 

<math display="inline" id="Distribution_learning_theory:42">
 <semantics>
  <mrow>
   <mi>S</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>x</mi>
      <mn>1</mn>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>y</mi>
      <mn>1</mn>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>x</mi>
      <mi>n</mi>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>y</mi>
      <mi>n</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>S</ci>
    <set>
     <interval closure="open">
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <cn type="integer">1</cn>
      </apply>
     </interval>
     <ci>normal-…</ci>
     <interval closure="open">
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>n</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>n</ci>
      </apply>
     </interval>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle S=\{(x_{1},y_{1}),\dots,(x_{n},y_{n})\}
  </annotation>
 </semantics>
</math>

 and the goal is to find a target function 

<math display="inline" id="Distribution_learning_theory:43">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>:</mo>
   <mrow>
    <mi>X</mi>
    <mo>→</mo>
    <mi>Y</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>f</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>X</ci>
     <ci>Y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle f:X\rightarrow Y
  </annotation>
 </semantics>
</math>


 that minimizes some loss function, e.g. the square loss function. More formally 

<math display="inline" id="Distribution_learning_theory:44">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi>arg</mi>
     <msub>
      <mi>min</mi>
      <mi>g</mi>
     </msub>
    </mrow>
    <mrow>
     <mo largeop="true" symmetric="true">∫</mo>
     <mrow>
      <mi>V</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>y</mi>
       <mo>,</mo>
       <mrow>
        <mi>g</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mi>d</mi>
      <mi>ρ</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo>,</mo>
       <mi>y</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>f</ci>
    <apply>
     <times></times>
     <apply>
      <arg></arg>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <min></min>
       <ci>g</ci>
      </apply>
     </apply>
     <apply>
      <int></int>
      <apply>
       <times></times>
       <ci>V</ci>
       <interval closure="open">
        <ci>y</ci>
        <apply>
         <times></times>
         <ci>g</ci>
         <ci>x</ci>
        </apply>
       </interval>
       <ci>d</ci>
       <ci>ρ</ci>
       <interval closure="open">
        <ci>x</ci>
        <ci>y</ci>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f=\arg\min_{g}\int V(y,g(x))d\rho(x,y)
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Distribution_learning_theory:45">
 <semantics>
  <mrow>
   <mi>V</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mo>⋅</mo>
    <mo>,</mo>
    <mo>⋅</mo>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>V</ci>
    <interval closure="open">
     <ci>normal-⋅</ci>
     <ci>normal-⋅</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(\cdot,\cdot)
  </annotation>
 </semantics>
</math>

 is the loss function, e.g. 

<math display="inline" id="Distribution_learning_theory:46">
 <semantics>
  <mrow>
   <mrow>
    <mi>V</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>y</mi>
     <mo>,</mo>
     <mi>z</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>y</mi>
      <mo>-</mo>
      <mi>z</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>V</ci>
     <interval closure="open">
      <ci>y</ci>
      <ci>z</ci>
     </interval>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <minus></minus>
      <ci>y</ci>
      <ci>z</ci>
     </apply>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(y,z)=(y-z)^{2}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Distribution_learning_theory:47">
 <semantics>
  <mrow>
   <mi>ρ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>ρ</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \rho(x,y)
  </annotation>
 </semantics>
</math>

 the probability distribution according to which the elements of the training set are sampled. If the <a href="conditional_probability_distribution" title="wikilink">conditional probability distribution</a> 

<math display="inline" id="Distribution_learning_theory:48">
 <semantics>
  <mrow>
   <msub>
    <mi>ρ</mi>
    <mi>x</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>ρ</ci>
     <ci>x</ci>
    </apply>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \rho_{x}(y)
  </annotation>
 </semantics>
</math>


 is known then the target function has the closed form 

<math display="inline" id="Distribution_learning_theory:49">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mo largeop="true" symmetric="true">∫</mo>
     <mi>y</mi>
    </msub>
    <mrow>
     <mi>y</mi>
     <mi>d</mi>
     <msub>
      <mi>ρ</mi>
      <mi>x</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>y</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <int></int>
      <ci>y</ci>
     </apply>
     <apply>
      <times></times>
      <ci>y</ci>
      <ci>d</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ρ</ci>
       <ci>x</ci>
      </apply>
      <ci>y</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)=\int_{y}yd\rho_{x}(y)
  </annotation>
 </semantics>
</math>

. So the set 

<math display="inline" id="Distribution_learning_theory:50">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

 is a set of samples from the <a href="probability_distribution" title="wikilink">probability distribution</a> 

<math display="inline" id="Distribution_learning_theory:51">
 <semantics>
  <mrow>
   <mi>ρ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>ρ</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \rho(x,y)
  </annotation>
 </semantics>
</math>

. Now the goal of distributional learning theory if to find 

<math display="inline" id="Distribution_learning_theory:52">
 <semantics>
  <mi>ρ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ρ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \rho
  </annotation>
 </semantics>
</math>

 given 

<math display="inline" id="Distribution_learning_theory:53">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>


 which can be used to find the target function 

<math display="inline" id="Distribution_learning_theory:54">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

.</p>

<p><strong>Definition of learnability</strong></p>

<p><em>A class of distributions 

<math display="inline" id="Distribution_learning_theory:55">
 <semantics>
  <mi>C</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>C</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C
  </annotation>
 </semantics>
</math>

 is called <strong>efficiently learnable</strong> if for every 

<math display="inline" id="Distribution_learning_theory:56">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>ϵ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon>0
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Distribution_learning_theory:57">
 <semantics>
  <mrow>
   <mn>0</mn>
   <mo><</mo>
   <mi>δ</mi>
   <mo>≤</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <lt></lt>
     <cn type="integer">0</cn>
     <ci>δ</ci>
    </apply>
    <apply>
     <leq></leq>
     <share href="#.cmml">
     </share>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle 0<\delta\leq 1
  </annotation>
 </semantics>
</math>

 given access to 

<math display="inline" id="Distribution_learning_theory:58">
 <semantics>
  <mrow>
   <mi>G</mi>
   <mi>E</mi>
   <mi>N</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>G</ci>
    <ci>E</ci>
    <ci>N</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle GEN(D)
  </annotation>
 </semantics>
</math>


 for an unknown distribution 

<math display="inline" id="Distribution_learning_theory:59">
 <semantics>
  <mrow>
   <mi>D</mi>
   <mo>∈</mo>
   <mi>C</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>D</ci>
    <ci>C</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D\in C
  </annotation>
 </semantics>
</math>

, there exists a polynomial time algorithm 

<math display="inline" id="Distribution_learning_theory:60">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle A
  </annotation>
 </semantics>
</math>

, called learning algorithm of 

<math display="inline" id="Distribution_learning_theory:61">
 <semantics>
  <mi>C</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>C</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C
  </annotation>
 </semantics>
</math>

, that outputs an generator or an evaluator of a distribution 

<math display="inline" id="Distribution_learning_theory:62">
 <semantics>
  <msup>
   <mi>D</mi>
   <mo>′</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>D</ci>
    <ci>normal-′</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D^{\prime}
  </annotation>
 </semantics>
</math>

 such that</em></p>

<p>

<math display="block" id="Distribution_learning_theory:63">
 <semantics>
  <mrow>
   <mrow>
    <mi>Pr</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mrow>
      <mrow>
       <mi>d</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>D</mi>
        <mo>,</mo>
        <msup>
         <mi>D</mi>
         <mo>′</mo>
        </msup>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>≤</mo>
      <mi>ϵ</mi>
     </mrow>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>≥</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <mi>δ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <geq></geq>
    <apply>
     <ci>Pr</ci>
     <apply>
      <leq></leq>
      <apply>
       <times></times>
       <ci>d</ci>
       <interval closure="open">
        <ci>D</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>D</ci>
         <ci>normal-′</ci>
        </apply>
       </interval>
      </apply>
      <ci>ϵ</ci>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <ci>δ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Pr[d(D,D^{\prime})\leq\epsilon]\geq 1-\delta
  </annotation>
 </semantics>
</math>

</p>

<p><em>If we know that 

<math display="inline" id="Distribution_learning_theory:64">
 <semantics>
  <mrow>
   <msup>
    <mi>D</mi>
    <mo>′</mo>
   </msup>
   <mo>∈</mo>
   <mi>C</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>D</ci>
     <ci>normal-′</ci>
    </apply>
    <ci>C</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D^{\prime}\in C
  </annotation>
 </semantics>
</math>

 then 

<math display="inline" id="Distribution_learning_theory:65">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle A
  </annotation>
 </semantics>
</math>

 is called <strong>proper learning algorithm</strong>, otherwise is called <strong>improper learning algorithm</strong>.</em></p>

<p>In some settings the class of distributions 

<math display="inline" id="Distribution_learning_theory:66">
 <semantics>
  <mi>C</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>C</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C
  </annotation>
 </semantics>
</math>

 is a class with well known distributions which can be described by set a set of parameters. For instance 

<math display="inline" id="Distribution_learning_theory:67">
 <semantics>
  <mi>C</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>C</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C
  </annotation>
 </semantics>
</math>

 could be the class of all the Gaussian distributions 

<math display="inline" id="Distribution_learning_theory:68">
 <semantics>
  <mrow>
   <mi>N</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>μ</mi>
    <mo>,</mo>
    <msup>
     <mi>σ</mi>
     <mn>2</mn>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>N</ci>
    <interval closure="open">
     <ci>μ</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>σ</ci>
      <cn type="integer">2</cn>
     </apply>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle N(\mu,\sigma^{2})
  </annotation>
 </semantics>
</math>


. In this case the algorithm 

<math display="inline" id="Distribution_learning_theory:69">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle A
  </annotation>
 </semantics>
</math>

 should be able to estimate the parameters 

<math display="inline" id="Distribution_learning_theory:70">
 <semantics>
  <mrow>
   <mi>μ</mi>
   <mo>,</mo>
   <mi>σ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <ci>μ</ci>
    <ci>σ</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\mu,\sigma
  </annotation>
 </semantics>
</math>

. In this case 

<math display="inline" id="Distribution_learning_theory:71">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle A
  </annotation>
 </semantics>
</math>

 is called <strong>parameter learning algorithm</strong>.</p>

<p>Obviously the parameter learning for simple distributions is a very well studied field that is called statistical estimation and there is a very long bibliography on different estimators for different kinds of simple known distributions. But distributions learning theory deals with learning class of distributions that have more complicated description.</p>
<h2 id="first-results">First results <a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></h2>

<p>In their seminal work, Kearns et. al.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> deal with the case where 

<math display="inline" id="Distribution_learning_theory:72">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle A
  </annotation>
 </semantics>
</math>

 is described in term of a finite polynomial sized circuit and they proved the following for some specific classes of distribution</p>
<ul>
<li><strong>

<math display="inline" id="Distribution_learning_theory:73">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mi>R</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <ci>R</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle OR
  </annotation>
 </semantics>
</math>


 gate distributions</strong> for this kind of distributions there is no polynomial-sized evaluator, unless 

<math display="inline" id="Distribution_learning_theory:74">
 <semantics>
  <mrow>
   <mrow>
    <mi mathvariant="normal">#</mi>
    <mi>P</mi>
   </mrow>
   <mo>⊆</mo>
   <mrow>
    <mi>P</mi>
    <mo>/</mo>
    <mtext>poly</mtext>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <subset></subset>
    <apply>
     <times></times>
     <ci>normal-#</ci>
     <ci>P</ci>
    </apply>
    <apply>
     <divide></divide>
     <ci>P</ci>
     <mtext>poly</mtext>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\#P\subseteq P/\text{poly}
  </annotation>
 </semantics>
</math>

. On the other hand this class is efficiently learnable with generator.</li>
</ul>
<ul>
<li><strong>Parity gate distributions</strong> this class is efficiently learnable with both generator and evaluator.</li>
</ul>
<ul>
<li><strong>Mixtures of Hamming Balls</strong> this class is efficiently learnable with both generator and evaluator.</li>
</ul>
<ul>
<li><strong>Probabilistic Finite Automata</strong> this class is not efficiently learnable with evaluator under the Noisy Parity Assumption which is an impossibility assumption in the PAC learning framework.</li>
</ul>
<h2 id="textstyle-epsilon-covers">

<math display="inline" id="Distribution_learning_theory:75">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mo>-</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">limit-from</csymbol>
    <ci>ϵ</ci>
    <minus></minus>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon-
  </annotation>
 </semantics>
</math>

Covers</h2>

<p>One very common technique in order to find a learning algorithm for a class of distributions 

<math display="inline" id="Distribution_learning_theory:76">
 <semantics>
  <mi>C</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>C</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C
  </annotation>
 </semantics>
</math>

 is to first find a small 

<math display="inline" id="Distribution_learning_theory:77">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mo>-</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">limit-from</csymbol>
    <ci>ϵ</ci>
    <minus></minus>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon-
  </annotation>
 </semantics>
</math>

cover of 

<math display="inline" id="Distribution_learning_theory:78">
 <semantics>
  <mi>C</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>C</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C
  </annotation>
 </semantics>
</math>


.</p>

<p><strong>Definition</strong></p>

<p><em>A set 

<math display="inline" id="Distribution_learning_theory:79">
 <semantics>
  <msub>
   <mi>C</mi>
   <mi>ϵ</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>C</ci>
    <ci>ϵ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C_{\epsilon}
  </annotation>
 </semantics>
</math>

 is called 

<math display="inline" id="Distribution_learning_theory:80">
 <semantics>
  <mi>ϵ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ϵ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon
  </annotation>
 </semantics>
</math>

-cover of 

<math display="inline" id="Distribution_learning_theory:81">
 <semantics>
  <mi>C</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>C</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C
  </annotation>
 </semantics>
</math>

 if for every 

<math display="inline" id="Distribution_learning_theory:82">
 <semantics>
  <mrow>
   <mi>D</mi>
   <mo>∈</mo>
   <mi>C</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>D</ci>
    <ci>C</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D\in C
  </annotation>
 </semantics>
</math>

 there is a 

<math display="inline" id="Distribution_learning_theory:83">
 <semantics>
  <mrow>
   <msup>
    <mi>D</mi>
    <mo>′</mo>
   </msup>
   <mo>∈</mo>
   <msub>
    <mi>C</mi>
    <mi>ϵ</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>D</ci>
     <ci>normal-′</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>C</ci>
     <ci>ϵ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D^{\prime}\in C_{\epsilon}
  </annotation>
 </semantics>
</math>


 such that 

<math display="inline" id="Distribution_learning_theory:84">
 <semantics>
  <mrow>
   <mrow>
    <mi>d</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>D</mi>
     <mo>,</mo>
     <msup>
      <mi>D</mi>
      <mo>′</mo>
     </msup>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>≤</mo>
   <mi>ϵ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <apply>
     <times></times>
     <ci>d</ci>
     <interval closure="open">
      <ci>D</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>D</ci>
       <ci>normal-′</ci>
      </apply>
     </interval>
    </apply>
    <ci>ϵ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle d(D,D^{\prime})\leq\epsilon
  </annotation>
 </semantics>
</math>

. An 

<math display="inline" id="Distribution_learning_theory:85">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mo>-</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">limit-from</csymbol>
    <ci>ϵ</ci>
    <minus></minus>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon-
  </annotation>
 </semantics>
</math>

 cover is small if it has polynomial size with respect to the parameters that describe 

<math display="inline" id="Distribution_learning_theory:86">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

.</em></p>

<p>Once there is an efficient procedure that for every 

<math display="inline" id="Distribution_learning_theory:87">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>ϵ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon>0
  </annotation>
 </semantics>
</math>

 finds a small 

<math display="inline" id="Distribution_learning_theory:88">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mo>-</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">limit-from</csymbol>
    <ci>ϵ</ci>
    <minus></minus>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon-
  </annotation>
 </semantics>
</math>


cover 

<math display="inline" id="Distribution_learning_theory:89">
 <semantics>
  <msub>
   <mi>C</mi>
   <mi>ϵ</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>C</ci>
    <ci>ϵ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C_{\epsilon}
  </annotation>
 </semantics>
</math>

 of C then the only left task is to select from 

<math display="inline" id="Distribution_learning_theory:90">
 <semantics>
  <msub>
   <mi>C</mi>
   <mi>ϵ</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>C</ci>
    <ci>ϵ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C_{\epsilon}
  </annotation>
 </semantics>
</math>

 the distribution 

<math display="inline" id="Distribution_learning_theory:91">
 <semantics>
  <mrow>
   <msup>
    <mi>D</mi>
    <mo>′</mo>
   </msup>
   <mo>∈</mo>
   <msub>
    <mi>C</mi>
    <mi>ϵ</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>D</ci>
     <ci>normal-′</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>C</ci>
     <ci>ϵ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D^{\prime}\in C_{\epsilon}
  </annotation>
 </semantics>
</math>

 that is closer to the distribution 

<math display="inline" id="Distribution_learning_theory:92">
 <semantics>
  <mrow>
   <mi>D</mi>
   <mo>∈</mo>
   <mi>C</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>D</ci>
    <ci>C</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D\in C
  </annotation>
 </semantics>
</math>

 that has to be learned.</p>

<p>The problem is that given 

<math display="inline" id="Distribution_learning_theory:93">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>D</mi>
     <mo>′</mo>
    </msup>
    <mo>,</mo>
    <msup>
     <mi>D</mi>
     <mi>′′</mi>
    </msup>
   </mrow>
   <mo>∈</mo>
   <msub>
    <mi>C</mi>
    <mi>ϵ</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <list>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>D</ci>
      <ci>normal-′</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>D</ci>
      <ci>′′</ci>
     </apply>
    </list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>C</ci>
     <ci>ϵ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D^{\prime},D^{\prime\prime}\in C_{\epsilon}
  </annotation>
 </semantics>
</math>


 it is not trivial how we can compare 

<math display="inline" id="Distribution_learning_theory:94">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo>,</mo>
    <msup>
     <mi>D</mi>
     <mo>′</mo>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>d</ci>
    <interval closure="open">
     <ci>D</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>D</ci>
      <ci>normal-′</ci>
     </apply>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle d(D,D^{\prime})
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Distribution_learning_theory:95">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo>,</mo>
    <msup>
     <mi>D</mi>
     <mi>′′</mi>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>d</ci>
    <interval closure="open">
     <ci>D</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>D</ci>
      <ci>′′</ci>
     </apply>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle d(D,D^{\prime\prime})
  </annotation>
 </semantics>
</math>

 in order to decide which one is the closest to 

<math display="inline" id="Distribution_learning_theory:96">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

, because 

<math display="inline" id="Distribution_learning_theory:97">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

 is unknown. Therefore the samples from 

<math display="inline" id="Distribution_learning_theory:98">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>


 have to be used to do these comparisons. Obviously the result of the comparison always has a probability of error. So the task is similar with finding the minimum in a set of element using noisy comparisons. There are a lot of classical algorithms in order to achieve this goal. The most recent one which achieves the best guarantees was proposed by <a href="Constantinos_Daskalakis" title="wikilink">Daskalakis</a> and <a href="Gautam_Kamath" title="wikilink">Kamath</a> <a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> This algorithm sets up a fast tournament between the elements of 

<math display="inline" id="Distribution_learning_theory:99">
 <semantics>
  <msub>
   <mi>C</mi>
   <mi>ϵ</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>C</ci>
    <ci>ϵ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle C_{\epsilon}
  </annotation>
 </semantics>
</math>

 where the winner 

<math display="inline" id="Distribution_learning_theory:100">
 <semantics>
  <msup>
   <mi>D</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>D</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D^{*}
  </annotation>
 </semantics>
</math>

 of this tournament is the element which is 

<math display="inline" id="Distribution_learning_theory:101">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mo>-</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">limit-from</csymbol>
    <ci>ϵ</ci>
    <minus></minus>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon-
  </annotation>
 </semantics>
</math>

close to 

<math display="inline" id="Distribution_learning_theory:102">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

 (i.e. 

<math display="inline" id="Distribution_learning_theory:103">
 <semantics>
  <mrow>
   <mrow>
    <mi>d</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msup>
      <mi>D</mi>
      <mo>*</mo>
     </msup>
     <mo>,</mo>
     <mi>D</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>≤</mo>
   <mi>ϵ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <apply>
     <times></times>
     <ci>d</ci>
     <interval closure="open">
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>D</ci>
       <times></times>
      </apply>
      <ci>D</ci>
     </interval>
    </apply>
    <ci>ϵ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle d(D^{*},D)\leq\epsilon
  </annotation>
 </semantics>
</math>


) with probability at least 

<math display="inline" id="Distribution_learning_theory:104">
 <semantics>
  <mrow>
   <mn>1</mn>
   <mo>-</mo>
   <mi>δ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <cn type="integer">1</cn>
    <ci>δ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle 1-\delta
  </annotation>
 </semantics>
</math>

. In order to do so their algorithm uses 

<math display="inline" id="Distribution_learning_theory:105">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>log</mi>
     <mrow>
      <mi>N</mi>
      <mo>/</mo>
      <msup>
       <mi>ϵ</mi>
       <mn>2</mn>
      </msup>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <log></log>
     <apply>
      <divide></divide>
      <ci>N</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>ϵ</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle O(\log N/\epsilon^{2})
  </annotation>
 </semantics>
</math>

 samples from 

<math display="inline" id="Distribution_learning_theory:106">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

 and runs in 

<math display="inline" id="Distribution_learning_theory:107">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>N</mi>
     <mrow>
      <mi>log</mi>
      <mrow>
       <mi>N</mi>
       <mo>/</mo>
       <msup>
        <mi>ϵ</mi>
        <mn>2</mn>
       </msup>
      </mrow>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <times></times>
     <ci>N</ci>
     <apply>
      <log></log>
      <apply>
       <divide></divide>
       <ci>N</ci>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>ϵ</ci>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle O(N\log N/\epsilon^{2})
  </annotation>
 </semantics>
</math>

 time, where 

<math display="inline" id="Distribution_learning_theory:108">
 <semantics>
  <mrow>
   <mi>N</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">|</mo>
    <msub>
     <mi>C</mi>
     <mi>ϵ</mi>
    </msub>
    <mo stretchy="false">|</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>N</ci>
    <apply>
     <abs></abs>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>C</ci>
      <ci>ϵ</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle N=|C_{\epsilon}|
  </annotation>
 </semantics>
</math>


.</p>
<h2 id="learning-sums-of-random-variables">Learning Sums of Random Variables</h2>

<p>Learning of simple well known distribution is an well studied field and there are a lot of estimators that can be used. One more complicated class of distributions is the distributions of sum of variables that follow simple distributions. These learning procedure have a close relation with limit theorems like the central limit theorem because they tent to examine the same object when the sum tends to an infinite sum. Recently there are two interesting results that we will describe here the : learning Poisson binomial distributions and learning sums of independent integer random variables. All the results below hold using the <a href="total_variation" title="wikilink">total variation</a> distance as a distance measure.</p>
<h3 id="learning-poisson-binomial-distributions">Learning Poisson Binomial Distributions <a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></h3>

<p>Consider 

<math display="inline" id="Distribution_learning_theory:109">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle n
  </annotation>
 </semantics>
</math>

 independent Bernoulli random variables 

<math display="inline" id="Distribution_learning_theory:110">
 <semantics>
  <mrow>
   <msub>
    <mi>X</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>X</mi>
    <mi>n</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>X</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>X</ci>
     <ci>n</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle X_{1},\dots,X_{n}
  </annotation>
 </semantics>
</math>

 with probabilities of success 

<math display="inline" id="Distribution_learning_theory:111">
 <semantics>
  <mrow>
   <msub>
    <mi>p</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>p</mi>
    <mi>n</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>p</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>p</ci>
     <ci>n</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle p_{1},\dots,p_{n}
  </annotation>
 </semantics>
</math>

. A Poisson Binomial Distribution of order 

<math display="inline" id="Distribution_learning_theory:112">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle n
  </annotation>
 </semantics>
</math>

 is the distribution of the sum 

<math display="inline" id="Distribution_learning_theory:113">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>=</mo>
   <mrow>
    <msub>
     <mo largeop="true" symmetric="true">∑</mo>
     <mi>i</mi>
    </msub>
    <msub>
     <mi>X</mi>
     <mi>i</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>X</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <ci>i</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>X</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle X=\sum_{i}X_{i}
  </annotation>
 </semantics>
</math>


. For learning the class 

<math display="inline" id="Distribution_learning_theory:114">
 <semantics>
  <mrow>
   <mrow>
    <mi>P</mi>
    <mi>B</mi>
    <mi>D</mi>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>D</mi>
    <mo>:</mo>
    <mrow>
     <mpadded width="+3.3pt">
      <mi>D</mi>
     </mpadded>
     <mtext>is a Poisson binomial distribution</mtext>
    </mrow>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>P</ci>
     <ci>B</ci>
     <ci>D</ci>
    </apply>
    <apply>
     <csymbol cd="latexml">conditional-set</csymbol>
     <ci>D</ci>
     <apply>
      <times></times>
      <ci>D</ci>
      <mtext>is a Poisson binomial distribution</mtext>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle PBD=\{D:D~{}\text{ is a Poisson binomial distribution}\}
  </annotation>
 </semantics>
</math>

. The first of the following results deals with the case of improper learning of 

<math display="inline" id="Distribution_learning_theory:115">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mi>B</mi>
   <mi>D</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <ci>B</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle PBD
  </annotation>
 </semantics>
</math>

 and the second with the proper learning of 

<math display="inline" id="Distribution_learning_theory:116">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mi>B</mi>
   <mi>D</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <ci>B</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle PBD
  </annotation>
 </semantics>
</math>

.</p>

<p><strong>Theorem</strong></p>

<p><em>Let 

<math display="inline" id="Distribution_learning_theory:117">
 <semantics>
  <mrow>
   <mi>D</mi>
   <mo>∈</mo>
   <mrow>
    <mi>P</mi>
    <mi>B</mi>
    <mi>D</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>D</ci>
    <apply>
     <times></times>
     <ci>P</ci>
     <ci>B</ci>
     <ci>D</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D\in PBD
  </annotation>
 </semantics>
</math>

 then there is an algorithm which given 

<math display="inline" id="Distribution_learning_theory:118">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle n
  </annotation>
 </semantics>
</math>


, 

<math display="inline" id="Distribution_learning_theory:119">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>ϵ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon>0
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Distribution_learning_theory:120">
 <semantics>
  <mrow>
   <mn>0</mn>
   <mo><</mo>
   <mi>δ</mi>
   <mo>≤</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <lt></lt>
     <cn type="integer">0</cn>
     <ci>δ</ci>
    </apply>
    <apply>
     <leq></leq>
     <share href="#.cmml">
     </share>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle 0<\delta\leq 1
  </annotation>
 </semantics>
</math>

 and access to 

<math display="inline" id="Distribution_learning_theory:121">
 <semantics>
  <mrow>
   <mi>G</mi>
   <mi>E</mi>
   <mi>N</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>G</ci>
    <ci>E</ci>
    <ci>N</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle GEN(D)
  </annotation>
 </semantics>
</math>

 finds a 

<math display="inline" id="Distribution_learning_theory:122">
 <semantics>
  <msup>
   <mi>D</mi>
   <mo>′</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>D</ci>
    <ci>normal-′</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D^{\prime}
  </annotation>
 </semantics>
</math>

 such that 

<math display="inline" id="Distribution_learning_theory:123">
 <semantics>
  <mrow>
   <mrow>
    <mi>Pr</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mrow>
      <mrow>
       <mi>d</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>D</mi>
        <mo>,</mo>
        <msup>
         <mi>D</mi>
         <mo>′</mo>
        </msup>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>≤</mo>
      <mi>ϵ</mi>
     </mrow>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>≥</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <mi>δ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <geq></geq>
    <apply>
     <ci>Pr</ci>
     <apply>
      <leq></leq>
      <apply>
       <times></times>
       <ci>d</ci>
       <interval closure="open">
        <ci>D</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>D</ci>
         <ci>normal-′</ci>
        </apply>
       </interval>
      </apply>
      <ci>ϵ</ci>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <ci>δ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\Pr[d(D,D^{\prime})\leq\epsilon]\geq 1-\delta
  </annotation>
 </semantics>
</math>


. The sample complexity of this algorithm is 

<math display="inline" id="Distribution_learning_theory:124">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>O</mi>
    <mo stretchy="false">~</mo>
   </mover>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mn>1</mn>
       <mo>/</mo>
       <msup>
        <mi>ϵ</mi>
        <mn>3</mn>
       </msup>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mrow>
      <mi>log</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mn>1</mn>
        <mo>/</mo>
        <mi>δ</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <ci>normal-~</ci>
     <ci>O</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>ϵ</ci>
       <cn type="integer">3</cn>
      </apply>
     </apply>
     <apply>
      <log></log>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>δ</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\tilde{O}((1/\epsilon^{3})\log(1/\delta))
  </annotation>
 </semantics>
</math>

 and the running time is 

<math display="inline" id="Distribution_learning_theory:125">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>O</mi>
    <mo stretchy="false">~</mo>
   </mover>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mn>1</mn>
       <mo>/</mo>
       <msup>
        <mi>ϵ</mi>
        <mn>3</mn>
       </msup>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mrow>
      <mi>log</mi>
      <mrow>
       <mi>n</mi>
       <mrow>
        <msup>
         <mi>log</mi>
         <mn>2</mn>
        </msup>
        <mrow>
         <mo stretchy="false">(</mo>
         <mrow>
          <mn>1</mn>
          <mo>/</mo>
          <mi>δ</mi>
         </mrow>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <ci>normal-~</ci>
     <ci>O</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>ϵ</ci>
       <cn type="integer">3</cn>
      </apply>
     </apply>
     <apply>
      <log></log>
      <apply>
       <times></times>
       <ci>n</ci>
       <apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <log></log>
         <cn type="integer">2</cn>
        </apply>
        <apply>
         <divide></divide>
         <cn type="integer">1</cn>
         <ci>δ</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\tilde{O}((1/\epsilon^{3})\log n\log^{2}(1/\delta))
  </annotation>
 </semantics>
</math>

.</em></p>

<p><strong>Theorem</strong></p>

<p><em>Let 

<math display="inline" id="Distribution_learning_theory:126">
 <semantics>
  <mrow>
   <mi>D</mi>
   <mo>∈</mo>
   <mrow>
    <mi>P</mi>
    <mi>B</mi>
    <mi>D</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>D</ci>
    <apply>
     <times></times>
     <ci>P</ci>
     <ci>B</ci>
     <ci>D</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D\in PBD
  </annotation>
 </semantics>
</math>

 then there is an algorithm which given 

<math display="inline" id="Distribution_learning_theory:127">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle n
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Distribution_learning_theory:128">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>ϵ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon>0
  </annotation>
 </semantics>
</math>


, 

<math display="inline" id="Distribution_learning_theory:129">
 <semantics>
  <mrow>
   <mn>0</mn>
   <mo><</mo>
   <mi>δ</mi>
   <mo>≤</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <lt></lt>
     <cn type="integer">0</cn>
     <ci>δ</ci>
    </apply>
    <apply>
     <leq></leq>
     <share href="#.cmml">
     </share>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle 0<\delta\leq 1
  </annotation>
 </semantics>
</math>

 and access to 

<math display="inline" id="Distribution_learning_theory:130">
 <semantics>
  <mrow>
   <mi>G</mi>
   <mi>E</mi>
   <mi>N</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>G</ci>
    <ci>E</ci>
    <ci>N</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle GEN(D)
  </annotation>
 </semantics>
</math>

 finds a 

<math display="inline" id="Distribution_learning_theory:131">
 <semantics>
  <mrow>
   <msup>
    <mi>D</mi>
    <mo>′</mo>
   </msup>
   <mo>∈</mo>
   <mrow>
    <mi>P</mi>
    <mi>B</mi>
    <mi>D</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>D</ci>
     <ci>normal-′</ci>
    </apply>
    <apply>
     <times></times>
     <ci>P</ci>
     <ci>B</ci>
     <ci>D</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D^{\prime}\in PBD
  </annotation>
 </semantics>
</math>

 such that 

<math display="inline" id="Distribution_learning_theory:132">
 <semantics>
  <mrow>
   <mrow>
    <mi>Pr</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mrow>
      <mrow>
       <mi>d</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>D</mi>
        <mo>,</mo>
        <msup>
         <mi>D</mi>
         <mo>′</mo>
        </msup>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>≤</mo>
      <mi>ϵ</mi>
     </mrow>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>≥</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <mi>δ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <geq></geq>
    <apply>
     <ci>Pr</ci>
     <apply>
      <leq></leq>
      <apply>
       <times></times>
       <ci>d</ci>
       <interval closure="open">
        <ci>D</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>D</ci>
         <ci>normal-′</ci>
        </apply>
       </interval>
      </apply>
      <ci>ϵ</ci>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <ci>δ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\Pr[d(D,D^{\prime})\leq\epsilon]\geq 1-\delta
  </annotation>
 </semantics>
</math>

. The sample complexity of this algorithm is 

<math display="inline" id="Distribution_learning_theory:133">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>O</mi>
    <mo stretchy="false">~</mo>
   </mover>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mn>1</mn>
      <mo>/</mo>
      <msup>
       <mi>ϵ</mi>
       <mn>2</mn>
      </msup>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mrow>
    <mi>log</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mn>1</mn>
      <mo>/</mo>
      <mi>δ</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <ci>normal-~</ci>
     <ci>O</ci>
    </apply>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>ϵ</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
    <apply>
     <log></log>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>δ</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\tilde{O}((1/\epsilon^{2}))\log(1/\delta)
  </annotation>
 </semantics>
</math>


 and the running time is 

<math display="inline" id="Distribution_learning_theory:134">
 <semantics>
  <mrow>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mn>1</mn>
      <mo>/</mo>
      <mi>ϵ</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mi>O</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <msup>
        <mi>log</mi>
        <mn>2</mn>
       </msup>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mn>1</mn>
         <mo>/</mo>
         <mi>ϵ</mi>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </msup>
   <mover accent="true">
    <mi>O</mi>
    <mo stretchy="false">~</mo>
   </mover>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>log</mi>
     <mrow>
      <mi>n</mi>
      <mrow>
       <mi>log</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mn>1</mn>
         <mo>/</mo>
         <mi>δ</mi>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>ϵ</ci>
     </apply>
     <apply>
      <times></times>
      <ci>O</ci>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <log></log>
        <cn type="integer">2</cn>
       </apply>
       <apply>
        <divide></divide>
        <cn type="integer">1</cn>
        <ci>ϵ</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <ci>normal-~</ci>
     <ci>O</ci>
    </apply>
    <apply>
     <log></log>
     <apply>
      <times></times>
      <ci>n</ci>
      <apply>
       <log></log>
       <apply>
        <divide></divide>
        <cn type="integer">1</cn>
        <ci>δ</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle(1/\epsilon)^{O(\log^{2}(1/\epsilon))}\tilde{O}(\log n\log(1/\delta))
  </annotation>
 </semantics>
</math>

.</em></p>

<p>One very interesting part of the above results is that the sample complexity of the learning algorithm doesn't depend on 

<math display="inline" id="Distribution_learning_theory:135">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle n
  </annotation>
 </semantics>
</math>

, although the description of 

<math display="inline" id="Distribution_learning_theory:136">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D
  </annotation>
 </semantics>
</math>

 is linear in 

<math display="inline" id="Distribution_learning_theory:137">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle n
  </annotation>
 </semantics>
</math>

. Also the second result is almost optimal with respect to the sample complexity because there is also a lower bound of 

<math display="inline" id="Distribution_learning_theory:138">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mn>1</mn>
     <mo>/</mo>
     <msup>
      <mi>ϵ</mi>
      <mn>2</mn>
     </msup>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>ϵ</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle O(1/\epsilon^{2})
  </annotation>
 </semantics>
</math>


.</p>

<p>The proof uses a small 

<math display="inline" id="Distribution_learning_theory:139">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mo>-</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">limit-from</csymbol>
    <ci>ϵ</ci>
    <minus></minus>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon-
  </annotation>
 </semantics>
</math>

cover of 

<math display="inline" id="Distribution_learning_theory:140">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mi>B</mi>
   <mi>D</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <ci>B</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle PBD
  </annotation>
 </semantics>
</math>

 that has been produced by Daskalakis and Papadimitriou,<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> in order to get this algorithm.</p>
<h3 id="learning-sums-of-independent-integer-random-variables">Learning Sums of Independent Integer Random Variables <a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></h3>

<p>Consider 

<math display="inline" id="Distribution_learning_theory:141">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle n
  </annotation>
 </semantics>
</math>

 independent random variables 

<math display="inline" id="Distribution_learning_theory:142">
 <semantics>
  <mrow>
   <msub>
    <mi>X</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>X</mi>
    <mi>n</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>X</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>X</ci>
     <ci>n</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle X_{1},\dots,X_{n}
  </annotation>
 </semantics>
</math>

 each of which follows an arbitrary distribution with support 

<math display="inline" id="Distribution_learning_theory:143">
 <semantics>
  <mrow>
   <mo stretchy="false">{</mo>
   <mn>0</mn>
   <mo>,</mo>
   <mn>1</mn>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <mrow>
    <mi>k</mi>
    <mo>-</mo>
    <mn>1</mn>
   </mrow>
   <mo stretchy="false">}</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <set>
    <cn type="integer">0</cn>
    <cn type="integer">1</cn>
    <ci>normal-…</ci>
    <apply>
     <minus></minus>
     <ci>k</ci>
     <cn type="integer">1</cn>
    </apply>
   </set>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\{0,1,\dots,k-1\}
  </annotation>
 </semantics>
</math>


. A 

<math display="inline" id="Distribution_learning_theory:144">
 <semantics>
  <mrow>
   <mi>k</mi>
   <mo>-</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">limit-from</csymbol>
    <ci>k</ci>
    <minus></minus>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle k-
  </annotation>
 </semantics>
</math>

sum of independent integer random variable of order 

<math display="inline" id="Distribution_learning_theory:145">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle n
  </annotation>
 </semantics>
</math>

 is the distribution of the sum 

<math display="inline" id="Distribution_learning_theory:146">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>=</mo>
   <mrow>
    <msub>
     <mo largeop="true" symmetric="true">∑</mo>
     <mi>i</mi>
    </msub>
    <msub>
     <mi>X</mi>
     <mi>i</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>X</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <ci>i</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>X</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle X=\sum_{i}X_{i}
  </annotation>
 </semantics>
</math>

. For learning the class</p>

<p>

<math display="inline" id="Distribution_learning_theory:147">
 <semantics>
  <mrow>
   <mrow>
    <mi>k</mi>
    <mo>-</mo>
    <mrow>
     <mi>S</mi>
     <mi>I</mi>
     <mi>I</mi>
     <mi>R</mi>
     <mi>V</mi>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>D</mi>
    <mo>:</mo>
    <mrow>
     <mi>D</mi>
     <mtext>is a k-sum of independent integer random variable</mtext>
    </mrow>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <minus></minus>
     <ci>k</ci>
     <apply>
      <times></times>
      <ci>S</ci>
      <ci>I</ci>
      <ci>I</ci>
      <ci>R</ci>
      <ci>V</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="latexml">conditional-set</csymbol>
     <ci>D</ci>
     <apply>
      <times></times>
      <ci>D</ci>
      <mtext>is a k-sum of independent integer random variable</mtext>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle k-SIIRV=\{D:D\text{is a k-sum of independent integer random %
variable }\}
  </annotation>
 </semantics>
</math>

</p>

<p>there is the following result</p>

<p><strong>Theorem</strong></p>

<p><em>Let 

<math display="inline" id="Distribution_learning_theory:148">
 <semantics>
  <mrow>
   <mi>D</mi>
   <mo>∈</mo>
   <mrow>
    <mi>k</mi>
    <mo>-</mo>
    <mrow>
     <mi>S</mi>
     <mi>I</mi>
     <mi>I</mi>
     <mi>R</mi>
     <mi>V</mi>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>D</ci>
    <apply>
     <minus></minus>
     <ci>k</ci>
     <apply>
      <times></times>
      <ci>S</ci>
      <ci>I</ci>
      <ci>I</ci>
      <ci>R</ci>
      <ci>V</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D\in k-SIIRV
  </annotation>
 </semantics>
</math>


 then there is an algorithm which given 

<math display="inline" id="Distribution_learning_theory:149">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle n
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Distribution_learning_theory:150">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>ϵ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon>0
  </annotation>
 </semantics>
</math>

 and access to 

<math display="inline" id="Distribution_learning_theory:151">
 <semantics>
  <mrow>
   <mi>G</mi>
   <mi>E</mi>
   <mi>N</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>G</ci>
    <ci>E</ci>
    <ci>N</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle GEN(D)
  </annotation>
 </semantics>
</math>

 finds a 

<math display="inline" id="Distribution_learning_theory:152">
 <semantics>
  <msup>
   <mi>D</mi>
   <mo>′</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>D</ci>
    <ci>normal-′</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D^{\prime}
  </annotation>
 </semantics>
</math>

 such that 

<math display="inline" id="Distribution_learning_theory:153">
 <semantics>
  <mrow>
   <mrow>
    <mi>Pr</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mrow>
      <mrow>
       <mi>d</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>D</mi>
        <mo>,</mo>
        <msup>
         <mi>D</mi>
         <mo>′</mo>
        </msup>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>≤</mo>
      <mi>ϵ</mi>
     </mrow>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>≥</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <mi>δ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <geq></geq>
    <apply>
     <ci>Pr</ci>
     <apply>
      <leq></leq>
      <apply>
       <times></times>
       <ci>d</ci>
       <interval closure="open">
        <ci>D</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>D</ci>
         <ci>normal-′</ci>
        </apply>
       </interval>
      </apply>
      <ci>ϵ</ci>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <ci>δ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\Pr[d(D,D^{\prime})\leq\epsilon]\geq 1-\delta
  </annotation>
 </semantics>
</math>


. The sample complexity of this algorithm is 

<math display="inline" id="Distribution_learning_theory:154">
 <semantics>
  <mrow>
   <mtext>poly</mtext>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>k</mi>
     <mo>/</mo>
     <mi>ϵ</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <mtext>poly</mtext>
    <apply>
     <divide></divide>
     <ci>k</ci>
     <ci>ϵ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\text{poly}(k/\epsilon)
  </annotation>
 </semantics>
</math>

 and the running time is also 

<math display="inline" id="Distribution_learning_theory:155">
 <semantics>
  <mrow>
   <mtext>poly</mtext>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>k</mi>
     <mo>/</mo>
     <mi>ϵ</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <mtext>poly</mtext>
    <apply>
     <divide></divide>
     <ci>k</ci>
     <ci>ϵ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\text{poly}(k/\epsilon)
  </annotation>
 </semantics>
</math>

.</em></p>

<p>Again one interesting part is that the sample and the time complexity does not depend on 

<math display="inline" id="Distribution_learning_theory:156">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle n
  </annotation>
 </semantics>
</math>

. Its possible to conclude this independence for the previous section if we set 

<math display="inline" id="Distribution_learning_theory:157">
 <semantics>
  <mrow>
   <mi>k</mi>
   <mo>=</mo>
   <mn>2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>k</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle k=2
  </annotation>
 </semantics>
</math>

.</p>
<h2 id="learning-mixtures-of-gaussians">Learning Mixtures of Gaussians <a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></h2>

<p>Let the random variables 

<math display="inline" id="Distribution_learning_theory:158">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>∼</mo>
   <mrow>
    <mi>N</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>μ</mi>
      <mn>1</mn>
     </msub>
     <mo>,</mo>
     <msub>
      <mi mathvariant="normal">Σ</mi>
      <mn>1</mn>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <ci>X</ci>
    <apply>
     <times></times>
     <ci>N</ci>
     <interval closure="open">
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>μ</ci>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-Σ</ci>
       <cn type="integer">1</cn>
      </apply>
     </interval>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle X\sim N(\mu_{1},\Sigma_{1})
  </annotation>
 </semantics>
</math>


 and 

<math display="inline" id="Distribution_learning_theory:159">
 <semantics>
  <mrow>
   <mi>Y</mi>
   <mo>∼</mo>
   <mrow>
    <mi>N</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>μ</mi>
      <mn>2</mn>
     </msub>
     <mo>,</mo>
     <msub>
      <mi mathvariant="normal">Σ</mi>
      <mn>2</mn>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <ci>Y</ci>
    <apply>
     <times></times>
     <ci>N</ci>
     <interval closure="open">
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>μ</ci>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-Σ</ci>
       <cn type="integer">2</cn>
      </apply>
     </interval>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle Y\sim N(\mu_{2},\Sigma_{2})
  </annotation>
 </semantics>
</math>

. Define the random variable 

<math display="inline" id="Distribution_learning_theory:160">
 <semantics>
  <mi>Z</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Z</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle Z
  </annotation>
 </semantics>
</math>

 which takes the same value as 

<math display="inline" id="Distribution_learning_theory:161">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle X
  </annotation>
 </semantics>
</math>

 with probability 

<math display="inline" id="Distribution_learning_theory:162">
 <semantics>
  <msub>
   <mi>w</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle w_{1}
  </annotation>
 </semantics>
</math>

 and the same value as 

<math display="inline" id="Distribution_learning_theory:163">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle Y
  </annotation>
 </semantics>
</math>


 with probability 

<math display="inline" id="Distribution_learning_theory:164">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mn>2</mn>
   </msub>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <msub>
     <mi>w</mi>
     <mn>1</mn>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <cn type="integer">2</cn>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle w_{2}=1-w_{1}
  </annotation>
 </semantics>
</math>

. Then if 

<math display="inline" id="Distribution_learning_theory:165">
 <semantics>
  <msub>
   <mi>F</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>F</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle F_{1}
  </annotation>
 </semantics>
</math>

 is the density of 

<math display="inline" id="Distribution_learning_theory:166">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle X
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Distribution_learning_theory:167">
 <semantics>
  <msub>
   <mi>F</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>F</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle F_{2}
  </annotation>
 </semantics>
</math>

 is the density of 

<math display="inline" id="Distribution_learning_theory:168">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle Y
  </annotation>
 </semantics>
</math>


 the density of 

<math display="inline" id="Distribution_learning_theory:169">
 <semantics>
  <mi>Z</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Z</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle Z
  </annotation>
 </semantics>
</math>

 is 

<math display="inline" id="Distribution_learning_theory:170">
 <semantics>
  <mrow>
   <mi>F</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <msub>
      <mi>w</mi>
      <mn>1</mn>
     </msub>
     <msub>
      <mi>F</mi>
      <mn>1</mn>
     </msub>
    </mrow>
    <mo>+</mo>
    <mrow>
     <msub>
      <mi>w</mi>
      <mn>2</mn>
     </msub>
     <msub>
      <mi>F</mi>
      <mn>2</mn>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>F</ci>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>F</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>F</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle F=w_{1}F_{1}+w_{2}F_{2}
  </annotation>
 </semantics>
</math>

. In this case 

<math display="inline" id="Distribution_learning_theory:171">
 <semantics>
  <mi>Z</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Z</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle Z
  </annotation>
 </semantics>
</math>

 is said to follow a mixture of Gaussians. Pearson <a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> was the first who introduced the notion of the mixtures of Gaussians in his attempt to explain the probability distribution from which he got same data that he wanted to analyze. So after doing a lot of calculations by hand, he finally fitted his data to a mixture of Gaussians. The learning task in this case is to determine the parameters of the mixture 

<math display="inline" id="Distribution_learning_theory:172">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>w</mi>
    <mn>2</mn>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>μ</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>μ</mi>
    <mn>2</mn>
   </msub>
   <mo>,</mo>
   <msub>
    <mi mathvariant="normal">Σ</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <msub>
    <mi mathvariant="normal">Σ</mi>
    <mn>2</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <cn type="integer">2</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>μ</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>μ</ci>
     <cn type="integer">2</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>normal-Σ</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>normal-Σ</ci>
     <cn type="integer">2</cn>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle w_{1},w_{2},\mu_{1},\mu_{2},\Sigma_{1},\Sigma_{2}
  </annotation>
 </semantics>
</math>

.</p>

<p>The first attempt to solve this problem was from <a href="S._Dasgupta" title="wikilink">Dasgupta</a>.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> In this work <a href="S._Dasgupta" title="wikilink">Dasgupta</a> assumes that the two means of the Gaussians are far enough from each other. This means that there is a lower bound on the distance 

<math display="inline" id="Distribution_learning_theory:173">
 <semantics>
  <mrow>
   <mo fence="true">||</mo>
   <mrow>
    <msub>
     <mi>μ</mi>
     <mn>1</mn>
    </msub>
    <mo>-</mo>
    <msub>
     <mi>μ</mi>
     <mn>2</mn>
    </msub>
   </mrow>
   <mo fence="true">||</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">norm</csymbol>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>μ</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>μ</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle||\mu_{1}-\mu_{2}||
  </annotation>
 </semantics>
</math>


. Using this assumption Dasgupta and a lot of scientists after him where able to learn the parameters of the mixture. The learning procedure starts with <a href="Cluster_analysis" title="wikilink">clustering</a> the samples into two different clusters minimizing some metric. Using the assumption that the means of the Gaussians are far away from each other with high probability the samples in the first cluster correspond to samples from the first Gaussian and the samples in the second cluster to samples from the second one. Now that the samples are partitioned the 

<math display="inline" id="Distribution_learning_theory:174">
 <semantics>
  <mrow>
   <msub>
    <mi>μ</mi>
    <mi>i</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi mathvariant="normal">Σ</mi>
    <mi>i</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>μ</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>normal-Σ</ci>
     <ci>i</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\mu_{i},\Sigma_{i}
  </annotation>
 </semantics>
</math>

 can be computed from simple statistical estimators and 

<math display="inline" id="Distribution_learning_theory:175">
 <semantics>
  <msub>
   <mi>w</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle w_{i}
  </annotation>
 </semantics>
</math>

 by comparing the magnitude of the clusters.</p>

<p>If 

<math display="inline" id="Distribution_learning_theory:176">
 <semantics>
  <mrow>
   <mi>G</mi>
   <mi>M</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>G</ci>
    <ci>M</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle GM
  </annotation>
 </semantics>
</math>

 is the set of all the mixtures of two Gaussians, using the above procedure theorems like the following can be proved.</p>

<p><strong>Theorem <a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></strong></p>

<p><em>Let 

<math display="inline" id="Distribution_learning_theory:177">
 <semantics>
  <mrow>
   <mi>D</mi>
   <mo>∈</mo>
   <mrow>
    <mi>G</mi>
    <mi>M</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>D</ci>
    <apply>
     <times></times>
     <ci>G</ci>
     <ci>M</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle D\in GM
  </annotation>
 </semantics>
</math>

 with 

<math display="inline" id="Distribution_learning_theory:178">
 <semantics>
  <mrow>
   <mrow>
    <mo fence="true">||</mo>
    <mrow>
     <msub>
      <mi>μ</mi>
      <mn>1</mn>
     </msub>
     <mo>-</mo>
     <msub>
      <mi>μ</mi>
      <mn>2</mn>
     </msub>
    </mrow>
    <mo fence="true">||</mo>
   </mrow>
   <mo>≥</mo>
   <mrow>
    <mi>c</mi>
    <msqrt>
     <mrow>
      <mi>n</mi>
      <mi>max</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>λ</mi>
        <mrow>
         <mi>m</mi>
         <mi>a</mi>
         <mi>x</mi>
        </mrow>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <msub>
         <mi mathvariant="normal">Σ</mi>
         <mn>1</mn>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
       <mo>,</mo>
       <msub>
        <mi>λ</mi>
        <mrow>
         <mi>m</mi>
         <mi>a</mi>
         <mi>x</mi>
        </mrow>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <msub>
         <mi mathvariant="normal">Σ</mi>
         <mn>2</mn>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </msqrt>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <geq></geq>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>μ</ci>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>μ</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>c</ci>
     <apply>
      <root></root>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <csymbol cd="unknown">n</csymbol>
       <max></max>
       <cerror>
        <csymbol cd="ambiguous">fragments</csymbol>
        <ci>normal-(</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>λ</ci>
         <apply>
          <times></times>
          <ci>m</ci>
          <ci>a</ci>
          <ci>x</ci>
         </apply>
        </apply>
        <cerror>
         <csymbol cd="ambiguous">fragments</csymbol>
         <ci>normal-(</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>normal-Σ</ci>
          <cn type="integer">1</cn>
         </apply>
         <ci>normal-)</ci>
        </cerror>
        <ci>normal-,</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>λ</ci>
         <apply>
          <times></times>
          <ci>m</ci>
          <ci>a</ci>
          <ci>x</ci>
         </apply>
        </apply>
        <cerror>
         <csymbol cd="ambiguous">fragments</csymbol>
         <ci>normal-(</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>normal-Σ</ci>
          <cn type="integer">2</cn>
         </apply>
         <ci>normal-)</ci>
        </cerror>
       </cerror>
      </cerror>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle||\mu_{1}-\mu_{2}||\geq c\sqrt{n\max(\lambda_{max}(\Sigma_{1}),%
\lambda_{max}(\Sigma_{2})}
  </annotation>
 </semantics>
</math>


, where 

<math display="inline" id="Distribution_learning_theory:179">
 <semantics>
  <mrow>
   <mi>c</mi>
   <mo>></mo>
   <mrow>
    <mn>1</mn>
    <mo>/</mo>
    <mn>2</mn>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>c</ci>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle c>1/2
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Distribution_learning_theory:180">
 <semantics>
  <mrow>
   <msub>
    <mi>λ</mi>
    <mrow>
     <mi>m</mi>
     <mi>a</mi>
     <mi>x</mi>
    </mrow>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>A</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>λ</ci>
     <apply>
      <times></times>
      <ci>m</ci>
      <ci>a</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <ci>A</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\lambda_{max}(A)
  </annotation>
 </semantics>
</math>

 the largest eigenvalue of 

<math display="inline" id="Distribution_learning_theory:181">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle A
  </annotation>
 </semantics>
</math>

, then there is an algorithm which given 

<math display="inline" id="Distribution_learning_theory:182">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>ϵ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon>0
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Distribution_learning_theory:183">
 <semantics>
  <mrow>
   <mn>0</mn>
   <mo><</mo>
   <mi>δ</mi>
   <mo>≤</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <lt></lt>
     <cn type="integer">0</cn>
     <ci>δ</ci>
    </apply>
    <apply>
     <leq></leq>
     <share href="#.cmml">
     </share>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle 0<\delta\leq 1
  </annotation>
 </semantics>
</math>


 and access to 

<math display="inline" id="Distribution_learning_theory:184">
 <semantics>
  <mrow>
   <mi>G</mi>
   <mi>E</mi>
   <mi>N</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>G</ci>
    <ci>E</ci>
    <ci>N</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle GEN(D)
  </annotation>
 </semantics>
</math>

 finds an approximation 

<math display="inline" id="Distribution_learning_theory:185">
 <semantics>
  <mrow>
   <msubsup>
    <mi>w</mi>
    <mi>i</mi>
    <mo>′</mo>
   </msubsup>
   <mo>,</mo>
   <msubsup>
    <mi>μ</mi>
    <mi>i</mi>
    <mo>′</mo>
   </msubsup>
   <mo>,</mo>
   <msubsup>
    <mi mathvariant="normal">Σ</mi>
    <mi>i</mi>
    <mo>′</mo>
   </msubsup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>w</ci>
      <ci>normal-′</ci>
     </apply>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>μ</ci>
      <ci>normal-′</ci>
     </apply>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>normal-Σ</ci>
      <ci>normal-′</ci>
     </apply>
     <ci>i</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle w^{\prime}_{i},\mu^{\prime}_{i},\Sigma^{\prime}_{i}
  </annotation>
 </semantics>
</math>

 of the parameters such that 

<math display="inline" id="Distribution_learning_theory:186">
 <semantics>
  <mrow>
   <mrow>
    <mi>Pr</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mrow>
      <mrow>
       <mo fence="true">||</mo>
       <mrow>
        <msub>
         <mi>w</mi>
         <mi>i</mi>
        </msub>
        <mo>-</mo>
        <msubsup>
         <mi>w</mi>
         <mi>i</mi>
         <mo>′</mo>
        </msubsup>
       </mrow>
       <mo fence="true">||</mo>
      </mrow>
      <mo>≤</mo>
      <mi>ϵ</mi>
     </mrow>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>≥</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <mi>δ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <geq></geq>
    <apply>
     <ci>Pr</ci>
     <apply>
      <leq></leq>
      <apply>
       <csymbol cd="latexml">norm</csymbol>
       <apply>
        <minus></minus>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>w</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <ci>w</ci>
          <ci>normal-′</ci>
         </apply>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
      <ci>ϵ</ci>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <ci>δ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\Pr[||w_{i}-w^{\prime}_{i}||\leq\epsilon]\geq 1-\delta
  </annotation>
 </semantics>
</math>

 (respectively for 

<math display="inline" id="Distribution_learning_theory:187">
 <semantics>
  <msub>
   <mi>μ</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>μ</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\mu_{i}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Distribution_learning_theory:188">
 <semantics>
  <msub>
   <mi mathvariant="normal">Σ</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-Σ</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\Sigma_{i}
  </annotation>
 </semantics>
</math>


. The sample complexity of this algorithm is 

<math display="inline" id="Distribution_learning_theory:189">
 <semantics>
  <mrow>
   <mi>M</mi>
   <mo>=</mo>
   <msup>
    <mn>2</mn>
    <mrow>
     <mi>O</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <msup>
        <mi>log</mi>
        <mn>2</mn>
       </msup>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mn>1</mn>
         <mo>/</mo>
         <mrow>
          <mo stretchy="false">(</mo>
          <mrow>
           <mi>ϵ</mi>
           <mi>δ</mi>
          </mrow>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>M</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cn type="integer">2</cn>
     <apply>
      <times></times>
      <ci>O</ci>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <log></log>
        <cn type="integer">2</cn>
       </apply>
       <apply>
        <divide></divide>
        <cn type="integer">1</cn>
        <apply>
         <times></times>
         <ci>ϵ</ci>
         <ci>δ</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle M=2^{O(\log^{2}(1/(\epsilon\delta)))}
  </annotation>
 </semantics>
</math>

 and the running time is 

<math display="inline" id="Distribution_learning_theory:190">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mrow>
      <msup>
       <mi>M</mi>
       <mn>2</mn>
      </msup>
      <mi>d</mi>
     </mrow>
     <mo>+</mo>
     <mrow>
      <mi>M</mi>
      <mi>d</mi>
      <mi>n</mi>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>M</ci>
       <cn type="integer">2</cn>
      </apply>
      <ci>d</ci>
     </apply>
     <apply>
      <times></times>
      <ci>M</ci>
      <ci>d</ci>
      <ci>n</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle O(M^{2}d+Mdn)
  </annotation>
 </semantics>
</math>

.</em></p>

<p>The above result could also be generalized in 

<math display="inline" id="Distribution_learning_theory:191">
 <semantics>
  <mrow>
   <mi>k</mi>
   <mo>-</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">limit-from</csymbol>
    <ci>k</ci>
    <minus></minus>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle k-
  </annotation>
 </semantics>
</math>

mixture of Gaussians.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a></p>

<p>Interestingly for the case of mixture of two Gaussians there are learning results without the assumption of the distance between their means, like the following one which uses the total variation distance as a distance measure.</p>

<p><strong>Theorem <a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></strong></p>

<p><em>Let 

<math display="inline" id="Distribution_learning_theory:192">
 <semantics>
  <mrow>
   <mi>F</mi>
   <mo>∈</mo>
   <mrow>
    <mi>G</mi>
    <mi>M</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>F</ci>
    <apply>
     <times></times>
     <ci>G</ci>
     <ci>M</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle F\in GM
  </annotation>
 </semantics>
</math>

 then there is an algorithm which given 

<math display="inline" id="Distribution_learning_theory:193">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>ϵ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\epsilon>0
  </annotation>
 </semantics>
</math>


, 

<math display="inline" id="Distribution_learning_theory:194">
 <semantics>
  <mrow>
   <mn>0</mn>
   <mo><</mo>
   <mi>δ</mi>
   <mo>≤</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <lt></lt>
     <cn type="integer">0</cn>
     <ci>δ</ci>
    </apply>
    <apply>
     <leq></leq>
     <share href="#.cmml">
     </share>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle 0<\delta\leq 1
  </annotation>
 </semantics>
</math>

 and access to 

<math display="inline" id="Distribution_learning_theory:195">
 <semantics>
  <mrow>
   <mi>G</mi>
   <mi>E</mi>
   <mi>N</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>G</ci>
    <ci>E</ci>
    <ci>N</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle GEN(D)
  </annotation>
 </semantics>
</math>

 finds 

<math display="inline" id="Distribution_learning_theory:196">
 <semantics>
  <mrow>
   <msubsup>
    <mi>w</mi>
    <mi>i</mi>
    <mo>′</mo>
   </msubsup>
   <mo>,</mo>
   <msubsup>
    <mi>μ</mi>
    <mi>i</mi>
    <mo>′</mo>
   </msubsup>
   <mo>,</mo>
   <msubsup>
    <mi mathvariant="normal">Σ</mi>
    <mi>i</mi>
    <mo>′</mo>
   </msubsup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>w</ci>
      <ci>normal-′</ci>
     </apply>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>μ</ci>
      <ci>normal-′</ci>
     </apply>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>normal-Σ</ci>
      <ci>normal-′</ci>
     </apply>
     <ci>i</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle w^{\prime}_{i},\mu^{\prime}_{i},\Sigma^{\prime}_{i}
  </annotation>
 </semantics>
</math>

 such that if 

<math display="inline" id="Distribution_learning_theory:197">
 <semantics>
  <mrow>
   <msup>
    <mi>F</mi>
    <mo>′</mo>
   </msup>
   <mo>=</mo>
   <mrow>
    <mrow>
     <msubsup>
      <mi>w</mi>
      <mn>1</mn>
      <mo>′</mo>
     </msubsup>
     <msubsup>
      <mi>F</mi>
      <mn>1</mn>
      <mo>′</mo>
     </msubsup>
    </mrow>
    <mo>+</mo>
    <mrow>
     <msubsup>
      <mi>w</mi>
      <mn>2</mn>
      <mo>′</mo>
     </msubsup>
     <msubsup>
      <mi>F</mi>
      <mn>2</mn>
      <mo>′</mo>
     </msubsup>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>F</ci>
     <ci>normal-′</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>w</ci>
        <ci>normal-′</ci>
       </apply>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>F</ci>
        <ci>normal-′</ci>
       </apply>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>w</ci>
        <ci>normal-′</ci>
       </apply>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>F</ci>
        <ci>normal-′</ci>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle F^{\prime}=w^{\prime}_{1}F^{\prime}_{1}+w^{\prime}_{2}F^{\prime}_{2}
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Distribution_learning_theory:198">
 <semantics>
  <mrow>
   <msubsup>
    <mi>F</mi>
    <mi>i</mi>
    <mo>′</mo>
   </msubsup>
   <mo>=</mo>
   <mrow>
    <mi>N</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msubsup>
      <mi>μ</mi>
      <mi>i</mi>
      <mo>′</mo>
     </msubsup>
     <mo>,</mo>
     <msubsup>
      <mi mathvariant="normal">Σ</mi>
      <mi>i</mi>
      <mo>′</mo>
     </msubsup>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>F</ci>
      <ci>normal-′</ci>
     </apply>
     <ci>i</ci>
    </apply>
    <apply>
     <times></times>
     <ci>N</ci>
     <interval closure="open">
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>μ</ci>
        <ci>normal-′</ci>
       </apply>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>normal-Σ</ci>
        <ci>normal-′</ci>
       </apply>
       <ci>i</ci>
      </apply>
     </interval>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle F^{\prime}_{i}=N(\mu^{\prime}_{i},\Sigma^{\prime}_{i})
  </annotation>
 </semantics>
</math>


 then 

<math display="inline" id="Distribution_learning_theory:199">
 <semantics>
  <mrow>
   <mrow>
    <mi>Pr</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mrow>
      <mrow>
       <mi>d</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>F</mi>
        <mo>,</mo>
        <msup>
         <mi>F</mi>
         <mo>′</mo>
        </msup>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>≤</mo>
      <mi>ϵ</mi>
     </mrow>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>≥</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <mi>δ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <geq></geq>
    <apply>
     <ci>Pr</ci>
     <apply>
      <leq></leq>
      <apply>
       <times></times>
       <ci>d</ci>
       <interval closure="open">
        <ci>F</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>F</ci>
         <ci>normal-′</ci>
        </apply>
       </interval>
      </apply>
      <ci>ϵ</ci>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <ci>δ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\Pr[d(F,F^{\prime})\leq\epsilon]\geq 1-\delta
  </annotation>
 </semantics>
</math>

. The sample complexity and the running time of this algorithm is 

<math display="inline" id="Distribution_learning_theory:200">
 <semantics>
  <mrow>
   <mtext>poly</mtext>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>n</mi>
    <mo>,</mo>
    <mrow>
     <mn>1</mn>
     <mo>/</mo>
     <mi>ϵ</mi>
    </mrow>
    <mo>,</mo>
    <mrow>
     <mn>1</mn>
     <mo>/</mo>
     <mi>δ</mi>
    </mrow>
    <mo>,</mo>
    <mrow>
     <mn>1</mn>
     <mo>/</mo>
     <msub>
      <mi>w</mi>
      <mn>1</mn>
     </msub>
    </mrow>
    <mo>,</mo>
    <mrow>
     <mn>1</mn>
     <mo>/</mo>
     <msub>
      <mi>w</mi>
      <mn>2</mn>
     </msub>
    </mrow>
    <mo>,</mo>
    <mrow>
     <mrow>
      <mn>1</mn>
      <mo>/</mo>
      <mi>d</mi>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>F</mi>
       <mn>1</mn>
      </msub>
      <mo>,</mo>
      <msub>
       <mi>F</mi>
       <mn>2</mn>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <mtext>poly</mtext>
    <vector>
     <ci>n</ci>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>ϵ</ci>
     </apply>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>δ</ci>
     </apply>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>d</ci>
      </apply>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>F</ci>
        <cn type="integer">1</cn>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>F</ci>
        <cn type="integer">2</cn>
       </apply>
      </interval>
     </apply>
    </vector>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle\text{poly}(n,1/\epsilon,1/\delta,1/w_{1},1/w_{2},1/d(F_{1},F_{2}))
  </annotation>
 </semantics>
</math>

.</em></p>

<p>It is very interesting in the above result that the distance between 

<math display="inline" id="Distribution_learning_theory:201">
 <semantics>
  <msub>
   <mi>F</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>F</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle F_{1}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Distribution_learning_theory:202">
 <semantics>
  <msub>
   <mi>F</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>F</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle F_{2}
  </annotation>
 </semantics>
</math>

 doesn't affect the quality of the result of the algorithm but just the sample complexity and the running time.</p>
<h2 id="references">References</h2>
<references>
</references>

<p>"</p>

<p><a href="Category:Computational_learning_theory" title="wikilink">Category:Computational learning theory</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. Schapire, L. Sellie <em>On the Learnability of Discrete Distributions</em>. ACM Symposium on Theory of Computing, 1994 <a href="http://dl.acm.org/citation.cfm?id=195155">1</a><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="http://dl.acm.org/citation.cfm?id=1972">L. Valiant <em>A theory of the learnable</em>. Communications of ACM, 1984</a><a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4">Lorenzo Rosasco, Tomaso Poggio, "A Regularization Tour of Machine Learning — MIT-9.520 Lectures Notes" Manuscript, Dec. 2014 <a href="http://www.mit.edu/~9.520/fall14/">2</a><a href="#fnref4">↩</a></li>
<li id="fn5"></li>
<li id="fn6"></li>
<li id="fn7">C. Daskalakis, G. Kamath <em>Faster and Sample Near-Optimal Algorithms for Proper Learning Mixtures of Gaussians</em>. Annual Conference on Learning Theory, 2014 <a href="http://arxiv.org/abs/1312.1054">3</a><a href="#fnref7">↩</a></li>
<li id="fn8">C. Daskalakis, I. Diakonikolas, R. Servedio <em>Learning Poisson Binomial Distributions</em>. ACM Symposium on Theory of Computing, 2012 <a href="http://dl.acm.org/citation.cfm?id=2214042">4</a><a href="#fnref8">↩</a></li>
<li id="fn9">C. Daskalakis, C. Papadimitriou <em>Sparse Covers for Sums of Indicators</em>. Probability Theory and Related Fields, 2014 <a href="http://link.springer.com/article/10.1007%2Fs00440-014-0582-8">5</a><a href="#fnref9">↩</a></li>
<li id="fn10">C. Daskalakis, I. Diakonikolas, R. O’Donnell, R. Servedio, L. Tan <em>Learning Sums of Independent Integer Random Variables</em>. IEEE Symposium on Foundations of Computer Science, 2013 <a href="http://dl.acm.org/citation.cfm?id=2570450.2570592">6</a><a href="#fnref10">↩</a></li>
<li id="fn11"></li>
<li id="fn12">A. Kalai, A. Moitra, G. Valiant <em>Efficiently Learning Mixtures of Two Gaussians</em> ACM Symposium on Theory of Computing, 2010 <a href="http://dl.acm.org/citation.cfm?id=1806765">7</a><a href="#fnref12">↩</a></li>
<li id="fn13">K. Pearson <em>Contribution to the Mathematical Theory of Evolution</em>. Philosophical Transaction of the Royal Society in London, 1894 [<a class="uri" href="http://www.jstor.org/discover/10.2307/90649?sid=21104909080371&amp;uid">http://www.jstor.org/discover/10.2307/90649?sid=21104909080371&amp;uid;</a>;=4&amp;uid;=2]<a href="#fnref13">↩</a></li>
<li id="fn14">S. Dasgupta <em>Learning Mixtures of Gaussians</em>. IEEE Symposium on Foundations of Computer Science, 1999 <a href="http://dl.acm.org/citation.cfm?id=796496">8</a><a href="#fnref14">↩</a></li>
<li id="fn15"></li>
<li id="fn16"></li>
<li id="fn17"></li>
</ol>
</section>
</body>
</html>
