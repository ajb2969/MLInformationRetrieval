   Background subtraction      Background subtraction  '''Background subtraction''', also known as Foreground Detection, is a technique in the fields of [[image processing]] and [[computer vision]] wherein an image's foreground is extracted for further processing (object recognition etc.). Generally an image's regions of interest are objects (humans, cars, text etc.) in its foreground. After the stage of image preprocessing (which may include [[image denoising]], post process ing like morphology etc.) object localisation is required which may make use of this technique. Background subtraction is a widely used approach for detecting moving objects in videos from static cameras. The rationale in the approach is that of detecting the moving objects from the difference between the current frame and a reference frame, often called “background image”, or “background model”. Background subtraction is mostly done if the image in question is a part of a video stream. Background subtraction provides important cues for numerous applications in computer vision, for example surveillance tracking or human poses estimation. However, background subtraction is generally based on a static background hypothesis which is often not applicable in real environments. With indoor scenes, reflections or animated images on screens lead to background changes. In a same way, due to wind, rain or illumination changes brought by weather, static backgrounds methods have difficulties with outdoor scenes. 1  Conventional Approaches  A robust background subtraction algorithm should be able to handle lighting changes, repetitive motions from clutter and long-term scene changes. 2 The following analyses make use of the function of V ( x , y , t ) as a video sequence where t is the time dimension, x and y are the pixel location variables. e.g. V (1,2,3) is the pixel intensity at (1,2) pixel location of the image at t = 3 in the video sequence.  Using frame differencing  A motion detection algorithm begins with the segmentation part where foreground or moving objects are segmented from the background. The simplest way to implement this is to take an image as background and take the frames obtained at the time t, denoted by I(t) to compare with the background image denoted by B. Here using simple arithmetic calculations, we can segment out the objects simply by using image subtraction technique of computer vision meaning for each pixels in I(t), take the pixel value denoted by P[I(t)] and subtract it with the corresponding pixels at the same position on the background image denoted as P[B].  In mathematical equation, it is written as:       P   [   F   (  t  )    ]    =    P   [   I   (  t  )    ]    -   P   [  B  ]           P   delimited-[]    F  t         P   delimited-[]    I  t       P   delimited-[]  B       P[F(t)]=P[I(t)]-P[B]\,     The background is assumed to be the frame at time t . This difference image would only show some intensity for the pixel locations which have changed in the two frames. Though we have seemingly removed the background, this approach will only work for cases where all foreground pixels are moving and all background pixels are static. 3  4 A threshold "Threshold" is put on this difference image to improve the subtraction (see Image thresholding ).       |    P   [   F   (  t  )    ]    -   P   [   F   (   t  +  1   )    ]     |   >   Threshold             P   delimited-[]    F  t       P   delimited-[]    F    t  1        Threshold    |P[F(t)]-P[F(t+1)]|>\mathrm{Threshold}\,     This means that the difference image's pixels' intensities are 'thresholded' or filtered on the basis of value of Threshold. 5 The accuracy of this approach is dependent on speed of movement in the scene. Faster movements may require higher thresholds.  Mean filter  For calculating the image containing only the background, a series of preceding images are averaged. For calculating the background image at the instant t ,       B   (  x  ,  y  )    =    1  N     ∑   i  =  1   N    V   (  x  ,  y  ,   t  -  i   )            B   x  y        1  N     superscript   subscript     i  1    N     V   x  y    t  i         B(x,y)={1\over N}\sum_{i=1}^{N}V(x,y,t-i)     where N is the number of preceding images taken for averaging. This averaging refers to averaging corresponding pixels in the given images. N would depend on the video speed (number of images per second in the video) and the amount of movement in the video. 6 After calculating the background B(x,y) we can then subtract it from the image V(x,y,t) at time t =t and threshold it. Thus the foreground is       |    V   (  x  ,  y  ,  t  )    -   B   (  x  ,  y  )     |   >   Th             V   x  y  t      B   x  y      Th    |V(x,y,t)-B(x,y)|>\mathrm{Th}\,     where Th is threshold. Similarly we can also use median instead of mean in the above calculation of B ( x , y ).  Usage of global and time-independent Thresholds (same Th value for all pixels in the image) may limit the accuracy of the above two approaches. 7  Running Gaussian average  For this method, Wren et al. 8 propose fitting a Gaussian probabilistic density function (pdf) on the most recent   n   n   n   frames. In order to avoid fitting the pdf from scratch at each new frame time   t   t   t   , a running (or on-line cumulative) average is computed.  The pdf of every pixel is characterized by mean     μ  t     subscript  μ  t    \mu_{t}   and variance      σ  t  2     subscript   superscript  σ  2   t    \sigma^{2}_{t}    . The following is a possible initial condition (assuming that initially every pixel is background):       μ  0   =   I  0        subscript  μ  0    subscript  I  0     \mu_{0}=I_{0}        σ  0  2   =  <       subscript   superscript  σ  2   0      \sigma^{2}_{0}=<   some default value   >     >     where     I  t     subscript  I  t    I_{t}    is the value of the pixel's intensity at time   t   t   t   . In order to initialize variance, we can, for example, use the variance in x and y from a small window around each pixel.  Note that background may change over time (e.g. due to illumination changes or non-static background objects). To accommodate for that change, at every frame   t   t   t   , every pixel's mean and variance must be updated, as follows:       μ  t   =    ρ   I  t    +    (   1  -  ρ   )    μ   t  -  1           subscript  μ  t       ρ   subscript  I  t        1  ρ    subscript  μ    t  1        \mu_{t}=\rho I_{t}+(1-\rho)\mu_{t-1}        σ  t  2   =     d  2   ρ   +    (   1  -  ρ   )    σ   t  -  1   2          subscript   superscript  σ  2   t        superscript  d  2   ρ       1  ρ    subscript   superscript  σ  2     t  1        \sigma^{2}_{t}=d^{2}\rho+(1-\rho)\sigma^{2}_{t-1}       d  =   |   (    I  t   -   μ  t    )   |       d       subscript  I  t    subscript  μ  t       d=|(I_{t}-\mu_{t})|     Where   ρ   ρ   \rho   determines the size of the temporal window that is used to fit the pdf (usually    ρ  =  0.01      ρ  0.01    \rho=0.01   ) and    d   d   d    is the Euclidean distance between the mean and the value of the pixel.  (Figure)  Gaussian distribution for each pixel.   We can now classify a pixel as background if its current intensity lies within some confidence interval of its distribution's mean:        |   (    I  t   -   μ  t    )   |    σ  t    >  k  ⟶  𝐹𝑜𝑟𝑒𝑔𝑟𝑜𝑢𝑛𝑑               subscript  I  t    subscript  μ  t      subscript  σ  t    k    normal-⟶    𝐹𝑜𝑟𝑒𝑔𝑟𝑜𝑢𝑛𝑑     \frac{|(I_{t}-\mu_{t})|}{\sigma_{t}}>k\longrightarrow\mathit{Foreground}         |   (    I  t   -   μ  t    )   |    σ  t    ≤  k  ⟶  𝐵𝑎𝑐𝑘𝑔𝑟𝑜𝑢𝑛𝑑               subscript  I  t    subscript  μ  t      subscript  σ  t    k    normal-⟶    𝐵𝑎𝑐𝑘𝑔𝑟𝑜𝑢𝑛𝑑     \frac{|(I_{t}-\mu_{t})|}{\sigma_{t}}\leq k\longrightarrow\mathit{Background}     where the parameter    k   k   k    is a free threshold (usually     k  =  2.5      k  2.5    k=2.5    ). A larger value for   k   k   k   allows for more dynamic background, while a smaller   k   k   k   increases the probability of a transition from background to foreground due to more subtle changes.  In a variant of the method, a pixel's distribution is only updated if it is classified as background. This is to prevent newly introduced foreground objects from fading into the background. The update formula for the mean is changed accordingly:       μ  t   =    M   μ   t  -  1     +    (   1  -  M   )    (     I  t   ρ   +    (   1  -  ρ   )    μ   t  -  1      )          subscript  μ  t       M   subscript  μ    t  1         1  M        subscript  I  t   ρ       1  ρ    subscript  μ    t  1          \mu_{t}=M\mu_{t-1}+(1-M)(I_{t}\rho+(1-\rho)\mu_{t-1})     where     M  =  1      M  1    M=1    when    I  t     subscript  I  t    I_{t}   is considered foreground and     M  =  0      M  0    M=0    otherwise. So when     M  =  1      M  1    M=1    , that is, when the pixel is detected as foreground, the mean will stay the same. As a result, a pixel, once it has become foreground, can only become background again when the intensity value gets close to what it was before turning foreground. This method, however, has several issues: It only works if all pixels are initially background pixels (or foreground pixels are annotated as such). Also, it cannot cope with gradual background changes: If a pixel is categorized as foreground for a too long period of time, the background intensity in that location might have changed (because illumination has changed etc.). As a result, once the foreground object is gone, the new background intensity might not be recognized as such anymore.  Background mixture models  In this technique, it is assumed that every pixel's intensity values in the video can be modeled using a Gaussian mixture model . 9 A simple heuristic determines which intensities are most probably of the background. Then the pixels which do not match to these are called the foreground pixels. Foreground pixels are grouped using 2D connected component analysis. 10  At any time t, a particular pixel (     x  0   ,   y  0       subscript  x  0    subscript  y  0     x_{0},y_{0}   )'s history is        X  1   ,  …  ,   X  t    =   {   V   (   x  0   ,   y  0   ,  i  )    :   1  ⩽  i  ⩽  t   }         subscript  X  1   normal-…   subscript  X  t     conditional-set    V    subscript  x  0    subscript  y  0   i        1  i       t       X_{1},\ldots,X_{t}=\{V(x_{0},y_{0},i):1\leqslant i\leqslant t\}\,     This history is modeled by a mixture of K Gaussian distributions:      P   (   X  t   )   =   ∑   i  =  1   K    ω   i  ,  t    N   (   X  t   ∣   μ   i  ,  t    ,   Σ   i  ,  t    )      fragments  P   fragments  normal-(   subscript  X  t   normal-)     superscript   subscript     i  1    K    subscript  ω   i  t    N   fragments  normal-(   subscript  X  t   normal-∣   subscript  μ   i  t    normal-,   subscript  normal-Σ   i  t    normal-)     P(X_{t})=\sum_{i=1}^{K}\omega_{i,t}N\left(X_{t}\mid\mu_{i,t},\Sigma_{i,t}\right)     where      N   (   X  t   ∣   μ   i  t    ,   Σ   i  ,  t    )   =   1    (   2  π   )    D  /  2      1    |   Σ   i  ,  t    |    1  /  2     exp   (  -   1  2     (   X  t   -   μ   i  ,  t    )   T    Σ   i  ,  t    -  1     (   X  t   -   μ   i  ,  t    )   )      fragments  N   fragments  normal-(   subscript  X  t   normal-∣   subscript  μ    i  t    normal-,   subscript  normal-Σ   i  t    normal-)      1   superscript    2  π     D  2       1   superscript     subscript  normal-Σ   i  t       1  2       fragments  normal-(     1  2    superscript   fragments  normal-(   subscript  X  t     subscript  μ   i  t    normal-)   T    superscript   subscript  normal-Σ   i  t      1     fragments  normal-(   subscript  X  t     subscript  μ   i  t    normal-)   normal-)     N\left(X_{t}\mid\mu_{it},\Sigma_{i,t}\right)=\dfrac{1}{(2\pi)^{D/2}}{1\over|%
 \Sigma_{i,t}|^{1/2}}\exp\left(-{1\over 2}(X_{t}-\mu_{i,t})^{T}\Sigma_{i,t}^{-1%
 }\left(X_{t}-\mu_{i,t}\right)\right)     An on-line K-means approximation is used to update the Gaussians. Numerous improvements of this original method developed by Stauffer and Grimson 11 have been proposed and a complete survey can be found in Bouwmans et al. 12  Surveys  Several surveys which concern categories or sub-categories of models can be found as follows:   MOG Background Subtraction   First, each pixel is characterized by its intensity in RGB color space. Then probability of observing the current pixel is given by the following formula in the multidimensional case       P   (   X  t   )    =    ∑   i  =  1   K     ω   i  ,  t    η   (     X  t     μ   i  ,  t     ,   Σ   i  ,  t    )           P   subscript  X  t      superscript   subscript     i  1    K      subscript  ω   i  t    η      subscript  X  t    subscript  μ   i  t      subscript  normal-Σ   i  t         P(X_{t})=\sum_{i=1}^{K}\omega_{i,t}\eta\left(X_{t}\,\mu_{i,t},\Sigma_{i,t}\right)     Where the parameters are K is the number of distributions, ω is a weight associated to the ith Gaussian at time t with mean µ and standard deviation Σ .       η   (     X  t     μ   i  ,  t     ,   Σ   i  ,  t    )    =    1     (    2  /  p   i   )    n  /  2     Σ   i  ,  t   0.5      exp   (   -    1  2    (    X  t   -   μ   i  ,  t     )    Σ   i  ,  t     (    X  t   -   μ   i  ,  t     )     )           η      subscript  X  t    subscript  μ   i  t      subscript  normal-Σ   i  t          1     superscript      2  p   i     n  2     superscript   subscript  normal-Σ   i  t    0.5             1  2      subscript  X  t    subscript  μ   i  t      subscript  normal-Σ   i  t       subscript  X  t    subscript  μ   i  t           \eta\left(X_{t}\,\mu_{i,t},\Sigma_{i,t}\right)=\dfrac{1}{(2/pi)^{n/2}\Sigma_{i%
 ,t}^{0.5}}\exp\left(-{1\over 2}(X_{t}-\mu_{i,t})\Sigma_{i,t}\left(X_{t}-\mu_{i%
 ,t}\right)\right)     Once the parameters initialization is made, a first foreground detection can be made then the parameters are updated. The first B Gaussian distribution which exceeds the threshold T is re-tained for a background distribution      B  =  a  r  g  m  i  n   (   Σ   i  -  1   B    ω   i  ,  t    >  T  )      fragments  B   a  r  g  m  i  n   fragments  normal-(   superscript   subscript  normal-Σ    i  1    B    subscript  ω   i  t     T  normal-)     B=argmin\left(\Sigma_{i-1}^{B}\omega_{i,t}>T\right)     The other distributions are considered to represent a foreground distribution. Then, when the new frame incomes at times    t  +  1      t  1    t+1   , a match test is made of each pixel. A pixel matches a Gaussian distribution if the Mahalanobis distance       σ   i  ,   t  +  1     =     (   1  -  ρ   )    σ   i  ,  t   2    +   ρ   (    X   x  +  1    -   μ   x  +  1     )     (    X   x  +  1    -   μ   x  +  1     )   T          subscript  σ   i    t  1           1  ρ    superscript   subscript  σ   i  t    2      ρ     subscript  X    x  1     subscript  μ    x  1      superscript     subscript  X    x  1     subscript  μ    x  1     T       \sigma_{i,t+1}=\left(1-\rho\right)\sigma_{i,t}^{2}+\rho\left(X_{x+1}-\mu_{x+1}%
 \right)\left(X_{x+1}-\mu_{x+1}\right)^{T}   .Then, two cases can occur:  Case 1: A match is found with one of the K Gaussians. For the matched component, the update is done as follows 13       σ   i  ,   t  +  1     =   (  1  -  α  )    ω   i  ,  t    +  α  P   (  k  ∣   X  t   ,  ϕ  )      fragments   subscript  σ   i    t  1       fragments  normal-(  1   α  normal-)    subscript  ω   i  t     α  P   fragments  normal-(  k  normal-∣   subscript  X  t   normal-,  ϕ  normal-)     \sigma_{i,t+1}=\left(1-\alpha\right)\omega_{i,t}+\alpha P\left(k\mid\ X_{t},%
 \phi\right)     Power and Schoonees [3] used the same algorithm to segment the foreground of the image      P   (  k  ∣   X  t   ,  ϕ  )      fragments  P   fragments  normal-(  k  normal-∣   subscript  X  t   normal-,  ϕ  normal-)     P\left(k\mid\ X_{t},\phi\right)     The essential approximation to     M  (   k  ,  t  )     fragments   subscript  M  normal-(   k  normal-,  t  normal-)    M_{(}k,t)   is given by      M  (   k  ,  t  )   =  1   (  m  a  t  c  h  )   ,   M  (   k  ,  t  )  =  0  (  o  t  h  e  r  w  i  s  e  )     fragments   fragments   subscript  M  normal-(   k  normal-,  t  normal-)    1   fragments  normal-(  m  a  t  c  h  normal-)   normal-,   subscript  M  normal-(   k  normal-,  t  normal-)   0  normal-(  o  t  h  e  r  w  i  s  e  normal-)    M_{(}k,t)=1\left(match\right),M_{(}k,t)=0\left(otherwise\right)    14     K   K   K     Case 2: No match is found with any of the   K   K   K   Gaussians. In this case, the least probable distribu-tion     k   i  .  t    =   l  o  w  P  r  i  o  r  W  e  i  g  h  t        subscript  k   formulae-sequence  i  t      l  o  w  P  r  i  o  r  W  e  i  g  h  t     k_{i.t}=lowPriorWeight   is replaced with a new one with parameters       μ   i  ,   t  +  1     =   X   t  +  1         subscript  μ   i    t  1      subscript  X    t  1      \mu_{i,t+1}=X_{t+1}          k   i  .   t  +  1     =   L  a  r  g  e  I  n  i  t  i  a  l  W  e  i  g  h  t        subscript  k   formulae-sequence  i    t  1       L  a  r  g  e  I  n  i  t  i  a  l  W  e  i  g  h  t     k_{i.t+1}=LargeInitialWeight     $$k_{i.t+1}= Large Initial Weight$$  Once the parameter maintenance is made, foreground detection can be made and so on. 15  Relative OPEN CV background /foreground segmentation algorithm can be found in the link below:  http://docs.opencv.org/trunk/doc/py_tutorials/py_video/py_bg_subtraction/py_bg_subtraction.html   Subspace Learning Background Subtraction 16    Statistical Background Subtraction 17 18    Fuzzy Background Subtraction 19    RPCA Background Subtraction 20 21 (See Robust principal component analysis for more details)    Traditional and Recent Approaches for Background Subtraction 22 23   Books, Journals and Workshops  Books  T. Bouwmans, F. Porikli, B. Horferlin, A. Vacavant, Handbook on "Background Modeling and Foreground Detection for Video Surveillance: Traditional and Recent Approaches, Implementations, Benchmarking and Evaluation", CRC Press, Taylor and Francis Group, June 2014. (For more information: http://www.crcpress.com/product/isbn/9781482205374 )  Journals   T. Bouwmans, L. Davis, J. Gonzalez, M. Piccardi, C. Shan, Special Issue on "Background Modeling for Foreground Detection in Real-World Dynamic Scenes", Special Issue in Machine Vision and Applications, July 2014.  A. Vacavant, L. Tougne, T. Chateau, "Special section on background models comparison", Computer Vision and Image Understanding, CVIU 2014, May 2014.   Workshops   Scene Background Modeling and Initialization (SBMI 2015) Workshop in conjunction with ICIAP 2015.  IEEE on Change Detection Workshop in conjunction with CVPR 2014.  Workshop on Background Model Challenges (BMC 2012) in conjunction with ACCV 2012.   Resources, Datasets and Codes  BGS Web Site  The Background Subtraction Web Site (T. Bouwmans, Univ. La Rochelle, France) contains a full list of the references in the field, links to available datasets and codes. (For more information: http://sites.google.com/site/backgroundsubtraction/overview )  BGS Datasets   ChangeDetection.net (For more information: http://www.changedetection.net/ )  Background Models Challenge (For more information: http://bmc.univ-bpclermont.fr/ )  Stuttgart Artificial Background Subtraction Dataset (For more information: http://www.vis.uni-stuttgart.de/index.php?id=sabs )  SBMI dataset (For more information: http://sbmi2015.na.icar.cnr.it/ )   BGS Libraries   BGS Library   The BGS Library (A. Sobral, Univ. La Rochelle, France) provides a C++ framework to perform background subtraction algorithms. The code works either on Windows or on Linux. Currently the library offers 29 BGS algorithms. (For more information: http://github.com/andrewssobral/bgslibrary )   LRS Library - Low-Rank and Sparse tools for Background Modeling and Subtraction in Videos   The LRSLibrary (A. Sobral, Univ. La Rochelle, France) provides a collection of low-rank and sparse decomposition algorithms in MATLAB. The library was designed for motion segmentation in videos, but it can be also used or adapted for other computer vision problems. Currently the LRSLibrary contains a total of 64 matrix-based and tensor-based algorithms. The LRSLibrary was tested successfully in MATLAB R2013b both x86 and x64 versions. (For more information: https://github.com/andrewssobral/lrslibrary#lrslibrary )  Applications   Video Surveillance  Optical Motion Capture  Human Computer Interaction  Content based Video Coding   See also   ViBe  PBAS  SOBS   References     "  Category:Articles created via the Article Wizard  Category:Mathematical examples  Category:Image processing  Category:Computer vision     ↩  ↩   ↩  ↩  ↩   ↩  ↩   ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩     