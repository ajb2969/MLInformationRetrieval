   Cauchy–Schwarz inequality      Cauchy–Schwarz inequality   In mathematics , the Cauchy–Schwarz inequality is a useful inequality encountered in many different settings, such as linear algebra , analysis , probability theory , and other areas. It is considered to be one of the most important inequalities in all of mathematics. 1 It has a number of generalizations, among them Hölder's inequality .  The inequality for sums was published by , while the corresponding inequality for integrals was first proved by . The modern proof of the integral inequality was given by . 2  Statement of the inequality  The Cauchy–Schwarz inequality states that for all vectors x and y of an inner product space it is true that         |   ⟨  x  ,  y  ⟩   |   2   ≤    ⟨  x  ,  x  ⟩   ⋅   ⟨  y  ,  y  ⟩     ,       superscript     x  y    2    normal-⋅   x  x    y  y      |\langle x,y\rangle|^{2}\leq\langle x,x\rangle\cdot\langle y,y\rangle,     where    ⟨  ⋅  ,  ⋅  ⟩     normal-⋅  normal-⋅    \langle\cdot,\cdot\rangle   is the inner product also known as dot product. Equivalently, by taking the square root of both sides, and referring to the norms of the vectors, the inequality is written as        |   ⟨  x  ,  y  ⟩   |   ≤    ∥  x  ∥   ⋅   ∥  y  ∥     .         x  y     normal-⋅   norm  x    norm  y      |\langle x,y\rangle|\leq\|x\|\cdot\|y\|.\,    3  Moreover, the two sides are equal if and only if x and y are linearly dependent (or, in a geometrical sense, they are parallel or one of the vectors' magnitude is zero).  If      x  1   ,  …  ,   x  n    ∈  ℂ        subscript  x  1   normal-…   subscript  x  n    ℂ    x_{1},\ldots,x_{n}\in\mathbb{C}   and      y  1   ,  …  ,   y  n    ∈  ℂ        subscript  y  1   normal-…   subscript  y  n    ℂ    y_{1},\ldots,y_{n}\in\mathbb{C}   have an imaginary component, the inner product is the standard inner product and the bar notation is used for complex conjugation then the inequality may be restated more explicitly as         |     x  1     y  ¯   1    +  ⋯  +    x  n     y  ¯   n     |   2   ≤    (     |   x  1   |   2   +  ⋯  +    |   x  n   |   2    )    (     |   y  1   |   2   +  ⋯  +    |   y  n   |   2    )     .       superscript         subscript  x  1    subscript   normal-¯  y   1    normal-⋯     subscript  x  n    subscript   normal-¯  y   n      2        superscript     subscript  x  1    2   normal-⋯   superscript     subscript  x  n    2       superscript     subscript  y  1    2   normal-⋯   superscript     subscript  y  n    2       |x_{1}\bar{y}_{1}+\cdots+x_{n}\bar{y}_{n}|^{2}\leq(|x_{1}|^{2}+\cdots+|x_{n}|^%
 {2})(|y_{1}|^{2}+\cdots+|y_{n}|^{2}).     When viewed in this way the numbers x 1 , ..., x n , and y 1 , ..., y n are the components of x and y with respect to an orthonormal basis of V .  Even more compactly written:         |    ∑   i  =  1   n     x  i     y  ¯   i     |   2   ≤    ∑   j  =  1   n      |   x  j   |   2     ∑   k  =  1   n     |   y  k   |   2       .       superscript      superscript   subscript     i  1    n      subscript  x  i    subscript   normal-¯  y   i      2     superscript   subscript     j  1    n      superscript     subscript  x  j    2     superscript   subscript     k  1    n    superscript     subscript  y  k    2        \left|\sum_{i=1}^{n}x_{i}\bar{y}_{i}\right|^{2}\leq\sum_{j=1}^{n}|x_{j}|^{2}%
 \sum_{k=1}^{n}|y_{k}|^{2}.     Equality holds if and only if x and y are linearly dependent , that is, one is a scalar multiple of the other (which includes the case when one or both are zero).  The finite-dimensional case of this inequality for real vectors was proven by Cauchy in 1821, and in 1859 Cauchy's student Bunyakovsky noted that by taking limits one can obtain an integral form of Cauchy's inequality. The general result for an inner product space was obtained by Schwarz in the year 1888.  Proof  Let u , v be arbitrary vectors in a vector space V over F with an inner product, where F is the field of real or complex numbers. We prove the inequality        |   ⟨  u  ,  v  ⟩   |   ≤    ∥  u  ∥    ∥  v  ∥     ,         u  v       norm  u    norm  v      \big|\langle u,v\rangle\big|\leq\left\|u\right\|\left\|v\right\|,     and the equality holds only when either u or v is a multiple of the other.  If v = 0 it is clear that we have equality, and in this case u and v are also linearly dependent (regardless of u ). We henceforth assume that v is nonzero. We also assume that     ⟨  u  ,  v  ⟩   ≠  0       u  v   0    \langle u,v\rangle\neq 0   otherwise the inequality is obviously true, because neither    ∥  u  ∥     norm  u    \left\|u\right\|   nor    ∥  v  ∥     norm  v    \left\|v\right\|   can be negative.  Let       z  =   u  -     ⟨  u  ,  v  ⟩    ⟨  v  ,  v  ⟩    v     .      z    u       u  v    v  v    v      z=u-\frac{\langle u,v\rangle}{\langle v,v\rangle}v.   Then, by linearity of the inner product in its first argument, one has        ⟨  z  ,  v  ⟩   =   ⟨   u  -     ⟨  u  ,  v  ⟩    ⟨  v  ,  v  ⟩    v    ,  v  ⟩   =    ⟨  u  ,  v  ⟩   -     ⟨  u  ,  v  ⟩    ⟨  v  ,  v  ⟩     ⟨  v  ,  v  ⟩     =  0   ,         z  v      u       u  v    v  v    v    v           u  v        u  v    v  v     v  v          0     \langle z,v\rangle=\left\langle u-\frac{\langle u,v\rangle}{\langle v,v\rangle%
 }v,v\right\rangle=\langle u,v\rangle-\frac{\langle u,v\rangle}{\langle v,v%
 \rangle}\langle v,v\rangle=0,   i.e., z is a vector orthogonal to the vector v (Indeed, z is the projection of u onto the plane orthogonal to v .). We can thus apply the Pythagorean theorem to       u  =      ⟨  u  ,  v  ⟩    ⟨  v  ,  v  ⟩    v   +  z    ,      u         u  v    v  v    v   z     u=\frac{\langle u,v\rangle}{\langle v,v\rangle}v+z,   which gives         ∥  u  ∥   2   =      |    ⟨  u  ,  v  ⟩    ⟨  v  ,  v  ⟩    |   2     ∥  v  ∥   2    +    ∥  z  ∥   2    =      |   ⟨  u  ,  v  ⟩   |   2     ∥  v  ∥   2    +    ∥  z  ∥   2    ≥     |   ⟨  u  ,  v  ⟩   |   2     ∥  v  ∥   2     ,         superscript   norm  u   2        superscript       u  v    v  v     2    superscript   norm  v   2     superscript   norm  z   2              superscript     u  v    2    superscript   norm  v   2     superscript   norm  z   2            superscript     u  v    2    superscript   norm  v   2       \left\|u\right\|^{2}=\left|\frac{\langle u,v\rangle}{\langle v,v\rangle}\right%
 |^{2}\left\|v\right\|^{2}+\left\|z\right\|^{2}=\frac{|\langle u,v\rangle|^{2}}%
 {\left\|v\right\|^{2}}+\left\|z\right\|^{2}\geq\frac{|\langle u,v\rangle|^{2}}%
 {\left\|v\right\|^{2}},   and, after multiplication by     ∥  v  ∥   2     superscript   norm  v   2    \left\|v\right\|^{2}   , the Cauchy–Schwarz inequality. Moreover, if the relation '≥' in the above expression is actually an equality, then      ∥  z  ∥   2   =  0       superscript   norm  z   2   0    \left\|z\right\|^{2}=0   and hence    z  =  0      z  0    z=0   ; the definition of z then establishes a relation of linear dependence between u and v . This establishes the theorem.  Alternative Proof  Let u , v be arbitrary vectors in a vector space V over F with an inner product, where F is the field of real or complex numbers.  If     ⟨  u  ,  v  ⟩   =  0       u  v   0    \langle u,v\rangle=0   , the theorem holds trivially.  If not, then    u  ≠  0      u  0    u\neq 0   ,    v  ≠  0      v  0    v\neq 0   . Choose    λ  =    |   ⟨  u  ,  v  ⟩   |    ⟨  u  ,  v  ⟩        λ       u  v     u  v      \lambda=\frac{|\langle u,v\rangle|}{\langle u,v\rangle}   . Then     |  λ  |   =  1        λ   1    |\lambda|=1   and       0  ≤    ∥     λ  u    ∥  u  ∥    -   v   ∥  v  ∥     ∥   2   =       |  λ  |   2      ∥  u  ∥   2     ∥  u  ∥   2     -   2  Re   (   ⟨    λ  u    ∥  u  ∥    ,   v   ∥  v  ∥    ⟩   )     +     ∥  v  ∥   2     ∥  v  ∥   2     =   2  -   2    λ   ⟨  u  ,  v  ⟩      ∥  u  ∥    ∥  v  ∥        .        0   superscript   norm        λ  u    norm  u      v   norm  v      2               superscript    λ   2      superscript   norm  u   2    superscript   norm  u   2       2  Re       λ  u    norm  u      v   norm  v          superscript   norm  v   2    superscript   norm  v   2            2    2      λ   u  v       norm  u    norm  v          0\leq\left\|\frac{\lambda u}{\|u\|}-\frac{v}{\|v\|}\right\|^{2}=|\lambda|^{2}%
 \frac{\|u\|^{2}}{\|u\|^{2}}-2\text{Re}\left(\left\langle\frac{\lambda u}{\|u\|%
 },\frac{v}{\|v\|}\right\rangle\right)+\frac{\|v\|^{2}}{\|v\|^{2}}=2-2\frac{%
 \lambda\langle u,v\rangle}{\|u\|\|v\|}.   It follows that        |   ⟨  u  ,  v  ⟩   |   =   λ   ⟨  u  ,  v  ⟩    ≤    ∥  u  ∥    ∥  v  ∥     .           u  v      λ   u  v            norm  u    norm  v       |\langle u,v\rangle|=\lambda\langle u,v\rangle\leq\|u\|\|v\|.     Special cases  R n  In Euclidean space     ℝ  n     superscript  ℝ  n    \mathbb{R}^{n}   with the standard inner product, the Cauchy–Schwarz inequality is         (    ∑   i  =  1   n     x  i    y  i     )   2   ≤    (    ∑   i  =  1   n    x  i  2    )    (    ∑   i  =  1   n    y  i  2    )     .       superscript    superscript   subscript     i  1    n      subscript  x  i    subscript  y  i     2       superscript   subscript     i  1    n    superscript   subscript  x  i   2      superscript   subscript     i  1    n    superscript   subscript  y  i   2       \left(\sum_{i=1}^{n}x_{i}y_{i}\right)^{2}\leq\left(\sum_{i=1}^{n}x_{i}^{2}%
 \right)\left(\sum_{i=1}^{n}y_{i}^{2}\right).     To prove this form of the inequality, consider the following quadratic polynomial in z .         (     x  1   z   +   y  1    )   2   +  ⋯  +    (     x  n   z   +   y  n    )   2    =     (   ∑   (   x  i  2   )    )   ⋅   z  2    +   2  ⋅   (   ∑   (    x  i   ⋅   y  i    )    )   ⋅  z   +   ∑   (   y  i  2   )            superscript       subscript  x  1   z    subscript  y  1    2   normal-⋯   superscript       subscript  x  n   z    subscript  y  n    2       normal-⋅     superscript   subscript  x  i   2     superscript  z  2     normal-⋅  2     normal-⋅   subscript  x  i    subscript  y  i     z      superscript   subscript  y  i   2       (x_{1}z+y_{1})^{2}+\cdots+(x_{n}z+y_{n})^{2}=\left(\sum(x_{i}^{2})\right)\cdot
 z%
 ^{2}+2\cdot\left(\sum(x_{i}\cdot y_{i})\right)\cdot z+\sum(y_{i}^{2})     Since it is nonnegative it has at most one real root in z , whence its discriminant is less than or equal to zero, that is,          (   ∑   (    x  i   ⋅   y  i    )    )   2   -   ∑    x  i  2   ⋅   ∑   y  i  2       ≤  0   ,         superscript     normal-⋅   subscript  x  i    subscript  y  i     2      normal-⋅   superscript   subscript  x  i   2      superscript   subscript  y  i   2       0    \left(\sum(x_{i}\cdot y_{i})\right)^{2}-\sum{x_{i}^{2}}\cdot\sum{y_{i}^{2}}%
 \leq 0,   which yields the Cauchy–Schwarz inequality.  An equivalent proof for    ℝ  n     superscript  ℝ  n    \mathbb{R}^{n}   starts with the summation below.  Expanding the brackets we have:         ∑   i  =  1   n     ∑   j  =  1   n     (     x  i    y  j    -    x  j    y  i     )   2     =      ∑   i  =  1   n     x  i  2     ∑   j  =  1   n    y  j  2      +    ∑   j  =  1   n     x  j  2     ∑   i  =  1   n    y  i  2       -   2    ∑   i  =  1   n     x  i    y  i     ∑   j  =  1   n     x  j    y  j          ,        superscript   subscript     i  1    n     superscript   subscript     j  1    n    superscript       subscript  x  i    subscript  y  j       subscript  x  j    subscript  y  i     2           superscript   subscript     i  1    n      superscript   subscript  x  i   2     superscript   subscript     j  1    n    superscript   subscript  y  j   2        superscript   subscript     j  1    n      superscript   subscript  x  j   2     superscript   subscript     i  1    n    superscript   subscript  y  i   2         2    superscript   subscript     i  1    n      subscript  x  i    subscript  y  i     superscript   subscript     j  1    n      subscript  x  j    subscript  y  j           \sum_{i=1}^{n}\sum_{j=1}^{n}\left(x_{i}y_{j}-x_{j}y_{i}\right)^{2}=\sum_{i=1}^%
 {n}x_{i}^{2}\sum_{j=1}^{n}y_{j}^{2}+\sum_{j=1}^{n}x_{j}^{2}\sum_{i=1}^{n}y_{i}%
 ^{2}-2\sum_{i=1}^{n}x_{i}y_{i}\sum_{j=1}^{n}x_{j}y_{j},     collecting together identical terms (albeit with different summation indices) we find:         1  2     ∑   i  =  1   n     ∑   j  =  1   n     (     x  i    y  j    -    x  j    y  i     )   2      =     ∑   i  =  1   n     x  i  2     ∑   i  =  1   n    y  i  2      -    (    ∑   i  =  1   n     x  i    y  i     )   2     .          1  2     superscript   subscript     i  1    n     superscript   subscript     j  1    n    superscript       subscript  x  i    subscript  y  j       subscript  x  j    subscript  y  i     2          superscript   subscript     i  1    n      superscript   subscript  x  i   2     superscript   subscript     i  1    n    superscript   subscript  y  i   2       superscript    superscript   subscript     i  1    n      subscript  x  i    subscript  y  i     2      \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\left(x_{i}y_{j}-x_{j}y_{i}\right)^{2}=%
 \sum_{i=1}^{n}x_{i}^{2}\sum_{i=1}^{n}y_{i}^{2}-\left(\sum_{i=1}^{n}x_{i}y_{i}%
 \right)^{2}.     Because the left-hand side of the equation is a sum of the squares of real numbers it is greater than or equal to zero, thus:         ∑   i  =  1   n     x  i  2     ∑   i  =  1   n    y  i  2      -    (    ∑   i  =  1   n     x  i    y  i     )   2    ≥  0.          superscript   subscript     i  1    n      superscript   subscript  x  i   2     superscript   subscript     i  1    n    superscript   subscript  y  i   2       superscript    superscript   subscript     i  1    n      subscript  x  i    subscript  y  i     2    0.    \sum_{i=1}^{n}x_{i}^{2}\sum_{i=1}^{n}y_{i}^{2}-\left(\sum_{i=1}^{n}x_{i}y_{i}%
 \right)^{2}\geq 0.     Yet another approach when n ≥ 2 ( n = 1 is trivial) is to consider the plane containing x and y . More precisely, recoordinatize R n with any orthonormal basis whose first two vectors span a subspace containing x and y . In this basis only     x  1   ,   x  2   ,   y  1       subscript  x  1    subscript  x  2    subscript  y  1     x_{1},~{}x_{2},~{}y_{1}   and     y  2      subscript  y  2    y_{2}~{}   are nonzero, and the inequality reduces to the algebra of dot product in the plane, which is related to the angle between two vectors, from which we obtain the inequality:        |   x  ⋅  y   |   =    ∥  x  ∥    ∥  y  ∥    |   cos  θ   |    ≤    ∥  x  ∥    ∥  y  ∥     .           normal-⋅  x  y       norm  x    norm  y       θ             norm  x    norm  y       |x\cdot y|=\|x\|\|y\||\cos\theta|\leq\|x\|\|y\|.     When n = 3 the Cauchy–Schwarz inequality can also be deduced from Lagrange's identity , which takes the form        ⟨  x  ,  x  ⟩   ⋅   ⟨  y  ,  y  ⟩    =     |   ⟨  x  ,  y  ⟩   |   2   +    |   x  ×  y   |   2         normal-⋅   x  x    y  y       superscript     x  y    2    superscript      x  y    2      \langle x,x\rangle\cdot\langle y,y\rangle=|\langle x,y\rangle|^{2}+|x\times y|%
 ^{2}     from which readily follows the Cauchy–Schwarz inequality.  Another proof of the general case for n can be done by using the technique used to prove Inequality of arithmetic and geometric means .  L 2  For the inner product space of square-integrable complex-valued functions , one has         |    ∫   ℝ  n     f   (  x  )      g   (  x  )    ¯    d  x    |   2   ≤    ∫   ℝ  n         |   f   (  x  )    |   2    d  x   ⋅    ∫   ℝ  n        |   g   (  x  )    |   2    d  x       .       superscript      subscript    superscript  ℝ  n      f  x   normal-¯    g  x    d  x     2     subscript    superscript  ℝ  n     normal-⋅     superscript      f  x    2   d  x     subscript    superscript  ℝ  n       superscript      g  x    2   d  x        \left|\int_{\mathbb{R}^{n}}f(x)\overline{g(x)}\,dx\right|^{2}\leq\int_{\mathbb%
 {R}^{n}}\left|f(x)\right|^{2}\,dx\cdot\int_{\mathbb{R}^{n}}\left|g(x)\right|^{%
 2}\,dx.     A generalization of this is the Hölder inequality .  Applications  The triangle inequality for the standard norm is often shown as a consequence of the Cauchy–Schwarz inequality, as follows: given vectors x and y :       ∥   x  +  y   ∥   2     superscript   norm    x  y    2    \displaystyle\|x+y\|^{2}     Taking square roots gives the triangle inequality.  The Cauchy–Schwarz inequality allows one to extend the notion of "angle between two vectors" to any real inner product space, by defining:        cos   θ   x  y     =    ⟨  x  ,  y  ⟩     ∥  x  ∥    ∥  y  ∥      .         subscript  θ    x  y        x  y      norm  x    norm  y       \cos\theta_{xy}=\frac{\langle x,y\rangle}{\|x\|\|y\|}.     The Cauchy–Schwarz inequality proves that this definition is sensible, by showing that the right-hand side lies in the interval [−1, 1], and justifies the notion that (real) Hilbert spaces are simply generalizations of the Euclidean space.  It can also be used to define an angle in complex  inner product spaces , by taking the absolute value of the right-hand side, as is done when extracting a metric from quantum fidelity .  The Cauchy–Schwarz is used to prove that the inner product is a continuous function with respect to the topology induced by the inner product itself.  The Cauchy–Schwarz inequality is usually used to show Bessel's inequality .  Probability theory  Let X , Y be random variables , then:        Var   (  Y  )    ≥    Cov   (  Y  ,  X  )   Cov   (  Y  ,  X  )     Var   (  X  )      .        Var  Y       Cov   Y  X   Cov   Y  X      Var  X      \text{Var}\left(Y\right)\geq\frac{\text{Cov}\left(Y,X\right)\text{Cov}\left(Y,%
 X\right)}{\text{Var}\left(X\right)}.     In fact we can define an inner product on the set of random variables using the expectation of their product:        ⟨  X  ,  Y  ⟩   ≜   E   (   X  Y   )     ,     normal-≜   X  Y    normal-E    X  Y      \langle X,Y\rangle\triangleq\operatorname{E}(XY),     and so, by the Cauchy–Schwarz inequality,         |   E   (   X  Y   )    |   2   ≤    E   (   X  2   )     E   (   Y  2   )      .       superscript     normal-E    X  Y     2      normal-E   superscript  X  2     normal-E   superscript  Y  2       |\operatorname{E}(XY)|^{2}\leq\operatorname{E}(X^{2})\operatorname{E}(Y^{2}).     Moreover, if μ = E( X ) and ν = E( Y ), then       |   Cov   (  X  ,  Y  )    |   2     superscript     Cov  X  Y    2    \displaystyle|\operatorname{Cov}(X,Y)|^{2}     where Var denotes variance and Cov denotes covariance .  Generalizations  Various generalizations of the Cauchy–Schwarz inequality exist in the context of operator theory , e.g. for operator-convex functions, and operator algebras , where the domain and/or range of φ are replaced by a C*-algebra or W*-algebra .  This section lists a few of such inequalities from the operator algebra setting, to give a flavor of results of this type.  Positive functionals on C*- and W*-algebras  One can discuss inner products as positive functionals. Given a Hilbert space L 2 ( m ), m being a finite measure, the inner product  gives rise to a positive functional φ by        ϕ   (  g  )    =   ⟨  g  ,  1  ⟩    .        ϕ  g    g  1     \phi(g)=\langle g,1\rangle.     Since  ≥ 0, φ ( f*f ) ≥ 0 for all f in L 2 ( m ), where f* is pointwise conjugate of f . So φ is positive. Conversely every positive functional φ gives a corresponding inner product φ = φ ( g*f ). In this language, the Cauchy–Schwarz inequality becomes         |   ϕ   (    g  *   f   )    |   2   ≤   ϕ   (    f  *   f   )   ϕ   (    g  *   g   )     ,       superscript      ϕ     superscript  g    f     2     ϕ     superscript  f    f   ϕ     superscript  g    g      |\phi(g^{*}f)|^{2}\leq\phi(f^{*}f)\phi(g^{*}g),     which extends verbatim to positive functionals on C*-algebras.  We now give an operator theoretic proof for the Cauchy–Schwarz inequality which passes to the C*-algebra setting. One can see from the proof that the Cauchy–Schwarz inequality is a consequence of the positivity and anti-symmetry inner-product axioms.  Consider the positive matrix       M  =    [      f  *        g  *      ]    [     f    g     ]    =   [       f  *   f       f  *   g         g  *   f       g  *   g      ]    .        M       superscript  f       superscript  g        f  g               superscript  f    f      superscript  f    g        superscript  g    f      superscript  g    g        M=\begin{bmatrix}f^{*}\\
 g^{*}\end{bmatrix}\begin{bmatrix}f&g\end{bmatrix}=\begin{bmatrix}f^{*}f&f^{*}g%
 \\
 g^{*}f&g^{*}g\end{bmatrix}.     Since φ is a positive linear map whose range, the complex numbers C , is a commutative C*-algebra, φ is completely positive . Therefore       M  ′   =    (    I  2   ⊗  ϕ   )    (  M  )    =   [      ϕ   (    f  *   f   )       ϕ   (    f  *   g   )         ϕ   (    g  *   f   )       ϕ   (    g  *   g   )       ]          superscript  M  normal-′      tensor-product   subscript  I  2   ϕ   M            ϕ     superscript  f    f      ϕ     superscript  f    g        ϕ     superscript  g    f      ϕ     superscript  g    g         M^{\prime}=(I_{2}\otimes\phi)(M)=\begin{bmatrix}\phi(f^{*}f)&\phi(f^{*}g)\\
 \phi(g^{*}f)&\phi(g^{*}g)\end{bmatrix}     is a positive 2 × 2 scalar matrix, which implies it has positive determinant:          ϕ   (    f  *   f   )   ϕ   (    g  *   g   )    -    |   ϕ   (    g  *   f   )    |   2    ≥   0  i.e.      ϕ   (    f  *   f   )    (    g  *   g   )    ≥    |   ϕ   (    g  *   f   )    |   2     .     formulae-sequence        ϕ     superscript  f    f   ϕ     superscript  g    g     superscript      ϕ     superscript  g    f     2     0  i.e.        ϕ     superscript  f    f      superscript  g    g     superscript      ϕ     superscript  g    f     2      \phi\left(f^{*}f\right)\phi\left(g^{*}g\right)-\left|\phi\left(g^{*}f\right)%
 \right|^{2}\geq 0\quad\text{i.e.}\quad\phi\left(f^{*}f\right)\left(g^{*}g%
 \right)\geq\left|\phi\left(g^{*}f\right)\right|^{2}.     This is precisely the Cauchy–Schwarz inequality. If f and g are elements of a C*-algebra, f* and g* denote their respective adjoints.  We can also deduce from above that every positive linear functional is bounded, corresponding to the fact that the inner product is jointly continuous.  Positive maps  Positive functionals are special cases of positive maps . A linear map Φ between C*-algebras is said to be a positive map if a ≥ 0 implies Φ( a ) ≥ 0. It is natural to ask whether inequalities of Schwarz-type exist for positive maps. In this more general setting, usually additional assumptions are needed to obtain such results.  Kadison–Schwarz inequality  The following theorem is named after Richard Kadison .  Theorem. If   Φ   normal-Φ   \Phi   is a unital positive map, then for every normal element    a   a   a   in its domain, we have     Φ   (    a  *   a   )    ≥   Φ   (   a  *   )   Φ   (  a  )          normal-Φ     superscript  a    a      normal-Φ   superscript  a    normal-Φ  a     \Phi(a^{*}a)\geq\Phi(a^{*})\Phi(a)   and     Φ   (    a  *   a   )    ≥   Φ   (  a  )   Φ   (   a  *   )          normal-Φ     superscript  a    a      normal-Φ  a  normal-Φ   superscript  a       \Phi(a^{*}a)\geq\Phi(a)\Phi(a^{*})   .  This extends the fact      φ   (    a  *   a   )    ⋅  1   ≥   φ    (  a  )   *   φ   (  a  )    =    |   φ   (  a  )    |   2          normal-⋅    φ     superscript  a    a    1     φ   superscript  a    φ  a         superscript      φ  a    2      \varphi(a^{*}a)\cdot 1\geq\varphi(a)^{*}\varphi(a)=|\varphi(a)|^{2}   , when   φ   φ   \varphi   is a linear functional.  The case when   a   a   a   is self-adjoint, i.e.    a  =   a  *       a   superscript  a      a=a^{*}   , is sometimes known as Kadison's inequality .  2-positive maps  When Φ is 2-positive, a stronger assumption than merely positive, one has something that looks very similar to the original Cauchy–Schwarz inequality:  Theorem (Modified Schwarz inequality for 2-positive maps). 4 For a 2-positive map Φ between C*-algebras, for all a , b in its domain,       Φ    (  a  )   *   Φ   (  a  )    ≤    ∥   Φ   (  1  )    ∥   Φ   (    a  *   a   )          normal-Φ   superscript  a    normal-Φ  a      norm    normal-Φ  1    normal-Φ     superscript  a    a      \Phi(a)^{*}\Phi(a)\leq\|\Phi(1)\|\Phi(a^{*}a)            ∥   Φ   (    a  *   b   )    ∥   2   ≤    ∥   Φ   (    a  *   a   )    ∥   ⋅   ∥   Φ   (    b  *   b   )    ∥     .       superscript   norm    normal-Φ     superscript  a    b     2    normal-⋅   norm    normal-Φ     superscript  a    a      norm    normal-Φ     superscript  b    b        \|\Phi(a^{*}b)\|^{2}\leq\|\Phi(a^{*}a)\|\cdot\|\Phi(b^{*}b)\|.     A simple argument for (2) is as follows. Consider the positive matrix       M  =    [      a  *     0       b  *     0     ]    [     a    b      0    0     ]    =   [       a  *   a       a  *   b         b  *   a       b  *   b      ]    .        M       superscript  a    0     superscript  b    0      a  b    0  0               superscript  a    a      superscript  a    b        superscript  b    a      superscript  b    b        M=\begin{bmatrix}a^{*}&0\\
 b^{*}&0\end{bmatrix}\begin{bmatrix}a&b\\
 0&0\end{bmatrix}=\begin{bmatrix}a^{*}a&a^{*}b\\
 b^{*}a&b^{*}b\end{bmatrix}.     By 2-positivity of Φ,        (    I  2   ⊗  Φ   )   M   =   [      Φ   (    a  *   a   )       Φ   (    a  *   b   )         Φ   (    b  *   a   )       Φ   (    b  *   b   )       ]          tensor-product   subscript  I  2   normal-Φ   M       normal-Φ     superscript  a    a      normal-Φ     superscript  a    b        normal-Φ     superscript  b    a      normal-Φ     superscript  b    b        (I_{2}\otimes\Phi)M=\begin{bmatrix}\Phi(a^{*}a)&\Phi(a^{*}b)\\
 \Phi(b^{*}a)&\Phi(b^{*}b)\end{bmatrix}     is positive. The desired inequality then follows from the properties of positive 2 × 2 (operator) matrices.  Part (1) is analogous. One can replace the matrix    [     a    b      0    0     ]      a  b    0  0     \begin{bmatrix}a&b\\
 0&0\end{bmatrix}   by     [     1    a      0    0     ]   .      1  a    0  0     \begin{bmatrix}1&a\\
 0&0\end{bmatrix}.     Physics  The general formulation of the Heisenberg uncertainty principle is derived using the Cauchy–Schwarz inequality.  See also   Hölder's inequality  Minkowski inequality  Jensen's inequality   Notes  References  J.M. Aldaz, S. Barza, M. Fujii and M.S. Moslehian, Advances in Operator Cauchy--Schwarz inequalities and their reverses, Ann. Funct. Anal. 6 (2015), no. 3, 275--295.        .    .      External links   Earliest Uses: The entry on the Cauchy–Schwarz inequality has some historical information.  Example of application of Cauchy–Schwarz inequality to determine Linearly Independent Vectors Tutorial and Interactive program.   "  Category:Inequalities  Category:Linear algebra  Category:Operator theory  Category:Articles containing proofs  Category:Probability theory  Category:Mathematical analysis  Category:Probabilistic inequalities     The Cauchy–Schwarz Master Class: an Introduction to the Art of Mathematical Inequalities, Ch. 1 by J. Michael Steele . ↩   ↩  page 40. ↩     