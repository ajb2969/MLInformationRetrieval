   Two-dimensional singular value decomposition      Two-dimensional singular value decomposition   Two-dimensional singular value decomposition ( 2DSVD ) computes the low-rank approximation of a set of matrices such as 2D images or weather maps in a manner almost identical to SVD ( singular value decomposition ) which computes the low-rank approximation of a single matrix (or a set of 1D vectors).  SVD  Let matrix    X  =   (   x  1   ,  …  ,   x  n   )       X    subscript  x  1   normal-…   subscript  x  n      X=(x_{1},...,x_{n})   contains the set of 1D vectors which have been centered. In PCA/SVD, we construct covariance matrix   F   F   F   and Gram matrix   G   G   G         F  =   X   X  T        F    X   superscript  X  T      F=XX^{T}   ,    G  =    X  T   X       G     superscript  X  T   X     G=X^{T}X   , and compute their eigenvectors    U  =   (   u  1   ,  …  ,   u  n   )       U    subscript  u  1   normal-…   subscript  u  n      U=(u_{1},...,u_{n})   and    V  =   (   v  1   ,  …  ,   v  n   )       V    subscript  v  1   normal-…   subscript  v  n      V=(v_{1},...,v_{n})   . Since      V   V  T    =  I   ,    U   U  T    =  I      formulae-sequence      V   superscript  V  T    I       U   superscript  U  T    I     VV^{T}=I,UU^{T}=I   , we have      X  =   U   U  T   X  V   V  T    =   U   (    U  T   X  V   )    V  T    =   U  Σ   V  T          X    U   superscript  U  T   X  V   superscript  V  T           U     superscript  U  T   X  V    superscript  V  T           U  normal-Σ   superscript  V  T       X=UU^{T}XVV^{T}=U(U^{T}XV)V^{T}=U\Sigma V^{T}   If we retain only   K   K   K   principal eigenvectors in    U  ,  V     U  V    U,V   , this gives low-rank approximation of   X   X   X   .  2DSVD  Here we deal with a set of 2D matrices    (   X  1   ,  …  ,   X  n   )      subscript  X  1   normal-…   subscript  X  n     (X_{1},...,X_{n})   . Suppose they are centered      ∑  i    X  i    =  0        subscript   i    subscript  X  i    0    \sum_{i}X_{i}=0   . We construct row–row and column–column covariance matrices      F  =    ∑  i     X  i    X  i  T         F    subscript   i      subscript  X  i    superscript   subscript  X  i   T       F=\sum_{i}X_{i}X_{i}^{T}   ,    G  =    ∑  i     X  i  T    X  i         G    subscript   i      superscript   subscript  X  i   T    subscript  X  i       G=\sum_{i}X_{i}^{T}X_{i}     in exactly the same manner as in SVD, and compute their eigenvectors   U   U   U   and   V   V   V   . We approximate    X  i     subscript  X  i    X_{i}   as       X  i   =   U   U  T    X  i   V   V  T    =   U   (    U  T    X  i   V   )    V  T    =   U   M  i    V  T           subscript  X  i     U   superscript  U  T    subscript  X  i   V   superscript  V  T           U     superscript  U  T    subscript  X  i   V    superscript  V  T           U   subscript  M  i    superscript  V  T       X_{i}=UU^{T}X_{i}VV^{T}=U(U^{T}X_{i}V)V^{T}=UM_{i}V^{T}     in identical fashion as in SVD. This gives a near optimal low-rank approximation of    (   X  1   ,  …  ,   X  n   )      subscript  X  1   normal-…   subscript  X  n     (X_{1},...,X_{n})   with the objective function      J  =    ∑  i     |    X  i   -   L   M  i    R  T     |   2        J    subscript   i    superscript       subscript  X  i     L   subscript  M  i    superscript  R  T      2      J=\sum_{i}|X_{i}-LM_{i}R^{T}|^{2}     Error bounds similar to Eckard-Young Theorem also exist.  2DSVD is mostly used in image compression and representation.  References   Chris Ding and Jieping Ye. "Two-dimensional Singular Value Decomposition (2DSVD) for 2D Maps and Images". Proc. SIAM Int'l Conf. Data Mining (SDM'05), pp. 32–43, April 2005. http://ranger.uta.edu/~chqding/papers/2dsvdSDM05.pdf  Jieping Ye. "Generalized Low Rank Approximations of Matrices". Machine Learning Journal. Vol. 61, pp. 167—191, 2005.   "  Category:Singular value decomposition   