<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="573">Oversampled binary image sensor</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Oversampled binary image sensor</h1>
<hr>An '''oversampled binary image sensor''' is a new [[image sensor]] that is
<p><code>reminiscent of traditional </code><a href="photographic_film" title="wikilink"><code>photographic</code> <code>film</code></a><code>.</code><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><code> Each pixel in the sensor has a binary response, giving only a one-bit quantized measurement of the local light intensity. The response function of the image sensor is non-linear and similar to a logarithmic function, which makes the sensor suitable for </code><a href="high_dynamic_range_imaging" title="wikilink"><code>high</code> <code>dynamic</code> <code>range</code> <code>imaging</code></a><code>.</code><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="introduction">Introduction</h2>

<p>Before the advent of digital image sensors, photography, for the most part of its history, used film to record light information. At the heart of every photographic film are a large number of light-sensitive grains of <a class="uri" href="silver-halide" title="wikilink">silver-halide</a> crystals.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> During exposure, each micron-sized grain has a binary fate: Either it is struck by some incident photons and becomes "exposed", or it is missed by the photon bombardment and remains "unexposed". In the subsequent film development process, exposed grains, due to their altered chemical properties, are converted to silver metal, contributing to opaque spots on the film; unexposed grains are washed away in a chemical bath, leaving behind the transparent regions on the film. Thus, in essence, photographic film is a binary imaging medium, using local densities of opaque silver grains to encode the original light intensity information. Thanks to the small size and large number of these grains, one hardly notices this quantized nature of film when viewing it at a distance, observing only a continuous gray tone.</p>

<p>The oversampled binary image sensor is reminiscent of photographic film. Each pixel in the sensor has a binary response, giving only a one-bit quantized measurement of the local light intensity. At the start of the exposure period, all pixels are set to 0. A pixel is then set to 1 if the number of photons reaching it during the exposure is at least equal to a given threshold <em>q</em>. One way to build such binary sensors is to modify standard memory chip technology, where each memory bit cell is designed to be sensitive to visible light.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> With current CMOS technology, the level of integration of such systems can exceed 10<sup>9</sup>~10<sup>10</sup> (i.e., 1 giga to 10 giga) pixels per chip. In this case, the corresponding pixel sizes (around 50~nm <a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a>) are far below the diffraction limit of light, and thus the image sensor is <em><a class="uri" href="oversampling" title="wikilink">oversampling</a></em> the optical resolution of the light field. Intuitively, one can exploit this spatial redundancy to compensate for the information loss due to one-bit quantizations, as is classic in oversampling <a class="uri" href="delta-sigma" title="wikilink">delta-sigma</a> conversions.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>

<p>Building a binary sensor that emulates the photographic film process was first envisioned by Fossum,<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> who coined the name <em>digital film sensor</em>. The original motivation was mainly out of technical necessity. The miniaturization of camera systems calls for the continuous shrinking of pixel sizes. At a certain point, however, the limited full-well capacity (i.e., the maximum photon-electrons a pixel can hold) of small pixels becomes a bottleneck, yielding very low <a href="signal-to-noise_ratio" title="wikilink">signal-to-noise ratios</a> (SNRs) and poor <a href="dynamic_range" title="wikilink">dynamic ranges</a>. In contrast, a binary sensor whose pixels only need to detect a few photon-electrons around a small threshold <em>q</em> has much less requirement for full-well capacities, allowing pixel sizes to shrink further.</p>
<h2 id="imaging-model">Imaging model</h2>
<h3 id="the-lens">The lens</h3>

<p> Consider a simplified camera model shown in Fig.1. The 

<math display="inline" id="Oversampled_binary_image_sensor:0">
 <semantics>
  <mrow>
   <msub>
    <mi>λ</mi>
    <mn>0</mn>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>λ</ci>
     <cn type="integer">0</cn>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda_{0}(x)
  </annotation>
 </semantics>
</math>

 is the incoming light intensity field. By assuming that light intensities remain constant within a short exposure period, the field can be modeled as only a function of the spatial variable 

<math display="inline" id="Oversampled_binary_image_sensor:1">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

. After passing through the optical system, the original light field 

<math display="inline" id="Oversampled_binary_image_sensor:2">
 <semantics>
  <mrow>
   <msub>
    <mi>λ</mi>
    <mn>0</mn>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>λ</ci>
     <cn type="integer">0</cn>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda_{0}(x)
  </annotation>
 </semantics>
</math>

 gets filtered by the lens, which acts like a linear system with a given impulse response. Due to imperfections (e.g., aberrations) in the lens, the impulse response, a.k.a. the <a href="point_spread_function" title="wikilink">point spread function</a> (PSF) of the optical system, cannot be a Dirac delta, thus, imposing a limit on the resolution of the observable light field. However, a more fundamental physical limit is due to light <a class="uri" href="diffraction" title="wikilink">diffraction</a>.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> As a result, even if the lens is ideal, the PSF is still unavoidably a small blurry spot. In optics, such diffraction-limited spot is often called the <a href="Airy_disk" title="wikilink">Airy disk</a>,<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> whose radius 

<math display="inline" id="Oversampled_binary_image_sensor:3">
 <semantics>
  <msub>
   <mi>R</mi>
   <mi>a</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>R</ci>
    <ci>a</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R_{a}
  </annotation>
 </semantics>
</math>

 can be computed as</p>

<p>

<math display="block" id="Oversampled_binary_image_sensor:4">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>R</mi>
     <mi>a</mi>
    </msub>
    <mo>=</mo>
    <mrow>
     <mpadded width="+1.7pt">
      <mn>1.22</mn>
     </mpadded>
     <mi>w</mi>
     <mi>f</mi>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>R</ci>
     <ci>a</ci>
    </apply>
    <apply>
     <times></times>
     <cn type="float">1.22</cn>
     <ci>w</ci>
     <ci>f</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R_{a}=1.22\,wf,
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Oversampled_binary_image_sensor:5">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

 is the <a class="uri" href="wavelength" title="wikilink">wavelength</a> of the light and 

<math display="inline" id="Oversampled_binary_image_sensor:6">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 is the <a class="uri" href="F-number" title="wikilink">F-number</a> of the optical system. Due to the <a class="uri" href="lowpass" title="wikilink">lowpass</a> (smoothing) nature of the PSF, the resulting 

<math display="inline" id="Oversampled_binary_image_sensor:7">
 <semantics>
  <mrow>
   <mi>λ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>λ</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda(x)
  </annotation>
 </semantics>
</math>

 has a finite spatial-resolution, i.e., it has a finite number of <a href="Degrees_of_freedom_(physics_and_chemistry)" title="wikilink">degrees of freedom</a> per unit space.</p>
<h3 id="the-sensor">The sensor</h3>
<figure><b>(Figure)</b>
<figcaption>Fig.2 The model of the binary image sensor. The pixels (shown as "buckets") collect photons, the numbers of which are compared against a quantization threshold <em>q</em>. In the figure, we illustrate the case when <em>q</em> = 2. The pixel outputs are binary

<math display="block" id="Oversampled_binary_image_sensor:8">
 <semantics>
  <mrow>
   <msub>
    <mi>b</mi>
    <mi>m</mi>
   </msub>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>b</ci>
     <ci>m</ci>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b_{m}=1
  </annotation>
 </semantics>
</math>

 (i.e., white pixels) if there are at least two photons received by the pixel; otherwise, 

<math display="inline" id="Oversampled_binary_image_sensor:9">
 <semantics>
  <mrow>
   <msub>
    <mi>b</mi>
    <mi>m</mi>
   </msub>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>b</ci>
     <ci>m</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b_{m}=0
  </annotation>
 </semantics>
</math>

 (i.e., gray pixels).</figcaption>
</figure>

<p>Fig.2 illustrates the binary sensor model. The 

<math display="inline" id="Oversampled_binary_image_sensor:10">
 <semantics>
  <msub>
   <mi>s</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>s</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s_{m}
  </annotation>
 </semantics>
</math>

 denote the exposure values accumulated by the sensor pixels. Depending on the local values of 

<math display="inline" id="Oversampled_binary_image_sensor:11">
 <semantics>
  <msub>
   <mi>s</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>s</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s_{m}
  </annotation>
 </semantics>
</math>

, each pixel (depicted as "buckets" in the figure) collects a different number of photons hitting on its surface. 

<math display="inline" id="Oversampled_binary_image_sensor:12">
 <semantics>
  <msub>
   <mi>y</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>y</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{m}
  </annotation>
 </semantics>
</math>

 is the number of photons impinging on the surface of the 

<math display="inline" id="Oversampled_binary_image_sensor:13">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

th pixel during an <a href="exposure_(photography)" title="wikilink">exposure</a> period. The relation between 

<math display="inline" id="Oversampled_binary_image_sensor:14">
 <semantics>
  <msub>
   <mi>s</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>s</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s_{m}
  </annotation>
 </semantics>
</math>

 and the photon count 

<math display="inline" id="Oversampled_binary_image_sensor:15">
 <semantics>
  <msub>
   <mi>y</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>y</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{m}
  </annotation>
 </semantics>
</math>

 is stochastic. More specifically, 

<math display="inline" id="Oversampled_binary_image_sensor:16">
 <semantics>
  <msub>
   <mi>y</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>y</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{m}
  </annotation>
 </semantics>
</math>

 can be modeled as realizations of a Poisson random variable, whose intensity parameter is equal to 

<math display="inline" id="Oversampled_binary_image_sensor:17">
 <semantics>
  <msub>
   <mi>s</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>s</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s_{m}
  </annotation>
 </semantics>
</math>

,</p>

<p>As a <a class="uri" href="photosensitive" title="wikilink">photosensitive</a> device, each pixel in the image sensor converts photons to electrical signals, whose amplitude is proportional to the number of photons impinging on that pixel. In a conventional sensor design, the analog electrical signals are then quantized by an <a href="Analog-to-digital_converter" title="wikilink">A/D converter</a> into 8 to 14 bits (usually the more bits the better). But in the binary sensor, the quantizer is 1 bit. In Fig.2, 

<math display="inline" id="Oversampled_binary_image_sensor:18">
 <semantics>
  <msub>
   <mi>b</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>b</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b_{m}
  </annotation>
 </semantics>
</math>

 is the quantized output of the 

<math display="inline" id="Oversampled_binary_image_sensor:19">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

th pixel. Since the photon counts 

<math display="inline" id="Oversampled_binary_image_sensor:20">
 <semantics>
  <msub>
   <mi>y</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>y</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{m}
  </annotation>
 </semantics>
</math>

 are drawn from random variables, so are the binary sensor output 

<math display="inline" id="Oversampled_binary_image_sensor:21">
 <semantics>
  <msub>
   <mi>b</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>b</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b_{m}
  </annotation>
 </semantics>
</math>

.</p>
<h3 id="spatial-and-temporal-oversampling">Spatial and temporal oversampling</h3>

<p>If it is allowed to have temporal oversampling, i.e.,taking multiple consecutive and independent frames without changing the total exposure time 

<math display="inline" id="Oversampled_binary_image_sensor:22">
 <semantics>
  <mi>τ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>τ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tau
  </annotation>
 </semantics>
</math>

, the performance of the binary sensor is equivalent to the sensor with same number of spatial oversampling under certain condition.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> It means that people can make trade off between spatial oversampling and temporal oversampling. This is quite important, since technology usually gives limitation on the size of the pixels and the exposure time.</p>
<h2 id="advantages-over-traditional-sensors">Advantages over traditional sensors</h2>

<p>Due to the limited full-well capacity of conventional image pixel, the pixel will saturate when the light intensity is too strong. This is the reason that the dynamic range of the pixel is low. For the oversampled binary image sensor, the dynamic range is not defined for a single pixel, but a group of pixels, which makes the dynamic range high.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></p>
<h2 id="reconstruction">Reconstruction</h2>
<figure><b>(Figure)</b>
<figcaption>Fig.4 Reconstructing an image from the binary measurements taken by a SPAD<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> sensor, with a spatial resolution of 32×32 pixels. The final image (lower-right corner) is obtained by incorporating 4096 consecutive frames, 11 of which are shown in the figure.</figcaption>
</figure>

<p>One of the most important challenges with the use of an oversampled binary image sensor is the reconstruction of the light intensity 

<math display="inline" id="Oversampled_binary_image_sensor:23">
 <semantics>
  <mrow>
   <mi>λ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>λ</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda(x)
  </annotation>
 </semantics>
</math>

 from the binary measurement 

<math display="inline" id="Oversampled_binary_image_sensor:24">
 <semantics>
  <msub>
   <mi>b</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>b</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b_{m}
  </annotation>
 </semantics>
</math>

. <a href="Maximum_likelihood" title="wikilink">Maximum likelihood estimation</a> can be used for solving this problem.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> Fig. 4 shows the results of reconstructing the light intensity from 4096 binary images taken by <a href="single_photon_avalanche_diode" title="wikilink">single photon avalanche diodes</a> (SPADs) camera.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Digital_photography" title="wikilink">Category:Digital photography</a> <a href="Category:Image_sensors" title="wikilink">Category:Image sensors</a> <a href="Category:Image_processing" title="wikilink">Category:Image processing</a> <a href="Category:Digital_signal_processing" title="wikilink">Category:Digital signal processing</a> <a href="Category:Digital_electronics" title="wikilink">Category:Digital electronics</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><code>[</code><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&amp;arnumber"><code>http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&amp;arnumber;</code></a><code>;=4959778&amp;contentType;=Conference+Publications</code> <code>L.</code> <code>Sbaiz,</code> <code>F.</code> <code>Yang,</code> <code>E.</code> <code>Charbon,</code> <code>S.</code> <code>Süsstrunk</code> <code>and</code> <code>M.</code> <code>Vetterli,</code> <code>The</code> <code>Gigavision</code> <code>Camera,</code> <em><code>Proceedings</code> <code>of</code> <code>IEEE</code> <code>International</code> <code>Conference</code> <code>on</code> <code>Acoustics,</code> <code>Speech</code> <code>and</code> <code>Signal</code> <code>Processing</code> <code>(ICASSP)</code></em><code>,</code> <code>pp.</code> <code>1093</code> <code>-</code> <code>1096,</code> <code>2009.]</code><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6104150"><code>F.</code> <code>Yang,</code> <code>Y.M.</code> <code>Lu,</code> <code>L.</code> <code>Saibz</code> <code>and</code> <code>M.</code> <code>Vetterli,</code> <code>Bits</code> <code>from</code> <code>Photons:</code> <code>Oversampled</code> <code>Image</code> <code>Acquisition</code> <code>Using</code> <code>Binary</code> <code>Poisson</code> <code>Statistics,</code> <em><code>IEEE</code> <code>Transaction</code> <code>on</code> <code>Image</code> <code>Processing</code></em><code>,</code> <code>vol.</code> <code>21,</code> <code>issue</code> <code>4,</code> <code>pp.1421-1436,</code> <code>2012.</code></a><a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4">T. H. James, The Theory of The Photographic Process, 4th ed., New York: Macmillan Publishing Co., Inc., 1977.<a href="#fnref4">↩</a></li>
<li id="fn5">S. A. Ciarcia, A 64K-bit dynamic RAM chip is the visual sensor in this digital image camera, <em>Byte Magazine</em>, pp.21-31, Sep. 1983.<a href="#fnref5">↩</a></li>
<li id="fn6">Y. K. Park, S. H. Lee, J. W. Lee et al., Fully integrated 56nm DRAM technology for 1Gb DRAM, in <em>IEEE Symposium on VLSI Technology</em>, Kyoto, Japan, Jun. 2007.<a href="#fnref6">↩</a></li>
<li id="fn7">J. C. Candy and G. C. Temes, Oversamling Delta-Sigma Data Converters-Theory, Design and Simulation. New York, NY: IEEE Press, 1992.<a href="#fnref7">↩</a></li>
<li id="fn8">E. R. Fossum, What to do with sub-diffraction-limit (SDL) pixels? - A proposal for a gigapixel digital film sensor (DFS), in <em>IEEE Workshop on Charge-Coupled Devices and Advanced Image Sensors</em>, nAGANO, jUN. 2005, PP.214-217.<a href="#fnref8">↩</a></li>
<li id="fn9">M. Born and E. Wolf, <em>Principles of Optics</em>, 7th ed. Cambridge: Cambridge University Press, 1999<a href="#fnref9">↩</a></li>
<li id="fn10">M. Born and E. Wolf, <em>Principles of Optics</em>, 7th ed. Cambridge: Cambridge University Press, 1999.<a href="#fnref10">↩</a></li>
<li id="fn11"></li>
<li id="fn12"></li>
<li id="fn13">L. Carrara, C. Niclass, N. Scheidegger, H. Shea, and E. Charbon, A gamma, X-ray and high energy proton radiation-tolerant CMOS image sensor for space applications, in ''IEEE International Solid-State Circuits Conference, Feb. 2009, pp.40-41.<a href="#fnref13">↩</a></li>
<li id="fn14"></li>
<li id="fn15"></li>
</ol>
</section>
</hr></body>
</html>
