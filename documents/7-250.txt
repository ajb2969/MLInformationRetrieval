   Duality (optimization)      Duality (optimization)   In mathematical optimization theory, duality means that optimization problems may be viewed from either of two perspectives, the primal problem or the dual problem (the duality principle ). The solution to the dual problem provides a lower bound to the solution of the primal (minimization) problem. 1 However in general the optimal values of the primal and dual problems need not be equal. Their difference is called the duality gap . For convex optimization problems, the duality gap is zero under a constraint qualification condition. Thus, a solution to the dual problem provides a bound on the value of the solution to the primal problem; when the problem is convex and satisfies a constraint qualification, then the value of an optimal solution of the primal problem is given by the dual problem.  Dual problem  Usually dual problem refers to the Lagrangian dual problem but other dual problems are used, for example, the Wolfe dual problem and the Fenchel dual problem . The Lagrangian dual problem is obtained by forming the Lagrangian , using nonnegative Lagrange multipliers to add the constraints to the objective function, and then solving for some primal variable values that minimize the Lagrangian. This solution gives the primal variables as functions of the Lagrange multipliers, which are called dual variables, so that the new problem is to maximize the objective function with respect to the dual variables under the derived constraints on the dual variables (including at least the nonnegativity).  In general given two dual pairs of separated  locally convex spaces     (  X  ,   X  *   )     X   superscript  X      \left(X,X^{*}\right)   and    (  Y  ,   Y  *   )     Y   superscript  Y      \left(Y,Y^{*}\right)   and the function    f  :   X  ‚Üí   ‚Ñù  ‚à™   {   +  ‚àû   }        normal-:  f   normal-‚Üí  X    ‚Ñù            f:X\to\mathbb{R}\cup\{+\infty\}   , we can define the primal problem as finding    x  ^     normal-^  x    \hat{x}   such that      f   (   x  ^   )    =    inf   x  ‚àà  X     f   (  x  )      .        f   normal-^  x      subscript  infimum    x  X      f  x      f(\hat{x})=\inf_{x\in X}f(x).\,   In other words,    f   (   x  ^   )       f   normal-^  x     f(\hat{x})   is the infimum (greatest lower bound) of the function   f   f   f   .  If there are constraint conditions, these can be built into the function   f   f   f   by letting     f  ~   =   f  +   I  constraints         normal-~  f     f   subscript  I  constraints      \tilde{f}=f+I_{\mathrm{constraints}}   where   I   I   I   is the indicator function . Then let    F  :    X  √ó  Y   ‚Üí   ‚Ñù  ‚à™   {   +  ‚àû   }        normal-:  F   normal-‚Üí    X  Y     ‚Ñù            F:X\times Y\to\mathbb{R}\cup\{+\infty\}   be a perturbation function such that     F   (  x  ,  0  )    =    f  ~    (  x  )          F   x  0       normal-~  f   x     F(x,0)=\tilde{f}(x)   . 2  The duality gap is the difference of the right and left hand sides of the inequality         sup    y  *   ‚àà   Y  *     -    F  *    (  0  ,   y  *   )     ‚â§    inf   x  ‚àà  X     F   (  x  ,  0  )      ,         subscript  supremum     superscript  y     superscript  Y         superscript  F     0   superscript  y         subscript  infimum    x  X      F   x  0       \sup_{y^{*}\in Y^{*}}-F^{*}(0,y^{*})\leq\inf_{x\in X}F(x,0),\,   where    F  *     superscript  F     F^{*}   is the convex conjugate in both variables and   sup   supremum   \sup   denotes the supremum (least upper bound). 3 4 5  Duality gap  The duality gap is the difference between the values of any primal solutions and any dual solutions. If    d  *     superscript  d     d^{*}   is the optimal dual value and    p  *     superscript  p     p^{*}   is the optimal primal value, then the duality gap is equal to     p  *   -   d  *        superscript  p     superscript  d      p^{*}-d^{*}   . This value is always greater than or equal to 0. The duality gap is zero if and only if strong duality holds. Otherwise the gap is strictly positive and weak duality holds. 6  In computational optimization, another "duality gap" is often reported, which is the difference in value between any dual solution and the value of a feasible but suboptimal iterate for the primal problem. This alternative "duality gap" quantifies the discrepancy between the value of a current feasible but suboptimal iterate for the primal problem and the value of the dual problem; the value of the dual problem is, under regularity conditions, equal to the value of the convex relaxation of the primal problem: The convex relaxation is the problem arising replacing a non-convex feasible set with its closed convex hull and with replacing a non-convex function with its convex closure , that is the function that has the epigraph that is the closed convex hull of the original primal objective function. 7  8 9 10  11  12  13  14  15  16 17  The linear case  Linear programming problems are optimization problems in which the objective function and the constraints are all linear . In the primal problem, the objective function is a linear combination of n variables. There are m constraints, each of which places an upper bound on a linear combination of the n variables. The goal is to maximize the value of the objective function subject to the constraints. A solution is a vector (a list) of n values that achieves the maximum value for the objective function.  In the dual problem, the objective function is a linear combination of the m values that are the limits in the m constraints from the primal problem. There are n dual constraints, each of which places a lower bound on a linear combination of m dual variables.  Relationship between the primal problem and the dual problem  In the linear case, in the primal problem, from each sub-optimal point that satisfies all the constraints, there is a direction or subspace of directions to move that increases the objective function. Moving in any such direction is said to remove slack between the candidate solution and one or more constraints. An infeasible value of the candidate solution is one that exceeds one or more of the constraints.  In the dual problem, the dual vector multiplies the constants that determine the positions of the constraints in the primal. Varying the dual vector in the dual problem is equivalent to revising the upper bounds in the primal problem. The lowest upper bound is sought. That is, the dual vector is minimized in order to remove slack between the candidate positions of the constraints and the actual optimum. An infeasible value of the dual vector is one that is too low. It sets the candidate positions of one or more of the constraints in a position that excludes the actual optimum.  This intuition is made formal by the equations in Linear programming: Duality .  An interesting example is the shortest path problem. The shortest path problem in a positively weighted graph can be formulated as a special minimum cost flow problem, which is in primal form. And the well-known Dijkstra's algorithm is the primal-dual algorithm that solves the dual form and starts from zeros. Ye et al . pointed out that the popular A* algorithm is also the primal-dual algorithm that solves the dual form. But it starts from -h, where h > 0 is the consistent heuristic. Hence one explanation that the A* algorithm is more efficient than the Dijkstra's algorithm is that as initial solution, h is better than 0.  Economic interpretation  If we interpret our primal LP problem as a classical "Resource Allocation" problem, its dual can be interpreted as a "Resource Valuation" problem.  The non-linear case  In non-linear programming , the constraints are not necessarily linear. Nonetheless, many of the same principles apply.  To ensure that the global maximum of a non-linear problem can be identified easily, the problem formulation often requires that the functions be convex and have compact lower level sets.  This is the significance of the Karush‚ÄìKuhn‚ÄìTucker conditions . They provide necessary conditions for identifying local optima of non-linear programming problems. There are additional conditions (constraint qualifications) that are necessary so that it will be possible to define the direction to an optimal solution. An optimal solution is one that is a local optimum, but possibly not a global optimum.  The strong Lagrangian principle: Lagrange duality  Given a nonlinear programming problem in standard form      f   ;  0    (  x  )      fragments  f   subscript  normal-;  0    fragments  normal-(  x  normal-)     \displaystyle f;_{0}(x)     with the domain    ùíü  ‚äÇ   ‚Ñù  n       ùíü   superscript  ‚Ñù  n     \mathcal{D}\subset\mathbb{R}^{n}   having non-empty interior, the Lagrangian function     Œõ  :     ‚Ñù  n   √ó   ‚Ñù  m   √ó   ‚Ñù  p    ‚Üí  ‚Ñù      normal-:  normal-Œõ   normal-‚Üí     superscript  ‚Ñù  n    superscript  ‚Ñù  m    superscript  ‚Ñù  p    ‚Ñù     \Lambda:\mathbb{R}^{n}\times\mathbb{R}^{m}\times\mathbb{R}^{p}\to\mathbb{R}   is defined as        Œõ   (  x  ,  Œª  ,  ŒΩ  )    =     f  0    (  x  )    +    ‚àë   i  =  1   m     Œª  i    f  i    (  x  )     +    ‚àë   i  =  1   p     ŒΩ  i    h  i    (  x  )       .        normal-Œõ   x  Œª  ŒΩ         subscript  f  0   x     superscript   subscript     i  1    m      subscript  Œª  i    subscript  f  i   x      superscript   subscript     i  1    p      subscript  ŒΩ  i    subscript  h  i   x       \Lambda(x,\lambda,\nu)=f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\sum_{i=1}^{%
 p}\nu_{i}h_{i}(x).     The vectors   Œª   Œª   \lambda   and   ŒΩ   ŒΩ   \nu   are called the dual variables or Lagrange multiplier vectors associated with the problem. The Lagrange dual function     g  :     ‚Ñù  m   √ó   ‚Ñù  p    ‚Üí  ‚Ñù      normal-:  g   normal-‚Üí     superscript  ‚Ñù  m    superscript  ‚Ñù  p    ‚Ñù     g:\mathbb{R}^{m}\times\mathbb{R}^{p}\to\mathbb{R}   is defined as        g   (  Œª  ,  ŒΩ  )    =    inf   x  ‚àà  ùíü     Œõ   (  x  ,  Œª  ,  ŒΩ  )     =    inf   x  ‚àà  ùíü     (     f  0    (  x  )    +    ‚àë   i  =  1   m     Œª  i    f  i    (  x  )     +    ‚àë   i  =  1   p     ŒΩ  i    h  i    (  x  )      )     .          g   Œª  ŒΩ      subscript  infimum    x  ùíü      normal-Œõ   x  Œª  ŒΩ            subscript  infimum    x  ùíü         subscript  f  0   x     superscript   subscript     i  1    m      subscript  Œª  i    subscript  f  i   x      superscript   subscript     i  1    p      subscript  ŒΩ  i    subscript  h  i   x         g(\lambda,\nu)=\inf_{x\in\mathcal{D}}\Lambda(x,\lambda,\nu)=\inf_{x\in\mathcal%
 {D}}\left(f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}f_{i}(x)+\sum_{i=1}^{p}\nu_{i}h_{i%
 }(x)\right).     The dual function g is concave, even when the initial problem is not convex, because it is a point-wise infimum of affine functions. The dual function yields lower bounds on the optimal value    p  *     superscript  p     p^{*}   of the initial problem; for any    Œª  ‚â•  0      Œª  0    \lambda\geq 0   and any   ŒΩ   ŒΩ   \nu   we have     g   (  Œª  ,  ŒΩ  )    ‚â§   p  *         g   Œª  ŒΩ     superscript  p      g(\lambda,\nu)\leq p^{*}   .  If a constraint qualification such as Slater's condition holds and the original problem is convex, then we have strong duality , i.e.     d  *   =     max   Œª  ‚â•   0  ,  ŒΩ     g    (  Œª  ,  ŒΩ  )    =   inf   f  0    =   p  *          superscript  d        subscript     Œª   0  ŒΩ     g    Œª  ŒΩ          infimum   subscript  f  0          superscript  p       d^{*}=\max_{\lambda\geq 0,\nu}g(\lambda,\nu)=\inf f_{0}=p^{*}   .  Convex problems  For a convex minimization problem with inequality constraints,      minimize  ùë•     x  minimize    \displaystyle\underset{x}{\operatorname{minimize}}   the Lagrangian dual problem is      maximize  ùë¢     u  maximize    \displaystyle\underset{u}{\operatorname{maximize}}   where the objective function is the Lagrange dual function. Provided that the functions   f   f   f   and     g  1   ,  ‚ãØ  ,   g  m       subscript  g  1   normal-‚ãØ   subscript  g  m     g_{1},\cdots,g_{m}   are continuously differentiable, the infimum occurs where the gradient is equal to zero. The problem      maximize   x  ,  u       x  u   maximize    \displaystyle\underset{x,u}{\operatorname{maximize}}   is called the Wolfe dual problem. This problem may be difficult to deal with computationally, because the objective function is not concave in the joint variables    (  u  ,  x  )     u  x    (u,x)   . Also, the equality constraint      ‚àá  f    (  x  )    +    ‚àë   j  =  1   m     u  j    ‚àá   g  j     (  x  )            normal-‚àá  f   x     superscript   subscript     j  1    m      subscript  u  j    normal-‚àá   subscript  g  j    x      \nabla f(x)+\sum_{j=1}^{m}u_{j}\nabla g_{j}(x)   is nonlinear in general, so the Wolfe dual problem is typically a nonconvex optimization problem. In any case, weak duality holds. 18  History  According to George Dantzig , the duality theorem for linear optimization was conjectured by John von Neumann immediately after Dantzig presented the linear programming problem. Von Neumann noted that he was using information from his game theory , and conjectured that two person zero sum matrix game was equivalent to linear programming. Rigorous proofs were first published in 1948 by Albert W. Tucker and his group. (Dantzig's foreword to Nering and Tucker, 1993)  See also   Duality  Relaxation (approximation)   Notes  References  Books                    Articles     Duality in Linear Programming Gary D. Knott    de:Lagrange-Dualit√§t "  Category:Mathematical optimization  Category:Linear programming  Category:Convex optimization  Category:Mathematical and quantitative methods (economics)     ‚Ü©  ‚Ü©   ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©     