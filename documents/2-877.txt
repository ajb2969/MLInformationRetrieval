   Einstein notation      Einstein notation   In mathematics , especially in applications of linear algebra to physics , the Einstein notation or Einstein summation convention is a notational convention that implies summation over a set of indexed terms in a formula, thus achieving notational brevity. As part of mathematics it is a notational subset of Ricci calculus ; however, it is often used in applications in physics that do not distinguish between tangent and cotangent spaces. It was introduced to physics by Albert Einstein in 1916. 1  Introduction  Statement of convention  According to this convention, when an index variable appears twice in a single term it implies summation of that term over all the values of the index. So where the indices can range over the set ,      y  =    ∑   i  =  1   3     c  i    x  i     =     c  1    x  1    +    c  2    x  2    +    c  3    x  3           y    superscript   subscript     i  1    3      subscript  c  i    superscript  x  i               subscript  c  1    superscript  x  1       subscript  c  2    superscript  x  2       subscript  c  3    superscript  x  3        y=\sum_{i=1}^{3}c_{i}x^{i}=c_{1}x^{1}+c_{2}x^{2}+c_{3}x^{3}     is reduced by the convention to:       y  =    c  i     x  i      .      y     subscript  c  i    superscript  x  i      y=c_{i}x^{i}\,.     The upper indices are not exponents but are indices of coordinates, coefficients or basis vectors . For example, should be read as "x-two", not "x squared", and typically would be equivalent to the traditional    (  x  ,  y  ,  z  )     x  y  z    (x,y,z)   .  In general relativity , a common convention is that   the Greek alphabet is used for space and time components, where indices take values 0,1,2,3 (frequently used letters are    μ  ,  ν  ,  …     μ  ν  normal-…    μ,ν,...   ),  the Latin alphabet is used for spatial components only, where indices take values 1,2,3 (frequently used letters are    i  ,  j  ,  …     i  j  normal-…    i,j,...   ),   In general, indices can range over any indexing set , including an infinite set . This should not be confused with a typographically similar convention used to distinguish between tensor index notation and the closely related but distinct basis-independent abstract index notation .  An index that is summed over is a summation index , in this case i . It is also called a dummy index since any symbol can replace i without changing the meaning of the expression, provided that it does not collide with index symbols in the same term.  An index that is not summed over is a free index and should be found in each term of the equation or formula if it appears in any term. Compare dummy indices and free indices with free variables and bound variables .  Application  Einstein notation can be applied in slightly different ways. Typically, each index occurs once in an upper (superscript) and once in a lower (subscript) position in a term; however, the convention can be applied more generally to any repeated indices within a term. 2 When dealing with covariant and contravariant vectors, where the position of an index also indicates the type of vector, the first case usually applies; a covariant vector can only be contracted with a contravariant vector, corresponding to summation of the products of coefficients. On the other hand, when there is a fixed coordinate basis (or when not considering coordinate vectors), one may choose to use only subscripts; see below .  Vector representations  Superscripts and subscripts vs. only subscripts  In terms of covariance and contravariance of vectors ,   upper indices represent components of contravariant vectors ( vectors ),  lower indices represent components of covariant vectors ( covectors ).   They transform contravariantly, resp. covariantly, with respect to change of basis.  In recognition of this fact, the following notation uses the same symbol both for a (co)vector and its components , as in:       v   =    v  i    e  i    =    [      e  1      e   ;  2      ⋯     e   ;  n       ]    [      v  1        v  2       ⋮       v  n      ]          v     superscript  v  i    subscript  e  i              subscript  e  1    fragments  e   subscript  normal-;  2    normal-⋯   fragments  e   subscript  normal-;  n         superscript  v  1      superscript  v  2     normal-⋮     superscript  v  n         \,v=v^{i}e_{i}=\begin{bmatrix}e_{1}&e_{2}&\cdots&e_{n}\end{bmatrix}\begin{%
 bmatrix}v^{1}\\
 v^{2}\\
 \vdots\\
 v^{n}\end{bmatrix}          w   =    w  i    e  i    =    [      w  1      w  2     ⋯     w  n      ]    [      e  1        e  2       ⋮       e  n      ]          w     subscript  w  i    superscript  e  i              subscript  w  1    subscript  w  2   normal-⋯   subscript  w  n        superscript  e  1      superscript  e  2     normal-⋮     superscript  e  n         \,w=w_{i}e^{i}=\begin{bmatrix}w_{1}&w_{2}&\cdots&w_{n}\end{bmatrix}\begin{%
 bmatrix}e^{1}\\
 e^{2}\\
 \vdots\\
 e^{n}\end{bmatrix}     where v is the vector and v i are its components (not the i th covector v ), w is the covector and w i are its components.  In the presence of a non-degenerate form (an isomorphism    V  →   V  *      normal-→  V   superscript  V      V\to V^{*}   , for instance a Riemannian metric or Minkowski metric ), one can raise and lower indices .  A basis gives such a form (via the dual basis ), hence when working on R n with a Euclidean metric and a fixed orthonormal basis, one can work with only subscripts.  However, if one changes coordinates, the way that coefficients change depends on the variance of the object, and one cannot ignore the distinction; see covariance and contravariance of vectors .  Mnemonics  In the above example, vectors are represented as n ×1 matrices (column vectors), while covectors are represented as 1× n matrices (row covectors).  When using the column vector convention   " Up per indices go up to down; l ower indices go l eft to right"  " CO variant tensors are ROW vectors that have indices that are bel OW . Co-below-row  Vectors can be stacked (column matrices) side-by-side:        [      v  1     ⋯     v  k      ]   .       subscript  v  1   normal-⋯   subscript  v  k      \begin{bmatrix}v_{1}&\cdots&v_{k}\end{bmatrix}.      Hence the lower index indicates which column you are in.    You can stack covectors (row matrices) top-to-bottom:       [      w  1       ⋮       w  k      ]       superscript  w  1     normal-⋮     superscript  w  k      \begin{bmatrix}w^{1}\\
 \vdots\\
 w^{k}\end{bmatrix}      Hence the upper index indicates which row you are in.   Abstract description  The virtue of Einstein notation is that it represents the invariant quantities with a simple notation.  In physics, a scalar is invariant under transformations of basis . In particular, a Lorentz scalar is invariant under a Lorentz transformation. The individual terms in the sum are not. When the basis is changed, the components of a vector change by a linear transformation described by a matrix. This led Einstein to propose the convention that repeated indices imply the summation is to be done.  As for covectors, they change by the inverse matrix. This is designed to guarantee that the linear function associated with the covector, the sum above, is the same no matter what the basis is.  The value of the Einstein convention is that it applies to other vector spaces built from V using the tensor product and duality . For example,    V  ⊗  V     tensor-product  V  V    V\otimes V   , the tensor product of V with itself, has a basis consisting of tensors of the form     𝐞   i  j    =    𝐞  i   ⊗   𝐞  j         subscript  𝐞    i  j     tensor-product   subscript  𝐞  i    subscript  𝐞  j      \mathbf{e}_{ij}=\mathbf{e}_{i}\otimes\mathbf{e}_{j}   . Any tensor   𝐓   𝐓   \mathbf{T}   in    V  ⊗  V     tensor-product  V  V    V\otimes V   can be written as:      𝐓  =    T   i  j     𝐞   i  j         𝐓     superscript  T    i  j     subscript  𝐞    i  j       \mathbf{T}=T^{ij}\mathbf{e}_{ij}   .      V  *     superscript  V     V^{*}   , the dual of   V   V   V   , has a basis e 1 , e 2 , ..., e n which obeys the rule         𝐞  i    (   𝐞  j   )    =   δ  j  i    .         superscript  𝐞  i    subscript  𝐞  j     subscript   superscript  δ  i   j     \mathbf{e}^{i}(\mathbf{e}_{j})=\delta^{i}_{j}.   where   δ   δ   \delta   is the Kronecker delta . As       Hom   (  V  ,  W  )    =    V  *   ⊗  W         Hom   V  W     tensor-product   superscript  V    W     \mathrm{Hom}(V,W)=V^{*}\otimes W   the row-column coordinates on a matrix correspond to the upper-lower indices on the tensor product.  Common operations in this notation  In Einstein notation, the usual element reference    A   m  n      subscript  A    m  n     A_{mn}   for the m th row and n th column of matrix A becomes     A  m     n      fragments   superscript  A  m    n     A^{m}{}_{n}   . We can then write the following operations in Einstein notation as follows.   Inner product (hence also vector dot product )   Using an orthogonal basis , the inner product is the sum of corresponding components multiplied together:       𝐮  ⋅  𝐯   =    u  j    v  j         normal-⋅  𝐮  𝐯      subscript  u  j    superscript  v  j      \mathbf{u}\cdot\mathbf{v}=u_{j}v^{j}     This can also be calculated by multiplying the covector on the vector.   Vector cross product   Again using an orthogonal basis (in 3d) the cross product intrinsically involves summations over permutations of components:       𝐮  ×  𝐯   =    u  j    v  k    ϵ  i    𝐞  i     j  k            𝐮  𝐯      superscript  u  j    superscript  v  k    superscript  ϵ  i    subscript   subscript  𝐞  i     j  k       \mathbf{u}\times\mathbf{v}=u^{j}v^{k}\epsilon^{i}{}_{jk}\mathbf{e}_{i}     where       ϵ  i    =    j  k      δ   i  l     ϵ   l  j  k       fragments   superscript  ϵ  i    subscript     j  k     superscript  δ    i  l     subscript  ϵ    l  j  k      \epsilon^{i}{}_{jk}=\delta^{il}\epsilon_{ljk}     and    ϵ   i  j  k      subscript  ϵ    i  j  k     \epsilon_{ijk}   is the Levi-Civita symbol . Based on this definition of   ϵ   ϵ   \epsilon   , there is no difference between     ϵ  i      j  k       fragments   superscript  ϵ  i      j  k      \epsilon^{i}{}_{jk}   and    ϵ   i  j  k      subscript  ϵ    i  j  k     \epsilon_{ijk}   but the position of indices.   Matrix multiplication   The matrix product of two matrices    A   i  j      subscript  A    i  j     A_{ij}   and    B   j  k      subscript  B    j  k     B_{jk}   is:       𝐂   i  k    =    (    𝐀   𝐁   )    i  k    =    ∑   j  =  1   N     A   i  j     B   j  k             subscript  𝐂    i  k     subscript    𝐀  𝐁     i  k           superscript   subscript     j  1    N      subscript  A    i  j     subscript  B    j  k         \mathbf{C}_{ik}=(\mathbf{A}\,\mathbf{B})_{ik}=\sum_{j=1}^{N}A_{ij}B_{jk}     equivalent to       C  i    =   k     A  i    B   j   j      k      fragments   superscript  C  i    subscript   k    superscript  A  i    subscript   superscript  B  j   j    k     C^{i}{}_{k}=A^{i}{}_{j}\,B^{j}{}_{k}      Trace   For a square matrix     A  i     j      fragments   superscript  A  i    j     A^{i}{}_{j}   , the trace is the sum of the diagonal elements, hence the sum over a common index     A  i     i      fragments   superscript  A  i    i     A^{i}{}_{i}   .   Outer product   The outer product of the column vector    u  i     superscript  u  i    u^{i}   by the row vector    v  j     subscript  v  j    v_{j}   yields an m × n matrix A :       A  i    =   j      u  i     v  j   =    (  u  v  )   i     j      fragments   superscript  A  i    subscript   j    superscript  u  i    subscript  v  j     superscript   fragments  normal-(  u  v  normal-)   i    j     A^{i}{}_{j}=u^{i}\,v_{j}=(uv)^{i}{}_{j}     Since i and j represent two different indices, there is no summation and the indices are not eliminated by the multiplication.   Raising and lowering indices   Given a tensor, one can raise an index or lower an index by contracting the tensor with the metric tensor,    g   μ  ν      subscript  g    μ  ν     g_{\mu\nu}   . For example, take the tensor    T  β  α     subscript   superscript  T  α   β    T^{\alpha}_{\beta}   , one can raise an index:       T   μ  α    =    g   μ  σ     T  σ   α          superscript  T    μ  α       superscript  g    μ  σ     subscript   superscript  T  α   σ      T^{\mu\alpha}=g^{\mu\sigma}T^{\;\alpha}_{\sigma}     Or one can lower an index:       T   μ  β    =    g   μ  σ     T   β   σ         subscript  T    μ  β       subscript  g    μ  σ     subscript   superscript  T  σ   β      T_{\mu\beta}=g_{\mu\sigma}T^{\sigma}_{\;\beta}     See also   Ricci calculus  Tensor  Abstract index notation  Bra–ket notation  Penrose graphical notation  Kronecker delta  Levi-Civita symbol   Notes    This applies only for numerical indices. The situation is the opposite for abstract indices . Then, vectors themselves carry upper abstract indices and covectors carry lower abstract indices, as per the example in the introduction of this article. Elements of a basis of vectors may carry a lower numerical index and an upper abstract index.   References  Bibliography    .   External links     "  Category:Mathematical notation  Category:Multilinear algebra  Category:Tensors  Category:Riemannian geometry  Category:Mathematical physics  Category:Albert Einstein     ↩  ↩     