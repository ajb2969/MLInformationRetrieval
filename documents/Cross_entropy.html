<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1351">Cross entropy</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Cross entropy</h1>
<hr/>

<p>In <a href="information_theory" title="wikilink">information theory</a>, the <strong>cross entropy</strong> between two <a href="probability_distribution" title="wikilink">probability distributions</a> over the same underlying set of events measures the average number of <a href="bit" title="wikilink">bits</a> needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an "unnatural" probability distribution 

<math display="inline" id="Cross_entropy:0">
 <semantics>
  <mi>q</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>q</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q
  </annotation>
 </semantics>
</math>

, rather than the "true" distribution 

<math display="inline" id="Cross_entropy:1">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

.</p>

<p>The cross entropy for the distributions 

<math display="inline" id="Cross_entropy:2">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Cross_entropy:3">
 <semantics>
  <mi>q</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>q</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q
  </annotation>
 </semantics>
</math>


 over a given set is defined as follows:</p>

<p>

<math display="block" id="Cross_entropy:4">
 <semantics>
  <mrow>
   <mi>H</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>p</mi>
    <mo>,</mo>
    <mi>q</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <msub>
    <mo>E</mo>
    <mi>p</mi>
   </msub>
   <mrow>
    <mo stretchy="false">[</mo>
    <mo>-</mo>
    <mi>log</mi>
    <mi>q</mi>
    <mo stretchy="false">]</mo>
   </mrow>
   <mo>=</mo>
   <mi>H</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>p</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>+</mo>
   <msub>
    <mi>D</mi>
    <mi>KL</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>p</mi>
    <mo>∥</mo>
    <mi>q</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo rspace="0.8pt">,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">H</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">p</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">q</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>normal-E</ci>
     <ci>p</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-[</ci>
     <minus></minus>
     <log></log>
     <csymbol cd="unknown">q</csymbol>
     <ci>normal-]</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">H</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">p</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <plus></plus>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>D</ci>
     <ci>KL</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">p</csymbol>
     <csymbol cd="latexml">parallel-to</csymbol>
     <csymbol cd="unknown">q</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-,</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(p,q)=\operatorname{E}_{p}[-\log q]=H(p)+D_{\mathrm{KL}}(p\|q),\!
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Cross_entropy:5">
 <semantics>
  <mrow>
   <mi>H</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>p</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>H</ci>
    <ci>p</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(p)
  </annotation>
 </semantics>
</math>

 is the <a href="information_entropy" title="wikilink">entropy</a> of 

<math display="inline" id="Cross_entropy:6">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

, and 

<math display="inline" id="Cross_entropy:7">
 <semantics>
  <mrow>
   <msub>
    <mi>D</mi>
    <mi>KL</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>p</mi>
    <mo stretchy="false">|</mo>
    <mo stretchy="false">|</mo>
    <mi>q</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>D</ci>
     <ci>KL</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">p</csymbol>
     <ci>normal-|</ci>
     <ci>normal-|</ci>
     <csymbol cd="unknown">q</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{\mathrm{KL}}(p||q)
  </annotation>
 </semantics>
</math>

 is the <a href="Kullback–Leibler_divergence" title="wikilink">Kullback–Leibler divergence</a> of 

<math display="inline" id="Cross_entropy:8">
 <semantics>
  <mi>q</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>q</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q
  </annotation>
 </semantics>
</math>


 from 

<math display="inline" id="Cross_entropy:9">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

 (also known as the <em>relative entropy</em> of <em>p</em> with respect to <em>q</em> — note the reversal of emphasis).</p>

<p>For <a href="discrete_random_variable" title="wikilink">discrete</a> 

<math display="inline" id="Cross_entropy:10">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Cross_entropy:11">
 <semantics>
  <mi>q</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>q</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q
  </annotation>
 </semantics>
</math>

 this means</p>

<p>

<math display="block" id="Cross_entropy:12">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>H</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>p</mi>
      <mo>,</mo>
      <mi>q</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mo>-</mo>
     <mrow>
      <munder>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mi>x</mi>
      </munder>
      <mrow>
       <mi>p</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo rspace="4.2pt" stretchy="false">)</mo>
       </mrow>
       <mrow>
        <mi>log</mi>
        <mi>q</mi>
       </mrow>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>H</ci>
     <interval closure="open">
      <ci>p</ci>
      <ci>q</ci>
     </interval>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <ci>x</ci>
      </apply>
      <apply>
       <times></times>
       <ci>p</ci>
       <ci>x</ci>
       <apply>
        <log></log>
        <ci>q</ci>
       </apply>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(p,q)=-\sum_{x}p(x)\,\log q(x).\!
  </annotation>
 </semantics>
</math>

</p>

<p>The situation for <a href="continuous_random_variable" title="wikilink">continuous</a> distributions is analogous:</p>

<p>

<math display="block" id="Cross_entropy:13">
 <semantics>
  <mrow>
   <mrow>
    <mo>-</mo>
    <mrow>
     <msub>
      <mo largeop="true" symmetric="true">∫</mo>
      <mi>X</mi>
     </msub>
     <mrow>
      <mi>p</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo rspace="4.2pt" stretchy="false">)</mo>
      </mrow>
      <mrow>
       <mi>log</mi>
       <mi>q</mi>
      </mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo rspace="4.2pt" stretchy="false">)</mo>
      </mrow>
      <mi>d</mi>
      <mi>x</mi>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <int></int>
      <ci>X</ci>
     </apply>
     <apply>
      <times></times>
      <ci>p</ci>
      <ci>x</ci>
      <apply>
       <log></log>
       <ci>q</ci>
      </apply>
      <ci>x</ci>
      <ci>d</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   -\int_{X}p(x)\,\log q(x)\,dx.\!
  </annotation>
 </semantics>
</math>

</p>

<p>NB: The notation 

<math display="inline" id="Cross_entropy:14">
 <semantics>
  <mrow>
   <mi>H</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>p</mi>
    <mo>,</mo>
    <mi>q</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>H</ci>
    <interval closure="open">
     <ci>p</ci>
     <ci>q</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(p,q)
  </annotation>
 </semantics>
</math>

 is also used for a different concept, the <a href="joint_entropy" title="wikilink">joint entropy</a> of 

<math display="inline" id="Cross_entropy:15">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Cross_entropy:16">
 <semantics>
  <mi>q</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>q</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q
  </annotation>
 </semantics>
</math>

.</p>
<h2 id="motivation">Motivation</h2>

<p>In information theory, the <a href="Kraft's_inequality" title="wikilink">Kraft–McMillan theorem</a> establishes that any directly decodable coding scheme for coding a message to identify one value 

<math display="inline" id="Cross_entropy:17">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}
  </annotation>
 </semantics>
</math>

 out of a set of possibilities 

<math display="inline" id="Cross_entropy:18">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>


 can be seen as representing an implicit probability distribution 

<math display="inline" id="Cross_entropy:19">
 <semantics>
  <mrow>
   <mrow>
    <mi>q</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>x</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mn>2</mn>
    <mrow>
     <mo>-</mo>
     <msub>
      <mi>l</mi>
      <mi>i</mi>
     </msub>
    </mrow>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>q</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cn type="integer">2</cn>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>l</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q(x_{i})=2^{-l_{i}}
  </annotation>
 </semantics>
</math>

 over 

<math display="inline" id="Cross_entropy:20">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Cross_entropy:21">
 <semantics>
  <msub>
   <mi>l</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>l</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   l_{i}
  </annotation>
 </semantics>
</math>

 is the length of the code for 

<math display="inline" id="Cross_entropy:22">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}
  </annotation>
 </semantics>
</math>

 in bits. Therefore, cross entropy can be interpreted as the expected message-length per datum when a wrong distribution 

<math display="inline" id="Cross_entropy:23">
 <semantics>
  <mi>Q</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Q</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Q
  </annotation>
 </semantics>
</math>


 is assumed, however the data actually follows a distribution 

<math display="inline" id="Cross_entropy:24">
 <semantics>
  <mi>P</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>P</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P
  </annotation>
 </semantics>
</math>

 — that is why the expectation is taken over the probability distribution 

<math display="inline" id="Cross_entropy:25">
 <semantics>
  <mi>P</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>P</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P
  </annotation>
 </semantics>
</math>

 and not 

<math display="inline" id="Cross_entropy:26">
 <semantics>
  <mi>Q</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Q</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Q
  </annotation>
 </semantics>
</math>

.</p>

<p>

<math display="block" id="Cross_entropy:27">
 <semantics>
  <mrow>
   <mrow>
    <mi>H</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>p</mi>
     <mo>,</mo>
     <mi>q</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mo>E</mo>
     <mi>p</mi>
    </msub>
    <mrow>
     <mo stretchy="false">[</mo>
     <msub>
      <mi>l</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mo>E</mo>
     <mi>p</mi>
    </msub>
    <mrow>
     <mo>[</mo>
     <mrow>
      <mi>log</mi>
      <mfrac>
       <mn>1</mn>
       <mrow>
        <mi>q</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <msub>
          <mi>x</mi>
          <mi>i</mi>
         </msub>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mfrac>
     </mrow>
     <mo>]</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <ci>H</ci>
      <interval closure="open">
       <ci>p</ci>
       <ci>q</ci>
      </interval>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-E</ci>
       <ci>p</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>l</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-E</ci>
       <ci>p</ci>
      </apply>
      <apply>
       <log></log>
       <apply>
        <divide></divide>
        <cn type="integer">1</cn>
        <apply>
         <times></times>
         <ci>q</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(p,q)=\operatorname{E}_{p}[l_{i}]=\operatorname{E}_{p}\left[\log\frac{1}{q(x_%
{i})}\right]
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="block" id="Cross_entropy:28">
 <semantics>
  <mrow>
   <mrow>
    <mi>H</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>p</mi>
     <mo>,</mo>
     <mi>q</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <munder>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <msub>
      <mi>x</mi>
      <mi>i</mi>
     </msub>
    </munder>
    <mrow>
     <mi>p</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
      <mo rspace="4.2pt" stretchy="false">)</mo>
     </mrow>
     <mrow>
      <mi>log</mi>
      <mpadded width="-1.7pt">
       <mfrac>
        <mn>1</mn>
        <mrow>
         <mi>q</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
       </mfrac>
      </mpadded>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>H</ci>
     <interval closure="open">
      <ci>p</ci>
      <ci>q</ci>
     </interval>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>i</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>p</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <log></log>
       <apply>
        <divide></divide>
        <cn type="integer">1</cn>
        <apply>
         <times></times>
         <ci>q</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(p,q)=\sum_{x_{i}}p(x_{i})\,\log\frac{1}{q(x_{i})}\!
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="block" id="Cross_entropy:29">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>H</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>p</mi>
      <mo>,</mo>
      <mi>q</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mo>-</mo>
     <mrow>
      <munder>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mi>x</mi>
      </munder>
      <mrow>
       <mi>p</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo rspace="4.2pt" stretchy="false">)</mo>
       </mrow>
       <mrow>
        <mi>log</mi>
        <mi>q</mi>
       </mrow>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>H</ci>
     <interval closure="open">
      <ci>p</ci>
      <ci>q</ci>
     </interval>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <ci>x</ci>
      </apply>
      <apply>
       <times></times>
       <ci>p</ci>
       <ci>x</ci>
       <apply>
        <log></log>
        <ci>q</ci>
       </apply>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(p,q)=-\sum_{x}p(x)\,\log q(x).\!
  </annotation>
 </semantics>
</math>

</p>
<h2 id="estimation">Estimation</h2>

<p>There are many situations where cross-entropy needs to be measured but the distribution of 

<math display="inline" id="Cross_entropy:30">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

 is unknown. An example is <a href="language_model" title="wikilink">language modeling</a>, where a model is created based on a training set 

<math display="inline" id="Cross_entropy:31">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>

, and then its cross-entropy is measured on a test set to assess how accurate the model is in predicting the test data. In this example, 

<math display="inline" id="Cross_entropy:32">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

 is the true distribution of words in any corpus, and 

<math display="inline" id="Cross_entropy:33">
 <semantics>
  <mi>q</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>q</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q
  </annotation>
 </semantics>
</math>


 is the distribution of words as predicted by the model. Since the true distribution is unknown, cross-entropy cannot be directly calculated. In these cases, an estimate of cross-entropy is calculated using the following formula:</p>

<p>

<math display="block" id="Cross_entropy:34">
 <semantics>
  <mrow>
   <mrow>
    <mi>H</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>T</mi>
     <mo>,</mo>
     <mi>q</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>N</mi>
     </munderover>
     <mrow>
      <mfrac>
       <mn>1</mn>
       <mi>N</mi>
      </mfrac>
      <mrow>
       <msub>
        <mi>log</mi>
        <mn>2</mn>
       </msub>
       <mi>q</mi>
      </mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>x</mi>
        <mi>i</mi>
       </msub>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>H</ci>
     <interval closure="open">
      <ci>T</ci>
      <ci>q</ci>
     </interval>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>N</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <divide></divide>
        <cn type="integer">1</cn>
        <ci>N</ci>
       </apply>
       <apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <log></log>
         <cn type="integer">2</cn>
        </apply>
        <ci>q</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(T,q)=-\sum_{i=1}^{N}\frac{1}{N}\log_{2}q(x_{i})
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Cross_entropy:35">
 <semantics>
  <mi>N</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>N</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N
  </annotation>
 </semantics>
</math>

 is the size of the test set, and 

<math display="inline" id="Cross_entropy:36">
 <semantics>
  <mrow>
   <mi>q</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>q</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q(x)
  </annotation>
 </semantics>
</math>

 is the probability of event 

<math display="inline" id="Cross_entropy:37">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 estimated from the training set. The sum is calculated over 

<math display="inline" id="Cross_entropy:38">
 <semantics>
  <mi>N</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>N</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N
  </annotation>
 </semantics>
</math>


. This is a Monte Carlo estimate of the true cross entropy, where the training set is treated as samples from 

<math display="inline" id="Cross_entropy:39">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>p</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(x)
  </annotation>
 </semantics>
</math>

.</p>
<h2 id="cross-entropy-minimization">Cross-entropy minimization</h2>

<p>Cross-entropy minimization is frequently used in optimization and rare-event probability estimation; see the <a href="cross-entropy_method" title="wikilink">cross-entropy method</a>.</p>

<p>When comparing a distribution 

<math display="inline" id="Cross_entropy:40">
 <semantics>
  <mi>q</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>q</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q
  </annotation>
 </semantics>
</math>

 against a fixed reference distribution 

<math display="inline" id="Cross_entropy:41">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

, cross entropy and <a href="Kullback–Leibler_divergence" title="wikilink">KL divergence</a> are identical up to an additive constant (since 

<math display="inline" id="Cross_entropy:42">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

 is fixed): both take on their minimal values when 

<math display="inline" id="Cross_entropy:43">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mo>=</mo>
   <mi>q</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>p</ci>
    <ci>q</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p=q
  </annotation>
 </semantics>
</math>


, which is 

<math display="inline" id="Cross_entropy:44">
 <semantics>
  <mn>0</mn>
  <annotation-xml encoding="MathML-Content">
   <cn type="integer">0</cn>
  </annotation-xml>
 </semantics>
</math>

 for KL divergence, and 

<math display="inline" id="Cross_entropy:45">
 <semantics>
  <mrow>
   <mi mathvariant="normal">H</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>p</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-H</ci>
    <ci>p</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{H}(p)
  </annotation>
 </semantics>
</math>

 for cross entropy. In the engineering literature, the principle of minimising KL Divergence (Kullback's "<a href="Kullback–Leibler_divergence#Principle_of_minimum_discrimination_information" title="wikilink">Principle of Minimum Discrimination Information</a>") is often called the <strong>Principle of Minimum Cross-Entropy</strong> (MCE), or <strong>Minxent</strong>.</p>

<p>However, as discussed in the article <em><a href="Kullback–Leibler_divergence" title="wikilink">Kullback–Leibler divergence</a></em>, sometimes the distribution 

<math display="inline" id="Cross_entropy:46">
 <semantics>
  <mi>q</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>q</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q
  </annotation>
 </semantics>
</math>

 is the fixed prior reference distribution, and the distribution 

<math display="inline" id="Cross_entropy:47">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

 is optimised to be as close to 

<math display="inline" id="Cross_entropy:48">
 <semantics>
  <mi>q</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>q</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q
  </annotation>
 </semantics>
</math>


 as possible, subject to some constraint. In this case the two minimisations are <em>not</em> equivalent. This has led to some ambiguity in the literature, with some authors attempting to resolve the inconsistency by redefining cross-entropy to be 

<math display="inline" id="Cross_entropy:49">
 <semantics>
  <mrow>
   <msub>
    <mi>D</mi>
    <mi>KL</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>p</mi>
    <mo>∥</mo>
    <mi>q</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>D</ci>
     <ci>KL</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">p</csymbol>
     <csymbol cd="latexml">parallel-to</csymbol>
     <csymbol cd="unknown">q</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{\mathrm{KL}}(p\|q)
  </annotation>
 </semantics>
</math>

, rather than 

<math display="inline" id="Cross_entropy:50">
 <semantics>
  <mrow>
   <mi>H</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>p</mi>
    <mo>,</mo>
    <mi>q</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>H</ci>
    <interval closure="open">
     <ci>p</ci>
     <ci>q</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(p,q)
  </annotation>
 </semantics>
</math>

.</p>
<h2 id="cross-entropy-error-function-and-logistic-regression">Cross-entropy error function and logistic regression</h2>

<p>Cross entropy can be used to define loss function in machine learning and optimization. The true probability 

<math display="inline" id="Cross_entropy:51">
 <semantics>
  <msub>
   <mi>p</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>p</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p_{i}
  </annotation>
 </semantics>
</math>

 is the true label, and the given distribution 

<math display="inline" id="Cross_entropy:52">
 <semantics>
  <msub>
   <mi>q</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>q</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q_{i}
  </annotation>
 </semantics>
</math>

 is the predicted value of the current model.</p>

<p>More specifically, let us consider <a href="logistic_regression" title="wikilink">logistic regression</a>, which (in its most basic guise) deals with classifying a given set of data points into two possible classes generically labelled 

<math display="inline" id="Cross_entropy:53">
 <semantics>
  <mn>0</mn>
  <annotation-xml encoding="MathML-Content">
   <cn type="integer">0</cn>
  </annotation-xml>
 </semantics>
</math>


 and 

<math display="inline" id="Cross_entropy:54">
 <semantics>
  <mn>1</mn>
  <annotation-xml encoding="MathML-Content">
   <cn type="integer">1</cn>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1
  </annotation>
 </semantics>
</math>

. The logistic regression model thus predicts an output 

<math display="inline" id="Cross_entropy:55">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>∈</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mn>0</mn>
    <mo>,</mo>
    <mn>1</mn>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>y</ci>
    <set>
     <cn type="integer">0</cn>
     <cn type="integer">1</cn>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y\in\{0,1\}
  </annotation>
 </semantics>
</math>

, given an input vector 

<math display="inline" id="Cross_entropy:56">
 <semantics>
  <mi>𝐱</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐱</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}
  </annotation>
 </semantics>
</math>

. The probability is modeled using the <a href="logistic_function" title="wikilink">logistic function</a> 

<math display="inline" id="Cross_entropy:57">
 <semantics>
  <mrow>
   <mrow>
    <mi>g</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>z</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>/</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mn>1</mn>
      <mo>+</mo>
      <msup>
       <mi>e</mi>
       <mrow>
        <mo>-</mo>
        <mi>z</mi>
       </mrow>
      </msup>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>g</ci>
     <ci>z</ci>
    </apply>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <apply>
      <plus></plus>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>e</ci>
       <apply>
        <minus></minus>
        <ci>z</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g(z)=1/(1+e^{-z})
  </annotation>
 </semantics>
</math>

. Namely, the probability of finding the output 

<math display="inline" id="Cross_entropy:58">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=1
  </annotation>
 </semantics>
</math>


 is given by</p>

<p>

<math display="block" id="Cross_entropy:59">
 <semantics>
  <mrow>
   <mrow>
    <mpadded width="+5pt">
     <msub>
      <mi>q</mi>
      <mrow>
       <mi>y</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
     </msub>
    </mpadded>
    <mo rspace="7.5pt">=</mo>
    <mpadded width="+5pt">
     <mover accent="true">
      <mi>y</mi>
      <mo stretchy="false">^</mo>
     </mover>
    </mpadded>
    <mo rspace="7.5pt">≡</mo>
    <mrow>
     <mi>g</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>𝐰</mi>
       <mo>⋅</mo>
       <mi>𝐱</mi>
      </mrow>
      <mo rspace="4.2pt" stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>q</ci>
      <apply>
       <eq></eq>
       <ci>y</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <ci>normal-^</ci>
      <ci>y</ci>
     </apply>
    </apply>
    <apply>
     <equivalent></equivalent>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>g</ci>
      <apply>
       <ci>normal-⋅</ci>
       <ci>𝐰</ci>
       <ci>𝐱</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q_{y=1}\ =\ \hat{y}\ \equiv\ g(\mathbf{w}\cdot\mathbf{x})\,,
  </annotation>
 </semantics>
</math>

 where the vector of weights 

<math display="inline" id="Cross_entropy:60">
 <semantics>
  <mi>𝐰</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐰</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}
  </annotation>
 </semantics>
</math>

 is learned through some appropriate algorithm such as <a href="gradient_descent" title="wikilink">gradient descent</a>. Similarly, the conjugate probability of finding the output 

<math display="inline" id="Cross_entropy:61">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=0
  </annotation>
 </semantics>
</math>

 is simply given by</p>

<p>

<math display="block" id="Cross_entropy:62">
 <semantics>
  <mrow>
   <mpadded width="+5pt">
    <msub>
     <mi>q</mi>
     <mrow>
      <mi>y</mi>
      <mo>=</mo>
      <mn>0</mn>
     </mrow>
    </msub>
   </mpadded>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <mover accent="true">
     <mi>y</mi>
     <mo stretchy="false">^</mo>
    </mover>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>q</ci>
     <apply>
      <eq></eq>
      <ci>y</ci>
      <cn type="integer">0</cn>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <cn type="float">1</cn>
     <apply>
      <ci>normal-^</ci>
      <ci>y</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q_{y=0}\ =\ 1-\hat{y}
  </annotation>
 </semantics>
</math>

 The true (observed) probabilities can be expressed similarly as 

<math display="inline" id="Cross_entropy:63">
 <semantics>
  <mrow>
   <msub>
    <mi>p</mi>
    <mrow>
     <mi>y</mi>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
   </msub>
   <mo>=</mo>
   <mi>y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>p</ci>
     <apply>
      <eq></eq>
      <ci>y</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p_{y=1}=y
  </annotation>
 </semantics>
</math>


 and 

<math display="inline" id="Cross_entropy:64">
 <semantics>
  <mrow>
   <msub>
    <mi>p</mi>
    <mrow>
     <mi>y</mi>
     <mo>=</mo>
     <mn>0</mn>
    </mrow>
   </msub>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <mi>y</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>p</ci>
     <apply>
      <eq></eq>
      <ci>y</ci>
      <cn type="integer">0</cn>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <ci>y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p_{y=0}=1-y
  </annotation>
 </semantics>
</math>

.</p>

<p>Having set up our notation, 

<math display="inline" id="Cross_entropy:65">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mo>∈</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>y</mi>
    <mo>,</mo>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mi>y</mi>
    </mrow>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>p</ci>
    <set>
     <ci>y</ci>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
      <ci>y</ci>
     </apply>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p\in\{y,1-y\}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Cross_entropy:66">
 <semantics>
  <mrow>
   <mi>q</mi>
   <mo>∈</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mover accent="true">
     <mi>y</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mo>,</mo>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mover accent="true">
      <mi>y</mi>
      <mo stretchy="false">^</mo>
     </mover>
    </mrow>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>q</ci>
    <set>
     <apply>
      <ci>normal-^</ci>
      <ci>y</ci>
     </apply>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
      <apply>
       <ci>normal-^</ci>
       <ci>y</ci>
      </apply>
     </apply>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q\in\{\hat{y},1-\hat{y}\}
  </annotation>
 </semantics>
</math>

, we can use cross entropy to get a measure for similarity between 

<math display="inline" id="Cross_entropy:67">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Cross_entropy:68">
 <semantics>
  <mi>q</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>q</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q
  </annotation>
 </semantics>
</math>


:</p>

<p>

<math display="block" id="Cross_entropy:69">
 <semantics>
  <mrow>
   <mrow>
    <mi>H</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>p</mi>
     <mo>,</mo>
     <mi>q</mi>
     <mo rspace="7.5pt" stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo rspace="7.5pt">=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <munder>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mi>i</mi>
     </munder>
     <mrow>
      <msub>
       <mi>p</mi>
       <mi>i</mi>
      </msub>
      <mrow>
       <mi>log</mi>
       <mpadded width="+5pt">
        <msub>
         <mi>q</mi>
         <mi>i</mi>
        </msub>
       </mpadded>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo rspace="7.5pt">=</mo>
   <mrow>
    <mrow>
     <mo>-</mo>
     <mrow>
      <mi>y</mi>
      <mrow>
       <mi>log</mi>
       <mover accent="true">
        <mi>y</mi>
        <mo stretchy="false">^</mo>
       </mover>
      </mrow>
     </mrow>
    </mrow>
    <mo>-</mo>
    <mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mn>1</mn>
       <mo>-</mo>
       <mi>y</mi>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mrow>
      <mi>log</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mn>1</mn>
        <mo>-</mo>
        <mover accent="true">
         <mi>y</mi>
         <mo stretchy="false">^</mo>
        </mover>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <ci>H</ci>
      <interval closure="open">
       <ci>p</ci>
       <ci>q</ci>
      </interval>
     </apply>
     <apply>
      <minus></minus>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <ci>i</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>p</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <log></log>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>q</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <minus></minus>
      <apply>
       <minus></minus>
       <apply>
        <times></times>
        <ci>y</ci>
        <apply>
         <log></log>
         <apply>
          <ci>normal-^</ci>
          <ci>y</ci>
         </apply>
        </apply>
       </apply>
      </apply>
      <apply>
       <times></times>
       <apply>
        <minus></minus>
        <cn type="integer">1</cn>
        <ci>y</ci>
       </apply>
       <apply>
        <log></log>
        <apply>
         <minus></minus>
         <cn type="integer">1</cn>
         <apply>
          <ci>normal-^</ci>
          <ci>y</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(p,q)\ =\ -\sum_{i}p_{i}\log q_{i}\ =\ -y\log\hat{y}-(1-y)\log(1-\hat{y})
  </annotation>
 </semantics>
</math>

</p>

<p>The typical loss function that one uses in logistic regression is computed by taking the average of all cross-entropies in the sample. For specifically, suppose we have 

<math display="inline" id="Cross_entropy:70">
 <semantics>
  <mi>N</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>N</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N
  </annotation>
 </semantics>
</math>

 samples with each sample labeled by 

<math display="inline" id="Cross_entropy:71">
 <semantics>
  <mrow>
   <mi>n</mi>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <mi>N</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>n</ci>
    <list>
     <cn type="integer">1</cn>
     <ci>normal-…</ci>
     <ci>N</ci>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n=1,\dots,N
  </annotation>
 </semantics>
</math>

. The loss function is then given by:</p>

<p>

<math display="inline" id="Cross_entropy:72">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐰</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>L</ci>
    <ci>𝐰</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle L(\mathbf{w})
  </annotation>
 </semantics>
</math>


</p>

<p>where 

<math display="inline" id="Cross_entropy:73">
 <semantics>
  <mrow>
   <msub>
    <mover accent="true">
     <mi>y</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mi>n</mi>
   </msub>
   <mo>≡</mo>
   <mrow>
    <mi>g</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>𝐰</mi>
      <mo>⋅</mo>
      <msub>
       <mi>𝐱</mi>
       <mi>n</mi>
      </msub>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <equivalent></equivalent>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <ci>normal-^</ci>
      <ci>y</ci>
     </apply>
     <ci>n</ci>
    </apply>
    <apply>
     <times></times>
     <ci>g</ci>
     <apply>
      <ci>normal-⋅</ci>
      <ci>𝐰</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>𝐱</ci>
       <ci>n</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{y}_{n}\equiv g(\mathbf{w}\cdot\mathbf{x}_{n})
  </annotation>
 </semantics>
</math>

, with 

<math display="inline" id="Cross_entropy:74">
 <semantics>
  <mrow>
   <mi>g</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>z</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>g</ci>
    <ci>z</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g(z)
  </annotation>
 </semantics>
</math>

 the logistic function as before.</p>

<p>The logistic loss is sometimes called cross-entropy loss. It's also known as log loss (In this case, the binary label is often denoted by {-1,+1}).<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="references">References</h2>

<p><a href="http://eprints.eemcs.utwente.nl/7716/01/fulltext.pdf">De Boer, Pieter-Tjerk, et al. "A tutorial on the cross-entropy method." Annals of operations research 134.1 (2005): 19-67.</a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Cross-entropy_method" title="wikilink">Cross-entropy method</a></li>
<li><a href="Logistic_regression" title="wikilink">Logistic regression</a></li>
<li><a href="Conditional_entropy" title="wikilink">Conditional entropy</a></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.cse.unsw.edu.au/~billw/cs9444/crossentropy.html">What is cross-entropy, and why use it?</a></li>
</ul>

<p>"</p>

<p><a href="Category:Entropy_and_information" title="wikilink">Category:Entropy and information</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
</ol>
</section>
</body>
</html>
