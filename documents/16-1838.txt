   Draft:Hawkes process      Draft:Hawkes process   Hawkes process are used to study social network. The Hawkes model of   d   d   d   users is in figure 1:  Hawkes models is to quantify the tendency of any users to make actions in a social network. In order to achieve it, we would like to set up some notations as follows:         N  i    (  t  )    :=    ∑   j  ≥  1     1    t   i  ,  j    ≤  t        assign     subscript  N  i   t     subscript     j  1     subscript  1     subscript  t   i  j    t       N_{i}\left(t\right):=\sum_{j\geq 1}1_{t_{i,j}\leq t}   is a counting process of actions to all users made by a user i up to time t. We define a total count vectors.     N  t   =    [    N  1    (  t  )    ,  …  ,    N  d    (  t  )    ]   T   ∈   ℕ  d          subscript  N  t    superscript      subscript  N  1   t   normal-…     subscript  N  d   t    T         superscript  ℕ  d      N_{t}=\left[N_{1}\left(t\right),...,N_{d}\left(t\right)\right]^{T}\in\mathbb{N%
 }^{d}   for    t  ≥  0      t  0    t\geq 0     Let    A  =   (   a   i  j    )       A   subscript  a    i  j      A=\left(a_{ij}\right)   be a    d  ×  d      d  d    d\times d   matrix. Each entry     a   i  j    ≥  0       subscript  a    i  j    0    a_{ij}\geq 0   is a coefficient that quantifies the influence from a user   j   j   j   to a user   i   i   i   . Note   A   A   A   is an asymmetric matrix which makes sense in reality because influence from   i   i   i   to   j   j   j   is not equal to that from   j   j   j   to   i   i   i   in general.       μ  i   ≥  0       subscript  μ  i   0    \mu_{i}\geq 0   is a baseline intensity (basic intensity) which mean a user   i   i   i   's basic tendency to make actions without any effects by other users.       h   i  ,  j    :    ℝ  +   →   ℝ  +       normal-:   subscript  h   i  j     normal-→   superscript  ℝ     superscript  ℝ       h_{i,j}:\mathbb{R}^{+}\rightarrow\mathbb{R}^{+}   is a decay function accounting for an influence diminishing from a user   j   j   j   to a user   i   i   i   along time   t   t   t   . A common decay function      h   i  j     (  t  )    =   e   -    β   i  ,  j    t            subscript  h    i  j    t    superscript  e       subscript  β   i  j    t       h_{ij}\left(t\right)=e^{-\beta_{i,j}t}   .  If we let a parameter    θ  :=   (  μ  ,  A  )      assign  θ   μ  A     \theta:=\left(\mu,A\right)   .     λ   i  ,  θ     (  t  )        subscript  λ   i  θ    t    \lambda_{i,\theta}\left(t\right)   is an intensity of    N  i     subscript  N  i    N_{i}   at the time   t   t   t   depending on   θ   θ   \theta   . It can be interpreted as the rate/tendency for a user   j   j   j   to make a decision given   θ   θ   \theta   . The relations are described as follows      λ   i  ,  θ     (  t  )    :=    μ  i   +    ∫  0  t     ∑   j  =  1   d     a   i  ,  j     h   i  ,  j     (   t  -  s   )   d   N  j          assign     subscript  λ   i  θ    t      subscript  μ  i     superscript   subscript   0   t     superscript   subscript     j  1    d      subscript  a   i  j     subscript  h   i  j      t  s   d   subscript  N  j         \lambda_{i,\theta}\left(t\right):=\mu_{i}+\int_{0}^{t}\sum_{j=1}^{d}a_{i,j}h_{%
 i,j}\left(t-s\right)dN_{j}   and       λ   i  ,  θ     (  t  )    :=    lim   s  ↓  0      P   (   N  j    (  t  +  s  )   -  N   (  j  )   =  1  |   ℱ   t  -    )    s     .     assign     subscript  λ   i  θ    t     subscript    normal-↓  s  0       fragments  P   fragments  normal-(   subscript  N  j    fragments  normal-(  t   s  normal-)    N   fragments  normal-(  j  normal-)    1  normal-|   subscript  ℱ   superscript  t     normal-)    s      \lambda_{i,\theta}\left(t\right):=\lim_{s\downarrow 0}\frac{P\left(N_{j}\left(%
 t+s\right)-N\left(j\right)=1|\mathcal{F}_{t^{-}}\right)}{s}.      Minimization of Loss Function  Let     λ  *   =   (   λ  1  *   ,  …  ,   λ  d  *   )        superscript  λ      superscript   subscript  λ  1     normal-…   superscript   subscript  λ  d        \lambda^{*}=\left(\lambda_{1}^{*},...,\lambda_{d}^{*}\right)   be a true intensity of the Hawkes model. Note that    λ  *     superscript  λ     \lambda^{*}   may not satisfy the intensity formula in the Hawkes model. We define a loss function      R  T    (  θ  )    :=     ∥   λ  θ   ∥   T  2   -    2  T     ∑   i  =  1   d     ∫   [  0  ,  T  ]      λ   i  ,  θ     (  t  )   d   N  i    (  t  )           assign     subscript  R  T   θ      superscript   subscript   norm   subscript  λ  θ    T   2       2  T     superscript   subscript     i  1    d     subscript    0  T       subscript  λ   i  θ    t  d   subscript  N  i   t         R_{T}\left(\theta\right):=\left\|\lambda_{\theta}\right\|_{T}^{2}-\frac{2}{T}%
 \sum_{i=1}^{d}\int_{[0,T]}\lambda_{i,\theta}\left(t\right)dN_{i}\left(t\right)   where      ∥   λ  θ   ∥   T  2   :=    1  T     ∑   i  =  1   d     ∫  0  T     λ   i  ,  θ      (  t  )   2   d  t         assign   superscript   subscript   norm   subscript  λ  θ    T   2       1  T     superscript   subscript     i  1    d     superscript   subscript   0   T      subscript  λ   i  θ     superscript  t  2   d  t        \left\|\lambda_{\theta}\right\|_{T}^{2}:=\frac{1}{T}\sum_{i=1}^{d}\int_{0}^{T}%
 \lambda_{i,\theta}\left(t\right)^{2}dt   and      ⟨  λ  ,   λ    ′    ⟩   T   :=    1  T     ∑   i  =  1   d     ∫  0  T     λ  i    (  t  )    λ  i    ′     (  t  )   d  t         assign   subscript   λ   superscript  λ   normal-′     T       1  T     superscript   subscript     i  1    d     superscript   subscript   0   T      subscript  λ  i   t   superscript   subscript  λ  i    normal-′    t  d  t        \left\langle\lambda,\lambda^{{}^{\prime}}\right\rangle_{T}:=\frac{1}{T}\sum_{i%
 =1}^{d}\int_{0}^{T}\lambda_{i}\left(t\right)\lambda_{i}^{{}^{\prime}}\left(t%
 \right)dt   . Note that the loss function is random because    N  i     subscript  N  i    N_{i}   ,     λ   i  ,  θ     (  t  )        subscript  λ   i  θ    t    \lambda_{i,\theta}\left(t\right)   are random variables. Therefore,     E   [    R  T    (  θ  )    ]    =    E   (    ∥   λ  θ   ∥   T  2   )    -    2  T     ∑   i  =  1   d    E   (    ∫   [  0  ,  T  ]      λ   i  ,  θ     (  t  )   d   N  i    (  t  )     )             E   delimited-[]     subscript  R  T   θ         E   superscript   subscript   norm   subscript  λ  θ    T   2        2  T     superscript   subscript     i  1    d     E    subscript    0  T       subscript  λ   i  θ    t  d   subscript  N  i   t          E\left[R_{T}\left(\theta\right)\right]=E\left(\left\|\lambda_{\theta}\right\|_%
 {T}^{2}\right)-\frac{2}{T}\sum_{i=1}^{d}E\left(\int_{[0,T]}\lambda_{i,\theta}%
 \left(t\right)dN_{i}\left(t\right)\right)        =    E   (    ∥   λ  θ   ∥   T  2   )    -   2  E   (    1  T     ∑   i  =  1   d     ∫   [  0  ,  T  ]      λ   i  ,  θ     (  t  )    λ  i  *    (  t  )   d  t      )         absent      E   superscript   subscript   norm   subscript  λ  θ    T   2      2  E      1  T     superscript   subscript     i  1    d     subscript    0  T       subscript  λ   i  θ    t   superscript   subscript  λ  i     t  d  t          =E\left(\left\|\lambda_{\theta}\right\|_{T}^{2}\right)-2E\left(\frac{1}{T}\sum%
 _{i=1}^{d}\int_{[0,T]}\lambda_{i,\theta}\left(t\right)\lambda_{i}^{*}\left(t%
 \right)dt\right)         =    E   (    ∥    λ  θ   -   λ  *    ∥   T  2   )    +   E   (    ∥   λ  θ  *   ∥   T  2   )      .      absent      E   superscript   subscript   norm     subscript  λ  θ    superscript  λ      T   2      E   superscript   subscript   norm   superscript   subscript  λ  θ      T   2       =E\left(\left\|\lambda_{\theta}-\lambda^{*}\right\|_{T}^{2}\right)+E\left(%
 \left\|\lambda_{\theta}^{*}\right\|_{T}^{2}\right).   The second line is because     M   (  t  )    :=     N  i    (  t  )    -    λ  *    (  t  )        assign    M  t        subscript  N  i   t      superscript  λ    t      M\left(t\right):=N_{i}\left(t\right)-\lambda^{*}\left(t\right)   is a martingale. Therefore,     ∫   [  0  ,  T  ]      λ   i  ,  θ     (  t  )    λ  i  *    (  t  )   d  M   (  t  )        subscript    0  T       subscript  λ   i  θ    t   superscript   subscript  λ  i     t  d  M  t     \int_{[0,T]}\lambda_{i,\theta}\left(t\right)\lambda_{i}^{*}\left(t\right)dM%
 \left(t\right)   is a martingale with expectation   0   0    . From the calculation, the expectation of the loss function achieve minimum when     λ  θ   =   λ  *        subscript  λ  θ    superscript  λ      \lambda_{\theta}=\lambda^{*}   . We wold like to minimize     R  T    (  θ  )        subscript  R  T   θ    R_{T}\left(\theta\right)   to find the best estimator     θ  ,   ^     normal-^   fragments  θ  normal-,     \hat{\theta,}   but the matrix    A  ^     normal-^  A    \hat{A}   in the estimator    θ  ^     normal-^  θ    \hat{\theta}   may not be sparse which does not match the reality which most of a user ' members and the user are not active to each other. In order to get a sparse matrix    A  ^     normal-^  A    \hat{A}   , a trace norm or nuclear norm penalization is added to the loss function. Let    ∥  .   ∥  *      fragments  normal-∥  normal-.   subscript  normal-∥      \left\|.\right\|_{*}   be a trace/nuclear norm of a square matrix and      ∥  A  ∥   *   :=   t  r   (     A  *   A    )       assign   subscript   norm  A       t  r       superscript  A    A       \left\|A\right\|_{*}:=tr\left(\sqrt{A^{*}A}\right)   or it can be interpreted as a sum of all singular values of    A  .    A   A.   Therefore, what we minimize is the following     θ  ^   :=    argmin  θ    {     R  T    (  θ  )    +   τ    ∥  A  ∥   *     }       assign   normal-^  θ      subscript  argmin  θ         subscript  R  T   θ     τ   subscript   norm  A           \hat{\theta}:=\textrm{argmin}_{\theta}\left\{R_{T}\left(\theta\right)+\tau%
 \left\|A\right\|_{*}\right\}   where    τ  >  0      τ  0    \tau>0   is a smooth parameter that balance goodness-of-fit and penalization.  Remark: Any non-differentiable norm is a penalization for sparse solution.  Control of     λ   θ  ^    -   λ   θ  *         subscript  λ   normal-^  θ     subscript  λ   superscript  θ       \lambda_{\hat{\theta}}-\lambda_{\theta^{*}}     The main concern is how close the best estimator    λ   θ  ^      subscript  λ   normal-^  θ     \lambda_{\hat{\theta}}   is to the true intensity    λ   θ  *      subscript  λ   superscript  θ      \lambda_{\theta^{*}}   under    ∥  .   ∥  T      fragments  normal-∥  normal-.   subscript  normal-∥  T     \left\|.\right\|_{T}   . The ultimate goal is to control     ∥    λ   θ  ^    -   λ  θ  *    ∥   T     subscript   norm     subscript  λ   normal-^  θ     superscript   subscript  λ  θ       T    \left\|\lambda_{\hat{\theta}}-\lambda_{\theta}^{*}\right\|_{T}   by    θ  ^     normal-^  θ    \hat{\theta}   only which is yet to overcome. The details is in the paper 1 . "     Emmanuel Bacry, Stéphane Gaïffas, Jean-François Muzy, "Concentration for matrix martingales in continuous time and microscopic activity of social networks" http://xxx.tau.ac.il/abs/1412.7705 ↩     