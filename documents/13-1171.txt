   Margin Infused Relaxed Algorithm      Margin Infused Relaxed Algorithm   Margin-infused relaxed algorithm (MIRA) 1 is a machine learning algorithm, an online algorithm for multiclass classification problems. It is designed to learn a set of parameters (vector or matrix) by processing all the given training examples one-by-one and updating the parameters according to each training example, so that the current training example is classified correctly with a margin against incorrect classifications at least as large as their loss. 2 The change of the parameters is kept as small as possible.  A two-class version called binary MIRA 3 simplifies the algorithm by not requiring the solution of a quadratic programming problem (see below). When used in a one-vs.-all configuration, binary MIRA can be extended to a multiclass learner that approximates full MIRA, but may be faster to train.  The flow of the algorithm 4 5 looks as follows:   Input: Training examples     T  =   {   x  i   ,   y  i   }       T    subscript  x  i    subscript  y  i      T=\{x_{i},y_{i}\}      Output: Set of parameters    w   w   w        i   i   i   ← 0,    w   (  0  )      superscript  w  0    w^{(0)}   ← 0   for     n   n   n    ← 1 to     N   N   N      for     t   t   t    ← 1 to      |  T  |      T    |T|         w   (   i  +  1   )      superscript  w    i  1     w^{(i+1)}   ← update    w   (  i  )      superscript  w  i    w^{(i)}   according to    {   x  t   ,   y  t   }      subscript  x  t    subscript  y  t     \{x_{t},y_{t}\}      i   i   i   ←    i  +  1      i  1    i+1      end  for   end  for   return         ∑   j  =  1    N  ×   |  T  |       w   (  j  )      N  ×   |  T  |          superscript   subscript     j  1      N    T      superscript  w  j      N    T      \frac{\sum_{j=1}^{N\times|T|}w^{(j)}}{N\times|T|}     The update step is then formalized as a quadratic programming 6 problem: Find    m  i  n   ∥    w   (   i  +  1   )    -   w   (  i  )     ∥       m  i  n   norm     superscript  w    i  1     superscript  w  i       min\|w^{(i+1)}-w^{(i)}\|   , so that      s  c  o  r  e   (   x  t   ,   y  t   )    -   s  c  o  r  e   (   x  t   ,   y  ′   )     ≥   L   (   y  t   ,   y  ′   )    ∀   y  ′             s  c  o  r  e    subscript  x  t    subscript  y  t       s  c  o  r  e    subscript  x  t    superscript  y  normal-′        L    subscript  y  t    superscript  y  normal-′     for-all   superscript  y  normal-′       score(x_{t},y_{t})-score(x_{t},y^{\prime})\geq L(y_{t},y^{\prime})\ \forall y^%
 {\prime}   , i.e. the score of the current correct training   y   y   y   must be greater than the score of any other possible    y  ′     superscript  y  normal-′    y^{\prime}   by at least the loss (number of errors) of that    y  ′     superscript  y  normal-′    y^{\prime}   in comparison to   y   y   y   .  References  External links   adMIRAble - MIRA implementation in C++  Miralium - MIRA implementation in Java  MIRA implementation for Mahout in Hadoop   "  Category:Classification algorithms     ↩  ↩   Watanabe, T. et al (2007): "Online Large Margin Training for Statistical Machine Translation". In: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , 764–773. ↩  Bohnet, B. (2009): Efficient Parsing of Syntactic and Semantic Dependency Structures . Proceedings of Conference on Natural Language Learning (CoNLL), Boulder, 67–72. ↩      