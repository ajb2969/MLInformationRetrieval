   Arbitrarily varying channel      Arbitrarily varying channel   An arbitrarily varying channel (AVC) is a communication channel model used in coding theory , and was first introduced by Blackwell, Breiman, and Thomasian. This particular channel has unknown parameters that can change over time and these changes may not have a uniform pattern during the transmission of a codeword .   n   n   \textstyle n   uses of this channel can be described using a stochastic matrix      W  n   :   X  n   ×     fragments   superscript  W  n   normal-:   superscript  X  n      \textstyle W^{n}:X^{n}\times        S  n   →   Y  n      normal-→   superscript  S  n    superscript  Y  n     \textstyle S^{n}\rightarrow Y^{n}   , where   X   X   \textstyle X   is the input alphabet,   Y   Y   \textstyle Y   is the output alphabet, and     W  n    (  y  |  x  ,  s  )      fragments   superscript  W  n    fragments  normal-(  y  normal-|  x  normal-,  s  normal-)     \textstyle W^{n}(y|x,s)   is the probability over a given set of states   S   S   \textstyle S   , that the transmitted input    x  =   (   x  1   ,  …  ,   x  n   )       x    subscript  x  1   normal-…   subscript  x  n      \textstyle x=(x_{1},\ldots,x_{n})   leads to the received output    y  =   (   y  1   ,  …  ,   y  n   )       y    subscript  y  1   normal-…   subscript  y  n      \textstyle y=(y_{1},\ldots,y_{n})   . The state    s  i     subscript  s  i    \textstyle s_{i}   in set   S   S   \textstyle S   can vary arbitrarily at each time unit   i   i   \textstyle i   . This channel was developed as an alternative to Shannon's  Binary Symmetric Channel (BSC), where the entire nature of the channel is known, to be more realistic to actual network channel situations.  Capacities and associated proofs  Capacity of deterministic AVCs  An AVC's capacity can vary depending on the certain parameters.     R   R   \textstyle R   is an achievable rate for a deterministic AVC code if it is larger than   0   0   \textstyle 0   , and if for every positive   ε   ε   \textstyle\varepsilon   and   δ   δ   \textstyle\delta   , and very large   n   n   \textstyle n   , length-   n   n   \textstyle n    block codes exist that satisfy the following equations       1  n     log  N    >   R  -  δ           1  n     N      R  δ     \textstyle\frac{1}{n}\log N>R-\delta   and       max   s  ∈   S  n      e  ¯     (  s  )    ≤  ε          subscript     s   superscript  S  n      normal-¯  e    s   ε    \displaystyle\max_{s\in S^{n}}\bar{e}(s)\leq\varepsilon   , where   N   N   \textstyle N   is the highest value in   Y   Y   \textstyle Y   and where     e  ¯    (  s  )        normal-¯  e   s    \textstyle\bar{e}(s)   is the average probability of error for a state sequence   s   s   \textstyle s   . The largest rate    R   R   \textstyle R   represents the capacity of the AVC, denoted by   c   c   \textstyle c   .  As you can see, the only useful situations are when the capacity of the AVC is greater than   0   0   \textstyle 0   , because then the channel can transmit a guaranteed amount of data     ≤  c      absent  c    \textstyle\leq c   without errors. So we start out with a theorem that shows when   c   c   \textstyle c   is positive in a AVC and the theorems discussed afterward will narrow down the range of   c   c   \textstyle c   for different circumstances.  Before stating Theorem 1, a few definitions need to be addressed:   An AVC is symmetric if      ∑   s  ∈  S     W   (  y  |  x  ,  s  )   U   (  s  |   x  ′   )   =    ∑   s  ∈  S     W   (  y  |   x  ′   ,  s  )   U   (  s  |  x  )      fragments   subscript     s  S    W   fragments  normal-(  y  normal-|  x  normal-,  s  normal-)   U   fragments  normal-(  s  normal-|   superscript  x  normal-′   normal-)     subscript     s  S    W   fragments  normal-(  y  normal-|   superscript  x  normal-′   normal-,  s  normal-)   U   fragments  normal-(  s  normal-|  x  normal-)     \displaystyle\sum_{s\in S}W(y|x,s)U(s|x^{\prime})=\sum_{s\in S}W(y|x^{\prime},%
 s)U(s|x)   for every    (  x  ,   x  ′   ,  y  ,  s  )     x   superscript  x  normal-′   y  s    \textstyle(x,x^{\prime},y,s)   , where     x  ,   x  ′    ∈  X       x   superscript  x  normal-′    X    \textstyle x,x^{\prime}\in X   ,    y  ∈  Y      y  Y    \textstyle y\in Y   , and    U   (  s  |  x  )      fragments  U   fragments  normal-(  s  normal-|  x  normal-)     \textstyle U(s|x)   is a channel function    U  :   X  →  S      normal-:  U   normal-→  X  S     \textstyle U:X\rightarrow S   .      X  r     subscript  X  r    \textstyle X_{r}   ,    S  r     subscript  S  r    \textstyle S_{r}   , and    Y  r     subscript  Y  r    \textstyle Y_{r}   are all random variables in sets   X   X   \textstyle X   ,   S   S   \textstyle S   , and   Y   Y   \textstyle Y   respectively.       P   X  r     (  x  )        subscript  P   subscript  X  r    x    \textstyle P_{X_{r}}(x)   is equal to the probability that the random variable     X  r     subscript  X  r    \textstyle X_{r}   is equal to   x   x   \textstyle x   .       P   S  r     (  s  )        subscript  P   subscript  S  r    s    \textstyle P_{S_{r}}(s)   is equal to the probability that the random variable     S  r     subscript  S  r    \textstyle S_{r}   is equal to   s   s   \textstyle s   .      P    X  r    S  r    Y  r       subscript  P     subscript  X  r    subscript  S  r    subscript  Y  r      \textstyle P_{X_{r}S_{r}Y_{r}}   is the combined probability mass function (pmf) of     P   X  r     (  x  )        subscript  P   subscript  X  r    x    \textstyle P_{X_{r}}(x)   ,     P   S  r     (  s  )        subscript  P   subscript  S  r    s    \textstyle P_{S_{r}}(s)   , and    W   (  y  |  x  ,  s  )      fragments  W   fragments  normal-(  y  normal-|  x  normal-,  s  normal-)     \textstyle W(y|x,s)   .    P    X  r    S  r    Y  r       subscript  P     subscript  X  r    subscript  S  r    subscript  Y  r      \textstyle P_{X_{r}S_{r}Y_{r}}   is defined formally as     P    X  r    S  r    Y  r      (  x  ,  s  ,  y  )   =   P   X  r     (  x  )    P   S  r     (  s  )   W   (  y  |  x  ,  s  )      fragments   subscript  P     subscript  X  r    subscript  S  r    subscript  Y  r      fragments  normal-(  x  normal-,  s  normal-,  y  normal-)     subscript  P   subscript  X  r     fragments  normal-(  x  normal-)    subscript  P   subscript  S  r     fragments  normal-(  s  normal-)   W   fragments  normal-(  y  normal-|  x  normal-,  s  normal-)     \textstyle P_{X_{r}S_{r}Y_{r}}(x,s,y)=P_{X_{r}}(x)P_{S_{r}}(s)W(y|x,s)   .      H   (   X  r   )       H   subscript  X  r     \textstyle H(X_{r})   is the entropy of    X  r     subscript  X  r    \textstyle X_{r}   .      H   (   X  r   |   Y  r   )      fragments  H   fragments  normal-(   subscript  X  r   normal-|   subscript  Y  r   normal-)     \textstyle H(X_{r}|Y_{r})   is equal to the average probability that    X  r     subscript  X  r    \textstyle X_{r}   will be a certain value based on all the values    Y  r     subscript  Y  r    \textstyle Y_{r}   could possibly be equal to.      I   (    X  r   ∧   Y  r    )       I     subscript  X  r    subscript  Y  r      \textstyle I(X_{r}\land Y_{r})   is the mutual information of    X  r     subscript  X  r    \textstyle X_{r}   and    Y  r     subscript  Y  r    \textstyle Y_{r}   , and is equal to    H   (   X  r   )   -  H   (   X  r   |   Y  r   )      fragments  H   fragments  normal-(   subscript  X  r   normal-)    H   fragments  normal-(   subscript  X  r   normal-|   subscript  Y  r   normal-)     \textstyle H(X_{r})-H(X_{r}|Y_{r})   .       I   (  P  )    =     min   Y  r    I    (    X  r   ∧   Y  r    )          I  P       subscript    subscript  Y  r    I      subscript  X  r    subscript  Y  r       \displaystyle I(P)=\min_{Y_{r}}I(X_{r}\land Y_{r})   , where the minimum is over all random variables    Y  r     subscript  Y  r    \textstyle Y_{r}   such that    X  r     subscript  X  r    \textstyle X_{r}   ,    S  r     subscript  S  r    \textstyle S_{r}   , and    Y  r     subscript  Y  r    \textstyle Y_{r}   are distributed in the form of    P    X  r    S  r    Y  r       subscript  P     subscript  X  r    subscript  S  r    subscript  Y  r      \textstyle P_{X_{r}S_{r}Y_{r}}   .   Theorem 1:     c  >  0      c  0    \textstyle c>0   if and only if the AVC is not symmetric. If    c  >  0      c  0    \textstyle c>0   , then    c  =     max  P   I    (  P  )        c      subscript   P   I   P     \displaystyle c=\max_{P}I(P)   .  Proof of 1st part for symmetry: If we can prove that    I   (  P  )       I  P    \textstyle I(P)   is positive when the AVC is not symmetric, and then prove that    c  =     max  P   I    (  P  )        c      subscript   P   I   P     \textstyle c=\max_{P}I(P)   , we will be able to prove Theorem 1. Assume    I   (  P  )       I  P    \textstyle I(P)   were equal to   0   0   \textstyle 0   . From the definition of    I   (  P  )       I  P    \textstyle I(P)   , this would make    X  r     subscript  X  r    \textstyle X_{r}   and    Y  r     subscript  Y  r    \textstyle Y_{r}    independent  random variables , for some    S  r     subscript  S  r    \textstyle S_{r}   , because this would mean that neither random variable 's entropy would rely on the other random variable 's value. By using equation    P    X  r    S  r    Y  r       subscript  P     subscript  X  r    subscript  S  r    subscript  Y  r      \textstyle P_{X_{r}S_{r}Y_{r}}   , (and remembering     P   X  r    =  P       subscript  P   subscript  X  r    P    \textstyle P_{X_{r}}=P   ,) we can get,       P   Y  r     (  y  )   =   ∑   x  ∈  X     ∑   s  ∈  S    P   (  x  )    P   S  r     (  s  )   W   (  y  |  x  ,  s  )      fragments   subscript  P   subscript  Y  r     fragments  normal-(  y  normal-)     subscript     x  X     subscript     s  S    P   fragments  normal-(  x  normal-)    subscript  P   subscript  S  r     fragments  normal-(  s  normal-)   W   fragments  normal-(  y  normal-|  x  normal-,  s  normal-)     \displaystyle P_{Y_{r}}(y)=\sum_{x\in X}\sum_{s\in S}P(x)P_{S_{r}}(s)W(y|x,s)         ≡  (     fragments   normal-(    \textstyle\equiv(   since    X  r     subscript  X  r    \textstyle X_{r}   and    Y  r     subscript  Y  r    \textstyle Y_{r}   are independent  random variables ,    W   (  y  |  x  ,  s  )   =   W  ′    (  y  |  s  )      fragments  W   fragments  normal-(  y  normal-|  x  normal-,  s  normal-)     superscript  W  normal-′    fragments  normal-(  y  normal-|  s  normal-)     \textstyle W(y|x,s)=W^{\prime}(y|s)   for some     W  ′   )     fragments   superscript  W  normal-′   normal-)    \textstyle W^{\prime})          P   Y  r     (  y  )   =   ∑   x  ∈  X     ∑   s  ∈  S    P   (  x  )    P   S  r     (  s  )    W  ′    (  y  |  s  )      fragments   subscript  P   subscript  Y  r     fragments  normal-(  y  normal-)     subscript     x  X     subscript     s  S    P   fragments  normal-(  x  normal-)    subscript  P   subscript  S  r     fragments  normal-(  s  normal-)    superscript  W  normal-′    fragments  normal-(  y  normal-|  s  normal-)     \displaystyle P_{Y_{r}}(y)=\sum_{x\in X}\sum_{s\in S}P(x)P_{S_{r}}(s)W^{\prime%
 }(y|s)         ≡  (     fragments   normal-(    \textstyle\equiv(   because only    P   (  x  )       P  x    \textstyle P(x)   depends on   x   x   \textstyle x   now   )   normal-)   \textstyle)          P   Y  r     (  y  )   =   ∑   s  ∈  S     P   S  r     (  s  )    W  ′    (  y  |  s  )    [   ∑   x  ∈  X    P   (  x  )   ]      fragments   subscript  P   subscript  Y  r     fragments  normal-(  y  normal-)     subscript     s  S     subscript  P   subscript  S  r     fragments  normal-(  s  normal-)    superscript  W  normal-′    fragments  normal-(  y  normal-|  s  normal-)    fragments  normal-[   subscript     x  X    P   fragments  normal-(  x  normal-)   normal-]     \displaystyle P_{Y_{r}}(y)=\sum_{s\in S}P_{S_{r}}(s)W^{\prime}(y|s)\left[\sum_%
 {x\in X}P(x)\right]         ≡  (     fragments   normal-(    \textstyle\equiv(   because      ∑   x  ∈  X     P   (  x  )   =  1  )     fragments   subscript     x  X    P   fragments  normal-(  x  normal-)    1  normal-)    \displaystyle\sum_{x\in X}P(x)=1)          P   Y  r     (  y  )   =   ∑   s  ∈  S     P   S  r     (  s  )    W  ′    (  y  |  s  )      fragments   subscript  P   subscript  Y  r     fragments  normal-(  y  normal-)     subscript     s  S     subscript  P   subscript  S  r     fragments  normal-(  s  normal-)    superscript  W  normal-′    fragments  normal-(  y  normal-|  s  normal-)     \displaystyle P_{Y_{r}}(y)=\sum_{s\in S}P_{S_{r}}(s)W^{\prime}(y|s)     So now we have a probability distribution on    Y  r     subscript  Y  r    \textstyle Y_{r}   that is independent of    X  r     subscript  X  r    \textstyle X_{r}   . So now the definition of a symmetric AVC can be rewritten as follows     ∑   s  ∈  S     W  ′    (  y  |  s  )    P   S  r     (  s  )   =   ∑   s  ∈  S     W  ′    (  y  |  s  )    P   S  r     (  s  )      fragments   subscript     s  S     superscript  W  normal-′    fragments  normal-(  y  normal-|  s  normal-)    subscript  P   subscript  S  r     fragments  normal-(  s  normal-)     subscript     s  S     superscript  W  normal-′    fragments  normal-(  y  normal-|  s  normal-)    subscript  P   subscript  S  r     fragments  normal-(  s  normal-)     \displaystyle\sum_{s\in S}W^{\prime}(y|s)P_{S_{r}}(s)=\sum_{s\in S}W^{\prime}(%
 y|s)P_{S_{r}}(s)   since    U   (  s  |  x  )      fragments  U   fragments  normal-(  s  normal-|  x  normal-)     \textstyle U(s|x)   and    W   (  y  |  x  ,  s  )      fragments  W   fragments  normal-(  y  normal-|  x  normal-,  s  normal-)     \textstyle W(y|x,s)   are both functions based on   x   x   \textstyle x   , they have been replaced with functions based on   s   s   \textstyle s   and   y   y   \textstyle y   only. As you can see, both sides are now equal to the     P   Y  r     (  y  )        subscript  P   subscript  Y  r    y    \textstyle P_{Y_{r}}(y)   we calculated earlier, so the AVC is indeed symmetric when    I   (  P  )       I  P    \textstyle I(P)   is equal to   0   0   \textstyle 0   . Therefore    I   (  P  )       I  P    \textstyle I(P)   can only be positive if the AVC is not symmetric.  Proof of second part for capacity : See the paper "The capacity of the arbitrarily varying channel revisited: positivity, constraints," referenced below for full proof.  Capacity of AVCs with input and state constraints  The next theorem will deal with the capacity for AVCs with input and/or state constraints. These constraints help to decrease the very large range of possibilities for transmission and error on an AVC, making it a bit easier to see how the AVC behaves.  Before we go on to Theorem 2, we need to define a few definitions and lemmas :  For such AVCs, there exists: :- An input constraint   Γ   normal-Γ   \textstyle\Gamma   based on the equation     g   (  x  )    =     1  n       ∑   i  =  1   n     g   (   x  i   )            g  x       1  n     superscript   subscript     i  1    n     g   subscript  x  i        \displaystyle g(x)=\frac{1}{n}\sum_{i=1}^{n}g(x_{i})   , where    x  ∈  X      x  X    \textstyle x\in X   and    x  =   (   x  1   ,  …  ,   x  n   )       x    subscript  x  1   normal-…   subscript  x  n      \textstyle x=(x_{1},\dots,x_{n})   .   - A state constraint   Λ   normal-Λ   \textstyle\Lambda   , based on the equation     l   (  s  )    =     1  n       ∑   i  =  1   n     l   (   s  i   )            l  s       1  n     superscript   subscript     i  1    n     l   subscript  s  i        \displaystyle l(s)=\frac{1}{n}\sum_{i=1}^{n}l(s_{i})   , where    s  ∈  X      s  X    \textstyle s\in X   and    s  =   (   s  1   ,  …  ,   s  n   )       s    subscript  s  1   normal-…   subscript  s  n      \textstyle s=(s_{1},\dots,s_{n})   .  -      Λ  0    (  P  )    =   min     ∑    x  ∈  X   ,   s  ∈  S       P   (  x  )   l   (  s  )             subscript  normal-Λ  0   P        subscript    formulae-sequence    x  X     s  S       P  x  l  s       \displaystyle\Lambda_{0}(P)=\min\sum_{x\in X,s\in S}P(x)l(s)     -    I   (  P  ,  Λ  )       I   P  normal-Λ     \textstyle I(P,\Lambda)   is very similar to    I   (  P  )       I  P    \textstyle I(P)   equation mentioned previously,     I   (  P  ,  Λ  )    =     min   Y  r    I    (    X  r   ∧   Y  r    )          I   P  normal-Λ        subscript    subscript  Y  r    I      subscript  X  r    subscript  Y  r       \displaystyle I(P,\Lambda)=\min_{Y_{r}}I(X_{r}\land Y_{r})   , but now any state   s   s   \textstyle s   or    S  r     subscript  S  r    \textstyle S_{r}   in the equation must follow the     l   (  s  )    ≤  Λ        l  s   normal-Λ    \textstyle l(s)\leq\Lambda   state restriction.   Assume    g   (  x  )       g  x    \textstyle g(x)   is a given non-negative-valued function on   X   X   \textstyle X   and    l   (  s  )       l  s    \textstyle l(s)   is a given non-negative-valued function on   S   S   \textstyle S   and that the minimum values for both is   0   0   \textstyle 0   . In the literature I have read on this subject, the exact definitions of both    g   (  x  )       g  x    \textstyle g(x)   and    l   (  s  )       l  s    \textstyle l(s)   (for one variable    x  i     subscript  x  i    \textstyle x_{i}   ,) is never described formally. The usefulness of the input constraint   Γ   normal-Γ   \textstyle\Gamma   and the state constraint   Λ   normal-Λ   \textstyle\Lambda   will be based on these equations.  For AVCs with input and/or state constraints, the rate    R   R   \textstyle R   is now limited to codewords of format     x  1   ,  …  ,   x  N       subscript  x  1   normal-…   subscript  x  N     \textstyle x_{1},\dots,x_{N}   that satisfy     g   (   x  i   )    ≤  Γ        g   subscript  x  i    normal-Γ    \textstyle g(x_{i})\leq\Gamma   , and now the state   s   s   \textstyle s   is limited to all states that satisfy     l   (  s  )    ≤  Λ        l  s   normal-Λ    \textstyle l(s)\leq\Lambda   . The largest rate is still considered the capacity of the AVC, and is now denoted as    c   (  Γ  ,  Λ  )       c   normal-Γ  normal-Λ     \textstyle c(\Gamma,\Lambda)   .  Lemma 1: Any codes where   Λ   normal-Λ   \textstyle\Lambda   is greater than     Λ  0    (  P  )        subscript  normal-Λ  0   P    \textstyle\Lambda_{0}(P)   cannot be considered "good" codes , because those kinds of codes have a maximum average probability of error greater than or equal to      N  -  1    2  N    -    1  n     l   m  a  x   2    n    (   Λ  -    Λ  0    (  P  )     )   2              N  1     2  N        1  n      superscript   subscript  l    m  a  x    2     n   superscript    normal-Λ     subscript  normal-Λ  0   P    2        \textstyle\frac{N-1}{2N}-\frac{1}{n}\frac{l_{max}^{2}}{n(\Lambda-\Lambda_{0}(P%
 ))^{2}}   , where    l   m  a  x      subscript  l    m  a  x     \textstyle l_{max}   is the maximum value of    l   (  s  )       l  s    \textstyle l(s)   . This isn't a good maximum average error probability because it is fairly large,     N  -  1    2  N         N  1     2  N     \textstyle\frac{N-1}{2N}   is close to    1  2      1  2    \textstyle\frac{1}{2}   , and the other part of the equation will be very small since the    (   Λ  -    Λ  0    (  P  )     )      normal-Λ     subscript  normal-Λ  0   P     \textstyle(\Lambda-\Lambda_{0}(P))   value is squared, and   Λ   normal-Λ   \textstyle\Lambda   is set to be larger than     Λ  0    (  P  )        subscript  normal-Λ  0   P    \textstyle\Lambda_{0}(P)   . Therefore it would be very unlikely to receive a codeword without error. This is why the     Λ  0    (  P  )        subscript  normal-Λ  0   P    \textstyle\Lambda_{0}(P)   condition is present in Theorem 2.  Theorem 2: Given a positive   Λ   normal-Λ   \textstyle\Lambda   and arbitrarily small    α  >  0      α  0    \textstyle\alpha>0   ,    β  >  0      β  0    \textstyle\beta>0   ,    δ  >  0      δ  0    \textstyle\delta>0   , for any block length    n  ≥   n  0       n   subscript  n  0     \textstyle n\geq n_{0}   and for any type   P   P   \textstyle P   with conditions      Λ  0    (  P  )    ≥   Λ  +  α          subscript  normal-Λ  0   P     normal-Λ  α     \textstyle\Lambda_{0}(P)\geq\Lambda+\alpha   and       min   x  ∈  X    P    (  x  )    ≥  β          subscript     x  X    P   x   β    \displaystyle\min_{x\in X}P(x)\geq\beta   , and where     P   X  r    =  P       subscript  P   subscript  X  r    P    \textstyle P_{X_{r}}=P   , there exists a code with codewords      x  1   ,  …  ,   x  N       subscript  x  1   normal-…   subscript  x  N     \textstyle x_{1},\dots,x_{N}   , each of type   P   P   \textstyle P   , that satisfy the following equations       1  n     log  N    >    I   (  P  ,  Λ  )    -  δ           1  n     N        I   P  normal-Λ    δ     \textstyle\frac{1}{n}\log N>I(P,\Lambda)-\delta   ,       max    l   (  s  )    ≤  Λ     e  ¯     (  s  )    ≤   exp   (   -   n  γ    )            subscript       l  s   normal-Λ     normal-¯  e    s         n  γ       \displaystyle\max_{l(s)\leq\Lambda}\bar{e}(s)\leq\exp(-n\gamma)   , and where positive    n  0     subscript  n  0    \textstyle n_{0}   and   γ   γ   \textstyle\gamma   depend only on   α   α   \textstyle\alpha   ,   β   β   \textstyle\beta   ,   δ   δ   \textstyle\delta   , and the given AVC.  Proof of Theorem 2 : See the paper "The capacity of the arbitrarily varying channel revisited: positivity, constraints," referenced below for full proof.  Capacity of randomized AVCs  The next theorem will be for AVCs with randomized  code . For such AVCs the code is a random variable with values from a family of length-n block codes , and these codes are not allowed to depend/rely on the actual value of the codeword . These codes have the same maximum and average error probability value for any channel because of its random nature. These types of codes also help to make certain properties of the AVC more clear.  Before we go on to Theorem 3, we need to define a couple important terms first:       W  ζ    (  y  |  x  )   =    ∑   s  ∈  S     W   (  y  |  x  ,  s  )    P   S  r     (  s  )      fragments   subscript  W  ζ    fragments  normal-(  y  normal-|  x  normal-)     subscript     s  S    W   fragments  normal-(  y  normal-|  x  normal-,  s  normal-)    subscript  P   subscript  S  r     fragments  normal-(  s  normal-)     \displaystyle W_{\zeta}(y|x)=\sum_{s\in S}W(y|x,s)P_{S_{r}}(s)        I   (  P  ,  ζ  )       I   P  ζ     \textstyle I(P,\zeta)   is very similar to the    I   (  P  )       I  P    \textstyle I(P)   equation mentioned previously,     I   (  P  ,  ζ  )    =     min   Y  r    I    (    X  r   ∧   Y  r    )          I   P  ζ        subscript    subscript  Y  r    I      subscript  X  r    subscript  Y  r       \displaystyle I(P,\zeta)=\min_{Y_{r}}I(X_{r}\land Y_{r})   , but now the pmf      P   S  r     (  s  )        subscript  P   subscript  S  r    s    \textstyle P_{S_{r}}(s)   is added to the equation, making the minimum of    I   (  P  ,  ζ  )       I   P  ζ     \textstyle I(P,\zeta)   based a new form of    P    X  r    S  r    Y  r       subscript  P     subscript  X  r    subscript  S  r    subscript  Y  r      \textstyle P_{X_{r}S_{r}Y_{r}}   , where     W  ζ    (  y  |  x  )      fragments   subscript  W  ζ    fragments  normal-(  y  normal-|  x  normal-)     \textstyle W_{\zeta}(y|x)   replaces    W   (  y  |  x  ,  s  )      fragments  W   fragments  normal-(  y  normal-|  x  normal-,  s  normal-)     \textstyle W(y|x,s)   .  Theorem 3: The capacity for randomized  codes of the AVC is    c  =   m  a   x  P   I   (  P  ,  ζ  )        c    m  a   subscript  x  P   I   P  ζ      \displaystyle c=max_{P}I(P,\zeta)   .  Proof of Theorem 3 : See paper "The Capacities of Certain Channel Classes Under Random Coding" referenced below for full proof.  See also   Binary symmetric channel  Binary erasure channel  Z-channel (information theory)  Channel model  Information theory  Coding theory   References   Ahlswede, Rudolf and Blinovsky, Vladimir, "Classical Capacity of Classical-Quantum Arbitrarily Varying Channels," http://ieeexplore.ieee.org.gate.lib.buffalo.edu/stamp/stamp.jsp?tp =&arnumber;=4069128  Blackwell, David, Breiman, Leo, and Thomasian, A. J., "The Capacities of Certain Channel Classes Under Random Coding," http://www.jstor.org/stable/2237566  Csiszar, I. and Narayan, P., "Arbitrarily varying channels with constrained inputs and states," http://ieeexplore.ieee.org/stamp/stamp.jsp?tp =&arnumber;=2598&isnumber;=154  Csiszar, I. and Narayan, P., "Capacity and Decoding Rules for Classes of Arbitrarily Varying Channels," http://ieeexplore.ieee.org/stamp/stamp.jsp?tp =&arnumber;=32153&isnumber;=139  Csiszar, I. and Narayan, P., "The capacity of the arbitrarily varying channel revisited: positivity, constraints," http://ieeexplore.ieee.org/stamp/stamp.jsp?tp =&arnumber;=2627&isnumber;=155  Lapidoth, A. and Narayan, P., "Reliable communication under channel uncertainty," http://ieeexplore.ieee.org/stamp/stamp.jsp?tp =&arnumber;=720535&isnumber;=15554   "  Category:Coding theory   