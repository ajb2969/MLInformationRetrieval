   Mean shift      Mean shift   Mean shift is a non-parametric  feature-space analysis technique for locating the maxima of a density function , a so-called mode -seeking algorithm. 1 Application domains include cluster analysis in computer vision and image processing . 2  History  The mean shift procedure was originally presented in 1975 by Fukunaga and Hostetler. 3  Overview  Mean shift is a procedure for locating the maxima of a density function given discrete data sampled from that function. 4 It is useful for detecting the modes of this density. 5 This is an iterative method, and we start with an initial estimate   x   x   x   . Let a kernel function     K   (    x  i   -  x   )       K     subscript  x  i   x     K(x_{i}-x)   be given. This function determines the weight of nearby points for re-estimation of the mean. Typically a Gaussian kernel on the distance to the current estimate is used,     K   (    x  i   -  x   )    =   e   -   c    ||    x  i   -  x   ||   2            K     subscript  x  i   x     superscript  e      c   superscript   norm     subscript  x  i   x    2        K(x_{i}-x)=e^{-c||x_{i}-x||^{2}}   . The weighted mean of the density in the window determined by   K   K   K        m   (  x  )    =      ∑    x  i   ∈   N   (  x  )        K   (    x  i   -  x   )    x  i        ∑    x  i   ∈   N   (  x  )        K   (    x  i   -  x   )            m  x       subscript      subscript  x  i     N  x       K     subscript  x  i   x    subscript  x  i       subscript      subscript  x  i     N  x       K     subscript  x  i   x        m(x)=\frac{\sum_{x_{i}\in N(x)}K(x_{i}-x)x_{i}}{\sum_{x_{i}\in N(x)}K(x_{i}-x)}     where    N   (  x  )       N  x    N(x)   is the neighborhood of   x   x   x   , a set of points for which     K   (  x  )    ≠  0        K  x   0    K(x)\neq 0   .  The mean-shift algorithm now sets    x  ←   m   (  x  )       normal-←  x    m  x     x\leftarrow m(x)   , and repeats the estimation until    m   (  x  )       m  x    m(x)   converges.  Details  Let data be a finite set S embedded in the n-dimensional Euclidean space, X. Let K be a flat kernel that is the characteristic function of the   λ   λ   \lambda   -ball in X,        K   (  x  )    =   {     1       if    ∥  x  ∥    ≤  λ       0       if    ∥  x  ∥    >  λ             K  x    cases  1      if   norm  x    λ   0      if   norm  x    λ      K(x)=\begin{cases}1&\text{if}\ \|x\|\leq\lambda\\
 0&\text{if}\ \|x\|>\lambda\\
 \end{cases}      The difference     m   (  x  )    -  x        m  x   x    m(x)-x   is called mean shift in Fukunaga and Hostetler. 6 The repeated movement of data points to the sample means is called the mean shift algorithm . In each iteration of the algorithm,    s  ←   m   (  s  )       normal-←  s    m  s     s\leftarrow m(s)   is performed for all    s  ∈  S      s  S    s\in S   simultaneously. The first question, then, is how to estimate the density function given a sparse set of samples. One of the simplest approaches is to just smooth the data, e.g., by convolving it with a fixed kernel of width   h   h   h   ,        f   (  x  )    =    ∑  i    K   (   x  -   x  i    )     =    ∑  i    k   (     ∥   x  -   x  i    ∥   2    h  2    )             f  x     subscript   i     K    x   subscript  x  i             subscript   i     k     superscript   norm    x   subscript  x  i     2    superscript  h  2         f(x)=\sum_{i}K(x-x_{i})=\sum_{i}k\left(\frac{\|x-x_{i}\|^{2}}{h^{2}}\right)      where    x  i     subscript  x  i    x_{i}   are the input samples and    k   (  r  )       k  r    k(r)   is the kernel function (or Parzen window ). h is the only parameter in the algorithm and is called the bandwidth. This approach is known as kernel density estimation or the Parzen window technique. Once we have computed    f   (  x  )       f  x    f(x)   from equation above, we can find its local maxima using gradient ascent or some other optimization technique. The problem with this "brute force" approach is that, for higher dimensions, it becomes computationally prohibitive to evaluate    f   (  x  )       f  x    f(x)   over the complete search space. Instead, mean shift uses a variant of what is known in the optimization literature as multiple restart gradient descent . Starting at some guess for a local maximum,    y  k     subscript  y  k    y_{k}   , which can be a random input data point    x  1     subscript  x  1    x_{1}   , mean shift computes the gradient of the density estimate    f   (  x  )       f  x    f(x)   at    y  k     subscript  y  k    y_{k}   and takes an uphill step in that direction.  Types of kernels  Kernel definition: Let X be the n-dimensional Euclidean space,    R  n     superscript  R  n    R^{n}   . Denote the ith component of x by    x  i     subscript  x  i    x_{i}   . The norm of x is a non-negative number.      ∥  x  ∥   2   =    x  T   x        superscript   norm  x   2      superscript  x  T   x     \|x\|^{2}=x^{T}x   A function K    X  ←  R     normal-←  X  R    X\leftarrow R   is said to be a kernel if there exists a profile,    k  :    [  0  ,  ∞  ]   →  R      normal-:  k   normal-→   0    R     k:[0,\infty]\rightarrow R   , such that       K   (  x  )    =   k   (    ∥  x  ∥   2   )          K  x     k   superscript   norm  x   2      K(x)=k(\|x\|^{2})   and   k is non-negative.  k is nonincreasing     k   (  a  )    ≥   k   (  b  )          k  a     k  b     k(a)\geq k(b)   if    a  <  b      a  b    a   .  k is piecewise continuous and      ∫  0  ∞    k   (  r  )   d  r    <  ∞        superscript   subscript   0       k  r  d  r       \int_{0}^{\infty}k(r)\,dr<\infty      The two frequently used kernels for mean shift are:   Flat kernel         F   (  x  )    =   {     1       if    ∥  x  ∥    ≤  λ       0       if    ∥  x  ∥    >  λ             F  x    cases  1      if   norm  x    λ   0      if   norm  x    λ      F(x)=\begin{cases}1&\text{if}\ \|x\|\leq\lambda\\
 0&\text{if}\ \|x\|>\lambda\\
 \end{cases}       Gaussian kernel         G   (  x  )    =    c   k  ,  d    k   (    ∥  x  ∥   2   )          G  x      subscript  c   k  d    k   superscript   norm  x   2      G(x)=c_{k,d}k(\|x\|^{2})      where    c   k  ,  d      subscript  c   k  d     c_{k,d}   , the normalization constant, makes G(x) integrate to one and    k   (  x  )       k  x    k(x)   is called the profile of the kernel. It simplifies calculation in the case of multivariate data. The profile of the Gaussian kernel is    e   -    1  /  2     ∥  x  ∥   2        superscript  e        1  2    superscript   norm  x   2       e^{-1/2\|x\|^{2}}   and therefore, the multivariate Gaussian kernel with the standard deviation   σ   σ   \sigma   , will be:     G   (  x  )    =    1     2  π     σ  d      e   -    1  /  2      ∥  x  ∥   2    σ  2              G  x       1        2  π     superscript  σ  d      superscript  e        1  2      superscript   norm  x   2    superscript  σ  2          G(x)=\frac{1}{\sqrt{2\pi}\sigma^{d}}e^{-1/2\frac{\|x\|^{2}}{\sigma^{2}}}   where d is the number of dimensions. It's also worth mentioning that the standard deviation for the Gaussian kernel works as the bandwidth parameter,   h   h   h     Applications  Clustering  Consider a set of points in two-dimensional space. Assume a circular window centered at C and having radius r as the kernel. Mean shift is a hill climbing algorithm which involves shifting this kernel iteratively to a higher density region until convergence. Every shift is defined by a mean shift vector. The mean shift vector always points toward the direction of the maximum increase in the density. At every iteration the kernel is shifted to the centroid or the mean of the points within it. The method of calculating this mean depends on the choice of the kernel. In this case if a Gaussian kernel is chosen instead of a flat kernel, then every point will first be assigned a weight which will decay exponentially as the distance from the kernel's center increases. At convergence, there will be no direction at which a shift can accommodate more points inside the kernel.  Tracking  The mean shift algorithm can be used for visual tracking. The simplest such algorithm would create a confidence map in the new image based on the color histogram of the object in the previous image, and use mean shift to find the peak of a confidence map near the object's old position. The confidence map is a probability density function on the new image, assigning each pixel of the new image a probability, which is the probability of the pixel color occurring in the object in the previous image. A few algorithms, such as ensemble tracking , 7 CAMshift, 8 expand on this idea.  Smoothing  Let    x  i     subscript  x  i    x_{i}   and        z  i   ,  i   =  1   ,   …  ,  n    ,     formulae-sequence      subscript  z  i   i   1    normal-…  n     z_{i},i=1,...,n,   be the d-dimensional input and filtered image pixels in the joint spatial-range domain. For each pixel,   Initialize    j  =  1      j  1    j=1   and     y   i  ,  1    =   x  i        subscript  y   i  1     subscript  x  i     y_{i,1}=x_{i}     Compute    y   i  ,   j  +  1       subscript  y   i    j  1      y_{i,j+1}   according to    m   (  ⋅  )       m  normal-⋅    m(\cdot)   until convergence,    y  =   y   i  ,  c        y   subscript  y   i  c      y=y_{i,c}   .  Assign     z  i   =   (   x  i  s   ,   y   i  ,  c   r   )        subscript  z  i     superscript   subscript  x  i   s    superscript   subscript  y   i  c    r      z_{i}=(x_{i}^{s},y_{i,c}^{r})   . The superscripts s and r denote the spatial and range components of a vector, respectively. The assignment specifies that the filtered data at the spatial location axis will have the range component of the point of convergence    y   i  ,  c   r     superscript   subscript  y   i  c    r    y_{i,c}^{r}   .   Strengths   Mean shift is an application-independent tool suitable for real data analysis.  Does not assume any predefined shape on data clusters.  It is capable of handling arbitrary feature spaces.  The procedure relies on choice of a single parameter: bandwidth.  The bandwidth/window size 'h' has a physical meaning, unlike k -means .   Weaknesses   The selection of a window size is not trivial.  Inappropriate window size can cause modes to be merged, or generate additional “shallow” modes.  Often requires using adaptive window size.   Mean shift and k -means clustering  The mean shift clustering algorithm has two main drawbacks. First, the algorithm is calculation intensive; it requires in general    O   (   k   N  2    )       O    k   superscript  N  2      O(kN^{2})   operations, where N is the number of data points and k is the number of average iteration steps for each data point. Second, the mean shift algorithm relies on sufficient high data density with clear gradient to locate the cluster centers. In particular, the mean shift algorithm often fails to find appropriate clusters for so called data outliers, or those data points located between natural clusters.  The k -means algorithm does not have the above two problems. The k -means algorithm normally requires only    O   (   k  N   )       O    k  N     O(kN)   operations, so that the k -means algorithm can be applied to relatively large dataset. However, k -means has two significant limitations. First, the k -means algorithm requires that the number of clusters to be pre-determined. In practise, it is often difficult to specify a priori an appropriate cluster number, resulting in some natural clusters being represented by multiple clusters found by the k -means algorithm. Second, the k -means algorithm is, in general, incapable of identifying non-convex clusters. The second limitation makes the k -means algorithm inadequate for complex non-linear data. These problems can be overcome, by simply combining the two algorithms mean shift and k -means together. 9  See also   Kernel density estimation (KDE)  Kernel (statistics)   References  External links  Code implementations   Scikit-learn library Numpy/Python implementation uses ball tree for efficient neighboring points lookup  EDISON library . C++ implementation of mean-shift-based image segmentation. There is also a Matlab interface for EDISON.  OpenCV contains mean-shift implementation via cvMeanShift Method  Aiphial . Java-based mean-shift implementation for numeric data clustering and image segmentation  Apache Mahout . An map-reduce based implementation of MeanShift clustering written on Apache Hadoop.  CAMSHIFT project . A MATLAB implementation of CAMSHIFT algorithm.  OTB MeanShift . A C++ implementation using the Orfeo Toolbox .  ImageJ Plug-in . Image filtering using the mean shift filter.  Mean-shift google code . A simple implementation of mean-shift as image filtering tool.   Short lessons   A lesson from Prof. M. Shah on this topic   "  Category:Computer vision  Category:Data clustering algorithms     ↩  ↩  ↩     ↩  ↩  ↩     