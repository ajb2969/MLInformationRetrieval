   Estimation of covariance matrices      Estimation of covariance matrices   In statistics , sometimes the covariance matrix of a multivariate random variable is not known but has to be estimated . Estimation of covariance matrices then deals with the question of how to approximate the actual covariance matrix on the basis of a sample from the multivariate distribution . Simple cases, where observations are complete, can be dealt with by using the sample covariance matrix . The sample covariance matrix (SCM) is an unbiased and efficient estimator of the covariance matrix if the space of covariance matrices is viewed as an extrinsic  convex cone in R p × p ; however, measured using the intrinsic geometry of positive-definite matrices , the SCM is a biased and inefficient estimator. 1 In addition, if the random variable has normal distribution , the sample covariance matrix has Wishart distribution and a slightly differently scaled version of it is the maximum likelihood estimate . Cases involving missing data require deeper considerations. Another issue is the robustness to outliers : 2 "Sample covariance matrices are extremely sensitive to outliers". 3 4  Statistical analyses of multivariate data often involve exploratory studies of the way in which the variables change in relation to one another and this may be followed up by explicit statistical models involving the covariance matrix of the variables. Thus the estimation of covariance matrices directly from observational data plays two roles:  :* to provide initial estimates that can be used to study the inter-relationships;  :* to provide sample estimates that can be used for model checking.  Estimates of covariance matrices are required at the initial stages of principal component analysis and factor analysis , and are also involved in versions of regression analysis that treat the dependent variables in a data-set, jointly with the independent variable as the outcome of a random sample.  Estimation in a general context  Given a sample consisting of n independent observations x 1 ,..., x n of a p -dimensional random vector  X ∈ R p ×1 (a p ×1 column-vector), an unbiased  estimator of the ( p × p ) covariance matrix       cov   (  X  )    =   E   [    (   X  -   E   [  X  ]     )     (   X  -   E   [  X  ]     )   T    ]         cov  X    normal-E      X   normal-E  X     superscript    X   normal-E  X    normal-T       \operatorname{cov}(X)=\operatorname{E}\left[\left(X-\operatorname{E}[X])(X-%
 \operatorname{E}[X]\right)^{\mathrm{T}}\right]     is the sample covariance matrix       𝐐  =    1   n  -  1      ∑   i  =  1   n     (    x  i   -   x  ¯    )     (    x  i   -   x  ¯    )   T       ,      𝐐      1    n  1      superscript   subscript     i  1    n        subscript  x  i    normal-¯  x     superscript     subscript  x  i    normal-¯  x    normal-T        \mathbf{Q}={1\over{n-1}}\sum_{i=1}^{n}(x_{i}-\overline{x})(x_{i}-\overline{x})%
 ^{\mathrm{T}},     where    x  i     subscript  x  i    \textstyle x_{i}   is the i -th observation of the p -dimensional random vector, and       x  ¯   =   [       x  ¯   1       ⋮        x  ¯   p      ]   =    1  n     ∑   i  =  1   n    x  i            normal-¯  x    delimited-[]     subscript   normal-¯  x   1     normal-⋮     subscript   normal-¯  x   p               1  n     superscript   subscript     i  1    n    subscript  x  i        \overline{x}=\left[\begin{array}[c]{c}\bar{x}_{1}\\
 \vdots\\
 \bar{x}_{p}\end{array}\right]={1\over{n}}\sum_{i=1}^{n}x_{i}     is the sample mean . This is true regardless of the distribution of the random variable X , provided of course that the theoretical means and covariances exist. The reason for the factor n − 1 rather than n is essentially the same as the reason for the same factor appearing in unbiased estimates of sample variances and sample covariances , which relates to the fact that the mean is not known and is replaced by the sample mean.  In cases where the distribution of the random variable  X is known to be within a certain family of distributions, other estimates may be derived on the basis of that assumption. A well-known instance is when the random variable  X is normally distributed : in this case the maximum likelihood  estimator of the covariance matrix is slightly different from the unbiased estimate, and is given by        𝐐  𝐧   =    1  n     ∑   i  =  1   n     (    x  i   -   x  ¯    )     (    x  i   -   x  ¯    )   T       .       subscript  𝐐  𝐧       1  n     superscript   subscript     i  1    n        subscript  x  i    normal-¯  x     superscript     subscript  x  i    normal-¯  x    normal-T        \mathbf{Q_{n}}={1\over n}\sum_{i=1}^{n}(x_{i}-\overline{x})(x_{i}-\overline{x}%
 )^{\mathrm{T}}.     A derivation of this result is given below. Clearly, the difference between the unbiased estimator and the maximum likelihood estimator diminishes for large n .  In the general case, the unbiased estimate of the covariance matrix provides an acceptable estimate when the data vectors in the observed data set are all complete: that is they contain no missing elements . One approach to estimating the covariance matrix is to treat the estimation of each variance or pairwise covariance separately, and to use all the observations for which both variables have valid values. Assuming the missing data are missing at random this results in an estimate for the covariance matrix which is unbiased. However, for many applications this estimate may not be acceptable because the estimated covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimated correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix.  When estimating the cross-covariance of a pair of signals that are wide-sense stationary , missing samples do not need be random (e.g., sub-sampling by an arbitrary factor is valid).  Maximum-likelihood estimation for the multivariate normal distribution  A random vector X ∈ R p (a p ×1 "column vector") has a multivariate normal distribution with a nonsingular covariance matrix Σ precisely if Σ ∈ R p × p is a positive-definite matrix and the probability density function of X is       f   (  x  )    =      (   2  π   )    -   p  /  2       det     (  Σ  )    -   1  /  2      exp   (   -    1  2     (   x  -  μ   )   T    Σ   -  1     (   x  -  μ   )     )             f  x      superscript    2  π       p  2          superscript  normal-Σ      1  2             1  2    superscript    x  μ   normal-T    superscript  normal-Σ    1      x  μ           f(x)=(2\pi)^{-p/2}\,\det(\Sigma)^{-1/2}\exp\left(-{1\over 2}(x-\mu)^{\mathrm{T%
 }}\Sigma^{-1}(x-\mu)\right)     where μ ∈ R p ×1 is the expected value of X . The covariance matrix  Σ is the multidimensional analog of what in one dimension would be the variance , and      (   2  π   )    -   p  /  2      det    (  Σ  )    -   1  /  2           superscript    2  π       p  2        superscript  normal-Σ      1  2        (2\pi)^{-p/2}\det(\Sigma)^{-1/2}   normalizes the density    f   (  x  )       f  x    f(x)   so that it integrates to 1.  Suppose now that X 1 , ..., X n are independent and identically distributed samples from the distribution above. Based on the observed values  x 1 , ..., x n of this sample , we wish to estimate Σ.  First steps  The likelihood function is:       ℒ   (  μ  ,  Σ  )    =      (   2  π   )    -    n  p   /  2        ∏   i  =  1   n    det     (  Σ  )    -   1  /  2      exp   (   -    1  2     (    x  i   -  μ   )   T    Σ   -  1     (    x  i   -  μ   )     )              ℒ   μ  normal-Σ       superscript    2  π         n  p   2       superscript   subscript  product    i  1    n        superscript  normal-Σ      1  2             1  2    superscript     subscript  x  i   μ   normal-T    superscript  normal-Σ    1       subscript  x  i   μ            \mathcal{L}(\mu,\Sigma)=(2\pi)^{-np/2}\,\prod_{i=1}^{n}\det(\Sigma)^{-1/2}\exp%
 \left(-{1\over 2}(x_{i}-\mu)^{\mathrm{T}}\Sigma^{-1}(x_{i}-\mu)\right)     It is fairly readily shown that the maximum-likelihood estimate of the mean vector μ is the " sample mean " vector:        x  ¯   =    (    x  1   +  ⋯  +   x  n    )   /  n    .       normal-¯  x        subscript  x  1   normal-⋯   subscript  x  n    n     \overline{x}=(x_{1}+\cdots+x_{n})/n.     See the section on estimation in the article on the normal distribution for details; the process here is similar.  Since the estimate    x  ¯     normal-¯  x    \bar{x}   does not depend on Σ, we can just substitute it for μ in the likelihood function , getting        ℒ   (   x  ¯   ,  Σ  )    ∝   det     (  Σ  )    -   n  /  2      exp   (   -    1  2     ∑   i  =  1   n      (    x  i   -   x  ¯    )   T    Σ   -  1     (    x  i   -   x  ¯    )       )       ,     proportional-to    ℒ    normal-¯  x   normal-Σ         superscript  normal-Σ      n  2             1  2     superscript   subscript     i  1    n      superscript     subscript  x  i    normal-¯  x    normal-T    superscript  normal-Σ    1       subscript  x  i    normal-¯  x             \mathcal{L}(\overline{x},\Sigma)\propto\det(\Sigma)^{-n/2}\exp\left(-{1\over 2%
 }\sum_{i=1}^{n}(x_{i}-\overline{x})^{\mathrm{T}}\Sigma^{-1}(x_{i}-\overline{x}%
 )\right),     and then seek the value of Σ that maximizes the likelihood of the data (in practice it is easier to work with log   ℒ   ℒ   \mathcal{L}   ).  The trace of a 1 × 1 matrix  Now we come to the first surprising step: regard the scalar       (    x  i   -   x  ¯    )   T    Σ   -  1     (    x  i   -   x  ¯    )        superscript     subscript  x  i    normal-¯  x    normal-T    superscript  normal-Σ    1       subscript  x  i    normal-¯  x      (x_{i}-\overline{x})^{\mathrm{T}}\Sigma^{-1}(x_{i}-\overline{x})   as the trace of a 1×1 matrix.  This makes it possible to use the identity tr( AB ) = tr( BA ) whenever A and B are matrices so shaped that both products exist. We get       ℒ   (   x  ¯   ,  Σ  )    ∝   det     (  Σ  )    -   n  /  2      exp   (   -    1  2     ∑   i  =  1   n    tr   (     (    x  i   -   x  ¯    )   T    Σ   -  1     (    x  i   -   x  ¯    )    )       )         proportional-to    ℒ    normal-¯  x   normal-Σ         superscript  normal-Σ      n  2             1  2     superscript   subscript     i  1    n    tr     superscript     subscript  x  i    normal-¯  x    normal-T    superscript  normal-Σ    1       subscript  x  i    normal-¯  x              \mathcal{L}(\overline{x},\Sigma)\propto\det(\Sigma)^{-n/2}\exp\left(-{1\over 2%
 }\sum_{i=1}^{n}\operatorname{tr}((x_{i}-\overline{x})^{\mathrm{T}}\Sigma^{-1}(%
 x_{i}-\overline{x}))\right)          =   det     (  Σ  )    -   n  /  2      exp   (   -    1  2     ∑   i  =  1   n    tr   (    (    x  i   -   x  ¯    )     (    x  i   -   x  ¯    )   T    Σ   -  1     )       )          absent       superscript  normal-Σ      n  2             1  2     superscript   subscript     i  1    n    tr       subscript  x  i    normal-¯  x     superscript     subscript  x  i    normal-¯  x    normal-T    superscript  normal-Σ    1              =\det(\Sigma)^{-n/2}\exp\left(-{1\over 2}\sum_{i=1}^{n}\operatorname{tr}((x_{i%
 }-\overline{x})(x_{i}-\overline{x})^{\mathrm{T}}\Sigma^{-1})\right)     (so now we are taking the trace of a p × p matrix)       =   det     (  Σ  )    -   n  /  2      exp   (   -    1  2    tr   (    ∑   i  =  1   n     (    x  i   -   x  ¯    )     (    x  i   -   x  ¯    )   T    Σ   -  1      )      )          absent       superscript  normal-Σ      n  2             1  2    tr    superscript   subscript     i  1    n        subscript  x  i    normal-¯  x     superscript     subscript  x  i    normal-¯  x    normal-T    superscript  normal-Σ    1              =\det(\Sigma)^{-n/2}\exp\left(-{1\over 2}\operatorname{tr}\left(\sum_{i=1}^{n}%
 (x_{i}-\overline{x})(x_{i}-\overline{x})^{\mathrm{T}}\Sigma^{-1}\right)\right)          =   det     (  Σ  )    -   n  /  2      exp   (   -    1  2    tr   (   S   Σ   -  1     )      )          absent       superscript  normal-Σ      n  2             1  2    tr    S   superscript  normal-Σ    1             =\det(\Sigma)^{-n/2}\exp\left(-{1\over 2}\operatorname{tr}\left(S\Sigma^{-1}%
 \right)\right)     where       S  =    ∑   i  =  1   n     (    x  i   -   x  ¯    )     (    x  i   -   x  ¯    )   T     ∈   𝐑   p  ×  p     .        S    superscript   subscript     i  1    n        subscript  x  i    normal-¯  x     superscript     subscript  x  i    normal-¯  x    normal-T           superscript  𝐑    p  p       S=\sum_{i=1}^{n}(x_{i}-\overline{x})(x_{i}-\overline{x})^{\mathrm{T}}\in%
 \mathbf{R}^{p\times p}.      S   S   S   is sometimes called the scatter matrix , and is positive definite if there exists a subset of the data consisting of   p   p   p   linearly independent observations (which we will assume).  Using the spectral theorem  It follows from the spectral theorem of linear algebra that a positive-definite symmetric matrix S has a unique positive-definite symmetric square root S 1/2 . We can again use the "cyclic property" of the trace to write       det     (  Σ  )    -   n  /  2      exp   (   -    1  2    tr   (    S   1  /  2     Σ   -  1     S   1  /  2     )      )      .         superscript  normal-Σ      n  2             1  2    tr     superscript  S    1  2     superscript  normal-Σ    1     superscript  S    1  2            \det(\Sigma)^{-n/2}\exp\left(-{1\over 2}\operatorname{tr}\left(S^{1/2}\Sigma^{%
 -1}S^{1/2}\right)\right).     Let B = S 1/2  Σ −1  S 1/2 . Then the expression above becomes       det     (  S  )    -   n  /  2      det     (  B  )    n  /  2     exp   (   -    1  2    tr   (  B  )      )        .         superscript  S      n  2          superscript  B    n  2            1  2    tr  B           \det(S)^{-n/2}\det(B)^{n/2}\exp\left(-{1\over 2}\operatorname{tr}(B)\right).     The positive-definite matrix B can be diagonalized, and then the problem of finding the value of B that maximizes      det     (  B  )    n  /  2     exp   (   -    1  2    tr   (  B  )      )            superscript  B    n  2            1  2    tr  B         \det(B)^{n/2}\exp\left(-{1\over 2}\operatorname{tr}(B)\right)     Since the trace of a square matrix equals the sum of eigen-values ( "trace and eigenvalues" ), the equation reduces to the problem of finding the eigen values λ 1 , ..., λ p that maximize        λ  i   n  /  2     exp   (   -    λ  i   /  2    )     .       superscript   subscript  λ  i     n  2           subscript  λ  i   2       \lambda_{i}^{n/2}\exp(-\lambda_{i}/2).     This is just a calculus problem and we get λ i = n for all i. Thus, assume Q is the matrix of eigen vectors, then      B  =   Q   (   n   I  p    )    Q   -  1     =   n   I  p          B    Q    n   subscript  I  p     superscript  Q    1            n   subscript  I  p       B=Q(nI_{p})Q^{-1}=nI_{p}     i.e., n times the p × p identity matrix.  Concluding steps  Finally we get       Σ  =    S   1  /  2     B   -  1     S   1  /  2     =    S   1  /  2     (    1  n    I  p    )    S   1  /  2     =   S  n    ,        normal-Σ     superscript  S    1  2     superscript  B    1     superscript  S    1  2             superscript  S    1  2        1  n    subscript  I  p     superscript  S    1  2            S  n      \Sigma=S^{1/2}B^{-1}S^{1/2}=S^{1/2}\left(\frac{1}{n}I_{p}\right)S^{1/2}=\frac{%
 S}{n},     i.e., the p × p "sample covariance matrix"       S  n   =    1  n     ∑   i  =  1   n     (    X  i   -   X  ¯    )     (    X  i   -   X  ¯    )   T            S  n       1  n     superscript   subscript     i  1    n        subscript  X  i    normal-¯  X     superscript     subscript  X  i    normal-¯  X    normal-T        {S\over n}={1\over n}\sum_{i=1}^{n}(X_{i}-\overline{X})(X_{i}-\overline{X})^{%
 \mathrm{T}}     is the maximum-likelihood estimator of the "population covariance matrix" Σ . At this point we are using a capital X rather than a lower-case x because we are thinking of it "as an estimator rather than as an estimate", i.e., as something random whose probability distribution we could profit by knowing. The random matrix S can be shown to have a Wishart distribution with n − 1 degrees of freedom. 5 That is:         ∑   i  =  1   n     (    X  i   -   X  ¯    )     (    X  i   -   X  ¯    )   T     ∼    W  p    (  Σ  ,   n  -  1   )     .     similar-to    superscript   subscript     i  1    n        subscript  X  i    normal-¯  X     superscript     subscript  X  i    normal-¯  X    normal-T        subscript  W  p    normal-Σ    n  1       \sum_{i=1}^{n}(X_{i}-\overline{X})(X_{i}-\overline{X})^{\mathrm{T}}\sim W_{p}(%
 \Sigma,n-1).     Alternative derivation  An alternative derivation of the maximum likelihood estimator can be performed via matrix calculus formulae (see also differential of a determinant and differential of the inverse matrix ). It also verifies the aforementioned fact about the maximum likelihood estimate of the mean. Re-write the likelihood in the log form using the trace trick:         ln  ℒ    (  μ  ,  Σ  )    =   const  -    n  2   ln   det   (  Σ  )     -    1  2    tr   [    Σ   -  1      ∑   i  =  1   n     (    x  i   -  μ   )     (    x  i   -  μ   )   T      ]       .          ℒ    μ  normal-Σ      const      n  2      normal-Σ        1  2    tr     superscript  normal-Σ    1      superscript   subscript     i  1    n        subscript  x  i   μ    superscript     subscript  x  i   μ   normal-T           \ln\mathcal{L}(\mu,\Sigma)=\operatorname{const}-{n\over 2}\ln\det(\Sigma)-{1%
 \over 2}\operatorname{tr}\left[\Sigma^{-1}\sum_{i=1}^{n}(x_{i}-\mu)(x_{i}-\mu)%
 ^{\mathrm{T}}\right].     The differential of this log-likelihood is       d   ln  ℒ    (  μ  ,  Σ  )    =   -    n  2    tr   [    Σ   -  1     {   d  Σ   }    ]            d    ℒ    μ  normal-Σ          n  2    tr     superscript  normal-Σ    1       d  normal-Σ          d\ln\mathcal{L}(\mu,\Sigma)=-{n\over 2}\operatorname{tr}\left[\Sigma^{-1}\left%
 \{d\Sigma\right\}\right]          -    1  2    tr   [    -    Σ   -  1     {   d  Σ   }    Σ   -  1      ∑   i  =  1   n     (    x  i   -  μ   )     (    x  i   -  μ   )   T       -   2   Σ   -  1      ∑   i  =  1   n     (    x  i   -  μ   )     {   d  μ   }   T       ]      .          1  2    tr         superscript  normal-Σ    1       d  normal-Σ     superscript  normal-Σ    1      superscript   subscript     i  1    n        subscript  x  i   μ    superscript     subscript  x  i   μ   normal-T         2   superscript  normal-Σ    1      superscript   subscript     i  1    n        subscript  x  i   μ    superscript     d  μ    normal-T           -{1\over 2}\operatorname{tr}\left[-\Sigma^{-1}\{d\Sigma\}\Sigma^{-1}\sum_{i=1}%
 ^{n}(x_{i}-\mu)(x_{i}-\mu)^{\mathrm{T}}-2\Sigma^{-1}\sum_{i=1}^{n}(x_{i}-\mu)%
 \{d\mu\}^{\mathrm{T}}\right].     It naturally breaks down into the part related to the estimation of the mean, and to the part related to the estimation of the variance. The first order condition for maximum,     d   ln  ℒ    (  μ  ,  Σ  )    =  0        d    ℒ    μ  normal-Σ    0    d\ln\mathcal{L}(\mu,\Sigma)=0   , is satisfied when the terms multiplying    d  μ      d  μ    d\mu   and    d  Σ      d  normal-Σ    d\Sigma   are identically zero. Assuming (the maximum likelihood estimate of)   Σ   normal-Σ   \Sigma   is non-singular, the first order condition for the estimate of the mean vector is         ∑   i  =  1   n    (    x  i   -  μ   )    =  0   ,        superscript   subscript     i  1    n      subscript  x  i   μ    0    \sum_{i=1}^{n}(x_{i}-\mu)=0,     which leads to the maximum likelihood estimator        μ  ^   =   X  ¯   =    1  n     ∑   i  =  1   n    X  i      .         normal-^  μ    normal-¯  X            1  n     superscript   subscript     i  1    n    subscript  X  i        \widehat{\mu}=\bar{X}={1\over n}\sum_{i=1}^{n}X_{i}.     This lets us simplify      ∑   i  =  1   n     (    x  i   -  μ   )     (    x  i   -  μ   )   T     =    ∑   i  =  1   n     (    x  i   -   x  ¯    )     (    x  i   -   x  ¯    )   T     =  S          superscript   subscript     i  1    n        subscript  x  i   μ    superscript     subscript  x  i   μ   normal-T       superscript   subscript     i  1    n        subscript  x  i    normal-¯  x     superscript     subscript  x  i    normal-¯  x    normal-T          S     \sum_{i=1}^{n}(x_{i}-\mu)(x_{i}-\mu)^{\mathrm{T}}=\sum_{i=1}^{n}(x_{i}-\bar{x}%
 )(x_{i}-\bar{x})^{\mathrm{T}}=S   as defined above. Then the terms involving    d  Σ      d  normal-Σ    d\Sigma   in    d   ln  L       d    L     d\ln L   can be combined as       -    1  2    tr   (    Σ   -  1     {   d  Σ   }    [    n   I  p    -    Σ   -  1    S    ]    )      .          1  2    tr     superscript  normal-Σ    1       d  normal-Σ     delimited-[]      n   subscript  I  p       superscript  normal-Σ    1    S          -{1\over 2}\operatorname{tr}\left(\Sigma^{-1}\left\{d\Sigma\right\}\left[nI_{p%
 }-\Sigma^{-1}S\right]\right).     The first order condition     d   ln  ℒ    (  μ  ,  Σ  )    =  0        d    ℒ    μ  normal-Σ    0    d\ln\mathcal{L}(\mu,\Sigma)=0   will hold when the term in the square bracket is (matrix-valued) zero. Pre-multiplying the latter by   Σ   normal-Σ   \Sigma   and dividing by   n   n   n   gives        Σ  ^   =    1  n   S    ,       normal-^  normal-Σ       1  n   S     \widehat{\Sigma}={1\over n}S,     which of course coincides with the canonical derivation given earlier.  Dwyer 6 points out that decomposition into two terms such as appears above is "unnecessary" and derives the estimator in two lines of working. Note that it may be not trivial to show that such derived estimator is the unique global maximizer for likelihood function.  Intrinsic covariance matrix estimation  Intrinsic expectation  Given a sample of n independent observations x 1 ,..., x n of a p -dimensional zero-mean Gaussian random variable X with covariance R , the maximum likelihood  estimator of R is given by        𝐑  ^   =    1  n     ∑   i  =  1   n     x  i    x  i  T       .       normal-^  𝐑       1  n     superscript   subscript     i  1    n      subscript  x  i    superscript   subscript  x  i   normal-T        \hat{\mathbf{R}}={1\over n}\sum_{i=1}^{n}x_{i}x_{i}^{\mathrm{T}}.     The parameter R belongs to the set of positive-definite matrices , which is a Riemannian manifold , not a vector space , hence the usual vector-space notions of expectation , i.e. "E[ R ^]", and estimator bias must be generalized to manifolds to make sense of the problem of covariance matrix estimation. This can be done by defining the expectation of an manifold-valued estimator R ^ with respect to the manifold-valued point R as        E  𝐑    [   𝐑  ^   ]      =  def       exp  𝐑   E    [    exp  𝐑   -  1     𝐑  ^    ]        superscript   def      subscript  normal-E  𝐑    delimited-[]   normal-^  𝐑         subscript   𝐑   normal-E    delimited-[]    superscript   subscript   𝐑     1     normal-^  𝐑        \mathrm{E}_{\mathbf{R}}[\hat{\mathbf{R}}]\ \stackrel{\mathrm{def}}{=}\ \exp_{%
 \mathbf{R}}\mathrm{E}\left[\exp_{\mathbf{R}}^{-1}\hat{\mathbf{R}}\right]     where        exp  𝐑    (   𝐑  ^   )    =    𝐑   1  2     exp   (    𝐑   -   1  2      𝐑  ^    𝐑   -   1  2      )     𝐑   1  2           subscript   𝐑    normal-^  𝐑       superscript  𝐑    1  2         superscript  𝐑      1  2      normal-^  𝐑    superscript  𝐑      1  2        superscript  𝐑    1  2       \exp_{\mathbf{R}}(\hat{\mathbf{R}})=\mathbf{R}^{\frac{1}{2}}\exp\left(\mathbf{%
 R}^{-\frac{1}{2}}\hat{\mathbf{R}}\mathbf{R}^{-\frac{1}{2}}\right)\mathbf{R}^{%
 \frac{1}{2}}           exp  𝐑   -  1     (   𝐑  ^   )    =    𝐑   1  2     (   log    𝐑   -   1  2      𝐑  ^    𝐑   -   1  2       )    𝐑   1  2           superscript   subscript   𝐑     1     normal-^  𝐑       superscript  𝐑    1  2         superscript  𝐑      1  2      normal-^  𝐑    superscript  𝐑      1  2        superscript  𝐑    1  2       \exp_{\mathbf{R}}^{-1}(\hat{\mathbf{R}})=\mathbf{R}^{\frac{1}{2}}\left(\log%
 \mathbf{R}^{-\frac{1}{2}}\hat{\mathbf{R}}\mathbf{R}^{-\frac{1}{2}}\right)%
 \mathbf{R}^{\frac{1}{2}}     are the exponential map and inverse exponential map, respectively, "exp" and "log" denote the ordinary matrix exponential and matrix logarithm , and E[·] is the ordinary expectation operator defined on a vector space, in this case the tangent space of the manifold. 7  Bias of the sample covariance matrix  The intrinsic bias  vector field of the SCM estimator R ^ is defined to be       𝐁   (   𝐑  ^   )    =     exp  𝐑   -  1     E  𝐑     [   𝐑  ^   ]    =   E   [    exp  𝐑   -  1     𝐑  ^    ]            𝐁   normal-^  𝐑        superscript   subscript   𝐑     1     subscript  normal-E  𝐑     delimited-[]   normal-^  𝐑            normal-E   delimited-[]    superscript   subscript   𝐑     1     normal-^  𝐑         \mathbf{B}(\hat{\mathbf{R}})=\exp_{\mathbf{R}}^{-1}\mathrm{E}_{\mathbf{R}}%
 \left[\hat{\mathbf{R}}\right]=\mathrm{E}\left[\exp_{\mathbf{R}}^{-1}\hat{%
 \mathbf{R}}\right]     The intrinsic estimator bias is then given by      exp  𝐑   𝐁    (   𝐑  ^   )         subscript   𝐑   𝐁    normal-^  𝐑     \exp_{\mathbf{R}}\mathbf{B}(\hat{\mathbf{R}})   .  For complex Gaussian random variables, this bias vector field can be shown 8 to equal       𝐁   (   𝐑  ^   )    =   -   β   (  p  ,  n  )   𝐑          𝐁   normal-^  𝐑        β   p  n   𝐑      \mathbf{B}(\hat{\mathbf{R}})=-\beta(p,n)\mathbf{R}     where       β   (  p  ,  n  )    =    1  p    (       p   log  n    +  p   -   ψ   (    n  -  p   +  1   )     +    (    n  -  p   +  1   )   ψ   (    n  -  p   +  2   )    +   ψ   (   n  +  1   )     -    (   n  +  1   )   ψ   (   n  +  2   )     )          β   p  n        1  p             p    n    p     ψ      n  p   1           n  p   1   ψ      n  p   2      ψ    n  1         n  1   ψ    n  2        \beta(p,n)=\frac{1}{p}\left(p\log n+p-\psi(n-p+1)+(n-p+1)\psi(n-p+2)+\psi(n+1)%
 -(n+1)\psi(n+2)\right)     and ψ(·) is the digamma function . The intrinsic bias of the sample covariance matrix equals         exp  𝐑   𝐁    (   𝐑  ^   )    =    e   -   β   (  p  ,  n  )      𝐑           subscript   𝐑   𝐁    normal-^  𝐑       superscript  e      β   p  n      𝐑     \exp_{\mathbf{R}}\mathbf{B}(\hat{\mathbf{R}})=e^{-\beta(p,n)}\mathbf{R}     and the SCM is asymptotically unbiased as n → ∞.  Similarly, the intrinsic inefficiency of the sample covariance matrix depends upon the Riemannian curvature of the space of positive-define matrices.  Shrinkage estimation  If the sample size n is small and the number of considered variables p is large, the above empirical estimators of covariance and correlation are very unstable. Specifically, it is possible to furnish estimators that improve considerably upon the maximum likelihood estimate in terms of mean squared error. Moreover, for n A) with some suitable chosen target (   B   B   B   ), e.g., the diagonal matrix. Subsequently, the mixing parameter (   δ   δ   \delta   ) is selected to maximize the expected accuracy of the shrunken estimator. This can be done by cross-validation , or by using an analytic estimate of the shrinkage intensity. The resulting regularized estimator (     δ  A   +    (   1  -  δ   )   B         δ  A       1  δ   B     \delta A+(1-\delta)B   ) can be shown to outperform the maximum likelihood estimator for small samples. For large samples, the shrinkage intensity will reduce to zero, hence in this case the shrinkage estimator will be identical to the empirical estimator. Apart from increased efficiency the shrinkage estimate has the additional advantage that it is always positive definite and well conditioned.  Various shrinkage targets have been proposed:   the identity matrix , scaled by the average sample variance ; 9  the single-index model ; 10  the constant-correlation model, where the sample variances are preserved, but all pairwise correlation coefficients are assumed to be equal to one another; 11  the two-parameter matrix, where all variances are identical, and all covariances are identical to one another (although not identical to the variances); 12  the diagonal matrix containing sample variances on the diagonal and zeros everywhere else. 13   A review on this topic is given, e.g., in Schäfer and Strimmer 2005. 14 Software for computing a covariance shrinkage estimator is available in R (packages corpcor 15 and ShrinkCovMat 16 ), in Python (library scikit-learn ), and in MATLAB . 17  See also   Propagation of uncertainty  Sample mean and sample covariance   References    "  Category:Estimation for specific parameters  Category:Statistical deviation and dispersion     ↩  ↩  Robust Statistics , Peter. J. Huber , Wiley, 1981 (republished in paperback, 2004) ↩  "Modern applied statistics with S", William N. Venables , Brian D. Ripley , Springer, 2002, ISBN 0-387-95457-0, ISBN 978-0-387-95457-8, page 336 ↩  K.V. Mardia , J.T. Kent , and J.M. Bibby (1979) Multivariate Analysis , Academic Press . ↩  ↩    O. Ledoit and M. Wolf (2004a) " A well-conditioned estimator for large-dimensional covariance matrices " Journal of Multivariate Analysis  88 (2): 365—411. ↩  O. Ledoit and M. Wolf (2003) " Improved estimation of the covariance matrix of stock returns with an application to portofolio selection " Journal of Empirical Finance  10 (5): 603—621. ↩  O. Ledoit and M. Wolf (2004b) " Honey, I shrunk the sample covariance matrix " The Journal of Portfolio Management  30 (4): 110—119. ↩  Appendix B.1 of O. Ledoit (1996) " Improved Covariance Matrix Estimation " Finance Working Paper No. 5-96, Anderson School of Management , University of California, Los Angeles . ↩  Appendix B.2 of O. Ledoit (1996). ↩  J. Schäfer and K. Strimmer (2005) A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and Implications for Functional Genomics , Statistical Applications in Genetics and Molecular Biology: Vol. 4: No. 1, Article 32. ↩  ↩  ↩  MATLAB code for shrinkage targets: scaled identity , single-index model , constant-correlation model , two-parameter matrix , and diagonal matrix . ↩     