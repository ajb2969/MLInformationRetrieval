   Error exponent      Error exponent   In information theory , the error exponent of a channel code or source code over the block length of the code is the logarithm of the error probability. For example, if the probability of error of a decoder drops as e − n α , where n is the block length, the error exponent is α. Many of the information-theoretic theorems are of asymptotic nature, for example, the channel coding theorem states that for any rate less than the channel capacity, the probability of the error of the channel code can be made to go to zero as the block length goes to infinity. In practical situations, there are limitations to the delay of the communication and the block length must be finite. Therefore it is important to study how the probability of error drops as the block length go to infinity.  Error exponent in channel coding  For time-invariant DMC channels  The channel coding theorem states that for any ε > 0 and for any rate less than the channel capacity, there is an encoding and decoding scheme that can be used to ensure that the probability of block error is less than ε > 0 for sufficiently long message block X . Also, for any rate greater than the channel capacity, the probability of block error at the receiver goes to one as the block length goes to infinity.  Assuming a channel coding setup as follows: the channel can transmit any of    M  =    2   n  R         M   superscript  2    n  R      M=2^{nR}\;   messages, by transmitting the corresponding codeword (which is of length n ). Each component in the codebook is drawn i.i.d. according to some probability distribution with probability mass function  Q . At the decoding end, ML decoding is done.  Given that    y  1  n     superscript   subscript  y  1   n    y_{1}^{n}   is received, X (1) or first message is transmitted, the probability that X (1) is incorrectly detected as X (2) is:       P    error  1   →  2    =   ∑    x  1  n    (  2  )     Q   (   x  1  n    (  2  )   )   1   (  p   (   y  1  n   |   x  1  n    (  2  )   )   >  p   (   y  1  n   |   x  1  n    (  1  )   )   )      fragments   subscript  P   normal-→    error  1   2      subscript      superscript   subscript  x  1   n   2    Q   fragments  normal-(   superscript   subscript  x  1   n    fragments  normal-(  2  normal-)   normal-)   1   fragments  normal-(  p   fragments  normal-(   superscript   subscript  y  1   n   normal-|   superscript   subscript  x  1   n    fragments  normal-(  2  normal-)   normal-)    p   fragments  normal-(   superscript   subscript  y  1   n   normal-|   superscript   subscript  x  1   n    fragments  normal-(  1  normal-)   normal-)   normal-)     P_{\mathrm{error}\ 1\to 2}=\sum_{x_{1}^{n}(2)}Q(x_{1}^{n}(2))1(p(y_{1}^{n}|x_{%
 1}^{n}(2))>p(y_{1}^{n}|x_{1}^{n}(1)))     The function    1   (  p   (   y  1  n   |   x  1  n    (  2  )   )   >  p   (   y  1  n   |   x  1  n    (  1  )   )       fragments  1   fragments  normal-(  p   fragments  normal-(   superscript   subscript  y  1   n   normal-|   superscript   subscript  x  1   n    fragments  normal-(  2  normal-)   normal-)    p   fragments  normal-(   superscript   subscript  y  1   n   normal-|   superscript   subscript  x  1   n    fragments  normal-(  1  normal-)   normal-)      1(p(y_{1}^{n}|x_{1}^{n}(2))>p(y_{1}^{n}|x_{1}^{n}(1))   has upper bound       (    p   (   y  1  n   |   x  1  n    (  2  )   )     p   (   y  1  n   |   x  1  n    (  1  )   )     )   s     superscript     fragments  p   fragments  normal-(   superscript   subscript  y  1   n   normal-|   superscript   subscript  x  1   n    fragments  normal-(  2  normal-)   normal-)     fragments  p   fragments  normal-(   superscript   subscript  y  1   n   normal-|   superscript   subscript  x  1   n    fragments  normal-(  1  normal-)   normal-)     s    \left(\frac{p(y_{1}^{n}|x_{1}^{n}(2))}{p(y_{1}^{n}|x_{1}^{n}(1))}\right)^{s}     for    s  >   0       s  0    s>0\;   Thus,        P    error  1   →  2    ≤    ∑    x  1  n    (  2  )      Q   (    x  1  n    (  2  )    )     (    p   (   y  1  n   |   x  2  n    (  2  )   )     p   (   y  1  n   |   x  1  n    (  1  )   )     )   s      .       subscript  P   normal-→    error  1   2      subscript      superscript   subscript  x  1   n   2      Q     superscript   subscript  x  1   n   2    superscript     fragments  p   fragments  normal-(   superscript   subscript  y  1   n   normal-|   superscript   subscript  x  2   n    fragments  normal-(  2  normal-)   normal-)     fragments  p   fragments  normal-(   superscript   subscript  y  1   n   normal-|   superscript   subscript  x  1   n    fragments  normal-(  1  normal-)   normal-)     s       P_{\mathrm{error}\ 1\to 2}\leq\sum_{x_{1}^{n}(2)}Q(x_{1}^{n}(2))\left(\frac{p(%
 y_{1}^{n}|x_{2}^{n}(2))}{p(y_{1}^{n}|x_{1}^{n}(1))}\right)^{s}.     Since there are a total of M messages, the Probability that X (1) is confused with any other message is M times the above expression. Since each entry in the codebook is i.i.d., the notation of X (2) can be replaced simply by X . Using the Hokey union bound, the probability of confusing X (1) with any message is bounded by:        P    error  1   →  any    ≤    M  ρ     ∑   x  1  n     Q   (   x  1  n   )     (    p   (   y  1  n   |   x  2  n   )     p   (   y  1  n   |   x  1  n    (  1  )   )     )    s  ρ        .       subscript  P   normal-→    error  1   any       superscript  M  ρ     subscript    superscript   subscript  x  1   n      Q   superscript   subscript  x  1   n    superscript     fragments  p   fragments  normal-(   superscript   subscript  y  1   n   normal-|   superscript   subscript  x  2   n   normal-)     fragments  p   fragments  normal-(   superscript   subscript  y  1   n   normal-|   superscript   subscript  x  1   n    fragments  normal-(  1  normal-)   normal-)       s  ρ         P_{\mathrm{error}\ 1\to\mathrm{any}}\leq M^{\rho}\sum_{x_{1}^{n}}Q(x_{1}^{n})%
 \left(\frac{p(y_{1}^{n}|x_{2}^{n})}{p(y_{1}^{n}|x_{1}^{n}(1))}\right)^{s\rho}.     Averaging over all combinations of      X  1  n    (  1  )    ,   y  1  n         superscript   subscript  X  1   n   1    superscript   subscript  y  1   n     X_{1}^{n}(1),y_{1}^{n}   :       P    error  1   →  any    ≤   M  ρ    ∑   y  1  n     (   ∑    x  1  n    (  1  )     Q   (   x  1  n    (  1  )   )     [  p   (   y  1  n   |   x  1  n    (  1  )   )   ]    1  -   s  ρ     )     (   ∑   x  1  n    Q   (   x  1  n   )     [  p   (   y  1  n   |   x  1  n   )   ]   s   )   ρ   .     fragments   subscript  P   normal-→    error  1   any      superscript  M  ρ    subscript    superscript   subscript  y  1   n     fragments  normal-(   subscript      superscript   subscript  x  1   n   1    Q   fragments  normal-(   superscript   subscript  x  1   n    fragments  normal-(  1  normal-)   normal-)    superscript   fragments  normal-[  p   fragments  normal-(   superscript   subscript  y  1   n   normal-|   superscript   subscript  x  1   n    fragments  normal-(  1  normal-)   normal-)   normal-]     1    s  ρ     normal-)    superscript   fragments  normal-(   subscript    superscript   subscript  x  1   n    Q   fragments  normal-(   superscript   subscript  x  1   n   normal-)    superscript   fragments  normal-[  p   fragments  normal-(   superscript   subscript  y  1   n   normal-|   superscript   subscript  x  1   n   normal-)   normal-]   s   normal-)   ρ   normal-.    P_{\mathrm{error}\ 1\to\mathrm{any}}\leq M^{\rho}\sum_{y_{1}^{n}}\left(\sum_{x%
 _{1}^{n}(1)}Q(x_{1}^{n}(1))[p(y_{1}^{n}|x_{1}^{n}(1))]^{1-s\rho}\right)\left(%
 \sum_{x_{1}^{n}}Q(x_{1}^{n})[p(y_{1}^{n}|x_{1}^{n})]^{s}\right)^{\rho}.     Choosing    s  =   1  -   s  ρ        s    1    s  ρ      s=1-s\rho   and combining the two sums over    x  1  n     superscript   subscript  x  1   n    x_{1}^{n}   in the above formula:       P    error  1   →  any    ≤   M  ρ    ∑   y  1  n      (   ∑   x  1  n    Q   (   x  1  n   )     [  p   (   y  1  n   |   x  1  n   )   ]    1   1  +  ρ     )    1  +  ρ    .     fragments   subscript  P   normal-→    error  1   any      superscript  M  ρ    subscript    superscript   subscript  y  1   n     superscript   fragments  normal-(   subscript    superscript   subscript  x  1   n    Q   fragments  normal-(   superscript   subscript  x  1   n   normal-)    superscript   fragments  normal-[  p   fragments  normal-(   superscript   subscript  y  1   n   normal-|   superscript   subscript  x  1   n   normal-)   normal-]     1    1  ρ     normal-)     1  ρ    normal-.    P_{\mathrm{error}\ 1\to\mathrm{any}}\leq M^{\rho}\sum_{y_{1}^{n}}\left(\sum_{x%
 _{1}^{n}}Q(x_{1}^{n})[p(y_{1}^{n}|x_{1}^{n})]^{\frac{1}{1+\rho}}\right)^{1+%
 \rho}.     Using the independence nature of the elements of the codeword, and the discrete memoryless nature of the channel:       P    error  1   →  any    ≤   M  ρ    ∏   i  =  1   n    ∑   y  i      (   ∑   x  i     Q  i    (   x  i   )     [   p  i    (   y  i   |   x  i   )   ]    1   1  +  ρ     )    1  +  ρ       fragments   subscript  P   normal-→    error  1   any      superscript  M  ρ    superscript   subscript  product    i  1    n    subscript    subscript  y  i     superscript   fragments  normal-(   subscript    subscript  x  i     subscript  Q  i    fragments  normal-(   subscript  x  i   normal-)    superscript   fragments  normal-[   subscript  p  i    fragments  normal-(   subscript  y  i   normal-|   subscript  x  i   normal-)   normal-]     1    1  ρ     normal-)     1  ρ      P_{\mathrm{error}\ 1\to\mathrm{any}}\leq M^{\rho}\prod_{i=1}^{n}\sum_{y_{i}}%
 \left(\sum_{x_{i}}Q_{i}(x_{i})[p_{i}(y_{i}|x_{i})]^{\frac{1}{1+\rho}}\right)^{%
 1+\rho}     Using the fact that each element of codeword is identically distributed and thus stationary:       P    error  1   →  any    ≤   M  ρ     (   ∑  y     (   ∑  x   Q   (  x  )     [  p   (  y  |  x  )   ]    1   1  +  ρ     )    1  +  ρ    )   n   .     fragments   subscript  P   normal-→    error  1   any      superscript  M  ρ    superscript   fragments  normal-(   subscript   y    superscript   fragments  normal-(   subscript   x   Q   fragments  normal-(  x  normal-)    superscript   fragments  normal-[  p   fragments  normal-(  y  normal-|  x  normal-)   normal-]     1    1  ρ     normal-)     1  ρ    normal-)   n   normal-.    P_{\mathrm{error}\ 1\to\mathrm{any}}\leq M^{\rho}\left(\sum_{y}\left(\sum_{x}Q%
 (x)[p(y|x)]^{\frac{1}{1+\rho}}\right)^{1+\rho}\right)^{n}.     Replacing M by 2 nR and defining       E  o    (  ρ  ,  Q  )   =  -  ln   (   ∑  y     (   ∑  x   Q   (  x  )     [  p   (  y  |  x  )   ]    1   1  +  ρ     )    1  +  ρ    )   ,     fragments   subscript  E  o    fragments  normal-(  ρ  normal-,  Q  normal-)       fragments  normal-(   subscript   y    superscript   fragments  normal-(   subscript   x   Q   fragments  normal-(  x  normal-)    superscript   fragments  normal-[  p   fragments  normal-(  y  normal-|  x  normal-)   normal-]     1    1  ρ     normal-)     1  ρ    normal-)   normal-,    E_{o}(\rho,Q)=-\ln\left(\sum_{y}\left(\sum_{x}Q(x)[p(y|x)]^{\frac{1}{1+\rho}}%
 \right)^{1+\rho}\right),     probability of error becomes        P  error   ≤   exp   (   -   n   (     E  o    (  ρ  ,  Q  )    -   ρ  R    )     )     .       subscript  P  error         n       subscript  E  o    ρ  Q      ρ  R         P_{\mathrm{error}}\leq\exp(-n(E_{o}(\rho,Q)-\rho R)).     Q and   ρ   ρ   \rho   should be chosen so that the bound is tighest. Thus, the error exponent can be defined as         E  r    (  R  )    =      max  Q     max   ρ  ε   [  0  ,  1  ]      E  o      (  ρ  ,  Q  )    -   ρ  R     .         subscript  E  r   R         subscript   Q     subscript     ρ  ε   0  1      subscript  E  o      ρ  Q      ρ  R      E_{r}(R)=\max_{Q}\max_{\rho\varepsilon[0,1]}E_{o}(\rho,Q)-\rho R.\;     For time variant DMC channels  Error exponent in source coding  For time invariant discrete memoryless sources  The source coding theorem states that for any    ε  >  0      ε  0    \varepsilon>0   and any discrete-time i.i.d. source such as   X   X   X   and for any rate less than the entropy of the source, there is large enough   n   n   n   and an encoder that takes   n   n   n   i.i.d. repetition of the source,    X   1  :  n      superscript  X   normal-:  1  n     X^{1:n}   , and maps it to    n  .   (    H   (  X  )    +  ε   )      formulae-sequence  n      H  X   ε     n.(H(X)+\varepsilon)   binary bits such that the source symbols    X   1  :  n      superscript  X   normal-:  1  n     X^{1:n}   are recoverable from the binary bits with probability at least    1  -  ε      1  ε    1-\varepsilon   .  Let    M  =   e   n  R        M   superscript  e    n  R      M=e^{nR}\,\!   be the total number of possible messages. Next map each of the possible source output sequences to one of the messages randomly using a uniform distribution and independently from everything else. When a source is generated the corresponding message    M  =   m       M  m    M=m\,   is then transmitted to the destination. The message gets decoded to one of the possible source strings. In order to minimize the probability of error the decoder will decode to the source sequence    X  1  n     superscript   subscript  X  1   n    X_{1}^{n}   that maximizes    P   (   X  1  n   |   A  m   )      fragments  P   fragments  normal-(   superscript   subscript  X  1   n   normal-|   subscript  A  m   normal-)     P(X_{1}^{n}|A_{m})   , where     A  m      subscript  A  m    A_{m}\,   denotes the event that message   m   m   m   was transmitted. This rule is equivalent to finding the source sequence    X  1  n     superscript   subscript  X  1   n    X_{1}^{n}   among the set of source sequences that map to message   m   m   m   that maximizes    P   (   X  1  n   )       P   superscript   subscript  X  1   n     P(X_{1}^{n})   . This reduction follows from the fact that the messages were assigned randomly and independently of everything else.  Thus, as an example of when an error occurs, supposed that the source sequence     X  1  n    (  1  )        superscript   subscript  X  1   n   1    X_{1}^{n}(1)   was mapped to message   1   1   1   as was the source sequence     X  1  n    (  2  )        superscript   subscript  X  1   n   2    X_{1}^{n}(2)   . If     X  1  n    (  1  )        superscript   subscript  X  1   n   1    X_{1}^{n}(1)\,   was generated at the source, but     P   (    X  1  n    (  2  )    )    >   P   (    X  1  n    (  1  )    )          P     superscript   subscript  X  1   n   2      P     superscript   subscript  X  1   n   1      P(X_{1}^{n}(2))>P(X_{1}^{n}(1))   then an error occurs.  Let     S  i      subscript  S  i    S_{i}\,   denote the event that the source sequence     X  1  n    (  i  )        superscript   subscript  X  1   n   i    X_{1}^{n}(i)   was generated at the source, so that      P   (   S  i   )    =   P   (    X  1  n    (  i  )    )     .        P   subscript  S  i      P     superscript   subscript  X  1   n   i      P(S_{i})=P(X_{1}^{n}(i))\,.   Then the probability of error can be broken down as    P   (  E  )   =   ∑  i   P   (  E  |   S  i   )   P   (   S  i   )   .     fragments  P   fragments  normal-(  E  normal-)     subscript   i   P   fragments  normal-(  E  normal-|   subscript  S  i   normal-)   P   fragments  normal-(   subscript  S  i   normal-)   normal-.    P(E)=\sum_{i}P(E|S_{i})P(S_{i})\,.   Thus, attention can be focused on finding an upper bound to the    P   (  E  |   S  i   )      fragments  P   fragments  normal-(  E  normal-|   subscript  S  i   normal-)     P(E|S_{i})\,   .  Let     A   i  ′       subscript  A   superscript  i  normal-′     A_{i^{\prime}}\,   denote the event that the source sequence     X  1  n    (   i  ′   )        superscript   subscript  X  1   n    superscript  i  normal-′     X_{1}^{n}(i^{\prime})   was mapped to the same message as the source sequence     X  1  n    (  i  )        superscript   subscript  X  1   n   i    X_{1}^{n}(i)   and that     P   (    X  1  n    (   i  ′   )    )    ≥   P   (    X  1  n    (  i  )    )          P     superscript   subscript  X  1   n    superscript  i  normal-′       P     superscript   subscript  X  1   n   i      P(X_{1}^{n}(i^{\prime}))\geq P(X_{1}^{n}(i))   . Thus, letting     X   i  ,   i  ′        subscript  X   i   superscript  i  normal-′      X_{i,i^{\prime}}\,   denote the event that the two source sequences    i    i   i\,   and     i  ′      superscript  i  normal-′    i^{\prime}\,   map to the same message, we have that      P   (   A   i  ′    )   =  P   (   X   i  ,   i  ′     ⋂  P   (   X  1  n    (   i  ′   )   )   ≥  P   (   X  1  n    (  i  )   )   )      fragments  P   fragments  normal-(   subscript  A   superscript  i  normal-′    normal-)    P   fragments  normal-(   subscript  X   i   superscript  i  normal-′      P   fragments  normal-(   superscript   subscript  X  1   n    fragments  normal-(   superscript  i  normal-′   normal-)   normal-)    P   fragments  normal-(   superscript   subscript  X  1   n    fragments  normal-(  i  normal-)   normal-)   normal-)     P(A_{i^{\prime}})=P\left(X_{i,i^{\prime}}\bigcap P(X_{1}^{n}(i^{\prime})\right%
 )\geq P(X_{1}^{n}(i)))\,     and using the fact that     P   (   X   i  ,   i  ′     )    =    1  M          P   subscript  X   i   superscript  i  normal-′        1  M     P(X_{i,i^{\prime}})=\frac{1}{M}\,   and is independent of everything else have that      P   (   A   i  ′    )   =   1  M   P   (  P   (   X  1  n    (   i  ′   )   )   ≥  P   (   X  1  n    (  i  )   )   )   .     fragments  P   fragments  normal-(   subscript  A   superscript  i  normal-′    normal-)      1  M   P   fragments  normal-(  P   fragments  normal-(   superscript   subscript  X  1   n    fragments  normal-(   superscript  i  normal-′   normal-)   normal-)    P   fragments  normal-(   superscript   subscript  X  1   n    fragments  normal-(  i  normal-)   normal-)   normal-)   normal-.    P(A_{i^{\prime}})=\frac{1}{M}P(P(X_{1}^{n}(i^{\prime}))\geq P(X_{1}^{n}(i)))\,.     A simple upper bound for the term on the left can be established as       [  P   (  P   (   X  1  n    (   i  ′   )   )   ≥  P   (   X  1  n    (  i  )   )   )   ]   ≤     (    P   (    X  1  n    (   i  ′   )    )     P   (    X  1  n    (  i  )    )     )   s       fragments   fragments  normal-[  P   fragments  normal-(  P   fragments  normal-(   superscript   subscript  X  1   n    fragments  normal-(   superscript  i  normal-′   normal-)   normal-)    P   fragments  normal-(   superscript   subscript  X  1   n    fragments  normal-(  i  normal-)   normal-)   normal-)   normal-]     superscript   fragments  normal-(      P     superscript   subscript  X  1   n    superscript  i  normal-′       P     superscript   subscript  X  1   n   i     normal-)   s     \left[P(P(X_{1}^{n}(i^{\prime}))\geq P(X_{1}^{n}(i)))\right]\leq\left(\frac{P(%
 X_{1}^{n}(i^{\prime}))}{P(X_{1}^{n}(i))}\right)^{s}\,     for some arbitrary real number    s  >  0 .      s  0 .    s>0\,.   This upper bound can be verified by noting that    P   (  P   (   X  1  n    (   i  ′   )   )   >  P   (   X  1  n    (  i  )   )   )      fragments  P   fragments  normal-(  P   fragments  normal-(   superscript   subscript  X  1   n    fragments  normal-(   superscript  i  normal-′   normal-)   normal-)    P   fragments  normal-(   superscript   subscript  X  1   n    fragments  normal-(  i  normal-)   normal-)   normal-)     P(P(X_{1}^{n}(i^{\prime}))>P(X_{1}^{n}(i)))\,   either equals    1    1   1\,   or    0    0   0\,   because the probabilities of a given input sequence are completely deterministic. Thus, if      P   (    X  1  n    (   i  ′   )    )    ≥   P   (    X  1  n    (  i  )    )     ,        P     superscript   subscript  X  1   n    superscript  i  normal-′       P     superscript   subscript  X  1   n   i      P(X_{1}^{n}(i^{\prime}))\geq P(X_{1}^{n}(i))\,,   then      P   (    X  1  n    (   i  ′   )    )     P   (    X  1  n    (  i  )    )     ≥   1           P     superscript   subscript  X  1   n    superscript  i  normal-′       P     superscript   subscript  X  1   n   i     1    \frac{P(X_{1}^{n}(i^{\prime}))}{P(X_{1}^{n}(i))}\geq 1\,   so that the inequality holds in that case. The inequality holds in the other case as well because        (    P   (    X  1  n    (   i  ′   )    )     P   (    X  1  n    (  i  )    )     )   s   ≥   0        superscript      P     superscript   subscript  X  1   n    superscript  i  normal-′       P     superscript   subscript  X  1   n   i     s   0    \left(\frac{P(X_{1}^{n}(i^{\prime}))}{P(X_{1}^{n}(i))}\right)^{s}\geq 0\,     for all possible source strings. Thus, combining everything and introducing some    ρ  ∈   [  0  ,  1  ]       ρ   0  1     \rho\in[0,1]\,   , have that      P   (  E  |   S  i   )   ≤  P   (   ⋃   i  ≠   i  ′      A   i  ′    )   ≤    (   ∑   i  ≠   i  ′     P   (   A   i  ′    )   )   ρ   ≤     (   1  M    ∑   i  ≠   i  ′       (    P   (    X  1  n    (   i  ′   )    )     P   (    X  1  n    (  i  )    )     )   s   )   ρ    .     fragments  P   fragments  normal-(  E  normal-|   subscript  S  i   normal-)    P   fragments  normal-(   subscript     i   superscript  i  normal-′      subscript  A   superscript  i  normal-′    normal-)     superscript   fragments  normal-(   subscript     i   superscript  i  normal-′     P   fragments  normal-(   subscript  A   superscript  i  normal-′    normal-)   normal-)   ρ     superscript   fragments  normal-(    1  M    subscript     i   superscript  i  normal-′      superscript   fragments  normal-(      P     superscript   subscript  X  1   n    superscript  i  normal-′       P     superscript   subscript  X  1   n   i     normal-)   s   normal-)   ρ   normal-.    P(E|S_{i})\leq P(\bigcup_{i\neq i^{\prime}}A_{i^{\prime}})\leq\left(\sum_{i%
 \neq i^{\prime}}P(A_{i^{\prime}})\right)^{\rho}\leq\left(\frac{1}{M}\sum_{i%
 \neq i^{\prime}}\left(\frac{P(X_{1}^{n}(i^{\prime}))}{P(X_{1}^{n}(i))}\right)^%
 {s}\right)^{\rho}\,.     Where the inequalities follow from a variation on the Union Bound. Finally applying this upper bound to the summation for    P   (  E  )       P  E    P(E)\,   have that:      P   (  E  )   =   ∑  i   P   (  E  |   S  i   )   P   (   S  i   )   ≤   ∑  i   P   (   X  1  n    (  i  )   )      (   1  M    ∑   i  ′      (    P   (    X  1  n    (   i  ′   )    )     P   (    X  1  n    (  i  )    )     )   s   )   ρ    .     fragments  P   fragments  normal-(  E  normal-)     subscript   i   P   fragments  normal-(  E  normal-|   subscript  S  i   normal-)   P   fragments  normal-(   subscript  S  i   normal-)     subscript   i   P   fragments  normal-(   superscript   subscript  X  1   n    fragments  normal-(  i  normal-)   normal-)    superscript   fragments  normal-(    1  M    subscript    superscript  i  normal-′     superscript   fragments  normal-(      P     superscript   subscript  X  1   n    superscript  i  normal-′       P     superscript   subscript  X  1   n   i     normal-)   s   normal-)   ρ   normal-.    P(E)=\sum_{i}P(E|S_{i})P(S_{i})\leq\sum_{i}P(X_{1}^{n}(i))\left(\frac{1}{M}%
 \sum_{i^{\prime}}\left(\frac{P(X_{1}^{n}(i^{\prime}))}{P(X_{1}^{n}(i))}\right)%
 ^{s}\right)^{\rho}\,.     Where the sum can now be taken over all     i  ′      superscript  i  normal-′    i^{\prime}\,   because that will only increase the bound. Ultimately yielding that        P   (  E  )    ≤    1   M  ρ      ∑  i    P    (    X  1  n    (  i  )    )    1  -   s  ρ        (    ∑   i  ′     P    (    X  1  n    (   i  ′   )    )   s     )   ρ        .        P  E       1   superscript  M  ρ      subscript   i     P   superscript     superscript   subscript  X  1   n   i     1    s  ρ      superscript    subscript    superscript  i  normal-′      P   superscript     superscript   subscript  X  1   n    superscript  i  normal-′    s     ρ        P(E)\leq\frac{1}{M^{\rho}}\sum_{i}P(X_{1}^{n}(i))^{1-s\rho}\left(\sum_{i^{%
 \prime}}P(X_{1}^{n}(i^{\prime}))^{s}\right)^{\rho}\,.     Now for simplicity let     1  -   s  ρ    =   s         1    s  ρ    s    1-s\rho=s\,   so that     s  =    1   1  +  ρ      .      s    1    1  ρ      s=\frac{1}{1+\rho}\,.   Substituting this new value of    s    s   s\,   into the above bound on the probability of error and using the fact that     i  ′      superscript  i  normal-′    i^{\prime}\,   is just a dummy variable in the sum gives the following as an upper bound on the probability of error:        P   (  E  )    ≤    1   M  ρ       (    ∑  i    P    (    X  1  n    (  i  )    )    1   1  +  ρ       )    1  +  ρ       .        P  E       1   superscript  M  ρ     superscript    subscript   i     P   superscript     superscript   subscript  X  1   n   i     1    1  ρ         1  ρ       P(E)\leq\frac{1}{M^{\rho}}\left(\sum_{i}P(X_{1}^{n}(i))^{\frac{1}{1+\rho}}%
 \right)^{1+\rho}\,.         M  =   e   n  R        M   superscript  e    n  R      M=e^{nR}\,\!   and each of the components of     X  1  n    (  i  )        superscript   subscript  X  1   n   i    X_{1}^{n}(i)\,   are independent. Thus, simplifying the above equation yields        P   (  E  )    ≤   exp   (   -   n   [    ρ  R   -    ln   (    ∑   x  i     P    (   x  i   )    1   1  +  ρ       )     (   1  +  ρ   )     ]     )     .        P  E         n   delimited-[]      ρ  R         subscript    subscript  x  i      P   superscript   subscript  x  i     1    1  ρ          1  ρ           P(E)\leq\exp\left(-n\left[\rho R-\ln\left(\sum_{x_{i}}P(x_{i})^{\frac{1}{1+%
 \rho}}\right)(1+\rho)\right]\right).     The term in the exponent should be maximized over    ρ    ρ   \rho\,   in order to achieve the tightest upper bound on the probability of error.  Letting       E  0    (  ρ  )    =    ln   (    ∑   x  i     P    (   x  i   )    1   1  +  ρ       )     (   1  +  ρ   )     ,         subscript  E  0   ρ         subscript    subscript  x  i      P   superscript   subscript  x  i     1    1  ρ          1  ρ      E_{0}(\rho)=\ln\left(\sum_{x_{i}}P(x_{i})^{\frac{1}{1+\rho}}\right)(1+\rho)\,,   see that the error exponent for the source coding case is:         E  r    (  R  )    =    max   ρ  ∈   [  0  ,  1  ]      [    ρ  R   -    E  0    (  ρ  )     ]     .         subscript  E  r   R     subscript     ρ   0  1         ρ  R      subscript  E  0   ρ       E_{r}(R)=\max_{\rho\in[0,1]}\left[\rho R-E_{0}(\rho)\right].\,     For time-variant DMC sources  See also   Source coding  Channel coding   References  R. Gallager, Information Theory and Reliable Communication, Wiley 1968  "  Category:Information theory  Category:Data compression   