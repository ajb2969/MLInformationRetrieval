   Empirical likelihood      Empirical likelihood   Empirical likelihood (EL) is an estimation method in statistics . Empirical likelihood estimates require few assumptions about the error distribution compared to similar methods like maximum likelihood . EL can handle data well as long as it is independent and identically distributed (iid). EL performs well even when the distribution is asymmetric or censored. EL methods are also useful since they can easily incorporate constraints and prior information. Art Owen pioneered work in this area with his 1988 paper.  Estimation procedure  EL estimates are calculated by maximizing the empirical likelihood function subject to constraints based on the estimating function and the trivial assumption that the probability weights of the likelihood function sum to 1. 1 This procedure is represented:       max    π  i   ,  θ      ∑   i  =  1   n    ln   π  i          subscript     subscript  π  i   θ      superscript   subscript     i  1    n      subscript  π  i       \max_{\pi_{i},\theta}\sum_{i=1}^{n}\ln\pi_{i}   Subject to the constraints      s  .  t  .      ∑   i  =  1   n    π  i    =  1   ,     ∑   i  =  1   n     π  i   h   (   y  i   ;  θ  )     =  0       formulae-sequence  s  t   formulae-sequence      superscript   subscript     i  1    n    subscript  π  i    1       superscript   subscript     i  1    n      subscript  π  i   h    subscript  y  i   θ     0      s.t.\sum_{i=1}^{n}\pi_{i}=1,\sum_{i=1}^{n}\pi_{i}h(y_{i};\theta)=0    2  The value of the theta parameter can be found by solving the Lagrangian :      ℒ  =      ∑   i  =  1   n    ln   π  i     +   μ   (   1  -    ∑   i  =  1   n    π  i     )     -   n   τ  ′     ∑   i  =  1   n     π  i   h   (   y  i   ;  θ  )           ℒ        superscript   subscript     i  1    n      subscript  π  i       μ    1    superscript   subscript     i  1    n    subscript  π  i         n   superscript  τ  normal-′     superscript   subscript     i  1    n      subscript  π  i   h    subscript  y  i   θ         \mathcal{L}=\sum_{i=1}^{n}\ln\pi_{i}+\mu(1-\sum_{i=1}^{n}\pi_{i})-n\tau^{%
 \prime}\sum_{i=1}^{n}\pi_{i}h(y_{i};\theta)    3  See also   Bootstrapping (statistics)  Jackknife (statistics)   Notes  References     Owen, Art B. "Empirical likelihood ratio confidence intervals for a single functional." Biometrika 75.2 (1988): 237-249. jstor   "  Category:Estimation theory  Category:Fitting probability distributions     Mittelhammer, Judge, and Miller (2000), 292. ↩  Bera, Y. Bilias (2002), 77. ↩  Bera, Y. Bilias (2002), 77. ↩     