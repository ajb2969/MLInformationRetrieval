   Big O in probability notation      Big O in probability notation   The order in probability notation is used in probability theory and statistical theory in direct parallel to the big-O notation that is standard in mathematics . Where the big-O notation deals with the convergence of sequences or sets of ordinary numbers, the order in probability notation deals with convergence of sets of random variables , where convergence is in the sense of convergence in probability . 1  Definitions  Small O: convergence in probability  For a set of random variables X n and a corresponding set of constants a n (both indexed by n , which need not be discrete), the notation       X  n   =    o  p    (   a  n   )         subscript  X  n      subscript  o  p    subscript  a  n      X_{n}=o_{p}(a_{n})\,     means that the set of values X n / a n converges to zero in probability as n approaches an appropriate limit. Equivalently, X n = o p ( a n ) can be written as X n / a n = o p (1), where X n = o p (1) is defined as,       lim   n  →  ∞    P   (  |   X  n   |  ≥  ε  )   =  0  ,     fragments   subscript    normal-→  n     P   fragments  normal-(  normal-|   subscript  X  n   normal-|   ε  normal-)    0  normal-,    \lim_{n\to\infty}P(|X_{n}|\geq\varepsilon)=0,     for every positive ε. 2  Big O: stochastic boundedness  The notation,        X  n   =    O  p    (   a  n   )     ,       subscript  X  n      subscript  O  p    subscript  a  n      X_{n}=O_{p}(a_{n}),\,   means that the set of values X n / a n is stochastically bounded. That is, for any ε > 0, there exists a finite M > 0 such that,      P   (  |   X  n   /   a  n   |  >  M  )   <  ε  ,  ∀  n  .     fragments  P   fragments  normal-(  normal-|   subscript  X  n     subscript  a  n   normal-|   M  normal-)    ε  normal-,  for-all  n  normal-.    P(|X_{n}/a_{n}|>M)<\varepsilon,\ \forall n.     Comparison of the two definitions  The difference between the definition is subtle. If one uses the definition of the limit, one gets:   Big O p (1)    ∀  ε  ∃   N  ε   ,   δ  ε   such that  P   (  |   X  n   |  ≥   δ  ε   )   ≤  ε  ∀  n  >   N  ε      fragments  for-all  ε     subscript  N  ε   normal-,   subscript  δ  ε    such that  P   fragments  normal-(  normal-|   subscript  X  n   normal-|    subscript  δ  ε   normal-)    ε   for-all  n    subscript  N  ε     \forall\varepsilon\quad\exists N_{\varepsilon},\delta_{\varepsilon}\quad\text{%
  such that }P(|X_{n}|\geq\delta_{\varepsilon})\leq\varepsilon\quad\forall n>N_%
 {\varepsilon}     Small o p (1)    ∀  ε  ,  δ  ∃   N  ε   such that  P   (  |   X  n   |  ≥  δ  )   ≤  ε  ∀  n  >   N  ε      fragments  for-all  ε  normal-,  δ     subscript  N  ε    such that  P   fragments  normal-(  normal-|   subscript  X  n   normal-|   δ  normal-)    ε   for-all  n    subscript  N  ε     \forall\varepsilon,\delta\quad\exists N_{\varepsilon}\quad\text{ such that }P(%
 |X_{n}|\geq\delta)\leq\varepsilon\quad\forall n>N_{\varepsilon}      The difference lies in the δ: for stochastic boundedness, it suffices that there exists one (arbitrary large) δ to satisfy the inequality, and δ is allowed to be dependent on ε (hence the δ ε ). On the other side, for convergence, the statement has to hold not only for one, but for any (arbitrary small) δ. In a sense, this means that the sequence must be bounded, with a bound that gets smaller as the sample size increases.  This suggests that if a sequence is o p (1), then it is O p (1), i.e. convergence in probability implies stochastic boundedness. But the reverse does not hold.  Example  If    (   X  n   )     subscript  X  n    (X_{n})   is a stochastic sequence such that each element has finite variance, then        X  n   -   E   (   X  n   )     =    O  p    (    var   (   X  n   )     )           subscript  X  n     E   subscript  X  n        subscript  O  p      var   subscript  X  n        X_{n}-E(X_{n})=O_{p}(\sqrt{\operatorname{var}(X_{n})})\,   (see Theorem 14.4-1 in Bishop et al.)  If, moreover,      a  n   -  2     var   (   X  n   )     =   var   (    a  n   -  1     X  n    )           superscript   subscript  a  n     2     var   subscript  X  n      var     superscript   subscript  a  n     1     subscript  X  n       a_{n}^{-2}\operatorname{var}(X_{n})=\operatorname{var}(a_{n}^{-1}X_{n})   is a null sequence for a sequence    (   a  n   )     subscript  a  n    (a_{n})   of real numbers, then     a  n   -  1     (    X  n   -   E   (   X  n   )     )        superscript   subscript  a  n     1       subscript  X  n     E   subscript  X  n       a_{n}^{-1}(X_{n}-E(X_{n}))   converges to zero in probability by Chebyshev's inequality , so        X  n   -   E   (   X  n   )     =    o  p    (   a  n   )           subscript  X  n     E   subscript  X  n        subscript  o  p    subscript  a  n      X_{n}-E(X_{n})=o_{p}(a_{n})   .  References    "  Category:Mathematical notation  Category:Probability theory  Category:Statistical terminology     Dodge, Y. (2003) The Oxford Dictionary of Statistical Terms , OUP. ISBN 0-19-920613-9 ↩  Yvonne M. Bishop, Stephen E. Fienberg, Paul W. Holland. (1975,2007) Discrete multivariate analysis , Springer. ISBN 0-387-72805-8, ISBN 978-0-387-72805-6 ↩     