   Optimal stopping      Optimal stopping   In mathematics , the theory of optimal stopping is concerned with the problem of choosing a time to take a particular action, in order to maximise an expected reward or minimise an expected cost. Optimal stopping problems can be found in areas of statistics , economics , and mathematical finance (related to the pricing of American options ). A key example of an optimal stopping problem is the secretary problem . Optimal stopping problems can often be written in the form of a Bellman equation , and are therefore often solved using dynamic programming .  Definition  Discrete time case  Stopping rule problems are associated with two objects:   A sequence of random variables     X  1   ,   X  2   ,  …      subscript  X  1    subscript  X  2   normal-…    X_{1},X_{2},\ldots   , whose joint distribution is something assumed to be known  A sequence of 'reward' functions     (   y  i   )    i  ≥  1      subscript   subscript  y  i     i  1     (y_{i})_{i\geq 1}   which depend on the observed values of the random variables in 1.:      y  i   =    y  i    (   x  1   ,  …  ,   x  i   )         subscript  y  i      subscript  y  i     subscript  x  1   normal-…   subscript  x  i       y_{i}=y_{i}(x_{1},\ldots,x_{i})      Given those objects, the problem is as follows:   You are observing the sequence of random variables, and at each step   i   i   i   , you can choose to either stop observing or continue  If you stop observing at step   i   i   i   , you will receive reward    y  i     subscript  y  i    y_{i}     You want to choose a stopping rule to maximise your expected reward (or equivalently, minimise your expected loss)   Continuous time case  Consider a gain processes    G  =    (   G  t   )    t  ≥  0        G   subscript   subscript  G  t     t  0      G=(G_{t})_{t\geq 0}   defined on a filtered  probability space     (  Ω  ,  ℱ  ,    (   ℱ  t   )    t  ≥  0    ,  ℙ  )     normal-Ω  ℱ   subscript   subscript  ℱ  t     t  0    ℙ    (\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\geq 0},\mathbb{P})   and assume that   G   G   G   is adapted to the filtration. The optimal stopping problem is to find the stopping time     τ  *     superscript  τ     \tau^{*}   which maximizes the expected gain       V  t  T   =   𝔼   G   τ  *     =    sup   t  ≤  τ  ≤  T     𝔼   G  τ            superscript   subscript  V  t   T     𝔼   subscript  G   superscript  τ             subscript  supremum      t  τ       T       𝔼   subscript  G  τ        V_{t}^{T}=\mathbb{E}G_{\tau^{*}}=\sup_{t\leq\tau\leq T}\mathbb{E}G_{\tau}   where    V  t  T     superscript   subscript  V  t   T    V_{t}^{T}   is called the value function . Here   T   T   T   can take value   ∞     \infty   .  A more specific formulation is as follows. We consider an adapted strong Markov process     X  =    (   X  t   )    t  ≥  0        X   subscript   subscript  X  t     t  0      X=(X_{t})_{t\geq 0}   defined on a filtered probability space    (  Ω  ,  ℱ  ,    (   ℱ  t   )    t  ≥  0    ,   ℙ  x   )     normal-Ω  ℱ   subscript   subscript  ℱ  t     t  0     subscript  ℙ  x     (\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\geq 0},\mathbb{P}_{x})   where    ℙ  x     subscript  ℙ  x    \mathbb{P}_{x}   denotes the probability measure where the stochastic process starts at   x   x   x   . Given continuous functions    M  ,  L     M  L    M,L   , and   K   K   K   , the optimal stopping problem is        V   (  x  )    =    sup   0  ≤  τ  ≤  T      𝔼  x    (    M   (   X  τ   )    +    ∫  0  τ    L   (   X  t   )   d  t    +    sup   0  ≤  t  ≤  τ     K   (   X  t   )      )      .        V  x     subscript  supremum      0  τ       T        subscript  𝔼  x       M   subscript  X  τ      superscript   subscript   0   τ     L   subscript  X  t   d  t      subscript  supremum      0  t       τ       K   subscript  X  t          V(x)=\sup_{0\leq\tau\leq T}\mathbb{E}_{x}\left(M(X_{\tau})+\int_{0}^{\tau}L(X_%
 {t})dt+\sup_{0\leq t\leq\tau}K(X_{t})\right).   This is sometimes called the MLS (which stand for Mayer, Lagrange, and supremum, respectively) formulation. 1  Solution methods  There are generally two approaches of solving optimal stopping problems. 2 When the underlying process (or the gain process) is described by its unconditional finite-dimensional distributions , the appropriate solution technique is the martingale approach, so called because it uses martingale theory, the most important concept being the Snell envelope . In the discrete time case, if the planning horizon   T   T   T   is finite, the problem can also be easily solved by dynamic programming .  When the underlying process is determined by a family of (conditional) transition functions leading to a Markovian family of transition probabilities, very powerful analytical tools provided by the theory of Markov processes can often be utilized and this approach is referred to as the Markovian method. The solution is usually obtained by solving the associated free-boundary problems ( Stefan problems ).  A jump diffusion result  Let    Y  t     subscript  Y  t    Y_{t}   be a Lévy diffusion in    ℝ  k     superscript  ℝ  k    \mathbb{R}^{k}   given by the SDE        d   Y  t    =    b   (   Y  t   )   d  t   +   σ   (   Y  t   )   d   B  t    +    ∫   ℝ  k     γ   (   Y   t  -    ,  z  )    N  ¯    (   d  t   ,   d  z   )       ,    Y  0   =  y      formulae-sequence      d   subscript  Y  t        b   subscript  Y  t   d  t     σ   subscript  Y  t   d   subscript  B  t      subscript    superscript  ℝ  k      γ    subscript  Y   limit-from  t     z    normal-¯  N      d  t     d  z           subscript  Y  0   y     dY_{t}=b(Y_{t})dt+\sigma(Y_{t})dB_{t}+\int_{\mathbb{R}^{k}}\gamma(Y_{t-},z)%
 \bar{N}(dt,dz),\quad Y_{0}=y   where   B   B   B   is an   m   m   m   -dimensional Brownian motion ,    N  ¯     normal-¯  N    \bar{N}   is an   l   l   l   -dimensional compensated Poisson random measure ,    b  :    ℝ  k   →   ℝ  k       normal-:  b   normal-→   superscript  ℝ  k    superscript  ℝ  k      b:\mathbb{R}^{k}\to\mathbb{R}^{k}   ,    σ  :    ℝ  k   →   ℝ   k  ×  m        normal-:  σ   normal-→   superscript  ℝ  k    superscript  ℝ    k  m       \sigma:\mathbb{R}^{k}\to\mathbb{R}^{k\times m}   , and    γ  :     ℝ  k   ×   ℝ  k    →   ℝ   k  ×  l        normal-:  γ   normal-→     superscript  ℝ  k    superscript  ℝ  k     superscript  ℝ    k  l       \gamma:\mathbb{R}^{k}\times\mathbb{R}^{k}\to\mathbb{R}^{k\times l}   are given functions such that a unique solution    (   Y  t   )     subscript  Y  t    (Y_{t})   exists. Let    𝒮  ⊂   ℝ  k       𝒮   superscript  ℝ  k     \mathcal{S}\subset\mathbb{R}^{k}   be an open set (the solvency region) and       τ  𝒮   =   inf   {   t  >  0   :    Y  t   ∉  𝒮   }         subscript  τ  𝒮    infimum   conditional-set    t  0      subscript  Y  t   𝒮       \tau_{\mathcal{S}}=\inf\{t>0:Y_{t}\notin\mathcal{S}\}   be the bankruptcy time. The optimal stopping problem is:        V   (  y  )    =    sup   τ  ≤   τ  𝒮       J  τ    (  y  )     =    sup   τ  ≤   τ  𝒮       𝔼  y    [    M   (   Y  τ   )    +    ∫  0  τ    L   (   Y  t   )   d  t     ]      .          V  y     subscript  supremum    τ   subscript  τ  𝒮        superscript  J  τ   y           subscript  supremum    τ   subscript  τ  𝒮        subscript  𝔼  y    delimited-[]      M   subscript  Y  τ      superscript   subscript   0   τ     L   subscript  Y  t   d  t           V(y)=\sup_{\tau\leq\tau_{\mathcal{S}}}J^{\tau}(y)=\sup_{\tau\leq\tau_{\mathcal%
 {S}}}\mathbb{E}_{y}\left[M(Y_{\tau})+\int_{0}^{\tau}L(Y_{t})dt\right].   It turns out that under some regularity conditions, 3 the following verification theorem holds:  If a function    ϕ  :    𝒮  ¯   →  ℝ      normal-:  ϕ   normal-→   normal-¯  𝒮   ℝ     \phi:\bar{\mathcal{S}}\to\mathbb{R}   satisfies       ϕ  ∈    C   (   𝒮  ¯   )    ∩    C  1    (  𝒮  )    ∩    C  2    (   𝒮  ∖   ∂  D    )         ϕ      C   normal-¯  𝒮       superscript  C  1   𝒮      superscript  C  2     𝒮    D        \phi\in C(\bar{\mathcal{S}})\cap C^{1}(\mathcal{S})\cap C^{2}(\mathcal{S}%
 \setminus\partial D)   where the continuation region is    D  =   {   y  ∈  𝒮   :    ϕ   (  y  )    >   M   (  y  )     }       D   conditional-set    y  𝒮       ϕ  y     M  y       D=\{y\in\mathcal{S}:\phi(y)>M(y)\}   ,      ϕ  ≥  M      ϕ  M    \phi\geq M   on   𝒮   𝒮   \mathcal{S}   , and        𝒜  ϕ   +  L   ≤  0          𝒜  ϕ   L   0    \mathcal{A}\phi+L\leq 0   on    𝒮  ∖   ∂  D       𝒮    D     \mathcal{S}\setminus\partial D   , where   𝒜   𝒜   \mathcal{A}   is the infinitesimal generator of    (   Y  t   )     subscript  Y  t    (Y_{t})      then     ϕ   (  y  )    ≥   V   (  y  )          ϕ  y     V  y     \phi(y)\geq V(y)   for all    y  ∈   𝒮  ¯       y   normal-¯  𝒮     y\in\bar{\mathcal{S}}   . Moreover, if         𝒜  ϕ   +  L   =  0          𝒜  ϕ   L   0    \mathcal{A}\phi+L=0   on   D   D   D      Then     ϕ   (  y  )    =   V   (  y  )          ϕ  y     V  y     \phi(y)=V(y)   for all    y  ∈   𝒮  ¯       y   normal-¯  𝒮     y\in\bar{\mathcal{S}}   and     τ  *   =   inf   {   t  >  0   :    Y  t   ∉  D   }         superscript  τ     infimum   conditional-set    t  0      subscript  Y  t   D       \tau^{*}=\inf\{t>0:Y_{t}\notin D\}   is an optimal stopping time.  These conditions can also be written is a more compact form (the integro-variational inequality ):        max   {    𝒜  ϕ   +  L   ,   M  -  ϕ   }    =  0            𝒜  ϕ   L     M  ϕ    0    \max\left\{\mathcal{A}\phi+L,M-\phi\right\}=0   on     𝒮  ∖   ∂  D    .      𝒮    D     \mathcal{S}\setminus\partial D.      Examples  Coin tossing  (Example where    𝔼   (   y  i   )       𝔼   subscript  y  i     \mathbb{E}(y_{i})   converges)  You have a fair coin and are repeatedly tossing it. Each time, before it is tossed, you can choose to stop tossing it and get paid (in dollars, say) the average number of heads observed.  You wish to maximise the amount you get paid by choosing a stopping rule. If X i (for i ≥ 1) forms a sequence of independent, identically distributed random variables with Bernoulli distribution       Bern   (   1  2   )    ,      Bern    1  2     \text{Bern}\left(\frac{1}{2}\right),   and if       y  i   =    1  i     ∑   k  =  1   i    X  k          subscript  y  i       1  i     superscript   subscript     k  1    i    subscript  X  k       y_{i}=\frac{1}{i}\sum_{k=1}^{i}X_{k}   then the sequences     (   X  i   )    i  ≥  1      subscript   subscript  X  i     i  1     (X_{i})_{i\geq 1}   , and     (   y  i   )    i  ≥  1      subscript   subscript  y  i     i  1     (y_{i})_{i\geq 1}   are the objects associated with this problem.  House selling  (Example where    𝔼   (   y  i   )       𝔼   subscript  y  i     \mathbb{E}(y_{i})   does not necessarily converge)  You have a house and wish to sell it. Each day you are offered    X  n     subscript  X  n    X_{n}   for your house, and pay   k   k   k   to continue advertising it. If you sell your house on day   n   n   n   , you will earn    y  n     subscript  y  n    y_{n}   , where     y  n   =   (    X  n   -   n  k    )        subscript  y  n      subscript  X  n     n  k      y_{n}=(X_{n}-nk)   .  You wish to maximise the amount you earn by choosing a stopping rule.  In this example, the sequence (    X  i     subscript  X  i    X_{i}   ) is the sequence of offers for your house, and the sequence of reward functions is how much you will earn.  Secretary problem  (Example where    (   X  i   )     subscript  X  i    (X_{i})   is a finite sequence)  You are observing a sequence of objects which can be ranked from best to worst. You wish to choose a stopping rule which maximises your chance of picking the best object.  Here, if     R  1   ,  …  ,   R  n       subscript  R  1   normal-…   subscript  R  n     R_{1},\ldots,R_{n}   ( n is some large number, perhaps) are the ranks of the objects, and    y  i     subscript  y  i    y_{i}   is the chance you pick the best object if you stop intentionally rejecting objects at step i, then    (   R  i   )     subscript  R  i    (R_{i})   and    (   y  i   )     subscript  y  i    (y_{i})   are the sequences associated with this problem. This problem was solved in the early 1960s by several people. An elegant solution to the secretary problem and several modifications of this problem is provided by the more recent odds algorithm of optimal stopping (Bruss algorithm).  Search theory  Economists have studied a number of optimal stopping problems similar to the 'secretary problem', and typically call this type of analysis 'search theory'. Search theory has especially focused on a worker's search for a high-wage job, or a consumer's search for a low-priced good.  Option trading  In the trading of options on financial markets , the holder of an American option is allowed to exercise the right to buy (or sell) the underlying asset at a predetermined price at any time before or at the expiry date. Therefore the valuation of American options is essentially an optimal stopping problem. Consider a classical Black-Scholes set-up and let   r   r   r   be the risk-free interest rate and   δ   δ   \delta   and   σ   σ   \sigma   be the dividend rate and volatility of the stock. The stock price   S   S   S   follows geometric Brownian motion       S  t   =    S  0    exp   {     (   r  -  δ  -    σ  2   2    )   t   +   σ   B  t     }          subscript  S  t      subscript  S  0           r  δ     superscript  σ  2   2    t     σ   subscript  B  t         S_{t}=S_{0}\exp\left\{\left(r-\delta-\frac{\sigma^{2}}{2}\right)t+\sigma B_{t}\right\}   under the risk-neutral measure. When the option is perpetual, the optimal stopping problem is       V   (  x  )    =    sup  τ     𝔼  x    [    e   -   r  τ     g   (   S  τ   )    ]           V  x     subscript  supremum  τ      subscript  𝔼  x    delimited-[]     superscript  e      r  τ     g   subscript  S  τ         V(x)=\sup_{\tau}\mathbb{E}_{x}\left[e^{-r\tau}g(S_{\tau})\right]   where the payoff function is     g   (  x  )    =    (   x  -  K   )   +         g  x    superscript    x  K       g(x)=(x-K)^{+}   for a call option and     g   (  x  )    =    (   K  -  x   )   +         g  x    superscript    K  x       g(x)=(K-x)^{+}   for a put option. The variational inequality is       max   {      1  2    σ  2    x  2    V  ′′    (  x  )    +    (   r  -  δ   )   x   V  ′    (  x  )     -   r  V   (  x  )     ,    g   (  x  )    -   V   (  x  )     }    =  0                1  2    superscript  σ  2    superscript  x  2    superscript  V  ′′   x       r  δ   x   superscript  V  normal-′   x      r  V  x        g  x     V  x     0    \max\left\{\frac{1}{2}\sigma^{2}x^{2}V^{\prime\prime}(x)+(r-\delta)xV^{\prime}%
 (x)-rV(x),g(x)-V(x)\right\}=0   for all    x  ∈    (  0  ,  ∞  )   ∖   {  b  }        x     0     b      x\in(0,\infty)\setminus\{b\}   where   b   b   b   is the exercise boundary. The solution is known to be 4   (Perpetual call)     V   (  x  )    =   {       (   b  -  K   )     (   x  /  b   )   γ       x  ∈   (  0  ,  b  )         x  -  K      x  ∈   [  b  ,  ∞  )              V  x    cases      b  K    superscript    x  b   γ      x   0  b      x  K     x   b        V(x)=\begin{cases}(b-K)(x/b)^{\gamma}&x\in(0,b)\\
 x-K&x\in[b,\infty)\end{cases}   where    γ  =    (      ν  2   +   2  r     -  ν   )   /  σ       γ           superscript  ν  2     2  r     ν   σ     \gamma=(\sqrt{\nu^{2}+2r}-\nu)/\sigma   and      ν  =     (   r  -  δ   )   /  σ   -   σ  /  2     ,   b  =    γ  K   /   (   γ  -  1   )      .     formulae-sequence    ν        r  δ   σ     σ  2       b      γ  K     γ  1       \nu=(r-\delta)/\sigma-\sigma/2,\quad b=\gamma K/(\gamma-1).     (Perpetual put)     V   (  x  )    =   {      K  -  x      x  ∈   (  0  ,  c  ]          (   K  -  c   )     (   x  /  c   )    γ  ~        x  ∈   (  c  ,  ∞  )              V  x    cases    K  x     x   0  c        K  c    superscript    x  c    normal-~  γ       x   c        V(x)=\begin{cases}K-x&x\in(0,c]\\
 (K-c)(x/c)^{\tilde{\gamma}}&x\in(c,\infty)\end{cases}   where     γ  ~   =   -    (      ν  2   +   2  r     +  ν   )   /  σ         normal-~  γ              superscript  ν  2     2  r     ν   σ      \tilde{\gamma}=-(\sqrt{\nu^{2}+2r}+\nu)/\sigma   and      ν  =     (   r  -  δ   )   /  σ   -   σ  /  2     ,   c  =     γ  ~   K   /   (    γ  ~   -  1   )      .     formulae-sequence    ν        r  δ   σ     σ  2       c       normal-~  γ   K      normal-~  γ   1       \nu=(r-\delta)/\sigma-\sigma/2,\quad c=\tilde{\gamma}K/(\tilde{\gamma}-1).      If the expiry date is finite, the problem is associated with a 2-dimensional free-boundary problem with no known closed-form solution. Various numerical methods can be used.  See also   Stochastic control  Markov decision process   References   Chow, Y.S., Robbins, H. and Siegmund, D. (1971) Great Expectations: The Theory of Optimal Stopping. Boston: Houghton Mifflin  T. P. Hill. " Knowing When to Stop ". American Scientist , Vol. 97, 126-133 (2009). (For French translation, see cover story in the July issue of Pour la Science (2009))  Optimal Stopping and Applications , retrieved on 21 June 2007  Thomas S. Ferguson. "Who solved the secretary problem?" Statistical Science , Vol. 4.,282-296, (1989)  F. Thomas Bruss . "Sum the odds to one and stop." Annals of Probability , Vol. 28, 1384–1391,(2000)  F. Thomas Bruss. "The art of a right decision: Why decision makers want to know the odds-algorithm." Newsletter of the European Mathematical Society , Issue 62, 14-20, (2006)  R. Rogerson, R. Shimer, and R. Wright (2005), 'Search-theoretic models of the labor market: a survey'. Journal of Economic Literature 43, pp. 959–88.   External links   Neil Bearden's Optimal Search Page   "  Category:Mathematical optimization  Category:Mathematical finance  Category:Decision theory  Category:Sequential methods  Category:Dynamic programming     ↩   ↩  ↩     