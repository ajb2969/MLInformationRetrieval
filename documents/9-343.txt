   Stochastic approximation      Stochastic approximation   Stochastic approximation methods are a family of iterative stochastic optimization  algorithms that attempt to find zeroes or extrema of functions which cannot be computed directly, but only estimated via noisy observations.  Mathematically, this refers to solving:          min   x  ‚àà  Œò     f    (  x  )    =     min   x  ‚àà  Œò    ùîº    [   F   (  x  ,  Œæ  )    ]            subscript     x  normal-Œò    f   x       subscript     x  normal-Œò    ùîº    delimited-[]    F   x  Œæ        \min_{x\in\Theta}\;f(x)=\min_{x\in\Theta}\mathbb{E}[F(x,\xi)]   where the objective is to find the parameter    x  ‚àà  Œò      x  normal-Œò    x\in\Theta   , which minimizes    f   (  x  )       f  x    f(x)   for some unknown random variable ,   Œæ   Œæ   \xi   . Denoting   d   d   d   as the dimension of the parameter   x   x   x   , we can assume that while the domain    Œò  ‚äÇ   ‚Ñù  d       normal-Œò   superscript  ‚Ñù  d     \Theta\subset\mathbb{R}^{d}   is known, the objective function,    f   (  x  )       f  x    f(x)   , cannot be computed exactly, but instead approximated via simulation. This can be intuitively explained as follows.    f   (  x  )       f  x    f(x)   is the original function we want to minimize. However, due to noise,    f   (  x  )       f  x    f(x)   can not be evaluated exactly. This situation is modeled by the function    F   (  x  ,  Œæ  )       F   x  Œæ     F(x,\xi)   , where   Œæ   Œæ   \xi   represents the noise and is a random variable. Since   Œæ   Œæ   \xi   is a random variable, so is the value of    F   (  x  ,  Œæ  )       F   x  Œæ     F(x,\xi)   . The objective is then to minimize    f   (  x  )       f  x    f(x)   , but through evaluating    F   (  x  ,  Œæ  )       F   x  Œæ     F(x,\xi)   . A reasonable way to do this is to minimize the expectation of    F   (  x  ,  Œæ  )       F   x  Œæ     F(x,\xi)   , i.e.,    ùîº   [   F   (  x  ,  Œæ  )    ]       ùîº   delimited-[]    F   x  Œæ       \mathbb{E}[F(x,\xi)]   .  The first, and prototypical, algorithms of this kind are the Robbins-Monro and Kiefer-Wolfowitz algorithms introduced respectively in 1951 and 1952.  Robbins‚ÄìMonro algorithm  The Robbins‚ÄìMonro algorithm, introduced in 1951 by Herbert Robbins and Sutton Monro, 1 presented a methodology for solving a root finding problem, where the function is represented as an expected value. Assume that we have a function    M   (  x  )       M  x    M(x)   , and a constant   Œ±   Œ±   \alpha   , such that the equation     M   (  x  )    =  Œ±        M  x   Œ±    M(x)=\alpha   has a unique root at    x  =  Œ∏      x  Œ∏    x=\theta   . It is assumed that while we cannot directly observe the function    M   (  x  )       M  x    M(x)   , we can instead obtain measurements of the random variable    N   (  x  )       N  x    N(x)   where     ùîº   [   N   (  x  )    ]    =   M   (  x  )          ùîº   delimited-[]    N  x       M  x     \mathbb{E}[N(x)]=M(x)   . The structure of the algorithm is to then generate iterates of the form:          x   n  +  1    -   x  n    =    a  n    (   Œ±  -   N   (   x  n   )     )           subscript  x    n  1     subscript  x  n       subscript  a  n     Œ±    N   subscript  x  n        x_{n+1}-x_{n}=a_{n}(\alpha-N(x_{n}))        Here,     a  1   ,   a  2   ,  ‚Ä¶      subscript  a  1    subscript  a  2   normal-‚Ä¶    a_{1},a_{2},\dots   is a sequence of positive step sizes. Robbins and Monro proved 2 , Theorem 2 that    x  n     subscript  x  n    x_{n}    converges in    L  2     superscript  L  2    L^{2}   (and hence also in probability) to   Œ∏   Œ∏   \theta   provided that:       N   (  x  )       N  x    N(x)   is uniformly bounded,      M   (  x  )       M  x    M(x)   is nondecreasing,       M  ‚Ä≤    (  Œ∏  )        superscript  M  normal-‚Ä≤   Œ∏    M^{\prime}(\theta)   exists and is positive, and  The sequence    a  n     subscript  a  n    a_{n}   satisfies the following requirements:             ‚àë   n  =  0   ‚àû    a  n    =   ‚àû  and       ‚àë   n  =  0   ‚àû    a  n  2    <  ‚àû       formulae-sequence      subscript   superscript       n  0     subscript  a  n      and        subscript   superscript       n  0     subscript   superscript  a  2   n        \qquad\sum^{\infty}_{n=0}a_{n}=\infty\quad\mbox{ and }\quad\sum^{\infty}_{n=0}%
 a^{2}_{n}<\infty\quad        A particular sequence of steps which satisfy these conditions, and was suggested by Robbins‚ÄìMonro, have the form     a  n   =   a  /  n        subscript  a  n     a  n     a_{n}=a/n   , for    a  >  0      a  0    a>0   . Other series are possible but in order to average out the noise in    N   (  x  )       N  x    N(x)   , the above condition must be met.  Complexity results   If    f   (  x  )       f  x    f(x)   is twice continuously differentiable, and strongly convex, and the minimizer of    f   (  x  )       f  x    f(x)   belongs to the interior of   Œò   normal-Œò   \Theta   , then the Robbins-Monro algorithm will achieve the asymptotically optimal convergence rate, with respect to the objective function, being     ùîº   [    f   (   x  n   )    -   f  *    ]    =   O   (   1  /  n   )          ùîº   delimited-[]      f   subscript  x  n     superscript  f         O    1  n      \mathbb{E}[f(x_{n})-f^{*}]=O(1/n)   , where    f  *     superscript  f     f^{*}   is the minimal value of    f   (  x  )       f  x    f(x)   over    x  ‚àà  Œò      x  normal-Œò    x\in\Theta   . 3 4  Conversely, in the general convex case, where we lack both the assumption of smoothness and strong convexity, Nemirovski and Yudin 5 have shown that the asymptotically optimal convergence rate, with respect to the objective function values, is    O   (   1  /   n    )       O    1    n      O(1/\sqrt{n})   . They have also proven that this rate cannot be improved.   Subsequent developments  While the Robbins-Monro algorithm is theoretically able to achieve    O   (   1  /  n   )       O    1  n     O(1/n)   under the assumption of twice continuous differentiability and strong convexity, it can perform quite poorly upon implementation. This is primarily due to the fact that the algorithm is very sensitive to the choice of the step size sequence, and the supposed asymptotically optimal step size policy can be quite harmful in the beginning. 6 7  To overcome this shortfall, Polyak and Juditsky, 8 presented a method of accelerating Robbins-Monro through the use of longer steps, and averaging of the iterates. The algorithm would have the following structure:           x   n  +  1    -   x  n    =    b  n    (   Œ±  -   N   (   x  n   )     )     ,     x  ¬Ø   n   =    1  n     ‚àë   i  =  0    n  -  1     x  i         formulae-sequence       subscript  x    n  1     subscript  x  n       subscript  b  n     Œ±    N   subscript  x  n          subscript   normal-¬Ø  x   n       1  n     subscript   superscript     n  1      i  0     subscript  x  i        x_{n+1}-x_{n}=b_{n}(\alpha-N(x_{n})),\qquad\bar{x}_{n}=\frac{1}{n}\sum^{n-1}_{%
 i=0}x_{i}        The convergence of     x  ¬Ø   n     subscript   normal-¬Ø  x   n    \bar{x}_{n}   to the unique root   Œ∏   Œ∏   \theta   relies on the condition that the step sequence    {   b  n   }      subscript  b  n     \{b_{n}\}   decreases sufficiently slowly. That is          b  n   ‚Üí  0   ,      b  n   -   b   n  +  1      b  n    =   o   (   b  n   )        formulae-sequence   normal-‚Üí   subscript  b  n   0          subscript  b  n    subscript  b    n  1      subscript  b  n      o   subscript  b  n       b_{n}\rightarrow 0,\qquad\frac{b_{n}-b_{n+1}}{b_{n}}=o(b_{n})        Therefore, the sequence     b  n   =   n   -  Œ±         subscript  b  n    superscript  n    Œ±      b_{n}=n^{-\alpha}   with    0  <  Œ±  <  1        0  Œ±       1     0<\alpha<1   satisfies this restriction, but    Œ±  =  1      Œ±  1    \alpha=1   does not, hence the longer steps. Under the assumptions outlined in the Robbins-Monro algorithm, the resulting modification will result in the same asymptotically optimal convergence rate    O   (   1  /  n   )       O    1  n     O(1/n)   yet with a more robust step size policy. 9  Prior to this, the idea of using longer steps and averaging the iterates had already been proposed by Nemirovski and Yudin 10 for the cases of solving the stochastic optimization problem with continuous convex objectives and for convex-concave saddle point problems. These algorithms were observed to attain the nonasymptotic rate    O   (   1  /   n    )       O    1    n      O(1/\sqrt{n})   .  Kiefer-Wolfowitz algorithm  The Kiefer-Wolfowitz algorithm, 11 was introduced in 1952, and was motivated by the publication of the Robbins-Monro algorithm. However, the algorithm was presented as a method which would stochastically estimate the maximum of a function. Let    M   (  x  )       M  x    M(x)   be a function which has a maximum at the point   Œ∏   Œ∏   \theta   . It is assumed that    M   (  x  )       M  x    M(x)   is unknown, however, certain observations    N   (  x  )       N  x    N(x)   , where     ùîº   [   N   (  x  )    ]    =   M   (  x  )          ùîº   delimited-[]    N  x       M  x     \mathbb{E}[N(x)]=M(x)   , can be made at any point   x   x   x   . The structure of the algorithm follows a gradient-like method, with the iterates being generated as follows:         x   n  +  1    =    x  n   +    a  n    (     N   (    x  n   +   c  n    )    -   N   (    x  n   -   c  n    )      c  n    )          subscript  x    n  1       subscript  x  n      subscript  a  n         N     subscript  x  n    subscript  c  n       N     subscript  x  n    subscript  c  n       subscript  c  n        x_{n+1}=x_{n}+a_{n}\bigg(\frac{N(x_{n}+c_{n})-N(x_{n}-c_{n})}{c_{n}}\bigg)        where the gradient of    M   (  x  )       M  x    M(x)   is approximated using finite differences. The sequence    {   c  n   }      subscript  c  n     \{c_{n}\}   specifies the sequence of finite difference widths used for the gradient approximation, while the sequence    {   a  n   }      subscript  a  n     \{a_{n}\}   specifies a sequence of positive step sizes taken along that direction. Kiefer and Wolfowitz proved that, if    M   (  x  )       M  x    M(x)   satisfied certain regularity conditions, then    x  n     subscript  x  n    x_{n}   will converge to   Œ∏   Œ∏   \theta   provided that:   The function    f   (  x  )       f  x    f(x)   has a unique point of maximum (minimum) and is strong concave (convex)  The algorithm was first presented with the requirement that the function    f   (  ‚ãÖ  )       f  normal-‚ãÖ    f(\cdot)   maintains strong global convexity (concavity) over the entire feasible space. Given this condition is too restrictive to impose over the entire domain, Kiefer and Wolfowitz proposed that it is sufficient to impose the condition over a compact set     C  0   ‚äÇ   ‚Ñù  d        subscript  C  0    superscript  ‚Ñù  d     C_{0}\subset\mathbb{R}^{d}   which is known to include the optimal solution.   The selected sequences    {   a  n   }      subscript  a  n     \{a_{n}\}   and    {   c  n   }      subscript  c  n     \{c_{n}\}   must be infinite sequences of positive numbers such that:           1.   c  n    ‚Üí  0   ,     a  n   ‚Üí   0  as     n  ‚Üí  ‚àû       formulae-sequence   normal-‚Üí   1.   subscript  c  n    0    formulae-sequence   normal-‚Üí   subscript  a  n    0  as     normal-‚Üí  n       \mbox{1. }\quad c_{n}\rightarrow 0,\quad a_{n}\rightarrow 0\quad\mbox{ as }%
 \quad n\rightarrow\infty                2.    ‚àë   n  =  0   ‚àû    a  n     =  ‚àû   ,     ‚àë   n  =  0   ‚àû     a  n  2    c  n  2     <  ‚àû      formulae-sequence     2.    subscript   superscript       n  0     subscript  a  n           subscript   superscript       n  0       subscript   superscript  a  2   n    subscript   superscript  c  2   n         \mbox{2. }\quad\sum^{\infty}_{n=0}a_{n}=\infty,\qquad\sum^{\infty}_{n=0}\frac{%
 a^{2}_{n}}{c^{2}_{n}}<\infty        A suitable choice of sequences, as recommended by Kiefer and Wolfowitz, would be     a  n   =   1  /  n        subscript  a  n     1  n     a_{n}=1/n   and     c  n   =   n   -   1  /  3          subscript  c  n    superscript  n      1  3       c_{n}=n^{-1/3}   .  Subsequent developments and important issues   The Kiefer Wolfowitz algorithm requires that for each gradient computation, at least    d  +  1      d  1    d+1   different parameter values must be simulated for every iteration of the algorithm, where   d   d   d   is the dimension of the search space. This means that when   d   d   d   is large, the Kiefer-Wolfowitz algorithm will require substantial computational effort per iteration, leading to slow convergence.  To address this problem, Spall proposed the use of simultaneous perturbations to estimate the gradient. This method would require only two simulations per iteration, regardless of the dimension   d   d   d   . 12   In the conditions required for convergence, the ability to specify a predetermined compact set that fulfills strong convexity (or concavity) and contains the unique solution can be difficult to find. With respect to real world applications, if the domain is quite large, these assumptions can be fairly restrictive and highly unrealistic.   Further developments  An extensive theoretical literature has grown up around these algorithms, concerning conditions for convergence, rates of convergence, multivariate and other generalizations, proper choice of step size, possible noise models, and so on. 13 14 These methods are also applied in control theory , in which case the unknown function which we wish to optimize or find the zero of may vary in time. In this case, the step size    a  n     subscript  a  n    a_{n}   should not converge to zero but should be chosen so as to track the function. 15 , 2nd ed., chapter 3  C. Johan Masreliez and R. Douglas Martin were the first to apply stochastic approximation to robust  estimation . 16  The main tool for analyzing stochastic approximations algorithms (including the Robbins-Monro and the Kiefer-Wolfowitz algorithms) is the theorem by Aryeh Dvoretzky published in the proceedings of the third Berkeley symposium on mathematical statistics and probability, 1956.  See also   Stochastic gradient descent  Stochastic optimization  Simultaneous perturbation stochastic approximation   References  "  Category:Stochastic optimization  Category:Statistical approximations     ‚Ü©   ‚Ü©  ‚Ü©  Problem Complexity and Method Efficiency in Optimization, A. Nemirovski and D. Yudin, Wiley -Intersci. Ser. Discrete Math  15  John Wiley  New York (1983) . ‚Ü©   Introduction to Stochastic Search and Optimization: Estimation, Simulation and Control, J.C. Spall, John Wiley  Hoboken, NJ , (2003). ‚Ü©  ‚Ü©   On Cezari's convergence of the steepest descent method for approximating saddle points of convex-concave functions, A. Nemirovski and D. Yudin, Dokl. Akad. Nauk SSR  2939 , (1978 (Russian)), Soviet Math. Dokl. 19 (1978 (English)). ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  Stochastic Approximation and Recursive Estimation , Mikhail Borisovich Nevel'son and Rafail Zalmanovich Has'minskiƒ≠, translated by Israel Program for Scientific Translations and B. Silver, Providence, RI: American Mathematical Society, 1973, 1976. ISBN 0-8218-1597-0. ‚Ü©   ‚Ü©     