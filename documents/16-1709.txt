   Multi-fractional order estimator      Multi-fractional order estimator  The multi-fractional order estimator ( MFOE ) 1 2 is a straightforward, practical, and flexible alternative to the Kalman filter (KF) 3 4 for tracking targets. 5 The MFOE is focused strictly on simple and pragmatic fundamentals along with the integrity of mathematical modeling. Like the KF, the MFOE is based on the least squares method (LSM) invented by Gauss 6 7 8 and the orthogonality principle at the center of Kalman's derivation. 9 10 11 12 Optimized, the MFOE yields better accuracy than the KF and subsequent algorithms such as the extended KF 13 and the interacting multiple model (IMM). 14 15 16 17 The MFOE is an expanded form of the LSM, which effectively includes the KF 18 19 20 and ordinary least squares (OLS) 21 as subsets (special cases). OLS is revolutionized in 22 for application in econometrics . The MFOE also intersects with signal processing , estimation theory , economics , finance , statistics , and the method of moments . The MFOE offers two major advances: (1) minimizing the mean squared error (MSE) with fractions of estimated coefficients (useful in target tracking) 23 24 and (2) describing the effect of deterministic OLS processing of statistical inputs (of value in econometrics) 25  Description  Consider equally time spaced noisy measurement samples of a target trajectory described by 26 27      y  n   =     ∑   j  =  1   J     c  j    n   j  -  1      +   η  n    =    x  n   +   η  n           subscript  y  n       superscript   subscript     j  1    J      subscript  c  j    superscript  n    j  1       subscript  η  n            subscript  x  n    subscript  η  n       y_{n}=\sum_{j=1}^{J}c_{j}n^{j-1}+\eta_{n}=x_{n}+\eta_{n}     where n represents both the time samples and the index; the polynomial describing the trajectory is of degree J-1; and    η  n     subscript  η  n    \eta_{n}   is zero mean, stationary, white noise (not necessarily Gaussian) with variance    σ  n  2     superscript   subscript  σ  n   2    \sigma_{n}^{2}   .  Estimating x(t) at time   τ   τ   \tau   with the MFOE is described by        x  ^    (  τ  )    =    ∑   n  =  1   N     y  n    w  n    (  τ  )            normal-^  x   τ     superscript   subscript     n  1    N      subscript  y  n    subscript  w  n   τ      \hat{x}(\tau)=\sum_{n=1}^{N}y_{n}w_{n}(\tau)     where the hat (^) denotes an estimate, N is the number of samples in the data window,   τ   τ   \tau   is the time of the desired estimate, and the data weights are        w  n    (  τ  )    =    ∑  m     U   m  n     T  m    (  τ  )    f  m            subscript  w  n   τ     subscript   m      subscript  U    m  n     subscript  T  m   τ   subscript  f  m       w_{n}(\tau)=\sum_{m}U_{mn}T_{m}(\tau)f_{m}     The    U   m  n      subscript  U    m  n     U_{mn}   are orthogonal polynomial coefficient estimators.     T  m    (  τ  )        subscript  T  m   τ    T_{m}(\tau)   (a function detailed in 28 29 ) projects the estimate of the polynomial coefficient    c  m     subscript  c  m    c_{m}   to the desired estimation time   τ   τ   \tau   . The MFOE parameter 0≤    f  m     subscript  f  m    f_{m}   ≤1 can apply a fraction of the projected coefficient estimate.  The combined terms     U   m  n     T  m        subscript  U    m  n     subscript  T  m     U_{mn}T_{m}   effectively constitute a novel set of expansion functions with coefficients    f  m     subscript  f  m    f_{m}   . The MFOE can be optimized at time   τ   τ   \tau   as a function of the    f  m     subscript  f  m    f_{m}   s for given measurement noise, target dynamics, and non-recursive sliding data window size, N . However, for all     f  m   =  1       subscript  f  m   1    f_{m}=1   , the MFOE reduces and is equivalent to the KF in the absence of process noise, and to the standard polynomial LSM.  As in the case of coefficients in conventional series expansions, the    f  m     subscript  f  m    f_{m}   s typically decrease monotonically as higher order terms are included to match complex target trajectories. For example, in 30 the    f  m     subscript  f  m    f_{m}   s monotonically decreased in the MFOE from     f  1   =  1       subscript  f  1   1    f_{1}=1   to     f  5   ≳  0     greater-than-or-equivalent-to   subscript  f  5   0    f_{5}\gtrsim 0   , where     f  m   =  0       subscript  f  m   0    f_{m}=0   for m ≧ 6. The MFOE in 31 consisted of five point, 5th order processing of composite real (but altered for declassification) cruise missile data. A window of only 5 data points provided excellent maneuver following; whereas, 5th order processing included fractions of higher order terms to better approximate the complex maneuvering target trajectory. The MFOE overcomes the long-ago rejection of terms higher than 3rd order because, taken at full value (i.e.,      f  m   s   =  1         subscript  f  m   s   1    f_{m}s=1   ), estimator variances increase exponentially with linear order increases. (This is elucidated below in the section "Application of the FOE".)  Fractional order estimator  As described in, 32 33 the MFOE can be written more efficiently as     x  ^   =  <  ψ  ,   ω  m   >     fragments   normal-^  x     ψ  normal-,   subscript  ω  m      \hat{x}=<\psi,\omega_{m}>   where the estimator weights     w  n    (  τ  )        subscript  w  n   τ    w_{n}(\tau)   of order m are components of the estimating vector     ω  m    (  τ  )        subscript  ω  m   τ    \omega_{m}(\tau)   . By definition     x  ^   ≐    x  ^    (  τ  )       approaches-limit   normal-^  x      normal-^  x   τ     \hat{x}\doteq\hat{x}(\tau)   and     ω  m   ≐    ω  m    (  τ  )       approaches-limit   subscript  ω  m      subscript  ω  m   τ     \omega_{m}\doteq\omega_{m}(\tau)   . The angle brackets and comma denote the inner product, and the data vector   ψ   ψ   \psi   comprises noisy measurement samples    y  n     subscript  y  n    y_{n}   .  Perhaps the most useful MFOE tracking estimator is the simple fractional order estimator (FOE) where     f  1   =   f  2   =  1         subscript  f  1    subscript  f  2        1     f_{1}=f_{2}=1   and     f  m   =  0       subscript  f  m   0    f_{m}=0   for all m > 3, leaving only    0  ≤   f  3   ≤  1        0   subscript  f  3        1     0\leq f_{3}\leq 1   . This is effectively an FOE of fractional order    2  +   f  3       2   subscript  f  3     2+f_{3}   , which linear interpolates between the 2nd and 3rd order estimators described in 34 35 ) as       w   2  +   f  3     =     (   1  -   f  3    )    ω  2    +    f  3    ω  3     =    ω  2   +    f  3    (    ω  3   -   ω  2    )     =    ω  2   +    f  3    ν  3            subscript  w    2   subscript  f  3           1   subscript  f  3     subscript  ω  2       subscript  f  3    subscript  ω  3             subscript  ω  2      subscript  f  3      subscript  ω  3    subscript  ω  2              subscript  ω  2      subscript  f  3    subscript  ν  3        w_{2+f_{3}}=(1-f_{3})\omega_{2}+f_{3}\omega_{3}=\omega_{2}+f_{3}(\omega_{3}-%
 \omega_{2})=\omega_{2}+f_{3}\nu_{3}     where the scalar fraction    f  3     subscript  f  3    f_{3}   is the linear interpolation factor, the vector     ν  3   =    ω  3   -   ω  2    =    υ  3    T  3           subscript  ν  3      subscript  ω  3    subscript  ω  2            subscript  υ  3    subscript  T  3       \nu_{3}=\omega_{3}-\omega_{2}=\upsilon_{3}T_{3}   , and    υ  3     subscript  υ  3    \upsilon_{3}   (which comprises the components    U   3  n      subscript  U    3  n     U_{3n}   ) is the vector estimator of the 3rd polynomial coefficient     c  3   ≡    a   Δ  2    2        subscript  c  3       a   superscript  normal-Δ  2    2     c_{3}\equiv\tfrac{a\Delta^{2}}{2}   ( a is acceleration and Δ is the sample period). The vector    ν  3     subscript  ν  3    \nu_{3}   is the acceleration estimator from    ω  3     subscript  ω  3    \omega_{3}   .  The mean-square error (MSE) from the FOE applied to an accelerating target is 36 37      M  S  E   =     σ  η  2    (     |   ω  2   |   2   +    f  3  2     |   ν  3   |   2     )    +    [    c  3    T  3    (   1  -   f  3    )    ]   2          M  S  E        superscript   subscript  σ  η   2      superscript     subscript  ω  2    2      superscript   subscript  f  3   2    superscript     subscript  ν  3    2       superscript   delimited-[]     subscript  c  3    subscript  T  3     1   subscript  f  3      2      MSE=\sigma_{\eta}^{2}(|\omega_{2}|^{2}+f_{3}^{2}|\nu_{3}|^{2})+[c_{3}T_{3}(1-f%
 _{3})]^{2}   , where for any vector   θ   θ   \theta   ,    |  θ   |  2   ≐  <  θ  ,  θ  >     fragments  normal-|  θ   superscript  normal-|  2   approaches-limit   θ  normal-,  θ     |\theta|^{2}\doteq<\theta,\theta>   .  The first term on the right of the equal sign is the FOE target location estimator variance     σ  η  2    (     |   ω  2   |   2   +    f  3  2     |   ν  3   |   2     )        superscript   subscript  σ  η   2      superscript     subscript  ω  2    2      superscript   subscript  f  3   2    superscript     subscript  ν  3    2       \sigma_{\eta}^{2}(|\omega_{2}|^{2}+f_{3}^{2}|\nu_{3}|^{2})   composed of the 2nd order location estimator variance and part of the variance from the 3rd order acceleration estimator as determined by the interpolation factor squared    f  3  2     superscript   subscript  f  3   2    f_{3}^{2}   . The second term is the bias squared     [    c  3    T  3    (   1  -   f  3    )    ]   2     superscript   delimited-[]     subscript  c  3    subscript  T  3     1   subscript  f  3      2    [c_{3}T_{3}(1-f_{3})]^{2}   from the 2nd order target location estimator as a function of acceleration in    c  3     subscript  c  3    c_{3}   .  Setting the derivative of the MSE with respect to    f  3     subscript  f  3    f_{3}   equal to zero and solving yields the optimal    f  3     subscript  f  3    f_{3}   :       f   3  ,   o  p  t     ≐    f   3  ,   o  p  t      (  τ  )    =     (    c  3    T  3    )   2      (    c  3    T  3    )   2   +    σ  η  2     |   ν  3   |   2      =    c  3  2     c  3  2   +    σ  η  2     |   υ  3   |   2      =    ρ  3  2     ρ  3  2   +    |   υ  3   |   2          approaches-limit   subscript  f   3    o  p  t        subscript  f   3    o  p  t     τ           superscript     subscript  c  3    subscript  T  3    2      superscript     subscript  c  3    subscript  T  3    2      superscript   subscript  σ  η   2    superscript     subscript  ν  3    2              superscript   subscript  c  3   2      superscript   subscript  c  3   2      superscript   subscript  σ  η   2    superscript     subscript  υ  3    2              superscript   subscript  ρ  3   2      superscript   subscript  ρ  3   2    superscript     subscript  υ  3    2        f_{3,opt}\doteq f_{3,opt}(\tau)=\frac{(c_{3}T_{3})^{2}}{(c_{3}T_{3})^{2}+%
 \sigma_{\eta}^{2}|\nu_{3}|^{2}}=\frac{c_{3}^{2}}{c_{3}^{2}+\sigma_{\eta}^{2}|%
 \upsilon_{3}|^{2}}=\frac{\rho_{3}^{2}}{\rho_{3}^{2}+|\upsilon_{3}|^{2}}     where     ρ  3   ≡    c  3    σ  η    =    a   Δ  2     2   σ  η            subscript  ρ  3      subscript  c  3    subscript  σ  η             a   superscript  normal-Δ  2      2   subscript  σ  η        \rho_{3}\equiv\frac{c_{3}}{\sigma_{\eta}}=\frac{a\Delta^{2}}{2\sigma_{\eta}}   , as defined in. 38  The optimal FOE is then very simply       w   2  +   f   3  ,   o  p  t       =    ω  2   +    f   3  ,   o  p  t      ν  3     =    ω  2   +    υ  3    T  3    f   3  ,   o  p  t       =    ω  2   +    υ  3    T  3     ρ  3  2     ρ  3  2   +    |   υ  3   |   2              subscript  w    2   subscript  f   3    o  p  t          subscript  ω  2      subscript  f   3    o  p  t      subscript  ν  3             subscript  ω  2      subscript  υ  3    subscript  T  3    subscript  f   3    o  p  t               subscript  ω  2      subscript  υ  3    subscript  T  3      superscript   subscript  ρ  3   2      superscript   subscript  ρ  3   2    superscript     subscript  υ  3    2          w_{2+f_{3,opt}}=\omega_{2}+f_{3,opt}\nu_{3}=\omega_{2}+\upsilon_{3}T_{3}f_{3,%
 opt}=\omega_{2}+\upsilon_{3}T_{3}\frac{\rho_{3}^{2}}{\rho_{3}^{2}+|\upsilon_{3%
 }|^{2}}     Substituting the optimal FOE into the MSE yields the minimum MSE:       M  S   E   m  i  n     =    σ  η  2    (     |   ω  2   |   2   +    f   3  ,   o  p  t       |   ν  3   |   2     )          M  S   subscript  E    m  i  n        superscript   subscript  σ  η   2      superscript     subscript  ω  2    2      subscript  f   3    o  p  t      superscript     subscript  ν  3    2        MSE_{min}=\sigma_{\eta}^{2}(|\omega_{2}|^{2}+f_{3,opt}|\nu_{3}|^{2})    39 40  Although not obvious, the    M  S   E   m  i  n        M  S   subscript  E    m  i  n      MSE_{min}   includes the bias squared. The variance in the FOE MSE is the quadratic interpolation between the 2nd and the 3rd order location estimator variances as a function of    f   3  ,   o  p  t    2     superscript   subscript  f   3    o  p  t     2    f_{3,opt}^{2}   . Whereas, the    M  S   E   m  i  n        M  S   subscript  E    m  i  n      MSE_{min}   is the linear interpolation between the same 2nd and the 3rd order location estimator variances as a function of    f   3  ,   o  p  t       subscript  f   3    o  p  t      f_{3,opt}   . The bias squared accounts for the difference.  Application of the FOE  Since a target's future location is generally of more interest than where it is or has been, consider one-step prediction. Normalized with respect to measurement noise variance, the MSE for equally spaced samples reduces for the predicted position to       M  S  E   =    1  N   +    3   (   N  +  1   )     N   (   N  -  1   )     +    f  3  2     5   (   N  +  1   )    (   N  +  2   )     N   (   (  N  -  1  )    (  N  -  2  )       +    ρ  3  2     [     (   N  +  1   )    (   N  +  2   )    6   ]   2     (   1  -   f  3    )   2           M  S  E       1  N       3    N  1      N    N  1        superscript   subscript  f  3   2       5    N  1     N  2     fragments  N   fragments  normal-(   fragments  normal-(  N   1  normal-)    fragments  normal-(  N   2  normal-)          superscript   subscript  ρ  3   2    superscript   delimited-[]        N  1     N  2    6    2    superscript    1   subscript  f  3    2       MSE=\frac{1}{N}+\frac{3(N+1)}{N(N-1)}+f_{3}^{2}\frac{5(N+1)(N+2)}{N((N-1)(N-2)%
 }+\rho_{3}^{2}\left[\frac{(N+1)(N+2)}{6}\right]^{2}(1-f_{3})^{2}     where N is the number of samples in the non-recursive sliding data window. 41 Note that the first term on the right of the equal sign is the variance from estimating the first coefficient (position); the second term is the variance from estimating the 2nd coefficient (velocity); and the 3rd term with     f  3   =  1       subscript  f  3   1    f_{3}=1   is the variance from estimating the 3rd coefficient (which includes acceleration). This pattern continues for higher order terms. Furthermore, the sum of the variances from estimating the first two coefficients is      4  N   +  2    N   (   N  -  1   )            4  N   2     N    N  1      \frac{4N+2}{N(N-1)}   ). Adding the variance from estimating the 3rd coefficient yields      9   N  2    +   9  N   +  6    N   (   N  -  1   )    (   N  -  2   )            9   superscript  N  2      9  N   6     N    N  1     N  2      \frac{9N^{2}+9N+6}{N(N-1)(N-2)}   .  Estimator variances obviously increase exponentially with unit order increases. In the absence of process noise, the KF yields variances equivalent to these. 42 43 (A derivation of the variance from a 1st degree polynomial corresponding to     f  3   =   ρ  3   =  0         subscript  f  3    subscript  ρ  3        0     f_{3}=\rho_{3}=0   for the generalized case of arbitrary estimation time and sample times is given in reference. 44 In addition, establishing a multi-dimensional tracking gate at the predicted position can easily be aided with the simple approximation of the error function in. 45 )  Kalman filter tuning  Tuning the KF consists of a trade-off between measurement noise and process noise to minimize the estimation error. 46 47 The KF process noise serves two roles: First, its covariance is sized to account for the maximum expected target acceleration. Second, process noise covariance establishes an effective recursive data window (analogous to the non-recursive sliding data window), described by Brookner as the Kalman filter memory. 48  Contrary to process noise covariance as a single independent parameter in the KF serving two roles, the FOE has the advantage of two separate independent parameters: one for acceleration and the other for sizing the sliding data window. Therefore, as opposed to being limited to just two tuning parameters (process and measurement noises) as is the KF, the FOE includes three independent tuning parameters: measurement noise variance, the assumed maximum deterministic target acceleration (for simplicity both target acceleration and measurement noise are included in the ratio of the single parameter    ρ  3     subscript  ρ  3    \rho_{3}   ), and the number of samples in the data window.  Consider tuning a 2nd order predictor applied to the simple and practical tracking example in 49 to minimize the MSE when the target acceleration is     20  m   /   s  2         20  m    superscript  s  2     20m/s^{2}   ; the zero mean, stationary, and white measurement noise is described as     σ  η   =   25  m        subscript  σ  η     25  m     \sigma_{\eta}=25m   ; and   Δ   normal-Δ   \Delta   = 1 second. Thus,       ρ  3   =    a   Δ  2     2   σ  η     =   20  /  2  /  25   =  0.4         subscript  ρ  3       a   superscript  normal-Δ  2      2   subscript  σ  η            20  2  25        0.4     \rho_{3}=\frac{a\Delta^{2}}{2\sigma_{\eta}}=20/2/25=0.4     Setting     f  3   =  0       subscript  f  3   0    f_{3}=0   in the normalized prediction MSE yields for the 2nd order predictor applied to an accelerating target,       M  S  E   =      4  N   +  2    N   (   N  -  1   )     +    ρ  3  2     [     (   N  +  1   )    (   N  +  2   )    6   ]   2           M  S  E           4  N   2     N    N  1        superscript   subscript  ρ  3   2    superscript   delimited-[]        N  1     N  2    6    2       MSE=\frac{4N+2}{N(N-1)}+\rho_{3}^{2}\left[\frac{(N+1)(N+2)}{6}\right]^{2}     where the first term on the right of the equal sign is the normalized 2nd order one-step prediction variance and the second term is the normalized bias squared from acceleration. This MSE is plotted as a function of N in Figure 1 along with both the variance and bias squared.  Clearly, only integer order steps are possible in a non-recursive estimator. However, for use in approximating the tuned 2nd order KF, this MSE plot is stepped in tenths of a unit to show more precisely where the minimum occurs. The minimum MSE of 4.09 occurs at N = 2.9. The tuned KF can be approximated by sizing the process noise covariance in the KF such that the effective recursive data window—i.e., the Kalman filter memory 50 —matches N = 2.9 in Figure 1 (i.e.,    α  ≈  0.85      α  0.85    \alpha\approx 0.85   and    β  ≈  0.53      β  0.53    \beta\approx 0.53   ), where    α  =     4  N   -  2    N   (   N  +  1   )         α        4  N   2     N    N  1       \alpha=\frac{4N-2}{N(N+1)}   and    β  =   6   N   (   N  +  1   )         β    6    N    N  1       \beta=\frac{6}{N(N+1)}   . 51 This hints at the fallacy of using a 2nd order estimator on accelerating targets as described in. 52 Comparing this with the filtered position in 53 demonstrates that the minimum MSE is a function of the time   τ   τ   \tau   of the desired estimate.  FOE as a multiple-model estimator  The FOE can be viewed as a non-recursive multiple-model (MM) estimator composed of 2nd and 3rd order estimator models with the fraction    0  ≤   f  3   ≤  1        0   subscript  f  3        1     0\leq f_{3}\leq 1   as the interpolation factor. Since the filtered position is generally used for comparisons in the literature, consider now the normalized MSE for the position estimate:       M  S  E   =    1  N   +    3   (   N  -  1   )     N   (   N  +  1   )     +    f  3  2     5   (   N  -  1   )    (   N  -  2   )     N   (   (  N  +  1  )    (  N  +  2  )       +    ρ  3  2     [     (   N  -  1   )    (   N  -  2   )    6   ]   2     (   1  -   f  3    )   2           M  S  E       1  N       3    N  1      N    N  1        superscript   subscript  f  3   2       5    N  1     N  2     fragments  N   fragments  normal-(   fragments  normal-(  N   1  normal-)    fragments  normal-(  N   2  normal-)          superscript   subscript  ρ  3   2    superscript   delimited-[]        N  1     N  2    6    2    superscript    1   subscript  f  3    2       MSE=\frac{1}{N}+\frac{3(N-1)}{N(N+1)}+f_{3}^{2}\frac{5(N-1)(N-2)}{N((N+1)(N+2)%
 }+\rho_{3}^{2}\left[\frac{(N-1)(N-2)}{6}\right]^{2}(1-f_{3})^{2}     Note that this differs from the one-step prediction MSE in that the signs within the parentheses containing N are reversed. The higher order pattern continues here also. Normalized with respect to the measurement noise variance, the minimum position MSE reduces for equally spaced samples to       M  S   E   m  i  n     =      4  N   -  2    N   (   N  +  1   )     +    f   3  ,   o  p  t       5   (   N  -  1   )    (   N  -  2   )     N   (   (  N  +  1  )    (  N  +  2  )              M  S   subscript  E    m  i  n             4  N   2     N    N  1        subscript  f   3    o  p  t         5    N  1     N  2     fragments  N   fragments  normal-(   fragments  normal-(  N   1  normal-)    fragments  normal-(  N   2  normal-)          MSE_{min}=\frac{4N-2}{N(N+1)}+f_{3,opt}\frac{5(N-1)(N-2)}{N((N+1)(N+2)}     where      |   υ  3   |   2   =   180   N   (    N  2   -  1   )    (    N  2   -  4   )          superscript     subscript  υ  3    2     180    N     superscript  N  2   1      superscript  N  2   4       |\upsilon_{3}|^{2}=\frac{180}{N(N^{2}-1)(N^{2}-4)}     in     f   3  ,   o  p  t     =    ρ  3  2     ρ  3  2   +    |   υ  3   |   2          subscript  f   3    o  p  t        superscript   subscript  ρ  3   2      superscript   subscript  ρ  3   2    superscript     subscript  υ  3    2       f_{3,opt}=\frac{\rho_{3}^{2}}{\rho_{3}^{2}+|\upsilon_{3}|^{2}}    54 A plot of the position    M  S   E   m  i  n        M  S   subscript  E    m  i  n      MSE_{min}   as a function of N for various values of    ρ  3     subscript  ρ  3    \rho_{3}   is shown in Figure 2, where there are several points of interest: First, the 2nd and 3rd order MSEs track each other very closely and bound all the    M  S   E   m  i  n        M  S   subscript  E    m  i  n      MSE_{min}   (interpolated) curves. Second, the curves drop rapidly to a knee. Third, the    M  S   E   m  i  n        M  S   subscript  E    m  i  n      MSE_{min}   curves flatten out beyond the knee yielding virtually no increase in accuracy until they begin to approach the 3rd order MSE (variance). 55 This suggests that choosing a window at the knee of the curve is advantageous—to be demonstrated below.  Consider again the scenario of, 56 in this case as the target maneuvers. After traveling at a constant velocity, the target accelerates at     20  m   /   s  2         20  m    superscript  s  2     20m/s^{2}   for 20 seconds and then continues again at a constant velocity. At worst case acceleration,     ρ  3   =  0.4       subscript  ρ  3   0.4    \rho_{3}=0.4   . The    M  S   E   m  i  n        M  S   subscript  E    m  i  n      MSE_{min}   is plotted in Figure 3 of as a function of N . Also shown are the 2nd order MSE as well as the 2nd and 3rd order MSEs (variances only since the bias is zero in each case) similar to those in Figure 2. There is a fifth curve not previously addressed: the variance portion of the optimal MSE. The variance also levels off for several increments of N like the    M  S   E   m  i  n        M  S   subscript  E    m  i  n      MSE_{min}   . Both the variance and    M  S   E   m  i  n        M  S   subscript  E    m  i  n      MSE_{min}   approach the 3rd order variance as    N  →  ∞     normal-→  N     N\to\infty   .  As the acceleration varies from zero to maximum, the MSE is automatically adjusted (no external tinkering or adaptivity) between the variance at     ρ  3   =  0       subscript  ρ  3   0    \rho_{3}=0   and maximum    M  S   E   m  i  n        M  S   subscript  E    m  i  n      MSE_{min}   at     ρ  3   =  0.4       subscript  ρ  3   0.4    \rho_{3}=0.4   . In other words, the MSE rides up and down the quadratic curve of the variance plus bias squared as a function of changes in acceleration    ρ  3     subscript  ρ  3    \rho_{3}   for any given value of N in the position estimate:       M  S  E   =      4  N   -  2    N   (   N  +  1   )     +    ρ  3  2     [     (   N  -  1   )    (   N  -  2   )    6   ]   2           M  S  E           4  N   2     N    N  1        superscript   subscript  ρ  3   2    superscript   delimited-[]        N  1     N  2    6    2       MSE=\frac{4N-2}{N(N+1)}+\rho_{3}^{2}\left[\frac{(N-1)(N-2)}{6}\right]^{2}     Choosing N = 4 at the knee of the    M  S   E   m  i  n        M  S   subscript  E    m  i  n      MSE_{min}   curve in Figure 3 yields the RMSE (square root of the MSE, which is more often used for comparison in the literature) shown in Figure 4. On the other hand, choosing N = 8 yields the second curve in Figure 4. As shown in Figure 3, the optimal 8–point FOE is essentially a 3rd order non-recursive estimator which yields less than 4% RMSE improvement over the optimal 4-point FOE in the case of no acceleration. However, in the case of maximum acceleration the optimal 8-point MSE is markedly volatile and has large error spikes that can confuse a tracker, one spike exceeding the optimal 4-point MSE for worst case acceleration by more than the optimal 4-point MSE exceeds the optimal 8-point MSE in the absence of acceleration. Obviously, higher values of N produce larger error spikes.  Since trackers encounter greatest difficulties and often lose track during target maneuvers at maximum acceleration, the much smoother    M  S   E   m  i  n        M  S   subscript  E    m  i  n      MSE_{min}   transition of the optimal 4-point FOE has a major advantage over larger data windows.  IMM compared with the optimal FOE  The 4-point FOE in Figure 4 yields much smoother MSE transitions than the IMM (as well as the KF) in the parallel 1 Hz case of. 57 It produces no error spikes or volatility as do the 8-point FOE and the IMM. In this example only 4 multiplies, 3 adds, and a window shift are required to implement the 4-point FOE, significantly few operations than required by the IMM or KF. Similar comparisons of several additional MMs from the literature with the optimal FOE are made in 58  Of the KF based MMs, the interacting MM (IMM) is generally considered the state-of-the-art tracking model and usually the method of choice. 59 60 Since two model IMMs are most often used, 61 consider the following two models: 2nd and 3rd order KFs. The estimated IMM state equation is the sum of the 2nd order KF times the model probability     μ  1    (  k  )        subscript  μ  1   k    \mu_{1}(k)   plus the 3rd order KF times the model probability     μ  2    (  k  )        subscript  μ  2   k    \mu_{2}(k)   :       X  ^    (  k  |  k  )   =    X  ^   1    (  k  |  k  )    μ  1    (  k  )   +    X  ^   2    (  k  |  k  )    μ  2    (  k  )      fragments   normal-^  X    fragments  normal-(  k  normal-|  k  normal-)     subscript   normal-^  X   1    fragments  normal-(  k  normal-|  k  normal-)    subscript  μ  1    fragments  normal-(  k  normal-)     subscript   normal-^  X   2    fragments  normal-(  k  normal-|  k  normal-)    subscript  μ  2    fragments  normal-(  k  normal-)     \hat{X}(k|k)=\hat{X}_{1}(k|k)\mu_{1}(k)+\hat{X}_{2}(k|k)\mu_{2}(k)     where      X  ^   1    (  k  |  k  )      fragments   subscript   normal-^  X   1    fragments  normal-(  k  normal-|  k  normal-)     \hat{X}_{1}(k|k)   represents the 2nd order KF,      X  ^   2    (    |  k  |   k   )        subscript   normal-^  X   2       k   k     \hat{X}_{2}(|k|k)   represents the 3rd order KF, and k represents the time increment. 62 63 Since the model probabilities sum to one, i.e.,       μ  1    (  k  )    +    μ  2    (  k  )     =  1           subscript  μ  1   k      subscript  μ  2   k    1    \mu_{1}(k)+\mu_{2}(k)=1   ; 64 this is actually linear interpolation, where     μ  1    (  k  )        subscript  μ  1   k    \mu_{1}(k)   is analogous to    (   1  -   f  3    )      1   subscript  f  3     (1-f_{3})   in the FOE and     μ  2    (  k  )        subscript  μ  2   k    \mu_{2}(k)   is analogous to    f  3     subscript  f  3    f_{3}   . Therefore, this two model IMM is analogous to the optimal FOE in that it also interpolates between 2nd and 3rd order estimators. Two model IMM interpolation is formed during each recursive cycle involving the interactively produced model probabilities. 65 66 67 68 69  As in the case of the FOE, this suggests a more descriptive estimate equal to the sum of the 2nd order KF plus the difference between the 3rd and 2nd order KFs times     μ  2    (  k  )        subscript  μ  2   k    \mu_{2}(k)   :       X  ^    (  k  |  k  )   =    X  ^   1    (  k  |  k  )   +   [    X  ^   2    (  k  |  k  )   -    X  ^   1    (  k  |  k  ]    μ  2    (  k  )       fragments   normal-^  X    fragments  normal-(  k  normal-|  k  normal-)     subscript   normal-^  X   1    fragments  normal-(  k  normal-|  k  normal-)     fragments  normal-[   subscript   normal-^  X   2    fragments  normal-(  k  normal-|  k  normal-)     subscript   normal-^  X   1    fragments  normal-(  k  normal-|  k  normal-]    subscript  μ  2    fragments  normal-(  k  normal-)      \hat{X}(k|k)=\hat{X}_{1}(k|k)+[\hat{X}_{2}(k|k)-\hat{X}_{1}(k|k]\mu_{2}(k)     In this formulation the difference between the 3rd and 2nd order KFs effectively augments the 2nd order KF with a fraction of the estimated target acceleration as a function of     μ  2    (  k  )        subscript  μ  2   k    \mu_{2}(k)   —as does    f  3     subscript  f  3    f_{3}   in the FOE.  One major difference between the IMM and optimal FOE is that the IMM is not optimum. The IMM model probabilities and interpolation are based on likelihoods and ad  hoc transition probabilities with no mechanism for minimizing the MSE. 70 Of course, not being optimum at any time increment k , the IMM cannot achieve the optimal FOE accuracy shown in Figure 2.  Moveover, the IMM     μ  2    (  k  )        subscript  μ  2   k    \mu_{2}(k)   fails to meet the boundary condition of zero to implement the 2nd order estimator in the absence of acceleration, which the FOE    f   3  ,   o  p  t       subscript  f   3    o  p  t      f_{3,opt}   does. This results from the fact that the likelihoods do not sum to unity 71 even though the model probabilities do. This causes an IMM bias toward a non-existent acceleration and unnecessarily increases the MSE above the 2nd order variance. Another major difference between the IMM and FOE is that the IMM is adaptive whereas the FOE is not.  In order to make a reasonable comparison of the IMM with the FOE, reference 72 constructs a non-recursive IMM analogy (IMMA). It includes     μ  2    (  k  )        subscript  μ  2   k    \mu_{2}(k)   which does go to zero allowing the 2nd order estimator to be implemented. Since the FOE is based on the actual acceleration not a noisy estimate, the acceleration estimate for the IMMA is assumed to be the expected value of the estimate, i.e., the actual acceleration. This is described here as the ideal for the purpose of illustration. These two modifications make the IMMA compatible for comparison with the FOE.  The     μ  2    (  k  )        subscript  μ  2   k    \mu_{2}(k)   based on the expected value or actual acceleration (described here as the ideal    μ  2     subscript  μ  2    \mu_{2}   where the k is dropped) then varies between zero and one in an S shaped curve as a function of    ρ  3     subscript  ρ  3    \rho_{3}   , as does    f   3  ,   o  p  t       subscript  f   3    o  p  t      f_{3,opt}   . This is shown in Figure 5, where a 4-point data window is assumed.  Two significant points of interest stand out as shown by the vertical lines. First, the largest deviation of the ideal    μ  2     subscript  μ  2    \mu_{2}   from    f   3  ,   o  p  t       subscript  f   3    o  p  t      f_{3,opt}   occurs near     ρ  3   =  0.7       subscript  ρ  3   0.7    \rho_{3}=0.7   . Second, the two curves cross near     ρ  3   =  1.4       subscript  ρ  3   1.4    \rho_{3}=1.4   . A comparison of the one-step predictor IMMA MSE as a function of ideal    μ  2     subscript  μ  2    \mu_{2}   with the FOE    M  S   E   m  i  n        M  S   subscript  E    m  i  n      MSE_{min}   is given in Figure 6. 73 For the IMMA, the linear interpolation factor    f  3     subscript  f  3    f_{3}   is replaced in the normalized FOE MSE by the ideal    μ  2     subscript  μ  2    \mu_{2}   as the interpolation factor for ideal IMMA MSE plotting.  Included in Figure 6 for reference are a curve of the 3rd order variance, 2nd order variance, and the 2nd order MSE. The large deviation of    μ  2     subscript  μ  2    \mu_{2}   from    f   3  ,   o  p  t       subscript  f   3    o  p  t      f_{3,opt}   in Figure 5 has a profound effect on the ideal IMMA MSE as shown in Figure 6. The ideal IMMA MSE exceeds the FOE MSE most near     ρ  3   =  0.7       subscript  ρ  3   0.7    \rho_{3}=0.7   , about where the    μ  2     subscript  μ  2    \mu_{2}   differs most from    f   3  ,   o  p  t       subscript  f   3    o  p  t      f_{3,opt}   in Figure 5. In addition, the ideal IMMA MSE exceeds the 3rd order variance most near     ρ  3   =  0.85       subscript  ρ  3   0.85    \rho_{3}=0.85   , even though the specific purpose of interpolation in the IMM is to produce an MSE smaller than the 3rd order variance. Nevertheless, as expected, the two MSE curves do osculate near     ρ  3   =  1.4       subscript  ρ  3   1.4    \rho_{3}=1.4   , where    μ  2     subscript  μ  2    \mu_{2}   and    f   3  ,   o  p  t       subscript  f   3    o  p  t      f_{3,opt}   cross in Figure 5.  Furthermore, the MSE is exacerbated in the non-ideal IMMA by adaptivity, as shown in Figure 7 where the IMMA from noisy    μ  2     subscript  μ  2    \mu_{2}   is superimposed on the curves in Figure 6 (although there is a slight change in scale to accommodate the larger noisy IMMA MSE). Reference 74 describes this in great detail. Clearly, since Figure 6 includes the ideal    μ  2     subscript  μ  2    \mu_{2}   based on the expected value of acceleration, i.e., the actual acceleration; an estimate which includes measurement noise can only degrade the accuracy—as shown in Figure 7.  Indeed, not only is the noisy IMMA MSE larger than the 3rd order variance (by nearly a factor of two at the worst point), once the noisy IMMA MSE exceeds the 3rd order variance, it does not drop below as does the ideal IMMA. In contrast, the optimal FOE MSE (i.e.,    M  S   E   m  i  n        M  S   subscript  E    m  i  n      MSE_{min}   ) always remains less than the 3rd order variance.  This analysis compellingly suggests that adaptivity significantly degrades IMM accuracy rather than improving it. Of course, this should not come as a surprise since for     ρ  3   <  0.5       subscript  ρ  3   0.5    \rho_{3}<0.5   , the acceleration is buried in the noise; i.e.,      (   a   Δ  2    )   /   σ  η    <  1          a   superscript  normal-Δ  2     subscript  σ  η    1    (a\Delta^{2})/\sigma_{\eta}<1   (a signal-to-noise ratio likeness of less than 0 dB).  These analyses reveal the incredible and disconcerting lack of tracking literature that addresses fundamentals (e.g., optimal IMM interpolation,    μ  2     subscript  μ  2    \mu_{2}   boundary conditions, and acceleration-to-noise ratio) and comparisons with standard benchmarks (e.g.; 2nd order, 3rd order, or other optimal estimators).  Deficiencies and oversights in the Kalman filter  Comparisons of the KF with the derivation, analysis, design, and implementation of MFOE have uncovered a number of deficiencies and oversights in the KF that are overcome by the MFOE. They are reported and discussed in. 75  References  "  Category:Target tracking  Category:Signal processing  Category:Estimation theory  Category:Polynomial least squares  Category:Curve fitting  Category:Kalman filter     Bell, J. W., Simple Disambiguation Of Orthogonal Projection In Kalman’s filter Derivation, Proceedings of the International Conference on Radar Systems, Glasgow, UK. October, 2012. ↩  Bell, J. W., A Simple Kalman Filter Alternative: The Multi-Fractional Order Estimator, IET-RSN, Vol. 7, Issue 8, October 2013. ↩  Kalman, R. E., A New Approach to Linear Filtering and Prediction Problems, Journal of Basic Engineering, Vol. 82D, Mar. 1960. ↩  Sorenson, H. W., Least-squares estimation: Gauss to Kalman, IEEE Spectrum, July, 1970. ↩  Radar tracker ↩         Burkhardt, R., et.al., Titan Systems Corporation Atlantic Aerospace Division; Shipboard IRST Processing with Enhanced Discrimination Capability; Sponsor: Naval Surface Warfare Center, Dahlgren, VA; Contract #: N00178-98-C-3020; September 19, 2000 (p. 41). ↩  Blom, H. A. P., An efficient filter for abruptly changing systems, in Proceedings of the 23rd IEEE Conference on Decision and Control Las Vegas, NV, Dec. 1984, 656-658. ↩  Blom, H. A. P., and Bar-Shalom, Y., The interacting multiple model algorithm for systems with Markovian switching coefficients, IEEE Trans. Autom. Control, 1988, 33, pp. 780–783 ↩  Bar-Shalom, Y. and Li, X. R., Estimation and Tracking : Principles, Techniques, and Software Artech House Radar Library, Boston, 1993. ↩  Mazor, E., Averbuch, A., Bar-Shalom, Y., Dayan, J., Interacting Multiple Model Methods in Target Tracking: A Survey; IEEE T-AES, Jan 1998 ↩     1 ↩                      Brookner, E., Tracking and Kalman Filtering Made Easy, Wiley, New York, 1998. ↩  Kingsley, S. and Quegan, S., Understanding Radar Systems, McGraw-Hill, New York, 1992. ↩   http://ssrn.com/abstract=2579686 ↩  Lau, Tak Kit and Lin, Kai-wun, Evolutionary Tuning of Sigma-Point Kalman Filters, Robotics and Automation (ICRA), 2011 IEEE International Conference on ↩  Bernt M. et al., A Tool for Kalman Filter Tuning, http://www.netegrate.com/index_files/Research%20Library/Catalogue/Quantitative%20Analysis/Kalman%20Filter/A%20Tool%20for%20Kalman%20Filter%20Tuning(Akesson,%20Jorgensen%20and%20Poulsen).pdf ↩   Blair, W. D., Bar-Shalom, Y., Tracking Maneuvering Targets With Multiple Sensors: Does More Data Always Mean Better Estimates? IEEE T-AES Vol. 32, No.1, Jan. 1996. ↩    2 ↩  3 ↩   4 ↩     Yang, Chun, Blasch, Erik, Characteristic Errors of the IMM Algorithm under Three Maneuver Models for an Accelerating Target, Information Fusion, 2008 11th International Conference on ↩  Gomes, J., An Overview on Target Tracking Using Multiple Model Methods, Masters Thesis, https://fenix.tecnico.ulisboa.pt/downloadFile/395137804053/thesis.pdf ↩  http://isif.org/fusion/proceedings/fusion02CD/pdffiles/papers/T1D03.pdf ↩  Watson, G. A., and Blair, W. D., Interacting Acceleration Compensation Algorithm for Tracking Maneuvering Targets. IEEE T-AES. Vol. 31, No. 3 July 1995. ↩  Pitre, Ryan, A Comparison of Multiple-Model Target Tracking Algorithms: University of New Orleans Theses and Dissertation,. December, 2004. ↩         5 ↩  6 ↩   7 ↩  8 ↩     