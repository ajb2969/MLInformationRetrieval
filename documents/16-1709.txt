


Multi-fractional order estimator




Multi-fractional order estimator
The multi-fractional order estimator (MFOE)12 is a straightforward, practical, and flexible alternative to the Kalman filter (KF)34 for tracking targets.5 The MFOE is focused strictly on simple and pragmatic fundamentals along with the integrity of mathematical modeling. Like the KF, the MFOE is based on the least squares method (LSM) invented by Gauss678 and the orthogonality principle at the center of Kalman's derivation.9101112 Optimized, the MFOE yields better accuracy than the KF and subsequent algorithms such as the extended KF13 and the interacting multiple model (IMM).14151617 The MFOE is an expanded form of the LSM, which effectively includes the KF181920 and ordinary least squares (OLS)21 as subsets (special cases). OLS is revolutionized in22 for application in econometrics. The MFOE also intersects with signal processing, estimation theory, economics, finance, statistics, and the method of moments. The MFOE offers two major advances: (1) minimizing the mean squared error (MSE) with fractions of estimated coefficients (useful in target tracking)2324 and (2) describing the effect of deterministic OLS processing of statistical inputs (of value in econometrics)25
Description
Consider equally time spaced noisy measurement samples of a target trajectory described by2627


where n represents both the time samples and the index; the polynomial describing the trajectory is of degree J-1; and 
 
 
 
  is zero mean, stationary, white noise (not necessarily Gaussian) with variance 
 
 
 
 .
Estimating x(t) at time 
 
 
 
 
  with the MFOE is described by



where the hat (^) denotes an estimate, N is the number of samples in the data window, 
 
 
 
  is the time of the desired estimate, and the data weights are



The 
 
 
 
  are orthogonal polynomial coefficient estimators. 
 
 
 
 
  (a function detailed in2829) projects the estimate of the polynomial coefficient 
 
 
 
  to the desired estimation time 
 
 
 
 . The MFOE parameter 0≤
 
 
 
 ≤1 can apply a fraction of the projected coefficient estimate.
The combined terms 
 
 
 
  effectively constitute a novel set of expansion functions with coefficients 
 
 
 
 
 . The MFOE can be optimized at time 
 
 
 
  as a function of the 
 
 
 
 s for given measurement noise, target dynamics, and non-recursive sliding data window size, N. However, for all 
 
 
 
 , the MFOE reduces and is equivalent to the KF in the absence of process noise, and to the standard polynomial LSM.
As in the case of coefficients in conventional series expansions, the 
 
 
 
 s typically decrease monotonically as higher order terms are included to match complex target trajectories. For example, in30 the 
 
 
 
 
 s monotonically decreased in the MFOE from 
 
 
 
  to 
 
 
 
  , where 
 
 
 
  for m ≧ 6. The MFOE in31 consisted of five point, 5th order processing of composite real (but altered for declassification) cruise missile data. A window of only 5 data points provided excellent maneuver following; whereas, 5th order processing included fractions of higher order terms to better approximate the complex maneuvering target trajectory. The MFOE overcomes the long-ago rejection of terms higher than 3rd order because, taken at full value (i.e., 
 
 
 
 ), estimator variances increase exponentially with linear order increases. (This is elucidated below in the section "Application of the FOE".)
Fractional order estimator
As described in,3233 the MFOE can be written more efficiently as 
 
 
 
 
  where the estimator weights 
 
 
 
  of order m are components of the estimating vector 
 
 
 
 . By definition 
 
 
 
  and 
 
 
 
 . The angle brackets and comma 
Perhaps the most useful MFOE tracking estimator is the simple fractional order estimator (FOE) where 
 
 
 
  and 
 
 
 
  for all m > 3, leaving only 
 
 
 
 . This is effectively an FOE of fractional order 
 
 
 
 
 , which linear interpolates between the 2nd and 3rd order estimators described in3435) as



where the scalar fraction 
 
 
 
  is the linear interpolation factor, the vector 
 
 
 
 , and 
 
 
 
  (which comprises the components 
 
 
 
 
 ) is the vector estimator of the 3rd polynomial coefficient 
 
 
 
  (a is acceleration and Δ is the sample period). The vector 
 
 
 
  is the acceleration estimator from 
 
 
 
 .
The mean-square error (MSE) from the FOE applied to an accelerating target is3637

 
 , where for any vector 
 
 
 
 
 , 
 
 
 
 .
The first term on the right of the equal sign is the FOE target location estimator variance 
 
 
 
  composed of the 2nd order location estimator variance and part of the variance from the 3rd order acceleration estimator as determined by the interpolation factor squared 
 
 
 
 . The second term is the bias squared 
 
 
 
  from the 2nd order target location estimator as a function of acceleration in 
 
 
 
 
 .
Setting the derivative of the MSE with respect to 
 
 
 
  equal to zero and solving yields the optimal 
 
 
 
 :



where 
 
 
 
  , as defined in.38
The optimal FOE is then very simply



Substituting the optimal FOE into the MSE yields the minimum MSE:


3940
Although not obvious, the 
 
 
 
  includes the bias squared. The variance in the FOE MSE is the quadratic interpolation between the 2nd and the 3rd order location estimator variances as a function of 
 
 
 
 . Whereas, the 
 
 
 
  is the linear interpolation between the same 2nd and the 3rd order location estimator variances as a function of 
 
 
 
 
 . The bias squared accounts for the difference.
Application of the FOE
Since a target's future location is generally of more interest than where it is or has been, consider one-step prediction. Normalized with respect to measurement noise variance, the MSE for equally spaced samples reduces for the predicted position to



where N is the number of samples in the non-recursive sliding data window.41 Note that the first term on the right of the equal sign is the variance from estimating the first coefficient (position); the second term is the variance from estimating the 2nd coefficient (velocity); and the 3rd term with 
 
 
 
  is the variance from estimating the 3rd coefficient (which includes acceleration). This pattern continues for higher order terms. Furthermore, the sum of the variances from estimating the first two coefficients is 
 
 
 
 ). Adding the variance from estimating the 3rd coefficient yields 
 
 
 
 .
Estimator variances obviously increase exponentially with unit order increases. In the absence of process noise, the KF yields variances equivalent to these.4243 (A derivation of the variance from a 1st degree polynomial corresponding to 
 
 
 
 
  for the generalized case of arbitrary estimation time and sample times is given in reference.44 In addition, establishing a multi-dimensional tracking gate at the predicted position can easily be aided with the simple approximation of the error function in.45)
Kalman filter tuning
Tuning the KF consists of a trade-off between measurement noise and process noise to minimize the estimation error.4647 The KF process noise serves two roles: First, its covariance is sized to account for the maximum expected target acceleration. Second, process noise covariance establishes an effective recursive data window (analogous to the non-recursive sliding data window), described by Brookner as the Kalman filter memory.48
Contrary to process noise covariance as a single independent parameter in the KF serving two roles, the FOE has the advantage of two separate independent parameters: one for acceleration and the other for sizing the sliding data window. Therefore, as opposed to being limited to just two tuning parameters (process and measurement noises) as is the KF, the FOE includes three independent tuning parameters: measurement noise variance, the assumed maximum deterministic target acceleration (for simplicity both target acceleration and measurement noise are included in the ratio of the single parameter 
 
 
 
 ), and the number of samples in the data window.
Consider tuning a 2nd order predictor applied to the simple and practical tracking example in49 to minimize the MSE when the target acceleration is 
 
 
 
 ; the zero mean, stationary, and white measurement noise is described as 
 
 
 
 ; and 
 
 
 
  = 1 second. Thus,



Setting 
 
 
 
  in the normalized prediction MSE yields for the 2nd order predictor applied to an accelerating target,



where the first term on the right of the equal sign is the normalized 2nd order one-step prediction variance and the second term is the normalized bias squared from acceleration. This MSE is plotted as a function of N in Figure 1 along with both the variance and bias squared.  Clearly, only integer order steps are possible in a non-recursive estimator. However, for use in approximating the tuned 2nd order KF, this MSE plot is stepped in tenths of a unit to show more precisely where the minimum occurs. The minimum MSE of 4.09 occurs at N = 2.9. The tuned KF can be approximated by sizing the process noise covariance in the KF such that the effective recursive data window—i.e., the Kalman filter memory50—matches N = 2.9 in Figure 1 (i.e., 
 
 
 
  and 
 
 
 
 ), where 
 
 
 
 
 and 
 
 
 
 .51 This hints at the fallacy of using a 2nd order estimator on accelerating targets as described in.52 Comparing this with the filtered position in53 demonstrates that the minimum MSE is a function of the time 
 
 
 
  of the desired estimate.
FOE as a multiple-model estimator
The FOE can be viewed as a non-recursive multiple-model (MM) estimator composed of 2nd and 3rd order estimator models with the fraction 
 
 
 
  as the interpolation factor. Since the filtered position is generally used for comparisons in the literature, consider now the normalized MSE for the position estimate:



Note that this differs from the one-step prediction MSE in that the signs within the parentheses containing N are reversed. The higher order pattern continues here also. Normalized with respect to the measurement noise variance, the minimum position MSE reduces for equally spaced samples to



where 
 
 

in 
 
 
54  A plot of the position 
 
 
 
  as a function of N for various values of 
 
 
 
  is shown in Figure 2, where there are several points of interest: First, the 2nd and 3rd order MSEs track each other very closely and bound all the 
 
 
 
 
  (interpolated) curves. Second, the curves drop rapidly to a knee. Third, the 
 
 
 
  curves flatten out beyond the knee yielding virtually no increase in accuracy until they begin to approach the 3rd order MSE (variance).55 This suggests that choosing a window at the knee of the curve is advantageous—to be demonstrated below.  Consider again the scenario of,56 in this case as the target maneuvers. After traveling at a constant velocity, the target accelerates at 
 
 
 
  for 20 seconds and then continues again at a constant velocity. At worst case acceleration, 
 
 
 
 . The 
 
 
 
  is plotted in Figure 3 of as a function of N. Also shown are the 2nd order MSE as well as the 2nd and 3rd order MSEs (variances only since the bias is zero in each case) similar to those in Figure 2. There is a fifth curve not previously addressed: the variance portion of the optimal MSE. The variance also levels off for several increments of N like the 
 
 
 
 
 . Both the variance and 
 
 
 
  approach the 3rd order variance as 
 
 
 
 .
As the acceleration varies from zero to maximum, the MSE is automatically adjusted (no external tinkering or adaptivity) between the variance at 
 
 
 
  and maximum 
 
 
 
  at 
 
 
 
 
 . In other words, the MSE rides up and down the quadratic curve of the variance plus bias squared as a function of changes in acceleration 
 
 
 
  for any given value of N in the position estimate:



 Choosing N = 4 at the knee of the 
 
 
 
  curve in Figure 3 yields the RMSE (square root of the MSE, which is more often used for comparison in the literature) shown in Figure 4. On the other hand, choosing N = 8 yields the second curve in Figure 4. As shown in Figure 3, the optimal 8–point FOE is essentially a 3rd order non-recursive estimator which yields less than 4% RMSE improvement over the optimal 4-point FOE in the case of no acceleration. However, in the case of maximum acceleration the optimal 8-point MSE is markedly volatile and has large error spikes that can confuse a tracker, one spike exceeding the optimal 4-point MSE for worst case acceleration by more than the optimal 4-point MSE exceeds the optimal 8-point MSE in the absence of acceleration. Obviously, higher values of N produce larger error spikes.
Since trackers encounter greatest difficulties and often lose track during target maneuvers at maximum acceleration, the much smoother 
 
 
 
 transition of the optimal 4-point FOE has a major advantage over larger data windows.
IMM compared with the optimal FOE
The 4-point FOE in Figure 4 yields much smoother MSE transitions than the IMM (as well as the KF) in the parallel 1 Hz case of.57 It produces no error spikes or volatility as do the 8-point FOE and the IMM. In this example only 4 multiplies, 3 adds, and a window shift are required to implement the 4-point FOE, significantly few operations than required by the IMM or KF. Similar comparisons of several additional MMs from the literature with the optimal FOE are made in58
Of the KF based MMs, the interacting MM (IMM) is generally considered the state-of-the-art tracking model and usually the method of choice.5960 Since two model IMMs are most often used,61 consider the following two models: 2nd and 3rd order KFs. The estimated IMM state equation is the sum of the 2nd order KF times the model probability 
 
 
 
 
  plus the 3rd order KF times the model probability 
 
 
 
 :



where 
 
 
 
  represents the 2nd order KF, 
 
 
 
  represents the 3rd order KF, and k represents the time increment.6263 Since the model probabilities sum to one, i.e., 
 
 
 
 
 ;64 this is actually linear interpolation, where 
 
 
 
  is analogous to 
 
 
 
  in the FOE and 
 
 
 
  is analogous to 
 
 
 
 . Therefore, this two model IMM is analogous to the optimal FOE in that it also interpolates between 2nd and 3rd order estimators. Two model IMM interpolation is formed during each recursive cycle involving the interactively produced model probabilities.6566676869
As in the case of the FOE, this suggests a more descriptive estimate equal to the sum of the 2nd order KF plus the difference between the 3rd and 2nd order KFs times 
 
 
 
 
  :



In this formulation the difference between the 3rd and 2nd order KFs effectively augments the 2nd order KF with a fraction of the estimated target acceleration as a function of 
 
 
 
 —as does 
 
 
 
  in the FOE.
One major difference between the IMM and optimal FOE is that the IMM is not optimum. The IMM model probabilities and interpolation are based on likelihoods and ad hoc transition probabilities with no mechanism for minimizing the MSE.70 Of course, not being optimum at any time increment k, the IMM cannot achieve the optimal FOE accuracy shown in Figure 2.
Moveover, the IMM 
 
 
 
  fails to meet the boundary condition of zero to implement the 2nd order estimator in the absence of acceleration, which the FOE 
 
 
 
 
  does. This results from the fact that the likelihoods do not sum to unity71 even though the model probabilities do. This causes an IMM bias toward a non-existent acceleration and unnecessarily increases the MSE above the 2nd order variance. Another major difference between the IMM and FOE is that the IMM is adaptive whereas the FOE is not.
In order to make a reasonable comparison of the IMM with the FOE, reference72 constructs a non-recursive IMM analogy (IMMA). It includes 
 
 
 
  which does go to zero allowing the 2nd order estimator to be implemented. Since the FOE is based on the actual acceleration not a noisy estimate, the acceleration estimate for the IMMA is assumed to be the expected value of the estimate, i.e., the actual acceleration. This is described here as the ideal for the purpose of illustration. These two modifications make the IMMA compatible for comparison with the FOE.  The 
 
 
 
  based on the expected value or actual acceleration (described here as the ideal 
 
 
 
  where the k is dropped) then varies between zero and one in an S shaped curve as a function of 
 
 
 
 , as does 
 
 
 
 
 . This is shown in Figure 5, where a 4-point data window is assumed.  Two significant points of interest stand out as shown by the vertical lines. First, the largest deviation of the ideal 
 
 
 
  from 
 
 
 
  occurs near 
 
 
 
 . Second, the two curves cross near 
 
 
 
 . A comparison of the one-step predictor IMMA MSE as a function of ideal 
 
 
 
 
  with the FOE 
 
 
 
  is given in Figure 6.73 For the IMMA, the linear interpolation factor 
 
 
 
  is replaced in the normalized FOE MSE by the ideal 
 
 
 
  as the interpolation factor for ideal IMMA MSE plotting.
Included in Figure 6 for reference are a curve of the 3rd order variance, 2nd order variance, and the 2nd order MSE. The large deviation of 
 
 
 
  from 
 
 
 
 
  in Figure 5 has a profound effect on the ideal IMMA MSE as shown in Figure 6. The ideal IMMA MSE exceeds the FOE MSE most near 
 
 
 
 , about where the 
 
 
 
  differs most from 
 
 
 
  in Figure 5. In addition, the ideal IMMA MSE exceeds the 3rd order variance most near 
 
 
 
 , even though the specific purpose of interpolation in the IMM is to produce an MSE smaller than the 3rd order variance. Nevertheless, as expected, the two MSE curves do osculate near 
 
 
 
 
 , where 
 
 
 
  and 
 
 
 
  cross in Figure 5. 
Furthermore, the MSE is exacerbated in the non-ideal IMMA by adaptivity, as shown in Figure 7 where the IMMA from noisy 
 
 
 
  is superimposed on the curves in Figure 6 (although there is a slight change in scale to accommodate the larger noisy IMMA MSE). Reference74 describes this in great detail. Clearly, since Figure 6 includes the ideal 
 
 
 
  based on the expected value of acceleration, i.e., the actual acceleration; an estimate which includes measurement noise can only degrade the accuracy—as shown in Figure 7.
Indeed, not only is the noisy IMMA MSE larger than the 3rd order variance (by nearly a factor of two at the worst point), once the noisy IMMA MSE exceeds the 3rd order variance, it does not drop below as does the ideal IMMA. In contrast, the optimal FOE MSE (i.e., 
 
 
 
 
 ) always remains less than the 3rd order variance.
This analysis compellingly suggests that adaptivity significantly degrades IMM accuracy rather than improving it. Of course, this should not come as a surprise since for 
 
 
 
  , the acceleration is buried in the noise; i.e., 
 
 
 
  (a signal-to-noise ratio likeness of less than 0 dB).
These analyses reveal the incredible and disconcerting lack of tracking literature that addresses fundamentals (e.g., optimal IMM interpolation, 
 
 
 
  boundary conditions, and acceleration-to-noise ratio) and comparisons with standard benchmarks (e.g.; 2nd order, 3rd order, or other optimal estimators).
Deficiencies and oversights in the Kalman filter
Comparisons of the KF with the derivation, analysis, design, and implementation of MFOE have uncovered a number of deficiencies and oversights in the KF that are overcome by the MFOE. They are reported and discussed in.75
References
"
Category:Target tracking Category:Signal processing Category:Estimation theory Category:Polynomial least squares Category:Curve fitting Category:Kalman filter



Bell, J. W., Simple Disambiguation Of Orthogonal Projection In Kalman’s filter Derivation, Proceedings of the International Conference on Radar Systems, Glasgow, UK. October, 2012.
Bell, J. W., A Simple Kalman Filter Alternative: The Multi-Fractional Order Estimator, IET-RSN, Vol. 7, Issue 8, October 2013.
Kalman, R. E., A New Approach to Linear Filtering and Prediction Problems, Journal of Basic Engineering, Vol. 82D, Mar. 1960.
Sorenson, H. W., Least-squares estimation: Gauss to Kalman, IEEE Spectrum, July, 1970.
Radar tracker







Burkhardt, R., et.al., Titan Systems Corporation Atlantic Aerospace Division; Shipboard IRST Processing with Enhanced Discrimination Capability; Sponsor: Naval Surface Warfare Center, Dahlgren, VA; Contract #: N00178-98-C-3020; September 19, 2000 (p. 41).
Blom, H. A. P., An efficient filter for abruptly changing systems, in Proceedings of the 23rd IEEE Conference on Decision and Control Las Vegas, NV, Dec. 1984, 656-658.
Blom, H. A. P., and Bar-Shalom, Y., The interacting multiple model algorithm for systems with Markovian switching coefficients, IEEE Trans. Autom. Control, 1988, 33, pp. 780–783
Bar-Shalom, Y. and Li, X. R., Estimation and Tracking : Principles, Techniques, and Software Artech House Radar Library, Boston, 1993.
Mazor, E., Averbuch, A., Bar-Shalom, Y., Dayan, J., Interacting Multiple Model Methods in Target Tracking: A Survey; IEEE T-AES, Jan 1998



1




















Brookner, E., Tracking and Kalman Filtering Made Easy, Wiley, New York, 1998.
Kingsley, S. and Quegan, S., Understanding Radar Systems, McGraw-Hill, New York, 1992.

http://ssrn.com/abstract=2579686
Lau, Tak Kit and Lin, Kai-wun, Evolutionary Tuning of Sigma-Point Kalman Filters, Robotics and Automation (ICRA), 2011 IEEE International Conference on
Bernt M. et al., A Tool for Kalman Filter Tuning, http://www.netegrate.com/index_files/Research%20Library/Catalogue/Quantitative%20Analysis/Kalman%20Filter/A%20Tool%20for%20Kalman%20Filter%20Tuning(Akesson,%20Jorgensen%20and%20Poulsen).pdf

Blair, W. D., Bar-Shalom, Y., Tracking Maneuvering Targets With Multiple Sensors: Does More Data Always Mean Better Estimates? IEEE T-AES Vol. 32, No.1, Jan. 1996.


2
3

4



Yang, Chun, Blasch, Erik, Characteristic Errors of the IMM Algorithm under Three Maneuver Models for an Accelerating Target, Information Fusion, 2008 11th International Conference on
Gomes, J., An Overview on Target Tracking Using Multiple Model Methods, Masters Thesis, https://fenix.tecnico.ulisboa.pt/downloadFile/395137804053/thesis.pdf
http://isif.org/fusion/proceedings/fusion02CD/pdffiles/papers/T1D03.pdf
Watson, G. A., and Blair, W. D., Interacting Acceleration Compensation Algorithm for Tracking Maneuvering Targets. IEEE T-AES. Vol. 31, No. 3 July 1995.
Pitre, Ryan, A Comparison of Multiple-Model Target Tracking Algorithms: University of New Orleans Theses and Dissertation,. December, 2004.







5
6

7
8




