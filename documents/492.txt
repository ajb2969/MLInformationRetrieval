   Dot product      Dot product   In mathematics , the dot product , or scalar product (or sometimes inner product in the context of Euclidean space ), is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors ) and returns a single number. This operation can be defined either algebraically or geometrically. Algebraically, it is the sum of the products of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. The name "dot product" is derived from the centered dot " · " that is often used to designate this operation; the alternative name "scalar product" emphasizes the scalar (rather than vectorial ) nature of the result.  In three-dimensional space, the dot product contrasts with the cross product of two vectors, which produces a pseudovector as the result. The dot product is directly related to the cosine of the angle between two vectors in Euclidean space of any number of dimensions.  Definition  The dot product is often defined in one of two ways: algebraically or geometrically. The geometric definition is based on the notions of angle and distance (magnitude of vectors). The equivalence of these two definitions relies on having a Cartesian coordinate system for Euclidean space.  In modern presentations of Euclidean geometry , the points of space are defined in terms of their Cartesian coordinates, and Euclidean space itself is commonly identified with the real coordinate space  R n . In such a presentation, the notions of length and angles are not primitive. They are defined by means of the dot product: the length of a vector is defined as the square root of the dot product of the vector by itself, and the cosine of the (non oriented) angle of two vectors of length one is defined as their dot product. So the equivalence of the two definitions of the dot product is a part of the equivalence of the classical and the modern formulations of Euclidean geometry.  Algebraic definition  The dot product of two vectors and is defined as: 1       𝐀  ⋅  𝐁   =    ∑   i  =  1   n     A  i    B  i     =     A  1    B  1    +    A  2    B  2    +  ⋯  +    A  n    B  n            normal-⋅  𝐀  𝐁     superscript   subscript     i  1    n      subscript  A  i    subscript  B  i               subscript  A  1    subscript  B  1       subscript  A  2    subscript  B  2    normal-⋯     subscript  A  n    subscript  B  n        \mathbf{A}\cdot\mathbf{B}=\sum_{i=1}^{n}A_{i}B_{i}=A_{1}B_{1}+A_{2}B_{2}+%
 \cdots+A_{n}B_{n}     where Σ denotes summation notation and n is the dimension of the vector space. For instance, in three-dimensional space , the dot product of vectors  and  is:       [  1  ,  3  ,   -  5   ]   ⋅   [  4  ,   -  2   ,   -  1   ]      normal-⋅   1  3    5     4    2     1      \displaystyle\ [1,3,-5]\cdot[4,-2,-1]     Geometric definition  In Euclidean space , a Euclidean vector is a geometrical object that possesses both a magnitude and a direction. A vector can be pictured as an arrow. Its magnitude is its length, and its direction is the direction that the arrow points. The magnitude of a vector A is denoted by    ∥  𝐀  ∥     norm  𝐀    \left\|\mathbf{A}\right\|   . The dot product of two Euclidean vectors A and B is defined by 2        𝐀  ⋅  𝐁   =    ∥  𝐀  ∥    ∥  𝐁  ∥    cos  θ     ,       normal-⋅  𝐀  𝐁      norm  𝐀    norm  𝐁     θ      \mathbf{A}\cdot\mathbf{B}=\left\|\mathbf{A}\right\|\,\left\|\mathbf{B}\right\|%
 \cos\theta,   where θ is the angle between A and B .  In particular, if A and B are orthogonal , then the angle between them is 90° and       𝐀  ⋅  𝐁   =  0.       normal-⋅  𝐀  𝐁   0.    \mathbf{A}\cdot\mathbf{B}=0.   At the other extreme, if they are codirectional, then the angle between them is 0° and       𝐀  ⋅  𝐁   =    ∥  𝐀  ∥    ∥  𝐁  ∥         normal-⋅  𝐀  𝐁      norm  𝐀    norm  𝐁      \mathbf{A}\cdot\mathbf{B}=\left\|\mathbf{A}\right\|\,\left\|\mathbf{B}\right\|   This implies that the dot product of a vector A by itself is        𝐀  ⋅  𝐀   =    ∥  𝐀  ∥   2    ,       normal-⋅  𝐀  𝐀    superscript   norm  𝐀   2     \mathbf{A}\cdot\mathbf{A}=\left\|\mathbf{A}\right\|^{2},   which gives        ∥  𝐀  ∥   =    𝐀  ⋅  𝐀     ,       norm  𝐀      normal-⋅  𝐀  𝐀      \left\|\mathbf{A}\right\|=\sqrt{\mathbf{A}\cdot\mathbf{A}},   the formula for the Euclidean length of the vector.  Scalar projection and first properties  The scalar projection (or scalar component) of a Euclidean vector A in the direction of a Euclidean vector B is given by        A  B   =    ∥  𝐀  ∥    cos  θ     ,       subscript  A  B      norm  𝐀     θ      A_{B}=\left\|\mathbf{A}\right\|\cos\theta,   where θ is the angle between A and B .  In terms of the geometric definition of the dot product, this can be rewritten        A  B   =   𝐀  ⋅   𝐁  ^     ,       subscript  A  B    normal-⋅  𝐀   normal-^  𝐁      A_{B}=\mathbf{A}\cdot\widehat{\mathbf{B}},   where     𝐁  ^   =   𝐁  /   ∥  𝐁  ∥         normal-^  𝐁     𝐁   norm  𝐁      \widehat{\mathbf{B}}=\mathbf{B}/\left\|\mathbf{B}\right\|   is the unit vector in the direction of B .  The dot product is thus characterized geometrically by 3        𝐀  ⋅  𝐁   =    A  B    ∥  𝐁  ∥    =    B  A    ∥  𝐀  ∥     .         normal-⋅  𝐀  𝐁      subscript  A  B    norm  𝐁            subscript  B  A    norm  𝐀       \mathbf{A}\cdot\mathbf{B}=A_{B}\left\|\mathbf{B}\right\|=B_{A}\left\|\mathbf{A%
 }\right\|.   The dot product, defined in this manner, is homogeneous under scaling in each variable, meaning that for any scalar α ,         (   α  𝐀   )   ⋅  𝐁   =   α   (   𝐀  ⋅  𝐁   )    =   𝐀  ⋅   (   α  𝐁   )     .         normal-⋅    α  𝐀   𝐁     α   normal-⋅  𝐀  𝐁          normal-⋅  𝐀    α  𝐁       (\alpha\mathbf{A})\cdot\mathbf{B}=\alpha(\mathbf{A}\cdot\mathbf{B})=\mathbf{A}%
 \cdot(\alpha\mathbf{B}).   It also satisfies a distributive law , meaning that        𝐀  ⋅   (   𝐁  +  𝐂   )    =    𝐀  ⋅  𝐁   +   𝐀  ⋅  𝐂     .       normal-⋅  𝐀    𝐁  𝐂       normal-⋅  𝐀  𝐁    normal-⋅  𝐀  𝐂      \mathbf{A}\cdot(\mathbf{B}+\mathbf{C})=\mathbf{A}\cdot\mathbf{B}+\mathbf{A}%
 \cdot\mathbf{C}.     These properties may be summarized by saying that the dot product is a bilinear form . Moreover, this bilinear form is positive definite , which means that    𝐀  ⋅  𝐀     normal-⋅  𝐀  𝐀    \mathbf{A}\cdot\mathbf{A}   is never negative and is zero if and only if    𝐀  =  0.      𝐀  0.    \mathbf{A}=\mathbf{0}.     Equivalence of the definitions  If e 1 , ..., e n are the standard basis vectors in R n , then we may write     𝐀   𝐀   \displaystyle\mathbf{A}   The vectors e i are an orthonormal basis , which means that they have unit length and are at right angles to each other. Hence since these vectors have unit length        𝐞  i   ⋅   𝐞  i    =  1       normal-⋅   subscript  𝐞  i    subscript  𝐞  i    1    \mathbf{e}_{i}\cdot\mathbf{e}_{i}=1   and since they form right angles with each other, if ,        𝐞  i   ⋅   𝐞  j    =  0.       normal-⋅   subscript  𝐞  i    subscript  𝐞  j    0.    \mathbf{e}_{i}\cdot\mathbf{e}_{j}=0.     Also, by the geometric definition, for any vector e i and a vector A , we note        𝐀  ⋅   𝐞  i    =    ∥  𝐀  ∥    ∥   𝐞  i   ∥    cos  θ    =    ∥  𝐀  ∥    cos  θ    =   A  i    ,         normal-⋅  𝐀   subscript  𝐞  i       norm  𝐀    norm   subscript  𝐞  i      θ            norm  𝐀     θ          subscript  A  i      \mathbf{A}\cdot\mathbf{e}_{i}=\left\|\mathbf{A}\right\|\,\left\|\mathbf{e}_{i}%
 \right\|\cos\theta=\left\|\mathbf{A}\right\|\cos\theta=A_{i},   where A i is the component of vector A in the direction of e i .  Now applying the distributivity of the geometric version of the dot product gives        𝐀  ⋅  𝐁   =   𝐀  ⋅    ∑  i     B  i    𝐞  i      =    ∑  i     B  i    (   𝐀  ⋅   𝐞  i    )     =    ∑  i     B  i    A  i      ,         normal-⋅  𝐀  𝐁    normal-⋅  𝐀    subscript   i      subscript  B  i    subscript  𝐞  i             subscript   i      subscript  B  i    normal-⋅  𝐀   subscript  𝐞  i             subscript   i      subscript  B  i    subscript  A  i        \mathbf{A}\cdot\mathbf{B}=\mathbf{A}\cdot\sum_{i}B_{i}\mathbf{e}_{i}=\sum_{i}B%
 _{i}(\mathbf{A}\cdot\mathbf{e}_{i})=\sum_{i}B_{i}A_{i},   which is precisely the algebraic definition of the dot product. So the (geometric) dot product equals the (algebraic) dot product.  Properties  The dot product fulfills the following properties if a , b , and c are real vectors and r is a scalar . 4 5   Commutative :        𝐚  ⋅  𝐛   =   𝐛  ⋅  𝐚    ,       normal-⋅  𝐚  𝐛    normal-⋅  𝐛  𝐚     \mathbf{a}\cdot\mathbf{b}=\mathbf{b}\cdot\mathbf{a},      which follows from the definition ( θ is the angle between a and b ):        𝐚  ⋅  𝐛   =    ∥  𝐚  ∥    ∥  𝐛  ∥    cos  θ    =    ∥  𝐛  ∥    ∥  𝐚  ∥    cos  θ    =   𝐛  ⋅  𝐚    .         normal-⋅  𝐚  𝐛      norm  𝐚    norm  𝐛     θ            norm  𝐛    norm  𝐚     θ          normal-⋅  𝐛  𝐚      \mathbf{a}\cdot\mathbf{b}=\left\|\mathbf{a}\right\|\left\|\mathbf{b}\right\|%
 \cos\theta=\left\|\mathbf{b}\right\|\left\|\mathbf{a}\right\|\cos\theta=%
 \mathbf{b}\cdot\mathbf{a}.      Distributive over vector addition:        𝐚  ⋅   (   𝐛  +  𝐜   )    =    𝐚  ⋅  𝐛   +   𝐚  ⋅  𝐜     .       normal-⋅  𝐚    𝐛  𝐜       normal-⋅  𝐚  𝐛    normal-⋅  𝐚  𝐜      \mathbf{a}\cdot(\mathbf{b}+\mathbf{c})=\mathbf{a}\cdot\mathbf{b}+\mathbf{a}%
 \cdot\mathbf{c}.     Bilinear :       𝐚  ⋅   (    r  𝐛   +  𝐜   )    =    r   (   𝐚  ⋅  𝐛   )    +   (   𝐚  ⋅  𝐜   )     .       normal-⋅  𝐚      r  𝐛   𝐜        r   normal-⋅  𝐚  𝐛     normal-⋅  𝐚  𝐜      \mathbf{a}\cdot(r\mathbf{b}+\mathbf{c})=r(\mathbf{a}\cdot\mathbf{b})+(\mathbf{%
 a}\cdot\mathbf{c}).     Scalar multiplication :         (    c  1   𝐚   )   ⋅   (    c  2   𝐛   )    =    c  1    c  2    (   𝐚  ⋅  𝐛   )     .       normal-⋅     subscript  c  1   𝐚      subscript  c  2   𝐛       subscript  c  1    subscript  c  2    normal-⋅  𝐚  𝐛      (c_{1}\mathbf{a})\cdot(c_{2}\mathbf{b})=c_{1}c_{2}(\mathbf{a}\cdot\mathbf{b}).     Orthogonal :   Two non-zero vectors a and b are orthogonal  if and only if .   No cancellation :   Unlike multiplication of ordinary numbers, where if , then b always equals c unless a is zero, the dot product does not obey the cancellation law :  If  and , then we can write:  by the distributive law ; the result above says this just means that a is perpendicular to , which still allows , and therefore .   Product Rule : If a and b are functions , then the derivative ( denoted by a prime ′) of  is .   Application to the cosine law  (Figure)  Triangle with vector edges a and b , separated by angle θ .   Given two vectors a and b separated by angle θ (see image right), they form a triangle with a third side . The dot product of this with itself is:      𝐜  ⋅  𝐜     normal-⋅  𝐜  𝐜    \displaystyle\mathbf{c}\cdot\mathbf{c}     which is the law of cosines .  Triple product expansion  This is an identity (also known as Lagrange's formula ) involving the dot- and cross-products . It is written as: 6 7        𝐚  ×   (   𝐛  ×  𝐜   )    =    𝐛   (   𝐚  ⋅  𝐜   )    -   𝐜   (   𝐚  ⋅  𝐛   )      ,        𝐚    𝐛  𝐜        𝐛   normal-⋅  𝐚  𝐜      𝐜   normal-⋅  𝐚  𝐛       \mathbf{a}\times(\mathbf{b}\times\mathbf{c})=\mathbf{b}(\mathbf{a}\cdot\mathbf%
 {c})-\mathbf{c}(\mathbf{a}\cdot\mathbf{b}),     which may be remembered as "BAC minus CAB", keeping in mind which vectors are dotted together. This formula finds application in simplifying vector calculations in physics .  Physics  In physics , vector magnitude is a scalar in the physical sense, i.e. a physical quantity independent of the coordinate system, expressed as the product of a numerical value and a physical unit , not just a number. The dot product is also a scalar in this sense, given by the formula, independent of the coordinate system. Examples include: 8 9   Mechanical work is the dot product of force and displacement vectors.  Magnetic flux is the dot product of the magnetic field and the vector area .   Generalizations  Complex vectors  For vectors with complex entries, using the given definition of the dot product would lead to quite different properties. For instance the dot product of a vector with itself would be an arbitrary complex number, and could be zero without the vector being the zero vector (such vectors are called isotropic ); this in turn would have consequences for notions like length and angle. Properties such as the positive-definite norm can be salvaged at the cost of giving up the symmetric and bilinear properties of the scalar product, through the alternative definition 10        𝐚  ⋅  𝐛   =   ∑    a  i     b  i   ¯      ,       normal-⋅  𝐚  𝐛        subscript  a  i    normal-¯   subscript  b  i        \mathbf{a}\cdot\mathbf{b}=\sum{a_{i}\overline{b_{i}}},   where b i is the complex conjugate of b i . Then the scalar product of any vector with itself is a non-negative real number, and it is nonzero except for the zero vector. However this scalar product is thus sesquilinear rather than bilinear: it is conjugate linear and not linear in b , and the scalar product is not symmetric, since        𝐚  ⋅  𝐛   =    𝐛  ⋅  𝐚   ¯    .       normal-⋅  𝐚  𝐛    normal-¯   normal-⋅  𝐛  𝐚      \mathbf{a}\cdot\mathbf{b}=\overline{\mathbf{b}\cdot\mathbf{a}}.   The angle between two complex vectors is then given by        cos  θ   =    Re   (   𝐚  ⋅  𝐛   )      ∥  𝐚  ∥    ∥  𝐛  ∥      .        θ      Re   normal-⋅  𝐚  𝐛       norm  𝐚    norm  𝐛       \cos\theta=\frac{\operatorname{Re}(\mathbf{a}\cdot\mathbf{b})}{\left\|\mathbf{%
 a}\right\|\,\left\|\mathbf{b}\right\|}.     This type of scalar product is nevertheless useful, and leads to the notions of Hermitian form and of general inner product spaces .  Inner product  The inner product generalizes the dot product to abstract vector spaces over a field of scalars , being either the field of real numbers     \R    \R   \R   or the field of complex numbers     \C    \C   \C   . It is usually denoted by    ⟨   𝐚   ,  𝐛  ⟩     𝐚  𝐛    \left\langle\mathbf{a}\,,\mathbf{b}\right\rangle   .  The inner product of two vectors over the field of complex numbers is, in general, a complex number, and is sesquilinear instead of bilinear. An inner product space is a normed vector space , and the inner product of a vector with itself is real and positive-definite.  Functions  The dot product is defined for vectors that have a finite number of entries . Thus these vectors can be regarded as discrete functions : a length-   n   n   n   vector   u   u   u   is, then, a function with domain }, and is a notation for the image of   i   i   i   by the function/vector   u   u   u   .  This notion can be generalized to continuous functions : just as the inner product on vectors uses a sum over corresponding components, the inner product on functions is defined as an integral over some interval     a  ≤  x  ≤  b      a  normal-≤  x  normal-≤  b    a≤x≤b   (also denoted     a  a   ,  b       a  a   b    aa,b   ): 11       ⟨  u  ,  v  ⟩   =    ∫  a  b    u   (  x  )   v   (  x  )   d  x         u  v     superscript   subscript   a   b     u  x  v  x  d  x      \left\langle u,v\right\rangle=\int_{a}^{b}u(x)v(x)dx     Generalized further to complex functions     ψ   (  x  )       ψ  x    ψ(x)   and    χ   (  x  )       χ  x    χ(x)   , by analogy with the complex inner product above, gives 12        ⟨  ψ  ,  χ  ⟩   =    ∫  a  b    ψ   (  x  )     χ   (  x  )    ¯   d  x     .       ψ  χ     superscript   subscript   a   b     ψ  x   normal-¯    χ  x    d  x      \left\langle\psi,\chi\right\rangle=\int_{a}^{b}\psi(x)\overline{\chi(x)}dx.     Weight function  Inner products can have a weight function , i.e. a function which weight each term of the inner product with a value.  Dyadics and matrices  Matrices have the Frobenius inner product , which is analogous to the vector inner product. It is defined as the sum of the products of the corresponding components of two matrices A and B having the same size:       𝔸  :   𝔹  =    ∑  i     ∑  j     A   i  j      B   i  j    ¯      =   tr   (    𝐁  H   𝐀   )    =   tr   (   𝐀𝐁  H   )      .     normal-:  𝔸      𝔹    subscript   i     subscript   j      subscript  A    i  j     normal-¯   subscript  B    i  j               tr     superscript  𝐁  normal-H   𝐀           tr   superscript  𝐀𝐁  normal-H        \mathbb{A}:\mathbb{B}=\sum_{i}\sum_{j}A_{ij}\overline{B_{ij}}=\mathrm{tr}(%
 \mathbf{B}^{\mathrm{H}}\mathbf{A})=\mathrm{tr}(\mathbf{A}\mathbf{B}^{\mathrm{H%
 }}).          𝔸  :   𝔹  =    ∑  i     ∑  j     A   i  j     B   i  j       =   tr   (    𝐁  T   𝐀   )    =   tr   (   𝐀𝐁  T   )    =   tr   (    𝐀  T   𝐁   )    =   tr   (   𝐁𝐀  T   )      .     normal-:  𝔸      𝔹    subscript   i     subscript   j      subscript  A    i  j     subscript  B    i  j              tr     superscript  𝐁  normal-T   𝐀           tr   superscript  𝐀𝐁  normal-T           tr     superscript  𝐀  normal-T   𝐁           tr   superscript  𝐁𝐀  normal-T        \mathbb{A}:\mathbb{B}=\sum_{i}\sum_{j}A_{ij}B_{ij}=\mathrm{tr}(\mathbf{B}^{%
 \mathrm{T}}\mathbf{A})=\mathrm{tr}(\mathbf{A}\mathbf{B}^{\mathrm{T}})=\mathrm{%
 tr}(\mathbf{A}^{\mathrm{T}}\mathbf{B})=\mathrm{tr}(\mathbf{B}\mathbf{A}^{%
 \mathrm{T}}).   (For real matrices)  Dyadics have a dot product and "double" dot product defined on them, see Dyadics (Product of dyadic and dyadic) for their definitions.  Tensors  The inner product between a tensor of order n and a tensor of order m is a tensor of order , see tensor contraction for details.  See also   Cauchy–Schwarz inequality  Cross product  Matrix multiplication   References  External links     Explanation of dot product including with complex vectors  "Dot Product" by Bruce Torrence, Wolfram Demonstrations Project , 2007.   "  Category:Articles containing proofs  Category:Bilinear forms  Category:Linear algebra  Category:Vectors  Category:Analytic geometry     ↩  ↩  . ↩      ↩  ↩        