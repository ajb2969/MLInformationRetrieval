<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1723">Attractor network</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Attractor network</h1>
<hr>An '
<p><em>attractor network</em>' is a type of recurrent <a href="dynamical_system" title="wikilink">dynamical</a> <a href="network_science" title="wikilink">network</a>, that evolves toward a stable pattern over time. Nodes in the attractor network converge toward a pattern that may either be fixed-point (a single state), cyclic (with regularly recurring states), <a href="chaos_theory" title="wikilink">chaotic</a> (locally but not globally unstable) or random (<a class="uri" href="stochastic" title="wikilink">stochastic</a>).<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Attractor networks have largely been used in <a href="computational_neuroscience" title="wikilink">computational neuroscience</a> to model neuronal processes such as associative memory<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> and motor behavior, as well as in <a href="bio-inspired_computing" title="wikilink">biologically inspired</a> methods of machine learning. An attractor network contains a set of <em>n</em> nodes, which can be represented as vectors in a <em>d</em>-dimensional space where <em>n</em>&gt;<em>d</em>. Over time, the network state tends toward one of a set of predefined states on a <em>d</em>-manifold; these are the <a href="attractor" title="wikilink">attractors</a>.</p>
<h2 id="overview">Overview</h2>

<p>In attractor networks, an <em>attractor</em> (or <em>attracting set</em>) is a closed subset of states <em>A</em> toward which the system of nodes evolves. A stationary attractor is a state or sets of states where the global dynamics of the network stabilize. Cyclic attractors evolve the network toward a set of states in a <a href="limit_cycle" title="wikilink">limit cycle</a>, which is repeatedly traversed. Chaotic attractors are non-repeating bounded attractors that are continuously traversed.</p>

<p>The network state space is the set of all possible node states. The attractor space is the set of nodes on the attractor. Attractor networks are initialized based on the input pattern. The dimensionality of the input pattern may differ from the dimensionality of the network nodes. The <em>trajectory</em> of the network consists of the set of states along the evolution path as the network converges toward the attractor state. The <em>basin of attraction</em> is the set of states that results in movement towards a certain attractor.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="types">Types</h2>

<p>Various types of attractors may be used to model different types of network dynamics. While fixed-point attractor networks are the most common (originating from <a href="Hopfield_networks" title="wikilink">Hopfield networks</a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a>), other types of networks are also examined.</p>
<h3 id="fixed-point-attractors">Fixed point attractors</h3>

<p>The fixed point attractor naturally follows from the <a href="Hopfield_network" title="wikilink">Hopfield network</a>. Conventionally, fixed points in this model represent encoded memories. These models have been used to explain associative memory, classification, and pattern completion. Hopfield nets contain an underlying <a href="Lyapunov_function" title="wikilink">energy function</a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> that allow the network to asymptotically approach a stationary state. One class of point attractor network is initialized with an input, after which the input is removed and the network moves toward a stable state. Another class of attractor network features predefined weights that are probed by different types of input. If this stable state is different during and after the input, it serves as a model of associative memory. However, if the states during and after input do not differ, the network can be used for pattern completion.</p>
<h3 id="other-stationary-attractors">Other stationary attractors</h3>

<p>Line attractors and plane attractors are used in the study of oculomotor control. These line attractors, or <em>neural integrators</em>, describe eye position in response to stimuli. Ring attractors have been used to model rodent head direction.</p>
<h3 id="cyclic-attractors">Cyclic attractors</h3>

<p>Cyclic attractors are instrumental in modelling <a href="central_pattern_generator" title="wikilink">central pattern generators</a>, neurons that govern oscillatory activity in animals such as chewing, walking, and breathing.</p>
<h3 id="chaotic-attractors">Chaotic attractors</h3>

<p>Chaotic attractors (also called <em>strange attractors</em>) have been hypothesized to reflect patterns in odor recognition. While chaotic attractors have the benefit of more quickly converging upon limit cycles, there is yet no experimental evidence to support this theory.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h2 id="implementations">Implementations</h2>

<p>Attractor networks have mainly been implemented as memory models using fixed-point attractors. However, they have been largely impractical for computational purposes because of difficulties in designing the attractor landscape and network wiring, resulting in spurious attractors and poorly conditioned basins of attraction. Furthermore, training on attractor networks is generally computationally expensive, compared to other methods such as <a href="k-nearest_neighbor" title="wikilink">k-nearest neighbor</a> classifiers.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h3 id="hopfield-networks">Hopfield Networks</h3>

<p>Hopfield attractor networks are an early implementation of attractor networks with associative memory. These recurrent networks are initialized by the input, and tend toward a fixed-point attractor. The update function in discrete time is 

<math display="inline" id="Attractor_network:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>x</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>t</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>W</mi>
      <mi>x</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>t</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>x</ci>
     <apply>
      <plus></plus>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>f</ci>
     <apply>
      <times></times>
      <ci>W</ci>
      <ci>x</ci>
      <ci>t</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x(t+1)=f(Wx(t))
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Attractor_network:1">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 is a vector of nodes in the network and 

<math display="inline" id="Attractor_network:2">
 <semantics>
  <mi>W</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>W</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W
  </annotation>
 </semantics>
</math>

 is a symmetric matrix describing their connectivity. The continuous time update is 

<math display="inline" id="Attractor_network:3">
 <semantics>
  <mrow>
   <mfrac>
    <mrow>
     <mi>d</mi>
     <mi>x</mi>
    </mrow>
    <mrow>
     <mi>d</mi>
     <mi>t</mi>
    </mrow>
   </mfrac>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mo>-</mo>
     <mrow>
      <mi>λ</mi>
      <mi>x</mi>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>W</mi>
       <mi>x</mi>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <ci>d</ci>
      <ci>x</ci>
     </apply>
     <apply>
      <times></times>
      <ci>d</ci>
      <ci>t</ci>
     </apply>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>λ</ci>
       <ci>x</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>f</ci>
      <apply>
       <times></times>
       <ci>W</ci>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{dx}{dt}=-\lambda x+f(Wx)
  </annotation>
 </semantics>
</math>

.</p>

<p><em><a href="Bidirectional_associative_memory" title="wikilink">Bidirectional networks</a></em> are similar to Hopfield networks, with the special case that the matrix 

<math display="inline" id="Attractor_network:4">
 <semantics>
  <mi>W</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>W</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W
  </annotation>
 </semantics>
</math>

 is a <a href="block_matrix" title="wikilink">block matrix</a>.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>
<h3 id="localist-attractor-networks">Localist Attractor Networks</h3>

<p>Zemel and Mozer (2001)<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> proposed a method to reduce the number of spurious attractors that arise from the encoding of multiple attractors by each connection in the network. Localist attractor networks encode knowledge locally by implementing an <a class="uri" href="expectation-maximization" title="wikilink">expectation-maximization</a> algorithm on a <a href="mixture_model" title="wikilink">mixture-of-gaussians</a> representing the attractors, to minimize the free energy in the network and converge only the most relevant attractor. This results in the following update equations:</p>
<ol>
<li>Determine the activity of attractors

<math display="block" id="Attractor_network:5">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>q</mi>
     <mi>i</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>t</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <msub>
      <mi>π</mi>
      <mi>i</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>y</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>,</mo>
      <msub>
       <mi>w</mi>
       <mi>i</mi>
      </msub>
      <mo>,</mo>
      <mrow>
       <mi>σ</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <msub>
      <mo largeop="true" symmetric="true">∑</mo>
      <mi>j</mi>
     </msub>
     <mrow>
      <msub>
       <mi>π</mi>
       <mi>j</mi>
      </msub>
      <mi>g</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>y</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>t</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>,</mo>
       <msub>
        <mi>w</mi>
        <mi>j</mi>
       </msub>
       <mo>,</mo>
       <mrow>
        <mi>σ</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>t</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>q</ci>
      <ci>i</ci>
     </apply>
     <ci>t</ci>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>π</ci>
       <ci>i</ci>
      </apply>
      <vector>
       <apply>
        <times></times>
        <ci>y</ci>
        <ci>t</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <times></times>
        <ci>σ</ci>
        <ci>t</ci>
       </apply>
      </vector>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <ci>j</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>π</ci>
        <ci>j</ci>
       </apply>
       <ci>g</ci>
       <vector>
        <apply>
         <times></times>
         <ci>y</ci>
         <ci>t</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>w</ci>
         <ci>j</ci>
        </apply>
        <apply>
         <times></times>
         <ci>σ</ci>
         <ci>t</ci>
        </apply>
       </vector>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q_{i}(t)=\frac{\pi_{i}(y(t),w_{i},\sigma(t))}{\sum_{j}\pi_{j}g(y(t),w_{j},%
\sigma(t))}
  </annotation>
 </semantics>
</math>

</li>
<li>Determine the next state of the network

<math display="block" id="Attractor_network:6">
 <semantics>
  <mrow>
   <mrow>
    <mi>y</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>t</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi>α</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>t</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>ξ</mi>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mn>1</mn>
       <mo>-</mo>
       <mrow>
        <mi>α</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>t</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mrow>
      <munder>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mi>i</mi>
      </munder>
      <mrow>
       <msub>
        <mi>q</mi>
        <mi>i</mi>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
       <msub>
        <mi>w</mi>
        <mi>i</mi>
       </msub>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>y</ci>
     <apply>
      <plus></plus>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <ci>α</ci>
      <ci>t</ci>
      <ci>ξ</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <apply>
        <times></times>
        <ci>α</ci>
        <ci>t</ci>
       </apply>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <ci>i</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>q</ci>
         <ci>i</ci>
        </apply>
        <ci>t</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>w</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y(t+1)=\alpha(t)\xi+(1-\alpha(t))\sum_{i}q_{i}(t)w_{i}\,\!
  </annotation>
 </semantics>
</math>

</li>
<li>Determine the attractor width through network

<math display="block" id="Attractor_network:7">
 <semantics>
  <mrow>
   <mrow>
    <msubsup>
     <mi>σ</mi>
     <mi>y</mi>
     <mn>2</mn>
    </msubsup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>t</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mi>n</mi>
    </mfrac>
    <mrow>
     <munder>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mi>i</mi>
     </munder>
     <mrow>
      <msub>
       <mi>q</mi>
       <mi>i</mi>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>t</mi>
       <mo stretchy="false">)</mo>
      </mrow>
      <msup>
       <mrow>
        <mo stretchy="false">|</mo>
        <mrow>
         <mrow>
          <mi>y</mi>
          <mrow>
           <mo stretchy="false">(</mo>
           <mi>t</mi>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
         <mo>-</mo>
         <msub>
          <mi>w</mi>
          <mi>i</mi>
         </msub>
        </mrow>
        <mo stretchy="false">|</mo>
       </mrow>
       <mn>2</mn>
      </msup>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>σ</ci>
       <cn type="integer">2</cn>
      </apply>
      <ci>y</ci>
     </apply>
     <ci>t</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>n</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <ci>i</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>q</ci>
        <ci>i</ci>
       </apply>
       <ci>t</ci>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <abs></abs>
         <apply>
          <minus></minus>
          <apply>
           <times></times>
           <ci>y</ci>
           <ci>t</ci>
          </apply>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>w</ci>
           <ci>i</ci>
          </apply>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma^{2}_{y}(t)=\frac{1}{n}\sum_{i}q_{i}(t)|y(t)-w_{i}|^{2}
  </annotation>
 </semantics>
</math>

</li>
</ol>

<p>(

<math display="inline" id="Attractor_network:8">
 <semantics>
  <msub>
   <mi>π</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>π</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi_{i}
  </annotation>
 </semantics>
</math>

 denotes basin strength, 

<math display="inline" id="Attractor_network:9">
 <semantics>
  <msub>
   <mi>w</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}
  </annotation>
 </semantics>
</math>

 denotes the center of the basin. 

<math display="inline" id="Attractor_network:10">
 <semantics>
  <mi>ξ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ξ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \xi
  </annotation>
 </semantics>
</math>

 denotes input to the net.)</p>

<p>The network is then re-observed, and the above steps repeat until convergence. The model also reflects two biologically relevant concepts. The change in 

<math display="inline" id="Attractor_network:11">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

 models stimulus <em><a href="Priming_(psychology)" title="wikilink">priming</a></em> by allowing quicker convergence toward a recently visited attractor. Furthermore, the summed activity of attractors allows a <em>gang effect</em> that causes two nearby attractors to mutually reinforce the other's basin.</p>
<h3 id="reconsolidation-attractor-networks">Reconsolidation Attractor Networks</h3>

<p>Siegelmann (2008)<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> generalized the localist attractor network model to include the tuning of attractors themselves. This algorithm uses the EM method above, with the following modifications: (1) early termination of the algorithm when the attractor's activity is most distributed, or when high entropy suggests a need for additional memories, and (2) the ability to update the attractors themselves

<math display="block" id="Attractor_network:12">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>w</mi>
     <mi>i</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>t</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mrow>
      <mrow>
       <mi>v</mi>
       <msub>
        <mi>q</mi>
        <mi>i</mi>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>⋅</mo>
      <mi>y</mi>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>t</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mrow>
      <mrow>
       <mo stretchy="false">[</mo>
       <mrow>
        <mn>1</mn>
        <mo>-</mo>
        <mrow>
         <mi>v</mi>
         <msub>
          <mi>q</mi>
          <mi>i</mi>
         </msub>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>t</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
       </mrow>
       <mo stretchy="false">]</mo>
      </mrow>
      <mo>⋅</mo>
      <msub>
       <mi>w</mi>
       <mi>i</mi>
      </msub>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>t</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <plus></plus>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <ci>normal-⋅</ci>
       <apply>
        <times></times>
        <ci>v</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>q</ci>
         <ci>i</ci>
        </apply>
        <ci>t</ci>
       </apply>
       <ci>y</ci>
      </apply>
      <ci>t</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <ci>normal-⋅</ci>
       <apply>
        <csymbol cd="latexml">delimited-[]</csymbol>
        <apply>
         <minus></minus>
         <cn type="integer">1</cn>
         <apply>
          <times></times>
          <ci>v</ci>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>q</ci>
           <ci>i</ci>
          </apply>
          <ci>t</ci>
         </apply>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <ci>i</ci>
       </apply>
      </apply>
      <ci>t</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}(t+1)=vq_{i}(t)\cdot y(t)+[1-vq_{i}(t)]\cdot w_{i}(t)\,\!
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Attractor_network:13">
 <semantics>
  <mi>v</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>v</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   v
  </annotation>
 </semantics>
</math>

 is the step size parameter of the change of 

<math display="inline" id="Attractor_network:14">
 <semantics>
  <msub>
   <mi>w</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}
  </annotation>
 </semantics>
</math>

. This model reflects <a href="memory_reconsolidation" title="wikilink">memory reconsolidation</a> in animals, and shows some of the same dynamics as those found in memory experiments.</p>

<p>Further developments in attractor networks, such as <a href="kernel_trick" title="wikilink">kernel</a> based attractor networks,<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> have improved the computational feasibility of attractor networks as a learning algorithm, while maintaining the high-level flexibility to perform pattern completion on complex compositional structures.</p>
<h2 id="references">References</h2>

<p>"</p>

<p><a class="uri" href="Category:Networks" title="wikilink">Category:Networks</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">*<a href="#fnref1">↩</a></li>
<li id="fn2">*<a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4">*<a href="#fnref4">↩</a></li>
<li id="fn5">*<a href="#fnref5">↩</a></li>
<li id="fn6">*<a href="#fnref6">↩</a></li>
<li id="fn7">*<a href="#fnref7">↩</a></li>
<li id="fn8"></li>
<li id="fn9"></li>
<li id="fn10">*<a href="#fnref10">↩</a></li>
<li id="fn11">*<a href="#fnref11">↩</a></li>
</ol>
</section>
</hr></body>
</html>
