   Generalized linear array model      Generalized linear array model   In statistics , the generalized linear array model ( GLAM ) is used for analyzing data sets with array structures. It based on the generalized linear model with the design matrix written as a Kronecker product .  Overview  The generalized linear array model or GLAM was introduced in 2006. 1 Such models provide a structure and a computational procedure for fitting generalized linear models or GLMs whose model matrix can be written as a Kronecker product and whose data can be written as an array. In a large GLM, the GLAM approach gives very substantial savings in both storage and computational time over the usual GLM algorithm.  Suppose that the data   𝐘   𝐘   \mathbf{Y}   is arranged in a   d   d   d   -dimensional array with size     n  1   ×   n  2   ×  …  ×   n  d        subscript  n  1    subscript  n  2   normal-…   subscript  n  d     n_{1}\times n_{2}\times\ldots\times n_{d}   ; thus,the corresponding data vector    𝐲  =   𝐯𝐞𝐜   (  𝐘  )        𝐲    vec  𝐘     \mathbf{y}=\textbf{vec}(\mathbf{Y})   has size     n  1    n  2    n  3   ⋯   n  d        subscript  n  1    subscript  n  2    subscript  n  3   normal-⋯   subscript  n  d     n_{1}n_{2}n_{3}\cdots n_{d}   . Suppose also that the design matrix is of the form       𝐗  =    𝐗  d   ⊗   𝐗   d  -  1    ⊗  …  ⊗   𝐗  1     .      𝐗   tensor-product   subscript  𝐗  d    subscript  𝐗    d  1    normal-…   subscript  𝐗  1      \mathbf{X}=\mathbf{X}_{d}\otimes\mathbf{X}_{d-1}\otimes\ldots\otimes\mathbf{X}%
 _{1}.     The standard analysis of a GLM with data vector   𝐲   𝐲   \mathbf{y}   and design matrix   𝐗   𝐗   \mathbf{X}   proceeds by repeated evaluation of the scoring algorithm         𝐗  ′     𝐖  ~   δ   𝐗   𝜽  ^    =    𝐗  ′     𝐖  ~   δ    𝐳  ~     ,         superscript  𝐗  normal-′    subscript   normal-~  𝐖   δ   𝐗   normal-^  𝜽       superscript  𝐗  normal-′    subscript   normal-~  𝐖   δ    normal-~  𝐳      \mathbf{X}^{\prime}\tilde{\mathbf{W}}_{\delta}\mathbf{X}\hat{\boldsymbol{%
 \theta}}=\mathbf{X}^{\prime}\tilde{\mathbf{W}}_{\delta}\tilde{\mathbf{z}},     where    𝜽  ~     normal-~  𝜽    \tilde{\boldsymbol{\theta}}   represents the approximate solution of   𝜽   𝜽   \boldsymbol{\theta}   , and    𝜽  ^     normal-^  𝜽    \hat{\boldsymbol{\theta}}   is the improved value of it;    𝐖  δ     subscript  𝐖  δ    \mathbf{W}_{\delta}   is the diagonal weight matrix with elements        w   i  i    -  1    =     (    ∂   η  i     ∂   μ  i     )   2   var   (   y  i   )     ,       superscript   subscript  w    i  i      1       superscript       subscript  η  i       subscript  μ  i     2   var   subscript  y  i      w_{ii}^{-1}=\left(\frac{\partial\eta_{i}}{\partial\mu_{i}}\right)^{2}\text{var%
 }(y_{i}),     and      𝐳  =   𝜼  +    𝐖  δ   -  1     (   𝐲  -  𝝁   )         𝐳    𝜼     superscript   subscript  𝐖  δ     1      𝐲  𝝁       \mathbf{z}=\boldsymbol{\eta}+\mathbf{W}_{\delta}^{-1}(\mathbf{y}-\boldsymbol{%
 \mu})   is the working variable.  Computationally, GLAM provides array algorithms to calculate the linear predictor,      𝜼  =   𝐗  𝜽       𝜼    𝐗  𝜽     \boldsymbol{\eta}=\mathbf{X}\boldsymbol{\theta}   and the weighted inner product       𝐗  ′     𝐖  ~   δ   𝐗       superscript  𝐗  normal-′    subscript   normal-~  𝐖   δ   𝐗    \mathbf{X}^{\prime}\tilde{\mathbf{W}}_{\delta}\mathbf{X}   without evaluation of the model matrix    𝐗  .    𝐗   \mathbf{X}.     Example  In 2 dimensions, let     𝐗  =    𝐗  2   ⊗   𝐗  1     ,      𝐗   tensor-product   subscript  𝐗  2    subscript  𝐗  1      \mathbf{X}=\mathbf{X}_{2}\otimes\mathbf{X}_{1},   then the linear predictor is written     𝐗  1   𝚯   𝐗  2  ′        subscript  𝐗  1   𝚯   superscript   subscript  𝐗  2   normal-′     \mathbf{X}_{1}\boldsymbol{\Theta}\mathbf{X}_{2}^{\prime}   where   𝚯   𝚯   \boldsymbol{\Theta}   is the matrix of coefficients; the weighted inner product is obtained from    G    (   𝐗  1   )   ′   𝐖  G   (   𝐗  2   )       G   superscript   subscript  𝐗  1   normal-′   𝐖  G   subscript  𝐗  2     G(\mathbf{X}_{1})^{\prime}\mathbf{W}G(\mathbf{X}_{2})   and   𝐖   𝐖   \mathbf{W}   is the matrix of weights; here    G   (  𝐌  )       G  𝐌    G(\mathbf{M})   is the row tensor function of the    r  ×  c      r  c    r\times c   matrix   𝐌   𝐌   \mathbf{M}   given by       G   (  𝐌  )    =    (   𝐌  ⊗   𝟏  ′    )   *   (    𝟏  ′   ⊗  𝐌   )          G  𝐌      tensor-product  𝐌   superscript  1  normal-′     tensor-product   superscript  1  normal-′   𝐌      G(\mathbf{M})=(\mathbf{M}\otimes\mathbf{1}^{\prime})*(\mathbf{1}^{\prime}%
 \otimes\mathbf{M})   where   *     *   means element by element multiplication and   𝟏   1   \mathbf{1}   is a vector of 1's of length   c   c   c   .  These low storage high speed formulae extend to   d   d   d   -dimensions.  Applications  GLAM is designed to be used in   d   d   d   -dimensional smoothing problems where the data are arranged in an array and the smoothing matrix is constructed as a Kronecker product of   d   d   d   one-dimensional smoothing matrices.  References  "  Category:Multivariate statistics  Category:Generalized linear models     ↩     