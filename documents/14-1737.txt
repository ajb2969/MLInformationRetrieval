   Thompson sampling      Thompson sampling   In artificial intelligence , Thompson sampling , 1 named after William R. Thompson, is a heuristic for choosing actions that addresses the exploration-exploitation dilemma in the multi-armed bandit problem. It consists in choosing the action that maximizes the expected reward with respect to a randomly drawn belief.  Description  Consider a set of contexts   ùí≥   ùí≥   \mathcal{X}   , a set of actions   ùíú   ùíú   \mathcal{A}   , and rewards in   ‚Ñù   ‚Ñù   \mathbb{R}   . In each round, the player obtains a context    x  ‚àà  ùí≥      x  ùí≥    x\in\mathcal{X}   , plays an action    a  ‚àà  ùíú      a  ùíú    a\in\mathcal{A}   and receives a reward    r  ‚àà  ‚Ñù      r  ‚Ñù    r\in\mathbb{R}   following a distribution that depends on the context and the issued action. The aim of the player is to play actions such as to maximize the cumulative rewards.  The elements of Thompson sampling are as follows:   a set   Œò   normal-Œò   \Theta   of parameters   Œ∏   Œ∏   \theta   ;  a prior distribution    P   (  Œ∏  )       P  Œ∏    P(\theta)   on these parameters;  past observations triplets    ùíü  =   {   (  x  ;  a  ;  r  )   }       ùíü    x  a  r      \mathcal{D}=\{(x;a;r)\}   ;  a likelihood function    P   (  r  |  Œ∏  ,  a  ,  x  )      fragments  P   fragments  normal-(  r  normal-|  Œ∏  normal-,  a  normal-,  x  normal-)     P(r|\theta,a,x)   ;  a posterior distribution    P   (  Œ∏  |  ùíü  )   ‚àù  P   (  ùíü  |  Œ∏  )   P   (  Œ∏  )      fragments  P   fragments  normal-(  Œ∏  normal-|  D  normal-)   proportional-to  P   fragments  normal-(  D  normal-|  Œ∏  normal-)   P   fragments  normal-(  Œ∏  normal-)     P(\theta|\mathcal{D})\propto P(\mathcal{D}|\theta)P(\theta)   , where    P   (  ùíü  |  Œ∏  )      fragments  P   fragments  normal-(  D  normal-|  Œ∏  normal-)     P(\mathcal{D}|\theta)   is the likelihood function.   Thompson sampling consists in playing the action     a  ‚àó   ‚àà  ùíú       superscript  a  normal-‚àó   ùíú    a^{\ast}\in\mathcal{A}   according to the probability that it maximizes the expected reward, i.e.      ‚à´  ùïÄ   [  ùîº   (  r  |  a  ,  x  ,  Œ∏  )   =   max   a  ‚Ä≤    ùîº   (  r  |   a  ‚Ä≤   ,  x  ,  Œ∏  )   ]   P   (  Œ∏  |  ùíü  )   d  Œ∏  ,     fragments   I   fragments  normal-[  E   fragments  normal-(  r  normal-|  a  normal-,  x  normal-,  Œ∏  normal-)     subscript    superscript  a  normal-‚Ä≤    E   fragments  normal-(  r  normal-|   superscript  a  normal-‚Ä≤   normal-,  x  normal-,  Œ∏  normal-)   normal-]   P   fragments  normal-(  Œ∏  normal-|  D  normal-)   d  Œ∏  normal-,    \int\mathbb{I}[\mathbb{E}(r|a,x,\theta)=\max_{a^{\prime}}\mathbb{E}(r|a^{%
 \prime},x,\theta)]P(\theta|\mathcal{D})\,d\theta,   where   ùïÄ   ùïÄ   \mathbb{I}   is the indicator function.  In practice, the rule is implemented by sampling, in each round, a parameter    Œ∏  ‚àó     superscript  Œ∏  normal-‚àó    \theta^{\ast}   from the posterior    P   (  Œ∏  |  ùíü  )      fragments  P   fragments  normal-(  Œ∏  normal-|  D  normal-)     P(\theta|\mathcal{D})   , and choosing the action    a  ‚àó     superscript  a  normal-‚àó    a^{\ast}   that maximizes    ùîº   [  r  |   Œ∏  ‚àó   ,   a  ‚àó   ,  x  ]      fragments  E   fragments  normal-[  r  normal-|   superscript  Œ∏  normal-‚àó   normal-,   superscript  a  normal-‚àó   normal-,  x  normal-]     \mathbb{E}[r|\theta^{\ast},a^{\ast},x]   , i.e. the expected reward given the parameter, the action and the current context. Conceptually, this means that the player instantiates his beliefs randomly in each round, and then he acts optimally according to them.  History  Thompson sampling was originally described in an article by Thompson from 1933 2 but has been largely ignored by the artificial intelligence community. It was subsequently rediscovered numerous times independently in the context of reinforcement learning. 3 4 5 6 7 8 A first proof of convergence for the bandit case has been shown in 1997. 9 The first application to Markov decision processes was in 2000. 10 A related approach (see Bayesian control rule ) was published in 2010. 11 In 2010 it was also shown that Thompson sampling is instantaneously self-correcting . 12 Asymptotic convergence results for contextual bandits were published in 2011. 13 Thompson sampling has also been applied to A/B testing in website design and online advertising. 14 Recently, Thompson sampling has formed the basis for accelerated learning in decentralized decision making. 15  Properties  Convergence  Optimality  Relationship to other approaches  Probability matching  Probability matching is a decision strategy in which predictions of class membership are proportional to the class base rates. Thus, if in the training set positive examples are observed 60% of the time, and negative examples are observed 40% of the time, the observer using a probability-matching strategy will predict (for unlabeled examples) a class label of "positive" on 60% of instances, and a class label of "negative" on 40% of instances.  Bayesian control rule  A generalization of Thompson sampling to arbitrary dynamical environments and causal structures, known as Bayesian control rule , has been shown to be the optimal solution to the adaptive coding problem with actions and observations. 16 In this formulation, an agent is conceptualized as a mixture over a set of behaviours. As the agent interacts with its environment, it learns the causal properties and adopts the behaviour that minimizes the relative entropy to the behaviour with the best prediction of the environment's behaviour. If these behaviours have been chosen according to the maximum expected utility principle, then the asymptotic behaviour of the Bayesian control rule matches the asymptotic behaviour of the perfectly rational agent.  The setup is as follows. Let     a  1   ,   a  2   ,  ‚Ä¶  ,   a  T       subscript  a  1    subscript  a  2   normal-‚Ä¶   subscript  a  T     a_{1},a_{2},\ldots,a_{T}   be the actions issued by an agent up to time   T   T   T   , and let     o  1   ,   o  2   ,  ‚Ä¶  ,   o  T       subscript  o  1    subscript  o  2   normal-‚Ä¶   subscript  o  T     o_{1},o_{2},\ldots,o_{T}   be the observations gathered by the agent up to time   T   T   T   . Then, the agent issues the action    a   T  +  1      subscript  a    T  1     a_{T+1}   with probability: 17      P   (   a   T  +  1    |    a  ^    1  :  T    ,   o   1  :  T    )   ,     fragments  P   fragments  normal-(   subscript  a    T  1    normal-|   subscript   normal-^  a    normal-:  1  T    normal-,   subscript  o   normal-:  1  T    normal-)   normal-,    P(a_{T+1}|\hat{a}_{1:T},o_{1:T}),   where the "hat"-notation     a  ^   t     subscript   normal-^  a   t    \hat{a}_{t}   denotes the fact that    a  t     subscript  a  t    a_{t}   is a causal intervention (see Causality ), and not an ordinary observation. If the agent holds beliefs    Œ∏  ‚àà  Œò      Œ∏  normal-Œò    \theta\in\Theta   over its behaviors, then the Bayesian control rule becomes      P   (   a   T  +  1    |    a  ^    1  :  T    ,   o   1  :  T    )   =   ‚à´  Œò   P   (   a   T  +  1    |  Œ∏  ,    a  ^    1  :  T    ,   o   1  :  T    )   P   (  Œ∏  |    a  ^    1  :  T    ,   o   1  :  T    )   d  Œ∏     fragments  P   fragments  normal-(   subscript  a    T  1    normal-|   subscript   normal-^  a    normal-:  1  T    normal-,   subscript  o   normal-:  1  T    normal-)     subscript   normal-Œò   P   fragments  normal-(   subscript  a    T  1    normal-|  Œ∏  normal-,   subscript   normal-^  a    normal-:  1  T    normal-,   subscript  o   normal-:  1  T    normal-)   P   fragments  normal-(  Œ∏  normal-|   subscript   normal-^  a    normal-:  1  T    normal-,   subscript  o   normal-:  1  T    normal-)   d  Œ∏    P(a_{T+1}|\hat{a}_{1:T},o_{1:T})=\int_{\Theta}P(a_{T+1}|\theta,\hat{a}_{1:T},o%
 _{1:T})P(\theta|\hat{a}_{1:T},o_{1:T})\,d\theta   , where    P   (  Œ∏  |    a  ^    1  :  T    ,   o   1  :  T    )      fragments  P   fragments  normal-(  Œ∏  normal-|   subscript   normal-^  a    normal-:  1  T    normal-,   subscript  o   normal-:  1  T    normal-)     P(\theta|\hat{a}_{1:T},o_{1:T})   is the posterior distribution over the parameter   Œ∏   Œ∏   \theta   given actions    a   1  :  T      subscript  a   normal-:  1  T     a_{1:T}   and observations    o   1  :  T      subscript  o   normal-:  1  T     o_{1:T}   .  In practice, the Bayesian control amounts to sampling, in each time step, a parameter    Œ∏  ‚àó     superscript  Œ∏  normal-‚àó    \theta^{\ast}   from the posterior distribution    P   (  Œ∏  |    a  ^    1  :  T    ,   o   1  :  T    )      fragments  P   fragments  normal-(  Œ∏  normal-|   subscript   normal-^  a    normal-:  1  T    normal-,   subscript  o   normal-:  1  T    normal-)     P(\theta|\hat{a}_{1:T},o_{1:T})   , where the posterior distribution is computed using Bayes' rule by only considering the (causal) likelihoods of the observations     o  1   ,   o  2   ,  ‚Ä¶  ,   o  T       subscript  o  1    subscript  o  2   normal-‚Ä¶   subscript  o  T     o_{1},o_{2},\ldots,o_{T}   and ignoring the (causal) likelihoods of the actions     a  1   ,   a  2   ,  ‚Ä¶  ,   a  T       subscript  a  1    subscript  a  2   normal-‚Ä¶   subscript  a  T     a_{1},a_{2},\ldots,a_{T}   , and then by sampling the action    a   T  +  1   ‚àó     subscript   superscript  a  normal-‚àó     T  1     a^{\ast}_{T+1}   from the action distribution    P   (   a   T  +  1    |   Œ∏  ‚àó   ,    a  ^    1  :  T    ,   o   1  :  T    )      fragments  P   fragments  normal-(   subscript  a    T  1    normal-|   superscript  Œ∏  normal-‚àó   normal-,   subscript   normal-^  a    normal-:  1  T    normal-,   subscript  o   normal-:  1  T    normal-)     P(a_{T+1}|\theta^{\ast},\hat{a}_{1:T},o_{1:T})   .  References  "  Category:Artificial intelligence  Category:Heuristic algorithms  Category:Sequential methods  Category:Sequential experiments  Category:Articles created via the Article Wizard                         