   Symmetric rank-one      Symmetric rank-one   The Symmetric Rank 1 ( SR1 ) method is a quasi-Newton method to update the second derivative (Hessian) based on the derivatives (gradients) calculated at two points. It is a generalization to the secant method for a multidimensional problem. This update maintains the symmetry of the matrix but does not guarantee that the update be positive definite .  The sequence of Hessian approximations generated by the SR1 method converges to the true Hessian under mild conditions, in theory; in practice, the approximate Hessians generated by the SR1 method show faster progress towards the true Hessian than do popular alternatives ( BFGS or DFP ), in preliminary numerical experiments. 1 The SR1 method has computational advantages for sparse or partially separable problems.  A twice continuously differentiable function    x  ↦   f   (  x  )       maps-to  x    f  x     x\mapsto f(x)   has a gradient (    ∇  f     normal-∇  f    \nabla f   ) and Hessian matrix    B   B   B   : The function   f   f   f   has an expansion as a Taylor series at    x  0     subscript  x  0    x_{0}   , which can be truncated         f   (    x  0   +   Δ  x    )    =    f   (   x  0   )    +    ∇  f     (   x  0   )   T   Δ  x   +    1  2   Δ   x  T   B  Δ  x          f     subscript  x  0     normal-Δ  x         f   subscript  x  0       normal-∇  f    superscript   subscript  x  0   T   normal-Δ  x       1  2   normal-Δ   superscript  x  T   B  normal-Δ  x      f(x_{0}+\Delta x)=f(x_{0})+\nabla f(x_{0})^{T}\Delta x+\frac{1}{2}\Delta x^{T}%
 {B}\Delta x   ;     its gradient has a Taylor-series approximation also          ∇  f    (    x  0   +   Δ  x    )    =     ∇  f    (   x  0   )    +   B  Δ  x           normal-∇  f      subscript  x  0     normal-Δ  x          normal-∇  f    subscript  x  0      B  normal-Δ  x      \nabla f(x_{0}+\Delta x)=\nabla f(x_{0})+B\Delta x   ,     which is used to update   B   B   B   . The above secant-equation need not have a unique solution   B   B   B   . The SR1 formula computes (via an update of rank 1) the symmetric solution that is closest to the current approximate-value    B  k     subscript  B  k    B_{k}   :         B   k  +  1    =    B  k   +     (    y  k   -    B  k   Δ   x  k     )     (    y  k   -    B  k   Δ   x  k     )   T       (    y  k   -    B  k   Δ   x  k     )   T   Δ   x  k           subscript  B    k  1       subscript  B  k          subscript  y  k      subscript  B  k   normal-Δ   subscript  x  k      superscript     subscript  y  k      subscript  B  k   normal-Δ   subscript  x  k     T       superscript     subscript  y  k      subscript  B  k   normal-Δ   subscript  x  k     T   normal-Δ   subscript  x  k        B_{k+1}=B_{k}+\frac{(y_{k}-B_{k}\Delta x_{k})(y_{k}-B_{k}\Delta x_{k})^{T}}{(y%
 _{k}-B_{k}\Delta x_{k})^{T}\Delta x_{k}}   ,     where         y  k   =     ∇  f    (    x  k   +   Δ   x  k     )    -    ∇  f    (   x  k   )          subscript  y  k        normal-∇  f      subscript  x  k     normal-Δ   subscript  x  k         normal-∇  f    subscript  x  k       y_{k}=\nabla f(x_{k}+\Delta x_{k})-\nabla f(x_{k})   .     The corresponding update to the approximate inverse-Hessian     H  k   =   B  k   -  1         subscript  H  k    superscript   subscript  B  k     1      H_{k}=B_{k}^{-1}   is         H   k  +  1    =    H  k   +     (    Δ   x  k    -    H  k    y  k     )     (    Δ   x  k    -    H  k    y  k     )   T       (    Δ   x  k    -    H  k    y  k     )   T    y  k           subscript  H    k  1       subscript  H  k           normal-Δ   subscript  x  k       subscript  H  k    subscript  y  k      superscript      normal-Δ   subscript  x  k       subscript  H  k    subscript  y  k     T       superscript      normal-Δ   subscript  x  k       subscript  H  k    subscript  y  k     T    subscript  y  k        H_{k+1}=H_{k}+\frac{(\Delta x_{k}-H_{k}y_{k})(\Delta x_{k}-H_{k}y_{k})^{T}}{(%
 \Delta x_{k}-H_{k}y_{k})^{T}y_{k}}   .     The SR1 formula has been rediscovered a number of times. A drawback is that the denominator can vanish. Some authors have suggested that the update be applied only if         |   Δ   x  k  T    (    y  k   -    B  k   Δ   x  k     )    |   ≥    r   ∥   Δ   x  k    ∥    ⋅   ∥    y  k   -    B  k   Δ   x  k     ∥            normal-Δ   superscript   subscript  x  k   T      subscript  y  k      subscript  B  k   normal-Δ   subscript  x  k        normal-⋅    r   norm    normal-Δ   subscript  x  k       norm     subscript  y  k      subscript  B  k   normal-Δ   subscript  x  k         |\Delta x_{k}^{T}(y_{k}-B_{k}\Delta x_{k})|\geq r\|\Delta x_{k}\|\cdot\|y_{k}-%
 B_{k}\Delta x_{k}\|   ,     where    r  ∈   (  0  ,  1  )       r   0  1     r\in(0,1)   is a small number, e.g.    10   -  8      superscript  10    8     10^{-8}   . 2  See also   Quasi-Newton method  Newton's method in optimization  Broyden-Fletcher-Goldfarb-Shanno (BFGS) method  L-BFGS method   Notes    References   Byrd, Richard H. (1996) Analysis of a Symmetric Rank-One Trust Region Method. SIAM Journal on Optimization 6(4)   Khalfan, H. Fayez (1993) A Theoretical and Experimental Study of the Symmetric Rank-One Update. SIAM Journal on Optimization 3(1)  Nocedal, Jorge & Wright, Stephen J. (1999). Numerical Optimization . Springer-Verlag. ISBN 0-387-98793-2.   "  Category:Optimization algorithms and methods     ↩  ↩     