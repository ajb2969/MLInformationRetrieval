   Bootstrapping populations      Bootstrapping populations   Starting with a sample     {   x  1   ,  …  ,   x  m   }      subscript  x  1   normal-…   subscript  x  m     \{x_{1},\ldots,x_{m}\}   observed from a random variable  X having a given distribution law with a set of non fixed parameters which we denote with a vector   𝜽   𝜽   \boldsymbol{\theta}   , a parametric inference problem consists of computing suitable values – call them estimates – of these parameters precisely on the basis of the sample. An estimate is suitable if replacing it with the unknown parameter does not cause major damage in next computations. In Algorithmic inference , suitability of an estimate reads in terms of compatibility with the observed sample.  In this framework, resampling methods are aimed at generating a set of candidate values to replace the unknown parameters that we read as compatible replicas of them. They represent a population of specifications of a random vector   𝚯   𝚯   \boldsymbol{\Theta}    1 compatible with an observed sample, where the compatibility of its values has the properties of a probability distribution. By plugging parameters into the expression of the questioned distribution law, we bootstrap entire populations of random variables compatible with the observed sample.  The rationale of the algorithms computing the replicas, which we denote population bootstrap procedures, is to identify a set of statistics    {   s  1   ,  …  ,   s  k   }      subscript  s  1   normal-…   subscript  s  k     \{s_{1},\ldots,s_{k}\}   exhibiting specific properties, denoting a well behavior , w.r.t. the unknown parameters. The statistics are expressed as functions of the observed values    {   x  1   ,  …  ,   x  m   }      subscript  x  1   normal-…   subscript  x  m     \{x_{1},\ldots,x_{m}\}   , by definition. The    x  i     subscript  x  i    x_{i}   may be expressed as a function of the unknown parameters and a random seed specification    z  i     subscript  z  i    z_{i}   through the sampling mechanism     (   g  𝜽   ,  Z  )      subscript  g  𝜽   Z    (g_{\boldsymbol{\theta}},Z)   , in turn. Then, by plugging the second expression in the former, we obtain    s  j     subscript  s  j    s_{j}   expressions as functions of seeds and parameters – the master equations – that we invert to find values of the latter as a function of: i) the statistics, whose values in turn are fixed at the observed ones; and ii) the seeds, which are random according to their own distribution. Hence from a set of seed samples we obtain a set of parameter replicas.  Method  Given a    𝒙  =   {   x  1   ,  …  ,   x  m   }       𝒙    subscript  x  1   normal-…   subscript  x  m      \boldsymbol{x}=\{x_{1},\ldots,x_{m}\}   of a random variable X and a sampling mechanism     (   g  𝜽   ,  Z  )      subscript  g  𝜽   Z    (g_{\boldsymbol{\theta}},Z)   for X , the realization x is given by    𝒙  =   {    g  𝜽    (   z  1   )    ,  …  ,    g  𝜽    (   z  m   )    }       𝒙      subscript  g  𝜽    subscript  z  1    normal-…     subscript  g  𝜽    subscript  z  m       \boldsymbol{x}=\{g_{\boldsymbol{\theta}}(z_{1}),\ldots,g_{\boldsymbol{\theta}}%
 (z_{m})\}   , with    𝜽  =   (   θ  1   ,  …  ,   θ  k   )       𝜽    subscript  θ  1   normal-…   subscript  θ  k      \boldsymbol{\theta}=(\theta_{1},\ldots,\theta_{k})   . Focusing on well-behaved statistics ,              s  1   =    h  1    (   x  1   ,  …  ,   x  m   )     ,       subscript  s  1      subscript  h  1     subscript  x  1   normal-…   subscript  x  m       s_{1}=h_{1}(x_{1},\ldots,x_{m}),            ⋮  ⋮     normal-⋮  normal-⋮    \vdots\ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots              s  k   =    h  k    (   x  1   ,  …  ,   x  m   )     ,       subscript  s  k      subscript  h  k     subscript  x  1   normal-…   subscript  x  m       s_{k}=h_{k}(x_{1},\ldots,x_{m}),          for their parameters, the master equations read             s  1   =    h  1    (    g  𝜽    (   z  1   )    ,  …  ,    g  𝜽    (   z  m   )    )    =    ρ  1    (  𝜽  ;   z  1   ,  …  ,   z  m   )           subscript  s  1      subscript  h  1       subscript  g  𝜽    subscript  z  1    normal-…     subscript  g  𝜽    subscript  z  m              subscript  ρ  1    𝜽   subscript  z  1   normal-…   subscript  z  m        s_{1}=h_{1}(g_{\boldsymbol{\theta}}(z_{1}),\ldots,g_{\boldsymbol{\theta}}(z_{m%
 }))=\rho_{1}(\boldsymbol{\theta};z_{1},\ldots,z_{m})            ⋮  ⋮  ⋮     normal-⋮  normal-⋮  normal-⋮    \vdots\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ %
 \ \ \ \ \ \ \ \ \ \ \ \ \vdots              s  k   =    h  k    (    g  𝜽    (   z  1   )    ,  …  ,    g  𝜽    (   z  m   )    )    =    ρ  k    (  𝜽  ;   z  1   ,  …  ,   z  m   )     .         subscript  s  k      subscript  h  k       subscript  g  𝜽    subscript  z  1    normal-…     subscript  g  𝜽    subscript  z  m              subscript  ρ  k    𝜽   subscript  z  1   normal-…   subscript  z  m        s_{k}=h_{k}(g_{\boldsymbol{\theta}}(z_{1}),\ldots,g_{\boldsymbol{\theta}}(z_{m%
 }))=\rho_{k}(\boldsymbol{\theta};z_{1},\ldots,z_{m}).          For each sample seed    {   z  1   ,  …  ,   z  m   }      subscript  z  1   normal-…   subscript  z  m     \{z_{1},\ldots,z_{m}\}   a vector of parameters   𝜽   𝜽   \boldsymbol{\theta}   is obtained from the solution of the above system with    s  i     subscript  s  i    s_{i}   fixed to the observed values. Having computed a huge set of compatible vectors, say N , the empirical marginal distribution of    Θ  j     subscript  normal-Θ  j    \Theta_{j}   is obtaineded by:               F  ^    Θ  j     (  θ  )    =    ∑   i  =  1   N     1  N    I   (   -  ∞   ,  θ  ]     (    θ  ˘    j  ,  i    )            subscript   normal-^  F    subscript  normal-Θ  j    θ     superscript   subscript     i  1    N       1  N    subscript  I       θ     subscript   normal-˘  θ    j  i        \widehat{F}_{\Theta_{j}}(\theta)=\sum_{i=1}^{N}\frac{1}{N}I_{(-\infty,\theta]}%
 (\breve{\theta}_{j,i})      (2)       where     θ  ˘    j  ,  i      subscript   normal-˘  θ    j  i     \breve{\theta}_{j,i}   is the j-th component of the generic solution of (1) and where     I   (   -  ∞   ,  θ  ]     (    θ  ˘    j  ,  i    )        subscript  I       θ     subscript   normal-˘  θ    j  i      I_{(-\infty,\theta]}(\breve{\theta}_{j,i})   is the indicator function of     θ  ˘    j  ,  i      subscript   normal-˘  θ    j  i     \breve{\theta}_{j,i}   in the interval     (   -  ∞   ,  θ  ]   .         θ    (-\infty,\theta].   Some indeterminacies remain if X is discrete and this we will be considered shortly. The whole procedure may be summed up in the form of the following Algorithm, where the index   𝚯   𝚯   \boldsymbol{\Theta}   of    𝒔  𝚯     subscript  𝒔  𝚯    \boldsymbol{s}_{\boldsymbol{\Theta}}   denotes the parameter vector from which the statistics vector is derived.  Algorithm      Generating parameter populations through a bootstrap       Given a sample    {   x  1   ,  …  ,   x  m   }      subscript  x  1   normal-…   subscript  x  m     \{x_{1},\ldots,x_{m}\}   from a random variable with parameter vector   𝜽   𝜽   \boldsymbol{\theta}   unknown,   Identify a vector of well-behaved statistics    𝑺   𝑺   \boldsymbol{S}   for   𝚯   𝚯   \boldsymbol{\Theta}   ;  compute a specification    𝒔  𝚯     subscript  𝒔  𝚯    \boldsymbol{s}_{\boldsymbol{\Theta}}   of   𝑺   𝑺   \boldsymbol{S}   from the sample;  repeat for a satisfactory number N of iterations:  draw a sample seed     𝒛  ˘   i     subscript   normal-˘  𝒛   i    \breve{\boldsymbol{z}}_{i}   of size m from the seed random variable;  get      𝜽  ˘   i   =   Inv   (  𝒔  ,   𝒛  i   )         subscript   normal-˘  𝜽   i     Inv   𝒔   subscript  𝒛  i       \breve{\boldsymbol{\theta}}_{i}=\mathrm{Inv}(\boldsymbol{s},\boldsymbol{z}_{i})   as a solution of (1) in θ with    𝒔  =   𝒔  𝚯       𝒔   subscript  𝒔  𝚯     \boldsymbol{s}=\boldsymbol{s}_{\boldsymbol{\Theta}}   and     𝒛  i   =   {    z  ˘   1   ,  …  ,    z  ˘   m   }        subscript  𝒛  i     subscript   normal-˘  z   1   normal-…   subscript   normal-˘  z   m      \boldsymbol{z}_{i}=\{\breve{z}_{1},\ldots,\breve{z}_{m}\}   ;  add     𝜽  ˘   i     subscript   normal-˘  𝜽   i    \breve{\boldsymbol{\theta}}_{i}   to   𝚯   𝚯   \boldsymbol{\Theta}   ; population.       You may easily see from a table of sufficient statistics that we obtain the curve in the picture on the left by computing the empirical distribution (2) on the population obtained through the above algorithm when: i) X is an Exponential random variable, ii)     s  Λ   =    ∑   j  =  1   m    x  j         subscript  s  normal-Λ     superscript   subscript     j  1    m    subscript  x  j      s_{\Lambda}=\sum_{j=1}^{m}x_{j}   , and       iii) Inv   (   s  Λ   ,   𝒖  i   )    =    ∑   j  =  1   m     (   -   log   u   i  j      )   /   s  Λ           iii) Inv    subscript  s  normal-Λ    subscript  𝒖  i       superscript   subscript     j  1    m          subscript  u    i  j       subscript  s  normal-Λ       \text{ iii) Inv}(s_{\Lambda},\boldsymbol{u}_{i})=\sum_{j=1}^{m}(-\log u_{ij})/%
 s_{\Lambda}   , and the curve in the picture on the right when: i) X is a Uniform random variable in    [  0  ,  a  ]     0  a    [0,a]   , ii)     s  A   =    max   j  =   1  ,  …  ,  m      x  j         subscript  s  A     subscript     j   1  normal-…  m      subscript  x  j      s_{A}=\max_{j=1,\ldots,m}x_{j}   , and       iii) Inv   (   s  A   ,   𝒖  i   )    =    s  A   /    max   j  =   1  ,  …  ,  m      {   u   i  j    }           iii) Inv    subscript  s  A    subscript  𝒖  i        subscript  s  A     subscript     j   1  normal-…  m      subscript  u    i  j        \text{iii) Inv}(s_{A},\boldsymbol{u}_{i})=s_{A}/\max_{j=1,\ldots,m}\{u_{ij}\}   .  Remark  Note that the accuracy with which a parameter distribution law of populations compatible with a sample is obtained is not a function of the sample size. Instead, it is a function of the number of seeds we draw. In turn, this number is purely a matter of computational time but does not require any extension of the observed data. With other bootstrapping methods focusing on a generation of sample replicas (like those proposed by ) the accuracy of the estimate distributions depends on the sample size.  Example  For   𝒙   𝒙   \boldsymbol{x}   expected to represent a Pareto distribution , whose specification requires values for the parameters   a   a   a   and k , 2 we have that the cumulative distribution function reads:        F  X    (  x  )    =   1  -    (   k  x   )   a           subscript  F  X   x     1   superscript    k  x   a      F_{X}(x)=1-\left(\frac{k}{x}\right)^{a}   .  A sampling mechanism     (   g   (  a  ,  k  )    ,  U  )      subscript  g   a  k    U    (g_{(a,k)},U)   has    [  0  ,  1  ]     0  1    [0,1]    uniform seed  U and explaining function    g   (  a  ,  k  )      subscript  g   a  k     g_{(a,k)}   described by:      x  =   g   (  a  ,  k  )    =     (   1  -  u   )    -   1  a     k         x   subscript  g   a  k            superscript    1  u       1  a     k      x=g_{(a,k)}=(1-u)^{-\frac{1}{a}}k     A relevant statistic    𝒔  𝚯     subscript  𝒔  𝚯    \boldsymbol{s}_{\boldsymbol{\Theta}}   is constituted by the pair of joint sufficient statistics for   A   A   A   and K , respectively      s  1   =    ∑   i  =  1   m    log   x  i      ,    s  2   =   min   {   x  i   }        formulae-sequence     subscript  s  1     superscript   subscript     i  1    m      subscript  x  i         subscript  s  2      subscript  x  i       s_{1}=\sum_{i=1}^{m}\log x_{i},s_{2}=\min\{x_{i}\}   . The master equations read       s  1   =     ∑   i  =  1   m   -    1  a    log   (   1  -   u  i    )      +   m   log  k          subscript  s  1        superscript   subscript     i  1    m       1  a       1   subscript  u  i         m    k       s_{1}=\sum_{i=1}^{m}-\frac{1}{a}\log(1-u_{i})+m\log k          s  2   =     (   1  -   u  min    )    -   1  a     k        subscript  s  2      superscript    1   subscript  u         1  a     k     s_{2}=(1-u_{\min})^{-\frac{1}{a}}k     with     u  min   =   min   {   u  i   }         subscript  u       subscript  u  i      u_{\min}=\min\{u_{i}\}   .  Figure on the right reports the three-dimensional plot of the empirical cumulative distribution function (2) of    (  A  ,  K  )     A  K    (A,K)   .  Notes    References       "  Category:Computational statistics  Category:Algorithmic inference  Category:Resampling (statistics)     By default, capital letters (such as U , X ) will denote random variables and small letters ( u , x ) their corresponding realizations. ↩  We denote here with symbols a and k the Pareto parameters elsewhere indicated through k and    x  min     subscript  x  min    x_{\mathrm{min}}   . ↩     