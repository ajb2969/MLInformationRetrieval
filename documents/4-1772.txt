   Matched filter      Matched filter  In [[signal processing]], a '''matched filter''' (originally known as a '''North filter''' After D.O. North who first introduced the concept: {{cite journal|title = An analysis of the factors which determine signal/noise discrimination in pulsed carrier systems|author = North, D. O. |journal = RCA Labs., Princeton, NJ, Rep. PTR-6C|year = 1963}}) is obtained by correlating a known signal , or template , with an unknown signal to detect the presence of the template in the unknown signal. This is equivalent to convolving the unknown signal with a conjugated time-reversed version of the template. The matched filter is the optimal linear  filter for maximizing the signal  to  noise  ratio (SNR) in the presence of additive stochastic  noise . Matched filters are commonly used in radar , in which a known signal is sent out, and the reflected signal is examined for common elements of the out-going signal. Pulse  compression is an example of matched filtering. It is so called  because impulse response is matched to input pulse signals.  Two-dimensional matched filters are commonly used in image  processing , e.g., to improve SNR for X-ray.  Matched filtering is a demodulation technique with LTI filters to maximize SNR. 1  Derivation of the matched filter impulse response  The following section derives the matched filter for a discrete-time system . The derivation for a continuous-time system is similar, with summations replaced with integrals.  The matched filter is the linear filter,   h   h   h   , that maximizes the output signal-to-noise ratio .         y    [  n  ]    =    ∑   k  =   -  ∞    ∞    h   [   n  -  k   ]   x   [  k  ]      .        y   delimited-[]  n      superscript   subscript     k            h   delimited-[]    n  k    x   delimited-[]  k       \ y[n]=\sum_{k=-\infty}^{\infty}h[n-k]x[k].     Though we most often express filters as the impulse response of convolution systems, as above (see LTI system theory ), it is easiest to think of the matched filter in the context of the inner product , which we will see shortly.  We can derive the linear filter that maximizes output signal-to-noise ratio by invoking a geometric argument. The intuition behind the matched filter relies on correlating the received signal (a vector) with a filter (another vector) that is parallel with the signal, maximizing the inner product. This enhances the signal. When we consider the additive stochastic noise, we have the additional challenge of minimizing the output due to noise by choosing a filter that is orthogonal to the noise.  Let us formally define the problem. We seek a filter,   h   h   h   , such that we maximize the output signal-to-noise ratio, where the output is the inner product of the filter and the observed signal   x   x   x   .  Our observed signal consists of the desirable signal   s   s   s   and additive noise   v   v   v   :        x   =   s  +  v    .      x    s  v     \ x=s+v.\,     Let us define the covariance matrix of the noise, reminding ourselves that this matrix has Hermitian symmetry , a property that will become useful in the derivation:        R   v   =   E   {   v   v  H    }         subscript  R  v     E     v   superscript  v  normal-H        \ R_{v}=E\{vv^{\mathrm{H}}\}\,     where    v  H     superscript  v  normal-H    v^{\mathrm{H}}   denotes the conjugate transpose of   v   v   v   , and   E   E   E   denotes expectation . Let us call our output,   y   y   y   , the inner product of our filter and the observed signal such that        y   =    ∑   k  =   -  ∞    ∞     h  *    [  k  ]   x   [  k  ]     =    h  H   x   =     h  H   s   +    h  H   v    =    y  s   +   y  v     .        y    superscript   subscript     k             superscript  h     delimited-[]  k   x   delimited-[]  k             superscript  h  normal-H   x             superscript  h  normal-H   s      superscript  h  normal-H   v            subscript  y  s    subscript  y  v       \ y=\sum_{k=-\infty}^{\infty}h^{*}[k]x[k]=h^{\mathrm{H}}x=h^{\mathrm{H}}s+h^{%
 \mathrm{H}}v=y_{s}+y_{v}.     We now define the signal-to-noise ratio, which is our objective function, to be the ratio of the power of the output due to the desired signal to the power of the output due to the noise:       SNR  =     |   y  s   |   2    E   {    |   y  v   |   2   }      .      SNR     superscript     subscript  y  s    2     E    superscript     subscript  y  v    2        \mathrm{SNR}=\frac{|y_{s}|^{2}}{E\{|y_{v}|^{2}\}}.     We rewrite the above:       SNR  =     |    h  H   s   |   2    E   {    |    h  H   v   |   2   }      .      SNR     superscript       superscript  h  normal-H   s    2     E    superscript       superscript  h  normal-H   v    2        \mathrm{SNR}=\frac{|h^{\mathrm{H}}s|^{2}}{E\{|h^{\mathrm{H}}v|^{2}\}}.     We wish to maximize this quantity by choosing   h   h   h   . Expanding the denominator of our objective function, we have         E    {    |    h  H   v   |   2   }    =   E   {    (    h  H   v   )     (    h  H   v   )   H    }    =    h  H   E   {   v   v  H    }   h   =    h  H    R  v   h    .          E    superscript       superscript  h  normal-H   v    2       E        superscript  h  normal-H   v    superscript     superscript  h  normal-H   v   normal-H              superscript  h  normal-H   E     v   superscript  v  normal-H     h           superscript  h  normal-H    subscript  R  v   h      \ E\{|h^{\mathrm{H}}v|^{2}\}=E\{(h^{\mathrm{H}}v){(h^{\mathrm{H}}v)}^{\mathrm{%
 H}}\}=h^{\mathrm{H}}E\{vv^{\mathrm{H}}\}h=h^{\mathrm{H}}R_{v}h.\,     Now, our   SNR   SNR   \mathrm{SNR}   becomes       SNR  =     |    h  H   s   |   2     h  H    R  v   h     .      SNR     superscript       superscript  h  normal-H   s    2      superscript  h  normal-H    subscript  R  v   h      \mathrm{SNR}=\frac{|h^{\mathrm{H}}s|^{2}}{h^{\mathrm{H}}R_{v}h}.     We will rewrite this expression with some matrix manipulation. The reason for this seemingly counterproductive measure will become evident shortly. Exploiting the Hermitian symmetry of the covariance matrix    R  v     subscript  R  v    R_{v}   , we can write       SNR  =     |     (    R  v   1  /  2    h   )   H    (    R  v   -   1  /  2     s   )    |   2      (    R  v   1  /  2    h   )   H    (    R  v   1  /  2    h   )      ,      SNR     superscript       superscript     superscript   subscript  R  v     1  2    h   normal-H      superscript   subscript  R  v       1  2     s     2      superscript     superscript   subscript  R  v     1  2    h   normal-H      superscript   subscript  R  v     1  2    h       \mathrm{SNR}=\frac{|{(R_{v}^{1/2}h)}^{\mathrm{H}}(R_{v}^{-1/2}s)|^{2}}{{(R_{v}%
 ^{1/2}h)}^{\mathrm{H}}(R_{v}^{1/2}h)},     We would like to find an upper bound on this expression. To do so, we first recognize a form of the Cauchy-Schwarz inequality :         |    a  H   b   |   2   ≤    (    a  H   a   )    (    b  H   b   )     ,       superscript       superscript  a  normal-H   b    2        superscript  a  normal-H   a      superscript  b  normal-H   b      \ |a^{\mathrm{H}}b|^{2}\leq(a^{\mathrm{H}}a)(b^{\mathrm{H}}b),\,     which is to say that the square of the inner product of two vectors can only be as large as the product of the individual inner products of the vectors. This concept returns to the intuition behind the matched filter: this upper bound is achieved when the two vectors   a   a   a   and   b   b   b   are parallel. We resume our derivation by expressing the upper bound on our   SNR   SNR   \mathrm{SNR}   in light of the geometric inequality above:       SNR  =     |     (    R  v   1  /  2    h   )   H    (    R  v   -   1  /  2     s   )    |   2      (    R  v   1  /  2    h   )   H    (    R  v   1  /  2    h   )     ≤     [     (    R  v   1  /  2    h   )   H    (    R  v   1  /  2    h   )    ]    [     (    R  v   -   1  /  2     s   )   H    (    R  v   -   1  /  2     s   )    ]       (    R  v   1  /  2    h   )   H    (    R  v   1  /  2    h   )      .        SNR     superscript       superscript     superscript   subscript  R  v     1  2    h   normal-H      superscript   subscript  R  v       1  2     s     2      superscript     superscript   subscript  R  v     1  2    h   normal-H      superscript   subscript  R  v     1  2    h               delimited-[]     superscript     superscript   subscript  R  v     1  2    h   normal-H      superscript   subscript  R  v     1  2    h      delimited-[]     superscript     superscript   subscript  R  v       1  2     s   normal-H      superscript   subscript  R  v       1  2     s         superscript     superscript   subscript  R  v     1  2    h   normal-H      superscript   subscript  R  v     1  2    h        \mathrm{SNR}=\frac{|{(R_{v}^{1/2}h)}^{\mathrm{H}}(R_{v}^{-1/2}s)|^{2}}{{(R_{v}%
 ^{1/2}h)}^{\mathrm{H}}(R_{v}^{1/2}h)}\leq\frac{\left[{(R_{v}^{1/2}h)}^{\mathrm%
 {H}}(R_{v}^{1/2}h)\right]\left[{(R_{v}^{-1/2}s)}^{\mathrm{H}}(R_{v}^{-1/2}s)%
 \right]}{{(R_{v}^{1/2}h)}^{\mathrm{H}}(R_{v}^{1/2}h)}.     Our valiant matrix manipulation has now paid off. We see that the expression for our upper bound can be greatly simplified:       SNR  =     |     (    R  v   1  /  2    h   )   H    (    R  v   -   1  /  2     s   )    |   2      (    R  v   1  /  2    h   )   H    (    R  v   1  /  2    h   )     ≤    s  H    R  v   -  1    s    .        SNR     superscript       superscript     superscript   subscript  R  v     1  2    h   normal-H      superscript   subscript  R  v       1  2     s     2      superscript     superscript   subscript  R  v     1  2    h   normal-H      superscript   subscript  R  v     1  2    h             superscript  s  normal-H    superscript   subscript  R  v     1    s      \mathrm{SNR}=\frac{|{(R_{v}^{1/2}h)}^{\mathrm{H}}(R_{v}^{-1/2}s)|^{2}}{{(R_{v}%
 ^{1/2}h)}^{\mathrm{H}}(R_{v}^{1/2}h)}\leq s^{\mathrm{H}}R_{v}^{-1}s.     We can achieve this upper bound if we choose,         R   v   1  /  2    h   =   α   R  v   -   1  /  2     s          superscript   subscript  R  v     1  2    h     α   superscript   subscript  R  v       1  2     s     \ R_{v}^{1/2}h=\alpha R_{v}^{-1/2}s     where   α   α   \alpha   is an arbitrary real number. To verify this, we plug into our expression for the output   SNR   SNR   \mathrm{SNR}   :       SNR  =     |     (    R  v   1  /  2    h   )   H    (    R  v   -   1  /  2     s   )    |   2      (    R  v   1  /  2    h   )   H    (    R  v   1  /  2    h   )     =     α  2     |     (    R  v   -   1  /  2     s   )   H    (    R  v   -   1  /  2     s   )    |   2      α  2     (    R  v   -   1  /  2     s   )   H    (    R  v   -   1  /  2     s   )     =     |    s  H    R  v   -  1    s   |   2     s  H    R  v   -  1    s    =    s  H    R  v   -  1    s    .        SNR     superscript       superscript     superscript   subscript  R  v     1  2    h   normal-H      superscript   subscript  R  v       1  2     s     2      superscript     superscript   subscript  R  v     1  2    h   normal-H      superscript   subscript  R  v     1  2    h               superscript  α  2    superscript       superscript     superscript   subscript  R  v       1  2     s   normal-H      superscript   subscript  R  v       1  2     s     2       superscript  α  2    superscript     superscript   subscript  R  v       1  2     s   normal-H      superscript   subscript  R  v       1  2     s             superscript       superscript  s  normal-H    superscript   subscript  R  v     1    s    2      superscript  s  normal-H    superscript   subscript  R  v     1    s            superscript  s  normal-H    superscript   subscript  R  v     1    s      \mathrm{SNR}=\frac{|{(R_{v}^{1/2}h)}^{\mathrm{H}}(R_{v}^{-1/2}s)|^{2}}{{(R_{v}%
 ^{1/2}h)}^{\mathrm{H}}(R_{v}^{1/2}h)}=\frac{\alpha^{2}|{(R_{v}^{-1/2}s)}^{%
 \mathrm{H}}(R_{v}^{-1/2}s)|^{2}}{\alpha^{2}{(R_{v}^{-1/2}s)}^{\mathrm{H}}(R_{v%
 }^{-1/2}s)}=\frac{|s^{\mathrm{H}}R_{v}^{-1}s|^{2}}{s^{\mathrm{H}}R_{v}^{-1}s}=%
 s^{\mathrm{H}}R_{v}^{-1}s.     Thus, our optimal matched filter is        h   =   α   R  v   -  1    s    .      h    α   superscript   subscript  R  v     1    s     \ h=\alpha R_{v}^{-1}s.     We often choose to normalize the expected value of the power of the filter output due to the noise to unity. That is, we constrain        E    {    |   y  v   |   2   }    =   1.         E    superscript     subscript  y  v    2     1.    \ E\{|y_{v}|^{2}\}=1.\,     This constraint implies a value of   α   α   \alpha   , for which we can solve:         E    {    |   y  v   |   2   }    =    α  2    s  H    R  v   -  1    s   =  1   ,          E    superscript     subscript  y  v    2        superscript  α  2    superscript  s  normal-H    superscript   subscript  R  v     1    s        1     \ E\{|y_{v}|^{2}\}=\alpha^{2}s^{\mathrm{H}}R_{v}^{-1}s=1,     yielding        α   =   1     s  H    R  v   -  1    s      ,      α    1       superscript  s  normal-H    superscript   subscript  R  v     1    s       \ \alpha=\frac{1}{\sqrt{s^{\mathrm{H}}R_{v}^{-1}s}},     giving us our normalized filter,        h   =    1     s  H    R  v   -  1    s      R  v   -  1    s    .      h      1       superscript  s  normal-H    superscript   subscript  R  v     1    s      superscript   subscript  R  v     1    s     \ h=\frac{1}{\sqrt{s^{\mathrm{H}}R_{v}^{-1}s}}R_{v}^{-1}s.     If we care to write the impulse response of the filter for the convolution system, it is simply the complex conjugate time reversal of   h   h   h   .  Though we have derived the matched filter in discrete time, we can extend the concept to continuous-time systems if we replace    R  v     subscript  R  v    R_{v}   with the continuous-time autocorrelation function of the noise, assuming a continuous signal    s   (  t  )       s  t    s(t)   , continuous noise    v   (  t  )       v  t    v(t)   , and a continuous filter    h   (  t  )       h  t    h(t)   .  Alternative derivation of the matched filter  Alternatively, we may solve for the matched filter by solving our maximization problem with a Lagrangian. Again, the matched filter endeavors to maximize the output signal-to-noise ratio (   SNR   SNR   \mathrm{SNR}   ) of a filtered deterministic signal in stochastic additive noise. The observed sequence, again, is        x   =   s  +  v    ,      x    s  v     \ x=s+v,\,     with the noise covariance matrix,         R   v   =   E   {   v   v  H    }     .       subscript  R  v     E     v   superscript  v  normal-H        \ R_{v}=E\{vv^{\mathrm{H}}\}.\,     The signal-to-noise ratio is       SNR  =     |   y  s   |   2    E   {    |   y  v   |   2   }      .      SNR     superscript     subscript  y  s    2     E    superscript     subscript  y  v    2        \mathrm{SNR}=\frac{|y_{s}|^{2}}{E\{|y_{v}|^{2}\}}.     Evaluating the expression in the numerator, we have         |   y  s   |   2   =    y  s    H    y  s    =    h  H   s   s  H   h    .         superscript     subscript  y  s    2      superscript   subscript  y  s   normal-H    subscript  y  s            superscript  h  normal-H   s   superscript  s  normal-H   h      \ |y_{s}|^{2}={y_{s}}^{\mathrm{H}}y_{s}=h^{\mathrm{H}}ss^{\mathrm{H}}h.\,     and in the denominator,         E    {    |   y  v   |   2   }    =   E   {    y  v    H    y  v    }    =   E   {    h  H   v   v  H   h   }    =    h  H    R  v   h    .          E    superscript     subscript  y  v    2       E      superscript   subscript  y  v   normal-H    subscript  y  v             E      superscript  h  normal-H   v   superscript  v  normal-H   h             superscript  h  normal-H    subscript  R  v   h      \ E\{|y_{v}|^{2}\}=E\{{y_{v}}^{\mathrm{H}}y_{v}\}=E\{h^{\mathrm{H}}vv^{\mathrm%
 {H}}h\}=h^{\mathrm{H}}R_{v}h.\,     The signal-to-noise ratio becomes       SNR  =     h  H   s   s  H   h     h  H    R  v   h     .      SNR       superscript  h  normal-H   s   superscript  s  normal-H   h      superscript  h  normal-H    subscript  R  v   h      \mathrm{SNR}=\frac{h^{\mathrm{H}}ss^{\mathrm{H}}h}{h^{\mathrm{H}}R_{v}h}.     If we now constrain the denominator to be 1, the problem of maximizing   SNR   SNR   \mathrm{SNR}   is reduced to maximizing the numerator. We can then formulate the problem using a Lagrange multiplier :         h   H    R  v   h   =  1         superscript  h  normal-H    subscript  R  v   h   1    \ h^{\mathrm{H}}R_{v}h=1          ℒ   =     h  H   s   s  H   h   +   λ   (   1  -    h  H    R  v   h    )         ℒ       superscript  h  normal-H   s   superscript  s  normal-H   h     λ    1     superscript  h  normal-H    subscript  R  v   h        \ \mathcal{L}=h^{\mathrm{H}}ss^{\mathrm{H}}h+\lambda(1-h^{\mathrm{H}}R_{v}h)           ∇   h  *    ℒ   =    s   s  H   h   -   λ   R  v   h    =  0          subscript  normal-∇   superscript  h     ℒ       s   superscript  s  normal-H   h     λ   subscript  R  v   h         0     \ \nabla_{h^{*}}\mathcal{L}=ss^{\mathrm{H}}h-\lambda R_{v}h=0           (   s   s  H    )   h   =   λ   R  v   h           s   superscript  s  normal-H    h     λ   subscript  R  v   h     \ (ss^{\mathrm{H}})h=\lambda R_{v}h     which we recognize as a generalized eigenvalue problem          h   H    (   s   s  H    )   h   =   λ   h  H    R  v   h    .         superscript  h  normal-H     s   superscript  s  normal-H    h     λ   superscript  h  normal-H    subscript  R  v   h     \ h^{\mathrm{H}}(ss^{\mathrm{H}})h=\lambda h^{\mathrm{H}}R_{v}h.     Since    s   s  H       s   superscript  s  normal-H     ss^{\mathrm{H}}   is of unit rank, it has only one nonzero eigenvalue. It can be shown that this eigenvalue equals         λ   max   =    s  H    R  v   -  1    s    ,       subscript  λ       superscript  s  normal-H    superscript   subscript  R  v     1    s     \ \lambda_{\max}=s^{\mathrm{H}}R_{v}^{-1}s,     yielding the following optimal matched filter        h   =    1     s  H    R  v   -  1    s      R  v   -  1    s    .      h      1       superscript  s  normal-H    superscript   subscript  R  v     1    s      superscript   subscript  R  v     1    s     \ h=\frac{1}{\sqrt{s^{\mathrm{H}}R_{v}^{-1}s}}R_{v}^{-1}s.     This is the same result found in the previous section.  The matched filter as a least squares estimator  Matched filtering can also be interpreted as a least squares estimator for the optimal location and scaling of a given model or template. Once again, let the observed sequence be defined as         x   k   =    s  k   +   v  k     ,       subscript  x  k      subscript  s  k    subscript  v  k      \ x_{k}=s_{k}+v_{k},\,     where    v  k     subscript  v  k    v_{k}   is uncorrelated zero mean noise. The signal    s  k     subscript  s  k    s_{k}   is assumed to be a scaled and shifted version of a known model sequence    f  k     subscript  f  k    f_{k}   :        s   k   =    μ  0   ⋅   f   k  -   j  0           subscript  s  k    normal-⋅   subscript  μ  0    subscript  f    k   subscript  j  0        \ s_{k}=\mu_{0}\cdot f_{k-j_{0}}     We want to find optimal estimates    j  *     superscript  j     j^{*}   and    μ  *     superscript  μ     \mu^{*}   for the unknown shift    j  0     subscript  j  0    j_{0}   and scaling    μ  0     subscript  μ  0    \mu_{0}   by minimizing the least squares residual between the observed sequence    x  k     subscript  x  k    x_{k}   and a "probing sequence"    h   j  -  k      subscript  h    j  k     h_{j-k}   :         j   *   ,   μ  *    =    arg   min   j  ,  μ       ∑  k     (    x  k   -   μ  ⋅   h   j  -  k      )   2           superscript  j     superscript  μ          subscript    j  μ       subscript   k    superscript     subscript  x  k    normal-⋅  μ   subscript  h    j  k      2       \ j^{*},\mu^{*}=\arg\min_{j,\mu}\sum_{k}\left(x_{k}-\mu\cdot h_{j-k}\right)^{2}     The appropriate    h   j  -  k      subscript  h    j  k     h_{j-k}   will later turn out to be the matched filter, but is as yet unspecified. Expanding    x  k     subscript  x  k    x_{k}   and the square within the sum yields         j   *   ,   μ  *    =   arg    min   j  ,  μ     [      ∑  k     (    s  k   +   v  k    )   2    +    μ  2     ∑  k    h   j  -  k   2      -   2  μ    ∑  k     s  k    h   j  -  k       -   2  μ    ∑  k     v  k    h   j  -  k        ]           superscript  j     superscript  μ         subscript    j  μ          subscript   k    superscript     subscript  s  k    subscript  v  k    2       superscript  μ  2     subscript   k    superscript   subscript  h    j  k    2        2  μ    subscript   k      subscript  s  k    subscript  h    j  k         2  μ    subscript   k      subscript  v  k    subscript  h    j  k            \ j^{*},\mu^{*}=\arg\min_{j,\mu}\left[\sum_{k}(s_{k}+v_{k})^{2}+\mu^{2}\sum_{k%
 }h_{j-k}^{2}-2\mu\sum_{k}s_{k}h_{j-k}-2\mu\sum_{k}v_{k}h_{j-k}\right]   .  The first term in brackets is a constant (since the observed signal is given) and has no influence on the optimal solution. The last term has constant expected value because the noise is uncorrelated and has zero mean. We can therefore drop both terms from the optimization. After reversing the sign, we obtain the equivalent optimization problem         j   *   ,   μ  *    =   arg    max   j  ,  μ     [    2  μ    ∑  k     s  k    h   j  -  k       -    μ  2     ∑  k    h   j  -  k   2      ]           superscript  j     superscript  μ         subscript    j  μ        2  μ    subscript   k      subscript  s  k    subscript  h    j  k          superscript  μ  2     subscript   k    superscript   subscript  h    j  k    2          \ j^{*},\mu^{*}=\arg\max_{j,\mu}\left[2\mu\sum_{k}s_{k}h_{j-k}-\mu^{2}\sum_{k}%
 h_{j-k}^{2}\right]   .  Setting the derivative w.r.t.   μ   μ   \mu   to zero gives an analytic solution for    μ  *     superscript  μ     \mu^{*}   :        μ   *   =     ∑  k     s  k    h   j  -  k        ∑  k    h   j  -  k   2          superscript  μ        subscript   k      subscript  s  k    subscript  h    j  k        subscript   k    superscript   subscript  h    j  k    2       \ \mu^{*}=\frac{\sum_{k}s_{k}h_{j-k}}{\sum_{k}h_{j-k}^{2}}   .  Inserting this into our objective function yields a reduced maximization problem for just    j  *     superscript  j     j^{*}   :        j   *   =   arg    max  j      (    ∑  k     s  k    h   j  -  k      )   2     ∑  k    h   j  -  k   2            superscript  j        subscript   j      superscript    subscript   k      subscript  s  k    subscript  h    j  k      2     subscript   k    superscript   subscript  h    j  k    2         \ j^{*}=\arg\max_{j}\frac{\left(\sum_{k}s_{k}h_{j-k}\right)^{2}}{\sum_{k}h_{j-%
 k}^{2}}   .  The numerator can be upper-bounded by means of the Cauchy-Schwarz inequality :          (    ∑  k     s  k    h   j  -  k      )   2     ∑  k    h   j  -  k   2      ≤     ∑  k     s  k  2   ⋅    ∑  k    h   j  -  k   2        ∑  k    h   j  -  k   2     =    ∑  k    s  k  2    =  const           superscript    subscript   k      subscript  s  k    subscript  h    j  k      2     subscript   k    superscript   subscript  h    j  k    2         subscript   k    normal-⋅   superscript   subscript  s  k   2     subscript   k    superscript   subscript  h    j  k    2        subscript   k    superscript   subscript  h    j  k    2            subscript   k    superscript   subscript  s  k   2         const     \ \frac{\left(\sum_{k}s_{k}h_{j-k}\right)^{2}}{\sum_{k}h_{j-k}^{2}}\leq\frac{%
 \sum_{k}s_{k}^{2}\cdot\sum_{k}h_{j-k}^{2}}{\sum_{k}h_{j-k}^{2}}=\sum_{k}s_{k}^%
 {2}=\mathrm{const}   .  The optimization problem assumes its maximum when equality holds in this expression. According to the properties of the Cauchy-Schwarz inequality, this is only possible when        h    j  -  k    =   ν  ⋅   s  k    =   κ  ⋅   f   k  -   j  0             subscript  h    j  k     normal-⋅  ν   subscript  s  k          normal-⋅  κ   subscript  f    k   subscript  j  0         \ h_{j-k}=\nu\cdot s_{k}=\kappa\cdot f_{k-j_{0}}   .  for arbitrary non-zero constants   ν   ν   \nu   or   κ   κ   \kappa   , and the optimal solution is obtained at     j  *   =   j  0        superscript  j     subscript  j  0     j^{*}=j_{0}   as desired. Thus, our "probing sequence"    h   j  -  k      subscript  h    j  k     h_{j-k}   must be proportional to the signal model    f   k  -   j  0       subscript  f    k   subscript  j  0      f_{k-j_{0}}   , and the convenient choice    κ  =  1      κ  1    \kappa=1   yields the matched filter        h   k   =   f   -  k         subscript  h  k    subscript  f    k      \ h_{k}=f_{-k}   .  Note that the filter is the mirrored signal model. This ensures that the operation     ∑  k     x  k    h   j  -  k         subscript   k      subscript  x  k    subscript  h    j  k       \sum_{k}x_{k}h_{j-k}   to be applied in order to find the optimum is indeed the convolution between the observed sequence    x  k     subscript  x  k    x_{k}   and the matched filter    h  k     subscript  h  k    h_{k}   . The filtered sequence assumes its maximum at the position where the observed sequence    x  k     subscript  x  k    x_{k}   best matches (in a least-squares sense) the signal model    f  k     subscript  f  k    f_{k}   .  Frequency-domain interpretation  When viewed in the frequency domain, it is evident that the matched filter applies the greatest weighting to spectral components that have the greatest signal-to-noise ratio. Although in general this requires a non-flat frequency response, the associated distortion is not significant in situations such as radar and digital communications , where the original waveform is known and the objective is to detect the presence of this signal against the background noise.  Example of matched filter in radar and sonar  Matched filters are often used in signal detection 2 (see detection theory ). As an example, suppose that we wish to judge the distance of an object by reflecting a signal off it. We may choose to transmit a pure-tone sinusoid at 1 Hz. We assume that our received signal is an attenuated and phase-shifted form of the transmitted signal with added noise.  To judge the distance of the object, we correlate the received signal with a matched filter, which, in the case of white (uncorrelated) noise , is another pure-tone 1-Hz sinusoid. When the output of the matched filter system exceeds a certain threshold, we conclude with high probability that the received signal has been reflected off the object. Using the speed of propagation and the time that we first observe the reflected signal, we can estimate the distance of the object. If we change the shape of the pulse in a specially-designed way, the signal-to-noise ratio and the distance resolution can be even improved after matched filtering: this is a technique known as pulse compression .  Additionally, matched filters can be used in parameter estimation problems (see estimation theory ). To return to our previous example, we may desire to estimate the speed of the object, in addition to its position. To exploit the Doppler effect , we would like to estimate the frequency of the received signal. To do so, we may correlate the received signal with several matched filters of sinusoids at varying frequencies. The matched filter with the highest output will reveal, with high probability, the frequency of the reflected signal and help us determine the speed of the object. This method is, in fact, a simple version of the discrete Fourier transform (DFT) . The DFT takes an   N   N   N   -valued complex input and correlates it with   N   N   N   matched filters, corresponding to complex exponentials at   N   N   N   different frequencies, to yield   N   N   N   complex-valued numbers corresponding to the relative amplitudes and phases of the sinusoidal components (see Moving target indication ).  Example of matched filter in digital communications  The matched filter is also used in communications. In the context of a communication system that sends binary messages from the transmitter to the receiver across a noisy channel, a matched filter can be used to detect the transmitted pulses in the noisy received signal.  (Figure)  Matched Filter Total System.jpg   Imagine we want to send the sequence "0101100100" coded in non polar Non-return-to-zero (NRZ) through a certain channel.  Mathematically, a sequence in NRZ code can be described as a sequence of unit pulses or shifted rect functions , each pulse being weighted by +1 if the bit is "1" and by 0 if the bit is "0". Formally, the scaling factor for the    k  th     superscript  k  th    k^{\mathrm{th}}   bit is,        a   k   =   {      1  ,       if bit  k  is 1   ,        0  ,       if bit  k  is 0   .            subscript  a  k    cases  1    if bit  k  is 1   0    if bit  k  is 0      \ a_{k}=\begin{cases}1,&\mbox{if bit }k\mbox{ is 1},\\
 0,&\mbox{if bit }k\mbox{ is 0}.\end{cases}     We can represent our message,    M   (  t  )       M  t    M(t)   , as the sum of shifted unit pulses:         M    (  t  )    =    ∑   k  =   -  ∞    ∞      a  k   ×  Π    (    t  -   k  T    T   )      .        M  t     superscript   subscript     k               subscript  a  k   normal-Π       t    k  T    T       \ M(t)=\sum_{k=-\infty}^{\infty}a_{k}\times\Pi\left(\frac{t-kT}{T}\right).     where   T   T   T   is the time length of one bit.  Thus, the signal to be sent by the transmitter is  (Figure)  Original message.svg   If we model our noisy channel as an AWGN channel, white Gaussian noise is added to the signal. At the receiver end, for a Signal-to-noise ratio of 3dB, this may look like:  (Figure)  Received message.svg   A first glance will not reveal the original transmitted sequence. There is a high power of noise relative to the power of the desired signal (i.e., there is a low signal-to-noise ratio ). If the receiver were to sample this signal at the correct moments, the resulting binary message would possibly belie the original transmitted one.  To increase our signal-to-noise ratio, we pass the received signal through a matched filter. In this case, the filter should be matched to an NRZ pulse (equivalent to a "1" coded in NRZ code). Precisely, the impulse response of the ideal matched filter, assuming white (uncorrelated) noise should be a time-reversed complex-conjugated scaled version of the signal that we are seeking. We choose         h    (  t  )    =   Π   (   t  T   )     .        h  t     normal-Π    t  T      \ h(t)=\Pi\left(\frac{t}{T}\right).     In this case, due to symmetry, the time-reversed complex conjugate of    h   (  t  )       h  t    h(t)   is in fact    h   (  t  )       h  t    h(t)   , allowing us to call    h   (  t  )       h  t    h(t)   the impulse response of our matched filter convolution system.  After convolving with the correct matched filter, the resulting signal,     M  filtered    (  t  )        subscript  M  filtered   t    M_{\mathrm{filtered}}(t)   is,         M   filtered    (  t  )    =     M   (  t  )    *  h    (  t  )           subscript  M  filtered   t         M  t   h   t     \ M_{\mathrm{filtered}}(t)=M(t)*h(t)     where   *     *   denotes convolution.  (Figure)  Filtered message.svg   Which can now be safely sampled by the receiver at the correct sampling instants, and compared to an appropriate threshold, resulting in a correct interpretation of the binary message.  (Figure)  Filtered message threshold.svg   See also   Channel capacity  Noisy channel coding theorem   References   Melvin, Willian L. "A STAP Overview." IEEE Aerospace and Electronic Systems Magazine  19 (1) (January 2004): 19-35.  Turin, George L. "An introduction to matched filters." IRE Transactions on Information Theory  6 (3) (June 1960): 311- 329..      "  Category:Estimation theory  Category:Telecommunication theory  Category:Signal processing     http://cnx.org/content/m10141/latest/ ↩  Woodward P.M. Probability and Information Theory with Applications to Radar , Norwood, MA: Artech House, 1980. ↩     