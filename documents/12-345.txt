   Chebyshev center      Chebyshev center   In geometry , the Chebyshev center of a bounded set   Q   Q   Q   having non-empty interior is the center of the minimal-radius ball enclosing the entire set   Q   Q   Q   , or alternatively (and non-equivalently) the center of largest inscribed ball of   Q   Q   Q   . 1  In the field of parameter estimation , the Chebyshev center approach tries to find an estimator    x  ^     normal-^  x    \hat{x}   for   x   x   x   given the feasibility set   Q   Q   Q   , such that    x  ^     normal-^  x    \hat{x}   minimizes the worst possible estimation error for x (e.g. best worst case).  Mathematical representation  There exist several alternative representations for the Chebyshev center. Consider the set   Q   Q   Q   and denote its Chebyshev center by    x  ^     normal-^  x    \hat{x}   .    x  ^     normal-^  x    \hat{x}   can be computed by solving:       min    x  ^   ,  r     {   r  :      ∥    x  ^   -  x   ∥   2   ≤  r   ,    ∀  x   ∈  Q     }       subscript     normal-^  x   r     normal-:  r   formulae-sequence     superscript   norm     normal-^  x   x    2   r      for-all  x   Q       \min_{{\hat{x}},r}\left\{r:\left\|{\hat{x}}-x\right\|^{2}\leq r,\forall x\in Q\right\}     or alternatively by solving:          arg  min    x  ^     max   x  ∈  Q       ∥   x  -   x  ^    ∥   2    .       subscript       normal-^  x     subscript     x  Q      superscript   norm    x   normal-^  x     2     \operatorname*{\arg\min}_{\hat{x}}\max_{x\in Q}\left\|x-\hat{x}\right\|^{2}.    2  Despite these properties, finding the Chebyshev center may be a hard numerical optimization problem . For example, in the second representation above, the inner maximization is non-convex if the set Q is not convex .  Relaxed Chebyshev center  Let us consider the case in which the set   Q   Q   Q   can be represented as the intersection of   k   k   k   ellipsoids.       min   x  ^      max  x    {     ∥    x  ^   -  x   ∥   2   :      f  i    (  x  )    ≤  0   ,   0  ≤  i  ≤  k     }        subscript    normal-^  x      subscript   x    normal-:   superscript   norm     normal-^  x   x    2    formulae-sequence       subscript  f  i   x   0       0  i       k         \min_{\hat{x}}\max_{x}\left\{\left\|{\hat{x}}-x\right\|^{2}:f_{i}(x)\leq 0,0%
 \leq i\leq k\right\}   with          f  i    (  x  )    =     x  T    Q  i   x   +   2   g  i  T   x   +   d  i    ≤  0   ,   0  ≤  i  ≤  k    .     formulae-sequence         subscript  f  i   x        superscript  x  T    subscript  Q  i   x     2   superscript   subscript  g  i   T   x    subscript  d  i         0        0  i       k      f_{i}(x)=x^{T}Q_{i}x+2g_{i}^{T}x+d_{i}\leq 0,0\leq i\leq k.\,     By introducing an additional matrix variable    Δ  =   x   x  T        normal-Δ    x   superscript  x  T      \Delta=xx^{T}   , we can write the inner maximization problem of the Chebyshev center as:       min   x  ^      max    (  Δ  ,  x  )   ∈  G     {      ∥   x  ^   ∥   2   -   2    x  ^   T   x    +   Tr   (  Δ  )     }        subscript    normal-^  x      subscript      normal-Δ  x   G         superscript   norm   normal-^  x    2     2   superscript   normal-^  x   T   x     Tr  normal-Δ       \min_{\hat{x}}\max_{(\Delta,x)\in G}\left\{\left\|{\hat{x}}\right\|^{2}-2{\hat%
 {x}}^{T}x+\operatorname{Tr}(\Delta)\right\}   where    Tr   (  ⋅  )      Tr  normal-⋅    \operatorname{Tr}(\cdot)   is the trace operator and      G  =   {   (  Δ  ,  x  )   :       f  i    (  Δ  ,  x  )    ≤  0   ,   0  ≤  i  ≤  k    ,   Δ  =   x   x  T      }       G   conditional-set   normal-Δ  x    formulae-sequence   formulae-sequence       subscript  normal-f  i    normal-Δ  x    0       0  i       k       normal-Δ    x   superscript  x  T         G=\left\{(\Delta,x):{\rm{f}}_{i}(\Delta,x)\leq 0,0\leq i\leq k,\Delta=xx^{T}\right\}            f  i    (  Δ  ,  x  )    =    Tr   (    Q  i   Δ   )    +   2   g  i  T   x   +   d  i     .         subscript  f  i    normal-Δ  x       Tr     subscript  Q  i   normal-Δ      2   superscript   subscript  g  i   T   x    subscript  d  i      f_{i}(\Delta,x)=\operatorname{Tr}(Q_{i}\Delta)+2g_{i}^{T}x+d_{i}.     Relaxing our demand on   Δ   normal-Δ   \Delta   by demanding    Δ  ≥   x   x  T        normal-Δ    x   superscript  x  T      \Delta\geq xx^{T}   , i.e.     Δ  -   x   x  T     ∈   S  +         normal-Δ    x   superscript  x  T      subscript  S      \Delta-xx^{T}\in S_{+}   where    S  +     subscript  S     S_{+}   is the set of positive semi-definite matrices , and changing the order of the min max to max min (see the references for more details), the optimization problem can be formulated as:       R  C  C   =    max    (  Δ  ,  x  )   ∈  T     {    -    ∥  x  ∥   2    +   Tr   (  Δ  )     }          R  C  C     subscript      normal-Δ  x   T         superscript   norm  x   2     Tr  normal-Δ       RCC=\max_{(\Delta,x)\in{T}}\left\{-\left\|x\right\|^{2}+\operatorname{Tr}(%
 \Delta)\right\}   with       T  =   {   (  Δ  ,  x  )   :       f  i    (  Δ  ,  x  )    ≤  0   ,   0  ≤  i  ≤  k    ,   Δ  ≥   xx  T     }    .      T   conditional-set   normal-Δ  x    formulae-sequence   formulae-sequence       subscript  normal-f  normal-i    normal-Δ  normal-x    0       0  normal-i       normal-k       normal-Δ   superscript  xx  normal-T        {T}=\left\{(\Delta,x):\rm{f}_{i}(\Delta,x)\leq 0,0\leq i\leq k,\Delta\geq xx^{%
 T}\right\}.     This last convex optimization problem is known as the relaxed Chebyshev center (RCC). The RCC has the following important properties:   The RCC is an upper bound for the exact Chebyshev center.  The RCC is unique.  The RCC is feasible.   Constrained least squares  It can be shown that the well-known constrained least squares (CLS) problem is a relaxed version of the Chebyshev center.  The original CLS problem can be formulated as:        x  ^    C  L  S    =     arg  min    x  ∈  C      ∥   y  -   A  x    ∥   2         subscript   normal-^  x     C  L  S      subscript        x  C     superscript   norm    y    A  x     2      {\hat{x}}_{CLS}=\operatorname*{\arg\min}_{x\in C}\left\|y-Ax\right\|^{2}   with      C  =   {  x  :      f  i    (  x  )    =     x  T    Q  i   x   +   2   g  i  T   x   +   d  i    ≤  0   ,   1  ≤  i  ≤  k    }       C   conditional-set  x   formulae-sequence         subscript  f  i   x        superscript  x  T    subscript  Q  i   x     2   superscript   subscript  g  i   T   x    subscript  d  i         0        1  i       k        {C}=\left\{x:f_{i}(x)=x^{T}Q_{i}x+2g_{i}^{T}x+d_{i}\leq 0,1\leq i\leq k\right\}            Q  i   ≥  0   ,     g  i   ∈   R  m    ,    d  i   ∈  R     .     formulae-sequence     subscript  Q  i   0    formulae-sequence     subscript  g  i    superscript  R  m       subscript  d  i   R      Q_{i}\geq 0,g_{i}\in R^{m},d_{i}\in R.     It can be shown that this problem is equivalent to the following optimization problem:       max    (  Δ  ,  x  )   ∈  V     {    -    ∥  x  ∥   2    +   Tr   (  Δ  )     }       subscript      normal-Δ  x   V         superscript   norm  x   2     Tr  normal-Δ      \max_{(\Delta,{{x}})\in{V}}\left\{{-\left\|{{x}}\right\|^{2}+\operatorname{Tr}%
 (\Delta)}\right\}   with       V  =   {       (  Δ  ,  x  )   :   x  ∈  C              Tr   (    A  T   A  Δ   )    -   2   y  T    A  T   x    +    ∥  y  ∥   2    -  ρ   ≤  0   ,   Δ  ≥   xx  T        }    .      V      normal-:   normal-Δ  x     x  C       formulae-sequence           Tr     superscript  A  T   A  normal-Δ      2   superscript  y  T    superscript  A  T   x     superscript   norm  y   2    ρ   0     normal-Δ   superscript  xx  normal-T          V=\left\{\begin{array}[]{c}(\Delta,x):x\in C\\
 \operatorname{Tr}(A^{T}A\Delta)-2y^{T}A^{T}x+\left\|y\right\|^{2}-\rho\leq 0,%
 \rm{}\Delta\geq xx^{T}\\
 \end{array}\right\}.     One can see that this problem is a relaxation of the Chebyshev center (though different than the RCC described above).  RCC vs. CLS  A solution set    (  x  ,  Δ  )     x  normal-Δ    (x,\Delta)   for the RCC is also a solution for the CLS, and thus    T  ∈  V      T  V    T\in V   . This means that the CLS estimate is the solution of a looser relaxation than that of the RCC. Hence the CLS is an upper bound for the RCC , which is an upper bound for the real Chebyshev center.  Modeling constraints  Since both the RCC and CLS are based upon relaxation of the real feasibility set   Q   Q   Q   , the form in which   Q   Q   Q   is defined affects its relaxed versions. This of course affects the quality of the RCC and CLS estimators. As a simple example consider the linear box constraints:      l  ≤    a  T   x   ≤  u        l     superscript  a  T   x        u     l\leq a^{T}x\leq u   which can alternatively be written as        (     a  T   x   -  l   )    (     a  T   x   -  u   )    ≤  0.             superscript  a  T   x   l        superscript  a  T   x   u    0.    (a^{T}x-l)(a^{T}x-u)\leq 0.   It turns out that the first representation results with an upper bound estimator for the second one, hence using it may dramatically decrease the quality of the calculated estimator.  This simple example shows us that great care should be given to the formulation of constraints when relaxation of the feasibility region is used.  Linear programming problem  This problem can be formulated as a Linear Programming problem. 3  See also   Bounding sphere  Smallest-circle problem  Centre (geometry)  Centroid   References   Y. C. Eldar, A. Beck, and M. Teboulle, "A Minimax Chebyshev Estimator for Bounded Error Estimation," IEEE Trans. Signal Processing, 56(4): 1388–1397 (2007).  A. Beck and Y. C. Eldar, "Regularization in Regression with Bounded Noise: A Chebyshev Center Approach," SIAM J. Matrix Anal. Appl. 29 (2): 606–625 (2007).   "  Category:Estimation theory  Category:Geometric centers  Category:Mathematical optimization      ↩  http://www.ifor.math.ethz.ch/teaching/lectures/intro_ss11/Exercises/solutionEx11-12.pdf ↩     