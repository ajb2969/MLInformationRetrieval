   Square root of a matrix      Square root of a matrix   In mathematics , the square root of a matrix extends the notion of square root from numbers to matrices .  Matrix   B   B   B   is said to be a square root of   A   A   A   if the matrix product    B   B   B      B   B   B   is equal to   A   A   A   . 1  Properties  In general, a matrix can have several square roots. For example, the matrix    (     33    24      48    57     )      33  24    48  57     \left(\begin{smallmatrix}33&24\\
 48&57\end{smallmatrix}\right)   has square roots    (     1    4      8    5     )      1  4    8  5     \left(\begin{smallmatrix}1&4\\
 8&5\end{smallmatrix}\right)   and    (     5    2      4    7     )      5  2    4  7     \left(\begin{smallmatrix}5&2\\
 4&7\end{smallmatrix}\right)   , as well as their additive inverses .  Another example is the 2×2 identity matrix      (     1    0      0    1     )   ,      1  0    0  1     \bigl(\begin{smallmatrix}1&0\\
 0&1\end{smallmatrix}\bigr),   which has infinitely many symmetric rational square roots given by         1  t    (     s     r  ;       r     -  s      )    ,    1  t    (     s     -  r        -  r      -  s      )    ,    1  t    (      -  s      r  ;       r     s  ;      )    ,    1  t    (      -  s      -  r        -  r      s  ;      )    ,   (     1    0      0     ±  1      )   ,  and   (      -  1     0      0     ±  1      )    ,         1  t     s  r    r    s          1  t     s    r       r     s          1  t       s   r    r  s         1  t       s     r       r   s       1  0    0   plus-or-minus  1     and      1   0    0   plus-or-minus  1       \frac{1}{t}\left(\begin{matrix}s&r\\
 r&-s\end{matrix}\right),\quad\frac{1}{t}\left(\begin{matrix}s&-r\\
 -r&-s\end{matrix}\right),\quad\frac{1}{t}\left(\begin{matrix}-s&r\\
 r&s\end{matrix}\right),\quad\frac{1}{t}\left(\begin{matrix}-s&-r\\
 -r&s\end{matrix}\right),\quad\left(\begin{matrix}1&0\\
 0&\pm 1\end{matrix}\right),\quad\text{and}\quad\left(\begin{matrix}-1&0\\
 0&\pm 1\end{matrix}\right),     where    (  r  ,  s  ,  t  )     r  s  t    (r,s,t)   is any Pythagorean triple —that is, any set of positive integers such that      r  2   +   s  2    =   t  2          superscript  r  2    superscript  s  2     superscript  t  2     r^{2}+s^{2}=t^{2}   . 2  However, a positive-semidefinite matrix has precisely one positive-semidefinite square root, which can be called its principal square root .  While the square root of a nonnegative integer is either again an integer or an irrational number , in contrast an integer matrix can have a square root whose entries are rational, yet not integer. For example, the matrix    (      0     4       -  1     5     )      0  4      1   5     \left(\begin{smallmatrix}~{}\;0&4\\
 -1&5\end{smallmatrix}\right)   has the non-integer square root    (       2  3       4  3        -   1  3       7  3      )        2  3     4  3         1  3      7  3      \left(\begin{smallmatrix}~{}\;\frac{2}{3}&\frac{4}{3}\\
 -\frac{1}{3}&\frac{7}{3}\end{smallmatrix}\right)   as well as the integer square root matrix    (     2     -  4       1     -  3      )      2    4     1    3      \left(\begin{smallmatrix}2&-4\\
 1&-3\end{smallmatrix}\right)   . The 2×2 identity matrix is another example.  A 2×2 matrix with two distinct nonzero eigenvalues has four square roots.  More generally, an    n  ×  n      n  normal-×  n    n×n   matrix with   n   n   n    distinct nonzero eigenvalues has 2 n square roots. Such a matrix,   A   A   A   , has a decomposition where   V   V   V   is the matrix whose columns are eigenvectors of   A   A   A   and   D   D   D   is the diagonal matrix whose diagonal elements are the corresponding   n   n   n   eigenvalues . Thus the square roots of   A   A   A   are given by , where   D   D   D    ½ is any square root matrix of   D   D   D   , which, for distinct eigenvalues, must be diagonal with diagonal elements equal to square roots of the diagonal elements of   D   D   D   ; since there are two possible choices for a square root of each diagonal element of   D   D   D   , there are 2 n choices for the matrix   D   D   D    ½ .  This also leads to a proof of the above observation, that a positive-definite matrix has precisely one positive-definite square root: a positive definite matrix has only positive eigenvalues, and each of these eigenvalues has only one positive square root; and since the eigenvalues of the square root matrix are the diagonal elements of   D   D   D    ½ , for the square root matrix to be itself positive definite necessitates the use of only the unique positive square roots of the original eigenvalues.  Just as with the real numbers , a real matrix may fail to have a real square root, but have a square root with complex -valued entries.  Some matrices have no square root . An example is the matrix    (          0    1      0    0     )      absent    0  1    0  0     \left(\begin{smallmatrix}\\
 0&1\\
 0&0\end{smallmatrix}\right)   .  In general, a complex matrix with positive real eigenvalues has a unique square root with positive eigenvalues called the principal square root . Moreover, the operation of taking the principal square root is continuous on this set of matrices. If the matrix has real entries, then the square root also has real entries. These properties are consequences of the holomorphic functional calculus applied to matrices. The existence and uniqueness of the principal square root can be deduced directly from the Jordan normal form (see below). For analytic functions of matrices, see       For the holomorphic functional calculus, see:        Computation methods  Explicit formulas  For a 2 × 2 matrix, there are explicit formulas that give up to four square roots, if the matrix has any roots.  If   D   D   D   is a diagonal  n × n matrix, one can obtain a square root by taking a diagonal matrix   R   R   R   , where each element along the diagonal is a square root of the corresponding element of   D   D   D   . If the diagonal elements of D are real and non-negative, and the square roots are taken with non-negative sign, the matrix   R   R   R   will be the principal root of   D   D   D   .  If a matrix is idempotent , one of its square roots is the matrix itself.  By diagonalization  An n × n matrix   A   A   A   is diagonalizable if there is a matrix   V   V   V   and a diagonal matrix   D   D   D   such that  VDV −1 }} . This happens if and only if   A   A   A   has n  eigenvectors which constitute a basis for . In this case,   V   V   V   can be chosen to be the matrix with the n eigenvectors as columns, and thus a square root of   A   A   A   is       R  =   V  S    V   -  1       ,      R    V  S   superscript  V    1       R=VSV^{-1}~{},   where   S   S   S   is any square root of   D   D   D   . Indeed,         (   V   D   1  /  2     V   -  1     )   2   =   V   D   1  /  2     (    V   -  1    V   )    D   1  /  2     V   -  1     =   V  D   V   -  1     =   A    .         superscript    V   superscript  D    1  2     superscript  V    1     2     V   superscript  D    1  2       superscript  V    1    V    superscript  D    1  2     superscript  V    1            V  D   superscript  V    1          A     (VD^{1/2}V^{-1})^{2}=VD^{1/2}(V^{-1}V)D^{1/2}V^{-1}=VDV^{-1}=A~{}.     For example, the matrix    A  =   (          33    24      48    57     )       A    absent    33  24    48  57      A=\bigl(\begin{smallmatrix}\\
 33&24\\
 48&57\end{smallmatrix}\bigr)   can be diagonalized as , where      V  =   (          1     1       2     -  1      )       V    absent    1  1    2    1       V=\bigl(\begin{smallmatrix}\\
 1&~{}\;1\\
 2&-1\end{smallmatrix}\bigr)   and    D  =   (          81    0       0     9     )       D    absent    81  0    0  9      D=\bigl(\begin{smallmatrix}\\
 81&0\\
 ~{}\;0&9\end{smallmatrix}\bigr)   .     D   D   D   has principal square root       D   1  /  2    =   (          9    0      0    3     )        superscript  D    1  2      absent    9  0    0  3      D^{1/2}=\bigl(\begin{smallmatrix}\\
 9&0\\
 0&3\end{smallmatrix}\bigr)   , giving the square root       A   1  /  2    =   V   D   1  /  2     V   -  1     =   (          5    2      4    7     )          superscript  A    1  2      V   superscript  D    1  2     superscript  V    1            absent    5  2    4  7       A^{1/2}=VD^{1/2}V^{-1}=\bigl(\begin{smallmatrix}\\
 5&2\\
 4&7\end{smallmatrix}\bigr)   .  When   A   A   A   is symmetric, the diagonalizing matrix   V   V   V   can be made an orthogonal matrix by suitably choosing the eigenvectors (see spectral theorem ). Then the inverse of   V   V   V   is simply the transpose, so that       R  =   V  S    V  T      .      R    V  S   superscript  V  T      R=VSV^{T}~{}.     By Jordan decomposition  For non-diagonalizable matrices one can calculate the Jordan normal form followed by a series expansion, similar to the approach described in logarithm of a matrix .  To see that any complex matrix with positive eigenvalues has a square root of the same form, it suffices to check this for a Jordan block. Any such block has the form λ( I + N ) with λ > 0 and N nilpotent. If is the binomial expansion for the square root (valid in | z |  gives a square root of the Jordan block with eigenvalue    √  λ      normal-√  λ    √λ   .  It suffices to check uniqueness for a Jordan block with λ = 1. The square constructed above has the form S = I + L where L is polynomial in N without constant term. Any other square root T with positive eigenvalues has the form T = I + M with   M   M   M   nilpotent, commuting with N and hence L . But then . Since L and   M   M   M   commute, the matrix    L  +  M      L  M    L+M   is nilpotent and    I  +    (   L  +  M   )   /  2       I      L  M   2     I+(L+M)/2   is invertible with inverse given by a Neumann series . Hence L =   M   M   M   .  If   A   A   A   is a matrix with positive eigenvalues and minimal polynomial     p   (  t  )       p  t    p(t)   , then the Jordan decomposition into generalized eigenspaces of   A   A   A   can be deduced from the partial fraction expansion of . The corresponding projections onto the generalized eigenspaces are given by real polynomials in   A   A   A   . On each eigenspace,   A   A   A   has the form    λ   (   I  +  N   )       λ    I  N     λ(I+N)   as above. The power series expression for the square root on the eigenspace show that the principal square root of   A   A   A   has the form q ( A ) where q ( t ) is a polynomial with real coefficients.  By Denman–Beavers iteration  Another way to find the square root of an n × n matrix A is the Denman–Beavers square root iteration. 3  Let Y 0 = A and Z 0 = I , where I is the n × n  identity matrix . The iteration is defined by      Y   k  +  1      subscript  Y    k  1     \displaystyle Y_{k+1}     As this uses a pair of sequences of matrix inverses whose later elements change comparatively little, only the first elements have a high computational cost since the remainder can be computed from earlier elements with only a few passes of a variant of Newton's method for computing inverses ,        X   n  +  1    =    2   X  n    -    X  n   B   X  n      .       subscript  X    n  1        2   subscript  X  n       subscript  X  n   B   subscript  X  n       X_{n+1}=2X_{n}-X_{n}BX_{n}.     With this, for later values of   k   k   k   one would set     X  0   =   Z   k  -  1    -  1         subscript  X  0    superscript   subscript  Z    k  1      1      X_{0}=Z_{k-1}^{-1}   and     B  =   Z  k    ,      B   subscript  Z  k     B=Z_{k},   and then use     Z  k   -  1    =   X  n        superscript   subscript  Z  k     1     subscript  X  n     Z_{k}^{-1}=X_{n}   for some small n (perhaps just 1), and similarly for     Y  k   -  1    .     superscript   subscript  Y  k     1     Y_{k}^{-1}.     Convergence is not guaranteed, even for matrices that do have square roots, but if the process converges, the matrix    Y  k     subscript  Y  k    Y_{k}   converges quadratically to a square root   A   A   A    1/2 , while    Z  k     subscript  Z  k    Z_{k}   converges to its inverse,   A   A   A    −1/2 .  By the Babylonian method  Yet another iterative method is obtained by taking the well-known formula of the Babylonian method for computing the square root of a real number, and applying it to matrices. Let X 0 = I , where I is the identity matrix . The iteration is defined by        X   k  +  1    =     1  2     (    X  k   +   A   X  k   -  1      )     .       subscript  X    k  1        1  2      subscript  X  k     A   superscript   subscript  X  k     1         X_{k+1}=\tfrac{1}{2}(X_{k}+AX_{k}^{-1})\,.   Again, convergence is not guaranteed, but if the process converges, the matrix    X  k     subscript  X  k    X_{k}   converges quadratically to a square root A 1/2 . Compared to Denman–Beavers iteration, an advantage of the Babylonian method is that only one matrix inverse need be computed per iteration step. On the other hand, as Denman–Beavers iteration uses a pair of sequences of matrix inverses whose later elements change comparatively little, only the first elements have a high computational cost since the remainder can be computed from earlier elements with only a few passes of a variant of Newton's method for computing inverses (see Denman–Beavers iteration above); of course, the same approach can be used to get the single sequence of inverses needed for the Babylonian method. However, unlike Denman–Beavers iteration, the Babylonian method is numerically unstable and more likely to fail to converge. 4  Square roots of positive operators  In linear algebra and operator theory , given a bounded  positive semidefinite operator (a non-negative operator) T on a complex Hilbert space, B is a square root of T if T = B* B , where B* denotes the Hermitian adjoint of B .  According to the spectral theorem , the continuous functional calculus can be applied to obtain an operator T ½ such that T ½ is itself positive and ( T ½ ) 2 = T . The operator T ½ is the unique non-negative square root of T .  A bounded non-negative operator on a complex Hilbert space is self adjoint by definition. So T = ( T ½ )* T ½ . Conversely, it is trivially true that every operator of the form B* B is non-negative. Therefore, an operator T is non-negative if and only if  T = B* B for some B (equivalently, T = CC* for some C ).  The Cholesky factorization provides another particular example of square root, which should not be confused with the unique non-negative square root.  Unitary freedom of square roots  If T is a non-negative operator on a finite-dimensional Hilbert space, then all square roots of T are related by unitary transformations. More precisely, if T = A*A = B*B , then there exists a unitary  U such that A = UB .  Indeed, take B = T ½ to be the unique non-negative square root of T . If T is strictly positive, then B is invertible, and so is unitary:       U  *   U       superscript  U    U    \displaystyle U^{*}U     If T is non-negative without being strictly positive, then the inverse of B cannot be defined, but the Moore–Penrose pseudoinverse  B + can be. In that case, the operator is a partial isometry , that is, a unitary operator from the range of T to itself. This can then be extended to a unitary operator U on the whole space by setting it equal to the identity on the kernel of T . More generally, this is true on an infinite-dimensional Hilbert space if, in addition, T has closed range . In general, if A , B are closed and densely defined operators on a Hilbert space H , and A* A = B* B , then A = UB where U is a partial isometry.  Some applications  Square roots, and the unitary freedom of square roots, have applications throughout functional analysis and linear algebra.  Polar decomposition  If A is an invertible operator on a finite-dimensional Hilbert space, then there is a unique unitary operator U and positive operator P such that       A  =   U  P    ;      A    U  P     A=UP;\,   this is the polar decomposition of A . The positive operator P is the unique positive square root of the positive operator A ∗ A , and U is defined by .  If A is not invertible, then it still has a polar composition in which P is defined in the same way (and is unique). The unitary operator U is not unique. Rather it is possible to determine a "natural" unitary operator as follows: AP + is a unitary operator from the range of A to itself, which can be extended by the identity on the kernel of A ∗ . The resulting unitary operator U then yields the polar decomposition of A .  Kraus operators  By Choi's result, a linear map      Φ  :    C   n  ×  n    →   C   m  ×  m        normal-:  normal-Φ   normal-→   superscript  C    n  n     superscript  C    m  m       \Phi:C^{n\times n}\rightarrow C^{m\times m}     is completely positive if and only if it is of the form       Φ   (  A  )    =    ∑  i  k     V  i   A   V  i  *           normal-Φ  A     superscript   subscript   i   k      subscript  V  i   A   superscript   subscript  V  i         \Phi(A)=\sum_{i}^{k}V_{i}AV_{i}^{*}     where k ≤ nm . Let { E p q } ⊂ C n × n be the n 2 elementary matrix units. The positive matrix       M  Φ   =    (   Φ   (   E   p  q    )    )    p  q    ∈   C     n  m   ×  n   m           subscript  M  normal-Φ    subscript    normal-Φ   subscript  E    p  q       p  q          superscript  C        n  m   n   m       M_{\Phi}=(\Phi(E_{pq}))_{pq}\in C^{nm\times nm}     is called the Choi matrix of Φ. The Kraus operators correspond to the, not necessarily square, square roots of M Φ : For any square root B of M Φ , one can obtain a family of Kraus operators V i by undoing the Vec operation to each column b i of B . Thus all sets of Kraus operators are related by partial isometries.  Mixed ensembles  In quantum physics, a density matrix for an n -level quantum system is an n × n complex matrix ρ that is positive semidefinite with trace 1. If ρ can be expressed as      ρ  =    ∑  i     p  i    v  i    v  i  *         ρ    subscript   i      subscript  p  i    subscript  v  i    superscript   subscript  v  i         \rho=\sum_{i}p_{i}v_{i}v_{i}^{*}     where ∑ p i = 1, the set      {   p  i   ,   v  i   }      subscript  p  i    subscript  v  i     \{p_{i},v_{i}\}\,     is said to be an ensemble that describes the mixed state ρ . Notice { v i } is not required to be orthogonal. Different ensembles describing the state ρ are related by unitary operators, via the square roots of ρ . For instance, suppose       ρ  =    ∑  j     a  j    a  j  *      .      ρ    subscript   j      subscript  a  j    superscript   subscript  a  j         \rho=\sum_{j}a_{j}a_{j}^{*}.     The trace 1 condition means        ∑  j     a  j  *    a  j     =  1.        subscript   j      superscript   subscript  a  j      subscript  a  j     1.    \sum_{j}a_{j}^{*}a_{j}=1.     Let        p  i   =    a  i  *    a  i     ,       subscript  p  i      superscript   subscript  a  i      subscript  a  i      p_{i}=a_{i}^{*}a_{i},     and v i be the normalized a i . We see that      {   p  i   ,   v  i   }      subscript  p  i    subscript  v  i     \{p_{i},v_{i}\}\,     gives the mixed state ρ .  Unscented Kalman Filter  In the Unscented Kalman Filter (UKF), 5 the square root of the state error covariance matrix is required for the unscented transform which is the statistical linearization method used. A comparison between different matrix square root calculation methods within a UKF application of GPS/INS sensor fusion was presented, which indicated that the Cholesky decomposition method was best suited for UKF applications. 6  See also   Matrix function  Holomorphic functional calculus  Logarithm of a matrix  Sylvester's formula  2 × 2 real matrices#Functions of 2 × 2 real matrices   Notes    References     , Chapter IV, Reisz functional calculus         "  Category:Matrix theory     ↩  Mitchell, Douglas W. "Using Pythagorean triples to generate square roots of I 2 ". The Mathematical Gazette 87, November 2003, 499-500. ↩  ; ↩   ↩  ↩     