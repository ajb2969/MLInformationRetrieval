   Forward algorithm      Forward algorithm   The forward algorithm , in the context of a hidden Markov model , is used to calculate a 'belief state': the probability of a state at a certain time, given the history of evidence. The process is also known as filtering . The forward algorithm is closely related to, but distinct from, the Viterbi algorithm .  For an HMM such as this one:  this probability is written as    P   (   x  t   |   y   1  :  t    )      fragments  P   fragments  normal-(   subscript  x  t   normal-|   subscript  y   normal-:  1  t    normal-)     P(x_{t}|y_{1:t})   . Here    x   (  t  )       x  t    x(t)   is the hidden state which is abbreviated as    x  t     subscript  x  t    x_{t}   and    y   1  :  t      subscript  y   normal-:  1  t     y_{1:t}   are the observations   1   1   1   to   t   t   t   . A belief state can be calculated at each time step, but doing this does not, in a strict sense, produce the most likely state sequence , but rather the most likely state at each time step, given the previous history.  Algorithm  The goal of the forward algorithm is to compute the joint probability     p   (   x  t   ,   y   1  :  t    )       p    subscript  x  t    subscript  y   normal-:  1  t       p(x_{t},y_{1:t})   , where for notational convenience we have abbreviated    x   (  t  )       x  t    x(t)   as    x  t     subscript  x  t    x_{t}   and    (   y   (  1  )    ,   y   (  2  )    ,  …  ,   y   (  t  )    )       y  1     y  2   normal-…    y  t     (y(1),y(2),...,y(t))   as    y   1  :  t      subscript  y   normal-:  1  t     y_{1:t}   . Computing    p   (   x  t   ,   y   1  :  t    )       p    subscript  x  t    subscript  y   normal-:  1  t       p(x_{t},y_{1:t})   directly would require marginalizing over all possible state sequences    {   x   1  :   t  -  1     }      subscript  x   normal-:  1    t  1       \{x_{1:t-1}\}   , the number of which grows exponentially with   t   t   t   . Instead, the forward algorithm takes advantage of the conditional independence rules of the hidden Markov model (HMM) to perform the calculation recursively.  To demonstrate the recursion, let          α  t    (   x  t   )    =   p   (   x  t   ,   y   1  :  t    )    =    ∑   x   t  -  1      p   (   x  t   ,   x   t  -  1    ,   y   1  :  t    )              subscript  α  t    subscript  x  t      p    subscript  x  t    subscript  y   normal-:  1  t             subscript    subscript  x    t  1       p    subscript  x  t    subscript  x    t  1     subscript  y   normal-:  1  t          \alpha_{t}(x_{t})=p(x_{t},y_{1:t})=\sum_{x_{t-1}}p(x_{t},x_{t-1},y_{1:t})   .     Using the chain rule to expand    p   (   x  t   ,   x   t  -  1    ,   y   1  :  t    )       p    subscript  x  t    subscript  x    t  1     subscript  y   normal-:  1  t       p(x_{t},x_{t-1},y_{1:t})   , we can then write         α  t    (   x  t   )   =   ∑   x   t  -  1     p   (   y  t   |   x  t   ,   x   t  -  1    ,   y   1  :   t  -  1     )   p   (   x  t   |   x   t  -  1    ,   y   1  :   t  -  1     )   p   (   x   t  -  1    ,   y   1  :   t  -  1     )      fragments   subscript  α  t    fragments  normal-(   subscript  x  t   normal-)     subscript    subscript  x    t  1     p   fragments  normal-(   subscript  y  t   normal-|   subscript  x  t   normal-,   subscript  x    t  1    normal-,   subscript  y   normal-:  1    t  1     normal-)   p   fragments  normal-(   subscript  x  t   normal-|   subscript  x    t  1    normal-,   subscript  y   normal-:  1    t  1     normal-)   p   fragments  normal-(   subscript  x    t  1    normal-,   subscript  y   normal-:  1    t  1     normal-)     \alpha_{t}(x_{t})=\sum_{x_{t-1}}p(y_{t}|x_{t},x_{t-1},y_{1:t-1})p(x_{t}|x_{t-1%
 },y_{1:t-1})p(x_{t-1},y_{1:t-1})   .     Because    y  t     subscript  y  t    y_{t}   is conditionally independent of everything but    x  t     subscript  x  t    x_{t}   , and    x  t     subscript  x  t    x_{t}   is conditionally independent of everything but    x   t  -  1      subscript  x    t  1     x_{t-1}   , this simplifies to         α  t    (   x  t   )   =  p   (   y  t   |   x  t   )    ∑   x   t  -  1     p   (   x  t   |   x   t  -  1    )    α   t  -  1     (   x   t  -  1    )      fragments   subscript  α  t    fragments  normal-(   subscript  x  t   normal-)    p   fragments  normal-(   subscript  y  t   normal-|   subscript  x  t   normal-)    subscript    subscript  x    t  1     p   fragments  normal-(   subscript  x  t   normal-|   subscript  x    t  1    normal-)    subscript  α    t  1     fragments  normal-(   subscript  x    t  1    normal-)     \alpha_{t}(x_{t})=p(y_{t}|x_{t})\sum_{x_{t-1}}p(x_{t}|x_{t-1})\alpha_{t-1}(x_{%
 t-1})   .     Thus, since    p   (   y  t   |   x  t   )      fragments  p   fragments  normal-(   subscript  y  t   normal-|   subscript  x  t   normal-)     p(y_{t}|x_{t})   and    p   (   x  t   |   x   t  -  1    )      fragments  p   fragments  normal-(   subscript  x  t   normal-|   subscript  x    t  1    normal-)     p(x_{t}|x_{t-1})   are given by the model's emission distributions and transition probabilities , one can quickly calculate     α  t    (   x  t   )        subscript  α  t    subscript  x  t     \alpha_{t}(x_{t})   from     α   t  -  1     (   x   t  -  1    )        subscript  α    t  1     subscript  x    t  1      \alpha_{t-1}(x_{t-1})   and avoid incurring exponential computation time.  The forward algorithm is easily modified to account for observations from variants of the hidden Markov model as well, such as the Markov jump linear system .  Smoothing  In order to take into account future history (i.e., if one wanted to improve the estimate for past times), you can run the backward algorithm, which complements the forward algorithm. This is called smoothing . The forward/backward algorithm computes    P   (   x  k   |   y   1  :  t    )      fragments  P   fragments  normal-(   subscript  x  k   normal-|   subscript  y   normal-:  1  t    normal-)     P(x_{k}|y_{1:t})   for    P   (   x  t   |   y   1  :  t    )      fragments  P   fragments  normal-(   subscript  x  t   normal-|   subscript  y   normal-:  1  t    normal-)     P(x_{t}|y_{1:t})   .  The difference between the state sequence that the Viterbi algorithm estimate generates and the state sequence that the forward algorithm generates is that the Viterbi algorithm recalculates the entire sequence with each new data point whereas the forward algorithm only appends the new current value to the previous sequence computed.  See also   Viterbi algorithm  Forward-backward algorithm   Further reading   Russell and Norvig's Artificial Intelligence, a Modern Approach , starting on page 541 of the 2003 edition, provides a succinct exposition of this and related topics   "  Category:Markov models   