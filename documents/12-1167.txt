   Auxiliary particle filter      Auxiliary particle filter   The auxiliary particle filter is a particle filtering algorithm introduced by Pitt and Shephard in 1999 to improve some deficiencies of the sequential importance resampling (SIR) algorithm when dealing with tailed observation densities.  Assume that the filtered posterior is described by the following M weighted samples:      p   (   x  t   |   z   1  :  t    )   ≈   ∑   i  =  1   M    ω  t   (  i  )    δ   (   x  t   -   x  t   (  i  )    )   .     fragments  p   fragments  normal-(   subscript  x  t   normal-|   subscript  z   normal-:  1  t    normal-)     superscript   subscript     i  1    M    subscript   superscript  ω  i   t   δ   fragments  normal-(   subscript  x  t     subscript   superscript  x  i   t   normal-)   normal-.    p(x_{t}|z_{1:t})\approx\sum_{i=1}^{M}\omega^{(i)}_{t}\delta\left(x_{t}-x^{(i)}%
 _{t}\right).     Then, each step in the algorithm consists of first drawing a sample of the particle index   k   k   k   which will be propagated from    t  -  1      t  1    t-1   into the new step   t   t   t   . These indexes are auxiliary variables only used as an intermediary step, hence the name of the algorithm. The indexes are drawn according to the likelihood of some reference point    μ  t   (  i  )      subscript   superscript  μ  i   t    \mu^{(i)}_{t}   which in some way is related to the transition model     x  t   |   x   t  -  1       fragments   subscript  x  t   normal-|   subscript  x    t  1      x_{t}|x_{t-1}   (for example, the mean, a sample, etc.):       k   (  i  )    ∼  P   (  i  =  k  |   z  t   )   ∝   ω  t   (  i  )    p   (   z  t   |   μ  t   (  i  )    )      fragments   superscript  k  i   similar-to  P   fragments  normal-(  i   k  normal-|   subscript  z  t   normal-)   proportional-to   subscript   superscript  ω  i   t   p   fragments  normal-(   subscript  z  t   normal-|   subscript   superscript  μ  i   t   normal-)     k^{(i)}\sim P(i=k|z_{t})\propto\omega^{(i)}_{t}p(z_{t}|\mu^{(i)}_{t})     This is repeated for    i  =   1  ,  2  ,  …  ,  M       i   1  2  normal-…  M     i=1,2,\dots,M   , and using these indexes we can now draw the conditional samples:       x  t   (  i  )    ∼  p   (  x  |   x   t  -  1    k   (  i  )     )   .     fragments   superscript   subscript  x  t   i   similar-to  p   fragments  normal-(  x  normal-|   subscript   superscript  x   superscript  k  i      t  1    normal-)   normal-.    x_{t}^{(i)}\sim p(x|x^{k^{(i)}}_{t-1}).     Finally, the weights are updated to account for the mismatch between the likelihood at the actual sample and the predicted point    μ  t   k   (  i  )       superscript   subscript  μ  t    superscript  k  i     \mu_{t}^{k^{(i)}}   :        ω  t   (  i  )    ∝    p   (   z  t   |   x  t   (  i  )    )     p   (   z  t   |   μ  t   k   (  i  )     )      .     proportional-to   superscript   subscript  ω  t   i      fragments  p   fragments  normal-(   subscript  z  t   normal-|   subscript   superscript  x  i   t   normal-)     fragments  p   fragments  normal-(   subscript  z  t   normal-|   subscript   superscript  μ   superscript  k  i    t   normal-)       \omega_{t}^{(i)}\propto\frac{p(z_{t}|x^{(i)}_{t})}{p(z_{t}|\mu^{k^{(i)}}_{t})}.     References     "  Category:Estimation theory  Category:Monte Carlo methods  Category:Computational statistics  Category:Nonlinear filters   