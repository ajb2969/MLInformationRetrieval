   Second-order co-occurrence pointwise mutual information      Second-order co-occurrence pointwise mutual information   Second-order co-occurrence pointwise mutual information is a semantic similarity measure using pointwise mutual information to sort lists of important neighbor words of the two target words from a large corpus. PMI-IR used AltaVista 's Advanced Search query syntax to calculate probabilities . Note that the ``NEAR" search operator of AltaVista is an essential operator in the PMI-IR method. However, it is no longer in use in AltaVista; this means that, from the implementation point of view, it is not possible to use the PMI-IR method in the same form in new systems. In any case, from the algorithmic point of view, the advantage of using SOC-PMI is that it can calculate the similarity between two words that do not co-occur frequently, because they co-occur with the same neighboring words. For example, the British National Corpus (BNC) has been used as a source of frequencies and contexts. The method considers the words that are common in both lists and aggregate their PMI values (from the opposite list) to calculate the relative semantic similarity. We define the pointwise mutual information function for only those words having      f  b    (   t  i   ,  w  )    >  0         superscript  f  b     subscript  t  i   w    0    f^{b}(t_{i},w)>0   ,         f  pmi    (   t  i   ,  w  )    =    log  2       f  b    (   t  i   ,  w  )    ×  m     f  t    (   t  i   )    f  t    (  w  )       ,         superscript  f  pmi     subscript  t  i   w      subscript   2          superscript  f  b     subscript  t  i   w    m      superscript  f  t    subscript  t  i    superscript  f  t   w       f^{\text{pmi}}(t_{i},w)=\log_{2}\frac{f^{b}(t_{i},w)\times m}{f^{t}(t_{i})f^{t%
 }(w)},     where     f  t    (   t  i   )        superscript  f  t    subscript  t  i     f^{t}(t_{i})   tells us how many times the type    t  i     subscript  t  i    t_{i}   appeared in the entire corpus,     f  b    (   t  i   ,  w  )        superscript  f  b     subscript  t  i   w     f^{b}(t_{i},w)   tells us how many times word    t  i     subscript  t  i    t_{i}   appeared with word   w   w   w   in a context window and   m   m   m   is total number of tokens in the corpus. Now, for word   w   w   w   , we define a set of words,    X  w     superscript  X  w    X^{w}   , sorted in descending order by their PMI values with   w   w   w   and taken the top-most   β   β   \beta   words having      f  pmi    (   t  i   ,  w  )    >  0         superscript  f  pmi     subscript  t  i   w    0    f^{\text{pmi}}(t_{i},w)>0   .  The set    X  w     superscript  X  w    X^{w}   , contains words    X  i  w     superscript   subscript  X  i   w    X_{i}^{w}   ,       X  w   =   {   X  i  w   }        superscript  X  w     superscript   subscript  X  i   w      X^{w}=\{X_{i}^{w}\}   , where    i  =   1  ,  2  ,  …  ,  β       i   1  2  normal-…  β     i=1,2,\ldots,\beta   and        f  pmi    (   X  1  w   ,  w  )    ≥    f  pmi    (   X  2  w   ,  w  )    ≥   ⋯   f  pmi    (   X   β  -  1   w   ,  w  )    ≥    f  pmi    (   X  β  w   ,  w  )             superscript  f  pmi     superscript   subscript  X  1   w   w       superscript  f  pmi     superscript   subscript  X  2   w   w           normal-⋯   superscript  f  pmi     superscript   subscript  X    β  1    w   w            superscript  f  pmi     superscript   subscript  X  β   w   w       f^{\text{pmi}}(X_{1}^{w},w)\geq f^{\text{pmi}}(X_{2}^{w},w)\geq\cdots f^{\text%
 {pmi}}(X_{\beta-1}^{w},w)\geq f^{\text{pmi}}(X_{\beta}^{w},w)     A rule of thumb is used to choose the value of   β   β   \beta   . The    β   β   \beta   -PMI summation function of a word is defined with respect to another word. For word    w  1     subscript  w  1    w_{1}   with respect to word    w  2     subscript  w  2    w_{2}   it is:       f   (   w  1   ,   w  2   ,  β  )    =    ∑   i  =  1   β     (    f  pmi    (   X  i   w  1    ,   w  2   )    )   γ          f    subscript  w  1    subscript  w  2   β      superscript   subscript     i  1    β    superscript     superscript  f  pmi     superscript   subscript  X  i    subscript  w  1     subscript  w  2     γ      f(w_{1},w_{2},\beta)=\sum_{i=1}^{\beta}(f^{\text{pmi}}(X_{i}^{w_{1}},w_{2}))^{\gamma}     where      f  pmi    (   X  i   w  1    ,   w  2   )    >  0         superscript  f  pmi     superscript   subscript  X  i    subscript  w  1     subscript  w  2     0    f^{\text{pmi}}(X_{i}^{w_{1}},w_{2})>0   which sums all the positive PMI values of words in the set    X   w  2      superscript  X   subscript  w  2     X^{w_{2}}   also common to the words in the set    X   w  1      superscript  X   subscript  w  1     X^{w_{1}}   . In other words, this function actually aggregates the positive PMI values of all the semantically close words of    w  2     subscript  w  2    w_{2}   which are also common in    w  1     subscript  w  1    w_{1}   's list.   γ   γ   \gamma   should have a value greater than 1. So, the    β   β   \beta   -PMI summation function for word    w  1     subscript  w  1    w_{1}   with respect to word    w  2     subscript  w  2    w_{2}   having    β  =   β  1       β   subscript  β  1     \beta=\beta_{1}   and the    β   β   \beta   -PMI summation function for word    w  2     subscript  w  2    w_{2}   with respect to word    w  1     subscript  w  1    w_{1}   having    β  =   β  2       β   subscript  β  2     \beta=\beta_{2}   are       f   (   w  1   ,   w  2   ,   β  1   )    =    ∑   i  =  1    β  1      (    f  pmi    (   X  i   w  1    ,   w  2   )    )   γ          f    subscript  w  1    subscript  w  2    subscript  β  1       superscript   subscript     i  1     subscript  β  1     superscript     superscript  f  pmi     superscript   subscript  X  i    subscript  w  1     subscript  w  2     γ      f(w_{1},w_{2},\beta_{1})=\sum_{i=1}^{\beta_{1}}(f^{\text{pmi}}(X_{i}^{w_{1}},w%
 _{2}))^{\gamma}     and       f   (   w  2   ,   w  1   ,   β  2   )    =    ∑   i  =  1    β  2      (    f  pmi    (   X  i   w  2    ,   w  1   )    )   γ          f    subscript  w  2    subscript  w  1    subscript  β  2       superscript   subscript     i  1     subscript  β  2     superscript     superscript  f  pmi     superscript   subscript  X  i    subscript  w  2     subscript  w  1     γ      f(w_{2},w_{1},\beta_{2})=\sum_{i=1}^{\beta_{2}}(f^{\text{pmi}}(X_{i}^{w_{2}},w%
 _{1}))^{\gamma}     respectively.  Finally, the semantic PMI similarity function between the two words,    w  1     subscript  w  1    w_{1}   and    w  2     subscript  w  2    w_{2}   , is defined as        Sim   (   w  1   ,   w  2   )    =     f   (   w  1   ,   w  2   ,   β  1   )     β  1    +    f   (   w  2   ,   w  1   ,   β  2   )     β  2      .        Sim    subscript  w  1    subscript  w  2           f    subscript  w  1    subscript  w  2    subscript  β  1      subscript  β  1        f    subscript  w  2    subscript  w  1    subscript  β  2      subscript  β  2       \mathrm{Sim}(w_{1},w_{2})=\frac{f(w_{1},w_{2},\beta_{1})}{\beta_{1}}+\frac{f(w%
 _{2},w_{1},\beta_{2})}{\beta_{2}}.     The semantic word similarity is normalized, so that it provides a similarity score between   0   0    and   1   1   1   inclusively. The normalization of semantic similarity algorithm returns a normalized score of similarity between two words. It takes as arguments the two words,    r  i     subscript  r  i    r_{i}   and    s  j     subscript  s  j    s_{j}   , and a maximum value,   λ   λ   \lambda   , that is returned by the semantic similarity function, Sim(). It returns a similarity score between 0 and 1 inclusively. For example, the algorithm returns 0.986 for words cemetery and graveyard with    λ  =  20      λ  20    \lambda=20   (for SOC-PMI method).  References   Islam, A. and Inkpen, D. (2008). Semantic text similarity using corpus-based word similarity and string similarity . ACM Trans. Knowl. Discov. Data 2, 2 (Jul. 2008), 1–25.  Islam, A. and Inkpen, D. (2006). Second Order Co-occurrence PMI for Determining the Semantic Similarity of Words , in Proceedings of the International Conference on Language Resources and Evaluation (LREC 2006), Genoa, Italy, pp. 1033–1038.   "  Category:Computational linguistics  Category:Statistical distance measures   