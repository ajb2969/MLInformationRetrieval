   Conjugate gradient method      Conjugate gradient method  table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
   margin: 0; padding: 0; vertical-align: baseline; border: none; }
 <style>
 table.sourceCode { width: 100%; line-height: 100%; }
 td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
 td.sourceCode { padding-left: 5px; }
 code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
 code > span.dt { color: #902000; } /* DataType */
 code > span.dv { color: #40a070; } /* DecVal */
 code > span.bn { color: #40a070; } /* BaseN */
 code > span.fl { color: #40a070; } /* Float */
 code > span.ch { color: #4070a0; } /* Char */
 code > span.st { color: #4070a0; } /* String */
 code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
 code > span.ot { color: #007020; } /* Other */
 code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
 code > span.fu { color: #06287e; } /* Function */
 code > span.er { color: #ff0000; font-weight: bold; } /* Error */
 code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 code > span.cn { color: #880000; } /* Constant */
 code > span.sc { color: #4070a0; } /* SpecialChar */
 code > span.vs { color: #4070a0; } /* VerbatimString */
 code > span.ss { color: #bb6688; } /* SpecialString */
 code > span.im { } /* Import */
 code > span.va { color: #19177c; } /* Variable */
 code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
 code > span.op { color: #666666; } /* Operator */
 code > span.bu { } /* BuiltIn */
 code > span.ex { } /* Extension */
 code > span.pp { color: #bc7a00; } /* Preprocessor */
 code > span.at { color: #7d9029; } /* Attribute */
 code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
 code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
 code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
 code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */     In mathematics , the conjugate gradient method is an algorithm for the numerical solution of particular systems of linear equations , namely those whose matrix is symmetric and positive-definite . The conjugate gradient method is often implemented as an iterative algorithm , applicable to sparse systems that are too large to be handled by a direct implementation or other direct methods such as the Cholesky decomposition . Large sparse systems often arise when numerically solving partial differential equations or optimization problems.  The conjugate gradient method can also be used to solve unconstrained optimization problems such as energy minimization . It was mainly developed by Magnus Hestenes and Eduard Stiefel . 1  The biconjugate gradient method provides a generalization to non-symmetric matrices. Various nonlinear conjugate gradient methods seek minima of nonlinear equations.  Description of the method  Suppose we want to solve the following system of linear equations   Ax = b    for the vector x where the known n -by- n matrix A is symmetric (i.e., A T = A ), positive definite (i.e. x T Ax > 0 for all non-zero vectors x in R n ), and real , and b is known as well.  We denote the unique solution of this system by    𝐱  *     subscript  𝐱     \mathbf{x}_{*}   .  The conjugate gradient method as a direct method  We say that two non-zero vectors u and v are conjugate (with respect to A ) if        𝐮  T   𝐀𝐯   =  0.         superscript  𝐮  normal-T   𝐀𝐯   0.    \mathbf{u}^{\mathrm{T}}\mathbf{A}\mathbf{v}=0.     Since A is symmetric and positive definite, the left-hand side defines an inner product         ⟨  𝐮  ,  𝐯  ⟩   𝐀   :=   ⟨  𝐀𝐮  ,  𝐯  ⟩   =   ⟨  𝐮  ,    𝐀  T   𝐯   ⟩   =   ⟨  𝐮  ,  𝐀𝐯  ⟩   =    𝐮  T   𝐀𝐯    .       assign   subscript   𝐮  𝐯   𝐀    𝐀𝐮  𝐯         𝐮     superscript  𝐀  normal-T   𝐯          𝐮  𝐀𝐯           superscript  𝐮  normal-T   𝐀𝐯      \langle\mathbf{u},\mathbf{v}\rangle_{\mathbf{A}}:=\langle\mathbf{A}\mathbf{u},%
 \mathbf{v}\rangle=\langle\mathbf{u},\mathbf{A}^{\mathrm{T}}\mathbf{v}\rangle=%
 \langle\mathbf{u},\mathbf{A}\mathbf{v}\rangle=\mathbf{u}^{\mathrm{T}}\mathbf{A%
 }\mathbf{v}.     Two vectors are conjugate if and only if they are orthogonal with respect to this inner product. Being conjugate is a symmetric relation: if u is conjugate to v , then v is conjugate to u .  Suppose that    P  =   {   𝐩  k   :     ∀  i   ≠   k  ,  i    ,    k  ∈   [  1  ,  n  ]    ,     ⟨   𝐩  i   ,   𝐩  k   ⟩   A   =  0     }       P   conditional-set   subscript  𝐩  k    formulae-sequence     for-all  i    k  i     formulae-sequence    k   1  n       subscript    subscript  𝐩  i    subscript  𝐩  k    A   0        P=\{\mathbf{p}_{k}:\forall i\neq k,i,k\in[1,n],\langle\mathbf{p}_{i},\mathbf{p%
 }_{k}\rangle_{A}=0\}   is a set of n mutually conjugate directions. Then   P   P   P   is a basis of    ℝ  n     superscript  ℝ  n    \mathbb{R}^{n}   , so within   P   P   P   we can expand the solution    𝐱  *     subscript  𝐱     \mathbf{x}_{*}   of    𝐀𝐱  =  𝐛      𝐀𝐱  𝐛    \mathbf{Ax}=\mathbf{b}   :       𝐱  *   =    ∑   i  =  1   n     α  i    𝐩  i          subscript  𝐱      subscript   superscript   n     i  1       subscript  α  i    subscript  𝐩  i       \mathbf{x}_{*}=\sum^{n}_{i=1}\alpha_{i}\mathbf{p}_{i}     and we see that       𝐛  =   𝐀𝐱  *   =    ∑   i  =  1   n     α  i    𝐀𝐩  i      .        𝐛   subscript  𝐀𝐱           subscript   superscript   n     i  1       subscript  α  i    subscript  𝐀𝐩  i        \mathbf{b}=\mathbf{A}\mathbf{x}_{*}=\sum^{n}_{i=1}\alpha_{i}\mathbf{A}\mathbf{%
 p}_{i}.     For any     𝐩  k   ∈  P       subscript  𝐩  k   P    \mathbf{p}_{k}\in P   ,         𝐩  k  T   𝐛   =    𝐩  k  T    𝐀𝐱  *    =    ∑   i  =  1   n     α  i    𝐩  k  T    𝐀𝐩  i     =    α  k    𝐩  k  T    𝐀𝐩  k     .           superscript   subscript  𝐩  k   normal-T   𝐛      superscript   subscript  𝐩  k   normal-T    subscript  𝐀𝐱            subscript   superscript   n     i  1       subscript  α  i    superscript   subscript  𝐩  k   normal-T    subscript  𝐀𝐩  i             subscript  α  k    superscript   subscript  𝐩  k   normal-T    subscript  𝐀𝐩  k       \mathbf{p}_{k}^{\mathrm{T}}\mathbf{b}=\mathbf{p}_{k}^{\mathrm{T}}\mathbf{A}%
 \mathbf{x}_{*}=\sum^{n}_{i=1}\alpha_{i}\mathbf{p}_{k}^{\mathrm{T}}\mathbf{A}%
 \mathbf{p}_{i}=\alpha_{k}\mathbf{p}_{k}^{\mathrm{T}}\mathbf{A}\mathbf{p}_{k}.     (because     ∀  i   ≠   k  ,   p  i   ,   p  k         for-all  i    k   subscript  p  i    subscript  p  k      \forall i\neq k,p_{i},p_{k}   are mutually conjugate)        α  k   =     𝐩  k  T   𝐛     𝐩  k  T    𝐀𝐩  k     =    ⟨   𝐩  k   ,  𝐛  ⟩     ⟨   𝐩  k   ,   𝐩  k   ⟩   𝐀    =    ⟨   𝐩  k   ,  𝐛  ⟩     ∥   𝐩  k   ∥   𝐀  2     .         subscript  α  k        superscript   subscript  𝐩  k   normal-T   𝐛      superscript   subscript  𝐩  k   normal-T    subscript  𝐀𝐩  k              subscript  𝐩  k   𝐛    subscript    subscript  𝐩  k    subscript  𝐩  k    𝐀             subscript  𝐩  k   𝐛    superscript   subscript   norm   subscript  𝐩  k    𝐀   2       \alpha_{k}=\frac{\mathbf{p}_{k}^{\mathrm{T}}\mathbf{b}}{\mathbf{p}_{k}^{%
 \mathrm{T}}\mathbf{A}\mathbf{p}_{k}}=\frac{\langle\mathbf{p}_{k},\mathbf{b}%
 \rangle}{\,\,\,\langle\mathbf{p}_{k},\mathbf{p}_{k}\rangle_{\mathbf{A}}}=\frac%
 {\langle\mathbf{p}_{k},\mathbf{b}\rangle}{\,\,\,\|\mathbf{p}_{k}\|_{\mathbf{A}%
 }^{2}}.     This result is perhaps most transparent by considering the inner product defined above.  This gives the following method for solving the equation Ax = b : find a sequence of n conjugate directions, and then compute the coefficients    α  k     subscript  α  k    \scriptstyle\alpha_{k}   .  The conjugate gradient method as an iterative method  If we choose the conjugate vectors p k carefully, then we may not need all of them to obtain a good approximation to the solution    𝐱  *     subscript  𝐱     \scriptstyle\mathbf{x}_{*}   . So, we want to regard the conjugate gradient method as an iterative method. This also allows us to approximately solve systems where n is so large that the direct method would take too much time.  We denote the initial guess for    𝐱  *     subscript  𝐱     \scriptstyle\mathbf{x}_{*}   by x 0 . We can assume without loss of generality that x 0 = 0 (otherwise, consider the system Az = b − Ax 0 instead). Starting with x 0 we search for the solution and in each iteration we need a metric to tell us whether we are closer to the solution    𝐱  *     subscript  𝐱     \scriptstyle\mathbf{x}_{*}   (that is unknown to us). This metric comes from the fact that the solution    𝐱  *     subscript  𝐱     \scriptstyle\mathbf{x}_{*}   is also the unique minimizer of the following quadratic function ; so if f( x ) becomes smaller in an iteration it means that we are closer to    𝐱  *     subscript  𝐱     \scriptstyle\mathbf{x}_{*}   .         f   (  𝐱  )    =     1  2    𝐱  T   𝐀𝐱   -    𝐱  T   𝐛     ,   𝐱  ∈   𝐑  n     .     formulae-sequence      f  𝐱         1  2    superscript  𝐱  normal-T   𝐀𝐱      superscript  𝐱  normal-T   𝐛       𝐱   superscript  𝐑  n      f(\mathbf{x})=\frac{1}{2}\mathbf{x}^{\mathrm{T}}\mathbf{A}\mathbf{x}-\mathbf{x%
 }^{\mathrm{T}}\mathbf{b},\quad\mathbf{x}\in\mathbf{R}^{n}.     This suggests taking the first basis vector p 0 to be the negative of the gradient of f at x = x 0 . The gradient of f equals Ax − b . Starting with a "guessed solution" x 0 (we can always guess that    𝐱  *     subscript  𝐱     \scriptstyle\mathbf{x}_{*}   is 0 and set x 0 to 0 if we have no reason to guess for anything else), this means we take p 0 = b − Ax 0 . The other vectors in the basis will be conjugate to the gradient, hence the name conjugate gradient method .  Let r k be the residual at the k th step:        𝐫  k   =   𝐛  -   𝐀𝐱  k     .       subscript  𝐫  k     𝐛   subscript  𝐀𝐱  k      \mathbf{r}_{k}=\mathbf{b}-\mathbf{Ax}_{k}.\,   Note that r k is the negative gradient of f at x = x k , so the gradient descent method would be to move in the direction r k . Here, we insist that the directions p k be conjugate to each other. We also require that the next search direction be built out of the current residue and all previous search directions, which is reasonable enough in practice.  The conjugation constraint is an orthonormal-type constraint and hence the algorithm bears resemblance to Gram-Schmidt orthonormalization .  This gives the following expression:       𝐩  k   =    𝐫  k   -    ∑   i  <  k        𝐩  i  T    𝐀𝐫  k      𝐩  i  T    𝐀𝐩  i      𝐩  i           subscript  𝐩  k      subscript  𝐫  k     subscript     i  k           superscript   subscript  𝐩  i   normal-T    subscript  𝐀𝐫  k       superscript   subscript  𝐩  i   normal-T    subscript  𝐀𝐩  i      subscript  𝐩  i        \mathbf{p}_{k}=\mathbf{r}_{k}-\sum_{i   (see the picture at the top of the article for the effect of the conjugacy constraint on convergence). Following this direction, the next optimal location is given by       𝐱   k  +  1    =    𝐱  k   +    α  k    𝐩  k          subscript  𝐱    k  1       subscript  𝐱  k      subscript  α  k    subscript  𝐩  k       \mathbf{x}_{k+1}=\mathbf{x}_{k}+\alpha_{k}\mathbf{p}_{k}   with        α  k   =     𝐩  k  T   𝐛     𝐩  k  T    𝐀𝐩  k     =     𝐩  k  T    (    𝐫   k  -  1    +   𝐀𝐱   k  -  1     )      𝐩  k  T    𝐀𝐩  k     =     𝐩  k  T    𝐫   k  -  1       𝐩  k  T    𝐀𝐩  k      ,         subscript  α  k        superscript   subscript  𝐩  k   normal-T   𝐛      superscript   subscript  𝐩  k   normal-T    subscript  𝐀𝐩  k               superscript   subscript  𝐩  k   normal-T      subscript  𝐫    k  1     subscript  𝐀𝐱    k  1         superscript   subscript  𝐩  k   normal-T    subscript  𝐀𝐩  k               superscript   subscript  𝐩  k   normal-T    subscript  𝐫    k  1        superscript   subscript  𝐩  k   normal-T    subscript  𝐀𝐩  k        \alpha_{k}=\frac{\mathbf{p}_{k}^{\mathrm{T}}\mathbf{b}}{\mathbf{p}_{k}^{%
 \mathrm{T}}\mathbf{A}\mathbf{p}_{k}}=\frac{\mathbf{p}_{k}^{\mathrm{T}}(\mathbf%
 {r}_{k-1}+\mathbf{Ax}_{k-1})}{\mathbf{p}_{k}^{\mathrm{T}}\mathbf{A}\mathbf{p}_%
 {k}}=\frac{\mathbf{p}_{k}^{\mathrm{T}}\mathbf{r}_{k-1}}{\mathbf{p}_{k}^{%
 \mathrm{T}}\mathbf{A}\mathbf{p}_{k}},   where the last equality holds because p k and x k-1 are conjugate.  The resulting algorithm  The above algorithm gives the most straightforward explanation of the conjugate gradient method. Seemingly, the algorithm as stated requires storage of all previous searching directions and residue vectors, as well as many matrix-vector multiplications, and thus can be computationally expensive. However, a closer analysis of the algorithm shows that r k+1 is conjugate to p i for all i k'', p k , and x k are needed to construct r k+1 , p k+1 , and x k+1 . Furthermore, only one matrix-vector multiplication is needed in each iteration.  The algorithm is detailed below for solving Ax = b where A is a real, symmetric, positive-definite matrix. The input vector x 0 can be an approximate initial solution or 0 . It is a different formulation of the exact procedure described above.           𝐫  0   :=   𝐛  -   𝐀𝐱  0            𝐩  0   :=   𝐫  0          k  :=  0        repeat          α   k   :=      𝐫  k  T    𝐫  k      𝐩  k  T    𝐀𝐩  k               𝐱    k  +  1    :=    𝐱  k   +    α  k    𝐩  k              𝐫    k  +  1    :=    𝐫  k   -    α  k    𝐀𝐩  k             if    r   k  +  1    is sufficiently small then exit loop           β   k   :=      𝐫   k  +  1   T    𝐫   k  +  1       𝐫  k  T    𝐫  k               𝐩    k  +  1    :=    𝐫   k  +  1    +    β  k    𝐩  k             k   :=   k  +  1         end repeat        The result is   𝐱   k  +  1            missing-subexpression    assign   subscript  𝐫  0     𝐛   subscript  𝐀𝐱  0        missing-subexpression    assign   subscript  𝐩  0    subscript  𝐫  0       missing-subexpression    assign  k  0      missing-subexpression   repeat     missing-subexpression    assign   subscript  α  k        superscript   subscript  𝐫  k   normal-T    subscript  𝐫  k       superscript   subscript  𝐩  k   normal-T    subscript  𝐀𝐩  k         missing-subexpression    assign   subscript  𝐱    k  1       subscript  𝐱  k      subscript  α  k    subscript  𝐩  k         missing-subexpression    assign   subscript  𝐫    k  1       subscript  𝐫  k      subscript  α  k    subscript  𝐀𝐩  k         missing-subexpression     if   subscript  r    k  1    is sufficiently small then exit loop      missing-subexpression    assign   subscript  β  k        superscript   subscript  𝐫    k  1    normal-T    subscript  𝐫    k  1        superscript   subscript  𝐫  k   normal-T    subscript  𝐫  k         missing-subexpression    assign   subscript  𝐩    k  1       subscript  𝐫    k  1       subscript  β  k    subscript  𝐩  k         missing-subexpression    assign  k    k  1       missing-subexpression   end repeat     missing-subexpression     The result is   subscript  𝐱    k  1        \begin{aligned}&\displaystyle\mathbf{r}_{0}:=\mathbf{b}-\mathbf{Ax}_{0}\\
 &\displaystyle\mathbf{p}_{0}:=\mathbf{r}_{0}\\
 &\displaystyle k:=0\\
 &\displaystyle\hbox{repeat}\\
 &\displaystyle\qquad\alpha_{k}:=\frac{\mathbf{r}_{k}^{\mathrm{T}}\mathbf{r}_{k%
 }}{\mathbf{p}_{k}^{\mathrm{T}}\mathbf{Ap}_{k}}\\
 &\displaystyle\qquad\mathbf{x}_{k+1}:=\mathbf{x}_{k}+\alpha_{k}\mathbf{p}_{k}%
 \\
 &\displaystyle\qquad\mathbf{r}_{k+1}:=\mathbf{r}_{k}-\alpha_{k}\mathbf{Ap}_{k}%
 \\
 &\displaystyle\qquad\hbox{if }r_{k+1}\hbox{ is sufficiently small then exit %
 loop}\\
 &\displaystyle\qquad\beta_{k}:=\frac{\mathbf{r}_{k+1}^{\mathrm{T}}\mathbf{r}_{%
 k+1}}{\mathbf{r}_{k}^{\mathrm{T}}\mathbf{r}_{k}}\\
 &\displaystyle\qquad\mathbf{p}_{k+1}:=\mathbf{r}_{k+1}+\beta_{k}\mathbf{p}_{k}%
 \\
 &\displaystyle\qquad k:=k+1\\
 &\displaystyle\hbox{end repeat}\\
 &\displaystyle\hbox{The result is }\mathbf{x}_{k+1}\end{aligned}     This is the most commonly used algorithm. The same formula for    β  k     subscript  β  k    \beta_{k}   is also used in the Fletcher–Reeves nonlinear conjugate gradient method .  Computation of alpha and beta  In the algorithm,    α  k     subscript  α  k    \alpha_{k}   is chosen such that    𝐫   k  +  1      subscript  𝐫    k  1     \mathbf{r}_{k+1}   is orthogonal to    𝐫  k     subscript  𝐫  k    \mathbf{r}_{k}   . The denominator is simplified from       α  k   =     𝐫  k  T    𝐫  k      𝐫  k  T    𝐀𝐩  k     =     𝐫  k  T    𝐫  k      𝐩  k  T    𝐀𝐩  k            subscript  α  k        superscript   subscript  𝐫  k   normal-T    subscript  𝐫  k       superscript   subscript  𝐫  k   normal-T    subscript  𝐀𝐩  k               superscript   subscript  𝐫  k   normal-T    subscript  𝐫  k       superscript   subscript  𝐩  k   normal-T    subscript  𝐀𝐩  k        \alpha_{k}=\frac{\mathbf{r}_{k}^{\mathrm{T}}\mathbf{r}_{k}}{\mathbf{r}_{k}^{%
 \mathrm{T}}\mathbf{Ap}_{k}}=\frac{\mathbf{r}_{k}^{\mathrm{T}}\mathbf{r}_{k}}{%
 \mathbf{p}_{k}^{\mathrm{T}}\mathbf{Ap}_{k}}     since     𝐫  k   =    𝐩  k   -    β   k  -  1     𝐩   k  -  1           subscript  𝐫  k      subscript  𝐩  k      subscript  β    k  1     subscript  𝐩    k  1        \mathbf{r}_{k}=\mathbf{p}_{k}-\mathbf{\beta}_{k-1}\mathbf{p}_{k-1}   . The    β  k     subscript  β  k    \beta_{k}   is chosen such that    𝐩   k  +  1      subscript  𝐩    k  1     \mathbf{p}_{k+1}   is conjugated to    𝐩  k     subscript  𝐩  k    \mathbf{p}_{k}   . Initially,    β  k     subscript  β  k    \beta_{k}   is       β  k   =   -     𝐫   k  +  1   T   A   𝐩  k      𝐩  k  T   A   𝐩  k           subscript  β  k          superscript   subscript  𝐫    k  1    normal-T   A   subscript  𝐩  k       superscript   subscript  𝐩  k   normal-T   A   subscript  𝐩  k        \beta_{k}=-\frac{\mathbf{r}_{k+1}^{\mathrm{T}}A\mathbf{p}_{k}}{\mathbf{p}_{k}^%
 {\mathrm{T}}A\mathbf{p}_{k}}     using     𝐫  k   =    𝐫   k  -  1    -    α   k  -  1    A   𝐩   k  -  1           subscript  𝐫  k      subscript  𝐫    k  1       subscript  α    k  1    A   subscript  𝐩    k  1        \mathbf{r}_{k}=\mathbf{r}_{k-1}-\alpha_{k-1}A\mathbf{p}_{k-1}   and equivalently     A   𝐩   k  -  1     =    1   α   k  -  1      (    𝐫   k  -  1    -   𝐫  k    )          A   subscript  𝐩    k  1         1   subscript  α    k  1        subscript  𝐫    k  1     subscript  𝐫  k       A\mathbf{p}_{k-1}=\frac{1}{\alpha_{k-1}}(\mathbf{r}_{k-1}-\mathbf{r}_{k})   , the numerator of    β  k     subscript  β  k    \beta_{k}   is rewritten as        𝐫   k  +  1   T   A   𝐩  k    =    1   α  k     𝐫   k  +  1   T    (    𝐫  k   -   𝐫   k  +  1     )    =   -    1   α  k     𝐫   k  +  1   T    𝐫   k  +  1               superscript   subscript  𝐫    k  1    normal-T   A   subscript  𝐩  k        1   subscript  α  k     superscript   subscript  𝐫    k  1    normal-T      subscript  𝐫  k    subscript  𝐫    k  1                 1   subscript  α  k     superscript   subscript  𝐫    k  1    normal-T    subscript  𝐫    k  1         \mathbf{r}_{k+1}^{\mathrm{T}}A\mathbf{p}_{k}=\frac{1}{\alpha_{k}}\mathbf{r}_{k%
 +1}^{\mathrm{T}}(\mathbf{r}_{k}-\mathbf{r}_{k+1})=-\frac{1}{\alpha_{k}}\mathbf%
 {r}_{k+1}^{\mathrm{T}}\mathbf{r}_{k+1}     because    𝐫   k  +  1      subscript  𝐫    k  1     \mathbf{r}_{k+1}   and    𝐫  k     subscript  𝐫  k    \mathbf{r}_{k}   are orthogonal by design. The denominator is rewritten as        𝐩  k  T   A   𝐩  k    =     (    𝐫  k   +    β   k  -  1     𝐩   k  -  1      )   T   A   𝐩  k    =    1   α  k     𝐫  k  T    (    𝐫  k   -   𝐫   k  +  1     )    =    1   α  k     𝐫  k  T    𝐫  k             superscript   subscript  𝐩  k   normal-T   A   subscript  𝐩  k       superscript     subscript  𝐫  k      subscript  β    k  1     subscript  𝐩    k  1      normal-T   A   subscript  𝐩  k             1   subscript  α  k     superscript   subscript  𝐫  k   normal-T      subscript  𝐫  k    subscript  𝐫    k  1               1   subscript  α  k     superscript   subscript  𝐫  k   normal-T    subscript  𝐫  k       \mathbf{p}_{k}^{\mathrm{T}}A\mathbf{p}_{k}=(\mathbf{r}_{k}+\beta_{k-1}\mathbf{%
 p}_{k-1})^{\mathrm{T}}A\mathbf{p}_{k}=\frac{1}{\alpha_{k}}\mathbf{r}_{k}^{%
 \mathrm{T}}(\mathbf{r}_{k}-\mathbf{r}_{k+1})=\frac{1}{\alpha_{k}}\mathbf{r}_{k%
 }^{\mathrm{T}}\mathbf{r}_{k}     using that the search directions    𝐩  k     subscript  𝐩  k    \mathbf{p}_{k}   are conjugated and again that the residuals are orthogonal. This gives the   β   β   \beta   in the algorithm after cancelling    α  k     subscript  α  k    \alpha_{k}   .  Example code in MATLAB  function [x] = conjgrad(A,b,x)
     r=b-A*x;
     p=r;
     rsold=r*r';
 
     for i= 1 : 1e6 Ap=A*p;
         alpha=rsold/(p*Ap');
         x=x+alpha*p;
         r=r-alpha*Ap;
         rsnew=r*r';
         if sqrt(rsnew)< 1e-10 break;
         end
         p=r+rsnew/rsold*p;
         rsold=rsnew;
     end
 end  Numerical example  To illustrate the conjugate gradient method, we will complete a simple example.  Considering the linear system Ax = b given by    \mathbf{A} \mathbf{x}= \begin{bmatrix}     4 & 1 \\ 1 & 3 \end{bmatrix}\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \end{bmatrix} ,  we will perform two steps of the conjugate gradient method beginning with the initial guess    \mathbf{x}_0 =     \begin{bmatrix} 2 \\ 1 \end{bmatrix}  in order to find an approximate solution to the system.  Solution  For reference, the exact solution is         \mathbf{x} = \begin{bmatrix} \frac{1}{11} \\\\ \frac{7}{11} \end{bmatrix}  Our first step is to calculate the residual vector r 0 associated with x 0 . This residual is computed from the formula r 0 = b - Ax 0 , and in our case is equal to         \mathbf{r}_0 = \begin{bmatrix} 1 \\ 2 \end{bmatrix} - \begin{bmatrix} 4 & 1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix}-8 \\ -3 \end{bmatrix}.   Since this is the first iteration, we will use the residual vector r 0 as our initial search direction p 0 ; the method of selecting p k will change in further iterations.  We now compute the scalar α 0 using the relationship         \alpha_0 = \frac{\mathbf{r}_0^\mathrm{T} \mathbf{r}_0}{\mathbf{p}_0^\mathrm{T} \mathbf{A p}_0} = \frac{\begin{bmatrix} -8 & -3 \end{bmatrix} \begin{bmatrix} -8 \\ -3 \end{bmatrix}}{ \begin{bmatrix} -8 & -3 \end{bmatrix} \begin{bmatrix} 4 & 1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} -8 \\ -3 \end{bmatrix} } = \frac{73}{331}.  We can now compute x 1 using the formula         \mathbf{x}_1 = \mathbf{x}_0 + \alpha_0\mathbf{p}_0 = \begin{bmatrix} 2 \\ 1 \end{bmatrix} + \frac{73}{331} \begin{bmatrix} -8 \\ -3 \end{bmatrix} = \begin{bmatrix} 0.2356 \\ 0.3384 \end{bmatrix}.  This result completes the first iteration, the result being an "improved" approximate solution to the system, x 1 . We may now move on and compute the next residual vector r 1 using the formula         \mathbf{r}_1 = \mathbf{r}_0 - \alpha_0 \mathbf{A} \mathbf{p}_0 = \begin{bmatrix} -8 \\ -3 \end{bmatrix} - \frac{73}{331} \begin{bmatrix} 4 & 1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} -8 \\ -3 \end{bmatrix} = \begin{bmatrix} -0.2810 \\ 0.7492 \end{bmatrix}.  Our next step in the process is to compute the scalar β 0 that will eventually be used to determine the next search direction p 1 .         \beta_0 = \frac{\mathbf{r}_1^\mathrm{T} \mathbf{r}_1}{\mathbf{r}_0^\mathrm{T} \mathbf{r}_0} = \frac{\begin{bmatrix} -0.2810 & 0.7492 \end{bmatrix} \begin{bmatrix} -0.2810 \\ 0.7492 \end{bmatrix}}{\begin{bmatrix} -8 & -3 \end{bmatrix} \begin{bmatrix} -8 \\ -3 \end{bmatrix}} = 0.0088.  Now, using this scalar β 0 , we can compute the next search direction p 1 using the relationship         \mathbf{p}_1 = \mathbf{r}_1 + \beta_0 \mathbf{p}_0 = \begin{bmatrix} -0.2810 \\ 0.7492 \end{bmatrix} + 0.0088 \begin{bmatrix} -8 \\ -3 \end{bmatrix} = \begin{bmatrix} -0.3511 \\ 0.7229 \end{bmatrix}.  We now compute the scalar α 1 using our newly acquired p 1 using the same method as that used for α 0 .         \alpha_1 = \frac{\mathbf{r}_1^\mathrm{T} \mathbf{r}_1}{\mathbf{p}_1^\mathrm{T} \mathbf{A p}_1} = \frac{\begin{bmatrix} -0.2810 & 0.7492 \end{bmatrix} \begin{bmatrix} -0.2810 \\ 0.7492 \end{bmatrix}}{ \begin{bmatrix} -0.3511 & 0.7229 \end{bmatrix} \begin{bmatrix} 4 & 1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} -0.3511 \\ 0.7229 \end{bmatrix} } = 0.4122.  Finally, we find x 2 using the same method as that used to find x 1 .         \mathbf{x}_2 = \mathbf{x}_1 + \alpha_1 \mathbf{p}_1 = \begin{bmatrix} 0.2356 \\ 0.3384 \end{bmatrix} + 0.4122 \begin{bmatrix} -0.3511 \\ 0.7229 \end{bmatrix} = \begin{bmatrix} 0.0909 \\ 0.6364 \end{bmatrix}.  The result, x 2 , is a "better" approximation to the system's solution than x 1 and x 0 . If exact arithmetic were to be used in this example instead of limited-precision, then the exact solution would theoretically have been reached after n = 2 iterations ( n being the order of the system).  Convergence properties of the conjugate gradient method  The conjugate gradient method can theoretically be viewed as a direct method, as it produces the exact solution after a finite number of iterations, which is not larger than the size of the matrix, in the absence of round-off error . However, the conjugate gradient method is unstable with respect to even small perturbations, e.g., most directions are not in practice conjugate, and the exact solution is never obtained. Fortunately, the conjugate gradient method can be used as an iterative method as it provides monotonically improving approximations    𝐱  k     subscript  𝐱  k    \mathbf{x}_{k}   to the exact solution, which may reach the required tolerance after a relatively small (compared to the problem size) number of iterations. The improvement is typically linear and its speed is determined by the condition number     κ   (  A  )       κ  A    \kappa(A)   of the system matrix   A   A   A   : the larger    κ   (  A  )       κ  A    \kappa(A)   is, the slower the improvement. 2  If    κ   (  A  )       κ  A    \kappa(A)   is large, preconditioning is used to replace the original system     𝐀𝐱  -  𝐛   =  0        𝐀𝐱  𝐛   0    \mathbf{Ax}-\mathbf{b}=0   with      𝐌   -  1     (   𝐀𝐱  -  𝐛   )    =  0         superscript  𝐌    1      𝐀𝐱  𝐛    0    \mathbf{M}^{-1}(\mathbf{Ax}-\mathbf{b})=0   so that    κ   (    𝐌   -  1    𝐀   )       κ     superscript  𝐌    1    𝐀     \kappa(\mathbf{M}^{-1}\mathbf{A})   gets smaller than    κ   (  𝐀  )       κ  𝐀    \kappa(\mathbf{A})   , see below.  The preconditioned conjugate gradient method  In most cases, preconditioning is necessary to ensure fast convergence of the conjugate gradient method. The preconditioned conjugate gradient method takes the following form:       𝐫  0   :=   𝐛  -   𝐀𝐱  0       assign   subscript  𝐫  0     𝐛   subscript  𝐀𝐱  0      \mathbf{r}_{0}:=\mathbf{b}-\mathbf{Ax}_{0}          𝐳  0   :=    𝐌   -  1     𝐫  0       assign   subscript  𝐳  0      superscript  𝐌    1     subscript  𝐫  0      \mathbf{z}_{0}:=\mathbf{M}^{-1}\mathbf{r}_{0}          𝐩  0   :=   𝐳  0      assign   subscript  𝐩  0    subscript  𝐳  0     \mathbf{p}_{0}:=\mathbf{z}_{0}         k  :=   0      assign  k  0    k:=0\,      repeat       α  k   :=     𝐫  k  T    𝐳  k      𝐩  k  T    𝐀𝐩  k        assign   subscript  α  k        superscript   subscript  𝐫  k   normal-T    subscript  𝐳  k       superscript   subscript  𝐩  k   normal-T    subscript  𝐀𝐩  k       \alpha_{k}:=\frac{\mathbf{r}_{k}^{\mathrm{T}}\mathbf{z}_{k}}{\mathbf{p}_{k}^{%
 \mathrm{T}}\mathbf{Ap}_{k}}          𝐱   k  +  1    :=    𝐱  k   +    α  k    𝐩  k        assign   subscript  𝐱    k  1       subscript  𝐱  k      subscript  α  k    subscript  𝐩  k       \mathbf{x}_{k+1}:=\mathbf{x}_{k}+\alpha_{k}\mathbf{p}_{k}          𝐫   k  +  1    :=    𝐫  k   -    α  k    𝐀𝐩  k        assign   subscript  𝐫    k  1       subscript  𝐫  k      subscript  α  k    subscript  𝐀𝐩  k       \mathbf{r}_{k+1}:=\mathbf{r}_{k}-\alpha_{k}\mathbf{Ap}_{k}      if  r k +1 is sufficiently small then exit loop end if        𝐳   k  +  1    :=    𝐌   -  1     𝐫   k  +  1        assign   subscript  𝐳    k  1       superscript  𝐌    1     subscript  𝐫    k  1       \mathbf{z}_{k+1}:=\mathbf{M}^{-1}\mathbf{r}_{k+1}          β  k   :=     𝐳   k  +  1   T    𝐫   k  +  1       𝐳  k  T    𝐫  k        assign   subscript  β  k        superscript   subscript  𝐳    k  1    normal-T    subscript  𝐫    k  1        superscript   subscript  𝐳  k   normal-T    subscript  𝐫  k       \beta_{k}:=\frac{\mathbf{z}_{k+1}^{\mathrm{T}}\mathbf{r}_{k+1}}{\mathbf{z}_{k}%
 ^{\mathrm{T}}\mathbf{r}_{k}}          𝐩   k  +  1    :=    𝐳   k  +  1    +    β  k    𝐩  k        assign   subscript  𝐩    k  1       subscript  𝐳    k  1       subscript  β  k    subscript  𝐩  k       \mathbf{p}_{k+1}:=\mathbf{z}_{k+1}+\beta_{k}\mathbf{p}_{k}         k  :=   k  +   1       assign  k    k  1     k:=k+1\,       end repeat   The result is x k +1    The above formulation is equivalent to applying the conjugate gradient method without preconditioning to the system        𝐄   -  1    𝐀    (   𝐄   -  1    )   T    𝐱  ^    =    𝐄   -  1    𝐛          superscript  𝐄    1    𝐀   superscript   superscript  𝐄    1    normal-T    normal-^  𝐱       superscript  𝐄    1    𝐛     \mathbf{E}^{-1}\mathbf{A}(\mathbf{E}^{-1})^{\mathrm{T}}\mathbf{\hat{x}}=%
 \mathbf{E}^{-1}\mathbf{b}   where     𝐄𝐄  T   =  𝐌       superscript  𝐄𝐄  normal-T   𝐌    \mathbf{EE}^{\mathrm{T}}=\mathbf{M}   and     𝐱  ^   =    𝐄  T   𝐱        normal-^  𝐱      superscript  𝐄  normal-T   𝐱     \mathbf{\hat{x}}=\mathbf{E}^{\mathrm{T}}\mathbf{x}   .  The preconditioner matrix M has to be symmetric positive-definite and fixed, i.e., cannot change from iteration to iteration. If any of these assumptions on the preconditioner is violated, the behavior of the preconditioned conjugate gradient method may become unpredictable.  An example of a commonly used preconditioner is the incomplete Cholesky factorization .  The flexible preconditioned conjugate gradient method  In numerically challenging applications, sophisticated preconditioners are used, which may lead to variable preconditioning, changing between iterations. Even if the preconditioner is symmetric positive-definite on every iteration, the fact that it may change makes the arguments above invalid, and in practical tests leads to a significant slow down of the convergence of the algorithm presented above. Using the Polak–Ribière formula         β  k   :=     𝐳   k  +  1   T    (    𝐫   k  +  1    -   𝐫  k    )      𝐳  k  T    𝐫  k        assign   subscript  β  k        superscript   subscript  𝐳    k  1    normal-T      subscript  𝐫    k  1     subscript  𝐫  k        superscript   subscript  𝐳  k   normal-T    subscript  𝐫  k       \beta_{k}:=\frac{\mathbf{z}_{k+1}^{\mathrm{T}}\left(\mathbf{r}_{k+1}-\mathbf{r%
 }_{k}\right)}{\mathbf{z}_{k}^{\mathrm{T}}\mathbf{r}_{k}}        instead of the Fletcher–Reeves formula         β  k   :=     𝐳   k  +  1   T    𝐫   k  +  1       𝐳  k  T    𝐫  k        assign   subscript  β  k        superscript   subscript  𝐳    k  1    normal-T    subscript  𝐫    k  1        superscript   subscript  𝐳  k   normal-T    subscript  𝐫  k       \beta_{k}:=\frac{\mathbf{z}_{k+1}^{\mathrm{T}}\mathbf{r}_{k+1}}{\mathbf{z}_{k}%
 ^{\mathrm{T}}\mathbf{r}_{k}}        may dramatically improve the convergence in this case. 3 This version of the preconditioned conjugate gradient method can be called 4  flexible, as it allows for variable preconditioning. The implementation of the flexible version requires storing an extra vector. For a fixed preconditioner,       𝐳   k  +  1   T    𝐫  k    =  0   ,         superscript   subscript  𝐳    k  1    normal-T    subscript  𝐫  k    0    \mathbf{z}_{k+1}^{\mathrm{T}}\mathbf{r}_{k}=0,   so both formulas for    β  k     subscript  β  k    \beta_{k}   are equivalent in exact arithmetic, i.e., without the round-off error .  The mathematical explanation of the better convergence behavior of the method with the Polak–Ribière formula is that the method is locally optimal in this case, in particular, it does not converge slower than the locally optimal steepest descent method. 5  Example code in MATLAB  function [x, k] = gcp(x0, A, C, b, mit, stol, bbA, bbC) % Synopsis:  % x0: initial point  % A: Matrix A of the system Ax=b  % C: Preconditioning Matrix can be left or right  % mit: Maximum number of iterations  % stol: residue norm tolerance  % bbA: Black Box that computes the matrix-vector product for A * u  % bbC: Black Box that computes:  %      for left-side preconditioner : ha = C \ ra  %      for right-side preconditioner: ha = C * ra  % x: Estimated solution point  % k: Number of iterations done  %  % Example:  % tic;[x, t] = cgp(x0, S, speye(1), b, 3000, 10^-8, @(Z, o) Z*o, @(Z, o) o);toc  % Elapsed time is 0.550190 seconds.  %  % Reference:  %  Métodos iterativos tipo Krylov para sistema lineales  %  B. Molina y M. Raydan - ISBN 908-261-078-X if ( nargin < 8 ), error( 'Not enough input arguments. Try help.' ); end;
         if ( isempty(A) ), error( 'Input matrix A must not be empty.' ); end;
         if ( isempty(C) ), error( 'Input preconditioner matrix C must not be empty.' ); end;
         x = x0;
         ha = 0 ;
         hp = 0 ;
         hpp = 0 ;
         ra = 0 ;
         rp = 0 ;
         rpp = 0 ;
         u = 0 ;
         k = 0 ;
 
         ra = b - bbA(A, x0); % <--- ra = b - A * x0; while ( norm(ra, inf) > stol ),
                 ha = bbC(C, ra); % <--- ha = C \ ra; k = k + 1 ;
                 if ( k == mit ), warning( 'GCP:MAXIT' , 'mit reached, no conversion.' ); return; end;
                 hpp = hp;
                 rpp = rp;
                 hp = ha;
                 rp = ra;
                 t = rp'*hp;
                 if ( k == 1 ),
                         u = hp;
                 else
                         u = hp + ( t / (rpp'*hpp) ) * u;
                 end;
                 Au = bbA(A, u); % <--- Au = A * u; a = t / (u'*Au);
                 x = x + a * u;
                 ra = rp - a * Au;
         end;  The conjugate gradient method vs. the locally optimal steepest descent method  In both the original and the preconditioned conjugate gradient methods one only needs to set     β  k   :=  0     assign   subscript  β  k   0    \beta_{k}:=0   in order to make them locally optimal, using the line search , steepest descent methods. With this substitution, vectors   𝐩   𝐩   \mathbf{p}   are always the same as vectors   𝐳   𝐳   \mathbf{z}   , so there is no need to store vectors   𝐩   𝐩   \mathbf{p}   . Thus, every iteration of these steepest descent methods is a bit cheaper compared to that for the conjugate gradient methods. However, the latter converge faster, unless a (highly) variable preconditioner is used, see above.  Derivation of the method  The conjugate gradient method can be derived from several different perspectives, including specialization of the conjugate direction method for optimization, and variation of the Arnoldi / Lanczos iteration for eigenvalue problems. Despite differences in their approaches, these derivations share a common topic—proving the orthogonality of the residuals and conjugacy of the search directions. These two properties are crucial to developing the well-known succinct formulation of the method.  Conjugate gradient on the normal equations  The conjugate gradient method can be applied to an arbitrary n -by- m matrix by applying it to normal equations  A T A and right-hand side vector A T b , since A T A is a symmetric positive-semidefinite matrix for any A . The result is conjugate gradient on the normal equations (CGNR).   A T Ax = A T b    As an iterative method, it is not necessary to form A T A explicitly in memory but only to perform the matrix-vector and transpose matrix-vector multiplications. Therefore CGNR is particularly useful when A is a sparse matrix since these operations are usually extremely efficient. However the downside of forming the normal equations is that the condition number κ( A T A ) is equal to κ 2 ( A ) and so the rate of convergence of CGNR may be slow and the quality of the approximate solution may be sensitive to roundoff errors. Finding a good preconditioner is often an important part of using the CGNR method.  Several algorithms have been proposed (e.g., CGLS, LSQR). The LSQR algorithm purportedly has the best numerical stability when A is ill-conditioned, i.e., A has a large condition number .  See also   Biconjugate gradient method (BiCG)  Conjugate residual method  Nonlinear conjugate gradient method  Iterative method. Linear systems  Preconditioning  Gaussian belief propagation  Krylov subspace  Sparse matrix-vector multiplication   Notes  References  The conjugate gradient method was originally proposed in     Descriptions of the method can be found in the following text books:        External links    An Introduction to the Conjugate Gradient Method Without the Agonizing Pain by Jonathan Richard Shewchuk.  Iterative methods for sparse linear systems by Yousef Saad  LSQR: Sparse Equations and Least Squares by Christopher Paige and Michael Saunders.  Derivation of fast implementation of conjugate gradient method and interactive example   "  Category:Numerical linear algebra  Category:Gradient methods  Category:Articles with example pseudocode  Category:Articles with example MATLAB/Octave code     ↩  ↩  ↩  ↩  ↩    