   K-SVD      K-SVD   In applied mathematics , K-SVD is a dictionary learning algorithm for creating a dictionary for sparse representations , via a singular value decomposition approach. K-SVD is a generalization of the k-means clustering method, and it works by iteratively alternating between sparse coding the input data based on the current dictionary, and updating the atoms in the dictionary to better fit the data. 1 2 K-SVD can be found widely in use in applications such as image processing, audio processing, biology, and document analysis.  Problem description  The goal of dictionary learning is to learn an overcomplete dictionary matrix    D  ∈   ℝ   n  ×  K        D   superscript  ℝ    n  K      D\in\mathbb{R}^{n\times K}   that contains   K   K   K   signal-atoms (in this notation, columns of   D   D   D   ). A signal vector    y  ∈   ℝ  n       y   superscript  ℝ  n     y\in\mathbb{R}^{n}   can be represented, sparsely , as a linear combination of these atoms; to represent   y   y   y   , the representation vector   x   x   x   should satisfy the exact condition    y  =   D  x       y    D  x     y=Dx   , or the approximate condition    y  ≈   D  x       y    D  x     y\approx Dx   , made precise by requiring that      ∥   y  -   D  x    ∥   p   ≤  ϵ       subscript   norm    y    D  x     p   ϵ    \|y-Dx\|_{p}\leq\epsilon   for some small value   ε   ε   ε   and some     L  ₚ      L  ₚ    Lₚ   norm . The vector    x  ∈   ℝ  K       x   superscript  ℝ  K     x\in\mathbb{R}^{K}   contains the representation coefficients of the signal   y   y   y   . Typically, the norm   p   p   p   is selected as , , or .  If    n  <  K      n  K    n   and D is a full-rank matrix, an infinite number of solutions are available for the representation problem, Hence, constraints should be set on the solution. Also, to ensure sparsity, the solution with the fewest number of nonzero coefficients is preferred. Thus, the sparsity representation is the solution of either        (   P  0   )     min  x     ∥  x  ∥   0     subject to  y    =   D  x         subscript  P  0      subscript   x    subscript   norm  x   0      subject to  y      D  x     (P_{0})\quad\min\limits_{x}\|x\|_{0}\qquad\text{subject to }y=Dx     or        (   P   0  ,  ϵ    )     min  x     ∥  x  ∥   0     subject to    ∥   y  -   D  x    ∥   2     ≤  ϵ        subscript  P   0  ϵ       subscript   x    subscript   norm  x   0      subject to   subscript   norm    y    D  x     2     ϵ    (P_{0,\epsilon})\quad\min\limits_{x}\|x\|_{0}\qquad\text{subject to }\|y-Dx\|_%
 {2}\leq\epsilon     where the     ∥  x  ∥   0     subscript   norm  x   0    \|x\|_{0}   counts the nonzero entries in the vector   x   x   x   . (See the zero "norm" .)  K-SVD algorithm  K-SVD is a kind of generalization of K-means, as follows. The k-means clustering can be also regarded as a method of sparse representation . That is, finding the best possible codebook to represent the data samples     {   y  i   }    i  =  1   M     subscript   superscript    subscript  y  i    M     i  1     \{y_{i}\}^{M}_{i=1}   by nearest neighbor , by solving           min    D  ,  X     {    ∥   Y  -   D  X    ∥   F  2   }     subject to   ∀  i    ,   x  i    =    e  k   for some  k    .         subscript    D  X     subscript   superscript   norm    Y    D  X     2   F      subject to   for-all  i     subscript  x  i       subscript  e  k   for some  k     \quad\min\limits_{D,X}\{\|Y-DX\|^{2}_{F}\}\qquad\text{subject to }\forall i,x_%
 {i}=e_{k}\text{ for some }k.     which is quite similar to          min    D  ,  X     {    ∥   Y  -   D  X    ∥   F  2   }    subject to   ∀  i   ,    ∥   x  i   ∥   0    =  1.         subscript    D  X     subscript   superscript   norm    Y    D  X     2   F    subject to   for-all  i    subscript   norm   subscript  x  i    0    1.    \quad\min\limits_{D,X}\{\|Y-DX\|^{2}_{F}\}\qquad\text{subject to }\quad\forall
 i%
 ,\|x_{i}\|_{0}=1.     The sparse representation term     x  i   =   e  k        subscript  x  i    subscript  e  k     x_{i}=e_{k}   enforces K-means algorithm to use only one atom (column) in dictionary D. To relax this constraint, the target of the K-SVD algorithm is to represent signal as a linear combination of atoms in D.  The K-SVD algorithm follows the construction flow of K-means algorithm. However, In contrary to K-means, in order to achieve linear combination of atoms in D, sparsity term of the constrain is relaxed so that nonzero entries of each column    x  i     subscript  x  i    x_{i}   can be more than 1, but less than a number    T  0     subscript  T  0    T_{0}   .  So, the objective function becomes           min    D  ,  X     {    ∥   Y  -   D  X    ∥   F  2   }    subject to   ∀   i    ,    ∥   x  i   ∥   0    ≤   T  0    .         subscript    D  X     subscript   superscript   norm    Y    D  X     2   F    subject to   for-all  i    subscript   norm   subscript  x  i    0     subscript  T  0     \quad\min\limits_{D,X}\{\|Y-DX\|^{2}_{F}\}\qquad\text{subject to }\quad\forall
 i%
 \;,\|x_{i}\|_{0}\leq T_{0}.     or in another objective form           min    D  ,  X      ∑  i     ∥   x  i   ∥   0     subject to   ∀   i    ,    ∥   Y  -   D  X    ∥   F  2    ≤  ϵ   .          subscript    D  X      subscript   i    subscript   norm   subscript  x  i    0     subject to   for-all  i    subscript   superscript   norm    Y    D  X     2   F    ϵ    \quad\min\limits_{D,X}\sum_{i}\|x_{i}\|_{0}\qquad\text{subject to }\quad%
 \forall i\;,\|Y-DX\|^{2}_{F}\leq\epsilon.     In the K-SVD algorithm, the   D   D   D   is first to be fixed and the best coefficient matrix   X   X   X   . As finding the truly optimal   X   X   X   is impossible, we use an approximation pursuit method. Any such algorithm as OMP, the orthogonal matching pursuit in can be used for the calculation of the coefficients, as long as it can supply a solution with a fixed and predetermined number of nonzero entries    T  0     subscript  T  0    T_{0}   .  After the sparse coding task, the next is to search for a better dictionary   D   D   D   . However, finding the whole dictionary all at a time is impossible, so the process then update only one column of the dictionary   D   D   D   each time while fix   X   X   X   . The update of    k  -   t  h       k    t  h     k-th   is done by rewriting the penalty term as        ∥   Y  -   D  X    ∥   F  2   =    |   Y  -    ∑   j  =  1   K     d  j    x  T  j      |   F  2   =    |    (   Y  -    ∑   j  ≠  k      d  j    x  T  j      )   -    d  k    x  T  k     |   F  2   =    ∥    E  k   -    d  k    x  T  k     ∥   F  2          subscript   superscript   norm    Y    D  X     2   F    subscript   superscript      Y    superscript   subscript     j  1    K      subscript  d  j    subscript   superscript  x  j   T       2   F         subscript   superscript        Y    subscript     j  k       subscript  d  j    subscript   superscript  x  j   T         subscript  d  k    subscript   superscript  x  k   T      2   F         subscript   superscript   norm     subscript  E  k      subscript  d  k    subscript   superscript  x  k   T      2   F      \|Y-DX\|^{2}_{F}=\left|Y-\sum_{j=1}^{K}d_{j}x^{j}_{T}\right|^{2}_{F}=\left|%
 \left(Y-\sum_{j\neq k}d_{j}x^{j}_{T}\right)-d_{k}x^{k}_{T}\right|^{2}_{F}=\|E_%
 {k}-d_{k}x^{k}_{T}\|^{2}_{F}     where    x  T  k     subscript   superscript  x  k   T    x^{k}_{T}   denotes the k -th row of X .  By decomposing the multiplication    D  X      D  X    DX   into sum of   K   K   K   rank 1 matrices, we can assume the other    K  -  1      K  1    K-1   terms are assumed fixed, and the    k  -   t  h       k    t  h     k-th   remains unknown. After this step, we can solve the minimization problem by approximate the    E  k     subscript  E  k    E_{k}   term with a     r  a  n  k   -  1        r  a  n  k   1    rank-1   matrix using singular value decomposition , then update    d  k     subscript  d  k    d_{k}   with it. However, the new solution of vector    x  T  k     subscript   superscript  x  k   T    x^{k}_{T}   is very likely to be filled, because the sparsity constrain is not enforced.  To cure this problem, Define    ω  k     subscript  ω  k    \omega_{k}   as        ω  k   =   {  i  ∣    1  ≤  i  ≤  N   ,     x  T  k    (  i  )    ≠  0    }    .       subscript  ω  k    conditional-set  i   formulae-sequence      1  i       N         subscript   superscript  x  k   T   i   0       \omega_{k}=\{i\mid 1\leq i\leq N,x^{k}_{T}(i)\neq 0\}.     Which points to examples    {   y  i   }      subscript  y  i     \{y_{i}\}   that use atom    d  k     subscript  d  k    d_{k}   (also the entries of    x  i     subscript  x  i    x_{i}   that is nonzero). Then, define    Ω  k     subscript  normal-Ω  k    \Omega_{k}   as a matrix of size    N  ×   |   ω  k   |       N     subscript  ω  k      N\times|\omega_{k}|   , with ones on the    (   i  -th   ,    ω  k    (  i  )    )       i  -th      subscript  ω  k   i     (i\text{-th},\omega_{k}(i))   entries and zeros otherwise. When multiplying     x  R  k   =    x  T  k    Ω  k         superscript   subscript  x  R   k      subscript   superscript  x  k   T    subscript  normal-Ω  k      x_{R}^{k}=x^{k}_{T}\Omega_{k}   , this shrinks the row vector    x  T  k     superscript   subscript  x  T   k    x_{T}^{k}   by discarding the nonzero entries. Similarly, the multiplication     Y  k  R   =   Y   Ω  k         subscript   superscript  Y  R   k     Y   subscript  normal-Ω  k      Y^{R}_{k}=Y\Omega_{k}   is the subset of the examples that are current using the    d  k     subscript  d  k    d_{k}   atom. The same effect can be seen on     E  k  R   =    E  k    Ω  k         subscript   superscript  E  R   k      subscript  E  k    subscript  normal-Ω  k      E^{R}_{k}=E_{k}\Omega_{k}   .  So the minimization problem as mentioned before becomes        ∥     E  k    Ω  k    -    d  k    x  T  k    Ω  k     ∥   F  2   =    ∥    E  k  R   -    d  k    x  R  k     ∥   F  2        subscript   superscript   norm       subscript  E  k    subscript  normal-Ω  k       subscript  d  k    subscript   superscript  x  k   T    subscript  normal-Ω  k      2   F    subscript   superscript   norm     subscript   superscript  E  R   k      subscript  d  k    subscript   superscript  x  k   R      2   F     \|E_{k}\Omega_{k}-d_{k}x^{k}_{T}\Omega_{k}\|^{2}_{F}=\|E^{R}_{k}-d_{k}x^{k}_{R%
 }\|^{2}_{F}     and can be done by directly using SVD. SVD decomposes    E  k  R     subscript   superscript  E  R   k    E^{R}_{k}   into    U  Δ   V  T       U  normal-Δ   superscript  V  T     U\Delta V^{T}   . The solution for    d  k     subscript  d  k    d_{k}   is the first column of U, the coefficient vector    x  R  k     subscript   superscript  x  k   R    x^{k}_{R}   as the first column of     V  ×  Δ    (  1  ,  1  )         V  normal-Δ    1  1     V\times\Delta(1,1)   . After updated the whole dictionary, the process then turns to iteratively solve X, then iteratively solve D.  Limitations  Choosing an appropriate "dictionary" for a dataset is a non-convex problem, and K-SVD operates by an iterative update which does not guarantee to find the global optimum. 3 However, this is common to other algorithms for this purpose, and K-SVD works fairly well in practice. 4  See also   Sparse approximation  Singular value decomposition  Matrix norm  K-means clustering   References    "  Category:Norms (mathematics)  Category:Linear algebra  Category:Data clustering algorithms     ↩  ↩       