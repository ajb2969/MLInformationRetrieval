   Random effects model      Random effects model   In statistics , a random effect(s) model , also called a variance components model , is a kind of hierarchical linear model . It assumes that the dataset being analysed consists of a hierarchy of different populations whose differences relate to that hierarchy. In econometrics , random effects models are used in the analysis of hierarchical or panel data when one assumes no fixed effects (it allows for individual effects). The random effects model is a special case of the fixed effects model . Contrast this to the biostatistics definitions, 1 2 3 4 as biostatisticians use "fixed" and "random" effects to respectively refer to the population-average and subject-specific effects (and where the latter are generally assumed to be unknown, latent variables ).  Qualitative description  Such models assist in controlling for unobserved heterogeneity when this heterogeneity is constant over time and correlated with independent variables. This constant can be removed from the data through differencing, for example by taking a first difference which will remove any time invariant components of the model.  There are two common assumptions made about the individual specific effect, the random effects assumption and the fixed effects assumption. The random effects assumption (made in a random effects model ) is that the individual specific effects are uncorrelated with the independent variables. The fixed effect assumption is that the individual specific effect is correlated with the independent variables. If the random effects assumption holds, the random effects model is more efficient than the fixed effects model. However, if this assumption does not hold, the random effects model is not consistent .  Simple example  Suppose m large elementary schools are chosen randomly from among thousands in a large country. Suppose also that n pupils of the same age are chosen randomly at each selected school. Their scores on a standard aptitude test are ascertained. Let Y ij be the score of the j th pupil at the i th school. A simple way to model the relationships of these quantities is        Y   i  j    =   μ  +   U  i   +   W   i  j      ,       subscript  Y    i  j      μ   subscript  U  i    subscript  W    i  j       Y_{ij}=\mu+U_{i}+W_{ij},\,   where μ is the average test score for the entire population. In this model U i is the school-specific random effect : it measures the difference between the average score at school i and the average score in the entire country and it is "random" because the school has been randomly selected from a larger population of schools. The term, W ij is the individual-specific error. That is, it is the deviation of the j -th pupil’s score from the average for the i -th school. Again this is regarded as random because of the random selection of pupils within the school, even though it is a fixed quantity for any given pupil.  The model can be augmented by including additional explanatory variables, which would capture differences in scores among different groups. For example:        Y   i  j    =   μ  +    β  1    Sex   i  j     +    β  2    Race   i  j     +    β  3    ParentsEduc   i  j     +   U  i   +   W   i  j      ,       subscript  Y    i  j      μ     subscript  β  1    subscript  Sex    i  j        subscript  β  2    subscript  Race    i  j        subscript  β  3    subscript  ParentsEduc    i  j      subscript  U  i    subscript  W    i  j       Y_{ij}=\mu+\beta_{1}\mathrm{Sex}_{ij}+\beta_{2}\mathrm{Race}_{ij}+\beta_{3}%
 \mathrm{ParentsEduc}_{ij}+U_{i}+W_{ij},\,   where Sex ij is the dummy variable for boys/girls, Race ij is the dummy variable for white/black pupils, and ParentsEduc ij records the average education level of child’s parents. This is a mixed model , not a purely random effects model.  Variance components  The variance of Y ij is the sum of the variances τ 2 and σ 2 of U i and W ij respectively.  Let        Y  ¯    i  ∙    =    1  n     ∑   j  =  1   n    Y   i  j           subscript   normal-¯  Y    i  normal-∙        1  n     superscript   subscript     j  1    n    subscript  Y    i  j        \overline{Y}_{i\bullet}=\frac{1}{n}\sum_{j=1}^{n}Y_{ij}   be the average, not of all scores at the i th school, but of those at the i th school that are included in the random sample . Let        Y  ¯     ∙  ∙    =    1   m  n      ∑   i  =  1   m     ∑   j  =  1   n    Y   i  j            subscript   normal-¯  Y    normal-∙  absent  normal-∙        1    m  n      superscript   subscript     i  1    m     superscript   subscript     j  1    n    subscript  Y    i  j         \overline{Y}_{\bullet\bullet}=\frac{1}{mn}\sum_{i=1}^{m}\sum_{j=1}^{n}Y_{ij}     be the "grand average".  Let       S  S  W   =    ∑   i  =  1   m     ∑   j  =  1   n      (    Y   i  j    -    Y  ¯    i  ∙     )   2            S  S  W     superscript   subscript     i  1    m     superscript   subscript     j  1    n    superscript     subscript  Y    i  j     subscript   normal-¯  Y    i  normal-∙     2       SSW=\sum_{i=1}^{m}\sum_{j=1}^{n}(Y_{ij}-\overline{Y}_{i\bullet})^{2}\,          S  S  B   =   n    ∑   i  =  1   m      (     Y  ¯    i  ∙    -    Y  ¯     ∙  ∙     )   2            S  S  B     n    superscript   subscript     i  1    m    superscript     subscript   normal-¯  Y    i  normal-∙     subscript   normal-¯  Y    normal-∙  absent  normal-∙     2       SSB=n\sum_{i=1}^{m}(\overline{Y}_{i\bullet}-\overline{Y}_{\bullet\bullet})^{2}\,     be respectively the sum of squares due to differences within groups and the sum of squares due to difference between groups. Then it can be shown that        1   m   (   n  -  1   )     E   (   S  S  W   )    =   σ  2           1    m    n  1     E    S  S  W     superscript  σ  2     \frac{1}{m(n-1)}E(SSW)=\sigma^{2}     and         1    (   m  -  1   )   n    E   (   S  S  B   )    =     σ  2   n   +   τ  2     .          1      m  1   n    E    S  S  B         superscript  σ  2   n    superscript  τ  2      \frac{1}{(m-1)n}E(SSB)=\frac{\sigma^{2}}{n}+\tau^{2}.     These " expected mean squares " can be used as the basis for estimation of the "variance components" σ 2 and τ 2 .  Unbiasedness  In general, random effects are efficient, and should be used (over fixed effects) if the assumptions underlying them are believed to be satisfied. For random effects to work in the school example it is necessary that the school-specific effects be uncorrelated to the other covariates of the model. This can be tested by running fixed effects, then random effects, and doing a Hausman specification test . If the test rejects, then random effects is biased and fixed effects is the correct estimation procedure.  See also   Bühlmann model  Hierarchical linear modeling  Fixed effects  MINQUE   Notes  Further reading       External links   Fixed and random effects models  How to Conduct a Meta-Analysis: Fixed and Random Effect Models   "  Category:Statistical models  Category:Analysis of variance     ↩  ↩  ↩  ↩     