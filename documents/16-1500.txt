   Hyper basis function network      Hyper basis function network   In machine learning , a Hyper basis function network , or HyperBF network , is a generalization of radial basis function (RBF) networks concept, where the Mahalanobis -like distance is used instead of Euclidean distance measure. Hyper basis function networks were first introduced by Poggio and Girosi in the 1990 paper â€œNetworks for Approximation and Learningâ€. 1 2  Network Architecture  The typical HyperBF network structure consists of a real input vector    x  âˆˆ   â„  n       x   superscript  â„  n     x\in\mathbb{R}^{n}   , a hidden layer of activation functions and a linear output layer. The output of the network is a scalar function of the input vector,    Ï•  :    â„  n   â†’  â„      normal-:  Ï•   normal-â†’   superscript  â„  n   â„     \phi:\mathbb{R}^{n}\to\mathbb{R}   , is given by        Ï•   (  x  )    =    âˆ‘   j  =  1   N     a  j    Ï  j    (   ||   x  -   Î¼  j    ||   )           Ï•  x     superscript   subscript     j  1    N      subscript  a  j    subscript  Ï  j    norm    x   subscript  Î¼  j         \phi(x)=\sum_{j=1}^{N}a_{j}\rho_{j}(||x-\mu_{j}||)      where   N   N   N   is a number of neurons in the hidden layer,    Î¼  j     subscript  Î¼  j    \mu_{j}   and    a  j     subscript  a  j    a_{j}   are the center and weight of neuron   j   j   j   . The activation function      Ï  j    (   ||   x  -   Î¼  j    ||   )        subscript  Ï  j    norm    x   subscript  Î¼  j       \rho_{j}(||x-\mu_{j}||)   at the HyperBF network takes the following form         Ï  j    (   ||   x  -   Î¼  j    ||   )    =   e     (   x  -   Î¼  j    )   T    R  j    (   x  -   Î¼  j    )            subscript  Ï  j    norm    x   subscript  Î¼  j       superscript  e     superscript    x   subscript  Î¼  j    T    subscript  R  j     x   subscript  Î¼  j        \rho_{j}(||x-\mu_{j}||)=e^{(x-\mu_{j})^{T}R_{j}(x-\mu_{j})}      where    R  j     subscript  R  j    R_{j}   is a positive definite    d  Ã—  d      d  d    d\times d   matrix. Depending on the application, the following types of matrices    R  j     subscript  R  j    R_{j}   are usually considered 3        R  j   =    1   2   Ïƒ  2      ð•€   d  Ã—  d          subscript  R  j       1    2   superscript  Ïƒ  2      subscript  ð•€    d  d       R_{j}=\frac{1}{2\sigma^{2}}\mathbb{I}_{d\times d}   , where    Ïƒ  >  0      Ïƒ  0    \sigma>0   . This case corresponds to the regular RBF network.       R  j   =    1   2   Ïƒ  j  2      ð•€   d  Ã—  d          subscript  R  j       1    2   superscript   subscript  Ïƒ  j   2      subscript  ð•€    d  d       R_{j}=\frac{1}{2\sigma_{j}^{2}}\mathbb{I}_{d\times d}   , where     Ïƒ  j   >  0       subscript  Ïƒ  j   0    \sigma_{j}>0   . In this case, the basis functions are radially symmetric, but are scaled with different width.       R  j   =   d  i  a  g   (   1   2   Ïƒ   j  1   2     ,  â€¦  ,   1   2   Ïƒ   j  z   2     )    ð•€   d  Ã—  d          subscript  R  j     d  i  a  g     1    2   superscript   subscript  Ïƒ    j  1    2     normal-â€¦    1    2   superscript   subscript  Ïƒ    j  z    2       subscript  ð•€    d  d       R_{j}=diag\left(\frac{1}{2\sigma_{j1}^{2}},...,\frac{1}{2\sigma_{jz}^{2}}%
 \right)\mathbb{I}_{d\times d}   , where     Ïƒ   j  i    >  0       subscript  Ïƒ    j  i    0    \sigma_{ji}>0   . Every neuron has an elliptic shape with a varying size.  Positive definite matrix, but not diagonal.   Training  Training HyperBF networks involves estimation of weights    a  j     subscript  a  j    a_{j}   , shape and centers of neurons    R  j     subscript  R  j    R_{j}   and    Î¼  j     subscript  Î¼  j    \mu_{j}   . Poggio and Girosi (1990) describe the training method with moving centers and adaptable neuron shapes. The outline of the method is provided below.  Consider the quadratic loss of the network     H   [   Ï•  *   ]    =    âˆ‘   i  =  1   N     (    y  i   -    Ï•  *    (   x  i   )     )   2          H   delimited-[]   superscript  Ï•        superscript   subscript     i  1    N    superscript     subscript  y  i      superscript  Ï•     subscript  x  i     2      H[\phi^{*}]=\sum_{i=1}^{N}(y_{i}-\phi^{*}(x_{i}))^{2}   . The following conditions must be satisfied at the optimum:          âˆ‚  H    (   Ï•  *   )     âˆ‚   a  j     =  0            H    superscript  Ï•        subscript  a  j     0    \frac{\partial H(\phi^{*})}{\partial a_{j}}=0   ,       âˆ‚  H    (   Ï•  *   )     âˆ‚   Î¼  j     =  0            H    superscript  Ï•        subscript  Î¼  j     0    \frac{\partial H(\phi^{*})}{\partial\mu_{j}}=0   ,       âˆ‚  H    (   Ï•  *   )     âˆ‚  W    =  0            H    superscript  Ï•       W    0    \frac{\partial H(\phi^{*})}{\partial W}=0      where     R  j   =    W  T   W        subscript  R  j      superscript  W  T   W     R_{j}=W^{T}W   . Then in the gradient descent method the values of     a  j   ,   Î¼  j   ,  W      subscript  a  j    subscript  Î¼  j   W    a_{j},\mu_{j},W   that minimize    H   [   Ï•  *   ]       H   delimited-[]   superscript  Ï•       H[\phi^{*}]   can be found as a stable fixed point of the following dynamic system:         a  j   Ë™   =   -   Ï‰     âˆ‚  H    (   Ï•  *   )     âˆ‚   a  j            normal-Ë™   subscript  a  j        Ï‰        H    superscript  Ï•        subscript  a  j         \dot{a_{j}}=-\omega\frac{\partial H(\phi^{*})}{\partial a_{j}}   ,      Î¼  j   Ë™   =   -   Ï‰     âˆ‚  H    (   Ï•  *   )     âˆ‚   Î¼  j            normal-Ë™   subscript  Î¼  j        Ï‰        H    superscript  Ï•        subscript  Î¼  j         \dot{\mu_{j}}=-\omega\frac{\partial H(\phi^{*})}{\partial\mu_{j}}   ,     W  Ë™   =   -   Ï‰     âˆ‚  H    (   Ï•  *   )     âˆ‚  W           normal-Ë™  W       Ï‰        H    superscript  Ï•       W        \dot{W}=-\omega\frac{\partial H(\phi^{*})}{\partial W}      where   Ï‰   Ï‰   \omega   determines the rate of convergence.  Overall, training HyperBF networks can be computationally challenging. Moreover, the high degree of freedom of HyperBF leads to overfitting and poor generalization. However, HyperBF networks have an important advantage that a small number of neurons is enough for learning complex functions. 4  References  "  Category:Artificial neural networks  Category:Classification algorithms  Category:Machine learning algorithms     T. Poggio and F. Girosi (1990). "Networks for Approximation and Learning". Proc. of the IEEE  Vol. 78, No. 9 :1481-1497. â†©  R.N. Mahdi, E.C. Rouchka (2011). "Reduced HyperBF Networks: Regularization by Explicit Complexity Reduction and Scaled Rprop-Based Training" . IEEE Transactions of Neural Networks  2 :673â€“686. â†©  F. Schwenker, H.A. Kestler and G. Palm (2001). "Three Learning Phases for Radial-Basis-Function Network" Neural Netw.  14 :439-458. â†©      