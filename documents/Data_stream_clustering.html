<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1356">Data stream clustering</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Data stream clustering</h1>
<hr/>

<p>In <a href="computer_science" title="wikilink">computer science</a>, <strong>data stream clustering</strong> is defined as the <a href="cluster_analysis" title="wikilink">clustering</a> of data that arrive continuously such as telephone records, multimedia data, financial transactions etc. Data stream clustering is usually studied as a <a href="streaming_algorithm" title="wikilink">streaming algorithm</a> and the objective is, given a sequence of points, to construct a good clustering of the stream, using a small amount of memory and time.</p>
<h2 id="history">History</h2>

<p>Data stream clustering has recently attracted attention for emerging applications that involve large amounts of streaming data. For clustering, <a href="k-means_clustering" title="wikilink">k-means</a> is a widely used heuristic but alternate algorithms have also been developed such as <a class="uri" href="k-medoids" title="wikilink">k-medoids</a>, <a href="CURE_data_clustering_algorithm" title="wikilink">CURE</a> and the popular <a href="BIRCH_(data_clustering)" title="wikilink">BIRCH</a>. For data streams, one of the first results appeared in 1980<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> but the model was formalized in 1998.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h2 id="definition">Definition</h2>

<p>The problem of data stream clustering is defined as:</p>

<p><strong>Input:</strong> a sequence of <em>n</em> points in metric space and an integer <em>k</em>.<br/>
<strong>Output:</strong> <em>k</em> centers in the set of the <em>n</em> points so as to minimize the sum of distances from data points to their closest cluster centers.</p>

<p>This is the streaming version of the k-median problem.</p>
<h2 id="algorithms">Algorithms</h2>
<h3 id="stream">STREAM</h3>

<p>STREAM is an algorithm for clustering data streams described by Guha, Mishra, Motwani and O'Callaghan<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> which achieves a <a href="approximation_algorithm" title="wikilink">constant factor approximation</a> for the k-Median problem in a single pass and using small space.</p>

<p>'<strong>'Theorem</strong>: STREAM can solve the <em>k</em>-Median problem on a data stream in a single pass, with time <em>O(n<sup>1+e</sup>)</em> and space <em>θ(n<sup>ε</sup>)</em> up to a factor <em>2<sup>O(1/e)</sup></em>, where <em>n</em> the number of points and ''e\ell pieces, clusters each one of them (using <em>k</em>-means) and then clusters the centers obtained.</p>

<p><a href="File:Small-Space.jpg" title="wikilink">thumb | 440x140px | right | Small-Space Algorithm representation</a></p>

<p><strong>Algorithm Small-Space(S)</strong></p>

<p>Where, if in Step 2 we run a bicriteria <em>(a,b)</em>-<a href="approximation_algorithm" title="wikilink">approximation algorithm</a> which outputs at most <em>ak</em> medians with cost at most <em>b</em> times the optimum k-Median solution and in Step 4 we run a <em>c</em>-approximation algorithm then the approximation factor of Small-Space() algorithm is <em>2c(1+2b)+2b</em>. We can also generalize Small-Space so that it recursively calls itself <em>i</em> times on a successively smaller set of weighted centers and achieves a constant factor approximation to the <em>k</em>-median problem.</p>

<p>The problem with the Small-Space is that the number of subsets 

<math display="inline" id="Data_stream_clustering:0">
 <semantics>
  <mi mathvariant="normal">ℓ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-ℓ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell
  </annotation>
 </semantics>
</math>

 that we partition <em>S</em> into is limited, since it has to store in memory the intermediate medians in <em>X</em>'. So, if <em>M</em> is the size of memory, we need to partition <em>S</em> into 

<math display="inline" id="Data_stream_clustering:1">
 <semantics>
  <mi mathvariant="normal">ℓ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-ℓ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell
  </annotation>
 </semantics>
</math>

 subsets such that each subset fits in memory, (n/

<math display="inline" id="Data_stream_clustering:2">
 <semantics>
  <mi mathvariant="normal">ℓ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-ℓ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell
  </annotation>
 </semantics>
</math>

) and so that the weighted 

<math display="inline" id="Data_stream_clustering:3">
 <semantics>
  <mi mathvariant="normal">ℓ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-ℓ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell
  </annotation>
 </semantics>
</math>

<em>k</em> centers also fit in memory, 

<math display="inline" id="Data_stream_clustering:4">
 <semantics>
  <mi mathvariant="normal">ℓ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-ℓ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell
  </annotation>
 </semantics>
</math>

<em>k<m< em="">. <math an="" but="" such="">\ell</math></m<></em></p>

<p>may not always exist.</p>

<p>The STREAM algorithm solves the problem of storing intermediate medians and achieves better running time and space requirements. The algorithm works as follows:<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> </p>
<h3 id="other-algorithms">Other Algorithms</h3>

<p>Other well-known algorithms used for data stream clustering are:</p>
<ul>
<li><a href="BIRCH_(data_clustering)" title="wikilink">BIRCH</a>:<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> builds a hierarchical data structure to incrementally cluster the incoming points using the available memory and minimizing the amount of I/O required. The complexity of the algorithm is <em>O(N)</em> since one pass suffices to get a good clustering (though, results can be improved by allowing several passes).</li>
<li><a href="Cobweb_(clustering)" title="wikilink">COBWEB</a>:<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> is an incremental clustering technique that keeps a hierarchical clustering model in the form of a <a href="Decision_tree_learning" title="wikilink">classification tree</a>. For each new point. COBWEB descends the tree, updates the nodes along the way and looks for the best node to put the point on (using a <a href="Category_utility" title="wikilink"> category utility function</a>).</li>
<li><a href="C2ICM(incremental_clustering)" title="wikilink">C2ICM</a>:<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> builds a flat partitioning clustering structure by selecting some objects as cluster seeds/initiators and a non-seed is assigned to the seed that provides the highest coverage, addition of new objects can introduce new seeds and falsify some existing old seeds, during incremental clustering new objects and the members of the falsified clusters are assigned to one of the existing new/old seeds.</li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Data_clustering_algorithms" title="wikilink">Category:Data clustering algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
</ol>
</section>
</body>
</html>
