


Draft:Hierarchical testing of variables in high-dimensional datasets




Draft:Hierarchical testing of variables in high-dimensional datasets


In statistics, hierarchical testing of variables in high-dimensional datasets, where the number of parameters is much greater than the number of observations (p ≫ n), can be used in calculating the  p-values for the significance of parameter clusters in predicting a response variable. The procedure outlined below calculates the p-values for testing the group of  null hypotheses $H_{0,C}$, where $C$ is a cluster of highly correlated parameters in the hierarchical structure $\tau$. The p-value calculation procedure contains four parts: hierarchical clustering, screening variables, testing and multiplicity adjustment, and aggregation and hierarchical adjustment.
Clustering
First, the parameters are grouped in hierarchical clusters using correlation between variables as the distance metric. Other clustering methods and distance measurements may be used as desired.
Variable screening
Next, a limited set of predictors are selected from the data. The data are randomly divided into two groups with equal sample size, with data from the first group, $N_\text{in}^{(b)}$, used for parameter selection with  lasso or other parameter selection procedure of choice. If the data have an odd number of observations, the group with fewer observations is used for variable selection. The data splitting occurs $B$ times, leading to selected variable sets $\hat{S}^{(1)},\ldots, \hat{S}^{(B)}$.
Testing and multiplicity adjustment
After screening variables, the second half of the data split, $N_\text{out}^{(b)}$, is used in generating p-values for the clusters as follows. If a given cluster $C$ contains at least one variable in the selected group $\hat{S}^{(b)}$, then the second half of the data split is utilized for a partial F test, using the  linear model generated with the selected set as the full model and the model with the intersection of the selected set and the cluster as the tested model.
$$p^{C,(b)} = \begin{cases}
    \displaystyle p_\text{partial F-test}^{C\ \cap\ S^{(b)}}  \text{ based on } \mathbf{Y}_{N_\text{out}^{(b)}}, \mathbf{X}_{N_\text{out}^{(b)}, \hat{S}^{(b)}}, & \text{if } C \cap \hat{S}^{(b)} \ne{} 0\\
        \displaystyle 1, & \text{if } C \cap \hat{S}^{(b)} = 0
     \end{cases}$$
The p-values are then multiplied by a ratio of the cardinality of the selected set to the cardinality of the intersection of the selected set and the cluster – as the result can sometimes be greater than one, the minimum of the adjusted p-value and 1 is taken.
$$p_\textrm{adj}^{C,(b)} = \min \left(1,  p^{C,{(b)}} \frac{|\hat{S}^{(b)}|} {|C \cap \hat{S}^{(b)}|} \right)$$
Aggregation and hierarchical adjustment
To aggregate p-values for $B$ clusters, $\gamma$ quantiles are taken across $1,\ldots, B \ p_\textrm{adj} ^ {C,(b)} / \gamma$ for each cluster. The aggregated p-value is again upper bounded by 1 (the minimum of the quantiles and 1 is taken). In practice, $\gamma$ values may be taken from 0.05 to 1 in 0.025 size steps.
$$Q^c(\gamma) = \min \left( 1, q_\gamma(p_\textrm{adj}^{C,(b)} / \gamma;
 b=1,\ldots,B) \right)$$
To eliminate $\gamma$, the infimum of the aggregated p-values is taken for $\gamma  \in ( \gamma_{\textrm{min}}, 1)$ and adjusted by $(1-\log(\gamma_{\textrm{min}}))$, and the minimum of the resulting value and 1 is taken.
$$P^C = \min \left( 1, (1-\log(\gamma_\min)) \inf_{\gamma \in (\gamma_\min, 1)} Q^C(\gamma) \right)$$
Finally, the p-values are hierarchically adjusted such that the p-value of a cluster is always greater than the p-value of its ancestor cluster, resulting in increasing p-values with each smaller cluster in the tree.
$$P_h^C = \max_{D \in{\tau : C \subseteq{D}}} P^C$$ 1
References
"



Mandozzi, Jacopo; Buhlmann, Peter. (2014). "Hierarchical Testing in the High-Dimensional Setting with Correlated Variables." Journal of the American Statistical Association. DOI: 10.1080/01621459.2015.1007209.




