   Variation of information      Variation of information   In probability theory and information theory , the variation of information or shared information distance is a measure of the distance between two clusterings ( partitions of elements ). It is closely related to mutual information ; indeed, it is a simple linear expression involving the mutual information. Unlike the mutual information, however, the variation of information is a true metric , in that it obeys the triangle inequality . Even more, it is a universal metric , in that if any other distance measure two items close-by, then the variation of information will also judge them close. 1  Background  Definition  Suppose we have two partitions    X   X   X   and   Y   Y   Y   of a set    A   A   A   into disjoint subsets , namely    X  =   {   X  1   ,   X  2   ,  .  .  ,  ,   X  k   }      fragments  X    fragments  normal-{   subscript  X  1   normal-,   subscript  X  2   normal-,  normal-.  normal-.  normal-,  normal-,   subscript  X  k   normal-}     X=\{X_{1},X_{2},..,,X_{k}\}   ,    Y  =   {   Y  1   ,   Y  2   ,  .  .  ,  ,   Y  l   }      fragments  Y    fragments  normal-{   subscript  Y  1   normal-,   subscript  Y  2   normal-,  normal-.  normal-.  normal-,  normal-,   subscript  Y  l   normal-}     Y=\{Y_{1},Y_{2},..,,Y_{l}\}   . Let    n  =    Σ  i    |   X  i   |    =    Σ  j    |   Y  j   |    =   |  A  |         n     subscript  normal-Σ  i      subscript  X  i             subscript  normal-Σ  j      subscript  Y  j            A      n=\Sigma_{i}|X_{i}|=\Sigma_{j}|Y_{j}|=|A|   ,     p  i   =    |   X  i   |   /  n        subscript  p  i        subscript  X  i    n     p_{i}=|X_{i}|/n   ,     q  j   =    |   Y  j   |   /  n        subscript  q  j        subscript  Y  j    n     q_{j}=|Y_{j}|/n   ,     r   i  j    =    |    X  i   ∩   Y  j    |   /  n        subscript  r    i  j           subscript  X  i    subscript  Y  j     n     r_{ij}=|X_{i}\cap Y_{j}|/n   . Then the variation of information between the two partitions is:       V  I   (  X  ;  Y  )    =   -    ∑   i  ,  j      r   i  j     [    log   (    r   i  j    /   p  i    )    +   log   (    r   i  j    /   q  j    )     ]            V  I   X  Y        subscript    i  j       subscript  r    i  j     delimited-[]         subscript  r    i  j     subscript  p  i          subscript  r    i  j     subscript  q  j            VI(X;Y)=-\sum_{i,j}r_{ij}\left[\log(r_{ij}/p_{i})+\log(r_{ij}/q_{j})\right]   .  This is equivalent to the shared information distance between the random variables i and j with respect to the uniform probability measure on   A   A   A   defined by     μ   (  B  )    :=    |  B  |   /  n      assign    μ  B       B   n     \mu(B):=|B|/n   for    B  ⊆  A      B  A    B\subseteq A   . The variation of information satisfies       V  I   (  X  ;  Y  )    =     H   (  X  )    +   H   (  Y  )     -   2  I   (  X  ,  Y  )           V  I   X  Y          H  X     H  Y      2  I   X  Y       VI(X;Y)=H(X)+H(Y)-2I(X,Y)   .  where    H   (  X  )       H  X    H(X)   is the entropy of   X   X   X   , and    I   (  X  ,  Y  )       I   X  Y     I(X,Y)   is mutual information between   X   X   X   and   Y   Y   Y   with respect to the uniform probability measure on   A   A   A   .  References    Further reading        External links   C++ implementation with MATLAB mex files   "  Category:Entropy and information     Alexander Kraskov, Harald Stögbauer, Ralph G. Andrzejak, and Peter Grassberger , "Hierarchical Clustering Based on Mutual Information", (2003) ArXiv q-bio/0311039 ↩     