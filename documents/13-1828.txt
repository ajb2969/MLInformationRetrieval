   Uncertainty coefficient      Uncertainty coefficient   In statistics , the uncertainty coefficient , also called proficiency , entropy coefficient or Theil's U , is a measure of nominal association . It was first introduced by Henri Theil and is based on the concept of information entropy .  Definition  Suppose we have samples of two discrete random variables, X and Y . By constructing the joint distribution, P X,Y ( x , y ), from which we can calculate the conditional distributions , P X|Y ( x | y ) = P X,Y ( x , y )/ P Y ( y ) and P Y|X ( y | x ) = P X,Y ( x , y )/ P X ( x ), and calculating the various entropies, we can determine the degree of association between the two variables.  The entropy of a single distribution is given as: 1        H   (  X  )    =   -    ∑  x     P  X    (  x  )    log   P  X     (  x  )       ,        H  X       subscript   x      subscript  P  X   x     subscript  P  X    x       H(X)=-\sum_{x}P_{X}(x)\log P_{X}(x),     while the conditional entropy is given as: 2      H   (  X  |  Y  )   =  -   ∑   x  ,  y     P   X  ,  Y     (  x  ,  y  )   log   P   X  |  Y     (  x  |  y  )   .     fragments  H   fragments  normal-(  X  normal-|  Y  normal-)      subscript    x  y     subscript  P   X  Y     fragments  normal-(  x  normal-,  y  normal-)     subscript  P   fragments  X  normal-|  Y     fragments  normal-(  x  normal-|  y  normal-)   normal-.    H(X|Y)=-\sum_{x,~{}y}P_{X,Y}(x,~{}y)\log P_{X|Y}(x|y).     The uncertainty coefficient 3 or proficiency 4 is defined as:      U   (  X  |  Y  )   =    H   (  X  )   -  H   (  X  |  Y  )     H   (  X  )     =    I   (  X  ;  Y  )     H   (  X  )     ,     fragments  U   fragments  normal-(  X  normal-|  Y  normal-)       fragments  H   fragments  normal-(  X  normal-)    H   fragments  normal-(  X  normal-|  Y  normal-)      H  X         I   X  Y      H  X    normal-,    U(X|Y)=\frac{H(X)-H(X|Y)}{H(X)}=\frac{I(X;Y)}{H(X)},     and tells us: given Y , what fraction of the bits of X can we predict? (The above expression makes clear that the uncertainty coefficient is a normalised mutual information  I(X;Y) .) In this case we can think of X as containing the "true" values. Note that the value of U (but not H !) is independent of the base of the log since all logarithms are proportional.  The uncertainty coefficient is useful for measuring the validity of a statistical classification algorithm and has the advantage over simpler accuracy measures such as precision and recall in that it is not affected by the relative fractions of the different classes, i.e., P ( x ) . 5 It also has the unique property that it won't penalize an algorithm for predicting the wrong classes, so long as it does so consistently (i.e., it simply rearranges the classes). This is useful in evaluating clustering algorithms since cluster labels typically have no particular ordering. 6  Variations  Symmetrised: The uncertainty coefficient is not symmetric with respect to the roles of X and Y . The roles can be reversed and a symmetrical measure thus defined as a weighted average between the two: 7      U   (  X  ,  Y  )       U   X  Y     \displaystyle U(X,~{}Y)     Continuous: Although normally applied to discrete variables, the uncertainty coefficient can be extended to continuous variables 8 using density estimation .  See also   Mutual information  Rand index  F1 score  Binary classification   References    External links   libagf Includes software for calculating uncertainty coefficients.   "  Category:Statistical dependence  Category:Statistical ratios  Category:Summary statistics for contingency tables  Category:Information theory  Category:Statistics articles needing expert attention     ↩   ↩  ↩  ↩        