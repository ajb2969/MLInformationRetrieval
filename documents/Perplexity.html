<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1627">Perplexity</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Perplexity</h1>
<hr/>

<p>In <a href="information_theory" title="wikilink">information theory</a>, <strong>perplexity</strong> is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models.</p>
<h2 id="perplexity-of-a-probability-distribution">Perplexity of a probability distribution</h2>

<p>The perplexity of a discrete <a href="probability_distribution" title="wikilink">probability distribution</a> <em>p</em> is defined as</p>

<p>

<math display="block" id="Perplexity:0">
 <semantics>
  <mrow>
   <msup>
    <mn>2</mn>
    <mrow>
     <mi>H</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>p</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </msup>
   <mo>=</mo>
   <msup>
    <mn>2</mn>
    <mrow>
     <mo>-</mo>
     <mrow>
      <mstyle displaystyle="false">
       <msub>
        <mo largeop="true" symmetric="true">∑</mo>
        <mi>x</mi>
       </msub>
      </mstyle>
      <mrow>
       <mi>p</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
       </mrow>
       <mrow>
        <msub>
         <mi>log</mi>
         <mn>2</mn>
        </msub>
        <mi>p</mi>
       </mrow>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cn type="integer">2</cn>
     <apply>
      <times></times>
      <ci>H</ci>
      <ci>p</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cn type="integer">2</cn>
     <apply>
      <minus></minus>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <ci>x</ci>
       </apply>
       <apply>
        <times></times>
        <ci>p</ci>
        <ci>x</ci>
        <apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <log></log>
          <cn type="integer">2</cn>
         </apply>
         <ci>p</ci>
        </apply>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   2^{H(p)}=2^{-\sum_{x}p(x)\log_{2}p(x)}
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>H</em>(<em>p</em>) is the <a href="entropy_(information_theory)" title="wikilink">entropy</a> of the distribution and <em>x</em> ranges over events.</p>

<p>Perplexity of a <a href="random_variable" title="wikilink">random variable</a> <em>X</em> may be defined as the perplexity of the distribution over its possible values <em>x</em>.</p>

<p>In the special case where <em>p</em> models a fair <em>k</em>-sided die (a uniform distribution over <em>k</em> discrete events), its perplexity is <em>k</em>. A random variable with perplexity <em>k</em> has the same uncertainty as a fair <em>k</em>-sided die, and one is said to be "<em>k</em>-ways perplexed" about the value of the random variable. (Unless it is a fair <em>k</em>-sided die, more than <em>k</em> values will be possible, but the overall uncertainty is no greater because some of these values will have probability greater than 1/<em>k</em>, decreasing the overall value while summing.)</p>
<h2 id="perplexity-of-a-probability-model">Perplexity of a probability model</h2>

<p>A model of an unknown probability distribution <em>p</em>, may be proposed based on a training sample that was drawn from <em>p</em>. Given a proposed probability model <em>q</em>, one may evaluate <em>q</em> by asking how well it predicts a separate test sample <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x<sub>N</sub></em> also drawn from <em>p</em>. The perplexity of the model <em>q</em> is defined as</p>

<p>

<math display="block" id="Perplexity:1">
 <semantics>
  <msup>
   <mi>b</mi>
   <mrow>
    <mo>-</mo>
    <mrow>
     <mfrac>
      <mn>1</mn>
      <mi>N</mi>
     </mfrac>
     <mrow>
      <mstyle displaystyle="false">
       <msubsup>
        <mo largeop="true" symmetric="true">∑</mo>
        <mrow>
         <mi>i</mi>
         <mo>=</mo>
         <mn>1</mn>
        </mrow>
        <mi>N</mi>
       </msubsup>
      </mstyle>
      <mrow>
       <mrow>
        <msub>
         <mi>log</mi>
         <mi>b</mi>
        </msub>
        <mi>q</mi>
       </mrow>
       <mrow>
        <mo stretchy="false">(</mo>
        <msub>
         <mi>x</mi>
         <mi>i</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>b</ci>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>N</ci>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>N</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <log></log>
          <ci>b</ci>
         </apply>
         <ci>q</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b^{-\frac{1}{N}\sum_{i=1}^{N}\log_{b}q(x_{i})}
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Perplexity:2">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>

 is customarily 2. Better models <em>q</em> of the unknown distribution <em>p</em> will tend to assign higher probabilities <em>q</em>(<em>x<sub>i</sub></em>) to the test events. Thus, they have lower perplexity: they are less surprised by the test sample.</p>

<p>The exponent above may be regarded as the average number of bits needed to represent a test event <em>x<sub>i</sub></em> if one uses an optimal code based on <em>q</em>. Low-perplexity models do a better job of compressing the test sample, requiring few bits per test element on average because <em>q</em>(<em>x<sub>i</sub></em>) tends to be high.</p>

<p>The exponent may also be regarded as a <a class="uri" href="cross-entropy" title="wikilink">cross-entropy</a>,</p>

<p>

<math display="block" id="Perplexity:3">
 <semantics>
  <mrow>
   <mrow>
    <mi>H</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mover accent="true">
      <mi>p</mi>
      <mo stretchy="false">~</mo>
     </mover>
     <mo>,</mo>
     <mi>q</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <munder>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mi>x</mi>
     </munder>
     <mrow>
      <mover accent="true">
       <mi>p</mi>
       <mo stretchy="false">~</mo>
      </mover>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
      <mrow>
       <msub>
        <mi>log</mi>
        <mn>2</mn>
       </msub>
       <mi>q</mi>
      </mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>H</ci>
     <interval closure="open">
      <apply>
       <ci>normal-~</ci>
       <ci>p</ci>
      </apply>
      <ci>q</ci>
     </interval>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <ci>x</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <ci>normal-~</ci>
        <ci>p</ci>
       </apply>
       <ci>x</ci>
       <apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <log></log>
         <cn type="integer">2</cn>
        </apply>
        <ci>q</ci>
       </apply>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(\tilde{p},q)=-\sum_{x}\tilde{p}(x)\log_{2}q(x)
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Perplexity:4">
 <semantics>
  <mover accent="true">
   <mi>p</mi>
   <mo stretchy="false">~</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-~</ci>
    <ci>p</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tilde{p}
  </annotation>
 </semantics>
</math>

 denotes the empirical distribution of the test sample (i.e., 

<math display="inline" id="Perplexity:5">
 <semantics>
  <mrow>
   <mrow>
    <mover accent="true">
     <mi>p</mi>
     <mo stretchy="false">~</mo>
    </mover>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>n</mi>
    <mo>/</mo>
    <mi>N</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <ci>normal-~</ci>
      <ci>p</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <apply>
     <divide></divide>
     <ci>n</ci>
     <ci>N</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tilde{p}(x)=n/N
  </annotation>
 </semantics>
</math>

 if <em>x</em> appeared <em>n</em> times in the test sample of size <em>N</em>).</p>
<h2 id="perplexity-per-word">Perplexity per word</h2>

<p>In <a href="natural_language_processing" title="wikilink">natural language processing</a>, perplexity is a way of evaluating <a href="language_model" title="wikilink">language models</a>. A language model is a probability distribution over entire sentences or texts.</p>

<p>Using the definition of perplexity for a probability model, one might find, for example, that the average sentence <em>x<sub>i</sub></em> in the test sample could be coded in 190 bits (i.e., the test sentences had an average log-probability of -190). This would give an enormous model perplexity of 2<sup>190</sup> per sentence. However, it is more common to normalize for sentence length and consider only the number of bits per word. Thus, if the test sample's sentences comprised a total of 1,000 words, and could be coded using a total of 7,950 bits, one could report a model perplexity of 2<sup>7.95</sup> = 247 <em>per word.</em> In other words, the model is as confused on test data as if it had to choose uniformly and independently among 247 possibilities for each word.</p>

<p>The lowest perplexity that has been published on the <a href="Brown_Corpus" title="wikilink">Brown Corpus</a> (1 million words of American <a href="English_language" title="wikilink">English</a> of varying topics and genres) as of 1992 is indeed about 247 per word, corresponding to a cross-entropy of log<sub>2</sub>247 = 7.95 bits per word or 1.75 bits per letter <a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> using a <a href="N-gram" title="wikilink">trigram</a> model. It is often possible to achieve lower perplexity on more specialized <a href="text_corpus" title="wikilink">corpora</a>, as they are more predictable.</p>
<h2 id="references">References</h2>
<references>
</references>

<p>"</p>

<p><a href="Category:Entropy_and_information" title="wikilink">Category:Entropy and information</a> <a href="Category:Language_modeling" title="wikilink">Category:Language modeling</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
</ol>
</section>
</body>
</html>
