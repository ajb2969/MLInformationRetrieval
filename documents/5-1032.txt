   Score test      Score test   Rao's score test , or the score test (often known as the Lagrange multiplier test in econometrics 1 ) is a statistical test of a simple  null hypothesis that a parameter of interest   θ   θ   \theta   is equal to some particular value    θ  0     subscript  θ  0    \theta_{0}   . It is the most powerful test when the true value of   θ   θ   \theta   is close to    θ  0     subscript  θ  0    \theta_{0}   . The main advantage of the Score-test is that it does not require an estimate of the information under the alternative hypothesis or unconstrained maximum likelihood. This makes testing feasible when the unconstrained maximum likelihood estimate is a boundary point in the parameter space.  Single parameter test  The statistic  Let   L   L   L   be the likelihood function which depends on a univariate parameter   θ   θ   \theta   and let   x   x   x   be the data. The score is    U   (  θ  )       U  θ    U(\theta)   where        U   (  θ  )    =    ∂  log  L   (  θ  |  x  )     ∂  θ     .        U  θ      fragments    L   fragments  normal-(  θ  normal-|  x  normal-)      θ      U(\theta)=\frac{\partial\log L(\theta|x)}{\partial\theta}.     The Fisher information is 2        ℐ   (  θ  )    =   -   E   [     ∂  2    ∂   θ  2      log  L    (  X  ;  θ  )    |  θ  ]      .        ℐ  θ      normal-E       superscript   2      superscript  θ  2       L    X  θ    θ      \mathcal{I}(\theta)=-\operatorname{E}\left[\left.\frac{\partial^{2}}{\partial%
 \theta^{2}}\log L(X;\theta)\right|\theta\right]\,.     The statistic to test     ℋ  0   :   θ  =   θ  0       normal-:   subscript  ℋ  0     θ   subscript  θ  0      \mathcal{H}_{0}:\theta=\theta_{0}   is     S   (   θ  0   )    =    U    (   θ  0   )   2     I   (   θ  0   )           S   subscript  θ  0        U   superscript   subscript  θ  0   2      I   subscript  θ  0       S(\theta_{0})=\frac{U(\theta_{0})^{2}}{I(\theta_{0})}     which has an asymptotic distribution of    χ  1  2     subscript   superscript  χ  2   1    \chi^{2}_{1}   , when    ℋ  0     subscript  ℋ  0    \mathcal{H}_{0}   is true.  Note on notation  Note that some texts use an alternative notation, in which the statistic      S  *    (  θ  )    =    S   (  θ  )            superscript  S    θ       S  θ      S^{*}(\theta)=\sqrt{S(\theta)}   is tested against a normal distribution. This approach is equivalent and gives identical results.  Justification  The case of a likelihood with nuisance parameters  As most powerful test for small deviations        (    ∂  log  L   (  θ  |  x  )     ∂  θ    )    θ  =   θ  0     ≥  C       subscript     fragments    L   fragments  normal-(  θ  normal-|  x  normal-)      θ      θ   subscript  θ  0     C    \left(\frac{\partial\log L(\theta|x)}{\partial\theta}\right)_{\theta=\theta_{0%
 }}\geq C   Where   L   L   L   is the likelihood function ,    θ  0     subscript  θ  0    \theta_{0}   is the value of the parameter of interest under the null hypothesis, and   C   C   C   is a constant set depending on the size of the test desired (i.e. the probability of rejecting    H  0     subscript  H  0    H_{0}   if    H  0     subscript  H  0    H_{0}   is true; see Type I error ).  The score test is the most powerful test for small deviations from    H  0     subscript  H  0    H_{0}   . To see this, consider testing    θ  =   θ  0       θ   subscript  θ  0     \theta=\theta_{0}   versus    θ  =    θ  0   +  h       θ     subscript  θ  0   h     \theta=\theta_{0}+h   . By the Neyman–Pearson lemma , the most powerful test has the form         L   (   θ  0   +  h  |  x  )     L   (   θ  0   |  x  )     ≥  K   ;         fragments  L   fragments  normal-(   subscript  θ  0    h  normal-|  x  normal-)     fragments  L   fragments  normal-(   subscript  θ  0   normal-|  x  normal-)     K    \frac{L(\theta_{0}+h|x)}{L(\theta_{0}|x)}\geq K;     Taking the log of both sides yields      log  L   (   θ  0   +  h  |  x  )   -  log  L   (   θ  0   |  x  )   ≥  log  K  .     fragments   L   fragments  normal-(   subscript  θ  0    h  normal-|  x  normal-)     L   fragments  normal-(   subscript  θ  0   normal-|  x  normal-)     K  normal-.    \log L(\theta_{0}+h|x)-\log L(\theta_{0}|x)\geq\log K.     The score test follows making the substitution (by Taylor series expansion)      log  L   (   θ  0   +  h  |  x  )   ≈  log  L   (   θ  0   |  x  )   +  h  ×    (    ∂  log  L   (  θ  |  x  )     ∂  θ    )    θ  =   θ  0        fragments   L   fragments  normal-(   subscript  θ  0    h  normal-|  x  normal-)     L   fragments  normal-(   subscript  θ  0   normal-|  x  normal-)    h    subscript   fragments  normal-(     fragments    L   fragments  normal-(  θ  normal-|  x  normal-)      θ    normal-)     θ   subscript  θ  0       \log L(\theta_{0}+h|x)\approx\log L(\theta_{0}|x)+h\times\left(\frac{\partial%
 \log L(\theta|x)}{\partial\theta}\right)_{\theta=\theta_{0}}     and identifying the   C   C   C   above with    log   (  K  )       K    \log(K)   .  Relationship with other hypothesis tests  The likelihood ratio test , the Wald test , and the Score test are asymptotically equivalent tests of hypotheses. 3 When testing nested models , the statistics for each test converge to a Chi-squared distribution with degrees of freedom equal to the difference in degrees of freedom in the two models.  Multiple parameters  A more general score test can be derived when there is more than one parameter. Suppose that     θ  ^   0     subscript   normal-^  θ   0    \hat{\theta}_{0}   is the maximum likelihood estimate of   θ   θ   \theta   under the null hypothesis    H  0     subscript  H  0    H_{0}   . Then        U  T    (    θ  ^   0   )    I   -  1     (    θ  ^   0   )   U   (    θ  ^   0   )    ∼   χ  k  2      similar-to     superscript  U  T    subscript   normal-^  θ   0    superscript  I    1     subscript   normal-^  θ   0   U   subscript   normal-^  θ   0     subscript   superscript  χ  2   k     U^{T}(\hat{\theta}_{0})I^{-1}(\hat{\theta}_{0})U(\hat{\theta}_{0})\sim\chi^{2}%
 _{k}     asymptotically under    H  0     subscript  H  0    H_{0}   , where   k   k   k   is the number of constraints imposed by the null hypothesis and       U   (    θ  ^   0   )    =    ∂  log  L   (    θ  ^   0   |  x  )     ∂  θ          U   subscript   normal-^  θ   0       fragments    L   fragments  normal-(   subscript   normal-^  θ   0   normal-|  x  normal-)      θ      U(\hat{\theta}_{0})=\frac{\partial\log L(\hat{\theta}_{0}|x)}{\partial\theta}     and        I   (    θ  ^   0   )    =   -   E   (     ∂  2   log  L   (    θ  ^   0   |  x  )      ∂  θ    ∂   θ  ′      )      .        I   subscript   normal-^  θ   0        E     fragments   superscript   2    L   fragments  normal-(   subscript   normal-^  θ   0   normal-|  x  normal-)        θ      superscript  θ  normal-′          I(\hat{\theta}_{0})=-E\left(\frac{\partial^{2}\log L(\hat{\theta}_{0}|x)}{%
 \partial\theta\partial\theta^{\prime}}\right).     This can be used to test    H  0     subscript  H  0    H_{0}   .  Special cases  In many situations, the score statistic reduces to another commonly used statistic. 4  When the data follows a normal distribution, the score statistic is the same as the t statistic .  When the data consists of binary observations, the score statistic is the same as the chi-squared statistic in the Pearson's chi-squared test .  When the data consists of failure time data in two groups, the score statistic for the Cox partial likelihood is the same as the log-rank statistic in the log-rank test . Hence the log-rank test for difference in survival between two groups is most powerful when the proportional hazards assumption holds.  See also   Fisher information  Uniformly most powerful test  Score (statistics)   References  "  Category:Statistical tests     Engle, Robert F (1984) . Wald, Likelihood Ratio and Lagrange Multiplier tests in Econometrics. in Handbook of Econometrics, Volume II, Edited by Z. Griliches and M.D. Intriligator. Elsevier Science Publishers BV. ↩  Lehmann and Casella, eq. (2.5.16). ↩  ↩  ↩     