   Empirical risk minimization      Empirical risk minimization   Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on the performance of learning algorithms .  Background  Consider the following situation, which is a general setting of many supervised learning problems. We have two spaces of objects   X   X   X   and   Y   Y   Y   and would like to learn a function     h   :   X  ‚Üí  Y      normal-:  h   normal-‚Üí  X  Y     \!h:X\to Y   (often called hypothesis ) which outputs an object    y  ‚àà  Y      y  Y    y\in Y   , given    x  ‚àà  X      x  X    x\in X   . To do so, we have at our disposal a training set of a few examples     (   x  1   ,   y  1   )   ,  ‚Ä¶  ,   (   x  m   ,   y  m   )        subscript  x  1    subscript  y  1    normal-‚Ä¶    subscript  x  m    subscript  y  m      \!(x_{1},y_{1}),\ldots,(x_{m},y_{m})   where     x  i   ‚àà  X       subscript  x  i   X    x_{i}\in X   is an input and     y  i   ‚àà  Y       subscript  y  i   Y    y_{i}\in Y   is the corresponding response that we wish to get from     h    (   x  i   )       h   subscript  x  i     \!h(x_{i})   .  To put it more formally, we assume that there is a joint probability distribution     P   (  x  ,  y  )       P   x  y     P(x,y)   over   X   X   X   and   Y   Y   Y   , and that the training set consists of   m   m   m   instances     (   x  1   ,   y  1   )   ,  ‚Ä¶  ,   (   x  m   ,   y  m   )        subscript  x  1    subscript  y  1    normal-‚Ä¶    subscript  x  m    subscript  y  m      \!(x_{1},y_{1}),\ldots,(x_{m},y_{m})   drawn i.i.d. from    P   (  x  ,  y  )       P   x  y     P(x,y)   . Note that the assumption of a joint probability distribution allows us to model uncertainty in predictions (e.g. from noise in data) because   y   y   y   is not a deterministic function of   x   x   x   , but rather a random variable with conditional distribution     P   (  y  |  x  )      fragments  P   fragments  normal-(  y  normal-|  x  normal-)     P(y|x)   for a fixed   x   x   x   .  We also assume that we are given a non-negative real-valued loss function     L   (   y  ^   ,  y  )       L    normal-^  y   y     L(\hat{y},y)   which measures how different the prediction    y  ^     normal-^  y    \hat{y}   of a hypothesis is from the true outcome   y   y   y   . The risk associated with hypothesis    h   (  x  )       h  x    h(x)   is then defined as the expectation of the loss function:        R   (  h  )    =   ùêÑ   [   L   (   h   (  x  )    ,  y  )    ]    =   ‚à´   L   (   h   (  x  )    ,  y  )   d  P   (  x  ,  y  )      .          R  h     ùêÑ   delimited-[]    L     h  x   y               L     h  x   y   d  P   x  y        R(h)=\mathbf{E}[L(h(x),y)]=\int L(h(x),y)\,dP(x,y).     A loss function commonly used in theory is the 0-1 loss function     L   (   y  ^   ,  y  )   =  I   (   y  ^   ‚â†  y  )      fragments  L   fragments  normal-(   normal-^  y   normal-,  y  normal-)    I   fragments  normal-(   normal-^  y    y  normal-)     L(\hat{y},y)=I(\hat{y}\neq y)   , where    I   (  ‚Ä¶  )       I  normal-‚Ä¶    I(...)   is the indicator notation .  The ultimate goal of a learning algorithm is to find a hypothesis    h  *     superscript  h     h^{*}   among a fixed class of functions   ‚Ñã   ‚Ñã   \mathcal{H}   for which the risk    R   (  h  )       R  h    R(h)   is minimal:        h  *   =    arg    min   h  ‚àà  ‚Ñã    R     (  h  )     .       superscript  h          subscript     h  ‚Ñã    R    h     h^{*}=\arg\min_{h\in\mathcal{H}}R(h).     Empirical risk minimization  In general, the risk    R   (  h  )       R  h    R(h)   cannot be computed because the distribution    P   (  x  ,  y  )       P   x  y     P(x,y)   is unknown to the learning algorithm (this situation is referred to as agnostic learning ). However, we can compute an approximation, called empirical risk , by averaging the loss function on the training set:          R   emp    (  h  )    =    1  m     ‚àë   i  =  1   m    L   (   h   (   x  i   )    ,   y  i   )       .         subscript  R  emp   h       1  m     superscript   subscript     i  1    m     L     h   subscript  x  i     subscript  y  i         \!R_{\mbox{emp}}(h)=\frac{1}{m}\sum_{i=1}^{m}L(h(x_{i}),y_{i}).     Empirical risk minimization principle states that the learning algorithm should choose a hypothesis    h  ^     normal-^  h    \hat{h}   which minimizes the empirical risk:        h  ^   =    arg    min   h  ‚àà  ‚Ñã     R  emp      (  h  )     .       normal-^  h         subscript     h  ‚Ñã     subscript  R  emp     h     \hat{h}=\arg\min_{h\in\mathcal{H}}R_{\mbox{emp}}(h).   Thus the learning algorithm defined by the ERM principle consists in solving the above optimization problem.  Properties  Computational complexity  Empirical risk minimization for a classification problem with 0-1 loss function is known to be an NP-hard problem even for such relatively simple class of functions as linear classifiers . 1 Though, it can be solved efficiently when minimal empirical risk is zero, i.e. data is linearly separable .  In practice, machine learning algorithms cope with that either by employing a convex approximation to 0-1 loss function (like hinge loss for SVM ), which is easier to optimize, or by posing assumptions on the distribution    P   (  x  ,  y  )       P   x  y     P(x,y)   (and thus stop being agnostic learning algorithms to which the above result applies,)  References  Literature     "  Category:Machine learning     V. Feldman, V. Guruswami, P. Raghavendra and Yi Wu (2009). Agnostic Learning of Monomials by Halfspaces is Hard. (See the paper and references therein) ‚Ü©     