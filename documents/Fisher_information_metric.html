<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="1228">Fisher information metric</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Fisher information metric</h1>
<hr/>
<p>In <a href="information_geometry" title="wikilink">information geometry</a>, the <strong>Fisher information metric</strong> is a particular <a href="Riemannian_metric" title="wikilink">Riemannian metric</a> which can be defined on a smooth <a href="statistical_manifold" title="wikilink">statistical manifold</a>, <em>i.e.</em>, a <a href="smooth_manifold" title="wikilink">smooth manifold</a> whose points are <a href="probability_measure" title="wikilink">probability measures</a> defined on a common <a href="probability_space" title="wikilink">probability space</a>. It can be used to calculate the informational difference between measurements.</p>
<p>The metric is interesting in several respects. First, it can be understood to be the infinitesimal form of the relative entropy (<em>i.e.</em>, the <a href="Kullback–Leibler_divergence" title="wikilink">Kullback–Leibler divergence</a>); specifically, it is the <a href="Hessian_matrix" title="wikilink">Hessian</a> of the divergence. Alternately, it can be understood as the metric induced by the flat space <a href="Euclidean_metric" title="wikilink">Euclidean metric</a>, after appropriate changes of variable. When extended to complex <a href="projective_Hilbert_space" title="wikilink">projective Hilbert space</a>, it becomes the <a href="Fubini–Study_metric" title="wikilink">Fubini–Study metric</a>; when written in terms of <a href="mixed_state_(physics)" title="wikilink">mixed states</a>, it is the quantum <a href="Bures_metric" title="wikilink">Bures metric</a>.</p>
<p>Considered purely as a matrix, it is known as the <a href="Fisher_information_matrix" title="wikilink">Fisher information matrix</a>. Considered as a measurement technique, where it is used to estimate hidden parameters in terms of observed random variables, it is known as the <a href="observed_information" title="wikilink">observed information</a>.</p>
<h2 id="definition">Definition</h2>
<p>Given a statistical manifold with coordinates <span class="LaTeX">$\theta=(\theta_1, \theta_2, \ldots, \theta_n)$</span>, one writes <span class="LaTeX">$p(x,\theta)$</span> for the probability distribution as a function of <span class="LaTeX">$\theta$</span>. Here <span class="LaTeX">$x$</span> is drawn from the value space <em>R</em> for a (discrete or continuous) <a href="random_variable" title="wikilink">random variable</a> <em>X</em>. The probability is normalized by <span class="LaTeX">$\int_R p(x,\theta) \,dx = 1$</span></p>
<p>The Fisher information metric then takes the form:</p>
<p><span class="LaTeX">$$g_{jk}(\theta)
=
\int_R
 \frac{\partial \log p(x,\theta)}{\partial \theta_j}
 \frac{\partial \log p(x,\theta)}{\partial \theta_k}
 p(x,\theta) \, dx.$$</span></p>
<p>The integral is performed over all values <em>x</em> in <em>R</em>. The variable <span class="LaTeX">$\theta$</span> is now a coordinate on a <a href="Riemann_manifold" title="wikilink">Riemann manifold</a>. The labels <em>j</em> and <em>k</em> index the local coordinate axes on the manifold.</p>
<p>When the probability is derived from the <a href="Gibbs_measure" title="wikilink">Gibbs measure</a>, as it would be for any <a href="Markovian_process" title="wikilink">Markovian process</a>, then <span class="LaTeX">$\theta$</span> can also be understood to be a <a href="Lagrange_multiplier" title="wikilink">Lagrange multiplier</a>; Lagrange multipliers are used to enforce constraints, such as holding the <a href="expectation_value" title="wikilink">expectation value</a> of some quantity constant. If there are <em>n</em> constraints holding <em>n</em> different expectation values constant, then the dimension of the manifold is <em>n</em> dimensions smaller than the original space. In this case, the metric can be explicitly derived from the <a href="partition_function_(mathematics)" title="wikilink">partition function</a>; a derivation and discussion is presented there.</p>
<p>Substituting <span class="LaTeX">$i = -\ln(p)$</span> from <a href="information_theory" title="wikilink">information theory</a>, an equivalent form of the above definition is:</p>
<p><span class="LaTeX">$$g_{jk}(\theta)
=
\int_X
 \frac{\partial^2 i(x,\theta)}{\partial \theta_j \,\partial \theta_k}
 p(x,\theta) \, dx
=
\mathrm{E}
\left[
 \frac{\partial^2 i(x,\theta)}{\partial \theta_j \,\partial \theta_k}
\right].$$</span></p>
<h2 id="relation-to-the-kullbackleibler-divergence">Relation to the Kullback–Leibler divergence</h2>
<p>Alternately, the metric can be obtained as the second derivative of the <em>relative entropy</em> or <a href="Kullback–Leibler_divergence" title="wikilink">Kullback–Leibler divergence</a>. To obtain this, one considers two probability distributions <span class="LaTeX">$P = P(\theta)$</span> and <span class="LaTeX">$Q = P(\theta_0)$</span>, which are infinitesimally close to one another, so that</p>
<p><span class="LaTeX">$$P = Q + \sum_j \Delta\theta^j Q_j$$</span></p>
<p>with <span class="LaTeX">$\Delta\theta^j$</span> an infinitesimally small change of <span class="LaTeX">$\theta$</span> in the <em>j</em> direction, and <span class="LaTeX">$Q_j = \left.\frac{\partial P}{\partial \theta^j}\right|_{\theta = \theta_0}$</span> the rate of change of the probability distribution. Then, since the Kullback–Leibler divergence <span class="LaTeX">$D_{\mathrm{KL}}(P\|Q)$</span> has an absolute minimum 0 for <em>P</em> = <em>Q</em> one has an expansion up to second order in <span class="LaTeX">$\theta = \theta_0$</span> of the form</p>
<p><span class="LaTeX">$$f_{\theta_0}(\theta) := D_{\mathrm{KL}}(P \| Q) = \frac{1}{2} \sum_{jk}\Delta\theta^j\Delta\theta^k g_{jk}(\theta_0)$$</span>.</p>
<p>The symmetric matrix <span class="LaTeX">$g_{jk}$</span> is positive (semi) definite and is the <a href="Hessian_matrix" title="wikilink">Hessian matrix</a> of the function <span class="LaTeX">$f_{\theta_0}$</span> at the stationary point <span class="LaTeX">$\theta_0$</span>. This can be thought of intuitively as: "The distance between two infinitesimally close points on a statistical differential manifold is the amount of information, i.e. the informational difference between them."</p>
<h2 id="relation-to-ruppeiner-geometry">Relation to Ruppeiner geometry</h2>
<p>The <a href="Ruppeiner_metric" title="wikilink">Ruppeiner metric</a> and <a href="Weinhold_metric" title="wikilink">Weinhold metric</a> arise as the <a href="thermodynamic_limit" title="wikilink">thermodynamic limit</a> of the Fisher information metric.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="change-in-entropy">Change in entropy</h2>
<p>The <a href="geodesic" title="wikilink">action</a> of a curve on a <a href="Riemannian_manifold" title="wikilink">Riemannian manifold</a> is given by <span class="LaTeX">$A=\frac{1}{2}\int_a^b 
\frac{\partial\theta^j}{\partial t}
g_{jk}(\theta)\frac{\partial\theta^k}{\partial t} dt$</span> The path parameter here is time <em>t</em>; this action can be understood to give the change in <a class="uri" href="entropy" title="wikilink">entropy</a> of a system as it is moved from time <em>a</em> to time <em>b</em>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Specifically, one has</p>
<p><span class="LaTeX">$$\Delta S = (b-a) A \,$$</span></p>
<p>as the change in entropy. This observation has resulted in practical applications in <a href="chemical_industry" title="wikilink">chemical</a> and <a href="processing_industry" title="wikilink">processing industry</a>: in order to minimize the change in entropy of a system, one should follow the minimum <a class="uri" href="geodesic" title="wikilink">geodesic</a> path between the desired endpoints of the process. The geodesic minimizes the entropy, due to the <a href="Cauchy–Schwarz_inequality" title="wikilink">Cauchy–Schwarz inequality</a>, which states that the action is bounded below by the length of the curve, squared.</p>
<h2 id="relation-to-the-jensenshannon-divergence">Relation to the Jensen–Shannon divergence</h2>
<p>The Fisher metric also allows the action and the curve length to be related to the <a href="Jensen–Shannon_divergence" title="wikilink">Jensen–Shannon divergence</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> Specifically, one has</p>
<p><span class="LaTeX">$$(b-a)\int_a^b \frac{\partial\theta^j}{\partial t} g_{jk}\frac{\partial\theta^k}{\partial t} \,dt =
8\int_a^b dJSD$$</span></p>
<p>where the integrand <em>dJSD</em> is understood to be the infinitessimal change in the Jensen–Shannon divergence along the path taken. Similarly, for the <a href="curve_length" title="wikilink">curve length</a>, one has</p>
<p><span class="LaTeX">$$\int_a^b \sqrt{\frac{\partial\theta^j}{\partial t} g_{jk}\frac{\partial\theta^k}{\partial t}} \,dt = \sqrt{8}\int_a^b \sqrt{dJSD}$$</span></p>
<p>That is, the square root of the Jensen–Shannon divergence is just the Fisher metric (divided by the square root of 8).</p>
<h2 id="as-euclidean-metric">As Euclidean metric</h2>
<p>For a <a href="discrete_probability_space" title="wikilink">discrete probability space</a>, that is, a probability space on a finite set of objects, the Fisher metric can be understood to simply be the <a href="Euclidean_metric" title="wikilink">Euclidean metric</a> restricted to a positive "guadrant" of a unit sphere, after appropriate changes of variable.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<p>An <em>N</em>-dimensional sphere embedded in (<em>N</em> + 1)-dimensional space is defined as</p>
<p><span class="LaTeX">$$\sum_i y_i^2 = 1$$</span></p>
<p>The metric on the surface of the sphere is given by</p>
<p><span class="LaTeX">$$h=\sum_i dy_i \; dy_i$$</span></p>
<p>where the <span class="LaTeX">$\textstyle dy_i$</span> are <a href="1-form" title="wikilink">1-forms</a>; they are the basis vectors for the <a href="cotangent_space" title="wikilink">cotangent space</a>. Writing <span class="LaTeX">$\textstyle \frac{\partial}{\partial y_j}$</span> as the basis vectors for the <a href="tangent_space" title="wikilink">tangent space</a>, so that</p>
<p><span class="LaTeX">$$dy_j\left(\frac{\partial}{\partial y_k}\right) = \delta_{jk}$$</span>,</p>
<p>the Euclidean metric may be written as</p>
<p><span class="LaTeX">$$h^\mathrm{flat}_{jk} = h\left(\frac{\partial}{\partial y_j}, \frac{\partial}{\partial y_k}\right) = \delta_{jk}$$</span></p>
<p>The superscript 'flat' is there to remind that, when written in coordinate form, this metric is with respect to the flat-space coordinate <span class="LaTeX">$y$</span>. Consider now the change of variable <span class="LaTeX">$p_i=y_i^2$</span>. The sphere condition now becomes the probability normalization condition</p>
<p><span class="LaTeX">$$\sum_i p_i = 1$$</span></p>
<p>while the metric becomes</p>
<p><span class="LaTeX">$$\begin{align} h &=\sum_i dy_i \; dy_i
= \sum_i d\sqrt{p_i} \; d\sqrt{p_i} \\
&= \frac{1}{4}\sum_i \frac{dp_i \; dp_i}{p_i} 
= \frac{1}{4}\sum_i p_i\; d(\log p_i) \; d(\log p_i)
\end{align}$$</span></p>
<p>The last can be recognized as one-fourth of the Fisher information metric. To complete the process, recall that the probabilities are parametric functions of the manifold variables <span class="LaTeX">$\theta$</span>, that is, one has <span class="LaTeX">$p_i = p_i(\theta)$</span>. Thus, the above induces a metric on the parameter manifold:</p>
<p><span class="LaTeX">$$\begin{align} h
& = \frac{1}{4}\sum_i p_i(\theta) \; d(\log p_i(\theta))\; d(\log p_i(\theta)) \\
&= \frac{1}{4}\sum_{jk} \sum_i p_i(\theta) \;
\frac{\partial \log p_i(\theta)} {\partial \theta_j}
\frac{\partial \log p_i(\theta)} {\partial \theta_k}
d\theta_j d\theta_k
\end{align}$$</span></p>
<p>or, in coordinate form, the Fisher information metric is:</p>
<p><span class="LaTeX">$$\begin{align}
g_{jk}(\theta)
 = 4h_{jk}^\mathrm{fisher}
&= 4 h\left(\frac{\partial}{\partial \theta_j},
\frac{\partial}{\partial \theta_k}\right) \\
& = \sum_i p_i(\theta) \;
\frac{\partial \log p_i(\theta)} {\partial \theta_j} \;
\frac{\partial \log p_i(\theta)} {\partial \theta_k}  \\
& = \mathrm{E}\left[
\frac{\partial \log p_i(\theta)} {\partial \theta_j} \;
\frac{\partial \log p_i(\theta)} {\partial \theta_k}
\right]
\end{align}$$</span></p>
<p>where, as before,</p>
<p><span class="LaTeX">$$d\theta_j\left(\frac{\partial}{\partial \theta_k}\right) = \delta_{jk}.$$</span></p>
<p>The superscript 'fisher' is present to remind that this expression is applicable for the coordinates <span class="LaTeX">$\theta$</span>; whereas the non-coordinate form is the same as the Euclidean (flat-space) metric. That is, the Fisher information metric on a statistical manifold is simply (four times) the Euclidean metric restricted to the positive quadrant of the sphere, after appropriate changes of variable.</p>
<p>When the random variable <span class="LaTeX">$p$</span> is not discrete, but continuous, the argument still holds. This can be seen in one of two different ways. One way is to carefully recast all of the above steps in an infinite-dimensional space, being careful to define limits appropriately, <em>etc.</em>, in order to make sure that all manipulations are well-defined, convergent, <em>etc.</em> The other way, as noted by <a href="Mikhail_Gromov_(mathematician)" title="wikilink">Gromov</a>,<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> is to use a <a class="uri" href="category-theoretic" title="wikilink">category-theoretic</a> approach; that is, to note that the above manipulations remain valid in the category of probabilities.</p>
<h2 id="as-fubinistudy-metric">As Fubini–Study metric</h2>
<p>The above manipulations deriving the Fisher metric from the Euclidean metric can be extended to complex <a href="projective_Hilbert_space" title="wikilink">projective Hilbert spaces</a>. In this case, one obtains the <a href="Fubini–Study_metric" title="wikilink">Fubini–Study metric</a>.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> This should perhaps be no surprise, as the Fubini–Study metric provides the means of measuring information in quantum mechanics. The <a href="Bures_metric" title="wikilink">Bures metric</a>, also known as the <a href="Helstrom_metric" title="wikilink">Helstrom metric</a>, is identical to the Fubini–Study metric,<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> although the latter is usually written in terms of <a href="pure_state" title="wikilink">pure states</a>, as below, whereas the Bures metric is written for <a href="mixed_state_(physics)" title="wikilink">mixed states</a>. By setting the phase of the complex coordinate to zero, one obtains exactly one-fourth of the Fisher information metric, exactly as above.</p>
<p>One begins with the same trick, of constructing a <a href="probability_amplitude" title="wikilink">probability amplitude</a>, written in <a href="polar_coordinate" title="wikilink">polar coordinates</a>, so:</p>
<p><span class="LaTeX">$$\psi(x;\theta) = \sqrt{p(x; \theta)} \; e^{i\alpha(x;\theta)}$$</span></p>
<p>Here, <span class="LaTeX">$\psi(x;\theta)$</span> is a complex-valued <a href="probability_amplitude" title="wikilink">probability amplitude</a>; <span class="LaTeX">$p(x; \theta)$</span> and <span class="LaTeX">$\alpha(x;\theta)$</span> are strictly real. The previous calculations are obtained by setting <span class="LaTeX">$\alpha(x;\theta)=0$</span>. The usual condition that probabilities lie within a <a class="uri" href="simplex" title="wikilink">simplex</a>, namely that</p>
<p><span class="LaTeX">$$\int_X p(x;\theta) \, dx =1$$</span></p>
<p>is equivalently expressed by the idea the square amplitude be normalized:</p>
<p><span class="LaTeX">$$\int_X \vert \psi(x;\theta)\vert^2 \, dx = 1$$</span></p>
<p>When <span class="LaTeX">$\psi(x;\theta)$</span> is real, this is the surface of a sphere.</p>
<p>The <a href="Fubini–Study_metric" title="wikilink">Fubini–Study metric</a>, written in infinitesimal form, using quantum-mechanical <a href="bra–ket_notation" title="wikilink">bra–ket notation</a>, is</p>
<p><span class="LaTeX">$$ds^2 = \frac{\langle \delta \psi \mid \delta \psi \rangle}
{\langle \psi \mid \psi \rangle} - 
\frac {\langle \delta \psi \mid \psi \rangle \; 
\langle \psi \mid \delta \psi \rangle}
{{\langle \psi \mid \psi \rangle}^2}.$$</span></p>
<p>In this notation, one has that <span class="LaTeX">$\langle x\mid \psi\rangle = \psi(x;\theta)$</span> and integration over the entire measure space <em>X</em> is written as</p>
<p><span class="LaTeX">$$\langle \phi \mid \psi\rangle = \int_X \phi^*(x;\theta) \psi(x;\theta) \, dx.$$</span></p>
<p>The expression <span class="LaTeX">$\vert \delta \psi \rangle$</span> can be understood to be an infinitesimal variation; equivalently, it can be understood to be a <a class="uri" href="1-form" title="wikilink">1-form</a> in the <a href="cotangent_space" title="wikilink">cotangent space</a>. Using the infinitesimal notation, the polar form of the probability above is simply</p>
<p><span class="LaTeX">$$\delta\psi = \left(\frac{\delta p}{2p} + i \delta \alpha\right) \psi$$</span></p>
<p>Inserting the above into the Fubini–Study metric gives:</p>
<p><span class="LaTeX">$$\begin{align} ds^2 = {} & 
\frac{1}{4}\int_X (\delta \log p)^2 \;p\,dx
 + \int_X  (\delta \alpha)^2 \;p\,dx
 - \left(\int_X \delta \alpha \;p\,dx\right)^2 \\[8pt]
& {} -\frac{i}{2} \int_X (\delta \log p \delta\alpha - \delta\alpha \delta \log p) \;p\,dx 
\end{align}$$</span></p>
<p>Setting <span class="LaTeX">$\delta\alpha=0$</span> in the above makes it clear that the first term is (one-fourth of) the Fisher information metric. The full form of the above can be made slightly clearer by changing notation to that of standard Riemannian geometry, so that the metric becomes a symmetric <a class="uri" href="2-form" title="wikilink">2-form</a> acting on the <a href="tangent_space" title="wikilink">tangent space</a>. The change of notation is done simply replacing <span class="LaTeX">$\delta \to d$</span> and <span class="LaTeX">$ds^2\to h$</span> and noting that the integrals are just expectation values; so:</p>
<p><span class="LaTeX">$$h = \frac{1}{4} \mathrm{E}\left[(d\log p)^2\right] 
+ \mathrm{E}\left[(d\alpha)^2\right]
- \left(\mathrm{E}\left[d\alpha\right]\right)^2
- \frac{i}{2}\mathrm{E}\left[d\log p\wedge d\alpha\right]$$</span></p>
<p>The imaginary term is a <a href="symplectic_form" title="wikilink">symplectic form</a>, it is the <a href="Berry_phase" title="wikilink">Berry phase</a> or <a href="geometric_phase" title="wikilink">geometric phase</a>. In index notation, the metric is:</p>
<p><span class="LaTeX">$$\begin{align}h_{jk} = {} &
h\left(\frac{\partial}{\partial\theta_j}, \frac{\partial}{\partial\theta_k}\right)  \\[8pt]
= {} & \frac{1}{4} \mathrm{E}\left[
\frac{\partial\log p}{\partial\theta_j}
\frac{\partial\log p}{\partial\theta_k}
\right]
 + \mathrm{E}\left[
\frac{\partial\alpha}{\partial\theta_j}
\frac{\partial\alpha}{\partial\theta_k}
\right]
 - \mathrm{E}\left[ \frac{\partial\alpha}{\partial\theta_j} \right]
\mathrm{E}\left[ \frac{\partial\alpha}{\partial\theta_k} \right] \\[8pt]
& {} - \frac{i}{2}\mathrm{E}\left[
\frac{\partial\log p}{\partial\theta_j}
\frac{\partial\alpha}{\partial\theta_k}
- \frac{\partial\alpha}{\partial\theta_j}
\frac{\partial\log p}{\partial\theta_k}
\right]
\end{align}$$</span></p>
<p>Again, the first term can be clearly seen to be (one fourth of) the Fisher information metric, by setting <span class="LaTeX">$\alpha=0$</span>. Equivalently, the Fubini–Study metric can be understood as the metric on complex projective Hilbert space that is induced by the complex extension of the flat Euclidean metric. The difference between this, and the Bures metric, is that the Bures metric is written in terms of mixed states.</p>
<h2 id="formal-definition">Formal definition</h2>
<p>A slightly more formal, abstract definition can be given, as follows.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>
<p>Let <em>X</em> be an <a href="orientable_manifold" title="wikilink">orientable manifold</a>, and let <span class="LaTeX">$(X,\Sigma,\mu)$</span> be a <a href="measure_space" title="wikilink">measure</a> on <em>X</em>. Equivalently, let <span class="LaTeX">$(\Omega, \mathcal{F},P)$</span> be a <a href="probability_space" title="wikilink">probability space</a> on <span class="LaTeX">$\Omega=X$</span>, with <a href="sigma_algebra" title="wikilink">sigma algebra</a> <span class="LaTeX">$\mathcal{F}=\Sigma$</span> and probability <span class="LaTeX">$P=\mu$</span>.</p>
<p>The <a href="statistical_manifold" title="wikilink">statistical manifold</a> <em>S</em>(<em>X</em>) of <em>X</em> is defined as the space of all measures <span class="LaTeX">$\mu$</span> on <em>X</em> (with the sigma-algebra <span class="LaTeX">$\Sigma$</span> held fixed). Note that this space is infinite-dimensional, and is commonly taken to be a <a href="Fréchet_space" title="wikilink">Fréchet space</a>. The points of <em>S</em>(<em>X</em>) are measures.</p>
<p>Pick a point <span class="LaTeX">$\mu\in S(X)$</span> and consider the <a href="tangent_space" title="wikilink">tangent space</a> <span class="LaTeX">$T_\mu S$</span>. The Fisher information metric is then an <a href="inner_product" title="wikilink">inner product</a> on the tangent space. With some <a href="abuse_of_notation" title="wikilink">abuse of notation</a>, one may write this as</p>
<p><span class="LaTeX">$$g(\sigma_1,\sigma_2)=\int_X \frac{d\sigma_1}{d\mu}\frac{d\sigma_2}{d\mu}d\mu$$</span></p>
<p>Here, <span class="LaTeX">$\sigma_1$</span> and <span class="LaTeX">$\sigma_2$</span> are vectors in the tangent space; that is, <span class="LaTeX">$\sigma_1,\sigma_2\in T_\mu S$</span>. The abuse of notation is to write the tangent vectors as if they are derivatives, and to insert the extraneous <em>d</em> in writing the integral: the integration is meant to be carried out using the measure <span class="LaTeX">$\mu$</span> over the whole space <em>X</em>.</p>
<p>This definition of the metric can be seen to be equivalent to the previous, in several steps. First, one selects a <a class="uri" href="submanifold" title="wikilink">submanifold</a> of <em>S</em>(<em>X</em>) by considering only those measures <span class="LaTeX">$\mu$</span> that are parameterized by some smoothly varying parameter <span class="LaTeX">$\theta$</span>. Then, if <span class="LaTeX">$\theta$</span> is finite-dimensional, then so is the submanifold; likewise, the tangent space has the same dimension as <span class="LaTeX">$\theta$</span>.</p>
<p>With some additional abuse of language, one notes that the <a href="exponential_map_(Riemannian_geometry)" title="wikilink">exponential map</a> provides a map from vectors in a tangent space to points in an underlying manifold. Thus, if <span class="LaTeX">$\sigma\in T_\mu S$</span> is a vector in the tangent space, then <span class="LaTeX">$p=\exp(\sigma)$</span> is the corresponding probability associated with point <span class="LaTeX">$p\in S(X)$</span> (after the <a href="parallel_transport" title="wikilink">parallel transport</a> of the exponential map to <span class="LaTeX">$\mu$</span>.) Conversely, given a point <span class="LaTeX">$p\in S(X)$</span>, the logarithm gives a point in the tangent space (roughly speaking, as again, one must transport from the origin to point <span class="LaTeX">$\mu$</span>; for details, refer to original sources). Thus, one has the appearance of logarithms in the simpler definition, previously given.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Cramér–Rao_bound" title="wikilink">Cramér–Rao bound</a></li>
<li><a href="Fisher_information" title="wikilink">Fisher information</a></li>
<li><a href="Hellinger_distance" title="wikilink">Hellinger distance</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li>Edward H. Feng, Gavin E. Crooks, "<a href="http://threeplusone.com/Feng2009.pdf">Far-from-equilibrium measurements of thermodynamic length</a>" (2009) <em>Physical Review E</em> <strong>79</strong>, pp 012104. DOI: 10.1103/PhysRevE.79.012104</li>
</ul>
<ul>
<li><a href="Shun'ichi_Amari" title="wikilink">Shun'ichi Amari</a> (1985) <em>Differential-geometrical methods in statistics</em>, Lecture notes in statistics, Springer-Verlag, Berlin.</li>
<li>Shun'ichi Amari, Hiroshi Nagaoka (2000) <em>Methods of information geometry</em>, Translations of mathematical monographs; v. 191, American Mathematical Society.</li>
<li>Paolo Gibilisco, Eva Riccomagno, Maria Piera Rogantin and Henry P. Wynn, (2009) <em>Algebraic and Geometric Methods in Statistics</em>, Cambridge U. Press, Cambridge.</li>
</ul>
<p>"</p>
<p><a href="Category:Differential_geometry" title="wikilink">Category:Differential geometry</a> <a href="Category:Statistical_distance_measures" title="wikilink">Category:Statistical distance measures</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Gavin E. Crooks, "Measuring thermodynamic length" (2007), <a href="http://arxiv.org/abs/0706.0559">ArXiv 0706.0559</a> <em>Physical Review Letters</em> (2009) pp100602. DOI: 10.1103/PhysRevLett.99.100602<a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3"></li>
<li id="fn4">Misha Gromov, (2012) "<a href="http://www.ihes.fr/~gromov/PDF/structre-serch-entropy-july5-2012.pdf">In a Search for a Structure, Part 1: On Entropy.</a>"<a href="#fnref4">↩</a></li>
<li id="fn5"></li>
<li id="fn6">Paolo Facchi, Ravi Kulkarni, V. I. Man'ko, Giuseppe Marmo, E. C. G. Sudarshan, Franco Ventriglia "<a href="http://arxiv.org/abs/1009.5219">Classical and Quantum Fisher Information in the Geometrical Formulation of Quantum Mechanics</a>" (2010), <em>Physics Letters</em> <strong>A 374</strong> pp. 4801. DOI: 10.1016/j.physleta.2010.10.005<a href="#fnref6">↩</a></li>
<li id="fn7"></li>
<li id="fn8">Mitsuhiro Itoh and Yuichi Shishido, "<a href="http://www.tulips.tsukuba.ac.jp/dspace/bitstream/2241/100265/1/DGA_26-4.pdf">Fisher information metric and Poisson kernels</a>" (2008)<a href="#fnref8">↩</a></li>
</ol>
</section>
</body>
</html>
