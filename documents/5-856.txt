   Omitted-variable bias      Omitted-variable bias   In statistics , omitted-variable bias ( OVB ) occurs when a model is created which incorrectly leaves out one or more important causal factors. The "bias" is created when the model compensates for the missing factor by over- or underestimating the effect of one of the other factors.  More specifically, OVB is the bias that appears in the estimates of parameters in a regression analysis , when the assumed specification is incorrect in that it omits an independent variable that is correlated with both the dependent variable and one or more included independent variables.  Example  Omitted-variable bias in linear regression  Intuition  Two conditions must hold true for omitted-variable bias to exist in linear regression :   the omitted variable must be a determinant of the dependent variable (i.e., its true regression coefficient is not zero); and  the omitted variable must be correlated with one or more of the included independent variables (i.e. the covariance of the omitted variable and the independent variable, cov(z,x), is not equal to zero).   Suppose the true cause-and-effect relationship is given by      y  =   a  +   b  x   +   c  z   +  u       y    a    b  x     c  z   u     y=a+bx+cz+u     with parameters a, b, c , dependent variable y , independent variables x and z , and error term u . We wish to know the effect of x itself upon y (that is, we wish to obtain an estimate of b ). But suppose that we omit z from the regression, and suppose the relation between x and z is given by      z  =   d  +   f  x   +  e       z    d    f  x   e     z=d+fx+e     with parameters d, f and error term e . Substituting the second equation into the first gives       y  =    (   a  +   c  d    )   +    (   b  +   c  f    )   x   +   (   u  +   c  e    )     .      y      a    c  d        b    c  f    x     u    c  e       y=(a+cd)+(b+cf)x+(u+ce).     If a regression of y is conducted upon x only, this last equation is what is estimated, and the regression coefficient on x is actually an estimate of ( b+cf ), giving not simply an estimate of the desired direct effect of x upon y (which is b ), but rather of its sum with the indirect effect (the effect f of x on z times the effect c of z on y ). Thus by omitting the variable z from the regression, we have estimated the total derivative of y with respect to x rather than its partial derivative with respect to x . These differ if both c and f are non-zero.  Detailed analysis  As an example, consider a linear model of the form        y  i   =     x  i   β   +    z  i   δ   +   u  i     ,   i  =   1  ,  …  ,  n       formulae-sequence     subscript  y  i        subscript  x  i   β      subscript  z  i   δ    subscript  u  i       i   1  normal-…  n      y_{i}=x_{i}\beta+z_{i}\delta+u_{i},\qquad i=1,\dots,n     where   x i is a 1 × p row vector of values of p  independent variables observed at time i or for the i  th study participant;  β is a p × 1 column vector of unobservable parameters (the response coefficients of the dependent variable to each of the p independent variables in x i ) to be estimated;  z i is a scalar and is the value of another independent variable that is observed at time i or for the i  th study participant;  δ is a scalar and is an unobservable parameter (the response coefficient of the dependent variable to z i ) to be estimated;  u i is the unobservable error term occurring at time i or for the i  th study participant; it is an unobserved realization of a random variable having expected value 0 (conditionally on x i and z i );  y i is the observation of the dependent variable at time i or for the i  th study participant.   We collect the observations of all variables subscripted i = 1, ..., n , and stack them one below another, to obtain the matrix  X and the vectors  Y , Z , and U :       X  =   [      x  1       ⋮       x  n      ]   ∈   ℝ   n  ×  p     ,        X   delimited-[]     subscript  x  1     normal-⋮     subscript  x  n            superscript  ℝ    n  p       X=\left[\begin{array}[]{c}x_{1}\\
 \vdots\\
 x_{n}\end{array}\right]\in\mathbb{R}^{n\times p},     and        Y  =   [      y  1       ⋮       y  n      ]    ,    Z  =   [      z  1       ⋮       z  n      ]    ,   U  =   [      u  1       ⋮       u  n      ]   ∈   ℝ   n  ×  1       .     formulae-sequence    Y   delimited-[]     subscript  y  1     normal-⋮     subscript  y  n        formulae-sequence    Z   delimited-[]     subscript  z  1     normal-⋮     subscript  z  n           U   delimited-[]     subscript  u  1     normal-⋮     subscript  u  n            superscript  ℝ    n  1         Y=\left[\begin{array}[]{c}y_{1}\\
 \vdots\\
 y_{n}\end{array}\right],\quad Z=\left[\begin{array}[]{c}z_{1}\\
 \vdots\\
 z_{n}\end{array}\right],\quad U=\left[\begin{array}[]{c}u_{1}\\
 \vdots\\
 u_{n}\end{array}\right]\in\mathbb{R}^{n\times 1}.     If the independent variable z is omitted from the regression, then the estimated values of the response parameters of the other independent variables will be given by, by the usual least squares calculation,       β  ^   =     (    X  ′   X   )    -  1     X  ′    Y         normal-^  β      superscript     superscript  X  normal-′   X     1     superscript  X  normal-′   Y     \hat{\beta}=(X^{\prime}X)^{-1}X^{\prime}Y\,     (where the "prime" notation means the transpose of a matrix and the -1 superscript is matrix inversion ).  Substituting for Y based on the assumed linear model,      β  ^     normal-^  β    \displaystyle\hat{\beta}     On taking expectations, the contribution of the final term is zero; this follows from the assumption that U has zero expectation. On simplifying the remaining terms:      E   [   β  ^   |  X  ]      fragments  E   fragments  normal-[   normal-^  β   normal-|  X  normal-]     \displaystyle E[\hat{\beta}|X]     The second term after the equal sign is the omitted-variable bias in this case, which is non-zero if the omitted variable z is correlated with any of the included variables in the matrix X (that is, if X'Z does not equal a vector of zeroes). Note that the bias is equal to the weighted portion of z i which is "explained" by x i .  Effects on ordinary least squares  The Gauss–Markov theorem states that regression models which fulfill the classical linear regression model assumptions provide the best, linear and unbiased estimators. With respect to ordinary least squares , the relevant assumption of the classical linear regression model is that the error term is uncorrelated with the regressors.  The presence of omitted-variable bias violates this particular assumption. The violation causes the OLS estimator to be biased and inconsistent . The direction of the bias depends on the estimators as well as the covariance between the regressors and the omitted variables. A positive covariance of the omitted variable with both a regressor and the dependent variable will lead the OLS estimate of the included regressor's coefficient to be greater than the true value of that coefficient. This effect can be seen by taking the expectation of the parameter, as shown in the previous section.  See also   Confounding variable   References        "  Category:Regression analysis  Category:Bias   