   Gaussâ€“Newton algorithm      Gaussâ€“Newton algorithm   The Gaussâ€“Newton algorithm is a method used to solve non-linear least squares problems. It is a modification of Newton's method for finding a minimum of a function . Unlike Newton's method, the Gaussâ€“Newton algorithm can only be used to minimize a sum of squared function values, but it has the advantage that second derivatives, which can be challenging to compute, are not required.  Non-linear least squares problems arise for instance in non-linear regression , where parameters in a model are sought such that the model is in good agreement with available observations.  The method is named after the mathematicians Carl Friedrich Gauss and Isaac Newton .  Description  Given m functions r = ( r 1 , â€¦, r m ) of n variables Î² =Â ( Î² 1 , â€¦, Î² n ), with m â‰¥ n , the Gaussâ€“Newton algorithm iteratively finds the minimum of the sum of squares 1        S   (  ğœ·  )    =    âˆ‘   i  =  1   m     r  i     (  ğœ·  )   2      .        S  ğœ·     superscript   subscript     i  1    m      subscript  r  i    superscript  ğœ·  2       S(\boldsymbol{\beta})=\sum_{i=1}^{m}r_{i}(\boldsymbol{\beta})^{2}.     Starting with an initial guess    ğœ·   (  0  )      superscript  ğœ·  0    \boldsymbol{\beta}^{(0)}   for the minimum, the method proceeds by the iterations       ğœ·   (   s  +  1   )    =    ğœ·   (  s  )    -     (    ğ‰  ğ«    ğ–³    ğ‰  ğ«    )    -  1     ğ‰  ğ«    ğ–³   ğ«   (   ğœ·   (  s  )    )          superscript  ğœ·    s  1       superscript  ğœ·  s      superscript     superscript   subscript  ğ‰  ğ«   ğ–³    subscript  ğ‰  ğ«      1     superscript   subscript  ğ‰  ğ«   ğ–³   ğ«   superscript  ğœ·  s       \boldsymbol{\beta}^{(s+1)}=\boldsymbol{\beta}^{(s)}-\left(\mathbf{J_{r}}^{%
 \mathsf{T}}\mathbf{J_{r}}\right)^{-1}\mathbf{J_{r}}^{\mathsf{T}}\mathbf{r}(%
 \boldsymbol{\beta}^{(s)})     where, if r and Î² are column vectors , the entries of the Jacobian matrix are         (   ğ‰  ğ«   )    i  j    =     âˆ‚   r  i     (   ğœ·   (  s  )    )     âˆ‚   Î²  j      ,       subscript   subscript  ğ‰  ğ«     i  j           subscript  r  i     superscript  ğœ·  s       subscript  Î²  j       (\mathbf{J_{r}})_{ij}=\frac{\partial r_{i}(\boldsymbol{\beta}^{(s)})}{\partial%
 \beta_{j}},     and the symbol     ğ–³     ğ–³    {}^{\mathsf{T}}   denotes the matrix transpose .  If m = n , the iteration simplifies to       ğœ·   (   s  +  1   )    =    ğœ·   (  s  )    -     (   ğ‰  ğ«   )    -  1    ğ«   (   ğœ·   (  s  )    )          superscript  ğœ·    s  1       superscript  ğœ·  s      superscript   subscript  ğ‰  ğ«     1    ğ«   superscript  ğœ·  s       \boldsymbol{\beta}^{(s+1)}=\boldsymbol{\beta}^{(s)}-\left(\mathbf{J_{r}}\right%
 )^{-1}\mathbf{r}(\boldsymbol{\beta}^{(s)})     which is a direct generalization of Newton's method in one dimension.  In data fitting, where the goal is to find the parameters Î² such that a given model function y = f ( x , Î² ) best fits some data points ( x i , y i ), the functions r i are the residuals         r  i    (  ğœ·  )    =    y  i   -   f   (   x  i   ,  ğœ·  )      .         subscript  r  i   ğœ·      subscript  y  i     f    subscript  x  i   ğœ·       r_{i}(\boldsymbol{\beta})=y_{i}-f(x_{i},\boldsymbol{\beta}).     Then, the Gauss-Newton method can be expressed in terms of the Jacobian J f of the function f as        ğœ·   (   s  +  1   )    =    ğœ·   (  s  )    +     (    ğ‰  ğŸ    ğ–³    ğ‰  ğŸ    )    -  1     ğ‰  ğŸ    ğ–³   ğ«   (   ğœ·   (  s  )    )      .       superscript  ğœ·    s  1       superscript  ğœ·  s      superscript     superscript   subscript  ğ‰  ğŸ   ğ–³    subscript  ğ‰  ğŸ      1     superscript   subscript  ğ‰  ğŸ   ğ–³   ğ«   superscript  ğœ·  s       \boldsymbol{\beta}^{(s+1)}=\boldsymbol{\beta}^{(s)}+\left(\mathbf{J_{f}}^{%
 \mathsf{T}}\mathbf{J_{f}}\right)^{-1}\mathbf{J_{f}}^{\mathsf{T}}\mathbf{r}(%
 \boldsymbol{\beta}^{(s)}).     Notes  The assumption m â‰¥ n in the algorithm statement is necessary, as otherwise the matrix J r T J r is not invertible and the normal equations cannot be solved (at least uniquely).  The Gaussâ€“Newton algorithm can be derived by linearly approximating the vector of functions r i . Using Taylor's theorem , we can write at every iteration:       ğ«   (  ğœ·  )    â‰ˆ    ğ«   (   ğœ·  s   )    +    ğ‰  ğ«    (   ğœ·  s   )   Î”          ğ«  ğœ·       ğ«   superscript  ğœ·  s       subscript  ğ‰  ğ«    superscript  ğœ·  s   normal-Î”      \mathbf{r}(\boldsymbol{\beta})\approx\mathbf{r}(\boldsymbol{\beta}^{s})+%
 \mathbf{J_{r}}(\boldsymbol{\beta}^{s})\Delta     with     Î”  =   ğœ·  -   ğœ·  s     .      normal-Î”    ğœ·   superscript  ğœ·  s      \Delta=\boldsymbol{\beta}-\boldsymbol{\beta}^{s}.   The task of finding Î” minimizing the sum of squares of the right-hand side, i.e.,      ğ¦ğ¢ğ§    âˆ¥    ğ«   (   ğœ·  s   )    +    ğ‰  ğ«    (   ğœ·  s   )   Î”    âˆ¥   2  2       ğ¦ğ¢ğ§   superscript   subscript   norm      ğ«   superscript  ğœ·  s       subscript  ğ‰  ğ«    superscript  ğœ·  s   normal-Î”     2   2     \mathbf{min}\|\mathbf{r}(\boldsymbol{\beta}^{s})+\mathbf{J_{r}}(\boldsymbol{%
 \beta}^{s})\Delta\|_{2}^{2}   , is a linear least squares problem, which can be solved explicitly, yielding the normal equations in the algorithm.  The normal equations are m linear simultaneous equations in the unknown increments, Î”. They may be solved in one step, using Cholesky decomposition , or, better, the QR factorization of J r . For large systems, an iterative method , such as the conjugate gradient method, may be more efficient. If there is a linear dependence between columns of J r , the iterations will fail as J r T J r becomes singular.  Example  In this example, the Gaussâ€“Newton algorithm will be used to fit a model to some data by minimizing the sum of squares of errors between the data and model's predictions.  In a biology experiment studying the relation between substrate concentration [ S ] and reaction rate in an enzyme-mediated reaction, the data in the following table were obtained.        i   1   2   3   4   5   6   7     [S]   0.038   0.194   0.425   0.626   1.253   2.500   3.740     rate   0.050   0.127   0.094   0.2122   0.2729   0.2665   0.3317       It is desired to find a curve (model function) of the form      rate  =     V  max    [  S  ]      K  M   +   [  S  ]         rate       subscript  V  max    delimited-[]  S       subscript  K  M    delimited-[]  S       \text{rate}=\frac{V_{\text{max}}[S]}{K_{M}+[S]}     that fits best the data in the least squares sense, with the parameters    V  max     subscript  V  max    V_{\text{max}}   and    K  M     subscript  K  M    K_{M}   to be determined.  Denote by    x  i     subscript  x  i    x_{i}   and    y  i     subscript  y  i    y_{i}   the value of    [  S  ]     delimited-[]  S    [S]   and the rate from the table,    i  =   1  ,  â€¦  ,  7.       i   1  normal-â€¦  7.     i=1,\dots,7.   Let     Î²  1   =   V  max        subscript  Î²  1    subscript  V  max     \beta_{1}=V_{\text{max}}   and      Î²  2   =   K  M    .       subscript  Î²  2    subscript  K  M     \beta_{2}=K_{M}.   We will find    Î²  1     subscript  Î²  1    \beta_{1}   and    Î²  2     subscript  Î²  2    \beta_{2}   such that the sum of squares of the residuals       r  i   =    y  i   -     Î²  1    x  i      Î²  2   +   x  i           subscript  r  i      subscript  y  i        subscript  Î²  1    subscript  x  i       subscript  Î²  2    subscript  x  i        r_{i}=y_{i}-\frac{\beta_{1}x_{i}}{\beta_{2}+x_{i}}   (    i  =   1  ,  â€¦  ,  7       i   1  normal-â€¦  7     i=1,\dots,7   ) is minimized.  The Jacobian    ğ‰  ğ«     subscript  ğ‰  ğ«    \mathbf{J_{r}}   of the vector of residuals    r  i     subscript  r  i    r_{i}   in respect to the unknowns    Î²  j     subscript  Î²  j    \beta_{j}   is an    7  Ã—  2      7  2    7\times 2   matrix with the   i   i   i   -th row having the entries          âˆ‚   r  i     âˆ‚   Î²  1     =   -    x  i     Î²  2   +   x  i       ,     âˆ‚   r  i     âˆ‚   Î²  2     =     Î²  1    x  i      (    Î²  2   +   x  i    )   2      .     formulae-sequence         subscript  r  i       subscript  Î²  1          subscript  x  i      subscript  Î²  2    subscript  x  i              subscript  r  i       subscript  Î²  2          subscript  Î²  1    subscript  x  i     superscript     subscript  Î²  2    subscript  x  i    2       \frac{\partial r_{i}}{\partial\beta_{1}}=-\frac{x_{i}}{\beta_{2}+x_{i}},\ %
 \frac{\partial r_{i}}{\partial\beta_{2}}=\frac{\beta_{1}x_{i}}{\left(\beta_{2}%
 +x_{i}\right)^{2}}.     Starting with the initial estimates of    Î²  1     subscript  Î²  1    \beta_{1}   =0.9 and    Î²  2     subscript  Î²  2    \beta_{2}   =0.2, after five iterations of the Gaussâ€“Newton algorithm the optimal values      Î²  ^   1   =  0.362       subscript   normal-^  Î²   1   0.362    \hat{\beta}_{1}=0.362   and      Î²  ^   2   =  0.556       subscript   normal-^  Î²   2   0.556    \hat{\beta}_{2}=0.556   are obtained. The sum of squares of residuals decreased from the initial value of 1.445 to 0.00784 after the fifth iteration. The plot in the figure on the right shows the curve determined by the model for the optimal parameters versus the observed data.  Convergence properties  It can be shown 2 that the increment Î” is a descent direction for S , and, if the algorithm converges, then the limit is a stationary point of S . However, convergence is not guaranteed, not even local convergence as in Newton's method .  The rate of convergence of the Gaussâ€“Newton algorithm can approach quadratic . 3 The algorithm may converge slowly or not at all if the initial guess is far from the minimum or the matrix     ğ‰  ğ«  ğ–³    ğ‰  ğ«        superscript   subscript  ğ‰  ğ«   ğ–³    subscript  ğ‰  ğ«     \mathbf{J_{r}^{\mathsf{T}}J_{r}}   is ill-conditioned . For example, consider the problem with    m  =  2      m  2    m=2   equations and    n  =  1      n  1    n=1   variable, given by       r  1    (  Î²  )        subscript  r  1   Î²    \displaystyle r_{1}(\beta)   The optimum is at    Î²  =  0      Î²  0    \beta=0   . ( Actually the optimum is at    Î²  =   -  1       Î²    1     \beta=-1   for    Î»  =  2      Î»  2    \lambda=2   , because     S   (  0  )    =    1  2   +    (   -  1   )   2    =  2          S  0      superscript  1  2    superscript    1   2         2     S(0)=1^{2}+(-1)^{2}=2   , but     S   (   -  1   )    =  0        S    1    0    S(-1)=0   . ) If    Î»  =  0      Î»  0    \lambda=0   then the problem is in fact linear and the method finds the optimum in one iteration. If |Î»|  1, then the method does not even converge locally. 4  Derivation from Newton's method  In what follows, the Gaussâ€“Newton algorithm will be derived from Newton's method for function optimization via an approximation. As a consequence, the rate of convergence of the Gaussâ€“Newton algorithm can be quadratic under certain regularity conditions. In general (under weaker conditions), the convergence rate is linear. 5  The recurrence relation for Newton's method for minimizing a function S of parameters,   ğœ·   ğœ·   \boldsymbol{\beta}   , is       ğœ·   (   s  +  1   )    =    ğœ·   (  s  )    -    ğ‡   -  1     ğ           superscript  ğœ·    s  1       superscript  ğœ·  s      superscript  ğ‡    1    ğ       \boldsymbol{\beta}^{(s+1)}=\boldsymbol{\beta}^{(s)}-\mathbf{H}^{-1}\mathbf{g}\,   where g denotes the gradient vector of S and H denotes the Hessian matrix of S . Since    S  =    âˆ‘   i  =  1   m    r  i  2        S    superscript   subscript     i  1    m    superscript   subscript  r  i   2      S=\sum_{i=1}^{m}r_{i}^{2}   , the gradient is given by        g  j   =   2    âˆ‘   i  =  1   m     r  i     âˆ‚   r  i     âˆ‚   Î²  j         .       subscript  g  j     2    superscript   subscript     i  1    m      subscript  r  i        subscript  r  i       subscript  Î²  j          g_{j}=2\sum_{i=1}^{m}r_{i}\frac{\partial r_{i}}{\partial\beta_{j}}.   Elements of the Hessian are calculated by differentiating the gradient elements,    g  j     subscript  g  j    g_{j}   , with respect to    Î²  k     subscript  Î²  k    \beta_{k}           H   j  k    =   2    âˆ‘   i  =  1   m    (      âˆ‚   r  i     âˆ‚   Î²  j       âˆ‚   r  i     âˆ‚   Î²  k      +    r  i      âˆ‚  2    r  i      âˆ‚   Î²  j     âˆ‚   Î²  k        )      .       subscript  H    j  k      2    superscript   subscript     i  1    m            subscript  r  i       subscript  Î²  j          subscript  r  i       subscript  Î²  k         subscript  r  i       superscript   2    subscript  r  i         subscript  Î²  j       subscript  Î²  k            H_{jk}=2\sum_{i=1}^{m}\left(\frac{\partial r_{i}}{\partial\beta_{j}}\frac{%
 \partial r_{i}}{\partial\beta_{k}}+r_{i}\frac{\partial^{2}r_{i}}{\partial\beta%
 _{j}\partial\beta_{k}}\right).     The Gaussâ€“Newton method is obtained by ignoring the second-order derivative terms (the second term in this expression). That is, the Hessian is approximated by       H   j  k    â‰ˆ   2    âˆ‘   i  =  1   m     J   i  j     J   i  k            subscript  H    j  k      2    superscript   subscript     i  1    m      subscript  J    i  j     subscript  J    i  k         H_{jk}\approx 2\sum_{i=1}^{m}J_{ij}J_{ik}     where     J   i  j    =    âˆ‚   r  i     âˆ‚   Î²  j          subscript  J    i  j         subscript  r  i       subscript  Î²  j       J_{ij}=\frac{\partial r_{i}}{\partial\beta_{j}}   are entries of the Jacobian J r . The gradient and the approximate Hessian can be written in matrix notation as        ğ   =   2   ğ‰  ğ«  ğ–³   ğ«    ,   ğ‡  â‰ˆ   2   ğ‰  ğ«  ğ–³    ğ‰  ğ«      .     formulae-sequence    ğ     2   superscript   subscript  ğ‰  ğ«   ğ–³   ğ«      ğ‡    2   superscript   subscript  ğ‰  ğ«   ğ–³    subscript  ğ‰  ğ«       \mathbf{g}=2\mathbf{J}_{\mathbf{r}}^{\mathsf{T}}\mathbf{r},\quad\mathbf{H}%
 \approx 2\mathbf{J}_{\mathbf{r}}^{\mathsf{T}}\mathbf{J_{r}}.\,     These expressions are substituted into the recurrence relation above to obtain the operational equations         ğœ·   (   s  +  1   )    =    ğœ·   (  s  )    +  Î”    ;   Î”  =   -     (    ğ‰  ğ«    ğ–³    ğ‰  ğ«    )    -  1     ğ‰  ğ«    ğ–³   ğ«      .     formulae-sequence     superscript  ğœ·    s  1       superscript  ğœ·  s   normal-Î”      normal-Î”       superscript     superscript   subscript  ğ‰  ğ«   ğ–³    subscript  ğ‰  ğ«      1     superscript   subscript  ğ‰  ğ«   ğ–³   ğ«       \boldsymbol{\beta}^{(s+1)}=\boldsymbol{\beta}^{(s)}+\Delta;\quad\Delta=-\left(%
 \mathbf{J_{r}}^{\mathsf{T}}\mathbf{J_{r}}\right)^{-1}\mathbf{J_{r}}^{\mathsf{T%
 }}\mathbf{r}.     Convergence of the Gaussâ€“Newton method is not guaranteed in all instances. The approximation       |    r  i      âˆ‚  2    r  i      âˆ‚   Î²  j     âˆ‚   Î²  k       |   â‰ª   |     âˆ‚   r  i     âˆ‚   Î²  j       âˆ‚   r  i     âˆ‚   Î²  k      |      much-less-than       subscript  r  i       superscript   2    subscript  r  i         subscript  Î²  j       subscript  Î²  k                 subscript  r  i       subscript  Î²  j          subscript  r  i       subscript  Î²  k         \left|r_{i}\frac{\partial^{2}r_{i}}{\partial\beta_{j}\partial\beta_{k}}\right|%
 \ll\left|\frac{\partial r_{i}}{\partial\beta_{j}}\frac{\partial r_{i}}{%
 \partial\beta_{k}}\right|   that needs to hold to be able to ignore the second-order derivative terms may be valid in two cases, for which convergence is to be expected. 6   The function values    r  i     subscript  r  i    r_{i}   are small in magnitude, at least around the minimum.  The functions are only "mildly" non linear, so that      âˆ‚  2    r  i      âˆ‚   Î²  j     âˆ‚   Î²  k           superscript   2    subscript  r  i         subscript  Î²  j       subscript  Î²  k       \frac{\partial^{2}r_{i}}{\partial\beta_{j}\partial\beta_{k}}   is relatively small in magnitude.   Improved versions  With the Gaussâ€“Newton method the sum of squares S may not decrease at every iteration. However, since Î” is a descent direction, unless    S   (   ğœ·  s   )       S   superscript  ğœ·  s     S(\boldsymbol{\beta}^{s})   is a stationary point, it holds that     S   (    ğœ·  s   +   Î±  Î”    )    <   S   (   ğœ·  s   )          S     superscript  ğœ·  s     Î±  normal-Î”       S   superscript  ğœ·  s      S(\boldsymbol{\beta}^{s}+\alpha\Delta)   for all sufficiently small    Î±  >  0      Î±  0    \alpha>0   . Thus, if divergence occurs, one solution is to employ a fraction,   Î±   Î±   \alpha   , of the increment vector, Î” in the updating formula       ğœ·   s  +  1    =    ğœ·  s   +    Î±   Î”         superscript  ğœ·    s  1       superscript  ğœ·  s     Î±  normal-Î”      \boldsymbol{\beta}^{s+1}=\boldsymbol{\beta}^{s}+\alpha\ \Delta   . In other words, the increment vector is too long, but it points in "downhill", so going just a part of the way will decrease the objective function S . An optimal value for   Î±   Î±   \alpha   can be found by using a line search algorithm, that is, the magnitude of   Î±   Î±   \alpha   is determined by finding the value that minimizes S , usually using a direct search method in the interval    0  <  Î±  <  1        0  Î±       1     0<\alpha<1   .  In cases where the direction of the shift vector is such that the optimal fraction,   Î±   Î±   \alpha   , is close to zero, an alternative method for handling divergence is the use of the Levenbergâ€“Marquardt algorithm , also known as the " trust region method". 7 The normal equations are modified in such a way that the increment vector is rotated towards the direction of steepest descent ,        (     ğ‰  ğ“   ğ‰   +   Î»  ğƒ    )   Î”   =   -    ğ‰  T   ğ«               superscript  ğ‰  ğ“   ğ‰     Î»  ğƒ    normal-Î”        superscript  ğ‰  T   ğ«      \left(\mathbf{J^{T}J+\lambda D}\right)\Delta=-\mathbf{J}^{T}\mathbf{r}   , where D is a positive diagonal matrix. Note that when D is the identity matrix and    Î»  â†’   +  âˆ      normal-â†’  Î»        \lambda\to+\infty   , then     Î”  /  Î»   â†’   -    ğ‰  T   ğ«       normal-â†’    normal-Î”  Î»        superscript  ğ‰  T   ğ«      \Delta/\lambda\to-\mathbf{J}^{T}\mathbf{r}   , therefore the direction of Î” approaches the direction of the negative gradient    -    ğ‰  T   ğ«          superscript  ğ‰  T   ğ«     -\mathbf{J}^{T}\mathbf{r}   .  The so-called Marquardt parameter,   Î»   Î»   \lambda   , may also be optimized by a line search, but this is inefficient as the shift vector must be re-calculated every time   Î»   Î»   \lambda   is changed. A more efficient strategy is this. When divergence occurs increase the Marquardt parameter until there is a decrease in S. Then, retain the value from one iteration to the next, but decrease it if possible until a cut-off value is reached when the Marquardt parameter can be set to zero; the minimization of S then becomes a standard Gaussâ€“Newton minimization.  Other applications  The Gaussâ€“Newton algorithm is a popular method for solving nonlinear inverse problems. A particular application is generating computational models of oil and gas reservoirs for consistency with observed production data. 8  Related algorithms  In a quasi-Newton method , such as that due to Davidon, Fletcher and Powell or Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno ( BFGS method ) an estimate of the full Hessian,      âˆ‚  2   S     âˆ‚   Î²  j     âˆ‚   Î²  k           superscript   2   S        subscript  Î²  j       subscript  Î²  k       \frac{\partial^{2}S}{\partial\beta_{j}\partial\beta_{k}}   , is built up numerically using first derivatives     âˆ‚   r  i     âˆ‚   Î²  j           subscript  r  i       subscript  Î²  j      \frac{\partial r_{i}}{\partial\beta_{j}}   only so that after n refinement cycles the method closely approximates to Newton's method in performance. Note that quasi-Newton methods can minimize general real-valued functions, whereas Gauss-Newton, Levenberg-Marquardt, etc. fits only to nonlinear least-squares problems.  Another method for solving minimization problems using only first derivatives is gradient descent . However, this method does not take into account the second derivatives even approximately. Consequently, it is highly inefficient for many functions, especially if the parameters have strong interactions.  Notes    References     .    "  Category:Optimization algorithms and methods  Category:Least squares  Category:Statistical algorithms     BjÃ¶rck (1996) â†©  BjÃ¶rck (1996) p260 â†©  BjÃ¶rck (1996) p341, 342 â†©  Fletcher (1987) p.113 â†©  http://www.henley.ac.uk/web/FILES/maths/09-04.pdf â†©  Nocedal (1999) p259 â†©   â†©     