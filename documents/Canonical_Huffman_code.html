<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1229">Canonical Huffman code</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Canonical Huffman code</h1>
<hr/>

<p>A <strong>canonical Huffman code</strong> is a particular type of <a href="Huffman_code" title="wikilink">Huffman code</a> with unique properties which allow it to be described in a very compact manner.</p>

<p><a href="Data_compression" title="wikilink">Data compressors</a> generally work in one of two ways. Either the decompressor can infer what <a class="uri" href="codebook" title="wikilink">codebook</a> the compressor has used from previous context, or the compressor must tell the decompressor what the codebook is. Since a canonical Huffman codebook can be stored especially efficiently, most compressors start by generating a "normal" Huffman codebook, and then convert it to canonical Huffman before using it.</p>

<p>In order for a <a href="symbol_code" title="wikilink">symbol code</a> scheme such as the <a href="Huffman_code" title="wikilink">Huffman code</a> to be decompressed, the same model that the encoding algorithm used to compress the source data must be provided to the decoding algorithm so that it can use it to decompress the encoded data. In standard Huffman coding this model takes the form of a tree of variable-length codes, with the most frequent symbols located at the top of the structure and being represented by the fewest number of bits.</p>

<p>However, this code tree introduces two critical inefficiencies into an implementation of the coding scheme. Firstly, each node of the tree must store either references to its child nodes or the symbol that it represents. This is expensive in memory usage and if there is a high proportion of unique symbols in the source data then the size of the code tree can account for a significant amount of the overall encoded data. Secondly, traversing the tree is computationally costly, since it requires the algorithm to jump randomly through the structure in memory as each bit in the encoded data is read in.</p>

<p>Canonical Huffman codes address these two issues by generating the codes in a clear standardized format; all the codes for a given length are assigned their values sequentially. This means that instead of storing the structure of the code tree for decompression only the lengths of the codes are required, reducing the size of the encoded data. Additionally, because the codes are sequential, the decoding algorithm can be dramatically simplified so that it is computationally efficient.</p>
<h2 id="algorithm">Algorithm</h2>

<p>The normal Huffman coding <a class="uri" href="algorithm" title="wikilink">algorithm</a> assigns a variable length code to every symbol in the alphabet. More frequently used symbols will be assigned a shorter code. For example, suppose we have the following <em>non</em>-canonical codebook:</p>

<p><code>A = 11</code><br/>
<code>B = 0</code><br/>
<code>C = 101</code><br/>
<code>D = 100</code></p>

<p>Here the letter A has been assigned 2 <a href="bit" title="wikilink">bits</a>, B has 1 bit, and C and D both have 3 bits. To make the code a <em>canonical</em> Huffman code, the codes are renumbered. The bit lengths stay the same with the code book being sorted <em>first</em> by codeword length and <em>secondly</em> by <a class="uri" href="alphabetical" title="wikilink">alphabetical</a> <a href="Value_(computer_science)" title="wikilink">value</a>:</p>

<p><code>B = 0</code><br/>
<code>A = 11</code><br/>
<code>C = 101</code><br/>
<code>D = 100</code></p>

<p>Each of the existing codes are replaced with a new one of the same length, using the following algorithm:</p>
<ul>
<li>The <em>first</em> symbol in the list gets assigned a codeword which is the same length as the symbol's original codeword but all zeros. This will often be a single zero ('0').</li>
<li>Each subsequent symbol is assigned the next <a href="Binary_numeral_system" title="wikilink">binary</a> number in sequence, ensuring that following codes are always higher in value.</li>
<li>When you reach a longer codeword, then <em>after</em> incrementing, append zeros until the length of the new codeword is equal to the length of the old codeword. This can be thought of as a <a href="Logical_shift" title="wikilink">left shift</a>.</li>
</ul>

<p>By following these three rules, the <em>canonical</em> version of the code book produced will be:</p>

<p><code>B = 0</code><br/>
<code>A = 10</code><br/>
<code>C = 110</code><br/>
<code>D = 111</code></p>
<h3 id="as-a-fractional-binary-number">As a fractional binary number</h3>

<p>Another perspective on the canonical codewords is that they are the digits past the <a href="radix_point" title="wikilink">radix point</a> (binary decimal point) in a binary representation of a certain series. Specifically, suppose the lengths of the codewords are <em>l</em><sub>1</sub> ... <em>l</em><sub>n</sub>. Then the canonical codeword for symbol <em>i</em> is the first <em>l</em><sub>i</sub> binary digits past the radix point in the binary representation of</p>

<p>

<math display="inline" id="Canonical_Huffman_code:0">
 <semantics>
  <mrow>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>j</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mrow>
      <mi>i</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msubsup>
    <msup>
     <mn>2</mn>
     <mrow>
      <mo>-</mo>
      <msub>
       <mi>l</mi>
       <mi>i</mi>
      </msub>
     </mrow>
    </msup>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <eq></eq>
       <ci>j</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <minus></minus>
      <ci>i</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cn type="integer">2</cn>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>l</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{j=1}^{i-1}2^{-l_{i}}.
  </annotation>
 </semantics>
</math>

</p>

<p>This perspective is particularly useful in light of <a href="Kraft's_inequality" title="wikilink">Kraft's inequality</a>, which says that the sum above will always be less than 1 (since the lengths come from a prefix free code). This shows that adding one in the algorithm above never overflows and creates a codeword that is longer than intended.</p>
<h2 id="encoding-the-codebook">Encoding the codebook</h2>

<p>The whole advantage of a canonical Huffman tree is that one can encode the description (the codebook) in fewer bits than a fully described tree.</p>

<p>Let us take our original Huffman codebook:</p>

<p><code>A = 11</code><br/>
<code>B = 0</code><br/>
<code>C = 101</code><br/>
<code>D = 100</code></p>

<p>There are several ways we could encode this Huffman tree. For example, we could write each <strong>symbol</strong> followed by the <strong>number of bits</strong> and <strong>code</strong>:</p>

<p><code>('A',2,11), ('B',1,0), ('C',3,101), ('D',3,100)</code></p>

<p>Since we are listing the symbols in sequential alphabetical order, we can omit the symbols themselves, listing just the <strong>number of bits</strong> and <strong>code</strong>:</p>

<p><code>(2,11), (1,0), (3,101), (3,100)</code></p>

<p>With our <em>canonical</em> version we have the knowledge that the symbols are in sequential alphabetical order <em>and</em> that a later code will always be higher in value than an earlier one. The only parts left to transmit are the <a href="bit-length" title="wikilink">bit-lengths</a> (<strong>number of bits</strong>) for each symbol. Note that our canonical Huffman tree always has higher values for longer bit lengths and that any symbols of the same bit length (<em>C</em> and <em>D</em>) have higher code values for higher symbols:</p>

<p><code>A = 10    (code value: 2 decimal, bits: </code><strong><code>2</code></strong><code>)</code><br/>
<code>B = 0     (code value: 0 decimal, bits: </code><strong><code>1</code></strong><code>)</code><br/>
<code>C = 110   (code value: 6 decimal, bits: </code><strong><code>3</code></strong><code>)</code><br/>
<code>D = 111   (code value: 7 decimal, bits: </code><strong><code>3</code></strong><code>)</code></p>

<p>Since two-thirds of the constraints are known, only the <strong>number of bits</strong> for each symbol need be transmitted:</p>

<p><code>2, 1, 3, 3</code></p>

<p>With knowledge of the canonical Huffman algorithm, it is then possible to recreate the entire table (symbol and code values) from just the bit-lengths. Unused symbols are normally transmitted as having zero bit length.</p>

<p>Another efficient way representing the codebook is to list all symbols in increasing order by their bit-lengths, and record the number of symbols for each bit-length. For the example mentioned above the encoding becomes:</p>

<p><code>(1,1,2), ('B','A','C','D')</code></p>

<p>It means that the first symbol <em>B</em> is of length 1, then the <em>A</em> of length 2, and remains of 3. Since the symbols are sorted by bit-length, we can efficiently reconstruct the codebook. A <a href="pseudo_code" title="wikilink">pseudo code</a> describing the reconstruction is introduced on the next section.</p>

<p>This type of encoding take great advantages when only a few symbols in the alphabet are being compressed. For example, if the codebook contains only 4 letters <em>C</em>, <em>O</em>, <em>D</em> and <em>E</em>, each of length 2. To represent the letter <em>O</em> using the previous method we need to either add a lot of zeros:</p>

<p><code>0, 0, 2, 2, 2, 0, ... , 2, ...</code></p>

<p>or record which 4 letters we have used. Each way makes the description longer than:</p>

<p><code>(0,4), ('C','O','D','E')</code></p>

<p>The <a href="JPEG_File_Interchange_Format" title="wikilink">JPEG File Interchange Format</a> actually adopts this way of encoding, because at most only 162 symbols out of the <a class="uri" href="8-bit" title="wikilink">8-bit</a> alphabet, which has size 256, will be in the codebook.</p>
<h2 id="pseudo-code">Pseudo code</h2>

<p>Given a list of symbols sorted by bit-length, the following <a href="pseudo_code" title="wikilink">pseudo code</a> will print a canonical Huffman code book:</p>

<p><code>code = 0</code><br/>
<code>while more symbols:</code><br/>
<code>    print symbol, code</code><br/>
<code>    code = (code + 1) </code></p>
</body>
</html>
