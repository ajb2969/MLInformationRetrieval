<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1621">K-means clustering</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>K-means clustering</h1>
<hr/>

<p><strong><em>k</em>-means clustering</strong> is a method of <a href="vector_quantization" title="wikilink">vector quantization</a>, originally from signal processing, that is popular for <a href="cluster_analysis" title="wikilink">cluster analysis</a> in <a href="data_mining" title="wikilink">data mining</a>. <em>k</em>-means clustering aims to <a href="partition_of_a_set" title="wikilink">partition</a> <em>n</em> observations into <em>k</em> clusters in which each observation belongs to the cluster with the nearest <a class="uri" href="mean" title="wikilink">mean</a>, serving as a <a class="uri" href="prototype" title="wikilink">prototype</a> of the cluster. This results in a partitioning of the data space into <a href="Voronoi_cell" title="wikilink">Voronoi cells</a>.</p>

<p>The problem is computationally difficult (<a class="uri" href="NP-hard" title="wikilink">NP-hard</a>); however, there are efficient <a href="heuristic_algorithm" title="wikilink">heuristic algorithms</a> that are commonly employed and converge quickly to a <a href="local_optimum" title="wikilink">local optimum</a>. These are usually similar to the <a href="expectation-maximization_algorithm" title="wikilink">expectation-maximization algorithm</a> for <a href="Mixture_model" title="wikilink">mixtures</a> of <a href="Gaussian_distribution" title="wikilink">Gaussian distributions</a> via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, <em>k</em>-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.</p>

<p>The algorithm has nothing to do with and should not be confused with <a href="k-nearest_neighbor" title="wikilink"><em>k</em>-nearest neighbor</a>, another popular <a href="machine_learning" title="wikilink">machine learning</a> technique.</p>
<h2 id="description">Description</h2>

<p>Given a set of observations (<strong>x</strong><sub>1</sub>, <strong>x</strong><sub>2</sub>, ‚Ä¶, <strong>x</strong><sub><em>n</em></sub>), where each observation is a <em>d</em>-dimensional real vector, <em>k</em>-means clustering aims to partition the <em>n</em> observations into <em>k</em> (‚â§ <em>n</em>) sets <strong>S</strong>¬†=¬†{<em>S</em><sub>1</sub>,¬†<em>S</em><sub>2</sub>,¬†‚Ä¶,¬†<em>S</em><sub><em>k</em></sub>} so as to minimize the within-cluster sum of squares (WCSS). In other words, its objective is to find:</p>
<center>

<p>

<math display="inline" id="K-means_clustering:0">
 <semantics>
  <mrow>
   <munder accentunder="true">
    <mrow>
     <mpadded width="+1.7pt">
      <mi>arg</mi>
     </mpadded>
     <mi>min</mi>
    </mrow>
    <mo>ùêí</mo>
   </munder>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">‚àë</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>k</mi>
    </msubsup>
    <mrow>
     <msub>
      <mo largeop="true" symmetric="true">‚àë</mo>
      <mrow>
       <mi>ùê±</mi>
       <mo>‚àà</mo>
       <msub>
        <mi>S</mi>
        <mi>i</mi>
       </msub>
      </mrow>
     </msub>
     <msup>
      <mrow>
       <mo>‚à•</mo>
       <mrow>
        <mi>ùê±</mi>
        <mo>-</mo>
        <msub>
         <mi>ùùÅ</mi>
         <mi>i</mi>
        </msub>
       </mrow>
       <mo>‚à•</mo>
      </mrow>
      <mn>2</mn>
     </msup>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <ci>ùêí</ci>
     <apply>
      <times></times>
      <ci>arg</ci>
      <ci>min</ci>
     </apply>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>k</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <in></in>
        <ci>ùê±</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>S</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="latexml">norm</csymbol>
        <apply>
         <minus></minus>
         <ci>ùê±</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>ùùÅ</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \underset{\mathbf{S}}{\operatorname{arg\,min}}\sum_{i=1}^{k}\sum_{\mathbf{x}%
\in S_{i}}\left\|\mathbf{x}-\boldsymbol{\mu}_{i}\right\|^{2}
  </annotation>
 </semantics>
</math>

</p>
</center>

<p>where <strong><em>Œº</em></strong><sub><em>i</em></sub> is the mean of points in <em>S</em><sub><em>i</em></sub>.</p>
<h2 id="history">History</h2>

<p>The term "<em>k</em>-means" was first used by James MacQueen in 1967,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> though the idea goes back to <a href="Hugo_Steinhaus" title="wikilink">Hugo Steinhaus</a> in 1957.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> The <a href="#Standard_algorithm" title="wikilink">standard algorithm</a> was first proposed by Stuart Lloyd in 1957 as a technique for <a href="pulse-code_modulation" title="wikilink">pulse-code modulation</a>, though it wasn't published outside of <a href="Bell_Labs" title="wikilink">Bell Labs</a> until 1982.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> In 1965, E.W.Forgy published essentially the same method, which is why it is sometimes referred to as Lloyd-Forgy.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> A more efficient version was proposed and published in Fortran by Hartigan and Wong in 1975/1979.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h2 id="algorithms">Algorithms</h2>
<h3 id="standard-algorithm">Standard algorithm</h3>

<p>The most common algorithm uses an iterative refinement technique. Due to its ubiquity it is often called the <strong><em>k</em>-means algorithm</strong>; it is also referred to as <strong><a href="Lloyd's_algorithm" title="wikilink">Lloyd's algorithm</a></strong>, particularly in the computer science community.</p>

<p>Given an initial set of <em>k</em> means <em>m</em><sub>1</sub><sup>(1)</sup>,‚Ä¶,<em>m</em><sub><em>k</em></sub><sup>(1)</sup> (see below), the algorithm proceeds by alternating between two steps:<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<dl>
<dd><strong>Assignment step</strong>: Assign each observation to the cluster whose mean yields the least within-cluster sum of squares (WCSS). Since the sum of squares is the squared <a href="Euclidean_distance" title="wikilink">Euclidean distance</a>, this is intuitively the "nearest" mean.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> (Mathematically, this means partitioning the observations according to the <a href="Voronoi_diagram" title="wikilink">Voronoi diagram</a> generated by the means).

<p>

<math display="block" id="K-means_clustering:1">
 <semantics>
  <mrow>
   <mrow>
    <msubsup>
     <mi>S</mi>
     <mi>i</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>t</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </msubsup>
    <mo>=</mo>
    <mrow>
     <mo maxsize="120%" minsize="120%">{</mo>
     <msub>
      <mi>x</mi>
      <mi>p</mi>
     </msub>
     <mo>:</mo>
     <mrow>
      <mrow>
       <msup>
        <mrow>
         <mo mathsize="120%" stretchy="false">‚à•</mo>
         <mrow>
          <msub>
           <mi>x</mi>
           <mi>p</mi>
          </msub>
          <mo>-</mo>
          <msubsup>
           <mi>m</mi>
           <mi>i</mi>
           <mrow>
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo stretchy="false">)</mo>
           </mrow>
          </msubsup>
         </mrow>
         <mo mathsize="120%" stretchy="false">‚à•</mo>
        </mrow>
        <mn>2</mn>
       </msup>
       <mo>‚â§</mo>
       <mrow>
        <mpadded width="+5pt">
         <msup>
          <mrow>
           <mo mathsize="120%" stretchy="false">‚à•</mo>
           <mrow>
            <msub>
             <mi>x</mi>
             <mi>p</mi>
            </msub>
            <mo>-</mo>
            <msubsup>
             <mi>m</mi>
             <mi>j</mi>
             <mrow>
              <mo stretchy="false">(</mo>
              <mi>t</mi>
              <mo stretchy="false">)</mo>
             </mrow>
            </msubsup>
           </mrow>
           <mo mathsize="120%" stretchy="false">‚à•</mo>
          </mrow>
          <mn>2</mn>
         </msup>
        </mpadded>
        <mrow>
         <mo>‚àÄ</mo>
         <mi>j</mi>
        </mrow>
       </mrow>
      </mrow>
      <mo>,</mo>
      <mrow>
       <mn>1</mn>
       <mo>‚â§</mo>
       <mi>j</mi>
       <mo>‚â§</mo>
       <mi>k</mi>
      </mrow>
     </mrow>
     <mo maxsize="120%" minsize="120%">}</mo>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>S</ci>
      <ci>i</ci>
     </apply>
     <ci>t</ci>
    </apply>
    <apply>
     <csymbol cd="latexml">conditional-set</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>p</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">formulae-sequence</csymbol>
      <apply>
       <leq></leq>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="latexml">norm</csymbol>
         <apply>
          <minus></minus>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>p</ci>
          </apply>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <apply>
            <csymbol cd="ambiguous">superscript</csymbol>
            <ci>m</ci>
            <ci>t</ci>
           </apply>
           <ci>i</ci>
          </apply>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="latexml">norm</csymbol>
          <apply>
           <minus></minus>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>x</ci>
            <ci>p</ci>
           </apply>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <apply>
             <csymbol cd="ambiguous">superscript</csymbol>
             <ci>m</ci>
             <ci>t</ci>
            </apply>
            <ci>j</ci>
           </apply>
          </apply>
         </apply>
         <cn type="integer">2</cn>
        </apply>
        <apply>
         <csymbol cd="latexml">for-all</csymbol>
         <ci>j</ci>
        </apply>
       </apply>
      </apply>
      <apply>
       <and></and>
       <apply>
        <leq></leq>
        <cn type="integer">1</cn>
        <ci>j</ci>
       </apply>
       <apply>
        <leq></leq>
        <share href="#.cmml">
        </share>
        <ci>k</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S_{i}^{(t)}=\big\{x_{p}:\big\|x_{p}-m^{(t)}_{i}\big\|^{2}\leq\big\|x_{p}-m^{(t%
)}_{j}\big\|^{2}\ \forall j,1\leq j\leq k\big\},
  </annotation>
 </semantics>
</math>

</p>
<dl>
<dd>where each 

<math display="inline" id="K-means_clustering:2">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>p</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>p</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{p}
  </annotation>
 </semantics>
</math>

 is assigned to exactly one 

<math display="inline" id="K-means_clustering:3">
 <semantics>
  <msup>
   <mi>S</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>t</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>S</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S^{(t)}
  </annotation>
 </semantics>
</math>

, even if it could be assigned to two or more of them.
</dd>
</dl>
</dd>
<dd><strong>Update step</strong>: Calculate the new means to be the <a class="uri" href="centroids" title="wikilink">centroids</a> of the observations in the new clusters.

<p>

<math display="block" id="K-means_clustering:4">
 <semantics>
  <mrow>
   <msubsup>
    <mi>m</mi>
    <mi>i</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>t</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </msubsup>
   <mo>=</mo>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mrow>
      <mo stretchy="false">|</mo>
      <msubsup>
       <mi>S</mi>
       <mi>i</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </msubsup>
      <mo stretchy="false">|</mo>
     </mrow>
    </mfrac>
    <mrow>
     <munder>
      <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
      <mrow>
       <msub>
        <mi>x</mi>
        <mi>j</mi>
       </msub>
       <mo>‚àà</mo>
       <msubsup>
        <mi>S</mi>
        <mi>i</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>t</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </msubsup>
      </mrow>
     </munder>
     <msub>
      <mi>x</mi>
      <mi>j</mi>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>m</ci>
      <apply>
       <plus></plus>
       <ci>t</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>i</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <apply>
       <abs></abs>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>S</ci>
         <ci>t</ci>
        </apply>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <in></in>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>j</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <ci>S</ci>
          <ci>t</ci>
         </apply>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>j</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m^{(t+1)}_{i}=\frac{1}{|S^{(t)}_{i}|}\sum_{x_{j}\in S^{(t)}_{i}}x_{j}
  </annotation>
 </semantics>
</math>

</p>
<dl>
<dd>Since the arithmetic mean is a <a href="least-squares_estimation" title="wikilink">least-squares estimator</a>, this also minimizes the within-cluster sum of squares (WCSS) objective.
</dd>
</dl>
</dd>
</dl>

<p>The algorithm has converged when the assignments no longer change. Since both steps optimize the WCSS objective, and there only exists a finite number of such partitionings, the algorithm must converge to a (local) optimum. There is no guarantee that the global optimum is found using this algorithm.</p>

<p>The algorithm is often presented as assigning objects to the nearest cluster by distance. The standard algorithm aims at minimizing the WCSS objective, and thus assigns by "least sum of squares", which is exactly equivalent to assigning by the smallest Euclidean distance. Using a different distance function other than (squared) Euclidean distance may stop the algorithm from converging. Various modifications of k-means such as spherical k-means and <a class="uri" href="k-medoids" title="wikilink">k-medoids</a> have been proposed to allow using other distance measures.</p>
<h4 id="initialization-methods">Initialization methods</h4>

<p>Commonly used initialization methods are Forgy and Random Partition.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> The Forgy method randomly chooses <em>k</em> observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster's randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al.,<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> the Random Partition method is generally preferable for algorithms such as the <em>k</em>-harmonic means and fuzzy <em>k</em>-means. For expectation maximization and standard <em>k</em>-means algorithms, the Forgy method of initialization is preferable.</p>

<p>Image:K Means Example Step 1.svg|1. <em>k</em> initial "means" (in this case <em>k</em>=3) are randomly generated within the data domain (shown in color). Image:K Means Example Step 2.svg|2. <em>k</em> clusters are created by associating every observation with the nearest mean. The partitions here represent the <a href="Voronoi_diagram" title="wikilink">Voronoi diagram</a> generated by the means. Image:K Means Example Step 3.svg|3. The <a class="uri" href="centroid" title="wikilink">centroid</a> of each of the <em>k</em> clusters becomes the new mean. Image:K Means Example Step 4.svg|4. Steps 2 and 3 are repeated until convergence has been reached.</p>

<p>As it is a heuristic algorithm, there is no guarantee that it will converge to the global optimum, and the result may depend on the initial clusters. As the algorithm is usually very fast, it is common to run it multiple times with different starting conditions. However, in the worst case, <em>k</em>-means can be very slow to converge: in particular it has been shown that there exist certain point sets, even in 2 dimensions, on which <em>k</em>-means takes exponential time, that is <mtpl></mtpl>, to converge.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> These point sets do not seem to arise in practice: this is corroborated by the fact that the <a href="Smoothed_analysis" title="wikilink">smoothed</a> running time of <em>k</em>-means is polynomial.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></p>

<p>The "assignment" step is also referred to as <strong>expectation step</strong>, the "update step" as <strong>maximization step</strong>, making this algorithm a variant of the <em>generalized</em> <a href="expectation-maximization_algorithm" title="wikilink">expectation-maximization algorithm</a>.</p>
<h3 id="complexity">Complexity</h3>

<p>Regarding computational complexity, finding the optimal solution to the <em>k</em>-means clustering problem for observations in <em>d</em> dimensions is:</p>
<ul>
<li><a class="uri" href="NP-hard" title="wikilink">NP-hard</a> in general Euclidean space <em>d</em> even for 2 clusters<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><ref></ref></li>
</ul>

<p></p>
<ul>
<li><a class="uri" href="NP-hard" title="wikilink">NP-hard</a> for a general number of clusters <em>k</em> even in the plane<ref></ref></li>
</ul>

<p></p>
<ul>
<li>If <em>k</em> and <em>d</em> (the dimension) are fixed, the problem can be exactly solved in time 

<math display="inline" id="K-means_clustering:5">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <msup>
      <mi>n</mi>
      <mrow>
       <mrow>
        <mi>d</mi>
        <mi>k</mi>
       </mrow>
       <mo>+</mo>
       <mn>1</mn>
      </mrow>
     </msup>
     <mrow>
      <mi>log</mi>
      <mi>n</mi>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>n</ci>
      <apply>
       <plus></plus>
       <apply>
        <times></times>
        <ci>d</ci>
        <ci>k</ci>
       </apply>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <log></log>
      <ci>n</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(n^{dk+1}\log{n})
  </annotation>
 </semantics>
</math>

, where <em>n</em> is the number of entities to be clustered<ref></ref></li>
</ul>

<p> Thus, a variety of <a href="heuristic_algorithm" title="wikilink">heuristic algorithms</a> such as Lloyds algorithm given above are generally used.</p>

<p>The running time of Lloyds algorithm is often given as 

<math display="inline" id="K-means_clustering:6">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>n</mi>
     <mi>k</mi>
     <mi>d</mi>
     <mi>i</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <times></times>
     <ci>n</ci>
     <ci>k</ci>
     <ci>d</ci>
     <ci>i</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(nkdi)
  </annotation>
 </semantics>
</math>

, where <em>n</em> is the number of <em>d</em>-dimensional vectors, <em>k</em> the number of clusters and <em>i</em> the number of iterations needed until convergence. On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyds algorithm is therefore often considered to be of "linear" complexity in practice.</p>

<p>Following are some recent insights into this algorithm complexity behaviour.</p>
<ul>
<li>Lloyd's <em>k</em>-means algorithm has polynomial smoothed running time. It is shown that<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> for arbitrary set of <em>n</em> points in 

<math display="inline" id="K-means_clustering:7">
 <semantics>
  <msup>
   <mrow>
    <mo stretchy="false">[</mo>
    <mn>0</mn>
    <mo>,</mo>
    <mn>1</mn>
    <mo stretchy="false">]</mo>
   </mrow>
   <mi>d</mi>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <interval closure="closed">
     <cn type="integer">0</cn>
     <cn type="integer">1</cn>
    </interval>
    <ci>d</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   [0,1]^{d}
  </annotation>
 </semantics>
</math>

, if each point is independently perturbed by a normal distribution with mean 

<math display="inline" id="K-means_clustering:8">
 <semantics>
  <mn>0</mn>
  <annotation-xml encoding="MathML-Content">
   <cn type="integer">0</cn>
  </annotation-xml>
 </semantics>
</math>

 and variance 

<math display="inline" id="K-means_clustering:9">
 <semantics>
  <msup>
   <mi>œÉ</mi>
   <mn>2</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>œÉ</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma^{2}
  </annotation>
 </semantics>
</math>

, then the expected running time of 

<math display="inline" id="K-means_clustering:10">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

-means algorithm is bounded by 

<math display="inline" id="K-means_clustering:11">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mrow>
      <msup>
       <mi>n</mi>
       <mn>34</mn>
      </msup>
      <msup>
       <mi>k</mi>
       <mn>34</mn>
      </msup>
      <msup>
       <mi>d</mi>
       <mn>8</mn>
      </msup>
      <mi>l</mi>
      <mi>o</mi>
      <msup>
       <mi>g</mi>
       <mn>4</mn>
      </msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>n</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>/</mo>
     <msup>
      <mi>œÉ</mi>
      <mn>6</mn>
     </msup>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>n</ci>
       <cn type="integer">34</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>k</ci>
       <cn type="integer">34</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>d</ci>
       <cn type="integer">8</cn>
      </apply>
      <ci>l</ci>
      <ci>o</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>g</ci>
       <cn type="integer">4</cn>
      </apply>
      <ci>n</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>œÉ</ci>
      <cn type="integer">6</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(n^{34}k^{34}d^{8}log^{4}(n)/\sigma^{6})
  </annotation>
 </semantics>
</math>

, which is a polynomial in 

<math display="inline" id="K-means_clustering:12">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="K-means_clustering:13">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="K-means_clustering:14">
 <semantics>
  <mi>d</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="K-means_clustering:15">
 <semantics>
  <mrow>
   <mn>1</mn>
   <mo>/</mo>
   <mi>œÉ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <cn type="integer">1</cn>
    <ci>œÉ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1/\sigma
  </annotation>
 </semantics>
</math>

.</li>
</ul>
<ul>
<li>Better bounds are proved for simple cases. For example,<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> showed that the running time of <em>k</em>-means algorithm is bounded by 

<math display="inline" id="K-means_clustering:16">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>d</mi>
     <msup>
      <mi>n</mi>
      <mn>4</mn>
     </msup>
     <msup>
      <mi>M</mi>
      <mn>2</mn>
     </msup>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <times></times>
     <ci>d</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>n</ci>
      <cn type="integer">4</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>M</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(dn^{4}M^{2})
  </annotation>
 </semantics>
</math>

 for 

<math display="inline" id="K-means_clustering:17">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 points in an <a href="integer_lattice" title="wikilink">integer lattice</a> 

<math display="inline" id="K-means_clustering:18">
 <semantics>
  <msup>
   <mrow>
    <mo stretchy="false">{</mo>
    <mn>1</mn>
    <mo>,</mo>
    <mi mathvariant="normal">‚Ä¶</mi>
    <mo>,</mo>
    <mi>M</mi>
    <mo stretchy="false">}</mo>
   </mrow>
   <mi>d</mi>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <set>
     <cn type="integer">1</cn>
     <ci>normal-‚Ä¶</ci>
     <ci>M</ci>
    </set>
    <ci>d</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{1,\dots,M\}^{d}
  </annotation>
 </semantics>
</math>

.</li>
</ul>
<h3 id="variations">Variations</h3>
<ul>
<li><a href="Jenks_natural_breaks_optimization" title="wikilink">Jenks natural breaks optimization</a>: <em>k</em>-means applied to univariate data</li>
<li><a href="k-medians_clustering" title="wikilink">k-medians clustering</a> uses the median in each dimension instead of the mean, and this way minimizes 

<math display="inline" id="K-means_clustering:19">
 <semantics>
  <msub>
   <mi>L</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>L</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L_{1}
  </annotation>
 </semantics>
</math>

 norm (<a href="Taxicab_geometry" title="wikilink">Taxicab geometry</a>).</li>
<li><a class="uri" href="k-medoids" title="wikilink">k-medoids</a> (also: Partitioning Around Medoids, PAM) uses the medoid instead of the mean, and this way minimizes the sum of distances for <em>arbitrary</em> distance functions.</li>
<li><a href="Fuzzy_clustering#Fuzzy_c-means_clustering" title="wikilink">Fuzzy C-Means Clustering</a> is a soft version of K-means, where each data point has a fuzzy degree of belonging to each cluster.</li>
<li><a href="Mixture_model#Gaussian_mixture_model" title="wikilink">Gaussian mixture</a> models trained with <a href="expectation-maximization_algorithm" title="wikilink">expectation-maximization algorithm</a> (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means.</li>
<li><a class="uri" href="k-means++" title="wikilink">k-means++</a> chooses initial centers in a way that gives a provable upper bound on the WCSS objective.</li>
<li>The filtering algorithm uses <a href="kd-tree" title="wikilink">kd-trees</a> to speed up each k-means step.<ref></ref></li>
</ul>

<p></p>
<ul>
<li>Some methods attempt to speed up each k-means step using <a href="coreset" title="wikilink">coresets</a><ref></ref></li>
</ul>

<p> or the <a href="triangle_inequality" title="wikilink">triangle inequality</a>.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a></p>
<ul>
<li>Escape local optima by swapping points between clusters.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></li>
</ul>
<ul>
<li>The <a href="Spherical_k-means" title="wikilink">Spherical k-means</a> clustering algorithm is suitable for directional data.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a></li>
</ul>
<ul>
<li>The <a href="Minkowski_metric_weighted_k-means" title="wikilink">Minkowski metric weighted k-means</a> deals with irrelevant features by assigning cluster specific weights to each feature<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a></li>
</ul>
<h2 id="discussion">Discussion</h2>

<p> </p>

<p>The two key features of <em>k</em>-means which make it efficient are often regarded as its biggest drawbacks:</p>
<ul>
<li><a href="Euclidean_distance" title="wikilink">Euclidean distance</a> is used as a <a href="metric_(mathematics)" title="wikilink">metric</a> and <a class="uri" href="variance" title="wikilink">variance</a> is used as a measure of cluster scatter.</li>
<li>The number of clusters <em>k</em> is an input parameter: an inappropriate choice of <em>k</em> may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for <a href="determining_the_number_of_clusters_in_a_data_set" title="wikilink">determining the number of clusters in the data set</a>.</li>
<li>Convergence to a local minimum may produce counterintuitive ("wrong") results (see example in Fig.).</li>
</ul>

<p>A key limitation of <em>k</em>-means is its cluster model. The concept is based on spherical clusters that are separable in a way so that the mean value converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying <em>k</em>-means with a value of 

<math display="inline" id="K-means_clustering:20">
 <semantics>
  <mrow>
   <mi>k</mi>
   <mo>=</mo>
   <mn>3</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>k</ci>
    <cn type="integer">3</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k=3
  </annotation>
 </semantics>
</math>

 onto the well-known <a href="Iris_flower_data_set" title="wikilink">Iris flower data set</a>, the result often fails to separate the three <a href="Iris_(plant)" title="wikilink">Iris</a> species contained in the data set. With 

<math display="inline" id="K-means_clustering:21">
 <semantics>
  <mrow>
   <mi>k</mi>
   <mo>=</mo>
   <mn>2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>k</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k=2
  </annotation>
 </semantics>
</math>

, the two visible clusters (one containing two species) will be discovered, whereas with 

<math display="inline" id="K-means_clustering:22">
 <semantics>
  <mrow>
   <mi>k</mi>
   <mo>=</mo>
   <mn>3</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>k</ci>
    <cn type="integer">3</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k=3
  </annotation>
 </semantics>
</math>

 one of the two clusters will be split into two even parts. In fact, 

<math display="inline" id="K-means_clustering:23">
 <semantics>
  <mrow>
   <mi>k</mi>
   <mo>=</mo>
   <mn>2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>k</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k=2
  </annotation>
 </semantics>
</math>

 is more appropriate for this data set, despite the data set containing 3 <em>classes</em>. As with any other clustering algorithm, the <em>k</em>-means result relies on the data set to satisfy the assumptions made by the clustering algorithms. It works well on some data sets, while failing on others.</p>

<p>The result of <em>k</em>-means can also be seen as the <a href="Voronoi_diagram" title="wikilink">Voronoi cells</a> of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the "mouse" example. The Gaussian models used by the <a href="Expectation-maximization_algorithm" title="wikilink">Expectation-maximization algorithm</a> (which can be seen as a generalization of <em>k</em>-means) are more flexible here by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than <em>k</em>-means as well as correlated clusters (not in this example).</p>
<h2 id="applications">Applications</h2>

<p><em>k</em>-means clustering in particular when using heuristics such as Lloyd's algorithm is rather easy to implement and apply even on large data sets. As such, it has been successfully used in various topics, including <a href="market_segmentation" title="wikilink">market segmentation</a>, <a href="computer_vision" title="wikilink">computer vision</a>, <a class="uri" href="geostatistics" title="wikilink">geostatistics</a>,<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> <a class="uri" href="astronomy" title="wikilink">astronomy</a> and <a href="Data_Mining_in_Agriculture" title="wikilink">agriculture</a>. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.</p>
<h3 id="vector-quantization">Vector quantization</h3>

<p> </p>

<p><em>k</em>-means originates from signal processing, and still finds use in this domain. For example in computer graphics, <a href="color_quantization" title="wikilink">color quantization</a> is the task of reducing the color palette of an image to a fixed number of colors <em>k</em>. The <em>k</em>-means algorithm can easily be used for this task and produces competitive results. Other uses of vector quantization include <a href="Sampling_(statistics)" title="wikilink">non-random sampling</a>, as <em>k</em>-means can easily be used to choose <em>k</em> different but prototypical objects from a large data set for further analysis.</p>
<h3 id="cluster-analysis">Cluster analysis</h3>

<p>In cluster analysis, the <em>k</em>-means algorithm can be used to partition the input data set into <em>k</em> partitions (clusters).</p>

<p>However, the pure <em>k</em>-means algorithm is not very flexible, and as such of limited use (except for when vector quantization as above is actually the desired use case!). In particular, the parameter <em>k</em> is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation of the algorithm is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms have been developed since.</p>
<h3 id="feature-learning">Feature learning</h3>

<p><em>k</em>-means clustering has been used as a <a href="feature_learning" title="wikilink">feature learning</a> (or <a href="dictionary_learning" title="wikilink">dictionary learning</a>) step, in either (<a href="semi-supervised_learning" title="wikilink">semi-</a>)<a href="supervised_learning" title="wikilink">supervised learning</a> or <a href="unsupervised_learning" title="wikilink">unsupervised learning</a>.<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> The basic approach is first to train a <em>k</em>-means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, we have a choice of "encoding" functions, but we can use for example the thresholded matrix-product of the datum with the centroid locations, the distance from the datum to each centroid, or simply an indicator function for the nearest centroid,<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a><a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a> or some smooth transformation of the distance.<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a> Alternatively, by transforming the sample-cluster distance through a <a href="Radial_basis_function" title="wikilink">Gaussian RBF</a>, one effectively obtains the hidden layer of a <a href="radial_basis_function_network" title="wikilink">radial basis function network</a>.<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a></p>

<p>This use of <em>k</em>-means has been successfully combined with simple, <a href="linear_classifier" title="wikilink">linear classifiers</a> for semi-supervised learning in <a href="natural_language_processing" title="wikilink">NLP</a> (specifically for <a href="named_entity_recognition" title="wikilink">named entity recognition</a>)<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> and in <a href="computer_vision" title="wikilink">computer vision</a>. On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as <a href="autoencoder" title="wikilink">autoencoders</a> and <a href="restricted_Boltzmann_machine" title="wikilink">restricted Boltzmann machines</a>.<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a> However, it generally requires more data than the sophisticated methods, for equivalent performance, because each data point only contributes to one "feature" rather than multiple.<a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a></p>
<h2 id="relation-to-other-statistical-machine-learning-algorithms">Relation to other statistical machine learning algorithms</h2>

<p><em>k</em>-means clustering, and its associated <a href="Expectation‚Äìmaximization_algorithm" title="wikilink">expectation-maximization algorithm</a>, is a special case of a <a href="Mixture_model" title="wikilink">Gaussian mixture model</a>, specifically, the limit of taking all covariances as diagonal, equal, and small. It is often easy to generalize a <em>k</em>-means problem into a Gaussian mixture model.<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a> Another generalization of the k-means algorithm is the <a class="uri" href="K-SVD" title="wikilink">K-SVD</a> algorithm, which estimates data points as a sparse linear combination of "codebook vectors". K-means corresponds to the special case of using a single codebook vector, with a weight of 1.<a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a></p>
<h3 id="mean-shift-clustering">Mean shift clustering</h3>

<p>Basic <a href="mean_shift" title="wikilink">mean shift</a> clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. Then this set is iteratively replaced by the mean of those points in the set that are within a given distance of that point. By contrast, <em>k</em>-means restricts this updated set to <em>k</em> points usually much less than the number of points in the input data set, and replaces each point in this set by the mean of all points in the <em>input set</em> that are closer to that point than any other (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to <em>k</em>-means, called <em>likelihood mean shift</em>, replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set.<a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a> One of the advantages of mean shift over <em>k</em>-means is that there is no need to choose the number of clusters, because mean shift is likely to find only a few clusters if indeed only a small number exist. However, mean shift can be much slower than <em>k</em>-means, and still requires selection of a bandwidth parameter. Mean shift has soft variants much as <em>k</em>-means does.</p>
<h3 id="principal-component-analysis-pca">Principal component analysis (PCA)</h3>

<p>It was asserted in<a class="footnoteRef" href="#fn32" id="fnref32"><sup>32</sup></a><a class="footnoteRef" href="#fn33" id="fnref33"><sup>33</sup></a> that the relaxed solution of <mtpl></mtpl>-means clustering, specified by the cluster indicators, is given by the PCA (<a href="principal_component_analysis" title="wikilink">principal component analysis</a>) principal components, and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace. However, that PCA is a useful relaxation of k-means clustering was not a new result (see, for example,<a class="footnoteRef" href="#fn34" id="fnref34"><sup>34</sup></a>), and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.<a class="footnoteRef" href="#fn35" id="fnref35"><sup>35</sup></a></p>
<h3 id="independent-component-analysis-ica">Independent component analysis (ICA)</h3>

<p>It has been shown in <a class="footnoteRef" href="#fn36" id="fnref36"><sup>36</sup></a> that under sparsity assumptions and when input data is pre-processed with the <a href="whitening_transformation" title="wikilink">whitening transformation</a> <em>k</em>-means produces the solution to the linear <a href="Independent_component_analysis" title="wikilink">Independent component analysis</a> task. This aids in explaining the successful application of <em>k</em>-means to <a href="k-means_clustering#Feature_learning" title="wikilink">feature learning</a>.</p>
<h3 id="bilateral-filtering">Bilateral filtering</h3>

<p><em>k</em>-means implicitly assumes that the ordering of the input data set does not matter. The <a href="bilateral_filter" title="wikilink">bilateral filter</a> is similar to K-means and <a href="mean_shift" title="wikilink">mean shift</a> in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data.<a class="footnoteRef" href="#fn37" id="fnref37"><sup>37</sup></a> This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance.</p>
<h2 id="similar-problems">Similar problems</h2>

<p>The set of squared error minimizing cluster functions also includes the <a href="k-medoids" title="wikilink"><mtpl></mtpl>-medoids</a> algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e., it uses <a class="uri" href="medoids" title="wikilink">medoids</a> in place of <a class="uri" href="centroids" title="wikilink">centroids</a>.</p>
<h2 id="software-implementations">Software Implementations</h2>
<h3 id="free">Free</h3>
<ul>
<li><a class="uri" href="CrimeStat" title="wikilink">CrimeStat</a> implements two spatial <em>k</em>-means algorithms, one of which allows the user to define the starting locations.</li>
<li><a class="uri" href="ELKI" title="wikilink">ELKI</a> contains <em>k</em>-means (with Lloyd and MacQueen iteration, along with different initializations such as <em>k</em>-means++ initialization) and various more advanced clustering algorithms.</li>
<li><a href="julia_language" title="wikilink">Julia</a> contains a <em>k</em>-means implementation in the Clustering package.<a class="footnoteRef" href="#fn38" id="fnref38"><sup>38</sup></a></li>
<li><a href="Apache_Mahout" title="wikilink">Mahout</a> contains a <a class="uri" href="MapReduce" title="wikilink">MapReduce</a> based <em>k</em>-means.</li>
<li><a href="MLPACK_(C++_library)" title="wikilink">MLPACK</a> contains a C++ implementation of <em>k</em>-means.</li>
<li><a href="GNU_Octave" title="wikilink">Octave</a> contains <em>k</em>-means.</li>
<li><a class="uri" href="OpenCV" title="wikilink">OpenCV</a> contains a <em>k</em>-means implementation.</li>
<li><a href="R_(programming_language)" title="wikilink">R</a> contains three <em>k</em>-means variations.<a class="footnoteRef" href="#fn39" id="fnref39"><sup>39</sup></a><a class="footnoteRef" href="#fn40" id="fnref40"><sup>40</sup></a><a class="footnoteRef" href="#fn41" id="fnref41"><sup>41</sup></a></li>
<li><a class="uri" href="SciPy" title="wikilink">SciPy</a> and <a class="uri" href="scikit-learn" title="wikilink">scikit-learn</a> contain multiple <em>k</em>-means implementations.</li>
<li><a href="Apache_Spark" title="wikilink">Spark</a> MLlib implements a distributed <em>k</em>-means algorithm.</li>
<li><a href="Torch_(machine_learning)" title="wikilink">Torch</a> contains an <em>unsup</em> package that provides <em>k</em>-means clustering.</li>
<li><a href="Weka_(machine_learning)" title="wikilink">Weka</a> contains <em>k</em>-means and <em>x</em>-means.</li>
</ul>
<h3 id="commercial">Commercial</h3>
<ul>
<li><a href="IChrome_Ltd.#Grapheme" title="wikilink">Grapheme</a></li>
<li><a class="uri" href="MATLAB" title="wikilink">MATLAB</a></li>
<li><a class="uri" href="Mathematica" title="wikilink">Mathematica</a></li>
<li><a href="SAS_System" title="wikilink">SAS</a></li>
<li><a class="uri" href="Stata" title="wikilink">Stata</a></li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Canopy_clustering_algorithm" title="wikilink">Canopy clustering algorithm</a></li>
<li><a href="Centroidal_Voronoi_tessellation" title="wikilink">Centroidal Voronoi tessellation</a></li>
<li><a href="k_q-flats" title="wikilink">k q-flats</a></li>
<li><a href="Linde‚ÄìBuzo‚ÄìGray_algorithm" title="wikilink">Linde‚ÄìBuzo‚ÄìGray algorithm</a></li>
<li><a href="Nearest_centroid_classifier" title="wikilink">Nearest centroid classifier</a></li>
<li><a href="Self-organizing_map" title="wikilink">Self-organizing map</a></li>
<li><a href="silhouette_(clustering)" title="wikilink">Silhouette clustering</a></li>
<li><a href="Head/tail_Breaks" title="wikilink">Head/tail Breaks</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Data_clustering_algorithms" title="wikilink">Category:Data clustering algorithms</a> <a href="Category:Statistical_algorithms" title="wikilink">Category:Statistical algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">‚Ü©</a></li>
<li id="fn2"><a href="#fnref2">‚Ü©</a></li>
<li id="fn3"> Published in journal much later: <a href="#fnref3">‚Ü©</a></li>
<li id="fn4"><a href="#fnref4">‚Ü©</a></li>
<li id="fn5"><a href="#fnref5">‚Ü©</a></li>
<li id="fn6"></li>
<li id="fn7"><a href="#fnref7">‚Ü©</a></li>
<li id="fn8">Since the square root is a monotone function, this also is the minimum Euclidean distance assignment.<a href="#fnref8">‚Ü©</a></li>
<li id="fn9"><a href="#fnref9">‚Ü©</a></li>
<li id="fn10"></li>
<li id="fn11"><a href="#fnref11">‚Ü©</a></li>
<li id="fn12"><a href="#fnref12">‚Ü©</a></li>
<li id="fn13"><a href="#fnref13">‚Ü©</a></li>
<li id="fn14"></li>
<li id="fn15"><a href="http://www.cse.iitk.ac.in/users/bhowmick/lloyd.pdf">1</a><a href="#fnref15">‚Ü©</a></li>
<li id="fn16"><a href="#fnref16">‚Ü©</a></li>
<li id="fn17"><a href="#fnref17">‚Ü©</a></li>
<li id="fn18"><a href="#fnref18">‚Ü©</a></li>
<li id="fn19"><a href="#fnref19">‚Ü©</a></li>
<li id="fn20"><a href="#fnref20">‚Ü©</a></li>
<li id="fn21"><a href="#fnref21">‚Ü©</a></li>
<li id="fn22"></li>
<li id="fn23"><a href="#fnref23">‚Ü©</a></li>
<li id="fn24"></li>
<li id="fn25"><a href="#fnref25">‚Ü©</a></li>
<li id="fn26"><a href="#fnref26">‚Ü©</a></li>
<li id="fn27"><a href="#fnref27">‚Ü©</a></li>
<li id="fn28"></li>
<li id="fn29"><a href="#fnref29">‚Ü©</a></li>
<li id="fn30"><a href="#fnref30">‚Ü©</a></li>
<li id="fn31"><a href="#fnref31">‚Ü©</a></li>
<li id="fn32"><a href="#fnref32">‚Ü©</a></li>
<li id="fn33"><a href="#fnref33">‚Ü©</a></li>
<li id="fn34"><a href="#fnref34">‚Ü©</a></li>
<li id="fn35"><a href="#fnref35">‚Ü©</a></li>
<li id="fn36"><a href="#fnref36">‚Ü©</a></li>
<li id="fn37"></li>
<li id="fn38"><a href="https://github.com/JuliaStats/Clustering.jl">Clustering.jl</a> www.github.com<a href="#fnref38">‚Ü©</a></li>
<li id="fn39"></li>
<li id="fn40"></li>
<li id="fn41"></li>
</ol>
</section>
</body>
</html>
