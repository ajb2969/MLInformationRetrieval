   Leverage (statistics)      Leverage (statistics)   In statistics , leverage is a term used in connection with regression analysis and, in particular, in analyses aimed at identifying those observations that are far away from corresponding average predictor values. Leverage points do not necessarily have a large effect on the outcome of fitting regression models.  Leverage points are those observations, if any, made at extreme or outlying values of the independent variables such that the lack of neighboring observations means that the fitted regression model will pass close to that particular observation. 1  Modern computer packages for statistical analysis include, as part of their facilities for regression analysis, various quantitative measures for identifying influential observations : among these measures is partial leverage , a measure of how a variable contributes to the leverage of a datum.  Linear regression model  Definition  In linear regression model, the leverage score for the    i   t  h      superscript  i    t  h     i^{th}   data unit is defined as:        h   i  i    =    (  H  )    i  i         subscript  h    i  i     subscript  H    i  i      h_{ii}=(H)_{ii}      the    i   t  h      superscript  i    t  h     i^{th}   diagonal of the hat matrix     H  =   X    (    X  ′   X   )    -  1     X  ′        H    X   superscript     superscript  X  normal-′   X     1     superscript  X  normal-′      H=X(X^{\prime}X)^{-1}X^{\prime}   , where the apostrophe denotes the matrix transpose.  The leverage score is also known as the observation self-sensitivity or self-influence, 2 as:        h   i  i    =    ∂    y  ^   i     ∂   y  i      ,       subscript  h    i  i         subscript   normal-^  y   i       subscript  y  i       h_{ii}=\frac{\partial\hat{y}_{i}}{\partial y_{i}},   where     y  ^   i     subscript   normal-^  y   i    \hat{y}_{i}   and    y  i     subscript  y  i    {y}_{i}   are the fitted and measured observation, respectively.  Property 1      0  ≤   h   i  i    ≤  1        0   subscript  h    i  i         1     0\leq h_{ii}\leq 1     Proof  First, note that     H  2   =   X    (    X  ′   X   )    -  1     X  ′   X    (    X  ′   X   )    -  1     X  ′    =   X  I    (    X  ′   X   )    -  1     X  ′    =  H         superscript  H  2     X   superscript     superscript  X  normal-′   X     1     superscript  X  normal-′   X   superscript     superscript  X  normal-′   X     1     superscript  X  normal-′           X  I   superscript     superscript  X  normal-′   X     1     superscript  X  normal-′         H     H^{2}=X(X^{\prime}X)^{-1}X^{\prime}X(X^{\prime}X)^{-1}X^{\prime}=XI(X^{\prime}%
 X)^{-1}X^{\prime}=H   . Also, observe that   H   H   H   is symmetric. So we have,        h   i  i    =    h   i  i   2   +    ∑   i  ≠  j     h   i  j   2     ≥  0         subscript  h    i  i       superscript   subscript  h    i  i    2     subscript     i  j     superscript   subscript  h    i  j    2          0     h_{ii}=h_{ii}^{2}+\sum_{i\neq j}h_{ij}^{2}\geq 0      and        h   i  i    ≥   h   i  i   2   ⟹   h   i  i    ≤  1         subscript  h    i  i     superscript   subscript  h    i  i    2         subscript  h    i  i         1     h_{ii}\geq h_{ii}^{2}\implies h_{ii}\leq 1      Property 2  If we are in an ordinary least squares setting with fixed X and:       Y  =    X  β   +  ϵ       Y      X  β   ϵ     Y=X\beta+\epsilon          v  a  r   (  ϵ  )    =    σ  2   I         v  a  r  ϵ      superscript  σ  2   I     var(\epsilon)=\sigma^{2}I      then     v  a  r   (   e  i   )    =    (   1  -   h   i  i     )    σ  2          v  a  r   subscript  e  i        1   subscript  h    i  i      superscript  σ  2      var(e_{i})=(1-h_{ii})\sigma^{2}   where     e  i   =    Y  i   -    Y  ^   i         subscript  e  i      subscript  Y  i    subscript   normal-^  Y   i      e_{i}=Y_{i}-\hat{Y}_{i}   .  In other words, if the   ϵ   ϵ   \epsilon   are homoscedastic, leverage scores determine the noise level in the model.  Proof  First, note that    I  -  H      I  H    I-H   is idempotent and symmetric. This gives,     v  a  r   (  e  )    =   v  a  r   (    (   I  -  H   )   Y   )    =    (   I  -  H   )   v  a  r   (  Y  )     (   I  -  H   )   ′    =    σ  2     (   I  -  H   )   2    =    σ  2    (   I  -  H   )            v  a  r  e     v  a  r      I  H   Y             I  H   v  a  r  Y   superscript    I  H   normal-′            superscript  σ  2    superscript    I  H   2            superscript  σ  2     I  H       var(e)=var((I-H)Y)=(I-H)var(Y)(I-H)^{\prime}=\sigma^{2}(I-H)^{2}=\sigma^{2}(I-H)   .  So that,     v  a  r   (   e  i   )    =    (   1  -   h   i  i     )    σ  2          v  a  r   subscript  e  i        1   subscript  h    i  i      superscript  σ  2      var(e_{i})=(1-h_{ii})\sigma^{2}   .  See also   Hat matrix — whose main diagonal entries are the leverages of the observations  Mahalanobis distance — a measure of leverage of a datum  Cook's distance - a measure of changes in regression coefficients when an observation is deleted  DFFITS  Outliers — observations with extreme Y values   References  "  Category:Regression analysis  Category:Statistical terminology  Category:Regression diagnostics     Everitt, B.S. (2002) Cambridge Dictionary of Statistics. CUP. ISBN 0-521-81099-X ↩  1 ↩     