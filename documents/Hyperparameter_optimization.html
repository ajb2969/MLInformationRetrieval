<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1257">Hyperparameter optimization</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Hyperparameter optimization</h1>
<hr/>

<p>In the context of <a href="machine_learning" title="wikilink">machine learning</a>, <strong>hyperparameter optimization</strong> or <strong><a href="model_selection" title="wikilink">model selection</a></strong> is the problem of choosing a set of hyperparameters for a learning algorithm, usually with the goal of optimizing a measure of the algorithm's performance on an independent data set. Often <a href="Cross-validation_(statistics)" title="wikilink">cross-validation</a> is used to estimate this <strong>generalization</strong> performance.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Hyperparameter optimization contrasts with actual learning problems, which are also often cast as optimization problems, but optimize a <a href="loss_function" title="wikilink">loss function</a> on the training set alone. In effect, learning algorithms learn parameters that model/reconstruct their inputs well, while hyperparameter optimization is to ensure the model does not <a href="overfitting" title="wikilink">overfit</a> its data by tuning, e.g., <a href="Regularization_(mathematics)" title="wikilink">regularization</a>.</p>
<h2 id="algorithms-for-hyperparameter-optimization">Algorithms for hyperparameter optimization</h2>
<h3 id="grid-search">Grid search</h3>

<p>The traditional way of performing hyperparameter optimization has been <em>grid search</em>, or a <em>parameter sweep</em>, which is simply an <a href="Brute-force_search" title="wikilink">exhaustive searching</a> through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by <a href="Cross-validation_(statistics)" title="wikilink">cross-validation</a> on the training set<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> or evaluation on a held-out validation set.</p>

<p>Since the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search.</p>

<p>For example, a typical soft-margin <a href="support_vector_machine" title="wikilink">SVM</a> <a href="statistical_classification" title="wikilink">classifier</a> equipped with an <a href="radial_basis_function_kernel" title="wikilink">RBF kernel</a> has at least two hyperparameters that need to be tuned for good performance on unseen data: a regularization constant <em>C</em> and a kernel hyperparameter γ. Both parameters are continuous, so to perform grid search, one selects a finite set of "reasonable" values for each, say</p>

<p>
<math display="block" id="Hyperparameter_optimization:0">
<semantics>
<mrow>
<mi>C</mi>
<mo>∈</mo>
<mrow>
<mo stretchy="false">{</mo>
<mn>10</mn>
<mo>,</mo>
<mn>100</mn>
<mo>,</mo>
<mn>1000</mn>
<mo stretchy="false">}</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<in></in>
<ci>C</ci>
<set>
<cn type="integer">10</cn>
<cn type="integer">100</cn>
<cn type="integer">1000</cn>
</set>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   C\in\{10,100,1000\}
  </annotation>
</semantics>
</math>
</p>

<p>
<math display="block" id="Hyperparameter_optimization:1">
<semantics>
<mrow>
<mi>γ</mi>
<mo>∈</mo>
<mrow>
<mo stretchy="false">{</mo>
<mn>0.1</mn>
<mo>,</mo>
<mn>0.2</mn>
<mo>,</mo>
<mn>0.5</mn>
<mo>,</mo>
<mn>1.0</mn>
<mo stretchy="false">}</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<in></in>
<ci>γ</ci>
<set>
<cn type="float">0.1</cn>
<cn type="float">0.2</cn>
<cn type="float">0.5</cn>
<cn type="float">1.0</cn>
</set>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \gamma\in\{0.1,0.2,0.5,1.0\}
  </annotation>
</semantics>
</math>
</p>

<p>Grid search then trains an SVM with each pair (<em>C</em>, γ) in the <a href="Cartesian_product" title="wikilink">Cartesian product</a> of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure.</p>

<p>Grid search suffers from the <a href="curse_of_dimensionality" title="wikilink">curse of dimensionality</a>, but is often <a href="embarrassingly_parallel" title="wikilink">embarrassingly parallel</a> because typically the hyperparameter settings it evaluates are independent of each other.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h3 id="bayesian-optimization">Bayesian Optimization</h3>

<p><a href="Bayesian_Optimization" title="wikilink">Bayesian Optimization</a> is a methodology for the global optimization of noisy black-box functions. Applied to hyperparameter optimization, Bayesian optimization consists of developing a statistical model of the function from hyperparameter values to the objective evaluated on a validation set. Intuitively, the methodology assumes that there is some smooth but noisy function that acts as a mapping from hyperparameters to the objective. In Bayesian optimization, one aims to gather observations in such a manner as to evaluate the machine learning model the least number of times while revealing as much information as possible about this function and, in particular, the location of the optimum. Bayesian optimization relies on assuming a very general prior over functions which when combined with observed hyperparameter values and corresponding outputs yields a distribution over functions. The methodology proceeds by iteratively picking hyperparameters to observe (experiments to run) in a manner that trades off exploration (hyperparameters for which the outcome is most uncertain) and exploitation (hyperparamters which are expected to have a good outcome). In practice, Bayesian optimization has been shown<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> to obtain better results in fewer experiments than grid search and random search, due to the ability to reason about the quality of experiments before they are run.</p>
<h3 id="random-search">Random Search</h3>

<p>Since grid searching is an exhaustive and therefore potentially expensive method, several alternatives have been proposed. In particular, a randomized search that simply samples parameter settings a fixed number of times has been found to be more effective in high-dimensional spaces than exhaustive search.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>
<h3 id="gradient-based-optimization">Gradient Based Optimization</h3>

<p>For specific learning algorithms, specialized model selection algorithms can be used. E.g., Chapelle <em>et al.</em> present a <a href="gradient_descent" title="wikilink">gradient descent</a> algorithm for minimizing the estimated generalization error of a <a href="support_vector_machine" title="wikilink">support vector machine</a>.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<h2 id="software">Software</h2>
<ul>
<li><a class="uri" href="LIBSVM" title="wikilink">LIBSVM</a> comes with scripts for performing grid search.</li>
<li>The machine learning toolkit <a class="uri" href="scikit-learn" title="wikilink">scikit-learn</a> includes <a href="http://scikit-learn.sourceforge.net/modules/grid_search.html">grid</a> and <a href="http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html">random</a> search modules.</li>
<li><a href="http://jaberg.github.com/hyperopt/">Hyperopt</a> is a distributed hyperparameter optimization library in Python.</li>
<li><a href="http://www.cs.ubc.ca/labs/beta/Projects/autoweka/">Auto-WEKA</a> is a hyperparameter optimization layer on top of <a href="Weka_(machine_learning)" title="wikilink">WEKA</a>.</li>
<li><a href="https://github.com/HIPS/Spearmint">spearmint</a> Spearmint is a package to perform Bayesian optimization of machine learning algorithms.</li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Bias-variance_dilemma" title="wikilink">Bias-variance dilemma</a></li>
<li><a class="uri" href="Self-tuning" title="wikilink">Self-tuning</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a> <a href="Category:Mathematical_optimization" title="wikilink">Category:Mathematical optimization</a> <a href="Category:Model_selection" title="wikilink">Category:Model selection</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"></li>
<li id="fn2">Chin-Wei Hsu, Chih-Chung Chang and Chih-Jen Lin (2010). <a href="http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf">A practical guide to support vector classification</a>. Technical Report, <a href="National_Taiwan_University" title="wikilink">National Taiwan University</a>.<a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
</ol>
</section>
</body>
</html>
