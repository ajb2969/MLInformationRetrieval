   Generalized method of moments      Generalized method of moments   In econometrics , the generalized method of moments ( GMM ) is a generic method for estimating parameters in statistical models . Usually it is applied in the context of semiparametric models , where the parameter of interest is finite-dimensional, whereas the full shape of the distribution function of the data may not be known, and therefore maximum likelihood estimation is not applicable.  The method requires that a certain number of moment conditions were specified for the model. These moment conditions are functions of the model parameters and the data, such that their expectation is zero at the true values of the parameters. The GMM method then minimizes a certain norm of the sample averages of the moment conditions.  The GMM estimators are known to be consistent , asymptotically normal , and efficient in the class of all estimators that don‚Äôt use any extra information aside from that contained in the moment conditions.  GMM was developed by Lars Peter Hansen in 1982 as a generalization of the method of moments which was introduced by Karl Pearson in 1894. Hansen shared the 2013 Nobel Prize in Economics in part for this work.  Description  Suppose the available data consists of T observations , where each observation Y t is an n -dimensional multivariate random variable . We assume that the data come from a certain statistical model , defined up to an unknown parameter . The goal of the estimation problem is to find the ‚Äútrue‚Äù value of this parameter, Œ∏ 0 , or at least a reasonably close estimate.  A general assumption of GMM is that the data Y t be generated by a weakly stationary  ergodic  stochastic process . (The case of independent and identically distributed (iid) variables Y t is a special case of this condition.)  In order to apply GMM, we need to have "moment conditions", i.e. we need to know a vector-valued function  g ( Y , Œ∏ ) such that        m   (   Œ∏  0   )    ‚â°   E   [   g   (   Y  t   ,   Œ∏  0   )    ]    =  0   ,          m   subscript  Œ∏  0     normal-E    g    subscript  Y  t    subscript  Œ∏  0           0     m(\theta_{0})\equiv\operatorname{E}[\,g(Y_{t},\theta_{0})\,]=0,   where E denotes expectation , and Y t is a generic observation. Moreover, the function m ( Œ∏ ) must differ from zero for , or otherwise the parameter Œ∏ will not be point- identified .  The basic idea behind GMM is to replace the theoretical expected value E[‚ãÖ] with its empirical analog ‚Äî sample average:        m  ^    (  Œ∏  )    ‚â°    1  T     ‚àë   t  =  1   T    g   (   Y  t   ,  Œ∏  )             normal-^  m   Œ∏       1  T     superscript   subscript     t  1    T     g    subscript  Y  t   Œ∏        \hat{m}(\theta)\equiv\frac{1}{T}\sum_{t=1}^{T}g(Y_{t},\theta)   and then to minimize the norm of this expression with respect to Œ∏ . The minimizing value of Œ∏ is our estimate for Œ∏ 0 .  By the law of large numbers ,      m  ^    (  Œ∏  )    ‚âà   E   [   g   (   Y  t   ,  Œ∏  )    ]    =   m   (  Œ∏  )             normal-^  m   Œ∏    normal-E    g    subscript  Y  t   Œ∏            m  Œ∏      \scriptstyle\hat{m}(\theta)\,\approx\;\operatorname{E}[g(Y_{t},\theta)]\,=\,m(\theta)   for large values of T , and thus we expect that      m  ^    (   Œ∏  0   )    ‚âà   m   (   Œ∏  0   )    =  0           normal-^  m    subscript  Œ∏  0      m   subscript  Œ∏  0         0     \scriptstyle\hat{m}(\theta_{0})\;\approx\;m(\theta_{0})\;=\;0   . The generalized method of moments looks for a number    Œ∏  ^     normal-^  Œ∏    \scriptstyle\hat{\theta}   which would make     m  ^    (    Œ∏  ^    )        normal-^  m    normal-^  Œ∏     \scriptstyle\hat{m}(\;\!\hat{\theta}\;\!)   as close to zero as possible. Mathematically, this is equivalent to minimizing a certain norm of     m  ^    (  Œ∏  )        normal-^  m   Œ∏    \scriptstyle\hat{m}(\theta)   (norm of m , denoted as || m ||, measures the distance between m and zero). The properties of the resulting estimator will depend on the particular choice of the norm function, and therefore the theory of GMM considers an entire family of norms, defined as         ‚à•    m  ^    (  Œ∏  )    ‚à•   W  2   =    m  ^      (  Œ∏  )   ‚Ä≤    W   m  ^    (  Œ∏  )     ,       subscript   superscript   norm     normal-^  m   Œ∏    2   W      normal-^  m    superscript  Œ∏  normal-‚Ä≤   W   normal-^  m   Œ∏     \|\hat{m}(\theta)\|^{2}_{W}=\hat{m}(\theta)^{\prime}\,W\hat{m}(\theta),   where W is a positive-definite weighting matrix, and m‚Ä≤ denotes transposition . In practice, the weighting matrix W is computed based on the available data set, which will be denoted as    W  ^     normal-^  W    \scriptstyle\hat{W}   . Thus, the GMM estimator can be written as       Œ∏  ^   =  arg   min   Œ∏  ‚àà  Œò      (   1  T    ‚àë   t  =  1   T   g   (   Y  t   ,  Œ∏  )   )   ‚Ä≤    W  ^    (   1  T    ‚àë   t  =  1   T   g   (   Y  t   ,  Œ∏  )   )      fragments   normal-^  Œ∏    arg   subscript     Œ∏  normal-Œò     superscript   fragments  normal-(    1  T    superscript   subscript     t  1    T   g   fragments  normal-(   subscript  Y  t   normal-,  Œ∏  normal-)   normal-)   normal-‚Ä≤    normal-^  W    fragments  normal-(    1  T    superscript   subscript     t  1    T   g   fragments  normal-(   subscript  Y  t   normal-,  Œ∏  normal-)   normal-)     \hat{\theta}=\operatorname{arg}\min_{\theta\in\Theta}\bigg(\frac{1}{T}\sum_{t=%
 1}^{T}g(Y_{t},\theta)\bigg)^{\prime}\hat{W}\bigg(\frac{1}{T}\sum_{t=1}^{T}g(Y_%
 {t},\theta)\bigg)     Under suitable conditions this estimator is consistent , asymptotically normal , and with right choice of weighting matrix    W  ^     normal-^  W    \scriptstyle\hat{W}   also asymptotically efficient .  Properties  Consistency  Consistency is a statistical property of an estimator stating that, having a sufficient number of observations, the estimator will get arbitrarily close to the true value of parameter:       Œ∏  ^    ‚Üí  ùëù      Œ∏  0     as   T   ‚Üí  ‚àû        p  normal-‚Üí    normal-^  Œ∏      subscript  Œ∏  0   as  T     normal-‚Üí        \hat{\theta}\xrightarrow{p}\theta_{0}\ \text{as}\ T\to\infty   (see Convergence in probability ). Necessary and sufficient conditions for a GMM estimator to be consistent are as follows:          W  ^   T    ‚Üí  ùëù   W   ,      p  normal-‚Üí    subscript   normal-^  W   T   W    \hat{W}_{T}\xrightarrow{p}W,   where W is a positive semi-definite matrix ,        W    E   [   g   (   Y  t   ,  Œ∏  )    ]     =  0        W   normal-E    g    subscript  Y  t   Œ∏      0    \,W\operatorname{E}[\,g(Y_{t},\theta)\,]=0   only for      Œ∏   =   Œ∏  0    ,      Œ∏   subscript  Œ∏  0     \,\theta=\theta_{0},     The set of possible parameters    Œò  ‚äÇ   ‚Ñù  k       normal-Œò   superscript  ‚Ñù  k     \Theta\subset\mathbb{R}^{k}   is compact ,       g    (  Y  ,  Œ∏  )       g   Y  Œ∏     \,g(Y,\theta)   is continuous at each Œ∏ with probability one,        E   [    sup   Œ∏  ‚àà  Œò     ‚à•   g   (  Y  ,  Œ∏  )    ‚à•    ]    <  ‚àû   .       normal-E    subscript  supremum    Œ∏  normal-Œò     norm    g   Y  Œ∏          \operatorname{E}[\,\textstyle\sup_{\theta\in\Theta}\lVert g(Y,\theta)\rVert\,]%
 <\infty.      The second condition here (so-called Global identification condition) is often particularly hard to verify. There exist simpler necessary but not sufficient conditions, which may be used to detect non-identification problem:   Order condition . The dimension of moment function m(Œ∏) should be at least as large as the dimension of parameter vector Œ∏ .  Local identification . If g(Y,Œ∏) is continuously differentiable in a neighborhood of    Œ∏  0     subscript  Œ∏  0    \theta_{0}   , then matrix    W   E   [     ‚àá  Œ∏   g    (   Y  t   ,   Œ∏  0   )    ]        W   normal-E      subscript  normal-‚àá  Œ∏   g     subscript  Y  t    subscript  Œ∏  0        W\operatorname{E}[\nabla_{\theta}g(Y_{t},\theta_{0})]   must have full column rank .   In practice applied econometricians often simply assume that global identification holds, without actually proving it. 1  Asymptotic normality  Asymptotic normality is a useful property, as it allows us to construct confidence bands for the estimator, and conduct different tests. Before we can make a statement about the asymptotic distribution of the GMM estimator, we need to define two auxiliary matrices:       G  =   E   [      ‚àá   Œ∏     g    (   Y  t   ,   Œ∏  0   )    ]     ,   Œ©  =   E   [   g   (   Y  t   ,   Œ∏  0   )   g     (   Y  t   ,   Œ∏  0   )   ‚Ä≤     ]        formulae-sequence    G   normal-E      subscript  normal-‚àá  Œ∏   g     subscript  Y  t    subscript  Œ∏  0         normal-Œ©   normal-E    g    subscript  Y  t    subscript  Œ∏  0    g   superscript    subscript  Y  t    subscript  Œ∏  0    normal-‚Ä≤        G=\operatorname{E}[\,\nabla_{\!\theta}\,g(Y_{t},\theta_{0})\,],\qquad\Omega=%
 \operatorname{E}[\,g(Y_{t},\theta_{0})g(Y_{t},\theta_{0})^{\prime}\,]   Then under conditions 1‚Äì6 listed below, the GMM estimator will be asymptotically normal with limiting distribution        T    (    Œ∏  ^   -   Œ∏  0    )      ‚Üí  ùëë     ùí©   [  0  ,     (    G  ‚Ä≤   W  G   )    -  1     G  ‚Ä≤   W  Œ©   W  ‚Ä≤   G    (    G  ‚Ä≤    W  ‚Ä≤   G   )    -  1     ]        d  normal-‚Üí       T      normal-^  Œ∏    subscript  Œ∏  0       ùí©   0     superscript     superscript  G  normal-‚Ä≤   W  G     1     superscript  G  normal-‚Ä≤   W  normal-Œ©   superscript  W  normal-‚Ä≤   G   superscript     superscript  G  normal-‚Ä≤    superscript  W  normal-‚Ä≤   G     1         \sqrt{T}\big(\hat{\theta}-\theta_{0}\big)\ \xrightarrow{d}\ \mathcal{N}\big[0,%
 (G^{\prime}WG)^{-1}G^{\prime}W\Omega W^{\prime}G(G^{\prime}W^{\prime}G)^{-1}\big]   (see Convergence in distribution ). Conditions:       Œ∏  ^     normal-^  Œ∏    \hat{\theta}   is consistent (see previous section),  The set of possible parameters    Œò  ‚äÇ   ‚Ñù  k       normal-Œò   superscript  ‚Ñù  k     \Theta\subset\mathbb{R}^{k}   is compact ,       g    (  Y  ,  Œ∏  )       g   Y  Œ∏     \,g(Y,\theta)   is continuously differentiable in some neighborhood N of    Œ∏  0     subscript  Œ∏  0    \theta_{0}   with probability one,        E   [     ‚à•   g   (   Y  t   ,  Œ∏  )    ‚à•   2    ]    <  ‚àû   ,       normal-E   superscript   norm    g    subscript  Y  t   Œ∏     2       \operatorname{E}[\,\lVert g(Y_{t},\theta)\rVert^{2}\,]<\infty,           E   [    sup   Œ∏  ‚àà  N     ‚à•     ‚àá  Œ∏   g    (   Y  t   ,  Œ∏  )    ‚à•    ]    <  ‚àû   ,       normal-E    subscript  supremum    Œ∏  N     norm      subscript  normal-‚àá  Œ∏   g     subscript  Y  t   Œ∏          \operatorname{E}[\,\textstyle\sup_{\theta\in N}\lVert\nabla_{\theta}g(Y_{t},%
 \theta)\rVert\,]<\infty,     the matrix     G  ‚Ä≤   W  G       superscript  G  normal-‚Ä≤   W  G    G^{\prime}WG   is nonsingular.   Efficiency  So far we have said nothing about the choice of matrix W , except that it must be positive semi-definite. In fact any such matrix will produce a consistent and asymptotically normal GMM estimator, the only difference will be in the asymptotic variance of that estimator. It can be shown that taking      W  ‚àù   Œ©   -  1       proportional-to  W   superscript  normal-Œ©    1      W\propto\ \Omega^{-1}   will result in the most efficient estimator in the class of all asymptotically normal estimators. Efficiency in this case means that such an estimator will have the smallest possible variance (we say that matrix A is smaller than matrix B if B‚ÄìA is positive semi-definite).  In this case the formula for the asymptotic distribution of the GMM estimator simplifies to        T    (    Œ∏  ^   -   Œ∏  0    )      ‚Üí  ùëë     ùí©   [  0  ,    (     G  ‚Ä≤     Œ©   -  1    G   )    -  1    ]        d  normal-‚Üí       T      normal-^  Œ∏    subscript  Œ∏  0       ùí©   0   superscript     superscript  G  normal-‚Ä≤    superscript  normal-Œ©    1    G     1        \sqrt{T}\big(\hat{\theta}-\theta_{0}\big)\ \xrightarrow{d}\ \mathcal{N}\big[0,%
 (G^{\prime}\,\Omega^{-1}G)^{-1}\big]     The proof that such a choice of weighting matrix is indeed optimal is often adopted with slight modifications when establishing efficiency of other estimators. As a rule of thumb, a weighting matrix is optimal whenever it makes the ‚Äúsandwich formula‚Äù for variance collapse into a simpler expression.      Proof . We will consider the difference between asymptotic variance with arbitrary W and asymptotic variance with    W  =   Œ©   -  1        W   superscript  normal-Œ©    1      W=\Omega^{-1}   . If we can factor this difference into a symmetric product of the form CC ' for some matrix C , then it will guarantee that this difference is nonnegative-definite, and thus    W  =   Œ©   -  1        W   superscript  normal-Œ©    1      W=\Omega^{-1}   will be optimal by definition.           V    (  W  )    -   V   (   Œ©   -  1    )          V  W     V   superscript  normal-Œ©    1       \,V(W)-V(\Omega^{-1})                 where we introduced matrices A and B in order to slightly simplify notation; I is an identity matrix . We can see that matrix B here is symmetric and idempotent      B  2   =  B       superscript  B  2   B    B^{2}=B   . This means I‚ÄìB is symmetric and idempotent as well     I  -  B   =    (   I  -  B   )     (   I  -  B   )   ‚Ä≤          I  B       I  B    superscript    I  B   normal-‚Ä≤      I-B=(I-B)(I-B)^{\prime}   . Thus we can continue to factor the previous expression as        Implementation  One difficulty with implementing the outlined method is that we cannot take Œ© ‚àí1 }} because, by the definition of matrix Œ©, we need to know the value of Œ∏ 0 in order to compute this matrix, and Œ∏ 0 is precisely the quantity we don‚Äôt know and are trying to estimate in the first place.  Several approaches exist to deal with this issue, the first one being the most popular:  Another important issue in implementation of minimization procedure is that the function is supposed to search through (possibly high-dimensional) parameter space Œò and find the value of Œ∏ which minimizes the objective function. No generic recommendation for such procedure exists, it is a subject of its own field, numerical optimization .  J-test  When the number of moment conditions is greater than the dimension of the parameter vector Œ∏ , the model is said to be over-identified . Over-identification allows us to check whether the model's moment conditions match the data well or not.  Conceptually we can check whether     m  ^    (   Œ∏  ^   )        normal-^  m    normal-^  Œ∏     \hat{m}(\hat{\theta})   is sufficiently close to zero to suggest that the model fits the data well. The GMM method has then replaced the problem of solving the equation      m  ^    (  Œ∏  )    =  0         normal-^  m   Œ∏   0    \hat{m}(\theta)=0   , which chooses   Œ∏   Œ∏   \theta   to match the restrictions exactly, by a minimization calculation. The minimization can always be conducted even when no    Œ∏  0     subscript  Œ∏  0    \theta_{0}   exists such that     m   (   Œ∏  0   )    =  0        m   subscript  Œ∏  0    0    m(\theta_{0})=0   . This is what J-test does. The J-test is also called a test for over-identifying restrictions .  Formally we consider two hypotheses :        H  0   :    m   (   Œ∏  0   )    =  0      normal-:   subscript  H  0       m   subscript  Œ∏  0    0     H_{0}:\ m(\theta_{0})=0   (the null hypothesis that the model is ‚Äúvalid‚Äù), and       H  1   :     m   (  Œ∏  )    ‚â†  0   ,    ‚àÄ  Œ∏   ‚àà  Œò       normal-:   subscript  H  1    formulae-sequence      m  Œ∏   0      for-all  Œ∏   normal-Œò      H_{1}:\ m(\theta)\neq 0,\ \forall\theta\in\Theta   (the alternative hypothesis that model is ‚Äúinvalid‚Äù; the data do not come close to meeting the restrictions)   Under hypothesis    H  0     subscript  H  0    H_{0}   , the following so-called J-statistic is asymptotically chi-squared with k‚Äìl degrees of freedom. Define J to be:      J  ‚â°    T  ‚ãÖ    (    1  T     ‚àë   t  =  1   T    g   (   Y  t   ,   Œ∏  ^   )      )   ‚Ä≤      W  ^   T    (    1  T     ‚àë   t  =  1   T    g   (   Y  t   ,   Œ∏  ^   )      )      ‚Üí  ùëë     œá   k  -  ‚Ñì   2         J     normal-‚ãÖ  T   superscript      1  T     superscript   subscript     t  1    T     g    subscript  Y  t    normal-^  Œ∏       normal-‚Ä≤     subscript   normal-^  W   T       1  T     superscript   subscript     t  1    T     g    subscript  Y  t    normal-^  Œ∏           d  normal-‚Üí      subscript   superscript  œá  2     k  normal-‚Ñì       J\equiv T\cdot\bigg(\frac{1}{T}\sum_{t=1}^{T}g(Y_{t},\hat{\theta})\bigg)^{%
 \prime}\hat{W}_{T}\bigg(\frac{1}{T}\sum_{t=1}^{T}g(Y_{t},\hat{\theta})\bigg)\ %
 \xrightarrow{d}\ \chi^{2}_{k-\ell}   under     H  0   ,     subscript  H  0    H_{0},     where    Œ∏  ^     normal-^  Œ∏    \hat{\theta}   is the GMM estimator of the parameter    Œ∏  0     subscript  Œ∏  0    \theta_{0}   , k is the number of moment conditions (dimension of vector g ), and l is the number of estimated parameters (dimension of vector Œ∏ ). Matrix     W  ^   T     subscript   normal-^  W   T    \hat{W}_{T}   must converge in probability to    Œ©   -  1      superscript  normal-Œ©    1     \Omega^{-1}   , the efficient weighting matrix (note that previously we only required that W be proportional to    Œ©   -  1      superscript  normal-Œ©    1     \Omega^{-1}   for estimator to be efficient; however in order to conduct the J-test W must be exactly equal to    Œ©   -  1      superscript  normal-Œ©    1     \Omega^{-1}   , not simply proportional).  Under the alternative hypothesis    H  1     subscript  H  1    H_{1}   , the J-statistic is asymptotically unbounded:       J     ‚Üí  ùëù    ‚àû      p  normal-‚Üí   J     J\ \xrightarrow{p}\ \infty   under    H  1     subscript  H  1    H_{1}     To conduct the test we compute the value of J from the data. It is a nonnegative number. We compare it with (for example) the 0.95 quantile of the    œá   k  -  ‚Ñì   2     subscript   superscript  œá  2     k  normal-‚Ñì     \chi^{2}_{k-\ell}   distribution:       H  0     subscript  H  0    H_{0}   is rejected at 95% confidence level if    J  >   q  0.95   œá   k  -  ‚Ñì   2        J   superscript   subscript  q  0.95    subscript   superscript  œá  2     k  normal-‚Ñì       J>q_{0.95}^{\chi^{2}_{k-\ell}}         H  0     subscript  H  0    H_{0}   cannot be rejected at 95% confidence level if    J  <   q  0.95   œá   k  -  ‚Ñì   2        J   superscript   subscript  q  0.95    subscript   superscript  œá  2     k  normal-‚Ñì       J      Scope  Many other popular estimation techniques can be cast in terms of GMM optimization:  Implementations   R Programming wikibook, Method of Moments  R  Stata  EViews  SAS   See also   Method of maximum likelihood  Generalized empirical likelihood   References   Kirby Faciane (2006): Statistics for Empirical and Quantitative Finance . H.C. Baird: Philadelphia. ISBN 0-9788208-9-4.  Alastair R. Hall (2005). Generalized Method of Moments (Advanced Texts in Econometrics) . Oxford University Press. ISBN 0-19-877520-2.   Lars Peter Hansen (2002): Method of Moments in International Encyclopedia of the Social and Behavior Sciences, N. J. Smelser and P. B. Bates (editors), Pergamon: Oxford.    Newey W., McFadden D. (1994). Large sample estimation and hypothesis testing , in Handbook of Econometrics, Ch.36. Elsevier Science.  Special issues of Journal of Business and Economic Statistics: vol. 14, no. 3 and vol. 20, no. 4 .   ru:–û–±–æ–±—â–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –º–æ–º–µ–Ω—Ç–æ–≤ "  Category:Estimation theory  Category:Econometrics     Newey, McFadden (1994), p.2127 ‚Ü©     