   Minimax estimator      Minimax estimator   In statistical decision theory , where we are faced with the problem of estimating a deterministic parameter (vector)    Î¸  âˆˆ  Î˜      Î¸  normal-Î˜    \theta\in\Theta   from observations     x  âˆˆ  ð’³   ,      x  ð’³    x\in\mathcal{X},   an estimator (estimation rule)    Î´  M     superscript  Î´  M    \delta^{M}\,\!   is called minimax if its maximal risk is minimal among all estimators of   Î¸   Î¸   \theta\,\!   . In a sense this means that    Î´  M     superscript  Î´  M    \delta^{M}\,\!   is an estimator which performs best in the worst possible case allowed in the problem.  Problem setup  Consider the problem of estimating a deterministic (not Bayesian ) parameter    Î¸  âˆˆ  Î˜      Î¸  normal-Î˜    \theta\in\Theta   from noisy or corrupt data    x  âˆˆ  ð’³      x  ð’³    x\in\mathcal{X}   related through the conditional probability distribution     P   (  x  |  Î¸  )      fragments  P   fragments  normal-(  x  normal-|  Î¸  normal-)     P(x|\theta)\,\!   . Our goal is to find a "good" estimator    Î´   (  x  )       Î´  x    \delta(x)\,\!   for estimating the parameter   Î¸   Î¸   \theta\,\!   , which minimizes some given risk function     R   (  Î¸  ,  Î´  )       R   Î¸  Î´     R(\theta,\delta)\,\!   . Here the risk function is the expectation of some loss function     L   (  Î¸  ,  Î´  )       L   Î¸  Î´     L(\theta,\delta)\,\!   with respect to    P   (  x  |  Î¸  )      fragments  P   fragments  normal-(  x  normal-|  Î¸  normal-)     P(x|\theta)\,\!   . A popular example for a loss function is the squared error loss     L   (  Î¸  ,  Î´  )    =    âˆ¥   Î¸  -  Î´   âˆ¥   2         L   Î¸  Î´     superscript   norm    Î¸  Î´    2     L(\theta,\delta)=\|\theta-\delta\|^{2}\,\!   , and the risk function for this loss is the mean squared error (MSE).  Unfortunately in general the risk cannot be minimized, since it depends on the unknown parameter   Î¸   Î¸   \theta\,\!   itself (If we knew what was the actual value of   Î¸   Î¸   \theta\,\!   , we wouldn't need to estimate it). Therefore additional criteria for finding an optimal estimator in some sense are required. One such criterion is the minimax criteria.  Definition  Definition : An estimator     Î´  M   :   ð’³  â†’  Î˜      normal-:   superscript  Î´  M    normal-â†’  ð’³  normal-Î˜     \delta^{M}:\mathcal{X}\rightarrow\Theta\,\!   is called minimax with respect to a risk function    R   (  Î¸  ,  Î´  )       R   Î¸  Î´     R(\theta,\delta)\,\!   if it achieves the smallest maximum risk among all estimators, meaning it satisfies         sup   Î¸  âˆˆ  Î˜     R   (  Î¸  ,   Î´  M   )     =    inf  Î´     sup   Î¸  âˆˆ  Î˜     R   (  Î¸  ,  Î´  )       .        subscript  supremum    Î¸  normal-Î˜      R   Î¸   superscript  Î´  M        subscript  infimum  Î´     subscript  supremum    Î¸  normal-Î˜      R   Î¸  Î´        \sup_{\theta\in\Theta}R(\theta,\delta^{M})=\inf_{\delta}\sup_{\theta\in\Theta}%
 R(\theta,\delta).\,     Least favorable distribution  Logically, an estimator is minimax when it is the best in the worst case. Continuing this logic, a minimax estimator should be a Bayes estimator with respect to a prior least favorable distribution of   Î¸   Î¸   \theta\,\!   . To demonstrate this notion denote the average risk of the Bayes estimator    Î´  Ï€     subscript  Î´  Ï€    \delta_{\pi}\,\!   with respect to a prior distribution   Ï€   Ï€   \pi\,\!   as       r  Ï€   =   âˆ«   R   (  Î¸  ,   Î´  Ï€   )   d  Ï€   (  Î¸  )          subscript  r  Ï€       R   Î¸   subscript  Î´  Ï€    d  Ï€  Î¸      r_{\pi}=\int R(\theta,\delta_{\pi})\,d\pi(\theta)\,     Definition: A prior distribution   Ï€   Ï€   \pi\,\!   is called least favorable if for any other distribution    Ï€  â€²     superscript  Ï€  normal-â€²    \pi^{\prime}\,\!   the average risk satisfies     r  Ï€   â‰¥    r   Ï€  â€²          subscript  r  Ï€    subscript  r   superscript  Ï€  normal-â€²      r_{\pi}\geq r_{\pi^{\prime}}\,   .  Theorem 1: If      r  Ï€   =    sup  Î¸    R   (  Î¸  ,   Î´  Ï€   )      ,       subscript  r  Ï€     subscript  supremum  Î¸     R   Î¸   subscript  Î´  Ï€        r_{\pi}=\sup_{\theta}R(\theta,\delta_{\pi}),\,   then:       Î´  Ï€     subscript  Î´  Ï€    \delta_{\pi}\,\!   is minimax.  If    Î´  Ï€     subscript  Î´  Ï€    \delta_{\pi}\,\!   is a unique Bayes estimator, it is also the unique minimax estimator.     Ï€   Ï€   \pi\,\!   is least favorable.   Corollary: If a Bayes estimator has constant risk, it is minimax. Note that this is not a necessary condition.  Example 1, Unfair coin: Consider the problem of estimating the "success" rate of a Binomial variable,    x  âˆ¼   B   (  n  ,  Î¸  )       similar-to  x    B   n  Î¸      x\sim B(n,\theta)\,\!   . This may be viewed as estimating the rate at which an unfair coin falls on "heads" or "tails". In this case the Bayes estimator with respect to a Beta -distributed prior,    Î¸  âˆ¼   Beta   (    n   /  2   ,    n   /  2   )       similar-to  Î¸    Beta       n   2       n   2       \theta\sim\text{Beta}(\sqrt{n}/2,\sqrt{n}/2)\,   is        Î´  M   =    x  +   0.5   n      n  +   n      ,       superscript  Î´  M       x    0.5    n       n    n       \delta^{M}=\frac{x+0.5\sqrt{n}}{n+\sqrt{n}},\,     with constant Bayes risk      r  =    1   4    (   1  +   n    )   2          r    1    4   superscript    1    n    2       r=\frac{1}{4(1+\sqrt{n})^{2}}\,     and, according to the Corollary, is minimax.  Definition: A sequence of prior distributions    Ï€  n     subscript  Ï€  n    {\pi}_{n}\,\!   is called least favorable if for any other distribution    Ï€  â€²     superscript  Ï€  normal-â€²    \pi^{\prime}\,\!   ,         lim   n  â†’  âˆž     r   Ï€  n     â‰¥   r   Ï€  â€²     .        subscript    normal-â†’  n      subscript  r   subscript  Ï€  n      subscript  r   superscript  Ï€  normal-â€²      \lim_{n\rightarrow\infty}r_{\pi_{n}}\geq r_{\pi^{\prime}}.\,     Theorem 2: If there are a sequence of priors    Ï€  n     subscript  Ï€  n    \pi_{n}\,\!   and an estimator   Î´   Î´   \delta\,\!   such that      sup  Î¸    R   (  Î¸  ,  Î´  )     =    lim   n  â†’  âˆž     r   Ï€  n           subscript  supremum  Î¸     R   Î¸  Î´       subscript    normal-â†’  n      subscript  r   subscript  Ï€  n       \sup_{\theta}R(\theta,\delta)=\lim_{n\rightarrow\infty}r_{\pi_{n}}\,\!   , then :      Î´   Î´   \delta\,\!   is minimax.  The sequence    Ï€  n     subscript  Ï€  n    {\pi}_{n}\,\!   is least favorable.   Notice that no uniqueness is guaranteed here. For example, the ML estimator from the previous example may be attained as the limit of Bayes estimators with respect to a uniform prior,     Ï€  n   âˆ¼   U   [   -  n   ,  n  ]       similar-to   subscript  Ï€  n     U     n   n      \pi_{n}\sim U[-n,n]\,\!   with increasing support and also with respect to a zero mean normal prior     Ï€  n   âˆ¼   N   (  0  ,   n   Ïƒ  2    )       similar-to   subscript  Ï€  n     N   0    n   superscript  Ïƒ  2        \pi_{n}\sim N(0,n\sigma^{2})\,\!   with increasing variance. So neither the resulting ML estimator is unique minimax nor the least favorable prior is unique.  Example 2: Consider the problem of estimating the mean of   p   p   p\,\!   dimensional Gaussian random vector,    x  âˆ¼   N   (  Î¸  ,    I  p    Ïƒ  2    )       similar-to  x    N   Î¸     subscript  I  p    superscript  Ïƒ  2        x\sim N(\theta,I_{p}\sigma^{2})\,\!   . The Maximum likelihood (ML) estimator for   Î¸   Î¸   \theta\,\!   in this case is simply     Î´   M  L    =  x       subscript  Î´    M  L    x    \delta_{ML}=x\,\!   , and its risk is        R   (  Î¸  ,   Î´   M  L    )    =   E    âˆ¥    Î´   M  L    -  Î¸   âˆ¥   2    =    âˆ‘  1  p    E    (    x  i   -   Î¸  i    )   2     =   p   Ïƒ  2     .          R   Î¸   subscript  Î´    M  L        E   superscript   norm     subscript  Î´    M  L    Î¸    2           superscript   subscript   1   p     E   superscript     subscript  x  i    subscript  Î¸  i    2            p   superscript  Ïƒ  2       R(\theta,\delta_{ML})=E{\|\delta_{ML}-\theta\|^{2}}=\sum\limits_{1}^{p}E{(x_{i%
 }-\theta_{i})^{2}}=p\sigma^{2}.\,     (Figure)  MSE of maximum likelihood estimator versus Jamesâ€“Stein estimator   The risk is constant, but the ML estimator is actually not a Bayes estimator, so the Corollary of Theorem 1 does not apply. However, the ML estimator is the limit of the Bayes estimators with respect to the prior sequence     Ï€  n   âˆ¼   N   (  0  ,   n   Ïƒ  2    )       similar-to   subscript  Ï€  n     N   0    n   superscript  Ïƒ  2        \pi_{n}\sim N(0,n\sigma^{2})\,\!   , and, hence, indeed minimax according to Theorem 2 . Nonetheless, minimaxity does not always imply admissibility . In fact in this example, the ML estimator is known to be inadmissible (not admissible) whenever    p  >  2      p  2    p>2\,\!   . The famous Jamesâ€“Stein estimator dominates the ML whenever    p  >  2      p  2    p>2\,\!   . Though both estimators have the same risk    p   Ïƒ  2       p   superscript  Ïƒ  2     p\sigma^{2}\,\!   when     âˆ¥  Î¸  âˆ¥   â†’  âˆž     normal-â†’   norm  Î¸      \|\theta\|\rightarrow\infty\,\!   , and they are both minimax, the Jamesâ€“Stein estimator has smaller risk for any finite    âˆ¥  Î¸  âˆ¥     norm  Î¸    \|\theta\|\,\!   . This fact is illustrated in the following figure.  Some examples  In general it is difficult, often even impossible to determine the minimax estimator. Nonetheless, in many cases a minimax estimator has been determined.  Example 3, Bounded Normal Mean: When estimating the Mean of a Normal Vector    x  âˆ¼   N   (  Î¸  ,    I  n    Ïƒ  2    )       similar-to  x    N   Î¸     subscript  I  n    superscript  Ïƒ  2        x\sim N(\theta,I_{n}\sigma^{2})\,\!   , where it is known that      âˆ¥  Î¸  âˆ¥   2   â‰¤  M       superscript   norm  Î¸   2   M    \|\theta\|^{2}\leq M\,\!   . The Bayes estimator with respect to a prior which is uniformly distributed on the edge of the bounding sphere is known to be minimax whenever    M  â‰¤  n      M  n    M\leq n\,\!   . The analytical expression for this estimator is        Î´  M   =    n   J   n  +  1     (   n   âˆ¥  x  âˆ¥    )      âˆ¥  x  âˆ¥    J  n    (   n   âˆ¥  x  âˆ¥    )      ,       superscript  Î´  M       n   subscript  J    n  1      n   norm  x        norm  x    subscript  J  n     n   norm  x        \delta^{M}=\frac{nJ_{n+1}(n\|x\|)}{\|x\|J_{n}(n\|x\|)},\,     where     J  n    (  t  )        subscript  J  n   t    J_{n}(t)\,\!   , is the modified Bessel function of the first kind of order n .  Asymptotic minimax estimator  The difficulty of determining the exact minimax estimator has motivated the study of estimators of asymptotic minimax --- an estimator    Î´  â€²     superscript  Î´  normal-â€²    \delta^{\prime}   is called   c   c   c   -asymptotic (or approximate) minimax if         sup   Î¸  âˆˆ  Î˜     R   (  Î¸  ,   Î´  â€²   )     â‰¤   c    inf  Î´     sup   Î¸  âˆˆ  Î˜     R   (  Î¸  ,  Î´  )        .        subscript  supremum    Î¸  normal-Î˜      R   Î¸   superscript  Î´  normal-â€²        c    subscript  infimum  Î´     subscript  supremum    Î¸  normal-Î˜      R   Î¸  Î´         \sup_{\theta\in\Theta}R(\theta,\delta^{\prime})\leq c\inf_{\delta}\sup_{\theta%
 \in\Theta}R(\theta,\delta).     For many estimation problems, especially in the non-parametric estimation setting, various approximate minimax estimators have been established. The design of approximate minimax estimator is intimately related to the geometry, such as the metric entropy number , of   Î˜   normal-Î˜   \Theta   .  Relationship to Robust Optimization  Robust optimization is an approach to solve optimization problems under uncertainty in the knowledge of underlying parameters,. 1 2 For instance, the MMSE Bayesian estimation of a parameter requires the knowledge of parameter correlation function. If the knowledge of this correlation function is not perfectly available, a popular minimax robust optimization approach 3 is to define a set characterizing the uncertainty about the correlation function, and then pursuing a minimax optimization over the uncertainty set and the estimator respectively. Similar minimax optimizations can be pursued to make estimators robust to certain imprecisely known parameters. For instance, a recent study dealing with such techniques in the area of signal processing can be found in. 4  In R. Fandom Noubiap and W. Seidel (2001) an algorithm for calculating a Gamma-minimax decision rule has been developed, when Gamma is given by a finite number of generalized moment conditions. Such a decision rule minimizes the maximum of the integrals of the risk function with respect to all distributions in Gamma. Gamma-minimax decision rules are of interest in robustness studies in Bayesian statistics.  References   E. L. Lehmann and G. Casella (1998), Theory of Point Estimation, 2nd ed. New York: Springer-Verlag.  F. Perron and E. Marchand (2002), "On the minimax estimator of a bounded normal mean," Statistics and Probability Letters  58 : 327â€“333.  J. O. Berger (1985), Statistical Decision Theory and Bayesian Analysis, 2nd ed. New York: Springer-Verlag. ISBN 0-387-96098-8.  R. Fandom Noubiap and W. Seidel (2001), An Algorithm for Calculating Gamma-Minimax Decision Rules under Generalized Moment Conditions, Annals of Statistics, Aug., 2001, vol. 29, no. 4, p.Â 1094â€“1116    "  Category:Decision theory  Category:Estimation theory            