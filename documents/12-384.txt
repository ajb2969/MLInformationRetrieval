   Minimax estimator      Minimax estimator   In statistical decision theory , where we are faced with the problem of estimating a deterministic parameter (vector)    θ  ∈  Θ      θ  normal-Θ    \theta\in\Theta   from observations     x  ∈  𝒳   ,      x  𝒳    x\in\mathcal{X},   an estimator (estimation rule)    δ  M     superscript  δ  M    \delta^{M}\,\!   is called minimax if its maximal risk is minimal among all estimators of   θ   θ   \theta\,\!   . In a sense this means that    δ  M     superscript  δ  M    \delta^{M}\,\!   is an estimator which performs best in the worst possible case allowed in the problem.  Problem setup  Consider the problem of estimating a deterministic (not Bayesian ) parameter    θ  ∈  Θ      θ  normal-Θ    \theta\in\Theta   from noisy or corrupt data    x  ∈  𝒳      x  𝒳    x\in\mathcal{X}   related through the conditional probability distribution     P   (  x  |  θ  )      fragments  P   fragments  normal-(  x  normal-|  θ  normal-)     P(x|\theta)\,\!   . Our goal is to find a "good" estimator    δ   (  x  )       δ  x    \delta(x)\,\!   for estimating the parameter   θ   θ   \theta\,\!   , which minimizes some given risk function     R   (  θ  ,  δ  )       R   θ  δ     R(\theta,\delta)\,\!   . Here the risk function is the expectation of some loss function     L   (  θ  ,  δ  )       L   θ  δ     L(\theta,\delta)\,\!   with respect to    P   (  x  |  θ  )      fragments  P   fragments  normal-(  x  normal-|  θ  normal-)     P(x|\theta)\,\!   . A popular example for a loss function is the squared error loss     L   (  θ  ,  δ  )    =    ∥   θ  -  δ   ∥   2         L   θ  δ     superscript   norm    θ  δ    2     L(\theta,\delta)=\|\theta-\delta\|^{2}\,\!   , and the risk function for this loss is the mean squared error (MSE).  Unfortunately in general the risk cannot be minimized, since it depends on the unknown parameter   θ   θ   \theta\,\!   itself (If we knew what was the actual value of   θ   θ   \theta\,\!   , we wouldn't need to estimate it). Therefore additional criteria for finding an optimal estimator in some sense are required. One such criterion is the minimax criteria.  Definition  Definition : An estimator     δ  M   :   𝒳  →  Θ      normal-:   superscript  δ  M    normal-→  𝒳  normal-Θ     \delta^{M}:\mathcal{X}\rightarrow\Theta\,\!   is called minimax with respect to a risk function    R   (  θ  ,  δ  )       R   θ  δ     R(\theta,\delta)\,\!   if it achieves the smallest maximum risk among all estimators, meaning it satisfies         sup   θ  ∈  Θ     R   (  θ  ,   δ  M   )     =    inf  δ     sup   θ  ∈  Θ     R   (  θ  ,  δ  )       .        subscript  supremum    θ  normal-Θ      R   θ   superscript  δ  M        subscript  infimum  δ     subscript  supremum    θ  normal-Θ      R   θ  δ        \sup_{\theta\in\Theta}R(\theta,\delta^{M})=\inf_{\delta}\sup_{\theta\in\Theta}%
 R(\theta,\delta).\,     Least favorable distribution  Logically, an estimator is minimax when it is the best in the worst case. Continuing this logic, a minimax estimator should be a Bayes estimator with respect to a prior least favorable distribution of   θ   θ   \theta\,\!   . To demonstrate this notion denote the average risk of the Bayes estimator    δ  π     subscript  δ  π    \delta_{\pi}\,\!   with respect to a prior distribution   π   π   \pi\,\!   as       r  π   =   ∫   R   (  θ  ,   δ  π   )   d  π   (  θ  )          subscript  r  π       R   θ   subscript  δ  π    d  π  θ      r_{\pi}=\int R(\theta,\delta_{\pi})\,d\pi(\theta)\,     Definition: A prior distribution   π   π   \pi\,\!   is called least favorable if for any other distribution    π  ′     superscript  π  normal-′    \pi^{\prime}\,\!   the average risk satisfies     r  π   ≥    r   π  ′          subscript  r  π    subscript  r   superscript  π  normal-′      r_{\pi}\geq r_{\pi^{\prime}}\,   .  Theorem 1: If      r  π   =    sup  θ    R   (  θ  ,   δ  π   )      ,       subscript  r  π     subscript  supremum  θ     R   θ   subscript  δ  π        r_{\pi}=\sup_{\theta}R(\theta,\delta_{\pi}),\,   then:       δ  π     subscript  δ  π    \delta_{\pi}\,\!   is minimax.  If    δ  π     subscript  δ  π    \delta_{\pi}\,\!   is a unique Bayes estimator, it is also the unique minimax estimator.     π   π   \pi\,\!   is least favorable.   Corollary: If a Bayes estimator has constant risk, it is minimax. Note that this is not a necessary condition.  Example 1, Unfair coin: Consider the problem of estimating the "success" rate of a Binomial variable,    x  ∼   B   (  n  ,  θ  )       similar-to  x    B   n  θ      x\sim B(n,\theta)\,\!   . This may be viewed as estimating the rate at which an unfair coin falls on "heads" or "tails". In this case the Bayes estimator with respect to a Beta -distributed prior,    θ  ∼   Beta   (    n   /  2   ,    n   /  2   )       similar-to  θ    Beta       n   2       n   2       \theta\sim\text{Beta}(\sqrt{n}/2,\sqrt{n}/2)\,   is        δ  M   =    x  +   0.5   n      n  +   n      ,       superscript  δ  M       x    0.5    n       n    n       \delta^{M}=\frac{x+0.5\sqrt{n}}{n+\sqrt{n}},\,     with constant Bayes risk      r  =    1   4    (   1  +   n    )   2          r    1    4   superscript    1    n    2       r=\frac{1}{4(1+\sqrt{n})^{2}}\,     and, according to the Corollary, is minimax.  Definition: A sequence of prior distributions    π  n     subscript  π  n    {\pi}_{n}\,\!   is called least favorable if for any other distribution    π  ′     superscript  π  normal-′    \pi^{\prime}\,\!   ,         lim   n  →  ∞     r   π  n     ≥   r   π  ′     .        subscript    normal-→  n      subscript  r   subscript  π  n      subscript  r   superscript  π  normal-′      \lim_{n\rightarrow\infty}r_{\pi_{n}}\geq r_{\pi^{\prime}}.\,     Theorem 2: If there are a sequence of priors    π  n     subscript  π  n    \pi_{n}\,\!   and an estimator   δ   δ   \delta\,\!   such that      sup  θ    R   (  θ  ,  δ  )     =    lim   n  →  ∞     r   π  n           subscript  supremum  θ     R   θ  δ       subscript    normal-→  n      subscript  r   subscript  π  n       \sup_{\theta}R(\theta,\delta)=\lim_{n\rightarrow\infty}r_{\pi_{n}}\,\!   , then :      δ   δ   \delta\,\!   is minimax.  The sequence    π  n     subscript  π  n    {\pi}_{n}\,\!   is least favorable.   Notice that no uniqueness is guaranteed here. For example, the ML estimator from the previous example may be attained as the limit of Bayes estimators with respect to a uniform prior,     π  n   ∼   U   [   -  n   ,  n  ]       similar-to   subscript  π  n     U     n   n      \pi_{n}\sim U[-n,n]\,\!   with increasing support and also with respect to a zero mean normal prior     π  n   ∼   N   (  0  ,   n   σ  2    )       similar-to   subscript  π  n     N   0    n   superscript  σ  2        \pi_{n}\sim N(0,n\sigma^{2})\,\!   with increasing variance. So neither the resulting ML estimator is unique minimax nor the least favorable prior is unique.  Example 2: Consider the problem of estimating the mean of   p   p   p\,\!   dimensional Gaussian random vector,    x  ∼   N   (  θ  ,    I  p    σ  2    )       similar-to  x    N   θ     subscript  I  p    superscript  σ  2        x\sim N(\theta,I_{p}\sigma^{2})\,\!   . The Maximum likelihood (ML) estimator for   θ   θ   \theta\,\!   in this case is simply     δ   M  L    =  x       subscript  δ    M  L    x    \delta_{ML}=x\,\!   , and its risk is        R   (  θ  ,   δ   M  L    )    =   E    ∥    δ   M  L    -  θ   ∥   2    =    ∑  1  p    E    (    x  i   -   θ  i    )   2     =   p   σ  2     .          R   θ   subscript  δ    M  L        E   superscript   norm     subscript  δ    M  L    θ    2           superscript   subscript   1   p     E   superscript     subscript  x  i    subscript  θ  i    2            p   superscript  σ  2       R(\theta,\delta_{ML})=E{\|\delta_{ML}-\theta\|^{2}}=\sum\limits_{1}^{p}E{(x_{i%
 }-\theta_{i})^{2}}=p\sigma^{2}.\,     (Figure)  MSE of maximum likelihood estimator versus James–Stein estimator   The risk is constant, but the ML estimator is actually not a Bayes estimator, so the Corollary of Theorem 1 does not apply. However, the ML estimator is the limit of the Bayes estimators with respect to the prior sequence     π  n   ∼   N   (  0  ,   n   σ  2    )       similar-to   subscript  π  n     N   0    n   superscript  σ  2        \pi_{n}\sim N(0,n\sigma^{2})\,\!   , and, hence, indeed minimax according to Theorem 2 . Nonetheless, minimaxity does not always imply admissibility . In fact in this example, the ML estimator is known to be inadmissible (not admissible) whenever    p  >  2      p  2    p>2\,\!   . The famous James–Stein estimator dominates the ML whenever    p  >  2      p  2    p>2\,\!   . Though both estimators have the same risk    p   σ  2       p   superscript  σ  2     p\sigma^{2}\,\!   when     ∥  θ  ∥   →  ∞     normal-→   norm  θ      \|\theta\|\rightarrow\infty\,\!   , and they are both minimax, the James–Stein estimator has smaller risk for any finite    ∥  θ  ∥     norm  θ    \|\theta\|\,\!   . This fact is illustrated in the following figure.  Some examples  In general it is difficult, often even impossible to determine the minimax estimator. Nonetheless, in many cases a minimax estimator has been determined.  Example 3, Bounded Normal Mean: When estimating the Mean of a Normal Vector    x  ∼   N   (  θ  ,    I  n    σ  2    )       similar-to  x    N   θ     subscript  I  n    superscript  σ  2        x\sim N(\theta,I_{n}\sigma^{2})\,\!   , where it is known that      ∥  θ  ∥   2   ≤  M       superscript   norm  θ   2   M    \|\theta\|^{2}\leq M\,\!   . The Bayes estimator with respect to a prior which is uniformly distributed on the edge of the bounding sphere is known to be minimax whenever    M  ≤  n      M  n    M\leq n\,\!   . The analytical expression for this estimator is        δ  M   =    n   J   n  +  1     (   n   ∥  x  ∥    )      ∥  x  ∥    J  n    (   n   ∥  x  ∥    )      ,       superscript  δ  M       n   subscript  J    n  1      n   norm  x        norm  x    subscript  J  n     n   norm  x        \delta^{M}=\frac{nJ_{n+1}(n\|x\|)}{\|x\|J_{n}(n\|x\|)},\,     where     J  n    (  t  )        subscript  J  n   t    J_{n}(t)\,\!   , is the modified Bessel function of the first kind of order n .  Asymptotic minimax estimator  The difficulty of determining the exact minimax estimator has motivated the study of estimators of asymptotic minimax --- an estimator    δ  ′     superscript  δ  normal-′    \delta^{\prime}   is called   c   c   c   -asymptotic (or approximate) minimax if         sup   θ  ∈  Θ     R   (  θ  ,   δ  ′   )     ≤   c    inf  δ     sup   θ  ∈  Θ     R   (  θ  ,  δ  )        .        subscript  supremum    θ  normal-Θ      R   θ   superscript  δ  normal-′        c    subscript  infimum  δ     subscript  supremum    θ  normal-Θ      R   θ  δ         \sup_{\theta\in\Theta}R(\theta,\delta^{\prime})\leq c\inf_{\delta}\sup_{\theta%
 \in\Theta}R(\theta,\delta).     For many estimation problems, especially in the non-parametric estimation setting, various approximate minimax estimators have been established. The design of approximate minimax estimator is intimately related to the geometry, such as the metric entropy number , of   Θ   normal-Θ   \Theta   .  Relationship to Robust Optimization  Robust optimization is an approach to solve optimization problems under uncertainty in the knowledge of underlying parameters,. 1 2 For instance, the MMSE Bayesian estimation of a parameter requires the knowledge of parameter correlation function. If the knowledge of this correlation function is not perfectly available, a popular minimax robust optimization approach 3 is to define a set characterizing the uncertainty about the correlation function, and then pursuing a minimax optimization over the uncertainty set and the estimator respectively. Similar minimax optimizations can be pursued to make estimators robust to certain imprecisely known parameters. For instance, a recent study dealing with such techniques in the area of signal processing can be found in. 4  In R. Fandom Noubiap and W. Seidel (2001) an algorithm for calculating a Gamma-minimax decision rule has been developed, when Gamma is given by a finite number of generalized moment conditions. Such a decision rule minimizes the maximum of the integrals of the risk function with respect to all distributions in Gamma. Gamma-minimax decision rules are of interest in robustness studies in Bayesian statistics.  References   E. L. Lehmann and G. Casella (1998), Theory of Point Estimation, 2nd ed. New York: Springer-Verlag.  F. Perron and E. Marchand (2002), "On the minimax estimator of a bounded normal mean," Statistics and Probability Letters  58 : 327–333.  J. O. Berger (1985), Statistical Decision Theory and Bayesian Analysis, 2nd ed. New York: Springer-Verlag. ISBN 0-387-96098-8.  R. Fandom Noubiap and W. Seidel (2001), An Algorithm for Calculating Gamma-Minimax Decision Rules under Generalized Moment Conditions, Annals of Statistics, Aug., 2001, vol. 29, no. 4, p. 1094–1116    "  Category:Decision theory  Category:Estimation theory            