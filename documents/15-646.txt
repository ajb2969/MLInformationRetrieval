


Inverse distribution




Inverse distribution

In probability theory and statistics, an inverse distribution is the distribution of the reciprocal of a random variable. Inverse distributions arise in particular in the Bayesian context of prior distributions and posterior distributions for scale parameters. In the algebra of random variables, inverse distributions are special cases of the class of ratio distributions, in which the numerator random variable has a degenerate distribution.
Relation to original distribution
In general, given the probability distribution of a random variable X with strictly positive support, it is possible to find the distribution of the reciprocal, Y = 1 / X. If the distribution of X is continuous with density function f(x) and cumulative distribution function F(x), then the cumulative distribution function, G(y), of the reciprocal is found by noting that



Then the density function of Y is found as the derivative of the cumulative distribution function:



Examples
Reciprocal distribution
The reciprocal distribution has a density function of the form.1


 
  means "is proportional to". It follows that the inverse distribution in this case is of the form


 
 |
 support    =

|
 pdf        =

|
 cdf        =

|
 mean       =  |
 median     = 


| variance   =
| skewness   =
| kurtosis   =
| entropy    =
| mgf        =
| char       =
| pgf        =
| fisher     =
}}
If the original random variable X is uniformly distributed on the interval (a,b), where a>0, then the reciprocal variable Y = 1 / X has the reciprocal distribution which takes values in the range (b−1 ,a−1), and the probability density function in this range is



and is zero elsewhere.
The cumulative distribution function of the reciprocal, within the same range, is



Inverse t distribution
Let X be a t distributed random variate with k degrees of freedom. Then its density function is
$$f( x ) = \frac{ 1 }{ \sqrt{ k \pi } } \frac{ \Gamma\left( \frac{ k + 1 }{ 2 } \right) }{ \Gamma\left( \frac{ k }{ 2 } \right) } \frac{ 1 }{ \left( 1 + \frac{ x^2 }{ k } \right)^{ \frac{ 1 + k }{ 2 } } } .$$
The density of Y = 1 / X is
$$g( y ) = \frac{ 1 }{ \sqrt{ k \pi } } \frac{ \Gamma\left( \frac{ k + 1 }{ 2 } \right) }{ \Gamma\left( \frac{ k }{ 2 } \right) } \frac{ 1 }{ y^2 \left( 1 + \frac{ 1 }{ y^2 k } \right)^{ \frac{ 1 + k }{ 2 } } } .$$
With k = 1, the distributions of X and 1 / X are identical. If k > 1 then the distribution of 1 / X is bimodal.
Reciprocal normal distribution
If X is a standard normally distributed variable then the distribution of 1/X is bimodal,2 and the first and higher-order moments do not exist.3
Inverse Cauchy distribution
If X is a Cauchy distributed (μ, σ) random variable, then 1 / X is a Cauchy ( μ / C, σ / C ) random variable where C = μ2 + σ2.
Inverse F distribution
If X is an F(ν1, ν2 ) distributed random variable then 1 / X is an F(ν2, ν1 ) random variable.
Other inverse distributions
Other inverse distributions include the inverse-chi-squared distribution, the inverse-gamma distribution, the inverse-Wishart distribution, and the inverse matrix gamma distribution.
Applications
Inverse distributions are widely used as prior distributions in Bayesian inference for scale parameters.
See also

Harmonic mean
Ratio distribution
Self-reciprocal distributions

References
"
Category:Algebra of random variables Category:Types of probability distributions



Hamming R. W. (1970) "On the distribution of numbers", The Bell System Technical Journal 49(8) 1609–1625
 (this is a special case of the generalized inverse normal distribution treated)





