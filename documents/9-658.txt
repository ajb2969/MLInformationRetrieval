   LPBoost      LPBoost   Linear Programming Boosting ( LPBoost ) is a supervised classifier from the boosting family of classifiers. LPBoost maximizes a margin between training samples of different classes and hence also belongs to the class of margin-maximizing supervised classification algorithms. Consider a classification function       f  :   𝒳  →   {   -  1   ,  1  }     ,     normal-:  f   normal-→  𝒳     1   1      f:\mathcal{X}\to\{-1,1\},   which classifies samples from a space   𝒳   𝒳   \mathcal{X}   into one of two classes, labelled 1 and -1, respectively. LPBoost is an algorithm to learn such a classification function given a set of training examples with known class labels. LPBoost is a machine learning technique and especially suited for applications of joint classification and feature selection in structured domains.  LPBoost overview  As in all boosting classifiers, the final classification function is of the form        f   (  𝒙  )    =    ∑   j  =  1   J     α  j    h  j    (  𝒙  )      ,        f  𝒙     superscript   subscript     j  1    J      subscript  α  j    subscript  h  j   𝒙      f(\boldsymbol{x})=\sum_{j=1}^{J}\alpha_{j}h_{j}(\boldsymbol{x}),   where    α  j     subscript  α  j    \alpha_{j}   are non-negative weightings for weak classifiers     h  j   :   𝒳  →   {   -  1   ,  1  }       normal-:   subscript  h  j    normal-→  𝒳     1   1      h_{j}:\mathcal{X}\to\{-1,1\}   . Each individual weak classifier    h  j     subscript  h  j    h_{j}   may be just a little bit better than random, but the resulting linear combination of many weak classifiers can perform very well.  LPBoost constructs   f   f   f   by starting with an empty set of weak classifiers. Iteratively, a single weak classifier to add to the set of considered weak classifiers is selected, added and all the weights   𝜶   𝜶   \boldsymbol{\alpha}   for the current set of weak classifiers are adjusted. This is repeated until no weak classifiers to add remain.  The property that all classifier weights are adjusted in each iteration is known as totally-corrective property. Early boosting methods, such as AdaBoost do not have this property and converge slower.  Linear program  More generally, let    ℋ  =   {   h   (  ⋅  ;  ω  )    |   ω  ∈  Ω   }       ℋ   conditional-set    h   normal-⋅  ω      ω  normal-Ω      \mathcal{H}=\{h(\cdot;\omega)|\omega\in\Omega\}   be the possibly infinite set of weak classifiers, also termed hypotheses . One way to write down the problem LPBoost solves is as a linear program with infinitely many variables.  The primal linear program of LPBoost, optimizing over the non-negative weight vector   𝜶   𝜶   \boldsymbol{\alpha}   , the non-negative vector   𝝃   𝝃   \boldsymbol{\xi}   of slack variables and the margin    ρ   ρ   \rho   is the following.         min   𝜶  ,  𝝃  ,  ρ        -  ρ   +   D    ∑   n  =  1   ℓ    ξ  n          sb.t.          ∑   ω  ∈  Ω      y  n    α  ω   h   (   𝒙  n   ;  ω  )     +   ξ  n    ≥  ρ   ,   n  =   1  ,  …  ,  ℓ     ,            ∑   ω  ∈  Ω     α  ω    =  1   ,            ξ  n   ≥  0   ,   n  =   1  ,  …  ,  ℓ     ,            α  ω   ≥  0   ,   ω  ∈  Ω    ,          ρ  ∈  ℝ   .           𝜶  𝝃  ρ         ρ     D    superscript   subscript     n  1    normal-ℓ    subscript  ξ  n        sb.t.   formulae-sequence        subscript     ω  normal-Ω       subscript  y  n    subscript  α  ω   h    subscript  𝒙  n   ω      subscript  ξ  n    ρ     n   1  normal-…  normal-ℓ        missing-subexpression       subscript     ω  normal-Ω     subscript  α  ω    1      missing-subexpression    formulae-sequence     subscript  ξ  n   0     n   1  normal-…  normal-ℓ        missing-subexpression    formulae-sequence     subscript  α  ω   0     ω  normal-Ω       missing-subexpression     ρ  ℝ      \begin{array}[]{cl}\underset{\boldsymbol{\alpha},\boldsymbol{\xi},\rho}{\min}&%
 -\rho+D\sum_{n=1}^{\ell}\xi_{n}\\
 \textrm{sb.t.}&\sum_{\omega\in\Omega}y_{n}\alpha_{\omega}h(\boldsymbol{x}_{n};%
 \omega)+\xi_{n}\geq\rho,\qquad n=1,\dots,\ell,\\
 &\sum_{\omega\in\Omega}\alpha_{\omega}=1,\\
 &\xi_{n}\geq 0,\qquad n=1,\dots,\ell,\\
 &\alpha_{\omega}\geq 0,\qquad\omega\in\Omega,\\
 &\rho\in{\mathbb{R}}.\end{array}     Note the effects of slack variables    𝝃  ≥  0      𝝃  0    \boldsymbol{\xi}\geq 0   : their one-norm is penalized in the objective function by a constant factor   D   D   D   , which—if small enough—always leads to a primal feasible linear program.  Here we adopted the notation of a parameter space   Ω   normal-Ω   \Omega   , such that for a choice    ω  ∈  Ω      ω  normal-Ω    \omega\in\Omega   the weak classifier     h   (  ⋅  ;  ω  )    :   𝒳  →   {   -  1   ,  1  }       normal-:    h   normal-⋅  ω     normal-→  𝒳     1   1      h(\cdot;\omega):\mathcal{X}\to\{-1,1\}   is uniquely defined.  When the above linear program was first written down in early publications about boosting methods it was disregarded as intractable due to the large number of variables   𝜶   𝜶   \boldsymbol{\alpha}   . Only later it was discovered that such linear programs can indeed be solved efficiently using the classic technique of column generation .  Column Generation for LPBoost  In a linear program a column corresponds to a primal variable. Column generation is a technique to solve large linear programs. It typically works in a restricted problem, dealing only with a subset of variables. By generating primal variables iteratively and on-demand, eventually the original unrestricted problem with all variables is recovered. By cleverly choosing the columns to generate the problem can be solved such that while still guaranteeing the obtained solution to be optimal for the original full problem, only a small fraction of columns has to be created.  LPBoost dual problem  Columns in the primal linear program corresponds to rows in the dual linear program . The equivalent dual linear program of LPBoost is the following linear program.         max   𝝀  ,  γ      γ      sb.t.          ∑   n  =  1   ℓ     y  n   h   (   𝒙  n   ;  ω  )    λ  n     +  γ   ≤  0   ,   ω  ∈  Ω    ,           0  ≤   λ  n   ≤  D   ,   n  =   1  ,  …  ,  ℓ     ,            ∑   n  =  1   ℓ    λ  n    =  1   ,          γ  ∈  ℝ   .           𝝀  γ     γ    sb.t.   formulae-sequence        superscript   subscript     n  1    normal-ℓ      subscript  y  n   h    subscript  𝒙  n   ω    subscript  λ  n     γ   0     ω  normal-Ω       missing-subexpression    formulae-sequence      0   subscript  λ  n        D      n   1  normal-…  normal-ℓ        missing-subexpression       superscript   subscript     n  1    normal-ℓ    subscript  λ  n    1      missing-subexpression     γ  ℝ      \begin{array}[]{cl}\underset{\boldsymbol{\lambda},\gamma}{\max}&\gamma\\
 \textrm{sb.t.}&\sum_{n=1}^{\ell}y_{n}h(\boldsymbol{x}_{n};\omega)\lambda_{n}+%
 \gamma\leq 0,\qquad\omega\in\Omega,\\
 &0\leq\lambda_{n}\leq D,\qquad n=1,\dots,\ell,\\
 &\sum_{n=1}^{\ell}\lambda_{n}=1,\\
 &\gamma\in\mathbb{R}.\end{array}     For linear programs the optimal value of the primal and dual problem are equal. For the above primal and dual problems, the optimal value is equal to the negative 'soft margin'. The soft margin is the size of the margin separating positive from negative training instances minus positive slack variables that carry penalties for margin-violating samples. Thus, the soft margin may be positive although not all samples are linearly separated by the classification function. The latter is called the 'hard margin' or 'realized margin'.  Convergence criterion  Consider a subset of the satisfied constraints in the dual problem. For any finite subset we can solve the linear program and thus satisfy all constraints. If we could prove that of all the constraints which we did not add to the dual problem no single constraint is violated, we would have proven that solving our restricted problem is equivalent to solving the original problem. More formally, let    γ  *     superscript  γ     \gamma^{*}   be the optimal objective function value for any restricted instance. Then, we can formulate a search problem for the 'most violated constraint' in the original problem space, namely finding     ω  *   ∈  Ω       superscript  ω    normal-Ω    \omega^{*}\in\Omega   as        ω  *   =    argmax   ω  ∈  Ω      ∑   n  =  1   ℓ     y  n   h   (   𝒙  n   ;  ω  )    λ  n       .       superscript  ω         ω  normal-Ω   argmax     superscript   subscript     n  1    normal-ℓ      subscript  y  n   h    subscript  𝒙  n   ω    subscript  λ  n        \omega^{*}=\underset{\omega\in\Omega}{\textrm{argmax}}\sum_{n=1}^{\ell}y_{n}h(%
 \boldsymbol{x}_{n};\omega)\lambda_{n}.     That is, we search the space   ℋ   ℋ   \mathcal{H}   for a single decision stump     h   (  ⋅  ;   ω  *   )       h   normal-⋅   superscript  ω       h(\cdot;\omega^{*})   maximizing the left hand side of the dual constraint. If the constraint cannot be violated by any choice of decision stump, none of the corresponding constraint can be active in the original problem and the restricted problem is equivalent.  Penalization constant   D   D   D     The positive value of penalization constant   D   D   D   has to be found using model selection techniques. However, if we choose    D  =   1   ℓ  ν        D    1    normal-ℓ  ν      D=\frac{1}{\ell\nu}   , where   ℓ   normal-ℓ   \ell   is the number of training samples and    0  <  ν  <  1        0  ν       1     0<\nu<1   , then the new parameter   ν   ν   \nu   has the following properties.      ν   ν   \nu   is an upper bound on the fraction of training errors; that is, if   k   k   k   denotes the number of misclassified training samples, then     k  ℓ   ≤  ν        k  normal-ℓ   ν    \frac{k}{\ell}\leq\nu   .     ν   ν   \nu   is a lower bound on the fraction of training samples outside or on the margin.   Algorithm   Input:  Training set    X  =   {   𝒙  1   ,  …  ,   𝒙  ℓ   }       X    subscript  𝒙  1   normal-…   subscript  𝒙  normal-ℓ      X=\{\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{\ell}\}   ,     𝒙  i   ∈  𝒳       subscript  𝒙  i   𝒳    \boldsymbol{x}_{i}\in\mathcal{X}     Training labels    Y  =   {   y  1   ,  …  ,   y  ℓ   }       Y    subscript  y  1   normal-…   subscript  y  normal-ℓ      Y=\{y_{1},\dots,y_{\ell}\}   ,     y  i   ∈   {   -  1   ,  1  }        subscript  y  i      1   1     y_{i}\in\{-1,1\}     Convergence threshold    θ  ≥  0      θ  0    \theta\geq 0      Output:  Classification function    f  :   𝒳  →   {   -  1   ,  1  }       normal-:  f   normal-→  𝒳     1   1      f:\mathcal{X}\to\{-1,1\}        Initialization  Weights, uniform      λ  n   ←   1  ℓ    ,   n  =   1  ,  …  ,  ℓ       formulae-sequence   normal-←   subscript  λ  n     1  normal-ℓ      n   1  normal-…  normal-ℓ      \lambda_{n}\leftarrow\frac{1}{\ell},\quad n=1,\dots,\ell     Edge    γ  ←  0     normal-←  γ  0    \gamma\leftarrow 0     Hypothesis count    J  ←  1     normal-←  J  1    J\leftarrow 1      Iterate       h  ^   ←    argmax   ω  ∈  Ω      ∑   n  =  1   ℓ     y  n   h   (   𝒙  n   ;  ω  )    λ  n         normal-←   normal-^  h        ω  normal-Ω   argmax     superscript   subscript     n  1    normal-ℓ      subscript  y  n   h    subscript  𝒙  n   ω    subscript  λ  n        \hat{h}\leftarrow\underset{\omega\in\Omega}{\textrm{argmax}}\sum_{n=1}^{\ell}y%
 _{n}h(\boldsymbol{x}_{n};\omega)\lambda_{n}     if       ∑   n  =  1   ℓ     y  n    h  ^    (   𝒙  n   )    λ  n     +  γ   ≤  θ          superscript   subscript     n  1    normal-ℓ      subscript  y  n    normal-^  h    subscript  𝒙  n    subscript  λ  n     γ   θ    \sum_{n=1}^{\ell}y_{n}\hat{h}(\boldsymbol{x}_{n})\lambda_{n}+\gamma\leq\theta   then  break        h  J   ←   h  ^      normal-←   subscript  h  J    normal-^  h     h_{J}\leftarrow\hat{h}         J  ←   J  +  1      normal-←  J    J  1     J\leftarrow J+1          (  𝝀  ,  γ  )   ←      normal-←   𝝀  γ   absent    (\boldsymbol{\lambda},\gamma)\leftarrow   solution of the LPBoost dual      𝜶  ←      normal-←  𝜶  absent    \boldsymbol{\alpha}\leftarrow   Lagrangian multipliers of solution to LPBoost dual problem        f   (  𝒙  )    :=   sign   (    ∑   j  =  1   J     α  j    h  j    (  𝒙  )     )       assign    f  𝒙     sign    superscript   subscript     j  1    J      subscript  α  j    subscript  h  j   𝒙       f(\boldsymbol{x}):=\textrm{sign}\left(\sum_{j=1}^{J}\alpha_{j}h_{j}(%
 \boldsymbol{x})\right)      Note that if the convergence threshold is set to    θ  =  0      θ  0    \theta=0   the solution obtained is the global optimal solution of the above linear program. In practice,   θ   θ   \theta   is set to a small positive value in order obtain a good solution quickly.  Realized margin  The actual margin separating the training samples is termed the realized margin and is defined as        ρ   (  𝜶  )    :=     min   n  =   1  ,  …  ,  ℓ      y  n      ∑    α  ω   ∈  Ω      α  ω   h   (   𝒙  n   ;  ω  )       .     assign    ρ  𝜶       subscript     n   1  normal-…  normal-ℓ      subscript  y  n      subscript      subscript  α  ω   normal-Ω       subscript  α  ω   h    subscript  𝒙  n   ω        \rho(\boldsymbol{\alpha}):=\min_{n=1,\dots,\ell}y_{n}\sum_{\alpha_{\omega}\in%
 \Omega}\alpha_{\omega}h(\boldsymbol{x}_{n};\omega).     The realized margin can and will usually be negative in the first iterations. For a hypothesis space that permits singling out of any single sample, as is commonly the case, the realized margin will eventually converge to some positive value.  Convergence guarantee  While the above algorithm is proven to converge, in contrast to other boosting formulations, such as AdaBoost and TotalBoost , there are no known convergence bounds for LPBoost. In practise however, LPBoost is known to converge quickly, often faster than other formulations.  Base learners  LPBoost is an ensemble learning method and thus does not dictate the choice of base learners, the space of hypotheses   ℋ   ℋ   \mathcal{H}   . Demiriz et al. showed that under mild assumptions, any base learner can be used. If the base learners are particularly simple, they are often referred to as decision stumps .  The number of base learners commonly used with Boosting in the literature is large. For example, if    𝒳  ⊆   ℝ  n       𝒳   superscript  ℝ  n     \mathcal{X}\subseteq{\mathbb{R}}^{n}   , a base learner could be a linear soft margin support vector machine . Or even more simple, a simple stump of the form      h   (  𝒙  ;  ω  ∈   {  1  ,  -  1  }   ,  p  ∈   {  1  ,  …  ,  n  }   ,  t  ∈  ℝ  )   :=   {     ω      if   𝒙  p    ≤  t        -  ω     otherwise     .      fragments  h   fragments  normal-(  x  normal-;  ω    fragments  normal-{  1  normal-,   1  normal-}   normal-,  p    fragments  normal-{  1  normal-,  normal-…  normal-,  n  normal-}   normal-,  t   R  normal-)   assign   fragments  normal-{    ω      if   subscript  𝒙  p    t       ω   otherwise    normal-.     h(\boldsymbol{x};\omega\in\{1,-1\},p\in\{1,\dots,n\},t\in{\mathbb{R}}):=\left%
 \{\begin{array}[]{cl}\omega&\textrm{if~{}}\boldsymbol{x}_{p}\leq t\\
 -\omega&\textrm{otherwise}\end{array}\right..     The above decision stumps looks only along a single dimension   p   p   p   of the input space and simply thresholds the respective column of the sample using a constant threshold   t   t   t   . Then, it can decide in either direction, depending on   ω   ω   \omega   for a positive or negative class.  Given weights for the training samples, constructing the optimal decision stump of the above form simply involves searching along all sample columns and determining   p   p   p   ,   t   t   t   and   ω   ω   \omega   in order to optimize the gain function.  References   Linear Programming Boosting via Column Generation , A. Demiriz and K.P. Bennett and J. Shawe-Taylor. Published 2002 in Kluwer Machine Learning 46, pages 225–254.   "  Category:Ensemble learning   