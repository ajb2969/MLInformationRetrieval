   Doob's martingale convergence theorems      Doob's martingale convergence theorems   In mathematics specifically, in stochastic analysis Doob's martingale convergence theorems are a collection of results on the long-time limits of supermartingales , named after the American mathematician Joseph L. Doob .  Statement of the theorems  In the following, (Ω, F , F ∗ , P ), F ∗ = ( F t ) t ≥0 , will be a filtered probability space and N : [0, +∞) × Ω → R will be a right- continuous supermartingale with respect to the filtration F ∗ ; in other words, for all 0 ≤ s ≤ t N_{s} \geq \mathbf{E} \big[ N_{t} \big| F_{s} \big].  Doob's first martingale convergence theorem  Doob's first martingale convergence theorem provides a sufficient condition for the random variables  N t to have a limit as t → +∞ in a pointwise sense, i.e. for each ω in the sample space Ω individually.  For t ≥ 0, let N t − = max(− N t , 0) and suppose that         sup   t  >  0     𝐄   [   N  t  -   ]     <   +  ∞    .        subscript  supremum    t  0      𝐄   delimited-[]   superscript   subscript  N  t              \sup_{t>0}\mathbf{E}\big[N_{t}^{-}\big]<+\infty.     Then the pointwise limit       N   (  ω  )    =    lim   t  →   +  ∞       N  t    (  ω  )           N  ω     subscript    normal-→  t           subscript  N  t   ω      N(\omega)=\lim_{t\to+\infty}N_{t}(\omega)     exists finite for P - almost all  ω ∈ Ω.  Doob's second martingale convergence theorem  It is important to note that the convergence in Doob's first martingale convergence theorem is pointwise, not uniform, and is unrelated to convergence in mean square, or indeed in any L p space . In order to obtain convergence in L 1 (i.e., convergence in mean ), one requires uniform integrability of the random variables N t . By Chebyshev's inequality , convergence in L 1 implies convergence in probability and convergence in distribution.  The following are equivalent:   ( N t ) t >0 is uniformly integrable , i.e.            lim   C  →  ∞      sup   t  >  0      ∫   {   ω  ∈  Ω   |     N  t    (  ω  )    >  C   }      |    N  t    (  ω  )    |   d  𝐏   (  ω  )       =  0   ;        subscript    normal-→  C       subscript  supremum    t  0      subscript    conditional-set    ω  normal-Ω        subscript  N  t   ω   C            subscript  N  t   ω    normal-d  𝐏  ω      0    \lim_{C\to\infty}\sup_{t>0}\int_{\{\omega\in\Omega|N_{t}(\omega)>C\}}\big|N_{t%
 }(\omega)\big|\,\mathrm{d}\mathbf{P}(\omega)=0;         there exists an integrable random variable N ∈ L 1 (Ω, P ; R ) such that N t → N as t → +∞ both P - almost surely and in L 1 (Ω, P ; R ), i.e.           𝐄   [   |    N  t   -  N   |   ]    =    ∫  Ω     |     N  t    (  ω  )    -   N   (  ω  )     |   d  𝐏   (  ω  )     →   0  as  t   →   +  ∞    .          𝐄   delimited-[]       subscript  N  t   N        subscript   normal-Ω            subscript  N  t   ω     N  ω     normal-d  𝐏  ω      normal-→      0  as  t     normal-→           \mathbf{E}\big[\big|N_{t}-N\big|\big]=\int_{\Omega}\big|N_{t}(\omega)-N(\omega%
 )\big|\,\mathrm{d}\mathbf{P}(\omega)\to 0\mbox{ as }t\to+\infty.        Corollary: convergence theorem for continuous martingales  Let M : [0, +∞) × Ω → R be a continuous martingale such that        sup   t  >  0     𝐄   [    |   M  t   |   p   ]     <   +  ∞         subscript  supremum    t  0      𝐄   delimited-[]   superscript     subscript  M  t    p            \sup_{t>0}\mathbf{E}\big[\big|M_{t}\big|^{p}\big]<+\infty     for some p > 1. Then there exists a random variable M ∈ L p (Ω, P ; R ) such that M t → M as t → +∞ both P -almost surely and in L p (Ω, P ; R ).  Discrete-time results  Similar results can be obtained for discrete-time supermartingales and submartingales, the obvious difference being that no continuity assumptions are required. For example, the result above becomes  Let M : N × Ω → R be a discrete-time martingale such that        sup   k  ∈  𝐍     𝐄   [    |   M  k   |   p   ]     <   +  ∞         subscript  supremum    k  𝐍      𝐄   delimited-[]   superscript     subscript  M  k    p            \sup_{k\in\mathbf{N}}\mathbf{E}\big[\big|M_{k}\big|^{p}\big]<+\infty     for some p > 1. Then there exists a random variable M ∈ L p (Ω, P ; R ) such that M k → M as k → +∞ both P -almost surely and in L p (Ω, P ; R )  Convergence of conditional expectations: Lévy's zero–one law  Doob's martingale convergence theorems imply that conditional expectations also have a convergence property.  Let (Ω, F , P ) be a probability space and let X be a random variable in L 1 . Let F ∗ = ( F k ) k ∈ N be any filtration of F , and define F ∞ to be the minimal σ -algebra generated by ( F k ) k ∈ N . Then      𝐄   [  X  |   F  k   ]   →  𝐄   [  X  |   F  ∞   ]   as  k  →  ∞     fragments  E   fragments  normal-[  X  normal-|   subscript  F  k   normal-]   normal-→  E   fragments  normal-[  X  normal-|   subscript  F    normal-]   as  k  normal-→     \mathbf{E}\big[X\big|F_{k}\big]\to\mathbf{E}\big[X\big|F_{\infty}\big]\mbox{ %
 as }k\to\infty     both P -almost surely and in L 1 .  This result is usually called Lévy's zero–one law . The reason for the name is that if A is an event in F ∞ , then the theorem says that    𝐏   [  A  |   F  k   ]   →   𝟏  A      fragments  P   fragments  normal-[  A  normal-|   subscript  F  k   normal-]   normal-→   subscript  1  A     \mathbf{P}[A|F_{k}]\to\mathbf{1}_{A}   almost surely, i.e., the limit of the probabilities is 0 or 1. In plain language, if we are learning gradually all the information that determines the outcome of an event, then we will become gradually certain what the outcome will be. This sounds almost like a tautology , but the result is still non-trivial. For instance, it easily implies Kolmogorov's zero–one law , since it says that for any tail event  A , we must have     𝐏   [  A  ]    =   𝟏  A         𝐏   delimited-[]  A     subscript  1  A     \mathbf{P}[A]=\mathbf{1}_{A}   almost surely, hence     𝐏   [  A  ]    ∈   {  0  ,  1  }         𝐏   delimited-[]  A     0  1     \mathbf{P}[A]\in\{0,1\}   .  See also   Backwards martingale convergence theorem   References    (See Appendix C)      "  Category:Probability theorems  Category:Martingale theory   