   Theorems and definitions in linear algebra      Theorems and definitions in linear algebra   This article collects the main theorems and definitions in linear algebra .  Vector Spaces  Let   V   V   V   be a set on which two operations ( vector addition and scalar multiplication ) are defined. If the listed axioms are satisfied for every    u  ‚Üí     normal-‚Üí  u    \vec{u}   ,    v  ‚Üí     normal-‚Üí  v    \vec{v}   , and    w  ‚Üí     normal-‚Üí  w    \vec{w}   in   V   V   V   and every scalar (real number)   c   c   c   and   d   d   d   , then   V   V   V   is called a vector space :  Addition:        u  ‚Üí   +    v  ‚Üí   is in  V  .        normal-‚Üí  u      normal-‚Üí  v   is in  V  .     \vec{u}+\vec{v}\text{ is in }V\text{.}           u  ‚Üí   +   v  ‚Üí    =    v  ‚Üí   +   u  ‚Üí           normal-‚Üí  u    normal-‚Üí  v       normal-‚Üí  v    normal-‚Üí  u      \vec{u}+\vec{v}=\vec{v}+\vec{u}           u  ‚Üí   +   (    v  ‚Üí   +   w  ‚Üí    )    =    (    u  ‚Üí   +   v  ‚Üí    )   +   w  ‚Üí           normal-‚Üí  u      normal-‚Üí  v    normal-‚Üí  w          normal-‚Üí  u    normal-‚Üí  v     normal-‚Üí  w      \vec{u}+(\vec{v}+\vec{w})=(\vec{u}+\vec{v})+\vec{w}           V  has a  ùê≥ùêûùê´ùê®   ùêØùêûùêúùê≠ùê®ùê´    0  ‚Üí   such that for every   u  ‚Üí   in  V  ,   u  ‚Üí    +   0  ‚Üí    =   u  ‚Üí           V  has a  ùê≥ùêûùê´ùê®   ùêØùêûùêúùê≠ùê®ùê´    normal-‚Üí  0   such that for every   normal-‚Üí  u   in  V  ,   normal-‚Üí  u     normal-‚Üí  0     normal-‚Üí  u     V\text{ has a }\mathbf{zero}\text{ }\mathbf{vector}\text{ }\vec{0}\text{ such %
 that for every }\vec{u}\text{ in }V\text{, }\vec{u}+\vec{0}=\vec{u}            For every   u  ‚Üí   in  V  , there is a vector in  V  denoted by   -    u  ‚Üí   such that   u  ‚Üí     +   (   -   u  ‚Üí    )    =    0  ‚Üí   .             For every   normal-‚Üí  u   in  V  , there is a vector in  V  denoted by      normal-‚Üí  u   such that   normal-‚Üí  u        normal-‚Üí  u        normal-‚Üí  0   .     \text{For every }\vec{u}\text{ in }V\text{, there is a vector in }V\text{ %
 denoted by }-\vec{u}\text{ such that }\vec{u}+(-\vec{u})=\vec{0}\text{.}      Scalar Multiplication:       c   u  ‚Üí   is in  V  .      c   normal-‚Üí  u   is in  V  .    c\vec{u}\text{ is in }V\text{.}          c   (    u  ‚Üí   +   v  ‚Üí    )    =    c   u  ‚Üí    +   c   v  ‚Üí           c     normal-‚Üí  u    normal-‚Üí  v         c   normal-‚Üí  u      c   normal-‚Üí  v       c(\vec{u}+\vec{v})=c\vec{u}+c\vec{v}           (   c  +  d   )    u  ‚Üí    =    c   u  ‚Üí    +   d   u  ‚Üí             c  d    normal-‚Üí  u        c   normal-‚Üí  u      d   normal-‚Üí  u       (c+d)\vec{u}=c\vec{u}+d\vec{u}          c   (   d   u  ‚Üí    )    =    (   c  d   )    u  ‚Üí          c    d   normal-‚Üí  u         c  d    normal-‚Üí  u      c(d\vec{u})=(cd)\vec{u}          1   (   u  ‚Üí   )    =   u  ‚Üí         1   normal-‚Üí  u     normal-‚Üí  u     1(\vec{u})=\vec{u}      Subspaces  If   W   W   W   is a nonempty subset of a vector space   V   V   V   , then   W   W   W   is a subspace of   V   V   V   if and only if the following closure conditions hold:        If   u  ‚Üí   and   v  ‚Üí   are in  W  , then   u  ‚Üí    +    v  ‚Üí   is in  W  .         If   normal-‚Üí  u   and   normal-‚Üí  v   are in  W  , then   normal-‚Üí  u       normal-‚Üí  v   is in  W  .     \text{If }\vec{u}\text{ and }\vec{v}\text{ are in }W\text{, then }\vec{u}+\vec%
 {v}\text{ is in }W\text{.}         If   u  ‚Üí   is in  W  and  c  is any scalar, then  c   u  ‚Üí   is in  W  .      If   normal-‚Üí  u   is in  W  and  c  is any scalar, then  c   normal-‚Üí  u   is in  W  .    \text{If }\vec{u}\text{ is in }W\text{ and }c\text{ is any scalar, then }c\vec%
 {u}\text{ is in }W\text{.}      Linear combinations  A vector    v  ‚Üí     normal-‚Üí  v    \vec{v}   in a vector space   V   V   V   is called a linear combination of the vectors     u  ‚Üí   1     subscript   normal-‚Üí  u   1    \vec{u}_{1}   ,     u  ‚Üí   2     subscript   normal-‚Üí  u   2    \vec{u}_{2}   ,   ‚Ä¶   normal-‚Ä¶   \dots   ,     u  ‚Üí   k     subscript   normal-‚Üí  u   k    \vec{u}_{k}   in   V   V   V   if    v  ‚Üí     normal-‚Üí  v    \vec{v}   can be written in the form     v  ‚Üí   =     c  1     u  ‚Üí   1    +    c  2     u  ‚Üí   2    +  ‚Ä¶  +    c  k     u  ‚Üí   k          normal-‚Üí  v        subscript  c  1    subscript   normal-‚Üí  u   1       subscript  c  2    subscript   normal-‚Üí  u   2    normal-‚Ä¶     subscript  c  k    subscript   normal-‚Üí  u   k       \vec{v}=c_{1}\vec{u}_{1}+c_{2}\vec{u}_{2}+\dots+c_{k}\vec{u}_{k}   , where    c  1     subscript  c  1    c_{1}   ,    c  2     subscript  c  2    c_{2}   ,   ‚Ä¶   normal-‚Ä¶   \dots   ,    c  k     subscript  c  k    c_{k}   are scalars.  Systems of linear equations  Cramer's Rule  If a system of   n   n   n   linear equations in   n   n   n   variables has a coefficient matrix with a nonzero determinant    |  A  |      A    \left|A\right|   , then the solution of the system is given by        x  1   =    det   (   A  1   )     det   (  A  )      ,     x  2   =     det   (   A  2   )     det   (  A  )     ,  ‚Ä¶    ,    x  n   =    det   (   A  n   )     det   (  A  )          formulae-sequence     subscript  x  1        subscript  A  1      A      formulae-sequence     subscript  x  2         subscript  A  2      A    normal-‚Ä¶       subscript  x  n        subscript  A  n      A        x_{1}=\frac{\det(A_{1})}{\det(A)},\qquad x_{2}=\frac{\det(A_{2})}{\det(A)},%
 \qquad\dots,\qquad x_{n}=\frac{\det(A_{n})}{\det(A)}   , where the   i   i   i   th column of    A  i     subscript  A  i    A_{i}   is the column of constants in the system of equations.  Linear dependence  Linear independence  Bases  A set of vectors    S  =   {    v  ‚Üí   1       fragments  S    fragments  normal-{   subscript   normal-‚Üí  v   1      S=\{\vec{v}_{1}   ,     v  ‚Üí   2     subscript   normal-‚Üí  v   2    \vec{v}_{2}   ,   ‚Ä¶   normal-‚Ä¶   \dots   ,      v  ‚Üí   n   }     fragments   subscript   normal-‚Üí  v   n   normal-}    \vec{v}_{n}\}   in a vector space   V   V   V   is called a basis if the following conditions are true:      S   S   S   spans   V   V   V   .     S   S   S   is linearly independent.   Dimension  Linear transformations and matrices  Change of coordinate matrix  Clique  Coordinate vector relative to a basis  Dimension theorem  Dominance relation  Identity matrix  Identity transformation  Incidence matrix  Inverse of a linear transformation  Inverse of a matrix  Invertible linear transformation  Isomorphic vector spaces  Isomorphism  Kronecker delta  Left-multiplication transformation  Linear operator  Linear transformation  Matrix representing a linear transformation  Nullity of a linear transformation  Null space  Ordered basis  Product of matrices  Projection on a subspace  Projection on the x-axis  Range  Rank of a linear transformation  Reflection about the x-axis  Rotation  Similar matrices Standard ordered basis for    F  n     subscript  F  n    F_{n}     Standard representation of a vector space with respect to a basis  Zero transformation  P.S. coefficient of the differential equation , differentiability of complex function ,vector space of functions differential operator , auxiliary polynomial , to the power of a complex number, exponential function .  Definition of a Linear Transformation  Let   V   V   V   and   W   W   W   be vector spaces. The function    T  :   V  ‚Üí  W      normal-:  T   normal-‚Üí  V  W     T:V\to W   is called a linear transformation of   V   V   V   into   W   W   W   if the following two properties are true for all    u  ‚Üí     normal-‚Üí  u    \vec{u}   and    v  ‚Üí     normal-‚Üí  v    \vec{v}   in   V   V   V   and for any scalar   c   c   c   .        T   (    u  ‚Üí   +   v  ‚Üí    )    =    T   (   u  ‚Üí   )    +   T   (   v  ‚Üí   )           T     normal-‚Üí  u    normal-‚Üí  v         T   normal-‚Üí  u      T   normal-‚Üí  v       T(\vec{u}+\vec{v})=T(\vec{u})+T(\vec{v})          T   (   c   u  ‚Üí    )    =   c  T   (   u  ‚Üí   )          T    c   normal-‚Üí  u       c  T   normal-‚Üí  u      T(c\vec{u})=cT(\vec{u})           \color   B  l  u   e   2.1      \color  B  l  u  e  2.1    {\color{Blue}~{}2.1}    N ( T ) and R ( T ) are subspaces  Let V and W be vector spaces and I : V ‚Üí W be linear. Then N ( T ) and R ( T ) are subspaces of V and W , respectively.  ===     \color   B  l  u   e   2.2      \color  B  l  u  e  2.2    {\color{Blue}~{}2.2}   R(T)= span of T(basis in V)=== Let V and W be vector spaces, and let T: V‚ÜíW be linear. If    Œ≤  =    v  1   ,   v  2   ,  ‚Ä¶  ,   v  n        Œ≤    subscript  v  1    subscript  v  2   normal-‚Ä¶   subscript  v  n      \beta={v_{1},v_{2},\ldots,v_{n}}   is a basis for V, then :     R   (  T  )    =   span   (   T   (  Œ≤  )    )    =   span   (   T   (   v  1   )    ,   T   (   v  2   )    ,  ‚Ä¶  ,   T   (   v  n   )    )            normal-R  normal-T     span    T  Œ≤           span     T   subscript  v  1      T   subscript  v  2    normal-‚Ä¶    T   subscript  v  n         \mathrm{R(T)}=\mathrm{span}(T(\beta\mathrm{))}=\mathrm{span}({T(v_{1}),T(v_{2}%
 ),\ldots,T(v_{n})})   .       \color   B  l  u   e   2.3      \color  B  l  u  e  2.3    {\color{Blue}~{}2.3}   Dimension theorem  Let V and W be vector spaces, and let T: V ‚Üí W be linear. If V is finite-dimensional, then :::::       nullity   (  T  )    +   rank   (  T  )     =   dim   (  V  )     .          nullity  T     rank  T     dimension  V     \mathrm{nullity}(T)+\mathrm{rank}(T)=\dim(V).     ===     \color   B  l  u   e   2.4      \color  B  l  u  e  2.4    {\color{Blue}~{}2.4}   one-to-one ‚áî N(T) = {0}=== Let    T  :   V  ‚Üí  W      normal-:  T   normal-‚Üí  V  W     T:V\to W   be a linear transformation. Then   T   T   T   is one-to-one if and only if     ker   (  T  )    =   {   0  ‚Üí   }        ker  T     normal-‚Üí  0      \operatorname{ker}(T)=\{\vec{0}\}   .  ===     \color   B  l  u   e   2.5      \color  B  l  u  e  2.5    {\color{Blue}~{}2.5}   one-to-one ‚áî onto ‚áî rank( T ) = dim( V )=== Let V and W be vector spaces of equal (finite) dimension, and let T : V ‚Üí W be linear. Then the following are equivalent. :(a) T is one-to-one.   (b) T is onto.  (c) rank( T ) = dim( V ).        \color   B  l  u   e   2.6      \color  B  l  u  e  2.6    {\color{Blue}~{}2.6}   ‚àÄ      w  1   ,   w  2   ,  ‚Ä¶  ,   w  n    =         subscript  w  1    subscript  w  2   normal-‚Ä¶   subscript  w  n    absent    {w_{1},w_{2},\ldots,w_{n}}=   exactly one T (basis),  Let V and W be vector space over F, and suppose that     v  1   ,   v  2   ,  ‚Ä¶  ,   v  n       subscript  v  1    subscript  v  2   normal-‚Ä¶   subscript  v  n     {v_{1},v_{2},\ldots,v_{n}}   is a basis for V. For     w  1   ,   w  2   ,  ‚Ä¶  ,   w  n       subscript  w  1    subscript  w  2   normal-‚Ä¶   subscript  w  n     w_{1},w_{2},\ldots,w_{n}   in W, there exists exactly one linear transformation T: V‚ÜíW such that     T   (   v  i   )    =   w  i         normal-T   subscript  v  i     subscript  w  i     \mathrm{T}(v_{i})=w_{i}   for     i  =   1  ,  2  ,  ‚Ä¶  ,  n    .      i   1  2  normal-‚Ä¶  n     i=1,2,\ldots,n.     Corollary. Let V and W be vector spaces, and suppose that V has a finite basis     v  1   ,   v  2   ,  ‚Ä¶  ,   v  n       subscript  v  1    subscript  v  2   normal-‚Ä¶   subscript  v  n     {v_{1},v_{2},\ldots,v_{n}}   . If U, T: V‚ÜíW are linear and     U   (   v  i   )    =   T   (   v  i   )          U   subscript  v  i      T   subscript  v  i      U(v_{i})=T(v_{i})   for     i  =   1  ,  2  ,  ‚Ä¶  ,  n    ,      i   1  2  normal-‚Ä¶  n     i=1,2,\ldots,n,   then U=T.       \color   B  l  u   e   2.7      \color  B  l  u  e  2.7    {\color{Blue}~{}2.7}   T is vector space  Let V and W be vector spaces over a field F, and let T, U: V‚ÜíW be linear. :(a) For all   a   a   a   ‚àà F ,     a  T   +  U        a  normal-T   normal-U    a\mathrm{T}+\mathrm{U}   is linear.   (b) Using the operations of addition and scalar multiplication in the preceding definition, the collection of all linear transformations form V to W is a vector space over F.        \color   B  l  u   e   2.8      \color  B  l  u  e  2.8    {\color{Blue}~{}2.8}   linearity of matrix representation of linear transformation  Let V and W be finite-dimensional vector spaces with ordered bases Œ≤ and Œ≥, respectively, and let T, U: V‚ÜíW be linear transformations. Then :(a)      [   T  +  U   ]   Œ≤  Œ≥   =     [  T  ]   Œ≤  Œ≥   +    [  U  ]   Œ≤  Œ≥         superscript   subscript   delimited-[]    T  U    Œ≤   Œ≥      superscript   subscript   delimited-[]  T   Œ≤   Œ≥    superscript   subscript   delimited-[]  U   Œ≤   Œ≥      [T+U]_{\beta}^{\gamma}=[T]_{\beta}^{\gamma}+[U]_{\beta}^{\gamma}   and   (b)      [   a  T   ]   Œ≤  Œ≥   =   a    [  T  ]   Œ≤  Œ≥         superscript   subscript   delimited-[]    a  T    Œ≤   Œ≥     a   superscript   subscript   delimited-[]  T   Œ≤   Œ≥      [aT]_{\beta}^{\gamma}=a[T]_{\beta}^{\gamma}   for all scalars   a   a   a   .        \color   B  l  u   e   2.9      \color  B  l  u  e  2.9    {\color{Blue}~{}2.9}   composition law of linear operators  Let V,W, and Z be vector spaces over the same field f, and let T:V‚ÜíW and U:W‚ÜíZ be linear. then UT:V‚ÜíZ is linear.       \color   B  l  u   e   2.10      \color  B  l  u  e  2.10    {\color{Blue}~{}2.10}   law of linear operator  Let v be a vector space. Let T, U 1 , U 2 ‚àà   ‚Ñí   ‚Ñí   \mathcal{L}   (V). Then (a) T(U 1 +U 2 )=TU 1 +TU 2 and (U 1 +U 2 )T=U 1 T+U 2 T (b) T(U 1 U 2 )=(TU 1 )U 2 (c) TI=IT=T (d)   a   a   a   (U 1 U 2 )=(   a   a   a   U 1 )U 2 =U 1 (   a   a   a   U 2 ) for all scalars   a   a   a   .  ===     \color   B  l  u   e   2.11      \color  B  l  u  e  2.11    {\color{Blue}~{}2.11}   [UT] Œ± Œ≥ =[U] Œ≤ Œ≥ [T] Œ± Œ≤ === Let V, W and Z be finite-dimensional vector spaces with ordered bases Œ± Œ≤ Œ≥, respectively. Let T: V‚áêW and U: W‚ÜíZ be linear transformations. Then ::::::      [   U  T   ]   Œ±  Œ≥   =     [  U  ]   Œ≤  Œ≥     [  T  ]   Œ±  Œ≤         superscript   subscript   delimited-[]    U  T    Œ±   Œ≥      superscript   subscript   delimited-[]  U   Œ≤   Œ≥    superscript   subscript   delimited-[]  T   Œ±   Œ≤      [UT]_{\alpha}^{\gamma}=[U]_{\beta}^{\gamma}[T]_{\alpha}^{\beta}   .  Corollary . Let V be a finite-dimensional vector space with an ordered basis Œ≤. Let T,U‚àà   ‚Ñí   ‚Ñí   \mathcal{L}   (V). Then [UT] Œ≤ =[U] Œ≤ [T] Œ≤ .       \color   B  l  u   e   2.12      \color  B  l  u  e  2.12    {\color{Blue}~{}2.12}   law of matrix  Let A be an m√ón matrix, B and C be n√óp matrices, and D and E be q√óm matrices. Then :(a) A(B+C)=AB+AC and (D+E)A=DA+EA.   (b)   a   a   a   (AB)=(   a   a   a   A)B=A(   a   a   a   B) for any scalar   a   a   a   .  (c) I m A=AI m .  (d) If V is an n-dimensional vector space with an ordered basis Œ≤, then [I v ] Œ≤ =I n .   Corollary. Let A be an m√ón matrix, B 1 ,B 2 ,...,B k be n√óp matrices, C 1 ,C 1 ,...,C 1 be q√óm matrices, and     a  1   ,   a  2   ,  ‚Ä¶  ,   a  k       subscript  a  1    subscript  a  2   normal-‚Ä¶   subscript  a  k     a_{1},a_{2},\ldots,a_{k}   be scalars. Then ::::::     A   (    ‚àë   i  =  1   k     a  i    B  i     )    =    ‚àë   i  =  1   k     a  i   A   B  i           A    superscript   subscript     i  1    k      subscript  a  i    subscript  B  i        superscript   subscript     i  1    k      subscript  a  i   A   subscript  B  i       A\Bigg(\sum_{i=1}^{k}a_{i}B_{i}\Bigg)=\sum_{i=1}^{k}a_{i}AB_{i}   and          (    ‚àë   i  =  1   k     a  i    C  i     )   A   =    ‚àë   i  =  1   k     a  i    C  i   A            superscript   subscript     i  1    k      subscript  a  i    subscript  C  i     A     superscript   subscript     i  1    k      subscript  a  i    subscript  C  i   A      \Bigg(\sum_{i=1}^{k}a_{i}C_{i}\Bigg)A=\sum_{i=1}^{k}a_{i}C_{i}A   .          \color   B  l  u   e   2.13      \color  B  l  u  e  2.13    {\color{Blue}~{}2.13}   law of column multiplication  Let A be an m√ón matrix and B be an n√óp matrix. For each    j   (  1  ‚â§  j  ‚â§  p  )      fragments  j   fragments  normal-(  1   j   p  normal-)     j(1\leq j\leq p)   let    u  j     subscript  u  j    u_{j}   and    v  j     subscript  v  j    v_{j}   denote the jth columns of AB and B, respectively. Then (a)     u  j   =   A   v  j         subscript  u  j     A   subscript  v  j      u_{j}=Av_{j}    (b)     v  j   =   B   e  j         subscript  v  j     B   subscript  e  j      v_{j}=Be_{j}   , where    e  j     subscript  e  j    e_{j}   is the jth standard vector of F p .  ===     \color   B  l  u   e   2.14      \color  B  l  u  e  2.14    {\color{Blue}~{}2.14}   [T(u)] Œ≥ =[T] Œ≤ Œ≥ [u] Œ≤ === Let V and W be finite-dimensional vector spaces having ordered bases Œ≤ and Œ≥, respectively, and let T: V‚ÜíW be linear. Then, for each u ‚àà V, we have :::::::      [   T   (  u  )    ]   Œ≥   =     [  T  ]   Œ≤  Œ≥     [  u  ]   Œ≤         subscript   delimited-[]    T  u    Œ≥      superscript   subscript   delimited-[]  T   Œ≤   Œ≥    subscript   delimited-[]  u   Œ≤      [T(u)]_{\gamma}=[T]_{\beta}^{\gamma}[u]_{\beta}   .       \color   B  l  u   e   2.15      \color  B  l  u  e  2.15    {\color{Blue}~{}2.15}   laws of L A  Let A be an m√ón matrix with entries from F. Then the left-multiplication transformation L A : F n ‚ÜíF m is linear. Furthermore, if B is any other m√ón matrix (with entries from F) and Œ≤ and Œ≥ are the standard ordered bases for F n and F m , respectively, then we have the following properties. (a)      [   L  A   ]   Œ≤  Œ≥   =  A       superscript   subscript   delimited-[]   subscript  L  A    Œ≤   Œ≥   A    [L_{A}]_{\beta}^{\gamma}=A   . (b) L A =L B if and only if A=B. (c) L A+B =L A +L B and L    a   a   a   A =   a   a   a   L A for all   a   a   a   ‚ààF. (d) If T:F n ‚ÜíF m is linear, then there exists a unique m√ón matrix C such that T=L C . In fact,    C  =    [   L  A   ]   Œ≤  Œ≥       normal-C   superscript   subscript   delimited-[]   subscript  L  A    Œ≤   Œ≥     \mathrm{C}=[L_{A}]_{\beta}^{\gamma}   . (e) If W is an n√óp matrix, then L AE =L A L E . (f ) If m=n, then     L   I  n    =   I   F  n         subscript  L   subscript  I  n     subscript  I   superscript  F  n      L_{I_{n}}=I_{F^{n}}   .  ===     \color   B  l  u   e   2.16      \color  B  l  u  e  2.16    {\color{Blue}~{}2.16}   A(BC)=(AB)C=== Let A,B, and C be matrices such that A(BC) is defined. Then A(BC)=(AB)C; that is, matrix multiplication is associative.       \color   B  l  u   e   2.17      \color  B  l  u  e  2.17    {\color{Blue}~{}2.17}   T ‚àí1 is linear  Let V and W be vector spaces, and let T:V‚ÜíW be linear and invertible. Then T ‚àí1 : W ‚ÜíV is linear.  ===     \color   B  l  u   e   2.18      \color  B  l  u  e  2.18    {\color{Blue}~{}2.18}   [T ‚àí1 ] Œ≥ Œ≤ =([T] Œ≤ Œ≥ ) ‚àí1 === Let V and W be finite-dimensional vector spaces with ordered bases Œ≤ and Œ≥, respectively. Let T:V‚ÜíW be linear. Then T is invertible if and only if     [  T  ]   Œ≤  Œ≥     superscript   subscript   delimited-[]  T   Œ≤   Œ≥    [T]_{\beta}^{\gamma}   is invertible. Furthermore,      [   T   -  1    ]   Œ≥  Œ≤   =    (    [  T  ]   Œ≤  Œ≥   )    -  1         superscript   subscript   delimited-[]   superscript  T    1     Œ≥   Œ≤    superscript   superscript   subscript   delimited-[]  T   Œ≤   Œ≥     1      [T^{-1}]_{\gamma}^{\beta}=([T]_{\beta}^{\gamma})^{-1}     Lemma. Let T be an invertible linear transformation from V to W. Then V is finite-dimensional if and only if W is finite-dimensional. In this case, dim(V)=dim(W).  Corollary 1. Let V be a finite-dimensional vector space with an ordered basis Œ≤, and let T:V‚ÜíV be linear. Then T is invertible if and only if [T] Œ≤ is invertible. Furthermore, [T ‚àí1 ] Œ≤ =([T] Œ≤ ) ‚àí1 .  Corollary 2. Let A be an n√ón matrix. Then A is invertible if and only if L A is invertible. Furthermore, (L A ) ‚àí1 =L A ‚àí1 .  ===     \color   B  l  u   e   2.19      \color  B  l  u  e  2.19    {\color{Blue}~{}2.19}   V is isomorphic to W ‚áî dim(V)=dim(W)=== Let W and W be finite-dimensional vector spaces (over the same field). Then V is isomorphic to W if and only if dim(V)=dim(W).  Corollary. Let V be a vector space over F. Then V is isomorphic to F n if and only if dim(V)=n.       \color   B  l  u   e   2.20      \color  B  l  u  e  2.20    {\color{Blue}~{}2.20}   ??  Let V and W be finite-dimensional vector spaces over F of dimensions n and m, respectively, and let Œ≤ and Œ≥ be ordered bases for V and W, respectively. Then the function    Œ¶    normal-Œ¶   ~{}\Phi      ‚Ñí   ‚Ñí   \mathcal{L}   (V,W)‚ÜíM m√ón (F), defined by      Œ¶    (  T  )    =    [  T  ]   Œ≤  Œ≥         normal-Œ¶  T    superscript   subscript   delimited-[]  T   Œ≤   Œ≥     ~{}\Phi(T)=[T]_{\beta}^{\gamma}   for T‚àà   ‚Ñí   ‚Ñí   \mathcal{L}   (V,W), is an isomorphism.  Corollary. Let V and W be finite-dimensional vector spaces of dimension n and m, respectively. Then   ‚Ñí   ‚Ñí   \mathcal{L}   (V,W) is finite-dimensional of dimension mn.       \color   B  l  u   e   2.21      \color  B  l  u  e  2.21    {\color{Blue}~{}2.21}    Œ¶ Œ≤ is an isomorphism  For any finite-dimensional vector space V with ordered basis Œ≤, Œ¶ Œ≤ is an isomorphism.       \color   B  l  u   e   2.22      \color  B  l  u  e  2.22    {\color{Blue}~{}2.22}   ??  Let Œ≤ and Œ≤' be two ordered bases for a finite-dimensional vector space V, and let    Q  =    [   I  V   ]    Œ≤  ‚Ä≤   Œ≤       Q   superscript   subscript   delimited-[]   subscript  I  V     superscript  Œ≤  normal-‚Ä≤    Œ≤     Q=[I_{V}]_{\beta^{\prime}}^{\beta}   . Then (a)   Q   Q   Q   is invertible. (b) For any    v  ‚àà       v  absent    v\in   V,      [  v  ]   Œ≤   =   Q    [  v  ]    Œ≤  ‚Ä≤          subscript   delimited-[]  v   Œ≤     Q   subscript   delimited-[]  v    superscript  Œ≤  normal-‚Ä≤       ~{}[v]_{\beta}=Q[v]_{\beta^{\prime}}   .  ===     \color   B  l  u   e   2.23      \color  B  l  u  e  2.23    {\color{Blue}~{}2.23}   [T] Œ≤' =Q ‚àí1 [T] Œ≤ Q=== Let T be a linear operator on a finite-dimensional vector space V,and let Œ≤ and Œ≤' be two ordered bases for V. Suppose that Q is the change of coordinate matrix that changes Œ≤'-coordinates into Œ≤-coordinates. Then ::::::      [  T  ]    Œ≤  ‚Ä≤    =    Q   -  1      [  T  ]   Œ≤   Q        subscript   delimited-[]  T    superscript  Œ≤  normal-‚Ä≤       superscript  Q    1     subscript   delimited-[]  T   Œ≤   Q     ~{}[T]_{\beta^{\prime}}=Q^{-1}[T]_{\beta}Q   .  Corollary. Let A‚ààM n√ón ( F ), and le t Œ≥ be an ordered basis for F n . Then [L A ] Œ≥ =Q ‚àí1 AQ, where Q is the n√ón matrix whose jth column is the jth vector of Œ≥.  Principal Axes Theorem  For a conic whose equation is      a   x  2    +   b  x  y   +   c   y  2    +   d  x   +   e  y   +  f   =  0          a   superscript  x  2      b  x  y     c   superscript  y  2      d  x     e  y   f   0    ax^{2}+bxy+cy^{2}+dx+ey+f=0   , the rotation given by    X  =   P   X  ‚Ä≤        X    P   superscript  X  normal-‚Ä≤      X=PX^{\prime}   eliminates the    x  y      x  y    xy   -term if   P   P   P   is an orthogonal matrix, with     |  P  |   =  1        P   1    \left|P\right|=1   , that diagonalizes   A   A   A   . That is,        P  T   A  P   =   [      Œª  1     0      0     Œª  2      ]          superscript  P  T   A  P      subscript  Œª  1   0    0   subscript  Œª  2       P^{T}AP=\begin{bmatrix}\lambda_{1}&0\\
 0&\lambda_{2}\end{bmatrix}   , where    Œª  1     subscript  Œª  1    \lambda_{1}   and    Œª  2     subscript  Œª  2    \lambda_{2}   are eigenvalues of   A   A   A   . The equation of the rotated conic is given by         Œª  1     (   x  ‚Ä≤   )   2    +    Œª  2     (   y  ‚Ä≤   )   2    +    [     d    e     ]   P   X  ‚Ä≤    +  f   =  0           subscript  Œª  1    superscript   superscript  x  normal-‚Ä≤   2       subscript  Œª  2    superscript   superscript  y  normal-‚Ä≤   2        d  e    P   superscript  X  normal-‚Ä≤    f   0    \lambda_{1}(x^{\prime})^{2}+\lambda_{2}(y^{\prime})^{2}+\begin{bmatrix}d&e\\
 \end{bmatrix}PX^{\prime}+f=0   .       \color   B  l  u   e   2.26      \color  B  l  u  e  2.26    {\color{Blue}~{}2.26}          \color   B  l  u   e   2.26      \color  B  l  u  e  2.26    {\color{Blue}~{}2.26}     ===     \color   B  l  u   e   2.27      \color  B  l  u  e  2.27    {\color{Blue}~{}2.27}    p (D)(x)=0 ( p (D)‚ààC ‚àû )‚áí x (k) exists (k‚ààN)=== Any solution to a homogeneous linear differential equation with constant coefficients has derivatives of all orders; that is, if   x   x   x   is a solution to such an equation, then    x   (  k  )      superscript  x  k    x^{(k)}   exists for every positive integer k.  ===     \color   B  l  u   e   2.28      \color  B  l  u  e  2.28    {\color{Blue}~{}2.28}   {solutions}= N(p(D))=== The set of all solutions to a homogeneous linear differential equation with constant coefficients coincides with the null space of p(D), where p(t) is the auxiliary polynomial with the equation.  Corollary . The set of all solutions to s homogeneous linear differential equation with constant coefficients is a subspace of    C  ‚àû     superscript  normal-C     \mathrm{C}^{\infty}   .       \color   B  l  u   e   2.29      \color  B  l  u  e  2.29    {\color{Blue}~{}2.29}   derivative of exponential function  For any exponential function      f   (  t  )    =   e   c  t     ,     f  ‚Ä≤    (  t  )    =   c   e   c  t         formulae-sequence      f  t    superscript  e    c  t          superscript  f  normal-‚Ä≤   t     c   superscript  e    c  t        f(t)=e^{ct},f^{\prime}(t)=ce^{ct}   .       \color   B  l  u   e   2.30      \color  B  l  u  e  2.30    {\color{Blue}~{}2.30}   {e ‚àíat } is a basis of N( p (D+aI))  The solution space for the differential equation, :::      y  ‚Ä≤   +    a  0   y    =  0         superscript  y  normal-‚Ä≤      subscript  a  0   y    0    y^{\prime}+a_{0}y=0   is of dimension 1 and has    {   e   -    a  0   t     }      superscript  e       subscript  a  0   t       \{e^{-a_{0}t}\}   as a basis.  Corollary. For any complex number c, the null space of the differential operator D-cI has {    e   c  t      superscript  e    c  t     e^{ct}   } as a basis.       \color   B  l  u   e   2.31      \color  B  l  u  e  2.31    {\color{Blue}~{}2.31}       e   c  t      superscript  e    c  t     e^{ct}   is a solution  Let p(t) be the auxiliary polynomial for a homogeneous linear differential equation with constant coefficients. For any complex number c, if c is a zero of p(t), then to the differential equation.  ===     \color   B  l  u   e   2.32      \color  B  l  u  e  2.32    {\color{Blue}~{}2.32}   dim(N( p (D)))=n=== For any differential operator p(D) of order n, the null space of p(D) is an n_dimensional subspace of C ‚àû .  Lemma 1 . The differential operator D-cI: C ‚àû to C ‚àû is onto for any complex number c.  Lemma 2 Let V be a vector space, and suppose that T and U are linear operators on V such that U is onto and the null spaces of T and U are finite-dimensional, Then the null space of TU is finite-dimensional, and :::::dim(N(TU))=dim(N(U))+dim(N(U)).  Corollary . The solution space of any nth-order homogeneous linear differential equation with constant coefficients is an n-dimensional subspace of C ‚àû .       \color   B  l  u   e   2.33      \color  B  l  u  e  2.33    {\color{Blue}~{}2.33}   e c i t is linearly independent with each other (c i are distinct)  Given n distinct complex numbers     c  1   ,   c  2   ,  ‚Ä¶  ,   c  n       subscript  c  1    subscript  c  2   normal-‚Ä¶   subscript  c  n     c_{1},c_{2},\ldots,c_{n}   , the set of exponential functions    {   e    c  1   t    ,   e    c  2   t    ,  ‚Ä¶  ,   e    c  n   t    }      superscript  e     subscript  c  1   t     superscript  e     subscript  c  2   t    normal-‚Ä¶   superscript  e     subscript  c  n   t      \{e^{c_{1}t},e^{c_{2}t},\ldots,e^{c_{n}t}\}   is linearly independent.  Corollary . For any nth-order homogeneous linear differential equation with constant coefficients, if the auxiliary polynomial has n distinct zeros     c  1   ,   c  2   ,  ‚Ä¶  ,   c  n       subscript  c  1    subscript  c  2   normal-‚Ä¶   subscript  c  n     c_{1},c_{2},\ldots,c_{n}   , then    {   e    c  1   t    ,   e    c  2   t    ,  ‚Ä¶  ,   e    c  n   t    }      superscript  e     subscript  c  1   t     superscript  e     subscript  c  2   t    normal-‚Ä¶   superscript  e     subscript  c  n   t      \{e^{c_{1}t},e^{c_{2}t},\ldots,e^{c_{n}t}\}   is a basis for the solution space of the differential equation.  Lemma . For a given complex number c and positive integer n, suppose that (t-c)^n is athe auxiliary polynomial of a homogeneous linear differential equation with constant coefficients. Then the set ::    Œ≤  =   {   e    c  1   t    ,   e    c  2   t    ,  ‚Ä¶  ,   e    c  n   t    }       Œ≤    superscript  e     subscript  c  1   t     superscript  e     subscript  c  2   t    normal-‚Ä¶   superscript  e     subscript  c  n   t       \beta=\{e^{c_{1}t},e^{c_{2}t},\ldots,e^{c_{n}t}\}   is a basis for the solution space of the equation.       \color   B  l  u   e   2.34      \color  B  l  u  e  2.34    {\color{Blue}~{}2.34}   general solution of homogeneous linear differential equation  Given a homogeneous linear differential equation with constant coefficients and auxiliary polynomial ::::       (   t  -   c  1    )   1  n     (   t  -   c  2    )   2  n   ‚ãØ    (   t  -   c  k    )   k  n    ,       subscript   superscript    t   subscript  c  1    n   1    subscript   superscript    t   subscript  c  2    n   2   normal-‚ãØ   subscript   superscript    t   subscript  c  k    n   k     (t-c_{1})^{n}_{1}(t-c_{2})^{n}_{2}\cdots(t-c_{k})^{n}_{k},   where     n  1   ,   n  2   ,  ‚Ä¶  ,   n  k       subscript  n  1    subscript  n  2   normal-‚Ä¶   subscript  n  k     n_{1},n_{2},\ldots,n_{k}   are positive integers and     c  1   ,   c  2   ,  ‚Ä¶  ,   c  n       subscript  c  1    subscript  c  2   normal-‚Ä¶   subscript  c  n     c_{1},c_{2},\ldots,c_{n}   are distinct complex numbers, the following set is a basis for the solution space of the equation: ::    {   e    c  1   t    ,   t   e    c  1   t     ,  ‚Ä¶  ,    t    n  1   -  1     e    c  1   t     ,  ‚Ä¶  ,   e   c  k   t   ,   t   e    c  k   t     ,  ‚Ä¶  ,    t    n  k   -  1     e    c  k   t     }      superscript  e     subscript  c  1   t      t   superscript  e     subscript  c  1   t     normal-‚Ä¶     superscript  t     subscript  n  1   1     superscript  e     subscript  c  1   t     normal-‚Ä¶    e   subscript  c  k   t     t   superscript  e     subscript  c  k   t     normal-‚Ä¶     superscript  t     subscript  n  k   1     superscript  e     subscript  c  k   t       \{e^{c_{1}t},te^{c_{1}t},\ldots,t^{n_{1}-1}e^{c_{1}t},\ldots,e{c_{k}t},te^{c_{%
 k}t},\ldots,t^{n_{k}-1}e^{c_{k}t}\}   .  Definition of an Orthogonal Matrix  A square matrix   P   P   P   is called orthogonal if it is invertible and if       P   -  1    =   P  T        superscript  P    1     superscript  P  T     P^{-1}=P^{T}   .  Real Spectral Theorem  If   A   A   A   is an    n  √ó  n      n  n    n\times n   symmetric matrix, then the following properties are true:      A   A   A   is diagonalizable.  All eigenvalues of   A   A   A   are real.  If   Œª   Œª   \lambda   is an eigenvalue of   A   A   A   with multiplicity   k   k   k   , then   Œª   Œª   \lambda   has   k   k   k   linearly independent eigenvectors. That is, the eigenspace of   Œª   Œª   \lambda   has dimension   k   k   k   .   Also, the set of eigenvalues of   A   A   A   is called the spectrum of   A   A   A   .  Elementary matrix operations and systems of linear equations  Elementary matrix operations  The three elementary row operations are the following:   Interchange two rows.  Multiply a row by a nonzero constant.  Add a multiple of a row to another row.   Elementary matrix  An    n  √ó  n      n  n    n\times n   matrix is called an elementary matrix if it can be obtained from the identity matrix    I  n     subscript  I  n    I_{n}   by a single elementary row operation.  Rank of a matrix  The rank of a matrix A is the number of pivot columns after the reduced row echelon form of A.  Invertible Matrices      If  A  is  n  √ó  n  , then the following statements are equivalent:      If  A  is  n  √ó  n  , then the following statements are equivalent:    \text{If }A\text{ is }n\text{ √ó }n\text{, then the following statements are %
 equivalent:}          A  is invertible.      A  is invertible.    A\text{ is invertible.}          A   x  ‚Üí    =      b  ‚Üí   has a unique solution for every  n   √ó  1   column matrix   b  ‚Üí   .         A   normal-‚Üí  x           normal-‚Üí  b   has a unique solution for every  n   1   column matrix   normal-‚Üí  b   .     A\vec{x}=\vec{b}\text{ has a unique solution for every }n\times 1\text{ column%
  matrix }\vec{b}\text{.}          A   x  ‚Üí    =    0  ‚Üí   has only the trivial solution.         A   normal-‚Üí  x       normal-‚Üí  0   has only the trivial solution.     A\vec{x}=\vec{0}\text{ has only the trivial solution.}         A  is row-equivalent to   I  n   .      A  is row-equivalent to   subscript  I  n   .    A\text{ is row-equivalent to }I_{n}\text{.}         A  can be written as the product of elementary matrices.      A  can be written as the product of elementary matrices.    A\text{ can be written as the product of elementary matrices.}          det   (  A  )    ‚â†  0        A   0    \det(A)\neq 0          rk   (  A  )    =   n  number of columns.        rk  A     n  number of columns.     \operatorname{rk}(A)=n\text{ number of columns.}          nul   (  A  )    =  0       nul  A   0    \operatorname{nul}(A)=0         All of the  n  -row vectors of  A  are linearly independent.      All of the  n  -row vectors of  A  are linearly independent.    \text{All of the }n\text{-row vectors of }A\text{ are linearly independent.}         All of the  n  -column vectors of  A  are linearly independent.      All of the  n  -column vectors of  A  are linearly independent.    \text{All of the }n\text{-column vectors of }A\text{ are linearly independent.}      System of linear equations  Determinants  If    A = \begin{pmatrix}     a & b \\ c & d \\ \end{pmatrix} is a 2√ó2'' matrix with entries form a field F, then we define the determinant of A, denoted det( A ) or |A|, to be the scalar     a  d   -   b  c         a  d     b  c     ad-bc   .''  ÔºäTheorem 1: linear function for a single row. ÔºäTheorem 2: nonzero determinant ‚áî invertible matrix  Theorem 1: '' The function det: M 2√ó2 ( F ) ‚Üí F is a linear function of each row of a 2√ó2 matrix when the other row is held fixed. That is, if     u  ,  v   ,     u  v    u,v,   and   w   w   w   are in F ¬≤ and   k   k   k   is a scalar, then''    \det\begin{pmatrix}     u + kv\\ w\\ \end{pmatrix} =\det\begin{pmatrix} u\\ w\\ \end{pmatrix} + k\det\begin{pmatrix} v\\ w\\ \end{pmatrix}  and    \det\begin{pmatrix}     w\\ u + kv\\ \end{pmatrix} =\det\begin{pmatrix} w\\ u\\ \end{pmatrix} + k\det\begin{pmatrix} w\\ v\\ \end{pmatrix}  Theorem 2 : ''Let A   ‚àà     \in    M 2√ó2 ( F ) . Then thee deter minant of A is nonzero if and only if A is invertible. Moreover, if A is invertible, then'' :::::::     A   -  1    =    1   det   (  A  )      (      A  22      -   A  12         -   A  21       A   ;  11       )         superscript  A    1        1    A       subscript  A  22      subscript  A  12         subscript  A  21     fragments  A   subscript  normal-;  11         A^{-1}=\frac{1}{\det(A)}\begin{pmatrix}A_{22}&-A_{12}\\
 -A_{21}&A_{11}\\
 \end{pmatrix}     Diagonalization  Characteristic polynomial of a linear operator/matrix       \color   B  l  u   e   5.1      \color  B  l  u  e  5.1    {\color{Blue}~{}5.1}   diagonalizable‚áîbasis of eigenvector  A linear operator T on a finite-dimensional vector space V is diagonalizable if and only if there exists an ordered basis Œ≤ for V consisting of eigenvectors of T. Furthermore, if T is diagonalizable,    Œ≤  =    v  1   ,   v  2   ,  ‚Ä¶  ,   v  n        Œ≤    subscript  v  1    subscript  v  2   normal-‚Ä¶   subscript  v  n      \beta={v_{1},v_{2},\ldots,v_{n}}   is an ordered basis of eigenvectors of T, and D = [T] Œ≤ then D is a diagonal matrix and    D   j  j      subscript  D    j  j     D_{jj}   is the eigenvalue corresponding to    v  j     subscript  v  j    v_{j}   for    1  ‚â§  j  ‚â§  n        1  j       n     1\leq j\leq n   .  ===     \color   B  l  u   e   5.2      \color  B  l  u  e  5.2    {\color{Blue}~{}5.2}   eigenvalue‚áîdet( A -Œª I n)=0=== Let A ‚ààM n√ón ( F ). Then a scalar Œª is an eigenvalue of A if and only if det( A -Œª I n )=0       \color   B  l  u   e   5.3      \color  B  l  u  e  5.3    {\color{Blue}~{}5.3}   characteristic polynomial  Let A‚ààMn√ón( F ). (a) The characteristic polynomial of A is a polynomial of degree n with leading coefficient(-1)n. (b) A has at most n distinct eigenvalues.       \color   B  l  u   e   5.4      \color  B  l  u  e  5.4    {\color{Blue}~{}5.4}   œÖ to Œª‚áîœÖ‚ààN(T-ŒªI)  Let T be a linear operator on a vector space V, and let Œª be an eigenvalue of T. A vector œÖ‚ààV is an eigenvector of T corresponding to Œª if and only if œÖ‚â†0 and œÖ‚ààN(T-ŒªI).       \color   B  l  u   e   5.5      \color  B  l  u  e  5.5    {\color{Blue}~{}5.5}   vi to Œªi‚áîvi is linearly independent  Let T be a linear operator on a vector space V, and let      Œª  1   ,   Œª  2   ,  ‚Ä¶  ,   Œª  k    ,      subscript  Œª  1    subscript  Œª  2   normal-‚Ä¶   subscript  Œª  k     \lambda_{1},\lambda_{2},\ldots,\lambda_{k},   be distinct eigenvalues of T. If     v  1   ,   v  2   ,  ‚Ä¶  ,   v  k       subscript  v  1    subscript  v  2   normal-‚Ä¶   subscript  v  k     v_{1},v_{2},\ldots,v_{k}   are eigenvectors of t such that    Œª  i     subscript  Œª  i    \lambda_{i}   corresponds to    v  i     subscript  v  i    v_{i}   (    1  ‚â§  i  ‚â§  k        1  i       k     1\leq i\leq k   ), then {     v  1   ,   v  2   ,  ‚Ä¶  ,   v  k       subscript  v  1    subscript  v  2   normal-‚Ä¶   subscript  v  k     v_{1},v_{2},\ldots,v_{k}   } is linearly independent.       \color   B  l  u   e   5.6      \color  B  l  u  e  5.6    {\color{Blue}~{}5.6}   characteristic polynomial splits  The characteristic polynomial of any diagonalizable linear operator splits.       \color   B  l  u   e   5.7      \color  B  l  u  e  5.7    {\color{Blue}~{}5.7}   1 ‚â§ dim(E Œª ) ‚â§ m  Let T be alinear operator on a finite-dimensional vectorspace V, and let Œª be an eigenvalue of T having multiplicity   m   m   m   . Then    1  ‚â§   dim   (   E  Œª   )    ‚â§  m        1   dimension   subscript  E  Œª         m     1\leq\dim(E_{\lambda})\leq m   .  ===     \color   B  l  u   e   5.8      \color  B  l  u  e  5.8    {\color{Blue}~{}5.8}    S = S 1 ‚à™ S 2 ‚à™ ...‚à™ S k is linearly independent=== Let T be a linear operator on a vector space V, and let      Œª  1   ,   Œª  2   ,  ‚Ä¶  ,   Œª  k    ,      subscript  Œª  1    subscript  Œª  2   normal-‚Ä¶   subscript  Œª  k     \lambda_{1},\lambda_{2},\ldots,\lambda_{k},   be distinct eigenvalues of T. For each     i  =   1  ,  2  ,  ‚Ä¶  ,  k    ,      i   1  2  normal-‚Ä¶  k     i=1,2,\ldots,k,   let    S  i     subscript  S  i    S_{i}   be a finite linearly independent subset of the eigenspace    E   Œª  i      subscript  E   subscript  Œª  i     E_{\lambda_{i}}   . Then    S  =    S  1   ‚à™   S  2   ‚à™  ‚ãØ  ‚à™   S  k        S     subscript  S  1    subscript  S  2   normal-‚ãØ   subscript  S  k      S=S_{1}\cup S_{2}\cup\cdots\cup S_{k}   is a linearly independent subset of V.       \color   B  l  u   e   5.9      \color  B  l  u  e  5.9    {\color{Blue}~{}5.9}   ‚áîT is diagonalizable  Let T be a linear operator on a finite-dimensional vector space V that the characteristic polynomial of T splits. Let     Œª  1   ,   Œª  2   ,  ‚Ä¶  ,   Œª  k       subscript  Œª  1    subscript  Œª  2   normal-‚Ä¶   subscript  Œª  k     \lambda_{1},\lambda_{2},\ldots,\lambda_{k}   be the distinct eigenvalues of T. Then (a) T is diagonalizable if and only if the multiplicity of    Œª  i     subscript  Œª  i    \lambda_{i}   is equal to    dim   (   E   Œª  i    )      dimension   subscript  E   subscript  Œª  i      \dim(E_{\lambda_{i}})   for all   i   i   i   . (b) If T is diagonalizable and    Œ≤  i     subscript  Œ≤  i    \beta_{i}   is an ordered basis for    E   Œª  i      subscript  E   subscript  Œª  i     E_{\lambda_{i}}   for each   i   i   i   , then    Œ≤  =   Œ≤  1   ‚à™   Œ≤  2   ‚à™  ‚à™   Œ≤  k      fragments  Œ≤    subscript  Œ≤  1     subscript  Œ≤  2      subscript  Œ≤  k     \beta=\beta_{1}\cup\beta_{2}\cup\cup\beta_{k}   is an ordered    b  a  s  i   s  2       b  a  s  i   superscript  s  2     basis^{2}   for V consisting of eigenvectors of T.  Test for diagonlization  Inner product spaces  Inner product , standard inner product on F n , conjugate transpose , adjoint , Frobenius inner product , complex/real inner product space , norm , length , conjugate linear , orthogonal , perpendicular , orthogonal , unit vector , orthonormal , normalization .       \color   B  l  u   e   6.1      \color  B  l  u  e  6.1    {\color{Blue}~{}6.1}   properties of linear product  Let V be an inner product space. Then for x,y,z\in V and c \in f, the following staements are true. (a)      ‚ü®  x  ,   y  +  z   ‚ü©   =    ‚ü®  x  ,  y  ‚ü©   +   ‚ü®  x  ,  z  ‚ü©     .       x    y  z       x  y    x  z      \langle x,y+z\rangle=\langle x,y\rangle+\langle x,z\rangle.    (b)      ‚ü®  x  ,   c  y   ‚ü©   =    c  ¬Ø    ‚ü®  x  ,  y  ‚ü©     .       x    c  y       normal-¬Ø  c    x  y      \langle x,cy\rangle=\bar{c}\langle x,y\rangle.    (c)     ‚ü®  x  ,  0  ‚ü©   =   ‚ü®  0  ,  x  ‚ü©   =  0.         x  0    0  x        0.     \langle x,\mathit{0}\rangle=\langle\mathit{0},x\rangle=0.    (d)     ‚ü®  x  ,  x  ‚ü©   =  0       x  x   0    \langle x,x\rangle=0   if and only if    x  =  0.      x  0.    x=\mathit{0}.    (e) If     ‚ü®  x  ,  y  ‚ü©   =   ‚ü®  x  ,  z  ‚ü©        x  y    x  z     \langle x,y\rangle=\langle x,z\rangle   for all    x  ‚àà       x  absent    x\in   V, then    y  =  z      y  z    y=z   .       \color   B  l  u   e   6.2      \color  B  l  u  e  6.2    {\color{Blue}~{}6.2}   law of norm  Let V be an inner product space over F. Then for all x,y\in V and c\in F, the following statements are true. (a)     ‚à•   c  x   ‚à•   =    |  c  |   ‚ãÖ   ‚à•  x  ‚à•         norm    c  x     normal-‚ãÖ    c    norm  x      \|cx\|=|c|\cdot\|x\|   . (b)     ‚à•  x  ‚à•   =  0       norm  x   0    \|x\|=0   if and only if    x  =  0      x  0    x=0   . In any case,     ‚à•  x  ‚à•   ‚â•  0       norm  x   0    \|x\|\geq 0   . (c)( Cauchy-Schwarz In equality )     |   ‚ü®  x  ,  y  ‚ü©   |   ‚â§    ‚à•  x  ‚à•   ‚ãÖ   ‚à•  y  ‚à•           x  y     normal-‚ãÖ   norm  x    norm  y      |\langle x,y\rangle|\leq\|x\|\cdot\|y\|   . (d)( Triangle Inequality )     ‚à•   x  +  y   ‚à•   ‚â§    ‚à•  x  ‚à•   +   ‚à•  y  ‚à•         norm    x  y       norm  x    norm  y      \|x+y\|\leq\|x\|+\|y\|   .  orthonormal basis , Gram‚ÄìSchmidt process , Fourier coefficients , orthogonal complement , orthogonal projection       \color   B  l  u   e   6.3      \color  B  l  u  e  6.3    {\color{Blue}~{}6.3}   span of orthogonal subset  Let V be an inner product space and    S  =   {   v  1   ,   v  2   ,  ‚Ä¶  ,   v  k   }       S    subscript  v  1    subscript  v  2   normal-‚Ä¶   subscript  v  k      S=\{v_{1},v_{2},\ldots,v_{k}\}   be an orthogonal subset of V consisting of nonzero vectors. If   y   y   y   ‚ààspan(S), then :::::    y  =    ‚àë   i  =  1   n      ‚ü®  y  ,   v  i   ‚ü©     ‚à•   v  i   ‚à•   2     v  i         y    superscript   subscript     i  1    n        y   subscript  v  i     superscript   norm   subscript  v  i    2     subscript  v  i       y=\sum_{i=1}^{n}{\langle y,v_{i}\rangle\over\|v_{i}\|^{2}}v_{i}          \color   B  l  u   e   6.4      \color  B  l  u  e  6.4    {\color{Blue}~{}6.4}   Gram-Schmidt process  Let V be an inner product space and S=    {   w  1   ,   w  2   ,  ‚Ä¶  ,   w  n   }      subscript  w  1    subscript  w  2   normal-‚Ä¶   subscript  w  n     \{w_{1},w_{2},\ldots,w_{n}\}   be a linearly independent subset of V. DefineS'=    {   v  1   ,   v  2   ,  ‚Ä¶  ,   v  n   }      subscript  v  1    subscript  v  2   normal-‚Ä¶   subscript  v  n     \{v_{1},v_{2},\ldots,v_{n}\}   , where     v  1   =   w  1        subscript  v  1    subscript  w  1     v_{1}=w_{1}   and :::::     v  k   =    w  k   -    ‚àë   j  =  1    k  -  1       ‚ü®   w  k   ,   v  j   ‚ü©     ‚à•   v  j   ‚à•   2     v  j           subscript  v  k      subscript  w  k     superscript   subscript     j  1      k  1          subscript  w  k    subscript  v  j     superscript   norm   subscript  v  j    2     subscript  v  j        v_{k}=w_{k}-\sum_{j=1}^{k-1}{\langle w_{k},v_{j}\rangle\over\|v_{j}\|^{2}}v_{j}   Then S' is an orhtogonal set of nonzero vectors such that span(S')=span(S).       \color   B  l  u   e   6.5      \color  B  l  u  e  6.5    {\color{Blue}~{}6.5}   orthonormal basis  Let V be a nonzero finite-dimensional inner product space. Then V has an orthonormal basis Œ≤. Furthermore, if Œ≤ =    {   v  1   ,   v  2   ,  ‚Ä¶  ,   v  n   }      subscript  v  1    subscript  v  2   normal-‚Ä¶   subscript  v  n     \{v_{1},v_{2},\ldots,v_{n}\}   and x‚ààV, then :::::    x  =    ‚àë   i  =  1   n     ‚ü®  x  ,   v  i   ‚ü©    v  i         x    superscript   subscript     i  1    n      x   subscript  v  i     subscript  v  i       x=\sum_{i=1}^{n}\langle x,v_{i}\rangle v_{i}   .  Corollary. Let V be a finite-dimensional inner product space with an orthonormal basis Œ≤ =    {   v  1   ,   v  2   ,  ‚Ä¶  ,   v  n   }      subscript  v  1    subscript  v  2   normal-‚Ä¶   subscript  v  n     \{v_{1},v_{2},\ldots,v_{n}\}   . Let T be a linear operator on V, and let A=[T] Œ≤ . Then for any   i   i   i   and   j   j   j   ,     A   i  j    =   ‚ü®   T   (   v  j   )    ,   v  i   ‚ü©        subscript  A    i  j       T   subscript  v  j     subscript  v  i      A_{ij}=\langle T(v_{j}),v_{i}\rangle   .       \color   B  l  u   e   6.6      \color  B  l  u  e  6.6    {\color{Blue}~{}6.6}   W ‚ä• by orthonormal basis  Let W be a finite-dimensional subspace of an inner product space V, and let   y   y   y   ‚ààV. Then there exist unique vectors   u   u   u   ‚ààW and   z   z   z   ‚ààW ‚ä• such that    y  =   u  +  z       y    u  z     y=u+z   . Furthermore, if    {   v  1   ,   v  2   ,  ‚Ä¶  ,   v  k   }      subscript  v  1    subscript  v  2   normal-‚Ä¶   subscript  v  k     \{v_{1},v_{2},\ldots,v_{k}\}   is an orthornormal basis for W, then :::::    u  =    ‚àë   i  =  1   k     ‚ü®  y  ,   v  i   ‚ü©    v  i         u    superscript   subscript     i  1    k      y   subscript  v  i     subscript  v  i       u=\sum_{i=1}^{k}\langle y,v_{i}\rangle v_{i}   . S=\{v_1,v_2,\ldots,v_k\} Corollary. In the notation of Theorem 6.6, the vector   u   u   u   is the unique vector in W that is "closest" to   y   y   y   ; thet is, for any   x   x   x   ‚ààW,     ‚à•   y  -  x   ‚à•   ‚â•   ‚à•   y  -  u   ‚à•        norm    y  x     norm    y  u      \|y-x\|\geq\|y-u\|   , and this inequality is an equality if and onlly if    x  =  u      x  u    x=u   .       \color   B  l  u   e   6.7      \color  B  l  u  e  6.7    {\color{Blue}~{}6.7}   properties of orthonormal set  Suppose that    S  =   {   v  1   ,   v  2   ,  ‚Ä¶  ,   v  k   }       S    subscript  v  1    subscript  v  2   normal-‚Ä¶   subscript  v  k      S=\{v_{1},v_{2},\ldots,v_{k}\}   is an orthonormal set in an   n   n   n   -dimensional inner product space V. Than (a) S can be extended to an orthonormal basis    {   v  1   ,   v  2   ,  ‚Ä¶  ,   v  k   ,   v   k  +  1    ,  ‚Ä¶  ,   v  n   }      subscript  v  1    subscript  v  2   normal-‚Ä¶   subscript  v  k    subscript  v    k  1    normal-‚Ä¶   subscript  v  n     \{v_{1},v_{2},\ldots,v_{k},v_{k+1},\ldots,v_{n}\}   for V. (b) If W=span(S), then     S  1   =   {   v   k  +  1    ,   v   k  +  2    ,  ‚Ä¶  ,   v  n   }        subscript  S  1     subscript  v    k  1     subscript  v    k  2    normal-‚Ä¶   subscript  v  n      S_{1}=\{v_{k+1},v_{k+2},\ldots,v_{n}\}   is an orhtonormal basis for W ‚ä• (using the preceding notation). (c) If W is any subspace of V, then dim(V)=dim(W)+dim(W ‚ä• ).  Least squares approximation , Minimal solutions to systems of linear equations       \color   B  l  u   e   6.8      \color  B  l  u  e  6.8    {\color{Blue}~{}6.8}   linear functional representation inner product  Let V be a finite-dimensional inner product space over F, and let   g   g   g   :V‚ÜíF be a linear transformation. Then there exists a unique vector   y   y   y   ‚àà V such that     g   (  x  )    =   ‚ü®  x  ,  y  ‚ü©         normal-g  normal-x    normal-x  normal-y     \rm{g}(x)=\langle x,y\rangle   for all   x   x   x   ‚àà V.       \color   B  l  u   e   6.9      \color  B  l  u  e  6.9    {\color{Blue}~{}6.9}   definition of T*  Let V be a finite-dimensional inner product space, and let T be a linear operator on V. Then there exists a unique function T*:V‚ÜíV such that     ‚ü®   T   (  x  )    ,  y  ‚ü©   =   ‚ü®  x  ,    T  *    (  y  )    ‚ü©          normal-T  normal-x   normal-y    normal-x     superscript  normal-T    normal-y      \langle\rm{T}(x),y\rangle=\langle x,\rm{T}^{*}(y)\rangle   for all    x  ,  y     x  y    x,y   ‚àà V. Furthermore, T* is linear  ===     \color   B  l  u   e   6.10      \color  B  l  u  e  6.10    {\color{Blue}~{}6.10}   [T*] Œ≤ =[T]* Œ≤ === Let V be a finite-dimensional inner product space, and let Œ≤ be an orthonormal basis for V. If T is a linear operator on V, then ::::      [   T  *   ]   Œ≤   =    [  T  ]   Œ≤  *        subscript   delimited-[]   superscript  T     Œ≤    subscript   superscript   delimited-[]  T     Œ≤     [T^{*}]_{\beta}=[T]^{*}_{\beta}   .       \color   B  l  u   e   6.11      \color  B  l  u  e  6.11    {\color{Blue}~{}6.11}   properties of T*  Let V be an inner product space, and let T and U be linear operators onV. Then (a) (T+U)*=T*+U*; (b) (   c   c   c   T)*=    c  ¬Ø     normal-¬Ø  c    \bar{c}   T* for any c‚àà F; (c) (TU)*=U*T*; (d) T**=T; (e) I*=I.  Corollary. Let A and B be n√ónmatrices. Then (a) ( A + B )*= A *+ B *; (b) (   c   c   c    A )*=    c  ¬Ø     normal-¬Ø  c    \bar{c}    A * for any   c   c   c   ‚àà F; (c) ( AB )*= B * A *; (d) A **= A ; (e) I *= I .       \color   B  l  u   e   6.12      \color  B  l  u  e  6.12    {\color{Blue}~{}6.12}   Least squares approximation  Let A ‚àà M m√ón ( F ) and   y   y   y   ‚ààF m . Then there exists    x  0     subscript  x  0    x_{0}   ‚àà F n such that      (   A  *  A   )    x  0    =   A  *  y           A  A    subscript  x  0      A  y     (A*A)x_{0}=A*y   and     ‚à•    A   x  0    -  Y   ‚à•   ‚â§   ‚à•    A  x   -  y   ‚à•        norm      A   subscript  x  0    Y     norm      A  x   y      \|Ax_{0}-Y\|\leq\|Ax-y\|   for all x‚àà F n  Lemma 1. let ''A ‚àà M m√ón ( F''),   x   x   x   ‚ààF n , and   y   y   y   ‚ààF m . Then ::::      ‚ü®   A  x   ,  y  ‚ü©   m   =    ‚ü®  x  ,   A  *  y   ‚ü©   n        subscript     A  x   y   m    subscript   x    A  y    n     \langle Ax,y\rangle_{m}=\langle x,A*y\rangle_{n}     Lemma 2. Let ''A ‚àà M m√ón ( F ). Then rank( A*A )=rank( A'').  Corollary. (of lemma 2) If A is an m√ón matrix such that rank( A )=n, then A*A is invertible.       \color   B  l  u   e   6.13      \color  B  l  u  e  6.13    {\color{Blue}~{}6.13}   Minimal solutions to systems of linear equations  Let ''A ‚àà M m√ón ( F'') and b‚àà F m . Suppose that     A  x   =  b        A  x   b    Ax=b   is consistent. Then the following statements are true. (a) There existes exactly one minimal solution   s   s   s   of     A  x   =  b        A  x   b    Ax=b   , and   s   s   s   ‚ààR(L A * ). (b) The vector   s   s   s   is the only solution to     A  x   =  b        A  x   b    Ax=b   that lies in R(L A * ); that is, if   u   u   u   satisfies     (  A  A  *  )   u  =  b     fragments   fragments  normal-(  A  A   normal-)   u   b    (AA*)u=b   , then    s  =   A  *  u       s    A  u     s=A*u   .  Canonical forms  References   Linear Algebra 4th edition, by Stephen H. Friedberg Arnold J. Insel and Lawrence E. spence ISBN 7-04-016733-6  Linear Algebra 3rd edition, by Serge Lang (UTM) ISBN 0-387-96412-6  Linear Algebra and Its Applications 4th edition, by Gilbert Strang ISBN 0-03-010567-6   "       