   Theorems and definitions in linear algebra      Theorems and definitions in linear algebra   This article collects the main theorems and definitions in linear algebra .  Vector Spaces  Let   V   V   V   be a set on which two operations ( vector addition and scalar multiplication ) are defined. If the listed axioms are satisfied for every    u  →     normal-→  u    \vec{u}   ,    v  →     normal-→  v    \vec{v}   , and    w  →     normal-→  w    \vec{w}   in   V   V   V   and every scalar (real number)   c   c   c   and   d   d   d   , then   V   V   V   is called a vector space :  Addition:        u  →   +    v  →   is in  V  .        normal-→  u      normal-→  v   is in  V  .     \vec{u}+\vec{v}\text{ is in }V\text{.}           u  →   +   v  →    =    v  →   +   u  →           normal-→  u    normal-→  v       normal-→  v    normal-→  u      \vec{u}+\vec{v}=\vec{v}+\vec{u}           u  →   +   (    v  →   +   w  →    )    =    (    u  →   +   v  →    )   +   w  →           normal-→  u      normal-→  v    normal-→  w          normal-→  u    normal-→  v     normal-→  w      \vec{u}+(\vec{v}+\vec{w})=(\vec{u}+\vec{v})+\vec{w}           V  has a  𝐳𝐞𝐫𝐨   𝐯𝐞𝐜𝐭𝐨𝐫    0  →   such that for every   u  →   in  V  ,   u  →    +   0  →    =   u  →           V  has a  𝐳𝐞𝐫𝐨   𝐯𝐞𝐜𝐭𝐨𝐫    normal-→  0   such that for every   normal-→  u   in  V  ,   normal-→  u     normal-→  0     normal-→  u     V\text{ has a }\mathbf{zero}\text{ }\mathbf{vector}\text{ }\vec{0}\text{ such %
 that for every }\vec{u}\text{ in }V\text{, }\vec{u}+\vec{0}=\vec{u}            For every   u  →   in  V  , there is a vector in  V  denoted by   -    u  →   such that   u  →     +   (   -   u  →    )    =    0  →   .             For every   normal-→  u   in  V  , there is a vector in  V  denoted by      normal-→  u   such that   normal-→  u        normal-→  u        normal-→  0   .     \text{For every }\vec{u}\text{ in }V\text{, there is a vector in }V\text{ %
 denoted by }-\vec{u}\text{ such that }\vec{u}+(-\vec{u})=\vec{0}\text{.}      Scalar Multiplication:       c   u  →   is in  V  .      c   normal-→  u   is in  V  .    c\vec{u}\text{ is in }V\text{.}          c   (    u  →   +   v  →    )    =    c   u  →    +   c   v  →           c     normal-→  u    normal-→  v         c   normal-→  u      c   normal-→  v       c(\vec{u}+\vec{v})=c\vec{u}+c\vec{v}           (   c  +  d   )    u  →    =    c   u  →    +   d   u  →             c  d    normal-→  u        c   normal-→  u      d   normal-→  u       (c+d)\vec{u}=c\vec{u}+d\vec{u}          c   (   d   u  →    )    =    (   c  d   )    u  →          c    d   normal-→  u         c  d    normal-→  u      c(d\vec{u})=(cd)\vec{u}          1   (   u  →   )    =   u  →         1   normal-→  u     normal-→  u     1(\vec{u})=\vec{u}      Subspaces  If   W   W   W   is a nonempty subset of a vector space   V   V   V   , then   W   W   W   is a subspace of   V   V   V   if and only if the following closure conditions hold:        If   u  →   and   v  →   are in  W  , then   u  →    +    v  →   is in  W  .         If   normal-→  u   and   normal-→  v   are in  W  , then   normal-→  u       normal-→  v   is in  W  .     \text{If }\vec{u}\text{ and }\vec{v}\text{ are in }W\text{, then }\vec{u}+\vec%
 {v}\text{ is in }W\text{.}         If   u  →   is in  W  and  c  is any scalar, then  c   u  →   is in  W  .      If   normal-→  u   is in  W  and  c  is any scalar, then  c   normal-→  u   is in  W  .    \text{If }\vec{u}\text{ is in }W\text{ and }c\text{ is any scalar, then }c\vec%
 {u}\text{ is in }W\text{.}      Linear combinations  A vector    v  →     normal-→  v    \vec{v}   in a vector space   V   V   V   is called a linear combination of the vectors     u  →   1     subscript   normal-→  u   1    \vec{u}_{1}   ,     u  →   2     subscript   normal-→  u   2    \vec{u}_{2}   ,   …   normal-…   \dots   ,     u  →   k     subscript   normal-→  u   k    \vec{u}_{k}   in   V   V   V   if    v  →     normal-→  v    \vec{v}   can be written in the form     v  →   =     c  1     u  →   1    +    c  2     u  →   2    +  …  +    c  k     u  →   k          normal-→  v        subscript  c  1    subscript   normal-→  u   1       subscript  c  2    subscript   normal-→  u   2    normal-…     subscript  c  k    subscript   normal-→  u   k       \vec{v}=c_{1}\vec{u}_{1}+c_{2}\vec{u}_{2}+\dots+c_{k}\vec{u}_{k}   , where    c  1     subscript  c  1    c_{1}   ,    c  2     subscript  c  2    c_{2}   ,   …   normal-…   \dots   ,    c  k     subscript  c  k    c_{k}   are scalars.  Systems of linear equations  Cramer's Rule  If a system of   n   n   n   linear equations in   n   n   n   variables has a coefficient matrix with a nonzero determinant    |  A  |      A    \left|A\right|   , then the solution of the system is given by        x  1   =    det   (   A  1   )     det   (  A  )      ,     x  2   =     det   (   A  2   )     det   (  A  )     ,  …    ,    x  n   =    det   (   A  n   )     det   (  A  )          formulae-sequence     subscript  x  1        subscript  A  1      A      formulae-sequence     subscript  x  2         subscript  A  2      A    normal-…       subscript  x  n        subscript  A  n      A        x_{1}=\frac{\det(A_{1})}{\det(A)},\qquad x_{2}=\frac{\det(A_{2})}{\det(A)},%
 \qquad\dots,\qquad x_{n}=\frac{\det(A_{n})}{\det(A)}   , where the   i   i   i   th column of    A  i     subscript  A  i    A_{i}   is the column of constants in the system of equations.  Linear dependence  Linear independence  Bases  A set of vectors    S  =   {    v  →   1       fragments  S    fragments  normal-{   subscript   normal-→  v   1      S=\{\vec{v}_{1}   ,     v  →   2     subscript   normal-→  v   2    \vec{v}_{2}   ,   …   normal-…   \dots   ,      v  →   n   }     fragments   subscript   normal-→  v   n   normal-}    \vec{v}_{n}\}   in a vector space   V   V   V   is called a basis if the following conditions are true:      S   S   S   spans   V   V   V   .     S   S   S   is linearly independent.   Dimension  Linear transformations and matrices  Change of coordinate matrix  Clique  Coordinate vector relative to a basis  Dimension theorem  Dominance relation  Identity matrix  Identity transformation  Incidence matrix  Inverse of a linear transformation  Inverse of a matrix  Invertible linear transformation  Isomorphic vector spaces  Isomorphism  Kronecker delta  Left-multiplication transformation  Linear operator  Linear transformation  Matrix representing a linear transformation  Nullity of a linear transformation  Null space  Ordered basis  Product of matrices  Projection on a subspace  Projection on the x-axis  Range  Rank of a linear transformation  Reflection about the x-axis  Rotation  Similar matrices Standard ordered basis for    F  n     subscript  F  n    F_{n}     Standard representation of a vector space with respect to a basis  Zero transformation  P.S. coefficient of the differential equation , differentiability of complex function ,vector space of functions differential operator , auxiliary polynomial , to the power of a complex number, exponential function .  Definition of a Linear Transformation  Let   V   V   V   and   W   W   W   be vector spaces. The function    T  :   V  →  W      normal-:  T   normal-→  V  W     T:V\to W   is called a linear transformation of   V   V   V   into   W   W   W   if the following two properties are true for all    u  →     normal-→  u    \vec{u}   and    v  →     normal-→  v    \vec{v}   in   V   V   V   and for any scalar   c   c   c   .        T   (    u  →   +   v  →    )    =    T   (   u  →   )    +   T   (   v  →   )           T     normal-→  u    normal-→  v         T   normal-→  u      T   normal-→  v       T(\vec{u}+\vec{v})=T(\vec{u})+T(\vec{v})          T   (   c   u  →    )    =   c  T   (   u  →   )          T    c   normal-→  u       c  T   normal-→  u      T(c\vec{u})=cT(\vec{u})           \color   B  l  u   e   2.1      \color  B  l  u  e  2.1    {\color{Blue}~{}2.1}    N ( T ) and R ( T ) are subspaces  Let V and W be vector spaces and I : V → W be linear. Then N ( T ) and R ( T ) are subspaces of V and W , respectively.  ===     \color   B  l  u   e   2.2      \color  B  l  u  e  2.2    {\color{Blue}~{}2.2}   R(T)= span of T(basis in V)=== Let V and W be vector spaces, and let T: V→W be linear. If    β  =    v  1   ,   v  2   ,  …  ,   v  n        β    subscript  v  1    subscript  v  2   normal-…   subscript  v  n      \beta={v_{1},v_{2},\ldots,v_{n}}   is a basis for V, then :     R   (  T  )    =   span   (   T   (  β  )    )    =   span   (   T   (   v  1   )    ,   T   (   v  2   )    ,  …  ,   T   (   v  n   )    )            normal-R  normal-T     span    T  β           span     T   subscript  v  1      T   subscript  v  2    normal-…    T   subscript  v  n         \mathrm{R(T)}=\mathrm{span}(T(\beta\mathrm{))}=\mathrm{span}({T(v_{1}),T(v_{2}%
 ),\ldots,T(v_{n})})   .       \color   B  l  u   e   2.3      \color  B  l  u  e  2.3    {\color{Blue}~{}2.3}   Dimension theorem  Let V and W be vector spaces, and let T: V → W be linear. If V is finite-dimensional, then :::::       nullity   (  T  )    +   rank   (  T  )     =   dim   (  V  )     .          nullity  T     rank  T     dimension  V     \mathrm{nullity}(T)+\mathrm{rank}(T)=\dim(V).     ===     \color   B  l  u   e   2.4      \color  B  l  u  e  2.4    {\color{Blue}~{}2.4}   one-to-one ⇔ N(T) = {0}=== Let    T  :   V  →  W      normal-:  T   normal-→  V  W     T:V\to W   be a linear transformation. Then   T   T   T   is one-to-one if and only if     ker   (  T  )    =   {   0  →   }        ker  T     normal-→  0      \operatorname{ker}(T)=\{\vec{0}\}   .  ===     \color   B  l  u   e   2.5      \color  B  l  u  e  2.5    {\color{Blue}~{}2.5}   one-to-one ⇔ onto ⇔ rank( T ) = dim( V )=== Let V and W be vector spaces of equal (finite) dimension, and let T : V → W be linear. Then the following are equivalent. :(a) T is one-to-one.   (b) T is onto.  (c) rank( T ) = dim( V ).        \color   B  l  u   e   2.6      \color  B  l  u  e  2.6    {\color{Blue}~{}2.6}   ∀      w  1   ,   w  2   ,  …  ,   w  n    =         subscript  w  1    subscript  w  2   normal-…   subscript  w  n    absent    {w_{1},w_{2},\ldots,w_{n}}=   exactly one T (basis),  Let V and W be vector space over F, and suppose that     v  1   ,   v  2   ,  …  ,   v  n       subscript  v  1    subscript  v  2   normal-…   subscript  v  n     {v_{1},v_{2},\ldots,v_{n}}   is a basis for V. For     w  1   ,   w  2   ,  …  ,   w  n       subscript  w  1    subscript  w  2   normal-…   subscript  w  n     w_{1},w_{2},\ldots,w_{n}   in W, there exists exactly one linear transformation T: V→W such that     T   (   v  i   )    =   w  i         normal-T   subscript  v  i     subscript  w  i     \mathrm{T}(v_{i})=w_{i}   for     i  =   1  ,  2  ,  …  ,  n    .      i   1  2  normal-…  n     i=1,2,\ldots,n.     Corollary. Let V and W be vector spaces, and suppose that V has a finite basis     v  1   ,   v  2   ,  …  ,   v  n       subscript  v  1    subscript  v  2   normal-…   subscript  v  n     {v_{1},v_{2},\ldots,v_{n}}   . If U, T: V→W are linear and     U   (   v  i   )    =   T   (   v  i   )          U   subscript  v  i      T   subscript  v  i      U(v_{i})=T(v_{i})   for     i  =   1  ,  2  ,  …  ,  n    ,      i   1  2  normal-…  n     i=1,2,\ldots,n,   then U=T.       \color   B  l  u   e   2.7      \color  B  l  u  e  2.7    {\color{Blue}~{}2.7}   T is vector space  Let V and W be vector spaces over a field F, and let T, U: V→W be linear. :(a) For all   a   a   a   ∈ F ,     a  T   +  U        a  normal-T   normal-U    a\mathrm{T}+\mathrm{U}   is linear.   (b) Using the operations of addition and scalar multiplication in the preceding definition, the collection of all linear transformations form V to W is a vector space over F.        \color   B  l  u   e   2.8      \color  B  l  u  e  2.8    {\color{Blue}~{}2.8}   linearity of matrix representation of linear transformation  Let V and W be finite-dimensional vector spaces with ordered bases β and γ, respectively, and let T, U: V→W be linear transformations. Then :(a)      [   T  +  U   ]   β  γ   =     [  T  ]   β  γ   +    [  U  ]   β  γ         superscript   subscript   delimited-[]    T  U    β   γ      superscript   subscript   delimited-[]  T   β   γ    superscript   subscript   delimited-[]  U   β   γ      [T+U]_{\beta}^{\gamma}=[T]_{\beta}^{\gamma}+[U]_{\beta}^{\gamma}   and   (b)      [   a  T   ]   β  γ   =   a    [  T  ]   β  γ         superscript   subscript   delimited-[]    a  T    β   γ     a   superscript   subscript   delimited-[]  T   β   γ      [aT]_{\beta}^{\gamma}=a[T]_{\beta}^{\gamma}   for all scalars   a   a   a   .        \color   B  l  u   e   2.9      \color  B  l  u  e  2.9    {\color{Blue}~{}2.9}   composition law of linear operators  Let V,W, and Z be vector spaces over the same field f, and let T:V→W and U:W→Z be linear. then UT:V→Z is linear.       \color   B  l  u   e   2.10      \color  B  l  u  e  2.10    {\color{Blue}~{}2.10}   law of linear operator  Let v be a vector space. Let T, U 1 , U 2 ∈   ℒ   ℒ   \mathcal{L}   (V). Then (a) T(U 1 +U 2 )=TU 1 +TU 2 and (U 1 +U 2 )T=U 1 T+U 2 T (b) T(U 1 U 2 )=(TU 1 )U 2 (c) TI=IT=T (d)   a   a   a   (U 1 U 2 )=(   a   a   a   U 1 )U 2 =U 1 (   a   a   a   U 2 ) for all scalars   a   a   a   .  ===     \color   B  l  u   e   2.11      \color  B  l  u  e  2.11    {\color{Blue}~{}2.11}   [UT] α γ =[U] β γ [T] α β === Let V, W and Z be finite-dimensional vector spaces with ordered bases α β γ, respectively. Let T: V⇐W and U: W→Z be linear transformations. Then ::::::      [   U  T   ]   α  γ   =     [  U  ]   β  γ     [  T  ]   α  β         superscript   subscript   delimited-[]    U  T    α   γ      superscript   subscript   delimited-[]  U   β   γ    superscript   subscript   delimited-[]  T   α   β      [UT]_{\alpha}^{\gamma}=[U]_{\beta}^{\gamma}[T]_{\alpha}^{\beta}   .  Corollary . Let V be a finite-dimensional vector space with an ordered basis β. Let T,U∈   ℒ   ℒ   \mathcal{L}   (V). Then [UT] β =[U] β [T] β .       \color   B  l  u   e   2.12      \color  B  l  u  e  2.12    {\color{Blue}~{}2.12}   law of matrix  Let A be an m×n matrix, B and C be n×p matrices, and D and E be q×m matrices. Then :(a) A(B+C)=AB+AC and (D+E)A=DA+EA.   (b)   a   a   a   (AB)=(   a   a   a   A)B=A(   a   a   a   B) for any scalar   a   a   a   .  (c) I m A=AI m .  (d) If V is an n-dimensional vector space with an ordered basis β, then [I v ] β =I n .   Corollary. Let A be an m×n matrix, B 1 ,B 2 ,...,B k be n×p matrices, C 1 ,C 1 ,...,C 1 be q×m matrices, and     a  1   ,   a  2   ,  …  ,   a  k       subscript  a  1    subscript  a  2   normal-…   subscript  a  k     a_{1},a_{2},\ldots,a_{k}   be scalars. Then ::::::     A   (    ∑   i  =  1   k     a  i    B  i     )    =    ∑   i  =  1   k     a  i   A   B  i           A    superscript   subscript     i  1    k      subscript  a  i    subscript  B  i        superscript   subscript     i  1    k      subscript  a  i   A   subscript  B  i       A\Bigg(\sum_{i=1}^{k}a_{i}B_{i}\Bigg)=\sum_{i=1}^{k}a_{i}AB_{i}   and          (    ∑   i  =  1   k     a  i    C  i     )   A   =    ∑   i  =  1   k     a  i    C  i   A            superscript   subscript     i  1    k      subscript  a  i    subscript  C  i     A     superscript   subscript     i  1    k      subscript  a  i    subscript  C  i   A      \Bigg(\sum_{i=1}^{k}a_{i}C_{i}\Bigg)A=\sum_{i=1}^{k}a_{i}C_{i}A   .          \color   B  l  u   e   2.13      \color  B  l  u  e  2.13    {\color{Blue}~{}2.13}   law of column multiplication  Let A be an m×n matrix and B be an n×p matrix. For each    j   (  1  ≤  j  ≤  p  )      fragments  j   fragments  normal-(  1   j   p  normal-)     j(1\leq j\leq p)   let    u  j     subscript  u  j    u_{j}   and    v  j     subscript  v  j    v_{j}   denote the jth columns of AB and B, respectively. Then (a)     u  j   =   A   v  j         subscript  u  j     A   subscript  v  j      u_{j}=Av_{j}    (b)     v  j   =   B   e  j         subscript  v  j     B   subscript  e  j      v_{j}=Be_{j}   , where    e  j     subscript  e  j    e_{j}   is the jth standard vector of F p .  ===     \color   B  l  u   e   2.14      \color  B  l  u  e  2.14    {\color{Blue}~{}2.14}   [T(u)] γ =[T] β γ [u] β === Let V and W be finite-dimensional vector spaces having ordered bases β and γ, respectively, and let T: V→W be linear. Then, for each u ∈ V, we have :::::::      [   T   (  u  )    ]   γ   =     [  T  ]   β  γ     [  u  ]   β         subscript   delimited-[]    T  u    γ      superscript   subscript   delimited-[]  T   β   γ    subscript   delimited-[]  u   β      [T(u)]_{\gamma}=[T]_{\beta}^{\gamma}[u]_{\beta}   .       \color   B  l  u   e   2.15      \color  B  l  u  e  2.15    {\color{Blue}~{}2.15}   laws of L A  Let A be an m×n matrix with entries from F. Then the left-multiplication transformation L A : F n →F m is linear. Furthermore, if B is any other m×n matrix (with entries from F) and β and γ are the standard ordered bases for F n and F m , respectively, then we have the following properties. (a)      [   L  A   ]   β  γ   =  A       superscript   subscript   delimited-[]   subscript  L  A    β   γ   A    [L_{A}]_{\beta}^{\gamma}=A   . (b) L A =L B if and only if A=B. (c) L A+B =L A +L B and L    a   a   a   A =   a   a   a   L A for all   a   a   a   ∈F. (d) If T:F n →F m is linear, then there exists a unique m×n matrix C such that T=L C . In fact,    C  =    [   L  A   ]   β  γ       normal-C   superscript   subscript   delimited-[]   subscript  L  A    β   γ     \mathrm{C}=[L_{A}]_{\beta}^{\gamma}   . (e) If W is an n×p matrix, then L AE =L A L E . (f ) If m=n, then     L   I  n    =   I   F  n         subscript  L   subscript  I  n     subscript  I   superscript  F  n      L_{I_{n}}=I_{F^{n}}   .  ===     \color   B  l  u   e   2.16      \color  B  l  u  e  2.16    {\color{Blue}~{}2.16}   A(BC)=(AB)C=== Let A,B, and C be matrices such that A(BC) is defined. Then A(BC)=(AB)C; that is, matrix multiplication is associative.       \color   B  l  u   e   2.17      \color  B  l  u  e  2.17    {\color{Blue}~{}2.17}   T −1 is linear  Let V and W be vector spaces, and let T:V→W be linear and invertible. Then T −1 : W →V is linear.  ===     \color   B  l  u   e   2.18      \color  B  l  u  e  2.18    {\color{Blue}~{}2.18}   [T −1 ] γ β =([T] β γ ) −1 === Let V and W be finite-dimensional vector spaces with ordered bases β and γ, respectively. Let T:V→W be linear. Then T is invertible if and only if     [  T  ]   β  γ     superscript   subscript   delimited-[]  T   β   γ    [T]_{\beta}^{\gamma}   is invertible. Furthermore,      [   T   -  1    ]   γ  β   =    (    [  T  ]   β  γ   )    -  1         superscript   subscript   delimited-[]   superscript  T    1     γ   β    superscript   superscript   subscript   delimited-[]  T   β   γ     1      [T^{-1}]_{\gamma}^{\beta}=([T]_{\beta}^{\gamma})^{-1}     Lemma. Let T be an invertible linear transformation from V to W. Then V is finite-dimensional if and only if W is finite-dimensional. In this case, dim(V)=dim(W).  Corollary 1. Let V be a finite-dimensional vector space with an ordered basis β, and let T:V→V be linear. Then T is invertible if and only if [T] β is invertible. Furthermore, [T −1 ] β =([T] β ) −1 .  Corollary 2. Let A be an n×n matrix. Then A is invertible if and only if L A is invertible. Furthermore, (L A ) −1 =L A −1 .  ===     \color   B  l  u   e   2.19      \color  B  l  u  e  2.19    {\color{Blue}~{}2.19}   V is isomorphic to W ⇔ dim(V)=dim(W)=== Let W and W be finite-dimensional vector spaces (over the same field). Then V is isomorphic to W if and only if dim(V)=dim(W).  Corollary. Let V be a vector space over F. Then V is isomorphic to F n if and only if dim(V)=n.       \color   B  l  u   e   2.20      \color  B  l  u  e  2.20    {\color{Blue}~{}2.20}   ??  Let V and W be finite-dimensional vector spaces over F of dimensions n and m, respectively, and let β and γ be ordered bases for V and W, respectively. Then the function    Φ    normal-Φ   ~{}\Phi      ℒ   ℒ   \mathcal{L}   (V,W)→M m×n (F), defined by      Φ    (  T  )    =    [  T  ]   β  γ         normal-Φ  T    superscript   subscript   delimited-[]  T   β   γ     ~{}\Phi(T)=[T]_{\beta}^{\gamma}   for T∈   ℒ   ℒ   \mathcal{L}   (V,W), is an isomorphism.  Corollary. Let V and W be finite-dimensional vector spaces of dimension n and m, respectively. Then   ℒ   ℒ   \mathcal{L}   (V,W) is finite-dimensional of dimension mn.       \color   B  l  u   e   2.21      \color  B  l  u  e  2.21    {\color{Blue}~{}2.21}    Φ β is an isomorphism  For any finite-dimensional vector space V with ordered basis β, Φ β is an isomorphism.       \color   B  l  u   e   2.22      \color  B  l  u  e  2.22    {\color{Blue}~{}2.22}   ??  Let β and β' be two ordered bases for a finite-dimensional vector space V, and let    Q  =    [   I  V   ]    β  ′   β       Q   superscript   subscript   delimited-[]   subscript  I  V     superscript  β  normal-′    β     Q=[I_{V}]_{\beta^{\prime}}^{\beta}   . Then (a)   Q   Q   Q   is invertible. (b) For any    v  ∈       v  absent    v\in   V,      [  v  ]   β   =   Q    [  v  ]    β  ′          subscript   delimited-[]  v   β     Q   subscript   delimited-[]  v    superscript  β  normal-′       ~{}[v]_{\beta}=Q[v]_{\beta^{\prime}}   .  ===     \color   B  l  u   e   2.23      \color  B  l  u  e  2.23    {\color{Blue}~{}2.23}   [T] β' =Q −1 [T] β Q=== Let T be a linear operator on a finite-dimensional vector space V,and let β and β' be two ordered bases for V. Suppose that Q is the change of coordinate matrix that changes β'-coordinates into β-coordinates. Then ::::::      [  T  ]    β  ′    =    Q   -  1      [  T  ]   β   Q        subscript   delimited-[]  T    superscript  β  normal-′       superscript  Q    1     subscript   delimited-[]  T   β   Q     ~{}[T]_{\beta^{\prime}}=Q^{-1}[T]_{\beta}Q   .  Corollary. Let A∈M n×n ( F ), and le t γ be an ordered basis for F n . Then [L A ] γ =Q −1 AQ, where Q is the n×n matrix whose jth column is the jth vector of γ.  Principal Axes Theorem  For a conic whose equation is      a   x  2    +   b  x  y   +   c   y  2    +   d  x   +   e  y   +  f   =  0          a   superscript  x  2      b  x  y     c   superscript  y  2      d  x     e  y   f   0    ax^{2}+bxy+cy^{2}+dx+ey+f=0   , the rotation given by    X  =   P   X  ′        X    P   superscript  X  normal-′      X=PX^{\prime}   eliminates the    x  y      x  y    xy   -term if   P   P   P   is an orthogonal matrix, with     |  P  |   =  1        P   1    \left|P\right|=1   , that diagonalizes   A   A   A   . That is,        P  T   A  P   =   [      λ  1     0      0     λ  2      ]          superscript  P  T   A  P      subscript  λ  1   0    0   subscript  λ  2       P^{T}AP=\begin{bmatrix}\lambda_{1}&0\\
 0&\lambda_{2}\end{bmatrix}   , where    λ  1     subscript  λ  1    \lambda_{1}   and    λ  2     subscript  λ  2    \lambda_{2}   are eigenvalues of   A   A   A   . The equation of the rotated conic is given by         λ  1     (   x  ′   )   2    +    λ  2     (   y  ′   )   2    +    [     d    e     ]   P   X  ′    +  f   =  0           subscript  λ  1    superscript   superscript  x  normal-′   2       subscript  λ  2    superscript   superscript  y  normal-′   2        d  e    P   superscript  X  normal-′    f   0    \lambda_{1}(x^{\prime})^{2}+\lambda_{2}(y^{\prime})^{2}+\begin{bmatrix}d&e\\
 \end{bmatrix}PX^{\prime}+f=0   .       \color   B  l  u   e   2.26      \color  B  l  u  e  2.26    {\color{Blue}~{}2.26}          \color   B  l  u   e   2.26      \color  B  l  u  e  2.26    {\color{Blue}~{}2.26}     ===     \color   B  l  u   e   2.27      \color  B  l  u  e  2.27    {\color{Blue}~{}2.27}    p (D)(x)=0 ( p (D)∈C ∞ )⇒ x (k) exists (k∈N)=== Any solution to a homogeneous linear differential equation with constant coefficients has derivatives of all orders; that is, if   x   x   x   is a solution to such an equation, then    x   (  k  )      superscript  x  k    x^{(k)}   exists for every positive integer k.  ===     \color   B  l  u   e   2.28      \color  B  l  u  e  2.28    {\color{Blue}~{}2.28}   {solutions}= N(p(D))=== The set of all solutions to a homogeneous linear differential equation with constant coefficients coincides with the null space of p(D), where p(t) is the auxiliary polynomial with the equation.  Corollary . The set of all solutions to s homogeneous linear differential equation with constant coefficients is a subspace of    C  ∞     superscript  normal-C     \mathrm{C}^{\infty}   .       \color   B  l  u   e   2.29      \color  B  l  u  e  2.29    {\color{Blue}~{}2.29}   derivative of exponential function  For any exponential function      f   (  t  )    =   e   c  t     ,     f  ′    (  t  )    =   c   e   c  t         formulae-sequence      f  t    superscript  e    c  t          superscript  f  normal-′   t     c   superscript  e    c  t        f(t)=e^{ct},f^{\prime}(t)=ce^{ct}   .       \color   B  l  u   e   2.30      \color  B  l  u  e  2.30    {\color{Blue}~{}2.30}   {e −at } is a basis of N( p (D+aI))  The solution space for the differential equation, :::      y  ′   +    a  0   y    =  0         superscript  y  normal-′      subscript  a  0   y    0    y^{\prime}+a_{0}y=0   is of dimension 1 and has    {   e   -    a  0   t     }      superscript  e       subscript  a  0   t       \{e^{-a_{0}t}\}   as a basis.  Corollary. For any complex number c, the null space of the differential operator D-cI has {    e   c  t      superscript  e    c  t     e^{ct}   } as a basis.       \color   B  l  u   e   2.31      \color  B  l  u  e  2.31    {\color{Blue}~{}2.31}       e   c  t      superscript  e    c  t     e^{ct}   is a solution  Let p(t) be the auxiliary polynomial for a homogeneous linear differential equation with constant coefficients. For any complex number c, if c is a zero of p(t), then to the differential equation.  ===     \color   B  l  u   e   2.32      \color  B  l  u  e  2.32    {\color{Blue}~{}2.32}   dim(N( p (D)))=n=== For any differential operator p(D) of order n, the null space of p(D) is an n_dimensional subspace of C ∞ .  Lemma 1 . The differential operator D-cI: C ∞ to C ∞ is onto for any complex number c.  Lemma 2 Let V be a vector space, and suppose that T and U are linear operators on V such that U is onto and the null spaces of T and U are finite-dimensional, Then the null space of TU is finite-dimensional, and :::::dim(N(TU))=dim(N(U))+dim(N(U)).  Corollary . The solution space of any nth-order homogeneous linear differential equation with constant coefficients is an n-dimensional subspace of C ∞ .       \color   B  l  u   e   2.33      \color  B  l  u  e  2.33    {\color{Blue}~{}2.33}   e c i t is linearly independent with each other (c i are distinct)  Given n distinct complex numbers     c  1   ,   c  2   ,  …  ,   c  n       subscript  c  1    subscript  c  2   normal-…   subscript  c  n     c_{1},c_{2},\ldots,c_{n}   , the set of exponential functions    {   e    c  1   t    ,   e    c  2   t    ,  …  ,   e    c  n   t    }      superscript  e     subscript  c  1   t     superscript  e     subscript  c  2   t    normal-…   superscript  e     subscript  c  n   t      \{e^{c_{1}t},e^{c_{2}t},\ldots,e^{c_{n}t}\}   is linearly independent.  Corollary . For any nth-order homogeneous linear differential equation with constant coefficients, if the auxiliary polynomial has n distinct zeros     c  1   ,   c  2   ,  …  ,   c  n       subscript  c  1    subscript  c  2   normal-…   subscript  c  n     c_{1},c_{2},\ldots,c_{n}   , then    {   e    c  1   t    ,   e    c  2   t    ,  …  ,   e    c  n   t    }      superscript  e     subscript  c  1   t     superscript  e     subscript  c  2   t    normal-…   superscript  e     subscript  c  n   t      \{e^{c_{1}t},e^{c_{2}t},\ldots,e^{c_{n}t}\}   is a basis for the solution space of the differential equation.  Lemma . For a given complex number c and positive integer n, suppose that (t-c)^n is athe auxiliary polynomial of a homogeneous linear differential equation with constant coefficients. Then the set ::    β  =   {   e    c  1   t    ,   e    c  2   t    ,  …  ,   e    c  n   t    }       β    superscript  e     subscript  c  1   t     superscript  e     subscript  c  2   t    normal-…   superscript  e     subscript  c  n   t       \beta=\{e^{c_{1}t},e^{c_{2}t},\ldots,e^{c_{n}t}\}   is a basis for the solution space of the equation.       \color   B  l  u   e   2.34      \color  B  l  u  e  2.34    {\color{Blue}~{}2.34}   general solution of homogeneous linear differential equation  Given a homogeneous linear differential equation with constant coefficients and auxiliary polynomial ::::       (   t  -   c  1    )   1  n     (   t  -   c  2    )   2  n   ⋯    (   t  -   c  k    )   k  n    ,       subscript   superscript    t   subscript  c  1    n   1    subscript   superscript    t   subscript  c  2    n   2   normal-⋯   subscript   superscript    t   subscript  c  k    n   k     (t-c_{1})^{n}_{1}(t-c_{2})^{n}_{2}\cdots(t-c_{k})^{n}_{k},   where     n  1   ,   n  2   ,  …  ,   n  k       subscript  n  1    subscript  n  2   normal-…   subscript  n  k     n_{1},n_{2},\ldots,n_{k}   are positive integers and     c  1   ,   c  2   ,  …  ,   c  n       subscript  c  1    subscript  c  2   normal-…   subscript  c  n     c_{1},c_{2},\ldots,c_{n}   are distinct complex numbers, the following set is a basis for the solution space of the equation: ::    {   e    c  1   t    ,   t   e    c  1   t     ,  …  ,    t    n  1   -  1     e    c  1   t     ,  …  ,   e   c  k   t   ,   t   e    c  k   t     ,  …  ,    t    n  k   -  1     e    c  k   t     }      superscript  e     subscript  c  1   t      t   superscript  e     subscript  c  1   t     normal-…     superscript  t     subscript  n  1   1     superscript  e     subscript  c  1   t     normal-…    e   subscript  c  k   t     t   superscript  e     subscript  c  k   t     normal-…     superscript  t     subscript  n  k   1     superscript  e     subscript  c  k   t       \{e^{c_{1}t},te^{c_{1}t},\ldots,t^{n_{1}-1}e^{c_{1}t},\ldots,e{c_{k}t},te^{c_{%
 k}t},\ldots,t^{n_{k}-1}e^{c_{k}t}\}   .  Definition of an Orthogonal Matrix  A square matrix   P   P   P   is called orthogonal if it is invertible and if       P   -  1    =   P  T        superscript  P    1     superscript  P  T     P^{-1}=P^{T}   .  Real Spectral Theorem  If   A   A   A   is an    n  ×  n      n  n    n\times n   symmetric matrix, then the following properties are true:      A   A   A   is diagonalizable.  All eigenvalues of   A   A   A   are real.  If   λ   λ   \lambda   is an eigenvalue of   A   A   A   with multiplicity   k   k   k   , then   λ   λ   \lambda   has   k   k   k   linearly independent eigenvectors. That is, the eigenspace of   λ   λ   \lambda   has dimension   k   k   k   .   Also, the set of eigenvalues of   A   A   A   is called the spectrum of   A   A   A   .  Elementary matrix operations and systems of linear equations  Elementary matrix operations  The three elementary row operations are the following:   Interchange two rows.  Multiply a row by a nonzero constant.  Add a multiple of a row to another row.   Elementary matrix  An    n  ×  n      n  n    n\times n   matrix is called an elementary matrix if it can be obtained from the identity matrix    I  n     subscript  I  n    I_{n}   by a single elementary row operation.  Rank of a matrix  The rank of a matrix A is the number of pivot columns after the reduced row echelon form of A.  Invertible Matrices      If  A  is  n  ×  n  , then the following statements are equivalent:      If  A  is  n  ×  n  , then the following statements are equivalent:    \text{If }A\text{ is }n\text{ × }n\text{, then the following statements are %
 equivalent:}          A  is invertible.      A  is invertible.    A\text{ is invertible.}          A   x  →    =      b  →   has a unique solution for every  n   ×  1   column matrix   b  →   .         A   normal-→  x           normal-→  b   has a unique solution for every  n   1   column matrix   normal-→  b   .     A\vec{x}=\vec{b}\text{ has a unique solution for every }n\times 1\text{ column%
  matrix }\vec{b}\text{.}          A   x  →    =    0  →   has only the trivial solution.         A   normal-→  x       normal-→  0   has only the trivial solution.     A\vec{x}=\vec{0}\text{ has only the trivial solution.}         A  is row-equivalent to   I  n   .      A  is row-equivalent to   subscript  I  n   .    A\text{ is row-equivalent to }I_{n}\text{.}         A  can be written as the product of elementary matrices.      A  can be written as the product of elementary matrices.    A\text{ can be written as the product of elementary matrices.}          det   (  A  )    ≠  0        A   0    \det(A)\neq 0          rk   (  A  )    =   n  number of columns.        rk  A     n  number of columns.     \operatorname{rk}(A)=n\text{ number of columns.}          nul   (  A  )    =  0       nul  A   0    \operatorname{nul}(A)=0         All of the  n  -row vectors of  A  are linearly independent.      All of the  n  -row vectors of  A  are linearly independent.    \text{All of the }n\text{-row vectors of }A\text{ are linearly independent.}         All of the  n  -column vectors of  A  are linearly independent.      All of the  n  -column vectors of  A  are linearly independent.    \text{All of the }n\text{-column vectors of }A\text{ are linearly independent.}      System of linear equations  Determinants  If    A = \begin{pmatrix}     a & b \\ c & d \\ \end{pmatrix} is a 2×2'' matrix with entries form a field F, then we define the determinant of A, denoted det( A ) or |A|, to be the scalar     a  d   -   b  c         a  d     b  c     ad-bc   .''  ＊Theorem 1: linear function for a single row. ＊Theorem 2: nonzero determinant ⇔ invertible matrix  Theorem 1: '' The function det: M 2×2 ( F ) → F is a linear function of each row of a 2×2 matrix when the other row is held fixed. That is, if     u  ,  v   ,     u  v    u,v,   and   w   w   w   are in F ² and   k   k   k   is a scalar, then''    \det\begin{pmatrix}     u + kv\\ w\\ \end{pmatrix} =\det\begin{pmatrix} u\\ w\\ \end{pmatrix} + k\det\begin{pmatrix} v\\ w\\ \end{pmatrix}  and    \det\begin{pmatrix}     w\\ u + kv\\ \end{pmatrix} =\det\begin{pmatrix} w\\ u\\ \end{pmatrix} + k\det\begin{pmatrix} w\\ v\\ \end{pmatrix}  Theorem 2 : ''Let A   ∈     \in    M 2×2 ( F ) . Then thee deter minant of A is nonzero if and only if A is invertible. Moreover, if A is invertible, then'' :::::::     A   -  1    =    1   det   (  A  )      (      A  22      -   A  12         -   A  21       A   ;  11       )         superscript  A    1        1    A       subscript  A  22      subscript  A  12         subscript  A  21     fragments  A   subscript  normal-;  11         A^{-1}=\frac{1}{\det(A)}\begin{pmatrix}A_{22}&-A_{12}\\
 -A_{21}&A_{11}\\
 \end{pmatrix}     Diagonalization  Characteristic polynomial of a linear operator/matrix       \color   B  l  u   e   5.1      \color  B  l  u  e  5.1    {\color{Blue}~{}5.1}   diagonalizable⇔basis of eigenvector  A linear operator T on a finite-dimensional vector space V is diagonalizable if and only if there exists an ordered basis β for V consisting of eigenvectors of T. Furthermore, if T is diagonalizable,    β  =    v  1   ,   v  2   ,  …  ,   v  n        β    subscript  v  1    subscript  v  2   normal-…   subscript  v  n      \beta={v_{1},v_{2},\ldots,v_{n}}   is an ordered basis of eigenvectors of T, and D = [T] β then D is a diagonal matrix and    D   j  j      subscript  D    j  j     D_{jj}   is the eigenvalue corresponding to    v  j     subscript  v  j    v_{j}   for    1  ≤  j  ≤  n        1  j       n     1\leq j\leq n   .  ===     \color   B  l  u   e   5.2      \color  B  l  u  e  5.2    {\color{Blue}~{}5.2}   eigenvalue⇔det( A -λ I n)=0=== Let A ∈M n×n ( F ). Then a scalar λ is an eigenvalue of A if and only if det( A -λ I n )=0       \color   B  l  u   e   5.3      \color  B  l  u  e  5.3    {\color{Blue}~{}5.3}   characteristic polynomial  Let A∈Mn×n( F ). (a) The characteristic polynomial of A is a polynomial of degree n with leading coefficient(-1)n. (b) A has at most n distinct eigenvalues.       \color   B  l  u   e   5.4      \color  B  l  u  e  5.4    {\color{Blue}~{}5.4}   υ to λ⇔υ∈N(T-λI)  Let T be a linear operator on a vector space V, and let λ be an eigenvalue of T. A vector υ∈V is an eigenvector of T corresponding to λ if and only if υ≠0 and υ∈N(T-λI).       \color   B  l  u   e   5.5      \color  B  l  u  e  5.5    {\color{Blue}~{}5.5}   vi to λi⇔vi is linearly independent  Let T be a linear operator on a vector space V, and let      λ  1   ,   λ  2   ,  …  ,   λ  k    ,      subscript  λ  1    subscript  λ  2   normal-…   subscript  λ  k     \lambda_{1},\lambda_{2},\ldots,\lambda_{k},   be distinct eigenvalues of T. If     v  1   ,   v  2   ,  …  ,   v  k       subscript  v  1    subscript  v  2   normal-…   subscript  v  k     v_{1},v_{2},\ldots,v_{k}   are eigenvectors of t such that    λ  i     subscript  λ  i    \lambda_{i}   corresponds to    v  i     subscript  v  i    v_{i}   (    1  ≤  i  ≤  k        1  i       k     1\leq i\leq k   ), then {     v  1   ,   v  2   ,  …  ,   v  k       subscript  v  1    subscript  v  2   normal-…   subscript  v  k     v_{1},v_{2},\ldots,v_{k}   } is linearly independent.       \color   B  l  u   e   5.6      \color  B  l  u  e  5.6    {\color{Blue}~{}5.6}   characteristic polynomial splits  The characteristic polynomial of any diagonalizable linear operator splits.       \color   B  l  u   e   5.7      \color  B  l  u  e  5.7    {\color{Blue}~{}5.7}   1 ≤ dim(E λ ) ≤ m  Let T be alinear operator on a finite-dimensional vectorspace V, and let λ be an eigenvalue of T having multiplicity   m   m   m   . Then    1  ≤   dim   (   E  λ   )    ≤  m        1   dimension   subscript  E  λ         m     1\leq\dim(E_{\lambda})\leq m   .  ===     \color   B  l  u   e   5.8      \color  B  l  u  e  5.8    {\color{Blue}~{}5.8}    S = S 1 ∪ S 2 ∪ ...∪ S k is linearly independent=== Let T be a linear operator on a vector space V, and let      λ  1   ,   λ  2   ,  …  ,   λ  k    ,      subscript  λ  1    subscript  λ  2   normal-…   subscript  λ  k     \lambda_{1},\lambda_{2},\ldots,\lambda_{k},   be distinct eigenvalues of T. For each     i  =   1  ,  2  ,  …  ,  k    ,      i   1  2  normal-…  k     i=1,2,\ldots,k,   let    S  i     subscript  S  i    S_{i}   be a finite linearly independent subset of the eigenspace    E   λ  i      subscript  E   subscript  λ  i     E_{\lambda_{i}}   . Then    S  =    S  1   ∪   S  2   ∪  ⋯  ∪   S  k        S     subscript  S  1    subscript  S  2   normal-⋯   subscript  S  k      S=S_{1}\cup S_{2}\cup\cdots\cup S_{k}   is a linearly independent subset of V.       \color   B  l  u   e   5.9      \color  B  l  u  e  5.9    {\color{Blue}~{}5.9}   ⇔T is diagonalizable  Let T be a linear operator on a finite-dimensional vector space V that the characteristic polynomial of T splits. Let     λ  1   ,   λ  2   ,  …  ,   λ  k       subscript  λ  1    subscript  λ  2   normal-…   subscript  λ  k     \lambda_{1},\lambda_{2},\ldots,\lambda_{k}   be the distinct eigenvalues of T. Then (a) T is diagonalizable if and only if the multiplicity of    λ  i     subscript  λ  i    \lambda_{i}   is equal to    dim   (   E   λ  i    )      dimension   subscript  E   subscript  λ  i      \dim(E_{\lambda_{i}})   for all   i   i   i   . (b) If T is diagonalizable and    β  i     subscript  β  i    \beta_{i}   is an ordered basis for    E   λ  i      subscript  E   subscript  λ  i     E_{\lambda_{i}}   for each   i   i   i   , then    β  =   β  1   ∪   β  2   ∪  ∪   β  k      fragments  β    subscript  β  1     subscript  β  2      subscript  β  k     \beta=\beta_{1}\cup\beta_{2}\cup\cup\beta_{k}   is an ordered    b  a  s  i   s  2       b  a  s  i   superscript  s  2     basis^{2}   for V consisting of eigenvectors of T.  Test for diagonlization  Inner product spaces  Inner product , standard inner product on F n , conjugate transpose , adjoint , Frobenius inner product , complex/real inner product space , norm , length , conjugate linear , orthogonal , perpendicular , orthogonal , unit vector , orthonormal , normalization .       \color   B  l  u   e   6.1      \color  B  l  u  e  6.1    {\color{Blue}~{}6.1}   properties of linear product  Let V be an inner product space. Then for x,y,z\in V and c \in f, the following staements are true. (a)      ⟨  x  ,   y  +  z   ⟩   =    ⟨  x  ,  y  ⟩   +   ⟨  x  ,  z  ⟩     .       x    y  z       x  y    x  z      \langle x,y+z\rangle=\langle x,y\rangle+\langle x,z\rangle.    (b)      ⟨  x  ,   c  y   ⟩   =    c  ¯    ⟨  x  ,  y  ⟩     .       x    c  y       normal-¯  c    x  y      \langle x,cy\rangle=\bar{c}\langle x,y\rangle.    (c)     ⟨  x  ,  0  ⟩   =   ⟨  0  ,  x  ⟩   =  0.         x  0    0  x        0.     \langle x,\mathit{0}\rangle=\langle\mathit{0},x\rangle=0.    (d)     ⟨  x  ,  x  ⟩   =  0       x  x   0    \langle x,x\rangle=0   if and only if    x  =  0.      x  0.    x=\mathit{0}.    (e) If     ⟨  x  ,  y  ⟩   =   ⟨  x  ,  z  ⟩        x  y    x  z     \langle x,y\rangle=\langle x,z\rangle   for all    x  ∈       x  absent    x\in   V, then    y  =  z      y  z    y=z   .       \color   B  l  u   e   6.2      \color  B  l  u  e  6.2    {\color{Blue}~{}6.2}   law of norm  Let V be an inner product space over F. Then for all x,y\in V and c\in F, the following statements are true. (a)     ∥   c  x   ∥   =    |  c  |   ⋅   ∥  x  ∥         norm    c  x     normal-⋅    c    norm  x      \|cx\|=|c|\cdot\|x\|   . (b)     ∥  x  ∥   =  0       norm  x   0    \|x\|=0   if and only if    x  =  0      x  0    x=0   . In any case,     ∥  x  ∥   ≥  0       norm  x   0    \|x\|\geq 0   . (c)( Cauchy-Schwarz In equality )     |   ⟨  x  ,  y  ⟩   |   ≤    ∥  x  ∥   ⋅   ∥  y  ∥           x  y     normal-⋅   norm  x    norm  y      |\langle x,y\rangle|\leq\|x\|\cdot\|y\|   . (d)( Triangle Inequality )     ∥   x  +  y   ∥   ≤    ∥  x  ∥   +   ∥  y  ∥         norm    x  y       norm  x    norm  y      \|x+y\|\leq\|x\|+\|y\|   .  orthonormal basis , Gram–Schmidt process , Fourier coefficients , orthogonal complement , orthogonal projection       \color   B  l  u   e   6.3      \color  B  l  u  e  6.3    {\color{Blue}~{}6.3}   span of orthogonal subset  Let V be an inner product space and    S  =   {   v  1   ,   v  2   ,  …  ,   v  k   }       S    subscript  v  1    subscript  v  2   normal-…   subscript  v  k      S=\{v_{1},v_{2},\ldots,v_{k}\}   be an orthogonal subset of V consisting of nonzero vectors. If   y   y   y   ∈span(S), then :::::    y  =    ∑   i  =  1   n      ⟨  y  ,   v  i   ⟩     ∥   v  i   ∥   2     v  i         y    superscript   subscript     i  1    n        y   subscript  v  i     superscript   norm   subscript  v  i    2     subscript  v  i       y=\sum_{i=1}^{n}{\langle y,v_{i}\rangle\over\|v_{i}\|^{2}}v_{i}          \color   B  l  u   e   6.4      \color  B  l  u  e  6.4    {\color{Blue}~{}6.4}   Gram-Schmidt process  Let V be an inner product space and S=    {   w  1   ,   w  2   ,  …  ,   w  n   }      subscript  w  1    subscript  w  2   normal-…   subscript  w  n     \{w_{1},w_{2},\ldots,w_{n}\}   be a linearly independent subset of V. DefineS'=    {   v  1   ,   v  2   ,  …  ,   v  n   }      subscript  v  1    subscript  v  2   normal-…   subscript  v  n     \{v_{1},v_{2},\ldots,v_{n}\}   , where     v  1   =   w  1        subscript  v  1    subscript  w  1     v_{1}=w_{1}   and :::::     v  k   =    w  k   -    ∑   j  =  1    k  -  1       ⟨   w  k   ,   v  j   ⟩     ∥   v  j   ∥   2     v  j           subscript  v  k      subscript  w  k     superscript   subscript     j  1      k  1          subscript  w  k    subscript  v  j     superscript   norm   subscript  v  j    2     subscript  v  j        v_{k}=w_{k}-\sum_{j=1}^{k-1}{\langle w_{k},v_{j}\rangle\over\|v_{j}\|^{2}}v_{j}   Then S' is an orhtogonal set of nonzero vectors such that span(S')=span(S).       \color   B  l  u   e   6.5      \color  B  l  u  e  6.5    {\color{Blue}~{}6.5}   orthonormal basis  Let V be a nonzero finite-dimensional inner product space. Then V has an orthonormal basis β. Furthermore, if β =    {   v  1   ,   v  2   ,  …  ,   v  n   }      subscript  v  1    subscript  v  2   normal-…   subscript  v  n     \{v_{1},v_{2},\ldots,v_{n}\}   and x∈V, then :::::    x  =    ∑   i  =  1   n     ⟨  x  ,   v  i   ⟩    v  i         x    superscript   subscript     i  1    n      x   subscript  v  i     subscript  v  i       x=\sum_{i=1}^{n}\langle x,v_{i}\rangle v_{i}   .  Corollary. Let V be a finite-dimensional inner product space with an orthonormal basis β =    {   v  1   ,   v  2   ,  …  ,   v  n   }      subscript  v  1    subscript  v  2   normal-…   subscript  v  n     \{v_{1},v_{2},\ldots,v_{n}\}   . Let T be a linear operator on V, and let A=[T] β . Then for any   i   i   i   and   j   j   j   ,     A   i  j    =   ⟨   T   (   v  j   )    ,   v  i   ⟩        subscript  A    i  j       T   subscript  v  j     subscript  v  i      A_{ij}=\langle T(v_{j}),v_{i}\rangle   .       \color   B  l  u   e   6.6      \color  B  l  u  e  6.6    {\color{Blue}~{}6.6}   W ⊥ by orthonormal basis  Let W be a finite-dimensional subspace of an inner product space V, and let   y   y   y   ∈V. Then there exist unique vectors   u   u   u   ∈W and   z   z   z   ∈W ⊥ such that    y  =   u  +  z       y    u  z     y=u+z   . Furthermore, if    {   v  1   ,   v  2   ,  …  ,   v  k   }      subscript  v  1    subscript  v  2   normal-…   subscript  v  k     \{v_{1},v_{2},\ldots,v_{k}\}   is an orthornormal basis for W, then :::::    u  =    ∑   i  =  1   k     ⟨  y  ,   v  i   ⟩    v  i         u    superscript   subscript     i  1    k      y   subscript  v  i     subscript  v  i       u=\sum_{i=1}^{k}\langle y,v_{i}\rangle v_{i}   . S=\{v_1,v_2,\ldots,v_k\} Corollary. In the notation of Theorem 6.6, the vector   u   u   u   is the unique vector in W that is "closest" to   y   y   y   ; thet is, for any   x   x   x   ∈W,     ∥   y  -  x   ∥   ≥   ∥   y  -  u   ∥        norm    y  x     norm    y  u      \|y-x\|\geq\|y-u\|   , and this inequality is an equality if and onlly if    x  =  u      x  u    x=u   .       \color   B  l  u   e   6.7      \color  B  l  u  e  6.7    {\color{Blue}~{}6.7}   properties of orthonormal set  Suppose that    S  =   {   v  1   ,   v  2   ,  …  ,   v  k   }       S    subscript  v  1    subscript  v  2   normal-…   subscript  v  k      S=\{v_{1},v_{2},\ldots,v_{k}\}   is an orthonormal set in an   n   n   n   -dimensional inner product space V. Than (a) S can be extended to an orthonormal basis    {   v  1   ,   v  2   ,  …  ,   v  k   ,   v   k  +  1    ,  …  ,   v  n   }      subscript  v  1    subscript  v  2   normal-…   subscript  v  k    subscript  v    k  1    normal-…   subscript  v  n     \{v_{1},v_{2},\ldots,v_{k},v_{k+1},\ldots,v_{n}\}   for V. (b) If W=span(S), then     S  1   =   {   v   k  +  1    ,   v   k  +  2    ,  …  ,   v  n   }        subscript  S  1     subscript  v    k  1     subscript  v    k  2    normal-…   subscript  v  n      S_{1}=\{v_{k+1},v_{k+2},\ldots,v_{n}\}   is an orhtonormal basis for W ⊥ (using the preceding notation). (c) If W is any subspace of V, then dim(V)=dim(W)+dim(W ⊥ ).  Least squares approximation , Minimal solutions to systems of linear equations       \color   B  l  u   e   6.8      \color  B  l  u  e  6.8    {\color{Blue}~{}6.8}   linear functional representation inner product  Let V be a finite-dimensional inner product space over F, and let   g   g   g   :V→F be a linear transformation. Then there exists a unique vector   y   y   y   ∈ V such that     g   (  x  )    =   ⟨  x  ,  y  ⟩         normal-g  normal-x    normal-x  normal-y     \rm{g}(x)=\langle x,y\rangle   for all   x   x   x   ∈ V.       \color   B  l  u   e   6.9      \color  B  l  u  e  6.9    {\color{Blue}~{}6.9}   definition of T*  Let V be a finite-dimensional inner product space, and let T be a linear operator on V. Then there exists a unique function T*:V→V such that     ⟨   T   (  x  )    ,  y  ⟩   =   ⟨  x  ,    T  *    (  y  )    ⟩          normal-T  normal-x   normal-y    normal-x     superscript  normal-T    normal-y      \langle\rm{T}(x),y\rangle=\langle x,\rm{T}^{*}(y)\rangle   for all    x  ,  y     x  y    x,y   ∈ V. Furthermore, T* is linear  ===     \color   B  l  u   e   6.10      \color  B  l  u  e  6.10    {\color{Blue}~{}6.10}   [T*] β =[T]* β === Let V be a finite-dimensional inner product space, and let β be an orthonormal basis for V. If T is a linear operator on V, then ::::      [   T  *   ]   β   =    [  T  ]   β  *        subscript   delimited-[]   superscript  T     β    subscript   superscript   delimited-[]  T     β     [T^{*}]_{\beta}=[T]^{*}_{\beta}   .       \color   B  l  u   e   6.11      \color  B  l  u  e  6.11    {\color{Blue}~{}6.11}   properties of T*  Let V be an inner product space, and let T and U be linear operators onV. Then (a) (T+U)*=T*+U*; (b) (   c   c   c   T)*=    c  ¯     normal-¯  c    \bar{c}   T* for any c∈ F; (c) (TU)*=U*T*; (d) T**=T; (e) I*=I.  Corollary. Let A and B be n×nmatrices. Then (a) ( A + B )*= A *+ B *; (b) (   c   c   c    A )*=    c  ¯     normal-¯  c    \bar{c}    A * for any   c   c   c   ∈ F; (c) ( AB )*= B * A *; (d) A **= A ; (e) I *= I .       \color   B  l  u   e   6.12      \color  B  l  u  e  6.12    {\color{Blue}~{}6.12}   Least squares approximation  Let A ∈ M m×n ( F ) and   y   y   y   ∈F m . Then there exists    x  0     subscript  x  0    x_{0}   ∈ F n such that      (   A  *  A   )    x  0    =   A  *  y           A  A    subscript  x  0      A  y     (A*A)x_{0}=A*y   and     ∥    A   x  0    -  Y   ∥   ≤   ∥    A  x   -  y   ∥        norm      A   subscript  x  0    Y     norm      A  x   y      \|Ax_{0}-Y\|\leq\|Ax-y\|   for all x∈ F n  Lemma 1. let ''A ∈ M m×n ( F''),   x   x   x   ∈F n , and   y   y   y   ∈F m . Then ::::      ⟨   A  x   ,  y  ⟩   m   =    ⟨  x  ,   A  *  y   ⟩   n        subscript     A  x   y   m    subscript   x    A  y    n     \langle Ax,y\rangle_{m}=\langle x,A*y\rangle_{n}     Lemma 2. Let ''A ∈ M m×n ( F ). Then rank( A*A )=rank( A'').  Corollary. (of lemma 2) If A is an m×n matrix such that rank( A )=n, then A*A is invertible.       \color   B  l  u   e   6.13      \color  B  l  u  e  6.13    {\color{Blue}~{}6.13}   Minimal solutions to systems of linear equations  Let ''A ∈ M m×n ( F'') and b∈ F m . Suppose that     A  x   =  b        A  x   b    Ax=b   is consistent. Then the following statements are true. (a) There existes exactly one minimal solution   s   s   s   of     A  x   =  b        A  x   b    Ax=b   , and   s   s   s   ∈R(L A * ). (b) The vector   s   s   s   is the only solution to     A  x   =  b        A  x   b    Ax=b   that lies in R(L A * ); that is, if   u   u   u   satisfies     (  A  A  *  )   u  =  b     fragments   fragments  normal-(  A  A   normal-)   u   b    (AA*)u=b   , then    s  =   A  *  u       s    A  u     s=A*u   .  Canonical forms  References   Linear Algebra 4th edition, by Stephen H. Friedberg Arnold J. Insel and Lawrence E. spence ISBN 7-04-016733-6  Linear Algebra 3rd edition, by Serge Lang (UTM) ISBN 0-387-96412-6  Linear Algebra and Its Applications 4th edition, by Gilbert Strang ISBN 0-03-010567-6   "       