   Wald test      Wald test   The Wald test is a parametric statistical test named after the Hungarian statistician Abraham Wald . Whenever a relationship within or between data items can be expressed as a statistical model with parameters to be estimated from a sample, the Wald test can be used to test the true value of the parameter based on the sample estimate.  Suppose an economist, who has data on social class and shoe size, wonders whether social class is associated with shoe size. Say   Œ∏   Œ∏   \theta   is the average increase in shoe size for upper-class people compared to middle-class people: then the Wald test can be used to test whether   Œ∏   Œ∏   \theta   is 0 (in which case social class has no association with shoe size) or non-zero (shoe size varies between social classes). Here,   Œ∏   Œ∏   \theta   , the hypothetical difference in shoe sizes between upper and middle-class people in the whole population, is a parameter. An estimate of   Œ∏   Œ∏   \theta   might be the difference in shoe size between upper and middle-class people in the sample. In the Wald test, the economist uses the estimate and an estimate of variability (see below) to draw conclusions about the unobserved true   Œ∏   Œ∏   \theta   . Or, for a medical example, suppose smoking multiplies the risk of lung cancer by some number R : then the Wald test can be used to test whether R =¬†1 (i.e. there is no effect of smoking) or is greater (or less) than 1 (i.e. smoking alters risk).  A Wald test can be used in a great variety of different models including models for dichotomous variables and models for continuous variables . 1  Mathematical details  Under the Wald statistical test, the maximum likelihood estimate    Œ∏  ^     normal-^  Œ∏    \hat{\theta}   of the parameter(s) of interest   Œ∏   Œ∏   \theta   is compared with the proposed value    Œ∏  0     subscript  Œ∏  0    \theta_{0}   , with the assumption that the difference between the two will be approximately normally distributed . Typically the square of the difference is compared to a chi-squared distribution .  Test on a single parameter  In the univariate case, the Wald statistic is        (    Œ∏  ^   -   Œ∏  0    )   2    var   (   Œ∏  ^   )         superscript     normal-^  Œ∏    subscript  Œ∏  0    2    var   normal-^  Œ∏      \frac{(\widehat{\theta}-\theta_{0})^{2}}{\operatorname{var}(\hat{\theta})}     which is compared against a chi-squared distribution .  Alternatively, the difference can be compared to a normal distribution . In this case the test statistic is        Œ∏  ^   -   Œ∏  0     se   (   Œ∏  ^   )           normal-^  Œ∏    subscript  Œ∏  0     se   normal-^  Œ∏      \frac{\widehat{\theta}-\theta_{0}}{\operatorname{se}(\hat{\theta})}     where    se   (   Œ∏  ^   )      se   normal-^  Œ∏     \operatorname{se}(\widehat{\theta})   is the standard error of the maximum likelihood estimate (MLE). A reasonable estimate of the standard error for the MLE can be given by    1     I  n    (   M  L  E   )         1       subscript  I  n     M  L  E       \frac{1}{\sqrt{I_{n}(MLE)}}   , where    I  n     subscript  I  n    I_{n}   is the Fisher information of the parameter.  Test(s) on multiple parameters  The Wald test can be used to test a single hypothesis on multiple parameters, as well as to test jointly multiple hypotheses on single/multiple parameters. Let     Œ∏  ^   n     subscript   normal-^  Œ∏   n    \hat{\theta}_{n}   be our sample estimator of P parameters (i.e,     Œ∏  ^   n     subscript   normal-^  Œ∏   n    \hat{\theta}_{n}   is a Px1 vector), which is supposed to follow asymptotically a normal distribution with covariance matrix V,      n    (     Œ∏  ^   n   -  Œ∏   )     ‚Üí  ùíü    N   (  0  ,  V  )        ùíü  normal-‚Üí       n      subscript   normal-^  Œ∏   n   Œ∏      N   0  V      \sqrt{n}(\hat{\theta}_{n}-\theta)\xrightarrow{\mathcal{D}}N(0,V)   . The test of Q hypotheses on the P parameters is expressed with a Q x P matrix R:       H  0   :    R  Œ∏   =  r      normal-:   subscript  H  0       R  Œ∏   r     H_{0}:R\theta=r          H  1   :    R  Œ∏   ‚â†  r      normal-:   subscript  H  1       R  Œ∏   r     H_{1}:R\theta\neq r     The test statistic is:         (    R    Œ∏  ^   n    -  r   )     ‚Ä≤      [   R   (     V  ^   n   /  N   )    R    ‚Ä≤     ]    -  1     (    R    Œ∏  ^   n    -  r   )     ‚Üí  ùíü     \Chi   Q  2         superscript      R   subscript   normal-^  Œ∏   n    r    normal-‚Ä≤     superscript   delimited-[]    R     subscript   normal-^  V   n   N    superscript  R   normal-‚Ä≤        1        R   subscript   normal-^  Œ∏   n    r     ùíü  normal-‚Üí    subscript   superscript  \Chi  2   Q     (R\hat{\theta}_{n}-r)^{{}^{\prime}}[R(\hat{V}_{n}/N)R^{{}^{\prime}}]^{-1}(R%
 \hat{\theta}_{n}-r)\quad\xrightarrow{\mathcal{D}}\quad\Chi^{2}_{Q}     where     V  ^   n     subscript   normal-^  V   n    \hat{V}_{n}   is an estimator of the covariance matrix. 2 Suppose      n    (     Œ∏  ^   n   -  Œ∏   )     ‚Üí  ùíü    N   (  0  ,  V  )        ùíü  normal-‚Üí       n      subscript   normal-^  Œ∏   n   Œ∏      N   0  V      \sqrt{n}(\hat{\theta}_{n}-\theta)\xrightarrow{\mathcal{D}}N(0,V)   . Then, by Slutsky's theorem and by the properties of the normal distribution , multiplying by R has distribution:       R   n    (     Œ∏  ^   n   -  Œ∏   )    =    n    (    R    Œ∏  ^   n    -  r   )     ‚Üí  ùíü    N   (  0  ,   R  V   R    ‚Ä≤     )            R    n      subscript   normal-^  Œ∏   n   Œ∏        n       R   subscript   normal-^  Œ∏   n    r       ùíü  normal-‚Üí       N   0    R  V   superscript  R   normal-‚Ä≤          R\sqrt{n}(\hat{\theta}_{n}-\theta)=\sqrt{n}(R\hat{\theta}_{n}-r)\xrightarrow{%
 \mathcal{D}}N(0,RVR^{{}^{\prime}})     Recalling that a quadratic form of normal distribution has a Chi-squared distribution :        n     (    R    Œ∏  ^   n    -  r   )     ‚Ä≤      [   R  V   R    ‚Ä≤     ]    -  1     n    (    R    Œ∏  ^   n    -  r   )     ‚Üí  ùíü     \Chi   Q  2       ùíü  normal-‚Üí       n    superscript      R   subscript   normal-^  Œ∏   n    r    normal-‚Ä≤     superscript   delimited-[]    R  V   superscript  R   normal-‚Ä≤        1      n       R   subscript   normal-^  Œ∏   n    r     subscript   superscript  \Chi  2   Q     \sqrt{n}(R\hat{\theta}_{n}-r)^{{}^{\prime}}[RVR^{{}^{\prime}}]^{-1}\sqrt{n}(R%
 \hat{\theta}_{n}-r)\xrightarrow{\mathcal{D}}\Chi^{2}_{Q}     Rearranging n finally gives:         (    R   Œ∏  n    -  r   )     ‚Ä≤      [   R   (   V  /  N   )    R    ‚Ä≤     ]    -  1     (    R   Œ∏  n    -  r   )     ‚Üí  ùíü     \Chi   Q  2         superscript      R   subscript  Œ∏  n    r    normal-‚Ä≤     superscript   delimited-[]    R    V  N    superscript  R   normal-‚Ä≤        1        R   subscript  Œ∏  n    r     ùíü  normal-‚Üí    subscript   superscript  \Chi  2   Q     (R\theta_{n}-r)^{{}^{\prime}}[R(V/N)R^{{}^{\prime}}]^{-1}(R\theta_{n}-r)\quad%
 \xrightarrow{\mathcal{D}}\quad\Chi^{2}_{Q}     What if the covariance matrix is not known a-priori and needs to be estimated from the data? If we have a consistent estimator      V  ^   n     subscript   normal-^  V   n    \hat{V}_{n}   of   V   V   V   , then again by Slutsky's theorem , we have:         (    R   Œ∏  n    -  r   )     ‚Ä≤      [   R   (     V  ^   n   /  N   )    R    ‚Ä≤     ]    -  1     (    R   Œ∏  n    -  r   )     ‚Üí  ùíü     \Chi   Q  2         superscript      R   subscript  Œ∏  n    r    normal-‚Ä≤     superscript   delimited-[]    R     subscript   normal-^  V   n   N    superscript  R   normal-‚Ä≤        1        R   subscript  Œ∏  n    r     ùíü  normal-‚Üí    subscript   superscript  \Chi  2   Q     (R\theta_{n}-r)^{{}^{\prime}}[R(\hat{V}_{n}/N)R^{{}^{\prime}}]^{-1}(R\theta_{n%
 }-r)\quad\xrightarrow{\mathcal{D}}\quad\Chi^{2}_{Q}     Nonlinear hypothesis  In the standard form, the Wald test is used to test linear hypotheses, that can be represented by a single matrix R. If one wishes to test a non-linear hypothesis of the form:       H  0   :    c   (  Œ∏  )    =  0      normal-:   subscript  H  0       c  Œ∏   0     H_{0}:c(\theta)=0          H  1   :    c   (  Œ∏  )    ‚â†  0      normal-:   subscript  H  1       c  Œ∏   0     H_{1}:c(\theta)\neq 0     The test statistic becomes:       c    (    Œ∏  ^   n   )     ‚Ä≤      [    c    ‚Ä≤     (    Œ∏  ^   n   )    (     V  ^   n   /  N   )    c    ‚Ä≤      (    Œ∏  ^   n   )     ‚Ä≤     ]    -  1    c   (    Œ∏  ^   n   )     ‚Üí  ùíü     \Chi   Q  2        c   superscript   subscript   normal-^  Œ∏   n    normal-‚Ä≤     superscript   delimited-[]     superscript  c   normal-‚Ä≤     subscript   normal-^  Œ∏   n      subscript   normal-^  V   n   N    superscript  c   normal-‚Ä≤     superscript   subscript   normal-^  Œ∏   n    normal-‚Ä≤        1    c   subscript   normal-^  Œ∏   n     ùíü  normal-‚Üí    subscript   superscript  \Chi  2   Q     c(\hat{\theta}_{n})^{{}^{\prime}}[c^{{}^{\prime}}(\hat{\theta}_{n})(\hat{V}_{n%
 }/N)c^{{}^{\prime}}(\hat{\theta}_{n})^{{}^{\prime}}]^{-1}c(\hat{\theta}_{n})%
 \quad\xrightarrow{\mathcal{D}}\quad\Chi^{2}_{Q}     where     c    ‚Ä≤     (    Œ∏  ^   n   )        superscript  c   normal-‚Ä≤     subscript   normal-^  Œ∏   n     c^{{}^{\prime}}(\hat{\theta}_{n})   is the derivative of c evaluated at the sample estimator. This result is obtained using the delta method , which uses a first order approximation of the variance.  Non-invariance to re-parametrisations  The fact that one uses an approximation of the variance has the drawback that the Wald statistic is not-invariant to a non-linear transformation/reparametrisation of the hypothesis: it can give different answers to the same question, depending on how the question is phrased. 3 For example, asking whether R =¬†1 is the same as asking whether log R =¬†0; but the Wald statistic for R =¬†1 is not the same as the Wald statistic for log R =¬†0 (because there is in general no neat relationship between the standard errors of R and log R , so it needs to be approximated).  Alternatives to the Wald test  There exist several alternatives to the Wald test, namely the likelihood-ratio test and the Lagrange multiplier test (also known as the score test). Robert F. Engle showed that these three tests, the Wald test, the likelihood-ratio test and the Lagrange multiplier test are asymptotically equivalent . 4 Although they are asymptotically equivalent, in finite samples, they could disagree enough to lead to different conclusions.  There are several reasons to prefer the likelihood ratio test or the lagrange multiplier to the Wald test: 5 6 7   Non-invariance: As argued above, the Wald test is not invariant to a reparametrization, while the Likelihood ratio tests will give exactly the same answer whether we work with R , log R or any other monotonic transformation of R .  The other reason is that the Wald test uses two approximations (that we know the standard error, and that the distribution is chi-squared ), whereas the likelihood ratio test uses one approximation (that the distribution is chi-squared).  The Wald test requires an estimate under the null hypothesis. In some cases, the model is simpler under the alternative hypothesis, so that one might prefer to use the score test (also called Lagrange Multiplier test), which has the advantage that it can be formulated in situations where the variability is difficult to estimate; e.g. the Cochran‚ÄìMantel‚ÄìHaenzel test is a score test. 8   See also   Chow test  Sequential probability ratio test   References  Further reading        External links   R Package: Wald's Sequential Probability Ratio Test  Wald test on the Earliest known uses of some of the words of mathematics   "  Category:Statistical tests     ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©     