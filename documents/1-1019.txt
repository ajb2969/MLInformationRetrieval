   Tensor      Tensor   (Figure)  Cauchy stress tensor , a second-order tensor. The tensor's components, in a three-dimensional Cartesian coordinate system, form the matrix       σ      =   [       𝐓   (   𝐞  1   )     𝐓   (   𝐞  2   )     𝐓   (   𝐞  3   )        ]           =   [      σ  11      σ  12      σ  13        σ  21      σ  22      σ  23        σ  31      σ  32      σ  33      ]          σ    absent       superscript  𝐓   subscript  𝐞  1     superscript  𝐓   subscript  𝐞  2     superscript  𝐓   subscript  𝐞  3           missing-subexpression     absent     subscript  σ  11    subscript  σ  12    subscript  σ  13      subscript  σ  21    subscript  σ  22    subscript  σ  23      subscript  σ  31    subscript  σ  32    subscript  σ  33         \begin{aligned}\displaystyle\sigma&\displaystyle=\begin{bmatrix}\mathbf{T}^{(%
 \mathbf{e}_{1})}\mathbf{T}^{(\mathbf{e}_{2})}\mathbf{T}^{(\mathbf{e}_{3})}\\
 \end{bmatrix}\\
 &\displaystyle=\begin{bmatrix}\sigma_{11}&\sigma_{12}&\sigma_{13}\\
 \sigma_{21}&\sigma_{22}&\sigma_{23}\\
 \sigma_{31}&\sigma_{32}&\sigma_{33}\end{bmatrix}\\
 \end{aligned}    whose columns are the stresses (forces per unit area) acting on the e 1 , e 2 , and e 3 faces of the cube.   Tensors are geometric objects that describe linear relations between geometric vectors , scalars , and other tensors. Elementary examples of such relations include the dot product , the cross product , and linear maps . Euclidean vectors , often used in physics and engineering applications, and scalars themselves are also tensors. 1 A more sophisticated example is the Cauchy stress tensor  T , which takes a direction v as input and produces the stress T ( v ) on the surface normal to this vector for output, thus expressing a relationship between these two vectors, shown in the figure (right).  In terms of a coordinate basis or fixed frame of reference , a tensor can be represented as an organized multidimensional array of numerical values. The (also degree ) of a tensor is the dimensionality of the array needed to represent it, or equivalently, the number of indices needed to label a component of that array. For example, a linear map is represented by a matrix (a 2-dimensional array) in a basis, and therefore is a 2nd-order tensor. A vector is represented as a 1-dimensional array in a basis, and is a 1st-order tensor. Scalars are single numbers and are thus 0th-order tensors. Because they express a relationship between vectors, tensors themselves must be independent of a particular choice of coordinate system . The coordinate independence of a tensor then takes the form of a "covariant" transformation law that relates the array computed in one coordinate system to that computed in another one. The precise form of the transformation law determines the type (or valence ) of the tensor. The tensor type is a pair of natural numbers  where n is the number of contravariant indices and m is the number of covariant indices. The total order of a tensor is the sum of these two numbers.  Tensors are important in physics because they provide a concise mathematical framework for formulating and solving physics problems in areas such as elasticity, fluid mechanics, and general relativity. Tensors were first conceived by Tullio Levi-Civita and Gregorio Ricci-Curbastro , who continued the earlier work of Bernhard Riemann and Elwin Bruno Christoffel and others, as part of the absolute differential calculus . The concept enabled an alternative formulation of the intrinsic differential geometry of a manifold in the form of the Riemann curvature tensor . 2  Definition  There are several approaches to defining tensors. Although seemingly different, the approaches just describe the same geometric concept using different languages and at different levels of abstraction.  As multidimensional arrays  Just as a vector in an n - dimensional space is represented by a one-dimensional array of length n with respect to a given basis , any tensor with respect to a basis is represented by a multidimensional array. For example, a linear transformation is represented in a basis as a two-dimensional square n × n array. The numbers in the multidimensional array are known as the scalar components of the tensor or simply its components . They are denoted by indices giving their position in the array, as subscripts and superscripts , following the symbolic name of the tensor. For example, the components of an order 2 tensor T could be denoted T ij , where i and j are indices running from 1 to n , or also by T i j . Whether an index is displayed as a superscript or subscript depends on the transformation properties of the tensor, described below. The total number of indices required to uniquely select each component is equal to the dimension of the array, and is called the order , degree or rank of the tensor. However, the term "rank" generally has another meaning in the context of matrices and tensors.  Just as the components of a vector change when we change the basis of the vector space, the components of a tensor also change under such a transformation. Each tensor comes equipped with a transformation law that details how the components of the tensor respond to a change of basis . The components of a vector can respond in two distinct ways to a change of basis (see covariance and contravariance of vectors ), where the new basis vectors     𝐞  ^   i     subscript   normal-^  𝐞   i    \mathbf{\hat{e}}_{i}   are expressed in terms of the old basis vectors    𝐞  j     subscript  𝐞  j    \mathbf{e}_{j}   as,         𝐞  ^   i   =    ∑   j  =  1   n     R  i  j    𝐞  j     =    R  i  j    𝐞  j     .         subscript   normal-^  𝐞   i     superscript   subscript     j  1    n      subscript   superscript  R  j   i    subscript  𝐞  j             subscript   superscript  R  j   i    subscript  𝐞  j       \mathbf{\hat{e}}_{i}=\sum_{j=1}^{n}R^{j}_{i}\mathbf{e}_{j}=R^{j}_{i}\mathbf{e}%
 _{j}.   Here R i '' j'' are the entries of the change of basis matrix, and in the second expression the summation sign was suppressed: this is the Einstein summation convention , which will be used throughout this article. 3 The components v i of a column vector v transform with the inverse of the matrix R ,         v  ^   i   =     (   R   -  1    )   j  i    v  j     ,       superscript   normal-^  v   i      subscript   superscript   superscript  R    1    i   j    superscript  v  j      \hat{v}^{i}=(R^{-1})^{i}_{j}v^{j},   where the hat denotes the components in the new basis. This is called a contravariant transformation law, because the vector transforms by the inverse of the change of basis. In contrast, the components, w i , of a covector (or row vector), w transform with the matrix R itself,         w  ^   i   =    R  i  j    w  j     .       subscript   normal-^  w   i      superscript   subscript  R  i   j    subscript  w  j      \hat{w}_{i}=R_{i}^{j}w_{j}.   This is called a covariant transformation law, because the covector transforms by the same matrix as the change of basis matrix. The components of a more general tensor transform by some combination of covariant and contravariant transformations, with one transformation law for each index. If the transformation matrix of an index is the inverse matrix of the basis transformation, then the index is called contravariant and is traditionally denoted with an upper index (superscript). If the transformation matrix of an index is the basis transformation itself, then the index is called covariant and is denoted with a lower index (subscript).  The transformation law for an order  tensor with p contravariant indices and q covariant indices is thus given as,         T  ^     j  1  ′   ,  …  ,   j  q  ′      i  1  ′   ,  …  ,   i  p  ′     =     (   R   -  1    )    i  1    i  1  ′    ⋯    (   R   -  1    )    i  p    i  p  ′     R   j  1  ′    j  1    ⋯   R   j  q  ′    j  q     T    j  1   ,  …  ,   j  q      i  1   ,  …  ,   i  p       .       subscript   superscript   normal-^  T     subscript   superscript  i  normal-′   1   normal-…   subscript   superscript  i  normal-′   p       subscript   superscript  j  normal-′   1   normal-…   subscript   superscript  j  normal-′   q        subscript   superscript   superscript  R    1     subscript   superscript  i  normal-′   1     subscript  i  1    normal-⋯   subscript   superscript   superscript  R    1     subscript   superscript  i  normal-′   p     subscript  i  p     subscript   superscript  R   subscript  j  1     subscript   superscript  j  normal-′   1    normal-⋯   subscript   superscript  R   subscript  j  q     subscript   superscript  j  normal-′   q     subscript   superscript  T    subscript  i  1   normal-…   subscript  i  p       subscript  j  1   normal-…   subscript  j  q        \hat{T}^{i^{\prime}_{1},\ldots,i^{\prime}_{p}}_{j^{\prime}_{1},\ldots,j^{%
 \prime}_{q}}=(R^{-1})^{i^{\prime}_{1}}_{i_{1}}\cdots(R^{-1})^{i^{\prime}_{p}}_%
 {i_{p}}R^{j_{1}}_{j^{\prime}_{1}}\cdots R^{j_{q}}_{j^{\prime}_{q}}T^{i_{1},%
 \ldots,i_{p}}_{j_{1},\ldots,j_{q}}.   Here the primed indices denote components in the new coordinates, and the unprimed indices denote the components in the old coordinates. Such a tensor is said to be of order or type . The terms "order", "type", "rank", "valence", and "degree" are all sometimes used for the same concept. Here, the term "order" or "total order" will be used for the total dimension of the array (or its generalisation in other definitions), p + q in the preceding example, and the term "type" for the pair giving the number contravariant and covariant indices. A tensor of type  is also called as a -tensor for short.  As an example, the matrix of a linear transformation in a basis is a rectangular array   T   T   T   that transforms under a change of basis R by     T  ^   =    R   -  1    T  R        normal-^  T      superscript  R    1    T  R     \hat{T}=R^{-1}TR   . In terms of the individual matrix entries, this transformation law has the form        T  ^    i  2  ′    i  1  ′    =     (   R   -  1    )    i  1    i  1  ′     T   i  2    i  1     R   i  2  ′    i  2          superscript   subscript   normal-^  T    subscript   superscript  i  normal-′   2     subscript   superscript  i  normal-′   1       superscript   subscript   superscript  R    1     subscript  i  1     subscript   superscript  i  normal-′   1     superscript   subscript  T   subscript  i  2     subscript  i  1     superscript   subscript  R   subscript   superscript  i  normal-′   2     subscript  i  2       \hat{T}_{i^{\prime}_{2}}^{i^{\prime}_{1}}=(R^{-1})_{i_{1}}^{i^{\prime}_{1}}T_{%
 i_{2}}^{i_{1}}R_{i^{\prime}_{2}}^{i_{2}}   so the tensor corresponding to the matrix of a linear transformation has one covariant and one contravariant index: it has type (1,1). A linear transformation itself does not actually depend on a basis: it is just a linear map that accepts a vector as an argument and produces another vector. The transformation law for the matrix of a linear transformation is consistent with the transformation law for a contravariant vector, so that the action of a linear transformation on a contravariant vector is represented in coordinates as the matrix product of their respective coordinate representations. That is, the components     (   T  v   )   i     superscript    T  v   i    (Tv)^{i}   are given by      (   T  v   )   i   =    T  j  i    v  j         superscript    T  v   i      subscript   superscript  T  i   j    superscript  v  j      (Tv)^{i}=T^{i}_{j}v^{j}   . These components transform contravariantly, since        (    T  v   ^   )    i  ′    =    T  ^    j  ′    i  ′      v  ^    j  ′    =   [    (   R   -  1    )   i   i  ′     T  j  i    R   j  ′   j   ]    [    (   R   -  1    )   j   j  ′     v  j   )   ]  =  (   R   -  1    )    i      i  ′    (  T  v  )    i   .     fragments   superscript   fragments  normal-(   normal-^    T  v    normal-)    superscript  i  normal-′      superscript   subscript   normal-^  T    superscript  j  normal-′     superscript  i  normal-′     superscript   normal-^  v    superscript  j  normal-′      fragments  normal-[   superscript   subscript   fragments  normal-(   superscript  R    1    normal-)   i    superscript  i  normal-′     superscript   subscript  T  j   i    superscript   subscript  R   superscript  j  normal-′    j   normal-]    fragments  normal-[   subscript   superscript   fragments  normal-(   superscript  R    1    normal-)    superscript  j  normal-′    j    superscript  v  j   normal-)   normal-]   normal-(   superscript  R    1    normal-)   i     superscript  i  normal-′    normal-(  T  v  normal-)   i   normal-.    (\widehat{Tv})^{i^{\prime}}=\hat{T}_{j^{\prime}}^{i^{\prime}}\hat{v}^{j^{%
 \prime}}=\left[(R^{-1})_{i}^{i^{\prime}}T_{j}^{i}R_{j^{\prime}}^{j}\right]%
 \left[(R^{-1})^{j^{\prime}}_{j}v^{j})\right]=(R^{-1})_{i}^{i^{\prime}}(Tv)^{i}.     This discussion motivates the following formal definition: 4  [\mathbf{f}] to each basis ( e 1 ,..., e n )}} of a fixed n -dimensional vector space such that, if we apply the change of basis      𝐟  ↦   𝐟  ⋅  R   =   (    R  1  i    𝐞  i    ,  …  ,    R  n  i    𝐞  i    )        maps-to  𝐟   normal-⋅  𝐟  R            superscript   subscript  R  1   i    subscript  𝐞  i    normal-…     superscript   subscript  R  n   i    subscript  𝐞  i        \mathbf{f}\mapsto\mathbf{f}\cdot R=\left(R_{1}^{i}\mathbf{e}_{i},\dots,R_{n}^{%
 i}\mathbf{e}_{i}\right)   then the multidimensional array obeys the transformation law         T    j  1  ′   …   j  q  ′      i  1  ′   …   i  p  ′      [   𝐟  ⋅  R   ]    =     (   R   -  1    )    i  1    i  1  ′    ⋯    (   R   -  1    )    i  p    i  p  ′     R   j  1  ′    j  1    ⋯   R   j  q  ′    j  q     T    j  1   ,  …  ,   j  q      i  1   ,  …  ,   i  p      [  𝐟  ]     .         subscript   superscript  T     subscript   superscript  i  normal-′   1   normal-…   subscript   superscript  i  normal-′   p        subscript   superscript  j  normal-′   1   normal-…   subscript   superscript  j  normal-′   q      delimited-[]   normal-⋅  𝐟  R        subscript   superscript   superscript  R    1     subscript   superscript  i  normal-′   1     subscript  i  1    normal-⋯   subscript   superscript   superscript  R    1     subscript   superscript  i  normal-′   p     subscript  i  p     subscript   superscript  R   subscript  j  1     subscript   superscript  j  normal-′   1    normal-⋯   subscript   superscript  R   subscript  j  q     subscript   superscript  j  normal-′   q     subscript   superscript  T    subscript  i  1   normal-…   subscript  i  p       subscript  j  1   normal-…   subscript  j  q      delimited-[]  𝐟      T^{i^{\prime}_{1}\dots i^{\prime}_{p}}_{j^{\prime}_{1}\dots j^{\prime}_{q}}[%
 \mathbf{f}\cdot R]=(R^{-1})^{i^{\prime}_{1}}_{i_{1}}\cdots(R^{-1})^{i^{\prime}%
 _{p}}_{i_{p}}R^{j_{1}}_{j^{\prime}_{1}}\cdots R^{j_{q}}_{j^{\prime}_{q}}T^{i_{%
 1},\ldots,i_{p}}_{j_{1},\ldots,j_{q}}[\mathbf{f}].   }}  The definition of a tensor as a multidimensional array satisfying a transformation law traces back to the work of Ricci. 5 Nowadays, this definition is still used in some physics and engineering text books. 6 7  Tensor fields  In many applications, especially in differential geometry and physics, it is natural to consider a tensor with components that are functions of the point in a space. This was the setting of Ricci's original work. In modern mathematical terminology such an object is called a tensor field , often referred to simply as a tensor. 8  In this context, a coordinate basis is often chosen for the tangent vector space . The transformation law may then be expressed in terms of partial derivatives of the coordinate functions,         x  ¯   i    (   x  1   ,  …  ,   x  n   )    ,       superscript   normal-¯  x   i     superscript  x  1   normal-…   superscript  x  n      \bar{x}^{i}(x^{1},\ldots,x^{n}),   defining a coordinate transformation, 9          T  ^     j  1  ′   …   j  q  ′      i  1  ′   …   i  p  ′      (    x  ¯   1   ,  …  ,    x  ¯   n   )    =     ∂    x  ¯    i  1  ′      ∂   x   i  1      ⋯    ∂    x  ¯    i  p  ′      ∂   x   i  p        ∂   x   j  1      ∂    x  ¯    j  1  ′      ⋯    ∂   x   j  q      ∂    x  ¯    j  q  ′       T    j  1   …   j  q      i  1   …   i  p      (   x  1   ,  …  ,   x  n   )     .         subscript   superscript   normal-^  T      subscript   superscript  i  normal-′   1   normal-…   subscript   superscript  i  normal-′   p        subscript   superscript  j  normal-′   1   normal-…   subscript   superscript  j  normal-′   q       superscript   normal-¯  x   1   normal-…   superscript   normal-¯  x   n            superscript   normal-¯  x    subscript   superscript  i  normal-′   1        superscript  x   subscript  i  1      normal-⋯       superscript   normal-¯  x    subscript   superscript  i  normal-′   p        superscript  x   subscript  i  p           superscript  x   subscript  j  1        superscript   normal-¯  x    subscript   superscript  j  normal-′   1      normal-⋯       superscript  x   subscript  j  q        superscript   normal-¯  x    subscript   superscript  j  normal-′   q       subscript   superscript  T     subscript  i  1   normal-…   subscript  i  p        subscript  j  1   normal-…   subscript  j  q       superscript  x  1   normal-…   superscript  x  n       \hat{T}^{i^{\prime}_{1}\dots i^{\prime}_{p}}_{j^{\prime}_{1}\dots j^{\prime}_{%
 q}}(\bar{x}^{1},\ldots,\bar{x}^{n})=\frac{\partial\bar{x}^{i^{\prime}_{1}}}{%
 \partial x^{i_{1}}}\cdots\frac{\partial\bar{x}^{i^{\prime}_{p}}}{\partial x^{i%
 _{p}}}\frac{\partial x^{j_{1}}}{\partial\bar{x}^{j^{\prime}_{1}}}\cdots\frac{%
 \partial x^{j_{q}}}{\partial\bar{x}^{j^{\prime}_{q}}}T^{i_{1}\dots i_{p}}_{j_{%
 1}\dots j_{q}}(x^{1},\ldots,x^{n}).     As multilinear maps  A downside to the definition of a tensor using the multidimensional array approach is that it is not apparent from the definition that the defined object is indeed basis independent, as is expected from an intrinsically geometric object. Although it is possible to show that transformation laws indeed ensure independence from the basis, sometimes a more intrinsic definition is preferred. One approach 10 is to define a tensor as a multilinear map . In that approach a type  tensor T is defined as a map,       T  :        V  *   ×  …  ×   V  *    ⏟    p  copies    ×     V  ×  …  ×  V   ⏟    q  copies     →  𝐑    ,     normal-:  T   normal-→     subscript   normal-⏟     superscript  V    normal-…   superscript  V        p  copies     subscript   normal-⏟    V  normal-…  V      q  copies     𝐑     T:\underbrace{V^{*}\times\dots\times V^{*}}_{p\text{ copies}}\times\underbrace%
 {V\times\dots\times V}_{q\text{ copies}}\rightarrow\mathbf{R},   where V is a (finite-dimensional) vector space and V * is the corresponding dual space of covectors, which is linear in each of its arguments.  By applying a multilinear map T of type  to a basis { e j } for V and a canonical cobasis { ε i } for V *,        T    j  1   …   j  q      i  1   …   i  p     ≡   T   (   ε   i  1    ,  …  ,   ε   i  p    ,   𝐞   j  1    ,  …  ,   𝐞   j  q    )     ,       subscript   superscript  T     subscript  i  1   normal-…   subscript  i  p        subscript  j  1   normal-…   subscript  j  q       T    superscript  ε   subscript  i  1    normal-…   superscript  ε   subscript  i  p     subscript  𝐞   subscript  j  1    normal-…   subscript  𝐞   subscript  j  q        T^{i_{1}\dots i_{p}}_{j_{1}\dots j_{q}}\equiv T(\mathbf{\varepsilon}^{i_{1}},%
 \ldots,\mathbf{\varepsilon}^{i_{p}},\mathbf{e}_{j_{1}},\ldots,\mathbf{e}_{j_{q%
 }}),   an ( p + q )-dimensional array of components can be obtained. A different choice of basis will yield different components. But, because T is linear in all of its arguments, the components satisfy the tensor transformation law used in the multilinear array definition. The multidimensional array of components of T thus form a tensor according to that definition. Moreover, such an array can be realized as the components of some multilinear map T . This motivates viewing multilinear maps as the intrinsic objects underlying tensors.  In viewing a tensor as a multilinear map, it is conventional to identify the vector space V with the space of linear functionals on the dual of V , the double dual  V ** . There is always a natural linear map from V to its double dual, given by evaluating a linear form in V * against a vector in V . This linear mapping is an isomorphism in finite dimensions, and it is often then expedient to identify V with its double dual.  Using tensor products  For some mathematical applications, a more abstract approach is sometimes useful. This can be achieved by defining tensors in terms of elements of tensor products of vector spaces, which in turn are defined through a universal property . A type    (  p  ,  q  )     p  q    (p,q)   tensor is defined in this context as an element of the tensor product of vector spaces, 11       T  ∈      V  ⊗  …  ⊗  V   ⏟    p  copies    ⊗      V  *   ⊗  …  ⊗   V  *    ⏟    q  copies      .      T   tensor-product   subscript   normal-⏟   tensor-product  V  normal-…  V      p  copies     subscript   normal-⏟   tensor-product   superscript  V    normal-…   superscript  V        q  copies       T\in\underbrace{V\otimes\dots\otimes V}_{p\text{ copies}}\otimes\underbrace{V^%
 {*}\otimes\dots\otimes V^{*}}_{q\text{ copies}}.     If is a basis of   V   V   V   and is a basis of   W   W   W   , then the tensor product    V  ⊗  W      V  normal-⊗  W    V⊗W   has a natural basis . The components of a tensor   T   T   T   are the coefficients of the tensor with respect to the basis obtained from a basis  for   V   V   V   and its dual , i.e.       T  =      T    j  1   …   j  q      i  1   …   i  p       𝐞   i  1     ⊗  ⋯  ⊗   𝐞   i  p    ⊗   ε   j  1    ⊗  ⋯  ⊗   ε   j  q      .      T   tensor-product     subscript   superscript  T     subscript  i  1   normal-…   subscript  i  p        subscript  j  1   normal-…   subscript  j  q      subscript  𝐞   subscript  i  1     normal-⋯   subscript  𝐞   subscript  i  p     superscript  ε   subscript  j  1    normal-⋯   superscript  ε   subscript  j  q       T=T^{i_{1}\dots i_{p}}_{j_{1}\dots j_{q}}\;\mathbf{e}_{i_{1}}\otimes\cdots%
 \otimes\mathbf{e}_{i_{p}}\otimes\mathbf{\varepsilon}^{j_{1}}\otimes\cdots%
 \otimes\mathbf{\varepsilon}^{j_{q}}.   Using the properties of the tensor product, it can be shown that these components satisfy the transformation law for a type    (  p  ,  q  )     p  q    (p,q)   tensor. Moreover, the universal property of the tensor product gives a    1   1   1   -to-   1   1   1   correspondence between tensors defined in this way and tensors defined as multilinear maps.  Tensor products can be defined in great generality – for example, involving arbitrary modules over a ring. In principle, one could define a "tensor" simply to be an element of any tensor product. However, the mathematics literature usually reserves the term tensor for an element of a tensor product of a single vector space   V   V   V   and its dual, as above.  Tensors in infinite dimensions  This discussion of tensors so far assumes finite dimensionality of the spaces involved, where the spaces of tensors obtained by each of these constructions are naturally isomorphic . 12 Constructions of spaces of tensors based on the tensor product and multilinear mappings can be generalized, essentially without modification, to vector bundles or coherent sheaves . 13 For infinite-dimensional vector spaces, inequivalent topologies lead to inequivalent notions of tensor, and these various isomorphisms may or may not hold depending on what exactly is meant by a tensor (see topological tensor product ). In some applications, it is the tensor product of Hilbert spaces that is intended, whose properties are the most similar to the finite-dimensional case. A more modern view is that it is the tensors' structure as a symmetric monoidal category that encodes their most important properties, rather than the specific models of those categories  Examples  This table shows important examples of tensors, including both tensors on vector spaces and tensor fields on manifolds. The tensors are classified according to their type , where n is the number of contravariant indices, m is the number of covariant indices, and  gives the total order of the tensor. For example, a bilinear form is the same thing as a -tensor; an inner product is an example of a -tensor, but not all -tensors are inner products. In the -entry of the table, M denotes the dimensionality of the underlying vector space or manifold because for each dimension of the space, a separate index is needed to select that dimension to get a maximally covariant antisymmetric tensor.        scope="col" = n , m   scope="col" = n = 0   scope="col" = n = 1   scope="col" = n = 2   scope="col" = ...   scope="col" = n   scope="col" = ...       m = 0   scalar , e.g. scalar curvature   vector (e.g. direction vector )   inverse metric tensor , bivector (e.g., a Poisson structure )    n -vector , a sum of n -blades      m = 1   covector , linear functional , 1-form (e.g. gradient of a scalar field)   linear transformation , 14  Kronecker delta         m = 2   bilinear form , e.g. inner product , metric tensor , Ricci curvature , 2-form , symplectic form   e.g. cross product in three dimensions   e.g. elasticity tensor        m = 3   e.g. 3-form   e.g. Riemann curvature tensor         ...           m = M   e.g. M -form i.e. volume form          ...               Raising an index on an -tensor produces an -tensor; this can be visualized as moving diagonally up and to the right on the table. Symmetrically, lowering an index can be visualized as moving diagonally down and to the left on the table. Contraction of an upper with a lower index of an -tensor produces an -tensor; this can be visualized as moving diagonally up and to the left on the table.  {{-}}    Notation  Ricci calculus  Ricci calculus is the modern formalism and notation for tensor indices: indicating inner and outer products , covariance and contravariance , summations of tensor components, symmetry and antisymmetry , and partial and covariant derivatives .  Einstein summation convention  The Einstein summation convention dispenses with writing summation signs , leaving the summation implicit. Any repeated index symbol is summed over: if the index i is used twice in a given term of a tensor expression, it means that the term is to be summed for all i . Several distinct pairs of indices may be summed this way.  Penrose graphical notation  Penrose graphical notation is a diagrammatic notation which replaces the symbols for tensors with shapes, and their indices by lines and curves. It is independent of basis elements, and requires no symbols for the indices.  Abstract index notation  The abstract index notation is a way to write tensors such that the indices are no longer thought of as numerical, but rather are indeterminates . This notation captures the expressiveness of indices and the basis-independence of index-free notation.  Component-free notation  A component-free treatment of tensors uses notation that emphasises that tensors do not rely on any basis, and is defined in terms of the tensor product of vector spaces .  Operations  There are a number of basic operations that may be conducted on tensors that again produce a tensor. The linear nature of tensor implies that two tensors of the same type may be added together, and that tensors may be multiplied by a scalar with results analogous to the scaling of a vector . On components, these operations are simply performed component for component. These operations do not change the type of the tensor, however there also exist operations that change the type of the tensors.  Tensor product  The tensor product takes two tensors, S and T , and produces a new tensor, , whose order is the sum of the orders of the original tensors. When described as multilinear maps, the tensor product simply multiplies the two tensors, i.e.         (   S  ⊗  T   )    (   v  1   ,  …  ,   v  n   ,   v   n  +  1    ,  …  ,   v   n  +  m    )    =   S   (   v  1   ,  …  ,   v  n   )   T   (   v   n  +  1    ,  …  ,   v   n  +  m    )     ,         tensor-product  S  T     subscript  v  1   normal-…   subscript  v  n    subscript  v    n  1    normal-…   subscript  v    n  m        S    subscript  v  1   normal-…   subscript  v  n    T    subscript  v    n  1    normal-…   subscript  v    n  m        (S\otimes T)(v_{1},\ldots,v_{n},v_{n+1},\ldots,v_{n+m})=S(v_{1},\ldots,v_{n})T%
 (v_{n+1},\ldots,v_{n+m}),   which again produces a map that is linear in all its arguments. On components the effect similarly is to multiply the components of the two input tensors, i.e.         (   S  ⊗  T   )     j  1   …   j  k    j   k  +  1    …   j   k  +  m       i  1   …   i  l    i   l  +  1    …   i   l  +  n      =    S    j  1   …   j  k      i  1   …   i  l      T    j   k  +  1    …   j   k  +  m       i   l  +  1    …   i   l  +  n        ,       subscript   superscript   tensor-product  S  T      subscript  i  1   normal-…   subscript  i  l    subscript  i    l  1    normal-…   subscript  i    l  n         subscript  j  1   normal-…   subscript  j  k    subscript  j    k  1    normal-…   subscript  j    k  m         subscript   superscript  S     subscript  i  1   normal-…   subscript  i  l        subscript  j  1   normal-…   subscript  j  k      subscript   superscript  T     subscript  i    l  1    normal-…   subscript  i    l  n         subscript  j    k  1    normal-…   subscript  j    k  m         (S\otimes T)^{i_{1}\ldots i_{l}i_{l+1}\ldots i_{l+n}}_{j_{1}\ldots j_{k}j_{k+1%
 }\ldots j_{k+m}}=S^{i_{1}\ldots i_{l}}_{j_{1}\ldots j_{k}}T^{i_{l+1}\ldots i_{%
 l+n}}_{j_{k+1}\ldots j_{k+m}},   If S is of type ( l , k ) and T is of type ( n , m ), then the tensor product  has type .  Contraction  Tensor contraction is an operation that reduces the total order of a tensor by two. More precisely, it reduces a type  tensor to a type  tensor. In terms of components, the operation is achieved by summing over one contravariant and one covariant index of tensor. For example, a -tensor    T  i  j     superscript   subscript  T  i   j    T_{i}^{j}   can be contracted to a scalar through      T  i  i     superscript   subscript  T  i   i    T_{i}^{i}   . Where the summation is again implied. When the -tensor is interpreted as a linear map, this operation is known as the trace .  The contraction is often used in conjunction with the tensor product to contract an index from each tensor.  The contraction can also be understood in terms of the definition of a tensor as an element of a tensor product of copies of the space V with the space V * by first decomposing the tensor into a linear combination of simple tensors, and then applying a factor from V * to a factor from V . For example, a tensor      T  ∈   V  ⊗  V  ⊗   V  *        T   tensor-product  V  V   superscript  V       T\in V\otimes V\otimes V^{*}   can be written as a linear combination       T  =     v  1   ⊗   w  1   ⊗   α  1    +    v  2   ⊗   w  2   ⊗   α  2    +  ⋯  +    v  N   ⊗   w  N   ⊗   α  N      .      T     tensor-product   subscript  v  1    subscript  w  1    subscript  α  1     tensor-product   subscript  v  2    subscript  w  2    subscript  α  2    normal-⋯   tensor-product   subscript  v  N    subscript  w  N    subscript  α  N       T=v_{1}\otimes w_{1}\otimes\alpha_{1}+v_{2}\otimes w_{2}\otimes\alpha_{2}+%
 \cdots+v_{N}\otimes w_{N}\otimes\alpha_{N}.   The contraction of T on the first and last slots is then the vector         α  1    (   v  1   )    w  1    +    α  2    (   v  2   )    w  2    +  ⋯  +    α  N    (   v  N   )    w  N     .         subscript  α  1    subscript  v  1    subscript  w  1       subscript  α  2    subscript  v  2    subscript  w  2    normal-⋯     subscript  α  N    subscript  v  N    subscript  w  N      \alpha_{1}(v_{1})w_{1}+\alpha_{2}(v_{2})w_{2}+\cdots+\alpha_{N}(v_{N})w_{N}.     Raising or lowering an index  When a vector space is equipped with a nondegenerate bilinear form (or metric tensor as it is often called in this context), operations can be defined that convert a contravariant (upper) index into a covariant (lower) index and vice versa. A metric tensor is a (symmetric) (-tensor, it is thus possible to contract an upper index of a tensor with one of lower indices of the metric tensor in the product. This produces a new tensor with the same index structure as the previous, but with lower index in the position of the contracted upper index. This operation is quite graphically known as lowering an index .  Conversely, the inverse operation can be defined, and is called raising an index . This is equivalent to a similar contraction on the product with a -tensor. This inverse metric tensor has components that are the matrix inverse of those if the metric tensor.  Applications  Continuum mechanics  Important examples are provided by continuum mechanics. The stresses inside a solid body or fluid are described by a tensor. The stress tensor and strain tensor are both second-order tensors, and are related in a general linear elastic material by a fourth-order elasticity tensor . In detail, the tensor quantifying stress in a 3-dimensional solid object has components that can be conveniently represented as a 3 × 3 array. The three faces of a cube-shaped infinitesimal volume segment of the solid are each subject to some given force. The force's vector components are also three in number. Thus, 3 × 3, or 9 components are required to describe the stress at this cube-shaped infinitesimal segment. Within the bounds of this solid is a whole mass of varying stress quantities, each requiring 9 quantities to describe. Thus, a second-order tensor is needed.  If a particular surface element inside the material is singled out, the material on one side of the surface will apply a force on the other side. In general, this force will not be orthogonal to the surface, but it will depend on the orientation of the surface in a linear manner. This is described by a tensor of type , in linear elasticity , or more precisely by a tensor field of type , since the stresses may vary from point to point.  Other examples from physics  Common applications include   Electromagnetic tensor (or Faraday's tensor) in electromagnetism  Finite deformation tensors for describing deformations and strain tensor for strain in continuum mechanics  Permittivity and electric susceptibility are tensors in anisotropic media  Four-tensors in general relativity (e.g. stress–energy tensor ), used to represent momentum  fluxes  Spherical tensor operators are the eigenfunctions of the quantum angular momentum operator in spherical coordinates  Diffusion tensors, the basis of diffusion tensor imaging , represent rates of diffusion in biologic environments  Quantum mechanics and quantum computing utilise tensor products for combination of quantum states   Applications of tensors of order > 2  The concept of a tensor of order two is often conflated with that of a matrix. Tensors of higher order do however capture ideas important in science and engineering, as has been shown successively in numerous areas as they develop. This happens, for instance, in the field of computer vision , with the trifocal tensor generalizing the fundamental matrix .  The field of nonlinear optics studies the changes to material polarization density under extreme electric fields. The polarization waves generated are related to the generating electric fields through the nonlinear susceptibility tensor. If the polarization P is not linearly proportional to the electric field E , the medium is termed nonlinear . To a good approximation (for sufficiently weak fields, assuming no permanent dipole moments are present), P is given by a Taylor series in E whose coefficients are the nonlinear susceptibilities:         P  i    ε  0    =     ∑  j     χ   i  j    (  1  )     E  j     +    ∑   j  k      χ   i  j  k    (  2  )     E  j    E  k     +    ∑   j  k  ℓ      χ   i  j  k  ℓ    (  3  )     E  j    E  k    E  ℓ     +  ⋯    .         subscript  P  i    subscript  ε  0        subscript   j      subscript   superscript  χ  1     i  j     subscript  E  j       subscript     j  k       superscript   subscript  χ    i  j  k    2    subscript  E  j    subscript  E  k       subscript     j  k  normal-ℓ       superscript   subscript  χ    i  j  k  normal-ℓ    3    subscript  E  j    subscript  E  k    subscript  E  normal-ℓ     normal-⋯     \frac{P_{i}}{\varepsilon_{0}}=\sum_{j}\chi^{(1)}_{ij}E_{j}+\sum_{jk}\chi_{ijk}%
 ^{(2)}E_{j}E_{k}+\sum_{jk\ell}\chi_{ijk\ell}^{(3)}E_{j}E_{k}E_{\ell}+\cdots.\!     Here    χ   (  1  )      superscript  χ  1    \chi^{(1)}   is the linear susceptibility,    χ   (  2  )      superscript  χ  2    \chi^{(2)}   gives the Pockels effect and second harmonic generation , and    χ   (  3  )      superscript  χ  3    \chi^{(3)}   gives the Kerr effect . This expansion shows the way higher-order tensors arise naturally in the subject matter.  Generalizations  Tensor products of vector spaces  The vector spaces of a tensor product need not be the same, and sometimes the elements of such a more general tensor product are called "tensors". For example, an element of the tensor product space    V  ⊗  W      V  normal-⊗  W    V⊗W   is a second-order "tensor" in this more general sense, 15 and an order-   d   d   d   tensor may likewise be defined as an element of a tensor product of   d   d   d   different vector spaces. 16 A type    (  n  ,  m  )     n  m    (n,m)   tensor, in the sense defined previously, is also a tensor of order    n  +  m      n  m    n+m   in this more general sense.  Tensors in infinite dimensions  The notion of a tensor can be generalized in a variety of ways to infinite dimensions . One, for instance, is via the tensor product of Hilbert spaces . 17 Another way of generalizing the idea of tensor, common in nonlinear analysis , is via the multilinear maps definition where instead of using finite-dimensional vector spaces and their algebraic duals , one uses infinite-dimensional Banach spaces and their continuous dual . 18 Tensors thus live naturally on Banach manifolds . 19  Tensor densities  The concept of a tensor field can be generalized by considering objects that transform differently. An object that transforms as an ordinary tensor field under coordinate transformations, except that it is also multiplied by the determinant of the Jacobian of the inverse coordinate transformation to the    w  th     superscript  w  th    w^{\text{th}}   power, is called a tensor density with weight   w   w   w   . 20 Invariantly, in the language of multilinear algebra, one can think of tensor densities as multilinear maps taking their values in a density bundle such as the (1-dimensional) space of n -forms (where n is the dimension of the space), as opposed to taking their values in just R . Higher "weights" then just correspond to taking additional tensor products with this space in the range.  A special case are the scalar densities. Scalar 1-densities are especially important because it makes sense to define their integral over a manifold. They appear, for instance, in the Einstein–Hilbert action in general relativity. The most common example of a scalar 1-density is the volume element , which in the presence of a metric tensor g is the square root of its determinant in coordinates, denoted     det  g         g     \sqrt{\det g}   . The metric tensor is a covariant tensor of order 2, and so its determinant scales by the square of the coordinate transition:       det   (   g  ′   )    =     (   det    ∂  x    ∂   x  ′      )   2    det   (  g  )            superscript  g  normal-′       superscript        x      superscript  x  normal-′      2     g      \det(g^{\prime})=\left(\det\frac{\partial x}{\partial x^{\prime}}\right)^{2}%
 \det(g)   which is the transformation law for a scalar density of weight +2.  More generally, any tensor density is the product of an ordinary tensor with a scalar density of the appropriate weight. In the language of vector bundles , the determinant bundle of the tangent bundle is a line bundle that can be used to 'twist' other bundles w times. While locally the more general transformation law can indeed be used to recognise these tensors, there is a global question that arises, reflecting that in the transformation law one may write either the Jacobian determinant, or its absolute value. Non-integral powers of the (positive) transition functions of the bundle of densities make sense, so that the weight of a density, in that sense, is not restricted to integer values. Restricting to changes of coordinates with positive Jacobian determinant is possible on orientable manifolds , because there is a consistent global way to eliminate the minus signs; but otherwise the line bundle of densities and the line bundle of n -forms are distinct. For more on the intrinsic meaning, see density on a manifold .  Spinors  When changing from one orthonormal basis (called a frame ) to another by a rotation, the components of a tensor transform by that same rotation. This transformation does not depend on the path taken through the space of frames. However, the space of frames is not simply connected (see orientation entanglement and plate trick ): there are continuous paths in the space of frames with the same beginning and ending configurations that are not deformable one into the other. It is possible to attach an additional discrete invariant to each frame called the "spin" that incorporates this path dependence, and which turns out to have values of ±1. A spinor is an object that transforms like a tensor under rotations in the frame, apart from a possible sign that is determined by the spin.  History  The concepts of later tensor analysis arose from the work of Carl Friedrich Gauss in differential geometry, and the formulation was much influenced by the theory of algebraic forms and invariants developed during the middle of the nineteenth century. 21 The word "tensor" itself was introduced in 1846 by William Rowan Hamilton 22 to describe something different from what is now meant by a tensor. 23 The contemporary usage was introduced by Woldemar Voigt in 1898. 24  Tensor calculus was developed around 1890 by Gregorio Ricci-Curbastro under the title absolute differential calculus , and originally presented by Ricci in 1892. 25 It was made accessible to many mathematicians by the publication of Ricci and Tullio Levi-Civita 's 1900 classic text Méthodes de calcul différentiel absolu et leurs applications (Methods of absolute differential calculus and their applications). 26  In the 20th century, the subject came to be known as tensor analysis , and achieved broader acceptance with the introduction of Einstein 's theory of general relativity , around 1915. General relativity is formulated completely in the language of tensors. Einstein had learned about them, with great difficulty, from the geometer Marcel Grossmann . 27 Levi-Civita then initiated a correspondence with Einstein to correct mistakes Einstein had made in his use of tensor analysis. The correspondence lasted 1915–17, and was characterized by mutual respect:  Tensors were also found to be useful in other fields such as continuum mechanics . Some well-known examples of tensors in differential geometry are quadratic forms such as metric tensors , and the Riemann curvature tensor . The exterior algebra of Hermann Grassmann , from the middle of the nineteenth century, is itself a tensor theory, and highly geometric, but it was some time before it was seen, with the theory of differential forms , as naturally unified with tensor calculus. The work of Élie Cartan made differential forms one of the basic kinds of tensors used in mathematics.  From about the 1920s onwards, it was realised that tensors play a basic role in algebraic topology (for example in the Künneth theorem ). 28 Correspondingly there are types of tensors at work in many branches of abstract algebra , particularly in homological algebra and representation theory . Multilinear algebra can be developed in greater generality than for scalars coming from a field . For example, scalars can come from a ring . But the theory is then less geometric and computations more technical and less algorithmic. 29 Tensors are generalized within category theory by means of the concept of monoidal category , from the 1960s. 30  See also  Foundational   Cartesian tensor  Fibre bundle  Glossary of tensor theory  Multilinear projection  One-form  Tensor product of modules   Applications   Application of tensor theory in engineering  Covariant derivative  Curvature  Diffusion tensor MRI  Einstein field equations  Fluid mechanics  Multilinear subspace learning  Riemannian geometry  Structure tensor  Tensor decomposition  Tensor derivative  Tensor software   Notes  References   General             Munkres, James, Analysis on Manifolds , Westview Press, 1991. Chapter six gives a "from scratch" introduction to covariant tensors.    Schutz, Bernard, Geometrical methods of mathematical physics , Cambridge University Press, 1980.     Specific   External links    Introduction to Vectors and Tensors, Vol 1: Linear and Multilinear Algebra by Ray M. Bowen and C. C. Wang.  Introduction to Vectors and Tensors, Vol 2: Vector and Tensor Analysis by Ray M. Bowen and C. C. Wang.  An Introduction to Tensors for Students of Physics and Engineering by Joseph C. Kolecki, Glenn Research Center, Cleveland, Ohio, released by NASA  Foundations of Tensor Analysis for Students of Physics and Engineering With an Introduction to the Theory of Relativity by Joseph C. Kolecki, Glenn Research Center, Cleveland, Ohio, released by NASA  A discussion of the various approaches to teaching tensors, and recommendations of textbooks  Introduction to tensors an original approach by S Poirier  A Quick Introduction to Tensor Analysis by R. A. Sharipov.  Richard P. Feynman's Lecture on tensors.   "  Category:Concepts in physics       ↩  ↩  The Einstein summation convention, in brief, requires the sum to be taken over all values of the index whenever the same symbol appears as a subscript and superscript in the same term. For example, under this convention      B  i    C  i    =     B  1    C  1    +    B  2    C  2    +   ⋯   B  n    C  n            subscript  B  i    superscript  C  i         subscript  B  1    superscript  C  1       subscript  B  2    superscript  C  2      normal-⋯   subscript  B  n    superscript  C  n       B_{i}C^{i}=B_{1}C^{1}+B_{2}C^{2}+\cdots B_{n}C^{n}    ↩  ↩   ↩  ↩    For instance, ↩  ↩  The double duality isomorphism , for instance, is used to identify V with the double dual space V **, which consists of multilinear forms of degree one on V *. It is typical in linear algebra to identify spaces that are naturally isomorphic, treating them as the same space. ↩  Bourbaki, "Algebra", III, where the case of finitely generated projective modules is treated. The global sections of sections of a vector bundle over a compact space form a projective module over the ring of smooth functions. All statements for coherent sheaves are true locally. ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  Namely, the norm operation in a certain type of algebraic system (now known as a Clifford algebra ). ↩  Woldemar Voigt, Die fundamentalen physikalischen Eigenschaften der Krystalle in elementarer Darstellung [The fundamental physical properties of crystals in an elementary presentation] (Leipzig, Germany: Veit & Co., 1898), p. 20. [ http://books.google.com/books?id=QhBDAAAAIAAJ&vq; ;=tensortripel&pg;=PA20#v=onepage&q;&f;=false From page 20:] "Wir wollen uns deshalb nur darauf stützen, dass Zustände der geschilderten Art bei Spannungen und Dehnungen nicht starrer Körper auftreten, und sie deshalb tensorielle, die für sie charakteristischen physikalischen Grössen aber Tensoren nennen." (We therefore want [our presentation] to be based only on [the assumption that] conditions of the type described occur during stresses and strains of non-rigid bodies, and therefore call them "tensorial" but call the characteristic physical quantities for them "tensors".) ↩  ↩  ↩  ↩  Edwin H. Spanier, Algebraic Topology , p. 227, "...the Künneth formula expressing the homology of the tensor product...", McGraw Hill, 1966. ↩  Thomas W. Hungerford, Algebra , p. 168, "...the classification (up to isomorphism) of modules over an arbitrary ring is quite difficult..." Springer, 1974, ISBN 0387905189. ↩  Saunders Mac Lane, Categories for the Working Mathematician , p. 4, "...for example the monoid M ... in the category of abelian groups, × is replaced by the usual tensor product...", Springer, 1971, ISBN 0387900365. ↩     