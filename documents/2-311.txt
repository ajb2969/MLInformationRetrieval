   Sufficient statistic      Sufficient statistic   In statistics , a statistic is sufficient with respect to a statistical model and its associated unknown parameter if "no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter". 1 In particular, a statistic is sufficient for a family of probability distributions if the sample from which it is calculated gives no additional information than does the statistic, as to which of those probability distributions is that of the population from which the sample was taken.  Roughly, given a set   ùêó   ùêó   \mathbf{X}   of independent identically distributed data conditioned on an unknown parameter   Œ∏   Œ∏   \theta   , a sufficient statistic is a function    T   (  ùêó  )       T  ùêó    T(\mathbf{X})   whose value contains all the information needed to compute any estimate of the parameter (e.g. a maximum likelihood estimate). Due to the factorization theorem (see below), for a sufficient statistic    T   (  ùêó  )       T  ùêó    T(\mathbf{X})   , the joint distribution can be written as     p   (  ùêó  )    =   h   (  ùêó  )   g   (  Œ∏  ,   T   (  ùêó  )    )          p  ùêó     h  ùêó  g   Œ∏    T  ùêó       p(\mathbf{X})=h(\mathbf{X})\,g(\theta,T(\mathbf{X}))\,   . From this factorization, it can easily be seen that the maximum likelihood estimate of   Œ∏   Œ∏   \theta   will interact with   ùêó   ùêó   \mathbf{X}   only through    T   (  ùêó  )       T  ùêó    T(\mathbf{X})   . Typically, the sufficient statistic is a simple function of the data, e.g. the sum of all the data points.  More generally, the "unknown parameter" may represent a vector of unknown quantities or may represent everything about the model that is unknown or not fully specified. In such a case, the sufficient statistic may be a set of functions, called a jointly sufficient statistic . Typically, there are as many functions as there are parameters. For example, for a Gaussian distribution with unknown mean and variance , the jointly sufficient statistic, from which maximum likelihood estimates of both parameters can be estimated, consists of two functions, the sum of all data points and the sum of all squared data points (or equivalently, the sample mean and sample variance ).  The concept, due to Ronald Fisher , is equivalent to the statement that, conditional on the value of a sufficient statistic for a parameter, the joint probability distribution of the data does not depend on that parameter. Both the statistic and the underlying parameter can be vectors.  A related concept is that of linear sufficiency , which is weaker than sufficiency but can be applied in some cases where there is no sufficient statistic, although it is restricted to linear estimators. 2 The Kolmogorov structure function deals with individual finite data, the related notion there is the algorithmic sufficient statistic.  The concept of sufficiency has fallen out of favor in descriptive statistics because of the strong dependence on an assumption of the distributional form (see Pitman‚ÄìKoopman‚ÄìDarmois theorem below), but remains very important in theoretical work. 3  Mathematical definition  A statistic T ( X ) is sufficient for underlying parameter Œ∏ precisely if the conditional probability distribution of the data X , given the statistic T ( X ), does not depend on the parameter Œ∏ , 4 i.e.        Pr   (  x  |  t  ,  Œ∏  )    =   Pr   (  x  |  t  )     .       Pr  x  t  Œ∏    Pr  x  t     \Pr(x|t,\theta)=\Pr(x|t).\,     Instead of this expression, the definition still holds if one uses either of the equivalent expressions:        Pr   (  Œ∏  |  t  ,  x  )    =   Pr   (  Œ∏  |  t  )     ,       Pr  Œ∏  t  x    Pr  Œ∏  t     \Pr(\theta|t,x)=\Pr(\theta|t),\,   or        Pr   (  Œ∏  ,  x  |  t  )    =    Pr   (  Œ∏  |  t  )     Pr   (  x  |  t  )      ,       Pr  Œ∏  x  t      Pr  Œ∏  t    Pr  x  t      \Pr(\theta,x|t)=\Pr(\theta|t)\Pr(x|t),\,   which indicate, respectively, that the conditional probability of the parameter Œ∏ , given the sufficient statistic t , does not depend on the data x ; and that the conditional probability of the parameter Œ∏ given the sufficient statistic t and the conditional probability of the data x given the sufficient statistic t are statistically independent .  Example  As an example, the sample mean is sufficient for the mean (Œº) of a normal distribution with known variance. Once the sample mean is known, no further information about Œº can be obtained from the sample itself. On the other hand, the median is not sufficient for the mean: even if the median of the sample is known, knowing the sample itself would provide further information about the population mean. For example, if the observations that are less than the median are only slightly less, but observations exceeding the median exceed it by a large amount, then this would have a bearing on one's inference about the population mean.  Fisher‚ÄìNeyman factorization theorem  Fisher's factorization theorem or factorization criterion provides a convenient characterization of a sufficient statistic. If the probability density function is ∆í Œ∏ ( x ), then T is sufficient for Œ∏  if and only if nonnegative functions g and h can be found such that         f  Œ∏    (  x  )    =   h   (  x  )    g  Œ∏    (   T   (  x  )    )     ,         subscript  f  Œ∏   x     h  x   subscript  g  Œ∏     T  x      f_{\theta}(x)=h(x)\,g_{\theta}(T(x)),\,\!     i.e. the density ∆í can be factored into a product such that one factor, h , does not depend on Œ∏ and the other factor, which does depend on Œ∏ , depends on x only through T ( x ).  It is easy to see that if F ( t ) is a one to one function and T is a sufficient statistic, then F ( T ) is a sufficient statistic. In particular we can multiply a sufficient statistic by a nonzero constant and get another sufficient statistic.  Likelihood principle interpretation  An implication of the theorem is that when using likelihood-based inference, two sets of data yielding the same value for the sufficient statistic T ( X ) will always yield the same inferences about Œ∏. By the factorization criterion, the likelihood's dependence on Œ∏ is only in conjunction with T ( X ). As this is the same in both cases, the dependence on Œ∏ will be the same as well, leading to identical inferences.  Proof  Due to Hogg and Craig. 5 Let     X  1   ,   X  2   ,  ‚Ä¶  ,   X  n       subscript  X  1    subscript  X  2   normal-‚Ä¶   subscript  X  n     X_{1},X_{2},\ldots,X_{n}   , denote a random sample from a distribution having the pdf  f ( x , Œ∏ ) for Œπ 1¬†= u 1 ( X 1 , X 2 ,¬†..., X n ) be a statistic whose pdf is g 1 ( y 1 ; Œ∏ ). Then Y 1 = u 1 ( X 1 , X 2 ,¬†..., X n ) is a sufficient statistic for Œ∏ if and only if, for some function H ,         ‚àè   i  =  1   n    f   (   x  i   ;  Œ∏  )     =    g  1    [    u  1    (   x  1   ,   x  2   ,  ‚Ä¶  ,   x  n   )    ;  Œ∏  ]   H   (   x  1   ,   x  2   ,  ‚Ä¶  ,   x  n   )     .        superscript   subscript  product    i  1    n     f    subscript  x  i   Œ∏        subscript  g  1       subscript  u  1     subscript  x  1    subscript  x  2   normal-‚Ä¶   subscript  x  n     Œ∏   H    subscript  x  1    subscript  x  2   normal-‚Ä¶   subscript  x  n       \prod_{i=1}^{n}f(x_{i};\theta)=g_{1}\left[u_{1}(x_{1},x_{2},\dots,x_{n});%
 \theta\right]H(x_{1},x_{2},\dots,x_{n}).\,     First, suppose that         ‚àè   i  =  1   n    f   (   x  i   ;  Œ∏  )     =    g  1    [    u  1    (   x  1   ,   x  2   ,  ‚Ä¶  ,   x  n   )    ;  Œ∏  ]   H   (   x  1   ,   x  2   ,  ‚Ä¶  ,   x  n   )     .        superscript   subscript  product    i  1    n     f    subscript  x  i   Œ∏        subscript  g  1       subscript  u  1     subscript  x  1    subscript  x  2   normal-‚Ä¶   subscript  x  n     Œ∏   H    subscript  x  1    subscript  x  2   normal-‚Ä¶   subscript  x  n       \prod_{i=1}^{n}f(x_{i};\theta)=g_{1}\left[u_{1}(x_{1},x_{2},\dots,x_{n});%
 \theta\right]H(x_{1},x_{2},\dots,x_{n}).\,     We shall make the transformation y i = u i ( x 1 , x 2 ,¬†..., x n ), for i =¬†1,¬†..., n , having inverse functions x i = w i ( y 1 , y 2 ,¬†..., y n ), for i =¬†1,¬†..., n , and Jacobian     J  =   [    w  i   /   y  j    ]       J   delimited-[]     subscript  w  i    subscript  y  j       J=\left[w_{i}/y_{j}\right]   . Thus,         ‚àè   i  =  1   n    f   [    w  i    (   y  1   ,   y  2   ,  ‚Ä¶  ,   y  n   )    ;  Œ∏  ]     =    |  J  |    g  1    (   y  1   ;  Œ∏  )   H   [    w  1    (   y  1   ,   y  2   ,  ‚Ä¶  ,   y  n   )    ,  ‚Ä¶  ,    w  n    (   y  1   ,   y  2   ,  ‚Ä¶  ,   y  n   )    ]     .        superscript   subscript  product    i  1    n     f      subscript  w  i     subscript  y  1    subscript  y  2   normal-‚Ä¶   subscript  y  n     Œ∏         J    subscript  g  1     subscript  y  1   Œ∏   H      subscript  w  1     subscript  y  1    subscript  y  2   normal-‚Ä¶   subscript  y  n     normal-‚Ä¶     subscript  w  n     subscript  y  1    subscript  y  2   normal-‚Ä¶   subscript  y  n         \prod_{i=1}^{n}f\left[w_{i}(y_{1},y_{2},\dots,y_{n});\theta\right]=|J|g_{1}(y_%
 {1};\theta)H\left[w_{1}(y_{1},y_{2},\dots,y_{n}),\dots,w_{n}(y_{1},y_{2},\dots%
 ,y_{n})\right].     The left-hand member is the joint pdf g ( y 1 , y 2 , ..., y n ; Œ∏) of Y 1 = u 1 ( X 1 , ..., X n ), ..., Y n = u n ( X 1 , ..., X n ). In the right-hand member,     g  1    (   y  1   ;  Œ∏  )        subscript  g  1     subscript  y  1   Œ∏     g_{1}(y_{1};\theta)   is the pdf of    Y  1     subscript  Y  1    Y_{1}   , so that    H   [   w  1   ,  ‚Ä¶  ,   w  n   ]    |  J  |       H    subscript  w  1   normal-‚Ä¶   subscript  w  n      J     H[w_{1},\dots,w_{n}]|J|   is the quotient of    g   (   y  1   ,  ‚Ä¶  ,   y  n   ;  Œ∏  )       g    subscript  y  1   normal-‚Ä¶   subscript  y  n   Œ∏     g(y_{1},\dots,y_{n};\theta)   and     g  1    (   y  1   ;  Œ∏  )        subscript  g  1     subscript  y  1   Œ∏     g_{1}(y_{1};\theta)   ; that is, it is the conditional pdf    h   (   y  2   ,  ‚Ä¶  ,   y  n   |   y  1   ;  Œ∏  )      fragments  h   fragments  normal-(   subscript  y  2   normal-,  normal-‚Ä¶  normal-,   subscript  y  n   normal-|   subscript  y  1   normal-;  Œ∏  normal-)     h(y_{2},\dots,y_{n}|y_{1};\theta)   of     Y  2   ,  ‚Ä¶  ,   Y  n       subscript  Y  2   normal-‚Ä¶   subscript  Y  n     Y_{2},\dots,Y_{n}   given     Y  1   =   y  1        subscript  Y  1    subscript  y  1     Y_{1}=y_{1}   .  But    H   (   x  1   ,   x  2   ,  ‚Ä¶  ,   x  n   )       H    subscript  x  1    subscript  x  2   normal-‚Ä¶   subscript  x  n      H(x_{1},x_{2},\dots,x_{n})   , and thus    H   [   w  1    (   y  1   ,  ‚Ä¶  ,   y  n   )   ,  ‚Ä¶  ,   w  n    (   y  1   ,  ‚Ä¶  ,   y  n   )   )   ]     fragments  H   fragments  normal-[   subscript  w  1    fragments  normal-(   subscript  y  1   normal-,  normal-‚Ä¶  normal-,   subscript  y  n   normal-)   normal-,  normal-‚Ä¶  normal-,   subscript  w  n    fragments  normal-(   subscript  y  1   normal-,  normal-‚Ä¶  normal-,   subscript  y  n   normal-)   normal-)   normal-]    H\left[w_{1}(y_{1},\dots,y_{n}),\dots,w_{n}(y_{1},\dots,y_{n}))\right]   , was given not to depend upon   Œ∏   Œ∏   \theta   . Since   Œ∏   Œ∏   \theta   was not introduced in the transformation and accordingly not in the Jacobian   J   J   J   , it follows that    h   (   y  2   ,  ‚Ä¶  ,   y  n   |   y  1   ;  Œ∏  )      fragments  h   fragments  normal-(   subscript  y  2   normal-,  normal-‚Ä¶  normal-,   subscript  y  n   normal-|   subscript  y  1   normal-;  Œ∏  normal-)     h(y_{2},\dots,y_{n}|y_{1};\theta)   does not depend upon   Œ∏   Œ∏   \theta   and that    Y  1     subscript  Y  1    Y_{1}   is a sufficient statistics for   Œ∏   Œ∏   \theta   .  The converse is proven by taking:      g   (   y  1   ,  ‚Ä¶  ,   y  n   ;  Œ∏  )   =   g  1    (   y  1   ;  Œ∏  )   h   (   y  2   ,  ‚Ä¶  ,   y  n   |   y  1   )   ,     fragments  g   fragments  normal-(   subscript  y  1   normal-,  normal-‚Ä¶  normal-,   subscript  y  n   normal-;  Œ∏  normal-)     subscript  g  1    fragments  normal-(   subscript  y  1   normal-;  Œ∏  normal-)   h   fragments  normal-(   subscript  y  2   normal-,  normal-‚Ä¶  normal-,   subscript  y  n   normal-|   subscript  y  1   normal-)   normal-,    g(y_{1},\dots,y_{n};\theta)=g_{1}(y_{1};\theta)h(y_{2},\dots,y_{n}|y_{1}),\,     where    h   (   y  2   ,  ‚Ä¶  ,   y  n   |   y  1   )      fragments  h   fragments  normal-(   subscript  y  2   normal-,  normal-‚Ä¶  normal-,   subscript  y  n   normal-|   subscript  y  1   normal-)     h(y_{2},\dots,y_{n}|y_{1})   does not depend upon   Œ∏   Œ∏   \theta   because     Y  2   ‚Ä¶   Y  n        subscript  Y  2   normal-‚Ä¶   subscript  Y  n     Y_{2}...Y_{n}   depend only upon     X  1   ‚Ä¶   X  n        subscript  X  1   normal-‚Ä¶   subscript  X  n     X_{1}...X_{n}   , which are independent on   Œò   normal-Œò   \Theta   when conditioned by    Y  1     subscript  Y  1    Y_{1}   , a sufficient statistics by hypothesis. Now divide both members by the absolute value of the non-vanishing Jacobian   J   J   J   , and replace     y  1   ,  ‚Ä¶  ,   y  n       subscript  y  1   normal-‚Ä¶   subscript  y  n     y_{1},\dots,y_{n}   by the functions      u  1    (   x  1   ,  ‚Ä¶  ,   x  n   )    ,  ‚Ä¶  ,    u  n    (   x  1   ,  ‚Ä¶  ,   x  n   )          subscript  u  1     subscript  x  1   normal-‚Ä¶   subscript  x  n     normal-‚Ä¶     subscript  u  n     subscript  x  1   normal-‚Ä¶   subscript  x  n       u_{1}(x_{1},\dots,x_{n}),\dots,u_{n}(x_{1},\dots,x_{n})   in     x  1   ,  ‚Ä¶  ,   x  n       subscript  x  1   normal-‚Ä¶   subscript  x  n     x_{1},\dots,x_{n}   . This yields        g   [    u  1    (   x  1   ,  ‚Ä¶  ,   x  n   )    ,  ‚Ä¶  ,    u  n    (   x  1   ,  ‚Ä¶  ,   x  n   )    ;  Œ∏  ]     |  J  *  |    =    g  1    [    u  1    (   x  1   ,  ‚Ä¶  ,   x  n   )    ;  Œ∏  ]     h   (   u  2   ,  ‚Ä¶  ,   u  n   |   u  1   )     |  J  *  |             g      subscript  u  1     subscript  x  1   normal-‚Ä¶   subscript  x  n     normal-‚Ä¶     subscript  u  n     subscript  x  1   normal-‚Ä¶   subscript  x  n     Œ∏     fragments  normal-|  J   normal-|       subscript  g  1       subscript  u  1     subscript  x  1   normal-‚Ä¶   subscript  x  n     Œ∏      fragments  h   fragments  normal-(   subscript  u  2   normal-,  normal-‚Ä¶  normal-,   subscript  u  n   normal-|   subscript  u  1   normal-)     fragments  normal-|  J   normal-|       \frac{g\left[u_{1}(x_{1},\dots,x_{n}),\dots,u_{n}(x_{1},\dots,x_{n});\theta%
 \right]}{|J*|}=g_{1}\left[u_{1}(x_{1},\dots,x_{n});\theta\right]\frac{h(u_{2},%
 \dots,u_{n}|u_{1})}{|J*|}     where    J  *     fragments  J     J*   is the Jacobian with     y  1   ,  ‚Ä¶  ,   y  n       subscript  y  1   normal-‚Ä¶   subscript  y  n     y_{1},\dots,y_{n}   replaced by their value in terms     x  1   ,  ‚Ä¶  ,   x  n       subscript  x  1   normal-‚Ä¶   subscript  x  n     x_{1},\dots,x_{n}   . The left-hand member is necessarily the joint pdf    f   (   x  1   ;  Œ∏  )   ‚ãØ  f   (   x  n   ;  Œ∏  )       f    subscript  x  1   Œ∏   normal-‚ãØ  f    subscript  x  n   Œ∏     f(x_{1};\theta)\cdots f(x_{n};\theta)   of     X  1   ,  ‚Ä¶  ,   X  n       subscript  X  1   normal-‚Ä¶   subscript  X  n     X_{1},\dots,X_{n}   . Since    h   (   y  2   ,  ‚Ä¶  ,   y  n   |   y  1   )      fragments  h   fragments  normal-(   subscript  y  2   normal-,  normal-‚Ä¶  normal-,   subscript  y  n   normal-|   subscript  y  1   normal-)     h(y_{2},\dots,y_{n}|y_{1})   , and thus    h   (   u  2   ,  ‚Ä¶  ,   u  n   |   u  1   )      fragments  h   fragments  normal-(   subscript  u  2   normal-,  normal-‚Ä¶  normal-,   subscript  u  n   normal-|   subscript  u  1   normal-)     h(u_{2},\dots,u_{n}|u_{1})   , does not depend upon   Œ∏   Œ∏   \theta   , then       H   (   x  1   ,  ‚Ä¶  ,   x  2   )    =    h   (   u  2   ,  ‚Ä¶  ,   u  n   |   u  1   )     |  J  *  |          H    subscript  x  1   normal-‚Ä¶   subscript  x  2        fragments  h   fragments  normal-(   subscript  u  2   normal-,  normal-‚Ä¶  normal-,   subscript  u  n   normal-|   subscript  u  1   normal-)     fragments  normal-|  J   normal-|      H(x_{1},\dots,x_{2})=\frac{h(u_{2},\dots,u_{n}|u_{1})}{|J*|}     is a function that does not depend upon   Œ∏   Œ∏   \theta   .  Another proof  A simpler more illustrative proof is as follows, although it applies only in the discrete case.  We use the shorthand notation to denote the joint probability of    (  X  ,   T   (  X  )    )     X    T  X     (X,T(X))   by     f  Œ∏    (  x  ,  t  )        subscript  f  Œ∏    x  t     f_{\theta}(x,t)   . Since   T   T   T   is a function of   X   X   X   , we have      f  Œ∏    (  x  ,  t  )    =    f  Œ∏    (  x  )           subscript  f  Œ∏    x  t       subscript  f  Œ∏   x     f_{\theta}(x,t)=f_{\theta}(x)   (only when    t  =   T   (  x  )        t    T  x     t=T(x)   and zero otherwise) and thus:        f  Œ∏    (  x  )    =    f  Œ∏    (  x  ,  t  )    =    f   Œ∏  |  t     (  x  )    f  Œ∏    (  t  )             subscript  f  Œ∏   x      subscript  f  Œ∏    x  t            subscript  f   fragments  Œ∏  normal-|  t    x   subscript  f  Œ∏   t      f_{\theta}(x)=f_{\theta}(x,t)=f_{\theta|t}(x)f_{\theta}(t)     with the last equality being true by the definition of conditional probability distributions . Thus      f  Œ∏    (  x  )    =   a   (  x  )    b  Œ∏    (  t  )           subscript  f  Œ∏   x     a  x   subscript  b  Œ∏   t     f_{\theta}(x)=a(x)b_{\theta}(t)   with     a   (  x  )    =    f   Œ∏  |  t     (  x  )          a  x      subscript  f   fragments  Œ∏  normal-|  t    x     a(x)=f_{\theta|t}(x)   and      b  Œ∏    (  t  )    =    f  Œ∏    (  t  )           subscript  b  Œ∏   t      subscript  f  Œ∏   t     b_{\theta}(t)=f_{\theta}(t)   .  Reciprocally, if      f  Œ∏    (  x  )    =   a   (  x  )    b  Œ∏    (  t  )           subscript  f  Œ∏   x     a  x   subscript  b  Œ∏   t     f_{\theta}(x)=a(x)b_{\theta}(t)   , we have       f  Œ∏    (  t  )        subscript  f  Œ∏   t    \displaystyle f_{\theta}(t)     With the first equality by the definition of pdf for multiple variables , the second by the remark above, the third by hypothesis, and the fourth because the summation is not over   t   t   t   .  Thus, the conditional probability distribution is:       f   Œ∏  |  t     (  x  )        subscript  f   fragments  Œ∏  normal-|  t    x    \displaystyle f_{\theta|t}(x)     With the first equality by definition of conditional probability density, the second by the remark above, the third by the equality proven above, and the fourth by simplification. This expression does not depend on   Œ∏   Œ∏   \theta   and thus   T   T   T   is a sufficient statistic. 6  Minimal sufficiency  A sufficient statistic is minimal sufficient if it can be represented as a function of any other sufficient statistic. In other words, S ( X ) is minimal sufficient if and only if 7   S ( X ) is sufficient, and  if T ( X ) is sufficient, then there exists a function f such that S ( X ) = f ( T ( X )).   Intuitively, a minimal sufficient statistic most efficiently captures all possible information about the parameter Œ∏ .  A useful characterization of minimal sufficiency is that when the density f Œ∏ exists, S ( X ) is minimal sufficient if and only if        f  Œ∏    (  x  )      f  Œ∏    (  y  )           subscript  f  Œ∏   x      subscript  f  Œ∏   y     \frac{f_{\theta}(x)}{f_{\theta}(y)}   is independent of Œ∏    ‚ü∫   normal-‚ü∫   \Longleftrightarrow    S ( x ) = S ( y )  This follows as a direct consequence from Fisher's factorization theorem stated above.  A case in which there is no minimal sufficient statistic was shown by Bahadur, 1954. 8 However, under mild conditions, a minimal sufficient statistic does always exist. In particular, in Euclidean space, these conditions always hold if the random variables (associated with    P  Œ∏     subscript  P  Œ∏    P_{\theta}   ) are all discrete or are all continuous.  If there exists a minimal sufficient statistic, and this is usually the case, then every complete sufficient statistic is necessarily minimal sufficient 9 (note that this statement does not exclude the option of a pathological case in which a complete sufficient exists while there is no minimal sufficient statistic). While it is hard to find cases in which a minimal sufficient statistic does not exist, it is not so hard to find cases in which there is no complete statistic.  The collection of likelihood ratios    {    L   (   Œ∏  1   |  X  )     L   (   Œ∏  2   |  X  )     }        fragments  L   fragments  normal-(   subscript  Œ∏  1   normal-|  X  normal-)     fragments  L   fragments  normal-(   subscript  Œ∏  2   normal-|  X  normal-)       \left\{\frac{L(\theta_{1}|X)}{L(\theta_{2}|X)}\right\}   is a minimal sufficient statistic if    P   (  X  |  Œ∏  )      fragments  P   fragments  normal-(  X  normal-|  Œ∏  normal-)     P(X|\theta)   is discrete or has a density function.  Examples  Bernoulli distribution  If X 1 ,¬†...., X n are independent Bernoulli-distributed random variables with expected value p , then the sum T ( X ) = X 1 +¬†...¬†+ X n is a sufficient statistic for p (here 'success' corresponds to X i =¬†1 and 'failure' to X i =¬†0; so T is the total number of successes)  This is seen by considering the joint probability distribution:        Pr   {   X  =  x   }    =   Pr   {    X  1   =   x  1    ,    X  2   =   x  2    ,  ‚Ä¶  ,    X  n   =   x  n    }     .       Pr    X  x     Pr     subscript  X  1    subscript  x  1       subscript  X  2    subscript  x  2    normal-‚Ä¶     subscript  X  n    subscript  x  n       \Pr\{X=x\}=\Pr\{X_{1}=x_{1},X_{2}=x_{2},\ldots,X_{n}=x_{n}\}.     Because the observations are independent, this can be written as       p   x  1      (   1  -  p   )    1  -   x  1      p   x  2      (   1  -  p   )    1  -   x  2     ‚ãØ   p   x  n      (   1  -  p   )    1  -   x  n          superscript  p   subscript  x  1     superscript    1  p     1   subscript  x  1      superscript  p   subscript  x  2     superscript    1  p     1   subscript  x  2     normal-‚ãØ   superscript  p   subscript  x  n     superscript    1  p     1   subscript  x  n       p^{x_{1}}(1-p)^{1-x_{1}}p^{x_{2}}(1-p)^{1-x_{2}}\cdots p^{x_{n}}(1-p)^{1-x_{n}%
 }\,\!     and, collecting powers of p and 1¬†‚àí p , gives        p    ‚àë   x  i        (   1  -  p   )    n  -    ‚àë   x  i        =    p   T   (  x  )       (   1  -  p   )    n  -   T   (  x  )              superscript  p     subscript  x  i      superscript    1  p     n     subscript  x  i          superscript  p    T  x     superscript    1  p     n    T  x        p^{\sum x_{i}}(1-p)^{n-\sum x_{i}}=p^{T(x)}(1-p)^{n-T(x)}\,\!     which satisfies the factorization criterion, with h ( x )¬†=¬†1 being just a constant.  Note the crucial feature: the unknown parameter p interacts with the data x only via the statistic T ( x ) =¬†Œ£ x i .  As a concrete application, this gives a procedure for creating a fair coin from a biased coin .  Uniform distribution  If X 1 , ...., X n are independent and uniformly distributed on the interval [0, Œ∏ ], then T ( X ) = max( X 1 , ..., X n ) is sufficient for Œ∏ ‚Äî the sample maximum is a sufficient statistic for the population maximum.  To see this, consider the joint probability density function of X =( X 1 ,..., X n ). Because the observations are independent, the pdf can be written as a product of individual densities       f  X    (   x  1   ,  ‚Ä¶  ,   x  n   )        subscript  f  X     subscript  x  1   normal-‚Ä¶   subscript  x  n      \displaystyle f_{X}(x_{1},\ldots,x_{n})     where 1 { ... } is the indicator function . Thus the density takes form required by the Fisher‚ÄìNeyman factorization theorem, where h ( x )¬†= 1 {min{ x i }‚â•0} , and the rest of the expression is a function of only Œ∏ and T ( x )¬†=¬†max{ x i }.  In fact, the minimum-variance unbiased estimator (MVUE) for Œ∏ is         n  +  1   n   T   (  X  )    .          n  1   n   T  X    \frac{n+1}{n}T(X).     This is the sample maximum, scaled to correct for the bias , and is MVUE by the Lehmann‚ÄìScheff√© theorem . Unscaled sample maximum T ( X ) is the maximum likelihood estimator for Œ∏ .  Uniform distribution (with two parameters)  If     X  1   ,  ‚Ä¶  ,    X  n        subscript  X  1   normal-‚Ä¶   subscript  X  n     X_{1},...,X_{n}\,   are independent and uniformly distributed on the interval    [  Œ±  ,  Œ≤  ]     Œ±  Œ≤    [\alpha,\beta]\,   (where    Œ±    Œ±   \alpha\,   and    Œ≤    Œ≤   \beta\,   are unknown parameters), then     T   (   X  1  n   )    =   (    min   1  ‚â§  i  ‚â§  n     X  i    ,    max   1  ‚â§  i  ‚â§  n     X  i    )         T   superscript   subscript  X  1   n       subscript       1  i       n      subscript  X  i      subscript       1  i       n      subscript  X  i       T(X_{1}^{n})=\left(\min_{1\leq i\leq n}X_{i},\max_{1\leq i\leq n}X_{i}\right)\,   is a two-dimensional sufficient statistic for    (   Œ±   ,  Œ≤  )     Œ±  Œ≤    (\alpha\,,\,\beta)   .  To see this, consider the joint probability density function of     X  1  n   =   (   X  1   ,  ‚Ä¶  ,   X  n   )        superscript   subscript  X  1   n     subscript  X  1   normal-‚Ä¶   subscript  X  n      X_{1}^{n}=(X_{1},\ldots,X_{n})   . Because the observations are independent, the pdf can be written as a product of individual densities, i.e.       f   X  1  n     (   x  1  n   )        subscript  f   superscript   subscript  X  1   n     superscript   subscript  x  1   n     \displaystyle f_{X_{1}^{n}}(x_{1}^{n})     The joint density of the sample takes the form required by the Fisher‚ÄìNeyman factorization theorem, by letting         h   (   x  1  n   )    =  1   ,     g   (  Œ±  ,  Œ≤  )     (   x  1  n   )    =     (    1   Œ≤  -  Œ±     )   n    ùüè   {   Œ±   ‚â§   min   1  ‚â§  i  ‚â§  n     X  i   }     ùüè   {   max   1  ‚â§  i  ‚â§  n      X  i    ‚â§  Œ≤  }       .     formulae-sequence      h   superscript   subscript  x  1   n    1        subscript  g   Œ±  Œ≤     superscript   subscript  x  1   n       superscript    1    Œ≤  Œ±    n    subscript  1   fragments  normal-{  Œ±    subscript       1  i       n      subscript  X  i   normal-}     subscript  1   fragments  normal-{   subscript       1  i       n      subscript  X  i    Œ≤  normal-}        \displaystyle h(x_{1}^{n})=1,\quad g_{(\alpha,\beta)}(x_{1}^{n})=\left({1\over%
 \beta-\alpha}\right)^{n}\mathbf{1}_{\{\alpha\,\leq\,\min_{1\leq i\leq n}X_{i}%
 \}}\mathbf{1}_{\{\max_{1\leq i\leq n}X_{i}\,\leq\,\beta\}}.     Since    h   (   x  1  n   )       h   superscript   subscript  x  1   n     h(x_{1}^{n})   does not depend on the parameter    (  Œ±  ,  Œ≤  )     Œ±  Œ≤    (\alpha,\beta)   and     g   (   Œ±   ,  Œ≤  )     (   x  1  n   )        subscript  g   Œ±  Œ≤     superscript   subscript  x  1   n     g_{(\alpha\,,\,\beta)}(x_{1}^{n})   depends only on    x  1  n     superscript   subscript  x  1   n    x_{1}^{n}   through the function      T   (   X  1  n   )    =   (    min   1  ‚â§  i  ‚â§  n     X  i    ,    max   1  ‚â§  i  ‚â§  n     X  i    )    ,        T   superscript   subscript  X  1   n       subscript       1  i       n      subscript  X  i      subscript       1  i       n      subscript  X  i       T(X_{1}^{n})=\left(\min_{1\leq i\leq n}X_{i},\max_{1\leq i\leq n}X_{i}\right),\,     the Fisher‚ÄìNeyman factorization theorem implies     T   (   X  1  n   )    =   (    min   1  ‚â§  i  ‚â§  n     X  i    ,    max   1  ‚â§  i  ‚â§  n     X  i    )         T   superscript   subscript  X  1   n       subscript       1  i       n      subscript  X  i      subscript       1  i       n      subscript  X  i       T(X_{1}^{n})=\left(\min_{1\leq i\leq n}X_{i},\max_{1\leq i\leq n}X_{i}\right)\,   is a sufficient statistic for    (   Œ±   ,  Œ≤  )     Œ±  Œ≤    (\alpha\,,\,\beta)   .  Poisson distribution  If X 1 ,¬†...., X n are independent and have a Poisson distribution with parameter Œª , then the sum T ( X ) = X 1 +¬†...¬†+ X n is a sufficient statistic for Œª .  To see this, consider the joint probability distribution:      Pr   (  X  =  x  )   =  P   (   X  1   =   x  1   ,   X  2   =   x  2   ,  ‚Ä¶  ,   X  n   =   x  n   )   .     fragments  Pr   fragments  normal-(  X   x  normal-)    P   fragments  normal-(   subscript  X  1     subscript  x  1   normal-,   subscript  X  2     subscript  x  2   normal-,  normal-‚Ä¶  normal-,   subscript  X  n     subscript  x  n   normal-)   normal-.    \Pr(X=x)=P(X_{1}=x_{1},X_{2}=x_{2},\ldots,X_{n}=x_{n}).\,     Because the observations are independent, this can be written as          e   -  Œª     Œª   x  1       x  1   !    ‚ãÖ     e   -  Œª     Œª   x  2       x  2   !     ‚ãØ      e   -  Œª     Œª   x  n       x  n   !          normal-‚ãÖ       superscript  e    Œª     superscript  Œª   subscript  x  1        subscript  x  1          superscript  e    Œª     superscript  Œª   subscript  x  2        subscript  x  2      normal-‚ãØ       superscript  e    Œª     superscript  Œª   subscript  x  n        subscript  x  n       {e^{-\lambda}\lambda^{x_{1}}\over x_{1}!}\cdot{e^{-\lambda}\lambda^{x_{2}}%
 \over x_{2}!}\cdots{e^{-\lambda}\lambda^{x_{n}}\over x_{n}!}\,     which may be written as        e   -   n  Œª      Œª   (    x  1   +   x  2   +  ‚ãØ  +   x  n    )     ‚ãÖ    1     x  1   !     x  2   !   ‚ãØ    x  n   !         normal-‚ãÖ     superscript  e      n  Œª      superscript  Œª     subscript  x  1    subscript  x  2   normal-‚ãØ   subscript  x  n        1       subscript  x  1       subscript  x  2    normal-‚ãØ     subscript  x  n        e^{-n\lambda}\lambda^{(x_{1}+x_{2}+\cdots+x_{n})}\cdot{1\over x_{1}!x_{2}!%
 \cdots x_{n}!}\,     which shows that the factorization criterion is satisfied, where h ( x ) is the reciprocal of the product of the factorials. Note the parameter Œª interacts with the data only through its sum T ( X ).  Normal distribution  If     X  1   ,  ‚Ä¶  ,   X  n       subscript  X  1   normal-‚Ä¶   subscript  X  n     X_{1},\dots,X_{n}   are independent and normally distributed with expected value Œ∏ (a parameter) and known finite variance    œÉ  2     superscript  œÉ  2    \sigma^{2}   , then     T   (   X  1  n   )    =   X  ¬Ø   =    1  n     ‚àë   i  =  1   n    X  i             T   superscript   subscript  X  1   n     normal-¬Ø  X            1  n     superscript   subscript     i  1    n    subscript  X  i        T(X_{1}^{n})=\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}   is a sufficient statistic for Œ∏.  To see this, consider the joint probability density function of     X  1  n   =   (   X  1   ,  ‚Ä¶  ,   X  n   )        superscript   subscript  X  1   n     subscript  X  1   normal-‚Ä¶   subscript  X  n      X_{1}^{n}=(X_{1},\dots,X_{n})   . Because the observations are independent, the pdf can be written as a product of individual densities, i.e. -       f   X  1  n     (   x  1  n   )        subscript  f   superscript   subscript  X  1   n     superscript   subscript  x  1   n     \displaystyle f_{X_{1}^{n}}(x_{1}^{n})     Then, since      ‚àë   i  =  1   n     (    x  i   -   x  ¬Ø    )    (   Œ∏  -   x  ¬Ø    )     =  0        superscript   subscript     i  1    n        subscript  x  i    normal-¬Ø  x      Œ∏   normal-¬Ø  x      0    \sum_{i=1}^{n}(x_{i}-\overline{x})(\theta-\overline{x})=0   , which can be shown simply by expanding this term,       f   X  1  n     (   x  1  n   )        subscript  f   superscript   subscript  X  1   n     superscript   subscript  x  1   n     \displaystyle f_{X_{1}^{n}}(x_{1}^{n})     The joint density of the sample takes the form required by the Fisher‚ÄìNeyman factorization theorem, by letting         h   (   x  1  n   )    =      (   2  œÄ   œÉ  2    )     -  n   2      e     -  1    2   œÉ  2        ‚àë   i  =  1   n      (    x  i   -   x  ¬Ø    )   2        ,     g  Œ∏    (   x  1  n   )    =   e     -  n    2   œÉ  2       (   Œ∏  -   x  ¬Ø    )   2       .     formulae-sequence      h   superscript   subscript  x  1   n       superscript    2  œÄ   superscript  œÉ  2        n   2     superscript  e        1     2   superscript  œÉ  2       superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¬Ø  x    2             subscript  g  Œ∏    superscript   subscript  x  1   n     superscript  e        n     2   superscript  œÉ  2      superscript    Œ∏   normal-¬Ø  x    2        \displaystyle h(x_{1}^{n})=(2\pi\sigma^{2})^{-n\over 2}\,e^{{-1\over 2\sigma^{%
 2}}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}},\,\,\,g_{\theta}(x_{1}^{n})=e^{{-n%
 \over 2\sigma^{2}}(\theta-\overline{x})^{2}}.     Since    h   (   x  1  n   )       h   superscript   subscript  x  1   n     h(x_{1}^{n})   does not depend on the parameter   Œ∏   Œ∏   \theta   and     g  Œ∏    (   x  1  n   )        subscript  g  Œ∏    superscript   subscript  x  1   n     g_{\theta}(x_{1}^{n})   depends only on    x  1  n     superscript   subscript  x  1   n    x_{1}^{n}   through the function      T   (   X  1  n   )    =   X  ¬Ø   =    1  n     ‚àë   i  =  1   n    X  i      ,          T   superscript   subscript  X  1   n     normal-¬Ø  X            1  n     superscript   subscript     i  1    n    subscript  X  i        T(X_{1}^{n})=\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i},     the Fisher‚ÄìNeyman factorization theorem implies     T   (   X  1  n   )    =   X  ¬Ø   =    1  n     ‚àë   i  =  1   n    X  i             T   superscript   subscript  X  1   n     normal-¬Ø  X            1  n     superscript   subscript     i  1    n    subscript  X  i        T(X_{1}^{n})=\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}   is a sufficient statistic for   Œ∏   Œ∏   \theta   .  Exponential distribution  If     X  1   ,  ‚Ä¶  ,   X  n       subscript  X  1   normal-‚Ä¶   subscript  X  n     X_{1},\dots,X_{n}   are independent and exponentially distributed with expected value Œ∏ (an unknown real-valued positive parameter), then     T   (   X  1  n   )    =    ‚àë   i  =  1   n    X  i          T   superscript   subscript  X  1   n      superscript   subscript     i  1    n    subscript  X  i      T(X_{1}^{n})=\sum_{i=1}^{n}X_{i}   is a sufficient statistic for Œ∏.  To see this, consider the joint probability density function of     X  1  n   =   (   X  1   ,  ‚Ä¶  ,   X  n   )        superscript   subscript  X  1   n     subscript  X  1   normal-‚Ä¶   subscript  X  n      X_{1}^{n}=(X_{1},\dots,X_{n})   . Because the observations are independent, the pdf can be written as a product of individual densities, i.e. -       f   X  1  n     (   x  1  n   )        subscript  f   superscript   subscript  X  1   n     superscript   subscript  x  1   n     \displaystyle f_{X_{1}^{n}}(x_{1}^{n})     The joint density of the sample takes the form required by the Fisher‚ÄìNeyman factorization theorem, by letting         h   (   x  1  n   )    =  1   ,     g  Œ∏    (   x  1  n   )    =      1   Œ∏  n       e     -  1   Œ∏      ‚àë   i  =  1   n     x  i         .     formulae-sequence      h   superscript   subscript  x  1   n    1        subscript  g  Œ∏    superscript   subscript  x  1   n        1   superscript  Œ∏  n     superscript  e        1   Œ∏     superscript   subscript     i  1    n    subscript  x  i          \displaystyle h(x_{1}^{n})=1,\,\,\,g_{\theta}(x_{1}^{n})={1\over\theta^{n}}\,e%
 ^{{-1\over\theta}\sum_{i=1}^{n}x_{i}}.     Since    h   (   x  1  n   )       h   superscript   subscript  x  1   n     h(x_{1}^{n})   does not depend on the parameter   Œ∏   Œ∏   \theta   and     g  Œ∏    (   x  1  n   )        subscript  g  Œ∏    superscript   subscript  x  1   n     g_{\theta}(x_{1}^{n})   depends only on    x  1  n     superscript   subscript  x  1   n    x_{1}^{n}   through the function     T   (   X  1  n   )    =    ‚àë   i  =  1   n    X  i          T   superscript   subscript  X  1   n      superscript   subscript     i  1    n    subscript  X  i      T(X_{1}^{n})=\sum_{i=1}^{n}X_{i}     the Fisher‚ÄìNeyman factorization theorem implies     T   (   X  1  n   )    =    ‚àë   i  =  1   n    X  i          T   superscript   subscript  X  1   n      superscript   subscript     i  1    n    subscript  X  i      T(X_{1}^{n})=\sum_{i=1}^{n}X_{i}   is a sufficient statistic for   Œ∏   Œ∏   \theta   .  Gamma distribution  If     X  1   ,  ‚Ä¶  ,    X  n        subscript  X  1   normal-‚Ä¶   subscript  X  n     X_{1},\dots,X_{n}\,   are independent and distributed as a     Œì   (   Œ±   ,  Œ≤  )       normal-Œì   Œ±  Œ≤     \Gamma(\alpha\,,\,\beta)\,\,    , where    Œ±    Œ±   \alpha\,   and    Œ≤    Œ≤   \beta\,   are unknown parameters of a Gamma distribution , then     T   (   X  1  n   )    =   (    ‚àè   i  =  1   n    x  i    ,    ‚àë   i  =  1   n    x  i    )         T   superscript   subscript  X  1   n       superscript   subscript  product    i  1    n    subscript  x  i      superscript   subscript     i  1    n    subscript  x  i       T(X_{1}^{n})=\left(\prod_{i=1}^{n}{x_{i}},\sum_{i=1}^{n}x_{i}\right)\,   is a two-dimensional sufficient statistic for    (  Œ±  ,  Œ≤  )     Œ±  Œ≤    (\alpha,\beta)   .  To see this, consider the joint probability density function of     X  1  n   =   (   X  1   ,  ‚Ä¶  ,   X  n   )        superscript   subscript  X  1   n     subscript  X  1   normal-‚Ä¶   subscript  X  n      X_{1}^{n}=(X_{1},\dots,X_{n})   . Because the observations are independent, the pdf can be written as a product of individual densities, i.e. -       f   X  1  n     (   x  1  n   )        subscript  f   superscript   subscript  X  1   n     superscript   subscript  x  1   n     \displaystyle f_{X_{1}^{n}}(x_{1}^{n})     The joint density of the sample takes the form required by the Fisher‚ÄìNeyman factorization theorem, by letting         h   (   x  1  n   )    =  1   ,     g   (   Œ±   ,  Œ≤  )     (   x  1  n   )    =     (    1   Œì   (  Œ±  )    Œ≤  Œ±      )   n     (     ‚àè   i  =  1   n     x  i    )    Œ±  -  1     e     -  1   Œ≤      ‚àë   i  =  1   n     x  i         .     formulae-sequence      h   superscript   subscript  x  1   n    1        subscript  g   Œ±  Œ≤     superscript   subscript  x  1   n       superscript    1    normal-Œì  Œ±   superscript  Œ≤  Œ±     n    superscript    superscript   subscript  product    i  1    n    subscript  x  i      Œ±  1     superscript  e        1   Œ≤     superscript   subscript     i  1    n    subscript  x  i          \displaystyle h(x_{1}^{n})=1,\,\,\,g_{(\alpha\,,\,\beta)}(x_{1}^{n})=\left({1%
 \over\Gamma(\alpha)\beta^{\alpha}}\right)^{n}\left(\prod_{i=1}^{n}x_{i}\right)%
 ^{\alpha-1}e^{{-1\over\beta}\sum_{i=1}^{n}{x_{i}}}.     Since    h   (   x  1  n   )       h   superscript   subscript  x  1   n     h(x_{1}^{n})   does not depend on the parameter    (   Œ±   ,  Œ≤  )     Œ±  Œ≤    (\alpha\,,\,\beta)   and     g   (   Œ±   ,  Œ≤  )     (   x  1  n   )        subscript  g   Œ±  Œ≤     superscript   subscript  x  1   n     g_{(\alpha\,,\,\beta)}(x_{1}^{n})   depends only on    x  1  n     superscript   subscript  x  1   n    x_{1}^{n}   through the function      T   (   X  1  n   )    =   (    ‚àè   i  =  1   n    x  i    ,    ‚àë   i  =  1   n    x  i    )    ,        T   superscript   subscript  X  1   n       superscript   subscript  product    i  1    n    subscript  x  i      superscript   subscript     i  1    n    subscript  x  i       T(X_{1}^{n})=\left(\prod_{i=1}^{n}{x_{i}},\sum_{i=1}^{n}{x_{i}}\right),     the Fisher‚ÄìNeyman factorization theorem implies     T   (   X  1  n   )    =   (    ‚àè   i  =  1   n    x  i    ,    ‚àë   i  =  1   n    x  i    )         T   superscript   subscript  X  1   n       superscript   subscript  product    i  1    n    subscript  x  i      superscript   subscript     i  1    n    subscript  x  i       T(X_{1}^{n})=\left(\prod_{i=1}^{n}{x_{i}},\sum_{i=1}^{n}{x_{i}}\right)   is a sufficient statistic for     (   Œ±   ,  Œ≤  )   .     Œ±  Œ≤    (\alpha\,,\,\beta).     Rao‚ÄìBlackwell theorem  Sufficiency finds a useful application in the Rao‚ÄìBlackwell theorem , which states that if g ( X ) is any kind of estimator of Œ∏ , then typically the conditional expectation of g ( X ) given sufficient statistic T ( X ) is a better estimator of Œ∏ , and is never worse. Sometimes one can very easily construct a very crude estimator g ( X ), and then evaluate that conditional expected value to get an estimator that is in various senses optimal.  Exponential family  According to the Pitman‚ÄìKoopman‚ÄìDarmois theorem, among families of probability distributions whose domain does not vary with the parameter being estimated, only in exponential families is there a sufficient statistic whose dimension remains bounded as sample size increases. Less tersely, suppose       X  n   ,  n   =  1   ,   2  ,  3  ,  ‚Ä¶      formulae-sequence      subscript  X  n   n   1    2  3  normal-‚Ä¶     X_{n},n=1,2,3,\dots   are independent identically distributed random variables whose distribution is known to be in some family of probability distributions. Only if that family is an exponential family is there a (possibly vector-valued) sufficient statistic    T   (   X  1   ,  ‚Ä¶  ,   X  n   )       T    subscript  X  1   normal-‚Ä¶   subscript  X  n      T(X_{1},\dots,X_{n})   whose number of scalar components does not increase as the sample size n increases.  This theorem shows that sufficiency (or rather, the existence of a scalar or vector-valued of bounded dimension sufficient statistic) sharply restricts the possible forms of the distribution.  Other types of sufficiency  Bayesian sufficiency  An alternative formulation of the condition that a statistic be sufficient, set in a Bayesian context, involves the posterior distributions obtained by using the full data-set and by using only a statistic. Thus the requirement is that, for almost every x,        Pr   (  Œ∏  |   X  =  x   )    =   Pr   (  Œ∏  |    T   (  X  )    =   t   (  x  )     )     .       Pr  Œ∏    X  x     Pr  Œ∏      T  X     t  x       \Pr(\theta|X=x)=\Pr(\theta|T(X)=t(x)).\,     It turns out that this "Bayesian sufficiency" is a consequence of the formulation above, 10 however they are not directly equivalent in the infinite-dimensional case. 11 A range of theoretical results for sufficiency in a Bayesian context is available. 12  Linear sufficiency  A concept called "linear sufficiency" can be formulated in a Bayesian context, 13 and more generally. 14 First define the best linear predictor of a vector Y based on X as     E  ^    [  Y  |  X  ]      fragments   normal-^  E    fragments  normal-[  Y  normal-|  X  normal-]     \hat{E}[Y|X]   . Then a linear statistic T ( x ) is linear sufficient 15 if       E  ^    [  Œ∏  |  X  ]   =   E  ^    [  Œ∏  |  T   (  X  )   ]   .     fragments   normal-^  E    fragments  normal-[  Œ∏  normal-|  X  normal-]     normal-^  E    fragments  normal-[  Œ∏  normal-|  T   fragments  normal-(  X  normal-)   normal-]   normal-.    \hat{E}[\theta|X]=\hat{E}[\theta|T(X)].     See also   Completeness of a statistic  Basu's theorem on independence of complete sufficient and ancillary statistics  Lehmann‚ÄìScheff√© theorem : a complete sufficient estimator is the best estimator of its expectation  Rao‚ÄìBlackwell theorem  Sufficient dimension reduction  Ancillary statistic   Notes  References     Dodge, Y. (2003) The Oxford Dictionary of Statistical Terms , OUP. ISBN 0-19-920613-9   "  Category:Statistical theory  Category:Statistical principles  Category:Articles containing proofs     ‚Ü©  Dodge, Y. (2003) ‚Äî entry for linear sufficiency ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  . Webpage at Connexions (cnx.org) ‚Ü©  Dodge (2003) ‚Äî entry for minimal sufficient statistics ‚Ü©  Lehmann and Casella (1998), Theory of Point Estimation , 2nd Edition, Springer, p 37 ‚Ü©  Lehmann and Casella (1998), Theory of Point Estimation , 2nd Edition, Springer, page 42 ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©     