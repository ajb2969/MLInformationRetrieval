   Elementary symmetric polynomial      Elementary symmetric polynomial   In mathematics , specifically in commutative algebra , the elementary symmetric polynomials are one type of basic building block for symmetric polynomials , in the sense that any symmetric polynomial can be expressed as a polynomial in elementary symmetric polynomials. That is, any symmetric polynomial P is given by an expression involving only additions and multiplication of constants and elementary symmetric polynomials. There is one elementary symmetric polynomial of degree d in n variables for each nonnegative integer d ≤ n , and it is formed by adding together all distinct products of d distinct variables.  Definition  The elementary symmetric polynomials in   n   n   n   variables X 1 , …, X n , written e k ( X 1 , …, X n ) for k = 0, 1, ..., n , are defined by       e  0    (   X  1   ,   X  2   ,  …  ,   X  n   )        subscript  e  0     subscript  X  1    subscript  X  2   normal-…   subscript  X  n      \displaystyle e_{0}(X_{1},X_{2},\dots,X_{n})   and so forth, ending with        e  n    (   X  1   ,   X  2   ,  …  ,   X  n   )    =    X  1    X  2   …   X  n           subscript  e  n     subscript  X  1    subscript  X  2   normal-…   subscript  X  n        subscript  X  1    subscript  X  2   normal-…   subscript  X  n      e_{n}(X_{1},X_{2},\dots,X_{n})=X_{1}X_{2}\ldots X_{n}   . In general, for k ≥ 0 we define         e  k    (   X  1   ,  …  ,   X  n   )    =    ∑   1  ≤   j  1   <   j  2   <  …  <   j  k   ≤  n      X   j  1    ⋯   X   j  k       ,         subscript  e  k     subscript  X  1   normal-…   subscript  X  n       subscript       1   subscript  j  1         subscript  j  2        normal-…        subscript  j  k        n        subscript  X   subscript  j  1    normal-⋯   subscript  X   subscript  j  k        e_{k}(X_{1},\ldots,X_{n})=\sum_{1\leq j_{1}   so that 0}} if    k  >  n      k  n    k>n   .  Thus, for each positive integer   k   k   k   less than or equal to   n   n   n   there exists exactly one elementary symmetric polynomial of degree   k   k   k   in   n   n   n   variables. To form the one that has degree   k   k   k   , we take the sum of all products of   k   k   k   -subsets of the   n   n   n   variables. (By contrast, if one performs the same operation using multisets of variables, that is, taking variables with repetition, one arrives at the complete homogeneous symmetric polynomials .)  Given an integer partition (that is, a finite decreasing sequence of positive integers) λ = (λ 1 , …, λ m ), one defines the symmetric polynomial     e  λ    (   X  1   ,  …  ,   X  n   )        subscript  e  λ     subscript  X  1   normal-…   subscript  X  n      e_{\lambda}(X_{1},\dots,X_{n})   , also called an elementary symmetric polynomial, by        e  λ    (   X  1   ,  …  ,   X  n   )    =      e   λ  1     (   X  1   ,  …  ,   X  n   )    ⋅   e   λ  2      (   X  1   ,  …  ,   X  n   )   ⋯   e   λ  m     (   X  1   ,  …  ,   X  n   )           subscript  e  λ     subscript  X  1   normal-…   subscript  X  n        normal-⋅     subscript  e   subscript  λ  1      subscript  X  1   normal-…   subscript  X  n      subscript  e   subscript  λ  2       subscript  X  1   normal-…   subscript  X  n    normal-⋯   subscript  e   subscript  λ  m      subscript  X  1   normal-…   subscript  X  n       e_{\lambda}(X_{1},\dots,X_{n})=e_{\lambda_{1}}(X_{1},\dots,X_{n})\cdot e_{%
 \lambda_{2}}(X_{1},\dots,X_{n})\cdots e_{\lambda_{m}}(X_{1},\dots,X_{n})   .  Sometimes the notation σ k is used instead of e k .  Examples  The following lists the n elementary symmetric polynomials for the first four positive values of n . (In every case, e 0 = 1 is also one of the polynomials.)  For n = 1:         e  1    (   X  1   )    =   X  1    .         subscript  e  1    subscript  X  1     subscript  X  1     e_{1}(X_{1})=X_{1}.\,     For n = 2:       e  1    (   X  1   ,   X  2   )        subscript  e  1     subscript  X  1    subscript  X  2      \displaystyle e_{1}(X_{1},X_{2})     For n = 3:       e  1    (   X  1   ,   X  2   ,   X  3   )        subscript  e  1     subscript  X  1    subscript  X  2    subscript  X  3      \displaystyle e_{1}(X_{1},X_{2},X_{3})     For n = 4:       e  1    (   X  1   ,   X  2   ,   X  3   ,   X  4   )        subscript  e  1     subscript  X  1    subscript  X  2    subscript  X  3    subscript  X  4      \displaystyle e_{1}(X_{1},X_{2},X_{3},X_{4})     Properties  The elementary symmetric polynomials appear when we expand a linear factorization of a monic polynomial: we have the identity         ∏   j  =  1   n    (   λ  -   X  j    )    =     λ  n   -    e  1    (   X  1   ,  …  ,   X  n   )    λ   n  -  1      +    e  2    (   X  1   ,  …  ,   X  n   )    λ   n  -  2     +  ⋯  +     (   -  1   )   n    e  n    (   X  1   ,  …  ,   X  n   )      .        superscript   subscript  product    j  1    n     λ   subscript  X  j          superscript  λ  n      subscript  e  1     subscript  X  1   normal-…   subscript  X  n     superscript  λ    n  1         subscript  e  2     subscript  X  1   normal-…   subscript  X  n     superscript  λ    n  2     normal-⋯     superscript    1   n    subscript  e  n     subscript  X  1   normal-…   subscript  X  n        \prod_{j=1}^{n}(\lambda-X_{j})=\lambda^{n}-e_{1}(X_{1},\ldots,X_{n})\lambda^{n%
 -1}+e_{2}(X_{1},\ldots,X_{n})\lambda^{n-2}+\cdots+(-1)^{n}e_{n}(X_{1},\ldots,X%
 _{n}).   That is, when we substitute numerical values for the variables     X  1   ,   X  2   ,  …  ,   X  n       subscript  X  1    subscript  X  2   normal-…   subscript  X  n     X_{1},X_{2},\dots,X_{n}   , we obtain the monic univariate polynomial (with variable λ) whose roots are the values substituted for     X  1   ,   X  2   ,  …  ,   X  n       subscript  X  1    subscript  X  2   normal-…   subscript  X  n     X_{1},X_{2},\dots,X_{n}   and whose coefficients are up to their sign the elementary symmetric polynomials. These relations between the roots and the coefficients of a polynomial are called Vieta's formulas .  The characteristic polynomial of a square matrix is an example of this. The roots are the eigenvalues of the matrix. When we substitute these eigenvalues into the elementary symmetric polynomials, we obtain, up to their sign, the coefficients of the characteristic polynomial, which are invariants of the matrix. In particular, the value of is the trace of the matrix, and the value of is its determinant .  The set of elementary symmetric polynomials in   n   n   n   variables generates the ring of symmetric polynomials in   n   n   n   variables. More specifically, the ring of symmetric polynomials with integer coefficients equals the integral polynomial ring     ℤ   [    e  1    (   X  1   ,  …  ,   X  n   )    ,  …  ,    e  n    (   X  1   ,  …  ,   X  n   )    ]    .      ℤ      subscript  e  1     subscript  X  1   normal-…   subscript  X  n     normal-…     subscript  e  n     subscript  X  1   normal-…   subscript  X  n        \mathbb{Z}[e_{1}(X_{1},\ldots,X_{n}),\ldots,e_{n}(X_{1},\ldots,X_{n})].   (See below for a more general statement and proof.) This fact is one of the foundations of invariant theory . For other systems of symmetric polynomials with a similar property see power sum symmetric polynomials and complete homogeneous symmetric polynomials .  The fundamental theorem of symmetric polynomials  For any commutative ring  A denote the ring of symmetric polynomials in the variables     X  1   ,  …  ,   X  n       subscript  X  1   normal-…   subscript  X  n     X_{1},\ldots,X_{n}   with coefficients in A by    A    [   X  1   ,  …  ,   X  n   ]    S  n        A   superscript    subscript  X  1   normal-…   subscript  X  n     subscript  S  n      A[X_{1},\ldots,X_{n}]^{S_{n}}   .      A    [   X  1   ,  …  ,   X  n   ]    S  n        A   superscript    subscript  X  1   normal-…   subscript  X  n     subscript  S  n      A[X_{1},\ldots,X_{n}]^{S_{n}}   is a polynomial ring in the n elementary symmetric polynomials     e  k    (   X  1   ,  …  ,   X  n   )        subscript  e  k     subscript  X  1   normal-…   subscript  X  n      e_{k}(X_{1},\ldots,X_{n})   for k = 1, ..., n . (Note that    e  0     subscript  e  0    e_{0}   is not among these polynomials; since     e  0   =  1       subscript  e  0   1    e_{0}=1   , it cannot be member of any set of algebraically independent elements.)  This means that every symmetric polynomial     P   (   X  1   ,  …  ,   X  n   )    ∈   A    [   X  1   ,  …  ,   X  n   ]    S  n           P    subscript  X  1   normal-…   subscript  X  n       A   superscript    subscript  X  1   normal-…   subscript  X  n     subscript  S  n       P(X_{1},\ldots,X_{n})\in A[X_{1},\ldots,X_{n}]^{S_{n}}   has a unique representation       P   (   X  1   ,  …  ,   X  n   )    =   Q   (    e  1    (   X  1   ,  …  ,   X  n   )    ,  …  ,    e  n    (   X  1   ,  …  ,   X  n   )    )          P    subscript  X  1   normal-…   subscript  X  n       Q      subscript  e  1     subscript  X  1   normal-…   subscript  X  n     normal-…     subscript  e  n     subscript  X  1   normal-…   subscript  X  n         P(X_{1},\ldots,X_{n})=Q(e_{1}(X_{1},\ldots,X_{n}),\ldots,e_{n}(X_{1},\ldots,X_%
 {n}))   for some polynomial    Q  ∈   A   [   Y  1   ,  …  ,   Y  n   ]        Q    A    subscript  Y  1   normal-…   subscript  Y  n       Q\in A[Y_{1},\ldots,Y_{n}]   . Another way of saying the same thing is that    A    [   X  1   ,  …  ,   X  n   ]    S  n        A   superscript    subscript  X  1   normal-…   subscript  X  n     subscript  S  n      A[X_{1},\ldots,X_{n}]^{S_{n}}   is isomorphic to the polynomial ring    A   [   Y  1   ,  …  ,   Y  n   ]       A    subscript  Y  1   normal-…   subscript  Y  n      A[Y_{1},\ldots,Y_{n}]   through an isomorphism that sends    Y  k     subscript  Y  k    Y_{k}   to     e  k    (   X  1   ,  …  ,   X  n   )        subscript  e  k     subscript  X  1   normal-…   subscript  X  n      e_{k}(X_{1},\ldots,X_{n})   for    k  =   1  ,  …  ,  n       k   1  normal-…  n     k=1,\ldots,n   .  Proof sketch  The theorem may be proved for symmetric homogeneous polynomials by a double mathematical induction with respect to the number of variables n and, for fixed n , with respect to the degree of the homogeneous polynomial. The general case then follows by splitting an arbitrary symmetric polynomial into its homogeneous components (which are again symmetric).  In the case n = 1 the result is obvious because every polynomial in one variable is automatically symmetric.  Assume now that the theorem has been proved for all polynomials for    m  <  n      m  n    m   variables and all symmetric polynomials in n variables with degree  A[X_1,\ldots,X_n]^{S_n}  can be decomposed as a sum of homogeneous symmetric polynomials        P   (   X  1   ,  …  ,   X  n   )    =     P  lacunary    (   X  1   ,  …  ,   X  n   )    +      X  1   ⋯   X  n    ⋅  Q    (   X  1   ,  …  ,   X  n   )      .        P    subscript  X  1   normal-…   subscript  X  n          subscript  P  lacunary     subscript  X  1   normal-…   subscript  X  n        normal-⋅     subscript  X  1   normal-⋯   subscript  X  n    Q     subscript  X  1   normal-…   subscript  X  n        P(X_{1},\ldots,X_{n})=P_{\mbox{lacunary}}(X_{1},\ldots,X_{n})+X_{1}\cdots X_{n%
 }\cdot Q(X_{1},\ldots,X_{n}).   Here the "lacunary part"    P  lacunary     subscript  P  lacunary    P_{\mbox{lacunary}}   is defined as the sum of all monomials in P which contain only a proper subset of the n variables X 1 , ..., X n , i.e., where at least one variable X j is missing.  Because P is symmetric, the lacunary part is determined by its terms containing only the variables X 1 , ..., X n −1 , i.e., which do not contain X n . More precisely: If A and B are two homogeneous symmetric polynomials in X 1 , ..., X n having the same degree, and if the coefficient of A before each monomial which contains only the variables X 1 , ..., X n −1 equals the corresponding coefficient of B , then A and B have equal lacunary parts. (This is because every monomial which can appear in a lacunary part must lack at least one variable, and thus can be transformed by a permutation of the variables into a monomial which contains only the variables X 1 , ..., X n −1 .)  But the terms of P which contain only the variables X 1 , ..., X n −1 are precisely the terms that survive the operation of setting X n to 0, so their sum equals    P   (   X  1   ,  …  ,   X   n  -  1    ,  0  )       P    subscript  X  1   normal-…   subscript  X    n  1    0     P(X_{1},\ldots,X_{n-1},0)   , which is a symmetric polynomial in the variables X 1 , ..., X n −1 that we shall denote by     P  ~    (   X  1   ,  …  ,   X   n  -  1    )        normal-~  P     subscript  X  1   normal-…   subscript  X    n  1       \tilde{P}(X_{1},\ldots,X_{n-1})   . By the inductive assumption, this polynomial can be written as        P  ~    (   X  1   ,  …  ,   X   n  -  1    )    =    Q  ~    (   σ   1  ,   n  -  1     ,  …  ,   σ    n  -  1   ,   n  -  1     )           normal-~  P     subscript  X  1   normal-…   subscript  X    n  1         normal-~  Q     subscript  σ   1    n  1     normal-…   subscript  σ     n  1     n  1         \tilde{P}(X_{1},\ldots,X_{n-1})=\tilde{Q}(\sigma_{1,n-1},\ldots,\sigma_{n-1,n-%
 1})   for some    Q  ~     normal-~  Q    \tilde{Q}   . Here the doubly indexed    σ   j  ,   n  -  1       subscript  σ   j    n  1      \sigma_{j,n-1}   denote the elementary symmetric polynomials in n −1 variables.  Consider now the polynomial        R   (   X  1   ,  …  ,   X  n   )    :=    Q  ~    (   σ   1  ,  n    ,  …  ,   σ    n  -  1   ,  n    )     .     assign    R    subscript  X  1   normal-…   subscript  X  n        normal-~  Q     subscript  σ   1  n    normal-…   subscript  σ     n  1   n        R(X_{1},\ldots,X_{n}):=\tilde{Q}(\sigma_{1,n},\ldots,\sigma_{n-1,n})\ .   Then    R   (   X  1   ,  …  ,   X  n   )       R    subscript  X  1   normal-…   subscript  X  n      R(X_{1},\ldots,X_{n})   is a symmetric polynomial in X 1 , ..., X n , of the same degree as    P  lacunary     subscript  P  lacunary    P_{\mbox{lacunary}}   , which satisfies       R   (   X  1   ,  …  ,   X   n  -  1    ,  0  )    =    Q  ~    (   σ   1  ,   n  -  1     ,  …  ,   σ    n  -  1   ,   n  -  1     )    =   P   (   X  1   ,  …  ,   X   n  -  1    ,  0  )            R    subscript  X  1   normal-…   subscript  X    n  1    0       normal-~  Q     subscript  σ   1    n  1     normal-…   subscript  σ     n  1     n  1              P    subscript  X  1   normal-…   subscript  X    n  1    0       R(X_{1},\ldots,X_{n-1},0)=\tilde{Q}(\sigma_{1,n-1},\ldots,\sigma_{n-1,n-1})=P(%
 X_{1},\ldots,X_{n-1},0)   (the first equality holds because setting X n to 0 in    σ   j  ,  n      subscript  σ   j  n     \sigma_{j,n}   gives    σ   j  ,   n  -  1       subscript  σ   j    n  1      \sigma_{j,n-1}   , for all    σ   n  ,  n      subscript  σ   n  n     \sigma_{n,n}   of all variables, which equals the elementary symmetric polynomial     P  -  R   =     σ   n  ,  n     Q         P  R      subscript  σ   n  n    Q     P-R=\sigma_{n,n}\,Q   . Then writing     e  1   ,  …  ,   e  n       subscript  e  1   normal-…   subscript  e  n     e_{1},\ldots,e_{n}   , the quotient Q is a homogeneous symmetric polynomial of degree less than d (in fact degree at most d − n ) which by the inductive assumption can be expressed as a polynomial in the elementary symmetric functions. Combining the representations for P − R and R one finds a polynomial representation for P .  The uniqueness of the representation can be proved inductively in a similar way. (It is equivalent to the fact that the n polynomials    A    [   X  1   ,  …  ,   X  n   ]    S  n        A   superscript    subscript  X  1   normal-…   subscript  X  n     subscript  S  n      A[X_{1},\ldots,X_{n}]^{S_{n}}   are algebraically independent over the ring A .) The fact that the polynomial representation is unique implies that    A   [   Y  1   ,  …  ,   Y  n   ]       A    subscript  Y  1   normal-…   subscript  Y  n      A[Y_{1},\ldots,Y_{n}]   is isomorphic to   d   d   d   .  An alternative proof  The following proof is also inductive, but does not involve other polynomials than those symmetric in X 1 ,..., X n , and also leads to a fairly direct procedure to effectively write a symmetric polynomial as a polynomial in the elementary symmetric ones. Assume the symmetric polynomial to be homogeneous of degree   d   d   d   ; different homogeneous components can be decomposed separately. Order the monomials in the variables  lexicographically , where the individual variables are ordered , in other words the dominant term of a polynomial is one with the highest occurring power of , and among those the one with the highest power of , etc. Furthermore parametrize all products of elementary symmetric polynomials that have degree   d   d   d   (they are in fact homogeneous) as follows by partitions of   i   i   i   . Order the individual elementary symmetric polynomials in the product so that those with larger indices   i   i   i   come first, then build for each such factor a column of   d   d   d   boxes, and arrange those columns from left to right to form a Young diagram containing   d   d   d   boxes in all. The shape of this diagram is a partition of   λ   λ   λ   , and each partition   d   d   d   of   λ   λ   λ   arises for exactly one product of elementary symmetric polynomials, which we shall denote by ,…, ) (the "t" is present only because traditionally this product is associated to the transpose partition of   i   i   i   ). The essential ingredient of the proof is the following simple property, which uses multi-index notation for monomials in the variables .  Lemma . The leading term of is .   Proof . The leading term of the product is the product of the leading terms of each factor (this is true whenever one uses a monomial order , like the lexicographic order used here), and the leading term of the factor is clearly . To count the occurrences of the individual variables in the resulting monomial, fill the column of the Young diagram corresponding to the factor concerned with the numbers 1…,   P   P   P   of the variables, then all boxes in the first row contain 1, those in the second row 2, and so forth, which means the leading term is .   Now one proves by induction on the leading monomial in lexicographic order, that any nonzero homogeneous symmetric polynomial   d   d   d   of degree   P   P   P   can be written as polynomial in the elementary symmetric polynomials. Since   λ   λ   λ   is symmetric, its leading monomial has weakly decreasing exponents, so it is some with   d   d   d   a partition of   c   c   c   . Let the coefficient of this term be   P   P   P   , then is either zero or a symmetric polynomial with a strictly smaller leading monomial. Writing this difference inductively as a polynomial in the elementary symmetric polynomials, and adding back to it, one obtains the sought for polynomial expression for $P$ .  The fact that this expression is unique, or equivalently that all the products (monomials) of elementary symmetric polynomials are linearly independent, is also easily proved. The lemma shows that all these products have different leading monomials, and this suffices: if a nontrivial linear combination of the were zero, one focuses on the contribution in the linear combination with nonzero coefficient and with (as polynomial in the variables ) the largest leading monomial; the leading term of this contribution cannot be cancelled by any other contribution of the linear combination, which gives a contradiction.  See also   Symmetric polynomial  Complete homogeneous symmetric polynomial  Schur polynomial  Newton's identities  MacMahon Master theorem  Symmetric function  Representation theory   References   Macdonald, I.G. (1995), Symmetric Functions and Hall Polynomials , second ed. Oxford: Clarendon Press. ISBN 0-19-850450-0 (paperback, 1998).  Richard P. Stanley (1999), Enumerative Combinatorics , Vol. 2. Cambridge: Cambridge University Press. ISBN 0-521-56069-1   "  Category:Homogeneous polynomials  Category:Symmetric functions  Category:Articles containing proofs   