   Cartesian tensor      Cartesian tensor   (Figure)  Two different 3d orthonormal bases : each basis consists of unit vectors that are mutually perpendicular.   In geometry and linear algebra , a Cartesian tensor uses an orthonormal basis to represent a tensor in a Euclidean space in the form of components. Converting a tensor's components from one such basis to another is through an orthogonal transformation .  The most familiar coordinate systems are the two-dimensional and three-dimensional  Cartesian coordinate systems. Cartesian tensors may be used with any Euclidean space, or more technically, any finite-dimensional vector space over the field of real numbers that has an inner product .  Use of Cartesian tensors occurs in physics and engineering , such as with the Cauchy stress tensor and the moment of inertia tensor in rigid body dynamics . Sometimes general curvilinear coordinates are convenient, as in high-deformation continuum mechanics , or even necessary, as in general relativity . While orthonormal bases may be found for some such coordinate systems (e.g. tangent to spherical coordinates ), Cartesian tensors may provide considerable simplification for applications in which rotations of rectilinear coordinate axes suffice. The transformation is a passive transformation , since the coordinates are changed and not the physical system.  Cartesian basis and related terminology  Vectors in three dimensions  In 3d  Euclidean space , ℝ 3 , the standard basis is e x , e y , e z . Each basis vector points along the x-, y-, and z-axes, and the vectors are all unit vectors (or normalized), so the basis is orthonormal .  Throughout, when referring to Cartesian coordinates in three dimensions , a right-handed system is assumed and this is much more common than a left-handed system in practice, see orientation (vector space) for details.  For Cartesian tensors of order 1, a Cartesian vector a can be written algebraically as a linear combination of the basis vectors e x , e y , e z :      𝐚  =     a  x    𝐞  x    +    a  y    𝐞  y    +    a  z    𝐞  z         𝐚       subscript  a  x    subscript  𝐞  x       subscript  a  y    subscript  𝐞  y       subscript  a  z    subscript  𝐞  z       \mathbf{a}=a_{\text{x}}\mathbf{e}_{\text{x}}+a_{\text{y}}\mathbf{e}_{\text{y}}%
 +a_{\text{z}}\mathbf{e}_{\text{z}}     where the coordinates of the vector with respect to the Cartesian basis are denoted a x , a y , a z . It is common and helpful to display the basis vectors as column vectors        𝐞  x   =    (     1      0      0     )     ,     𝐞  y   =    (     0      1      0     )     ,    𝐞  z   =   (     0      0      1     )        formulae-sequence     subscript  𝐞  x     1    0    0      formulae-sequence     subscript  𝐞  y     0    1    0        subscript  𝐞  z     0    0    1        \mathbf{e}_{\text{x}}=\begin{pmatrix}1\\
 0\\
 0\end{pmatrix}\,,\quad\mathbf{e}_{\text{y}}=\begin{pmatrix}0\\
 1\\
 0\end{pmatrix}\,,\quad\mathbf{e}_{\text{z}}=\begin{pmatrix}0\\
 0\\
 1\end{pmatrix}     when we have a coordinate vector in a column vector representation:      𝐚  =   (      a  x        a  y        a  z      )       𝐚     subscript  a  x      subscript  a  y      subscript  a  z       \mathbf{a}=\begin{pmatrix}a_{\text{x}}\\
 a_{\text{y}}\\
 a_{\text{z}}\end{pmatrix}     A row vector representation is also legitimate, although in the context of general curvilinear coordinate systems the row and column vector representations are used separately for specific reasons – see Einstein notation and covariance and contravariance of vectors for why.  The term "component" of a vector is ambiguous: it could refer to:   a specific coordinate of the vector such as a z (a scalar), and similarly for x and y , or  the coordinate scalar-multiplying the corresponding basis vector, in which case the "y-component" of a is a y e y (a vector), and similarly for y and z.   A more general notation is tensor index notation , which has the flexibility of numerical values rather than fixed coordinate labels. The Cartesian labels are replaced by tensor indices in the basis vectors e x ↦ e 1 , e y ↦ e 2 , e z ↦ e 3 and coordinates A x ↦ A 1 , A y ↦ A 2 , A z ↦ A 3 . In general, the notation e 1 , e 2 , e 3 refers to any basis, and A 1 , A 2 , A 3 refers to the corresponding coordinate system; although here they are restricted to the Cartesian system. Then:      𝐚  =     a  1    𝐞  1    +    a  2    𝐞  2    +    a  3    𝐞  3     =    ∑   i  =  1   3     a  i    𝐞  i           𝐚       subscript  a  1    subscript  𝐞  1       subscript  a  2    subscript  𝐞  2       subscript  a  3    subscript  𝐞  3            superscript   subscript     i  1    3      subscript  a  i    subscript  𝐞  i        \mathbf{a}=a_{1}\mathbf{e}_{1}+a_{2}\mathbf{e}_{2}+a_{3}\mathbf{e}_{3}=\sum_{i%
 =1}^{3}a_{i}\mathbf{e}_{i}     It is standard to use the Einstein notation – the summation sign for summation over an index repeated only twice within a term may be suppressed for notational conciseness:      𝐚  =    ∑   i  =  1   3     a  i    𝐞  i     ≡    a  i    𝐞  i          𝐚    superscript   subscript     i  1    3      subscript  a  i    subscript  𝐞  i             subscript  a  i    subscript  𝐞  i       \mathbf{a}=\sum_{i=1}^{3}a_{i}\mathbf{e}_{i}\equiv a_{i}\mathbf{e}_{i}     An advantage of the index notation over coordinate-specific notations is the independence of the dimension of the underlying vector space, i.e. the same expression on the right hand side takes the same form in higher dimensions (see below). Previously, the Cartesian labels x, y, z were just labels and not indices. (It is informal to say " i = x, y, z").  Second order tensors in three dimensions  A dyadic tensor  T is an order 2 tensor formed by the tensor product ⊗ of two Cartesian vectors a and b , written T = a ⊗ b . Analogous to vectors, it can be written as a linear combination of the tensor basis , , ..., (the right hand side of each identity is only an abbreviation, nothing more):        𝐓    =      (     a  x    𝐞  x    +    a  y    𝐞  y    +    a  z    𝐞  z     )   ⊗   (     b  x    𝐞  x    +    b  y    𝐞  y    +    b  z    𝐞  z     )              =        a  x    b  x    𝐞  x    ⊗   𝐞  x    +     a  x    b  y    𝐞  x    ⊗   𝐞  y    +     a  x    b  z    𝐞  x    ⊗   𝐞  z             +     a  y    b  x    𝐞  y    ⊗   𝐞  x     +     a  y    b  y    𝐞  y    ⊗   𝐞  y    +     a  y    b  z    𝐞  y    ⊗   𝐞  z             +     a  z    b  x    𝐞  z    ⊗   𝐞  x     +     a  z    b  y    𝐞  z    ⊗   𝐞  y    +     a  z    b  z    𝐞  z    ⊗   𝐞  z           𝐓    tensor-product       subscript  a  x    subscript  𝐞  x       subscript  a  y    subscript  𝐞  y       subscript  a  z    subscript  𝐞  z          subscript  b  x    subscript  𝐞  x       subscript  b  y    subscript  𝐞  y       subscript  b  z    subscript  𝐞  z         missing-subexpression    missing-subexpression    missing-subexpression      missing-subexpression       tensor-product     subscript  a  x    subscript  b  x    subscript  𝐞  x     subscript  𝐞  x     tensor-product     subscript  a  x    subscript  b  y    subscript  𝐞  x     subscript  𝐞  y     tensor-product     subscript  a  x    subscript  b  z    subscript  𝐞  x     subscript  𝐞  z        missing-subexpression    missing-subexpression        tensor-product     subscript  a  y    subscript  b  x    subscript  𝐞  y     subscript  𝐞  x      tensor-product     subscript  a  y    subscript  b  y    subscript  𝐞  y     subscript  𝐞  y     tensor-product     subscript  a  y    subscript  b  z    subscript  𝐞  y     subscript  𝐞  z        missing-subexpression    missing-subexpression        tensor-product     subscript  a  z    subscript  b  x    subscript  𝐞  z     subscript  𝐞  x      tensor-product     subscript  a  z    subscript  b  y    subscript  𝐞  z     subscript  𝐞  y     tensor-product     subscript  a  z    subscript  b  z    subscript  𝐞  z     subscript  𝐞  z        \begin{array}[]{ccl}\mathbf{T}&=&\left(a_{\text{x}}\mathbf{e}_{\text{x}}+a_{%
 \text{y}}\mathbf{e}_{\text{y}}+a_{\text{z}}\mathbf{e}_{\text{z}}\right)\otimes%
 \left(b_{\text{x}}\mathbf{e}_{\text{x}}+b_{\text{y}}\mathbf{e}_{\text{y}}+b_{%
 \text{z}}\mathbf{e}_{\text{z}}\right)\\
 &&\\
 &=&a_{\text{x}}b_{\text{x}}\mathbf{e}_{\text{x}}\otimes\mathbf{e}_{\text{x}}+a%
 _{\text{x}}b_{\text{y}}\mathbf{e}_{\text{x}}\otimes\mathbf{e}_{\text{y}}+a_{%
 \text{x}}b_{\text{z}}\mathbf{e}_{\text{x}}\otimes\mathbf{e}_{\text{z}}\\
 &&{}+a_{\text{y}}b_{\text{x}}\mathbf{e}_{\text{y}}\otimes\mathbf{e}_{\text{x}}%
 +a_{\text{y}}b_{\text{y}}\mathbf{e}_{\text{y}}\otimes\mathbf{e}_{\text{y}}+a_{%
 \text{y}}b_{\text{z}}\mathbf{e}_{\text{y}}\otimes\mathbf{e}_{\text{z}}\\
 &&{}+a_{\text{z}}b_{\text{x}}\mathbf{e}_{\text{z}}\otimes\mathbf{e}_{\text{x}}%
 +a_{\text{z}}b_{\text{y}}\mathbf{e}_{\text{z}}\otimes\mathbf{e}_{\text{y}}+a_{%
 \text{z}}b_{\text{z}}\mathbf{e}_{\text{z}}\otimes\mathbf{e}_{\text{z}}\\
 \end{array}     Representing each basis tensor as a matrix:         𝐞  x   ⊗   𝐞  x    ≡   𝐞  xx   =    (     1    0    0      0    0    0      0    0    0     )     ,     𝐞  x   ⊗   𝐞  y    ≡   𝐞  xy   =    (     0    1    0      0    0    0      0    0    0     )     ,    ⋯    𝐞  z   ⊗   𝐞  z     ≡   𝐞  zz   =   (     0    0    0      0    0    0      0    0    1     )       formulae-sequence       tensor-product   subscript  𝐞  x    subscript  𝐞  x     subscript  𝐞  xx          1  0  0    0  0  0    0  0  0           tensor-product   subscript  𝐞  x    subscript  𝐞  y     subscript  𝐞  xy          0  1  0    0  0  0    0  0  0           normal-⋯   tensor-product   subscript  𝐞  z    subscript  𝐞  z      subscript  𝐞  zz          0  0  0    0  0  0    0  0  1        {\mathbf{e}_{\text{x}}\otimes\mathbf{e}_{\text{x}}}\equiv\mathbf{e}_{\text{xx}%
 }=\begin{pmatrix}1&0&0\\
 0&0&0\\
 0&0&0\end{pmatrix}\,,\quad{\mathbf{e}_{\text{x}}\otimes\mathbf{e}_{\text{y}}}%
 \equiv\mathbf{e}_{\text{xy}}=\begin{pmatrix}0&1&0\\
 0&0&0\\
 0&0&0\end{pmatrix}\,,\cdots\quad{\mathbf{e}_{\text{z}}\otimes\mathbf{e}_{\text%
 {z}}}\equiv\mathbf{e}_{\text{zz}}=\begin{pmatrix}0&0&0\\
 0&0&0\\
 0&0&1\end{pmatrix}     then T can be represented more systematically as a matrix:      𝐓  =   (       a  x    b  x        a  x    b  y        a  x    b  z          a  y    b  x        a  y    b  y        a  y    b  z          a  z    b  x        a  z    b  y        a  z    b  z       )       𝐓       subscript  a  x    subscript  b  x       subscript  a  x    subscript  b  y       subscript  a  x    subscript  b  z         subscript  a  y    subscript  b  x       subscript  a  y    subscript  b  y       subscript  a  y    subscript  b  z         subscript  a  z    subscript  b  x       subscript  a  z    subscript  b  y       subscript  a  z    subscript  b  z        \mathbf{T}=\begin{pmatrix}a_{\text{x}}b_{\text{x}}&a_{\text{x}}b_{\text{y}}&a_%
 {\text{x}}b_{\text{z}}\\
 a_{\text{y}}b_{\text{x}}&a_{\text{y}}b_{\text{y}}&a_{\text{y}}b_{\text{z}}\\
 a_{\text{z}}b_{\text{x}}&a_{\text{z}}b_{\text{y}}&a_{\text{z}}b_{\text{z}}\end%
 {pmatrix}     See matrix multiplication for the notational correspondence between matrices and the dot and tensor products.  More generally, whether or not T is a tensor product of two vectors, it is always a linear combination of the basis tensors with coordinates T xx , T xy , ... T zz :        𝐓    =       T  xx    𝐞  xx    +    T  xy    𝐞  xy    +    T  xz    𝐞  xz             +    T  yx    𝐞  yx     +    T  yy    𝐞  yy    +    T  yz    𝐞  yz             +    T  zx    𝐞  zx     +    T  zy    𝐞  zy    +    T  zz    𝐞  zz           𝐓        subscript  T  xx    subscript  𝐞  xx       subscript  T  xy    subscript  𝐞  xy       subscript  T  xz    subscript  𝐞  xz        missing-subexpression    missing-subexpression          subscript  T  yx    subscript  𝐞  yx        subscript  T  yy    subscript  𝐞  yy       subscript  T  yz    subscript  𝐞  yz        missing-subexpression    missing-subexpression          subscript  T  zx    subscript  𝐞  zx        subscript  T  zy    subscript  𝐞  zy       subscript  T  zz    subscript  𝐞  zz        \begin{array}[]{ccl}\mathbf{T}&=&T_{\text{xx}}\mathbf{e}_{\text{xx}}+T_{\text{%
 xy}}\mathbf{e}_{\text{xy}}+T_{\text{xz}}\mathbf{e}_{\text{xz}}\\
 &&{}+T_{\text{yx}}\mathbf{e}_{\text{yx}}+T_{\text{yy}}\mathbf{e}_{\text{yy}}+T%
 _{\text{yz}}\mathbf{e}_{\text{yz}}\\
 &&{}+T_{\text{zx}}\mathbf{e}_{\text{zx}}+T_{\text{zy}}\mathbf{e}_{\text{zy}}+T%
 _{\text{zz}}\mathbf{e}_{\text{zz}}\end{array}     while in terms of tensor indices:       𝐓  =    T   i  j     𝐞   i  j     ≡    ∑   i  j       T   i  j     𝐞  i    ⊗    𝐞  j       ,        𝐓     subscript  T    i  j     subscript  𝐞    i  j            subscript     i  j     tensor-product     subscript  T    i  j     subscript  𝐞  i     subscript  𝐞  j        \mathbf{T}=T_{ij}\mathbf{e}_{ij}\equiv\sum_{ij}T_{ij}\mathbf{e}_{i}\otimes%
 \mathbf{e}_{j}\,,     and in matrix form:      𝐓  =   (      T  xx      T  xy      T  xz        T  yx      T  yy      T  yz        T  zx      T  zy      T  zz      )       𝐓     subscript  T  xx    subscript  T  xy    subscript  T  xz      subscript  T  yx    subscript  T  yy    subscript  T  yz      subscript  T  zx    subscript  T  zy    subscript  T  zz       \mathbf{T}=\begin{pmatrix}T_{\text{xx}}&T_{\text{xy}}&T_{\text{xz}}\\
 T_{\text{yx}}&T_{\text{yy}}&T_{\text{yz}}\\
 T_{\text{zx}}&T_{\text{zy}}&T_{\text{zz}}\end{pmatrix}     Second order tensors occur naturally in physics and engineering when physical quantities have directional dependence in the system, often in a "stimulus-response" way. This can be mathematically seen through one aspect of tensors - they are multilinear functions . A second order tensor T which takes in a vector u of some magnitude and direction will return a vector v ; of a different magnitude and in a different direction to u , in general. The notation used for functions in mathematical analysis leads us to write , 1 while the same idea can be expressed in matrix and index notations 2 (including the summation convention), respectively:        (      v  x        v  y        v  z      )   =    (      T  xx      T  xy      T  xz        T  yx      T  yy      T  yz        T  zx      T  zy      T  zz      )     (      u  x        u  y        u  z      )      ,    v  i   =    T   i  j     u  j        formulae-sequence       subscript  v  x      subscript  v  y      subscript  v  z          subscript  T  xx    subscript  T  xy    subscript  T  xz      subscript  T  yx    subscript  T  yy    subscript  T  yz      subscript  T  zx    subscript  T  zy    subscript  T  zz        subscript  u  x      subscript  u  y      subscript  u  z          subscript  v  i      subscript  T    i  j     subscript  u  j       \begin{pmatrix}v_{\text{x}}\\
 v_{\text{y}}\\
 v_{\text{z}}\end{pmatrix}=\begin{pmatrix}T_{\text{xx}}&T_{\text{xy}}&T_{\text{%
 xz}}\\
 T_{\text{yx}}&T_{\text{yy}}&T_{\text{yz}}\\
 T_{\text{zx}}&T_{\text{zy}}&T_{\text{zz}}\end{pmatrix}\begin{pmatrix}u_{\text{%
 x}}\\
 u_{\text{y}}\\
 u_{\text{z}}\end{pmatrix}\,,\quad v_{i}=T_{ij}u_{j}     By "linear", if  for two scalars ρ and σ and vectors r and s , then in function and index notations:      𝐯  =   𝐓   (    ρ  𝐫   +   σ  𝐬    )    =    ρ  𝐓   (  𝐫  )    +   σ  𝐓   (  𝐬  )           𝐯    𝐓      ρ  𝐫     σ  𝐬              ρ  𝐓  𝐫     σ  𝐓  𝐬       \mathbf{v}=\mathbf{T}(\rho\mathbf{r}+\sigma\mathbf{s})=\rho\mathbf{T}(\mathbf{%
 r})+\sigma\mathbf{T}(\mathbf{s})          v  i   =    T   i  j     (    ρ   r  j    +   σ   s  j     )    =    ρ   T   i  j     r  j    +   σ   T   i  j     s  j            subscript  v  i      subscript  T    i  j        ρ   subscript  r  j      σ   subscript  s  j               ρ   subscript  T    i  j     subscript  r  j      σ   subscript  T    i  j     subscript  s  j        v_{i}=T_{ij}(\rho r_{j}+\sigma s_{j})=\rho T_{ij}r_{j}+\sigma T_{ij}s_{j}     and similarly for the matrix notation. The function, matrix, and index notations all mean the same thing. The matrix forms provide a clear display of the components, while the index form allows easier tensor-algebraic manipulation of the formulae in a compact manner. Both provide the physical interpretation of directions ; vectors have one direction, while second order tensors connect two directions together. One can associate a tensor index or coordinate label with a basis vector direction.  The use of second order tensors are the minimum to describe changes in magnitudes and directions of vectors, as the dot product of two vectors is always a scalar, while the cross product of two vectors is always a pseudovector perpendicular to the plane defined by the vectors, so these products of vectors alone cannot obtain a new vector of any magnitude in any direction. (See also below for more on the dot and cross products). The tensor product of two vectors is a second order tensor, although this has no obvious directional interpretation by itself.  The previous idea can be continued: if T takes in two vectors p and q , it will return a scalar r . In function notation we write r = T ( p , q ), while in matrix and index notations (including the summation convention) respectively:       r  =    (      p  x      p  y      p  z      )    (      T  xx      T  xy      T  xz        T  yx      T  yy      T  yz        T  zx      T  zy      T  zz      )     (      q  x        q  y        q  z      )      ,   r  =    p  i    T   i  j     q  j        formulae-sequence    r       subscript  p  x    subscript  p  y    subscript  p  z        subscript  T  xx    subscript  T  xy    subscript  T  xz      subscript  T  yx    subscript  T  yy    subscript  T  yz      subscript  T  zx    subscript  T  zy    subscript  T  zz        subscript  q  x      subscript  q  y      subscript  q  z         r     subscript  p  i    subscript  T    i  j     subscript  q  j       r=\begin{pmatrix}p_{\text{x}}&p_{\text{y}}&p_{\text{z}}\end{pmatrix}\begin{%
 pmatrix}T_{\text{xx}}&T_{\text{xy}}&T_{\text{xz}}\\
 T_{\text{yx}}&T_{\text{yy}}&T_{\text{yz}}\\
 T_{\text{zx}}&T_{\text{zy}}&T_{\text{zz}}\end{pmatrix}\begin{pmatrix}q_{\text{%
 x}}\\
 q_{\text{y}}\\
 q_{\text{z}}\end{pmatrix}\,,\quad r=p_{i}T_{ij}q_{j}     The tensor T is linear in both input vectors. When vectors and tensors are written without reference to components, and indices are not used, sometimes a dot · is placed where summations over indices (known as tensor contractions ) are taken. For the above cases: 3 4      𝐯  =   𝐓  ⋅  𝐮       𝐯   normal-⋅  𝐓  𝐮     \mathbf{v}=\mathbf{T}\cdot\mathbf{u}         r  =   𝐩  ⋅  𝐓  ⋅  𝐪       r   normal-⋅  𝐩  𝐓  𝐪     r=\mathbf{p}\cdot\mathbf{T}\cdot\mathbf{q}     motivated by the dot product notation:       𝐚  ⋅  𝐛   ≡    a  i    b  i         normal-⋅  𝐚  𝐛      subscript  a  i    subscript  b  i      \mathbf{a}\cdot\mathbf{b}\equiv a_{i}b_{i}     More generally, a tensor of order m which takes in n vectors (where n is between 0 and m inclusive) will return a tensor of order , see Tensor: As multilinear maps for further generalizations and details. The concepts above also apply to pseudovectors in the same way as for vectors. The vectors and tensors themselves can vary within throughout space, in which case we have vector fields and tensor fields , and can also depend on time.  Following are some examples:        scope="col" An applied or given...   scope="col" ...to a material or object of...   scope="col" ...results in...   scope="col" ...in the material or object, given by:       unit vector  n   Cauchy stress tensor  σ   a traction force t       𝐭  =   𝝈  ⋅  𝐧       𝐭   normal-⋅  𝝈  𝐧     \mathbf{t}=\boldsymbol{\sigma}\cdot\mathbf{n}        angular velocity  ω   moment of inertia  I   an angular momentum  J       𝐉  =   𝐈  ⋅  𝝎       𝐉   normal-⋅  𝐈  𝝎     \mathbf{J}=\mathbf{I}\cdot\boldsymbol{\omega}        | moment of inertia  I   a rotational kinetic energy  T       T  =     1  2   𝝎   ⋅  𝐈  ⋅  𝝎       T   normal-⋅      1  2   𝝎   𝐈  𝝎     T=\frac{1}{2}\boldsymbol{\omega}\cdot\mathbf{I}\cdot\boldsymbol{\omega}        electric field  E   electrical conductivity  σ   a current density flow J       𝐉  =   𝝈  ⋅  𝐄       𝐉   normal-⋅  𝝈  𝐄     \mathbf{J}=\boldsymbol{\sigma}\cdot\mathbf{E}        | polarizability  α (related to the permittivity  ε and electric susceptibility  χ E )   an induced polarization field P       𝐏  =   𝜶  ⋅  𝐄       𝐏   normal-⋅  𝜶  𝐄     \mathbf{P}=\boldsymbol{\alpha}\cdot\mathbf{E}        magnetic H field   magnetic permeability  μ   a magnetic B field       𝐁  =   𝝁  ⋅  𝐇       𝐁   normal-⋅  𝝁  𝐇     \mathbf{B}=\boldsymbol{\mu}\cdot\mathbf{H}            For the electrical conduction example, the index and matrix notations would be:       J  i   =    σ   i  j     E  j    ≡    ∑  j     σ   i  j     E  j            subscript  J  i      subscript  σ    i  j     subscript  E  j           subscript   j      subscript  σ    i  j     subscript  E  j        J_{i}=\sigma_{ij}E_{j}\equiv\sum_{j}\sigma_{ij}E_{j}          (      J  x        J  y        J  z      )   =    (      σ  xx      σ  xy      σ  xz        σ  yx      σ  yy      σ  yz        σ  zx      σ  zy      σ  zz      )    (      E  x        E  y        E  z      )           subscript  J  x      subscript  J  y      subscript  J  z          subscript  σ  xx    subscript  σ  xy    subscript  σ  xz      subscript  σ  yx    subscript  σ  yy    subscript  σ  yz      subscript  σ  zx    subscript  σ  zy    subscript  σ  zz        subscript  E  x      subscript  E  y      subscript  E  z        \begin{pmatrix}J_{\text{x}}\\
 J_{\text{y}}\\
 J_{\text{z}}\end{pmatrix}=\begin{pmatrix}\sigma_{\text{xx}}&\sigma_{\text{xy}}%
 &\sigma_{\text{xz}}\\
 \sigma_{\text{yx}}&\sigma_{\text{yy}}&\sigma_{\text{yz}}\\
 \sigma_{\text{zx}}&\sigma_{\text{zy}}&\sigma_{\text{zz}}\end{pmatrix}\begin{%
 pmatrix}E_{\text{x}}\\
 E_{\text{y}}\\
 E_{\text{z}}\end{pmatrix}     while for the rotational kinetic energy T :       T  =    1  2    ω  i    I   i  j     ω  j    ≡    1  2     ∑   i  j      ω  i    I   i  j      ω  j        ,        T      1  2    subscript  ω  i    subscript  I    i  j     subscript  ω  j             1  2     subscript     i  j       subscript  ω  i    subscript  I    i  j     subscript  ω  j         T=\frac{1}{2}\omega_{i}I_{ij}\omega_{j}\equiv\frac{1}{2}\sum_{ij}\omega_{i}I_{%
 ij}\omega_{j}\,,          T  =    1  2    (      ω  x      ω  y      ω  z      )    (      I  xx      I  xy      I  xz        I  yx      I  yy      I  yz        I  zx      I  zy      I  zz      )     (      ω  x        ω  y        ω  z      )      .      T      1  2      subscript  ω  x    subscript  ω  y    subscript  ω  z        subscript  I  xx    subscript  I  xy    subscript  I  xz      subscript  I  yx    subscript  I  yy    subscript  I  yz      subscript  I  zx    subscript  I  zy    subscript  I  zz        subscript  ω  x      subscript  ω  y      subscript  ω  z        T=\frac{1}{2}\begin{pmatrix}\omega_{\text{x}}&\omega_{\text{y}}&\omega_{\text{%
 z}}\end{pmatrix}\begin{pmatrix}I_{\text{xx}}&I_{\text{xy}}&I_{\text{xz}}\\
 I_{\text{yx}}&I_{\text{yy}}&I_{\text{yz}}\\
 I_{\text{zx}}&I_{\text{zy}}&I_{\text{zz}}\end{pmatrix}\begin{pmatrix}\omega_{%
 \text{x}}\\
 \omega_{\text{y}}\\
 \omega_{\text{z}}\end{pmatrix}\,.     See also constitutive equation for more specialized examples.  Vectors and tensors in n dimensions  In n -dimensional Euclidean space over the real numbers, ℝ n , the standard basis is denoted e 1 , e 2 , e 3 , ... e n . Each basis vector e i points along the positive x i axis, with the basis being orthonormal. Component j of e i is given by the Kronecker delta :        (   𝐞  i   )   j   =   δ   i  j         subscript   subscript  𝐞  i   j    subscript  δ    i  j      (\mathbf{e}_{i})_{j}=\delta_{ij}     A vector in ℝ n takes the form:       𝐚  =    a  i    𝐞  i    ≡    ∑  i     a  i     𝐞  i       .        𝐚     subscript  a  i    subscript  𝐞  i           subscript   i      subscript  a  i    subscript  𝐞  i        \mathbf{a}=a_{i}\mathbf{e}_{i}\equiv\sum_{i}a_{i}\mathbf{e}_{i}\,.     Similarly for the order 2 tensor above, for each vector a and b in ℝ n :       𝐓  =    a  i    b  j    𝐞   i  j     ≡    ∑   i  j       a  i    b  j    𝐞  i    ⊗    𝐞  j       ,        𝐓     subscript  a  i    subscript  b  j    subscript  𝐞    i  j            subscript     i  j     tensor-product     subscript  a  i    subscript  b  j    subscript  𝐞  i     subscript  𝐞  j        \mathbf{T}=a_{i}b_{j}\mathbf{e}_{ij}\equiv\sum_{ij}a_{i}b_{j}\mathbf{e}_{i}%
 \otimes\mathbf{e}_{j}\,,     or more generally:       𝐓  =    T   i  j     𝐞   i  j     ≡    ∑   i  j       T   i  j     𝐞  i    ⊗    𝐞  j       .        𝐓     subscript  T    i  j     subscript  𝐞    i  j            subscript     i  j     tensor-product     subscript  T    i  j     subscript  𝐞  i     subscript  𝐞  j        \mathbf{T}=T_{ij}\mathbf{e}_{ij}\equiv\sum_{ij}T_{ij}\mathbf{e}_{i}\otimes%
 \mathbf{e}_{j}\,.     Transformations of Cartesian vectors (any number of dimensions)  (Figure)  The same position vector  x represented in two 3d rectangular coordinate systems each with an orthonormal basis , the cuboids illustrate the parallelogram law for adding vector components.   Meaning of "invariance" under coordinate transformations  The position vector  x in ℝ n is a simple and common example of a vector, and can be represented in any  coordinate system . Consider the case of rectangular coordinate systems with orthonormal bases only. It is possible to have a coordinate system with rectangular geometry if the basis vectors are all mutually perpendicular and not normalized, in which case the basis is ortho gonal but not ortho normal . However, orthonormal bases are easier to manipulate and are often used in practice. The following results are true for orthonormal bases, not orthogonal ones.  In one rectangular coordinate system, x as a contravector has coordinates x i and basis vectors e i , while as a covector it has coordinates x i and basis covectors e i , and we have:       𝐱  =    x  i     𝐞  i      ,   𝐱  =    x  i    𝐞  i        formulae-sequence    𝐱     superscript  x  i    subscript  𝐞  i       𝐱     subscript  x  i    superscript  𝐞  i       \mathbf{x}=x^{i}\mathbf{e}_{i}\,,\quad\mathbf{x}=x_{i}\mathbf{e}^{i}     In another rectangular coordinate system, x as a contravector has coordinates     x  ¯     normal-¯  x    \overline{x}    i and bases    𝐞  ¯     normal-¯  𝐞    \overline{\mathbf{e}}    i , while as a covector it has coordinates     x  ¯     normal-¯  x    \overline{x}    i and bases    𝐞  ¯     normal-¯  𝐞    \overline{\mathbf{e}}    i , and we have:       𝐱  =     x  ¯   i      𝐞  ¯   i      ,   𝐱  =     x  ¯   i     𝐞  ¯   i        formulae-sequence    𝐱     superscript   normal-¯  x   i    subscript   normal-¯  𝐞   i       𝐱     subscript   normal-¯  x   i    superscript   normal-¯  𝐞   i       \mathbf{x}=\bar{x}^{i}\bar{\mathbf{e}}_{i}\,,\quad\mathbf{x}=\bar{x}_{i}\bar{%
 \mathbf{e}}^{i}     Each new coordinate is a function of all the old ones, and vice versa for the inverse function :       x  ¯    =    i    x  ¯     (   x  1   ,   x  2   ,  ⋯  )     i   ⇌  x   =    i   x    (    x  ¯   1   ,    x  ¯   2   ,  ⋯  )     i      fragments   normal-¯  x    superscript   i    normal-¯  x    superscript   fragments  normal-(   superscript  x  1   normal-,   superscript  x  2   normal-,  normal-⋯  normal-)   i    normal-⇌   x   superscript   i   x   superscript   fragments  normal-(   superscript   normal-¯  x   1   normal-,   superscript   normal-¯  x   2   normal-,  normal-⋯  normal-)   i     \bar{x}{}^{i}=\bar{x}{}^{i}\left(x^{1},x^{2},\cdots\right)\quad%
 \rightleftharpoons\quad x{}^{i}=x{}^{i}\left(\bar{x}^{1},\bar{x}^{2},\cdots\right)          x  ¯    =   i     x  ¯     (   x  1   ,   x  2   ,  ⋯  )    i    ⇌  x   =   i    x    (    x  ¯   1   ,    x  ¯   2   ,  ⋯  )    i       fragments   normal-¯  x    subscript   i    normal-¯  x    subscript   fragments  normal-(   subscript  x  1   normal-,   subscript  x  2   normal-,  normal-⋯  normal-)   i    normal-⇌   x   subscript   i   x   subscript   fragments  normal-(   subscript   normal-¯  x   1   normal-,   subscript   normal-¯  x   2   normal-,  normal-⋯  normal-)   i     \bar{x}{}_{i}=\bar{x}{}_{i}\left(x_{1},x_{2},\cdots\right)\quad%
 \rightleftharpoons\quad x{}_{i}=x{}_{i}\left(\bar{x}_{1},\bar{x}_{2},\cdots\right)     and similarly each new basis vector is a function of all the old ones, and vice versa for the inverse function:       𝐞  ¯    =   j     𝐞  ¯     (   𝐞  1   ,   𝐞  2   ⋯  )    j    ⇌  𝐞   =   j    𝐞    (    𝐞  ¯   1   ,    𝐞  ¯   2   ⋯  )    j       fragments   normal-¯  𝐞    subscript   j    normal-¯  𝐞    subscript   fragments  normal-(   subscript  𝐞  1   normal-,   subscript  𝐞  2   normal-⋯  normal-)   j    normal-⇌   e   subscript   j   e   subscript   fragments  normal-(   subscript   normal-¯  𝐞   1   normal-,   subscript   normal-¯  𝐞   2   normal-⋯  normal-)   j     \bar{\mathbf{e}}{}_{j}=\bar{\mathbf{e}}{}_{j}\left(\mathbf{e}_{1},\mathbf{e}_{%
 2}\cdots\right)\quad\rightleftharpoons\quad\mathbf{e}{}_{j}=\mathbf{e}{}_{j}%
 \left(\bar{\mathbf{e}}_{1},\bar{\mathbf{e}}_{2}\cdots\right)          𝐞  ¯    =    j    𝐞  ¯     (   𝐞  1   ,   𝐞  2   ⋯  )     j   ⇌  𝐞   =    j   𝐞    (    𝐞  ¯   1   ,    𝐞  ¯   2   ⋯  )     j      fragments   normal-¯  𝐞    superscript   j    normal-¯  𝐞    superscript   fragments  normal-(   superscript  𝐞  1   normal-,   superscript  𝐞  2   normal-⋯  normal-)   j    normal-⇌   e   superscript   j   e   superscript   fragments  normal-(   superscript   normal-¯  𝐞   1   normal-,   superscript   normal-¯  𝐞   2   normal-⋯  normal-)   j     \bar{\mathbf{e}}{}^{j}=\bar{\mathbf{e}}{}^{j}\left(\mathbf{e}^{1},\mathbf{e}^{%
 2}\cdots\right)\quad\rightleftharpoons\quad\mathbf{e}{}^{j}=\mathbf{e}{}^{j}%
 \left(\bar{\mathbf{e}}^{1},\bar{\mathbf{e}}^{2}\cdots\right)     for all i , j .  A vector is invariant under any change of basis, so if coordinates transform according to a transformation matrix  L , the bases transform according to the matrix inverse  L −1 , and conversely if the coordinates transform according to inverse L −1 , the bases transform according to the matrix L . The difference between each of these transformations is shown conventionally through the indices as superscripts for contravariance and subscripts for covariance, and the coordinates and bases are linearly transformed according to the following rules:        Vector elements   Contravariant transformation law   Covariant transformation law       Coordinates         x  ¯   j   =   x  i     (  L  )   i    =    j    x  i    𝖫  i     j      fragments   superscript   normal-¯  x   j     superscript  x  i    subscript   fragments  normal-(  L  normal-)   i    superscript   j    superscript  x  i    subscript  𝖫  i    j     \bar{x}^{j}=x^{i}(\boldsymbol{\mathsf{L}})_{i}{}^{j}=x^{i}\mathsf{L}_{i}{}^{j}            x  ¯   j   =   x  k     (   L   -  1    )   j     k      fragments   subscript   normal-¯  x   j     subscript  x  k    subscript   fragments  normal-(   superscript  normal-L    1    normal-)   j    k     \bar{x}_{j}=x_{k}(\boldsymbol{\mathsf{L}}^{-1})_{j}{}^{k}        Basis         𝐞  ¯   j   =     (   L   -  1    )   j    𝐞  k     k         subscript   normal-¯  𝐞   j      subscript   superscript  normal-L    1    j    superscript   subscript  𝐞  k   k      \bar{\mathbf{e}}_{j}=(\boldsymbol{\mathsf{L}}^{-1})_{j}{}^{k}\mathbf{e}_{k}            𝐞  ¯   j   =     (  L  )   i    𝐞   i    j    =    𝖫  i    𝐞   i    j           superscript   normal-¯  𝐞   j      subscript  normal-L  i    superscript   superscript  𝐞  i   j            subscript  𝖫  i    superscript   superscript  𝐞  i   j       \bar{\mathbf{e}}^{j}=(\boldsymbol{\mathsf{L}})_{i}{}^{j}\mathbf{e}^{i}=\mathsf%
 {L}_{i}{}^{j}\mathbf{e}^{i}        Any vector         x  ¯   j     𝐞  ¯   j   =   x  i    𝖫  i     (   L   -  1    )   j     j    𝐞  k     k   =   x  i    δ  i    𝐞  k     k   =   x  i    𝐞  i      fragments   superscript   normal-¯  x   j    subscript   normal-¯  𝐞   j     superscript  x  i    subscript  𝖫  i    superscript   subscript   fragments  normal-(   superscript  normal-L    1    normal-)   j   j    superscript   subscript  𝐞  k   k     superscript  x  i    subscript  δ  i    superscript   subscript  𝐞  k   k     superscript  x  i    subscript  𝐞  i     \bar{x}^{j}\bar{\mathbf{e}}_{j}=x^{i}\mathsf{L}_{i}{}^{j}(\boldsymbol{\mathsf{%
 L}}^{-1})_{j}{}^{k}\mathbf{e}_{k}=x^{i}\delta_{i}{}^{k}\mathbf{e}_{k}=x^{i}%
 \mathbf{e}_{i}             x  ¯   j     𝐞  ¯   j    =    x  i     (   L   -  1    )   j    𝖫  k     i    𝐞   k    j    =    x  i    δ  i    𝐞   k   k     =    x  i    𝐞  i             subscript   normal-¯  x   j    superscript   normal-¯  𝐞   j       subscript  x  i    subscript   superscript  normal-L    1    j    superscript   subscript  𝖫  k   i    superscript   superscript  𝐞  k   j            subscript  x  i    superscript  δ  i    subscript   superscript  𝐞  k   k            subscript  x  i    superscript  𝐞  i       \bar{x}_{j}\bar{\mathbf{e}}^{j}=x_{i}(\boldsymbol{\mathsf{L}}^{-1})_{j}{}^{i}%
 \mathsf{L}_{k}{}^{j}\mathbf{e}^{k}=x_{i}\delta^{i}{}_{k}\mathbf{e}^{k}=x_{i}%
 \mathbf{e}^{i}            where L i j represents the entries of the transformation matrix (row number is i and column number is j ) and ( L −1 ) i k denotes the entries of the inverse matrix of the matrix L i k .  If L is an orthogonal transformation ( orthogonal matrix ), the objects transforming by it are defined as Cartesian tensors . This geometrically has the interpretation that a rectangular coordinate system is mapped to another rectangular coordinate system, in which the norm of the vector x is preserved (and distances are preserved).  The determinant of L is det( L ) = ±1, which corresponds to two types of orthogonal transformation: (+1) for rotations and (−1) for improper rotations (including reflections ).  There are considerable algebraic simplifications, the matrix transpose is the inverse from the definition of an orthogonal transformation:       L  T   =   L   -  1    ⇒    (   L   -  1    )   i    =    j     (   L  T   )   i    =    j     (  L  )   j    =   i     𝖫  j     i      fragments   superscript  normal-L  normal-T     superscript  normal-L    1    normal-⇒   subscript   fragments  normal-(   superscript  normal-L    1    normal-)   i    superscript   j    subscript   fragments  normal-(   superscript  normal-L  normal-T   normal-)   i    superscript   j    superscript   fragments  normal-(  L  normal-)   j    subscript   i    superscript  𝖫  j    i     \boldsymbol{\mathsf{L}}^{\mathrm{T}}=\boldsymbol{\mathsf{L}}^{-1}\Rightarrow(%
 \boldsymbol{\mathsf{L}}^{-1})_{i}{}^{j}=(\boldsymbol{\mathsf{L}}^{\mathrm{T}})%
 _{i}{}^{j}=(\boldsymbol{\mathsf{L}})^{j}{}_{i}=\mathsf{L}^{j}{}_{i}     From the previous table, orthogonal transformations of covectors and contravectors are identical. There is no need to differ between raising and lowering indices , and in this context and applications to physics and engineering the indices are usually all subscripted to remove confusion for exponents . All indices will be lowered in the remainder of this article. One can determine the actual raised and lowered indices by considering which quantities are covectors or contravectors, and the relevant transformation rules.  Exactly the same transformation rules apply to any vector a , not only the position vector. If its components a i do not transform according to the rules, a is not a vector.  Despite the similarity between the expressions above, for the change of coordinates such as  L i j x i }} , and the action of a tensor on a vector like  T ij a j }} , L is not a tensor, but T is. In the change of coordinates, L is a matrix , used to relate two rectangular coordinate systems with orthonormal bases together. For the tensor relating a vector to a vector, the vectors and tensors throughout the equation all belong to the same coordinate system and basis.  Derivatives and Jacobian matrix elements  The entries of L are partial derivatives of the new or old coordinates with respect to the old or new coordinates, respectively.  Differentiating     x  ¯     normal-¯  x    \overline{x}    i with respect to x k :        ∂    x  ¯   i     ∂   x  k     =    ∂   ∂   x  k      (    x  j    𝖫   j  i     )    =    𝖫   j  i      ∂   x  j     ∂   x  k      =    δ   k  j     𝖫   j  i     =   𝖫   k  i               subscript   normal-¯  x   i       subscript  x  k             subscript  x  k        subscript  x  j    subscript  𝖫    j  i              subscript  𝖫    j  i         subscript  x  j       subscript  x  k              subscript  δ    k  j     subscript  𝖫    j  i           subscript  𝖫    k  i       \frac{\partial\bar{x}_{i}}{\partial x_{k}}=\frac{\partial}{\partial x_{k}}(x_{%
 j}\mathsf{L}_{ji})=\mathsf{L}_{ji}\frac{\partial x_{j}}{\partial x_{k}}=\delta%
 _{kj}\mathsf{L}_{ji}=\mathsf{L}_{ki}     so       𝖫  i    ≡    j    𝖫   i  j    =    ∂    x  ¯   j     ∂   x  i        fragments   subscript  𝖫  i    superscript   j    subscript  𝖫    i  j          subscript   normal-¯  x   j       subscript  x  i       \mathsf{L}_{i}{}^{j}\equiv\mathsf{L}_{ij}=\frac{\partial\bar{x}_{j}}{\partial x%
 _{i}}     is an element of the Jacobian matrix . There is a (partially mnemonical) correspondence between index positions attached to L and in the partial derivative: i at the top and j at the bottom, in each case, although for Cartesian tensors the indices can be lowered.  Conversely, differentiating x j with respect to     x  ¯     normal-¯  x    \overline{x}    i :        ∂   x  j     ∂    x  ¯   k     =    ∂   ∂    x  ¯   k      (     x  ¯   i     (   L   -  1    )    i  j     )    =     ∂    x  ¯   i     ∂    x  ¯   k       (   L   -  1    )    i  j     =    δ   k  i      (   L   -  1    )    i  j     =    (   L   -  1    )    k  j               subscript  x  j       subscript   normal-¯  x   k             subscript   normal-¯  x   k        subscript   normal-¯  x   i    subscript   superscript  normal-L    1      i  j                  subscript   normal-¯  x   i       subscript   normal-¯  x   k      subscript   superscript  normal-L    1      i  j             subscript  δ    k  i     subscript   superscript  normal-L    1      i  j           subscript   superscript  normal-L    1      k  j       \frac{\partial x_{j}}{\partial\bar{x}_{k}}=\frac{\partial}{\partial\bar{x}_{k}%
 }(\bar{x}_{i}(\boldsymbol{\mathsf{L}}^{-1})_{ij})=\frac{\partial\bar{x}_{i}}{%
 \partial\bar{x}_{k}}(\boldsymbol{\mathsf{L}}^{-1})_{ij}=\delta_{ki}(%
 \boldsymbol{\mathsf{L}}^{-1})_{ij}=(\boldsymbol{\mathsf{L}}^{-1})_{kj}     so        (   L   -  1    )   i    ≡    j     (   L   -  1    )    i  j    =    ∂   x  j     ∂    x  ¯   i        fragments   subscript   fragments  normal-(   superscript  normal-L    1    normal-)   i    superscript   j    subscript   fragments  normal-(   superscript  normal-L    1    normal-)     i  j          subscript  x  j       subscript   normal-¯  x   i       (\boldsymbol{\mathsf{L}}^{-1})_{i}{}^{j}\equiv(\boldsymbol{\mathsf{L}}^{-1})_{%
 ij}=\frac{\partial x_{j}}{\partial\bar{x}_{i}}     is an element of the inverse Jacobian matrix, with a similar index correspondence.  Many sources state transformations in terms of the partial derivatives:  and the explicit matrix equations in 3d are:       𝐱  ¯   =   L  𝐱        normal-¯  𝐱     normal-L  𝐱     \bar{\mathbf{x}}=\boldsymbol{\mathsf{L}}\mathbf{x}          (       x  ¯   1         x  ¯   2         x  ¯   3      )   =    (        ∂    x  ¯   1     ∂   x  1           ∂    x  ¯   1     ∂   x  2           ∂    x  ¯   1     ∂   x  3             ∂    x  ¯   2     ∂   x  1           ∂    x  ¯   2     ∂   x  2           ∂    x  ¯   2     ∂   x  3             ∂    x  ¯   3     ∂   x  1           ∂    x  ¯   3     ∂   x  2           ∂    x  ¯   3     ∂   x  3         )    (      x  1        x  2        x  3      )           subscript   normal-¯  x   1      subscript   normal-¯  x   2      subscript   normal-¯  x   3              subscript   normal-¯  x   1       subscript  x  1          subscript   normal-¯  x   1       subscript  x  2          subscript   normal-¯  x   1       subscript  x  3            subscript   normal-¯  x   2       subscript  x  1          subscript   normal-¯  x   2       subscript  x  2          subscript   normal-¯  x   2       subscript  x  3            subscript   normal-¯  x   3       subscript  x  1          subscript   normal-¯  x   3       subscript  x  2          subscript   normal-¯  x   3       subscript  x  3          subscript  x  1      subscript  x  2      subscript  x  3        \begin{pmatrix}\bar{x}_{1}\\
 \bar{x}_{2}\\
 \bar{x}_{3}\end{pmatrix}=\begin{pmatrix}\frac{\partial\bar{x}_{1}}{\partial x_%
 {1}}&\frac{\partial\bar{x}_{1}}{\partial x_{2}}&\frac{\partial\bar{x}_{1}}{%
 \partial x_{3}}\\
 \frac{\partial\bar{x}_{2}}{\partial x_{1}}&\frac{\partial\bar{x}_{2}}{\partial
 x%
 _{2}}&\frac{\partial\bar{x}_{2}}{\partial x_{3}}\\
 \frac{\partial\bar{x}_{3}}{\partial x_{1}}&\frac{\partial\bar{x}_{3}}{\partial
 x%
 _{2}}&\frac{\partial\bar{x}_{3}}{\partial x_{3}}\end{pmatrix}\begin{pmatrix}x_%
 {1}\\
 x_{2}\\
 x_{3}\end{pmatrix}     similarly for      𝐱  =    L   -  1     𝐱  ¯    =    L  T    𝐱  ¯          𝐱     superscript  normal-L    1     normal-¯  𝐱            superscript  normal-L  normal-T    normal-¯  𝐱       \mathbf{x}=\boldsymbol{\mathsf{L}}^{-1}\bar{\mathbf{x}}=\boldsymbol{\mathsf{L}%
 }^{\mathrm{T}}\bar{\mathbf{x}}     Projections along coordinate axes  (Figure)  Top: Angles from the x i axes to the     x  ¯     normal-¯  x    \overline{x}    i axes. Bottom: Vice versa.   As with all linear transformations, L depends on the basis chosen. For two orthonormal bases           𝐞  ¯   i   ⋅    𝐞  ¯   j    =    𝐞  i   ⋅   𝐞  j    =    δ   i  j      ,    |   𝐞  i   |   =   |    𝐞  ¯   i   |   =   1     ,     formulae-sequence       normal-⋅   subscript   normal-¯  𝐞   i    subscript   normal-¯  𝐞   j     normal-⋅   subscript  𝐞  i    subscript  𝐞  j          subscript  δ    i  j             subscript  𝐞  i       subscript   normal-¯  𝐞   i         1      \bar{\mathbf{e}}_{i}\cdot\bar{\mathbf{e}}_{j}=\mathbf{e}_{i}\cdot\mathbf{e}_{j%
 }=\delta_{ij}\,,\quad\left|\mathbf{e}_{i}\right|=\left|\bar{\mathbf{e}}_{i}%
 \right|=1\,,      projecting x to the     x  ¯     normal-¯  x    \overline{x}    axes       x  ¯   i   =     𝐞  ¯   i   ⋅  𝐱   =      𝐞  ¯   i   ⋅   x  j     𝐞  j    =    x  i     𝖫   i  j       ,         subscript   normal-¯  x   i    normal-⋅   subscript   normal-¯  𝐞   i   𝐱           normal-⋅   subscript   normal-¯  𝐞   i    subscript  x  j     subscript  𝐞  j            subscript  x  i    subscript  𝖫    i  j        \bar{x}_{i}=\bar{\mathbf{e}}_{i}\cdot\mathbf{x}=\bar{\mathbf{e}}_{i}\cdot x_{j%
 }\mathbf{e}_{j}=x_{i}\mathsf{L}_{ij}\,,       projecting x to the x axes      x  i   =    𝐞  i   ⋅  𝐱   =     𝐞  i   ⋅    x  ¯   j      𝐞  ¯   j    =     x  ¯   j      (   L   -  1    )    j  i       .         subscript  x  i    normal-⋅   subscript  𝐞  i   𝐱           normal-⋅   subscript  𝐞  i    subscript   normal-¯  x   j     subscript   normal-¯  𝐞   j            subscript   normal-¯  x   j    subscript   superscript  normal-L    1      j  i        x_{i}=\mathbf{e}_{i}\cdot\mathbf{x}=\mathbf{e}_{i}\cdot\bar{x}_{j}\bar{\mathbf%
 {e}}_{j}=\bar{x}_{j}(\boldsymbol{\mathsf{L}}^{-1})_{ji}\,.      Hence the components reduce to direction cosines between the     x  ¯     normal-¯  x    \overline{x}    i and x j axes:       𝖫   i  j    =     𝐞  ¯   i   ⋅   𝐞  j    =   cos   θ   i  j            subscript  𝖫    i  j     normal-⋅   subscript   normal-¯  𝐞   i    subscript  𝐞  j            subscript  θ    i  j        \mathsf{L}_{ij}=\bar{\mathbf{e}}_{i}\cdot\mathbf{e}_{j}=\cos\theta_{ij}           (   L   -  1    )    i  j    =    𝐞  i   ⋅    𝐞  ¯   j    =   cos   θ   j  i            subscript   superscript  normal-L    1      i  j     normal-⋅   subscript  𝐞  i    subscript   normal-¯  𝐞   j            subscript  θ    j  i        (\boldsymbol{\mathsf{L}}^{-1})_{ij}=\mathbf{e}_{i}\cdot\bar{\mathbf{e}}_{j}=%
 \cos\theta_{ji}     where θ ij and θ ji are the angles between the     x  ¯     normal-¯  x    \overline{x}    i and x j axes. In general, θ ij is not equal to θ ji , because for example θ 12 and θ 21 are two different angles.  The transformation of coordinates can be written:  _i\cdot\mathbf{e}_j \right) = x_i\cos\theta_{ij}\\ \upharpoonleft\downharpoonright\\ x_j = \bar{x}_i \left( \mathbf{e}_i\cdot\bar{\mathbf{e}}_j \right) = \bar{x}_i\cos\theta_{ji} \end{array}  |cellpadding= 6 |border = 1 |border colour = black |background colour=white}}  and the explicit matrix equations in 3d are:       𝐱  ¯   =   L  𝐱        normal-¯  𝐱     normal-L  𝐱     \bar{\mathbf{x}}=\boldsymbol{\mathsf{L}}\mathbf{x}          (       x  ¯   1         x  ¯   2         x  ¯   3      )   =    (        𝐞  ¯   1   ⋅   𝐞  1         𝐞  ¯   1   ⋅   𝐞  2         𝐞  ¯   1   ⋅   𝐞  3           𝐞  ¯   2   ⋅   𝐞  1         𝐞  ¯   2   ⋅   𝐞  2         𝐞  ¯   2   ⋅   𝐞  3           𝐞  ¯   3   ⋅   𝐞  1         𝐞  ¯   3   ⋅   𝐞  2         𝐞  ¯   3   ⋅   𝐞  3       )    (      x  1        x  2        x  3      )    =    (      cos   θ  11       cos   θ  12       cos   θ  13         cos   θ  21       cos   θ  22       cos   θ  23         cos   θ  31       cos   θ  32       cos   θ  33       )    (      x  1        x  2        x  3      )             subscript   normal-¯  x   1      subscript   normal-¯  x   2      subscript   normal-¯  x   3          normal-⋅   subscript   normal-¯  𝐞   1    subscript  𝐞  1     normal-⋅   subscript   normal-¯  𝐞   1    subscript  𝐞  2     normal-⋅   subscript   normal-¯  𝐞   1    subscript  𝐞  3       normal-⋅   subscript   normal-¯  𝐞   2    subscript  𝐞  1     normal-⋅   subscript   normal-¯  𝐞   2    subscript  𝐞  2     normal-⋅   subscript   normal-¯  𝐞   2    subscript  𝐞  3       normal-⋅   subscript   normal-¯  𝐞   3    subscript  𝐞  1     normal-⋅   subscript   normal-¯  𝐞   3    subscript  𝐞  2     normal-⋅   subscript   normal-¯  𝐞   3    subscript  𝐞  3         subscript  x  1      subscript  x  2      subscript  x  3                  subscript  θ  11       subscript  θ  12       subscript  θ  13         subscript  θ  21       subscript  θ  22       subscript  θ  23         subscript  θ  31       subscript  θ  32       subscript  θ  33         subscript  x  1      subscript  x  2      subscript  x  3         \begin{pmatrix}\bar{x}_{1}\\
 \bar{x}_{2}\\
 \bar{x}_{3}\end{pmatrix}=\begin{pmatrix}\bar{\mathbf{e}}_{1}\cdot\mathbf{e}_{1%
 }&\bar{\mathbf{e}}_{1}\cdot\mathbf{e}_{2}&\bar{\mathbf{e}}_{1}\cdot\mathbf{e}_%
 {3}\\
 \bar{\mathbf{e}}_{2}\cdot\mathbf{e}_{1}&\bar{\mathbf{e}}_{2}\cdot\mathbf{e}_{2%
 }&\bar{\mathbf{e}}_{2}\cdot\mathbf{e}_{3}\\
 \bar{\mathbf{e}}_{3}\cdot\mathbf{e}_{1}&\bar{\mathbf{e}}_{3}\cdot\mathbf{e}_{2%
 }&\bar{\mathbf{e}}_{3}\cdot\mathbf{e}_{3}\end{pmatrix}\begin{pmatrix}x_{1}\\
 x_{2}\\
 x_{3}\end{pmatrix}=\begin{pmatrix}\cos\theta_{11}&\cos\theta_{12}&\cos\theta_{%
 13}\\
 \cos\theta_{21}&\cos\theta_{22}&\cos\theta_{23}\\
 \cos\theta_{31}&\cos\theta_{32}&\cos\theta_{33}\end{pmatrix}\begin{pmatrix}x_{%
 1}\\
 x_{2}\\
 x_{3}\end{pmatrix}     similarly for      𝐱  =    L   -  1     𝐱  ¯    =    L  T    𝐱  ¯          𝐱     superscript  normal-L    1     normal-¯  𝐱            superscript  normal-L  normal-T    normal-¯  𝐱       \mathbf{x}=\boldsymbol{\mathsf{L}}^{-1}\bar{\mathbf{x}}=\boldsymbol{\mathsf{L}%
 }^{\mathrm{T}}\bar{\mathbf{x}}     The geometric interpretation is the     x  ¯     normal-¯  x    \overline{x}    i components equal to the sum of projecting the x j components onto the     x  ¯     normal-¯  x    \overline{x}    j axes.  The numbers e i ⋅ e j arranged into a matrix would form a symmetric matrix (a matrix equal to its own transpose) due to the symmetry in the dot products, in fact it is the metric tensor  g . By contrast e i ⋅    𝐞  ¯     normal-¯  𝐞    \overline{\mathbf{e}}    j or    𝐞  ¯     normal-¯  𝐞    \overline{\mathbf{e}}    i ⋅ e j do not form symmetric matrices in general, as displayed above. Therefore, while the L matrices are still orthogonal, they are not symmetric.  Apart from a rotation about any one axis, in which the x i and     x  ¯     normal-¯  x    \overline{x}    i for some i coincide, the angles are not the same as Euler angles , and so the L matrices are not the same as the rotation matrices .  Transformation of the dot and cross products (three dimensions only)  The dot product and cross product occur very frequently, in applications of vector analysis to physics and engineering, examples include:   power transferred P by an object exerting a force F with velocity v along a straight-line path:         P  =   𝐯  ⋅  𝐅       P   normal-⋅  𝐯  𝐅     P=\mathbf{v}\cdot\mathbf{F}         tangential velocity  v at a point x of a rotating rigid body with angular velocity  ω :         𝐯  =   𝝎  ×  𝐱       𝐯    𝝎  𝐱     \mathbf{v}=\boldsymbol{\omega}\times\mathbf{x}         potential energy  U of a magnetic dipole of magnetic moment  m in a uniform external magnetic field  B :         U  =   -   𝐦  ⋅  𝐁        U     normal-⋅  𝐦  𝐁      U=-\mathbf{m}\cdot\mathbf{B}         angular momentum  J for a particle with position vector  r and momentum  p :         𝐉  =   𝐫  ×  𝐩       𝐉    𝐫  𝐩     \mathbf{J}=\mathbf{r}\times\mathbf{p}         torque  τ acting on an electric dipole of electric dipole moment  p in a uniform external electric field  E :         𝝉  =   𝐩  ×  𝐄       𝝉    𝐩  𝐄     \boldsymbol{\tau}=\mathbf{p}\times\mathbf{E}         induced surface current density  j S in a magnetic material of magnetization  M on a surface with unit normal  n :          𝐣  S   =   𝐌  ×  𝐧        subscript  𝐣  normal-S     𝐌  𝐧     \mathbf{j}_{\mathrm{S}}=\mathbf{M}\times\mathbf{n}        How these products transform under orthogonal transformations is illustrated below.  Dot product, Kronecker delta, and metric tensor  The dot product ⋅ of each possible pairing of the basis vectors follows from the basis being orthonormal. For perpendicular pairs we have          𝐞  x   ⋅   𝐞  y        =    𝐞  y   ⋅   𝐞  z         =    𝐞  z   ⋅   𝐞  x            𝐞  y   ⋅   𝐞  x        =    𝐞  z   ⋅   𝐞  y         =    𝐞  x   ⋅   𝐞  z         =  0          normal-⋅   subscript  𝐞  x    subscript  𝐞  y      absent   normal-⋅   subscript  𝐞  y    subscript  𝐞  z       absent   normal-⋅   subscript  𝐞  z    subscript  𝐞  x      missing-subexpression      normal-⋅   subscript  𝐞  y    subscript  𝐞  x      absent   normal-⋅   subscript  𝐞  z    subscript  𝐞  y       absent   normal-⋅   subscript  𝐞  x    subscript  𝐞  z       absent  0      \begin{array}[]{cccc}\mathbf{e}_{\text{x}}\cdot\mathbf{e}_{\text{y}}&=\mathbf{%
 e}_{\text{y}}\cdot\mathbf{e}_{\text{z}}&=\mathbf{e}_{\text{z}}\cdot\mathbf{e}_%
 {\text{x}}\\
 \mathbf{e}_{\text{y}}\cdot\mathbf{e}_{\text{x}}&=\mathbf{e}_{\text{z}}\cdot%
 \mathbf{e}_{\text{y}}&=\mathbf{e}_{\text{x}}\cdot\mathbf{e}_{\text{z}}&=0\end{array}     while for parallel pairs we have        𝐞  x   ⋅   𝐞  x    =    𝐞  y   ⋅   𝐞  y    =    𝐞  z   ⋅   𝐞  z    =  1.         normal-⋅   subscript  𝐞  x    subscript  𝐞  x     normal-⋅   subscript  𝐞  y    subscript  𝐞  y          normal-⋅   subscript  𝐞  z    subscript  𝐞  z         1.     \mathbf{e}_{\text{x}}\cdot\mathbf{e}_{\text{x}}=\mathbf{e}_{\text{y}}\cdot%
 \mathbf{e}_{\text{y}}=\mathbf{e}_{\text{z}}\cdot\mathbf{e}_{\text{z}}=1.     Replacing Cartesian labels by index notation as shown above , these results can be summarized by        𝐞  i   ⋅   𝐞  j    =   δ   i  j         normal-⋅   subscript  𝐞  i    subscript  𝐞  j     subscript  δ    i  j      \mathbf{e}_{i}\cdot\mathbf{e}_{j}=\delta_{ij}     where δ ij are the components of the Kronecker delta . The Cartesian basis can be used to represent δ in this way.  In addition, each metric tensor component g ij with respect to any basis is the dot product of a pairing of basis vectors:        g   i  j    =    𝐞  i   ⋅   𝐞  j     .       subscript  g    i  j     normal-⋅   subscript  𝐞  i    subscript  𝐞  j      g_{ij}=\mathbf{e}_{i}\cdot\mathbf{e}_{j}.     For the Cartesian basis the components arranged into a matrix are:      𝐠  =   (      g  xx      g  xy      g  xz        g  yx      g  yy      g  zz        g  zx      g  zy      g  zz      )   =   (       𝐞  x   ⋅   𝐞  x        𝐞  x   ⋅   𝐞  y        𝐞  x   ⋅   𝐞  z          𝐞  y   ⋅   𝐞  x        𝐞  y   ⋅   𝐞  y        𝐞  y   ⋅   𝐞  z          𝐞  z   ⋅   𝐞  x        𝐞  z   ⋅   𝐞  y        𝐞  z   ⋅   𝐞  z       )   =   (     1    0    0      0    1    0      0    0    1     )         𝐠     subscript  g  xx    subscript  g  xy    subscript  g  xz      subscript  g  yx    subscript  g  yy    subscript  g  zz      subscript  g  zx    subscript  g  zy    subscript  g  zz             normal-⋅   subscript  𝐞  x    subscript  𝐞  x     normal-⋅   subscript  𝐞  x    subscript  𝐞  y     normal-⋅   subscript  𝐞  x    subscript  𝐞  z       normal-⋅   subscript  𝐞  y    subscript  𝐞  x     normal-⋅   subscript  𝐞  y    subscript  𝐞  y     normal-⋅   subscript  𝐞  y    subscript  𝐞  z       normal-⋅   subscript  𝐞  z    subscript  𝐞  x     normal-⋅   subscript  𝐞  z    subscript  𝐞  y     normal-⋅   subscript  𝐞  z    subscript  𝐞  z             1  0  0    0  1  0    0  0  1       \mathbf{g}=\begin{pmatrix}g_{\text{xx}}&g_{\text{xy}}&g_{\text{xz}}\\
 g_{\text{yx}}&g_{\text{yy}}&g_{\text{zz}}\\
 g_{\text{zx}}&g_{\text{zy}}&g_{\text{zz}}\\
 \end{pmatrix}=\begin{pmatrix}\mathbf{e}_{\text{x}}\cdot\mathbf{e}_{\text{x}}&%
 \mathbf{e}_{\text{x}}\cdot\mathbf{e}_{\text{y}}&\mathbf{e}_{\text{x}}\cdot%
 \mathbf{e}_{\text{z}}\\
 \mathbf{e}_{\text{y}}\cdot\mathbf{e}_{\text{x}}&\mathbf{e}_{\text{y}}\cdot%
 \mathbf{e}_{\text{y}}&\mathbf{e}_{\text{y}}\cdot\mathbf{e}_{\text{z}}\\
 \mathbf{e}_{\text{z}}\cdot\mathbf{e}_{\text{x}}&\mathbf{e}_{\text{z}}\cdot%
 \mathbf{e}_{\text{y}}&\mathbf{e}_{\text{z}}\cdot\mathbf{e}_{\text{z}}\\
 \end{pmatrix}=\begin{pmatrix}1&0&0\\
 0&1&0\\
 0&0&1\\
 \end{pmatrix}     so are the simplest possible for the metric tensor, namely the δ :       g   i  j    =   δ   i  j         subscript  g    i  j     subscript  δ    i  j      g_{ij}=\delta_{ij}     This is not true for general bases: orthogonal coordinates have diagonal metrics containing various scale factors (i.e. not necessarily 1), while general curvilinear coordinates could also have nonzero entries for off-diagonal components.  The dot product of two vectors a and b transforms according to       𝐚  ⋅  𝐛   =     a  ¯   j     b  ¯   j    =    a  i    𝖫   i  j     b  k     (   L   -  1    )    j  k     =    a  i    δ  i    b  k    k     =    a  i    b  i           normal-⋅  𝐚  𝐛      subscript   normal-¯  a   j    subscript   normal-¯  b   j            subscript  a  i    subscript  𝖫    i  j     subscript  b  k    subscript   superscript  normal-L    1      j  k             subscript  a  i    subscript  δ  i    subscript   subscript  b  k   k            subscript  a  i    subscript  b  i       \mathbf{a}\cdot\mathbf{b}=\bar{a}_{j}\bar{b}_{j}=a_{i}\mathsf{L}_{ij}b_{k}(%
 \boldsymbol{\mathsf{L}}^{-1})_{jk}=a_{i}\delta_{i}{}_{k}b_{k}=a_{i}b_{i}     which is intuitive, since the dot product of two vectors is a single scalar independent of any coordinates. This also applies more generally to any coordinate systems, not just rectangular ones; the dot product in one coordinate system is the same in any other.  Cross and product, Levi-Civita symbol, and pseudovectors  For the cross product × of two vectors, the results are (almost) the other way round. Again, assuming a right-handed 3d Cartesian coordinate system, cyclic permutations in perpendicular directions yield the next vector in the cyclic collection of vectors:         𝐞  x   ×   𝐞  y    =   𝐞  z        𝐞  y   ×   𝐞  z    =   𝐞  x       𝐞  z   ×   𝐞  x    =   𝐞  y        formulae-sequence       subscript  𝐞  x    subscript  𝐞  y     subscript  𝐞  z     formulae-sequence       subscript  𝐞  y    subscript  𝐞  z     subscript  𝐞  x         subscript  𝐞  z    subscript  𝐞  x     subscript  𝐞  y       \mathbf{e}_{\text{x}}\times\mathbf{e}_{\text{y}}=\mathbf{e}_{\text{z}}\,\quad%
 \mathbf{e}_{\text{y}}\times\mathbf{e}_{\text{z}}=\mathbf{e}_{\text{x}}\,\quad%
 \mathbf{e}_{\text{z}}\times\mathbf{e}_{\text{x}}=\mathbf{e}_{\text{y}}            𝐞  y   ×   𝐞  x    =   -   𝐞  z         𝐞  z   ×   𝐞  y    =   -   𝐞  x        𝐞  x   ×   𝐞  z    =   -   𝐞  y         formulae-sequence       subscript  𝐞  y    subscript  𝐞  x       subscript  𝐞  z      formulae-sequence       subscript  𝐞  z    subscript  𝐞  y       subscript  𝐞  x          subscript  𝐞  x    subscript  𝐞  z       subscript  𝐞  y        \mathbf{e}_{\text{y}}\times\mathbf{e}_{\text{x}}=-\mathbf{e}_{\text{z}}\,\quad%
 \mathbf{e}_{\text{z}}\times\mathbf{e}_{\text{y}}=-\mathbf{e}_{\text{x}}\,\quad%
 \mathbf{e}_{\text{x}}\times\mathbf{e}_{\text{z}}=-\mathbf{e}_{\text{y}}     while parallel vectors clearly vanish:        𝐞  x   ×   𝐞  x    =    𝐞  y   ×   𝐞  y    =    𝐞  z   ×   𝐞  z    =  𝟎           subscript  𝐞  x    subscript  𝐞  x       subscript  𝐞  y    subscript  𝐞  y            subscript  𝐞  z    subscript  𝐞  z         0     \mathbf{e}_{\text{x}}\times\mathbf{e}_{\text{x}}=\mathbf{e}_{\text{y}}\times%
 \mathbf{e}_{\text{y}}=\mathbf{e}_{\text{z}}\times\mathbf{e}_{\text{z}}=%
 \boldsymbol{0}     and replacing Cartesian labels by index notation as above , these can be summarized by:       𝐞  i   ×   𝐞  j   =   [      +   𝐞  k        cyclic permutations:   (  i  ,  j  ,  k  )    =    (  1  ,  2  ,  3  )   ,   (  2  ,  3  ,  1  )   ,   (  3  ,  1  ,  2  )          -   𝐞  k        anticyclic permutations:   (  i  ,  j  ,  k  )    =    (  2  ,  1  ,  3  )   ,   (  3  ,  2  ,  1  )   ,   (  1  ,  3  ,  2  )         𝟎     i  =  j          fragments   subscript  𝐞  i     subscript  𝐞  j     fragments  normal-[       subscript  𝐞  k        cyclic permutations:   i  j  k      1  2  3    2  3  1    3  1  2          subscript  𝐞  k        anticyclic permutations:   i  j  k      2  1  3    3  2  1    1  3  2       0    i  j        \mathbf{e}_{i}\times\mathbf{e}_{j}=\left[\begin{array}[]{cc}+\mathbf{e}_{k}&%
 \text{cyclic permutations: }(i,j,k)=(1,2,3),(2,3,1),(3,1,2)\\
 -\mathbf{e}_{k}&\text{anticyclic permutations: }(i,j,k)=(2,1,3),(3,2,1),(1,3,2%
 )\\
 \boldsymbol{0}&i=j\end{array}\right.     where i , j , k are indices which take values 1, 2, 3. It follows that:       𝐞  k   ⋅   𝐞  i   ×   𝐞  j   =   [      +  1       cyclic permutations:   (  i  ,  j  ,  k  )    =    (  1  ,  2  ,  3  )   ,   (  2  ,  3  ,  1  )   ,   (  3  ,  1  ,  2  )          -  1       anticyclic permutations:   (  i  ,  j  ,  k  )    =    (  2  ,  1  ,  3  )   ,   (  3  ,  2  ,  1  )   ,   (  1  ,  3  ,  2  )         0     i  =   j  or  j   =   k  or  k   =  i          fragments   subscript  𝐞  k   normal-⋅   subscript  𝐞  i     subscript  𝐞  j     fragments  normal-[      1       cyclic permutations:   i  j  k      1  2  3    2  3  1    3  1  2         1       anticyclic permutations:   i  j  k      2  1  3    3  2  1    1  3  2       0      i    j  or  j          k  or  k        i         {\mathbf{e}_{k}\cdot\mathbf{e}_{i}\times\mathbf{e}_{j}}=\left[\begin{array}[]{%
 cc}+1&\text{cyclic permutations: }(i,j,k)=(1,2,3),(2,3,1),(3,1,2)\\
 -1&\text{anticyclic permutations: }(i,j,k)=(2,1,3),(3,2,1),(1,3,2)\\
 0&i=j\text{ or }j=k\text{ or }k=i\end{array}\right.     These permutation relations and their corresponding values are important, and there is an object coinciding with this property: the Levi-Civita symbol , denoted by ε . The Levi-Civita symbol entries can be represented by the Cartesian basis:       ε   i  j  k    =     𝐞  i   ⋅   𝐞  j    ×   𝐞  k         subscript  ε    i  j  k       normal-⋅   subscript  𝐞  i    subscript  𝐞  j     subscript  𝐞  k      \varepsilon_{ijk}=\mathbf{e}_{i}\cdot\mathbf{e}_{j}\times\mathbf{e}_{k}     which geometrically corresponds to the volume of a cube spanned by the orthonormal basis vectors, with sign indicating orientation (and not a "positive or negative volume"). Here, the orientation is fixed by ε 123 = +1, for a right-handed system. A left-handed system would fix ε 123 = −1 or equivalently ε 321 = +1.  The scalar triple product can now be written:        𝐜  ⋅  𝐚   ×  𝐛   =        c  i    𝐞  i    ⋅   a  j     𝐞  j    ×   b  k     𝐞  k    =    ε   i  j  k     c  i    a  j    b  k             normal-⋅  𝐜  𝐚   𝐛          normal-⋅     subscript  c  i    subscript  𝐞  i     subscript  a  j     subscript  𝐞  j     subscript  b  k     subscript  𝐞  k            subscript  ε    i  j  k     subscript  c  i    subscript  a  j    subscript  b  k       \mathbf{c}\cdot\mathbf{a}\times\mathbf{b}=c_{i}\mathbf{e}_{i}\cdot a_{j}%
 \mathbf{e}_{j}\times b_{k}\mathbf{e}_{k}=\varepsilon_{ijk}c_{i}a_{j}b_{k}     with the geometric interpretation of volume (of the parallelepiped spanned by a , b , c ) and algebraically is a determinant : 5        𝐜  ⋅  𝐚   ×  𝐛   =   |      c  x      a  x      b  x        c  y      a  y      b  y        c  z      a  z      b  z      |          normal-⋅  𝐜  𝐚   𝐛        subscript  c  x    subscript  a  x    subscript  b  x      subscript  c  y    subscript  a  y    subscript  b  y      subscript  c  z    subscript  a  z    subscript  b  z        \mathbf{c}\cdot\mathbf{a}\times\mathbf{b}=\begin{vmatrix}c_{\text{x}}&a_{\text%
 {x}}&b_{\text{x}}\\
 c_{\text{y}}&a_{\text{y}}&b_{\text{y}}\\
 c_{\text{z}}&a_{\text{z}}&b_{\text{z}}\end{vmatrix}     This in turn can be used to rewrite the cross product of two vectors as follows:            (   𝐚  ×  𝐛   )   i   =     𝐞  i   ⋅  𝐚   ×  𝐛   =    ε   ℓ  j  k      (   𝐞  i   )   ℓ    a  j    b  k    =    ε   ℓ  j  k     δ   i  ℓ     a  j    b  k    =    ε   i  j  k     a  j    b  k         ⇒      𝐚  ×  𝐛   =     (   𝐚  ×  𝐛   )   i    𝐞  i    =    ε   i  j  k     a  j    b  k    𝐞  i            missing-subexpression        subscript    𝐚  𝐛   i      normal-⋅   subscript  𝐞  i   𝐚   𝐛           subscript  ε    normal-ℓ  j  k     subscript   subscript  𝐞  i   normal-ℓ    subscript  a  j    subscript  b  k            subscript  ε    normal-ℓ  j  k     subscript  δ    i  normal-ℓ     subscript  a  j    subscript  b  k            subscript  ε    i  j  k     subscript  a  j    subscript  b  k        normal-⇒        𝐚  𝐛      subscript    𝐚  𝐛   i    subscript  𝐞  i            subscript  ε    i  j  k     subscript  a  j    subscript  b  k    subscript  𝐞  i         \begin{array}[]{ll}&(\mathbf{a}\times\mathbf{b})_{i}={\mathbf{e}_{i}\cdot%
 \mathbf{a}\times\mathbf{b}}=\varepsilon_{\ell jk}{(\mathbf{e}_{i})}_{\ell}a_{j%
 }b_{k}=\varepsilon_{\ell jk}\delta_{i\ell}a_{j}b_{k}=\varepsilon_{ijk}a_{j}b_{%
 k}\\
 \Rightarrow&{\mathbf{a}\times\mathbf{b}}=(\mathbf{a}\times\mathbf{b})_{i}%
 \mathbf{e}_{i}=\varepsilon_{ijk}a_{j}b_{k}\mathbf{e}_{i}\end{array}     Contrary to its appearance, the Levi-Civita symbol is not a tensor , but a pseudotensor , the components transform according to:         ε  ¯    p  q  r    =   det    (  L  )    ε   i  j  k     𝖫   i  p     𝖫   j  q      𝖫   k  r        .       subscript   normal-¯  ε     p  q  r        normal-L   subscript  ε    i  j  k     subscript  𝖫    i  p     subscript  𝖫    j  q     subscript  𝖫    k  r        \bar{\varepsilon}_{pqr}=\det(\boldsymbol{\mathsf{L}})\varepsilon_{ijk}\mathsf{%
 L}_{ip}\mathsf{L}_{jq}\mathsf{L}_{kr}\,.     Therefore the transformation of the cross product of a and b is:       (    𝐚  ¯   ×   𝐛  ¯    )   i     subscript     normal-¯  𝐚    normal-¯  𝐛    i    \displaystyle(\bar{\mathbf{a}}\times\bar{\mathbf{b}})_{i}     and so a × b transforms as a pseudovector , because of the determinant factor.  The tensor index notation applies to any object which has entities that form multidimensional arrays – not everything with indices is a tensor by default. Instead, tensors are defined by how their coordinates and basis elements change under a transformation from one coordinate system to another.  Note the cross product of two vectors is a pseudovector, while the cross product of a pseudovector with a vector is another vector.  Applications of the δ tensor and ε pseudotensor  Other identities can be formed from the δ tensor and ε pseudotensor, a notable and very useful identity is one that converts two Levi-Civita symbols adjacently contracted over two indices into an antisymmetrized combination of Kronecker deltas:        ε   i  j  k     ε   p  q  k     =     δ   i  p     δ   j  q     -    δ   i  q     δ   j  p             subscript  ε    i  j  k     subscript  ε    p  q  k          subscript  δ    i  p     subscript  δ    j  q        subscript  δ    i  q     subscript  δ    j  p        \varepsilon_{ijk}\varepsilon_{pqk}=\delta_{ip}\delta_{jq}-\delta_{iq}\delta_{jp}     The index forms of the dot and cross products, together with this identity, greatly facilitate the manipulation and derivation of other identities in vector calculus and algebra, which in turn are used extensively in physics and engineering. For instance, it is clear the dot and cross products are distributive over vector addition:       𝐚  ⋅   (   𝐛  +  𝐜   )    =    a  i    (    b  i   +   c  i    )    =     a  i    b  i    +    a  i    c  i     =    𝐚  ⋅  𝐛   +   𝐚  ⋅  𝐜           normal-⋅  𝐚    𝐛  𝐜       subscript  a  i      subscript  b  i    subscript  c  i               subscript  a  i    subscript  b  i       subscript  a  i    subscript  c  i             normal-⋅  𝐚  𝐛    normal-⋅  𝐚  𝐜       \mathbf{a}\cdot(\mathbf{b}+\mathbf{c})=a_{i}(b_{i}+c_{i})=a_{i}b_{i}+a_{i}c_{i%
 }=\mathbf{a}\cdot\mathbf{b}+\mathbf{a}\cdot\mathbf{c}          𝐚  ×   (   𝐛  +  𝐜   )    =    𝐞  i    ε   i  j  k     a  j    (    b  k   +   c  k    )    =     𝐞  i    ε   i  j  k     a  j    b  k    +    𝐞  i    ε   i  j  k     a  j    c  k     =    𝐚  ×  𝐛   +   𝐚  ×  𝐜            𝐚    𝐛  𝐜       subscript  𝐞  i    subscript  ε    i  j  k     subscript  a  j      subscript  b  k    subscript  c  k               subscript  𝐞  i    subscript  ε    i  j  k     subscript  a  j    subscript  b  k       subscript  𝐞  i    subscript  ε    i  j  k     subscript  a  j    subscript  c  k              𝐚  𝐛     𝐚  𝐜       \mathbf{a}\times(\mathbf{b}+\mathbf{c})=\mathbf{e}_{i}\varepsilon_{ijk}a_{j}(b%
 _{k}+c_{k})=\mathbf{e}_{i}\varepsilon_{ijk}a_{j}b_{k}+\mathbf{e}_{i}%
 \varepsilon_{ijk}a_{j}c_{k}=\mathbf{a}\times\mathbf{b}+\mathbf{a}\times\mathbf%
 {c}     without resort to any geometric constructions - the derivation in each case is a quick line of algebra. Although the procedure is less obvious, the vector triple product can also be derived. Rewriting in index notation:        [   𝐚  ×   (   𝐛  ×  𝐜   )    ]   i   =    ε   i  j  k     a  j    (    ε   k  ℓ  m     b  ℓ    c  m    )    =    (    ε   i  j  k     ε   k  ℓ  m     )    a  j    b  ℓ    c  m           subscript   delimited-[]    𝐚    𝐛  𝐜     i      subscript  ε    i  j  k     subscript  a  j      subscript  ε    k  normal-ℓ  m     subscript  b  normal-ℓ    subscript  c  m               subscript  ε    i  j  k     subscript  ε    k  normal-ℓ  m      subscript  a  j    subscript  b  normal-ℓ    subscript  c  m       \left[\mathbf{a}\times(\mathbf{b}\times\mathbf{c})\right]_{i}=\varepsilon_{ijk%
 }a_{j}(\varepsilon_{k\ell m}b_{\ell}c_{m})=(\varepsilon_{ijk}\varepsilon_{k%
 \ell m})a_{j}b_{\ell}c_{m}     and because cyclic permutations of indices in the ε symbol does not change its value, cyclically permuting indices in ε km to obtain ε mk allows us to use the above δ - ε identity to convert the ε symbols into δ tensors:       [   𝐚  ×   (   𝐛  ×  𝐜   )    ]   i     subscript   delimited-[]    𝐚    𝐛  𝐜     i    \displaystyle\left[\mathbf{a}\times(\mathbf{b}\times\mathbf{c})\right]_{i}     thusly:       𝐚  ×   (   𝐛  ×  𝐜   )    =     (   𝐚  ⋅  𝐜   )   𝐛   -    (   𝐚  ⋅  𝐛   )   𝐜          𝐚    𝐛  𝐜         normal-⋅  𝐚  𝐜   𝐛      normal-⋅  𝐚  𝐛   𝐜      \mathbf{a}\times(\mathbf{b}\times\mathbf{c})=(\mathbf{a}\cdot\mathbf{c})%
 \mathbf{b}-(\mathbf{a}\cdot\mathbf{b})\mathbf{c}     Note this is antisymmetric in b and c , as expected from the left hand side. Similarly, via index notation or even just cyclically relabelling a , b , and c in the previous result and taking the negative:        (   𝐚  ×  𝐛   )   ×  𝐜   =     (   𝐜  ⋅  𝐚   )   𝐛   -    (   𝐜  ⋅  𝐛   )   𝐚            𝐚  𝐛   𝐜        normal-⋅  𝐜  𝐚   𝐛      normal-⋅  𝐜  𝐛   𝐚      (\mathbf{a}\times\mathbf{b})\times\mathbf{c}=(\mathbf{c}\cdot\mathbf{a})%
 \mathbf{b}-(\mathbf{c}\cdot\mathbf{b})\mathbf{a}     and the difference in results show that the cross product is not associative. More complex identities, like quadruple products;        (   𝐚  ×  𝐛   )   ⋅   (   𝐜  ×  𝐝   )    ,    (   𝐚  ×  𝐛   )   ×   (   𝐜  ×  𝐝   )    ,  …      normal-⋅    𝐚  𝐛     𝐜  𝐝        𝐚  𝐛     𝐜  𝐝    normal-…    (\mathbf{a}\times\mathbf{b})\cdot(\mathbf{c}\times\mathbf{d}),\quad(\mathbf{a}%
 \times\mathbf{b})\times(\mathbf{c}\times\mathbf{d}),\ldots     and so on, can be derived in a similar manner.  Transformations of Cartesian tensors (any number of dimensions)  Tensors are defined as quantities which transform in a certain way under linear transformations of coordinates.  Second order  Let a = a i e i and b = b i e i be two vectors, so that they transform according to     a  ¯     normal-¯  a    \overline{a}    j = a i L ij ,     b  ¯     normal-¯  b    \overline{b}    j = b i L ij .  Taking the tensor product gives:       𝐚  ⊗  𝐛   =      a  i    𝐞  i    ⊗   b  j     𝐞  j    =     a  i    b  j    𝐞  i    ⊗   𝐞  j           tensor-product  𝐚  𝐛      tensor-product     subscript  a  i    subscript  𝐞  i     subscript  b  j     subscript  𝐞  j          tensor-product     subscript  a  i    subscript  b  j    subscript  𝐞  i     subscript  𝐞  j       \mathbf{a}\otimes\mathbf{b}=a_{i}\mathbf{e}_{i}\otimes b_{j}\mathbf{e}_{j}=a_{%
 i}b_{j}\mathbf{e}_{i}\otimes\mathbf{e}_{j}     then applying the transformation to the components        a  ¯   p     b  ¯   q   =   a  i    𝖫  i    b  j    p     𝖫  j    =   q     𝖫  i    𝖫  j    p     a  i    q     b  j      fragments   subscript   normal-¯  a   p    subscript   normal-¯  b   q     subscript  a  i    subscript  𝖫  i    subscript   subscript  b  j   p    subscript  𝖫  j    subscript   q    subscript  𝖫  i    subscript   subscript  𝖫  j   p    subscript   subscript  a  i   q    subscript  b  j     \bar{a}_{p}\bar{b}_{q}=a_{i}\mathsf{L}_{i}{}_{p}b_{j}\mathsf{L}_{j}{}_{q}=%
 \mathsf{L}_{i}{}_{p}\mathsf{L}_{j}{}_{q}a_{i}b_{j}     and to the bases         𝐞  ¯   p   ⊗    𝐞  ¯   q    =       (   L   -  1    )    p  i     𝐞  i    ⊗    (   L   -  1    )    q  j       𝐞  ¯   j    =      (   L   -  1    )    p  i      (   L   -  1    )    q  j     𝐞  i    ⊗    𝐞  ¯   j    =     𝖫   i  p     𝖫   j  q     𝐞  i    ⊗    𝐞  ¯   j           tensor-product   subscript   normal-¯  𝐞   p    subscript   normal-¯  𝐞   q       tensor-product     subscript   superscript  normal-L    1      p  i     subscript  𝐞  i     subscript   superscript  normal-L    1      q  j      subscript   normal-¯  𝐞   j          tensor-product     subscript   superscript  normal-L    1      p  i     subscript   superscript  normal-L    1      q  j     subscript  𝐞  i     subscript   normal-¯  𝐞   j          tensor-product     subscript  𝖫    i  p     subscript  𝖫    j  q     subscript  𝐞  i     subscript   normal-¯  𝐞   j       \bar{\mathbf{e}}_{p}\otimes\bar{\mathbf{e}}_{q}=(\boldsymbol{\mathsf{L}}^{-1})%
 _{pi}\mathbf{e}_{i}\otimes(\boldsymbol{\mathsf{L}}^{-1})_{qj}\bar{\mathbf{e}}_%
 {j}=(\boldsymbol{\mathsf{L}}^{-1})_{pi}(\boldsymbol{\mathsf{L}}^{-1})_{qj}%
 \mathbf{e}_{i}\otimes\bar{\mathbf{e}}_{j}=\mathsf{L}_{ip}\mathsf{L}_{jq}%
 \mathbf{e}_{i}\otimes\bar{\mathbf{e}}_{j}     gives the transformation law of an order-2 tensor. The tensor a ⊗ b is invariant under this transformation:            a  ¯   p     b  ¯   q     𝐞  ¯   p    ⊗    𝐞  ¯   q        =     𝖫   k  p     𝖫   ℓ  q     a  k     b  ℓ      (   L   -  1    )    p  i      (   L   -  1    )    q  j     𝐞  i    ⊗   𝐞  j            =     𝖫   k  p      (   L   -  1    )    p  i     𝖫   ℓ  q       (   L   -  1    )    q  j      a  k    b  ℓ    𝐞  i    ⊗   𝐞  j            =     δ  k    δ   ℓ  j     i     a  k    b  ℓ    𝐞  i    ⊗   𝐞  j            =     a  i    b  j    𝐞  i    ⊗   𝐞  j            tensor-product     subscript   normal-¯  a   p    subscript   normal-¯  b   q    subscript   normal-¯  𝐞   p     subscript   normal-¯  𝐞   q      absent   tensor-product     subscript  𝖫    k  p     subscript  𝖫    normal-ℓ  q     subscript  a  k    subscript  b  normal-ℓ    subscript   superscript  normal-L    1      p  i     subscript   superscript  normal-L    1      q  j     subscript  𝐞  i     subscript  𝐞  j        missing-subexpression     absent   tensor-product     subscript  𝖫    k  p     subscript   superscript  normal-L    1      p  i     subscript  𝖫    normal-ℓ  q     subscript   superscript  normal-L    1      q  j     subscript  a  k    subscript  b  normal-ℓ    subscript  𝐞  i     subscript  𝐞  j        missing-subexpression     absent   tensor-product     subscript  δ  k    subscript   subscript  δ    normal-ℓ  j    i    subscript  a  k    subscript  b  normal-ℓ    subscript  𝐞  i     subscript  𝐞  j        missing-subexpression     absent   tensor-product     subscript  a  i    subscript  b  j    subscript  𝐞  i     subscript  𝐞  j        \begin{array}[]{cl}\bar{a}_{p}\bar{b}_{q}\bar{\mathbf{e}}_{p}\otimes\bar{%
 \mathbf{e}}_{q}&=\mathsf{L}_{kp}\mathsf{L}_{\ell q}a_{k}b_{\ell}\,(\boldsymbol%
 {\mathsf{L}}^{-1})_{pi}(\boldsymbol{\mathsf{L}}^{-1})_{qj}\mathbf{e}_{i}%
 \otimes\mathbf{e}_{j}\\
 &=\mathsf{L}_{kp}(\boldsymbol{\mathsf{L}}^{-1})_{pi}\mathsf{L}_{\ell q}(%
 \boldsymbol{\mathsf{L}}^{-1})_{qj}\,a_{k}b_{\ell}\mathbf{e}_{i}\otimes\mathbf{%
 e}_{j}\\
 &=\delta_{k}{}_{i}\delta_{\ell j}\,a_{k}b_{\ell}\mathbf{e}_{i}\otimes\mathbf{e%
 }_{j}\\
 &=a_{i}b_{j}\mathbf{e}_{i}\otimes\mathbf{e}_{j}\end{array}     More generally, for any order-2 tensor       𝐑  =     R   i  j     𝐞  i    ⊗    𝐞  j      ,      𝐑   tensor-product     subscript  R    i  j     subscript  𝐞  i     subscript  𝐞  j      \mathbf{R}=R_{ij}\mathbf{e}_{i}\otimes\mathbf{e}_{j}\,,     the components transform according to;        R  ¯    p  q    =    𝖫  i    𝖫  j    p     R   i  j     q          subscript   normal-¯  R     p  q       subscript  𝖫  i    subscript   subscript  𝖫  j   p    subscript   subscript  R    i  j    q      \bar{R}_{pq}=\mathsf{L}_{i}{}_{p}\mathsf{L}_{j}{}_{q}R_{ij}   ,  and the basis transforms by:         𝐞  ¯   p   ⊗    𝐞  ¯   q    =       (   L   -  1    )    i  p     𝐞  i    ⊗    (   L   -  1    )    j  q      𝐞  j         tensor-product   subscript   normal-¯  𝐞   p    subscript   normal-¯  𝐞   q       tensor-product     subscript   superscript  normal-L    1      i  p     subscript  𝐞  i     subscript   superscript  normal-L    1      j  q      subscript  𝐞  j      \bar{\mathbf{e}}_{p}\otimes\bar{\mathbf{e}}_{q}=(\boldsymbol{\mathsf{L}}^{-1})%
 _{ip}\mathbf{e}_{i}\otimes(\boldsymbol{\mathsf{L}}^{-1})_{jq}\mathbf{e}_{j}     If R does not transform according to this rule - whatever quantity R may be, it's not an order 2 tensor.  Any order  More generally, for any order p tensor      𝐓  =      T    j  1    j  2   ⋯   j  p      𝐞   j  1     ⊗   𝐞   j  2    ⊗  ⋯    𝐞   j  p         𝐓     tensor-product     subscript  T     subscript  j  1    subscript  j  2   normal-⋯   subscript  j  p      subscript  𝐞   subscript  j  1      subscript  𝐞   subscript  j  2    normal-⋯    subscript  𝐞   subscript  j  p       \mathbf{T}=T_{j_{1}j_{2}\cdots j_{p}}\mathbf{e}_{j_{1}}\otimes\mathbf{e}_{j_{2%
 }}\otimes\cdots\mathbf{e}_{j_{p}}     the components transform according to;        T  ¯     j  1    j  2   ⋯   j  p     =    𝖫    i  1    j  1      𝖫    i  2    j  2     ⋯   𝖫    i  p    j  p      T    i  1    i  2   ⋯   i  p           subscript   normal-¯  T      subscript  j  1    subscript  j  2   normal-⋯   subscript  j  p        subscript  𝖫     subscript  i  1    subscript  j  1      subscript  𝖫     subscript  i  2    subscript  j  2     normal-⋯   subscript  𝖫     subscript  i  p    subscript  j  p      subscript  T     subscript  i  1    subscript  i  2   normal-⋯   subscript  i  p        \bar{T}_{j_{1}j_{2}\cdots j_{p}}=\mathsf{L}_{i_{1}j_{1}}\mathsf{L}_{i_{2}j_{2}%
 }\cdots\mathsf{L}_{i_{p}j_{p}}T_{i_{1}i_{2}\cdots i_{p}}     and the basis transforms by:           𝐞  ¯    j  1    ⊗    𝐞  ¯    j  2     ⋯   ⊗    𝐞  ¯    j  p     =         (   L   -  1    )     j  1    i  1      𝐞   i  1     ⊗    (   L   -  1    )     j  2    i  2       𝐞   i  2    ⋯   ⊗    (   L   -  1    )     j  p    i  p       𝐞   i  p          tensor-product     tensor-product   subscript   normal-¯  𝐞    subscript  j  1     subscript   normal-¯  𝐞    subscript  j  2     normal-⋯    subscript   normal-¯  𝐞    subscript  j  p        tensor-product     tensor-product     subscript   superscript  normal-L    1       subscript  j  1    subscript  i  1      subscript  𝐞   subscript  i  1      subscript   superscript  normal-L    1       subscript  j  2    subscript  i  2       subscript  𝐞   subscript  i  2    normal-⋯    subscript   superscript  normal-L    1       subscript  j  p    subscript  i  p       subscript  𝐞   subscript  i  p       \bar{\mathbf{e}}_{j_{1}}\otimes\bar{\mathbf{e}}_{j_{2}}\cdots\otimes\bar{%
 \mathbf{e}}_{j_{p}}=(\boldsymbol{\mathsf{L}}^{-1})_{j_{1}i_{1}}\mathbf{e}_{i_{%
 1}}\otimes(\boldsymbol{\mathsf{L}}^{-1})_{j_{2}i_{2}}\mathbf{e}_{i_{2}}\cdots%
 \otimes(\boldsymbol{\mathsf{L}}^{-1})_{j_{p}i_{p}}\mathbf{e}_{i_{p}}     For a pseudotensor  S of order p , the components transform according to;         S  ¯     j  1    j  2   ⋯   j  p     =   det    (  L  )    𝖫    i  1    j  1      𝖫    i  2    j  2     ⋯   𝖫    i  p    j  p       S    i  1    i  2   ⋯   i  p         .       subscript   normal-¯  S      subscript  j  1    subscript  j  2   normal-⋯   subscript  j  p         normal-L   subscript  𝖫     subscript  i  1    subscript  j  1      subscript  𝖫     subscript  i  2    subscript  j  2     normal-⋯   subscript  𝖫     subscript  i  p    subscript  j  p      subscript  S     subscript  i  1    subscript  i  2   normal-⋯   subscript  i  p         \bar{S}_{j_{1}j_{2}\cdots j_{p}}=\det(\boldsymbol{\mathsf{L}})\mathsf{L}_{i_{1%
 }j_{1}}\mathsf{L}_{i_{2}j_{2}}\cdots\mathsf{L}_{i_{p}j_{p}}S_{i_{1}i_{2}\cdots
 i%
 _{p}}\,.     Pseudovectors as antisymmetric second order tensors  The antisymmetric nature of the cross product can be recast into a tensorial form as follows. 6 Let c be a vector, a be a pseudovector, b be another vector, and T be a second order tensor such that:      𝐜  =   𝐚  ×  𝐛   =   𝐓  ⋅  𝐛         𝐜    𝐚  𝐛         normal-⋅  𝐓  𝐛      \mathbf{c}=\mathbf{a}\times\mathbf{b}=\mathbf{T}\cdot\mathbf{b}     As the cross product is linear in a and b , the components of T can be found by inspection, and they are:      𝐓  =   (     0     -   a  z       a  y        a  z     0     -   a  x         -   a  y       a  x     0     )       𝐓    0     subscript  a  z     subscript  a  y      subscript  a  z   0     subscript  a  x         subscript  a  y     subscript  a  x   0      \mathbf{T}=\begin{pmatrix}0&-a_{\text{z}}&a_{\text{y}}\\
 a_{\text{z}}&0&-a_{\text{x}}\\
 -a_{\text{y}}&a_{\text{x}}&0\\
 \end{pmatrix}     so the pseudovector a can be written as an antisymmetric tensor. This transforms as a tensor, not a pseudotensor. For the mechanical example above for the tangential velocity of a rigid body, given by , this can be rewritten as  where Ω is the tensor corresponding to the pseudovector ω :      𝛀  =   (     0     -   ω  z       ω  y        ω  z     0     -   ω  x         -   ω  y       ω  x     0     )       𝛀    0     subscript  ω  z     subscript  ω  y      subscript  ω  z   0     subscript  ω  x         subscript  ω  y     subscript  ω  x   0      \boldsymbol{\Omega}=\begin{pmatrix}0&-\omega_{\text{z}}&\omega_{\text{y}}\\
 \omega_{\text{z}}&0&-\omega_{\text{x}}\\
 -\omega_{\text{y}}&\omega_{\text{x}}&0\\
 \end{pmatrix}     For an example in electromagnetism , while the electric field  E is a vector field , the magnetic field  B is a pseudovector field. These fields are defined from the Lorentz force for a particle of electric charge  q traveling at velocity v :      𝐅  =   q   (   𝐄  +   𝐯  ×  𝐁    )    =   q   (   𝐄  -   𝐁  ×  𝐯    )          𝐅    q    𝐄    𝐯  𝐁            q    𝐄    𝐁  𝐯        \mathbf{F}=q(\mathbf{E}+\mathbf{v}\times\mathbf{B})=q(\mathbf{E}-\mathbf{B}%
 \times\mathbf{v})     and considering the second term containing the cross product of a pseudovector B and velocity vector v , it can be written in matrix form, with F , E , and v as column vectors and B as an antisymmetric matrix:       (      F  x        F  y        F  z      )   =    q   (      E  x        E  y        E  z      )    -   q   (     0     -   B  z       B  y        B  z     0     -   B  x         -   B  y       B  x     0     )    (      v  x        v  y        v  z      )            subscript  F  x      subscript  F  y      subscript  F  z         q     subscript  E  x      subscript  E  y      subscript  E  z        q    0     subscript  B  z     subscript  B  y      subscript  B  z   0     subscript  B  x         subscript  B  y     subscript  B  x   0       subscript  v  x      subscript  v  y      subscript  v  z         \begin{pmatrix}F_{\text{x}}\\
 F_{\text{y}}\\
 F_{\text{z}}\\
 \end{pmatrix}=q\begin{pmatrix}E_{\text{x}}\\
 E_{\text{y}}\\
 E_{\text{z}}\\
 \end{pmatrix}-q\begin{pmatrix}0&-B_{\text{z}}&B_{\text{y}}\\
 B_{\text{z}}&0&-B_{\text{x}}\\
 -B_{\text{y}}&B_{\text{x}}&0\\
 \end{pmatrix}\begin{pmatrix}v_{\text{x}}\\
 v_{\text{y}}\\
 v_{\text{z}}\\
 \end{pmatrix}     If a pseudovector is explicitly given by a cross product of two vectors (as opposed to entering the cross product with another vector), then such pseudovectors can also be written as antisymmetric tensors of second order, with each entry a component of the cross product. The angular momentum of a classical pointlike particle orbiting about an axis, defined by , is another example of a pesudovector, with corresponding antisymmetric tensor:      𝐉  =   (     0     -   J  z       J  y        J  z     0     -   J  x         -   J  y       J  x     0     )   =   (     0     -   (    x   p  y    -   y   p  x     )       (    z   p  x    -   x   p  z     )        (    x   p  y    -   y   p  x     )     0     -   (    y   p  z    -   z   p  y     )         -   (    z   p  x    -   x   p  z     )       (    y   p  z    -   z   p  y     )     0     )         𝐉    0     subscript  J  z     subscript  J  y      subscript  J  z   0     subscript  J  x         subscript  J  y     subscript  J  x   0           0        x   subscript  p  y      y   subscript  p  x          z   subscript  p  x      x   subscript  p  z           x   subscript  p  y      y   subscript  p  x     0        y   subscript  p  z      z   subscript  p  y              z   subscript  p  x      x   subscript  p  z          y   subscript  p  z      z   subscript  p  y     0       \mathbf{J}=\begin{pmatrix}0&-J_{\text{z}}&J_{\text{y}}\\
 J_{\text{z}}&0&-J_{\text{x}}\\
 -J_{\text{y}}&J_{\text{x}}&0\\
 \end{pmatrix}=\begin{pmatrix}0&-(xp_{\text{y}}-yp_{\text{x}})&(zp_{\text{x}}-%
 xp_{\text{z}})\\
 (xp_{\text{y}}-yp_{\text{x}})&0&-(yp_{\text{z}}-zp_{\text{y}})\\
 -(zp_{\text{x}}-xp_{\text{z}})&(yp_{\text{z}}-zp_{\text{y}})&0\\
 \end{pmatrix}     Although Cartesian tensors do not occur in the theory of relativity; the tensor form of orbital angular momentum J enters the spacelike part of the relativistic angular momentum tensor, and the above tensor form of the magnetic field B enters the spacelike part of the electromagnetic tensor .  Vector and tensor calculus  It should be emphasized the following formulae are only so simple in Cartesian coordinates - in general curvilinear coordinates there are factors of the metric and its determinant - see tensors in curvilinear coordinates for more general analysis.  Vector calculus  Following are the differential operators of vector calculus . Throughout, left Φ( r , t ) be a scalar field , and       𝐀   (  𝐫  ,  t  )    =     A  x    (  𝐫  ,  t  )    𝐞  x    +    A  y    (  𝐫  ,  t  )    𝐞  y    +    A  z    (  𝐫  ,  t  )    𝐞  z           𝐀   𝐫  t         subscript  A  x    𝐫  t    subscript  𝐞  x       subscript  A  y    𝐫  t    subscript  𝐞  y       subscript  A  z    𝐫  t    subscript  𝐞  z       \mathbf{A}(\mathbf{r},t)=A_{\text{x}}(\mathbf{r},t)\mathbf{e}_{\text{x}}+A_{%
 \text{y}}(\mathbf{r},t)\mathbf{e}_{\text{y}}+A_{\text{z}}(\mathbf{r},t)\mathbf%
 {e}_{\text{z}}          𝐁   (  𝐫  ,  t  )    =     B  x    (  𝐫  ,  t  )    𝐞  x    +    B  y    (  𝐫  ,  t  )    𝐞  y    +    B  z    (  𝐫  ,  t  )    𝐞  z           𝐁   𝐫  t         subscript  B  x    𝐫  t    subscript  𝐞  x       subscript  B  y    𝐫  t    subscript  𝐞  y       subscript  B  z    𝐫  t    subscript  𝐞  z       \mathbf{B}(\mathbf{r},t)=B_{\text{x}}(\mathbf{r},t)\mathbf{e}_{\text{x}}+B_{%
 \text{y}}(\mathbf{r},t)\mathbf{e}_{\text{y}}+B_{\text{z}}(\mathbf{r},t)\mathbf%
 {e}_{\text{z}}     be vector fields , in which all scalar and vector fields are functions of the position vector  r and time t .  The gradient operator in Cartesian coordinates is given by:      ∇  =     𝐞  x    ∂   ∂  x     +    𝐞  y    ∂   ∂  y     +    𝐞  z    ∂   ∂  z          normal-∇       subscript  𝐞  x        x        subscript  𝐞  y        y        subscript  𝐞  z        z        \nabla=\mathbf{e}_{\text{x}}\frac{\partial}{\partial x}+\mathbf{e}_{\text{y}}%
 \frac{\partial}{\partial y}+\mathbf{e}_{\text{z}}\frac{\partial}{\partial z}     and in index notation, this is usually abbreviated in various ways:       ∇  i   ≡   ∂  i   ≡   ∂   ∂   x  i            subscript  normal-∇  i    subscript   i              subscript  x  i        \nabla_{i}\equiv\partial_{i}\equiv\frac{\partial}{\partial x_{i}}     This operator acts on a scalar field Φ to obtain the vector field directed in the maximum rate of increase of Φ:        (   ∇  Φ   )   i   =    ∇  i   Φ        subscript   normal-∇  normal-Φ   i     subscript  normal-∇  i   normal-Φ     \left(\nabla\Phi\right)_{i}=\nabla_{i}\Phi     The index notation for the dot and cross products carries over to the differential operators of vector calculus. 7  The directional derivative of a scalar field Φ is the rate of change of Φ along some direction vector a (not necessarily a unit vector ), formed out of the components of a and the gradient:       𝐚  ⋅   (   ∇  Φ   )    =    a  j     (   ∇  Φ   )   j         normal-⋅  𝐚   normal-∇  normal-Φ       subscript  a  j    subscript   normal-∇  normal-Φ   j      \mathbf{a}\cdot(\nabla\Phi)=a_{j}(\nabla\Phi)_{j}     The divergence of a vector field A is:       ∇  ⋅  𝐀   =    ∇  i    A  i         normal-⋅  normal-∇  𝐀     subscript  normal-∇  i    subscript  A  i      \nabla\cdot\mathbf{A}=\nabla_{i}A_{i}     Note the interchange of the components of the gradient and vector field yields a different differential operator       𝐀  ⋅  ∇   =    A  i    ∇  i         normal-⋅  𝐀  normal-∇      subscript  A  i    subscript  normal-∇  i      \mathbf{A}\cdot\nabla=A_{i}\nabla_{i}     which could act on scalar or vector fields. In fact, if A is replaced by the velocity field  u ( r , t ) of a fluid, this is a term in the material derivative (with many other names) of continuum mechanics , with another term being the partial time derivative:       D   D  t    =    ∂   ∂  t    +   𝐮  ⋅  ∇          D    D  t           t     normal-⋅  𝐮  normal-∇      \frac{D}{Dt}=\frac{\partial}{\partial t}+\mathbf{u}\cdot\nabla     which usually acts on the velocity field leading to the non-linearity in the Navier-Stokes equations .  As for the curl of a vector field A , this can be defined as a pseudovector field by means of the ε symbol:        (   ∇  ×  𝐀   )   i   =    ε   i  j  k      ∇  j    A  k          subscript    normal-∇  𝐀   i      subscript  ε    i  j  k      subscript  normal-∇  j    subscript  A  k       \left(\nabla\times\mathbf{A}\right)_{i}=\varepsilon_{ijk}\nabla_{j}A_{k}     which is only valid in three dimensions, or an antisymmetric tensor field of second order via antisymmetrization of indices, indicated by delimiting the antisymmetrized indices by square brackets (see Ricci calculus ):        (   ∇  ×  𝐀   )    i  j    =     ∇  i    A  j    -    ∇  j    A  i     =   2    ∇   [  i     A   j  ]             subscript    normal-∇  𝐀     i  j        subscript  normal-∇  i    subscript  A  j      subscript  normal-∇  j    subscript  A  i            2    subscript  normal-∇   fragments  normal-[  i     subscript  A   fragments  j  normal-]         \left(\nabla\times\mathbf{A}\right)_{ij}=\nabla_{i}A_{j}-\nabla_{j}A_{i}=2%
 \nabla_{[i}A_{j]}     which is valid in any number of dimensions. In each case, the order of the gradient and vector field components should not be interchanged as this would result in a different differential operator:       ε   i  j  k     A  j    ∇  k        subscript  ε    i  j  k     subscript  A  j    subscript  normal-∇  k     \varepsilon_{ijk}A_{j}\nabla_{k}            A  i    ∇  j    -    A  j    ∇  i     =   2   A   [  i     ∇   j  ]              subscript  A  i    subscript  normal-∇  j       subscript  A  j    subscript  normal-∇  i       2   subscript  A   fragments  normal-[  i     subscript  normal-∇   fragments  j  normal-]       A_{i}\nabla_{j}-A_{j}\nabla_{i}=2A_{[i}\nabla_{j]}     which could act on scalar or vector fields.  Finally, the Laplacian operator is defined in two ways, the divergence of the gradient of a scalar field Φ:       ∇  ⋅   (   ∇  Φ   )    =    ∇  i    (    ∇  i   Φ   )         normal-⋅  normal-∇   normal-∇  normal-Φ      subscript  normal-∇  i     subscript  normal-∇  i   normal-Φ      \nabla\cdot(\nabla\Phi)=\nabla_{i}(\nabla_{i}\Phi)     or the square of the gradient operator, which acts on a scalar field Φ or a vector field A :        (   ∇  ⋅  ∇   )   Φ   =    (    ∇  i    ∇  i    )   Φ          normal-⋅  normal-∇  normal-∇   normal-Φ       subscript  normal-∇  i    subscript  normal-∇  i    normal-Φ     (\nabla\cdot\nabla)\Phi=(\nabla_{i}\nabla_{i})\Phi           (   ∇  ⋅  ∇   )   𝐀   =    (    ∇  i    ∇  i    )   𝐀          normal-⋅  normal-∇  normal-∇   𝐀       subscript  normal-∇  i    subscript  normal-∇  i    𝐀     (\nabla\cdot\nabla)\mathbf{A}=(\nabla_{i}\nabla_{i})\mathbf{A}     In physics and engineering, the gradient, divergence, curl, and Laplacian operator arise inevitably in fluid mechanics , Newtonian gravitation , electromagnetism , heat conduction , and even quantum mechanics .  Vector calculus identities can be derived in a similar way to those of vector dot and cross products and combinations. For example, in three dimensions, the curl of a cross product of two vector fields A and B :       [   ∇  ×   (   𝐀  ×  𝐁   )    ]   i     subscript   delimited-[]    normal-∇    𝐀  𝐁     i    \displaystyle\left[\nabla\times(\mathbf{A}\times\mathbf{B})\right]_{i}     where the product rule was used, and throughout the differential operator was not interchanged with A or B . Thus:       ∇  ×   (   𝐀  ×  𝐁   )    =      (   𝐁  ⋅  ∇   )   𝐀   +   𝐀   (   ∇  ⋅  𝐁   )     -   𝐁   (   ∇  ⋅  𝐀   )    -    (   𝐀  ⋅  ∇   )   𝐁          normal-∇    𝐀  𝐁           normal-⋅  𝐁  normal-∇   𝐀     𝐀   normal-⋅  normal-∇  𝐁       𝐁   normal-⋅  normal-∇  𝐀       normal-⋅  𝐀  normal-∇   𝐁      \nabla\times(\mathbf{A}\times\mathbf{B})=(\mathbf{B}\cdot\nabla)\mathbf{A}+%
 \mathbf{A}(\nabla\cdot\mathbf{B})-\mathbf{B}(\nabla\cdot\mathbf{A})-(\mathbf{A%
 }\cdot\nabla)\mathbf{B}     Tensor calculus  One can continue the operations on tensors of higher order. Let T = T ( r , t ) denote a second order tensor field, again dependent on the position vector r and time t .  For instance, the gradient of a vector field in two equivalent notations ("dyadic" and "tensor", respectively) is:        (   ∇  𝐀   )    i  j    ≡    (   ∇  ⊗  𝐀   )    i  j    =    ∇  i    A  j           subscript   normal-∇  𝐀     i  j     subscript   tensor-product  normal-∇  𝐀     i  j           subscript  normal-∇  i    subscript  A  j       (\nabla\mathbf{A})_{ij}\equiv(\nabla\otimes\mathbf{A})_{ij}=\nabla_{i}A_{j}     which is a tensor field of second order.  The divergence of a tensor is:        (   ∇  ⋅  𝐓   )   j   =    ∇  i    T   i  j          subscript   normal-⋅  normal-∇  𝐓   j     subscript  normal-∇  i    subscript  T    i  j       (\nabla\cdot\mathbf{T})_{j}=\nabla_{i}T_{ij}     which is a vector field. This arises in continuum mechanics in Cauchy's laws of motion - the divergence of the Cauchy stress tensor σ is a vector field, related to body forces acting on the fluid.  Difference from the standard tensor calculus  Cartesian tensors are as in tensor algebra , but Euclidean structure of and restriction of the basis brings some simplifications compared to the general theory.  The general tensor algebra consists of general mixed tensors of type ( p , q ):      𝐓  =    T    j  1    j  2   ⋯   j  q      i  1    i  2   ⋯   i  p      𝐞    i  1    i  2   ⋯   i  p      j  1    j  2   ⋯   j  q          𝐓     superscript   subscript  T     subscript  j  1    subscript  j  2   normal-⋯   subscript  j  q        subscript  i  1    subscript  i  2   normal-⋯   subscript  i  p      superscript   subscript  𝐞     subscript  i  1    subscript  i  2   normal-⋯   subscript  i  p        subscript  j  1    subscript  j  2   normal-⋯   subscript  j  q        \mathbf{T}=T_{j_{1}j_{2}\cdots j_{q}}^{i_{1}i_{2}\cdots i_{p}}\mathbf{e}_{i_{1%
 }i_{2}\cdots i_{p}}^{j_{1}j_{2}\cdots j_{q}}     with basis elements:       𝐞    i  1    i  2   ⋯   i  p      j  1    j  2   ⋯   j  q     =       𝐞   i  1    ⊗   𝐞   i  2    ⊗  ⋯    𝐞   i  p     ⊗   𝐞   j  1    ⊗   𝐞   j  2    ⊗  ⋯    𝐞   j  q          superscript   subscript  𝐞     subscript  i  1    subscript  i  2   normal-⋯   subscript  i  p        subscript  j  1    subscript  j  2   normal-⋯   subscript  j  q        tensor-product     tensor-product   subscript  𝐞   subscript  i  1     subscript  𝐞   subscript  i  2    normal-⋯    subscript  𝐞   subscript  i  p      superscript  𝐞   subscript  j  1     superscript  𝐞   subscript  j  2    normal-⋯    superscript  𝐞   subscript  j  q       \mathbf{e}_{i_{1}i_{2}\cdots i_{p}}^{j_{1}j_{2}\cdots j_{q}}=\mathbf{e}_{i_{1}%
 }\otimes\mathbf{e}_{i_{2}}\otimes\cdots\mathbf{e}_{i_{p}}\otimes\mathbf{e}^{j_%
 {1}}\otimes\mathbf{e}^{j_{2}}\otimes\cdots\mathbf{e}^{j_{q}}     the components transform according to:        T  ¯     ℓ  1    ℓ  2   ⋯   ℓ  q      k  1    k  2   ⋯   k  p     =   𝖫   i  1     𝖫   i  2       k  1     ⋯     k  2     𝖫   i  p      (   L   -  1    )    ℓ  1       k  p      (   L   -  1    )    ℓ  2       j  1     ⋯     j  2      (   L   -  1    )    ℓ  q     T    j  1    j  2   ⋯   j  q      i  1    i  2   ⋯   i  p       j  q       fragments   superscript   subscript   normal-¯  T      subscript  normal-ℓ  1    subscript  normal-ℓ  2   normal-⋯   subscript  normal-ℓ  q        subscript  k  1    subscript  k  2   normal-⋯   subscript  k  p       subscript  𝖫   subscript  i  1     superscript   subscript  𝖫   subscript  i  2     subscript  k  1     superscript  normal-⋯   subscript  k  2     subscript  𝖫   subscript  i  p     superscript   subscript   fragments  normal-(   superscript  normal-L    1    normal-)    subscript  normal-ℓ  1     subscript  k  p     superscript   subscript   fragments  normal-(   superscript  normal-L    1    normal-)    subscript  normal-ℓ  2     subscript  j  1     superscript  normal-⋯   subscript  j  2     subscript   fragments  normal-(   superscript  normal-L    1    normal-)    subscript  normal-ℓ  q     superscript   superscript   subscript  T     subscript  j  1    subscript  j  2   normal-⋯   subscript  j  q        subscript  i  1    subscript  i  2   normal-⋯   subscript  i  p      subscript  j  q      \bar{T}_{\ell_{1}\ell_{2}\cdots\ell_{q}}^{k_{1}k_{2}\cdots k_{p}}=\mathsf{L}_{%
 i_{1}}{}^{k_{1}}\mathsf{L}_{i_{2}}{}^{k_{2}}\cdots\mathsf{L}_{i_{p}}{}^{k_{p}}%
 (\boldsymbol{\mathsf{L}}^{-1})_{\ell_{1}}{}^{j_{1}}(\boldsymbol{\mathsf{L}}^{-%
 1})_{\ell_{2}}{}^{j_{2}}\cdots(\boldsymbol{\mathsf{L}}^{-1})_{\ell_{q}}{}^{j_{%
 q}}T_{j_{1}j_{2}\cdots j_{q}}^{i_{1}i_{2}\cdots i_{p}}     as for the bases:        𝐞  ¯     k  1    k  2   ⋯   k  p      ℓ  1    ℓ  2   ⋯   ℓ  q     =    (   L   -  1    )    k  1      (   L   -  1    )    k  2       i  1     ⋯     i  2      (   L   -  1    )    k  p     𝖫   j  1       i  p     𝖫   j  2       ℓ  1     ⋯     ℓ  2     𝖫   j  q     𝐞    i  1    i  2   ⋯   i  p      j  1    j  2   ⋯   j  q       ℓ  q       fragments   superscript   subscript   normal-¯  𝐞      subscript  k  1    subscript  k  2   normal-⋯   subscript  k  p        subscript  normal-ℓ  1    subscript  normal-ℓ  2   normal-⋯   subscript  normal-ℓ  q       subscript   fragments  normal-(   superscript  normal-L    1    normal-)    subscript  k  1     superscript   subscript   fragments  normal-(   superscript  normal-L    1    normal-)    subscript  k  2     subscript  i  1     superscript  normal-⋯   subscript  i  2     subscript   fragments  normal-(   superscript  normal-L    1    normal-)    subscript  k  p     superscript   subscript  𝖫   subscript  j  1     subscript  i  p     superscript   subscript  𝖫   subscript  j  2     subscript  normal-ℓ  1     superscript  normal-⋯   subscript  normal-ℓ  2     subscript  𝖫   subscript  j  q     superscript   superscript   subscript  𝐞     subscript  i  1    subscript  i  2   normal-⋯   subscript  i  p        subscript  j  1    subscript  j  2   normal-⋯   subscript  j  q      subscript  normal-ℓ  q      \bar{\mathbf{e}}_{k_{1}k_{2}\cdots k_{p}}^{\ell_{1}\ell_{2}\cdots\ell_{q}}=(%
 \boldsymbol{\mathsf{L}}^{-1})_{k_{1}}{}^{i_{1}}(\boldsymbol{\mathsf{L}}^{-1})_%
 {k_{2}}{}^{i_{2}}\cdots(\boldsymbol{\mathsf{L}}^{-1})_{k_{p}}{}^{i_{p}}\mathsf%
 {L}_{j_{1}}{}^{\ell_{1}}\mathsf{L}_{j_{2}}{}^{\ell_{2}}\cdots\mathsf{L}_{j_{q}%
 }{}^{\ell_{q}}\mathbf{e}_{i_{1}i_{2}\cdots i_{p}}^{j_{1}j_{2}\cdots j_{q}}     For Cartesian tensors, only the order  of the tensor matters in a Euclidean space with an orthonormal basis, and all  indices can be lowered. A Cartesian basis does not exist unless the vector space has a positive-definite metric, and thus cannot be used in relativistic contexts.  History  Dyadic tensors were historically the first approach to formulating second-order tensors, similarly triadic tensors for third-order tensors, and so on. Cartesian tensors use tensor index notation , in which the variance may be glossed over and is often ignored, since the components remain unchanged by raising and lowering indices .  See also   Tensor algebra  Tensor calculus  Tensors in curvilinear coordinates  Rotation group   References  Notes           Further reading and applications                                External links   Cartesian Tensors  V. N. Kaliakin, Brief Review of Tensors , University of Delaware  R. E. Hunt, Cartesian Tensors , University of Cambridge   "  Category:Linear algebra  Category:Tensors  Category:Applied mathematics     , used throughout ↩  , see Appendix C. ↩    ↩  , see Appendix C. ↩  ↩     