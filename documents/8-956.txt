


K-medoids




K-medoids

The -medoids algorithm is a clustering algorithm related to the -means algorithm and the medoidshift algorithm. Both the -means and -medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the -means algorithm, -medoids chooses datapoints as centers (medoids or exemplars) and works with an arbitrary matrix of distances between datapoints instead of 
 
 
 
 . This method was proposed in 19871 for the work with 
 
 
 
  norm and other distances.
-medoid is a classical partitioning technique of clustering that clusters the data set of  objects into  clusters known a priori. A useful tool for determining  is the silhouette.
It is more robust to noise and outliers as compared to -means because it minimizes a sum of pairwise dissimilarities instead of a sum of squared Euclidean distances.
A medoid can be defined as the object of a cluster whose average dissimilarity to all the objects in the cluster is minimal. i.e. it is a most centrally located point in the cluster.
Algorithms
The most common realisation of -medoid clustering is the Partitioning Around Medoids (PAM) algorithm and is as follows:2

Initialize: randomly select (without replacement) 
 
 
 
  of the 
 
 
 
  data points as the medoids
Associate each data point to the closest medoid. ("closest" here is defined using any valid distance metric, most commonly Euclidean distance, Manhattan distance or Minkowski distance)
While the cost of the configuration decreases:
 
For each medoid 
 
 
 
 , for each non-medoid data point 
 
 
 
 :
 
Swap 
 
 
 
  and 
 
 
 
 , recompute the cost
If the total cost of the configuration increased in the previous step, undo the swap



Other algorithms than PAM have been suggested in the literature, including the following Voronoi iteration method:

Select initial medoids
Iterate while the cost (sum of distances of points to their medoid) decreases:
 
In each cluster, make the point that minimizes the sum of distances within the cluster the medoid
Reassign each point to the cluster defined by the closest medoid determined in the previous step.


Demonstration of PAM
Cluster the following data set of ten objects into two clusters i.e.  2}}.
Consider a data set of ten objects as follows:
(Figure)
350 px|Figure 1.1 – distribution of the data





X1

2

6



X2

3

4



X3

3

8



X4

4

7



X5

6

2



X6

6

4



X7

7

3



X8

7

4



X9

8

5



X10

7

6



{{-}}
Step 1
(Figure)
350 px|Figure 1.2 – clusters after step 1

Initialize  centers.
Let us assume  and  are selected as medoids, so the centers are  (3,4)}} and  (7,4)}}
Calculate distances to each center so as to associate each data object to its nearest medoid. Cost is calculated using Manhattan distance (Minkowski distance metric with  1}}). Costs to the nearest medoid are shown bold in the table.




Cost (distance) to 









1



3



4



5



6



7



9



10







Cost (distance) to 









1



3



4



5



6



7



9



10



Then the clusters become:
Since the points 
 
 

 
  and 
 
 
 
  are closer to  hence they form one cluster whilst remaining points form another cluster.
So the total cost involved is 
 
 
 
 .
Where cost between any two points is found using formula



where  is any data object,  is the medoid, and  is the dimension of the object which in this case is 
 
 
 
 .
Total cost is the summation of the cost of data object from its medoid in its cluster so here:
{{-}} 
 
 

Step 2
 Select one of the nonmedoids 
 
 

Let us assume 
 
 
 
 , i.e. .
So now the medoids are  and 
 
 

If 
 
 
 
  and 
 
 
 
  are new medoids, calculate the total cost involved
By using the formula in the step 1










Data objects ()

Cost (distance)



1

3

4

2



3

3

4

3



4

3

4

4



5

3

4

6



6

3

4

6



8

3

4

7



9

3

4

8



10

3

4

7















Data objects ()

Cost (distance)



1

7

3

2



3

7

3

3



4

7

3

4



5

7

3

6



6

7

3

6



8

7

3

7



9

7

3

8



10

7

3

7



{{-}} 



So cost of swapping medoid from  to 
 
 
 
  is


 
 === program:
So moving to 
 
 
 
  would be a bad idea, so the previous choice was good. So we try other nonmedoids and found that our first choice was the best. So the configuration does not change and algorithm terminates here (i.e. there is no change in the medoids).
It may happen some data points may shift from one cluster to another cluster depending upon their closeness to medoid.
In some standard situations, k-medoids demonstrate better performance than k-means. An example is presented in Fig. 2. The most time-consuming part of the k-medoids algorithm is the calculation of the distances between objects. If a quadratic preprocessing and storage is applicable, the distances matrix can be precomputed to achieve consequent speed-up. See for example,3 where the authors also introduce a heuristic to choose the initial  medoids.
Software

ELKI includes several k-means variants, including an EM-based k-medoids and the original PAM algorithm.
Java-ML. Includes a k-medoid implementation that is incorrect in the same way as the RapidMiner version.
Julia contains a k-medoid implementation in the JuliaStats clustering package.4
R includes variants of k-means in the "flexclust" package and PAM is implemented in the "cluster" package.
RapidMiner has an operator named KMedoids, but it does not implement the KMedoids algorithm correctly. Instead, it is a k-means variant, that substitutes the mean with the closest data point (which is not the medoid).
MATLAB implements PAM, CLARA, and two other algorithms to solve the k-medoid clustering problem.

External links
E.M. Mirkes, K-means and K-medoids (Applet), University of Leicester, 2011.
References
"
Category:Statistical algorithms Category:Data clustering algorithms



Kaufman, L. and Rousseeuw, P.J. (1987), Clustering by means of Medoids, in Statistical Data Analysis Based on the 
 
 
 
 –Norm and Related Methods, edited by Y. Dodge, North-Holland, 405–416.↩
↩
H.S. Park , C.H. Jun, A simple and fast algorithm for K-medoids clustering, Expert Systems with Applications, 36, (2) (2009), 3336–3341.↩
Clustering.jl↩




