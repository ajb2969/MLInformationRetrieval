   Tobit model      Tobit model   The Tobit model is a statistical model proposed by James Tobin (1958) 1 to describe the relationship between a non-negative dependent variable    y  i     subscript  y  i    y_{i}   and an independent variable (or vector )    x  i     subscript  x  i    x_{i}   . The term Tobit was derived from Tobin's name by truncating and adding -it by analogy with the probit model . 2  The model supposes that there is a latent (i.e. unobservable) variable     y  i  *     superscript   subscript  y  i      y_{i}^{*}   . This variable linearly depends on    x  i     subscript  x  i    x_{i}   via a parameter (vector)   β   β   \beta   which determines the relationship between the independent variable (or vector)    x  i     subscript  x  i    x_{i}   and the latent variable     y  i  *     superscript   subscript  y  i      y_{i}^{*}   (just as in a linear model ). In addition, there is a normally distributed  error term     u  i     subscript  u  i    u_{i}   to capture random influences on this relationship. The observable variable    y  i     subscript  y  i    y_{i}   is defined to be equal to the latent variable whenever the latent variable is above zero and zero otherwise.       y  i   =   {      y  i  *        if    y  i  *    >  0       0       if    y  i  *    ≤  0            subscript  y  i    cases   superscript   subscript  y  i         if   superscript   subscript  y  i      0   0      if   superscript   subscript  y  i      0      y_{i}=\begin{cases}y_{i}^{*}&\textrm{if}\;y_{i}^{*}>0\\
 0&\textrm{if}\;y_{i}^{*}\leq 0\end{cases}     where    y  i  *     superscript   subscript  y  i      y_{i}^{*}   is a latent variable:        y  i  *   =    β   x  i    +   u  i     ,    u  i   ∼   N   (  0  ,   σ  2   )        formulae-sequence     superscript   subscript  y  i         β   subscript  x  i     subscript  u  i      similar-to   subscript  u  i     N   0   superscript  σ  2        y_{i}^{*}=\beta x_{i}+u_{i},u_{i}\sim N(0,\sigma^{2})\,     Consistency  If the relationship parameter   β   β   \beta   is estimated by regressing the observed    y  i     subscript  y  i    y_{i}   on    x  i     subscript  x  i    x_{i}   , the resulting ordinary least squares regression estimator is inconsistent . It will yield a downwards-biased estimate of the slope coefficient and an upward-biased estimate of the intercept. Takeshi Amemiya (1973) has proven that the maximum likelihood estimator suggested by Tobin for this model is consistent.  Interpretation  The   β   β   \beta   coefficient should not be interpreted as the effect of    x  i     subscript  x  i    x_{i}   on    y  i     subscript  y  i    y_{i}   , as one would with a linear regression model ; this is a common error. Instead, it should be interpreted as the combination of (1) the change in    y  i     subscript  y  i    y_{i}   of those above the limit, weighted by the probability of being above the limit; and (2) the change in the probability of being above the limit, weighted by the expected value of    y  i     subscript  y  i    y_{i}   if above. 3  Variations of the Tobit model  Variations of the Tobit model can be produced by changing where and when censoring occurs.  classifies these variations into five categories (Tobit type I - Tobit type V), where Tobit type I stands for the first model described above. Schnedler (2005) provides a general formula to obtain consistent likelihood estimators for these and other variations of the Tobit model.  Type I  The Tobit model is a special case of a censored regression model , because the latent variable    y  i  *     superscript   subscript  y  i      y_{i}^{*}   cannot always be observed while the independent variable    x  i     subscript  x  i    x_{i}   is observable. A common variation of the Tobit model is censoring at a value    y  L     subscript  y  L    y_{L}   different from zero:       y  i   =   {      y  i  *        if    y  i  *    >   y  L         y  L         if    y  i  *    ≤   y  L    .            subscript  y  i    cases   superscript   subscript  y  i         if   superscript   subscript  y  i       subscript  y  L     subscript  y  L       if   superscript   subscript  y  i       subscript  y  L       y_{i}=\begin{cases}y_{i}^{*}&\textrm{if}\;y_{i}^{*}>y_{L}\\
 y_{L}&\textrm{if}\;y_{i}^{*}\leq y_{L}.\end{cases}     Another example is censoring of values above    y  U     subscript  y  U    y_{U}   .       y   1  i    =   {      y   1  i   *        if    y   1  i   *    >  0       0       if    y   1  i   *    ≤  0.            subscript  y    1  i     cases   superscript   subscript  y    1  i          if   superscript   subscript  y    1  i       0   0      if   superscript   subscript  y    1  i       0.      y_{1i}=\begin{cases}y_{1i}^{*}&\textrm{if}\;y_{1i}^{*}>0\\
 0&\textrm{if}\;y_{1i}^{*}\leq 0.\end{cases}   is censored from above and below at the same time.       y   2  i    =   {      y   2  i   *        if    y   1  i   *    >  0       0       if    y   1  i   *    ≤  0.            subscript  y    2  i     cases   superscript   subscript  y    2  i          if   superscript   subscript  y    1  i       0   0      if   superscript   subscript  y    1  i       0.      y_{2i}=\begin{cases}y_{2i}^{*}&\textrm{if}\;y_{1i}^{*}>0\\
 0&\textrm{if}\;y_{1i}^{*}\leq 0.\end{cases}     Heckman (1987) falls into the Type II Tobit. In Type I Tobit, the latent variable absorb both the process of participation and 'outcome' of interest. Type II Tobit allows the process of participation/selection and the process of 'outcome' to be independent, conditional on x.  Type III  Type III introduces a second observed dependent variable.       y   1  i    =   {      y   1  i   *        if    y   1  i   *    >  0       0       if    y   1  i   *    ≤  0.            subscript  y    1  i     cases   superscript   subscript  y    1  i          if   superscript   subscript  y    1  i       0   0      if   superscript   subscript  y    1  i       0.      y_{1i}=\begin{cases}y_{1i}^{*}&\textrm{if}\;y_{1i}^{*}>0\\
 0&\textrm{if}\;y_{1i}^{*}\leq 0.\end{cases}          y   2  i    =   {      y   2  i   *        if    y   1  i   *    >  0       0       if    y   1  i   *    ≤  0.            subscript  y    2  i     cases   superscript   subscript  y    2  i          if   superscript   subscript  y    1  i       0   0      if   superscript   subscript  y    1  i       0.      y_{2i}=\begin{cases}y_{2i}^{*}&\textrm{if}\;y_{1i}^{*}>0\\
 0&\textrm{if}\;y_{1i}^{*}\leq 0.\end{cases}   The Heckman model falls into this type.  Type IV  Type IV introduces a third observed dependent variable and a third latent variable.       y   3  i    =   {      y   3  i   *        if    y   1  i   *    >  0       0       if    y   1  i   *    ≤  0.            subscript  y    3  i     cases   superscript   subscript  y    3  i          if   superscript   subscript  y    1  i       0   0      if   superscript   subscript  y    1  i       0.      y_{3i}=\begin{cases}y_{3i}^{*}&\textrm{if}\;y_{1i}^{*}>0\\
 0&\textrm{if}\;y_{1i}^{*}\leq 0.\end{cases}         y   1  i   *     superscript   subscript  y    1  i       y_{1i}^{*}          y   2  i    =   {      y   2  i   *        if    y   1  i   *    >  0       0       if    y   1  i   *    ≤  0.            subscript  y    2  i     cases   superscript   subscript  y    2  i          if   superscript   subscript  y    1  i       0   0      if   superscript   subscript  y    1  i       0.      y_{2i}=\begin{cases}y_{2i}^{*}&\textrm{if}\;y_{1i}^{*}>0\\
 0&\textrm{if}\;y_{1i}^{*}\leq 0.\end{cases}     Type V  Similar to Type II, in Type V we only observe the sign of     y   3  i    =   {      y   3  i   *        if    y   1  i   *    >  0       0       if    y   1  i   *    ≤  0.            subscript  y    3  i     cases   superscript   subscript  y    3  i          if   superscript   subscript  y    1  i       0   0      if   superscript   subscript  y    1  i       0.      y_{3i}=\begin{cases}y_{3i}^{*}&\textrm{if}\;y_{1i}^{*}>0\\
 0&\textrm{if}\;y_{1i}^{*}\leq 0.\end{cases}   .      y  L     subscript  y  L    y_{L}          y  j  *   ≤   y  L        superscript   subscript  y  j      subscript  y  L     y_{j}^{*}\leq y_{L}     The likelihood function  Below are the likelihood and log likelihood functions for a type I Tobit. This is a Tobit that is censored from below at    I   (   y  j   )       I   subscript  y  j     I(y_{j})   when the latent variable     I   (   y  j   )    =   {     0       if    y  j    =   y  L        1        if    y  j    ≠   y  L    .             I   subscript  y  j     cases  0      if   subscript  y  j     subscript  y  L    1      if   subscript  y  j     subscript  y  L       I(y_{j})=\begin{cases}0&\textrm{if}\;y_{j}=y_{L}\\
 1&\textrm{if}\;y_{j}\neq y_{L}.\end{cases}   . In writing out the likelihood function, we first define an indicator function   Φ   normal-Φ   \Phi   where:     ϕ   ϕ   \phi     Next, we mean     ℒ   (  β  ,  σ  )    =    ∏   j  =  1   N      (    1  σ   ϕ   (     y  j   -    X  j   β    σ   )    )    I   (   y  j   )       (   1  -   Φ   (      X  j   β   -   y  L    σ   )     )    1  -   I   (   y  j   )              ℒ   β  σ      superscript   subscript  product    j  1    N      superscript      1  σ   ϕ       subscript  y  j      subscript  X  j   β    σ      I   subscript  y  j      superscript    1    normal-Φ         subscript  X  j   β    subscript  y  L    σ       1    I   subscript  y  j          \mathcal{L}(\beta,\sigma)=\prod_{j=1}^{N}\left(\frac{1}{\sigma}\phi\left(\frac%
 {y_{j}-X_{j}\beta}{\sigma}\right)\right)^{I\left(y_{j}\right)}\left(1-\Phi%
 \left(\frac{X_{j}\beta-y_{L}}{\sigma}\right)\right)^{1-I\left(y_{j}\right)}   to be the standard normal cumulative distribution function and      log  ℒ    (  β  ,  σ  )    =     ∑   j  =  1   n    I   (   y  j   )    log   (    1  σ   ϕ   (     y  j   -    X  j   β    σ   )    )      +    (   1  -   I   (   y  j   )     )    log   (   1  -   Φ   (      X  j   β   -   y  L    σ   )     )              ℒ    β  σ        subscript   superscript   n     j  1      I   subscript  y  j         1  σ   ϕ       subscript  y  j      subscript  X  j   β    σ           1    I   subscript  y  j         1    normal-Φ         subscript  X  j   β    subscript  y  L    σ          \log\mathcal{L}(\beta,\sigma)=\sum^{n}_{j=1}I(y_{j})\log\left(\frac{1}{\sigma}%
 \phi\left(\frac{y_{j}-X_{j}\beta}{\sigma}\right)\right)+(1-I(y_{j}))\log\left(%
 1-\Phi\left(\frac{X_{j}\beta-y_{L}}{\sigma}\right)\right)   to be the standard normal probability density function . For a data set with N observations the likelihood function for a type I Tobit is  $$\mathcal{L}(\beta, \sigma) =  \prod _{j=1}^N \left(\frac{1}{\sigma}\phi \left(\frac{y_j-X_j\beta  }{\sigma
    }\right)\right)^{I\left(y_j\right)} \left(1-\Phi
    \left(\frac{X_j\beta-y_L}{\sigma}\right)\right)^{1-I\left(y_j\right)}$$  and the log likelihood is given by  $$\log \mathcal{L}(\beta, \sigma) = \sum^n_{j = 1} I(y_j) \log \left( \frac{1}{\sigma} \phi\left( \frac{y_j - X_j\beta}{\sigma} \right) \right) + (1 - I(y_j)) \log\left( 1- \Phi\left( \frac{X_j \beta - y_L}{\sigma} \right) \right)$$  See also   Generalized Tobit  Limited dependent variable  Rectifier (neural networks)  Truncated regression model   References  Further reading         "  Category:Regression analysis  Category:Econometrics  Category:Single-equation methods (econometrics)     ↩  International Encyclopedia of the Social Sciences (2008) ↩  ↩     