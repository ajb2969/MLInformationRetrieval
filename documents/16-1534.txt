   Social cognitive optimization      Social cognitive optimization   Social cognitive optimization (SCO) is a population-based metaheuristic  optimization algorithm which was developed in 2002. 1 This algorithm is based on the social cognitive theory , and the key point of the ergodicity is the process of individual learning of a set of agents with their own memory and their social learning with the knowledge points in the social sharing library. It has been used for solving continuous optimization , 2 3  integer programming , 4 and combinatorial optimization problems. It has been incorporated into the NLPSolver extension of Calc in Apache OpenOffice .  Algorithm  Let    f   (  x  )       f  x    f(x)   be a global optimization problem, where   x   x   x   is a state in the problem space   S   S   S   . In SCO, each state is called a knowledge point , and the function   f   f   f   is the goodness function .  In SCO, there are a population of    N  c     subscript  N  c    N_{c}   cognitive agents solving in parallel, with a social sharing library. Each agent holds a private memory containing one knowledge point, and the social sharing library contains a set of    N  L     subscript  N  L    N_{L}   knowledge points. The algorithm runs in T iterative learning cycles. By running as a Markov chain process, the system behavior in the t th cycle only depends on the system status in the ( t − 1)th cycle. The process flow is in follows:   [1. Initialization]：Initialize the private knowledge point    x  i     subscript  x  i    x_{i}   in the memory of each agent   i   i   i   , and all knowledge points in the social sharing library   X   X   X   , normally at random in the problem space   S   S   S   .  [2. Learning cycle]： At each cycle   t   t   t       (   t  =   1  ,  …  ,  T    )      t   1  normal-…  T     (t=1,\ldots,T)   ：  [2.1. Observational learning] For each agent   i   i   i       (   i  =   1  ,  …  ,   N  c     )      i   1  normal-…   subscript  N  c      (i=1,\ldots,N_{c})   ：  [2.1.1. Model selection]：Find a high-quality model point     x  M     subscript  x  M    x_{M}   in    X   (  t  )       X  t    X(t)   , normally realized using tournament selection , which returns the best knowledge point from randomly selected    τ  B     subscript  τ  B    \tau_{B}   points.  [2.1.2. Quality Evaluation]：Compare the private knowledge point     x  i    (  t  )        subscript  x  i   t    x_{i}(t)   and the model point    x  M     subscript  x  M    x_{M}   ，and return the one with higher quality as the base point     x   B  a  s  e      subscript  x    B  a  s  e     x_{Base}   ，and another as the reference point     x   R  e  f      subscript  x    R  e  f     x_{Ref}   。  [2.1.3. Learning]：Combine    x   B  a  s  e      subscript  x    B  a  s  e     x_{Base}   and    x   R  e  f      subscript  x    R  e  f     x_{Ref}   to generate a new knowledge point     x  i    (   t  +  1   )        subscript  x  i     t  1     x_{i}(t+1)   . Normally     x  i    (   t  +  1   )        subscript  x  i     t  1     x_{i}(t+1)   should be around    x   B  a  s  e      subscript  x    B  a  s  e     x_{Base}   ，and the distance with    x   B  a  s  e      subscript  x    B  a  s  e     x_{Base}   is related to the distance between    x   R  e  f      subscript  x    R  e  f     x_{Ref}   and    x   B  a  s  e      subscript  x    B  a  s  e     x_{Base}   , and boundary handling mechanism should be incorporated here to ensure that      x  i    (   t  +  1   )    ∈  S         subscript  x  i     t  1    S    x_{i}(t+1)\in S   .  [2.1.4. Knowledge sharing]：Share a knowledge point, normally     x  i    (   t  +  1   )        subscript  x  i     t  1     x_{i}(t+1)   , to the social sharing library   X   X   X   .  [2.1.5. Individual update]：Update the private knowledge of agent   i   i   i   , normally replace     x  i    (  t  )        subscript  x  i   t    x_{i}(t)   by     x  i    (   t  +  1   )        subscript  x  i     t  1     x_{i}(t+1)   . Some Monte Carlo types might also be considered.   [2.2. Library Maintenance]：The social sharing library using all knowledge points submitted by agents to update    X   (  t  )       X  t    X(t)   into    X   (   t  +  1   )       X    t  1     X(t+1)   . A simple way is one by one tournament selection: for each knowledge point submitted by an agent, replace the worse one among    τ  W     subscript  τ  W    \tau_{W}   points randomly selected from    X   (  t  )       X  t    X(t)   .   [3. Termination]：Return the best knowledge point found by the agents.   SCO has three main parameters, i.e., the number of agents    N  c     subscript  N  c    N_{c}   , the size of social sharing library    N  L     subscript  N  L    N_{L}   , and the learning cycle   T   T   T   . With the initialization process, the total number of knowledge points to be generated is     N  L   +    N  c   *   (   T  +  1   )         subscript  N  L      subscript  N  c     T  1      N_{L}+N_{c}*(T+1)   , and is not related too much with    N  L     subscript  N  L    N_{L}   if   T   T   T   is large.  Compared to traditional swarm algorithms, e.g. particle swarm optimization , SCO can achieving high-quality solutions as    N  c     subscript  N  c    N_{c}   is small, even as     N  c   =  1       subscript  N  c   1    N_{c}=1   . Nevertheless, smaller    N  c     subscript  N  c    N_{c}   and    N  L     subscript  N  L    N_{L}   might lead to premature convergence . Some variants 5 were proposed to guaranteed the global convergence.  References  "  Category:Heuristic algorithms  Category:Optimization_algorithms_and_methods  Category:Metaheuristics  Category:Collective intelligence             