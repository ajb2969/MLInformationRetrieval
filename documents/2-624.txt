   Maxwell–Boltzmann statistics      Maxwell–Boltzmann statistics   In statistical mechanics , Maxwell–Boltzmann statistics describes the average distribution of non-interacting material particles over various energy states in thermal equilibrium , and is applicable when the temperature is high enough or the particle density is low enough to render quantum effects negligible.  The expected number of particles with energy    ε  i     subscript  ε  i    \varepsilon_{i}   for Maxwell–Boltzmann statistics is    ⟨   N  i   ⟩     delimited-⟨⟩   subscript  N  i     \langle N_{i}\rangle   where:       ⟨   N  i   ⟩   =    g  i    e     (    ε  i   -  μ   )   /  k   T     =     N  Z     g  i    e   -     ε  i   /  k   T             delimited-⟨⟩   subscript  N  i       subscript  g  i    superscript  e         subscript  ε  i   μ   k   T              N  Z    subscript  g  i    superscript  e         subscript  ε  i   k   T         \langle N_{i}\rangle=\frac{g_{i}}{e^{(\varepsilon_{i}-\mu)/kT}}=\frac{N}{Z}\,g%
 _{i}e^{-\varepsilon_{i}/kT}   where:       ε  i     subscript  ε  i    \varepsilon_{i}   is the i -th energy level      ⟨   N  i   ⟩     delimited-⟨⟩   subscript  N  i     \langle N_{i}\rangle   is the number of particles in the set of states with energy    ε  i     subscript  ε  i    \varepsilon_{i}         g  i     subscript  g  i    g_{i}   is the degeneracy of energy level i , that is the number of single-particle states with energy    ε  i     subscript  ε  i    \varepsilon_{i}     μ is the chemical potential  k is Boltzmann's constant  T is absolute temperature  N is the total number of particles         N  =    ∑  i     N  i         N    subscript   i    subscript  N  i      N=\sum_{i}N_{i}\,         Z is the partition function         Z  =    ∑  i     g  i    e   -     ε  i   /  k   T           Z    subscript   i      subscript  g  i    superscript  e         subscript  ε  i   k   T         Z=\sum_{i}g_{i}e^{-\varepsilon_{i}/kT}         e (...) is the exponential function   Equivalently, the particle number is sometimes expressed as       ⟨   N  i   ⟩   =   1   e     (    ε  i   -  μ   )   /  k   T     =     N  Z     e   -     ε  i   /  k   T             delimited-⟨⟩   subscript  N  i      1   superscript  e         subscript  ε  i   μ   k   T              N  Z    superscript  e         subscript  ε  i   k   T         \langle N_{i}\rangle=\frac{1}{e^{(\varepsilon_{i}-\mu)/kT}}=\frac{N}{Z}\,e^{-%
 \varepsilon_{i}/kT}     where the index i now specifies a particular state rather than the set of all states with energy    ε  i     subscript  ε  i    \varepsilon_{i}   , and    Z  =    ∑  i    e   -     ε  i   /  k   T          Z    subscript   i    superscript  e         subscript  ε  i   k   T        Z=\sum_{i}e^{-\varepsilon_{i}/kT}     Applications  Maxwell–Boltzmann statistics may be used to derive the Maxwell–Boltzmann distribution (for an ideal gas of classical particles in a three-dimensional box). However, they apply to other situations as well. Maxwell–Boltzmann statistics can be used to extend that distribution to particles with a different energy–momentum relation , such as relativistic particles ( Maxwell–Jüttner distribution ). In addition, hypothetical situations can be considered, such as particles in a box with different numbers of dimensions (four-dimensional, two-dimensional, etc.).  Limits of applicability  Maxwell–Boltzmann statistics are often described as the statistics of "distinguishable" classical particles. In other words, the configuration of particle A in state 1 and particle B in state 2 is different from the case in which particle B is in state 1 and particle A is in state 2. This assumption leads to the proper (Boltzmann) statistics of particles in the energy states, but yields non-physical results for the entropy, as embodied in the Gibbs paradox .  At the same time, there are no real particles which have the characteristics required by Maxwell–Boltzmann statistics. Indeed, the Gibbs paradox is resolved if we treat all particles of a certain type (e.g., electrons, protons, etc.) as indistinguishable, and this assumption can be justified in the context of quantum mechanics. Once this assumption is made, the particle statistics change. Quantum particles are either bosons (following instead Bose–Einstein statistics ) or fermions (subject to the Pauli exclusion principle , following instead Fermi–Dirac statistics ). Both of these quantum statistics approach the Maxwell–Boltzmann statistics in the limit of high temperature and low particle density, without the need for any ad hoc assumptions. The Fermi–Dirac and Bose–Einstein statistics give the energy level occupation as:        ⟨   N  i   ⟩   =    g  i     e     (    ε  i   -  μ   )   /  k   T    ±  1     .       delimited-⟨⟩   subscript  N  i       subscript  g  i    plus-or-minus   superscript  e         subscript  ε  i   μ   k   T    1      \langle N_{i}\rangle=\frac{g_{i}}{e^{(\varepsilon_{i}-\mu)/kT}\pm 1}.   It can be seen that the condition under which the Maxwell–Boltzmann statistics are valid is when        e     (    ε  min   -  μ   )   /  k   T    ≫  1   ,     much-greater-than   superscript  e         subscript  ε  min   μ   k   T    1    e^{(\varepsilon_{\rm min}-\mu)/kT}\gg 1,   where    ε  min     subscript  ε  min    \varepsilon_{\rm min}   is the lowest (minimum) value of    ε  i     subscript  ε  i    \varepsilon_{i}   .  Maxwell–Boltzmann statistics are particularly useful for studying gases that are not very dense. Note, however, that all of these statistics assume that the particles are non-interacting and have static energy states.  Derivations of Maxwell–Boltzmann statistics  Maxwell–Boltzmann statistics can be derived in various statistical mechanical thermodynamic ensembles: 1   The grand canonical ensemble , exactly.  The canonical ensemble , exactly.  The microcanonical ensemble , but only in the thermodynamic limit.   In each case it is necessary to assume that the particles are non-interacting, and that multiple particles can occupy the same state and do so independently.  Derivation from microcanonical ensemble  Suppose we have a container with a huge number of very small particles all with identical physical characteristics (such as mass, charge, etc.). Let's refer to this as the system . Assume that though the particles have identical properties, they are distinguishable. For example, we might identify each particle by continually observing their trajectories, or by placing a marking on each one, e.g., drawing a different number on each one as is done with lottery balls.  The particles are moving inside that container in all directions with great speed. Because the particles are speeding around, they possess some energy. The Maxwell–Boltzmann distribution is a mathematical function that speaks about how many particles in the container have a certain energy.  In general, there may be many particles with the same amount of energy   ε   ε   \varepsilon   . Let the number of particles with the same energy    ε  1     subscript  ε  1    \varepsilon_{1}   be    N  1     subscript  N  1    N_{1}   , the number of particles possessing another energy    ε  2     subscript  ε  2    \varepsilon_{2}   be    N  2     subscript  N  2    N_{2}   , and so forth for all the possible energies {    ε  i     subscript  ε  i    \varepsilon_{i}   | i=1,2,3,...}. To describe this situation, we say that    N  i     subscript  N  i    N_{i}   is the occupation number of the energy level     i  .    i   i.   If we know all the occupation numbers {    N  i     subscript  N  i    N_{i}   | i=1,2,3,...}, then we know the total energy of the system. However, because we can distinguish between which particles are occupying each energy level, the set of occupation numbers {    N  i     subscript  N  i    N_{i}   | i=1,2,3,...} does not completely describe the state of the system. To completely describe the state of the system, or the microstate , we must specify exactly which particles are in each energy level. Thus when we count the number of possible states of the system, we must count each and every microstate, and not just the possible sets of occupation numbers.  To begin with, let's ignore the degeneracy problem: assume that there is only one way to put    N  i     subscript  N  i    N_{i}   particles into the energy level   i   i   i   . What follows next is a bit of combinatorial thinking which has little to do in accurately describing the reservoir of particles.  The number of different ways of performing an ordered selection of one single object from N objects is obviously N . The number of different ways of selecting two objects from N objects, in a particular order, is thus N ( N − 1) and that of selecting n objects in a particular order is seen to be N !/( N − n )!. It is divided by the number of permutations , n !, if order does not matter. The binomial coefficient , N !/( n !( N − n )!), is, thus, the number of ways to pick n objects from   N   N   N   . If we now have a set of boxes labelled a, b, c, d, e, ..., k , then the number of ways of selecting N a objects from a total of N objects and placing them in box a , then selecting N b objects from the remaining N − N a objects and placing them in box b , then selecting N c objects from the remaining N − N a − N b objects and placing them in box c , and continuing until no object is left outside is     W   W   \displaystyle W     and because not even a single object is to be left outside the boxes, implies that the sum made of the terms N a , N b , N c , N d , N e , ..., N k must equal N , thus the term (N - N a - N b - N c - ... - N l - N k )! in the relation above evaluates to 0! . (0!=1) which makes possible to write down that relation as     W   W   \displaystyle W     Now going back to the degeneracy problem which characterize the reservoir of particles. If the i -th box has a "degeneracy" of    g  i     subscript  g  i    g_{i}   , that is, it has    g  i     subscript  g  i    g_{i}   "sub-boxes", such that any way of filling the i -th box where the number in the sub-boxes is changed is a distinct way of filling the box, then the number of ways of filling the i -th box must be increased by the number of ways of distributing the    N  i     subscript  N  i    N_{i}   objects in the    g  i     subscript  g  i    g_{i}   "sub-boxes". The number of ways of placing    N  i     subscript  N  i    N_{i}   distinguishable objects in    g  i     subscript  g  i    g_{i}   "sub-boxes" is    g  i   N  i      superscript   subscript  g  i    subscript  N  i     g_{i}^{N_{i}}   (the first object can go into any of the    g  i     subscript  g  i    g_{i}   boxes, the second object can also go into any of the    g  i     subscript  g  i    g_{i}   boxes, and so on). Thus the number of ways   W   W   W   that a total of   N   N   N   particles can be classified into energy levels according to their energies, while each level   i   i   i   having    g  i     subscript  g  i    g_{i}   distinct states such that the i -th level accommodates    N  i     subscript  N  i    N_{i}   particles is:      W  =    N  !    ∏    g  i   N  i      N  i   !          W      N    product     superscript   subscript  g  i    subscript  N  i       subscript  N  i         W=N!\prod\frac{g_{i}^{N_{i}}}{N_{i}!}     This is the form for W first derived by Boltzmann . Boltzmann's fundamental equation    S  =    k    ln  W        S    k    W      S=k\,\ln W   relates the thermodynamic entropy  S to the number of microstates W , where k is the Boltzmann constant . It was pointed out by Gibbs however, that the above expression for W does not yield an extensive entropy, and is therefore faulty. This problem is known as the Gibbs paradox . The problem is that the particles considered by the above equation are not indistinguishable . In other words, for two particles ( A and B ) in two energy sublevels the population represented by [A,B] is considered distinct from the population [B,A] while for indistinguishable particles, they are not. If we carry out the argument for indistinguishable particles, we are led to the Bose–Einstein expression for W :      W  =    ∏  i      (     N  i   +   g  i    -  1   )   !      N  i   !     (    g  i   -  1   )   !          W    subscript  product  i            subscript  N  i    subscript  g  i    1         subscript  N  i         subscript  g  i   1         W=\prod_{i}\frac{(N_{i}+g_{i}-1)!}{N_{i}!(g_{i}-1)!}     The Maxwell–Boltzmann distribution follows from this Bose–Einstein distribution for temperatures well above absolute zero, implying that     g  i   ≫  1     much-greater-than   subscript  g  i   1    g_{i}\gg 1   . The Maxwell–Boltzmann distribution also requires low density, implying that     g  i   ≫   N  i      much-greater-than   subscript  g  i    subscript  N  i     g_{i}\gg N_{i}   . Under these conditions, we may use Stirling's approximation for the factorial:        N  !   ≈    N  N    e   -  N      ,        N      superscript  N  N    superscript  e    N       N!\approx N^{N}e^{-N},     to write:      W  ≈    ∏  i      (    N  i   +   g  i    )     N  i   +   g  i       N  i   N  i     g  i   g  i       ≈    ∏  i      g  i   N  i      (   1  +    N  i   /   g  i     )    g  i      N  i   N  i            W    subscript  product  i      superscript     subscript  N  i    subscript  g  i       subscript  N  i    subscript  g  i        superscript   subscript  N  i    subscript  N  i     superscript   subscript  g  i    subscript  g  i              subscript  product  i        superscript   subscript  g  i    subscript  N  i     superscript    1     subscript  N  i    subscript  g  i      subscript  g  i      superscript   subscript  N  i    subscript  N  i         W\approx\prod_{i}\frac{(N_{i}+g_{i})^{N_{i}+g_{i}}}{N_{i}^{N_{i}}g_{i}^{g_{i}}%
 }\approx\prod_{i}\frac{g_{i}^{N_{i}}(1+N_{i}/g_{i})^{g_{i}}}{N_{i}^{N_{i}}}     Using the fact that      (   1  +    N  i   /   g  i     )    g  i    ≈   e   N  i         superscript    1     subscript  N  i    subscript  g  i      subscript  g  i     superscript  e   subscript  N  i      (1+N_{i}/g_{i})^{g_{i}}\approx e^{N_{i}}   for     g  i   ≫   N  i      much-greater-than   subscript  g  i    subscript  N  i     g_{i}\gg N_{i}   we can again use Stirlings approximation to write:      W  ≈    ∏  i     g  i   N  i      N  i   !         W    subscript  product  i      superscript   subscript  g  i    subscript  N  i       subscript  N  i        W\approx\prod_{i}\frac{g_{i}^{N_{i}}}{N_{i}!}     This is essentially a division by N! of Boltzmann's original expression for W , and this correction is referred to as correct Boltzmann counting .  We wish to find the    N  i     subscript  N  i    N_{i}   for which the function   W   W   W   is maximized, while considering the constraint that there is a fixed number of particles    (   N  =   ∑   N  i     )      N     subscript  N  i      \left(N=\textstyle\sum N_{i}\right)   and a fixed energy    (   E  =   ∑    N  i    ε  i      )      E       subscript  N  i    subscript  ε  i       \left(E=\textstyle\sum N_{i}\varepsilon_{i}\right)   in the container. The maxima of   W   W   W   and    ln   (  W  )       W    \ln(W)   are achieved by the same values of    N  i     subscript  N  i    N_{i}   and, since it is easier to accomplish mathematically, we will maximize the latter function instead. We constrain our solution using Lagrange multipliers forming the function:       f   (   N  1   ,   N  2   ,  …  ,   N  n   )    =    ln   (  W  )    +   α   (   N  -   ∑   N  i     )    +   β   (   E  -   ∑    N  i    ε  i      )           f    subscript  N  1    subscript  N  2   normal-…   subscript  N  n         W     α    N     subscript  N  i        β    E       subscript  N  i    subscript  ε  i          f(N_{1},N_{2},\ldots,N_{n})=\ln(W)+\alpha(N-\sum N_{i})+\beta(E-\sum N_{i}%
 \varepsilon_{i})          ln  W   =   ln   [    ∏   i  =  1   n     g  i   N  i      N  i   !     ]    ≈    ∑   i  =  1   n    (      N  i    ln   g  i     -    N  i    ln   N  i      +   N  i    )            W       superscript   subscript  product    i  1    n      superscript   subscript  g  i    subscript  N  i       subscript  N  i              superscript   subscript     i  1    n          subscript  N  i      subscript  g  i        subscript  N  i      subscript  N  i       subscript  N  i        \ln W=\ln\left[\prod\limits_{i=1}^{n}\frac{g_{i}^{N_{i}}}{N_{i}!}\right]%
 \approx\sum\limits_{i=1}^{n}\left(N_{i}\ln g_{i}-N_{i}\ln N_{i}+N_{i}\right)     Finally       f   (   N  1   ,   N  2   ,  …  ,   N  n   )    =    α  N   +   β  E   +    ∑   i  =  1   n    (       N  i    ln   g  i     -    N  i    ln   N  i      +   N  i    -    (   α  +   β   ε  i     )    N  i     )           f    subscript  N  1    subscript  N  2   normal-…   subscript  N  n         α  N     β  E     superscript   subscript     i  1    n            subscript  N  i      subscript  g  i        subscript  N  i      subscript  N  i       subscript  N  i        α    β   subscript  ε  i      subscript  N  i         f(N_{1},N_{2},\ldots,N_{n})=\alpha N+\beta E+\sum\limits_{i=1}^{n}\left(N_{i}%
 \ln g_{i}-N_{i}\ln N_{i}+N_{i}-(\alpha+\beta\varepsilon_{i})N_{i}\right)     In order to maximize the expression above we apply Fermat's theorem (stationary points) , according to which local extrema, if exist, must be at critical points (partial derivatives vanish):        ∂  f    ∂   N  i     =    ln   g  i    -   ln   N  i    -   (   α  +   β   ε  i     )    =  0            f      subscript  N  i          subscript  g  i       subscript  N  i      α    β   subscript  ε  i           0     \frac{\partial f}{\partial N_{i}}=\ln g_{i}-\ln N_{i}-(\alpha+\beta\varepsilon%
 _{i})=0     By solving the equations above (    i  =   1  …  n       i    1  normal-…  n     i=1\ldots n   ) we arrive to an expression for    N  i     subscript  N  i    N_{i}   :       N  i   =    g  i    e   α  +   β   ε  i            subscript  N  i      subscript  g  i    superscript  e    α    β   subscript  ε  i         N_{i}=\frac{g_{i}}{e^{\alpha+\beta\varepsilon_{i}}}     Substituting this expression for    N  i     subscript  N  i    N_{i}   into the equation for    ln  W      W    \ln W   and assuming that    N  ≫  1     much-greater-than  N  1    N\gg 1   yields:       ln  W   =     (   α  +  1   )   N   +   β   E           W         α  1   N     β  E      \ln W=(\alpha+1)N+\beta E\,     or, differentiating and rearranging:       d  E   =     1  β   d   ln  W    -    α  β   d  N          d  E         1  β   d    W        α  β   d  N      dE=\frac{1}{\beta}d\ln W-\frac{\alpha}{\beta}dN     Boltzmann realized that this is just an expression of the second law of thermodynamics . Identifying dE as the internal energy, the second law of thermodynamics states that for variation only in entropy ( S ) and particle number ( N ):       d  E   =     T   d  S   +    μ   d  N          d  E       T  d  S     μ  d  N      dE=T\,dS+\mu\,dN     where T is the temperature and μ is the chemical potential . Boltzmann's famous equation    S  =    k    ln  W        S    k    W      S=k\,\ln W   is the realization that the entropy is proportional to    ln  W      W    \ln W   with the constant of proportionality being Boltzmann's constant . It follows immediately that    β  =    1  /  k   T       β      1  k   T     \beta=1/kT   and    α  =   -    μ  /  k   T        α        μ  k   T      \alpha=-\mu/kT   so that the populations may now be written:       N  i   =    g  i    e     (    ε  i   -  μ   )   /  k   T          subscript  N  i      subscript  g  i    superscript  e         subscript  ε  i   μ   k   T       N_{i}=\frac{g_{i}}{e^{(\varepsilon_{i}-\mu)/kT}}     Note that the above formula is sometimes written:       N  i   =    g  i     e     ε  i   /  k   T    /  z         subscript  N  i      subscript  g  i      superscript  e       subscript  ε  i   k   T    z      N_{i}=\frac{g_{i}}{e^{\varepsilon_{i}/kT}/z}     where    z  =   exp   (    μ  /  k   T   )        z        μ  k   T      z=\exp(\mu/kT)   is the absolute activity .  Alternatively, we may use the fact that        ∑  i    N  i    =   N         subscript   i    subscript  N  i    N    \sum_{i}N_{i}=N\,     to obtain the population numbers as       N  i   =   N     g  i    e   -     ε  i   /  k   T      Z         subscript  N  i     N       subscript  g  i    superscript  e         subscript  ε  i   k   T      Z      N_{i}=N\frac{g_{i}e^{-\varepsilon_{i}/kT}}{Z}     where Z is the partition function defined by:      Z  =    ∑  i     g  i    e   -     ε  i   /  k   T           Z    subscript   i      subscript  g  i    superscript  e         subscript  ε  i   k   T         Z=\sum_{i}g_{i}e^{-\varepsilon_{i}/kT}     Derivation from canonical ensemble  In the above discussion, the Boltzmann distribution function was obtained via directly analysing the multiplicities of a system. Alternatively, one can make use of the canonical ensemble . In a canonical ensemble, a system is in thermal contact with a reservoir. While energy is free to flow between the system and the reservoir, the reservoir is thought to have infinitely large heat capacity as to maintain constant temperature, T , for the combined system.  In the present context, our system is assumed to have the energy levels    ε  i     subscript  ε  i    \varepsilon_{i}   with degeneracies    g  i     subscript  g  i    g_{i}   . As before, we would like to calculate the probability that our system has energy    ε  i     subscript  ε  i    \varepsilon_{i}   .  If our system is in state     s   1     subscript  s  1    \;s_{1}   , then there would be a corresponding number of microstates available to the reservoir. Call this number      Ω   R    (   s  1   )        subscript  normal-Ω  R    subscript  s  1     \;\Omega_{R}(s_{1})   . By assumption, the combined system (of the system we are interested in and the reservoir) is isolated, so all microstates are equally probable. Therefore, for instance, if       Ω   R    (   s  1   )    =    2    Ω  R    (   s  2   )           subscript  normal-Ω  R    subscript  s  1      2   subscript  normal-Ω  R    subscript  s  2      \;\Omega_{R}(s_{1})=2\;\Omega_{R}(s_{2})   , we can conclude that our system is twice as likely to be in state     s   1     subscript  s  1    \;s_{1}   than     s   2     subscript  s  2    \;s_{2}   . In general, if     P    (   s  i   )       P   subscript  s  i     \;P(s_{i})   is the probability that our system is in state     s   i     subscript  s  i    \;s_{i}   ,         P   (   s  1   )     P   (   s  2   )     =     Ω  R    (   s  1   )      Ω  R    (   s  2   )      .          P   subscript  s  1      P   subscript  s  2          subscript  normal-Ω  R    subscript  s  1       subscript  normal-Ω  R    subscript  s  2       \frac{P(s_{1})}{P(s_{2})}=\frac{\Omega_{R}(s_{1})}{\Omega_{R}(s_{2})}.     Since the entropy of the reservoir      S   R   =   k   ln   Ω  R          subscript  S  R     k     subscript  normal-Ω  R       \;S_{R}=k\ln\Omega_{R}   , the above becomes         P   (   s  1   )     P   (   s  2   )     =    e     S  R    (   s  1   )    /  k     e     S  R    (   s  2   )    /  k     =   e    (     S  R    (   s  1   )    -    S  R    (   s  2   )     )   /  k     .            P   subscript  s  1      P   subscript  s  2        superscript  e       subscript  S  R    subscript  s  1    k     superscript  e       subscript  S  R    subscript  s  2    k           superscript  e         subscript  S  R    subscript  s  1       subscript  S  R    subscript  s  2     k       \frac{P(s_{1})}{P(s_{2})}=\frac{e^{S_{R}(s_{1})/k}}{e^{S_{R}(s_{2})/k}}=e^{(S_%
 {R}(s_{1})-S_{R}(s_{2}))/k}.     Next we recall the thermodynamic identity (from the first law of thermodynamics ):        d   S  R    =    1  T    (     d   U  R    +    P   d   V  R     -    μ   d   N  R     )     .        d   subscript  S  R        1  T         d   subscript  U  R      P  d   subscript  V  R       μ  d   subscript  N  R        dS_{R}=\frac{1}{T}(dU_{R}+P\,dV_{R}-\mu\,dN_{R}).     In a canonical ensemble, there is no exchange of particles, so the    d   N  R       d   subscript  N  R     dN_{R}   term is zero. Similarly,     d   V  R    =  0.        d   subscript  V  R    0.    dV_{R}=0.   This gives          S  R    (   s  1   )    -    S  R    (   s  2   )     =    1  T    (     U  R    (   s  1   )    -    U  R    (   s  2   )     )    =   -    1  T    (    E   (   s  1   )    -   E   (   s  2   )     )      ,             subscript  S  R    subscript  s  1       subscript  S  R    subscript  s  2         1  T        subscript  U  R    subscript  s  1       subscript  U  R    subscript  s  2                 1  T       E   subscript  s  1      E   subscript  s  2          S_{R}(s_{1})-S_{R}(s_{2})=\frac{1}{T}(U_{R}(s_{1})-U_{R}(s_{2}))=-\frac{1}{T}(%
 E(s_{1})-E(s_{2})),     where      U   R    (   s  i   )        subscript  U  R    subscript  s  i     \;U_{R}(s_{i})   and     E    (   s  i   )       E   subscript  s  i     \;E(s_{i})   denote the energies of the reservoir and the system at    s  i     subscript  s  i    s_{i}   , respectively. For the second equality we have used the conservation of energy. Substituting into the first equation relating     P   (   s  1   )    ,   P   (   s  2   )         P   subscript  s  1      P   subscript  s  2      P(s_{1}),\;P(s_{2})   :         P   (   s  1   )     P   (   s  2   )     =    e   -     E   (   s  1   )    /  k   T      e   -     E   (   s  2   )    /  k   T       ,          P   subscript  s  1      P   subscript  s  2        superscript  e          E   subscript  s  1    k   T      superscript  e          E   subscript  s  2    k   T        \frac{P(s_{1})}{P(s_{2})}=\frac{e^{-E(s_{1})/kT}}{e^{-E(s_{2})/kT}},     which implies, for any state s of the system        P   (  s  )    =    1  Z    e   -     E   (  s  )    /  k   T       ,        P  s       1  Z    superscript  e          E  s   k   T        P(s)=\frac{1}{Z}e^{-E(s)/kT},     where Z is an appropriately chosen "constant" to make total probability 1. ( Z is constant provided that the temperature T is invariant.) It is obvious that        Z   =    ∑  s    e   -     E   (  s  )    /  k   T       ,      Z    subscript   s    superscript  e          E  s   k   T        \;Z=\sum_{s}e^{-E(s)/kT},     where the index s runs through all microstates of the system. Z is sometimes called the Boltzmann sum over states (or "Zustandsumme" in the original German). If we index the summation via the energy eigenvalues instead of all possible states, degeneracy must be taken into account. The probability of our system having energy    ε  i     subscript  ε  i    \varepsilon_{i}   is simply the sum of the probabilities of all corresponding microstates:       P   (   ε  i   )    =    1  Z    g  i    e   -     ε  i   /  k   T            P   subscript  ε  i        1  Z    subscript  g  i    superscript  e         subscript  ε  i   k   T        P(\varepsilon_{i})=\frac{1}{Z}g_{i}e^{-\varepsilon_{i}/kT}     where, with obvious modification,       Z  =    ∑  j     g  j    e   -     ε  j   /  k   T        ,      Z    subscript   j      subscript  g  j    superscript  e         subscript  ε  j   k   T         Z=\sum_{j}g_{j}e^{-\varepsilon_{j}/kT},     this is the same result as before.  Comments on this derivation:   Notice that in this formulation, the initial assumption "... ''suppose the system has total N particles''..." is dispensed with. Indeed, the number of particles possessed by the system plays no role in arriving at the distribution. Rather, how many particles would occupy states with energy    ε  i     subscript  ε  i    \varepsilon_{i}   follows as an easy consequence.  What has been presented above is essentially a derivation of the canonical partition function. As one can see by comparing the definitions, the Boltzmann sum over states is equal to the canonical partition function.  Exactly the same approach can be used to derive Fermi–Dirac and Bose–Einstein statistics. However, there one would replace the canonical ensemble with the grand canonical ensemble , since there is exchange of particles between the system and the reservoir. Also, the system one considers in those cases is a single particle state , not a particle. (In the above discussion, we could have assumed our system to be a single atom.)   See also   Bose–Einstein statistics  Fermi–Dirac statistics  Boltzmann factor   References  Bibliography   Carter, Ashley H., "Classical and Statistical Thermodynamics", Prentice–Hall, Inc., 2001, New Jersey.  Raj Pathria , "Statistical Mechanics", Butterworth–Heinemann, 1996.   "  Category:Concepts in physics       ↩     