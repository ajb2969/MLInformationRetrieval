   Information theory and measure theory      Information theory and measure theory   This article discusses how information theory (a branch of mathematics studying the transmission, processing and storage of information ) is related to measure theory (a branch of mathematics related to integration and probability ).  Measures in information theory  Many of the concepts in information theory have separate definitions and formulas for continuous and discrete cases. For example, entropy     H   (  X  )       H  X    H(X)   is usually defined for discrete random variables, whereas for continuous random variables the related concept of differential entropy , written    h   (  X  )       h  X    h(X)   , is used (see Cover and Thomas, 2006, chapter 8). Both these concepts are mathematical expectations , but the expectation is defined with an integral for the continuous case, and a sum for the discrete case.  These separate definitions can be more closely related in terms of measure theory . For discrete random variables, probability mass functions can be considered density functions with respect to the counting measure, thus requiring only basic discrete mathematics to perform the integration, in the measure-theoretic sense. Because the same integration expression is used for the continuous case, which uses basic calculus , the same concepts and expressions can be used for both discrete and continuous cases. Consider the formula for the differential entropy of a continuous random variable    X   X   X   with probability density function     f   (  x  )       f  x    f(x)   :        h   (  X  )    =   -    âˆ«  X    f   (  x  )    log  f    (  x  )   d  x      .        h  X       subscript   X     f  x    f   x  d  x       h(X)=-\int_{X}f(x)\log f(x)\,dx.   This can usually be interpreted as the following Riemann-Stieltjes integral :        h   (  X  )    =   -    âˆ«  X    f   (  x  )    log  f    (  x  )   d  Î¼   (  x  )       ,        h  X       subscript   X     f  x    f   x  d  Î¼  x       h(X)=-\int_{X}f(x)\log f(x)\,d\mu(x),   where   Î¼   Î¼   \mu   is the Lebesgue measure . But if instead,   X   X   X   is discrete,   f   f   f   is a probability mass function, and   Î½   Î½   \nu   is the counting measure , we can write:        H   (  X  )    =   -    âˆ«  X    f   (  x  )    log  f    (  x  )   d  Î½   (  x  )      =   -    âˆ‘   x  âˆˆ  X     f   (  x  )    log  f    (  x  )       .          H  X       subscript   X     f  x    f   x  d  Î½  x              subscript     x  X      f  x    f   x        H(X)=-\int_{X}f(x)\log f(x)\,d\nu(x)=-\sum_{x\in X}f(x)\log f(x).   The integral expression and the general concept is identical to the continuous case; the only difference is the measure used. In both cases the probability density function   f   f   f   is the Radonâ€“Nikodym derivative of the probability measure with respect to the measure against which the integral is taken.  If   â„™   â„™   \mathbb{P}   is the probability measure on   X   X   X   , then the integral can also be taken directly with respect to   â„™   â„™   \mathbb{P}   :        h   (  X  )    =   -    âˆ«  X    log      d  â„™    d  Î¼     d  â„™       ,        h  X       subscript   X           normal-d  â„™     normal-d  Î¼    d  â„™        h(X)=-\int_{X}\log\frac{\mathrm{d}\mathbb{P}}{\mathrm{d}\mu}\,d\mathbb{P},     If instead of the underlying measure Î¼ we take another probability measure   â„š   â„š   \mathbb{Q}   , we are led to the Kullbackâ€“Leibler divergence : let   â„™   â„™   \mathbb{P}   and   â„š   â„š   \mathbb{Q}   be probability measures over the same space. Then if   â„™   â„™   \mathbb{P}   is absolutely continuous with respect to   â„š   â„š   \mathbb{Q}   , written     â„™  â‰ª  â„š   ,     much-less-than  â„™  â„š    \mathbb{P}<<\mathbb{Q},   the Radonâ€“Nikodym derivative      d  â„™    d  â„š         normal-d  â„™     normal-d  â„š     \frac{\mathrm{d}\mathbb{P}}{\mathrm{d}\mathbb{Q}}   exists and the Kullbackâ€“Leibler divergence can be expressed in its full generality:       D  KL    (  â„™  âˆ¥  â„š  )   =   âˆ«   supp  â„™      d  â„™    d  â„š    log     d  â„™    d  â„š     d  â„š  =   âˆ«   supp  â„™    log     d  â„™    d  â„š     d  â„™  ,     fragments   subscript  D  KL    fragments  normal-(  P  parallel-to  Q  normal-)     subscript     supp  â„™        normal-d  â„™     normal-d  â„š         normal-d  â„™     normal-d  â„š    d  Q    subscript     supp  â„™         normal-d  â„™     normal-d  â„š    d  P  normal-,    D_{\mathrm{KL}}(\mathbb{P}\|\mathbb{Q})=\int_{\mathrm{supp}\mathbb{P}}\frac{%
 \mathrm{d}\mathbb{P}}{\mathrm{d}\mathbb{Q}}\log\frac{\mathrm{d}\mathbb{P}}{%
 \mathrm{d}\mathbb{Q}}\,d\mathbb{Q}=\int_{\mathrm{supp}\mathbb{P}}\log\frac{%
 \mathrm{d}\mathbb{P}}{\mathrm{d}\mathbb{Q}}\,d\mathbb{P},   where the integral runs over the support of    â„™  .    â„™   \mathbb{P}.   Note that we have dropped the negative sign: the Kullbackâ€“Leibler divergence is always non-negative due to Gibbs' inequality .  Entropy as a "measure"    There is an analogy between Shannon 's basic " measures " of the information content of random variables and a measure over sets. Namely the joint entropy , conditional entropy , and mutual information can be considered as the measure of a set union , set difference , and set intersection , respectively (Reza pp.Â 106â€“108).  If we associate the existence of abstract sets     X  ~     normal-~  X    \tilde{X}   and    Y  ~     normal-~  Y    \tilde{Y}   to arbitrary discrete  random variables  X and Y , somehow representing the information borne by X and Y , respectively, such that:        Î¼   (    X  ~   âˆ©   Y  ~    )    =  0        Î¼     normal-~  X    normal-~  Y     0    \mu(\tilde{X}\cap\tilde{Y})=0   whenever X and Y are unconditionally independent , and       X  ~   =   Y  ~        normal-~  X    normal-~  Y     \tilde{X}=\tilde{Y}   whenever X and Y are such that either one is completely determined by the other (i.e. by a bijection);   where   Î¼   Î¼   \mu   is a signed measure over these sets, and we set:        H   (  X  )    =   Î¼   (   X  ~   )     ,        H  X     Î¼   normal-~  X      H(X)=\mu(\tilde{X}),           H   (  Y  )    =   Î¼   (   Y  ~   )     ,        H  Y     Î¼   normal-~  Y      H(Y)=\mu(\tilde{Y}),           H   (  X  ,  Y  )    =   Î¼   (    X  ~   âˆª   Y  ~    )     ,        H   X  Y      Î¼     normal-~  X    normal-~  Y       H(X,Y)=\mu(\tilde{X}\cup\tilde{Y}),         H   (  X  |  Y  )   =  Î¼   (    X  ~    \   Y  ~   )   ,     fragments  H   fragments  normal-(  X  normal-|  Y  normal-)    Î¼   fragments  normal-(   normal-~  X   normal-\   normal-~  Y   normal-)   normal-,    H(X|Y)=\mu(\tilde{X}\,\backslash\,\tilde{Y}),           I   (  X  ;  Y  )    =   Î¼   (    X  ~   âˆ©   Y  ~    )     ;        I   X  Y      Î¼     normal-~  X    normal-~  Y       I(X;Y)=\mu(\tilde{X}\cap\tilde{Y});     we find that Shannon 's "measure" of information content satisfies all the postulates and basic properties of a formal signed measure over sets, as commonly illustrated in an information diagram . This allows the sum of two measures to be written:        Î¼   (  A  )    +   Î¼   (  B  )     =    Î¼   (   A  âˆª  B   )    +   Î¼   (   A  âˆ©  B   )             Î¼  A     Î¼  B        Î¼    A  B      Î¼    A  B       \mu(A)+\mu(B)=\mu(A\cup B)+\mu(A\cap B)     and the analog of Bayes' theorem (      Î¼   (  A  )    +   Î¼   (   B  \  A   )     =    Î¼   (  B  )    +   Î¼   (   A  \  B   )             Î¼  A     Î¼   normal-\  B  A         Î¼  B     Î¼   normal-\  A  B       \mu(A)+\mu(B\backslash A)=\mu(B)+\mu(A\backslash B)   ) allows the difference of two measures to be written:        Î¼   (  A  )    -   Î¼   (  B  )     =    Î¼   (   A  \  B   )    -   Î¼   (   B  \  A   )             Î¼  A     Î¼  B        Î¼   normal-\  A  B      Î¼   normal-\  B  A       \mu(A)-\mu(B)=\mu(A\backslash B)-\mu(B\backslash A)     This can be a handy mnemonic device in some situations, e.g.            H   (  X  ,  Y  )   =  H   (  X  )   +  H   (  Y  |  X  )      fragments  H   fragments  normal-(  X  normal-,  Y  normal-)    H   fragments  normal-(  X  normal-)    H   fragments  normal-(  Y  normal-|  X  normal-)   italic-    H(X,Y)=H(X)+H(Y|X)\,\qquad\qquad\qquad\,           Î¼   (    X  ~   âˆª   Y  ~    )    =    Î¼   (   X  ~   )    +   Î¼   (     Y  ~    \   X  ~    )           Î¼     normal-~  X    normal-~  Y         Î¼   normal-~  X      Î¼   normal-\   normal-~  Y    normal-~  X        \mu(\tilde{X}\cup\tilde{Y})=\mu(\tilde{X})+\mu(\tilde{Y}\,\backslash\,\tilde{X})            I   (  X  ;  Y  )   =  H   (  X  )   -  H   (  X  |  Y  )      fragments  I   fragments  normal-(  X  normal-;  Y  normal-)    H   fragments  normal-(  X  normal-)    H   fragments  normal-(  X  normal-|  Y  normal-)     I(X;Y)=H(X)-H(X|Y)\,           Î¼   (    X  ~   âˆ©   Y  ~    )    =    Î¼   (   X  ~   )    -   Î¼   (     X  ~    \   Y  ~    )           Î¼     normal-~  X    normal-~  Y         Î¼   normal-~  X      Î¼   normal-\   normal-~  X    normal-~  Y        \mu(\tilde{X}\cap\tilde{Y})=\mu(\tilde{X})-\mu(\tilde{X}\,\backslash\,\tilde{Y})          Note that measures (expectation values of the logarithm) of true probabilities are called "entropy" and generally represented by the letter H , while other measures are often referred to as "information" or "correlation" and generally represented by the letter I . For notational simplicity, the letter I is sometimes used for all measures.  Multivariate mutual information  Certain extensions to the definitions of Shannon's basic measures of information are necessary to deal with the Ïƒ-algebra generated by the sets that would be associated to three or more arbitrary random variables. (See Reza pp.Â 106â€“108 for an informal but rather complete discussion.) Namely    H   (  X  ,  Y  ,  Z  ,  â‹¯  )       H   X  Y  Z  normal-â‹¯     H(X,Y,Z,\cdots)   needs to be defined in the obvious way as the entropy of a joint distribution, and a multivariate mutual information     I   (  X  ;  Y  ;  Z  ;  â‹¯  )       I   X  Y  Z  normal-â‹¯     I(X;Y;Z;\cdots)   defined in a suitable manner so that we can set:        H   (  X  ,  Y  ,  Z  ,  â‹¯  )    =   Î¼   (    X  ~   âˆª   Y  ~   âˆª   Z  ~   âˆª  â‹¯   )     ,        H   X  Y  Z  normal-â‹¯      Î¼     normal-~  X    normal-~  Y    normal-~  Z   normal-â‹¯      H(X,Y,Z,\cdots)=\mu(\tilde{X}\cup\tilde{Y}\cup\tilde{Z}\cup\cdots),           I   (  X  ;  Y  ;  Z  ;  â‹¯  )    =   Î¼   (    X  ~   âˆ©   Y  ~   âˆ©   Z  ~   âˆ©  â‹¯   )     ;        I   X  Y  Z  normal-â‹¯      Î¼     normal-~  X    normal-~  Y    normal-~  Z   normal-â‹¯      I(X;Y;Z;\cdots)=\mu(\tilde{X}\cap\tilde{Y}\cap\tilde{Z}\cap\cdots);     in order to define the (signed) measure over the whole Ïƒ-algebra. There is no single universally accepted definition for the mutivariate mutual information, but the one that corresponds here to the measure of a set intersection is due to Fano (Srinivasa). The definition is recursive. As a base case the mutual information of a single random variable is defined to be its entropy     I   (  X  )    =   H   (  X  )          I  X     H  X     I(X)=H(X)   . Then for    n  â‰¥  2      n  2    n\geq 2   we set      I   (   X  1   ;  â‹¯  ;   X  n   )   =  I   (   X  1   ;  â‹¯  ;   X   n  -  1    )   -  I   (   X  1   ;  â‹¯  ;   X   n  -  1    |   X  n   )   ,     fragments  I   fragments  normal-(   subscript  X  1   normal-;  normal-â‹¯  normal-;   subscript  X  n   normal-)    I   fragments  normal-(   subscript  X  1   normal-;  normal-â‹¯  normal-;   subscript  X    n  1    normal-)    I   fragments  normal-(   subscript  X  1   normal-;  normal-â‹¯  normal-;   subscript  X    n  1    normal-|   subscript  X  n   normal-)   normal-,    I(X_{1};\cdots;X_{n})=I(X_{1};\cdots;X_{n-1})-I(X_{1};\cdots;X_{n-1}|X_{n}),   where the conditional mutual information is defined as      I   (   X  1   ;  â‹¯  ;   X   n  -  1    |   X  n   )   =   ð”¼   X  n     (  I   (   X  1   ;  â‹¯  ;   X   n  -  1    )   |   X  n   )   .     fragments  I   fragments  normal-(   subscript  X  1   normal-;  normal-â‹¯  normal-;   subscript  X    n  1    normal-|   subscript  X  n   normal-)     subscript  ð”¼   subscript  X  n     fragments  normal-(  I   fragments  normal-(   subscript  X  1   normal-;  normal-â‹¯  normal-;   subscript  X    n  1    normal-)   normal-|   subscript  X  n   normal-)   normal-.    I(X_{1};\cdots;X_{n-1}|X_{n})=\mathbb{E}_{X_{n}}\big(I(X_{1};\cdots;X_{n-1})|X%
 _{n}\big).   The first step in the recursion yields Shannon's definition    I   (   X  1   ;   X  2   )   =  H   (   X  1   )   -  H   (   X  1   |   X  2   )   .     fragments  I   fragments  normal-(   subscript  X  1   normal-;   subscript  X  2   normal-)    H   fragments  normal-(   subscript  X  1   normal-)    H   fragments  normal-(   subscript  X  1   normal-|   subscript  X  2   normal-)   normal-.    I(X_{1};X_{2})=H(X_{1})-H(X_{1}|X_{2}).   It is interesting to note that the multivariate mutual information (same as interaction information but for a change in sign) of three or more random variables can be negative as well as positive: Let X and Y be two independent fair coin flips, and let Z be their exclusive or . Then     I   (  X  ;  Y  ;  Z  )    =   -  1         I   X  Y  Z      1     I(X;Y;Z)=-1   bit.  Many other variations are possible for three or more random variables: for example,    I   (  X  ,  Y  ;  Z  )       I   X  Y  Z     I(X,Y;Z)   is the mutual information of the joint distribution of X and Y relative to Z , and can be interpreted as     Î¼   (    (    X  ~   âˆª   Y  ~    )   âˆ©   Z  ~    )    .      Î¼       normal-~  X    normal-~  Y     normal-~  Z      \mu((\tilde{X}\cup\tilde{Y})\cap\tilde{Z}).   Many more complicated expressions can be built this way, and still have meaning, e.g.    I   (  X  ,  Y  ;  Z  |  W  )   ,     fragments  I   fragments  normal-(  X  normal-,  Y  normal-;  Z  normal-|  W  normal-)   normal-,    I(X,Y;Z|W),   or    H   (  X  ,  Z  |  W  ,  Y  )   .     fragments  H   fragments  normal-(  X  normal-,  Z  normal-|  W  normal-,  Y  normal-)   normal-.    H(X,Z|W,Y).     References   Thomas M. Cover and Joy A. Thomas. Elements of Information Theory , second edition, 2006. New Jersey: Wiley and Sons. ISBN 978-0-471-24195-9.  Fazlollah M. Reza. An Introduction to Information Theory . New York: McGraw-Hill 1961. New York: Dover 1994. ISBN 0-486-68210-2  Sunil Srinivasa. A Review on Multivariate Mutual Information . Notre Dame EE-80653 Information Theory Tutorials, Fall 2005. PDF .   (contains errors in formulas (9) and (23) )   R. W. Yeung, "On entropy, information inequalities, and Groups." PS   See also   Information theory  Measure theory  Set theory   "  Category:Information theory  Category:Measure theory   