


Inverse iteration




Inverse iteration

In numerical analysis, inverse iteration is an iterative eigenvalue algorithm. It allows one to find an approximate eigenvector when an approximation to a corresponding eigenvalue is already known. The method is conceptually similar to the power method and is also known as the inverse power method. It appears to have originally been developed to compute resonance frequencies in the field of structural mechanics. 1
The inverse power iteration algorithm starts with an approximation 
 
 
 
  for the eigenvalue corresponding to the desired eigenvector and a vector b0, either a randomly selected vector or an approximation to the eigenvector. The method is described by the iteration



where Ck are some constants usually chosen as 
 
 
 
  Since eigenvectors are defined up to multiplication by constant, the choice of Ck can be arbitrary in theory; practical aspects of the choice of 
 
 
 
  are discussed below.
At every iteration, the vector bk is multiplied by the inverse of the matrix 
 
 
 
  and normalized. It is exactly the same formula as in the power method, except replacing the matrix A by 
 
 
 
  The closer the approximation 
 
 
 
  to the eigenvalue is chosen, the faster the algorithm converges; however, incorrect choice of 
 
 
 
  can lead to slow convergence or to the convergence to an eigenvector other than the one desired. In practice, the method is used when a good approximation for the eigenvalue is known, and hence one needs only few (quite often just one) iterations.
Theory and convergence
The basic idea of the power iteration is choosing an initial vector b (either an eigenvector approximation or a random vector) and iteratively calculating 
 
 
 
 . Except for a set of zero measure, for any initial vector, the result will converge to an eigenvector corresponding to the dominant eigenvalue.
The inverse iteration does the same for the matrix 
 
 
 
 , so it converges to eigenvector corresponding to the dominant eigenvalue of the matrix 
 
 
 
 . Eigenvalues of this matrix are 
 
 
 
  where 
 
 
 
  are eigenvalues of A. The largest of these numbers corresponds to the smallest of 
 
 
 
  It is obvious that the eigenvectors of A and of 
 
 
 
  are the same.
Conclusion: The method converges to the eigenvector of the matrix A corresponding to the closest eigenvalue to 
 
 

In particular taking 
 
 
 
  we see that 
 
 
 
  converges to the eigenvector corresponding to the eigenvalue of A with the smallest absolute value.
Speed of convergence
Let us analyze the rate of convergence of the method.
The power method is known to converge linearly to the limit, more precisely:



hence for the inverse iteration method similar result sounds as:



This is a key formula for understanding the method's convergence. It shows that if 
 
 
 
  is chosen close enough to some eigenvalue 
 
 
 
 , for example 
 
 
 
  each iteration will improve the accuracy 
 
 
 
  times. (We use that for small enough 
 
 
 
  "closest to 
 
 
 
 " and "closest to 
 
 
 
 " is the same.) For small enough 
 
 
 
  it is approximately the same as 
 
 
 
 . Hence if one is able to find 
 
 
 
 , such the 
 
 
 
  will be small enough, then very few iterations may be satisfactory.
Complexity
The inverse iteration algorithm requires solving a linear system or calculation of the inverse matrix. For non-structured matrices (not sparse, not Toeplitz,...) this requires 
 
 
 
  operations.
Implementation options
The method is defined by the formula:



There are, however, multiple options for its implementation.
Calculate inverse matrix or solve system of linear equations
We can rewrite the formula in the following way:



emphasizing that to find the next approximation 
 
 
 
  we may solve a system of linear equations. There are two options: one may choose an algorithm that solves a linear system, or one may calculate the inverse 
 
 
 
  and then apply it to the vector. Both options have complexity O(n3), the exact number depends on the chosen method.
The choice depends also on the number of iterations. Naively, if at each iteration one solves a linear system, the complexity will be k*O(n3), where k is number of iterations; similarly, calculating the inverse matrix and applying it at each iteration is of complexity k*O(n3). Note, however, that if the eigenvalue estimate 
 
 
 
  remains constant, then we may reduce the complexity to O(n3) + k*O(n2) with either method. Calculating the inverse matrix once, and storing it to apply at each iteration is of complexity O(n3) + k*O(n2). Storing an LU decomposition of 
 
 
 
  and using forward and back substitution to solve the system of equations at each iteration is also of complexity O(n3) + k*O(n2).
Inverting the matrix will typically have a greater initial cost, but lower cost at each iteration. Conversely, solving systems of linear equations will typically have a lesser initial cost, but require more operations for each iteration.
Tridiagonalization, Hessenberg form
If it is necessary to perform many iterations (or few iterations, but for many eigenvectors), then it might be wise to bring the matrix to the upper Hessenberg form first (for symmetric matrix this will be tridiagonal form). Which costs 
 
 
 
  arithmetic operations using a technique based on Householder reduction), with a finite sequence of orthogonal similarity transforms, somewhat like a two-sided QR decomposition.23 (For QR decomposition, the Householder rotations are multiplied only on the left, but for the Hessenberg case they are multiplied on both left and right.) For symmetric matrices this procedure costs 
 
 
 
  arithmetic operations using a technique based on Householder reduction.45
Solution of the system of linear equations for the tridiagonal matrix costs O(n) operations, so the complexity grows like O(n3)+k*O(n), where k is the iteration number, which is better than for the direct inversion. However for few iterations such transformation may not be practical.
Also transformation to the Hessenberg form involves square roots and the division operation, which are not universally supported by hardware.
Choice of the normalization constant Ck
On general purpose processors (e.g. produced by Intel) the execution time of addition, multiplication and division is approximately equal. But on embedded and/or low energy consuming hardware (digital signal processors, FPGA, ASIC) division may not be supported by hardware, and so should be avoided. Choosing Ck=2nk allows fast division without explicit hardware support, as division by a power of 2 may be implemented as either a bit shift (for fixed-point arithmetic) or subtraction of k from the exponent (for floating-point arithmetic).
When implementing the algorithm using fixed-point arithmetic, the choice of the constant Ck is especially important. Small values will lead to fast growth of the norm of bk and to overflow; large values of Ck will cause the vector bk to tend toward zero.
Usage
The main application of the method is the situation when an approximation to an eigenvalue is found and one needs to find the corresponding approximate eigenvector. In such situation the inverse iteration is the main and probably the only method to use. So typically the method is used in combination with some other methods which allows to find approximate eigenvalues: the standard example is the bisection eigenvalue algorithm, another example is the Rayleigh quotient iteration which is actually the same inverse iteration with the choice of the approximate eigenvalue as the Rayleigh quotient corresponding to the vector obtained on the previous step of the iteration.
There are some situations where the method can be used by itself, however they are quite marginal.
Dominant eigenvector. The dominant eigenvalue can be easily estimated for any matrix. For any induced norm it is true that 
 
 
 
  for any eigenvalue 
 
 
 
 . So taking the norm of the matrix as an approximate eigenvalue one can see that the method will converge to the dominant eigenvector.
Estimates based on statistics. In some real-time applications one needs to find eigenvectors for matrices with a speed may be millions matrices per second. In such applications typically the statistics of matrices is known in advance and one can take as approximate eigenvalue the average eigenvalue for some large matrix sample, or better one calculates the mean ratio of the eigenvalue to the trace or the norm of the matrix and eigenvalue is estimated as trace or norm multiplied on the average value the their ratio. Clearly such method can be used with much care and only in situations when the mistake in calculations is allowed. Actually such idea can be combined with other methods to avoid too big errors.
See also

Power iteration
Rayleigh quotient iteration
List of eigenvalue algorithms

References
External links

Inverse Iteration to find eigenvectors, physics.arizona.edu
The Power Method for Eigenvectors, math.fullerton.edu

"
Category:Numerical linear algebra



Ernst Pohlhausen, Berechnung der Eigenschwingungen statisch-bestimmter Fachwerke, ZAMM - Zeitschrift f√ºr Angewandte Mathematik und Mechanik 1, 28-42 (1921).
.
Lloyd N. Trefethen and David Bau, Numerical Linear Algebra (SIAM, 1997).






