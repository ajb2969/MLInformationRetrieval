   Rasch model estimation      Rasch model estimation   Estimation of a Rasch model is used to estimate the parameters of the Rasch model . Various techniques are employed to estimate the parameters from matrices of response data. The most common approaches are types of maximum likelihood estimation, such as joint and conditional maximum likelihood estimation. Joint maximum likelihood (JML) equations are efficient, but inconsistent for a finite number of items, whereas conditional maximum likelihood (CML) equations give consistent and unbiased item estimates. Person estimates are generally thought to have bias associated with them, although weighted likelihood estimation methods for the estimation of person parameters reduce the bias.  Rasch model  The Rasch model for dichotomous data takes the form:        Pr   {    X   n  i    =  1   }    =    exp   (    β  n   -   δ  i    )     1  +   exp   (    β  n   -   δ  i    )       ,       Pr     subscript  X    n  i    1           subscript  β  n    subscript  δ  i       1       subscript  β  n    subscript  δ  i         \Pr\{X_{ni}=1\}=\frac{\exp({\beta_{n}}-{\delta_{i}})}{1+\exp({\beta_{n}}-{%
 \delta_{i}})},     where    β  n     subscript  β  n    \beta_{n}   is the ability of person   n   n   n   and    δ  i     subscript  δ  i    \delta_{i}   is the difficulty of item   i   i   i   .  Joint maximum likelihood  Let    x   n  i      subscript  x    n  i     x_{ni}   denote the observed response for person n on item i . The probability of the observed data matrix, which is the product of the probabilities of the individual responses, is given by the likelihood function       Λ  =     ∏  n     ∏  i    exp   (    x   n  i     (    β  n   -   δ  i    )    )        ∏  n     ∏  i    (   1  +   exp   (    β  n   -   δ  i    )     )       .      normal-Λ      subscript  product  n     subscript  product  i        subscript  x    n  i       subscript  β  n    subscript  δ  i          subscript  product  n     subscript  product  i     1       subscript  β  n    subscript  δ  i           \Lambda=\frac{\prod_{n}\prod_{i}\exp(x_{ni}(\beta_{n}-\delta_{i}))}{\prod_{n}%
 \prod_{i}(1+\exp(\beta_{n}-\delta_{i}))}.     The log-likelihood function is then       log  Λ   =     ∑  n  N     β  n    r  n     -    ∑  i  I     δ  i    s  i     -    ∑  n  N     ∑  i  I    log   (   1  +   exp   (    β  n   -   δ  i    )     )             normal-Λ       superscript   subscript   n   N      subscript  β  n    subscript  r  n       superscript   subscript   i   I      subscript  δ  i    subscript  s  i       superscript   subscript   n   N     superscript   subscript   i   I       1       subscript  β  n    subscript  δ  i            \log\Lambda=\sum_{n}^{N}\beta_{n}r_{n}-\sum_{i}^{I}\delta_{i}s_{i}-\sum_{n}^{N%
 }\sum_{i}^{I}\log(1+\exp(\beta_{n}-\delta_{i}))     where     r  n   =    ∑  i  I    x   n  i          subscript  r  n     superscript   subscript   i   I    subscript  x    n  i       r_{n}=\sum_{i}^{I}x_{ni}   is the total raw score for person n ,     s  i   =    ∑  n  N    x   n  i          subscript  s  i     superscript   subscript   n   N    subscript  x    n  i       s_{i}=\sum_{n}^{N}x_{ni}   is the total raw score for item i , N is the total number of persons and I is the total number of items.  Solution equations are obtained by taking partial derivatives with respect to    δ  i     subscript  δ  i    \delta_{i}   and    β  n     subscript  β  n    \beta_{n}   and setting the result equal to 0. The JML solution equations are:        s  i   =    ∑  n  N    p   n  i      ,   i  =   1  ,  …  ,  I       formulae-sequence     subscript  s  i     superscript   subscript   n   N    subscript  p    n  i        i   1  normal-…  I      s_{i}=\sum_{n}^{N}p_{ni},\quad i=1,\dots,I           r  n   =    ∑  i  I    p   n  i      ,   n  =   1  ,  …  ,  N       formulae-sequence     subscript  r  n     superscript   subscript   i   I    subscript  p    n  i        n   1  normal-…  N      r_{n}=\sum_{i}^{I}p_{ni},\quad n=1,\dots,N     where     p   n  i    =    exp   (    β  n   -   δ  i    )    /   (   1  +   exp   (    β  n   -   δ  i    )     )         subscript  p    n  i           subscript  β  n    subscript  δ  i       1       subscript  β  n    subscript  δ  i         p_{ni}=\exp(\beta_{n}-\delta_{i})/(1+\exp(\beta_{n}-\delta_{i}))   . A more accurate estimate of each    δ  i     subscript  δ  i    \delta_{i}   is obtained by multiplying the estimates by     (   I  -  1   )   /  I        I  1   I    (I-1)/I   .  Conditional maximum likelihood  The conditional likelihood function is defined as      Λ  =    ∏  n    Pr   {   (   x   n  i    )   ∣   r  n   }     =    exp   (    ∑  i   -    s  i    δ  i     )      ∏  n    γ  r           normal-Λ    subscript  product  n    Pr   subscript  x    n  i     subscript  r  n                 subscript   i      subscript  s  i    subscript  δ  i        subscript  product  n    subscript  γ  r        \Lambda=\prod_{n}\Pr\{(x_{ni})\mid r_{n}\}=\frac{\exp(\sum_{i}-s_{i}\delta_{i}%
 )}{\prod_{n}\gamma_{r}}     in which       γ  r   =    ∑    (  x  )   ∣  r     exp   (   -    ∑  i     x   n  i     δ  i      )          subscript  γ  r     subscript    fragments   fragments  normal-(  x  normal-)   normal-∣  r          subscript   i      subscript  x    n  i     subscript  δ  i          \gamma_{r}=\sum_{(x)\mid r}\exp(-\sum_{i}x_{ni}\delta_{i})     is the elementary symmetric function of order r , which represents the sum over all combinations of r items. For example, in the case of three items,        γ  2   =    exp   (    -   δ  1    -   δ  2    )    +   exp   (    -   δ  1    -   δ  3    )    +   exp   (    -   δ  2    -   δ  3    )      .       subscript  γ  2            subscript  δ  1     subscript  δ  2            subscript  δ  1     subscript  δ  3            subscript  δ  2     subscript  δ  3        \gamma_{2}=\exp(-\delta_{1}-\delta_{2})+\exp(-\delta_{1}-\delta_{3})+\exp(-%
 \delta_{2}-\delta_{3}).     Estimation algorithms  Some kind of expectation-maximization algorithm is used in the estimation of the parameters of Rasch models. Algorithms for implementing Maximum Likelihood estimation commonly employ Newton-Raphson iterations to solve for solution equations obtained from setting the partial derivatives of the log-likelihood functions equal to 0. Convergence criteria are used to determine when the iterations cease. For example, the criterion might be that the mean item estimate changes by less than a certain value, such as 0.001, between one iteration and another for all items.  See also   Expectation-maximization algorithm  Rasch model   References   Linacre, J.M. (2004). Estimation methods for Rasch measures . Chapter 2 in E.V. Smith & R. M. Smith (Eds.) Introduction to Rasch Measurement. Maple Grove MN: JAM Press.  Linacre, J.M. (2004). Rasch model estimation: further topics . Chapter 24 in E.V. Smith & R. M. Smith (Eds.) Introduction to Rasch Measurement. Maple Grove MN: JAM Press.   "  Category:Psychometrics  Category:Statistical models   