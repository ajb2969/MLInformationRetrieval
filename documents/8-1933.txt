   Whitening transformation      Whitening transformation   A whitening transformation is a decorrelation transformation that transforms an arbitrary set of variables having a known covariance matrix    M   M   M   into a set of new variables whose covariance is the identity matrix (meaning that they are uncorrelated and all have variance 1).  The transformation is called "whitening" because it changes the input vector into a white noise vector . It differs from a general decorrelation transformation in that the latter only makes the covariances equal to zero, so that the correlation matrix may be any diagonal matrix.  The inverse coloring transformation transforms a vector   Y   Y   Y   of uncorrelated variables (a white random vector) into a vector   X   X   X   with a specified covariance matrix.  Definition  Suppose   X   X   X   is a random (column) vector with covariance matrix   M   M   M   and mean   0   0    . Typically (when   M   M   M   is not singular) whitening   X   X   X   means multiplying by    M   -   1  /  2       superscript  M      1  2      M^{-1/2}   .  The matrix   M   M   M   can be written as the expected value of the outer product of   X   X   X   with itself, namely:      M  =   E   [   X   X  T    ]        M   normal-E    X   superscript  X  T       M=\operatorname{E}[XX^{T}]   When   M   M   M   is symmetric and positive definite (and therefore not singular), it has a positive definite symmetric square root     M   1  /  2      superscript  M    1  2     M^{1/2}   , such that      M   1  /  2     M   1  /  2     =  M         superscript  M    1  2     superscript  M    1  2     M    M^{1/2}M^{1/2}=M   . Since   M   M   M   is positive definite,    M   1  /  2      superscript  M    1  2     M^{1/2}   is invertible, and the vector    Y  =    M   -   1  /  2     X       Y     superscript  M      1  2     X     Y=M^{-1/2}X   has covariance matrix:      Cov   (  Y  )   =  E   [  Y   Y  T   ]   =   M   -   1  /  2     E   [  X   X  T   ]     (   M   -   1  /  2     )   T   =   M   -   1  /  2     M   M   -   1  /  2     =  I     fragments  Cov   fragments  normal-(  Y  normal-)    normal-E   fragments  normal-[  Y   superscript  Y  T   normal-]     superscript  M      1  2     normal-E   fragments  normal-[  X   superscript  X  T   normal-]    superscript   fragments  normal-(   superscript  M      1  2     normal-)   T     superscript  M      1  2     M   superscript  M      1  2      I    \operatorname{Cov}(Y)=\operatorname{E}[YY^{T}]=M^{-1/2}\operatorname{E}[XX^{T}%
 ](M^{-1/2})^{T}=M^{-1/2}MM^{-1/2}=I     and is therefore a white random vector.  If   M   M   M   is singular (and hence not positive definite), then    M   1  /  2      superscript  M    1  2     M^{1/2}   is not invertible, and it is impossible to map   X   X   X   to a white vector with the same number of components. In that case the vector   X   X   X   can still be mapped to a smaller white vector   Y   Y   Y   with   m   m   m   elements, where   m   m   m   is the number of non-zero eigenvalues of   M   M   M   .  See also   Decorrelation  Randomness extractor  Hardware random number generator  Principal component analysis   References  External links   http://courses.media.mit.edu/2010fall/mas622j/whiten.pdf  The ZCA whitening transformation . Appendix A of Learning Multiple Layers of Features from Tiny Images by A. Krizhevsky.   "  Category:Classification algorithms   