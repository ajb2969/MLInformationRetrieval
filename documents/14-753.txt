   Jarqueâ€“Bera test      Jarqueâ€“Bera test   In statistics , the Jarqueâ€“Bera test is a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution . The test is named after Carlos Jarque and Anil K. Bera . The test statistic  JB is defined as      ğ½ğµ  =      n  -  k   +  1   6    (    S  2   +    1  4     (   C  -  3   )   2     )        ğ½ğµ          n  k   1   6      superscript  S  2       1  4    superscript    C  3   2        \mathit{JB}=\frac{n-k+1}{6}\left(S^{2}+\frac{1}{4}(C-3)^{2}\right)     where n is the number of observations (or degrees of freedom in general); S is the sample skewness , C is the sample kurtosis ,and k are the number of regressors:       S  =     Î¼  ^   3     Ïƒ  ^   3    =     1  n     âˆ‘   i  =  1   n     (    x  i   -   x  Â¯    )   3       (    1  n     âˆ‘   i  =  1   n     (    x  i   -   x  Â¯    )   2     )    3  /  2      ,        S     subscript   normal-^  Î¼   3    superscript   normal-^  Ïƒ   3               1  n     superscript   subscript     i  1    n    superscript     subscript  x  i    normal-Â¯  x    3      superscript      1  n     superscript   subscript     i  1    n    superscript     subscript  x  i    normal-Â¯  x    2       3  2        S=\frac{\hat{\mu}_{3}}{\hat{\sigma}^{3}}=\frac{\frac{1}{n}\sum_{i=1}^{n}(x_{i}%
 -\bar{x})^{3}}{\left(\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\right)^{3/2}},          C  =     Î¼  ^   4     Ïƒ  ^   4    =     1  n     âˆ‘   i  =  1   n     (    x  i   -   x  Â¯    )   4       (    1  n     âˆ‘   i  =  1   n     (    x  i   -   x  Â¯    )   2     )   2     ,        C     subscript   normal-^  Î¼   4    superscript   normal-^  Ïƒ   4               1  n     superscript   subscript     i  1    n    superscript     subscript  x  i    normal-Â¯  x    4      superscript      1  n     superscript   subscript     i  1    n    superscript     subscript  x  i    normal-Â¯  x    2     2       C=\frac{\hat{\mu}_{4}}{\hat{\sigma}^{4}}=\frac{\frac{1}{n}\sum_{i=1}^{n}(x_{i}%
 -\bar{x})^{4}}{\left(\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\right)^{2}},   where     Î¼  ^   3     subscript   normal-^  Î¼   3    \hat{\mu}_{3}   and     Î¼  ^   4     subscript   normal-^  Î¼   4    \hat{\mu}_{4}   are the estimates of third and fourth central moments , respectively,    x  Â¯     normal-Â¯  x    \bar{x}   is the sample mean , and     Ïƒ  ^   2     superscript   normal-^  Ïƒ   2    \hat{\sigma}^{2}   is the estimate of the second central moment, the variance .  If the data come from a normal distribution, the JB statistic asymptotically has a chi-squared distribution with two degrees of freedom , so the statistic can be used to test the hypothesis that the data are from a normal distribution . The null hypothesis is a joint hypothesis of the skewness being zero and the excess kurtosis being zero. Samples from a normal distribution have an expected skewness of 0 and an expected excess kurtosis of 0 (which is the same as a kurtosis of 3). As the definition of JB shows, any deviation from this increases the JB statistic.  For small samples the chi-squared approximation is overly sensitive, often rejecting the null hypothesis when it is in fact true. Furthermore, the distribution of p -values departs from a uniform distribution and becomes a right-skewed uni-modal distribution, especially for small p -values. This leads to a large Type I error rate. The table below shows some p -values approximated by a chi-squared distribution that differ from their true alpha levels for small samples.        Calculated p -values equivalents to true alpha levels at given sample sizes   True Î± level   20   30   50   70   100     0.1   0.307   0.252   0.201   0.183   0.1560     0.05   0.1461   0.109   0.079   0.067   0.062     0.025   0.051   0.0303   0.020   0.016   0.0168     0.01   0.0064   0.0033   0.0015   0.0012   0.002       (These values have been approximated by using Monte Carlo simulation in Matlab )  In MATLAB 's implementation, the chi-squared approximation for the JB statistic's distribution is only used for large sample sizes (>Â 2000). For smaller samples, it uses a table derived from Monte Carlo simulations in order to interpolate p -values. 1  History  Considering normal sampling, and âˆš Î² 1 and Î² 2 contours,  noticed that the statistic JB will be asymptotically Ï‡ 2 (2)-distributed; however they also noted that â€œlarge sample sizes would doubtless be required for the Ï‡ 2 approximation to holdâ€. Bowman and Shelton did not study the properties any further, preferring Dâ€™Agostinoâ€™s K-squared test .  Jarqueâ€“Bera test in regression analysis  According to Robert Hall, David Lilien, et al. (1995) when using this test along with multiple regression analysis the right estimate is:      ğ½ğµ  =     n  -  k   6    (    S  2   +    1  4     (   K  -  3   )   2     )        ğ½ğµ        n  k   6      superscript  S  2       1  4    superscript    K  3   2        \mathit{JB}=\frac{n-k}{6}\left(S^{2}+\frac{1}{4}(K-3)^{2}\right)     where n is the number of observations and k is the number of regressors when examining residuals to an equation.  Implementations   ALGLIB includes implementation of the Jarqueâ€“Bera test in C++, C#, Delphi, Visual Basic, etc.  gretl includes an implementation of the Jarqueâ€“Bera test  R includes implementations of the Jarqueâ€“Bera test: jarque.bera.test in package tseries , 2 for example, and jarque.test in package moments . 3  MATLAB includes implementation of the Jarqueâ€“Bera test, the function "jbtest".  Python  statsmodels includes implementation of the Jarqueâ€“Bera test, "statsmodels.stats.stattools.py".   References  Further reading          "  Category:Normality tests     â†©  â†©  â†©     