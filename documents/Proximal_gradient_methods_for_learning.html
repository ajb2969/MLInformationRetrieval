<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="677">Proximal gradient methods for learning</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Proximal gradient methods for learning</h1>
<hr/>

<p><strong>Proximal gradient</strong> (forward backward splitting) <strong>methods for learning</strong> is an area of research in <a class="uri" href="optimization" title="wikilink">optimization</a> and <a href="statistical_learning_theory" title="wikilink">statistical learning theory</a> which studies algorithms for a general class of <a href="Convex_function#Definition" title="wikilink">convex</a> <a href="Regularization_(mathematics)" title="wikilink">regularization</a> problems where the regularization penalty may not be <a href="Differentiable_function" title="wikilink">differentiable</a>. One such example is 

<math display="inline" id="Proximal_gradient_methods_for_learning:0">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{1}
  </annotation>
 </semantics>
</math>

 regularization (also known as Lasso) of the form</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:1">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <mrow>
       <mrow>
        <munder>
         <mi>min</mi>
         <mrow>
          <mi>w</mi>
          <mo>∈</mo>
          <msup>
           <mi>ℝ</mi>
           <mi>d</mi>
          </msup>
         </mrow>
        </munder>
        <mfrac>
         <mn>1</mn>
         <mi>n</mi>
        </mfrac>
       </mrow>
       <mrow>
        <munderover>
         <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
         <mrow>
          <mi>i</mi>
          <mo>=</mo>
          <mn>1</mn>
         </mrow>
         <mi>n</mi>
        </munderover>
        <msup>
         <mrow>
          <mo stretchy="false">(</mo>
          <mrow>
           <msub>
            <mi>y</mi>
            <mi>i</mi>
           </msub>
           <mo>-</mo>
           <mrow>
            <mo stretchy="false">⟨</mo>
            <mi>w</mi>
            <mo>,</mo>
            <msub>
             <mi>x</mi>
             <mi>i</mi>
            </msub>
            <mo stretchy="false">⟩</mo>
           </mrow>
          </mrow>
          <mo stretchy="false">)</mo>
         </mrow>
         <mn>2</mn>
        </msup>
       </mrow>
      </mrow>
      <mo>+</mo>
      <mrow>
       <mi>λ</mi>
       <msub>
        <mrow>
         <mo>∥</mo>
         <mi>w</mi>
         <mo>∥</mo>
        </mrow>
        <mn>1</mn>
       </msub>
      </mrow>
     </mrow>
     <mo rspace="12.5pt">,</mo>
     <mrow>
      <mtext>where</mtext>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
     </mrow>
    </mrow>
    <mo>∈</mo>
    <mrow>
     <msup>
      <mi>ℝ</mi>
      <mi>d</mi>
     </msup>
     <mtext>and</mtext>
     <msub>
      <mi>y</mi>
      <mi>i</mi>
     </msub>
    </mrow>
    <mo>∈</mo>
    <mi>ℝ</mi>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <in></in>
     <list>
      <apply>
       <plus></plus>
       <apply>
        <times></times>
        <apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <min></min>
          <apply>
           <in></in>
           <ci>w</ci>
           <apply>
            <csymbol cd="ambiguous">superscript</csymbol>
            <ci>ℝ</ci>
            <ci>d</ci>
           </apply>
          </apply>
         </apply>
         <apply>
          <divide></divide>
          <cn type="integer">1</cn>
          <ci>n</ci>
         </apply>
        </apply>
        <apply>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <sum></sum>
           <apply>
            <eq></eq>
            <ci>i</ci>
            <cn type="integer">1</cn>
           </apply>
          </apply>
          <ci>n</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <apply>
           <minus></minus>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>y</ci>
            <ci>i</ci>
           </apply>
           <list>
            <ci>w</ci>
            <apply>
             <csymbol cd="ambiguous">subscript</csymbol>
             <ci>x</ci>
             <ci>i</ci>
            </apply>
           </list>
          </apply>
          <cn type="integer">2</cn>
         </apply>
        </apply>
       </apply>
       <apply>
        <times></times>
        <ci>λ</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <apply>
          <csymbol cd="latexml">norm</csymbol>
          <ci>w</ci>
         </apply>
         <cn type="integer">1</cn>
        </apply>
       </apply>
      </apply>
      <apply>
       <times></times>
       <mtext>where</mtext>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </list>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>ℝ</ci>
       <ci>d</ci>
      </apply>
      <mtext>and</mtext>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <in></in>
     <share href="#.cmml">
     </share>
     <ci>ℝ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{w\in\mathbb{R}^{d}}\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\langle w,x_{i}%
\rangle)^{2}+\lambda\|w\|_{1},\quad\text{ where }x_{i}\in\mathbb{R}^{d}\text{ %
and }y_{i}\in\mathbb{R}.
  </annotation>
 </semantics>
</math>

</p>

<p>Proximal gradient methods offer a general framework for solving regularization problems from statistical learning theory with penalties that are tailored to a specific problem application.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Such customized penalties can help to induce certain structure in problem solutions, such as <em>sparsity</em> (in the case of <a href="#Lasso_regularization" title="wikilink">lasso</a>) or <em>group structure</em> (in the case of <a href="#Exploiting_group_structure" title="wikilink">group lasso</a>).</p>
<h2 id="relevant-background">Relevant background</h2>

<p><a href="Proximal_gradient_method" title="wikilink">Proximal gradient methods</a> are applicable in a wide variety of scenarios for solving <a href="convex_optimization" title="wikilink">convex optimization</a> problems of the form</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:2">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <munder>
       <mi>min</mi>
       <mrow>
        <mi>x</mi>
        <mo>∈</mo>
        <mi class="ltx_font_mathcaligraphic">ℋ</mi>
       </mrow>
      </munder>
      <mi>F</mi>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>R</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <min></min>
       <apply>
        <in></in>
        <ci>x</ci>
        <ci>ℋ</ci>
       </apply>
      </apply>
      <ci>F</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <apply>
     <times></times>
     <ci>R</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{x\in\mathcal{H}}F(x)+R(x),
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Proximal_gradient_methods_for_learning:3">
 <semantics>
  <mi>F</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>F</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F
  </annotation>
 </semantics>
</math>

 is <a href="Convex_function" title="wikilink">convex</a> and differentiable with <a href="Lipschitz_continuity" title="wikilink">Lipschitz continuous</a> <a class="uri" href="gradient" title="wikilink">gradient</a>, 

<math display="inline" id="Proximal_gradient_methods_for_learning:4">
 <semantics>
  <mi>R</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>R</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R
  </annotation>
 </semantics>
</math>

 is a <a href="Convex_function" title="wikilink">convex</a>, <a href="Semicontinuous_function" title="wikilink">lower semicontinuous</a> function which is possibly nondifferentiable, and 

<math display="inline" id="Proximal_gradient_methods_for_learning:5">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ℋ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ℋ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{H}
  </annotation>
 </semantics>
</math>

 is some set, typically a <a href="Hilbert_space" title="wikilink">Hilbert space</a>. The usual criterion of 

<math display="inline" id="Proximal_gradient_methods_for_learning:6">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 minimizes 

<math display="inline" id="Proximal_gradient_methods_for_learning:7">
 <semantics>
  <mrow>
   <mrow>
    <mi>F</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mrow>
    <mi>R</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <ci>F</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <times></times>
     <ci>R</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F(x)+R(x)
  </annotation>
 </semantics>
</math>

 if and only if 

<math display="inline" id="Proximal_gradient_methods_for_learning:8">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mo>∇</mo>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>F</mi>
       <mo>+</mo>
       <mi>R</mi>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <apply>
      <ci>normal-∇</ci>
      <apply>
       <plus></plus>
       <ci>F</ci>
       <ci>R</ci>
      </apply>
     </apply>
     <ci>x</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \nabla(F+R)(x)=0
  </annotation>
 </semantics>
</math>

 in the convex, differentiable setting is now replaced by</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:9">
 <semantics>
  <mrow>
   <mrow>
    <mn>0</mn>
    <mo>∈</mo>
    <mrow>
     <mrow>
      <mo>∂</mo>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>F</mi>
        <mo>+</mo>
        <mi>R</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <cn type="integer">0</cn>
    <apply>
     <apply>
      <partialdiff></partialdiff>
      <apply>
       <plus></plus>
       <ci>F</ci>
       <ci>R</ci>
      </apply>
     </apply>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   0\in\partial(F+R)(x),
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Proximal_gradient_methods_for_learning:10">
 <semantics>
  <mrow>
   <mo>∂</mo>
   <mi>φ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <partialdiff></partialdiff>
    <ci>φ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \partial\varphi
  </annotation>
 </semantics>
</math>

 denotes the <a class="uri" href="subdifferential" title="wikilink">subdifferential</a> of a real-valued, convex function 

<math display="inline" id="Proximal_gradient_methods_for_learning:11">
 <semantics>
  <mi>φ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>φ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \varphi
  </annotation>
 </semantics>
</math>

.</p>

<p>Given a convex function 

<math display="inline" id="Proximal_gradient_methods_for_learning:12">
 <semantics>
  <mrow>
   <mi>φ</mi>
   <mo>:</mo>
   <mrow>
    <mi class="ltx_font_mathcaligraphic">ℋ</mi>
    <mo>→</mo>
    <mi>ℝ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>φ</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>ℋ</ci>
     <ci>ℝ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \varphi:\mathcal{H}\to\mathbb{R}
  </annotation>
 </semantics>
</math>

 an important operator to consider is its <strong>proximity operator</strong> 

<math display="inline" id="Proximal_gradient_methods_for_learning:13">
 <semantics>
  <mrow>
   <msub>
    <mo>prox</mo>
    <mi>φ</mi>
   </msub>
   <mo>:</mo>
   <mrow>
    <mi class="ltx_font_mathcaligraphic">ℋ</mi>
    <mo>→</mo>
    <mi class="ltx_font_mathcaligraphic">ℋ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>prox</ci>
     <ci>φ</ci>
    </apply>
    <apply>
     <ci>normal-→</ci>
     <ci>ℋ</ci>
     <ci>ℋ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{prox}_{\varphi}:\mathcal{H}\to\mathcal{H}
  </annotation>
 </semantics>
</math>

 defined by</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:14">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mo>prox</mo>
      <mi>φ</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>u</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mrow>
       <mrow>
        <mo>arg</mo>
        <munder>
         <mi>min</mi>
         <mrow>
          <mi>x</mi>
          <mo>∈</mo>
          <mi class="ltx_font_mathcaligraphic">ℋ</mi>
         </mrow>
        </munder>
       </mrow>
       <mi>φ</mi>
      </mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>+</mo>
     <mrow>
      <mfrac>
       <mn>1</mn>
       <mn>2</mn>
      </mfrac>
      <msubsup>
       <mrow>
        <mo>∥</mo>
        <mrow>
         <mi>u</mi>
         <mo>-</mo>
         <mi>x</mi>
        </mrow>
        <mo>∥</mo>
       </mrow>
       <mn>2</mn>
       <mn>2</mn>
      </msubsup>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>prox</ci>
      <ci>φ</ci>
     </apply>
     <ci>u</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <apply>
        <ci>arg</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <min></min>
         <apply>
          <in></in>
          <ci>x</ci>
          <ci>ℋ</ci>
         </apply>
        </apply>
       </apply>
       <ci>φ</ci>
      </apply>
      <ci>x</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <csymbol cd="latexml">norm</csymbol>
         <apply>
          <minus></minus>
          <ci>u</ci>
          <ci>x</ci>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{prox}_{\varphi}(u)=\operatorname{arg}\min_{x\in\mathcal{H}}%
\varphi(x)+\frac{1}{2}\|u-x\|_{2}^{2},
  </annotation>
 </semantics>
</math>

 which is well-defined because of the strict convexity of the 

<math display="inline" id="Proximal_gradient_methods_for_learning:15">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{2}
  </annotation>
 </semantics>
</math>

 norm. The proximity operator can be seen as a generalization of a <a href="Projection_(linear_algebra)" title="wikilink">projection</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> We see that the proximity operator is important because 

<math display="inline" id="Proximal_gradient_methods_for_learning:16">
 <semantics>
  <msup>
   <mi>x</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>x</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x^{*}
  </annotation>
 </semantics>
</math>

 is a minimizer to the problem 

<math display="inline" id="Proximal_gradient_methods_for_learning:17">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mi>min</mi>
      <mrow>
       <mi>x</mi>
       <mo>∈</mo>
       <mi class="ltx_font_mathcaligraphic">ℋ</mi>
      </mrow>
     </msub>
     <mi>F</mi>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mrow>
    <mi>R</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <min></min>
       <apply>
        <in></in>
        <ci>x</ci>
        <ci>ℋ</ci>
       </apply>
      </apply>
      <ci>F</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <apply>
     <times></times>
     <ci>R</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{x\in\mathcal{H}}F(x)+R(x)
  </annotation>
 </semantics>
</math>

 if and only if</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:18">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>x</mi>
     <mo>*</mo>
    </msup>
    <mo>=</mo>
    <mrow>
     <msub>
      <mo>prox</mo>
      <mrow>
       <mi>γ</mi>
       <mi>R</mi>
      </mrow>
     </msub>
     <mrow>
      <mo>(</mo>
      <mrow>
       <msup>
        <mi>x</mi>
        <mo>*</mo>
       </msup>
       <mo>-</mo>
       <mrow>
        <mi>γ</mi>
        <mrow>
         <mo>∇</mo>
         <mi>F</mi>
        </mrow>
        <mrow>
         <mo stretchy="false">(</mo>
         <msup>
          <mi>x</mi>
          <mo>*</mo>
         </msup>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>x</ci>
     <times></times>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>prox</ci>
      <apply>
       <times></times>
       <ci>γ</ci>
       <ci>R</ci>
      </apply>
     </apply>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>x</ci>
       <times></times>
      </apply>
      <apply>
       <times></times>
       <ci>γ</ci>
       <apply>
        <ci>normal-∇</ci>
        <ci>F</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>x</ci>
        <times></times>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x^{*}=\operatorname{prox}_{\gamma R}\left(x^{*}-\gamma\nabla F(x^{*})\right),
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Proximal_gradient_methods_for_learning:19">
 <semantics>
  <mrow>
   <mi>γ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>γ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma>0
  </annotation>
 </semantics>
</math>

 is any positive real number.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h3 id="moreau-decomposition">Moreau decomposition</h3>

<p>One important technique related to proximal gradient methods is the <strong>Moreau decomposition,</strong> which decomposes the identity operator as the sum of two proximity operators.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> Namely, let 

<math display="inline" id="Proximal_gradient_methods_for_learning:20">
 <semantics>
  <mrow>
   <mi>φ</mi>
   <mo>:</mo>
   <mrow>
    <mi class="ltx_font_mathcaligraphic">𝒳</mi>
    <mo>→</mo>
    <mi>ℝ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>φ</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>𝒳</ci>
     <ci>ℝ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \varphi:\mathcal{X}\to\mathbb{R}
  </annotation>
 </semantics>
</math>

 be a <a href="Semi-continuity" title="wikilink">lower semicontinuous</a>, convex function on a vector space 

<math display="inline" id="Proximal_gradient_methods_for_learning:21">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">𝒳</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝒳</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{X}
  </annotation>
 </semantics>
</math>

. We define its <a href="Convex_conjugate" title="wikilink">Fenchel conjugate</a> 

<math display="inline" id="Proximal_gradient_methods_for_learning:22">
 <semantics>
  <mrow>
   <msup>
    <mi>φ</mi>
    <mo>*</mo>
   </msup>
   <mo>:</mo>
   <mrow>
    <mi class="ltx_font_mathcaligraphic">𝒳</mi>
    <mo>→</mo>
    <mi>ℝ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>φ</ci>
     <times></times>
    </apply>
    <apply>
     <ci>normal-→</ci>
     <ci>𝒳</ci>
     <ci>ℝ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \varphi^{*}:\mathcal{X}\to\mathbb{R}
  </annotation>
 </semantics>
</math>

 to be the function</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:23">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msup>
      <mi>φ</mi>
      <mo>*</mo>
     </msup>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>u</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>:=</mo>
    <mrow>
     <mrow>
      <munder>
       <mo movablelimits="false">sup</mo>
       <mrow>
        <mi>x</mi>
        <mo>∈</mo>
        <mi class="ltx_font_mathcaligraphic">𝒳</mi>
       </mrow>
      </munder>
      <mrow>
       <mo stretchy="false">⟨</mo>
       <mi>x</mi>
       <mo>,</mo>
       <mi>u</mi>
       <mo stretchy="false">⟩</mo>
      </mrow>
     </mrow>
     <mo>-</mo>
     <mrow>
      <mi>φ</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">assign</csymbol>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>φ</ci>
      <times></times>
     </apply>
     <ci>u</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <csymbol cd="latexml">supremum</csymbol>
       <apply>
        <in></in>
        <ci>x</ci>
        <ci>𝒳</ci>
       </apply>
      </apply>
      <list>
       <ci>x</ci>
       <ci>u</ci>
      </list>
     </apply>
     <apply>
      <times></times>
      <ci>φ</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \varphi^{*}(u):=\sup_{x\in\mathcal{X}}\langle x,u\rangle-\varphi(x).
  </annotation>
 </semantics>
</math>

 The general form of Moreau's decomposition states that for any 

<math display="inline" id="Proximal_gradient_methods_for_learning:24">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>∈</mo>
   <mi class="ltx_font_mathcaligraphic">𝒳</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>x</ci>
    <ci>𝒳</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x\in\mathcal{X}
  </annotation>
 </semantics>
</math>

 and any 

<math display="inline" id="Proximal_gradient_methods_for_learning:25">
 <semantics>
  <mrow>
   <mi>γ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>γ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma>0
  </annotation>
 </semantics>
</math>

 that</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:26">
 <semantics>
  <mrow>
   <mrow>
    <mi>x</mi>
    <mo>=</mo>
    <mrow>
     <mrow>
      <msub>
       <mo>prox</mo>
       <mrow>
        <mi>γ</mi>
        <mi>φ</mi>
       </mrow>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>+</mo>
     <mrow>
      <mi>γ</mi>
      <mrow>
       <msub>
        <mo>prox</mo>
        <mrow>
         <msup>
          <mi>φ</mi>
          <mo>*</mo>
         </msup>
         <mo>/</mo>
         <mi>γ</mi>
        </mrow>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mi>x</mi>
         <mo>/</mo>
         <mi>γ</mi>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>x</ci>
    <apply>
     <plus></plus>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>prox</ci>
       <apply>
        <times></times>
        <ci>γ</ci>
        <ci>φ</ci>
       </apply>
      </apply>
      <ci>x</ci>
     </apply>
     <apply>
      <times></times>
      <ci>γ</ci>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>prox</ci>
        <apply>
         <divide></divide>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <ci>φ</ci>
          <times></times>
         </apply>
         <ci>γ</ci>
        </apply>
       </apply>
       <apply>
        <divide></divide>
        <ci>x</ci>
        <ci>γ</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x=\operatorname{prox}_{\gamma\varphi}(x)+\gamma\operatorname{prox}_{\varphi^{*%
}/\gamma}(x/\gamma),
  </annotation>
 </semantics>
</math>

 which for 

<math display="inline" id="Proximal_gradient_methods_for_learning:27">
 <semantics>
  <mrow>
   <mi>γ</mi>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>γ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma=1
  </annotation>
 </semantics>
</math>

 implies that 

<math display="inline" id="Proximal_gradient_methods_for_learning:28">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <msub>
      <mo>prox</mo>
      <mi>φ</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <msub>
      <mo>prox</mo>
      <msup>
       <mi>φ</mi>
       <mo>*</mo>
      </msup>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>x</ci>
    <apply>
     <plus></plus>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>prox</ci>
       <ci>φ</ci>
      </apply>
      <ci>x</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>prox</ci>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>φ</ci>
        <times></times>
       </apply>
      </apply>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x=\operatorname{prox}_{\varphi}(x)+\operatorname{prox}_{\varphi^{*}}(x)
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> The Moreau decomposition can be seen to be a generalization of the usual orthogonal decomposition of a vector space, analogous with the fact that proximity operators are generalizations of projections.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>

<p>In certain situations it may be easier to compute the proximity operator for the conjugate 

<math display="inline" id="Proximal_gradient_methods_for_learning:29">
 <semantics>
  <msup>
   <mi>φ</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>φ</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \varphi^{*}
  </annotation>
 </semantics>
</math>

 instead of the function 

<math display="inline" id="Proximal_gradient_methods_for_learning:30">
 <semantics>
  <mi>φ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>φ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \varphi
  </annotation>
 </semantics>
</math>

, and therefore the Moreau decomposition can be applied. This is the case for <a href="#Exploiting_group_structure" title="wikilink">group lasso</a>.</p>
<h2 id="lasso-regularization">Lasso regularization</h2>

<p>Consider the <a href="Regularization_(mathematics)" title="wikilink">regularized</a> <a href="empirical_risk_minimization" title="wikilink">empirical risk minimization</a> problem with square loss and with the <a href="L1-norm" title="wikilink">

<math display="inline" id="Proximal_gradient_methods_for_learning:31">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{1}
  </annotation>
 </semantics>
</math>

 norm</a> as the regularization penalty:</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:32">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <munder>
       <mi>min</mi>
       <mrow>
        <mi>w</mi>
        <mo>∈</mo>
        <msup>
         <mi>ℝ</mi>
         <mi>d</mi>
        </msup>
       </mrow>
      </munder>
      <mfrac>
       <mn>1</mn>
       <mi>n</mi>
      </mfrac>
     </mrow>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>n</mi>
      </munderover>
      <msup>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <msub>
          <mi>y</mi>
          <mi>i</mi>
         </msub>
         <mo>-</mo>
         <mrow>
          <mo stretchy="false">⟨</mo>
          <mi>w</mi>
          <mo>,</mo>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo stretchy="false">⟩</mo>
         </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
       <mn>2</mn>
      </msup>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>λ</mi>
     <msub>
      <mrow>
       <mo>∥</mo>
       <mi>w</mi>
       <mo>∥</mo>
      </mrow>
      <mn>1</mn>
     </msub>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <min></min>
       <apply>
        <in></in>
        <ci>w</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>ℝ</ci>
         <ci>d</ci>
        </apply>
       </apply>
      </apply>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>n</ci>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>i</ci>
        </apply>
        <list>
         <ci>w</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
        </list>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>λ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="latexml">norm</csymbol>
       <ci>w</ci>
      </apply>
      <cn type="integer">1</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{w\in\mathbb{R}^{d}}\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\langle w,x_{i}%
\rangle)^{2}+\lambda\|w\|_{1},
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Proximal_gradient_methods_for_learning:33">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>x</mi>
     <mi>i</mi>
    </msub>
    <mo>∈</mo>
    <mrow>
     <msup>
      <mi>ℝ</mi>
      <mi>d</mi>
     </msup>
     <mtext>and</mtext>
     <msub>
      <mi>y</mi>
      <mi>i</mi>
     </msub>
    </mrow>
    <mo>∈</mo>
    <mi>ℝ</mi>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <in></in>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>ℝ</ci>
       <ci>d</ci>
      </apply>
      <mtext>and</mtext>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <in></in>
     <share href="#.cmml">
     </share>
     <ci>ℝ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}\in\mathbb{R}^{d}\text{ and }y_{i}\in\mathbb{R}.
  </annotation>
 </semantics>
</math>

 The 

<math display="inline" id="Proximal_gradient_methods_for_learning:34">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{1}
  </annotation>
 </semantics>
</math>

 regularization problem is sometimes referred to as <em>lasso</em> (<a href="Least_squares#Lasso_method" title="wikilink">least absolute shrinkage and selection operator</a>).<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> Such 

<math display="inline" id="Proximal_gradient_methods_for_learning:35">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{1}
  </annotation>
 </semantics>
</math>

 regularization problems are interesting because they induce '' sparse'' solutions, that is, solutions 

<math display="inline" id="Proximal_gradient_methods_for_learning:36">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

 to the minimization problem have relatively few nonzero components. Lasso can be seen to be a convex relaxation of the non-convex problem</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:37">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <munder>
       <mi>min</mi>
       <mrow>
        <mi>w</mi>
        <mo>∈</mo>
        <msup>
         <mi>ℝ</mi>
         <mi>d</mi>
        </msup>
       </mrow>
      </munder>
      <mfrac>
       <mn>1</mn>
       <mi>n</mi>
      </mfrac>
     </mrow>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>n</mi>
      </munderover>
      <msup>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <msub>
          <mi>y</mi>
          <mi>i</mi>
         </msub>
         <mo>-</mo>
         <mrow>
          <mo stretchy="false">⟨</mo>
          <mi>w</mi>
          <mo>,</mo>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo stretchy="false">⟩</mo>
         </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
       <mn>2</mn>
      </msup>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>λ</mi>
     <msub>
      <mrow>
       <mo>∥</mo>
       <mi>w</mi>
       <mo>∥</mo>
      </mrow>
      <mn>0</mn>
     </msub>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <min></min>
       <apply>
        <in></in>
        <ci>w</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>ℝ</ci>
         <ci>d</ci>
        </apply>
       </apply>
      </apply>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>n</ci>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>i</ci>
        </apply>
        <list>
         <ci>w</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
        </list>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>λ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="latexml">norm</csymbol>
       <ci>w</ci>
      </apply>
      <cn type="integer">0</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{w\in\mathbb{R}^{d}}\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\langle w,x_{i}%
\rangle)^{2}+\lambda\|w\|_{0},
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Proximal_gradient_methods_for_learning:38">
 <semantics>
  <msub>
   <mrow>
    <mo>∥</mo>
    <mi>w</mi>
    <mo>∥</mo>
   </mrow>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <ci>w</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|w\|_{0}
  </annotation>
 </semantics>
</math>

 denotes the 

<math display="inline" id="Proximal_gradient_methods_for_learning:39">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{0}
  </annotation>
 </semantics>
</math>

 "norm", which is the number of nonzero entries of the vector 

<math display="inline" id="Proximal_gradient_methods_for_learning:40">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

. Sparse solutions are of particular interest in learning theory for interpretability of results: a sparse solution can identify a small number of important factors.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></p>
<h3 id="solving-for-ell_1-proximity-operator">Solving for 

<math display="inline" id="Proximal_gradient_methods_for_learning:41">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{1}
  </annotation>
 </semantics>
</math>

 proximity operator</h3>

<p>For simplicity we restrict our attention to the problem where 

<math display="inline" id="Proximal_gradient_methods_for_learning:42">
 <semantics>
  <mrow>
   <mi>λ</mi>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>λ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda=1
  </annotation>
 </semantics>
</math>

. To solve the problem</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:43">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <munder>
       <mi>min</mi>
       <mrow>
        <mi>w</mi>
        <mo>∈</mo>
        <msup>
         <mi>ℝ</mi>
         <mi>d</mi>
        </msup>
       </mrow>
      </munder>
      <mfrac>
       <mn>1</mn>
       <mi>n</mi>
      </mfrac>
     </mrow>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>n</mi>
      </munderover>
      <msup>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <msub>
          <mi>y</mi>
          <mi>i</mi>
         </msub>
         <mo>-</mo>
         <mrow>
          <mo stretchy="false">⟨</mo>
          <mi>w</mi>
          <mo>,</mo>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo stretchy="false">⟩</mo>
         </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
       <mn>2</mn>
      </msup>
     </mrow>
    </mrow>
    <mo>+</mo>
    <msub>
     <mrow>
      <mo>∥</mo>
      <mi>w</mi>
      <mo>∥</mo>
     </mrow>
     <mn>1</mn>
    </msub>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <min></min>
       <apply>
        <in></in>
        <ci>w</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>ℝ</ci>
         <ci>d</ci>
        </apply>
       </apply>
      </apply>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>n</ci>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>i</ci>
        </apply>
        <list>
         <ci>w</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
        </list>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <ci>w</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{w\in\mathbb{R}^{d}}\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\langle w,x_{i}%
\rangle)^{2}+\|w\|_{1},
  </annotation>
 </semantics>
</math>

 we consider our objective function in two parts: a convex, differentiable term 

<math display="inline" id="Proximal_gradient_methods_for_learning:44">
 <semantics>
  <mrow>
   <mrow>
    <mi>F</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>w</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mi>n</mi>
    </mfrac>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </msubsup>
     <msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <msub>
         <mi>y</mi>
         <mi>i</mi>
        </msub>
        <mo>-</mo>
        <mrow>
         <mo stretchy="false">⟨</mo>
         <mi>w</mi>
         <mo>,</mo>
         <msub>
          <mi>x</mi>
          <mi>i</mi>
         </msub>
         <mo stretchy="false">⟩</mo>
        </mrow>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mn>2</mn>
     </msup>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>F</ci>
     <ci>w</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>n</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>i</ci>
        </apply>
        <list>
         <ci>w</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
        </list>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F(w)=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\langle w,x_{i}\rangle)^{2}
  </annotation>
 </semantics>
</math>

 and a convex function 

<math display="inline" id="Proximal_gradient_methods_for_learning:45">
 <semantics>
  <mrow>
   <mrow>
    <mi>R</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>w</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msub>
    <mrow>
     <mo>∥</mo>
     <mi>w</mi>
     <mo>∥</mo>
    </mrow>
    <mn>1</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>R</ci>
     <ci>w</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <ci>w</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R(w)=\|w\|_{1}
  </annotation>
 </semantics>
</math>

. Note that 

<math display="inline" id="Proximal_gradient_methods_for_learning:46">
 <semantics>
  <mi>R</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>R</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R
  </annotation>
 </semantics>
</math>

 is not strictly convex.</p>

<p>Let us compute the proximity operator for 

<math display="inline" id="Proximal_gradient_methods_for_learning:47">
 <semantics>
  <mrow>
   <mi>R</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>w</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>R</ci>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R(w)
  </annotation>
 </semantics>
</math>

. First we find an alternative characterization of the proximity operator 

<math display="inline" id="Proximal_gradient_methods_for_learning:48">
 <semantics>
  <mrow>
   <msub>
    <mo>prox</mo>
    <mi>R</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>prox</ci>
     <ci>R</ci>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{prox}_{R}(x)
  </annotation>
 </semantics>
</math>

 as follows:</p>

<p>

<math display="inline" id="Proximal_gradient_methods_for_learning:49">
 <semantics>
  <mtable>
   <mtr>
    <mtd columnalign="right">
     <mrow>
      <mrow>
       <mi>u</mi>
       <mo>=</mo>
       <mrow>
        <msub>
         <mo>prox</mo>
         <mi>R</mi>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo>⇔</mo>
      <mi></mi>
     </mrow>
    </mtd>
    <mtd columnalign="left">
     <mrow>
      <mn>0</mn>
      <mo>∈</mo>
      <mrow>
       <mo>∂</mo>
       <mrow>
        <mo>(</mo>
        <mrow>
         <mrow>
          <mi>R</mi>
          <mrow>
           <mo stretchy="false">(</mo>
           <mi>u</mi>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
         <mo>+</mo>
         <mrow>
          <mstyle displaystyle="true">
           <mfrac>
            <mn>1</mn>
            <mn>2</mn>
           </mfrac>
          </mstyle>
          <msubsup>
           <mrow>
            <mo>∥</mo>
            <mrow>
             <mi>u</mi>
             <mo>-</mo>
             <mi>x</mi>
            </mrow>
            <mo>∥</mo>
           </mrow>
           <mn>2</mn>
           <mn>2</mn>
          </msubsup>
         </mrow>
        </mrow>
        <mo>)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mtd>
   </mtr>
   <mtr>
    <mtd columnalign="right">
     <mo>⇔</mo>
    </mtd>
    <mtd columnalign="left">
     <mrow>
      <mn>0</mn>
      <mo>∈</mo>
      <mrow>
       <mrow>
        <mrow>
         <mrow>
          <mo>∂</mo>
          <mi>R</mi>
         </mrow>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>u</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>+</mo>
        <mi>u</mi>
       </mrow>
       <mo>-</mo>
       <mi>x</mi>
      </mrow>
     </mrow>
    </mtd>
   </mtr>
   <mtr>
    <mtd columnalign="right">
     <mo>⇔</mo>
    </mtd>
    <mtd columnalign="left">
     <mrow>
      <mrow>
       <mrow>
        <mi>x</mi>
        <mo>-</mo>
        <mi>u</mi>
       </mrow>
       <mo>∈</mo>
       <mrow>
        <mrow>
         <mo>∂</mo>
         <mi>R</mi>
        </mrow>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>u</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo>.</mo>
     </mrow>
    </mtd>
   </mtr>
  </mtable>
  <annotation-xml encoding="MathML-Content">
   <matrix>
    <matrixrow>
     <apply>
      <csymbol cd="latexml">iff</csymbol>
      <apply>
       <eq></eq>
       <ci>u</ci>
       <apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>prox</ci>
         <ci>R</ci>
        </apply>
        <ci>x</ci>
       </apply>
      </apply>
      <csymbol cd="latexml">absent</csymbol>
     </apply>
     <apply>
      <in></in>
      <cn type="integer">0</cn>
      <apply>
       <partialdiff></partialdiff>
       <apply>
        <plus></plus>
        <apply>
         <times></times>
         <ci>R</ci>
         <ci>u</ci>
        </apply>
        <apply>
         <times></times>
         <apply>
          <divide></divide>
          <cn type="integer">1</cn>
          <cn type="integer">2</cn>
         </apply>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <apply>
            <csymbol cd="latexml">norm</csymbol>
            <apply>
             <minus></minus>
             <ci>u</ci>
             <ci>x</ci>
            </apply>
           </apply>
           <cn type="integer">2</cn>
          </apply>
          <cn type="integer">2</cn>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </matrixrow>
    <matrixrow>
     <csymbol cd="latexml">iff</csymbol>
     <apply>
      <in></in>
      <cn type="integer">0</cn>
      <apply>
       <minus></minus>
       <apply>
        <plus></plus>
        <apply>
         <times></times>
         <apply>
          <partialdiff></partialdiff>
          <ci>R</ci>
         </apply>
         <ci>u</ci>
        </apply>
        <ci>u</ci>
       </apply>
       <ci>x</ci>
      </apply>
     </apply>
    </matrixrow>
    <matrixrow>
     <csymbol cd="latexml">iff</csymbol>
     <apply>
      <in></in>
      <apply>
       <minus></minus>
       <ci>x</ci>
       <ci>u</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <partialdiff></partialdiff>
        <ci>R</ci>
       </apply>
       <ci>u</ci>
      </apply>
     </apply>
    </matrixrow>
   </matrix>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \begin{aligned}\displaystyle u=\operatorname{prox}_{R}(x)\iff&\displaystyle 0%
\in\partial\left(R(u)+\frac{1}{2}\|u-x\|_{2}^{2}\right)\\
\displaystyle\iff&\displaystyle 0\in\partial R(u)+u-x\\
\displaystyle\iff&\displaystyle x-u\in\partial R(u).\end{aligned}
  </annotation>
 </semantics>
</math>

</p>

<p>For 

<math display="inline" id="Proximal_gradient_methods_for_learning:50">
 <semantics>
  <mrow>
   <mrow>
    <mi>R</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>w</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msub>
    <mrow>
     <mo>∥</mo>
     <mi>w</mi>
     <mo>∥</mo>
    </mrow>
    <mn>1</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>R</ci>
     <ci>w</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <ci>w</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R(w)=\|w\|_{1}
  </annotation>
 </semantics>
</math>

 it is easy to compute 

<math display="inline" id="Proximal_gradient_methods_for_learning:51">
 <semantics>
  <mrow>
   <mrow>
    <mo>∂</mo>
    <mi>R</mi>
   </mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>w</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <partialdiff></partialdiff>
     <ci>R</ci>
    </apply>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \partial R(w)
  </annotation>
 </semantics>
</math>

: the 

<math display="inline" id="Proximal_gradient_methods_for_learning:52">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

th entry of 

<math display="inline" id="Proximal_gradient_methods_for_learning:53">
 <semantics>
  <mrow>
   <mrow>
    <mo>∂</mo>
    <mi>R</mi>
   </mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>w</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <partialdiff></partialdiff>
     <ci>R</ci>
    </apply>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \partial R(w)
  </annotation>
 </semantics>
</math>

 is precisely</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:54">
 <semantics>
  <mrow>
   <mrow>
    <mo>∂</mo>
    <mrow>
     <mo stretchy="false">|</mo>
     <msub>
      <mi>w</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">|</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>{</mo>
    <mtable displaystyle="true">
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <mn>1</mn>
        <mo>,</mo>
       </mrow>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mi>w</mi>
        <msub>
         <mo>;</mo>
         <mi>i</mi>
        </msub>
        <mo>></mo>
        <mn>0</mn>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mo>-</mo>
         <mn>1</mn>
        </mrow>
        <mo>,</mo>
       </mrow>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mi>w</mi>
        <msub>
         <mo>;</mo>
         <mi>i</mi>
        </msub>
        <mo><</mo>
        <mn>0</mn>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mo>[</mo>
         <mrow>
          <mo>-</mo>
          <mn>1</mn>
         </mrow>
         <mo>,</mo>
         <mn>1</mn>
         <mo>]</mo>
        </mrow>
        <mo>,</mo>
       </mrow>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mi>w</mi>
        <msub>
         <mo>;</mo>
         <mi>i</mi>
        </msub>
        <mo>=</mo>
        <mn>0.</mn>
       </mrow>
      </mtd>
     </mtr>
    </mtable>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <partialdiff></partialdiff>
     <apply>
      <abs></abs>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <csymbol cd="latexml">cases</csymbol>
     <cn type="integer">1</cn>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">w</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-;</ci>
       <ci>i</ci>
      </apply>
      <gt></gt>
      <cn type="integer">0</cn>
     </cerror>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
     </apply>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">w</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-;</ci>
       <ci>i</ci>
      </apply>
      <lt></lt>
      <cn type="integer">0</cn>
     </cerror>
     <interval closure="closed">
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
      </apply>
      <cn type="integer">1</cn>
     </interval>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">w</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-;</ci>
       <ci>i</ci>
      </apply>
      <eq></eq>
      <cn type="float">0.</cn>
     </cerror>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \partial|w_{i}|=\begin{cases}1,&w;_{i}>0\\
-1,&w;_{i}<0\\
\left[-1,1\right],&w;_{i}=0.\end{cases}
  </annotation>
 </semantics>
</math>

</p>

<p>Using the recharacterization of the proximity operator given above, for the choice of 

<math display="inline" id="Proximal_gradient_methods_for_learning:55">
 <semantics>
  <mrow>
   <mrow>
    <mi>R</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>w</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msub>
    <mrow>
     <mo>∥</mo>
     <mi>w</mi>
     <mo>∥</mo>
    </mrow>
    <mn>1</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>R</ci>
     <ci>w</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <ci>w</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R(w)=\|w\|_{1}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Proximal_gradient_methods_for_learning:56">
 <semantics>
  <mrow>
   <mi>γ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>γ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma>0
  </annotation>
 </semantics>
</math>


 we have that 

<math display="inline" id="Proximal_gradient_methods_for_learning:57">
 <semantics>
  <mrow>
   <msub>
    <mo>prox</mo>
    <mrow>
     <mi>γ</mi>
     <mi>R</mi>
    </mrow>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>prox</ci>
     <apply>
      <times></times>
      <ci>γ</ci>
      <ci>R</ci>
     </apply>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{prox}_{\gamma R}(x)
  </annotation>
 </semantics>
</math>

 is defined entrywise by</p>
<dl>
<dd><dl>
<dd><math>\left(\operatorname{prox}_{\gamma R}(x)\right)_i = \begin{cases}
</math></dd>
</dl>
</dd>
</dl>

<p>x_i-\gamma,&amp;x;_i&gt;\gamma\\ 0,&amp;|x_i|\leq\gamma\\ x_i+\gamma,&amp;x;_i</p>

<p>which is known as the <a href="Thresholding_(image_processing)" title="wikilink">soft thresholding</a> operator 

<math display="inline" id="Proximal_gradient_methods_for_learning:58">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>S</mi>
     <mi>γ</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mo>prox</mo>
     <mrow>
      <mi>γ</mi>
      <mo>∥</mo>
      <mo>⋅</mo>
      <msub>
       <mo>∥</mo>
       <mn>1</mn>
      </msub>
     </mrow>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>S</ci>
      <ci>γ</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>prox</ci>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <csymbol cd="unknown">γ</csymbol>
       <csymbol cd="latexml">parallel-to</csymbol>
       <ci>normal-⋅</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <csymbol cd="latexml">parallel-to</csymbol>
        <cn type="integer">1</cn>
       </apply>
      </cerror>
     </apply>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S_{\gamma}(x)=\operatorname{prox}_{\gamma\|\cdot\|_{1}}(x)
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></p>
<h3 id="fixed-point-iterative-schemes">Fixed point iterative schemes</h3>

<p>To finally solve the lasso problem we consider the fixed point equation shown earlier:</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:59">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>x</mi>
     <mo>*</mo>
    </msup>
    <mo>=</mo>
    <mrow>
     <msub>
      <mo>prox</mo>
      <mrow>
       <mi>γ</mi>
       <mi>R</mi>
      </mrow>
     </msub>
     <mrow>
      <mo>(</mo>
      <mrow>
       <msup>
        <mi>x</mi>
        <mo>*</mo>
       </msup>
       <mo>-</mo>
       <mrow>
        <mi>γ</mi>
        <mrow>
         <mo>∇</mo>
         <mi>F</mi>
        </mrow>
        <mrow>
         <mo stretchy="false">(</mo>
         <msup>
          <mi>x</mi>
          <mo>*</mo>
         </msup>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>x</ci>
     <times></times>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>prox</ci>
      <apply>
       <times></times>
       <ci>γ</ci>
       <ci>R</ci>
      </apply>
     </apply>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>x</ci>
       <times></times>
      </apply>
      <apply>
       <times></times>
       <ci>γ</ci>
       <apply>
        <ci>normal-∇</ci>
        <ci>F</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>x</ci>
        <times></times>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x^{*}=\operatorname{prox}_{\gamma R}\left(x^{*}-\gamma\nabla F(x^{*})\right).
  </annotation>
 </semantics>
</math>

</p>

<p>Given that we have computed the form of the proximity operator explicitly, then we can define a standard fixed point iteration procedure. Namely, fix some initial 

<math display="inline" id="Proximal_gradient_methods_for_learning:60">
 <semantics>
  <mrow>
   <msup>
    <mi>w</mi>
    <mn>0</mn>
   </msup>
   <mo>∈</mo>
   <msup>
    <mi>ℝ</mi>
    <mi>d</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>w</ci>
     <cn type="integer">0</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>ℝ</ci>
     <ci>d</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w^{0}\in\mathbb{R}^{d}
  </annotation>
 </semantics>
</math>

, and for 

<math display="inline" id="Proximal_gradient_methods_for_learning:61">
 <semantics>
  <mrow>
   <mi>k</mi>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>,</mo>
    <mn>2</mn>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>k</ci>
    <list>
     <cn type="integer">1</cn>
     <cn type="integer">2</cn>
     <ci>normal-…</ci>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k=1,2,\ldots
  </annotation>
 </semantics>
</math>


 define</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:62">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>w</mi>
     <mrow>
      <mi>k</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
    </msup>
    <mo>=</mo>
    <mrow>
     <msub>
      <mi>S</mi>
      <mi>γ</mi>
     </msub>
     <mrow>
      <mo>(</mo>
      <mrow>
       <msup>
        <mi>w</mi>
        <mi>k</mi>
       </msup>
       <mo>-</mo>
       <mrow>
        <mi>γ</mi>
        <mrow>
         <mo>∇</mo>
         <mi>F</mi>
        </mrow>
        <mrow>
         <mo>(</mo>
         <msup>
          <mi>w</mi>
          <mi>k</mi>
         </msup>
         <mo>)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>w</ci>
     <apply>
      <plus></plus>
      <ci>k</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>S</ci>
      <ci>γ</ci>
     </apply>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>w</ci>
       <ci>k</ci>
      </apply>
      <apply>
       <times></times>
       <ci>γ</ci>
       <apply>
        <ci>normal-∇</ci>
        <ci>F</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>w</ci>
        <ci>k</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w^{k+1}=S_{\gamma}\left(w^{k}-\gamma\nabla F\left(w^{k}\right)\right).
  </annotation>
 </semantics>
</math>

 Note here the effective trade-off between the empirical error term 

<math display="inline" id="Proximal_gradient_methods_for_learning:63">
 <semantics>
  <mrow>
   <mi>F</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>w</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>F</ci>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F(w)
  </annotation>
 </semantics>
</math>

 and the regularization penalty 

<math display="inline" id="Proximal_gradient_methods_for_learning:64">
 <semantics>
  <mrow>
   <mi>R</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>w</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>R</ci>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R(w)
  </annotation>
 </semantics>
</math>

. This fixed point method has decoupled the effect of the two different convex functions which comprise the objective function into a gradient descent step (

<math display="inline" id="Proximal_gradient_methods_for_learning:65">
 <semantics>
  <mrow>
   <msup>
    <mi>w</mi>
    <mi>k</mi>
   </msup>
   <mo>-</mo>
   <mrow>
    <mi>γ</mi>
    <mrow>
     <mo>∇</mo>
     <mi>F</mi>
    </mrow>
    <mrow>
     <mo>(</mo>
     <msup>
      <mi>w</mi>
      <mi>k</mi>
     </msup>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>w</ci>
     <ci>k</ci>
    </apply>
    <apply>
     <times></times>
     <ci>γ</ci>
     <apply>
      <ci>normal-∇</ci>
      <ci>F</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>w</ci>
      <ci>k</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w^{k}-\gamma\nabla F\left(w^{k}\right)
  </annotation>
 </semantics>
</math>

) and a soft thresholding step (via 

<math display="inline" id="Proximal_gradient_methods_for_learning:66">
 <semantics>
  <msub>
   <mi>S</mi>
   <mi>γ</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>S</ci>
    <ci>γ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S_{\gamma}
  </annotation>
 </semantics>
</math>


).</p>

<p>Convergence of this fixed point scheme is well-studied in the literature<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a><a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> and is guaranteed under appropriate choice of step size 

<math display="inline" id="Proximal_gradient_methods_for_learning:67">
 <semantics>
  <mi>γ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>γ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma
  </annotation>
 </semantics>
</math>

 and loss function (such as the square loss taken here). <a href="Gradient_descent#Extensions" title="wikilink">Accelerated methods</a> were introduced by Nesterov in 1983 which improve the rate of convergence under certain regularity assumptions on 

<math display="inline" id="Proximal_gradient_methods_for_learning:68">
 <semantics>
  <mi>F</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>F</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> Such methods have been studied extensively in previous years.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> For more general learning problems where the proximity operator cannot be computed explicitly for some regularization term 

<math display="inline" id="Proximal_gradient_methods_for_learning:69">
 <semantics>
  <mi>R</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>R</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R
  </annotation>
 </semantics>
</math>

, such fixed point schemes can still be carried out using approximations to both the gradient and the proximity operator.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a><a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a></p>
<h2 id="practical-considerations">Practical considerations</h2>

<p>There have been numerous developments within the past decade in <a href="convex_optimization" title="wikilink">convex optimization</a> techniques which have influenced the application of proximal gradient methods in statistical learning theory. Here we survey a few important topics which can greatly improve practical algorithmic performance of these methods.<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a><a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a></p>
<h3 id="adaptive-step-size">Adaptive step size</h3>

<p>In the fixed point iteration scheme</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:70">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>w</mi>
     <mrow>
      <mi>k</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
    </msup>
    <mo>=</mo>
    <mrow>
     <msub>
      <mo>prox</mo>
      <mrow>
       <mi>γ</mi>
       <mi>R</mi>
      </mrow>
     </msub>
     <mrow>
      <mo>(</mo>
      <mrow>
       <msup>
        <mi>w</mi>
        <mi>k</mi>
       </msup>
       <mo>-</mo>
       <mrow>
        <mi>γ</mi>
        <mrow>
         <mo>∇</mo>
         <mi>F</mi>
        </mrow>
        <mrow>
         <mo>(</mo>
         <msup>
          <mi>w</mi>
          <mi>k</mi>
         </msup>
         <mo>)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>w</ci>
     <apply>
      <plus></plus>
      <ci>k</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>prox</ci>
      <apply>
       <times></times>
       <ci>γ</ci>
       <ci>R</ci>
      </apply>
     </apply>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>w</ci>
       <ci>k</ci>
      </apply>
      <apply>
       <times></times>
       <ci>γ</ci>
       <apply>
        <ci>normal-∇</ci>
        <ci>F</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>w</ci>
        <ci>k</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w^{k+1}=\operatorname{prox}_{\gamma R}\left(w^{k}-\gamma\nabla F\left(w^{k}%
\right)\right),
  </annotation>
 </semantics>
</math>

 one can allow variable step size 

<math display="inline" id="Proximal_gradient_methods_for_learning:71">
 <semantics>
  <msub>
   <mi>γ</mi>
   <mi>k</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>γ</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma_{k}
  </annotation>
 </semantics>
</math>


 instead of a constant 

<math display="inline" id="Proximal_gradient_methods_for_learning:72">
 <semantics>
  <mi>γ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>γ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma
  </annotation>
 </semantics>
</math>

. Numerous adaptive step size schemes have been proposed throughout the literature.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a><a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a><a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a><a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> Applications of these schemes<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a><a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a> suggest that these can offer substantial improvement in number of iterations required for fixed point convergence.</p>
<h3 id="elastic-net-mixed-norm-regularization">Elastic net (mixed norm regularization)</h3>

<p><a href="Elastic_net_regularization" title="wikilink">Elastic net regularization</a> offers an alternative to pure 

<math display="inline" id="Proximal_gradient_methods_for_learning:73">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{1}
  </annotation>
 </semantics>
</math>

 regularization. The problem of lasso (

<math display="inline" id="Proximal_gradient_methods_for_learning:74">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{1}
  </annotation>
 </semantics>
</math>

) regularization involves the penalty term 

<math display="inline" id="Proximal_gradient_methods_for_learning:75">
 <semantics>
  <mrow>
   <mrow>
    <mi>R</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>w</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msub>
    <mrow>
     <mo>∥</mo>
     <mi>w</mi>
     <mo>∥</mo>
    </mrow>
    <mn>1</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>R</ci>
     <ci>w</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <ci>w</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R(w)=\|w\|_{1}
  </annotation>
 </semantics>
</math>

, which is not strictly convex. Hence, solutions to 

<math display="inline" id="Proximal_gradient_methods_for_learning:76">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <msub>
       <mi>min</mi>
       <mi>w</mi>
      </msub>
      <mi>F</mi>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>w</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>R</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>w</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <min></min>
       <ci>w</ci>
      </apply>
      <ci>F</ci>
     </apply>
     <ci>w</ci>
    </apply>
    <apply>
     <times></times>
     <ci>R</ci>
     <ci>w</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{w}F(w)+R(w),
  </annotation>
 </semantics>
</math>


 where 

<math display="inline" id="Proximal_gradient_methods_for_learning:77">
 <semantics>
  <mi>F</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>F</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F
  </annotation>
 </semantics>
</math>

 is some empirical loss function, need not be unique. This is often avoided by the inclusion of an additional strictly convex term, such as an 

<math display="inline" id="Proximal_gradient_methods_for_learning:78">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{2}
  </annotation>
 </semantics>
</math>

 norm regularization penalty. For example, one can consider the problem</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:79">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <munder>
       <mi>min</mi>
       <mrow>
        <mi>w</mi>
        <mo>∈</mo>
        <msup>
         <mi>ℝ</mi>
         <mi>d</mi>
        </msup>
       </mrow>
      </munder>
      <mfrac>
       <mn>1</mn>
       <mi>n</mi>
      </mfrac>
     </mrow>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>n</mi>
      </munderover>
      <msup>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <msub>
          <mi>y</mi>
          <mi>i</mi>
         </msub>
         <mo>-</mo>
         <mrow>
          <mo stretchy="false">⟨</mo>
          <mi>w</mi>
          <mo>,</mo>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo stretchy="false">⟩</mo>
         </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
       <mn>2</mn>
      </msup>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>λ</mi>
     <mrow>
      <mo>(</mo>
      <mrow>
       <mrow>
        <mrow>
         <mo stretchy="false">(</mo>
         <mrow>
          <mn>1</mn>
          <mo>-</mo>
          <mi>μ</mi>
         </mrow>
         <mo stretchy="false">)</mo>
        </mrow>
        <msub>
         <mrow>
          <mo>∥</mo>
          <mi>w</mi>
          <mo>∥</mo>
         </mrow>
         <mn>1</mn>
        </msub>
       </mrow>
       <mo>+</mo>
       <mrow>
        <mi>μ</mi>
        <msubsup>
         <mrow>
          <mo>∥</mo>
          <mi>w</mi>
          <mo>∥</mo>
         </mrow>
         <mn>2</mn>
         <mn>2</mn>
        </msubsup>
       </mrow>
      </mrow>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <min></min>
       <apply>
        <in></in>
        <ci>w</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>ℝ</ci>
         <ci>d</ci>
        </apply>
       </apply>
      </apply>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>n</ci>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>i</ci>
        </apply>
        <list>
         <ci>w</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
        </list>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>λ</ci>
     <apply>
      <plus></plus>
      <apply>
       <times></times>
       <apply>
        <minus></minus>
        <cn type="integer">1</cn>
        <ci>μ</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <csymbol cd="latexml">norm</csymbol>
         <ci>w</ci>
        </apply>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <apply>
       <times></times>
       <ci>μ</ci>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <apply>
          <csymbol cd="latexml">norm</csymbol>
          <ci>w</ci>
         </apply>
         <cn type="integer">2</cn>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{w\in\mathbb{R}^{d}}\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\langle w,x_{i}%
\rangle)^{2}+\lambda\left((1-\mu)\|w\|_{1}+\mu\|w\|_{2}^{2}\right),
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Proximal_gradient_methods_for_learning:80">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>x</mi>
     <mi>i</mi>
    </msub>
    <mo>∈</mo>
    <mrow>
     <msup>
      <mi>ℝ</mi>
      <mi>d</mi>
     </msup>
     <mtext>and</mtext>
     <msub>
      <mi>y</mi>
      <mi>i</mi>
     </msub>
    </mrow>
    <mo>∈</mo>
    <mi>ℝ</mi>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <in></in>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>ℝ</ci>
       <ci>d</ci>
      </apply>
      <mtext>and</mtext>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <in></in>
     <share href="#.cmml">
     </share>
     <ci>ℝ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}\in\mathbb{R}^{d}\text{ and }y_{i}\in\mathbb{R}.
  </annotation>
 </semantics>
</math>

 For 

<math display="inline" id="Proximal_gradient_methods_for_learning:81">
 <semantics>
  <mrow>
   <mn>0</mn>
   <mo><</mo>
   <mi>μ</mi>
   <mo>≤</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <lt></lt>
     <cn type="integer">0</cn>
     <ci>μ</ci>
    </apply>
    <apply>
     <leq></leq>
     <share href="#.cmml">
     </share>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   0<\mu\leq 1
  </annotation>
 </semantics>
</math>


 the penalty term 

<math display="inline" id="Proximal_gradient_methods_for_learning:82">
 <semantics>
  <mrow>
   <mi>λ</mi>
   <mrow>
    <mo>(</mo>
    <mrow>
     <mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mn>1</mn>
        <mo>-</mo>
        <mi>μ</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <msub>
       <mrow>
        <mo>∥</mo>
        <mi>w</mi>
        <mo>∥</mo>
       </mrow>
       <mn>1</mn>
      </msub>
     </mrow>
     <mo>+</mo>
     <mrow>
      <mi>μ</mi>
      <msubsup>
       <mrow>
        <mo>∥</mo>
        <mi>w</mi>
        <mo>∥</mo>
       </mrow>
       <mn>2</mn>
       <mn>2</mn>
      </msubsup>
     </mrow>
    </mrow>
    <mo>)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>λ</ci>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <ci>μ</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="latexml">norm</csymbol>
        <ci>w</ci>
       </apply>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>μ</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <apply>
         <csymbol cd="latexml">norm</csymbol>
         <ci>w</ci>
        </apply>
        <cn type="integer">2</cn>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda\left((1-\mu)\|w\|_{1}+\mu\|w\|_{2}^{2}\right)
  </annotation>
 </semantics>
</math>

 is now strictly convex, and hence the minimization problem now admits a unique solution. It has been observed that for sufficiently small 

<math display="inline" id="Proximal_gradient_methods_for_learning:83">
 <semantics>
  <mrow>
   <mi>μ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>μ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu>0
  </annotation>
 </semantics>
</math>

, the additional penalty term 

<math display="inline" id="Proximal_gradient_methods_for_learning:84">
 <semantics>
  <mrow>
   <mi>μ</mi>
   <msubsup>
    <mrow>
     <mo>∥</mo>
     <mi>w</mi>
     <mo>∥</mo>
    </mrow>
    <mn>2</mn>
    <mn>2</mn>
   </msubsup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>μ</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="latexml">norm</csymbol>
       <ci>w</ci>
      </apply>
      <cn type="integer">2</cn>
     </apply>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu\|w\|_{2}^{2}
  </annotation>
 </semantics>
</math>

 acts as a preconditioner and can substantially improve convergence while not adversely affecting the sparsity of solutions.<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a><a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a></p>
<h2 id="exploiting-group-structure">Exploiting group structure</h2>

<p>Proximal gradient methods provide a general framework which is applicable to a wide variety of problems in <a href="statistical_learning_theory" title="wikilink">statistical learning theory</a>. Certain problems in learning can often involve data which has additional structure that is known '' a priori''. In the past several years there have been new developments which incorporate information about group structure to provide methods which are tailored to different applications. Here we survey a few such methods.</p>
<h3 id="group-lasso">Group lasso</h3>

<p>Group lasso is a generalization of the <a href="#Lasso_regularization" title="wikilink">lasso method</a> when features are grouped into disjoint blocks.<a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a> Suppose the features are grouped into blocks 

<math display="inline" id="Proximal_gradient_methods_for_learning:85">
 <semantics>
  <mrow>
   <mo stretchy="false">{</mo>
   <msub>
    <mi>w</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>w</mi>
    <mi>G</mi>
   </msub>
   <mo stretchy="false">}</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <set>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <ci>G</ci>
    </apply>
   </set>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{w_{1},\ldots,w_{G}\}
  </annotation>
 </semantics>
</math>

. Here we take as a regularization penalty</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:86">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>R</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>w</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>g</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>G</mi>
     </munderover>
     <msub>
      <mrow>
       <mo>∥</mo>
       <msub>
        <mi>w</mi>
        <mi>g</mi>
       </msub>
       <mo>∥</mo>
      </mrow>
      <mn>2</mn>
     </msub>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>R</ci>
     <ci>w</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>g</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>G</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="latexml">norm</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <ci>g</ci>
       </apply>
      </apply>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R(w)=\sum_{g=1}^{G}\|w_{g}\|_{2},
  </annotation>
 </semantics>
</math>

</p>

<p>which is the sum of the 

<math display="inline" id="Proximal_gradient_methods_for_learning:87">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{2}
  </annotation>
 </semantics>
</math>

 norm on corresponding feature vectors for the different groups. A similar proximity operator analysis as above can be used to compute the proximity operator for this penalty. Where the lasso penalty has a proximity operator which is soft thresholding on each individual component, the proximity operator for the group lasso is soft thresholding on each group. For the group 

<math display="inline" id="Proximal_gradient_methods_for_learning:88">
 <semantics>
  <msub>
   <mi>w</mi>
   <mi>g</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>g</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{g}
  </annotation>
 </semantics>
</math>

 we have that proximity operator of 

<math display="inline" id="Proximal_gradient_methods_for_learning:89">
 <semantics>
  <mrow>
   <mi>λ</mi>
   <mi>γ</mi>
   <mrow>
    <mo>(</mo>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mi>g</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>G</mi>
     </msubsup>
     <msub>
      <mrow>
       <mo>∥</mo>
       <msub>
        <mi>w</mi>
        <mi>g</mi>
       </msub>
       <mo>∥</mo>
      </mrow>
      <mn>2</mn>
     </msub>
    </mrow>
    <mo>)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>λ</ci>
    <ci>γ</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>g</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>G</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="latexml">norm</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <ci>g</ci>
       </apply>
      </apply>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda\gamma\left(\sum_{g=1}^{G}\|w_{g}\|_{2}\right)
  </annotation>
 </semantics>
</math>

 is given by</p>

<p>

<math display="block" id="Proximal_gradient_methods_for_learning:90">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mover accent="true">
      <mi>S</mi>
      <mo>~</mo>
     </mover>
     <mrow>
      <mi>λ</mi>
      <mi>γ</mi>
     </mrow>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>w</mi>
      <mi>g</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>{</mo>
    <mtable displaystyle="true">
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <msub>
          <mi>w</mi>
          <mi>g</mi>
         </msub>
         <mo>-</mo>
         <mrow>
          <mi>λ</mi>
          <mi>γ</mi>
          <mstyle displaystyle="false">
           <mfrac>
            <msub>
             <mi>w</mi>
             <mi>g</mi>
            </msub>
            <msub>
             <mrow>
              <mo>∥</mo>
              <msub>
               <mi>w</mi>
               <mi>g</mi>
              </msub>
              <mo>∥</mo>
             </mrow>
             <mn>2</mn>
            </msub>
           </mfrac>
          </mstyle>
         </mrow>
        </mrow>
        <mo>,</mo>
       </mrow>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <msub>
         <mrow>
          <mo>∥</mo>
          <msub>
           <mi>w</mi>
           <mi>g</mi>
          </msub>
          <mo>∥</mo>
         </mrow>
         <mn>2</mn>
        </msub>
        <mo>></mo>
        <mrow>
         <mi>λ</mi>
         <mi>γ</mi>
        </mrow>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <mn>0</mn>
        <mo>,</mo>
       </mrow>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <msub>
         <mrow>
          <mo>∥</mo>
          <msub>
           <mi>w</mi>
           <mi>g</mi>
          </msub>
          <mo>∥</mo>
         </mrow>
         <mn>2</mn>
        </msub>
        <mo>≤</mo>
        <mrow>
         <mi>λ</mi>
         <mi>γ</mi>
        </mrow>
       </mrow>
      </mtd>
     </mtr>
    </mtable>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <ci>normal-~</ci>
       <ci>S</ci>
      </apply>
      <apply>
       <times></times>
       <ci>λ</ci>
       <ci>γ</ci>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>g</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="latexml">cases</csymbol>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <ci>g</ci>
      </apply>
      <apply>
       <times></times>
       <ci>λ</ci>
       <ci>γ</ci>
       <apply>
        <divide></divide>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>w</ci>
         <ci>g</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <apply>
          <csymbol cd="latexml">norm</csymbol>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>w</ci>
           <ci>g</ci>
          </apply>
         </apply>
         <cn type="integer">2</cn>
        </apply>
       </apply>
      </apply>
     </apply>
     <apply>
      <gt></gt>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="latexml">norm</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>w</ci>
         <ci>g</ci>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <times></times>
       <ci>λ</ci>
       <ci>γ</ci>
      </apply>
     </apply>
     <cn type="integer">0</cn>
     <apply>
      <leq></leq>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="latexml">norm</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>w</ci>
         <ci>g</ci>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <times></times>
       <ci>λ</ci>
       <ci>γ</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \widetilde{S}_{\lambda\gamma}(w_{g})=\begin{cases}w_{g}-\lambda\gamma\frac{w_{%
g}}{\|w_{g}\|_{2}},&\|w_{g}\|_{2}>\lambda\gamma\\
0,&\|w_{g}\|_{2}\leq\lambda\gamma\end{cases}
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Proximal_gradient_methods_for_learning:91">
 <semantics>
  <msub>
   <mi>w</mi>
   <mi>g</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>g</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{g}
  </annotation>
 </semantics>
</math>

 is the 

<math display="inline" id="Proximal_gradient_methods_for_learning:92">
 <semantics>
  <mi>g</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>g</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g
  </annotation>
 </semantics>
</math>

th group.</p>

<p>In contrast to lasso, the derivation of the proximity operator for group lasso relies on the <a href="#Moreau_decomposition" title="wikilink">Moreau decomposition</a>. Here the proximity operator of the conjugate of the group lasso penalty becomes a projection onto the <a href="Ball_(mathematics)" title="wikilink">ball</a> of a <a href="dual_norm" title="wikilink">dual norm</a>.<a class="footnoteRef" href="#fn32" id="fnref32"><sup>32</sup></a></p>
<h3 id="other-group-structures">Other group structures</h3>

<p>In contrast to the group lasso problem, where features are grouped into disjoint blocks, it may be the case that grouped features are overlapping or have a nested structure. Such generalizations of group lasso have been considered in a variety of contexts.<a class="footnoteRef" href="#fn33" id="fnref33"><sup>33</sup></a><a class="footnoteRef" href="#fn34" id="fnref34"><sup>34</sup></a><a class="footnoteRef" href="#fn35" id="fnref35"><sup>35</sup></a><a class="footnoteRef" href="#fn36" id="fnref36"><sup>36</sup></a> For overlapping groups one common approach is known as <em>latent group lasso</em> which introduces latent variables to account for overlap.<a class="footnoteRef" href="#fn37" id="fnref37"><sup>37</sup></a><a class="footnoteRef" href="#fn38" id="fnref38"><sup>38</sup></a> Nested group structures are studied in <em>hierarchical structure prediction</em> and with <a href="directed_acyclic_graph" title="wikilink">directed acyclic graphs</a>.<a class="footnoteRef" href="#fn39" id="fnref39"><sup>39</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Proximal_gradient_method" title="wikilink">Proximal gradient method</a></li>
<li><a href="Statistical_learning_theory" title="wikilink">Statistical learning theory</a></li>
<li><a href="Regularization_(mathematics)#Regularization_in_statistics_and_machine_learning" title="wikilink">Regularization</a></li>
<li><a href="Convex_analysis" title="wikilink">Convex analysis</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:First_order_methods" title="wikilink">First order methods</a> <a href="Category:Convex_optimization" title="wikilink">Category:Convex optimization</a> <a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4"></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"></li>
<li id="fn7"></li>
<li id="fn8"></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"></li>
<li id="fn11"></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"></li>
<li id="fn14"><a href="#fnref14">↩</a></li>
<li id="fn15"></li>
<li id="fn16"></li>
<li id="fn17"><mtpl></mtpl><a href="#fnref17">↩</a></li>
<li id="fn18"><a href="#fnref18">↩</a></li>
<li id="fn19"></li>
<li id="fn20"><a href="#fnref20">↩</a></li>
<li id="fn21"></li>
<li id="fn22"><a href="#fnref22">↩</a></li>
<li id="fn23"></li>
<li id="fn24"></li>
<li id="fn25"><mtpl></mtpl><a href="#fnref25">↩</a></li>
<li id="fn26"><a href="#fnref26">↩</a></li>
<li id="fn27"></li>
<li id="fn28"><mtpl></mtpl><a href="#fnref28">↩</a></li>
<li id="fn29"></li>
<li id="fn30"><a href="#fnref30">↩</a></li>
<li id="fn31"><a href="#fnref31">↩</a></li>
<li id="fn32"></li>
<li id="fn33"><a href="#fnref33">↩</a></li>
<li id="fn34"><a href="#fnref34">↩</a></li>
<li id="fn35"><a href="#fnref35">↩</a></li>
<li id="fn36"><a href="#fnref36">↩</a></li>
<li id="fn37"><a href="#fnref37">↩</a></li>
<li id="fn38"><a href="#fnref38">↩</a></li>
<li id="fn39"></li>
</ol>
</section>
</body>
</html>
