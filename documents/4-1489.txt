   Partial least squares regression      Partial least squares regression   Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression ; instead of finding hyperplanes of minimum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares Discriminant Analysis (PLS-DA) is a variant used when the Y is categorical.  PLS is used to find the fundamental relations between two matrices ( X and Y ), i.e. a latent variable approach to modeling the covariance structures in these two spaces. A PLS model will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. PLS regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among X values. By contrast, standard regression will fail in these cases (unless it is regularized ).  The PLS algorithm is employed in partial least squares path modeling , 1 2 a method of modeling a "causal" network of latent variables (causes cannot be determined without experimental or quasi-experimental methods, but one typically bases a latent variable model on the prior theoretical assumption that latent variables cause manifestations in their measured indicators). This technique is a form of structural equation modeling , distinguished from the classical method by being component-based rather than covariance-based. 3  Partial least squares was introduced by the Swedish statistician Herman Wold , who then developed it with his son, Svante Wold. An alternative term for PLS (and more correct according to Svante Wold 4 ) is projection to latent structures , but the term partial least squares is still dominant in many areas. Although the original applications were in the social sciences, PLS regression is today most widely used in chemometrics and related areas. It is also used in bioinformatics, sensometrics, neuroscience and anthropology. In contrast, PLS path modeling is most often used in social sciences, econometrics, marketing and strategic management.  Underlying model  The general underlying model of multivariate PLS is      X  =    T   P  ⊤    +  E       X      T   superscript  P  top    E     X=TP^{\top}+E         Y  =    U   Q  ⊤    +  F       Y      U   superscript  Q  top    F     Y=UQ^{\top}+F     where   X   X   X   is an    n  ×  m      n  m    n\times m   matrix of predictors,   Y   Y   Y   is an    n  ×  p      n  p    n\times p   matrix of responses;   T   T   T   and   U   U   U   are    n  ×  l      n  l    n\times l   matrices that are, respectively, projections of   X   X   X   (the X score , component or factor matrix) and projections of   Y   Y   Y   (the Y scores );   P   P   P   and   Q   Q   Q   are, respectively,    m  ×  l      m  l    m\times l   and    p  ×  l      p  l    p\times l   orthogonal loading matrices; and matrices   E   E   E   and   F   F   F   are the error terms, assumed to be independent and identically distributed random normal variables. The decompositions of   X   X   X   and   Y   Y   Y   are made so as to maximise the covariance between   T   T   T   and   U   U   U   .  Algorithms  A number of variants of PLS exist for estimating the factor and loading matrices    T  ,  U  ,  P     T  U  P    T,U,P   and   Q   Q   Q   . Most of them construct estimates of the linear regression between   X   X   X   and   Y   Y   Y   as    Y  =    X   B  ~    +    B  ~   0        Y      X   normal-~  B     subscript   normal-~  B   0      Y=X\tilde{B}+\tilde{B}_{0}   . Some PLS algorithms are only appropriate for the case where   Y   Y   Y   is a column vector, while others deal with the general case of a matrix   Y   Y   Y   . Algorithms also differ on whether they estimate the factor matrix   T   T   T   as an orthogonal, an orthonormal matrix or not. 5 6 7 8 9 10 The final prediction will be the same for all these varieties of PLS, but the components will differ.  PLS1  PLS1 is a widely used algorithm appropriate for the vector   Y   Y   Y   case. It estimates   T   T   T   as an orthonormal matrix. In pseudocode it is expressed below (capital letters are matrices, lower case letters are vectors if they are superscripted and scalars if they are subscripted):  1 function PLS1(     X  ,  y  ,  l     X  y  l    X,y,l    )  2      X   (  0  )    ←  X     normal-←   superscript  X  0   X    X^{(0)}\leftarrow X     3      w   (  0  )    ←     X  T   y   /   ||    X  T   y   ||       normal-←   superscript  w  0        superscript  X  T   y    norm     superscript  X  T   y       w^{(0)}\leftarrow X^{T}y/||X^{T}y||    , an initial estimate of    w   w   w    .  4      t   (  0  )    ←   X   w   (  0  )        normal-←   superscript  t  0     X   superscript  w  0      t^{(0)}\leftarrow Xw^{(0)}      5 for     k   k   k    = 0 to     l   l   l     6      t  k   ←    t    (  k  )    T    t   (  k  )        normal-←   subscript  t  k      superscript   superscript  t  k   T    superscript  t  k      t_{k}\leftarrow{t^{(k)}}^{T}t^{(k)}    (note this is a scalar)  7      t   (  k  )    ←    t   (  k  )    /   t  k       normal-←   superscript  t  k      superscript  t  k    subscript  t  k      t^{(k)}\leftarrow t^{(k)}/t_{k}     8      p   (  k  )    ←    X    (  k  )    T    t   (  k  )        normal-←   superscript  p  k      superscript   superscript  X  k   T    superscript  t  k      p^{(k)}\leftarrow{X^{(k)}}^{T}t^{(k)}     9      q  k   ←    y  T    t   (  k  )        normal-←   subscript  q  k      superscript  y  T    superscript  t  k      q_{k}\leftarrow{y}^{T}t^{(k)}    (note this is a scalar)  10 if      q  k     subscript  q  k    q_{k}    = 0  11     l  ←  k     normal-←  l  k    l\leftarrow k    , break the for  loop  12 if      k  <  l      k  l    k     13      X   (   k  +  1   )    ←    X   (  k  )    -    t  k    t   (  k  )     p    (  k  )    T        normal-←   superscript  X    k  1       superscript  X  k      subscript  t  k    superscript  t  k    superscript   superscript  p  k   T       X^{(k+1)}\leftarrow X^{(k)}-t_{k}t^{(k)}{p^{(k)}}^{T}     14      w   (   k  +  1   )    ←    X    (   k  +  1   )    T   y      normal-←   superscript  w    k  1       superscript   superscript  X    k  1    T   y     w^{(k+1)}\leftarrow{X^{(k+1)}}^{T}y     15      t   (   k  +  1   )    ←    X   (   k  +  1   )     w   (   k  +  1   )        normal-←   superscript  t    k  1       superscript  X    k  1     superscript  w    k  1       t^{(k+1)}\leftarrow X^{(k+1)}w^{(k+1)}     16  end for  17 define     W   W   W    to be the matrix with columns      w   (  0  )    ,   w   (  1  )    ,  …  ,   w   (   l  -  1   )        superscript  w  0    superscript  w  1   normal-…   superscript  w    l  1      w^{(0)},w^{(1)},...,w^{(l-1)}    .  Do the same to form the    P   P   P    matrix and    q   q   q    vector.  18     B  ←   W    (    P  T   W   )    -  1    q      normal-←  B    W   superscript     superscript  P  T   W     1    q     B\leftarrow W{(P^{T}W)}^{-1}q     19      B  0   ←    q  0   -    P    (  0  )    T   B       normal-←   subscript  B  0      subscript  q  0      superscript   superscript  P  0   T   B      B_{0}\leftarrow q_{0}-{P^{(0)}}^{T}B     20 return      B  ,   B  0      B   subscript  B  0     B,B_{0}     This form of the algorithm does not require centering of the input   X   X   X   and   Y   Y   Y   , as this is performed implicitly by the algorithm. This algorithm features 'deflation' of the matrix   X   X   X   (subtraction of     t  k    t   (  k  )     p    (  k  )    T        subscript  t  k    superscript  t  k    superscript   superscript  p  k   T     t_{k}t^{(k)}{p^{(k)}}^{T}   ), but deflation of the vector   y   y   y   is not performed, as it is not necessary (it can be proved that deflating   y   y   y   yields the same results as not deflating.). The user-supplied variable   l   l   l   is the limit on the number of latent factors in the regression; if it equals the rank of the matrix   X   X   X   , the algorithm will yield the least squares regression estimates for   B   B   B   and    B  0     subscript  B  0    B_{0}     Extensions  In 2002 a new method was published called orthogonal projections to latent structures (OPLS). In OPLS, continuous variable data is separated into predictive and uncorrelated information. This leads to improved diagnostics, as well as more easily interpreted visualization. However, these changes only improve the interpretability, not the predictivity, of the PLS models. 11 L-PLS extends PLS regression to 3 connected data blocks. 12 Similarly, OPLS-DA (Discriminant Analysis) may be applied when working with discrete variables, as in classification and biomarker studies.  Software implementation  Most major statistical software packages offer PLS regression. The 'pls' package in R provides a range of algorithms. 13  See also   Feature extraction  Data mining  Machine learning  Regression analysis  Canonical correlation  Deming regression  Multilinear subspace learning  Principal component analysis  Total sum of squares   Further reading                  Wan Mohamad Asyraf Bin Wan Afthanorhan. (2013). A Comparison Of Partial Least Square Structural Equation Modeling (PLS-SEM) and Covariance Based Structural EquationModeling (CB-SEM) for Confirmatory Factor Analysis International Journal of Engineering Science and Innovative Technology (IJESIT), 2(5), 9.   References  External links   imDEV free Excel add-in for PLS and PLS-DA  PLS in Brain Imaging  on-line PLS regression (PLSR) at Virtual Computational Chemistry Laboratory  Uncertainty estimation for PLS  A short introduction to PLS regression and its history   "  Category:Regression analysis  Category:Latent variable models  Category:Least squares  Category:Articles with example pseudocode     ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩     