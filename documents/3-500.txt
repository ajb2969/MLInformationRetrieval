


Probably approximately correct learning




Probably approximately correct learning

In computational learning theory, probably approximately correct learning (PAC learning) is a framework for mathematical analysis of machine learning. It was proposed in 1984 by Leslie Valiant.1
In this framework, the learner receives samples and must select a generalization function (called the hypothesis) from a certain class of possible functions. The goal is that, with high probability (the "probably" part), the selected function will have low generalization error (the "approximately correct" part). The learner must be able to learn the concept given any arbitrary approximation ratio, probability of success, or distribution of the samples.
The model was later extended to treat noise (misclassified samples).
An important innovation of the PAC framework is the introduction of computational complexity theory concepts to machine learning. In particular, the learner is expected to find efficient functions (time and space requirements bounded to a polynomial of the example size), and the learner itself must implement an efficient procedure (requiring an example count bounded to a polynomial of the concept size, modified by the approximation and likelihood bounds).
Definitions and terminology
In order to give the definition for something that is PAC-learnable, we first have to introduce some terminology.2 3
For the following definitions, two examples will be used. The first is the problem of character recognition given an array of 
 
 
 
  bits encoding a binary-valued image. The other example is the problem of finding an interval that will correctly classify points within the interval as positive and the points outside of the range as negative.
Let 
 
 
 
  be a set called the instance space or the encoding of all the samples, and each instance have length assigned. In the character recognition problem, the instance space is 
 
 
 
 . In the interval problem the instance space is 
 
 
 
 
 , where 
 
 
 
  denotes the set of all real numbers.
A concept is a subset 
 
 
 
 . One concept is the set of all patterns of bits in 
 
 
 
  that encode a picture of the letter "P". An example concept from the second example is the set of all of the numbers between 
 
 
 
  and 
 
 
 
 
 . A concept class

 
  is a set of concepts over 
 
 
 
 . This could be the set of all subsets of the array of bits that are skeletonized 4-connected (width of the font is 1).
Let 
 
 
 
  be a procedure that draws an example, 
 
 
 
 , using a probability distribution 
 
 
 
 
  and gives the correct label 
 
 
 
 , that is 1 if 
 
 
 
  and 0 otherwise.
Say that there is an algorithm 
 
 
 
  that given access to 
 
 
 
  and inputs 
 
 
 
 
  and 
 
 
 
  that, with probability of at least 
 
 
 
 , 
 
 
 
  outputs a hypothesis 
 
 
 
  that has error less than or equal to 
 
 
 
 
  with examples drawn from 
 
 
 
  with the distribution 
 
 
 
 . If there is such an algorithm for every concept 
 
 
 
 , for every distribution 
 
 
 
  over 
 
 
 
 
 , and for all 
 
 
 
  and 
 
 
 
  then 
 
 
 
  is PAC learnable (or distribution-free PAC learnable). We can also say that 
 
 
 
  is a PAC learning algorithm for 
 
 
 
 
 .
An algorithm runs in time 
 
 
 
  if it draws at most 
 
 
 
  examples and requires at most 
 
 
 
  time steps. A concept class is efficiently PAC learnable if it is PAC learnable by an algorithm that runs in time polynomial in 
 
 
 
 , 
 
 
 
 
  and instance length.
Equivalence
Under some regularity conditions these three conditions are equivalent:

The concept class C is PAC learnable.
The VC dimension of C is finite.
C is a uniform Glivenko-Cantelli class.

References


Further reading

M. Kearns, U. Vazirani. An Introduction to Computational Learning Theory. MIT Press, 1994. A textbook.
D. Haussler. Overview of the Probably Approximately Correct (PAC) Learning Framework. An introduction to the topic.
L. Valiant. Probably Approximately Correct. Basic Books, 2013. In which Valiant argues that PAC learning describes how organisms evolve and learn.

"
Category:Computational learning theory



L. Valiant. A theory of the learnable. Communications of the ACM, 27, 1984.↩
Kearns and Vazirani, pg. 1-12,↩
Balas Kausik Natarajan, Machine Learning , A Theoretical Approach, Morgan Kaufmann Publishers, 1991↩




