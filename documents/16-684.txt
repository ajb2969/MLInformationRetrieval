   Uzawa iteration      Uzawa iteration   In numerical mathematics , the Uzawa iteration is an algorithm for solving saddle point problems. It is named after Hirofumi Uzawa and was originally introduced in the context of concave programming. 1  Basic idea  We consider a saddle point problem of the form         (     A    B       B  *         )    (      x  1        x  2      )    =   (      b  1        b  2      )    ,          A  B     superscript  B    absent       subscript  x  1      subscript  x  2         subscript  b  1      subscript  b  2       \begin{pmatrix}A&B\\
 B^{*}&\end{pmatrix}\begin{pmatrix}x_{1}\\
 x_{2}\end{pmatrix}=\begin{pmatrix}b_{1}\\
 b_{2}\end{pmatrix},     where   A   A   A   is a symmetric positive-definite matrix . Multiplying the first row by     B  *    A   -  1         superscript  B     superscript  A    1      B^{*}A^{-1}   and subtracting from the second row yields the upper-triangular system         (     A    B          -  S      )    (      x  1        x  2      )    =   (      b  1         b  2   -    B  *    A   -  1     b  1        )    ,          A  B    absent    S        subscript  x  1      subscript  x  2         subscript  b  1        subscript  b  2      superscript  B     superscript  A    1     subscript  b  1         \begin{pmatrix}A&B\\
 &-S\end{pmatrix}\begin{pmatrix}x_{1}\\
 x_{2}\end{pmatrix}=\begin{pmatrix}b_{1}\\
 b_{2}-B^{*}A^{-1}b_{1}\end{pmatrix},     where    S  :=    B  *    A   -  1    B      assign  S     superscript  B     superscript  A    1    B     S:=B^{*}A^{-1}B   denotes the Schur complement . Since   S   S   S   is symmetric positive-definite, we can apply standard iterative methods like the gradient descent method or the conjugate gradient method to       S   x  2    =     B  *    A   -  1     b  1    -   b  2          S   subscript  x  2         superscript  B     superscript  A    1     subscript  b  1     subscript  b  2      Sx_{2}=B^{*}A^{-1}b_{1}-b_{2}     in order to compute    x  2     subscript  x  2    x_{2}   . The vector    x  1     subscript  x  1    x_{1}   can be reconstructed by solving        A   x  1    =    b  1   -   B   x  2      .        A   subscript  x  1       subscript  b  1     B   subscript  x  2       Ax_{1}=b_{1}-Bx_{2}.\,     It is possible to update    x  1     subscript  x  1    x_{1}   alongside    x  2     subscript  x  2    x_{2}   during the iteration for the Schur complement system and thus obtain an efficient algorithm.  Implementation  We start the conjugate gradient iteration by computing the residual        r  2   :=     B  *    A   -  1     b  1    -   b  2   -   S   x  2     =     B  *    A   -  1     (    b  1   -   B   x  2     )    -   b  2    =     B  *    x  1    -   b  2     ,       assign   subscript  r  2        superscript  B     superscript  A    1     subscript  b  1     subscript  b  2     S   subscript  x  2               superscript  B     superscript  A    1       subscript  b  1     B   subscript  x  2       subscript  b  2              superscript  B     subscript  x  1     subscript  b  2       r_{2}:=B^{*}A^{-1}b_{1}-b_{2}-Sx_{2}=B^{*}A^{-1}(b_{1}-Bx_{2})-b_{2}=B^{*}x_{1%
 }-b_{2},     of the Schur complement system, where       x  1   :=    A   -  1     (    b  1   -   B   x  2     )       assign   subscript  x  1      superscript  A    1       subscript  b  1     B   subscript  x  2        x_{1}:=A^{-1}(b_{1}-Bx_{2})     denotes the upper half of the solution vector matching the initial guess    x  2     subscript  x  2    x_{2}   for its lower half. We complete the initialization by choosing the first search direction        p  2   :=   r  2    .     assign   subscript  p  2    subscript  r  2     p_{2}:=r_{2}.\,     In each step, we compute       a  2   :=   S   p  2    =    B  *    A   -  1    B   p  2    =    B  *    p  1         assign   subscript  a  2     S   subscript  p  2            superscript  B     superscript  A    1    B   subscript  p  2            superscript  B     subscript  p  1       a_{2}:=Sp_{2}=B^{*}A^{-1}Bp_{2}=B^{*}p_{1}     and keep the intermediate result       p  1   :=    A   -  1    B   p  2       assign   subscript  p  1      superscript  A    1    B   subscript  p  2      p_{1}:=A^{-1}Bp_{2}     for later. The scaling factor is given by      α  :=      p  2  *    r  2    /   p  2  *     a  2       assign  α         superscript   subscript  p  2      subscript  r  2     superscript   subscript  p  2       subscript  a  2      \alpha:=p_{2}^{*}r_{2}/p_{2}^{*}a_{2}     and leads to the updates         x  2   :=    x  2   +   α   p  2      ,    r  2   :=    r  2   -   α   a  2       .     formulae-sequence   assign   subscript  x  2      subscript  x  2     α   subscript  p  2       assign   subscript  r  2      subscript  r  2     α   subscript  a  2        x_{2}:=x_{2}+\alpha p_{2},\quad r_{2}:=r_{2}-\alpha a_{2}.     Using the intermediate result    p  1     subscript  p  1    p_{1}   saved earlier, we can also update the upper part of the solution vector        x  1   :=    x  1   -   α   p  1      .     assign   subscript  x  1      subscript  x  1     α   subscript  p  1       x_{1}:=x_{1}-\alpha p_{1}.\,     Now we only have to construct the new search direction by the Gram–Schmidt process , i.e.,        β  :=      r  2  *    a  2    /   p  2  *     a  2     ,    p  2   :=    r  2   -   β   p  2       .     formulae-sequence   assign  β         superscript   subscript  r  2      subscript  a  2     superscript   subscript  p  2       subscript  a  2      assign   subscript  p  2      subscript  r  2     β   subscript  p  2        \beta:=r_{2}^{*}a_{2}/p_{2}^{*}a_{2},\quad p_{2}:=r_{2}-\beta p_{2}.     The iteration terminates if the residual    r  2     subscript  r  2    r_{2}   has become sufficiently small or if the norm of    p  2     subscript  p  2    p_{2}   is significantly smaller than    r  2     subscript  r  2    r_{2}   indicating that the Krylov subspace has been almost exhausted.  Modifications and extensions  If solving the linear system     A  x   =  b        A  x   b    Ax=b   exactly is not feasible, inexact solvers can be applied. 2 3 4  If the Schur complement system is ill-conditioned, preconditioners can be employed to improve the speed of convergence of the underlying gradient method. 5 6  Inequality constraints can be incorporated, e.g., in order to handle obstacle problems. 7  References  Further reading     "  Category:Numerical analysis     ↩  ↩  ↩  ↩   ↩      