   Kernel Fisher discriminant analysis      Kernel Fisher discriminant analysis  In [[statistics]], '''kernel Fisher discriminant analysis (KFD)''', {{cite journal|last=Mika|first=S |author2=Rätsch, G. |author3=Weston, J. |author4=Schölkopf, B. |author5=Müller, KR|title=F isher discriminant analysis with kernels|journal=Neural Networks for Signal Processing|year=1999|volume=IX|pages=41-48|doi=10.1109/NNSP.1999.788121}} also known as generalized discriminant analysis 1 and kernel discriminant analysis , 2 is a kernelized version of linear discriminant analysis . It is named after Ronald Fisher . Using the kernel trick , LDA is implicitly performed in a new feature space, which allows non-linear mappings to be learned.  Linear discriminant analysis  Intuitively, the idea of LDA is to find a projection where class separation is maximized. Given two sets of labeled data,    𝐂  1     subscript  𝐂  1    \mathbf{C}_{1}   and    𝐂  2     subscript  𝐂  2    \mathbf{C}_{2}   , define the class means    𝐦  1     subscript  𝐦  1    \mathbf{m}_{1}   and    𝐦  2     subscript  𝐦  2    \mathbf{m}_{2}   to be        𝐦  i   =    1   l  i      ∑   n  =  1    l  i     𝐱  n  i      ,       subscript  𝐦  i       1   subscript  l  i      superscript   subscript     n  1     subscript  l  i     superscript   subscript  𝐱  n   i       \mathbf{m}_{i}=\frac{1}{l_{i}}\sum_{n=1}^{l_{i}}\mathbf{x}_{n}^{i},     where    l  i     subscript  l  i    l_{i}   is the number of examples of class    𝐂  i     subscript  𝐂  i    \mathbf{C}_{i}   . The goal of linear discriminant analysis is to give a large separation of the class means while also keeping the in-class variance small. 3 This is formulated as maximizing        J   (  𝐰  )    =     𝐰  T    𝐒  B   𝐰     𝐰  T    𝐒  W   𝐰     ,        J  𝐰        superscript  𝐰  T    subscript  𝐒  B   𝐰      superscript  𝐰  T    subscript  𝐒  W   𝐰      J(\mathbf{w})=\frac{\mathbf{w}^{\text{T}}\mathbf{S}_{B}\mathbf{w}}{\mathbf{w}^%
 {\text{T}}\mathbf{S}_{W}\mathbf{w}},     where    𝐒  B     subscript  𝐒  B    \mathbf{S}_{B}   is the between-class covariance matrix and    𝐒  W     subscript  𝐒  W    \mathbf{S}_{W}   is the total within-class covariance matrix:      𝐒  B     subscript  𝐒  B    \displaystyle\mathbf{S}_{B}     Differentiating    J   (  𝐰  )       J  𝐰    J(\mathbf{w})   with respect to   𝐰   𝐰   \mathbf{w}   , setting equal to zero, and rearranging gives         (    𝐰  T    𝐒  B   𝐰   )    𝐒  W   𝐰   =    (    𝐰  T    𝐒  W   𝐰   )    𝐒  B   𝐰    .           superscript  𝐰  T    subscript  𝐒  B   𝐰    subscript  𝐒  W   𝐰        superscript  𝐰  T    subscript  𝐒  W   𝐰    subscript  𝐒  B   𝐰     (\mathbf{w}^{\text{T}}\mathbf{S}_{B}\mathbf{w})\mathbf{S}_{W}\mathbf{w}=(%
 \mathbf{w}^{\text{T}}\mathbf{S}_{W}\mathbf{w})\mathbf{S}_{B}\mathbf{w}.     Since we only care about the direction of   𝐰   𝐰   \mathbf{w}   and     𝐒  B   𝐰       subscript  𝐒  B   𝐰    \mathbf{S}_{B}\mathbf{w}   has the same direction as    (    𝐦  2   -   𝐦  1    )       subscript  𝐦  2    subscript  𝐦  1     (\mathbf{m}_{2}-\mathbf{m}_{1})   ,     𝐒  B   𝐰       subscript  𝐒  B   𝐰    \mathbf{S}_{B}\mathbf{w}   can be replaced by    (    𝐦  2   -   𝐦  1    )       subscript  𝐦  2    subscript  𝐦  1     (\mathbf{m}_{2}-\mathbf{m}_{1})   and we can drop the scalars    (    𝐰  T    𝐒  B   𝐰   )       superscript  𝐰  T    subscript  𝐒  B   𝐰    (\mathbf{w}^{\text{T}}\mathbf{S}_{B}\mathbf{w})   and    (    𝐰  T    𝐒  W   𝐰   )       superscript  𝐰  T    subscript  𝐒  W   𝐰    (\mathbf{w}^{\text{T}}\mathbf{S}_{W}\mathbf{w})   to give       𝐰  ∝    𝐒  W   -  1     (    𝐦  2   -   𝐦  1    )     .     proportional-to  𝐰     subscript   superscript  𝐒    1    W      subscript  𝐦  2    subscript  𝐦  1       \mathbf{w}\propto\mathbf{S}^{-1}_{W}(\mathbf{m}_{2}-\mathbf{m}_{1}).     Kernel trick with LDA  To extend LDA to non-linear mappings, the data can be mapped to a new feature space,   F   F   F   , via some function   ϕ   ϕ   \phi   . In this new feature space, the function that needs to be maximized is 4        J   (  𝐰  )    =     𝐰  T    𝐒  B  ϕ   𝐰     𝐰  T    𝐒  W  ϕ   𝐰     ,        J  𝐰        superscript  𝐰  T    superscript   subscript  𝐒  B   ϕ   𝐰      superscript  𝐰  T    superscript   subscript  𝐒  W   ϕ   𝐰      J(\mathbf{w})=\frac{\mathbf{w}^{\text{T}}\mathbf{S}_{B}^{\phi}\mathbf{w}}{%
 \mathbf{w}^{\text{T}}\mathbf{S}_{W}^{\phi}\mathbf{w}},     where      𝐒  B  ϕ     superscript   subscript  𝐒  B   ϕ    \displaystyle\mathbf{S}_{B}^{\phi}     and        𝐦  i  ϕ   =    1   l  i      ∑   j  =  1    l  i     ϕ   (   𝐱  j  i   )       .       superscript   subscript  𝐦  i   ϕ       1   subscript  l  i      superscript   subscript     j  1     subscript  l  i      ϕ   superscript   subscript  𝐱  j   i        \mathbf{m}_{i}^{\phi}=\frac{1}{l_{i}}\sum_{j=1}^{l_{i}}\phi(\mathbf{x}_{j}^{i}).     Further, note that    𝐰  ∈  F      𝐰  F    \mathbf{w}\in F   . Explicitly computing the mappings    ϕ   (   𝐱  i   )       ϕ   subscript  𝐱  i     \phi(\mathbf{x}_{i})   and then performing LDA can be computationally expensive, and in many cases intractable. For example,   F   F   F   may be infinitely dimensional. Thus, rather than explicitly mapping the data to   F   F   F   , the data can be implicitly embedded by rewriting the algorithm in terms of dot products and using the kernel trick in which the dot product in the new feature space is replaced by a kernel function,     k   (  𝐱  ,  𝐲  )    =     ϕ   (  𝐱  )    ⋅  ϕ    (  𝐲  )          k   𝐱  𝐲       normal-⋅    ϕ  𝐱   ϕ   𝐲     k(\mathbf{x},\mathbf{y})=\phi(\mathbf{x})\cdot\phi(\mathbf{y})   .  LDA can be reformulated in terms of dot products by first noting that   𝐰   𝐰   \mathbf{w}   will have an expansion of the form 5       𝐰  =    ∑   i  =  1   l     α  i   ϕ   (   𝐱  i   )      .      𝐰    superscript   subscript     i  1    l      subscript  α  i   ϕ   subscript  𝐱  i       \mathbf{w}=\sum_{i=1}^{l}\alpha_{i}\phi(\mathbf{x}_{i}).   Then note that         𝐰  T    𝐦  i  ϕ    =    1   l  i      ∑   j  =  1   l     ∑   k  =  1    l  i      α  j   k   (   𝐱  j   ,   𝐱  k  i   )       =    α  T    𝐌  i     ,           superscript  𝐰  T    superscript   subscript  𝐦  i   ϕ        1   subscript  l  i      superscript   subscript     j  1    l     superscript   subscript     k  1     subscript  l  i       subscript  α  j   k    subscript  𝐱  j    superscript   subscript  𝐱  k   i                superscript  α  T    subscript  𝐌  i       \mathbf{w}^{\text{T}}\mathbf{m}_{i}^{\phi}=\frac{1}{l_{i}}\sum_{j=1}^{l}\sum_{%
 k=1}^{l_{i}}\alpha_{j}k(\mathbf{x}_{j},\mathbf{x}_{k}^{i})=\mathbf{\alpha}^{%
 \text{T}}\mathbf{M}_{i},     where         (   𝐌  i   )   j   =    1   l  i      ∑   k  =  1    l  i     k   (   𝐱  j   ,   𝐱  k  i   )       .       subscript   subscript  𝐌  i   j       1   subscript  l  i      superscript   subscript     k  1     subscript  l  i      k    subscript  𝐱  j    superscript   subscript  𝐱  k   i         (\mathbf{M}_{i})_{j}=\frac{1}{l_{i}}\sum_{k=1}^{l_{i}}k(\mathbf{x}_{j},\mathbf%
 {x}_{k}^{i}).     The numerator of    J   (  𝐰  )       J  𝐰    J(\mathbf{w})   can then be written as:       𝐰  T    𝐒  B  ϕ   𝐰       superscript  𝐰  T    superscript   subscript  𝐒  B   ϕ   𝐰    \displaystyle\mathbf{w}^{\text{T}}\mathbf{S}_{B}^{\phi}\mathbf{w}   where    𝐌  =    (    𝐌  2   -   𝐌  1    )     (    𝐌  2   -   𝐌  1    )   T        𝐌       subscript  𝐌  2    subscript  𝐌  1     superscript     subscript  𝐌  2    subscript  𝐌  1    T      \mathbf{M}=(\mathbf{M}_{2}-\mathbf{M}_{1})(\mathbf{M}_{2}-\mathbf{M}_{1})^{%
 \text{T}}   . Similarly, the denominator can be written as         𝐰  T    𝐒  W  ϕ   𝐰   =    α  T   𝐍  α    ,         superscript  𝐰  T    superscript   subscript  𝐒  W   ϕ   𝐰      superscript  α  T   𝐍  α     \mathbf{w}^{\text{T}}\mathbf{S}_{W}^{\phi}\mathbf{w}=\mathbf{\alpha}^{\text{T}%
 }\mathbf{N}\mathbf{\alpha},     where       𝐍  =    ∑   j  =   1  ,  2       𝐊  j    (   𝐈  -   𝟏   l  j     )    𝐊  j  T      ,      𝐍    subscript     j   1  2        subscript  𝐊  j     𝐈   subscript  1   subscript  l  j      superscript   subscript  𝐊  j   T       \mathbf{N}=\sum_{j=1,2}\mathbf{K}_{j}(\mathbf{I}-\mathbf{1}_{l_{j}})\mathbf{K}%
 _{j}^{\text{T}},     with the     n  th   ,   m  th       superscript  n  th    superscript  m  th     n^{\text{th}},m^{\text{th}}   component of    𝐊  j     subscript  𝐊  j    \mathbf{K}_{j}   defined as    k   (   𝐱  n   ,   𝐱  m  j   )       k    subscript  𝐱  n    superscript   subscript  𝐱  m   j      k(\mathbf{x}_{n},\mathbf{x}_{m}^{j})   ,   𝐈   𝐈   \mathbf{I}   is the identity matrix, and    𝟏   l  j      subscript  1   subscript  l  j     \mathbf{1}_{l_{j}}   the matrix with all entries equal to    1  /   l  j       1   subscript  l  j     1/l_{j}   . This identity can be derived by starting out with the expression for     𝐰  T    𝐒  W  ϕ   𝐰       superscript  𝐰  T    superscript   subscript  𝐒  W   ϕ   𝐰    \mathbf{w}^{\text{T}}\mathbf{S}_{W}^{\phi}\mathbf{w}   and using the expansion of   𝐰   𝐰   \mathbf{w}   and the definitions of    𝐒  W  ϕ     superscript   subscript  𝐒  W   ϕ    \mathbf{S}_{W}^{\phi}   and    𝐦  i  ϕ     superscript   subscript  𝐦  i   ϕ    \mathbf{m}_{i}^{\phi}          𝐰  T    𝐒  W  ϕ   𝐰       superscript  𝐰  T    superscript   subscript  𝐒  W   ϕ   𝐰    \displaystyle\mathbf{w}^{\text{T}}\mathbf{S}_{W}^{\phi}\mathbf{w}     With these equations for the numerator and denominator of    J   (  𝐰  )       J  𝐰    J(\mathbf{w})   , the equation for   J   J   J   can be rewritten as        J   (  α  )    =     α  T   𝐌  α     α  T   𝐍  α     .        J  α        superscript  α  T   𝐌  α      superscript  α  T   𝐍  α      J(\mathbf{\alpha})=\frac{\mathbf{\alpha}^{\text{T}}\mathbf{M}\mathbf{\alpha}}{%
 \mathbf{\alpha}^{\text{T}}\mathbf{N}\mathbf{\alpha}}.     Then, differentiating and setting equal to zero gives         (    α  T   𝐌  α   )   𝐍  α   =    (    α  T   𝐍  α   )   𝐌  α    .           superscript  α  T   𝐌  α   𝐍  α        superscript  α  T   𝐍  α   𝐌  α     (\mathbf{\alpha}^{\text{T}}\mathbf{M}\mathbf{\alpha})\mathbf{N}\mathbf{\alpha}%
 =(\mathbf{\alpha}^{\text{T}}\mathbf{N}\mathbf{\alpha})\mathbf{M}\mathbf{\alpha}.     Since only the direction of   𝐰   𝐰   \mathbf{w}   , and hence the direction of   α   α   \mathbf{\alpha}   , matters, the above can be solved for   α   α   \mathbf{\alpha}   as       α  =    𝐍   -  1     (    𝐌  2   -   𝐌  1    )     .      α     superscript  𝐍    1       subscript  𝐌  2    subscript  𝐌  1       \mathbf{\alpha}=\mathbf{N}^{-1}(\mathbf{M}_{2}-\mathbf{M}_{1}).     Note that in practice,   𝐍   𝐍   \mathbf{N}   is usually singular and so a multiple of the identity is added to it 6        𝐍  ϵ   =   𝐍  +   ϵ  𝐈     .       subscript  𝐍  ϵ     𝐍    ϵ  𝐈      \mathbf{N}_{\epsilon}=\mathbf{N}+\epsilon\mathbf{I}.     Given the solution for   α   α   \mathbf{\alpha}   , the projection of a new data point is given by 7        y   (  𝐱  )    =   (    𝐰  ⋅  ϕ    (  𝐱  )    )   =    ∑   i  =  1   l     α  i   k   (   𝐱  i   ,  𝐱  )      .          y  𝐱      normal-⋅  𝐰  ϕ   𝐱          superscript   subscript     i  1    l      subscript  α  i   k    subscript  𝐱  i   𝐱        y(\mathbf{x})=(\mathbf{w}\cdot\phi(\mathbf{x}))=\sum_{i=1}^{l}\alpha_{i}k(%
 \mathbf{x}_{i},\mathbf{x}).     Multi-class KFD  The extension to cases where there are more than two classes is relatively straightforward. 8 9 10 Let   c   c   c   be the number of classes. Then multi-class KFD involves projecting the data into a    (   c  -  1   )      c  1    (c-1)   -dimensional space using    (   c  -  1   )      c  1    (c-1)   discriminant functions        y  i   =    𝐰  i  T   ϕ   (  𝐱  )      i  =   1  ,  …  ,   c  -  1.        formulae-sequence     subscript  y  i      superscript   subscript  𝐰  i   T   ϕ  𝐱      i   1  normal-…    c  1.       y_{i}=\mathbf{w}_{i}^{\text{T}}\phi(\mathbf{x})\qquad i=1,\ldots,c-1.     This can be written in matrix notation       𝐲  =    𝐖  T   ϕ   (  𝐱  )     ,      𝐲     superscript  𝐖  T   ϕ  𝐱     \mathbf{y}=\mathbf{W}^{\text{T}}\phi(\mathbf{x}),     where the    𝐰  i     subscript  𝐰  i    \mathbf{w}_{i}   are the columns of   𝐖   𝐖   \mathbf{W}   . 11 Further, the between-class covariance matrix is now        𝐒  B  ϕ   =    ∑   i  =  1   c     l  i    (    𝐦  i  ϕ   -   𝐦  ϕ    )     (    𝐦  i  ϕ   -   𝐦  ϕ    )   T      ,       superscript   subscript  𝐒  B   ϕ     superscript   subscript     i  1    c      subscript  l  i      superscript   subscript  𝐦  i   ϕ    superscript  𝐦  ϕ     superscript     superscript   subscript  𝐦  i   ϕ    superscript  𝐦  ϕ    T       \mathbf{S}_{B}^{\phi}=\sum_{i=1}^{c}l_{i}(\mathbf{m}_{i}^{\phi}-\mathbf{m}^{%
 \phi})(\mathbf{m}_{i}^{\phi}-\mathbf{m}^{\phi})^{\text{T}},     where    𝐦  ϕ     superscript  𝐦  ϕ    \mathbf{m}^{\phi}   is the mean of all the data in the new feature space. The within-class covariance matrix is        𝐒  W  ϕ   =    ∑   i  =  1   c     ∑   n  =  1    l  i      (    ϕ   (   𝐱  n  i   )    -   𝐦  i  ϕ    )     (    ϕ   (   𝐱  n  i   )    -   𝐦  i  ϕ    )   T       ,       superscript   subscript  𝐒  W   ϕ     superscript   subscript     i  1    c     superscript   subscript     n  1     subscript  l  i          ϕ   superscript   subscript  𝐱  n   i     superscript   subscript  𝐦  i   ϕ     superscript      ϕ   superscript   subscript  𝐱  n   i     superscript   subscript  𝐦  i   ϕ    T        \mathbf{S}_{W}^{\phi}=\sum_{i=1}^{c}\sum_{n=1}^{l_{i}}(\phi(\mathbf{x}_{n}^{i}%
 )-\mathbf{m}_{i}^{\phi})(\phi(\mathbf{x}_{n}^{i})-\mathbf{m}_{i}^{\phi})^{%
 \text{T}},     The solution is now obtained by maximizing        J   (  𝐖  )    =    |    𝐖  T    𝐒  B  ϕ   𝐖   |    |    𝐖  T    𝐒  W  ϕ   𝐖   |     .        J  𝐖          superscript  𝐖  T    superscript   subscript  𝐒  B   ϕ   𝐖         superscript  𝐖  T    superscript   subscript  𝐒  W   ϕ   𝐖       J(\mathbf{W})=\frac{\left|\mathbf{W}^{\text{T}}\mathbf{S}_{B}^{\phi}\mathbf{W}%
 \right|}{\left|\mathbf{W}^{\text{T}}\mathbf{S}_{W}^{\phi}\mathbf{W}\right|}.     The kernel trick can again be used and the goal of multi-class KFD becomes 12        𝐀  *   =   argmax  𝐀   =    |    𝐀  T   𝐌𝐀   |    |    𝐀  T   𝐍𝐀   |     ,         superscript  𝐀     𝐀  argmax               superscript  𝐀  T   𝐌𝐀         superscript  𝐀  T   𝐍𝐀        \mathbf{A}^{*}=\underset{\mathbf{A}}{\operatorname{argmax}}=\frac{\left|%
 \mathbf{A}^{\text{T}}\mathbf{M}\mathbf{A}\right|}{\left|\mathbf{A}^{\text{T}}%
 \mathbf{N}\mathbf{A}\right|},     where    A  =   [   α  1   ,  …  ,   α   c  -  1    ]       A    subscript  α  1   normal-…   subscript  α    c  1       A=[\mathbf{\alpha}_{1},\ldots,\mathbf{\alpha}_{c-1}]   and     M   M   \displaystyle M     The    𝐌  i     subscript  𝐌  i    \mathbf{M}_{i}   are defined as in the above section and    𝐌  *     subscript  𝐌     \mathbf{M}_{*}   is defined as         (   𝐌  *   )   j   =    1  l     ∑   k  =  1   l    k   (   𝐱  j   ,   𝐱  k   )       .       subscript   subscript  𝐌    j       1  l     superscript   subscript     k  1    l     k    subscript  𝐱  j    subscript  𝐱  k         (\mathbf{M}_{*})_{j}=\frac{1}{l}\sum_{k=1}^{l}k(\mathbf{x}_{j},\mathbf{x}_{k}).       𝐀  *     superscript  𝐀     \mathbf{A}^{*}   can then be computed by finding the    (   c  -  1   )      c  1    (c-1)   leading eigenvectors of     𝐍   -  1    𝐌       superscript  𝐍    1    𝐌    \mathbf{N}^{-1}\mathbf{M}   . 13 Furthermore, the projection of a new input,    𝐱  t     subscript  𝐱  t    \mathbf{x}_{t}   , is given by 14        𝐲   (   𝐱  t   )    =     (   𝐀  *   )   T    𝐊  t     ,        𝐲   subscript  𝐱  t       superscript   superscript  𝐀    T    subscript  𝐊  t      \mathbf{y}(\mathbf{x}_{t})=\left(\mathbf{A}^{*}\right)^{\text{T}}\mathbf{K}_{t},     where the    i   t  h      superscript  i    t  h     i^{th}   component of    𝐊  t     subscript  𝐊  t    \mathbf{K}_{t}   is given by    k   (   𝐱  i   ,   𝐱  t   )       k    subscript  𝐱  i    subscript  𝐱  t      k(\mathbf{x}_{i},\mathbf{x}_{t})   .  Classification using KFD  In both two-class and multi-class KFD, the class label of a new input can be assigned as 15        f   (  𝐱  )    =   a  r  g    min  j   D    (   𝐲   (  𝐱  )    ,    𝐲  ¯   j   )     ,        f  𝐱     a  r  g    subscript   j   D      𝐲  𝐱    subscript   normal-¯  𝐲   j       f(\mathbf{x})=arg\min_{j}D(\mathbf{y}(\mathbf{x}),\bar{\mathbf{y}}_{j}),     where     𝐲  ¯   j     subscript   normal-¯  𝐲   j    \bar{\mathbf{y}}_{j}   is the projected mean for class   j   j   j   and    D   (  ⋅  ,  ⋅  )       D   normal-⋅  normal-⋅     D(\cdot,\cdot)   is a distance function.  Applications  Kernel discriminant analysis has been used in a variety of applications. These include:   Face recognition 16 17 18 and detection 19 20  Hand-written digit recognition 21 22  Palmprint recognition 23  Classification of malignant and benign cluster microcalcifications 24  Seed classification 25   See also   Factor analysis  Kernel principal component analysis  Kernel trick  Linear discriminant analysis   References  External links   Kernel Discriminant Analysis - This site gives a high level explanation of KFD.  Kernel Discriminant Analysis in C# - C# code to perform KFD.  Matlab Toolbox for Dimensionality Reduction - Includes a method for performing KFD.  Handwriting Recognition using Kernel Discriminant Analysis - C# code that demonstrates handwritten digit recognition using KFD.   "  Category:Multivariate statistics     ↩  ↩  ↩   ↩     ↩  ↩        ↩  ↩  ↩  ↩   ↩  ↩  ↩      