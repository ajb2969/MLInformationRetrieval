


Kernel Fisher discriminant analysis




Kernel Fisher discriminant analysis
In [[statistics]], '''kernel Fisher discriminant analysis (KFD)''',{{cite journal|last=Mika|first=S |author2=Rätsch, G. |author3=Weston, J. |author4=Schölkopf, B. |author5=Müller, KR|title=F
 isher discriminant analysis with kernels|journal=Neural Networks for Signal Processing|year=1999|volume=IX|pages=41-48|doi=10.1109/NNSP.1999.788121}} also known as generalized discriminant analysis1 and kernel discriminant analysis,2 is a kernelized version of linear discriminant analysis. It is named after Ronald Fisher. Using the kernel trick, LDA is implicitly performed in a new feature space, which allows non-linear mappings to be learned.
Linear discriminant analysis
Intuitively, the idea of LDA is to find a projection where class separation is maximized. Given two sets of labeled data, 
 
 
 
  and 
 
 
 
 , define the class means 
 
 
 
  and 
 
 
 
 
  to be



where 
 
 
 
  is the number of examples of class 
 
 
 
 . The goal of linear discriminant analysis is to give a large separation of the class means while also keeping the in-class variance small.3 This is formulated as maximizing



where 
 
 
 
 
  is the between-class covariance matrix and 
 
 
 
  is the total within-class covariance matrix:



Differentiating 
 
 
 
  with respect to 
 
 
 
 , setting equal to zero, and rearranging gives



Since we only care about the direction of 
 
 
 
  and 
 
 
 
  has the same direction as 
 
 
 
  , 
 
 
 
  can be replaced by 
 
 
 
  and we can drop the scalars 
 
 
 
  and 
 
 
 
  to give



Kernel trick with LDA
To extend LDA to non-linear mappings, the data can be mapped to a new feature space, 
 
 
 
 , via some function 
 
 
 
 . In this new feature space, the function that needs to be maximized is4



where



and



Further, note that 
 
 
 
 . Explicitly computing the mappings 
 
 
 
  and then performing LDA can be computationally expensive, and in many cases intractable. For example, 
 
 
 
  may be infinitely dimensional. Thus, rather than explicitly mapping the data to 
 
 
 
 , the data can be implicitly embedded by rewriting the algorithm in terms of dot products and using the kernel trick in which the dot product in the new feature space is replaced by a kernel function, 
 
 
 
 .
LDA can be reformulated in terms of dot products by first noting that 
 
 
 
  will have an expansion of the form5


 
  Then note that



where



The numerator of 
 
 
 
  can then be written as:


 
 
  where 
 
 
 
 . Similarly, the denominator can be written as



where



with the 
 
 
 
  component of 
 
 
 
  defined as 
 
 
 
 , 
 
 
 
  is the identity matrix, and 
 
 
 
  the matrix with all entries equal to 
 
 
 
 . This identity can be derived by starting out with the expression for 
 
 
 
  and using the expansion of 
 
 
 
  and the definitions of 
 
 
 
  and 
 
 




With these equations for the numerator and denominator of 
 
 
 
 , the equation for 
 
 
 
  can be rewritten as



Then, differentiating and setting equal to zero gives



Since only the direction of 
 
 
 
 , and hence the direction of 
 
 
 
 , matters, the above can be solved for 
 
 
 
  as



Note that in practice, 
 
 
 
  is usually singular and so a multiple of the identity is added to it 6



Given the solution for 
 
 
 
 , the projection of a new data point is given by7



Multi-class KFD
The extension to cases where there are more than two classes is relatively straightforward.8910 Let 
 
 
 
  be the number of classes. Then multi-class KFD involves projecting the data into a 
 
 
 
 -dimensional space using 
 
 
 
  discriminant functions



This can be written in matrix notation



where the 
 
 
 
  are the columns of 
 
 
 
 .11 Further, the between-class covariance matrix is now



where 
 
 
 
  is the mean of all the data in the new feature space. The within-class covariance matrix is



The solution is now obtained by maximizing



The kernel trick can again be used and the goal of multi-class KFD becomes12



where 
 
 
 
  and



The 
 
 
 
  are defined as in the above section and 
 
 
 
  is defined as



 
  can then be computed by finding the 
 
 
 
  leading eigenvectors of 
 
 
 
 .13 Furthermore, the projection of a new input, 
 
 
 
 , is given by14



where the 
 
 
 
  component of 
 
 
 
  is given by 
 
 
 
 .
Classification using KFD
In both two-class and multi-class KFD, the class label of a new input can be assigned as15



where 
 
 
 
  is the projected mean for class 
 
 
 
  and 
 
 
 
  is a distance function.
Applications
Kernel discriminant analysis has been used in a variety of applications. These include:

Face recognition161718 and detection1920
Hand-written digit recognition2122
Palmprint recognition23
Classification of malignant and benign cluster microcalcifications24
Seed classification25

See also

Factor analysis
Kernel principal component analysis
Kernel trick
Linear discriminant analysis

References
External links

Kernel Discriminant Analysis - This site gives a high level explanation of KFD.
Kernel Discriminant Analysis in C# - C# code to perform KFD.
Matlab Toolbox for Dimensionality Reduction - Includes a method for performing KFD.
Handwriting Recognition using Kernel Discriminant Analysis - C# code that demonstrates handwritten digit recognition using KFD.

"
Category:Multivariate statistics
































