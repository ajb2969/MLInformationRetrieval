   Levinson recursion      Levinson recursion   Levinson recursion or Levinson–Durbin recursion is a procedure in linear algebra to recursively calculate the solution to an equation involving a Toeplitz matrix . The algorithm runs in Θ ( n 2 ) time, which is a strong improvement over Gauss–Jordan elimination , which runs in Θ( n 3 ).  The Levinson–Durbin algorithm was proposed first by Norman Levinson in 1947, improved by James Durbin in 1960, and subsequently improved to 4 n 2 and then 3 n 2 multiplications by W. F. Trench and S. Zohar, respectively.  Other methods to process data include Schur decomposition and Cholesky decomposition . In comparison to these, Levinson recursion (particularly split Levinson recursion) tends to be faster computationally, but more sensitive to computational inaccuracies like round-off errors .  The Bareiss algorithm for Toeplitz matrices (not to be confused with the general Bareiss algorithm ) runs about as fast as Levinson recursion, but it uses O ( n 2 ) space, whereas Levinson recursion uses only O ( n ) space. The Bareiss algorithm, though, is numerically stable , 1 2 whereas Levinson recursion is at best only weakly stable (i.e. it exhibits numerical stability for well-conditioned linear systems). 3  Newer algorithms, called asymptotically fast or sometimes superfast Toeplitz algorithms, can solve in Θ( n log p n ) for various p (e.g. p = 2, 4 5  p = 3 6 ). Levinson recursion remains popular for several reasons; for one, it is relatively easy to understand in comparison; for another, it can be faster than a superfast algorithm for small n (usually n  http://www.math.niu.edu/~ammar/papers/amgr88.pdf  Derivation  Background  Matrix equations follow the form:        y  →   =    𝐌    x  →     .       normal-→  y     𝐌   normal-→  x      \vec{y}=\mathbf{M}\ \vec{x}.     The Levinson–Durbin algorithm may be used for any such equation, as long as M is a known Toeplitz matrix with a nonzero main diagonal. Here    y  →     normal-→  y    \vec{y}   is a known vector , and    x  →     normal-→  x    \vec{x}   is an unknown vector of numbers x i yet to be determined.  For the sake of this article, ê i is a vector made up entirely of zeroes, except for its i th place, which holds the value one. Its length will be implicitly determined by the surrounding context. The term N refers to the width of the matrix above – M is an N × N matrix. Finally, in this article, superscripts refer to an inductive index , whereas subscripts denote indices. For example (and definition), in this article, the matrix T n is an n×n matrix which copies the upper left n×n block from M – that is, T n ij = M ij .  T n is also a Toeplitz matrix; meaning that it can be written as:        𝐓  n   =   [      t  0      t   -  1       t   -  2      …     t    -  n   +  1         t  1      t  0      t   -  1      …     t    -  n   +  2         t  2      t  1      t  0     …     t    -  n   +  3        ⋮    ⋮    ⋮    ⋱    ⋮       t   n  -  1       t   n  -  2       t   n  -  3      …     t  0      ]    .       superscript  𝐓  n      subscript  t  0    subscript  t    1     subscript  t    2    normal-…   subscript  t      n   1       subscript  t  1    subscript  t  0    subscript  t    1    normal-…   subscript  t      n   2       subscript  t  2    subscript  t  1    subscript  t  0   normal-…   subscript  t      n   3      normal-⋮  normal-⋮  normal-⋮  normal-⋱  normal-⋮     subscript  t    n  1     subscript  t    n  2     subscript  t    n  3    normal-…   subscript  t  0       \mathbf{T}^{n}=\begin{bmatrix}t_{0}&t_{-1}&t_{-2}&\dots&t_{-n+1}\\
 t_{1}&t_{0}&t_{-1}&\dots&t_{-n+2}\\
 t_{2}&t_{1}&t_{0}&\dots&t_{-n+3}\\
 \vdots&\vdots&\vdots&\ddots&\vdots\\
 t_{n-1}&t_{n-2}&t_{n-3}&\dots&t_{0}\end{bmatrix}.     Introductory steps  The algorithm proceeds in two steps. In the first step, two sets of vectors, called the forward and backward vectors, are established. The forward vectors are used to help get the set of backward vectors; then they can be immediately discarded. The backwards vectors are necessary for the second step, where they are used to build the solution desired.  Levinson–Durbin recursion defines the n th "forward vector", denoted     f  →   n     superscript   normal-→  f   n    \vec{f}^{n}   , as the vector of length n which satisfies:         𝐓  n     f  →   n    =    e  ^   1    .         superscript  𝐓  n    superscript   normal-→  f   n     subscript   normal-^  e   1     \mathbf{T}^{n}\vec{f}^{n}=\hat{e}_{1}.     The n th "backward vector"     b  →   n     superscript   normal-→  b   n    \vec{b}^{n}   is defined similarly; it is the vector of length n which satisfies:         𝐓  n     b  →   n    =    e  ^   n    .         superscript  𝐓  n    superscript   normal-→  b   n     subscript   normal-^  e   n     \mathbf{T}^{n}\vec{b}^{n}=\hat{e}_{n}.     An important simplification can occur when M is a symmetric matrix ; then the two vectors are related by b n i = f n n +1− i —that is, they are row-reversals of each other. This can save some extra computation in that special case.  Obtaining the backward vectors  Even if the matrix is not symmetric, then the n th forward and backward vector may be found from the vectors of length n − 1 as follows. First, the forward vector may be extended with a zero to obtain:         𝐓  n    [       f  →    n  -  1        0     ]    =    [               t    -  n   +  1            𝐓   n  -  1          t    -  n   +  2                 ⋮       t   n  -  1       t   n  -  2      …     t  0      ]    [            f  →    n  -  1             0          ]    =   [     1      0      ⋮      0       ϵ  f  n      ]    .           superscript  𝐓  n      superscript   normal-→  f     n  1      0         absent  absent  absent   subscript  t      n   1      absent   superscript  𝐓    n  1    absent   subscript  t      n   2      absent  absent  absent  normal-⋮     subscript  t    n  1     subscript  t    n  2    normal-…   subscript  t  0       absent     superscript   normal-→  f     n  1      absent    0    absent            1    0    normal-⋮    0     superscript   subscript  ϵ  f   n        \mathbf{T}^{n}\begin{bmatrix}\vec{f}^{n-1}\\
 0\\
 \end{bmatrix}=\begin{bmatrix}&&&t_{-n+1}\\
 &\mathbf{T}^{n-1}&&t_{-n+2}\\
 &&&\vdots\\
 t_{n-1}&t_{n-2}&\dots&t_{0}\\
 \end{bmatrix}\begin{bmatrix}\\
 \vec{f}^{n-1}\\
 \\
 0\\
 \\
 \end{bmatrix}=\begin{bmatrix}1\\
 0\\
 \vdots\\
 0\\
 \epsilon_{f}^{n}\end{bmatrix}.     In going from T n −1 to T n , the extra column added to the matrix does not perturb the solution when a zero is used to extend the forward vector. However, the extra row added to the matrix has perturbed the solution; and it has created an unwanted error term ε f which occurs in the last place. The above equation gives it the value of:         ϵ  f  n    =     ∑   i  =  1    n  -  1        M   n  i       f  i   n  -  1       =     ∑   i  =  1    n  -  1        t   n  -  i      f  i   n  -  1       .         superscript   subscript  ϵ  f   n     superscript   subscript     i  1      n  1       subscript  M    n  i     superscript   subscript  f  i     n  1             superscript   subscript     i  1      n  1       subscript  t    n  i     superscript   subscript  f  i     n  1         \epsilon_{f}^{n}\ =\ \sum_{i=1}^{n-1}\ M_{ni}\ f_{i}^{n-1}\ =\ \sum_{i=1}^{n-1%
 }\ t_{n-i}\ f_{i}^{n-1}.     This error will be returned to shortly and eliminated from the new forward vector; but first, the backwards vector must be extended in a similar (albeit reversed) fashion. For the backwards vector,         𝐓  n    [     0        b  →    n  -  1       ]    =    [      t  0     …     t    -  n   +  2       t    -  n   +  1        ⋮                t   n  -  2          𝐓   n  -  1            t   n  -  1                ]    [          0             b  →    n  -  1            ]    =   [      ϵ  b  n       0      ⋮      0      1     ]    .           superscript  𝐓  n     0     superscript   normal-→  b     n  1            subscript  t  0   normal-…   subscript  t      n   2     subscript  t      n   1      normal-⋮  absent  absent  absent     subscript  t    n  2    absent   superscript  𝐓    n  1    absent     subscript  t    n  1    absent  absent  absent      absent    0    absent     superscript   normal-→  b     n  1      absent             superscript   subscript  ϵ  b   n     0    normal-⋮    0    1       \mathbf{T}^{n}\begin{bmatrix}0\\
 \vec{b}^{n-1}\\
 \end{bmatrix}=\begin{bmatrix}t_{0}&\dots&t_{-n+2}&t_{-n+1}\\
 \vdots&&&\\
 t_{n-2}&&\mathbf{T}^{n-1}&\\
 t_{n-1}&&&\end{bmatrix}\begin{bmatrix}\\
 0\\
 \\
 \vec{b}^{n-1}\\
 \\
 \end{bmatrix}=\begin{bmatrix}\epsilon_{b}^{n}\\
 0\\
 \vdots\\
 0\\
 1\end{bmatrix}.     As before, the extra column added to the matrix does not perturb this new backwards vector; but the extra row does. Here we have another unwanted error ε b with value:         ϵ  b  n    =     ∑   i  =  2   n       M   1  i       b   i  -  1    n  -  1       =     ∑   i  =  1    n  -  1        t   -  i      b  i   n  -  1       .         superscript   subscript  ϵ  b   n     superscript   subscript     i  2    n      subscript  M    1  i     superscript   subscript  b    i  1      n  1             superscript   subscript     i  1      n  1       subscript  t    i     superscript   subscript  b  i     n  1         \epsilon_{b}^{n}\ =\ \sum_{i=2}^{n}\ M_{1i}\ b_{i-1}^{n-1}\ =\ \sum_{i=1}^{n-1%
 }\ t_{-i}\ b_{i}^{n-1}.     These two error terms can be used to eliminate each other. Using the linearity of matrices,        ∀    (  α  ,  β  )   𝐓   (    α   [      f  →            0     ]    +   β   [     0            b  →      ]     )     =    α   [     1      0      ⋮      0       ϵ  f      ]    +   β   [      ϵ  b       0      ⋮      0      1     ]      .       for-all     α  β   𝐓      α     normal-→  f     absent    0       β    0    absent     normal-→  b             α    1    0    normal-⋮    0     subscript  ϵ  f        β     subscript  ϵ  b     0    normal-⋮    0    1        \forall(\alpha,\beta)\ \mathbf{T}\left(\alpha\begin{bmatrix}\vec{f}\\
 \\
 0\\
 \end{bmatrix}+\beta\begin{bmatrix}0\\
 \\
 \vec{b}\end{bmatrix}\right)=\alpha\begin{bmatrix}1\\
 0\\
 \vdots\\
 0\\
 \epsilon_{f}\\
 \end{bmatrix}+\beta\begin{bmatrix}\epsilon_{b}\\
 0\\
 \vdots\\
 0\\
 1\end{bmatrix}.     If α and β are chosen so that the right hand side yields ê 1 or ê n , then the quantity in the parentheses will fulfill the definition of the n th forward or backward vector, respectively. With those alpha and beta chosen, the vector sum in the parentheses is simple and yields the desired result.  To find these coefficients,    α  f  n     subscript   superscript  α  n   f    \alpha^{n}_{f}   ,    β  f  n     subscript   superscript  β  n   f    \beta^{n}_{f}   are such that :        f  →   n   =     α  f  n    [       f  →    n  -  1            ]    +    β  f  n    [     0        b  →    n  -  1       ]          subscript   normal-→  f   n        subscript   superscript  α  n   f      subscript   normal-→  f     n  1      absent        subscript   superscript  β  n   f     0     subscript   normal-→  b     n  1          \vec{f}_{n}=\alpha^{n}_{f}\begin{bmatrix}\vec{f}_{n-1}\\
 \par
 \end{bmatrix}+\beta^{n}_{f}\begin{bmatrix}0\\
 \vec{b}_{n-1}\end{bmatrix}   and respectively    α  b  n     subscript   superscript  α  n   b    \alpha^{n}_{b}   ,    β  b  n     subscript   superscript  β  n   b    \beta^{n}_{b}   are such that :         b  →   n   =     α  b  n    [       f  →    n  -  1            ]    +    β  b  n    [     0        b  →    n  -  1       ]      .       subscript   normal-→  b   n        subscript   superscript  α  n   b      subscript   normal-→  f     n  1      absent        subscript   superscript  β  n   b     0     subscript   normal-→  b     n  1          \vec{b}_{n}=\alpha^{n}_{b}\begin{bmatrix}\vec{f}_{n-1}\\
 \par
 \end{bmatrix}+\beta^{n}_{b}\begin{bmatrix}0\\
 \vec{b}_{n-1}\end{bmatrix}.   By multiplying both previous equations by    𝐓  n     superscript  𝐓  n    {\mathbf{T}}^{n}   one gets the following equation:         [     1     ϵ  b  n       0    0      ⋮    ⋮      0    0       ϵ  f  n     1     ]    [      α  f  n      α  b  n        β  f  n      β  b  n      ]    =   [     1    0      0    0      ⋮    ⋮      0    0      0    1     ]    .          1   subscript   superscript  ϵ  n   b     0  0    normal-⋮  normal-⋮    0  0     subscript   superscript  ϵ  n   f   1       subscript   superscript  α  n   f    subscript   superscript  α  n   b      subscript   superscript  β  n   f    subscript   superscript  β  n   b        1  0    0  0    normal-⋮  normal-⋮    0  0    0  1      \begin{bmatrix}1&\epsilon^{n}_{b}\\
 0&0\\
 \vdots&\vdots\\
 0&0\\
 \epsilon^{n}_{f}&1\end{bmatrix}\begin{bmatrix}\alpha^{n}_{f}&\alpha^{n}_{b}\\
 \beta^{n}_{f}&\beta^{n}_{b}\end{bmatrix}=\begin{bmatrix}1&0\\
 0&0\\
 \vdots&\vdots\\
 0&0\\
 0&1\end{bmatrix}.     Now, all the zeroes in the middle of the two vectors above being disregarded and collapsed, only the following equation is left:         [     1     ϵ  b  n        ϵ  f  n     1     ]    [      α  f  n      α  b  n        β  f  n      β  b  n      ]    =   [     1    0      0    1     ]    .          1   subscript   superscript  ϵ  n   b      subscript   superscript  ϵ  n   f   1       subscript   superscript  α  n   f    subscript   superscript  α  n   b      subscript   superscript  β  n   f    subscript   superscript  β  n   b        1  0    0  1      \begin{bmatrix}1&\epsilon^{n}_{b}\\
 \epsilon^{n}_{f}&1\end{bmatrix}\begin{bmatrix}\alpha^{n}_{f}&\alpha^{n}_{b}\\
 \beta^{n}_{f}&\beta^{n}_{b}\end{bmatrix}=\begin{bmatrix}1&0\\
 0&1\end{bmatrix}.     With these solved for (by using the Cramer 2×2 matrix inverse formula), the new forward and backward vectors are:        f  →   n   =     1   1  -    ϵ  b  n    ϵ  f  n       [       f  →    n  -  1        0     ]    -     ϵ  f  n    1  -    ϵ  b  n    ϵ  f  n       [     0        b  →    n  -  1       ]          superscript   normal-→  f   n         1    1     superscript   subscript  ϵ  b   n    superscript   subscript  ϵ  f   n         superscript   normal-→  f     n  1      0          superscript   subscript  ϵ  f   n     1     superscript   subscript  ϵ  b   n    superscript   subscript  ϵ  f   n        0     superscript   normal-→  b     n  1          \vec{f}^{n}={1\over{1-\epsilon_{b}^{n}\epsilon_{f}^{n}}}\begin{bmatrix}\vec{f}%
 ^{n-1}\\
 0\end{bmatrix}-{\epsilon_{f}^{n}\over{1-\epsilon_{b}^{n}\epsilon_{f}^{n}}}%
 \begin{bmatrix}0\\
 \vec{b}^{n-1}\end{bmatrix}            b  →   n   =     1   1  -    ϵ  b  n    ϵ  f  n       [     0        b  →    n  -  1       ]    -     ϵ  b  n    1  -    ϵ  b  n    ϵ  f  n       [       f  →    n  -  1        0     ]      .       superscript   normal-→  b   n         1    1     superscript   subscript  ϵ  b   n    superscript   subscript  ϵ  f   n        0     superscript   normal-→  b     n  1            superscript   subscript  ϵ  b   n     1     superscript   subscript  ϵ  b   n    superscript   subscript  ϵ  f   n         superscript   normal-→  f     n  1      0        \vec{b}^{n}={1\over{1-\epsilon_{b}^{n}\epsilon_{f}^{n}}}\begin{bmatrix}0\\
 \vec{b}^{n-1}\end{bmatrix}-{\epsilon_{b}^{n}\over{1-\epsilon_{b}^{n}\epsilon_{%
 f}^{n}}}\begin{bmatrix}\vec{f}^{n-1}\\
 0\end{bmatrix}.     Performing these vector summations, then, gives the n th forward and backward vectors from the prior ones. All that remains is to find the first of these vectors, and then some quick sums and multiplications give the remaining ones. The first forward and backward vectors are simply:         f  →   1   =    b  →   1   =   [       1   M  11        ]   =   [       1   t  0        ]    .         superscript   normal-→  f   1    superscript   normal-→  b   1            1   subscript  M  11               1   subscript  t  0         \vec{f}^{1}=\vec{b}^{1}=\begin{bmatrix}{1\over M_{11}}\end{bmatrix}=\begin{%
 bmatrix}{1\over t_{0}}\end{bmatrix}.     Using the backward vectors  The above steps give the N backward vectors for M . From there, a more arbitrary equation is:        y  →   =    𝐌    x  →     .       normal-→  y     𝐌   normal-→  x      \vec{y}=\mathbf{M}\ \vec{x}.     The solution can be built in the same recursive way that the backwards vectors were built. Accordingly,    x  →     normal-→  x    \vec{x}   must be generalized to a sequence     x  →   n     superscript   normal-→  x   n    \vec{x}^{n}   , from which      x  →   N   =   x  →        superscript   normal-→  x   N    normal-→  x     \vec{x}^{N}=\vec{x}   .  The solution is then built recursively by noticing that if         𝐓   n  -  1     [      x  1   n  -  1         x  2   n  -  1        ⋮       x   n  -  1    n  -  1       ]    =   [      y  1        y  2       ⋮       y   n  -  1       ]    .         superscript  𝐓    n  1       superscript   subscript  x  1     n  1       superscript   subscript  x  2     n  1      normal-⋮     superscript   subscript  x    n  1      n  1          subscript  y  1      subscript  y  2     normal-⋮     subscript  y    n  1        \mathbf{T}^{n-1}\begin{bmatrix}x_{1}^{n-1}\\
 x_{2}^{n-1}\\
 \vdots\\
 x_{n-1}^{n-1}\\
 \end{bmatrix}=\begin{bmatrix}y_{1}\\
 y_{2}\\
 \vdots\\
 y_{n-1}\end{bmatrix}.     Then, extending with a zero again, and defining an error constant where necessary:         𝐓  n    [      x  1   n  -  1         x  2   n  -  1        ⋮       x   n  -  1    n  -  1        0     ]    =   [      y  1        y  2       ⋮       y   n  -  1         ϵ  x   n  -  1       ]    .         superscript  𝐓  n      superscript   subscript  x  1     n  1       superscript   subscript  x  2     n  1      normal-⋮     superscript   subscript  x    n  1      n  1      0        subscript  y  1      subscript  y  2     normal-⋮     subscript  y    n  1       superscript   subscript  ϵ  x     n  1        \mathbf{T}^{n}\begin{bmatrix}x_{1}^{n-1}\\
 x_{2}^{n-1}\\
 \vdots\\
 x_{n-1}^{n-1}\\
 0\end{bmatrix}=\begin{bmatrix}y_{1}\\
 y_{2}\\
 \vdots\\
 y_{n-1}\\
 \epsilon_{x}^{n-1}\end{bmatrix}.     We can then use the n th backward vector to eliminate the error term and replace it with the desired formula as follows:         𝐓  n    (    [      x  1   n  -  1         x  2   n  -  1        ⋮       x   n  -  1    n  -  1        0     ]   +    (    y  n   -   ϵ  x   n  -  1     )     b  →   n     )    =   [      y  1        y  2       ⋮       y   n  -  1         y  n      ]    .         superscript  𝐓  n        superscript   subscript  x  1     n  1       superscript   subscript  x  2     n  1      normal-⋮     superscript   subscript  x    n  1      n  1      0         subscript  y  n    superscript   subscript  ϵ  x     n  1      superscript   normal-→  b   n         subscript  y  1      subscript  y  2     normal-⋮     subscript  y    n  1       subscript  y  n       \mathbf{T}^{n}\left(\begin{bmatrix}x_{1}^{n-1}\\
 x_{2}^{n-1}\\
 \vdots\\
 x_{n-1}^{n-1}\\
 0\\
 \end{bmatrix}+(y_{n}-\epsilon_{x}^{n-1})\ \vec{b}^{n}\right)=\begin{bmatrix}y_%
 {1}\\
 y_{2}\\
 \vdots\\
 y_{n-1}\\
 y_{n}\end{bmatrix}.     Extending this method until n = N yields the solution    x  →     normal-→  x    \vec{x}   .  In practice, these steps are often done concurrently with the rest of the procedure, but they form a coherent unit and deserve to be treated as their own step.  Block Levinson algorithm  If M is not strictly Toeplitz, but block Toeplitz, the Levinson recursion can be derived in much the same way by regarding the block Toeplitz matrix as a Toeplitz matrix with matrix elements (Musicus 1988). Block Toeplitz matrices arise naturally in signal processing algorithms when dealing with multiple signal streams (e.g., in MIMO systems) or cyclo-stationary signals.  See also   Split Levinson recursion  Linear prediction  Autoregressive model   Notes  References  Defining sources   Levinson, N. (1947). "The Wiener RMS error criterion in filter design and prediction." J. Math. Phys. , v. 25, pp. 261–278.  Durbin, J. (1960). "The fitting of time series models." Rev. Inst. Int. Stat. , v. 28, pp. 233–243.  Trench, W. F. (1964). "An algorithm for the inversion of finite Toeplitz matrices." J. Soc. Indust. Appl. Math. , v. 12, pp. 515–522.  Musicus, B. R. (1988). "Levinson and Fast Choleski Algorithms for Toeplitz and Almost Toeplitz Matrices." RLE TR No. 538, MIT. 1  Delsarte, P. and Genin, Y. V. (1986). "The split Levinson algorithm." IEEE Transactions on Acoustics, Speech, and Signal Processing , v. ASSP-34(3), pp. 470–478.   Further work    Brent R.P. (1999), "Stability of fast algorithms for structured linear systems", Fast Reliable Algorithms for Matrices with Structure (editors—T. Kailath, A.H. Sayed), ch.4 ( SIAM ).  Bunch, J. R. (1985). "Stability of methods for solving Toeplitz systems of equations." SIAM J. Sci. Stat. Comput. , v. 6, pp. 349–364. 2    Summaries   Bäckström, T. (2004). "2.2. Levinson–Durbin Recursion." Linear Predictive Modelling of Speech – Constraints and Line Spectrum Pair Decomposition. Doctoral thesis. Report no. 71 / Helsinki University of Technology, Laboratory of Acoustics and Audio Signal Processing. Espoo, Finland. 3  Claerbout, Jon F. (1976). "Chapter 7 – Waveform Applications of Least-Squares." Fundamentals of Geophysical Data Processing. Palo Alto: Blackwell Scientific Publications. 4   Golub, G.H., and Loan, C.F. Van (1996). "Section 4.7 : Toeplitz and related Systems" Matrix Computations , Johns Hopkins University Press   "  Category:Matrices  Category:Numerical analysis     Bojanczyk et al. (1995). ↩  Brent (1999). ↩  Krishna & Wang (1993). ↩  http://www.maths.anu.edu.au/~brent/pd/rpb143tr.pdf ↩  http://etd.gsu.edu/theses/available/etd-04182008-174330/unrestricted/kimitei_symon_k_200804.pdf ↩  http://web.archive.org/web/20070418074240/http://saaz.cs.gsu.edu/papers/sfast.pdf ↩     