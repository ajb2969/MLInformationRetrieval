   Additive model      Additive model   In statistics , an additive model ( AM ) is a nonparametric regression method. It was suggested by Jerome H. Friedman and Werner Stuetzle (1981) 1 and is an essential part of the ACE algorithm. The AM uses a one-dimensional smoother to build a restricted class of nonparametric regression models. Because of this, it is less affected by the curse of dimensionality than e.g. a p -dimensional smoother. Furthermore, the AM is more flexible than a standard linear model , while being more interpretable than a general regression surface at the cost of approximation errors. Problems with AM include model selection , overfitting , and multicollinearity .  Description  Given a data set     {   y  i   ,   x   i  1    ,  …  ,   x   i  p    }    i  =  1   n     superscript   subscript    subscript  y  i    subscript  x    i  1    normal-…   subscript  x    i  p       i  1    n    \{y_{i},\,x_{i1},\ldots,x_{ip}\}_{i=1}^{n}   of n  statistical units , where     {   x   i  1    ,  …  ,   x   i  p    }    i  =  1   n     superscript   subscript    subscript  x    i  1    normal-…   subscript  x    i  p       i  1    n    \{x_{i1},\ldots,x_{ip}\}_{i=1}^{n}   represent predictors and    y  i     subscript  y  i    y_{i}   is the outcome, the additive model takes the form      E   [   y  i   |   x   i  1    ,  …  ,   x   i  p    ]   =   β  0   +   ∑   j  =  1   p    f  j    (   x   i  j    )      fragments  E   fragments  normal-[   subscript  y  i   normal-|   subscript  x    i  1    normal-,  normal-…  normal-,   subscript  x    i  p    normal-]     subscript  β  0     superscript   subscript     j  1    p    subscript  f  j    fragments  normal-(   subscript  x    i  j    normal-)     E[y_{i}|x_{i1},\ldots,x_{ip}]=\beta_{0}+\sum_{j=1}^{p}f_{j}(x_{ij})   or      Y  =    β  0   +    ∑   j  =  1   p     f  j    (   X  j   )     +  ε       Y     subscript  β  0     superscript   subscript     j  1    p      subscript  f  j    subscript  X  j     ε     Y=\beta_{0}+\sum_{j=1}^{p}f_{j}(X_{j})+\varepsilon   Where     E   [  ϵ  ]    =  0        E   delimited-[]  ϵ    0    E[\epsilon]=0   ,     V  a  r   (  ϵ  )    =   σ  2         V  a  r  ϵ    superscript  σ  2     Var(\epsilon)=\sigma^{2}   and     E   [    f  j    (   X  j   )    ]    =  0        E   delimited-[]     subscript  f  j    subscript  X  j      0    E[f_{j}(X_{j})]=0   . The functions     f  j    (   x   i  j    )        subscript  f  j    subscript  x    i  j      f_{j}(x_{ij})   are unknown smooth functions fit from the data. Fitting the AM (i.e. the functions     f  j    (   x   i  j    )        subscript  f  j    subscript  x    i  j      f_{j}(x_{ij})   ) can be done using the backfitting algorithm proposed by Andreas Buja, Trevor Hastie and Robert Tibshirani (1989). 2  See also   Generalized additive model  Backfitting algorithm  Projection pursuit regression  Generalized additive model for location, scale, and shape (GAMLSS)  Median polish   References  Further reading   Breiman, L. and Friedman, J.H. (1985). "Estimating Optimal Transformations for Multiple Regression and Correlation", Journal of the American Statistical Association 80:580–598.   "  Category:Regression analysis  Category:Nonparametric regression     Friedman, J.H. and Stuetzle, W. (1981). "Projection Pursuit Regression", Journal of the American Statistical Association 76:817–823. ↩  Buja, A., Hastie, T., and Tibshirani, R. (1989). "Linear Smoothers and Additive Models", The Annals of Statistics 17(2):453–555. ↩     