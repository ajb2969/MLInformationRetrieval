   Law of total expectation      Law of total expectation   The proposition in probability theory known as the law of total expectation , 1 the law of iterated expectations , the tower rule , the smoothing theorem , and Adam's Law among other names, states that if X is an integrable random variable (i.e., a random variable satisfying E( | X | ) \operatorname{E} (X) = \operatorname{E}_Y ( \operatorname{E}_{X \mid Y} ( X \mid Y)),  i.e., the expected value of the conditional expected value of X given Y is the same as the expected value of X .  (The conditional expected value E( X | Y ) is a random variable in its own right, whose value depends on the value of Y . Notice that the conditional expected value of X given the event  Y = y is a function of y (this is where adherence to the conventional, rigidly case-sensitive notation of probability theory becomes important!). If we write E( X | Y = y ) = g ( y ) then the random variable E( X | Y ) is just g ( Y ).  One special case states that if     A  1   ,   A  2   ,  …  ,   A  n       subscript  A  1    subscript  A  2   normal-…   subscript  A  n     A_{1},A_{2},\ldots,A_{n}   is a partition of the whole outcome space, i.e. these events are mutually exclusive and exhaustive, then        E   (  X  )    =    ∑   i  =  1   n     E   (  X  ∣   A  i   )     P   (   A  i   )       .       normal-E  X     superscript   subscript     i  1    n      normal-E  X   subscript  A  i     normal-P   subscript  A  i        \operatorname{E}(X)=\sum_{i=1}^{n}{\operatorname{E}(X\mid A_{i})\operatorname{%
 P}(A_{i})}.     Example  Suppose that two factories supply light bulbs to the market. Factory X 's bulbs work for an average of 5000 hours, whereas factory Y 's bulbs work for an average of 4000 hours. It is known that factory X supplies 60% of the total bulbs available. What is the expected length of time that a purchased bulb will work for?  Applying the law of total expectation, we have:       E   (  L  )    =     E   (  L  ∣  X  )     P   (  X  )     +    E   (  L  ∣  Y  )     P   (  Y  )      =    5000   (  .6  )    +   4000   (  .4  )     =  4600         normal-E  L        normal-E  L  X    normal-P  X       normal-E  L  Y    normal-P  Y              5000  .6     4000  .4         4600     \operatorname{E}(L)=\operatorname{E}(L\mid X)\operatorname{P}(X)+\operatorname%
 {E}(L\mid Y)\operatorname{P}(Y)=5000(.6)+4000(.4)=4600     where       E   (  L  )      normal-E  L    \operatorname{E}(L)   is the expected life of the bulb;       Pr   (  X  )    =   6  10        Pr  X     6  10     \Pr(X)={6\over 10}   is the probability that the purchased bulb was manufactured by factory X ;       Pr   (  Y  )    =   4  10        Pr  Y     4  10     \Pr(Y)={4\over 10}   is the probability that the purchased bulb was manufactured by factory Y ;       E   (  L  ∣  X  )    =  5000       normal-E  L  X   5000    \operatorname{E}(L\mid X)=5000   is the expected lifetime of a bulb manufactured by X ;       E   (  L  ∣  Y  )    =  4000       normal-E  L  Y   4000    \operatorname{E}(L\mid Y)=4000   is the expected lifetime of a bulb manufactured by Y .   Thus each purchased light bulb has an expected lifetime of 4600 hours.  Proof in the discrete case       E  Y    (    E   X  ∣  Y     (  X  ∣  Y  )    )       subscript  normal-E  Y     subscript  normal-E   fragments  X  normal-∣  Y    X  Y     \displaystyle\operatorname{E}_{Y}\left(\operatorname{E}_{X\mid Y}(X\mid Y)\right)     Proof in the general case  The general statement of the result makes reference to a probability space     (  Ω  ,  ℱ  ,  P  )     normal-Ω  ℱ  P    (\Omega,\mathcal{F},P)   on which two sub    σ   σ   \sigma   -algebras      𝒢  1   ⊆   𝒢  2   ⊆  ℱ         subscript  𝒢  1    subscript  𝒢  2        ℱ     \mathcal{G}_{1}\subseteq\mathcal{G}_{2}\subseteq\mathcal{F}   are defined. For a random variable   X   X   X   on such a space, the smoothing law states that        E   [   E   [  X  ∣   𝒢  2   ]    ∣   𝒢  1   ]    =   E   [  X  ∣   𝒢  1   ]     .       normal-E   normal-E  X   subscript  𝒢  2     subscript  𝒢  1     normal-E  X   subscript  𝒢  1      \operatorname{E}[\operatorname{E}[X\mid\mathcal{G}_{2}]\mid\mathcal{G}_{1}]=%
 \operatorname{E}[X\mid\mathcal{G}_{1}].   Since a conditional expectation is a Radon–Nikodym derivative , verifying the following two properties establishes the smoothing law:        E   [   E   [  X  ∣   𝒢  2   ]    ∣   𝒢  1   ]    is   𝒢  1        normal-E   normal-E  X   subscript  𝒢  2     subscript  𝒢  1    is   subscript  𝒢  1     \operatorname{E}[\operatorname{E}[X\mid\mathcal{G}_{2}]\mid\mathcal{G}_{1}]%
 \mbox{ is }\mathcal{G}_{1}   - measurable  \int_{G_1} \operatorname{E}[ \operatorname{E}[X \mid \mathcal{G}_2] \mid \mathcal{G}_1] dP = \int_{G_1} X dP   \mbox{ holds for all } G_1 \in \mathcal{G}_2  The first of these properties holds by the definition of the conditional expectation, and the second holds since     G  1   ∈   𝒢  1   ⊆   𝒢  2          subscript  G  1    subscript  𝒢  1         subscript  𝒢  2      G_{1}\in\mathcal{G}_{1}\subseteq\mathcal{G}_{2}   implies         ∫   G  1      E   [   E   [  X  ∣   𝒢  2   ]    ∣   𝒢  1   ]    d  P    =    ∫   G  1      E   [  X  ∣   𝒢  2   ]    d  P    =    ∫   G  1     X  d  P     .          subscript    subscript  G  1       normal-E   normal-E  X   subscript  𝒢  2     subscript  𝒢  1    d  P      subscript    subscript  G  1       normal-E  X   subscript  𝒢  2    d  P           subscript    subscript  G  1      X  d  P       \int_{G_{1}}\operatorname{E}[\operatorname{E}[X\mid\mathcal{G}_{2}]\mid%
 \mathcal{G}_{1}]dP=\int_{G_{1}}\operatorname{E}[X\mid\mathcal{G}_{2}]dP=\int_{%
 G_{1}}XdP.     In the special case that     𝒢  1   =   {  ,  Ω  }      fragments   subscript  𝒢  1     fragments  normal-{  normal-,  Ω  normal-}     \mathcal{G}_{1}=\{,\Omega\}   and     𝒢  2   =   σ   (  Y  )         subscript  𝒢  2     σ  Y     \mathcal{G}_{2}=\sigma(Y)   , the smoothing law reduces to the statement        E   [   E   [  X  ∣  Y  ]    ]    =   E   [  X  ]     .       normal-E   normal-E  X  Y     normal-E  X     \operatorname{E}[\operatorname{E}[X\mid Y]]=\operatorname{E}[X].     Notation without indices  When using the expectation operator   E   normal-E   \operatorname{E}   , adding indices to the operator may lead to cumbersome notations and these indices are often omitted. In the case of iterated expectations    E   (   E   (  X  ∣  Y  )    )      normal-E   normal-E  X  Y     \operatorname{E}\left(\operatorname{E}(X\mid Y)\right)   stands for     E  Y    (    E   X  ∣  Y     (  X  ∣  Y  )    )       subscript  normal-E  Y     subscript  normal-E   fragments  X  normal-∣  Y    X  Y     \operatorname{E}_{Y}\left(\operatorname{E}_{X\mid Y}(X\mid Y)\right)   . The innermost expectation is the conditional expectation of   X   X   X   given   Y   Y   Y   , and the outermost expectation is taken with respect to the conditioning variable   Y   Y   Y   . This convention is notably used in the rest of this article.  Iterated expectations with nested conditioning sets  The following formulation of the law of iterated expectations plays an important role in many economic and finance models:        E   (  X  ∣   I  1   )    =   E   (   E   (  X  ∣   I  2   )    ∣   I  1   )     ,       normal-E  X   subscript  I  1     normal-E   normal-E  X   subscript  I  2     subscript  I  1      \operatorname{E}(X\mid I_{1})=\operatorname{E}(\operatorname{E}(X\mid I_{2})%
 \mid I_{1}),     where the value of I 2 is determined by that of I 1 . To build intuition, imagine an investor who forecasts a random stock price X based on the limited information set I 1 . The law of iterated expectations says that the investor can never gain a more precise forecast of X by conditioning on more specific information ( I 2 ), if the more specific forecast must itself be forecast with the original information ( I 1 ).  This formulation is often applied in a time series context, where E t denotes expectations conditional on only the information observed up to and including time period t . In typical models the information set t + 1 contains all information available through time t , plus additional information revealed at time t + 1. One can then write: 2         E  t    (  X  )    =    E  t    (    E   t  +  1     (  X  )    )     .        subscript  normal-E  t   X     subscript  normal-E  t     subscript  normal-E    t  1    X      \operatorname{E}_{t}(X)=\operatorname{E}_{t}(\operatorname{E}_{t+1}(X)).     See also   The fundamental theorem of poker for one practical application.  Law of total probability  Law of total variance  Law of total covariance   References    (Theorem 34.4)  Christopher Sims , "Notes on Random Variables, Expectations, Probability Densities, and Martingales" , especially equations (16) through (18)   "  Category:Algebra of random variables  Category:Theory of probability distributions  Category:Statistical laws     ↩  ↩     