   Optimal computing budget allocation      Optimal computing budget allocation   '''Optimal computing budget allocation (OCBA) ''' is a concept first introduced in the mid 1990s by Dr Chun-Hung Chen. This approach intends to maximize the overall simulation efficiency for finding an optimal decision. 1 Simply put, OCBA is an approach to simulation that will help determine the number of replication and/or the simulation time that is needed in order to receive acceptable/best results within a set of given parameters. 2 This is accomplished by using an asymptotic framework to analyze the structure of the optimal allocation. 3  Intuitive explanations  OCBA’s goal is to provide a systematic approach to run a large number of simulations including only the critical alternatives in order to select the best alternative. In other words, focusing on only part the most critical alternatives, which minimizes computation time and reduces these critical estimators’ variances. The expected result maintains the required level of accuracy, while requiring less amount of work. 4 For example we can create a simple simulation between 5 alternatives. The goal is to select an alternative with minimum average delay time. The figure below shows preliminary simulation results ( i.e. having run only a fraction of the required number of simulation replications). It is clear to see that alternative 2 and 3 have a significantly lower delay time ( highlighted in red). In order to save computation cost (which is time, resources and money spend on the process of running the simulation) OCBA suggests that more replications are required for alternative 2 and 3, and simulation can be stopped for 1, 4, and 5 much earlier without compromising results.  framed|Observing the above graphic, it is clear that alternative 2 and 3 have the lowest cost. OCBA suggests to run further simulations on only alternatives 2 and 3 in order to minimize computation cost  What problem does OCBA intend to solve  The main objective of OCBA is to maximize the probability of correct selection (PCS). PCS is subject to the sampling budget of a given stage of sampling τ .       max    τ  1   ,   τ  2   ,  …  ,   τ  k     PCS      subscript     subscript  τ  1    subscript  τ  2   normal-…   subscript  τ  k     PCS    \displaystyle\max_{\tau_{1},\tau_{2},\ldots,\tau_{k}}\mathrm{PCS}     In this case      ∑   i  =  1   k    τ  i    =  τ        superscript   subscript     i  1    k    subscript  τ  i    τ    \sum_{i=1}^{k}\tau_{i}=\tau   stands for the total computational cost. 5  Main OCBA results  The main results of OCBA is done by taking into consideration an asymptotic case. In this case ( τ ) begins to increase toward ∞ in such a way that the total sampling budget β increases to ∞ and τ i also increases to n i ). Using Equation 1 from section "What problem does OCBA intend to solve" PCS can be maximized by the following 2 Equations.  (insert equations 1 and 2 here – the article needs to be published first in order to add copyrighted images – I have permission by the author to publish the specific images .  Something interesting to notice is that Equation 2 suggests that the number of replications for each alternative i is proportional to the square noise-to-signal ratio. To clarify noise is referring to the sample Standard deviation and signal is referring to the difference between i's sample mean and the best sample mean.  One numerical testing example  OCBA was put to the test when numerical results were published using a simple example that is shown in figure 1. The goal is to allocate 31 parallel servers within a two-stage queuing system. A constraint in this example is that each queue (p1, p2) can have no less than 11 servers. Mathematically put: p 1 + p 2 = 31, p 1 > 11 and p 2 > 11.  Given this information we can see that there are 10 alternative computations ( p 1 , p 2 ). The goal is to minimize the customer wait time for the first 100 customers. In order to express the performance of different procedures, we introduce a function β computational budget (i.e. resources used such as time, power) which we will vary between 200 and 8000 for all of the sequential procedures. The estimated PCS is a function of β. PCS is estimated by the fraction of the event of correct selection out of the independent experiment that were conducted. The results of varying β and its corresponding PCS are shown in Figure 2. FIGURE 2 goes here – the article needs to be published first in order to add copyrighted images – I have permission by the author to publish the specific images  Table 2 shows the numerical results of Figure 2. The sampling budget to attain a PCS of 0.95 and 0.99 is compared using OCBA allocation and equal allocation.      PCS   OCBA   Equal allocation     0.95   470   1450     0.99   850   2890     Next, we are doing a separate experiment with multiple alternatives that are called k. For example we can now have more than 31 servers. We can observe the speedup factor of reaching a desired level of PCS. In this case it is .99. Table 3 shows the speedup factor using OCBA compared with the equal allocation method. The Speedup factor is calculated by β_EA/ β_OCBA. We can see that as the number of alternatives increases so does the speedup factor. This is how computation time is saved when performing simulations with large number of alternatives.      Number of alternatives ( k )   4   10   20   50   75   100     Speedup factor   1.75   3.42   6.45   12.8   16.3   19.8     6  Some extensions of OCBA  Experts in the field explain that in some problems it is important to not only know the best alternative among a sample, but the top 5, 10, or even 50, because the decision maker may have other concerns that may affect the decision which are not modeled in the simulation. According to Szechtman and Yücesan (2008), 7 OCBA is also helpful in feasibility determination problems. This is where the decisions makers are only interested in differentiating feasible alternatives from the infeasible ones. Further, choosing an alternative that is simpler, yet similar in performance is crucial for other decision makers. In this case, the best choice is among top-r simplest alternatives, whose performance rank above desired levels. 8 In addition, Trailovic 9 and Pao 10 (2004) demonstrate an OCBA approach, where we find alternatives with minimum variance, instead of with best mean. Here, we assume unknown variances, voiding the OCBA rule (assuming that the variances are known). During 2010 research was done on an OCBA algorithm that is based on a t distribution. The results show no significant differences between those from t-distribution and normal distribution. The above presented extensions of OCBA is not a complete list and is yet to be fully explored and compiled. 11  Multi-objective Optimal Computing Budget Allocation  Multi-objective Optimal Computing Budget Allocation (MOCBA) is the OCBA concept that applies to multi-objective problems. In a typical MOCBA, the PCS is defined as        Pr   {   C  S   }    ≡   Pr   {    (    ⋂   i  ∈   S  p      E  i    )    ⋂   (    ⋂   i  ∈    S  ¯   p      E  i  c    )     }     ,       Pr    C  S     Pr      subscript     i   subscript  S  p      subscript  E  i        subscript     i   subscript   normal-¯  S   p      superscript   subscript  E  i   c         \Pr\{CS\}\equiv\Pr\left\{\left(\bigcap_{i\in S_{p}}E_{i}\right)\bigcap\left(%
 \bigcap_{i\in\overline{S}_{p}}E_{i}^{c}\right)\right\},     in which       S  p     subscript  S  p    S_{p}   is the observed Pareto set,       S  ¯   p     subscript   normal-¯  S   p    \overline{S}_{p}   is the non-Pareto set, i.e.,      S  ¯   p   =   Θ  \   S  p         subscript   normal-¯  S   p    normal-\  normal-Θ   subscript  S  p      \overline{S}_{p}=\Theta\backslash S_{p}   ,      E  i     subscript  E  i    E_{i}   is the event that design   i   i   i   is non-dominated by all other designs,      E  i  c     superscript   subscript  E  i   c    E_{i}^{c}   is the event that design   i   i   i   is dominated by at least one design.   We notice that, the Type I error    e  1     subscript  e  1    e_{1}   and Type II error    e  2     subscript  e  2    e_{2}   for identifying a correct Pareto set are respectively       e  1   =   1  -   Pr   {    ⋂   i  ∈    S  ¯   p      E  i  c    }          subscript  e  1     1   Pr    subscript     i   subscript   normal-¯  S   p      superscript   subscript  E  i   c        e_{1}=1-\Pr\left\{\bigcap_{i\in\overline{S}_{p}}E_{i}^{c}\right\}   and     e  2   =   1  -   Pr   {    ⋂   i  ∈   S  p      E  i    }          subscript  e  2     1   Pr    subscript     i   subscript  S  p      subscript  E  i        e_{2}=1-\Pr\left\{\bigcap_{i\in S_{p}}E_{i}\right\}   .  Besides, it can be proven that       e  1   ≤   u   b  1    =    H   |    S  ¯   p   |    -   H    ∑   i  ∈    S  ¯   p       max    j  ∈  Θ   ,   j  ≠  i      [    min   l  ∈   1  ,  …  ,  H      Pr   {     J  ~    j  l    ≤    J  ~    i  l     }     ]              subscript  e  1     u   subscript  b  1             H     subscript   normal-¯  S   p       H    subscript     i   subscript   normal-¯  S   p       subscript    formulae-sequence    j  normal-Θ     j  i       subscript     l   1  normal-…  H      Pr     subscript   normal-~  J     j  l     subscript   normal-~  J     i  l              e_{1}\leq ub_{1}=H\left|\overline{S}_{p}\right|-H\sum_{i\in\overline{S}_{p}}{%
 \max_{j\in\Theta,j\neq i}\left[\min_{l\in{1,\ldots,H}}\Pr\left\{\tilde{J}_{jl}%
 \leq\tilde{J}_{il}\right\}\right]}     and        e  2   ≤   u   b  2    =    (   k  -  1   )     ∑   i  ∈   S  p       max    j  ∈  Θ   ,   j  ≠  i      [    min   l  ∈   1  ,  …  ,  H      Pr   {     J  ~    j  l    ≤    J  ~    i  l     }     ]       ,         subscript  e  2     u   subscript  b  2             k  1     subscript     i   subscript  S  p       subscript    formulae-sequence    j  normal-Θ     j  i       subscript     l   1  normal-…  H      Pr     subscript   normal-~  J     j  l     subscript   normal-~  J     i  l             e_{2}\leq ub_{2}=(k-1)\sum_{i\in S_{p}}\max_{j\in\Theta,j\neq i}\left[\min_{l%
 \in{1,\ldots,H}}\Pr\left\{\tilde{J}_{jl}\leq\tilde{J}_{il}\right\}\right],     where   H   H   H   is the number of objectives, and     J  ~    i  l      subscript   normal-~  J     i  l     \tilde{J}_{il}   follows posterior distribution     N  o  r  m  a  l   (    J  ¯    i  l    ,    σ   i  l   2    N  i    )    .      N  o  r  m  a  l    subscript   normal-¯  J     i  l       superscript   subscript  σ    i  l    2    subscript  N  i       Normal\left(\bar{J}_{il},\frac{\sigma_{il}^{2}}{N_{i}}\right).   Noted that     J  ¯    i  l      subscript   normal-¯  J     i  l     \bar{J}_{il}   and    σ   i  l      subscript  σ    i  l     \sigma_{il}   are the average and standard deviation of the observed performance measures for objective   l   l   l   of design   i   i   i   , and    N  i     subscript  N  i    N_{i}   is the number of observations.  Thus, instead of maximizing    Pr   {   C  S   }      Pr    C  S     \Pr\{CS\}   , we can maximize its lower bound, i.e.,       A  P  C  S   -  M   ≡   1  -   u   b  1    -   u   b  2      .          A  P  C  S   M     1    u   subscript  b  1      u   subscript  b  2       APCS{-}M\equiv 1-ub_{1}-ub_{2}.   Assuming    τ  →  ∞     normal-→  τ     \tau\rightarrow\infty   , the Lagrange method can be applied to conclude the following rules:        τ  i   =     β  i      ∑   j  ∈  Θ      β  j     τ    ,       subscript  τ  i        subscript  β  i     subscript     j  normal-Θ     subscript  β  j     τ     \tau_{i}=\frac{\beta_{i}}{\sum_{j\in\Theta}\beta_{j}}\tau,     in which   for a design    h  ∈   S  A       h   subscript  S  A     h\in S_{A}   ,     β  h   =     (     σ  ^    h   l   j  h   h    2   +     σ  ^     j  h    l   j  h   h    2   /   ρ  h     )   /   δ   h   j  h    l   j  h   h    2      (     σ  ^    m   l   j  m   m    2   +     σ  ^     j  m    l   j  m   m    2   /   ρ  m     )   /   δ   m   j  m    l   j  m   m    2          subscript  β  h          subscript   superscript   normal-^  σ   2     h   superscript   subscript  l   subscript  j  h    h        subscript   superscript   normal-^  σ   2      subscript  j  h    superscript   subscript  l   subscript  j  h    h      subscript  ρ  h      subscript   superscript  δ  2     h   subscript  j  h    superscript   subscript  l   subscript  j  h    h           subscript   superscript   normal-^  σ   2     m   superscript   subscript  l    j  m    m        subscript   superscript   normal-^  σ   2      subscript  j  m    superscript   subscript  l    j  m    m      subscript  ρ  m      subscript   superscript  δ  2     m   subscript  j  m    superscript   subscript  l   subscript  j  m    m         \beta_{h}=\frac{\left(\hat{\sigma}^{2}_{hl_{j_{h}}^{h}}+\hat{\sigma}^{2}_{j_{h%
 }l_{j_{h}}^{h}}/\rho_{h}\right)/{\delta^{2}_{hj_{h}l_{j_{h}}^{h}}}}{\left(\hat%
 {\sigma}^{2}_{ml_{jm}^{m}}+\hat{\sigma}^{2}_{j_{m}l_{jm}^{m}}/\rho_{m}\right)/%
 {\delta^{2}_{mj_{m}l_{j_{m}}^{m}}}}   ,    for a design    d  ∈   S  B       d   subscript  S  B     d\in S_{B}   ,     β  d   =     ∑   i  ∈   Θ  d  *        σ   d   l  d  i    2    σ   i   l  d  i    2     β  i  2           subscript  β  d       subscript     i   superscript   subscript  normal-Θ  d            subscript   superscript  σ  2     d   superscript   subscript  l  d   i      subscript   superscript  σ  2     i   superscript   subscript  l  d   i       superscript   subscript  β  i   2        \beta_{d}=\sqrt{\sum_{i\in\Theta_{d}^{*}}\frac{\sigma^{2}_{dl_{d}^{i}}}{\sigma%
 ^{2}_{il_{d}^{i}}}\beta_{i}^{2}}   ,   and      δ   i  j  l    =     J  ¯    j  l    -    J  ¯    i  l      ,       subscript  δ    i  j  l       subscript   normal-¯  J     j  l     subscript   normal-¯  J     i  l       \delta_{ijl}=\bar{J}_{jl}-\bar{J}_{il},         j  i   ≡    arg   max    j  ∈  Θ   ,   j  ≠  i        ∏   l  =  1   H    Pr   {     J  ~    j  l    ≤    J  ~    i  l     }       ,       subscript  j  i        subscript    formulae-sequence    j  normal-Θ     j  i        superscript   subscript  product    l  1    H    Pr     subscript   normal-~  J     j  l     subscript   normal-~  J     i  l          j_{i}\equiv\arg\max_{j\in\Theta,j\neq i}\prod_{l=1}^{H}{\Pr\left\{\tilde{J}_{%
 jl}\leq\tilde{J}_{il}\right\}},         l   j  i   i   ≡   arg    min   l  ∈   1  ,  …  ,  H      Pr   {     J  ~    j  l    ≤    J  ~    i  l     }       ,       superscript   subscript  l   subscript  j  i    i       subscript     l   1  normal-…  H      Pr     subscript   normal-~  J     j  l     subscript   normal-~  J     i  l          l_{j_{i}}^{i}\equiv\arg\min_{l\in{1,\ldots,H}}\Pr\left\{\tilde{J}_{jl}\leq%
 \tilde{J}_{il}\right\},         S  A   ≡   {    d  e  s  i  g   n   h   ∈  S   ∣     δ   h   j  h    l   j  h   h    2       σ  ^    h   l   j  h   h    2    α  h    +     σ  ^     j  h    l   j  h   h    2    α   j  h       <    min   i  ∈   Θ  h       δ   i  h   l  h  i    2       σ  ^    i   l  h  i    2    α  i    +     σ  ^    h   l  h  i    2    α  h        }    ,       subscript  S  A    conditional-set      d  e  s  i  g  n  h   S        subscript   superscript  δ  2     h   subscript  j  h    subscript   superscript  l  h    subscript  j  h           subscript   superscript   normal-^  σ   2     h   subscript   superscript  l  h    subscript  j  h       subscript  α  h       subscript   superscript   normal-^  σ   2      subscript  j  h    subscript   superscript  l  h    subscript  j  h       subscript  α   subscript  j  h         subscript     i   subscript  normal-Θ  h        subscript   superscript  δ  2     i  h   subscript   superscript  l  i   h          subscript   superscript   normal-^  σ   2     i   subscript   superscript  l  i   h      subscript  α  i       subscript   superscript   normal-^  σ   2     h   subscript   superscript  l  i   h      subscript  α  h           S_{A}\equiv\left\{design\;h\in S\mid\frac{\delta^{2}_{hj_{h}l^{h}_{j_{h}}}}{%
 \frac{\hat{\sigma}^{2}_{hl^{h}_{j_{h}}}}{\alpha_{h}}+\frac{\hat{\sigma}^{2}_{j%
 _{h}l^{h}_{j_{h}}}}{\alpha_{j_{h}}}}<\min_{i\in\Theta_{h}}\frac{\delta^{2}_{%
 ihl^{i}_{h}}}{\frac{\hat{\sigma}^{2}_{il^{i}_{h}}}{\alpha_{i}}+\frac{\hat{%
 \sigma}^{2}_{hl^{i}_{h}}}{\alpha_{h}}}\right\},         S  B   ≡   S  \   S  A     ,       subscript  S  B    normal-\  S   subscript  S  A      S_{B}\equiv S\backslash S_{A},        Θ  h   =  i  |  i  ∈  S  ,   j  i   =  h  ,     fragments   subscript  normal-Θ  h    i  normal-|  i   S  normal-,   subscript  j  i    h  normal-,    \Theta_{h}={i|i\in S,j_{i}=h},        Θ  d  *   =  h  |  h  ∈   S  A   ,   j  h   =  d  ,     fragments   superscript   subscript  normal-Θ  d      h  normal-|  h    subscript  S  A   normal-,   subscript  j  h    d  normal-,    \Theta_{d}^{*}={h|h\in S_{A},j_{h}=d},         ρ  i   =    α   j  i    /   α  i     .       subscript  ρ  i      subscript  α   subscript  j  i     subscript  α  i      \rho_{i}=\alpha_{j_{i}}/\alpha_{i}.     Optimal Computing Budget Allocation for Constrained Optimization  Similar to the previous section, there are many situations with multiple performance measures. If the multiple performance measures are equally important, the decision makers can use the MOCBA. In other situations, the decision makers have one primary performance measure to be optimized while the secondary performance measures are constrained by certain limits. The primary performance measure can be called the main objective while the secondary performance measures are referred as the constraint measures. This falls into the problem of constrained optimization. When the number of alternatives is fixed, the problem is called constrained ranking and selection where the goal is to select the best feasible design given that both the main objective and the constraint measures need to be estimated via stochastic simulation. The OCBA method for constrained optimization (called OCBA-CO) can be found in Pujowidianto et al. (2009) 12 and Lee et al. (2012). 13  The key change is in the definition of PCS. There are two components in constrained optimisation, namely optimality and feasibility. As a result, the simulation budget can be allocated to each non-best design either based on the optimality or feasibility. In other word, a non-best design will not be wrongly selected as the best feasible design if it remains either infeasible or worse than the true best feasible design. The idea is that it is not necessary to spend a large portion of the budget to determine the feasibility if the design is clearly worse than the best. Similarly, we can save the budget by allocating based on feasibility if the design is already better than the best in terms of the main objective.  Web-based demonstration of OCBA  The following link provides an OCBA demonstration using a simple example. In the demo, you will see how OCBA performs and allocates computing budget differently as compared with traditional equal allocation approach.   http://seor.gmu.edu/~cchen9/OCBA/welcome.html   References  External links   http://seor.gmu.edu/~cchen9/ocba.html  Russian translation of OCBA page by Carschimp  Ukrainian translation of OCBA page by sciposts   "  Category:Stochastic optimization     Fu, M, C. H. Chen, and L. Shi, “Some Topics for Simulation Optimization,” Proceedings of 2008 Winter Simulation Conference, pp. 27–38, Miami, FL, December 2008. ↩  Chen, and Loo H. Lee. Stochastic simulation optimization an optimal computing budget allocation. Singapore Hackensack, NJ: World Scientific, 2011. Print.. ↩  Chen, C. H. "An Effective Approach to Smartly Allocate Computing Budget for Discrete Event Simulation," Proceedings of the 34th IEEE Conference on Decision and Control, pp. 2598–2605, December 1995. ↩  ↩  Chen, and Loo H. Lee. Stochastic simulation optimization an optimal computing budget allocation. Singapore Hackensack, NJ: World Scientific, 2011. Print. ↩  Chen, C. H., M. Fu, L. Shi, and L. H. Lee, “Stochastic Systems Simulation Optimization,” Frontiers of Electrical and Electronic Engineering in China, 6(3), 468–480, 2011 ↩  Szechtman R, Yücesan E (2008) A new perspective on feasibility determination. Proc of the 2008 Winter Simul Conf 273–280 ↩  Jia QS, Zhou E, Chen CH (2012). efficient computing budget allocation for finding simplest good designs. IIE Trans, To Appear. ↩  Trailovic Tekin E, Sabuncuoglu I (2004) Simulation optimization: A comprehensive review on theory and applications. IIE Trans 36:1067–1081 ↩  Trailovic L, Pao LY (2004) Computing budget allocation for efficient ranking and selection of variances with application to target tracking algorithms, IEEE Trans Autom Control 49:58–67. ↩   Pujowidianto NA, Lee LH, Chen CH, Yap CM (2009) Optimal computing budget allocation for constrained optimization. Proc of the 2009 Winter Simul Conf 584–589. ↩  Lee LH, Pujowidianto NA, Li LW, Chen CH, Yap CM (2012) Approximation simulation budget allocation for selecting the best design in the presence of stochastic constraints, IEEE Trans Autom Control 57:2940–2945. ↩     