   Good–Turing frequency estimation      Good–Turing frequency estimation   Good–Turing frequency estimation is a statistical technique for estimating the probability of encountering an object of a hitherto unseen species, given a set of past observations of objects from different species. (In drawing balls from an urn, the 'objects' would be balls and the 'species' would be the distinct colors of the balls (finite but unknown in number). After drawing    R  red     subscript  R  red    R_{\text{red}}   red balls,    R  black     subscript  R  black    R_{\text{black}}   black balls and    R  green     subscript  R  green    R_{\text{green}}   green balls, we would ask what is the probability of drawing a red ball, a black ball, a green ball or one of a previously unseen color.  Historical background  Good–Turing frequency estimation was developed by Alan Turing and his assistant I. J. Good as part of their efforts at Bletchley Park to crack German  ciphers for the Enigma machine during World War II . Turing at first modeled the frequencies as a multinomial distribution , but found it inaccurate. Good developed smoothing algorithms to improve the estimator's accuracy.  The discovery was recognized as significant when published by Good in 1953, 1 but the calculations were difficult so it was not used as widely as it might have been. 2 The method even gained some literary fame due to the Robert Harris novel Enigma .  In the 1990s, Geoffrey Sampson worked with William A. Gale of AT&T; , to create and implement a simplified and easier-to-use variant of the Good–Turing method 3 described below.  The method  First notation and some required data structures are defined:   Assuming that X distinct species have been observed, numbered x = 1, ..., X .  Then the frequency vector,     R  ¯     normal-¯  R    \bar{R}    , has elements    R  x     subscript  R  x    R_{x}   that give the number of individuals that have been observed for species x .  The frequency of frequencies vector,     (   N  r   )    r  =   0  ,  1  ,  …       subscript   subscript  N  r     r   0  1  normal-…      (N_{r})_{r=0,1,\ldots}   , shows how many times the frequency r occurs in the vector R ; i.e. among the elements    R  x     subscript  R  x    R_{x}   .        N  r   =   |   {  x  ∣    R  x   =  r   }   |        subscript  N  r      conditional-set  x     subscript  R  x   r       N_{r}=|\{x\mid R_{x}=r\}|     For example    N  1     subscript  N  1    N_{1}   is the number of species for which only one individual was observed. Note that the total number of objects observed, N , can be found from       N  =    ∑   r  =  1   ∞    r   N  r      .      N    superscript   subscript     r  1        r   subscript  N  r       N=\sum_{r=1}^{\infty}rN_{r}.     The first step in the calculation is to find an estimate of the total probability of unseen species. This estimate is 4        p  0   =    N  1   N    .       subscript  p  0      subscript  N  1   N     p_{0}=\frac{N_{1}}{N}.     The next step is to find an estimate of probability for species which were seen r times. For a single species this estimate is:        p  r   =     (   r  +  1   )   S   (   N   r  +  1    )     N  S   (   N  r   )      .       subscript  p  r         r  1   S   subscript  N    r  1       N  S   subscript  N  r       p_{r}=\frac{(r+1)S(N_{r+1})}{NS(N_{r})}.   To estimate a probability of encountering any species from this group (i.e., the group of species seen r times) one can use the following formula:         (   r  +  1   )   S   (   N   r  +  1    )    N   .          r  1   S   subscript  N    r  1     N    \frac{(r+1)S(N_{r+1})}{N}.   Here, the notation    S   (  )       S     S()   means the smoothed or adjusted value of the frequency shown in parenthesis (see also empirical Bayes method ). An overview of how to perform this smoothing follows.  We would like to make a plot of    log   N  r        subscript  N  r     \log N_{r}   versus    log  r      r    \log r   but this is problematic because for large r many    N  r     subscript  N  r    N_{r}   will be zero. Instead a revised quantity,    log   Z  r        subscript  Z  r     \log Z_{r}   , is plotted versus    log  r      r    \log r   , where Z r is defined as        Z  r   =    N  r    0.5   (   t  -  q   )      ,       subscript  Z  r      subscript  N  r     0.5    t  q       Z_{r}=\frac{N_{r}}{0.5(t-q)},     and where q , r and t are consecutive subscripts having     N  q   ,   N  r   ,   N  t       subscript  N  q    subscript  N  r    subscript  N  t     N_{q},N_{r},N_{t}   non-zero. When r is 1, take q to be 0. When r is the last non-zero frequency, take t to be 2 r − q .  The assumption of Good–Turing estimation is that the number of occurrence for each species follows a binomial distribution. 5  A simple linear regression is then fitted to the log–log plot. For small values of r it is reasonable to set     S   (   N  r   )    =   N  r         S   subscript  N  r     subscript  N  r     S(N_{r})=N_{r}   (that is, no smoothing is performed), while for large values of r , values of    S   (   N  r   )       S   subscript  N  r     S(N_{r})   are read off the regression line. An automatic procedure (not described here) can be used to specify at what point the switch from no smoothing to linear smoothing should take place. 6 Code for the method is available in the public domain. 7  See also   Ewens sampling formula  Pseudocount   References   David A. McAllester, Robert Schapire (2000) ''On the Convergence Rate of Good–Turing Estimators' ', Proceedings of the Thirteenth Annual Conference on Computational Learning Theory pp. 1–6  David A. McAllester, Ortiz, Luis (2003) ''Concentration Inequalities for the Missing Mass and for Histogram Rule Error' ', Journal of Machine Learning Research pp. 895–911   "  Category:Categorical data  Category:Probability assessment     ↩  Newsise: Scientists Explain and Improve Upon 'Enigmatic' Probability Formula , a popular review of ↩  Sampson, Geoffrey and Gale, William A. (1995) Good‐turing frequency estimation without tears ↩  ↩  Lecture 11: The Good–Turing Estimate. CS 6740, Cornell University, 2010 1 ↩  ↩  Sampson, Geoffrey (2005) Simple Good–Turing Frequency Estimator (code in C) ↩     