   Dual total correlation      Dual total correlation   In information theory , dual total correlation (Han 1978) or excess entropy (Olbrich 2008) is one of the two known non-negative generalizations of mutual information . While total correlation is bounded by the sum entropies of the n elements, the dual total correlation is bounded by the joint-entropy of the n elements. Although well behaved, dual total correlation has received much less attention than the total correlation. A measure known as "TSE-complexity" defines a continuum between the total correlation and dual total correlation (Ay 2001).  Definition  (Figure)  Venn diagram of information theoretic measures for three variables x, y, and z. The dual total correlation is represented by the union of the three mutual informations and is shown in the diagram by the yellow, magenta, cyan, and gray regions.   For a set of n  random variables     {   X  1   ,  …  ,   X  n   }      subscript  X  1   normal-…   subscript  X  n     \{X_{1},\ldots,X_{n}\}   , the dual total correlation    D   (   X  1   ,  …  ,   X  n   )       D    subscript  X  1   normal-…   subscript  X  n      D(X_{1},\ldots,X_{n})   is given by      D   (   X  1   ,  …  ,   X  n   )   =  H   (   X  1   ,  …  ,   X  n   )   -   ∑   i  =  1   n   H   (   X  i   |   X  1   ,  …  ,   X   i  -  1    ,   X   i  +  1    ,  …  ,   X  n   )   ,     fragments  D   fragments  normal-(   subscript  X  1   normal-,  normal-…  normal-,   subscript  X  n   normal-)    H   fragments  normal-(   subscript  X  1   normal-,  normal-…  normal-,   subscript  X  n   normal-)     superscript   subscript     i  1    n   H   fragments  normal-(   subscript  X  i   normal-|   subscript  X  1   normal-,  normal-…  normal-,   subscript  X    i  1    normal-,   subscript  X    i  1    normal-,  normal-…  normal-,   subscript  X  n   normal-)   normal-,    D(X_{1},\ldots,X_{n})=H\left(X_{1},\ldots,X_{n}\right)-\sum_{i=1}^{n}H\left(X_%
 {i}|X_{1},\ldots,X_{i-1},X_{i+1},\ldots,X_{n}\right),     where    H   (   X  1   ,  …  ,   X  n   )       H    subscript  X  1   normal-…   subscript  X  n      H(X_{1},\ldots,X_{n})   is the joint entropy of the variable set    {   X  1   ,  …  ,   X  n   }      subscript  X  1   normal-…   subscript  X  n     \{X_{1},\ldots,X_{n}\}   and    H   (   X  i   |  …  )      fragments  H   fragments  normal-(   subscript  X  i   normal-|  normal-…  normal-)     H(X_{i}|...)   is the conditional entropy of variable    X  i     subscript  X  i    X_{i}   , given the rest.  Normalized  The dual total correlation normalized between [0,1] is simply the dual total correlation divided by its maximum value    H   (   X  1   ,  …  ,   X  n   )       H    subscript  X  1   normal-…   subscript  X  n      H(X_{1},\ldots,X_{n})   ,        N  D   (   X  1   ,  …  ,   X  n   )    =    D   (   X  1   ,  …  ,   X  n   )     H   (   X  1   ,  …  ,   X  n   )      .        N  D    subscript  X  1   normal-…   subscript  X  n         D    subscript  X  1   normal-…   subscript  X  n       H    subscript  X  1   normal-…   subscript  X  n        ND(X_{1},\ldots,X_{n})=\frac{D(X_{1},\ldots,X_{n})}{H(X_{1},\ldots,X_{n})}.     Bounds  Dual total correlation is non-negative and bounded above by the joint entropy    H   (   X  1   ,  …  ,   X  n   )       H    subscript  X  1   normal-…   subscript  X  n      H(X_{1},\ldots,X_{n})   .       0  ≤   D   (   X  1   ,  …  ,   X  n   )    ≤   H   (   X  1   ,  …  ,   X  n   )     .        0    D    subscript  X  1   normal-…   subscript  X  n            H    subscript  X  1   normal-…   subscript  X  n        0\leq D(X_{1},\ldots,X_{n})\leq H(X_{1},\ldots,X_{n}).     Secondly, Dual total correlation has a close relationship with total correlation,    C   (   X  1   ,  …  ,   X  n   )       C    subscript  X  1   normal-…   subscript  X  n      C(X_{1},\ldots,X_{n})   . In particular,         C   (   X  1   ,  …  ,   X  n   )     n  -  1    ≤   D   (   X  1   ,  …  ,   X  n   )    ≤    (   n  -  1   )   C   (   X  1   ,  …  ,   X  n   )     .            C    subscript  X  1   normal-…   subscript  X  n       n  1      D    subscript  X  1   normal-…   subscript  X  n              n  1   C    subscript  X  1   normal-…   subscript  X  n        \frac{C(X_{1},\ldots,X_{n})}{n-1}\leq D(X_{1},\ldots,X_{n})\leq(n-1)\;C(X_{1},%
 \ldots,X_{n}).     Relation to other quantities  In measure theoretic terms, by the definition of dual total correlation:      D   (   X  1   ,  …  ,   X  n   )   =  μ   (   ⋃  i     X  ~   i   \   (   ⋃  j     X  ~   j   \   ⋃   k  ≠  j      X  ~   k   )   )   )     fragments  D   fragments  normal-(   subscript  X  1   normal-,  normal-…  normal-,   subscript  X  n   normal-)    μ   fragments  normal-(   subscript   i    subscript   normal-~  X   i   normal-\   fragments  normal-(   subscript   j    subscript   normal-~  X   j   normal-\   subscript     k  j     subscript   normal-~  X   k   normal-)   normal-)   normal-)    D(X_{1},\ldots,X_{n})=\mu\left(\bigcup_{i}\tilde{X}_{i}\backslash\left(\bigcup%
 _{j}\tilde{X}_{j}\backslash\bigcup_{k\neq j}\tilde{X}_{k})\right)\right)     which is equal to the union of the pairwise mutual informations:       D   (   X  1   ,  …  ,   X  n   )    =   μ   (    ⋃  i     ⋃   j  ≠  i     (     X  ~   i   ∩    X  ~   j    )     )          D    subscript  X  1   normal-…   subscript  X  n       μ    subscript   i     subscript     j  i       subscript   normal-~  X   i    subscript   normal-~  X   j         D(X_{1},\ldots,X_{n})=\mu\left(\bigcup_{i}\bigcup_{j\neq i}\left(\tilde{X}_{i}%
 \cap\tilde{X}_{j}\right)\right)     History  Han (1978) originally defined the dual total correlation as,       D    (   X  1   ,  …  ,   X  n   )       D    subscript  X  1   normal-…   subscript  X  n      \displaystyle{}\qquad D(X_{1},\ldots,X_{n})   However Abdallah and Plumbley (2010) showed its equivalence to the easier-to-understand form of the joint entropy minus the sum of conditional entropies via the following:       D    (   X  1   ,  …  ,   X  n   )       D    subscript  X  1   normal-…   subscript  X  n      \displaystyle{}\qquad D(X_{1},\ldots,X_{n})     See also   Mutual information  Total correlation   References   Han T. S. (1978). Nonnegative entropy measures of multivariate symmetric correlations, Information and Control  36 , 133–156.  Fujishige Satoru (1978). Polymatroidal Dependence Structure of a Set of Random Variables, Information and Control  39 , 55–72. .  Olbrich, E. and Bertschinger, N. and Ay, N. and Jost, J. (2008). How should complexity scale with system size?, The European Physical Journal B - Condensed Matter and Complex Systems . .  Abdallah S. A. and Plumbley, M. D. (2010). A measure of statistical complexity based on predictive information, ArXiv e-prints . .    Nihat Ay, E. Olbrich, N. Bertschinger (2001). A unifying framework for complexity measures of finite systems. European Conference on Complex Systems. pdf .   "  Category:Information theory  Category:Probability theory  Category:Statistical dependence   