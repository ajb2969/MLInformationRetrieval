   Conditional independence      Conditional independence   In probability theory , two events R and B are conditionally independent given a third event Y precisely if the occurrence or non-occurrence of R  and the occurrence or non-occurrence of B are independent events in their conditional probability distribution given Y . In other words, R and B are conditionally independent given Y if and only if, given knowledge that Y occurs, knowledge of whether R occurs provides no information on the likelihood of B occurring, and knowledge of whether B occurs provides no information on the likelihood of R occurring.  Formal definition  [[Image:Conditional independence.svg|thumb|450px|These are two examples illustrating conditional independence . Each cell represents a possible outcome. The events R , B and Y are represented by the areas shaded red, blue and yellow respectively. The overlap between the events R and B is shaded purple. The probabilities of these events are shaded areas with respect to the total area. In both examples R and B are conditionally independent given Y because:       Pr   (   R  ∩  B   ∣  Y  )    =    Pr   (  R  ∣  Y  )     Pr   (  B  ∣  Y  )          Pr    R  B   Y      Pr  R  Y    Pr  B  Y      \Pr(R\cap B\mid Y)=\Pr(R\mid Y)\Pr(B\mid Y)\,    1 but not conditionally independent given not Y because:        Pr   (   R  ∩  B   ∣   not  Y   )    ≠    Pr   (  R  ∣   not  Y   )     Pr   (  B  ∣   not  Y   )      .       Pr    R  B     not  Y       Pr  R    not  Y     Pr  B    not  Y       \Pr(R\cap B\mid\text{not }Y)\not=\Pr(R\mid\text{not }Y)\Pr(B\mid\text{not }Y).\,   ]]  In the standard notation of probability theory, R and B are conditionally independent given Y if and only if        Pr   (   R  ∩  B   ∣  Y  )    =    Pr   (  R  ∣  Y  )     Pr   (  B  ∣  Y  )      ,       Pr    R  B   Y      Pr  R  Y    Pr  B  Y      \Pr(R\cap B\mid Y)=\Pr(R\mid Y)\Pr(B\mid Y),\,     or equivalently,        Pr   (  R  ∣   B  ∩  Y   )    =   Pr   (  R  ∣  Y  )     .       Pr  R    B  Y     Pr  R  Y     \Pr(R\mid B\cap Y)=\Pr(R\mid Y).\,     Two random variables  X and Y are conditionally independent given a third random variable Z if and only if they are independent in their conditional probability distribution given Z . That is, X and Y are conditionally independent given Z if and only if, given any value of Z , the probability distribution of X is the same for all values of Y and the probability distribution of Y is the same for all values of X .  Two events R and B are conditionally independent given a σ-algebra Σ if         Pr   (   R  ∩  B   ∣  Σ  )    =    Pr   (  R  ∣  Σ  )     Pr   (  B  ∣  Σ  )    a    .  s   .     formulae-sequence     Pr    R  B   normal-Σ      Pr  R  normal-Σ    Pr  B  normal-Σ   a    s    \Pr(R\cap B\mid\Sigma)=\Pr(R\mid\Sigma)\Pr(B\mid\Sigma)\ a.s.     where    Pr   (  A  ∣  Σ  )      Pr  A  normal-Σ    \Pr(A\mid\Sigma)   denotes the conditional expectation of the indicator function of the event   A   A   A   ,    χ  A     subscript  χ  A    \chi_{A}   , given the sigma algebra   Σ   normal-Σ   \Sigma   . That is,        Pr   (  A  ∣  Σ  )    :=   E   [   χ  A   ∣  Σ  ]     .     assign   Pr  A  normal-Σ    normal-E   subscript  χ  A   normal-Σ     \Pr(A\mid\Sigma):=\operatorname{E}[\chi_{A}\mid\Sigma].     Two random variables X and Y are conditionally independent given a σ-algebra Σ if the above equation holds for all R in σ( X ) and B in σ( Y ).  Two random variables X and Y are conditionally independent given a random variable W if they are independent given σ( W ): the σ-algebra generated by W . This is commonly written:      X  ⟂  ⟂  Y  ∣  W     fragments  X  perpendicular-to  perpendicular-to  Y  normal-∣  W    X\perp\!\!\!\perp Y\mid W   or      X  ⟂  Y  ∣  W     fragments  X  perpendicular-to  Y  normal-∣  W    X\perp Y\mid W     This is read "X is independent of Y, given W"; the conditioning applies to the whole statement: "(X is independent of Y) given W".       (  X  ⟂  ⟂  Y  )   ∣  W     fragments   fragments  normal-(  X  perpendicular-to  perpendicular-to  Y  normal-)   normal-∣  W    (X\perp\!\!\!\perp Y)\mid W     If W assumes a countable set of values, this is equivalent to the conditional independence of X and Y for the events of the form [ W = w ]. Conditional independence of more than two events, or of more than two random variables, is defined analogously.  The following two examples show that X  Y  neither implies nor is implied by  X  Y | W . First, suppose W is 0 with probability 0.5 and is the value 1 otherwise. When W = 0 take X and Y to be independent, each having the value 0 with probability 0.99 and the value 1 otherwise. When W = 1, X and Y are again independent, but this time they take the value 1 with probability 0.99. Then X  Y | W . But X and Y are dependent, because Pr( X = 0)  0.5. For the second example, suppose X  Y , each taking the values 0 and 1 with probability 0.5. Let W be the product X Y . Then when W = 0, Pr( X = 0) = 2/3, but Pr( X = 0| Y = 0) = 1/2, so X  Y | W is false. This is also an example of Explaining Away. See Kevin Murphy's tutorial 2 where X and Y take the values "brainy" and "sporty".  Uses in Bayesian inference  Let p be the proportion of voters who will vote "yes" in an upcoming referendum . In taking an opinion poll , one chooses n voters randomly from the population. For i = 1, ..., n , let X i = 1 or 0 according as the i th chosen voter will or will not vote "yes".  In a frequentist approach to statistical inference one would not attribute any probability distribution to p (unless the probabilities could be somehow interpreted as relative frequencies of occurrence of some event or as proportions of some population) and one would say that X 1 , ..., X n are independent random variables.  By contrast, in a Bayesian approach to statistical inference, one would assign a probability distribution to p regardless of the non-existence of any such "frequency" interpretation, and one would construe the probabilities as degrees of belief that p is in any interval to which a probability is assigned. In that model, the random variables X 1 , ..., X n are not independent, but they are conditionally independent given the value of p . In particular, if a large number of the X s are observed to be equal to 1, that would imply a high conditional probability, given that observation, that p is near 1, and thus a high conditional probability, given that observation, that the next  X to be observed will be equal to 1.  Rules of conditional independence  A set of rules governing statements of conditional independence have been derived from the basic definition. 3 4  Note: since these implications hold for any probability space, they will still hold if one considers a sub-universe by conditioning everything on another variable, say K . For example,    X  ⟂  ⟂  Y  ⇒  Y  ⟂  ⟂  X     fragments  X  perpendicular-to  perpendicular-to  Y  normal-⇒  Y  perpendicular-to  perpendicular-to  X    X\perp\!\!\!\perp Y\Rightarrow Y\perp\!\!\!\perp X   would also mean that    X  ⟂  ⟂  Y  ∣  K  ⇒  Y  ⟂  ⟂  X  ∣  K     fragments  X  perpendicular-to  perpendicular-to  Y  normal-∣  K  normal-⇒  Y  perpendicular-to  perpendicular-to  X  normal-∣  K    X\perp\!\!\!\perp Y\mid K\Rightarrow Y\perp\!\!\!\perp X\mid K   .  Note: below, the comma can be read as an "AND".  Symmetry      X  ⟂  ⟂  Y  ⇒  Y  ⟂  ⟂  X     fragments  X  perpendicular-to  perpendicular-to  Y   normal-⇒   Y  perpendicular-to  perpendicular-to  X    X\perp\!\!\!\perp Y\quad\Rightarrow\quad Y\perp\!\!\!\perp X     Decomposition      X  ⟂  ⟂  A  ,  B  ⇒  and   {      X  ⟂  ⟂  A         X  ⟂  ⟂  B           fragments  X  perpendicular-to  perpendicular-to  A  normal-,  B   normal-⇒   and   cases   fragments  X  perpendicular-to  perpendicular-to  A   otherwise   fragments  X  perpendicular-to  perpendicular-to  B   otherwise     X\perp\!\!\!\perp A,B\quad\Rightarrow\quad\text{ and }\begin{cases}X\perp\!\!%
 \!\perp A\\
 X\perp\!\!\!\perp B\end{cases}     Proof:     p_{X,A,B}(x,a,b) = p_X(x) p_{A,B}(a,b)       (meaning of    X  ⟂   A  ,  B      perpendicular-to  X   A  B     X\perp A,B   )     \int_{B} \! p_{X,A,B}(x,a,b) = \int_{B} \! p_X(x) p_{A,B}(a,b)       (ignore variable B by integrating it out)     p_{X,A}(x,a) = p_X(x) p_A(a)  A similar proof shows the independence of X and B .  Weak union      X  ⟂  ⟂  A  ,  B  ⇒  X  ⟂  ⟂  A  ∣  B     fragments  X  perpendicular-to  perpendicular-to  A  normal-,  B   normal-⇒   X  perpendicular-to  perpendicular-to  A  normal-∣  B    X\perp\!\!\!\perp A,B\quad\Rightarrow\quad X\perp\!\!\!\perp A\mid B     Contraction          X  ⟂  ⟂  A  ∣  B        X  ⟂  ⟂  B      }  and  ⇒  X  ⟂  ⟂  A  ,  B     fragments     fragments  X  perpendicular-to  perpendicular-to  A  normal-∣  B      fragments  X  perpendicular-to  perpendicular-to  B     normal-}  and   normal-⇒   X  perpendicular-to  perpendicular-to  A  normal-,  B    \left.\begin{aligned}\displaystyle X\perp\!\!\!\perp A\mid B\\
 \displaystyle X\perp\!\!\!\perp B\end{aligned}\right\}\text{ and }\quad%
 \Rightarrow\quad X\perp\!\!\!\perp A,B     Contraction-weak-union-decomposition  Putting the above three together, we have:          X  ⟂  ⟂  A  ∣  B        X  ⟂  ⟂  B      }  and  ⇔  X  ⟂  ⟂  A  ,  B  ⇒  and   {      X  ⟂  ⟂  A  ∣  B         X  ⟂  ⟂  B         X  ⟂  ⟂  B  ∣  A         X  ⟂  ⟂  A           fragments     fragments  X  perpendicular-to  perpendicular-to  A  normal-∣  B      fragments  X  perpendicular-to  perpendicular-to  B     normal-}  and   iff   X  perpendicular-to  perpendicular-to  A  normal-,  B   normal-⇒   and   cases   fragments  X  perpendicular-to  perpendicular-to  A  normal-∣  B   otherwise   fragments  X  perpendicular-to  perpendicular-to  B   otherwise   fragments  X  perpendicular-to  perpendicular-to  B  normal-∣  A   otherwise   fragments  X  perpendicular-to  perpendicular-to  A   otherwise     \left.\begin{aligned}\displaystyle X\perp\!\!\!\perp A\mid B\\
 \displaystyle X\perp\!\!\!\perp B\end{aligned}\right\}\text{ and }\quad\iff%
 \quad X\perp\!\!\!\perp A,B\quad\Rightarrow\quad\text{ and }\begin{cases}X%
 \perp\!\!\!\perp A\mid B\\
 X\perp\!\!\!\perp B\\
 X\perp\!\!\!\perp B\mid A\\
 X\perp\!\!\!\perp A\\
 \end{cases}     Intersection  If the probabilities of X , A , B are all positive, then the following also holds:          X  ⟂  ⟂  A  ∣  B        X  ⟂  ⟂  B  ∣  A      }  and  ⇒  X  ⟂  ⟂  A  ,  B     fragments     fragments  X  perpendicular-to  perpendicular-to  A  normal-∣  B      fragments  X  perpendicular-to  perpendicular-to  B  normal-∣  A     normal-}  and   normal-⇒   X  perpendicular-to  perpendicular-to  A  normal-,  B    \left.\begin{aligned}\displaystyle X\perp\!\!\!\perp A\mid B\\
 \displaystyle X\perp\!\!\!\perp B\mid A\end{aligned}\right\}\text{ and }\quad%
 \Rightarrow\quad X\perp\!\!\!\perp A,B     The five rules above were termed " Graphoid Axioms" by Pearl and Paz, 5 because they hold in graphs, if    X  ⟂  ⟂  A  ∣  B     fragments  X  perpendicular-to  perpendicular-to  A  normal-∣  B    X\perp\!\!\!\perp A\mid B   is interpreted to mean: "All paths from X to A are intercepted by the set B ". 6  See also   Graphoid  Conditional dependence  de Finetti's theorem  Conditional expectation   References  "  Category:Probability theory  Category:Statistical dependence     To see that this is the case, one needs to realise that Pr( R ∩ B | Y ) is the probability of an overlap of R and B (the purple shaded area) in the Y area. Since, in the picture on the left, there are two squares where R and B overlap within the Y area, and the Y area has twelve squares, Pr( R ∩ B | Y ) =    2  12      2  12    \frac{2}{12}   =    1  6      1  6    \frac{1}{6}   . Similarly, Pr( R | Y ) =    4  12      4  12    \frac{4}{12}   =    1  3      1  3    \frac{1}{3}   and Pr( B | Y ) =    6  12      6  12    \frac{6}{12}   =    1  2      1  2    \frac{1}{2}   . ↩  http://people.cs.ubc.ca/~murphyk/Bayes/bnintro.html ↩  ↩  J Pearl, Causality: Models, Reasoning, and Inference, 2000, Cambridge University Press ↩  ↩  ↩     