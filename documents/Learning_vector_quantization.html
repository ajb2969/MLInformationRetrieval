<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="767">Learning vector quantization</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Learning vector quantization</h1>
<hr/>

<p>In <a href="computer_science" title="wikilink">computer science</a>, <strong>learning vector quantization (LVQ)</strong>, is a <a href="prototype" title="wikilink">prototype-based</a> <a href="supervised_learning" title="wikilink">supervised</a> <a href="Statistical_classification" title="wikilink">classification</a> <a class="uri" href="algorithm" title="wikilink">algorithm</a>. LVQ is the supervised counterpart of <a href="vector_quantization" title="wikilink">vector quantization</a> systems.</p>
<h2 id="overview">Overview</h2>

<p>LVQ can be understood as a special case of an <a href="artificial_neural_network" title="wikilink">artificial neural network</a>, more precisely, it applies a <a href="Winner-take-all_(computing)" title="wikilink">winner-take-all</a> <a href="Hebbian_learning" title="wikilink">Hebbian learning</a>-based approach. It is a precursor to <a href="Self-organizing_map" title="wikilink">Self-organizing maps</a> (SOM) and related to <a href="Neural_gas" title="wikilink">Neural gas</a>, and to the <a href="k-Nearest_Neighbor_algorithm" title="wikilink">k-Nearest Neighbor algorithm</a> (k-NN). LVQ was invented by <a href="Teuvo_Kohonen" title="wikilink">Teuvo Kohonen</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>

<p>An LVQ system is represented by prototypes W=(w(i),...,w(n)) which are defined in the <a href="feature_space" title="wikilink">feature space</a> of observed data. In <a href="Winner-take-all_(computing)" title="wikilink">winner-take-all</a> training algorithms one determines, for each data point, the prototype which is closest to the input according to a given distance measure. The position of this so-called winner prototype is then adapted, i.e. the winner is moved closer if it correctly classifies the data point or moved away if it classifies the data point incorrectly.</p>

<p>An advantage of LVQ is that it creates prototypes that are easy to interpret for experts in the respective application domain.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> LVQ systems can be applied to multi-class classification problems in a natural way. It is used in a variety of practical applications, see <a class="uri" href="http://liinwww.ira.uka.de/bibliography/Neural/SOM.LVQ.html">http://liinwww.ira.uka.de/bibliography/Neural/SOM.LVQ.html</a> for an extensive bibliography.</p>

<p>A key issue in LVQ is the choice of an appropriate measure of distance or similarity for training and classification. Recently, techniques have been developed which adapt a parameterized distance measure in the course of training the system, see e.g. (Schneider, Biehl, and Hammer, 2009) <a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> and references therein.</p>

<p>LVQ can be a source of great help in classifying text documents.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<h2 id="algorithm">Algorithm</h2>

<p>Below follows an informal description.<br/>
The algorithm is consisted by 3 basic steps. The algorithm's input is:</p>
<ul>
<li>how many neurons the system will have 

<math display="inline" id="Learning_vector_quantization:0">
 <semantics>
  <mi>M</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>M</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   M
  </annotation>
 </semantics>
</math>

</li>
<li>what weight each neuron has 

<math display="inline" id="Learning_vector_quantization:1">
 <semantics>
  <mover accent="true">
   <msub>
    <mi>W</mi>
    <mi>i</mi>
   </msub>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>W</ci>
     <ci>i</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{W_{i}}
  </annotation>
 </semantics>
</math>

 for 

<math display="inline" id="Learning_vector_quantization:2">
 <semantics>
  <mrow>
   <mi>i</mi>
   <mo>=</mo>
   <mrow>
    <mn>0</mn>
    <mo>,</mo>
    <mn>1</mn>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <mrow>
     <mi>M</mi>
     <mo>-</mo>
     <mn>1</mn>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>i</ci>
    <list>
     <cn type="integer">0</cn>
     <cn type="integer">1</cn>
     <ci>normal-…</ci>
     <apply>
      <minus></minus>
      <ci>M</ci>
      <cn type="integer">1</cn>
     </apply>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i=0,1,...,M-1
  </annotation>
 </semantics>
</math>

</li>
<li>how fast the neurons are learning 

<math display="inline" id="Learning_vector_quantization:3">
 <semantics>
  <mi>η</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>η</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \eta
  </annotation>
 </semantics>
</math>

.</li>
<li>and an input list containing vectors to train the neurons 

<math display="inline" id="Learning_vector_quantization:4">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

</li>
</ul>

<p>The algorithm's flow is:</p>
<ol>
<li>For next input 

<math display="inline" id="Learning_vector_quantization:5">
 <semantics>
  <mover accent="true">
   <mi>X</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{X}
  </annotation>
 </semantics>
</math>

 in 

<math display="inline" id="Learning_vector_quantization:6">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

 find the neuron 

<math display="inline" id="Learning_vector_quantization:7">
 <semantics>
  <mover accent="true">
   <mrow>
    <mi>W</mi>
    <mi>m</mi>
   </mrow>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <apply>
     <times></times>
     <ci>W</ci>
     <ci>m</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{Wm}
  </annotation>
 </semantics>
</math>

 at which 

<math display="inline" id="Learning_vector_quantization:8">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>X</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo>,</mo>
    <mover accent="true">
     <msub>
      <mi>W</mi>
      <mi>m</mi>
     </msub>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>d</ci>
    <interval closure="open">
     <apply>
      <ci>normal-→</ci>
      <ci>X</ci>
     </apply>
     <apply>
      <ci>normal-→</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>W</ci>
       <ci>m</ci>
      </apply>
     </apply>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d(\vec{X},\vec{W_{m}})
  </annotation>
 </semantics>
</math>

 gets its minimum value, where 

<math display="inline" id="Learning_vector_quantization:9">
 <semantics>
  <mpadded lspace="1.7pt" width="+3.4pt">
   <mi>d</mi>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \,d\,
  </annotation>
 </semantics>
</math>

 is the metric used ( <a href="Euclidean_distance" title="wikilink">Euclidean</a>, etc. ).</li>
<li>Update 

<math display="inline" id="Learning_vector_quantization:10">
 <semantics>
  <mover accent="true">
   <msub>
    <mi>W</mi>
    <mi>m</mi>
   </msub>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>W</ci>
     <ci>m</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{W_{m}}
  </annotation>
 </semantics>
</math>

. A better explanation is get 

<math display="inline" id="Learning_vector_quantization:11">
 <semantics>
  <mover accent="true">
   <msub>
    <mi>W</mi>
    <mi>m</mi>
   </msub>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>W</ci>
     <ci>m</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{W_{m}}
  </annotation>
 </semantics>
</math>

 closer to the input 

<math display="inline" id="Learning_vector_quantization:12">
 <semantics>
  <mover accent="true">
   <mi>X</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{X}
  </annotation>
 </semantics>
</math>

.<br/>


<math display="inline" id="Learning_vector_quantization:13">
 <semantics>
  <mrow>
   <mover accent="true">
    <msub>
     <mi>W</mi>
     <mi>m</mi>
    </msub>
    <mo stretchy="false">→</mo>
   </mover>
   <mo>←</mo>
   <mrow>
    <mover accent="true">
     <msub>
      <mi>W</mi>
      <mi>m</mi>
     </msub>
     <mo stretchy="false">→</mo>
    </mover>
    <mo>+</mo>
    <mrow>
     <mi>η</mi>
     <mo>⋅</mo>
     <mrow>
      <mo>(</mo>
      <mrow>
       <mover accent="true">
        <mi>X</mi>
        <mo stretchy="false">→</mo>
       </mover>
       <mo>-</mo>
       <mover accent="true">
        <msub>
         <mi>W</mi>
         <mi>m</mi>
        </msub>
        <mo stretchy="false">→</mo>
       </mover>
      </mrow>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-←</ci>
    <apply>
     <ci>normal-→</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>W</ci>
      <ci>m</ci>
     </apply>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <ci>normal-→</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>W</ci>
       <ci>m</ci>
      </apply>
     </apply>
     <apply>
      <ci>normal-⋅</ci>
      <ci>η</ci>
      <apply>
       <minus></minus>
       <apply>
        <ci>normal-→</ci>
        <ci>X</ci>
       </apply>
       <apply>
        <ci>normal-→</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>W</ci>
         <ci>m</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{W_{m}}\leftarrow\vec{W_{m}}+\eta\cdot\left(\vec{X}-\vec{W_{m}}\right)
  </annotation>
 </semantics>
</math>

.</li>
<li>While there are vectors left in 

<math display="inline" id="Learning_vector_quantization:14">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

 go to step 1, else terminate.</li>
</ol>

<p>Note

<math display="block" id="Learning_vector_quantization:15">
 <semantics>
  <mover accent="true">
   <msub>
    <mi>W</mi>
    <mi>i</mi>
   </msub>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>W</ci>
     <ci>i</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{W_{i}}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Learning_vector_quantization:16">
 <semantics>
  <mover accent="true">
   <mi>X</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{X}
  </annotation>
 </semantics>
</math>

 are <a href="vector_space" title="wikilink">vectors</a> in feature space.<br/>
A more formal description can be found here: <a class="uri" href="http://jsalatas.ictpro.gr/implementation-of-competitive-learning-networks-for-weka/">http://jsalatas.ictpro.gr/implementation-of-competitive-learning-networks-for-weka/</a></p>
<h2 id="references">References</h2>
<references>
</references>
<ul>
<li><a href="http://www.cis.hut.fi/panus/papers/dtwsom.pdf">Self-Organizing Maps and Learning Vector Quantization for Feature Sequences, Somervuo and Kohonen. 2004</a> (pdf)</li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://wekaclassalgos.sourceforge.net/">LVQ for WEKA</a>: Implementation of LVQ variants (LVQ1, OLVQ1, LVQ2.1, LVQ3, OLVQ3) for the WEKA Machine Learning Workbench.</li>
<li><a href="http://www.cis.hut.fi/research/lvq_pak/">lvq_pak</a> official release (1996) by Kohonen and his team</li>
<li><a href="http://jsalatas.ictpro.gr/weka/">LVQ for WEKA</a>: Another implementation of LVQ in Java for the WEKA Machine Learning Workbench.</li>
</ul>

<p>"</p>

<p><a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a> <a href="Category:Classification_algorithms" title="wikilink">Category:Classification algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">T. Kohonen. Self-Organizing Maps. Springer, Berlin, 1997.<a href="#fnref1">↩</a></li>
<li id="fn2">T. Kohonen. Learning vector quantization. In: M.A. Arbib, editor, The Handbook of Brain Theory and Neural Networks., pages 537–540. MIT Press, Cambridge, MA, 1995.<a href="#fnref2">↩</a></li>
<li id="fn3">P. Schneider, B. Hammer, and M. Biehl. Adaptive Relevance Matrices in Learning Vector Quantization.Neural Computation 21: 3532–3561, 2009. <a class="uri" href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.2009.10-08-892">http://www.mitpressjournals.org/doi/abs/10.1162/neco.2009.10-08-892</a><a href="#fnref3">↩</a></li>
<li id="fn4">Fahad and Sikander. Classification of textual documents using learning vector quantization. Information Technology Journal 6.1 (2007): 154-159. <a class="uri" href="http://198.170.104.138/itj/2007/154-159.pdf">http://198.170.104.138/itj/2007/154-159.pdf</a><a href="#fnref4">↩</a></li>
</ol>
</section>
</body>
</html>
