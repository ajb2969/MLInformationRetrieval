   Mixed model      Mixed model   A mixed model is a statistical model containing both fixed effects and random effects . These models are useful in a wide variety of disciplines in the physical, biological and social sciences. They are particularly useful in settings where repeated measurements are made on the same statistical units ( longitudinal study ), or where measurements are made on clusters of related statistical units. Because of their advantage in dealing with missing values, mixed effects models are often preferred over more traditional approaches such as repeated measures ANOVA .  History and current status  Ronald Fisher introduced random effects models to study the correlations of trait values between relatives. 1 In the 1950s, Charles Roy Henderson provided best linear unbiased estimates (BLUE) of fixed effects and best linear unbiased predictions (BLUP) of random effects. 2 3 4 5 Subsequently, mixed modeling has become a major area of statistical research, including work on computation of maximum likelihood estimates, non-linear mixed effect models, missing data in mixed effects models, and Bayesian estimation of mixed effects models. Mixed models are applied in many disciplines where multiple correlated measurements are made on each unit of interest. They are prominently used in research involving human and animal subjects in fields ranging from genetics to marketing, and have also been used in industrial statistics.  Definition  In matrix notation a mixed model can be represented as      ğ’š  =    X  ğœ·   +   Z  ğ’–   +  Ïµ       ğ’š      X  ğœ·     Z  ğ’–   bold-italic-Ïµ     \boldsymbol{y}=X\boldsymbol{\beta}+Z\boldsymbol{u}+\boldsymbol{\epsilon}     where      ğ’š   ğ’š   \boldsymbol{y}   is a known vector of observations, with mean     E   (  ğ’š  )    =   X  ğœ·         E  ğ’š     X  ğœ·     E(\boldsymbol{y})=X\boldsymbol{\beta}   ;       ğœ·   ğœ·   \boldsymbol{\beta}   is an unknown vector of fixed effects;       ğ’–   ğ’–   \boldsymbol{u}   is an unknown vector of random effects, with mean     E   (  ğ’–  )    =  ğŸ        E  ğ’–   0    E(\boldsymbol{u})=\boldsymbol{0}   and variance-covariance matrix      var   (  ğ’–  )    =  G       var  ğ’–   G    \operatorname{var}(\boldsymbol{u})=G   ;       Ïµ   bold-italic-Ïµ   \boldsymbol{\epsilon}   is an unknown vector of random errors, with mean     E   (  Ïµ  )    =  ğŸ        E  bold-italic-Ïµ   0    E(\boldsymbol{\epsilon})=\boldsymbol{0}   and variance     var   (  Ïµ  )    =  R       var  bold-italic-Ïµ   R    \operatorname{var}(\boldsymbol{\epsilon})=R   ;       X   X   X   and   Z   Z   Z   are known design matrices relating the observations   ğ’š   ğ’š   \boldsymbol{y}   to   ğœ·   ğœ·   \boldsymbol{\beta}   and   ğ’–   ğ’–   \boldsymbol{u}   , respectively.   Estimation  The joint density of   ğ’š   ğ’š   \boldsymbol{y}   and   ğ’–   ğ’–   \boldsymbol{u}   can be written as    f   (  ğ’š  ,  ğ’–  )   =  f   (  ğ’š  |  ğ’–  )   f   (  ğ’–  )      fragments  f   fragments  normal-(  y  normal-,  u  normal-)    f   fragments  normal-(  y  normal-|  u  normal-)   f   fragments  normal-(  u  normal-)     f(\boldsymbol{y},\boldsymbol{u})=f(\boldsymbol{y}|\boldsymbol{u})\,f(%
 \boldsymbol{u})   . Assuming normality,    ğ’–  âˆ¼   ğ’©   (  ğŸ  ,  G  )       similar-to  ğ’–    ğ’©   0  G      \boldsymbol{u}\sim\mathcal{N}(\boldsymbol{0},G)   ,    Ïµ  âˆ¼   ğ’©   (  ğŸ  ,  R  )       similar-to  bold-italic-Ïµ    ğ’©   0  R      \boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0},R)   and     C  o  v   (  ğ’–  ,  Ïµ  )    =  ğŸ        C  o  v   ğ’–  bold-italic-Ïµ    0    Cov(\boldsymbol{u},\boldsymbol{\epsilon})=\boldsymbol{0}   , and maximizing the joint density for   ğœ·   ğœ·   \boldsymbol{\beta}   and   ğ’–   ğ’–   \boldsymbol{u}   , gives Henderson's "mixed model equations" (MME): 6 7 8        (       X  â€²    R   -  1    X       X  â€²    R   -  1    Z         Z  â€²    R   -  1    X        Z  â€²    R   -  1    Z   +   G   -  1        )    (      ğœ·  ^        ğ’–  ^      )    =   (       X  â€²    R   -  1    ğ’š         Z  â€²    R   -  1    ğ’š      )              superscript  X  normal-â€²    superscript  R    1    X      superscript  X  normal-â€²    superscript  R    1    Z        superscript  Z  normal-â€²    superscript  R    1    X        superscript  Z  normal-â€²    superscript  R    1    Z    superscript  G    1          normal-^  ğœ·      normal-^  ğ’–           superscript  X  normal-â€²    superscript  R    1    ğ’š        superscript  Z  normal-â€²    superscript  R    1    ğ’š       \begin{pmatrix}X^{\prime}R^{-1}X&X^{\prime}R^{-1}Z\\
 Z^{\prime}R^{-1}X&Z^{\prime}R^{-1}Z+G^{-1}\end{pmatrix}\begin{pmatrix}\hat{%
 \boldsymbol{\beta}}\\
 \hat{\boldsymbol{u}}\end{pmatrix}=\begin{pmatrix}X^{\prime}R^{-1}\boldsymbol{y%
 }\\
 Z^{\prime}R^{-1}\boldsymbol{y}\end{pmatrix}     The solutions to the MME,    ğœ·  ^     normal-^  ğœ·    \textstyle\hat{\boldsymbol{\beta}}   and    ğ’–  ^     normal-^  ğ’–    \textstyle\hat{\boldsymbol{u}}   are best linear unbiased estimates (BLUE) and predictors (BLUP) for   ğœ·   ğœ·   \boldsymbol{\beta}   and   ğ’–   ğ’–   \boldsymbol{u}   , respectively. This is a consequence of the Gauss-Markov theorem when the conditional variance of the outcome is not scalable to the identity matrix. When the conditional variance is known, then the inverse variance weighted least squares estimate is BLUE. However, the conditional variance is rarely, if ever, known. So it is desirable to jointly estimate the variance and weighted parameter estimates when solving MMEs.  One method used to fit such mixed models is that of the EM algorithm 9 where the variance components are treated as unobserved nuisance parameters in the joint likelihood. Currently, this is the implemented method for the major statistical software packages R (lme in the nlme library) and SAS (proc mixed). The solution to the mixed model equations is a maximum likelihood estimate when the distribution of the errors is normal. 10 11  See also   Fixed effects model  Generalized linear mixed model  Linear regression  Mixed-design analysis of variance  Multilevel model  Random effects model  Repeated measures design   References  Further reading   Milliken, G. A., & Johnson, D. E. (1992). Analysis of messy data: Vol. I. Designed experiments . New York: Chapman & Hall.    West, B. T., Welch, K. B., & Galecki, A. T. (2007). Linear mixed models: A practical guide using statistical software. New York: Chapman & Hall/CRC.   Commercial   NCSS (statistical software) includes longitudinal mixed models analysis.   "  Category:Statistical methods  Category:Regression analysis  Category:Analysis of variance     â†©  â†©  â†©  â†©  â†©  â†©    â†©  â†©  Garrett M. Fitzmaurice , Nan M. Laird , and James H. Ware , 2004. Applied Longitudinal Analysis . John Wiley & Sons, Inc., p. 326-328. â†©     