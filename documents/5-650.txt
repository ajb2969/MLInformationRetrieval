   Sensor fusion      Sensor fusion  '''Sensor fusion''' is the combining of [[sensor]]y data or data derived from sensory data from di sparate sources such that the resulting information has less uncertainty than would be possible when these sources were used individually. The term uncertainty reduction in this case can mean more accurate, more complete, or more dependable, or refer to the result of an emerging view, such as stereoscopic vision (calculation of depth information by combining two-dimensional images from two cameras at slightly different viewpoints). 1 2  The data sources for a fusion process are not specified to originate from identical sensors. One can distinguish direct fusion , indirect fusion and fusion of the outputs of the former two. Direct fusion is the fusion of sensor data from a set of heterogeneous or homogeneous sensors, soft sensors , and history values of sensor data, while indirect fusion uses information sources like a priori knowledge about the environment and human input.  Sensor fusion is also known as (multi-sensor) Data fusion and is a subset of information fusion .  Sensory fusion is simply defined as the unification of visual excitations from corresponding retinal images into a single visual perception a single visual image. Single vision is the hallmark of retinal correspondence Double vision is the hallmark of retinal disparity  Examples of sensors   Radar  Sonar and other acoustic  Infra-red / thermal imaging camera  TV cameras  Sonobuoys  Seismic sensors  Magnetic sensors  Electronic Support Measures (ESM)  Phased array  MEMS  Accelerometers  Global Positioning System (GPS)   Sensor fusion algorithms  Sensor fusion is a term that covers a number of methods and algorithms, including:   Central Limit Theorem  Kalman filter  Bayesian networks  Dempster-Shafer   Example sensor fusion calculations  Two example sensor fusion calculations are illustrated below.  Let    ğ±  1     subscript  x  1    {\textbf{x}}_{1}   and    ğ±  2     subscript  x  2    {\textbf{x}}_{2}   denote two sensor measurements with noise variances    Ïƒ  1  2     superscript   subscript  Ïƒ  1   2    \scriptstyle\sigma_{1}^{2}   and    Ïƒ  2  2     superscript   subscript  Ïƒ  2   2    \scriptstyle\sigma_{2}^{2}   , respectively. One way of obtaining a combined measurement    ğ±  3     subscript  x  3    {\textbf{x}}_{3}   is to apply the Central Limit Theorem , which is also employed within the Fraser-Potter fixed-interval smoother, namely 3  4       ğ±  3   =    Ïƒ  3  2    (     Ïƒ  1   -  2     ğ±  1    +    Ïƒ  2   -  2     ğ±  2     )         subscript  x  3      superscript   subscript  Ïƒ  3   2        superscript   subscript  Ïƒ  1     2     subscript  x  1       superscript   subscript  Ïƒ  2     2     subscript  x  2        {\textbf{x}}_{3}=\scriptstyle\sigma_{3}^{2}(\scriptstyle\sigma_{1}^{-2}{%
 \textbf{x}}_{1}+\scriptstyle\sigma_{2}^{-2}{\textbf{x}}_{2})   ,  where     Ïƒ  3  2   =    (    Ïƒ  1   -  2    +   Ïƒ  2   -  2     )    -  1         superscript   subscript  Ïƒ  3   2    superscript     superscript   subscript  Ïƒ  1     2     superscript   subscript  Ïƒ  2     2       1      \scriptstyle\sigma_{3}^{2}=(\scriptstyle\sigma_{1}^{-2}+\scriptstyle\sigma_{2}%
 ^{-2})^{-1}   is the variance of the combined estimate. It can be seen that the fused result is simply a linear combination of the two measurements weighted by their respective noise variances.  Another method to fuse two measurements is to use the optimal Kalman filter . Suppose that the data is generated by a first-order system and let    ğ  k     subscript  P  k    {\textbf{P}}_{k}   denote the solution of the filter's Riccati equation . By applying Cramer's rule within the gain calculation it can be found that the filter gain is given by 5        ğ‹  k   =   [         Ïƒ  2  2    ğ  k       Ïƒ  2  2    ğ  k    +    Ïƒ  1  2    ğ  k    +    Ïƒ  1  2    Ïƒ  2  2             Ïƒ  1  2    ğ  k       Ïƒ  2  2    ğ  k    +    Ïƒ  1  2    ğ  k    +    Ïƒ  1  2    Ïƒ  2  2          ]    .       subscript  L  k          superscript   subscript  Ïƒ  2   2    subscript  P  k         superscript   subscript  Ïƒ  2   2    subscript  P  k       superscript   subscript  Ïƒ  1   2    subscript  P  k       superscript   subscript  Ïƒ  1   2    superscript   subscript  Ïƒ  2   2           superscript   subscript  Ïƒ  1   2    subscript  P  k         superscript   subscript  Ïƒ  2   2    subscript  P  k       superscript   subscript  Ïƒ  1   2    subscript  P  k       superscript   subscript  Ïƒ  1   2    superscript   subscript  Ïƒ  2   2          {\textbf{L}}_{k}=\par
 \begin{bmatrix}\tfrac{\scriptstyle\sigma_{2}^{2}{\textbf%
 {P}}_{k}}{\scriptstyle\sigma_{2}^{2}{\textbf{P}}_{k}+\scriptstyle\sigma_{1}^{2%
 }{\textbf{P}}_{k}+\scriptstyle\sigma_{1}^{2}\scriptstyle\sigma_{2}^{2}}&\tfrac%
 {\scriptstyle\sigma_{1}^{2}{\textbf{P}}_{k}}{\scriptstyle\sigma_{2}^{2}{%
 \textbf{P}}_{k}+\scriptstyle\sigma_{1}^{2}{\textbf{P}}_{k}+\scriptstyle\sigma_%
 {1}^{2}\scriptstyle\sigma_{2}^{2}}\end{bmatrix}.     By inspection, when the first measurement is noise free, the filter ignores the second measurement and vice versa. That is, the combined estimate is weighted by the quality of the measurements.  Centralized versus decentralized  In sensor fusion, centralized versus decentralized refers to where the fusion of the data occurs. In centralized fusion, the clients simply forward all of the data to a central location, and some entity at the central location is responsible for correlating and fusing the data. In decentralized, the clients take full responsibility for fusing the data. "In this case, every sensor or platform can be viewed as an intelligent asset having some degree of autonomy in decision-making." 6  Multiple combinations of centralized and decentralized systems exist.  Levels  There are several categories or levels of sensor fusion that are commonly used.* 7  8  9  10  11  12   Level 0 â€“ Data alignment  Level 1 â€“ Entity assessment (e.g. signal/feature/object).  Tracking and object detection/recognition/identification   Level 2 â€“ Situation assessment  Level 3 â€“ Impact assessment  Level 4 â€“ Process refinement (i.e. sensor management)  Level 5 â€“ User refinement   Applications  One application of sensor fusion is GPS/INS , where Global Positioning System and Inertial Navigation System data is fused using various different methods, e.g. the Extended Kalman Filter . This is useful, for example, in determining the attitude of an aircraft using low-cost sensors. 13  A practical example how to combine data of different displacement and position sensors in order to obtain high bandwidth at high resolution can be found in this master thesis. 14 One can see the applied methods of optimal filtering (in sense of minimizing e.g. the energy norm) or the MIMO Kalman filter.  See also   Information integration  Data mining  Data fusion  Image fusion  Information: Information is not data  Data (computing)  multimodal integration  Fisher's method for combining independent tests of significance  Transducer Markup Language (TML) is an XML based markup language which enables sensor fusion.  Brooks â€“ Iyengar algorithm  Inertial navigation system  Sensor Grid  Semantic Perception   References   J. L. Crowley and Y. Demazeau Principles and Techniques for Sensor Data Fusion Signal Processing, Volume 32, Issues 1â€“2, May 1993, Pages 5â€“27   External links   International Society of Information Fusion   "  Category:Robotic sensing  Category:Computer data  Category:Sensors     â†©  Haghighat, M. B. A., Aghagolzadeh, A., & Seyedarabi, H. (2011). Multi-focus image fusion for visual sensor networks in DCT domain . Computers & Electrical Engineering, 37(5), 789-797. â†©  â†©  â†©   â†©  Rethinking JDL Data Fusion Levels â†©  Blasch, E., Plano, S. (2003) â€œLevel 5: User Refinement to aid the Fusion Processâ€, Proceedings of the SPIE, Vol. 5099. â†©  â†©  Blasch, E. (2006) " Sensor, user, mission (SUM) resource management and their interaction with level 2/3 fusion " International Conference on Information Fusion. â†©  http://defensesystems.com/articles/2009/09/02/c4isr1-sensor-fusion.aspx â†©  Blasch, E., Steinberg, A., Das, S., Llinas, J., Chong, C.-Y., Kessler, O., Waltz, E., White, F. (2013) "Revisiting the JDL model for information Exploitation," International Conference on Information Fusion. â†©  â†©  â†©     