   Differential entropy      Differential entropy   Differential entropy (also referred to as continuous entropy ) is a concept in information theory that extends the idea of (Shannon) entropy , a measure of average surprisal of a random variable , to continuous probability distributions .  Definition  Let X be a random variable with a probability density function  f whose support is a set   𝕏   𝕏   \mathbb{X}   . The differential entropy  h ( X ) or h ( f ) is defined as       h   (  X  )    =   -    ∫  𝕏    f   (  x  )    log  f    (  x  )   d  x           h  X       subscript   𝕏     f  x    f   x  d  x       h(X)=-\int_{\mathbb{X}}f(x)\log f(x)\,dx   .  For probability distributions which don't have an explicit density function expression, but have an explicit quantile function expression, Q ( p ), then h ( Q ) can be defined in terms of the derivative of Q ( p ) i.e. the quantile density function Q '( p ) as 1       h   (  Q  )    =    ∫  0  1     log   Q  ′     (  p  )   d  p          h  Q     superscript   subscript   0   1        superscript  Q  normal-′    p  d  p      h(Q)=\int_{0}^{1}\log Q^{\prime}(p)\,dp   .  As with its discrete analog, the units of differential entropy depend on the base of the logarithm , which is usually 2 (i.e., the units are bits ). See logarithmic units for logarithms taken in different bases. Related concepts such as joint , conditional differential entropy, and relative entropy are defined in a similar fashion. Unlike the discrete analog, the differential entropy has an offset that depends on the units used to measure X . 2 For example, the differential entropy of a quantity measured in millimeters will be log(1000) more than the same quantity measured in meters; a dimensionless quantity will have differential entropy of log(1000) more than the same quantity divided by 1000.  One must take care in trying to apply properties of discrete entropy to differential entropy, since probability density functions can be greater than 1. For example, Uniform (0,1/2) has negative differential entropy        ∫  0   1  2    -   2   log   (  2  )    d  x    =   -   log   (  2  )            superscript   subscript   0     1  2      2    2   d  x        2      \int_{0}^{\frac{1}{2}}-2\log(2)\,dx=-\log(2)\,   .  Thus, differential entropy does not share all properties of discrete entropy.  Note that the continuous mutual information  I ( X ; Y ) has the distinction of retaining its fundamental significance as a measure of discrete information since it is actually the limit of the discrete mutual information of partitions of X and Y as these partitions become finer and finer. Thus it is invariant under non-linear homeomorphisms (continuous and uniquely invertible maps) , 3 including linear 4 transformations of X and Y , and still represents the amount of discrete information that can be transmitted over a channel that admits a continuous space of values.  Properties of differential entropy   For densities f and g , the Kullback–Leibler divergence  D ( f || g ) is nonnegative with equality if f = g  almost everywhere . Similarly, for two random variables X and Y , I ( X ; Y ) ≥ 0 and h ( X | Y ) ≤ h ( X ) with equality if and only if  X and Y are independent .  The chain rule for differential entropy holds as in the discrete case         h   (   X  1   ,  …  ,   X  n   )   =   ∑   i  =  1   n   h   (   X  i   |   X  1   ,  …  ,   X   i  -  1    )   ≤  ∑  h   (   X  i   )      fragments  h   fragments  normal-(   subscript  X  1   normal-,  normal-…  normal-,   subscript  X  n   normal-)     superscript   subscript     i  1    n   h   fragments  normal-(   subscript  X  i   normal-|   subscript  X  1   normal-,  normal-…  normal-,   subscript  X    i  1    normal-)     h   fragments  normal-(   subscript  X  i   normal-)     h(X_{1},\ldots,X_{n})=\sum_{i=1}^{n}h(X_{i}|X_{1},\ldots,X_{i-1})\leq\sum h(X_%
 {i})   .      Differential entropy is translation invariant, i.e., h ( X + c ) = h ( X ) for a constant c .  Differential entropy is in general not invariant under arbitrary invertible maps. In particular, for a constant a , h ( aX ) = h ( X ) + log| a |. For a vector valued random variable X and a matrix A , h ( A  X ) = h ( X ) + log|det( A )|.  In general, for a transformation from a random vector to another random vector with same dimension Y = m ( X ), the corresponding entropies are related via          h   (  𝐘  )    ≤    h   (  𝐗  )    +   ∫   f   (  x  )    log    |    ∂  m    ∂  x    |   d  x             h  𝐘       h  𝐗       f  x            m     x     d  x         h(\mathbf{Y})\leq h(\mathbf{X})+\int f(x)\log\left|\frac{\partial m}{\partial x%
 }\right|dx       where    |    ∂  m    ∂  x    |          m     x      \left|\frac{\partial m}{\partial x}\right|   is the Jacobian of the transformation m . The above inequality becomes an equality if the transform is a bijection. Furthermore, when m is a rigid rotation, translation, or combination thereof, the Jacobian determinant is always 1, and h ( Y ) = h ( X ).    If a random vector X in R n has mean zero and covariance matrix K ,     h   (  𝐗  )    ≤    1  2    log   [     (   2  π  e   )   n    det  K    ]           h  𝐗       1  2        superscript    2  π  e   n     K        h(\mathbf{X})\leq\frac{1}{2}\log[(2\pi e)^{n}\det{K}]   with equality if and only if X is jointly gaussian (see below ).   However, differential entropy does not have other desirable properties:   It is not invariant under change of variables , and is therefore most useful with dimensionless variables.  It can be negative.   A modification of differential entropy that addresses these drawbacks is the relative information entropy , also known as the Kullback–Leibler divergence , which includes an invariant measure factor (see limiting density of discrete points ).  Maximization in the normal distribution  With a normal distribution , differential entropy is maximized for a given variance. The following is a proof that a Gaussian variable has the largest entropy amongst all random variables of equal variance, or, alternatively, that the maximum entropy distribution under constraints of mean and variance is the Gaussian.  Let g ( x ) be a Gaussian  PDF with mean μ and variance σ 2 and f ( x ) an arbitrary PDF with the same variance. Since differential entropy is translation invariant we can assume that f ( x ) has the same mean of μ as g ( x ).  Consider the Kullback–Leibler divergence between the two distributions      0  ≤   D   K  L     (  f  |  |  g  )   =   ∫   -  ∞   ∞   f   (  x  )   log   (    f   (  x  )     g   (  x  )     )   d  x  =  -  h   (  f  )   -   ∫   -  ∞   ∞   f   (  x  )   log   (  g   (  x  )   )   d  x  .     fragments  0    subscript  D    K  L     fragments  normal-(  f  normal-|  normal-|  g  normal-)     superscript   subscript          f   fragments  normal-(  x  normal-)     fragments  normal-(      f  x     g  x    normal-)   d  x    h   fragments  normal-(  f  normal-)     superscript   subscript          f   fragments  normal-(  x  normal-)     fragments  normal-(  g   fragments  normal-(  x  normal-)   normal-)   d  x  normal-.    0\leq D_{KL}(f||g)=\int_{-\infty}^{\infty}f(x)\log\left(\frac{f(x)}{g(x)}%
 \right)dx=-h(f)-\int_{-\infty}^{\infty}f(x)\log(g(x))dx.   Now note that        ∫   -  ∞   ∞     f   (  x  )    log   (   g   (  x  )    )    d  x       superscript   subscript            f  x      g  x    d  x     \displaystyle\int_{-\infty}^{\infty}f(x)\log(g(x))dx   because the result does not depend on f ( x ) other than through the variance. Combining the two results yields        h   (  g  )    -   h   (  f  )     ≥   0           h  g     h  f    0    h(g)-h(f)\geq 0\!   with equality when g ( x ) = f ( x ) following from the properties of Kullback–Leibler divergence .  This result may also be demonstrated using the variational calculus . A Lagrangian function with two Lagrangian multipliers may be defined as:      L  =     ∫   -  ∞   ∞    g   (  x  )    ln   (   g   (  x  )    )    d  x    -    λ  0    (   1  -    ∫   -  ∞   ∞    g   (  x  )   d  x     )    -   λ   (    σ  2   -    ∫   -  ∞   ∞    g   (  x  )      (   x  -  μ   )   2    d  x     )         L      superscript   subscript            g  x      g  x    d  x       subscript  λ  0     1    superscript   subscript            g  x  d  x        λ     superscript  σ  2     superscript   subscript            g  x   superscript    x  μ   2   d  x         L=\int_{-\infty}^{\infty}g(x)\ln(g(x))\,dx-\lambda_{0}\left(1-\int_{-\infty}^{%
 \infty}g(x)\,dx\right)-\lambda\left(\sigma^{2}-\int_{-\infty}^{\infty}g(x)(x-%
 \mu)^{2}\,dx\right)     where g(x) is some function with mean μ. When the entropy of g(x) is at a maximum and the constraint equations, which consist of the normalization condition    (   1  =    ∫   -  ∞   ∞    g   (  x  )   d  x     )      1    superscript   subscript            g  x  d  x      \left(1=\int_{-\infty}^{\infty}g(x)\,dx\right)   and the requirement of fixed variance    (    σ  2   =    ∫   -  ∞   ∞    g   (  x  )      (   x  -  μ   )   2    d  x     )       superscript  σ  2     superscript   subscript            g  x   superscript    x  μ   2   d  x      \left(\sigma^{2}=\int_{-\infty}^{\infty}g(x)(x-\mu)^{2}\,dx\right)   , are both satisfied, then a small variation δ g ( x ) about g(x) will produce a variation δ L about L which is equal to zero:      0  =   δ  L   =    ∫   -  ∞   ∞    δ  g   (  x  )    (    ln   (   g   (  x  )    )    +  1  +   λ  0   +   λ    (   x  -  μ   )   2     )   d  x          0    δ  L          superscript   subscript            δ  g  x        g  x    1   subscript  λ  0     λ   superscript    x  μ   2     d  x       0=\delta L=\int_{-\infty}^{\infty}\delta g(x)\left(\ln(g(x))+1+\lambda_{0}+%
 \lambda(x-\mu)^{2}\right)\,dx     Since this must hold for any small δ g ( x ), the term in brackets must be zero, and solving for g(x) yields:       g   (  x  )    =   e    -   λ  0    -  1  -   λ    (   x  -  μ   )   2            g  x    superscript  e       subscript  λ  0    1    λ   superscript    x  μ   2        g(x)=e^{-\lambda_{0}-1-\lambda(x-\mu)^{2}}     Using the constraint equations to solve for λ 0 and λ yields the normal distribution:       g   (  x  )    =    1    2  π   σ  2       e   -     (   x  -  μ   )   2    2   σ  2              g  x       1      2  π   superscript  σ  2       superscript  e       superscript    x  μ   2     2   superscript  σ  2          g(x)=\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}     Example: Exponential distribution  Let X be an exponentially distributed random variable with parameter λ, that is, with probability density function       f   (  x  )    =   λ   e   -   λ  x     for  x   ≥  0.          f  x     λ   superscript  e      λ  x     for  x        0.     f(x)=\lambda e^{-\lambda x}\mbox{ for }x\geq 0.     Its differential entropy is then           h  e    (  X  )        subscript  h  e   X    h_{e}(X)\,           =   -    ∫  0  ∞    λ   e   -   λ  x      log   (   λ   e   -   λ  x      )    d  x         absent      superscript   subscript   0       λ   superscript  e      λ  x         λ   superscript  e      λ  x       d  x       =-\int_{0}^{\infty}\lambda e^{-\lambda x}\log(\lambda e^{-\lambda x})\,dx              =   -   (     ∫  0  ∞     (   log  λ   )   λ    e   -   λ  x      d  x    +    ∫  0  ∞     (   -   λ  x    )   λ    e   -   λ  x      d  x     )        absent        superscript   subscript   0         λ   λ   superscript  e      λ  x     d  x      superscript   subscript   0           λ  x    λ   superscript  e      λ  x     d  x        =-\left(\int_{0}^{\infty}(\log\lambda)\lambda e^{-\lambda x}\,dx+\int_{0}^{%
 \infty}(-\lambda x)\lambda e^{-\lambda x}\,dx\right)              =    -    log  λ     ∫  0  ∞    f   (  x  )   d  x      +   λ  E   [  X  ]         absent          λ     superscript   subscript   0       f  x  d  x        λ  E   delimited-[]  X       =-\log\lambda\int_{0}^{\infty}f(x)\,dx+\lambda E[X]              =    -   log  λ    +  1 .       absent        λ    1 .     =-\log\lambda+1\,.        Here,     h  e    (  X  )        subscript  h  e   X    h_{e}(X)   was used rather than    h   (  X  )       h  X    h(X)   to make it explicit that the logarithm was taken to base e , to simplify the calculation.  Differential entropies for various distributions  In the table below     Γ   (  x  )    =    ∫  0  ∞     e   -  t     t   x  -  1    d  t          normal-Γ  x     superscript   subscript   0        superscript  e    t     superscript  t    x  1    d  t      \Gamma(x)=\int_{0}^{\infty}e^{-t}t^{x-1}dt   is the gamma function ,     ψ   (  x  )    =    d   d  x     ln  Γ    (  x  )    =     Γ  ′    (  x  )     Γ   (  x  )             ψ  x       d    d  x      normal-Γ   x             superscript  normal-Γ  normal-′   x     normal-Γ  x       \psi(x)=\frac{d}{dx}\ln\Gamma(x)=\frac{\Gamma^{\prime}(x)}{\Gamma(x)}   is the digamma function ,     B   (  p  ,  q  )    =    Γ   (  p  )   Γ   (  q  )     Γ   (   p  +  q   )           B   p  q        normal-Γ  p  normal-Γ  q     normal-Γ    p  q       B(p,q)=\frac{\Gamma(p)\Gamma(q)}{\Gamma(p+q)}   is the beta function , and γ E is Euler's constant . 5      Table of differential entropies     Distribution Name     Uniform     Normal     Exponential     Rayleigh     Beta     Cauchy     Chi     Chi-squared     Erlang     F     Gamma     Laplace     Logistic     Lognormal     Maxwell–Boltzmann     Generalized normal     Pareto     Student's t     Triangular     Weibull     Multivariate normal       (Many of the differential entropies are from. 6  Variants  As described above, differential entropy does not share all properties of discrete entropy. For example, the differential entropy can be negative; also it is not invariant under continuous coordinate transformations. Edwin Thompson Jaynes showed in fact that the expression above is not the correct limit of the expression for a finite set of probabilities. 7  A modification of differential entropy adds an invariant measure factor to correct this, (see limiting density of discrete points ). If m(x) is further constrained to be a probability density, the resulting notion is called relative entropy in information theory:      D   (  p  |  |  m  )   =  ∫  p   (  x  )   log     p   (  x  )     m   (  x  )      d  x  .     fragments  D   fragments  normal-(  p  normal-|  normal-|  m  normal-)     p   fragments  normal-(  x  normal-)        p  x     m  x    d  x  normal-.    D(p||m)=\int p(x)\log\frac{p(x)}{m(x)}\,dx.     The definition of differential entropy above can be obtained by partitioning the range of X into bins of length h with associated sample points ih within the bins, for X Riemann integrable. This gives a quantized version of X , defined by X h = ih if ih ≤ X ≤ ( i +1) h . Then the entropy of X h is        H  h   =    -    ∑  i    h  f   (   i  h   )    log   (   f   (   i  h   )    )       -   ∑   h  f   (   i  h   )    log   (  h  )        .       subscript  H  h         subscript   i     h  f    i  h       f    i  h            h  f    i  h     h        H_{h}=-\sum_{i}hf(ih)\log(f(ih))-\sum hf(ih)\log(h).     The first term on the right approximates the differential entropy, while the second term is approximately −log( h ). Note that this procedure suggests that the entropy in the discrete sense of a continuous random variable should be ∞.  See also   Information entropy  Information theory  Limiting density of discrete points  Self-information  Kullback–Leibler divergence  Entropy estimation   References   Thomas M. Cover, Joy A. Thomas. Elements of Information Theory New York: Wiley, 1991. ISBN 0-471-06259-6   External links      "  Category:Entropy and information  Category:Information theory  Category:Statistical randomness  Category:Randomness     ↩  Pages 183-184, ↩  ↩  ↩  ↩  ↩  ↩     