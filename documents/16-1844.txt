   Jackknife Variance Estimates for Random Forest      Jackknife Variance Estimates for Random Forest   References        Random Forest is a good method for classification. There are some ideas for measuring performance of Random Forest Models in classification problems. Jackknife is a good idea to estimate the variance in random forest models to eliminate the bootstrap effects.  Jackknife variance estimates  The sampling variance of bagged learners is:       V   (  x  )    =   V  a  r   [     θ  ^   ∞    (  x  )    ]          V  x     V  a  r   delimited-[]     superscript   normal-^  θ     x       V(x)=Var[\hat{\theta}^{\infty}(x)]   Jackknife estimates can be considered to eliminate the bootstrap effects. The jackknife variance estimator is defined as: 1        V  ^   j   =     n  -  1   n     ∑   i  =  1   n     (     θ  ^    (   -  i   )    -   θ  ¯    )   2          subscript   normal-^  V   j         n  1   n     superscript   subscript     i  1    n    superscript     subscript   normal-^  θ     i     normal-¯  θ    2       \hat{V}_{j}=\frac{n-1}{n}\sum_{i=1}^{n}(\hat{\theta}_{(-i)}-\overline{\theta})%
 ^{2}   In some classification problems, when random forest is used to fit models, jackknife estimated variance is defined as:        V  ^   j   =     n  -  1   n     ∑   i  =  1   n     (      t  ¯    (   -  i   )   ⋆    (  x  )    -     t  ¯   ⋆    (  x  )     )   2          subscript   normal-^  V   j         n  1   n     superscript   subscript     i  1    n    superscript       subscript   superscript   normal-¯  t   normal-⋆     i    x      superscript   normal-¯  t   normal-⋆   x    2       \hat{V}_{j}=\frac{n-1}{n}\sum_{i=1}^{n}(\overline{t}^{\star}_{(-i)}(x)-%
 \overline{t}^{\star}(x))^{2}   Here,    t  ⋆     superscript  t  normal-⋆    t^{\star}   denotes a decision tree after training,    t   (   -  i   )   ⋆     subscript   superscript  t  normal-⋆     i     t^{\star}_{(-i)}   denotes the result based on samples without    i  t  h      i  t  h    ith   observation.  Examples  E-mail spam problem is a common classification problem, in this problem, 57 features are used to classify spam e-mail and non-spam e-mail. Applying IJ-U variance formula to evaluate the accuracy of models with m=15,19 and 57. The results shows in paper( Confidence Intervals for Random Forests: The jackknife and the Infinitesimal Jackknife ) that m = 57 random forest appears to be quite unstable, while predictions made by m=5 random forest appear to be quite stable, this results is corresponding to the evaluation made by error percentage, in which the accuracy of model with m=5 is high and m=57 is low.  Here, accuracy is measured by error rate, which is defined as:        E  r  r  o  r  R  a  t  e   =    1  N     ∑   i  =  1   N     ∑   j  =  1   M    y   i  j        ,        E  r  r  o  r  R  a  t  e       1  N     superscript   subscript     i  1    N     superscript   subscript     j  1    M    subscript  y    i  j         ErrorRate=\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{M}y_{ij},   Here N is also the number of samples, M is the number of classes,    y   i  j      subscript  y    i  j     y_{ij}   is the indicator function which equals 1 when    i  t  h      i  t  h    ith   observation is in class j, equals 0 when in other classes. No probability is considered here. There is an another method which is similar to error rate to measure accuracy:       l  o  g  l  o  s  s   =    1  N     ∑   i  =  1   N     ∑   j  =  1   M     y   i  j    l  o  g   (   p   i  j    )             l  o  g  l  o  s  s       1  N     superscript   subscript     i  1    N     superscript   subscript     j  1    M      subscript  y    i  j    l  o  g   subscript  p    i  j          logloss=\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{M}y_{ij}log(p_{ij})   Here N is the number of samples, M is the number of classes,    y   i  j      subscript  y    i  j     y_{ij}   is the indicator function which equals 1 when    i  t  h      i  t  h    ith   observation is in class j, equals 0 when in other classes.    p   i  j      subscript  p    i  j     p_{ij}   is the predicted probability of    i  t  h      i  t  h    ith   observation in class   j   j   j   .This method is used in Kaggle 2 These two methods are very similar.  Modification for Bias  When using Monte Carlo MSEs for estimating    V   I  J   ∞     superscript   subscript  V    I  J       V_{IJ}^{\infty}   and    V  J  ∞     superscript   subscript  V  J      V_{J}^{\infty}   , a problem about the Monte Carlo bias should be considered, especially when n is large, the bias is getting large:        E   [    V  ^    I  J   B   ]    -    V  ^    I  J   ∞    ≈    n    ∑   b  =  1   B     (    t  b  ⋆   -    t  ¯   ⋆    )   2     B           E   delimited-[]   superscript   subscript   normal-^  V     I  J    B      superscript   subscript   normal-^  V     I  J           n    superscript   subscript     b  1    B    superscript     superscript   subscript  t  b   normal-⋆    superscript   normal-¯  t   normal-⋆    2     B     E[\hat{V}_{IJ}^{B}]-\hat{V}_{IJ}^{\infty}\approx\frac{n\sum_{b=1}^{B}(t_{b}^{%
 \star}-\bar{t}^{\star})^{2}}{B}   To eliminate this influence, bias-corrected modifications are suggested:        V  ^     I  J   -  U   B   =     V  ^    I  J   B   -    n    ∑   b  =  1   B     (    t  b  ⋆   -    t  ¯   ⋆    )   2     B         superscript   subscript   normal-^  V       I  J   U    B      superscript   subscript   normal-^  V     I  J    B       n    superscript   subscript     b  1    B    superscript     superscript   subscript  t  b   normal-⋆    superscript   normal-¯  t   normal-⋆    2     B      \hat{V}_{IJ-U}^{B}=\hat{V}_{IJ}^{B}-\frac{n\sum_{b=1}^{B}(t_{b}^{\star}-\bar{t%
 }^{\star})^{2}}{B}           V  ^    J  -  U   B   =     V  ^   J  B   -    (   e  -  1   )     n    ∑   b  =  1   B     (    t  b  ⋆   -    t  ¯   ⋆    )   2     B          superscript   subscript   normal-^  V     J  U    B      superscript   subscript   normal-^  V   J   B       e  1       n    superscript   subscript     b  1    B    superscript     superscript   subscript  t  b   normal-⋆    superscript   normal-¯  t   normal-⋆    2     B       \hat{V}_{J-U}^{B}=\hat{V}_{J}^{B}-(e-1)\frac{n\sum_{b=1}^{B}(t_{b}^{\star}-%
 \bar{t}^{\star})^{2}}{B}   '     ↩  ↩     