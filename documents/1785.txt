   Randomized rounding      Randomized rounding   Within computer science and operations research , many combinatorial optimization problems are computationally intractable to solve exactly (to optimality). Many such problems do admit fast ( polynomial time ) approximation algorithms â€”that is, algorithms that are guaranteed to return an approximately optimal solution given any input.  Randomized rounding is a widely used approach for designing and analyzing such approximation algorithms . 1 2 The basic idea is to use the probabilistic method to convert an optimal solution of a relaxation of the problem into an approximately optimal solution to the original problem.  Overview  The basic approach has three steps:   Formulate the problem to be solved as an integer linear program (ILP).  Compute an optimal fractional solution   x   x   x   to the linear programming relaxation (LP) of the ILP.  Round the fractional solution   x   x   x   of the LP to an integer solution    x  â€²     superscript  x  normal-â€²    x^{\prime}   of the ILP.   (Although the approach is most commonly applied with linear programs, other kinds of relaxations are sometimes used. For example, see Goeman's and Williamson's semi-definite programming -based Max-Cut approximation algorithm .)  The challenge in the first step is to choose a suitable integer linear program. Familiarity with linear programming is required, in particular, familiarity with how to model problems using linear programs and integer linear programs. But, for many problems, there is a natural integer linear program that works well, such as in the Set Cover example below. (The integer linear program should have a small integrality gap ; indeed randomized rounding is often used to prove bounds on integrality gaps.)  In the second step, the optimal fractional solution can typically be computed in polynomial time using any standard linear programming algorithm.  In the third step, the fractional solution must be converted into an integer solution (and thus a solution to the original problem). This is called rounding the fractional solution. The resulting integer solution should (provably) have cost not much larger than the cost of the fractional solution. This will ensure that the cost of the integer solution is not much larger than the cost of the optimal integer solution.  The main technique used to do the third step (rounding) is to use randomization, and then to use probabilistic arguments to bound the increase in cost due to the rounding (following the probabilistic method from combinatorics). There, probabilistic arguments are used to show the existence of discrete structures with desired properties. In this context, one uses such arguments to show the following:   Given any fractional solution   x   x   x   of the LP, with positive probability the randomized rounding process produces an integer solution    x  â€²     superscript  x  normal-â€²    x^{\prime}   that approximates   x   x   x    according to some desired criterion.   Finally, to make the third step computationally efficient, one either shows that    x  â€²     superscript  x  normal-â€²    x^{\prime}   approximates   x   x   x   with high probability (so that the step can remain randomized) or one derandomizes the rounding step, typically using the method of conditional probabilities . The latter method converts the randomized rounding process into an efficient deterministic process that is guaranteed to reach a good outcome.  Comparison to other applications of the probabilistic method  The randomized rounding step differs from most applications of the probabilistic method in two respects:   The computational complexity of the rounding step is important. It should be implementable by a fast (e.g. polynomial time ) algorithm .  The probability distribution underlying the random experiment is a function of the solution   x   x   x   of a relaxation of the problem instance. This fact is crucial to proving the performance guarantee of the approximation algorithm --- that is, that for any problem instance, the algorithm returns a solution that approximates the optimal solution for that specific instance . In comparison, applications of the probabilistic method in combinatorics typically show the existence of structures whose features depend on other parameters of the input. For example, consider TurÃ¡n's theorem , which can be stated as "any graph with   n   n   n   vertices of average degree   d   d   d   must have an independent set of size at least    n  /   (   d  +  1   )       n    d  1     n/(d+1)   . (See this for a probabilistic proof of TurÃ¡n's theorem .) While there are graphs for which this bound is tight, there are also graphs which have independent sets much larger than    n  /   (   d  +  1   )       n    d  1     n/(d+1)   . Thus, the size of the independent set shown to exist by TurÃ¡n's theorem in a graph may, in general, be much smaller than the maximum independent set for that graph.   Set Cover example  The method is best illustrated by example. The following example illustrates how randomized rounding can be used to design an approximation algorithm for the Set Cover problem.  Fix any instance    âŸ¨  c  ,  ğ’®  âŸ©     c  ğ’®    \langle c,\mathcal{S}\rangle   of the Set Cover problem over a universe   ğ’°   ğ’°   \mathcal{U}   .  For step 1, let IP be the standard integer linear program for set cover for this instance.  For step 2, let LP be the linear programming relaxation of IP, and compute an optimal solution    x  *     superscript  x     x^{*}   to LP using any standard linear programming algorithm. (This takes time polynomial in the input size.)  (The feasible solutions to LP are the vectors   x   x   x   that assign each set    s  âˆˆ  ğ’®      s  ğ’®    s\in\mathcal{S}   a non-negative weight    x  s     subscript  x  s    x_{s}   , such that, for each element    e  âˆˆ  ğ’°      e  ğ’°    e\in\mathcal{U}   ,    x  â€²     superscript  x  normal-â€²    x^{\prime}    covers    e   e   e   -- the total weight assigned to the sets containing   e   e   e   is at least 1, that is,          âˆ‘   s  âˆ‹  e     x  s    â‰¥  1.        subscript     e  s     subscript  x  s    1.    \sum_{s\ni e}x_{s}\geq 1.        The optimal solution    x  *     superscript  x     x^{*}   is a feasible solution whose cost         âˆ‘   s  âˆˆ  ğ’°     c   (  S  )    x  s  *        subscript     s  ğ’°      c  S   subscript   superscript  x    s      \sum_{s\in\mathcal{U}}c(S)x^{*}_{s}        is as small as possible.)   Note that any set cover   ğ’   ğ’   \mathcal{C}   for   ğ’®   ğ’®   \mathcal{S}   gives a feasible solution   x   x   x   (where     x  s   =  1       subscript  x  s   1    x_{s}=1   for    s  âˆˆ  ğ’      s  ğ’    s\in\mathcal{C}   ,     x  s   =  0       subscript  x  s   0    x_{s}=0   otherwise). The cost of this   ğ’   ğ’   \mathcal{C}   equals the cost of   x   x   x   , that is,           âˆ‘   s  âˆˆ  ğ’     c   (  s  )     =    âˆ‘   s  âˆˆ  ğ’®     c   (  s  )    x  s      .        subscript     s  ğ’      c  s      subscript     s  ğ’®      c  s   subscript  x  s       \sum_{s\in\mathcal{C}}c(s)=\sum_{s\in\mathcal{S}}c(s)x_{s}.        In other words, the linear program LP is a relaxation of the given set-cover problem.  Since    x  *     superscript  x     x^{*}   has minimum cost among feasible solutions to the LP, the cost of    x  *     superscript  x     x^{*}   is a lower bound on the cost of the optimal set cover .  Step 3: The randomized rounding step  Here is a description of the third stepâ€”the rounding step, which must convert the minimum-cost fractional set cover    x  *     superscript  x     x^{*}   into a feasible integer solution    x  â€²     superscript  x  normal-â€²    x^{\prime}   (corresponding to a true set cover).  The rounding step should produce an    x  â€²     superscript  x  normal-â€²    x^{\prime}   that, with positive probability, has cost within a small factor of the cost of    x  *     superscript  x     x^{*}   . Then (since the cost of    x  *     superscript  x     x^{*}   is a lower bound on the cost of the optimal set cover), the cost of    x  â€²     superscript  x  normal-â€²    x^{\prime}   will be within a small factor of the optimal cost.  As a starting point, consider the most natural rounding scheme:    For each set    s  âˆˆ  ğ’®      s  ğ’®    s\in\mathcal{S}   in turn, take     x  s  â€²   =  1       subscript   superscript  x  normal-â€²   s   1    x^{\prime}_{s}=1   with probability    min   (  1  ,   x  s  *   )       1   subscript   superscript  x    s     \min(1,x^{*}_{s})   , otherwise take     x  s  â€²   =  0       subscript   superscript  x  normal-â€²   s   0    x^{\prime}_{s}=0   .      With this rounding scheme, the expected cost of the chosen sets is at most     âˆ‘  s    c   (  s  )    x  s  *        subscript   s     c  s   subscript   superscript  x    s      \sum_{s}c(s)x^{*}_{s}   , the cost of the fractional cover. This is good. Unfortunately the coverage is not good. When the variables    x  s  *     subscript   superscript  x    s    x^{*}_{s}   are small, the probability that an element   e   e   e   is not covered is about          âˆ   s  âˆ‹  e    1   -   x  s  *    â‰ˆ    âˆ   s  âˆ‹  e     exp   (   -   x  s  *    )     =   exp   (   -    âˆ‘   s  âˆ‹  e     x  s  *     )    â‰ˆ   exp   (   -  1   )     .            subscript  product    e  s    1    subscript   superscript  x    s      subscript  product    e  s         subscript   superscript  x    s                 subscript     e  s     subscript   superscript  x    s               1       \prod_{s\ni e}1-x^{*}_{s}\approx\prod_{s\ni e}\exp(-x^{*}_{s})=\exp\Big(-\sum_%
 {s\ni e}x^{*}_{s}\Big)\approx\exp(-1).     So only a constant fraction of the elements will be covered in expectation.  To make    x  â€²     superscript  x  normal-â€²    x^{\prime}   cover every element with high probability, the standard rounding scheme first scales up the rounding probabilities by an appropriate factor    Î»  >  1      Î»  1    \lambda>1   . Here is the standard rounding scheme:    Fix a parameter    Î»  â‰¥  1      Î»  1    \lambda\geq 1   . For each set    s  âˆˆ  ğ’®      s  ğ’®    s\in\mathcal{S}   in turn,   take     x  s  â€²   =  1       subscript   superscript  x  normal-â€²   s   1    x^{\prime}_{s}=1   with probability    min   (   Î»   x  s  *    ,  1  )         Î»   subscript   superscript  x    s    1    \min(\lambda x^{*}_{s},1)   , otherwise take     x  s  â€²   =  0       subscript   superscript  x  normal-â€²   s   0    x^{\prime}_{s}=0   .      Scaling the probabilities up by   Î»   Î»   \lambda   increases the expected cost by   Î»   Î»   \lambda   , but makes coverage of all elements likely. The idea is to choose   Î»   Î»   \lambda   as small as possible so that all elements are provably covered with non-zero probability. Here is a detailed analysis.   lemma (approximation guarantee for rounding scheme)    Fix    Î»  =   ln   (   2   |  ğ’°  |    )        Î»      2    ğ’°       \lambda=\ln(2|\mathcal{U}|)   . With positive probability, the rounding scheme returns a set cover    x  â€²     superscript  x  normal-â€²    x^{\prime}   of cost at most     2   ln   (   2   |  ğ’°  |    )    c   â‹…   x  *      normal-â‹…    2      2    ğ’°     c    superscript  x      2\ln(2|\mathcal{U}|)c\cdot x^{*}   (and thus of cost    O   (   log   |  ğ’°  |    )       O      ğ’°      O(\log|\mathcal{U}|)   times the cost of the optimal set cover).      (Note: with care the    O   (   log   |  ğ’°  |    )       O      ğ’°      O(\log|\mathcal{U}|)   can be reduced to     ln   (   |  ğ’°  |   )    +   O   (   log   log   |  ğ’°  |     )            ğ’°      O        ğ’°        \ln(|\mathcal{U}|)+O(\log\log|\mathcal{U}|)   .)  proof  The output    x  â€²     superscript  x  normal-â€²    x^{\prime}   of the random rounding scheme has the desired properties as long as none of the following "bad" events occur:   the cost    c  â‹…   x  â€²      normal-â‹…  c   superscript  x  normal-â€²     c\cdot x^{\prime}   of    x  â€²     superscript  x  normal-â€²    x^{\prime}   exceeds     2  Î»  c   â‹…   x  *      normal-â‹…    2  Î»  c    superscript  x      2\lambda c\cdot x^{*}   , or  for some element   e   e   e   ,    x  â€²     superscript  x  normal-â€²    x^{\prime}   fails to cover   e   e   e   .   The expectation of each    x  s  â€²     subscript   superscript  x  normal-â€²   s    x^{\prime}_{s}   is at most    Î»   x  s  *       Î»   superscript   subscript  x  s       \lambda x_{s}^{*}   . By linearity of expectation , the expectation of    c  â‹…   x  â€²      normal-â‹…  c   superscript  x  normal-â€²     c\cdot x^{\prime}   is at most      âˆ‘  s    c   (  s  )   Î»   x  s  *     =    Î»  c   â‹…   x  *          subscript   s     c  s  Î»   superscript   subscript  x  s        normal-â‹…    Î»  c    superscript  x       \sum_{s}c(s)\lambda x_{s}^{*}=\lambda c\cdot x^{*}   . Thus, by Markov's inequality , the probability of the first bad event above is at most    1  /  2      1  2    1/2   .  For the remaining bad events (one for each element   e   e   e   ), note that, since      âˆ‘   s  âˆ‹  e     x  s  *    â‰¥  1        subscript     e  s     subscript   superscript  x    s    1    \sum_{s\ni e}x^{*}_{s}\geq 1   for any given element   e   e   e   , the probability that   e   e   e   is not covered is        âˆ   s  âˆ‹  e      (   1  -   min   (   Î»   x  s  *    ,  1  )     )       subscript  product    e  s      1      Î»   subscript   superscript  x    s    1      \displaystyle\prod_{s\ni e}\big(1-\min(\lambda x^{*}_{s},1)\big)     (This uses the inequality     1  +  z   â‰¤   e  z         1  z    superscript  e  z     1+z\leq e^{z}   , which is strict for    z  â‰   0      z  0    z\neq 0   .)  Thus, for each of the    |  ğ’°  |      ğ’°    |\mathcal{U}|   elements, the probability that the element is not covered is less than    1  /   (   2  ğ’°   )       1    2  ğ’°     1/(2\mathcal{U})   .  By the naive union bound , the probability that one of the    1  +   |  ğ’°  |       1    ğ’°     1+|\mathcal{U}|   bad events happens is less than      1  /  2   +    |  ğ’°  |   /   (   2  ğ’°   )     =  1          1  2       ğ’°     2  ğ’°     1    1/2+|\mathcal{U}|/(2\mathcal{U})=1   . Thus, with positive probability there are no bad events and    x  â€²     superscript  x  normal-â€²    x^{\prime}   is a set cover of cost at most     2  Î»  c   â‹…   x  *      normal-â‹…    2  Î»  c    superscript  x      2\lambda c\cdot x^{*}   . QED  Derandomization using the method of conditional probabilities  The lemma above shows the existence of a set cover of cost    O   (  log   (  |  ğ’°  |  )   c  â‹…   x  *       fragments  O   fragments  normal-(    fragments  normal-(  normal-|  U  normal-|  normal-)   c  normal-â‹…   superscript  x       O(\log(|\mathcal{U}|)c\cdot x^{*}   ). In this context our goal is an efficient approximation algorithm, not just an existence proof, so we are not done.  One approach would be to increase   Î»   Î»   \lambda   a little bit, then show that the probability of success is at least, say, 1/4. With this modification, repeating the random rounding step a few times is enough to ensure a successful outcome with high probability.  That approach weakens the approximation ratio. We next describe a different approach that yields a deterministic algorithm that is guaranteed to match the approximation ratio of the existence proof above. The approach is called the method of conditional probabilities .  The deterministic algorithm emulates the randomized rounding scheme: it considers each set    s  âˆˆ  ğ’®      s  ğ’®    s\in\mathcal{S}   in turn, and chooses     x  s  â€²   âˆˆ   {  0  ,  1  }        subscript   superscript  x  normal-â€²   s    0  1     x^{\prime}_{s}\in\{0,1\}   . But instead of making each choice randomly based on    x  *     superscript  x     x^{*}   , it makes the choice deterministically , so as to keep the conditional probability of failure, given the choices so far, below 1 .  Bounding the conditional probability of failure  We want to be able to set each variable    x  s  â€²     subscript   superscript  x  normal-â€²   s    x^{\prime}_{s}   in turn so as to keep the conditional probability of failure below 1. To do this, we need a good bound on the conditional probability of failure. The bound will come by refining the original existence proof. That proof implicitly bounds the probability of failure by the expectation of the random variable      F  =     c  â‹…   x  â€²      2  Î»  c   â‹…   x  *     +   |   ğ’°   (  m  )    |        F       normal-â‹…  c   superscript  x  normal-â€²     normal-â‹…    2  Î»  c    superscript  x         superscript  ğ’°  m       F=\frac{c\cdot x^{\prime}}{2\lambda c\cdot x^{*}}+|\mathcal{U}^{(m)}|   , where       ğ’°   (  m  )    =   {  e  :     âˆ   s  âˆ‹  e     (   1  -   x  s  â€²    )    =  1   }        superscript  ğ’°  m    conditional-set  e      subscript  product    e  s      1   subscript   superscript  x  normal-â€²   s     1      \mathcal{U}^{(m)}=\Big\{e:\prod_{s\ni e}(1-x^{\prime}_{s})=1\Big\}   is the set of elements left uncovered at the end.  The random variable   F   F   F   may appear a bit mysterious, but it mirrors the probabilistic proof in a systematic way. The first term in   F   F   F   comes from applying Markov's inequality to bound the probability of the first bad event (the cost is too high). It contributes at least 1 to   F   F   F   if the cost of    x  â€²     superscript  x  normal-â€²    x^{\prime}   is too high. The second term counts the number of bad events of the second kind (uncovered elements). It contributes at least 1 to   F   F   F   if    x  â€²     superscript  x  normal-â€²    x^{\prime}   leaves any element uncovered. Thus, in any outcome where   F   F   F   is less than 1,    x  â€²     superscript  x  normal-â€²    x^{\prime}   must cover all the elements and have cost meeting the desired bound from the lemma. In short, if the rounding step fails, then    F  â‰¥  1      F  1    F\geq 1   . This implies (by Markov's inequality ) that     E   [  F  ]       E   delimited-[]  F     E[F]   is an upper bound on the probability of failure. Note that the argument above is implicit already in the proof of the lemma, which also shows by calculation that     E   [  F  ]    <  1        E   delimited-[]  F    1    E[F]<1   .  To apply the method of conditional probabilities, we need to extend the argument to bound the conditional probability of failure as the rounding step proceeds. Usually, this can be done in a systematic way, although it can be technically tedious.  So, what about the conditional probability of failure as the rounding step iterates through the sets? Since    F  â‰¥  1      F  1    F\geq 1   in any outcome where the rounding step fails, by Markov's inequality , the conditional probability of failure is at most the conditional expectation of   F   F   F   .  Next we calculate the conditional expectation of   F   F   F   , much as we calculated the unconditioned expectation of   F   F   F   in the original proof. Consider the state of the rounding process at the end of some iteration   t   t   t   . Let    S   (  t  )      superscript  S  t    S^{(t)}   denote the sets considered so far (the first   t   t   t   sets in   ğ’®   ğ’®   \mathcal{S}   ). Let    x   (  t  )      superscript  x  t    x^{(t)}   denote the (partially assigned) vector    x  â€²     superscript  x  normal-â€²    x^{\prime}   (so    x  s   (  t  )      subscript   superscript  x  t   s    x^{(t)}_{s}   is determined only if    s  âˆˆ   S   (  t  )        s   superscript  S  t     s\in S^{(t)}   ). For each set    s  âˆ‰   S   (  t  )        s   superscript  S  t     s\not\in S^{(t)}   , let     p  s   =   min   (   Î»   x  s  *    ,  1  )         subscript  p  s       Î»   subscript   superscript  x    s    1     p_{s}=\min(\lambda x^{*}_{s},1)   denote the probability with which    x  s  â€²     subscript   superscript  x  normal-â€²   s    x^{\prime}_{s}   will be set to 1. Let    ğ’°   (  t  )      superscript  ğ’°  t    \mathcal{U}^{(t)}   contain the not-yet-covered elements. Then the conditional expectation of   F   F   F   , given the choices made so far, that is, given    x   (  t  )      superscript  x  t    x^{(t)}   , is      E   [  F  |   x   (  t  )    ]   =       âˆ‘   s  âˆˆ   S   (  t  )       c   (  s  )    x  s  â€²     +    âˆ‘   s  âˆ‰   S   (  t  )       c   (  s  )    p  s        2  Î»  c   â‹…   x  *      +   âˆ‘   e  âˆˆ   ğ’°   (  t  )       âˆ    s  âˆ‰   S   (  t  )     ,   s  âˆ‹  e      (  1  -   p  s   )   .     fragments  E   fragments  normal-[  F  normal-|   superscript  x  t   normal-]          subscript     s   superscript  S  t       c  s   subscript   superscript  x  normal-â€²   s       subscript     s   superscript  S  t       c  s   subscript  p  s       normal-â‹…    2  Î»  c    superscript  x        subscript     e   superscript  ğ’°  t      subscript  product   formulae-sequence    s   superscript  S  t      e  s      fragments  normal-(  1    subscript  p  s   normal-)   normal-.    E[F|x^{(t)}]~{}=~{}\frac{\sum_{s\in S^{(t)}}c(s)x^{\prime}_{s}+\sum_{s\not\in S%
 ^{(t)}}c(s)p_{s}}{2\lambda c\cdot x^{*}}~{}+~{}\sum_{e\in\mathcal{U}^{(t)}}%
 \prod_{s\not\in S^{(t)},s\ni e}(1-p_{s}).     Note that    E   [  F  |   x   (  t  )    ]      fragments  E   fragments  normal-[  F  normal-|   superscript  x  t   normal-]     E[F|x^{(t)}]   is determined only after iteration   t   t   t   .  Keeping the conditional probability of failure below 1  To keep the conditional probability of failure below 1, it suffices to keep the conditional expectation of   F   F   F   below 1. To do this, it suffices to keep the conditional expectation of   F   F   F   from increasing. This is what the algorithm will do. It will set    x  s  â€²     subscript   superscript  x  normal-â€²   s    x^{\prime}_{s}   in each iteration to ensure that        E   [  F  |   x   (  m  )    ]   â‰¤  E   [  F  |   x   (   m  -  1   )    ]   â‰¤  â‹¯  â‰¤  E   [  F  |   x   (  1  )    ]   â‰¤  E   [  F  |   x   (  0  )    ]   <  1     fragments  E   fragments  normal-[  F  normal-|   superscript  x  m   normal-]    E   fragments  normal-[  F  normal-|   superscript  x    m  1    normal-]    normal-â‹¯   E   fragments  normal-[  F  normal-|   superscript  x  1   normal-]    E   fragments  normal-[  F  normal-|   superscript  x  0   normal-]    1    E[F|x^{(m)}]\leq E[F|x^{(m-1)}]\leq\cdots\leq E[F|x^{(1)}]\leq E[F|x^{(0)}]<1        (where    m  =   |  ğ’®  |       m    ğ’®     m=|\mathcal{S}|   ).  In the   t   t   t   th iteration, how can the algorithm set    x   s  â€²   â€²     subscript   superscript  x  normal-â€²    superscript  s  normal-â€²     x^{\prime}_{s^{\prime}}   to ensure that    E   [  F  |   x   (  t  )    ]   â‰¤  E   [  F  |   S   (   t  -  1   )    ]      fragments  E   fragments  normal-[  F  normal-|   superscript  x  t   normal-]    E   fragments  normal-[  F  normal-|   superscript  S    t  1    normal-]     E[F|x^{(t)}]\leq E[F|S^{(t-1)}]   ? It turns out that it can simply set    x   s  â€²   â€²     subscript   superscript  x  normal-â€²    superscript  s  normal-â€²     x^{\prime}_{s^{\prime}}   so as to minimize the resulting value of    E   [  F  |   x   (  t  )    ]      fragments  E   fragments  normal-[  F  normal-|   superscript  x  t   normal-]     E[F|x^{(t)}]   .  To see why, focus on the point in time when iteration   t   t   t   starts. At that time,    E   [  F  |   x   (   t  -  1   )    ]      fragments  E   fragments  normal-[  F  normal-|   superscript  x    t  1    normal-]     E[F|x^{(t-1)}]   is determined, but    E   [  F  |   x   (  t  )    ]      fragments  E   fragments  normal-[  F  normal-|   superscript  x  t   normal-]     E[F|x^{(t)}]   is not yet determined --- it can take two possible values depending on how    x   s  â€²   â€²     subscript   superscript  x  normal-â€²    superscript  s  normal-â€²     x^{\prime}_{s^{\prime}}   is set in iteration   t   t   t   . Let    E   (   t  -  1   )      superscript  E    t  1     E^{(t-1)}   denote the value of    E   [  F  |   x   â€²   (   t  -  1   )     ]      fragments  E   fragments  normal-[  F  normal-|   superscript  x   normal-â€²    t  1     normal-]     E[F|x^{\prime(t-1)}]   . Let    E  0   (  t  )      subscript   superscript  E  t   0    E^{(t)}_{0}   and    E  1   (  t  )      subscript   superscript  E  t   1    E^{(t)}_{1}   , denote the two possible values of    E   [  F  |   x   (  t  )    ]      fragments  E   fragments  normal-[  F  normal-|   superscript  x  t   normal-]     E[F|x^{(t)}]   , depending on whether    x   s  â€²   â€²     subscript   superscript  x  normal-â€²    superscript  s  normal-â€²     x^{\prime}_{s^{\prime}}   is set to 0, or 1, respectively. By the definition of conditional expectation,         E^{(t-1)} ~=~ \Pr[x'_{s'}=0] E^{(t)}_0 + \Pr[x'_{s'}=1] E^{(t)}_1.  Since a weighted average of two quantities is always at least the minimum of those two quantities, it follows that         E^{(t-1)} ~\ge~ \min( E^{(t)}_0, E^{(t)}_1 ).  Thus, setting    x   s  â€²   â€²     subscript   superscript  x  normal-â€²    superscript  s  normal-â€²     x^{\prime}_{s^{\prime}}   so as to minimize the resulting value of    E   [  F  |   x   (  t  )    ]      fragments  E   fragments  normal-[  F  normal-|   superscript  x  t   normal-]     E[F|x^{(t)}]   will guarantee that    E   [  F  |   x   (  t  )    ]   â‰¤  E   [  F  |   x   (   t  -  1   )    ]      fragments  E   fragments  normal-[  F  normal-|   superscript  x  t   normal-]    E   fragments  normal-[  F  normal-|   superscript  x    t  1    normal-]     E[F|x^{(t)}]\leq E[F|x^{(t-1)}]   . This is what the algorithm will do.  In detail, what does this mean? Considered as a function of    x   s  â€²   â€²     subscript   superscript  x  normal-â€²    superscript  s  normal-â€²     x^{\prime}_{s^{\prime}}   (with all other quantities fixed)    E   [  F  |   x   (  t  )    ]      fragments  E   fragments  normal-[  F  normal-|   superscript  x  t   normal-]     E[F|x^{(t)}]   is a linear function of    x   s  â€²   â€²     subscript   superscript  x  normal-â€²    superscript  s  normal-â€²     x^{\prime}_{s^{\prime}}   , and the coefficient of    x   s  â€²   â€²     subscript   superscript  x  normal-â€²    superscript  s  normal-â€²     x^{\prime}_{s^{\prime}}   in that function is          c   s  â€²      2  Î»  c   â‹…   x  *      -    âˆ‘   e  âˆˆ    s  â€²   âˆ©   ğ’°   t  -  1         âˆ    s  âˆ‰   S   (  t  )     ,   s  âˆ‹  e      (   1  -   p  s    )      .         subscript  c   superscript  s  normal-â€²     normal-â‹…    2  Î»  c    superscript  x        subscript     e     superscript  s  normal-â€²    subscript  ğ’°    t  1         subscript  product   formulae-sequence    s   superscript  S  t      e  s       1   subscript  p  s        \frac{c_{s^{\prime}}}{2\lambda c\cdot x^{*}}~{}-~{}\sum_{e\in s^{\prime}\cap%
 \mathcal{U}_{t-1}}\prod_{s\not\in S^{(t)},s\ni e}(1-p_{s}).     Thus, the algorithm should set    x   s  â€²   â€²     subscript   superscript  x  normal-â€²    superscript  s  normal-â€²     x^{\prime}_{s^{\prime}}   to 0 if this expression is positive, and 1 otherwise. This gives the following algorithm.  Randomized-rounding algorithm for Set Cover  input: set system   ğ’®   ğ’®   \mathcal{S}   , universe   ğ’°   ğ’°   \mathcal{U}   , cost vector   c   c   c     output: set cover    x  â€²     superscript  x  normal-â€²    x^{\prime}   (a solution to the standard integer linear program for set cover)    Compute a min-cost fractional set cover    x  *     superscript  x     x^{*}   (an optimal solution to the LP relaxation).  Let    Î»  â†   ln   (   2   |  ğ’°  |    )       normal-â†  Î»      2    ğ’°       \lambda\leftarrow\ln(2|\mathcal{U}|)   . Let     p  s   â†   min   (   Î»   x  s  *    ,  1  )       normal-â†   subscript  p  s       Î»   subscript   superscript  x    s    1     p_{s}\leftarrow\min(\lambda x^{*}_{s},1)   for each    s  âˆˆ  ğ’®      s  ğ’®    s\in\mathcal{S}   .  For each     s  â€²   âˆˆ  ğ’®       superscript  s  normal-â€²   ğ’®    s^{\prime}\in\mathcal{S}   do:  Let    ğ’®  â†   ğ’®  -   {   s  â€²   }       normal-â†  ğ’®    ğ’®    superscript  s  normal-â€²       \mathcal{S}\leftarrow\mathcal{S}-\{s^{\prime}\}   . Â  (   ğ’®   ğ’®   \mathcal{S}   contains the not-yet-decided sets.)  If    \frac{c_{s'}}{2\lambda c\cdot x^*} > \sum_{e\in s'\cap\mathcal U} \prod_{s\in \mathcal S, s\ni e}(1-p_s)  ##: then set     x  s  â€²   â†  0     normal-â†   subscript   superscript  x  normal-â€²   s   0    x^{\prime}_{s}\leftarrow 0   ,  ##: else set     x  s  â€²   â†  1     normal-â†   subscript   superscript  x  normal-â€²   s   1    x^{\prime}_{s}\leftarrow 1   and    ğ’°  â†   ğ’°  -   s  â€²       normal-â†  ğ’°    ğ’°   superscript  s  normal-â€²      \mathcal{U}\leftarrow\mathcal{U}-s^{\prime}   .  ##: Â Â (   ğ’°   ğ’°   \mathcal{U}   contains the not-yet-covered elements.)   Return    x  â€²     superscript  x  normal-â€²    x^{\prime}   .    lemma (approximation guarantee for algorithm)    The algorithm above returns a set cover    x  â€²     superscript  x  normal-â€²    x^{\prime}   of cost at most    2   ln   (   2   |  ğ’°  |    )        2      2    ğ’°       2\ln(2|\mathcal{U}|)   times the minimum cost of any (fractional) set cover.      proof   The algorithm ensures that the conditional expectation of   F   F   F   ,    E   [   F   |   x   (  t  )    ]      fragments  E   fragments  normal-[  F  normal-|   superscript  x  t   normal-]     E[F\,|\,x^{(t)}]   , does not increase at each iteration. Since this conditional expectation is initially less than 1 (as shown previously), the algorithm ensures that the conditional expectation stays below 1. Since the conditional probability of failure is at most the conditional expectation of   F   F   F   , in this way the algorithm ensures that the conditional probability of failure stays below 1. Thus, at the end, when all choices are determined, the algorithm reaches a successful outcome. That is, the algorithm above returns a set cover    x  â€²     superscript  x  normal-â€²    x^{\prime}   of cost at most    2   ln   (   2   |  ğ’°  |    )        2      2    ğ’°       2\ln(2|\mathcal{U}|)   times the minimum cost of any (fractional) set cover.  Remarks  In the example above, the algorithm was guided by the conditional expectation of a random variable   F   F   F   . In some cases, instead of an exact conditional expectation, an upper bound (or sometimes a lower bound) on some conditional expectation is used instead. This is called a pessimistic estimator .  See also   Method of conditional probabilities   References    .   .   Further reading       "  Category:Algorithms  Category:Probabilistic arguments     â†©  â†©     