   Jamesâ€“Stein estimator      Jamesâ€“Stein estimator   The Jamesâ€“Stein estimator is a biased  estimator of the mean of Gaussian random vectors. It can be shown that the Jamesâ€“Stein estimator dominates the "ordinary" least squares approach, i.e., it has lower mean squared error on average. It is the best-known example of Stein's phenomenon .  An earlier version of the estimator was developed by Charles Stein in 1956, 1 and is sometimes referred to as Stein's estimator . The result was improved by Willard James and Charles Stein in 1961. 2  Setting  Suppose Î¸ is an unknown parameter vector of length   m   m   m   , and let y be a vector of observations (also of length   m   m   m   ), such that the observations are normally distributed :       ğ²  âˆ¼   N   (  ğœ½  ,    Ïƒ  2   I   )     .     similar-to  ğ²    N   ğœ½     superscript  Ïƒ  2   I       {\mathbf{y}}\sim N({\boldsymbol{\theta}},\sigma^{2}I).\,     We are interested in obtaining an estimate     ğœ½  ^   =    ğœ½  ^    (  ğ²  )         normal-^  ğœ½      normal-^  ğœ½   ğ²     \widehat{\boldsymbol{\theta}}=\widehat{\boldsymbol{\theta}}({\mathbf{y}})   of Î¸ , based on a single observation vector y .  This is an everyday situation in which a set of parameters is measured, and the measurements are corrupted by independent Gaussian noise. Since the noise has zero mean, it is very reasonable to use the measurements themselves as an estimate of the parameters. This is the approach of the least squares estimator, which is      ğœ½  ^    L  S    =  ğ²       subscript   normal-^  ğœ½     L  S    ğ²    \widehat{\boldsymbol{\theta}}_{LS}={\mathbf{y}}   .  As a result, there was considerable shock and disbelief when Stein demonstrated that, in terms of mean squared error     E   {    âˆ¥   ğœ½  -   ğœ½  ^    âˆ¥   2   }       E    superscript   norm    ğœ½   normal-^  ğœ½     2      E\{\|{\boldsymbol{\theta}}-\widehat{\boldsymbol{\theta}}\|^{2}\}   , this approach is suboptimal. 3 The result became known as Stein's phenomenon .  The Jamesâ€“Stein estimator  If    Ïƒ  2     superscript  Ïƒ  2    \sigma^{2}   is known, the Jamesâ€“Stein estimator is given by         ğœ½  ^    J  S    =    (   1  -     (   m  -  2   )    Ïƒ  2      âˆ¥  ğ²  âˆ¥   2     )   ğ²    .       subscript   normal-^  ğœ½     J  S        1        m  2    superscript  Ïƒ  2     superscript   norm  ğ²   2     ğ²     \widehat{\boldsymbol{\theta}}_{JS}=\left(1-\frac{(m-2)\sigma^{2}}{\|{\mathbf{y%
 }}\|^{2}}\right){\mathbf{y}}.     James and Stein showed that the above estimator dominates      ğœ½  ^    L  S      subscript   normal-^  ğœ½     L  S     \widehat{\boldsymbol{\theta}}_{LS}   for any    m  â‰¥  3      m  3    m\geq 3   , meaning that the Jamesâ€“Stein estimator always achieves lower MSE than the maximum likelihood estimator. 4 5 By definition, this makes the least squares estimator inadmissible when    m  â‰¥  3      m  3    m\geq 3   .  Notice that if      (   m  -  2   )    Ïƒ  2    <    âˆ¥  ğ²  âˆ¥   2           m  2    superscript  Ïƒ  2     superscript   norm  ğ²   2     (m-2)\sigma^{2}<\|{\mathbf{y}}\|^{2}   then this estimator simply takes the natural estimator   ğ²   ğ²   \mathbf{y}   and shrinks it towards the origin 0 . In fact this is not the only direction of shrinkage that works. Let Î½ be an arbitrary fixed vector of length   m   m   m   . Then there exists an estimator of the James-Stein type that shrinks toward Î½ , namely         ğœ½  ^    J  S    =     (   1  -     (   m  -  2   )    Ïƒ  2      âˆ¥   ğ²  -  ğ‚   âˆ¥   2     )    (   ğ²  -  ğ‚   )    +  ğ‚    .       subscript   normal-^  ğœ½     J  S          1        m  2    superscript  Ïƒ  2     superscript   norm    ğ²  ğ‚    2       ğ²  ğ‚    ğ‚     \widehat{\boldsymbol{\theta}}_{JS}=\left(1-\frac{(m-2)\sigma^{2}}{\|{\mathbf{y%
 }}-{\boldsymbol{\nu}}\|^{2}}\right)({\mathbf{y}}-{\boldsymbol{\nu}})+{%
 \boldsymbol{\nu}}.     It is interesting to note that the Jamesâ€“Stein estimator dominates the usual estimator for any Î½ . A natural question to ask is whether the improvement over the usual estimator is independent of the choice of Î½ . The answer is no. The improvement is small if    âˆ¥   ğœ½  -  ğ‚   âˆ¥     norm    ğœ½  ğ‚     \|{\boldsymbol{\theta}-\boldsymbol{\nu}}\|   is large. Thus to get a very great improvement some knowledge of the location of Î¸ is necessary. Of course this is the quantity we are trying to estimate so we don't have this knowledge a priori. But we may have some guess as to what the mean vector is. This can be considered a disadvantage of the estimator: the choice is not objective as it may depend on the beliefs of the researcher.  Interpretation  Seeing the Jamesâ€“Stein estimator as an empirical Bayes method gives some intuition to this result: One assumes that Î¸ itself is a random variable with prior distribution      âˆ¼   N   (  0  ,  A  )       similar-to  absent    N   0  A      \sim N(0,A)   , where A is estimated from the data itself. Estimating A only gives an advantage compared to the maximum-likelihood estimator when the dimension   m   m   m   is large enough; hence it does not work for    m  â‰¤  2      m  2    m\leq 2   . The Jamesâ€“Stein estimator is a member of a class of Bayesian estimators that dominate the maximum-likelihood estimator. 6  A consequence of the above discussion is the following counterintuitive result: When three or more unrelated parameters are measured, their total MSE can be reduced by using a combined estimator such as the Jamesâ€“Stein estimator; whereas when each parameter is estimated separately, the least squares (LS) estimator is admissible. A quirky example would be estimating the speed of light, tea consumption in Taiwan, and hog weight in Montana, all together. The Jamesâ€“Stein estimator always improves upon the total MSE, i.e., the sum of the expected errors of each component. Therefore, the total MSE in measuring light speed, tea consumption, and hog weight would improve by using the Jamesâ€“Stein estimator. However, any particular component (such as the speed of light) would improve for some parameter values, and deteriorate for others. Thus, although the Jamesâ€“Stein estimator dominates the LS estimator when three or more parameters are estimated, any single component does not dominate the respective component of the LS estimator.  The conclusion from this hypothetical example is that measurements should be combined if one is interested in minimizing their total MSE. For example, in a telecommunication setting, it is reasonable to combine channel tap measurements in a channel estimation scenario, as the goal is to minimize the total channel estimation error. Conversely, there could be objections to combining channel estimates of different users, since no user would want their channel estimate to deteriorate in order to improve the average network performance.  Improvements  The basic Jamesâ€“Stein estimator has the peculiar property that for small values of     âˆ¥   ğ²  -  ğ‚   âˆ¥   ,     norm    ğ²  ğ‚     \|{\mathbf{y}}-{\boldsymbol{\nu}}\|,   the multiplier on    ğ²  -  ğ‚      ğ²  ğ‚    {\mathbf{y}}-{\boldsymbol{\nu}}   is actually negative. This can be easily remedied by replacing this multiplier by zero when it is negative. The resulting estimator is called the positive-part Jamesâ€“Stein estimator and is given by         ğœ½  ^     J  S   +    =      (   1  -     (   m  -  2   )    Ïƒ  2      âˆ¥   ğ²  -  ğ‚   âˆ¥   2     )   +    (   ğ²  -  ğ‚   )    +  ğ‚    .       subscript   normal-^  ğœ½    limit-from    J  S           superscript    1        m  2    superscript  Ïƒ  2     superscript   norm    ğ²  ğ‚    2         ğ²  ğ‚    ğ‚     \widehat{\boldsymbol{\theta}}_{JS+}=\left(1-\frac{(m-2)\sigma^{2}}{\|{\mathbf{%
 y}}-{\boldsymbol{\nu}}\|^{2}}\right)^{+}({\mathbf{y}}-{\boldsymbol{\nu}})+{%
 \boldsymbol{\nu}}.     This estimator has a smaller risk than the basic Jamesâ€“Stein estimator. It follows that the basic Jamesâ€“Stein estimator is itself inadmissible . 7  It turns out, however, that the positive-part estimator is also inadmissible. 8 This follows from a general result which requires admissible estimators to be smooth.  Extensions  The Jamesâ€“Stein estimator may seem at first sight to be a result of some peculiarity of the problem setting. In fact, the estimator exemplifies a very wide-ranging effect, namely, the fact that the "ordinary" or least squares estimator is often inadmissible for simultaneous estimation of several parameters. This effect has been called Stein's phenomenon , and has been demonstrated for several different problem settings, some of which are briefly outlined below.   James and Stein demonstrated that the estimator presented above can still be used when the variance    Ïƒ  2     superscript  Ïƒ  2    \sigma^{2}   is unknown, by replacing it with the standard estimator of the variance,      Ïƒ  ^   2   =    1  n    âˆ‘    (    y  i   -   y  Â¯    )   2          superscript   normal-^  Ïƒ   2       1  n      superscript     subscript  y  i    normal-Â¯  y    2       \widehat{\sigma}^{2}=\frac{1}{n}\sum(y_{i}-\overline{y})^{2}   . The dominance result still holds under the same condition, namely,    m  >  2      m  2    m>2   . 9  The results in this article are for the case when only a single observation vector y is available. For the more general case when   n   n   n   vectors are available, the results are similar:          \widehat{\boldsymbol \theta}_{JS} = \left( 1 - \frac{(m-2) \frac{\sigma^2}{n}}{\|{\overline{\mathbf y}}\|^2} \right) {\overline{\mathbf y}},   where    ğ²  Â¯     normal-Â¯  ğ²    {\overline{\mathbf{y}}}   is the   m   m   m   -length average of the   n   n   n   observations.    The work of James and Stein has been extended to the case of a general measurement covariance matrix, i.e., where measurements may be statistically dependent and may have differing variances. 10 A similar dominating estimator can be constructed, with a suitably generalized dominance condition. This can be used to construct a linear regression technique which outperforms the standard application of the LS estimator. 11  Stein's result has been extended to a wide class of distributions and loss functions. However, this theory provides only an existence result, in that explicit dominating estimators were not actually exhibited. 12 It is quite difficult to obtain explicit estimators improving upon the usual estimator without specific restrictions on the underlying distributions. 13   See also   Admissible decision rule  Hodgesâ€™ estimator  Shrinkage estimator   References  Further reading     "  Category:Decision theory  Category:Estimation theory     â†©  â†©    â†©  â†©  â†©     â†©  â†©      