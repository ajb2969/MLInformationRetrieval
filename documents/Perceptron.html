<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="658">Perceptron</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Perceptron</h1>
<style>
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
<style>
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</body></html>
<body>
<hr/>

<p>In <a href="machine_learning" title="wikilink">machine learning</a>, the <strong>perceptron</strong> is an algorithm for <a href="supervised_classification" title="wikilink">supervised</a> learning of <a href="binary_classification" title="wikilink">binary classifiers</a>: functions that can decide whether an input (represented by a vector of numbers) belong to one class or another. It is a type of <a href="linear_classifier" title="wikilink">linear classifier</a>, i.e. a classification algorithm that makes its predictions based on a <a href="linear_predictor_function" title="wikilink">linear predictor function</a> combining a set of weights with the <a href="feature_vector" title="wikilink">feature vector</a>. The algorithm allows for <a href="online_algorithm" title="wikilink">online learning</a>, in that it processes elements in the training set one at a time.</p>

<p>The perceptron algorithm dates back to the late 1950s; its first implementation, in custom hardware, was one of the first <a href="artificial_neural_network" title="wikilink">artificial neural networks</a> to be produced.</p>
<h2 id="history">History</h2>
<dl>
<dd><em>See also: <a href="History_of_artificial_intelligence#Perceptrons_and_the_dark_age_of_connectionism" title="wikilink">History of artificial intelligence</a>, <a href="AI_winter#The_abandonment_of_connectionism_in_1969" title="wikilink">AI winter</a></em>
</dd>
</dl>

<p>The perceptron algorithm was invented in 1957 at the <a href="Cornell_Aeronautical_Laboratory" title="wikilink">Cornell Aeronautical Laboratory</a> by <a href="Frank_Rosenblatt" title="wikilink">Frank Rosenblatt</a>,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> funded by the United States <a href="Office_of_Naval_Research" title="wikilink">Office of Naval Research</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> The perceptron was intended to be a machine, rather than a program, and while its first implementation was in software for the <a href="IBM_704" title="wikilink">IBM 704</a>, it was subsequently implemented in custom-built hardware as the "Mark 1 perceptron". This machine was designed for image recognition: it had an array of 400 <a href="photocell" title="wikilink">photocells</a>, randomly connected to the "neurons". Weights were encoded in <a href="potentiometer" title="wikilink">potentiometers</a>, and weight updates during learning were performed by electric motors.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>In a 1958 press conference organized by the US Navy, Rosenblatt made statements about the perceptron that caused a heated controversy among the fledgling <a href="Artificial_intelligence" title="wikilink">AI</a> community; based on Rosenblatt's statements, <em><a href="The_New_York_Times" title="wikilink">The New York Times</a></em> reported the perceptron to be "the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence."<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>Although the perceptron initially seemed promising, it was quickly proved that perceptrons could not be trained to recognise many classes of patterns. This led to the field of neural network research stagnating for many years, before it was recognised that a feedforward neural network with two or more layers (also called a <a href="Feedforward_neural_network#Multi-layer_perceptron" title="wikilink">multilayer perceptron</a>) had far greater processing power than perceptrons with one layer (also called a <a href="Feedforward_neural_network#Single-layer_perceptron" title="wikilink">single layer perceptron</a>). Single layer perceptrons are only capable of learning <a href="linearly_separable" title="wikilink">linearly separable</a> patterns; in 1969 a famous book entitled <em><a href="Perceptrons_(book)" title="wikilink">Perceptrons</a></em> by <a href="Marvin_Minsky" title="wikilink">Marvin Minsky</a> and <a href="Seymour_Papert" title="wikilink">Seymour Papert</a> showed that it was impossible for these classes of network to learn an <a class="uri" href="XOR" title="wikilink">XOR</a> function. It is often believed that they also conjectured (incorrectly) that a similar result would hold for a multi-layer perceptron network. However, this is not true, as both Minsky and Papert already knew that multi-layer perceptrons were capable of producing an XOR function. (See the page on <em><a href="Perceptrons_(book)" title="wikilink">Perceptrons (book)</a></em> for more information.) Three years later <a href="Stephen_Grossberg" title="wikilink">Stephen Grossberg</a> published a series of papers introducing networks capable of modelling differential, contrast-enhancing and XOR functions. (The papers were published in 1972 and 1973, see e.g.:). Nevertheless the often-miscited Minsky/Papert text caused a significant decline in interest and funding of neural network research. It took ten more years until <a href="neural_network" title="wikilink">neural network</a> research experienced a resurgence in the 1980s. This text was reprinted in 1987 as "Perceptrons - Expanded Edition" where some errors in the original text are shown and corrected.</p>

<p>The <a href="kernel_perceptron" title="wikilink">kernel perceptron</a> algorithm was already introduced in 1964 by Aizerman et al.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> Margin bounds guarantees were given for the Perceptron algorithm in the general non-separable case first by <a href="Yoav_Freund" title="wikilink">Freund</a> and <a href="Robert_Schapire" title="wikilink">Schapire</a> (1998),<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> and more recently by <a href="Mehryar_Mohri" title="wikilink">Mohri</a> and Rostamizadeh (2013) who extend previous results and give new L1 bounds.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h2 id="definition">Definition</h2>

<p>In the modern sense, the perceptron is an algorithm for learning a binary classifier: a function that maps its input 

<math display="inline" id="Perceptron:0">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 (a real-valued <a href="Vector_space" title="wikilink">vector</a>) to an output value 

<math display="inline" id="Perceptron:1">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)
  </annotation>
 </semantics>
</math>

 (a single <a href="Binary_function" title="wikilink">binary</a> value):</p>

<p>

<math display="block" id="Perceptron:2">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>{</mo>
    <mtable displaystyle="true">
     <mtr>
      <mtd columnalign="left">
       <mn>1</mn>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mrow>
          <mrow>
           <mtext>if</mtext>
           <mi>w</mi>
          </mrow>
          <mo>⋅</mo>
          <mi>x</mi>
         </mrow>
         <mo>+</mo>
         <mi>b</mi>
        </mrow>
        <mo>></mo>
        <mn>0</mn>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mn>0</mn>
      </mtd>
      <mtd columnalign="left">
       <mtext>otherwise</mtext>
      </mtd>
     </mtr>
    </mtable>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <csymbol cd="latexml">cases</csymbol>
     <cn type="integer">1</cn>
     <apply>
      <gt></gt>
      <apply>
       <plus></plus>
       <apply>
        <ci>normal-⋅</ci>
        <apply>
         <times></times>
         <mtext>if</mtext>
         <ci>w</ci>
        </apply>
        <ci>x</ci>
       </apply>
       <ci>b</ci>
      </apply>
      <cn type="integer">0</cn>
     </apply>
     <cn type="integer">0</cn>
     <mtext>otherwise</mtext>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)=\begin{cases}1&\text{if }w\cdot x+b>0\\
0&\text{otherwise}\end{cases}
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Perceptron:3">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

 is a vector of real-valued weights, 

<math display="inline" id="Perceptron:4">
 <semantics>
  <mrow>
   <mi>w</mi>
   <mo>⋅</mo>
   <mi>x</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-⋅</ci>
    <ci>w</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w\cdot x
  </annotation>
 </semantics>
</math>

 is the <a href="dot_product" title="wikilink">dot product</a> 

<math display="inline" id="Perceptron:5">
 <semantics>
  <mrow>
   <msub>
    <mo largeop="true" symmetric="true">∑</mo>
    <mi>i</mi>
   </msub>
   <mrow>
    <msub>
     <mi>w</mi>
     <mi>i</mi>
    </msub>
    <msub>
     <mi>x</mi>
     <mi>i</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <sum></sum>
     <ci>i</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{i}w_{i}x_{i}
  </annotation>
 </semantics>
</math>

, and 

<math display="inline" id="Perceptron:6">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>

 is the <em>bias</em>, a term that shifts the decision boundary away from the origin and does not depend on any input value.</p>

<p>The value of 

<math display="inline" id="Perceptron:7">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)
  </annotation>
 </semantics>
</math>

 (0 or 1) is used to classify 

<math display="inline" id="Perceptron:8">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 as either a positive or a negative instance, in the case of a binary classification problem. If 

<math display="inline" id="Perceptron:9">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>

 is negative, then the weighted combination of inputs must produce a positive value greater than 

<math display="inline" id="Perceptron:10">
 <semantics>
  <mrow>
   <mo stretchy="false">|</mo>
   <mi>b</mi>
   <mo stretchy="false">|</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <abs></abs>
    <ci>b</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   |b|
  </annotation>
 </semantics>
</math>

 in order to push the classifier neuron over the 0 threshold. Spatially, the bias alters the position (though not the orientation) of the <a href="decision_boundary" title="wikilink">decision boundary</a>. The perceptron learning algorithm does not terminate if the learning set is not <a href="linearly_separable" title="wikilink">linearly separable</a>. If the vectors are not linearly separable learning will never reach a point where all vectors are classified properly. The most famous example of the perceptron's inability to solve problems with linearly nonseparable vectors is the Boolean exclusive-or problem. The solution spaces of decision boundaries for all binary functions and learning behaviors are studied in the reference.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>

<p>In the context of neural networks, a perceptron is an <a href="artificial_neuron" title="wikilink">artificial neuron</a> using the <a href="Heaviside_step_function" title="wikilink">Heaviside step function</a> as the activation function. The perceptron algorithm is also termed the <strong>single-layer perceptron</strong>, to distinguish it from a <a href="multilayer_perceptron" title="wikilink">multilayer perceptron</a>, which is a misnomer for a more complicated neural network. As a linear classifier, the single-layer perceptron is the simplest <a href="feedforward_neural_network" title="wikilink">feedforward neural network</a>.</p>
<h2 id="learning-algorithm">Learning algorithm</h2>

<p>Below is an example of a learning algorithm for a (single-layer) perceptron. For <a href="multilayer_perceptron" title="wikilink">multilayer perceptrons</a>, where a hidden layer exists, more sophisticated algorithms such as <a class="uri" href="backpropagation" title="wikilink">backpropagation</a> must be used. Alternatively, methods such as the <a href="delta_rule" title="wikilink">delta rule</a> can be used if the function is non-linear and differentiable, although the one below will work as well.</p>

<p>When multiple perceptrons are combined in an artificial neural network, each output neuron operates independently of all the others; thus, learning each output can be considered in isolation. <a href="File:Perceptron_example.svg" title="wikilink">500px |thumb |Right |A diagram showing a perceptron updating its linear boundary as more training examples are added.</a></p>
<h3 id="definitions">Definitions</h3>

<p>We first define some variables:</p>
<ul>
<li>

<math display="inline" id="Perceptron:11">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>𝐳</mi>
     <mo rspace="4.2pt" stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>𝐳</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=f(\mathbf{z})\,
  </annotation>
 </semantics>
</math>

 denotes the <em>output</em> from the perceptron for an input vector 

<math display="inline" id="Perceptron:12">
 <semantics>
  <mi>𝐳</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐳</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{z}
  </annotation>
 </semantics>
</math>

.</li>
<li>

<math display="inline" id="Perceptron:13">
 <semantics>
  <mpadded width="+1.7pt">
   <mi>b</mi>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b\,
  </annotation>
 </semantics>
</math>

 is the <em>bias</em> term, which in the example below we take to be 0.</li>
<li>

<math display="inline" id="Perceptron:14">
 <semantics>
  <mrow>
   <mi>D</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>𝐱</mi>
      <mn>1</mn>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>d</mi>
      <mn>1</mn>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>𝐱</mi>
      <mi>s</mi>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>d</mi>
      <mi>s</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo rspace="4.2pt" stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>D</ci>
    <set>
     <interval closure="open">
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>𝐱</ci>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>d</ci>
       <cn type="integer">1</cn>
      </apply>
     </interval>
     <ci>normal-…</ci>
     <interval closure="open">
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>𝐱</ci>
       <ci>s</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>d</ci>
       <ci>s</ci>
      </apply>
     </interval>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D=\{(\mathbf{x}_{1},d_{1}),\dots,(\mathbf{x}_{s},d_{s})\}\,
  </annotation>
 </semantics>
</math>

 is the <em>training set</em> of 

<math display="inline" id="Perceptron:15">
 <semantics>
  <mi>s</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>s</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s
  </annotation>
 </semantics>
</math>

 samples, where:
<ul>
<li>

<math display="inline" id="Perceptron:16">
 <semantics>
  <msub>
   <mi>𝐱</mi>
   <mi>j</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>𝐱</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}_{j}
  </annotation>
 </semantics>
</math>

 is the 

<math display="inline" id="Perceptron:17">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

-dimensional input vector.</li>
<li>

<math display="inline" id="Perceptron:18">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>d</mi>
    <mi>j</mi>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>d</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d_{j}\,
  </annotation>
 </semantics>
</math>

 is the desired output value of the perceptron for that input.</li>
</ul></li>
</ul>

<p>We show the values of the features as follows:</p>
<ul>
<li>

<math display="inline" id="Perceptron:19">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>x</mi>
    <mrow>
     <mi>j</mi>
     <mo>,</mo>
     <mi>i</mi>
    </mrow>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <list>
     <ci>j</ci>
     <ci>i</ci>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{j,i}\,
  </annotation>
 </semantics>
</math>

 is the value of the 

<math display="inline" id="Perceptron:20">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

th feature of the 

<math display="inline" id="Perceptron:21">
 <semantics>
  <mi>j</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>j</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   j
  </annotation>
 </semantics>
</math>

th training <em>input vector</em>.</li>
<li>

<math display="inline" id="Perceptron:22">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mrow>
     <mi>j</mi>
     <mo>,</mo>
     <mn>0</mn>
    </mrow>
   </msub>
   <mo>=</mo>
   <mpadded width="+1.7pt">
    <mn>1</mn>
   </mpadded>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <list>
      <ci>j</ci>
      <cn type="integer">0</cn>
     </list>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{j,0}=1\,
  </annotation>
 </semantics>
</math>

.</li>
</ul>

<p>To represent the weights:</p>
<ul>
<li>

<math display="inline" id="Perceptron:23">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>w</mi>
    <mi>i</mi>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}\,
  </annotation>
 </semantics>
</math>

 is the 

<math display="inline" id="Perceptron:24">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

th value in the <em>weight vector</em>, to be multiplied by the value of the 

<math display="inline" id="Perceptron:25">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

th input feature.</li>
<li>Because 

<math display="inline" id="Perceptron:26">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mrow>
     <mi>j</mi>
     <mo>,</mo>
     <mn>0</mn>
    </mrow>
   </msub>
   <mo>=</mo>
   <mpadded width="+1.7pt">
    <mn>1</mn>
   </mpadded>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <list>
      <ci>j</ci>
      <cn type="integer">0</cn>
     </list>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{j,0}=1\,
  </annotation>
 </semantics>
</math>

, the 

<math display="inline" id="Perceptron:27">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>w</mi>
    <mn>0</mn>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{0}\,
  </annotation>
 </semantics>
</math>

 is effectively a learned bias that we use instead of the bias constant 

<math display="inline" id="Perceptron:28">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>

.</li>
</ul>

<p>To show the time-dependence of 

<math display="inline" id="Perceptron:29">
 <semantics>
  <mi>𝐰</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐰</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}
  </annotation>
 </semantics>
</math>

, we use:</p>
<ul>
<li>

<math display="inline" id="Perceptron:30">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mi>i</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>t</mi>
    <mo rspace="4.2pt" stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <ci>i</ci>
    </apply>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}(t)\,
  </annotation>
 </semantics>
</math>

 is the weight 

<math display="inline" id="Perceptron:31">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

 at time 

<math display="inline" id="Perceptron:32">
 <semantics>
  <mi>t</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>t</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   t
  </annotation>
 </semantics>
</math>

.</li>
<li>

<math display="inline" id="Perceptron:33">
 <semantics>
  <mpadded width="+1.7pt">
   <mi>α</mi>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha\,
  </annotation>
 </semantics>
</math>

 is the <em>learning rate</em>, where 

<math display="inline" id="Perceptron:34">
 <semantics>
  <mrow>
   <mn>0</mn>
   <mo><</mo>
   <mi>α</mi>
   <mo>≤</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <lt></lt>
     <cn type="integer">0</cn>
     <ci>α</ci>
    </apply>
    <apply>
     <leq></leq>
     <share href="#.cmml">
     </share>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   0<\alpha\leq 1
  </annotation>
 </semantics>
</math>

.</li>
</ul>

<p>Too high a learning rate makes the perceptron periodically oscillate around the solution unless <a href="Perceptron#Variants" title="wikilink">additional steps</a> are taken.</p>
<figure><b>(Figure)</b>
<figcaption>The appropriate weights are applied to the inputs, and the resulting weighted sum passed to a function that produces the output y.</figcaption>
</figure>
<h3 id="steps">Steps</h3>

<p>1. Initialize the weights and the threshold. Weights may be initialized to 0 or to a small random value. In the example below, we use 0.</p>

<p>2. For each example 

<math display="inline" id="Perceptron:35">
 <semantics>
  <mpadded width="+1.7pt">
   <mi>j</mi>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <ci>j</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   j\,
  </annotation>
 </semantics>
</math>

 in our training set 

<math display="inline" id="Perceptron:36">
 <semantics>
  <mpadded width="+1.7pt">
   <mi>D</mi>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D\,
  </annotation>
 </semantics>
</math>

, perform the following steps over the input 

<math display="inline" id="Perceptron:37">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>𝐱</mi>
    <mi>j</mi>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>𝐱</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}_{j}\,
  </annotation>
 </semantics>
</math>

 and desired output 

<math display="inline" id="Perceptron:38">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>d</mi>
    <mi>j</mi>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>d</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d_{j}\,
  </annotation>
 </semantics>
</math>

:</p>
<dl>
<dd>2a. Calculate the actual output:

<p>

<math display="block" id="Perceptron:39">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>y</mi>
     <mi>j</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>t</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mrow>
      <mrow>
       <mi>𝐰</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>⋅</mo>
      <msub>
       <mi>𝐱</mi>
       <mi>j</mi>
      </msub>
     </mrow>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mrow>
      <mrow>
       <msub>
        <mi>w</mi>
        <mn>0</mn>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>+</mo>
      <mrow>
       <msub>
        <mi>w</mi>
        <mn>1</mn>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
       <msub>
        <mi>x</mi>
        <mrow>
         <mi>j</mi>
         <mo>,</mo>
         <mn>1</mn>
        </mrow>
       </msub>
      </mrow>
      <mo>+</mo>
      <mrow>
       <msub>
        <mi>w</mi>
        <mn>2</mn>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
       <msub>
        <mi>x</mi>
        <mrow>
         <mi>j</mi>
         <mo>,</mo>
         <mn>2</mn>
        </mrow>
       </msub>
      </mrow>
      <mo>+</mo>
      <mi mathvariant="normal">⋯</mi>
      <mo>+</mo>
      <mrow>
       <msub>
        <mi>w</mi>
        <mi>n</mi>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
       <msub>
        <mi>x</mi>
        <mrow>
         <mi>j</mi>
         <mo>,</mo>
         <mi>n</mi>
        </mrow>
       </msub>
      </mrow>
     </mrow>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>j</ci>
      </apply>
      <ci>t</ci>
     </apply>
     <apply>
      <times></times>
      <ci>f</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <ci>normal-⋅</ci>
        <apply>
         <times></times>
         <ci>𝐰</ci>
         <ci>t</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>𝐱</ci>
         <ci>j</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>f</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <plus></plus>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>w</ci>
          <cn type="integer">0</cn>
         </apply>
         <ci>t</ci>
        </apply>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>w</ci>
          <cn type="integer">1</cn>
         </apply>
         <ci>t</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <list>
           <ci>j</ci>
           <cn type="integer">1</cn>
          </list>
         </apply>
        </apply>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>w</ci>
          <cn type="integer">2</cn>
         </apply>
         <ci>t</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <list>
           <ci>j</ci>
           <cn type="integer">2</cn>
          </list>
         </apply>
        </apply>
        <ci>normal-⋯</ci>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>w</ci>
          <ci>n</ci>
         </apply>
         <ci>t</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <list>
           <ci>j</ci>
           <ci>n</ci>
          </list>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{j}(t)=f[\mathbf{w}(t)\cdot\mathbf{x}_{j}]=f[w_{0}(t)+w_{1}(t)x_{j,1}+w_{2}(%
t)x_{j,2}+\cdots+w_{n}(t)x_{j,n}]
  </annotation>
 </semantics>
</math>

</p>
</dd>
<dd>2b. Update the weights:

<p>

<math display="block" id="Perceptron:40">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>w</mi>
     <mi>i</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>t</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <msub>
      <mi>w</mi>
      <mi>i</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>t</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>α</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <msub>
        <mi>d</mi>
        <mi>j</mi>
       </msub>
       <mo>-</mo>
       <mrow>
        <msub>
         <mi>y</mi>
         <mi>j</mi>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>t</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mpadded width="+1.7pt">
      <msub>
       <mi>x</mi>
       <mrow>
        <mi>j</mi>
        <mo>,</mo>
        <mi>i</mi>
       </mrow>
      </msub>
     </mpadded>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <plus></plus>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <ci>i</ci>
      </apply>
      <ci>t</ci>
     </apply>
     <apply>
      <times></times>
      <ci>α</ci>
      <apply>
       <minus></minus>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>d</ci>
        <ci>j</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>j</ci>
        </apply>
        <ci>t</ci>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <list>
        <ci>j</ci>
        <ci>i</ci>
       </list>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}(t+1)=w_{i}(t)+\alpha(d_{j}-y_{j}(t))x_{j,i}\,
  </annotation>
 </semantics>
</math>

, for all feature 

<math display="inline" id="Perceptron:41">
 <semantics>
  <mrow>
   <mn>0</mn>
   <mo>≤</mo>
   <mi>i</mi>
   <mo>≤</mo>
   <mi>n</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <leq></leq>
     <cn type="integer">0</cn>
     <ci>i</ci>
    </apply>
    <apply>
     <leq></leq>
     <share href="#.cmml">
     </share>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   0\leq i\leq n
  </annotation>
 </semantics>
</math>

.</p>
</dd>
</dl>

<p>3. For <a href="offline_learning" title="wikilink">offline learning</a>, the step 2 may be repeated until the iteration error 

<math display="inline" id="Perceptron:42">
 <semantics>
  <mrow>
   <mfrac>
    <mn>1</mn>
    <mi>s</mi>
   </mfrac>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>j</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>s</mi>
    </msubsup>
    <mrow>
     <mo stretchy="false">|</mo>
     <mrow>
      <msub>
       <mi>d</mi>
       <mi>j</mi>
      </msub>
      <mo>-</mo>
      <mrow>
       <msub>
        <mi>y</mi>
        <mi>j</mi>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo rspace="4.2pt" stretchy="false">|</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <ci>s</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>j</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>s</ci>
     </apply>
     <apply>
      <abs></abs>
      <apply>
       <minus></minus>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>d</ci>
        <ci>j</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>j</ci>
        </apply>
        <ci>t</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{1}{s}\sum_{j=1}^{s}|d_{j}-y_{j}(t)|\,
  </annotation>
 </semantics>
</math>

 is less than a user-specified error threshold 

<math display="inline" id="Perceptron:43">
 <semantics>
  <mpadded width="+1.7pt">
   <mi>γ</mi>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <ci>γ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma\,
  </annotation>
 </semantics>
</math>

, or a predetermined number of iterations have been completed.</p>

<p>The algorithm updates the weights after steps 2a and 2b. These weights are immediately applied to a pair in the training set, and subsequently updated, rather than waiting until all pairs in the training set have undergone these steps.</p>
<h3 id="convergence">Convergence</h3>

<p>The perceptron is a <a href="linear_classifier" title="wikilink">linear classifier</a>, therefore it will never get to the state with all the input vectors classified correctly if the training set 

<math display="inline" id="Perceptron:44">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D
  </annotation>
 </semantics>
</math>

 is not <a href="linearly_separable" title="wikilink">linearly separable</a>, i.e. if the positive examples can not be separated from the negative examples by a hyperplane. In this case, no "approximate" solution will be gradually approached under the standard learning algorithm, but instead learning will fail completely. Hence, if linear separability of the training set is not known a priori, one of the training variants below should be used.</p>

<p>But if the training set <em>is</em> linearly separable, then the perceptron is guaranteed to converge, and there is an upper bound on the number of times the perceptron will adjust its weights during the training.</p>

<p>Suppose that the input vectors from the two classes can be separated by a hyperplane with a margin 

<math display="inline" id="Perceptron:45">
 <semantics>
  <mi>γ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>γ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma
  </annotation>
 </semantics>
</math>

, i.e. there exists a weight vector 

<math display="inline" id="Perceptron:46">
 <semantics>
  <mrow>
   <mrow>
    <mi>𝐰</mi>
    <mo>,</mo>
    <mrow>
     <mo fence="true">||</mo>
     <mi>𝐰</mi>
     <mo fence="true">||</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <list>
     <ci>𝐰</ci>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <ci>𝐰</ci>
     </apply>
    </list>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w},||\mathbf{w}||=1
  </annotation>
 </semantics>
</math>

, and a bias term 

<math display="inline" id="Perceptron:47">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>

 such that 

<math display="inline" id="Perceptron:48">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>𝐰</mi>
     <mo>⋅</mo>
     <msub>
      <mi>𝐱</mi>
      <mi>j</mi>
     </msub>
    </mrow>
    <mo>+</mo>
    <mi>b</mi>
   </mrow>
   <mo>></mo>
   <mi>γ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <apply>
     <plus></plus>
     <apply>
      <ci>normal-⋅</ci>
      <ci>𝐰</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>𝐱</ci>
       <ci>j</ci>
      </apply>
     </apply>
     <ci>b</ci>
    </apply>
    <ci>γ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}\cdot\mathbf{x}_{j}+b>\gamma
  </annotation>
 </semantics>
</math>

 for all 

<math display="inline" id="Perceptron:49">
 <semantics>
  <mrow>
   <mi>j</mi>
   <mo>:</mo>
   <mrow>
    <msub>
     <mi>d</mi>
     <mi>j</mi>
    </msub>
    <mo>=</mo>
    <mn>1</mn>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>j</ci>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>d</ci>
      <ci>j</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   j:d_{j}=1
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Perceptron:50">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>𝐰</mi>
     <mo>⋅</mo>
     <msub>
      <mi>𝐱</mi>
      <mi>j</mi>
     </msub>
    </mrow>
    <mo>+</mo>
    <mi>b</mi>
   </mrow>
   <mo><</mo>
   <mrow>
    <mo>-</mo>
    <mi>γ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <lt></lt>
    <apply>
     <plus></plus>
     <apply>
      <ci>normal-⋅</ci>
      <ci>𝐰</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>𝐱</ci>
       <ci>j</ci>
      </apply>
     </apply>
     <ci>b</ci>
    </apply>
    <apply>
     <minus></minus>
     <ci>γ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}\cdot\mathbf{x}_{j}+b<-\gamma
  </annotation>
 </semantics>
</math>

 for all 

<math display="inline" id="Perceptron:51">
 <semantics>
  <mrow>
   <mi>j</mi>
   <mo>:</mo>
   <mrow>
    <msub>
     <mi>d</mi>
     <mi>j</mi>
    </msub>
    <mo>=</mo>
    <mn>0</mn>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>j</ci>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>d</ci>
      <ci>j</ci>
     </apply>
     <cn type="integer">0</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   j:d_{j}=0
  </annotation>
 </semantics>
</math>

. And also let 

<math display="inline" id="Perceptron:52">
 <semantics>
  <mi>R</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>R</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R
  </annotation>
 </semantics>
</math>

 denote the maximum norm of an input vector. Novikoff (1962) proved that in this case the perceptron algorithm converges after making 

<math display="inline" id="Perceptron:53">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <msup>
      <mi>R</mi>
      <mn>2</mn>
     </msup>
     <mo>/</mo>
     <msup>
      <mi>γ</mi>
      <mn>2</mn>
     </msup>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <divide></divide>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>R</ci>
      <cn type="integer">2</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>γ</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(R^{2}/\gamma^{2})
  </annotation>
 </semantics>
</math>

 updates. The idea of the proof is that the weight vector is always adjusted by a bounded amount in a direction that it has a negative <a href="dot_product" title="wikilink">dot product</a> with, and thus can be bounded above by 

<math display="inline" id="Perceptron:54">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msqrt>
     <mi>t</mi>
    </msqrt>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <root></root>
     <ci>t</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(\sqrt{t})
  </annotation>
 </semantics>
</math>

 where <em>t</em> is the number of changes to the weight vector. But it can also be bounded below by 

<math display="inline" id="Perceptron:55">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>t</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(t)
  </annotation>
 </semantics>
</math>

 because if there exists an (unknown) satisfactory weight vector, then every change makes progress in this (unknown) direction by a positive amount that depends only on the input vector.</p>

<p> While the perceptron algorithm is guaranteed to converge on <em>some</em> solution in the case of a linearly separable training set, it may still pick <em>any</em> solution and problems may admit many solutions of varying quality.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> The <em>perceptron of optimal stability</em>, nowadays better known as the linear <a href="support_vector_machine" title="wikilink">support vector machine</a>, was designed to solve this problem.</p>

<p>The decision boundary of a perceptron is invariant with respect to scaling of the weight vector; that is, a perceptron trained with initial weight vector 

<math display="inline" id="Perceptron:56">
 <semantics>
  <mi>𝐰</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐰</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}
  </annotation>
 </semantics>
</math>

 and learning rate 

<math display="inline" id="Perceptron:57">
 <semantics>
  <mpadded width="+1.7pt">
   <mi>α</mi>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha\,
  </annotation>
 </semantics>
</math>

 behaves identically to a perceptron trained with initial weight vector 

<math display="inline" id="Perceptron:58">
 <semantics>
  <mrow>
   <mi>𝐰</mi>
   <mo>/</mo>
   <mpadded width="+1.7pt">
    <mi>α</mi>
   </mpadded>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <ci>𝐰</ci>
    <ci>α</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}/\alpha\,
  </annotation>
 </semantics>
</math>

 and learning rate 1. Thus, since the initial weights become irrelevant with increasing number of iterations, the learning rate does not matter in the case of the perceptron and is usually just set to 1.</p>
<h2 id="variants">Variants</h2>

<p>The pocket algorithm with ratchet (Gallant, 1990) solves the stability problem of perceptron learning by keeping the best solution seen so far "in its pocket". The pocket algorithm then returns the solution in the pocket, rather than the last solution. It can be used also for non-separable data sets, where the aim is to find a perceptron with a small number of misclassifications. However, these solutions appear purely stochastically and hence the pocket algorithm neither approaches them gradually in the course of learning, nor are they guaranteed to show up within a given number of learning steps.</p>

<p>The Maxover algorithm (Wendemuth, 1995) <a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> is <a href="Robustness_(computer_science)" title="wikilink">"robust"</a> in the sense that it will converge regardless of (prior) knowledge of linear separability of the data set. In the linear separable case, it will solve the training problem - if desired, even with optimal stability (<a href="Hyperplane_separation_theorem" title="wikilink">maximum margin</a> between the classes). For non-separable data sets, it will return a solution with a small number of misclassifications. In all cases, the algorithm gradually approaches the solution in the course of learning, without memorizing previous states and without stochastic jumps. Convergence is to global optimality for separable data sets and to local optimality for non-separable data sets.</p>

<p>In separable problems, perceptron training can also aim at finding the largest separating margin between the classes. The so-called perceptron of optimal stability can be determined by means of iterative training and optimization schemes, such as the Min-Over algorithm (Krauth and Mezard, 1987)<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> or the AdaTron (Anlauf and Biehl, 1989)) .<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> AdaTron uses the fact that the corresponding quadratic optimization problem is convex. The perceptron of optimal stability, together with the <a href="kernel_trick" title="wikilink">kernel trick</a>, are the conceptual foundations of the <a href="support_vector_machine" title="wikilink">support vector machine</a>.</p>

<p>The 

<math display="inline" id="Perceptron:59">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

-perceptron further used a pre-processing layer of fixed random weights, with thresholded output units. This enabled the perceptron to classify <a href=":wiktionary:analogue" title="wikilink">analogue</a> patterns, by projecting them into a <a href="Binary_Space_Partition" title="wikilink">binary space</a>. In fact, for a projection space of sufficiently high dimension, patterns can become linearly separable.</p>

<p>For example, consider the case of having to classify data into two classes. Here is a small such data set, consisting of points coming from two <a href="Gaussian_distribution" title="wikilink">Gaussian distributions</a>.</p>

<p>Image:Two_class_Gaussian_data.png|Two-class Gaussian data Image:Linear_classifier_on_Gaussian_data.png|A linear classifier operating on the original space Image:Hidden_space_linear_classifier_on_Gaussian_data.png|A linear classifier operating on a high-dimensional projection</p>

<p>A linear classifier can only separate points with a <a class="uri" href="hyperplane" title="wikilink">hyperplane</a>, so no linear classifier can classify all the points here perfectly. On the other hand, the data can be projected into a large number of dimensions. In our example, a <a href="random_matrix" title="wikilink">random matrix</a> was used to project the data linearly to a 1000-dimensional space; then each resulting data point was transformed through the <a href="hyperbolic_function" title="wikilink">hyperbolic tangent function</a>. A linear classifier can then separate the data, as shown in the third figure. However the data may still not be completely separable in this space, in which the perceptron algorithm would not converge. In the example shown, <a href="stochastic_gradient_descent" title="wikilink">stochastic steepest gradient descent</a> was used to adapt the parameters.</p>

<p>Another way to solve nonlinear problems without using multiple layers is to use higher order networks (<a href="sigma-pi_unit" title="wikilink">sigma-pi unit</a>). In this type of network, each element in the input vector is extended with each pairwise combination of multiplied inputs (second order). This can be extended to an <em>n</em>-order network.</p>

<p>It should be kept in mind, however, that the best classifier is not necessarily that which classifies all the training data perfectly. Indeed, if we had the prior constraint that the data come from equi-variant Gaussian distributions, the linear separation in the input space is optimal, and the nonlinear solution is <a href="overfitting" title="wikilink">overfitted</a>.</p>

<p>Other linear classification algorithms include <a href="Winnow_(algorithm)" title="wikilink">Winnow</a>, <a href="support_vector_machine" title="wikilink">support vector machine</a> and <a href="logistic_regression" title="wikilink">logistic regression</a>.</p>
<h2 id="example">Example</h2>

<p>A perceptron learns to perform a binary <a href="Sheffer_stroke" title="wikilink">NAND</a> function on inputs 

<math display="inline" id="Perceptron:60">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>x</mi>
    <mn>1</mn>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{1}\,
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Perceptron:61">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>x</mi>
    <mn>2</mn>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{2}\,
  </annotation>
 </semantics>
</math>

.</p>

<p>Inputs

<math display="block" id="Perceptron:62">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>x</mi>
    <mn>0</mn>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{0}\,
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Perceptron:63">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>x</mi>
    <mn>1</mn>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{1}\,
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Perceptron:64">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>x</mi>
    <mn>2</mn>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{2}\,
  </annotation>
 </semantics>
</math>

, with input 

<math display="inline" id="Perceptron:65">
 <semantics>
  <mpadded width="+1.7pt">
   <msub>
    <mi>x</mi>
    <mn>0</mn>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{0}\,
  </annotation>
 </semantics>
</math>

 held constant at 1.</p>

<p>Threshold (

<math display="inline" id="Perceptron:66">
 <semantics>
  <mi>t</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>t</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   t
  </annotation>
 </semantics>
</math>

): 0.5</p>

<p>Bias (

<math display="inline" id="Perceptron:67">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>

): 1</p>

<p>Learning rate (

<math display="inline" id="Perceptron:68">
 <semantics>
  <mi>r</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>r</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r
  </annotation>
 </semantics>
</math>

): 0.1</p>

<p>Training set, consisting of four samples: 

<math display="inline" id="Perceptron:69">
 <semantics>
  <mrow>
   <mo stretchy="false">{</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mn>1</mn>
     <mo>,</mo>
     <mn>0</mn>
     <mo>,</mo>
     <mn>0</mn>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>,</mo>
    <mn>1</mn>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mn>1</mn>
     <mo>,</mo>
     <mn>0</mn>
     <mo>,</mo>
     <mn>1</mn>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>,</mo>
    <mn>1</mn>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mn>1</mn>
     <mo>,</mo>
     <mn>1</mn>
     <mo>,</mo>
     <mn>0</mn>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>,</mo>
    <mn>1</mn>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mn>1</mn>
     <mo>,</mo>
     <mn>1</mn>
     <mo>,</mo>
     <mn>1</mn>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>,</mo>
    <mn>0</mn>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo rspace="4.2pt" stretchy="false">}</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <set>
    <interval closure="open">
     <vector>
      <cn type="integer">1</cn>
      <cn type="integer">0</cn>
      <cn type="integer">0</cn>
     </vector>
     <cn type="integer">1</cn>
    </interval>
    <interval closure="open">
     <vector>
      <cn type="integer">1</cn>
      <cn type="integer">0</cn>
      <cn type="integer">1</cn>
     </vector>
     <cn type="integer">1</cn>
    </interval>
    <interval closure="open">
     <vector>
      <cn type="integer">1</cn>
      <cn type="integer">1</cn>
      <cn type="integer">0</cn>
     </vector>
     <cn type="integer">1</cn>
    </interval>
    <interval closure="open">
     <vector>
      <cn type="integer">1</cn>
      <cn type="integer">1</cn>
      <cn type="integer">1</cn>
     </vector>
     <cn type="integer">0</cn>
    </interval>
   </set>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{((1,0,0),1),((1,0,1),1),((1,1,0),1),((1,1,1),0)\}\,
  </annotation>
 </semantics>
</math>

</p>

<p>In the following, the final weights of one iteration become the initial weights of the next. Each cycle over all the samples in the training set is demarcated with heavy lines.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">
<p>Input</p></th>
<th style="text-align: left;">
<p>rowspan="2" Initial weights</p></th>
<th style="text-align: left;">
<p>Output</p></th>
<th style="text-align: left;">
<p>Error</p></th>
<th style="text-align: left;">
<p>Correction</p></th>
<th style="text-align: left;">
<p>rowspan="2" Final weights</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p>Sensor values</p></td>
<td style="text-align: left;">
<p>Desired output</p></td>
<td style="text-align: left;">
<p>Per sensor</p></td>
<td style="text-align: left;">
<p>Sum</p></td>
<td style="text-align: left;">
<p>Network</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>

<math display="inline" id="Perceptron:70">
 <semantics>
  <msub>
   <mi>x</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{0}
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Perceptron:71">
 <semantics>
  <msub>
   <mi>x</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{1}
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Perceptron:72">
 <semantics>
  <msub>
   <mi>x</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{2}
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Perceptron:73">
 <semantics>
  <mi>z</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>z</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   z
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Perceptron:74">
 <semantics>
  <msub>
   <mi>w</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{0}
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Perceptron:75">
 <semantics>
  <msub>
   <mi>w</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{1}
  </annotation>
 </semantics>
</math>

</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.1</p></td>
<td style="text-align: left;">
<p>0</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.2</p></td>
<td style="text-align: left;">
<p>0</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0.3</p></td>
<td style="text-align: left;">
<p>0.1</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.3</p></td>
<td style="text-align: left;">
<p>0.1</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.4</p></td>
<td style="text-align: left;">
<p>0.1</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.5</p></td>
<td style="text-align: left;">
<p>0.1</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0.5</p></td>
<td style="text-align: left;">
<p>0.1</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.4</p></td>
<td style="text-align: left;">
<p>0</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.5</p></td>
<td style="text-align: left;">
<p>0</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.5</p></td>
<td style="text-align: left;">
<p>0</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0.6</p></td>
<td style="text-align: left;">
<p>0.1</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.5</p></td>
<td style="text-align: left;">
<p>0</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.6</p></td>
<td style="text-align: left;">
<p>0</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.6</p></td>
<td style="text-align: left;">
<p>0</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0.6</p></td>
<td style="text-align: left;">
<p>0</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.5</p></td>
<td style="text-align: left;">
<p>-0.1</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.6</p></td>
<td style="text-align: left;">
<p>-0.1</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.7</p></td>
<td style="text-align: left;">
<p>-0.1</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0.7</p></td>
<td style="text-align: left;">
<p>-0.1</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.6</p></td>
<td style="text-align: left;">
<p>-0.2</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.6</p></td>
<td style="text-align: left;">
<p>-0.2</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.7</p></td>
<td style="text-align: left;">
<p>-0.2</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0.8</p></td>
<td style="text-align: left;">
<p>-0.1</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.7</p></td>
<td style="text-align: left;">
<p>-0.2</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.7</p></td>
<td style="text-align: left;">
<p>-0.2</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.7</p></td>
<td style="text-align: left;">
<p>-0.2</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0.8</p></td>
<td style="text-align: left;">
<p>-0.1</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.7</p></td>
<td style="text-align: left;">
<p>-0.2</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0.7</p></td>
<td style="text-align: left;">
<p>-0.2</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>0</strong></p></td>
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>0.8</strong></p></td>
<td style="text-align: left;">
<p><strong>-0.2</strong></p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>0</strong></p></td>
<td style="text-align: left;">
<p><strong>0.8</strong></p></td>
<td style="text-align: left;">
<p><strong>-0.2</strong></p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>0</strong></p></td>
<td style="text-align: left;">
<p><strong>0</strong></p></td>
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>0.8</strong></p></td>
<td style="text-align: left;">
<p><strong>-0.2</strong></p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>0</strong></p></td>
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>0.8</strong></p></td>
<td style="text-align: left;">
<p><strong>-0.2</strong></p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>0</strong></p></td>
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>0.8</strong></p></td>
<td style="text-align: left;">
<p><strong>-0.2</strong></p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>1</strong></p></td>
<td style="text-align: left;">
<p><strong>0</strong></p></td>
<td style="text-align: left;">
<p><strong>0.8</strong></p></td>
<td style="text-align: left;">
<p><strong>-0.2</strong></p></td>
</tr>
</tbody>
</table>

<p>This example can be implemented in the following <a href="Python_(programming_language)" title="wikilink">Python</a> code.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">threshold <span class="op">=</span> <span class="fl">0.5</span>
learning_rate <span class="op">=</span> <span class="fl">0.1</span>
weights <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]
training_set <span class="op">=</span> [((<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dv">1</span>), ((<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dv">1</span>), ((<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>), <span class="dv">1</span>), ((<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>), <span class="dv">0</span>)]

<span class="kw">def</span> dot_product(values, weights):
    <span class="cf">return</span> <span class="bu">sum</span>(value <span class="op">*</span> weight <span class="cf">for</span> value, weight <span class="op">in</span> <span class="bu">zip</span>(values, weights))

<span class="cf">while</span> <span class="va">True</span>:
    <span class="bu">print</span>(<span class="st">'-'</span> <span class="op">*</span> <span class="dv">60</span>)
    error_count <span class="op">=</span> <span class="dv">0</span>
    <span class="cf">for</span> input_vector, desired_output <span class="op">in</span> training_set:
        <span class="bu">print</span>(weights)
        result <span class="op">=</span> dot_product(input_vector, weights) <span class="op">&gt;</span> threshold
        error <span class="op">=</span> desired_output <span class="op">-</span> result
        <span class="cf">if</span> error <span class="op">!=</span> <span class="dv">0</span>:
            error_count <span class="op">+=</span> <span class="dv">1</span>
            <span class="cf">for</span> index, value <span class="op">in</span> <span class="bu">enumerate</span>(input_vector):
                weights[index] <span class="op">+=</span> learning_rate <span class="op">*</span> error <span class="op">*</span> value
    <span class="cf">if</span> error_count <span class="op">==</span> <span class="dv">0</span>:
        <span class="cf">break</span></code></pre></div>
<h2 id="multiclass-perceptron">Multiclass perceptron</h2>

<p>Like most other techniques for training linear classifiers, the perceptron generalizes naturally to <a href="multiclass_classification" title="wikilink">multiclass classification</a>. Here, the input 

<math display="inline" id="Perceptron:76">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 and the output 

<math display="inline" id="Perceptron:77">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 are drawn from arbitrary sets. A feature representation function 

<math display="inline" id="Perceptron:78">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x,y)
  </annotation>
 </semantics>
</math>

 maps each possible input/output pair to a finite-dimensional real-valued feature vector. As before, the feature vector is multiplied by a weight vector 

<math display="inline" id="Perceptron:79">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

, but now the resulting score is used to choose among many possible outputs:</p>

<p>

<math display="block" id="Perceptron:80">
 <semantics>
  <mrow>
   <mrow>
    <mover accent="true">
     <mi>y</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mrow>
       <msub>
        <mo>argmax</mo>
        <mi>y</mi>
       </msub>
       <mi>f</mi>
      </mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo>,</mo>
       <mi>y</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>⋅</mo>
     <mi>w</mi>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>normal-^</ci>
     <ci>y</ci>
    </apply>
    <apply>
     <ci>normal-⋅</ci>
     <apply>
      <times></times>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>argmax</ci>
        <ci>y</ci>
       </apply>
       <ci>f</ci>
      </apply>
      <interval closure="open">
       <ci>x</ci>
       <ci>y</ci>
      </interval>
     </apply>
     <ci>w</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{y}=\operatorname{argmax}_{y}f(x,y)\cdot w.
  </annotation>
 </semantics>
</math>

</p>

<p>Learning again iterates over the examples, predicting an output for each, leaving the weights unchanged when the predicted output matches the target, and changing them when it does not. The update becomes:</p>

<p>

<math display="block" id="Perceptron:81">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>t</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo>=</mo>
    <mrow>
     <mrow>
      <msub>
       <mi>w</mi>
       <mi>t</mi>
      </msub>
      <mo>+</mo>
      <mrow>
       <mi>f</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo>-</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo>,</mo>
       <mover accent="true">
        <mi>y</mi>
        <mo stretchy="false">^</mo>
       </mover>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <apply>
      <plus></plus>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <plus></plus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <ci>t</ci>
      </apply>
      <apply>
       <times></times>
       <ci>f</ci>
       <interval closure="open">
        <ci>x</ci>
        <ci>y</ci>
       </interval>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>f</ci>
      <interval closure="open">
       <ci>x</ci>
       <apply>
        <ci>normal-^</ci>
        <ci>y</ci>
       </apply>
      </interval>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{t+1}=w_{t}+f(x,y)-f(x,\hat{y}).
  </annotation>
 </semantics>
</math>

</p>

<p>This multiclass formulation reduces to the original perceptron when 

<math display="inline" id="Perceptron:82">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 is a real-valued vector, 

<math display="inline" id="Perceptron:83">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 is chosen from 

<math display="inline" id="Perceptron:84">
 <semantics>
  <mrow>
   <mo stretchy="false">{</mo>
   <mn>0</mn>
   <mo>,</mo>
   <mn>1</mn>
   <mo stretchy="false">}</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <set>
    <cn type="integer">0</cn>
    <cn type="integer">1</cn>
   </set>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{0,1\}
  </annotation>
 </semantics>
</math>

, and 

<math display="inline" id="Perceptron:85">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo>,</mo>
     <mi>y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>y</mi>
    <mi>x</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <interval closure="open">
      <ci>x</ci>
      <ci>y</ci>
     </interval>
    </apply>
    <apply>
     <times></times>
     <ci>y</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x,y)=yx
  </annotation>
 </semantics>
</math>

.</p>

<p>For certain problems, input/output representations and features can be chosen so that 

<math display="inline" id="Perceptron:86">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>argmax</mi>
     <mi>y</mi>
    </msub>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo>,</mo>
     <mi>y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>⋅</mo>
   <mi>w</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-⋅</ci>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>argmax</ci>
      <ci>y</ci>
     </apply>
     <ci>f</ci>
     <interval closure="open">
      <ci>x</ci>
      <ci>y</ci>
     </interval>
    </apply>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{argmax}_{y}f(x,y)\cdot w
  </annotation>
 </semantics>
</math>

 can be found efficiently even though 

<math display="inline" id="Perceptron:87">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 is chosen from a very large or even infinite set.</p>

<p>In recent years, perceptron training has become popular in the field of <a href="natural_language_processing" title="wikilink">natural language processing</a> for such tasks as <a href="part-of-speech_tagging" title="wikilink">part-of-speech tagging</a> and <a href="syntactic_parsing" title="wikilink">syntactic parsing</a> (Collins, 2002).</p>
<h2 id="references">References</h2>
<ul>
<li>Aizerman, M. A. and Braverman, E. M. and Lev I. Rozonoer. Theoretical foundations of the potential function method in pattern recognition learning. Automation and Remote Control, 25:821–837, 1964.</li>
<li>Rosenblatt, Frank (1958), The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain, Cornell Aeronautical Laboratory, Psychological Review, v65, No. 6, pp. 386–408. .</li>
<li>Rosenblatt, Frank (1962), Principles of Neurodynamics. Washington, DC:Spartan Books.</li>
<li>Minsky M. L. and Papert S. A. 1969. <em>Perceptrons</em>. Cambridge, MA: MIT Press.</li>
<li>Gallant, S. I. (1990). <a href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=80230">Perceptron-based learning algorithms.</a> IEEE Transactions on Neural Networks, vol. 1, no. 2, pp. 179–191.</li>
<li>Mohri, Mehryar and Rostamizadeh, Afshin (2013). <a href="http://arxiv.org/pdf/1305.0208.pdf">Perceptron Mistake Bounds</a> arXiv:1305.0208, 2013.</li>
<li>Novikoff, A. B. (1962). On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata, 12, 615-622. Polytechnic Institute of Brooklyn.</li>
<li><a href="Bernard_Widrow" title="wikilink">Widrow, B.</a>, Lehr, M.A., "30 years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation," <em>Proc. IEEE</em>, vol 78, no 9, pp. 1415–1442, (1990).</li>
<li><a href="Michael_Collins_(computational_linguist)" title="wikilink">Collins, M.</a> 2002. Discriminative training methods for hidden Markov models: Theory and experiments with the perceptron algorithm in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP '02).</li>
<li>Yin, Hongfeng (1996), Perceptron-Based Algorithms and Analysis, Spectrum Library, Concordia University, Canada</li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.mathworks.com/matlabcentral/fileexchange/32949-a-perceptron-learns-to-perform-a-binary-nand-function/content/PerceptronImpl.m">A Perceptron implemented in MATLAB to learn binary NAND function</a></li>
<li>Chapter 3 <a href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K3.pdf">Weighted networks - the perceptron</a> and chapter 4 <a href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K4.pdf">Perceptron learning</a> of <a href="http://page.mi.fu-berlin.de/rojas/neural/index.html.html"><em>Neural Networks - A Systematic Introduction</em></a> by <a href="Raúl_Rojas" title="wikilink">Raúl Rojas</a> (ISBN 978-3-540-60505-8)</li>
<li><a href="http://www-cse.ucsd.edu/users/elkan/250B/perceptron.pdf">Explanation of the update rule</a> by Charles Elkan</li>
<li><a href="http://www.csulb.edu/~cwallis/artificialn/History.htm">History of perceptrons</a></li>
<li><a href="http://www.cis.hut.fi/ahonkela/dippa/node41.html">Mathematics of perceptrons</a></li>
</ul>

<p>"</p>

<p><a href="Category:Classification_algorithms" title="wikilink">Category:Classification algorithms</a> <a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a> <a href="Category:Articles_with_example_Python_code" title="wikilink">Category:Articles with example Python code</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Rosenblatt, Frank (1957), The Perceptron--a perceiving and recognizing automaton. Report 85-460-1, Cornell Aeronautical Laboratory.<a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7">Mohri, Mehryar and Rostamizadeh, Afshin (2013). <a href="http://arxiv.org/pdf/1305.0208.pdf">Perceptron Mistake Bounds</a> arXiv:1305.0208, 2013.<a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10">A. Wendemuth. <a href="http://www.iikt.ovgu.de/iesk_media/Downloads/ks/publications/papers/1995/wendemuth1995_learning_unlearnable-p-1452.pdf">Learning the Unlearnable</a>. J. of Physics A: Math. Gen. 28: 5423-5436 (1995)<a href="#fnref10">↩</a></li>
<li id="fn11">W. Krauth and M. Mezard. Learning algorithms with optimal stability in neural networks. J. of Physics A: Math. Gen. 20: L745-L752 (1987)<a href="#fnref11">↩</a></li>
<li id="fn12">J.K. Anlauf and M. Biehl. The AdaTron: an Adaptive Perceptron algorithm. Europhysics Letters 10: 687-692 (1989)<a href="#fnref12">↩</a></li>
</ol>
</section>
</body>

