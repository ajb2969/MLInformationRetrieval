   Inequalities in information theory      Inequalities in information theory   Inequalities are very important in the study of information theory . There are a number of different contexts in which these inequalities appear.  Shannon-type inequalities  Consider a finite collection of finitely (or at most countably) supported  random variables on the same probability space . For a collection of n random variables, there are 2 n − 1 such non-empty subsets for which entropies can be defined. For example, when n = 2, we may consider the entropies     H   (   X  1   )    ,      H   subscript  X  1     H(X_{1}),        H   (   X  2   )    ,      H   subscript  X  2     H(X_{2}),   and     H   (   X  1   ,   X  2   )    ,      H    subscript  X  1    subscript  X  2      H(X_{1},X_{2}),   and express the following inequalities (which together characterize the range of the marginal and joint entropies of two random variables):        H   (   X  1   )    ≥  0        H   subscript  X  1    0    H(X_{1})\geq 0          H   (   X  2   )    ≥  0        H   subscript  X  2    0    H(X_{2})\geq 0          H   (   X  1   )    ≤   H   (   X  1   ,   X  2   )          H   subscript  X  1      H    subscript  X  1    subscript  X  2       H(X_{1})\leq H(X_{1},X_{2})          H   (   X  2   )    ≤   H   (   X  1   ,   X  2   )          H   subscript  X  2      H    subscript  X  1    subscript  X  2       H(X_{2})\leq H(X_{1},X_{2})           H   (   X  1   ,   X  2   )    ≤    H   (   X  1   )    +   H   (   X  2   )      .        H    subscript  X  1    subscript  X  2         H   subscript  X  1      H   subscript  X  2       H(X_{1},X_{2})\leq H(X_{1})+H(X_{2}).      In fact, these can all be expressed as special cases of a single inequality involving the conditional mutual information , namely      I   (  A  ;  B  |  C  )   ≥  0  ,     fragments  I   fragments  normal-(  A  normal-;  B  normal-|  C  normal-)    0  normal-,    I(A;B|C)\geq 0,   where   A   A   A   ,   B   B   B   , and   C   C   C   each denote the joint distribution of some arbitrary (possibly empty) subset of our collection of random variables. Inequalities that can be derived from this are known as Shannon-type inequalities. More formally (following the notation of Yeung 1 ), define    Γ  n  *     subscript   superscript  normal-Γ    n    \Gamma^{*}_{n}   to be the set of all constructible points in     ℝ    2  n   -  1    ,     superscript  ℝ     superscript  2  n   1     \mathbb{R}^{2^{n}-1},   where a point is said to be constructible if and only if there is a joint, discrete distribution of n random variables such that each coordinate of that point, indexed by a non-empty subset of {1, 2, ..., n }, is equal to the joint entropy of the corresponding subset of the n random variables. The closure of    Γ  n  *     subscript   superscript  normal-Γ    n    \Gamma^{*}_{n}   is denoted      Γ  n  *   ¯   .     normal-¯   subscript   superscript  normal-Γ    n     \overline{\Gamma^{*}_{n}}.   In general        Γ  n  *   ⊆    Γ  n  *   ¯   ⊆   Γ  n    .         subscript   superscript  normal-Γ    n    normal-¯   subscript   superscript  normal-Γ    n          subscript  normal-Γ  n      \Gamma^{*}_{n}\subseteq\overline{\Gamma^{*}_{n}}\subseteq\Gamma_{n}.     The cone in    ℝ    2  n   -  1      superscript  ℝ     superscript  2  n   1     \mathbb{R}^{2^{n}-1}   characterized by all Shannon-type inequalities among n random variables is denoted     Γ  n   .     subscript  normal-Γ  n    \Gamma_{n}.   Software has been developed to automate the task of proving such inequalities 2 . 3 Given an inequality, such software is able to determine whether the given inequality contains the cone     Γ  n   ,     subscript  normal-Γ  n    \Gamma_{n},   in which case the inequality can be verified, since      Γ  n  *   ⊆   Γ  n    .       subscript   superscript  normal-Γ    n    subscript  normal-Γ  n     \Gamma^{*}_{n}\subseteq\Gamma_{n}.     Non-Shannon-type inequalities  Other, less trivial inequalities have been discovered among the entropies and joint entropies of four or more random variables, which cannot be derived from Shannon's basic inequalities. These are known as non-Shannon-type inequalities. In 1997 and 1998, Zhang and Yeung reported two non-Shannon-type inequalities. 4 5 The latter implies that         Γ  n  *   ¯   ⊂   Γ  n    ,       normal-¯   subscript   superscript  normal-Γ    n     subscript  normal-Γ  n     \overline{\Gamma^{*}_{n}}\subset\Gamma_{n},   where the inclusions are proper for    n  ≥  4.      n  4.    n\geq 4.   The two sets above are, in fact, convex cones .  Further non-Shannon-type inequalities were reported in. 6 7 8 Dougherty et al. 9 found a number of non-Shannon-type inequalities by computer search. Matus 10 proved the existence of infinitely many linear non-Shannon-type inequalities.  Lower bounds for the Kullback–Leibler divergence  A great many important inequalities in information theory are actually lower bounds for the Kullback–Leibler divergence . Even the Shannon-type inequalities can be considered part of this category, since the bivariate mutual information can be expressed as the Kullback–Leibler divergence of the joint distribution with respect to the product of the marginals, and thus these inequalities can be seen as a special case of Gibbs' inequality .  On the other hand, it seems to be much more difficult to derive useful upper bounds for the Kullback–Leibler divergence. This is because the Kullback–Leibler divergence D KL ( P || Q ) depends very sensitively on events that are very rare in the reference distribution Q . D KL ( P || Q ) increases without bound as an event of finite non-zero probability in the distribution P becomes exceedingly rare in the reference distribution Q , and in fact D KL ( P || Q ) is not even defined if an event of non-zero probability in P has zero probability in Q . (Hence the requirement that P be absolutely continuous with respect to Q .)  Gibbs' inequality  This fundamental inequality states that the Kullback–Leibler divergence is non-negative.  Kullback's inequality  Another inequality concerning the Kullback–Leibler divergence is known as Kullback's inequality . 11 If P and Q are probability distributions on the real line with P  absolutely continuous with respect to Q, and whose first moments exist, then       D   K  L     (  P  ∥  Q  )   ≥   Ψ  Q  *    (   μ  1  ′    (  P  )   )   ,     fragments   subscript  D    K  L     fragments  normal-(  P  parallel-to  Q  normal-)     superscript   subscript  normal-Ψ  Q      fragments  normal-(   subscript   superscript  μ  normal-′   1    fragments  normal-(  P  normal-)   normal-)   normal-,    D_{KL}(P\|Q)\geq\Psi_{Q}^{*}(\mu^{\prime}_{1}(P)),   where    Ψ  Q  *     superscript   subscript  normal-Ψ  Q      \Psi_{Q}^{*}   is the large deviations  rate function , i.e. the convex conjugate of the cumulant -generating function, of Q , and     μ  1  ′    (  P  )        subscript   superscript  μ  normal-′   1   P    \mu^{\prime}_{1}(P)   is the first moment of P .  The Cramér–Rao bound is a corollary of this result.  Pinsker's inequality  Pinsker's inequality relates Kullback–Leibler divergence and total variation distance . It states that if P , Q are two probability distributions , then          1  2    D   K  L    (  e  )     (  P  ∥  Q  )     ≥   sup   {   |    P   (  A  )    -   Q   (  A  )     |   :   A  is an event to which probabilities are assigned.   }     .         fragments    1  2    superscript   subscript  D    K  L    e    fragments  normal-(  P  parallel-to  Q  normal-)      supremum   conditional-set        P  A     Q  A       A  is an event to which probabilities are assigned.       \sqrt{\frac{1}{2}D_{KL}^{(e)}(P\|Q)}\geq\sup\{|P(A)-Q(A)|:A\text{ is an event %
 to which probabilities are assigned.}\}.     where       D   K  L    (  e  )     (  P  |  |  Q  )      fragments   superscript   subscript  D    K  L    e    fragments  normal-(  P  normal-|  normal-|  Q  normal-)     D_{KL}^{(e)}(P||Q)     is the Kullback–Leibler divergence in nats and       sup  A    |    P   (  A  )    -   Q   (  A  )     |       subscript  supremum  A         P  A     Q  A       \sup_{A}|P(A)-Q(A)|\,     is the total variation distance.  Other inequalities  Hirschman uncertainty  In 1957, 12 Hirschman showed that for a (reasonably well-behaved) function    f  :   ℝ  →  ℂ      normal-:  f   normal-→  ℝ  ℂ     f:\mathbb{R}\rightarrow\mathbb{C}   such that       ∫   -  ∞   ∞       |   f   (  x  )    |   2    d  x    =  1   ,        superscript   subscript             superscript      f  x    2   d  x    1    \int_{-\infty}^{\infty}|f(x)|^{2}\,dx=1,   and its Fourier transform       g   (  y  )    =    ∫   -  ∞   ∞    f   (  x  )     e   -   2  π  i  x  y      d  x     ,        g  y     superscript   subscript            f  x   superscript  e      2  π  i  x  y     d  x      g(y)=\int_{-\infty}^{\infty}f(x)e^{-2\pi ixy}\,dx,   the sum of the differential entropies of     |  f  |   2     superscript    f   2    |f|^{2}   and     |  g  |   2     superscript    g   2    |g|^{2}   is non-negative, i.e.        -    ∫   -  ∞   ∞      |   f   (  x  )    |   2    log      |   f   (  x  )    |   2    d  x       -    ∫   -  ∞   ∞      |   g   (  y  )    |   2    log      |   g   (  y  )    |   2    d  y       ≥  0.            superscript   subscript             superscript      f  x    2        superscript      f  x    2   d  x         superscript   subscript             superscript      g  y    2        superscript      g  y    2   d  y       0.    -\int_{-\infty}^{\infty}|f(x)|^{2}\log|f(x)|^{2}\,dx-\int_{-\infty}^{\infty}|g%
 (y)|^{2}\log|g(y)|^{2}\,dy\geq 0.   Hirschman conjectured, and it was later proved, 13 that a sharper bound of     log   (   e  /  2   )    ,        e  2     \log(e/2),   which is attained in the case of a Gaussian distribution , could replace the right-hand side of this inequality. This is especially significant since it implies, and is stronger than, Weyl's formulation of Heisenberg's uncertainty principle .  Tao's inequality  Given discrete random variables   X   X   X   ,   Y   Y   Y   , and    Y  ′     superscript  Y  normal-′    Y^{\prime}   , such that   X   X   X   takes values only in the interval [−1, 1] and    Y  ′     superscript  Y  normal-′    Y^{\prime}   is determined by   Y   Y   Y   (so that    H   (   Y  ′   |  Y  )   =  0     fragments  H   fragments  normal-(   superscript  Y  normal-′   normal-|  Y  normal-)    0    H(Y^{\prime}|Y)=0   ), we have 14 15      𝔼   (  |  𝔼   (  X  |   Y  ′   )   -  𝔼   (  X  |  Y  )   |  )   ≤    2  log   2   I   (  X  ;  Y  |   Y  ′   )     ,     fragments  E   fragments  normal-(  normal-|  E   fragments  normal-(  X  normal-|   superscript  Y  normal-′   normal-)    E   fragments  normal-(  X  normal-|  Y  normal-)   normal-|  normal-)       fragments  2   2  I   fragments  normal-(  X  normal-;  Y  normal-|   superscript  Y  normal-′   normal-)     normal-,    \mathbb{E}\big(\big|\mathbb{E}(X|Y^{\prime})-\mathbb{E}(X|Y)\big|\big)\leq%
 \sqrt{2\log 2\,I(X;Y|Y^{\prime})},     relating the conditional expectation to the conditional mutual information . This is a simple consequence of Pinsker's inequality . (Note: the correction factor log 2 inside the radical arises because we are measuring the conditional mutual information in bits rather than nats .)  See also   Cramér–Rao bound  Entropy power inequality  Fano's inequality  Jensen's inequality  Kraft inequality  Pinsker's inequality  Multivariate mutual information   References    External links   Thomas M. Cover, Joy A. Thomas. Elements of Information Theory , Chapter 16, "Inequalities in Information Theory" John Wiley & Sons, Inc. 1991 Print ISBN 0-471-06259-6 Online ISBN 0-471-20061-1 pdf  Amir Dembo, Thomas M. Cover, Joy A. Thomas. Information Theoretic Inequalities. IEEE Transactions on Information Theory, Vol. 37, No. 6, November 1991. pdf   "    Category:Entropy and information  Category:Information theory     ) ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩     