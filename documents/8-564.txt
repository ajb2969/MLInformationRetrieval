   Cross-entropy method      Cross-entropy method   The cross-entropy (CE) method attributed to Reuven Rubinstein is a general Monte Carlo approach to combinatorial and continuous multi-extremal optimization and importance sampling . The method originated from the field of rare event simulation , where very small probabilities need to be accurately estimated, for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems. The CE method can be applied to static and noisy combinatorial optimization problems such as the traveling salesman problem , the quadratic assignment problem , DNA sequence alignment , the max-cut problem and the buffer allocation problem, as well as continuous global optimization problems with many local extrema .  In a nutshell the CE method consists of two phases:   Generate a random data sample (trajectories, vectors, etc.) according to a specified mechanism.  Update the parameters of the random mechanism based on the data to produce a "better" sample in the next iteration. This step involves minimizing the cross-entropy or Kullbackâ€“Leibler divergence .   Estimation via importance sampling  Consider the general problem of estimating the quantity    â„“  =    ğ”¼  ğ®    [   H   (  ğ—  )    ]    =   âˆ«   H   (  ğ±  )   f   (  ğ±  ;  ğ®  )   d  ğ±          normal-â„“     subscript  ğ”¼  ğ®    delimited-[]    H  ğ—              H  ğ±  f   ğ±  ğ®   d  ğ±       \ell=\mathbb{E}_{\mathbf{u}}[H(\mathbf{X})]=\int H(\mathbf{x})\,f(\mathbf{x};%
 \mathbf{u})\,\textrm{d}\mathbf{x}   , where   H   H   H   is some performance function and    f   (  ğ±  ;  ğ®  )       f   ğ±  ğ®     f(\mathbf{x};\mathbf{u})   is a member of some parametric family of distributions. Using importance sampling this quantity can be estimated as     â„“  ^   =    1  N     âˆ‘   i  =  1   N    H   (   ğ—  i   )     f   (   ğ—  i   ;  ğ®  )     g   (   ğ—  i   )             normal-^  normal-â„“       1  N     superscript   subscript     i  1    N     H   subscript  ğ—  i       f    subscript  ğ—  i   ğ®      g   subscript  ğ—  i          \hat{\ell}=\frac{1}{N}\sum_{i=1}^{N}H(\mathbf{X}_{i})\frac{f(\mathbf{X}_{i};%
 \mathbf{u})}{g(\mathbf{X}_{i})}   , where     ğ—  1   ,  â€¦  ,   ğ—  N       subscript  ğ—  1   normal-â€¦   subscript  ğ—  N     \mathbf{X}_{1},\dots,\mathbf{X}_{N}   is a random sample from    g    g   g\,   . For positive   H   H   H   , the theoretically optimal importance sampling density (pdf) is given by      g  *    (  ğ±  )    =    H   (  ğ±  )   f   (  ğ±  ;  ğ®  )    /  â„“          superscript  g    ğ±       H  ğ±  f   ğ±  ğ®    normal-â„“     g^{*}(\mathbf{x})=H(\mathbf{x})f(\mathbf{x};\mathbf{u})/\ell   . This, however, depends on the unknown   â„“   normal-â„“   \ell   . The CE method aims to approximate the optimal PDF by adaptively selecting members of the parametric family that are closest (in the Kullbackâ€“Leibler sense) to the optimal PDF    g  *     superscript  g     g^{*}   .  Generic CE algorithm   Choose initial parameter vector    ğ¯   (  0  )      superscript  ğ¯  0    \mathbf{v}^{(0)}   ; set t = 1.  Generate a random sample     ğ—  1   ,  â€¦  ,   ğ—  N       subscript  ğ—  1   normal-â€¦   subscript  ğ—  N     \mathbf{X}_{1},\dots,\mathbf{X}_{N}   from    f   (  â‹…  ;   ğ¯   (   t  -  1   )    )       f   normal-â‹…   superscript  ğ¯    t  1       f(\cdot;\mathbf{v}^{(t-1)})     Solve for    ğ¯   (  t  )      superscript  ğ¯  t    \mathbf{v}^{(t)}   , where      ğ¯   (  t  )    =    argmax  ğ¯     1  N     âˆ‘   i  =  1   N    H   (   ğ—  i   )     f   (   ğ—  i   ;  ğ®  )     f   (   ğ—  i   ;   ğ¯   (   t  -  1   )    )      log  f    (   ğ—  i   ;  ğ¯  )            superscript  ğ¯  t     subscript  argmax  ğ¯       1  N     superscript   subscript     i  1    N     H   subscript  ğ—  i       f    subscript  ğ—  i   ğ®      f    subscript  ğ—  i    superscript  ğ¯    t  1         f     subscript  ğ—  i   ğ¯         \mathbf{v}^{(t)}=\mathop{\textrm{argmax}}_{\mathbf{v}}\frac{1}{N}\sum_{i=1}^{N%
 }H(\mathbf{X}_{i})\frac{f(\mathbf{X}_{i};\mathbf{u})}{f(\mathbf{X}_{i};\mathbf%
 {v}^{(t-1)})}\log f(\mathbf{X}_{i};\mathbf{v})     If convergence is reached then stop ; otherwise, increase t by 1 and reiterate from step 2.   In several cases, the solution to step 3 can be found analytically . Situations in which this occurs are   When    f    f   f\,   belongs to the natural exponential family  When    f    f   f\,   is discrete with finite support  When     H   (  ğ—  )    =   I   {  ğ±  âˆˆ  A  }          H  ğ—    subscript  normal-I   fragments  normal-{  x   A  normal-}      H(\mathbf{X})=\mathrm{I}_{\{\mathbf{x}\in A\}}   and     f   (   ğ—  i   ;  ğ®  )    =   f   (   ğ—  i   ;   ğ¯   (   t  -  1   )    )          f    subscript  ğ—  i   ğ®      f    subscript  ğ—  i    superscript  ğ¯    t  1        f(\mathbf{X}_{i};\mathbf{u})=f(\mathbf{X}_{i};\mathbf{v}^{(t-1)})   , then    ğ¯   (  t  )      superscript  ğ¯  t    \mathbf{v}^{(t)}   corresponds to the maximum likelihood estimator based on those     ğ—  k   âˆˆ  A       subscript  ğ—  k   A    \mathbf{X}_{k}\in A   .   Continuous optimizationâ€”example  The same CE algorithm can be used for optimization, rather than estimation. Suppose the problem is to maximize some function    S   (  x  )       S  x    S(x)   , for example,     S   (  x  )    =    e   -    (   x  -  2   )   2     +    0.8    e   -    (   x  +  2   )   2             S  x      superscript  e     superscript    x  2   2       0.8   superscript  e     superscript    x  2   2         S(x)=\textrm{e}^{-(x-2)^{2}}+0.8\,\textrm{e}^{-(x+2)^{2}}   . To apply CE, one considers first the associated stochastic problem of estimating     â„™  ğœ½    (  S   (  X  )   â‰¥  Î³  )      fragments   subscript  â„™  ğœ½    fragments  normal-(  S   fragments  normal-(  X  normal-)    Î³  normal-)     \mathbb{P}_{\boldsymbol{\theta}}(S(X)\geq\gamma)   for a given level     Î³    Î³   \gamma\,   , and parametric family    {   f   (  â‹…  ;  ğœ½  )    }       f   normal-â‹…  ğœ½      \left\{f(\cdot;\boldsymbol{\theta})\right\}   , for example the 1-dimensional Gaussian distribution , parameterized by its mean     Î¼  t      subscript  Î¼  t    \mu_{t}\,   and variance    Ïƒ  t  2     superscript   subscript  Ïƒ  t   2    \sigma_{t}^{2}   (so    ğœ½  =   (  Î¼  ,   Ïƒ  2   )       ğœ½   Î¼   superscript  Ïƒ  2      \boldsymbol{\theta}=(\mu,\sigma^{2})   here). Hence, for a given    Î³    Î³   \gamma\,   , the goal is to find   ğœ½   ğœ½   \boldsymbol{\theta}   so that     D  KL    (   I   {  S   (  x  )   â‰¥  Î³  }    âˆ¥   f  ğœ½   )      fragments   subscript  D  KL    fragments  normal-(   subscript  I   fragments  normal-{  S   fragments  normal-(  x  normal-)    Î³  normal-}    parallel-to   subscript  f  ğœ½   normal-)     D_{\mathrm{KL}}(\textrm{I}_{\{S(x)\geq\gamma\}}\|f_{\boldsymbol{\theta}})   is minimized. This is done by solving the sample version (stochastic counterpart) of the KL divergence minimization problem, as in step 3 above. It turns out that parameters that minimize the stochastic counterpart for this choice of target distribution and parametric family are the sample mean and sample variance corresponding to the elite samples , which are those samples that have objective function value     â‰¥  Î³      absent  Î³    \geq\gamma   . The worst of the elite samples is then used as the level parameter for the next iteration. This yields the following randomized algorithm that happens to coincide with the so-called Estimation of Multivariate Normal Algorithm (EMNA), an estimation of distribution algorithm .  Pseudo-code  1.Â mu:=-6;Â sigma2:=100;Â t:=0;Â maxits=100;Â Â Â Â //Â InitializeÂ parameters  2.Â N:=100;Â Ne:=10;Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â //  3.Â whileÂ t epsilonÂ Â Â Â Â //Â WhileÂ maxitsÂ notÂ exceededÂ andÂ notÂ converged  4.Â Â XÂ =Â SampleGaussian(mu,sigma2,N);Â Â Â Â Â Â Â Â Â //Â ObtainÂ NÂ samplesÂ fromÂ currentÂ samplingÂ distribution  5.Â Â SÂ =Â exp(-(X-2)^2)Â +Â 0.8Â exp(-(X+2)^2);Â Â Â //Â EvaluateÂ objectiveÂ functionÂ atÂ sampledÂ points  6.Â Â XÂ =Â sort(X,S);Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â //Â SortÂ XÂ byÂ objectiveÂ functionÂ valuesÂ (inÂ descendingÂ order)  7.Â Â muÂ =Â mean(X(1:Ne));Â sigma2=var(X(1:Ne));Â //Â UpdateÂ parametersÂ ofÂ samplingÂ distribution  8.Â Â tÂ =Â t+1;Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â //Â IncrementÂ iterationÂ counter  9.Â returnÂ muÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â //Â ReturnÂ meanÂ ofÂ finalÂ samplingÂ distributionÂ asÂ solution  Related methods   Simulated annealing  Genetic algorithms  Harmony search  Estimation of distribution algorithm  Tabu search   See also   Cross entropy  Kullbackâ€“Leibler divergence  Randomized algorithm  Importance sampling   References   De Boer, P-T., Kroese, D.P, Mannor, S. and Rubinstein, R.Y. (2005). A Tutorial on the Cross-Entropy Method. Annals of Operations Research , 134 (1), 19â€“67. 1  Rubinstein, R.Y. (1997). Optimization of Computer simulation Models with Rare Events, European Journal of Operations Research , 99 , 89â€“112.  Rubinstein, R.Y., Kroese, D.P. (2004). The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning , Springer-Verlag, New York.   External links   Homepage for the CE method    CEoptim R package   "  Category:Heuristics  Category:Optimization algorithms and methods  Category:Monte Carlo methods  Category:Machine learning   