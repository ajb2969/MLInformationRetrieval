   Adjusted mutual information      Adjusted mutual information   In probability theory and information theory , adjusted mutual information , a variation of mutual information may be used for comparing clusterings. 1 It corrects the effect of agreement solely due to chance between clusterings, similar to the way the adjusted rand index corrects the Rand index . It is closely related to variation of information : 2 when a similar adjustment is made to the VI index, it becomes equivalent to the AMI. 3 The adjusted measure however is no longer metrical. 4  Mutual Information of two Partitions  Given a set S of N elements    S  =   {   s  1   ,   s  2   ,   …   s  N    }       S    subscript  s  1    subscript  s  2     normal-…   subscript  s  N       S=\{s_{1},s_{2},\ldots s_{N}\}   , consider two partitions of S , namely    U  =   {   U  1   ,   U  2   ,  …  ,   U  R   }       U    subscript  U  1    subscript  U  2   normal-…   subscript  U  R      U=\{U_{1},U_{2},\ldots,U_{R}\}   with R clusters, and    V  =   {   V  1   ,   V  2   ,  …  ,   V  C   }       V    subscript  V  1    subscript  V  2   normal-…   subscript  V  C      V=\{V_{1},V_{2},\ldots,V_{C}\}   with C clusters. It is presumed here that the partitions are so-called hard clusters; the partitions are pairwise disjoint:        U  i   ∩   U  j    =    V  i   ∩   V  j    =  ∅           subscript  U  i    subscript  U  j       subscript  V  i    subscript  V  j             U_{i}\cap U_{j}=V_{i}\cap V_{j}=\varnothing   for all    i  ≠  j      i  j    i\neq j   , and complete:        ∪   i  =  1   R    U  i    =    ∪   j  =  1   C    V  j    =  S          superscript   subscript     i  1    R    subscript  U  i      superscript   subscript     j  1    C    subscript  V  j         S     \cup_{i=1}^{R}U_{i}=\cup_{j=1}^{C}V_{j}=S   The mutual information of cluster overlap between U and V can be summarized in the form of an R x C  contingency table     M  =    [   n   i  j    ]    j  =   1  …  C     i  =   1  …  R         M   subscript   superscript   delimited-[]   subscript  n    i  j       i    1  normal-…  R       j    1  normal-…  C       M=[n_{ij}]^{i=1\ldots R}_{j=1\ldots C}   , where    n   i  j      subscript  n    i  j     n_{ij}   denotes the number of objects that are common to clusters    U  i     subscript  U  i    U_{i}   and    V  j     subscript  V  j    V_{j}   . That is,       n   i  j    =   |    U  i   ∩   V  j    |        subscript  n    i  j         subscript  U  i    subscript  V  j       n_{ij}=\left|U_{i}\cap V_{j}\right|     Suppose an object is picked at random from S ; the probability that the object falls into cluster    U  i     subscript  U  i    U_{i}   is:       P   (  i  )    =    |   U  i   |   N         P  i        subscript  U  i    N     P(i)=\frac{|U_{i}|}{N}   The entropy associated with the partitioning U is:       H   (  U  )    =   -    ∑   i  =  1   R    P   (  i  )    log  P    (  i  )            H  U       superscript   subscript     i  1    R     P  i    P   i       H(U)=-\sum_{i=1}^{R}P(i)\log P(i)    H(U) is non-negative and takes the value 0 only when there is no uncertainty determining an object's cluster membership, i.e. , when there is only one cluster. Similarly, the entropy of the clustering V can be calculated as:       H   (  V  )    =   -    ∑   j  =  1   C     P  ′    (  j  )    log   P  ′     (  j  )            H  V       superscript   subscript     j  1    C      superscript  P  normal-′   j     superscript  P  normal-′    j       H(V)=-\sum_{j=1}^{C}P^{\prime}(j)\log P^{\prime}(j)   where      P  ′    (  j  )    =    |   V  j   |   /  N          superscript  P  normal-′   j        subscript  V  j    N     P^{\prime}(j)={|V_{j}|}/{N}   . The mutual information (MI) between two partitions:       M  I   (  U  ,  V  )    =    ∑   i  =  1   R     ∑   j  =  1   C    P   (  i  ,  j  )    log    P   (  i  ,  j  )     P   (  i  )    P  ′    (  j  )               M  I   U  V      superscript   subscript     i  1    R     superscript   subscript     j  1    C     P   i  j         P   i  j      P  i   superscript  P  normal-′   j          MI(U,V)=\sum_{i=1}^{R}\sum_{j=1}^{C}P(i,j)\log\frac{P(i,j)}{P(i)P^{\prime}(j)}   where P(i,j) denotes the probability that a point belongs to both the cluster    U  i     subscript  U  i    U_{i}   in U and cluster    V  j     subscript  V  j    V_{j}   in V :       P   (  i  ,  j  )    =    |    U  i   ∩   V  j    |   N         P   i  j           subscript  U  i    subscript  V  j     N     P(i,j)=\frac{|U_{i}\cap V_{j}|}{N}   MI is a non-negative quantity upper bounded by the entropies H ( U ) and H ( V ). It quantifies the information shared by the two clusterings and thus can be employed as a clustering similarity measure.  Adjustment for chance  Like the Rand index , the baseline value of mutual information between two random clusterings does not take on a constant value, and tends to be larger when the two partitions have a larger number of clusters (with a fixed number of set elements N ). By adopting a hypergeometric model of randomness, it can be shown that the expected mutual information between two random clusterings is:       E   {   M  I   (  U  ,  V  )    }    =         E     M  I   U  V      absent    \displaystyle E\{MI(U,V)\}=   where     (     a  i   +   b  j    -  N   )   +     superscript       subscript  a  i    subscript  b  j    N      (a_{i}+b_{j}-N)^{+}   denotes    max   (  1  ,     a  i   +   b  j    -  N   )       1       subscript  a  i    subscript  b  j    N     \max(1,a_{i}+b_{j}-N)   . The variables    a  i     subscript  a  i    a_{i}   and    b  j     subscript  b  j    b_{j}   are partial sums of the contingency table; that is,       a  i   =    ∑   j  =  1   C    n   i  j          subscript  a  i     superscript   subscript     j  1    C    subscript  n    i  j       a_{i}=\sum_{j=1}^{C}n_{ij}   and       b  j   =    ∑   i  =  1   R    n   i  j          subscript  b  j     superscript   subscript     i  1    R    subscript  n    i  j       b_{j}=\sum_{i=1}^{R}n_{ij}     The adjusted measure 5 for the mutual information may then be defined to be:       A  M  I   (  U  ,  V  )    =     M  I   (  U  ,  V  )    -   E   {   M  I   (  U  ,  V  )    }       max   {   H   (  U  )    ,   H   (  V  )    }    -   E   {   M  I   (  U  ,  V  )    }            A  M  I   U  V          M  I   U  V      E     M  I   U  V             H  U     H  V      E     M  I   U  V          AMI(U,V)=\frac{MI(U,V)-E\{MI(U,V)\}}{\max{\{H(U),H(V)\}}-E\{MI(U,V)\}}   .  The AMI takes a value of 1 when the two partitions are identical and 0 when the MI between two partitions equals to that expected by chance.  References    External links   Matlab code for computing the adjusted mutual information   "  Category:Information theory  Category:Clustering criteria     ↩  ↩   ↩      