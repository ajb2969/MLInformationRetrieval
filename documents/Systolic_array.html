<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="273">Systolic array</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Systolic array</h1>
<hr/>

<p> In <a href="parallel_computing" title="wikilink">parallel</a> <a href="computer_architectures" title="wikilink">computer architectures</a>, a <strong>systolic array</strong> is a homogeneous <a href="Graph_(mathematics)" title="wikilink">network</a> of tightly coupled <a href="Data_Processing_Unit" title="wikilink">Data Processing Units</a> (DPUs) called cells or <a href="Node_(computer_science)" title="wikilink">nodes</a>. Each node or DPU independently computes a partial result as a function of the data received from its upstream neighbors, stores the result within itself and passes it downstream. Systolic arrays are often classified as Multiple Instruction Single Data (<a class="uri" href="MISD" title="wikilink">MISD</a>) architectures under <a href="Flynn's_Taxonomy" title="wikilink">Flynn's Taxonomy</a>, although this classification is somewhat controversial because a strong argument can be made to distinguish systolic arrays from any of Flynn's four categories: <a class="uri" href="SISD" title="wikilink">SISD</a>, <a class="uri" href="SIMD" title="wikilink">SIMD</a>, <a class="uri" href="MISD" title="wikilink">MISD</a>, <a class="uri" href="MIMD" title="wikilink">MIMD</a>, as discussed later in this article.</p>

<p>The <a href="parallel_computing" title="wikilink">parallel</a> input <a class="uri" href="data" title="wikilink">data</a> flows through a <a href="Graph_(mathematics)" title="wikilink">network</a> of hard-wired <a href="Microprocessor" title="wikilink">processor</a> nodes, resembling the human <a class="uri" href="brain" title="wikilink">brain</a> which combine, process, <a href="merge_algorithm" title="wikilink">merge</a> or <a href="sorting_algorithm" title="wikilink">sort</a> the input data into a derived result. Because the <a class="uri" href="wave" title="wikilink">wave</a>-like propagation of <a class="uri" href="data" title="wikilink">data</a> through a <a class="uri" href="systolic" title="wikilink">systolic</a> array resembles the <a class="uri" href="pulse" title="wikilink">pulse</a> of the human circulatory system, the name <a class="uri" href="systolic" title="wikilink">systolic</a> was coined from medical terminology. The name is derived from <a href="Systole_(medicine)" title="wikilink">Systole (medicine)</a> as an analogy to the regular pumping of blood by the heart.</p>
<h2 id="applications">Applications</h2>

<p>Systolic arrays are often hard-wired for specific operations, such as "multiply and accumulate", to perform massively <a href="parallel_computing" title="wikilink">parallel</a> integration, <a class="uri" href="convolution" title="wikilink">convolution</a>, <a class="uri" href="correlation" title="wikilink">correlation</a>, <a href="matrix_multiplication" title="wikilink">matrix multiplication</a> or data sorting tasks.</p>
<h2 id="architecture">Architecture</h2>

<p>A systolic array typically consists of a large <a href="monolithic_system" title="wikilink">monolithic</a> <a href="Graph_(mathematics)" title="wikilink">network</a> of primitive computing <a href="Node_(computer_science)" title="wikilink">nodes</a> which can be hardwired or software configured for a specific application. The nodes are usually fixed and identical, while the interconnect is programmable. The more general <strong>wavefront</strong> processors, by contrast, employ sophisticated and individually programmable nodes which may or may not be monolithic, depending on the array size and design parameters. The other distinction is that systolic arrays rely on <a class="uri" href="synchronous" title="wikilink">synchronous</a> data transfers, while <a class="uri" href="wavefront" title="wikilink">wavefront</a> tend to work <a href="asynchronous" title="wikilink">asynchronously</a>.</p>

<p>Unlike the more common <a href="Von_Neumann_architecture" title="wikilink">Von Neumann architecture</a>, where program execution follows a script of instructions stored in common memory, <a href="address_space" title="wikilink">addressed</a> and sequenced under the control of the <a class="uri" href="CPU" title="wikilink">CPU</a>'s <a href="program_counter" title="wikilink">program counter</a> (PC), the individual nodes within a systolic array are triggered by the arrival of new data and always process the data in exactly the same way. The actual processing within each node may be hard wired or block <a href="microcode" title="wikilink">microcoded</a>, in which case the common node personality can be block programmable.</p>

<p>The systolic array paradigm with data-streams driven by data <a href="Counter_(digital)" title="wikilink">counters</a>, is the counterpart of the Von Neumann architecture with instruction-stream driven by a program counter. Because a systolic array usually sends and receives multiple data streams, and multiple data counters are needed to generate these data streams, it supports <a href="data_parallelism" title="wikilink">data parallelism</a>.</p>

<p>The actual nodes can be simple and hardwired or consist of more sophisticated units using micro code, which may be block programmable.</p>
<h2 id="goals-and-benefits">Goals and benefits</h2>

<p>A major benefit of systolic arrays is that all operand data and partial results are stored within (passing through) the processor array. There is no need to access external buses, main memory or internal caches during each operation as is the case with Von Neumann or <a href="Harvard_architecture" title="wikilink">Harvard</a> sequential machines. The sequential limits on <a href="parallel_computing" title="wikilink">parallel</a> performance dictated by <a href="Amdahl's_Law" title="wikilink">Amdahl's Law</a> also do not apply in the same way, because data dependencies are implicitly handled by the programmable <a href="Node_(computer_science)" title="wikilink">node</a> interconnect and there are no sequential steps in managing the highly <a href="parallel_computing" title="wikilink">parallel</a> data flow.</p>

<p>Systolic arrays are therefore extremely good at artificial intelligence, image processing, pattern recognition, computer vision and other tasks which animal brains do so particularly well. Wavefront processors in general can also be very good at machine learning by implementing self configuring neural nets in hardware.</p>
<h2 id="classification-controversy">Classification controversy</h2>

<p>While systolic arrays are officially classified as <a class="uri" href="MISD" title="wikilink">MISD</a>, their classification is somewhat problematic. Because the input is typically a vector of independent values, the systolic array is definitely not <a class="uri" href="SISD" title="wikilink">SISD</a>. Since these <a href="input_(computer_science)" title="wikilink">input</a> values are merged and combined into the result(s) and do not maintain their <a class="uri" href="independence" title="wikilink">independence</a> as they would in a <a class="uri" href="SIMD" title="wikilink">SIMD</a> vector processing unit, the <a href="array_data_structure" title="wikilink">array</a> cannot be classified as such. Consequently, the <a href="array_data_structure" title="wikilink">array</a> cannot be classified as a <a class="uri" href="MIMD" title="wikilink">MIMD</a> either, because <a class="uri" href="MIMD" title="wikilink">MIMD</a> can be viewed as a mere collection of smaller <a class="uri" href="SISD" title="wikilink">SISD</a> and <a class="uri" href="SIMD" title="wikilink">SIMD</a> machines.</p>

<p>Finally, because the data <a class="uri" href="swarm" title="wikilink">swarm</a> is transformed as it passes through the <a href="array_data_structure" title="wikilink">array</a> from <a href="Node_(computer_science)" title="wikilink">node</a> to node, the multiple nodes are not operating on the same data, which makes the <a class="uri" href="MISD" title="wikilink">MISD</a> classification a <a class="uri" href="misnomer" title="wikilink">misnomer</a>. The other reason why a systolic array should not qualify as a <strong>MISD</strong> is the same as the one which disqualifies it from the <a class="uri" href="SISD" title="wikilink">SISD</a> category: The input data is typically a vector not a <strong>s</strong>ingle <strong>d</strong>ata value, although one could argue that any given input vector is a single data set.</p>

<p>All of the above not withstanding, systolic arrays are often offered as a classic example of <a class="uri" href="MISD" title="wikilink">MISD</a> architecture in textbooks on <a href="parallel_computing" title="wikilink">parallel computing</a> and in the engineering class. If the <a href="array_data_structure" title="wikilink">array</a> is viewed from the outside as <a href="atomic_operation" title="wikilink">atomic</a> it should perhaps be classified as <strong>SFMuDMeR</strong> = Single Function, Multiple Data, Merged Result(s).</p>
<h2 id="detailed-description">Detailed description</h2>

<p>A systolic array is composed of matrix-like rows of <a href="data_processing_unit" title="wikilink">data processing units</a> called cells. Data processing units (DPUs) are similar to <a href="central_processing_unit" title="wikilink">central processing units</a> (CPUs), (except for the usual lack of a <a href="program_counter" title="wikilink">program counter</a>,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> since operation is <a href="transport_triggered_architecture" title="wikilink">transport-triggered</a>, i.e., by the arrival of a data object). Each cell shares the information with its neighbors immediately after processing. The systolic array is often rectangular where data flows across the array between neighbour <a href="data_processing_unit" title="wikilink">DPUs</a>, often with different data flowing in different directions. The data streams entering and leaving the ports of the array are generated by auto-sequencing memory units, ASMs. Each ASM includes a data <a href="Counter_(digital)" title="wikilink">counter</a>. In <a href="embedded_system" title="wikilink">embedded systems</a> a data stream may also be input from and/or output to an external source.</p>

<p>An example of a systolic <a class="uri" href="algorithm" title="wikilink">algorithm</a> might be designed for <a href="matrix_multiplication" title="wikilink">matrix multiplication</a>. One <a href="matrix_(math)" title="wikilink">matrix</a> is fed in a row at a time from the top of the array and is passed down the array, the other matrix is fed in a column at a time from the left hand side of the array and passes from left to right. Dummy values are then passed in until each processor has seen one whole row and one whole column. At this point, the result of the multiplication is stored in the array and can now be output a row or a column at a time, flowing down or across the array.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>Systolic arrays are arrays of <a href="data_processing_unit" title="wikilink">DPUs</a> which are connected to a small number of nearest neighbour DPUs in a mesh-like topology. DPUs perform a sequence of operations on data that flows between them. Because the traditional systolic array synthesis methods have been practiced by algebraic algorithms, only uniform arrays with only linear pipes can be obtained, so that the architectures are the same in all DPUs. The consequence is, that only applications with regular data dependencies can be implemented on classical systolic arrays. Like <a class="uri" href="SIMD" title="wikilink">SIMD</a> machines, clocked systolic arrays compute in "lock-step" with each processor undertaking alternate compute | communicate phases. But systolic arrays with asynchronous handshake between DPUs are called <em>wavefront arrays</em>. One well-known systolic array is Carnegie Mellon University's <a class="uri" href="iWarp" title="wikilink">iWarp</a> processor, which has been manufactured by Intel. An iWarp system has a linear array processor connected by data buses going in both directions.</p>
<h2 id="history">History</h2>

<p>Systolic arrays ( y = ( ... ( ( (a_n*x + a_{n-1})*x + a_{n-2})*x + a_{n-3})*x + ... + a_1)*x + a_0 </p>

<p>A linear systolic array in which the processors are arranged in pairs: one multiplies its input by 

<math display="inline" id="Systolic_array:0">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 and passes the result to the right, the next adds 

<math display="inline" id="Systolic_array:1">
 <semantics>
  <msub>
   <mi>a</mi>
   <mi>j</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>a</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a_{j}
  </annotation>
 </semantics>
</math>

 and passes the result to the right:</p>
<h2 id="advantages-and-disadvantages">Advantages and disadvantages</h2>

<p>Pros</p>
<ul>
<li>Faster</li>
<li>Scalable</li>
</ul>

<p>Cons</p>
<ul>
<li>Expensive</li>
<li>Highly specialized, custom hardware is required often application specific.</li>
<li>Not widely implemented</li>
<li>Limited code base of programs and algorithms.</li>
</ul>
<h2 id="implementations">Implementations</h2>

<p><a class="uri" href="Cisco" title="wikilink">Cisco</a> PXF network processor is internally organized as systolic array.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a class="uri" href="MISD" title="wikilink">MISD</a> - Multiple Instruction Single Data, Example: Systolic Arrays</li>
<li><a class="uri" href="iWarp" title="wikilink">iWarp</a> - Systolic Array Computer, VLSI, Intel/CMU</li>
<li><a href="WARP_(systolic_array)" title="wikilink">WARP (systolic array)</a> - Systolic Array Computer, GE/CMU</li>
</ul>
<h2 id="notes">Notes</h2>
<references>
</references>
<h2 id="references">References</h2>
<ul>
<li>H. T. Kung, C. E. Leiserson: Algorithms for VLSI processor arrays; in: C. Mead, L. Conway (eds.): Introduction to VLSI Systems; Addison-Wesley, 1979</li>
<li>S. Y. Kung: VLSI Array Processors; Prentice-Hall, Inc., 1988</li>
<li>N. Petkov: Systolic Parallel Processing; North Holland Publishing Co, 1992</li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.iti.fh-flensburg.de/lang/papers/isa/index.htm"><em>Instruction Systolic Array (ISA)</em></a></li>
<li><a href="http://ieeexplore.ieee.org/iel5/92/4292150/04292156.pdf">'A VLSI Architecture for Image Registration in Real Time' (Based on systolic array), Vol. 15, September 2007</a></li>
</ul>

<p>"</p>

<p><a href="Category:Parallel_computing" title="wikilink">Category:Parallel computing</a> <a href="Category:Reconfigurable_computing" title="wikilink">Category:Reconfigurable computing</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">The Paracel GeneMatcher series of systolic array processors do have a <a href="program_counter" title="wikilink">program counter</a>. More complicated algorithms are implemented as a series of simple steps, with shifts specified in the instructions.<a href="#fnref1">↩</a></li>
<li id="fn2"><a href="http://web.cecs.pdx.edu/~mperkows/temp/May22/0020.Matrix-multiplication-systolic.pdf">Systolic Array Matrix Multiplication</a><a href="#fnref2">↩</a></li>
<li id="fn3"><a class="uri" href="http://www.cisco.com/en/US/prod/collateral/routers/ps133/prod_white_paper09186a008008902a.html">http://www.cisco.com/en/US/prod/collateral/routers/ps133/prod_white_paper09186a008008902a.html</a><a href="#fnref3">↩</a></li>
</ol>
</section>
</body>
</html>
