   Wiener filter      Wiener filter   In signal processing , the Wiener filter is a filter used to produce an estimate of a desired or target random process by linear time-invariant ( LTI ) filtering of an observed noisy process, assuming known stationary signal and noise spectra, and additive noise. The Wiener filter minimizes the mean square error between the estimated random process and the desired process.  Description  The goal of the Wiener filter is to compute a statistical estimate of an unknown signal using a related signal as an input and filtering that known signal to produce the estimate as an output. For example, the known signal might consist of an unknown signal of interest that has been corrupted by additive noise . The Wiener filter can be used to filter out the noise from the corrupted signal to provide an estimate of the underlying signal of interest. The Wiener filter is based on a statistical approach, and a more statistical account of the theory is given in the minimum mean-square error (MMSE) article.  Typical deterministic filters are designed for a desired frequency response . However, the design of the Wiener filter takes a different approach. One is assumed to have knowledge of the spectral properties of the original signal and the noise, and one seeks the linear time-invariant filter whose output would come as close to the original signal as possible. Wiener filters are characterized by the following: 1   Assumption: signal and (additive) noise are stationary linear stochastic processes with known spectral characteristics or known autocorrelation and cross-correlation  Requirement: the filter must be physically realizable/ causal (this requirement can be dropped, resulting in a non-causal solution)  Performance criterion: minimum mean-square error (MMSE)   This filter is frequently used in the process of deconvolution ; for this application, see Wiener deconvolution .  Wiener filter solutions  The Wiener filter problem has solutions for three possible cases: one where a noncausal filter is acceptable (requiring an infinite amount of both past and future data), the case where a causal filter is desired (using an infinite amount of past data), and the finite impulse response (FIR) case where a finite amount of past data is used. The first case is simple to solve but is not suited for real-time applications. Wiener's main accomplishment was solving the case where the causality requirement is in effect, and in an appendix of Wiener's book Levinson gave the FIR solution.  Noncausal solution        G   (  s  )    =      S   x  ,  s     (  s  )      S  x    (  s  )      e   α  s      .        G  s          subscript  S   x  s    s      subscript  S  x   s     superscript  e    α  s       G(s)=\frac{S_{x,s}(s)}{S_{x}(s)}e^{\alpha s}.     Where   S   S   S   are spectra. Provided that    g   (  t  )       g  t    g(t)   is optimal, then the minimum mean-square error equation reduces to        E   (   e  2   )    =     R  s    (  0  )    -    ∫   -  ∞   ∞    g   (  τ  )    R   x  ,  s     (   τ  +  α   )   d  τ      ,        E   superscript  e  2         subscript  R  s   0     superscript   subscript            g  τ   subscript  R   x  s      τ  α   d  τ       E(e^{2})=R_{s}(0)-\int_{-\infty}^{\infty}{g(\tau)R_{x,s}(\tau+\alpha)\,d\tau},     and the solution    g   (  t  )       g  t    g(t)   is the inverse two-sided Laplace transform of    G   (  s  )       G  s    G(s)   .  Causal solution        G   (  s  )    =    H   (  s  )      S  x  +    (  s  )      ,        G  s       H  s      superscript   subscript  S  x     s      G(s)=\frac{H(s)}{S_{x}^{+}(s)},     where       H   (  s  )       H  s    H(s)   consists of the causal part of       S   x  ,  s     (  s  )      S  x  -    (  s  )      e   α  s             subscript  S   x  s    s      superscript   subscript  S  x     s     superscript  e    α  s      \frac{S_{x,s}(s)}{S_{x}^{-}(s)}e^{\alpha s}   (that is, that part of this fraction having a positive time solution under the inverse Laplace transform)       S  x  +    (  s  )        superscript   subscript  S  x     s    S_{x}^{+}(s)   is the causal component of     S  x    (  s  )        subscript  S  x   s    S_{x}(s)   (i.e., the inverse Laplace transform of     S  x  +    (  s  )        superscript   subscript  S  x     s    S_{x}^{+}(s)   is non-zero only for     t   ≥  0      t  0    t\,\geq\,0   )       S  x  -    (  s  )        superscript   subscript  S  x     s    S_{x}^{-}(s)   is the anti-causal component of     S  x    (  s  )        subscript  S  x   s    S_{x}(s)   (i.e., the inverse Laplace transform of     S  x  -    (  s  )        superscript   subscript  S  x     s    S_{x}^{-}(s)   is non-zero only for    t  <  0      t  0    t<0   )   This general formula is complicated and deserves a more detailed explanation. To write down the solution    G   (  s  )       G  s    G(s)   in a specific case, one should follow these steps: 2   Start with the spectrum     S  x    (  s  )        subscript  S  x   s    S_{x}(s)   in rational form and factor it into causal and anti-causal components:         S  x    (  s  )    =    S  x  +    (  s  )    S  x  -    (  s  )           subscript  S  x   s      superscript   subscript  S  x     s   superscript   subscript  S  x     s     S_{x}(s)=S_{x}^{+}(s)S_{x}^{-}(s)       where    S  +     superscript  S     S^{+}   contains all the zeros and poles in the left half plane (LHP) and    S  -     superscript  S     S^{-}   contains the zeroes and poles in the right half plane (RHP). This is called the Wiener–Hopf factorization .  Divide     S   x  ,  s     (  s  )    e   α  s         subscript  S   x  s    s   superscript  e    α  s      S_{x,s}(s)e^{\alpha s}   by     S  x  -    (  s  )        superscript   subscript  S  x     s    S_{x}^{-}(s)   and write out the result as a partial fraction expansion.  Select only those terms in this expansion having poles in the LHP. Call these terms    H   (  s  )       H  s    H(s)   .  Divide    H   (  s  )       H  s    H(s)   by     S  x  +    (  s  )        superscript   subscript  S  x     s    S_{x}^{+}(s)   . The result is the desired filter transfer function    G   (  s  )       G  s    G(s)   .   Finite impulse response Wiener filter for discrete series  The causal finite impulse response (FIR) Wiener filter, instead of using some given data matrix X and output vector Y, finds optimal tap weights by using the statistics of the input and output signals. It populates the input matrix X with estimates of the auto-correlation of the input signal (T) and populates the output vector Y with estimates of the cross-correlation between the output and input signals (V).  In order to derive the coefficients of the Wiener filter, consider the signal w [ n ] being fed to a Wiener filter of order N and with coefficients    {   a  i   }      subscript  a  i     \{a_{i}\}   ,     i   =   0  ,  …  ,  N       i   0  normal-…  N     i\,=\,0,\,\ldots,\,N   . The output of the filter is denoted x [ n ] which is given by the expression        x   [  n  ]    =    ∑   i  =  0   N     a  i   w   [   n  -  i   ]      .        x   delimited-[]  n      superscript   subscript     i  0    N      subscript  a  i   w   delimited-[]    n  i        x[n]=\sum_{i=0}^{N}a_{i}w[n-i].     The residual error is denoted e [ n ] and is defined as e [ n ] = x [ n ] − s [ n ] (see the corresponding block diagram). The Wiener filter is designed so as to minimize the mean square error ( MMSE criteria) which can be stated concisely as follows:        a  i   =    arg    min   E     {    e  2    [  n  ]    }     ,       subscript  a  i         E        superscript  e  2    delimited-[]  n        a_{i}=\arg\min~{}E\{e^{2}[n]\},     where    E   {  ⋅  }       E   normal-⋅     E\{\cdot\}   denotes the expectation operator. In the general case, the coefficients    a  i     subscript  a  i    a_{i}   may be complex and may be derived for the case where w [ n ] and s [ n ] are complex as well. With a complex signal, the matrix to be solved is a Hermitian  Toeplitz matrix , rather than symmetric  Toeplitz matrix . For simplicity, the following considers only the case where all these quantities are real. The mean square error (MSE) may be rewritten as:         E   {    e  2    [  n  ]    }      =     E   {    (    x   [  n  ]    -   s   [  n  ]     )   2   }         =       E   {    x  2    [  n  ]    }    +   E   {    s  2    [  n  ]    }     -   2  E   {   x   [  n  ]   s   [  n  ]    }          =        E   {    (    ∑   i  =  0   N     a  i   w   [   n  -  i   ]     )   2   }    +   E   {    s  2    [  n  ]    }     -   2  E   {    ∑   i  =  0   N     a  i   w   [   n  -  i   ]   s   [  n  ]     }     .           E      superscript  e  2    delimited-[]  n         E    superscript      x   delimited-[]  n      s   delimited-[]  n     2        missing-subexpression          E      superscript  x  2    delimited-[]  n        E      superscript  s  2    delimited-[]  n         2  E     x   delimited-[]  n   s   delimited-[]  n          missing-subexpression          E    superscript    superscript   subscript     i  0    N      subscript  a  i   w   delimited-[]    n  i      2       E      superscript  s  2    delimited-[]  n         2  E     superscript   subscript     i  0    N      subscript  a  i   w   delimited-[]    n  i    s   delimited-[]  n           \begin{array}[]{rcl}E\{e^{2}[n]\}&=&E\{(x[n]-s[n])^{2}\}\\
 &=&E\{x^{2}[n]\}+E\{s^{2}[n]\}-2E\{x[n]s[n]\}\\
 &=&E\{\big(\sum_{i=0}^{N}a_{i}w[n-i]\big)^{2}\}+E\{s^{2}[n]\}-2E\{\sum_{i=0}^{%
 N}a_{i}w[n-i]s[n]\}.\end{array}     To find the vector    [   a  0   ,  …  ,   a  N   ]      subscript  a  0   normal-…   subscript  a  N     [a_{0},\,\ldots,\,a_{N}]   which minimizes the expression above, calculate its derivative with respect to    a  i     subscript  a  i    a_{i}             ∂   ∂   a  i     E   {    e  2    [  n  ]    }      =         2  E   {    (    ∑   j  =  0   N     a  j   w   [   n  -  j   ]     )   w   [   n  -  i   ]    }    -   2  E   {   s   [  n  ]   w   [   n  -  i   ]    }     i   =  0   ,   …  ,  N         =       2    ∑   j  =  0   N    E   {   w   [   n  -  j   ]   w   [   n  -  i   ]    }    a  j      -   2  E   {   w   [   n  -  i   ]   s   [  n  ]    }     .                 subscript  a  i     E      superscript  e  2    delimited-[]  n        formulae-sequence         2  E       superscript   subscript     j  0    N      subscript  a  j   w   delimited-[]    n  j      w   delimited-[]    n  i         2  E     s   delimited-[]  n   w   delimited-[]    n  i        i   0    normal-…  N       missing-subexpression        2    superscript   subscript     j  0    N     E     w   delimited-[]    n  j    w   delimited-[]    n  i       subscript  a  j        2  E     w   delimited-[]    n  i    s   delimited-[]  n          \begin{array}[]{rcl}\frac{\partial}{\partial a_{i}}E\{e^{2}[n]\}&=&2E\{\big(%
 \sum_{j=0}^{N}a_{j}w[n-j]\big)w[n-i]\}-2E\{s[n]w[n-i]\}\quad i=0,\,\ldots,\,N%
 \\
 &=&2\sum_{j=0}^{N}E\{w[n-j]w[n-i]\}a_{j}-2E\{w[n-i]s[n]\}.\end{array}     Assuming that w [ n ] and s [ n ] are each stationary and jointly stationary, the sequences     R  w    [  m  ]        subscript  R  w    delimited-[]  m     R_{w}[m]   and     R   w  s     [  m  ]        subscript  R    w  s     delimited-[]  m     R_{ws}[m]   known respectively as the autocorrelation of w [ n ] and the cross-correlation between w [ n ] and s [ n ] can be defined as follows:        R  w    [  m  ]    =          subscript  R  w    delimited-[]  m    absent    \displaystyle R_{w}[m]=     The derivative of the MSE may therefore be rewritten as (notice that      R   w  s     [   -  i   ]    =    R   s  w     [  i  ]           subscript  R    w  s     delimited-[]    i        subscript  R    s  w     delimited-[]  i      R_{ws}[-i]\,=\,R_{sw}[i]   )          ∂   ∂   a  i     E   {    e  2    [  n  ]    }    =    2    ∑   j  =  0   N     R  w    [   j  -  i   ]    a  j      -   2   R   s  w     [  i  ]       i  =   0  ,  …  ,  N     .     formulae-sequence            subscript  a  i     E      superscript  e  2    delimited-[]  n          2    superscript   subscript     j  0    N      subscript  R  w    delimited-[]    j  i     subscript  a  j        2   subscript  R    s  w     delimited-[]  i        i   0  normal-…  N      \frac{\partial}{\partial a_{i}}E\{e^{2}[n]\}=2\sum_{j=0}^{N}R_{w}[j-i]a_{j}-2R%
 _{sw}[i]\quad i=0,\,\ldots,\,N.     Letting the derivative be equal to zero results in          ∑   j  =  0   N     R  w    [   j  -  i   ]    a  j     =    R   s  w     [  i  ]      i  =   0  ,  …  ,  N     ,     formulae-sequence      superscript   subscript     j  0    N      subscript  R  w    delimited-[]    j  i     subscript  a  j        subscript  R    s  w     delimited-[]  i       i   0  normal-…  N      \sum_{j=0}^{N}R_{w}[j-i]a_{j}=R_{sw}[i]\quad i=0,\,\ldots,\,N,     which can be rewritten in matrix form      𝐓𝐚  =  𝐯      𝐓𝐚  𝐯    \displaystyle\mathbf{T}\mathbf{a}=\mathbf{v}     These equations are known as the Wiener–Hopf equations . The matrix T appearing in the equation is a symmetric Toeplitz matrix . Under suitable conditions on   R   R   R   , these matrices are known to be positive definite and therefore non-singular yielding a unique solution to the determination of the Wiener filter coefficient vector,     𝐚   =    𝐓   -  1    𝐯       𝐚     superscript  𝐓    1    𝐯     \mathbf{a}\,=\,\mathbf{T}^{-1}\mathbf{v}   . Furthermore, there exists an efficient algorithm to solve such Wiener–Hopf equations known as the Levinson-Durbin algorithm so an explicit inversion of   𝐓   𝐓   \mathbf{T}   is not required.  Relationship to the least squares filter  The realization of the causal Wiener filter looks a lot like the solution to the least squares estimate, except in the signal processing domain. The least squares solution, for input matrix   𝐗   𝐗   \mathbf{X}   and output vector   𝐲   𝐲   \mathbf{y}   is        𝜷  ^   =     (    𝐗  𝐓   𝐗   )    -  1     𝐗  𝐓   𝒚    .       bold-^  𝜷      superscript     superscript  𝐗  𝐓   𝐗     1     superscript  𝐗  𝐓   𝒚     \boldsymbol{\hat{\beta}}=(\mathbf{X}^{\mathbf{T}}\mathbf{X})^{-1}\mathbf{X}^{%
 \mathbf{T}}\boldsymbol{y}.     The FIR Wiener filter is related to the least mean squares filter , but minimizing the error criterion of the latter does not rely on cross-correlations or auto-correlations. Its solution converges to the Wiener filter solution.  Applications  The Wiener filter has a variety of applications in signal processing, image processing, control systems, and digital communications. These applications generally fall into one of four main categories:   System identification  Deconvolution  Noise reduction  Signal detection   For example, the Wiener filter can be used in image processing to remove noise from a picture. For example, using the Mathematica function: WienerFilter[image,2] on the first image on the right, produces the filtered image below it.  It is commonly used to denoise audio signals, especially speech, as a preprocessor before speech recognition .  History  The filter was proposed by Norbert Wiener during the 1940s and published in 1949. 3 The discrete-time equivalent of Wiener's work was derived independently by Andrey Kolmogorov and published in 1941. Hence the theory is often called the Wiener–Kolmogorov filtering theory ( cf.  Kriging ). The Wiener filter was the first statistically designed filter to be proposed and subsequently gave rise to many others including the famous Kalman filter .  See also   Norbert Wiener  Kalman filter  Wiener deconvolution  Eberhard Hopf  Least mean squares filter  Similarities between Wiener and LMS  Linear prediction  MMSE estimator  Generalized Wiener filter   References   Thomas Kailath , Ali H. Sayed , and Babak Hassibi , Linear Estimation, Prentice-Hall, NJ, 2000, ISBN 978-0-13-022464-4.  Wiener N: The interpolation, extrapolation and smoothing of stationary time series', Report of the Services 19, Research Project DIC-6037 MIT, February 1942  Kolmogorov A.N: 'Stationary sequences in Hilbert space', (In Russian) Bull. Moscow Univ. 1941 vol.2 no.6 1-40. English translation in Kailath T. (ed.) Linear least squares estimation Dowden, Hutchinson & Ross 1977   External links   Mathematica WienerFilter function   "  Category:Linear filters  Category:Estimation theory  Category:Stochastic processes  Category:Time series analysis  Category:Image noise reduction techniques     ↩  ↩  ↩     