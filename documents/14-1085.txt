   Åukaszykâ€“Karmowski metric      Åukaszykâ€“Karmowski metric   In mathematics , the Åukaszykâ€“Karmowski metric is a function defining a distance between two random variables or two random vectors . 1 2 This function is not a metric as it does not satisfy the identity of indiscernibles condition of the metric, that is for two identical arguments its value is greater than zero. The concept is named after Szymon Åukaszyk and Wojciech Karmowski.  Continuous random variables  The Åukaszykâ€“Karmowski metric D between two continuous independent random variables  X and Y is defined as:       D   (  X  ,  Y  )    =    âˆ«   -  âˆ   âˆ     âˆ«   -  âˆ   âˆ     |   x  -  y   |   f   (  x  )   g   (  y  )   d   x   d  y           D   X  Y      superscript   subscript            superscript   subscript                x  y    f  x  g  y  d  x  d  y       D(X,Y)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|x-y|f(x)g(y)\,dx\,dy     where f ( x ) and g ( y ) are the probability density functions of X and Y respectively.  One may easily show that such metrics above do not satisfy the identity of indiscernibles condition required to be satisfied by the metric of the metric space . In fact they satisfy this condition if and only if both arguments X , Y are certain events described by Dirac delta density probability distribution functions . In such a case:        D   Î´  Î´     (  X  ,  Y  )    =    âˆ«   -  âˆ   âˆ     âˆ«   -  âˆ   âˆ     |   x  -  y   |   Î´   (   x  -   Î¼  x    )   Î´   (   y  -   Î¼  y    )   d   x   d  y     =   |    Î¼  x   -   Î¼  y    |            subscript  D    Î´  Î´     X  Y      superscript   subscript            superscript   subscript                x  y    Î´    x   subscript  Î¼  x    Î´    y   subscript  Î¼  y    d  x  d  y               subscript  Î¼  x    subscript  Î¼  y        D_{\delta\delta}(X,Y)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|x-y|%
 \delta(x-\mu_{x})\delta(y-\mu_{y})\,dx\,dy=|\mu_{x}-\mu_{y}|     the Åukaszykâ€“Karmowski metric simply transforms into the metric between expected values     Î¼  x     subscript  Î¼  x    \mu_{x}   ,    Î¼  y     subscript  Î¼  y    \mu_{y}   of the variables X and Y and obviously:        D   Î´  Î´     (  X  ,  X  )    =   |    Î¼  x   -   Î¼  x    |   =  0.           subscript  D    Î´  Î´     X  X         subscript  Î¼  x    subscript  Î¼  x          0.     D_{\delta\delta}(X,X)=|\mu_{x}-\mu_{x}|=0.     For all the other cases however:       D   (  X  ,  X  )    >   0.         D   X  X    0.    D\left(X,X\right)>0.\,     The Åukaszykâ€“Karmowski metric satisfies the remaining non-negativity and symmetry conditions of metric directly from its definition (symmetry of modulus), as well as subadditivity / triangle inequality condition:       D   (  X  ,  Z  )    =     âˆ«   -  âˆ   âˆ       âˆ«   -  âˆ   âˆ      |   x  -  z   |   f   (  x  )   h   (  z  )   d   x   d   z      =     âˆ«   -  âˆ   âˆ       âˆ«   -  âˆ   âˆ      |   x  -  z   |   f   (  x  )   h   (  z  )   d   x   d  z     âˆ«   -  âˆ   âˆ     g   (  y  )   d  y               D   X  Z      superscript   subscript            superscript   subscript                x  z    f  x  h  z  d  x  d  z            superscript   subscript            superscript   subscript                x  z    f  x  h  z  d  x  d  z    superscript   subscript            g  y  d  y          \displaystyle{}D(X,Z)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|x-z|f(x)h%
 (z)\,dx\,dz\ =\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|x-z|f(x)h(z)\,dx%
 \,dz\int_{-\infty}^{\infty}g(y)dy   Thus        D   (  X  ,  Z  )    â‰¤    D   (  X  ,  Y  )    +   D   (  Y  ,  Z  )      .        D   X  Z        D   X  Y      D   Y  Z       D(X,Z)\leq D(X,Y)+D(Y,Z).\,     (Figure)  Lâ€“K metric between two random variables X and Y having normal distributions and the same standard deviation      Ïƒ  =  0   ,    Ïƒ  =  0.2   ,    Ïƒ  =  0.4   ,    Ïƒ  =  0.6   ,    Ïƒ  =  0.8   ,   Ïƒ  =  1          formulae-sequence    Ïƒ  0    formulae-sequence    Ïƒ  0.2    formulae-sequence    Ïƒ  0.4    formulae-sequence    Ïƒ  0.6    formulae-sequence    Ïƒ  0.8     Ïƒ  1         \sigma=0,\sigma=0.2,\sigma=0.4,\sigma=0.6,\sigma=0.8,\sigma=1   (starting with the bottom curve).     m   x  y    =   |    Î¼  x   -   Î¼  y    |        subscript  m    x  y         subscript  Î¼  x    subscript  Î¼  y       m_{xy}=|\mu_{x}-\mu_{y}|   denotes a distance between means of X and Y .   In the case where X and Y are dependent on each other, having a joint probability density function  f ( x , y ), the Lâ€“K metric has the following form:        âˆ«   -  âˆ   âˆ     âˆ«   -  âˆ   âˆ     |   x  -  y   |   f   (  x  ,  y  )   d   x   d  y     .      superscript   subscript            superscript   subscript                x  y    f   x  y   d  x  d  y      \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|x-y|f(x,y)\,dx\,dy.     Example: two continuous random variables with normal distributions (NN)  If both random variables X and Y have normal distributions with the same standard deviation Ïƒ, and if moreover X and Y are independent, then D ( X , Y ) is given by         D   N  N     (  X  ,  Y  )    =     Î¼   x  y    +     2  Ïƒ    Ï€     exp   (   -    Î¼   x  y   2    4   Ïƒ  2      )      -    Î¼   x  y     erfc   (    Î¼   x  y     2  Ïƒ    )       ,         subscript  D    N  N     X  Y         subscript  Î¼    x  y          2  Ïƒ     Ï€     exp       superscript   subscript  Î¼    x  y    2     4   superscript  Ïƒ  2            subscript  Î¼    x  y     erfc     subscript  Î¼    x  y      2  Ïƒ         D_{NN}(X,Y)=\mu_{xy}+\frac{2\sigma}{\sqrt{\pi}}\operatorname{exp}\left(-\frac{%
 \mu_{xy}^{2}}{4\sigma^{2}}\right)-\mu_{xy}\operatorname{erfc}\left(\frac{\mu_{%
 xy}}{2\sigma}\right),     where        Î¼   x  y    =   |    Î¼  x   -   Î¼  y    |    ,       subscript  Î¼    x  y         subscript  Î¼  x    subscript  Î¼  y       \mu_{xy}=\left|\mu_{x}-\mu_{y}\right|,     where erfc( x ) is the complementary error function and where the subscripts NN indicate the type of the Lâ€“K metric.  In this case, the lowest possible value of the function     D   N  N     (  X  ,  Y  )        subscript  D    N  N     X  Y     D_{NN}(X,Y)   is given by         lim    Î¼   x  y    â†’  0      D   N  N     (  X  ,  Y  )     =    D   N  N     (  X  ,  X  )    =    2  Ïƒ    Ï€     .          subscript    normal-â†’   subscript  Î¼    x  y    0       subscript  D    N  N     X  Y        subscript  D    N  N     X  X             2  Ïƒ     Ï€       \lim_{\mu_{xy}\to 0}D_{NN}(X,Y)=D_{NN}(X,X)=\frac{2\sigma}{\sqrt{\pi}}.     Example: two continuous random variables with uniform distributions (RR)  When both random variables X and Y have uniform distributions ( R ) of the same standard deviation Ïƒ, D ( X , Y ) is given by        D   R  R     (  X  ,  Y  )    =   {           24   3    Ïƒ  3    -   Î¼   x  y   3    +   6   3   Ïƒ   Î¼   x  y   2      36   Ïƒ  2      ,        Î¼   x  y    <   2   3   Ïƒ    ,         Î¼   x  y    ,        Î¼   x  y    â‰¥   2   3   Ïƒ    .              subscript  D    R  R     X  Y     cases          24    3    superscript  Ïƒ  3     superscript   subscript  Î¼    x  y    3      6    3   Ïƒ   superscript   subscript  Î¼    x  y    2       36   superscript  Ïƒ  2        subscript  Î¼    x  y      2    3   Ïƒ     subscript  Î¼    x  y       subscript  Î¼    x  y      2    3   Ïƒ       D_{RR}(X,Y)=\begin{cases}\frac{24\sqrt{3}\sigma^{3}-\mu_{xy}^{3}+6\sqrt{3}%
 \sigma\mu_{xy}^{2}}{36\sigma^{2}},&\mu_{xy}<2\sqrt{3}\sigma,\\
 \mu_{xy},&\mu_{xy}\geq 2\sqrt{3}\sigma.\end{cases}     The minimal value of this kind of Lâ€“K metric is         D   R  R     (  X  ,  X  )    =    2  Ïƒ    3     .         subscript  D    R  R     X  X        2  Ïƒ     3      D_{RR}(X,X)=\frac{2\sigma}{\sqrt{3}}.     Discrete random variables  In case the random variables X and Y are characterized by discrete probability distribution the Åukaszykâ€“Karmowski metric D is defined as:      D   (  X  ,  Y  )   =   âˆ‘  i    âˆ‘  j   |   x  i   -   y  j   |  P   (  X  =   x  i   )   P   (  Y  =   y  j   )   .     fragments  D   fragments  normal-(  X  normal-,  Y  normal-)     subscript   i    subscript   j   normal-|   subscript  x  i     subscript  y  j   normal-|  P   fragments  normal-(  X    subscript  x  i   normal-)   P   fragments  normal-(  Y    subscript  y  j   normal-)   normal-.    D(X,Y)=\sum_{i}\sum_{j}|x_{i}-y_{j}|P(X=x_{i})P(Y=y_{j}).\,     For example for two discrete Poisson-distributed random variables X and Y the equation above transforms into:         D   P  P     (  X  ,  Y  )    =    âˆ‘   x  =  0   n     âˆ‘   y  =  0   n     |   x  -  y   |      Î»  x    x    Î»  y    y    e   -   (    Î»  x   +   Î»  y    )        x  !    y  !         .         subscript  D    P  P     X  Y      superscript   subscript     x  0    n     superscript   subscript     y  0    n         x  y         superscript   subscript  Î»  x   x    superscript   subscript  Î»  y   y    superscript  e       subscript  Î»  x    subscript  Î»  y           x     y          D_{PP}(X,Y)=\sum_{x=0}^{n}\sum_{y=0}^{n}|x-y|\frac{{\lambda_{x}}^{x}{\lambda_{%
 y}}^{y}e^{-(\lambda_{x}+\lambda_{y})}}{x!y!}.     Random vectors    The Åukaszykâ€“Karmowski metric of random variables may be easily extended into metric D ( X , Y ) of random vectors  X , Y by substituting    |   x  -  y   |        x  y     |x-y|   with any metric operator d ( x , y ):        D   (  ğ—  ,  ğ˜  )    =    âˆ«  Î©     âˆ«  Î©    d   (  ğ±  ,  ğ²  )   F   (  ğ±  )   G   (  ğ²  )   d    Î©  x    d   Î©  y       .        D   ğ—  ğ˜      subscript   normal-Î©     subscript   normal-Î©     d   ğ±  ğ²   F  ğ±  G  ğ²  d   subscript  normal-Î©  x   d   subscript  normal-Î©  y        D(\mathbf{X},\mathbf{Y})=\int_{\Omega}\int_{\Omega}d(\mathbf{x},\mathbf{y})F(%
 \mathbf{x})G(\mathbf{y})\,d\Omega_{x}\,d\Omega_{y}.     For example substituting d ( x , y ) with an Euclidean metric and assuming two-dimensionality of random vectors X , Y would yield:        D   (  ğ—  ,  ğ˜  )    =    âˆ«  Î©     âˆ«  Î©       âˆ‘   i  =  1   2     |    x  i   -   y  i    |   2     F   (   x  1   ,   x  2   )   G   (   y  1   ,   y  2   )   d    x  1    d    x  2    d    y  1    d   y  2       .        D   ğ—  ğ˜      subscript   normal-Î©     subscript   normal-Î©         superscript   subscript     i  1    2    superscript       subscript  x  i    subscript  y  i     2     F    subscript  x  1    subscript  x  2    G    subscript  y  1    subscript  y  2    d   subscript  x  1   d   subscript  x  2   d   subscript  y  1   d   subscript  y  2        D(\mathbf{X},\mathbf{Y})=\int_{\Omega}\int_{\Omega}\sqrt{\sum_{i=1}^{2}|x_{i}-%
 y_{i}|^{2}}F(x_{1},x_{2})G(y_{1},y_{2})\,dx_{1}\,dx_{2}\,dy_{1}\,dy_{2}.     This form of Lâ€“K metric is also greater than zero for the same vectors being measured (with the exception of two vectors having Dirac delta coefficients) and satisfies non-negativity and symmetry conditions of metric. The proofs are analogous to the ones provided for the Lâ€“K metric of random variables discussed above.  In case random vectors X and Y are dependent on each other, sharing common joint probability distribution  F ( X , Y ) the Lâ€“K metric has the form:        D   (  ğ—  ,  ğ˜  )    =    âˆ«  Î©     âˆ«  Î©    d   (  ğ±  ,  ğ²  )   F   (  ğ±  ,  ğ²  )   d    Î©  x    d   Î©  y       .        D   ğ—  ğ˜      subscript   normal-Î©     subscript   normal-Î©     d   ğ±  ğ²   F   ğ±  ğ²   d   subscript  normal-Î©  x   d   subscript  normal-Î©  y        D(\mathbf{X},\mathbf{Y})=\int_{\Omega}\int_{\Omega}d(\mathbf{x},\mathbf{y})F(%
 \mathbf{x},\mathbf{y})\,d\Omega_{x}\,d\Omega_{y}.     Random vectors â€“ the Euclidean form  If the random vectors X and Y are not also only mutually independent but also all components of each vector are mutually independent , the Åukaszykâ€“Karmowski metric for random vectors is defined as:        D    *  *    (  p  )     (  ğ—  ,  ğ˜  )    =    (    âˆ‘  i     D    *  *      (   X  i   ,   Y  i   )   p     )    1  p           superscript   subscript  D    absent     p    ğ—  ğ˜     superscript    subscript   i      subscript  D    absent      superscript    subscript  X  i    subscript  Y  i    p       1  p      D_{**}^{(p)}(\mathbf{X},\mathbf{Y})=\left({\sum_{i}{D_{**}(X_{i},Y_{i})}^{p}}%
 \right)^{\frac{1}{p}}     where:       D    *  *     (   X  i   ,   Y  i   )        subscript  D    absent       subscript  X  i    subscript  Y  i      D_{**}(X_{i},Y_{i})\,     is a particular form of Lâ€“K metric of random variables chosen in dependence of the distributions of particular coefficients    X  i     subscript  X  i    X_{i}   and    Y  i     subscript  Y  i    Y_{i}   of vectors X , Y .  Such a form of Lâ€“K metric also shares the common properties of all Lâ€“K metrics.   It does not satisfy the identity of indiscernibles condition:         âˆ€  ğ—   ,    ğ˜    D    *  *    (  p  )     (  ğ—  ,  ğ˜  )     =   0   â‡  ğ—  =   ğ˜           for-all  ğ—     ğ˜   superscript   subscript  D    absent     p    ğ—  ğ˜     0    normal-â‡    ğ—       ğ˜     \forall{\mathbf{X},\mathbf{Y}}\ D_{**}^{(p)}(\mathbf{X},\mathbf{Y})=0\ %
 \nLeftrightarrow\ \mathbf{X}=\mathbf{Y}\,      since:         D    *  *    (  p  )     (  ğ—  ,  ğ—  )    =  0   â‡”    âˆ€    i    D    *  *     (   X  i   ,   X  i   )     =  0      normal-â‡”       superscript   subscript  D    absent     p    ğ—  ğ—    0      for-all    i   subscript  D    absent       subscript  X  i    subscript  X  i      0     D_{**}^{(p)}(\mathbf{X},\mathbf{X})=0\Leftrightarrow\ \forall{i}\ D_{**}(X_{i}%
 ,X_{i})=0     but from the properties of Lâ€“K metric for random variables it follows that:       âˆƒ     X  i     D    *  *     (   X  i   ,   X  i   )     >  0           subscript  X  i    subscript  D    absent       subscript  X  i    subscript  X  i      0    \exists\ X_{i}\ D_{**}(X_{i},X_{i})>0       It is non-negative and symmetric since the particular coefficients are also non-negative and symmetric:        âˆ€    i    D    *  *     (   X  i   ,   Y  i   )     >   0        for-all    i   subscript  D    absent       subscript  X  i    subscript  Y  i      0    \forall\ i\ D_{**}(X_{i},Y_{i})>0\,          âˆ€    i    D    *  *     (   X  i   ,   Y  i   )     =    D    *  *     (   Y  i   ,   X  i   )         for-all    i   subscript  D    absent       subscript  X  i    subscript  Y  i         subscript  D    absent       subscript  Y  i    subscript  X  i       \forall\ i\ D_{**}(X_{i},Y_{i})=D_{**}(Y_{i},X_{i})      It satisfies the triangle inequality:         âˆ€  ğ—   ,  ğ˜  ,    ğ™    D    *  *    (  p  )     (  ğ—  ,  ğ™  )     â‰¤     D    *  *    (  p  )     (  ğ—  ,  ğ˜  )    +    D    *  *    (  p  )     (  ğ˜  ,  ğ™  )           for-all  ğ—   ğ˜    ğ™   superscript   subscript  D    absent     p    ğ—  ğ™          superscript   subscript  D    absent     p    ğ—  ğ˜       superscript   subscript  D    absent     p    ğ˜  ğ™       \forall\ \mathbf{X},\mathbf{Y},\mathbf{Z}\ D_{**}^{(p)}(\mathbf{X},\mathbf{Z})%
 \leq D_{**}^{(p)}(\mathbf{X},\mathbf{Y})+D_{**}^{(p)}(\mathbf{Y},\mathbf{Z})      since (cf. Minkowski inequality ):          (     âˆ‘  i      D    *  *      (   X  i   ,   Y  i   )   p     )    1  p    +     (     âˆ‘  i      D    *  *      (   Y  i   ,   Z  i   )   p     )    1  p      â‰¥          superscript    subscript   i      subscript  D    absent      superscript    subscript  X  i    subscript  Y  i    p       1  p     superscript    subscript   i      subscript  D    absent      superscript    subscript  Y  i    subscript  Z  i    p       1  p     absent    \displaystyle{}\left({\sum_{i}{D_{**}(X_{i},Y_{i})}^{p}}\right)^{\frac{1}{p}}+%
 \left({\sum_{i}{D_{**}(Y_{i},Z_{i})}^{p}}\right)^{\frac{1}{p}}\ \geq     Physical interpretation  The Åukaszykâ€“Karmowski metric may be considered as a distance between quantum mechanics particles described by wavefunctions  Ïˆ , where the probability  dP that given particle is present in given volume of space dV amounts:        d  P   =     |   Ïˆ   (  x  ,  y  ,  z  )    |   2   d  V    .        d  P      superscript      Ïˆ   x  y  z     2   d  V     dP=|\psi(x,y,z)|^{2}dV.\,     A quantum particle in a box  (Figure)  Lâ€“KÂ metric between a quantum particle in a one-dimensional box of length L and a given point Î¾ of the box    (   0  â‰¤  Î¾  â‰¤  L   )        0  Î¾       L     (0\leq\xi\leq L)   .   For example the wavefunction of a quantum particle ( X ) in a box of length L has the form:         Ïˆ  m    (  x  )    =     2  L     sin   (    m  Ï€  x   L   )      ,         subscript  Ïˆ  m   x         2  L          m  Ï€  x   L       \psi_{m}(x)=\sqrt{\frac{2}{L}}\sin{\left(\frac{m\pi x}{L}\right)},\,     In this case the Lâ€“K metric between this particle and any point    Î¾  âˆˆ   (  0  ,  L  )       Î¾   0  L     \xi\in(0,L)\,   of the box amounts:       D   (  X  ,  Î¾  )    =     âˆ«  0  L      |   x  -  Î¾   |     |    Ïˆ  m    (  x  )    |   2   d  x    =           D   X  Î¾      superscript   subscript   0   L         x  Î¾     superscript       subscript  Ïˆ  m   x    2   d  x         absent     \displaystyle{}D(X,\xi)=\int\limits_{0}^{L}|x-\xi||\psi_{m}(x)|^{2}dx=     From the properties of the Lâ€“K metric it follows that the sum of distances between the edge of the box ( Î¾ = 0 or Î¾ = L ) and any given point and the Lâ€“K metric between this point and the particle X is greater than Lâ€“K metric between the edge of the box and the particle. E.g. for a quantum particle X at an energy level m = 2 and point Î¾ = 0.2:         d   (  0  ,   0.2  L   )    +   D   (   0.2  L   ,  X  )     â‰ˆ    0.2  L   +   0.3171  L    =   0.517  L   â‰    D   (  0  ,  X  )    =   0.5  L   =   d   (  0  ,   0.5  L   )     .            d   0    0.2  L       D     0.2  L   X         0.2  L     0.3171  L           0.517  L          D   0  X           0.5  L          d   0    0.5  L        d(0,0.2L)+D(0.2L,X)\approx 0.2L+0.3171L=0.517L\neq D(0,X)=0.5L=d(0,0.5L).\,     Obviously the Lâ€“K metric between the particle and the edge of the box (D(0, X) or D( L , X)) amounts 0.5 L and is independent on the particle's energy level.  Two quantum particles in a box  A distance between two particles bouncing in a one-dimensional box of length L having time-independent wavefunctions :         Ïˆ  m    (  x  )    =     2  L     sin   (    m  Ï€  x   L   )      ,         subscript  Ïˆ  m   x         2  L          m  Ï€  x   L       \psi_{m}(x)=\sqrt{\frac{2}{L}}\sin{\left(\frac{m\pi x}{L}\right)},\,            Ïˆ  n    (  y  )    =     2  L     sin   (    n  Ï€  y   L   )      ,         subscript  Ïˆ  n   y         2  L          n  Ï€  y   L       \psi_{n}(y)=\sqrt{\frac{2}{L}}\sin{\left(\frac{n\pi y}{L}\right)},\,     may be defined in terms of Åukaszykâ€“Karmowski metric of independent random variables as:       D   (  X  ,  Y  )    =     âˆ«  0  L       âˆ«  0  L      |   x  -  y   |     |    Ïˆ  m    (  x  )    |   2      |    Ïˆ  n    (  y  )    |   2    d   x   d  y           D   X  Y      superscript   subscript   0   L     superscript   subscript   0   L         x  y     superscript       subscript  Ïˆ  m   x    2    superscript       subscript  Ïˆ  n   y    2   d  x  d  y       \displaystyle{}D(X,Y)=\int\limits_{0}^{L}\int\limits_{0}^{L}|x-y||\psi_{m}(x)|%
 ^{2}|\psi_{n}(y)|^{2}\,dx\,dy     The distance between particles X and Y is minimal for m = 1 i n = 1, that is for the minimum energy levels of these particles and amounts:        min   (   D   (  X  ,  Y  )    )    =   L   (     4   Ï€  2    -  15    12   Ï€  2     )    â‰ˆ   0.2067  L    .            D   X  Y       L        4   superscript  Ï€  2    15     12   superscript  Ï€  2             0.2067  L      \min(D(X,Y))=L\left(\frac{4\pi^{2}-15}{12\pi^{2}}\right)\approx 0.2067L.\,     According to properties of this function, the minimum distance is nonzero. For greater energy levels m , n it approaches to L /3.  Popular explanation  (Figure)   Normal distributions of two random variables X and Y of the same variance for three locations of their means Âµ x , Âµ y   Suppose we have to measure the distance between point Âµ x and point Âµ y , which are collinear with some point 0 . Suppose further that we instructed this task to two independent and large groups of surveyors equipped with tape measures , wherein each surveyor of the first group will measure distance between 0 and Âµ x and each surveyor of the second group will measure distance between 0 and Âµ y .  Under the following assumptions we may consider the two sets of received observations x i , y j as random variables X and Y having normal distribution of the same variance Ïƒ 2 and distributed over "factual locations" of points Âµ x , Âµ y .  Calculating the arithmetic mean for all pairs | x i âˆ’ y j | we should then obtain the value of Lâ€“K metric D NN ( X , Y ). Its characteristic curvilinearity arises from the symmetry of modulus and overlapping of distributions f ( x ), g ( y ) when their means approach each other.  An interesting experiment the results of which coincide with the properties of Lâ€“K metric was performed in 1967 by Robert Moyer and Thomas Landauer who measured the precise time an adult took to decide which of two Arabic digits was the largest. When the two digits were numerically distanced such as 2 and 9. subjects responded quickly and accurately. But their response time slowed by more than 100 milliseconds when they were closer such as 5 and 6, and subjects then erred as often as once in every ten trials. The distance effect was present both among highly intelligent persons, as well as those who were trained to escape it. 3  Practical applications  A Åukaszykâ€“Karmowski metric may be used instead of a metric operator (commonly the Euclidean distance ) in various numerical methods, and in particular in approximation algorithms such us radial basis function networks , 4 5  inverse distance weighting or Kohonen  self-organizing maps .  This approach is physically based, allowing the real uncertainty in the location of the sample points to be considered. 6 7  See also   Probabilistic metric space  Statistical distance   References  "  Category:Statistical distance measures     [ http://nauka-polska.pl/dhtml/raporty/praceBadawcze?rtype=opis âŒ©=pl&objectId;=42057 Metryka Pomiarowa, przykÅ‚ady zastosowaÅ„ aproksymacyjnych w mechanice doÅ›wiadczalnej (Measurement metric, examples of approximation applications in experimental mechanics)], PhD thesis , Szymon Åukaszyk (author), Wojciech Karmowski (supervisor), Tadeusz KoÅ›ciuszko Cracow University of Technology, submitted December 31, 2001, completed March 31, 2004 â†©  A new concept of probability metric and its applications in approximation of scattered data sets , Åukaszyk Szymon, Computational Mechanics Volume 33, Number 4, 299â€“304, Springer-Verlag 2003 â†©  [ http://books.google.com/books?id=CbCDKLbm_-UC&hl; ;=en The Number Sense: How the Mind Creates Mathematics], Stanislas Dehaene, Oxford University Press US, 1999, ISBN 0-19-513240-8, pp. 73â€“75 â†©  Taher Zaki, Driss Mammass, Abdellatif Ennaji, Fathallah Nouboud (2010) Classification of Arabic Documents by a Model of Fuzzy Proximity with a Radial Basis Function , International Journal of Future Generation Communication and Networking , 3 (4), p. 34 â†©  Florian Hogewind, Peter Bissolli (2010) [ http://www.met.hu/download.php?id=2&vol; ;=115&no;=1-2&a;=3 Operational maps of monthly mean temperature for WMO-Region VI (Europe and Middle East) ], IDÅJÃRÃS, Quarterly Journal of the Hungarian Meteorological Service, Vol. 115, No. 1-2, Januaryâ€“June 2011, pp. 31-49, p. 41 â†©  Gang Meng, Jane Law, Mary E. Thompson (2010) "Small-scale health-related indicator acquisition using secondary data spatial interpolation" , International Journal of Health Geographics , 9:50 â†©  Gang Meng (2010) Social and Spatial Determinants of Adverse Birth Outcome Inequalities in Socially Advanced Societies , Thesis (Doctor of Philosophy in Planning), University of Waterloo, Canada, â†©     