   Deviance (statistics)      Deviance (statistics)   In statistics , deviance is a quality of fit statistic for a model that is often used for statistical hypothesis testing . It is a generalization of the idea of using the sum of squares of residuals in ordinary least squares to cases where model-fitting is achieved by maximum likelihood .  Definition  The deviance for a model M 0 , based on a dataset y , is defined as: 1 2      D   (  y  )   =  -  2   (  log   (  p   (  y  ∣    θ  ^   0   )   )   -  log   (  p   (  y  ∣    θ  ^   s   )   )   )   .     fragments  D   fragments  normal-(  y  normal-)     2   fragments  normal-(    fragments  normal-(  p   fragments  normal-(  y  normal-∣   subscript   normal-^  θ   0   normal-)   normal-)      fragments  normal-(  p   fragments  normal-(  y  normal-∣   subscript   normal-^  θ   s   normal-)   normal-)   normal-)   normal-.    D(y)=-2\Big(\log\big(p(y\mid\hat{\theta}_{0})\big)-\log\big(p(y\mid\hat{\theta%
 }_{s})\big)\Big).\,     Here     θ  ^   0     subscript   normal-^  θ   0    \hat{\theta}_{0}   denotes the fitted values of the parameters in the model M 0 , while     θ  ^   s     subscript   normal-^  θ   s    \hat{\theta}_{s}   denotes the fitted parameters for the "full model" (or "saturated model"): both sets of fitted values are implicitly functions of the observations y . Here the full model is a model with a parameter for every observation so that the data are fitted exactly. This expression is simply −2 times the log-likelihood ratio of the reduced model compared to the full model. The deviance is used to compare two models – in particular in the case of generalized linear models where it has a similar role to residual variance from ANOVA in linear models ( RSS ).  Suppose in the framework of the GLM, we have two nested models , M 1 and M 2 . In particular, suppose that M 1 contains the parameters in M 2 , and k additional parameters. Then, under the null hypothesis that M 2 is the true model, the difference between the deviances for the two models follows an approximate chi-squared distribution with k -degrees of freedom. 3  Some usage of the term "deviance" can be confusing. According to Collett: 4   "the quantity    -  2  log   (  p   (  y  ∣    θ  ^   0   )   )      fragments   2    fragments  normal-(  p   fragments  normal-(  y  normal-∣   subscript   normal-^  θ   0   normal-)   normal-)     -2\log\big(p(y\mid\hat{\theta}_{0})\big)   is sometimes referred to as a deviance . This is [...] inappropriate, since unlike the deviance used in the context of generalized linear modelling,    -  2  log   (  p   (  y  ∣    θ  ^   0   )   )      fragments   2    fragments  normal-(  p   fragments  normal-(  y  normal-∣   subscript   normal-^  θ   0   normal-)   normal-)     -2\log\big(p(y\mid\hat{\theta}_{0})\big)   does not measure deviation from a model that is a perfect fit to the data." However, since the principal use is in the form of the difference of the deviances of two models, this confusion in definition is unimportant.   See also   Pearson's chi-squared test , an alternative quality of fit statistic for generalized linear models for count data.  Hosmer–Lemeshow test , a quality of fit statistic that can be used for binary data.  Akaike information criterion  Deviance information criterion  Peirce's criterion  Discrepancy function   Notes  References        External links   Generalized Linear Models - Edward F. Connor  Lectures notes on Deviance   "  Category:Hypothesis testing  Category:Statistical deviation and dispersion     ↩  McCullagh and Nelder (1989) ↩   Collett (2003) ↩     