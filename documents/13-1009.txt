


Cover's theorem




Cover's theorem

Cover's Theorem is a statement in computational learning theory and is one of the primary theoretical motivations for the use of non-linear kernel methods in machine learning applications. The theorem states that given a set of training data that is not linearly separable, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some non-linear transformation.
The proof is easy. A deterministic mapping may be used. Indeed, suppose there are 
 
 
 
  samples. Lift them onto the vertices of the simplex in the 
 
 
 
  dimensional real space. Every partition of the samples into two sets is separable by a linear separator. QED.
References







Mehrotra, K., Mohan, C.K., Ranka, S. (1997) Elements of artificial neural networks, 2nd edition. MIT Press. (Section 3.5) ISBN 0-262-13328-8 [http://books.google.co.uk/books?id=6d68Y4Wq_R4C&pg;;=PA88&lpg;=PA88&dq;=Cover's+theorem&source;=bl&ots;=6pEdU0CYz4&sig;=V2FqwVwkYaDQgmUNk22XEjnQFpw&hl;=en&ei;=mYdFTNqTMpHQjAfJw5j1Bg&sa;=X&oi;=book_result&ct;=result&resnum;=7&ved;=0CDAQ6AEwBg#v=onepage&q;=Cover's%20theorem&f;=false Google books]

"
Category:Computational learning theory Category:Statistical classification Category:Artificial neural networks


