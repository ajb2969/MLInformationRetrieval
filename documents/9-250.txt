   Kernel regression      Kernel regression    Not to be confused with Kernel principal component analysis .    The kernel regression is a non-parametric technique in statistics to estimate the conditional expectation of a random variable . The objective is to find a non-linear relation between a pair of random variables X and Y .  In any nonparametric regression , the conditional expectation of a variable   Y   Y   Y   relative to a variable   X   X   X   may be written:       E   (  Y  |  X  )    =   m   (  X  )         normal-E  Y  X     m  X     \operatorname{E}(Y|X)=m(X)     where   m   m   m   is an unknown function.  Nadaraya-Watson kernel regression  and  proposed to estimate   m   m   m   as a locally weighted average, using a kernel as a weighting function. The Nadaraya-Watson estimator is:         m  ^   h    (  x  )    =      ∑   i  =  1   n      K  h    (   x  -   x  i    )    y  i        ∑   i  =  1   n      K  h    (   x  -   x  i    )             subscript   normal-^  m   h   x       superscript   subscript     i  1    n      subscript  K  h     x   subscript  x  i     subscript  y  i       superscript   subscript     i  1    n      subscript  K  h     x   subscript  x  i         \widehat{m}_{h}(x)=\frac{\sum_{i=1}^{n}K_{h}(x-x_{i})y_{i}}{\sum_{i=1}^{n}K_{h%
 }(x-x_{i})}     where   K   K   K   is a kernel with a bandwidth   h   h   h   . The fraction is a weighting term with sum 1.  Derivation      E   (  Y  |  X  =  x  )   =  ∫  y  f   (  y  |  x  )   d  y  =  ∫  y    f   (  x  ,  y  )     f   (  x  )     d  y     fragments  normal-E   fragments  normal-(  Y  normal-|  X   x  normal-)     y  f   fragments  normal-(  y  normal-|  x  normal-)   d  y    y      f   x  y      f  x    d  y    \operatorname{E}(Y|X=x)=\int yf(y|x)dy=\int y\frac{f(x,y)}{f(x)}dy     Using the kernel density estimation for the joint distribution f(x,y) and f(x) with a kernel K ,        f  ^    (  x  ,  y  )    =    n   -  1     h   -  2      ∑   i  =  1   n    K   (    x  -   x  i    h   )   K   (    y  -   y  i    h   )             normal-^  f    x  y       superscript  n    1     superscript  h    2      superscript   subscript     i  1    n     K      x   subscript  x  i    h   K      y   subscript  y  i    h        \hat{f}(x,y)=n^{-1}h^{-2}\sum_{i=1}^{n}K\left(\frac{x-x_{i}}{h}\right)K\left(%
 \frac{y-y_{i}}{h}\right)   ,       f  ^    (  x  )    =    n   -  1     h   -  1      ∑   i  =  1   n    K   (    x  -   x  i    h   )             normal-^  f   x      superscript  n    1     superscript  h    1      superscript   subscript     i  1    n     K      x   subscript  x  i    h        \hat{f}(x)=n^{-1}h^{-1}\sum_{i=1}^{n}K\left(\frac{x-x_{i}}{h}\right)     we obtain the Nadaraya-Watson estimator.  Priestley-Chao kernel estimator         m  ^    P  C     (  x  )    =    h   -  1      ∑   i  =  1   n     (    x  i   -   x   i  -  1     )   K   (    x  -   x  i    h   )    y  i             subscript   normal-^  m     P  C    x      superscript  h    1      superscript   subscript     i  1    n        subscript  x  i    subscript  x    i  1     K      x   subscript  x  i    h    subscript  y  i        \widehat{m}_{PC}(x)=h^{-1}\sum_{i=1}^{n}(x_{i}-x_{i-1})K\left(\frac{x-x_{i}}{h%
 }\right)y_{i}     Gasser-Müller kernel estimator         m  ^    G  M     (  x  )    =    h   -  1      ∑   i  =  1   n     [    ∫   s   i  -  1     s  i     K   (    x  -  u   h   )   d  u    ]    y  i             subscript   normal-^  m     G  M    x      superscript  h    1      superscript   subscript     i  1    n      delimited-[]    superscript   subscript    subscript  s    i  1      subscript  s  i      K      x  u   h   d  u      subscript  y  i        \widehat{m}_{GM}(x)=h^{-1}\sum_{i=1}^{n}\left[\int_{s_{i-1}}^{s_{i}}K\left(%
 \frac{x-u}{h}\right)du\right]y_{i}     where     s  i   =     x   i  -  1    +   x  i    2        subscript  s  i        subscript  x    i  1     subscript  x  i    2     s_{i}=\frac{x_{i-1}+x_{i}}{2}     Example  This example is based upon Canadian cross-section wage data consisting of a random sample taken from the 1971 Canadian Census Public Use Tapes for male individuals having common education (grade 13). There are 205 observations in total.  We consider estimating the unknown regression function using Nadaraya-Watson kernel regression via the R np package that uses automatic (data-driven) bandwidth selection; see the np vignette for an introduction to the np package.  The figure below shows the estimated regression function using a second order Gaussian kernel along with asymptotic variability bounds  (Figure)  cps71 lc mean.png    Estimated Regression Function.   Script for example  The following commands of the R programming language use the npreg() function to deliver optimal smoothing and to create the figure given above. These commands can be entered at the command prompt via cut and paste.  install.packages("np")  library(np) # non parametric library  data(cps71)  attach(cps71)   m kernreg2 y x, bwidth(.5) kercode(3) npoint(500) gen(kernelprediction gridofpoints)   R : npreg (package np )  GNU/octave mathematical program package:   External links   Scale-adaptive kernel regression (with Matlab software).  Tutorial of Kernel regression using spreadsheet (with Microsoft Excel).  An online kernel regression demonstration Requires .NET 3.0 or later.  The np package An R package that provides a variety of nonparametric and semiparametric kernel methods that seamlessly handle a mix of continuous, unordered, and ordered factor data types.   "  Category:Non-parametric statistics   