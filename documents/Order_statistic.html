<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="522">Order statistic</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Order statistic</h1>
<hr/>
<figure><b>(Figure)</b>
<figcaption><a href="Probability_density_function" title="wikilink">Probability density functions</a> of the order statistics for a sample of size <em>n</em>=5 from an <a href="exponential_distribution" title="wikilink">exponential distribution</a> with unit scale parameter.</figcaption>
</figure>
<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, the <em>k</em>th <strong>order statistic</strong> of a <a href="statistical_sample" title="wikilink">statistical sample</a> is equal to its <em>k</em>th-smallest value.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Together with rank statistics, order statistics are among the most fundamental tools in <a href="non-parametric_statistics" title="wikilink">non-parametric statistics</a> and <a href="non-parametric_inference" title="wikilink">inference</a>.</p>
<p>Important special cases of the order statistics are the <a class="uri" href="minimum" title="wikilink">minimum</a> and <a class="uri" href="maximum" title="wikilink">maximum</a> value of a sample, and (with some qualifications discussed below) the <a href="sample_median" title="wikilink">sample median</a> and other <a href="quantile" title="wikilink">sample quantiles</a>.</p>
<p>When using <a href="probability_theory" title="wikilink">probability theory</a> to analyze order statistics of <a href="random_sample" title="wikilink">random samples</a> from a <a href="continuous_probability_distribution" title="wikilink">continuous distribution</a>, the <a href="cumulative_distribution_function" title="wikilink">cumulative distribution function</a> is used to reduce the analysis to the case of order statistics of the <a href="uniform_distribution_(continuous)" title="wikilink">uniform distribution</a>.</p>
<h2 id="notation-and-examples">Notation and examples</h2>
<p>For example, suppose that four numbers are observed or recorded, resulting in a sample of size 4. if the sample values are</p>
<dl>
<dd>6, 9, 3, 8,
</dd>
</dl>
<p>they will usually be denoted</p>
<p><span class="LaTeX">$$x_1=6,\ \ x_2=9,\ \ x_3=3,\ \ x_4=8,\,$$</span></p>
<p>where the subscript <span class="LaTeX">$i$</span> in <span class="LaTeX">$x_i$</span> indicates simply the order in which the observations were recorded and is usually assumed not to be significant. A case when the order is significant is when the observations are part of a <a href="time_series" title="wikilink">time series</a>.</p>
<p>The order statistics would be denoted</p>
<p><span class="LaTeX">$$x_{(1)}=3,\ \ x_{(2)}=6,\ \ x_{(3)}=8,\ \ x_{(4)}=9,\,$$</span></p>
<p>where the subscript <span class="LaTeX">$(  )$</span> enclosed in parentheses indicates the <span class="LaTeX">$$</span>th order statistic of the sample.</p>
<p>The <strong>first order statistic</strong> (or <strong>smallest order statistic</strong>) is always the <a class="uri" href="minimum" title="wikilink">minimum</a> of the sample, that is,</p>
<p><span class="LaTeX">$$X_{(1)}=\min\{\,X_1,\ldots,X_n\,\}$$</span></p>
<p>where, following a common convention, we use upper-case letters to refer to random variables, and lower-case letters (as above) to refer to their actual observed values.</p>
<p>Similarly, for a sample of size <span class="LaTeX">$n$</span>, the <span class="LaTeX">$$</span>th order statistic (or <strong>largest order statistic</strong>) is the <a class="uri" href="maximum" title="wikilink">maximum</a>, that is,</p>
<p><span class="LaTeX">$$X_{(n)}=\max\{\,X_1,\ldots,X_n\,\}.$$</span></p>
<p>The <a href="sample_range" title="wikilink">sample range</a> is the difference between the maximum and minimum. It is clearly a function of the order statistics:</p>
<p><span class="LaTeX">$${\rm Range}\{\,X_1,\ldots,X_n\,\} = X_{(n)}-X_{(1)}.$$</span></p>
<p>A similar important statistic in <a href="exploratory_data_analysis" title="wikilink">exploratory data analysis</a> that is simply related to the order statistics is the sample <a href="interquartile_range" title="wikilink">interquartile range</a>.</p>
<p>The sample median may or may not be an order statistic, since there is a single middle value only when the number <span class="LaTeX">$n$</span> of observations is <a href="Even_and_odd_numbers" title="wikilink">odd</a>. More precisely, if <span class="LaTeX">$ n = 2 m +1$</span> for some <span class="LaTeX">$m$</span>, then the sample median is <span class="LaTeX">$X_{(m+1)}$</span> and so is an order statistic. On the other hand, when <span class="LaTeX">$n$</span> is <a href="even_and_odd_numbers" title="wikilink">even</a>, <span class="LaTeX">$ n = 2 m$</span> and there are two middle values, <span class="LaTeX">$X_{(m)}$</span> and <span class="LaTeX">$X_{(m+1)}$</span>, and the sample median is some function of the two (usually the average) and hence not an order statistic. Similar remarks apply to all sample quantiles.</p>
<h2 id="probabilistic-analysis">Probabilistic analysis</h2>
<p>Given any random variables <em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>..., <em>X</em><sub><em>n</em></sub>, the order statistics X<sub>(1)</sub>, X<sub>(2)</sub>, ..., X<sub>(<em>n</em>)</sub> are also random variables, defined by sorting the values (<a href="realization_(probability)" title="wikilink">realizations</a>) of <em>X</em><sub>1</sub>, ..., <em>X</em><sub><em>n</em></sub> in increasing order.</p>
<p>When the random variables <em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>..., <em>X</em><sub><em>n</em></sub> form a <a href="sample_(statistics)" title="wikilink">sample</a> they are <a href="independent_and_identically_distributed" title="wikilink">independent and identically distributed</a>. This is the case treated below. In general, the random variables <em>X</em><sub>1</sub>, ..., <em>X</em><sub><em>n</em></sub> can arise by sampling from more than one population. Then they are <a href="independent_(statistics)" title="wikilink">independent</a>, but not necessarily identically distributed, and their <a href="joint_probability_distribution" title="wikilink">joint probability distribution</a> is given by the <a href="Bapat–Beg_theorem" title="wikilink">Bapat–Beg theorem</a>.</p>
<p>From now on, we will assume that the random variables under consideration are <a href="continuous_probability_distribution" title="wikilink">continuous</a> and, where convenient, we will also assume that they have a <a href="probability_density_function" title="wikilink">probability density function</a> (that is, they are <a href="absolute_continuity" title="wikilink">absolutely continuous</a>). The peculiarities of the analysis of distributions assigning mass to points (in particular, <a href="discrete_distribution" title="wikilink">discrete distributions</a>) are discussed at the end.</p>
<h3 id="probability-distributions-of-order-statistics">Probability distributions of order statistics</h3>
<p>In this section we show that the order statistics of the <a href="uniform_distribution_(continuous)" title="wikilink">uniform distribution</a> on the <a href="unit_interval" title="wikilink">unit interval</a> have <a href="marginal_distribution" title="wikilink">marginal distributions</a> belonging to the <a href="Beta_distribution" title="wikilink">Beta distribution</a> family. We also give a simple method to derive the joint distribution of any number of order statistics, and finally translate these results to arbitrary continuous distributions using the <a href="cumulative_distribution_function" title="wikilink">cdf</a>.</p>
<p>We assume throughout this section that <span class="LaTeX">$X_{1}, X_{2}, \ldots, X_{n}$</span> is a <a href="random_sample" title="wikilink">random sample</a> drawn from a continuous distribution with cdf <span class="LaTeX">$F_X$</span>. Denoting <span class="LaTeX">$U_i=F_X(X_i)$</span> we obtain the corresponding random sample <span class="LaTeX">$U_1,\ldots,U_n$</span> from the standard <a href="uniform_distribution_(continuous)" title="wikilink">uniform distribution</a>. Note that the order statistics also satisfy <span class="LaTeX">$U_{(i)}=F_X(X_{(i)})$</span>.</p>
<h4 id="order-statistics-sampled-from-a-uniform-distribution">Order statistics sampled from a uniform distribution</h4>
<p>The probability of the order statistic <span class="LaTeX">$U_{(k)}$</span> falling in the interval <span class="LaTeX">$[u,\ u+du]$</span> is equal to<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<p><span class="LaTeX">$${n!\over (k-1)!(n-k)!}u^{k-1}(1-u)^{n-k}\,du+O(du^2),$$</span></p>
<p>that is, the <em>k</em>th order statistic of the uniform distribution is a <a href="beta_distribution" title="wikilink">Beta</a> random variable.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<p><span class="LaTeX">$$U_{(k)} \sim B(k,n+1-k).$$</span></p>
<p>The proof of these statements is as follows. For <span class="LaTeX">$U_{(k)}$</span> to be between <em>u</em> and <em>u</em> + <em>du</em>, it is necessary that exactly <em>k</em> − 1 elements of the sample are smaller than <em>u</em>, and that at least one is between <em>u</em> and <em>u</em> + d<em>u</em>. The probability that more than one is in this latter interval is already <span class="LaTeX">$O(du^2)$</span>, so we have to calculate the probability that exactly <em>k</em> − 1, 1 and <em>n</em> − <em>k</em> observations fall in the intervals <span class="LaTeX">$(0,u)$</span>, <span class="LaTeX">$(u,u+du)$</span> and <span class="LaTeX">$(u+du,1)$</span> respectively. This equals (refer to <a href="multinomial_distribution" title="wikilink">multinomial distribution</a> for details)</p>
<p><span class="LaTeX">$${n!\over (k-1)!(n-k)!}u^{k-1}\cdot du\cdot(1-u-du)^{n-k}$$</span></p>
<p>and the result follows.</p>
<p>The mean of this distribution is <em>k</em> / (<em>n</em> + 1).</p>
<h4 id="the-joint-distribution-of-the-order-statistics-of-the-uniform-distribution">The joint distribution of the order statistics of the uniform distribution</h4>
<p>Similarly, for <em>i</em> (<em>i</em>) (<em>j</em>) can be shown to be</p>
<p><span class="LaTeX">$$f_{U_{(i)},U_{(j)}}(u,v)\,du\,dv= n!{u^{i-1}\over (i-1)!}{(v-u)^{j-i-1}\over(j-i-1)!}{(1-v)^{n-j}\over (n-j)!}\,du\,dv$$</span></p>
<p>which is (up to terms of higher order than <span class="LaTeX">$O(du\,dv)$</span>) the probability that <em>i</em> − 1, 1, <em>j</em> − 1 − <em>i</em>, 1 and <em>n</em> − <em>j</em> sample elements fall in the intervals <span class="LaTeX">$(0,u)$</span>, <span class="LaTeX">$(u,u+du)$</span>, <span class="LaTeX">$(u+du,v)$</span>, <span class="LaTeX">$(v,v+dv)$</span>, <span class="LaTeX">$(v+dv,1)$</span> respectively.</p>
<p>One reasons in an entirely analogous way to derive the higher-order joint distributions. Perhaps surprisingly, the joint density of the <em>n</em> order statistics turns out to be <em>constant</em>:</p>
<p><span class="LaTeX">$$f_{U_{(1)},U_{(2)},\ldots,U_{(n)}}(u_{1},u_{2},\ldots,u_{n})\,du_1\cdots du_n = n! \, du_1\cdots du_n.$$</span></p>
<p>One way to understand this is that the unordered sample does have constant density equal to 1, and that there are <em>n</em>! different permutations of the sample corresponding to the same sequence of order statistics. This is related to the fact that 1/<em>n</em>! is the volume of the region <span class="LaTeX">$0<u_1<\cdots<u_n<1< math="">.

==== Order statistics sampled from an Erlang distribution ====

The [[Laplace transform]] of order statistics sampled from an [[Erlang distribution]] via a path counting method.<ref>{{cite doi|10.1016/j.spl.2009.09.006}}</ref>

==== The joint distribution of the order statistics of an absolutely continuous distribution ====

If ''F''<sub>''X''</sub> is [[absolute continuity|absolutely continuous]], it has a density such that <math>dF_X(x)=f_X(x)\,dx$</span>, and we can use the substitutions</p>
<p><span class="LaTeX">$$u=F_X(x)$$</span></p>
<p>and</p>
<p><span class="LaTeX">$$du=f_X(x)\,dx$$</span></p>
<p>to derive the following <a href="probability_density_function" title="wikilink">probability density functions</a> (pdfs) for the order statistics of a sample of size <em>n</em> drawn from the distribution of <em>X</em>:</p>
<p><span class="LaTeX">$$f_{X_{(k)}}(x) =\frac{n!}{(k-1)!(n-k)!}[F_X(x)]^{k-1}[1-F_X(x)]^{n-k} f_X(x)$$</span></p>
<p><span class="LaTeX">$$f_{X_{(j)},X_{(k)}}(x,y) = \frac{n!}{(j-1)!(k-j-1)!(n-k)!}[F_X(x)]^{j-1}[F_X(y)-F_X(x)]^{k-1-j}[1-F_X(y)]^{n-k}f_X(x)f_X(y)$$</span> where <span class="LaTeX">$x\le y$</span></p>
<p><span class="LaTeX">$$f_{X_{(1)},\ldots,X_{(n)}}(x_1,\ldots,x_n)=n!f_X(x_1)\cdots f_X(x_n)$$</span> where <span class="LaTeX">$x_1\le x_2\le \dots \le x_n.$</span></p>
<h2 id="application-confidence-intervals-for-quantiles">Application: confidence intervals for quantiles</h2>
<p>An interesting question is how well the order statistics perform as estimators of the <a href="quantile" title="wikilink">quantiles</a> of the underlying distribution.</p>
<h3 id="a-small-sample-size-example">A small-sample-size example</h3>
<p>The simplest case to consider is how well the sample median estimates the population median.</p>
<p>As an example, consider a random sample of size 6. In that case, the sample median is usually defined as the midpoint of the interval delimited by the 3rd and 4th order statistics. However, we know from the preceding discussion that the probability that this interval actually contains the population median is</p>
<p><span class="LaTeX">$${6\choose 3}2^{-6} = {5\over 16} \approx 31\%.$$</span></p>
<p>Although the sample median is probably among the best distribution-independent <a href="point_estimate" title="wikilink">point estimates</a> of the population median, what this example illustrates is that it is not a particularly good one in absolute terms. In this particular case, a better confidence interval for the median is the one delimited by the 2nd and 5th order statistics, which contains the population median with probability</p>
<p><span class="LaTeX">$$\left[{6\choose 2}+{6\choose 3}+{6\choose 4}\right]2^{-6} = {25\over 32} \approx 78\%.$$</span></p>
<p>With such a small sample size, if one wants at least 95% confidence, one is reduced to saying that the median is between the minimum and the maximum of the 6 observations with probability 31/32 or approximately 97%. Size 6 is, in fact, the smallest sample size such that the interval determined by the minimum and the maximum is at least a 95% confidence interval for the population median.</p>
<h3 id="large-sample-sizes">Large sample sizes</h3>
<p>For the uniform distribution, as <em>n</em> tends to infinity, the <em>p</em><sup>th</sup> sample quantile is asymptotically normally distributed, since it is approximated by</p>
<p><span class="LaTeX">$$U_{(\lceil np \rceil)} \sim AN\left(p,\frac{p(1-p)}{n}\right).$$</span></p>
<p>For a general distribution <em>F</em> with a continuous non-zero density at <em>F</em><sup> −1</sup>(<em>p</em>), a similar asymptotic normality applies:</p>
<p><span class="LaTeX">$$X_{(\lceil np \rceil)} \sim AN\left(F^{-1}(p),\frac{p(1-p)}{n[f(F^{-1}(p))]^2}\right)$$</span></p>
<p>where <em>f</em> is the <a href="density_function" title="wikilink">density function</a>, and <em>F</em><sup> −1</sup> is the <a href="quantile_function" title="wikilink">quantile function</a> associated with <em>F</em>. One of the first people to mention and prove this result was <a href="Frederick_Mosteller" title="wikilink">Frederick Mosteller</a> in his seminal paper in 1946.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> Further research lead in the 1960s to the <a href="Raghu_Raj_Bahadur" title="wikilink">Bahadur</a> representation which provides information about the errorbounds.</p>
<p>An interesting observation can be made in the case where the distribution is symmetric, and the population median equals the population mean. In this case, the <a href="sample_mean" title="wikilink">sample mean</a>, by the <a href="central_limit_theorem" title="wikilink">central limit theorem</a>, is also asymptotically normally distributed, but with variance σ<sup>2</sup><em>/n</em> instead. This asymptotic analysis suggests that the mean outperforms the median in cases of low <a class="uri" href="kurtosis" title="wikilink">kurtosis</a>, and vice versa. For example, the median achieves better confidence intervals for the <a href="Laplace_distribution" title="wikilink">Laplace distribution</a>, while the mean performs better for <em>X</em> that are normally distributed.</p>
<h4 id="proof">Proof</h4>
<p>It can be shown that</p>
<p><span class="LaTeX">$$B(k,n+1-k)\ \stackrel{\mathrm{d}}{=}\ \frac{X}{X + Y},$$</span></p>
<p>where</p>
<p><span class="LaTeX">$$X = \sum_{i=1}^{k} Z_i, \quad Y = \sum_{i=k+1}^{n+1} Z_i,$$</span></p>
<p>with <em>Z<sub>i</sub></em> being independent identically distributed <a href="exponential_distribution" title="wikilink">exponential</a> random variables with rate 1. Since <em>X/n</em> and <em>Y/n</em> are asymptotically normally distributed by the CLT, our results follow by application of the <a href="delta_method" title="wikilink">delta method</a>.</p>
<h2 id="dealing-with-discrete-variables">Dealing with discrete variables</h2>
<p>Suppose <span class="LaTeX">$X_1,X_2,...,X_n$</span> are i.i.d. random variables from a discrete distribution with cumulative distribution function <span class="LaTeX">$F(x)$</span> and probability mass function <span class="LaTeX">$f(x)$</span>. To find the probabilities of the <span class="LaTeX">$k^\text{th}$</span> order statistics, three values are first needed, namely</p>
<p><span class="LaTeX">$$p_1=P(X<x)=f(x)-f(x), \="" and="" p_2="P(X=x)=f(x),\text{" }p_3="P(X">x)=1-F(x).</x)=f(x)-f(x),>$$</span></p>
<p>The cumulative distribution function of the <span class="LaTeX">$k^\text{th}$</span> order statistic can be computed by noting that</p>
<p><span class="LaTeX">$$\begin{align}
P(X_{(k)}\leq x)& =P(\text{there are at most }n-k\text{ observations greater than }x) ,\\
& =\sum_{j=0}^{n-k}{n\choose j}p_3^j(p_1+p_2)^{n-j} .
\end{align}$$</span></p>
<p>Similarly, <span class="LaTeX">$P(X_{(k)}<x)< math=""> is given by

:<math>
\begin{align}
P(X_{(k)}< x)& =P(\text{there are at most }n-k\text{ observations greater than or equal to }x) ,\\
&=\sum_{j=0}^{n-k}{n\choose j}(p_2+p_3)^j(p_1)^{n-j} .
\end{align}$</span></p>
<p>Note that the probability mass function of <span class="LaTeX">$X_{k}$</span> is just the difference of these values, that is to say</p>
<p><span class="LaTeX">$$\begin{align}
P(X_{(k)}=x)&=P(X_{(k)}\leq x)-P(X_{(k)}< x) ,\\
&=\sum_{j=0}^{n-k}{n\choose j}\left(p_3^j(p_1+p_2)^{n-j}-(p_2+p_3)^j(p_1)^{n-j}\right) ,\\
&=\sum_{j=0}^{n-k}{n\choose j}\left((1-F(x))^j(F(x))^{n-j}-(1-F(x)+f(x))^j(F(x)-f(x))^{n-j}\right).
\end{align}$$</span></p>
<h2 id="computing-order-statistics">Computing order statistics</h2>
<p>The problem of computing the <em>k</em>th smallest (or largest) element of a list is called the selection problem and is solved by a <a href="selection_algorithm" title="wikilink">selection algorithm</a>. Although this problem is difficult for very large lists, sophisticated selection algorithms have been created that can solve this problem in time proportional to the number of elements in the list, even if the list is totally unordered. If the data is stored in certain specialized data structures, this time can be brought down to O(log <em>n</em>). In many applications all order statistics are required, in which case a <a href="sorting_algorithm" title="wikilink">sorting algorithm</a> can be used and the time taken is O(<em>n</em> log <em>n</em>). More sophisticated methods can reduce the time to O(<em>n</em>).</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a class="uri" href="Rankit" title="wikilink">Rankit</a></li>
<li><a href="Box_plot" title="wikilink">Box plot</a></li>
<li><a href="Concomitant_(statistics)" title="wikilink">Concomitant (statistics)</a></li>
<li><a href="Fisher–Tippett_distribution" title="wikilink">Fisher–Tippett distribution</a></li>
<li><a href="Bapat–Beg_theorem" title="wikilink">Bapat–Beg theorem</a> for the order statistics of independent but not necessarily identically distributed random variables</li>
<li><a href="Bernstein_polynomial" title="wikilink">Bernstein polynomial</a></li>
<li><a class="uri" href="L-estimator" title="wikilink">L-estimator</a> – linear combinations of order statistics</li>
<li><a href="Rank-size_distribution" title="wikilink">Rank-size distribution</a></li>
<li><a href="Selection_algorithm" title="wikilink">Selection algorithm</a></li>
</ul>
<h3 id="examples-of-order-statistics">Examples of order statistics</h3>
<ul>
<li><a href="Sample_maximum_and_minimum" title="wikilink">Sample maximum and minimum</a></li>
<li><a class="uri" href="Quantile" title="wikilink">Quantile</a></li>
<li><a class="uri" href="Percentile" title="wikilink">Percentile</a></li>
<li><a href="Descriptive_statistics" title="wikilink">Decile</a></li>
<li><a class="uri" href="Quartile" title="wikilink">Quartile</a></li>
<li><a class="uri" href="Median" title="wikilink">Median</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li>
<p>Retrieved Feb 02,2005</p></li>
<li>
<p>Retrieved Feb 02,2005</p></li>
<li>Dr. Susan Holmes <a href="http://www-stat.stanford.edu/~susan/courses/s116/node79.html">Order Statistics</a> Retrieved Feb 02,2005</li>
<li>C++ source <a href="https://github.com/xtaci/algorithms/blob/master/include/dos_tree.h">Dynamic Order Statistics</a></li>
</ul>
<p>"</p>
<p><a href="Category:Non-parametric_statistics" title="wikilink">Category:Non-parametric statistics</a> <a href="Category:Summary_statistics" title="wikilink">Category:Summary statistics</a> <a class="uri" href="Category:Permutations" title="wikilink">Category:Permutations</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2">.<a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
</ol>
</section>
</body>
</html>
