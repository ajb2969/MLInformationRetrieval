   Partial correlation      Partial correlation   In probability theory and statistics , partial correlation measures the degree of association between two random variables , with the effect of a set of controlling random variables removed.  Formal definition  Formally, the partial correlation between X and Y given a set of n controlling variables Z = { Z 1 , Z 2 , ..., Z n }, written ρ XY · Z , is the correlation between the residuals  R X and R Y resulting from the linear regression of X with Z and of Y with Z , respectively. The first-order partial correlation (i.e. when n=1) is the difference between a correlation and the product of the removable correlations divided by the product of the coefficients of alienation of the removable correlations. The coefficient of alienation , and its relation with joint variance through correlation are available in Guilford (1973, pp. 344–345). 1  Computation  Using linear regression  A simple way to compute the sample partial correlation for some data is to solve the two associated linear regression problems, get the residuals, and calculate the correlation between the residuals. Let X and Y be, as above, random variables taking real values, and let Z be the n -dimensional vector-valued random variable. If we write x i , y i and z i to denote the i th of N  i.i.d. samples of some joint probability distribution over real random variables X , Y and Z , solving the linear regression problem amounts to finding n -dimensional coefficient vectors    𝐰  X  *     superscript   subscript  𝐰  X      \mathbf{w}_{X}^{*}   and    𝐰  Y  *     superscript   subscript  𝐰  Y      \mathbf{w}_{Y}^{*}   such that       𝐰  X  *   =   arg    min  𝐰    {    ∑   i  =  1   N     (    x  i   -   ⟨  𝐰  ,   𝐳  i   ⟩    )   2    }          superscript   subscript  𝐰  X         subscript   𝐰     superscript   subscript     i  1    N    superscript     subscript  x  i    𝐰   subscript  𝐳  i     2        \mathbf{w}_{X}^{*}=\arg\min_{\mathbf{w}}\left\{\sum_{i=1}^{N}(x_{i}-\langle%
 \mathbf{w},\mathbf{z}_{i}\rangle)^{2}\right\}          𝐰  Y  *   =   arg    min  𝐰    {    ∑   i  =  1   N     (    y  i   -   ⟨  𝐰  ,   𝐳  i   ⟩    )   2    }          superscript   subscript  𝐰  Y         subscript   𝐰     superscript   subscript     i  1    N    superscript     subscript  y  i    𝐰   subscript  𝐳  i     2        \mathbf{w}_{Y}^{*}=\arg\min_{\mathbf{w}}\left\{\sum_{i=1}^{N}(y_{i}-\langle%
 \mathbf{w},\mathbf{z}_{i}\rangle)^{2}\right\}     with N being the number of samples and    ⟨  𝐯  ,  𝐰  ⟩     𝐯  𝐰    \langle\mathbf{v},\mathbf{w}\rangle   the scalar product between the vectors v and w . Note that in some formulations the regression includes a constant term, so the matrix   𝐳   𝐳   \mathbf{z}   would have an additional column of ones.  The residuals are then       r   X  ,  i    =    x  i   -   ⟨   𝐰  X  *   ,   𝐳  i   ⟩         subscript  r   X  i       subscript  x  i     superscript   subscript  𝐰  X      subscript  𝐳  i       r_{X,i}=x_{i}-\langle\mathbf{w}_{X}^{*},\mathbf{z}_{i}\rangle          r   Y  ,  i    =    y  i   -   ⟨   𝐰  Y  *   ,   𝐳  i   ⟩         subscript  r   Y  i       subscript  y  i     superscript   subscript  𝐰  Y      subscript  𝐳  i       r_{Y,i}=y_{i}-\langle\mathbf{w}_{Y}^{*},\mathbf{z}_{i}\rangle     and the sample partial correlation is then given by the usual formula for sample correlation , but between these new derived values.         ρ  ^     X  Y   ⋅  𝐙    =     N    ∑   i  =  1   N     r   X  ,  i     r   Y  ,  i       -    ∑   i  =  1   N     r   X  ,  i      ∑   i  =  1   N    r   Y  ,  i             N    ∑   i  =  1   N    r   X  ,  i   2     -    (    ∑   i  =  1   N    r   X  ,  i     )   2         N    ∑   i  =  1   N    r   Y  ,  i   2     -    (    ∑   i  =  1   N    r   Y  ,  i     )   2        .       subscript   normal-^  ρ    normal-⋅    X  Y   𝐙          N    superscript   subscript     i  1    N      subscript  r   X  i     subscript  r   Y  i         superscript   subscript     i  1    N      subscript  r   X  i      superscript   subscript     i  1    N    subscript  r   Y  i                N    superscript   subscript     i  1    N    superscript   subscript  r   X  i    2      superscript    superscript   subscript     i  1    N    subscript  r   X  i     2           N    superscript   subscript     i  1    N    superscript   subscript  r   Y  i    2      superscript    superscript   subscript     i  1    N    subscript  r   Y  i     2         \hat{\rho}_{XY\cdot\mathbf{Z}}=\frac{N\sum_{i=1}^{N}r_{X,i}r_{Y,i}-\sum_{i=1}^%
 {N}r_{X,i}\sum_{i=1}^{N}r_{Y,i}}{\sqrt{N\sum_{i=1}^{N}r_{X,i}^{2}-\left(\sum_{%
 i=1}^{N}r_{X,i}\right)^{2}}~{}\sqrt{N\sum_{i=1}^{N}r_{Y,i}^{2}-\left(\sum_{i=1%
 }^{N}r_{Y,i}\right)^{2}}}.     Using recursive formula  It can be computationally expensive to solve the linear regression problems. Actually, the n th-order partial correlation (i.e., with | Z | = n ) can be easily computed from three ( n - 1)th-order partial correlations. The zeroth-order partial correlation ρ XY ·Ø is defined to be the regular correlation coefficient  ρ XY .  It holds, for any     Z  0   ∈  𝐙       subscript  Z  0   𝐙    Z_{0}\in\mathbf{Z}   :        ρ    X  Y   ⋅  𝐙    =     ρ     X  Y   ⋅  𝐙   ∖   {   Z  0   }     -    ρ     X   Z  0    ⋅  𝐙   ∖   {   Z  0   }      ρ      Z  0   Y   ⋅  𝐙   ∖   {   Z  0   }          1  -   ρ     X   Z  0    ⋅  𝐙   ∖   {   Z  0   }    2       1  -   ρ      Z  0   Y   ⋅  𝐙   ∖   {   Z  0   }    2        .       subscript  ρ   normal-⋅    X  Y   𝐙         subscript  ρ     normal-⋅    X  Y   𝐙     subscript  Z  0         subscript  ρ     normal-⋅    X   subscript  Z  0    𝐙     subscript  Z  0       subscript  ρ     normal-⋅     subscript  Z  0   Y   𝐙     subscript  Z  0              1   superscript   subscript  ρ     normal-⋅    X   subscript  Z  0    𝐙     subscript  Z  0      2         1   superscript   subscript  ρ     normal-⋅     subscript  Z  0   Y   𝐙     subscript  Z  0      2         \rho_{XY\cdot\mathbf{Z}}=\frac{\rho_{XY\cdot\mathbf{Z}\setminus\{Z_{0}\}}-\rho%
 _{XZ_{0}\cdot\mathbf{Z}\setminus\{Z_{0}\}}\rho_{Z_{0}Y\cdot\mathbf{Z}\setminus%
 \{Z_{0}\}}}{\sqrt{1-\rho_{XZ_{0}\cdot\mathbf{Z}\setminus\{Z_{0}\}}^{2}}\sqrt{1%
 -\rho_{Z_{0}Y\cdot\mathbf{Z}\setminus\{Z_{0}\}}^{2}}}.     Naïvely implementing this computation as a recursive algorithm yields an exponential time complexity . However, this computation has the overlapping subproblems property, such that using dynamic programming or simply caching the results of the recursive calls yields a complexity of    𝒪   (   n  3   )       𝒪   superscript  n  3     \mathcal{O}(n^{3})   .  Note in the case where Z is a single variable, this reduces to:        ρ    X  Y   ⋅  Z    =     ρ   X  Y    -    ρ   X  Z     ρ   Z  Y         1  -   ρ   X  Z   2       1  -   ρ   Z  Y   2        .       subscript  ρ   normal-⋅    X  Y   Z         subscript  ρ    X  Y       subscript  ρ    X  Z     subscript  ρ    Z  Y            1   superscript   subscript  ρ    X  Z    2         1   superscript   subscript  ρ    Z  Y    2         \rho_{XY\cdot Z}=\frac{\rho_{XY}-\rho_{XZ}\rho_{ZY}}{\sqrt{1-\rho_{XZ}^{2}}%
 \sqrt{1-\rho_{ZY}^{2}}}.     Using matrix inversion  In    𝒪   (   n  3   )       𝒪   superscript  n  3     \mathcal{O}(n^{3})   time, another approach allows all partial correlations to be computed between any two variables X i and X j of a set V of cardinality n , given all others, i.e.,    𝐕  ∖   {   X  i   ,   X  j   }       𝐕    subscript  X  i    subscript  X  j      \mathbf{V}\setminus\{X_{i},X_{j}\}   , if the correlation matrix (or alternatively covariance matrix ) Ω = ( ω ij ), where ω ij = ρ X i X j , is positive definite and therefore invertible . If we define P = Ω −1 , we have:        ρ      X  i    X  j    ⋅  𝐕   ∖   {   X  i   ,   X  j   }     =   -    p   i  j       p   i  i     p   j  j         .       subscript  ρ     normal-⋅     subscript  X  i    subscript  X  j    𝐕     subscript  X  i    subscript  X  j           subscript  p    i  j         subscript  p    i  i     subscript  p    j  j          \rho_{X_{i}X_{j}\cdot\mathbf{V}\setminus\{X_{i},X_{j}\}}=-\frac{p_{ij}}{\sqrt{%
 p_{ii}p_{jj}}}.     Interpretation  (Figure)  Geometrical interpretation of partial correlation for the case of N =3 samples and thus a 2-dimensional hyperplane   Geometrical  Let three variables X , Y , Z (where Z is the "control" or "extra variable") be chosen from a joint probability distribution over n variables V . Further let v i , 1 ≤ i ≤ N , be N  n -dimensional i.i.d. samples taken from the joint probability distribution over V . We then consider the N -dimensional vectors x (formed by the successive values of X over the samples), y (formed by the values of Y ) and z (formed by the values of Z ).  It can be shown that the residuals R X coming from the linear regression of X on Z , if also considered as an N -dimensional vector r X , have a zero scalar product with the vector z generated by Z . This means that the residuals vector lies on an ( N –1)-dimensional hyperplane  S z that is perpendicular to z .  The same also applies to the residuals R Y generating a vector r Y . The desired partial correlation is then the cosine of the angle φ between the projections  r X and r Y of x and y , respectively, onto the hyperplane perpendicular to z . 2  As conditional independence test  With the assumption that all involved variables are multivariate Gaussian , the partial correlation ρ XY · Z is zero if and only if X is conditionally independent from Y given Z . 3 This property does not hold in the general case.  To test if a sample partial correlation     ρ  ^     X  Y   ⋅  𝐙      subscript   normal-^  ρ    normal-⋅    X  Y   𝐙     \hat{\rho}_{XY\cdot\mathbf{Z}}   vanishes, Fisher's z-transform of the partial correlation can be used:        z   (    ρ  ^     X  Y   ⋅  𝐙    )    =    1  2    ln   (    1  +    ρ  ^     X  Y   ⋅  𝐙      1  -    ρ  ^     X  Y   ⋅  𝐙      )      .        z   subscript   normal-^  ρ    normal-⋅    X  Y   𝐙         1  2         1   subscript   normal-^  ρ    normal-⋅    X  Y   𝐙       1   subscript   normal-^  ρ    normal-⋅    X  Y   𝐙          z(\hat{\rho}_{XY\cdot\mathbf{Z}})=\frac{1}{2}\ln\left(\frac{1+\hat{\rho}_{XY%
 \cdot\mathbf{Z}}}{1-\hat{\rho}_{XY\cdot\mathbf{Z}}}\right).     The null hypothesis is     H  0   :     ρ  ^     X  Y   ⋅  𝐙    =  0      normal-:   subscript  H  0      subscript   normal-^  ρ    normal-⋅    X  Y   𝐙    0     H_{0}:\hat{\rho}_{XY\cdot\mathbf{Z}}=0   , to be tested against the two-tail alternative     H  A   :     ρ  ^     X  Y   ⋅  𝐙    ≠  0      normal-:   subscript  H  A      subscript   normal-^  ρ    normal-⋅    X  Y   𝐙    0     H_{A}:\hat{\rho}_{XY\cdot\mathbf{Z}}\neq 0   . We reject H 0 with significance level  α if:          N  -   |  𝐙  |   -  3    ⋅   |   z   (    ρ  ^     X  Y   ⋅  𝐙    )    |    >    Φ   -  1     (   1  -   α  /  2    )     ,       normal-⋅      N    𝐙   3        z   subscript   normal-^  ρ    normal-⋅    X  Y   𝐙          superscript  normal-Φ    1      1    α  2       \sqrt{N-|\mathbf{Z}|-3}\cdot|z(\hat{\rho}_{XY\cdot\mathbf{Z}})|>\Phi^{-1}(1-%
 \alpha/2),     where Φ(·) is the cumulative distribution function of a Gaussian distribution with zero mean and unit standard deviation , and N is the sample size . Note that this z -transform is approximate and that the actual distribution of the sample (partial) correlation coefficient is not straightforward. However, an exact t-test based on a combination of the partial regression coefficient, the partial correlation coefficient and the partial variances is available. 4  The distribution of the sample partial correlation was described by Fisher. 5  Semipartial correlation (part correlation)  The semipartial (or part) correlation statistic is similar to the partial correlation statistic. Both compare variations of two variables after certain factors are controlled for, but to calculate the semipartial correlation one holds the third variable constant for either X or Y but not both, whereas for the partial correlation one holds the third variable constant for both. 6 The semipartial correlation compares the unique variation of one variable (having removed variation associated with the Z variable(s)), with the unfiltered variation of the other, while the partial correlation compares the unique variation of one variable to the unique variation of the other.  The semipartial (or part) correlation can be viewed as more practically relevant "because it is scaled to (i.e., relative to) the total variability in the dependent (response) variable." 7 Conversely, it is less theoretically useful because it is less precise about the role of the unique contribution of the independent variable.  The absolute value of the semipartial correlation of X with Y is always less than or equal to that of the partial correlation of X with Y . The reason is this: Suppose the correlation of X with Z has been removed from X , giving the residual vector r x . In computing the semipartial correlation, Y still contains both unique variance and variance due to its association with Z . But r x , being uncorrelated with Z , can only explain some of the unique part of the variance of Y and not the part related to Z . In contrast, with the partial correlation, only r y (the part of the variance of Y that is unrelated to Z ) is to be explained, so there is less variance of the type that r x cannot explain.  Use in time series analysis  In time series analysis , the partial autocorrelation function (sometimes "partial correlation function") of a time series is defined, for lag h , as        ϕ   (  h  )    =   ρ     X  0    X  h    ⋅   {   X  1   ,  …  ,   X   h  -  1    }      .        ϕ  h    subscript  ρ   normal-⋅     subscript  X  0    subscript  X  h      subscript  X  1   normal-…   subscript  X    h  1         \phi(h)=\rho_{X_{0}X_{h}\cdot\{X_{1},\dots,X_{h-1}\}}.     This function is used to determine the appropriate lag length for an autoregression .  See also   Linear regression  Conditional independence  Multiple correlation   References  External links    What is a partial correlation?  Mathematical formulae in the "Description" section of the IMSL Numerical Library PCORR routine  A three-variable example   "  Category:Covariance and correlation  Category:Time series analysis     ↩  ↩  ↩  Kendall MG, Stuart A. (1973) The Advanced Theory of Statistics , Volume 2 (3rd Edition), ISBN 0-85264-215-6, Section 27.22 ↩  ↩  ↩  StatSoft, Inc. (2010). "Semi-Partial (or Part) Correlation" , Electronic Statistics Textbook. Tulsa, OK: StatSoft, accessed January 15, 2011. ↩     