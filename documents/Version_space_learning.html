<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1576">Version space learning</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Version space learning</h1>
<hr/>

<p> <strong>Version space learning</strong> is a <a href="Symbolic_artificial_intelligence" title="wikilink">logical</a> approach to <a href="machine_learning" title="wikilink">machine learning</a>, specifically <a href="binary_classification" title="wikilink">binary classification</a>. Version space learning algorithms search a predefined space of <a href="hypothesis" title="wikilink">hypotheses</a>, viewed as a set of <a href="Sentence_(logic)" title="wikilink">logical sentences</a>. Formally, the hypothesis space is a <a href="Logical_disjunction" title="wikilink">disjunction</a><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>

<p>

<math display="block" id="Version_space_learning:0">
 <semantics>
  <mrow>
   <msub>
    <mi>H</mi>
    <mn>1</mn>
   </msub>
   <mo>∨</mo>
   <msub>
    <mi>H</mi>
    <mn>2</mn>
   </msub>
   <mo>∨</mo>
   <mi mathvariant="normal">…</mi>
   <mo>∨</mo>
   <msub>
    <mi>H</mi>
    <mi>n</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <or></or>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>H</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>H</ci>
     <cn type="integer">2</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>H</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H_{1}\lor H_{2}\lor...\lor H_{n}
  </annotation>
 </semantics>
</math>

</p>

<p>(i.e., either hypothesis 1 is true, or hypothesis 2, or any subset of the hypotheses 1 through 

<math display="inline" id="Version_space_learning:1">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

). A version space learning algorithm is presented with examples, which it will use to restrict its hypothesis space; for each example 

<math display="inline" id="Version_space_learning:2">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

, the hypotheses that are <a href="Consistency" title="wikilink">inconsistent</a> with 

<math display="inline" id="Version_space_learning:3">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>


 are removed from the space.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> This iterative refining of the hypothesis space is called the <strong>candidate elimination</strong> algorithm, the hypothesis space maintained inside the algorithm its <em>version space</em>.</p>
<h2 id="the-version-space-algorithm">The version space algorithm</h2>

<p>In settings where there is a generality-ordering on hypotheses, it is possible to represent the version space by two sets of hypotheses: (1) the <strong>most specific</strong> consistent hypotheses, and (2) the <strong>most general</strong> consistent hypotheses, where "consistent" indicates agreement with observed data.</p>

<p>The most specific hypotheses (i.e., the specific boundary <strong>SB</strong>) cover the observed positive training examples, and as little of the remaining feature space as possible. These hypotheses, if reduced any further, <em>exclude</em> a <em>positive</em> training example, and hence become inconsistent. These minimal hypotheses essentially constitute a (pessimistic) claim that the true concept is defined just by the <em>positive</em> data already observed: Thus, if a novel (never-before-seen) data point is observed, it should be assumed to be negative. (I.e., if data has not previously been ruled in, then it's ruled out.)</p>

<p>The most general hypotheses (i.e., the general boundary <strong>GB</strong>) cover the observed positive training examples, but also cover as much of the remaining feature space without including any negative training examples. These, if enlarged any further, <em>include</em> a <em>negative</em> training example, and hence become inconsistent. These maximal hypotheses essentially constitute a (optimistic) claim that the true concept is defined just by the <em>negative</em> data already observed: Thus, if a novel (never-before-seen) data point is observed, it should be assumed to be positive. (I.e., if data has not previously been ruled out, then it's ruled in.)</p>

<p>Thus, during learning, the version space (which itself is a set – possibly infinite – containing <em>all</em> consistent hypotheses) can be represented by just its lower and upper bounds (maximally general and maximally specific hypothesis sets), and learning operations can be performed just on these representative sets.</p>

<p>After learning, classification can be performed on unseen examples by testing the hypothesis learned by the algorithm. If tye example is consistent with multiple hypotheses, a majority vote rule can be applied.</p>
<h2 id="historical-background">Historical background</h2>

<p>The notion of version spaces was introduced by Mitchell in the early 1980s as a framework for understanding the basic problem of supervised learning within the context of <a href="state_space_search" title="wikilink">solution search</a>. Although the basic "<strong>candidate elimination</strong>" search method that accompanies the version space framework is not a popular learning algorithm, there are some practical implementations that have been developed (e.g., Sverdlik &amp; Reynolds 1992, Hong &amp; Tsang 1997, Dubois &amp; Quafafou 2002).</p>

<p>A major drawback of version space learning is its inability to deal with noise: any pair of inconsistent examples can cause the version space to <em>collapse</em>, i.e., become empty, so that classification becomes impossible.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Formal_concept_analysis" title="wikilink">Formal concept analysis</a></li>
<li><a href="Inductive_logic_programming" title="wikilink">Inductive logic programming</a></li>
<li><a href="Rough_set" title="wikilink">Rough set</a>. [The rough set framework focuses on the case where ambiguity is introduced by an impoverished <strong>feature set</strong>. That is, the target concept cannot be decisively described because the available feature set fails to disambiguate objects belonging to different categories. The version space framework focuses on the (classical induction) case where the ambiguity is introduced by an impoverished <strong>data set</strong>. That is, the target concept cannot be decisively described because the available data fails to uniquely pick out a hypothesis. Naturally, both types of ambiguity can occur in the same learning problem.]</li>
<li>

<p><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> provides an interesting discussion of the general problem of induction.</p></li>
<li>Mill (1843/2002) is the classic source on <a href="inductive_logic" title="wikilink">inductive logic</a>.</li>
</ul>
<h2 id="references">References</h2>
<references>

<p><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> <a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
</references>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3"></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
</ol>
</section>
</body>
</html>
