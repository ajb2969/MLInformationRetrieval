<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="1856">Minimum message length</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Minimum message length</h1>
<hr/>
<p><strong>Minimum message length</strong> (MML) is a formal <a href="information_theory" title="wikilink">information theory</a> restatement of <a href="Occam's_Razor" title="wikilink">Occam's Razor</a>: even when models are not equal in goodness of fit accuracy to the observed data, the one generating the shortest overall message is more likely to be correct (where the message consists of a statement of the model, followed by a statement of data encoded concisely using that model). MML was invented by <a href="Chris_Wallace_(computer_scientist)" title="wikilink">Chris Wallace</a>, first appearing in the seminal Wallace and Boulton (1968).</p>
<p>MML is intended not just as a theoretical construct, but as a technique that may be deployed in practice. It differs from the related concept of <a href="Kolmogorov_complexity" title="wikilink">Kolmogorov complexity</a> in that it does not require use of a <a href="Turing_completeness" title="wikilink">Turing-complete</a> language to model data. The relation between Strict MML (SMML) and <a href="Kolmogorov_complexity" title="wikilink">Kolmogorov complexity</a> is outlined in <a href="http://comjnl.oxfordjournals.org/cgi/reprint/42/4/270">Wallace and Dowe (1999a)</a>. Further, a variety of mathematical approximations to "Strict" MML can be used — see, e.g., <a href="http://www.csse.monash.edu.au/mml/toc.pdf">Chapters 4 and 5</a> of <a href="http://www.springeronline.com/sgw/cda/frontpage/0,11855,4-10129-22-35893962-0,00.html">Wallace (posthumous) 2005</a>.</p>
<h2 id="definition">Definition</h2>
<p><a href="Claude_E._Shannon" title="wikilink">Shannon</a>'s <em><a href="A_Mathematical_Theory_of_Communication" title="wikilink">A Mathematical Theory of Communication</a></em> (1949) states that in an optimal code, the message length (in binary) of an event <span class="LaTeX">$E$</span>, <span class="LaTeX">$\operatorname{length}(E)$</span>, where <span class="LaTeX">$E$</span> has probability <span class="LaTeX">$P(E)$</span>, is given by <span class="LaTeX">$\operatorname{length}(E) = -\log_2(P(E))$</span>.</p>
<p><a href="Bayes's_theorem" title="wikilink">Bayes's theorem</a> states that the probability of a (variable) hypothesis <span class="LaTeX">$H$</span> given fixed evidence <span class="LaTeX">$E$</span> is proportional to <span class="LaTeX">$P(E|H) P(H)$</span>, which, by the definition of conditional probability, is equal to <span class="LaTeX">$P(H \and E)$</span>. We want the model (hypothesis) with the highest such <em>posterior probability</em>. Suppose we encode a message which represents (describes) both model and data jointly. Since <span class="LaTeX">$\operatorname{length}(H \and E) = -\log_2(P(H \and E))$</span>, the most probable model will have the shortest such message. The message breaks into two parts<span class="LaTeX">$$-\log_2(P(H \and E)) = -\log_2(P(H)) + -\log_2(P(E|H))$$</span>. The first part encodes the model itself. The second part contains information (e.g., values of parameters, or initial conditions, etc.) that, when processed by the model, outputs the observed data.</p>
<p>MML naturally and precisely trades model complexity for goodness of fit. A more complicated model takes longer to state (longer first part) but probably fits the data better (shorter second part). So, an MML metric won't choose a complicated model unless that model pays for itself.</p>
<h2 id="continuous-valued-parameters">Continuous-valued parameters</h2>
<p>One reason why a model might be longer would be simply because its various parameters are stated to greater precision, thus requiring transmission of more digits. Much of the power of MML derives from its handling of how accurately to state parameters in a model, and a variety of approximations that make this feasible in practice. This allows it to usefully compare, say, a model with many parameters imprecisely stated against a model with fewer parameters more accurately stated.</p>
<h2 id="key-features-of-mml">Key features of MML</h2>
<ul>
<li>MML can be used to compare models of different structure. For example, its earliest application was in finding <a href="mixture_model" title="wikilink">mixture models</a> with the optimal number of classes. Adding extra classes to a mixture model will always allow the data to be fitted to greater accuracy, but according to MML this must be weighed against the extra bits required to encode the parameters defining those classes.</li>
<li>MML is a method of <a href="Bayesian_model_comparison" title="wikilink">Bayesian model comparison</a>. It gives every model a score.</li>
<li>MML is scale-invariant and statistically invariant. Unlike many Bayesian selection methods, MML doesn't care if you change from measuring length to volume or from Cartesian co-ordinates to polar co-ordinates.</li>
<li>MML is statistically consistent. For problems like the <a href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#DoweWallace1997">Neyman-Scott</a> (1948) problem or factor analysis where the amount of data per parameter is bounded above, MML can estimate all parameters with statistical consistency.</li>
<li>MML accounts for the precision of measurement. It uses the <a href="Fisher_information" title="wikilink">Fisher information</a> (in the Wallace-Freeman 1987 approximation, or other hyper-volumes in <a href="http://www.csse.monash.edu.au/~dld/CSWallacePublications/CSWallace2005book_toc.pdf">other approximations</a>) to optimally discretize continuous parameters. Therefore the posterior is always a probability, not a probability density.</li>
<li>MML has been in use since 1968. MML coding schemes have been developed for several distributions, and many kinds of machine learners including unsupervised classification, decision trees and graphs, DNA sequences, <a href="Bayesian_network" title="wikilink">Bayesian networks</a>, neural networks (one-layer only so far), image compression, image and function segmentation, etc.</li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Algorithmic_probability" title="wikilink">Algorithmic probability</a></li>
<li><a href="Algorithmic_information_theory" title="wikilink">Algorithmic information theory</a></li>
<li><a href="Grammar_induction" title="wikilink">Grammar induction</a></li>
<li><a href="Inductive_inference" title="wikilink">Inductive inference</a></li>
<li><a href="Inductive_probability" title="wikilink">Inductive probability</a></li>
<li><a href="Kolmogorov_complexity" title="wikilink">Kolmogorov complexity</a> — absolute complexity (within a constant, depending on the particular choice of Universal <a href="Turing_machine" title="wikilink">Turing Machine</a>); MML is typically a computable approximation (see</li>
<li><a href="Minimum_description_length" title="wikilink">Minimum description length</a> — a supposedly non-Bayesian alternative with a possibly different motivation, which was introduced 10 years later — for comparisons, see, e.g., (sec. 10.2 of <a href="http://www.springeronline.com/sgw/cda/frontpage/0,11855,4-10129-22-35893962-0,00.html">Wallace (posthumous) 2005</a>) and (sec. 11.4.3, pp <a href="http://www.csse.monash.edu.au/~dld/Publications/2005/ComleyDowe2005MMLGeneralizedBayesianNetsAsymmetricLanguages_p272.jpg">272</a>-<a href="http://www.csse.monash.edu.au/~dld/Publications/2005/ComleyDowe2005MMLGeneralizedBayesianNetsAsymmetricLanguages_p273.jpg">273</a> of <a href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#ComleyDowe2005">Comley and Dowe, 2005</a>) and <a href="http://comjnl.oxfordjournals.org/cgi/reprint/42/4">the special issue on Kolmogorov Complexity in the Computer Journal: Vol. 42, No. 4, 1999</a>.</li>
<li><a href="Occam's_razor" title="wikilink">Occam's razor</a></li>
</ul>
<p><a href="http://comjnl.oxfordjournals.org/cgi/reprint/42/4/270">Wallace and Dowe (1999a)</a> below for elaboration)</p>
<h2 id="external-links">External links</h2>
<ul>
<li>Links to all <a href="http://www.csse.monash.edu.au/~dld/CSWallacePublications/">Chris Wallace</a>'s known publications.</li>
<li><a href="Chris_Wallace_(computer_scientist)" title="wikilink">C.S. Wallace</a>, <a href="http://www.springeronline.com/sgw/cda/frontpage/0,11855,4-10129-22-35893962-0,00.html">Statistical and Inductive Inference by Minimum Message Length</a>, Springer-Verlag (Information Science and Statistics), ISBN 0-387-23795-X, May 2005 - <a href="http://www.springer.com/west/home/statistics/theory?SGWID=4-10129-22-35893962-detailsPage=ppmmedia|toc">chapter headings</a>, <a href="http://www.csse.monash.edu.au/mml/toc.pdf">table of contents</a> and [<a class="uri" href="http://books.google.com/books?ie=ISO-8859-1&id">http://books.google.com/books?ie=ISO-8859-1&id</a>;=3NmFwNHaNbUC&q;=wallace+%22statistical+and+inductive+inference+by+minimum+message+length%22&dq;=wallace+%22statistical+and+inductive+inference+by+minimum+message+length%22 sample pages].</li>
<li>A <a href="http://www.allisons.org/ll/Images/People/Wallace/">searchable database of Chris Wallace's publications</a>.</li>
<li><a href="http://comjnl.oxfordjournals.org/cgi/reprint/42/4/270">Minimum Message Length and Kolmogorov Complexity</a> (by <a href="http://www.csse.monash.edu.au/~dld/CSWallacePublications/">C.S. Wallace</a> and <a href="http://www.csse.monash.edu.au/~dld">D.L. Dowe</a>, Computer Journal, Vol. 42, No. 4, 1999, <a href="http://comjnl.oxfordjournals.org/cgi/reprint/42/4/270">pp270-283</a>).</li>
<li><a href="http://www.allisons.org/ll/MML/20031120e/">History of MML, CSW's last talk</a>.</li>
<li><a href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#NeedhamDowe2001">Message Length as an Effective Ockham's Razor in Decision Tree Induction</a>, by S. Needham and <a href="http://www.csse.monash.edu.au/~dld">D. Dowe</a>, Proc. <a href="http://www.ai.mit.edu/conferences/aistats2001">8th International Workshop on AI and Statistics</a> (2001), <a href="http://www.csse.monash.edu.au/~dld/Publications/2001/Needham+Dowe2001_Ockham.pdf">pp253-260</a>. (Shows how <a href="Occam's_razor" title="wikilink">Occam's razor</a> works fine when interpreted as <a href="http://www.csse.monash.edu.au/~dld/MML.html">MML</a>.)</li>
<li>L.Allison,</li>
<li><a href="http://dx.doi.org/10.1017/S0956796804005301">Models for machine learning and data mining in functional programming</a>, J. Functional Programming, 15(1), pp15–32, Jan. 2005 (MML, FP, and Haskell <a href="http://www.allisons.org/ll/Publications/200309/READ-ME.shtml">code</a>).</li>
<li>J.W.Comley and <a href="http://www.csse.monash.edu.au/~dld">D.L. Dowe</a> (2005), "<a href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#ComleyDowe2005">Minimum Message Length, MDL and Generalised Bayesian Networks with Asymmetric Languages</a>", [<a class="uri" href="http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid">http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid</a>;=10478&mode;=toc Chapter 11] (pp <a href="http://www.csse.monash.edu.au/~dld/Publications/2005/ComleyDowe2005MMLGeneralizedBayesianNetsAsymmetricLanguages_p265.jpg">265</a>-<a href="http://www.csse.monash.edu.au/~dld/Publications/2005/ComleyDowe2005MMLGeneralizedBayesianNetsAsymmetricLanguages_p294.jpg">294</a>) in P. Grunwald, M. A. Pitt and I. J. Myung (ed.), [<a class="uri" href="http://mitpress.mit.edu/catalog/item/default.asp?sid=4C100C6F-2255-40FF-A2ED-02FC49FEBE7C&ttype">http://mitpress.mit.edu/catalog/item/default.asp?sid=4C100C6F-2255-40FF-A2ED-02FC49FEBE7C&ttype</a>;=2&tid;=10478 Advances in Minimum Description Length: Theory and Applications], M.I.T. Press (MIT Press), April 2005, [<a class="uri" href="http://mitpress.mit.edu/catalog/item/default.asp?sid=4C100C6F-2255-40FF-A2ED-02FC49FEBE7C&ttype">http://mitpress.mit.edu/catalog/item/default.asp?sid=4C100C6F-2255-40FF-A2ED-02FC49FEBE7C&ttype</a>;=2&tid;=10478 ISBN] [<a class="uri" href="http://mitpress.mit.edu/catalog/item/default.asp?sid=4C100C6F-2255-40FF-A2ED-02FC49FEBE7C&ttype">http://mitpress.mit.edu/catalog/item/default.asp?sid=4C100C6F-2255-40FF-A2ED-02FC49FEBE7C&ttype</a>;=2&tid;=10478 0-262-07262-9].</li>
</ul>
<p>[See also <a href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#ComleyDowe2003">Comley and Dowe (2003)</a>, <a href="http://www.csse.monash.edu.au/~dld/Publications/2003/Comley+Dowe03_HICS2003_GeneralBayesianNetworksAsymmetricLanguages.pdf">.pdf</a>. Comley & Dowe (<a href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#ComleyDowe2003">2003</a>, <a href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#ComleyDowe2005">2005</a>) are the first two papers on MML Bayesian nets using both discrete and continuous valued parameters.]</p>
<ul>
<li>Dowe, David L. (2010). <a href="http://www.csse.monash.edu.au/~dld/Publications/2010/Dowe2010_MML_HandbookPhilSci_Vol7_HandbookPhilStat_MML+hybridBayesianNetworkGraphicalModels+StatisticalConsistency+InvarianceAndUniqueness_pp901-982.pdf">MML, hybrid Bayesian network graphical models, statistical consistency, invariance and uniqueness</a>, in Handbook of Philosophy of Science (Volume 7: Handbook of Philosophy of Statistics), Elsevier, <a href="http://japan.elsevier.com/products/books/HPS.pdf">ISBN 978-0-444-51862-0</a>, pp <a href="http://www.csse.monash.edu.au/~dld/Publications/2010/Dowe2010_MML_HandbookPhilSci_Vol7_HandbookPhilStat_MML+hybridBayesianNetworkGraphicalModels+StatisticalConsistency+InvarianceAndUniqueness_pp901-982.pdf">901-982</a>.</li>
<li><a href="http://www.csse.monash.edu.au/~lloyd/tildeMML/">Minimum Message Length (MML)</a>, LA's MML introduction, <a href="http://www.allisons.org/ll/MML/">(MML alt.)</a>.</li>
<li><a href="http://www.csse.monash.edu.au/~dld/MML.html">Minimum Message Length (MML), researchers and links</a>.</li>
<li><a href="http://www.csse.monash.edu.au/mml/">Another MML research website.</a></li>
<li><a href="http://www.csse.monash.edu.au/~dld/Snob.html">Snob page</a> for MML <a href="mixture_model" title="wikilink">mixture modelling</a>.</li>
<li><a href="http://ai.ato.ms/MITECS/Entry/wallace">MITECS</a>: <a href="http://www.csse.monash.edu.au/~dld/CSWallacePublications/">Chris Wallace</a> wrote an entry on MML for MITECS. (Requires account)</li>
<li><a href="http://www.cs.helsinki.fi/u/floreen/sem/mikko.ps">mikko.ps</a>: Short introductory slides by Mikko Koivisto in Helsinki]</li>
<li><a href="Akaike_information_criterion" title="wikilink">Akaike information criterion</a> (<a href="Akaike_information_criterion" title="wikilink">AIC</a>) method of <a href="model_selection" title="wikilink">model selection</a>, and a <a href="http://www.csse.monash.edu.au/~dld/David.Dowe.publications.html#DoweGardnerOppy2007">comparison</a> with MML: <a href="http://www.csse.monash.edu.au/~dld">D.L. Dowe</a>, S. Gardner & G. Oppy (2007), "<a href="http://bjps.oxfordjournals.org/cgi/content/abstract/axm033v1">Bayes not Bust! Why Simplicity is no Problem for Bayesians</a>", <a href="http://bjps.oxfordjournals.org">Brit. J. Philos. Sci.</a>, Vol. 58, Dec. 2007, pp709–754.</li>
</ul>
<p>"</p>
<p><a href="Category:Algorithmic_information_theory" title="wikilink">Category:Algorithmic information theory</a></p>
</body>
</html>
