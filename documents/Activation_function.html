<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1582">Activation function</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Activation function</h1>
<hr/>

<p>In computational networks, the <strong>activation function</strong> of a node defines the output of that node given an input or set of inputs. A standard <a href="Integrated_circuit" title="wikilink">computer chip circuit</a> can be seen as a <a href="Digital_electronics" title="wikilink">digital network</a> of activation functions that can be "ON" (1) or "OFF" (0), depending on input. This is similar to the behavior of the <a href="linear_perceptron" title="wikilink">linear perceptron</a> in <a href="neural_networks" title="wikilink">neural networks</a>. However, it is the <em>nonlinear</em> activation function that allows such networks to compute nontrivial problems using only a small number of nodes.</p>
<h2 id="functions">Functions</h2>

<p>In biologically inspired neural networks, the activation function is usually an abstraction representing the rate of <a href="action_potential" title="wikilink">action potential</a> firing in the cell. In its simplest form, this function is <a href="Binary_function" title="wikilink">binary</a>—that is, either the <a class="uri" href="neuron" title="wikilink">neuron</a> is firing or not. The function looks like 

<math display="inline" id="Activation_function:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>ϕ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>U</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>ϕ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>U</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \phi(v_{i})=U(v_{i})
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Activation_function:1">
 <semantics>
  <mi>U</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>U</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   U
  </annotation>
 </semantics>
</math>

 is the <a href="Heaviside_step_function" title="wikilink">Heaviside step function</a>. In this case a large number of neurons must be used in computation beyond linear separation of categories.</p>

<p>A line of positive <a class="uri" href="slope" title="wikilink">slope</a> may also be used to reflect the increase in firing rate that occurs as input current increases. The function would then be of the form 

<math display="inline" id="Activation_function:2">
 <semantics>
  <mrow>
   <mrow>
    <mi>ϕ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>μ</mi>
    <msub>
     <mi>v</mi>
     <mi>i</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>ϕ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>μ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \phi(v_{i})=\mu v_{i}
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Activation_function:3">
 <semantics>
  <mi>μ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>μ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu
  </annotation>
 </semantics>
</math>

 is the slope. This activation function is linear, and therefore has the same problems as the binary function. In addition, networks constructed using this model have <a href="BIBO_stability" title="wikilink">unstable convergence</a> because neuron inputs along favored paths tend to increase without bound, as this function is not <a href="Normalizing_constant" title="wikilink">normalizable</a>.</p>

<p>All problems mentioned above can be handled by using a normalizable <a href="Sigmoid_function" title="wikilink">sigmoid</a> activation function. One realistic model stays at zero until input current is received, at which point the firing frequency increases quickly at first, but gradually approaches an <a class="uri" href="asymptote" title="wikilink">asymptote</a> at 100% firing rate. Mathematically, this looks like 

<math display="inline" id="Activation_function:4">
 <semantics>
  <mrow>
   <mrow>
    <mi>ϕ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>U</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mi>tanh</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>v</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>ϕ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>U</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <tanh></tanh>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>v</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \phi(v_{i})=U(v_{i})\tanh(v_{i})
  </annotation>
 </semantics>
</math>

, where the <a href="hyperbolic_tangent" title="wikilink">hyperbolic tangent</a> function can also be replaced by any <a href="sigmoid_function" title="wikilink">sigmoid function</a>. This behavior is realistically reflected in the neuron, as neurons cannot physically fire faster than a certain rate. This model runs into problems, however, in computational networks as it is not <a href="wikt:differentiation" title="wikilink">differentiable</a>, a requirement in order to calculate <a class="uri" href="backpropagation" title="wikilink">backpropagation</a>.</p>

<p>The final model, then, that is used in <a href="multilayer_perceptron" title="wikilink">multilayer perceptrons</a> is a sigmoidal activation function in the form of a hyperbolic tangent. Two forms of this function are commonly used

<math display="block" id="Activation_function:5">
 <semantics>
  <mrow>
   <mrow>
    <mi>ϕ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>tanh</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>ϕ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <tanh></tanh>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \phi(v_{i})=\tanh(v_{i})
  </annotation>
 </semantics>
</math>

 whose range is normalized from -1 to 1, and 

<math display="inline" id="Activation_function:6">
 <semantics>
  <mrow>
   <mrow>
    <mi>ϕ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mn>1</mn>
      <mo>+</mo>
      <mrow>
       <mi>exp</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mo>-</mo>
         <msub>
          <mi>v</mi>
          <mi>i</mi>
         </msub>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mo>-</mo>
     <mn>1</mn>
    </mrow>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>ϕ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <plus></plus>
      <cn type="integer">1</cn>
      <apply>
       <exp></exp>
       <apply>
        <minus></minus>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>v</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
     </apply>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \phi(v_{i})=(1+\exp(-v_{i}))^{-1}
  </annotation>
 </semantics>
</math>

 is vertically translated to normalize from 0 to 1. The latter model is often considered more biologically realistic, but it runs into theoretical and experimental difficulties with certain types of computational problems.</p>
<h3 id="alternative-structures">Alternative structures</h3>

<p>A special class of activation functions known as <a href="radial_basis_function" title="wikilink">radial basis functions</a> (RBFs) are used in <a href="Radial_basis_function_network" title="wikilink">RBF networks</a>, which are extremely efficient as universal function approximators. These activation functions can take many forms, but they are usually found as one of three functions:</p>
<ul>
<li>Gaussian

<math display="block" id="Activation_function:7">
 <semantics>
  <mrow>
   <mrow>
    <mpadded lspace="1.7pt" width="+1.7pt">
     <mi>ϕ</mi>
    </mpadded>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>exp</mi>
    <mrow>
     <mo>(</mo>
     <mrow>
      <mo>-</mo>
      <mfrac>
       <msup>
        <mrow>
         <mo>∥</mo>
         <mrow>
          <msub>
           <mi>v</mi>
           <mi>i</mi>
          </msub>
          <mo>-</mo>
          <msub>
           <mi>c</mi>
           <mi>i</mi>
          </msub>
         </mrow>
         <mo>∥</mo>
        </mrow>
        <mn>2</mn>
       </msup>
       <mrow>
        <mn>2</mn>
        <msup>
         <mi>σ</mi>
         <mn>2</mn>
        </msup>
       </mrow>
      </mfrac>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>ϕ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <exp></exp>
     <apply>
      <minus></minus>
      <apply>
       <divide></divide>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="latexml">norm</csymbol>
         <apply>
          <minus></minus>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>v</ci>
           <ci>i</ci>
          </apply>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>c</ci>
           <ci>i</ci>
          </apply>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
       <apply>
        <times></times>
        <cn type="integer">2</cn>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>σ</ci>
         <cn type="integer">2</cn>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \,\phi(v_{i})=\exp\left(-\frac{\|v_{i}-c_{i}\|^{2}}{2\sigma^{2}}\right)
  </annotation>
 </semantics>
</math>

</li>
<li>Multiquadratics

<math display="block" id="Activation_function:8">
 <semantics>
  <mrow>
   <mrow>
    <mpadded lspace="1.7pt" width="+1.7pt">
     <mi>ϕ</mi>
    </mpadded>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msqrt>
    <mrow>
     <msup>
      <mrow>
       <mo>∥</mo>
       <mrow>
        <msub>
         <mi>v</mi>
         <mi>i</mi>
        </msub>
        <mo>-</mo>
        <msub>
         <mi>c</mi>
         <mi>i</mi>
        </msub>
       </mrow>
       <mo>∥</mo>
      </mrow>
      <mn>2</mn>
     </msup>
     <mo>+</mo>
     <msup>
      <mi>a</mi>
      <mn>2</mn>
     </msup>
    </mrow>
   </msqrt>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>ϕ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <root></root>
     <apply>
      <plus></plus>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="latexml">norm</csymbol>
        <apply>
         <minus></minus>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>v</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>c</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>a</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \,\phi(v_{i})=\sqrt{\|v_{i}-c_{i}\|^{2}+a^{2}}
  </annotation>
 </semantics>
</math>

</li>
<li>Inverse multiquadratics

<math display="block" id="Activation_function:9">
 <semantics>
  <mrow>
   <mrow>
    <mpadded lspace="1.7pt" width="+1.7pt">
     <mi>ϕ</mi>
    </mpadded>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msup>
       <mrow>
        <mo>∥</mo>
        <mrow>
         <msub>
          <mi>v</mi>
          <mi>i</mi>
         </msub>
         <mo>-</mo>
         <msub>
          <mi>c</mi>
          <mi>i</mi>
         </msub>
        </mrow>
        <mo>∥</mo>
       </mrow>
       <mn>2</mn>
      </msup>
      <mo>+</mo>
      <msup>
       <mi>a</mi>
       <mn>2</mn>
      </msup>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mo>-</mo>
     <mrow>
      <mn>1</mn>
      <mo>/</mo>
      <mn>2</mn>
     </mrow>
    </mrow>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>ϕ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <plus></plus>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="latexml">norm</csymbol>
        <apply>
         <minus></minus>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>v</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>c</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>a</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
     <apply>
      <minus></minus>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \,\phi(v_{i})=(\|v_{i}-c_{i}\|^{2}+a^{2})^{-1/2}
  </annotation>
 </semantics>
</math>

</li>
</ul>

<p>where 

<math display="inline" id="Activation_function:10">
 <semantics>
  <msub>
   <mi>c</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>c</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   c_{i}
  </annotation>
 </semantics>
</math>

 is the vector representing the function <em>center</em> and 

<math display="inline" id="Activation_function:11">
 <semantics>
  <mi>a</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>a</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Activation_function:12">
 <semantics>
  <mi>σ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>σ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma
  </annotation>
 </semantics>
</math>

 are parameters affecting the spread of the radius.</p>

<p><a href="Support_vector_machines" title="wikilink">Support vector machines</a> (SVMs) can effectively utilize a class of activation functions that includes both sigmoids and RBFs. In this case, the input is transformed to reflect a decision boundary hyperplane based on a few training inputs called <em>support vectors</em> 

<math display="inline" id="Activation_function:13">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

. The activation function for the hidden layer of these machines is referred to as the <em>inner product kernel</em>, 

<math display="inline" id="Activation_function:14">
 <semantics>
  <mrow>
   <mrow>
    <mi>K</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo>,</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>ϕ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>K</ci>
     <interval closure="open">
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>v</ci>
       <ci>i</ci>
      </apply>
      <ci>x</ci>
     </interval>
    </apply>
    <apply>
     <times></times>
     <ci>ϕ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K(v_{i},x)=\phi(v_{i})
  </annotation>
 </semantics>
</math>

. The support vectors are represented as the centers in RBFs with the kernel equal to the activation function, but they take a unique form in the perceptron as</p>

<p>

<math display="block" id="Activation_function:15">
 <semantics>
  <mrow>
   <mrow>
    <mpadded lspace="1.7pt" width="+1.7pt">
     <mi>ϕ</mi>
    </mpadded>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>tanh</mi>
    <mrow>
     <mo>(</mo>
     <mrow>
      <msub>
       <mi>β</mi>
       <mn>1</mn>
      </msub>
      <mo>+</mo>
      <mrow>
       <msub>
        <mi>β</mi>
        <mn>0</mn>
       </msub>
       <mrow>
        <munder>
         <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
         <mi>j</mi>
        </munder>
        <mrow>
         <msub>
          <mi>v</mi>
          <mrow>
           <mi>i</mi>
           <mo>,</mo>
           <mi>j</mi>
          </mrow>
         </msub>
         <msub>
          <mi>x</mi>
          <mi>j</mi>
         </msub>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>ϕ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <tanh></tanh>
     <apply>
      <plus></plus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>β</ci>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>β</ci>
        <cn type="integer">0</cn>
       </apply>
       <apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <ci>j</ci>
        </apply>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>v</ci>
          <list>
           <ci>i</ci>
           <ci>j</ci>
          </list>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>j</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \,\phi(v_{i})=\tanh\left(\beta_{1}+\beta_{0}\sum_{j}v_{i,j}x_{j}\right)
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Activation_function:16">
 <semantics>
  <msub>
   <mi>β</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>β</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \beta_{0}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Activation_function:17">
 <semantics>
  <msub>
   <mi>β</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>β</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \beta_{1}
  </annotation>
 </semantics>
</math>

 must satisfy certain conditions for convergence. These machines can also accept arbitrary-order polynomial activation functions where</p>

<p>

<math display="block" id="Activation_function:18">
 <semantics>
  <mrow>
   <mrow>
    <mpadded lspace="1.7pt" width="+1.7pt">
     <mi>ϕ</mi>
    </mpadded>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo>(</mo>
     <mrow>
      <mn>1</mn>
      <mo>+</mo>
      <mrow>
       <munder>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mi>j</mi>
       </munder>
       <mrow>
        <msub>
         <mi>v</mi>
         <mrow>
          <mi>i</mi>
          <mo>,</mo>
          <mi>j</mi>
         </mrow>
        </msub>
        <msub>
         <mi>x</mi>
         <mi>j</mi>
        </msub>
       </mrow>
      </mrow>
     </mrow>
     <mo>)</mo>
    </mrow>
    <mi>p</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>ϕ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>v</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <plus></plus>
      <cn type="integer">1</cn>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <ci>j</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>v</ci>
         <list>
          <ci>i</ci>
          <ci>j</ci>
         </list>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>j</ci>
        </apply>
       </apply>
      </apply>
     </apply>
     <ci>p</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \,\phi(v_{i})=\left(1+\sum_{j}v_{i,j}x_{j}\right)^{p}
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Activation function having types:</p>
<ol>
<li>Identity function.</li>
<li>Binary step function.</li>
<li>Bipolar step function.</li>
<li>Sigmoidal function.
<ol>
<li>Binary sigmoidal function.</li>
<li>Bipolar sigmoidal function.</li>
</ol></li>
<li>Ramp function.</li>
</ol>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Logistic_function" title="wikilink">Logistic function</a></li>
<li><a href="Rectifier_(neural_networks)" title="wikilink">Rectifier (neural networks)</a></li>
<li><a href="Stability_(learning_theory)" title="wikilink">Stability (learning theory)</a></li>
<li><a href="Softmax_function" title="wikilink">Softmax function</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
</ol>
</section>
</body>
</html>
