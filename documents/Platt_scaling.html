<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="892">Platt scaling</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Platt scaling</h1>
<hr/>

<p>In <a href="machine_learning" title="wikilink">machine learning</a>, <strong>Platt scaling</strong> or <strong>Platt calibration</strong> is a way of transforming the outputs of a <a href="statistical_classification" title="wikilink">classification model</a> into a <a href="probabilistic_classification" title="wikilink">probability distribution over classes</a>. The method was invented by <a href="John_Platt_(computer_scientist)" title="wikilink">John Platt</a> in the context of <a href="support_vector_machines" title="wikilink">support vector machines</a>,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> replacing an earlier method by <a href="Vladimir_Vapnik" title="wikilink">Vapnik</a>, but can be applied to other classification models.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Platt scaling works by fitting a <a href="logistic_regression" title="wikilink">logistic regression</a> model to a classifier's scores.</p>
<h2 id="description">Description</h2>

<p>Consider the problem of <a href="binary_classification" title="wikilink">binary classification</a>: for inputs 

<math display="inline" id="Platt_scaling:0">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

, we want to determine whether they belong to one of two classes, arbitrarily labeled 

<math display="inline" id="Platt_scaling:1">
 <semantics>
  <mrow>
   <mo>+</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   +1
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Platt_scaling:2">
 <semantics>
  <mrow>
   <mi mathvariant="normal">−</mi>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-−</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   −1
  </annotation>
 </semantics>
</math>

. We assume that the classification problem will be solved by a real-valued function 

<math display="inline" id="Platt_scaling:3">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>


, by predicting a class label 

<math display="inline" id="Platt_scaling:4">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mrow>
    <mi>s</mi>
    <mi>i</mi>
    <mi>g</mi>
    <mi>n</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <apply>
     <times></times>
     <ci>s</ci>
     <ci>i</ci>
     <ci>g</ci>
     <ci>n</ci>
     <apply>
      <times></times>
      <ci>f</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=sign(f(x))
  </annotation>
 </semantics>
</math>

. For many problems, it is convenient to get a probability 

<math display="inline" id="Platt_scaling:5">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo>=</mo>
    <mn>1</mn>
    <mo stretchy="false">|</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">y</csymbol>
     <eq></eq>
     <cn type="integer">1</cn>
     <ci>normal-|</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(y=1|x)
  </annotation>
 </semantics>
</math>

, i.e. a classification that not only gives an answer, but also a degree of certainty about the answer. Some classification models do not provide such a probability, or give poor probability estimates.</p>

<p>Platt scaling is an algorithm to solve the aforementioned problem. It produces probability estimates</p>

<p>

<math display="block" id="Platt_scaling:6">
 <semantics>
  <mrow>
   <mi mathvariant="normal">P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo>=</mo>
    <mn>1</mn>
    <mo stretchy="false">|</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mn>1</mn>
    <mrow>
     <mn>1</mn>
     <mo>+</mo>
     <mrow>
      <mi>exp</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mrow>
         <mi>A</mi>
         <mi>f</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>x</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>+</mo>
        <mi>B</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">y</csymbol>
     <eq></eq>
     <cn type="integer">1</cn>
     <ci>normal-|</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <apply>
      <plus></plus>
      <cn type="integer">1</cn>
      <apply>
       <exp></exp>
       <apply>
        <plus></plus>
        <apply>
         <times></times>
         <ci>A</ci>
         <ci>f</ci>
         <ci>x</ci>
        </apply>
        <ci>B</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{P}(y=1|x)=\frac{1}{1+\exp(Af(x)+B)}
  </annotation>
 </semantics>
</math>

,</p>

<p>i.e., a <a href="logistic_function" title="wikilink">logistic</a> transformation of the classifier scores 

<math display="inline" id="Platt_scaling:7">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Platt_scaling:8">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A
  </annotation>
 </semantics>
</math>


 and 

<math display="inline" id="Platt_scaling:9">
 <semantics>
  <mi>B</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>B</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B
  </annotation>
 </semantics>
</math>

 are two <a href="scalar_(mathematics)" title="wikilink">scalar</a> parameters that are learned by the algorithm. Note that predictions can now be made according to 

<math display="inline" id="Platt_scaling:10">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=1
  </annotation>
 </semantics>
</math>

 iff 

<math display="inline" id="Platt_scaling:11">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo>=</mo>
    <mn>1</mn>
    <mo stretchy="false">|</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>></mo>
   <mi mathvariant="normal">½</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">y</csymbol>
     <eq></eq>
     <cn type="integer">1</cn>
     <ci>normal-|</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <gt></gt>
    <csymbol cd="unknown">½</csymbol>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(y=1|x)>½
  </annotation>
 </semantics>
</math>

; if 

<math display="inline" id="Platt_scaling:12">
 <semantics>
  <mrow>
   <mi>B</mi>
   <mi mathvariant="normal">≠</mi>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>B</ci>
    <ci>normal-≠</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B≠0
  </annotation>
 </semantics>
</math>

, the probability estimates contain a correction compared to the old decision function 

<math display="inline" id="Platt_scaling:13">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mrow>
    <mi>s</mi>
    <mi>i</mi>
    <mi>g</mi>
    <mi>n</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <apply>
     <times></times>
     <ci>s</ci>
     <ci>i</ci>
     <ci>g</ci>
     <ci>n</ci>
     <apply>
      <times></times>
      <ci>f</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=sign(f(x))
  </annotation>
 </semantics>
</math>


.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>The parameters 

<math display="inline" id="Platt_scaling:14">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Platt_scaling:15">
 <semantics>
  <mi>B</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>B</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B
  </annotation>
 </semantics>
</math>

 are estimated using a <a href="maximum_likelihood_estimation" title="wikilink">maximum likelihood</a> method that optimizes on the same training set as that for the original classifier 

<math display="inline" id="Platt_scaling:16">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

. To avoid <a class="uri" href="overfitting" title="wikilink">overfitting</a> to this set, a held-out <a href="validation_set" title="wikilink">calibration set</a> or <a href="cross-validation_(statistics)" title="wikilink">cross-validation</a> can be used, but Platt additionally suggests transforming the labels 

<math display="inline" id="Platt_scaling:17">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 to target probabilities</p>

<p>

<math display="block" id="Platt_scaling:18">
 <semantics>
  <mrow>
   <msub>
    <mi>t</mi>
    <mo>+</mo>
   </msub>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <msub>
      <mi>N</mi>
      <mo>+</mo>
     </msub>
     <mo>+</mo>
     <mn>1</mn>
    </mrow>
    <mrow>
     <msub>
      <mi>N</mi>
      <mo>+</mo>
     </msub>
     <mo>+</mo>
     <mn>2</mn>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>t</ci>
     <plus></plus>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <plus></plus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>N</ci>
       <plus></plus>
      </apply>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <plus></plus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>N</ci>
       <plus></plus>
      </apply>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   t_{+}=\frac{N_{+}+1}{N_{+}+2}
  </annotation>
 </semantics>
</math>

 for positive samples (

<math display="inline" id="Platt_scaling:19">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=1
  </annotation>
 </semantics>
</math>

), and</p>

<p>

<math display="block" id="Platt_scaling:20">
 <semantics>
  <mrow>
   <msub>
    <mi>t</mi>
    <mo>-</mo>
   </msub>
   <mo>=</mo>
   <mfrac>
    <mn>1</mn>
    <mrow>
     <msub>
      <mi>N</mi>
      <mo>-</mo>
     </msub>
     <mo>+</mo>
     <mn>2</mn>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>t</ci>
     <minus></minus>
    </apply>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <apply>
      <plus></plus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>N</ci>
       <minus></minus>
      </apply>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   t_{-}=\frac{1}{N_{-}+2}
  </annotation>
 </semantics>
</math>

 for negative samples, 

<math display="inline" id="Platt_scaling:21">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mn>1</mn>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=-1
  </annotation>
 </semantics>
</math>

.</p>

<p>Here, 

<math display="inline" id="Platt_scaling:22">
 <semantics>
  <mrow>
   <mi>N</mi>
   <mi mathvariant="normal">₊</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>N</ci>
    <ci>normal-₊</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N₊
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Platt_scaling:23">
 <semantics>
  <mrow>
   <mi>N</mi>
   <mi mathvariant="normal">₋</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>N</ci>
    <ci>normal-₋</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N₋
  </annotation>
 </semantics>
</math>


 are the number of positive and negative samples, resp. This transformation follows by applying <a href="Bayes'_rule" title="wikilink">Bayes' rule</a> to a model of out-of-sample data that has a uniform prior over the labels.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>Platt himself suggested using the <a href="Levenberg–Marquardt_algorithm" title="wikilink">Levenberg–Marquardt algorithm</a> to optimize the parameters, but a <a href="Newton's_method_in_optimization" title="wikilink">Newton algorithm</a> was later proposed that should be more <a href="numerical_stability" title="wikilink">numerically stable</a>.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h2 id="analysis">Analysis</h2>

<p>Platt scaling has been shown to be effective for SVMs as well as other types of classification models, including <a href="Boosting_(machine_learning)" title="wikilink">boosted</a> models and even <a href="naive_Bayes_classifier" title="wikilink">naive Bayes classifiers</a>, which produce distorted probability distributions. It is particularly effective for max-margin methods such as SVMs and boosted trees, which show sigmoidal distortions in their predicted probabilities, but has less of an effect with well-calibrated models such as <a href="logistic_regression" title="wikilink">logistic regression</a>, <a href="multilayer_perceptron" title="wikilink">multilayer perceptrons</a> and <a href="random_forest" title="wikilink">random forests</a>.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>

<p>An alternative approach to probability calibration is to fit an <a href="isotonic_regression" title="wikilink">isotonic regression</a> model to an ill-calibrated probability model. This has been shown to work better than Platt scaling, in particular when enough training data is available.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Relevance_vector_machine" title="wikilink">Relevance vector machine</a>: probabilistic alternative to the support vector machine</li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Probabilistic_models" title="wikilink">Category:Probabilistic models</a> <a href="Category:Statistical_classification" title="wikilink">Category:Statistical classification</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"></li>
<li id="fn7"></li>
</ol>
</section>
</body>
</html>
