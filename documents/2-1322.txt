   Markov's inequality      Markov's inequality   In probability theory , Markov's inequality gives an upper bound for the probability that a non-negative  function of a random variable is greater than or equal to some positive constant . It is named after the Russian mathematician Andrey Markov , although it appeared earlier in the work of Pafnuty Chebyshev (Markov's teacher), and many sources, especially in analysis , refer to it as Chebyshev's inequality (sometimes, calling it the first Chebyshev inequality, while referring to the Chebyshev's inequality as the second Chebyshev's inequality) or BienaymÃ© 's inequality.  Markov's inequality (and other similar inequalities) relate probabilities to expectations , and provide (frequently loose but still useful) bounds for the cumulative distribution function of a random variable.  An example of an application of Markov's inequality is the fact that (assuming incomes are non-negative) no more than 1/5 of the population can have more than 5 times the average income.  Statement  If   X   X   X   is any nonnegative integrable random variable and    a  >    0       a    normal-  0     a>Â 0   , then      â„™   (  X  â‰¥  a  )   â‰¤    ğ”¼   (  X  )    a   .     fragments  P   fragments  normal-(  X   a  normal-)        ğ”¼  X   a   normal-.    \mathbb{P}(X\geq a)\leq\frac{\mathbb{E}(X)}{a}.     In the language of measure theory , Markov's inequality states that if    (  X  ,    Î£   ,  Î¼  )     X    normal-  normal-Î£   Î¼    (X,Â Î£,Î¼)   is a measure space ,   f   f   f   is a measurable  extended real -valued function, and    Îµ  >  0      Îµ  0    Îµ>0   , then        Î¼   (   {   x  âˆˆ  X   :    |   f   (  x  )    |   â‰¥  Îµ   }   )    â‰¤    1  Îµ     âˆ«  X     |  f  |   d  Î¼      .        Î¼   conditional-set    x  X         f  x    Îµ         1  Îµ     subscript   X       f   d  Î¼       \mu(\{x\in X:|f(x)|\geq\varepsilon\})\leq{1\over\varepsilon}\int_{X}|f|\,d\mu.     (This measure theoretic definition may sometimes be referred to as Chebyshev's inequality . 1 )  Extended version for monotonically increasing functions  If   Ï†   Ï†   Ï†   is a monotonically increasing function from the nonnegative reals to the nonnegative reals,   X   X   X   is a random variable,    a  â‰¥  0      a  normal-â‰¥  0    aâ‰¥0   , and     Ï†   (  a  )    >  0        Ï†  a   0    Ï†(a)>0   , then      â„™   (  |  X  |  â‰¥  a  )   â‰¤    ğ”¼   (   Ï†   (   |  X  |   )    )     Ï†   (  a  )        fragments  P   fragments  normal-(  normal-|  X  normal-|   a  normal-)        ğ”¼    Ï†    X       Ï†  a      \mathbb{P}(|X|\geq a)\leq\frac{\mathbb{E}(\varphi(|X|))}{\varphi(a)}     Proofs  We separate the case in which the measure space is a probability space from the more general case because the probability case is more accessible for the general reader.  Proof In the language of probability theory  For any event E , let I E be the indicator random variable of E , that is, I E =Â 1 if E occurs and I E =Â 0 otherwise.  Using this notation, we have I ( X â‰¥ a ) =Â 1 if the event X â‰¥ a occurs, and I ( X â‰¥ a ) =Â 0 if X 0,       a   I   (  X  â‰¥  a  )     â‰¤   X         a   subscript  I   fragments  normal-(  X   a  normal-)     X    aI_{(X\geq a)}\leq X\,     which is clear if we consider the two possible values of I ( X â‰¥ a ) . If X ( X â‰¥ a ) = 0, and so aI ( X â‰¥ a ) =Â 0 â‰¤ X . Otherwise, we have X â‰¥ a , for which I ( X â‰¥ a ) = 1 and so aI ( X â‰¥ a ) =Â a â‰¤ X .  Since   ğ”¼   ğ”¼   \mathbb{E}   is a monotonically increasing function, taking expectation of both sides of an inequality cannot reverse it. Therefore        ğ”¼   (   a   I   (  X  â‰¥  a  )     )    â‰¤   ğ”¼   (  X  )     .        ğ”¼    a   subscript  I   fragments  normal-(  X   a  normal-)        ğ”¼  X     \mathbb{E}(aI_{(X\geq a)})\leq\mathbb{E}(X).\,     Now, using linearity of expectations, the left side of this inequality is the same as      a  ğ”¼   (   I   (  X  â‰¥  a  )    )   =  a   (  1  â‹…  â„™   (  X  â‰¥  a  )   +  0  â‹…  â„™   (  X  <  a  )   )   =  a  â„™   (  X  â‰¥  a  )   .     fragments  a  E   fragments  normal-(   subscript  I   fragments  normal-(  X   a  normal-)    normal-)    a   fragments  normal-(  1  normal-â‹…  P   fragments  normal-(  X   a  normal-)    0  normal-â‹…  P   fragments  normal-(  X   a  normal-)   normal-)    a  P   fragments  normal-(  X   a  normal-)   normal-.    a\mathbb{E}(I_{(X\geq a)})=a(1\cdot\mathbb{P}(X\geq a)+0\cdot\mathbb{P}(X     Thus we have      a  â„™   (  X  â‰¥  a  )   â‰¤  ğ”¼   (  X  )      fragments  a  P   fragments  normal-(  X   a  normal-)    E   fragments  normal-(  X  normal-)     a\mathbb{P}(X\geq a)\leq\mathbb{E}(X)\,     and since a > 0, we can divide both sides by a .  In the language of measure theory  We may assume that the function   f   f   f   is non-negative, since only its absolute value enters in the equation. Now, consider the real-valued function s on X given by       s   (  x  )    =   {      Îµ  ,       if  f   (  x  )    â‰¥  Îµ        0  ,       if  f   (  x  )    <  Îµ             s  x    cases  Îµ      if  f  x   Îµ   0      if  f  x   Îµ      s(x)=\begin{cases}\varepsilon,&\text{if }f(x)\geq\varepsilon\\
 0,&\text{if }f(x)<\varepsilon\end{cases}     Then    0  â‰¤   s   (  x  )    â‰¤   f   (  x  )          0    s  x          f  x      0\leq s(x)\leq f(x)   . By the definition of the Lebesgue integral        âˆ«  X    f   (  x  )   d  Î¼    â‰¥    âˆ«  X    s   (  x  )   d  Î¼    =   Îµ  Î¼   (   {   x  âˆˆ  X   :    f   (  x  )    â‰¥  Îµ   }   )            subscript   X     f  x  d  Î¼      subscript   X     s  x  d  Î¼           Îµ  Î¼   conditional-set    x  X       f  x   Îµ        \int_{X}f(x)\,d\mu\geq\int_{X}s(x)\,d\mu=\varepsilon\mu(\{x\in X:\,f(x)\geq%
 \varepsilon\})     and since    Îµ  >  0      Îµ  0    \varepsilon>0   , both sides can be divided by   Îµ   Îµ   \varepsilon   , obtaining        Î¼   (   {   x  âˆˆ  X   :    f   (  x  )    â‰¥  Îµ   }   )    â‰¤    1  Îµ     âˆ«  X     f   d  Î¼      .        Î¼   conditional-set    x  X       f  x   Îµ         1  Îµ     subscript   X     f  d  Î¼       \mu(\{x\in X:\,f(x)\geq\varepsilon\})\leq{1\over\varepsilon}\int_{X}f\,d\mu.     Q.E.D.  Corollaries  Chebyshev's inequality  Chebyshev's inequality uses the variance to bound the probability that a random variable deviates far from the mean. Specifically:      â„™   (  |  X  -  ğ”¼   (  X  )   |  â‰¥  a  )   â‰¤    Var   (  X  )     a  2    ,     fragments  P   fragments  normal-(  normal-|  X   E   fragments  normal-(  X  normal-)   normal-|   a  normal-)        Var  X    superscript  a  2    normal-,    \mathbb{P}(|X-\mathbb{E}(X)|\geq a)\leq\frac{\mathrm{Var}(X)}{a^{2}},     for any a>0 . Here Var(X) is the variance of X, defined as:        Var   (  X  )    =   ğ”¼   [    (   X  -   ğ”¼   (  X  )     )   2   ]     .       Var  X     ğ”¼   delimited-[]   superscript    X    ğ”¼  X    2       \operatorname{Var}(X)=\mathbb{E}[(X-\mathbb{E}(X))^{2}].     Chebyshev's inequality follows from Markov's inequality by considering the random variable       (   X  -   ğ”¼   (  X  )     )   2     superscript    X    ğ”¼  X    2    (X-\mathbb{E}(X))^{2}     and the constant      a  2     superscript  a  2    a^{2}     for which Markov's inequality reads      â„™   (    (  X  -  ğ”¼   (  X  )   )   2   â‰¥   a  2   )   â‰¤    Var   (  X  )     a  2    ,     fragments  P   fragments  normal-(   superscript   fragments  normal-(  X   E   fragments  normal-(  X  normal-)   normal-)   2     superscript  a  2   normal-)       Var  X    superscript  a  2    normal-,    \mathbb{P}((X-\mathbb{E}(X))^{2}\geq a^{2})\leq\frac{\operatorname{Var}(X)}{a^%
 {2}},     This argument can be summarized (where "MI" indicates use of Markov's inequality):      â„™   (  |  X  -  ğ”¼   (  X  )   |  â‰¥  a  )   =  â„™   (    (  X  -  ğ”¼   (  X  )   )   2   â‰¥   a  2   )    â‰¤    MI      ğ”¼   (    (   X  -   ğ”¼   (  X  )     )   2   )     a  2    =    Var   (  X  )     a  2       fragments  P   fragments  normal-(  normal-|  X   E   fragments  normal-(  X  normal-)   normal-|   a  normal-)    P   fragments  normal-(   superscript   fragments  normal-(  X   E   fragments  normal-(  X  normal-)   normal-)   2     superscript  a  2   normal-)     MI  absent         ğ”¼   superscript    X    ğ”¼  X    2     superscript  a  2        Var  X    superscript  a  2      \mathbb{P}(|X-\mathbb{E}(X)|\geq a)=\mathbb{P}\left((X-\mathbb{E}(X))^{2}\geq a%
 ^{2}\right)\overset{\underset{\mathrm{MI}}{}}{\leq}\frac{\mathbb{E}\left({(X-%
 \mathbb{E}(X))}^{2}\right)}{a^{2}}=\frac{\operatorname{Var}(X)}{a^{2}}     Other corollaries   The "monotonic" result can be demonstrated by:     â„™   (  |  X  |  â‰¥  a  )   â‰¤  â„™   (  Ï†   (  |  X  |  )   â‰¥  Ï†   (  a  )   )    â‰¤    MI    =    ğ”¼   (   Ï†   (   |  X  |   )    )     Ï†   (  a  )        fragments  P   fragments  normal-(  normal-|  X  normal-|   a  normal-)    P   fragments  normal-(  Ï†   fragments  normal-(  normal-|  X  normal-|  normal-)    Ï†   fragments  normal-(  a  normal-)   normal-)     MI  absent          ğ”¼    Ï†    X       Ï†  a      \mathbb{P}(|X|\geq a)\leq\mathbb{P}(\varphi(|X|)\geq\varphi(a))\overset{%
 \underset{\mathrm{MI}}{}}{\leq}=\frac{\mathbb{E}(\varphi(|X|))}{\varphi(a)}         The result that, for a nonnegative random variable   X   X   X   , the quantile function of   X   X   X   satisfies:        Q  X    (   1  -  p   )    â‰¤    ğ”¼   (  X  )    p    ,         subscript  Q  X     1  p        ğ”¼  X   p     Q_{X}(1-p)\leq\frac{\mathbb{E}(X)}{p},      the proof using      p  â‰¤  â„™   (  X  â‰¥   Q  X    (  1  -  p  )   )    â‰¤    MI      ğ”¼   (  X  )      Q  X    (   1  -  p   )     .     fragments  p   P   fragments  normal-(  X    subscript  Q  X    fragments  normal-(  1   p  normal-)   normal-)     MI  absent         ğ”¼  X      subscript  Q  X     1  p     normal-.    p\leq\mathbb{P}(X\geq Q_{X}(1-p))\overset{\underset{\mathrm{MI}}{}}{\leq}\frac%
 {\mathbb{E}(X)}{Q_{X}(1-p)}.        Let    M  âª°  0     succeeds-or-equals  M  0    M\succeq 0   be a self-adjoint matrix-valued random variable and    a  >  0      a  0    a>0   . Then :   \mathbb{P}(M \npreceq a \cdot I) \leq \frac{\mathrm{tr}\left( E(M) \right)}{n a}.  #:can be shown in a similar manner.  See also   McDiarmid's inequality  Bernstein inequalities (probability theory)  Dvoretzkyâ€“Kieferâ€“Wolfowitz inequality  Chernoff bound  Concentration inequality   References  External links   The formal proof of Markov's inequality in the Mizar system .   "  Category:Probabilistic inequalities  Category:Articles containing proofs     E.M. Stein, R. Shakarchi, "Real Analysis, Measure Theory, Integration, & Hilbert Spaces", vol. 3, 1st ed., 2005, p.91 â†©     