   Proofs of convergence of random variables      Proofs of convergence of random variables   This article is supplemental for “ Convergence of random variables ” and provides proofs for selected results.  Several results will be established using the portmanteau lemma : A sequence { X n } converges in distribution to X if and only if any of the following conditions are met:   E[ f ( X n )] → E[ f ( X )] for all bounded , continuous functions  f ;  E[ f ( X n )] → E[ f ( X )] for all bounded, Lipschitz functions  f ;  limsup{Pr( X n ∈ C )} ≤ Pr( X ∈ C ) for all closed sets  C ;   Convergence almost surely implies convergence in probability         X  n      →   a  s      X  ⇒       X  n      →  𝑝    X      formulae-sequence      a  s   normal-→    subscript  X  n    X  normal-⇒      p  normal-→    subscript  X  n   X     X_{n}\ \xrightarrow{as}\ X\quad\Rightarrow\quad X_{n}\ \xrightarrow{p}\ X    Proof: If { X n } converges to X almost surely, it means that the set of points {ω: lim X n (ω) ≠ X (ω)} has measure zero; denote this set O . Now fix ε > 0 and consider a sequence of sets       A  n   =   ⋃   m  ≥  n     {  |   X  m   -  X  |  >  ε  }      fragments   subscript  A  n     subscript     m  n     fragments  normal-{  normal-|   subscript  X  m    X  normal-|   ε  normal-}     A_{n}=\bigcup_{m\geq n}\left\{\left|X_{m}-X\right|>\varepsilon\right\}     This sequence of sets is decreasing: A n ⊇ A n +1 ⊇ ..., and it decreases towards the set        A  ∞   =    ⋂   n  ≥  1     A  n     .       subscript  A      subscript     n  1     subscript  A  n      A_{\infty}=\bigcap_{n\geq 1}A_{n}.     For this decreasing sequence of events, their probabilities are also a decreasing sequence, and it decreases towards the Pr( A ∞ ); we shall show now that this number is equal to zero. Now any point ω in the complement of O is such that lim X n (ω) = X (ω), which implies that | X n (ω) − X (ω)| n'', and consequently it will not belong to A ∞ . This means that A ∞ is disjoint with O , or equivalently, A ∞ is a subset of O and therefore Pr( A ∞ ) = 0.  Finally, consider        Pr   (    |    X  n   -  X   |   >  ε   )    ≤    Pr   (   A  n   )      →   n  →  ∞    0     ,       Pr         subscript  X  n   X    ε      Pr   subscript  A  n        normal-→  n    normal-→   0      \operatorname{Pr}\left(|X_{n}-X|>\varepsilon\right)\leq\operatorname{Pr}(A_{n}%
 )\ \underset{n\to\infty}{\rightarrow}0,   which by definition means that X n converges in probability to X .  Convergence in probability does not imply almost sure convergence in the discrete case  If X n are independent random variables assuming value one with probability 1/ n and zero otherwise, then X n converges to zero in probability but not almost surely. This can be verified using the Borel–Cantelli lemmas .  Convergence in probability implies convergence in distribution          X  n      →  𝑝     X  ⇒       X  n      →  𝑑    X    ,     formulae-sequence    p  normal-→    subscript  X  n    X  normal-⇒      d  normal-→    subscript  X  n   X     X_{n}\ \xrightarrow{p}\ X\quad\Rightarrow\quad X_{n}\ \xrightarrow{d}\ X,     Proof for the case of scalar random variables  Lemma. Let X , Y be random variables, a a real number and ε > 0. Then        Pr   (   Y  ≤  a   )    ≤    Pr   (   X  ≤   a  +  ε    )    +   Pr   (    |   Y  -  X   |   >  ε   )      .       Pr    Y  a       Pr    X    a  ε      Pr        Y  X    ε       \operatorname{Pr}(Y\leq a)\leq\operatorname{Pr}(X\leq a+\varepsilon)+%
 \operatorname{Pr}(|Y-X|>\varepsilon).   (or     {  Y  ≤  a  }   ⊂   {  X  ≤  a  +  ε  }   ∪   {  |  Y  -  X  |  >  ε  }   .     fragments   fragments  normal-{  Y   a  normal-}     fragments  normal-{  X   a   ε  normal-}     fragments  normal-{  normal-|  Y   X  normal-|   ε  normal-}   normal-.    \{Y\leq a\}\subset\{X\leq a+\varepsilon\}\cup\{|Y-X|>\varepsilon\}.   )  Proof of lemma:      Pr   (   Y  ≤  a   )      Pr    Y  a     \displaystyle\operatorname{Pr}(Y\leq a)     Proof of the theorem: Recall that in order to prove convergence in distribution, one must show that the sequence of cumulative distribution functions converges to the F X at every point where F X is continuous. Let a be such a point. For every ε > 0, due to the preceding lemma, we have:      Pr   (    X  n   ≤  a   )      Pr     subscript  X  n   a     \displaystyle\operatorname{Pr}(X_{n}\leq a)     So, we have         Pr   (   X  ≤   a  -  ε    )    -   Pr   (    |    X  n   -  X   |   >  ε   )     ≤   Pr   (    X  n   ≤  a   )    ≤    Pr   (   X  ≤   a  +  ε    )    +   Pr   (    |    X  n   -  X   |   >  ε   )      .           Pr    X    a  ε      Pr         subscript  X  n   X    ε      Pr     subscript  X  n   a            Pr    X    a  ε      Pr         subscript  X  n   X    ε        \operatorname{Pr}(X\leq a-\varepsilon)-\operatorname{Pr}\left(\left|X_{n}-X%
 \right|>\varepsilon\right)\leq\operatorname{Pr}\left(X_{n}\leq a\right)\leq%
 \operatorname{Pr}(X\leq a+\varepsilon)+\operatorname{Pr}\left(\left|X_{n}-X%
 \right|>\varepsilon\right).     Taking the limit as n → ∞, we obtain:         F  X    (   a  -  ε   )    ≤    lim   n  →  ∞     Pr   (    X  n   ≤  a   )     ≤    F  X    (   a  +  ε   )     ,           subscript  F  X     a  ε      subscript    normal-→  n      Pr     subscript  X  n   a             subscript  F  X     a  ε       F_{X}(a-\varepsilon)\leq\lim_{n\to\infty}\operatorname{Pr}(X_{n}\leq a)\leq F_%
 {X}(a+\varepsilon),   where F X ( a ) = Pr( X ≤ a ) is the cumulative distribution function of X . This function is continuous at a by assumption, and therefore both F X ( a −ε) and F X ( a +ε) converge to F X ( a ) as ε → 0 + . Taking this limit, we obtain         lim   n  →  ∞     Pr   (    X  n   ≤  a   )     =   Pr   (   X  ≤  a   )     ,        subscript    normal-→  n      Pr     subscript  X  n   a      Pr    X  a      \lim_{n\to\infty}\operatorname{Pr}(X_{n}\leq a)=\operatorname{Pr}(X\leq a),   which means that { X n } converges to X in distribution.  Proof for the generic case  We see that | X n − X | converges in probability to zero, and also X converges to X in distribution trivially. Applying the property proved later on this page we conclude that X n converges to X in distribution.  Convergence in distribution to a constant implies convergence in probability          X  n      →  𝑑     c  ⇒       X  n      →  𝑝    c    ,     formulae-sequence    d  normal-→    subscript  X  n    c  normal-⇒      p  normal-→    subscript  X  n   c     X_{n}\ \xrightarrow{d}\ c\quad\Rightarrow\quad X_{n}\ \xrightarrow{p}\ c,    provided c is a constant.  Proof: Fix ε > 0. Let B ε ( c ) be the open ball of radius ε around point c , and B ε c ( c ) its complement. Then        Pr   (    |    X  n   -  c   |   ≥  ε   )    =   Pr   (    X  n   ∈    B  ε  c    (  c  )     )     .       Pr         subscript  X  n   c    ε     Pr     subscript  X  n      superscript   subscript  B  ε   c   c       \operatorname{Pr}\left(|X_{n}-c|\geq\varepsilon\right)=\operatorname{Pr}\left(%
 X_{n}\in B_{\varepsilon}^{c}(c)\right).   By the portmanteau lemma (part C), if X n converges in distribution to c , then the limsup of the latter probability must be less than or equal to Pr( c ∈ B ε c ( c )), which is obviously equal to zero. Therefore       lim   n  →  ∞     Pr   (    |    X  n   -  c   |   ≥  ε   )        subscript    normal-→  n      Pr         subscript  X  n   c    ε      \displaystyle\lim_{n\to\infty}\operatorname{Pr}\left(\left|X_{n}-c\right|\geq%
 \varepsilon\right)     which by definition means that X n converges to c in probability.  Convergence in probability to a sequence converging in distribution implies convergence to the same distribution        |    Y  n   -   X  n    |    →  𝑝   0   ,      X  n      →  𝑑     X  ⇒       Y  n      →  𝑑    X       formulae-sequence    p  normal-→        subscript  Y  n    subscript  X  n     0    formulae-sequence    d  normal-→    subscript  X  n    X  normal-⇒      d  normal-→    subscript  Y  n   X      |Y_{n}-X_{n}|\ \xrightarrow{p}\ 0,\ \ X_{n}\ \xrightarrow{d}\ X\ \quad%
 \Rightarrow\quad Y_{n}\ \xrightarrow{d}\ X     Proof: We will prove this theorem using the portmanteau lemma, part B. As required in that lemma, consider any bounded function f (i.e. | f ( x )| ≤ M ) which is also Lipschitz:      ∃  K  >  0  ,  ∀  x  ,  y  :  |  f   (  x  )   -  f   (  y  )   |  ≤  K  |  x  -  y  |  .     fragments   K   0  normal-,  for-all  x  normal-,  y  normal-:   normal-|  f   fragments  normal-(  x  normal-)    f   fragments  normal-(  y  normal-)   normal-|   K  normal-|  x   y  normal-|  normal-.    \exists K>0,\forall x,y:\quad|f(x)-f(y)|\leq K|x-y|.     Take some ε > 0 and majorize the expression |E[ f ( Y n )] − E[ f ( X n )]| as      |    E   [   f   (   Y  n   )    ]    -   E   [   f   (   X  n   )    ]     |         normal-E    f   subscript  Y  n      normal-E    f   subscript  X  n        \displaystyle\left|\operatorname{E}\left[f(Y_{n})\right]-\operatorname{E}\left%
 [f(X_{n})\right]\right|     (here 1 {...} denotes the indicator function ; the expectation of the indicator function is equal to the probability of corresponding event). Therefore      |    E   [   f   (   Y  n   )    ]    -   E   [   f   (  X  )    ]     |         normal-E    f   subscript  Y  n      normal-E    f  X       \displaystyle\left|\operatorname{E}\left[f(Y_{n})\right]-\operatorname{E}\left%
 [f(X)\right]\right|   If we take the limit in this expression as n → ∞, the second term will go to zero since { Y n −X n } converges to zero in probability; and the third term will also converge to zero, by the portmanteau lemma and the fact that X n converges to X in distribution. Thus         lim   n  →  ∞     |    E   [   f   (   Y  n   )    ]    -   E   [   f   (  X  )    ]     |    ≤   K  ε    .        subscript    normal-→  n          normal-E    f   subscript  Y  n      normal-E    f  X         K  ε     \lim_{n\to\infty}\left|\operatorname{E}\left[f(Y_{n})\right]-\operatorname{E}%
 \left[f(X)\right]\right|\leq K\varepsilon.   Since ε was arbitrary, we conclude that the limit must in fact be equal to zero, and therefore E[ f ( Y n )] → E[ f ( X )], which again by the portmanteau lemma implies that { Y n } converges to X in distribution. QED.  Convergence of one sequence in distribution and another to a constant implies joint convergence in distribution         X  n      →  𝑑    X   ,      Y  n      →  𝑑     c  ⇒      (   X  n   ,   Y  n   )     →  𝑑     (  X  ,  c  )        formulae-sequence    d  normal-→    subscript  X  n   X    formulae-sequence    d  normal-→    subscript  Y  n    c  normal-⇒      d  normal-→     subscript  X  n    subscript  Y  n     X  c       X_{n}\ \xrightarrow{d}\ X,\ \ Y_{n}\ \xrightarrow{d}\ c\ \quad\Rightarrow\quad%
 (X_{n},Y_{n})\ \xrightarrow{d}\ (X,c)    provided c is a constant.  Proof: We will prove this statement using the portmanteau lemma, part A.  First we want to show that ( X n , c ) converges in distribution to ( X , c ). By the portmanteau lemma this will be true if we can show that E[ f ( X n , c )] → E[ f ( X , c )] for any bounded continuous function f ( x , y ). So let f be such arbitrary bounded continuous function. Now consider the function of a single variable g ( x ) := f ( x , c ). This will obviously be also bounded and continuous, and therefore by the portmanteau lemma for sequence { X n } converging in distribution to X , we will have that E[ g ( X n )] → E[ g ( X )]. However the latter expression is equivalent to “E[ f ( X n , c )] → E[ f ( X , c )]”, and therefore we now know that ( X n , c ) converges in distribution to ( X , c ).  Secondly, consider |( X n , Y n ) − ( X n , c )| = | Y n − c |. This expression converges in probability to zero because Y n converges in probability to c . Thus we have demonstrated two facts:      {        |    (   X  n   ,   Y  n   )   -   (   X  n   ,  c  )    |    →  𝑝   0   ,           (   X  n   ,  c  )     →  𝑑     (  X  ,  c  )    .          cases    p  normal-→         subscript  X  n    subscript  Y  n      subscript  X  n   c     0   otherwise    d  normal-→     subscript  X  n   c    X  c    otherwise    \begin{cases}\left|(X_{n},Y_{n})-(X_{n},c)\right|\ \xrightarrow{p}\ 0,\\
 (X_{n},c)\ \xrightarrow{d}\ (X,c).\end{cases}   By the property proved earlier , these two facts imply that ( X n , Y n ) converge in distribution to ( X , c ).  Convergence of two sequences in probability implies joint convergence in probability         X  n      →  𝑝    X   ,      Y  n      →  𝑝     Y  ⇒      (   X  n   ,   Y  n   )     →  𝑝     (  X  ,  Y  )        formulae-sequence    p  normal-→    subscript  X  n   X    formulae-sequence    p  normal-→    subscript  Y  n    Y  normal-⇒      p  normal-→     subscript  X  n    subscript  Y  n     X  Y       X_{n}\ \xrightarrow{p}\ X,\ \ Y_{n}\ \xrightarrow{p}\ Y\ \quad\Rightarrow\quad%
 (X_{n},Y_{n})\ \xrightarrow{p}\ (X,Y)     Proof:      Pr   (    |    (   X  n   ,   Y  n   )   -   (  X  ,  Y  )    |   ≥  ε   )      Pr          subscript  X  n    subscript  Y  n     X  Y     ε     \displaystyle\operatorname{Pr}\left(\left|(X_{n},Y_{n})-(X,Y)\right|\geq%
 \varepsilon\right)   Each of the probabilities on the right-hand side converge to zero as n → ∞ by definition of the convergence of { X n } and { Y n } in probability to X and Y respectively. Taking the limit we conclude that the left-hand side also converges to zero, and therefore the sequence {( X n , Y n )} converges in probability to {( X , Y )}.  See also   Convergence of random variables   References     "  Category:Article proofs  Category:Probability theory   