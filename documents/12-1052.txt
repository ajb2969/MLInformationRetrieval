   Tensor rank decomposition      Tensor rank decomposition   In multilinear algebra , the tensor rank decomposition or canonical polyadic decomposition (CPD) may be regarded as a generalization of the matrix singular value decomposition (SVD) to tensors , which has found application in statistics , signal processing , psychometrics , linguistics and chemometrics . It was introduced by Hitchcock in 1927 1 and later rediscovered several times, notably in psychometrics. 2 3 For this reason, the tensor rank decomposition is sometimes historically referred to as PARAFAC 4 or CANDECOMP. 5  Definition  Consider a tensor space     ğ”½    n  1   Ã—  â‹¯  Ã—   n  d     â‰…    ğ”½   n  1    âŠ—  â‹¯  âŠ—   ğ”½   n  d          superscript  ğ”½     subscript  n  1   normal-â‹¯   subscript  n  d      tensor-product   superscript  ğ”½   subscript  n  1    normal-â‹¯   superscript  ğ”½   subscript  n  d       \mathbb{F}^{n_{1}\times\cdots\times n_{d}}\cong\mathbb{F}^{n_{1}}\otimes\cdots%
 \otimes\mathbb{F}^{n_{d}}   , where   ğ”½   ğ”½   \mathbb{F}   is either the real field   â„   â„   \mathbb{R}   or the complex field   â„‚   â„‚   \mathbb{C}   . Every (order-   d   d   d   ) tensor in this space may then be represented with a suitably large   r   r   r   as a linear combination of   r   r   r   rank-1 tensors:       ğ’œ  =    âˆ‘   i  =  1   r      Î»  i    ğš  i  1    âŠ—   ğš  i  2   âŠ—  â‹¯  âŠ—   ğš  i  d      ,      ğ’œ    superscript   subscript     i  1    r    tensor-product     subscript  Î»  i    superscript   subscript  ğš  i   1     superscript   subscript  ğš  i   2   normal-â‹¯   superscript   subscript  ğš  i   d       \mathcal{A}=\sum_{i=1}^{r}\lambda_{i}\mathbf{a}_{i}^{1}\otimes\mathbf{a}_{i}^{%
 2}\otimes\cdots\otimes\mathbf{a}_{i}^{d},     where     Î»  i   âˆˆ  ğ”½       subscript  Î»  i   ğ”½    \lambda_{i}\in\mathbb{F}   and     ğš  i  k   âˆˆ   ğ”½   n  k         superscript   subscript  ğš  i   k    superscript  ğ”½   subscript  n  k      \mathbf{a}_{i}^{k}\in\mathbb{F}^{n_{k}}   ; note that the superscript   k   k   k   should not be interpreted as an exponent, it is merely another index. When the number of terms   r   r   r   is minimal in the above expression, then   r   r   r   is called the rank of the tensor, and the decomposition is often referred to as a (tensor) rank decomposition , minimal CP decomposition , or Canonical Polyadic Decomposition (CPD) . Contrariwise, if the number of terms is not minimal, then the above decomposition is often referred to as    r   r   r   -term decomposition , CANDECOMP/PARAFAC or Polyadic decomposition .  Tensor rank  Contrary to the case of matrices, the rank of a tensor is presently not understood well. It is known that the problem of computing the rank of a tensor is NP-hard . 6 The only notable well-understood case consists of tensors in     ğ”½  m   âŠ—   ğ”½  n   âŠ—   ğ”½  2      tensor-product   superscript  ğ”½  m    superscript  ğ”½  n    superscript  ğ”½  2     \mathbb{F}^{m}\otimes\mathbb{F}^{n}\otimes\mathbb{F}^{2}   , whose rank can be obtained from the Kronecker â€“ Weierstrass normal form of the linear matrix pencil that the tensor represents. 7 A simple polynomial-time algorithm exists for certifying that a tensor is of rank 1, namely the higher-order singular value decomposition .  The rank of the tensor of zeros is zero by convention. The rank of a tensor     ğš  1   âŠ—  â‹¯  âŠ—   ğš  d      tensor-product   subscript  ğš  1   normal-â‹¯   subscript  ğš  d     \mathbf{a}_{1}\otimes\cdots\otimes\mathbf{a}_{d}   is one, provided that     ğš  k   âˆˆ    ğ”½   n  k    âˆ–   {  0  }         subscript  ğš  k      superscript  ğ”½   subscript  n  k     0      \mathbf{a}_{k}\in\mathbb{F}^{n_{k}}\setminus\{0\}   .  Field dependence  The rank of a tensor depends on the field over which the tensor is decomposed. It is known that some real tensors may admit a complex decomposition whose rank is strictly less than the rank of a real decomposition of the same tensor. As an example, 8 consider the following real tensor       ğ’œ  =       ğ±  1   âŠ—   ğ±  2   âŠ—   ğ±  3    +    ğ±  1   âŠ—   ğ²  2   âŠ—   ğ²  3     -    ğ²  1   âŠ—   ğ±  2   âŠ—   ğ²  3     +    ğ²  1   âŠ—   ğ²  2   âŠ—   ğ²  3      ,      ğ’œ         tensor-product   subscript  ğ±  1    subscript  ğ±  2    subscript  ğ±  3     tensor-product   subscript  ğ±  1    subscript  ğ²  2    subscript  ğ²  3      tensor-product   subscript  ğ²  1    subscript  ğ±  2    subscript  ğ²  3      tensor-product   subscript  ğ²  1    subscript  ğ²  2    subscript  ğ²  3       \mathcal{A}=\mathbf{x}_{1}\otimes\mathbf{x}_{2}\otimes\mathbf{x}_{3}+\mathbf{x%
 }_{1}\otimes\mathbf{y}_{2}\otimes\mathbf{y}_{3}-\mathbf{y}_{1}\otimes\mathbf{x%
 }_{2}\otimes\mathbf{y}_{3}+\mathbf{y}_{1}\otimes\mathbf{y}_{2}\otimes\mathbf{y%
 }_{3},     whose rank over the reals is known to be 3, while its complex rank is only 2 because it is the sum of a complex rank-1 tensor with its complex conjugate :       ğ’œ  =    1  2    (      ğ³  Â¯   1   âŠ—   ğ³  2   âŠ—    ğ³  Â¯   3    +    ğ³  1   âŠ—    ğ³  Â¯   2   âŠ—   ğ³  3     )     .      ğ’œ      1  2      tensor-product   subscript   normal-Â¯  ğ³   1    subscript  ğ³  2    subscript   normal-Â¯  ğ³   3     tensor-product   subscript  ğ³  1    subscript   normal-Â¯  ğ³   2    subscript  ğ³  3        \mathcal{A}=\frac{1}{2}(\bar{\mathbf{z}}_{1}\otimes\mathbf{z}_{2}\otimes\bar{%
 \mathbf{z}}_{3}+\mathbf{z}_{1}\otimes\bar{\mathbf{z}}_{2}\otimes\mathbf{z}_{3}).     In contrast, the rank of real matrices will never decrease under a field extension to   â„‚   â„‚   \mathbb{C}   : real matrix rank and complex matrix rank coincide for real matrices.  Generic rank  The generic rank     r   (   n  1   ,  â€¦  ,   n  d   )       r    subscript  n  1   normal-â€¦   subscript  n  d      r(n_{1},\ldots,n_{d})   is defined as the least rank   r   r   r   such that the closure in the Zariski topology of the set of tensors of rank at most   r   r   r   is the entire space     ğ”½   n  1    âŠ—  â‹¯  âŠ—   ğ”½   n  d       tensor-product   superscript  ğ”½   subscript  n  1    normal-â‹¯   superscript  ğ”½   subscript  n  d      \mathbb{F}^{n_{1}}\otimes\cdots\otimes\mathbb{F}^{n_{d}}   . In the case of complex tensors, tensors of rank at most    r   (   n  1   ,  â€¦  ,   n  d   )       r    subscript  n  1   normal-â€¦   subscript  n  d      r(n_{1},\ldots,n_{d})   form a dense set    S   S   S   : every tensor in the aforementioned space is either of rank less than the generic rank, or it is the limit in the Euclidean topology of a sequence of tensors from   S   S   S   . In the case of real tensors, the set of tensors of rank at most    r   (   n  1   ,  â€¦  ,   n  d   )       r    subscript  n  1   normal-â€¦   subscript  n  d      r(n_{1},\ldots,n_{d})   only forms an open set of positive measure in the Euclidean topology. There may exist Euclidean-open sets of tensors of rank strictly higher than the generic rank. All ranks appearing on open sets in the Euclidean topology are called typical ranks . The smallest typical rank is called the generic rank; this definition applies to both complex and real tensors. The generic rank of tensor spaces was initially studied in 1983 by Volker Strassen . 9  As an illustration of the above concepts, it is known that both 2 and 3 are typical ranks of     â„  2   âŠ—   â„  2   âŠ—   â„  2      tensor-product   superscript  â„  2    superscript  â„  2    superscript  â„  2     \mathbb{R}^{2}\otimes\mathbb{R}^{2}\otimes\mathbb{R}^{2}   while the generic rank of     â„‚  2   âŠ—   â„‚  2   âŠ—   â„‚  2      tensor-product   superscript  â„‚  2    superscript  â„‚  2    superscript  â„‚  2     \mathbb{C}^{2}\otimes\mathbb{C}^{2}\otimes\mathbb{C}^{2}   is 2. Practically, this means that a randomly sampled real tensor (from a continuous probability measure on the space of tensors) of size    2  Ã—  2  Ã—  2      2  2  2    2\times 2\times 2   will be a rank-1 tensor with probability zero, a rank-2 tensor with positive probability, and rank-3 with positive probability. On the other hand, a randomly sampled complex tensor of the same size will be a rank-1 tensor with probability zero, a rank-2 tensor with probability one, and a rank-3 tensor with probability zero. It is even known that the generic rank-3 real tensor in     â„  2   âŠ—   â„  2   âŠ—   â„  2      tensor-product   superscript  â„  2    superscript  â„  2    superscript  â„  2     \mathbb{R}^{2}\otimes\mathbb{R}^{2}\otimes\mathbb{R}^{2}   will be of complex rank equal to 2.  The generic rank of tensor spaces depends on the distinction between balanced and unbalanced tensor spaces. A tensor space     ğ”½   n  1    âŠ—  â‹¯  âŠ—   ğ”½   n  d       tensor-product   superscript  ğ”½   subscript  n  1    normal-â‹¯   superscript  ğ”½   subscript  n  d      \mathbb{F}^{n_{1}}\otimes\cdots\otimes\mathbb{F}^{n_{d}}   , where     n  1   â‰¥   n  2   â‰¥  â‹¯  â‰¥   n  d          subscript  n  1    subscript  n  2        normal-â‹¯        subscript  n  d      n_{1}\geq n_{2}\geq\cdots\geq n_{d}   , is called unbalanced whenever        n  1   >    1  +    âˆ   k  =  2   d    n  k     -    âˆ‘   k  =  2   d    (    n  k   -  1   )      ,       subscript  n  1       1    superscript   subscript  product    k  2    d    subscript  n  k       superscript   subscript     k  2    d      subscript  n  k   1       n_{1}>1+\prod_{k=2}^{d}n_{k}-\sum_{k=2}^{d}(n_{k}-1),     and it is called balanced otherwise.  Unbalanced tensor spaces  When the first factor is very large with respect to the other factors in the tensor product, then the tensor space essentially behaves as a linear space. The generic rank of tensors living in an unbalanced tensor spaces is known to equal       r   (   n  1   ,  â€¦  ,   r  d   )    =   min   {   n  1   ,    âˆ   k  =  2   d    n  k    }          r    subscript  n  1   normal-â€¦   subscript  r  d        subscript  n  1     superscript   subscript  product    k  2    d    subscript  n  k       r(n_{1},\ldots,r_{d})=\min\left\{n_{1},\prod_{k=2}^{d}n_{k}\right\}     almost everywhere . More precisely, the rank of every tensor in an unbalanced tensor space     ğ”½    n  1   Ã—  â‹¯  Ã—   n  d     âˆ–  Z       superscript  ğ”½     subscript  n  1   normal-â‹¯   subscript  n  d     Z    \mathbb{F}^{n_{1}\times\cdots\times n_{d}}\setminus Z   , where   Z   Z   Z   is some indeterminate closed set in the Zariski topology, equals the above value. 10  Balanced tensor spaces  The generic rank of tensors living in a balanced tensor space in is expected to equal        r  E    (   n  1   ,  â€¦  ,   n  d   )    =   âŒˆ   Î    Î£  +  1    âŒ‰          subscript  r  E     subscript  n  1   normal-â€¦   subscript  n  d         normal-Î     normal-Î£  1       r_{E}(n_{1},\ldots,n_{d})=\left\lceil\frac{\Pi}{\Sigma+1}\right\rceil     almost everywhere for complex tensors and on a Euclidean-open set for real tensors, where        Î   =     âˆ   k  =  1   d    n  k    and     Î£  =    âˆ‘   k  =  1   d    (    n  k   -  1   )      .     formulae-sequence    normal-Î      superscript   subscript  product    k  1    d    subscript  n  k    and      normal-Î£    superscript   subscript     k  1    d      subscript  n  k   1       \Pi=\prod_{k=1}^{d}n_{k}\quad\text{and}\quad\Sigma=\sum_{k=1}^{d}(n_{k}-1).     More precisely, the rank of every tensor in     â„‚    n  1   Ã—  â‹¯  Ã—   n  d     âˆ–  Z       superscript  â„‚     subscript  n  1   normal-â‹¯   subscript  n  d     Z    \mathbb{C}^{n_{1}\times\cdots\times n_{d}}\setminus Z   , where   Z   Z   Z   is some indeterminate closed set in the Zariski topology , is expected to equal the above value. 11 For real tensors,     r  E    (   n  1   ,  â€¦  ,   n  d   )        subscript  r  E     subscript  n  1   normal-â€¦   subscript  n  d      r_{E}(n_{1},\ldots,n_{d})   is the least rank that is expected to occur on a set of positive Euclidean measure. The value     r  E    (   n  1   ,  â€¦  ,   n  d   )        subscript  r  E     subscript  n  1   normal-â€¦   subscript  n  d      r_{E}(n_{1},\ldots,n_{d})   is often referred to as the expected generic rank of the tensor space    ğ”½    n  1   Ã—  â‹¯  Ã—   n  d       superscript  ğ”½     subscript  n  1   normal-â‹¯   subscript  n  d      \mathbb{F}^{n_{1}\times\cdots\times n_{d}}   because it is only conjecturally correct. It is known that the true generic rank always satisfies        r   (   n  1   ,  â€¦  ,   n  d   )    â‰¥    r  E    (   n  1   ,  â€¦  ,   n  d   )     .        r    subscript  n  1   normal-â€¦   subscript  n  d        subscript  r  E     subscript  n  1   normal-â€¦   subscript  n  d       r(n_{1},\ldots,n_{d})\geq r_{E}(n_{1},\ldots,n_{d}).     The Aboâ€“Ottavianiâ€“Peterson conjecture 12 states that equality is expected, i.e.,     r   (   n  1   ,  â€¦  ,   n  d   )    =    r  E    (   n  1   ,  â€¦  ,   n  d   )          r    subscript  n  1   normal-â€¦   subscript  n  d        subscript  r  E     subscript  n  1   normal-â€¦   subscript  n  d       r(n_{1},\ldots,n_{d})=r_{E}(n_{1},\ldots,n_{d})   , with the following exceptional cases:       ğ”½   4  Ã—  4  Ã—  3      superscript  ğ”½    4  4  3     \mathbb{F}^{4\times 4\times 3}           ğ”½    (    2  n   +  1   )   Ã—   (    2  n   +  1   )   Ã—  3    with  n   =   1  ,  2  ,  â€¦          superscript  ğ”½        2  n   1       2  n   1   3    with  n    1  2  normal-â€¦     \mathbb{F}^{(2n+1)\times(2n+1)\times 3}\text{ with }n=1,2,\ldots           ğ”½    (   n  +  1   )   Ã—   (   n  +  1   )   Ã—  2  Ã—  2    with  n   =   1  ,  2  ,  â€¦          superscript  ğ”½      n  1     n  1   2  2    with  n    1  2  normal-â€¦     \mathbb{F}^{(n+1)\times(n+1)\times 2\times 2}\text{ with }n=1,2,\ldots      In each of these exceptional cases, the generic rank is known to be     r   (   n  1   ,  â€¦  ,   n  d   )    =     r  E    (   n  1   ,  â€¦  ,   n  d   )    +  1         r    subscript  n  1   normal-â€¦   subscript  n  d          subscript  r  E     subscript  n  1   normal-â€¦   subscript  n  d     1     r(n_{1},\ldots,n_{d})=r_{E}(n_{1},\ldots,n_{d})+1   . The conjecture has been proved completely in a number of special cases. Lickteig showed already in 1985 that     r   (  n  ,  n  ,  n  )    =    r  E    (  n  ,  n  ,  n  )          r   n  n  n       subscript  r  E    n  n  n      r(n,n,n)=r_{E}(n,n,n)   , provided that    n  â‰   3      n  3    n\neq 3   . 13 In 2011, a major breakthrough was established by Catalisano, Geramita, and Gimigliano who proved that     r   (  2  ,  2  ,  â€¦  ,  2  )    =    r  E    (  2  ,  2  ,  â€¦  ,  2  )          r   2  2  normal-â€¦  2       subscript  r  E    2  2  normal-â€¦  2      r(2,2,\ldots,2)=r_{E}(2,2,\ldots,2)   , except for the space     ğ”½  2   âŠ—   ğ”½  2   âŠ—   ğ”½  2   âŠ—   ğ”½  2      tensor-product   superscript  ğ”½  2    superscript  ğ”½  2    superscript  ğ”½  2    superscript  ğ”½  2     \mathbb{F}^{2}\otimes\mathbb{F}^{2}\otimes\mathbb{F}^{2}\otimes\mathbb{F}^{2}   . 14  Maximum rank  The maximum rank that can be admitted by any of the tensors in a tensor space is unknown in general; even a conjecture about this maximum rank is missing. Presently, the best general upper bound states that the maximum rank     r  M    (   n  1   ,  â€¦  ,   n  d   )        subscript  r  M     subscript  n  1   normal-â€¦   subscript  n  d      r_{M}(n_{1},\ldots,n_{d})   of     ğ”½   n  1    âŠ—  â‹¯  âŠ—   ğ”½   n  d       tensor-product   superscript  ğ”½   subscript  n  1    normal-â‹¯   superscript  ğ”½   subscript  n  d      \mathbb{F}^{n_{1}}\otimes\cdots\otimes\mathbb{F}^{n_{d}}   , where     n  1   â‰¥   n  2   â‰¥  â‹¯  â‰¥   n  d          subscript  n  1    subscript  n  2        normal-â‹¯        subscript  n  d      n_{1}\geq n_{2}\geq\cdots\geq n_{d}   , satisfies         r  M    (   n  1   ,  â€¦  ,   n  d   )    â‰¤   min   {    âˆ   k  =  2   d    n  k    ,    2  â‹…  r    (   n  1   ,  â€¦  ,   n  d   )    }     ,         subscript  r  M     subscript  n  1   normal-â€¦   subscript  n  d         superscript   subscript  product    k  2    d    subscript  n  k       normal-â‹…  2  r     subscript  n  1   normal-â€¦   subscript  n  d        r_{M}(n_{1},\ldots,n_{d})\leq\min\left\{\prod_{k=2}^{d}n_{k},2\cdot r(n_{1},%
 \ldots,n_{d})\right\},     where    r   (   n  1   ,  â€¦  ,   n  d   )       r    subscript  n  1   normal-â€¦   subscript  n  d      r(n_{1},\ldots,n_{d})   is the (least) generic rank of     ğ”½   n  1    âŠ—  â‹¯  âŠ—   ğ”½   n  d       tensor-product   superscript  ğ”½   subscript  n  1    normal-â‹¯   superscript  ğ”½   subscript  n  d      \mathbb{F}^{n_{1}}\otimes\cdots\otimes\mathbb{F}^{n_{d}}   . 15 It is well-known that the foregoing inequality may be strict. For instance, the generic rank of tensors in    â„   2  Ã—  2  Ã—  2      superscript  â„    2  2  2     \mathbb{R}^{2\times 2\times 2}   is two, so that the above bound yields      r  M    (  2  ,  2  ,  2  )    â‰¤  4         subscript  r  M    2  2  2    4    r_{M}(2,2,2)\leq 4   , while it is known that the maximum rank equals 3. 16  Border rank  A rank-   s   s   s   tensor   ğ’œ   ğ’œ   \mathcal{A}   is called a border tensor if there exists a sequence of tensors of rank at most    r  <  s      r  s    r   whose limit is   ğ’œ   ğ’œ   \mathcal{A}   . If   s   s   s   is the least value for which such a convergent sequence exists, then it is called the border rank of   ğ’œ   ğ’œ   \mathcal{A}   . For order-2 tensors, i.e., matrices, rank and border rank always coincide, however, for tensors of order     â‰¥  3      absent  3    \geq 3   they may differ. Border tensors were first studied in the context of fast approximate  matrix multiplication algorithms by Bini, Lotti, and Romani in 1980. 17  A classic example of a border tensor is the rank-3 tensor       ğ’œ  =    ğ®  âŠ—  ğ®  âŠ—  ğ¯   +   ğ®  âŠ—  ğ¯  âŠ—  ğ®   +   ğ¯  âŠ—  ğ®  âŠ—  ğ®     ,    with   âˆ¥  ğ®  âˆ¥    =   âˆ¥  ğ¯  âˆ¥   =   1  and   âŸ¨  ğ®  ,  ğ¯  âŸ©    â‰   1.      formulae-sequence    ğ’œ     tensor-product  ğ®  ğ®  ğ¯    tensor-product  ğ®  ğ¯  ğ®    tensor-product  ğ¯  ğ®  ğ®           with   norm  ğ®     norm  ğ¯          1  and   ğ®  ğ¯         1.      \mathcal{A}=\mathbf{u}\otimes\mathbf{u}\otimes\mathbf{v}+\mathbf{u}\otimes%
 \mathbf{v}\otimes\mathbf{u}+\mathbf{v}\otimes\mathbf{u}\otimes\mathbf{u},\quad%
 \text{with }\|\mathbf{u}\|=\|\mathbf{v}\|=1\text{ and }\langle\mathbf{u},%
 \mathbf{v}\rangle\neq 1.     It can be approximated arbitrarily well by the following sequence of rank-2 tensors      ğ’œ  n     subscript  ğ’œ  n    \displaystyle\mathcal{A}_{n}     as    n  â†’  âˆ     normal-â†’  n     n\to\infty   . Therefore, its border rank is 2, which is strictly less than its rank.  Properties  Ill-posedness of the standard approximation problem  The rank approximation problem asks for the rank-   r   r   r   decomposition closest (in the usual Euclidean topology) to some rank-   s   s   s   tensor   ğ’œ   ğ’œ   \mathcal{A}   , where    r  <  s      r  s    r   . That is, one seeks to solve        min    ğš  i  k   âˆˆ   ğ”½   n  k        âˆ¥   ğ’œ  -    âˆ‘   i  =  1   r     ğš  i  1   âŠ—   ğš  i  2   âŠ—  â‹¯  âŠ—   ğš  i  d      âˆ¥   F    ,       subscript      superscript   subscript  ğš  i   k    superscript  ğ”½   subscript  n  k       subscript   norm    ğ’œ    superscript   subscript     i  1    r    tensor-product   superscript   subscript  ğš  i   1    superscript   subscript  ğš  i   2   normal-â‹¯   superscript   subscript  ğš  i   d       F     \min_{\mathbf{a}_{i}^{k}\in\mathbb{F}^{n_{k}}}\|\mathcal{A}-\sum_{i=1}^{r}%
 \mathbf{a}_{i}^{1}\otimes\mathbf{a}_{i}^{2}\otimes\cdots\otimes\mathbf{a}_{i}^%
 {d}\|_{F},     where    âˆ¥  â‹…   âˆ¥  F      fragments  parallel-to  normal-â‹…   subscript  parallel-to  F     \|\cdot\|_{F}   is the Frobenius norm .  It was shown in a 2008 paper by de Silva and Lim 18 that the above standard approximation problem may be ill-posed . A solution to aforementioned problem may sometimes not exist because the set over which one optimizes is not closed. As such, a minimizer may not exist, even though an infimum would exist. In particular, it is known that certain so-called border tensors may be approximated arbitrarily well by a sequence of tensor of rank at most   r   r   r   , even though the limit of the sequence converges to a tensor of rank strictly higher than   r   r   r   . The rank-3 tensor       ğ’œ  =    ğ®  âŠ—  ğ®  âŠ—  ğ¯   +   ğ®  âŠ—  ğ¯  âŠ—  ğ®   +   ğ¯  âŠ—  ğ®  âŠ—  ğ®     ,    with   âˆ¥  ğ®  âˆ¥    =   âˆ¥  ğ¯  âˆ¥   =   1  and   âŸ¨  ğ®  ,  ğ¯  âŸ©    â‰   1      formulae-sequence    ğ’œ     tensor-product  ğ®  ğ®  ğ¯    tensor-product  ğ®  ğ¯  ğ®    tensor-product  ğ¯  ğ®  ğ®           with   norm  ğ®     norm  ğ¯          1  and   ğ®  ğ¯         1      \mathcal{A}=\mathbf{u}\otimes\mathbf{u}\otimes\mathbf{v}+\mathbf{u}\otimes%
 \mathbf{v}\otimes\mathbf{u}+\mathbf{v}\otimes\mathbf{u}\otimes\mathbf{u},\quad%
 \text{with }\|\mathbf{u}\|=\|\mathbf{v}\|=1\text{ and }\langle\mathbf{u},%
 \mathbf{v}\rangle\neq 1     can be approximated arbitrarily well by the following sequence of rank-2 tensors       ğ’œ  n   =     n   (   ğ®  +    1  n   ğ¯    )    âŠ—   (   ğ®  +    1  n   ğ¯    )   âŠ—   (   ğ®  +    1  n   ğ¯    )    -    n  ğ®   âŠ—  ğ®  âŠ—  ğ®         subscript  ğ’œ  n      tensor-product    n    ğ®      1  n   ğ¯       ğ®      1  n   ğ¯      ğ®      1  n   ğ¯      tensor-product    n  ğ®   ğ®  ğ®      \mathcal{A}_{n}=n(\mathbf{u}+\frac{1}{n}\mathbf{v})\otimes(\mathbf{u}+\frac{1}%
 {n}\mathbf{v})\otimes(\mathbf{u}+\frac{1}{n}\mathbf{v})-n\mathbf{u}\otimes%
 \mathbf{u}\otimes\mathbf{u}     as    n  â†’  âˆ     normal-â†’  n     n\to\infty   . This example neatly illustrates the general principle that a sequence of rank-   r   r   r   tensors that converges to a tensor of strictly higher rank needs to admit at least two individual rank-1 terms whose norms become unbounded. Stated formally, whenever a sequence       ğ’œ  n   =    âˆ‘   i  =  1   r     ğš   i  ,  n   1   âŠ—   ğš   i  ,  n   2   âŠ—  â‹¯  âŠ—   ğš   i  ,  n   d          subscript  ğ’œ  n     superscript   subscript     i  1    r    tensor-product   subscript   superscript  ğš  1    i  n     subscript   superscript  ğš  2    i  n    normal-â‹¯   subscript   superscript  ğš  d    i  n        \mathcal{A}_{n}=\sum_{i=1}^{r}\mathbf{a}^{1}_{i,n}\otimes\mathbf{a}^{2}_{i,n}%
 \otimes\cdots\otimes\mathbf{a}^{d}_{i,n}     has the property that     ğ’œ  n   â†’  ğ’œ     normal-â†’   subscript  ğ’œ  n   ğ’œ    \mathcal{A}_{n}\to\mathcal{A}   (in the Euclidean topology) as    n  â†’  âˆ     normal-â†’  n     n\to\infty   , then there should exist at least    1  â‰¤  i  â‰   j  â‰¤  r        1  i       j       r     1\leq i\neq j\leq r   such that        âˆ¥    ğš   i  ,  n   1   âŠ—   ğš   i  ,  n   2   âŠ—  â‹¯  âŠ—   ğš   i  ,  n   d    âˆ¥   F   â†’   âˆ  and    âˆ¥    ğš   j  ,  n   1   âŠ—   ğš   j  ,  n   2   âŠ—  â‹¯  âŠ—   ğš   j  ,  n   d    âˆ¥   F    â†’  âˆ       normal-â†’   subscript   norm   tensor-product   subscript   superscript  ğš  1    i  n     subscript   superscript  ğš  2    i  n    normal-â‹¯   subscript   superscript  ğš  d    i  n      F      and   subscript   norm   tensor-product   subscript   superscript  ğš  1    j  n     subscript   superscript  ğš  2    j  n    normal-â‹¯   subscript   superscript  ğš  d    j  n      F      normal-â†’        \|\mathbf{a}^{1}_{i,n}\otimes\mathbf{a}^{2}_{i,n}\otimes\cdots\otimes\mathbf{a%
 }^{d}_{i,n}\|_{F}\to\infty\text{ and }\|\mathbf{a}^{1}_{j,n}\otimes\mathbf{a}^%
 {2}_{j,n}\otimes\cdots\otimes\mathbf{a}^{d}_{j,n}\|_{F}\to\infty     as    n  â†’  âˆ     normal-â†’  n     n\to\infty   . This phenomenon is often encountered when attempting to approximate a tensor using numerical optimization algorithms. It is sometimes called the problem of diverging components . It was, in addition, shown that a random low-rank tensor over the reals may not admit a rank-2 approximation with positive probability, leading to the understanding that the ill-posedness problem is an important consideration when employing the tensor rank decomposition.  A common partial solution to the ill-posedness problem consists of imposing an additional inequality constraint that bounds the norm of the individual rank-1 terms by some constant. Other constraints that result in a closed set, and, thus, well-posed optimization problem, include imposing positivity or a bounded inner product strictly less than unity between the rank-1 terms appearing in the sought decomposition.  Calculating the CPD  Alternating algorithms:   alternating least squares (ALS)  alternating slice-wise diagonalisation (ASD)   Algebraic algorithms:   simultaneous diagonalization (SD)  simultaneous generalized Schur decomposition (SGSD)   Optimization algorithms:   Levenbergâ€“Marquardt (LM)  nonlinear conjugate gradient (NCG)  limited memory BFGS (L-BFGS)   Direct methods:   Direct multilinear decomposition (DMLD)   Applications of the CPD  Chemometrics  Multi-way data are characterized by several sets of categorical variables that are measured in a crossed fashion. Chemical examples could be fluorescence emission spectra measured at several excitation wavelengths for several samples, fluorescence lifetime measured at several excitation and emission wavelengths or any kind of spectrum measured chromatographically for several samples. Determining such variables will give rise to three-way data; i.e., the data can be arranged in a cube instead of a matrix as in standard multivariate data sets.  Other decompositions  PARAFAC is one of several decomposition methods for multi-way data. The two main competitors are the Tucker3 method, and simply unfolding of the multi-way array to a matrix and then performing standard two-way methods as principal component analysis (PCA). The Tucker3 method should rightfully be called three-mode principal component analysis (or N -mode principal component analysis), but here the term Tucker3 or just Tucker decomposition will be used instead. PARAFAC, Tucker and two-way PCA are all multi- or bi-linear decomposition methods, which decompose the array into sets of "scores" and "loadings", that hopefully describe the data in a more condensed form than the original data array. There are advantages and disadvantages with all the methods, and often several methods must be tried to find the most appropriate.  In the field of chemometrics, a number of diagnostic tools and techniques exist to help a PARAFAC user determine the best fitting model. These include the core consistency diagnostic (CORCONDIA), 19 split-half analyses, 20 examination of the loadings, 21 and residual analysis. 22  See also   Latent class analysis  Multilinear subspace learning  Singular value decomposition  Tucker decomposition   References  Further reading      External links   PARAFAC Tutorial  Parallel Factor Analysis (PARAFAC)  FactoMineR (free exploratory multivariate data analysis software linked to R )   "  Category:Multivariate statistics  Category:Multilinear algebra     â†©  â†©  â†©    â†©  â†©   â†©  â†©  â†©   â†©  â†©  â†©  â†©  â†©   â†©  â†©  â†©      