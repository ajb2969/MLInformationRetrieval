<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1533">Subjective video quality</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Subjective video quality</h1>
<hr/>

<p><strong>Subjective video quality</strong> is <a href="video_quality" title="wikilink">video quality</a> as experienced by humans. It is concerned with how video is perceived by a viewer (also called "observer" or "subject") and designates their opinion on a particular <a class="uri" href="video" title="wikilink">video</a> sequence. The measurement of subjective video quality is necessary since objective algorithms such as <a class="uri" href="PSNR" title="wikilink">PSNR</a> have been shown to correlate badly with ratings. Subjective ratings may also be used as ground truth to develop new algorithms.</p>

<p><strong>Subjective video quality tests</strong> are <a href="Psychophysics" title="wikilink">psychophysical experiments</a> in which a number of viewers rate a given set of stimuli. These tests are quite expensive in terms of time (preparation and running) and human resources and must therefore be carefully designed.</p>

<p>In subjective video quality tests, typically, <strong>SRCs</strong> ("Sources", i.e. original video sequences) are treated with various conditions (<strong>HRCs</strong> for "Hypothetical Reference Circuits") to generate <strong>PVSs</strong> ("Processed Video Sequences").<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="measurement">Measurement</h2>

<p>The main idea of measuring subjective video quality is similar to the <a href="Mean_Opinion_Score" title="wikilink">Mean Opinion Score</a> (MOS) evaluation for <a href="sound" title="wikilink">audio</a>. To evaluate the subjective video quality of a video processing system, the following steps are typically taken:</p>
<ul>
<li>Choose original, unimpaired video sequences for testing</li>
<li>Choose settings of the system that should be evaluated</li>
<li>Apply settings to the SRC, which results in the test sequences</li>
<li>Choose a test method, describing how sequences are presented to viewers and how their opinion is collected</li>
<li>Invite a panel of viewers</li>
<li>Carry out testing in a specific environment (e.g. a laboratory context) and present each PVS in a certain order to every viewer</li>
<li>Calculate rating results for individual PVSs, SRCs and HRCs, e.g. the <a href="Mean_opinion_score" title="wikilink">MOS</a></li>
</ul>

<p>Many parameters of the viewing conditions may influence the results, such as room illumination, display type, brightness, contrast, resolution, viewing distance, and the age and educational level of viewers. It is therefore advised to report this information along with the obtained ratings.</p>
<h3 id="source-selection">Source selection</h3>

<p>Typically, a system should be tested with a representative number of different contents and content characteristics. For example, one may select excerpts from contents of different genres, such as action movies, news shows, and cartoons. The length of the source video depends on the purpose of the test, but typically, sequences of no less than 10 seconds are used.</p>

<p>The amount of motion and spatial detail should also cover a broad range. This ensures that the test contains sequences which are of different complexity.</p>

<p>Sources should be of pristine quality. There should be no visible <a href="Compression_artifact" title="wikilink">coding artifacts</a> or other properties that would lower the quality of the original sequence.</p>
<h3 id="settings">Settings</h3>

<p>The design of the HRCs depends on the system under study. Typically, multiple independent variables are introduced at this stage, and they are varied with a number of levels. For example, to test the quality of a video <a href="Video_codec" title="wikilink">codec</a>, independent variables may be the video encoding software, a target bitrate, and the target resolution of the processed sequence.</p>

<p>It is advised to select settings that result in ratings which cover the full quality range. In other words, assuming an <a href="Absolute_Category_Rating" title="wikilink">Absolute Category Rating</a> scale, the test should show sequences that viewers would rate from bad to excellent.</p>
<h3 id="viewers">Viewers</h3>

<p>Viewers are also called "observers" or "subjects". In order to obtain representative ratings, a certain number of viewers should be invited. This number is not strictly defined. According to ITU-T, any number between 4 and 40 is possible, where 4 is the absolute minimum for statistical reasons, and inviting more than 40 subjects has no added value.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> It is claimed that at minimum 10 subjects are needed to obtain meaningful averaged ratings.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>Viewers should be non-experts in the sense of not being professionals in the field of video coding or related domains. This requirement is introduced to avoid potential subject bias.</p>

<p>Typically, viewers are screened for <a href="normal_vision" title="wikilink">normal vision</a> or corrected-to-normal vision.</p>
<h3 id="test-environment">Test environment</h3>

<p>Subjective quality tests can be done in any environment. However, due to possible influence factors from heterogenous contexts, it is typically advised to perform tests in a neutral environment, such as a dedicated laboratory room. Such a room may be sound-proofed, with walls painted in neutral grey, and using properly calibrated light sources. Several recommendations specify these conditions.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>

<p><a class="uri" href="Crowdsourcing" title="wikilink">Crowdsourcing</a> has recently been used for subjective video quality evaluation, and more generally, in the context of <a href="Quality_of_Experience" title="wikilink">Quality of Experience</a>.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> Here, viewers give ratings using their own computer, at home, rather than taking part in a subjective quality test in laboratory rooms.</p>
<h3 id="analysis-of-results">Analysis of results</h3>

<p>Opinions of viewers are typically averaged into the <a href="Mean_Opinion_Score" title="wikilink">Mean Opinion Score</a> (MOS). To this aim, the labels of categorical scales may be translated into numbers. For example, the responses "bad" to "excellent" can be mapped to the values 1 to 5, and then averaged. MOS values should always be reported with their statistical <a href="confidence_interval" title="wikilink">confidence intervals</a> so that the general agreement between observers can be evaluated.</p>

<p>Often, additional measures are taken before evaluating the results. Subject screening is a process in which viewers whose ratings are considered invalid or unreliable are rejected from further analysis. Invalid ratings are hard to detect, as subjects may have rated without looking at a video, or cheat during the test. The overall reliability of a subject can be determined by various procedures, some of which are outlined in ITU-R and ITU-T recommendations.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a><a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> For example, the correlation between a person's individual scores and the overall MOS, evaluated for all sequences, is a good indicator of their reliability in comparison with the remaining test participants.</p>
<h2 id="standardized-testing-methods">Standardized testing methods</h2>

<p>There are many ways to select proper sequences, system settings, and test methodologies. A few of them have been standardized. They are thoroughly described in several ITU-R and ITU-T recommendations, among those ITU-R BT.500<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> and ITU-T P.910.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> While there is an overlap in certain aspects, the BT.500 recommendation has its roots in broadcasting, whereas P.910 focuses on multimedia content.</p>

<p>A standardized testing method usually describes the following aspects:</p>
<ul>
<li>how long an experiment session lasts</li>
<li>where the experiment takes place</li>
<li>how many times and in which order each PVS should be viewed</li>
<li>whether ratings are taken once per stimulus (e.g. after presentation) or continuously</li>
<li>whether ratings are absolute, i.e. referring to one stimulus only, or relative (comparing two or more stimuli)</li>
<li>which scale ratings are taken on</li>
</ul>

<p>Another recommendation, ITU-T P.913,<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> gives researchers more freedom to conduct subjective quality tests in environments different from a typical testing laboratory, while still requiring them to report all details necessary to make such tests reproducible.</p>
<h3 id="examples">Examples</h3>

<p>Below, some examples of standardized testing procedures are explained.</p>
<h4 id="single-stimulus">Single-Stimulus</h4>
<ul>
<li>'''ACR '''(Absolute Category Rating):<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> each sequence is rated individually on the <em>ACR scale</em>. The labels on the scale are "bad", "poor", "fair", "good", and "excellent", and they are translated to the values 1, 2, 3, 4 and 5 when calculating the MOS.</li>
<li><strong>ACR-HR</strong> (Absolute Category Rating with Hidden Reference): a variation of ACR, in which an original unimpaired source sequence is shown in addition to the impaired sequences, without informing the subjects of its presence (hence, "hidden"). The ratings are calculated as differential scores between the reference and the impaired versions. The differential score is defined as the score of the PVS minus the score given to the hidden reference, plus the number of points on the scale. For example, if a PVS is rated as “poor", and its corresponding hidden reference as “good", then the rating is 

<math display="inline" id="Subjective_video_quality:0">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mn>2</mn>
     <mo>-</mo>
     <mn>4</mn>
    </mrow>
    <mo>+</mo>
    <mn>5</mn>
   </mrow>
   <mo>=</mo>
   <mn>3</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <plus></plus>
     <apply>
      <minus></minus>
      <cn type="integer">2</cn>
      <cn type="integer">4</cn>
     </apply>
     <cn type="integer">5</cn>
    </apply>
    <cn type="integer">3</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   2-4+5=3
  </annotation>
 </semantics>
</math>

. When these ratings are averaged, the result is not a MOS, but a differential MOS ("DMOS").</li>
<li><strong>SSCQE</strong> (Single Stimulus Continuous Quality Rating):<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> a longer sequence is rated continuously over time using a slider device (a variation of a <a href="Fade_(audio_engineering)" title="wikilink">fader</a>), on which subjects rate the current quality. Samples are taken in regular intervals, resulting in a quality curve over time rather than a single quality rating.</li>
</ul>
<h4 id="double-stimulus-or-multiple-stimulus">Double-Stimulus or Multiple Stimulus</h4>
<ul>
<li><strong>DSCQS</strong> (Double Stimulus Continuous Quality Scale):<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> the viewer sees an unimpaired reference and the impaired sequence in a random order. They are allowed to re-view the sequences, and then rate the quality for both on a continuous scale labeled with the ACR categories.</li>
<li><strong>DSIS</strong> (Double Stimulus Impairment Scale)<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> and <strong>DCR</strong> (Degradation Category Rating):<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> both refer to the same method. The viewer sees an unimpaired reference video, then the same video impaired, and after that they are asked to vote on the second video using a so-called <em>impairment scale</em> (from "impairments are imperceptible" to "impairments are very annoying").</li>
<li><strong>PC</strong> (Pair Comparison):<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> instead of comparing an unimpaired and impaired sequence, different impairment types (HRCs) are compared. All possible combinations of HRCs should be evaluated.</li>
</ul>
<h3 id="choice-of-methodology">Choice of methodology</h3>

<p>Which method to choose largely depends on the purpose of the test and possible constraints in time and other resources. Some methods may have fewer context effects (i.e. where the order of stimuli influences the results), which are unwanted test biases.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> In ITU-T P.910, it is noted that methods such as DCR should be used for testing the fidelity of transmission, especially in high quality systems. ACR and ACR-HR are better suited for qualification tests and – due to giving absolute results – comparison of systems. The PC method has a high discriminatory power, but it requires longer test sessions.</p>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.its.bldrdoc.gov/vqeg/">Video Quality Experts Group</a></li>
</ul>
<h2 id="references">References</h2>
<references>
</references>

<p>"</p>

<p><a href="Category:Film_and_video_technology" title="wikilink">Category:Film and video technology</a> <a href="Category:Digital_television" title="wikilink">Category:Digital television</a> <a href="Category:Video_codecs" title="wikilink">Category:Video codecs</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="http://www.itu.int/ITU-T/studygroups/com09/docs/tutorial_opavc.pdf">ITU-T Tutorial: Objective perceptual assessment of video quality: Full reference television</a>, 2004.<a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3">Winkler, Stefan. [<a class="uri" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.3958&amp;rep">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.3958&amp;rep;</a>;=rep1&amp;type;=pdf "On the properties of subjective''' '''ratings in video quality experiments"]. <em>Proc.</em> <em>Quality of Multimedia Experience</em>, 2009.<a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="https://www.itu.int/rec/T-REC-P.910/en">ITU-T Rec. P.910 : Subjective video quality assessment methods for multimedia applications</a>, 2008.<a href="#fnref7">↩</a></li>
<li id="fn8"><a href="http://www.itu.int/rec/R-REC-BT.500/en">ITU-R BT.500: Methodology for the subjective assessment of the quality of television pictures</a>, 2012.<a href="#fnref8">↩</a></li>
<li id="fn9"></li>
<li id="fn10"></li>
<li id="fn11"><a href="http://www.itu.int/rec/T-REC-P.913/en">ITU-T P.913: Methods for the subjective assessment of video quality, audio quality and audiovisual quality of Internet video and distribution quality television in any environment</a>, 2014.<a href="#fnref11">↩</a></li>
<li id="fn12"></li>
<li id="fn13"></li>
<li id="fn14"></li>
<li id="fn15"></li>
<li id="fn16"></li>
<li id="fn17"></li>
<li id="fn18">Pinson, Margaret and Wolf, Stephen. <a href="http://www.its.bldrdoc.gov/publications/2577.aspx">"Comparing Subjective Video Quality Testing Methodologies"</a>. <em>SPIE Video Communications and Image Processing Conference</em>, Lugano, Switzerland, July 2003.<a href="#fnref18">↩</a></li>
</ol>
</section>
</body>
</html>
