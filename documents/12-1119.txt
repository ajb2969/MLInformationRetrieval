   Smoothing spline      Smoothing spline   The smoothing spline is a method of smoothing (fitting a smooth curve to a set of noisy observations ) using a spline function.  Definition  Let    μ  ^     normal-^  μ    \hat{\mu}   . The smoothing spline estimate   μ   μ   \mu   of the function       ∑   i  =  1   n     (    Y  i   -    μ  ^    (   x  i   )     )   2    +   λ    ∫   x  1    x  n       μ  ^   ′′      (  x  )   2    d  x      .        superscript   subscript     i  1    n    superscript     subscript  Y  i      normal-^  μ    subscript  x  i     2      λ    superscript   subscript    subscript  x  1     subscript  x  n       superscript   normal-^  μ   ′′    superscript  x  2   d  x       \sum_{i=1}^{n}(Y_{i}-\hat{\mu}(x_{i}))^{2}+\lambda\int_{x_{1}}^{x_{n}}\hat{\mu%
 }^{\prime\prime}(x)^{2}\,dx.   is defined to be the minimizer (over the class of twice differentiable functions) of 1      λ  ≥  0      λ  0    \lambda\geq 0     Remarks:       x  i     subscript  x  i    x_{i}   is a smoothing parameter, controlling the trade-off between fidelity to the data and roughness of the function estimate.  The integral is evaluated over the range of the    λ  →  0     normal-→  λ  0    \lambda\to 0   .  As    λ  →  ∞     normal-→  λ     \lambda\to\infty   (no smoothing), the smoothing spline converges to the interpolating spline .  As    x  i     subscript  x  i    x_{i}   (infinite smoothing), the roughness penalty becomes paramount and the estimate converges to a linear least squares estimate.  The roughness penalty based on the second derivative is the most common in modern statistics literature, although the method can easily be adapted to penalties based on other derivatives.  In early literature, with equally-spaced        μ  ^    (   x  i   )    ;  i   =  1   ,   …  ,  n      formulae-sequence        normal-^  μ    subscript  x  i    i   1    normal-…  n     \hat{\mu}(x_{i});i=1,\ldots,n   , second or third-order differences were used in the penalty, rather than derivatives.  When the sum-of-squares term is replaced by a log-likelihood, the resulting estimate is termed penalized likelihood . The smoothing spline is the special case of penalized likelihood resulting from a Gaussian likelihood.   Derivation of the smoothing spline  It is useful to think of fitting a smoothing spline in two steps:   First, derive the values     μ  ^    (  x  )        normal-^  μ   x    \hat{\mu}(x)   .  From these values, derive     m  ^   =    (    μ  ^    (   x  1   )    ,  …  ,    μ  ^    (   x  n   )    )   T        normal-^  m    superscript      normal-^  μ    subscript  x  1    normal-…     normal-^  μ    subscript  x  n     T     \hat{m}=(\hat{\mu}(x_{1}),\ldots,\hat{\mu}(x_{n}))^{T}   for all x .   Now, treat the second step first.  Given the vector    ∫     μ  ^   ′′      (  x  )   2    d  x          superscript   normal-^  μ   ′′    superscript  x  2   d  x     \int\hat{\mu}^{\prime\prime}(x)^{2}\,dx   of fitted values, the sum-of-squares part of the spline criterion is fixed. It remains only to minimize    (   x  i   ,    μ  ^    (   x  i   )    )      subscript  x  i      normal-^  μ    subscript  x  i      (x_{i},\hat{\mu}(x_{i}))   , and the minimizer is a natural cubic spline that interpolates the points      μ  ^    (  x  )    =    ∑   i  =  1   n     μ  ^    (   x  i   )    f  i    (  x  )            normal-^  μ   x     superscript   subscript     i  1    n      normal-^  μ    subscript  x  i    subscript  f  i   x      \hat{\mu}(x)=\sum_{i=1}^{n}\hat{\mu}(x_{i})f_{i}(x)   . This interpolating spline is a linear operator, and can be written in the form       f  i    (  x  )        subscript  f  i   x    f_{i}(x)   where      ∫     μ  ^   ′′     (  x  )   2   d  x    =     m  ^   T   A   m  ^     .           superscript   normal-^  μ   ′′    superscript  x  2   d  x       superscript   normal-^  m   T   A   normal-^  m      \int\hat{\mu}^{\prime\prime}(x)^{2}dx=\hat{m}^{T}A\hat{m}.   are a set of spline basis functions. As a result, the roughness penalty has the form      ∫    f  i  ′′    (  x  )    f  j  ′′    (  x  )   d  x          superscript   subscript  f  i   ′′   x   superscript   subscript  f  j   ′′   x  d  x     \int f_{i}^{\prime\prime}(x)f_{j}^{\prime\prime}(x)dx   where the elements of A are    x  i     subscript  x  i    x_{i}   . The basis functions, and hence the matrix A , depend on the configuration of the predictor variables    Y  i     subscript  Y  i    Y_{i}   , but not on the responses    m  ^     normal-^  m    \hat{m}   or       ∥   Y  -   m  ^    ∥   2   +   λ    m  ^   T   A   m  ^     ,       superscript   norm    Y   normal-^  m     2     λ   superscript   normal-^  m   T   A   normal-^  m      \|Y-\hat{m}\|^{2}+\lambda\hat{m}^{T}A\hat{m},   .  Now back to the first step. The penalized sum-of-squares can be written as      Y  =    (   Y  1   ,  …  ,   Y  n   )   T       Y   superscript    subscript  Y  1   normal-…   subscript  Y  n    T     Y=(Y_{1},\ldots,Y_{n})^{T}   where    m  ^     normal-^  m    \hat{m}   . Minimizing over      m  ^   =     (   I  +   λ  A    )    -  1    Y    .       normal-^  m      superscript    I    λ  A      1    Y     \hat{m}=(I+\lambda A)^{-1}Y.   gives       p    ∑   i  =  1   n     (     Y  i   -    μ  ^    (   x  i   )      δ  i    )   2     +    (   1  -  p   )    ∫      (     μ  ^    (  m  )     (  x  )    )   2    d  x           p    superscript   subscript     i  1    n    superscript       subscript  Y  i      normal-^  μ    subscript  x  i      subscript  δ  i    2         1  p        superscript     superscript   normal-^  μ   m   x   2   d  x       p\sum_{i=1}^{n}\left(\frac{Y_{i}-\hat{\mu}\left(x_{i}\right)}{\delta_{i}}%
 \right)^{2}+\left(1-p\right)\int\left(\hat{\mu}^{\left(m\right)}\left(x\right)%
 \right)^{2}\,dx     De Boor's approach  De Boor's approach exploits the same idea, of finding a balance between having a smooth curve and being close to the given data. 2    p   p   p     where    [  0  ,  1  ]     0  1    [0,1]   is a parameter called smooth factor and belongs to the interval       δ  i   ;  i   =  1   ,   …  ,  n      formulae-sequence      subscript  δ  i   i   1    normal-…  n     \delta_{i};i=1,\dots,n   , and    δ  i   -  2      superscript   subscript  δ  i     2     \delta_{i}^{-2}   are the quantities controlling the extent of smoothing (they represent the weight    Y  i     subscript  Y  i    Y_{i}   of each point   m   m   m   ). In practice, since cubic splines are mostly used,   2   2   2   is usually    m  =  2      m  2    m=2   . The solution for    m  =  2      m  2    m=2   was proposed by Reinsch in 1967. 3 For   p   p   p   , when   1   1   1   approaches    μ  ^     normal-^  μ    \hat{\mu}   ,   p   p   p   converges to the "natural" spline interpolant to the given data. 4 As   0   0    approaches    μ  ^     normal-^  μ    \hat{\mu}   ,   p   p   p   converges to a straight line (the smoothest curve). Since finding a suitable value of   S   S   S   is a task of trial and error, a redundant constant   S   S   S   was introduced for convenience. 5    p   p   p   is used to numerically determine the value of    μ  ^     normal-^  μ    \hat{\mu}   so that the function      ∑   i  =  1   n     (     Y  i   -    μ  ^    (   x  i   )      δ  i    )   2    ≤  S        superscript   subscript     i  1    n    superscript       subscript  Y  i      normal-^  μ    subscript  x  i      subscript  δ  i    2    S    \sum_{i=1}^{n}\left(\frac{Y_{i}-\hat{\mu}\left(x_{i}\right)}{\delta_{i}}\right%
 )^{2}\leq S   meets the following condition:      p  =  0      p  0    p=0     The algorithm described by de Boor starts with   p   p   p   and increases    δ  i     subscript  δ  i    \delta_{i}   until the condition is met. 6 If    Y  i     subscript  Y  i    Y_{i}   is an estimation of the standard deviation for   S   S   S   , the constant    [   n  -    2  n     ,   n  +    2  n     ]       n      2  n       n      2  n       \left[n-\sqrt{2n},n+\sqrt{2n}\right]   is recommended to be chosen in the interval    S  =  0      S  0    S=0   . Having   S   S   S   means the solution is the "natural" spline interpolant. 7 Increasing   y   y   y   means we obtain a smoother curve by getting farther from the given data.  Creating a multidimensional spline  Given the constraint from the definition formula    x   (  t  )       x  t    x(t)   and    y   (  t  )       y  t    y(t)   so that they would become     t   i  +  1    =    t  i   +      (    x   i  +  1    -   x  i    )   2   +    (    y   i  +  1    -   y  i    )   2           subscript  t    i  1       subscript  t  i        superscript     subscript  x    i  1     subscript  x  i    2    superscript     subscript  y    i  1     subscript  y  i    2        t_{i+1}=t_{i}+\sqrt{(x_{i+1}-x_{i})^{2}+(y_{i+1}-y_{i})^{2}}   and     t  1   =  0       subscript  t  1   0    t_{1}=0   where    μ  ^     normal-^  μ    \hat{\mu}   is the cumulating distance   μ   μ   \mu   where       ∑   i  =  1   n     (    Y  i   -    μ  ^    (   x  i   )     )   2    +   λ    ∫   x  1    x  n       μ  ^   ′′      (  x  )   2    d  x      .        superscript   subscript     i  1    n    superscript     subscript  Y  i      normal-^  μ    subscript  x  i     2      λ    superscript   subscript    subscript  x  1     subscript  x  n       superscript   normal-^  μ   ′′    superscript  x  2   d  x       \sum_{i=1}^{n}(Y_{i}-\hat{\mu}(x_{i}))^{2}+\lambda\int_{x_{1}}^{x_{n}}\hat{\mu%
 }^{\prime\prime}(x)^{2}\,dx.   . 8 9  A more detailed analysis on parametrization is done by E.T.Y Lee. 10  Related methods  Smoothing splines are related to, but distinct from:   Regression splines. In this method, the data is fitted to a set of spline basis functions with a reduced set of knots, typically by least squares. No roughness penalty is used.  Penalized Splines. This combines the reduced knots of regression splines, with the roughness penalty of smoothing splines. 11  Elastic maps method for manifold learning . This method combines the least squares penalty for approximation error with the bending and stretching penalty of the approximating manifold and uses the coarse discretization of the optimization problem.   Source code  Source code for spline smoothing can be found in the examples from Carl de Boor's book A Practical Guide to Splines . The examples are in Fortran  programming language . The updated sources are available also on Carl de Boor's official site 1 .  Further reading   Wahba, G. (1990). Spline Models for Observational Data . SIAM, Philadelphia.  Green, P. J. and Silverman, B. W. (1994). Nonparametric Regression and Generalized Linear Models . CRC Press.  De Boor, C. (2001). A Practical Guide to Splines (Revised Edition) . Springer.   References  "  Category:Regression analysis  Category:Splines  Category:Statistical methods     ↩  ↩    ↩    ↩  ↩  ↩  ↩     