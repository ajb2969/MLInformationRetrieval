   Smooth maximum      Smooth maximum   In mathematics , a smooth maximum of an indexed family x 1 , ..., x n of numbers is a differentiable approximation to the maximum function        {   x  1   ,  …  ,   x  n   }   ↦   max   {   x  1   ,  …  ,   x  n   }     ,     maps-to    subscript  x  1   normal-…   subscript  x  n       subscript  x  1   normal-…   subscript  x  n      \{x_{1},\ldots,x_{n}\}\mapsto\max\{x_{1},\ldots,x_{n}\},\,     and the concept of smooth minimum is similarly defined.  For large positive values of the parameter    α  >  0      α  0    \alpha>0   , the following formulation is one smooth, differentiable approximation of the maximum function. For negative values of the parameter that are large in absolute value, it approximates the minimum.        𝒮  α    (    {   x  i   }    i  =  1   n   )    =     ∑   i  =  1   n     x  i    e   α   x  i         ∑   i  =  1   n    e   α   x  i              subscript  𝒮  α    superscript   subscript    subscript  x  i      i  1    n        superscript   subscript     i  1    n      subscript  x  i    superscript  e    α   subscript  x  i         superscript   subscript     i  1    n    superscript  e    α   subscript  x  i         \mathcal{S}_{\alpha}(\{x_{i}\}_{i=1}^{n})=\frac{\sum_{i=1}^{n}x_{i}e^{\alpha x%
 _{i}}}{\sum_{i=1}^{n}e^{\alpha x_{i}}}       𝒮  α     subscript  𝒮  α    \mathcal{S}_{\alpha}   has the following properties:        𝒮  α   →  max     normal-→   subscript  𝒮  α      \mathcal{S}_{\alpha}\to\max   as    α  →  ∞     normal-→  α     \alpha\to\infty         𝒮  0     subscript  𝒮  0    \mathcal{S}_{0}   is the average of its inputs       𝒮  α   →  min     normal-→   subscript  𝒮  α      \mathcal{S}_{\alpha}\to\min   as    α  →   -  ∞      normal-→  α        \alpha\to-\infty      The gradient of    𝒮  α     subscript  𝒮  α    \mathcal{S}_{\alpha}   is closely related to softmax and is given by          ∇   x  i     𝒮  α     (    {   x  i   }    i  =  1   n   )    =     e   α   x  i       ∑   j  =  1   n    e   α   x  j        [   1  +   α   (    x  i   -    𝒮  α    (    {   x  i   }    i  =  1   n   )     )     ]     .          subscript  normal-∇   subscript  x  i     subscript  𝒮  α     superscript   subscript    subscript  x  i      i  1    n         superscript  e    α   subscript  x  i       superscript   subscript     j  1    n    superscript  e    α   subscript  x  j        delimited-[]    1    α     subscript  x  i      subscript  𝒮  α    superscript   subscript    subscript  x  i      i  1    n           \nabla_{x_{i}}\mathcal{S}_{\alpha}(\{x_{i}\}_{i=1}^{n})=\frac{e^{\alpha x_{i}}%
 }{\sum_{j=1}^{n}e^{\alpha x_{j}}}[1+\alpha(x_{i}-\mathcal{S}_{\alpha}(\{x_{i}%
 \}_{i=1}^{n}))].     This makes the softmax function useful for optimization techniques that use gradient descent .  Another formulation is:       g   (   x  1   ,  …  ,   x  n   )    =   log   (    exp   (   x  1   )    +  …  +   exp   (   x  n   )     )          g    subscript  x  1   normal-…   subscript  x  n            subscript  x  1    normal-…     subscript  x  n        g(x_{1},\ldots,x_{n})=\log(\exp(x_{1})+\ldots+\exp(x_{n}))     Smooth minimum  Use in numerical methods  Other choices of smoothing function  References  "  Category:Mathematical notation  Category:Basic concepts in set theory   