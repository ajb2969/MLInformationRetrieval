   Smooth maximum      Smooth maximum   In mathematics , a smooth maximum of an indexed family x 1 ,Â ..., x n of numbers is a differentiable approximation to the maximum function        {   x  1   ,  â€¦  ,   x  n   }   â†¦   max   {   x  1   ,  â€¦  ,   x  n   }     ,     maps-to    subscript  x  1   normal-â€¦   subscript  x  n       subscript  x  1   normal-â€¦   subscript  x  n      \{x_{1},\ldots,x_{n}\}\mapsto\max\{x_{1},\ldots,x_{n}\},\,     and the concept of smooth minimum is similarly defined.  For large positive values of the parameter    Î±  >  0      Î±  0    \alpha>0   , the following formulation is one smooth, differentiable approximation of the maximum function. For negative values of the parameter that are large in absolute value, it approximates the minimum.        ğ’®  Î±    (    {   x  i   }    i  =  1   n   )    =     âˆ‘   i  =  1   n     x  i    e   Î±   x  i         âˆ‘   i  =  1   n    e   Î±   x  i              subscript  ğ’®  Î±    superscript   subscript    subscript  x  i      i  1    n        superscript   subscript     i  1    n      subscript  x  i    superscript  e    Î±   subscript  x  i         superscript   subscript     i  1    n    superscript  e    Î±   subscript  x  i         \mathcal{S}_{\alpha}(\{x_{i}\}_{i=1}^{n})=\frac{\sum_{i=1}^{n}x_{i}e^{\alpha x%
 _{i}}}{\sum_{i=1}^{n}e^{\alpha x_{i}}}       ğ’®  Î±     subscript  ğ’®  Î±    \mathcal{S}_{\alpha}   has the following properties:        ğ’®  Î±   â†’  max     normal-â†’   subscript  ğ’®  Î±      \mathcal{S}_{\alpha}\to\max   as    Î±  â†’  âˆ     normal-â†’  Î±     \alpha\to\infty         ğ’®  0     subscript  ğ’®  0    \mathcal{S}_{0}   is the average of its inputs       ğ’®  Î±   â†’  min     normal-â†’   subscript  ğ’®  Î±      \mathcal{S}_{\alpha}\to\min   as    Î±  â†’   -  âˆ      normal-â†’  Î±        \alpha\to-\infty      The gradient of    ğ’®  Î±     subscript  ğ’®  Î±    \mathcal{S}_{\alpha}   is closely related to softmax and is given by          âˆ‡   x  i     ğ’®  Î±     (    {   x  i   }    i  =  1   n   )    =     e   Î±   x  i       âˆ‘   j  =  1   n    e   Î±   x  j        [   1  +   Î±   (    x  i   -    ğ’®  Î±    (    {   x  i   }    i  =  1   n   )     )     ]     .          subscript  normal-âˆ‡   subscript  x  i     subscript  ğ’®  Î±     superscript   subscript    subscript  x  i      i  1    n         superscript  e    Î±   subscript  x  i       superscript   subscript     j  1    n    superscript  e    Î±   subscript  x  j        delimited-[]    1    Î±     subscript  x  i      subscript  ğ’®  Î±    superscript   subscript    subscript  x  i      i  1    n           \nabla_{x_{i}}\mathcal{S}_{\alpha}(\{x_{i}\}_{i=1}^{n})=\frac{e^{\alpha x_{i}}%
 }{\sum_{j=1}^{n}e^{\alpha x_{j}}}[1+\alpha(x_{i}-\mathcal{S}_{\alpha}(\{x_{i}%
 \}_{i=1}^{n}))].     This makes the softmax function useful for optimization techniques that use gradient descent .  Another formulation is:       g   (   x  1   ,  â€¦  ,   x  n   )    =   log   (    exp   (   x  1   )    +  â€¦  +   exp   (   x  n   )     )          g    subscript  x  1   normal-â€¦   subscript  x  n            subscript  x  1    normal-â€¦     subscript  x  n        g(x_{1},\ldots,x_{n})=\log(\exp(x_{1})+\ldots+\exp(x_{n}))     Smooth minimum  Use in numerical methods  Other choices of smoothing function  References  "  Category:Mathematical notation  Category:Basic concepts in set theory   