   Noisy-channel coding theorem      Noisy-channel coding theorem   In information theory , the noisy-channel coding theorem (sometimes Shannon's theorem ), establishes that for any given degree of noise contamination of a communication channel , it is possible to communicate discrete data (digital information ) nearly error-free up to a computable maximum rate through the channel. This result was presented by Claude Shannon in 1948 and was based in part on earlier work and ideas of Harry Nyquist and Ralph Hartley .  The Shannon limit or Shannon capacity of a communications channel is the theoretical maximum information transfer rate of the channel, for a particular noise level.  Overview  Stated by Claude Shannon in 1948, the theorem describes the maximum possible efficiency of error-correcting methods versus levels of noise interference and data corruption. Shannon's theorem has wide-ranging applications in both communications and data storage . This theorem is of foundational importance to the modern field of information theory . Shannon only gave an outline of the proof. The first rigorous proof for the discrete case is due to Amiel Feinstein in 1954.  The Shannon theorem states that given a noisy channel with channel capacity  C and information transmitted at a rate R , then if    R  <  C      R  C    R   there exist codes that allow the probability of error at the receiver to be made arbitrarily small. This means that, theoretically, it is possible to transmit information nearly without error at any rate below a limiting rate, C .  The converse is also important. If    R  >  C      R  C    R>C   , an arbitrarily small probability of error is not achievable. All codes will have a probability of error greater than a certain positive minimal level, and this level increases as the rate increases. So, information cannot be guaranteed to be transmitted reliably across a channel at rates beyond the channel capacity. The theorem does not address the rare situation in which rate and capacity are equal.  The channel capacity C can be calculated from the physical properties of a channel; for a band-limited channel with Gaussian noise, using the Shannon–Hartley theorem .  Simple schemes such as "send the message 3 times and use a best 2 out of 3 voting scheme if the copies differ" are inefficient error-correction methods, unable to asymptotically guarantee that a block of data can be communicated free of error. Advanced techniques such as Reed–Solomon codes and, more recently, low-density parity-check (LDPC) codes and turbo codes , come much closer to reaching the theoretical Shannon limit, but at a cost of high computational complexity. Using these highly efficient codes and with the computing power in today's digital signal processors , it is now possible to reach very close to the Shannon limit. In fact, it was shown that LDPC codes can reach within 0.0045 dB of the Shannon limit (for binary AWGN channels, with very long block lengths). 1  Mathematical statement  (Figure)  Comm Channel.svg   Theorem (Shannon, 1948):   1. For every discrete memoryless channel, the channel capacity           C   =    sup   p  X     I   (  X  ;  Y  )         C    subscript  supremum   subscript  p  X      I   X  Y       \ C=\sup_{p_{X}}I(X;Y)\,    2       has the following property. For any ε > 0 and R < C , for large enough N , there exists a code of length N and rate ≥ R and a decoding algorithm, such that the maximal probability of block error is ≤ ε.    2. If a probability of bit error p b is acceptable, rates up to R(p b ) are achievable, where           R   (   p  b   )    =   C   1  -    H  2    (   p  b   )       .        R   subscript  p  b      C    1     subscript  H  2    subscript  p  b        R(p_{b})=\frac{C}{1-H_{2}(p_{b})}.         and     H  2    (   p  b   )        subscript  H  2    subscript  p  b     H_{2}(p_{b})   is the binary entropy function            H  2    (   p  b   )    =   -   [     p  b     log  2    p  b     +    (   1  -   p  b    )     log  2    (   1  -   p  b    )      ]           subscript  H  2    subscript  p  b       delimited-[]       subscript  p  b     subscript   2    subscript  p  b         1   subscript  p  b      subscript   2     1   subscript  p  b           H_{2}(p_{b})=-\left[p_{b}\log_{2}{p_{b}}+(1-p_{b})\log_{2}({1-p_{b}})\right]         3. For any p b , rates greater than R ( p b ) are not achievable.   (MacKay (2003), p. 162; cf Gallager (1968), ch.5; Cover and Thomas (1991), p. 198; Shannon (1948) thm. 11)  Outline of proof  As with several other major results in information theory, the proof of the noisy channel coding theorem includes an achievability result and a matching converse result. These two components serve to bound, in this case, the set of possible rates at which one can communicate over a noisy channel, and matching serves to show that these bounds are tight bounds.  The following outlines are only one set of many different styles available for study in information theory texts.  Achievability for discrete memoryless channels  This particular proof of achievability follows the style of proofs that make use of the asymptotic equipartition property (AEP). Another style can be found in information theory texts using error exponents .  Both types of proofs make use of a random coding argument where the codebook used across a channel is randomly constructed - this serves to make the analysis simpler while still proving the existence of a code satisfying a desired low probability of error at any data rate below the channel capacity .  By an AEP-related argument, given a channel, length   n   n   n   strings of source symbols    X  1  n     superscript   subscript  X  1   n    X_{1}^{n}   , and length   n   n   n   strings of channel outputs    Y  1  n     superscript   subscript  Y  1   n    Y_{1}^{n}   , we can define a jointly typical set by the following:       A  ε   (  n  )    =   {   (   x  n   ,   y  n   )   ∈   𝒳  n   ×   𝒴  n       fragments   superscript   subscript  A  ε   n     fragments  normal-{   fragments  normal-(   superscript  x  n   normal-,   superscript  y  n   normal-)     superscript  𝒳  n     superscript  𝒴  n      A_{\varepsilon}^{(n)}=\{(x^{n},y^{n})\in\mathcal{X}^{n}\times\mathcal{Y}^{n}            2   -   n   (    H   (  X  )    +  ε   )      ≤   p   (   X  1  n   )    ≤   2   -   n   (    H   (  X  )    -  ε   )             superscript  2      n      H  X   ε        p   superscript   subscript  X  1   n          superscript  2      n      H  X   ε         2^{-n(H(X)+\varepsilon)}\leq p(X_{1}^{n})\leq 2^{-n(H(X)-\varepsilon)}               2   -   n   (    H   (  Y  )    +  ε   )      ≤   p   (   Y  1  n   )    ≤   2   -   n   (    H   (  Y  )    -  ε   )             superscript  2      n      H  Y   ε        p   superscript   subscript  Y  1   n          superscript  2      n      H  Y   ε         2^{-n(H(Y)+\varepsilon)}\leq p(Y_{1}^{n})\leq 2^{-n(H(Y)-\varepsilon)}               2   -   n   (    H   (  X  ,  Y  )    +  ε   )      ≤  p   (   X  1  n   ,   Y  1  n   )   ≤   2   -   n   (    H   (  X  ,  Y  )    -  ε   )      }     fragments   superscript  2      n      H   X  Y    ε       p   fragments  normal-(   superscript   subscript  X  1   n   normal-,   superscript   subscript  Y  1   n   normal-)     superscript  2      n      H   X  Y    ε      normal-}    {2^{-n(H(X,Y)+\varepsilon)}}\leq p(X_{1}^{n},Y_{1}^{n})\leq 2^{-n(H(X,Y)-%
 \varepsilon)}\}        We say that two sequences    X  1  n     superscript   subscript  X  1   n    {X_{1}^{n}}   and    Y  1  n     superscript   subscript  Y  1   n    Y_{1}^{n}   are jointly typical if they lie in the jointly typical set defined above.  Steps   In the style of the random coding argument, we randomly generate    2   n  R      superscript  2    n  R     2^{nR}   codewords of length n from a probability distribution Q.  This code is revealed to the sender and receiver. It is also assumed that one knows the transition matrix    p   (  y  |  x  )      fragments  p   fragments  normal-(  y  normal-|  x  normal-)     p(y|x)   for the channel being used.  A message W is chosen according to the uniform distribution on the set of codewords. That is,    P  r   (  W  =  w  )   =   2   -   n  R     ,  w  =  1  ,  2  ,  …  ,   2   n  R       fragments  P  r   fragments  normal-(  W   w  normal-)     superscript  2      n  R     normal-,  w   1  normal-,  2  normal-,  normal-…  normal-,   superscript  2    n  R      Pr(W=w)=2^{-nR},w=1,2,\dots,2^{nR}   .  The message W is sent across the channel.  The receiver receives a sequence according to    P   (   y  n   |   x  n    (  w  )   )   =   ∏   i  =  1   n   p   (   y  i   |   x  i    (  w  )   )      fragments  P   fragments  normal-(   superscript  y  n   normal-|   superscript  x  n    fragments  normal-(  w  normal-)   normal-)     superscript   subscript  product    i  1    n   p   fragments  normal-(   subscript  y  i   normal-|   subscript  x  i    fragments  normal-(  w  normal-)   normal-)     P(y^{n}|x^{n}(w))=\prod_{i=1}^{n}p(y_{i}|x_{i}(w))     Sending these codewords across the channel, we receive    Y  1  n     superscript   subscript  Y  1   n    Y_{1}^{n}   , and decode to some source sequence if there exists exactly 1 codeword that is jointly typical with Y. If there are no jointly typical codewords, or if there are more than one, an error is declared. An error also occurs if a decoded codeword doesn't match the original codeword. This is called typical set decoding .   The probability of error of this scheme is divided into two parts:   First, error can occur if no jointly typical X sequences are found for a received Y sequence  Second, error can occur if an incorrect X sequence is jointly typical with a received Y sequence.    By the randomness of the code construction, we can assume that the average probability of error averaged over all codes does not depend on the index sent. Thus, without loss of generality, we can assume W = 1.    From the joint AEP, we know that the probability that no jointly typical X exists goes to 0 as n grows large. We can bound this error probability by   ε   ε   \varepsilon   .    Also from the joint AEP, we know the probability that a particular     X  1  n    (  i  )        superscript   subscript  X  1   n   i    X_{1}^{n}(i)   and the    Y  1  n     superscript   subscript  Y  1   n    Y_{1}^{n}   resulting from W = 1 are jointly typical is     ≤   2   -   n   (    I   (  X  ;  Y  )    -   3  ε    )          absent   superscript  2      n      I   X  Y      3  ε         \leq 2^{-n(I(X;Y)-3\varepsilon)}   .   Define     E  i   =   {   (   X  1  n    (  i  )   ,   Y  1  n   )   ∈   A  ε   (  n  )    }   ,  i  =  1  ,  2  ,  …  ,   2   n  R       fragments   subscript  E  i     fragments  normal-{   fragments  normal-(   superscript   subscript  X  1   n    fragments  normal-(  i  normal-)   normal-,   superscript   subscript  Y  1   n   normal-)     superscript   subscript  A  ε   n   normal-}   normal-,  i   1  normal-,  2  normal-,  normal-…  normal-,   superscript  2    n  R      E_{i}=\{(X_{1}^{n}(i),Y_{1}^{n})\in A_{\varepsilon}^{(n)}\},i=1,2,\dots,2^{nR}     as the event that message i is jointly typical with the sequence received when message 1 is sent.      P   (  error  )       P  error    \displaystyle P(\text{error})     We can observe that as   n   n   n   goes to infinity, if    R  <   I   (  X  ;  Y  )        R    I   X  Y      R   for the channel, the probability of error will go to 0.  Finally, given that the average codebook is shown to be "good," we know that there exists a codebook whose performance is better than the average, and so satisfies our need for arbitrarily low error probability communicating across the noisy channel.  Weak converse for discrete memoryless channels  Suppose a code of    2   n  R      superscript  2    n  R     2^{nR}   codewords. Let W be drawn uniformly over this set as an index. Let    X  n     superscript  X  n    X^{n}   and    Y  n     superscript  Y  n    Y^{n}   be the codewords and received codewords, respectively.       n  R  =  H   (  W  )   =  H   (  W  |   Y  n   )   +  I   (  W  ;   Y  n   )      fragments  n  R   H   fragments  normal-(  W  normal-)    H   fragments  normal-(  W  normal-|   superscript  Y  n   normal-)    I   fragments  normal-(  W  normal-;   superscript  Y  n   normal-)     nR=H(W)=H(W|Y^{n})+I(W;Y^{n})\;   using identities involving entropy and mutual information      ≤  H   (  W  |   Y  n   )   +  I   (   X  n    (  W  )   ;   Y  n   )      fragments   H   fragments  normal-(  W  normal-|   superscript  Y  n   normal-)    I   fragments  normal-(   superscript  X  n    fragments  normal-(  W  normal-)   normal-;   superscript  Y  n   normal-)     \leq H(W|Y^{n})+I(X^{n}(W);Y^{n})   since X is a function of W       ≤   1  +    P  e   (  n  )    n  R   +   I   (    X  n    (  W  )    ;   Y  n   )         absent    1     superscript   subscript  P  e   n   n  R     I      superscript  X  n   W    superscript  Y  n        \leq 1+P_{e}^{(n)}nR+I(X^{n}(W);Y^{n})   by the use of Fano's Inequality       ≤   1  +    P  e   (  n  )    n  R   +   n  C        absent    1     superscript   subscript  P  e   n   n  R     n  C      \leq 1+P_{e}^{(n)}nR+nC   by the fact that capacity is maximized mutual information.   The result of these steps is that     P  e   (  n  )    ≥   1  -   1   n  R    -   C  R         superscript   subscript  P  e   n     1    1    n  R      C  R      P_{e}^{(n)}\geq 1-\frac{1}{nR}-\frac{C}{R}   . As the block length   n   n   n   goes to infinity, we obtain    P  e   (  n  )      superscript   subscript  P  e   n    P_{e}^{(n)}   is bounded away from 0 if R is greater than C - we can get arbitrarily low rates of error only if R is less than C.  Strong converse for discrete memoryless channels  A strong converse theorem, proven by Wolfowitz in 1957, 3 states that,       P  e   ≥   1  -    4  A    n    (   R  -  C   )   2     -   e   -    n   (   R  -  C   )    2           subscript  P  e     1      4  A     n   superscript    R  C   2      superscript  e        n    R  C    2        P_{e}\geq 1-\frac{4A}{n(R-C)^{2}}-e^{-\frac{n(R-C)}{2}}     for some finite positive constant   A   A   A   . While the weak converse states that the error probability is bounded away from zero as   n   n   n   goes to infinity, the strong converse states that the error goes to 1. Thus,   C   C   C   is a sharp threshold between perfectly reliable and completely unreliable communication.  Channel coding theorem for non-stationary memoryless channels  We assume that the channel is memoryless, but its transition probabilities change with time, in a fashion known at the transmitter as well as the receiver.  Then the channel capacity is given by       C  =   lim   inf     max     p  (    X  1   )   ,   p  (    X  2   )  ,  …     1  n      ∑   i  =  1   n    I   (   X  i   ;   Y  i   )         .      C     infimum      subscript    fragments   fragments   superscript  p  normal-(    subscript  X  1   normal-)   normal-,   superscript  p  normal-(    subscript  X  2   normal-)  normal-,  normal-…      1  n      superscript   subscript     i  1    n     I    subscript  X  i    subscript  Y  i           C=\lim\inf\max_{p^{(}X_{1}),p^{(}X_{2}),...}\frac{1}{n}\sum_{i=1}^{n}I(X_{i};Y%
 _{i}).     The maximum is attained at the capacity achieving distributions for each respective channel. That is,    C  =   lim   inf    1  n     ∑   i  =  1   n    C  i           C     infimum      1  n     superscript   subscript     i  1    n    subscript  C  i         C=\lim\inf\frac{1}{n}\sum_{i=1}^{n}C_{i}   where    C  i     subscript  C  i    C_{i}   is the capacity of the i th channel.  Outline of the proof  The proof runs through in almost the same way as that of channel coding theorem. Achievability follows from random coding with each symbol chosen randomly from the capacity achieving distribution for that particular channel. Typicality arguments use the definition of typical sets for non-stationary sources defined in the asymptotic equipartition property article.  The technicality of lim inf comes into play when     1  n     ∑   i  =  1   n    C  i          1  n     superscript   subscript     i  1    n    subscript  C  i      \frac{1}{n}\sum_{i=1}^{n}C_{i}   does not converge.  See also   Asymptotic equipartition property (AEP)  Fano's Inequality  Rate–distortion theory  Shannon's source coding theorem  Shannon–Hartley theorem  Turbo code   Notes  References   Cover T. M. , Thomas J. A., Elements of Information Theory , John Wiley & Sons , 1991. ISBN 0-471-06259-6  Fano, R. A. , Transmission of information; a statistical theory of communications , MIT Press , 1961. ISBN 0-262-06001-9  Feinstein, Amiel , "A New basic theorem of information theory", IEEE Transactions on Information Theory , 4(4): 2-22, 1954.  MacKay, D. J. C. , Information Theory, Inference, and Learning Algorithms , Cambridge University Press , 2003. ISBN 0-521-64298-1 [free online]  Shannon, C. E. , A Mathematical Theory of Communication Urbana, IL: University of Illinois Press, 1949 (reprinted 1998).  Wolfowitz, J. , "The coding of messages subject to chance errors", Illinois J. Math. , 1: 591–606, 1957.   External links   On Shannon and Shannon's law  Shannon's Noisy Channel Coding Theorem   "  Category:Information theory  Category:Theorems in discrete mathematics  Category:Telecommunication theory  Category:Coding theory     Sae-Young Chung , G. David Forney, Jr. , Thomas J. Richardson , and Rüdiger Urbanke , " On the Design of Low-Density Parity-Check Codes within 0.0045 dB of the Shannon Limit ", IEEE Communications Letters , 5: 58-60, Feb. 2001. ISSN 1089-7798 ↩  For a description of the "sup" function, see Supremum ↩  Robert Gallager. Information Theory and Reliable Communication. New York: John Wiley & Sons , 1968. ISBN 0-471-29048-3 ↩     