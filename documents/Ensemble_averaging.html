<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="69">Ensemble averaging</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Ensemble averaging</h1>
<hr/>

<p>In <a href="machine_learning" title="wikilink">machine learning</a>, particularly in the creation of <a href="artificial_neural_networks" title="wikilink">artificial neural networks</a>, <strong>ensemble averaging</strong> is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models "average out."</p>
<h2 id="overview">Overview</h2>

<p>Ensemble averaging is one of the simplest types of <a href="committee_machine" title="wikilink">committee machines</a>. Along with <a href="Boosting_(meta-algorithm)" title="wikilink">boosting</a>, it is one of the two major types of static committee machines.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> In contrast to standard network design in which many networks are generated but only one is kept, ensemble averaging keeps the less satisfactory networks around, but with less weight.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> The theory of ensemble averaging relies on two properties of artificial neural networks:<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<ol>
<li>In any network, the bias can be reduced at the cost of increased variance</li>
<li>In a group of networks, the variance can be reduced at no cost to bias</li>
</ol>

<p>Ensemble averaging creates a group of networks, each with low bias and high variance, then combines them to a new network with (hopefully) low bias and low variance. It is thus a resolution of the <a href="bias-variance_dilemma" title="wikilink">bias-variance dilemma</a>.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> The idea of combining experts has been traced back to <a href="Pierre-Simon_Laplace" title="wikilink">Pierre-Simon Laplace</a>.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h2 id="method">Method</h2>

<p>The theory mentioned above gives an obvious strategy: create a set of experts with low bias and high variance, and then average them. Generally, what this means is to create a set of experts with varying parameters; frequently, these are the initial synaptic weights, although other factors (such as the learning rate, momentum etc.) may be varied as well. Some authors recommend against varying weight decay and early stopping.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> The steps are therefore:</p>
<ol>
<li>Generate <em>N</em> experts, each with their own initial values. (Initial values are usually chosen randomly from a distribution.)</li>
<li>Train each expert separately.</li>
<li>Combine the experts and average their values.</li>
</ol>

<p>Alternatively, <a href="domain_knowledge" title="wikilink">domain knowledge</a> may be used to generate several <em>classes</em> of experts. An expert from each class is trained, and then combined.</p>

<p>A more complex version of ensemble average views the final result not as a mere average of all the experts, but rather as a weighted sum. If each expert is 

<math display="inline" id="Ensemble_averaging:0">
 <semantics>
  <msub>
   <mi>y</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>y</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{i}
  </annotation>
 </semantics>
</math>

, then the overall result 

<math display="inline" id="Ensemble_averaging:1">
 <semantics>
  <mover accent="true">
   <mi>y</mi>
   <mo stretchy="false">~</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-~</ci>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tilde{y}
  </annotation>
 </semantics>
</math>

 can be definied as:</p>

<p>

<math display="block" id="Ensemble_averaging:2">
 <semantics>
  <mrow>
   <mrow>
    <mover accent="true">
     <mi>y</mi>
     <mo stretchy="false">~</mo>
    </mover>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>ùï©</mi>
     <mo>;</mo>
     <mi>Œ±</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <munderover>
     <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
     <mrow>
      <mi>j</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>p</mi>
    </munderover>
    <mrow>
     <msub>
      <mi>Œ±</mi>
      <mi>j</mi>
     </msub>
     <msub>
      <mi>y</mi>
      <mi>j</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>ùï©</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <ci>normal-~</ci>
      <ci>y</ci>
     </apply>
     <list>
      <ci>ùï©</ci>
      <ci>Œ±</ci>
     </list>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>j</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>p</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Œ±</ci>
       <ci>j</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>j</ci>
      </apply>
      <ci>ùï©</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tilde{y}(\mathbb{x};\mathbb{\alpha})=\sum_{j=1}^{p}\alpha_{j}y_{j}(\mathbb{x})
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Ensemble_averaging:3">
 <semantics>
  <mi>Œ±</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Œ±</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbb{\alpha}
  </annotation>
 </semantics>
</math>


 is a set of weights. The optimization problem of finding alpha is readily solved through neural networks, hence a "meta-network" where each "neuron" is in fact an entire neural network can be trained, and the synaptic weights of the final network is the weight applied to each expert. This is known as a <em>linear combination of experts</em>.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>

<p>It can be seen that most forms of neural networks are some subset of a linear combination: the standard neural net (where only one expert is used) is simply a linear combination with all 

<math display="inline" id="Ensemble_averaging:4">
 <semantics>
  <mrow>
   <msub>
    <mi>Œ±</mi>
    <mi>j</mi>
   </msub>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>Œ±</ci>
     <ci>j</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha_{j}=0
  </annotation>
 </semantics>
</math>

 and one 

<math display="inline" id="Ensemble_averaging:5">
 <semantics>
  <mrow>
   <msub>
    <mi>Œ±</mi>
    <mi>k</mi>
   </msub>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>Œ±</ci>
     <ci>k</ci>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha_{k}=1
  </annotation>
 </semantics>
</math>

. A raw average is where all 

<math display="inline" id="Ensemble_averaging:6">
 <semantics>
  <msub>
   <mi>Œ±</mi>
   <mi>j</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>Œ±</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha_{j}
  </annotation>
 </semantics>
</math>

 are equal to some constant value, namely one over the total number of experts.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>

<p>A more recent ensemble averaging method is negative correlation learning,<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> proposed by Y. Liu and X. Yao. Now this method has been widely used in <a href="evolutionary_computing" title="wikilink">evolutionary computing</a>.</p>

<p>In probabilistic networks combining models has been also demonstrated as shown by Cardenas et al.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h2 id="benefits">Benefits</h2>
<ul>
<li>The resulting committee is almost always less complex than a single network which would achieve the same level of performance<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></li>
<li>The resulting committee can be trained more easily on smaller input sets<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></li>
<li>The resulting committee often has improved performance over any single network<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></li>
<li>The risk of <a class="uri" href="overfitting" title="wikilink">overfitting</a> is lessened, as there are fewer parameters (weights) which need to be set<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></li>
</ul>
<h2 id="references">References</h2>
<references>
</references>
<h2 id="further-reading">Further reading</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Artificial_intelligence" title="wikilink">Category:Artificial intelligence</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Haykin, Simon. Neural networks : a comprehensive foundation. 2nd ed. Upper Saddle River N.J.: Prentice Hall, 1999.<a href="#fnref1">‚Ü©</a></li>
<li id="fn2">Hashem, S. "Optimal linear combinations of neural networks." Neural Networks 10, no. 4 (1997): 599‚Äì614.<a href="#fnref2">‚Ü©</a></li>
<li id="fn3">Naftaly, U., N. Intrator, and D. Horn. "Optimal ensemble averaging of neural networks." Network: Computation in Neural Systems 8, no. 3 (1997): 283‚Äì296.<a href="#fnref3">‚Ü©</a></li>
<li id="fn4">Geman, S., E. Bienenstock, and R. Doursat. "Neural networks and the bias/variance dilemma." Neural computation 4, no. 1 (1992): 1‚Äì58.<a href="#fnref4">‚Ü©</a></li>
<li id="fn5">Clemen, R. T. "Combining forecasts: A review and annotated bibliography." International Journal of Forecasting 5, no. 4 (1989): 559‚Äì583.<a href="#fnref5">‚Ü©</a></li>
<li id="fn6"></li>
<li id="fn7"></li>
<li id="fn8"></li>
<li id="fn9">Y. Liu and X. Yao, [<a href="http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6T08-3XXCXC7-6&amp;_user=10&amp;_coverDate=12%2F31%2F1999&amp;_rdoc=1&amp;_fmt=high&amp;_orig=gateway&amp;_origin=gateway&amp;_sort=d&amp;_docanchor">http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6T08-3XXCXC7-6&amp;_user=10&amp;_coverDate=12%2F31%2F1999&amp;_rdoc=1&amp;_fmt=high&amp;_orig=gateway&amp;_origin=gateway&amp;_sort=d&amp;_docanchor</a>=&amp;view;=c&amp;_searchStrId=1697373933&amp;_rerunOrigin=google&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5;=57475686ca0dcb592a456830d077cd1e&amp;searchtype;=a Ensemble Learning via Negative Correlation] Neural Networks, Volume 12, Issue 10, December 1999, Pages 1399-1404<a href="#fnref9">‚Ü©</a></li>
<li id="fn10"><a href="#fnref10">‚Ü©</a></li>
<li id="fn11">Pearlmutter, B. A., and R. Rosenfeld. "Chaitin‚ÄìKolmogorov complexity and generalization in neural networks." In Proceedings of the 1990 conference on Advances in neural information processing systems 3, 931. Morgan Kaufmann Publishers Inc., 1990.<a href="#fnref11">‚Ü©</a></li>
<li id="fn12"></li>
<li id="fn13"></li>
<li id="fn14"></li>
</ol>
</section>
</body>
</html>
