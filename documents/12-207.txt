   German tank problem      German tank problem   (Figure)  During World War II , production of German tanks such as the Panther was accurately estimated by Allied intelligence using statistical methods.   In the statistical theory of estimation , the problem of estimating the maximum of a discrete uniform distribution from sampling without replacement is known in English as the German tank problem , due to its application in World War II to the estimation of the number of German tanks.  The analyses illustrate the difference between frequentist inference and Bayesian inference .  Estimating the population maximum based on a single sample yields divergent results, while the estimation based on multiple samples is an instructive practical estimation question whose answer is simple but not obvious.  Example  Suppose an intelligence officer has spotted  tanks with serial numbers, 2, 6, 7, and 14, with the maximum observed serial number, . The unknown total number of tanks is called N .  The formula for estimating the total number of tanks suggested by the frequentist approach outlined below is      N  ≈    m  +   m  k    -  1   =  16.5        N      m    m  k    1        16.5     N\approx m+\frac{m}{k}-1=16.5     Whereas, the Bayesian analysis below yields (primarily) a probability mass function for the number of tanks       Pr   (   N  =  n   )    =   {     0      if  n   <  m           k  -  1   k       (       m  -  1        k  -  1       )    (      n      k      )          if  n   ≥  m            Pr    N  n     cases  0      if  n   m         k  1   k      binomial    m  1     k  1     binomial  n  k         if  n   m      \Pr(N=n)=\begin{cases}0&\text{if }n     from which we can estimate the number of tanks according to     N   N   \displaystyle N     This distribution has positive skewness , related to the fact that there are at least 14 tanks.  Historical problem  During the course of the war the Western Allies made sustained efforts to determine the extent of German production, and approached this in two major ways: conventional intelligence gathering and statistical estimation. In many cases, statistical analysis substantially improved on conventional intelligence. In some cases, conventional intelligence was used in conjunction with statistical methods, as was the case in estimation of Panther tank production just prior to D-Day.  The allied command structure had thought the Panzer V (Panther) tanks seen in Italy, with their high velocity, long barreled 75 mm/L70 guns, were unusual heavy tanks, and would only be seen in northern France in small numbers, much the same way as the Tiger I was seen in Tunisia. The US Army was confident that the Sherman tank would continue to perform well, as it had versus the Panzer III and Panzer IV tanks in North Africa and Sicily. Shortly before D-Day , rumors indicated that large numbers of Panzer V tanks were being used.  To ascertain if this were true the Allies attempted to estimate the number of tanks being produced. To do this they used the serial numbers on captured or destroyed tanks. The principal numbers used were gearbox numbers, as these fell in two unbroken sequences. Chassis and engine numbers were also used, though their use was more complicated. Various other components were used to cross-check the analysis. Similar analyses were done on tires, which were observed to be sequentially numbered (i.e., 1, 2, 3, ..., N ). 1 2  The analysis of tank wheels yielded an estimate for the number of wheel molds that were in use. A discussion with British road wheel makers then estimated the number of wheels that could be produced from this many molds, which yielded the number of tanks that were being produced each month. Analysis of wheels from two tanks (32 road wheels each, 64 road wheels total) yielded an estimate of 270 produced in February 1944, substantially more than had previously been suspected. 3  German records after the war showed production for the month of February 1944 was 276. The statistical approach proved to be far more accurate than conventional intelligence methods, and the phrase "German tank problem" became accepted as a descriptor for this type of statistical analysis.  Estimating production was not the only use of this serial number analysis. It was also used to understand German production more generally, including number of factories, relative importance of factories, length of supply chain (based on lag between production and use), changes in production, and use of resources such as rubber.  Specific data  According to conventional Allied intelligence estimates, the Germans were producing around 1,400 tanks a month between June 1940 and September 1942. Applying the formula below to the serial numbers of captured tanks, the number was calculated to be 246 a month. After the war, captured German production figures from the ministry of Albert Speer showed the actual number to be 245. 4  Estimates for some specific months are given as: 5      Month   Statistical estimate   Intelligence estimate   German records     June 1940   169   1,000   122     June 1941   244   1,550   271     August 1942   327   1,550   342     Similar analyses  Similar serial number analysis was used for other military equipment during World War II, most successfully for the V-2 rocket.  During World War II, German intelligence analyzed factory markings on Soviet military equipment, and during the Korean War , factory markings on Soviet equipment were analyzed. The Soviets also estimated German tank production during World War II.  In the 1980s, some Americans were given access to the production line of Israel's Merkava tanks. The production numbers were classified, but the tanks had serial numbers, allowing estimation of production.  The formula has been used in non-military contexts, for example to estimate the number of Commodore 64 computers built, where the result (12.5 million) matches the official figures quite well. 6  Countermeasures  To prevent serial number analysis, serial numbers can be excluded, or usable auxiliary information reduced. Alternatively, serial numbers that resist cryptanalysis can be used, most effectively by randomly choosing numbers without replacement from a list that is much larger than the number of objects produced (compare the one-time pad ), or produce random numbers and check them against the list of already assigned numbers; collisions are likely to occur unless the number of digits possible is more than twice the number of digits in the number of objects produced (where the serial number can be in any base); see birthday problem . For this, a cryptographically secure pseudorandom number generator may be used. All these methods require a lookup table (or breaking the cypher) to back out from serial number to production order, which complicates use of serial numbers: a range of serial numbers cannot be recalled, for instance, but each must be looked up individually, or a list generated.  Alternatively, sequential serial numbers can be encrypted, which allows easy decoding, but then there is a known-plaintext attack : even if starting from an arbitrary point, the plaintext has a pattern (namely, numbers are in sequence). One example is given in Ken Follett 's novel " Code to Zero ", where the encryption of the Jupiter C rocket serial numbers is described as:       H   U   N   T   S   V   I   L   E   X     1   2   3   4   5   6   7   8   9   0      The code word here is Huntsville (with repeated letters omitted) to get a 10-letter key. The rocket number 13 was therefore "HN", or the rocket number 24 was "UT".  Frequentist analysis  Minimum-variance unbiased estimator  For point estimation (estimating a single value for the total(    N  ^     normal-^  N    \hat{N}   )), the minimum-variance unbiased estimator (MVUE, or UMVU estimator) is given by:       N  ^   =    m   (   1  +   k   -  1     )    -  1        normal-^  N       m    1   superscript  k    1      1     \hat{N}=m\left(1+k^{-1}\right)-1     where m is the largest serial number observed ( sample maximum ) and k is the number of tanks observed ( sample size ). 7 8 Note that once a serial number has been observed, it is no longer in the pool and will not be observed again.  This has a variance of       var   (   N  ^   )    =    1  k      (   N  -  k   )    (   N  +  1   )     (   k  +  2   )     ≈     N  2    k  2    for small samples  k   ≪  N         var   normal-^  N        1  k         N  k     N  1      k  2               superscript  N  2    superscript  k  2    for small samples  k     much-less-than    N     \operatorname{var}(\hat{N})=\frac{1}{k}\frac{(N-k)(N+1)}{(k+2)}\approx\frac{N^%
 {2}}{k^{2}}\text{ for small samples }k\ll N     so a standard deviation of approximately N / k , the (population) average size of a gap between samples; compare m / k above.  Intuition  The formula may be understood intuitively as the sample maximum plus the average gap between observations in the sample, the sample maximum being chosen as the initial estimator, due to being the maximum likelihood estimator , with the gap being added to compensate for the negative bias of the sample maximum as an estimator for the population maximum, and written as       N  ^   =   m  +    m  -  k   k    =    m  +   m   k   -  1      -  1   =    m   (   1  +   k   -  1     )    -  1          normal-^  N     m      m  k   k             m    m   superscript  k    1      1            m    1   superscript  k    1      1      \hat{N}=m+\frac{m-k}{k}=m+mk^{-1}-1=m\left(1+k^{-1}\right)-1     This can be visualized by imagining that the samples are evenly spaced throughout the range, with additional samples just outside the range at 0 and N + 1. If starting with an initial gap between 0 and the lowest sample (sample minimum), the average gap between samples is     (   m  -  k   )   /  k        m  k   k    (m-k)/k   ; the    -  k      k    -k   being because the samples themselves are not counted in computing the gap between samples.  This philosophy is formalized and generalized in the method of maximum spacing estimation .  Derivation  The probability that the sample maximum equals m is     (       m  -  1        k  -  1       )   /   (      N      k      )        binomial    m  1     k  1     binomial  N  k     {\textstyle\left({{m-1}\atop{k-1}}\right)}\big/{\textstyle\left({{N}\atop{k}}%
 \right)}   , where    (      ⋅      ⋅      )     binomial  normal-⋅  normal-⋅    {\textstyle\left({{\cdot}\atop{\cdot}}\right)}   is the binomial coefficient .  Given the total number N and the sample size k , the expected value of the sample maximum is     μ   μ   \displaystyle\mu   From this the unknown quantity N can be expressed in terms of expectation and sample size as     N   N   \displaystyle N     By linearity of the expectation it is obtained that       μ   (   1  +   k   -  1     )    -  1        μ    1   superscript  k    1      1    \displaystyle\mu\left(1+k^{-1}\right)-1   and so an unbiased estimator of N is obtained by replacing the expectation with the observation, so that      N  ^     normal-^  N    \displaystyle\hat{N}     To show that this is the UMVU estimator:   first show that the sample maximum is a sufficient statistic for the population maximum, using a method similar to that detailed at sufficiency: uniform distribution (but for the German tank problem we must exclude the outcomes in which a serial number occurs twice in the sample);  Next, show that it is a complete statistic .  Then the Lehmann–Scheffé theorem states that the sample maximum, corrected for bias as above to be unbiased, is the UMVU estimator.   Confidence intervals  Instead of, or in addition to, point estimation, interval estimation can be carried out, such as confidence intervals . These are easily computed, based on the observation that the probability that k samples will fall in an interval covering p of the range (0 ≤ p ≤ 1) is p k (assuming in this section that draws are with replacement, to simplify computations; if draws are without replacement, this overstates the likelihood, and intervals will be overly conservative).  Thus the sampling distribution of the quantile of the sample maximum is the graph x 1/ k from 0 to 1: the p th to q th quantile of the sample maximum m are the interval [ p 1/ k N , q 1/ k N ]. Inverting this yields the corresponding confidence interval for the population maximum of [ m / q 1/ k , m / p 1/ k ].  For example, taking the symmetric 95% interval p = 2.5% and q = 97.5% for k = 5 yields       0.025   1  /  5     ≈  0.48   ,       superscript  0.025    1  5    0.48    \scriptstyle 0.025^{1/5}\;\approx\;0.48,\,         0.975   1  /  5     ≈  0.995       superscript  0.975    1  5    0.995    \scriptstyle 0.975^{1/5}\;\approx\;0.995   , so a confidence interval of approximately    [   1.005  m   ,   2.08  m   ]       1.005  m     2.08  m     \scriptstyle\left[1.005m,\,2.08m\right]   . The lower bound is very close to m, so more informative is the asymmetric confidence interval from p = 5% to 100%; for k = 5 this yields       0.05   1  /  5     ≈  0.55   ,       superscript  0.05    1  5    0.55    \scriptstyle 0.05^{1/5}\;\approx\;0.55,   so the interval [ m , 1.82 m ].  More generally, the (downward biased) 95% confidence interval is     [  m  ,   m  /   0.05   1  /  k     ]   =   [  m  ,   m  ⋅   20   1  /  k     ]        m    m   superscript  0.05    1  k       m   normal-⋅  m   superscript  20    1  k        \scriptstyle\left[m,\,m/0.05^{1/k}\right]\;=\;\left[m,\,m\cdot 20^{1/k}\right]   . For a range of k, with the UMVU point estimator (plus 1 for legibility) for reference, this yields:      k   point estimate   confidence interval       1       2  m      2  m    \scriptstyle 2m          [  m  ,   20  m   ]     m    20  m     \scriptstyle[m,\,20m]        2       1.5  m      1.5  m    \scriptstyle 1.5m          [  m  ,   4.5  m   ]     m    4.5  m     \scriptstyle[m,\,4.5m]        5       1.2  m      1.2  m    \scriptstyle 1.2m          [  m  ,   1.82  m   ]     m    1.82  m     \scriptstyle[m,\,1.82m]        10       1.1  m      1.1  m    \scriptstyle 1.1m          [  m  ,   1.35  m   ]     m    1.35  m     \scriptstyle[m,\,1.35m]        20       1.05  m      1.05  m    \scriptstyle 1.05m          [  m  ,   1.16  m   ]     m    1.16  m     \scriptstyle[m,\,1.16m]        Immediate observations are:   For small sample sizes, the confidence interval is very wide, reflecting great uncertainty in the estimate.  The range shrinks rapidly, reflecting the exponentially decaying likelihood that all samples will be significantly below the maximum.  The confidence interval exhibits positive skew, as N can never be below the sample maximum, but can potentially be arbitrarily high above it.   Note that m / k cannot be used naively (or rather ( m + m / k − 1)/ k ) as an estimate of the standard error  SE , as the standard error of an estimator is based on the population maximum (a parameter), and using an estimate to estimate the error in that very estimate is circular reasoning .  In some fields, notably futurology , estimation of confidence intervals in this way, based on a single sample – considering it as a randomly sampled quantile (by mediocrity principle ) – is known as the Copernican principle . This is particularly applied to estimate lifetimes based on current age, notably in the doomsday argument , which applies it to estimate the expected survival time of the human race.  Bayesian analysis  The Bayesian approach to the German tank problem is to consider the credibility    (  N  =  n  ∣  M  =  m  ,  K  =  k  )     fragments  normal-(  N   n  normal-∣  M   m  normal-,  K   k  normal-)    \scriptstyle(N=n\mid M=m,K=k)   that the number of enemy tanks   N   N   \scriptstyle N   is equal to the number   n   n   \scriptstyle n   , when the number of observed tanks,   K   K   \scriptstyle K   is equal to the number   k   k   \scriptstyle k   , and the maximum serial number   M   M   \scriptstyle M   is equal to the number   m   m   \scriptstyle m   .  For brevity    (  N  =  n  ∣  M  =  m  ,  K  =  k  )     fragments  normal-(  N   n  normal-∣  M   m  normal-,  K   k  normal-)    \scriptstyle(N=n\mid M=m,K=k)   is written    (  n  ∣  m  ,  k  )     fragments  normal-(  n  normal-∣  m  normal-,  k  normal-)    \scriptstyle(n\mid m,k)     The rule for conditional probability gives       (  n  ∣  m  ,  k  )   =   (  m  ∣  n  ,  k  )     (  n  ∣  k  )    (  m  ∣  k  )       fragments   fragments  normal-(  n  normal-∣  m  normal-,  k  normal-)     fragments  normal-(  m  normal-∣  n  normal-,  k  normal-)      fragments  normal-(  n  normal-∣  k  normal-)    fragments  normal-(  m  normal-∣  k  normal-)      (n\mid m,k)=(m\mid n,k)\frac{(n\mid k)}{(m\mid k)}     The expression     (  m  ∣  n  ,  k  )   =   (  M  =  m  ∣  N  =  n  ,  K  =  k  )      fragments   fragments  normal-(  m  normal-∣  n  normal-,  k  normal-)     fragments  normal-(  M   m  normal-∣  N   n  normal-,  K   k  normal-)     \scriptstyle(m\mid n,k)=(M=m\mid N=n,K=k)   is the conditional probability that the maximum serial number observed is equal to   m   m   \scriptstyle m   , when the number of enemy tanks is known to be equal to   n   n   \scriptstyle n   , and   k   k   \scriptstyle k   enemy tanks have been observed. It is       (  m  ∣  n  ,  k  )   =   {        (       m  -  1        k  -  1       )    (      n      k      )         if  k   ≤  m  ≤  n       0    otherwise         fragments   fragments  normal-(  m  normal-∣  n  normal-,  k  normal-)     cases     binomial    m  1     k  1     binomial  n  k          if  k   m       n    0  otherwise     (m\mid n,k)=\begin{cases}\frac{{\left({{m-1}\atop{k-1}}\right)}}{{\left({{n}%
 \atop{k}}\right)}}&\text{if }k\leq m\leq n\\
 0&\text{otherwise}\end{cases}     where the binomial coefficient     (       n      k       )     binomial  n  k    \scriptstyle{\left({{n}\atop{k}}\right)}   is the number of   k   k   \scriptstyle k   -sized samples from an   n   n   \scriptstyle n   -sized population.  The expression     (  m  ∣  k  )   =   (  M  =  m  ∣  K  =  k  )      fragments   fragments  normal-(  m  normal-∣  k  normal-)     fragments  normal-(  M   m  normal-∣  K   k  normal-)     \scriptstyle(m\mid k)=(M=m\mid K=k)   is the probability that the maximum serial number is equal to m once k tanks have been observed but before the serial numbers have actually been observed.    (  m  ∣  k  )     fragments  normal-(  m  normal-∣  k  normal-)    \scriptstyle(m\mid k)   can be re-written in terms of the other quantities by marginalizing over all possible   n   n   \scriptstyle n   .      (  m  ∣  k  )     fragments  normal-(  m  normal-∣  k  normal-)    \displaystyle(m\mid k)     The expression     (  n  ∣  k  )   =   (  N  =  n  ∣  K  =  k  )      fragments   fragments  normal-(  n  normal-∣  k  normal-)     fragments  normal-(  N   n  normal-∣  K   k  normal-)     \scriptstyle(n\mid k)=(N=n\mid K=k)   is the credibility that the total number of tanks is equal to n when k tanks have been observed but before the serial numbers have actually been observed. Assume that it is some discrete uniform distribution       (  n  ∣  k  )   =   {       1   Ω  -  k         if  k   ≤  n  <  Ω       0    otherwise         fragments   fragments  normal-(  n  normal-∣  k  normal-)     cases    1    normal-Ω  k          if  k   n       normal-Ω    0  otherwise     (n\mid k)=\begin{cases}\frac{1}{\Omega-k}&\text{if }k\leq n<\Omega\\
 0&\text{otherwise}\end{cases}     The upper limit   Ω   normal-Ω   \Omega   must be finite, because the function       f   (  n  )    =    lim   Ω  →  ∞     {       1   Ω  -  k         if  k   ≤  n  <  Ω       0    otherwise             f  n     subscript    normal-→  normal-Ω      cases    1    normal-Ω  k          if  k   n       normal-Ω    0  otherwise      f(n)=\lim_{\Omega\rightarrow\infty}\begin{cases}\frac{1}{\Omega-k}&\text{if }k%
 \leq n<\Omega\\
 0&\text{otherwise}\end{cases}     is     f   (  n  )    =  0        f  n   0    \scriptstyle f(n)=0   which is not a probability mass distribution function.  Then       (  n  ∣  m  ,  k  )   =   {        (  m  ∣  n  ,  k  )      ∑   n  =  m    Ω  -  1      (  m  ∣  n  ,  k  )          if  m   ≤  n  <  Ω       0    otherwise         fragments   fragments  normal-(  n  normal-∣  m  normal-,  k  normal-)     cases     fragments  normal-(  m  normal-∣  n  normal-,  k  normal-)    fragments   superscript   subscript     n  m      normal-Ω  1     fragments  normal-(  m  normal-∣  n  normal-,  k  normal-)           if  m   n       normal-Ω    0  otherwise     (n\mid m,k)=\begin{cases}\frac{(m\mid n,k)}{\sum_{n=m}^{\Omega-1}(m\mid n,k)}&%
 \text{if }m\leq n<\Omega\\
 0&\text{otherwise}\end{cases}     If     ∑   n  =  m   ∞    (  m  ∣  n  ,  k  )   <  ∞     fragments   superscript   subscript     n  m       fragments  normal-(  m  normal-∣  n  normal-,  k  normal-)       \scriptstyle\sum_{n=m}^{\infty}(m\mid n,k)<\infty   , then the unwelcome variable   Ω   normal-Ω   \scriptstyle\Omega   disappears from the expression.       (  n  ∣  m  ,  k  )   =   {     0      if  n   <  m          (  m  ∣  n  ,  k  )      ∑   n  =  m   ∞     (  m  ∣  n  ,  k  )          if  n   ≥  m          fragments   fragments  normal-(  n  normal-∣  m  normal-,  k  normal-)     cases  0      if  n   m      fragments  normal-(  m  normal-∣  n  normal-,  k  normal-)    fragments   superscript   subscript     n  m       fragments  normal-(  m  normal-∣  n  normal-,  k  normal-)         if  n   m      (n\mid m,k)=\begin{cases}0&\text{if }n     For k ≥ 1 the mode of the distribution of the number of enemy tanks is m .  For k ≥ 2, the credibility that the number of enemy tanks is equal to    n   n   n   , is       (  N  =  n  ∣  M  =  m  ≥  k  ,  K  =  k  ≥  2  )   =   {     0      if  n   <  m           k  -  1   k       (       m  -  1        k  -  1       )    (      n      k      )          if  n   ≥  m          fragments   fragments  normal-(  N   n  normal-∣  M   m   k  normal-,  K   k   2  normal-)     cases  0      if  n   m         k  1   k      binomial    m  1     k  1     binomial  n  k         if  n   m      (N=n\mid M=m\geq k,K=k\geq 2)=\begin{cases}0&\text{if }n     and the credibility that the number of enemy tanks,   N   N   \scriptstyle N   , is greater than    n   n   \scriptstyle n   , is       (  N  >  n  ∣  M  =  m  ≥  k  ,  K  =  k  ≥  2  )   =   {     1      if  n   <  m          (       m  -  1        k  -  1       )    (      n       k  -  1       )         if  n   ≥  m          fragments   fragments  normal-(  N   n  normal-∣  M   m   k  normal-,  K   k   2  normal-)     cases  1      if  n   m      binomial    m  1     k  1     binomial  n    k  1         if  n   m      (N>n\mid M=m\geq k,K=k\geq 2)=\begin{cases}1&\text{if }n     For k ≥ 3,   N   N   N   has the finite mean value :        (   m  -  1   )    (   k  -  1   )     k  -  2           m  1     k  1      k  2     \frac{(m-1)(k-1)}{k-2}     For k ≥ 4,   N   N   \scriptstyle N   has the finite standard deviation :         (   m  -  1   )    (   k  -  1   )    (    m  +  1   -  k   )       (   k  -  2   )   2    (   k  -  3   )               m  1     k  1       m  1   k       superscript    k  2   2     k  3       \sqrt{\frac{(m-1)(k-1)(m+1-k)}{(k-2)^{2}(k-3)}}     These formulas are derived below.  Summation formula  The following binomial coefficient identity is used below for simplifying series relating to the German Tank Problem.        ∑   n  =  m   ∞    1   (      n      k      )     =    k   k  -  1     1   (       m  -  1        k  -  1       )           superscript   subscript     n  m        1   binomial  n  k         k    k  1      1   binomial    m  1     k  1        \sum_{n=m}^{\infty}\frac{1}{{\left({{n}\atop{k}}\right)}}=\frac{k}{k-1}\frac{1%
 }{{\left({{m-1}\atop{k-1}}\right)}}     This sum formula is somewhat analogous to the integral formula        ∫   n  =  m   ∞     d  n    n  k     =    1   k  -  1     1   m   k  -  1            superscript   subscript     n  m          d  n    superscript  n  k         1    k  1      1   superscript  m    k  1        \int_{n=m}^{\infty}\frac{dn}{n^{k}}=\frac{1}{k-1}\frac{1}{m^{k-1}}     These formulas apply for k > 1.  One tank  Observing one tank randomly out of a population of n tanks gives the serial number m with probability 1/ n for m ≤ n , and zero probability for m > n . Using Iverson bracket notation this is written       (  M  =  m  ∣  N  =  n  ,  K  =  1  )   =   (  m  ∣  n  )   =    [   m  ≤  n   ]   n      fragments   fragments  normal-(  M   m  normal-∣  N   n  normal-,  K   1  normal-)     fragments  normal-(  m  normal-∣  n  normal-)       delimited-[]    m  n    n     (M=m\mid N=n,K=1)=(m\mid n)=\frac{[m\leq n]}{n}     This is the conditional probability mass distribution function of   m   m   \scriptstyle m   .  When considered a function of n for fixed m this is a likelihood function.       ℒ   (  n  )    =    [   n  ≥  m   ]   n         ℒ  n      delimited-[]    n  m    n     \mathcal{L}(n)=\frac{[n\geq m]}{n}     The maximum likelihood estimate for the total number of tanks is N 0 = m .  The total likelihood is infinite , being a tail of the harmonic series .        ∑  n    ℒ   (  n  )     =    ∑   n  =  m   ∞    1  n    =  ∞          subscript   n     ℒ  n      superscript   subscript     n  m        1  n             \sum_{n}\mathcal{L}(n)=\sum_{n=m}^{\infty}\frac{1}{n}=\infty     but        ∑  n    ℒ   (  n  )    [  n  <  Ω  ]      fragments   subscript   n   L   fragments  normal-(  n  normal-)    fragments  normal-[  n   Ω  normal-]     \displaystyle\sum_{n}\mathcal{L}(n)[n<\Omega]     where    H  n     subscript  H  n    H_{n}   is the harmonic number .  The credibility mass distribution function depends on the prior limit   Ω   normal-Ω   \scriptstyle\Omega   :      (  N  =  n  ∣  M  =  m  ,  K  =  1  )     fragments  normal-(  N   n  normal-∣  M   m  normal-,  K   1  normal-)    \displaystyle(N=n\mid M=m,K=1)     The mean value of   N   N   \scriptstyle N   is        ∑  n    n  ⋅   (  n  ∣  m  )      fragments   subscript   n   n  normal-⋅   fragments  normal-(  n  normal-∣  m  normal-)     \displaystyle\sum_{n}n\cdot(n\mid m)     Two tanks  If two tanks rather than one are observed, then the probability that the larger of the observed two serial numbers is equal to m , is       (  M  =  m  ∣  N  =  n  ,  K  =  2  )   =   (  m  ∣  n  )   =   [  m  ≤  n  ]     m  -  1    (      n      2      )       fragments   fragments  normal-(  M   m  normal-∣  N   n  normal-,  K   2  normal-)     fragments  normal-(  m  normal-∣  n  normal-)     fragments  normal-[  m   n  normal-]       m  1    binomial  n  2      (M=m\mid N=n,K=2)=(m\mid n)=[m\leq n]\frac{m-1}{{\left({{n}\atop{2}}\right)}}     When considered a function of n for fixed m this is a likelihood function      ℒ   (  n  )   =   [  n  ≥  m  ]     m  -  1    (      n      2      )       fragments  L   fragments  normal-(  n  normal-)     fragments  normal-[  n   m  normal-]       m  1    binomial  n  2      \mathcal{L}(n)=[n\geq m]\frac{m-1}{{\left({{n}\atop{2}}\right)}}     The total likelihood is        ∑  n     ℒ   (  n  )        subscript   n     ℒ  n     \displaystyle\sum_{n}\mathcal{L}(n)     and the credibility mass distribution function is      (  N  =  n  ∣  M  =  m  ,  K  =  2  )     fragments  normal-(  N   n  normal-∣  M   m  normal-,  K   2  normal-)    \displaystyle(N=n\mid M=m,K=2)     The median     N  ~     normal-~  N    \scriptstyle\tilde{N}   satisfies       ∑  n    [  n  ≥   N  ~   ]    (  n  ∣  m  )   =   1  2      fragments   subscript   n    fragments  normal-[  n    normal-~  N   normal-]    fragments  normal-(  n  normal-∣  m  normal-)      1  2     \sum_{n}[n\geq\tilde{N}](n\mid m)=\frac{1}{2}     so        m  -  1     N  ~   -  1    =   1  2           m  1      normal-~  N   1      1  2     \frac{m-1}{\tilde{N}-1}=\frac{1}{2}     and so the median is       N  ~   =    2  m   -  1        normal-~  N       2  m   1     \tilde{N}=2m-1     but the mean value of N is infinite      μ  =   ∑  n   n  ⋅   (  n  ∣  m  )   =    m  -  1   1    ∑   n  =  m   ∞    1   n  -  1    =  ∞     fragments  μ    subscript   n   n  normal-⋅   fragments  normal-(  n  normal-∣  m  normal-)        m  1   1    superscript   subscript     n  m        1    n  1        \mu=\sum_{n}n\cdot(n\mid m)=\frac{m-1}{1}\sum_{n=m}^{\infty}\frac{1}{n-1}=\infty     Many tanks  Credibility mass distribution function  The conditional probability that the largest of k observations taken from the serial numbers {1,..., n }, is equal to m , is      (  M  =  m  ∣  N  =  n  ,  K  =  k  ≥  2  )     fragments  normal-(  M   m  normal-∣  N   n  normal-,  K   k   2  normal-)    \displaystyle(M=m\mid N=n,K=k\geq 2)     The likelihood function of n is the same expression      ℒ   (  n  )   =   [  n  ≥  m  ]     (       m  -  1        k  -  1       )    (      n      k      )       fragments  L   fragments  normal-(  n  normal-)     fragments  normal-[  n   m  normal-]      binomial    m  1     k  1     binomial  n  k      \mathcal{L}(n)=[n\geq m]\frac{{\left({{m-1}\atop{k-1}}\right)}}{{\left({{n}%
 \atop{k}}\right)}}     The total likelihood is finite for k ≥ 2:        ∑  n     ℒ   (  n  )        subscript   n     ℒ  n     \displaystyle\sum_{n}\mathcal{L}(n)     The credibility mass distribution function is       (  N  =  n  ∣  M  =  m  ,  K  =  k  ≥  2  )   =   (  n  ∣  m  ,  k  )      fragments   fragments  normal-(  N   n  normal-∣  M   m  normal-,  K   k   2  normal-)     fragments  normal-(  n  normal-∣  m  normal-,  k  normal-)     \displaystyle(N=n\mid M=m,K=k\geq 2)=(n\mid m,k)     The complementary cumulative distribution function is the credibility that N > x     μ   μ   \displaystyle\mu     Order of magnitude  The order of magnitude of the number of enemy tanks is       σ  2   +   μ  2   =   ∑  n    n  2   ⋅   (  N  =  n  ∣  M  =  m  ,  K  =  k  )      fragments   superscript  σ  2     superscript  μ  2     subscript   n    superscript  n  2   normal-⋅   fragments  normal-(  N   n  normal-∣  M   m  normal-,  K   k  normal-)     \sigma^{2}+\mu^{2}=\sum_{n}n^{2}\cdot(N=n\mid M=m,K=k)     Statistical uncertainty  The statistical uncertainty is the standard deviation σ , satisfying the equation        σ  2   +   μ  2    -  μ         superscript  σ  2    superscript  μ  2    μ    \displaystyle\sigma^{2}+\mu^{2}-\mu     So     σ   σ   \displaystyle\sigma     and        σ  2   μ   =     m  -  k   +  1     (   k  -  3   )    (   k  -  2   )            superscript  σ  2   μ         m  k   1       k  3     k  2       \frac{\sigma^{2}}{\mu}=\frac{m-k+1}{(k-3)(k-2)}     The variance-to-mean ratio is simply  $$\frac{\sigma^2}\mu = \frac{m - k + 1}{(k - 3)(k - 2)}$$  See also   Capture-recapture , other method of estimating population size  Maximum spacing estimation , which generalizes the intuition of "assume uniformly distributed"  Copernican principle and Lindy effect , analogous predictions of lifetime assuming one sample (current age).  The Doomsday argument , application to estimate expected survival time of the human race.    Other discussions of the estimation   Maximum likelihood#Bias  Bias of an estimator#Maximum of a discrete uniform distribution  Likelihood function#Example 2   References   Notes    Citations    Bibliography         "  Category:Estimation for specific distributions  Category:World War II tanks of Germany  Category:Applied mathematics  Category:Data analysis  Category:Named probability problems  Category:Discrete distributions  Category:Probability distributions     ↩  ↩  ↩   ↩  , not sufficient. ↩  ↩  ↩     