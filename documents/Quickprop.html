<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="760">Quickprop</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Quickprop</h1>
<hr/>

<p><strong>Quickprop</strong> is an iterative method for determining the minimum of the <a href="loss_function" title="wikilink">loss function</a> of an <a href="artificial_neural_network" title="wikilink">artificial neural network</a>, following an algorithm inspired by the <a href="Newton's_method" title="wikilink">Newton's method</a>. Sometimes, the algorithm is classified to the group of the second order learning methods. It follows a quadratic approximation of the previous <a class="uri" href="gradient" title="wikilink">gradient</a> step and the current gradient, which is expected to be closed to the minimum of the loss function, under the assumption that the loss function is locally approximately square, trying to describe it by means of an upwardly open <a class="uri" href="parabola" title="wikilink">parabola</a>. The minimum is sought in the vertex of the parabola. The procedure requires only local information of the <a href="artificial_neuron" title="wikilink">artificial neuron</a> to which it is applied. The k-th approximation step is given by:</p>

<p>

<math display="inline" id="Quickprop:0">
 <semantics>
  <mrow>
   <mrow>
    <mpadded width="+1.7pt">
     <msup>
      <mi mathvariant="normal">Δ</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>k</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </msup>
    </mpadded>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>i</mi>
      <mi>j</mi>
     </mrow>
    </msub>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mpadded width="+1.7pt">
     <msup>
      <mi mathvariant="normal">Δ</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>k</mi>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </msup>
    </mpadded>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>i</mi>
      <mi>j</mi>
     </mrow>
    </msub>
    <mrow>
     <mo>(</mo>
     <mfrac>
      <mrow>
       <mpadded width="+1.7pt">
        <msub>
         <mo>∇</mo>
         <mrow>
          <mi>i</mi>
          <mi>j</mi>
         </mrow>
        </msub>
       </mpadded>
       <msup>
        <mi>E</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>k</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </msup>
      </mrow>
      <mrow>
       <mrow>
        <mpadded width="+1.7pt">
         <msub>
          <mo>∇</mo>
          <mrow>
           <mi>i</mi>
           <mi>j</mi>
          </mrow>
         </msub>
        </mpadded>
        <msup>
         <mi>E</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mrow>
           <mi>k</mi>
           <mo>-</mo>
           <mn>1</mn>
          </mrow>
          <mo stretchy="false">)</mo>
         </mrow>
        </msup>
       </mrow>
       <mo>-</mo>
       <mrow>
        <mpadded width="+1.7pt">
         <msub>
          <mo>∇</mo>
          <mrow>
           <mi>i</mi>
           <mi>j</mi>
          </mrow>
         </msub>
        </mpadded>
        <msup>
         <mi>E</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>k</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </msup>
       </mrow>
      </mrow>
     </mfrac>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>normal-Δ</ci>
      <ci>k</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <times></times>
       <ci>i</ci>
       <ci>j</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>normal-Δ</ci>
      <apply>
       <minus></minus>
       <ci>k</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <times></times>
       <ci>i</ci>
       <ci>j</ci>
      </apply>
     </apply>
     <apply>
      <divide></divide>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>normal-∇</ci>
        <apply>
         <times></times>
         <ci>i</ci>
         <ci>j</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>E</ci>
        <ci>k</ci>
       </apply>
      </apply>
      <apply>
       <minus></minus>
       <apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>normal-∇</ci>
         <apply>
          <times></times>
          <ci>i</ci>
          <ci>j</ci>
         </apply>
        </apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>E</ci>
         <apply>
          <minus></minus>
          <ci>k</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
       </apply>
       <apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>normal-∇</ci>
         <apply>
          <times></times>
          <ci>i</ci>
          <ci>j</ci>
         </apply>
        </apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>E</ci>
         <ci>k</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Delta^{(k)}\,w_{ij}=\Delta^{(k-1)}\,w_{ij}\left(\frac{\nabla_{ij}\,E^{(k)}}{%
\nabla_{ij}\,E^{(k-1)}-\nabla_{ij}\,E^{(k)}}\right)
  </annotation>
 </semantics>
</math>

</p>

<p>Being 

<math display="inline" id="Quickprop:1">
 <semantics>
  <msub>
   <mi>w</mi>
   <mrow>
    <mi>i</mi>
    <mi>j</mi>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <apply>
     <times></times>
     <ci>i</ci>
     <ci>j</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{ij}
  </annotation>
 </semantics>
</math>

 the neuron j weight of its i input and E is the loss function.</p>

<p>The Quickprop algorithm is an implementation of the error <a class="uri" href="backpropagation" title="wikilink">backpropagation</a> algorithm, but the network can behave chaotically during the learning phase due to large step sizes.</p>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li>Scott E. Fahlman: <em><a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/user/sef/www/publications/qp-tr.ps">An Empirical Study of Learning Speed in Back-Propagation Networks</a></em>, September 1988</li>
</ul>

<p>"</p>

<p><a href="Category:Machine_learning_algorithms" title="wikilink">Category:Machine learning algorithms</a> <a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a> <a href="Category:Computational_neuroscience" title="wikilink">Category:Computational neuroscience</a></p>
</body>
</html>
