   Coupling (probability)      Coupling (probability)   In probability theory , coupling is a proof technique that allows one to compare two unrelated variables by "forcing" them to be related in some way.  Definition  Using the standard formalism of probability, let    X  1     subscript  X  1    X_{1}   and    X  2     subscript  X  2    X_{2}   be two random variables defined on probability spaces     (   Ω  1   ,   F  1   ,   P  1   )      subscript  normal-Ω  1    subscript  F  1    subscript  P  1     (\Omega_{1},F_{1},P_{1})   and    (   Ω  2   ,   F  2   ,   P  2   )      subscript  normal-Ω  2    subscript  F  2    subscript  P  2     (\Omega_{2},F_{2},P_{2})   . Then a coupling of    X  1     subscript  X  1    X_{1}   and    X  2     subscript  X  2    X_{2}   is a new probability space    (  Ω  ,  F  ,  P  )     normal-Ω  F  P    (\Omega,F,P)   over which there are two random variables    Y  1     subscript  Y  1    Y_{1}   and    Y  2     subscript  Y  2    Y_{2}   such that    Y  1     subscript  Y  1    Y_{1}   has the same distribution as    X  1     subscript  X  1    X_{1}   while    Y  2     subscript  Y  2    Y_{2}   has the same distribution as    X  2     subscript  X  2    X_{2}   .  An interesting case is when    Y  1     subscript  Y  1    Y_{1}   and    Y  2     subscript  Y  2    Y_{2}   are not independent.  Examples  Random walk  Assume two particles A and B perform a simple random walk in two dimensions, but they start from different points. The simplest way to couple them is simply to force them to walk together. On every step, if A walks up, so does B , if A moves to the left, so does B , etc. Thus, the difference between the two particles stays fixed. As far as A is concerned, it is doing a perfect random walk, while B is the copycat. B holds the opposite view, i.e. that he is, in effect, the original and that A is the copy. And in a sense they both are right. In other words, any mathematical theorem, or result that holds for a regular random walk, will also hold for both A and B .  Consider now a more elaborate example. Assume that A starts from the point (0,0) and B from (10,10). First couple them so that they walk together in the vertical direction, i.e. if A goes up, so does B , etc., but are mirror images in the horizontal direction i.e. if A goes left, B goes right and vice versa. We continue this coupling until A and B have the same horizontal coordinate, or in other words are on the vertical line (5, y ). If they never meet, we continue this process forever (the probability for that is zero, though). After this event, we change the coupling rule. We let them walk together in the horizontal direction, but in a mirror image rule in the vertical direction. We continue this rule until they meet in the vertical direction too (if they do), and from that point on, we just let them walk together.  This is a coupling in the sense that neither particle, taken on its own, can feel anything we did. Nor that fact that the other particle follows him in one way or the other, nor the fact that we changed the coupling rule or when we did it. Each particle performs a simple random walk. And yet, our coupling rule forces them to meet almost surely and to continue from that point on together permanently. This allows one to prove many interesting results that say that "in the long run", it is not important where you started.  Biased coins  Assume two biased coins, the first with probability p of turning up heads and the second with probability q > p of turning up heads. Intuitively, if both coins are tossed the same number of times, the first coin should turn up fewer heads than the second one. More specifically, for any fixed k , the probability that the first coin produces at least k heads should be less than the probability that the second coin produces at least k heads. However proving such a fact can be difficult with a standard counting argument. 1 Coupling easily circumvents this problem.  Let X 1 , X 2 , ..., X n be indicator variables for heads in a sequence of flips of the first coin. For the second coin, define a new sequence Y 1 , Y 2 , ..., Y n such that   if X i = 1, then Y i = 1,  if X i = 0, then Y i = 1 with probability ( q - p )/(1- p ).   Then the sequence of Y i has exactly the probability distribution of tosses made with the second coin. However, because Y i depends on X i , a toss by toss comparison of the two coins is now possible. That is, for any k ≤ n        Pr   (     X  1   +  ⋯  +   X  n    >  k   )    ≤   Pr   (     Y  1   +  ⋯  +   Y  n    >  k   )     .       Pr       subscript  X  1   normal-⋯   subscript  X  n    k     Pr       subscript  Y  1   normal-⋯   subscript  Y  n    k      \Pr(X_{1}+\cdots+X_{n}>k)\leq\Pr(Y_{1}+\cdots+Y_{n}>k).     See also  Copula (probability theory)  Notes  References   T. Lindvall, Lectures on the coupling method . Wiley, New York, 1992.  H. Thorisson, Coupling, Stationarity, and Regeneration . Springer, New York, 2000.   "  Category:Probability theory     ↩     