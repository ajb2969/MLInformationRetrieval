<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1254">Template matching</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Template matching</h1>
<hr>'''Template matching'''<ref>R. Brunelli, ''Template Matching Techniques in Computer Vision: Theory and Practice'',<style>
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
<style>
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</ref></hr></body></html>
<body>

<p><code>Wiley, ISBN 978-0-470-51706-2, 2009 </code><em><code>(</code><a href="http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470517069.html"><code>1</code></a> <code>TM</code> <code>book)</code></em><code> is a technique in </code><a href="digital_image_processing" title="wikilink"><code>digital</code> <code>image</code> <code>processing</code></a><code> for finding small parts of an image which match a template image. It can be used in manufacturing as a part of quality control,</code><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><code> a way to navigate a mobile robot,</code><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><code> or as a way to detect edges in images.</code><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="approach">Approach</h2>
<h2 id="feature-based-approach">Feature-based approach</h2>

<p>If the template image has strong features, a feature-based approach may be considered; the approach may prove further useful if the match in the search image might be <a href="Transformation_(geometry)" title="wikilink">transformed</a> in some fashion. Since this approach does not consider the entirety of the template image, it can be more computationally efficient when working with source images of larger resolution, as the alternative approach, template-based, may require searching potentially large amounts of points in order to determine the best matching location.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> <a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h2 id="template-based-approach">Template-based approach</h2>

<p>For templates without strong features, or for when the bulk of the template image constitutes the matching image, a template-based approach may be effective. As aforementioned, since template-based template matching may potentially require sampling of a large number of points, it is possible to reduce the number of sampling points by reducing the resolution of the search and template images by the same factor and performing the operation on the resultant downsized images (multiresolution, or <a href="Pyramid_(image_processing)" title="wikilink">Pyramid (image processing)</a>), providing a search window of data points within the search image so that the template does not have to search every viable data point, or a combination of both.</p>
<h2 id="motion-tracking-and-occlusion-handling">Motion tracking and occlusion handling</h2>

<p>In instances where the template may not provide a direct match, it may be useful to implement the use of <a href="eigenspace" title="wikilink">eigenspaces</a> – templates that detail the matching object under a number of different conditions, such as varying perspectives, illuminations, color contrasts, or acceptable matching object “poses”.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> For example, if the user was looking for a face, the eigenspaces may consist of images (templates) of faces in different positions to the camera, in different lighting conditions, or with different expressions.</p>

<p>It is also possible for the matching image to be obscured, or occluded by an object; in these cases, it is unreasonable to provide a multitude of templates to cover each possible occlusion. For example, the search image may be a playing card, and in some of the search images, the card is obscured by the fingers of someone holding the card, or by another card on top of it, or any object in front of the camera for that matter. In cases where the object is malleable or poseable, motion also becomes a problem, and problems involving both motion and occlusion become ambiguous.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> In these cases, one possible solution is to divide the template image into multiple sub-images and perform matching on each subdivision.</p>
<h2 id="template-based-matching-and-convolution">Template-based matching and convolution</h2>

<p>A basic method of template matching uses a <a class="uri" href="convolution" title="wikilink">convolution</a> mask (template), tailored to a specific feature of the search image, which we want to detect. This technique can be easily performed on grey images or <a href="Edge_detection" title="wikilink">edge</a> images. The convolution output will be highest at places where the image structure matches the mask structure, where large image values get multiplied by large mask values.</p>

<p>This method is normally implemented by first picking out a part of the search image to use as a template: We will call the search image <strong>S(x, y)</strong>, where <strong>(x, y)</strong> represent the coordinates of each pixel in the search image. We will call the template <strong>T(x <sub>t</sub>, y <sub>t</sub>)</strong>, where <strong>(x<sub>t</sub>, y<sub>t</sub>)</strong> represent the coordinates of each pixel in the template. We then simply move the center (or the origin) of the template <strong>T(x <sub>t</sub>, y <sub>t</sub>)</strong> over each <strong>(x, y)</strong> point in the search image and calculate the sum of products between the coefficients in <strong>S(x, y)</strong> and <strong>T(x<sub>t</sub>, y<sub>t</sub>)</strong> over the whole area spanned by the template. As all possible positions of the template with respect to the search image are considered, the position with the highest score is the best position. This method is sometimes referred to as <a href="Spatial_filter" title="wikilink">'Linear Spatial Filtering'</a> and the template is called a filter mask.</p>

<p>For example, one way to handle translation problems on images, using template matching is to compare the intensities of the <a href="pixel" title="wikilink">pixels</a>, using the <strong>SAD</strong> (<a href="Sum_of_absolute_differences" title="wikilink">Sum of absolute differences</a>) measure.</p>

<p>A pixel in the search image with coordinates <strong>(x<sub>s</sub>, y<sub>s</sub>)</strong> has intensity <strong>I<sub>s</sub>(x<sub>s</sub>, y<sub>s</sub>)</strong> and a pixel in the template with coordinates <strong>(x<sub>t</sub>, y<sub>t</sub>)</strong> has intensity <strong>I<sub>t</sub>(x<sub>t</sub>, y<sub>t</sub> )</strong>. Thus the <a href="absolute_difference" title="wikilink">absolute difference</a> in the pixel intensities is defined as <strong>Diff(x<sub>s</sub>, y<sub>s</sub>, x <sub>t</sub>, y <sub>t</sub>) = | I<sub>s</sub>(x<sub>s</sub>, y<sub>s</sub>) – I<sub>t</sub>(x <sub>t</sub>, y <sub>t</sub>) |</strong>.</p>

<p><strong>

<math display="inline" id="Template_matching:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>S</mi>
    <mi>A</mi>
    <mi>D</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo>,</mo>
     <mi>y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>0</mn>
     </mrow>
     <msub>
      <mi>T</mi>
      <mtext>rows</mtext>
     </msub>
    </msubsup>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mi>j</mi>
       <mo>=</mo>
       <mn>0</mn>
      </mrow>
      <msub>
       <mi>T</mi>
       <mtext>cols</mtext>
      </msub>
     </msubsup>
     <mrow>
      <mtext>Diff</mtext>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>x</mi>
        <mo>+</mo>
        <mi>i</mi>
       </mrow>
       <mo>,</mo>
       <mrow>
        <mi>y</mi>
        <mo>+</mo>
        <mi>j</mi>
       </mrow>
       <mo>,</mo>
       <mi>i</mi>
       <mo>,</mo>
       <mi>j</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>S</ci>
     <ci>A</ci>
     <ci>D</ci>
     <interval closure="open">
      <ci>x</ci>
      <ci>y</ci>
     </interval>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">0</cn>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>T</ci>
       <mtext>rows</mtext>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>j</ci>
         <cn type="integer">0</cn>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>T</ci>
        <mtext>cols</mtext>
       </apply>
      </apply>
      <apply>
       <times></times>
       <mtext>Diff</mtext>
       <vector>
        <apply>
         <plus></plus>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <plus></plus>
         <ci>y</ci>
         <ci>j</ci>
        </apply>
        <ci>i</ci>
        <ci>j</ci>
       </vector>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   SAD(x,y)=\sum_{i=0}^{T_{\text{rows}}}\sum_{j=0}^{T_{\text{cols}}}{\text{Diff}(%
x+i,y+j,i,j)}
  </annotation>
 </semantics>
</math>

</strong></p>

<p>The mathematical representation of the idea about looping through the pixels in the search image as we translate the origin of the template at every pixel and take the SAD measure is the following:</p>

<p><strong>

<math display="inline" id="Template_matching:1">
 <semantics>
  <mrow>
   <msubsup>
    <mo largeop="true" symmetric="true">∑</mo>
    <mrow>
     <mi>x</mi>
     <mo>=</mo>
     <mn>0</mn>
    </mrow>
    <msub>
     <mi>S</mi>
     <mtext>rows</mtext>
    </msub>
   </msubsup>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>y</mi>
      <mo>=</mo>
      <mn>0</mn>
     </mrow>
     <msub>
      <mi>S</mi>
      <mtext>cols</mtext>
     </msub>
    </msubsup>
    <mrow>
     <mi>S</mi>
     <mi>A</mi>
     <mi>D</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo>,</mo>
      <mi>y</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <eq></eq>
       <ci>x</ci>
       <cn type="integer">0</cn>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>S</ci>
      <mtext>rows</mtext>
     </apply>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>y</ci>
        <cn type="integer">0</cn>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>S</ci>
       <mtext>cols</mtext>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>S</ci>
      <ci>A</ci>
      <ci>D</ci>
      <interval closure="open">
       <ci>x</ci>
       <ci>y</ci>
      </interval>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{x=0}^{S_{\text{rows}}}\sum_{y=0}^{S_{\text{cols}}}{SAD(x,y)}
  </annotation>
 </semantics>
</math>

</strong></p>

<p><strong>S<sub>rows</sub></strong> and <strong>S<sub>cols</sub></strong> denote the rows and the columns of the search image and <strong>T<sub>rows</sub></strong> and <strong>T<sub>cols</sub></strong> denote the rows and the columns of the template image, respectively. In this method the lowest SAD score gives the estimate for the best position of template within the search image. The method is simple to implement and understand, but it is one of the slowest methods.''</p>
<h2 id="implementation">Implementation</h2>

<p>In this simple implementation, it is assumed that the above described method is applied on grey images: This is why <strong>Grey</strong> is used as pixel intensity. The final position in this implementation gives the top left location for where the template image best matches the search image.</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c">minSAD = VALUE_MAX;

<span class="co">// loop through the search image</span>
<span class="kw">for</span> ( <span class="dt">int</span> x = <span class="dv">0</span>; x &lt;= S_rows - T_rows; x++ ) {
    <span class="kw">for</span> ( <span class="dt">int</span> y = <span class="dv">0</span>; y &lt;= S_cols - T_cols; y++ ) {
        SAD = <span class="fl">0.0</span>;

    <span class="co">// loop through the template image</span>
    
    <span class="kw">for</span> ( <span class="dt">int</span> j = <span class="dv">0</span>; j &lt; T_cols; j++ )
            <span class="kw">for</span> ( <span class="dt">int</span> i = <span class="dv">0</span>; i &lt; T_rows; i++ ) {

                pixel p_SearchIMG = S[x+i][y+j];
                pixel p_TemplateIMG = T[i][j];
        
                SAD += abs( p_SearchIMG.Grey - p_TemplateIMG.Grey );
            }
   
        <span class="co">// save the best found position </span>
        <span class="kw">if</span> ( minSAD &gt; SAD ) { 
            minSAD = SAD;
            <span class="co">// give me min SAD</span>
            position.bestRow = x;
            position.bestCol = y;
            position.bestSAD = SAD;
        }
    }
}</code></pre></div>

<p>One way to perform template matching on color images is to decompose the <a href="pixel" title="wikilink">pixels</a> into their color components and measure the quality of match between the color template and search image using the sum of the SAD computed for each color separately.</p>
<h2 id="speeding-up-the-process">Speeding up the Process</h2>

<p>In the past, this type of spatial filtering was normally only used in dedicated hardware solutions because of the computational complexity of the operation,<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> however we can lessen this complexity by filtering it in the frequency domain of the image, referred to as 'frequency domain filtering,' this is done through the use of the <a href="convolution_theorem" title="wikilink">convolution theorem</a>.</p>

<p>Another way of speeding up the matching process is through the use of an image pyramid. This is a series of images, at different scales, which are formed by repeatedly filtering and subsampling the original image in order to generate a sequence of reduced resolution images.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> These lower resolution images can then be searched for the template (with a similarly reduced resolution), in order to yield possible start positions for searching at the larger scales. The larger images can then be searched in a small window around the start position to find the best template location.</p>

<p>Other methods can handle problems such as translation, scale, image rotation and even all affine transformations.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a><a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></p>
<h2 id="improving-the-accuracy-of-the-matching">Improving the accuracy of the matching</h2>

<p>Improvements can be made to the matching method by using more than one template (eigenspaces), these other templates can have different scales and rotations.</p>

<p>It is also possible to improve the accuracy of the matching method by hybridizing the feature-based and template-based approaches.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> Naturally, this requires that the search and template images have features that are apparent enough to support feature matching.</p>
<h2 id="similar-methods">Similar Methods</h2>

<p>Other methods which are similar include '<a href="Stereo_matching" title="wikilink">Stereo matching</a>', '<a href="Image_registration" title="wikilink">Image registration</a>' and '<a href="Scale-invariant_feature_transform" title="wikilink">Scale-invariant feature transform</a>'.</p>
<h2 id="examples-of-use">Examples of Use</h2>

<p>Template matching has various applications and is used in such fields as face recognition (see <a href="facial_recognition_system" title="wikilink">facial recognition system</a>) and medical image processing. Systems have been developed and used in the past to count the number of faces that walk across part of a bridge within a certain amount of time. Other systems include automated calcified nodule detection within digital chest X-rays.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> Recently, this method was implemented in geostatistical simulation which could provide a fast algorithm.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Facial_recognition_system" title="wikilink">Facial recognition system</a></li>
<li><a href="Pattern_recognition" title="wikilink">Pattern recognition</a></li>
<li><a href="Computer_vision" title="wikilink">Computer vision</a></li>
<li><a href="Elastic_Matching" title="wikilink">Elastic Matching</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://docs.opencv.org/doc/tutorials/imgproc/histograms/template_matching/template_matching.html">Template Matching in OpenCV</a></li>
<li><a href="http://rkb.home.cern.ch/rkb/AN16pp/node283.html#SECTION0002830000000000000000">Template Matching</a></li>
<li><a href="http://www.araa.asn.au/acra/acra2004/papers/cole.pdf">Visual Object Recognition using Template Matching</a></li>
<li><a href="http://www.lps.usp.br/~hae/software/cirateg/index.html">Rotation, scale, translation-invariant template matching demonstration program</a></li>
<li><a href="http://campar.in.tum.de/Main/AndreasHofhauser">perspective-invariant template matching</a></li>
</ul>

<p>"</p>

<p><a href="Category:Image_processing" title="wikilink">Category:Image processing</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><code>Aksoy,</code> <code>M.</code> <code>S.,</code> <code>O.</code> <code>Torkul,</code> <code>and</code> <code>I.</code> <code>H.</code> <code>Cedimoglu.</code> <code>"An</code> <code>industrial</code> <code>visual</code> <code>inspection</code> <code>system</code> <code>that</code> <code>uses</code> <code>inductive</code> <code>learning."</code> <code>Journal</code> <code>of</code> <code>Intelligent</code> <code>Manufacturing</code> <code>15.4</code> <code>(August</code> <code>2004):</code> <code>569(6).</code> <code>Expanded</code> <code>Academic</code> <code>ASAP.</code> <code>Thomson</code> <code>Gale.</code><a href="#fnref1">↩</a></li>
<li id="fn2"><code>Kyriacou,</code> <code>Theocharis,</code> <code>Guido</code> <code>Bugmann,</code> <code>and</code> <code>Stanislao</code> <code>Lauria.</code> <code>"Vision-based</code> <code>urban</code> <code>navigation</code> <code>procedures</code> <code>for</code> <code>verbally</code> <code>instructed</code> <code>robots."</code> <code>Robotics</code> <code>and</code> <code>Autonomous</code> <code>Systems</code> <code>51.1</code> <code>(April</code> <code>30,</code> <code>2005):</code> <code>69-80.</code> <code>Expanded</code> <code>Academic</code> <code>ASAP.</code> <code>Thomson</code> <code>Gale.</code><a href="#fnref2">↩</a></li>
<li id="fn3"><code>WANG,</code> <code>CHING</code> <code>YANG,</code> <code>Ph.D.</code> <code>"EDGE</code> <code>DETECTION</code> <code>USING</code> <code>TEMPLATE</code> <code>MATCHING</code> <code>(IMAGE</code> <code>PROCESSING,</code> <code>THRESHOLD</code> <code>LOGIC,</code> <code>ANALYSIS,</code> <code>FILTERS)".</code> <code>Duke</code> <code>University,</code> <code>1985,</code> <code>288</code> <code>pages;</code> <code>AAT</code> <code>8523046</code><a href="#fnref3">↩</a></li>
<li id="fn4">Li, Yuhai, L. Jian, T. Jinwen, X. Honbo. “A fast rotated template matching based on point feature.” Proceedings of the SPIE 6043 (2005): 453-459. MIPPR 2005: SAR and Multispectral Image Processing.<a href="#fnref4">↩</a></li>
<li id="fn5">B. Sirmacek, C. Unsalan. “Urban Area and Building Detection Using SIFT Keypoints and Graph Theory”, IEEE Transactions on Geoscience and Remote Sensing, Vol.47 (4), pp. 1156-1167, April 2009.<a href="#fnref5">↩</a></li>
<li id="fn6">Luis A. Mateos, Dan Shao and Walter G. Kropatsch. Expanding Irregular Graph Pyramid for an Approaching Object. CIARP 2009: 885-891.<a href="#fnref6">↩</a></li>
<li id="fn7">F. Jurie and M. Dhome. Real time robust template matching. In British Machine Vision Conference, pages 123–131, 2002.<a href="#fnref7">↩</a></li>
<li id="fn8">Gonzalez, R, Woods, R, Eddins, S "Digital Image Processing using Matlab" Prentice Hall, 2004<a href="#fnref8">↩</a></li>
<li id="fn9">E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt and J. M. Ogden, Pyramid methods in image processing <a class="uri" href="http://web.mit.edu/persci/people/adelson/pub_pdfs/RCA84.pdf">http://web.mit.edu/persci/people/adelson/pub_pdfs/RCA84.pdf</a><a href="#fnref9">↩</a></li>
<li id="fn10">Yuan, Po, M.S.E.E. "Translation, scale, rotation and threshold invariant pattern recognition system". The University of Texas at Dallas, 1993, 62 pages; AAT EP13780<a href="#fnref10">↩</a></li>
<li id="fn11">H. Y. Kim and S. A. Araújo, "Grayscale Template-Matching Invariant to Rotation, Scale, Translation, Brightness and Contrast," IEEE Pacific-Rim Symposium on Image and Video Technology, Lecture Notes in Computer Science, vol. 4872, pp. 100-113, 2007.<a href="#fnref11">↩</a></li>
<li id="fn12">Korman S., Reichman D., Tsur G. and Avidan S., "FAsT-Match: Fast Affine Template Matching", CVPR2013.<a href="#fnref12">↩</a></li>
<li id="fn13">C. T. Yuen, M. Rizon, W. S. San, and T. C. Seong. “Facial Features for Template Matching Based Face Recognition.” American J. of Engineering and Applied Sciences 3 (1): 899-903, 2010.<a href="#fnref13">↩</a></li>
<li id="fn14">Ashley Aberneithy. "Automatic Detection of Calcified Nodules of Patients with Tuberculous". University College London, 2007<a href="#fnref14">↩</a></li>
<li id="fn15">Tahmasebi, P., Hezarkhani, A., Sahimi, M., 2012, <a href="http://www.springerlink.com/content/0150455247264825/fulltext.pdf">Multiple-point geostatistical modeling based on the cross-correlation functions</a>, Computational Geosciences, 16(3):779-79742.<a href="#fnref15">↩</a></li>
</ol>
</section>
</body>

