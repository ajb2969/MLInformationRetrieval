   Singular spectrum analysis      Singular spectrum analysis   In time series analysis , singular spectrum analysis (SSA) is a nonparametric spectral estimation method. It combines elements of classical time series analysis, multivariate statistics , multivariate geometry, dynamical systems and signal processing . Its roots lie in the classical Karhunen (1946)‚ÄìLo√®ve (1945, 1978) spectral decomposition of time series and random fields and in the Ma√±√© (1981)‚ÄìTakens (1981) embedding theorem . SSA can be an aid in the decomposition of time series into a sum of components, each having a meaningful interpretation. The name "singular spectrum analysis" relates to the spectrum of eigenvalues in a singular value decomposition of a covariance matrix , and not directly to a frequency domain decomposition .  Brief history  The origins of SSA and, more generally, of subspace-based methods for signal processing, go back to the eighteenth century ( Prony's method ). A key development was the formulation of the spectral decomposition of the covariance operator of stochastic processes by Kari Karhunen and Michel Lo√®ve in the late 1940s (Lo√®ve, 1945; Karhunen, 1947).  Broomhead and King (1986a, b) and Fraedrich (1986) proposed to use SSA and multichannel SSA (M-SSA) in the context of nonlinear dynamics for the purpose of reconstructing the attractor of a system from measured time series. These authors provided an extension and a more robust application of the idea of reconstructing dynamics from a single time series based on the embedding theorem . Several other authors had already applied simple versions of M-SSA to meteorological and ecological data sets (Colebrook, 1978; Barnett and Hasselmann, 1979; Weare and Nasstrom, 1982).  Ghil, Vautard and their colleagues (Vautard and Ghil, 1989; Ghil and Vautard, 1991; Vautard et al., 1992; Ghil et al., 2002) noticed the analogy between the trajectory matrix of Broomhead and King, on the one hand, and the Karhunen‚ÄìLoeve decomposition ( Principal component analysis in the time domain), on the other. Thus, SSA can be used as a time-and-frequency domain method for time series analysis ‚Äî independently from attractor reconstruction and including cases in which the latter may fail. The survey paper of Ghil et al. (2002) is the basis of the #Singular spectrum analysis (SSA) section of this article. A crucial result of the work of these authors is that SSA can robustly recover the "skeleton" of an attractor, including in the presence of noise. This skeleton is formed by the least unstable periodic orbits, which can be identified in the eigenvalue spectra of SSA and M-SSA. The identification and detailed description of these orbits can provide highly useful pointers to the underlying nonlinear dynamics.  The so-called ‚ÄòCaterpillar‚Äô methodology is a version of SSA that was developed in the former Soviet Union, independently of the mainstream SSA work in the West. This methodology became known in the rest of the world more recently (Danilov and Zhigljavsky, Eds., 1997; Golyandina et al., 2001; Zhigljavsky, Ed., 2010; Golyandina and Zhigljavsky, 2013). ‚ÄòCaterpillar-SSA‚Äô emphasizes the concept of separability, a concept that leads, for example, to specific recommendations concerning the choice of SSA parameters. This method is thoroughly described in #SSA as a model-free tool section of this article.  Methodology  In practice, SSA is a nonparametric spectral estimation method based on embedding a time series     {   X   (  t  )    :   t  =   1  ,  ‚Ä¶  ,  N    }     conditional-set    X  t     t   1  normal-‚Ä¶  N      \{X(t):t=1,\ldots,N\}   in a vector space of dimension   M   M   M   . SSA proceeds by diagonalizing the    M  √ó  M      M  M    M\times M   lag-covariance matrix    ùêÇ  X     subscript  C  X    {\textbf{C}}_{X}   of    X   (  t  )       X  t    X(t)   to obtain spectral information on the time series, assumed to be stationary in the weak sense. The matrix    ùêÇ  X     subscript  C  X    {\textbf{C}}_{X}   can be estimated directly from the data as a Toeplitz matrix with constant diagonals (Vautard and Ghil, 1989), i.e., its entries    c   i  j      subscript  c    i  j     c_{ij}   depend only on the lag    |   i  -  j   |        i  j     |i-j|   :        c   i  j    =    1   N  -   |   i  -  j   |       ‚àë   t  =  1    N  -   |   i  -  j   |      X   (  t  )   X   (   t  +   |   i  -  j   |    )       .       subscript  c    i  j        1    N      i  j        superscript   subscript     t  1      N      i  j        X  t  X    t      i  j          c_{ij}=\frac{1}{N-|i-j|}\sum_{t=1}^{N-|i-j|}X(t)X(t+|i-j|).     An alternative way to compute    ùêÇ  X     subscript  C  X    {\textbf{C}}_{X}   , is by using the     N  ‚Ä≤   √ó  M       superscript  N  normal-‚Ä≤   M    N^{\prime}\times M   "trajectory matrix"   M   M   M   that is formed by    X   (  t  )       X  t    {\it X(t)}   lag-shifted copies of     N  ‚Ä≤   =    N  -  M   +  1        superscript  N  normal-‚Ä≤       N  M   1     N^{\prime}=N-M+1   , which are      ùêÇ  X   =    1   N  ‚Ä≤     ùêÉ  t   ùêÉ    .       subscript  C  X       1   superscript  N  normal-‚Ä≤     superscript  D  normal-t   D     {\textbf{C}}_{X}=\frac{1}{N^{\prime}}{\textbf{D}}^{\rm t}{\textbf{D}}.   long; then     M   M   M     The    ùêÑ  k     subscript  E  k    {\textbf{E}}_{k}   eigenvectors    ùêÇ  X     subscript  C  X    {\textbf{C}}_{X}   of the lag-covariance matrix    Œª  k     subscript  Œª  k    \lambda_{k}   are called temporal empirical orthogonal functions (EOFs) . The eigenvalues    ùêÇ  X     subscript  C  X    {\textbf{C}}_{X}   of    ùêÑ  k     subscript  E  k    {\textbf{E}}_{k}   account for the partial variance in the direction    ùêÇ  X     subscript  C  X    {\textbf{C}}_{X}   and the sum of the eigenvalues, i.e., the trace of    X   (  t  )       X  t    X(t)   , gives the total variance of the original time series    Œª  k   1  /  2      subscript   superscript  Œª    1  2    k    \lambda^{1/2}_{k}   . The name of the method derives from the singular values     ùêÇ  X   .     subscript  C  X    {\textbf{C}}_{X}.   of    ùêÄ  k     subscript  A  k    {\textbf{A}}_{k}     Decomposition and reconstruction  Projecting the time series onto each EOF yields the corresponding temporal principal components (PCs)       A  k    (  t  )    =    ‚àë   j  =  1   M    X   (    t  +  j   -  1   )    E  k    (  j  )      .         subscript  A  k   t     superscript   subscript     j  1    M     X      t  j   1    subscript  E  k   j      A_{k}(t)=\sum_{j=1}^{M}X(t+j-1)E_{k}(j).   :     M   M   M     An oscillatory mode is characterized by a pair of nearly equal SSA eigenvalues and associated PCs that are in approximate phase quadrature (Ghil et al., 2002). Such a pair can represent efficiently a nonlinear, anharmonic oscillation. This is due to the fact that a single pair of data-adaptive SSA eigenmodes often will capture better the basic periodicity of an oscillatory mode than methods with fixed basis functions , such as the sines and cosines used in the Fourier transform .  The window width    Œª  k     subscript  Œª  k    \lambda_{k}   determines the longest periodicity captured by SSA. Signal-to-noise separation can be obtained by merely inspecting the slope break in a "scree diagram" of eigenvalues    Œª  k   1  /  2      subscript   superscript  Œª    1  2    k    \lambda^{1/2}_{k}   or singular values   k   k   k   vs.     k  *   =  S       superscript  k    S    k^{*}=S   . The point   D   D   D   at which this break occurs should not be confused with a "dimension"    ùêë  K     subscript  R  K    {\textbf{R}}_{K}   of the underlying deterministic dynamics (Vautard and Ghil, 1989).  A Monte-Carlo test (Allen and Robertson, 1996) can be applied to ascertain the statistical significance of the oscillatory pairs detected by SSA. The entire time series or parts of it that correspond to trends, oscillatory modes or noise can be reconstructed by using linear combinations of the PCs and EOFs, which provide the reconstructed components (RCs)       R  K    (  t  )    =    1   M  t      ‚àë   k  ‚àà  ùêæ      ‚àë   j  =   L  t     U  t      A  k    (    t  -  j   +  1   )    E  k    (  j  )        ;         subscript  R  K   t       1   subscript  M  t      subscript     k  K      superscript   subscript     j   subscript  L  t      subscript  U  t       subscript  A  k       t  j   1    subscript  E  k   j        R_{K}(t)=\frac{1}{M_{t}}\sum_{k\in{\textit{K}}}\sum_{j={L_{t}}}^{U_{t}}A_{k}(t%
 -j+1)E_{k}(j);   :     K   K   K     here    M  t     subscript  M  t    M_{t}   is the set of EOFs on which the reconstruction is based. The values of the normalization factor    L  t     subscript  L  t    L_{t}   , as well as of the lower and upper bound of summation    U  t     subscript  U  t    U_{t}   and   L   L   L   , differ between the central part of the time series and the vicinity of its endpoints (Ghil et al., 2002).  Multivariate extension  Multi-channel SSA (or M-SSA) is a natural extension of SSA to an   N   N   N   -channel time series of vectors or maps with    {    X  l    (  t  )    :    l  =   1  ,  ‚Ä¶  ,  L    ;   t  =   1  ,  ‚Ä¶  ,  N     }     conditional-set     subscript  X  l   t    formulae-sequence    l   1  normal-‚Ä¶  L      t   1  normal-‚Ä¶  N       \{X_{l}(t):l=1,\dots,L;t=1,\dots,N\}   data points   L   L   L   . In the meteorological literature, extended EOF (EEOF) analysis is often assumed to be synonymous with M-SSA. The two methods are both extensions of classical principal component analysis (PCA) but they differ in emphasis: EEOF analysis typically utilizes a number   M   M   M   of spatial channels much greater than the number    L  ‚â§  M      L  M    L\leq M   of temporal lags, thus limiting the temporal and spectral information. In M-SSA, on the other hand, one usually chooses   M   M   M   . Often M-SSA is applied to a few leading PCs of the spatial data, with    ùêÇ  X     subscript  C  X    {\textbf{C}}_{X}   chosen large enough to extract detailed temporal and spectral information from the multivariate time series (Ghil et al., 2002).  Recently, Groth and Ghil (2011) have demonstrated that a classical M-SSA analysis suffers from a degeneracy problem, namely the EOFs do not separate well between distinct oscillations when the corresponding eigenvalues are similar in size. This problem is a shortcoming of principal component analysis in general, not just of M-SSA in particular. In order to reduce mixture effects and to improve the physical interpretation, Groth and Ghil (2011) have proposed a subsequent VARIMAX rotation of the spatio-temporal EOFs (ST-EOFs) of the M-SSA. To avoid a loss of spectral properties (Plaut and Vautard 1994), they have introduced a slight modification of the common VARIMAX rotation that does take the spatio-temporal structure of ST-EOFs into account.  MSSA has two forecasting approaches known as recurrent and vector. The discrepancies between these two approaches are attributable to the organization of the single trajectory matrix    ùêÑ  k     subscript  E  k    {\textbf{E}}_{k}   of each series into the block trajectory matrix in the multivariate case. Two trajectory matrices can be organized as either vertical (VMSSA) or horizontal (HMSSA) as was recently introduced in Hassani and Mahmoudvand (2013), and it was shown that these constructions lead to better forecasts. Accordingly, we have four different forecasting algorithms that can be exploited in this version of MSSA (Hassani and Mahmoudvand, 2013).  Prediction  In this subsection, we focus on phenomena that exhibit a significant oscillatory component: repetition increases understanding and hence confidence in a prediction method that is closely connected with such understanding.  Singular spectrum analysis (SSA) and the maximum entropy method (MEM) have been combined to predict a variety of phenomena in meteorology, oceanography and climate dynamics (Ghil et al., 2002, and references therein). First, the ‚Äúnoise‚Äù is filtered out by projecting the time series onto a subset of leading EOFs obtained by SSA; the selected subset should include statistically significant, oscillatory modes. Experience shows that this approach works best when the partial variance associated with the pairs of RCs that capture these modes is large (Ghil and Jiang, 1998).  The prefiltered RCs are then extrapolated by least-square fitting to an autoregressive model  AR [ p ], whose coefficients give the MEM spectrum of the remaining ‚Äúsignal‚Äù. Finally, the extended RCs are used in the SSA reconstruction process to produce the forecast values. The reason why this approach ‚Äì via SSA prefiltering, AR extrapolation of the RCs, and SSA reconstruction ‚Äì works better than the customary AR-based prediction is explained by the fact that the individual RCs are narrow-band signals, unlike the original, noisy time series X ( t ) (Penland et al., 1991; Keppenne and Ghil, 1993). In fact, the optimal order p obtained for the individual RCs is considerably lower than the one given by the standard Akaike information criterion (AIC) or similar ones.  Spatio-temporal gap filling  The gap-filling version of SSA can be used to analyze data sets that are unevenly sampled or contain missing data (Kondrashov and Ghil, 2006; Kondrashov et al. 2010). For a univariate time series, the SSA gap filling procedure utilizes temporal correlations to fill in the missing points. For a multivariate data set, gap filling by M-SSA takes advantage of both spatial and temporal correlations. In either case: (i) estimates of missing data points are produced iteratively, and are then used to compute a self-consistent lag-covariance matrix   M   M   M   and its EOFs    ùïè  =   (   x  1   ,  ‚Ä¶  ,   x  N   )       ùïè    subscript  x  1   normal-‚Ä¶   subscript  x  N      \mathbb{X}=(x_{1},\ldots,x_{N})   ; and (ii) cross-validation is used to optimize the window width   N   N   N   and the number of leading SSA modes to fill the gaps with the iteratively estimated "signal," while the noise is discarded.  SSA as a model-free tool  The areas where SSA can be applied are very broad: climatology, marine science, geophysics, engineering, image processing, medicine, econometrics among them. Hence different modifications of SSA have been proposed and different methodologies of SSA are used in practical applications such as trend extraction, periodicity detection, seasonal adjustment , smoothing , noise reduction (Golyandina et all, 2001).  Basic SSA  SSA can be used as a model-free technique so that it can be applied to arbitrary time series including non-stationary time series. The basic aim of SSA is to decompose the time series into the sum of interpretable components such as trend, periodic components and noise with no a-priori assumptions about the parametric form of these components.  Consider a real-valued time series   L   L   L   of length   ùïè   ùïè   \mathbb{X}   . Let     L   √ó  K      L  K    L\!\times\!K       ùêó  =   [   X  1   :  ‚Ä¶  :   X  K   ]   =    (   x   i  j    )     i  ,  j   =  1    L  ,  K    =   [      x  1      x   ;  2       x   ;  3      ‚Ä¶     x   ;  K         x  2      x   ;  3       x   ;  4      ‚Ä¶     x   ;   K  +  1          x  3      x   ;  4       x   ;  5      ‚Ä¶     x   ;   K  +  2         ‚ãÆ    ‚ãÆ    ‚ãÆ    ‚ã±    ‚ãÆ       x  L      x   ;   L  +  1        x   ;   L  +  2       ‚Ä¶     x   ;  N       ]      fragments  X    fragments  normal-[   subscript  X  1   normal-:  normal-‚Ä¶  normal-:   subscript  X  K   normal-]     superscript   subscript   fragments  normal-(   subscript  x    i  j    normal-)      i  j   1     L  K        subscript  x  1    fragments  x   subscript  normal-;  2     fragments  x   subscript  normal-;  3    normal-‚Ä¶   fragments  x   subscript  normal-;  K       subscript  x  2    fragments  x   subscript  normal-;  3     fragments  x   subscript  normal-;  4    normal-‚Ä¶   fragments  x   subscript  normal-;    K  1        subscript  x  3    fragments  x   subscript  normal-;  4     fragments  x   subscript  normal-;  5    normal-‚Ä¶   fragments  x   subscript  normal-;    K  2       normal-‚ãÆ  normal-‚ãÆ  normal-‚ãÆ  normal-‚ã±  normal-‚ãÆ     subscript  x  L    fragments  x   subscript  normal-;    L  1      fragments  x   subscript  normal-;    L  2     normal-‚Ä¶   fragments  x   subscript  normal-;  N        \mathbf{X}=[X_{1}:\ldots:X_{K}]=(x_{ij})_{i,j=1}^{L,K}=\begin{bmatrix}x_{1}&x%
 _{2}&x_{3}&\ldots&x_{K}\\
 x_{2}&x_{3}&x_{4}&\ldots&x_{K+1}\\
 x_{3}&x_{4}&x_{5}&\ldots&x_{K+2}\\
 \vdots&\vdots&\vdots&\ddots&\vdots\\
 x_{L}&x_{L+1}&x_{L+2}&\ldots&x_{N}\\
 \end{bmatrix}   .  Main algorithm of SSA  1st step: Embedding.  Form the trajectory matrix of the series     X  i   =    (   x  i   ,  ‚Ä¶  ,   x    i  +  L   -  1    )   T    (  1  ‚â§  i  ‚â§  K  )      fragments   subscript  X  i     superscript   fragments  normal-(   subscript  x  i   normal-,  normal-‚Ä¶  normal-,   subscript  x      i  L   1    normal-)   normal-T   italic-   fragments  normal-(  1   i   K  normal-)     X_{i}=(x_{i},\ldots,x_{i+L-1})^{\mathrm{T}}\;\quad(1\leq i\leq K)   , which is the   L   L   L   matrix     ùêó   ùêó   \mathbf{X}     where   ùêó   ùêó   \mathbf{X}   are lagged vectors of size    x   i  j      subscript  x    i  j     x_{ij}   . The matrix     i  +  j   =  const        i  j   const    i+j=\,{\rm const}   is a Hankel matrix which means that   ùêó   ùêó   \mathbf{X}   has equal elements    ùêí  =   ùêóùêó  T       ùêí   superscript  ùêóùêó  normal-T     \mathbf{S}=\mathbf{X}\mathbf{X}^{\mathrm{T}}   on the anti-diagonals     Œª  1   ,  ‚Ä¶  ,   Œª  L       subscript  Œª  1   normal-‚Ä¶   subscript  Œª  L     \lambda_{1},\ldots,\lambda_{L}   .  2nd step: Singular Value Decomposition (SVD).  Perform the singular value decomposition (SVD) of the trajectory matrix   ùêí   ùêí   \mathbf{S}   . Set     Œª  1   ‚â•  ‚Ä¶  ‚â•   Œª  L   ‚â•  0         subscript  Œª  1   normal-‚Ä¶        subscript  Œª  L        0     \lambda_{1}\geq\ldots\geq\lambda_{L}\geq 0   and denote by     U  1   ,  ‚Ä¶  ,   U  L       subscript  U  1   normal-‚Ä¶   subscript  U  L     U_{1},\ldots,U_{L}   the eigenvalues of   ùêí   ùêí   \mathbf{S}   taken in the decreasing order of magnitude (    d  =   rank  ùêó   =   max   {  i  ,     such that    Œª  i    >  0   }          d   rank  ùêó          i      such that   subscript  Œª  i    0       d=\mathop{\mathrm{rank}}\mathbf{X}=\max\{i,\ \mbox{such that}\ \lambda_{i}>0\}   ) and by    d  =  L      d  L    d=L   the orthonormal system of the eigenvectors of the matrix     V  i   =     ùêó  T    U  i    /    Œª  i          subscript  V  i        superscript  ùêó  normal-T    subscript  U  i       subscript  Œª  i       V_{i}=\mathbf{X}^{\mathrm{T}}U_{i}/\sqrt{\lambda_{i}}   corresponding to these eigenvalues.  Set    (   i  =   1  ,  ‚Ä¶  ,  d    )      i   1  normal-‚Ä¶  d     (i=1,\ldots,d)   (note that   ùêó   ùêó   \mathbf{X}   for a typical real-life series) and     ùêó  =    ùêó  1   +  ‚Ä¶  +   ùêó  d     ,      ùêó     subscript  ùêó  1   normal-‚Ä¶   subscript  ùêó  d      \mathbf{X}=\mathbf{X}_{1}+\ldots+\mathbf{X}_{d},        ùêó  i   =     Œª  i     U  i    V  i  T         subscript  ùêó  i        subscript  Œª  i     subscript  U  i    superscript   subscript  V  i   normal-T      \mathbf{X}_{i}=\sqrt{\lambda_{i}}U_{i}V_{i}^{\mathrm{T}}   . In this notation, the SVD of the trajectory matrix    (    Œª  i    ,   U  i   ,   V  i   )        subscript  Œª  i     subscript  U  i    subscript  V  i     (\sqrt{\lambda_{i}},U_{i},V_{i})   can be written as     i   i   i     where      U  i     subscript  U  i    U_{i}   are matrices having rank 1; these are called elementary matrices . The collection   ùêó   ùêó   \mathbf{X}   will be called the     Œª  i        subscript  Œª  i     \sqrt{\lambda_{i}}   th eigentriple (abbreviated as ET) of the SVD. Vectors   ùêó   ùêó   \mathbf{X}   are the left singular vectors of the matrix       Œª  i     V  i    =    ùêó  T    U  i             subscript  Œª  i     subscript  V  i       superscript  ùêó  normal-T    subscript  U  i      \sqrt{\lambda_{i}}V_{i}=\mathbf{X}^{\mathrm{T}}U_{i}   , numbers    {  1  ,  ‚Ä¶  ,  d  }     1  normal-‚Ä¶  d    \{1,\ldots,d\}   are the singular values and provide the singular spectrum of   m   m   m   ; this gives the name to SSA. Vectors     I  1   ,  ‚Ä¶  ,   I  m       subscript  I  1   normal-‚Ä¶   subscript  I  m     I_{1},\ldots,I_{m}   are called vectors of principal components (PCs).  3rd step: Eigentriple grouping.  Partition the set of indices    I  =   {   i  1   ,  ‚Ä¶  ,   i  p   }       I    subscript  i  1   normal-‚Ä¶   subscript  i  p      I=\{i_{1},\ldots,i_{p}\}   into    ùêó  I     subscript  ùêó  I    \mathbf{X}_{I}   disjoint subsets   I   I   I   .  Let     ùêó  I   =    ùêó   i  1    +  ‚Ä¶  +   ùêó   i  p          subscript  ùêó  I      subscript  ùêó   subscript  i  1    normal-‚Ä¶   subscript  ùêó   subscript  i  p       \mathbf{X}_{I}=\mathbf{X}_{i_{1}}+\ldots+\mathbf{X}_{i_{p}}   . Then the resultant matrix    I  =    I  1   ,  ‚Ä¶  ,   I  m        I    subscript  I  1   normal-‚Ä¶   subscript  I  m      I=I_{1},\ldots,I_{m}   corresponding to the group   ùêó   ùêó   \mathbf{X}   is defined as     ùêó  =    ùêó   I  1    +  ‚Ä¶  +   ùêó   I  m      .      ùêó     subscript  ùêó   subscript  I  1    normal-‚Ä¶   subscript  ùêó   subscript  I  m       \mathbf{X}=\mathbf{X}_{I_{1}}+\ldots+\mathbf{X}_{I_{m}}.   . The resultant matrices are computed for the groups    ùêó   I  j      subscript  ùêó   subscript  I  j     \mathbf{X}_{I_{j}}   and the grouped SVD expansion of   N   N   N   can now be written as      ùêó   I  k      subscript  ùêó   subscript  I  k     \mathbf{X}_{I_{k}}     4th step: Diagonal averaging.  Each matrix      ùïè  ~    (  k  )    =   (    x  ~   1   (  k  )    ,  ‚Ä¶  ,    x  ~   N   (  k  )    )        superscript   normal-~  ùïè   k     subscript   superscript   normal-~  x   k   1   normal-‚Ä¶   subscript   superscript   normal-~  x   k   N      \widetilde{\mathbb{X}}^{(k)}=(\widetilde{x}^{(k)}_{1},\ldots,\widetilde{x}^{(k%
 )}_{N})   of the grouped decomposition is hankelized and then the obtained Hankel matrix is transformed into a new series of length     x  1   ,  ‚Ä¶  ,   x  N       subscript  x  1   normal-‚Ä¶   subscript  x  N     x_{1},\ldots,x_{N}   using the one-to-one correspondence between Hankel matrices and time series. Diagonal averaging applied to a resultant matrix   m   m   m   produces a reconstructed series      x  n   =   ‚àë   k  =  1   m     x  ~   n   (  k  )     (  n  =  1  ,  2  ,  ‚Ä¶  ,  N  )   .     fragments   subscript  x  n     superscript   subscript     k  1    m    subscript   superscript   normal-~  x   k   n     fragments  normal-(  n   1  normal-,  2  normal-,  normal-‚Ä¶  normal-,  N  normal-)   normal-.    x_{n}=\sum\limits_{k=1}^{m}\widetilde{x}^{(k)}_{n}\ \ (n=1,2,\ldots,N).   . In this way, the initial series   L   L   L   is decomposed into a sum of    N  ‚Üí  ‚àû     normal-‚Üí  N     N\rightarrow\infty   reconstructed subseries:     N   N   N     This decomposition is the main result of the SSA algorithm. The decomposition is meaningful if each reconstructed subseries could be classified as a part of either trend or some periodic component or noise.  Theory of SSA separability  The two main questions which the theory of SSA attempts to answer are: (a) what time series components can be separated by SSA, and (b) how to choose the window length   L   L   L   and make proper grouping for extraction of a desirable component. Many theoretical results can be found in Golyandina et al. (2001, Ch. 1 and 6).  Trend (which is defined as a slowly varying component of the time series), periodic components and noise are asymptotically separable as   L   L   L   . In practice   L   L   L   is fixed and one is interested in approximate separability between time series components. A number of indicators of approximate separability can be used, see Golyandina et al. (2001, Ch. 1). The window length    œÄ  /  2      œÄ  2    \pi/2   determines the resolution of the method: larger values of   ùïè   ùïè   \mathbb{X}   provide more refined decomposition into elementary components and therefore better separability. The window length   d   d   d   determines the longest periodicity captured by SSA. Trends can be extracted by grouping of eigentriples with slowly varying eigenvectors. A sinusoid with frequency smaller than 0.5 produces two approximately equal eigenvalues and two sine-wave eigenvectors with the same frequencies and     x  n   =    ‚àë   k  =  1   d     b  k    x   n  -  k           subscript  x  n     superscript   subscript     k  1    d      subscript  b  k    subscript  x    n  k        x_{n}=\sum_{k=1}^{d}b_{k}x_{n-k}   -shifted phases.  Separation of two time series components can be considered as extraction of one component in the presence of perturbation by the other component. SSA perturbation theory is developed in Nekrutkin (2010) and Hassani et al. (2011).  Forecasting by SSA  If for some series    L  >  d      L  d    L>d   the SVD step in Basic SSA gives     U  1   ,  ‚Ä¶  ,   U  d       subscript  U  1   normal-‚Ä¶   subscript  U  d     U_{1},\ldots,U_{d}   '' (Golyandina et al., 2001, Ch.5). The subspace spanned by the   L   L   L   leading eigenvectors is called signal subspace . This subspace is used for estimating the signal parameters in signal processing , e.g. ESPRIT for high-resolution frequency estimation. Also, this subspace determines the linear homogeneous recurrence relation (LRR) governing the series, which can be used for forecasting. Continuation of the series by the LRR is similar to forward linear prediction in signal processing.  Let the series be governed by the minimal LRR     x  n   =    ‚àë   k  =  1    L  -  1      a  k    x   n  -  k           subscript  x  n     superscript   subscript     k  1      L  1       subscript  a  k    subscript  x    n  k        x_{n}=\sum_{k=1}^{L-1}a_{k}x_{n-k}   . Let us choose     (   a   L  -  1    ,  ‚Ä¶  ,   a  1   )   T     superscript    subscript  a    L  1    normal-‚Ä¶   subscript  a  1    normal-T    (a_{L-1},\ldots,a_{1})^{\mathrm{T}}   ,     U  1   ,  ‚Ä¶  ,   U  d       subscript  U  1   normal-‚Ä¶   subscript  U  d     U_{1},\ldots,U_{d}   be the eigenvectors (left singular vectors of the     L  x   √ó   L  y        subscript  L  x    subscript  L  y     L_{x}\times L_{y}   -trajectory matrix), which are provided by the SVD step of SSA. Then this series is governed by an LRR     x  n   =    s  n   +   e  n         subscript  x  n      subscript  s  n    subscript  e  n      x_{n}=s_{n}+e_{n}   , where     s  n   =    ‚àë   k  =  1   r     a  k    s   n  -  k           subscript  s  n     superscript   subscript     k  1    r      subscript  a  k    subscript  s    n  k        s_{n}=\sum_{k=1}^{r}a_{k}s_{n-k}   are expressed through    e  n     subscript  e  n    e_{n}   (Golyandina et al., 2001, Ch.5), and can be continued by the same LRR.  This provides the basis for SSA recurrent and vector forecasting algorithms (Golyandina et al., 2001, Ch.2). In practice, the signal is corrupted by a perturbation, e.g., by noise, and its subspace is estimated by SSA approximately. Thus, SSA forecasting can be applied for forecasting of a time series component that is approximately governed by an LRR and is approximately separated from the residual.  Multivariate extension  Multi-channel, Multivariate SSA (or M-SSA) is a natural extension of SSA to for analyzing multivariate time series, where the size of different univariate series does not have to be the same. The trajectory matrix of multi-channel time series consists of stacked trajectory matrices of separate times series. The rest of the algorithm is the same as in the univariate case. System of series can be forecasted analogously to SSA recurrent and vector algorithms (Golyandina and Stepanov, 2005). MSSA has many applications. It is especially popular in analyzing and forecasting economic and financial time series with short and long series length (Patterson et al., 2011, Hassani et al., 2012, Hassani and Mahmoudvand, 2013). Other multivariate extension is 2D-SSA that can be applied to two-dimensional data like digital images (Golyandina and Usevich, 2010). The analogue of trajectory matrix is constructed by moving 2D windows of size     x  n   =     ‚àë   k  =  1   r     a  k    x   n  -  k      +   e  n         subscript  x  n       superscript   subscript     k  1    r      subscript  a  k    subscript  x    n  k       subscript  e  n      x_{n}=\sum_{k=1}^{r}a_{k}x_{n-k}+e_{n}   .  MSSA and causality  A question that frequently arises in time series analysis is whether one economic variable can help in predicting another economic variable. One way to address this question was proposed by Granger (1969), in which he formalized the causality concept. A comprehensive causality test based on MSSA has recently introduced for causality measurement. The test is based on the forecasting accuracy and predictability of the direction of change of the MSSA algorithms (Hassani et al., 2011 and Hassani et al.,2012).  MSSA and EMH  The MSSA forecasting results can be used in examining the efficient market hypothesis controversy (EMH). The EMH suggests that the information contained in the price series of an asset is reflected ‚Äúinstantly, fully, and perpetually‚Äù in the asset‚Äôs current price. Since the price series and the information contained in it are available to all market participants, no one can benefit by attempting to take advantage of the information contained in the price history of an asset by trading in the markets. This is evaluated using two series with different series length in a multivariate system in SSA analysis (Hassani et al. 2010).  MSSA, SSA and Unit Root  SSA's applicability to any kind of stationary or deterministically trending series has been extented to the case of a series with a stochastic trend, also known as a series with a unit root. In Hassani and Thomakos (2010) and Thomakos (2010) the basic theory on the properties and application of SSA in the case of series of a unit root is given, along with several examples. It is shown that SSA in such series produces a special kind of filter, whose form and spectral properties are derived, and that forecasting the single reconstructed component reduces to a moving average. SSA in unit roots thus provides an `optimizing' non-parametric framework for smoothing series with a unit root. This line of work is also extended to the case of two series, both of which have a unit root but are cointegrated. The application of SSA in this bivariate framework produces a smoothed series of the common root component.  Gap-filling  The gap-filling versions of SSA can be used to analyze data sets that are unevenly sampled or contain missing data (Schoellhamer, 2001; Golyandina and Osipov, 2007).  Schoellhamer (2001) shows that the straightforward idea to formally calculate approximate inner products omitting unknown terms is workable for long stationary time series. Golyandina and Osipov (2007) uses the idea of filling in missing entries in vectors taken from the given subspace. The recurrent and vector SSA forecasting can be considered as particular cases of filling in algorithms described in the paper.  Detection of structural changes  SSA can be effectively used as a non-parametric method of time series monitoring and change detection . To do that, SSA performs the subspace tracking in the following way. SSA is applied sequentially to the initial parts of the series, constructs the corresponding signal subspaces and checks the distances between these subspaces and the lagged vectors formed from the few most recent observations. If these distances become too large, a structural change is suspected to have occurred in the series (Golyandina et al., 2001, Ch.3; Moskvina and Zhigljavsky, 2003).  In this way, SSA could be used for change detection not only in trends but also in the variability of the series, in the mechanism that determines dependence between different series and even in the noise structure. The method have proved to be useful in different engineering problems (e.g. Mohammad and Nishida (2011) in robotics).  Relation between SSA and other methods  SSA and Autoregression . Typical model for SSA is    k  /  N      k  N    k/N   , where     s  n   =    ‚àë   k  =  1   r     a  k    s   n  -  k           subscript  s  n     superscript   subscript     k  1    r      subscript  a  k    subscript  s    n  k        s_{n}=\sum_{k=1}^{r}a_{k}s_{n-k}   (signal satisfying an LRR) and     s  n   =    ‚àë  k     C  k    œÅ  k  n    e   i  2  œÄ   œâ  k   n           subscript  s  n     subscript   k      subscript  C  k    superscript   subscript  œÅ  k   n    superscript  e    i  2  œÄ   subscript  œâ  k   n        s_{n}=\sum_{k}C_{k}\rho_{k}^{n}e^{i2\pi\omega_{k}n}   is noise. The model of AR is    œâ  k     subscript  œâ  k    \omega_{k}   . Despite these two models look similar they are very different. SSA considers AR as a noise component only. AR(1), which is red noise, is typical model of noise for Monte-Carlo SSA (Allen and Smith,1996 ).  SSA and spectral Fourier Analysis . In contrast with Fourier analysis with fixed basis of sine and cosine functions, SSA uses an adaptive basis generated by the time series itself. As a result, the underlying model in SSA is more general and SSA can extract amplitude-modulated sine wave components with frequencies different from    œÅ  k     subscript  œÅ  k    \rho_{k}   . SSA-related methods like ESPRIT can estimate frequencies with higher resolution than spectral Fourier analysis .  SSA and Linear Recurrence Relations . Let the signal be modeled by a series, which satisfies a linear recurrence relation    C  k     subscript  C  k    C_{k}   ; that is, a series that can be represented as sums of products of exponential, polynomial and sine wave functions. This includes the sum of dumped sinusoids model whose complex-valued form is    C  k     subscript  C  k    C_{k}   . SSA-related methods allow estimation of frequencies    n   n   n   and exponential factors   r   r   r   (Golyandina and Zhigljavsky, 2013, Sect 3.8). Coefficients    span   (   U  1   ,  ‚Ä¶  ,   U  r   )      span    subscript  U  1   normal-‚Ä¶   subscript  U  r      \mathop{\mathrm{span}}(U_{1},\ldots,U_{r})   can be estimated by the least squares method. Extension of the model, where     x  n   =    s  n   +   e  n         subscript  x  n      subscript  s  n    subscript  e  n      x_{n}=s_{n}+e_{n}   are replaced by polynomials of     s  n   =    ‚àë   k  =  1   r     a  k    s   n  -  k           subscript  s  n     superscript   subscript     k  1    r      subscript  a  k    subscript  s    n  k        s_{n}=\sum_{k=1}^{r}a_{k}s_{n-k}   , can be also considered within the SSA-related methods (Badeau et al., 2008).  SSA and Signal Subspace methods. SSA can be considered as a subspace-based method, since it allows estimation of the signal subspace of dimension    e  n     subscript  e  n    e_{n}   by   L   L   L   .  SSA and State Space Models . The main model behind SSA is     U  i   =    (   u  1   ,  ‚Ä¶  ,   u  L   )   T        subscript  U  i    superscript    subscript  u  1   normal-‚Ä¶   subscript  u  L    normal-T     U_{i}=(u_{1},\ldots,u_{L})^{\mathrm{T}}   , where     2  L   -  1        2  L   1    2L-1   and     x  ~   s     subscript   normal-~  x   s    \widetilde{x}_{s}   is noise. Formally, this model belongs to the general class of state space models. The specifics of SSA is in the facts that parameter estimation is a problem of secondary importance in SSA and the data analysis procedures in SSA are nonlinear as they are based on the SVD of either trajectory or lag-covariance matrix.  SSA and Independent Component Analysis (ICA). SSA is used in blind source separation by ICA as a preprocessing step (Pietil√§ et al., 2006). On the other hand, ICA can be used as a replacement of the SVD step in the SSA algorithm for achieving better separability (Golyandina and Zhigljavsky, 2013, Sect. 2.5.4).  SSA and Regression . SSA is able to extract polynomial and exponential trends. However, unlike regression, SSA does not assume any parametric model which may give significant advantage when an exploratory data analysis is performed with no obvious model in hand (Golyandina et al., 2001, Ch.1).  SSA and Linear Filters . The reconstruction of the series by SSA can be considered as adaptive linear filtration. If the window length    L  ‚â§  s  ‚â§  K        L  s       K     L\leq s\leq K   is small, then each eigenvector $U_i=(u_1, \ldots, u_L)^\mathrm{T}$ generates a linear filter of width $2L-1$ for reconstruction of the middle of the series $\widetilde{x}_s$ , $L\le s\le K$ . The filtration is non-causal. However, the so-called Last-point SSA can be used as a causal filter (Golyandina and Zhigljavsky 2013, Sect. 3.9).  SSA and Density Estimation . Since SSA can be used as a method of data smoothing it can be used as a method of non-parametric density estimation (Golyandina et al., 2012).  See also   Multitaper method  Short-time Fourier transform  Spectral density estimation   References   Akaike, H. (1969): "Fitting autoregressive models for prediction, " Ann. Inst. Stat. Math., 21, 243‚Äì247.    Allen, M.R., and A.W. Robertson (1996): "Distinguishing modulated oscillations from coloured noise in multivariate datasets", Clim. Dyn. , 12, 775‚Äì-784.    Allen, M.R. and L.A. Smith (1996) "Monte Carlo SSA: detecting irregular oscillations in the presence of colored noise". Journal of Climate , 9 (12), 3373‚Äì3404.    Badeau, R., G. Richard, and B. David (2008): "Performance of ESPRIT for Estimating Mixtures of Complex Exponentials Modulated by Polynomials". IEEE Transactions on signal processing , 56(2), 492‚Äì504.    Barnett, T. P., and K. Hasselmann (1979): "Techniques of linear prediction, with application to oceanic and atmospheric fields in the tropical Pacific, " Rev. Geophys., 17, 949‚Äì968.    Bozzo, E., R. Carniel and D. Fasino (2010): "Relationship between singular spectrum analysis and Fourier analysis: Theory and application to the monitoring of volcanic activity", Comput. Math. Appl. 60(3), 812‚Äì820    Broomhead, D.S., and G.P. King (1986a): "Extracting qualitative dynamics from experimental data", Physica D , 20, 217‚Äì236.    Broomhead, D.S., and G. P. King (1986b): "On the qualitative analysis of experimental dynamical systems". Nonlinear Phenomena and Chaos , Sarkar S (Ed.), Adam Hilger, Bristol, 113-‚Äì144.    Colebrook, J. M., (1978): "Continuous plankton records: Zooplankton and environment, Northeast Atlantic and North Sea," Oceanol. Acta , 1, 9‚Äì23.    Danilov, D. and Zhigljavsky, A. (Eds.) (1997): Principal Components of Time Series: the Caterpillar method , University of St. Petersburg Press. (In Russian.)    Elsner, J.B. and Tsonis, A.A. (1996): Singular Spectrum Analysis. A New Tool in Time Series Analysis , Plenum Press.    Fraedrich, K. (1986) "Estimating dimensions of weather and climate attractors". J. Atmos. Sci. 43, 419‚Äì432.    Ghil, M., and R. Vautard (1991): "Interdecadal oscillations and the warming trend in global temperature time series", Nature , 350, 324‚Äì327.    Ghil, M. and Jiang, N. (1998): "Recent forecast skill for the El Nin ÃÉo/Southern Oscillation ", Geophys. Res. Lett. , 25, 171‚Äì174, 1998.    Ghil, M., R. M. Allen, M. D. Dettinger, K. Ide, D. Kondrashov, et al. (2002) "Advanced spectral methods for climatic time series" , Rev. Geophys. 40(1), 3.1‚Äì3.41.    Golyandina, N., V. Nekrutkin and A. Zhigljavsky (2001): Analysis of Time Series Structure: SSA and related techniques . Chapman and Hall/CRC. ISBN 1-58488-194-1.    Golyandina, N., and E. Osipov (2007) "The ‚ÄòCaterpillar‚Äô-SSA method for analysis of time series with missing values", J. Stat. Plan. Inference 137(8), 2642‚Äì2653.    Golyandina, N., A. Pepelyshev and A. Steland (2012): "New approaches to nonparametric density estimation and selection of smoothing parameters", Comput. Stat. Data Anal. 56(7), 2206‚Äì2218.    Golyandina, N. and D. Stepanov (2005): "SSA-based approaches to analysis and forecast of multidimensional time series" . In: Proceedings of the 5th St.Petersburg Workshop on Simulation, June 26-July 2, 2005 , St. Petersburg State University, St. Petersburg, pp.¬†293‚Äì298.    Golyandina, N. and K. Usevich (2010): "2D-extension of Singular Spectrum Analysis: algorithm and elements of theory". In: Matrix Methods: Theory, Algorithms and Applications (Eds. V.Olshevsky and E.Tyrtyshnikov). World Scientific Publishing, 449‚Äì473.    Golyandina, N., and A. Zhigljavsky (2013) Singular Spectrum Analysis for time series . Springer Briefs in Statistics, Springer, ISBN 978-3-642-34912-6.    Groth, A., and M. Ghil (2011): "Multivariate singular spectrum analysis and the road to phase synchronization", Phys Rev E 84(3 Pt 2), 036206.    Harris, T. and H. Yan (2010): "Filtering and frequency interpretations of singular spectrum analysis". Physica D 239, 1958‚Äì1967.    Hassani, H.and D. Thomakos, (2010): "A Review on Singular Spectrum Analysis for Economic and Financial Time Series". Statistics and Its Interface 3(3), 377-397.    Hassani, H., A. Soofi and A. Zhigljavsky (2011): "Predicting Daily Exchange Rate with Singular Spectrum Analysis". Nonlinear Analysis: Real World Applications 11, 2023-2034.    Hassani, H., Z. Xu and A. Zhigljavsky (2011): "Singular spectrum analysis based on the perturbation theory". Nonlinear Analysis: Real World Applications 12 (5), 2752-2766.    Hassani, H., S. Heravi and A. Zhigljavsky (2012): " Forecasting UK industrial production with multivariate singular spectrum analysis". Journal of Forecasting 10.1002/for.2244    Hassani, H., A. Zhigljavsky., K. Patterson and A. Soofi (2011): " A comprehensive causality test based on the singular spectrum analysis". In: Illari, P.M., Russo, F., Williamson, J. (eds.) Causality in Science , 1st edn., p.¬†379. Oxford University Press, London.    Hassani, H., and Mahmoudvand, R. (2013). Multivariate Singular Spectrum Analysis: A General View and New Vector Forecasting Approach;. '' International Journal of Energy and Statistics'' 1(1), 55-83.    Keppenne, C. L. and M. Ghil (1993): "Adaptive filtering and prediction of noisy multivariate signals: An application to subannual variability in atmospheric angular momentum," Intl. J. Bifurcation & Chaos , 3, 625‚Äì634.    Kondrashov, D., and M. Ghil (2006): "Spatio-temporal filling of missing points in geophysical data sets" , Nonlin. Processes Geophys. , 13, 151‚Äì159.    Kondrashov, D., Y. Shprits, M. Ghil, 2010: " Gap Filling of Solar Wind Data by Singular Spectrum Analysis," Geophys. Res. Lett , 37, L15101,    Mohammad, Y., and T. Nishida (2011) "On comparing SSA-based change point discovery algorithms". IEEE SII , 938‚Äì945.    Moskvina, V., and A. Zhigljavsky (2003) "An algorithm based on singular spectrum analysis for change-point detection". Commun Stat Simul Comput 32, 319‚Äì352.    Nekrutkin, V. (2010) "Perturbation expansions of signal subspaces for long signals". J. Stat. Interface 3, 297‚Äì319.    Patterson, K., H. Hassani, S. Heravi and A. Zhigljavsky (2011) "Multivariate singular spectrum analysis for forecasting revisions to real-time data". Journal of Applied Statistics 38 (10), 2183-2211.    Penland, C., Ghil, M., and Weickmann, K. M. (1991): "Adaptive filtering and maximum entropy spectra, with application to changes in atmospheric angular momentum," J. Geophys. Res. , 96, 22659‚Äì22671.    Pietil√§, A., M. El-Segaier, R. Vig√°rio and E. Pesonen (2006) "Blind source separation of cardiac murmurs from heart recordings". In: Rosca J, et al. (eds) Independent Component Analysis and Blind Signal Separation, Lecture Notes in Computer Science , vol 3889, Springer, pp 470‚Äì477.    de Prony, G. (1795) "Essai exp√©rimental et analytique sur les lois de la dilatabilit√© des fluides √©lastiques et sur celles de la force expansive de la vapeur de l‚Äôeau et la vapeur de l‚Äôalkool √† diff√©rentes temp√©ratures". J. de l‚ÄôEcole Polytechnique , 1(2), 24‚Äì76.    Schoellhamer, D. (2001) "Singular spectrum analysis for time series with missing data". Geophys. Res. Lett. 28(16), 3187‚Äì3190.    Thomakos, D. (2010) "Median Unbiased Optimal Smoothing and Trend. Extraction". Journal of Modern Applied Statistical Methods 9,144-159.    Vautard, R., and M. Ghil (1989): "Singular spectrum analysis in nonlinear dynamics, with applications to paleoclimatic time series", Physica D , 35, 395‚Äì424.    Vautard, R., Yiou, P., and M. Ghil (1992): "Singular-spectrum analysis: A toolkit for short, noisy chaotic signals", Physica D , 58, 95-126.    Weare, B. C., and J. N. Nasstrom (1982): "Examples of extended empirical orthogonal function analyses," Mon. Weather Rev. , 110, 784‚Äì812.    Zhigljavsky, A. (Guest Editor) (2010) "Special issue on theory and practice in singular spectrum analysis of time series". Stat. Interface 3(3)   External links   Singular Spectrum Analysis‚ÄìMulti-taper Method (SSA-MTM) Toolkit freeware from UCLA.  kSpectra Toolkit for Mac OS X from SpectraWorks.  Yet another SSAwiki page.  Caterpillar-SSA Papers and software from Gistat Group.  Efficient implementation of SSA in R  SSA and Phase Synchronisation in R  Singular Spectrum Analysis Excel Demo With VBA   "  Category:Time series analysis  Category:Signal processing  Category:Time domain analysis   