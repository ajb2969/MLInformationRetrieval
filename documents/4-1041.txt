   Conditional entropy      Conditional entropy   (Figure)  Venn diagram for various information measures associated with correlated variables X and Y. The area contained by both circles is the joint entropy H(X,Y). The circle on the left (red and violet) is the individual entropy H(X), with the red being the conditional entropy H(X|Y). The circle on the right (blue and violet) is H(Y), with the blue being H(Y|X). The violet is the mutual information I(X;Y).   In information theory , the conditional entropy (or equivocation ) quantifies the amount of information needed to describe the outcome of a random variable    Y   Y   Y   given that the value of another random variable   X   X   X   is known. Here, information is measured in shannons , nats , or hartleys . The entropy of   Y   Y   Y   conditioned on   X   X   X    is written as    H   (  Y  |  X  )      fragments  H   fragments  normal-(  Y  normal-|  X  normal-)     H(Y|X)   .  Definition  If    H   (  Y  |  X  =  x  )      fragments  H   fragments  normal-(  Y  normal-|  X   x  normal-)     H(Y|X=x)   is the entropy of the variable   Y   Y   Y   conditioned on the variable   X   X   X   taking a certain value   x   x   x   , then    H   (  Y  |  X  )      fragments  H   fragments  normal-(  Y  normal-|  X  normal-)     H(Y|X)   is the result of averaging    H   (  Y  |  X  =  x  )      fragments  H   fragments  normal-(  Y  normal-|  X   x  normal-)     H(Y|X=x)   over all possible values   x   x   x   that   X   X   X   may take.  Given discrete random variables   X   X   X   with domain   ùí≥   ùí≥   \mathcal{X}   and   Y   Y   Y   with domain   ùí¥   ùí¥   \mathcal{Y}   , the conditional entropy of   Y   Y   Y   given   X   X   X   is defined as: 1      H   (  Y  |  X  )      fragments  H   fragments  normal-(  Y  normal-|  X  normal-)     \displaystyle H(Y|X)     Note: It is understood that the expressions 0 log 0 and 0 log ( c /0) for fixed c >0 should be treated as being equal to zero.      H   (  Y  |  X  )   =  0     fragments  H   fragments  normal-(  Y  normal-|  X  normal-)    0    H(Y|X)=0   if and only if the value of   Y   Y   Y   is completely determined by the value of   X   X   X   . Conversely,    H   (  Y  |  X  )   =  H   (  Y  )      fragments  H   fragments  normal-(  Y  normal-|  X  normal-)    H   fragments  normal-(  Y  normal-)     H(Y|X)=H(Y)   if and only if   Y   Y   Y   and   X   X   X   are independent random variables .  Chain rule  Assume that the combined system determined by two random variables X and Y has joint entropy     H   (  X  ,  Y  )       H   X  Y     H(X,Y)   , that is, we need    H   (  X  ,  Y  )       H   X  Y     H(X,Y)   bits of information to describe its exact state. Now if we first learn the value of   X   X   X   , we have gained    H   (  X  )       H  X    H(X)   bits of information. Once   X   X   X   is known, we only need     H   (  X  ,  Y  )    -   H   (  X  )          H   X  Y      H  X     H(X,Y)-H(X)   bits to describe the state of the whole system. This quantity is exactly    H   (  Y  |  X  )      fragments  H   fragments  normal-(  Y  normal-|  X  normal-)     H(Y|X)   , which gives the chain rule of conditional entropy:      H   (  Y  |  X  )   =  H   (  X  ,  Y  )   -  H   (  X  )   .     fragments  H   fragments  normal-(  Y  normal-|  X  normal-)    H   fragments  normal-(  X  normal-,  Y  normal-)    H   fragments  normal-(  X  normal-)   normal-.    H(Y|X)\,=\,H(X,Y)-H(X)\,.     The chain rule follows from the above definition of conditional entropy:      H   (  Y  |  X  )   =   ‚àë    x  ‚àà  ùí≥   ,   y  ‚àà  ùí¥     p   (  x  ,  y  )   log    p   (  x  )     p   (  x  ,  y  )        fragments  H   fragments  normal-(  Y  normal-|  X  normal-)     subscript    formulae-sequence    x  ùí≥     y  ùí¥     p   fragments  normal-(  x  normal-,  y  normal-)        p  x     p   x  y       H(Y|X)=\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log\frac{p(x)}{p(x,y)}          =    -    ‚àë    x  ‚àà  ùí≥   ,   y  ‚àà  ùí¥      p   (  x  ,  y  )     log   p    (  x  ,  y  )      +    ‚àë    x  ‚àà  ùí≥   ,   y  ‚àà  ùí¥      p   (  x  ,  y  )     log   p    (  x  )          absent        subscript    formulae-sequence    x  ùí≥     y  ùí¥       p   x  y     p    x  y        subscript    formulae-sequence    x  ùí≥     y  ùí¥       p   x  y     p   x       =-\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log\,p(x,y)+\sum_{x\in\mathcal{%
 X},y\in\mathcal{Y}}p(x,y)\log\,p(x)          =    H   (  X  ,  Y  )    +    ‚àë   x  ‚àà  ùí≥     p   (  x  )     log   p    (  x  )          absent      H   X  Y      subscript     x  ùí≥      p  x    p   x       =H(X,Y)+\sum_{x\in\mathcal{X}}p(x)\log\,p(x)           =    H   (  X  ,  Y  )    -   H   (  X  )      .      absent      H   X  Y      H  X      =H(X,Y)-H(X).     Bayes' rule  Bayes' rule for conditional entropy states      H   (  Y  |  X  )   =  H   (  X  |  Y  )   -  H   (  X  )   +  H   (  Y  )   .     fragments  H   fragments  normal-(  Y  normal-|  X  normal-)    H   fragments  normal-(  X  normal-|  Y  normal-)    H   fragments  normal-(  X  normal-)    H   fragments  normal-(  Y  normal-)   normal-.    H(Y|X)\,=\,H(X|Y)-H(X)+H(Y)\,.     Proof.     H   (  Y  |  X  )   =  H   (  X  ,  Y  )   -  H   (  X  )      fragments  H   fragments  normal-(  Y  normal-|  X  normal-)    H   fragments  normal-(  X  normal-,  Y  normal-)    H   fragments  normal-(  X  normal-)     H(Y|X)=H(X,Y)-H(X)   and    H   (  X  |  Y  )   =  H   (  Y  ,  X  )   -  H   (  Y  )      fragments  H   fragments  normal-(  X  normal-|  Y  normal-)    H   fragments  normal-(  Y  normal-,  X  normal-)    H   fragments  normal-(  Y  normal-)     H(X|Y)=H(Y,X)-H(Y)   . Symmetry implies     H   (  X  ,  Y  )    =   H   (  Y  ,  X  )          H   X  Y      H   Y  X      H(X,Y)=H(Y,X)   . Subtracting the two equations implies Bayes' rule.  Generalization to quantum theory  In quantum information theory , the conditional entropy is generalized to the conditional quantum entropy . The latter can take negative values, unlike its classical counterpart. Bayes' rule does not hold for conditional quantum entropy , since     H   (  X  ,  Y  )    ‚â†   H   (  Y  ,  X  )          H   X  Y      H   Y  X      H(X,Y)\neq H(Y,X)   .  Other properties  For any   X   X   X   and   Y   Y   Y   :      H   (  Y  |  X  )   ‚â§  H   (  Y  )      fragments  H   fragments  normal-(  Y  normal-|  X  normal-)    H   fragments  normal-(  Y  normal-)     H(Y|X)\leq H(Y)\,         H   (  X  ,  Y  )   =  H   (  X  |  Y  )   +  H   (  Y  |  X  )   +  I   (  X  ;  Y  )   ,     fragments  H   fragments  normal-(  X  normal-,  Y  normal-)    H   fragments  normal-(  X  normal-|  Y  normal-)    H   fragments  normal-(  Y  normal-|  X  normal-)    I   fragments  normal-(  X  normal-;  Y  normal-)   normal-,    H(X,Y)=H(X|Y)+H(Y|X)+I(X;Y),\qquad           H   (  X  ,  Y  )    =     H   (  X  )    +   H   (  Y  )     -   I   (  X  ;  Y  )      ,        H   X  Y          H  X     H  Y      I   X  Y       H(X,Y)=H(X)+H(Y)-I(X;Y),\,           I   (  X  ;  Y  )    ‚â§   H   (  X  )     ,        I   X  Y      H  X     I(X;Y)\leq H(X),\,     where    I   (  X  ;  Y  )       I   X  Y     I(X;Y)   is the mutual information between   X   X   X   and   Y   Y   Y   .  For independent   X   X   X   and   Y   Y   Y   :      H   (  Y  |  X  )   =  H   (  Y  )   and  H   (  X  |  Y  )   =  H   (  X  )      fragments  H   fragments  normal-(  Y  normal-|  X  normal-)    H   fragments  normal-(  Y  normal-)   and  H   fragments  normal-(  X  normal-|  Y  normal-)    H   fragments  normal-(  X  normal-)     H(Y|X)=H(Y)\text{ and }H(X|Y)=H(X)\,     Although the specific-conditional entropy,    H   (  X  |  Y  =  y  )      fragments  H   fragments  normal-(  X  normal-|  Y   y  normal-)     H(X|Y=y)   , can be either less or greater than    H   (  X  )       H  X    H(X)   ,    H   (  X  |  Y  )      fragments  H   fragments  normal-(  X  normal-|  Y  normal-)     H(X|Y)   can never exceed    H   (  X  )       H  X    H(X)   .  References  See also   Entropy (information theory)  Mutual information  Conditional quantum entropy  Variation of information  Entropy power inequality  Likelihood function   "  Category:Entropy and information  Category:Information theory     ‚Ü©     