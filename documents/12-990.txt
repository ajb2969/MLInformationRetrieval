   Orthogonality principle      Orthogonality principle   In statistics and signal processing , the orthogonality principle is a necessary and sufficient condition for the optimality of a Bayesian estimator . Loosely stated, the orthogonality principle says that the error vector of the optimal estimator (in a mean square error sense) is orthogonal to any possible estimator. The orthogonality principle is most commonly stated for linear estimators, but more general formulations are possible. Since the principle is a necessary and sufficient condition for optimality, it can be used to find the minimum mean square error estimator.  Orthogonality principle for linear estimators  The orthogonality principle is most commonly used in the setting of linear estimation. 1 In this context, let x be an unknown random vector which is to be estimated based on the observation vector y . One wishes to construct a linear estimator     x  ^   =    H  y   +  c        normal-^  x       H  y   c     \hat{x}=Hy+c   for some matrix H and vector c . Then, the orthogonality principle states that an estimator    x  ^     normal-^  x    \hat{x}   achieves minimum mean square error if and only if         E   {    (    x  ^   -  x   )    y  T    }    =  0   ,        E        normal-^  x   x    superscript  y  T      0    E\{(\hat{x}-x)y^{T}\}=0,   and       E   {    x  ^   -  x   }    =  0.        E      normal-^  x   x     0.    E\{\hat{x}-x\}=0.      If x and y have zero mean, then it suffices to require the first condition.  Example  Suppose x is a Gaussian random variable with mean m and variance     σ  x  2   .     superscript   subscript  σ  x   2    \sigma_{x}^{2}.   Also suppose we observe a value     y  =   x  +  w    ,      y    x  w     y=x+w,   where w is Gaussian noise which is independent of x and has mean 0 and variance     σ  w  2   .     superscript   subscript  σ  w   2    \sigma_{w}^{2}.   We wish to find a linear estimator     x  ^   =    h  y   +  c        normal-^  x       h  y   c     \hat{x}=hy+c   minimizing the MSE. Substituting the expression     x  ^   =    h  y   +  c        normal-^  x       h  y   c     \hat{x}=hy+c   into the two requirements of the orthogonality principle, we obtain      0  =   E   {    (    x  ^   -  x   )   y   }        0    E        normal-^  x   x   y       0=E\{(\hat{x}-x)y\}         0  =   E   {    (     h  x   +   h  w   +  c   -  x   )    (   x  +  w   )    }        0    E           h  x     h  w   c   x     x  w        0=E\{(hx+hw+c-x)(x+w)\}         0  =     h   (    σ  x  2   +   σ  w  2    )    +   c  m    -   σ  x  2        0        h     superscript   subscript  σ  x   2    superscript   subscript  σ  w   2       c  m     superscript   subscript  σ  x   2      0=h(\sigma_{x}^{2}+\sigma_{w}^{2})+cm-\sigma_{x}^{2}   and      0  =   E   {    x  ^   -  x   }        0    E      normal-^  x   x       0=E\{\hat{x}-x\}         0  =   E   {     h  x   +   h  w   +  c   -  x   }        0    E         h  x     h  w   c   x       0=E\{hx+hw+c-x\}          0  =     (   h  -  1   )   m   +  c    .      0        h  1   m   c     0=(h-1)m+c.   Solving these two linear equations for h and c results in        h  =     σ  x  2   -   m  2      (    σ  x  2   -   m  2    )   +   σ  w  2      ,   c  =     σ  w  2     (    σ  x  2   -   m  2    )   +   σ  w  2     m     ,     formulae-sequence    h       superscript   subscript  σ  x   2    superscript  m  2         superscript   subscript  σ  x   2    superscript  m  2     superscript   subscript  σ  w   2        c       superscript   subscript  σ  w   2        superscript   subscript  σ  x   2    superscript  m  2     superscript   subscript  σ  w   2     m      h=\frac{\sigma_{x}^{2}-m^{2}}{(\sigma_{x}^{2}-m^{2})+\sigma_{w}^{2}},\quad c=%
 \frac{\sigma_{w}^{2}}{(\sigma_{x}^{2}-m^{2})+\sigma_{w}^{2}}m,   so that the linear minimum mean square error estimator is given by        x  ^   =       σ  x  2   -   m  2      (    σ  x  2   -   m  2    )   +   σ  w  2     y   +     σ  w  2     (    σ  x  2   -   m  2    )   +   σ  w  2     m     .       normal-^  x            superscript   subscript  σ  x   2    superscript  m  2         superscript   subscript  σ  x   2    superscript  m  2     superscript   subscript  σ  w   2     y        superscript   subscript  σ  w   2        superscript   subscript  σ  x   2    superscript  m  2     superscript   subscript  σ  w   2     m      \hat{x}=\frac{\sigma_{x}^{2}-m^{2}}{(\sigma_{x}^{2}-m^{2})+\sigma_{w}^{2}}y+%
 \frac{\sigma_{w}^{2}}{(\sigma_{x}^{2}-m^{2})+\sigma_{w}^{2}}m.     This estimator can be interpreted as a weighted average between the noisy measurements y and the prior expected value m . If the noise variance    σ  w  2     superscript   subscript  σ  w   2    \sigma_{w}^{2}   is low compared with the variance of the prior minus the squared mean     σ  x  2   -   m  2        superscript   subscript  σ  x   2    superscript  m  2     \sigma_{x}^{2}-m^{2}   (corresponding to a high SNR ), then most of the weight is given to the measurements y , which are deemed more reliable than the prior information. Conversely, if the noise variance is relatively higher, then the estimate will be close to m , as the measurements are not reliable enough to outweigh the prior information.  Finally, note that because the variables x and y are jointly Gaussian, the minimum MSE estimator is linear. 2 Therefore, in this case, the estimator above minimizes the MSE among all estimators, not only linear estimators.  General formulation  Let   V   V   V   be a Hilbert space of random variables with an inner product defined by     ⟨  x  ,  y  ⟩   =   E   {    x  H   y   }         x  y     E      superscript  x  H   y       \langle x,y\rangle=E\{x^{H}y\}   . Suppose   W   W   W   is a closed subspace of   V   V   V   , representing the space of all possible estimators. One wishes to find a vector     x  ^   ∈  W       normal-^  x   W    \hat{x}\in W   which will approximate a vector    x  ∈  V      x  V    x\in V   . More accurately, one would like to minimize the mean squared error (MSE)    E    ∥   x  -   x  ^    ∥   2       E   superscript   norm    x   normal-^  x     2     E\|x-\hat{x}\|^{2}   between    x  ^     normal-^  x    \hat{x}   and   x   x   x   .  In the special case of linear estimators described above, the space   V   V   V   is the set of all functions of   x   x   x   and   y   y   y   , while   W   W   W   is the set of linear estimators, i.e., linear functions of   y   y   y   only. Other settings which can be formulated in this way include the subspace of causal linear filters and the subspace of all (possibly nonlinear) estimators.  Geometrically, we can see this problem by the following simple case where   W   W   W   is a one-dimensional subspace:  (Figure)  Orthogonality principle.png   We want to find the closest approximation to the vector   x   x   x   by a vector    x  ^     normal-^  x    \hat{x}   in the space   W   W   W   . From the geometric interpretation, it is intuitive that the best approximation, or smallest error, occurs when the error vector,   e   e   e   , is orthogonal to vectors in the space   W   W   W   .  More accurately, the general orthogonality principle states the following: Given a closed subspace   W   W   W   of estimators within a Hilbert space   V   V   V   and an element   x   x   x   in   V   V   V   , an element     x  ^   ∈  W       normal-^  x   W    \hat{x}\in W   achieves minimum MSE among all elements in   W   W   W   if and only if     E   {    (   x  -   x  ^    )    y  T    }    =  0        E       x   normal-^  x     superscript  y  T      0    E\{(x-\hat{x})y^{T}\}=0   for all     y  ∈  W   .      y  W    y\in W.     Stated in such a manner, this principle is simply a statement of the Hilbert projection theorem . Nevertheless, the extensive use of this result in signal processing has resulted in the name "orthogonality principle."  A solution to error minimization problems  The following is one way to find the minimum mean square error estimator by using the orthogonality principle.  We want to be able to approximate a vector   x   x   x   by      x  =    x  ^   +   e        x     normal-^  x   e     x=\hat{x}+e\,     where       x  ^   =    ∑  i     c  i    p  i          normal-^  x     subscript   i      subscript  c  i    subscript  p  i       \hat{x}=\sum_{i}c_{i}p_{i}     is the approximation of   x   x   x   as a linear combination of vectors in the subspace   W   W   W   spanned by      p  1   ,   p  2   ,  …   .      subscript  p  1    subscript  p  2   normal-…    p_{1},p_{2},\ldots.   Therefore, we want to be able to solve for the coefficients,    c  i     subscript  c  i    c_{i}   , so that we may write our approximation in known terms.  By the orthogonality theorem, the square norm of the error vector,     ∥  e  ∥   2     superscript   norm  e   2    \left\|e\right\|^{2}   , is minimized when, for all j ,       ⟨   x  -    ∑  i     c  i    p  i      ,   p  j   ⟩   =  0.         x    subscript   i      subscript  c  i    subscript  p  i       subscript  p  j    0.    \left\langle x-\sum_{i}c_{i}p_{i},p_{j}\right\rangle=0.     Developing this equation, we obtain        ⟨  x  ,   p  j   ⟩   =   ⟨    ∑  i     c  i    p  i     ,   p  j   ⟩   =    ∑  i     c  i    ⟨   p  i   ,   p  j   ⟩      .         x   subscript  p  j       subscript   i      subscript  c  i    subscript  p  i      subscript  p  j           subscript   i      subscript  c  i     subscript  p  i    subscript  p  j         \left\langle x,p_{j}\right\rangle=\left\langle\sum_{i}c_{i}p_{i},p_{j}\right%
 \rangle=\sum_{i}c_{i}\left\langle p_{i},p_{j}\right\rangle.     If there is a finite number   n   n   n   of vectors    p  i     subscript  p  i    p_{i}   , one can write this equation in matrix form as        [      ⟨  x  ,   p  1   ⟩        ⟨  x  ,   p  2   ⟩       ⋮       ⟨  x  ,   p  n   ⟩      ]   =    [      ⟨   p  1   ,   p  1   ⟩      ⟨   p  2   ,   p  1   ⟩     ⋯     ⟨   p  n   ,   p  1   ⟩        ⟨   p  1   ,   p  2   ⟩      ⟨   p  2   ,   p  2   ⟩     ⋯     ⟨   p  n   ,   p  2   ⟩       ⋮    ⋮    ⋱    ⋮       ⟨   p  1   ,   p  n   ⟩      ⟨   p  2   ,   p  n   ⟩     ⋯     ⟨   p  n   ,   p  n   ⟩      ]    [      c  1        c  2       ⋮       c  n      ]     .         x   subscript  p  1       x   subscript  p  2      normal-⋮     x   subscript  p  n            subscript  p  1    subscript  p  1      subscript  p  2    subscript  p  1    normal-⋯    subscript  p  n    subscript  p  1        subscript  p  1    subscript  p  2      subscript  p  2    subscript  p  2    normal-⋯    subscript  p  n    subscript  p  2      normal-⋮  normal-⋮  normal-⋱  normal-⋮      subscript  p  1    subscript  p  n      subscript  p  2    subscript  p  n    normal-⋯    subscript  p  n    subscript  p  n         subscript  c  1      subscript  c  2     normal-⋮     subscript  c  n        \begin{bmatrix}\left\langle x,p_{1}\right\rangle\\
 \left\langle x,p_{2}\right\rangle\\
 \vdots\\
 \left\langle x,p_{n}\right\rangle\end{bmatrix}=\begin{bmatrix}\left\langle p_{%
 1},p_{1}\right\rangle&\left\langle p_{2},p_{1}\right\rangle&\cdots&\left%
 \langle p_{n},p_{1}\right\rangle\\
 \left\langle p_{1},p_{2}\right\rangle&\left\langle p_{2},p_{2}\right\rangle&%
 \cdots&\left\langle p_{n},p_{2}\right\rangle\\
 \vdots&\vdots&\ddots&\vdots\\
 \left\langle p_{1},p_{n}\right\rangle&\left\langle p_{2},p_{n}\right\rangle&%
 \cdots&\left\langle p_{n},p_{n}\right\rangle\end{bmatrix}\begin{bmatrix}c_{1}%
 \\
 c_{2}\\
 \vdots\\
 c_{n}\end{bmatrix}.     Assuming the    p  i     subscript  p  i    p_{i}   are linearly independent , the Gramian matrix can be inverted to obtain        [      c  1        c  2       ⋮       c  n      ]   =     [      ⟨   p  1   ,   p  1   ⟩      ⟨   p  2   ,   p  1   ⟩     ⋯     ⟨   p  n   ,   p  1   ⟩        ⟨   p  1   ,   p  2   ⟩      ⟨   p  2   ,   p  2   ⟩     ⋯     ⟨   p  n   ,   p  2   ⟩       ⋮    ⋮    ⋱    ⋮       ⟨   p  1   ,   p  n   ⟩      ⟨   p  2   ,   p  n   ⟩     ⋯     ⟨   p  n   ,   p  n   ⟩      ]    -  1     [      ⟨  x  ,   p  1   ⟩        ⟨  x  ,   p  2   ⟩       ⋮       ⟨  x  ,   p  n   ⟩      ]     ,         subscript  c  1      subscript  c  2     normal-⋮     subscript  c  n        superscript      subscript  p  1    subscript  p  1      subscript  p  2    subscript  p  1    normal-⋯    subscript  p  n    subscript  p  1        subscript  p  1    subscript  p  2      subscript  p  2    subscript  p  2    normal-⋯    subscript  p  n    subscript  p  2      normal-⋮  normal-⋮  normal-⋱  normal-⋮      subscript  p  1    subscript  p  n      subscript  p  2    subscript  p  n    normal-⋯    subscript  p  n    subscript  p  n        1       x   subscript  p  1       x   subscript  p  2      normal-⋮     x   subscript  p  n         \begin{bmatrix}c_{1}\\
 c_{2}\\
 \vdots\\
 c_{n}\end{bmatrix}=\begin{bmatrix}\left\langle p_{1},p_{1}\right\rangle&\left%
 \langle p_{2},p_{1}\right\rangle&\cdots&\left\langle p_{n},p_{1}\right\rangle%
 \\
 \left\langle p_{1},p_{2}\right\rangle&\left\langle p_{2},p_{2}\right\rangle&%
 \cdots&\left\langle p_{n},p_{2}\right\rangle\\
 \vdots&\vdots&\ddots&\vdots\\
 \left\langle p_{1},p_{n}\right\rangle&\left\langle p_{2},p_{n}\right\rangle&%
 \cdots&\left\langle p_{n},p_{n}\right\rangle\end{bmatrix}^{-1}\begin{bmatrix}%
 \left\langle x,p_{1}\right\rangle\\
 \left\langle x,p_{2}\right\rangle\\
 \vdots\\
 \left\langle x,p_{n}\right\rangle\end{bmatrix},     thus providing an expression for the coefficients    c  i     subscript  c  i    c_{i}   of the minimum mean square error estimator.  See also   Minimum mean square error  Hilbert projection theorem   Notes  References      "  Category:Estimation theory  Category:Statistical principles     Kay, p.386 ↩  See the article minimum mean square error . ↩     