   Renewal theory      Renewal theory   Renewal theory is the branch of probability theory that generalizes Poisson processes for arbitrary holding times . Applications include calculating the best strategy for replacing worn-out machinery in a factory and comparing the long-term benefits of different insurance policies.  Renewal processes  Introduction  A renewal process is a generalization of the Poisson process. In essence, the Poisson process is a continuous-time Markov process on the positive integers (usually starting at zero) which has independent identically distributed  holding times at each integer   i   i   i   ( exponentially distributed ) before advancing (with probability 1) to the next integer    i  +  1      i  1    i+1   . In the same informal spirit, we may define a renewal process to be the same thing, except that the holding times take on a more general distribution. (Note however that the independence and identical distribution ( IID ) property of the holding times is retained).  Formal definition  (Figure)  Sample evolution of a renewal process with holding times  S i and jump times J n .   Let     S  1   ,   S  2   ,   S  3   ,   S  4   ,   S  5   ,  ‚Ä¶      subscript  S  1    subscript  S  2    subscript  S  3    subscript  S  4    subscript  S  5   normal-‚Ä¶    S_{1},S_{2},S_{3},S_{4},S_{5},\ldots   be a sequence of positive independent identically distributed  random variables such that       0  <   ùîº   [   S  i   ]    <  ‚àû   .        0    ùîº   delimited-[]   subscript  S  i              0<\mathbb{E}[S_{i}]<\infty.     We refer to the random variable    S  i     subscript  S  i    S_{i}   as the "   i   i   i   th" holding time .    ùîº   [   S  i   ]       ùîº   delimited-[]   subscript  S  i      \mathbb{E}[S_{i}]   is the expectation of    S  i     subscript  S  i    S_{i}   .  Define for each n > 0 :        J  n   =    ‚àë   i  =  1   n    S  i     ,       subscript  J  n     superscript   subscript     i  1    n    subscript  S  i      J_{n}=\sum_{i=1}^{n}S_{i},     each    J  n     subscript  J  n    J_{n}   referred to as the "   n   n   n   th" jump time and the intervals      [   J  n   ,   J   n  +  1    ]      subscript  J  n    subscript  J    n  1      [J_{n},J_{n+1}]     being called renewal intervals .  Then the random variable     (   X  t   )    t  ‚â•  0      subscript   subscript  X  t     t  0     (X_{t})_{t\geq 0}   given by       X  t   =    ‚àë   n  =  1   ‚àû    ùïÄ   {   J  n   ‚â§  t  }     =   sup   {  n  :    J  n   ‚â§   t    }           subscript  X  t     subscript   superscript       n  1     subscript  ùïÄ   fragments  normal-{   subscript  J  n    t  normal-}           supremum   conditional-set  n     subscript  J  n   t        X_{t}=\sum^{\infty}_{n=1}\mathbb{I}_{\{J_{n}\leq t\}}=\sup\left\{\,n:J_{n}\leq
 t%
 \,\right\}     (where   ùïÄ   ùïÄ   \mathbb{I}   is the indicator function ) represents the number of jumps that have occurred by time t , and is called a renewal process .  Interpretation  If one considers events occurring at random times, one may choose to think of the holding times     {   S  i   :   i  ‚â•  1   }     conditional-set   subscript  S  i     i  1     \{S_{i}:i\geq 1\}   as the random time elapsed between two subsequent events. For example, if the renewal process is modelling the breakdown of different machines, then the holding times represent the time between one machine breaking down before another one does.  Renewal-reward processes  (Figure)  Sample evolution of a renewal-reward process with holding times  S i , jump times J n and rewards W i   Let     W  1   ,   W  2   ,  ‚Ä¶      subscript  W  1    subscript  W  2   normal-‚Ä¶    W_{1},W_{2},\ldots   be a sequence of IID random variables ( rewards ) satisfying        ùîº   |   W  i   |    <  ‚àû   .        ùîº     subscript  W  i        \mathbb{E}|W_{i}|<\infty.\,     Then the random variable       Y  t   =    ‚àë   i  =  1    X  t     W  i         subscript  Y  t     superscript   subscript     i  1     subscript  X  t     subscript  W  i      Y_{t}=\sum_{i=1}^{X_{t}}W_{i}     is called a renewal-reward process . Note that unlike the    S  i     subscript  S  i    S_{i}   , each    W  i     subscript  W  i    W_{i}   may take negative values as well as positive values.  The random variable    Y  t     subscript  Y  t    Y_{t}   depends on two sequences: the holding times     S  1   ,   S  2   ,  ‚Ä¶      subscript  S  1    subscript  S  2   normal-‚Ä¶    S_{1},S_{2},\ldots   and the rewards     W  1   ,   W  2   ,  ‚Ä¶      subscript  W  1    subscript  W  2   normal-‚Ä¶    W_{1},W_{2},\ldots   These two sequences need not be independent. In particular,    W  i     subscript  W  i    W_{i}   may be a function of    S  i     subscript  S  i    S_{i}   .  Interpretation  In the context of the above interpretation of the holding times as the time between successive malfunctions of a machine, the "rewards"     W  1   ,   W  2   ,  ‚Ä¶      subscript  W  1    subscript  W  2   normal-‚Ä¶    W_{1},W_{2},\ldots   (which in this case happen to be negative) may be viewed as the successive repair costs incurred as a result of the successive malfunctions.  An alternative analogy is that we have a magic goose which lays eggs at intervals (holding times) distributed as    S  i     subscript  S  i    S_{i}   . Sometimes it lays golden eggs of random weight, and sometimes it lays toxic eggs (also of random weight) which require responsible (and costly) disposal. The "rewards"    W  i     subscript  W  i    W_{i}   are the successive (random) financial losses/gains resulting from successive eggs ( i = 1,2,3,...) and    Y  t     subscript  Y  t    Y_{t}   records the total financial "reward" at time t .  Properties of renewal processes and renewal-reward processes  We define the renewal function as the expected value of the number of jumps observed up to some time   t   t   t   :        m   (  t  )    =   ùîº   [   X  t   ]     .        m  t     ùîº   delimited-[]   subscript  X  t       m(t)=\mathbb{E}[X_{t}].\,     The elementary renewal theorem  The renewal function satisfies         lim   t  ‚Üí  ‚àû      1  t   m   (  t  )     =    1  /  ùîº    [   S  1   ]     .        subscript    normal-‚Üí  t         1  t   m  t        1  ùîº    delimited-[]   subscript  S  1       \lim_{t\to\infty}\frac{1}{t}m(t)=1/\mathbb{E}[S_{1}].     Proof  Below, you find that the strong law of large numbers for renewal processes tell us that         lim   t  ‚Üí  ‚àû      X  t   t    =   1   ùîº   [   S  1   ]      .        subscript    normal-‚Üí  t        subscript  X  t   t      1    ùîº   delimited-[]   subscript  S  1        \lim_{t\to\infty}\frac{X_{t}}{t}=\frac{1}{\mathbb{E}[S_{1}]}.     To prove the elementary renewal theorem, it is sufficient to show that    {      X  t   t   ;  t   ‚â•  0   }           subscript  X  t   t   t   0     \left\{\frac{X_{t}}{t};t\geq 0\right\}   is uniformly integrable.  To do this, consider some truncated renewal process where the holding times are defined by      S  n   ¬Ø   =  a  ùïÄ   {   S  n   >  a  }      fragments   normal-¬Ø   subscript  S  n     a  I   fragments  normal-{   subscript  S  n    a  normal-}     \overline{S_{n}}=a\mathbb{I}\{S_{n}>a\}   where   a   a   a   is a point such that    0  <   F   (  a  )    =  p  <  1        0    F  a        p       1     0   which exists for all non-deterministic renewal processes. This new renewal process     X  t   ¬Ø     normal-¬Ø   subscript  X  t     \overline{X_{t}}   is an upper bound on    X  t     subscript  X  t    X_{t}   and its renewals can only occur on the lattice    {     n  a   ;  n   ‚àà  ‚Ñï   }          n  a   n   ‚Ñï     \{na;n\in\mathbb{N}\}   . Furthermore, the number of renewals at each time is geometric with parameter   p   p   p   . So we have       X  t   ¬Ø     normal-¬Ø   subscript  X  t     \displaystyle\overline{X_{t}}     The elementary renewal theorem for renewal reward processes  We define the reward function :        g   (  t  )    =   ùîº   [   Y  t   ]     .        g  t     ùîº   delimited-[]   subscript  Y  t       g(t)=\mathbb{E}[Y_{t}].\,     The reward function satisfies         lim   t  ‚Üí  ‚àû      1  t   g   (  t  )     =    ùîº   [   W  1   ]     ùîº   [   S  1   ]      .        subscript    normal-‚Üí  t         1  t   g  t        ùîº   delimited-[]   subscript  W  1       ùîº   delimited-[]   subscript  S  1        \lim_{t\to\infty}\frac{1}{t}g(t)=\frac{\mathbb{E}[W_{1}]}{\mathbb{E}[S_{1}]}.     The renewal equation  The renewal function satisfies       m   (  t  )    =     F  S    (  t  )    +    ‚à´  0  t    m   (   t  -  s   )    f  S    (  s  )   d  s           m  t        subscript  F  S   t     superscript   subscript   0   t     m    t  s    subscript  f  S   s  d  s       m(t)=F_{S}(t)+\int_{0}^{t}m(t-s)f_{S}(s)\,ds     where    F  S     subscript  F  S    F_{S}   is the cumulative distribution function of    S  1     subscript  S  1    S_{1}   and    f  S     subscript  f  S    f_{S}   is the corresponding probability density function.  Proof of the renewal equation   We may iterate the expectation about the first holding time:         m   (  t  )   =  ùîº   [   X  t   ]   =  ùîº   [  ùîº   (   X  t   ‚à£   S  1   )   ]   .     fragments  m   fragments  normal-(  t  normal-)    E   fragments  normal-[   subscript  X  t   normal-]    E   fragments  normal-[  E   fragments  normal-(   subscript  X  t   normal-‚à£   subscript  S  1   normal-)   normal-]   normal-.    m(t)=\mathbb{E}[X_{t}]=\mathbb{E}[\mathbb{E}(X_{t}\mid S_{1})].\,         But by the Markov property          ùîº   (   X  t   ‚à£   S  1   =  s  )   =   ùïÄ   {  t  ‚â•  s  }     (  1  +  ùîº   [   X   t  -  s    ]   )   .     fragments  E   fragments  normal-(   subscript  X  t   normal-‚à£   subscript  S  1    s  normal-)     subscript  ùïÄ   fragments  normal-{  t   s  normal-}     fragments  normal-(  1   E   fragments  normal-[   subscript  X    t  s    normal-]   normal-)   normal-.    \mathbb{E}(X_{t}\mid S_{1}=s)=\mathbb{I}_{\{t\geq s\}}\left(1+\mathbb{E}[X_{t-%
 s}]\right).\,         So          \begin{align} m(t) & {} = \mathbb{E}[X_t] \\[12pt] & {} = \mathbb{E}[\mathbb{E}(X_t \mid S_1)] \\[12pt] & {} = \int_0^\infty \mathbb{E}(X_t \mid S_1=s) f_S(s)\, ds \\[12pt] & {} = \int_0^\infty \mathbb{I}_{\{t \geq s\}} \left( 1 + \mathbb{E}[X_{t-s}] \right) f_S(s)\, ds \\[12pt] & {} = \int_0^t \left( 1 + m(t-s) \right) f_S(s)\, ds \\[12pt] & {} = F_S(t) + \int_0^t m(t-s) f_S(s)\, ds, \end{align}   as required.   Asymptotic properties       (   X  t   )    t  ‚â•  0      subscript   subscript  X  t     t  0     (X_{t})_{t\geq 0}   and     (   Y  t   )    t  ‚â•  0      subscript   subscript  Y  t     t  0     (Y_{t})_{t\geq 0}   satisfy        lim   t  ‚Üí  ‚àû      1  t    X  t     =   1   ùîº   S  1           subscript    normal-‚Üí  t         1  t    subscript  X  t       1    ùîº   subscript  S  1       \lim_{t\to\infty}\frac{1}{t}X_{t}=\frac{1}{\mathbb{E}S_{1}}   (strong law of large numbers for renewal processes)        lim   t  ‚Üí  ‚àû      1  t    Y  t     =    1   ùîº   S  1     ùîº   W  1          subscript    normal-‚Üí  t         1  t    subscript  Y  t         1    ùîº   subscript  S  1     ùîº   subscript  W  1      \lim_{t\to\infty}\frac{1}{t}Y_{t}=\frac{1}{\mathbb{E}S_{1}}\mathbb{E}W_{1}   (strong law of large numbers for renewal-reward processes)  almost surely.  Proof   First consider     (   X  t   )    t  ‚â•  0      subscript   subscript  X  t     t  0     (X_{t})_{t\geq 0}   . By definition we have:          J   X  t    ‚â§  t  ‚â§   J    X  t   +  1           subscript  J   subscript  X  t    t        subscript  J     subscript  X  t   1       J_{X_{t}}\leq t\leq J_{X_{t}+1}         for all    t  ‚â•  0      t  0    t\geq 0   and so          \frac{J_{X_t}}{X_t} \leq \frac{t}{X_t} \leq \frac{J_{X_t+1}}{X_t}   for all t ‚â• 0.    Now since    0  <   ùîº   S  i    <  ‚àû        0    ùîº   subscript  S  i             0<\mathbb{E}S_{i}<\infty   we have:          X  t   ‚Üí  ‚àû     normal-‚Üí   subscript  X  t      X_{t}\to\infty         as    t  ‚Üí  ‚àû     normal-‚Üí  t     t\to\infty    almost surely (with probability 1). Hence:           J   X  t     X  t    =    J  n   n   =    1  n     ‚àë   i  =  1   n    S  i     ‚Üí   ùîº   S  1             subscript  J   subscript  X  t     subscript  X  t       subscript  J  n   n            1  n     superscript   subscript     i  1    n    subscript  S  i       normal-‚Üí      ùîº   subscript  S  1       \frac{J_{X_{t}}}{X_{t}}=\frac{J_{n}}{n}=\frac{1}{n}\sum_{i=1}^{n}S_{i}\to%
 \mathbb{E}S_{1}         almost surely (using the strong law of large numbers); similarly:           J    X  t   +  1     X  t    =     J    X  t   +  1      X  t   +  1       X  t   +  1    X  t     =     J   n  +  1     n  +  1      n  +  1   n    ‚Üí    ùîº   S  1    ‚ãÖ  1            subscript  J     subscript  X  t   1     subscript  X  t         subscript  J     subscript  X  t   1       subscript  X  t   1         subscript  X  t   1    subscript  X  t               subscript  J    n  1      n  1        n  1   n      normal-‚Üí     normal-‚ãÖ    ùîº   subscript  S  1    1      \frac{J_{X_{t}+1}}{X_{t}}=\frac{J_{X_{t}+1}}{X_{t}+1}\frac{X_{t}+1}{X_{t}}=%
 \frac{J_{n+1}}{n+1}\frac{n+1}{n}\to\mathbb{E}S_{1}\cdot 1         almost surely.    Thus (since    t  /   X  t       t   subscript  X  t     t/X_{t}   is sandwiched between the two terms)          \frac{1}{t} X_t \to \frac{1}{\mathbb{E}S_1}   almost surely.    Next consider     (   Y  t   )    t  ‚â•  0      subscript   subscript  Y  t     t  0     (Y_{t})_{t\geq 0}   . We have           1  t    Y  t    =     X  t   t    1   X  t     Y  t    ‚Üí     1   ùîº   S  1     ‚ãÖ  ùîº    W  1              1  t    subscript  Y  t         subscript  X  t   t     1   subscript  X  t     subscript  Y  t      normal-‚Üí       normal-‚ãÖ    1    ùîº   subscript  S  1     ùîº    subscript  W  1       \frac{1}{t}Y_{t}=\frac{X_{t}}{t}\frac{1}{X_{t}}Y_{t}\to\frac{1}{\mathbb{E}S_{1%
 }}\cdot\mathbb{E}W_{1}         almost surely (using the first result and using the law of large numbers on    Y  t     subscript  Y  t    Y_{t}   ).   The inspection paradox  A curious feature of renewal processes is that if we wait some predetermined time t and then observe how large the renewal interval containing t is, we should expect it to be typically larger than a renewal interval of average size.  Mathematically the inspection paradox states: for any t > 0 the renewal interval containing t is stochastically larger than the first renewal interval. That is, for all x > 0 and for all t > 0:      ‚Ñô   (   S    X  t   +  1    >  x  )   ‚â•  ‚Ñô   (   S  1   >  x  )   =  1  -   F  S    (  x  )      fragments  P   fragments  normal-(   subscript  S     subscript  X  t   1     x  normal-)    P   fragments  normal-(   subscript  S  1    x  normal-)    1    subscript  F  S    fragments  normal-(  x  normal-)     \mathbb{P}(S_{X_{t}+1}>x)\geq\mathbb{P}(S_{1}>x)=1-F_{S}(x)     where F S is the cumulative distribution function of the IID holding times S i .  Proof of the inspection paradox  (Figure)  The renewal interval determined by the random point t (shown in red) is stochastically larger than the first renewal interval.   Observe that the last jump-time before t is    J   X  t      subscript  J   subscript  X  t     J_{X_{t}}   ; and that the renewal interval containing t is    S    X  t   +  1      subscript  S     subscript  X  t   1     S_{X_{t}+1}   . Then      ‚Ñô   (   S    X  t   +  1    >  x  )      fragments  P   fragments  normal-(   subscript  S     subscript  X  t   1     x  normal-)     \displaystyle\mathbb{P}(S_{X_{t}+1}>x)     as required.  Superposition  The superposition of independent renewal processes is not generally a renewal process, but it can be described within a larger class of processes called the Markov-renewal processes. 1 However, the cumulative distribution function of the first inter-event time in the superposition process is given by 2         R   (  t  )    =   1  -    ‚àë   k  =  1   K      Œ±  k      ‚àë   l  =  1   K     Œ±  l      (   1  -    R  k    (  t  )     )     ‚àè    j  =  1   ,   j  ‚â†  k    K     Œ±  j     ‚à´  t  ‚àû     (   1  -    R  j    (  u  )     )   d  u               R  t     1    superscript   subscript     k  1    K        subscript  Œ±  k     superscript   subscript     l  1    K    subscript  Œ±  l       1     subscript  R  k   t      superscript   subscript  product   formulae-sequence    j  1     j  k     K      subscript  Œ±  j     superscript   subscript   t         1     subscript  R  j   u    d  u           R(t)=1-\sum_{k=1}^{K}\frac{\alpha_{k}}{\sum_{l=1}^{K}\alpha_{l}}(1-R_{k}(t))%
 \prod_{j=1,j\neq k}^{K}\alpha_{j}\int_{t}^{\infty}(1-R_{j}(u))\text{d}u        where R k ( t ) and Œ± k >¬†0 are the CDF of the inter-event times and the arrival rate of process k . 3  Example applications  Example 1: use of the strong law of large numbers  Eric the entrepreneur has n machines, each having an operational lifetime uniformly distributed between zero and two years. Eric may let each machine run until it fails with replacement cost ‚Ç¨2600; alternatively he may replace a machine at any time while it is still functional at a cost of ‚Ç¨200.  What is his optimal replacement policy?  Solution  We may model the lifetime of the n machines as n independent concurrent renewal-reward processes, so it is sufficient to consider the case n=1 . Denote this process by     (   Y  t   )    t  ‚â•  0      subscript   subscript  Y  t     t  0     (Y_{t})_{t\geq 0}   . The successive lifetimes S of the replacement machines are independent and identically distributed, so the optimal policy is the same for all replacement machines in the process.  If Eric decides at the start of a machine's life to replace it at time 0  \begin{align} \mathbb{E}S & = \mathbb{E}[S \mid \mbox{fails before } t] \cdot \mathbb{P}[\mbox{fails before } t] + \mathbb{E}[S \mid \mbox{does not fail before } t] \cdot \mathbb{P}[\mbox{does not fail before } t] \\ & = \frac{t}{2}\left(0.5t\right) + \frac{2-t}{2}\left( t \right) \end{align}  and the expected cost W per machine is:      ùîº  W      ùîº  W    \displaystyle\mathbb{E}W     So by the strong law of large numbers, his long-term average cost per unit time is:        1  t    Y  t    ‚âÉ    ùîº  W    ùîº  S    =    4   (    1200  t   +  200   )       t  2   +   4  t    -   2   t  2           similar-to-or-equals      1  t    subscript  Y  t        ùîº  W     ùîº  S             4      1200  t   200         superscript  t  2     4  t      2   superscript  t  2         \frac{1}{t}Y_{t}\simeq\frac{\mathbb{E}W}{\mathbb{E}S}=\frac{4(1200t+200)}{t^{2%
 }+4t-2t^{2}}     then differentiating with respect to t :         ‚àÇ   ‚àÇ  t      4   (    1200  t   +  200   )       t  2   +   4  t    -   2   t  2       =   4      (    4  t   -   t  2    )    (  1200  )    -    (   4  -   2  t    )    (    1200  t   +  200   )       (     t  2   +   4  t    -   2   t  2     )   2      ,             t        4      1200  t   200         superscript  t  2     4  t      2   superscript  t  2         4            4  t    superscript  t  2    1200       4    2  t        1200  t   200      superscript       superscript  t  2     4  t      2   superscript  t  2     2       \frac{\partial}{\partial t}\frac{4(1200t+200)}{t^{2}+4t-2t^{2}}=4\frac{(4t-t^{%
 2})(1200)-(4-2t)(1200t+200)}{(t^{2}+4t-2t^{2})^{2}},     this implies that the turning points satisfy:     0   0   \displaystyle 0     and thus       0  =     3   t  2    +  t   -  2   =    (    3  t   -  2   )    (   t  +  1   )     .        0        3   superscript  t  2    t   2              3  t   2     t  1       0=3t^{2}+t-2=(3t-2)(t+1).     We take the only solution t in [0, 2]: t = 2/3. This is indeed a minimum (and not a maximum) since the cost per unit time tends to infinity as t tends to zero, meaning that the cost is decreasing as t increases, until the point 2/3 where it starts to increase.  See also   Campbell's theorem (probability)  Compound Poisson process  Continuous-time Markov process  Little's lemma  Palm‚ÄìKhintchine theorem  Poisson process  Queueing theory  Ruin theory  Semi-Markov process   References           "  Category:Probability theory  Category:Stochastic processes  Category:Point processes     ‚Ü©  formula 4.1 ‚Ü©  ‚Ü©     