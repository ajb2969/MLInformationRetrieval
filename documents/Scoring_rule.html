<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="1082">Scoring rule</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Scoring rule</h1>
<hr/>
<p>In <a href="decision_theory" title="wikilink">decision theory</a>, a <strong>score function</strong>, or <strong>scoring rule</strong>, measures the accuracy of probabilistic predictions. It is applicable to tasks in which predictions must assign probabilities to a set of mutually exclusive discrete outcomes. The set of possible outcomes can be either binary or categorical in nature, and the probabilities assigned to this set of outcomes must sum to one (where each individual probability is in the range of 0 to 1). A score can be thought of as either a measure of the "calibration" of a set of probabilistic predictions, or as a "cost function" or "<a href="loss_function" title="wikilink">loss function</a>".</p>
<p>If a cost is levied in proportion to a proper scoring rule, the minimal expected cost corresponds to reporting the true set of probabilities. Proper scoring rules are used in meteorology, finance, and pattern classification where a forecaster or algorithm will attempt to minimize the average score to yield refined, calibrated probabilities (i.e. accurate probabilities). Various scoring rules have also been used to assess the predictive accuracy of forecast models for <a href="association_football" title="wikilink">association football</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="example-application-of-scoring-rules">Example application of scoring rules</h2>
<p><a href="Image:LogScore.png" title="wikilink">thumb|upright=1.25|The logarithmic rule</a> An example of <a href="probabilistic_forecasting" title="wikilink">probabilistic forecasting</a> is in meteorology where a <a href="Weather_forecasting" title="wikilink">weather forecaster</a> may give the probability of rain on the next day. One could note the number of times that a 25% probability was quoted, over a long period, and compare this with the actual proportion of times that rain fell. If the actual percentage was substantially different from the stated probability we say that the forecaster is <a href="Calibrated_probability_assessment" title="wikilink">poorly calibrated</a>. A poorly calibrated forecaster might be encouraged to do better by a <a class="uri" href="bonus" title="wikilink">bonus</a> system. A bonus system designed around a proper scoring rule will incentivize the forecaster to report probabilities equal to his <a href="Personal_Probability" title="wikilink">personal beliefs</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<p>In addition to the simple case of a <a href="binary_decision" title="wikilink">binary decision</a>, such as assigning probabilities to 'rain' or 'no rain', scoring rules may be used for multiple classes, such as 'rain', 'snow', or 'clear'.</p>
<p>The image to the right shows an example of a scoring rule, the logarithmic scoring rule, as a function of the probability reported for the event that actually occurred. One way to use this rule would be as a cost based on the probability that a forecaster or algorithm assigns, then checking to see which event actually occurs.</p>
<h2 id="proper-scoring-rules">Proper scoring rules</h2>
<p><a href="Image:ExpectedLog.png" title="wikilink">thumb|upright=1.25|Expected value of Logarithmic rule, when Event 1 is expected to occur with probability of 0.8</a></p>
<p>A probabilistic forecaster or algorithm will return a <a href="probability_vector" title="wikilink">probability vector</a> <strong>r</strong> with a probability for each of the i outcomes. One usage of a scoring function could be to give a reward of <span class="LaTeX">$S(\mathbf{r},i)$</span> if the <em>i</em>th event occurs. If a <em>proper</em> scoring rule is used, then the highest <a href="Expected_value" title="wikilink">expected</a> reward is obtained by reporting the true probability distribution. The use of a proper scoring rule encourages the forecaster to be honest to maximize the expected reward.</p>
<p>A scoring rule is <em>strictly proper</em> if it is uniquely optimized by the true probabilities. Optimized in this case will correspond to maximization for the quadratic, spherical, and logarithmic rules but minimization for the Brier Score. This can be seen in the image at right for the logarithmic rule. Here, Event 1 is expected to occur with probability of 0.8, and the expected score (or reward) is shown as a function of the reported probability. The way to maximize the expected reward is to report the actual probability of 0.8 as all other reported probabilities will yield a lower expected score. This property holds because the logarithmic score is proper.</p>
<h3 id="examples-of-proper-scoring-rules">Examples of proper scoring rules</h3>
<p>There are an infinite number of scoring rules, including entire parameterized families of proper scoring rules. The ones shown below are simply popular examples.</p>
<h4 id="logarithmic-scoring-rule">Logarithmic scoring rule</h4>
<p>The logarithmic scoring rule is a local strictly proper scoring rule. This is also the negative of <a href="Self-information" title="wikilink">surprisal</a>, which is commonly using a scoring criteria in Bayesian Inference; the goal is to minimize expected surprisal. This scoring rule has strong foundations in information theory.</p>
<p><span class="LaTeX">$$L(\mathbf{r},i) = \ln(r_i)$$</span></p>
<p>That is, a prediction of 80% or 0.8 which proved true (good) would receive a score of ln(0.8) = -0.22, while the same prediction which proved false (bad) would receive a score of the <em>right</em> prediction 20%: ln(1-0.8) = ln(0.2) = -1.6. The goal of a forecaster is to maximize his score and for the score to be as large as possible, and -0.22 is indeed larger than -1.6.</p>
<p>If one treats the truth or falsity of the prediction as a variable <em>x</em> which is 1 or 0 respectively, and the expressed probability as <em>p</em>, then one could write the logarithmic scoring rule as x*log(p) + (1-x)*log(1-p).</p>
<p>Since strictly proper scoring rules remain strictly proper under linear transformation</p>
<p><span class="LaTeX">$$L(\mathbf{r},i) = \log_b(r_i)$$</span> is strictly proper for all <span class="LaTeX">$b>0$</span></p>
<h4 id="brierquadratic-scoring-rule">Brier/quadratic scoring rule</h4>
<p>The quadratic scoring rule is a strictly proper scoring rule</p>
<p><span class="LaTeX">$$Q(\mathbf{r},i) = 2r_i - \mathbf{r}\cdot \mathbf{r} = 2r_i -\sum_{j=1}^C r_j^2$$</span> where <span class="LaTeX">$r_i$</span> is the probability assigned to the correct answer.</p>
<p>The <a href="Brier_score" title="wikilink">Brier score</a>, originally proposed by Glenn W. Brier in 1950,<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> can be obtained by an affine transform from the quadratic scoring rule.</p>
<p><span class="LaTeX">$$B(\mathbf{r},i) = \sum_{j=1}^C (y_j-r_j)^2$$</span> Where <span class="LaTeX">$y_j = 1$</span> when the jth event is correct and <span class="LaTeX">$y_j = 0$</span> otherwise and C is the number of classes.</p>
<p>An important difference between these two rules is that a forecaster should strive to maximize the quadratic score yet minimize the Brier score. This is due to a negative sign in the linear transformation between them.</p>
<h4 id="spherical-scoring-rule">Spherical scoring rule</h4>
<p>The spherical scoring rule is also a strictly proper scoring rule</p>
<p><span class="LaTeX">$$S(\mathbf{r},i) = \frac{r_i}{\lVert \mathbf{r} \rVert} = \frac{r_i}{\sqrt{r_1^2 + \cdots + r_c^2}}$$</span></p>
<h3 id="comparison-of-proper-scoring-rules">Comparison of proper scoring rules</h3>
<p>Shown below on the left is a graphical comparison of the Logarithmic, Quadratic, and Spherical scoring rules for a binary classification problem. The x-axis indicates the reported probability for the event that actually occurred.</p>
<p>It is important to note that each of the scores have different magnitudes and locations. The magnitude differences are not relevant however as scores remain proper under affine transformation. Therefore, to compare different scores it is necessary to move them to a common scale. A reasonable choice of normalization is shown at the picture on the right where all scores intersect the points (0.5,0) and (1,1). This ensures that they yield 0 for a uniform distribution (two probabilities of 0.5 each), reflecting no cost or reward for reporting what is often the baseline distribution. All normalized scores below also yield 1 when the true class is assigned a probability of 1.</p>
<center>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><p><a href="Image:RawScore.png" title="wikilink">thumb|right|upright=1.25|Score of a binary classification for the true class showing logarithmic (blue), spherical (green), and quadratic (red)</a></p></td>
<td style="text-align: left;"><p><a href="Image:NormalizedScore.png" title="wikilink">thumb|left|upright=1.25|Normalized score of a binary classification for the true class showing logarithmic (blue), spherical (green), and quadratic (red)</a></p></td>
</tr>
</tbody>
</table>
</center>
<h2 id="characteristics">Characteristics</h2>
<h3 id="positive-affine-transformation">Positive-affine transformation</h3>
<p>A strictly proper scoring rule, whether binary or multiclass, after a <a href="positive-affine_transformation" title="wikilink">positive-affine transformation</a> remains a strictly proper scoring rule.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> That is, if <span class="LaTeX">$S(\mathbf{r},i)$</span> is a strictly proper scoring rule then <span class="LaTeX">$a+bS(\mathbf{r},i)$</span> with <span class="LaTeX">$b>0$</span> is also a strictly proper scoring rule.</p>
<h3 id="locality">Locality</h3>
<p>A proper scoring rule is said to be <em>local</em> if its value depends only on the probability <span class="LaTeX">$r_i$</span>. All binary scores are local because the probability assigned to the event that did not occur is directly producible as <span class="LaTeX">$1-r_i$</span>.</p>
<p>Affine functions of the logarithmic scoring rule is are only strictly proper local scoring rules on a finite set that is not binary.</p>
<h3 id="decomposition">Decomposition</h3>
<p>The expectation value of a proper scoring rule <span class="LaTeX">$S$</span> can be decomposed into the sum of three components, called <em>uncertainty</em>, <em>reliability</em>, and <em>resolution</em>,<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> which characterize different attributes of probabilistic forecasts:</p>
<p><span class="LaTeX">$$E(S) = UNC + REL - RES.$$</span></p>
<p>If a score is proper and negatively oriented (such as the Brier Score), all three terms are positive definite. The uncertainty component is equal to the expected score of the forecast which constantly predicts the average event frequency. The reliability component penalizes poorly calibrated forecasts, in which the predicted probabilities do not coincide with the event frequencies. Resolution rewards probabilities that are close to one whenever the event happens, and which are close to zero if the event does not happen.</p>
<p>The equations for the individual components depend on the particular scoring rule. For the Brier Score, they are given by</p>
<p><span class="LaTeX">$$UNC = \bar{x}(1-\bar{x})$$</span></p>
<p><span class="LaTeX">$$REL = E(p-\pi(p))^2$$</span></p>
<p><span class="LaTeX">$$RES = E(\pi(p)-\bar{x})^2$$</span></p>
<p>where <span class="LaTeX">$\bar{x}$</span> is the average probability of occurrence of the binary event <span class="LaTeX">$x$</span>, and <span class="LaTeX">$\pi(p)$</span> is the conditional event probability, given <span class="LaTeX">$p$</span>, i.e. <span class="LaTeX">$\pi(p) = P(x=1\mid p)$</span></p>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.decisionsciencenews.com/?p=963">Video comparing spherical, quadratic and logarithmic scoring rules</a></li>
<li><a href="https://www.stat.washington.edu/research/reports/2009/tr551.pdf">Local Proper Scoring Rules</a></li>
<li><a href="http://faculty.engr.utexas.edu/bickel/working_papers/scoring_rules_experiential_learning.pdf">Scoring Rules and Decision Analysis Education</a></li>
<li><a href="http://www.stat.washington.edu/research/reports/2004/tr463.pdf">Strictly Proper Scoring Rules</a></li>
<li>[<a class="uri" href="http://www.jstor.org/discover/10.2307/1402448?uid=16779064&uid">http://www.jstor.org/discover/10.2307/1402448?uid=16779064&uid</a>;=3737864&uid;=2129&uid;=2&uid;=70&uid;=16734048&uid;=3&uid;=67&uid;=62&sid;=21101527707467 Scoring Rules and uncertainty]</li>
</ul>
<p>"</p>
<p><a href="Category:Decision_theory" title="wikilink">Category:Decision theory</a> <a href="Category:Probability_assessment" title="wikilink">Category:Probability assessment</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
</ol>
</section>
</body>
</html>
