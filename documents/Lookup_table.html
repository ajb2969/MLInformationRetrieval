<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="306">Lookup table</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Lookup table</h1>
<style>
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
<style>
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</body></html>
<body>
<hr/>

<p>In <a href="computer_science" title="wikilink">computer science</a>, a <strong>lookup table</strong> is an <a href="Array_data_structure" title="wikilink">array</a> that replaces runtime computation with a simpler array indexing operation. The savings in terms of processing time can be significant, since retrieving a value from memory is often faster than undergoing an 'expensive' computation or <a class="uri" href="input/output" title="wikilink">input/output</a> operation.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> The tables may be precalculated and stored in <a href="Static_memory_allocation" title="wikilink">static</a> program storage, calculated (or <a href="Prefetcher" title="wikilink">"pre-fetched"</a>) as part of a program's initialization phase (<a class="uri" href="memoization" title="wikilink">memoization</a>), or even stored in hardware in application-specific platforms. Lookup tables are also used extensively to validate input values by matching against a list of valid (or invalid) items in an array and, in some programming languages, may include pointer functions (or offsets to labels) to process the matching input.</p>
<h2 id="history">History</h2>

<p><embed src="Abramowitz&amp;Stegun.page97.agr.jpg;" title="fig:Part of a 20th-century table of common logarithms in the reference book Abramowitz and Stegun."></embed> Before the advent of computers, lookup tables of values were used to speed up hand calculations of complex functions, such as in <a class="uri" href="trigonometry" title="wikilink">trigonometry</a>, <a href="Common_logarithm" title="wikilink">logarithms</a>, and statistical density functions<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>In ancient (499 CE) India, <a class="uri" href="Aryabhata" title="wikilink">Aryabhata</a> created one of the first <a href="Aryabhata's_sine_table" title="wikilink">sine tables</a>, which he encoded in a Sanskrit-letter-based number system. In 493 A.D., <a href="Victorius_of_Aquitaine" title="wikilink">Victorius of Aquitaine</a> wrote a 98-column multiplication table which gave (in <a href="Roman_numerals" title="wikilink">Roman numerals</a>) the product of every number from 2 to 50 times and the rows were "a list of numbers starting with one thousand, descending by hundreds to one hundred, then descending by tens to ten, then by ones to one, and then the fractions down to 1/144" <a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> Modern school children are often taught to memorize "<a href="times_table" title="wikilink">times tables</a>" to avoid calculations of the most commonly used numbers (up to 9 x 9 or 12 x 12).</p>

<p>Early in the history of computers, <a class="uri" href="input/output" title="wikilink">input/output</a> operations were particularly slow â€“ even in comparison to processor speeds of the time. It made sense to reduce expensive read operations by a form of manual <a href="cache_(computing)" title="wikilink">caching</a> by creating either static lookup tables (embedded in the program) or dynamic prefetched arrays to contain only the most commonly occurring data items. Despite the introduction of systemwide caching that now automates this process, application level lookup tables can still improve performance for data items that rarely, if ever, change.</p>
<h2 id="examples">Examples</h2>
<h3 id="simple-lookup-in-an-array-an-associative-array-or-a-linked-list-unsorted-list">Simple lookup in an array, an associative array or a linked list (unsorted list)</h3>

<p>This is known as a <a href="linear_search" title="wikilink">linear search</a> or <a href="brute-force_search" title="wikilink">brute-force search</a>, each element being checked for equality in turn and the associated value, if any, used as a result of the search. This is often the slowest search method unless frequently occurring values occur early in the list. For a one-dimensional array or <a href="linked_list" title="wikilink">linked list</a>, the lookup is usually to determine whether or not there is a match with an 'input' data value.</p>
<h3 id="binary-search-in-an-array-or-an-associative-array-sorted-list">Binary search in an array or an associative array (sorted list)</h3>

<p>An example of a "<a href="divide_and_conquer_algorithm" title="wikilink">divide and conquer algorithm</a>", <a href="binary_search" title="wikilink">binary search</a> involves each element being found by determining which half of the table a match may be found in and repeating until either success or failure. This is only possible if the list is sorted but gives good performance even if the list is lengthy.</p>
<h3 id="trivial-hash-function">Trivial hash function</h3>

<p>For a <a href="trivial_hash_function" title="wikilink">trivial hash function</a> lookup, the unsigned <a href="raw_data" title="wikilink">raw data</a> value is used <em>directly</em> as an index to a one-dimensional table to extract a result. For small ranges, this can be amongst the fastest lookup, even exceeding binary search speed with zero branches and executing in <a href="constant_time" title="wikilink">constant time</a>.</p>
<h4 id="counting-bits-in-a-series-of-bytes">Counting '' bits in a series of bytes</h4>

<p>One discrete problem that is expensive to solve on many computers, is that of counting the number of bits which are set to 1 in a (binary) number, sometimes called the <em><a href="Hamming_weight" title="wikilink">population function</a></em>. For example, the decimal number "37" is "00100101" in binary, so it contains three bits that are set to binary "1".</p>

<p>A simple example of <a href="C_(programming_language)" title="wikilink">C</a> code, designed to count the 1 bits in a <em>int</em>, might look like this:</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="dt">int</span> count_ones(<span class="dt">unsigned</span> <span class="dt">int</span> x) {
    <span class="dt">int</span> result = <span class="dv">0</span>;
    <span class="kw">while</span> (x != <span class="dv">0</span>)
        result++, x = x &amp; (x<span class="dv">-1</span>);
    <span class="kw">return</span> result;
}</code></pre></div>

<p>This apparently simple algorithm can take potentially hundreds of cycles even on a modern architecture, because it makes many branches in the loop - and branching is slow. This can be ameliorated using <a href="loop_unrolling" title="wikilink">loop unrolling</a> and some other compiler optimizations. There is however a simple and much faster algorithmic solution - using a <a href="trivial_hash_function" title="wikilink">trivial hash function</a> table lookup.</p>

<p>Simply construct a static table, <em>bits_set</em>, with 256 entries giving the number of one bits set in each possible byte value (e.g. 0x00 = 0, 0x01 = 1, 0x02 = 1, and so on). Then use this table to find the number of ones in each byte of the integer using a <a href="trivial_hash_function" title="wikilink">trivial hash function</a> lookup on each byte in turn, and sum them. This requires no branches, and just four indexed memory accesses, considerably faster than the earlier code.</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"> <span class="co">/* (this code assumes that 'int' is 32-bits wide) */</span>
 <span class="dt">int</span> count_ones(<span class="dt">unsigned</span> <span class="dt">int</span> x) {
    <span class="kw">return</span> bits_set[ x        &amp; <span class="dv">255</span>] + bits_set[(x &gt;&gt;  <span class="dv">8</span>) &amp; <span class="dv">255</span>]
         + bits_set[(x &gt;&gt; <span class="dv">16</span>) &amp; <span class="dv">255</span>] + bits_set[(x &gt;&gt; <span class="dv">24</span>) &amp; <span class="dv">255</span>];
}</code></pre></div>

<p>The above source can be improved easily, (avoiding AND'ing, and shifting) by 'recasting' 'x' as a 4 byte unsigned char array and, preferably, coded in-line as a single statement instead of being a function. Note that even this simple algorithm can be too slow now, because the original code might run faster from the cache of modern processors, and (large) lookup tables do not fit well in caches and can cause a slower access to memory (in addition, in the above example, it requires computing addresses within a table, to perform the four lookups needed).</p>
<h3 id="lookup-tables-in-image-processing">Lookup tables in image processing</h3>

<p>In data analysis applications, such as <a href="Image_Processing" title="wikilink">image processing</a>, a lookup table (LUT) is used to transform the input data into a more desirable output format. For example, a grayscale picture of the planet Saturn will be transformed into a color image to emphasize the differences in its rings.</p>

<p>A classic example of reducing run-time computations using lookup tables is to obtain the result of a <a class="uri" href="trigonometry" title="wikilink">trigonometry</a> calculation, such as the <a class="uri" href="sine" title="wikilink">sine</a> of a value. Calculating trigonometric functions can substantially slow a computing application. The same application can finish much sooner when it first precalculates the sine of a number of values, for example for each whole number of degrees (The table can be defined as static variables at compile time, reducing repeated run time costs). When the program requires the sine of a value, it can use the lookup table to retrieve the closest sine value from a memory address, and may also take the step of interpolating to the sine of the desired value, instead of calculating by mathematical formula. Lookup tables are thus used by mathematics co-processors in computer systems. An error in a lookup table was responsible for Intel's infamous <a href="Pentium_FDIV_bug" title="wikilink">floating-point divide bug</a>.</p>

<p>Functions of a single variable (such as sine and cosine) may be implemented by a simple array. Functions involving two or more variables require multidimensional array indexing techniques. The latter case may thus employ a two-dimensional array of <strong>power[x][y]</strong> to replace a function to calculate <strong>x<sup>y</sup></strong> for a limited range of x and y values. Functions that have more than one result may be implemented with lookup tables that are arrays of structures.</p>

<p>As mentioned, there are intermediate solutions that use tables in combination with a small amount of computation, often using <a class="uri" href="interpolation" title="wikilink">interpolation</a>. Pre-calculation combined with interpolation can produce higher accuracy for values that fall between two precomputed values. This technique requires slightly more time to be performed but can greatly enhance accuracy in applications that require the higher accuracy. Depending on the values being precomputed, pre-computation with interpolation can also be used to shrink the lookup table size while maintaining accuracy.</p>

<p>In <a href="image_processing" title="wikilink">image processing</a>, lookup tables are often called <strong><a href="3D_LUT" title="wikilink">LUT</a></strong>s and give an output value for each of a range of index values. One common LUT, called the <em>colormap</em> or <em><a href="Palette_(computing)" title="wikilink">palette</a></em>, is used to determine the colors and intensity values with which a particular image will be displayed. In <a href="computed_tomography" title="wikilink">computed tomography</a>, "windowing" refers to a related concept for determining how to display the intensity of measured radiation.</p>

<p>While often effective, employing a lookup table may nevertheless result in a severe penalty if the computation that the LUT replaces is relatively simple. Memory retrieval time and the complexity of memory requirements can increase application operation time and system complexity relative to what would be required by straight formula computation. The possibility of <a href="Cache_pollution" title="wikilink">polluting the cache</a> may also become a problem. Table accesses for large tables will almost certainly cause a <a href="cache_miss" title="wikilink">cache miss</a>. This phenomenon is increasingly becoming an issue as processors outpace memory. A similar issue appears in <a class="uri" href="rematerialization" title="wikilink">rematerialization</a>, a <a href="compiler_optimization" title="wikilink">compiler optimization</a>. In some environments, such as the <a href="Java_(programming_language)" title="wikilink">Java programming language</a>, table lookups can be even more expensive due to mandatory bounds-checking involving an additional comparison and branch for each lookup.</p>

<p>There are two fundamental limitations on when it is possible to construct a lookup table for a required operation. One is the amount of memory that is available: one cannot construct a lookup table larger than the space available for the table, although it is possible to construct disk-based lookup tables at the expense of lookup time. The other is the time required to compute the table values in the first instance; although this usually needs to be done only once, if it takes a prohibitively long time, it may make the use of a lookup table an inappropriate solution. As previously stated however, tables can be statically defined in many cases.</p>
<h3 id="computing-sines">Computing sines</h3>

<p>Most computers, which only perform basic arithmetic operations, cannot directly calculate the <a class="uri" href="sine" title="wikilink">sine</a> of a given value. Instead, they use the <a class="uri" href="CORDIC" title="wikilink">CORDIC</a> algorithm or a complex formula such as the following <a href="Taylor_series" title="wikilink">Taylor series</a> to compute the value of sine to a high degree of precision:</p>

<p>

<math display="block" id="Lookup_table:0">
 <semantics>
  <mrow>
   <mrow>
    <mo>sin</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>â‰ˆ</mo>
   <mrow>
    <mrow>
     <mrow>
      <mi>x</mi>
      <mo>-</mo>
      <mfrac>
       <msup>
        <mi>x</mi>
        <mn>3</mn>
       </msup>
       <mn>6</mn>
      </mfrac>
     </mrow>
     <mo>+</mo>
     <mfrac>
      <msup>
       <mi>x</mi>
       <mn>5</mn>
      </msup>
      <mn>120</mn>
     </mfrac>
    </mrow>
    <mo>-</mo>
    <mfrac>
     <msup>
      <mi>x</mi>
      <mn>7</mn>
     </msup>
     <mn>5040</mn>
    </mfrac>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <approx></approx>
    <apply>
     <ci>sin</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <plus></plus>
      <apply>
       <minus></minus>
       <ci>x</ci>
       <apply>
        <divide></divide>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>x</ci>
         <cn type="integer">3</cn>
        </apply>
        <cn type="integer">6</cn>
       </apply>
      </apply>
      <apply>
       <divide></divide>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>x</ci>
        <cn type="integer">5</cn>
       </apply>
       <cn type="integer">120</cn>
      </apply>
     </apply>
     <apply>
      <divide></divide>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>x</ci>
       <cn type="integer">7</cn>
      </apply>
      <cn type="integer">5040</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{sin}(x)\approx x-\frac{x^{3}}{6}+\frac{x^{5}}{120}-\frac{x^{7}}{%
5040}
  </annotation>
 </semantics>
</math>

 (for <em>x</em> close to 0)</p>

<p>However, this can be expensive to compute, especially on slow processors, and there are many applications, particularly in traditional <a href="computer_graphics" title="wikilink">computer graphics</a>, that need to compute many thousands of sine values every second. A common solution is to initially compute the sine of many evenly distributed values, and then to find the sine of <em>x</em> we choose the sine of the value closest to <em>x</em>. This will be close to the correct value because sine is a <a href="continuous_function" title="wikilink">continuous function</a> with a bounded rate of change. For example:</p>

<p><code>Â </code><em><code>real</code> <code>array</code></em><code>Â sine_table[-1000..1000]</code><br/>
<code>Â </code><strong><code>for</code></strong><code>Â xÂ </code><strong><code>from</code></strong><code>Â -1000Â </code><strong><code>to</code></strong><code>Â 1000</code><br/>
<code>Â Â Â Â Â sine_table[x]Â :=Â sine(piÂ *Â xÂ /Â 1000)</code></p>

<p><code>Â </code><strong><code>function</code></strong><code>Â lookup_sine(x)</code><br/>
<code>Â Â Â Â Â </code><strong><code>return</code></strong><code>Â sine_table[round(1000Â *Â xÂ /Â pi)]</code></p>
<figure><b>(Figure)</b>
<figcaption>Linear interpolation on a portion of the sine function|right</figcaption>
</figure>

<p>Unfortunately, the table requires quite a bit of space: if IEEE double-precision floating-point numbers are used, over 16,000 bytes would be required. We can use fewer samples, but then our precision will significantly worsen. One good solution is <a href="linear_interpolation" title="wikilink">linear interpolation</a>, which draws a line between the two points in the table on either side of the value and locates the answer on that line. This is still quick to compute, and much more accurate for <a href="smooth_function" title="wikilink">smooth functions</a> such as the sine function. Here is our example using linear interpolation:</p>

<p><code>Â </code><strong><code>function</code></strong><code>Â lookup_sine(x)</code><br/>
<code>Â Â Â Â Â x1Â :=Â floor(x*1000/pi)</code><br/>
<code>Â Â Â Â Â y1Â :=Â sine_table[x1]</code><br/>
<code>Â Â Â Â Â y2Â :=Â sine_table[x1+1]</code><br/>
<code>Â Â Â Â Â </code><strong><code>return</code></strong><code>Â y1Â +Â (y2-y1)*(x*1000/pi-x1)</code></p>

<p>Another solution that uses a quarter of the space but takes a bit longer to compute would be to take into account the relationships between sine and cosine along with their symmetry rules. In this case, the lookup table is calculated by using the sine function for the first quadrant (i.e. sin(0..pi/2)). When we need a value, we assign a variable to be the angle wrapped to the first quadrant. We then wrap the angle to the four quadrants (not needed if values are always between 0 and 2*pi) and return the correct value (i.e. first quadrant is a straight return, second quadrant is read from pi/2-x, third and fourth are negatives of the first and second respectively). For cosine, we only have to return the angle shifted by pi/2 (i.e. x+pi/2). For tangent, we divide the sine by the cosine (divide-by-zero handling may be needed depending on implementation):</p>

<p><code>Â </code><strong><code>function</code></strong><code>Â init_sine()</code><br/>
<code>Â Â Â Â Â </code><strong><code>for</code></strong><code>Â xÂ </code><strong><code>from</code></strong><code>Â 0Â </code><strong><code>to</code></strong><code>Â (360/4)+1</code><br/>
<code>Â Â Â Â Â Â Â Â Â sine_table[x]Â :=Â sine(2*piÂ *Â xÂ /Â 360)</code><br/>
<code>Â </code><br/>
<code>Â </code><strong><code>function</code></strong><code>Â lookup_sine(x)</code><br/>
<code>Â Â Â Â Â xÂ Â =Â </code><strong><code>wrap</code></strong><code>Â xÂ </code><strong><code>from</code></strong><code>Â 0Â </code><strong><code>to</code></strong><code>Â 360</code><br/>
<code>Â Â Â Â Â yÂ :=Â </code><strong><code>mod</code></strong><code>Â (x,Â 90)</code><br/>
<code>Â </code><br/>
<code>Â Â Â Â Â </code><strong><code>if</code></strong><code>Â (xÂ </code></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a class="uri" href="http://pmcnamee.net/c++-memoization.html">http://pmcnamee.net/c++-memoization.html</a><a href="#fnref1">â†©</a></li>
<li id="fn2"><a href="#fnref2">â†©</a></li>
<li id="fn3">Maher, David. W. J. and John F. Makowski. "Literary Evidence for Roman Arithmetic With Fractions", 'Classical Philology' (2001) Vol. 96 No. 4 (2001) pp. 376â€“399. (See page p.383.)<a href="#fnref3">â†©</a></li>
</ol>
</section>
</body>

