   Low-rank approximation      Low-rank approximation  table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
   margin: 0; padding: 0; vertical-align: baseline; border: none; }
 <style>
 table.sourceCode { width: 100%; line-height: 100%; }
 td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
 td.sourceCode { padding-left: 5px; }
 code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
 code > span.dt { color: #902000; } /* DataType */
 code > span.dv { color: #40a070; } /* DecVal */
 code > span.bn { color: #40a070; } /* BaseN */
 code > span.fl { color: #40a070; } /* Float */
 code > span.ch { color: #4070a0; } /* Char */
 code > span.st { color: #4070a0; } /* String */
 code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
 code > span.ot { color: #007020; } /* Other */
 code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
 code > span.fu { color: #06287e; } /* Function */
 code > span.er { color: #ff0000; font-weight: bold; } /* Error */
 code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 code > span.cn { color: #880000; } /* Constant */
 code > span.sc { color: #4070a0; } /* SpecialChar */
 code > span.vs { color: #4070a0; } /* VerbatimString */
 code > span.ss { color: #bb6688; } /* SpecialString */
 code > span.im { } /* Import */
 code > span.va { color: #19177c; } /* Variable */
 code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
 code > span.op { color: #666666; } /* Operator */
 code > span.bu { } /* BuiltIn */
 code > span.ex { } /* Extension */
 code > span.pp { color: #bc7a00; } /* Preprocessor */
 code > span.at { color: #7d9029; } /* Attribute */
 code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
 code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
 code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
 code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */     In mathematics, low-rank approximation is a minimization problem, in which the cost function measures the fit between a given matrix (the data) and an approximating matrix (the optimization variable), subject to a constraint that the approximating matrix has reduced rank . The problem is used for mathematical modeling and data compression . The rank constraint is related to a constraint on the complexity of a model that fits the data. In applications, often there are other constraints on the approximating matrix apart from the rank constraint, e.g., non-negativity and Hankel structure .  Low-rank approximation is closely related to:   principal component analysis ,  factor analysis ,  total least squares ,  latent semantic analysis , and  orthogonal regression .   Definition  Given   structure specification    ùíÆ  :    ‚Ñù   n  p    ‚Üí   ‚Ñù   m  √ó  n        normal-:  ùíÆ   normal-‚Üí   superscript  ‚Ñù   subscript  n  p     superscript  ‚Ñù    m  n       \mathcal{S}:\mathbb{R}^{n_{p}}\to\mathbb{R}^{m\times n}   ,  vector of structure parameters    p  ‚àà   ‚Ñù   n  p        p   superscript  ‚Ñù   subscript  n  p      p\in\mathbb{R}^{n_{p}}   , and  desired rank   r   r   r   ,         minimize   over   p  ^     ‚à•   p  -   p  ^    ‚à•   subject to   rank   (   ùíÆ   (   p  ^   )    )     ‚â§  r   .       minimize    over   normal-^  p     norm    p   normal-^  p     subject to   rank    ùíÆ   normal-^  p      r    \text{minimize}\quad\text{over }\widehat{p}\quad\|p-\widehat{p}\|\quad\text{%
 subject to}\quad\operatorname{rank}\big(\mathcal{S}(\widehat{p})\big)\leq r.     Applications   Linear system identification , in which case the approximating matrix is Hankel structured .   I. Markovsky, Structured low-rank approximation and its applications, Automatica, Volume 44, Issue 4, April 2008, Pages 891‚Äì909. 1   Machine learning , in which case the approximating matrix is nonlinearly structured. 2  Recommender system , in which case the data matrix has missing values and the approximation is categorical .  Distance matrix completion , in which case there is a positive definiteness constraint.  Natural language processing , in which case the approximation is nonnegative .  Computer algebra , in which case the approximation is Sylvester structured .   Basic low-rank approximation problem  The unstructured problem with fit measured by the Frobenius norm , i.e.,       minimize   over   D  ^      ‚à•   D  -   D  ^    ‚à•   F   subject to   rank   (   D  ^   )     ‚â§  r       minimize    over   normal-^  D     subscript   norm    D   normal-^  D     F   subject to   rank   normal-^  D     r    \text{minimize}\quad\text{over }\widehat{D}\quad\|D-\widehat{D}\|_{\text{F}}%
 \quad\text{subject to}\quad\operatorname{rank}\big(\widehat{D}\big)\leq r   has analytic solution in terms of the singular value decomposition of the data matrix. The result is referred to as the matrix approximation lemma or Eckart‚ÄìYoung‚ÄìMirsky theorem. 3 Let       D  =   U  Œ£   V  ‚ä§    ‚àà   ‚Ñù   m  √ó  n     ,   m  ‚â§  n      formulae-sequence      D    U  normal-Œ£   superscript  V  top          superscript  ‚Ñù    m  n        m  n     D=U\Sigma V^{\top}\in\mathbb{R}^{m\times n},\quad m\leq n   be the singular value decomposition of   D   D   D   and partition   U   U   U   ,    Œ£  =  :  diag   (   œÉ  1   ,  ‚Ä¶  ,   œÉ  m   )      fragments  Œ£   normal-:  diag   fragments  normal-(   subscript  œÉ  1   normal-,  normal-‚Ä¶  normal-,   subscript  œÉ  m   normal-)     \Sigma=:\operatorname{diag}(\sigma_{1},\ldots,\sigma_{m})   , and   V   V   V   as follows:      U  =  :   [      U  1      U  2      ]   ,  Œ£  =  :   [      Œ£  1     0      0     Œ£  2      ]   ,  and  V  =  :   [      V  1      V  2      ]   ,     fragments  U   normal-:     subscript  U  1    subscript  U  2     normal-,  Œ£   normal-:     subscript  normal-Œ£  1   0    0   subscript  normal-Œ£  2     normal-,  and   V   normal-:     subscript  V  1    subscript  V  2     normal-,    U=:\begin{bmatrix}U_{1}&U_{2}\end{bmatrix},\quad\Sigma=:\begin{bmatrix}\Sigma_%
 {1}&0\\
 0&\Sigma_{2}\end{bmatrix},\quad\text{and}\quad V=:\begin{bmatrix}V_{1}&V_{2}%
 \end{bmatrix},   where    Œ£  1     subscript  normal-Œ£  1    \Sigma_{1}   is    r  √ó  r      r  r    r\times r   ,    U  1     subscript  U  1    U_{1}   is    m  √ó  r      m  r    m\times r   , and    V  1     subscript  V  1    V_{1}   is    n  √ó  r      n  r    n\times r   . Then the rank-   r   r   r   matrix, obtained from the truncated singular value decomposition         D  ^   *   =    U  1    Œ£  1    V  1  ‚ä§     ,       superscript   normal-^  D        subscript  U  1    subscript  normal-Œ£  1    superscript   subscript  V  1   top      \widehat{D}^{*}=U_{1}\Sigma_{1}V_{1}^{\top},   is such that         ‚à•   D  -    D  ^   *    ‚à•   F   =    min    rank   (   D  ^   )    ‚â§  r      ‚à•   D  -   D  ^    ‚à•   F    =     œÉ   r  +  1   2   +  ‚ãØ  +   œÉ  m  2      .         subscript   norm    D   superscript   normal-^  D       F      subscript      rank   normal-^  D    r     subscript   norm    D   normal-^  D     F              subscript   superscript  œÉ  2     r  1    normal-‚ãØ   subscript   superscript  œÉ  2   m        \|D-\widehat{D}^{*}\|_{\text{F}}=\min_{\operatorname{rank}(\widehat{D})\leq r}%
 \|D-\widehat{D}\|_{\text{F}}=\sqrt{\sigma^{2}_{r+1}+\cdots+\sigma^{2}_{m}}.   The minimizer     D  ^   *     superscript   normal-^  D      \widehat{D}^{*}   is unique if and only if     œÉ   r  +  1    ‚â†   œÉ  r        subscript  œÉ    r  1     subscript  œÉ  r     \sigma_{r+1}\neq\sigma_{r}   .  Proof of Eckart‚ÄìYoung‚ÄìMirsky theorem      A  =    U  n    Œ£  n    V  n  ‚ä§        A     subscript  U  n    subscript  normal-Œ£  n    subscript   superscript  V  top   n      A=U_{n}\Sigma_{n}V^{\top}_{n}     where     U  n      subscript  U  n    U_{n}\quad   and     V   n  ‚ä§     subscript   superscript  V  top   n    \quad V^{\top}_{n}   are orthogonal matrices, and    Œ£  n     subscript  normal-Œ£  n    \Sigma_{n}   is a diagonal matrix with entries    (    œÉ  1    œÉ  2   ‚ãØ   œÉ  n    )       subscript  œÉ  1    subscript  œÉ  2   normal-‚ãØ   subscript  œÉ  n     (\sigma_{1}\sigma_{2}\cdots\sigma_{n})     s.t    (    œÉ  n   ‚â§   œÉ   n  -  1    ‚â§  ‚ãØ  ‚â§   œÉ  1    )         subscript  œÉ  n    subscript  œÉ    n  1         normal-‚ãØ        subscript  œÉ  1      (\sigma_{n}\leq\sigma_{n-1}\leq\cdots\leq\sigma_{1})   .  We claim that the best approximation for   A   A   A   be given by       A  k   =    Œ£   i  =  1   k    u  i    œÉ  i    v  i         superscript  A  k      subscript   superscript  normal-Œ£  k     i  1     subscript  u  i    subscript  œÉ  i    subscript  v  i      A^{k}=\Sigma^{k}_{i=1}u_{i}\sigma_{i}v_{i}     To Prove     A  k      superscript  A  k    A^{k}\quad   is indeed the Best approximation i.e.     ‚à•   A  -   A  k    ‚à•   is minimum      norm    A   superscript  A  k     is minimum    \|A-A^{k}\|\quad\text{is minimum}     Proof by Contradiction:  Let us suppose      ‚àÉ  B   s.t.    ‚à•   A  -  B   ‚à•   2  2     <    ‚à•   A  -   A  k    ‚à•   2  2     =   œÉ   k  +  1   2       formulae-sequence      B    s.t.   subscript   superscript   norm    A  B    2   2      subscript   superscript   norm    A   superscript  A  k     2   2       subscript   superscript  œÉ  2     k  1       \quad\exists\quad B\quad\text{s.t.}\|A-B\|^{2}_{2}<\|A-A^{k}\|^{2}_{2}\quad=%
 \quad\sigma^{2}_{k+1}         rank   (  B  )    ‚â§  k    (Assuming in Low Rank Approximation, we are approximating via a matrix whose rank  ‚â§  k      formulae-sequence     rank  B   k     (Assuming in Low Rank Approximation, we are approximating via a matrix whose rank  k     \operatorname{rank}(B)\leq k\quad\text{(Assuming in Low Rank Approximation, we%
  are approximating via a matrix whose rank}\leq k         dim   (   null   (  B  )    )    +   rank   (  B  )     =  n  ‚Üí   dim   (   null   (  B  )    )    ‚â•   n  -  k            dim   null  B     rank  B    n    normal-‚Üí      dim   null  normal-B           n  k      \operatorname{dim}(\operatorname{null}(B))+\operatorname{rank}(B)=n\rightarrow%
 \operatorname{dim(\operatorname{null}(B))}\geq n-k     Let    w  ‚àà   null   (  B  )        w   null  B     w\in\operatorname{null}(B)         ‚à•    (   A  -  B   )   w   ‚à•   2   =    ‚à•   A  w   ‚à•   2   <   œÉ   k  +  1           subscript   norm      A  B   w    2    subscript   norm    A  w    2         subscript  œÉ    k  1       \|(A-B)w\|_{2}=\|Aw\|_{2}<\sigma_{k+1}     We know that     ‚àÉ   (   k  +  1   )          k  1     \exists(k+1)\quad   dimension space     (   v  1   ,   v  2   ,  ‚ãØ  ,   v  n   )       subscript  v  1    subscript  v  2   normal-‚ãØ   subscript  v  n     (v_{1},v_{2},\cdots,v_{n})\quad    s.t.     V  ‚àà   span   (   v  1   ,   v  2   ,  ‚ãØ  ,   v  n   )       and    ‚à•   A  V   ‚à•   2    ‚â•   œÉ   k  +  1        formulae-sequence    V   span   subscript  v  1    subscript  v  2   normal-‚ãØ   subscript  v  n         and   subscript   norm    A  V    2     subscript  œÉ    k  1       V\in\operatorname{span}(v_{1},v_{2},\cdots,v_{n})\quad\text{and}\|AV\|_{2}\geq%
 \sigma_{k+1}     Since       n  -  k   +  k  +  1   >  n           n  k   k  1   n    n-k+k+1>n\quad   Hence a contradiction. So we get that    A  k     superscript  A  k    A^{k}   is the best approximation.  Weighted low-rank approximation problems  The Frobenius norm weights uniformly all elements of the approximation error    D  -   D  ^       D   normal-^  D     D-\widehat{D}   . Prior knowledge about distribution of the errors can be taken into account by considering the weighted low-rank approximation problem        minimize   over   D  ^        vec  ‚ä§    (   D  -   D  ^    )    W    vec   (   D  -   D  ^    )     subject to   rank   (   D  ^   )     ‚â§  r   ,       minimize    over   normal-^  D         superscript  vec  top     D   normal-^  D     W    vec    D   normal-^  D      subject to   rank   normal-^  D     r    \text{minimize}\quad\text{over }\widehat{D}\quad\operatorname{vec}^{\top}(D-%
 \widehat{D})W\operatorname{vec}(D-\widehat{D})\quad\text{subject to}\quad%
 \operatorname{rank}(\widehat{D})\leq r,   where    v  e  c   (  A  )       v  e  c  A    vec(A)    vectorizes the matrix   A   A   A   column wise and   W   W   W   is a given positive (semi)definite weight matrix.  The general weighted low-rank approximation problem does not admit an analytic solution in terms of the singular value decomposition and is solved by local optimization methods, which provide no guarantee that a globally optimal solution is found.  Image and kernel representations of the rank constraints  Using the equivalences        rank   (   D  ^   )    ‚â§   r  ‚áî      there are  P   ‚àà     \R    m  √ó  r    and  L   ‚àà     \R    r  √ó  n    such that   D  ^    =   P  L       formulae-sequence     rank   normal-^  D     r  iff          there are  P      superscript  \R    m  r    and  L           superscript  \R    r  n    such that   normal-^  D           P  L       \operatorname{rank}(\widehat{D})\leq r\quad\iff\quad\text{there are }P\in\R^{m%
 \times r}\text{ and }L\in\R^{r\times n}\text{ such that }\widehat{D}=PL   and        rank   (   D  ^   )    ‚â§   r  ‚áî      there is full row rank  R   ‚àà     \R    m  -   r  √ó  m     such that  R   D  ^    =  0      formulae-sequence     rank   normal-^  D     r  iff          there is full row rank  R      superscript  \R    m    r  m     such that  R   normal-^  D         0      \operatorname{rank}(\widehat{D})\leq r\quad\iff\quad\text{there is full row %
 rank }R\in\R^{m-r\times m}\text{ such that }R\widehat{D}=0   the weighted low-rank approximation problem becomes equivalent to the parameter optimization problems       minimize   over   D  ^    ,   P  and  L       vec  ‚ä§    (   D  -   D  ^    )    W    vec   (   D  -   D  ^    )     subject to   D  ^    =   P  L        minimize    over   normal-^  D      P  and  L        superscript  vec  top     D   normal-^  D     W    vec    D   normal-^  D      subject to   normal-^  D      P  L     \text{minimize}\quad\text{over }\widehat{D},P\text{ and }L\quad\operatorname{%
 vec}^{\top}(D-\widehat{D})W\operatorname{vec}(D-\widehat{D})\quad\text{subject%
  to}\quad\widehat{D}=PL   and         minimize   over   D  ^   and  R       vec  ‚ä§    (   D  -   D  ^    )    W    vec   (   D  -   D  ^    )     subject to   R   D  ^     =  0     and   R   R  ‚ä§     =   I  r     ,     formulae-sequence     minimize    over   normal-^  D   and  R        superscript  vec  top     D   normal-^  D     W    vec    D   normal-^  D      subject to    R   normal-^  D     0      and    R   superscript  R  top      subscript  I  r      \text{minimize}\quad\text{over }\widehat{D}\text{ and }R\quad\operatorname{vec%
 }^{\top}(D-\widehat{D})W\operatorname{vec}(D-\widehat{D})\quad\text{subject to%
 }\quad R\widehat{D}=0\quad\text{and}\quad RR^{\top}=I_{r},   where    I  r     subscript  I  r    I_{r}   is the identity matrix of size   r   r   r   .  Alternating projections algorithm  The image representation of the rank constraint suggests a parameter optimization methods, in which the cost function is minimized alternatively over one of the variables (   P   P   P   or   L   L   L   ) with the other one fixed. Although simultaneous minimization over both   P   P   P   and   L   L   L   is a difficult biconvex optimization problem, minimization over one of the variables alone is a linear least squares problem and can be solved globally and efficiently.  The resulting optimization algorithm (called alternating projections) is globally convergent with a linear convergence rate to a locally optimal solution of the weighted low-rank approximation problem. Starting value for the   P   P   P   (or   L   L   L   ) parameter should be given. The iteration is stopped when a user defined convergence condition is satisfied.  Matlab implementation of the alternating projections algorithm for weighted low-rank approximation:  function [dh, f] = wlra_ap(d, w, p, tol, maxiter)
 [m, n] = size(d); r = size(p, 2 ); f = inf;
 for i = 2 :maxiter % minimization over L bp = kron(eye(n), p);
     vl = (bp' * w * bp) \ bp' * w * d(:);
     l  = reshape(vl, r, n); % minimization over P bl = kron(l', eye(m));
     vp = (bl' * w * bl) \ bl' * w * d(:);
     p  = reshape(vp, m, r); % check exit condition dh = p * l; dd = d - dh;
     f(i) = dd(:)' * w * dd(:);
     if abs(f(i - 1 ) - f(i)) < tol, break, end
 end  Variable projections algorithm  The alternating projections algorithm exploits the fact that the low rank approximation problem, parameterized in the image form, is bilinear in the variables   P   P   P   or   L   L   L   . The bilinear nature of the problem is effectively used in an alternative approach, called variable projections. 4  Consider again the weighted low rank approximation problem, parameterized in the image form. Minimization with respect to the   L   L   L   variable (a linear least squares problem) leads to the closed form expression of the approximation error as a function of   P   P   P           f   (  P  )    =       vec  ‚ä§    (  D  )     (   W  -   W   (    I  n   ‚äó  P   )     (     (    I  n   ‚äó  P   )   ‚ä§   W   (    I  n   ‚äó  P   )    )    -  1      (    I  n   ‚äó  P   )   ‚ä§   W    )     vec   (  D  )       .        f  P          superscript  vec  top   D     W    W   tensor-product   subscript  I  n   P    superscript     superscript   tensor-product   subscript  I  n   P   top   W   tensor-product   subscript  I  n   P      1     superscript   tensor-product   subscript  I  n   P   top   W      vec  D       f(P)=\sqrt{\operatorname{vec}^{\top}(D)\Big(W-W(I_{n}\otimes P)\big((I_{n}%
 \otimes P)^{\top}W(I_{n}\otimes P)\big)^{-1}(I_{n}\otimes P)^{\top}W\Big)%
 \operatorname{vec}(D)}.   The original problem is therefore equivalent to the nonlinear least squares problem of minimizing    f   (  P  )       f  P    f(P)   with respect to   P   P   P   . For this purpose standard optimization methods, e.g. the Levenberg-Marquardt algorithm can be used.  Matlab implementation of the variable projections algorithm for weighted low-rank approximation:  function [dh, f] = wlra_varpro(d, w, p, tol, maxiter)
 prob = optimset(); prob.solver = 'lsqnonlin' ;
 prob.options = optimset( 'MaxIter' , maxiter, 'TolFun' , tol); 
 prob.x0 = p; prob.objective = @(p) cost_fun(p, d, w);
 [p, f ] = lsqnonlin(prob); 
 [f, vl] = cost_fun(p, d, w); 
 dh = p * reshape(vl, size(p, 2 ), size(d, 2 ));
 
 function [f, vl] = cost_fun(p, d, w)
 bp = kron(eye(size(d, 2 )), p);
 vl = (bp' * w * bp) \ bp' * w * d(:);
 f = d(:)' * w * (d(:) - bp * vl);  The variable projections approach can be applied also to low rank approximation problems parameterized in the kernel form. The method is effective when the number of eliminated variables is much larger than the number of optimization variables left at the stage of the nonlinear least squares minimization. Such problems occur in system identification, parameterized in the kernel form, where the eliminated variables are the approximating trajectory and the remaining variables are the model parameters. In the context of linear time-invariant systems , the elimination step is equivalent to Kalman smoothing .  See also   CUR matrix approximation is made from the rows and columns of the original matrix   References   M. T. Chu, R. E. Funderlic, R. J. Plemmons, Structured low-rank approximation, Linear Algebra and its Applications, Volume 366, 1 June 2003, Pages 157‚Äì172   External links   C++ package for structured-low rank approximation   "  Category:Numerical linear algebra  Category:Dimension reduction  Category:Mathematical optimization     I. Markovsky, J. C. Willems, S. Van Huffel, B. De Moor, and R. Pintelon, Application of structured total least squares for system identification and model reduction. IEEE Transactions on Automatic Control, Volume 50, Number 10, 2005, pages 1490‚Äì1500. ‚Ü©  I. Markovsky, Low-Rank Approximation: Algorithms, Implementation, Applications, Springer, 2012, ISBN 978-1-4471-2226-5 ‚Ü©  C. Eckart, G. Young, The approximation of one matrix by another of lower rank. Psychometrika, Volume 1, 1936, Pages 211‚Äì8. ‚Ü©  G. Golub and V. Pereyra, Separable nonlinear least squares: the variable projection method and its applications, Institute of Physics, Inverse Problems, Volume 19, 2003, Pages 1-26. ‚Ü©    