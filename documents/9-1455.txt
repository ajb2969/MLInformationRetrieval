


Marsaglia polar method




Marsaglia polar method

 table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
   margin: 0; padding: 0; vertical-align: baseline; border: none; }
 <style>
 table.sourceCode { width: 100%; line-height: 100%; }
 td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
 td.sourceCode { padding-left: 5px; }
 code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
 code > span.dt { color: #902000; } /* DataType */
 code > span.dv { color: #40a070; } /* DecVal */
 code > span.bn { color: #40a070; } /* BaseN */
 code > span.fl { color: #40a070; } /* Float */
 code > span.ch { color: #4070a0; } /* Char */
 code > span.st { color: #4070a0; } /* String */
 code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
 code > span.ot { color: #007020; } /* Other */
 code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
 code > span.fu { color: #06287e; } /* Function */
 code > span.er { color: #ff0000; font-weight: bold; } /* Error */
 code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 code > span.cn { color: #880000; } /* Constant */
 code > span.sc { color: #4070a0; } /* SpecialChar */
 code > span.vs { color: #4070a0; } /* VerbatimString */
 code > span.ss { color: #bb6688; } /* SpecialString */
 code > span.im { } /* Import */
 code > span.va { color: #19177c; } /* Variable */
 code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
 code > span.op { color: #666666; } /* Operator */
 code > span.bu { } /* BuiltIn */
 code > span.ex { } /* Extension */
 code > span.pp { color: #bc7a00; } /* Preprocessor */
 code > span.at { color: #7d9029; } /* Attribute */
 code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
 code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
 code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
 code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
   



The polar method (attributed to George Marsaglia, 19641) is a pseudo-random number sampling method for generating a pair of independent standard normal random variables.2 While it is superior to the Box–Muller transform,3 the Ziggurat algorithm is even more efficient.4
Standard normal random variables are frequently used in computer science, computational statistics, and in particular, in applications of the Monte Carlo method.
The polar method works by choosing random points (x, y) in the square −1  s=x^2+y^2 
and then returning the required pair of normal random variables as



Theoretical basis
The underlying theory may be summarized as follows:
If u is uniformly distributed in the interval 0 ≤ u 2 + y2 = 1, and multiplying that point by an independent random variable ρ whose distribution is



whose coordinates are jointly distributed as two independent standard normal random variables.
History
This idea dates back to Laplace, whom Gauss credits with finding the above



by taking the square root of



The transformation to polar coordinates makes evident that θ is uniformly distributed (constant density) from 0 to 2π, and that the radial distance r has density



(r2 has the appropriate chi square distribution.)
This method of producing a pair of independent standard normal variates by radially projecting a random point on the unit circumference to a distance given by the square root of a chi-square-2 variate is called the polar method for generating a pair of normal random variables,
Practical considerations
A direct application of this idea,



is called the Box Muller transform, in which the chi variate is usually generated as



but that transform requires logarithm, square root, sine and cosine functions. On some processors, the cosine and sine of the same argument can be calculated in parallel using a single instruction.5 Notably for Intel based machines, one can use fsincos assembler instruction or the expi instruction (available e.g. in D), to calculate complex



and just separate the real and imaginary parts.
The polar method, in which a random point (x, y) inside the unit circle is projected onto the unit circumference by setting s = x2 + y2 and forming the point



is a faster procedure. Some researchers argue that the conditional if instruction (for rejecting a point outside of the unit circle), can make programs slower on modern processors equipped with pipelining and branch prediction.6 Also this procedure requires about 21% more evaluations of the underlying random number generator (only 
 
 
 
  of generated points lie inside of unit circle).
That random point on the circumference is then radially projected the required random distance by means of
$$\sqrt{-2\ln(s)}, \,$$
using the same s because that s is independent of the random point on the circumference and is itself uniformly distributed from 0 to 1.
Implementation
Simple implementation in Java using the mean and standard deviation:
private static double spare;
 private static boolean isSpareReady = false;
 
 public static synchronized double getGaussian(double mean, double stdDev) {
     if (isSpareReady) {
         isSpareReady = false;
         return spare * stdDev + mean;
     } else {
         double u, v, s;
         do {
             u = Math.random() * 2 - 1;
             v = Math.random() * 2 - 1;
             s = u * u + v * v;
         } while (s >= 1 || s == 0);
         double mul = Math.sqrt(-2.0 * Math.log(s) / s);
         spare = v * mul;
         isSpareReady = true;
         return mean + stdDev * u * mul;
     }
 }
An implementation in C++ using the variance:
double generateGaussianNoise(const double& mean, const double &variance;)
 {
     static bool hasSpare = false;
     static double spare;
 
     if(hasSpare)
     {
         hasSpare = false;
         return mean + variance * spare;
     }
 
     hasSpare = true;
     static double u, v, s;
     do
     {
         u = (rand() / ((double) RAND_MAX)) * 2.0 - 1.0;
         v = (rand() / ((double) RAND_MAX)) * 2.0 - 1.0;
         s = u * u + v * v;
     }
     while( (s >= 1.0) || (s == 0.0) );
 
     s = sqrt(-2.0 * log(s) / s);
     spare = v * s;
     return mean + variance * u * s;
 }
References
"
Category:Monte Carlo methods Category:Pseudorandom number generators Category:Non-uniform random numbers



A convenient method for generating normal variables, G. Marsaglia and T. A. Bray, SIAM Rev. 6, 260–264, 1964
Peter E. Kloeden Eckhard Platen Henri Schurz, Numerical Solution of SDE Through Computer Experiments, Springer, 1994.
.
http://doi.acm.org/10.1145/1287620.1287622 Gaussian Random Number Generators, D. Thomas and W. Luk and P. Leong and J. Villasenor, ACM Computing Surveys, Vol. 39(4), Article 11, 2007, 

This effect can be heightened in a GPU generating many variates in parallel, where a rejection on one processor can slow down many other processors. See section 7 of .



