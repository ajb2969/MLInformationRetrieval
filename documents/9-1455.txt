   Marsaglia polar method      Marsaglia polar method  table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
   margin: 0; padding: 0; vertical-align: baseline; border: none; }
 <style>
 table.sourceCode { width: 100%; line-height: 100%; }
 td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
 td.sourceCode { padding-left: 5px; }
 code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
 code > span.dt { color: #902000; } /* DataType */
 code > span.dv { color: #40a070; } /* DecVal */
 code > span.bn { color: #40a070; } /* BaseN */
 code > span.fl { color: #40a070; } /* Float */
 code > span.ch { color: #4070a0; } /* Char */
 code > span.st { color: #4070a0; } /* String */
 code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
 code > span.ot { color: #007020; } /* Other */
 code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
 code > span.fu { color: #06287e; } /* Function */
 code > span.er { color: #ff0000; font-weight: bold; } /* Error */
 code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 code > span.cn { color: #880000; } /* Constant */
 code > span.sc { color: #4070a0; } /* SpecialChar */
 code > span.vs { color: #4070a0; } /* VerbatimString */
 code > span.ss { color: #bb6688; } /* SpecialString */
 code > span.im { } /* Import */
 code > span.va { color: #19177c; } /* Variable */
 code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
 code > span.op { color: #666666; } /* Operator */
 code > span.bu { } /* BuiltIn */
 code > span.ex { } /* Extension */
 code > span.pp { color: #bc7a00; } /* Preprocessor */
 code > span.at { color: #7d9029; } /* Attribute */
 code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
 code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
 code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
 code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */     The polar method (attributed to George Marsaglia , 1964 1 ) is a pseudo-random number sampling method for generating a pair of independent standard normal random variables . 2 While it is superior to the Box–Muller transform , 3 the Ziggurat algorithm is even more efficient. 4  Standard normal random variables are frequently used in computer science , computational statistics , and in particular, in applications of the Monte Carlo method .  The polar method works by choosing random points ( x , y ) in the square −1  s=x^2+y^2  and then returning the required pair of normal random variables as        x      -   2   ln   (  s  )      s      ,   y     -   2   ln   (  s  )      s      .       x          2    s     s       y          2    s     s       x\sqrt{\frac{-2\ln(s)}{s}}\,,\ \ y\sqrt{\frac{-2\ln(s)}{s}}.     Theoretical basis  The underlying theory may be summarized as follows:  If u is uniformly distributed in the interval 0 ≤ u 2 + y 2 = 1, and multiplying that point by an independent random variable ρ whose distribution is      I  =    ∫   -  ∞   ∞      e   -    x  2   /  2      d  x        I    superscript   subscript             superscript  e       superscript  x  2   2     d  x      I=\int_{-\infty}^{\infty}e^{-x^{2}/2}\,dx     whose coordinates are jointly distributed as two independent standard normal random variables.  History  This idea dates back to Laplace , whom Gauss credits with finding the above        I  2   =    ∫   -  ∞   ∞     ∫   -  ∞   ∞      e   -    (    x  2   +   y  2    )   /  2      d   x   d  y     =    ∫  0   2  π      ∫  0  ∞    r    e   -    r  2   /  2      d   r   d  θ      .         superscript  I  2     superscript   subscript            superscript   subscript             superscript  e         superscript  x  2    superscript  y  2    2     d  x  d  y            superscript   subscript   0     2  π      superscript   subscript   0       r   superscript  e       superscript  r  2   2     d  r  d  θ        I^{2}=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x^{2}+y^{2})/2}\,dx\,%
 dy=\int_{0}^{2\pi}\int_{0}^{\infty}re^{-r^{2}/2}\,dr\,d\theta.     by taking the square root of       r   e   -    r  2   /  2      .      r   superscript  e       superscript  r  2   2       re^{-r^{2}/2}.\,     The transformation to polar coordinates makes evident that θ is uniformly distributed (constant density) from 0 to 2π, and that the radial distance r has density       x  =     -   2   ln   (   u  1   )        cos   (   2  π   u  2    )      ,   y  =     -   2   ln   (   u  1   )        sin   (   2  π   u  2    )         formulae-sequence    x          2     subscript  u  1           2  π   subscript  u  2         y          2     subscript  u  1           2  π   subscript  u  2         x=\sqrt{-2\ln(u_{1})}\cos(2\pi u_{2}),\quad y=\sqrt{-2\ln(u_{1})}\sin(2\pi u_{%
 2})     ( r 2 has the appropriate chi square distribution.)  This method of producing a pair of independent standard normal variates by radially projecting a random point on the unit circumference to a distance given by the square root of a chi-square-2 variate is called the polar method for generating a pair of normal random variables,  Practical considerations  A direct application of this idea,        -   2   ln   (   u  1   )       ;          2     subscript  u  1        \sqrt{-2\ln(u_{1})};     is called the Box Muller transform , in which the chi variate is usually generated as        expi   (  z  )    =   e   i  z    =    cos   (  z  )    +   i   sin   (  z  )       ,          expi  z    superscript  e    i  z             z     i    z        \text{expi}(z)=e^{iz}=\cos(z)+i\sin(z),\,     but that transform requires logarithm, square root, sine and cosine functions. On some processors, the cosine and sine of the same argument can be calculated in parallel using a single instruction. 5 Notably for Intel based machines, one can use fsincos assembler instruction or the expi instruction (available e.g. in D), to calculate complex       (   x   s    ,   y   s    )   ,       x    s      y    s      \left(\frac{x}{\sqrt{s}},\frac{y}{\sqrt{s}}\right),\,     and just separate the real and imaginary parts.  The polar method, in which a random point ( x , y ) inside the unit circle is projected onto the unit circumference by setting s = x 2 + y 2 and forming the point       π  /  4   ≈   79  %         π  4    percent  79     \pi/4\approx 79\%     is a faster procedure. Some researchers argue that the conditional if instruction (for rejecting a point outside of the unit circle), can make programs slower on modern processors equipped with pipelining and branch prediction. 6 Also this procedure requires about 21% more evaluations of the underlying random number generator (only      -   2   ln   (  s  )       ,          2    s       \sqrt{-2\ln(s)},\,   of generated points lie inside of unit circle).  That random point on the circumference is then radially projected the required random distance by means of  $$\sqrt{-2\ln(s)}, \,$$  using the same s because that s is independent of the random point on the circumference and is itself uniformly distributed from 0 to 1.  Implementation  Simple implementation in Java using the mean and standard deviation:  private  static  double spare; private  static  boolean isSpareReady = false ; public  static  synchronized  double  getGaussian ( double mean, double stdDev) { if (isSpareReady) {
         isSpareReady = false ; return spare * stdDev + mean;
     } else { double u, v, s; do {
             u = Math. random () * 2 - 1 ;
             v = Math. random () * 2 - 1 ;
             s = u * u + v * v;
         } while (s >= 1 || s == 0 ); double mul = Math. sqrt (- 2.0 * Math. log (s) / s);
         spare = v * mul;
         isSpareReady = true ; return mean + stdDev * u * mul;
     }
 }  An implementation in C++ using the variance :  double generateGaussianNoise( const  double & mean, const  double &variance;)
 { static bool hasSpare = false; static  double spare; if (hasSpare)
     {
         hasSpare = false; return mean + variance * spare;
     }
 
     hasSpare = true; static  double u, v, s; do {
         u = (rand() / (( double ) RAND_MAX)) * 2.0 - 1.0 ;
         v = (rand() / (( double ) RAND_MAX)) * 2.0 - 1.0 ;
         s = u * u + v * v;
     } while ( (s >= 1.0 ) || (s == 0.0 ) );
 
     s = sqrt(- 2.0 * log(s) / s);
     spare = v * s; return mean + variance * u * s;
 }  References  "  Category:Monte Carlo methods  Category:Pseudorandom number generators  Category:Non-uniform random numbers     A convenient method for generating normal variables, G. Marsaglia and T. A. Bray, SIAM Rev. 6, 260–264, 1964 ↩  Peter E. Kloeden Eckhard Platen Henri Schurz, Numerical Solution of SDE Through Computer Experiments, Springer, 1994. ↩  . ↩  http://doi.acm.org/10.1145/1287620.1287622 Gaussian Random Number Generators, D. Thomas and W. Luk and P. Leong and J. Villasenor, ACM Computing Surveys, Vol. 39(4), Article 11, 2007, ↩  ↩  This effect can be heightened in a GPU generating many variates in parallel, where a rejection on one processor can slow down many other processors. See section 7 of . ↩    