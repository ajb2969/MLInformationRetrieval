   FastICA      FastICA   FastICA is an efficient and popular algorithm for independent component analysis invented by Aapo HyvÃ¤rinen at Helsinki University of Technology . 1 2 The algorithm is based on a fixed-point iteration scheme maximizing non-Gaussianity as a measure of statistical independence . It can also be derived as an approximative Newton iteration.  Algorithm  Preprocess the data  Before the FastICA algorithm can be applied, the input vector data   ğ±   ğ±   \mathbf{x}   should be centered and whitened.  Centering the data  The input data   ğ±   ğ±   \mathbf{x}   is centered by computing the mean of each component of   ğ±   ğ±   \mathbf{x}   and subtracting that mean. This has the effect of making each component have zero mean. Thus:       ğ±  â†   ğ±  -   E   {  ğ±  }        normal-â†  ğ±    ğ±    E   ğ±       \mathbf{x}\leftarrow\mathbf{x}-E\left\{\mathbf{x}\right\}      Whitening the data  Whitening the data involves linearly transforming the data so that the new components are uncorrelated and have variance one. If    ğ±  ~     normal-~  ğ±    \widetilde{\mathbf{x}}   is the whitened data, then the covariance matrix of the whitened data is the identity matrix:        E   {    ğ±  ~     ğ±  ~   T    }    =  ğˆ        E      normal-~  ğ±    superscript   normal-~  ğ±   T      ğˆ    E\left\{\widetilde{\mathbf{x}}\widetilde{\mathbf{x}}^{T}\right\}=\mathbf{I}      This can be done using eigenvalue decomposition of the covariance matrix of the data     E   {   ğ±ğ±  T   }    =   ğ„ğƒğ„  T         E    superscript  ğ±ğ±  T      superscript  ğ„ğƒğ„  T     E\left\{\mathbf{x}\mathbf{x}^{T}\right\}=\mathbf{E}\mathbf{D}\mathbf{E}^{T}   , where   ğ„   ğ„   \mathbf{E}   is the matrix of eigenvectors and   ğƒ   ğƒ   \mathbf{D}   is the diagonal matrix of eigenvalues. Once eigenvalue decomposition is done, the whitened data is:       ğ±  â†    ğ„ğƒ   -   1  /  2      ğ„  T   ğ±      normal-â†  ğ±     superscript  ğ„ğƒ      1  2      superscript  ğ„  T   ğ±     \mathbf{x}\leftarrow\mathbf{E}\mathbf{D}^{-1/2}\mathbf{E}^{T}\mathbf{x}      Single component extraction  The iterative algorithm finds the direction for the weight vector   ğ°   ğ°   \mathbf{w}   maximizing the non-Gaussianity of the projection     ğ°  T   ğ±       superscript  ğ°  T   ğ±    \mathbf{w}^{T}\mathbf{x}   for the data   ğ±   ğ±   \mathbf{x}   . The function     g   (  u  )       g  u    g(u)   is the derivative of a nonquadratic nonlinearity function    f   (  u  )       f  u    f(u)   . HyvÃ¤rinen states that good equations for   f   f   f   (shown with their derivatives   g   g   g   and second derivatives    g  â€²     superscript  g  normal-â€²    {g}^{\prime}   ) are:         f   (  u  )    =   log   cosh   (  u  )      ;     g   (  u  )    =   tanh   (  u  )     ;     g  â€²    (  u  )    =   1  -    tanh  2    (  u  )          formulae-sequence      f  u       u      formulae-sequence      g  u     u         superscript  g  normal-â€²   u     1    superscript   2   u        f(u)=\log\cosh(u);\quad g(u)=\tanh(u);\quad{g}^{\prime}(u)=1-\tanh^{2}(u)             f   (  u  )    =   -   e   -    u  2   /  2       ;     g   (  u  )    =   u   e   -    u  2   /  2       ;     g  â€²    (  u  )    =    (   1  -   u  2    )    e   -    u  2   /  2           formulae-sequence      f  u      superscript  e       superscript  u  2   2        formulae-sequence      g  u     u   superscript  e       superscript  u  2   2            superscript  g  normal-â€²   u       1   superscript  u  2     superscript  e       superscript  u  2   2          f(u)=-e^{-u^{2}/2};\quad g(u)=ue^{-u^{2}/2};\quad{g}^{\prime}(u)=(1-u^{2})e^{-%
 u^{2}/2}      The first equation is a good general-purpose equation, while the second is highly robust.   Randomize the initial weight vector   ğ°   ğ°   \mathbf{w}     Let   \mathbf{w}^+Â \leftarrowÂ E\left\{\mathbf{x}Â g(\mathbf{w}^TÂ \mathbf{x})^T\right\}Â -  E\left\{g'(\mathbf{w}^TÂ \mathbf{x})\right\}\mathbf{w}   ,Â where     E   {  â€¦  }       E   normal-â€¦     E\left\{...\right\}    meansÂ averagingÂ overÂ allÂ column-vectorsÂ ofÂ matrix    ğ—   ğ—   \mathbf{X}      Let    ğ°  â†    ğ°  +   /   âˆ¥   ğ°  +   âˆ¥       normal-â†  ğ°     superscript  ğ°     norm   superscript  ğ°        \mathbf{w}\leftarrow\mathbf{w}^{+}/\|\mathbf{w}^{+}\|     If not converged, go back to 2   Multiple component extraction  The single unit iterative algorithm only estimates one of the independent components, to estimate more the algorithm must repeated, and the projection vectors decorrelated. Although HyvÃ¤rinen provides several ways of decorrelating results, the simplest multiple unit algorithm follows.   ğŸ   1   \mathbf{1}   indicates a column vector of 1's with dimension M .  Algorithm FastICA   Input:    C   C   C   Number of desired components  Input:     ğ—  âˆˆ   â„   N  Ã—  M        ğ—   superscript  â„    N  M      \mathbf{X}\in\mathbb{R}^{N\times M}   Matrix, where each column represents an N -dimensional sample, where    C  â‰¤  N      C  N    C<=N     Output:     ğ–  âˆˆ   â„   C  Ã—  N        ğ–   superscript  â„    C  N      \mathbf{W}\in\mathbb{R}^{C\times N}   Un-mixing matrix where each row projects X onto into independent component.  Output:     ğ’  âˆˆ   â„   C  Ã—  M        ğ’   superscript  â„    C  M      \mathbf{S}\in\mathbb{R}^{C\times M}   Independent components matrix, with M columns representing a sample with C dimensions.   for p in 1Â toÂ C:  ''      ğ°  ğ©   â†      normal-â†   subscript  ğ°  ğ©   absent    \mathbf{w_{p}}\leftarrow    RandomÂ vectorÂ ofÂ lengthÂ N   while      ğ°  ğ©     subscript  ğ°  ğ©    \mathbf{w_{p}}    changes  ''      ğ°  ğ©   â†     1  M   ğ—  g    (    ğ°  ğ©    T   ğ—   )   T    -    1  M    g  â€²    (    ğ°  ğ©    T   ğ—   )   ğŸ   ğ°  ğ©        normal-â†   subscript  ğ°  ğ©         1  M   ğ—  g   superscript     superscript   subscript  ğ°  ğ©   T   ğ—   T        1  M    superscript  g  normal-â€²      superscript   subscript  ğ°  ğ©   T   ğ—   1   subscript  ğ°  ğ©       \mathbf{w_{p}}\leftarrow\frac{1}{M}\mathbf{X}g(\mathbf{w_{p}}^{T}\mathbf{X})^{%
 T}-\frac{1}{M}g^{\prime}(\mathbf{w_{p}}^{T}\mathbf{X})\mathbf{1}\mathbf{w_{p}}     ''      ğ°  ğ©   â†    ğ°  ğ©   -    âˆ‘   j  =  1    p  -  1      ğ°  ğ£    ğ°  ğ©    T    ğ°  ğ£         normal-â†   subscript  ğ°  ğ©      subscript  ğ°  ğ©     superscript   subscript     j  1      p  1       subscript  ğ°  ğ£    superscript   subscript  ğ°  ğ©   T    subscript  ğ°  ğ£        \mathbf{w_{p}}\leftarrow\mathbf{w_{p}}-\sum_{j=1}^{p-1}\mathbf{w_{j}}\mathbf{w%
 _{p}}^{T}\mathbf{w_{j}}     ''      ğ°  ğ©   â†    ğ°  ğ©    âˆ¥   ğ°  ğ©   âˆ¥       normal-â†   subscript  ğ°  ğ©      subscript  ğ°  ğ©    norm   subscript  ğ°  ğ©       \mathbf{w_{p}}\leftarrow\frac{\mathbf{w_{p}}}{\|\mathbf{w_{p}}\|}     Output:      ğ–  =    [      ğ°  ğŸ       â‹®       ğ°  ğ‚      ]   T       ğ–   superscript     subscript  ğ°  1     normal-â‹®     subscript  ğ°  ğ‚     T     \mathbf{W}=\begin{bmatrix}\mathbf{w_{1}}\\
 \vdots\\
 \mathbf{w_{C}}\end{bmatrix}^{T}     Output:      ğ’  =  ğ–ğ—      ğ’  ğ–ğ—    \mathbf{S}=\mathbf{W}\mathbf{X}     See also   Unsupervised learning  Machine learning  The IT++ library features a FastICA implementation in C++   References  External links   FastICA package for Matlab or Octave  fastICA package in R programming language  FastICA in Java on SourceForge  FastICA in Java in RapidMiner .   "  Category:Multivariate statistics  Category:Computational statistics  Category:Machine learning algorithms     â†©  â†©     