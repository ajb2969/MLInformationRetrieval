   FastICA      FastICA   FastICA is an efficient and popular algorithm for independent component analysis invented by Aapo Hyvärinen at Helsinki University of Technology . 1 2 The algorithm is based on a fixed-point iteration scheme maximizing non-Gaussianity as a measure of statistical independence . It can also be derived as an approximative Newton iteration.  Algorithm  Preprocess the data  Before the FastICA algorithm can be applied, the input vector data   𝐱   𝐱   \mathbf{x}   should be centered and whitened.  Centering the data  The input data   𝐱   𝐱   \mathbf{x}   is centered by computing the mean of each component of   𝐱   𝐱   \mathbf{x}   and subtracting that mean. This has the effect of making each component have zero mean. Thus:       𝐱  ←   𝐱  -   E   {  𝐱  }        normal-←  𝐱    𝐱    E   𝐱       \mathbf{x}\leftarrow\mathbf{x}-E\left\{\mathbf{x}\right\}      Whitening the data  Whitening the data involves linearly transforming the data so that the new components are uncorrelated and have variance one. If    𝐱  ~     normal-~  𝐱    \widetilde{\mathbf{x}}   is the whitened data, then the covariance matrix of the whitened data is the identity matrix:        E   {    𝐱  ~     𝐱  ~   T    }    =  𝐈        E      normal-~  𝐱    superscript   normal-~  𝐱   T      𝐈    E\left\{\widetilde{\mathbf{x}}\widetilde{\mathbf{x}}^{T}\right\}=\mathbf{I}      This can be done using eigenvalue decomposition of the covariance matrix of the data     E   {   𝐱𝐱  T   }    =   𝐄𝐃𝐄  T         E    superscript  𝐱𝐱  T      superscript  𝐄𝐃𝐄  T     E\left\{\mathbf{x}\mathbf{x}^{T}\right\}=\mathbf{E}\mathbf{D}\mathbf{E}^{T}   , where   𝐄   𝐄   \mathbf{E}   is the matrix of eigenvectors and   𝐃   𝐃   \mathbf{D}   is the diagonal matrix of eigenvalues. Once eigenvalue decomposition is done, the whitened data is:       𝐱  ←    𝐄𝐃   -   1  /  2      𝐄  T   𝐱      normal-←  𝐱     superscript  𝐄𝐃      1  2      superscript  𝐄  T   𝐱     \mathbf{x}\leftarrow\mathbf{E}\mathbf{D}^{-1/2}\mathbf{E}^{T}\mathbf{x}      Single component extraction  The iterative algorithm finds the direction for the weight vector   𝐰   𝐰   \mathbf{w}   maximizing the non-Gaussianity of the projection     𝐰  T   𝐱       superscript  𝐰  T   𝐱    \mathbf{w}^{T}\mathbf{x}   for the data   𝐱   𝐱   \mathbf{x}   . The function     g   (  u  )       g  u    g(u)   is the derivative of a nonquadratic nonlinearity function    f   (  u  )       f  u    f(u)   . Hyvärinen states that good equations for   f   f   f   (shown with their derivatives   g   g   g   and second derivatives    g  ′     superscript  g  normal-′    {g}^{\prime}   ) are:         f   (  u  )    =   log   cosh   (  u  )      ;     g   (  u  )    =   tanh   (  u  )     ;     g  ′    (  u  )    =   1  -    tanh  2    (  u  )          formulae-sequence      f  u       u      formulae-sequence      g  u     u         superscript  g  normal-′   u     1    superscript   2   u        f(u)=\log\cosh(u);\quad g(u)=\tanh(u);\quad{g}^{\prime}(u)=1-\tanh^{2}(u)             f   (  u  )    =   -   e   -    u  2   /  2       ;     g   (  u  )    =   u   e   -    u  2   /  2       ;     g  ′    (  u  )    =    (   1  -   u  2    )    e   -    u  2   /  2           formulae-sequence      f  u      superscript  e       superscript  u  2   2        formulae-sequence      g  u     u   superscript  e       superscript  u  2   2            superscript  g  normal-′   u       1   superscript  u  2     superscript  e       superscript  u  2   2          f(u)=-e^{-u^{2}/2};\quad g(u)=ue^{-u^{2}/2};\quad{g}^{\prime}(u)=(1-u^{2})e^{-%
 u^{2}/2}      The first equation is a good general-purpose equation, while the second is highly robust.   Randomize the initial weight vector   𝐰   𝐰   \mathbf{w}     Let   \mathbf{w}^+ \leftarrow E\left\{\mathbf{x} g(\mathbf{w}^T \mathbf{x})^T\right\} -  E\left\{g'(\mathbf{w}^T \mathbf{x})\right\}\mathbf{w}   , where     E   {  …  }       E   normal-…     E\left\{...\right\}    means averaging over all column-vectors of matrix    𝐗   𝐗   \mathbf{X}      Let    𝐰  ←    𝐰  +   /   ∥   𝐰  +   ∥       normal-←  𝐰     superscript  𝐰     norm   superscript  𝐰        \mathbf{w}\leftarrow\mathbf{w}^{+}/\|\mathbf{w}^{+}\|     If not converged, go back to 2   Multiple component extraction  The single unit iterative algorithm only estimates one of the independent components, to estimate more the algorithm must repeated, and the projection vectors decorrelated. Although Hyvärinen provides several ways of decorrelating results, the simplest multiple unit algorithm follows.   𝟏   1   \mathbf{1}   indicates a column vector of 1's with dimension M .  Algorithm FastICA   Input:    C   C   C   Number of desired components  Input:     𝐗  ∈   ℝ   N  ×  M        𝐗   superscript  ℝ    N  M      \mathbf{X}\in\mathbb{R}^{N\times M}   Matrix, where each column represents an N -dimensional sample, where    C  ≤  N      C  N    C<=N     Output:     𝐖  ∈   ℝ   C  ×  N        𝐖   superscript  ℝ    C  N      \mathbf{W}\in\mathbb{R}^{C\times N}   Un-mixing matrix where each row projects X onto into independent component.  Output:     𝐒  ∈   ℝ   C  ×  M        𝐒   superscript  ℝ    C  M      \mathbf{S}\in\mathbb{R}^{C\times M}   Independent components matrix, with M columns representing a sample with C dimensions.   for p in 1 to C:  ''      𝐰  𝐩   ←      normal-←   subscript  𝐰  𝐩   absent    \mathbf{w_{p}}\leftarrow    Random vector of length N   while      𝐰  𝐩     subscript  𝐰  𝐩    \mathbf{w_{p}}    changes  ''      𝐰  𝐩   ←     1  M   𝐗  g    (    𝐰  𝐩    T   𝐗   )   T    -    1  M    g  ′    (    𝐰  𝐩    T   𝐗   )   𝟏   𝐰  𝐩        normal-←   subscript  𝐰  𝐩         1  M   𝐗  g   superscript     superscript   subscript  𝐰  𝐩   T   𝐗   T        1  M    superscript  g  normal-′      superscript   subscript  𝐰  𝐩   T   𝐗   1   subscript  𝐰  𝐩       \mathbf{w_{p}}\leftarrow\frac{1}{M}\mathbf{X}g(\mathbf{w_{p}}^{T}\mathbf{X})^{%
 T}-\frac{1}{M}g^{\prime}(\mathbf{w_{p}}^{T}\mathbf{X})\mathbf{1}\mathbf{w_{p}}     ''      𝐰  𝐩   ←    𝐰  𝐩   -    ∑   j  =  1    p  -  1      𝐰  𝐣    𝐰  𝐩    T    𝐰  𝐣         normal-←   subscript  𝐰  𝐩      subscript  𝐰  𝐩     superscript   subscript     j  1      p  1       subscript  𝐰  𝐣    superscript   subscript  𝐰  𝐩   T    subscript  𝐰  𝐣        \mathbf{w_{p}}\leftarrow\mathbf{w_{p}}-\sum_{j=1}^{p-1}\mathbf{w_{j}}\mathbf{w%
 _{p}}^{T}\mathbf{w_{j}}     ''      𝐰  𝐩   ←    𝐰  𝐩    ∥   𝐰  𝐩   ∥       normal-←   subscript  𝐰  𝐩      subscript  𝐰  𝐩    norm   subscript  𝐰  𝐩       \mathbf{w_{p}}\leftarrow\frac{\mathbf{w_{p}}}{\|\mathbf{w_{p}}\|}     Output:      𝐖  =    [      𝐰  𝟏       ⋮       𝐰  𝐂      ]   T       𝐖   superscript     subscript  𝐰  1     normal-⋮     subscript  𝐰  𝐂     T     \mathbf{W}=\begin{bmatrix}\mathbf{w_{1}}\\
 \vdots\\
 \mathbf{w_{C}}\end{bmatrix}^{T}     Output:      𝐒  =  𝐖𝐗      𝐒  𝐖𝐗    \mathbf{S}=\mathbf{W}\mathbf{X}     See also   Unsupervised learning  Machine learning  The IT++ library features a FastICA implementation in C++   References  External links   FastICA package for Matlab or Octave  fastICA package in R programming language  FastICA in Java on SourceForge  FastICA in Java in RapidMiner .   "  Category:Multivariate statistics  Category:Computational statistics  Category:Machine learning algorithms     ↩  ↩     