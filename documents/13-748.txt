   Errors-in-variables models      Errors-in-variables models   In statistics , errors-in-variables models or measurement error models 1 2 are regression models that account for measurement errors in the independent variables . In contrast, standard regression models assume that those regressors have been measured exactly, or observed without error; as such, those models account only for errors in the dependent variables , or responses.  In the case when some regressors have been measured with errors, estimation based on the standard assumption leads to inconsistent estimates, meaning that the parameter estimates do not tend to the true values even in very large samples. For simple linear regression the effect is an underestimate of the coefficient, known as the attenuation bias . In non-linear models the direction of the bias is likely to be more complicated. 3  Motivational example  Consider a simple linear regression model of the form         y  t   =   Œ±  +   Œ≤   x  t  *    +    Œµ  t      ,   t  =   1  ,  ‚Ä¶  ,  T     ,     formulae-sequence     subscript  y  t     Œ±    Œ≤   superscript   subscript  x  t       subscript  Œµ  t       t   1  normal-‚Ä¶  T      y_{t}=\alpha+\beta x_{t}^{*}+\varepsilon_{t}\,,\quad t=1,\ldots,T,   where x* denotes the true but unobserved value of the regressor. Instead we observe this value with an error:        x  t   =    x  t  *   +    Œ∑  t      ,       subscript  x  t      subscript   superscript  x    t    subscript  Œ∑  t      x_{t}=x^{*}_{t}+\eta_{t}\,,   where the measurement error Œ∑ t is assumed to be independent from the true value x* t .  If the y t ‚Ä≤s are simply regressed on the x t ‚Ä≤s (see simple linear regression ), then the estimator for the slope coefficient is        Œ≤  ^   =      1  T     ‚àë   t  =  1   T     (    x  t   -   x  ¬Ø    )    (    y  t   -   y  ¬Ø    )        1  T     ‚àë   t  =  1   T     (    x  t   -   x  ¬Ø    )   2        ,       normal-^  Œ≤         1  T     superscript   subscript     t  1    T        subscript  x  t    normal-¬Ø  x       subscript  y  t    normal-¬Ø  y           1  T     superscript   subscript     t  1    T    superscript     subscript  x  t    normal-¬Ø  x    2        \hat{\beta}=\frac{\tfrac{1}{T}\sum_{t=1}^{T}(x_{t}-\bar{x})(y_{t}-\bar{y})}{%
 \tfrac{1}{T}\sum_{t=1}^{T}(x_{t}-\bar{x})^{2}}\,,   which converges as the sample size T increases without bound:         Œ≤  ^      ‚Üí  ùëù      Cov   [   x  t   ,    y  t    ]     Var   [    x  t    ]     =    Œ≤   œÉ   x  *   2      œÉ   x  *   2   +   œÉ  Œ∑  2     =    Œ≤   1  +    œÉ  Œ∑  2   /   œÉ   x  *   2        .        p  normal-‚Üí    normal-^  Œ≤      Cov   subscript  x  t    subscript  y  t     Var   subscript  x  t              Œ≤   subscript   superscript  œÉ  2    superscript  x         superscript   subscript  œÉ   superscript  x     2    superscript   subscript  œÉ  Œ∑   2            Œ≤    1     superscript   subscript  œÉ  Œ∑   2    superscript   subscript  œÉ   superscript  x     2         \hat{\beta}\ \xrightarrow{p}\ \frac{\operatorname{Cov}[\,x_{t},y_{t}\,]}{%
 \operatorname{Var}[\,x_{t}\,]}=\frac{\beta\sigma^{2}_{x^{*}}}{\sigma_{x^{*}}^{%
 2}+\sigma_{\eta}^{2}}=\frac{\beta}{1+\sigma_{\eta}^{2}/\sigma_{x^{*}}^{2}}\,.   Variances are non-negative, so that in the limit the estimate is smaller in magnitude than the true value of Œ≤ , an effect which statisticians call attenuation or regression dilution . 4 Thus the "na—óve" least squares estimator is inconsistent in this setting. However, the estimator is a consistent estimator of the parameter required for a best linear predictor of y given x : in some applications this may be what is required, rather than an estimate of the "true" regression coefficient, although that would assume that the variance of the errors in observing x* remains fixed. This follows directly from the result quoted immediately above, and the fact that the regression coefficient relating the y t ‚Ä≤s to the actually observed x t ‚Ä≤s, in a simple linear regression, is given by        Œ≤  x   =    Cov   [   x  t   ,    y  t    ]     Var   [    x  t    ]      .       subscript  Œ≤  x      Cov   subscript  x  t    subscript  y  t     Var   subscript  x  t       \beta_{x}=\frac{\operatorname{Cov}[\,x_{t},y_{t}\,]}{\operatorname{Var}[\,x_{t%
 }\,]}.   It is this coefficient, rather than Œ≤ , that would be required for constructing a predictor of y based on an observed x which is subject to noise.  It can be argued that almost all existing data sets contain errors of different nature and magnitude, so that attenuation bias is extremely frequent (although in multivariate regression the direction of bias is ambiguous. 5  Jerry Hausman sees this as an iron law of econometrics : "The magnitude of the estimate is usually smaller than expected." 6  Specification  Usually measurement error models are described using the latent variables approach. If y is the response variable and x are observed values of the regressors, then we assume there exist some latent variables  y* and x* which follow the model's "true" functional relationship g , and such that the observed quantities are their noisy observations:      {       x  =    x  *   +  Œ∑    ,          y  =    y  *   +  Œµ    ,          y  *   =  g   (    x  *    ,   w   |  Œ∏  )   ,          cases    x     superscript  x    Œ∑    otherwise    y     superscript  y    Œµ    otherwise   fragments   superscript  y     g   fragments  normal-(   superscript  x    normal-,  w  normal-|  Œ∏  normal-)   normal-,   otherwise    \begin{cases}x=x^{*}+\eta,\\
 y=y^{*}+\varepsilon,\\
 y^{*}=g(x^{*}\!,w\,|\,\theta),\end{cases}     where Œ∏ is the model's parameter and w are those regressors which are assumed to be error-free (for example when linear regression contains an intercept, the regressor which corresponds to the constant certainly has no "measurement errors"). Depending on the specification these error-free regressors may or may not be treated separately; in the latter case it is simply assumed that corresponding entries in the variance matrix of Œ∑ 's are zero.  The variables y , x , w are all observed , meaning that the statistician possesses a data set of n  statistical units which follow the data generating process described above; the latent variables x* , y* , Œµ , and Œ∑ are not observed however.  This specification does not encompass all the existing EiV models. For example in some of them function g may be non-parametric or semi-parametric. Other approaches model the relationship between y* and x* as distributional instead of functional, that is they assume that y* conditionally on x* follows a certain (usually parametric) distribution.  Terminology and assumptions   The observed variable x may be called the manifest , indicator , or proxy variable.  The unobserved variable x* may be called the latent or true variable. It may be regarded either as an unknown constant (in which case the model is called a functional model ), or as a random variable (correspondingly a structural model ). 7  The relationship between the measurement error Œ∑ and the latent variable x* can be modeled in different ways:  Classical errors       Œ∑   ‚üÇ   x  *    ,     perpendicular-to  Œ∑   superscript  x      \scriptstyle\eta\,\perp\,x^{*},   the errors are independent from the latent variable. This is the most common assumption, it implies that the errors are introduced by the measuring device and their magnitude does not depend on the value being measured.  Mean-independence       E   [  Œ∑  |   x  *   ]    =  0   ,       normal-E  Œ∑   superscript  x     0    \scriptstyle\operatorname{E}[\eta|x^{*}]\,=\,0,   the errors are mean-zero for every value of the latent regressor. This is a less restrictive assumption than the classical one, as it allows for the presence of heteroscedasticity or other effects in the measurement errors.  Berkson's errors       Œ∑   ‚üÇ  x   ,     perpendicular-to  Œ∑  x    \scriptstyle\eta\,\perp\,x,   the errors are independent from the observed regressor x . This assumption has very limited applicability. One example is round-off errors: for example if a person's age* is a continuous random variable, whereas the observed age is truncated to the next smallest integer, then the truncation error is approximately independent from the observed age . Another possibility is with the fixed design experiment: for example if a scientist decides to make a measurement at a certain predetermined moment of time x , say at x = 10 s, then the real measurement may occur at some other value of x* (for example due to her finite reaction time) and such measurement error will be generally independent from the "observed" value of the regressor.  Misclassification errors : special case used for the dummy regressors . If x* is an indicator of a certain event or condition (such as person is male/female, some medical treatment given/not, etc.), then the measurement error in such regressor will correspond to the incorrect classification similar to type I and type II errors in statistical testing. In this case the error Œ∑ may take only 3 possible values, and its distribution conditional on x* is modeled with two parameters: Œ± = Pr[ Œ∑ =‚àí1 | x* =1], and Œ≤ = Pr[ Œ∑ =1 | x* =0]. The necessary condition for identification is that Œ±+Œ≤ <1, that is misclassification should not happen "too often". (This idea can be generalized to discrete variables with more than two possible values.)    Linear model  Linear errors-in-variables models were studied first, probably because linear models were so widely used and they are easier than non-linear ones. Unlike standard least squares regression (OLS), extending errors in variables regression (EiV) from the simple to the multivariable case is not straightforward.  Simple linear model  The simple linear errors-in-variables model was already presented in the "motivation" section:      {        y  t   =   Œ±  +   Œ≤   x  t  *    +   Œµ  t     ,           x  t   =    x  t  *   +   Œ∑  t     ,          cases     subscript  y  t     Œ±    Œ≤   superscript   subscript  x  t       subscript  Œµ  t     otherwise     subscript  x  t      superscript   subscript  x  t      subscript  Œ∑  t     otherwise    \begin{cases}y_{t}=\alpha+\beta x_{t}^{*}+\varepsilon_{t},\\
 x_{t}=x_{t}^{*}+\eta_{t},\end{cases}   where all variables are scalar. Here Œ± and Œ≤ are the parameters of interest, whereas œÉ Œµ and œÉ Œ∑ ‚Äî standard deviations of the error terms ‚Äî are the nuisance parameters . The "true" regressor x* is treated as a random variable ( structural model), independent from the measurement error Œ∑ ( classic assumption).  This model is identifiable in two cases: (1) either the latent regressor x* is not  normally distributed , (2) or x* has normal distribution, but neither Œµ t nor Œ∑ t are divisible by a normal distribution. 8 That is, the parameters Œ± , Œ≤ can be consistently estimated from the data set     (   x  t   ,   y  t   )    t  =  1   T     superscript   subscript    subscript  x  t    subscript  y  t      t  1    T    \scriptstyle(x_{t},\,y_{t})_{t=1}^{T}   without any additional information, provided the latent regressor is not Gaussian.  Before this identifiability result was established, statisticians attempted to apply the maximum likelihood technique by assuming that all variables are normal, and then concluded that the model is not identified. The suggested remedy was to assume that some of the parameters of the model are known or can be estimated from the outside source. Such estimation methods include 9   Deming regression ‚Äî assumes that the ratio Œ¥ = œÉ¬≤ Œµ / œÉ¬≤ Œ∑ is known. This could be appropriate for example when errors in y and x are both caused by measurements, and the accuracy of measuring devices or procedures are known. The case when Œ¥ = 1 is also known as the orthogonal regression .  Regression with known reliability ratio  Œª = œÉ¬≤ ‚àó / ( œÉ¬≤ Œ∑ + œÉ¬≤ ‚àó ), where œÉ¬≤ ‚àó is the variance of the latent regressor. Such approach may be applicable for example when repeating measurements of the same unit are available, or when the reliability ratio has been known from the independent study. In this case the consistent estimate of slope is equal to the least-squares estimate divided by Œª .  Regression with known œÉ¬≤ Œ∑ may occur when the source of the errors in x 's is known and their variance can be calculated. This could include rounding errors, or errors introduced by the measuring device. When œÉ¬≤ Œ∑ is known we can compute the reliability ratio as Œª = ( œÉ¬≤ x ‚àí œÉ¬≤ Œ∑ ) / œÉ¬≤ x and reduce the problem to the previous case.   Newer estimation methods that do not assume knowledge of some of the parameters of the model, include  Multivariable linear model  Multivariable model looks exactly like the linear model, only this time Œ≤ , Œ∑ t , x t and x* t are k√ó 1 vectors.      {        y  t   =   Œ±  +    Œ≤  ‚Ä≤    x  t  *    +   Œµ  t     ,           x  t   =    x  t  *   +   Œ∑  t     .          cases     subscript  y  t     Œ±     superscript  Œ≤  normal-‚Ä≤    superscript   subscript  x  t       subscript  Œµ  t     otherwise     subscript  x  t      superscript   subscript  x  t      subscript  Œ∑  t     otherwise    \begin{cases}y_{t}=\alpha+\beta^{\prime}x_{t}^{*}+\varepsilon_{t},\\
 x_{t}=x_{t}^{*}+\eta_{t}.\end{cases}     The general identifiability condition for this model remains an open question. It is known however that in the case when ( Œµ , Œ∑ ) are independent and jointly normal, the parameter Œ≤ is identified if and only if it is impossible to find a non-singular k√ók block matrix [ a A ] (where a is a k√ó 1 vector) such that a‚Ä≤x* is distributed normally and independently from A‚Ä≤x* . 10  Some of the estimation methods for multivariable linear models are  Non-linear models  A generic non-linear measurement error model takes form      {        y  t   =    g   (   x  t  *   )    +   Œµ  t     ,           x  t   =    x  t  *   +   Œ∑  t     .          cases     subscript  y  t       g   subscript   superscript  x    t     subscript  Œµ  t     otherwise     subscript  x  t      subscript   superscript  x    t    subscript  Œ∑  t     otherwise    \begin{cases}y_{t}=g(x^{*}_{t})+\varepsilon_{t},\\
 x_{t}=x^{*}_{t}+\eta_{t}.\end{cases}   Here function g can be either parametric or non-parametric. When function g is parametric it will be written as g(x*, Œ≤) .  For a general vector-valued regressor x* the conditions for model identifiability are not known. However in the case of scalar x* the model is identified unless the function g is of the "log-exponential" form 11       g   (   x  *   )    =   a  +   b   ln   (    e   c   x  *     +  d   )            g   superscript  x       a    b       superscript  e    c   superscript  x      d        g(x^{*})=a+b\ln\big(e^{cx^{*}}+d\big)   and the latent regressor x* has density        f   x  *     (  x  )    =   {       A   e    -   B   e   C  x      +   C  D  x       (    e   C  x    +  E   )    -  F     ,        if   d   >  0        A   e    -   B   x  2     +   C  x           if   d   =  0              subscript  f   superscript  x     x    cases    A   superscript  e        B   superscript  e    C  x        C  D  x      superscript     superscript  e    C  x    E     F         if  d   0     A   superscript  e        B   superscript  x  2       C  x          if  d   0      f_{x^{*}}(x)=\begin{cases}Ae^{-Be^{Cx}+CDx}(e^{Cx}+E)^{-F},&\text{if}\ d>0\\
 Ae^{-Bx^{2}+Cx}&\text{if}\ d=0\end{cases}   where constants A,B,C,D,E,F may depend on a,b,c,d .  Despite this optimistic result, as of now no methods exist for estimating non-linear errors-in-variables models without any extraneous information. However there are several techniques which make use of some additional data: either the instrumental variables, or repeated observations.  Instrumental variables methods  Repeated observations  In this approach two (or maybe more) repeated observations of the regressor x* are available. Both observations contain their own measurement errors, however those errors are required to be independent:      {        x   1  t    =    x  t  *   +   Œ∑   1  t      ,           x   2  t    =    x  t  *   +   Œ∑   2  t      ,          cases     subscript  x    1  t       subscript   superscript  x    t    subscript  Œ∑    1  t      otherwise     subscript  x    2  t       subscript   superscript  x    t    subscript  Œ∑    2  t      otherwise    \begin{cases}x_{1t}=x^{*}_{t}+\eta_{1t},\\
 x_{2t}=x^{*}_{t}+\eta_{2t},\end{cases}   where x* ‚ä• Œ∑ 1 ‚ä• Œ∑ 2 . Variables Œ∑ 1 , Œ∑ 2 need not be identically distributed (although if they are efficiency of the estimator can be slightly improved). With only these two observations it is possible to consistently estimate the density function of x* using Kotlarski's deconvolution technique. 12 \big( x_{j} - x^*_{j} \big),    where with slight abuse of notation x j denotes the j -th component of a vector. All densities in this formula can be estimated using inversion of the empirical characteristic functions . In particular,           œÜ  ^    Œ∑  j     (  v  )    =       œÜ  ^    x  j     (  v  ,  0  )       œÜ  ^    x  j  *     (  v  )       ,     where    œÜ  ^    x  j     (   v  1   ,   v  2   )    =     1  T       ‚àë   t  =  1   T     e    i   v  1    x   1  t  j     +   i   v  2    x   2  t  j          ,      œÜ  ^    x  j  *     (  v  )    =   exp     ‚à´  0  v          ‚àÇ    œÜ  ^    x  j      (  0  ,   v  2   )    /   ‚àÇ   v  1        œÜ  ^    x  j     (  0  ,   v  2   )      d   v  2         ,     formulae-sequence       subscript   normal-^  œÜ    subscript  Œ∑  j    v        subscript   normal-^  œÜ    subscript  x  j     v  0       subscript   normal-^  œÜ    subscript   superscript  x    j    v      formulae-sequence      where   subscript   normal-^  œÜ    subscript  x  j      subscript  v  1    subscript  v  2         1  T     superscript   subscript     t  1    T    superscript  e      i   subscript  v  1    subscript  x    1  t  j       i   subscript  v  2    subscript  x    2  t  j               subscript   normal-^  œÜ    subscript   superscript  x    j    v        superscript   subscript   0   v              subscript   normal-^  œÜ    subscript  x  j      0   subscript  v  2        subscript  v  1        subscript   normal-^  œÜ    subscript  x  j     0   subscript  v  2      d   subscript  v  2          \displaystyle\hat{\varphi}_{\eta_{j}}(v)=\frac{\hat{\varphi}_{x_{j}}(v,0)}{%
 \hat{\varphi}_{x^{*}_{j}}(v)},\quad\text{where }\hat{\varphi}_{x_{j}}(v_{1},v_%
 {2})=\frac{1}{T}\sum_{t=1}^{T}e^{iv_{1}x_{1tj}+iv_{2}x_{2tj}},\ \ \hat{\varphi%
 }_{x^{*}_{j}}(v)=\exp\int_{0}^{v}\frac{\partial\hat{\varphi}_{x_{j}}(0,v_{2})/%
 \partial v_{1}}{\hat{\varphi}_{x_{j}}(0,v_{2})}dv_{2},     In order to invert these characteristic function one has to apply the inverse Fourier transform, with a trimming parameter C needed to ensure the numerical stability. For example:          f  ^   x    (  x  )    =    1    (   2  œÄ   )   k      ‚à´   -  C   C    ‚ãØ    ‚à´   -  C   C     e   -   i   u  ‚Ä≤   x       œÜ  ^   x    (  u  )   d  u        .         subscript   normal-^  f   x   x       1   superscript    2  œÄ   k      superscript   subscript     C    C     normal-‚ãØ    superscript   subscript     C    C      superscript  e      i   superscript  u  normal-‚Ä≤   x      subscript   normal-^  œÜ   x   u  d  u         \hat{f}_{x}(x)=\frac{1}{(2\pi)^{k}}\int_{-C}^{C}\cdots\int_{-C}^{C}e^{-iu^{%
 \prime}x}\hat{\varphi}_{x}(u)du.     |2= Schennach's estimator 13 for a parametric linear-in-parameters nonlinear-in-variables model. This is a model of the form      {        y  t   =      ‚àë   j  =  1   k      Œ≤  j    g  j    (   x  t  *   )     +     ‚àë   j  =  1   ‚Ñì      Œ≤   k  +  j     w   j  t      +   Œµ  t     ,           x   1  t    =    x  t  *   +   Œ∑   1  t      ,           x   2  t    =    x  t  *   +   Œ∑   2  t      ,          cases     subscript  y  t       superscript   subscript     j  1    k      subscript  Œ≤  j    subscript  g  j    subscript   superscript  x    t       superscript   subscript     j  1    normal-‚Ñì      subscript  Œ≤    k  j     subscript  w    j  t       subscript  Œµ  t     otherwise     subscript  x    1  t       subscript   superscript  x    t    subscript  Œ∑    1  t      otherwise     subscript  x    2  t       subscript   superscript  x    t    subscript  Œ∑    2  t      otherwise    \begin{cases}y_{t}=\textstyle\sum_{j=1}^{k}\beta_{j}g_{j}(x^{*}_{t})+\sum_{j=1%
 }^{\ell}\beta_{k+j}w_{jt}+\varepsilon_{t},\\
 x_{1t}=x^{*}_{t}+\eta_{1t},\\
 x_{2t}=x^{*}_{t}+\eta_{2t},\end{cases}   where w t represents variables measured without errors. The regressor x* here is scalar (the method can be extended to the case of vector x* as well). If not for the measurement errors, this would have been a standard linear model with the estimator        Œ≤  ^   =     (    E  ^    [    Œæ  t     Œæ  t  ‚Ä≤     ]    )    -  1     E  ^    [    Œæ  t     y  t     ]     ,       normal-^  Œ≤      superscript     normal-^  normal-E    delimited-[]     subscript  Œæ  t    superscript   subscript  Œæ  t   normal-‚Ä≤        1     normal-^  normal-E    delimited-[]     subscript  Œæ  t    subscript  y  t        \hat{\beta}=\big(\hat{\operatorname{E}}[\,\xi_{t}\xi_{t}^{\prime}\,]\big)^{-1}%
 \hat{\operatorname{E}}[\,\xi_{t}y_{t}\,],   where        Œæ  t  ‚Ä≤   =   (    g  1    (   x  t  *   )    ,  ‚ãØ  ,    g  k    (   x  t  *   )    ,   w   1  ,  t    ,  ‚ãØ  ,   w   l  ,  t    )    .       superscript   subscript  Œæ  t   normal-‚Ä≤       subscript  g  1    subscript   superscript  x    t    normal-‚ãØ     subscript  g  k    subscript   superscript  x    t     subscript  w   1  t    normal-‚ãØ   subscript  w   l  t       \xi_{t}^{\prime}=(g_{1}(x^{*}_{t}),\cdots,g_{k}(x^{*}_{t}),w_{1,t},\cdots,w_{l%
 ,t}).     It turns out that all the expected values in this formula are estimable using the same deconvolution trick. In particular, for a generic observable w t (which could be 1, w 1 t , ‚Ä¶, w ‚Ñì ¬†t , or y t ) and some function h (which could represent any g j or g i g j ) we have        E   [    w  t   h   (   x  t  *   )    ]    =    1   2  œÄ      ‚à´   -  ‚àû   ‚àû     œÜ  h    (   -  u   )    œà  w    (  u  )   d  u      ,       normal-E     subscript  w  t   h   subscript   superscript  x    t         1    2  œÄ      superscript   subscript             subscript  œÜ  h     u    subscript  œà  w   u  d  u       \operatorname{E}[\,w_{t}h(x^{*}_{t})\,]=\frac{1}{2\pi}\int_{-\infty}^{\infty}%
 \varphi_{h}(-u)\psi_{w}(u)du,   where œÜ h is the Fourier transform of h ( x* ), but using the same convention as for the characteristic functions ,        œÜ  h    (  u  )    =   ‚à´    e   i  u  x    h   (  x  )   d  x           subscript  œÜ  h   u        superscript  e    i  u  x    h  x  d  x      \varphi_{h}(u)=\int e^{iux}h(x)dx   ,  and        œà  w    (  u  )    =   E   [    w  t     e   i  u   x  *       ]    =     E   [    w  t    e   i  u   x   1  t       ]     E   [   e   i  u   x   1  t      ]     exp    ‚à´  0  u    i    E   [    x   2  t     e   i  v   x   1  t       ]     E   [   e   i  v   x   1  t      ]     d  v              subscript  œà  w   u    normal-E     subscript  w  t    superscript  e    i  u   superscript  x                  normal-E     subscript  w  t    superscript  e    i  u   subscript  x    1  t         normal-E   superscript  e    i  u   subscript  x    1  t           superscript   subscript   0   u     i     normal-E     subscript  x    2  t     superscript  e    i  v   subscript  x    1  t         normal-E   superscript  e    i  v   subscript  x    1  t        d  v        \psi_{w}(u)=\operatorname{E}[\,w_{t}e^{iux^{*}}\,]=\frac{\operatorname{E}[w_{t%
 }e^{iux_{1t}}]}{\operatorname{E}[e^{iux_{1t}}]}\exp\int_{0}^{u}i\frac{%
 \operatorname{E}[x_{2t}e^{ivx_{1t}}]}{\operatorname{E}[e^{ivx_{1t}}]}dv   The resulting estimator    Œ≤  ^     normal-^  Œ≤    \scriptstyle\hat{\beta}   is consistent and asymptotically normal.  |3= Schennach's estimator  14 for a nonparametric model. The standard Nadaraya‚ÄìWatson estimator for a nonparametric model takes form         g  ^    (  x  )    =     E  ^    [    y  t    K  h    (    x  t  *   -  x   )    ]      E  ^    [    K  h    (    x  t  *   -  x   )    ]      ,         normal-^  g   x        normal-^  normal-E    delimited-[]     subscript  y  t    subscript  K  h      subscript   superscript  x    t   x         normal-^  normal-E    delimited-[]     subscript  K  h      subscript   superscript  x    t   x         \hat{g}(x)=\frac{\hat{\operatorname{E}}[\,y_{t}K_{h}(x^{*}_{t}-x)\,]}{\hat{%
 \operatorname{E}}[\,K_{h}(x^{*}_{t}-x)\,]},   for a suitable choice of the kernel K and the bandwidth h . Both expectations here can be estimated using the same technique as in the previous method. }}  Notes  References           Jung, Kang-Mo (2007) "Least Trimmed Squares Estimator in the Errors-in-Variables Model", Journal of Applied Statistics , 34 (3), 331‚Äì338.             Further reading   An Historical Overview of Linear Regression with Errors in both Variables , J.W. Gillard 2006   A. R. Amiri-Simkooei and S. Jazaeri Weighted total least squares formulated by standard least squares theory ,in Journal of Geodetic Science, 2 (2): 113-124, 2012 1 .      "  Category:Regression analysis  Category:Statistical models  Category:Econometrics     ‚Ü©  ‚Ü©  , ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  . A somewhat more restrictive result was established earlier by R. C. Geary in "Inherent relations between random variables", Proceedings of Royal Irish Academy , vol.47 (1950). He showed that under the additional assumption that ( Œµ, Œ∑ ) are jointly normal, the model is not identified if and only if x* s are normal. ‚Ü©  ‚Ü©  . An earlier proof by Y. Willassen in "Extension of some results by Reiers√∏l to multivariate models", Scand. J. Statistics , 6 (2) (1979) contained errors. ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©     