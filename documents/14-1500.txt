   Tschuprow's T      Tschuprow's T   {| class="wikitable" align="right" valign |- |    T  =     ϕ  2      (   r  -  1   )    (   c  -  1   )           T       superscript  ϕ  2         r  1     c  1         T=\sqrt{\frac{\phi^{2}}{\sqrt{(r-1)(c-1)}}}   |- ! Tschuprow's T |}  In statistics , Tschuprow's T is a measure of association between two nominal variables , giving a value between 0 and 1 (inclusive). It is closely related to Cramér's V , coinciding with it for square contingency tables . It was published by Alexander Tschuprow (alternative spelling: Chuprov) in 1939. 1  Definition  For an r × c contingency table with r rows and c columns, let    π   i  j      subscript  π    i  j     \pi_{ij}   be the proportion of the population in cell    (  i  ,  j  )     i  j    (i,j)   and let       π   i  +    =    ∑   j  =  1   c    π   i  j          subscript  π   limit-from  i       superscript   subscript     j  1    c    subscript  π    i  j       \pi_{i+}=\sum_{j=1}^{c}\pi_{ij}   and      π   +  j    =    ∑   i  =  1   r    π   i  j      .       subscript  π    j      superscript   subscript     i  1    r    subscript  π    i  j       \pi_{+j}=\sum_{i=1}^{r}\pi_{ij}.     Then the mean square contingency is given as        ϕ  2   =    ∑   i  =  1   r     ∑   j  =  1   c      (    π   i  j    -    π   i  +     π   +  j      )   2     π   i  +     π   +  j         ,       superscript  ϕ  2     superscript   subscript     i  1    r     superscript   subscript     j  1    c      superscript     subscript  π    i  j       subscript  π   limit-from  i      subscript  π    j      2      subscript  π   limit-from  i      subscript  π    j          \phi^{2}=\sum_{i=1}^{r}\sum_{j=1}^{c}\frac{(\pi_{ij}-\pi_{i+}\pi_{+j})^{2}}{%
 \pi_{i+}\pi_{+j}},     and Tschuprow's T as       T  =     ϕ  2      (   r  -  1   )    (   c  -  1   )        .      T       superscript  ϕ  2         r  1     c  1         T=\sqrt{\frac{\phi^{2}}{\sqrt{(r-1)(c-1)}}}.     Properties  T equals zero if and only if independence holds in the table, i.e., if and only if     π   i  j    =    π   i  +     π   +  j          subscript  π    i  j       subscript  π   limit-from  i      subscript  π    j       \pi_{ij}=\pi_{i+}\pi_{+j}   . T equals one if and only there is perfect dependence in the table, i.e., if and only if for each i there is only one j such that     π   i  j    >  0       subscript  π    i  j    0    \pi_{ij}>0   and vice versa. Hence, it can only equal 1 for square tables. In this it differs from Cramér's V , which can be equal to 1 for any rectangular table.  Estimation  If we have a multinomial sample of size n , the usual way to estimate T from the data is via the formula        T  ^   =      ∑   i  =  1   r     ∑   j  =  1   c      (    p   i  j    -    p   i  +     p   +  j      )   2     p   i  +     p   +  j           (   r  -  1   )    (   c  -  1   )        ,       normal-^  T         superscript   subscript     i  1    r     superscript   subscript     j  1    c      superscript     subscript  p    i  j       subscript  p   limit-from  i      subscript  p    j      2      subscript  p   limit-from  i      subscript  p    j              r  1     c  1         \hat{T}=\sqrt{\frac{\sum_{i=1}^{r}\sum_{j=1}^{c}\frac{(p_{ij}-p_{i+}p_{+j})^{2%
 }}{p_{i+}p_{+j}}}{\sqrt{(r-1)(c-1)}}},     where     p   i  j    =    n   i  j    /  n        subscript  p    i  j       subscript  n    i  j    n     p_{ij}=n_{ij}/n   is the proportion of the sample in cell    (  i  ,  j  )     i  j    (i,j)   . This is the empirical value of T . With    χ  2     superscript  χ  2    \chi^{2}   the Pearson chi-square statistic , this formula can also be written as        T  ^   =      χ  2   /  n      (   r  -  1   )    (   c  -  1   )        .       normal-^  T          superscript  χ  2   n         r  1     c  1         \hat{T}=\sqrt{\frac{\chi^{2}/n}{\sqrt{(r-1)(c-1)}}}.     See also  Other measures of correlation for nominal data:   Cramér's V  Phi coefficient  Uncertainty coefficient  Lambda coefficient   Other related articles:   Effect size   References   Liebetrau, A. (1983). Measures of Association (Quantitative Applications in the Social Sciences). Sage Publications   "  Category:Articles created via the Article Wizard  Category:Summary statistics for contingency tables  Category:Statistical dependence     Tschuprow, A. A. (1939) Principles of the Mathematical Theory of Correlation ; translated by M. Kantorowitsch. W. Hodge & Co. ↩     