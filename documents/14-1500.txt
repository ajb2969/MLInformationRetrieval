


Tschuprow's T




Tschuprow's T

{| class="wikitable" align="right" valign |- |  
 
 
 
   |- ! Tschuprow's T |}
In statistics, Tschuprow's T is a measure of association between two nominal variables, giving a value between 0 and 1 (inclusive). It is closely related to Cramér's V, coinciding with it for square contingency tables. It was published by Alexander Tschuprow (alternative spelling: Chuprov) in 1939.1
Definition
For an r × c contingency table with r rows and c columns, let 
 
 
 
  be the proportion of the population in cell 
 
 
 
  and let


 
  and 
 
 

Then the mean square contingency is given as



and Tschuprow's T as



Properties
T equals zero if and only if independence holds in the table, i.e., if and only if 
 
 
 
 . T equals one if and only there is perfect dependence in the table, i.e., if and only if for each i there is only one j such that 
 
 
 
  and vice versa. Hence, it can only equal 1 for square tables. In this it differs from Cramér's V, which can be equal to 1 for any rectangular table.
Estimation
If we have a multinomial sample of size n, the usual way to estimate T from the data is via the formula



where 
 
 
 
  is the proportion of the sample in cell 
 
 
 
 . This is the empirical value of T. With 
 
 
 
  the Pearson chi-square statistic, this formula can also be written as



See also
Other measures of correlation for nominal data:

Cramér's V
Phi coefficient
Uncertainty coefficient
Lambda coefficient

Other related articles:

Effect size

References

Liebetrau, A. (1983). Measures of Association (Quantitative Applications in the Social Sciences). Sage Publications

"
Category:Articles created via the Article Wizard Category:Summary statistics for contingency tables Category:Statistical dependence



Tschuprow, A. A. (1939) Principles of the Mathematical Theory of Correlation; translated by M. Kantorowitsch. W. Hodge & Co.↩




