   D'Agostino's K-squared test      D'Agostino's K-squared test   In statistics , D’Agostino’s K 2 test , named for Ralph D'Agostino , is a goodness-of-fit measure of departure from normality , that is the test aims to establish whether or not the given sample comes from a normally distributed population. The test is based on transformations of the sample kurtosis and skewness , and has power only against the alternatives that the distribution is skewed and/or kurtic.  Skewness and kurtosis  In the following, let { x i } denote a sample of n observations, g 1 and g 2 are the sample skewness and kurtosis , m j ’s are the j -th sample central moments , and    x  ¯     normal-¯  x    \bar{x}   is the sample mean . (Note that quite frequently in the literature related to normality testing the skewness and kurtosis are denoted as √ β 1 and β 2 respectively. Such notation is less convenient since for example √ β 1 can be a negative quantity).  The sample skewness and kurtosis are defined as        g  1   =     m  3    m  2   3  /  2      =       1  n     ∑   i  =  1   n     (    x  i   -   x  ¯    )   3       (    1  n     ∑   i  =  1   n     (    x  i   -   x  ¯    )   2     )    3  /  2        ,         subscript  g  1      subscript  m  3    superscript   subscript  m  2     3  2                1  n     superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¯  x    3      superscript      1  n     superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¯  x    2       3  2        \displaystyle g_{1}=\frac{m_{3}}{m_{2}^{3/2}}=\frac{\frac{1}{n}\sum_{i=1}^{n}%
 \left(x_{i}-\bar{x}\right)^{3}}{\left(\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-%
 \bar{x}\right)^{2}\right)^{3/2}}\ ,     These quantities consistently estimate the theoretical skewness and kurtosis of the distribution, respectively. Moreover, if the sample indeed comes from a normal population, then the exact finite sample distributions of the skewness and kurtosis can themselves be analysed in terms of their means μ 1 , variances μ 2 , skewnesses γ 1 , and kurtoses γ 2 . This has been done by , who derived the following expressions:         μ  1    (   g  1   )    =  0   ,         subscript  μ  1    subscript  g  1    0    \displaystyle\mu_{1}(g_{1})=0,   and         μ  1    (   g  2   )    =   -    6   n  +  1       ,         subscript  μ  1    subscript  g  2        6    n  1       \displaystyle\mu_{1}(g_{2})=-\frac{6}{n+1},   For example, a sample with size  drawn from a normally distributed population can be expected to have a skewness of  and a kurtosis of , where SD indicates the standard deviation.  Transformed sample skewness and kurtosis  The sample skewness g 1 and kurtosis g 2 are both asymptotically normal. However, the rate of their convergence to the distribution limit is frustratingly slow, especially for g 2 . For example even with  observations the sample kurtosis g 2 has both the skewness and the kurtosis of approximately 0.3, which is not negligible. In order to remedy this situation, it has been suggested to transform the quantities g 1 and g 2 in a way that makes their distribution as close to standard normal as possible.  In particular,  suggested the following transformation for sample skewness:         Z  1    (   g  1   )    =   δ  ⋅    ln    (     g  1    α    μ  2      +      g  1  2     α  2    μ  2     +  1     )      ,         subscript  Z  1    subscript  g  1     normal-⋅  δ         subscript  g  1     α     subscript  μ  2             superscript   subscript  g  1   2      superscript  α  2    subscript  μ  2     1         Z_{1}(g_{1})=\delta\cdot\ln\!\left(\frac{g_{1}}{\alpha\sqrt{\mu_{2}}}+\sqrt{%
 \frac{g_{1}^{2}}{\alpha^{2}\mu_{2}}+1}\right),   where constants α and δ are computed as        W  2   =      2   γ  2    +  4    -  1    ,       superscript  W  2           2   subscript  γ  2    4    1     \displaystyle W^{2}=\sqrt{2\gamma_{2}+4}-1,   and where μ 2 = μ 2 ( g 1 ) is the variance of g 1 , and γ 2 = γ 2 ( g 1 ) is the kurtosis — the expressions given in the previous section.  Similarly,  suggested a transformation for g 2 , which works reasonably well for sample sizes of 20 or greater:         Z  2    (   g  2   )    =      9  A   2     {   1  -   2   9  A    -    (    1  -   2  /  A     1  +      g  2   -   μ  1      μ  2       2  /   (   A  -  4   )        )     1   /  3     }     ,         subscript  Z  2    subscript  g  2            9  A   2       1    2    9  A     superscript      1    2  A      1         subscript  g  2    subscript  μ  1       subscript  μ  2         2    A  4          1  3         Z_{2}(g_{2})=\sqrt{\frac{9A}{2}}\left\{1-\frac{2}{9A}-\left(\frac{1-2/A}{1+%
 \frac{g_{2}-\mu_{1}}{\sqrt{\mu_{2}}}\sqrt{2/(A-4)}}\right)^{\!1/3}\right\},   where       A  =   6  +    8   γ  1     (    2   γ  1    +    1  +   4  /   γ  1  2       )      ,      A    6      8   subscript  γ  1        2   subscript  γ  1        1    4   superscript   subscript  γ  1   2           A=6+\frac{8}{\gamma_{1}}\left(\frac{2}{\gamma_{1}}+\sqrt{1+4/\gamma_{1}^{2}}%
 \right),   and μ 1 = μ 1 ( g 2 ), μ 2 = μ 2 ( g 2 ), γ 1 = γ 1 ( g 2 ) are the quantities computed by Pearson.  Omnibus K 2 statistic  Statistics Z 1 and Z 2 can be combined to produce an omnibus test, able to detect deviations from normality due to either skewness or kurtosis :       K  2   =     Z  1     (   g  1   )   2    +    Z  2      (   g  2   )   2           superscript  K  2        subscript  Z  1    superscript   subscript  g  1   2       subscript  Z  2    superscript   subscript  g  2   2       K^{2}=Z_{1}(g_{1})^{2}+Z_{2}(g_{2})^{2}\,     If the null hypothesis of normality is true, then K 2 is approximately χ 2 -distributed with 2 degrees of freedom.  Note that the statistics g 1 , g 2 are not independent, only uncorrelated. Therefore, their transforms Z 1 , Z 2 will be dependent also , rendering the validity of χ 2 approximation questionable. Simulations show that under the null hypothesis the K 2 test statistic is characterized by       expected value   standard deviation   95% quantile       n = 20   1.971   2.339   6.373     n = 50   2.017   2.308   6.339     n = 100   2.026   2.267   6.271     n = 250   2.012   2.174   6.129     n = 500   2.009   2.113   6.063     n = 1000   2.000   2.062   6.038     χ 2 (2) distribution   2.000   2.000   5.991     References         "  Category:Parametric statistics  Category:Normality tests   