<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1684">Entropy power inequality</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Entropy power inequality</h1>
<hr/>

<p>In <a class="uri" href="mathematics" title="wikilink">mathematics</a>, the <strong>entropy power inequality</strong> is a result in <a href="information_theory" title="wikilink">information theory</a> that relates to so-called "entropy power" of <a href="random_variable" title="wikilink">random variables</a>. It shows that the entropy power of suitably <a class="uri" href="well-behaved" title="wikilink">well-behaved</a> random variables is a <a class="uri" href="superadditive" title="wikilink">superadditive</a> <a href="function_(mathematics)" title="wikilink">function</a>. The entropy power inequality was proved in 1948 by <a href="Claude_Shannon" title="wikilink">Claude Shannon</a> in his seminal paper "<a href="A_Mathematical_Theory_of_Communication" title="wikilink">A Mathematical Theory of Communication</a>". Shannon also provided a sufficient condition for equality to hold; Stam (1959) showed that the condition is in fact necessary.</p>
<h2 id="statement-of-the-inequality">Statement of the inequality</h2>

<p>For a random variable <em>X</em> : Ω → <strong>R</strong><sup><em>n</em></sup> with <a href="probability_density_function" title="wikilink">probability density function</a> <em>f</em> : <strong>R</strong><sup><em>n</em></sup> → <strong>R</strong>, the <a href="differential_entropy" title="wikilink">differential entropy</a> of <em>X</em>, denoted <em>h</em>(<em>X</em>), is defined to be</p>

<p>

<math display="block" id="Entropy_power_inequality:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>h</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>X</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <msub>
      <mo largeop="true" symmetric="true">∫</mo>
      <msup>
       <mi>ℝ</mi>
       <mi>n</mi>
      </msup>
     </msub>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
      <mrow>
       <mi>log</mi>
       <mi>f</mi>
      </mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo rspace="4.2pt" stretchy="false">)</mo>
      </mrow>
      <mi>d</mi>
      <mi>x</mi>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>h</ci>
     <ci>X</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <int></int>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>ℝ</ci>
        <ci>n</ci>
       </apply>
      </apply>
      <apply>
       <times></times>
       <ci>f</ci>
       <ci>x</ci>
       <apply>
        <log></log>
        <ci>f</ci>
       </apply>
       <ci>x</ci>
       <ci>d</ci>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h(X)=-\int_{\mathbb{R}^{n}}f(x)\log f(x)\,dx
  </annotation>
 </semantics>
</math>

</p>

<p>and the entropy power of <em>X</em>, denoted <em>N</em>(<em>X</em>), is defined to be</p>

<p>

<math display="block" id="Entropy_power_inequality:1">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>N</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>X</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mfrac>
      <mn>1</mn>
      <mrow>
       <mn>2</mn>
       <mi>π</mi>
       <mi>e</mi>
      </mrow>
     </mfrac>
     <msup>
      <mi>e</mi>
      <mrow>
       <mfrac>
        <mn>2</mn>
        <mi>n</mi>
       </mfrac>
       <mi>h</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </msup>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>N</ci>
     <ci>X</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <apply>
       <times></times>
       <cn type="integer">2</cn>
       <ci>π</ci>
       <ci>e</ci>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>e</ci>
      <apply>
       <times></times>
       <apply>
        <divide></divide>
        <cn type="integer">2</cn>
        <ci>n</ci>
       </apply>
       <ci>h</ci>
       <ci>X</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N(X)=\frac{1}{2\pi e}e^{\frac{2}{n}h(X)}.
  </annotation>
 </semantics>
</math>

</p>

<p>In particular, <em>N</em>(<em>X</em>) = |<em>K</em>| <sup>1/<em>n</em></sup> when <em>X</em> is normal distributed with covariance matrix <em>K</em>.</p>

<p>Let <em>X</em> and <em>Y</em> be <a href="independent_random_variables" title="wikilink">independent random variables</a> with probability density functions in the <a href="Lp_space" title="wikilink"><em>L</em><sup><em>p</em></sup> space</a> <em>L</em><sup><em>p</em></sup>(<strong>R</strong><sup><em>n</em></sup>) for some <em>p</em> &gt; 1. Then</p>

<p>

<math display="block" id="Entropy_power_inequality:2">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>N</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>X</mi>
       <mo>+</mo>
       <mi>Y</mi>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>≥</mo>
    <mrow>
     <mrow>
      <mi>N</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>X</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>+</mo>
     <mrow>
      <mi>N</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>Y</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <geq></geq>
    <apply>
     <times></times>
     <ci>N</ci>
     <apply>
      <plus></plus>
      <ci>X</ci>
      <ci>Y</ci>
     </apply>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <ci>N</ci>
      <ci>X</ci>
     </apply>
     <apply>
      <times></times>
      <ci>N</ci>
      <ci>Y</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N(X+Y)\geq N(X)+N(Y).\,
  </annotation>
 </semantics>
</math>

</p>

<p>Moreover, equality holds <a href="if_and_only_if" title="wikilink">if and only if</a> <em>X</em> and <em>Y</em> are <a href="multivariate_normal" title="wikilink">multivariate normal</a> random variables with proportional <a href="covariance_matrix" title="wikilink">covariance matrices</a>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Information_entropy" title="wikilink">Information entropy</a></li>
<li><a href="Information_theory" title="wikilink">Information theory</a></li>
<li><a href="Limiting_density_of_discrete_points" title="wikilink">Limiting density of discrete points</a></li>
<li><a class="uri" href="Self-information" title="wikilink">Self-information</a></li>
<li><a href="Kullback–Leibler_divergence" title="wikilink">Kullback–Leibler divergence</a></li>
<li><a href="Entropy_estimation" title="wikilink">Entropy estimation</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Information_theory" title="wikilink">Category:Information theory</a> <a href="Category:Probabilistic_inequalities" title="wikilink">Category:Probabilistic inequalities</a> <a href="Category:Statistical_inequalities" title="wikilink">Category:Statistical inequalities</a></p>
</body>
</html>
