<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1336">Shannon–Weaver model</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Shannon–Weaver model</h1>
<hr/>
<figure><b>(Figure)</b>
<figcaption>The Shannon–Weaver model as portrayed in a report from the <a href="United_States" title="wikilink">United States</a> <a href="Office_of_Technology_Assessment" title="wikilink">Office of Technology Assessment</a></figcaption>
</figure>

<p>The <strong>Shannon–Weaver model of communication</strong> has been called the "mother of all models."<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> It embodies the concepts of <a href="Information_theory#Source_theory" title="wikilink">information source</a>, <a class="uri" href="message" title="wikilink">message</a>, <a href="transmission_(telecommunications)" title="wikilink">transmitter</a>, <a href="signal_(electrical_engineering)" title="wikilink">signal</a>, <a href="channel_(communications)" title="wikilink">channel</a>, <a href="signal_noise" title="wikilink">noise</a>, <a href="receiver_(information_theory)" title="wikilink">receiver</a>, information destination, probability of error, <a class="uri" href="encoding" title="wikilink">encoding</a>, <a href="Code" title="wikilink">decoding</a>, <a href="Information_theory#Rate" title="wikilink">information rate</a>, <a href="channel_capacity" title="wikilink">channel capacity</a>, etc.</p>

<p>In 1948 <a href="Claude_Elwood_Shannon" title="wikilink">Claude Elwood Shannon</a> published <em><a href="A_Mathematical_Theory_of_Communication" title="wikilink">A Mathematical Theory of Communication</a></em> article in two parts in the July and October numbers of the <em><a href="Bell_System_Technical_Journal" title="wikilink">Bell System Technical Journal</a></em>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> In this fundamental work he used tools in probability theory, developed by <a href="Norbert_Wiener" title="wikilink">Norbert Wiener</a>, which were in their nascent stages of being applied to communication theory at that time. Shannon developed <a href="information_entropy" title="wikilink">information entropy</a> as a measure for the uncertainty in a message while essentially inventing what became known as the dominant form of "information theory."</p>

<p>The book co-authored with <a href="Warren_Weaver" title="wikilink">Warren Weaver</a>, <em>The Mathematical Theory of Communication</em>, reprints Shannon's 1948 article and Weaver's popularization of it, which is accessible to the non-specialist.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> Shannon's concepts were also popularized, subject to his own proofreading, in <a href="John_Robinson_Pierce" title="wikilink">John Robinson Pierce</a>'s <em>Symbols, Signals, and Noise</em>.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>The term <em>Shannon–Weaver model</em> was widely adopted into <a href="social_science" title="wikilink">social science</a> fields such as education, organizational analysis, psychology, etc., however some critics have labeled it a "misleading misrepresentation of the nature of human communication", citing its simplicity and inability to consider context.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> In <a class="uri" href="engineering" title="wikilink">engineering</a> and <a class="uri" href="mathematics" title="wikilink">mathematics</a>, Shannon's theory is used more literally and is referred to as <em>Shannon theory</em>, or <a href="information_theory" title="wikilink">information theory</a>.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>

<p>Shannon's capacity formula applied to the <a href="additive_white_Gaussian_noise" title="wikilink">additive white Gaussian noise</a> channel gives the <a href="Shannon–Hartley_theorem" title="wikilink">Shannon–Hartley formula</a>,</p>

<p>

<math display="block" id="Shannon–Weaver_model:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>C</mi>
    <mo>=</mo>
    <mrow>
     <mi>W</mi>
     <mrow>
      <msub>
       <mi>log</mi>
       <mn>2</mn>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mn>1</mn>
        <mo>+</mo>
        <mstyle displaystyle="false">
         <mfrac>
          <mi>S</mi>
          <mi>N</mi>
         </mfrac>
        </mstyle>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>C</ci>
    <apply>
     <times></times>
     <ci>W</ci>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <log></log>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <plus></plus>
       <cn type="integer">1</cn>
       <apply>
        <divide></divide>
        <ci>S</ci>
        <ci>N</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   C=W\log_{2}(1+\tfrac{S}{N}),
  </annotation>
 </semantics>
</math>

</p>

<p>where C is <a href="channel_capacity" title="wikilink">channel capacity</a> measured in bits/second, W is the bandwidth in Hz, S is the signal level in watts across the bandwidth W, and N is the noise power in watts in the bandwidth W.</p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Information_theory" title="wikilink">Category:Information theory</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"> (<a href="http://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-3-379.pdf">July</a>, <a href="http://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-4-623.pdf">October</a>)<a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
</ol>
</section>
</body>
</html>
