<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="221">Matrix exponential</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Matrix exponential</h1>
<hr/>
<p>In <a class="uri" href="mathematics" title="wikilink">mathematics</a>, the <strong>matrix exponential</strong> is a <a href="matrix_function" title="wikilink">matrix function</a> on <a href="square_matrix" title="wikilink">square matrices</a> analogous to the ordinary <a href="exponential_function" title="wikilink">exponential function</a>. Abstractly, the matrix exponential gives the connection between a matrix <a href="Lie_algebra" title="wikilink">Lie algebra</a> and the corresponding <a href="Lie_group" title="wikilink">Lie group</a>.</p>
<p>Let <span class="LaTeX">$X$</span> be an <span class="LaTeX">$n × n$</span> <a href="real_number" title="wikilink">real</a> or <a href="complex_number" title="wikilink">complex</a> <a href="matrix_(mathematics)" title="wikilink">matrix</a>. The exponential of <span class="LaTeX">$X$</span>, denoted by <mtpl></mtpl> or <span class="LaTeX">$exp( X )$</span>, is the <span class="LaTeX">$n × n$</span> matrix given by the <a href="power_series" title="wikilink">power series</a></p>
<p><span class="LaTeX">$$e^X = \sum_{k=0}^\infty{1 \over k!}X^k.$$</span></p>
<p>The above series always converges, so the exponential of <span class="LaTeX">$X$</span> is well-defined. If <span class="LaTeX">$X$</span> is a 1×1 matrix the matrix exponential of <span class="LaTeX">$X$</span> is a 1×1 matrix whose single element is the ordinary <a href="Exponential_function" title="wikilink">exponential</a> of the single element of <span class="LaTeX">$X$</span>.</p>
<h2 id="properties">Properties</h2>
<p>Let <span class="LaTeX">$X$</span> and <span class="LaTeX">$Y$</span> be <span class="LaTeX">$n × n$</span> complex matrices and let <span class="LaTeX">$a$</span> and <span class="LaTeX">$b$</span> be arbitrary complex numbers. We denote the <span class="LaTeX">$n × n$</span> <a href="identity_matrix" title="wikilink">identity matrix</a> by <span class="LaTeX">$I$</span> and the <a href="zero_matrix" title="wikilink">zero matrix</a> by 0. The matrix exponential satisfies the following properties:</p>
<ul>
<li><mtpl> <em>I</em>}}</mtpl></li>
<li><mtpl> <em>e</em><sup>(<em>a</em> + <em>b</em>)<em>X</em></sup>}}</mtpl></li>
<li><mtpl> <em>I</em>}}</mtpl></li>
<li>If <span class="LaTeX">$XY = YX$</span> then <mtpl> <em>e</em><sup><em>Y</em></sup><em>e</em><sup><em>X</em></sup> {{=}} <em>e</em><sup>(<em>X</em> + <em>Y</em>)</sup>.}}</mtpl></li>
<li>If <span class="LaTeX">$Y$</span> is <a href="invertible_matrix" title="wikilink">invertible</a> then <mtpl> <em>Ye</em><sup><em>X</em></sup><em>Y</em><sup>−1</sup>.}}</mtpl></li>
<li><mtpl> (exp <em>X</em>)<sup>T</sup>}}</mtpl>, where <mtpl></mtpl> denotes the <a class="uri" href="transpose" title="wikilink">transpose</a> of <span class="LaTeX">$X$</span>. It follows that if <span class="LaTeX">$X$</span> is <a href="symmetric_matrix" title="wikilink">symmetric</a> then <mtpl></mtpl> is also symmetric, and that if <span class="LaTeX">$X$</span> is <a href="skew-symmetric_matrix" title="wikilink">skew-symmetric</a> then <mtpl></mtpl> is <a href="orthogonal_matrix" title="wikilink">orthogonal</a>.</li>
<li><mtpl> (exp <em>X</em>)<sup>∗</sup>}}</mtpl>, where <mtpl></mtpl> denotes the <a href="conjugate_transpose" title="wikilink">conjugate transpose</a> of <span class="LaTeX">$X$</span>. It follows that if <span class="LaTeX">$X$</span> is <a href="Hermitian_matrix" title="wikilink">Hermitian</a> then <mtpl></mtpl> is also Hermitian, and that if <span class="LaTeX">$X$</span> is <a href="skew-Hermitian_matrix" title="wikilink">skew-Hermitian</a> then <mtpl></mtpl> is <a href="unitary_matrix" title="wikilink">unitary</a>.</li>
<li>A <a href="Laplace_transform" title="wikilink">Laplace transform</a> of matrix exponentials amounts to the <a href="resolvent_formalism" title="wikilink">resolvent</a>, <mtpl> <em>I</em> / (<em>I−X</em>)}}</mtpl>.</li>
</ul>
<h3 id="linear-differential-equation-systems">Linear differential equation systems</h3>
<p>One of the reasons for the importance of the matrix exponential is that it can be used to solve systems of linear <a href="ordinary_differential_equations" title="wikilink">ordinary differential equations</a>. The solution of</p>
<p><span class="LaTeX">$$\frac{d}{dt} y(t) = Ay(t), \quad y(0) = y_0,$$</span> where <span class="LaTeX">$A$</span> is a constant matrix, is given by</p>
<p><span class="LaTeX">$$y(t) = e^{At} y_0. \,$$</span> The matrix exponential can also be used to solve the inhomogeneous equation</p>
<p><span class="LaTeX">$$\frac{d}{dt} y(t) = Ay(t) + z(t), \quad y(0) = y_0.$$</span> See the section on <a href="#Applications" title="wikilink">applications</a> below for examples.</p>
<p>There is no closed-form solution for differential equations of the form</p>
<p><span class="LaTeX">$$\frac{d}{dt} y(t) = A(t) \, y(t), \quad y(0) = y_0,$$</span> where <span class="LaTeX">$A$</span> is not constant, but the <a href="Magnus_series" title="wikilink">Magnus series</a> gives the solution as an infinite sum.</p>
<h3 id="the-exponential-of-sums">The exponential of sums</h3>
<p>For any real numbers (scalars) <span class="LaTeX">$x$</span> and <span class="LaTeX">$y$</span> we know that the exponential function satisfies <mtpl> <em>e</em><sup><em>x</em></sup> <em>e</em><sup><em>y</em></sup>}}</mtpl>. The same is true for commuting matrices. If matrices <span class="LaTeX">$X$</span> and <span class="LaTeX">$Y$</span> commute (meaning that <span class="LaTeX">$XY = YX$</span>), then</p>
<p><span class="LaTeX">$$e^{X+Y} = e^Xe^Y ~.$$</span></p>
<p>However, for matrices that do not commute the above equality does not necessarily hold. In this case the <a href="Baker–Campbell–Hausdorff_formula" title="wikilink">Baker–Campbell–Hausdorff formula</a> can be used to calculate <mtpl></mtpl>.</p>
<p>The converse is not true in general. The equation <mtpl> <em>e</em><sup><em>X</em></sup> <em>e</em><sup><em>Y</em></sup>}}</mtpl> does not imply that <span class="LaTeX">$X$</span> and <span class="LaTeX">$Y$</span> commute.</p>
<p>For <a href="Hermitian_matrix" title="wikilink">Hermitian matrices</a> there are two notable theorems related to the <a href="Matrix_trace" title="wikilink">trace</a> of matrix exponentials.</p>
<h4 id="goldenthompson-inequality">Golden–Thompson inequality</h4>
<p>If <span class="LaTeX">$A$</span> and <span class="LaTeX">$H$</span> are Hermitian matrices, then</p>
<p><span class="LaTeX">$$\operatorname{tr}\exp(A+H) \leq \operatorname{tr}(\exp(A)\exp(H)).$$</span><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Note that there is no requirement of commutativity. There are counterexamples to show that the Golden–Thompson inequality cannot be extended to three matrices – and, in any event, <span class="LaTeX">$tr(exp( A )exp( B )exp( C ))$</span> is not guaranteed to be real for Hermitian <span class="LaTeX">$A$</span>, <span class="LaTeX">$B$</span>, <span class="LaTeX">$C$</span>. However, the next theorem accomplishes this in one sense.</p>
<h4 id="liebs-theorem">Lieb's theorem</h4>
<p>The <strong>Lieb's theorem</strong>, named after <a href="Elliott_H._Lieb" title="wikilink">Elliott H. Lieb</a>, states that, for a fixed <a href="Hermitian_matrix" title="wikilink">Hermitian matrix</a> <span class="LaTeX">$H$</span>, the function</p>
<p><span class="LaTeX">$$f(A) = \operatorname{tr} \,\exp \left (H + \log A \right)$$</span> is <a href="Concave_function" title="wikilink">concave</a> on the <a href="Convex_cone" title="wikilink">cone</a> of <a href="positive-definite_matrix" title="wikilink">positive-definite matrices</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h3 id="the-exponential-map">The exponential map</h3>
<p>Note that the exponential of a matrix is always an <a href="invertible_matrix" title="wikilink">invertible matrix</a>. The inverse matrix of <mtpl></mtpl> is given by <mtpl></mtpl>. This is analogous to the fact that the exponential of a complex number is always nonzero. The matrix exponential then gives us a map</p>
<p><span class="LaTeX">$$\exp \colon M_n(\mathbb C) \to \mathrm{GL}(n,\mathbb C)$$</span> from the space of all <em>n</em>×<em>n</em> matrices to the <a href="general_linear_group" title="wikilink">general linear group</a> of degree <span class="LaTeX">$n$</span>, i.e. the <a href="group_(mathematics)" title="wikilink">group</a> of all <em>n</em>×<em>n</em> invertible matrices. In fact, this map is <a class="uri" href="surjective" title="wikilink">surjective</a> which means that every invertible matrix can be written as the exponential of some other matrix (for this, it is essential to consider the field <strong>C</strong> of complex numbers and not <strong>R</strong>).</p>
<p>For any two matrices <span class="LaTeX">$X$</span> and <span class="LaTeX">$Y$</span>,</p>
<p><span class="LaTeX">$$\| e^{X+Y} - e^X \| \le \|Y\| e^{\|X\|} e^{\|Y\|},$$</span></p>
<p>where || · || denotes an arbitrary <a href="matrix_norm" title="wikilink">matrix norm</a>. It follows that the exponential map is <a href="continuity_(mathematics)" title="wikilink">continuous</a> and <a href="Lipschitz_continuous" title="wikilink">Lipschitz continuous</a> on <a href="compact_set" title="wikilink">compact</a> subsets of <mtpl></mtpl>.</p>
<p>The map</p>
<p><span class="LaTeX">$$t \mapsto e^{tX}, \qquad t \in \mathbb R$$</span> defines a <a href="Smooth_function#Smoothness" title="wikilink">smooth</a> curve in the general linear group which passes through the identity element at <em>t</em> = 0.</p>
<p>In fact, this gives a <a href="one-parameter_subgroup" title="wikilink">one-parameter subgroup</a> of the general linear group since</p>
<p><span class="LaTeX">$$e^{tX}e^{sX} = e^{(t+s)X}.\,$$</span></p>
<p>The derivative of this curve (or <a href="tangent_vector" title="wikilink">tangent vector</a>) at a point <em>t</em> is given by</p>
<p><span class="LaTeX">$$\frac{d}{dt}e^{tX} = Xe^{tX} = e^{tX}X. \qquad (1)$$</span> The derivative at <em>t</em> = 0 is just the matrix <em>X</em>, which is to say that <em>X</em> generates this one-parameter subgroup.</p>
<p>More generally,<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> for a generic <span class="LaTeX">$t$</span>-dependent exponent, <span class="LaTeX">$X(t)$</span>, </p>
<p>Taking the above expression <mtpl></mtpl> outside the integral sign and expanding the integrand with the help of the <a href="Baker–Campbell–Hausdorff_formula" title="wikilink">Hadamard lemma</a> one can obtain the following useful expression for the derivative of the matrix exponent,</p>
<p><span class="LaTeX">$$\left(\frac{d}{dt}e^{X(t)}\right)e^{-X(t)} = \frac{d}{dt}X(t) + \frac{1}{2!}[X(t),\frac{d}{dt}X(t)] + \frac{1}{3!}[X(t),[X(t),\frac{d}{dt}X(t)]]+\cdots$$</span> Note that the coefficients in the expression above are different from what appears in the exponential. For a closed form, see <a href="derivative_of_the_exponential_map" title="wikilink">derivative of the exponential map</a>.</p>
<h3 id="the-determinant-of-the-matrix-exponential">The determinant of the matrix exponential</h3>
<p>By <a href="Jacobi's_formula" title="wikilink">Jacobi's formula</a>, for any complex square matrix the following <a href="trace_identity" title="wikilink">trace identity</a> holds: </p>
<p>In addition to providing a computational tool, this formula demonstrates that a matrix exponential is always an <a href="invertible_matrix" title="wikilink">invertible matrix</a>. This follows from the fact that the right hand side of the above equation is always non-zero, and so <mtpl></mtpl>, which implies that <mtpl></mtpl> must be invertible.</p>
<p>In the real-valued case, the formula also exhibits the map</p>
<p><span class="LaTeX">$$\exp \colon M_n(\mathbb R) \to \mathrm{GL}(n,\mathbb R)$$</span> to not be <a href="surjective_function" title="wikilink">surjective</a>, in contrast to the complex case mentioned earlier. This follows from the fact that, for real-valued matrices, the right-hand side of the formula is always positive, while there exist invertible matrices with a negative determinant.</p>
<h2 id="computing-the-matrix-exponential">Computing the matrix exponential</h2>
<p>Finding reliable and accurate methods to compute the matrix exponential is difficult, and this is still a topic of considerable current research in mathematics and numerical analysis. <a class="uri" href="Matlab" title="wikilink">Matlab</a>, <a href="GNU_Octave" title="wikilink">GNU Octave</a>, and <a class="uri" href="SciPy" title="wikilink">SciPy</a> all use the <a href="Padé_approximant" title="wikilink">Padé approximant</a>.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h3 id="diagonalizable-case">Diagonalizable case</h3>
<p>If a matrix is <a href="diagonal_matrix" title="wikilink">diagonal</a>:</p>
<p><span class="LaTeX">$$A=\begin{bmatrix} a_1 & 0 & \ldots & 0 \\
0 & a_2 & \ldots & 0  \\ \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & a_n \end{bmatrix}$$</span>,</p>
<p>then its exponential can be obtained by exponentiating each entry on the main diagonal:</p>
<p><span class="LaTeX">$$e^A=\begin{bmatrix} e^{a_1} & 0 & \ldots & 0 \\
0 & e^{a_2} & \ldots & 0  \\ \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & e^{a_n} \end{bmatrix}$$</span>.</p>
<p>This also allows one to exponentiate <a href="diagonalizable_matrix" title="wikilink">diagonalizable matrices</a>. If <mtpl></mtpl> and <span class="LaTeX">$D$</span> is diagonal, then <mtpl></mtpl>. Application of <a href="Sylvester's_formula" title="wikilink">Sylvester's formula</a> yields the same result. (To see this, note that addition and multiplication, hence also exponentiation, of diagonal matrices is equivalent to element-wise addition and multiplication, and hence exponentiation; in particular, the "one-dimensional" exponentiation is felt element-wise for the diagonal case.)</p>
<h3 id="projection-case">Projection case</h3>
<p>If <span class="LaTeX">$P$</span> is a <a href="projection_matrix" title="wikilink">projection matrix</a> (i.e. is <a class="uri" href="idempotent" title="wikilink">idempotent</a>), its matrix exponential is <mtpl></mtpl>. This may be derived by expansion of the definition of the exponential function and by use of the idempotency of <span class="LaTeX">$P$</span>:</p>
<p><span class="LaTeX">$$e^P = \sum_{k=0}^{\infty} \frac{P^k}{k!}=I+\left(\sum_{k=1}^{\infty} \frac{1}{k!}\right)P=I+(e-1)P ~.$$</span></p>
<h3 id="rotation-case">Rotation case</h3>
<p>For a simple rotation in which the perpendicular unit vectors <span class="LaTeX">$a$</span> and <span class="LaTeX">$b$</span> specify a plane,<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> the <a href="Rotation_matrix#Exponential_map" title="wikilink">rotation matrix</a> <span class="LaTeX">$R$</span> can be expressed in terms of a similar exponential function involving a <a href="Euler's_rotation_theorem#Generators_of_rotations" title="wikilink">generator</a> <span class="LaTeX">$G$</span> and angle <span class="LaTeX">$θ$</span>.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<p><span class="LaTeX">$$G=ba^\mathsf{T}-ab^\mathsf{T} \qquad a^\mathsf{T}b=0$$</span></p>
<p><span class="LaTeX">$$-G^2=aa^\mathsf{T}+bb^\mathsf{T}=P \qquad P^2=P \qquad PG=GP=G ~,$$</span></p>
<p><span class="LaTeX">$$\begin{align}
R\left( \theta  \right) &= {{e}^{G\theta }}=I+G\sin (\theta ) +{{G}^{2}}(1- \cos (\theta ) ) \\ 
 &=I-P+P\cos (\theta )+G\sin (\theta ) ~.\\ 
\end{align}$$</span></p>
<p>The formula for the exponential results from reducing the powers of <span class="LaTeX">$G$</span> in the series expansion and identifying the respective series coefficients of <mtpl></mtpl> and <span class="LaTeX">$G$</span> with <span class="LaTeX">$−cos( θ )$</span> and <span class="LaTeX">$sin( θ )$</span> respectively. The second expression here for <mtpl></mtpl> is the same as the expression for <span class="LaTeX">$R ( θ )$</span> in the article containing the derivation of the <a href="Euler's_rotation_theorem#Generators_of_rotations" title="wikilink">generator</a>, <mtpl> <em>e<sup>Gθ</sup></em>}}</mtpl>.</p>
<p>In two dimensions, if <span class="LaTeX">$a= \left ( \begin{smallmatrix}1\\0\end{smallmatrix} \right )$</span> and <span class="LaTeX">$b= \left ( \begin{smallmatrix}0\\1\end{smallmatrix} \right )$</span>, then <span class="LaTeX">$G= \left ( \begin{smallmatrix}0&-1\\1&0\end{smallmatrix} \right )$</span>, <span class="LaTeX">$G^2= \left ( \begin{smallmatrix}-1&0\\0&-1\end{smallmatrix} \right )$</span>, and</p>
<p><span class="LaTeX">$$R(\theta)= \left ( \begin{matrix}\cos(\theta)&-\sin(\theta)\\ \sin(\theta)&\cos(\theta)\end{matrix} \right )=I \cos(\theta) + G \sin(\theta)$$</span> reduces to the standard matrix for a plane rotation.</p>
<p>The matrix <mtpl></mtpl> <a href="Projection_(linear_algebra)" title="wikilink">projects</a> a vector onto the <span class="LaTeX">$ab$</span>-plane and the rotation only affects this part of the vector. An example illustrating this is a rotation of <span class="LaTeX">$30° = π/6$</span> in the plane spanned by <span class="LaTeX">$a$</span> and <span class="LaTeX">$b$</span>,</p>
<p><span class="LaTeX">$\begin{align}
  & a=\left( \begin{matrix}
   1  \\
   0  \\
   0  \\
\end{matrix} \right)\quad b=\frac{1}{\sqrt{5}}\left( \begin{matrix}
   0  \\
   1  \\
   2  \\
\end{matrix} \right) \\ 
 & I=\left( \begin{matrix}
   1 & 0 & 0  \\
   0 & 1 & 0  \\
   0 & 0 & 1  \\
\end{matrix} \right)\quad G=\frac{1}{\sqrt{5}}\left( \begin{matrix}
   0 & -1 & -2  \\
   1 & 0 & 0  \\
   2 & 0 & 0  \\
\end{matrix} \right) \\
 & P=-{{G}^{2}}=\frac{1}{5}\left( \begin{matrix}
   5 & 0 & 0  \\
   0 & 1 & 2  \\
   0 & 2 & 4  \\
\end{matrix} \right)\quad P\left( \begin{matrix}
   1  \\
   2  \\
   3  \\
\end{matrix} \right)=\frac{1}{5}\left( \begin{matrix}
   5  \\
   8  \\
   16  \\
\end{matrix} \right)=a+\frac{8}{\sqrt{5}}b \\
 & \theta =\frac{\pi}{6} \quad \Rightarrow \quad R=\frac{1}{10}\left( \begin{matrix}
   5\sqrt{3} & -\sqrt{5} & -2\sqrt{5}  \\
   \sqrt{5} & 8+\sqrt{3} & -4+2\sqrt{3}  \\
   2\sqrt{5} & -4+2\sqrt{3} & 2+4\sqrt{3}  \\
\end{matrix} \right) \\
\end{align}$</span></p>
<p>Let <span class="LaTeX">$ N = I − P$</span>, so <mtpl></mtpl> and its products with <span class="LaTeX">$P$</span> and <span class="LaTeX">$G$</span> are zero. This will allow us to evaluate powers of <span class="LaTeX">$R$</span>.</p>
<p><span class="LaTeX">$\begin{align} 
 & R\left( \frac{\pi }{6} \right)=N+P\frac{\sqrt{3}}{2}+G\frac{1}{2}\quad \quad R{{\left( \frac{\pi }{6} 

\right)}^{2}}=N+P\frac{1}{2}+G\frac{\sqrt{3}}{2} \\ 
 & R{{\left( \frac{\pi }{6} \right)}^{3}}=N+G\quad \quad R{{\left( \frac{\pi }{6} \right)}^{6}}=N-P\quad 

\quad R{{\left( \frac{\pi }{6} \right)}^{12}}=N+P=I \\ 
\end{align}$</span> </p>
<h3 id="nilpotent-case">Nilpotent case</h3>
<p>A matrix <em>N</em> is <a href="nilpotent_matrix" title="wikilink">nilpotent</a> if <em>N</em><sup><em>q</em></sup> = 0 for some integer <em>q</em>. In this case, the matrix exponential <em>e</em><sup><em>N</em></sup> can be computed directly from the series expansion, as the series terminates after a finite number of terms:</p>
<p><span class="LaTeX">$$e^N = I + N + \frac{1}{2}N^2 + \frac{1}{6}N^3 + \cdots + \frac{1}{(q-1)!}N^{q-1} ~.$$</span></p>
<h3 id="generalization">Generalization</h3>
<p>When the <a href="Minimal_polynomial_(linear_algebra)" title="wikilink">minimal polynomial</a> of a matrix <em>X</em> can be factored into a product of first degree polynomials, it can be expressed as a sum</p>
<p><span class="LaTeX">$$X = A + N \,$$</span> where</p>
<ul>
<li><em>A</em> is diagonalizable</li>
<li><em>N</em> is nilpotent</li>
<li><em>A</em> commutes with <em>N</em> (i.e. <em>AN</em> = <em>NA</em>)</li>
</ul>
<p>This is the <a href="Jordan–Chevalley_decomposition" title="wikilink">Jordan–Chevalley decomposition</a>.</p>
<p>This means that we can compute the exponential of <em>X</em> by reducing to the previous two cases:</p>
<p><span class="LaTeX">$$e^X = e^{A+N} = e^A e^N. \,$$</span> Note that we need the commutativity of <em>A</em> and <em>N</em> for the last step to work.</p>
<p>Another (closely related) method if the field is <a href="algebraically_closed" title="wikilink">algebraically closed</a> is to work with the <a href="Jordan_form" title="wikilink">Jordan form</a> of <em>X</em>. Suppose that <em>X</em> = <em>PJP</em><sup> −1</sup> where <em>J</em> is the Jordan form of <em>X</em>. Then</p>
<p><span class="LaTeX">$$e^{X}=Pe^{J}P^{-1}.\,$$</span></p>
<p>Also, since</p>
<p><span class="LaTeX">$$J=J_{a_1}(\lambda_1)\oplus J_{a_2}(\lambda_2)\oplus\cdots\oplus J_{a_n}(\lambda_n),$$</span></p>
<p><span class="LaTeX">$$\begin{align}
e^{J} & {} = \exp \big( J_{a_1}(\lambda_1)\oplus J_{a_2}(\lambda_2)\oplus\cdots\oplus J_{a_n}(\lambda_n) \big) \\
& {} = \exp \big( J_{a_1}(\lambda_1) \big) \oplus \exp \big( J_{a_2}(\lambda_2) \big) \oplus\cdots\oplus \exp \big( J_{a_k}(\lambda_k) \big).
\end{align}$$</span></p>
<p>Therefore, we need only know how to compute the matrix exponential of a Jordan block. But each Jordan block is of the form</p>
<p><span class="LaTeX">$$J_{a}(\lambda) = \lambda I + N \,$$</span> where <em>N</em> is a special nilpotent matrix. The matrix exponential of this block is given by</p>
<p><span class="LaTeX">$$e^{\lambda I + N} = e^{\lambda}e^N. \,$$</span></p>
<h3 id="evaluation-by-laurent-series">Evaluation by Laurent series</h3>
<p>By virtue of the <a href="Cayley–Hamilton_theorem" title="wikilink">Cayley–Hamilton theorem</a> the matrix exponential is expressible as a polynomial of order <span class="LaTeX">$n$</span>−1.</p>
<p>If <span class="LaTeX">$P$</span> and <mtpl></mtpl> are nonzero polynomials in one variable, such that <span class="LaTeX">$P ( A ) = 0$</span>, and if the <a href="meromorphic_function" title="wikilink">meromorphic function</a></p>
<p><span class="LaTeX">$$f(z)=\frac{e^{t z}-Q_t(z)}{P(z)}$$</span> is <a href="entire_function" title="wikilink">entire</a>, then</p>
<p><span class="LaTeX">$$e^{t A} = Q_t(A)$$</span>. To prove this, multiply the first of the two above equalities by <span class="LaTeX">$P ( z )$</span> and replace <span class="LaTeX">$z$</span> by <span class="LaTeX">$A$</span>.</p>
<p>Such a polynomial <mtpl></mtpl> can be found as follows−−see <a href="Sylvester's_formula" title="wikilink">Sylvester's formula</a>. Letting <span class="LaTeX">$a$</span> be a root of <span class="LaTeX">$P$</span>, <mtpl></mtpl> is solved from the product of <span class="LaTeX">$P$</span> by the <a href="Laurent_series#Principal_part" title="wikilink">principal part</a> of the <a href="Laurent_series" title="wikilink">Laurent series</a> of <span class="LaTeX">$f$</span> at <span class="LaTeX">$a$</span>: It is proportional to the relevant <a href="Frobenius_covariant" title="wikilink">Frobenius covariant</a>. Then the sum <em>S<sub>t</sub></em> of the <em>Q<sub>a,t</sub></em>, where <span class="LaTeX">$a$</span> runs over all the roots of <span class="LaTeX">$P$</span>, can be taken as a particular <mtpl></mtpl>. All the other <em>Q<sub>t</sub></em> will be obtained by adding a multiple of <span class="LaTeX">$P$</span> to <mtpl></mtpl>. In particular, <mtpl></mtpl>, the <a href="Sylvester's_formula" title="wikilink">Lagrange-Sylvester polynomial</a>, is the only <mtpl></mtpl> whose degree is less than that of <span class="LaTeX">$P$</span>.</p>
<p><strong>Example</strong>: Consider the case of an arbitrary 2-by-2 matrix,</p>
<p><span class="LaTeX">$$A:=\begin{bmatrix}
a & b \\
c & d \end{bmatrix}.$$</span></p>
<p>The exponential matrix <mtpl></mtpl>, by virtue of the <a href="Cayley–Hamilton_theorem" title="wikilink">Cayley–Hamilton theorem</a>, must be of the form</p>
<dl>
<dd><dl>
<dd><span class="LaTeX">$e^{tA}=s_0(t)\,I+s_1(t)\,A$</span>.
</dd>
</dl>
</dd>
</dl>
<p>(For any complex number <span class="LaTeX">$z$</span> and any <strong><em>C</em></strong>-algebra <span class="LaTeX">$B$</span>, we denote again by <span class="LaTeX">$z$</span> the product of <span class="LaTeX">$z$</span> by the unit of <span class="LaTeX">$B$</span>.)</p>
<p>Let <span class="LaTeX">$α$</span> and <span class="LaTeX">$β$</span> be the roots of the <a href="characteristic_polynomial" title="wikilink">characteristic polynomial</a> of <span class="LaTeX">$A$</span>,</p>
<p><span class="LaTeX">$$P(z)=z^2-(a+d)\ z+ ad-bc= (z-\alpha)(z-\beta) ~ .$$</span></p>
<p>Then we have</p>
<p><span class="LaTeX">$$S_t(z)= e^{\alpha t} \frac{z-\beta}{\alpha-\beta}   + e^{\beta t} \frac{z-\alpha}{\beta-\alpha}  ~,$$</span> and hence</p>
<p><span class="LaTeX">$$s_0(t)=\frac{\alpha\,e^{\beta t}
-\beta\,e^{\alpha t}}{\alpha-\beta},\quad
s_1(t)=\frac{e^{\alpha t}-e^{\beta t}}{\alpha-\beta}\quad$$</span> if <span class="LaTeX">$α ≠ β$</span>; while, if <span class="LaTeX">$α = β$</span>,</p>
<p><span class="LaTeX">$$S_t(z)= e^{\alpha t} ( 1+ t (z-\alpha  ))  ~,$$</span> so that</p>
<p><span class="LaTeX">$$s_0(t)=(1-\alpha\,t)\,e^{\alpha t},\quad
s_1(t)=t\,e^{\alpha t}~.$$</span></p>
<p>Defining</p>
<p><span class="LaTeX">$$s \equiv \frac{\alpha + \beta}{2}=\frac{\operatorname{tr} A}{2}~, \qquad \qquad  q\equiv \frac{\alpha-\beta}{2}=\pm\sqrt{-\det\left(A-s I\right)},$$</span> we have</p>
<p><span class="LaTeX">$$s_0(t) = e^{s t}\left(\cosh (q t) - s \frac{\sinh (q t)}{q}\right),\qquad s_1(t) =e^{s t}\frac{\sinh(q t)}{q},$$</span> where <span class="LaTeX">$sin( qt )/ q$</span> is 0 if <span class="LaTeX">$t$</span> = 0, and <span class="LaTeX">$t$</span> if <span class="LaTeX">$q$</span> = 0. Thus,  Thus, as indicated above, the matrix <span class="LaTeX">$A$</span> having decomposed into the sum of two mutually commuting pieces, the traceful piece and the traceless piece,</p>
<p><span class="LaTeX">$$A= sI + (A-sI)~,$$</span> the matrix exponential reduces to a plain product of the exponentials of the two respective pieces. This is a formula often used in physics, as it amounts to the analog of <a href="Euler's_formula" title="wikilink">Euler's formula</a> for <a href="Pauli_spin_matrices#Exponential_of_a_Pauli_vector" title="wikilink">Pauli spin matrices</a>, that is rotations of the doublet representation of the group <a class="uri" href="SU(2)" title="wikilink">SU(2)</a>.</p>
<p>The polynomial <mtpl></mtpl> can also be given the following "<a class="uri" href="interpolation" title="wikilink">interpolation</a>" characterization. Define <mtpl></mtpl>, and <span class="LaTeX">$n$</span> ≡ deg<span class="LaTeX">$P$</span>. Then <mtpl></mtpl> is the unique degree <span class="LaTeX">$ polynomial which satisfies <mtpl> <em>e<sub>t</sub><sup>(k)</sup>(a)</em>}}</mtpl> whenever <span class="LaTeX">$k$</span> is less than the multiplicity of <span class="LaTeX">$a$</span> as a root of <span class="LaTeX">$P$</span>. We assume, as we obviously can, that <span class="LaTeX">$P$</span> is the <a href="Minimal_polynomial_(linear_algebra)" title="wikilink">minimal polynomial</a> of <span class="LaTeX">$A$</span>. We further assume that <span class="LaTeX">$A$</span> is a <a href="diagonalizable_matrix" title="wikilink">diagonalizable matrix</a>. In particular, the roots of <span class="LaTeX">$P$</span> are simple, and the "<a class="uri" href="interpolation" title="wikilink">interpolation</a>" characterization indicates that <mtpl></mtpl> is given by the <a href="Lagrange_interpolation" title="wikilink">Lagrange interpolation</a> formula, so it is the <a href="Sylvester's_formula" title="wikilink">Lagrange−Sylvester polynomial</a> .</span></p>
<p>At the other extreme, if <mtpl> <em>(z−a)<sup>n</sup></em>}}</mtpl>, then</p>
<p><span class="LaTeX">$$S_t=e^{at}\ \sum_{k=0}^{n-1}\ \frac{t^k}{k!}\ (z-a)^k ~.$$</span> The simplest case not covered by the above observations is when <span class="LaTeX">$P=(z-a)^2\,(z-b)$</span> with <span class="LaTeX">$a ≠ b$</span>, which yields</p>
<p><span class="LaTeX">$$S_t=e^{at}\ \frac{z-b}{a-b}\ \Bigg(1+\left(t+\frac{1}{b-a}\right)(z-a)\Bigg)+e^{bt}\ \frac{(z-a)^2}{(b-a)^2}\quad.$$</span></p>
<h3 id="evaluation-by-implementation-of-sylvesters-formula">Evaluation by implementation of <a href="Sylvester's_formula" title="wikilink">Sylvester's formula</a></h3>
<p>A practical, expedited computation of the above reduces to the following rapid steps. Recall from above that an <em>n</em>-by-<em>n</em> matrix <span class="LaTeX">$exp( tA )$</span> amounts to a linear combination of the first <span class="LaTeX">$n$</span>−1 powers of <span class="LaTeX">$A$</span> by the <a href="Cayley-Hamilton_theorem" title="wikilink">Cayley-Hamilton theorem</a>. For <a href="diagonalizable_matrix" title="wikilink">diagonalizable</a> matrices, as illustrated above, e.g. in the 2 by 2 case, <a href="Sylvester's_formula" title="wikilink">Sylvester's formula</a> yields <mtpl> <em>B<sub>α</sub></em> exp(<em>tα</em>)+<em>B<sub>β</sub></em> exp(<em>tβ</em>)}}</mtpl>, where the <span class="LaTeX">$B$</span>s are the <a href="Frobenius_covariant" title="wikilink">Frobenius covariants</a> of <span class="LaTeX">$A$</span>.</p>
<p>It is easiest, however, to simply solve for these <span class="LaTeX">$B$</span>s directly, by evaluating this expression and its first derivative at <span class="LaTeX">$t$</span>=0, in terms of <span class="LaTeX">$A$</span> and <span class="LaTeX">$I$</span>, to find the same answer as above.</p>
<p>But this simple procedure also works for <a href="defective_matrix" title="wikilink">defective</a> matrices, in a generalization due to Buchheim.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> This is illustrated here for a 4-by-4 example of a matrix which is not diagonalizable, and the <span class="LaTeX">$B$</span>s are not projection matrices.</p>
<p>Consider</p>
<p><span class="LaTeX">$$A = 
\begin{pmatrix}
1 & 1 & 0 & 0 \\
0 & 1 & 1 & 0 \\
0 & 0 & 1 & -1/8 \\
0 & 0 & 1/2 & 1/2
\end{pmatrix}  ~,$$</span> with eigenvalues <mtpl>3/4}}</mtpl> and <mtpl>1}}</mtpl>, each with a multiplicity of two.</p>
<p>Consider the exponential of each eigenvalue multiplied by <span class="LaTeX">$t$</span>, <mtpl></mtpl>. Multiply each such by the corresponding undetermined coefficient matrix <mtpl></mtpl>. If the eigenvalues have an algebraic multiplicity greater than 1, then repeat the process, but now multiplying by an extra factor of <span class="LaTeX">$t$</span> for each repetition, to ensure linear independence. (If one eigenvalue had a multiplicity of three, then there would be the three terms<span class="LaTeX">$$B_{i_1} e^{\lambda_i t}, ~ B_{i_2} t e^{\lambda_i t}, ~ B_{i_3} t^2 e^{\lambda_i t}$$</span>. By contrast, when all eigenvalues are distinct, the <span class="LaTeX">$B$</span>s are just the <a href="Frobenius_covariant" title="wikilink">Frobenius covariants</a>, and solving for them as below just amounts to the inversion of the <a href="Vandermonde_matrix" title="wikilink">Vandermonde matrix</a> of these 4 eigenvalues.)</p>
<p>Sum all such terms, here four such:</p>
<p><span class="LaTeX">$$e^{A t} = B_{1_1} e^{\lambda_1 t} + B_{1_2} t e^{\lambda_1 t} + B_{2_1} e^{\lambda_2 t} + B_{2_2} t e^{\lambda_2 t} ,$$</span></p>
<p><span class="LaTeX">$$e^{A t} = B_{1_1} e^{3/4 t} + B_{1_2} t e^{3/4 t} + B_{2_1} e^{1 t} + B_{2_2} t e^{1 t}$$</span>.</p>
<p>To solve for all of the unknown matrices <span class="LaTeX">$B$</span> in terms of the first three powers of <span class="LaTeX">$A$</span> and the identity, we need four equations, the above one providing one such at <span class="LaTeX">$t$</span> =0. Further, differentiate it with respect to <span class="LaTeX">$t$</span>,</p>
<p><span class="LaTeX">$$A e^{A t} = 3/4 B_{1_1} e^{3/4 t} + \left( 3/4 t + 1 \right) B_{1_2} e^{3/4 t} + 1 B_{2_1} e^{1 t} + \left(1 t + 1 \right) B_{2_2} e^{1 t}   ~,$$</span> and again,</p>
<p><span class="LaTeX">$$\begin{align}
A^2 e^{A t} =& (3/4)^2 B_{1_1} e^{3/4 t} + \left( (3/4)^2 t + ( 3/4 + 1 \cdot 3/4) \right) B_{1_2} e^{3/4 t} + B_{2_1} e^{1 t}\\ +& \left(1^2 t + (1 + 1 \cdot 1 )\right) B_{2_2} e^{1 t} \\  =& (3/4)^2 B_{1_1} e^{3/4 t} + \left( (3/4)^2 t + 3/2 \right) B_{1_2} e^{3/4 t} + B_{2_1} e^{t} + \left(t + 2\right) B_{2_2} e^{t} ~,
\end{align}$$</span> and once more,</p>
<p><span class="LaTeX">$$\begin{align}
A^3 e^{A t} =& (3/4)^3 B_{1_1} e^{3/4 t} + \left( (3/4)^3 t + ( (3/4)^2 + (3/2) \cdot 3/4) ) \right) B_{1_2} e^{3/4 t}\\ +& B_{2_1} e^{1 t} + \left(1^3 t + (1 + 2) \cdot 1 \right) B_{2_2} e^{1 t} \\ =&  (3/4)^3 B_{1_1} e^{3/4 t}\! + \left( (3/4)^3 t\! + 27/16 ) \right) B_{1_2} e^{3/4 t}\! + B_{2_1} e^{t}\! + \left(t + 3\cdot 1\right) B_{2_2} e^{t}
\end{align}$$</span>. (In the general case, <span class="LaTeX">$n$</span>−1 derivatives need be taken.)</p>
<p>Setting <span class="LaTeX">$t$</span>=0 in these four equations, the four coefficient matrices <span class="LaTeX">$B$</span>s may be solved for,</p>
<p><span class="LaTeX">$$\begin{align}
I =& B_{1_1} + B_{2_1} \\
A =& 3/4 B_{1_1} + B_{1_2} + B_{2_1} + B_{2_2} \\
A^2 =& (3/4)^2 B_{1_1} + (3/2) B_{1_2} + B_{2_1} + 2 B_{2_2} \\
A^3 =& (3/4)^3 B_{1_1} + (27/16) B_{1_2} + B_{2_1} + 3 B_{2_2}
\end{align}$$</span> , to yield</p>
<p><span class="LaTeX">$$\begin{align}
B_{1_1} =& 128 A^3 - 366 A^2 + 288 A - 80 I \\
B_{1_2} =& 16 A^3 - 44 A^2 + 40 A - 12 I \\
B_{2_1} =&-128 A^3 + 366 A^2 - 288 A + 80 I\\
B_{2_2} =& 16 A^3 - 40 A^2 + 33 A - 9 I
\end{align}$$</span> .</p>
<p>Substituting with the value for <span class="LaTeX">$A$</span> yields the coefficient matrices</p>
<p><span class="LaTeX">$$\begin{align}
B_{1_1} =& \begin{pmatrix}0 & 0 & 48 & -16\\ 0 & 0 & -8 & 2\\ 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 1\end{pmatrix}\\
B_{1_2} =& \begin{pmatrix}0 & 0 & 4 & -2\\ 0 & 0 & -1 & 1/2\\ 0 & 0 & 1/4 & -1/8\\ 0 & 0 & 1/2 & -1/4 \end{pmatrix}\\
B_{2_1} =& \begin{pmatrix}1 & 0 & -48 & 16\\ 0 & 1 & 8 & -2\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\end{pmatrix}\\
B_{2_2} =& \begin{pmatrix}0 & 1 & 8 & -2\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\end{pmatrix}  
\end{align}$$</span> so the final answer is</p>
<p><span class="LaTeX">$${e}^{tA}\!=\!\begin{pmatrix}{e}^{t} & t{e}^{t} & \left( 8t-48\right) {e}^{t}\!+\left( 4t+48\right){e}^{3t/4} & \left( 16-2\,t\right){e}^{t}\!+\left( -2t-16\right){e}^{3t/4}\\ 0 & {e}^{t} & 8{e}^{t}\!+\left( -t-8\right) {e}^{3t/4} & -\frac{4{e}^{t}+\left(-t-4\right){e}^{3t/4}}{2}\\ 0 & 0 & \frac{\left( t+4\right) {e}^{3t/4}}{4} & -\frac{t {e}^{3t/4}}{8}\\ 0 & 0 & \frac{t{e}^{3t/4}}{2} & -\frac{\left( t-4\right) {e}^{3t/4}}{4}\end{pmatrix}$$</span>.</p>
<p>The procedure is much shorter than <a href="Matrix_differential_equation#Putzer_Algorithm_for_computing_eAt" title="wikilink">Putzer's algorithm</a> sometimes utilized in such cases.</p>
<h2 id="illustrations">Illustrations</h2>
<p>Suppose that we want to compute the exponential of</p>
<p><span class="LaTeX">$$B=\begin{bmatrix}
21 & 17 & 6 \\
-5 & -1 & -6 \\
4 & 4 & 16 \end{bmatrix}.$$</span></p>
<p>Its <a href="Jordan_normal_form" title="wikilink">Jordan form</a> is</p>
<p><span class="LaTeX">$$J = P^{-1}BP = \begin{bmatrix}
4 & 0 & 0 \\
0 & 16 & 1 \\
0 & 0 & 16 \end{bmatrix},$$</span></p>
<p>where the matrix <em>P</em> is given by</p>
<p><span class="LaTeX">$$P=\begin{bmatrix}
-\frac14 & 2 & \frac54 \\
\frac14 & -2 & -\frac14 \\
0 & 4 & 0 \end{bmatrix}.$$</span></p>
<p>Let us first calculate exp(<em>J</em>). We have</p>
<p><span class="LaTeX">$$J=J_1(4)\oplus J_2(16) \,$$</span></p>
<p>The exponential of a 1×1 matrix is just the exponential of the one entry of the matrix, so exp(<em>J</em><sub>1</sub>(4)) = [<em>e</em><sup>4</sup>]. The exponential of <em>J</em><sub>2</sub>(16) can be calculated by the formula <em>e</em><sup>(λ<em>I</em> + <em>N</em>)</sup> = <em>e</em><sup>λ</sup> <em>e</em><sup>N</sup> mentioned above; this yields<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></p>
<p><span class="LaTeX">$$\begin{align}
\exp \left( \begin{bmatrix} 16 & 1 \\ 0 & 16 \end{bmatrix} \right)
& = e^{16} \exp \left( \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \right) \\[6pt]
& = e^{16} \left(\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} + \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} + {1 \over 2!}\begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} + \cdots \right)
= \begin{bmatrix} e^{16} & e^{16} \\ 0 & e^{16} \end{bmatrix}.
\end{align}$$</span></p>
<p>Therefore, the exponential of the original matrix <em>B</em> is</p>
<p><span class="LaTeX">$$\begin{align}
\exp(B)
& = P \exp(J) P^{-1}
= P \begin{bmatrix} e^4 & 0 & 0 \\ 0 & e^{16} & e^{16} \\ 0 & 0 & e^{16}  \end{bmatrix} P^{-1} \\[6pt]
& = {1\over 4} \begin{bmatrix}
   13e^{16} - e^4 & 13e^{16} - 5e^4 & 2e^{16} - 2e^4 \\
   -9e^{16} + e^4 & -9e^{16} + 5e^4 & -2e^{16} + 2e^4 \\
   16e^{16}       & 16e^{16}        & 4e^{16}
\end{bmatrix}.
\end{align}$$</span></p>
<h2 id="applications">Applications</h2>
<h3 id="linear-differential-equations">Linear differential equations</h3>
<p>The matrix exponential has applications to systems of <a href="linear_differential_equation" title="wikilink">linear differential equations</a>. (See also <a href="matrix_differential_equation" title="wikilink">matrix differential equation</a>.) Recall from earlier in this article that a <em>homogeneous</em> differential equation of the form</p>
<p><span class="LaTeX">$$\mathbf{y}' = A\mathbf{y}$$</span> has solution <mtpl></mtpl>.</p>
<p>If we consider the vector</p>
<p><span class="LaTeX">$$\mathbf{y}(t) = \begin{pmatrix} y_1(t) \\ \vdots \\y_n(t) \end{pmatrix}   ~,$$</span> we can express a system of <em>inhomogeneous</em> coupled linear differential equations as</p>
<p><span class="LaTeX">$$\mathbf{y}'(t) = A\mathbf{y}(t)+\mathbf{b}(t).$$</span> Making an <a class="uri" href="ansatz" title="wikilink">ansatz</a> to use an integrating factor of <mtpl></mtpl> and multiplying throughout, yields</p>
<p><span class="LaTeX">$$e^{-At}\mathbf{y}'-e^{-At}A\mathbf{y} = e^{-At}\mathbf{b}$$</span></p>
<p><span class="LaTeX">$$e^{-At}\mathbf{y}'-Ae^{-At}\mathbf{y} = e^{-At}\mathbf{b}$$</span></p>
<p><span class="LaTeX">$$\frac{d}{dt} (e^{-At}\mathbf{y}) = e^{-At}\mathbf{b}~.$$</span></p>
<p>The second step is possible due to the fact that, if <span class="LaTeX">$AB = BA$</span>, then <mtpl> <em>Be</em><sup><em>At</em></sup>}}</mtpl>. So, calculating <mtpl></mtpl> leads to the solution to the system, by simply integrating the third step in <span class="LaTeX">$t$</span>s.</p>
<h4 id="example-homogeneous">Example (homogeneous)</h4>
<p>Consider the system</p>
<p><span class="LaTeX">$$\begin{matrix}
x' &=& 2x&-y&+z \\
y' &=&   &3y&-1z \\
z' &=& 2x&+y&+3z  ~.\end{matrix}$$</span></p>
<p>The associated <a href="defective_matrix" title="wikilink">defective matrix</a> is</p>
<p><span class="LaTeX">$$A=\begin{bmatrix}
2 & -1 &  1 \\
0 &  3 & -1 \\
2 &  1 &  3 \end{bmatrix}  ~.$$</span></p>
<p>The matrix exponential is</p>
<p><span class="LaTeX">$$e^{tA}=\frac{1}{2}\begin{bmatrix}
    e^{2t}(1+e^{2t}-2t)  & -2te^{2t}    &  e^{2t}(-1+e^{2t}) \\
   -e^{2t}(-1+e^{2t}-2t) & 2(t+1)e^{2t} & -e^{2t}(-1+e^{2t}) \\
    e^{2t}(-1+e^{2t}+2t) & 2te^{2t}     &  e^{2t}(1+e^{2t})  \end{bmatrix}~,$$</span></p>
<p>so that the general solution of the homogeneous system is</p>
<p><span class="LaTeX">$$\begin{bmatrix}x \\y \\ z\end{bmatrix}=
\frac{x(0)}{2}\begin{bmatrix}e^{2t}(1+e^{2t}-2t) \\-e^{2t}(-1+e^{2t}-2t)\\e^{2t}(-1+e^{2t}+2t)\end{bmatrix}
+\frac{y(0)}{2}\begin{bmatrix}-2te^{2t}\\2(t+1)e^{2t}\\2te^{2t}\end{bmatrix}
+\frac{z(0)}{2}\begin{bmatrix}e^{2t}(-1+e^{2t})\\-e^{2t}(-1+e^{2t})\\e^{2t}(1+e^{2t})\end{bmatrix} ~,$$</span> amounting to</p>
<p><span class="LaTeX">$$\begin{align}
2x & = x(0)(e^{2t}(1+e^{2t}-2t)) + y(0) (-2te^{2t}) + z(0)(e^{2t}(-1+e^{2t})) \\
2y & = x(0)(-e^{2t}(-1+e^{2t}-2t)) + y(0)(2(t+1)e^{2t}) + z(0)(-e^{2t}(-1+e^{2t})) \\
2z & = x(0)(e^{2t}(-1+e^{2t}+2t)) + y(0)(2te^{2t}) + z(0)(e^{2t}(1+e^{2t}))  ~.
\end{align}$$</span></p>
<h4 id="example-inhomogeneous">Example (inhomogeneous)</h4>
<p>Consider now the inhomogeneous system</p>
<p><span class="LaTeX">$$\begin{matrix}
x' &=& 2x & - & y & + & z & + & e^{2t} \\
y' &=&    &   & 3y& - & z & \\
z' &=& 2x & + & y & + & 3z & + & e^{2t} \end{matrix} ~.$$</span></p>
<p>We again have</p>
<p><span class="LaTeX">$$A= \left[ \begin{array}{rrr}
2 & -1 &  1 \\
0 &  3 & -1 \\
2 &  1 &  3 \end{array} \right] ~,$$</span> and</p>
<p><span class="LaTeX">$$\mathbf{b}=e^{2t}\begin{bmatrix}1 \\0\\1\end{bmatrix}.$$</span></p>
<p>From before, we already have the general solution to the homogeneous equation. Since the sum of the homogeneous and particular solutions give the general solution to the inhomogeneous problem, we now only need find the particular solution.</p>
<p>We have, by above,</p>
<p><span class="LaTeX">$$\mathbf{y}_p = e^{tA}\int_0^t e^{(-u)A}\begin{bmatrix}e^{2u} \\0\\e^{2u}\end{bmatrix}\,du+e^{tA}\mathbf{c}$$</span></p>
<p><span class="LaTeX">$$\mathbf{y}_p = e^{tA}\int_0^t
\begin{bmatrix}
     2e^u - 2ue^{2u} & -2ue^{2u}    & 0 \\  \\
-2e^u + 2(u+1)e^{2u} & 2(u+1)e^{2u} & 0 \\  \\
            2ue^{2u} & 2ue^{2u}     & 2e^u\end{bmatrix}\begin{bmatrix}e^{2u} \\0\\e^{2u}\end{bmatrix}\,du+e^{tA}\mathbf{c}$$</span></p>
<p><span class="LaTeX">$$\mathbf{y}_p = e^{tA}\int_0^t
\begin{bmatrix}
e^{2u}( 2e^u - 2ue^{2u}) \\  \\
  e^{2u}(-2e^u + 2(1 + u)e^{2u}) \\  \\
  2e^{3u} + 2ue^{4u}\end{bmatrix}\,du+e^{tA}\mathbf{c}$$</span></p>
<p><span class="LaTeX">$$\mathbf{y}_p = e^{tA}\begin{bmatrix}
-{1 \over 24}e^{3t}(3e^t(4t-1)-16) \\  \\
{1 \over 24}e^{3t}(3e^t(4t+4)-16) \\  \\
{1 \over 24}e^{3t}(3e^t(4t-1)-16)\end{bmatrix}+
\begin{bmatrix}
     2e^t - 2te^{2t} & -2te^{2t}    & 0 \\  \\
-2e^t + 2(t+1)e^{2t} & 2(t+1)e^{2t} & 0 \\  \\
            2te^{2t} & 2te^{2t}     & 2e^t\end{bmatrix}\begin{bmatrix}c_1 \\c_2 \\c_3\end{bmatrix} ~,$$</span> which could be further simplified to get the requisite particular solution determined through variation of parameters. Note <strong>c</strong> = <strong>y</strong><sub><em>p</em></sub>(0). For more rigor, see the following generalization.</p>
<h3 id="inhomogeneous-case-generalization-variation-of-parameters">Inhomogeneous case generalization: variation of parameters</h3>
<p>For the inhomogeneous case, we can use <a href="integrating_factor" title="wikilink">integrating factors</a> (a method akin to <a href="variation_of_parameters" title="wikilink">variation of parameters</a>). We seek a particular solution of the form <mtpl> exp(<em>tA</em>) <strong>z</strong> (<em>t</em>) }}</mtpl>,</p>
<p><span class="LaTeX">$$\begin{align}
\mathbf{y}_p'(t) & = (e^{tA})'\mathbf{z}(t)+e^{tA}\mathbf{z}'(t) \\[6pt]
& = Ae^{tA}\mathbf{z}(t)+e^{tA}\mathbf{z}'(t) \\[6pt]
& = A\mathbf{y}_p(t)+e^{tA}\mathbf{z}'(t)~.
\end{align}$$</span></p>
<p>For <mtpl></mtpl> to be a solution,</p>
<p><span class="LaTeX">$$\begin{align}
e^{tA}\mathbf{z}'(t) & = \mathbf{b}(t) \\[6pt]
\mathbf{z}'(t) & = (e^{tA})^{-1}\mathbf{b}(t) \\[6pt]
\mathbf{z}(t) & = \int_0^t e^{-uA}\mathbf{b}(u)\,du+\mathbf{c} ~.
\end{align}$$</span></p>
<p>Thus,</p>
<p><span class="LaTeX">$$\begin{align}
\mathbf{y}_p(t) & {} = e^{tA}\int_0^t e^{-uA}\mathbf{b}(u)\,du+e^{tA}\mathbf{c} \\
& {} = \int_0^t e^{(t-u)A}\mathbf{b}(u)\,du+e^{tA}\mathbf{c}
\end{align} ~,$$</span> where <span class="LaTeX">$\mathbf{ c } $</span> is determined by the initial conditions of the problem.</p>
<p>More precisely, consider the equation</p>
<p><span class="LaTeX">$$Y'-A\ Y=F(t)$$</span> with the initial condition <mtpl> <em>Y<sub>0</sub></em>}}</mtpl>, where <span class="LaTeX">$A$</span> is an <span class="LaTeX">$n$</span> by <span class="LaTeX">$n$</span> complex matrix,</p>
<p><span class="LaTeX">$F$</span> is a continuous function from some open interval <span class="LaTeX">$I$</span> to ℂ<sup><em>n</em></sup>,</p>
<p><span class="LaTeX">$t_0$</span> is a point of <span class="LaTeX">$I$</span>, and</p>
<p><span class="LaTeX">$Y_0$</span> is a vector of ℂ<sup><em>n</em></sup>.</p>
<p>Left-multiplying the above displayed equality by <mtpl></mtpl> yields</p>
<p><span class="LaTeX">$$Y(t)=e^{(t-t_0)A}\ Y_0+\int_{t_0}^t e^{(t-x)A}\ F(x)\ dx  ~.$$</span></p>
<p>We claim that the solution to the equation</p>
<p><span class="LaTeX">$$P(d/dt)\ y = f(t)$$</span> with the initial conditions <span class="LaTeX">$y^{(k)}(t_0)=y_k$</span> for 0 ≤ <span class="LaTeX">$k  is</span></p>
<p><span class="LaTeX">$$y(t)=\sum_{k=0}^{n-1}\ y_k\ s_k(t-t_0)+\int_{t_0}^t s_{n-1}(t-x)\ f(x)\ dx ~,$$</span> where the notation is as follows:</p>
<p><span class="LaTeX">$P\in\mathbb{C}[X]$</span> is a monic polynomial of degree <span class="LaTeX">$n > 0$</span>,</p>
<p><span class="LaTeX">$f$</span> is a continuous complex valued function defined on some open interval <span class="LaTeX">$I$</span>,</p>
<p><span class="LaTeX">$t_0$</span> is a point of <span class="LaTeX">$I$</span>,</p>
<p><span class="LaTeX">$y_k$</span> is a complex number, and</p>
<p><mtpl></mtpl> is the coefficient of <span class="LaTeX">$X^k$</span> in the polynomial denoted by <span class="LaTeX">$S_t\in\mathbb{C}[X]$</span> in Subsection <a href="matrix_exponential#Evaluation_by_Laurent_series" title="wikilink">Evaluation by Laurent series</a> above.</p>
<p>To justify this claim, we transform our order <span class="LaTeX">$n$</span> scalar equation into an order one vector equation by the usual <a href="Ordinary_differential_equation#Reduction_to_a_first_order_system" title="wikilink">reduction to a first order system</a>. Our vector equation takes the form</p>
<p><span class="LaTeX">$$\frac{dY}{dt}-A\ Y=F(t),\quad Y(t_0)=Y_0,$$</span></p>
<p>where <span class="LaTeX">$A$</span> is the <a class="uri" href="transpose" title="wikilink">transpose</a> <a href="companion_matrix" title="wikilink">companion matrix</a> of <span class="LaTeX">$P$</span>. We solve this equation as explained above, computing the matrix exponentials by the observation made in Subsection <a href="matrix_exponential#Alternative" title="wikilink">Alternative</a> above.</p>
<p>In the case <span class="LaTeX">$n$</span> = 2 we get the following statement. The solution to</p>
<p><span class="LaTeX">$$y''-(\alpha+\beta)\ y'
+\alpha\,\beta\ y=f(t),\quad
y(t_0)=y_0,\quad y'(t_0)=y_1$$</span> is</p>
<p><span class="LaTeX">$$y(t)=y_0\ s_0(t-t_0)+y_1\ s_1(t-t_0)
+\int_{t_0}^t s_1(t-x)\,f(x)\ dx,$$</span> where the functions <mtpl></mtpl> and <mtpl></mtpl> are as in Subsection <a href="matrix_exponential#Evaluation_by_Laurent_series" title="wikilink">Evaluation by Laurent series</a> above.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Matrix_function" title="wikilink">Matrix function</a></li>
<li><a href="Matrix_logarithm" title="wikilink">Matrix logarithm</a></li>
<li><a href="Exponential_function" title="wikilink">Exponential function</a></li>
<li><a href="Exponential_map_(Lie_theory)" title="wikilink">Exponential map (Lie theory)</a></li>
<li><a href="Magnus_expansion" title="wikilink">Magnus expansion</a></li>
<li><a href="Derivative_of_the_exponential_map" title="wikilink">Derivative of the exponential map</a></li>
<li><a href="Vector_flow" title="wikilink">Vector flow</a></li>
<li><a href="Golden–Thompson_inequality" title="wikilink">Golden–Thompson inequality</a></li>
<li><a href="Phase-type_distribution" title="wikilink">Phase-type distribution</a></li>
<li><a href="Lie_product_formula" title="wikilink">Lie product formula</a></li>
<li><a href="Baker–Campbell–Hausdorff_formula" title="wikilink">Baker–Campbell–Hausdorff formula</a></li>
<li><a href="Frobenius_covariant" title="wikilink">Frobenius covariant</a></li>
<li><a href="Sylvester's_formula" title="wikilink">Sylvester's formula</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li>
<p>.</p></li>
<li>
<p>.</p></li>
<li></li>
<li></li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li></li>
<li><a href="http://math.fullerton.edu/mathews/n2003/MatrixExponentialMod.html">Module for the Matrix Exponential</a></li>
</ul>
<p>"</p>
<p><a href="Category:Matrix_theory" title="wikilink">Category:Matrix theory</a> <a href="Category:Lie_groups" title="wikilink">Category:Lie groups</a> <a class="uri" href="Category:Exponentials" title="wikilink">Category:Exponentials</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"> <a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7">in a Euclidean space<a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10">Rinehart, R. F. (1955). "The equivalence of definitions of a matric function". <em>The American Mathematical Monthly</em>, <strong>62</strong> (6), 395-414.<a href="#fnref10">↩</a></li>
<li id="fn11">This can be generalized; in general, the exponential of <em>J</em><sub><em>n</em></sub>(<em>a</em>) is an upper triangular matrix with <em>e</em><sup><em>a</em></sup>/0! on the main diagonal, <em>e</em><sup><em>a</em></sup>/1! on the one above, <em>e</em><sup><em>a</em></sup>/2! on the next one, and so on.<a href="#fnref11">↩</a></li>
</ol>
</section>
</body>
</html>
