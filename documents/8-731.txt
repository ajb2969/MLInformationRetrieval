   Quantum mutual information      Quantum mutual information   In quantum information theory , quantum mutual information , or von Neumann mutual information , after John von Neumann , is a measure of correlation between subsystems of quantum state. It is the quantum mechanical analog of Shannon mutual information .  Motivation  For simplicity, it will be assumed that all objects in the article are finite-dimensional.  The definition of quantum mutual entropy is motivated by the classical case. For a probability distribution of two variables p ( x , y ), the two marginal distributions are         p   (  x  )    =    ∑  y    p   (  x  ,  y  )      ,    p   (  y  )    =    ∑  x    p   (  x  ,  y  )       .     formulae-sequence      p  x     subscript   y     p   x  y          p  y     subscript   x     p   x  y        p(x)=\sum_{y}p(x,y)\;,\;p(y)=\sum_{x}p(x,y).     The classical mutual information I ( X , Y ) is defined by        I    (  X  ,  Y  )    =     S   (   p   (  x  )    )    +   S   (   p   (  y  )    )     -   S   (   p   (  x  ,  y  )    )           I   X  Y          S    p  x      S    p  y       S    p   x  y        \;I(X,Y)=S(p(x))+S(p(y))-S(p(x,y))     where S ( q ) denotes the Shannon entropy of the probability distribution q .  One can calculate directly        S    (   p   (  x  )    )    +   S   (   p   (  y  )    )          S    p  x      S    p  y      \;S(p(x))+S(p(y))          =   -   (     ∑  x     p  x    log  p    (  x  )     +    ∑  y     p  y    log  p    (  y  )      )        absent        subscript   x      subscript  p  x     p   x      subscript   y      subscript  p  y     p   y        \;=-(\sum_{x}p_{x}\log p(x)+\sum_{y}p_{y}\log p(y))          =   -   (      ∑  x     (    ∑   y  ′     p   (  x  ,   y  ′   )   log    ∑   y  ′     p   (  x  ,   y  ′   )       )    +    ∑  y    (    ∑   x  ′     p   (   x  ′   ,  y  )   log    ∑   x  ′     p   (   x  ′   ,  y  )       )     )        absent        subscript   x     subscript    superscript  y  normal-′      p   x   superscript  y  normal-′       subscript    superscript  y  normal-′      p   x   superscript  y  normal-′           subscript   y     subscript    superscript  x  normal-′      p    superscript  x  normal-′   y      subscript    superscript  x  normal-′      p    superscript  x  normal-′   y            \;=-(\sum_{x}\;(\sum_{y^{\prime}}p(x,y^{\prime})\log\sum_{y^{\prime}}p(x,y^{%
 \prime}))+\sum_{y}(\sum_{x^{\prime}}p(x^{\prime},y)\log\sum_{x^{\prime}}p(x^{%
 \prime},y)))          =   -   (    ∑   x  ,  y     p   (  x  ,  y  )    (    log    ∑   y  ′     p   (  x  ,   y  ′   )      +   log    ∑   x  ′     p   (   x  ′   ,  y  )       )     )        absent      subscript    x  y      p   x  y          subscript    superscript  y  normal-′      p   x   superscript  y  normal-′            subscript    superscript  x  normal-′      p    superscript  x  normal-′   y            \;=-(\sum_{x,y}p(x,y)(\log\sum_{y^{\prime}}p(x,y^{\prime})+\log\sum_{x^{\prime%
 }}p(x^{\prime},y)))           =   -    ∑   x  ,  y     p   (  x  ,  y  )    log  p    (  x  )   p   (  y  )       .      absent      subscript    x  y      p   x  y     p   x  p  y       \;=-\sum_{x,y}p(x,y)\log p(x)p(y).     So the mutual information is        I   (  X  ,  Y  )    =    ∑   x  ,  y     p   (  x  ,  y  )    log    p   (  x  ,  y  )     p   (  x  )   p   (  y  )         .        I   X  Y      subscript    x  y      p   x  y         p   x  y      p  x  p  y         I(X,Y)=\sum_{x,y}p(x,y)\log\frac{p(x,y)}{p(x)p(y)}.     But this is precisely the relative entropy between p ( x , y ) and p ( x ) p ( y ). In other words, if we assume the two variables x and y to be uncorrelated, mutual information is the discrepancy in uncertainty resulting from this (possibly erroneous) assumption.  It follows from the property of relative entropy that I ( X , Y ) ≥ 0 and equality holds if and only if p ( x , y ) = p ( x ) p ( y ).  Definition  The quantum mechanical counterpart of classical probability distributions are density matrices .  Consider a composite quantum system whose state space is the tensor product       H  =    H  A   ⊗   H  B     .      H   tensor-product   subscript  H  A    subscript  H  B      H=H_{A}\otimes H_{B}.     Let ρ AB be a density matrix acting on H . The von Neumann entropy of ρ , which is the quantum mechanical analogy of the Shannon entropy, is given by        S   (   ρ   A  B    )    =   -   Tr    ρ   A  B     log   ρ   A  B         .        S   superscript  ρ    A  B        Tr     superscript  ρ    A  B       superscript  ρ    A  B          S(\rho^{AB})=-\operatorname{Tr}\rho^{AB}\log\rho^{AB}.     For a probability distribution p ( x , y ), the marginal distributions are obtained by integrating away the variables x or y . The corresponding operation for density matrices is the partial trace . So one can assign to ρ a state on the subsystem A by       ρ  A   =     Tr  B     ρ   A  B          superscript  ρ  A     subscript  Tr  B    superscript  ρ    A  B       \rho^{A}=\operatorname{Tr}_{B}\;\rho^{AB}     where Tr B is partial trace with respect to system B . This is the reduced state of ρ AB on system A . The reduced von Neumann entropy of ρ AB with respect to system A is        S    (   ρ  A   )    .      S   superscript  ρ  A     \;S(\rho^{A}).     S ( ρ B ) is defined in the same way.  Technical Note: In mathematical language, passing from the classical to quantum setting can be described as follows. The algebra of observables of a physical system is a C*-algebra and states are unital linear functionals on the algebra. Classical systems are described by commutative C*-algebras, therefore classical states are probability measures . Quantum mechanical systems have non-commutative observable algebras. In concrete considerations, quantum states are density operators. If the probability measure μ is a state on classical composite system consisting of two subsystem A and B , we project μ onto the system A to obtain the reduced state. As stated above, the quantum analog of this is the partial trace operation, which can be viewed as projection onto a tensor component. End of note  It can now be seen that the appropriate definition of quantum mutual information should be         I    (   ρ   A  B    )    =     S   (   ρ  A   )    +   S   (   ρ  B   )     -   S   (   ρ   A  B    )      .        I   superscript  ρ    A  B           S   superscript  ρ  A      S   superscript  ρ  B       S   superscript  ρ    A  B        \;I(\rho^{AB})=S(\rho^{A})+S(\rho^{B})-S(\rho^{AB}).     Quantum mutual information can be interpreted the same way as in the classical case: it can be shown that      I   (   ρ   A  B    )   =  S   (   ρ   A  B    ∥   ρ  A   ⊗   ρ  B   )      fragments  I   fragments  normal-(   superscript  ρ    A  B    normal-)    S   fragments  normal-(   superscript  ρ    A  B    parallel-to   superscript  ρ  A   tensor-product   superscript  ρ  B   normal-)     I(\rho^{AB})=S(\rho^{AB}\|\rho^{A}\otimes\rho^{B})     where    S   (  ⋅  ∥  ⋅  )      fragments  S   fragments  normal-(  normal-⋅  parallel-to  normal-⋅  normal-)     S(\cdot\|\cdot)   denotes quantum relative entropy .  "  Category:Quantum mechanical entropy  Category:Quantum information theory   