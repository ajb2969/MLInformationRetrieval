   Gibbs' inequality      Gibbs' inequality   In information theory , Gibbs' inequality is a statement about the mathematical entropy of a discrete probability distribution . Several other bounds on the entropy of probability distributions are derived from Gibbs' inequality, including Fano's inequality . It was first presented by J. Willard Gibbs in the 19th century.  Gibbs' inequality  Suppose that      P  =   {   p  1   ,  …  ,   p  n   }       P    subscript  p  1   normal-…   subscript  p  n      P=\{p_{1},\ldots,p_{n}\}     is a probability distribution . Then for any other probability distribution      Q  =   {   q  1   ,  …  ,   q  n   }       Q    subscript  q  1   normal-…   subscript  q  n      Q=\{q_{1},\ldots,q_{n}\}     the following inequality between positive quantities (since the p i and q i are positive numbers less than one) holds       -    ∑   i  =  1   n     p  i     log  2    p  i       ≤   -    ∑   i  =  1   n     p  i     log  2    q  i               superscript   subscript     i  1    n      subscript  p  i     subscript   2    subscript  p  i           superscript   subscript     i  1    n      subscript  p  i     subscript   2    subscript  q  i         -\sum_{i=1}^{n}p_{i}\log_{2}p_{i}\leq-\sum_{i=1}^{n}p_{i}\log_{2}q_{i}     with equality if and only if       p  i   =    q  i         subscript  p  i    subscript  q  i     p_{i}=q_{i}\,     for all i . Put in words, the information entropy of a distribution P is less than or equal to its cross entropy with any other distribution Q.  The difference between the two quantities is the Kullback–Leibler divergence or relative entropy, so the inequality can also be written:       D  KL    (  P  ∥  Q  )   ≡   ∑   i  =  1   n    p  i    log  2     p  i    q  i    ≥  0.     fragments   subscript  D  KL    fragments  normal-(  P  parallel-to  Q  normal-)     superscript   subscript     i  1    n    subscript  p  i    subscript   2      subscript  p  i    subscript  q  i     0.    D_{\mathrm{KL}}(P\|Q)\equiv\sum_{i=1}^{n}p_{i}\log_{2}\frac{p_{i}}{q_{i}}\geq 0.     Note that the use of base-2 logarithms is optional, and allows one to refer to the quantity on each side of the inequality as an "average surprisal " measured in bits .  Proof  Since        log  2   a   =    ln  a    ln  2          subscript   2   a       a     2      \log_{2}a=\frac{\ln a}{\ln 2}     it is sufficient to prove the statement using the natural logarithm (ln). Note that the natural logarithm satisfies       ln  x   ≤   x  -  1         x     x  1     \ln x\leq x-1     for all x > 0 with equality if and only if x=1 .  Let   I   I   I   denote the set of all   i   i   i   for which p i is non-zero. Then       -    ∑   i  ∈  I      p  i    ln    q  i    p  i        ≥   -    ∑   i  ∈  I      p  i    (     q  i    p  i    -  1   )              subscript     i  I       subscript  p  i        subscript  q  i    subscript  p  i            subscript     i  I       subscript  p  i        subscript  q  i    subscript  p  i    1        -\sum_{i\in I}p_{i}\ln\frac{q_{i}}{p_{i}}\geq-\sum_{i\in I}p_{i}\left(\frac{q_%
 {i}}{p_{i}}-1\right)          =    -    ∑   i  ∈  I     q  i     +    ∑   i  ∈  I     p  i         absent        subscript     i  I     subscript  q  i       subscript     i  I     subscript  p  i       =-\sum_{i\in I}q_{i}+\sum_{i\in I}p_{i}          =    -    ∑   i  ∈  I     q  i     +  1   ≥  0.        absent        subscript     i  I     subscript  q  i     1        0.     =-\sum_{i\in I}q_{i}+1\geq 0.     So       -    ∑   i  ∈  I      p  i    ln   q  i       ≥   -    ∑   i  ∈  I      p  i    ln   p  i               subscript     i  I       subscript  p  i      subscript  q  i           subscript     i  I       subscript  p  i      subscript  p  i         -\sum_{i\in I}p_{i}\ln q_{i}\geq-\sum_{i\in I}p_{i}\ln p_{i}     and then trivially       -    ∑   i  =  1   n     p  i    ln   q  i       ≥   -    ∑   i  =  1   n     p  i    ln   p  i               superscript   subscript     i  1    n      subscript  p  i      subscript  q  i           superscript   subscript     i  1    n      subscript  p  i      subscript  p  i         -\sum_{i=1}^{n}p_{i}\ln q_{i}\geq-\sum_{i=1}^{n}p_{i}\ln p_{i}     since the right hand side does not grow, but the left hand side may grow or may stay the same.  For equality to hold, we require:         q  i    p  i    =  1         subscript  q  i    subscript  p  i    1    \frac{q_{i}}{p_{i}}=1   for all    i  ∈  I      i  I    i\in I   so that the approximation     ln    q  i    p  i     =     q  i    p  i    -  1            subscript  q  i    subscript  p  i          subscript  q  i    subscript  p  i    1     \ln\frac{q_{i}}{p_{i}}=\frac{q_{i}}{p_{i}}-1   is exact.        ∑   i  ∈  I     q  i    =  1        subscript     i  I     subscript  q  i    1    \sum_{i\in I}q_{i}=1   so that equality continues to hold between the third and fourth lines of the proof.   This can happen if and only if       p  i   =   q  i        subscript  p  i    subscript  q  i     p_{i}=q_{i}     for i = 1, ..., n .  Alternative proofs  The result can alternatively be proved using Jensen's inequality or log sum inequality .  Corollary  The entropy of   P   P   P   is bounded by:        H   (   p  1   ,  …  ,   p  n   )    ≤   log  n    .        H    subscript  p  1   normal-…   subscript  p  n       n     H(p_{1},\ldots,p_{n})\leq\log n.     The proof is trivial - simply set     q  i   =   1  /  n        subscript  q  i     1  n     q_{i}=1/n   for all i .  See also   Information entropy   "  Category:Information theory  Category:Coding theory  Category:Inequalities  Category:Articles containing proofs   