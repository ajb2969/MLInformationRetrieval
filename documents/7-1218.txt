   Berndt–Hall–Hall–Hausman algorithm      Berndt–Hall–Hall–Hausman algorithm   The Berndt–Hall–Hall–Hausman ( BHHH ) algorithm is a numerical optimization  algorithm similar to the Gauss–Newton algorithm . It is named after the four originators: Ernst R. Berndt , B. Hall, Robert Hall , and Jerry Hausman .  Usage  If a nonlinear model is fitted to the data one often needs to estimate coefficients through optimization . A number of optimisation algorithms have the following general structure. Suppose that the function to be optimized is Q ( β ). Then the algorithms are iterative, defining a sequence of approximations, β k given by        β   k  +  1    =    β  k   -    λ  k    A  k     ∂  Q    ∂  β     (   β  k   )      ,       subscript  β    k  1       subscript  β  k      subscript  λ  k    subscript  A  k       Q     β     subscript  β  k       \beta_{k+1}=\beta_{k}-\lambda_{k}A_{k}\frac{\partial Q}{\partial\beta}(\beta_{%
 k}),   ,  where    β  k     subscript  β  k    \beta_{k}   is the parameter estimate at step k, and    λ  k     subscript  λ  k    \lambda_{k}   is a parameter (called step size) which partly determines the particular algorithm. For the BHHH algorithm λ k is determined by calculations within a given iterative step, involving a line-search until a point ''β k''+1 is found satisfying certain criteria. In addition, for the BHHH algorithm, Q has the form      Q  =    ∑   i  =  1   N    Q  i        Q    superscript   subscript     i  1    N    subscript  Q  i      Q=\sum_{i=1}^{N}Q_{i}   and A is calculated using        A  k   =    [    ∑   i  =  1   N       ∂  ln    Q  i     ∂  β     (   β  k   )      ∂  ln    Q  i     ∂  β      (   β  k   )   ′     ]    -  1     .       subscript  A  k    superscript   delimited-[]    superscript   subscript     i  1    N             subscript  Q  i      β     subscript  β  k           subscript  Q  i      β     superscript   subscript  β  k   normal-′        1      A_{k}=\left[\sum_{i=1}^{N}\frac{\partial\ln Q_{i}}{\partial\beta}(\beta_{k})%
 \frac{\partial\ln Q_{i}}{\partial\beta}(\beta_{k})^{\prime}\right]^{-1}.   In other cases, e.g. Newton–Raphson ,    A  k     subscript  A  k    A_{k}   can have other forms. The BHHH algorithm has the advantage that, if certain conditions apply, convergence of the iterative procedure is guaranteed.  See also   Davidon–Fletcher–Powell (DFP) algorithm  Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm   Further reading         "  Category:Econometrics  Category:Optimization algorithms and methods   