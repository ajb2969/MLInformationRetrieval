<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1712">Feature scaling</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Feature scaling</h1>
<hr/>

<p><strong>Feature scaling</strong> is a method used to standardize the range of independent variables or features of data. In <a href="data_processing" title="wikilink">data processing</a>, it is also known as data normalization and is generally performed during the data preprocessing step.</p>
<h2 id="motivation">Motivation</h2>

<p>Since the range of values of raw data varies widely, in some <a href="machine_learning" title="wikilink">machine learning</a> algorithms, objective functions will not work properly without normalization . For example, the majority of <a href="Statistical_classification" title="wikilink">classifiers</a> calculate the distance between two points by the distance. If one of the features has a broad range of values, the distance will be governed by this particular feature . Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance .</p>

<p>Another reason why feature scaling is applied is that <a href="gradient_descent" title="wikilink">gradient descent</a> converges much faster with feature scaling than without it.</p>
<table>
<tbody>
<tr class="odd">
</tr>
</tbody>
</table>
<h2 id="methods">Methods</h2>
<h3 id="rescaling">Rescaling</h3>

<p>The simplest method is rescaling the range of features to scale the range in [0, 1] or [−1, 1]. Selecting the target range depends on the nature of the data. The general formula is given as:</p>

<p><code>                                   </code>

<math display="inline" id="Feature_scaling:0">
 <semantics>
  <mrow>
   <msup>
    <mi>x</mi>
    <mo>′</mo>
   </msup>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>x</mi>
     <mo>-</mo>
     <mrow>
      <mtext>min</mtext>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mrow>
     <mrow>
      <mtext>max</mtext>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>-</mo>
     <mrow>
      <mtext>min</mtext>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>x</ci>
     <ci>normal-′</ci>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <minus></minus>
      <ci>x</ci>
      <apply>
       <times></times>
       <mtext>min</mtext>
       <ci>x</ci>
      </apply>
     </apply>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <mtext>max</mtext>
       <ci>x</ci>
      </apply>
      <apply>
       <times></times>
       <mtext>min</mtext>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x^{\prime}=\frac{x-\text{min}(x)}{\text{max}(x)-\text{min}(x)}
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Feature_scaling:1">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 is an original value, 

<math display="inline" id="Feature_scaling:2">
 <semantics>
  <msup>
   <mi>x</mi>
   <mo>′</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>x</ci>
    <ci>normal-′</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x^{\prime}
  </annotation>
 </semantics>
</math>

 is the normalized value. For example, suppose that we have the students' weight data, and the students' weights span [160 pounds, 200 pounds]. To rescale this data, we first subtract 160 from each student's weight and divide the result by 40 (the difference between the maximum and minimum weights).</p>
<h3 id="standardization">Standardization</h3>

<p>In machine learning, we can handle various types of data, e.g. audio signals and pixel values for image data, and this data can include multiple <a class="uri" href="dimensions" title="wikilink">dimensions</a>. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the enumerator) and unit-variance. This method is widely used for normalization in many machine learning algorithms (e.g., <a href="support_vector_machine" title="wikilink">support vector machines</a>, <a href="logistic_regression" title="wikilink">logistic regression</a>, and <a href="neural_network" title="wikilink">neural networks</a>) . This is typically done by calculating <a href="standard_score" title="wikilink">standard scores</a> . The general method of calculation is to determine the distribution <a class="uri" href="mean" title="wikilink">mean</a> and <a href="standard_deviation" title="wikilink">standard deviation</a> for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation.</p>

<p><code>                                   </code>

<math display="inline" id="Feature_scaling:3">
 <semantics>
  <mrow>
   <msup>
    <mi>x</mi>
    <mo>′</mo>
   </msup>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>x</mi>
     <mo>-</mo>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">¯</mo>
     </mover>
    </mrow>
    <mi>σ</mi>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>x</ci>
     <ci>normal-′</ci>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <minus></minus>
      <ci>x</ci>
      <apply>
       <ci>normal-¯</ci>
       <ci>x</ci>
      </apply>
     </apply>
     <ci>σ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x^{\prime}=\frac{x-\bar{x}}{\sigma}
  </annotation>
 </semantics>
</math>

</p>

<p>Where 

<math display="inline" id="Feature_scaling:4">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 is the original feature vector, 

<math display="inline" id="Feature_scaling:5">
 <semantics>
  <mover accent="true">
   <mi>x</mi>
   <mo stretchy="false">¯</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-¯</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{x}
  </annotation>
 </semantics>
</math>

 is the mean of that feature vector, and 

<math display="inline" id="Feature_scaling:6">
 <semantics>
  <mi>σ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>σ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma
  </annotation>
 </semantics>
</math>

 is its standard deviation.</p>
<h3 id="scaling-to-unit-length">Scaling to unit length</h3>

<p>Another option that is widely used in machine-learning is to scale the components of a feature vector such that the complete vector has length one. This usually means dividing each component by the Euclidean length of the vector. In some applications (e.g. Histogram features) it can be more practical to use the L1 norm (i.e. Manhattan Distance, City-Block Length or <a href="Taxicab_Geometry" title="wikilink">Taxicab Geometry</a>) of the feature vector:</p>

<p>

<math display="block" id="Feature_scaling:7">
 <semantics>
  <mrow>
   <msup>
    <mi>x</mi>
    <mo>′</mo>
   </msup>
   <mo>=</mo>
   <mfrac>
    <mi>x</mi>
    <mrow>
     <mo fence="true">||</mo>
     <mi>x</mi>
     <mo fence="true">||</mo>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>x</ci>
     <ci>normal-′</ci>
    </apply>
    <apply>
     <divide></divide>
     <ci>x</ci>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x^{\prime}=\frac{x}{||x||}
  </annotation>
 </semantics>
</math>

</p>

<p>This is especially important if in the following learning steps the Scalar Metric is used as a distance measure.</p>
<h2 id="application">Application</h2>

<p>In stochastic <a href="gradient_descent" title="wikilink">gradient descent</a>, feature scaling can sometimes improve the convergence speed of the algorithm . In support vector machines,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> it can reduce the time to find support vectors. Note that feature scaling changes the SVM result .</p>
<h2 id="references">References</h2>
<ul>
<li>S. Aksoy and R. Haralick, "Feature normalization and likelihood-based similarity measures for image retrieval," Pattern Recognit. Lett., Special Issue on Image and Video Retrieval, 2000 <a class="uri" href="http://www.cs.bilkent.edu.tr/~saksoy/papers/prletters01_likelihood.pdf">http://www.cs.bilkent.edu.tr/~saksoy/papers/prletters01_likelihood.pdf</a></li>
</ul>
<ul>
<li>S. Tsakalidis, V. Doumpiotis &amp; W. Byrne, "Discriminative Linear Transforms for Feature Normalization and Speaker Adaptation in HMM Estimation", Proc. ICSLP'02, Denver. <a class="uri" href="http://malach.umiacs.umd.edu/pubs/VD_05_Discrim_linear.pdf">http://malach.umiacs.umd.edu/pubs/VD_05_Discrim_linear.pdf</a></li>
</ul>
<ul>
<li>Liefeng Bo, Ling Wang, and Licheng Jiao, "Feature Scaling for Kernel Fisher Discriminant Analysis Using Leave-one-out Cross Validation", Neural Computation (NECO), vol. 18(4), pp. 961–978, 2006 <a class="uri" href="http://www.cs.washington.edu/homes/lfb/paper/nc06.pdf">http://www.cs.washington.edu/homes/lfb/paper/nc06.pdf</a></li>
</ul>
<ul>
<li>A. Stolcke, S. Kajarekar, and L. Ferrer, "Nonparametric feature normalization for SVM-based speaker verification," in Proc. ICASSP, Las Vegas, Apr. 2008. <a class="uri" href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4517925">http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4517925</a></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li>S. Theodoridis, K. Koutroumbas. (2008) “Pattern Recognition”, Academic Press, 4 edition, ISBN 978-1-59749-272-0</li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li>[<a class="uri" href="http://openclassroom.stanford.edu/MainFolder/VideoPage.php?course=MachineLearning&amp;video">http://openclassroom.stanford.edu/MainFolder/VideoPage.php?course=MachineLearning&amp;video;</a>;=03.1-LinearRegressionII-FeatureScaling&amp;speed;=100/ Lecture by Andrew Ng on feature scaling]</li>
</ul>

<p>"</p>

<p><a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
</ol>
</section>
</body>
</html>
