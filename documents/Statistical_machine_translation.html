<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1551">Statistical machine translation</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Statistical machine translation</h1>
<hr/>

<p><strong>Statistical machine translation</strong> (<strong>SMT</strong>) is a <a href="machine_translation" title="wikilink">machine translation</a> <a class="uri" href="paradigm" title="wikilink">paradigm</a> where translations are generated on the basis of <a href="statistical_model" title="wikilink">statistical models</a> whose parameters are derived from the analysis of bilingual <a href="text_corpora" title="wikilink">text corpora</a>. The statistical approach contrasts with the rule-based approaches to <a href="machine_translation" title="wikilink">machine translation</a> as well as with <a href="example-based_machine_translation" title="wikilink">example-based machine translation</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>

<p>The first ideas of statistical machine translation were introduced by <a href="Warren_Weaver" title="wikilink">Warren Weaver</a> in 1949,<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> including the ideas of applying <a href="Claude_Shannon" title="wikilink">Claude Shannon</a>'s <a href="information_theory" title="wikilink">information theory</a>. Statistical machine translation was re-introduced in the late 1980s and early 1990s by researchers at <a class="uri" href="IBM" title="wikilink">IBM</a>'s <a href="Thomas_J._Watson_Research_Center" title="wikilink">Thomas J. Watson Research Center</a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> and has contributed to the significant resurgence in interest in machine translation in recent years. Nowadays it is by far the most widely studied machine translation method.</p>
<h2 id="basis">Basis</h2>

<p>The idea behind statistical machine translation comes from <a href="information_theory" title="wikilink">information theory</a>. A document is translated according to the <a href="probability_distribution" title="wikilink">probability distribution</a> 

<math display="inline" id="Statistical_machine_translation:0">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>e</mi>
    <mo stretchy="false">|</mo>
    <mi>f</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">e</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">f</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(e|f)
  </annotation>
 </semantics>
</math>

 that a string 

<math display="inline" id="Statistical_machine_translation:1">
 <semantics>
  <mi>e</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>e</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   e
  </annotation>
 </semantics>
</math>

 in the target language (for example, English) is the translation of a string 

<math display="inline" id="Statistical_machine_translation:2">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 in the source language (for example, French).</p>

<p>The problem of modeling the probability distribution 

<math display="inline" id="Statistical_machine_translation:3">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>e</mi>
    <mo stretchy="false">|</mo>
    <mi>f</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">e</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">f</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(e|f)
  </annotation>
 </semantics>
</math>

 has been approached in a number of ways. One approach which lends itself well to computer implementation is to apply <a href="Bayes_Theorem" title="wikilink">Bayes Theorem</a>, that is 

<math display="inline" id="Statistical_machine_translation:4">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>e</mi>
    <mo stretchy="false">|</mo>
    <mi>f</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>∝</mo>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>f</mi>
    <mo stretchy="false">|</mo>
    <mi>e</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>e</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">e</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">f</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="latexml">proportional-to</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">f</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">e</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">e</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(e|f)\propto p(f|e)p(e)
  </annotation>
 </semantics>
</math>

, where the <a href="translation_model" title="wikilink">translation model</a> 

<math display="inline" id="Statistical_machine_translation:5">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>f</mi>
    <mo stretchy="false">|</mo>
    <mi>e</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">f</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">e</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(f|e)
  </annotation>
 </semantics>
</math>

 is the probability that the source string is the translation of the target string, and the <a href="language_model" title="wikilink">language model</a> 

<math display="inline" id="Statistical_machine_translation:6">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>e</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>p</ci>
    <ci>e</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(e)
  </annotation>
 </semantics>
</math>

 is the probability of seeing that target language string. This decomposition is attractive as it splits the problem into two subproblems. Finding the best translation 

<math display="inline" id="Statistical_machine_translation:7">
 <semantics>
  <mover accent="true">
   <mi>e</mi>
   <mo stretchy="false">~</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-~</ci>
    <ci>e</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tilde{e}
  </annotation>
 </semantics>
</math>

 is done by picking up the one that gives the highest probability:</p>

<p>

<math display="block" id="Statistical_machine_translation:8">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>e</mi>
    <mo stretchy="false">~</mo>
   </mover>
   <mo>=</mo>
   <mi>a</mi>
   <mi>r</mi>
   <mi>g</mi>
   <munder>
    <mi>max</mi>
    <mrow>
     <mi>e</mi>
     <mo>∈</mo>
     <msup>
      <mi>e</mi>
      <mo>*</mo>
     </msup>
    </mrow>
   </munder>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>e</mi>
    <mo stretchy="false">|</mo>
    <mi>f</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>a</mi>
   <mi>r</mi>
   <mi>g</mi>
   <munder>
    <mi>max</mi>
    <mrow>
     <mi>e</mi>
     <mo>∈</mo>
     <msup>
      <mi>e</mi>
      <mo>*</mo>
     </msup>
    </mrow>
   </munder>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>f</mi>
    <mo stretchy="false">|</mo>
    <mi>e</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>e</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <ci>normal-~</ci>
     <ci>e</ci>
    </apply>
    <eq></eq>
    <csymbol cd="unknown">a</csymbol>
    <csymbol cd="unknown">r</csymbol>
    <csymbol cd="unknown">g</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <max></max>
     <apply>
      <in></in>
      <ci>e</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>e</ci>
       <times></times>
      </apply>
     </apply>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">e</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">f</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">a</csymbol>
    <csymbol cd="unknown">r</csymbol>
    <csymbol cd="unknown">g</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <max></max>
     <apply>
      <in></in>
      <ci>e</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>e</ci>
       <times></times>
      </apply>
     </apply>
    </apply>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">f</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">e</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">e</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tilde{e}=arg\max_{e\in e^{*}}p(e|f)=arg\max_{e\in e^{*}}p(f|e)p(e)
  </annotation>
 </semantics>
</math>

.</p>

<p>For a rigorous implementation of this one would have to perform an exhaustive search by going through all strings 

<math display="inline" id="Statistical_machine_translation:9">
 <semantics>
  <msup>
   <mi>e</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>e</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   e^{*}
  </annotation>
 </semantics>
</math>

 in the native language. Performing the search efficiently is the work of a <a href="machine_translation_decoder" title="wikilink">machine translation decoder</a> that uses the foreign string, heuristics and other methods to limit the search space and at the same time keeping acceptable quality. This trade-off between quality and time usage can also be found in <a href="speech_recognition" title="wikilink">speech recognition</a>.</p>

<p>As the translation systems are not able to store all native strings and their translations, a document is typically translated sentence by sentence, but even this is not enough. Language models are typically approximated by <a href="N-gram#Smoothing_techniques" title="wikilink">smoothed <em>n</em>-gram models</a>, and similar approaches have been applied to translation models, but there is additional complexity due to different sentence lengths and word orders in the languages.</p>

<p>The statistical translation models were initially <a class="uri" href="word" title="wikilink">word</a> based (Models 1-5 from <a class="uri" href="IBM" title="wikilink">IBM</a> <a href="Hidden_Markov_model" title="wikilink">Hidden Markov model</a> from Stephan Vogel<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> and Model 6 from Franz-Joseph Och<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a>), but significant advances were made with the introduction of <a class="uri" href="phrase" title="wikilink">phrase</a> based models.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> Recent work has incorporated <a class="uri" href="syntax" title="wikilink">syntax</a> or quasi-syntactic structures.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<h2 id="benefits">Benefits</h2>

<p>The most frequently cited benefits of statistical machine translation over rule-based approach are:</p>
<ul>
<li>Better use of resources
<ul>
<li>There is a great deal of natural language in machine-readable format.</li>
<li>Generally, SMT systems are not tailored to any specific pair of languages.</li>
<li>Rule-based translation systems require the manual development of linguistic rules, which can be costly, and which often do not generalize to other languages.</li>
</ul></li>
<li>More natural translations
<ul>
<li>Rule-based translation systems are likely to result in Literal translation. While it appears that SMT should avoid this problem and result in natural translations, this is counterbalanced by the fact that using statistical matching to translate rather than a dictionary/grammar rules approach can often result in text that include apparently nonsensical and obvious errors.</li>
</ul></li>
</ul>
<h2 id="shortcomings">Shortcomings</h2>
<ul>
<li>Corpus creation can be costly for users with limited resources.</li>
<li>The results are unexpected. Superficial fluency can be deceiving.</li>
<li>Statistical machine translation does not work well between languages that have significantly different word orders (e.g. Japanese and European languages).</li>
<li>The benefits are overemphasized for European languages.</li>
</ul>
<h2 id="word-based-translation">Word-based translation</h2>

<p>In word-based translation, the fundamental unit of translation is a word in some natural language. Typically, the number of words in translated sentences are different, because of compound words, morphology and idioms. The ratio of the lengths of sequences of translated words is called fertility, which tells how many foreign words each native word produces. Necessarily it is assumed by information theory that each covers the same concept. In practice this is not really true. For example, the English word <em>corner</em> can be translated in Spanish by either <em>rincón</em> or <em>esquina</em>, depending on whether it is to mean its internal or external angle.</p>

<p>Simple word-based translation can't translate between languages with different fertility. Word-based translation systems can relatively simply be made to cope with high fertility, but they could map a single word to multiple words, but not the other way about. For example, if we were translating from English to French, each word in English could produce any number of French words— sometimes none at all. But there's no way to group two English words producing a single French word.</p>

<p>An example of a word-based translation system is the freely available <a class="uri" href="GIZA++" title="wikilink">GIZA++</a> package (<a href="GPL" title="wikilink">GPLed</a>), which includes the training program for <a class="uri" href="IBM" title="wikilink">IBM</a> models and HMM model and Model 6.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>

<p>The word-based translation is not widely used today; phrase-based systems are more common. Most phrase-based system are still using GIZA++ to align the corpus. The alignments are used to extract phrases or deduce syntax rules.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> And matching words in bi-text is still a problem actively discussed in the community. Because of the predominance of GIZA++, there are now several distributed implementations of it online.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></p>
<h2 id="phrase-based-translation">Phrase-based translation</h2>

<p>In phrase-based translation, the aim is to reduce the restrictions of word-based translation by translating whole sequences of words, where the lengths may differ. The sequences of words are called blocks or phrases, but typically are not linguistic <a href="phrase" title="wikilink">phrases</a>, but <a href="phraseme" title="wikilink">phrasemes</a> found using statistical methods from corpora. It has been shown that restricting the phrases to linguistic phrases (syntactically motivated groups of words, see <a href="syntactic_categories" title="wikilink">syntactic categories</a>) decreases the quality of translation.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></p>
<h2 id="syntax-based-translation">Syntax-based translation</h2>

<p>Syntax-based translation is based on the idea of translating <a href="syntax_(linguistics)" title="wikilink">syntactic</a> units, rather than single words or strings of words (as in phrase-based MT), i.e. (partial) <a href="parse_tree" title="wikilink">parse trees</a> of sentences/utterances. The idea of syntax-based translation is quite old in MT, though its statistical counterpart did not take off until the advent of strong <a href="stochastic_parsing" title="wikilink">stochastic parsers</a> in the 1990s. Examples of this approach include <a href="Data-oriented_parsing" title="wikilink">DOP</a>-based MT and, more recently, <a href="synchronous_context-free_grammar" title="wikilink">synchronous context-free grammars</a>.</p>
<h2 id="hierarchical-phrase-based-translation">Hierarchical phrase-based translation</h2>

<p>Hierarchical phrase-based translation combines the strengths of phrase-based and syntax-based translation. It uses <a href="synchronous_context-free_grammar" title="wikilink">synchronous context-free grammar</a> rules, but the grammars may be constructed by an extension of methods for phrase-based translation without reference to linguistically motivated syntactic constituents. This idea was first introduced in Chiang's Hiero system (2005).<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></p>
<h2 id="challenges-with-statistical-machine-translation">Challenges with statistical machine translation</h2>

<p>Problems that statistical machine translation have to deal with include:</p>
<h3 id="sentence-alignment">Sentence alignment</h3>

<p>In parallel corpora single sentences in one language can be found translated into several sentences in the other and vice versa. Sentence aligning can be performed through the <a href="Gale-Church_alignment_algorithm" title="wikilink">Gale-Church alignment algorithm</a>.</p>
<h3 id="statistical-anomalies">Statistical anomalies</h3>

<p>Real-world training sets may override translations of, say, proper nouns. An example would be that "I took the train to Berlin" gets mis-translated as "I took the train to Paris" due to an abundance of "train to Paris" in the training set.</p>
<h3 id="data-dilution">Data dilution</h3>

<p>A common anomaly is caused when attempting to construct a new statistical model (engine) to represent a distinct terminology (for a specific corporate brand or domain). Training sets used from alternative sources to the specific brand to compensate for a limited quantity of brand-specific corpora may ‘dilute’ brand terminology, choice of words, text format and style. <a href="http://www.machinetranslation.net/quick-guide-to-machine-translation/statistical-machine-translation-data-dilution-effect">Data dilution</a> is a statistical anomaly unique to a subset of natural language and has shown a negative impact on Machine Translation adoption for commercial use. Various solutions exist that augment statistical MT and optimize translated text to resemble more accurately brand/domain-specific choice of terminology, words and style.</p>
<h3 id="idioms">Idioms</h3>

<p>Depending on the corpora used, idioms may not translate "idiomatically". For example, using Canadian Hansard as the bilingual corpus, "hear" may almost invariably be translated to "Bravo!" since in Parliament "Hear, Hear!" becomes "Bravo!". <a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>
<h3 id="different-word-orders">Different word orders</h3>

<p>Word order in languages differ. Some classification can be done by naming the typical order of subject (S), verb (V) and object (O) in a sentence and one can talk, for instance, of SVO or VSO languages. There are also additional differences in word orders, for instance, where modifiers for nouns are located, or where the same words are used as a question or a statement.</p>

<p>In <a href="speech_recognition" title="wikilink">speech recognition</a>, the speech signal and the corresponding textual representation can be mapped to each other in blocks in order. This is not always the case with the same text in two languages. For SMT, the machine translator can only manage small sequences of words, and word order has to be thought of by the program designer. Attempts at solutions have included re-ordering models, where a distribution of location changes for each item of translation is guessed from aligned bi-text. Different location changes can be ranked with the help of the language model and the best can be selected.</p>
<h3 id="out-of-vocabulary-oov-words">Out of vocabulary (OOV) words</h3>

<p>SMT systems typically store different word forms as separate symbols without any relation to each other and word forms or phrases that were not in the training data cannot be translated. This might be because of the lack of training data, changes in the human domain where the system is used, or differences in morphology.</p>
<h2 id="systems-implementing-statistical-machine-translation">Systems implementing statistical machine translation</h2>

<p><a href="Google_Translate" title="wikilink">Google Translate</a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Apptek" title="wikilink">AppTek</a></li>
<li><a href="Asia_Online" title="wikilink">Asia Online</a></li>
<li><a class="uri" href="KantanMT" title="wikilink">KantanMT</a></li>
<li><a href="Cache_language_model" title="wikilink">Cache language model</a></li>
<li><a href="Example-based_machine_translation" title="wikilink">Example-based machine translation</a></li>
<li><a href="Google_Translate" title="wikilink">Google Translate</a></li>
<li><a href="Machine_translation" title="wikilink">Machine translation</a></li>
<li><a href="Moses_(machine_translation)" title="wikilink">Moses (machine translation)</a>, free software</li>
<li><a href="Language_Weaver" title="wikilink">SDL Language Weaver</a></li>
<li><a class="uri" href="Duolingo" title="wikilink">Duolingo</a></li>
<li><a class="uri" href="Targoman" title="wikilink">Targoman</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.statmt.org/">Statistical Machine Translation</a> — includes introduction to research, conference, corpus and software listings.</li>
<li><a href="http://www.statmt.org/moses/">Moses: a state-of-the-art open source SMT system</a></li>
<li><a href="http://www.asiaonline.net/">Asia Online Language Studio Platform</a></li>
<li><a href="http://www.machinetranslation.net/">A Quick Guide to Machine Translation</a></li>
<li><a href="http://www-nlp.stanford.edu/links/statnlp.html">Annotated list of statistical natural language processing resources</a> — Includes links to freely available statistical machine translation software.</li>
<li><a href="http://code.google.com/p/giza-pp/">GIZA++: Word Alignment Tool</a></li>
<li><a href="http://geek.kyloo.net/software/doku.php/mgiza:overview">MGIZA++/PGIZA++ Parallel Implementations of GIZA++</a></li>
<li><a href="http://www.cunei.org/">Cunei</a> — an open source platform for data-driven machine translation that combines the approaches of <a class="uri" href="SMT" title="wikilink">SMT</a> and <a class="uri" href="EBMT" title="wikilink">EBMT</a></li>
<li><a href="https://github.com/jladcr/Moses-for-Mere-Mortals">Moses for Mere Mortals</a> — open source Linux based system; translation memories integration</li>
<li><a href="http://olanto.org/software">Olanto</a> — an open source platform for statistical machine translation</li>
<li><a href="http://daormar.github.io/thot/">Thot</a> — an open source SMT tool including interactive machine translation and incremental learning</li>
<li><a href="http://sishitra.iti.upv.es/">SiShiTra</a> — A hybrid machine translation engine for Spanish-Catalan translation]</li>
<li><a href="http://www.safaba.com/machine-translation/machine-translation-technologies/statistical-machine-translation/">Statistical MT - Overview</a></li>
<li><a href="http://prhlt.iti.upv.es/content.php?page=software.php">GREAT</a> — Giati and Refx Enhanced via Annotation Techniques]</li>
<li><a href="http://www.garuda.dikti.go.id/jurnal/detil/id/0:168667/q/pengarang:Tanuwijaya%20hansel/offset/0/limit/15">Garuda DIKTI</a> — an open national journal</li>
<li><a href="http://jiki.cs.ui.ac.id/index.php/jiki/article/view/122">JIKI NATIONAL</a> — an open national journal</li>
<li><a href="http://www.ingilizceturkce.gen.tr/">Ceviri</a> — Statistical Machine translation containing 36 languages to translate instantly.</li>
<li><a href="http://targoman.com">Targoman</a> — First Iranian attempt to Statistical machine translation</li>
</ul>

<p><a href="Hybrid_Machine_Translation" title="wikilink">Hybrid Machine Translation</a></p>

<p>"</p>

<p><a href="Category:Machine_translation" title="wikilink">Category:Machine translation</a> <a href="Category:Statistical_natural_language_processing" title="wikilink">Category:Statistical natural language processing</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2">W. Weaver (1955). Translation (1949). In: <em>Machine Translation of Languages</em>, MIT Press, Cambridge, MA.<a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6">S. Vogel, H. Ney and C. Tillmann. 1996. HMM-based Word Alignment in StatisticalTranslation. In COLING ’96: The 16th International Conference on Computational Linguistics, pp. 836-841, Copenhagen, Denmark.<a href="#fnref6">↩</a></li>
<li id="fn7">F. Och and H. Ney. (2003). A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19-51<a href="#fnref7">↩</a></li>
<li id="fn8">P. Koehn, F.J. Och, and D. Marcu (2003). Statistical phrase based translation. In <em>Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT/NAACL)</em>.<a href="#fnref8">↩</a></li>
<li id="fn9">D. Chiang (2005). A Hierarchical Phrase-Based Model for Statistical Machine Translation. In <em>Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)</em>.<a href="#fnref9">↩</a></li>
<li id="fn10"></li>
<li id="fn11">P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. ACL 2007, Demonstration Session, Prague, Czech Republic<a href="#fnref11">↩</a></li>
<li id="fn12">Q. Gao, S. Vogel, "Parallel Implementations of Word Alignment Tool", Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pp. 49-57, June, 2008<a href="#fnref12">↩</a></li>
<li id="fn13">Philipp Koehn, Franz Josef Och, Daniel Marcu: Statistical Phrase-Based Translation (2003)<a href="#fnref13">↩</a></li>
<li id="fn14"></li>
<li id="fn15">W. J. Hutchins and H. Somers. (1992). <em>An Introduction to Machine Translation</em>, 18.3:322. ISBN 978-0-12-362830-5<a href="#fnref15">↩</a></li>
</ol>
</section>
</body>
</html>
