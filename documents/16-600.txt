   Bayesian programming      Bayesian programming   Bayesian programming is a formalism and a methodology to specify probabilistic models and solve problems when all the necessary information is not available.  Edwin T. Jaynes proposed that probability could be considered as an alternative and an extension of logic for rational reasoning with incomplete and uncertain information. In his founding book Probability Theory: The Logic of Science 1 he developed this theory and proposed what he called “the robot,” which was not a physical device, but an inference engine to automate probabilistic reasoning—a kind of Prolog for probability instead of logic. Bayesian programming 2 is a formal and concrete implementation of this "robot".  Bayesian programming may also be seen as an algebraic formalism to specify graphical models such as, for instance, Bayesian networks , dynamic Bayesian networks , Kalman filters or hidden Markov models . Indeed, Bayesian Programming is more general than Bayesian networks and has a power of expression equivalent to probabilistic factor graphs .  Formalism  A Bayesian program is a means of specifying a family of probability distributions.  The constituent elements of a Bayesian program are presented below:      Program   {      Description   {      Specification   (  π  )    {     Variables       Decomposition       Forms              Identification (based on  δ  )              Question           Program   cases    Description   cases    Specification  π   cases  Variables  otherwise  Decomposition  otherwise  Forms  otherwise    otherwise   fragments  Identification (based on  δ  normal-)   otherwise    otherwise  Question  otherwise     \text{Program}\begin{cases}\text{Description}\begin{cases}\text{Specification}%
 (\pi)\begin{cases}\text{Variables}\\
 \text{Decomposition}\\
 \text{Forms}\\
 \end{cases}\\
 \text{Identification (based on }\delta)\end{cases}\\
 \text{Question}\end{cases}      A program is constructed from a description and a question.  A description is constructed using some specification (   π   π   \pi   ) as given by the programmer and an identification or learning process for the parameters not completely specified by the specification, using a data set (   δ   δ   \delta   ).  A specification is constructed from a set of pertinent variables, a decomposition and a set of forms.  Forms are either parametric forms or questions to other Bayesian programs.  A question specifies which probability distribution has to be computed.   Description  The purpose of a description is to specify an effective method of computing a joint probability distribution on a set of variables     {   X  1   ,   X  2   ,  ⋯  ,   X  N   }      subscript  X  1    subscript  X  2   normal-⋯   subscript  X  N     \left\{X_{1},X_{2},\cdots,X_{N}\right\}   given a set of experimental data   δ   δ   \delta   and some specification   π   π   \pi   . This joint distribution is denoted as    P   (   X  1   ∧   X  2   ∧  ⋯  ∧   X  N   ∣  δ  ∧  π  )      fragments  P   fragments  normal-(   subscript  X  1     subscript  X  2    normal-⋯    subscript  X  N   normal-∣  δ   π  normal-)     P\left(X_{1}\wedge X_{2}\wedge\cdots\wedge X_{N}\mid\delta\wedge\pi\right)   .  To specify preliminary knowledge   π   π   \pi   , the programmer must undertake the following:   Define the set of relevant variables     {   X  1   ,   X  2   ,  ⋯  ,   X  N   }      subscript  X  1    subscript  X  2   normal-⋯   subscript  X  N     \left\{X_{1},X_{2},\cdots,X_{N}\right\}   on which the joint distribution is defined.  Decompose the joint distribution (break it into relevant independent or conditional probabilities ).  Define the forms of each of the distributions (e.g., for each variable, one of the list of probability distributions ).   Decomposition  Given a partition    {   X  1   ,   X  2   ,  …  ,   X  N   }      subscript  X  1    subscript  X  2   normal-…   subscript  X  N     \left\{X_{1},X_{2},\ldots,X_{N}\right\}   containing   K   K   K   subsets,   K   K   K   variables are defined     L  1   ,  ⋯  ,   L  K       subscript  L  1   normal-⋯   subscript  L  K     L_{1},\cdots,L_{K}   , each corresponding to one of these subsets. Each variable    L  k     subscript  L  k    L_{k}   is obtained as the conjunction of the variables    {   X   k  1    ,   X   k  2    ,  ⋯  }      subscript  X   subscript  k  1     subscript  X   subscript  k  2    normal-⋯    \left\{X_{k_{1}},X_{k_{2}},\cdots\right\}   belonging to the    k   t  h      superscript  k    t  h     k^{th}   subset. Recursive application of Bayes' theorem leads to:      P   (   X  1   ∧   X  2   ∧  ⋯  ∧   X  N   ∣  δ  ∧  π  )      fragments  P   fragments  normal-(   subscript  X  1     subscript  X  2    normal-⋯    subscript  X  N   normal-∣  δ   π  normal-)     \displaystyle P\left(X_{1}\wedge X_{2}\wedge\cdots\wedge X_{N}\mid\delta\wedge%
 \pi\right)     Conditional independence hypotheses then allow further simplifications. A conditional independence hypothesis for variable    L  k     subscript  L  k    L_{k}   is defined by choosing some variable    X  n     subscript  X  n    X_{n}   among the variables appearing in the conjunction     L   k  -  1    ∧  ⋯  ∧   L  2   ∧   L  1        subscript  L    k  1    normal-⋯   subscript  L  2    subscript  L  1     L_{k-1}\wedge\cdots\wedge L_{2}\wedge L_{1}   , labelling    R  k     subscript  R  k    R_{k}   as the conjunction of these chosen variables and setting:      P   (   L  k   ∣   L   k  -  1    ∧  ⋯  ∧   L  1   ∧  δ  ∧  π  )   =  P   (   L  k   ∣   R  k   ∧  δ  ∧  π  )      fragments  P   fragments  normal-(   subscript  L  k   normal-∣   subscript  L    k  1     normal-⋯    subscript  L  1    δ   π  normal-)    P   fragments  normal-(   subscript  L  k   normal-∣   subscript  R  k    δ   π  normal-)     P\left(L_{k}\mid L_{k-1}\wedge\cdots\wedge L_{1}\wedge\delta\wedge\pi\right)=P%
 \left(L_{k}\mid R_{k}\wedge\delta\wedge\pi\right)     We then obtain:      P   (   X  1   ∧   X  2   ∧  ⋯  ∧   X  N   ∣  δ  ∧  π  )      fragments  P   fragments  normal-(   subscript  X  1     subscript  X  2    normal-⋯    subscript  X  N   normal-∣  δ   π  normal-)     \displaystyle P\left(X_{1}\wedge X_{2}\wedge\cdots\wedge X_{N}\mid\delta\wedge%
 \pi\right)     Such a simplification of the joint distribution as a product of simpler distributions is called a decomposition, derived using the chain rule .  This ensures that each variable appears at the most once on the left of a conditioning bar, which is the necessary and sufficient condition to write mathematically valid decompositions.  Forms  Each distribution    P   (   L  k   ∣   R  k   ∧  δ  ∧  π  )      fragments  P   fragments  normal-(   subscript  L  k   normal-∣   subscript  R  k    δ   π  normal-)     P\left(L_{k}\mid R_{k}\wedge\delta\wedge\pi\right)   appearing in the product is then associated with either a parametric form (i.e., a function     f  μ    (   L  k   )        subscript  f  μ    subscript  L  k     f_{\mu}\left(L_{k}\right)   ) or a question to another Bayesian program    P   (   L  k   ∣   R  k   ∧  δ  ∧  π  )   =  P   (  L  ∣  R  ∧   δ  ^   ∧   π  ^   )      fragments  P   fragments  normal-(   subscript  L  k   normal-∣   subscript  R  k    δ   π  normal-)    P   fragments  normal-(  L  normal-∣  R    normal-^  δ     normal-^  π   normal-)     P\left(L_{k}\mid R_{k}\wedge\delta\wedge\pi\right)=P\left(L\mid R\wedge%
 \widehat{\delta}\wedge\widehat{\pi}\right)   .  When it is a form     f  μ    (   L  k   )        subscript  f  μ    subscript  L  k     f_{\mu}\left(L_{k}\right)   , in general,   μ   μ   \mu   is a vector of parameters that may depend on    R  k     subscript  R  k    R_{k}   or   δ   δ   \delta   or both. Learning takes place when some of these parameters are computed using the data set   δ   δ   \delta   .  An important feature of Bayesian Programming is this capacity to use questions to other Bayesian programs as components of the definition of a new Bayesian program.    P   (   L  k   ∣   R  k   ∧  δ  ∧  π  )      fragments  P   fragments  normal-(   subscript  L  k   normal-∣   subscript  R  k    δ   π  normal-)     P\left(L_{k}\mid R_{k}\wedge\delta\wedge\pi\right)   is obtained by some inferences done by another Bayesian program defined by the specifications    π  ^     normal-^  π    \widehat{\pi}   and the data    δ  ^     normal-^  δ    \widehat{\delta}   . This is similar to calling a subroutine in classical programming and provides an easy way to build hierarchical models .  Question  Given a description (i.e.,    P   (   X  1   ∧   X  2   ∧  ⋯  ∧   X  N   ∣  δ  ∧  π  )      fragments  P   fragments  normal-(   subscript  X  1     subscript  X  2    normal-⋯    subscript  X  N   normal-∣  δ   π  normal-)     P\left(X_{1}\wedge X_{2}\wedge\cdots\wedge X_{N}\mid\delta\wedge\pi\right)   ), a question is obtained by partitioning    {   X  1   ,   X  2   ,  ⋯  ,   X  N   }      subscript  X  1    subscript  X  2   normal-⋯   subscript  X  N     \left\{X_{1},X_{2},\cdots,X_{N}\right\}   into three sets: the searched variables, the known variables and the free variables.  The 3 variables    S  e  a  r  c  h  e  d      S  e  a  r  c  h  e  d    Searched   ,    K  n  o  w  n      K  n  o  w  n    Known   and    F  r  e  e      F  r  e  e    Free   are defined as the conjunction of the variables belonging to these sets.  A question is defined as the set of distributions:      P   (  S  e  a  r  c  h  e  d  ∣  Known  ∧  δ  ∧  π  )      fragments  P   fragments  normal-(  S  e  a  r  c  h  e  d  normal-∣  Known   δ   π  normal-)     P\left(Searched\mid\text{Known}\wedge\delta\wedge\pi\right)     made of many "instantiated questions" as the cardinal of    K  n  o  w  n      K  n  o  w  n    Known   , each instantiated question being the distribution:      P   (  Searched  ∣  Known  ∧  δ  ∧  π  )      fragments  P   fragments  normal-(  Searched  normal-∣  Known   δ   π  normal-)     P\left(\text{Searched}\mid\text{Known}\wedge\delta\wedge\pi\right)     Inference  Given the joint distribution    P   (   X  1   ∧   X  2   ∧  ⋯  ∧   X  N   ∣  δ  ∧  π  )      fragments  P   fragments  normal-(   subscript  X  1     subscript  X  2    normal-⋯    subscript  X  N   normal-∣  δ   π  normal-)     P\left(X_{1}\wedge X_{2}\wedge\cdots\wedge X_{N}\mid\delta\wedge\pi\right)   , it is always possible to compute any possible question using the following general inference:      P   (  Searched  ∣  Known  ∧  δ  ∧  π  )      fragments  P   fragments  normal-(  Searched  normal-∣  Known   δ   π  normal-)     \displaystyle P\left(\text{Searched}\mid\text{Known}\wedge\delta\wedge\pi\right)     where the first equality results from the marginalization rule, the second results from Bayes' theorem and the third corresponds to a second application of marginalization. The denominator appears to be a normalization term and can be replaced by a constant   Z   Z   Z   .  Theoretically, this allows to solve any Bayesian inference problem. In practice, however, the cost of computing exhaustively and exactly    P   (  Searched  ∣  Known  ∧  δ  ∧  π  )      fragments  P   fragments  normal-(  Searched  normal-∣  Known   δ   π  normal-)     P\left(\text{Searched}\mid\text{Known}\wedge\delta\wedge\pi\right)   is too great in almost all cases.  Replacing the joint distribution by its decomposition we get:      P   (  Searched  ∣  Known  ∧  δ  ∧  π  )      fragments  P   fragments  normal-(  Searched  normal-∣  Known   δ   π  normal-)     \displaystyle P\left(\text{Searched}\mid\text{Known}\wedge\delta\wedge\pi\right)     which is usually a much simpler expression to compute, as the dimensionality of the problem is considerably reduced by the decomposition into a product of lower dimension distributions.  Example  Bayesian spam detection  The purpose of Bayesian spam filtering is to eliminate junk e-mails.  The problem is very easy to formulate. E-mails should be classified into one of two categories: non-spam or spam. The only available information to classify the e-mails is their content: a set of words. Using these words without taking the order into account is commonly called a bag of words model .  The classifier should furthermore be able to adapt to its user and to learn from experience. Starting from an initial standard setting, the classifier should modify its internal parameters when the user disagrees with its own decision. It will hence adapt to the user’s criteria to differentiate between non-spam and spam. It will improve its results as it encounters increasingly classified e-mails.  Variables  The variables necessary to write this program are as follows:       S  p  a  m      S  p  a  m    Spam   : a binary variable, false if the e-mail is not spam and true otherwise.       W  0   ,   W  1   ,  …  ,   W   N  -  1        subscript  W  0    subscript  W  1   normal-…   subscript  W    N  1      W_{0},W_{1},\ldots,W_{N-1}      N   N   N   binary variables.    W  n     subscript  W  n    W_{n}   is true if the    n   t  h      superscript  n    t  h     n^{th}   word of the dictionary is present in the text.   These    N  +  1      N  1    N+1   binary variables sum up all the information about an e-mail.  Decomposition  Starting from the joint distribution and applying recursively Bayes' theorem we obtain:      P   (   Spam  ∧   W  0   ∧  ⋯  ∧   W   N  -  1     )       P    Spam   subscript  W  0   normal-⋯   subscript  W    N  1       \displaystyle P(\text{Spam}\wedge W_{0}\wedge\cdots\wedge W_{N-1})     This is an exact mathematical expression.  It can be drastically simplified by assuming that the probability of appearance of a word knowing the nature of the text (spam or not) is independent of the appearance of the other words. This is the naive Bayes assumption and this makes this spam filter a naive Bayes model.  For instance, the programmer can assume that:      P   (   W  1   ∣  Spam  ∧   W  0   )   =  P   (   W  1   ∣  Spam  )      fragments  P   fragments  normal-(   subscript  W  1   normal-∣  Spam    subscript  W  0   normal-)    P   fragments  normal-(   subscript  W  1   normal-∣  Spam  normal-)     P(W_{1}\mid\text{Spam}\land W_{0})=P(W_{1}\mid\text{Spam})     to finally obtain:      P   (  Spam  ∧   W  0   ∧  …  ∧   W   N  -  1    )   =  P   (  Spam  )    ∏   n  =  0    N  -  1     [  P   (   W  n   ∣  Spam  )   ]      fragments  P   fragments  normal-(  Spam    subscript  W  0    normal-…    subscript  W    N  1    normal-)    P   fragments  normal-(  Spam  normal-)    superscript   subscript  product    n  0      N  1     fragments  normal-[  P   fragments  normal-(   subscript  W  n   normal-∣  Spam  normal-)   normal-]     P(\text{Spam}\land W_{0}\land\ldots\land W_{N-1})=P(\text{Spam})\prod_{n=0}^{N%
 -1}[P(W_{n}\mid\text{Spam})]     This kind of assumption is known as the naive Bayes' assumption . It is "naive" in the sense that the independence between words is clearly not completely true. For instance, it completely neglects that the appearance of pairs of words may be more significant than isolated appearances. However, the programmer may assume this hypothesis and may develop the model and the associated inferences to test how reliable and efficient it is.  Parametric forms  To be able to compute the joint distribution, the programmer must now specify the    N  +  1      N  1    N+1   distributions appearing in the decomposition:       P   (  Spam  )       P  Spam    P(\text{Spam})   is a prior defined, for instance, by    P   (   [  Spam  =  1  ]   )   =  0.75     fragments  P   fragments  normal-(   fragments  normal-[  Spam   1  normal-]   normal-)    0.75    P([\text{Spam}=1])=0.75     Each of the   N   N   N   forms    P   (   W  n   ∣  Spam  )      fragments  P   fragments  normal-(   subscript  W  n   normal-∣  Spam  normal-)     P(W_{n}\mid\text{Spam})   may be specified using Laplace rule of succession (this is a pseudocounts-based smoothing technique to counter the zero-frequency problem of words never-seen-before):      P   (   W  n   ∣   [  Spam  =  false  ]   )   =    1  +   a  f  n     2  +   a  f        fragments  P   fragments  normal-(   subscript  W  n   normal-∣   fragments  normal-[  Spam   false  normal-]   normal-)        1   subscript   superscript  a  n   f      2   subscript  a  f       P(W_{n}\mid[\text{Spam}=\text{false}])=\frac{1+a^{n}_{f}}{2+a_{f}}         P   (   W  n   ∣   [  Spam  =  true  ]   )   =    1  +   a  t  n     2  +   a  t        fragments  P   fragments  normal-(   subscript  W  n   normal-∣   fragments  normal-[  Spam   true  normal-]   normal-)        1   subscript   superscript  a  n   t      2   subscript  a  t       P(W_{n}\mid[\text{Spam}=\text{true}])=\frac{1+a^{n}_{t}}{2+a_{t}}       where    a  f  n     subscript   superscript  a  n   f    a^{n}_{f}   stands for the number of appearances of the    n   t  h      superscript  n    t  h     n^{th}   word in non-spam e-mails and    a  f     subscript  a  f    a_{f}   stands for the total number of non-spam e-mails. Similarly,    a  t  n     superscript   subscript  a  t   n    a_{t}^{n}   stands for the number of appearances of the    n   t  h      superscript  n    t  h     n^{th}   word in spam e-mails and    a  t     subscript  a  t    a_{t}   stands for the total number of spam e-mails.  Identification  The   N   N   N   forms    P   (   W  n   ∣  Spam  )      fragments  P   fragments  normal-(   subscript  W  n   normal-∣  Spam  normal-)     P(W_{n}\mid\text{Spam})   are not yet completely specified because the     2  N   +  2        2  N   2    2N+2   parameters    a  f   n  =   0  ,  …  ,   N  -  1        superscript   subscript  a  f     n   0  normal-…    N  1       a_{f}^{n=0,\ldots,N-1}   ,    a  t   n  =   0  ,  …  ,   N  -  1        superscript   subscript  a  t     n   0  normal-…    N  1       a_{t}^{n=0,\ldots,N-1}   ,    a  f     subscript  a  f    a_{f}   and    a  t     subscript  a  t    a_{t}   have no values yet.  The identification of these parameters could be done either by batch processing a series of classified e-mails or by an incremental updating of the parameters using the user's classifications of the e-mails as they arrive.  Both methods could be combined: the system could start with initial standard values of these parameters issued from a generic database, then some incremental learning customizes the classifier to each individual user.  Question  The question asked to the program is: "what is the probability for a given text to be spam knowing which words appear and don't appear in this text?" It can be formalized by:      P   (  Spam  ∣   w  0   ∧  ⋯  ∧   w   N  -  1    )      fragments  P   fragments  normal-(  Spam  normal-∣   subscript  w  0    normal-⋯    subscript  w    N  1    normal-)     P(\text{Spam}\mid w_{0}\wedge\cdots\wedge w_{N-1})     which can be computed as follows:      P   (  Spam  ∣   w  0   ∧  ⋯  ∧   w   N  -  1    )      fragments  P   fragments  normal-(  Spam  normal-∣   subscript  w  0    normal-⋯    subscript  w    N  1    normal-)     \displaystyle P(\text{Spam}\mid w_{0}\wedge\cdots\wedge w_{N-1})     The denominator appears to be a normalization constant . It is not necessary to compute it to decide if we are dealing with spam. For instance, an easy trick is to compute the ratio:        P   (   [  Spam  =  true  ]   ∣   w  0   ∧  ⋯  ∧   w   N  -  1    )     P   (   [  Spam  =  false  ]   ∣   w  0   ∧  ⋯  ∧   w   N  -  1    )          fragments  P   fragments  normal-(   fragments  normal-[  Spam   true  normal-]   normal-∣   subscript  w  0    normal-⋯    subscript  w    N  1    normal-)     fragments  P   fragments  normal-(   fragments  normal-[  Spam   false  normal-]   normal-∣   subscript  w  0    normal-⋯    subscript  w    N  1    normal-)      \displaystyle\frac{P([\text{Spam}=\text{true}]\mid w_{0}\wedge\cdots\wedge w_{%
 N-1})}{P([\text{Spam}=\text{false}]\mid w_{0}\wedge\cdots\wedge w_{N-1})}     This computation is faster and easier because it requires only    2  N      2  N    2N   products.  Bayesian program  The Bayesian spam filter program is completely defined by:      Pr   {      D  s   {      S  p   (  π  )    {       V  a   :   Spam  ,   W  0   ,    W  1   …   W   N  -  1              D  c   :   {      P   (   Spam  ∧   W  0   ∧  …  ∧   W  n   ∧  …  ∧   W   N  -  1     )          =  P   (  Spam  )     ∏   n  =  0    N  -  1     P   (   W  n   ∣  Spam  )                 F  o   :   {       P   (  Spam  )    :   {      P   (   [  Spam  =  false  ]   )   =  0.25         P   (   [  Spam  =  true  ]   )   =  0.75               P   (   W  n   ∣  Spam  )   :   {      P   (   W  n   ∣   [  Spam  =  false  ]   )           =     1  +   a  f  n     2  +   a  f             P   (   W  n   ∣   [  Spam  =  true  ]   )           =     1  +   a  t  n     2  +   a  t                               Identification (based on  δ  )               Q  u  :  P   (  Spam  ∣   w  0   ∧  …  ∧   w  n   ∧  …  ∧   w   N  -  1    )            Pr   cases    D  s   cases    S  p  π   cases   normal-:    V  a    Spam   subscript  W  0      subscript  W  1   normal-…   subscript  W    N  1       otherwise   normal-:    D  c    cases    P    Spam   subscript  W  0   normal-…   subscript  W  n   normal-…   subscript  W    N  1      otherwise   fragments   P   fragments  normal-(  Spam  normal-)    superscript   subscript  product    n  0      N  1    P   fragments  normal-(   subscript  W  n   normal-∣  Spam  normal-)    otherwise    otherwise   normal-:    F  o    cases   normal-:    P  Spam    cases   fragments  P   fragments  normal-(   fragments  normal-[  Spam   false  normal-]   normal-)    0.25   otherwise   fragments  P   fragments  normal-(   fragments  normal-[  Spam   true  normal-]   normal-)    0.75   otherwise    otherwise   fragments  P   fragments  normal-(   subscript  W  n   normal-∣  Spam  normal-)   normal-:   cases   fragments  P   fragments  normal-(   subscript  W  n   normal-∣   fragments  normal-[  Spam   false  normal-]   normal-)    otherwise    absent      1   subscript   superscript  a  n   f      2   subscript  a  f      otherwise   fragments  P   fragments  normal-(   subscript  W  n   normal-∣   fragments  normal-[  Spam   true  normal-]   normal-)    otherwise    absent      1   subscript   superscript  a  n   t      2   subscript  a  t      otherwise    otherwise    otherwise    otherwise   fragments  Identification (based on  δ  normal-)   otherwise    otherwise   fragments  Q  u  normal-:  P   fragments  normal-(  Spam  normal-∣   subscript  w  0    normal-…    subscript  w  n    normal-…    subscript  w    N  1    normal-)    otherwise     \Pr\begin{cases}Ds\begin{cases}Sp(\pi)\begin{cases}Va:\text{Spam},W_{0},W_{1}%
 \ldots W_{N-1}\\
 Dc:\begin{cases}P(\text{Spam}\land W_{0}\land\ldots\land W_{n}\land\ldots\land
 W%
 _{N-1})\\
 =P(\text{Spam})\prod_{n=0}^{N-1}P(W_{n}\mid\text{Spam})\end{cases}\\
 Fo:\begin{cases}P(\text{Spam}):\begin{cases}P([\text{Spam}=\text{false}])=0.25%
 \\
 P([\text{Spam}=\text{true}])=0.75\end{cases}\\
 P(W_{n}\mid\text{Spam}):\begin{cases}P(W_{n}\mid[\text{Spam}=\text{false}])\\
 =\frac{1+a^{n}_{f}}{2+a_{f}}\\
 P(W_{n}\mid[\text{Spam}=\text{true}])\\
 =\frac{1+a^{n}_{t}}{2+a_{t}}\end{cases}\\
 \end{cases}\\
 \end{cases}\\
 \text{Identification (based on }\delta)\end{cases}\\
 Qu:P(\text{Spam}\mid w_{0}\land\ldots\land w_{n}\land\ldots\land w_{N-1})\end{cases}     Bayesian filter, Kalman filter and hidden Markov model  Bayesian filters (often called Recursive Bayesian estimation ) are generic probabilistic models for time evolving processes. Numerous models are particular instances of this generic approach, for instance: the Kalman filter or the Hidden Markov model .  Variables   Variables     S  0   ,  …  ,   S  T       superscript  S  0   normal-…   superscript  S  T     S^{0},\ldots,S^{T}   are a time series of state variables considered to be on a time horizon ranging from   0   0    to   T   T   T   .  Variables     O  0   ,  …  ,   O  T       superscript  O  0   normal-…   superscript  O  T     O^{0},\ldots,O^{T}   are a time series of observation variables on the same horizon.   Decomposition  The decomposition is based:   on    P   (   S  t   ∣   S   t  -  1    )      fragments  P   fragments  normal-(   superscript  S  t   normal-∣   superscript  S    t  1    normal-)     P(S^{t}\mid S^{t-1})   , called the system model, transition model or dynamic model, which formalizes the transition from the state at time    t  -  1      t  1    t-1   to the state at time   t   t   t   ;  on    P   (   O  t   ∣   S  t   )      fragments  P   fragments  normal-(   superscript  O  t   normal-∣   superscript  S  t   normal-)     P(O^{t}\mid S^{t})   , called the observation model, which expresses what can be observed at time   t   t   t   when the system is in state    S  t     superscript  S  t    S^{t}   ;  on an initial state at time   0   0        P   (    S  0   ∧   O  0    )       P     superscript  S  0    superscript  O  0      P(S^{0}\wedge O^{0})   .   Parametrical forms  The parametrical forms are not constrained and different choices lead to different well-known models: see Kalman filters and Hidden Markov models just below.  Question  The question usually asked of these models is    P   (   S   t  +  k    ∣   O  0   ∧  ⋯  ∧   O  t   )      fragments  P   fragments  normal-(   superscript  S    t  k    normal-∣   superscript  O  0    normal-⋯    superscript  O  t   normal-)     P\left(S^{t+k}\mid O^{0}\wedge\cdots\wedge O^{t}\right)   : what is the probability distribution for the state at time    t  +  k      t  k    t+k   knowing the observations from instant   0   0    to   t   t   t   ?  The most common case is Bayesian filtering where    k  =  0      k  0    k=0   , which means that one searches for the present state, knowing the past observations.  However it is also possible to do a prediction    (   k  >  0   )      k  0    (k>0)   , where one tries to extrapolate a future state from past observations, or to do smoothing    (   k  <  0   )      k  0    (k<0)   , where one tries to recover a past state from observations made either before or after that instant.  Some more complicated questions may also be asked as shown below in the HMM section.  Bayesian filters    (   k  =  0   )      k  0    (k=0)   have a very interesting recursive property, which contributes greatly to their attractiveness.    P   (   S  t   |   O  0   ∧  ⋯  ∧   O  t   )      fragments  P   fragments  normal-(   superscript  S  t   normal-|   superscript  O  0    normal-⋯    superscript  O  t   normal-)     P\left(S^{t}|O^{0}\wedge\cdots\wedge O^{t}\right)   may be computed simply from    P   (   S   t  1    ∣   O  0   ∧  ⋯  ∧   O   t  -  1    )      fragments  P   fragments  normal-(   superscript  S    t  1    normal-∣   superscript  O  0    normal-⋯    superscript  O    t  1    normal-)     P\left(S^{t1}\mid O^{0}\wedge\cdots\wedge O^{t-1}\right)   with the following formula:          P   (   S  t   |   O  0   ∧  ⋯  ∧   O  t   )        =     P   (   O  t   |   S  t   )   ×   ∑   S   t  -  1      [  P   (   S  t   |   S   t  -  1    )   ×  P   (   S   t  -  1    |   O  0   ∧  ⋯  ∧   O   t  -  1    )   ]           missing-subexpression    fragments  P   fragments  normal-(   superscript  S  t   normal-|   superscript  O  0    normal-⋯    superscript  O  t   normal-)        fragments  P   fragments  normal-(   superscript  O  t   normal-|   superscript  S  t   normal-)     subscript    superscript  S    t  1      fragments  normal-[  P   fragments  normal-(   superscript  S  t   normal-|   superscript  S    t  1    normal-)    P   fragments  normal-(   superscript  S    t  1    normal-|   superscript  O  0    normal-⋯    superscript  O    t  1    normal-)   normal-]       \begin{array}[]{ll}&P\left(S^{t}|O^{0}\wedge\cdots\wedge O^{t}\right)\\
 =&P\left(O^{t}|S^{t}\right)\times\sum_{S^{t-1}}\left[P\left(S^{t}|S^{t-1}%
 \right)\times P\left(S^{t-1}|O^{0}\wedge\cdots\wedge O^{t-1}\right)\right]\end%
 {array}     Another interesting point of view for this equation is to consider that there are two phases: a prediction phase and an estimation phase:   During the prediction phase, the state is predicted using the dynamic model and the estimation of the state at the previous moment:          \begin{array}{ll}  & P\left(S^{t}|O^{0}\wedge\cdots\wedge O^{t-1}\right)\\  = & \sum_{S^{t-1}}\left[P\left(S^{t}|S^{t-1}\right)\times P\left(S^{t-1}|O^{0}\wedge\cdots\wedge O^{t-1}\right)\right]\end{array}   During the estimation phase, the prediction is either confirmed or invalidated using the last observation:          \begin{align}  & P\left(S^{t}\mid O^{0}\wedge\cdots\wedge O^{t}\right)\\  ={} & P\left(O^{t}\mid S^{t}\right)\times P\left(S^{t}|O^{0}\wedge\cdots\wedge O^{t-1}\right) \end{align}  Bayesian program      P  r   {      D  s   {      S  p   (  π  )    {       V  a   :           S  0   ,  ⋯  ,   S  T   ,   O  0   ,  ⋯  ,   O  T           D  c   :          {         P   (   S  0   ∧  ⋯  ∧   S  T   ∧   O  0   ∧  ⋯  ∧   O  T   |  π  )        =     P   (   S  0   ∧   O  0   )   ×    ∏   t  =  1   T     [  P   (   S  t   |   S   t  -  1    )   ×  P   (   O  t   |   S  t   )   ]               F  o   :          {      P   (    S  0   ∧   O  0    )          P   (   S  t   |   S   t  -  1    )          P   (   O  t   |   S  t   )                     I  d                Q  u   :          {         P   (   S   t  +  k    |   O  0   ∧  ⋯  ∧   O  t   )          (  k  =  0  )   ≡  Filtering         (  k  >  0  )   ≡  Prediction         (  k  <  0  )   ≡  Smoothing                    P  r   cases    D  s   cases    S  p  π   cases   normal-:    V  a   absent   otherwise    superscript  S  0   normal-⋯   superscript  S  T    superscript  O  0   normal-⋯   superscript  O  T    otherwise   normal-:    D  c   absent   otherwise   cases  absent   fragments  P   fragments  normal-(   superscript  S  0    normal-⋯    superscript  S  T     superscript  O  0    normal-⋯    superscript  O  T   normal-|  π  normal-)      fragments  P   fragments  normal-(   superscript  S  0     superscript  O  0   normal-)     superscript   subscript  product    t  1    T    fragments  normal-[  P   fragments  normal-(   superscript  S  t   normal-|   superscript  S    t  1    normal-)    P   fragments  normal-(   superscript  O  t   normal-|   superscript  S  t   normal-)   normal-]     otherwise   normal-:    F  o   absent   otherwise   cases    P     superscript  S  0    superscript  O  0     otherwise   fragments  P   fragments  normal-(   superscript  S  t   normal-|   superscript  S    t  1    normal-)    otherwise   fragments  P   fragments  normal-(   superscript  O  t   normal-|   superscript  S  t   normal-)    otherwise   otherwise    otherwise    I  d   otherwise    otherwise   normal-:    Q  u   absent   otherwise   cases     fragments  P   fragments  normal-(   superscript  S    t  k    normal-|   superscript  O  0    normal-⋯    superscript  O  t   normal-)       fragments   fragments  normal-(  k   0  normal-)    Filtering      fragments   fragments  normal-(  k   0  normal-)    Prediction      fragments   fragments  normal-(  k   0  normal-)    Smoothing     otherwise   otherwise     Pr\begin{cases}Ds\begin{cases}Sp(\pi)\begin{cases}Va:\\
 S^{0},\cdots,S^{T},O^{0},\cdots,O^{T}\\
 Dc:\\
 \begin{cases}&P\left(S^{0}\wedge\cdots\wedge S^{T}\wedge O^{0}\wedge\cdots%
 \wedge O^{T}|\pi\right)\\
 =&P\left(S^{0}\wedge O^{0}\right)\times\prod_{t=1}^{T}\left[P\left(S^{t}|S^{t-%
 1}\right)\times P\left(O^{t}|S^{t}\right)\right]\end{cases}\\
 Fo:\\
 \begin{cases}P\left(S^{0}\wedge O^{0}\right)\\
 P\left(S^{t}|S^{t-1}\right)\\
 P\left(O^{t}|S^{t}\right)\end{cases}\end{cases}\\
 Id\end{cases}\\
 Qu:\\
 \begin{cases}\begin{array}[]{l}P\left(S^{t+k}|O^{0}\wedge\cdots\wedge O^{t}%
 \right)\\
 \left(k=0\right)\equiv\text{Filtering}\\
 \left(k>0\right)\equiv\text{Prediction}\\
 \left(k<0\right)\equiv\text{Smoothing}\end{array}\end{cases}\end{cases}     Kalman filter  The very well-known Kalman filters 3 are a special case of Bayesian filters.  They are defined by the following Bayesian program:      P  r   {      D  s   {      S  p   (  π  )    {       V  a   :           S  0   ,  ⋯  ,   S  T   ,   O  0   ,  ⋯  ,   O  T           D  c   :          {         P   (   S  0   ∧  ⋯  ∧   O  T   |  π  )        =     [      P   (   S  0   ∧   O  0   |  π  )           ∏   t  =  1   T     [  P   (   S  t   |   S   t  -  1    ∧  π  )   ×  P   (   O  t   |   S  t   ∧  π  )   ]       ]              F  o   :          {      P   (   S  t   ∣   S   t  -  1    ∧  π  )   ≡  G   (   S  t   ,  A  ∙   S   t  -  1    ,  Q  )          P   (   O  t   ∣   S  t   ∧  π  )   ≡  G   (   O  t   ,  H  ∙   S  t   ,  R  )                     I  d                Q  u   :          P   (   S  T   ∣   O  0   ∧  ⋯  ∧   O  T   ∧  π  )             P  r   cases    D  s   cases    S  p  π   cases   normal-:    V  a   absent   otherwise    superscript  S  0   normal-⋯   superscript  S  T    superscript  O  0   normal-⋯   superscript  O  T    otherwise   normal-:    D  c   absent   otherwise   cases  absent   fragments  P   fragments  normal-(   superscript  S  0    normal-⋯    superscript  O  T   normal-|  π  normal-)      delimited-[]     fragments  P   fragments  normal-(   superscript  S  0     superscript  O  0   normal-|  π  normal-)       fragments   superscript   subscript  product    t  1    T    fragments  normal-[  P   fragments  normal-(   superscript  S  t   normal-|   superscript  S    t  1     π  normal-)    P   fragments  normal-(   superscript  O  t   normal-|   superscript  S  t    π  normal-)   normal-]        otherwise   normal-:    F  o   absent   otherwise   cases   fragments  P   fragments  normal-(   superscript  S  t   normal-∣   superscript  S    t  1     π  normal-)    G   fragments  normal-(   superscript  S  t   normal-,  A  normal-∙   superscript  S    t  1    normal-,  Q  normal-)    otherwise   fragments  P   fragments  normal-(   superscript  O  t   normal-∣   superscript  S  t    π  normal-)    G   fragments  normal-(   superscript  O  t   normal-,  H  normal-∙   superscript  S  t   normal-,  R  normal-)    otherwise   otherwise    otherwise    I  d   otherwise    otherwise   normal-:    Q  u   absent   otherwise   fragments  P   fragments  normal-(   superscript  S  T   normal-∣   superscript  O  0    normal-⋯    superscript  O  T    π  normal-)    otherwise     Pr\begin{cases}Ds\begin{cases}Sp(\pi)\begin{cases}Va:\\
 S^{0},\cdots,S^{T},O^{0},\cdots,O^{T}\\
 Dc:\\
 \begin{cases}&P\left(S^{0}\wedge\cdots\wedge O^{T}|\pi\right)\\
 =&\left[\begin{array}[]{c}P\left(S^{0}\wedge O^{0}|\pi\right)\\
 \prod_{t=1}^{T}\left[P\left(S^{t}|S^{t-1}\wedge\pi\right)\times P\left(O^{t}|S%
 ^{t}\wedge\pi\right)\right]\end{array}\right]\end{cases}\\
 Fo:\\
 \begin{cases}P\left(S^{t}\mid S^{t-1}\wedge\pi\right)\equiv G\left(S^{t},A%
 \bullet S^{t-1},Q\right)\\
 P\left(O^{t}\mid S^{t}\wedge\pi\right)\equiv G\left(O^{t},H\bullet S^{t},R%
 \right)\end{cases}\end{cases}\\
 Id\end{cases}\\
 Qu:\\
 P\left(S^{T}\mid O^{0}\wedge\cdots\wedge O^{T}\wedge\pi\right)\end{cases}      Variables are continuous.  The transition model    P   (   S  t   ∣   S   t  -  1    ∧  π  )      fragments  P   fragments  normal-(   superscript  S  t   normal-∣   superscript  S    t  1     π  normal-)     P(S^{t}\mid S^{t-1}\wedge\pi)   and the observation model    P   (   O  t   ∣   S  t   ∧  π  )      fragments  P   fragments  normal-(   superscript  O  t   normal-∣   superscript  S  t    π  normal-)     P(O^{t}\mid S^{t}\wedge\pi)   are both specified using Gaussian laws with means that are linear functions of the conditioning variables.   With these hypotheses and by using the recursive formula, it is possible to solve the inference problem analytically to answer the usual    P   (   S  T   ∣   O  0   ∧  ⋯  ∧   O  T   ∧  π  )      fragments  P   fragments  normal-(   superscript  S  T   normal-∣   superscript  O  0    normal-⋯    superscript  O  T    π  normal-)     P(S^{T}\mid O^{0}\wedge\cdots\wedge O^{T}\wedge\pi)   question. This leads to an extremely efficient algorithm, which explains the popularity of Kalman filters and the number of their everyday applications.  When there are no obvious linear transition and observation models, it is still often possible, using a first-order Taylor's expansion, to treat these models as locally linear. This generalization is commonly called the extended Kalman filter .  Hidden Markov model  Hidden Markov models (HMMs) are another very popular specialization of Bayesian filters.  They are defined by the following Bayesian program:      Pr   {      D  s   {      S  p   (  π  )    {       V  a   :           S  0   ,  …  ,   S  T   ,   O  0   ,  …  ,   O  T           D  c   :          {         P   (   S  0   ∧  ⋯  ∧   O  T   ∣  π  )        =     [      P   (   S  0   ∧   O  0   ∣  π  )           ∏   t  =  1   T     [  P   (   S  t   ∣   S   t  -  1    ∧  π  )   ×  P   (   O  t   ∣   S  t   ∧  π  )   ]       ]              F  o   :          {      P   (   S  0   ∧   O  0   ∣  π  )   ≡  Matrix         P   (   S  t   ∣   S   t  -  1    ∧  π  )   ≡  Matrix         P   (   O  t   ∣   S  t   ∧  π  )   ≡  Matrix                    I  d                Q  u   :           max    S  1   ∧  ⋯  ∧   S   T  -  1       [  P   (   S  1   ∧  ⋯  ∧   S   T  -  1    ∣   S  T   ∧   O  0   ∧  ⋯  ∧   O  T   ∧  π  )   ]            Pr   cases    D  s   cases    S  p  π   cases   normal-:    V  a   absent   otherwise    superscript  S  0   normal-…   superscript  S  T    superscript  O  0   normal-…   superscript  O  T    otherwise   normal-:    D  c   absent   otherwise   cases  absent   fragments  P   fragments  normal-(   superscript  S  0    normal-⋯    superscript  O  T   normal-∣  π  normal-)      delimited-[]     fragments  P   fragments  normal-(   superscript  S  0     superscript  O  0   normal-∣  π  normal-)       fragments   superscript   subscript  product    t  1    T    fragments  normal-[  P   fragments  normal-(   superscript  S  t   normal-∣   superscript  S    t  1     π  normal-)    P   fragments  normal-(   superscript  O  t   normal-∣   superscript  S  t    π  normal-)   normal-]        otherwise   normal-:    F  o   absent   otherwise   cases   fragments  P   fragments  normal-(   superscript  S  0     superscript  O  0   normal-∣  π  normal-)    Matrix   otherwise   fragments  P   fragments  normal-(   superscript  S  t   normal-∣   superscript  S    t  1     π  normal-)    Matrix   otherwise   fragments  P   fragments  normal-(   superscript  O  t   normal-∣   superscript  S  t    π  normal-)    Matrix   otherwise   otherwise    otherwise    I  d   otherwise    otherwise   normal-:    Q  u   absent   otherwise   fragments   subscript      superscript  S  1   normal-⋯   superscript  S    T  1       fragments  normal-[  P   fragments  normal-(   superscript  S  1    normal-⋯    superscript  S    T  1    normal-∣   superscript  S  T     superscript  O  0    normal-⋯    superscript  O  T    π  normal-)   normal-]    otherwise     \Pr\begin{cases}Ds\begin{cases}Sp(\pi)\begin{cases}Va:\\
 S^{0},\ldots,S^{T},O^{0},\ldots,O^{T}\\
 Dc:\\
 \begin{cases}&P\left(S^{0}\wedge\cdots\wedge O^{T}\mid\pi\right)\\
 =&\left[\begin{array}[]{c}P\left(S^{0}\wedge O^{0}\mid\pi\right)\\
 \prod_{t=1}^{T}\left[P\left(S^{t}\mid S^{t-1}\wedge\pi\right)\times P\left(O^{%
 t}\mid S^{t}\wedge\pi\right)\right]\end{array}\right]\end{cases}\\
 Fo:\\
 \begin{cases}P\left(S^{0}\wedge O^{0}\mid\pi\right)\equiv\text{Matrix}\\
 P\left(S^{t}\mid S^{t-1}\wedge\pi\right)\equiv\text{Matrix}\\
 P\left(O^{t}\mid S^{t}\wedge\pi\right)\equiv\text{Matrix}\end{cases}\end{cases%
 }\\
 Id\end{cases}\\
 Qu:\\
 \max_{S^{1}\wedge\cdots\wedge S^{T-1}}\left[P\left(S^{1}\wedge\cdots\wedge S^{%
 T-1}\mid S^{T}\wedge O^{0}\wedge\cdots\wedge O^{T}\wedge\pi\right)\right]\end{cases}      Variables are treated as being discrete.  The transition model    P   (   S  t   ∣   S   t  -  1    ∧  π  )      fragments  P   fragments  normal-(   superscript  S  t   normal-∣   superscript  S    t  1     π  normal-)     P\left(S^{t}\mid S^{t-1}\wedge\pi\right)   and the observation model    P   (   O  t   ∣   S  t   ∧  π  )      fragments  P   fragments  normal-(   superscript  O  t   normal-∣   superscript  S  t    π  normal-)     P\left(O^{t}\mid S^{t}\wedge\pi\right)   are   both specified using probability matrices.   The question most frequently asked of HMMs is:          \max_{S^{1}\wedge\cdots\wedge S^{T-1}}\left[P\left(S^{1}\wedge\cdots\wedge S^{T-1}\mid S^{T}\wedge O^{0}\wedge\cdots\wedge O^{T}\wedge\pi\right)\right]  What is the most probable series of states that leads to the present state, knowing the past observations?  This particular question may be answered with a specific and very efficient algorithm called the Viterbi algorithm .  A specific learning algorithm called the Baum–Welch algorithm has also been developed for HMMs.  Applications  Academic applications  For the last 15 years, Bayesian programming approach has been used in various universities to develop both robotics applications and life sciences models. 4  Robotics  In robotics, Bayesian programming has been applied to autonomous robotics , 5 6 7 8 9 robotic CAD systems, 10  Advanced driver assistance systems , 11 robotic arm control, mobile robotics , 12 13 Human-robots interactions, 14 Human-vehicle interactions (Bayesian autonomous driver models) 15  16  17  18  19  20 video game avatar programming and training 21 and real-time strategy games (AI). 22  Life sciences  In life sciences, Bayesian Programming has been used in vision to reconstruct shape from motion, 23 to model visuo-vestibular interaction 24 and to study saccadic eye movements; 25 in speech perception and control to study early acquisition of speech 26 and the emergence of articulatory-acoustic systems; 27 and to model handwriting perception and control. 28  Bayesian programming versus possibility theories  The comparison between probabilistic approaches (not only Bayesian programming) and possibility theories has been debated for a long time and is, unfortunately, a very controversial matter.  Possibility theories like, for instance, fuzzy sets , 29  Fuzzy logic 30 and Possibility theory 31 propose different alternatives to probability to model uncertainty. They argue that probability is insufficient or inconvenient to model certain aspects of incomplete and uncertain knowledge.  The defense of probability is mainly based on Cox's theorem which, starting from four postulates concerning rational reasoning in the presence of uncertainty, demonstrates that the only mathematical framework that satisfies these postulates is probability theory. The argument then goes like this: if you use a different approach than probability, then you necessarily infringe on one of these postulates. Let us see which one and discuss its utility.  Bayesian programming versus probabilistic programming  The purpose of probabilistic programming is to unify the scope of classical programming languages with probabilistic modeling (especially Bayesian networks ) in order to be able to deal with uncertainty but still profit from the power of expression of programming languages to describe complex models.  The extended classical programming languages can be logical languages as proposed in Probabilistic Horn Abduction, 32 Independent Choice Logic, 33 PRISM, 34 and ProbLog which propose an extension of Prolog.  It can also be extensions of functional programming languages (essentially Lisp and Scheme ) such as IBAL or CHURCH. The inspiring programming languages can even be object oriented like in BLOG and FACTORIE or more standard ones like in CES and FIGARO .  The purpose of Bayesian programming is different. Jaynes' precept of "probability as logic" defends that probability is an extension of and an alternative to logic above which a complete theory of rationality, computation and programming can be rebuilt. Bayesian programming does not search to extend classical languages but rather to replace them by a new programming approach based on probability and taking fully into account incompleteness and uncertainty .  The precise comparison between the semantic and power of expression of Bayesian and probabilistic programming is still an open question.  See also  References  Further reading     External links   A companion site to the Bayesian programming book where to download ProBT an inference engine dedicated to Bayesian programming.  The Bayesian-programming.org site for the promotion of Bayesian programming with detailed information and numerous publications.   "  Category:Bayesian statistics  Category:Probability theory  Category:Artificial intelligence     ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩     