   Nested sampling algorithm      Nested sampling algorithm   The nested sampling algorithm is a computational approach to the problem of comparing models in Bayesian statistics , developed in 2004 by physicist John Skilling. 1  Background  Bayes' theorem can be applied to a pair of competing models    M  1      M  1    M1   and    M  2      M  2    M2   for data   D   D   D   , one of which may be true (though which one is not known) but which both cannot simultaneously be true. The posterior probability for    M  1      M  1    M1   may be calculated as follows:      P   (  M  1  |  D  )      fragments  P   fragments  normal-(  M  1  normal-|  D  normal-)     \displaystyle P(M1|D)     Given no a priori information in favor of    M  1      M  1    M1   or    M  2      M  2    M2   , it is reasonable to assign prior probabilities     P   (   M  1   )    =   P   (   M  2   )    =   1  /  2           P    M  1      P    M  2           1  2      P(M1)=P(M2)=1/2   , so that       P   (   M  2   )    /  P    (   M  1   )    =  1            P    M  2    P     M  1    1    P(M2)/P(M1)=1   . The remaining Bayes factor     P   (  D  |  M  2  )   /  P   (  D  |  M  1  )      fragments  P   fragments  normal-(  D  normal-|  M  2  normal-)    P   fragments  normal-(  D  normal-|  M  1  normal-)     P(D|M2)/P(D|M1)   is not so easy to evaluate since in general it requires marginalization of nuisance parameters. Generally,    M  1      M  1    M1   has a collection of parameters that can be lumped together and called   θ   θ   \theta   , and    M  2      M  2    M2   has its own vector of parameters that may be of different dimensionality but is still referred to as   θ   θ   \theta   . The marginalization for    M  1      M  1    M1   is      P   (  D  |  M  1  )   =  ∫  d  θ  P   (  D  |  θ  ,  M  1  )   P   (  θ  |  M  1  )      fragments  P   fragments  normal-(  D  normal-|  M  1  normal-)     d  θ  P   fragments  normal-(  D  normal-|  θ  normal-,  M  1  normal-)   P   fragments  normal-(  θ  normal-|  M  1  normal-)     P(D|M1)=\int d\theta P(D|\theta,M1)P(\theta|M1)     and likewise for    M  2      M  2    M2   . This integral is often analytically intractable, and in these cases it is necessary to employ a numerical algorithm to find an approximation. The nested sampling algorithm was developed by John Skilling specifically to approximate these marginalization integrals, and it has the added benefit of generating samples from the posterior distribution    P   (  θ  |  D  ,  M  1  )      fragments  P   fragments  normal-(  θ  normal-|  D  normal-,  M  1  normal-)     P(\theta|D,M1)   . 2 It is an alternative to methods from the Bayesian literature 3 such as bridge sampling and defensive importance sampling.  Here is a simple version of the nested sampling algorithm, followed by a description of how it computes the marginal probability density    Z  =  P   (  D  |  M  )      fragments  Z   P   fragments  normal-(  D  normal-|  M  normal-)     Z=P(D|M)   where   M   M   M   is    M  1      M  1    M1   or    M  2      M  2    M2   :  Start with    N   N   N    points      θ  1   ,  …  ,   θ  N       subscript  θ  1   normal-…   subscript  θ  N     \theta_{1},...,\theta_{N}    sampled from prior.  for     i  =  1      i  1    i=1    to    j   j   j    do        % The number of iterations j is chosen by guesswork.        L  i   :=  min  (     fragments   subscript  L  i   assign   normal-(    L_{i}:=\min(    current likelihood values of the points    )   normal-)   )    ;         X  i   :=   exp   (   -   i  /  N    )     ;     assign   subscript  X  i         i  N       X_{i}:=\exp(-i/N);          w  i   :=    X   i  -  1    -   X  i       assign   subscript  w  i      subscript  X    i  1     subscript  X  i      w_{i}:=X_{i-1}-X_{i}        Z  :=   Z  +    L  i   *   w  i      ;     assign  Z    Z     subscript  L  i    subscript  w  i       Z:=Z+L_{i}*w_{i};     Save the point with least likelihood as a sample point with weight     w  i     subscript  w  i    w_{i}    .  Update the point with least likelihood with some Markov Chain  Monte Carlo steps according to the prior, accepting only steps that  keep the likelihood above     L  i     subscript  L  i    L_{i}    .  end  return    Z   Z   Z    ;  At each iteration,    X  i     subscript  X  i    X_{i}   is an estimate of the amount of prior mass covered by the hypervolume in parameter space of all points with likelihood greater than    θ  i     subscript  θ  i    \theta_{i}   . The weight factor    w  i     subscript  w  i    w_{i}   is an estimate of the amount of prior mass that lies between two nested hypersurfaces    {  θ  |  P   (  D  |  θ  ,  M  )   =  P   (  D  |   θ   i  -  1    ,  M  )   }     fragments  normal-{  θ  normal-|  P   fragments  normal-(  D  normal-|  θ  normal-,  M  normal-)    P   fragments  normal-(  D  normal-|   subscript  θ    i  1    normal-,  M  normal-)   normal-}    \{\theta|P(D|\theta,M)=P(D|\theta_{i-1},M)\}   and    {  θ  |  P   (  D  |  θ  ,  M  )   =  P   (  D  |   θ  i   ,  M  )   }     fragments  normal-{  θ  normal-|  P   fragments  normal-(  D  normal-|  θ  normal-,  M  normal-)    P   fragments  normal-(  D  normal-|   subscript  θ  i   normal-,  M  normal-)   normal-}    \{\theta|P(D|\theta,M)=P(D|\theta_{i},M)\}   . The update step    Z  :=   Z  +    L  i   *   w  i        assign  Z    Z     subscript  L  i    subscript  w  i       Z:=Z+L_{i}*w_{i}   computes the sum over   i   i   i   of     L  i   *   w  i        subscript  L  i    subscript  w  i     L_{i}*w_{i}   to numerically approximate the integral         P   (  D  |  M  )      =     ∫  P   (  D  |  θ  ,  M  )   P   (  θ  |  M  )   d  θ        =     ∫  P   (  D  |  θ  ,  M  )   d  P   (  θ  |  M  )           fragments  P   fragments  normal-(  D  normal-|  M  normal-)      fragments   P   fragments  normal-(  D  normal-|  θ  normal-,  M  normal-)   P   fragments  normal-(  θ  normal-|  M  normal-)   d  θ      missing-subexpression     fragments   P   fragments  normal-(  D  normal-|  θ  normal-,  M  normal-)   d  P   fragments  normal-(  θ  normal-|  M  normal-)       \begin{array}[]{lcl}P(D|M)&=&\int P(D|\theta,M)P(\theta|M)d\theta\\
 &=&\int P(D|\theta,M)dP(\theta|M)\\
 \end{array}     The idea is to chop up the range of    f   (  θ  )   =  P   (  D  |  θ  ,  M  )      fragments  f   fragments  normal-(  θ  normal-)    P   fragments  normal-(  D  normal-|  θ  normal-,  M  normal-)     f(\theta)=P(D|\theta,M)   and estimate, for each interval    [   f   (   θ   i  -  1    )    ,   f   (   θ  i   )    ]       f   subscript  θ    i  1       f   subscript  θ  i      [f(\theta_{i-1}),f(\theta_{i})]   , how likely it is a priori that a randomly chosen   θ   θ   \theta   would map to this interval. This can be thought of as a Bayesian's way to numerically implement Lebesgue integration .  Implementations   Simple example code written in C , R , or Python demonstrating this algorithm can be downloaded from John Skilling's website  There is also a Haskell port of the above simple codes on Hackage  An implementation in R originally designed for fitting of spectra is described at 1 and can be obtained on GitHub at 2  A highly modular Python parallel implementation of Nested Sampling for statistical physics and condensed matter physics applications is publicly available from GitHub 3 .   Applications  Since nested sampling was proposed in 2004, it has been used in multiple settings within the field of astronomy . One paper suggested using nested sampling for cosmological  model selection and object detection, as it "uniquely combines accuracy, general applicability and computational feasibility." 4 A refinement of the nested sampling algorithm to handle multimodal posteriors has also been suggested as a means of detecting astronomical objects in existing datasets. 5 Other applications of nested sampling is in the field of finite element updating where nested sampling is used to choose an optimal finite element model and this was applied to structural dynamics . 6  See also   Bayesian model comparison   References  "  Category:Bayesian statistics  Category:Model selection  Category:Statistical algorithms     ↩  ↩  ↩  ↩  ↩  Mthembu, L., Marwala, T., Friswell, M.I. and Adhikari, S. 2011. Model selection in finite element model updating using the Bayesian evidence statistic.Mechanical Systems and Signal Processing, 25(7), 2399– 2412. ↩     