<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="572">Unit-weighted regression</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Unit-weighted regression</h1>
<hr/>

<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, <strong>unit-weighted regression</strong> is a simplified and <a href="robust_statistics" title="wikilink">robust</a> version (<a href="Howard_Wainer" title="wikilink">Wainer</a> &amp; Thissen, 1976) of <a href="multiple_regression" title="wikilink">multiple regression</a> analysis where only the intercept term is estimated. That is, it fits a model</p>

<p>

<math display="block" id="Unit-weighted_regression:0">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>y</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mo>=</mo>
   <mrow>
    <mover accent="true">
     <mi>f</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>ùê±</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mover accent="true">
     <mi>b</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mo>+</mo>
    <mrow>
     <munder>
      <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
      <mi>i</mi>
     </munder>
     <msub>
      <mi>x</mi>
      <mi>i</mi>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <ci>normal-^</ci>
      <ci>y</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <ci>normal-^</ci>
       <ci>f</ci>
      </apply>
      <ci>ùê±</ci>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <plus></plus>
      <apply>
       <ci>normal-^</ci>
       <ci>b</ci>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <ci>i</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{y}=\hat{f}(\mathbf{x})=\hat{b}+\sum_{i}x_{i}
  </annotation>
 </semantics>
</math>

</p>

<p>where each of the 

<math display="inline" id="Unit-weighted_regression:1">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}
  </annotation>
 </semantics>
</math>

 are binary variables, perhaps multiplied with an arbitrary weight.</p>

<p>Contrast this with the more common multiple regression model, where each predictor has its own estimated coefficient:</p>

<p>

<math display="block" id="Unit-weighted_regression:2">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>y</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mo>=</mo>
   <mrow>
    <mover accent="true">
     <mi>f</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>ùê±</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mover accent="true">
     <mi>b</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mo>+</mo>
    <mrow>
     <munder>
      <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
      <mi>i</mi>
     </munder>
     <mrow>
      <msub>
       <mover accent="true">
        <mi>w</mi>
        <mo stretchy="false">^</mo>
       </mover>
       <mi>i</mi>
      </msub>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <ci>normal-^</ci>
      <ci>y</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <ci>normal-^</ci>
       <ci>f</ci>
      </apply>
      <ci>ùê±</ci>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <plus></plus>
      <apply>
       <ci>normal-^</ci>
       <ci>b</ci>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <ci>i</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <apply>
          <ci>normal-^</ci>
          <ci>w</ci>
         </apply>
         <ci>i</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{y}=\hat{f}(\mathbf{x})=\hat{b}+\sum_{i}\hat{w}_{i}x_{i}
  </annotation>
 </semantics>
</math>

</p>

<p>In the <a href="social_science" title="wikilink">social sciences</a>, unit-weighted regression is sometimes used for <a href="statistical_classification" title="wikilink">classification</a> purposes, i.e. to <a href="binary_classification" title="wikilink">predict a yes-no answer</a> where 

<math display="inline" id="Unit-weighted_regression:3">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>y</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mo><</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <lt></lt>
    <apply>
     <ci>normal-^</ci>
     <ci>y</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{y}<0
  </annotation>
 </semantics>
</math>

 indicates "no", 

<math display="inline" id="Unit-weighted_regression:4">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>y</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mo>‚â•</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <geq></geq>
    <apply>
     <ci>normal-^</ci>
     <ci>y</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{y}\geq 0
  </annotation>
 </semantics>
</math>

 "yes". It is easier to interpret than multiple linear regression (known as <a href="linear_discriminant_analysis" title="wikilink">linear discriminant analysis</a> in the classification case).</p>
<h2 id="unit-weights">Unit weights</h2>

<p>Unit-weighted regression is a method of <a href="robust_regression" title="wikilink">robust regression</a> that proceeds in three steps. First, predictors for the outcome of interest are selected; ideally, there should be good empirical or theoretical reasons for the selection. Second, the predictors are converted to a standard form. Finally, the predictors are added together, and this sum is called the variate, which is used as the predictor of the outcome.</p>
<h3 id="burgess-method">Burgess method</h3>

<p>The Burgess method was first presented by the sociologist <a href="Ernest_W._Burgess" title="wikilink">Ernest W. Burgess</a> in a 1928 study to determine success or failure of inmates placed on parole. First, he selected 21 variables believed to be associated with parole success. Next, he converted each predictor to the standard form of zero or one (Burgess, 1928). When predictors had two values, the value associated with the target outcome was coded as one. Burgess selected success on parole as the target outcome, so a predictor such as a <em>history of theft</em> was coded as ‚Äúyes‚Äù = 0 and ‚Äúno‚Äù = 1. These coded values were then added to create a predictor score, so that higher scores predicted a better chance of success. The scores could possibly range from zero (no predictors of success) to 21 (all 21 predictors scored as predicting success).</p>

<p>For predictors with more than two values, the Burgess method selects a cutoff score based on subjective judgment. As an example, a study using the Burgess method (Gottfredson &amp; Snyder, 2005) selected as one predictor the number of complaints for delinquent behavior. With failure on parole as the target outcome, the number of complaints was coded as follows: ‚Äúzero to two complaints‚Äù = 0, and ‚Äúthree or more complaints‚Äù = 1 (Gottfredson &amp; Snyder, 2005. p. 18).</p>
<h3 id="kerby-method">Kerby method</h3>

<p>The Kerby method is similar to the Burgess method, but differs in two ways. First, while the Burgess method uses subjective judgment to select a cutoff score for a multi-valued predictor with a binary outcome, the Kerby method uses classification and regression tree (<a href="decision_tree_learning" title="wikilink">CART</a>) analysis. In this way, the selection of the cutoff score is based not on subjective judgment, but on a statistical criterion, such as the point where the chi-square value is a maximum.</p>

<p>The second difference is that while the Burgess method is applied to a binary outcome, the Kerby method can apply to a multi-valued outcome, because CART analysis can identify cutoff scores in such cases, using a criterion such as the point where the t-value is a maximum. Because CART analysis is not only binary, but also recursive, the result can be that a predictor variable will be divided again, yielding two cutoff scores. The standard form for each predictor is that a score of one is added when CART analysis creates a partition.</p>

<p>One study (Kerby, 2003) selected as predictors the five traits of the <a href="Big_five_personality_traits" title="wikilink">Big five personality traits</a>, predicting a multi-valued measure of <a href="suicidal_ideation" title="wikilink">suicidal ideation</a>. Next, the personality scores were converted into standard form with CART analysis. When the CART analysis yielded one partition, the result was like the Burgess method in that the predictor was coded as either zero or one. But for the measure of neuroticism, the result was two cutoff scores. Because higher neuroticism scores correlated with more suicidal thinking, the two cutoff scores led to the following coding: ‚Äúlow Neuroticism‚Äù = 0, ‚Äúmoderate Neuroticism‚Äù = 1, ‚Äúhigh Neuroticism‚Äù = 2 (Kerby, 2003).</p>
<h3 id="z-score-method"><em>z</em>-score method</h3>

<p>Another method can be applied when the predictors are measured on a continuous scale. In such a case, each predictor can be converted into a <a href="standard_score" title="wikilink">standard score</a>, or <em>z</em>-score, so that all the predictors have a mean of zero and a standard deviation of one. With this method of unit-weighted regression, the variate is a sum of the <em>z</em>-scores (e.g., Dawes, 1979; Bobko, Roth, &amp; Buster, 2007).</p>
<h2 id="literature-review">Literature review</h2>

<p>The first empirical study using unit-weighted regression is widely considered to be a 1928 study by sociologist <a href="Ernest_W._Burgess" title="wikilink">Ernest W. Burgess</a>. He used 21 variables to predict parole success or failure, and the results suggest that unit weights are a useful tool in making decisions about which inmates to parole. Of those inmates with the best scores, 98% did in fact succeed on parole; and of those with the worst scores, only 24% did in fact succeed (Burgess, 1928).</p>

<p>The mathematical issues involved in unit-weighted regression were first discussed in 1938 by <a href="Samuel_Stanley_Wilks" title="wikilink">Samuel Stanley Wilks</a>, a leading statistician who had a special interest in <a href="multivariate_analysis" title="wikilink">multivariate analysis</a>. Wilks described how unit weights could be used in practical settings, when data were not available to estimate beta weights. For example, a small college may want to select good students for admission. But the school may have no money to gather data and conduct a standard multiple regression analysis. In this case, the school could use several predictors‚Äîhigh school grades, SAT scores, teacher ratings. Wilks (1938) showed mathematically why unit weights should work well in practice.</p>

<p>Frank Schmidt (1971) conducted a simulation study of unit weights. His results showed that Wilks was indeed correct and that unit weights tend to perform well in simulations of practical studies.</p>

<p><a href="Robyn_Dawes" title="wikilink">Robyn Dawes</a> (1979) discussed the use of unit weights in applied studies, referring to the robust beauty of unit weighted models. <a href="Jacob_Cohen_(statistician)" title="wikilink">Jacob Cohen</a> also discussed the value of unit weights and noted their practical utility. Indeed, he wrote, "As a practical matter, most of the time, we are better off using unit weights" (Cohen, 1990, p.¬†1306).</p>

<p>Dave Kerby (2003) showed that unit weights compare well with standard regression, doing so with a <a href="Cross-validation_(statistics)" title="wikilink">cross validation</a> study‚Äîthat is, he derived beta weights in one sample and applied them to a second sample. The outcome of interest was suicidal thinking, and the predictor variables were broad personality traits. In the cross validation sample, the correlation between personality and suicidal thinking was slightly stronger with unit-weighted regression (<em>r</em> = .48) than with standard multiple regression (<em>r</em> = .47).</p>

<p>Gottfredson and Snyder (2005) compared the Burgess method of unit-weighted regression to other methods, with a construction sample of N = 1,924 and a cross-validation sample of N = 7,552. Using the Pearson point-biserial, the effect size in the cross validation sample for the unit-weights model was <em>r</em> = .392, which was somewhat larger than for logistic regression (<em>r</em> = .368) and predictive attribute analysis (<em>r</em> = .387), and less than multiple regression only in the third decimal place (<em>r</em> = .397).</p>

<p>In a review of the literature on unit weights, Bobko, Roth, and Buster (2007) noted that "unit weights and regression weights perform similarly in terms of the magnitude of cross-validated multiple correlation, and empirical studies have confirmed this result across several decades" (p. 693).</p>

<p>Andreas Graefe applied an equal weighting approach to nine established <a href="multiple_regression" title="wikilink">multiple regression models</a> for forecasting <a href="U.S._presidential_elections" title="wikilink">U.S. presidential elections</a>. Across the ten elections from 1976 to 2012, equally weighted predictors reduced the forecast error of the original regression models on average by four percent. An equal-weights model that includes all variables provided well-calibrated forecasts that reduced the error of the most accurate regression model by 29% percent.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="example">Example</h2>

<p>An example may clarify how unit weights can be useful in practice.</p>

<p>Brenna Bry and colleagues (1982) addressed the question of what causes drug use in adolescents. Previous research had made use of multiple regression; with this method, it is natural to look for the best predictor, the one with the highest beta weight. Bry and colleagues noted that one previous study had found that early use of alcohol was the best predictor. Another study had found that alienation from parents was the best predictor. Still another study had found that low grades in school were the best predictor. The failure to replicate was clearly a problem, a problem that could be caused by bouncing betas.</p>

<p>Bry and colleagues suggested a different approach: instead of looking for the best predictor, they looked at the number of predictors. In other words, they gave a unit weight to each predictor. Their study had six predictors: 1) low grades in school, 2) lack of affiliation with religion, 3) early age of alcohol use, 4) psychological distress, 5) low self-esteem, and 6) alienation from parents. To convert the predictors to standard form, each risk factor was scored as absent (scored as zero) or present (scored as one). For example, the coding for low grades in school were as follows: "C or higher" = 0, "D or F" = 1. The results showed that the number of risk factors was a good predictor of drug use: adolescents with more risk factors were more likely to use drugs.</p>

<p>The model used by Bry and colleagues was that drug users do not differ in any special way from non-drug users. Rather, they differ in the number of problems they must face. "The number of factors an individual must cope with is more important than exactly what those factors are" (p.¬†277). Given this model, unit-weighted regression is an appropriate method of analysis.</p>
<h2 id="beta-weights">Beta weights</h2>

<p>In the standard form of multiple regression, each predictor is multiplied by a number that is called the beta weight. The prediction is obtained by adding these products (and usually by adding a constant, as well). When the weights are chosen to give the best prediction by some criterion, the model is called a <a href="proper_linear_model" title="wikilink">proper linear model</a>. Therefore, multiple regression is a proper linear model. By contrast, unit-weighted regression is called an improper linear model.</p>
<h2 id="model-specification">Model specification</h2>

<p>Standard multiple regression has a major assumption: it assumes that all the important predictors are in the equation. This assumption is called model specification. A model is specified when all the predictors are in the equation, and no irrelevant predictors are in the equation.</p>

<p>However, in the social sciences, it is rare for a study to be able to know all the important predictors of a behavioral outcome. Therefore, most models are not specified. When the model is not specified, the estimates for the beta weights are not accurate. That is, the beta weights may change from one sample to the next, a situation sometimes called the problem of the bouncing betas. It is this problem with bouncing betas that makes unit-weighted regression a useful method.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="regression_analysis" title="wikilink">regression analysis</a></li>
<li><a href="linear_regression" title="wikilink">linear regression</a></li>
<li><a href="robust_regression" title="wikilink">robust regression</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li>Bobko, P., Roth, P. L., &amp; Buster, M. A. (2007). "The usefulness of unit weights in creating composite scores: A literature review, application to content validity, and meta-analysis". <em>Organizational Research Methods</em>, volume 10, pages 689-709. </li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li>Burgess, E. W. (1928). "Factors determining success or failure on parole". In A. A. Bruce (Ed.), <em>The Workings of the Indeterminate Sentence Law and Parole in Illinois</em> (pp. 205-249). Springfield, Illinois: Illinois State Parole Board. <a href="http://books.google.com/books/about/The_Workings_of_the_Indeterminate_senten.html?id=V6xCAAAAIAAJ">Google books</a></li>
</ul>
<ul>
<li>Cohen, Jacob. (1990). "Things I have learned (so far)". <em>American Psychologist</em>, volume 45, pages 1304-1312. </li>
</ul>
<ul>
<li>Dawes, Robyn M. (1979). "The robust beauty of improper linear models in decision making". <em>American Psychologist</em>, volume 34, pages 571-582. . <a href="http://www.cmu.edu/dietrich/sds/docs/dawes/the-robust-beauty-of-improper-linear-models-in-decision-making.pdf">archived pdf</a></li>
</ul>
<ul>
<li>Gottfredson, D. M., &amp; Snyder, H. N. (July 2005). <em>The mathematics of risk classification: Changing data into valid instruments for juvenile courts</em>. Pittsburgh, Penn.: National Center for Juvenile Justice. NCJ 209158. <a href="http://files.eric.ed.gov/fulltext/ED485849.pdf">Eric.ed.gov pdf</a></li>
</ul>
<ul>
<li>Kerby, Dave S. (2003). "CART analysis with unit-weighted regression to predict suicidal ideation from Big Five traits". <em>Personality and Individual Differences</em>, volume 35, pages 249-261. </li>
</ul>
<ul>
<li>Schmidt, Frank L. (1971). "The relative efficiency of regression and simple unit predictor weights in applied differential psychology". <em>Educational and Psychological Measurement</em>, volume 31, pages 699-714. </li>
</ul>
<ul>
<li>Wainer, H., &amp; Thissen, D. (1976). Three steps toward robust regression. <em>Psychometrika</em>, volume 41(1), pages 9-34. </li>
</ul>
<ul>
<li></li>
</ul>
<h2 id="further-reading">Further reading</h2>
<ul>
<li>Dana, J., &amp; Dawes, R. M. (2004). "The superiority of simple alternatives to regression for social science predictions". <em><a href="Journal_of_Educational_and_Behavioral_Statistics" title="wikilink">Journal of Educational and Behavioral Statistics</a></em>, volume 29(3), pages 317-331. </li>
</ul>
<ul>
<li>Dawes, R. M., &amp; Corrigan, B. (1974). Linear models in decision making. <em><a href="Psychological_Bulletin" title="wikilink">Psychological Bulletin</a></em>, volume 81, pages 95-106. </li>
</ul>
<ul>
<li>Einhorn, H. J., &amp; Hogarth, R. M. (1975). Unit weighting schemes for decision making. <em>Organizational Behavior and Human Performance</em>, volume 13(2), pages 171-192. </li>
</ul>
<ul>
<li>Hakeem, M. (1948). The validity of the Burgess method of parole prediction. <em>American Journal of Sociology</em>, volume 53(5), pages 376-386. <a href="http://www.jstor.org/stable/2771477">JSTOR</a></li>
</ul>
<ul>
<li>Newman, J. R., Seaver, D., Edwards, W. (1976). Unit versus differential weighting schemes for decision making: A method of study and some preliminary results. Los Angeles, CA: Social Science Research Institute. <a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a033183.pdf">archived pdf</a></li>
</ul>
<ul>
<li>Raju, N. S., Bilgic, R., Edwards, J. E., Fleer, P. F. (1997). Methodology review: Estimation of population validity and cross-validity, and the use of equal weights in prediction. <em>Applied Psychological Measurement</em>, volume 21(4), pages 291-305. </li>
</ul>
<ul>
<li>Ree, M. J., Carretta, T. R., &amp; Earles, J. A. (1998). "In top-down decisions, weighting variables does not matter: A consequence of Wilk's theorem. <em>Organizational Research Methods</em>, volume 1(4), pages 407-420. </li>
</ul>
<ul>
<li>

<p><a href="http://www-stat.wharton.upenn.edu/~hwainer/Readings/Wainer_Estimating%20Coefficients%20in%20Linear%20Models.pdf">archived pdf</a></p></li>
</ul>
<ul>
<li>Wainer, H. (1978). On the sensitivity of regression and regressors. <em><a href="Psychological_Bulletin" title="wikilink">Psychological Bulletin</a></em>, volume 85(2), pages 267-273. </li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://andrewgelman.com/wp-content/uploads/2013/08/Graefe-2013-Improving-forecasts-using-equally-weighted-predictors-JBR.pdf?8b0eec">Paper by Andreas Graefe</a> - Improving forecasts using equally weighted predictors.</li>
<li><a href="http://www.uiowa.edu/~c073005a/SVFolder/svhome.htm">Paper by Douglas Langbehn</a> - Simplest may be best</li>
</ul>

<p>"</p>

<p><a href="Category:Regression_analysis" title="wikilink">Category:Regression analysis</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">‚Ü©</a></li>
</ol>
</section>
</body>
</html>
