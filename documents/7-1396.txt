   Doob martingale      Doob martingale   A Doob martingale (also known as a Levy martingale ) is a mathematical construction of a stochastic process which approximates a given random variable and has the martingale property with respect to the given filtration . It may be thought of as the evolving sequence of best approximations to the random variable based on information accumulated up to a certain time.  When analyzing sums, random walks , or other additive functions of independent random variables , one can often apply the central limit theorem , law of large numbers , Chernoff's inequality , Chebyshev's inequality or similar tools. When analyzing similar objects where the differences are not independent, the main tools are martingales and Azuma's inequality .  Definition  A Doob martingale (named after Joseph L. Doob ) 1 is a generic construction that is always a martingale. Specifically, consider any set of random variables       X  →   =    X  1   ,   X  2   ,  …  ,   X  n         normal-→  X     subscript  X  1    subscript  X  2   normal-…   subscript  X  n      \vec{X}=X_{1},X_{2},...,X_{n}     taking values in a set   A   A   A   for which we are interested in the function    f  :    A  n   →  ℝ      normal-:  f   normal-→   superscript  A  n   ℝ     f:A^{n}\to\mathbb{R}   and define:       B  i   =   E    X   i  +  1    ,   X   i  +  2    ,  …  ,   X  n      [  f   (   X  →   )   |   X  1   ,   X  2   ,  …   X  i   ]      fragments   subscript  B  i     subscript  E    subscript  X    i  1     subscript  X    i  2    normal-…   subscript  X  n      fragments  normal-[  f   fragments  normal-(   normal-→  X   normal-)   normal-|   subscript  X  1   normal-,   subscript  X  2   normal-,  normal-…   subscript  X  i   normal-]     B_{i}=E_{X_{i+1},X_{i+2},...,X_{n}}[f(\vec{X})|X_{1},X_{2},...X_{i}]     where the above expectation is itself a random quantity since the expectation is only taken over        X   i  +  1    ,   X   i  +  2    ,  …  ,   X  n    ,      subscript  X    i  1     subscript  X    i  2    normal-…   subscript  X  n     X_{i+1},X_{i+2},...,X_{n},     and       X  1   ,   X  2   ,   …   X  i        subscript  X  1    subscript  X  2     normal-…   subscript  X  i      X_{1},X_{2},...X_{i}     are treated as random variables. It is possible to show that    B  i     subscript  B  i    B_{i}   is always a martingale regardless of the properties of    X  i     subscript  X  i    X_{i}   .  The sequence    B  i     subscript  B  i    {B_{i}}   is the Doob martigale for f . 2  Application  Thus if one can bound the differences      |    B   i  +  1    -   B  i    |         subscript  B    i  1     subscript  B  i      |B_{i+1}-B_{i}|   ,  one can apply Azuma's inequality and show that with high probability    f   (   X  →   )       f   normal-→  X     f(\vec{X})   is concentrated around its expected value        E   [   f   (   X  →   )    ]    =   B  0    .        E   delimited-[]    f   normal-→  X       subscript  B  0     E[f(\vec{X})]=B_{0}.     McDiarmid's inequality  One common way of bounding the differences and applying Azuma's inequality to a Doob martingale is called McDiarmid's inequality. 3  Suppose     X  1   ,   X  2   ,  …  ,   X  n       subscript  X  1    subscript  X  2   normal-…   subscript  X  n     X_{1},X_{2},\dots,X_{n}   are independent and assume that   f   f   f   satisfies          sup    x  1   ,   x  2   ,  …  ,   x  n   ,    x  ^   i      |    f   (   x  1   ,   x  2   ,  …  ,   x  n   )    -   f   (   x  1   ,   x  2   ,  …  ,   x   i  -  1    ,    x  ^   i   ,   x   i  +  1    ,  …  ,   x  n   )     |    ≤    c  i   for     1  ≤  i  ≤   n     .     formulae-sequence      subscript  supremum    subscript  x  1    subscript  x  2   normal-…   subscript  x  n    subscript   normal-^  x   i           f    subscript  x  1    subscript  x  2   normal-…   subscript  x  n       f    subscript  x  1    subscript  x  2   normal-…   subscript  x    i  1     subscript   normal-^  x   i    subscript  x    i  1    normal-…   subscript  x  n          subscript  c  i   for        1  i       n      \sup_{x_{1},x_{2},\dots,x_{n},\hat{x}_{i}}|f(x_{1},x_{2},\dots,x_{n})-f(x_{1},%
 x_{2},\dots,x_{i-1},\hat{x}_{i},x_{i+1},\dots,x_{n})|\leq c_{i}\qquad\text{for%
 }\quad 1\leq i\leq n\;.     (In other words, replacing the   i   i   i   -th coordinate    x  i     subscript  x  i    x_{i}   by some other value changes the value of   f   f   f   by at most    c  i     subscript  c  i    c_{i}   .)  It follows that       |    B   i  +  1    -   B  i    |   ≤   c  i            subscript  B    i  1     subscript  B  i      subscript  c  i     |B_{i+1}-B_{i}|\leq c_{i}     and therefore Azuma's inequality yields the following McDiarmid inequalities for any    ε  >  0      ε  0    \varepsilon>0   :       Pr   {     f   (   X  1   ,   X  2   ,  …  ,   X  n   )    -   E   [   f   (   X  1   ,   X  2   ,  …  ,   X  n   )    ]     ≥  ε   }    ≤   exp   (   -    2   ε  2      ∑   i  =  1   n    c  i  2      )         Pr        f    subscript  X  1    subscript  X  2   normal-…   subscript  X  n       E   delimited-[]    f    subscript  X  1    subscript  X  2   normal-…   subscript  X  n        ε            2   superscript  ε  2      superscript   subscript     i  1    n    superscript   subscript  c  i   2         \Pr\left\{f(X_{1},X_{2},\dots,X_{n})-E[f(X_{1},X_{2},\dots,X_{n})]\geq%
 \varepsilon\right\}\leq\exp\left(-\frac{2\varepsilon^{2}}{\sum_{i=1}^{n}c_{i}^%
 {2}}\right)     and       Pr   {     E   [   f   (   X  1   ,   X  2   ,  …  ,   X  n   )    ]    -   f   (   X  1   ,   X  2   ,  …  ,   X  n   )     ≥  ε   }    ≤   exp   (   -    2   ε  2      ∑   i  =  1   n    c  i  2      )         Pr        E   delimited-[]    f    subscript  X  1    subscript  X  2   normal-…   subscript  X  n         f    subscript  X  1    subscript  X  2   normal-…   subscript  X  n      ε            2   superscript  ε  2      superscript   subscript     i  1    n    superscript   subscript  c  i   2         \Pr\left\{E[f(X_{1},X_{2},\dots,X_{n})]-f(X_{1},X_{2},\dots,X_{n})\geq%
 \varepsilon\right\}\leq\exp\left(-\frac{2\varepsilon^{2}}{\sum_{i=1}^{n}c_{i}^%
 {2}}\right)     and        Pr   {    |    E   [   f   (   X  1   ,   X  2   ,  …  ,   X  n   )    ]    -   f   (   X  1   ,   X  2   ,  …  ,   X  n   )     |   ≥  ε   }    ≤   2   exp   (   -    2   ε  2      ∑   i  =  1   n    c  i  2      )      .       Pr          E   delimited-[]    f    subscript  X  1    subscript  X  2   normal-…   subscript  X  n         f    subscript  X  1    subscript  X  2   normal-…   subscript  X  n       ε      2          2   superscript  ε  2      superscript   subscript     i  1    n    superscript   subscript  c  i   2          \Pr\left\{|E[f(X_{1},X_{2},\dots,X_{n})]-f(X_{1},X_{2},\dots,X_{n})|\geq%
 \varepsilon\right\}\leq 2\exp\left(-\frac{2\varepsilon^{2}}{\sum_{i=1}^{n}c_{i%
 }^{2}}\right).\;     See also   Markov inequality  Chebyshev's inequality  Bernstein inequalities (probability theory)  Dvoretzky–Kiefer–Wolfowitz inequality   Notes  References     "  Category:Probabilistic inequalities  Category:Statistical inequalities  Category:Martingale theory     ↩  Anupam Gupta (2011) http://www.cs.cmu.edu/~avrim/Randalgs11/lectures/lect0321.pdf Lecture notes ↩  ↩     