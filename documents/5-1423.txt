   Sample size determination      Sample size determination   Sample size determination is the act of choosing the number of observations or replicates to include in a statistical sample . The sample size is an important feature of any empirical study in which the goal is to make inferences about a population from a sample. In practice, the sample size used in a study is determined based on the expense of data collection, and the need to have sufficient statistical power . In complicated studies there may be several different sample sizes involved in the study: for example, in a stratified  survey there would be different sample sizes for each stratum. In a census , data are collected on the entire population, hence the sample size is equal to the population size. In experimental design , where a study may be divided into different treatment groups , there may be different sample sizes for each group.  Sample sizes may be chosen in several different ways:   expedience - For example, include those items readily available or convenient to collect. A choice of small sample sizes, though sometimes necessary, can result in wide confidence intervals or risks of errors in statistical hypothesis testing .  using a target variance for an estimate to be derived from the sample eventually obtained  using a target for the power of a statistical test to be applied once the sample is collected.   How samples are collected is discussed in sampling (statistics) and survey data collection .  Introduction  Larger sample sizes generally lead to increased precision when estimating unknown parameters. For example, if we wish to know the proportion of a certain species of fish that is infected with a pathogen, we would generally have a more precise estimate of this proportion if we sampled and examined 200 rather than 100 fish. Several fundamental facts of mathematical statistics describe this phenomenon, including the law of large numbers and the central limit theorem .  In some situations, the increase in precision for larger sample sizes is minimal, or even non-existent. This can result from the presence of systematic errors or strong dependence in the data, or if the data follow a heavy-tailed distribution.  Sample sizes are judged based on the quality of the resulting estimates. For example, if a proportion is being estimated, one may wish to have the 95% confidence interval be less than 0.06 units wide. Alternatively, sample size may be assessed based on the power of a hypothesis test. For example, if we are comparing the support for a certain political candidate among women with the support for that candidate among men, we may wish to have 80% power to detect a difference in the support levels of 0.04 units.  Estimation  Proportions  A relatively simple situation is estimation of a proportion . For example, we may wish to estimate the proportion of residents in a community who are at least 65 years old.  The estimator of a proportion is     p  ^   =   X  /  n        normal-^  p     X  n     \hat{p}=X/n   , where X is the number of 'positive' observations (e.g. the number of people out of the n sampled people who are at least 65 years old). When the observations are independent , this estimator has a (scaled) binomial distribution (and is also the sample  mean of data from a Bernoulli distribution ). The maximum variance of this distribution is 0.25/ n , which occurs when the true parameter is p = 0.5. In practice, since p is unknown, the maximum variance is often used for sample size assessments.  For sufficiently large n , the distribution of    p  ^     normal-^  p    \hat{p}   will be closely approximated by a normal distribution . 1 Using this approximation, it can be shown that around 95% of this distribution's probability lies within 2 standard deviations of the mean. Using the Wald method for the binomial distribution , an interval of the form      (    p  ^   -   2    0.25  /  n      ,    p  ^   +   2    0.25  /  n      )        normal-^  p     2      0.25  n         normal-^  p     2      0.25  n        (\hat{p}-2\sqrt{0.25/n},\hat{p}+2\sqrt{0.25/n})     will form a 95% confidence interval for the true proportion. If this interval needs to be no more than W units wide, the equation       4    0.25  /  n     =  W        4      0.25  n     W    4\sqrt{0.25/n}=W     can be solved for n , yielding 2 3  n = 4/ W 2 = 1/ B 2 where B is the error bound on the estimate, i.e., the estimate is usually given as within ± B . So, for B = 10% one requires n = 100, for B = 5% one needs n = 400, for B = 3% the requirement approximates to n = 1000, while for B = 1% a sample size of n = 10000 is required. These numbers are quoted often in news reports of opinion polls and other sample surveys .  Means  A proportion is a special case of a mean when the moon is both full and half and Deez Mutz are clearly visable in the sky. When estimating the population mean using an independent and identically distributed (iid) sample of size n , where each data value has variance σ 2 , the standard error of the sample mean is:         σ  /   n    .      σ    n     \sigma/\sqrt{n}.        This expression describes quantitatively how the estimate becomes more precise as the sample size increases. Using the central limit theorem to justify approximating the sample mean with a normal distribution yields an approximate 95% confidence interval of the form       (    x  ¯   -    2  σ   /   n     ,    x  ¯   +    2  σ   /   n     )   .        normal-¯  x       2  σ     n        normal-¯  x       2  σ     n       (\bar{x}-2\sigma/\sqrt{n},\bar{x}+2\sigma/\sqrt{n}).     If we wish to have a confidence interval that is W units in width, we would solve        4  σ   /   n    =  W          4  σ     n    W    4\sigma/\sqrt{n}=W     for n , yielding the sample size n = 16''σ 2 /W 2 .  For example, if we are interested in estimating the amount by which a drug lowers a subject's blood pressure with a confidence interval that is six units wide, and we know that the standard deviation of blood pressure in the population is 15, then the required sample size is 100.  Required sample sizes for hypothesis tests  A common problem faced by statisticians is calculating the sample size required to yield a certain power for a test, given a predetermined Type I error rate α. As follows, this can be estimated by pre-determined tables for certain values, by Mead's resource equation, or, more generally, by the cumulative distribution function :  Tables      4   Power   Cohen's d       0.2   0.5     0.25   84     0.50   193     0.60   246     0.70   310     0.80   393     0.90   526     0.95   651     0.99   920     The table shown at right can be used in a two-sample t-test to estimate the sample sizes of an experimental group and a control group that are of equal size, that is, the total number of individuals in the trial is twice that of the number given, and the desired significance level is 0.05. 5 The parameters used are:   The desired statistical power of the trial, shown in column to the right.  Cohen's d (=effect size), which is the expected difference between the means of the target values between the experimental group and the control group , divided by the expected standard deviation .   Mead's resource equation  Mead's resource equation is often used for estimating sample sizes of laboratory animals , as well as in many other laboratory experiments. It may not be as accurate as using other methods in estimating sample size, but gives a hint of what is the appropriate sample size where parameters such as expected standard deviations or expected differences in values between groups are unknown or very hard to estimate. [ http://books.google.se/books?id=Wjr9u1AAhsdsdsdsdsdssdsdddddddddddddddddddst4C&pg; ;=PA29 online Page 29]  All the parameters in the equation are in fact the degrees of freedom of the number of their concepts, and hence, their numbers are subtracted by 1 before insertion into the equation.  The equation is:       E  =   N  -  B  -  T    ,      E    N  B  T     E=N-B-T,   where:   N is the total number of individuals or units in the study (minus 1)  B is the blocking component , representing environmental effects allowed for in the design (minus 1)  T is the treatment component , corresponding to the number of treatment groups (including control group ) being used, or the number of questions being asked (minus 1)  E is the degrees of freedom of the error component , and should be somewhere between 10 and 20.   For example, if a study using laboratory animals is planned with four treatment groups ( T =3), with eight animals per group, making 32 animals total ( N =31), without any further stratification ( B =0), then E would equal 28, which is above the cutoff of 20, indicating that sample size may be a bit too large, and six animals per group might be more appropriate. 6  Cumulative distribution function  Let X i , i = 1, 2, ..., n be independent observations taken from a normal distribution with unknown mean μ and known variance σ 2 . Let us consider two hypotheses, a null hypothesis :       H  0   :   μ  =  0      normal-:   subscript  H  0     μ  0     H_{0}:\mu=0     and an alternative hypothesis:       H  a   :   μ  =   μ  *       normal-:   subscript  H  a     μ   superscript  μ       H_{a}:\mu=\mu^{*}     for some 'smallest significant difference' μ * >0. This is the smallest value for which we care about observing a difference. Now, if we wish to (1) reject H 0 with a probability of at least 1-β when H a is true (i.e. a power of 1-β), and (2) reject H 0 with probability α when H 0 is true, then we need the following:  If z α is the upper α percentage point of the standard normal distribution, then       Pr   (    x  ¯   >     z  α   σ   /   n     |    H  0   true   )    =  α       Pr     normal-¯  x        subscript  z  α   σ     n        subscript  H  0   true    α    \Pr(\bar{x}>z_{\alpha}\sigma/\sqrt{n}|H_{0}\text{ true})=\alpha     and so   'Reject H 0 if our sample average (    x  ¯     normal-¯  x    \bar{x}   ) is more than      z  α   σ   /   n          subscript  z  α   σ     n     z_{\alpha}\sigma/\sqrt{n}   '   is a decision rule which satisfies (2). (Note, this is a 1-tailed test)  Now we wish for this to happen with a probability at least 1-β when H a is true. In this case, our sample average will come from a Normal distribution with mean μ * . Therefore we require       Pr   (    x  ¯   >     z  α   σ   /   n     |    H  a   true   )    ≥   1  -  β        Pr     normal-¯  x        subscript  z  α   σ     n        subscript  H  a   true      1  β     \Pr(\bar{x}>z_{\alpha}\sigma/\sqrt{n}|H_{a}\text{ true})\geq 1-\beta     Through careful manipulation, this can be shown to happen when      n  ≥    (     z  α   +    Φ   -  1     (   1  -  β   )       μ  *   /  σ    )   2       n   superscript       subscript  z  α      superscript  normal-Φ    1      1  β        superscript  μ    σ    2     n\geq\left(\frac{z_{\alpha}+\Phi^{-1}(1-\beta)}{\mu^{*}/\sigma}\right)^{2}     where   Φ   normal-Φ   \Phi   is the normal cumulative distribution function .  Stratified sample size  With more complicated sampling techniques, such as stratified sampling , the sample can often be split up into sub-samples. Typically, if there are H such sub-samples (from H different strata) then each of them will have a sample size n h , h = 1, 2, ..., H . These n h must conform to the rule that n 1 + n 2 + ... + n H = n (i.e. that the total sample size is given by the sum of the sub-sample sizes). Selecting these n h optimally can be done in various ways, using (for example) Neyman's optimal allocation.  There are many reasons to use stratified sampling: 7 to decrease variances of sample estimates, to use partly non-random methods, or to study strata individually. A useful, partly non-random method would be to sample individuals where easily accessible, but, where not, sample clusters to save travel costs. 8  In general, for H strata, a weighted sample mean is         x  ¯   w   =    ∑   h  =  1   H     W  h     x  ¯   h      ,       subscript   normal-¯  x   w     superscript   subscript     h  1    H      subscript  W  h    subscript   normal-¯  x   h       \bar{x}_{w}=\sum_{h=1}^{H}W_{h}\bar{x}_{h},   with        Var   (    x  ¯   w   )    =    ∑   h  =  1   H      W  h  2     Var   (    x  ¯   h   )       .       Var   subscript   normal-¯  x   w      superscript   subscript     h  1    H      superscript   subscript  W  h   2    Var   subscript   normal-¯  x   h        \operatorname{Var}(\bar{x}_{w})=\sum_{h=1}^{H}W_{h}^{2}\,\operatorname{Var}(%
 \bar{x}_{h}).    9  The weights,    W  h     subscript  W  h    W_{h}   , frequently, but not always, represent the proportions of the population elements in the strata, and     W  h   =    N  h   /  N        subscript  W  h      subscript  N  h   N     W_{h}=N_{h}/N   . For a fixed sample size, that is    N  =   ∑   N  h        N     subscript  N  h      N=\sum{N_{h}}   ,        Var   (    x  ¯   w   )    =    ∑   h  =  1   H      W  h  2    V  a   r  h    (    1   n  h    -   1   N  h     )      ,       Var   subscript   normal-¯  x   w      superscript   subscript     h  1    H      superscript   subscript  W  h   2   V  a   subscript  r  h       1   subscript  n  h      1   subscript  N  h         \operatorname{Var}(\bar{x}_{w})=\sum_{h=1}^{H}W_{h}^{2}\,Var_{h}\left(\frac{1}%
 {n_{h}}-\frac{1}{N_{h}}\right),    10  which can be made a minimum if the sampling rate within each stratum is made proportional to the standard deviation within each stratum      n  h   /   N  h    =   k   S  h           subscript  n  h    subscript  N  h      k   subscript  S  h      n_{h}/N_{h}=kS_{h}   , where     S  h   =    V  a   r  h          subscript  S  h       V  a   subscript  r  h       S_{h}=\sqrt{Var_{h}}   and   k   k   k   is a constant such that     ∑   n  h    =  n         subscript  n  h    n    \sum{n_{h}}=n   .  An "optimum allocation" is reached when the sampling rates within the strata are made directly proportional to the standard deviations within the strata and inversely proportional to the square root of the sampling cost per element within the strata,    C  h     subscript  C  h    C_{h}   :         n  h    N  h    =    K   S  h      C  h      ,         subscript  n  h    subscript  N  h        K   subscript  S  h       subscript  C  h       \frac{n_{h}}{N_{h}}=\frac{KS_{h}}{\sqrt{C_{h}}},    11  where   K   K   K   is a constant such that     ∑   n  h    =  n         subscript  n  h    n    \sum{n_{h}}=n   , or, more generally, when        n  h   =     K  ′    W  h    S  h      C  h      .       subscript  n  h        superscript  K  normal-′    subscript  W  h    subscript  S  h       subscript  C  h       n_{h}=\frac{K^{\prime}W_{h}S_{h}}{\sqrt{C_{h}}}.    12  Qualitative research  Sample size determination in qualitative studies takes a different approach. It is generally a subjective judgment, taken as the research proceeds. 13 One approach is to continue to include further participants or material until saturation is reached. 14 The number needed to reach saturation has been investigated empirically. 15 16 17 18  There is a paucity of reliable guidance on estimating sample sizes before starting the research, with a range of suggestions given. 19 20 21 22 A tool akin to a quantitative power calculation, based on the negative binomial distribution , has been suggested for thematic analysis . 23  See also   Design of experiments  Engineering response surface example under Stepwise regression  Cohen's h   Notes  References      Further reading   NIST: Selecting Sample Sizes  Raven Analytics: Sample Size Calculations  ASTM E122-07: Standard Practice for Calculating Sample Size to Estimate, With Specified Precision, the Average for a Characteristic of a Lot or Process   External links   Video: Power and Sample Size Primer by NCSS  PowerAndSampleSize.com – free, online power and sample size calculators with graphics highlighting sensitivity to input values  Sample size calculator from the Australian National Statistical Service  Sample Size Calculator by Raosoft, Inc.  PASS power analysis and sample size software   de:Zufallsstichprobe#Stichprobenumfang "  Category:Sampling (statistics)     NIST / SEMATECH , "7.2.4.2. Sample sizes required" , e-Handbook of Statistical Methods. ↩  "Large Sample Estimation of a Population Proportion" ↩  "Confidence Interval for a Proportion" ↩   Chapter 13 , page 215, in: ↩  Isogenic.info > Resource equation by Michael FW Festing. Updated Sept. 2006 ↩  Kish (1965, Section 3.1) ↩  Kish (1965), p.148. ↩  Kish (1965), p.78. ↩  Kish (1965), p.81. ↩  Kish (1965), p.93. ↩  Kish (1965), p.94. ↩  Sandelowski, M. (1995). Sample size in qualitative research. Research in Nursing & Health , 18, 179–183 ↩  Glaser, B. (1965). The constant comparative method of qualitative analysis. Social Problems , 12, 436–445 ↩  Francis, J. J., Johnston, M., Robertson, C., Glidewell, L., Entwistle, V., Eccles, M. P., & Grimshaw, J. M. (2010). What is an adequate sample size? Operationalising data saturation for theory-based interview studies. Psychology and Health , 25, 1229–1245. doi:10.1080/08870440903194015 ↩   Wright, A., Maloney, F. L., & Feblowitz, J. C. (2011) . Clinician attitudes toward and use of electronic problem lists: a thematic analysis. BMC Medical Informatics and Decision Making , 11, 36. doi:10.1186/1472-6947-11-36 ↩  http://www.qualitative-research.net/index.php/fqs/article/view/1428/3027 ↩  Guest, G., Bunce, A., & Johnson, L. (2006) . How many interviews are enough?: An experiment with data saturation and variability. Field Methods , 18, 59–82. doi:10.1177/1525822X05279903 ↩  Emmel, N. (2013). Sampling and choosing cases in qualitative research: A realist approach. London: Sage. ↩  Onwuegbuzie, A. J., & Leech, N. L. (2007) . A call for qualitative power analyses. Quality & Quantity , 41, 105–121. doi:10.1007/s11135-005-1098-1 ↩  ↩      