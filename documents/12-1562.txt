   Higher-order singular value decomposition      Higher-order singular value decomposition   In multilinear algebra , there does not exist a general decomposition method for multi-way arrays (also known as N-arrays , higher-order arrays , or data-tensors ) with all the properties of a matrix singular value decomposition (SVD). A matrix SVD simultaneously computes   (a) a rank- R decomposition and    (b) the orthonormal row/column matrices.   These two properties can be captured separately by two different decompositions for multi-way arrays .  Property (a) is extended to higher order by a class of closely related constructions known collectively as CP decomposition (named after the two most popular and general variants, CANDECOMP and PARAFAC). Such decompositions represent a tensor as the sum of the n-fold outer products of rank-1 tensors, where n is the dimension of the tensor indices.  Property (b) is extended to higher order by a class of methods known variably as Tucker3 , N-mode SVD , and N-mode principal component analysis (PCA). (This article will use the general term "Tucker decomposition".) These methods compute the orthonormal spaces associated with the different axes (or modes) of a tensor. The Tucker decomposition is also used in multilinear subspace learning as multilinear principal component analysis . This terminology was coined by P. Kroonenberg in the 1980s, but it was later called multilinear SVD and HOSVD (higher-order SVD) by L. De Lathauwer.  Historically, much of the interest in higher-order SVDs was driven by the need to analyze empirical data, especially in psychometrics and chemometrics . As such, many of the methods have been independently invented several times, often with subtle variations, leading to a confusing literature. Abstract and general mathematical theorems are rare (though see Kruskal 1 with regard to the CP decomposition); instead, the methods are often designed for analyzing specific data types. The 2008 review article by Kolda and Bader 2 provides a compact summary of the history of these decompositions, and many references for further reading.  The concept of HOSVD was carried over to functions by Baranyi and Yam via the TP model transformation  3 . 4 This extension led to the definition of the HOSVD based canonical form of tensor product functions and Linear Parameter Varying system models 5 and to convex hull manipulation based control optimization theory, see TP model transformation in control theories .  CP decomposition  Definition  A CP decomposition of an N-way array X , with elements    x    i  1   â‹¯   i  N       subscript  x     subscript  i  1   normal-â‹¯   subscript  i  N      x_{i_{1}\cdots i_{N}}   , is      X  =    âˆ‘   r  =  1   R    D   (  r  )     =    âˆ‘   r  =  1   R     a   (  r  )    âŠ—  â‹¯  âŠ—   z   (  r  )            X    superscript   subscript     r  1    R    superscript  D  r           superscript   subscript     r  1    R    tensor-product   superscript  a  r   normal-â‹¯   superscript  z  r        X=\sum_{r=1}^{R}D^{(r)}=\sum_{r=1}^{R}a^{(r)}\otimes\cdots\otimes z^{(r)}     where   âŠ—   tensor-product   \otimes   denotes the tensor product . The R tensors    D   (  r  )      superscript  D  r    D^{(r)}   (known as simple tensors , rank-1 tensors , dyads , or, in quantum mechanics , product states ) are constructed from the rN vectors     a   (  r  )    ,  â‹¯  ,   z   (  r  )        superscript  a  r   normal-â‹¯   superscript  z  r     a^{(r)},\cdots,z^{(r)}   . With indices, this is       x    i  1   â‹¯   i  N     =    âˆ‘   r  =  1   R     a   i  1    (  r  )    â‹¯   z   i  N    (  r  )           subscript  x     subscript  i  1   normal-â‹¯   subscript  i  N       superscript   subscript     r  1    R      subscript   superscript  a  r    subscript  i  1    normal-â‹¯   subscript   superscript  z  r    subscript  i  N        x_{i_{1}\cdots i_{N}}=\sum_{r=1}^{R}a^{(r)}_{i_{1}}\cdots z^{(r)}_{i_{N}}     where    a  i   (  r  )      subscript   superscript  a  r   i    a^{(r)}_{i}   is the i -th element of the vector    a   (  r  )      superscript  a  r    a^{(r)}   , etc.  Tucker decomposition  History  In 1966, L. Tucker proposed a decomposition method for three-way arrays (referred to as a 3-mode " tensors ") as a multidimensional extension of factor analysis . 6 This decomposition was further developed in the 1980s by P. Kroonenberg, who coined the terms Tucker3, Tucker3ALS (an alternating least squares dimensionality reduction algorithm), 3-Mode SVD, and 3-Mode PCA. 7 In the intervening years, several authors developed the decomposition for N -way arrays. Most recently, this work was treated in an elegant fashion and introduced to the SIAM community by L. De Lathauwer et al. who referred to the decomposition as an N -way SVD, multilinear SVD and HOSVD. 8  Definitions  Let the SVD of a real matrix be    A  =   U  S   V  T        A    U  S   superscript  V  T      A=USV^{T}   , then it can be written in an elementwise form as        a    i  1   ,   i  2     =    âˆ‘   j  1      âˆ‘   j  2      s    j  1   ,   j  2      u    i  1   ,   j  1      v    i  2   ,   j  2         .       subscript  a    subscript  i  1    subscript  i  2       subscript    subscript  j  1      subscript    subscript  j  2       subscript  s    subscript  j  1    subscript  j  2      subscript  u    subscript  i  1    subscript  j  1      subscript  v    subscript  i  2    subscript  j  2          a_{i_{1},i_{2}}=\sum_{j_{1}}\sum_{j_{2}}s_{j_{1},j_{2}}u_{i_{1},j_{1}}v_{i_{2}%
 ,j_{2}}.      U   U   U   and   V   V   V   give, in a certain sense optimal, orthonormal basis for the column and row space,   S   S   S   is diagonal with decreasing elements. The higher-order singular value decomposition (HOSVD) can be defined by the multidimensional generalization of this concept:        a    i  1   ,   i  2   ,  â€¦  ,   i  N     =    âˆ‘   j  1      âˆ‘   j  2     â‹¯    âˆ‘   j  N      s    j  1   ,   j  2   ,  â€¦  ,   j  N      u    i  1   ,   j  1     (  1  )     u    i  2   ,   j  2     (  2  )    â€¦   u    i  N   ,   j  N     (  N  )          ,       subscript  a    subscript  i  1    subscript  i  2   normal-â€¦   subscript  i  N       subscript    subscript  j  1      subscript    subscript  j  2      normal-â‹¯    subscript    subscript  j  N       subscript  s    subscript  j  1    subscript  j  2   normal-â€¦   subscript  j  N      subscript   superscript  u  1     subscript  i  1    subscript  j  1      subscript   superscript  u  2     subscript  i  2    subscript  j  2     normal-â€¦   subscript   superscript  u  N     subscript  i  N    subscript  j  N            a_{i_{1},i_{2},\dots,i_{N}}=\sum_{j_{1}}\sum_{j_{2}}\cdots\sum_{j_{N}}s_{j_{1}%
 ,j_{2},\dots,j_{N}}u^{(1)}_{i_{1},j_{1}}u^{(2)}_{i_{2},j_{2}}\dots u^{(N)}_{i_%
 {N},j_{N}},     where the     U   (  n  )    =    [   u   i  ,  j    (  n  )    ]     I  n   Ã—   I  n          superscript  U  n    subscript   delimited-[]   subscript   superscript  u  n    i  j        subscript  I  n    subscript  I  n       U^{(n)}=[u^{(n)}_{i,j}]_{I_{n}\times I_{n}}   matrices and the    ğ’®  =    [   s    j  1   ,  â€¦  ,   j  N     ]     I  1   Ã—   I  2   Ã—  â‹¯  Ã—   I  N         ğ’®   subscript   delimited-[]   subscript  s    subscript  j  1   normal-â€¦   subscript  j  N         subscript  I  1    subscript  I  2   normal-â‹¯   subscript  I  N       \mathcal{S}=[s_{j_{1},\dots,j_{N}}]_{I_{1}\times I_{2}\times\cdots\times I_{N}}   core tensor should satisfy certain requirements (similar ones to the matrix SVD), namely   Each    U   (  n  )      superscript  U  n    U^{(n)}   is an orthogonal matrix .  Two subtensors of the core tensor   ğ’®   ğ’®   \mathcal{S}   are orthogonal i.e.,     âŸ¨   ğ’®    i  n   =  p    ,   ğ’®    i  n   =  q    âŸ©   =  0        subscript  ğ’®     subscript  i  n   p     subscript  ğ’®     subscript  i  n   q     0    \langle\mathcal{S}_{i_{n}=p},\mathcal{S}_{i_{n}=q}\rangle=0   if    p  â‰   q      p  q    p\neq q   .  The subtensors in the core tensor   ğ’®   ğ’®   \mathcal{S}   are ordered according to their Frobenius norm , i.e.     âˆ¥   ğ’®    i  n   =  1    âˆ¥   â‰¥   âˆ¥   ğ’®    i  n   =  2    âˆ¥   â‰¥  â€¦  â‰¥   âˆ¥   ğ’®    i  n   =   I  n     âˆ¥          norm   subscript  ğ’®     subscript  i  n   1      norm   subscript  ğ’®     subscript  i  n   2          normal-â€¦        norm   subscript  ğ’®     subscript  i  n    subscript  I  n         \|\mathcal{S}_{i_{n}=1}\|\geq\|\mathcal{S}_{i_{n}=2}\|\geq\dots\geq\|\mathcal{%
 S}_{i_{n}=I_{n}}\|   for n =Â 1,Â ..., N .   Notation:      ğ’œ  =   ğ’®   Ã—   n  =  1   N    U   (  n  )         ğ’œ    superscript   subscript     n  1    N   ğ’®   superscript  U  n      \mathcal{A}=\mathcal{S}\times_{n=1}^{N}U^{(n)}     Algorithm  The HOSVD can be built from several SVDs, as follows: 9   Given a tensor    ğ’œ  âˆˆ   â„    I  1   Ã—   I  2   Ã—  â‹¯  Ã—   I  N         ğ’œ   superscript  â„     subscript  I  1    subscript  I  2   normal-â‹¯   subscript  I  N       \mathcal{A}\in\mathbb{R}^{I_{1}\times I_{2}\times\cdots\times I_{N}}   , construct the mode- k flattening    ğ’œ   (  k  )      subscript  ğ’œ  k    \mathcal{A}_{(k)}   . That is, the     I  k   Ã—   (    âˆ   j  â‰   k     I  j    )        subscript  I  k     subscript  product    j  k     subscript  I  j      I_{k}\times(\prod_{j\neq k}I_{j})   matrix that corresponds to   ğ’œ   ğ’œ   \mathcal{A}   .  Compute the singular value decomposition      ğ’œ   (  k  )    =    U  k    Î£  k    V  k  T         subscript  ğ’œ  k      subscript  U  k    subscript  normal-Î£  k    subscript   superscript  V  T   k      \mathcal{A}_{(k)}=U_{k}\Sigma_{k}V^{T}_{k}   , and store the left singular vectors    U  k     subscript  U  k    U_{k}   .  The core tensor   ğ’®   ğ’®   \mathcal{S}   is then the projection of   ğ’œ   ğ’œ   \mathcal{A}   onto the tensor basis formed by the factor matrices     {   U  n   }    n  =  1   N     superscript   subscript    subscript  U  n      n  1    N    \{U_{n}\}_{n=1}^{N}   , i.e.,     ğ’®  =   ğ’œ   Ã—   n  =  1   N    U  n  T     .      ğ’®    superscript   subscript     n  1    N   ğ’œ   superscript   subscript  U  n   T      \mathcal{S}=\mathcal{A}\times_{n=1}^{N}U_{n}^{T}.      Applications  Main applications are extracting relevant information from multi-way arrays. Used in factor analysis, face recognition ( TensorFaces ), human motion analysis and synthesis.  The HOSVD has been successfully applied to signal processing and big data, e.g., in genomic signal processing. 10 11 12 These applications also inspired a higher-order GSVD (HO GSVD) 13 and a tensor GSVD. 14  A combination of HOSVD and SVD also has been applied for real time event detection from complex data streams (multivariate data with space and time dimensions) in Disease surveillance . 15  It is also used in tensor product model transformation -based controller design. 16 17 In multilinear subspace learning , 18 it is modified to multilinear principal component analysis 19 for gait recognition.  In machine learning, the CP-decomposition is the central ingredient in learning probabilistic latent variables models via the technique of moment-matching. For example, let us consider the multi-view model 20 which is a probabilistic latent variable model. In this model, we posit the generation of samples as follows: there exists a hidden random variable that is not observed directly, given which, there are several conditionally independent random variables known as the different "views" of the hidden variable. For simplicity, let us say we have three symmetrical views   x   x   x   of a   k   k   k   -state categorical hidden variable   h   h   h   . Then the empirical third moment of this latent variable model can be written as:    T  =   âˆ‘   i  =  1   k   P  r   (  h  =  k  )   E    [  x  |  h  =  k  ]     âŠ—  3       fragments  T    superscript   subscript     i  1    k   P  r   fragments  normal-(  h   k  normal-)   E   superscript   fragments  normal-[  x  normal-|  h   k  normal-]    tensor-product  absent  3      T=\sum_{i=1}^{k}Pr(h=k)E[x|h=k]^{\otimes 3}   .  In applications such as topic modeling , this can be interpreted as the co-occurrence of words in a document. Then the eigenvalues of this empirical moment tensor can be interpreted as the probability of choosing a specific topic and each column of the factor matrix    E   [  x  |  h  =  k  ]      fragments  E   fragments  normal-[  x  normal-|  h   k  normal-]     E[x|h=k]   corresponds to probabilities of words in the vocabulary in the corresponding topic.  References  "  Category:Multilinear algebra     Kruskal, J. B. (1989). "Rank, decomposition, and uniqueness for 3-way and N-way arrays". In R. Coppi & S. Bolasco (Eds.), Multiway data analysis (pp. 7â€“18). Amsterdam: Elsevier. [ PDF ]. â†©  â†©  â†©  â†©  â†©  â†©  â†©  â†©   â†©  â†©  â†©  â†©  â†©  â†©    Haiping Lu, K.N. Plataniotis and A.N. Venetsanopoulos, " A Survey of Multilinear Subspace Learning for Tensor Data ", Pattern Recognition, Vol. 44, No. 7, pp. 1540â€“1551, Jul. 2011. â†©  H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, " MPCA: Multilinear principal component analysis of tensor objects ," IEEE Trans. Neural Netw., vol. 19, no. 1, pp. 18â€“39, Jan. 2008. â†©  â†©     