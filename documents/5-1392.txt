


Minimum mean square error




Minimum mean square error

In statistics and signal processing, a minimum mean square error (MMSE) estimator is an estimation method which minimizes the mean square error (MSE) of the fitted values of a dependent variable, which is a common measure of estimator quality.
The term MMSE more specifically refers to estimation in a Bayesian setting with quadratic cost function. The basic idea behind the Bayesian approach to estimation stems from practical situations where we often have some prior information about the parameter to be estimated. For instance, we may have prior information about the range that the parameter can assume; or we may have an old estimate of the parameter that we want to modify when a new observation is made available; or the statistics of an actual random signal such as speech. This is in contrast to the non-Bayesian approach like minimum-variance unbiased estimator (MVUE) where absolutely nothing is assumed to be known about the parameter in advance and which does not account for such situations. In the Bayesian approach, such prior information is captured by the prior probability density function of the parameters; and based directly on Bayes theorem, it allows us to make better posterior estimates as more observations become available. Thus unlike non-Bayesian approach where parameters of interest are assumed to be deterministic, but unknown constants, the Bayesian estimator seeks to estimate a parameter that is itself a random variable. Furthermore, Bayesian estimation can also deal with situations where the sequence of observations are not necessarily independent. Thus Bayesian estimation provides yet another alternative to the MVUE. This is useful when the MVUE does not exist or cannot be found.
Definition
Let 
 
 
 
  be a 
 
 
 
  hidden random vector variable, and let 
 
 
 
  be a 
 
 
 
  known random vector variable (the measurement or observation), both of them not necessarily of the same dimension. An estimator

 
  of 
 
 
 
  is any function of the measurement 
 
 
 
 . The estimation error vector is given by 
 
 
 
  and its mean squared error (MSE) is given by the trace of error covariance matrix


 
 , where the expectation

 
  is taken over both 
 
 
 
  and 
 
 
 
 . When 
 
 
 
  is a scalar variable, the MSE expression simplifies to 
 
 
 
 . Note that MSE can equivalently be defined in other ways, since



The MMSE estimator is then defined as the estimator achieving minimal MSE.
Properties

Under some weak regularity assumptions,1 the MMSE estimator is uniquely defined, and is given by








In other words, the MMSE estimator is the conditional expectation of 
 
 
 
  given the known observed value of the measurements.
 


The MMSE estimator is unbiased (under the regularity assumptions mentioned above):










The MMSE estimator is asymptotically unbiased and it converges in distribution to the normal distribution:








where 
 
 
 
  is the Fisher information of 
 
 
 
 . Thus, the MMSE estimator is asymptotically efficient.
 


The orthogonality principle: When 
 
 
 
  is a scalar, an estimator constrained to be of certain form 
 
 
 
  is an optimal estimator, i.e. 
 
 
 
  if and only if








for all 
 
 
 
  in closed, linear subspace 
 
 
 
  of the measurements. For random vectors, since the MSE for estimation of a random vector is the sum of the MSEs of the coordinates, finding the MMSE estimator of a random vector decomposes into finding the MMSE estimators of the coordinates of X separately:
 
 



for all i and j. More succinctly put, the cross-correlation between the minimum estimation error 
 
 
 
  and the estimator 
 
 
 
  should be zero,
 
 





If 
 
 
 
  and 
 
 
 
  are jointly Gaussian, then the MMSE estimator is linear, i.e., it has the form 
 
 
 
  for matrix 
 
 
 
  and constant 
 
 
 
 . This can be directly shown using the Bayes theorem. As a consequence, to find the MMSE estimator, it is sufficient to find the linear MMSE estimator.

Linear MMSE estimator
In many cases, it is not possible to determine a closed form expression for the conditional expectation 
 
 
 
  required to obtain the MMSE estimator. Direct numerical evaluation of the conditional expectation is computationally expensive, since they often require multidimensional integration usually done using Monte Carlo methods. In such cases, one possibility is to abandon the full optimality requirements and seek a technique minimizing the MSE within a particular class of estimators, such as the class of linear estimators. Thus we postulate that the conditional expectation of 
 
 
 
  given 
 
 
 
  is a simple linear function of 
 
 
 
 , 
 
 
 
 , where the measurement 
 
 
 
  is a random vector, 
 
 
 
  is a matrix and 
 
 
 
  is a vector. The linear MMSE estimator is the estimator achieving minimum MSE among all estimators of such form. One advantage of such linear MMSE estimator is that it is not necessary to explicitly calculate the posterior probability density function of 
 
 
 
 . Such linear estimator only depends on the first two moments of the probability density function. So although it may be convenient to assume that 
 
 
 
  and 
 
 
 
  are jointly Gaussian, it is not necessary to make this assumption, so long as the assumed distribution has well defined first and second moments. The form of the linear estimator does not depend on the type of the assumed underlying distribution.
The expression for optimal 
 
 
 
  and 
 
 
 
  is given by






Thus the expression for linear MMSE estimator, its mean, and its auto-covariance is given by









where 
 
 
 
 , 
 
 
 
  the 
 
 
 
  is cross-covariance matrix between 
 
 
 
  and 
 
 
 
 , the 
 
 
 
  is auto-covariance matrix of 
 
 
 
 , and the 
 
 
 
  is cross-covariance matrix between 
 
 
 
  and 
 
 
 
 . Lastly, the error covariance and minimum mean square error achievable by such estimator is






For the special case when both 
 
 
 
  and 
 
 
 
  are scalars, the above relations simplify to






Let us have the optimal linear MMSE estimator given as 
 
 
 
 , where we are required to find the expression for 
 
 
 
  and 
 
 
 
 . It is required that the MMSE estimator be unbiased. This means,



Plugging the expression for 
 
 
 
  in above, we get



where 
 
 
 
  and 
 
 
 
 . Thus we can re-write the estimator as



and the expression for estimation error becomes



From the orthogonality principle, we can have 
 
 
 
 , where we take 
 
 
 
 . Here the left hand side term is



When equated to zero, we obtain the desired expression for 
 
 
 
  as



The 
 
 
 
  is cross-covariance matrix between X and Y, and 
 
 
 
  is auto-covariance matrix of Y. Since 
 
 
 
 , the expression can also be re-written in terms of 
 
 
 
  as



Thus the full expression for the linear MMSE estimator is



Since the estimate 
 
 
 
  is itself a random variable with 
 
 
 
 , we can also obtain its auto-covariance as



Putting the expression for 
 
 
 
  and 
 
 
 
 , we get



Lastly, the covariance of linear MMSE estimation error will then be given by



The first term in the third line is zero due to the orthogonality principle. Since 
 
 
 
 , we can re-write 
 
 
 
  in terms of covariance matrices as



This we can recognize to be the same as 
 
 
 
  Thus the minimum mean square error achievable by such a linear estimator is


 
 .
Standard method like Gauss elimination can be used to solve the matrix equation for 
 
 
 
 . A more numerically stable method is provided by QR decomposition method. Since the matrix 
 
 
 
  is a symmetric positive definite matrix, 
 
 
 
  can be solved twice as fast with the Cholesky decomposition, while for large sparse systems conjugate gradient method is more effective. Levinson recursion is a fast method when 
 
 
 
  is also a Toeplitz matrix. This can happen when 
 
 
 
  is a wide sense stationary process. In such stationary cases, these estimators are also referred to as Wiener-Kolmogorov filters.
Linear MMSE estimator for linear observation process
Let us further model the underlying process of observation as a linear process
 
 
 
 , where 
 
 
 
  is a known matrix and 
 
 
 
  is random noise vector with the mean 
 
 
 
  and cross-covariance 
 
 
 
 . Here the required mean and the covariance matrices will be









Thus the expression for the linear MMSE estimator matrix 
 
 
 
  further modifies to



Putting everything into the expression for 
 
 
 
 , we get



Lastly, the error covariance is



The significant difference between the estimation problem treated above and those of least squares and Gauss-Markov estimate is that the number of observations m, (i.e. the dimension of 
 
 
 
 ) need not be at least as large as the number of unknowns, n, (i.e. the dimension of 
 
 
 
 ). The estimate for the linear observation process exists so long as the m-by-m matrix 
 
 
 
  exists; this is the case for any m if, for instance, 
 
 
 
  is positive definite. Physically the reason for this property is that since 
 
 
 
  is now a random variable, it is possible to form a meaningful estimate (namely its mean) even with no measurements. Every new measurement simply provides additional information which may modify our original estimate. Another feature of this estimate is that for m C_Z = 0, because as long as 
 
 
 
  is positive definite, the estimate still exists. Lastly, this technique can handle cases where the noise is correlated, or in other words, when the noise is non-white.
Alternative form
An alternative form of expression can be obtained by using the matrix identity


 
  which can be established by post-multiplying by 
 
 
 
  and pre-multiplying by 
 
 
 
  to obtain


 
  and



Since 
 
 
 
  can now be written in terms of 
 
 
 
  as 
 
 
 
 , we get a simplified expression for 
 
 
 
  as



In this form the above expression can be easily compared with weighed least square and Gauss-Markov estimate. In particular, when 
 
 
 
 , corresponding to infinite variance of the apriori information concerning 
 
 
 
 , the result 
 
 
 
  is identical to the weighed linear least square estimate with 
 
 
 
  as the weight matrix. Moreover, if the components of 
 
 
 
  are uncorrelated and have equal variance such that 
 
 
 
  where 
 
 
 
  is an identity matrix, then 
 
 
 
  which has the same expression as the ordinary least square estimate.
Sequential linear MMSE estimation
For stationary process
In many real-time application, observational data is not available in a single batch. Instead the observations are made in a sequence. A naive application of previous formulas would have us discard an old estimate and recompute a new estimate as fresh data is made available. But then we lose all information provided by the old observation. When the observations are scalar quantities, one possible way of avoiding such re-computation is to first concatenate the entire sequence of observations and then apply the standard estimation formula as done in Example 2. But this can be very tedious because as the number of observation increases so does the size of the matrices that need to be inverted and multiplied grow. Also, this method is difficult to extend to the case of vector observations. Another approach to estimation from sequential observations is to simply update an old estimate as additional data becomes available, leading to finer estimates. Thus a recursive method is desired where the new measurements can modify the old estimates. Implicit in these discussions is the assumption that the statistical properties of 
 
 
 
  does not change with time. In other words, 
 
 
 
  is stationary.
For sequential estimation, if we have an estimate 
 
 
 
  based on measurements generating space 
 
 
 
 , then after receiving another set of measurements, we should subtract out from these measurements that part that could be anticipated from the result of the first measurements. In other words, the updating must be based on that part of the new data which is orthogonal to the old data.
Suppose an optimal estimate 
 
 
 
  has been formed on the basis of past measurements and that error covariance matrix is 
 
 
 
 . For linear observation processes the best estimate of 
 
 
 
  based on past observation, and hence old estimate 
 
 
 
 , is 
 
 
 
 . Subtracting 
 
 
 
  from 
 
 
 
 , we obtain 
 
 
 
 . The new estimate based on additional data is now


 
  where 
 
 
 
  is the cross-covariance between 
 
 
 
  and 
 
 
 
  and 
 
 
 
  is the auto-covariance of 
 
 

Using the fact that 
 
 
 
  and 
 
 
 
 , we can obtain the covariance matrices in terms of error covariance as





 
  Putting everything together, we have the new estimate as


 
  and the new error covariance as



The repeated use of the above two equations as more observations become available lead to recursive estimation techniques. The expressions can be more compactly written as
:#
 
 

:#
 
 

:#
 
 

The matrix 
 
 
 
  is often referred to as the gain factor. The repetition of these three steps as more data becomes available leads to an iterative estimation algorithm.
For example, an easy to use recursive expression can be derived when at each m-th time instant the underlying linear observation process yields a scalar such that 
 
 
 
 , where 
 
 
 
  is 1-by-n known row vector whose values can change with time, 
 
 
 
  is n-by-1 random column vector to be estimated, and 
 
 
 
  is scalar noise term with variance 
 
 
 
 . After (m+1)-th observation, the direct use of above recursive equations give the expression for the estimate 
 
 
 
  as:



where 
 
 
 
  is the new scalar observation and the gain factor 
 
 
 
  is n-by-1 column vector given by


 
  The 
 
 
 
  is n-by-n error covariance matrix given by



Here no matrix inversion is required. Also the gain factor 
 
 
 
  depends on our confidence in the new data sample, as measured by the noise variance, versus that in the previous data. The initial values of 
 
 
 
  and 
 
 
 
  are taken to be the mean and covariance of the aprior probability density function of 
 
 
 
 .
Examples
Example 1
We shall take a linear prediction problem as an example. Let a linear combination of observed scalar random variables 
 
 
 
  and 
 
 
 
  be used to estimate another future scalar random variable 
 
 
 
  such that 
 
 
 
 . If the random variables 
 
 
 
  are real Gaussian random variables with zero mean and its covariance matrix given by


 
  then our task is to find the coefficients 
 
 
 
  such that it will yield an optimal linear estimate 
 
 
 
 .
In terms of the terminology developed in the previous section, for this problem we have the observation vector 
 
 
 
 , the estimator matrix 
 
 
 
  as a row vector, and the estimated variable 
 
 
 
  as a scalar quantity. The autocorrelation matrix 
 
 
 
  is defined as


 
  The cross correlation matrix 
 
 
 
  is defined as



We now solve the equation 
 
 
 
  by inverting 
 
 
 
  and pre-multiplying to get



So we have 
 
 

 
  and 
 
 
 
  as the optimal coefficients for 
 
 
 
 . Computing the minimum mean square error then gives 
 
 
 
 .2 Note that it is not necessary to obtain an explicit matrix inverse of 
 
 
 
  to compute the value of 
 
 
 
 . The matrix equation can be solved by well known methods such as Gauss elimination method. A shorter, non-numerical example can be found in orthogonality principle.
Example 2
Consider a vector 
 
 
 
  formed by taking 
 
 
 
  observations of a fixed but unknown scalar parameter 
 
 
 
  disturbed by white Gaussian noise. We can describe the process by a linear equation 
 
 
 
 , where 
 
 
 
 . Depending on context it will be clear if 
 
 
 
  represents a scalar or a vector. Suppose that we know 
 
 
 
  to be the range within which the value of 
 
 
 
  is going to fall in. We can model our uncertainty of 
 
 
 
  by an aprior uniform distribution over an interval 
 
 
 
 , and thus 
 
 
 
  will have variance of 
 
 
 
 . Let the noise vector 
 
 
 
  be normally distributed as 
 
 
 
  where 
 
 
 
  is an identity matrix. Also 
 
 
 
  and 
 
 
 
  are independent and 
 
 
 
 . It is easy to see that



Thus, the linear MMSE estimator is given by


 
 
  We can simplify the expression by using the alternative form for 
 
 
 
  as



where for 
 
 
 
  we have 
 
 

Similarly, the variance of the estimator is



Thus the MMSE of this linear estimator is



For very large 
 
 
 
 , we see that the MMSE estimator of a scalar unknown random variable with uniform aprior distribution can be approximated by the arithmetic average of all the observed data


 
  while the variance will be unaffected by data 
 
 
 
  and the LMMSE of the estimate will tend to zero.
However, the estimator is suboptimal since it is constrained to be linear. Had the random variable 
 
 
 
  also been Gaussian, then the estimator would have been optimal. Notice, that the form of the estimator will remain unchanged, regardless of the apriori distribution of 
 
 
 
 , so long as the mean and variance of these distributions are the same.
Example 3
Consider a variation of the above example: Two candidates are standing for an election. Let the fraction of votes that a candidate will receive on an election day be 
 
 
 
  Thus the fraction of votes the other candidate will receive will be 
 
 
 
  We shall take 
 
 
 
  as a random variable with a uniform prior distribution over 
 
 
 
  so that its mean is 
 
 
 
  and variance is 
 
 
 
  A few weeks before the election, two independent public opinion polls were conducted by two different pollsters. The first poll revealed that the candidate is likely to get 
 
 
 
  fraction of votes. Since some error is always present due to finite sampling and the particular polling methodology adopted, the first pollster declares their estimate to have an error 
 
 
 
  with zero mean and variance 
 
 
 
  Similarly, the second pollster declares their estimate to be 
 
 
 
  with an error 
 
 
 
  with zero mean and variance 
 
 
 
  Note that except for the mean and variance of the error, the error distribution is unspecified. How should the two polls be combined to obtain the voting prediction for the given candidate?
As with previous example, we have



Here both the 
 
 
 
 . Thus we can obtain the LMMSE estimate as the linear combination of 
 
 
 
  and 
 
 
 
  as


 
  where the weights are given by


 
 
  Here since the denominator term is constant, the poll with lower error is given higher weight in order to predict the election outcome. Lastly, the variance of the prediction is given by


 
  which makes 
 
 
 
 
  smaller than 
 
 

In general, if we have 
 
 
 
  pollsters, then the weight for i-th pollster is is given by 
 
 

Example 4
Suppose that a musician is playing an instrument and that the sound is received by two microphones, each of them located at two different places. Let the attenuation of sound due to distance at each microphone be 
 
 
 
  and 
 
 
 
 
 , which are assumed to be known constants. Similarly, let the noise at each microphone be 
 
 
 
  and 
 
 
 
 , each with zero mean and variances 
 
 
 
  and 
 
 
 
  respectively. Let 
 
 
 
 
  denote the sound produced by the musician, which is a random variable with zero mean and variance 
 
 
 
  How should the recorded music from these two microphones be combined, after being synced with each other?
We can model the sound received by each microphone as



Here both the 
 
 
 
 . Thus, we can combine the two sounds as


 
  where the i-th weight is given as



See also

Bayesian estimator
Mean squared error
Least squares
Minimum-variance unbiased estimator (MVUE)
Orthogonality principle
Wiener filter
Kalman filter
Linear prediction
Zero forcing equalizer

Notes


Further reading











"
Category:Statistical deviation and dispersion Category:Estimation theory Category:Signal processing



Lehmann and Casella, Corollary 4.1.2.↩
Moon and Stirling.↩




