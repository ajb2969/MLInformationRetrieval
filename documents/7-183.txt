   Rank correlation      Rank correlation   In statistics , a rank correlation is any of several statistics that measure the relationship between rankings of different ordinal variables or different rankings of the same variable, where a "ranking" is the assignment of the labels "first", "second", "third", etc. to different observations of a particular variable. A rank correlation coefficient measures the degree of similarity between two rankings, and can be used to assess the significance of the relation between them. For example, two common nonparametric methods of significance that use rank correlation are the Mann–Whitney U test and the Wilcoxon signed-rank test .  Context  If, for example, one variable is the identity of a college basketball program and another variable is the identity of a college football program, one could test for a relationship between the poll rankings of the two types of program: do colleges with a higher-ranked basketball program tend to have a higher-ranked football program? A rank correlation coefficient can measure that relationship, and the measure of significance of the rank correlation coefficient can show whether the measured relationship is small enough to likely be a coincidence.  If there is only one variable, the identity of a college football program, but it is subject to two different poll rankings (say, one by coaches and one by sportswriters), then the similarity of the two different polls' rankings can be measured with a rank correlation coefficient.  Correlation coefficients  Some of the more popular rank correlation statistics include   Spearman's ρ  Kendall's τ  Goodman and Kruskal's γ   An increasing rank correlation coefficient implies increasing agreement between rankings. The coefficient is inside the interval [−1, 1] and assumes the value:   1 if the agreement between the two rankings is perfect; the two rankings are the same.  0 if the rankings are completely independent.  −1 if the disagreement between the two rankings is perfect; one ranking is the reverse of the other.   Following , a ranking can be seen as a permutation of a set of objects. Thus we can look at observed rankings as data obtained when the sample space is (identified with) a symmetric group . We can then introduce a metric , making the symmetric group into a metric space . Different metrics will correspond to different rank correlations.  General correlation coefficient  showed that his   τ   τ   \tau   (tau) and Spearman's   ρ   ρ   \rho   (rho) are particular cases of a general correlation coefficient.  Suppose we have a set of   n   n   n   objects, which are being considered in relation to two properties, represented by   x   x   x   and   y   y   y   , forming the sets of values     {   x  i   }    i  ≤  n      subscript    subscript  x  i      i  n     \{x_{i}\}_{i\leq n}   and     {   y  i   }    i  ≤  n      subscript    subscript  y  i      i  n     \{y_{i}\}_{i\leq n}   . To any pair of individuals, say the   i   i   i   -th and the   j   j   j   -th we assign a   x   x   x   -score, denoted by    a   i  j      subscript  a    i  j     a_{ij}   , and a   y   y   y   -score, denoted by    b   i  j      subscript  b    i  j     b_{ij}   . The only requirement made to this functions is anti-symmetry, so     a   i  j    =   -   a   j  i          subscript  a    i  j       subscript  a    j  i       a_{ij}=-a_{ji}   and     b   i  j    =   -   b   j  i          subscript  b    i  j       subscript  b    j  i       b_{ij}=-b_{ji}   . Then the generalised correlation coefficient   Γ   normal-Γ   \Gamma   is defined by      Γ  =     ∑    i  ,  j   =  1   n     a   i  j     b   i  j         ∑    i  ,  j   =  1   n     a   i  j   2     ∑    i  ,  j   =  1   n    b   i  j   2            normal-Γ      superscript   subscript      i  j   1    n      subscript  a    i  j     subscript  b    i  j          superscript   subscript      i  j   1    n      superscript   subscript  a    i  j    2     superscript   subscript      i  j   1    n    superscript   subscript  b    i  j    2          \Gamma=\frac{\sum_{i,j=1}^{n}a_{ij}b_{ij}}{\sqrt{\sum_{i,j=1}^{n}a_{ij}^{2}%
 \sum_{i,j=1}^{n}b_{ij}^{2}}}     Kendall's   τ   τ   \tau   as a particular case  If    r  i     subscript  r  i    r_{i}   is the rank of the   i   i   i   -member according to the   x   x   x   -quality, we can define       a   i  j    =   sgn   (    r  j   -   r  i    )         subscript  a    i  j     sgn     subscript  r  j    subscript  r  i       a_{ij}=\operatorname{sgn}(r_{j}-r_{i})     and similarly for   b   b   b   . The sum    ∑    a   i  j     b   i  j            subscript  a    i  j     subscript  b    i  j       \sum a_{ij}b_{ij}   is twice the amount of concordant pairs minus the discordant pairs (see Kendall tau rank correlation coefficient ). The sum    ∑   a   i  j   2        superscript   subscript  a    i  j    2     \sum a_{ij}^{2}   is just the number of terms    a   i  j      subscript  a    i  j     a_{ij}   , equal to    n   (   n  -  1   )       n    n  1     n(n-1)   , and so for    ∑   b   i  j   2        superscript   subscript  b    i  j    2     \sum b_{ij}^{2}   . It follows that   Γ   normal-Γ   \Gamma   is equal to the Kendall's   τ   τ   \tau   coefficient.  Spearman's   ρ   ρ   \rho   as a particular case  If    r  i     subscript  r  i    r_{i}   ,    s  i     subscript  s  i    s_{i}   are the ranks of the   i   i   i   -member according to the   x   x   x   and the   y   y   y   -quality respectively, we can simply define       a   i  j    =    r  j   -   r  i         subscript  a    i  j       subscript  r  j    subscript  r  i      a_{ij}=r_{j}-r_{i}          b   i  j    =    s  j   -   s  i         subscript  b    i  j       subscript  s  j    subscript  s  i      b_{ij}=s_{j}-s_{i}     The sums    ∑   a   i  j   2        superscript   subscript  a    i  j    2     \sum a_{ij}^{2}   and    ∑   b   i  j   2        superscript   subscript  b    i  j    2     \sum b_{ij}^{2}   are equal, since both    r  i     subscript  r  i    r_{i}   and    s  i     subscript  s  i    s_{i}   range from   1   1   1   to   n   n   n   . Then we have:      Γ  =    ∑    (    r  j   -   r  i    )    (    s  j   -   s  i    )      ∑    (    r  j   -   r  i    )   2         normal-Γ           subscript  r  j    subscript  r  i       subscript  s  j    subscript  s  i         superscript     subscript  r  j    subscript  r  i    2       \Gamma=\frac{\sum(r_{j}-r_{i})(s_{j}-s_{i})}{\sum(r_{j}-r_{i})^{2}}     now        ∑    i  ,  j   =  1   n     (    r  j   -   r  i    )    (    s  j   -   s  i    )     =      ∑   i  =  1   n     ∑   j  =  1   n     r  i    s  i      +    ∑   i  =  1   n     ∑   j  =  1   n     r  j    s  j       -    ∑   i  =  1   n     ∑   j  =  1   n    (     r  i    s  j    +    r  j    s  i     )            superscript   subscript      i  j   1    n        subscript  r  j    subscript  r  i       subscript  s  j    subscript  s  i            superscript   subscript     i  1    n     superscript   subscript     j  1    n      subscript  r  i    subscript  s  i        superscript   subscript     i  1    n     superscript   subscript     j  1    n      subscript  r  j    subscript  s  j         superscript   subscript     i  1    n     superscript   subscript     j  1    n        subscript  r  i    subscript  s  j       subscript  r  j    subscript  s  i          \sum_{i,j=1}^{n}(r_{j}-r_{i})(s_{j}-s_{i})=\sum_{i=1}^{n}\sum_{j=1}^{n}r_{i}s_%
 {i}+\sum_{i=1}^{n}\sum_{j=1}^{n}r_{j}s_{j}-\sum_{i=1}^{n}\sum_{j=1}^{n}(r_{i}s%
 _{j}+r_{j}s_{i})          =    2  n    ∑   i  =  1   n     r  i    s  i      -   2    ∑   i  =  1   n     r  i     ∑   j  =  1   n    s  j            absent      2  n    superscript   subscript     i  1    n      subscript  r  i    subscript  s  i        2    superscript   subscript     i  1    n      subscript  r  i     superscript   subscript     j  1    n    subscript  s  j          =2n\sum_{i=1}^{n}r_{i}s_{i}-2\sum_{i=1}^{n}r_{i}\sum_{j=1}^{n}s_{j}          =    2  n    ∑   i  =  1   n     r  i    s  i      -    1  2    n  2     (   n  +  1   )   2         absent      2  n    superscript   subscript     i  1    n      subscript  r  i    subscript  s  i          1  2    superscript  n  2    superscript    n  1   2       =2n\sum_{i=1}^{n}r_{i}s_{i}-\frac{1}{2}n^{2}(n+1)^{2}   since    ∑   r  i        subscript  r  i     \sum r_{i}   and    ∑   s  j        subscript  s  j     \sum s_{j}   are both equal to the sum of the first   n   n   n   natural numbers, namely     1  2   n   (   n  +  1   )         1  2   n    n  1     \frac{1}{2}n(n+1)   .  We also have      S  =    ∑   i  =  1   n     (    r  i   -   s  i    )   2    =    2   ∑   r  i  2     -   2   ∑    r  i    s  i             S    superscript   subscript     i  1    n    superscript     subscript  r  i    subscript  s  i    2             2     superscript   subscript  r  i   2       2       subscript  r  i    subscript  s  i          S=\sum_{i=1}^{n}(r_{i}-s_{i})^{2}=2\sum r_{i}^{2}-2\sum r_{i}s_{i}   and hence       ∑    (    r  j   -   r  i    )    (    s  j   -   s  i    )     =    2  n   ∑   r  i  2     -    1  2    n  2     (   n  +  1   )   2    -   n  S               subscript  r  j    subscript  r  i       subscript  s  j    subscript  s  i          2  n     superscript   subscript  r  i   2         1  2    superscript  n  2    superscript    n  1   2      n  S      \sum(r_{j}-r_{i})(s_{j}-s_{i})=2n\sum r_{i}^{2}-\frac{1}{2}n^{2}(n+1)^{2}-nS       ∑   r  i  2        superscript   subscript  r  i   2     \sum r_{i}^{2}   being the sum of squares of the first   n   n   n   naturals equals     1  6   n   (   n  +  1   )    (    2  n   +  1   )         1  6   n    n  1       2  n   1     \frac{1}{6}n(n+1)(2n+1)   . Thus, the last equation reduces to       ∑    (    r  j   -   r  i    )    (    s  j   -   s  i    )     =     1  6    n  2    (    n  2   -  1   )    -   n  S               subscript  r  j    subscript  r  i       subscript  s  j    subscript  s  i            1  6    superscript  n  2      superscript  n  2   1      n  S      \sum(r_{j}-r_{i})(s_{j}-s_{i})=\frac{1}{6}n^{2}(n^{2}-1)-nS     Further       ∑    (    r  j   -   r  i    )   2    =    2  n   ∑   r  i  2     -   2   ∑    r  i    r  j              superscript     subscript  r  j    subscript  r  i    2        2  n     superscript   subscript  r  i   2       2       subscript  r  i    subscript  r  j         \sum(r_{j}-r_{i})^{2}=2n\sum r_{i}^{2}-2\sum r_{i}r_{j}          =    2  n   ∑   r  i  2     -   2    (   ∑   r  i    )   2     =    1  6    n  2    (    n  2   -  1   )          absent      2  n     superscript   subscript  r  i   2       2   superscript     subscript  r  i    2              1  6    superscript  n  2      superscript  n  2   1       =2n\sum r_{i}^{2}-2(\sum r_{i})^{2}=\frac{1}{6}n^{2}(n^{2}-1)     and thus, substituting into the original formula these results we get       Γ  R   =   1  -    6   ∑   d  i  2       n  3   -  n          subscript  normal-Γ  R     1      6     superscript   subscript  d  i   2        superscript  n  3   n       \Gamma_{R}=1-\frac{6\sum d_{i}^{2}}{n^{3}-n}   where      d  i   =    x  i   -   y  i     ,       subscript  d  i      subscript  x  i    subscript  y  i      d_{i}=x_{i}-y_{i},   is the difference between ranks.  which is exactly the Spearman's rank correlation coefficient    ρ   ρ   \rho   .  Rank-biserial correlation  Gene Glass (1965) noted that the rank-biserial can be derived from Spearman's   ρ   ρ   \rho   . "One can derive a coefficient defined on X, the dichotomous variable, and Y, the ranking variable, which estimates Spearman's rho between X and Y in the same way that biserial r estimates Pearson's r between two normal variables” (p. 91). The rank-biserial correlation had been introduced nine years before by Edward Cureton (1956) as a measure of rank correlation when the ranks are in two groups.  Kerby simple difference formula  Dave Kerby (2014) recommended the rank-biserial as the measure to introduce students to rank correlation, because the general logic can be explained at an introductory level. The rank-biserial is the correlation used with the Mann–Whitney U test , a method commonly covered in introductory college courses on statistics. The data for this test consists of two groups; and for each member of the groups, the outcome is ranked for the study as a whole.  Kerby showed that this rank correlation can be expressed in terms of two concepts: the percent of data that support a stated hypothesis, and the percent of data that do not support it. The Kerby simple difference formula states that the rank correlation can be expressed as the difference between the proportion of favorable evidence ( f ) minus the proportion of unfavorable evidence ( u ).      r  =   f  -  u       r    f  u     r=f-u     Example and interpretation  To illustrate the computation, suppose a coach trains long-distance runners for one month using two methods. Group A has 5 runners, and Group B has 4 runners. The stated hypothesis is that method A produces faster runners. The race to assess the results finds that the runners from Group A do indeed run faster, with the following ranks: 1, 2, 3, 4, and 6. The slower runners from Group B thus have ranks of 5, 7, 8, and 9.  The analysis is conducted on pairs, defined as a member of one group compared to a member of the other group. For example, the fastest runner in the study is a member of four pairs: (1,5), (1,7), (1,8), and (1,9). All four of these pairs support the hypothesis, because in each pair the runner from Group A is faster than the runner from Group B. There are a total of 20 pairs, and 19 pairs support the hypothesis. The only pair that does not support the hypothesis are the two runners with ranks 5 and 6, because in this pair, the runner from Group B had the faster time. By the Kerby simple difference formula, 95% of the data support the hypothesis (19 of 20 pairs), and 5% do not support (1 of 20 pairs), so the rank correlation is r = .95 - .05 = .90.  The maximum value for the correlation is r = 1, which means that 100% of the pairs favor the hypothesis. A correlation of r = 0 indicates that half the pairs favor the hypothesis and half do not; in other words, the groups do not differ in ranks, so there is no evidence that the two groups differ. An effect size of r = 0 can be said to describe no relationship between group membership and the members' ranks.  References   Cureton, E. E. (1956). Rank-biserial correlation. Psychometrika 21, 287-290.    Glass, G. V. (1965). A ranking variable analogue of biserial correlation: implications for short-cut item analysis. Journal of Educational Measurement , 2(1), 91–95. DOI: 10.1111/j.1745-3984.1965.tb00396.x   Kerby, D. S. (2014). The simple difference formula: An approach to teaching nonparametric correlation. Innovative Teaching , volume 3, article 1. doi:10.2466/11.CP.3.1 . link to pdf   "  Category:Covariance and correlation  Category:Non-parametric statistics  Category:Statistical dependence   