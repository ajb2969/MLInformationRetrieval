   Decoding methods      Decoding methods   In coding theory , decoding is the process of translating received messages into codewords of a given code . There have been many common methods of mapping messages to codewords. These are often used to recover messages sent over a noisy channel , such as a binary symmetric channel .  Notation      C  ‚äÇ   ùîΩ  2  n       C   superscript   subscript  ùîΩ  2   n     C\subset\mathbb{F}_{2}^{n}   is considered a binary code with the length   n   n   n   ;    x  ,  y     x  y    x,y   shall be elements of    ùîΩ  2  n     superscript   subscript  ùîΩ  2   n    \mathbb{F}_{2}^{n}   ; and    d   (  x  ,  y  )       d   x  y     d(x,y)   is the distance between those elements.  Ideal observer decoding  One may be given the message    x  ‚àà   ùîΩ  2  n       x   superscript   subscript  ùîΩ  2   n     x\in\mathbb{F}_{2}^{n}   , then ideal observer decoding generates the codeword    y  ‚àà  C      y  C    y\in C   . The process results in this solution:      ‚Ñô   (  y  sent  ‚à£  x  received  )      fragments  P   fragments  normal-(  y  sent  normal-‚à£  x  received  normal-)     \mathbb{P}(y\mbox{ sent}\mid x\mbox{ received})     For example, a person can choose the codeword   y   y   y   that is most likely to be received as the message   x   x   x   after transmission.  Decoding conventions  Each codeword does not have an expected possibility: there may be more than one codeword with an equal likelihood of mutating into the received message. In such a case, the sender and receiver(s) must agree ahead of time on a decoding convention. Popular conventions include:  :# Request that the codeword be resent -- automatic repeat-request  :# Choose any random codeword from the set of most likely codewords which is nearer to that.  Maximum likelihood decoding  Given a received codeword    x  ‚àà   ùîΩ  2  n       x   superscript   subscript  ùîΩ  2   n     x\in\mathbb{F}_{2}^{n}    maximum likelihood decoding picks a codeword    y  ‚àà  C      y  C    y\in C   that maximizes      ‚Ñô   (  x  received  ‚à£  y  sent  )      fragments  P   fragments  normal-(  x  received  normal-‚à£  y  sent  normal-)     \mathbb{P}(x\mbox{ received}\mid y\mbox{ sent})   ,  that is, the codeword   y   y   y   that maximizes the probability that   x   x   x   was received, given that    y   y   y   was sent. If all codewords are equally likely to be sent then this scheme is equivalent to ideal observer decoding. In fact, by Bayes Theorem ,      ‚Ñô   (  x  received  ‚à£  y  sent  )      fragments  P   fragments  normal-(  x  received  normal-‚à£  y  sent  normal-)     \displaystyle\mathbb{P}(x\mbox{ received}\mid y\mbox{ sent})     Upon fixing    ‚Ñô   (   x  received   )       ‚Ñô    x  received     \mathbb{P}(x\mbox{ received})   ,   x   x   x   is restructured and    ‚Ñô   (   y  sent   )       ‚Ñô    y  sent     \mathbb{P}(y\mbox{ sent})   is constant as all codewords are equally likely to be sent. Therefore    ‚Ñô   (  x  received  ‚à£  y  sent  )      fragments  P   fragments  normal-(  x  received  normal-‚à£  y  sent  normal-)     \mathbb{P}(x\mbox{ received}\mid y\mbox{ sent})   is maximised as a function of the variable   y   y   y   precisely when    ‚Ñô   (  y  sent  ‚à£  x  received  )      fragments  P   fragments  normal-(  y  sent  normal-‚à£  x  received  normal-)     \mathbb{P}(y\mbox{ sent}\mid x\mbox{ received})   is maximised, and the claim follows.  As with ideal observer decoding, a convention must be agreed to for non-unique decoding.  The maximum likelihood decoding problem can also be modeled as an integer programming problem. 1  The maximum likelihood decoding algorithm is an instance of the "marginalize a product function" problem which is solved by applying the generalized distributive law . 2  Minimum distance decoding  Given a received codeword    x  ‚àà   ùîΩ  2  n       x   superscript   subscript  ùîΩ  2   n     x\in\mathbb{F}_{2}^{n}   , minimum distance decoding picks a codeword    y  ‚àà  C      y  C    y\in C   to minimise the Hamming distance :       d   (  x  ,  y  )    =   #   {  i  :    x  i   ‚â†   y  i    }          d   x  y      normal-#   conditional-set  i     subscript  x  i    subscript  y  i        d(x,y)=\#\{i:x_{i}\not=y_{i}\}     i.e. choose the codeword   y   y   y   that is as close as possible to   x   x   x   .  Note that if the probability of error on a discrete memoryless channel    p   p   p   is strictly less than one half, then minimum distance decoding is equivalent to maximum likelihood decoding , since if        d   (  x  ,  y  )    =  d   ,        d   x  y    d    d(x,y)=d,\,     then:      ‚Ñô   (  y  received  ‚à£  x  sent  )      fragments  P   fragments  normal-(  y  received  normal-‚à£  x  sent  normal-)     \displaystyle\mathbb{P}(y\mbox{ received}\mid x\mbox{ sent})     which (since p is less than one half) is maximised by minimising d .  Minimum distance decoding is also known as nearest neighbour decoding . It can be assisted or automated by using a standard array . Minimum distance decoding is a reasonable decoding method when the following conditions are met:  :#The probability   p   p   p   that an error occurs is independent of the position of the symbol  :#Errors are independent events - an error at one position in the message does not affect other positions  These assumptions may be reasonable for transmissions over a binary symmetric channel . They may be unreasonable for other media, such as a DVD, where a single scratch on the disk can cause an error in many neighbouring symbols or codewords.  As with other decoding methods, a convention must be agreed to for non-unique decoding.  Syndrome decoding  Syndrome decoding is a highly efficient method of decoding a linear code over a noisy channel - i.e. one on which errors are made. In essence, syndrome decoding is minimum distance decoding using a reduced lookup table. This is allowed by the linearity of the code.  Suppose that    C  ‚äÇ   ùîΩ  2  n       C   superscript   subscript  ùîΩ  2   n     C\subset\mathbb{F}_{2}^{n}   is a linear code of length   n   n   n   and minimum distance   d   d   d   with parity-check matrix    H   H   H   . Then clearly   C   C   C   is capable of correcting up to      t  =   ‚åä    d  -  1   2   ‚åã       t        d  1   2      t=\left\lfloor\frac{d-1}{2}\right\rfloor     errors made by the channel (since if no more than   t   t   t   errors are made then minimum distance decoding will still correctly decode the incorrectly transmitted codeword).  Now suppose that a codeword    x  ‚àà   ùîΩ  2  n       x   superscript   subscript  ùîΩ  2   n     x\in\mathbb{F}_{2}^{n}   is sent over the channel and the error pattern    e  ‚àà   ùîΩ  2  n       e   superscript   subscript  ùîΩ  2   n     e\in\mathbb{F}_{2}^{n}   occurs. Then    z  =   x  +  e       z    x  e     z=x+e   is received. Ordinary minimum distance decoding would lookup the vector   z   z   z   in a table of size    |  C  |      C    |C|   for the nearest match - i.e. an element (not necessarily unique)    c  ‚àà  C      c  C    c\in C   with       d   (  c  ,  z  )    ‚â§   d   (  y  ,  z  )          d   c  z      d   y  z      d(c,z)\leq d(y,z)     for all    y  ‚àà  C      y  C    y\in C   . Syndrome decoding takes advantage of the property of the parity matrix that:       H  x   =  0        H  x   0    Hx=0     for all    x  ‚àà  C      x  C    x\in C   . The syndrome of the received    z  =   x  +  e       z    x  e     z=x+e   is defined to be:       H  z   =   H   (   x  +  e   )    =    H  x   +   H  e    =   0  +   H  e    =   H  e           H  z     H    x  e             H  x     H  e           0    H  e           H  e      Hz=H(x+e)=Hx+He=0+He=He     Under the assumption that no more than   t   t   t   errors were made during transmission, the receiver looks up the value    H  e      H  e    He   in a table of size            ‚àë   i  =  0   t     (       n      i       )    <   |  C  |              superscript   subscript     i  0    t    binomial  n  i      C       \begin{matrix}\sum_{i=0}^{t}{\left({{n}\atop{i}}\right)}<|C|\\
 \end{matrix}     (for a binary code) against pre-computed values of    H  e      H  e    He   for all possible error patterns    e  ‚àà   ùîΩ  2  n       e   superscript   subscript  ùîΩ  2   n     e\in\mathbb{F}_{2}^{n}   . Knowing what   e   e   e   is, it is then trivial to decode   x   x   x   as:      x  =   z  -  e       x    z  e     x=z-e     Partial response maximum likelihood  Partial response maximum likelihood ( PRML ) is a method for converting the weak analog signal from the head of a magnetic disk or tape drive into a digital signal.  Viterbi decoder  A Viterbi decoder uses the Viterbi algorithm for decoding a bitstream that has been encoded using forward error correction based on a convolutional code. The Hamming distance is used as a metric for hard decision Viterbi decoders. The squared  Euclidean distance is used as a metric for soft decision decoders.  See also   Error detection and correction   Sources       References  "  Category:Coding theory     ‚Ü©  ‚Ü©     