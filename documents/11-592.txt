   Conductance (graph)      Conductance (graph)   In graph theory the conductance of a graph  G =( V , E ) measures how "well-knit" the graph is: it controls how fast a random walk on G converges to a uniform distribution . The conductance of a graph is often called the Cheeger constant of a graph as the analog of its counterpart in spectral geometry . Since electrical networks are intimately related to random walks with a long history in the usage of the term "conductance", this alternative name helps avoid possible confusion.  The conductance of a cut     (  S  ,   S  ¯   )     S   normal-¯  S     (S,\bar{S})   in a graph is defined as:       φ   (  S  )    =     ∑    i  ∈  S   ,   j  ∈   S  ¯       a   i  j      min   (   a   (  S  )    ,   a   (   S  ¯   )    )           φ  S       subscript    formulae-sequence    i  S     j   normal-¯  S       subscript  a    i  j         a  S     a   normal-¯  S        \varphi(S)=\frac{\sum_{i\in S,j\in\bar{S}}a_{ij}}{\min(a(S),a(\bar{S}))}     where the    a   i  j      subscript  a    i  j     a_{ij}   are the entries of the adjacency matrix for G , so that       a   (  S  )    =    ∑   i  ∈  S      ∑   j  ∈  V     a   i  j            a  S     subscript     i  S      subscript     j  V     subscript  a    i  j        a(S)=\sum_{i\in S}\sum_{j\in V}a_{ij}     is the total number (or weight) of the edges incident with S .  The conductance of the whole graph is the minimum conductance over all the possible cuts:        ϕ   (  G  )    =     min   S  ⊆  V    φ    (  S  )     .        ϕ  G       subscript     S  V    φ   S     \phi(G)=\min_{S\subseteq V}\varphi(S).     Equivalently, conductance of a graph is defined as follows:        ϕ   (  G  )    :=    min    S  ⊆  V   ;   0  ≤   a   (  S  )    ≤    a   (  V  )    /  2         ∑    i  ∈  S   ,   j  ∈   S  ¯       a   i  j      a   (  S  )       .     assign    ϕ  G     subscript    formulae-sequence    S  V       0    a  S            a  V   2           subscript    formulae-sequence    i  S     j   normal-¯  S       subscript  a    i  j       a  S       \phi(G):=\min_{S\subseteq V;0\leq a(S)\leq a(V)/2}\frac{\sum_{i\in S,j\in\bar{%
 S}}a_{ij}}{a(S)}.\,     For a d -regular graph, the conductance is equal to the isoperimetric number divided by d .  Generalizations and applications  In practical applications, one often considers the conductance only over a cut. A common generalization of conductance is to handle the case of weights assigned to the edges: then the weights are added; if the weight is in the form of a resistance, then the reciprocal weights are added.  The notion of conductance underpins the study of percolation in physics and other applied areas; thus, for example, the permeability of petroleum through porous rock can be modeled in terms of the conductance of a graph, with weights given by pore sizes.  Markov chains  For an ergodic reversible Markov chain with an underlying graph  G , the conductance is a way to measure how hard it is to leave a small set of nodes. Formally, the conductance of a graph is defined as the minimum over all sets   S   S   S   of the capacity of   S   S   S   divided by the ergodic flow out of   S   S   S   . Alistair Sinclair showed that conductance is closely tied to mixing time in ergodic reversible Markov chains. We can also view conductance in a more probabilistic way, as the minimal probability of leaving a small set of nodes given that we started in that set to begin with. Writing    Φ  S     subscript  normal-Φ  S    \Phi_{S}   for the conditional probability of leaving a set of nodes S given that we were in that set to begin with, the conductance is the minimal    Φ  S     subscript  normal-Φ  S    \Phi_{S}   over sets   S   S   S   that have a total stationary probability of at most 1/2.  Conductance is related to Markov chain mixing time in the reversible setting.  See also   Resistance distance  Percolation  Krackhardt E/I Ratio   References      A. Sinclair. Algorithms for Random Generation and Counting: A Markov Chain Approach. Birkhauser, Boston-Basel-Berlin, 1993.  D. Levin, Y. Peres, E. L. Wilmer: Markov Chains and Mixing Times   "  Category:Probability theory  Category:Algebraic graph theory  Category:Matrices  Category:Graph invariants   