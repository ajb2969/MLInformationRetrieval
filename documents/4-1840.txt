   Detection theory      Detection theory   Detection theory , or signal detection theory , is a means to quantify the ability to discern between information-bearing patterns (called stimulus in humans, signal in machines) and random patterns that distract from the information (called noise , consisting of background stimuli and random activity of the detection machine and of the nervous system of the operator). In the field of electronics , the separation of such patterns from a disguising background is referred to as signal recovery . 1  According to the theory, there are a number of determiners of how a detecting system will detect a signal, and where its threshold levels will be. The theory can explain how changing the threshold will affect the ability to discern, often exposing how adapted the system is to the task, purpose or goal at which it is aimed.  When the detecting system is a human being, experience, expectations, physiological state (e.g., fatigue) and other factors can affect the threshold applied. For instance, a sentry in wartime might be likely to detect fainter stimuli than the same sentry in peacetime due to a lower criterion, however they might also be more likely to treat innocuous stimuli as a threat.  Much of the early work in detection theory was done by radar researchers. 2 By 1954, the theory was fully developed on the theoretical side as described by Peterson , Birdsall and Fox 3 and the foundation for the psychological theory was made by Wilson P. Tanner, David M. Green, and John A. Swets , also in 1954. 4 Detection theory was used in 1966 by John A. Swets and David M. Green for psychophysics . 5 Green and Swets criticized the traditional methods of psychophysics for their inability to discriminate between the real sensitivity of subjects and their (potential) response biases . Green, D.M., Swets J.A. (1966) Signal Detection Theory and Psychophysics . New York: Wiley. (ISBN 0-471-32420-5)  Detection theory has applications in many fields such as diagnostics of any kind, quality control , telecommunications , and psychology . The concept is similar to the signal to noise ratio used in the sciences and confusion matrices used in artificial intelligence . It is also usable in alarm management , where it is important to separate important events from background  noise .  Psychology  Signal detection theory (SDT) is used when psychologists want to measure the way we make decisions under conditions of uncertainty, such as how we would perceive distances in foggy conditions. SDT assumes that the decision maker is not a passive receiver of information, but an active decision-maker who makes difficult perceptual judgments under conditions of uncertainty. In foggy circumstances, we are forced to decide how far away from us an object is, based solely upon visual stimulus which is impaired by the fog. Since the brightness of the object, such as a traffic light, is used by the brain to discriminate the distance of an object, and the fog reduces the brightness of objects, we perceive the object to be much farther away than it actually is (see also decision theory ).  To apply signal detection theory to a data set where stimuli were either present or absent, and the observer categorized each trial as having the stimulus present or absent, the trials are sorted into one of four categories:         Respond "Absent"   Respond "Present"       Stimulus Present   Miss   Hit     Stimulus Absent   Correct Rejection   False Alarm       Based on the proportions of these types of trials, numerical estimates of sensitivity can be obtained with statistics like the sensitivity index d ' and A', 6 and response bias can be estimated with statistics like c and β. 7  Signal detection theory can also be applied to memory experiments, where items are presented on a study list for later testing. A test list is created by combining these 'old' items with novel, 'new' items that did not appear on the study list. On each test trial the subject will respond 'yes, this was on the study list' or 'no, this was not on the study list'. Items presented on the study list are called Targets, and new items are called Distractors. Saying 'Yes' to a target constitutes a Hit, while saying 'Yes' to a distractor constitutes a False Alarm.         Respond "No"   Respond "Yes"       Target   Miss   Hit     Distractor   Correct Rejection   False Alarm       Applications  Signal Detection Theory has wide application, both in humans and other animals . Topics include memory , stimulus characterists of schedules of reinforcement, etc.  Sensitivity or discriminability  Conceptually, sensitivity refers to how hard or easy it is to detect that a target stimulus is present from background events. For example, in a recognition memory paradigm, having longer to study to-be-remembered words makes it easier to recognize previously seen or heard words. In contrast, having to remember 30 words rather than 5 makes the discrimination harder. One of the most commonly used statistics for computing sensitivity is the so-called sensitivity index or d '. There are also non-parametric measures, such as the area under the ROC-curve .  Bias  Bias is the extent to which one response is more probable than another. That is, a receiver may be more likely to respond that a stimulus is present or more likely to respond that a stimulus is not present. Bias is independent of sensitivity. For example, if there is a penalty for either false alarms or misses, this may influence bias. If the stimulus is a bomber, then a miss (failing to detect the plane) may increase deaths, so a liberal bias is likely. In contrast, crying wolf (a false alarm) too often may make people less likely to respond, grounds for a conservative bias.  Mathematics  P(H1|y) > P(H2|y) / MAP Testing  In the case of making a decision between two hypotheses , H1 , absent, and H2 , present, in the event of a particular observation , y , a classical approach is to choose H1 when p(H1|y) > p(H2|y) and H2 in the reverse case. 8 In the event that the two a posteriori  probabilities are equal, one typically defaults to a single choice, say H2 . One could also flip a coin although the expected number of errors would be the same.  When taking this approach, usually what one knows are the conditional probabilities, p(y|H1) and p(y|H2) , and the a priori probabilities     p   (   H  1   )    =   π  1         p    H  1     subscript  π  1     p(H1)=\pi_{1}   and     p   (   H  2   )    =   π  2         p    H  2     subscript  π  2     p(H2)=\pi_{2}   . In this case,      p   (  H  1  |  y  )   =    p   (  y  |  H  1  )   ⋅   π  1     p   (  y  )        fragments  p   fragments  normal-(  H  1  normal-|  y  normal-)       fragments  p   fragments  normal-(  y  normal-|  H  1  normal-)   normal-⋅   subscript  π  1      p  y      p(H1|y)=\frac{p(y|H1)\cdot\pi_{1}}{p(y)}   ,      p   (  H  2  |  y  )   =    p   (  y  |  H  2  )   ⋅   π  2     p   (  y  )        fragments  p   fragments  normal-(  H  2  normal-|  y  normal-)       fragments  p   fragments  normal-(  y  normal-|  H  2  normal-)   normal-⋅   subscript  π  2      p  y      p(H2|y)=\frac{p(y|H2)\cdot\pi_{2}}{p(y)}     where p(y) is the total probability of event y ,      p   (  y  |  H  1  )   ⋅   π  1   +  p   (  y  |  H  2  )   ⋅   π  2      fragments  p   fragments  normal-(  y  normal-|  H  1  normal-)   normal-⋅   subscript  π  1    p   fragments  normal-(  y  normal-|  H  2  normal-)   normal-⋅   subscript  π  2     p(y|H1)\cdot\pi_{1}+p(y|H2)\cdot\pi_{2}   .  H2 is chosen in case        p   (  y  |  H  2  )   ⋅   π  2     p   (  y  |  H  1  )   ⋅   π  1   +  p   (  y  |  H  2  )   ⋅   π  2     ≥    p   (  y  |  H  1  )   ⋅   π  1     p   (  y  |  H  1  )   ⋅   π  1   +  p   (  y  |  H  2  )   ⋅   π  2            fragments  p   fragments  normal-(  y  normal-|  H  2  normal-)   normal-⋅   subscript  π  2     fragments  p   fragments  normal-(  y  normal-|  H  1  normal-)   normal-⋅   subscript  π  1    p   fragments  normal-(  y  normal-|  H  2  normal-)   normal-⋅   subscript  π  2        fragments  p   fragments  normal-(  y  normal-|  H  1  normal-)   normal-⋅   subscript  π  1     fragments  p   fragments  normal-(  y  normal-|  H  1  normal-)   normal-⋅   subscript  π  1    p   fragments  normal-(  y  normal-|  H  2  normal-)   normal-⋅   subscript  π  2       \frac{p(y|H2)\cdot\pi_{2}}{p(y|H1)\cdot\pi_{1}+p(y|H2)\cdot\pi_{2}}\geq\frac{p%
 (y|H1)\cdot\pi_{1}}{p(y|H1)\cdot\pi_{1}+p(y|H2)\cdot\pi_{2}}        ⇒    p   (  y  |  H  2  )     p   (  y  |  H  1  )     ≥    π  1    π  2         normal-⇒  absent     fragments  p   fragments  normal-(  y  normal-|  H  2  normal-)     fragments  p   fragments  normal-(  y  normal-|  H  1  normal-)             subscript  π  1    subscript  π  2       \Rightarrow\frac{p(y|H2)}{p(y|H1)}\geq\frac{\pi_{1}}{\pi_{2}}     and H1 otherwise.  Often, the ratio     π  1    π  2        subscript  π  1    subscript  π  2     \frac{\pi_{1}}{\pi_{2}}   is called    τ   M  A  P      subscript  τ    M  A  P     \tau_{MAP}   and     p   (  y  |  H  2  )     p   (  y  |  H  1  )         fragments  p   fragments  normal-(  y  normal-|  H  2  normal-)     fragments  p   fragments  normal-(  y  normal-|  H  1  normal-)      \frac{p(y|H2)}{p(y|H1)}   is called    L   (  y  )       L  y    L(y)   , the likelihood ratio .  Using this terminology, H2 is chosen in case     L   (  y  )    ≥   τ   M  A  P          L  y    subscript  τ    M  A  P      L(y)\geq\tau_{MAP}   . This is called MAP testing, where MAP stands for "maximum a posteriori ").  Taking this approach minimizes the expected number of errors one will make.  Bayes Criterion  In some cases, it is far more important to respond appropriately to H1 than it is to respond appropriately to H2 . For example, if an alarm goes off, indicating H1 (an incoming bomber is carrying a nuclear weapon ), it is much more important to shoot down the bomber if H1 = TRUE, than it is to send a fighter squadron to inspect a false alarm (i.e., H1 = FALSE, H2 = TRUE) (assuming a large supply of fighter squadrons). The Bayes criterion is an approach suitable for such cases. 9  Here a utility is associated with each of four situations:       U  11     subscript  U  11    U_{11}   : One responds with behavior appropriate to H1 and H1 is true: fighters destroy bomber, incurring fuel, maintenance, and weapons costs, take risk of some being shot down;      U  12     subscript  U  12    U_{12}   : One responds with behavior appropriate to H1 and H2 is true: fighters sent out, incurring fuel and maintenance costs, bomber location remains unknown;      U  21     subscript  U  21    U_{21}   : One responds with behavior appropriate to H2 and H1 is true: city destroyed;      U  22     subscript  U  22    U_{22}   : One responds with behavior appropriate to H2 and H2 is true: fighters stay home, bomber location remains unknown;   As is shown below, what is important are the differences,     U  11   -   U  21        subscript  U  11    subscript  U  21     U_{11}-U_{21}   and     U  22   -   U  12        subscript  U  22    subscript  U  12     U_{22}-U_{12}   .  Similarly, there are four probabilities,    P  11     subscript  P  11    P_{11}   ,    P  12     subscript  P  12    P_{12}   , etc., for each of the cases (which are dependent on one's decision strategy).  The Bayes criterion approach is to maximize the expected utility:      U  =     P  11   ⋅   U  11    +    P  21   ⋅   U  21    +    P  12   ⋅   U  12    +    P  22   ⋅   U  22         U     normal-⋅   subscript  P  11    subscript  U  11     normal-⋅   subscript  P  21    subscript  U  21     normal-⋅   subscript  P  12    subscript  U  12     normal-⋅   subscript  P  22    subscript  U  22       U=P_{11}\cdot U_{11}+P_{21}\cdot U_{21}+P_{12}\cdot U_{12}+P_{22}\cdot U_{22}       U  =     P  11   ⋅   U  11    +    (   1  -   P  11    )   ⋅   U  21    +    P  12   ⋅   U  12    +    (   1  -   P  12    )   ⋅   U  22         U     normal-⋅   subscript  P  11    subscript  U  11     normal-⋅    1   subscript  P  11     subscript  U  21     normal-⋅   subscript  P  12    subscript  U  12     normal-⋅    1   subscript  P  12     subscript  U  22       U=P_{11}\cdot U_{11}+(1-P_{11})\cdot U_{21}+P_{12}\cdot U_{12}+(1-P_{12})\cdot
 U%
 _{22}       U  =     U  21   +   U  22   +    P  11   ⋅   (    U  11   -   U  21    )     -    P  12   ⋅   (    U  22   -   U  12    )         U       subscript  U  21    subscript  U  22    normal-⋅   subscript  P  11      subscript  U  11    subscript  U  21       normal-⋅   subscript  P  12      subscript  U  22    subscript  U  12        U=U_{21}+U_{22}+P_{11}\cdot(U_{11}-U_{21})-P_{12}\cdot(U_{22}-U_{12})     Effectively, one may maximize the sum,       U  ′   =     P  11   ⋅   (    U  11   -   U  21    )    -    P  12   ⋅   (    U  22   -   U  12    )          superscript  U  normal-′      normal-⋅   subscript  P  11      subscript  U  11    subscript  U  21      normal-⋅   subscript  P  12      subscript  U  22    subscript  U  12        U^{\prime}=P_{11}\cdot(U_{11}-U_{21})-P_{12}\cdot(U_{22}-U_{12})   ,  and make the following substitutions:       P  11   =   π  1   ⋅   ∫   R  1    p   (  y  |  H  1  )   d  y     fragments   subscript  P  11     subscript  π  1   normal-⋅   subscript    subscript  R  1    p   fragments  normal-(  y  normal-|  H  1  normal-)   d  y    P_{11}=\pi_{1}\cdot\int_{R_{1}}p(y|H1)\,dy        P  12   =   π  2   ⋅   ∫   R  1    p   (  y  |  H  2  )   d  y     fragments   subscript  P  12     subscript  π  2   normal-⋅   subscript    subscript  R  1    p   fragments  normal-(  y  normal-|  H  2  normal-)   d  y    P_{12}=\pi_{2}\cdot\int_{R_{1}}p(y|H2)\,dy     where    π  1     subscript  π  1    \pi_{1}   and    π  2     subscript  π  2    \pi_{2}   are the a priori probabilities,    P   (   H  1   )       P    H  1     P(H1)   and    P   (   H  2   )       P    H  2     P(H2)   , and    R  1     subscript  R  1    R_{1}   is the region of observation events, y , that are responded to as though H1 is true.      ⇒   U  ′   =   ∫   R  1     {   π  1   ⋅   (   U  11   -   U  21   )   ⋅  p   (  y  |  H  1  )   -   π  2   ⋅   (   U  22   -   U  12   )   ⋅  p   (  y  |  H  2  )   }   d  y     fragments  normal-⇒   superscript  U  normal-′     subscript    subscript  R  1     fragments  normal-{   subscript  π  1   normal-⋅   fragments  normal-(   subscript  U  11     subscript  U  21   normal-)   normal-⋅  p   fragments  normal-(  y  normal-|  H  1  normal-)     subscript  π  2   normal-⋅   fragments  normal-(   subscript  U  22     subscript  U  12   normal-)   normal-⋅  p   fragments  normal-(  y  normal-|  H  2  normal-)   normal-}   d  y    \Rightarrow U^{\prime}=\int_{R_{1}}\left\{\pi_{1}\cdot(U_{11}-U_{21})\cdot p(y%
 |H1)-\pi_{2}\cdot(U_{22}-U_{12})\cdot p(y|H2)\right\}\,dy       U  ′     superscript  U  normal-′    U^{\prime}   and thus   U   U   U   are maximized by extending    R  1     subscript  R  1    R_{1}   over the region where       π  1   ⋅   (   U  11   -   U  21   )   ⋅  p   (  y  |  H  1  )   -   π  2   ⋅   (   U  22   -   U  12   )   ⋅  p   (  y  |  H  2  )   >  0     fragments   subscript  π  1   normal-⋅   fragments  normal-(   subscript  U  11     subscript  U  21   normal-)   normal-⋅  p   fragments  normal-(  y  normal-|  H  1  normal-)     subscript  π  2   normal-⋅   fragments  normal-(   subscript  U  22     subscript  U  12   normal-)   normal-⋅  p   fragments  normal-(  y  normal-|  H  2  normal-)    0    \pi_{1}\cdot(U_{11}-U_{21})\cdot p(y|H1)-\pi_{2}\cdot(U_{22}-U_{12})\cdot p(y|%
 H2)>0     This is accomplished by deciding H2 in case       π  2   ⋅   (   U  22   -   U  12   )   ⋅  p   (  y  |  H  2  )   ≥   π  1   ⋅   (   U  11   -   U  21   )   ⋅  p   (  y  |  H  1  )      fragments   subscript  π  2   normal-⋅   fragments  normal-(   subscript  U  22     subscript  U  12   normal-)   normal-⋅  p   fragments  normal-(  y  normal-|  H  2  normal-)     subscript  π  1   normal-⋅   fragments  normal-(   subscript  U  11     subscript  U  21   normal-)   normal-⋅  p   fragments  normal-(  y  normal-|  H  1  normal-)     \pi_{2}\cdot(U_{22}-U_{12})\cdot p(y|H2)\geq\pi_{1}\cdot(U_{11}-U_{21})\cdot p%
 (y|H1)        ⇒   L   (  y  )    ≡    p   (  y  |  H  2  )     p   (  y  |  H  1  )     ≥     π  1   ⋅   (    U  11   -   U  21    )      π  2   ⋅   (    U  22   -   U  12    )     ≡   τ  B        normal-⇒  absent    L  y           fragments  p   fragments  normal-(  y  normal-|  H  2  normal-)     fragments  p   fragments  normal-(  y  normal-|  H  1  normal-)             normal-⋅   subscript  π  1      subscript  U  11    subscript  U  21      normal-⋅   subscript  π  2      subscript  U  22    subscript  U  12            subscript  τ  B      \Rightarrow L(y)\equiv\frac{p(y|H2)}{p(y|H1)}\geq\frac{\pi_{1}\cdot(U_{11}-U_{%
 21})}{\pi_{2}\cdot(U_{22}-U_{12})}\equiv\tau_{B}     and H1 otherwise, where L(y) is the so-defined likelihood ratio .  See also   Binary classification  Constant false alarm rate  Demodulation  Detector (radio)  Decision theory  Estimation theory  Likelihood-ratio test  Modulation  Neyman-Pearson lemma  Receiver operating characteristic  Statistical signal processing  Just noticeable difference  Psychometric function  Statistical hypothesis testing  Two-alternative forced choice  Type I and type II errors   References     Coren, S., Ward, L.M., Enns, J. T. (1994) Sensation and Perception . (4th Ed.) Toronto: Harcourt Brace.  Kay, SM. Fundamentals of Statistical Signal Processing: Detection Theory (ISBN 0-13-504135-X)  McNichol, D. (1972) A Primer of Signal Detection Theory . London: George Allen & Unwin.  Van Trees HL. Detection, Estimation, and Modulation Theory, Part 1 (ISBN 0-471-09517-6; website )  Wickens, Thomas D., (2002) Elementary Signal Detection Theory . New York: Oxford University Press. (ISBN 0-19-509250-3)   External links   A Description of Signal Detection Theory  An application of SDT to safety  Signal Detection Theory by Garrett Neske, The Wolfram Demonstrations Project   "    Category:Signal processing  Category:Telecommunication theory  Category:Psychophysics  Category:Mathematical psychology     ↩  ↩  Peterson,W.W., Birdsall, T. G. & Fox, W. C. (1954) The theory of signal detectability. Proceedings of the IRE Professional Group on Information Theory 4, 171-212. ↩  ↩  Swets, J.A. (ed.) (1964) Signal detection and recognition by human observers . New York: Wiley ↩  ↩   Schonhoff, T.A. and Giordano, A.A. (2006) Detection and Estimation Theory and Its Applications . New Jersey: Pearson Education (ISBN 0-13-089499-0) ↩      