<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1886">VC dimension</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>VC dimension</h1>
<hr/>

<p>In <a href="Vapnik–Chervonenkis_theory" title="wikilink">statistical learning theory</a>, or sometimes <a href="computational_learning_theory" title="wikilink">computational learning theory</a>, the <strong>VC dimension</strong> (for <strong>Vapnik–Chervonenkis dimension</strong>) is a measure of the <strong>capacity</strong> (complexity, expressive power, richness, or flexibility) of a <a href="statistical_classification" title="wikilink">statistical classification</a> <a class="uri" href="algorithm" title="wikilink">algorithm</a>, defined as the <a class="uri" href="cardinality" title="wikilink">cardinality</a> of the largest set of points that the algorithm can <a href="Shattering_(machine_learning)" title="wikilink">shatter</a>. It is a core concept in <a href="Vapnik–Chervonenkis_theory" title="wikilink">Vapnik–Chervonenkis theory</a>, and was originally defined by <a href="Vladimir_Vapnik" title="wikilink">Vladimir Vapnik</a> and <a href="Alexey_Chervonenkis" title="wikilink">Alexey Chervonenkis</a>.</p>

<p>Informally, the capacity of a classification model is related to how complicated it can be. For example, consider the <a href="Heaviside_step_function" title="wikilink">thresholding</a> of a high-<a href="degree_of_a_polynomial" title="wikilink">degree</a> <a class="uri" href="polynomial" title="wikilink">polynomial</a>: if the polynomial evaluates above zero, that point is classified as positive, otherwise as negative. A high-degree polynomial can be wiggly, so it can fit a given set of training points well. But one can expect that the classifier will make errors on other points, because it is too wiggly. Such a polynomial has a high capacity. A much simpler alternative is to threshold a linear function. This function may not fit the training set well, because it has a low capacity. This notion of capacity is made rigorous below.</p>
<h2 id="shattering">Shattering</h2>

<p>A classification model 

<math display="inline" id="VC_dimension:0">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 with some parameter vector 

<math display="inline" id="VC_dimension:1">
 <semantics>
  <mi>θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta
  </annotation>
 </semantics>
</math>

 is said to <em>shatter</em> a set of data points 

<math display="inline" id="VC_dimension:2">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <msub>
    <mi>x</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>x</mi>
    <mn>2</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>x</mi>
    <mi>n</mi>
   </msub>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <vector>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">2</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>n</ci>
    </apply>
   </vector>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x_{1},x_{2},\ldots,x_{n})
  </annotation>
 </semantics>
</math>

 if, for all assignments of labels to those points, there exists a 

<math display="inline" id="VC_dimension:3">
 <semantics>
  <mi>θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta
  </annotation>
 </semantics>
</math>

 such that the model 

<math display="inline" id="VC_dimension:4">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>


 makes no errors when evaluating that set of data points.</p>

<p>The VC dimension of a model 

<math display="inline" id="VC_dimension:5">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 is the maximum number of points that can be arranged so that 

<math display="inline" id="VC_dimension:6">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 shatters them. More formally, it is 

<math display="inline" id="VC_dimension:7">
 <semantics>
  <msup>
   <mi>h</mi>
   <mo>′</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>h</ci>
    <ci>normal-′</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h^{\prime}
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="VC_dimension:8">
 <semantics>
  <msup>
   <mi>h</mi>
   <mo>′</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>h</ci>
    <ci>normal-′</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h^{\prime}
  </annotation>
 </semantics>
</math>

 is the maximum 

<math display="inline" id="VC_dimension:9">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>


 such that some data point set of <a class="uri" href="cardinality" title="wikilink">cardinality</a> 

<math display="inline" id="VC_dimension:10">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>

 can be shattered by 

<math display="inline" id="VC_dimension:11">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

.</p>

<p>For example, consider a <a href="linear_classifier" title="wikilink">straight line</a> as the classification model: the model used by a <a class="uri" href="perceptron" title="wikilink">perceptron</a>. The line should separate positive data points from negative data points. There exist sets of 3 points that can indeed be shattered using this model (any 3 points that are not collinear can be shattered). However, no set of 4 points can be shattered: by <a href="Radon's_theorem" title="wikilink">Radon's theorem</a>, any four points can be partitioned into two subsets with intersecting convex hulls, so it is not possible to separate one of these two subsets from the other. Thus, the VC dimension of this particular classifier is 3. It is important to remember that while one can choose any arrangement of points, the arrangement of those points cannot change when attempting to shatter for some label assignment. Note, only 3 of the 2<sup>3</sup> = 8 possible label assignments are shown for the three points.</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><figure><b>(Figure)</b>
<figcaption>VC1.svg</figcaption>
</figure></td>
<td style="text-align: left;"><figure><b>(Figure)</b>
<figcaption>VC2.svg</figcaption>
</figure></td>
<td style="text-align: left;"><figure><b>(Figure)</b>
<figcaption>VC3.svg</figcaption>
</figure></td>
<td style="text-align: left;"><figure><b>(Figure)</b>
<figcaption>VC4.svg</figcaption>
</figure></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p><strong>3 points shattered</strong></p></td>
<td style="text-align: left;">
<p><strong>4 points impossible</strong></p></td>
</tr>
</tbody>
</table>
<h2 id="uses">Uses</h2>

<p>The VC dimension has utility in statistical learning theory, because it can predict a <a class="uri" href="probabilistic" title="wikilink">probabilistic</a> <a href="upper_bound" title="wikilink">upper bound</a> on the test error of a classification model.</p>

<p>Vapnik <a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> proved that the probability of the test error distancing from an upper bound (on data that is drawn <a href="Independent_identically-distributed_random_variables" title="wikilink">i.i.d.</a> from the same distribution as the training set) is given by</p>

<p>

<math display="inline" id="VC_dimension:12">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo>(</mo>
    <mtext>test error</mtext>
    <mo>≤</mo>
    <mtext>training error</mtext>
    <mo>+</mo>
    <msqrt>
     <mfrac>
      <mrow>
       <mrow>
        <mi>h</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mrow>
          <mrow>
           <mi>log</mi>
           <mrow>
            <mo stretchy="false">(</mo>
            <mrow>
             <mrow>
              <mn>2</mn>
              <mi>N</mi>
             </mrow>
             <mo>/</mo>
             <mi>h</mi>
            </mrow>
            <mo stretchy="false">)</mo>
           </mrow>
          </mrow>
          <mo>+</mo>
          <mn>1</mn>
         </mrow>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>-</mo>
       <mrow>
        <mi>log</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mrow>
          <mi>η</mi>
          <mo>/</mo>
          <mn>4</mn>
         </mrow>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mi>N</mi>
     </mfrac>
    </msqrt>
    <mo>)</mo>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
   <mo>-</mo>
   <mi>η</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <mtext>test error</mtext>
     <leq></leq>
     <mtext>training error</mtext>
     <plus></plus>
     <apply>
      <root></root>
      <apply>
       <divide></divide>
       <apply>
        <minus></minus>
        <apply>
         <times></times>
         <ci>h</ci>
         <apply>
          <plus></plus>
          <apply>
           <log></log>
           <apply>
            <divide></divide>
            <apply>
             <times></times>
             <cn type="integer">2</cn>
             <ci>N</ci>
            </apply>
            <ci>h</ci>
           </apply>
          </apply>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <apply>
         <log></log>
         <apply>
          <divide></divide>
          <ci>η</ci>
          <cn type="integer">4</cn>
         </apply>
        </apply>
       </apply>
       <ci>N</ci>
      </apply>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="integer">1</cn>
    <minus></minus>
    <csymbol cd="unknown">η</csymbol>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P\left(\text{test error}\leq\text{training error}+\sqrt{h(\log(2N/h)+1)-\log(%
\eta/4)\over N}\right)=1-\eta
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="VC_dimension:13">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>

 is the VC dimension of the classification model, 

<math display="inline" id="VC_dimension:14">
 <semantics>
  <mrow>
   <mn>0</mn>
   <mo>≤</mo>
   <mi>η</mi>
   <mo>≤</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <leq></leq>
     <cn type="integer">0</cn>
     <ci>η</ci>
    </apply>
    <apply>
     <leq></leq>
     <share href="#.cmml">
     </share>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   0\leq\eta\leq 1
  </annotation>
 </semantics>
</math>


, and 

<math display="inline" id="VC_dimension:15">
 <semantics>
  <mi>N</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>N</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N
  </annotation>
 </semantics>
</math>

 is the size of the training set (restriction: this formula is valid when 

<math display="inline" id="VC_dimension:16">
 <semantics>
  <mrow>
   <mi>h</mi>
   <mo>≪</mo>
   <mi>N</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">much-less-than</csymbol>
    <ci>h</ci>
    <ci>N</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h\ll N
  </annotation>
 </semantics>
</math>

). Similar complexity bounds can be derived using <a href="Rademacher_complexity" title="wikilink">Rademacher complexity</a>, but Rademacher complexity can sometimes provide more insight than VC dimension calculations into such statistical methods such as those using <a href="kernel_methods" title="wikilink">kernels</a>. The generalization of the VC dimension for multi-valued functions is the <a href="Natarajan_dimension" title="wikilink">Natarajan dimension</a>.</p>

<p>In <a href="computational_geometry" title="wikilink">computational geometry</a>, VC dimension is one of the critical parameters in the size of <a href="E-net_(computational_geometry)" title="wikilink">ε-nets</a>, which determines the complexity of approximation algorithms based on them; range sets without finite VC dimension may not have finite ε-nets at all.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Sauer–Shelah_lemma" title="wikilink">Sauer–Shelah lemma</a>, a bound on the number of sets in a set system in terms of the VC dimension</li>
<li><a href="http://www.sciencedirect.com/science/article/pii/S002200009791477X">Karpinski-Macintyre theorem</a>, a bound on the VC dimension of general Pfaffian formulas</li>
</ul>
<h2 id="references">References</h2>
<references>
</references>
<ul>
<li>Andrew Moore's <a href="http://www-2.cs.cmu.edu/~awm/tutorials/vcdim.html">VC dimension tutorial</a></li>
<li>Vapnik, Vladimir. "The nature of statistical learning theory". springer, 2000.</li>
<li>V. Vapnik and A. Chervonenkis. "On the uniform convergence of relative frequencies of events to their probabilities." <em>Theory of Probability and its Applications</em>, 16(2):264–280, 1971.</li>
<li>A. Blumer, A. Ehrenfeucht, D. Haussler, and <a href="Manfred_K._Warmuth" title="wikilink">M. K. Warmuth</a>. "Learnability and the Vapnik–Chervonenkis dimension." <em>Journal of the ACM</em>, 36(4):929–865, 1989.</li>
<li>Christopher Burges Tutorial on SVMs for Pattern Recognition (containing information also for VC dimension) <a href="http://citeseer.ist.psu.edu/burges98tutorial.html">1</a></li>
<li><a href="Bernard_Chazelle" title="wikilink">Bernard Chazelle</a>. "The Discrepancy Method." <a href="http://www.cs.princeton.edu/~chazelle/book.html">2</a></li>
<li>B.K. Natarajan. "On Learning sets and functions." <em>Machine Learning</em>, 4, 67-97, 1989. <a href="http://link.springer.com/article/10.1007%2FBF00114804#page-1">3</a></li>
</ul>

<p>"</p>

<p><a class="uri" href="Category:Dimension" title="wikilink">Category:Dimension</a> <a href="Category:Statistical_classification" title="wikilink">Category:Statistical classification</a> <a href="Category:Computational_learning_theory" title="wikilink">Category:Computational learning theory</a> <a href="Category:Measures_of_complexity" title="wikilink">Category:Measures of complexity</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Vapnik, Vladimir. The nature of statistical learning theory. springer, 2000.<a href="#fnref1">↩</a></li>
</ol>
</section>
</body>
</html>
