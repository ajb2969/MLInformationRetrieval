   Min-max theorem      Min-max theorem   In linear algebra and functional analysis , the min-max theorem , or variational theorem , or Courant‚ÄìFischer‚ÄìWeyl min-max principle , is a result that gives a variational characterization of eigenvalues of compact Hermitian operators on Hilbert spaces . It can be viewed as the starting point of many results of similar nature.  This article first discusses the finite-dimensional case and its applications before considering compact operators on infinite-dimensional Hilbert spaces. We will see that for compact operators, the proof of the main theorem uses essentially the same idea from the finite-dimensional argument.  In the case that the operator is non-Hermitian, the theorem provides an equivalent characterization of the associated singular values . The min-max theorem can be extended to self-adjoint operators that are bounded below.  Matrices  Let   A   A   A   be a    n  √ó  n      n  normal-√ó  n    n√ón    Hermitian matrix . As with many other variational results on eigenvalues, one considers the Rayleigh‚ÄìRitz quotient defined by        R  A    (  x  )    =    (   A  x   ,  x  )    (  x  ,  x  )           subscript  R  A   x        A  x   x    x  x      R_{A}(x)=\frac{(Ax,x)}{(x,x)}     where    (  ‚ãÖ  ,  ‚ãÖ  )     normal-‚ãÖ  normal-‚ãÖ    (‚ãÖ,‚ãÖ)   denotes the Euclidean inner product on . Clearly, the Rayleigh quotient of an eigenvector is its associated eigenvalue. Equivalently, the Rayleigh‚ÄìRitz quotient can be replaced by        f   (  x  )    =   (   A  x   ,  x  )    ,    ‚à•  x  ‚à•   =  1.      formulae-sequence      f  x      A  x   x       norm  x   1.     f(x)=(Ax,x),\;\|x\|=1.     For Hermitian matrices, the range of the continuous function R A ( x ), or f ( x ), is a compact subset [ a , b ] of the real line. The maximum b and the minimum a are the largest and smallest eigenvalue of A , respectively. The min-max theorem is a refinement of this fact.  Min-max Theorem  Let   A   A   A   be a    n  √ó  n      n  normal-√ó  n    n√ón    Hermitian matrix with eigenvalues then       Œª  k   =   min   {   max   {    R  A    (  x  )    ‚à£   x  ‚àà   U  and  x   ‚â†  0   }    ‚à£    dim   (  U  )    =  k   }         subscript  Œª  k          subscript  R  A   x       x    U  and  x        0        dimension  U   k      \lambda_{k}=\min\{\max\{R_{A}(x)\mid x\in U\text{ and }x\neq 0\}\mid\dim(U)=k\}   and       Œª  k   =   max   {   min   {    R  A    (  x  )    ‚à£   x  ‚àà   U  and  x   ‚â†  0   }    ‚à£    dim   (  U  )    =    n  -  k   +  1    }         subscript  Œª  k          subscript  R  A   x       x    U  and  x        0        dimension  U       n  k   1       \lambda_{k}=\max\{\min\{R_{A}(x)\mid x\in U\text{ and }x\neq 0\}\mid\dim(U)=n-%
 k+1\}   in particular,        Œª  1   ‚â§    R  A    (  x  )    ‚â§   Œª  n      ‚àÄ  x   ‚àà    ùêÇ  n   \   {  0  }        formulae-sequence       subscript  Œª  1      subscript  R  A   x         subscript  Œª  n        for-all  x    normal-\   superscript  ùêÇ  n    0       \lambda_{1}\leq R_{A}(x)\leq\lambda_{n}\quad\forall x\in\mathbf{C}^{n}%
 \backslash\{0\}   and these bounds are attained when   x   x   x   is an eigenvector of the appropriate eigenvalues.  Also note that the simpler formulation for the maximal eigenvalue Œª n is given by:        Œª  n   =   max   {     R  A    (  x  )    :   x  ‚â†  0    }     .       subscript  Œª  n      normal-:     subscript  R  A   x     x  0       \lambda_{n}=\max\{R_{A}(x):x\neq 0\}.   Similarly, the minimal eigenvalue Œª 1 is given by:        Œª  1   =   min   {     R  A    (  x  )    :   x  ‚â†  0    }     .       subscript  Œª  1      normal-:     subscript  R  A   x     x  0       \lambda_{1}=\min\{R_{A}(x):x\neq 0\}.     Proof  Since the matrix   A   A   A   is Hermitian it is diagonalizable and we can choose an orthonormal basis of eigenvectors { u 1 , ..., u n } that is, u i is an eigenvector for the eigenvalue Œª i and such that ( u i , u i ) = 1 and ( u i , u j ) = 0 for all i ‚â† j .  If U is a subspace of dimension k then its intersection with the subspace  isn't zero (by simply checking dimensions) and hence there exists a vector    v  ‚â†  0      v  normal-‚â†  0    v‚â†0   in this intersection that we can write as      v  =    ‚àë   i  =  k   n     Œ±  i    u  i         v    superscript   subscript     i  k    n      subscript  Œ±  i    subscript  u  i       v=\sum_{i=k}^{n}\alpha_{i}u_{i}     and whose Rayleigh quotient is        R  A    (  v  )    =     ‚àë   i  =  k   n     Œª  i    Œ±  i  2       ‚àë   i  =  k   n    Œ±  i  2     ‚â•   Œª  k            subscript  R  A   v       superscript   subscript     i  k    n      subscript  Œª  i    superscript   subscript  Œ±  i   2       superscript   subscript     i  k    n    superscript   subscript  Œ±  i   2           subscript  Œª  k      R_{A}(v)=\frac{\sum_{i=k}^{n}\lambda_{i}\alpha_{i}^{2}}{\sum_{i=k}^{n}\alpha_{%
 i}^{2}}\geq\lambda_{k}   (as all     Œª  i   ‚â•   Œª  k        subscript  Œª  i    subscript  Œª  k     \lambda_{i}\geq\lambda_{k}   for i=k,..,n) and hence       max   {    R  A    (  x  )    ‚à£   x  ‚àà  U   }    ‚â•   Œª  k            subscript  R  A   x     x  U     subscript  Œª  k     \max\{R_{A}(x)\mid x\in U\}\geq\lambda_{k}   Since this is true for all U, we can conclude that       min   {   max   {    R  A    (  x  )    ‚à£   x  ‚àà   U  and  x   ‚â†  0   }    ‚à£    dim   (  U  )    =  k   }    ‚â•   Œª  k              subscript  R  A   x       x    U  and  x        0        dimension  U   k     subscript  Œª  k     \min\{\max\{R_{A}(x)\mid x\in U\text{ and }x\neq 0\}\mid\dim(U)=k\}\geq\lambda%
 _{k}     This is one inequality. To establish the other inequality, chose the specific k-dimensional space , for which       max   {    R  A    (  x  )    ‚à£   x  ‚àà   V  and  x   ‚â†  0   }    ‚â§   Œª  k            subscript  R  A   x       x    V  and  x        0      subscript  Œª  k     \max\{R_{A}(x)\mid x\in V\text{ and }x\neq 0\}\leq\lambda_{k}   because    Œª  k     subscript  Œª  k    \lambda_{k}   is the largest eigenvalue in V. Therefore, also       min   {   max   {    R  A    (  x  )    ‚à£   x  ‚àà   U  and  x   ‚â†  0   }    ‚à£    dim   (  U  )    =  k   }    ‚â§   Œª  k              subscript  R  A   x       x    U  and  x        0        dimension  U   k     subscript  Œª  k     \min\{\max\{R_{A}(x)\mid x\in U\text{ and }x\neq 0\}\mid\dim(U)=k\}\leq\lambda%
 _{k}     In the case where U is a subspace of dimension n-k+1 , we proceed in a similar fashion: Consider the subspace of dimension k ,  Its intersection with the subspace U isn't zero (by simply checking dimensions) and hence there exists a vector v in this intersection that we can write as      v  =    ‚àë   i  =  1   k     Œ±  i    u  i         v    superscript   subscript     i  1    k      subscript  Œ±  i    subscript  u  i       v=\sum_{i=1}^{k}\alpha_{i}u_{i}   and whose Rayleigh quotient is        R  A    (  v  )    =     ‚àë   i  =  1   k     Œª  i    Œ±  i  2       ‚àë   i  =  1   k    Œ±  i  2     ‚â§   Œª  k            subscript  R  A   v       superscript   subscript     i  1    k      subscript  Œª  i    superscript   subscript  Œ±  i   2       superscript   subscript     i  1    k    superscript   subscript  Œ±  i   2           subscript  Œª  k      R_{A}(v)=\frac{\sum_{i=1}^{k}\lambda_{i}\alpha_{i}^{2}}{\sum_{i=1}^{k}\alpha_{%
 i}^{2}}\leq\lambda_{k}   and hence       min   {    R  A    (  x  )    ‚à£   x  ‚àà  U   }    ‚â§   Œª  k            subscript  R  A   x     x  U     subscript  Œª  k     \min\{R_{A}(x)\mid x\in U\}\leq\lambda_{k}   Since this is true for all U, we can conclude that       max   {   min   {    R  A    (  x  )    ‚à£   x  ‚àà   U  and  x   ‚â†  0   }    ‚à£    dim   (  U  )    =    n  -  k   +  1    }    ‚â§   Œª  k              subscript  R  A   x       x    U  and  x        0        dimension  U       n  k   1      subscript  Œª  k     \max\{\min\{R_{A}(x)\mid x\in U\text{ and }x\neq 0\}\mid\dim(U)=n-k+1\}\leq%
 \lambda_{k}     Again, this is one part of the equation. To get the other inequality, note again that the eigenvector u of    Œª  k     subscript  Œª  k    \lambda_{k}   is contained in so that we can conclude the equality.  Counterexample in the non-Hermitian case  Let N be the nilpotent matrix       [     0    1      0    0     ]   .      0  1    0  0     \begin{bmatrix}0&1\\
 0&0\end{bmatrix}.     Define the Rayleigh quotient     R  N    (  x  )        subscript  R  N   x    R_{N}(x)   exactly as above in the Hermitian case. Then it is easy to see that the only eigenvalue of N is zero, while the maximum value of the Rayleigh ratio is    1  2      1  2    \frac{1}{2}   . That is, the maximum value of the Rayleigh quotient is larger than the maximum eigenvalue.  Applications  Min-max principle for singular values  The singular values { œÉ k } of a square matrix M are the square roots of eigenvalues of M * M (equivalently MM* ). An immediate consequence of the first equality from min-max theorem is       œÉ  k  ‚Üë   =   min   S  :    dim   (  S  )    =  k      max    x  ‚àà  S   ,    ‚à•  x  ‚à•   =  1       (   M  *   M  x  ,  x  )    1  2    =   min   S  :    dim   (  S  )    =  k      max    x  ‚àà  S   ,    ‚à•  x  ‚à•   =  1     ‚à•  M  x  ‚à•  .     fragments   superscript   subscript  œÉ  k   normal-‚Üë     subscript    normal-:  S     dimension  S   k      subscript    formulae-sequence    x  S      norm  x   1      superscript   fragments  normal-(   superscript  M    M  x  normal-,  x  normal-)     1  2      subscript    normal-:  S     dimension  S   k      subscript    formulae-sequence    x  S      norm  x   1     parallel-to  M  x  parallel-to  normal-.    \sigma_{k}^{\uparrow}=\min_{S:\dim(S)=k}\max_{x\in S,\|x\|=1}(M^{*}Mx,x)^{%
 \frac{1}{2}}=\min_{S:\dim(S)=k}\max_{x\in S,\|x\|=1}\|Mx\|.     Similarly,        œÉ  k  ‚Üì   =    max   S  :    dim   (  S  )    =    n  -  k   +  1        min    x  ‚àà  S   ,    ‚à•  x  ‚à•   =  1      ‚à•   M  x   ‚à•      .       superscript   subscript  œÉ  k   normal-‚Üì     subscript    normal-:  S     dimension  S       n  k   1        subscript    formulae-sequence    x  S      norm  x   1      norm    M  x        \sigma_{k}^{\downarrow}=\max_{S:\dim(S)=n-k+1}\min_{x\in S,\|x\|=1}\|Mx\|.     Cauchy interlacing theorem  Let   A   A   A   be a symmetric n √ó n matrix. The m √ó m matrix B , where m ‚â§ n , is called a compression of   A   A   A   if there exists an orthogonal projection P onto a subspace of dimension m such that P*AP = B . The Cauchy interlacing theorem states:   Theorem. If the eigenvalues of   A   A   A   are , and those of B are , then for all      Œ±  j   ‚â§   Œ≤  j   ‚â§   Œ±    n  -  m   +  j     .         subscript  Œ±  j    subscript  Œ≤  j         subscript  Œ±      n  m   j       \alpha_{j}\leq\beta_{j}\leq\alpha_{n-m+j}.      This can be proven using the min-max principle. Let Œ≤ i have corresponding eigenvector b i and S j be the j dimensional subspace  then        Œ≤  j   =    min    x  ‚àà   S    m  -  j   +  1     ,    ‚à•  x  ‚à•   =  1      (   B  x   ,  x  )    =    min    x  ‚àà   S    m  -  j   +  1     ,    ‚à•  x  ‚à•   =  1      (    P  *   A  P  x   ,  x  )    =    min    x  ‚àà   S    m  -  j   +  1     ,    ‚à•  x  ‚à•   =  1      (   A  x   ,  x  )    ‚â§   Œ±    n  -  m   +  j     ,         subscript  Œ≤  j     subscript    formulae-sequence    x   subscript  S      m  j   1        norm  x   1       B  x   x          subscript    formulae-sequence    x   subscript  S      m  j   1        norm  x   1        superscript  P    A  P  x   x          subscript    formulae-sequence    x   subscript  S      m  j   1        norm  x   1       A  x   x         subscript  Œ±      n  m   j       \beta_{j}=\min_{x\in S_{m-j+1},\|x\|=1}(Bx,x)=\min_{x\in S_{m-j+1},\|x\|=1}(P^%
 {*}APx,x)=\min_{x\in S_{m-j+1},\|x\|=1}(Ax,x)\leq\alpha_{n-m+j},     According to first part of min-max, On the other hand, if we define  then       n  ‚àí  m   =  1        n  normal-‚àí  m   1    n‚àím=1     where the last inequality is given by the second part of min-max.  Notice that, when   A   A   A   , we have , hence the name interlacing theorem.  Compact operators  Let   A   A   A   be a compact , Hermitian operator on a Hilbert space H . Recall that the spectrum of such an operator form a sequence of real numbers whose only possible cluster point is zero. Every nonzero number in the spectrum is an eigenvalue. It no longer makes sense here to list the positive eigenvalues in increasing order. Let the positive eigenvalues of     ‚ãØ  ‚â§   Œª  k   ‚â§  ‚ãØ  ‚â§   Œª  1    ,        normal-‚ãØ   subscript  Œª  k        normal-‚ãØ        subscript  Œª  1      \cdots\leq\lambda_{k}\leq\cdots\leq\lambda_{1},   be     A   A   A     where multiplicity is taken into account as in the matrix case. When H is infinite-dimensional, the above sequence of eigenvalues is necessarily infinite. We now apply the same reasoning as in the matrix case. Letting S k ‚äÇ H be a k dimensional subspace, we can obtain the following theorem.   Theorem (Min-Max). Let   H   H   H   be a compact, self-adjoint operator on a Hilbert space       min   S   k  -  1       max    x  ‚àà   S   k  -  1   ‚üÇ    ,    ‚à•  x  ‚à•   =  1      (   A  x   ,  x  )     =   Œª  k    .        subscript    subscript  S    k  1       subscript    formulae-sequence    x   superscript   subscript  S    k  1    perpendicular-to       norm  x   1       A  x   x     subscript  Œª  k     \min_{S_{k-1}}\max_{x\in S_{k-1}^{\perp},\|x\|=1}(Ax,x)=\lambda_{k}.   , whose positive eigenvalues are listed in decreasing order . Then: : \begin{align}    \max_{S_k} \min_{x \in S_k, \|x\| = 1} (Ax,x) &= \lambda_k ^{\downarrow}, \\ \min_{S_{k-1}} \max_{x \in S_{k-1}^{\perp}, \|x\|=1} (Ax, x) &= \lambda_k^{\downarrow}. \end{align}  A similar pair of equalities hold for negative eigenvalues.  Proof:  \max_{x \in S_{k-1}^{\perp}, \|x\|=1} (Ax, x) \ge \lambda_k.  Pick S k ‚àí1 = span{ u 1 , ..., u k ‚àí1 } and we deduce       E  1   ‚â§   E  2   ‚â§   E  3   ‚â§  ‚ãØ         subscript  E  1    subscript  E  2         subscript  E  3        normal-‚ãØ     E_{1}\leq E_{2}\leq E_{3}\leq\cdots   }}  Self-adjoint operators  The min-max theorem also applies to (possibly unbounded) self-adjoint operators. 1  2 Recall the essential spectrum is the spectrum without isolated eigenvalues of finite multiplicity. Sometimes we have some eigenvalues below the bottom of the eessential spectrum, and we would like to approximate the eigenvalues and eigenfunctions.  Theorem (Min-Max). Let A be self-adjoint, and let     E  n   =    min    œà  1   ,  ‚Ä¶  ,   œà   n  -  1       max   {    ‚ü®  œà  ,   A  œà   ‚ü©   :   œà  ‚àà   span   (   œà  1   ,  ‚Ä¶  ,   œà   n  -  1    )      }          subscript  E  n     subscript     subscript  œà  1   normal-‚Ä¶   subscript  œà    n  1         normal-:   œà    A  œà      œà   span   subscript  œà  1   normal-‚Ä¶   subscript  œà    n  1           E_{n}=\min_{\psi_{1},\ldots,\psi_{n-1}}\max\{\langle\psi,A\psi\rangle:\psi\in%
 \operatorname{span}(\psi_{1},\ldots,\psi_{n-1})\}   be the eigenvalues of A below the essential spectrum. Then       E  n   :=   inf    œÉ   e  s  s     (  A  )        assign   subscript  E  n    infimum     subscript  œÉ    e  s  s    A      E_{n}:=\inf\sigma_{ess}(A)   .  If we only have N eigenvalues and hence run out of eigenvalues, then we let     E  1   ‚â§   E  2   ‚â§   E  3   ‚â§  ‚ãØ         subscript  E  1    subscript  E  2         subscript  E  3        normal-‚ãØ     E_{1}\leq E_{2}\leq E_{3}\leq\cdots   (the bottom of the essential spectrum) for n>N , and the above statement holds after replacing min-max with inf-sup.  Theorem (Max-Min). Let A be self-adjoint, and let     E  n   =    max    œà  1   ,  ‚Ä¶  ,   œà   n  -  1       min   {    ‚ü®  œà  ,   A  œà   ‚ü©   :   œà  ‚üÇ    œà  1   ,  ‚Ä¶  ,   œà   n  -  1       }          subscript  E  n     subscript     subscript  œà  1   normal-‚Ä¶   subscript  œà    n  1         normal-:   œà    A  œà     perpendicular-to  œà    subscript  œà  1   normal-‚Ä¶   subscript  œà    n  1           E_{n}=\max_{\psi_{1},\ldots,\psi_{n-1}}\min\{\langle\psi,A\psi\rangle:\psi%
 \perp\psi_{1},\ldots,\psi_{n-1}\}   be the eigenvalues of A below the essential spectrum. Then       E  n   :=   inf    œÉ   e  s  s     (  A  )        assign   subscript  E  n    infimum     subscript  œÉ    e  s  s    A      E_{n}:=\inf\sigma_{ess}(A)   .  If we only have N eigenvalues and hence run out of eigenvalues, then we let     (   A  -  E   )   ‚â•  0        A  E   0    (A-E)\geq 0   (the bottom of the essential spectrum) for n>N , and the above statement holds after replacing max-min with sup-inf.  The proofs 3 4 use the following results about self-adjoint operators:  Theorem. Let A be self-adjoint. Then    E  ‚àà  ‚Ñù      E  ‚Ñù    E\in\mathbb{R}   for     œÉ   (  A  )    ‚äÜ   [  E  ,  ‚àû  )         œÉ  A    E      \sigma(A)\subseteq[E,\infty)   if and only if     inf   œÉ   (  A  )     =    inf    œà  ‚àà   ùîá   (  A  )     ,    ‚à•  œà  ‚à•   =  1      ‚ü®  œà  ,   A  œà   ‚ü©         infimum    œÉ  A      subscript  infimum   formulae-sequence    œà    ùîá  A       norm  œà   1      œà    A  œà       \inf\sigma(A)=\inf_{\psi\in\mathfrak{D}(A),\|\psi\|=1}\langle\psi,A\psi\rangle   .  Theorem. If A is self-adjoint, then       sup   œÉ   (  A  )     =    sup    œà  ‚àà   ùîá   (  A  )     ,    ‚à•  œà  ‚à•   =  1      ‚ü®  œà  ,   A  œà   ‚ü©         supremum    œÉ  A      subscript  supremum   formulae-sequence    œà    ùîá  A       norm  œà   1      œà    A  œà       \sup\sigma(A)=\sup_{\psi\in\mathfrak{D}(A),\|\psi\|=1}\langle\psi,A\psi\rangle     and  $\sup\sigma(A)=\sup_{\psi\in\mathfrak{D}(A),\|\psi\|=1}\langle\psi,A\psi\rangle$ .  See also   Courant minimax principle  Max‚Äìmin inequality   References   M. Reed and B. Simon, Methods of Modern Mathematical Physics IV: Analysis of Operators , Academic Press, 1978.   "  Category:Articles containing proofs  Category:Theorems in functional analysis  Category:Spectral theory  Category:Operator theory     G. Teschl, Mathematical Methods in Quantum Mechanics (GSM 99) http://www.mat.univie.ac.at/~gerald/ftp/book-schroe/schroe.pdf ‚Ü©  Lieb-Loss, Analysis 2nd ed. (GSM 14) ‚Ü©       