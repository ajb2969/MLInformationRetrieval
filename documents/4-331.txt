   Min-max theorem      Min-max theorem   In linear algebra and functional analysis , the min-max theorem , or variational theorem , or Courantâ€“Fischerâ€“Weyl min-max principle , is a result that gives a variational characterization of eigenvalues of compact Hermitian operators on Hilbert spaces . It can be viewed as the starting point of many results of similar nature.  This article first discusses the finite-dimensional case and its applications before considering compact operators on infinite-dimensional Hilbert spaces. We will see that for compact operators, the proof of the main theorem uses essentially the same idea from the finite-dimensional argument.  In the case that the operator is non-Hermitian, the theorem provides an equivalent characterization of the associated singular values . The min-max theorem can be extended to self-adjoint operators that are bounded below.  Matrices  Let   A   A   A   be a    n  Ã—  n      n  normal-Ã—  n    nÃ—n    Hermitian matrix . As with many other variational results on eigenvalues, one considers the Rayleighâ€“Ritz quotient defined by        R  A    (  x  )    =    (   A  x   ,  x  )    (  x  ,  x  )           subscript  R  A   x        A  x   x    x  x      R_{A}(x)=\frac{(Ax,x)}{(x,x)}     where    (  â‹…  ,  â‹…  )     normal-â‹…  normal-â‹…    (â‹…,â‹…)   denotes the Euclidean inner product on . Clearly, the Rayleigh quotient of an eigenvector is its associated eigenvalue. Equivalently, the Rayleighâ€“Ritz quotient can be replaced by        f   (  x  )    =   (   A  x   ,  x  )    ,    âˆ¥  x  âˆ¥   =  1.      formulae-sequence      f  x      A  x   x       norm  x   1.     f(x)=(Ax,x),\;\|x\|=1.     For Hermitian matrices, the range of the continuous function R A ( x ), or f ( x ), is a compact subset [ a , b ] of the real line. The maximum b and the minimum a are the largest and smallest eigenvalue of A , respectively. The min-max theorem is a refinement of this fact.  Min-max Theorem  Let   A   A   A   be a    n  Ã—  n      n  normal-Ã—  n    nÃ—n    Hermitian matrix with eigenvalues then       Î»  k   =   min   {   max   {    R  A    (  x  )    âˆ£   x  âˆˆ   U  and  x   â‰   0   }    âˆ£    dim   (  U  )    =  k   }         subscript  Î»  k          subscript  R  A   x       x    U  and  x        0        dimension  U   k      \lambda_{k}=\min\{\max\{R_{A}(x)\mid x\in U\text{ and }x\neq 0\}\mid\dim(U)=k\}   and       Î»  k   =   max   {   min   {    R  A    (  x  )    âˆ£   x  âˆˆ   U  and  x   â‰   0   }    âˆ£    dim   (  U  )    =    n  -  k   +  1    }         subscript  Î»  k          subscript  R  A   x       x    U  and  x        0        dimension  U       n  k   1       \lambda_{k}=\max\{\min\{R_{A}(x)\mid x\in U\text{ and }x\neq 0\}\mid\dim(U)=n-%
 k+1\}   in particular,        Î»  1   â‰¤    R  A    (  x  )    â‰¤   Î»  n      âˆ€  x   âˆˆ    ğ‚  n   \   {  0  }        formulae-sequence       subscript  Î»  1      subscript  R  A   x         subscript  Î»  n        for-all  x    normal-\   superscript  ğ‚  n    0       \lambda_{1}\leq R_{A}(x)\leq\lambda_{n}\quad\forall x\in\mathbf{C}^{n}%
 \backslash\{0\}   and these bounds are attained when   x   x   x   is an eigenvector of the appropriate eigenvalues.  Also note that the simpler formulation for the maximal eigenvalue Î» n is given by:        Î»  n   =   max   {     R  A    (  x  )    :   x  â‰   0    }     .       subscript  Î»  n      normal-:     subscript  R  A   x     x  0       \lambda_{n}=\max\{R_{A}(x):x\neq 0\}.   Similarly, the minimal eigenvalue Î» 1 is given by:        Î»  1   =   min   {     R  A    (  x  )    :   x  â‰   0    }     .       subscript  Î»  1      normal-:     subscript  R  A   x     x  0       \lambda_{1}=\min\{R_{A}(x):x\neq 0\}.     Proof  Since the matrix   A   A   A   is Hermitian it is diagonalizable and we can choose an orthonormal basis of eigenvectors { u 1 , ..., u n } that is, u i is an eigenvector for the eigenvalue Î» i and such that ( u i , u i ) = 1 and ( u i , u j ) = 0 for all i â‰  j .  If U is a subspace of dimension k then its intersection with the subspace  isn't zero (by simply checking dimensions) and hence there exists a vector    v  â‰   0      v  normal-â‰   0    vâ‰ 0   in this intersection that we can write as      v  =    âˆ‘   i  =  k   n     Î±  i    u  i         v    superscript   subscript     i  k    n      subscript  Î±  i    subscript  u  i       v=\sum_{i=k}^{n}\alpha_{i}u_{i}     and whose Rayleigh quotient is        R  A    (  v  )    =     âˆ‘   i  =  k   n     Î»  i    Î±  i  2       âˆ‘   i  =  k   n    Î±  i  2     â‰¥   Î»  k            subscript  R  A   v       superscript   subscript     i  k    n      subscript  Î»  i    superscript   subscript  Î±  i   2       superscript   subscript     i  k    n    superscript   subscript  Î±  i   2           subscript  Î»  k      R_{A}(v)=\frac{\sum_{i=k}^{n}\lambda_{i}\alpha_{i}^{2}}{\sum_{i=k}^{n}\alpha_{%
 i}^{2}}\geq\lambda_{k}   (as all     Î»  i   â‰¥   Î»  k        subscript  Î»  i    subscript  Î»  k     \lambda_{i}\geq\lambda_{k}   for i=k,..,n) and hence       max   {    R  A    (  x  )    âˆ£   x  âˆˆ  U   }    â‰¥   Î»  k            subscript  R  A   x     x  U     subscript  Î»  k     \max\{R_{A}(x)\mid x\in U\}\geq\lambda_{k}   Since this is true for all U, we can conclude that       min   {   max   {    R  A    (  x  )    âˆ£   x  âˆˆ   U  and  x   â‰   0   }    âˆ£    dim   (  U  )    =  k   }    â‰¥   Î»  k              subscript  R  A   x       x    U  and  x        0        dimension  U   k     subscript  Î»  k     \min\{\max\{R_{A}(x)\mid x\in U\text{ and }x\neq 0\}\mid\dim(U)=k\}\geq\lambda%
 _{k}     This is one inequality. To establish the other inequality, chose the specific k-dimensional space , for which       max   {    R  A    (  x  )    âˆ£   x  âˆˆ   V  and  x   â‰   0   }    â‰¤   Î»  k            subscript  R  A   x       x    V  and  x        0      subscript  Î»  k     \max\{R_{A}(x)\mid x\in V\text{ and }x\neq 0\}\leq\lambda_{k}   because    Î»  k     subscript  Î»  k    \lambda_{k}   is the largest eigenvalue in V. Therefore, also       min   {   max   {    R  A    (  x  )    âˆ£   x  âˆˆ   U  and  x   â‰   0   }    âˆ£    dim   (  U  )    =  k   }    â‰¤   Î»  k              subscript  R  A   x       x    U  and  x        0        dimension  U   k     subscript  Î»  k     \min\{\max\{R_{A}(x)\mid x\in U\text{ and }x\neq 0\}\mid\dim(U)=k\}\leq\lambda%
 _{k}     In the case where U is a subspace of dimension n-k+1 , we proceed in a similar fashion: Consider the subspace of dimension k ,  Its intersection with the subspace U isn't zero (by simply checking dimensions) and hence there exists a vector v in this intersection that we can write as      v  =    âˆ‘   i  =  1   k     Î±  i    u  i         v    superscript   subscript     i  1    k      subscript  Î±  i    subscript  u  i       v=\sum_{i=1}^{k}\alpha_{i}u_{i}   and whose Rayleigh quotient is        R  A    (  v  )    =     âˆ‘   i  =  1   k     Î»  i    Î±  i  2       âˆ‘   i  =  1   k    Î±  i  2     â‰¤   Î»  k            subscript  R  A   v       superscript   subscript     i  1    k      subscript  Î»  i    superscript   subscript  Î±  i   2       superscript   subscript     i  1    k    superscript   subscript  Î±  i   2           subscript  Î»  k      R_{A}(v)=\frac{\sum_{i=1}^{k}\lambda_{i}\alpha_{i}^{2}}{\sum_{i=1}^{k}\alpha_{%
 i}^{2}}\leq\lambda_{k}   and hence       min   {    R  A    (  x  )    âˆ£   x  âˆˆ  U   }    â‰¤   Î»  k            subscript  R  A   x     x  U     subscript  Î»  k     \min\{R_{A}(x)\mid x\in U\}\leq\lambda_{k}   Since this is true for all U, we can conclude that       max   {   min   {    R  A    (  x  )    âˆ£   x  âˆˆ   U  and  x   â‰   0   }    âˆ£    dim   (  U  )    =    n  -  k   +  1    }    â‰¤   Î»  k              subscript  R  A   x       x    U  and  x        0        dimension  U       n  k   1      subscript  Î»  k     \max\{\min\{R_{A}(x)\mid x\in U\text{ and }x\neq 0\}\mid\dim(U)=n-k+1\}\leq%
 \lambda_{k}     Again, this is one part of the equation. To get the other inequality, note again that the eigenvector u of    Î»  k     subscript  Î»  k    \lambda_{k}   is contained in so that we can conclude the equality.  Counterexample in the non-Hermitian case  Let N be the nilpotent matrix       [     0    1      0    0     ]   .      0  1    0  0     \begin{bmatrix}0&1\\
 0&0\end{bmatrix}.     Define the Rayleigh quotient     R  N    (  x  )        subscript  R  N   x    R_{N}(x)   exactly as above in the Hermitian case. Then it is easy to see that the only eigenvalue of N is zero, while the maximum value of the Rayleigh ratio is    1  2      1  2    \frac{1}{2}   . That is, the maximum value of the Rayleigh quotient is larger than the maximum eigenvalue.  Applications  Min-max principle for singular values  The singular values { Ïƒ k } of a square matrix M are the square roots of eigenvalues of M * M (equivalently MM* ). An immediate consequence of the first equality from min-max theorem is       Ïƒ  k  â†‘   =   min   S  :    dim   (  S  )    =  k      max    x  âˆˆ  S   ,    âˆ¥  x  âˆ¥   =  1       (   M  *   M  x  ,  x  )    1  2    =   min   S  :    dim   (  S  )    =  k      max    x  âˆˆ  S   ,    âˆ¥  x  âˆ¥   =  1     âˆ¥  M  x  âˆ¥  .     fragments   superscript   subscript  Ïƒ  k   normal-â†‘     subscript    normal-:  S     dimension  S   k      subscript    formulae-sequence    x  S      norm  x   1      superscript   fragments  normal-(   superscript  M    M  x  normal-,  x  normal-)     1  2      subscript    normal-:  S     dimension  S   k      subscript    formulae-sequence    x  S      norm  x   1     parallel-to  M  x  parallel-to  normal-.    \sigma_{k}^{\uparrow}=\min_{S:\dim(S)=k}\max_{x\in S,\|x\|=1}(M^{*}Mx,x)^{%
 \frac{1}{2}}=\min_{S:\dim(S)=k}\max_{x\in S,\|x\|=1}\|Mx\|.     Similarly,        Ïƒ  k  â†“   =    max   S  :    dim   (  S  )    =    n  -  k   +  1        min    x  âˆˆ  S   ,    âˆ¥  x  âˆ¥   =  1      âˆ¥   M  x   âˆ¥      .       superscript   subscript  Ïƒ  k   normal-â†“     subscript    normal-:  S     dimension  S       n  k   1        subscript    formulae-sequence    x  S      norm  x   1      norm    M  x        \sigma_{k}^{\downarrow}=\max_{S:\dim(S)=n-k+1}\min_{x\in S,\|x\|=1}\|Mx\|.     Cauchy interlacing theorem  Let   A   A   A   be a symmetric n Ã— n matrix. The m Ã— m matrix B , where m â‰¤ n , is called a compression of   A   A   A   if there exists an orthogonal projection P onto a subspace of dimension m such that P*AP = B . The Cauchy interlacing theorem states:   Theorem. If the eigenvalues of   A   A   A   are , and those of B are , then for all      Î±  j   â‰¤   Î²  j   â‰¤   Î±    n  -  m   +  j     .         subscript  Î±  j    subscript  Î²  j         subscript  Î±      n  m   j       \alpha_{j}\leq\beta_{j}\leq\alpha_{n-m+j}.      This can be proven using the min-max principle. Let Î² i have corresponding eigenvector b i and S j be the j dimensional subspace  then        Î²  j   =    min    x  âˆˆ   S    m  -  j   +  1     ,    âˆ¥  x  âˆ¥   =  1      (   B  x   ,  x  )    =    min    x  âˆˆ   S    m  -  j   +  1     ,    âˆ¥  x  âˆ¥   =  1      (    P  *   A  P  x   ,  x  )    =    min    x  âˆˆ   S    m  -  j   +  1     ,    âˆ¥  x  âˆ¥   =  1      (   A  x   ,  x  )    â‰¤   Î±    n  -  m   +  j     ,         subscript  Î²  j     subscript    formulae-sequence    x   subscript  S      m  j   1        norm  x   1       B  x   x          subscript    formulae-sequence    x   subscript  S      m  j   1        norm  x   1        superscript  P    A  P  x   x          subscript    formulae-sequence    x   subscript  S      m  j   1        norm  x   1       A  x   x         subscript  Î±      n  m   j       \beta_{j}=\min_{x\in S_{m-j+1},\|x\|=1}(Bx,x)=\min_{x\in S_{m-j+1},\|x\|=1}(P^%
 {*}APx,x)=\min_{x\in S_{m-j+1},\|x\|=1}(Ax,x)\leq\alpha_{n-m+j},     According to first part of min-max, On the other hand, if we define  then       n  âˆ’  m   =  1        n  normal-âˆ’  m   1    nâˆ’m=1     where the last inequality is given by the second part of min-max.  Notice that, when   A   A   A   , we have , hence the name interlacing theorem.  Compact operators  Let   A   A   A   be a compact , Hermitian operator on a Hilbert space H . Recall that the spectrum of such an operator form a sequence of real numbers whose only possible cluster point is zero. Every nonzero number in the spectrum is an eigenvalue. It no longer makes sense here to list the positive eigenvalues in increasing order. Let the positive eigenvalues of     â‹¯  â‰¤   Î»  k   â‰¤  â‹¯  â‰¤   Î»  1    ,        normal-â‹¯   subscript  Î»  k        normal-â‹¯        subscript  Î»  1      \cdots\leq\lambda_{k}\leq\cdots\leq\lambda_{1},   be     A   A   A     where multiplicity is taken into account as in the matrix case. When H is infinite-dimensional, the above sequence of eigenvalues is necessarily infinite. We now apply the same reasoning as in the matrix case. Letting S k âŠ‚ H be a k dimensional subspace, we can obtain the following theorem.   Theorem (Min-Max). Let   H   H   H   be a compact, self-adjoint operator on a Hilbert space       min   S   k  -  1       max    x  âˆˆ   S   k  -  1   âŸ‚    ,    âˆ¥  x  âˆ¥   =  1      (   A  x   ,  x  )     =   Î»  k    .        subscript    subscript  S    k  1       subscript    formulae-sequence    x   superscript   subscript  S    k  1    perpendicular-to       norm  x   1       A  x   x     subscript  Î»  k     \min_{S_{k-1}}\max_{x\in S_{k-1}^{\perp},\|x\|=1}(Ax,x)=\lambda_{k}.   , whose positive eigenvalues are listed in decreasing order . Then: : \begin{align}    \max_{S_k} \min_{x \in S_k, \|x\| = 1} (Ax,x) &= \lambda_k ^{\downarrow}, \\ \min_{S_{k-1}} \max_{x \in S_{k-1}^{\perp}, \|x\|=1} (Ax, x) &= \lambda_k^{\downarrow}. \end{align}  A similar pair of equalities hold for negative eigenvalues.  Proof:  \max_{x \in S_{k-1}^{\perp}, \|x\|=1} (Ax, x) \ge \lambda_k.  Pick S k âˆ’1 = span{ u 1 , ..., u k âˆ’1 } and we deduce       E  1   â‰¤   E  2   â‰¤   E  3   â‰¤  â‹¯         subscript  E  1    subscript  E  2         subscript  E  3        normal-â‹¯     E_{1}\leq E_{2}\leq E_{3}\leq\cdots   }}  Self-adjoint operators  The min-max theorem also applies to (possibly unbounded) self-adjoint operators. 1  2 Recall the essential spectrum is the spectrum without isolated eigenvalues of finite multiplicity. Sometimes we have some eigenvalues below the bottom of the eessential spectrum, and we would like to approximate the eigenvalues and eigenfunctions.  Theorem (Min-Max). Let A be self-adjoint, and let     E  n   =    min    Ïˆ  1   ,  â€¦  ,   Ïˆ   n  -  1       max   {    âŸ¨  Ïˆ  ,   A  Ïˆ   âŸ©   :   Ïˆ  âˆˆ   span   (   Ïˆ  1   ,  â€¦  ,   Ïˆ   n  -  1    )      }          subscript  E  n     subscript     subscript  Ïˆ  1   normal-â€¦   subscript  Ïˆ    n  1         normal-:   Ïˆ    A  Ïˆ      Ïˆ   span   subscript  Ïˆ  1   normal-â€¦   subscript  Ïˆ    n  1           E_{n}=\min_{\psi_{1},\ldots,\psi_{n-1}}\max\{\langle\psi,A\psi\rangle:\psi\in%
 \operatorname{span}(\psi_{1},\ldots,\psi_{n-1})\}   be the eigenvalues of A below the essential spectrum. Then       E  n   :=   inf    Ïƒ   e  s  s     (  A  )        assign   subscript  E  n    infimum     subscript  Ïƒ    e  s  s    A      E_{n}:=\inf\sigma_{ess}(A)   .  If we only have N eigenvalues and hence run out of eigenvalues, then we let     E  1   â‰¤   E  2   â‰¤   E  3   â‰¤  â‹¯         subscript  E  1    subscript  E  2         subscript  E  3        normal-â‹¯     E_{1}\leq E_{2}\leq E_{3}\leq\cdots   (the bottom of the essential spectrum) for n>N , and the above statement holds after replacing min-max with inf-sup.  Theorem (Max-Min). Let A be self-adjoint, and let     E  n   =    max    Ïˆ  1   ,  â€¦  ,   Ïˆ   n  -  1       min   {    âŸ¨  Ïˆ  ,   A  Ïˆ   âŸ©   :   Ïˆ  âŸ‚    Ïˆ  1   ,  â€¦  ,   Ïˆ   n  -  1       }          subscript  E  n     subscript     subscript  Ïˆ  1   normal-â€¦   subscript  Ïˆ    n  1         normal-:   Ïˆ    A  Ïˆ     perpendicular-to  Ïˆ    subscript  Ïˆ  1   normal-â€¦   subscript  Ïˆ    n  1           E_{n}=\max_{\psi_{1},\ldots,\psi_{n-1}}\min\{\langle\psi,A\psi\rangle:\psi%
 \perp\psi_{1},\ldots,\psi_{n-1}\}   be the eigenvalues of A below the essential spectrum. Then       E  n   :=   inf    Ïƒ   e  s  s     (  A  )        assign   subscript  E  n    infimum     subscript  Ïƒ    e  s  s    A      E_{n}:=\inf\sigma_{ess}(A)   .  If we only have N eigenvalues and hence run out of eigenvalues, then we let     (   A  -  E   )   â‰¥  0        A  E   0    (A-E)\geq 0   (the bottom of the essential spectrum) for n>N , and the above statement holds after replacing max-min with sup-inf.  The proofs 3 4 use the following results about self-adjoint operators:  Theorem. Let A be self-adjoint. Then    E  âˆˆ  â„      E  â„    E\in\mathbb{R}   for     Ïƒ   (  A  )    âŠ†   [  E  ,  âˆ  )         Ïƒ  A    E      \sigma(A)\subseteq[E,\infty)   if and only if     inf   Ïƒ   (  A  )     =    inf    Ïˆ  âˆˆ   ğ”‡   (  A  )     ,    âˆ¥  Ïˆ  âˆ¥   =  1      âŸ¨  Ïˆ  ,   A  Ïˆ   âŸ©         infimum    Ïƒ  A      subscript  infimum   formulae-sequence    Ïˆ    ğ”‡  A       norm  Ïˆ   1      Ïˆ    A  Ïˆ       \inf\sigma(A)=\inf_{\psi\in\mathfrak{D}(A),\|\psi\|=1}\langle\psi,A\psi\rangle   .  Theorem. If A is self-adjoint, then       sup   Ïƒ   (  A  )     =    sup    Ïˆ  âˆˆ   ğ”‡   (  A  )     ,    âˆ¥  Ïˆ  âˆ¥   =  1      âŸ¨  Ïˆ  ,   A  Ïˆ   âŸ©         supremum    Ïƒ  A      subscript  supremum   formulae-sequence    Ïˆ    ğ”‡  A       norm  Ïˆ   1      Ïˆ    A  Ïˆ       \sup\sigma(A)=\sup_{\psi\in\mathfrak{D}(A),\|\psi\|=1}\langle\psi,A\psi\rangle     and  $\sup\sigma(A)=\sup_{\psi\in\mathfrak{D}(A),\|\psi\|=1}\langle\psi,A\psi\rangle$ .  See also   Courant minimax principle  Maxâ€“min inequality   References   M. Reed and B. Simon, Methods of Modern Mathematical Physics IV: Analysis of Operators , Academic Press, 1978.   "  Category:Articles containing proofs  Category:Theorems in functional analysis  Category:Spectral theory  Category:Operator theory     G. Teschl, Mathematical Methods in Quantum Mechanics (GSM 99) http://www.mat.univie.ac.at/~gerald/ftp/book-schroe/schroe.pdf â†©  Lieb-Loss, Analysis 2nd ed. (GSM 14) â†©       