   Nearest neighbour classifiers      Nearest neighbour classifiers   Nearest neighbour classifiers are a class of non-parametric methods used in statistical classification (or pattern recognition ). The method classifies objects based on closest training examples in the feature space.  Statistical setting  Suppose we have pairs     (  X  ,  Y  )   ,   (   X  1   ,   Y  1   )   ,  ‚Ä¶  ,   (   X  n   ,   Y  n   )       X  Y     subscript  X  1    subscript  Y  1    normal-‚Ä¶    subscript  X  n    subscript  Y  n      (X,Y),(X_{1},Y_{1}),\dots,(X_{n},Y_{n})   taking values in     ‚Ñù  d   √ó   {  1  ,  2  }        superscript  ‚Ñù  d    1  2     \mathbb{R}^{d}\times\{1,2\}   , where   Y   Y   Y   is the class label of   X   X   X   , so that    X  |  Y  =  r  ‚àº   P  r      fragments  X  normal-|  Y   r  similar-to   subscript  P  r     X|Y=r\sim P_{r}   for    r  =   1  ,  2       r   1  2     r=1,2   (and probability distributions    P  r     subscript  P  r    P_{r}   ). Given some norm    ‚à•  ‚ãÖ  ‚à•     fragments  parallel-to  normal-‚ãÖ  parallel-to    \|\cdot\|   on    ‚Ñù  d     superscript  ‚Ñù  d    \mathbb{R}^{d}   and a point    x  ‚àà   ‚Ñù  d       x   superscript  ‚Ñù  d     x\in\mathbb{R}^{d}   , let     (   X   (  1  )    ,   Y   (  1  )    )   ,  ‚Ä¶  ,   (   X   (  n  )    ,   Y   (  n  )    )        subscript  X  1    subscript  Y  1    normal-‚Ä¶    subscript  X  n    subscript  Y  n      (X_{(1)},Y_{(1)}),\dots,(X_{(n)},Y_{(n)})   be a reordering of the training data such that     ‚à•    X   (  1  )    -  x   ‚à•   ‚â§  ‚Ä¶  ‚â§   ‚à•    X   (  n  )    -  x   ‚à•          norm     subscript  X  1   x    normal-‚Ä¶        norm     subscript  X  n   x       \|X_{(1)}-x\|\leq\dots\leq\|X_{(n)}-x\|   .  The   1   1   1   -nearest neighbour classifier  The most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point   x   x   x   to the class of its closest neighbour in the feature space, that is      C  n   1  n  n     (  x  )    =   Y   (  1  )           superscript   subscript  C  n     1  n  n    x    subscript  Y  1     C_{n}^{1nn}(x)=Y_{(1)}   .  As the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data).  The   k   k   k   -nearest neighbour classifier  The   k   k   k   -nearest neighbour classifier assigns a point   x   x   x   to a particular class based on a majority vote among the classes of the   k   k   k   nearest training points to   x   x   x   .  Properties  There are many results on the error rate of the   k   k   k   nearest neighbour classifiers. 1 The   k   k   k   -nearest neighbour classifier is strongly (that is for any joint distribution on    (  X  ,  Y  )     X  Y    (X,Y)   ) consistent provided    k  :=   k  n      assign  k   subscript  k  n     k:=k_{n}   diverges and     k  n   /  n       subscript  k  n   n    k_{n}/n   converges to zero as    n  ‚Üí  ‚àû     normal-‚Üí  n     n\to\infty   .  Let    C  n   k  n  n      superscript   subscript  C  n     k  n  n     C_{n}^{knn}   denote the   k   k   k   nearest neighbour classifier based on a training set of size   n   n   n   . Under certain regularity conditions, the excess risk yields the following asymptotic expansion 2            ‚Ñõ  ‚Ñõ    (   C  n   k  n  n    )    -    ‚Ñõ  ‚Ñõ    (   C   B  a  y  e  s    )     =    {     B  1    1  k    +    B  2     (   k  n   )    4  /  d      }    {   1  +   o   (  1  )     }     ,           subscript  ‚Ñõ  ‚Ñõ    subscript   superscript  C    k  n  n    n       subscript  ‚Ñõ  ‚Ñõ    superscript  C    B  a  y  e  s              subscript  B  1     1  k       subscript  B  2    superscript    k  n     4  d          1    o  1        \mathcal{R}_{\mathcal{R}}(C^{knn}_{n})-\mathcal{R}_{\mathcal{R}}(C^{Bayes})=%
 \left\{B_{1}\frac{1}{k}+B_{2}\left(\frac{k}{n}\right)^{4/d}\right\}\{1+o(1)\},        for some constants    B  1     subscript  B  1    B_{1}   and    B  2     subscript  B  2    B_{2}   .  The choice     k  *   =   ‚åä   B   n   4   d  +  4      ‚åã        superscript  k        B   superscript  n    4    d  4         k^{*}=\lfloor Bn^{\frac{4}{d+4}}\rfloor   offers a trade off between the two terms in the above display, for which the    k  *     superscript  k     k^{*}   -nearest neighbour error converges to the Bayes error at the optimal ( minimax ) rate    ùí™   (   n   -   4   d  +  4      )       ùí™   superscript  n      4    d  4        \mathcal{O}(n^{-\frac{4}{d+4}})   .  The weighted nearest neighbour classifier  The   k   k   k   -nearest neighbour classifier can be viewed as assigning the   k   k   k   nearest neighbours a weight    1  /  k      1  k    1/k   and all others   0   0    weight. This can be generalised to weighted nearest neighbour classifiers. That is, where the   i   i   i   th nearest neighbour is assigned a weight    w   n  i      subscript  w    n  i     w_{ni}   , with      ‚àë   i  =  1   n    w   n  i     =  1        superscript   subscript     i  1    n    subscript  w    n  i     1    \sum_{i=1}^{n}w_{ni}=1   . An analogous result on the strong consistency of weighted nearest neighbour classifiers also holds. 3  Let    C  n   w  n  n      subscript   superscript  C    w  n  n    n    C^{wnn}_{n}   denote the weighted nearest classifier with weights     {   w   n  i    }    i  =  1   n     superscript   subscript    subscript  w    n  i       i  1    n    \{w_{ni}\}_{i=1}^{n}   . Subject to regularity conditions on to class distributions the excess risk has the following asymptotic expansion 4          ‚Ñõ  ‚Ñõ    (   C  n   w  n  n    )    -    ‚Ñõ  ‚Ñõ    (   C   B  a  y  e  s    )     =    (     B  1    s  n  2    +    B  2    t  n  2     )    {   1  +   o   (  1  )     }     ,           subscript  ‚Ñõ  ‚Ñõ    subscript   superscript  C    w  n  n    n       subscript  ‚Ñõ  ‚Ñõ    superscript  C    B  a  y  e  s             subscript  B  1    superscript   subscript  s  n   2       subscript  B  2    superscript   subscript  t  n   2        1    o  1        \mathcal{R}_{\mathcal{R}}(C^{wnn}_{n})-\mathcal{R}_{\mathcal{R}}(C^{Bayes})=%
 \left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\right)\{1+o(1)\},   for constants    B  1     subscript  B  1    B_{1}   and    B  2     subscript  B  2    B_{2}   where     s  n  2   =    ‚àë   i  =  1   n    w   n  i   2         superscript   subscript  s  n   2     superscript   subscript     i  1    n    superscript   subscript  w    n  i    2      s_{n}^{2}=\sum_{i=1}^{n}w_{ni}^{2}   and     t  n   =    n   -   2  /  d       ‚àë   i  =  1   n     w   n  i     {    i   1  +   2  /  d     -    (   i  -  1   )    1  +   2  /  d      }           subscript  t  n      superscript  n      2  d       superscript   subscript     i  1    n      subscript  w    n  i        superscript  i    1    2  d      superscript    i  1     1    2  d            t_{n}=n^{-2/d}\sum_{i=1}^{n}w_{ni}\{i^{1+2/d}-(i-1)^{1+2/d}\}   .  The optimal weighting scheme     {   w   n  i   *   }    i  =  1   n     superscript   subscript    superscript   subscript  w    n  i         i  1    n    \{w_{ni}^{*}\}_{i=1}^{n}   , that balances the two terms in the display above, is given as follows: set     k  *   =   ‚åä   B   n   4   d  +  4      ‚åã        superscript  k        B   superscript  n    4    d  4         k^{*}=\lfloor Bn^{\frac{4}{d+4}}\rfloor   ,       w   n  i   *   =    1   k  *     [    1  +   d  2    -    d   2   k   *    2  /  d       {    i   1  +   2  /  d     -    (   i  -  1   )    1  +   2  /  d      }     ]         superscript   subscript  w    n  i          1   superscript  k      delimited-[]      1    d  2        d    2   superscript   superscript  k      2  d          superscript  i    1    2  d      superscript    i  1     1    2  d             w_{ni}^{*}=\frac{1}{k^{*}}\left[1+\frac{d}{2}-\frac{d}{2{k^{*}}^{2/d}}\{i^{1+2%
 /d}-(i-1)^{1+2/d}\}\right]   for    i  =   1  ,  2  ,  ‚Ä¶  ,   k  *        i   1  2  normal-‚Ä¶   superscript  k       i=1,2,\dots,k^{*}   and       w   n  i   *   =  0       subscript   superscript  w      n  i    0    w^{*}_{ni}=0   for    i  =     k  *   +  1   ,  ‚Ä¶  ,  n       i      superscript  k    1   normal-‚Ä¶  n     i=k^{*}+1,\dots,n   .  With optimal weights the dominant term in the asymptotic expansion of the excess risk is    ùí™   (   n   -   4   d  +  4      )       ùí™   superscript  n      4    d  4        \mathcal{O}(n^{-\frac{4}{d+4}})   . Similar results are true when using a bagged nearest neighbour classifier .  References  Further reading  "  Category:Statistical classification  Category:Classification algorithms     ‚Ü©  ‚Ü©  ‚Ü©      