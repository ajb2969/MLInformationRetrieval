<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1109">Instantaneously trained neural networks</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Instantaneously trained neural networks</h1>
<hr/>

<p><strong>Instantaneously trained neural networks</strong> are feedforward <a href="artificial_neural_networks" title="wikilink">artificial neural networks</a> that create a new hidden neuron node for each novel training sample. The weights to this hidden neuron separate out not only this training sample but others that are near it, thus providing generalization.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> This training can be done in a variety of ways and the most popular network in this family is called the CC4 network where the separation is done using the nearest hyperplane that can be written down instantaneously. These networks use <a href="unary_coding" title="wikilink">unary coding</a> for an effective representation of the data sets.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>Instantaneously trained neural networks have been proposed as models of short term <a class="uri" href="learning" title="wikilink">learning</a> and used in <a href="web_search" title="wikilink">web search</a>, and financial <a href="time_series_prediction" title="wikilink">time series prediction</a> applications.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> They have also been used in instant <a href="document_classification" title="wikilink">classification of documents</a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> and for deep learning and data mining.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>

<p>As in other neural networks, their normal use is as software, but they have also been implemented in hardware using FPGAs<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> and by optical implementation.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<h2 id="cc4-network">CC4 network</h2>

<p>In the CC4 network, which is a three-stage network, the number of input nodes is one more than the size of the training vector, with the extra node serving as the biasing node whose input is always 1. For binary input vectors, the weights from the input nodes to the hidden neuron (say of index j) corresponding to the trained vector is given by the following formula:</p>

<p>

<math display="block" id="Instantaneously_trained_neural_networks:0">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mrow>
     <mi>i</mi>
     <mi>j</mi>
    </mrow>
   </msub>
   <mo>=</mo>
   <mrow>
    <mo>{</mo>
    <mtable displaystyle="true">
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mo>-</mo>
         <mn>1</mn>
        </mrow>
        <mo>,</mo>
       </mrow>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mtext>for</mtext>
         <msub>
          <mi>x</mi>
          <mi>i</mi>
         </msub>
        </mrow>
        <mo>=</mo>
        <mn>0</mn>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mo>+</mo>
         <mn>1</mn>
        </mrow>
        <mo>,</mo>
       </mrow>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mtext>for</mtext>
         <msub>
          <mi>x</mi>
          <mi>i</mi>
         </msub>
        </mrow>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mrow>
          <mi>r</mi>
          <mo>+</mo>
          <mi>s</mi>
         </mrow>
         <mo>-</mo>
         <mn>1</mn>
        </mrow>
        <mo>,</mo>
       </mrow>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mtext>for</mtext>
         <mi>i</mi>
        </mrow>
        <mo>=</mo>
        <mrow>
         <mi>n</mi>
         <mo>+</mo>
         <mn>1</mn>
        </mrow>
       </mrow>
      </mtd>
     </mtr>
    </mtable>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <apply>
      <times></times>
      <ci>i</ci>
      <ci>j</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="latexml">cases</csymbol>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <eq></eq>
      <apply>
       <times></times>
       <mtext>for</mtext>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
      </apply>
      <cn type="integer">0</cn>
     </apply>
     <apply>
      <plus></plus>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <eq></eq>
      <apply>
       <times></times>
       <mtext>for</mtext>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
      </apply>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <minus></minus>
      <apply>
       <plus></plus>
       <ci>r</ci>
       <ci>s</ci>
      </apply>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <eq></eq>
      <apply>
       <times></times>
       <mtext>for</mtext>
       <ci>i</ci>
      </apply>
      <apply>
       <plus></plus>
       <ci>n</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{ij}=\begin{cases}-1,&\mbox{for }x_{i}=0\\
+1,&\mbox{for }x_{i}=1\\
r+s-1,&\mbox{for }i=n+1\end{cases}
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Instantaneously_trained_neural_networks:1">
 <semantics>
  <mi>r</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>r</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r
  </annotation>
 </semantics>
</math>

 is the radius of generalization and 

<math display="inline" id="Instantaneously_trained_neural_networks:2">
 <semantics>
  <mi>s</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>s</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s
  </annotation>
 </semantics>
</math>

 is the <a href="Hamming_weight" title="wikilink">Hamming weight</a> (the number of 1s) of the binary sequence. From the hidden layer to the output layer the weights are 1 or -1 depending on whether the vector belongs to a given output class or not. The neurons in the hidden and output layers output 1 if the weighted sum to the input is 0 or positive and 0, if the weighted sum to the input is negative:</p>

<p>

<math display="block" id="Instantaneously_trained_neural_networks:3">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mrow>
    <mo>{</mo>
    <mtable displaystyle="true">
     <mtr>
      <mtd columnalign="center">
       <mn>1</mn>
      </mtd>
      <mtd columnalign="center">
       <mrow>
        <mrow>
         <mtext>if</mtext>
         <mstyle displaystyle="false">
          <mrow>
           <mo largeop="true" symmetric="true">∑</mo>
           <msub>
            <mi>x</mi>
            <mi>i</mi>
           </msub>
          </mrow>
         </mstyle>
        </mrow>
        <mo>≥</mo>
        <mn>0</mn>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="center">
       <mn>0</mn>
      </mtd>
      <mtd columnalign="center">
       <mrow>
        <mrow>
         <mtext>if</mtext>
         <mstyle displaystyle="false">
          <mrow>
           <mo largeop="true" symmetric="true">∑</mo>
           <msub>
            <mi>x</mi>
            <mi>i</mi>
           </msub>
          </mrow>
         </mstyle>
        </mrow>
        <mo><</mo>
        <mn>0</mn>
       </mrow>
      </mtd>
     </mtr>
    </mtable>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">y</csymbol>
    <eq></eq>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-{</ci>
     <matrix>
      <matrixrow>
       <cn type="integer">1</cn>
       <apply>
        <geq></geq>
        <apply>
         <times></times>
         <mtext>if</mtext>
         <apply>
          <sum></sum>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>i</ci>
          </apply>
         </apply>
        </apply>
        <cn type="integer">0</cn>
       </apply>
      </matrixrow>
      <matrixrow>
       <cn type="integer">0</cn>
       <apply>
        <lt></lt>
        <apply>
         <times></times>
         <mtext>if</mtext>
         <apply>
          <sum></sum>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>i</ci>
          </apply>
         </apply>
        </apply>
        <cn type="integer">0</cn>
       </apply>
      </matrixrow>
     </matrix>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=\left\{\begin{matrix}1&\mbox{if }\sum x_{i}\geq 0\\
0&\mbox{if }\sum x_{i}<0\end{matrix}\right.
  </annotation>
 </semantics>
</math>

</p>
<h2 id="other-networks">Other networks</h2>

<p>In feedback networks the Willshaw network as well as the <a href="Hopfield_network" title="wikilink">Hopfield network</a> are able to learn instantaneously. But these networks are plagued with spurious memories.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h2 id="references">References</h2>

<p>"</p>

<p><a class="uri" href="Category:Learning" title="wikilink">Category:Learning</a> <a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a> <a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Kak, S. On training feedforward neural networks. Pramana, vol. 40, pp. 35-42, 1993<a href="#fnref1">↩</a></li>
<li id="fn2">Kak, S. New algorithms for training feedforward neural networks. Pattern Recognition Letters 15: 295-298, 1994.<a href="#fnref2">↩</a></li>
<li id="fn3">Kak, S. On generalization by neural networks, Information Sciences 111: 293-302, 1998.<a href="#fnref3">↩</a></li>
<li id="fn4">Kak, S. Faster web search and prediction using instantaneously trained neural networks. IEEE Intelligent Systems 14: 79-82, November/December 1999.<a href="#fnref4">↩</a></li>
<li id="fn5">Zhang, Z. et al., TextCC: New feedforward neural network for classifying documents instantly. Advances in Neural Networks ISNN 2005. Lecture Notes in Computer Science 3497: 232-237, 2005.<a href="#fnref5">↩</a></li>
<li id="fn6">Zhang, Z. et al., Document Classification Via TextCC Based on Stereographic Projection and for deep learning, International Conference on Machine Learning and Cybernetics, Dalin, 2006<a href="#fnref6">↩</a></li>
<li id="fn7">Schmidhuber, J. Deep Learning in Neural Networks: An Overview, arXiv:1404.7828, 2014 <a class="uri" href="http://arxiv.org/abs/1404.7828">http://arxiv.org/abs/1404.7828</a><a href="#fnref7">↩</a></li>
<li id="fn8">Zhu, J. and G. Milne, Implementing Kak Neural Networks on a Reconfigurable Computing Platform, Lecture Notes in Computer Science Volume 1896: 260-269, 2000.<a href="#fnref8">↩</a></li>
<li id="fn9">Shortt, A., J.G. Keating, L. Moulinier, C.N. Pannell, Optical implementation of the Kak neural network, Information Sciences 171: 273-287, 2005.<a href="#fnref9">↩</a></li>
<li id="fn10">Ponnath, A. Instantaneously trained neural networks. 2006. <a class="uri" href="http://arxiv.org/pdf/cs/0601129.pdf">http://arxiv.org/pdf/cs/0601129.pdf</a><a href="#fnref10">↩</a></li>
</ol>
</section>
</body>
</html>
