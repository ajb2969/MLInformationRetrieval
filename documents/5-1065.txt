   Linear probability model      Linear probability model   In statistics , a linear probability model is a special case of a binomial regression model. Here the dependent variable for each observation takes values which are either 0 or 1. The probability of observing a 0 or 1 in any one case is treated as depending on one or more explanatory variables . For the "linear probability model", this relationship is a particularly simple one, and allows the model to be fitted by simple linear regression .  The model assumes that, for a binary outcome ( Bernoulli trial ),   Y   Y   Y   , and its associated vector of explanatory variables,   X   X   X   , 1        Pr   (   Y  =  1   |   X  =  x   )    =    x  ′   β    .       Pr    Y  1     X  x       superscript  x  normal-′   β     \Pr(Y=1|X=x)=x^{\prime}\beta.     For this model,      E   [  Y  |  X  ]   =  Pr   (  Y  =  1  |  X  )   =   x  ′   β  ,     fragments  E   fragments  normal-[  Y  normal-|  X  normal-]    Pr   fragments  normal-(  Y   1  normal-|  X  normal-)     superscript  x  normal-′   β  normal-,    E[Y|X]=\Pr(Y=1|X)=x^{\prime}\beta,   and hence the vector of parameters β can be estimated using least squares . This method of fitting would be inefficient. 2 This method of fitting can be improved by adopting an iterative scheme based on weighted least squares , 3 in which the model from the previous iteration is used to supply estimates of the conditional variances,    Var   (  Y  |   X  =  x   )      Var  Y    X  x     \operatorname{Var}(Y|X=x)   , which would vary between observations. This approach can be related to fitting the model by maximum likelihood . 4  A drawback of this model is that, unless restrictions are placed on   β   β   \beta   , the estimated coefficients can imply probabilities outside the unit interval     [  0  ,  1  ]     0  1    [0,1]   . For this reason, models such as the logit model or the probit model are more commonly used.  References  Further reading     "  Category:Generalized linear models     ↩        