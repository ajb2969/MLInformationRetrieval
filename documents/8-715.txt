   Hamiltonian (control theory)      Hamiltonian (control theory)   The Hamiltonian of optimal control theory was developed by Lev Pontryagin as part of his minimum principle . 1 It was inspired by, but is distinct from, the Hamiltonian of classical mechanics. Pontryagin proved that a necessary condition for solving the optimal control problem is that the control should be chosen so as to minimize the Hamiltonian. For details see Pontryagin's minimum principle .  Notation and Problem statement  A control    u   (  t  )       u  t    u(t)   is to be chosen so as to minimize the objective function       J   (  u  )    =    Ψ   (   x   (  T  )    )    +    ∫  0  T    L   (  x  ,  u  ,  t  )   d  t           J  u       normal-Ψ    x  T      subscript   superscript   T   0     L   x  u  t   d  t       J(u)=\Psi(x(T))+\int^{T}_{0}L(x,u,t)dt     where    x   (  t  )       x  t    x(t)   is the system state, which evolves according to the state equations        x  ˙   =   f   (  x  ,  u  ,  t  )        x   (  0  )    =   x  0     t  ∈   [  0  ,  T  ]        formulae-sequence     normal-˙  x     f   x  u  t      formulae-sequence      x  0    subscript  x  0      t   0  T       \dot{x}=f(x,u,t)\qquad x(0)=x_{0}\quad t\in[0,T]     and the control must satisfy the constraints       a  ≤   u   (  t  )    ≤  b    t  ∈   [  0  ,  T  ]       formulae-sequence      a    u  t        b      t   0  T      a\leq u(t)\leq b\quad t\in[0,T]     Definition of the Hamiltonian       H   (  x  ,  λ  ,  u  ,  t  )    =     λ  T    (  t  )   f   (  x  ,  u  ,  t  )    -   L   (  x  ,  u  ,  t  )           H   x  λ  u  t         superscript  λ  T   t  f   x  u  t      L   x  u  t       H(x,\lambda,u,t)=\lambda^{T}(t)f(x,u,t)-L(x,u,t)\,     where    λ   (  t  )       λ  t    \lambda(t)   is a vector of costate variables of the same dimension as the state variables    x   (  t  )       x  t    x(t)   .  For information on the properties of the Hamiltonian, see Pontryagin's maximum principle .  The Hamiltonian in discrete time  When the problem is formulated in discrete time, the Hamiltonian is defined as:       H   (  x  ,  λ  ,  u  ,  t  )    =     λ  T    (   t  +  1   )   f   (  x  ,  u  ,  t  )    -   L   (  x  ,  u  ,  t  )           H   x  λ  u  t         superscript  λ  T     t  1   f   x  u  t      L   x  u  t       H(x,\lambda,u,t)=\lambda^{T}(t+1)f(x,u,t)-L(x,u,t)\,     and the costate equations are       λ   (   t  +  1   )    =    -     ∂  H    ∂  x    d  t    +   λ   (  t  )           λ    t  1              H     x    d  t      λ  t      \lambda(t+1)=-\frac{\partial H}{\partial x}dt+\lambda(t)   (Note that the discrete time Hamiltonian at time   t   t   t   involves the costate variable at time    t  +  1.      t  1.    t+1.    2 This small detail is essential so that when we differentiate with respect to   x   x   x   we get a term involving    λ   (   t  +  1   )       λ    t  1     \lambda(t+1)   on the right hand side of the costate equations. Using a wrong convention here can lead to incorrect results, i.e. a costate equation which is not a backwards difference equation).  The Hamiltonian of control compared to the Hamiltonian of mechanics  William Rowan Hamilton defined the Hamiltonian as a function of three variables:      ℋ  =   ℋ   (  p  ,  q  ,  t  )    =    ⟨  p  ,   q  ˙   ⟩   -   L   (  q  ,   q  ˙   ,  t  )           ℋ    ℋ   p  q  t            p   normal-˙  q      L   q   normal-˙  q   t        \mathcal{H}=\mathcal{H}(p,q,t)=\langle p,\dot{q}\rangle-L(q,\dot{q},t)     where    q  ˙     normal-˙  q    \dot{q}   is defined implicitly by      p  =    ∂  L    ∂   q  ˙         p      L      normal-˙  q       p=\frac{\partial L}{\partial\dot{q}}     Hamilton then formulated his equations as        d   d  t    p   (  t  )    =   -    ∂   ∂  q    ℋ            d    d  t    p  t            q    ℋ      \frac{d}{dt}p(t)=-\frac{\partial}{\partial q}\mathcal{H}           d   d  t    q   (  t  )    =    ∂   ∂  p    ℋ           d    d  t    q  t          p    ℋ     \frac{d}{dt}q(t)=~{}~{}\frac{\partial}{\partial p}\mathcal{H}     Similarly the Hamiltonian of control theory (as normally defined) is a function of 4 variables       H   (  q  ,  u  ,  p  ,  t  )    =    ⟨  p  ,   q  ˙   ⟩   -   L   (  q  ,  u  ,  t  )           H   q  u  p  t       p   normal-˙  q      L   q  u  t       H(q,u,p,t)=\langle p,\dot{q}\rangle-L(q,u,t)     and the associated conditions for a maximum are        d  p    d  t    =   -    ∂  H    ∂  q             d  p     d  t          H     q       \frac{dp}{dt}=-\frac{\partial H}{\partial q}           d  q    d  t    =    ∂  H    ∂  p            d  q     d  t        H     p      \frac{dq}{dt}=~{}~{}\frac{\partial H}{\partial p}           ∂  H    ∂  u    =  0          H     u    0    \frac{\partial H}{\partial u}=0     This definition agrees with that given by the article by Sussmann and Willems. 3 (see p. 39, equation 14). Sussmann-Willems show how the control Hamiltonian can be used in dynamics e.g. for the brachystochrone problem, but do not mention the prior work of Carathéodory on this approach . 4  References  External links   P. Varaiya: Lecture Notes on Optimization , 2d. ed. (1998) 1  I. M. Ross , Pontryagin's Hamiltonian Illustrated with Examples, 2009, Chapter 2 download   "  Category:Optimal control     I. M. Ross A Primer on Pontryagin's Principle in Optimal Control , Collegiate Publishers, 2009. ↩  Varaiya, Chapter 6 ↩  ↩  See H. J. Pesch- R. Bulirsch: J.O.T.A. 80 1994 199-225 ↩     