   Diffusion map      Diffusion map  '''Diffusion maps''' is a [[dimensionality reduction]] or [[feature extraction]] algorithm introduced by [[Ronald Coifman|R. R. Coifman]] and S. Lafon. 1 2 3 4 It computes a family of embeddings of a data set into Euclidean space (often low-dimensional) whose coordinates can be computed from the eigenvectors and eigenvalues of a diffusion operator on the data. The Euclidean distance between points in the embedded space is equal to the "diffusion distance" between probability distributions centered at those points. Different from linear dimensionality reduction methods such as principal component analysis (PCA) and multi-dimensional scaling (MDS), diffusion maps is part of the family of nonlinear dimensionality reduction methods which focus on discovering the underlying manifold that the data has been sampled from. By integrating local similarities at different scales, diffusion maps gives a global description of the data-set. Compared with other methods, the diffusion maps algorithm is robust to noise perturbation and is computationally inexpensive.  Definition of diffusion maps  Following 5 and, 6 diffusion maps can be defined in four steps.  Connectivity  Diffusion maps exploit the relationship between heat diffusion and random walk Markov chain . The basic observation is that if we take a random walk on the data, walking to a nearby data-point is more likely than walking to another that is far away. Let    (  X  ,  ùíú  ,  Œº  )     X  ùíú  Œº    (X,\mathcal{A},\mu)   be a measure space, where   X   X   X   is the data set and   Œº   Œº   \mu   represents the distribution on the points on   X   X   X   .  Based on this, the connectivity   k   k   k   between two data points,   x   x   x   and   y   y   y   , can be defined as the probability of walking from   x   x   x   to   y   y   y   in one step of the random walk. Usually, this probability is specified in terms of kernel function on the two points    k  :    X  √ó  X   ‚Üí  ‚Ñù      normal-:  k   normal-‚Üí    X  X   ‚Ñù     k:X\times X\rightarrow\mathbb{R}   . For example, the popular Gaussian kernel:       k   (  x  ,  y  )    =   e   -     ||   x  -  y   ||   2   œµ           k   x  y     superscript  e       superscript   norm    x  y    2   œµ       k(x,y)=e^{-\frac{||x-y||^{2}}{\epsilon}}     More generally, the kernel function has the following properties       k   (  x  ,  y  )    =   k   (  y  ,  x  )          k   x  y      k   y  x      k(x,y)=k(y,x)   (   k   k   k   is symmetric)       k   (  x  ,  y  )    ‚â•     0    ‚àÄ  x    ,  y         k   x  y       0   for-all  x    y     k(x,y)\geq 0\,\,\forall x,y   (   k   k   k   is positivity preserving).  The kernel constitutes the prior definition of the local geometry of data-set. Since a given kernel will capture a specific feature of the data set, its choice should be guided by the application that one has in mind. This is a major difference with methods such as principal component analysis , where correlations between all data points are taken into account at once.  Given    (  X  ,  k  )     X  k    (X,k)   , we can then construct a reversible Markov chain on   X   X   X   (a process known as the normalized graph Laplacian construction):       d   (  x  )    =    ‚à´  X    k   (  x  ,  y  )   d  Œº   (  y  )           d  x     subscript   X     k   x  y   d  Œº  y      d(x)=\int_{X}k(x,y)d\mu(y)     and define:       p   (  x  ,  y  )    =    k   (  x  ,  y  )     d   (  x  )           p   x  y        k   x  y      d  x      p(x,y)=\frac{k(x,y)}{d(x)}     Although the new normalized kernel does not inherit the symmetric property, it does inherit the positivity-preserving property and gains a conservation property:        ‚à´  X    p   (  x  ,  y  )   d  Œº   (  y  )     =  1        subscript   X     p   x  y   d  Œº  y    1    \int_{X}p(x,y)d\mu(y)=1     Diffusion process  From    p   (  x  ,  y  )       p   x  y     p(x,y)   we can construct a transition matrix of a Markov chain (   M   M   M   ) on   X   X   X   . In other words,    p   (  x  ,  y  )       p   x  y     p(x,y)   represents the one-step transition probability from   x   x   x   to   y   y   y   , and    M  t     superscript  M  t    M^{t}   gives the t-step transition matrix.  We define the diffusion matrix   L   L   L   (it is also a version of graph Laplacian matrix)       L   i  ,  j    =   k   (   x  i   ,   x  j   )         subscript  L   i  j      k    subscript  x  i    subscript  x  j       L_{i,j}=k(x_{i},x_{j})\,     We then define the new kernel       L   i  ,  j    (  Œ±  )    =    k   (  Œ±  )     (   x  i   ,   x  j   )    =     L   i  ,  j      (   d   (   x  i   )   d   (   x  j   )    )   Œ±            subscript   superscript  L  Œ±    i  j       superscript  k  Œ±     subscript  x  i    subscript  x  j             subscript  L   i  j     superscript    d   subscript  x  i   d   subscript  x  j    Œ±       L^{(\alpha)}_{i,j}=k^{(\alpha)}(x_{i},x_{j})=\frac{L_{i,j}}{(d(x_{i})d(x_{j}))%
 ^{\alpha}}\,   or equivalently,       L   (  Œ±  )    =    D   -  Œ±    L    D   -  Œ±           superscript  L  Œ±      superscript  D    Œ±    L   superscript  D    Œ±       L^{(\alpha)}=D^{-\alpha}LD^{-\alpha}\,   where D is a diagonal matrix and      D   i  ,  i    =    ‚àë  j    L   i  ,  j      .       subscript  D   i  i      subscript   j    subscript  L   i  j       D_{i,i}=\sum_{j}L_{i,j}.     We apply the graph Laplacian normalization to this new kernel:       M  =     (   D   (  Œ±  )    )    -  1     L   (  Œ±  )      ,      M     superscript   superscript  D  Œ±     1     superscript  L  Œ±      M=({D}^{(\alpha)})^{-1}L^{(\alpha)},\,   where    D   (  Œ±  )      superscript  D  Œ±    D^{(\alpha)}   is a diagonal matrix and      D   i  ,  i    (  Œ±  )    =    ‚àë  j    L   i  ,  j    (  Œ±  )      .       subscript   superscript  D  Œ±    i  i      subscript   j    subscript   superscript  L  Œ±    i  j       {D}^{(\alpha)}_{i,i}=\sum_{j}L^{(\alpha)}_{i,j}.         p   (   x  j   ,  t  |   x  i   )   =    M   i  ,  j   t       fragments  p   fragments  normal-(   subscript  x  j   normal-,  t  normal-|   subscript  x  i   normal-)     subscript   superscript  M  t    i  j      p(x_{j},t|x_{i})=M^{t}_{i,j}\,     One of the main ideas of diffusion framework is that running the chain forward in time (taking larger and larger powers of   M   M   M   ) reveals the geometric structure of   X   X   X   at larger and larger scales (the diffusion process). Specifically, the notion of a cluster in the data set is quantified as a region in which the probability of escaping this region is low (within a certain time t). Therefore, t not only serves as a time parameter, but also has the dual role of scale parameter.  The eigendecomposition of the matrix    M  t     superscript  M  t    M^{t}   yields       M   i  ,  j   t   =    ‚àë  l     Œª  l  t    œà  l    (   x  i   )    œï  l    (   x  j   )          subscript   superscript  M  t    i  j      subscript   l      superscript   subscript  Œª  l   t    subscript  œà  l    subscript  x  i    subscript  œï  l    subscript  x  j       M^{t}_{i,j}=\sum_{l}\lambda_{l}^{t}\psi_{l}(x_{i})\phi_{l}(x_{j})\,     where    {   Œª  l   }      subscript  Œª  l     \{\lambda_{l}\}   is the sequence of eigenvalues of    M  t     superscript  M  t    M^{t}   and    {   œà  l   }      subscript  œà  l     \{\psi_{l}\}   and    {   œï  l   }      subscript  œï  l     \{\phi_{l}\}   are the biorthogonal right and left eigenvectors respectively. Due to the spectrum decay of the eigenvalues, only a few terms are necessary to achieve a given relative accuracy in this sum.  Parameter   Œ±   Œ±   \alpha   and the Diffusion Operator  The reason to introduce the normalization step involving   Œ±   Œ±   \alpha   is to tune the influence of the data point density on the infinitesimal transition of the diffusion. In some applications, the sampling of the data is generally not related to the geometry of the manifold we are interested in describing. In this case, we can set    Œ±  =  1      Œ±  1    \alpha=1   and the diffusion operator approximates the Laplace‚ÄìBeltrami operator. We then recover the Riemannian geometry of the data set regardless of the distribution of the points. To describe the long-term behavior of the point distribution of a system of stochastic differential equations, we can use    Œ±  =  0.5      Œ±  0.5    \alpha=0.5   and the resulting Markov chain approximates the Fokker-Planck diffusion. With    Œ±  =  0      Œ±  0    \alpha=0   , it reduces to the classical graph Laplacian normalization.  Diffusion distance  The diffusion distance at time   t   t   t   between two points can be measured as the similarity of two points in the observation space with the connectivity between them. It is given by        D  t     (   x  i   ,   x  j   )   2    =    ‚àë  y      (  p   (  y  ,  t  |   x  i   )   -  p   (  y  ,  t  |   x  j   )   )   2     œï  0    (  y  )             subscript  D  t    superscript    subscript  x  i    subscript  x  j    2      subscript   y      superscript   fragments  normal-(  p   fragments  normal-(  y  normal-,  t  normal-|   subscript  x  i   normal-)    p   fragments  normal-(  y  normal-,  t  normal-|   subscript  x  j   normal-)   normal-)   2      subscript  œï  0   y       D_{t}(x_{i},x_{j})^{2}=\sum_{y}\frac{(p(y,t|x_{i})-p(y,t|x_{j}))^{2}}{\phi_{0}%
 (y)}     where     œï  0    (  y  )        subscript  œï  0   y    \phi_{0}(y)   is the stationary distribution of the Markov chain, given by the first left eigenvector of   M   M   M   . Explicitly:        œï  0    (  y  )    =    d   (  y  )      ‚àë   z  ‚àà  X     d   (  z  )             subscript  œï  0   y       d  y     subscript     z  X      d  z       \phi_{0}(y)=\frac{d(y)}{\sum_{z\in X}d(z)}   Intuitively,     D  t    (   x  i   ,   x  j   )        subscript  D  t     subscript  x  i    subscript  x  j      D_{t}(x_{i},x_{j})   is small if there is a large number of short paths connecting    x  i     subscript  x  i    x_{i}   and    x  j     subscript  x  j    x_{j}   . There are several interesting features associated with the diffusion distance, based on our previous discussion that   t   t   t   also serves as a scale parameter:   Points are closer at a given scale (as specified by     D  t    (   x  i   ,   x  j   )        subscript  D  t     subscript  x  i    subscript  x  j      D_{t}(x_{i},x_{j})   ) if they are highly connected in the graph, therefore emphasizing the concept of a cluster.  This distance is robust to noise, since the distance between two points depends on all possible paths of length   t   t   t   between the points.  From a machine learning point of view, the distance takes into account all evidences linking    x  i     subscript  x  i    x_{i}   to    x  j     subscript  x  j    x_{j}   , allowing us to conclude that this distance is appropriate for the design of inference algorithms based on the majority of preponderance. 7   Diffusion process and low-dimensional embedding  The diffusion distance can be calculated using the eigenvectors by        D  t     (   x  i   ,   x  j   )   2    =    ‚àë  l     Œª  l   2  t       (     œà  l    (   x  i   )    -    œà  l    (   x  j   )     )   2             subscript  D  t    superscript    subscript  x  i    subscript  x  j    2      subscript   l      superscript   subscript  Œª  l     2  t     superscript       subscript  œà  l    subscript  x  i       subscript  œà  l    subscript  x  j     2       D_{t}(x_{i},x_{j})^{2}=\sum_{l}\lambda_{l}^{2t}(\psi_{l}(x_{i})-\psi_{l}(x_{j}%
 ))^{2}\,     So the eigenvectors can be used as a new set of coordinates for the data. The diffusion map is defined as:        Œ®  t    (  x  )    =   (    Œª  1  t    œà  1    (  x  )    ,    Œª  2  t    œà  2    (  x  )    ,  ‚Ä¶  ,    Œª  k  t    œà  k    (  x  )    )          subscript  normal-Œ®  t   x       superscript   subscript  Œª  1   t    subscript  œà  1   x      superscript   subscript  Œª  2   t    subscript  œà  2   x   normal-‚Ä¶     superscript   subscript  Œª  k   t    subscript  œà  k   x      \Psi_{t}(x)=(\lambda_{1}^{t}\psi_{1}(x),\lambda_{2}^{t}\psi_{2}(x),\ldots,%
 \lambda_{k}^{t}\psi_{k}(x))     Because of the spectrum decay, it is sufficient to use only the first k eigenvectors and eigenvalues. Thus we get the diffusion map from the original data to a k -dimensional space which is embedded in the original space.  In, 8 it is proved that        D  t     (   x  i   ,   x  j   )   2    =     ||     Œ®  t    (   x  i   )    -    Œ®  t    (   x  j   )     ||   2           subscript  D  t    superscript    subscript  x  i    subscript  x  j    2     superscript   norm       subscript  normal-Œ®  t    subscript  x  i       subscript  normal-Œ®  t    subscript  x  j      2     D_{t}(x_{i},x_{j})^{2}=||\Psi_{t}(x_{i})-\Psi_{t}(x_{j})||^{2}\,   so the Euclidean distance in the diffusion coordinates approximates the diffusion distance.  Algorithm  The basic algorithm framework of diffusion map is as:  Step 1. Given the similarity matrix L  Step 2. Normalize the matrix according to parameter   Œ±   Œ±   \alpha        L   (  Œ±  )    =    D   -  Œ±    L   D   -  Œ±          superscript  L  Œ±      superscript  D    Œ±    L   superscript  D    Œ±       L^{(\alpha)}=D^{-\alpha}LD^{-\alpha}     Step 3. Form the normalized matrix    M  =     (   D   (  Œ±  )    )    -  1     L   (  Œ±  )         M     superscript   superscript  D  Œ±     1     superscript  L  Œ±      M=({D}^{(\alpha)})^{-1}L^{(\alpha)}     Step 4. Compute the k largest eigenvalues of    M  t     superscript  M  t    M^{t}   and the corresponding eigenvectors  Step 5. Use diffusion map to get the embedding    Œ®  t     subscript  normal-Œ®  t    \Psi_{t}     Application  In the paper, 9 they showed how to design a kernel that reproduces the diffusion induced by a Fokker-Planck equation . Also, they explained that when the data approximate a manifold, then one can recover the geometry of this manifold by computing an approximation of the Laplace-Beltrami operator . This computation is completely insensitive to the distribution of the points and therefore provides a separation of the statistics and the geometry of the data. Since Diffusion map gives a global description of the data-set, it can measure the distances between pair of sample points in the manifold the data is embedded. Based on diffusion map, there are many applications, such as spectral clustering, low dimensional representation of images, image segmentation, 10 3D model segmentation, 11 speaker identification, 12 sampling on manifolds,anomaly detection, 13 image inpainting, 14 and so on.  See also   Nonlinear dimensionality reduction   References  "  Category:Machine learning algorithms                      