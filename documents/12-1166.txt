   Khmaladze transformation      Khmaladze transformation   In statistics , the Khmaladze transformation is a mathematical tool used in constructing convenient goodness of fit tests for hypothetical distribution functions . More precisely, suppose     X  1   ,  …  ,   X  n       subscript  X  1   normal-…   subscript  X  n     X_{1},\ldots,X_{n}   are i.i.d. , possibly multi-dimensional, random observations generated from an unknown probability distribution . A classical problem in statistics is to decide how well a given hypothetical distribution function   F   F   F   , or a given hypothetical parametric family of distribution functions    {   F  θ   :   θ  ∈  Θ   }     conditional-set   subscript  F  θ     θ  normal-Θ     \{F_{\theta}:\theta\in\Theta\}   , fits the set of observations. The Khmaladze transformation allows us to construct goodness of fit tests with desirable properties. It is named after Estate V. Khmaladze .  Consider the sequence of empirical distribution functions     F  n     subscript  F  n    F_{n}   based on a sequence of i.i.d random variables,     X  1   ,  …  ,   X  n       subscript  X  1   normal-…   subscript  X  n     X_{1},\ldots,X_{n}   , as n increases. Suppose   F   F   F   is the hypothetical distribution function of each    X  i     subscript  X  i    X_{i}   . To test whether the choice of   F   F   F   is correct or not, statisticians use the normalized difference,         v  n    (  x  )    =    n    [     F  n    (  x  )    -   F   (  x  )     ]     .         subscript  v  n   x       n    delimited-[]       subscript  F  n   x     F  x        v_{n}(x)=\sqrt{n}[F_{n}(x)-F(x)].     This    v  n     subscript  v  n    v_{n}   , as a random process in   x   x   x   , is called the empirical process . Various functionals of    v  n     subscript  v  n    v_{n}   are used as test statistics. The change of the variable      v  n    (  x  )    =    u  n    (  t  )           subscript  v  n   x      subscript  u  n   t     v_{n}(x)=u_{n}(t)   ,    t  =   F   (  x  )        t    F  x     t=F(x)   transforms to the so-called uniform empirical process    u  n     subscript  u  n    u_{n}   . The latter is an empirical processes based on independent random variables     U  i   =   F   (   X  i   )         subscript  U  i     F   subscript  X  i      U_{i}=F(X_{i})   , which are uniformly distributed on    [  0  ,  1  ]     0  1    [0,1]   if the    X  i     subscript  X  i    X_{i}   s do indeed have distribution function   F   F   F   .  This fact was discovered and first utilized by Kolmogorov (1933), Wald and Wolfowitz (1936) and Smirnov (1937) and, especially after Doob (1949) and Anderson and Darling (1952), 1 it led to the standard rule to choose test statistics based on    v  n     subscript  v  n    v_{n}   . That is, test statistics    ψ   (   v  n   ,  F  )       ψ    subscript  v  n   F     \psi(v_{n},F)   are defined (which possibly depend on the   F   F   F   being tested) in such a way that there exists another statistic    φ   (   u  n   )       φ   subscript  u  n     \varphi(u_{n})   derived from the uniform empirical process, such that     ψ   (   v  n   ,  F  )    =   φ   (   u  n   )          ψ    subscript  v  n   F      φ   subscript  u  n      \psi(v_{n},F)=\varphi(u_{n})   . Examples are         sup  x    |    v  n    (  x  )    |    =    sup  t    |    u  n    (  t  )    |     ,     sup  x     |    v  n    (  x  )    |    a   (   F   (  x  )    )      =    sup  t     |    u  n    (  t  )    |    a   (  t  )          formulae-sequence      subscript  supremum  x        subscript  v  n   x       subscript  supremum  t        subscript  u  n   t          subscript  supremum  x          subscript  v  n   x      a    F  x        subscript  supremum  t          subscript  u  n   t      a  t        \sup_{x}|v_{n}(x)|=\sup_{t}|u_{n}(t)|,\quad\sup_{x}\frac{|v_{n}(x)|}{a(F(x))}=%
 \sup_{t}\frac{|u_{n}(t)|}{a(t)}     and         ∫   -  ∞   ∞     v  n  2    (  x  )   d  F   (  x  )     =    ∫  0  1     u  n  2    (  t  )   d  t     .        superscript   subscript             superscript   subscript  v  n   2   x  d  F  x      superscript   subscript   0   1      superscript   subscript  u  n   2   t  d  t      \int_{-\infty}^{\infty}v_{n}^{2}(x)\,dF(x)=\int_{0}^{1}u_{n}^{2}(t)\,dt.     For all such functionals, their null distribution (under the hypothetical   F   F   F   ) does not depend on   F   F   F   , and can be calculated once and then used to test any   F   F   F   .  However, it is only rarely that one needs to test a simple hypothesis, when a fixed   F   F   F   as a hypothesis is given. Much more often, one needs to verify parametric hypotheses where the hypothetical    F  =   F   θ  n        F   subscript  F   subscript  θ  n      F=F_{\theta_{n}}   , depends on some parameters    θ  n     subscript  θ  n    \theta_{n}   , which the hypothesis does not specify and which have to be estimated from the sample     X  1   ,  …  ,   X  n       subscript  X  1   normal-…   subscript  X  n     X_{1},\ldots,X_{n}   itself.  Although the estimators     θ  ^   n     subscript   normal-^  θ   n    \hat{\theta}_{n}   , most commonly converge to true value of   θ   θ   \theta   , it was discovered that the parametric, 2 3 or estimated, empirical process         v  ^   n    (  x  )    =    n    [     F  n    (  x  )    -    F    θ  ^   n     (  x  )     ]           subscript   normal-^  v   n   x       n    delimited-[]       subscript  F  n   x      subscript  F   subscript   normal-^  θ   n    x        \hat{v}_{n}(x)=\sqrt{n}[F_{n}(x)-F_{\hat{\theta}_{n}}(x)]     differs significantly from    v  n     subscript  v  n    v_{n}   and that the transformed process       u  ^   n    (  t  )    =     v  ^   n    (  x  )           subscript   normal-^  u   n   t      subscript   normal-^  v   n   x     \hat{u}_{n}(t)=\hat{v}_{n}(x)   ,    t  =    F    θ  ^   n     (  x  )        t     subscript  F   subscript   normal-^  θ   n    x     t=F_{\hat{\theta}_{n}}(x)   has a distribution for which the limit distribution, as    n  →  ∞     normal-→  n     n\to\infty   , is dependent on the parametric form of    F  θ     subscript  F  θ    F_{\theta}   and on the particular estimator     θ  ^   n     subscript   normal-^  θ   n    \hat{\theta}_{n}   and, in general, within one parametric family , on the value of   θ   θ   \theta   .  From mid-1950s to the late-1980s, much work was done to clarify the situation and understand the nature of the process     v  ^   n     subscript   normal-^  v   n    \hat{v}_{n}   .  In 1981, 4 and then 1987 and 1993, 5 Khmaladze suggested to replace the parametric empirical process     v  ^   n     subscript   normal-^  v   n    \hat{v}_{n}   by its martingale part    w  n     subscript  w  n    w_{n}   only.          v  ^   n    (  x  )    -    K  n    (  x  )     =    w  n    (  x  )             subscript   normal-^  v   n   x      subscript  K  n   x       subscript  w  n   x     \hat{v}_{n}(x)-K_{n}(x)=w_{n}(x)     where     K  n    (  x  )        subscript  K  n   x    K_{n}(x)   is the compensator of      v  ^   n    (  x  )        subscript   normal-^  v   n   x    \hat{v}_{n}(x)   . Then the following properties of    w  n     subscript  w  n    w_{n}   were established:   Although the form of    K  n     subscript  K  n    K_{n}   , and therefore, of    w  n     subscript  w  n    w_{n}   , depends on     F    θ  ^   n     (  x  )        subscript  F   subscript   normal-^  θ   n    x    F_{\hat{\theta}_{n}}(x)   , as a function of both   x   x   x   and    θ  n     subscript  θ  n    \theta_{n}   , the limit distribution of the time transformed process            ω  n    (  t  )    =    w  n    (  x  )     ,   t  =    F    θ  ^   n     (  x  )        formulae-sequence       subscript  ω  n   t      subscript  w  n   x      t     subscript  F   subscript   normal-^  θ   n    x      \omega_{n}(t)=w_{n}(x),t=F_{\hat{\theta}_{n}}(x)         is that of standard Brownian motion on    [  0  ,  1  ]     0  1    [0,1]   , i.e., is again standard and independent of the choice of    F    θ  ^   n      subscript  F   subscript   normal-^  θ   n     F_{\hat{\theta}_{n}}   .    The relationship between     v  ^   n     subscript   normal-^  v   n    \hat{v}_{n}   and    w  n     subscript  w  n    w_{n}   and between their limits, is one to one, so that the statistical inference based on     v  ^   n     subscript   normal-^  v   n    \hat{v}_{n}   or on    w  n     subscript  w  n    w_{n}   are equivalent, and in    w  n     subscript  w  n    w_{n}   , nothing is lost compared to     v  ^   n     subscript   normal-^  v   n    \hat{v}_{n}   .    The construction of innovation martingale    w  n     subscript  w  n    w_{n}   could be carried over to the case of vector-valued     X  1   ,  …  ,   X  n       subscript  X  1   normal-…   subscript  X  n     X_{1},\ldots,X_{n}   , giving rise to the definition of the so-called scanning martingales in    ℝ  d     superscript  ℝ  d    \mathbb{R}^{d}   .   For a long time the transformation was, although known, still not used. Later, the work of researchers like Koenker , Stute , Bai , Koul , Koening, and others made it popular in econometrics and other fields of statistics.  See also   Empirical process   References  Further reading     "  Category:Probability theory  Category:Empirical process  Category:Transforms     ↩  ↩  Gikhman (1954) ↩  ↩  ↩     