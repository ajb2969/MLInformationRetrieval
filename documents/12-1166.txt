


Khmaladze transformation




Khmaladze transformation

In statistics, the Khmaladze transformation is a mathematical tool used in constructing convenient goodness of fit tests for hypothetical distribution functions. More precisely, suppose 
 
 
 
  are i.i.d., possibly multi-dimensional, random observations generated from an unknown probability distribution. A classical problem in statistics is to decide how well a given hypothetical distribution function 
 
 
 
 , or a given hypothetical parametric family of distribution functions 
 
 
 
 , fits the set of observations. The Khmaladze transformation allows us to construct goodness of fit tests with desirable properties. It is named after Estate V. Khmaladze.
Consider the sequence of empirical distribution functions

 
  based on a sequence of i.i.d random variables, 
 
 
 
 , as n increases. Suppose 
 
 
 
  is the hypothetical distribution function of each 
 
 
 
 . To test whether the choice of 
 
 
 
  is correct or not, statisticians use the normalized difference,



This 
 
 
 
 , as a random process in 
 
 
 
 , is called the empirical process. Various functionals of 
 
 
 
  are used as test statistics. The change of the variable 
 
 
 
 , 
 
 
 
  transforms to the so-called uniform empirical process 
 
 
 
 . The latter is an empirical processes based on independent random variables 
 
 
 
 , which are uniformly distributed on 
 
 
 
  if the 
 
 
 
 s do indeed have distribution function 
 
 
 
 .
This fact was discovered and first utilized by Kolmogorov (1933), Wald and Wolfowitz (1936) and Smirnov (1937) and, especially after Doob (1949) and Anderson and Darling (1952),1 it led to the standard rule to choose test statistics based on 
 
 
 
 . That is, test statistics 
 
 
 
  are defined (which possibly depend on the 
 
 
 
  being tested) in such a way that there exists another statistic 
 
 
 
  derived from the uniform empirical process, such that 
 
 
 
 . Examples are



and



For all such functionals, their null distribution (under the hypothetical 
 
 
 
 ) does not depend on 
 
 
 
 , and can be calculated once and then used to test any 
 
 
 
 .
However, it is only rarely that one needs to test a simple hypothesis, when a fixed 
 
 
 
  as a hypothesis is given. Much more often, one needs to verify parametric hypotheses where the hypothetical 
 
 
 
 , depends on some parameters 
 
 
 
 , which the hypothesis does not specify and which have to be estimated from the sample 
 
 
 
  itself.
Although the estimators 
 
 
 
 , most commonly converge to true value of 
 
 
 
 , it was discovered that the parametric,23 or estimated, empirical process



differs significantly from 
 
 
 
  and that the transformed process 
 
 
 
 , 
 
 
 
  has a distribution for which the limit distribution, as 
 
 
 
 , is dependent on the parametric form of 
 
 
 
  and on the particular estimator 
 
 
 
  and, in general, within one parametric family, on the value of 
 
 
 
 .
From mid-1950s to the late-1980s, much work was done to clarify the situation and understand the nature of the process 
 
 
 
 .
In 1981,4 and then 1987 and 1993,5 Khmaladze suggested to replace the parametric empirical process 
 
 
 
  by its martingale part 
 
 
 
  only.



where 
 
 
 
  is the compensator of 
 
 
 
 . Then the following properties of 
 
 
 
  were established:

Although the form of 
 
 
 
 , and therefore, of 
 
 
 
 , depends on 
 
 
 
 , as a function of both 
 
 
 
  and 
 
 
 
 , the limit distribution of the time transformed process










is that of standard Brownian motion on 
 
 
 
 , i.e., is again standard and independent of the choice of 
 
 
 
 .
 


The relationship between 
 
 
 
  and 
 
 
 
  and between their limits, is one to one, so that the statistical inference based on 
 
 
 
  or on 
 
 
 
  are equivalent, and in 
 
 
 
 , nothing is lost compared to 
 
 
 
 .


The construction of innovation martingale 
 
 
 
  could be carried over to the case of vector-valued 
 
 
 
 , giving rise to the definition of the so-called scanning martingales in 
 
 
 
 .

For a long time the transformation was, although known, still not used. Later, the work of researchers like Koenker, Stute, Bai, Koul, Koening, and others made it popular in econometrics and other fields of statistics.
See also

Empirical process

References
Further reading



"
Category:Probability theory Category:Empirical process Category:Transforms





Gikhman (1954)






