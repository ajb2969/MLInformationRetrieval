   Free energy principle      Free energy principle  The '''free energy principle''' tries to explain how [[Biological_systems|(biological) systems]] maintain their order ([[Non-equilibrium thermodynamics |non-equilibrium steady-state]] ) by restricting themselves to a limited number of states. 1 It says that biological systems minimise a free energy functional of their internal states, which entail beliefs about hidden states in their environment. The implicit minimisation of variational free energy is formally related to variational Bayesian methods and was originally introduced by Karl Friston as an explanation for embodied perception in neuroscience , 2 where it is also known as active inference .  Background  The notion that self-organising biological systems ‚Äì like a cell or brain ‚Äì can be understood as minimising variational free energy is based upon Helmholtz ‚Äôs observations on unconscious inference 3 and subsequent treatments in psychology 4 and machine learning. 5 Variational free energy is a functional of some outcomes and a probability density over their (hidden) causes. This variational density is defined in relation to a probabilistic model that generates outcomes from causes. In this setting, free energy provides an (upper bound) approximation to Bayesian model evidence . 6 Its minimisation can therefore be used to explain Bayesian inference and learning. When a system actively samples outcomes to minimise free energy, it implicitly performs active inference and maximises the evidence for its (generative) model.  However, free energy is also an upper bound on the self-information (or surprise) of outcomes, where the long-term average of surprise is entropy. This means that if a system acts to minimise free energy, it will implicitly place an upper bound on the entropy of the outcomes ‚Äì or sensory states ‚Äì it samples. 7  Relationship to other theories  Active inference is closely related to the good regulator theorem  8 and related accounts of self-organisation , 9 10 such as self-assembly , pattern formation and autopoiesis . 11 It addresses the themes considered in cybernetics , synergetics  12 and embodied cognition . Because free energy can be expressed as the expected energy (of outcomes) under the variational density minus its entropy, it is also related to the maximum entropy principle . 13 Finally, because the time average of energy is action, the principle of minimum variational free energy is a principle of least action .  Definition  (Figure)  These schematics illustrate the partition of states into internal and hidden or external states that are separated by a Markov blanket ‚Äì comprising sensory and active states. The upper panel shows this partition as it would be applied to action and perception in the brain; where active and internal states minimise a free energy functional of sensory states. The ensuing self-organisation of internal states then correspond perception, while action couples brain states back to external states. The lower panel shows exactly the same dependencies but rearranged so that the internal states are associated with the intracellular states of a cell, while the sensory states become the surface states of the cell membrane overlying active states (e.g., the actin filaments of the cytoskeleton).   Definition (continuous formulation): Active inference rests on the tuple    (  Œ©  ,  Œ®  ,  S  ,  A  ,  R  ,  q  ,  p  )     normal-Œ©  normal-Œ®  S  A  R  q  p    (\Omega,\Psi,S,A,R,q,p)   ,   A sample space    Œ©   normal-Œ©   \Omega   ‚Äì from which random fluctuations    œâ  ‚àà  Œ©      œâ  normal-Œ©    \omega\in\Omega   are drawn  Hidden or external states     Œ®  :    Œ®  √ó  A  √ó  Œ©   ‚Üí  ‚Ñù      normal-:  normal-Œ®   normal-‚Üí    normal-Œ®  A  normal-Œ©   ‚Ñù     \Psi:\Psi\times A\times\Omega\to\mathbb{R}   ‚Äì that cause sensory states and depend on action  Sensory states     S  :    Œ®  √ó  A  √ó  Œ©   ‚Üí  ‚Ñù      normal-:  S   normal-‚Üí    normal-Œ®  A  normal-Œ©   ‚Ñù     S:\Psi\times A\times\Omega\to\mathbb{R}   ‚Äì a probabilistic mapping from action and hidden states  Action     A  :    S  √ó  R   ‚Üí  ‚Ñù      normal-:  A   normal-‚Üí    S  R   ‚Ñù     A:S\times R\to\mathbb{R}   ‚Äì that depends on sensory and internal states  Internal states     R  :    R  √ó  S   ‚Üí  ‚Ñù      normal-:  R   normal-‚Üí    R  S   ‚Ñù     R:R\times S\to\mathbb{R}   ‚Äì that cause action and depend on sensory states  Generative density     p   (  s  ,  œà  |  m  )      fragments  p   fragments  normal-(  s  normal-,  œà  normal-|  m  normal-)     p(s,\psi|m)   ‚Äì over sensory and hidden states under a generative model   m   m   m     Variational density     q   (  œà  |  Œº  )      fragments  q   fragments  normal-(  œà  normal-|  Œº  normal-)     q(\psi|\mu)   ‚Äì over hidden states    œà  ‚àà  Œ®      œà  normal-Œ®    \psi\in\Psi   that is parameterised by internal states    Œº  ‚àà  R      Œº  R    \mu\in R      Action and perception  The objective is to maximise model evidence    p   (  s  |  m  )      fragments  p   fragments  normal-(  s  normal-|  m  normal-)     p(s|m)   or minimise surprise    -  log  p   (  s  |  m  )      fragments    p   fragments  normal-(  s  normal-|  m  normal-)     -\log p(s|m)   . This generally involves an intractable marginalisation over hidden states, so surprise is replaced with an upper variational free energy bound. 14 However, this means that internal states must also minimise free energy, because free energy is a functional of sensory and internal states:       a   (  t  )    =      arg   min   ùëé    {   F   (   s   (  t  )    ,   Œº   (  t  )    )    }          a  t      a    arg  min       F     s  t     Œº  t         a(t)=\underset{a}{\operatorname{arg\,min}}\{F(s(t),\mu(t))\}         Œº   (  t  )   =     arg   min   ùúá    {  F   (  s   (  t  )   ,  Œº  )   )   }     fragments  Œº   fragments  normal-(  t  normal-)     Œº    arg  min     fragments  normal-{  F   fragments  normal-(  s   fragments  normal-(  t  normal-)   normal-,  Œº  normal-)   normal-)   normal-}    \mu(t)=\underset{\mu}{\operatorname{arg\,min}}\{F(s(t),\mu))\}            F   (  s  ,  Œº  )    ‚èü    free  -  energy    =       E  q    [  -  log  p   (  s  ,  œà  ‚à£  m  )   ]    ‚èü    e  n  e  r  g  y    -     H   [  q   (  œà  |  Œº  )   ]    ‚èü   entropy    =      -  log  p   (  s  |  m  )    ‚èü    s  u  r  p  r  i  s  e    +      D  KL    [  q   (  œà  |  Œº  )   ‚à•  p   (  œà  ‚à£  s  ,  m  )   ]    ‚èü    d  i  v  e  r  g  e  n  c  e     ‚â•     -  log  p   (  s  |  m  )    ‚èü    s  u  r  p  r  i  s  e             free  energy    normal-‚èü    F   s  Œº           e  n  e  r  g  y    normal-‚èü   fragments   subscript  E  q    fragments  normal-[    p   fragments  normal-(  s  normal-,  œà  normal-‚à£  m  normal-)   normal-]       entropy   normal-‚èü   fragments  H   fragments  normal-[  q   fragments  normal-(  œà  normal-|  Œº  normal-)   normal-]                 s  u  r  p  r  i  s  e    normal-‚èü   fragments    p   fragments  normal-(  s  normal-|  m  normal-)         d  i  v  e  r  g  e  n  c  e    normal-‚èü   fragments   subscript  D  KL    fragments  normal-[  q   fragments  normal-(  œà  normal-|  Œº  normal-)   parallel-to  p   fragments  normal-(  œà  normal-‚à£  s  normal-,  m  normal-)   normal-]               s  u  r  p  r  i  s  e    normal-‚èü   fragments    p   fragments  normal-(  s  normal-|  m  normal-)         \underset{\mathrm{free-energy}}{\underbrace{F(s,\mu)}}=\underset{energy}{%
 \underbrace{E_{q}[-\log p(s,\psi\mid m)]}}-\underset{\mathrm{entropy}}{%
 \underbrace{H[q(\psi|\mu)]}}=\underset{surprise}{\underbrace{-\log p(s|m)}}+%
 \underset{divergence}{\underbrace{D_{\mathrm{KL}}[q(\psi|\mu)\|p(\psi\mid s,m)%
 ]}}\geq\underset{surprise}{\underbrace{-\log p(s|m)}}     This induces a dual minimisation with respect to action and internal states that correspond to action and perception respectively.  Free energy minimisation  Free energy minimisation and self-organisation  Free energy minimisation has been proposed as a hallmark of self-organising systems, when cast as random dynamical systems . 15 This formulation rests on a Markov blanket (comprising action and sensory states) that separates internal and external states. If internal states and action minimise free energy, then they place an upper bound on the entropy of sensory states       lim   T  ‚Üí  ‚àû     1  T       ‚à´  0  T    F   (   s   (  t  )    ,   Œº   (  t  )    )   d  t    ‚èü     f  r  e  e   -   a  c  t  i  o  n     ‚â•   lim   T  ‚Üí  ‚àû     1  T    ‚à´  0  T      -  log  p   (  s   (  t  )   |  m  )    ‚èü    s  u  r  p  r  i  s  e    d  t  =  H   [  p   (  s  |  m  )   ]      fragments   subscript    normal-‚Üí  T       1  T        f  r  e  e     a  c  t  i  o  n     normal-‚èü    superscript   subscript   0   T     F     s  t     Œº  t    d  t        subscript    normal-‚Üí  T       1  T    superscript   subscript   0   T      s  u  r  p  r  i  s  e    normal-‚èü   fragments    p   fragments  normal-(  s   fragments  normal-(  t  normal-)   normal-|  m  normal-)      d  t   H   fragments  normal-[  p   fragments  normal-(  s  normal-|  m  normal-)   normal-]     \lim_{T\to\infty}\frac{1}{T}\underset{free-action}{\underbrace{\int_{0}^{T}F(s%
 (t),\mu(t))dt}}\geq\lim_{T\to\infty}\frac{1}{T}\int_{0}^{T}\underset{surprise}%
 {\underbrace{-\log p(s(t)|m)}}dt=H[p(s|m)]     This is because ‚Äì under ergodic assumptions ‚Äì the long-term average of surprise is entropy. This bound resists a natural tendency to disorder ‚Äì of the sort associated with the second law of thermodynamics and the fluctuation theorem .  Free energy minimisation and Bayesian inference  All Bayesian inference can be cast in terms of free energy minimisation; e.g.,. 16 When free energy is minimised with respect to internal states, the Kullback‚ÄìLeibler divergence between the variational and posterior density over hidden states is minimised. This corresponds to approximate Bayesian inference ‚Äì when the form of the variational density is fixed ‚Äì and exact Bayesian inference otherwise. Free energy minimisation therefore provides a generic description of Bayesian inference and filtering (e.g., Kalman filtering ). It is also used in Bayesian model selection , where free energy can be usefully decomposed into complexity and accuracy:         F   (  s  ,  Œº  )    ‚èü     f  r  e  e   -   e  n  e  r  g  y     =       D  KL    [  q   (  œà  |  Œº  )   ‚à•  p   (  œà  |  m  )   ]    ‚èü    c  o  m  p  l  e  x  i  t  y    -      E  q    [  log  p   (  s  |  œà  ,  m  )   ]    ‚èü    a  c  c  u  r  a  c  y              f  r  e  e     e  n  e  r  g  y     normal-‚èü    F   s  Œº           c  o  m  p  l  e  x  i  t  y    normal-‚èü   fragments   subscript  D  KL    fragments  normal-[  q   fragments  normal-(  œà  normal-|  Œº  normal-)   parallel-to  p   fragments  normal-(  œà  normal-|  m  normal-)   normal-]         a  c  c  u  r  a  c  y    normal-‚èü   fragments   subscript  E  q    fragments  normal-[   p   fragments  normal-(  s  normal-|  œà  normal-,  m  normal-)   normal-]         \underset{free-energy}{\underbrace{F(s,\mu)}}=\underset{complexity}{%
 \underbrace{D_{\mathrm{KL}}[q(\psi|\mu)\|p(\psi|m)]}}-\underset{accuracy}{%
 \underbrace{E_{q}[\log p(s|\psi,m)]}}     Models with minimum free energy provide an accurate explanation of data, under complexity costs (c.f., Occam's razor and more formal treatments of computational costs 17 ). Here, complexity is the divergence between the variational density and prior beliefs about hidden states (i.e., the effective degrees of freedom used to explain the data).  Free energy minimisation and thermodynamics  Variational free energy is an information theoretic functional and is distinct from thermodynamic (Helmholtz) free energy . 18 However, the complexity term of variational free energy shares the same fixed point as Helmholtz free energy (under the assumption the system is thermodynamically closed but not isolated). This is because if sensory perturbations are suspended (for a suitably long period of time), complexity is minimised (because accuracy can be neglected). At this point, the system is at equilibrium and internal states minimise Helmholtz free energy, by the principle of minimum energy . 19  Free energy minimisation and information theory  Free energy minimisation is equivalent to maximising the mutual information between sensory states and internal states that parameterise the variational density (for a fixed entropy variational density). 20 This relates free energy minimization to the principle of minimum redundancy 21 and related treatments using information theory to describe optimal behaviour. 22 23  Free energy minimisation in neuroscience  Free energy minimisation provides a useful way to formulate normative (Bayes optimal) models of neuronal inference and learning under uncertainty 24 and therefore subscribes to the Bayesian brain hypothesis. 25 The neuronal processes described by free energy minimisation depend on the nature of hidden states    Œ®  =   X  √ó  Œò  √ó  Œ†       normal-Œ®    X  normal-Œò  normal-Œ†     \Psi=X\times\Theta\times\Pi   that can comprise time dependent variables, time invariant parameters and the precision (inverse variance or temperature) of random fluctuations. Minimising variables, parameters and precision corresponds to inference, learning and the encoding of uncertainty, respectively:  Perceptual inference and categorisation  Free energy minimisation formalises the notion of unconscious inference in perception 26 27 and provides a normative (Bayesian) theory of neuronal processing. The associated process theory of neuronal dynamics is based on minimising free energy through gradient descent. This corresponds to generalised Bayesian filtering (where ~ denotes a variable in generalised coordinates of motion and   D   D   D   is a derivative matrix operator): 28        Œº  ~   Àô   =    D   Œº  ~    -     ‚àÇ   Œº  ~    F    (  s  ,   Œº  ~   )          normal-Àô   normal-~  Œº        D   normal-~  Œº        subscript    normal-~  Œº    F    s   normal-~  Œº        \dot{\tilde{\mu}}=D\tilde{\mu}-\partial_{\tilde{\mu}}F(s,\tilde{\mu})     Usually, the generative models that define free energy are non-linear and hierarchical (like cortical hierarchies in the brain). Special cases of generalised filtering include Kalman filtering , which is formally equivalent to predictive coding 29 ‚Äì a popular metaphor for message passing in the brain. Under hierarchical models, predictive coding involves the recurrent exchange of ascending (bottom-up) prediction errors and descending (top-down) predictions 30 that is consistent with the anatomy and physiology of sensory 31 and motor systems. 32  Perceptual learning and memory  In predictive coding, optimising model parameters through a gradient ascent on the time integral of free energy (free action) reduces to associative or Hebbian plasticity and is associated with synaptic plasticity in the brain.  Perceptual precision, attention and salience  Optimising the precision parameters corresponds to optimising the gain of prediction errors (c.f., Kalman gain). In neuronally plausible implementations of predictive coding, 33 this corresponds to optimising the excitability superficial pyramidal cells and has been interpreted in terms of attentional gain. 34  Active inference  When gradient descent is applied to action     a  Àô   =   -     ‚àÇ  a   F    (  s  ,   Œº  ~   )          normal-Àô  a         subscript   a   F    s   normal-~  Œº        \dot{a}=-\partial_{a}F(s,\tilde{\mu})   , motor control can be understood in terms of classical reflex arcs that are engaged by descending (corticospinal) predictions. This provides a formalism that generalizes the equilibrium point solution ‚Äì to the degrees of freedom problem  35 ‚Äì to movement trajectories.  Active inference and optimal control  Active inference is related to optimal control by replacing value or cost-to-go functions with prior beliefs about state transitions or flow. 36 This exploits the close connection between Bayesian filtering and the solution to the Bellman equation . However, active inference starts with (priors over) flow    f  =    Œì  ‚ãÖ   ‚àá  V    +   ‚àá  √ó  W        f     normal-‚ãÖ  normal-Œì   normal-‚àá  V      normal-‚àá  W      f=\Gamma\cdot\nabla V+\nabla\times W   that are specified with scalar    V   (  x  )       V  x    V(x)   and vector    W   (  x  )       W  x    W(x)   value functions of state space (c.f., the Helmholtz decomposition ). Here,   Œì   normal-Œì   \Gamma   is the amplitude of random fluctuations and cost is     c   (  x  )    =    f  ‚ãÖ   ‚àá  V    +   ‚àá  ‚ãÖ  Œì  ‚ãÖ  V          c  x      normal-‚ãÖ  f   normal-‚àá  V     normal-‚ãÖ  normal-‚àá  normal-Œì  V      c(x)=f\cdot\nabla V+\nabla\cdot\Gamma\cdot V   . The priors over flow    p   (   x  ~   |  m  )      fragments  p   fragments  normal-(   normal-~  x   normal-|  m  normal-)     p(\tilde{x}|m)   induce a prior over states    p   (  x  |  m  )   =  exp   (  V   (  x  )   )      fragments  p   fragments  normal-(  x  normal-|  m  normal-)      fragments  normal-(  V   fragments  normal-(  x  normal-)   normal-)     p(x|m)=\exp(V(x))   that is the solution to the appropriate forward Kolmogorov equations . 37 In contrast, optimal control optimises the flow, given a cost function, under the assumption that    W  =  0      W  0    W=0   (i.e., the flow is curl free or has detailed balance). Usually, this entails solving backward Kolmogorov equations . 38  Active inference and optimal decision (game) theory  Optimal decision problems (usually formulated as partially observable Markov decision processes ) are treated within active inference by absorbing utility functions into prior beliefs. In this setting, states that have a high utility (low cost) are states an agent expects to occupy. By equipping the generative model with hidden states that model control, policies (control sequences) that minimise variational free energy lead to high utility states. 39  Neurobiologically, neuromodulators like dopamine are considered to report the precision of prediction errors by modulating the gain of principal cells encoding prediction error. 40 This is closely related to ‚Äì but formally distinct from ‚Äì the role of dopamine in reporting prediction errors per se  41 and related computational accounts. 42  Active inference and cognitive neuroscience  Active inference has been used to address a range of issues in cognitive neuroscience , brain function and neuropsychiatry, including: action observation, 43 mirror neurons, 44 saccades and visual search, 45 sleep, 46 illusions, 47 attention, 48 action section, 49 hysteria 50 and psychosis. 51  See also   Action-specific perception  Affordance  Autopoiesis  Bayesian brain  Decision theory  Embodied cognition  Free energy  Optimal control  Self-organization  Synergetics (Haken)  Variational Bayesian methods   References  External links   Lectures  Publications  [ http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid; ;=8918803 Behavioral and Brain Sciences (by Andy Clark)]   "  Category:Systems  Category:Systems theory     Ashby, W. R. (1962). Principles of the self-organizing system .in Principles of Self-Organization: Transactions of the University of Illinois Symposium, H. Von Foerster and G. W. Zopf, Jr. (eds.), Pergamon Press: London, UK, pp. 255‚Äì278. ‚Ü©  Friston, K., Kilner, J., & Harrison, L. (2006). A free energy principle for the brain . J Physiol Paris. , 100 (1‚Äì3), 70‚Äì87. ‚Ü©  Helmholtz, H. (1866/1962). Concerning the perceptions in general. In Treatise on physiological optics (J. Southall, Trans., 3rd ed., Vol. III). New York: Dover. ‚Ü©  Gregory, R. L. (1980). Perceptions as hypotheses . Phil Trans R Soc Lond B. , 290, 181‚Äì197. ‚Ü©  Dayan, P., Hinton, G. E., & Neal, R. (1995). The Helmholtz machine . Neural Computation , 7, 889‚Äì904. ‚Ü©  Beal, M. J. (2003). Variational Algorithms for Approximate Bayesian Inference . PhD. Thesis, University College London. ‚Ü©  Friston, K. (2012). A Free Energy Principle for Biological Systems . Entropy , 14, 2100‚Äì2121. ‚Ü©  Conant, R. C., & Ashby, R. W. (1970). Every Good Regulator of a system must be a model of that system. Int. J. Systems Sci. , 1 (2), 89‚Äì97. ‚Ü©  Kauffman, S. (1993). The Origins of Order: Self-Organization and Selection in Evolution. Oxford: Oxford University Press. ‚Ü©  Nicolis, G., & Prigogine, I. (1977). Self-organization in non-equilibrium systems. New York: John Wiley. ‚Ü©  Maturana, H. R., & Varela, F. (1980). Autopoiesis: the organization of the living . In V. F. Maturana HR (Ed.), Autopoiesis and Cognition. Dordrecht, Netherlands: Reidel. ‚Ü©  Haken, H. (1983). Synergetics: An introduction. Non-equilibrium phase transition and self-organisation in physics, chemistry and biology (3rd ed.). Berlin: Springer Verlag. ‚Ü©  Jaynes, E. T. (1957). Information Theory and Statistical Mechanics . Physical Review Series II , 106 (4), 620‚Äì30. ‚Ü©  Dayan, P., Hinton, G. E., & Neal, R. (1995). The Helmholtz machine . Neural Computation , 7, 889‚Äì904. ‚Ü©  Crauel, H., & Flandoli, F. (1994). Attractors for random dynamical systems. Probab Theory Relat Fields , 100, 365‚Äì393. ‚Ü©  Roweis, S., & Ghahramani, Z. (1999). A unifying review of linear Gaussian models . Neural Computat. , 11 (2), 305‚Äì45. ‚Ü©  Ortega, P. A., & Braun, D. A. (2012). Thermodynamics as a theory of decision-making with information processing costs . Proceedings of the Royal Society A, vol. 469, no. 2153 (20120683) . ‚Ü©  Evans, D. J. (2003). A non-equilibrium free energy theorem for deterministic systems . Molecular Physics , 101, 15551‚Äì4. ‚Ü©  Jarzynski, C. (1997). Nonequilibrium equality for free energy differences . Phys. Rev. Lett., 78, 2690. ‚Ü©   Barlow, H. (1961). Possible principles underlying the transformations of sensory messages . In W. Rosenblith (Ed.), Sensory Communication (pp. 217-34). Cambridge, MA: MIT Press. ‚Ü©  Linsker, R. (1990). Perceptual neural organization: some approaches based on network models and information theory . Annu Rev Neurosci. , 13, 257‚Äì81. ‚Ü©  Bialek, W., Nemenman, I., & Tishby, N. (2001). Predictability, complexity, and learning . Neural Computat., 13 (11), 2409‚Äì63. ‚Ü©  Friston, K. (2010). The free-energy principle: a unified brain theory? Nat Rev Neurosci. , 11 (2), 127‚Äì38. ‚Ü©  Knill, D. C., & Pouget, A. (2004). The Bayesian brain: the role of uncertainty in neural coding and computation . Trends Neurosci. , 27 (12), 712‚Äì9. ‚Ü©    Friston, K., Stephan, K., Li, B., & Daunizeau, J. (2010). Generalised Filtering . Mathematical Problems in Engineering , vol., 2010, 621670 ‚Ü©  Rao, R. P., & Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat Neurosci. , 2 (1), 79‚Äì87. ‚Ü©  Mumford, D. (1992). On the computational architecture of the neocortex . II. Biol. Cybern. , 66, 241‚Äì51. ‚Ü©  Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries, P., & Friston, K. J. (2012). Canonical microcircuits for predictive coding . Neuron , 76 (4), 695‚Äì711. ‚Ü©  Adams, R. A., Shipp, S., & Friston, K. J. (2013). Predictions not commands: active inference in the motor system . Brain Struct Funct. , 218 (3), 611‚Äì43 ‚Ü©   Feldman, H., & Friston, K. J. (2010). Attention, uncertainty, and free-energy . Frontiers in Human Neuroscience , 4, 215. ‚Ü©  Feldman, A. G., & Levin, M. F. (1995). The origin and use of positional frames of reference in motor control . Behav Brain Sci. , 18, 723‚Äì806. ‚Ü©  Friston, K., (2011). What is optimal about motor control? . Neuron, 72(3), 488‚Äì98. ‚Ü©  Friston, K., & Ao, P. (2012). Free-energy, value and attractors . Computational and mathematical methods in medicine, 2012, 937860. ‚Ü©  Kappen, H., (2005). Path integrals and symmetry breaking for optimal control theory . Journal of Statistical Mechanics: Theory and Experiment, 11, p. P11011. ‚Ü©  Friston, K., Samothrakis, S. & Montague, R., (2012). Active inference and agency: optimal control without cost functions . Biol. Cybernetics, 106(8‚Äì9), 523‚Äì41. ‚Ü©  Friston, K. J. Shiner T, FitzGerald T, Galea JM, Adams R, Brown H, Dolan RJ, Moran R, Stephan KE, Bestmann S. (2012). Dopamine, affordance and active inference . PLoS Comput Biol., 8(1), p. e1002327. ‚Ü©  Fiorillo, C. D., Tobler, P. N. & Schultz, W., (2003). Discrete coding of reward probability and uncertainty by dopamine neurons . Science, 299(5614), 1898‚Äì902. ‚Ü©  Frank, M. J., (2005). Dynamic dopamine modulation in the basal ganglia: a neurocomputational account of cognitive deficits in medicated and nonmedicated Parkinsonism . J Cogn Neurosci., Jan, 1, 51‚Äì72. ‚Ü©  Friston, K., Mattout, J. & Kilner, J., (2011). Action understanding and active inference . Biol Cybern., 104, 137‚Äì160. ‚Ü©  Kilner, J. M., Friston, K. J. & Frith, C. D., (2007). Predictive coding: an account of the mirror neuron system . Cogn Process., 8(3), pp. 159‚Äì66. ‚Ü©  Friston, K., Adams, R. A., Perrinet, L. & Breakspear, M., (2012). Perceptions as hypotheses: saccades as experiments . Front Psychol., 3, 151. ‚Ü©  Hobson, J. A. & Friston, K. J., (2012). Waking and dreaming consciousness: Neurobiological and functional considerations. Prog Neurobiol, 98(1), pp. 82‚Äì98. ‚Ü©  Brown, H., & Friston, K. J. (2012). Free-energy and illusions: the cornsweet effect . Front Psychol , 3, 43. ‚Ü©    Edwards, M. J., Adams, R. A., Brown, H., Pare√©s, I., & Friston, K. J. (2012). A Bayesian account of 'hysteria' . Brain , 135(Pt 11):3495‚Äì512. ‚Ü©  Adams RA, Perrinet LU, Friston K. (2012). Smooth pursuit and visual occlusion: active inference and oculomotor control in schizophrenia. PLoS One. , 12;7(10):e47502 ‚Ü©     