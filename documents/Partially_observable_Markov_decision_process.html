<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1754">Partially observable Markov decision process</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Partially observable Markov decision process</h1>
<hr/>

<p>A <strong>partially observable Markov decision process</strong> (POMDP) is a generalization of a <a href="Markov_decision_process" title="wikilink">Markov decision process</a> (MDP). A POMDP models an agent decision process in which it is assumed that the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. Instead, it must maintain a probability distribution over the set of possible states, based on a set of observations and observation probabilities, and the underlying MDP.</p>

<p>The POMDP framework is general enough to model a variety of real-world sequential decision processes. Applications include robot navigation problems, machine maintenance, and planning under uncertainty in general. The framework originated in the <a href="operations_research" title="wikilink">operations research</a> community, and was later taken over by the <a href="artificial_intelligence" title="wikilink">artificial intelligence</a> and <a href="automated_planning" title="wikilink">automated planning</a> communities.</p>

<p>An exact solution to a POMDP yields the optimal action for each possible belief over the world states. The optimal action maximizes (or minimizes) the expected reward (or cost) of the agent over a possibly infinite horizon. The sequence of optimal actions is known as the optimal policy of the agent for interacting with its environment.</p>
<h2 id="definition">Definition</h2>
<h3 id="formal-definition">Formal definition</h3>

<p>A discrete-time POMDP models the relationship between an agent and its environment. Formally, a POMDP is a 7-tuple 

<math display="inline" id="Partially_observable_Markov_decision_process:0">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mi>S</mi>
   <mo>,</mo>
   <mi>A</mi>
   <mo>,</mo>
   <mi>T</mi>
   <mo>,</mo>
   <mi>R</mi>
   <mo>,</mo>
   <mi mathvariant="normal">Œ©</mi>
   <mo>,</mo>
   <mi>O</mi>
   <mo>,</mo>
   <mi>Œ≥</mi>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <vector>
    <ci>S</ci>
    <ci>A</ci>
    <ci>T</ci>
    <ci>R</ci>
    <ci>normal-Œ©</ci>
    <ci>O</ci>
    <ci>Œ≥</ci>
   </vector>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (S,A,T,R,\Omega,O,\gamma)
  </annotation>
 </semantics>
</math>


, where</p>
<ul>
<li>

<math display="inline" id="Partially_observable_Markov_decision_process:1">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

 is a set of states,</li>
<li>

<math display="inline" id="Partially_observable_Markov_decision_process:2">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A
  </annotation>
 </semantics>
</math>

 is a set of actions,</li>
<li>

<math display="inline" id="Partially_observable_Markov_decision_process:3">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>

 is a set of conditional transition probabilities between states,</li>
<li>

<math display="inline" id="Partially_observable_Markov_decision_process:4">
 <semantics>
  <mrow>
   <mi>R</mi>
   <mo>:</mo>
   <mrow>
    <mrow>
     <mi>S</mi>
     <mo>√ó</mo>
     <mi>A</mi>
    </mrow>
    <mo>‚Üí</mo>
    <mi>‚Ñù</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>R</ci>
    <apply>
     <ci>normal-‚Üí</ci>
     <apply>
      <times></times>
      <ci>S</ci>
      <ci>A</ci>
     </apply>
     <ci>‚Ñù</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R:S\times A\to\mathbb{R}
  </annotation>
 </semantics>
</math>

 is the reward function.</li>
<li>

<math display="inline" id="Partially_observable_Markov_decision_process:5">
 <semantics>
  <mi mathvariant="normal">Œ©</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-Œ©</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Omega
  </annotation>
 </semantics>
</math>


 is a set of observations,</li>
<li>

<math display="inline" id="Partially_observable_Markov_decision_process:6">
 <semantics>
  <mi>O</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>O</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O
  </annotation>
 </semantics>
</math>

 is a set of conditional observation probabilities, and</li>
<li>

<math display="inline" id="Partially_observable_Markov_decision_process:7">
 <semantics>
  <mrow>
   <mi>Œ≥</mi>
   <mo>‚àà</mo>
   <mrow>
    <mo stretchy="false">[</mo>
    <mn>0</mn>
    <mo>,</mo>
    <mn>1</mn>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>Œ≥</ci>
    <interval closure="closed">
     <cn type="integer">0</cn>
     <cn type="integer">1</cn>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma\in[0,1]
  </annotation>
 </semantics>
</math>

 is the discount factor.</li>
</ul>

<p>At each time period, the environment is in some state 

<math display="inline" id="Partially_observable_Markov_decision_process:8">
 <semantics>
  <mrow>
   <mi>s</mi>
   <mo>‚àà</mo>
   <mi>S</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>s</ci>
    <ci>S</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s\in S
  </annotation>
 </semantics>
</math>

. The agent takes an action 

<math display="inline" id="Partially_observable_Markov_decision_process:9">
 <semantics>
  <mrow>
   <mi>a</mi>
   <mo>‚àà</mo>
   <mi>A</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>a</ci>
    <ci>A</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a\in A
  </annotation>
 </semantics>
</math>

, which causes the environment to transition to state 

<math display="inline" id="Partially_observable_Markov_decision_process:10">
 <semantics>
  <msup>
   <mi>s</mi>
   <mo>‚Ä≤</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>s</ci>
    <ci>normal-‚Ä≤</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s^{\prime}
  </annotation>
 </semantics>
</math>


 with probability 

<math display="inline" id="Partially_observable_Markov_decision_process:11">
 <semantics>
  <mrow>
   <mi>T</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msup>
     <mi>s</mi>
     <mo>‚Ä≤</mo>
    </msup>
    <mo>‚à£</mo>
    <mi>s</mi>
    <mo>,</mo>
    <mi>a</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">T</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-‚Ä≤</ci>
     </apply>
     <ci>normal-‚à£</ci>
     <csymbol cd="unknown">s</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">a</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T(s^{\prime}\mid s,a)
  </annotation>
 </semantics>
</math>

. At the same time, the agent receives an observation 

<math display="inline" id="Partially_observable_Markov_decision_process:12">
 <semantics>
  <mrow>
   <mi>o</mi>
   <mo>‚àà</mo>
   <mi mathvariant="normal">Œ©</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>o</ci>
    <ci>normal-Œ©</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   o\in\Omega
  </annotation>
 </semantics>
</math>

 which depends on the new state of the environment with probability 

<math display="inline" id="Partially_observable_Markov_decision_process:13">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>o</mi>
    <mo>‚à£</mo>
    <msup>
     <mi>s</mi>
     <mo>‚Ä≤</mo>
    </msup>
    <mo>,</mo>
    <mi>a</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">O</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">o</csymbol>
     <ci>normal-‚à£</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-‚Ä≤</ci>
     </apply>
     <ci>normal-,</ci>
     <csymbol cd="unknown">a</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(o\mid s^{\prime},a)
  </annotation>
 </semantics>
</math>

. Finally, the agent receives a reward equal to 

<math display="inline" id="Partially_observable_Markov_decision_process:14">
 <semantics>
  <mrow>
   <mi>R</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>s</mi>
    <mo>,</mo>
    <mi>a</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>R</ci>
    <interval closure="open">
     <ci>s</ci>
     <ci>a</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R(s,a)
  </annotation>
 </semantics>
</math>

. Then the process repeats. The goal is for the agent to choose actions at each time step that maximize its expected future discounted reward

<math display="block" id="Partially_observable_Markov_decision_process:15">
 <semantics>
  <mrow>
   <mi>E</mi>
   <mrow>
    <mo>[</mo>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
      <mrow>
       <mi>t</mi>
       <mo>=</mo>
       <mn>0</mn>
      </mrow>
      <mi mathvariant="normal">‚àû</mi>
     </munderover>
     <mrow>
      <msup>
       <mi>Œ≥</mi>
       <mi>t</mi>
      </msup>
      <msub>
       <mi>r</mi>
       <mi>t</mi>
      </msub>
     </mrow>
    </mrow>
    <mo>]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>E</ci>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>t</ci>
         <cn type="integer">0</cn>
        </apply>
       </apply>
       <infinity></infinity>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>Œ≥</ci>
        <ci>t</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>r</ci>
        <ci>t</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   E\left[\sum_{t=0}^{\infty}\gamma^{t}r_{t}\right]
  </annotation>
 </semantics>
</math>

. The discount factor 

<math display="inline" id="Partially_observable_Markov_decision_process:16">
 <semantics>
  <mi>Œ≥</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Œ≥</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma
  </annotation>
 </semantics>
</math>

 determines how much immediate rewards are favored over more distant rewards. When 

<math display="inline" id="Partially_observable_Markov_decision_process:17">
 <semantics>
  <mrow>
   <mi>Œ≥</mi>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>Œ≥</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma=0
  </annotation>
 </semantics>
</math>

 the agent only cares about which action will yield the largest expected immediate reward; when 

<math display="inline" id="Partially_observable_Markov_decision_process:18">
 <semantics>
  <mrow>
   <mi>Œ≥</mi>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>Œ≥</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma=1
  </annotation>
 </semantics>
</math>

 the agent cares about maximizing the expected sum of future rewards.</p>
<h3 id="discussion">Discussion</h3>

<p>Because the agent does not directly observe the environment's state, the agent must make decisions under uncertainty of the true environment state. However, by interacting with the environment and receiving observations, the agent may update its belief in the true state by updating the probability distribution of the current state. A consequence of this property is that the optimal behavior may often include information gathering actions that are taken purely because they improve the agent's estimate of the current state, thereby allowing it to make better decisions in the future.</p>

<p>It is instructive to compare the above definition with the definition of a <a href="Markov_decision_process#Definition" title="wikilink">Markov decision process</a>. An MDP does not include the observation set, because the agent always knows with certainty the environment's current state. Alternatively, an MDP can be reformulated as a POMDP by setting the observation set to be equal to the set of states and defining the observation conditional probabilities to deterministically select the observation that corresponds to the true state.</p>
<h2 id="belief-update">Belief update</h2>

<p>An agent needs to update its belief upon taking the action 

<math display="inline" id="Partially_observable_Markov_decision_process:19">
 <semantics>
  <mi>a</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>a</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a
  </annotation>
 </semantics>
</math>

 and observing 

<math display="inline" id="Partially_observable_Markov_decision_process:20">
 <semantics>
  <mi>o</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>o</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   o
  </annotation>
 </semantics>
</math>


. Since the state is Markovian, maintaining a belief over the states solely requires knowledge of the previous belief state, the action taken, and the current observation. The operation is denoted 

<math display="inline" id="Partially_observable_Markov_decision_process:21">
 <semantics>
  <mrow>
   <msup>
    <mi>b</mi>
    <mo>‚Ä≤</mo>
   </msup>
   <mo>=</mo>
   <mrow>
    <mi>œÑ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>b</mi>
     <mo>,</mo>
     <mi>a</mi>
     <mo>,</mo>
     <mi>o</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>b</ci>
     <ci>normal-‚Ä≤</ci>
    </apply>
    <apply>
     <times></times>
     <ci>œÑ</ci>
     <vector>
      <ci>b</ci>
      <ci>a</ci>
      <ci>o</ci>
     </vector>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b^{\prime}=\tau(b,a,o)
  </annotation>
 </semantics>
</math>

. Below we describe how this belief update is computed.</p>

<p>After reaching 

<math display="inline" id="Partially_observable_Markov_decision_process:22">
 <semantics>
  <msup>
   <mi>s</mi>
   <mo>‚Ä≤</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>s</ci>
    <ci>normal-‚Ä≤</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s^{\prime}
  </annotation>
 </semantics>
</math>

, the agent observes 

<math display="inline" id="Partially_observable_Markov_decision_process:23">
 <semantics>
  <mrow>
   <mi>o</mi>
   <mo>‚àà</mo>
   <mi mathvariant="normal">Œ©</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>o</ci>
    <ci>normal-Œ©</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   o\in\Omega
  </annotation>
 </semantics>
</math>

 with probability 

<math display="inline" id="Partially_observable_Markov_decision_process:24">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>o</mi>
    <mo>‚à£</mo>
    <msup>
     <mi>s</mi>
     <mo>‚Ä≤</mo>
    </msup>
    <mo>,</mo>
    <mi>a</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">O</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">o</csymbol>
     <ci>normal-‚à£</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-‚Ä≤</ci>
     </apply>
     <ci>normal-,</ci>
     <csymbol cd="unknown">a</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(o\mid s^{\prime},a)
  </annotation>
 </semantics>
</math>

. Let 

<math display="inline" id="Partially_observable_Markov_decision_process:25">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>


 be a probability distribution over the state space 

<math display="inline" id="Partially_observable_Markov_decision_process:26">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

. 

<math display="inline" id="Partially_observable_Markov_decision_process:27">
 <semantics>
  <mrow>
   <mi>b</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>s</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>b</ci>
    <ci>s</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b(s)
  </annotation>
 </semantics>
</math>

 denotes the probability that the environment is in state 

<math display="inline" id="Partially_observable_Markov_decision_process:28">
 <semantics>
  <mi>s</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>s</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s
  </annotation>
 </semantics>
</math>

. Given 

<math display="inline" id="Partially_observable_Markov_decision_process:29">
 <semantics>
  <mrow>
   <mi>b</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>s</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>b</ci>
    <ci>s</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b(s)
  </annotation>
 </semantics>
</math>

, then after taking action 

<math display="inline" id="Partially_observable_Markov_decision_process:30">
 <semantics>
  <mi>a</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>a</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a
  </annotation>
 </semantics>
</math>


 and observing 

<math display="inline" id="Partially_observable_Markov_decision_process:31">
 <semantics>
  <mi>o</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>o</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   o
  </annotation>
 </semantics>
</math>

,</p>

<p>

<math display="block" id="Partially_observable_Markov_decision_process:32">
 <semantics>
  <mrow>
   <msup>
    <mi>b</mi>
    <mo>‚Ä≤</mo>
   </msup>
   <mrow>
    <mo stretchy="false">(</mo>
    <msup>
     <mi>s</mi>
     <mo>‚Ä≤</mo>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>Œ∑</mi>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>o</mi>
    <mo>‚à£</mo>
    <msup>
     <mi>s</mi>
     <mo>‚Ä≤</mo>
    </msup>
    <mo>,</mo>
    <mi>a</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <munder>
    <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
    <mrow>
     <mi>s</mi>
     <mo>‚àà</mo>
     <mi>S</mi>
    </mrow>
   </munder>
   <mi>T</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msup>
     <mi>s</mi>
     <mo>‚Ä≤</mo>
    </msup>
    <mo>‚à£</mo>
    <mi>s</mi>
    <mo>,</mo>
    <mi>a</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>b</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>s</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>b</ci>
     <ci>normal-‚Ä≤</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-‚Ä≤</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">Œ∑</csymbol>
    <csymbol cd="unknown">O</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">o</csymbol>
     <ci>normal-‚à£</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-‚Ä≤</ci>
     </apply>
     <ci>normal-,</ci>
     <csymbol cd="unknown">a</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <sum></sum>
     <apply>
      <in></in>
      <ci>s</ci>
      <ci>S</ci>
     </apply>
    </apply>
    <csymbol cd="unknown">T</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-‚Ä≤</ci>
     </apply>
     <ci>normal-‚à£</ci>
     <csymbol cd="unknown">s</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">a</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">b</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">s</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b^{\prime}(s^{\prime})=\eta O(o\mid s^{\prime},a)\sum_{s\in S}T(s^{\prime}\mid
s%
,a)b(s)
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Partially_observable_Markov_decision_process:33">
 <semantics>
  <mrow>
   <mi>Œ∑</mi>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>/</mo>
    <mrow>
     <mi>Pr</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>o</mi>
      <mo>‚à£</mo>
      <mi>b</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>Œ∑</ci>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <apply>
      <ci>Pr</ci>
      <ci>o</ci>
      <ci>b</ci>
      <ci>a</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \eta=1/\Pr(o\mid b,a)
  </annotation>
 </semantics>
</math>

 is a normalizing constant with 

<math display="inline" id="Partially_observable_Markov_decision_process:34">
 <semantics>
  <mrow>
   <mi>Pr</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>o</mi>
    <mo>‚à£</mo>
    <mi>b</mi>
    <mo>,</mo>
    <mi>a</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <msub>
    <mo largeop="true" symmetric="true">‚àë</mo>
    <mrow>
     <msup>
      <mi>s</mi>
      <mo>‚Ä≤</mo>
     </msup>
     <mo>‚àà</mo>
     <mi>S</mi>
    </mrow>
   </msub>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>o</mi>
    <mo>‚à£</mo>
    <msup>
     <mi>s</mi>
     <mo>‚Ä≤</mo>
    </msup>
    <mo>,</mo>
    <mi>a</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <msub>
    <mo largeop="true" symmetric="true">‚àë</mo>
    <mrow>
     <mi>s</mi>
     <mo>‚àà</mo>
     <mi>S</mi>
    </mrow>
   </msub>
   <mi>T</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msup>
     <mi>s</mi>
     <mo>‚Ä≤</mo>
    </msup>
    <mo>‚à£</mo>
    <mi>s</mi>
    <mo>,</mo>
    <mi>a</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>b</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>s</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <ci>Pr</ci>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">o</csymbol>
     <ci>normal-‚à£</ci>
     <csymbol cd="unknown">b</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">a</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <sum></sum>
     <apply>
      <in></in>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>s</ci>
       <ci>normal-‚Ä≤</ci>
      </apply>
      <ci>S</ci>
     </apply>
    </apply>
    <csymbol cd="unknown">O</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">o</csymbol>
     <ci>normal-‚à£</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-‚Ä≤</ci>
     </apply>
     <ci>normal-,</ci>
     <csymbol cd="unknown">a</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <sum></sum>
     <apply>
      <in></in>
      <ci>s</ci>
      <ci>S</ci>
     </apply>
    </apply>
    <csymbol cd="unknown">T</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-‚Ä≤</ci>
     </apply>
     <ci>normal-‚à£</ci>
     <csymbol cd="unknown">s</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">a</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">b</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">s</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Pr(o\mid b,a)=\sum_{s^{\prime}\in S}O(o\mid s^{\prime},a)\sum_{s\in S}T(s^{%
\prime}\mid s,a)b(s)
  </annotation>
 </semantics>
</math>

.</p>
<h2 id="belief-mdp">Belief MDP</h2>

<p>A Markovian belief state allows a POMDP to be formulated as a <a href="Markov_decision_process" title="wikilink">Markov decision process</a> where every belief is a state. The resulting <em>belief MDP</em> will thus be defined on a continuous state space, since there are infinite beliefs for any given POMDP.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> In other words, there are technically infinite belief states (in 

<math display="inline" id="Partially_observable_Markov_decision_process:35">
 <semantics>
  <mi>B</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>B</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B
  </annotation>
 </semantics>
</math>


) because there are an infinite number of mixtures of (

<math display="inline" id="Partially_observable_Markov_decision_process:36">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

) the originating states.</p>

<p>Formally, the belief MDP is defined as a tuple 

<math display="inline" id="Partially_observable_Markov_decision_process:37">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mi>B</mi>
   <mo>,</mo>
   <mi>A</mi>
   <mo>,</mo>
   <mi>œÑ</mi>
   <mo>,</mo>
   <mi>r</mi>
   <mo>,</mo>
   <mi>Œ≥</mi>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <vector>
    <ci>B</ci>
    <ci>A</ci>
    <ci>œÑ</ci>
    <ci>r</ci>
    <ci>Œ≥</ci>
   </vector>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (B,A,\tau,r,\gamma)
  </annotation>
 </semantics>
</math>

 where</p>
<ul>
<li>

<math display="inline" id="Partially_observable_Markov_decision_process:38">
 <semantics>
  <mi>B</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>B</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B
  </annotation>
 </semantics>
</math>

 is the set of belief states over the POMDP states,</li>
<li>

<math display="inline" id="Partially_observable_Markov_decision_process:39">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A
  </annotation>
 </semantics>
</math>

 is the same finite set of action as for the original POMDP,</li>
<li>

<math display="inline" id="Partially_observable_Markov_decision_process:40">
 <semantics>
  <mi>œÑ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>œÑ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tau
  </annotation>
 </semantics>
</math>


 is the belief state transition function,</li>
<li>

<math display="inline" id="Partially_observable_Markov_decision_process:41">
 <semantics>
  <mrow>
   <mi>r</mi>
   <mo>:</mo>
   <mrow>
    <mrow>
     <mi>B</mi>
     <mo>√ó</mo>
     <mi>A</mi>
    </mrow>
    <mo>‚Üí</mo>
    <mi>‚Ñù</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>r</ci>
    <apply>
     <ci>normal-‚Üí</ci>
     <apply>
      <times></times>
      <ci>B</ci>
      <ci>A</ci>
     </apply>
     <ci>‚Ñù</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r:B\times A\to\mathbb{R}
  </annotation>
 </semantics>
</math>

 is the reward function on belief states,</li>
<li>

<math display="inline" id="Partially_observable_Markov_decision_process:42">
 <semantics>
  <mi>Œ≥</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Œ≥</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma
  </annotation>
 </semantics>
</math>

 is the discount factor equal to the 

<math display="inline" id="Partially_observable_Markov_decision_process:43">
 <semantics>
  <mi>Œ≥</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Œ≥</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma
  </annotation>
 </semantics>
</math>

 in the original POMDP.</li>
</ul>

<p>Of these, 

<math display="inline" id="Partially_observable_Markov_decision_process:44">
 <semantics>
  <mi>œÑ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>œÑ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tau
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Partially_observable_Markov_decision_process:45">
 <semantics>
  <mi>r</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>r</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r
  </annotation>
 </semantics>
</math>


 need to be derived from the original POMDP. 

<math display="inline" id="Partially_observable_Markov_decision_process:46">
 <semantics>
  <mi>œÑ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>œÑ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tau
  </annotation>
 </semantics>
</math>

 is</p>

<p>

<math display="inline" id="Partially_observable_Markov_decision_process:47">
 <semantics>
  <mrow>
   <mi>œÑ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>b</mi>
    <mo>,</mo>
    <mi>a</mi>
    <mo>,</mo>
    <msup>
     <mi>b</mi>
     <mo>‚Ä≤</mo>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <msub>
    <mo largeop="true" symmetric="true">‚àë</mo>
    <mrow>
     <mi>o</mi>
     <mo>‚àà</mo>
     <mi mathvariant="normal">Œ©</mi>
    </mrow>
   </msub>
   <mi>P</mi>
   <mi>r</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msup>
     <mi>b</mi>
     <mo>‚Ä≤</mo>
    </msup>
    <mo stretchy="false">|</mo>
    <mi>b</mi>
    <mo>,</mo>
    <mi>a</mi>
    <mo>,</mo>
    <mi>o</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>Pr</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>o</mi>
    <mo stretchy="false">|</mo>
    <mi>a</mi>
    <mo>,</mo>
    <mi>b</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">œÑ</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">b</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">a</csymbol>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>b</ci>
      <ci>normal-‚Ä≤</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <sum></sum>
     <apply>
      <in></in>
      <ci>o</ci>
      <ci>normal-Œ©</ci>
     </apply>
    </apply>
    <csymbol cd="unknown">P</csymbol>
    <csymbol cd="unknown">r</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>b</ci>
      <ci>normal-‚Ä≤</ci>
     </apply>
     <ci>normal-|</ci>
     <csymbol cd="unknown">b</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">a</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">o</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>Pr</ci>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">o</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">a</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">b</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-,</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tau(b,a,b^{\prime})=\sum_{o\in\Omega}Pr(b^{\prime}|b,a,o)\Pr(o|a,b),
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Partially_observable_Markov_decision_process:48">
 <semantics>
  <mrow>
   <mi>Pr</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>o</mi>
    <mo stretchy="false">|</mo>
    <mi>a</mi>
    <mo>,</mo>
    <mi>b</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>Pr</ci>
    <ci>o</ci>
    <ci>a</ci>
    <ci>b</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Pr(o|a,b)
  </annotation>
 </semantics>
</math>

 is the value derived in the previous section and</p>

<p>

<math display="inline" id="Partially_observable_Markov_decision_process:49">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mi>r</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msup>
     <mi>b</mi>
     <mo>‚Ä≤</mo>
    </msup>
    <mo stretchy="false">|</mo>
    <mi>b</mi>
    <mo>,</mo>
    <mi>a</mi>
    <mo>,</mo>
    <mi>o</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>{</mo>
    <mtable>
     <mtr>
      <mtd columnalign="left">
       <mn>1</mn>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mtext>if the belief update with arguments</mtext>
         <mi>b</mi>
        </mrow>
        <mo>,</mo>
        <mi>a</mi>
        <mo>,</mo>
        <mrow>
         <mi>o</mi>
         <mtext>returns</mtext>
         <msup>
          <mi>b</mi>
          <mo>‚Ä≤</mo>
         </msup>
        </mrow>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mn>0</mn>
      </mtd>
      <mtd columnalign="left">
       <mtext>otherwise</mtext>
      </mtd>
     </mtr>
    </mtable>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <csymbol cd="unknown">r</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>b</ci>
      <ci>normal-‚Ä≤</ci>
     </apply>
     <ci>normal-|</ci>
     <csymbol cd="unknown">b</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">a</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">o</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="latexml">cases</csymbol>
     <cn type="integer">1</cn>
     <list>
      <apply>
       <times></times>
       <mtext>if the belief update with arguments</mtext>
       <ci>b</ci>
      </apply>
      <ci>a</ci>
      <apply>
       <times></times>
       <ci>o</ci>
       <mtext>returns</mtext>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>b</ci>
        <ci>normal-‚Ä≤</ci>
       </apply>
      </apply>
     </list>
     <cn type="integer">0</cn>
     <mtext>otherwise</mtext>
    </apply>
    <ci>normal-.</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Pr(b^{\prime}|b,a,o)=\begin{cases}1&\text{if the belief update with arguments %
}b,a,o\text{ returns }b^{\prime}\\
0&\text{otherwise }\end{cases}.
  </annotation>
 </semantics>
</math>

</p>

<p>The belief MDP reward function (

<math display="inline" id="Partially_observable_Markov_decision_process:50">
 <semantics>
  <mi>r</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>r</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r
  </annotation>
 </semantics>
</math>

) is the expected reward from the POMDP reward function over the belief state distribution:</p>

<p>

<math display="inline" id="Partially_observable_Markov_decision_process:51">
 <semantics>
  <mrow>
   <mrow>
    <mi>r</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>b</mi>
     <mo>,</mo>
     <mi>a</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mo largeop="true" symmetric="true">‚àë</mo>
     <mrow>
      <mi>s</mi>
      <mo>‚àà</mo>
      <mi>S</mi>
     </mrow>
    </msub>
    <mrow>
     <mi>b</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>s</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>R</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>r</ci>
     <interval closure="open">
      <ci>b</ci>
      <ci>a</ci>
     </interval>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <in></in>
       <ci>s</ci>
       <ci>S</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>b</ci>
      <ci>s</ci>
      <ci>R</ci>
      <interval closure="open">
       <ci>s</ci>
       <ci>a</ci>
      </interval>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r(b,a)=\sum_{s\in S}b(s)R(s,a)
  </annotation>
 </semantics>
</math>

.</p>

<p>The belief MDP is not partially observable anymore, since at any given time the agent knows its belief, and by extension the state of the belief MDP.</p>

<p>Also, unlike the "originating" MDP, where each action is available from only one state; in the corresponding Belief MDP, all belief states allow all actions, since you (almost) always have <em>some</em> probability of believing you are in any (originating) state.</p>
<h3 id="policy-and-value-function">Policy and Value Function</h3>

<p>

<math display="inline" id="Partially_observable_Markov_decision_process:52">
 <semantics>
  <mi>œÄ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>œÄ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi
  </annotation>
 </semantics>
</math>

 specifies an action 

<math display="inline" id="Partially_observable_Markov_decision_process:53">
 <semantics>
  <mrow>
   <mi>a</mi>
   <mo>=</mo>
   <mrow>
    <mi>œÄ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>b</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>a</ci>
    <apply>
     <times></times>
     <ci>œÄ</ci>
     <ci>b</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a=\pi(b)
  </annotation>
 </semantics>
</math>

 for any belief 

<math display="inline" id="Partially_observable_Markov_decision_process:54">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>

. Here it is assumed the objective is to maximize the expected total discounted reward over an infinite horizon. When 

<math display="inline" id="Partially_observable_Markov_decision_process:55">
 <semantics>
  <mi>R</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>R</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R
  </annotation>
 </semantics>
</math>

 defines a cost, the objective becomes the minimization of the expected cost.</p>

<p>The expected reward for policy 

<math display="inline" id="Partially_observable_Markov_decision_process:56">
 <semantics>
  <mi>œÄ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>œÄ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi
  </annotation>
 </semantics>
</math>

 starting from belief 

<math display="inline" id="Partially_observable_Markov_decision_process:57">
 <semantics>
  <msub>
   <mi>b</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>b</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b_{0}
  </annotation>
 </semantics>
</math>

 is defined as</p>

<p>

<math display="block" id="Partially_observable_Markov_decision_process:58">
 <semantics>
  <mrow>
   <msup>
    <mi>V</mi>
    <mi>œÄ</mi>
   </msup>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>b</mi>
     <mn>0</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <munderover>
    <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
    <mrow>
     <mi>t</mi>
     <mo>=</mo>
     <mn>0</mn>
    </mrow>
    <mi mathvariant="normal">‚àû</mi>
   </munderover>
   <msup>
    <mi>Œ≥</mi>
    <mi>t</mi>
   </msup>
   <mi>r</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>b</mi>
     <mi>t</mi>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>a</mi>
     <mi>t</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <munderover>
    <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
    <mrow>
     <mi>t</mi>
     <mo>=</mo>
     <mn>0</mn>
    </mrow>
    <mi mathvariant="normal">‚àû</mi>
   </munderover>
   <msup>
    <mi>Œ≥</mi>
    <mi>t</mi>
   </msup>
   <mi>E</mi>
   <mrow>
    <mo maxsize="160%" minsize="160%">[</mo>
    <mi>R</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>s</mi>
      <mi>t</mi>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>a</mi>
      <mi>t</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>‚à£</mo>
    <msub>
     <mi>b</mi>
     <mn>0</mn>
    </msub>
    <mo>,</mo>
    <mi>œÄ</mi>
    <mo maxsize="160%" minsize="160%">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>V</ci>
     <ci>œÄ</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>b</ci>
      <cn type="integer">0</cn>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <eq></eq>
       <ci>t</ci>
       <cn type="integer">0</cn>
      </apply>
     </apply>
     <infinity></infinity>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>Œ≥</ci>
     <ci>t</ci>
    </apply>
    <csymbol cd="unknown">r</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>b</ci>
      <ci>t</ci>
     </apply>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>a</ci>
      <ci>t</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <eq></eq>
       <ci>t</ci>
       <cn type="integer">0</cn>
      </apply>
     </apply>
     <infinity></infinity>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>Œ≥</ci>
     <ci>t</ci>
    </apply>
    <csymbol cd="unknown">E</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-[</ci>
     <csymbol cd="unknown">R</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>s</ci>
       <ci>t</ci>
      </apply>
      <ci>normal-,</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>a</ci>
       <ci>t</ci>
      </apply>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-‚à£</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>b</ci>
      <cn type="integer">0</cn>
     </apply>
     <ci>normal-,</ci>
     <csymbol cd="unknown">œÄ</csymbol>
     <ci>normal-]</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V^{\pi}(b_{0})=\sum_{t=0}^{\infty}\gamma^{t}r(b_{t},a_{t})=\sum_{t=0}^{\infty}%
\gamma^{t}E\Bigl[R(s_{t},a_{t})\mid b_{0},\pi\Bigr]
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Partially_observable_Markov_decision_process:59">
 <semantics>
  <mrow>
   <mi>Œ≥</mi>
   <mo><</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <lt></lt>
    <ci>Œ≥</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma<1
  </annotation>
 </semantics>
</math>

 is the discount factor. The optimal policy 

<math display="inline" id="Partially_observable_Markov_decision_process:60">
 <semantics>
  <msup>
   <mi>œÄ</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>œÄ</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi^{*}
  </annotation>
 </semantics>
</math>

 is obtained by optimizing the long-term reward.</p>

<p>

<math display="block" id="Partially_observable_Markov_decision_process:61">
 <semantics>
  <mrow>
   <msup>
    <mi>œÄ</mi>
    <mo>*</mo>
   </msup>
   <mo>=</mo>
   <mrow>
    <mpadded width="+5pt">
     <munder accentunder="true">
      <mtext>argmax</mtext>
      <mo>ùúã</mo>
     </munder>
    </mpadded>
    <msup>
     <mi>V</mi>
     <mi>œÄ</mi>
    </msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>b</mi>
      <mn>0</mn>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>œÄ</ci>
     <times></times>
    </apply>
    <apply>
     <times></times>
     <apply>
      <ci>œÄ</ci>
      <mtext>argmax</mtext>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>V</ci>
      <ci>œÄ</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>b</ci>
      <cn type="integer">0</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi^{*}=\underset{\pi}{\mbox{argmax}}\ V^{\pi}(b_{0})
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Partially_observable_Markov_decision_process:62">
 <semantics>
  <msub>
   <mi>b</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>b</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b_{0}
  </annotation>
 </semantics>
</math>

 is the initial belief.</p>

<p>The optimal policy, denoted by 

<math display="inline" id="Partially_observable_Markov_decision_process:63">
 <semantics>
  <msup>
   <mi>œÄ</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>œÄ</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi^{*}
  </annotation>
 </semantics>
</math>

, yields the highest expected reward value for each belief state, compactly represented by the optimal value function 

<math display="inline" id="Partially_observable_Markov_decision_process:64">
 <semantics>
  <msup>
   <mi>V</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>V</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V^{*}
  </annotation>
 </semantics>
</math>

. This value function is solution to the <a href="Bellman_equation" title="wikilink">Bellman optimality equation</a>:</p>

<p>

<math display="block" id="Partially_observable_Markov_decision_process:65">
 <semantics>
  <mrow>
   <msup>
    <mi>V</mi>
    <mo>*</mo>
   </msup>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>b</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <munder>
    <mi>max</mi>
    <mrow>
     <mi>a</mi>
     <mo>‚àà</mo>
     <mi>A</mi>
    </mrow>
   </munder>
   <mrow>
    <mo maxsize="160%" minsize="160%">[</mo>
    <mi>r</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>b</mi>
     <mo>,</mo>
     <mi>a</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>+</mo>
    <mi>Œ≥</mi>
    <munder>
     <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
     <mrow>
      <mi>o</mi>
      <mo>‚àà</mo>
      <mi mathvariant="normal">Œ©</mi>
     </mrow>
    </munder>
    <mi>O</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>o</mi>
     <mo>‚à£</mo>
     <mi>b</mi>
     <mo>,</mo>
     <mi>a</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <msup>
     <mi>V</mi>
     <mo>*</mo>
    </msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>œÑ</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>b</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo>,</mo>
      <mi>o</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo maxsize="160%" minsize="160%">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>V</ci>
     <times></times>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">b</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <max></max>
     <apply>
      <in></in>
      <ci>a</ci>
      <ci>A</ci>
     </apply>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-[</ci>
     <csymbol cd="unknown">r</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">b</csymbol>
      <ci>normal-,</ci>
      <csymbol cd="unknown">a</csymbol>
      <ci>normal-)</ci>
     </cerror>
     <plus></plus>
     <csymbol cd="unknown">Œ≥</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <in></in>
       <ci>o</ci>
       <ci>normal-Œ©</ci>
      </apply>
     </apply>
     <csymbol cd="unknown">O</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">o</csymbol>
      <ci>normal-‚à£</ci>
      <csymbol cd="unknown">b</csymbol>
      <ci>normal-,</ci>
      <csymbol cd="unknown">a</csymbol>
      <ci>normal-)</ci>
     </cerror>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>V</ci>
      <times></times>
     </apply>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">œÑ</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">b</csymbol>
       <ci>normal-,</ci>
       <csymbol cd="unknown">a</csymbol>
       <ci>normal-,</ci>
       <csymbol cd="unknown">o</csymbol>
       <ci>normal-)</ci>
      </cerror>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-]</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V^{*}(b)=\max_{a\in A}\Bigl[r(b,a)+\gamma\sum_{o\in\Omega}O(o\mid b,a)V^{*}(%
\tau(b,a,o))\Bigr]
  </annotation>
 </semantics>
</math>

 For finite-horizon POMDPs, the optimal value function is piecewise-linear and convex.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> It can be represented as a finite set of vectors. In the infinite-horizon formulation, a finite vector set can approximate 

<math display="inline" id="Partially_observable_Markov_decision_process:66">
 <semantics>
  <msup>
   <mi>V</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>V</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V^{*}
  </annotation>
 </semantics>
</math>

 arbitrarily closely, whose shape remains convex. Value iteration applies dynamic programming update to gradually improve on the value until convergence to an 

<math display="inline" id="Partially_observable_Markov_decision_process:67">
 <semantics>
  <mi>œµ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>œµ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \epsilon
  </annotation>
 </semantics>
</math>

-optimal value function, and preserves its piecewise linearity and convexity.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> By improving the value, the policy is implicitly improved. Another dynamic programming technique called policy iteration explicitly represents and improves the policy instead.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h2 id="approximate-pomdp-solutions">Approximate POMDP solutions</h2>

<p>In practice, POMDPs are often computationally <a href="Computational_complexity_theory#Intractability" title="wikilink">intractable</a> to solve exactly, so computer scientists have developed methods that approximate solutions for POMDPs.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>

<p>Grid-based algorithms <a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> comprise one approximate solution technique. In this approach, the value function is computed for a set of points in the belief space, and interpolation is used to determine the optimal action to take for other belief states that are encountered which are not in the set of grid points. More recent work makes use of sampling techniques, generalization techniques and exploitation of problem structure, and has extended POMDP solving into large domains with millions of states <a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> For example, point-based methods sample random reachable belief points to constrain the planning to relevant areas in the belief space.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> Dimensionality reduction using <a href="Principle_component_analysis" title="wikilink">PCA</a> has also been explored.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></p>
<h2 id="pomdp-uses">POMDP uses</h2>

<p>POMDPs model many kinds of real-world problems. Notable works include the use of a POMDP in management of patients with ischemic heart disease,<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> assistive technology for persons with dementia <a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> and the conservation of the critically endangered and difficult to detect Sumatran tigers.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.cassandra.org/pomdp/index.shtml">Tony Cassandra's POMDP pages</a> with a tutorial, examples of problems modeled as POMDPs, and software for solving them.</li>
<li><a href="http://www.cs.cmu.edu/~trey/zmdp/">zmdp</a>, a POMDP solver by Trey Smith</li>
<li><a href="http://bigbird.comp.nus.edu.sg/pmwiki/farm/appl/index.php?n=Main.HomePage">APPL</a>, a fast point-based POMDP solver</li>
<li><a href="http://www.cs.uwaterloo.ca/~jhoey/research/spudd/">SPUDD</a>, a factored structured (PO)MDP solver that uses algebraic decision diagrams (ADDs).</li>
<li><a href="http://bitbucket.org/bami/pypomdp">pyPOMDP</a>, a (PO)MDP toolbox (simulator, solver, learner, file reader) for Python by Oliver Stollmann and Bastian Migge</li>
<li><a href="http://www.cs.uwaterloo.ca/~mgrzes/code/IsoFreeBB/">Finite-state Controllers using Branch-and-Bound</a> An Exact POMDP Solver for Policies of a Bounded Size</li>
</ul>

<p>"</p>

<p><a href="Category:Dynamic_programming" title="wikilink">Category:Dynamic programming</a> <a href="Category:Markov_processes" title="wikilink">Category:Markov processes</a> <a href="Category:Stochastic_control" title="wikilink">Category:Stochastic control</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">‚Ü©</a></li>
<li id="fn2"><a href="#fnref2">‚Ü©</a></li>
<li id="fn3"><a href="#fnref3">‚Ü©</a></li>
<li id="fn4"><a href="#fnref4">‚Ü©</a></li>
<li id="fn5"><a href="#fnref5">‚Ü©</a></li>
<li id="fn6"><a href="#fnref6">‚Ü©</a></li>
<li id="fn7"><a href="#fnref7">‚Ü©</a></li>
<li id="fn8"><a href="#fnref8">‚Ü©</a></li>
<li id="fn9"><a href="#fnref9">‚Ü©</a></li>
<li id="fn10"><a href="#fnref10">‚Ü©</a></li>
<li id="fn11"><a href="#fnref11">‚Ü©</a></li>
<li id="fn12"><a href="#fnref12">‚Ü©</a></li>
<li id="fn13"></li>
<li id="fn14"></li>
<li id="fn15"><a href="#fnref15">‚Ü©</a></li>
</ol>
</section>
</body>
</html>
