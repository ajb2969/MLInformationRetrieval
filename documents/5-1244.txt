   Probit      Probit   In probability theory and statistics , the probit function is the quantile function associated with the standard normal distribution , which is commonly denoted as N(0,1). Mathematically, it is the inverse of the cumulative distribution function of the standard normal distribution, which is denoted as    Φ   (  z  )       normal-Φ  z    \Phi(z)   , so the probit is denoted as     Φ   -  1     (  p  )        superscript  normal-Φ    1    p    \Phi^{-1}(p)   . It has applications in exploratory statistical graphics and specialized regression modeling of binary response variables .  Largely because of the central limit theorem , the standard normal distribution plays a fundamental role in probability theory and statistics. For example, when we consider the developers which choose to build a housing unit with or without fittings. 1 If we consider the familiar fact that the standard normal distribution places 95% of probability between −1.96 and 1.96, and is symmetric around zero. It follows that        Φ   (   -  1.96   )    =  0.025  =   1  -   Φ   (  1.96  )      .          normal-Φ    1.96    0.025         1    normal-Φ  1.96       \Phi(-1.96)=0.025=1-\Phi(1.96).\,\!     The probit function gives the 'inverse' computation, generating a value of an N(0,1) random variable, associated with specified cumulative probability. Continuing the example,       probit   (  0.025  )    =   -  1.96   =   -   probit   (  0.975  )            probit  0.025     1.96           probit  0.975       \operatorname{probit}(0.025)=-1.96=-\operatorname{probit}(0.975)   .  In general,       Φ   (   probit   (  p  )    )    =  p        normal-Φ   probit  p    p    \Phi(\operatorname{probit}(p))=p      and        probit   (   Φ   (  z  )    )    =  z   .       probit    normal-Φ  z    z    \operatorname{probit}(\Phi(z))=z.      Conceptual development  The idea of the probit function was published by Chester Ittner Bliss (1899–1979) in a 1934 article in Science on how to treat data such as the percentage of a pest killed by a pesticide . 2 Bliss proposed transforming the percentage killed into a "probability unit" (or "probit") which was linearly related to the modern definition (he defined it arbitrarily as equal to 0 for 0.0001 and 10 for 0.9999). He included a table to aid other researchers to convert their kill percentages to his probit, which they could then plot against the logarithm of the dose and thereby, it was hoped, obtain a more or less straight line. Such a so-called probit model is still important in toxicology, as well as other fields. The approach is justified in particular if response variation can be rationalized as a lognormal distribution of tolerances among subjects on test, where the tolerance of a particular subject is the dose just sufficient for the response of interest.  The method introduced by Bliss was carried forward in Probit Analysis , an important text on toxicological applications by D. J. Finney . 3 4 Values tabled by Finney can be derived from probits as defined here by adding a value of 5. This distinction is summarized by Collett (p. 55): 5 "The original definition of a probit [with 5 added] was primarily to avoid having to work with negative probits; ... This definition is still used in some quarters, but in the major statistical software packages for what is referred to as probit analysis , probits are defined without the addition of 5." It should be observed that probit methodology, including numerical optimization for fitting of probit functions, was introduced before widespread availability of electronic computing. When using tables, it was convenient to have probits uniformly positive. Common areas of application do not require positive probits.  Diagnosing deviation of a distribution from normality  In addition to providing a basis for important types of regression, the probit function is useful in statistical analysis for diagnosing deviation from normality, according to the method of Q-Q plotting. If a set of data is actually a sample of a normal distribution , a plot of the values against their probit scores will be approximately linear. Specific deviations from normality such as asymmetry , heavy tails , or bimodality can be diagnosed based on detection of specific deviations from linearity. While the Q-Q plot can be used for comparison to any distribution family (not only the normal), the normal Q-Q plot is a relatively standard exploratory data analysis procedure because the assumption of normality is often a starting point for analysis.  Computation  The normal distribution CDF and its inverse are not available in closed form , and computation requires careful use of numerical procedures. However, the functions are widely available in software for statistics and probability modeling, and in spreadsheets. In Microsoft Excel , for example, the probit function is available as norm.s.inv(p). In computing environments where numerical implementations of the inverse error function are available, the probit function may be obtained as        probit   (  p  )    =     2      erf   -  1     (    2  p   -  1   )      .       probit  p       2     superscript  erf    1        2  p   1       \operatorname{probit}(p)=\sqrt{2}\,\operatorname{erf}^{-1}(2p-1).   An example is MATLAB , where an 'erfinv' function is available. The language Mathematica implements 'InverseErf'. Other environments directly implement the probit function as is shown in the following session in the R programming language .  > qnorm(0.025)
  [1] -1.959964
  > pnorm(-1.96)
  [1] 0.02499790  Details for computing the inverse error function can be found at 1 . Wichura gives a fast algorithm for computing the probit function to 16 decimal places; this is used in R to generate random variates for the normal distribution. 6  An ordinary differential equation for the probit function  Another means of computation is based on forming a non-linear ordinary differential equation for probit, as per the Steinbrecher and Shaw method. 7 Abbreviating the probit function as    w   (  p  )       w  p    w(p)   , the ODE is        d  w    d  p    =   1   f   (  w  )             d  w     d  p      1    f  w      \frac{dw}{dp}=\frac{1}{f(w)}   where    f   (  w  )       f  w    f(w)   is the probability density function of   w   w   w   .  In the case of the Gaussian:        d  w    d  p    =      2  π      e    w  2   2             d  w     d  p          2  π     superscript  e     superscript  w  2   2       \frac{dw}{dp}=\sqrt{2\pi}\ e^{\frac{w^{2}}{2}}     Differentiating again:         d  2   w    d   p  2     =   w    (    d  w    d  p    )   2             superscript  d  2   w     d   superscript  p  2       w   superscript      d  w     d  p    2      \frac{d^{2}w}{dp^{2}}=w\left(\frac{dw}{dp}\right)^{2}     with the centre (initial) conditions        w   (   1  /  2   )    =  0   ,        w    1  2    0    w\left(1/2\right)=0,            w  ′    (   1  /  2   )    =    2  π     .         superscript  w  normal-′     1  2        2  π      w^{\prime}\left(1/2\right)=\sqrt{2\pi}.     This equation may be solved by several methods, including the classical power series approach. From this, solutions of arbitrarily high accuracy may be developed based on Steinbrecher's approach to the series for the inverse error function. The power series solution is given by       w   (  p  )    =     π  2      ∑   k  =  0   ∞      d  k    (    2  k   +  1   )      (    2  p   -  1   )    (    2  k   +  1   )             w  p         π  2      superscript   subscript     k  0           subscript  d  k       2  k   1     superscript      2  p   1       2  k   1         w(p)=\sqrt{\frac{\pi}{2}}\sum_{k=0}^{\infty}\frac{d_{k}}{(2k+1)}(2p-1)^{(2k+1)}     where the coefficients    d  k     subscript  d  k    d_{k}   satisfy the non-linear recurrence       d   k  +  1    =    π  4     ∑   j  =  0   k      d  j    d   k  -  j       (   j  +  1   )    (    2  j   +  1   )            subscript  d    k  1        π  4     superscript   subscript     j  0    k        subscript  d  j    subscript  d    k  j         j  1       2  j   1         d_{k+1}=\frac{\pi}{4}\sum_{j=0}^{k}\frac{d_{j}d_{k-j}}{(j+1)(2j+1)}     with     d  0   =  1       subscript  d  0   1    d_{0}=1   . In this form the ratio      d   k  +  1    /   d  k    →  1     normal-→     subscript  d    k  1     subscript  d  k    1    d_{k+1}/d_{k}\rightarrow 1   as    k  →  ∞     normal-→  k     k\rightarrow\infty   .  See also  (Figure)  Comparison of the logit function with a scaled probit (i.e. the inverse CDF of the normal distribution ), comparing    logit   (  x  )      logit  x    \operatorname{logit}(x)   vs.      Φ   -  1     (  x  )    /    π  8           superscript  normal-Φ    1    x       π  8      \Phi^{-1}(x)/\sqrt{\frac{\pi}{8}}   , which makes the slopes the same at the origin.   Closely related to the probit function (and probit model ) are the logit function and logit model . The inverse of the logistic function is given by        logit   (  p  )    =   log   (   p   1  -  p    )     .       logit  p       p    1  p       \operatorname{logit}(p)=\log\left(\frac{p}{1-p}\right).     Analogously to the probit model, we may assume that such a quantity is related linearly to a set of predictors, resulting in the logit model , the basis in particular of logistic regression model, the most prevalent form of regression analysis for categorical response data. In current statistical practice, probit and logit regression models are often handled as cases of the generalized linear model .  See also   Detection error tradeoff graphs (DET Graphs, an alternative to the ROC)  Logistic regression (a.k.a. logit model)  Logit  Probit model  Multinomial probit  Q-Q plot  Continuous function  Monotonic function  Quantile function  Sigmoid function  Rankit analysis, also developed by Chester Bliss  Ridit scoring   References  ru:Пробит регрессия "  Category:Statistical terminology  Category:Data analysis  Category:Single-equation methods (econometrics)  Category:Econometrics  Category:Normal distribution  Category:Statistical functions  Category:Probability theory     ↩  ↩  Finney, D.J. (1947), Probit Analysis . (1st edition) Cambridge University Press, Cambridge, UK. ↩  ↩  ↩  ↩  ↩     