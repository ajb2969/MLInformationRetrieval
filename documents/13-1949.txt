   Graphical models for protein structure      Graphical models for protein structure   Graphical models have become powerful frameworks for protein structure prediction , protein–protein interaction and free energy calculations for protein structures. Using a graphical model to represent the protein structure allows the solution of many problems including secondary structure prediction, protein protein interactions, protein-drug interaction, and free energy calculations.  There are two main approaches to use graphical models in protein structure modeling. The first approach uses discrete variables for representing coordinates or dihedral angles of the protein structure. The variables are originally all continuous values and, to transform them into discrete values, a discretization process is typically applied. The second approach uses continuous variables for the coordinates or dihedral angles.  Discrete graphical models for protein structure  Markov random fields , also known as undirected graphical models are common representations for this problem. Given an undirected graph  G = ( V , E ), a set of random variables  X = ( X v ) v ∈ V indexed by V , form a Markov random field with respect to G if they satisfy the pairwise Markov property:   any two non-adjacent variables are conditionally independent given all other variables:        X  u   ⟂  ⟂   X  v   |   X   V  ∖   {  u  ,  v  }     if   {  u  ,  v  }   ∉  E  .     fragments   subscript  X  u   perpendicular-to  perpendicular-to   subscript  X  v   normal-|   subscript  X    V   u  v      if   fragments  normal-{  u  normal-,  v  normal-}    E  normal-.    X_{u}\perp\!\!\!\perp X_{v}|X_{V\setminus\{u,v\}}\quad\text{if }\{u,v\}\notin E.     In the discrete model, the continuous variables are discretized into a set of favorable discrete values. If the variables of choice are dihedral angles , the discretization is typically done by mapping each value to the corresponding rotamer conformation.  Model  Let X = { X b , X s } be the random variables representing the entire protein structure. X b can be represented by a set of 3-d coordinates of the backbone atoms, or equivalently, by a sequence of bond lengths and dihedral angles . The probability of a particular conformation  x can then be written as:      p   (  X  =  x  |  Θ  )   =  p   (   X  b   =   x  b   )   p   (   X  s   =   x  s   |   X  b   ,  Θ  )   ,     fragments  p   fragments  normal-(  X   x  normal-|  Θ  normal-)    p   fragments  normal-(   subscript  X  b     subscript  x  b   normal-)   p   fragments  normal-(   subscript  X  s     subscript  x  s   normal-|   subscript  X  b   normal-,  Θ  normal-)   normal-,    p(X=x|\Theta)=p(X_{b}=x_{b})p(X_{s}=x_{s}|X_{b},\Theta),\,     where   Θ   normal-Θ   \Theta   represents any parameters used to describe this model, including sequence information, temperature etc. Frequently the backbone is assumed to be rigid with a known conformation, and the problem is then transformed to a side-chain placement problem. The structure of the graph is also encoded in   Θ   normal-Θ   \Theta   . This structure shows which two variables are conditionally independent. As an example, side chain angles of two residues far apart can be independent given all other angles in the protein. To extract this structure, researchers use a distance threshold, and only pair of residues which are within that threshold are considered connected (i.e. have an edge between them).  Given this representation, the probability of a particular side chain conformation x s given the backbone conformation x b can be expressed as      p   (   X  s   =   x  s   |   X  b   =   x  b   )   =   1  Z    ∏   c  ∈   C   (  G  )       Φ  c    (   x  s  c   ,   x  b  c   )      fragments  p   fragments  normal-(   subscript  X  s     subscript  x  s   normal-|   subscript  X  b     subscript  x  b   normal-)      1  Z    subscript  product    c    C  G      subscript  normal-Φ  c    fragments  normal-(   superscript   subscript  x  s   c   normal-,   superscript   subscript  x  b   c   normal-)     p(X_{s}=x_{s}|X_{b}=x_{b})=\frac{1}{Z}\prod_{c\in C(G)}\Phi_{c}(x_{s}^{c},x_{b%
 }^{c})     where C ( G ) is the set of all cliques in G ,   Φ   normal-Φ   \Phi   is a potential function defined over the variables, and Z is the partition function .  To completely characterize the MRF, it is necessary to define the potential function   Φ   normal-Φ   \Phi   . To simplify, the cliques of a graph are usually restricted to only the cliques of size 2, which means the potential function is only defined over pairs of variables. In Goblin System , this pairwise functions are defined as       Φ   (   x  s   i  p    ,   x  b   j  q    )    =   exp   (   -     E   (   x  s   i  p    ,   x  b   j  q    )    /   K  B    T    )          normal-Φ    superscript   subscript  x  s    subscript  i  p     superscript   subscript  x  b    subscript  j  q                E    superscript   subscript  x  s    subscript  i  p     superscript   subscript  x  b    subscript  j  q       subscript  K  B    T       \Phi(x_{s}^{i_{p}},x_{b}^{j_{q}})=\exp(-E(x_{s}^{i_{p}},x_{b}^{j_{q}})/K_{B}T)     where    E   (   x  s   i  p    ,   x  b   j  q    )       E    superscript   subscript  x  s    subscript  i  p     superscript   subscript  x  b    subscript  j  q       E(x_{s}^{i_{p}},x_{b}^{j_{q}})   is the energy of interaction between rotamer state p of residue    X  i  s     superscript   subscript  X  i   s    X_{i}^{s}   and rotamer state q of residue    X  j  s     superscript   subscript  X  j   s    X_{j}^{s}   and    k  B     subscript  k  B    k_{B}   is the Boltzmann constant .  Using a PDB file, this model can be built over the protein structure. From this model free energy can be calculated.  Free energy calculation: belief propagation  It has been shown that the free energy of a system is calculated as      G  =   E  -   T  S        G    E    T  S      G=E-TS     where E is the enthalpy of the system, T the temperature and S, the entropy. Now if we associate a probability with each state of the system, (p(x) for each conformation value, x), G can be rewritten as      G  =     ∑  x    p   (  x  )   E   (  x  )     -   T    ∑  x    p   (  x  )    ln   (   p   (  x  )    )            G      subscript   x     p  x  E  x      T    subscript   x     p  x      p  x          G=\sum_{x}p(x)E(x)-T\sum_{x}p(x)\ln(p(x))\,     Calculating p(x) on discrete graphs is done by the generalized belief propagation algorithm. This algorithm calculates an approximation to the probabilities, and it is not guaranteed to converge to a final value set. However, in practice, it has been shown to converge successfully in many cases.  Continuous graphical models for protein structures  Graphical models can still be used when the variables of choice are continuous. In these cases, the probability distribution is represented as a multivariate probability distribution over continuous variables. Each family of distribution will then impose certain properties on the graphical model. Multivariate Gaussian distribution is one of the most convenient distributions in this problem. The simple form of the probability, and the direct relation with the corresponding graphical model makes it a popular choice among researchers.  Gaussian graphical models of protein structures  Gaussian graphical models are multivariate probability distributions encoding a network of dependencies among variables. Let    Θ  =   [   θ  1   ,   θ  2   ,  …  ,   θ  n   ]       normal-Θ    subscript  θ  1    subscript  θ  2   normal-…   subscript  θ  n      \Theta=[\theta_{1},\theta_{2},\dots,\theta_{n}]   be a set of   n   n   n   variables, such as   n   n   n    dihedral angles , and let    f   (  Θ  =  D  )      fragments  f   fragments  normal-(  Θ   D  normal-)     f(\Theta=D)   be the value of the probability density function at a particular value D . A multivariate Gaussian graphical model defines this probability as follows:      f   (  Θ  =  D  )   =   1  Z   exp   {  -   1  2     (  D  -  μ  )   T    Σ   -  1     (  D  -  μ  )   }      fragments  f   fragments  normal-(  Θ   D  normal-)      1  Z     fragments  normal-{     1  2    superscript   fragments  normal-(  D   μ  normal-)   T    superscript  normal-Σ    1     fragments  normal-(  D   μ  normal-)   normal-}     f(\Theta=D)=\frac{1}{Z}\exp\left\{-\frac{1}{2}(D-\mu)^{T}\Sigma^{-1}(D-\mu)\right\}     Where    Z  =     (   2  π   )    n  /  2      |  Σ  |    1  /  2         Z     superscript    2  π     n  2     superscript    normal-Σ     1  2       Z=(2\pi)^{n/2}|\Sigma|^{1/2}   is the closed form for the partition function . The parameters of this distribution are   μ   μ   \mu   and   Σ   normal-Σ   \Sigma   .   μ   μ   \mu   is the vector of mean values of each variable, and    Σ   -  1      superscript  normal-Σ    1     \Sigma^{-1}   , the inverse of the covariance matrix , also known as the precision matrix . Precision matrix contains the pairwise dependencies between the variables. A zero value in    Σ   -  1      superscript  normal-Σ    1     \Sigma^{-1}   means that conditioned on the values of the other variables, the two corresponding variable are independent of each other.  To learn the graph structure as a multivariate Gaussian graphical model, we can use either L-1 regularization , or neighborhood selection algorithms. These algorithms simultaneously learn a graph structure and the edge strength of the connected nodes. An edge strength corresponds to the potential function defined on the corresponding two-node clique . We use a training set of a number of PDB structures to learn the   μ   μ   \mu   and    Σ   -  1      superscript  normal-Σ    1     \Sigma^{-1}   .  Once the model is learned, we can repeat the same step as in the discrete case, to get the density functions at each node, and use analytical form to calculate the free energy. Here, the partition function already has a closed form , so the inference , at least for the Gaussian graphical models is trivial. If the analytical form of the partition function is not available, particle filtering or expectation propagation can be used to approximate Z , and then perform the inference and calculate free energy.  References   Time Varying Undirected Graphs, Shuheng Zhou and John D. Lafferty and Larry A. Wasserman, COLT 2008  Free Energy Estimates of All-atom Protein Structures Using Generalized Belief Propagation, Hetunandan Kamisetty Eric P. Xing Christopher J. Langmead, RECOMB 2008   External links   http://www.liebertonline.com/doi/pdf/10.1089/cmb.2007.0131  http://www.learningtheory.org/colt2008/81-Zhou.pdf   Predicting Protein Folds with Structural Repeats Using a Chain Graph Model   "  Category:Graphical models  Category:Protein methods  Category:Computational chemistry   