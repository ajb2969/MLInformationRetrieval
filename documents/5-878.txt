   Line search      Line search   In optimization , the line search strategy is one of two basic iterative approaches to find a local minimum     𝐱  *     superscript  𝐱     \mathbf{x}^{*}   of an objective function     f  :    ℝ  n   →  ℝ      normal-:  f   normal-→   superscript  ℝ  n   ℝ     f:\mathbb{R}^{n}\to\mathbb{R}   . The other approach is trust region .  The line search approach first finds a descent direction along which the objective function   f   f   f   will be reduced and then computes a step size that determines how far   𝐱   𝐱   \mathbf{x}   should move along that direction. The descent direction can be computed by various methods, such as gradient descent , Newton's method and Quasi-Newton method . The step size can be determined either exactly or inexactly.  Example use  Here is an example gradient method that uses a line search in step 4.   Set iteration counter    k  =  0      k  0    \displaystyle k=0   , and make an initial guess,    𝐱  0     subscript  𝐱  0    \mathbf{x}_{0}   for the minimum  Repeat:  Compute a descent direction     𝐩  k     subscript  𝐩  k    \mathbf{p}_{k}     Choose    α  k     subscript  α  k    \displaystyle\alpha_{k}   to 'loosely' minimize     h   (  α  )    =   f   (    𝐱  k   +   α   𝐩  k     )          h  α     f     subscript  𝐱  k     α   subscript  𝐩  k        h(\alpha)=f(\mathbf{x}_{k}+\alpha\mathbf{p}_{k})   over    α  ∈   ℝ  +       α   subscript  ℝ      \alpha\in\mathbb{R}_{+}     Update     𝐱   k  +  1    =    𝐱  k   +    α  k    𝐩  k          subscript  𝐱    k  1       subscript  𝐱  k      subscript  α  k    subscript  𝐩  k       \mathbf{x}_{k+1}=\mathbf{x}_{k}+\alpha_{k}\mathbf{p}_{k}   , and    k  =   k  +  1       k    k  1     \displaystyle k=k+1     Until    ∥    ∇  f    (   𝐱  k   )    ∥     norm     normal-∇  f    subscript  𝐱  k      \|\nabla f(\mathbf{x}_{k})\|   < tolerance   At the line search step (4) the algorithm might either exactly minimize h , by solving      h  ′    (   α  k   )    =  0         superscript  h  normal-′    subscript  α  k    0    h^{\prime}(\alpha_{k})=0   , or loosely , by asking for a sufficient decrease in h . One example of the former is conjugate gradient method . The latter is called inexact line search and may be performed in a number of ways, such as a backtracking line search or using the Wolfe conditions .  Like other optimization methods, line search may be combined with simulated annealing to allow it to jump over some local minima .  Algorithms  Direct search methods  In this method, the minimum must first be bracketed, so the algorithm must identify points x 1 and x 2 such that the sought minimum lies between them. The interval is then divided by computing    f   (  x  )       f  x    f(x)   at two internal points, x 3 and x 4 , and rejecting whichever of the two outer points is not adjacent to that of x 3 and x 4 which has the lowest function value. In subsequent steps, only one extra internal point needs to be calculated. Of the various methods of dividing the interval, 1  golden section search is particularly simple and effective, as the interval proportions are preserved regardless of how the search proceeds:        1  ϕ    (    x  2   -   x  1    )    =    x  4   -   x  1    =    x  2   -   x  3    =   ϕ   (    x  2   -   x  4    )    =   ϕ   (    x  3   -   x  1    )    =    ϕ  2    (    x  4   -   x  3    )              1  ϕ      subscript  x  2    subscript  x  1        subscript  x  4    subscript  x  1            subscript  x  2    subscript  x  3           ϕ     subscript  x  2    subscript  x  4            ϕ     subscript  x  3    subscript  x  1             superscript  ϕ  2      subscript  x  4    subscript  x  3        \frac{1}{\phi}(x_{2}-x_{1})=x_{4}-x_{1}=x_{2}-x_{3}=\phi(x_{2}-x_{4})=\phi(x_{%
 3}-x_{1})=\phi^{2}(x_{4}-x_{3})   where    ϕ  =    1  2    (   1  +   5    )    ≈  1.618        ϕ      1  2     1    5          1.618     \phi=\frac{1}{2}(1+\sqrt{5})\approx 1.618     See also   Backtracking line search  Secant method  Newton–Raphson method  Pattern search (optimization)  Nelder–Mead method  Golden section search   References    "  Category:Mathematical optimization  Category:Optimization algorithms and methods     ↩     