   Line search      Line search   In optimization , the line search strategy is one of two basic iterative approaches to find a local minimum     ğ±  *     superscript  ğ±     \mathbf{x}^{*}   of an objective function     f  :    â„  n   â†’  â„      normal-:  f   normal-â†’   superscript  â„  n   â„     f:\mathbb{R}^{n}\to\mathbb{R}   . The other approach is trust region .  The line search approach first finds a descent direction along which the objective function   f   f   f   will be reduced and then computes a step size that determines how far   ğ±   ğ±   \mathbf{x}   should move along that direction. The descent direction can be computed by various methods, such as gradient descent , Newton's method and Quasi-Newton method . The step size can be determined either exactly or inexactly.  Example use  Here is an example gradient method that uses a line search in step 4.   Set iteration counter    k  =  0      k  0    \displaystyle k=0   , and make an initial guess,    ğ±  0     subscript  ğ±  0    \mathbf{x}_{0}   for the minimum  Repeat:  Compute a descent direction     ğ©  k     subscript  ğ©  k    \mathbf{p}_{k}     Choose    Î±  k     subscript  Î±  k    \displaystyle\alpha_{k}   to 'loosely' minimize     h   (  Î±  )    =   f   (    ğ±  k   +   Î±   ğ©  k     )          h  Î±     f     subscript  ğ±  k     Î±   subscript  ğ©  k        h(\alpha)=f(\mathbf{x}_{k}+\alpha\mathbf{p}_{k})   over    Î±  âˆˆ   â„  +       Î±   subscript  â„      \alpha\in\mathbb{R}_{+}     Update     ğ±   k  +  1    =    ğ±  k   +    Î±  k    ğ©  k          subscript  ğ±    k  1       subscript  ğ±  k      subscript  Î±  k    subscript  ğ©  k       \mathbf{x}_{k+1}=\mathbf{x}_{k}+\alpha_{k}\mathbf{p}_{k}   , and    k  =   k  +  1       k    k  1     \displaystyle k=k+1     Until    âˆ¥    âˆ‡  f    (   ğ±  k   )    âˆ¥     norm     normal-âˆ‡  f    subscript  ğ±  k      \|\nabla f(\mathbf{x}_{k})\|   < tolerance   At the line search step (4) the algorithm might either exactly minimize h , by solving      h  â€²    (   Î±  k   )    =  0         superscript  h  normal-â€²    subscript  Î±  k    0    h^{\prime}(\alpha_{k})=0   , or loosely , by asking for a sufficient decrease in h . One example of the former is conjugate gradient method . The latter is called inexact line search and may be performed in a number of ways, such as a backtracking line search or using the Wolfe conditions .  Like other optimization methods, line search may be combined with simulated annealing to allow it to jump over some local minima .  Algorithms  Direct search methods  In this method, the minimum must first be bracketed, so the algorithm must identify points x 1 and x 2 such that the sought minimum lies between them. The interval is then divided by computing    f   (  x  )       f  x    f(x)   at two internal points, x 3 and x 4 , and rejecting whichever of the two outer points is not adjacent to that of x 3 and x 4 which has the lowest function value. In subsequent steps, only one extra internal point needs to be calculated. Of the various methods of dividing the interval, 1  golden section search is particularly simple and effective, as the interval proportions are preserved regardless of how the search proceeds:        1  Ï•    (    x  2   -   x  1    )    =    x  4   -   x  1    =    x  2   -   x  3    =   Ï•   (    x  2   -   x  4    )    =   Ï•   (    x  3   -   x  1    )    =    Ï•  2    (    x  4   -   x  3    )              1  Ï•      subscript  x  2    subscript  x  1        subscript  x  4    subscript  x  1            subscript  x  2    subscript  x  3           Ï•     subscript  x  2    subscript  x  4            Ï•     subscript  x  3    subscript  x  1             superscript  Ï•  2      subscript  x  4    subscript  x  3        \frac{1}{\phi}(x_{2}-x_{1})=x_{4}-x_{1}=x_{2}-x_{3}=\phi(x_{2}-x_{4})=\phi(x_{%
 3}-x_{1})=\phi^{2}(x_{4}-x_{3})   where    Ï•  =    1  2    (   1  +   5    )    â‰ˆ  1.618        Ï•      1  2     1    5          1.618     \phi=\frac{1}{2}(1+\sqrt{5})\approx 1.618     See also   Backtracking line search  Secant method  Newtonâ€“Raphson method  Pattern search (optimization)  Nelderâ€“Mead method  Golden section search   References    "  Category:Mathematical optimization  Category:Optimization algorithms and methods     â†©     