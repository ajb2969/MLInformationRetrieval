   Concentration inequality      Concentration inequality   In mathematics , concentration inequalities provide probability bounds on how a random variable deviates from some value (e.g., its expectation ). The laws of large numbers of classical probability theory state that sums of independent random variables are, under very mild conditions, close to their expectation with a large probability. Such sums are the most basic examples of random variables concentrated around their mean . Recent results shows that such behavior is shared by other functions of independent random variables.  Markov's inequality  If X is any random variable and a > 0, then        Pr   (    |  X  |   ≥  a   )    ≤    E   (   |  X  |   )    a    .       Pr      X   a        E    X    a     \Pr(|X|\geq a)\leq\frac{\textrm{E}(|X|)}{a}.     Proof can be found here .  We can extend Markov's inequality to a strictly increasing and non-negative function   Φ   normal-Φ   \Phi   . We have        Pr   (   X  ≥  a   )    =   Pr   (    Φ   (  X  )    ≥   Φ   (  a  )     )    ≤    E   (   Φ   (  X  )    )     Φ   (  a  )      .         Pr    X  a     Pr      normal-Φ  X     normal-Φ  a              E    normal-Φ  X      normal-Φ  a       \Pr(X\geq a)=\Pr(\Phi(X)\geq\Phi(a))\leq\frac{\textrm{E}(\Phi(X))}{\Phi(a)}.     Chebyshev's inequality  Chebyshev's inequality is a special case of generalized Markov's inequality when    Φ  =   x  2       normal-Φ   superscript  x  2     \Phi=x^{2}     If X is any random variable and a > 0, then        Pr   (    |   X  -   E   (  X  )     |   ≥  a   )    ≤    Var   (  X  )     a  2     ,       Pr        X    E  X     a        Var  X    superscript  a  2      \Pr(|X-\textrm{E}(X)|\geq a)\leq\frac{\textrm{Var}(X)}{a^{2}},     Where Var(X) is the variance of X, defined as:        Var   (  X  )    =   E   [    (   X  -   E   (  X  )     )   2   ]     .       Var  X    normal-E   superscript    X   normal-E  X    2      \operatorname{Var}(X)=\operatorname{E}[(X-\operatorname{E}(X))^{2}].     Asymptotic behavior of binomial distribution  If a random variable X follows the binomial distribution with parameter   n   n   n   and   p   p   p   . The probability of getting exact   k   k   k   successes in   n   n   n   trials is given by the probability mass function        f   (  k  ;  n  ,  p  )    =   Pr   (   K  =  k   )    =    (     n      k     )    p  k     (   1  -  p   )    n  -  k      .          f   k  n  p     Pr    K  k            binomial  n  k    superscript  p  k    superscript    1  p     n  k        f(k;n,p)=\Pr(K=k)={n\choose k}p^{k}(1-p)^{n-k}.     Let     S  n   =    ∑   i  =  1   n    X  i         subscript  S  n     superscript   subscript     i  1    n    subscript  X  i      S_{n}=\sum_{i=1}^{n}X_{i}   and    X  i     subscript  X  i    X_{i}   's are i.i.d.  Bernoulli random variables with parameter   p   p   p   .    S  n     subscript  S  n    S_{n}   follows the binomial distribution with parameter   n   n   n   and   p   p   p   . Central Limit Theorem suggests when    n  →  ∞     normal-→  n     n\to\infty   ,    S  n     subscript  S  n    S_{n}   is approximately normally distributed with mean    n  p      n  p    np   and variance    n  p   (   1  -  p   )       n  p    1  p     np(1-p)   , and     λ   λ   \lambda   , where    B   (  n  ,  p  )       B   n  p     B(n,p)   is a constant, the limit distribution of binomial distribution    P   (  λ  )       P  λ    P(\lambda)   is the Poisson distribution     X  i     subscript  X  i    X_{i}     General Chernoff inequality  A Chernoff bound gives exponentially decreasing bounds on tail distributions of sums of independent random variables. 1 Let     X  i   ≥    E   (   X  i   )    -   a  i   -  M        subscript  X  i       E   subscript  X  i     subscript  a  i   M     X_{i}\geq E(X_{i})-a_{i}-M   denote independent but not necessarily identical random variables, satisfying    1  ≤  i  ≤  n        1  i       n     1\leq i\leq n   , for    X  =    ∑   i  =  1   n    X  i        X    superscript   subscript     i  1    n    subscript  X  i      X=\sum_{i=1}^{n}X_{i}   .       Pr   [   X  ≤    E   (  X  )    -  λ    ]    ≤   e   -    λ  2    2   (    V  a  r   (  X  )    +     ∑   i  =  1   n     a  i  2    +    M  λ   /  3    )            Pr    X      E  X   λ      superscript  e       superscript  λ  2     2      V  a  r  X     superscript   subscript     i  1    n    superscript   subscript  a  i   2        M  λ   3          \Pr[X\leq E(X)-\lambda]\leq e^{-\frac{\lambda^{2}}{2(Var(X)+\sum_{i=1}^{n}a_{i%
 }^{2}+M\lambda/3)}}     we have lower tail inequality:      X  i     subscript  X  i    X_{i}   If     X  i   ≤    E   (   X  i   )    +   a  i   +  M        subscript  X  i       E   subscript  X  i     subscript  a  i   M     X_{i}\leq E(X_{i})+a_{i}+M   satisfies     Pr   [   X  ≥    E   (  X  )    +  λ    ]    ≤   e   -    λ  2    2   (    V  a  r   (  X  )    +     ∑   i  =  1   n     a  i  2    +    M  λ   /  3    )            Pr    X      E  X   λ      superscript  e       superscript  λ  2     2      V  a  r  X     superscript   subscript     i  1    n    superscript   subscript  a  i   2        M  λ   3          \Pr[X\geq E(X)+\lambda]\leq e^{-\frac{\lambda^{2}}{2(Var(X)+\sum_{i=1}^{n}a_{i%
 }^{2}+M\lambda/3)}}   , we have upper tail inequality:      X  i     subscript  X  i    X_{i}     If     |   X  i   |   ≤  1         subscript  X  i    1    |X_{i}|\leq 1   are i.i.d. ,    σ  2     superscript  σ  2    \sigma^{2}   and    X  i     subscript  X  i    X_{i}   is the variance of     Pr   [    |  X  |   ≥   k  σ    ]    ≤   2   e   -     k  2   /  4   n           Pr      X     k  σ       2   superscript  e         superscript  k  2   4   n        \Pr[|X|\geq k\sigma]\leq 2e^{-k^{2}/4n}   . A typical version of Chernoff Inequality is:      0  ≤  k  ≤   2  σ         0  k         2  σ      0\leq k\leq 2\sigma   for     X  1   ,  …  ,    X  n        subscript  X  1   normal-…   subscript  X  n     X_{1},\dots,X_{n}\!     Hoeffding's inequality  Hoeffding's inequality can be stated as follows:  If    X  i     subscript  X  i    X_{i}   are independent. Assume that the    1  ≤  i  ≤  n        1  i       n     1\leq i\leq n   are almost surely bounded; that is, assume for     Pr   (    X  i   ∈   [   a  i   ,   b  i   ]    )    =   1.        Pr     subscript  X  i     subscript  a  i    subscript  b  i      1.    \Pr(X_{i}\in[a_{i},b_{i}])=1.\!   that       X  ¯   =     X  1   +  ⋯  +   X  n    n        normal-¯  X        subscript  X  1   normal-⋯   subscript  X  n    n     \overline{X}=\frac{X_{1}+\cdots+X_{n}}{n}     Then, for the empirical mean of these variables        Pr   (     X  ¯   -   E   [   X  ¯   ]     ≥  t   )    ≤   exp   (   -    2   t  2    n  2      ∑   i  =  1   n     (    b  i   -   a  i    )   2      )     ,       Pr       normal-¯  X     normal-E   delimited-[]   normal-¯  X      t            2   superscript  t  2    superscript  n  2      superscript   subscript     i  1    n    superscript     subscript  b  i    subscript  a  i    2         \Pr(\overline{X}-\mathrm{E}[\overline{X}]\geq t)\leq\exp\left(-\frac{2t^{2}n^{%
 2}}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}\right),\!     we have the inequalities (Hoeffding 1963, Theorem 2 2 ):        Pr   (    |    X  ¯   -   E   [   X  ¯   ]     |   ≥  t   )    ≤   2   exp   (   -    2   t  2    n  2      ∑   i  =  1   n     (    b  i   -   a  i    )   2      )      ,       Pr         normal-¯  X     normal-E   delimited-[]   normal-¯  X       t      2          2   superscript  t  2    superscript  n  2      superscript   subscript     i  1    n    superscript     subscript  b  i    subscript  a  i    2          \Pr(|\overline{X}-\mathrm{E}[\overline{X}]|\geq t)\leq 2\exp\left(-\frac{2t^{2%
 }n^{2}}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}\right),\!        i   i   i     Bennett's inequality  Bennett's inequality was proved by George Bennett of the University of New South Wales in 1962. 3  Let be independent random variables , and assume (for simplicity but without loss of generality ) they all have zero expected value. Further assume X i {{!}} ≤ a }}  almost surely for all      σ  2   =    1  n     ∑   i  =  1   n    Var   (   X  i   )       .       superscript  σ  2       1  n     superscript   subscript     i  1    n    Var   subscript  X  i        \sigma^{2}=\frac{1}{n}\sum_{i=1}^{n}\operatorname{Var}(X_{i}).   , and let      t  ≥  0      t  normal-≥  0    t≥0   Then for any      Pr   (     ∑   i  =  1   n    X  i    >  t   )    ≤   exp   (   -     n   σ  2     a  2    h   (    a  t    n   σ  2     )     )     ,       Pr      superscript   subscript     i  1    n    subscript  X  i    t              n   superscript  σ  2     superscript  a  2    h      a  t     n   superscript  σ  2          \Pr\left(\sum_{i=1}^{n}X_{i}>t\right)\leq\exp\left(-\frac{n\sigma^{2}}{a^{2}}h%
 \left(\frac{at}{n\sigma^{2}}\right)\right),   ,       h   (  u  )    =    (   1  +  u   )   l  o  g   (   1  +  u   )   –  u         h  u       1  u   l  o  g    1  u   normal-–  u     h(u)=(1+u)log(1+u)–u     where   ε   ε   \varepsilon   , 4 see also Fan et al. (2012) 5 for martingale version of Bennett's inequality and its improvement.  Bernstein's inequality  Bernstein inequalities give bounds on the probability that the sum of random variables deviates from its mean. In the simplest case, let X 1 , ..., X n be independent Bernoulli random variables taking values +1 and −1 with probability 1/2, then for every positive    𝐏   {  |   1  n    ∑   i  =  1   n     X  i    |  >  ε  }   ≤  2  exp   {  -    n   ε  2     2   (   1  +   ε  /  3    )     }   .     fragments  P   fragments  normal-{  normal-|    1  n    superscript   subscript     i  1    n    subscript  X  i   normal-|   ε  normal-}    2    fragments  normal-{       n   superscript  ε  2      2    1    ε  3      normal-}   normal-.    \mathbf{P}\left\{\left|\;\frac{1}{n}\sum_{i=1}^{n}X_{i}\;\right|>\varepsilon%
 \right\}\leq 2\exp\left\{-\frac{n\varepsilon^{2}}{2(1+\varepsilon/3)}\right\}.   ,       X  1   …   X  n        subscript  X  1   normal-…   subscript  X  n     X_{1}\dots X_{n}     Efron–Stein inequality  The Efron–Stein inequality (or influence inequality, or MG bound on variance) bounds the variance of a general function.  Suppose that     X  1  ′   …   X  n  ′        superscript   subscript  X  1   normal-′   normal-…   superscript   subscript  X  n   normal-′     X_{1}^{\prime}\dots X_{n}^{\prime}   ,    X  i  ′     superscript   subscript  X  i   normal-′    X_{i}^{\prime}   are independent with    X  i     subscript  X  i    X_{i}   and   i   i   i   having the same distribution for all      X  =   (   X  1   ,  …  ,   X  n   )    ,    X   (  i  )    =   (   X  1   ,  …  ,   X   i  -  1    ,   X  i  ′   ,   X   i  +  1    ,  …  ,   X  n   )     .     formulae-sequence    X    subscript  X  1   normal-…   subscript  X  n        superscript  X  i     subscript  X  1   normal-…   subscript  X    i  1     superscript   subscript  X  i   normal-′    subscript  X    i  1    normal-…   subscript  X  n       X=(X_{1},\dots,X_{n}),X^{(i)}=(X_{1},\dots,X_{i-1},X_{i}^{\prime},X_{i+1},%
 \dots,X_{n}).   .  Let      Var   (   f   (  X  )    )    ≤    1  2     ∑   i  =  1   n    E   [    (    f   (  X  )    -   f   (   X   (  i  )    )     )   2   ]       .        Var    f  X        1  2     superscript   subscript     i  1    n     E   delimited-[]   superscript      f  X     f   superscript  X  i     2         \mathrm{Var}(f(X))\leq\frac{1}{2}\sum_{i=1}^{n}E[(f(X)-f(X^{(i)}))^{2}].   Then  $$\mathrm{Var}(f(X)) \leq \frac{1}{2} \sum_{i=1}^{n} E[(f(X)-f(X^{(i)}))^2].$$  References  "  Category:Inequality     ↩  Wassily Hoeffding, Probability inequalities for sums of bounded random variables, Journal of the American Statistical Association  58 (301): 13–30, March 1963. ( JSTOR ) ↩  ↩  ↩  ↩     