


Concentration inequality




Concentration inequality

In mathematics, concentration inequalities provide probability bounds on how a random variable deviates from some value (e.g., its expectation). The laws of large numbers of classical probability theory state that sums of independent random variables are, under very mild conditions, close to their expectation with a large probability. Such sums are the most basic examples of random variables concentrated around their mean. Recent results shows that such behavior is shared by other functions of independent random variables.
Markov's inequality
If X is any random variable and a > 0, then



Proof can be found here.
We can extend Markov's inequality to a strictly increasing and non-negative function 
 
 
 
 . We have



Chebyshev's inequality
Chebyshev's inequality is a special case of generalized Markov's inequality when 
 
 

If X is any random variable and a > 0, then



Where Var(X) is the variance of X, defined as:



Asymptotic behavior of binomial distribution
If a random variable X follows the binomial distribution with parameter 
 
 
 
  and 
 
 
 
 . The probability of getting exact 
 
 
 
  successes in 
 
 
 
  trials is given by the probability mass function



Let 
 
 
 
  and 
 
 
 
 's are i.i.d. Bernoulli random variables with parameter 
 
 
 
 . 
 
 
 
  follows the binomial distribution with parameter 
 
 
 
 
  and 
 
 
 
 . Central Limit Theorem suggests when 
 
 
 
 , 
 
 
 
  is approximately normally distributed with mean 
 
 
 
  and variance 
 
 
 
 
 , and


 
 , where 
 
 
 
  is a constant, the limit distribution of binomial distribution 
 
 
 
  is the Poisson distribution


General Chernoff inequality
A Chernoff bound gives exponentially decreasing bounds on tail distributions of sums of independent random variables.1 Let 
 
 
 
 
  denote independent but not necessarily identical random variables, satisfying 
 
 
 
 , for 
 
 
 
 .



we have lower tail inequality:


 
  If 
 
 
 
 
  satisfies 
 
 
 
 , we have upper tail inequality:



If 
 
 
 
  are i.i.d., 
 
 
 
  and 
 
 
 
 
  is the variance of 
 
 
 
 . A typical version of Chernoff Inequality is:


 
  for 
 
 

Hoeffding's inequality
Hoeffding's inequality can be stated as follows:
If 
 
 
 
  are independent. Assume that the 
 
 
 
 
  are almost surely bounded; that is, assume for 
 
 
 
  that



Then, for the empirical mean of these variables



we have the inequalities (Hoeffding 1963, Theorem 2 2):






Bennett's inequality
Bennett's inequality was proved by George Bennett of the University of New South Wales in 1962.3
Let  be independent random variables, and assume (for simplicity but without loss of generality) they all have zero expected value. Further assume Xi{{!}} ≤ a}} almost surely for all 
 
 
 
 , and let


 
  Then for any 
 
 
 
 ,



where 
 
 
 
 ,4 see also Fan et al. (2012) 5 for martingale version of Bennett's inequality and its improvement.
Bernstein's inequality
Bernstein inequalities give bounds on the probability that the sum of random variables deviates from its mean. In the simplest case, let X1, ..., Xn be independent Bernoulli random variables taking values +1 and −1 with probability 1/2, then for every positive 
 
 
 
 ,



Efron–Stein inequality
The Efron–Stein inequality (or influence inequality, or MG bound on variance) bounds the variance of a general function.
Suppose that 
 
 
 
 , 
 
 
 
  are independent with 
 
 
 
  and 
 
 
 
  having the same distribution for all 
 
 
 
 .
Let 
 
 
 
  Then
$$\mathrm{Var}(f(X)) \leq \frac{1}{2} \sum_{i=1}^{n} E[(f(X)-f(X^{(i)}))^2].$$
References
"
Category:Inequality



↩
Wassily Hoeffding, Probability inequalities for sums of bounded random variables, Journal of the American Statistical Association 58 (301): 13–30, March 1963. (JSTOR)↩
↩
↩
↩




