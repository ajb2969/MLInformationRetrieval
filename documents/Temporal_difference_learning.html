<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="40">Temporal difference learning</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Temporal difference learning</h1>
<hr>'''Temporal difference (TD) learning''' is a prediction-based [[machine learning]] method. It has primarily been used for the [[reinforcement learning]] problem, and is said to b
<p>e "a combination of <a href="Monte_Carlo_method" title="wikilink">Monte Carlo</a> ideas and <a href="dynamic_programming" title="wikilink">dynamic programming</a> (DP) ideas."<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> TD resembles a <a href="Monte_Carlo_method" title="wikilink">Monte Carlo method</a> because it learns by <a href="Sampling_(statistics)" title="wikilink">sampling</a> the environment according to some <em>policy</em>, and is related to <a href="dynamic_programming" title="wikilink">dynamic programming</a> techniques as it approximates its current estimate based on previously learned estimates (a process known as <a href="Bootstrapping_(machine_learning)" title="wikilink">bootstrapping</a>). The TD learning algorithm is related to the temporal difference model of animal learning.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>As a prediction method, TD learning takes into account the fact that subsequent predictions are often correlated in some sense. In standard supervised predictive learning, one learns only from actually observed values: A prediction is made, and when the observation is available, the prediction is adjusted to better match the observation. As elucidated by Richard Sutton, the core idea of TD learning is that we adjust predictions to match other, more accurate, predictions about the future.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> This procedure is a form of bootstrapping, as illustrated with the following example:</p>
<dl>
<dd>Suppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday - and thus be able to change, say, Monday's model before Saturday arrives.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a>
</dd>
</dl>

<p>Mathematically speaking, both in a standard and a TD approach, we would try to optimize some cost function, related to the error in our predictions of the expectation of some random variable, E[z]. However, while in the standard approach we in some sense assume E[z] = z (the actual observed value), in the TD approach we use a model. For the particular case of reinforcement learning, which is the major application of TD methods, z is the total return and E[z] is given by the <a href="Bellman_equation" title="wikilink">Bellman equation</a> of the return.</p>
<h2 id="mathematical-formulation">Mathematical formulation</h2>

<p>Let 

<math display="inline" id="Temporal_difference_learning:0">
 <semantics>
  <msub>
   <mi>r</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>r</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r_{t}
  </annotation>
 </semantics>
</math>

 be the reinforcement on time step <em>t</em>. Let 

<math display="inline" id="Temporal_difference_learning:1">
 <semantics>
  <msub>
   <mover accent="true">
    <mi>V</mi>
    <mo stretchy="false">¯</mo>
   </mover>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <ci>normal-¯</ci>
     <ci>V</ci>
    </apply>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{V}_{t}
  </annotation>
 </semantics>
</math>

 be the correct prediction that is equal to the discounted sum of all future reinforcement. The discounting is done by powers of factor of 

<math display="inline" id="Temporal_difference_learning:2">
 <semantics>
  <mi>γ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>γ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma
  </annotation>
 </semantics>
</math>

 such that reinforcement at distant time step is less important.</p>

<p>

<math display="block" id="Temporal_difference_learning:3">
 <semantics>
  <mrow>
   <msub>
    <mover accent="true">
     <mi>V</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mi>t</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <munderover>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>0</mn>
     </mrow>
     <mi mathvariant="normal">∞</mi>
    </munderover>
    <mrow>
     <msup>
      <mi>γ</mi>
      <mi>i</mi>
     </msup>
     <msub>
      <mi>r</mi>
      <mrow>
       <mi>t</mi>
       <mo>+</mo>
       <mi>i</mi>
      </mrow>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <ci>normal-¯</ci>
      <ci>V</ci>
     </apply>
     <ci>t</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">0</cn>
       </apply>
      </apply>
      <infinity></infinity>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>γ</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>r</ci>
       <apply>
        <plus></plus>
        <ci>t</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{V}_{t}=\sum_{i=0}^{\infty}\gamma^{i}r_{t+i}
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Temporal_difference_learning:4">
 <semantics>
  <mrow>
   <mn>0</mn>
   <mo>≤</mo>
   <mi>γ</mi>
   <mo><</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <leq></leq>
     <cn type="integer">0</cn>
     <ci>γ</ci>
    </apply>
    <apply>
     <lt></lt>
     <share href="#.cmml">
     </share>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   0\leq\gamma<1
  </annotation>
 </semantics>
</math>

. This formula can be expanded</p>

<p>

<math display="block" id="Temporal_difference_learning:5">
 <semantics>
  <mrow>
   <msub>
    <mover accent="true">
     <mi>V</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mi>t</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>r</mi>
     <mi>t</mi>
    </msub>
    <mo>+</mo>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi mathvariant="normal">∞</mi>
     </munderover>
     <mrow>
      <msup>
       <mi>γ</mi>
       <mi>i</mi>
      </msup>
      <msub>
       <mi>r</mi>
       <mrow>
        <mi>t</mi>
        <mo>+</mo>
        <mi>i</mi>
       </mrow>
      </msub>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <ci>normal-¯</ci>
      <ci>V</ci>
     </apply>
     <ci>t</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>r</ci>
      <ci>t</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <infinity></infinity>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>γ</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>r</ci>
        <apply>
         <plus></plus>
         <ci>t</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{V}_{t}=r_{t}+\sum_{i=1}^{\infty}\gamma^{i}r_{t+i}
  </annotation>
 </semantics>
</math>

 by changing the index of i to start from 0.</p>

<p>

<math display="block" id="Temporal_difference_learning:6">
 <semantics>
  <mrow>
   <msub>
    <mover accent="true">
     <mi>V</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mi>t</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>r</mi>
     <mi>t</mi>
    </msub>
    <mo>+</mo>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>0</mn>
      </mrow>
      <mi mathvariant="normal">∞</mi>
     </munderover>
     <mrow>
      <msup>
       <mi>γ</mi>
       <mrow>
        <mi>i</mi>
        <mo>+</mo>
        <mn>1</mn>
       </mrow>
      </msup>
      <msub>
       <mi>r</mi>
       <mrow>
        <mi>t</mi>
        <mo>+</mo>
        <mi>i</mi>
        <mo>+</mo>
        <mn>1</mn>
       </mrow>
      </msub>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <ci>normal-¯</ci>
      <ci>V</ci>
     </apply>
     <ci>t</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>r</ci>
      <ci>t</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">0</cn>
        </apply>
       </apply>
       <infinity></infinity>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>γ</ci>
        <apply>
         <plus></plus>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>r</ci>
        <apply>
         <plus></plus>
         <ci>t</ci>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{V}_{t}=r_{t}+\sum_{i=0}^{\infty}\gamma^{i+1}r_{t+i+1}
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="block" id="Temporal_difference_learning:7">
 <semantics>
  <mrow>
   <msub>
    <mover accent="true">
     <mi>V</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mi>t</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>r</mi>
     <mi>t</mi>
    </msub>
    <mo>+</mo>
    <mrow>
     <mi>γ</mi>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>0</mn>
       </mrow>
       <mi mathvariant="normal">∞</mi>
      </munderover>
      <mrow>
       <msup>
        <mi>γ</mi>
        <mi>i</mi>
       </msup>
       <msub>
        <mi>r</mi>
        <mrow>
         <mi>t</mi>
         <mo>+</mo>
         <mi>i</mi>
         <mo>+</mo>
         <mn>1</mn>
        </mrow>
       </msub>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <ci>normal-¯</ci>
      <ci>V</ci>
     </apply>
     <ci>t</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>r</ci>
      <ci>t</ci>
     </apply>
     <apply>
      <times></times>
      <ci>γ</ci>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>i</ci>
          <cn type="integer">0</cn>
         </apply>
        </apply>
        <infinity></infinity>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>γ</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>r</ci>
         <apply>
          <plus></plus>
          <ci>t</ci>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{V}_{t}=r_{t}+\gamma\sum_{i=0}^{\infty}\gamma^{i}r_{t+i+1}
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="block" id="Temporal_difference_learning:8">
 <semantics>
  <mrow>
   <msub>
    <mover accent="true">
     <mi>V</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mi>t</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>r</mi>
     <mi>t</mi>
    </msub>
    <mo>+</mo>
    <mrow>
     <mi>γ</mi>
     <msub>
      <mover accent="true">
       <mi>V</mi>
       <mo stretchy="false">¯</mo>
      </mover>
      <mrow>
       <mi>t</mi>
       <mo>+</mo>
       <mn>1</mn>
      </mrow>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <ci>normal-¯</ci>
      <ci>V</ci>
     </apply>
     <ci>t</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>r</ci>
      <ci>t</ci>
     </apply>
     <apply>
      <times></times>
      <ci>γ</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-¯</ci>
        <ci>V</ci>
       </apply>
       <apply>
        <plus></plus>
        <ci>t</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{V}_{t}=r_{t}+\gamma\bar{V}_{t+1}
  </annotation>
 </semantics>
</math>

</p>

<p>Thus, the reinforcement is the difference between the ideal prediction and the current prediction.</p>

<p>

<math display="block" id="Temporal_difference_learning:9">
 <semantics>
  <mrow>
   <msub>
    <mi>r</mi>
    <mi>t</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msub>
     <mover accent="true">
      <mi>V</mi>
      <mo stretchy="false">¯</mo>
     </mover>
     <mi>t</mi>
    </msub>
    <mo>-</mo>
    <mrow>
     <mi>γ</mi>
     <msub>
      <mover accent="true">
       <mi>V</mi>
       <mo stretchy="false">¯</mo>
      </mover>
      <mrow>
       <mi>t</mi>
       <mo>+</mo>
       <mn>1</mn>
      </mrow>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>r</ci>
     <ci>t</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <ci>normal-¯</ci>
       <ci>V</ci>
      </apply>
      <ci>t</ci>
     </apply>
     <apply>
      <times></times>
      <ci>γ</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-¯</ci>
        <ci>V</ci>
       </apply>
       <apply>
        <plus></plus>
        <ci>t</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r_{t}=\bar{V}_{t}-\gamma\bar{V}_{t+1}
  </annotation>
 </semantics>
</math>

</p>
<h2 id="td-lambda">TD-Lambda</h2>

<p><strong>TD-Lambda</strong> is a learning algorithm invented by <a href="Richard_S._Sutton" title="wikilink">Richard S. Sutton</a> based on earlier work on temporal difference learning by <a href="Arthur_Samuel" title="wikilink">Arthur Samuel</a>.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> This algorithm was famously applied by <a href="Gerald_Tesauro" title="wikilink">Gerald Tesauro</a> to create <a class="uri" href="TD-Gammon" title="wikilink">TD-Gammon</a>, a program that learned to play the game of <a class="uri" href="backgammon" title="wikilink">backgammon</a> at the level of expert human players.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>

<p>The lambda (

<math display="inline" id="Temporal_difference_learning:10">
 <semantics>
  <mi>λ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>λ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda
  </annotation>
 </semantics>
</math>

) parameter refers to the trace decay parameter, with 

<math display="inline" id="Temporal_difference_learning:11">
 <semantics>
  <mrow>
   <mn>0</mn>
   <mo>≤</mo>
   <mi>λ</mi>
   <mo>≤</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <leq></leq>
     <cn type="integer">0</cn>
     <ci>λ</ci>
    </apply>
    <apply>
     <leq></leq>
     <share href="#.cmml">
     </share>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   0\leq\lambda\leq 1
  </annotation>
 </semantics>
</math>

. Higher settings lead to longer lasting traces; that is, a larger proportion of credit from a reward can be given to more distant states and actions when 

<math display="inline" id="Temporal_difference_learning:12">
 <semantics>
  <mi>λ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>λ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda
  </annotation>
 </semantics>
</math>

 is higher, with 

<math display="inline" id="Temporal_difference_learning:13">
 <semantics>
  <mrow>
   <mi>λ</mi>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>λ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda=1
  </annotation>
 </semantics>
</math>

 producing parallel learning to Monte Carlo RL algorithms.</p>
<h2 id="td-algorithm-in-neuroscience">TD algorithm in neuroscience</h2>

<p>The TD <a class="uri" href="algorithm" title="wikilink">algorithm</a> has also received attention in the field of <a class="uri" href="neuroscience" title="wikilink">neuroscience</a>. Researchers discovered that the firing rate of <a class="uri" href="dopamine" title="wikilink">dopamine</a> <a class="uri" href="neurons" title="wikilink">neurons</a> in the <a href="ventral_tegmental_area" title="wikilink">ventral tegmental area</a> (VTA) and <a href="substantia_nigra" title="wikilink">substantia nigra</a> (SNc) appear to mimic the error function in the algorithm.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> The error function reports back the difference between the estimated reward at any given state or time step and the actual reward received. The larger the error function, the larger the difference between the expected and actual reward. When this is paired with a stimulus that accurately reflects a future reward, the error can be used to associate the stimulus with the future <a href="reward_system" title="wikilink">reward</a>.</p>

<p><a class="uri" href="Dopamine" title="wikilink">Dopamine</a> cells appear to behave in a similar manner. In one experiment measurements of dopamine cells were made while training a monkey to associate a stimulus with the reward of juice.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> Initially the dopamine cells increased firing rates when the monkey received juice, indicating a difference in expected and actual rewards. Over time this increase in firing back propagated to the earliest reliable stimulus for the reward. Once the monkey was fully trained, there was no increase in firing rate upon presentation of the predicted reward. Continually, the firing rate for the dopamine cells decreased below normal activation when the expected reward was not produced. This mimics closely how the error function in TD is used for <a href="reinforcement_learning" title="wikilink">reinforcement learning</a>.</p>

<p>The relationship between the model and potential neurological function has produced research attempting to use TD to explain many aspects of behavioral research.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> It has also been used to study conditions such as <a class="uri" href="schizophrenia" title="wikilink">schizophrenia</a> or the consequences of pharmacological manipulations of dopamine on learning.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Reinforcement_learning" title="wikilink">Reinforcement learning</a></li>
<li><a class="uri" href="Q-learning" title="wikilink">Q-learning</a></li>
<li><a href="State-Action-Reward-State-Action" title="wikilink">SARSA</a></li>
<li><a href="Rescorla-Wagner_model" title="wikilink">Rescorla-Wagner model</a></li>
<li><a class="uri" href="PVLV" title="wikilink">PVLV</a></li>
</ul>
<h2 id="notes">Notes</h2>
<references>
</references>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li>Imran Ghory. <a href="http://www.cs.bris.ac.uk/Publications/Papers/2000100.pdf">Reinforcement Learning in Board Games</a>.</li>
</ul>
<ul>
<li>S. P. Meyn, 2007. <a href="https://netfiles.uiuc.edu/meyn/www/spm_files/CTCN/CTCN.html">Control Techniques for Complex Networks</a>, Cambridge University Press, 2007. See final chapter, and appendix with abridged <a href="https://netfiles.uiuc.edu/meyn/www/spm_files/book.html">Meyn &amp; Tweedie</a>.</li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://scholarpedia.org/article/Temporal_Difference_Learning">Scholarpedia Temporal difference Learning</a></li>
<li><a href="http://webdocs.cs.ualberta.ca/~sutton/book/11/node2.html">TD-Gammon</a></li>
<li><a href="http://rlai.cs.ualberta.ca/TDNets/index.html">TD-Networks Research Group</a></li>
<li><a href="http://pitoko.net/tdgravity">Connect Four TDGravity Applet</a> (+ mobile phone version) - self-learned using TD-Leaf method (combination of TD-Lambda with shallow tree search)</li>
<li><a href="http://chet-weger.herokuapp.com/learn_meta_ttt/">Self Learning Meta-Tic-Tac-Toe</a> Example web app showing how temporal difference learning can be used to learn state evaluation constants for a minimax AI playing a simple board game.</li>
<li><a href="http://www.cs.colorado.edu/~grudic/teaching/CSCI4202/RL.pdf">Reinforcement Learning Problem</a>, document explaining how temporal difference learning can be used to speed up <a class="uri" href="Q-learning" title="wikilink">Q-learning</a></li>
</ul>

<p>"</p>

<p><a href="Category:Computational_neuroscience" title="wikilink">Category:Computational neuroscience</a> <a href="Category:Machine_learning_algorithms" title="wikilink">Category:Machine learning algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"> (A revised version is available on <a href="http://www.cs.ualberta.ca/~sutton/publications.html">Richard Sutton's publication page</a>)<a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
</ol>
</section>
</hr></body>
</html>
