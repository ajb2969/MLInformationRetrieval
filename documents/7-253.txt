   Jackson network      Jackson network  In queueing theory , a discipline within the mathematical theory of probability , a Jackson network (sometimes Jacksonian network 1 ) is a class of queueing network where the equilibrium distribution is particularly simple to compute as the network has a product-form solution . It was the first significant development in the theory of networks of queues , and generalising and applying the ideas of the theorem to search for similar product-form solutions in other networks has been the subject of much research, 2 including ideas used in the development of the Internet. 3 The networks were first identified by James R. Jackson 4 5 and his paper was re-printed in the journal Management Science ’s ‘Ten Most Influential Titles of Management Sciences First Fifty Years.’ 6  Jackson was inspired by the work of Burke and Reich, 7 though Jean Walrand notes "product-form results … [are] a much less immediate result of the output theorem than Jackson himself appeared to believe in his fundamental paper". 8  An earlier product-form solution was found by R. R. P. Jackson for tandem queues (a finite chain of queues where each customer must visit each queue in order) and cyclic networks (a loop of queues where each customer must visit each queue in order). 9  A Jackson network consists of a number of nodes, where each node represents a queue in which the service rate can be both node-dependent and state-dependent. Jobs travel among the nodes following a fixed routing matrix. All jobs at each node belong to a single "class" and jobs follow the same service-time distribution and the same routing mechanism. Consequently, there is no notion of priority in serving the jobs: all jobs at each node are served on a first-come, first-served basis.  Jackson networks where a finite population of jobs travel around a closed network also have a product-form solution described by the Gordon–Newell theorem . 10  Definition  In an open network, jobs arrive from outside following a Poisson process with rate    α  >  0      α  0    \alpha>0   . Each arrival is independently routed to node j with probability     p   0  j    ≥  0       subscript  p    0  j    0    p_{0j}\geq 0   and      ∑   j  =  1   J    p   0  j     =  1        superscript   subscript     j  1    J    subscript  p    0  j     1    \sum_{j=1}^{J}p_{0j}=1   . Upon service completion at node i , a job may go to another node j with probability    p   i  j      subscript  p    i  j     p_{ij}   or leave the network with probability     p   i  0    =   1  -    ∑   j  =  1   J    p   i  j           subscript  p    i  0      1    superscript   subscript     j  1    J    subscript  p    i  j        p_{i0}=1-\sum_{j=1}^{J}p_{ij}   .  Hence we have the overall arrival rate to node i ,    λ  i     subscript  λ  i    \lambda_{i}   , including both external arrivals and internal transitions:       λ  i   =  α   p   0  i    +   ∑   j  =  1   J    λ  j    p   j  i    ,  i  =  1  ,  …  ,  J  .   (  1  )      fragments   subscript  λ  i    α   subscript  p    0  i      superscript   subscript     j  1    J    subscript  λ  j    subscript  p    j  i    normal-,  i   1  normal-,  normal-…  normal-,  J  normal-.  italic-   fragments  normal-(  1  normal-)     \lambda_{i}=\alpha p_{0i}+\sum_{j=1}^{J}\lambda_{j}p_{ji},i=1,\ldots,J.\qquad(1)     Define    a  =    (   α   p   0  i     )    i  =  1   J       a   superscript   subscript    α   subscript  p    0  i       i  1    J     a=(\alpha p_{0i})_{i=1}^{J}   , then we can solve    λ  =     (   I  -   P  ′    )    -  1    a       λ     superscript    I   superscript  P  normal-′      1    a     \lambda=(I-P^{\prime})^{-1}a   .  All jobs leave each node also following Poisson process, and define     μ  i    (   x  i   )        subscript  μ  i    subscript  x  i     \mu_{i}(x_{i})   as the service rate of node i when there are    x  i     subscript  x  i    x_{i}   jobs at node i .  Let     X  i    (  t  )        subscript  X  i   t    X_{i}(t)   denote the number of jobs at node i at time t , and    𝐗  =    (   X  i   )    i  =  1   J       𝐗   superscript   subscript   subscript  X  i     i  1    J     \mathbf{X}=(X_{i})_{i=1}^{J}   . Then the equilibrium distribution of   𝐗   𝐗   \mathbf{X}   ,    π   (  𝐱  )   =  P   (  𝐗  =  𝐱  )      fragments  π   fragments  normal-(  x  normal-)    P   fragments  normal-(  X   x  normal-)     \pi(\mathbf{x})=P(\mathbf{X}=\mathbf{x})   is determined by the following system of balance equations:      π   (  𝐱  )     ∑   i  =  1   J    [    α   p   0  i     +    μ  i    (   x  i   )    (   1  -   p   i  i     )     ]        π  𝐱    superscript   subscript     i  1    J    delimited-[]      α   subscript  p    0  i        subscript  μ  i    subscript  x  i     1   subscript  p    i  i           \pi(\mathbf{x})\sum_{i=1}^{J}[\alpha p_{0i}+\mu_{i}(x_{i})(1-p_{ii})]       =   ∑   i  =  1   J    [  π   (  𝐱  -   𝐞  i   )   α   p   0  i    +  π   (  𝐱  +   𝐞  i   )    μ  i    (   x  i   +  1  )    p   i  0    ]   +   ∑   i  =  1   J    ∑   j  ≠  i    π   (  𝐱  +   𝐞  i   -   𝐞  j   )    μ  i    (   x  i   +  1  )    p   i  j    .   (  2  )      fragments    superscript   subscript     i  1    J    fragments  normal-[  π   fragments  normal-(  x    subscript  𝐞  i   normal-)   α   subscript  p    0  i     π   fragments  normal-(  x    subscript  𝐞  i   normal-)    subscript  μ  i    fragments  normal-(   subscript  x  i    1  normal-)    subscript  p    i  0    normal-]     superscript   subscript     i  1    J    subscript     j  i    π   fragments  normal-(  x    subscript  𝐞  i     subscript  𝐞  j   normal-)    subscript  μ  i    fragments  normal-(   subscript  x  i    1  normal-)    subscript  p    i  j    normal-.  italic-   fragments  normal-(  2  normal-)     =\sum_{i=1}^{J}[\pi(\mathbf{x}-\mathbf{e}_{i})\alpha p_{0i}+\pi(\mathbf{x}+%
 \mathbf{e}_{i})\mu_{i}(x_{i}+1)p_{i0}]+\sum_{i=1}^{J}\sum_{j\neq i}\pi(\mathbf%
 {x}+\mathbf{e}_{i}-\mathbf{e}_{j})\mu_{i}(x_{i}+1)p_{ij}.\qquad(2)     where    𝐞  i     subscript  𝐞  i    \mathbf{e}_{i}   denote the    i   t  h      superscript  i    t  h     i^{th}    unit vector .  Theorem  Suppose a vector of independent random variables    (   Y  1   ,  …  ,   Y  J   )      subscript  Y  1   normal-…   subscript  Y  J     (Y_{1},\ldots,Y_{J})   with each    Y  i     subscript  Y  i    Y_{i}   having a probability mass function as      P   (   Y  i   =  n  )   =  p   (   Y  i   =  0  )   ⋅    λ  i  n     M  i    (  n  )     ,   (  3  )      fragments  P   fragments  normal-(   subscript  Y  i    n  normal-)    p   fragments  normal-(   subscript  Y  i    0  normal-)   normal-⋅     superscript   subscript  λ  i   n      subscript  M  i   n    normal-,   fragments  normal-(  3  normal-)     P(Y_{i}=n)=p(Y_{i}=0)\cdot\frac{\lambda_{i}^{n}}{M_{i}(n)},\quad(3)     where      M  i    (  n  )    =    ∏   j  =  1   n     μ  i    (  j  )            subscript  M  i   n     superscript   subscript  product    j  1    n      subscript  μ  i   j      M_{i}(n)=\prod_{j=1}^{n}\mu_{i}(j)   . If      ∑   n  =  1   ∞     λ  i  n     M  i    (  n  )      <  ∞        superscript   subscript     n  1         superscript   subscript  λ  i   n      subscript  M  i   n        \sum_{n=1}^{\infty}\frac{\lambda_{i}^{n}}{M_{i}(n)}<\infty   i.e.    P   (   Y  i   =  0  )   =    (  1  +   ∑   n  =  1   ∞     λ  i  n     M  i    (  n  )     )    -  1       fragments  P   fragments  normal-(   subscript  Y  i    0  normal-)     superscript   fragments  normal-(  1    superscript   subscript     n  1         superscript   subscript  λ  i   n      subscript  M  i   n    normal-)     1      P(Y_{i}=0)=\left(1+\sum_{n=1}^{\infty}\frac{\lambda_{i}^{n}}{M_{i}(n)}\right)^%
 {-1}   is well defined, then the equilibrium distribution of the open Jackson network has the following product form:      π   (  𝐱  )   =   ∏   i  =  1   J   P   (   Y  i   =   x  i   )   .     fragments  π   fragments  normal-(  x  normal-)     superscript   subscript  product    i  1    J   P   fragments  normal-(   subscript  Y  i     subscript  x  i   normal-)   normal-.    \pi(\mathbf{x})=\prod_{i=1}^{J}P(Y_{i}=x_{i}).     for all    𝐱  ∈   𝒵  +  J       𝐱   superscript   subscript  𝒵    J     \mathbf{x}\in\mathcal{Z}_{+}^{J}   .⟩  It suffices to verify equation    (  2  )    2   (2)   is satisfied. By the product form and formula (3), we have:       π   (  𝐱  )    =    π   (   𝐱  +   𝐞  i    )    μ  i    (    x  i   +  1   )    /   λ  i    =    π   (    𝐱  +   𝐞  i    -   𝐞  j    )    μ  i    (    x  i   +  1   )    λ  j    /   [    λ  i    μ  j    (   x  j   )    ]            π  𝐱       π    𝐱   subscript  𝐞  i     subscript  μ  i      subscript  x  i   1     subscript  λ  i             π      𝐱   subscript  𝐞  i     subscript  𝐞  j     subscript  μ  i      subscript  x  i   1    subscript  λ  j     delimited-[]     subscript  λ  i    subscript  μ  j    subscript  x  j         \pi(\mathbf{x})=\pi(\mathbf{x}+\mathbf{e}_{i})\mu_{i}(x_{i}+1)/\lambda_{i}=\pi%
 (\mathbf{x}+\mathbf{e}_{i}-\mathbf{e}_{j})\mu_{i}(x_{i}+1)\lambda_{j}/[\lambda%
 _{i}\mu_{j}(x_{j})]     Substituting these into the right side of    (  2  )    2   (2)   we get:       ∑   i  =  1   J    [  α   p   0  i    +   μ  i    (   x  i   )    (  1  -   p   i  i    )   ]   =   ∑   i  =  1   J    [    α   p   0  i      λ  i     μ  i    (   x  i   )   +   λ  i    p   i  0    ]   +   ∑   i  =  1   J    ∑   j  ≠  i      λ  i    λ  j     p   i  j     μ  j    (   x  j   )   .   (  4  )      fragments   superscript   subscript     i  1    J    fragments  normal-[  α   subscript  p    0  i      subscript  μ  i    fragments  normal-(   subscript  x  i   normal-)    fragments  normal-(  1    subscript  p    i  i    normal-)   normal-]     superscript   subscript     i  1    J    fragments  normal-[      α   subscript  p    0  i      subscript  λ  i     subscript  μ  i    fragments  normal-(   subscript  x  i   normal-)     subscript  λ  i    subscript  p    i  0    normal-]     superscript   subscript     i  1    J    subscript     j  i       subscript  λ  i    subscript  λ  j     subscript  p    i  j     subscript  μ  j    fragments  normal-(   subscript  x  j   normal-)   normal-.  italic-   fragments  normal-(  4  normal-)     \sum_{i=1}^{J}[\alpha p_{0i}+\mu_{i}(x_{i})(1-p_{ii})]=\sum_{i=1}^{J}[\frac{%
 \alpha p_{0i}}{\lambda_{i}}\mu_{i}(x_{i})+\lambda_{i}p_{i0}]+\sum_{i=1}^{J}%
 \sum_{j\neq i}\frac{\lambda_{i}}{\lambda_{j}}p_{ij}\mu_{j}(x_{j}).\qquad(4)     Then use    (  1  )    1   (1)   , we have:         ∑   i  =  1   J     ∑   j  ≠  i       λ  i    λ  j     p   i  j     μ  j    (   x  j   )      =    ∑   j  =  1   J     [    ∑   i  ≠  j       λ  i    λ  j     p   i  j      ]    μ  j    (   x  j   )     =    ∑   j  =  1   J     [   1  -   p   j  j    -    α   p   0  j      λ  j     ]    μ  j    (   x  j   )      .          superscript   subscript     i  1    J     subscript     j  i         subscript  λ  i    subscript  λ  j     subscript  p    i  j     subscript  μ  j    subscript  x  j        superscript   subscript     j  1    J      delimited-[]    subscript     i  j         subscript  λ  i    subscript  λ  j     subscript  p    i  j        subscript  μ  j    subscript  x  j            superscript   subscript     j  1    J      delimited-[]    1   subscript  p    j  j        α   subscript  p    0  j      subscript  λ  j       subscript  μ  j    subscript  x  j        \sum_{i=1}^{J}\sum_{j\neq i}\frac{\lambda_{i}}{\lambda_{j}}p_{ij}\mu_{j}(x_{j}%
 )=\sum_{j=1}^{J}[\sum_{i\neq j}\frac{\lambda_{i}}{\lambda_{j}}p_{ij}]\mu_{j}(x%
 _{j})=\sum_{j=1}^{J}[1-p_{jj}-\frac{\alpha p_{0j}}{\lambda_{j}}]\mu_{j}(x_{j}).     Substituting the above into    (  4  )    4   (4)   , we have:        ∑   i  =  1   J    α   p   0  i      =    ∑   i  =  1   J     λ  i    p   i  0            superscript   subscript     i  1    J     α   subscript  p    0  i        superscript   subscript     i  1    J      subscript  λ  i    subscript  p    i  0        \sum_{i=1}^{J}\alpha p_{0i}=\sum_{i=1}^{J}\lambda_{i}p_{i0}     This can be verified by      ∑   i  =  1   J    α   p   0  i      =     ∑   i  =  1   J    λ  i    -    ∑   i  =  1   J     ∑   j  =  1   J     λ  j    p   j  i        =     ∑   i  =  1   J    λ  i    -    ∑   j  =  1   J     λ  j    (   1  -   p   j  0     )      =    ∑   i  =  1   J     λ  i    p   i  0              superscript   subscript     i  1    J     α   subscript  p    0  i          superscript   subscript     i  1    J    subscript  λ  i      superscript   subscript     i  1    J     superscript   subscript     j  1    J      subscript  λ  j    subscript  p    j  i                 superscript   subscript     i  1    J    subscript  λ  i      superscript   subscript     j  1    J      subscript  λ  j     1   subscript  p    j  0               superscript   subscript     i  1    J      subscript  λ  i    subscript  p    i  0         \sum_{i=1}^{J}\alpha p_{0i}=\sum_{i=1}^{J}\lambda_{i}-\sum_{i=1}^{J}\sum_{j=1}%
 ^{J}\lambda_{j}p_{ji}=\sum_{i=1}^{J}\lambda_{i}-\sum_{j=1}^{J}\lambda_{j}(1-p_%
 {j0})=\sum_{i=1}^{J}\lambda_{i}p_{i0}   . Hence both side of    (  2  )    2   (2)   are equal.⟨  This theorem extends the one shown on Jackson's Theorem page by allowing state-dependent service rate of each node. It relates the distribution of   𝐗   𝐗   \mathbf{X}   by a vector of independent variable   𝐘   𝐘   \mathbf{Y}   .  Example  thumb|upright=1.5|A three-node open Jackson network Suppose we have a three-node Jackson shown in the graph, the coefficients are:         α  =  5   ,    p  01   =   p  02   =  0.5    ,    p  03   =  0    ,     formulae-sequence   formulae-sequence    α  5        subscript  p  01    subscript  p  02        0.5        subscript  p  03   0     \alpha=5,\quad p_{01}=p_{02}=0.5,\quad p_{03}=0,\quad          P  =   [     0    0.5    0.5      0    0    0      0    0    0     ]    ,   μ  =   [       μ  1    (   x  1   )          μ  2    (   x  2   )          μ  3    (   x  3   )       ]   =    [     15      12      10     ]   for all   x  i    >  0      formulae-sequence    P    0  0.5  0.5    0  0  0    0  0  0         μ       subscript  μ  1    subscript  x  1         subscript  μ  2    subscript  x  2         subscript  μ  3    subscript  x  3               15    12    10    for all   subscript  x  i         0      P=\begin{bmatrix}0&0.5&0.5\\
 0&0&0\\
 0&0&0\end{bmatrix},\quad\mu=\begin{bmatrix}\mu_{1}(x_{1})\\
 \mu_{2}(x_{2})\\
 \mu_{3}(x_{3})\end{bmatrix}=\begin{bmatrix}15\\
 12\\
 10\end{bmatrix}\text{ for all }x_{i}>0     Then by the theorem, we can calculate:      λ  =     (   I  -   P  ′    )    -  1    a   =     [     1    0    0       -  0.5     1    0       -  0.5     0    1     ]    -  1     [      0.5  ×  5        0.5  ×  5           ]    =    [     1    0    0      0.5    1    0      0.5    0    1     ]    [     2.5      2.5      0     ]    =   [     2.5      3.75      1.25     ]         λ     superscript    I   superscript  P  normal-′      1    a           superscript    1  0  0      0.5   1  0      0.5   0  1      1        0.5  5       0.5  5     absent              1  0  0    0.5  1  0    0.5  0  1      2.5    2.5    0            2.5    3.75    1.25       \lambda=(I-P^{\prime})^{-1}a=\begin{bmatrix}1&0&0\\
 -0.5&1&0\\
 -0.5&0&1\end{bmatrix}^{-1}\begin{bmatrix}0.5\times 5\\
 0.5\times 5\\
 \par
 \end{bmatrix}=\begin{bmatrix}1&0&0\\
 0.5&1&0\\
 0.5&0&1\end{bmatrix}\begin{bmatrix}2.5\\
 2.5\\
 0\end{bmatrix}=\begin{bmatrix}2.5\\
 3.75\\
 1.25\end{bmatrix}     According to the definition of   𝐘   𝐘   \mathbf{Y}   , we have:      P   (   Y  1   =  0  )   =    (  1  +   ∑   n  =  1   ∞     (   2.5  15   )   n   )    -  1    =   5  6      fragments  P   fragments  normal-(   subscript  Y  1    0  normal-)     superscript   fragments  normal-(  1    superscript   subscript     n  1       superscript   fragments  normal-(    2.5  15   normal-)   n   normal-)     1       5  6     P(Y_{1}=0)=\left(1+\sum_{n=1}^{\infty}\left(\frac{2.5}{15}\right)^{n}\right)^{%
 -1}=\frac{5}{6}         P   (   Y  2   =  0  )   =    (  1  +   ∑   n  =  1   ∞     (   3.75  12   )   n   )    -  1    =   11  16      fragments  P   fragments  normal-(   subscript  Y  2    0  normal-)     superscript   fragments  normal-(  1    superscript   subscript     n  1       superscript   fragments  normal-(    3.75  12   normal-)   n   normal-)     1       11  16     P(Y_{2}=0)=\left(1+\sum_{n=1}^{\infty}\left(\frac{3.75}{12}\right)^{n}\right)^%
 {-1}=\frac{11}{16}         P   (   Y  3   =  0  )   =    (  1  +   ∑   n  =  1   ∞     (   1.25  10   )   n   )    -  1    =   7  8      fragments  P   fragments  normal-(   subscript  Y  3    0  normal-)     superscript   fragments  normal-(  1    superscript   subscript     n  1       superscript   fragments  normal-(    1.25  10   normal-)   n   normal-)     1       7  8     P(Y_{3}=0)=\left(1+\sum_{n=1}^{\infty}\left(\frac{1.25}{10}\right)^{n}\right)^%
 {-1}=\frac{7}{8}     Hence the probability that there is one job at each node is:       π   (  1  ,  1  ,  1  )    =    5  6   ⋅   2.5  15   ⋅   11  16   ⋅   3.75  12   ⋅   7  8   ⋅   1.25  10    ≈  0.00326          π   1  1  1     normal-⋅    5  6     2.5  15     11  16     3.75  12     7  8     1.25  10         0.00326     \pi(1,1,1)=\frac{5}{6}\cdot\frac{2.5}{15}\cdot\frac{11}{16}\cdot\frac{3.75}{12%
 }\cdot\frac{7}{8}\cdot\frac{1.25}{10}\approx 0.00326     Since the service rate here does not depend on state, the    Y  i     subscript  Y  i    Y_{i}   s simply follow a geometric distribution .  Generalized Jackson network  A generalized Jackson network allows renewal arrival processes that need not be Poisson processes, and independent, identically distributed non-exponential service times. In general, this network does not have a product-form stationary distribution , so approximations are sought. 11  Brownian approximation  Under some mild conditions the queue-length process    Q   (  t  )       Q  t    Q(t)   of an open generalized Jackson network can be approximated by a reflected Brownian motion defined as     R  B   M   Q   (  0  )      (  θ  ,  Γ  ;  R  )    .      R  B   subscript  M    Q  0     θ  normal-Γ  R     RBM_{Q(0)}(\theta,\Gamma;R).   , where   θ   θ   \theta   is the drift of the process,   Γ   normal-Γ   \Gamma   is the covariance matrix, and   R   R   R   is the reflection matrix. This is a two-order approximation obtained by relation between general Jackson network with homogeneous fluid network and reflected Brownian motion.  The parameters of the reflected Brownian process is specified as follows:      θ  =   α  -    (   I  -   P  ′    )   μ        θ    α      I   superscript  P  normal-′    μ      \theta=\alpha-(I-P^{\prime})\mu         Γ  =   (   Γ   k  l    )       normal-Γ   subscript  normal-Γ    k  l      \Gamma=(\Gamma_{kl})   with     Γ   k  l    =     ∑   j  =  1   J     (    λ  j   ∧   μ  j    )    [     p   j  k     (    δ   k  l    -   p   j  l     )    +    c  j  2    (    p   j  k    -   δ   j  k     )    (    p   j  l    -   δ   j  l     )     ]     +    α  k    c   0  ,  k   2    δ   k  l           subscript  normal-Γ    k  l        superscript   subscript     j  1    J        subscript  λ  j    subscript  μ  j     delimited-[]       subscript  p    j  k       subscript  δ    k  l     subscript  p    j  l         superscript   subscript  c  j   2      subscript  p    j  k     subscript  δ    j  k        subscript  p    j  l     subscript  δ    j  l             subscript  α  k    superscript   subscript  c   0  k    2    subscript  δ    k  l        \Gamma_{kl}=\sum_{j=1}^{J}(\lambda_{j}\wedge\mu_{j})[p_{jk}(\delta_{kl}-p_{jl}%
 )+c_{j}^{2}(p_{jk}-\delta_{jk})(p_{jl}-\delta_{jl})]+\alpha_{k}c_{0,k}^{2}%
 \delta_{kl}         R  =   I  -   P  ′        R    I   superscript  P  normal-′      R=I-P^{\prime}     where the symbols are defined as:      Definitions of symbols in the approximation formula   symbol   Meaning         α  =    (   α  j   )    j  =  1   J       α   superscript   subscript   subscript  α  j     j  1    J     \alpha=(\alpha_{j})_{j=1}^{J}      a J-vector specifying the arrival rates to each node.         μ  =    (  μ  )    j  =  1   J       μ   superscript   subscript  μ    j  1    J     \mu=(\mu)_{j=1}^{J}      a J-vector specifying the service rates of each node.        P   P   P      routing matrix.         λ  j     subscript  λ  j    \lambda_{j}      effective arrival of    j   t  h      superscript  j    t  h     j^{th}   node.         c  j     subscript  c  j    c_{j}      variation of service time at    j   t  h      superscript  j    t  h     j^{th}   node.         c   0  ,  j      subscript  c   0  j     c_{0,j}      variation of inter-arrival time at    j   t  h      superscript  j    t  h     j^{th}   node.         δ   i  j      subscript  δ    i  j     \delta_{ij}      coefficients to specify correlation between nodes.{{hidden begin   toggle = left}} They are defined in this way: Let    A   (  t  )       A  t    A(t)   be the arrival process of the system, then      A   (  t  )    -   α  t    ≈    A  ^    (  t  )            A  t     α  t       normal-^  A   t     A(t)-\alpha t{}\approx\hat{A}(t)   in distribution, where     A  ^    (  t  )        normal-^  A   t    \hat{A}(t)   is a driftless Brownian process with covariate matrix     Γ  0   =   (   Γ   i  j   0   )        superscript  normal-Γ  0    subscript   superscript  normal-Γ  0     i  j      \Gamma^{0}=(\Gamma^{0}_{ij})   , with     Γ   i  j   0   =    α  i    c   0  ,  i   2    δ   i  j          subscript   superscript  normal-Γ  0     i  j       subscript  α  i    superscript   subscript  c   0  i    2    subscript  δ    i  j       \Gamma^{0}_{ij}=\alpha_{i}c_{0,i}^{2}\delta_{ij}   , for any     i  ,  j   ∈   {  1  ,  …  ,  J  }        i  j    1  normal-…  J     i,j\in\{1,\dots,J\}        References  "  Category:Stochastic processes  Category:Queueing theory     ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩     