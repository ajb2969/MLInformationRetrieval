   Coupling from the past      Coupling from the past   Among Markov chain Monte Carlo (MCMC) algorithms , coupling from the past is a method for sampling from the stationary distribution of a Markov chain . Contrary to many MCMC algorithms, coupling from the past gives in principle a perfect sample from the stationary distribution . It was invented by James Propp and David Wilson in 1996.  The basic idea  Consider a finite state irreducible  aperiodic Markov chain   M   M   M   with state space   S   S   S   and (unique) stationary distribution   π   π   \pi   (   π   π   \pi   is a probability vector). Suppose that we come up with a probability distribution   μ   μ   \mu   on the set of maps    f  :   S  →  S      normal-:  f   normal-→  S  S     f:S\to S   with the property that for every fixed    s  ∈  S      s  S    s\in S   , its image    f   (  s  )       f  s    f(s)   is distributed according to the transition probability of   M   M   M   from state   s   s   s   . An example of such a probability distribution is the one where    f   (  s  )       f  s    f(s)   is independent from    f   (   s  ′   )       f   superscript  s  normal-′     f(s^{\prime})   whenever    s  ≠   s  ′       s   superscript  s  normal-′     s\neq s^{\prime}   , but it is often worthwhile to consider other distributions. Now let    f  j     subscript  f  j    f_{j}   for    j  ∈  ℤ      j  ℤ    j\in\mathbb{Z}   be independent samples from   μ   μ   \mu   .  Suppose that   x   x   x   is chosen randomly according to   π   π   \pi   and is independent from the sequence    f  j     subscript  f  j    f_{j}   . (We do not worry for now where this   x   x   x   is coming from.) Then     f   -  1     (  x  )        subscript  f    1    x    f_{-1}(x)   is also distributed according to   π   π   \pi   , because   π   π   \pi   is   M   M   M   -stationary and our assumption on the law of   f   f   f   . Define        F  j   :=    f   -  1    ∘   f   -  2    ∘  ⋯  ∘   f   -  j      .     assign   subscript  F  j      subscript  f    1     subscript  f    2    normal-⋯   subscript  f    j       F_{j}:=f_{-1}\circ f_{-2}\circ\cdots\circ f_{-j}.   Then it follows by induction that     F  j    (  x  )        subscript  F  j   x    F_{j}(x)   is also distributed according to   π   π   \pi   for every    j  ∈  ℕ      j  ℕ    j\in\mathbb{N}   . Now here is the main point. It may happen that for some    n  ∈  ℕ      n  ℕ    n\in\mathbb{N}   the image of the map    F  n     subscript  F  n    F_{n}   is a single element of   S   S   S   . In other words,      F  n    (  x  )    =    F  n    (  y  )           subscript  F  n   x      subscript  F  n   y     F_{n}(x)=F_{n}(y)   for each    y  ∈  S      y  S    y\in S   . Therefore, we do not need to have access to   x   x   x   in order to compute     F  n    (  x  )        subscript  F  n   x    F_{n}(x)   . The algorithm then involves finding some    n  ∈  ℕ      n  ℕ    n\in\mathbb{N}   such that     F  n    (  S  )        subscript  F  n   S    F_{n}(S)   is a singleton , and outputing the element of that singleton. The design of a good distribution   μ   μ   \mu   for which the task of finding such an   n   n   n   and computing    F  n     subscript  F  n    F_{n}   is not too costly is not always obvious, but has been accomplished successfully in several important instances.  The monotone case  There is a special class of Markov chains in which there are particularly good choices for   μ   μ   \mu   and a tool for determining if     |    F  n    (  S  )    |   =  1           subscript  F  n   S    1    |F_{n}(S)|=1   . (Here    |  ⋅  |     fragments  normal-|  normal-⋅  normal-|    |\cdot|   denotes cardinality .) Suppose that   S   S   S   is a partially ordered set with order   ≤     \leq   , which has a unique minimal element    s  0     subscript  s  0    s_{0}   and a unique maximal element    s  1     subscript  s  1    s_{1}   ; that is, every    s  ∈  S      s  S    s\in S   satisfies     s  0   ≤  s  ≤   s  1          subscript  s  0   s        subscript  s  1      s_{0}\leq s\leq s_{1}   . Also, suppose that   μ   μ   \mu   may be chosen to be supported on the set of monotone maps    f  :   S  →  S      normal-:  f   normal-→  S  S     f:S\to S   . Then it is easy to see that     |    F  n    (  S  )    |   =  1           subscript  F  n   S    1    |F_{n}(S)|=1   if and only if      F  n    (   s  0   )    =    F  n    (   s  1   )           subscript  F  n    subscript  s  0       subscript  F  n    subscript  s  1      F_{n}(s_{0})=F_{n}(s_{1})   , since    F  n     subscript  F  n    F_{n}   is monotone. Thus, checking this becomes rather easy. The algorithm can proceed by choosing    n  :=   n  0      assign  n   subscript  n  0     n:=n_{0}   for some constant    n  0     subscript  n  0    n_{0}   , sampling the maps     f   -  1    ,  …  ,   f   -  n        subscript  f    1    normal-…   subscript  f    n      f_{-1},\dots,f_{-n}   , and outputing     F  n    (   s  0   )        subscript  F  n    subscript  s  0     F_{n}(s_{0})   if      F  n    (   s  0   )    =    F  n    (   s  1   )           subscript  F  n    subscript  s  0       subscript  F  n    subscript  s  1      F_{n}(s_{0})=F_{n}(s_{1})   . If      F  n    (   s  0   )    ≠    F  n    (   s  1   )           subscript  F  n    subscript  s  0       subscript  F  n    subscript  s  1      F_{n}(s_{0})\neq F_{n}(s_{1})   the algorithm proceeds by doubling   n   n   n   and repeating as necessary until an output is obtained. (But the algorithm does not resample the maps    f   -  j      subscript  f    j     f_{-j}   which were already sampled; it uses the previously sampled maps when needed.)  References      "  Category:Monte Carlo methods  Category:Markov chain Monte Carlo   