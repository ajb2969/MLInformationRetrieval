


Pinsker's inequality




Pinsker's inequality

In information theory, Pinsker's inequality, named after its inventor Mark Semenovich Pinsker, is an inequality that bounds the total variation distance (or statistical distance) in terms of the Kullback–Leibler divergence. The inequality is tight up to constant factors.1
Formal statement
Pinsker's inequality states that, if P and Q are two probability distributions, then



where



is the total variation distance (or statistical distance) between P and Q and



is the Kullback–Leibler divergence in nats.
History
Pinsker first proved the inequality with a worse constant. The inequality in the above form was proved independently by Kullback, Csiszár, and Kemperman.2
Inverse problem
An inverse of the inequality cannot hold: for every 
 
 
 
 , there are distributions with 
 
 
 
  but 
 
 
 
 .3
References
Additional reading

Thomas M. Cover and Joy A. Thomas: Elements of Information Theory, 2nd edition, Willey-Interscience, 2006
Nicolo Cesa-Bianchi and Gábor Lugosi: Prediction, Learning, and Games, Cambridge University Press, 2006

"
Category:Information theory Category:Probabilistic inequalities



↩
↩
The divergence becomes infinite whenever one of the two distributions assigns probability zero to an event while the other assigns it a nonzero probability (no matter how small); see e.g. .↩




