   Pinsker's inequality      Pinsker's inequality   In information theory , Pinsker's inequality , named after its inventor Mark Semenovich Pinsker , is an inequality that bounds the total variation distance (or statistical distance) in terms of the Kullback–Leibler divergence . The inequality is tight up to constant factors. 1  Formal statement  Pinsker's inequality states that, if P and Q are two probability distributions , then       δ   (  P  ,  Q  )    ≤     1  2    D  KL    (  P  ∥  Q  )           δ   P  Q       fragments    1  2    subscript  D  KL    fragments  normal-(  P  parallel-to  Q  normal-)       \delta(P,Q)\leq\sqrt{\frac{1}{2}D_{\mathrm{KL}}(P\|Q)}     where       δ   (  P  ,  Q  )    =   sup   {   |    P   (  A  )    -   Q   (  A  )     |   :   A  is an event to which probabilities are assigned.   }          δ   P  Q     supremum   conditional-set        P  A     Q  A       A  is an event to which probabilities are assigned.       \delta(P,Q)=\sup\{|P(A)-Q(A)|:A\text{ is an event to which probabilities are %
 assigned.}\}     is the total variation distance (or statistical distance) between P and Q and       D  KL    (  P  ∥  Q  )   =   ∑  i   ln   (    P   (  i  )     Q   (  i  )     )   P   (  i  )      fragments   subscript  D  KL    fragments  normal-(  P  parallel-to  Q  normal-)     subscript   i     fragments  normal-(      P  i     Q  i    normal-)   P   fragments  normal-(  i  normal-)     D_{\mathrm{KL}}(P\|Q)=\sum_{i}\ln\left(\frac{P(i)}{Q(i)}\right)P(i)\!     is the Kullback–Leibler divergence in nats .  History  Pinsker first proved the inequality with a worse constant. The inequality in the above form was proved independently by Kullback , Csiszár , and Kemperman . 2  Inverse problem  An inverse of the inequality cannot hold: for every    ϵ  >  0      ϵ  0    \epsilon>0   , there are distributions with     δ   (  P  ,  Q  )    ≤  ϵ        δ   P  Q    ϵ    \delta(P,Q)\leq\epsilon   but     D  KL    (  P  ∥  Q  )   =  ∞     fragments   subscript  D  KL    fragments  normal-(  P  parallel-to  Q  normal-)       D_{\mathrm{KL}}(P\|Q)=\infty   . 3  References  Additional reading   Thomas M. Cover and Joy A. Thomas: Elements of Information Theory , 2nd edition, Willey-Interscience, 2006  Nicolo Cesa-Bianchi and Gábor Lugosi: Prediction, Learning, and Games , Cambridge University Press, 2006   "  Category:Information theory  Category:Probabilistic inequalities     ↩  ↩  The divergence becomes infinite whenever one of the two distributions assigns probability zero to an event while the other assigns it a nonzero probability (no matter how small); see e.g. . ↩     