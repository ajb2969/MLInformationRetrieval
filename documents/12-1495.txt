   Structured support vector machine      Structured support vector machine   The structured support vector machine is a machine learning algorithm that generalizes the Support Vector Machine (SVM) classifier. Whereas the SVM classifier supports binary classification , multiclass classification and regression , the structured SVM allows training of a classifier for general structured output labels .  As an example, a sample instance might be a natural language sentence, and the output label is an annotated parse tree . Training a classifier consists of showing pairs of correct sample and output label pairs. After training, the structured SVM model allows one to predict for new sample instances the corresponding output label; that is, given a natural language sentence, the classifier can produce the most likely parse tree.  Training  For a set of   ℓ   normal-ℓ   \ell   training instances     (   𝒙  n   ,   y  n   )   ∈   𝒳  ×  𝒴         subscript  𝒙  n    subscript  y  n      𝒳  𝒴     (\boldsymbol{x}_{n},y_{n})\in\mathcal{X}\times\mathcal{Y}   ,    n  =   1  ,  …  ,  ℓ       n   1  normal-…  normal-ℓ     n=1,\dots,\ell   from a sample space   𝒳   𝒳   \mathcal{X}   and label space   𝒴   𝒴   \mathcal{Y}   , the structured SVM minimizes the following regularized risk function.       min  𝒘      ∥  𝒘  ∥   2   +   C    ∑   n  =  1   ℓ     max   y  ∈  𝒴     (     Δ   (   y  n   ,  y  )    +    𝒘  ′   Ψ   (   𝒙  n   ,  y  )     -    𝒘  ′   Ψ   (   𝒙  n   ,   y  n   )     )           𝒘       superscript   norm  𝒘   2     C    superscript   subscript     n  1    normal-ℓ        y  𝒴           normal-Δ    subscript  y  n   y       superscript  𝒘  normal-′   normal-Ψ    subscript  𝒙  n   y        superscript  𝒘  normal-′   normal-Ψ    subscript  𝒙  n    subscript  y  n            \underset{\boldsymbol{w}}{\min}\quad\|\boldsymbol{w}\|^{2}+C\sum_{n=1}^{\ell}%
 \underset{y\in\mathcal{Y}}{\max}\left(\Delta(y_{n},y)+\boldsymbol{w}^{\prime}%
 \Psi(\boldsymbol{x}_{n},y)-\boldsymbol{w}^{\prime}\Psi(\boldsymbol{x}_{n},y_{n%
 })\right)   The function is convex in   𝒘   𝒘   \boldsymbol{w}   because the maximum of a set of affine functions is convex. The function    Δ  :    𝒴  ×  𝒴   →   ℝ  +       normal-:  normal-Δ   normal-→    𝒴  𝒴    subscript  ℝ       \Delta:\mathcal{Y}\times\mathcal{Y}\to\mathbb{R}_{+}   measures a distance in label space and is an arbitrary function (not necessarily a metric ) satisfying     Δ   (  y  ,  z  )    ≥  0        normal-Δ   y  z    0    \Delta(y,z)\geq 0   and      Δ   (  y  ,  y  )    =    0    ∀  y     ,   z  ∈  𝒴      formulae-sequence      normal-Δ   y  y      0   for-all  y       z  𝒴     \Delta(y,y)=0\;\;\forall y,z\in\mathcal{Y}   . The function    Ψ  :    𝒳  ×  𝒴   →   ℝ  d       normal-:  normal-Ψ   normal-→    𝒳  𝒴    superscript  ℝ  d      \Psi:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}^{d}   is a feature function, extracting some feature vector from a given sample and label. The design of this function depends very much on the application.  Because the regularized risk function above is non-differentiable, it is often reformulated in terms of a quadratic program by introducing one slack variable    ξ  n     subscript  ξ  n    \xi_{n}   for each sample, each representing the value of the maximum. The standard structured SVM primal formulation is given as follows.         min   𝒘  ,  𝝃         ∥  𝒘  ∥   2   +   C    ∑   n  =  1   ℓ    ξ  n          s.t.          𝒘  ′   Ψ   (   𝒙  n   ,   y  n   )    -    𝒘  ′   Ψ   (   𝒙  n   ,  y  )     +   ξ  n    ≥   Δ   (   y  n   ,  y  )     ,    n  =   1  ,  …  ,  ℓ    ,    ∀  y   ∈  𝒴             𝒘  𝝃        superscript   norm  𝒘   2     C    superscript   subscript     n  1    normal-ℓ    subscript  ξ  n        s.t.   formulae-sequence           superscript  𝒘  normal-′   normal-Ψ    subscript  𝒙  n    subscript  y  n        superscript  𝒘  normal-′   normal-Ψ    subscript  𝒙  n   y      subscript  ξ  n      normal-Δ    subscript  y  n   y      formulae-sequence    n   1  normal-…  normal-ℓ       for-all  y   𝒴        \begin{array}[]{cl}\underset{\boldsymbol{w},\boldsymbol{\xi}}{\min}&\|%
 \boldsymbol{w}\|^{2}+C\sum_{n=1}^{\ell}\xi_{n}\\
 \textrm{s.t.}&\boldsymbol{w}^{\prime}\Psi(\boldsymbol{x}_{n},y_{n})-%
 \boldsymbol{w}^{\prime}\Psi(\boldsymbol{x}_{n},y)+\xi_{n}\geq\Delta(y_{n},y),%
 \qquad n=1,\dots,\ell,\quad\forall y\in\mathcal{Y}\end{array}     Inference  At test time, only a sample    𝒙  ∈  𝒳      𝒙  𝒳    \boldsymbol{x}\in\mathcal{X}   is known, and a prediction function    f  :   𝒳  →  𝒴      normal-:  f   normal-→  𝒳  𝒴     f:\mathcal{X}\to\mathcal{Y}   maps it to a predicted label from the label space   𝒴   𝒴   \mathcal{Y}   . For structured SVMs, given the vector   𝒘   𝒘   \boldsymbol{w}   obtained from training, the prediction function is the following.       f   (  𝒙  )    =    argmax   y  ∈  𝒴      𝒘  ′   Ψ   (  𝒙  ,  y  )           f  𝒙       y  𝒴   argmax      superscript  𝒘  normal-′   normal-Ψ   𝒙  y       f(\boldsymbol{x})=\underset{y\in\mathcal{Y}}{\textrm{argmax}}\quad\boldsymbol{%
 w}^{\prime}\Psi(\boldsymbol{x},y)     Therefore, the maximizer over the label space is the predicted label. Solving for this maximizer is the so-called inference problem and similar to making a maximum a-posteriori (MAP) prediction in probabilistic models. Depending on the structure of the function   Ψ   normal-Ψ   \Psi   , solving for the maximizer can be a hard problem.  Separation  The above quadratic program involves a very large, possibly infinite number of linear inequality constraints. In general, the number of inequalities is too large to be optimized over explicitly. Instead the problem is solved by using delayed constraint generation where only a finite and small subset of the constraints is used. Optimizing over a subset of the constraints enlarges the feasible set and will yield a solution which provides a lower bound on the objective. To test whether the solution   𝒘   𝒘   \boldsymbol{w}   violates constraints of the complete set inequalities, a separation problem needs to be solved. As the inequalities decompose over the samples, for each sample    (   𝒙  n   ,   y  n   )      subscript  𝒙  n    subscript  y  n     (\boldsymbol{x}_{n},y_{n})   the following problem needs to be solved.       y  n  *   =    argmax   y  ∈  𝒴     (     Δ   (   y  n   ,  y  )    +    𝒘  ′   Ψ   (   𝒙  n   ,  y  )     -    𝒘  ′   Ψ   (   𝒙  n   ,   y  n   )    -   ξ  n    )         superscript   subscript  y  n          y  𝒴   argmax         normal-Δ    subscript  y  n   y       superscript  𝒘  normal-′   normal-Ψ    subscript  𝒙  n   y        superscript  𝒘  normal-′   normal-Ψ    subscript  𝒙  n    subscript  y  n      subscript  ξ  n       y_{n}^{*}=\underset{y\in\mathcal{Y}}{\textrm{argmax}}\left(\Delta(y_{n},y)+%
 \boldsymbol{w}^{\prime}\Psi(\boldsymbol{x}_{n},y)-\boldsymbol{w}^{\prime}\Psi(%
 \boldsymbol{x}_{n},y_{n})-\xi_{n}\right)     The right hand side objective to be maximized is composed of the constant     -    𝒘  ′   Ψ   (   𝒙  n   ,   y  n   )     -   ξ  n            superscript  𝒘  normal-′   normal-Ψ    subscript  𝒙  n    subscript  y  n       subscript  ξ  n     -\boldsymbol{w}^{\prime}\Psi(\boldsymbol{x}_{n},y_{n})-\xi_{n}   and a term dependent on the variables optimized over, namely     Δ   (   y  n   ,  y  )    +    𝒘  ′   Ψ   (   𝒙  n   ,  y  )          normal-Δ    subscript  y  n   y       superscript  𝒘  normal-′   normal-Ψ    subscript  𝒙  n   y      \Delta(y_{n},y)+\boldsymbol{w}^{\prime}\Psi(\boldsymbol{x}_{n},y)   . If the achieved right hand side objective is smaller or equal to zero, no violated constraints for this sample exist. If it is strictly larger than zero, the most violated constraint with respect to this sample has been identified. The problem is enlarged by this constraint and resolved. The process continues until no violated inequalities can be identified.  If the constants are dropped from the above problem, we obtain the following problem to be solved.       y  n  *   =    argmax   y  ∈  𝒴     (    Δ   (   y  n   ,  y  )    +    𝒘  ′   Ψ   (   𝒙  n   ,  y  )     )         superscript   subscript  y  n          y  𝒴   argmax       normal-Δ    subscript  y  n   y       superscript  𝒘  normal-′   normal-Ψ    subscript  𝒙  n   y        y_{n}^{*}=\underset{y\in\mathcal{Y}}{\textrm{argmax}}\left(\Delta(y_{n},y)+%
 \boldsymbol{w}^{\prime}\Psi(\boldsymbol{x}_{n},y)\right)     This problem looks very similar to the inference problem. The only difference is the addition of the term    Δ   (   y  n   ,  y  )       normal-Δ    subscript  y  n   y     \Delta(y_{n},y)   . Most often, it is chosen such that it has a natural decomposition in label space. In that case, the influence of   Δ   normal-Δ   \Delta   can be encoded into the inference problem and solving for the most violating constraint is equivalent to solving the inference problem.  References   Ioannis Tsochantaridis, Thorsten Joachims , Thomas Hofmann and Yasemin Altun (2005), Large Margin Methods for Structured and Interdependent Output Variables , JMLR, Vol. 6, pages 1453-1484.  Thomas Finley and Thorsten Joachims (2008), Training Structural SVMs when Exact Inference is Intractable , ICML 2008.  Sunita Sarawagi and Rahul Gupta (2008), Accurate Max-margin Training for Structured Output Spaces , ICML 2008.  Gökhan BakIr, Ben Taskar, Thomas Hofmann, Bernhard Schölkopf, Alex Smola and SVN Vishwanathan (2007), [ http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid; ;=11332 Predicting Structured Data], MIT Press.  Vojtěch Franc and Bogdan Savchynskyy Discriminative Learning of Max-Sum Classifiers , Journal of Machine Learning Research, 9(Jan):67—104, 2008, Microtome Publishing  Kevin Murphy 1 Machine Learning, Mit Press   "  Category:Structured prediction  Category:Support vector machines   