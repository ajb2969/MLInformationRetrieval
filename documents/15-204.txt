   Wolfe duality      Wolfe duality   In mathematical optimization , Wolfe duality , named after Philip Wolfe , is type of dual problem in which the objective function and constraints are all differentiable functions . Using this concept a lower bound for a minimization problem can be found because of the weak duality principle. 1  Mathematical formulation  For a minimization problem with inequality constraints,      minimize  ùë•     x  minimize    \displaystyle\underset{x}{\operatorname{minimize}}     the Lagrangian dual problem is      maximize  ùë¢     u  maximize    \displaystyle\underset{u}{\operatorname{maximize}}     where the objective function is the Lagrange dual function. Provided that the functions   f   f   f   and     g  1   ,  ‚Ä¶  ,   g  m       subscript  g  1   normal-‚Ä¶   subscript  g  m     g_{1},\ldots,g_{m}   are continuously differentiable, the infimum occurs where the gradient is equal to zero. The problem      maximize   x  ,  u       x  u   maximize    \displaystyle\underset{x,u}{\operatorname{maximize}}     is called the Wolfe dual problem. 2 This problem employs the KKT conditions as a constraint. This problem may be difficult to deal with computationally, because the objective function is not concave in the joint variables    (  u  ,  x  )     u  x    (u,x)   . Also, the equality constraint      ‚àá  f    (  x  )    +    ‚àë   j  =  1   m     u  j    ‚àá   g  j     (  x  )            normal-‚àá  f   x     superscript   subscript     j  1    m      subscript  u  j    normal-‚àá   subscript  g  j    x      \nabla f(x)+\sum_{j=1}^{m}u_{j}\nabla g_{j}(x)   is nonlinear in general, so the Wolfe dual problem is typically a nonconvex optimization problem. In any case, weak duality holds. 3  See also   Lagrangian duality  Fenchel duality   References  "  Category:Mathematical optimization  Category:Convex optimization     ‚Ü©  ‚Ü©  ‚Ü©     