<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="749">Kernel perceptron</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Kernel perceptron</h1>
<hr/>

<p>In <a href="machine_learning" title="wikilink">machine learning</a>, the <strong>kernel perceptron</strong> is a variant of the popular <a class="uri" href="perceptron" title="wikilink">perceptron</a> learning algorithm that can learn <a href="kernel_trick" title="wikilink">kernel machines</a>, i.e. non-linear <a href="statistical_classification" title="wikilink">classifiers</a> that employ a kernel function to compute the similarity of unseen samples to training samples. The algorithm was invented in 1964,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> making it the first kernel classification learner.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h2 id="preliminaries">Preliminaries</h2>
<h3 id="the-perceptron-algorithm">The perceptron algorithm</h3>

<p>The perceptron algorithm is an old but popular <a href="online_machine_learning" title="wikilink">online learning</a> algorithm that operates by a principle called "error-driven learning". It iteratively improves a model by running it on training samples, then updating the model whenever it finds it has made an incorrect classification with respect to a <a href="supervised_learning" title="wikilink">supervised</a> signal. The model learned by the standard perceptron algorithm is a <a href="linear_classifier" title="wikilink">linear</a> binary classifier: a vector of weights 

<math display="inline" id="Kernel_perceptron:0">
 <semantics>
  <mi>𝐰</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐰</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}
  </annotation>
 </semantics>
</math>

 (and optionally an intercept term 

<math display="inline" id="Kernel_perceptron:1">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>

, omitted here for simplicity) that is used to classify a sample vector 

<math display="inline" id="Kernel_perceptron:2">
 <semantics>
  <mi>𝐱</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐱</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}
  </annotation>
 </semantics>
</math>

 as class "one" or class "minus one" according to</p>

<p>

<math display="block" id="Kernel_perceptron:3">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>y</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mo>=</mo>
   <mrow>
    <mo>sgn</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msup>
       <mi>𝐰</mi>
       <mo>⊤</mo>
      </msup>
      <mi>𝐱</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>normal-^</ci>
     <ci>y</ci>
    </apply>
    <apply>
     <ci>sgn</ci>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>𝐰</ci>
       <csymbol cd="latexml">top</csymbol>
      </apply>
      <ci>𝐱</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{y}=\operatorname{sgn}(\mathbf{w}^{\top}\mathbf{x})
  </annotation>
 </semantics>
</math>

</p>

<p>where a zero is arbitrarily mapped to one or minus one. (The "<a href="Circumflex#Mathematics" title="wikilink">hat</a>" on 

<math display="inline" id="Kernel_perceptron:4">
 <semantics>
  <mi>ŷ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ŷ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ŷ
  </annotation>
 </semantics>
</math>

 denotes an estimated value.)</p>

<p>In pseudocode, the perceptron algorithm is given by:</p>
<dl>
<dd>Initialize 

<math display="inline" id="Kernel_perceptron:5">
 <semantics>
  <mi>𝐰</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐰</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}
  </annotation>
 </semantics>
</math>

 to an all-zero vector of length 

<math display="inline" id="Kernel_perceptron:6">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

, the number of predictors (features).
</dd>
<dd>For some fixed number of iterations, or until some stopping criterion is met:
<dl>
<dd>For each training example 

<math display="inline" id="Kernel_perceptron:7">
 <semantics>
  <mrow>
   <mi>𝐱</mi>
   <mi>ᵢ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>𝐱</ci>
    <ci>ᵢ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}ᵢ
  </annotation>
 </semantics>
</math>

 with ground truth label }:
<dl>
<dd>Let <mtpl> sgn(<strong>w</strong><sup>T</sup> <strong>x</strong><em>ᵢ</em>)}}</mtpl>.
</dd>
<dd>If 

<math display="inline" id="Kernel_perceptron:8">
 <semantics>
  <mrow>
   <mi>ŷ</mi>
   <mi mathvariant="normal">≠</mi>
   <mi>y</mi>
   <mi>ᵢ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>ŷ</ci>
    <ci>normal-≠</ci>
    <ci>y</ci>
    <ci>ᵢ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ŷ≠yᵢ
  </annotation>
 </semantics>
</math>

, update 

<math display="inline" id="Kernel_perceptron:9">
 <semantics>
  <mrow>
   <mrow>
    <mi>𝐰</mi>
    <mi mathvariant="normal">←</mi>
    <mi>𝐰</mi>
   </mrow>
   <mo>+</mo>
   <mrow>
    <mi>y</mi>
    <mi>ᵢ</mi>
    <mi>𝐱</mi>
    <mi>ᵢ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <ci>𝐰</ci>
     <ci>normal-←</ci>
     <ci>𝐰</ci>
    </apply>
    <apply>
     <times></times>
     <ci>y</ci>
     <ci>ᵢ</ci>
     <ci>𝐱</ci>
     <ci>ᵢ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}←\mathbf{w}+yᵢ\mathbf{x}ᵢ
  </annotation>
 </semantics>
</math>

.
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<h3 id="kernel-machines">Kernel machines</h3>

<p>By contrast with the linear models learned by the perceptron, a kernel machine<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> is a classifier that stores a subset of its training examples <mtpl></mtpl>, associates with each a weight <mtpl></mtpl>, and makes decisions for new samples 

<math display="inline" id="Kernel_perceptron:10">
 <semantics>
  <mi>𝐱</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐱</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}
  </annotation>
 </semantics>
</math>

 by evaluating</p>

<p>

<math display="block" id="Kernel_perceptron:11">
 <semantics>
  <mrow>
   <mo>sgn</mo>
   <mrow>
    <munder>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mi>i</mi>
    </munder>
    <mrow>
     <msub>
      <mi>α</mi>
      <mi>i</mi>
     </msub>
     <msub>
      <mi>y</mi>
      <mi>i</mi>
     </msub>
     <mi>K</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>𝐱</mi>
       <mi>i</mi>
      </msub>
      <mo>,</mo>
      <msup>
       <mi>𝐱</mi>
       <mo>′</mo>
      </msup>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>sgn</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <ci>i</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>α</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>i</ci>
      </apply>
      <ci>K</ci>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>𝐱</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>𝐱</ci>
        <ci>normal-′</ci>
       </apply>
      </interval>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{sgn}\sum_{i}\alpha_{i}y_{i}K(\mathbf{x}_{i},\mathbf{x^{\prime}})
  </annotation>
 </semantics>
</math>

.</p>

<p>Here, 

<math display="inline" id="Kernel_perceptron:12">
 <semantics>
  <mi>K</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>K</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K
  </annotation>
 </semantics>
</math>

 is some kernel function. Formally, a kernel function is a <a href="positive_definite_kernel" title="wikilink">non-negative semidefinite kernel</a> (see <a href="Mercer's_condition" title="wikilink">Mercer's condition</a>), representing an <a href="inner_product" title="wikilink">inner product</a> between samples in a high-dimensional space, as if the samples had been expanded to include additional features by a function 

<math display="inline" id="Kernel_perceptron:13">
 <semantics>
  <mi mathvariant="normal">Φ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-Φ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Φ
  </annotation>
 </semantics>
</math>

: 

<math display="inline" id="Kernel_perceptron:14">
 <semantics>
  <mrow>
   <mi>K</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐱</mi>
    <mo>,</mo>
    <mi>𝐱</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi mathvariant="normal">Φ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐱</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi mathvariant="normal">·</mi>
   <mi mathvariant="normal">Φ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐱</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">K</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">Φ</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">·</csymbol>
    <csymbol cd="unknown">Φ</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-)</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K(\mathbf{x},\mathbf{x})=Φ(\mathbf{x})·Φ(\mathbf{x}))
  </annotation>
 </semantics>
</math>

. Intuitively, it can be thought of as a <a href="similarity_function" title="wikilink">similarity function</a> between samples, so the kernel machine establishes the class of a new sample by weighted comparison to the training set. Each function 

<math display="inline" id="Kernel_perceptron:15">
 <semantics>
  <mrow>
   <mi>𝐱</mi>
   <mi mathvariant="normal">↦</mi>
   <mi>K</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐱</mi>
    <mi>ᵢ</mi>
    <mo>,</mo>
    <mi>𝐱</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">x</csymbol>
    <csymbol cd="unknown">↦</csymbol>
    <csymbol cd="unknown">K</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <csymbol cd="unknown">ᵢ</csymbol>
     <ci>normal-,</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-)</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}↦K(\mathbf{x}ᵢ,\mathbf{x}))
  </annotation>
 </semantics>
</math>

 serves as a <a href="basis_function" title="wikilink">basis function</a> in the classification.</p>
<h2 id="algorithm">Algorithm</h2>

<p>To derive a kernelized version of the perceptron algorithm, we must first formulate it in <a href="Duality_(optimization)" title="wikilink">dual form</a>, starting from the observation that the weight vector 

<math display="inline" id="Kernel_perceptron:16">
 <semantics>
  <mi>𝐰</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐰</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}
  </annotation>
 </semantics>
</math>

 can be expressed as a <a href="linear_combination" title="wikilink">linear combination</a> of the 

<math display="inline" id="Kernel_perceptron:17">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 training samples. The equation for the weight vector is</p>

<p>

<math display="block" id="Kernel_perceptron:18">
 <semantics>
  <mrow>
   <mi>𝐰</mi>
   <mo>=</mo>
   <mrow>
    <munderover>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mi>i</mi>
     <mi>n</mi>
    </munderover>
    <mrow>
     <msub>
      <mi>α</mi>
      <mi>i</mi>
     </msub>
     <msub>
      <mi>y</mi>
      <mi>i</mi>
     </msub>
     <msub>
      <mi>𝐱</mi>
      <mi>i</mi>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>𝐰</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <ci>i</ci>
      </apply>
      <ci>n</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>α</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>𝐱</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}=\sum_{i}^{n}\alpha_{i}y_{i}\mathbf{x}_{i}
  </annotation>
 </semantics>
</math>

</p>

<p>where <mtpl></mtpl> is the number of times <mtpl></mtpl> was misclassified, forcing an update <mtpl></mtpl>. Using this result, we can formulate the dual perceptron algorithm, which loops through the samples as before, making predictions, but instead of storing and updating a weight vector 

<math display="inline" id="Kernel_perceptron:19">
 <semantics>
  <mi>𝐰</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐰</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}
  </annotation>
 </semantics>
</math>

, it updates a "mistake counter" vector 

<math display="inline" id="Kernel_perceptron:20">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{α}
  </annotation>
 </semantics>
</math>

. We must also rewrite the prediction formula to get rid of 

<math display="inline" id="Kernel_perceptron:21">
 <semantics>
  <mi>𝐰</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐰</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}
  </annotation>
 </semantics>
</math>

:</p>

<p>

<math display="inline" id="Kernel_perceptron:22">
 <semantics>
  <mover accent="true">
   <mi>y</mi>
   <mo stretchy="false">^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\hat{y}
  </annotation>
 </semantics>
</math>


</p>

<p>Plugging these two equations into the training loop turn it into the <em>dual perceptron</em> algorithm.</p>

<p>Finally, we can replace the <a href="dot_product" title="wikilink">dot product</a> in the dual perceptron by an arbitrary kernel function, to get the effect of a feature map 

<math display="inline" id="Kernel_perceptron:23">
 <semantics>
  <mi mathvariant="normal">Φ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-Φ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Φ
  </annotation>
 </semantics>
</math>

 without computing 

<math display="inline" id="Kernel_perceptron:24">
 <semantics>
  <mrow>
   <mi mathvariant="normal">Φ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐱</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-Φ</ci>
    <ci>𝐱</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Φ(\mathbf{x})
  </annotation>
 </semantics>
</math>

 explicitly for any samples. Doing this yields the kernel perceptron algorithm:<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<dl>
<dd>Initialize 

<math display="inline" id="Kernel_perceptron:25">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{α}
  </annotation>
 </semantics>
</math>

 to an all-zeros vector of length 

<math display="inline" id="Kernel_perceptron:26">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

, the number of training samples.
</dd>
<dd>For some fixed number of iterations, or until some stopping criterion is met:
<dl>
<dd>For each training example 

<math display="inline" id="Kernel_perceptron:27">
 <semantics>
  <mrow>
   <mi>𝐱</mi>
   <mo>,</mo>
   <mi>y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <ci>𝐱</ci>
    <ci>y</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x},y
  </annotation>
 </semantics>
</math>

:
<dl>
<dd>Let 

<math display="inline" id="Kernel_perceptron:28">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>y</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mo>=</mo>
   <mrow>
    <mo>sgn</mo>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mi>i</mi>
      <mi>n</mi>
     </msubsup>
     <mrow>
      <msub>
       <mi>α</mi>
       <mi>i</mi>
      </msub>
      <msub>
       <mi>y</mi>
       <mi>i</mi>
      </msub>
      <mi>K</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>𝐱</mi>
        <mi>i</mi>
       </msub>
       <mo>,</mo>
       <mi>𝐱</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>normal-^</ci>
     <ci>y</ci>
    </apply>
    <apply>
     <times></times>
     <ci>sgn</ci>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <ci>i</ci>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>α</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <ci>i</ci>
       </apply>
       <ci>K</ci>
       <interval closure="open">
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>𝐱</ci>
         <ci>i</ci>
        </apply>
        <ci>𝐱</ci>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{y}=\operatorname{sgn}\sum_{i}^{n}\alpha_{i}y_{i}K(\mathbf{x}_{i},\mathbf{%
x})
  </annotation>
 </semantics>
</math>


</dd>
<dd>If 

<math display="inline" id="Kernel_perceptron:29">
 <semantics>
  <mrow>
   <mi>ŷ</mi>
   <mi mathvariant="normal">≠</mi>
   <mi>y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>ŷ</ci>
    <ci>normal-≠</ci>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ŷ≠y
  </annotation>
 </semantics>
</math>

, perform an update:
<dl>
<dd><mtpl></mtpl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<h2 id="variants-and-extensions">Variants and extensions</h2>

<p>One problem with the kernel perceptron, as presented above, is that it does not learn <a href="sparsity" title="wikilink">sparse</a> kernel machines. Initially, all the 

<math display="inline" id="Kernel_perceptron:30">
 <semantics>
  <mrow>
   <mi>α</mi>
   <mi>ᵢ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>α</ci>
    <ci>ᵢ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   αᵢ
  </annotation>
 </semantics>
</math>

 are zero so that evaluating the decision function to get 

<math display="inline" id="Kernel_perceptron:31">
 <semantics>
  <mi>ŷ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ŷ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ŷ
  </annotation>
 </semantics>
</math>

 requires no kernel evaluations at all, but each update increments a single 

<math display="inline" id="Kernel_perceptron:32">
 <semantics>
  <mrow>
   <mi>α</mi>
   <mi>ᵢ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>α</ci>
    <ci>ᵢ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   αᵢ
  </annotation>
 </semantics>
</math>

, making the evaluation increasingly more costly. Moreover, when the kernel perceptron is used in an <a href="Online_machine_learning" title="wikilink">online</a> setting, the number of non-zero 

<math display="inline" id="Kernel_perceptron:33">
 <semantics>
  <mrow>
   <mi>α</mi>
   <mi>ᵢ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>α</ci>
    <ci>ᵢ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   αᵢ
  </annotation>
 </semantics>
</math>

 and thus the evaluation cost grow linearly in the number of examples presented to the algorithm.</p>

<p>The forgetron variant of the kernel perceptron was suggested to deal with this problem. It maintains an <a href="Active_set_method" title="wikilink">active set</a> of examples with non-zero 

<math display="inline" id="Kernel_perceptron:34">
 <semantics>
  <mrow>
   <mi>α</mi>
   <mi>ᵢ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>α</ci>
    <ci>ᵢ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   αᵢ
  </annotation>
 </semantics>
</math>

, removing ("forgetting") examples from the active set when it exceeds a pre-determined budget and "shrinking" (lowering the weight of) old examples as new ones are promoted to non-zero 

<math display="inline" id="Kernel_perceptron:35">
 <semantics>
  <mrow>
   <mi>α</mi>
   <mi>ᵢ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>α</ci>
    <ci>ᵢ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   αᵢ
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>

<p>Another problem with the kernel perceptron is that it does not <a href="Regularization_(mathematics)" title="wikilink">regularize</a>, making it vulnerable to <a class="uri" href="overfitting" title="wikilink">overfitting</a>. The NORMA online kernel learning algorithm can be regarded as a generalization of the kernel perceptron algorithm with regularization.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> The <a href="sequential_minimal_optimization" title="wikilink">sequential minimal optimization</a> (SMO) algorithm used to learn <a href="support_vector_machine" title="wikilink">support vector machines</a> can also be regarded as a generalization of the kernel perceptron.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>

<p>The voted perceptron algorithm of Freund and Schapire also extends to the kernelized case,<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> giving generalization bounds comparable to the kernel SVM.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Kernel_methods_for_machine_learning" title="wikilink">Category:Kernel methods for machine learning</a> <a href="Category:Statistical_classification" title="wikilink">Category:Statistical classification</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"> Cited in <a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3">Schölkopf, Bernhard; and Smola, Alexander J.; <em>Learning with Kernels</em>, MIT Press, Cambridge, MA, 2002. ISBN 0-262-19475-9<a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"></li>
</ol>
</section>
</body>
</html>
