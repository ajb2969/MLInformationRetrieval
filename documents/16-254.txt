   Proximal gradient method      Proximal gradient method   Proximal gradient methods are a generalized form of projection used to solve non-differentiable convex optimization problems. Many interesting problems can be formulated as convex optimization problems of form       minimize   x  ∈   ℝ  N        f  1    (  x  )    +    f  2    (  x  )    +  ⋯  +    f   n  -  1     (  x  )    +    f  n    (  x  )         subscript  minimize    x   superscript  ℝ  N          subscript  f  1   x      subscript  f  2   x   normal-⋯     subscript  f    n  1    x      subscript  f  n   x      \operatorname{minimize}_{x\in\mathbb{R}^{N}}\qquad f_{1}(x)+f_{2}(x)+\cdots+f_%
 {n-1}(x)+f_{n}(x)     where     f  1   ,   f  2   ,  …  ,   f  n       subscript  f  1    subscript  f  2   normal-…   subscript  f  n     f_{1},f_{2},...,f_{n}   are convex functions defined from    f  :    ℝ  N   →  ℝ      normal-:  f   normal-→   superscript  ℝ  N   ℝ     f:\mathbb{R}^{N}\rightarrow\mathbb{R}   where some of the functions are non-differentiable, this rules out our conventional smooth optimization techniques like Steepest descent method , conjugate gradient method etc. There is a specific class of algorithms which can solve above optimization problem. These methods proceed by splitting, in that the functions     f  1   ,  …  ,   f  n       subscript  f  1   normal-…   subscript  f  n     f_{1},...,f_{n}   are used individually so as to yield an easily implementable algorithm. They are called proximal because each non smooth function among     f  1   ,  …  ,   f  n       subscript  f  1   normal-…   subscript  f  n     f_{1},...,f_{n}   is involved via its proximity operator. Iterative Shrinkage thresholding algorithm, projected Landweber , projected gradient, alternating projections , alternating-direction method of multipliers , alternating split Bregman are special instances of proximal algorithms. Details of proximal methods are discussed in Combettes and Pesquet. 1 For the theory of proximal gradient methods from the perspective of and with applications to statistical learning theory , see proximal gradient methods for learning .  Notations and terminology  Let    ℝ  N     superscript  ℝ  N    \mathbb{R}^{N}   , the   N   N   N   -dimensional euclidean space , be the domain of the function    f  :    ℝ  N   →   [   -  ∞   ,   +  ∞   ]       normal-:  f   normal-→   superscript  ℝ  N                f:\mathbb{R}^{N}\rightarrow[-\infty,+\infty]   . Suppose   C   C   C   is the non-empty convex subset of    ℝ  N     superscript  ℝ  N    \mathbb{R}^{N}   . Then, the indicator function of   C   C   C   is defined as       i  C   :   x  ↦   {     0      if  x   ∈  C        +  ∞       if  x   ∉  C           normal-:   subscript  i  C    maps-to  x   cases  0      if  x   C           if  x   C       i_{C}:x\mapsto\begin{cases}0&\text{if }x\in C\\
 +\infty&\text{if }x\notin C\end{cases}        p   p   p   -norm is defined as (    ∥  ⋅   ∥  p      fragments  parallel-to  normal-⋅   subscript  parallel-to  p     \|\cdot\|_{p}   )        ∥  x  ∥   p   =    (     |   x  1   |   p   +    |   x  2   |   p   +  ⋯  +    |   x  N   |   p    )    1  /  p         subscript   norm  x   p    superscript     superscript     subscript  x  1    p    superscript     subscript  x  2    p   normal-⋯   superscript     subscript  x  N    p      1  p      \|x\|_{p}=(|x_{1}|^{p}+|x_{2}|^{p}+\cdots+|x_{N}|^{p})^{1/p}     The distance from    x  ∈   ℝ  N       x   superscript  ℝ  N     x\in\mathbb{R}^{N}   to   C   C   C   is defined as        D  C    (  x  )    =    min   y  ∈  C     ∥   x  -  y   ∥           subscript  D  C   x     subscript     y  C     norm    x  y       D_{C}(x)=\min_{y\in C}\|x-y\|     If   C   C   C   is closed and convex, the projection of    x  ∈   ℝ  N       x   superscript  ℝ  N     x\in\mathbb{R}^{N}   onto   C   C   C   is the unique point      P  C   x   ∈  C         subscript  P  C   x   C    P_{C}x\in C   such that      D  C    (  x  )    =    ∥   x  -    P  C   x    ∥   2          subscript  D  C   x    subscript   norm    x     subscript  P  C   x     2     D_{C}(x)=\|x-P_{C}x\|_{2}   .  The subdifferential of   f   f   f   is given by      ∂  f  :   ℝ  N   →   2   ℝ  N    :  x  ↦   {  u  ∈   ℝ  N   ∣  ∀  y  ∈   ℝ  N   ,    (  y  -  x  )   T   u  +  f   (  x  )   ≤  f   (  y  )   )   .  }     fragments   f  normal-:   superscript  ℝ  N   normal-→   superscript  2   superscript  ℝ  N    normal-:  x  maps-to   fragments  normal-{  u    superscript  ℝ  N   normal-∣  for-all  y    superscript  ℝ  N   normal-,   superscript   fragments  normal-(  y   x  normal-)   normal-T   u   f   fragments  normal-(  x  normal-)    f   fragments  normal-(  y  normal-)   normal-)   normal-.  normal-}    \partial f:\mathbb{R}^{N}\rightarrow 2^{\mathbb{R}^{N}}:x\mapsto\{u\in\mathbb{%
 R}^{N}\mid\forall y\in\mathbb{R}^{N},(y-x)^{\mathrm{T}}u+f(x)\leq f(y)).\}     Projection onto convex sets (POCS)  One of the widely used convex optimization algorithms is POCS ( Projection Onto Convex Sets ). This algorithm is employed to recover/synthesize a signal satisfying simultaneously several convex constraints. Let    f  i     subscript  f  i    f_{i}   be the indicator function of non-empty closed convex set    C  i     subscript  C  i    C_{i}   modeling a constraint. This reduces to convex feasibility problem, which require us to find a solution such that it lies in the intersection of all convex sets    C  i     subscript  C  i    C_{i}   . In POCS method each set    C  i     subscript  C  i    C_{i}   is incorporated by its projection operator     P   C  i      subscript  P   subscript  C  i     P_{C_{i}}   . So in each iteration    x   x   x   is updated as       x   k  +  1    =    P   C  1     P   C  2    ⋯   P   C  n     x  k         subscript  x    k  1       subscript  P   subscript  C  1     subscript  P   subscript  C  2    normal-⋯   subscript  P   subscript  C  n     subscript  x  k      x_{k+1}=P_{C_{1}}P_{C_{2}}\cdots P_{C_{n}}x_{k}     However beyond such problems projection operators are not appropriate and more general operators are required to tackle them. Among the various generalizations of the notion of a convex projection operator that exist, proximity operators are best suited for other purposes.  Definition  Proximity operators of function   f   f   f   at   x   x   x   is defined as  For every    x  ∈   ℝ  N       x   superscript  ℝ  N     x\in\mathbb{R}^{N}   , the minimization problem       minimize   y  ∈  C      f   (  y  )    +    1  2     ∥   x  -  y   ∥   2  2         subscript  minimize    y  C        f  y       1  2    superscript   subscript   norm    x  y    2   2       \text{minimize}_{y\in C}\qquad f(y)+\frac{1}{2}\left\|x-y\right\|_{2}^{2}   admits a unique solution which is denoted by     prox  f    (  x  )       subscript  prox  f   x    \operatorname{prox}_{f}(x)   .        prox  f    (  x  )    :    ℝ  N   →   ℝ  N       normal-:    subscript  prox  f   x    normal-→   superscript  ℝ  N    superscript  ℝ  N      \operatorname{prox}_{f}(x):\mathbb{R}^{N}\rightarrow\mathbb{R}^{N}     The proximity operator of   f   f   f   is characterized by inclusion      p  =   prox  f    (  x  )   ⇔  x  -  p  ∈  ∂  f   (  p  )    (  ∀   (  x  ,  p  )   ∈   ℝ  N   ×   ℝ  N   )      fragments  p    subscript  prox  f    fragments  normal-(  x  normal-)   normal-⇔  x   p    f   fragments  normal-(  p  normal-)   italic-   fragments  normal-(  for-all   fragments  normal-(  x  normal-,  p  normal-)     superscript  ℝ  N     superscript  ℝ  N   normal-)     p=\operatorname{prox}_{f}(x)\Leftrightarrow x-p\in\partial f(p)\qquad(\forall(%
 x,p)\in\mathbb{R}^{N}\times\mathbb{R}^{N})     If   f   f   f   is differentiable then above equation reduces to      p  =   prox  f    (  x  )   ⇔  x  -  p  ∈  ∇  f   (  p  )    (  ∀   (  x  ,  p  )   ∈   ℝ  N   ×   ℝ  N   )      fragments  p    subscript  prox  f    fragments  normal-(  x  normal-)   normal-⇔  x   p   normal-∇  f   fragments  normal-(  p  normal-)     fragments  normal-(  for-all   fragments  normal-(  x  normal-,  p  normal-)     superscript  ℝ  N     superscript  ℝ  N   normal-)     p=\operatorname{prox}_{f}(x)\Leftrightarrow x-p\in\nabla f(p)\quad(\forall(x,p%
 )\in\mathbb{R}^{N}\times\mathbb{R}^{N})     Examples  Special instances of Proximal Gradient Methods are   Projected Landweber  Alternating projection  Alternating-direction method of multipliers  Fast Iterative Shrinkage Thresholding Algorithm (FISTA)    See also   Alternating projection  Convex optimization  Frank–Wolfe algorithm  Proximal gradient methods for learning   References    {{ cite book   | last1 = Combettes  | first1 = Patrick L.  | last2 = Pesquet  | first2 = Jean-Christophe  | title = Springer's Fixed-Point Algorithms for Inverse Problems in Science and Engineering  | volume = 49  | year = 2011  | pages = 185–212  }}  Notes    External links   Stephen Boyd and Lieven Vandenberghe Book, Convex optimization  EE364a: Convex Optimization I and EE364b: Convex Optimization II , Stanford course homepages  EE227A: Lieven Vandenberghe Notes Lecture 18   "  Category:Gradient methods     ↩     