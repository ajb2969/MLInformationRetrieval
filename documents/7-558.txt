   Generalized additive model      Generalized additive model   In statistics , a generalized additive model (GAM) is a generalized linear model in which the linear predictor depends linearly on unknown smooth functions of some predictor variables, and interest focuses on inference about these smooth functions. GAMs were originally developed by Trevor Hastie and Robert Tibshirani  1 to blend properties of generalized linear models with additive models .  The model relates a univariate response variable, Y , to some predictor variables, x i . An exponential family distribution is specified for Y (for example normal , binomial or Poisson distributions) along with a link function  g (for example the identity or log functions) relating the expected value of Y to the predictor variables via a structure such as        g   (   E   (  Y  )    )    =    β  0   +    f  1    (   x  1   )    +    f  2    (   x  2   )    +  ⋯  +    f  m    (   x  m   )      .        g   normal-E  Y       subscript  β  0      subscript  f  1    subscript  x  1       subscript  f  2    subscript  x  2    normal-⋯     subscript  f  m    subscript  x  m       g(\operatorname{E}(Y))=\beta_{0}+f_{1}(x_{1})+f_{2}(x_{2})+\cdots+f_{m}(x_{m})%
 .\,\!     The functions f i ( x i ) may be functions with a specified parametric form (for example a polynomial, or a coefficient depending on the levels of a factor variable) or may be specified non-parametrically, or semi-parametrically, simply as 'smooth functions', to be estimated by non-parametric means . So a typical GAM might use a scatterplot smoothing function, such as a locally weighted mean, for f 1 ( x 1 ), and then use a factor model for f 2 ( x 2 ). This flexibility to allow non-parametric fits with relaxed assumptions on the actual relationship between response and predictor, provides the potential for better fits to data than purely parametric models, but arguably with some loss of interpretability.  Estimation  The original GAM estimation method was the backfitting algorithm , 2 which provides a very general modular estimation method capable of using a wide variety of smoothing methods to estimate the   2.3
                   
                   2   2.3
                   
                   2   \par
 \par
 \@@section{subsection}{S2.SS3}{2.3}{2.3}{{\@tag[][]{2.3}2}}{{\@tag[]%
 []{2.3}2}}\par     ( xᵢ )}}. A disadvantage of backfitting is that it is difficult to integrate with well founded methods for choosing the degree of smoothness of the $$ ( xᵢ )}}. As a result alternative methods have been developed in which smooth functions are represented semi-parametrically, using penalized regression splines, 3 in order to allow computationally efficient estimation of the degree of smoothness of the model components using generalized cross validation 4 or similar criteria.  Overfitting can be a problem with GAMs. 5 The number of smoothing parameters can be specified, and this number should be reasonably small, certainly well under the degrees of freedom offered by the data. Cross-validation can be used to detect and/or reduce overfitting problems with GAMs (or other statistical methods). 6 Other models such as GLMs may be preferable to GAMs unless GAMs improve predictive ability substantially (in validation sets) for the application in question.  See also   Additive model  Backfitting algorithm  Generalized additive model for location, scale, and shape (GAMLSS)  Hilbert's thirteenth problem  Nomogram  Residual effective degrees of freedom   References    External links   gam , an R package for GAMs by backfitting  mgcv , an R package for GAMs using penalized regression splines  GAM: The Predictive Modeling Silver Bullet   "  Category:Generalized linear models  Category:Nonparametric regression     ↩   ↩  Wood, S.N. (2000) Modelling and smoothing parameter estimation with multiple quadratic penalties. Journal of the Royal Statistical Society: Series B 62(2),413-428. ↩  ↩  ↩     