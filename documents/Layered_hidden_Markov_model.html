<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="782">Layered hidden Markov model</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Layered hidden Markov model</h1>
<hr/>

<p>The <strong>layered <a href="hidden_Markov_model" title="wikilink">hidden Markov model</a> (LHMM)</strong> is a <a href="statistical_model" title="wikilink">statistical model</a> derived from the hidden Markov model (HMM). A layered hidden Markov model (LHMM) consists of <em>N</em> levels of HMMs, where the HMMs on level <em>i</em>¬†+¬†1 correspond to observation symbols or probability generators at level <em>i</em>. Every level <em>i</em> of the LHMM consists of <em>K</em><sub><em>i</em></sub> HMMs running in parallel.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="background">Background</h2>

<p>LHMMs are sometimes useful in specific structures because they can facilitate learning and generalization. For example, even though a fully connected HMM could always be used if enough training data were available, it is often useful to constrain the model by not allowing arbitrary state transitions. In the same way it can be beneficial to embed the HMM in a layered structure which, theoretically, may not be able to solve any problems the basic HMM cannot, but can solve some problems more efficiently because less training data is needed.</p>
<h2 id="the-layered-hidden-markov-model">The layered hidden Markov model</h2>

<p>A layered hidden Markov model (LHMM) consists of 

<math display="inline" id="Layered_hidden_Markov_model:0">
 <semantics>
  <mi>N</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>N</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N
  </annotation>
 </semantics>
</math>

 levels of HMMs where the HMMs on level 

<math display="inline" id="Layered_hidden_Markov_model:1">
 <semantics>
  <mrow>
   <mi>N</mi>
   <mo>+</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <ci>N</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N+1
  </annotation>
 </semantics>
</math>

 corresponds to observation symbols or probability generators at level 

<math display="inline" id="Layered_hidden_Markov_model:2">
 <semantics>
  <mi>N</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>N</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N
  </annotation>
 </semantics>
</math>

. Every level 

<math display="inline" id="Layered_hidden_Markov_model:3">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

 of the LHMM consists of 

<math display="inline" id="Layered_hidden_Markov_model:4">
 <semantics>
  <msub>
   <mi>K</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>K</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K_{i}
  </annotation>
 </semantics>
</math>

 HMMs running in parallel.</p>
<figure><b>(Figure)</b>
<figcaption>A layered hidden Markov model</figcaption>
</figure>

<p>At any given level 

<math display="inline" id="Layered_hidden_Markov_model:5">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

 in the LHMM a sequence of 

<math display="inline" id="Layered_hidden_Markov_model:6">
 <semantics>
  <msub>
   <mi>T</mi>
   <mi>L</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>T</ci>
    <ci>L</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T_{L}
  </annotation>
 </semantics>
</math>

 observation symbols 

<math display="inline" id="Layered_hidden_Markov_model:7">
 <semantics>
  <mrow>
   <msub>
    <mi>ùê®</mi>
    <mi>L</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <msub>
     <mi>o</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>o</mi>
     <mn>2</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">‚Ä¶</mi>
    <mo>,</mo>
    <msub>
     <mi>o</mi>
     <msub>
      <mi>T</mi>
      <mi>L</mi>
     </msub>
    </msub>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>ùê®</ci>
     <ci>L</ci>
    </apply>
    <set>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>o</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>o</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>normal-‚Ä¶</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>o</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>T</ci>
       <ci>L</ci>
      </apply>
     </apply>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{o}_{L}=\{o_{1},o_{2},\dots,o_{T_{L}}\}
  </annotation>
 </semantics>
</math>

 can be used to classify the input into one of 

<math display="inline" id="Layered_hidden_Markov_model:8">
 <semantics>
  <msub>
   <mi>K</mi>
   <mi>L</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>K</ci>
    <ci>L</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K_{L}
  </annotation>
 </semantics>
</math>

 classes, where each class corresponds to each of the 

<math display="inline" id="Layered_hidden_Markov_model:9">
 <semantics>
  <msub>
   <mi>K</mi>
   <mi>L</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>K</ci>
    <ci>L</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K_{L}
  </annotation>
 </semantics>
</math>

 HMMs at level 

<math display="inline" id="Layered_hidden_Markov_model:10">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

. This classification can then be used to generate a new observation for the level 

<math display="inline" id="Layered_hidden_Markov_model:11">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mo>-</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <ci>L</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L-1
  </annotation>
 </semantics>
</math>

 HMMs. At the lowest layer, i.e. level 

<math display="inline" id="Layered_hidden_Markov_model:12">
 <semantics>
  <mi>N</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>N</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N
  </annotation>
 </semantics>
</math>

, primitive observation symbols 

<math display="inline" id="Layered_hidden_Markov_model:13">
 <semantics>
  <mrow>
   <msub>
    <mi>ùê®</mi>
    <mi>p</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <msub>
     <mi>o</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>o</mi>
     <mn>2</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">‚Ä¶</mi>
    <mo>,</mo>
    <msub>
     <mi>o</mi>
     <msub>
      <mi>T</mi>
      <mi>p</mi>
     </msub>
    </msub>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>ùê®</ci>
     <ci>p</ci>
    </apply>
    <set>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>o</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>o</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>normal-‚Ä¶</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>o</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>T</ci>
       <ci>p</ci>
      </apply>
     </apply>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{o}_{p}=\{o_{1},o_{2},\dots,o_{T_{p}}\}
  </annotation>
 </semantics>
</math>

 would be generated directly from observations of the modeled process. For example in a trajectory tracking task the primitive observation symbols would originate from the quantized sensor values. Thus at each layer in the LHMM the observations originate from the classification of the underlying layer, except for the lowest layer where the observation symbols originate from measurements of the observed process.</p>

<p>It is not necessary to run all levels at the same time granularity. For example it is possible to use windowing at any level in the structure so that the classification takes the average of several classifications into consideration before passing the results up the layers of the LHMM.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>Instead of simply using the winning HMM at level 

<math display="inline" id="Layered_hidden_Markov_model:14">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mo>+</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <ci>L</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L+1
  </annotation>
 </semantics>
</math>

 as an input symbol for the HMM at level 

<math display="inline" id="Layered_hidden_Markov_model:15">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

 it is possible to use it as a <a href="probability_generator" title="wikilink">probability generator</a> by passing the complete <a href="probability_distribution" title="wikilink">probability distribution</a> up the layers of the LHMM. Thus instead of having a "winner takes all" strategy where the most probable HMM is selected as an observation symbol, the likelihood 

<math display="inline" id="Layered_hidden_Markov_model:16">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>i</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>L</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L(i)
  </annotation>
 </semantics>
</math>

 of observing the 

<math display="inline" id="Layered_hidden_Markov_model:17">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

th HMM can be used in the recursion formula of the level 

<math display="inline" id="Layered_hidden_Markov_model:18">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

 HMM to account for the uncertainty in the classification of the HMMs at level 

<math display="inline" id="Layered_hidden_Markov_model:19">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mo>+</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <ci>L</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L+1
  </annotation>
 </semantics>
</math>

. Thus, if the classification of the HMMs at level 

<math display="inline" id="Layered_hidden_Markov_model:20">
 <semantics>
  <mrow>
   <mi>n</mi>
   <mo>+</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <ci>n</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n+1
  </annotation>
 </semantics>
</math>

 is uncertain, it is possible to pay more attention to the a-priori information encoded in the HMM at level 

<math display="inline" id="Layered_hidden_Markov_model:21">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

.</p>

<p>A LHMM could in practice be transformed into a single layered HMM where all the different models are concatenated together.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> Some of the advantages that may be expected from using the LHMM over a large single layer HMM is that the LHMM is less likely to suffer from <a class="uri" href="overfitting" title="wikilink">overfitting</a> since the individual sub-components are trained independently on smaller amounts of data. A consequence of this is that a significantly smaller amount of training data is required for the LHMM to achieve a performance comparable of the HMM. Another advantage is that the layers at the bottom of the LHMM, which are more sensitive to changes in the environment such as the type of sensors, sampling rate etc. can be retrained separately without altering the higher layers of the LHMM.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Hierarchical_hidden_Markov_model" title="wikilink">Hierarchical hidden Markov model</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Hidden_Markov_models" title="wikilink">Category:Hidden Markov models</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">N. Oliver, A. Garg and E. Horvitz, "Layered Representations for Learning and Inferring Office Activity from Multiple Sensory Channels", Computer Vision and Image Understanding, vol. 96, p. 163‚Äì180, 2004.<a href="#fnref1">‚Ü©</a></li>
<li id="fn2">D. Aarno and D. Kragic "Evaluation of Layered HMM for Motion Intention Recognition", IEEE International Conference on Advanced Robotics, 2007<a href="#fnref2">‚Ü©</a></li>
<li id="fn3">D. Aarno and D. Kragic: "Layered HMM for Motion Intention Recognition", IEEE/RSJ International Conference on Intelligent Robots and Systems, 2006.<a href="#fnref3">‚Ü©</a></li>
</ol>
</section>
</body>
</html>
