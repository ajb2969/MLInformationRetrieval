   Rank–nullity theorem      Rank–nullity theorem   In mathematics , the rank–nullity theorem of linear algebra , in its simplest form, states that the rank and the nullity of a matrix add up to the number of columns of the matrix. Specifically, if A is an m -by- n matrix (with m rows and n columns) over some field , then 1         rk   (  A  )    +   nul   (  A  )     =  n   .         rk  A    nul  A    n    \operatorname{rk}(A)+\operatorname{nul}(A)=n.     This applies to linear maps as well. Let V and W be vector spaces over some field and let  be a linear map. Then the rank of T is the dimension of the image of T and the nullity of T is the dimension of the kernel of T , so we have         dim   (   im   (  T  )    )    +   dim   (   ker   (  T  )    )     =   dim   (  V  )     ,         dim   im  T     dim   ker  T      dim  V     \operatorname{dim}(\operatorname{im}(T))+\operatorname{dim}(\operatorname{ker}%
 (T))=\operatorname{dim}(V),     or, equivalently,         rk   (  T  )    +   nul   (  T  )     =   dim   (  V  )     .         rk  T    nul  T     dim  V     \operatorname{rk}(T)+\operatorname{nul}(T)=\operatorname{dim}(V).     One can refine this statement (via the splitting lemma or the below proof) to be a statement about an isomorphism of spaces, not just dimensions.  More generally, one can consider the image, kernel, coimage, and cokernel, which are related by the fundamental theorem of linear algebra .  Proofs  We give two proofs. The first proof uses notations for linear transformations, but can be easily adapted to matrices by writing , where A is . The second proof looks at the homogeneous system  associated with an  matrix A of rank  r and shows explicitly that there exist a set of linearly independent solutions that span the null space of A .  First proof: Suppose    {   𝐮  1   ,  …  ,   𝐮  m   }      subscript  𝐮  1   normal-…   subscript  𝐮  m     \{\mathbf{u}_{1},\ldots,\mathbf{u}_{m}\}   forms a basis of ker T . We can extend this to form a basis of V     {   𝐮  1   ,  …  ,   𝐮  m   ,   𝐰  1   ,  …  ,   𝐰  n   }      subscript  𝐮  1   normal-…   subscript  𝐮  m    subscript  𝐰  1   normal-…   subscript  𝐰  n     \{\mathbf{u}_{1},\ldots,\mathbf{u}_{m},\mathbf{w}_{1},\ldots,\mathbf{w}_{n}\}   . Since the dimension of ker T is m and the dimension of V is , it suffices to show that the dimension of  is n .  Let us see that    {   T   𝐰  1    ,  …  ,   T   𝐰  n    }       T   subscript  𝐰  1    normal-…    T   subscript  𝐰  n      \{T\mathbf{w}_{1},\ldots,T\mathbf{w}_{n}\}   is a basis of . Let v be an arbitrary vector in V . There exist unique scalars such that:      𝐯  =     a  1    𝐮  1    +  ⋯  +    a  m    𝐮  m    +    b  1    𝐰  1    +  ⋯  +    b  n    𝐰  n         𝐯       subscript  a  1    subscript  𝐮  1    normal-⋯     subscript  a  m    subscript  𝐮  m       subscript  b  1    subscript  𝐰  1    normal-⋯     subscript  b  n    subscript  𝐰  n       \mathbf{v}=a_{1}\mathbf{u}_{1}+\cdots+a_{m}\mathbf{u}_{m}+b_{1}\mathbf{w}_{1}+%
 \cdots+b_{n}\mathbf{w}_{n}          ⇒   T  𝐯   =     a  1   T   𝐮  1    +  ⋯  +    a  m   T   𝐮  m    +    b  1   T   𝐰  1    +  ⋯  +    b  n   T   𝐰  n          normal-⇒  absent    T  𝐯             subscript  a  1   T   subscript  𝐮  1    normal-⋯     subscript  a  m   T   subscript  𝐮  m       subscript  b  1   T   subscript  𝐰  1    normal-⋯     subscript  b  n   T   subscript  𝐰  n        \Rightarrow T\mathbf{v}=a_{1}T\mathbf{u}_{1}+\cdots+a_{m}T\mathbf{u}_{m}+b_{1}%
 T\mathbf{w}_{1}+\cdots+b_{n}T\mathbf{w}_{n}           ⇒   T  𝐯   =     b  1   T   𝐰  1    +  ⋯  +    b  n   T    𝐰  n       ∵    T   𝐮  i    =  0      because     normal-⇒  absent    T  𝐯             subscript  b  1   T   subscript  𝐰  1    normal-⋯     subscript  b  n   T   subscript  𝐰  n           T   subscript  𝐮  i    0     \Rightarrow T\mathbf{v}=b_{1}T\mathbf{w}_{1}+\cdots+b_{n}T\mathbf{w}_{n}\;\;%
 \because T\mathbf{u}_{i}=0     Thus,    {   T   𝐰  1    ,  …  ,   T   𝐰  n    }       T   subscript  𝐰  1    normal-…    T   subscript  𝐰  n      \{T\mathbf{w}_{1},\ldots,T\mathbf{w}_{n}\}   spans .  We only now need to show that this list is not redundant; that is, that    {   T   𝐰  1    ,  …  ,   T   𝐰  n    }       T   subscript  𝐰  1    normal-…    T   subscript  𝐰  n      \{T\mathbf{w}_{1},\ldots,T\mathbf{w}_{n}\}   are linearly independent. We can do this by showing that a linear combination of these vectors is zero if and only if the coefficient on each vector is zero. Let:          c  1   T   𝐰  1    +  ⋯  +    c  n   T   𝐰  n     =  0   ⇔    T   {     c  1    𝐰  1    +  ⋯  +    c  n    𝐰  n     }    =  0      normal-⇔         subscript  c  1   T   subscript  𝐰  1    normal-⋯     subscript  c  n   T   subscript  𝐰  n     0       T        subscript  c  1    subscript  𝐰  1    normal-⋯     subscript  c  n    subscript  𝐰  n       0     c_{1}T\mathbf{w}_{1}+\cdots+c_{n}T\mathbf{w}_{n}=0\Leftrightarrow T\{c_{1}%
 \mathbf{w}_{1}+\cdots+c_{n}\mathbf{w}_{n}\}=0          ∴      c  1    𝐰  1    +  ⋯  +    c  n    𝐰  n     ∈   ker  T       therefore  absent         subscript  c  1    subscript  𝐰  1    normal-⋯     subscript  c  n    subscript  𝐰  n      ker  T      \therefore c_{1}\mathbf{w}_{1}+\cdots+c_{n}\mathbf{w}_{n}\in\operatorname{ker}\;T     Then, since u i span ker T , there exists a set of scalars d i such that:         c  1    𝐰  1    +  ⋯  +    c  n    𝐰  n     =     d  1    𝐮  1    +  ⋯  +    d  m    𝐮  m              subscript  c  1    subscript  𝐰  1    normal-⋯     subscript  c  n    subscript  𝐰  n          subscript  d  1    subscript  𝐮  1    normal-⋯     subscript  d  m    subscript  𝐮  m       c_{1}\mathbf{w}_{1}+\cdots+c_{n}\mathbf{w}_{n}=d_{1}\mathbf{u}_{1}+\cdots+d_{m%
 }\mathbf{u}_{m}     But, since    {   𝐮  1   ,  …  ,   𝐮  m   ,   𝐰  1   ,  …  ,   𝐰  n   }      subscript  𝐮  1   normal-…   subscript  𝐮  m    subscript  𝐰  1   normal-…   subscript  𝐰  n     \{\mathbf{u}_{1},\ldots,\mathbf{u}_{m},\mathbf{w}_{1},\ldots,\mathbf{w}_{n}\}   form a basis of V , all c i , d i must be zero. Therefore,    {   T   𝐰  1    ,  …  ,   T   𝐰  n    }       T   subscript  𝐰  1    normal-…    T   subscript  𝐰  n      \{T\mathbf{w}_{1},\ldots,T\mathbf{w}_{n}\}   is linearly independent and indeed a basis of . This proves that the dimension of  is n , as desired.  In more abstract terms, the map splits .  Second proof: Let A be an  matrix with r  linearly independent columns (i.e. rank of A is r ). We will show that: (i) there exists a set of  linearly independent solutions to the homogeneous system , and (ii) that every other solution is a linear combination of these  solutions. In other words, we will produce an  matrix X whose columns form a basis of the null space of A .  Without loss of generality, assume that the first r columns of A are linearly independent. So, we can write , where A 1 is  with r linearly independent column vectors and A 2 is , each of whose  columns are linear combinations of the columns of A 1 . This means that for some '' matrix B (see rank factorization ) and, hence, . Let    𝐗  =   (      -  𝐁        𝐈   n  -  r       )       𝐗      𝐁      subscript  𝐈    n  r        \displaystyle\mathbf{X}=\begin{pmatrix}-\mathbf{B}\\
 \mathbf{I}_{n-r}\end{pmatrix}   , where    𝐈   n  -  r      subscript  𝐈    n  r     \mathbf{I}_{n-r}   is the identity matrix . We note that X is an  matrix that satisfies      𝐀𝐗  =   [   𝐀  1   :   𝐀  1   𝐁  ]    (      -  𝐁        𝐈   n  -  r       )   =  -   𝐀  1   𝐁  +   𝐀  1   𝐁  =   𝐎   .     fragments  AX    fragments  normal-[   subscript  𝐀  1   normal-:   subscript  𝐀  1   B  normal-]       𝐁      subscript  𝐈    n  r         subscript  𝐀  1   B    subscript  𝐀  1   B   O  normal-.    \mathbf{A}\mathbf{X}=[\mathbf{A}_{1}:\mathbf{A}_{1}\mathbf{B}]\begin{pmatrix}-%
 \mathbf{B}\\
 \mathbf{I}_{n-r}\end{pmatrix}=-\mathbf{A}_{1}\mathbf{B}+\mathbf{A}_{1}\mathbf{%
 B}=\mathbf{O}\;.   Therefore, each of the  columns of X are particular solutions of . Furthermore, the  columns of X are linearly independent because  will imply :      𝐗𝐮  =  𝟎  ⇒    (      -  𝐁        𝐈   n  -  r       )   𝐮   =  𝟎  ⇒   (      -  𝐁𝐮       𝐮     )   =   (     𝟎      𝟎     )   ⇒  𝐮  =  0 .        𝐗𝐮  0    normal-⇒          𝐁      subscript  𝐈    n  r      𝐮        0    normal-⇒        𝐁𝐮     𝐮           0    0      normal-⇒    𝐮       0 .     \mathbf{X}\mathbf{u}=\mathbf{0}\Rightarrow\begin{pmatrix}-\mathbf{B}\\
 \mathbf{I}_{n-r}\end{pmatrix}\mathbf{u}=\mathbf{0}\Rightarrow\begin{pmatrix}-%
 \mathbf{B}\mathbf{u}\\
 \mathbf{u}\end{pmatrix}=\begin{pmatrix}\mathbf{0}\\
 \mathbf{0}\end{pmatrix}\Rightarrow\mathbf{u}=\mathbf{0}\;.   Therefore, the column vectors of X constitute a set of n − r linearly independent solutions for Ax = 0 .  We next prove that any solution of  must be a linear combination of the columns of X For this, let    𝐮  =   (      𝐮  1        𝐮  2      )       𝐮     subscript  𝐮  1      subscript  𝐮  2       \displaystyle\mathbf{u}=\begin{pmatrix}\mathbf{u}_{1}\\
 \mathbf{u}_{2}\end{pmatrix}   be any vector such that . Note that since the columns of A 1 are linearly independent, implies . Therefore,      𝐀𝐮  =  𝟎  ⇒   [   𝐀  1   :   𝐀  1   𝐁  ]    (      𝐮  1        𝐮  2      )   =  𝟎  ⇒   𝐀  1    (   𝐮  1   +   𝐁𝐮  2   )   =  𝟎  ⇒   𝐮  1   +   𝐁𝐮  2   =  𝟎  ⇒   𝐮  1   =  -   𝐁𝐮  2      fragments  Au   0  normal-⇒   fragments  normal-[   subscript  𝐀  1   normal-:   subscript  𝐀  1   B  normal-]      subscript  𝐮  1      subscript  𝐮  2      0  normal-⇒   subscript  𝐀  1    fragments  normal-(   subscript  𝐮  1     subscript  𝐁𝐮  2   normal-)    0  normal-⇒   subscript  𝐮  1     subscript  𝐁𝐮  2    0  normal-⇒   subscript  𝐮  1      subscript  𝐁𝐮  2     \mathbf{A}\mathbf{u}=\mathbf{0}\Rightarrow[\mathbf{A}_{1}:\mathbf{A}_{1}%
 \mathbf{B}]\begin{pmatrix}\mathbf{u}_{1}\\
 \mathbf{u}_{2}\end{pmatrix}=\mathbf{0}\Rightarrow\mathbf{A}_{1}(\mathbf{u}_{1}%
 +\mathbf{B}\mathbf{u}_{2})=\mathbf{0}\Rightarrow\mathbf{u}_{1}+\mathbf{B}%
 \mathbf{u}_{2}=\mathbf{0}\Rightarrow\mathbf{u}_{1}=-\mathbf{B}\mathbf{u}_{2}           ⇒  𝐮  =   (      𝐮  1        𝐮  2      )   =    (      -  𝐁        𝐈   n  -  r       )    𝐮  2    =   𝐗𝐮  2    .       normal-⇒  absent  𝐮          subscript  𝐮  1      subscript  𝐮  2                𝐁      subscript  𝐈    n  r       subscript  𝐮  2          subscript  𝐗𝐮  2      \Rightarrow\mathbf{u}=\begin{pmatrix}\mathbf{u}_{1}\\
 \mathbf{u}_{2}\end{pmatrix}=\begin{pmatrix}-\mathbf{B}\\
 \mathbf{I}_{n-r}\end{pmatrix}\mathbf{u}_{2}=\mathbf{X}\mathbf{u}_{2}.   This proves that any vector u that is a solution of  must be a linear combination of the  special solutions given by the columns of X . And we have already seen that the columns of X are linearly independent. Hence, the columns of X constitute a basis for the null space of A . Therefore, the nullity of A is . Since r equals rank of A , it follows that . QED.  Reformulations and generalizations  This theorem is a statement of the first isomorphism theorem of algebra to the case of vector spaces; it generalizes to the splitting lemma .  In more modern language, the theorem can also be phrased as follows: if   0 → U → V → R → 0   is a short exact sequence of vector spaces, then   dim( U ) + dim( R ) = dim( V ).   Here R plays the role of im T and U is ker T , i.e.      0  →    ker    T     →   I  d      V     →  𝑇       im  T    →  0       normal-→  0     kernel    T     I  d   normal-→   V   T  normal-→      im  T      normal-→    0     0\rightarrow\ker T~{}\overset{Id}{\rightarrow}~{}V~{}\overset{T}{\rightarrow}~%
 {}\operatorname{im}T\rightarrow 0     In the finite-dimensional case, this formulation is susceptible to a generalization: if   0 → V 1 → V 2 → ... → V r → 0   is an exact sequence of finite-dimensional vector spaces, then        ∑   i  =  1   r      (   -  1   )   i    dim   (   V  i   )      =  0.        superscript   subscript     i  1    r      superscript    1   i    dimension   subscript  V  i      0.    \sum_{i=1}^{r}(-1)^{i}\dim(V_{i})=0.     The rank–nullity theorem for finite-dimensional vector spaces may also be formulated in terms of the index of a linear map. The index of a linear map , where V and W are finite-dimensional, is defined by   index T = dim(ker T ) − dim( coker  T ).   Intuitively, dim(ker T ) is the number of independent solutions x of the equation , and dim(coker T ) is the number of independent restrictions that have to be put on y to make  solvable. The rank–nullity theorem for finite-dimensional vector spaces is equivalent to the statement   index T = dim( V ) − dim( W ).   We see that we can easily read off the index of the linear map T from the involved spaces, without any need to analyze T in detail. This effect also occurs in a much deeper result: the Atiyah–Singer index theorem states that the index of certain differential operators can be read off the geometry of the involved spaces.  Notes    References    .   "  Category:Theorems in linear algebra  Category:Isomorphism theorems  Category:Articles containing proofs     , page 199. ↩     