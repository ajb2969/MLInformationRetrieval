<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1687">History of entropy</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>History of entropy</h1>
<hr/>

<p>The concept of <strong><a class="uri" href="entropy" title="wikilink">entropy</a></strong> developed in response to the observation that a certain amount of functional energy released from <a href="combustion_reactions" title="wikilink">combustion reactions</a> is always lost to dissipation or friction and is thus not transformed into <a href="Work_(thermodynamics)" title="wikilink">useful work</a>. Early heat-powered engines such as <a href="Thomas_Savery" title="wikilink">Thomas Savery</a>'s (1698), the <a href="Newcomen_engine" title="wikilink">Newcomen engine</a> (1712) and the Cugnot <a href="steam_tricycle" title="wikilink">steam tricycle</a> (1769) were inefficient, converting less than two percent of the input energy into useful <a href="work_output" title="wikilink">work output</a>; a great deal of useful energy was dissipated or lost. Over the next two centuries, physicists investigated this puzzle of lost energy; the result was the concept of <a class="uri" href="entropy" title="wikilink">entropy</a>.</p>

<p>In the early 1850s, <a href="Rudolf_Clausius" title="wikilink">Rudolf Clausius</a> set forth the concept of the <a href="thermodynamic_system" title="wikilink">thermodynamic system</a> and posited the argument that in any <a href="irreversible_process" title="wikilink">irreversible process</a> a small amount of <a class="uri" href="heat" title="wikilink">heat</a> energy <em>δQ</em> is incrementally dissipated across the system boundary. Clausius continued to develop his ideas of lost energy, and coined the term <em>entropy</em>.</p>

<p>Since the mid-20th century the concept of entropy has found application in the field of <a href="information_theory" title="wikilink">information theory</a>, describing an analogous loss of data in information transmission systems.</p>
<h2 id="classical-thermodynamic-views">Classical thermodynamic views</h2>

<p>In 1803, mathematician <a href="Lazare_Carnot" title="wikilink">Lazare Carnot</a> published a work entitled <em>Fundamental Principles of Equilibrium and Movement</em>. This work includes a discussion on the efficiency of fundamental machines, i.e. pulleys and inclined planes. Lazare Carnot saw through all the details of the mechanisms to develop a general discussion on the conservation of mechanical energy. Over the next three decades, Lazare Carnot's theorem was taken as a statement that in any machine the accelerations and shocks of the moving parts all represent losses of <em>moment of activity</em>, i.e. the <a href="Work_(thermodynamics)" title="wikilink">useful work</a> done. From this Lazare drew the inference that <a href="perpetual_motion" title="wikilink">perpetual motion</a> was impossible. This <em>loss of moment of activity</em> was the first-ever rudimentary statement of the <a href="second_law_of_thermodynamics" title="wikilink">second law of thermodynamics</a> and the concept of 'transformation-energy' or <em>entropy</em>, i.e. energy lost to dissipation and friction.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>

<p>Lazare Carnot died in exile in 1823. During the following year Lazare's son <a href="Nicolas_Léonard_Sadi_Carnot" title="wikilink">Sadi Carnot</a>, having graduated from the <a href="École_Polytechnique" title="wikilink">École Polytechnique</a> training school for engineers, but now living on half-pay with his brother Hippolyte in a small apartment in Paris, wrote <em><a href="Reflections_on_the_Motive_Power_of_Fire" title="wikilink">Reflections on the Motive Power of Fire</a></em>. In this book, Sadi visualized an <a href="Carnot_heat_engine" title="wikilink">ideal engine</a> in which any heat (i.e., <a href="Caloric_theory" title="wikilink">caloric</a>) converted into work, could be reinstated by reversing the motion of the cycle, a concept subsequently known as <a href="thermodynamic_reversibility" title="wikilink">thermodynamic reversibility</a>. Building on his father's work, Sadi postulated the concept that "some caloric is always lost" in the conversion into work, even in his idealized reversible heat engine, which excluded frictional losses and other losses due to the imperfections of any real machine. He also discovered that this idealized efficiency was dependent only on the temperatures of the heat reservoirs between which the engine was working, and not on the types of working fluids. Any real <a href="heat_engine" title="wikilink">heat engine</a> could not realize the <a href="Carnot_cycle" title="wikilink">Carnot cycle</a>'s reversibility, and was condemned to be even less efficient. This loss of usable caloric was a precursory form of the increase in entropy as we now know it. Though formulated in terms of caloric, rather than entropy, this was an early insight into the <a href="second_law_of_thermodynamics" title="wikilink">second law of thermodynamics</a>.</p>
<h2 id="definition">1854 definition</h2>

<p> In his 1854 memoir, Clausius first develops the concepts of <em>interior work</em>, i.e. that "which the atoms of the body exert upon each other", and <em>exterior work</em>, i.e. that "which arise from foreign influences [to] which the body may be exposed", which may act on a working body of fluid or gas, typically functioning to work a piston. He then discusses the three categories into which heat <em>Q</em> may be divided:</p>
<ol>
<li>Heat employed in increasing the heat actually existing in the body.</li>
<li>Heat employed in producing the interior work.</li>
<li>Heat employed in producing the exterior work.</li>
</ol>

<p>Building on this logic, and following a mathematical presentation of the <em>first fundamental theorem</em>, Clausius then presented the first-ever mathematical formulation of entropy, although at this point in the development of his theories he called it "equivalence-value", perhaps referring to the concept of the <a href="mechanical_equivalent_of_heat" title="wikilink">mechanical equivalent of heat</a> which was developing at the time rather than entropy, a term which was to come into use later.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> He stated:<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<blockquote>

<p>the <em>second fundamental theorem</em> in the mechanical <a href="theory_of_heat" title="wikilink">theory of heat</a> may thus be enunciated:</p>

<p>If two transformations which, without necessitating any other permanent change, can mutually replace one another, be called equivalent, then the generations of the quantity of heat <em>Q</em> from <a href="work_(thermodynamics)" title="wikilink">work</a> at the temperature <em>T</em>, has the <em>equivalence-value</em>:</p>
<dl>
<dd><dl>
<dd>

<math display="inline" id="History_of_entropy:0">
 <semantics>
  <mfrac>
   <mi>Q</mi>
   <mi>T</mi>
  </mfrac>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <ci>Q</ci>
    <ci>T</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{Q}{T}
  </annotation>
 </semantics>
</math>


</dd>
</dl>
</dd>
</dl>

<p>and the passage of the quantity of heat <em>Q</em> from the <a class="uri" href="temperature" title="wikilink">temperature</a> <em>T<sub>1</sub></em> to the temperature <em>T<sub>2</sub></em>, has the equivalence-value:</p>
<dl>
<dd><dl>
<dd>

<math display="inline" id="History_of_entropy:1">
 <semantics>
  <mrow>
   <mi>Q</mi>
   <mrow>
    <mo>(</mo>
    <mrow>
     <mfrac>
      <mn>1</mn>
      <msub>
       <mi>T</mi>
       <mn>2</mn>
      </msub>
     </mfrac>
     <mo>-</mo>
     <mfrac>
      <mn>1</mn>
      <msub>
       <mi>T</mi>
       <mn>1</mn>
      </msub>
     </mfrac>
    </mrow>
    <mo>)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>Q</ci>
    <apply>
     <minus></minus>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>T</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>T</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Q\left(\frac{1}{T_{2}}-\frac{1}{T_{1}}\right)
  </annotation>
 </semantics>
</math>


</dd>
</dl>
</dd>
</dl>

<p>wherein <em>T</em> is a function of the temperature, independent of the nature of the process by which the transformation is effected.</p>
</blockquote>

<p>In modern terminology, we think of this equivalence-value as "entropy", symbolized by <em>S</em>. Thus, using the above description, we can calculate the entropy change Δ<em>S</em> for the passage of the quantity of heat <em>Q</em> from the <a class="uri" href="temperature" title="wikilink">temperature</a> <em>T<sub>1</sub></em>, through the "working body" of fluid (see <a href="heat_engine" title="wikilink">heat engine</a>), which was typically a body of steam, to the temperature <em>T<sub>2</sub></em> as shown below:  If we make the assignment:</p>

<p>

<math display="block" id="History_of_entropy:2">
 <semantics>
  <mrow>
   <mi>S</mi>
   <mo>=</mo>
   <mfrac>
    <mi>Q</mi>
    <mi>T</mi>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>S</ci>
    <apply>
     <divide></divide>
     <ci>Q</ci>
     <ci>T</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S=\frac{Q}{T}
  </annotation>
 </semantics>
</math>

</p>

<p>Then, the entropy change or "equivalence-value" for this transformation is:</p>

<p>

<math display="block" id="History_of_entropy:3">
 <semantics>
  <mrow>
   <mrow>
    <mi mathvariant="normal">Δ</mi>
    <mi>S</mi>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>S</mi>
     <mi>final</mi>
    </msub>
    <mo>-</mo>
    <mpadded width="+1.7pt">
     <msub>
      <mi>S</mi>
      <mi>initial</mi>
     </msub>
    </mpadded>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>normal-Δ</ci>
     <ci>S</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>S</ci>
      <ci>final</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>S</ci>
      <ci>initial</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Delta S=S_{\rm final}-S_{\rm initial}\,
  </annotation>
 </semantics>
</math>

</p>

<p>which equals:</p>

<p>

<math display="block" id="History_of_entropy:4">
 <semantics>
  <mrow>
   <mrow>
    <mi mathvariant="normal">Δ</mi>
    <mi>S</mi>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>(</mo>
    <mrow>
     <mfrac>
      <mi>Q</mi>
      <msub>
       <mi>T</mi>
       <mn>2</mn>
      </msub>
     </mfrac>
     <mo>-</mo>
     <mfrac>
      <mi>Q</mi>
      <msub>
       <mi>T</mi>
       <mn>1</mn>
      </msub>
     </mfrac>
    </mrow>
    <mo>)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>normal-Δ</ci>
     <ci>S</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <divide></divide>
      <ci>Q</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>T</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
     <apply>
      <divide></divide>
      <ci>Q</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>T</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Delta S=\left(\frac{Q}{T_{2}}-\frac{Q}{T_{1}}\right)
  </annotation>
 </semantics>
</math>

</p>

<p>and by factoring out Q, we have the following form, as was derived by Clausius:</p>

<p>

<math display="block" id="History_of_entropy:5">
 <semantics>
  <mrow>
   <mrow>
    <mi mathvariant="normal">Δ</mi>
    <mi>S</mi>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>Q</mi>
    <mrow>
     <mo>(</mo>
     <mrow>
      <mfrac>
       <mn>1</mn>
       <msub>
        <mi>T</mi>
        <mn>2</mn>
       </msub>
      </mfrac>
      <mo>-</mo>
      <mfrac>
       <mn>1</mn>
       <msub>
        <mi>T</mi>
        <mn>1</mn>
       </msub>
      </mfrac>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>normal-Δ</ci>
     <ci>S</ci>
    </apply>
    <apply>
     <times></times>
     <ci>Q</ci>
     <apply>
      <minus></minus>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>T</ci>
        <cn type="integer">2</cn>
       </apply>
      </apply>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>T</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Delta S=Q\left(\frac{1}{T_{2}}-\frac{1}{T_{1}}\right)
  </annotation>
 </semantics>
</math>

</p>
<h2 id="definition-1">1856 definition</h2>

<p>In 1856, Clausius stated what he called the "second fundamental theorem in the <a href="mechanical_theory_of_heat" title="wikilink">mechanical theory of heat</a>" in the following form:</p>

<p>

<math display="block" id="History_of_entropy:6">
 <semantics>
  <mrow>
   <mrow>
    <mo largeop="true" symmetric="true">∫</mo>
    <mfrac>
     <mrow>
      <mi>δ</mi>
      <mi>Q</mi>
     </mrow>
     <mi>T</mi>
    </mfrac>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mi>N</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <int></int>
     <apply>
      <divide></divide>
      <apply>
       <times></times>
       <ci>δ</ci>
       <ci>Q</ci>
      </apply>
      <ci>T</ci>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <ci>N</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \int\frac{\delta Q}{T}=-N
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>N</em> is the "equivalence-value" of all uncompensated transformations involved in a cyclical process. This equivalence-value was a precursory formulation of entropy.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<h2 id="definition-2">1862 definition</h2>

<p>In 1862, Clausius stated what he calls the "theorem respecting the equivalence-values of the transformations" or what is now known as the <a href="second_law_of_thermodynamics" title="wikilink">second law of thermodynamics</a>, as such:</p>
<dl>
<dd><em>The algebraic sum of all the transformations occurring in a cyclical process can only be positive, or, as an extreme case, equal to nothing.</em>
</dd>
</dl>

<p>Quantitatively, Clausius states the mathematical expression for this theorem is as follows. Let <em>δQ</em> be an element of the heat given up by the body to any reservoir of heat during its own changes, heat which it may absorb from a reservoir being here reckoned as negative, and <em>T</em> the <a href="absolute_temperature" title="wikilink">absolute temperature</a> of the body at the moment of giving up this heat, then the equation:</p>

<p>

<math display="block" id="History_of_entropy:7">
 <semantics>
  <mrow>
   <mrow>
    <mo largeop="true" symmetric="true">∫</mo>
    <mfrac>
     <mrow>
      <mi>δ</mi>
      <mi>Q</mi>
     </mrow>
     <mi>T</mi>
    </mfrac>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <int></int>
     <apply>
      <divide></divide>
      <apply>
       <times></times>
       <ci>δ</ci>
       <ci>Q</ci>
      </apply>
      <ci>T</ci>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \int\frac{\delta Q}{T}=0
  </annotation>
 </semantics>
</math>

</p>

<p>must be true for every reversible cyclical process, and the relation:</p>

<p>

<math display="block" id="History_of_entropy:8">
 <semantics>
  <mrow>
   <mrow>
    <mo largeop="true" symmetric="true">∫</mo>
    <mfrac>
     <mrow>
      <mi>δ</mi>
      <mi>Q</mi>
     </mrow>
     <mi>T</mi>
    </mfrac>
   </mrow>
   <mo>≥</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <geq></geq>
    <apply>
     <int></int>
     <apply>
      <divide></divide>
      <apply>
       <times></times>
       <ci>δ</ci>
       <ci>Q</ci>
      </apply>
      <ci>T</ci>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \int\frac{\delta Q}{T}\geq 0
  </annotation>
 </semantics>
</math>

</p>

<p>must hold good for every cyclical process which is in any way possible. This was an early formulation of the second law and one of the original forms of the concept of entropy.</p>
<h2 id="definition-3">1865 definition</h2>

<p>In 1865, Clausius gave irreversible heat loss, or what he had previously been calling "equivalence-value", a name:<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> </p>

<p>Although Clausius did not specify why he chose the symbol "S" to represent entropy, it is arguable that Clausius chose "S" in honor of <a href="Nicolas_Léonard_Sadi_Carnot" title="wikilink">Sadi Carnot</a>, to whose 1824 article Clausius devoted over 15 years of work and research. On the first page of his original 1850 article "On the Motive Power of Heat, and on the Laws which can be Deduced from it for the Theory of Heat", Clausius calls Carnot the most important of the researchers in the <a href="theory_of_heat" title="wikilink">theory of heat</a>.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h2 id="later-developments">Later developments</h2>

<p>In 1876, physicist <a href="J._Willard_Gibbs" title="wikilink">J. Willard Gibbs</a>, building on the work of Clausius, <a href="Hermann_von_Helmholtz" title="wikilink">Hermann von Helmholtz</a> and others, proposed that the measurement of "available energy" Δ<em>G</em> in a thermodynamic system could be mathematically accounted for by subtracting the "energy loss" <em>T</em>Δ<em>S</em> from total energy change of the system Δ<em>H</em>. These concepts were further developed by <a href="James_Clerk_Maxwell" title="wikilink">James Clerk Maxwell</a> [1871] and <a href="Max_Planck" title="wikilink">Max Planck</a> [1903].</p>
<h2 id="statistical-thermodynamic-views">Statistical thermodynamic views</h2>

<p>In 1877, <a href="Ludwig_Boltzmann" title="wikilink">Ludwig Boltzmann</a> developed a statistical mechanical evaluation of the entropy 

<math display="inline" id="History_of_entropy:9">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

, of a body in its own given macrostate of internal thermodynamic equilibrium. It may be written as:</p>

<p>

<math display="block" id="History_of_entropy:10">
 <semantics>
  <mrow>
   <mi>S</mi>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>k</mi>
     <mi mathvariant="normal">B</mi>
    </msub>
    <mrow>
     <mi>ln</mi>
     <mpadded width="-1.7pt">
      <mi mathvariant="normal">Ω</mi>
     </mpadded>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>S</ci>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>k</ci>
      <ci>normal-B</ci>
     </apply>
     <apply>
      <ln></ln>
      <ci>normal-Ω</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S=k_{\rm B}\ln\Omega\!
  </annotation>
 </semantics>
</math>

 where</p>
<dl>
<dd><mtpl></mtpl> denotes <a href="Boltzmann_constant" title="wikilink">Boltzmann's constant</a> and
</dd>
<dd>

<math display="inline" id="History_of_entropy:11">
 <semantics>
  <mi mathvariant="normal">Ω</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-Ω</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Ω
  </annotation>
 </semantics>
</math>

 denotes the number of microstates consistent with the given equilibrium macrostate.
</dd>
</dl>

<p>Boltzmann himself did not actually write this formula expressed with the named constant <mtpl></mtpl>, which is due to Planck's reading of Boltzmann.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>

<p>Boltzmann saw entropy as a measure of statistical "mixedupness" or disorder. This concept was soon refined by <a href="J._Willard_Gibbs" title="wikilink">J. Willard Gibbs</a>, and is now regarded as one of the cornerstones of the theory of <a href="statistical_mechanics" title="wikilink">statistical mechanics</a>.</p>
<h2 id="information-theory">Information theory</h2>

<p>An analog to <em>thermodynamic entropy</em> is <strong>information entropy</strong>. In 1948, while working at <a href="Bell_Telephone_Company" title="wikilink">Bell Telephone</a> Laboratories electrical engineer <a href="Claude_Shannon" title="wikilink">Claude Shannon</a> set out to mathematically quantify the statistical nature of "lost information" in phone-line signals. To do this, Shannon developed the very general concept of <a href="information_entropy" title="wikilink">information entropy</a>, a fundamental cornerstone of <a href="information_theory" title="wikilink">information theory</a>. Although the story varies, initially it seems that Shannon was not particularly aware of the close similarity between his new quantity and earlier work in thermodynamics. In 1949, however, when Shannon had been working on his equations for some time, he happened to visit the mathematician <a href="John_von_Neumann" title="wikilink">John von Neumann</a>. During their discussions, regarding what Shannon should call the "measure of uncertainty" or attenuation in phone-line signals with reference to his new information theory, according to one source:<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<dl>
<dd>
</dd>
</dl>

<p>According to another source, when von Neumann asked him how he was getting on with his information theory, Shannon replied:<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<dl>
<dd>
</dd>
</dl>

<p>In 1948 Shannon published his famous paper <em>A Mathematical Theory of Communication</em>, in which he devoted a section to what he calls Choice, Uncertainty, and Entropy.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> In this section, Shannon introduces an <em>H function</em> of the following form:</p>

<p>

<math display="block" id="History_of_entropy:12">
 <semantics>
  <mrow>
   <mrow>
    <mi>H</mi>
    <mo>=</mo>
    <mrow>
     <mo>-</mo>
     <mrow>
      <mi>K</mi>
      <mrow>
       <munderover>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mrow>
         <mi>i</mi>
         <mo>=</mo>
         <mn>1</mn>
        </mrow>
        <mi>k</mi>
       </munderover>
       <mrow>
        <mi>p</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>i</mi>
         <mo stretchy="false">)</mo>
        </mrow>
        <mrow>
         <mi>log</mi>
         <mi>p</mi>
        </mrow>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>i</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>H</ci>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <ci>K</ci>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>k</ci>
       </apply>
       <apply>
        <times></times>
        <ci>p</ci>
        <ci>i</ci>
        <apply>
         <log></log>
         <ci>p</ci>
        </apply>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H=-K\sum_{i=1}^{k}p(i)\log p(i),
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>K</em> is a positive constant. Shannon then states that "any quantity of this form, where <em>K</em> merely amounts to a choice of a unit of measurement, plays a central role in information theory as measures of information, choice, and uncertainty." Then, as an example of how this expression applies in a number of different fields, he references R.C. Tolman's 1938 <em>Principles of Statistical Mechanics</em>, stating that "the form of <em>H</em> will be recognized as that of entropy as defined in certain formulations of statistical mechanics where <em>p<sub>i</sub></em> is the probability of a system being in cell <em>i</em> of its phase space… <em>H</em> is then, for example, the <em>H</em> in Boltzmann's famous <a href="H-theorem" title="wikilink">H theorem</a>." As such, over the last fifty years, ever since this statement was made, people have been overlapping the two concepts or even stating that they are exactly the same.</p>

<p>Shannon's information entropy is a much more general concept than statistical thermodynamic entropy. Information entropy is present whenever there are unknown quantities that can be described only by a probability distribution. In a series of papers by <a href="E._T._Jaynes" title="wikilink">E. T. Jaynes</a> starting in 1957,<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a><a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> the statistical thermodynamic entropy can be seen as just a particular application of Shannon's information entropy to the probabilities of particular microstates of a system occurring in order to produce a particular macrostate.</p>
<h2 id="popular-use">Popular use</h2>

<p>The term entropy is often used in popular language to denote a variety of unrelated phenomena. One example is the concept of <strong>corporate entropy</strong> as put forward somewhat humorously by authors Tom DeMarco and Timothy Lister in their 1987 classic publication <em>Peopleware</em>, a book on growing and managing productive teams and successful software projects. Here, they view energy waste as red tape and business team inefficiency as a form of entropy, i.e. energy lost to waste. This concept has caught on and is now common jargon in business schools.</p>

<p>In another example, entropy plays the main villain in <a href="Isaac_Asimov" title="wikilink">Isaac Asimov</a>'s short story <a href="The_Last_Question" title="wikilink">The Last Question</a> (first copyrighted in 1956). The story plays with the idea that when tampering with the Second law of thermodynamics, entropy must always increase.</p>
<h2 id="terminology-overlap">Terminology overlap</h2>

<p>When necessary, to disambiguate between the statistical thermodynamic concept of entropy, and entropy-like formulae put forward by different researchers, the statistical thermodynamic entropy is most properly referred to as the <strong><a href="Gibbs_entropy" title="wikilink">Gibbs entropy</a></strong>. The terms <em>Boltzmann–Gibbs entropy</em> or <em>BG entropy</em>, and <em>Boltzmann–Gibbs–Shannon entropy</em> or <em>BGS entropy</em> are also seen in the literature.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a class="uri" href="Entropy" title="wikilink">Entropy</a></li>
<li><a class="uri" href="Enthalpy" title="wikilink">Enthalpy</a></li>
<li><a href="Thermodynamic_free_energy" title="wikilink">Thermodynamic free energy</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="Max_Jammer" title="wikilink">Max Jammer</a> (1973). <a href="http://etext.lib.virginia.edu/cgi-local/DHI/dhi.cgi?id=dv2-12"><em>Dictionary of the History of Ideas</em>: Entropy</a></li>
</ul>

<p>"</p>

<p><a href="Category:Thermodynamic_entropy" title="wikilink">Category:Thermodynamic entropy</a> <a href="Category:History_of_thermodynamics" title="wikilink">Category:History of thermodynamics</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><em>Mechanical Theory of Heat</em>, by <a href="Rudolf_Clausius" title="wikilink">Rudolf Clausius</a>, 1850-1865<a href="#fnref2">↩</a></li>
<li id="fn3">Published in <em>Poggendoff's Annalen</em>, December 1854, vol. xciii. p. 481; translated in the <em>Journal de Mathematiques</em>, vol. xx. Paris, 1855, and in the <em>Philosophical Magazine</em>, August 1856, s. 4. vol. xii, p. 81<a href="#fnref3">↩</a></li>
<li id="fn4">Clausius, Rudolf. (1856). "<em>On the Application of the Mechanical theory of Heat to the Steam-Engine</em>." as found in: Clausius, R. (1865). <a href="http://books.google.com/books?id=8LIEAAAAYAAJ">The Mechanical Theory of Heat – with its Applications to the Steam Engine and to Physical Properties of Bodies</a>. London: John van Voorst, 1 Paternoster Row. MDCCCLXVII.<a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a class="uri" href="OED" title="wikilink">OED</a>, Second Edition, 1989, "<em>Clausius (Pogg. Ann. CXXV. 390), assuming (unhistorically) the etymological sense of energy to be ‘work-contents’ (werk-inhalt), devised the term entropy as a corresponding designation for the ‘transformation-contents’ (verwandlungsinhalt) of a system"</em><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9">M. Tribus, E.C. McIrvine, "Energy and information", <em>Scientific American</em>, 224 (September 1971).<a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11">C.E. Shannon, "A Mathematical Theory of Communication", <em><a href="Bell_System_Technical_Journal" title="wikilink">Bell System Technical Journal</a></em>, vol. 27, pp. 379-423, 623-656, July, October, 1948, <a href="http://cm.bell-labs.com/cm/ms/what/shannonday/paper.html">Eprint</a>, <a href="http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf">PDF</a><a href="#fnref11">↩</a></li>
<li id="fn12">E. T. Jaynes (1957) <a href="http://bayes.wustl.edu/etj/articles/theory.1.pdf">Information theory and statistical mechanics</a>, <em>Physical Review</em> <strong>106</strong>:620<a href="#fnref12">↩</a></li>
<li id="fn13">E. T. Jaynes (1957) <a href="http://bayes.wustl.edu/etj/articles/theory.2.pdf">Information theory and statistical mechanics II</a>, <em>Physical Review</em> <strong>108</strong>:171<a href="#fnref13">↩</a></li>
</ol>
</section>
</body>
</html>
