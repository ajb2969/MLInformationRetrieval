   Generalized filtering      Generalized filtering  '''Generalized filtering''' is a generic [[Recursive Bayesian estimation|Bayesian filtering]] scheme for nonlinear state-space models. K Friston, K Stephan, B Li, and J. Daunizeau, "[http://www.fil.ion.ucl.ac.uk/~karl/Generalised%20Filtering.pdf Generalised Filtering]," [[Mathematical Problems in Engineering]], vol. vol., 2010, p. 621670, 2010. It is based on a [[Principle of least action|variational principle of least action]], formulated in generalized coordinates of motion. 1 Generalized filtering furnishes posterior densities over hidden states (and parameters) generating observed data using a generalized gradient descent on variational free energy, under the Laplace assumption . Unlike classical (e.g., Kalman-Bucy or particle ) filtering, generalized filtering eschews Markovian assumptions about random fluctuations. Furthermore, it operates online, assimilating data to approximate the posterior density over unknown quantities, without the need for a backward pass. Special cases include variational filtering , 2  dynamic expectation maximization 3 and generalized predictive coding .  Definition  Definition : Generalized filtering rests on the tuple     (  Œ©  ,  U  ,  X  ,  S  ,  p  ,  q  )     normal-Œ©  U  X  S  p  q    (\Omega,U,X,S,p,q)   :   A sample space    Œ©   normal-Œ©   \Omega   from which random fluctuations    œâ  ‚àà  Œ©      œâ  normal-Œ©    \omega\in\Omega   are drawn    Control states     U  ‚àà  ‚Ñù      U  ‚Ñù    U\in\mathbb{R}   ‚Äì that act as external causes, input or forcing terms    Hidden states     X  :    X  √ó  U  √ó  Œ©   ‚Üí  ‚Ñù      normal-:  X   normal-‚Üí    X  U  normal-Œ©   ‚Ñù     X:X\times U\times\Omega\to\mathbb{R}   ‚Äì that cause sensor states and depend on control states    Sensor states     S  :    X  √ó  U  √ó  Œ©   ‚Üí  ‚Ñù      normal-:  S   normal-‚Üí    X  U  normal-Œ©   ‚Ñù     S:X\times U\times\Omega\to\mathbb{R}   ‚Äì a probabilistic mapping from hidden and control states    Generative density     p   (   s  ~   ,   x  ~   ,   u  ~   ‚à£  m  )      fragments  p   fragments  normal-(   normal-~  s   normal-,   normal-~  x   normal-,   normal-~  u   normal-‚à£  m  normal-)     p(\tilde{s},\tilde{x},\tilde{u}\mid m)   ‚Äì over sensory, hidden and control states under a generative model    Variational density     q   (   x  ~   ,   u  ~   ‚à£  m  )      fragments  q   fragments  normal-(   normal-~  x   normal-,   normal-~  u   normal-‚à£  m  normal-)     q(\tilde{x},\tilde{u}\mid m)   ‚Äì over hidden and control states with mean     Œº  ~   ‚àà  ‚Ñù       normal-~  Œº   ‚Ñù    \tilde{\mu}\in\mathbb{R}      Here ~ denotes a variable in generalized coordinates of motion     u  ~   =    [  u  ,   u  ‚Ä≤   ,   u  ‚Ä≤‚Ä≤   ,  ‚Ä¶  ]   T        normal-~  u    superscript   u   superscript  u  normal-‚Ä≤    superscript  u  ‚Ä≤‚Ä≤   normal-‚Ä¶   T     \tilde{u}=[u,u^{\prime},u^{\prime\prime},\ldots]^{T}     Generalized filtering  The objective is to approximate the posterior density over hidden and control states, given sensor states and a generative model ‚Äì and estimate the (path integral of) model evidence     p   (   s  ~    (  t  )   |  m  )      fragments  p   fragments  normal-(   normal-~  s    fragments  normal-(  t  normal-)   normal-|  m  normal-)     p(\tilde{s}(t)|m)   to compare different models. This generally involves an intractable marginalization over hidden states, so model evidence (or marginal likelihood) is replaced with a variational free energy bound. 4 Given the following definitions:        Œº  ~    (  t  )    =      arg   min    Œº  ~     {   F   (    s  ~    (  t  )    ,   Œº  ~   )    }           normal-~  Œº   t       normal-~  Œº     arg  min       F      normal-~  s   t    normal-~  Œº         \tilde{\mu}(t)=\underset{\tilde{\mu}}{\operatorname{arg\,min}}\{F(\tilde{{s}}(%
 t),\tilde{{\mu}})\}         G   (   s  ~   ,   x  ~   ,   u  ~   )   =  -  ln  p   (   s  ~   ,   x  ~   ,   u  ~   |  m  )      fragments  G   fragments  normal-(   normal-~  s   normal-,   normal-~  x   normal-,   normal-~  u   normal-)      p   fragments  normal-(   normal-~  s   normal-,   normal-~  x   normal-,   normal-~  u   normal-|  m  normal-)     G(\tilde{s},\tilde{x},\tilde{u})=-\ln p(\tilde{s},\tilde{x},\tilde{u}|m)     Denote the Shannon entropy of the density   q   q   q   by     H   [  q  ]    =    E  q    [   -   log   (  q  )     ]          H   delimited-[]  q       subscript  E  q    delimited-[]      q        H[q]=E_{q}[-\log(q)]   . We can then write the variational free energy in two ways:      F   (   s  ~   ,   Œº  ~   )   =   E  q    [  G   (   s  ~   ,  ‚ãÖ  ,  ‚ãÖ  )   ]   -  H   [  q   (  ‚ãÖ  ,  ‚ãÖ  |   Œº  ~   )   ]   =  -  ln  p   (   s  ~   |  m  )   +   D   K  L     [  q   (  ‚ãÖ  ,  ‚ãÖ  |   Œº  ~   )   |  |  p   (  ‚ãÖ  ,  ‚ãÖ  |   s  ~   ,  m  )   ]      fragments  F   fragments  normal-(   normal-~  s   normal-,   normal-~  Œº   normal-)     subscript  E  q    fragments  normal-[  G   fragments  normal-(   normal-~  s   normal-,  normal-‚ãÖ  normal-,  normal-‚ãÖ  normal-)   normal-]    H   fragments  normal-[  q   fragments  normal-(  normal-‚ãÖ  normal-,  normal-‚ãÖ  normal-|   normal-~  Œº   normal-)   normal-]      p   fragments  normal-(   normal-~  s   normal-|  m  normal-)     subscript  D    K  L     fragments  normal-[  q   fragments  normal-(  normal-‚ãÖ  normal-,  normal-‚ãÖ  normal-|   normal-~  Œº   normal-)   normal-|  normal-|  p   fragments  normal-(  normal-‚ãÖ  normal-,  normal-‚ãÖ  normal-|   normal-~  s   normal-,  m  normal-)   normal-]     F(\tilde{s},\tilde{\mu})=E_{q}[G(\tilde{s},\cdot,\cdot)]-H[q(\cdot,\cdot|%
 \tilde{\mu})]=-\ln p(\tilde{s}|m)+D_{KL}[q(\cdot,\cdot|\tilde{\mu})||p(\cdot,%
 \cdot|\tilde{s},m)]     The second equality shows that minimizing variational free energy (i) minimizes the Kullback-Leibler divergence between the variational and true posterior density and (ii) renders the variational free energy (a bound approximation to) the negative log evidence (because the divergence can never be less than zero). 5 Under the Laplace assumption    q   (   x  ~   ,   u  ~   |   Œº  ~   )   =  ùí©   (   Œº  ~   ,  C  )      fragments  q   fragments  normal-(   normal-~  x   normal-,   normal-~  u   normal-|   normal-~  Œº   normal-)    N   fragments  normal-(   normal-~  Œº   normal-,  C  normal-)     q(\tilde{x},\tilde{{u}}|\tilde{\mu})=\mathcal{N}(\tilde{\mu},C)   the variational density is Gaussian and the precision that minimizes free energy is     C   -  1    =  Œ†  =     ‚àÇ    Œº  ~    Œº  ~     G    (   Œº  ~   )           superscript  C    1    normal-Œ†           subscript      normal-~  Œº    normal-~  Œº     G    normal-~  Œº       C^{-1}=\Pi=\partial_{\tilde{\mu}\tilde{\mu}}G(\tilde{\mu})   . This means that free-energy can be expressed in terms of the variational mean 6 (omitting constants):      F  =    G   (   Œº  ~   )    +     1  2     ln   |     ‚àÇ    Œº  ~    Œº  ~     G    (   Œº  ~   )    |          F      G   normal-~  Œº        1  2           subscript      normal-~  Œº    normal-~  Œº     G    normal-~  Œº          F=G(\tilde{\mu})+\textstyle{1\over 2}\ln|\partial_{\tilde{\mu}\tilde{\mu}}G(%
 \tilde{\mu})|     The variational means that minimize the (path integral) of free energy can now be recovered by solving the generalized filter:        Œº  ~   Àô   =    D   Œº  ~    -     ‚àÇ   Œº  ~    F    (   s  ~   ,   Œº  ~   )          normal-Àô   normal-~  Œº        D   normal-~  Œº        subscript    normal-~  Œº    F     normal-~  s    normal-~  Œº        \dot{\tilde{\mu}}=D\tilde{\mu}-\partial_{\tilde{\mu}}F(\tilde{s},\tilde{\mu})     where,   D   D   D   is a block matrix derivative operator of identify matrices such that     D   u  ~    =    [   u  ‚Ä≤   ,   u  ‚Ä≤‚Ä≤   ,  ‚Ä¶  ]   T         D   normal-~  u     superscript    superscript  u  normal-‚Ä≤    superscript  u  ‚Ä≤‚Ä≤   normal-‚Ä¶   T     D\tilde{u}=[u^{\prime},u^{\prime\prime},\ldots]^{T}     Variational basis  Generalized filtering is based on the following lemma: The self-consistent solution to       Œº  ~   Àô   =    D   Œº  ~    -     ‚àÇ   Œº  ~    F    (  s  ,   Œº  ~   )          normal-Àô   normal-~  Œº        D   normal-~  Œº        subscript    normal-~  Œº    F    s   normal-~  Œº        \dot{\tilde{\mu}}=D\tilde{\mu}-\partial_{\tilde{\mu}}F(s,\tilde{\mu})    satisfies the variational principle of stationary action , where action is the path integral of variational free energy      S  =   ‚à´   d   t   F   (    s  ~    (  t  )    ,    Œº  ~    (  t  )    )         S      d  t  F      normal-~  s   t      normal-~  Œº   t        S=\int dt\,F(\tilde{s}(t),\tilde{\mu}(t))     Proof : self-consistency requires the motion of the mean to be the mean of the motion and (by the fundamental lemma of variational calculus )         Œº  ~   Àô   =   D   Œº  ~     ‚áî      ‚àÇ   Œº  ~    F    (   s  ~   ,   Œº  ~   )    =  0   ‚áî     Œ¥   Œº  ~    S   =  0        normal-‚áî     normal-Àô   normal-~  Œº      D   normal-~  Œº           subscript    normal-~  Œº    F     normal-~  s    normal-~  Œº     0     normal-‚áî         subscript  Œ¥   normal-~  Œº    S   0      \dot{\tilde{\mu}}=D\tilde{\mu}\Leftrightarrow\partial_{\tilde{\mu}}F(\tilde{s}%
 ,\tilde{\mu})=0\Leftrightarrow\delta_{\tilde{\mu}}S=0     Put simply, small perturbations to the path of the mean do not change variational free energy and it has the least action of all possible (local) paths.  Remarks : Heuristically, generalized filtering performs a gradient descent on variational free energy in a moving frame of reference        |  ~   Œº   Àô   -   D   Œº  ~     =   -     ‚àÇ   Œº  ~    F    (  s  ,   Œº  ~   )            normal-Àô     normal-~  normal-|   Œº      D   normal-~  Œº           subscript    normal-~  Œº    F    s   normal-~  Œº        \dot{{\tilde{|}{{\mu}}}}-D\tilde{{\mu}}=-\partial_{\tilde{\mu}}F(s,\tilde{\mu})   , where the frame itself minimizes variational free energy. For a related example in statistical physics, see Kerr and Graham 7 who use ensemble dynamics in generalized coordinates to provide a generalized phase-space version of Langevin and associated Fokker-Planck equations.  In practice, generalized filtering uses local linearization  8 over intervals    Œî  t      normal-Œî  t    \Delta t   to recover discrete updates      Œî   Œº  ~       normal-Œî   normal-~  Œº     \displaystyle\Delta\tilde{\mu}     This updates the means of hidden variables at each interval (usually the interval between observations).  Generative (state-space) models in generalized coordinates  Usually, the generative density or model is specified in terms of a nonlinear input-state-output model with continuous nonlinear functions:     s   s   \displaystyle s     The corresponding generalized model (under local linearity assumptions) obtains the from the chain rule          s  ~       =     g  ~    (   x  ~   ,   u  ~   )    +    œâ  ~   s            s      =    g   (  x  ,  u  )    +   œâ  s          s  ‚Ä≤       =     ‚àÇ  x    g  ‚ãÖ   x  ‚Ä≤     +    ‚àÇ  u    g  ‚ãÖ   u  ‚Ä≤     +   œâ  x  ‚Ä≤          s  ‚Ä≤‚Ä≤       =     ‚àÇ  x    g  ‚ãÖ   x  ‚Ä≤‚Ä≤     +    ‚àÇ  u    g  ‚ãÖ   u  ‚Ä≤‚Ä≤     +   œâ  x  ‚Ä≤‚Ä≤          ‚ãÆ          x  ~   Àô       =     f  ~    (   x  ~   ,   u  ~   )    +    œâ  ~   x             x  Àô       =    f   (  x  ,  u  )    +   œâ  x           x  Àô   ‚Ä≤       =     ‚àÇ  x    f  ‚ãÖ   x  ‚Ä≤     +    ‚àÇ  u    f  ‚ãÖ   u  ‚Ä≤     +   œâ  x  ‚Ä≤           x  Àô   ‚Ä≤‚Ä≤       =     ‚àÇ  x    f  ‚ãÖ   x  ‚Ä≤‚Ä≤     +    ‚àÇ  u    f  ‚ãÖ   u  ‚Ä≤‚Ä≤     +   œâ  x  ‚Ä≤‚Ä≤          ‚ãÆ           normal-~  s     absent       normal-~  g     normal-~  x    normal-~  u      subscript   normal-~  œâ   s        missing-subexpression     s    absent      g   x  u     subscript  œâ  s        superscript  s  normal-‚Ä≤     absent      subscript   x    normal-‚ãÖ  g   superscript  x  normal-‚Ä≤       subscript   u    normal-‚ãÖ  g   superscript  u  normal-‚Ä≤      subscript   superscript  œâ  normal-‚Ä≤   x        superscript  s  ‚Ä≤‚Ä≤     absent      subscript   x    normal-‚ãÖ  g   superscript  x  ‚Ä≤‚Ä≤       subscript   u    normal-‚ãÖ  g   superscript  u  ‚Ä≤‚Ä≤      subscript   superscript  œâ  ‚Ä≤‚Ä≤   x        missing-subexpression   normal-‚ãÆ       normal-Àô   normal-~  x      absent       normal-~  f     normal-~  x    normal-~  u      subscript   normal-~  œâ   x        missing-subexpression      normal-Àô  x     absent      f   x  u     subscript  œâ  x        superscript   normal-Àô  x   normal-‚Ä≤     absent      subscript   x    normal-‚ãÖ  f   superscript  x  normal-‚Ä≤       subscript   u    normal-‚ãÖ  f   superscript  u  normal-‚Ä≤      subscript   superscript  œâ  normal-‚Ä≤   x        superscript   normal-Àô  x   ‚Ä≤‚Ä≤     absent      subscript   x    normal-‚ãÖ  f   superscript  x  ‚Ä≤‚Ä≤       subscript   u    normal-‚ãÖ  f   superscript  u  ‚Ä≤‚Ä≤      subscript   superscript  œâ  ‚Ä≤‚Ä≤   x        missing-subexpression   normal-‚ãÆ      \begin{aligned}\displaystyle\tilde{s}&\displaystyle=\tilde{g}(\tilde{x},\tilde%
 {u})+\tilde{\omega}_{s}\\
 \\
 \displaystyle s&\displaystyle=g(x,u)+\omega_{s}\\
 \displaystyle s^{\prime}&\displaystyle=\partial_{x}g\cdot x^{\prime}+\partial_%
 {u}g\cdot u^{\prime}+\omega^{\prime}_{x}\\
 \displaystyle s^{\prime\prime}&\displaystyle=\partial_{x}g\cdot x^{\prime%
 \prime}+\partial_{u}g\cdot u^{\prime\prime}+\omega^{\prime\prime}_{x}\\
 &\displaystyle\vdots\\
 \end{aligned}\qquad\begin{aligned}\displaystyle\dot{\tilde{x}}&\displaystyle=%
 \tilde{f}(\tilde{x},\tilde{u})+\tilde{\omega}_{x}\\
 \\
 \displaystyle\dot{x}&\displaystyle=f(x,u)+\omega_{x}\\
 \displaystyle\dot{x}^{\prime}&\displaystyle=\partial_{x}f\cdot x^{\prime}+%
 \partial_{u}f\cdot u^{\prime}+\omega^{\prime}_{x}\\
 \displaystyle\dot{x}^{\prime\prime}&\displaystyle=\partial_{x}f\cdot x^{\prime%
 \prime}+\partial_{u}f\cdot u^{\prime\prime}+\omega^{\prime\prime}_{x}\\
 &\displaystyle\vdots\end{aligned}     Gaussian assumptions about the random fluctuations   œâ   œâ   \omega   then prescribe the likelihood and empirical priors on the motion of hidden states      p   (   s  ~   ,   x  ~   ,   u  ~   |  m  )      fragments  p   fragments  normal-(   normal-~  s   normal-,   normal-~  x   normal-,   normal-~  u   normal-|  m  normal-)     \displaystyle p\left(\tilde{s},\tilde{x},\tilde{u}|m\right)     The covariances     Œ£  ~   =   V  ‚äó  Œ£        normal-~  normal-Œ£    tensor-product  V  normal-Œ£     \tilde{{\Sigma}}=V\otimes\Sigma   factorize into a covariance among variables and correlations   V   V   V   among generalized fluctuations that encodes their autocorrelation :      V  =   [     1    0      œÅ  ¬®    (  0  )      ‚ãØ      0     -    œÅ  ¬®    (  0  )       0           œÅ  ¬®    (  0  )      0       œÅ  ¬®   ¬®    (  0  )           ‚ãÆ          ‚ã±     ]       V    1  0     normal-¬®  œÅ   0   normal-‚ãØ    0       normal-¬®  œÅ   0    0  absent       normal-¬®  œÅ   0   0     normal-¬®   normal-¬®  œÅ    0   absent    normal-‚ãÆ  absent  absent  normal-‚ã±      V=\begin{bmatrix}1&0&\ddot{\rho}(0)&\cdots\\
 0&-\ddot{\rho}(0)&0&\\
 \ddot{\rho}(0)&0&\ddot{\ddot{\rho}}(0)&\\
 \vdots&&&\ddots\\
 \end{bmatrix}     Here,     œÅ  ¬®    (  0  )        normal-¬®  œÅ   0    \ddot{{\rho}}(0)   is the second derivative of the autocorrelation function evaluated at zero. This is a ubiquitous measure of roughness in the theory of stochastic processes . 9 Crucially, the precision (inverse variance) of high order derivatives fall to zero fairly quickly, which means it is only necessary to model relatively low order generalized motion (usually between two and eight) for any given or parameterized autocorrelation function.  Special cases  Filtering discrete time series  When time series are observed as a discrete sequence of   N   N   N   observations, the implicit sampling is treated as part of the generative process, where (using Taylor's theorem )        [   s  1   ,  ‚Ä¶  ,   s  N   ]   T   =   (  E  ‚äó  I  )   ‚ãÖ   s  ~    (  t  )   :   E   i  j    =     (   i  -  t   )    (   j  -  1   )      (   j  -  1   )   !       fragments   superscript   fragments  normal-[   subscript  s  1   normal-,  normal-‚Ä¶  normal-,   subscript  s  N   normal-]   T     fragments  normal-(  E  tensor-product  I  normal-)   normal-‚ãÖ   normal-~  s    fragments  normal-(  t  normal-)   normal-:  italic-   subscript  E    i  j        superscript    i  t     j  1        j  1       [s_{1},\dots,s_{N}]^{T}=(E\otimes I)\cdot\tilde{s}(t):\qquad E_{ij}=\frac{(i-t%
 )^{(j-1)}}{(j-1)!}     In principle, the entire sequence could be used to estimate hidden variables at each point in time. However, the precision of samples in the past and future falls quickly and can be ignored. This allows the scheme to assimilate data online, using local observations around each time point (typically between and eight).  Generalized filtering and model parameters  For any slowly varying model parameters of the equations of motion    f   (  x  ,  u  ,  Œ∏  )       f   x  u  Œ∏     f(x,u,\theta)   or precision     Œ†  ~    (  x  ,  u  ,  Œ∏  )        normal-~  normal-Œ†    x  u  Œ∏     \tilde{{\Pi}}(x,u,\theta)   generalized filtering takes the following form (where   Œº   Œº   \mu   corresponds to the variational mean of the parameters)      Œº  Àô     normal-Àô  Œº    \displaystyle\dot{\mu}     Here, the solution       |  ~   Œº   Àô   =  0       normal-Àô     normal-~  normal-|   Œº    0    \dot{{\tilde{|}{{\mu}}}}=0   minimizes variational free energy, when the motion of the mean is small. This can be seen by noting     Œº  Àô   =    Œº  Àô   ‚Ä≤   =  0  ‚áí    ‚àÇ  Œº   F   =  0  ‚áí    Œ¥  Œº   S   =  0         normal-Àô  Œº    superscript   normal-Àô  Œº   normal-‚Ä≤        0    normal-‚áí      subscript   Œº   F        0    normal-‚áí       subscript  Œ¥  Œº   S        0     \dot{{\mu}}={\dot{{\mu}}}^{\prime}=0\Rightarrow\partial_{\mu}F=0\Rightarrow%
 \delta_{\mu}S=0   . It is straightforward to show that this solution corresponds to a classical Newton update . 10  Relationship to Bayesian filtering and predictive coding  Generalized filtering and Kalman filtering  Classical filtering under Markovian or Wiener assumptions is equivalent to assuming the precision of the motion of random fluctuations is zero. In this limiting case, one only has to consider the states and their first derivative     Œº  ~   =   (  Œº  ,   Œº  ‚Ä≤   )        normal-~  Œº    Œº   superscript  Œº  normal-‚Ä≤      \tilde{{\mu}}=(\mu,{\mu}^{\prime})   . This means generalized filtering takes the form of a Kalman-Bucy filter, with prediction and correction terms:      Œº  Àô     normal-Àô  Œº    \displaystyle\dot{\mu}     Substituting this first-order filtering into the discrete update scheme above gives the equivalent of (extended) Kalman filtering. 11  Generalized filtering and particle filtering  Particle filtering is a sampling-based scheme that relaxes assumptions about the form of the variational or approximate posterior density. The corresponding generalized filtering scheme is called variational filtering . 12 In variational filtering, an ensemble of particles diffuse over the free energy landscape in a frame of reference that moves with the expected (generalized) motion of the ensemble. This provides a relatively simple scheme that eschews Gaussian (unimodal) assumptions. Unlike particle filtering it does not require proposal densities‚Äîor the elimination or creation of particles.  Generalized filtering and variational Bayes  Variational Bayes rests on a mean field partition of the variational density:      q   (   x  ~   ,   u  ~   ,  Œ∏  ‚Ä¶  |   Œº  ~   ,  Œº  )   =  q   (   x  ~   ,   u  ~   |   Œº  ~   )   q   (  Œ∏  |  Œº  )   ‚Ä¶     fragments  q   fragments  normal-(   normal-~  x   normal-,   normal-~  u   normal-,  Œ∏  normal-‚Ä¶  normal-|   normal-~  Œº   normal-,  Œº  normal-)    q   fragments  normal-(   normal-~  x   normal-,   normal-~  u   normal-|   normal-~  Œº   normal-)   q   fragments  normal-(  Œ∏  normal-|  Œº  normal-)   normal-‚Ä¶    q(\tilde{x},\tilde{u},\theta\dots|\tilde{\mu},\mu)=q(\tilde{x},\tilde{u}|%
 \tilde{\mu})q(\theta|\mu)\dots     This partition induces a variational update or step for each marginal density‚Äîthat is usually solved analytically using conjugate priors. In generalized filtering, this leads to dynamic expectation maximisation . 13 that comprises a D-step that optimizes the sufficient statistics of unknown states, an E-step for parameters and an M-step for precisions.  Generalized filtering and predictive coding  Generalized filtering is usually used to invert hierarchical models of the following form      s  ~     normal-~  s    \displaystyle\tilde{s}     The ensuing generalized gradient descent on free energy can then be expressed compactly in terms of prediction errors, where (omitting high order terms):        Œº  ~   Àô   u   (  i  )      superscript   subscript   normal-Àô   normal-~  Œº    u   i    \displaystyle\dot{\tilde{\mu}}_{u}^{(i)}     Here,    Œ†   (  i  )      superscript  normal-Œ†  i    \Pi^{(i)}   is the precision of random fluctuations at the i -th level. This is known as generalized predictive coding [11], with linear predictive coding as a special case.  Applications  Generalized filtering has been primarily applied to biological timeseries‚Äîin particular functional magnetic resonance imaging and electrophysiological data. This is usually in the context of dynamic causal modelling to make inferences about the underlying architectures of (neuronal) systems generating data. 14 It is also used to simulate inference in terms of generalized (hierarchical) predictive coding in the brain. 15  See also   Dynamic Bayesian network  Kalman filter  Linear predictive coding  Optimal control  Particle filter  Recursive Bayesian estimation  System identification  Variational Bayesian methods   References  External links   software demonstrations and applications are available as academic freeware (as Matlab code) in the DEM toolbox of SPM  papers collection of technical and application papers   "  Category:Systems  Category:Systems theory  Category:Control theory  Category:Nonlinear filters  Category:Linear filters  Category:Signal estimation  Category:Stochastic differential equations  Category:Markov models     B Balaji and K Friston, " Bayesian state estimation using generalized coordinates ," Proc. SPIE, p. 80501Y , 2011 ‚Ü©    R P Feynman, Statistical mechanics. Reading MA: Benjamin, 1972 ‚Ü©  M J Beal, " Variational Algorithms for Approximate Bayesian Inference ," PhD. Thesis, University College London, 2003. ‚Ü©  K Friston, J Mattout, N Trujillo-Barreto, J Ashburner, and W Penny, " Variational free energy and the Laplace approximation ," NeuroImage, vol. 34, no. 1, pp. 220-34, 2007 ‚Ü©  W C Kerr and A J Graham, " Generalised phase space version of Langevin equations and associated Fokker-Planck equations ," Eur. Phys. J. B., vol. 15, pp. 305-11, 2000. ‚Ü©  T Ozaki, " A bridge between nonlinear time-series models and nonlinear stochastic dynamical systems: A local linearization approach ," Statistica Sin., vol. 2, pp. 113-135, 1992 ‚Ü©  D R Cox and H D Miller, The theory of stochastic processes. London: Methuen, 1965. ‚Ü©  K Friston, K Stephan, B Li, and J. Daunizeau, "Generalised Filtering," Mathematical Problems in Engineering, vol. vol., 2010, p. 621670, 2010. ‚Ü©  K J Friston, N Trujillo-Barreto, and J Daunizeau, "DEM: A variational treatment of dynamic systems," Neuroimage, vol. 41, no. 3, pp. 849-85, 2008 ‚Ü©  K J Friston, " Variational filtering ," Neuroimage, vol. 41, no. 3, pp. 747-66, 2008. ‚Ü©  K J Friston, N Trujillo-Barreto, and J Daunizeau, " DEM: A variational treatment of dynamic systems ," Neuroimage, vol. 41, no. 3, pp. 849-85, 2008 ‚Ü©  J Daunizeau, O David, and K E Stephan, " Dynamic causal modelling: a critical review of the biophysical and statistical foundations ," Neuroimage, vol. 58, no. 2, pp. 312-22, 2011 ‚Ü©  K Friston, " Hierarchical models in the brain ," PLoS Comput Biol., vol. 4, no. 11, p. e1000211, 2008. ‚Ü©     