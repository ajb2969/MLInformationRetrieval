   Constraint algorithm      Constraint algorithm   In computational chemistry , a constraint algorithm is a method for satisfying a Newtonian motion of a rigid which consists of mass points. By this algorithm, the distance between mass points is maintained constant. In general, the algorithm is constructed by following procedures; (i) choosing novel unconstrained coordinates (internal coordinates), (ii) introducing explicit constraint forces, (iii) minimizing constraint forces implicitly by the technique of Lagrange multipliers or projection methods.  Constraint algorithms are often applied to molecular dynamics simulations. Although such simulations are sometimes performed using internal coordinates that automatically satisfy the bond-length, bond-angle and torsion-angle constraints, simulations may also be performed using explicit or implicit constraint forces for these three constraints. However, explicit constraint forces give a cause of the inefficient computation; more computational power is required to get a trajectory of a given length. Therefore, internal coordinates and implicit-force constraint solvers are generally preferred.  Constraint algorithms achieve efficient computational time by neglecting motions of intramolecular atoms. If intramolecular behavior is important, e.g. hydrogen bonding and its behavior, constraint algorithms should not be used.  Mathematical background  The motion of a set of N particles can be described by a set of second-order ordinary differential equations, Newton's second law, which can be written in matrix form       𝐌  ⋅     d  2   𝐪    d   t  2      =  𝐟  =   -    ∂  V    ∂  𝐪            normal-⋅  𝐌       superscript  d  2   𝐪     d   superscript  t  2      𝐟             V     𝐪        \mathbf{M}\cdot\frac{d^{2}\mathbf{q}}{dt^{2}}=\mathbf{f}=-\frac{\partial V}{%
 \partial\mathbf{q}}     where M is a mass matrix and q is the vector of generalized coordinates that describe the particles' positions. For example, the vector q may be a 3N Cartesian coordinates of the particle positions r k , where k runs from 1 to N ; in the absence of constraints, M would be the 3N x 3N diagonal square matrix of the particle masses. The vector f represents the generalized forces and the scalar V ( q ) represents the potential energy, both of which are functions of the generalized coordinates q .  If M constraints are present, the coordinates must also satisfy M time-independent algebraic equations        g  j    (  𝐪  )    =  0         subscript  g  j   𝐪   0    g_{j}(\mathbf{q})=0     where the index j runs from 1 to M . For brevity, these functions g i are grouped into an M -dimensional vector g below. The task is to solve the combined set of differential-algebraic (DAE) equations, instead of just the ordinary differential equations (ODE) of Newton's second law.  This problem was studied in detail by Joseph Louis Lagrange , who laid out most of the methods for solving it. 1 The simplest approach is to define new generalized coordinates that are unconstrained; this approach eliminates the algebraic equations and reduces the problem once again to solving an ordinary differential equation. Such an approach is used, for example, in describing the motion of a rigid body; the position and orientation of a rigid body can be described by six independent, unconstrained coordinates, rather than describing the positions of the particles that make it up and the constraints among them that maintain their relative distances. The drawback of this approach is that the equations may become unwieldy and complex; for example, the mass matrix M may become non-diagonal and depend on the generalized coordinates.  A second approach is to introduce explicit forces that work to maintain the constraint; for example, one could introduce strong spring forces that enforce the distances among mass points within a "rigid" body. The two difficulties of this approach are that the constraints are not satisfied exactly, and the strong forces may require very short time-steps, making simulations inefficient computationally.  A third approach is to use a method such as Lagrange multipliers or projection to the constraint manifold to determine the coordinate adjustments necessary to satisfy the constraints. Finally, there are various hybrid approaches in which different sets of constraints are satisfied by different methods, e.g., internal coordinates, explicit forces and implicit-force solutions.  Internal coordinate methods  The simplest approach to satisfying constraints in energy minimization and molecular dynamics is to represent the mechanical system in so-called internal coordinates corresponding to unconstrained independent degrees of freedom of the system. For example, the dihedral angles of a protein are an independent set of coordinates that specify the positions of all the atoms without requiring any constraints. The difficulty of such internal-coordinate approaches is twofold: the Newtonian equations of motion become much more complex and the internal coordinates may be difficult to define for cyclic systems of constraints, e.g., in ring puckering or when a protein has a disulfide bond.  The original methods for efficient recursive energy minimization in internal coordinates were developed by Gō and coworkers. 2 3  Efficient recursive, internal-coordinate constraint solvers were extended to molecular dynamics. 4 5 Analogous methods were applied later to other systems. 6 7 8  Lagrange multiplier-based methods  (Figure)  Resolving the constraints of a rigid water molecule using Lagrange multipliers : a) the unconstrained positions are obtained after a simulation time-step, b) the gradients of each constraint over each particle are computed and c) the Lagrange multipliers are computed for each gradient such that the constraints are satisfied.   In most of molecular dynamics simulations that use constraint algorithms, constraints are enforced using the method of Lagrange multipliers . Given a set of n linear ( holonomic ) constraints at the time t ,         σ  k    (  t  )    :=     ∥     𝐱   k  α     (  t  )    -    𝐱   k  β     (  t  )     ∥   2   -   d  k  2    =  0   ,   k  =   1  …  n       formulae-sequence     assign     subscript  σ  k   t      superscript   norm       subscript  𝐱    k  α    t      subscript  𝐱    k  β    t     2    superscript   subscript  d  k   2         0      k    1  normal-…  n      \sigma_{k}(t):=\|\mathbf{x}_{k\alpha}(t)-\mathbf{x}_{k\beta}(t)\|^{2}-d_{k}^{2%
 }=0,\quad k=1\dots n     where     𝐱   k  α     (  t  )        subscript  𝐱    k  α    t    \scriptstyle\mathbf{x}_{k\alpha}(t)   and     𝐱   k  β     (  t  )        subscript  𝐱    k  β    t    \scriptstyle\mathbf{x}_{k\beta}(t)   are the positions of the two particles involved in the k th constraint at the time t and    d  k     subscript  d  k    d_{k}   is the prescribed inter-particle distance.  The forces due to these constraints are added in the equations of motion, resulting in, for each of the N particles in the system             ∂  2    𝐱  i     (  t  )     ∂   t  2      m  i    =   -    ∂   ∂   𝐱  i      [    V   (    𝐱  i    (  t  )    )    +    ∑   k  =  1   n     λ  k    σ  k    (  t  )      ]      ,   i  =   1  …  N     .     formulae-sequence            superscript   2    subscript  𝐱  i    t      superscript  t  2      subscript  m  i              subscript  𝐱  i      delimited-[]      V     subscript  𝐱  i   t      superscript   subscript     k  1    n      subscript  λ  k    subscript  σ  k   t           i    1  normal-…  N      \frac{\partial^{2}\mathbf{x}_{i}(t)}{\partial t^{2}}m_{i}=-\frac{\partial}{%
 \partial\mathbf{x}_{i}}\left[V(\mathbf{x}_{i}(t))+\sum_{k=1}^{n}\lambda_{k}%
 \sigma_{k}(t)\right],\quad i=1\dots N.     Adding the constraint forces does not change the total energy, as the net work done by the constraint forces (taken over the set of particles that the constraints act on) is zero.  From integrating both sides of the equation with respect to the time, the constrainted coordinates of particles at the time,    t  +   Δ  t       t    normal-Δ  t     t+\Delta t   , are given,         𝐱  i    (   t  +   Δ  t    )    =      𝐱  ^   i    (   t  +   Δ  t    )    +    ∑   k  =  1   n     λ  k      ∂   σ  k     (  t  )     ∂   𝐱  i       (   Δ  t   )   2    m  i   -  1        ,   i  =   1  …  N       formulae-sequence       subscript  𝐱  i     t    normal-Δ  t          subscript   normal-^  𝐱   i     t    normal-Δ  t       superscript   subscript     k  1    n      subscript  λ  k          subscript  σ  k    t      subscript  𝐱  i      superscript    normal-Δ  t   2    superscript   subscript  m  i     1          i    1  normal-…  N      \mathbf{x}_{i}(t+\Delta t)=\hat{\mathbf{x}}_{i}(t+\Delta t)+\sum_{k=1}^{n}%
 \lambda_{k}\frac{\partial\sigma_{k}(t)}{\partial\mathbf{x}_{i}}\left(\Delta t%
 \right)^{2}m_{i}^{-1},\quad i=1\dots N     where      𝐱  ^   i    (   t  +   Δ  t    )        subscript   normal-^  𝐱   i     t    normal-Δ  t      \hat{\mathbf{x}}_{i}(t+\Delta t)   is the unconstrained (or uncorrected) position of the i th particle after integrating the unconstrained equations of motion.  To satisfy the constraints     σ  k    (   t  +   Δ  t    )        subscript  σ  k     t    normal-Δ  t      \sigma_{k}(t+\Delta t)   in the next timestep, the Lagrange multipliers should be determined as the following equation,        σ  k    (   t  +   Δ  t    )    :=     ∥     𝐱   k  α     (   t  +   Δ  t    )    -    𝐱   k  β     (   t  +   Δ  t    )     ∥   2   -   d  k  2    =  0.       assign     subscript  σ  k     t    normal-Δ  t        superscript   norm       subscript  𝐱    k  α      t    normal-Δ  t        subscript  𝐱    k  β      t    normal-Δ  t       2    superscript   subscript  d  k   2         0.     \sigma_{k}(t+\Delta t):=\left\|\mathbf{x}_{k\alpha}(t+\Delta t)-\mathbf{x}_{k%
 \beta}(t+\Delta t)\right\|^{2}-d_{k}^{2}=0.     This implies solving a system of   n   n   n   non-linear equations         σ  j    (   t  +   Δ  t    )    :=     ∥       𝐱  ^    j  α     (   t  +   Δ  t    )    -     𝐱  ^    j  β     (   t  +   Δ  t    )     +    ∑   k  =  1   n     λ  k     (   Δ  t   )   2    [       ∂   σ  k     (  t  )     ∂   𝐱   j  α       m   j  α    -  1     -      ∂   σ  k     (  t  )     ∂   𝐱   j  β       m   j  β    -  1      ]      ∥   2   -   d  j  2    =  0   ,   j  =   1  …  n       formulae-sequence     assign     subscript  σ  j     t    normal-Δ  t        superscript   norm         subscript   normal-^  𝐱     j  α      t    normal-Δ  t        subscript   normal-^  𝐱     j  β      t    normal-Δ  t        superscript   subscript     k  1    n      subscript  λ  k    superscript    normal-Δ  t   2    delimited-[]             subscript  σ  k    t      subscript  𝐱    j  α       superscript   subscript  m    j  α      1              subscript  σ  k    t      subscript  𝐱    j  β       superscript   subscript  m    j  β      1           2    superscript   subscript  d  j   2         0      j    1  normal-…  n      \sigma_{j}(t+\Delta t):=\left\|\hat{\mathbf{x}}_{j\alpha}(t+\Delta t)-\hat{%
 \mathbf{x}}_{j\beta}(t+\Delta t)+\sum_{k=1}^{n}\lambda_{k}\left(\Delta t\right%
 )^{2}\left[\frac{\partial\sigma_{k}(t)}{\partial\mathbf{x}_{j\alpha}}m_{j%
 \alpha}^{-1}-\frac{\partial\sigma_{k}(t)}{\partial\mathbf{x}_{j\beta}}m_{j%
 \beta}^{-1}\right]\right\|^{2}-d_{j}^{2}=0,\quad j=1\dots n     simultaneously for the   n   n   n   unknown Lagrange multipliers    λ  k     subscript  λ  k    \lambda_{k}   .  This system of   n   n   n   non-linear equations in   n   n   n   unknowns is commonly solved using Newton–Raphson method where the solution vector    λ  ¯     normal-¯  λ    \underline{\lambda}   is updated using        λ  ¯    (   l  +  1   )    ←     λ  ¯    (  l  )    -    𝐉  σ   -  1     σ  ¯    (   t  +   Δ  t    )        normal-←   superscript   normal-¯  λ     l  1       superscript   normal-¯  λ   l      superscript   subscript  𝐉  σ     1     normal-¯  σ     t    normal-Δ  t        \underline{\lambda}^{(l+1)}\leftarrow\underline{\lambda}^{(l)}-\mathbf{J}_{%
 \sigma}^{-1}\underline{\sigma}(t+\Delta t)     where    𝐉  σ     subscript  𝐉  σ    \mathbf{J}_{\sigma}   is the Jacobian of the equations σ k :       𝐉  =   (       ∂   σ  1     ∂   λ  1         ∂   σ  1     ∂   λ  2       …      ∂   σ  1     ∂   λ  n           ∂   σ  2     ∂   λ  1         ∂   σ  2     ∂   λ  2       …      ∂   σ  2     ∂   λ  n         ⋮    ⋮    ⋱    ⋮        ∂   σ  n     ∂   λ  1         ∂   σ  n     ∂   λ  2       …      ∂   σ  n     ∂   λ  n        )    .      𝐉         subscript  σ  1       subscript  λ  1          subscript  σ  1       subscript  λ  2     normal-…       subscript  σ  1       subscript  λ  n            subscript  σ  2       subscript  λ  1          subscript  σ  2       subscript  λ  2     normal-…       subscript  σ  2       subscript  λ  n       normal-⋮  normal-⋮  normal-⋱  normal-⋮         subscript  σ  n       subscript  λ  1          subscript  σ  n       subscript  λ  2     normal-…       subscript  σ  n       subscript  λ  n         \mathbf{J}=\left(\begin{array}[]{cccc}\frac{\partial\sigma_{1}}{\partial%
 \lambda_{1}}&\frac{\partial\sigma_{1}}{\partial\lambda_{2}}&\dots&\frac{%
 \partial\sigma_{1}}{\partial\lambda_{n}}\\
 \frac{\partial\sigma_{2}}{\partial\lambda_{1}}&\frac{\partial\sigma_{2}}{%
 \partial\lambda_{2}}&\dots&\frac{\partial\sigma_{2}}{\partial\lambda_{n}}\\
 \vdots&\vdots&\ddots&\vdots\\
 \frac{\partial\sigma_{n}}{\partial\lambda_{1}}&\frac{\partial\sigma_{n}}{%
 \partial\lambda_{2}}&\dots&\frac{\partial\sigma_{n}}{\partial\lambda_{n}}\end{%
 array}\right).     Since not all particles contribute to all of constraints,    𝐉  σ     subscript  𝐉  σ    \mathbf{J}_{\sigma}   is a block matrix and can be solved individually to block-unit of the matrix. In other words,    𝐉  σ     subscript  𝐉  σ    \mathbf{J}_{\sigma}   can be solved individually for each molecule.  Instead of constantly updating the vector    λ  ¯     normal-¯  λ    \underline{\lambda}   , the iteration can be started with      λ  ¯    (  0  )    =  𝟎       superscript   normal-¯  λ   0   0    \underline{\lambda}^{(0)}=\mathbf{0}   , resulting in simpler expressions for     σ  k    (  t  )        subscript  σ  k   t    \sigma_{k}(t)   and      ∂   σ  k     (  t  )     ∂   λ  j             subscript  σ  k    t      subscript  λ  j      \frac{\partial\sigma_{k}(t)}{\partial\lambda_{j}}   . In this case        J   i  j    =      ∂   σ  j     ∂   λ  i     |    λ  =  0    =   2   [     x  ^    j  α    -    x  ^    j  β     ]    [      ∂   σ  i     ∂   x   j  α       m   j  α    -  1     -     ∂   σ  i     ∂   x   j  β       m   j  β    -  1      ]     .         subscript  J    i  j     evaluated-at       subscript  σ  j       subscript  λ  i       λ  0           2   delimited-[]     subscript   normal-^  x     j  α     subscript   normal-^  x     j  β       delimited-[]           subscript  σ  i       subscript  x    j  α       superscript   subscript  m    j  α      1            subscript  σ  i       subscript  x    j  β       superscript   subscript  m    j  β      1           J_{ij}=\left.\frac{\partial\sigma_{j}}{\partial\lambda_{i}}\right|_{\mathbf{%
 \lambda}=0}=2\left[\hat{x}_{j\alpha}-\hat{x}_{j\beta}\right]\left[\frac{%
 \partial\sigma_{i}}{\partial x_{j\alpha}}m_{j\alpha}^{-1}-\frac{\partial\sigma%
 _{i}}{\partial x_{j\beta}}m_{j\beta}^{-1}\right].     then   λ   λ   \lambda   is updated to        λ  j   =   -    𝐉   -  1     [     ∥      𝐱  ^    j  α     (   t  +   Δ  t    )    -     𝐱  ^    j  β     (   t  +   Δ  t    )     ∥   2   -   d  j  2    ]      .       subscript  λ  j        superscript  𝐉    1     delimited-[]     superscript   norm       subscript   normal-^  𝐱     j  α      t    normal-Δ  t        subscript   normal-^  𝐱     j  β      t    normal-Δ  t       2    superscript   subscript  d  j   2         \mathbf{\lambda}_{j}=-\mathbf{J}^{-1}\left[\left\|\hat{\mathbf{x}}_{j\alpha}(t%
 +\Delta t)-\hat{\mathbf{x}}_{j\beta}(t+\Delta t)\right\|^{2}-d_{j}^{2}\right].     After each iteration, the unconstrained particle positions are updated using         𝐱  ^   i    (   t  +   Δ  t    )    ←      𝐱  ^   i    (   t  +   Δ  t    )    +    ∑   k  =  1   n     λ  k     ∂   σ  k     ∂   𝐱  i           normal-←     subscript   normal-^  𝐱   i     t    normal-Δ  t          subscript   normal-^  𝐱   i     t    normal-Δ  t       superscript   subscript     k  1    n      subscript  λ  k        subscript  σ  k       subscript  𝐱  i          \hat{\mathbf{x}}_{i}(t+\Delta t)\leftarrow\hat{\mathbf{x}}_{i}(t+\Delta t)+%
 \sum_{k=1}^{n}\lambda_{k}\frac{\partial\sigma_{k}}{\partial\mathbf{x}_{i}}   .  The vector is then reset to       λ  ¯   =  0.       normal-¯  λ   0.    \underline{\lambda}=\mathbf{0}.     The above procedure is repeated until the solution of constraint equations,     σ  k    (   t  +   Δ  t    )        subscript  σ  k     t    normal-Δ  t      \sigma_{k}(t+\Delta t)   , converges to a prescribed tolerance of a numerical error.  Although there are a number of algorithms to compute the Lagrange multipliers, these difference is rely only on the methods to solve the system of equations. For this methods, quasi-Newton methods are commonly used.  The SETTLE algorithm  The SETTLE algorithm 9 solves the system of non-linear equations analytically for    n  =  3      n  3    n=3   constraints in constant time. Although it does not scale to larger numbers of constraints, it is very often used to constrain rigid water molecules, which are present in almost all biological simulations and are usually modelled using three constraints (e.g. SPC/E and TIP3P  water models ).  The SHAKE algorithm  The SHAKE algorithm was first developed for satisfying a bond geometry constraint during molecular dynamics simulations. 10 In SHAKE algorithm, the system of non-linear constraint equations is solved using the Gauss-Seidel method which approximates the solution of the linear system of equations using the Newton-Raphson method ;       λ  ¯   =   -    𝐉  σ   -  1     σ  ¯          normal-¯  λ        superscript   subscript  𝐉  σ     1     normal-¯  σ       \underline{\lambda}=-\mathbf{J}_{\sigma}^{-1}\underline{\sigma}   .  This amounts to assuming that    𝐉  σ     subscript  𝐉  σ    \mathbf{J}_{\sigma}   is diagonally dominant and solving the   k   k   k   th equation only for the   k   k   k   unknown. In practice, we compute            λ  k     subscript  λ  k    \lambda_{k}         ←   normal-←   \leftarrow            σ  k    (  t  )       ∂   σ  k     (  t  )    /   ∂   λ  k            subscript  σ  k   t          subscript  σ  k    t      subscript  λ  k       \frac{\sigma_{k}(t)}{\partial\sigma_{k}(t)/\partial\lambda_{k}}   ,         𝐱   k  α      subscript  𝐱    k  α     \mathbf{x}_{k\alpha}         ←   normal-←   \leftarrow           𝐱   k  α    +    λ  k      ∂   σ  k     (  t  )     ∂   𝐱   k  α            subscript  𝐱    k  α       subscript  λ  k          subscript  σ  k    t      subscript  𝐱    k  α         \mathbf{x}_{k\alpha}+\lambda_{k}\frac{\partial\sigma_{k}(t)}{\partial\mathbf{x%
 }_{k\alpha}}   ,         𝐱   k  β      subscript  𝐱    k  β     \mathbf{x}_{k\beta}         ←   normal-←   \leftarrow           𝐱   k  β    +    λ  k      ∂   σ  k     (  t  )     ∂   𝐱   k  β            subscript  𝐱    k  β       subscript  λ  k          subscript  σ  k    t      subscript  𝐱    k  β         \mathbf{x}_{k\beta}+\lambda_{k}\frac{\partial\sigma_{k}(t)}{\partial\mathbf{x}%
 _{k\beta}}   ,       for all    k  =   1  …  n       k    1  normal-…  n     k=1\dots n   iteratively until the constraint equations     σ  k    (   t  +   Δ  t    )        subscript  σ  k     t    normal-Δ  t      \sigma_{k}(t+\Delta t)   are solved to a given tolerance.  The calculation cost of each iteration is    𝒪   (  n  )       𝒪  n    \mathcal{O}(n)   , and the iterations themselves converge linearly.  A noniterative form of SHAKE was developed later. 11  Several variants of the SHAKE algorithm exist. Although they differ in how they compute or apply the constraints themselves, the constraints are still modelled using Lagrange multipliers which are computed using the Gauss-Seidel method .  The original SHAKE algorithm is limited to mechanical systems with a tree structure, i.e., no closed loops of constraints. A later extension of the method, QSHAKE ( Quaternion SHAKE) was developed to amend this. 12 It works satisfactorily for rigid loops such as aromatic ring systems but fails for flexible loops, such as when a protein has a disulfide bond. 13  Further extensions include RATTLE, 14 WIGGLE 15 and MSHAKE. 16 RATTLE works the same way as SHAKE, 17 yet using the Velocity Verlet time integration scheme. WIGGLE extends SHAKE and RATTLE by using an initial estimate for the Lagrange multipliers     λ  k     subscript  λ  k    \lambda_{k}   based on the particle velocities. Finally, MSHAKE computes corrections on the constraint forces , achieving better convergence.  A final modification is the P-SHAKE algorithm 18 for rigid or semi-rigid molecules. P-SHAKE computes and updates a pre-conditioner which is applied to the constraint gradients before the SHAKE iteration, causing the Jacobian    𝐉  σ     subscript  𝐉  σ    \mathbf{J}_{\sigma}   to become diagonal or strongly diagonally dominant. The thus de-coupled constraints converge much faster (quadratically as opposed to linearly) at a cost of    𝒪   (   n  2   )       𝒪   superscript  n  2     \mathcal{O}(n^{2})   .  The M-SHAKE algorithm  The M-SHAKE algorithm 19 solves the non-linear system of equations using Newton's method directly. In each iteration, the linear system of equations       λ  ¯   =   -    𝐉  σ   -  1     σ  ¯          normal-¯  λ        superscript   subscript  𝐉  σ     1     normal-¯  σ       \underline{\lambda}=-\mathbf{J}_{\sigma}^{-1}\underline{\sigma}     is solved exactly using an LU decomposition . Each iteration costs    𝒪   (   n  3   )       𝒪   superscript  n  3     \mathcal{O}(n^{3})   operations, yet the solution converges quadratically , requiring fewer iterations than SHAKE.  This solution was first proposed in 1986 by Ciccotti and Ryckaert 20 under the title "the matrix method", yet differed in the solution of the linear system of equations. Ciccotti and Ryckaert suggest inverting the matrix    𝐉  σ     subscript  𝐉  σ    \mathbf{J}_{\sigma}   directly, yet doing so only once, in the first iteration. The first iteration then costs    𝒪   (   n  3   )       𝒪   superscript  n  3     \mathcal{O}(n^{3})   operations, whereas the following iterations cost only    𝒪   (   n  2   )       𝒪   superscript  n  2     \mathcal{O}(n^{2})   operations (for the matrix-vector multiplication). This improvement comes at a cost though, since the Jacobian is no longer updated, convergence is only linear , albeit at a much faster rate than for the SHAKE algorithm.  Several variants of this approach based on sparse matrix techniques were studied by Barth et al. . 21  The SHAPE algorithm  The SHAPE algorithm 22 is a multicenter analog of SHAKE for constraining rigid bodies of three or more centers. Like SHAKE, an unconstrained step is taken and then corrected by directly calculating and applying the rigid body rotation matrix that satisfies:        L   r  i  g  i  d     (   t  +    Δ  t   2    )    =    L   n  o  n  r  i  g  i  d     (   t  +    Δ  t   2    )           superscript  L    r  i  g  i  d      t      normal-Δ  t   2        superscript  L    n  o  n  r  i  g  i  d      t      normal-Δ  t   2       L^{rigid}\left(t+\frac{\Delta t}{2}\right)=L^{nonrigid}\left(t+\frac{\Delta t}%
 {2}\right)   This approach involves a single 3x3 matrix diagonalization followed by three or four rapid Newton iterations to determine the rotation matrix. SHAPE provides the identical trajectory that is provided with fully converged iterative SHAKE, yet it is found to be more efficient and more accurate than SHAKE when applied to systems involving three or more centers. It extends the ability of SHAKE like constraints to linear systems with three or more atoms, planar systems with four or more atoms, and to significantly larger rigid structures where SHAKE is intractable. It also allows rigid bodies to be linked with one or two common centers (e.g. peptide planes) by solving rigid body constraints iteratively in the same basic manner that SHAKE is used for atoms involving more than one SHAKE constraint.  The LINCS algorithm  An alternative constraint method, LINCS (Linear Constraint Solver) was developed in 1997 by Hess, Bekker, Berendsen and Fraaije, 23 and was based on the 1986 method of Edberg, Evans and Morriss (EEM), 24 and a modification thereof by Baranyai and Evans (BE). 25  LINCS applies Lagrange multipliers to the constraint forces and solves for the multipliers by using a series expansion to approximate the inverse of the Jacobian    𝐉  σ     subscript  𝐉  σ    \mathbf{J}_{\sigma}   :        (   𝐈  -   𝐉  σ    )    -  1    =   𝐈  +   𝐉  σ   +   𝐉  σ  2   +   𝐉  σ  3   +  …        superscript    𝐈   subscript  𝐉  σ      1      𝐈   subscript  𝐉  σ    superscript   subscript  𝐉  σ   2    superscript   subscript  𝐉  σ   3   normal-…     (\mathbf{I}-\mathbf{J}_{\sigma})^{-1}=\mathbf{I}+\mathbf{J}_{\sigma}+\mathbf{J%
 }_{\sigma}^{2}+\mathbf{J}_{\sigma}^{3}+\dots     in each step of the Newton iteration. This approximation only works for matrices with Eigenvalues smaller than 1, making the LINCS algorithm suitable only for molecules with low connectivity.  LINCS has been reported to be 3-4 times faster than SHAKE. 26  Hybrid methods  Hybrid methods have also been introduced in which the constraints are divided into two groups; the constraints of the first group are solved using internal coordinates whereas those of the second group are solved using constraint forces, e.g., by a Lagrange multiplier or projection method. 27 28 29 This approach was pioneered by Lagrange , 30 and result in Lagrange equations of the mixed type . 31  See also   Molecular dynamics  Software for molecular mechanics modeling   References and footnotes  "  Category:Molecular dynamics  Category:Computational chemistry  Category:Molecular physics  Category:Computational physics  Category:Numerical differential equations     ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩   ↩  ↩  ↩   ↩     