   Independence (probability theory)      Independence (probability theory)   In probability theory , two events are independent , statistically independent , or stochastically independent 1 if the occurrence of one does not affect the probability of the other. Similarly, two random variables are independent if the realization of one does not affect the probability distribution of the other.  The concept of independence extends to dealing with collections of more than two events or random variables, in which case the events are pairwise independent if each pair are independent of each other, and the events are mutually independent if each event is independent of each other combination of events.  Definition  For events  Two events  Two events A and B are independent (often written as    A  ⟂  B     perpendicular-to  A  B    A\perp B   or    A  ⟂  ⟂  B     fragments  A  perpendicular-to  perpendicular-to  B    A\perp\!\!\!\perp B   ) if and only if their joint probability equals the product of their probabilities:       P   (   A  ∩  B   )    =   P   (  A  )   P   (  B  )          normal-P    A  B      normal-P  A  normal-P  B     \mathrm{P}(A\cap B)=\mathrm{P}(A)\mathrm{P}(B)   .  Why this defines independence is made clear by rewriting with conditional probabilities :      P   (  A  ∩  B  )   =  P   (  A  )   P   (  B  )   ⇔  P   (  A  )   =    P   (  A  )   P   (  B  )     P   (  B  )     =    P   (   A  ∩  B   )     P   (  B  )     =  P   (  A  ∣  B  )      fragments  P   fragments  normal-(  A   B  normal-)    P   fragments  normal-(  A  normal-)   P   fragments  normal-(  B  normal-)   normal-⇔  P   fragments  normal-(  A  normal-)        normal-P  A  normal-P  B     normal-P  B         normal-P    A  B      normal-P  B     P   fragments  normal-(  A  normal-∣  B  normal-)     \mathrm{P}(A\cap B)=\mathrm{P}(A)\mathrm{P}(B)\Leftrightarrow\mathrm{P}(A)=%
 \frac{\mathrm{P}(A)\mathrm{P}(B)}{\mathrm{P}(B)}=\frac{\mathrm{P}(A\cap B)}{%
 \mathrm{P}(B)}=\mathrm{P}(A\mid B)     and similarly      P   (  A  ∩  B  )   =  P   (  A  )   P   (  B  )   ⇔  P   (  B  )   =  P   (  B  ∣  A  )      fragments  P   fragments  normal-(  A   B  normal-)    P   fragments  normal-(  A  normal-)   P   fragments  normal-(  B  normal-)   normal-⇔  P   fragments  normal-(  B  normal-)    P   fragments  normal-(  B  normal-∣  A  normal-)     \mathrm{P}(A\cap B)=\mathrm{P}(A)\mathrm{P}(B)\Leftrightarrow\mathrm{P}(B)=%
 \mathrm{P}(B\mid A)   .  Thus, the occurrence of B does not affect the probability of A , and vice versa. Although the derived expressions may seem more intuitive, they are not the preferred definition, as the conditional probabilities may be undefined if P ( A ) or P ( B ) are 0. Furthermore, the preferred definition makes clear by symmetry that when A is independent of B , B is also independent of A .  More than two events  A finite set of events { A i } is pairwise independent  if and only if every pair of events is independent 2 —that is, if and only if for all distinct pairs of indices m , k ,        P   (    A  m   ∩   A  k    )    =   P   (   A  m   )   P   (   A  k   )     .        normal-P     subscript  A  m    subscript  A  k       normal-P   subscript  A  m   normal-P   subscript  A  k      \mathrm{P}(A_{m}\cap A_{k})=\mathrm{P}(A_{m})\mathrm{P}(A_{k}).     A finite set of events is mutually independent if and only if every event is independent of any intersection of the other events 3 —that is, if and only if for every n -element subset { A i },        P   (    ⋂   i  =  1   n    A  i    )    =    ∏   i  =  1   n    P   (   A  i   )      .        normal-P    superscript   subscript     i  1    n    subscript  A  i       superscript   subscript  product    i  1    n     normal-P   subscript  A  i       \mathrm{P}\left(\bigcap_{i=1}^{n}A_{i}\right)=\prod_{i=1}^{n}\mathrm{P}(A_{i}).     This is called the multiplication rule for independent events. Note that it is not a single condition involving only the product of all the probabilities of all single events (see below for a counterexample); it must hold true for all subset of events.  For more than two events, a mutually independent set of events is (by definition) pairwise independent; but the converse is not necessarily true (see below for a counterexample).  For random variables  Two random variables  Two random variables X and Y are independent  iff the elements of the π-system generated by them are independent; that is to say, for every a and b , the events { X ≤ a } and { Y ≤ b } are independent events (as defined above). That is, X and Y with cumulative distribution functions      F  X    (  x  )        subscript  F  X   x    F_{X}(x)   and     F  Y    (  y  )        subscript  F  Y   y    F_{Y}(y)   , and probability densities      f  X    (  x  )        subscript  f  X   x    f_{X}(x)   and     f  Y    (  y  )        subscript  f  Y   y    f_{Y}(y)   , are independent if and only if (iff) the combined random variable ( X , Y ) has a joint cumulative distribution function         F   X  ,  Y     (  x  ,  y  )    =    F  X    (  x  )    F  Y    (  y  )     ,         subscript  F   X  Y     x  y       subscript  F  X   x   subscript  F  Y   y     F_{X,Y}(x,y)=F_{X}(x)F_{Y}(y),     or equivalently, a joint density         f   X  ,  Y     (  x  ,  y  )    =    f  X    (  x  )    f  Y    (  y  )     .         subscript  f   X  Y     x  y       subscript  f  X   x   subscript  f  Y   y     f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y).     More than two random variables  A set of random variables is pairwise independent if and only if every pair of random variables is independent.  A set of random variables is mutually independent if and only if for any finite subset     X  1   ,  …  ,   X  n       subscript  X  1   normal-…   subscript  X  n     X_{1},\ldots,X_{n}   and any finite sequence of numbers     a  1   ,  …  ,   a  n       subscript  a  1   normal-…   subscript  a  n     a_{1},\ldots,a_{n}   , the events     {   X  1   ≤   a  1   }   ,  …  ,   {   X  n   ≤   a  n   }      fragments   fragments  normal-{   subscript  X  1     subscript  a  1   normal-}   normal-,  normal-…  normal-,   fragments  normal-{   subscript  X  n     subscript  a  n   normal-}     \{X_{1}\leq a_{1}\},\ldots,\{X_{n}\leq a_{n}\}   are mutually independent events (as defined above).  The measure-theoretically inclined may prefer to substitute events { X ∈ A } for events { X ≤ a } in the above definition, where A is any Borel set . That definition is exactly equivalent to the one above when the values of the random variables are real numbers . It has the advantage of working also for complex-valued random variables or for random variables taking values in any measurable space (which includes topological spaces endowed by appropriate σ-algebras).  Conditional independence  Intuitively, two random variables X and Y are conditionally independent given Z if, once Z is known, the value of Y does not add any additional information about X . For instance, two measurements X and Y of the same underlying quantity Z are not independent, but they are conditionally independent given Z (unless the errors in the two measurements are somehow connected).  The formal definition of conditional independence is based on the idea of conditional distributions . If X , Y , and Z are discrete random variables , then we define X and Y to be conditionally independent given  Z if      P   (  X  ≤  x  ,  Y  ≤   y   |  Z  =  z  )   =  P   (  X  ≤   x   |  Z  =  z  )   ⋅  P   (  Y  ≤   y   |  Z  =  z  )      fragments  P   fragments  normal-(  X   x  normal-,  Y   y  normal-|  Z   z  normal-)    P   fragments  normal-(  X   x  normal-|  Z   z  normal-)   normal-⋅  P   fragments  normal-(  Y   y  normal-|  Z   z  normal-)     \mathrm{P}(X\leq x,Y\leq y\;|\;Z=z)=\mathrm{P}(X\leq x\;|\;Z=z)\cdot\mathrm{P}%
 (Y\leq y\;|\;Z=z)     for all x , y and z such that P( Z = z ) > 0. On the other hand, if the random variables are continuous and have a joint probability density function  p , then X and Y are conditionally independent given Z if       p   X  Y  |  Z     (  x  ,  y  |  z  )   =   p   X  |  Z     (  x  |  z  )   ⋅   p   Y  |  Z     (  y  |  z  )      fragments   subscript  p   fragments  X  Y  normal-|  Z     fragments  normal-(  x  normal-,  y  normal-|  z  normal-)     subscript  p   fragments  X  normal-|  Z     fragments  normal-(  x  normal-|  z  normal-)   normal-⋅   subscript  p   fragments  Y  normal-|  Z     fragments  normal-(  y  normal-|  z  normal-)     p_{XY|Z}(x,y|z)=p_{X|Z}(x|z)\cdot p_{Y|Z}(y|z)     for all real numbers x , y and z such that p Z ( z ) > 0.  If X and Y are conditionally independent given Z , then      P   (  X  =  x  |  Y  =  y  ,  Z  =  z  )   =  P   (  X  =  x  |  Z  =  z  )      fragments  P   fragments  normal-(  X   x  normal-|  Y   y  normal-,  Z   z  normal-)    P   fragments  normal-(  X   x  normal-|  Z   z  normal-)     \mathrm{P}(X=x|Y=y,Z=z)=\mathrm{P}(X=x|Z=z)     for any x , y and z with P( Z = z ) > 0. That is, the conditional distribution for X given Y and Z is the same as that given Z alone. A similar equation holds for the conditional probability density functions in the continuous case.  Independence can be seen as a special kind of conditional independence, since probability can be seen as a kind of conditional probability given no events.  Independent σ-algebras  The definitions above are both generalized by the following definition of independence for σ-algebras . Let (Ω, Σ, Pr) be a probability space and let A and B be two sub-σ-algebras of Σ. A and B are said to be independent if, whenever A ∈ A and B ∈ B ,        P   (   A  ∩  B   )    =   P   (  A  )   P   (  B  )     .        normal-P    A  B      normal-P  A  normal-P  B     \mathrm{P}(A\cap B)=\mathrm{P}(A)\mathrm{P}(B).     Likewise, a finite family of σ-algebras     (   τ  i   )    i  ∈  I      subscript   subscript  τ  i     i  I     (\tau_{i})_{i\in I}   is said to be independent if and only if for all        ∀    (   A  i   )    i  ∈  I     ∈    ∏   i  ∈  I      τ  i      :    P   (    ⋂   i  ∈  I     A  i    )    =    ∏   i  ∈  I     P   (   A  i   )         normal-:     for-all   subscript   subscript  A  i     i  I       subscript  product    i  I     subscript  τ  i         normal-P    subscript     i  I     subscript  A  i       subscript  product    i  I      normal-P   subscript  A  i        \forall\left(A_{i}\right)_{i\in I}\in\prod\nolimits_{i\in I}\tau_{i}\ :\ %
 \mathrm{P}\left(\bigcap\nolimits_{i\in I}A_{i}\right)=\prod\nolimits_{i\in I}%
 \mathrm{P}\left(A_{i}\right)     and an infinite family of σ-algebras is said to be independent if all its finite subfamilies are independent.  The new definition relates to the previous ones very directly:   Two events are independent (in the old sense) if and only if the σ-algebras that they generate are independent (in the new sense). The σ-algebra generated by an event E ∈ Σ is, by definition,           σ   (   {  E  }   )    =   {  ∅  ,  E  ,   Ω  ∖  E   ,  Ω  }    .        σ   E      E    normal-Ω  E   normal-Ω     \sigma(\{E\})=\{\emptyset,E,\Omega\setminus E,\Omega\}.         Two random variables X and Y defined over Ω are independent (in the old sense) if and only if the σ-algebras that they generate are independent (in the new sense). The σ-algebra generated by a random variable X taking values in some measurable space  S consists, by definition, of all subsets of Ω of the form X −1 ( U ), where U is any measurable subset of S .   Using this definition, it is easy to show that if X and Y are random variables and Y is constant, then X and Y are independent, since the σ-algebra generated by a constant random variable is the trivial σ-algebra {∅, Ω}. Probability zero events cannot affect independence so independence also holds if Y is only Pr- almost surely constant.  Properties  Self-independence  Note that an event is independent of itself if and only if        P   (  A  )    =   P   (   A  ∩  A   )    =     P   (  A  )    ⋅  P    (  A  )     ⇔    P   (  A  )    =   0  or  1       normal-⇔        normal-P  A     normal-P    A  A            normal-⋅    normal-P  A   normal-P   A         normal-P  A     0  or  1      \mathrm{P}(A)=\mathrm{P}(A\cap A)=\mathrm{P}(A)\cdot\mathrm{P}(A)%
 \Leftrightarrow\mathrm{P}(A)=0\text{ or }1   .  Thus an event is independent of itself if and only if it almost surely occurs or its complement almost surely occurs. For example, if A is choosing any number but 0.5 from a uniform distribution on the unit interval , A is independent of itself, even though, tautologically , A fully determines A .  Expectation and covariance  If X and Y are independent, then the expectation operator  E has the property        E   [   X  Y   ]    =   E   [  X  ]   E   [  Y  ]     ,        E   delimited-[]    X  Y       E   delimited-[]  X   E   delimited-[]  Y      E[XY]=E[X]E[Y],     and the covariance cov( X , Y ) is zero, since we have        cov   [  X  ,  Y  ]    =    E   [   X  Y   ]    -   E   [  X  ]   E   [  Y  ]      .        cov   X  Y        E   delimited-[]    X  Y       E   delimited-[]  X   E   delimited-[]  Y       \text{cov}[X,Y]=E[XY]-E[X]E[Y].     (The converse of these, i.e. the proposition that if two random variables have a covariance of 0 they must be independent, is not true. See uncorrelated .)  Characteristic function  Two random variables X and Y are independent if and only if the characteristic function of the random vector ( X , Y ) satisfies         φ   (  X  ,  Y  )     (  t  ,  s  )    =      φ  X    (  t  )    ⋅   φ  Y     (  s  )     .         subscript  φ   X  Y     t  s       normal-⋅     subscript  φ  X   t    subscript  φ  Y    s     \varphi_{(X,Y)}(t,s)=\varphi_{X}(t)\cdot\varphi_{Y}(s).     In particular the characteristic function of their sum is the product of their marginal characteristic functions:         φ   X  +  Y     (  t  )    =      φ  X    (  t  )    ⋅   φ  Y     (  t  )     ,         subscript  φ    X  Y    t      normal-⋅     subscript  φ  X   t    subscript  φ  Y    t     \varphi_{X+Y}(t)=\varphi_{X}(t)\cdot\varphi_{Y}(t),     though the reverse implication is not true. Random variables that satisfy the latter condition are called subindependent .  Examples  Rolling a die  The event of getting a 6 the first time a die is rolled and the event of getting a 6 the second time are independent . By contrast, the event of getting a 6 the first time a die is rolled and the event that the sum of the numbers seen on the first and second trials is 8 are not independent.  Drawing cards  If two cards are drawn with replacement from a deck of cards, the event of drawing a red card on the first trial and that of drawing a red card on the second trial are independent . By contrast, if two cards are drawn without replacement from a deck of cards, the event of drawing a red card on the first trial and that of drawing a red card on the second trial are again not independent.  Pairwise and mutual independence    Consider the two probability spaces shown. In both cases, P ( A ) = P ( B ) = 1/2 and P ( C ) = 1/4 The first space is pairwise independent but not mutually independent. The second space is both pairwise independent and mutually independent. To illustrate the difference, consider conditioning on two events. In the pairwise independent case, although any one event is independent of each of the other two individually, it is not independent of the intersection of the other two:      P   (  A  |  B  C  )   =    4  40     4  40   +   1  40     =    4  5    ≠  P   (  A  )      fragments  P   fragments  normal-(  A  normal-|  B  C  normal-)        4  40       4  40     1  40        4  5    P   fragments  normal-(  A  normal-)     \mathrm{P}(A|BC)=\frac{\frac{4}{40}}{\frac{4}{40}+\frac{1}{40}}=\tfrac{4}{5}%
 \neq\mathrm{P}(A)         P   (  B  |  A  C  )   =    4  40     4  40   +   1  40     =    4  5    ≠  P   (  B  )      fragments  P   fragments  normal-(  B  normal-|  A  C  normal-)        4  40       4  40     1  40        4  5    P   fragments  normal-(  B  normal-)     \mathrm{P}(B|AC)=\frac{\frac{4}{40}}{\frac{4}{40}+\frac{1}{40}}=\tfrac{4}{5}%
 \neq\mathrm{P}(B)         P   (  C  |  A  B  )   =    4  40     4  40   +   6  40     =    2  5    ≠  P   (  C  )      fragments  P   fragments  normal-(  C  normal-|  A  B  normal-)        4  40       4  40     6  40        2  5    P   fragments  normal-(  C  normal-)     \mathrm{P}(C|AB)=\frac{\frac{4}{40}}{\frac{4}{40}+\frac{6}{40}}=\tfrac{2}{5}%
 \neq\mathrm{P}(C)     In the mutually independent case however:      P   (  A  |  B  C  )   =    1  16     1  16   +   1  16     =    1  2    =  P   (  A  )      fragments  P   fragments  normal-(  A  normal-|  B  C  normal-)        1  16       1  16     1  16        1  2    P   fragments  normal-(  A  normal-)     \mathrm{P}(A|BC)=\frac{\frac{1}{16}}{\frac{1}{16}+\frac{1}{16}}=\tfrac{1}{2}=%
 \mathrm{P}(A)         P   (  B  |  A  C  )   =    1  16     1  16   +   1  16     =    1  2    =  P   (  B  )      fragments  P   fragments  normal-(  B  normal-|  A  C  normal-)        1  16       1  16     1  16        1  2    P   fragments  normal-(  B  normal-)     \mathrm{P}(B|AC)=\frac{\frac{1}{16}}{\frac{1}{16}+\frac{1}{16}}=\tfrac{1}{2}=%
 \mathrm{P}(B)         P   (  C  |  A  B  )   =    1  16     1  16   +   3  16     =    1  4    =  P   (  C  )      fragments  P   fragments  normal-(  C  normal-|  A  B  normal-)        1  16       1  16     3  16        1  4    P   fragments  normal-(  C  normal-)     \mathrm{P}(C|AB)=\frac{\frac{1}{16}}{\frac{1}{16}+\frac{3}{16}}=\tfrac{1}{4}=%
 \mathrm{P}(C)     Mutual independence  See 4 for a three-event example in which        P   (   A  ∩  B  ∩  C   )    =   P   (  A  )   P   (  B  )   P   (  C  )     ,        normal-P    A  B  C      normal-P  A  normal-P  B  normal-P  C     \mathrm{P}(A\cap B\cap C)=\mathrm{P}(A)\mathrm{P}(B)\mathrm{P}(C),     and yet no two of the three events are pairwise independent (and hence the set of events are not mutually independent). This example shows that mutual independence involves requirements on the products of probabilities of all combinations of events, not just the single events as in this example.  See also   Copula (statistics)  Independent and identically distributed random variables  Mutually exclusive events  Subindependence  Linear dependence between random variables  Conditional independence  Normally distributed and uncorrelated does not imply independent  Mean dependence   References  "  Category:Probability theory  Category:Statistical dependence     ↩  ↩   George, Glyn, "Testing for the independence of three events," Mathematical Gazette 88, November 2004, 568. PDF ↩     