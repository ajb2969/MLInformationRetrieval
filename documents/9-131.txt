   Modified Richardson iteration      Modified Richardson iteration   Modified Richardson iteration is an iterative method for solving a system of linear equations . Richardson iteration was proposed by Lewis Richardson in his work dated 1910. It is similar to the Jacobi and Gauss–Seidel method .  We seek the solution to a set of linear equations, expressed in matrix terms as        A  x   =  b   .        A  x   b    Ax=b.\,     The Richardson iteration is        x   (   k  +  1   )    =    x   (  k  )    +   ω   (   b  -   A   x   (  k  )      )      ,       superscript  x    k  1       superscript  x  k     ω    b    A   superscript  x  k         x^{(k+1)}=x^{(k)}+\omega\left(b-Ax^{(k)}\right),     where   ω   ω   \omega   is a scalar parameter that has to be chosen such that the sequence    x   (  k  )      superscript  x  k    x^{(k)}   converges.  It is easy to see that the method has the correct fixed points , because if it converges, then     x   (   k  +  1   )    ≈   x   (  k  )         superscript  x    k  1     superscript  x  k     x^{(k+1)}\approx x^{(k)}   and    x   (  k  )      superscript  x  k    x^{(k)}   has to approximate a solution of     A  x   =  b        A  x   b    Ax=b   .  Convergence  Subtracting the exact solution   x   x   x   , and introducing the notation for the error     e   (  k  )    =    x   (  k  )    -  x        superscript  e  k      superscript  x  k   x     e^{(k)}=x^{(k)}-x   , we get the equality for the errors        e   (   k  +  1   )    =    e   (  k  )    -   ω  A   e   (  k  )      =    (   I  -   ω  A    )    e   (  k  )      .         superscript  e    k  1       superscript  e  k     ω  A   superscript  e  k              I    ω  A     superscript  e  k       e^{(k+1)}=e^{(k)}-\omega Ae^{(k)}=(I-\omega A)e^{(k)}.     Thus,        ∥   e   (   k  +  1   )    ∥   =   ∥    (   I  -   ω  A    )    e   (  k  )     ∥   ≤    ∥   I  -   ω  A    ∥    ∥   e   (  k  )    ∥     ,         norm   superscript  e    k  1      norm      I    ω  A     superscript  e  k             norm    I    ω  A      norm   superscript  e  k        \|e^{(k+1)}\|=\|(I-\omega A)e^{(k)}\|\leq\|I-\omega A\|\|e^{(k)}\|,     for any vector norm and the corresponding induced matrix norm. Thus, if     ∥   I  -   ω  A    ∥   <  1       norm    I    ω  A     1    \|I-\omega A\|<1   , the method converges.  Suppose that   A   A   A   is diagonalizable and that    (   λ  j   ,   v  j   )      subscript  λ  j    subscript  v  j     (\lambda_{j},v_{j})   are the eigenvalues and eigenvectors of   A   A   A   . The error converges to   0   0    if     |   1  -   ω   λ  j     |   <  1          1    ω   subscript  λ  j      1    |1-\omega\lambda_{j}|<1   for all eigenvalues    λ  j     subscript  λ  j    \lambda_{j}   . If, e.g., all eigenvalues are positive, this can be guaranteed if   ω   ω   \omega   is chosen such that    0  <  ω  <    2  /   λ   m  a  x      (  A  )          0  ω           2   subscript  λ    m  a  x     A      0<\omega<2/\lambda_{max}(A)   . The optimal choice, minimizing all    |   1  -   ω   λ  j     |        1    ω   subscript  λ  j       |1-\omega\lambda_{j}|   , is    ω  =   2  /   (     λ   m  i  n     (  A  )    +    λ   m  a  x     (  A  )     )        ω    2       subscript  λ    m  i  n    A      subscript  λ    m  a  x    A       \omega=2/(\lambda_{min}(A)+\lambda_{max}(A))   , which gives the simplest Chebyshev iteration .  If there are both positive and negative eigenvalues, the method will diverge for any   ω   ω   \omega   if the initial error    e   (  0  )      superscript  e  0    e^{(0)}   has nonzero components in the corresponding eigenvectors .  Equivalence to gradient descent  Consider minimizing the function     F   (  x  )    =    1  2     ∥     A  ~   x   -  b   ∥   2  2          F  x       1  2    superscript   subscript   norm       normal-~  A   x   b    2   2      F(x)=\frac{1}{2}\|\tilde{A}x-b\|_{2}^{2}   . Since this is a convex function , a sufficient condition for optimality is that the gradient is zero (      ∇  F    (  x  )    =  0         normal-∇  F   x   0    \nabla F(x)=0   ) which gives rise to the equation          A  ~   T    A  ~   x   =     A  ~   T    b  ~     .         superscript   normal-~  A   T    normal-~  A   x      superscript   normal-~  A   T    normal-~  b      \tilde{A}^{T}\tilde{A}x=\tilde{A}^{T}\tilde{b}.     Define    A  =     A  ~   T    A  ~        A     superscript   normal-~  A   T    normal-~  A      A=\tilde{A}^{T}\tilde{A}   and    b  =     A  ~   T    b  ~        b     superscript   normal-~  A   T    normal-~  b      b=\tilde{A}^{T}\tilde{b}   . Because of the form of A , it is a positive semi-definite matrix , so it has no negative eigenvalues.  A step of gradient descent is       x   (   k  +  1   )    =    x   (  k  )    -   t   ∇  F    (   x   (  k  )    )     =    x   (  k  )    -   t   (    A   x   (  k  )     -  b   )            superscript  x    k  1       superscript  x  k     t   normal-∇  F    superscript  x  k             superscript  x  k     t      A   superscript  x  k    b        x^{(k+1)}=x^{(k)}-t\nabla F(x^{(k)})=x^{(k)}-t(Ax^{(k)}-b)   which is equivalent to the Richardson iteration by making    t  =  ω      t  ω    t=\omega   .    See also   Richardson extrapolation   References     Appeared in Encyclopaedia of Mathematics (2002), Ed. by Michiel Hazewinkel , Kluwer - ISBN 1-4020-0609-8   "  Category:Numerical linear algebra   