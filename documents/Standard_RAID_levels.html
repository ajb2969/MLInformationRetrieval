<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="120">Standard RAID levels</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Standard RAID levels</h1>
<hr/>
<p>In <a href="computer_storage" title="wikilink">computer storage</a>, the <strong>standard RAID levels</strong> comprise a basic set of <a class="uri" href="RAID" title="wikilink">RAID</a> configurations that employ the techniques of <a href="data_striping" title="wikilink">striping</a>, <a href="Disk_mirroring" title="wikilink">mirroring</a>, or <a href="Parity_bit#RAID" title="wikilink">parity</a> to create large reliable data stores from multiple general-purpose computer <a href="hard_disk_drive" title="wikilink">hard disk drives</a> (HDDs). The most common types are RAID 0 (striping), RAID 1 and its variants (mirroring), RAID 5 (distributed parity), and RAID 6 (dual parity). RAID levels and their associated data formats are standardized by the <a href="Storage_Networking_Industry_Association" title="wikilink">Storage Networking Industry Association</a> (SNIA) in the Common RAID Disk Drive Format (DDF) standard.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="raid-0">RAID 0</h2>
<figure><b>(Figure)</b>
<figcaption>Diagram of a RAID 0 setup</figcaption>
</figure>
<p><strong>RAID 0</strong> (also known as a <em>stripe set</em> or <em>striped volume</em>) splits ("<a href="data_striping" title="wikilink">stripes</a>") data evenly across two or more disks, without <a href="Parity_bit" title="wikilink">parity</a> information, redundancy, or <a href="fault_tolerance" title="wikilink">fault tolerance</a>. Since RAID 0 provides no fault tolerance or redundancy, the failure of one drive will cause the entire array to fail; as a result of having data striped across all disks, the failure will result in total data loss. This configuration is typically implemented having speed as the intended goal.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> RAID 0 is normally used to increase performance, although it can also be used as a way to create a large logical <a href="Volume_(computing)" title="wikilink">volume</a> out of two or more physical disks.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<p>A RAID 0 setup can be created with disks of differing sizes, but the storage space added to the array by each disk is limited to the size of the smallest disk. For example, if a 120 GB disk is striped together with a 320 GB disk, the size of the array will be 240 GB (120 GB × 2).</p>
<p>The diagram shows how the data is distributed into A<em>x</em> stripes to the disks. Accessing the stripes in the order A1, A2, A3, and so forth provides the illusion of a larger and faster drive. Once the stripe size is defined on creation it needs to be maintained at all times.</p>
<h3 id="performance">Performance</h3>
<p>RAID 0 is also used in areas where performance is desired but data integrity is of minimal importance, such as in <a href="computer_gaming" title="wikilink">computer gaming</a> systems. Although some real-world tests with computer games showed a minimal performance gain when using RAID 0, albeit with some desktop applications benefiting,<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> another article examined these claims and concluded: "Striping does not always increase performance (in certain situations it will actually be slower than a non-RAID setup), but in most situations it will yield a significant improvement in performance."<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h2 id="raid-1">RAID 1</h2>
<figure><b>(Figure)</b>
<figcaption>Diagram of a RAID 1 setup</figcaption>
</figure>
<p><strong>RAID 1</strong> consists of an exact copy (or <em><a href="Disk_mirroring" title="wikilink">mirror</a></em>) of a set of data on two or more disks; a classic RAID 1 mirrored pair contains two disks. This configuration offers no parity, striping, or spanning of disk space across multiple disks, since the data is mirrored on all disks belonging to the array, and the array can only be as big as the smallest member disk. This layout is useful when read performance or reliability is more important than write performance or the resulting data storage capacity.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<p>The array will continue to operate so long as at least one member drive is operational.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h3 id="performance-1">Performance</h3>
<p>Any read request can be serviced and handled by any drive in the array; thus, depending on the nature of I/O load, random read performance of a RAID 1 array may equal up to the sum of each member's performance, while the write performance remains at the level of a single disk. However, if disks with different speeds are used in a RAID 1 array, overall write performance is equal to the speed of the slowest disk.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> </p>
<h2 id="raid-2">RAID 2</h2>
<figure><b>(Figure)</b>
<figcaption>Diagram of a RAID 2 setup</figcaption>
</figure>
<p><strong>RAID 2</strong> stripes data at the <a class="uri" href="bit" title="wikilink">bit</a> (rather than block) level, and uses a <a href="Hamming_code" title="wikilink">Hamming code</a> for <a href="error_correction" title="wikilink">error correction</a>. The disks are synchronized by the controller to spin at the same angular orientation (they reach index at the same time), so it generally cannot service multiple requests simultaneously. Extremely high data transfer rates are possible.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></p>
<p>With all hard disk drives implementing internal error correction, the complexity of an external Hamming code offered little advantage over parity so RAID 2 has been rarely implemented; it is the only original level of RAID that is not currently used.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a><a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> </p>
<h2 id="raid-3">RAID 3</h2>
<figure><b>(Figure)</b>
<figcaption>Diagram of a RAID 3 setup of six-byte blocks and two <a href="Parity_bit" title="wikilink">parity</a> bytes, shown are two blocks of data in different colors.</figcaption>
</figure>
<p><strong>RAID 3</strong> consists of <a class="uri" href="byte" title="wikilink">byte</a>-level striping with a dedicated <a href="Parity_bit" title="wikilink">parity</a> disk. RAID 3 is rarely used in practice. One of the characteristics of RAID 3 is that it generally cannot service multiple requests simultaneously, which happens because any single block of data will, by definition, be spread across all members of the set and will reside in the same location. Therefore, any <a href="Input/output" title="wikilink">I/O</a> operation requires activity on every disk and usually requires synchronized spindles.</p>
<p>This makes it suitable for applications that demand the highest transfer rates in long sequential reads and writes, for example <a href="uncompressed_video" title="wikilink">uncompressed video</a> editing. Applications that make small reads and writes from random disk locations will get the worst performance out of this level.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></p>
<p>The requirement that all disks spin synchronously (in a <a href="Lockstep_(computing)" title="wikilink">lockstep</a>) added design considerations to a level that provided no significant advantages over other RAID levels, so it quickly became useless and is now obsolete.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> Both RAID 3 and RAID 4 were quickly replaced by RAID 5.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a> RAID 3 was usually implemented in hardware, and the performance issues were addressed by using large disk caches.<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> </p>
<h2 id="raid-4">RAID 4</h2>
<figure><b>(Figure)</b>
<figcaption>Diagram of a RAID 4 setup with dedicated <a href="Parity_bit" title="wikilink">parity</a> disk with each color representing the group of blocks in the respective <a href="Parity_bit" title="wikilink">parity</a> block (a stripe)</figcaption>
</figure>
<p><strong>RAID 4</strong>, which is rarely used in practice, consists of <a href="Block_size_(data_storage_and_transmission)" title="wikilink">block</a>-level striping with a dedicated <a href="Parity_bit" title="wikilink">parity</a> disk. As a result of its layout, RAID 4 provides good performance of random reads, while the performance of random writes is low due to the need to write all parity data to a single disk.<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a></p>
<p>In the example on the right, a read request for block A1 would be serviced by disk 0. A simultaneous read request for block B1 would have to wait, but a read request for B2 could be serviced concurrently by disk 1. </p>
<h2 id="raid-5">RAID 5</h2>
<figure><b>(Figure)</b>
<figcaption>Diagram of a RAID 5 setup with distributed <a href="Parity_bit" title="wikilink">parity</a> with each color representing the group of blocks in the respective <a href="Parity_bit" title="wikilink">parity</a> block (a stripe). This diagram shows left asymmetric algorithm</figcaption>
</figure>
<p><strong>RAID 5</strong> consists of block-level striping with distributed parity. Unlike in RAID 4, parity information is distributed among the drives. It requires that all drives but one be present to operate. Upon failure of a single drive, subsequent reads can be calculated from the distributed parity such that no data is lost.<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a> RAID 5 requires at least three disks.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a></p>
<p>In comparison to RAID 4, RAID 5's distributed parity evens out the stress of a dedicated parity disk among all RAID members. Additionally, read performance is increased since all RAID members participate in serving of the read requests.<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a> </p>
<h2 id="raid-6">RAID 6</h2>
<figure><b>(Figure)</b>
<figcaption>Diagram of a RAID 6 setup, which is identical to RAID 5 other than the addition of a second <a href="Parity_bit" title="wikilink">parity</a> block</figcaption>
</figure>
<p><strong>RAID 6</strong> extends RAID 5 by adding another <a href="Parity_bit" title="wikilink">parity</a> block; thus, it uses <a href="Block_(data_storage)" title="wikilink">block</a>-level striping with two <a href="Parity_bit" title="wikilink">parity</a> blocks distributed across all member disks.</p>
<h3 id="performance-2">Performance</h3>
<p>RAID 6 does not have a performance penalty for read operations, but it does have a performance penalty on write operations because of the overhead associated with <a href="Parity_bit" title="wikilink">parity</a> calculations. Performance varies greatly depending on how RAID 6 is implemented in the manufacturer's storage architecture—in software, firmware, or by using firmware and specialized <a href="ASIC" title="wikilink">ASICs</a> for intensive <a href="Parity_bit" title="wikilink">parity</a> calculations. It can be as fast as a RAID 5 system with one fewer drive (same number of data drives).<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a></p>
<h3 id="implementation">Implementation</h3>
<p>According to the Storage Networking Industry Association (SNIA), the definition of RAID 6 is: "Any form of RAID that can continue to execute read and write requests to all of a RAID array's virtual disks in the presence of any two concurrent disk failures. Several methods, including dual check data computations (<a href="Parity_bit" title="wikilink">parity</a> and <a href="Reed-Solomon_error_correction" title="wikilink">Reed-Solomon</a>), orthogonal dual <a href="Parity_bit" title="wikilink">parity</a> check data and diagonal <a href="Parity_bit" title="wikilink">parity</a>, have been used to implement RAID Level 6."<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a></p>
<h4 id="parity-computation">Parity computation</h4>
<p>Two different <em><a href="Decoding_methods#Syndrome_decoding" title="wikilink">syndromes</a></em> need to be computed in order to allow the loss of any two drives. One of them, <strong>P</strong> can be the simple <a href="Exclusive_or" title="wikilink">XOR</a> of the data across the stripes, as with RAID 5. A second, independent syndrome is more complicated and requires the assistance of <a href="Field_(mathematics)" title="wikilink">field theory</a>.</p>
<p>To deal with this, the <a href="Finite_field" title="wikilink">Galois field</a> <span class="LaTeX">$GF(m)$</span> is introduced with <span class="LaTeX">$m=2^k$</span>, where <span class="LaTeX">$GF(m) \cong F_2[x]/(p(x))$</span> for a suitable <a href="irreducible_polynomial" title="wikilink">irreducible polynomial</a> <span class="LaTeX">$p(x)$</span> of degree <span class="LaTeX">$k$</span>. A chunk of data can be written as <span class="LaTeX">$d_{k-1}d_{k-2}...d_0$</span> in base 2 where each <span class="LaTeX">$d_i$</span> is either 0 or 1. This is chosen to correspond with the element <span class="LaTeX">$d_{k-1}x^{k-1} + d_{k-2}x^{k-2} + ... + d_1x + d_0$</span> in the Galois field. Let <span class="LaTeX">$D_0,...,D_{n-1} \in GF(m)$</span> correspond to the stripes of data across hard drives encoded as field elements in this manner (in practice they would probably be broken into byte-sized chunks). If <span class="LaTeX">$g$</span> is some <a href="Field_(mathematics)#Some_first_theorems" title="wikilink">generator</a> of the field and <span class="LaTeX">$\oplus$</span> denotes addition in the field while concatenation denotes multiplication, then <span class="LaTeX">$\mathbf{P}$</span> and <span class="LaTeX">$\mathbf{Q}$</span> may be computed as follows (<span class="LaTeX">$n$</span> denotes the number of data disks):</p>
<p><span class="LaTeX">$$\mathbf{P} = \bigoplus_i{D_i} = \mathbf{D}_0 \;\oplus\; \mathbf{D}_1 \;\oplus\; \mathbf{D}_2 \;\oplus\; ... \;\oplus\; \mathbf{D}_{n-1}$$</span></p>
<p><span class="LaTeX">$$\mathbf{Q} = \bigoplus_i{g^iD_i} = g^0\mathbf{D}_0 \;\oplus\; g^1\mathbf{D}_1 \;\oplus\; g^2\mathbf{D}_2 \;\oplus\; ... \;\oplus\; g^{n-1}\mathbf{D}_{n-1}$$</span></p>
<p><em>For a computer scientist, a good way to think about this is that <span class="LaTeX">$\oplus$</span> is a bitwise XOR operator and <span class="LaTeX">$g^i$</span> is the action of a <a href="linear_feedback_shift_register" title="wikilink">linear feedback shift register</a> on a chunk of data.</em> Thus, in the formula above,<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a> the calculation of <strong>P</strong> is just the XOR of each stripe. This is because addition in any <a href="Characteristic_(algebra)" title="wikilink">characteristic two</a> finite field reduces to the XOR operation. The computation of <strong>Q</strong> is the XOR of a shifted version of each stripe.</p>
<p>Mathematically, the <em>generator</em> is an element of the field such that <span class="LaTeX">$g^i$</span> is different for each nonnegative <span class="LaTeX">$i$</span> satisfying <span class="LaTeX">$i < n$</span>.</p>
<p>If one data drive is lost, the data can be recomputed from <strong>P</strong> just like with RAID 5. If two data drives are lost or a data drive and the drive containing <strong>P</strong> are lost, the data can be recovered from <strong>P</strong> and <strong>Q</strong> or from just <strong>Q</strong>, respectively, using a more complex process. The details can be computed using field theory; suppose that <span class="LaTeX">$D_i$</span> and <span class="LaTeX">$D_j$</span> are the lost values with <span class="LaTeX">$i \neq j$</span>, then using the other values of <span class="LaTeX">$D$</span>, constants <span class="LaTeX">$A$</span> and <span class="LaTeX">$B$</span> may be found so that <span class="LaTeX">$D_i \oplus D_j = A$</span> and <span class="LaTeX">$g^iD_i \oplus g^jD_j = B$</span>:</p>
<p><span class="LaTeX">$$A = \bigoplus_{\ell:\;\ell\not=i\;\mathrm{and}\;\ell\not=j}{D_\ell} = \mathbf{P} \;\oplus\; \mathbf{D}_0 \;\oplus\; \mathbf{D}_1 \;\oplus\; \dots \;\oplus\; \mathbf{D}_{i-1} \;\oplus\;  \mathbf{D}_{i+1} \;\oplus\;  \dots \;\oplus\; \mathbf{D}_{j-1}  \;\oplus\; \mathbf{D}_{j+1} \;\oplus\;  \dots \;\oplus\;  \mathbf{D}_{n-1}$$</span></p>
<p><span class="LaTeX">$$B = \bigoplus_{\ell:\;\ell\not=i\;\mathrm{and}\;\ell\not=j}{g^{\ell}D_\ell} = \mathbf{Q} \;\oplus\; g^0\mathbf{D}_0 \;\oplus\; g^1\mathbf{D}_1 \;\oplus\; \dots \;\oplus\; g^{i-1}\mathbf{D}_{i-1} \;\oplus\;  g^{i+1}\mathbf{D}_{i+1} \;\oplus\;  \dots \;\oplus\; g^{j-1}\mathbf{D}_{j-1}  \;\oplus\; g^{j+1}\mathbf{D}_{j+1} \;\oplus\;  \dots \;\oplus\; g^{n-1}\mathbf{D}_{n-1}$$</span></p>
<p>Multiplying both sides of the equation for <span class="LaTeX">$B$</span> by <span class="LaTeX">$g^{n-i}$</span> and adding to the former equation yields <span class="LaTeX">$(g^{n-i+j}\oplus1)D_j = g^{n-i}B\oplus A$</span> and thus a solution for <span class="LaTeX">$D_j$</span>, which may be used to compute <span class="LaTeX">$D_i$</span>.</p>
<p>The computation of <strong>Q</strong> is CPU intensive compared to the simplicity of <strong>P</strong>. Thus, RAID 6 implemented in software will have a more significant effect on system performance, and a hardware solution will be more complex.</p>
<h2 id="comparison">Comparison</h2>
<p>The following table provides an overview of some considerations for standard RAID levels. In each case:</p>
<ul>
<li>Array space efficiency is given as an expression in terms of the number of drives, <span class="LaTeX">$n$</span>; this expression designates a fractional value between zero and one, representing the fraction of the sum of the drives' capacities that is available for use. For example, if three drives are arranged in RAID 3, this gives an array space efficiency of <span class="LaTeX">$1 − 1/ n = 1 − 1/3 = 2/3 ≈ 67%$</span>; thus, if each drive in this example has a capacity of 250 GB, then the array has a total capacity of 750 GB but the capacity that is usable for data storage is only 500 GB.</li>
<li>Array failure rate is given as an expression in terms of the number of drives, <span class="LaTeX">$n$</span>, and the drive failure rate, <span class="LaTeX">$r$</span> (which is assumed identical and independent for each drive). For example, if each of three drives has a failure rate of 5% over the next three years, and these drives are arranged in RAID 3, then this gives an array failure rate over the next three years of:</li>
</ul>
<p><span class="LaTeX">$$\begin{align} 1 - (1 - r)^{n} - nr(1 - r)^{n - 1} & = 1 - (1 - 5\%)^{3} - 3 \times 5\% \times (1 - 5\%)^{3 - 1} \\
& = 1 - 0.95^{3} - 0.15 \times 0.95^{2} \\
& = 1 - 0.857375 - 0.135375 \\
& = 0.00725 \\
& \approx 0.7\% \end{align}$$</span></p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><p>Level</p></th>
<th style="text-align: left;"><p>Description</p></th>
<th style="text-align: left;"><p>Minimum number of drives</p></th>
<th style="text-align: left;"><p>Space efficiency</p></th>
<th style="text-align: left;"><p>Fault tolerance</p></th>
<th style="text-align: left;"><p>Array failure rate</p></th>
<th style="text-align: left;"><p>Read performance</p></th>
<th style="text-align: left;"><p>Write performance</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><p><a href="#RAID_0" title="wikilink">RAID 0</a></p></td>
<td style="text-align: left;"><p>Block-level <a href="Data_striping" title="wikilink">striping</a> without <a href="Parity_bit" title="wikilink">parity</a> or <a href="Disk_mirroring" title="wikilink">mirroring</a></p></td>
<td style="text-align: left;"><p>2</p></td>
<td style="text-align: left;"><p><span class="LaTeX">$1$</span></p></td>
<td style="text-align: left;"><p>None</p></td>
<td style="text-align: left;"><p><mtpl></mtpl></p></td>
<td style="text-align: left;"><p><span class="LaTeX">$n$</span>×</p></td>
<td style="text-align: left;"><p><span class="LaTeX">$n$</span>×</p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p><a href="#RAID_1" title="wikilink">RAID 1</a></p></td>
<td style="text-align: left;"><p>Mirroring without parity or striping</p></td>
<td style="text-align: left;"><p>2</p></td>
<td style="text-align: left;"><p><span class="LaTeX">$\frac{1}{n}$</span></p></td>
<td style="text-align: left;"><p><span class="LaTeX">$n − 1$</span> drive failures</p></td>
<td style="text-align: left;"><p><mtpl></mtpl></p></td>
<td style="text-align: left;"><p><span class="LaTeX">$n$</span>×<a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a></p></td>
<td style="text-align: left;"><p><span class="LaTeX">$1$</span>×<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a></p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p><a href="#RAID_2" title="wikilink">RAID 2</a></p></td>
<td style="text-align: left;"><p>Bit-level striping with <a href="Hamming_code" title="wikilink">Hamming code</a> for error correction</p></td>
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p><mtpl></mtpl></p></td>
<td style="text-align: left;"><p>One drive failure</p></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p><a href="#RAID_3" title="wikilink">RAID 3</a></p></td>
<td style="text-align: left;"><p>Byte-level striping with dedicated parity</p></td>
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p><span class="LaTeX">$1 − \frac{1}{n}$</span></p></td>
<td style="text-align: left;"><p>One drive failure</p></td>
<td style="text-align: left;"><p><mtpl></mtpl></p></td>
<td style="text-align: left;"><p><span class="LaTeX">$( n − 1)$</span>×</p></td>
<td style="text-align: left;"><p><span class="LaTeX">$( n − 1)$</span>×</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p><a href="#RAID_4" title="wikilink">RAID 4</a></p></td>
<td style="text-align: left;"><p>Block-level striping with dedicated parity</p></td>
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p><span class="LaTeX">$1 − \frac{1}{n}$</span></p></td>
<td style="text-align: left;"><p>One drive failure</p></td>
<td style="text-align: left;"><p><mtpl></mtpl></p></td>
<td style="text-align: left;"><p><span class="LaTeX">$( n − 1)$</span>×</p></td>
<td style="text-align: left;"><p><span class="LaTeX">$( n − 1)$</span>×</p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p><a href="#RAID_5" title="wikilink">RAID 5</a></p></td>
<td style="text-align: left;"><p>Block-level striping with distributed parity</p></td>
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p><span class="LaTeX">$1 − \frac{1}{n}$</span></p></td>
<td style="text-align: left;"><p>One drive failure</p></td>
<td style="text-align: left;"><p><mtpl></mtpl></p></td>
<td style="text-align: left;"><p><span class="LaTeX">$n$</span>×</p></td>
<td style="text-align: left;"><p><span class="LaTeX">$( n − 1)$</span>×</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p><a href="#RAID_6" title="wikilink">RAID 6</a></p></td>
<td style="text-align: left;"><p>Block-level striping with double distributed parity</p></td>
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p><span class="LaTeX">$1 − \frac{2}{n}$</span></p></td>
<td style="text-align: left;"><p>Two drive failures</p></td>
<td style="text-align: left;"><p><mtpl></mtpl></p></td>
<td style="text-align: left;"><p><span class="LaTeX">$n$</span>×</p></td>
<td style="text-align: left;"><p><span class="LaTeX">$( n − 2)$</span>×</p></td>
</tr>
</tbody>
</table>
<h2 id="non-standard-raid-levels-and-non-raid-drive-architectures">Non-standard RAID levels and non-RAID drive architectures</h2>
<p>Alternatives to the above designs include <a href="nested_RAID_levels" title="wikilink">nested RAID levels</a>, <a href="non-standard_RAID_levels" title="wikilink">non-standard RAID levels</a>, and <a href="non-RAID_drive_architectures" title="wikilink">non-RAID drive architectures</a>. Non-RAID drive architectures are referred to by similar terms and acronyms, notably <a class="uri" href="JBOD" title="wikilink">JBOD</a> ("just a bunch of disks"), <a href="Spanned_volume" title="wikilink">SPAN/BIG</a>, and <a href="Massive_array_of_idle_disks" title="wikilink">MAID</a> ("massive array of idle disks").</p>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li>[<a class="uri" href="http://web.archive.org/web/20090220092914/http://support.dell.com/support/topics/global.aspx/support/entvideos/raid?c=us&l">http://web.archive.org/web/20090220092914/http://support.dell.com/support/topics/global.aspx/support/entvideos/raid?c=us&l</a>;=en&s;=gen Animations and details on RAID levels 0, 1, and 5], <a class="uri" href="Dell" title="wikilink">Dell</a> (archived from the original on February 20, 2009)</li>
<li><a href="http://www-1.ibm.com/support/docview.wss?uid=swg21149421">IBM summary on RAID levels</a></li>
<li><a href="http://www.dtidata.com/resourcecenter/2008/05/08/raid-configuration-parity-check/">RAID 5 parity explanation and checking tool</a></li>
<li><a href="http://www.icc-usa.com/raid-calculator/">RAID Calculator for Standard RAID Levels and Other RAID Tools</a></li>
<li><a href="http://pages.cs.wisc.edu/~remzi/OSTEP/file-raid.pdf">Redundant Arrays of Inexpensive Disks (RAIDs)</a>, Chapter 38 from the <em>Operating Systems: Three Easy Pieces</em> book</li>
<li><a href="https://docs.oracle.com/cd/E19168-01/817-3337-18/appa_raid_basic.html">Sun StorEdge 3000 Family Configuration Service 2.5 User’s Guide: RAID Basics</a></li>
</ul>
<p>"</p>
<p><a class="uri" href="Category:RAID" title="wikilink">Category:RAID</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"></li>
<li id="fn11"></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"><a href="#fnref14">↩</a></li>
<li id="fn15"></li>
<li id="fn16"></li>
<li id="fn17"></li>
<li id="fn18"></li>
<li id="fn19"><a href="#fnref19">↩</a></li>
<li id="fn20"></li>
<li id="fn21"><a href="#fnref21">↩</a></li>
<li id="fn22"><a href="#fnref22">↩</a></li>
<li id="fn23"><a href="#fnref23">↩</a></li>
<li id="fn24"><a href="#fnref24">↩</a></li>
<li id="fn25"><a href="#fnref25">↩</a></li>
<li id="fn26"><a href="#fnref26">↩</a></li>
<li id="fn27"><a href="#fnref27">↩</a></li>
<li id="fn28"></li>
<li id="fn29"></li>
</ol>
</section>
</body>
</html>
