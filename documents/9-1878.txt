   Matthews correlation coefficient      Matthews correlation coefficient  The '''Matthews correlation coefficient''' is used in [[machine learning]] as a measure of the quality of binary (two-class) [[Binary classification|classifications]], introduced by bi ochemist Brian W. Matthews in 1975. 1 It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between −1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation. The statistic is also known as the phi coefficient . MCC is related to the chi-square statistic for a 2×2 contingency table       |  MCC  |   =     χ  2   n          MCC        superscript  χ  2   n      |\text{MCC}|=\sqrt{\frac{\chi^{2}}{n}}     where n is the total number of observations.  While there is no perfect way of describing the confusion matrix of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures. Other measures, such as the proportion of correct predictions (also termed accuracy ), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.  The MCC can be calculated directly from the confusion matrix using the formula:      MCC  =       T  P   ×  T   N   -     F  P   ×  F   N       (    T  P   +   F  P    )    (    T  P   +   F  N    )    (    T  N   +   F  P    )    (    T  N   +   F  N    )          MCC            T  P   T   N         F  P   F   N            T  P     F  P        T  P     F  N        T  N     F  P        T  N     F  N         \text{MCC}=\frac{TP\times TN-FP\times FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}     In this equation, TP is the number of true positives , TN the number of true negatives , FP the number of false positives and FN the number of false negatives . If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.  The original formula as given by Matthews was: 2      N  =    T  N   +   T  P   +   F  N   +   F  P        N      T  N     T  P     F  N     F  P      \text{N}=TN+TP+FN+FP         S  =     T  P   +   F  N    N       S        T  P     F  N    N     \text{S}=\frac{TP+FN}{N}         P  =     T  P   +   F  P    N       P        T  P     F  P    N     \text{P}=\frac{TP+FP}{N}         MCC  =      T  P   /  N   -   S  ×  P      P  S   (   1  -  S   )    (   1  -  P   )          MCC          T  P   N     S  P        P  S    1  S     1  P        \text{MCC}=\frac{TP/N-S\times P}{\sqrt{PS(1-S)(1-P)}}     This is equal to the formula given above. As a correlation coefficient , the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual . The component regression coefficients of the Matthews correlation coefficient are markedness (deltap) and informedness (deltap'). 3 4  Confusion Matrix      Terminology and derivations from a confusion matrix     true positive (TP)  eqv. with hit  true negative (TN)  eqv. with correct rejection  false positive (FP)  eqv. with false alarm , Type I error   false negative (FN)  eqv. with miss, Type II error      sensitivity or true positive rate (TPR)  eqv. with hit rate , recall       𝑇𝑃𝑅  =   𝑇𝑃  /  P   =   𝑇𝑃  /   (   𝑇𝑃  +  𝐹𝑁   )          𝑇𝑃𝑅    𝑇𝑃  P          𝑇𝑃    𝑇𝑃  𝐹𝑁       \mathit{TPR}=\mathit{TP}/P=\mathit{TP}/(\mathit{TP}+\mathit{FN})     specificity (SPC) or True Negative Rate      𝑆𝑃𝐶  =   𝑇𝑁  /  N   =   𝑇𝑁  /   (   𝐹𝑃  +  𝑇𝑁   )          𝑆𝑃𝐶    𝑇𝑁  N          𝑇𝑁    𝐹𝑃  𝑇𝑁       \mathit{SPC}=\mathit{TN}/N=\mathit{TN}/(\mathit{FP}+\mathit{TN})     precision or positive predictive value (PPV)      𝑃𝑃𝑉  =   𝑇𝑃  /   (   𝑇𝑃  +  𝐹𝑃   )        𝑃𝑃𝑉    𝑇𝑃    𝑇𝑃  𝐹𝑃      \mathit{PPV}=\mathit{TP}/(\mathit{TP}+\mathit{FP})     negative predictive value (NPV)      𝑁𝑃𝑉  =   𝑇𝑁  /   (   𝑇𝑁  +  𝐹𝑁   )        𝑁𝑃𝑉    𝑇𝑁    𝑇𝑁  𝐹𝑁      \mathit{NPV}=\mathit{TN}/(\mathit{TN}+\mathit{FN})     fall-out or false positive rate (FPR)      𝐹𝑃𝑅  =   𝐹𝑃  /  N   =   𝐹𝑃  /   (   𝐹𝑃  +  𝑇𝑁   )          𝐹𝑃𝑅    𝐹𝑃  N          𝐹𝑃    𝐹𝑃  𝑇𝑁       \mathit{FPR}=\mathit{FP}/N=\mathit{FP}/(\mathit{FP}+\mathit{TN})     false discovery rate (FDR)      𝐹𝐷𝑅  =   𝐹𝑃  /   (   𝐹𝑃  +  𝑇𝑃   )    =   1  -  𝑃𝑃𝑉         𝐹𝐷𝑅    𝐹𝑃    𝐹𝑃  𝑇𝑃           1  𝑃𝑃𝑉      \mathit{FDR}=\mathit{FP}/(\mathit{FP}+\mathit{TP})=1-\mathit{PPV}     Miss Rate or False Negative Rate (FNR)      𝐹𝑁𝑅  =   𝐹𝑁  /   (   𝐹𝑁  +  𝑇𝑃   )        𝐹𝑁𝑅    𝐹𝑁    𝐹𝑁  𝑇𝑃      \mathit{FNR}=\mathit{FN}/(\mathit{FN}+\mathit{TP})        accuracy (ACC)      𝐴𝐶𝐶  =    (   𝑇𝑃  +  𝑇𝑁   )   /   (   P  +  N   )        𝐴𝐶𝐶      𝑇𝑃  𝑇𝑁     P  N      \mathit{ACC}=(\mathit{TP}+\mathit{TN})/(P+N)     F1 score  is the harmonic mean of precision and sensitivity       F1  =    2  𝑇𝑃   /   (    2  𝑇𝑃   +  𝐹𝑃  +  𝐹𝑁   )        italic-F1      2  𝑇𝑃       2  𝑇𝑃   𝐹𝑃  𝐹𝑁      \mathit{F1}=2\mathit{TP}/(2\mathit{TP}+\mathit{FP}+\mathit{FN})     Matthews correlation coefficient (MCC)  \frac{ TP \times TN - FP \times FN } {\sqrt{ (TP+FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }     Informedness        T  P  R   +   S  P  C    -  1          T  P  R     S  P  C    1    TPR+SPC-1     Markedness        P  P  V   +   N  P  V    -  1          P  P  V     N  P  V    1    PPV+NPV-1      ; Sources: Fawcett (2006) and Powers (2011). 5 6     Let us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 contingency table or confusion matrix , as follows:  See also   Phi coefficient  F1 score  Cramér's V , a similar measure of association between nominal variables.  Cohen's kappa   References  "  Category:Machine learning  Category:Information retrieval evaluation  Category:Statistical classification  Category:Computational chemistry  Category:Cheminformatics  Category:Bioinformatics  Category:Statistical ratios  Category:Summary statistics for contingency tables     ↩   ↩  ↩  ↩      