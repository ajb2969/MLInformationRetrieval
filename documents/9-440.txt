   Dirichlet-multinomial distribution      Dirichlet-multinomial distribution   In probability and statistics , the Dirichlet-multinomial distribution is a probability distribution for a multivariate discrete random variable . It is also called the Dirichlet compound multinomial distribution (DCM) or multivariate Pólya distribution (after George Pólya ). It is a compound probability distribution , where a probability vector p is drawn from a Dirichlet distribution with parameter vector   𝜶   𝜶   \boldsymbol{\alpha}   , and a set of discrete samples is drawn from the categorical distribution with probability vector p . The compounding corresponds to a Polya urn scheme . In document classification , for example, the distribution is used to represent the distributions of word counts for different document types.  Probability mass function  Conceptually, we are doing N independent draws from a categorical distribution with K categories. Let us represent the independent draws as random categorical variables    z  n     subscript  z  n    z_{n}   for    n  =   1  …  N       n    1  normal-…  N     n=1\dots N   . Let us denote the number of times a particular category   k   k   k   has been seen (for    k  =   1  …  K       k    1  normal-…  K     k=1\dots K   ) among all the categorical variables as    n  k     subscript  n  k    n_{k}   . Note that      ∑  k    n  k    =  N        subscript   k    subscript  n  k    N    \sum_{k}n_{k}=N   . Then, we have two separate views onto this problem:   A set of   N   N   N   categorical variables     z  1   ,  …  ,   z  N       subscript  z  1   normal-…   subscript  z  N     z_{1},\dots,z_{N}   .  A single vector-valued variable    𝐱  =   (   n  1   ,  …  ,   n  K   )       𝐱    subscript  n  1   normal-…   subscript  n  K      \mathbf{x}=(n_{1},\dots,n_{K})   , distributed according to a multinomial distribution .   The former case is a set of random variables specifying each individual outcome, while the latter is a variable specifying the number of outcomes of each of the K categories. The distinction is important, as the two cases have correspondingly different probability distributions.  The parameter of the categorical distribution is     𝐩  =   (   p  1   ,   p  2   ,  …  ,   p  K   )    ,      𝐩    subscript  p  1    subscript  p  2   normal-…   subscript  p  K      \mathbf{p}=(p_{1},p_{2},\dots,p_{K}),   where    p  k     subscript  p  k    p_{k}   is the probability to draw value   k   k   k   ;   𝐩   𝐩   \mathbf{p}   is likewise the parameter of the multinomial distribution    P   (  𝐱  |  𝐩  )      fragments  P   fragments  normal-(  x  normal-|  p  normal-)     P(\mathbf{x}|\mathbf{p})   . Rather than specifying   𝐩   𝐩   \mathbf{p}   directly, we give it a conjugate prior distribution , and hence it is drawn from a Dirichlet distribution with parameter vector    𝜶  =   (   α  1   ,   α  2   ,  …  ,   α  K   )       𝜶    subscript  α  1    subscript  α  2   normal-…   subscript  α  K      \boldsymbol{\alpha}=(\alpha_{1},\alpha_{2},\ldots,\alpha_{K})   .  By integrating out   𝐩   𝐩   \mathbf{p}   , we obtain a compound distribution. However, the form of the distribution is different depending on which view we take.  For a set of individual outcomes  Joint distribution  For categorical variables    ℤ  =    z  1   ,  …  ,   z  N        ℤ    subscript  z  1   normal-…   subscript  z  N      \mathbb{Z}=z_{1},\dots,z_{N}   , the marginal  joint distribution is obtained by integrating out   𝐩   𝐩   \mathbf{p}   :       Pr   (  ℤ  ∣  𝜶  )    =    ∫  𝐩     Pr   (  ℤ  ∣  𝐩  )     Pr   (  𝐩  ∣  𝜶  )    d  𝐩         Pr  ℤ  𝜶     subscript   𝐩      Pr  ℤ  𝐩    Pr  𝐩  𝜶   d  𝐩      \Pr(\mathbb{Z}\mid\boldsymbol{\alpha})=\int_{\mathbf{p}}\Pr(\mathbb{Z}\mid%
 \mathbf{p})\Pr(\mathbf{p}\mid\boldsymbol{\alpha})\textrm{d}\mathbf{p}     which results in the following explicit formula:       Pr   (  ℤ  ∣  𝜶  )    =     Γ   (  A  )     Γ   (   N  +  A   )       ∏   k  =  1   K     Γ   (    n  k   +   α  k    )     Γ   (   α  k   )            Pr  ℤ  𝜶         normal-Γ  A     normal-Γ    N  A       superscript   subscript  product    k  1    K       normal-Γ     subscript  n  k    subscript  α  k       normal-Γ   subscript  α  k         \Pr(\mathbb{Z}\mid\boldsymbol{\alpha})=\frac{\Gamma\left(A\right)}{\Gamma\left%
 (N+A\right)}\prod_{k=1}^{K}\frac{\Gamma(n_{k}+\alpha_{k})}{\Gamma(\alpha_{k})}     where   Γ   normal-Γ   \Gamma   is the gamma function , with      A  =    ∑  k     α  k   and  N    =    ∑  k     n  k   , and where   n  k     =   number of   z  n   ’s with the value  k  .         A    subscript   k      subscript  α  k   and  N           subscript   k      subscript  n  k   , and where   subscript  n  k            number of   subscript  z  n   ’s with the value  k  .      A=\sum_{k}\alpha_{k}\text{ and }N=\sum_{k}n_{k}\text{, and where }n_{k}=\text{%
 number of }z_{n}\text{'s with the value }k\text{.}     Note that, although the variables     z  1   ,  …  ,   z  N       subscript  z  1   normal-…   subscript  z  N     z_{1},\dots,z_{N}   do not appear explicitly in the above formula, they enter in through the    n  k     subscript  n  k    n_{k}   values.  Conditional distribution  Another useful formula, particularly in the context of Gibbs sampling , asks what the conditional density of a given variable    z  n     subscript  z  n    z_{n}   is, conditioned on all the other variables (which we will denote    ℤ   (   -  n   )      superscript  ℤ    n     \mathbb{Z}^{(-n)}   ). It turns out to have an extremely simple form:       Pr   (    z  n   =  k   ∣   ℤ   (   -  n   )    ,  𝜶  )    ∝    n  k   (   -  n   )    +   α  k       proportional-to   Pr     subscript  z  n   k    superscript  ℤ    n    𝜶      superscript   subscript  n  k     n     subscript  α  k      \Pr(z_{n}=k\mid\mathbb{Z}^{(-n)},\boldsymbol{\alpha})\propto n_{k}^{(-n)}+%
 \alpha_{k}     where    n  k   (   -  n   )      superscript   subscript  n  k     n     n_{k}^{(-n)}   specifies the number of counts of category   k   k   k   seen in all variables other than    z  n     subscript  z  n    z_{n}   .  It may be useful to show how to derive this formula. In general, conditional distributions are proportional to the corresponding joint distributions , so we simply start with the above formula for the joint distribution of all the     z  1   ,  …  ,   z  N       subscript  z  1   normal-…   subscript  z  N     z_{1},\dots,z_{N}   values and then eliminate any factors not dependent on the particular    z  n     subscript  z  n    z_{n}   in question. To do this, we make use of the notation    n  k   (   -  n   )      superscript   subscript  n  k     n     n_{k}^{(-n)}   defined above, and note that       n  j   =   {       n  j   (   -  n   )    ,       if  j   ≠  k          n  j   (   -  n   )    +  1   ,       if  j   =  k            subscript  n  j    cases   superscript   subscript  n  j     n        if  j   k      superscript   subscript  n  j     n    1       if  j   k      n_{j}=\begin{cases}n_{j}^{(-n)},&\text{if }j\not=k\\
 n_{j}^{(-n)}+1,&\text{if }j=k\end{cases}     We also use the fact that       Γ   (   n  +  1   )    =   n  Γ   (  n  )          normal-Γ    n  1      n  normal-Γ  n     \Gamma(n+1)=n\Gamma(n)     Then:      Pr   (    z  n   =  k   ∣   ℤ   (   -  n   )    ,  𝜶  )      Pr     subscript  z  n   k    superscript  ℤ    n    𝜶    \displaystyle\Pr(z_{n}=k\mid\mathbb{Z}^{(-n)},\boldsymbol{\alpha})     In general, it is not necessary to worry about the normalizing constant at the time of deriving the equations for conditional distributions. The normalizing constant will be determined as part of the algorithm for sampling from the distribution (see Categorical distribution#Sampling ). However, when the conditional distribution is written in the simple form above, it turns out that the normalizing constant assumes a simple form:        ∑  k    (    n  k   (   -  n   )    +   α  k    )    =   A  +    ∑  k    n  k   (   -  n   )      =    A  +  N   -  1           subscript   k      superscript   subscript  n  k     n     subscript  α  k       A    subscript   k    superscript   subscript  n  k     n               A  N   1      \sum_{k}\left(n_{k}^{(-n)}+\alpha_{k}\right)=A+\sum_{k}n_{k}^{(-n)}=A+N-1     Hence       Pr   (    z  n   =  k   ∣   ℤ   (   -  n   )    ,  𝜶  )    =     n  k   (   -  n   )    +   α  k      A  +  N   -  1         Pr     subscript  z  n   k    superscript  ℤ    n    𝜶        superscript   subscript  n  k     n     subscript  α  k        A  N   1      \Pr(z_{n}=k\mid\mathbb{Z}^{(-n)},\boldsymbol{\alpha})=\frac{n_{k}^{(-n)}+%
 \alpha_{k}}{A+N-1}     This formula is closely related to the Chinese restaurant process , which results from taking the limit as    K  →  ∞     normal-→  K     K\to\infty   .  In a Bayesian network  In a larger Bayesian network in which categorical (or so-called "multinomial") distributions occur with Dirichlet distribution priors as part of a larger network, all Dirichlet priors can be collapsed provided that the only nodes depending on them are categorical distributions. The collapsing happens for each Dirichlet-distribution node separately from the others, and occurs regardless of any other nodes that may depend on the categorical distributions. It also occurs regardless of whether the categorical distributions depend on nodes additional to the Dirichlet priors (although in such a case, those other nodes must remain as additional conditioning factors). Essentially, all of the categorical distributions depending on a given Dirichlet-distribution node become connected into a single Dirichlet-multinomial joint distribution defined by the above formula. The joint distribution as defined this way will depend on the parent(s) of the integrated-out Dirichet prior nodes, as well as any parent(s) of the categorical nodes other than the Dirichlet prior nodes themselves.  In the following sections, we discuss different configurations commonly found in Bayesian networks. We repeat the probability density from above, and define it using the symbol    DirMult   (  ℤ  ∣  𝜶  )      DirMult  ℤ  𝜶    \operatorname{DirMult}(\mathbb{Z}\mid\boldsymbol{\alpha})   :      P  r   (  ℤ  ∣  𝜶  )   =  DirMult   (  ℤ  ∣  𝜶  )   =    Γ   (    ∑  k    α  k    )     Γ   (     ∑  k    n  k    +   α  k    )      ∏   k  =  1   K     Γ   (    n  k   +   α  k    )     Γ   (   α  k   )        fragments  P  r   fragments  normal-(  Z  normal-∣  α  normal-)    DirMult   fragments  normal-(  Z  normal-∣  α  normal-)        normal-Γ    subscript   k    subscript  α  k       normal-Γ      subscript   k    subscript  n  k     subscript  α  k       superscript   subscript  product    k  1    K       normal-Γ     subscript  n  k    subscript  α  k       normal-Γ   subscript  α  k       Pr(\mathbb{Z}\mid\boldsymbol{\alpha})=\operatorname{DirMult}(\mathbb{Z}\mid%
 \boldsymbol{\alpha})=\frac{\Gamma\left(\sum_{k}\alpha_{k}\right)}{\Gamma\left(%
 \sum_{k}n_{k}+\alpha_{k}\right)}\prod_{k=1}^{K}\frac{\Gamma(n_{k}+\alpha_{k})}%
 {\Gamma(\alpha_{k})}     Multiple Dirichlet priors with the same hyperprior  Imagine we have a hierarchical model as follows:        𝜶    ∼    some distribution       𝜽   d  =   1  …  M       ∼      Dirichlet  K    (  𝜶  )         z    d  =   1  …  M    ,   n  =   1  …   N  d         ∼      Categorical  K    (   𝜽  d   )          𝜶  similar-to  some distribution     subscript  𝜽    d    1  normal-…  M     similar-to    subscript  Dirichlet  K   𝜶      subscript  z   formulae-sequence    d    1  normal-…  M      n    1  normal-…   subscript  N  d       similar-to    subscript  Categorical  K    subscript  𝜽  d       \begin{array}[]{lcl}\boldsymbol{\alpha}&\sim&\text{some distribution}\\
 \boldsymbol{\theta}_{d=1\dots M}&\sim&\operatorname{Dirichlet}_{K}(\boldsymbol%
 {\alpha})\\
 z_{d=1\dots M,n=1\dots N_{d}}&\sim&\operatorname{Categorical}_{K}(\boldsymbol{%
 \theta}_{d})\end{array}     In cases like this, we have multiple Dirichet priors, each of which generates some number of categorical observations (possibly a different number for each prior). The fact that they are all dependent on the same hyperprior, even if this is a random variable as above, makes no difference. The effect of integrating out a Dirichlet prior links the categorical variables attached to that prior, whose joint distribution simply inherits any conditioning factors of the Dirichlet prior. The fact that multiple priors may share a hyperprior makes no difference:       Pr   (  ℤ  ∣  𝜶  )    =    ∏  d    DirMult   (   ℤ  d   ∣  𝜶  )          Pr  ℤ  𝜶     subscript  product  d    DirMult   subscript  ℤ  d   𝜶      \Pr(\mathbb{Z}\mid\boldsymbol{\alpha})=\prod_{d}\operatorname{DirMult}(\mathbb%
 {Z}_{d}\mid\boldsymbol{\alpha})     where    ℤ  d     subscript  ℤ  d    \mathbb{Z}_{d}   is simply the collection of categorical variables dependent on prior d .  Accordingly, the conditional probability distribution can be written as follows:       Pr   (    z   d  n    =  k   ∣   ℤ   (   -   d  n    )    ,  𝜶  )    ∝    n   k  ,  d    (   -  n   )    +   α  k       proportional-to   Pr     subscript  z    d  n    k    superscript  ℤ      d  n     𝜶      superscript   subscript  n   k  d      n     subscript  α  k      \Pr(z_{dn}=k\mid\mathbb{Z}^{(-dn)},\boldsymbol{\alpha})\ \propto\ n_{k,d}^{(-n%
 )}+\alpha_{k}     where    n   k  ,  d    (   -  n   )      superscript   subscript  n   k  d      n     n_{k,d}^{(-n)}   specifically means the number of variables among the set     ℤ  d     subscript  ℤ  d    \mathbb{Z}_{d}   , excluding    z   d  n      subscript  z    d  n     z_{dn}   itself, that have the value   k   k   k   .  Note in particular that we need to count only the variables having the value k that are tied together to the variable in question through having the same prior. We do not want to count any other variables also having the value k .  Multiple Dirichlet priors with the same hyperprior, with dependent children  Now imagine a slightly more complicated hierarchical model as follows:        𝜶    ∼    some distribution       𝜽   d  =   1  …  M       ∼      Dirichlet  K    (  𝜶  )         z    d  =   1  …  M    ,   n  =   1  …   N  d         ∼      Categorical  K    (   𝜽  d   )        ϕ    ∼    some other distribution       w    d  =   1  …  M    ,   n  =   1  …   N  d         ∼     F   (   w   d  n    ∣   z   d  n    ,  ϕ  )          𝜶  similar-to  some distribution     subscript  𝜽    d    1  normal-…  M     similar-to    subscript  Dirichlet  K   𝜶      subscript  z   formulae-sequence    d    1  normal-…  M      n    1  normal-…   subscript  N  d       similar-to    subscript  Categorical  K    subscript  𝜽  d      bold-italic-ϕ  similar-to  some other distribution     subscript  w   formulae-sequence    d    1  normal-…  M      n    1  normal-…   subscript  N  d       similar-to   normal-F   subscript  w    d  n     subscript  z    d  n    bold-italic-ϕ      \begin{array}[]{lcl}\boldsymbol{\alpha}&\sim&\text{some distribution}\\
 \boldsymbol{\theta}_{d=1\dots M}&\sim&\operatorname{Dirichlet}_{K}(\boldsymbol%
 {\alpha})\\
 z_{d=1\dots M,n=1\dots N_{d}}&\sim&\operatorname{Categorical}_{K}(\boldsymbol{%
 \theta}_{d})\\
 \boldsymbol{\phi}&\sim&\text{some other distribution}\\
 w_{d=1\dots M,n=1\dots N_{d}}&\sim&\operatorname{F}(w_{dn}\mid z_{dn},%
 \boldsymbol{\phi})\end{array}     This model is the same as above, but in addition, each of the categorical variables has a child variable dependent on it. This is typical of a mixture model .  Again, in the joint distribution, only the categorical variables dependent on the same prior are linked into a single Dirichlet-multinomial:       Pr   (  ℤ  ,  𝕎  ∣  𝜶  ,  ϕ  )    =    ∏  d     DirMult   (   ℤ  d   ∣  𝜶  )      ∏   d  =  1   M     ∏   n  =  1    N  d     F   (   w   d  n    ∣   z   d  n    ,  ϕ  )             Pr  ℤ  𝕎  𝜶  bold-italic-ϕ     subscript  product  d      DirMult   subscript  ℤ  d   𝜶     superscript   subscript  product    d  1    M     superscript   subscript  product    n  1     subscript  N  d     normal-F   subscript  w    d  n     subscript  z    d  n    bold-italic-ϕ         \Pr(\mathbb{Z},\mathbb{W}\mid\boldsymbol{\alpha},\boldsymbol{\phi})=\prod_{d}%
 \operatorname{DirMult}(\mathbb{Z}_{d}\mid\boldsymbol{\alpha})\prod_{d=1}^{M}%
 \prod_{n=1}^{N_{d}}\operatorname{F}(w_{dn}\mid z_{dn},\boldsymbol{\phi})     The conditional distribution of the categorical variables dependent only on their parents and ancestors would have the identical form as above in the simpler case. However, in Gibbs sampling it is necessary to determine the conditional distribution of a given node    z   d  n      subscript  z    d  n     z_{dn}   dependent not only on    ℤ   (   -   d  n    )      superscript  ℤ      d  n      \mathbb{Z}^{(-dn)}   and ancestors such as   α   α   \alpha   but on all the other parameters.  Note however that we derived the simplified expression for the conditional distribution above simply by rewriting the expression for the joint probability and removing constant factors. Hence, the same simplification would apply in a larger joint probability expression such as the one in this model, composed of Dirichlet-multinomial densities plus factors for many other random variables dependent on the values of the categorical variables.  This yields the following:       Pr   (    z   d  n    =  k   ∣   ℤ   (   -   d  n    )    ,  𝕎  ,  𝜶  ,  ϕ  )    ∝    (    n   k  ,  d    (   -  n   )    +   α  k    )    F   (   w   d  n    ∣   z   d  n    ,  ϕ  )        proportional-to   Pr     subscript  z    d  n    k    superscript  ℤ      d  n     𝕎  𝜶  bold-italic-ϕ        superscript   subscript  n   k  d      n     subscript  α  k     normal-F   subscript  w    d  n     subscript  z    d  n    bold-italic-ϕ      \Pr(z_{dn}=k\mid\mathbb{Z}^{(-dn)},\mathbb{W},\boldsymbol{\alpha},\boldsymbol{%
 \phi})\ \propto\ (n_{k,d}^{(-n)}+\alpha_{k})\operatorname{F}(w_{dn}\mid z_{dn}%
 ,\boldsymbol{\phi})     Here the probability density of   F   normal-F   \operatorname{F}   appears directly. To do random sampling over    z   d  n      subscript  z    d  n     z_{dn}   , we would compute the unnormalized probabilities for all K possibilities for    z   d  n      subscript  z    d  n     z_{dn}   using the above formula, then normalize them and proceed as normal using the algorithm described in the categorical distribution article.  NOTE : Correctly speaking, the additional factor that appears in the conditional distribution is derived not from the model specification but directly from the joint distribution. This distinction is important when considering models where a given node with Dirichlet-prior parent has multiple dependent children, particularly when those children are dependent on each other (e.g. if they share a parent that is collapsed out). This is discussed more below.  Multiple Dirichlet priors with shifting prior membership  Now imagine we have a hierarchical model as follows:        𝜽    ∼    some distribution       z   n  =   1  …  N       ∼      Categorical  K    (  𝜽  )        𝜶    ∼    some distribution       ϕ   k  =   1  …  K       ∼      Dirichlet  V    (  𝜶  )         w   n  =   1  …  N       ∼      Categorical  V    (   ϕ   z  n    )          𝜽  similar-to  some distribution     subscript  z    n    1  normal-…  N     similar-to    subscript  Categorical  K   𝜽     𝜶  similar-to  some distribution     subscript  bold-italic-ϕ    k    1  normal-…  K     similar-to    subscript  Dirichlet  V   𝜶      subscript  w    n    1  normal-…  N     similar-to    subscript  Categorical  V    subscript  bold-italic-ϕ   subscript  z  n        \begin{array}[]{lcl}\boldsymbol{\theta}&\sim&\text{some distribution}\\
 z_{n=1\dots N}&\sim&\operatorname{Categorical}_{K}(\boldsymbol{\theta})\\
 \boldsymbol{\alpha}&\sim&\text{some distribution}\\
 \boldsymbol{\phi}_{k=1\dots K}&\sim&\operatorname{Dirichlet}_{V}(\boldsymbol{%
 \alpha})\\
 w_{n=1\dots N}&\sim&\operatorname{Categorical}_{V}(\boldsymbol{\phi}_{z_{n}})%
 \\
 \end{array}     Here we have a tricky situation where we have multiple Dirichlet priors as before and a set of dependent categorical variables, but the relationship between the priors and dependent variables isn't fixed, unlike before. Instead, the choice of which prior to use is dependent on another random categorical variable. This occurs, for example, in topic models, and indeed the names of the variables above are meant to correspond to those in latent Dirichlet allocation . In this case, the set   𝕎   𝕎   \mathbb{W}   is a set of words, each of which is drawn from one of   K   K   K   possible topics, where each topic is a Dirichlet prior over a vocabulary of   V   V   V   possible words, specifying the frequency of different words in the topic. However, the topic membership of a given word isn't fixed; rather, it's determined from a set of latent variables    ℤ   ℤ   \mathbb{Z}   . There is one latent variable per word, a   K   K   K   -dimensional categorical variable specifying the topic the word belongs to.  In this case, all variables dependent on a given prior are tied together (i.e. correlated ) in a group, as before — specifically, all words belonging to a given topic are linked. In this case, however, the group membership shifts, in that the words are not fixed to a given topic but the topic depends on the value of a latent variable associated with the word. However, note that the definition of the Dirichlet-multinomial density doesn't actually depend on the number of categorical variables in a group (i.e. the number of words in the document generated from a given topic), but only on the counts of how many variables in the group have a given value (i.e. among all the word tokens generated from a given topic, how many of them are a given word). Hence, we can still write an explicit formula for the joint distribution:       Pr   (  𝕎  ∣  𝜶  ,  ℤ  )    =    ∏   k  =  1   K    DirMult   (   𝕎  k   ∣  ℤ  ,  𝜶  )     =    ∏   k  =  1   K    [     Γ   (    ∑  v    α  v    )     Γ   (     ∑  v    n  v  k    +   α  v    )       ∏   v  =  1   V     Γ   (    n  v  k   +   α  v    )     Γ   (   α  v   )       ]           Pr  𝕎  𝜶  ℤ     superscript   subscript  product    k  1    K    DirMult   subscript  𝕎  k   ℤ  𝜶           superscript   subscript  product    k  1    K    delimited-[]        normal-Γ    subscript   v    subscript  α  v       normal-Γ      subscript   v    superscript   subscript  n  v   k     subscript  α  v        superscript   subscript  product    v  1    V       normal-Γ     superscript   subscript  n  v   k    subscript  α  v       normal-Γ   subscript  α  v            \Pr(\mathbb{W}\mid\boldsymbol{\alpha},\mathbb{Z})=\prod_{k=1}^{K}\operatorname%
 {DirMult}(\mathbb{W}_{k}\mid\mathbb{Z},\boldsymbol{\alpha})=\prod_{k=1}^{K}%
 \left[\frac{\Gamma\left(\sum_{v}\alpha_{v}\right)}{\Gamma\left(\sum_{v}n_{v}^{%
 k}+\alpha_{v}\right)}\prod_{v=1}^{V}\frac{\Gamma(n_{v}^{k}+\alpha_{v})}{\Gamma%
 (\alpha_{v})}\right]     Here we use the notation    n  v  k     superscript   subscript  n  v   k    n_{v}^{k}   to denote the number of word tokens whose value is word symbol v and which belong to topic k .  The conditional distribution still has the same form:       Pr   (    w  n   =  v   ∣   𝕎   (   -  n   )    ,  ℤ  ,  𝜶  )    ∝    n  v   k  ,   (   -  n   )     +   α  v       proportional-to   Pr     subscript  w  n   v    superscript  𝕎    n    ℤ  𝜶      superscript   subscript  n  v    k    n      subscript  α  v      \Pr(w_{n}=v\mid\mathbb{W}^{(-n)},\mathbb{Z},\boldsymbol{\alpha})\ \propto\ n_{%
 v}^{k,(-n)}+\alpha_{v}     Here again, only the categorical variables for words belonging to a given topic are linked (even though this linking will depend on the assignments of the latent variables), and hence the word counts need to be over only the words generated by a given topic. Hence the symbol    n  v   k  ,   (   -  n   )       superscript   subscript  n  v    k    n      n_{v}^{k,(-n)}   , which is the count of words tokens having the word symbol v , but only among those generated by topic k , and excluding the word itself whose distribution is being described.  (Note that the reason why excluding the word itself is necessary, and why it even makes sense at all, is that in a Gibbs sampling context, we repeatedly resample the values of each random variable, after having run through and sampled all previous variables. Hence the variable will already have a value, and we need to exclude this existing value from the various counts that we make use of.)  A combined example: LDA topic models  We now show how to combine some of the above scenarios to demonstrate how to Gibbs sample a real-world model, specifically a smoothed latent Dirichlet allocation (LDA) topic model .  The model is as follows:        𝜶    ∼    A Dirichlet hyperprior, either a constant or a random variable      𝜷    ∼    A Dirichlet hyperprior, either a constant or a random variable       𝜽   d  =   1  …  M       ∼      Dirichlet  K    (  𝜶  )         ϕ   k  =   1  …  K       ∼      Dirichlet  V    (  𝜷  )         z    d  =   1  …  M    ,   n  =   1  …   N  d         ∼      Categorical  K    (   𝜽  d   )         w    d  =   1  …  M    ,   n  =   1  …   N  d         ∼      Categorical  V    (   ϕ   z   d  n     )          𝜶  similar-to  A Dirichlet hyperprior, either a constant or a random variable    𝜷  similar-to  A Dirichlet hyperprior, either a constant or a random variable     subscript  𝜽    d    1  normal-…  M     similar-to    subscript  Dirichlet  K   𝜶      subscript  bold-italic-ϕ    k    1  normal-…  K     similar-to    subscript  Dirichlet  V   𝜷      subscript  z   formulae-sequence    d    1  normal-…  M      n    1  normal-…   subscript  N  d       similar-to    subscript  Categorical  K    subscript  𝜽  d       subscript  w   formulae-sequence    d    1  normal-…  M      n    1  normal-…   subscript  N  d       similar-to    subscript  Categorical  V    subscript  bold-italic-ϕ   subscript  z    d  n         \begin{array}[]{lcl}\boldsymbol{\alpha}&\sim&\text{A Dirichlet hyperprior, %
 either a constant or a random variable}\\
 \boldsymbol{\beta}&\sim&\text{A Dirichlet hyperprior, either a constant or a %
 random variable}\\
 \boldsymbol{\theta}_{d=1\dots M}&\sim&\operatorname{Dirichlet}_{K}(\boldsymbol%
 {\alpha})\\
 \boldsymbol{\phi}_{k=1\dots K}&\sim&\operatorname{Dirichlet}_{V}(\boldsymbol{%
 \beta})\\
 z_{d=1\dots M,n=1\dots N_{d}}&\sim&\operatorname{Categorical}_{K}(\boldsymbol{%
 \theta}_{d})\\
 w_{d=1\dots M,n=1\dots N_{d}}&\sim&\operatorname{Categorical}_{V}(\boldsymbol{%
 \phi}_{z_{dn}})\\
 \end{array}     Essentially we combine the previous three scenarios: We have categorical variables dependent on multiple priors sharing a hyperprior; we have categorical variables with dependent children (the latent variable topic identities); and we have categorical variables with shifting membership in multiple priors sharing a hyperprior. Note also that in the standard LDA model, the words are completely observed, and hence we never need to resample them. (However, Gibbs sampling would equally be possible if only some or none of the words were observed. In such a case, we would want to initialize the distribution over the words in some reasonable fashion — e.g. from the output of some process that generates sentences, such as a machine translation model — in order for the resulting posterior latent variable distributions to make any sense.)  Using the above formulas, we can write down the conditional probabilities directly:         Pr   (    w   d  n    =  v   ∣   𝕎   (   -   d  n    )    ,  ℤ  ,  𝜷  )      ∝      #   𝕎  v   k  ,   (   -   d  n    )      +   β  v         Pr   (    z   d  n    =  k   ∣   ℤ   (   -   d  n    )    ,    w   d  n    =  v   ,   𝕎   (   -   d  n    )    ,  𝜶  )      ∝      (    #   ℤ  k   d  ,   (   -   d  n    )      +   α  k    )    Pr   (    w   d  n    =  v   ∣   𝕎   (   -   d  n    )    ,  ℤ  ,  𝜷  )            Pr     subscript  w    d  n    v    superscript  𝕎      d  n     ℤ  𝜷   proportional-to      normal-#   superscript   subscript  𝕎  v    k      d  n        subscript  β  v       Pr     subscript  z    d  n    k    superscript  ℤ      d  n        subscript  w    d  n    v    superscript  𝕎      d  n     𝜶   proportional-to        normal-#   superscript   subscript  ℤ  k    d      d  n        subscript  α  k     Pr     subscript  w    d  n    v    superscript  𝕎      d  n     ℤ  𝜷       \begin{array}[]{lcl}\Pr(w_{dn}=v\mid\mathbb{W}^{(-dn)},\mathbb{Z},\boldsymbol{%
 \beta})&\propto&\#\mathbb{W}_{v}^{k,(-dn)}+\beta_{v}\\
 \Pr(z_{dn}=k\mid\mathbb{Z}^{(-dn)},w_{dn}=v,\mathbb{W}^{(-dn)},\boldsymbol{%
 \alpha})&\propto&(\#\mathbb{Z}_{k}^{d,(-dn)}+\alpha_{k})\Pr(w_{dn}=v\mid%
 \mathbb{W}^{(-dn)},\mathbb{Z},\boldsymbol{\beta})\\
 \end{array}     Here we have defined the counts more explicitly to clearly separate counts of words and counts of topics:         #   𝕎  v   k  ,   (   -   d  n    )        =     number of words having value  v  among topic  k  excluding   w   d  n          #   ℤ  k   d  ,   (   -   d  n    )        =     number of topics having value  k  among document  d  excluding   z   d  n             normal-#   superscript   subscript  𝕎  v    k      d  n          number of words having value  v  among topic  k  excluding   subscript  w    d  n         normal-#   superscript   subscript  ℤ  k    d      d  n          number of topics having value  k  among document  d  excluding   subscript  z    d  n        \begin{array}[]{lcl}\#\mathbb{W}_{v}^{k,(-dn)}&=&\text{number of words having %
 value }v\text{ among topic }k\text{ excluding }w_{dn}\\
 \#\mathbb{Z}_{k}^{d,(-dn)}&=&\text{number of topics having value }k\text{ %
 among document }d\text{ excluding }z_{dn}\\
 \end{array}     Note that, as in the scenario above with categorical variables with dependent children, the conditional probability of those dependent children appears in the definition of the parent's conditional probability. In this case, each latent variable has only a single dependent child word, so only one such term appears. (If there were multiple dependent children, all would have to appear in the parent's conditional probability, regardless of whether there was overlap between different parents and the same children, i.e. regardless of whether the dependent children of a given parent also have other parents. In a case where a child has multiple parents, the conditional probability for that child appears in the conditional probability definition of each of its parents.)  Note, critically, however, that the definition above specifies only the unnormalized conditional probability of the words, while the topic conditional probability requires the actual (i.e. normalized) probability. Hence we have to normalize by summing over all word symbols:         Pr   (    z   d  n    =  k   ∣   ℤ   (   -   d  n    )    ,    w   d  n    =  v   ,   𝕎   (   -   d  n    )    ,  𝜶  )      ∝      (    #   ℤ  k   d  ,   (   -   d  n    )      +   α  k    )      #   𝕎  v   k  ,   (   -   d  n    )      +   β  v      ∑    v  ′   =  1   V    (    #   𝕎   v  ′    k  ,   (   -   d  n    )      +   β   v  ′     )                =      (    #   ℤ  k   d  ,   (   -   d  n    )      +   α  k    )      #   𝕎  v   k  ,   (   -   d  n    )      +   β  v       #   𝕎  k    +  B   -  1            Pr     subscript  z    d  n    k    superscript  ℤ      d  n        subscript  w    d  n    v    superscript  𝕎      d  n     𝜶   proportional-to        normal-#   superscript   subscript  ℤ  k    d      d  n        subscript  α  k          normal-#   superscript   subscript  𝕎  v    k      d  n        subscript  β  v      superscript   subscript      superscript  v  normal-′   1    V       normal-#   superscript   subscript  𝕎   superscript  v  normal-′     k      d  n        subscript  β   superscript  v  normal-′           missing-subexpression    missing-subexpression    missing-subexpression      missing-subexpression          normal-#   superscript   subscript  ℤ  k    d      d  n        subscript  α  k          normal-#   superscript   subscript  𝕎  v    k      d  n        subscript  β  v          normal-#   superscript  𝕎  k    B   1        \begin{array}[]{rcl}\Pr(z_{dn}=k\mid\mathbb{Z}^{(-dn)},w_{dn}=v,\mathbb{W}^{(-%
 dn)},\boldsymbol{\alpha})&\propto&\bigl(\#\mathbb{Z}_{k}^{d,(-dn)}+\alpha_{k}%
 \bigr)\dfrac{\#\mathbb{W}_{v}^{k,(-dn)}+\beta_{v}}{\sum_{v^{\prime}=1}^{V}(\#%
 \mathbb{W}_{v^{\prime}}^{k,(-dn)}+\beta_{v^{\prime}})}\\
 &&\\
 &=&\bigl(\#\mathbb{Z}_{k}^{d,(-dn)}+\alpha_{k}\bigr)\dfrac{\#\mathbb{W}_{v}^{k%
 ,(-dn)}+\beta_{v}}{\#\mathbb{W}^{k}+B-1}\end{array}     where         #   𝕎  k      =     number of words generated by topic  k       B    =      ∑   v  =  1   V    β  v            normal-#   superscript  𝕎  k       number of words generated by topic  k     B     superscript   subscript     v  1    V    subscript  β  v       \begin{array}[]{lcl}\#\mathbb{W}^{k}&=&\text{number of words generated by %
 topic }k\\
 B&=&\sum_{v=1}^{V}\beta_{v}\\
 \end{array}     It's also worth making another point in detail, which concerns the second factor above in the conditional probability. Remember that the conditional distribution in general is derived from the joint distribution, and simplified by removing terms not dependent on the domain of the conditional (the part on the left side of the vertical bar). When a node   z   z   z   has dependent children, there will be one or more factors    F   (  …  ∣  z  )      normal-F  normal-…  z    \operatorname{F}(\dots\mid z)   in the joint distribution that are dependent on   z   z   z   . Usually there is one factor for each dependent node, and it has the same density function as the distribution appearing the mathematical definition. However, if a dependent node has another parent as well (a co-parent), and that co-parent is collapsed out, then the node will become dependent on all other nodes sharing that co-parent, and in place of multiple terms for each such node, the joint distribution will have only one joint term. We have exactly that situation here. Even though    z   d  n      subscript  z    d  n     z_{dn}   has only one child    w   d  n      subscript  w    d  n     w_{dn}   , that child has a Dirichlet co-parent that we have collapsed out, which induces a Dirichlet-multinomial over the entire set of nodes    𝕎  k     superscript  𝕎  k    \mathbb{W}^{k}   .  It happens in this case that this issue does not cause major problems, precisely because of the one-to-one relationship between    z   d  n      subscript  z    d  n     z_{dn}   and    w   d  n      subscript  w    d  n     w_{dn}   . We can rewrite the joint distribution as follows:         p   (   𝕎  k   ∣   z   d  n    )      =     p   (   w   d  n    ∣   𝕎   k  ,   (   -   d  n    )     ,   z   d  n    )   p   (   𝕎   k  ,   (   -   d  n    )     ∣   z   d  n    )         =     p   (   w   d  n    ∣   𝕎   k  ,   (   -   d  n    )     ,   z   d  n    )   p   (   𝕎   k  ,   (   -   d  n    )     )         ∼     p   (   w   d  n    ∣   𝕎   k  ,   (   -   d  n    )     ,   z   d  n    )           fragments  p   fragments  normal-(   superscript  𝕎  k   normal-∣   subscript  z    d  n    normal-)      fragments  p   fragments  normal-(   subscript  w    d  n    normal-∣   superscript  𝕎   k      d  n      normal-,   subscript  z    d  n    normal-)   p   fragments  normal-(   superscript  𝕎   k      d  n      normal-∣   subscript  z    d  n    normal-)       missing-subexpression     fragments  p   fragments  normal-(   subscript  w    d  n    normal-∣   superscript  𝕎   k      d  n      normal-,   subscript  z    d  n    normal-)   p   fragments  normal-(   superscript  𝕎   k      d  n      normal-)       missing-subexpression   similar-to   fragments  p   fragments  normal-(   subscript  w    d  n    normal-∣   superscript  𝕎   k      d  n      normal-,   subscript  z    d  n    normal-)       \begin{array}[]{lcl}p(\mathbb{W}^{k}\mid z_{dn})&=&p(w_{dn}\mid\mathbb{W}^{k,(%
 -dn)},z_{dn})\,p(\mathbb{W}^{k,(-dn)}\mid z_{dn})\\
 &=&p(w_{dn}\mid\mathbb{W}^{k,(-dn)},z_{dn})\,p(\mathbb{W}^{k,(-dn)})\\
 &\sim&p(w_{dn}\mid\mathbb{W}^{k,(-dn)},z_{dn})\end{array}     where we note that in the set    𝕎   k  ,   (   -   d  n    )       superscript  𝕎   k      d  n       \mathbb{W}^{k,(-dn)}   (i.e. the set of nodes    𝕎  k     superscript  𝕎  k    \mathbb{W}^{k}   excluding    w   d  n      subscript  w    d  n     w_{dn}   ), none of the nodes have    z   d  n      subscript  z    d  n     z_{dn}   as a parent. Hence it can be eliminated as a conditioning factor (line 2), meaning that the entire factor can be eliminated from the conditional distribution (line 3).  A second example: Naive Bayes document clustering  Here is another model, with a different set of issues. This is an implementation of an unsupervised Naive Bayes model for document clustering. That is, we would like to classify documents into multiple categories (e.g. " spam " or "non-spam", or "scientific journal article", "newspaper article about finance", "newspaper article about politics", "love letter") based on textual content. However, we don't already know the correct category of any documents; instead, we want to cluster them based on mutual similarities. (For example, a set of scientific articles will tend to be similar to each other in word use but very different from a set of love letters.) This is a type of unsupervised learning . (The same technique can be used for doing semi-supervised learning , i.e. where we know the correct category of some fraction of the documents and would like to use this knowledge to help in clustering the remaining documents.)  The model is as follows:        𝜶    ∼    A Dirichlet hyperprior, either a constant or a random variable      𝜷    ∼    A Dirichlet hyperprior, either a constant or a random variable       𝜽   d  =   1  …  M       ∼      Dirichlet  K    (  𝜶  )         ϕ   k  =   1  …  K       ∼      Dirichlet  V    (  𝜷  )         z   d  =   1  …  M       ∼      Categorical  K    (   𝜽  d   )         w    d  =   1  …  M    ,   n  =   1  …   N  d         ∼      Categorical  V    (   ϕ   z  d    )          𝜶  similar-to  A Dirichlet hyperprior, either a constant or a random variable    𝜷  similar-to  A Dirichlet hyperprior, either a constant or a random variable     subscript  𝜽    d    1  normal-…  M     similar-to    subscript  Dirichlet  K   𝜶      subscript  bold-italic-ϕ    k    1  normal-…  K     similar-to    subscript  Dirichlet  V   𝜷      subscript  z    d    1  normal-…  M     similar-to    subscript  Categorical  K    subscript  𝜽  d       subscript  w   formulae-sequence    d    1  normal-…  M      n    1  normal-…   subscript  N  d       similar-to    subscript  Categorical  V    subscript  bold-italic-ϕ   subscript  z  d        \begin{array}[]{lcl}\boldsymbol{\alpha}&\sim&\text{A Dirichlet hyperprior, %
 either a constant or a random variable}\\
 \boldsymbol{\beta}&\sim&\text{A Dirichlet hyperprior, either a constant or a %
 random variable}\\
 \boldsymbol{\theta}_{d=1\dots M}&\sim&\operatorname{Dirichlet}_{K}(\boldsymbol%
 {\alpha})\\
 \boldsymbol{\phi}_{k=1\dots K}&\sim&\operatorname{Dirichlet}_{V}(\boldsymbol{%
 \beta})\\
 z_{d=1\dots M}&\sim&\operatorname{Categorical}_{K}(\boldsymbol{\theta}_{d})\\
 w_{d=1\dots M,n=1\dots N_{d}}&\sim&\operatorname{Categorical}_{V}(\boldsymbol{%
 \phi}_{z_{d}})\\
 \end{array}     In many ways, this model is very similar to the LDA  topic model described above, but it assumes one topic per document rather than one topic per word, with a document consisting of a mixture of topics. This can be seen clearly in the above model, which is identical to the LDA model except that there is only one latent variable per document instead of one per word. Once again, we assume that we are collapsing all of the Dirichlet priors.  The conditional probability for a given word is almost identical to the LDA case. Once again, all words generated by the same Dirichlet prior are interdependent. In this case, this means the words of all documents having a given label — again, this can vary depending on the label assignments, but all we care about is the total counts. Hence:         Pr   (    w   d  n    =  v   ∣   𝕎   (   -   d  n    )    ,  ℤ  ,  𝜷  )      ∝      #   𝕎  v   k  ,   (   -   d  n    )      +   β  v           Pr     subscript  w    d  n    v    superscript  𝕎      d  n     ℤ  𝜷   proportional-to      normal-#   superscript   subscript  𝕎  v    k      d  n        subscript  β  v       \begin{array}[]{lcl}\Pr(w_{dn}=v\mid\mathbb{W}^{(-dn)},\mathbb{Z},\boldsymbol{%
 \beta})&\propto&\#\mathbb{W}_{v}^{k,(-dn)}+\beta_{v}\\
 \end{array}     where         #   𝕎  v   k  ,   (   -   d  n    )        =     number of words having value  v  among documents with label  k  excluding   w   d  n             normal-#   superscript   subscript  𝕎  v    k      d  n          number of words having value  v  among documents with label  k  excluding   subscript  w    d  n        \begin{array}[]{lcl}\#\mathbb{W}_{v}^{k,(-dn)}&=&\text{number of words having %
 value }v\text{ among documents with label }k\text{ excluding }w_{dn}\\
 \end{array}     However, there is a critical difference in the conditional distribution of the latent variables for the label assignments, which is that a given label variable has multiple children nodes instead of just one — in particular, the nodes for all the words in the label's document. This relates closely to the discussion above about the factor    F   (  …  ∣   z  d   )      normal-F  normal-…   subscript  z  d     \operatorname{F}(\dots\mid z_{d})   that stems from the joint distribution. In this case, the joint distribution needs to be taken over all words in all documents containing a label assignment equal to the value of    z  d     subscript  z  d    z_{d}   , and has the value of a Dirichlet-multinomial distribution. Furthermore, we cannot reduce this joint distribution down to a conditional distribution over a single word. Rather, we can reduce it down only to a smaller joint conditional distribution over the words in the document for the label in question, and hence we cannot simplify it using the trick above that yields a simple sum of expected count and prior. Although it is in fact possible to rewrite it as a product of such individual sums, the number of factors is very large, and is not clearly more efficient than directly computing the Dirichlet-multinomial distribution probability.  For a multinomial distribution over category counts  For a random vector of category counts    𝐱  =   (   n  1   ,  …  ,   n  K   )       𝐱    subscript  n  1   normal-…   subscript  n  K      \mathbf{x}=(n_{1},\dots,n_{K})   , distributed according to a multinomial distribution , the marginal distribution is obtained by integrating out p :       Pr   (  𝐱  ∣  𝜶  )    =    ∫  𝐩     Pr   (  𝐱  ∣  𝐩  )     Pr   (  𝐩  ∣  𝜶  )    d  𝐩         Pr  𝐱  𝜶     subscript   𝐩      Pr  𝐱  𝐩    Pr  𝐩  𝜶   d  𝐩      \Pr(\mathbf{x}\mid\boldsymbol{\alpha})=\int_{\mathbf{p}}\Pr(\mathbf{x}\mid%
 \mathbf{p})\Pr(\mathbf{p}\mid\boldsymbol{\alpha})\textrm{d}\mathbf{p}     which results in the following explicit formula:       Pr   (  𝐱  ∣  𝜶  )    =     N  !     ∏  k    (    n  k   !   )       Γ   (  A  )     Γ   (   N  +  A   )       ∏  k     Γ   (    n  k   +   α  k    )     Γ   (   α  k   )            Pr  𝐱  𝜶         N     subscript  product  k      subscript  n  k          normal-Γ  A     normal-Γ    N  A       subscript  product  k       normal-Γ     subscript  n  k    subscript  α  k       normal-Γ   subscript  α  k         \Pr(\mathbf{x}\mid\boldsymbol{\alpha})=\frac{N!}{\prod_{k}\left(n_{k}!\right)}%
 \frac{\Gamma\left(A\right)}{\Gamma\left(N+A\right)}\prod_{k}\frac{\Gamma(n_{k}%
 +\alpha_{k})}{\Gamma(\alpha_{k})}     where A is defined as the sum    A  =   ∑   α  k        A     subscript  α  k      A=\sum\alpha_{k}   . Note that this differs crucially from the above formula in having an extra term at the front that looks like the factor at the front of a multinomial distribution. Another form for this same compound distribution, written more compactly in terms of the beta function , B , is as follows:        Pr   (  𝐱  ∣  𝜶  )    =    N  B   (  A  ,  N  )       ∏   k  :    n  k   >  0        n  k   B   (   α  k   ,   n  k   )       .       Pr  𝐱  𝜶       N  B   A  N      subscript  product   normal-:  k     subscript  n  k   0        subscript  n  k   B    subscript  α  k    subscript  n  k         \Pr(\mathbf{x}\mid\boldsymbol{\alpha})=\frac{NB\left(A,N\right)}{\prod_{k:n_{k%
 }>0}n_{k}B\left(\alpha_{k},n_{k}\right)}.     Related distributions  The one-dimensional version of the multivariate Pólya distribution is known as the Beta-binomial distribution .  Uses  The multivariate Pólya distribution is used in automated document classification and clustering, genetics , economy , combat modeling, and quantitative marketing.  See also   Beta-binomial distribution  Chinese restaurant process  Dirichlet process  Generalized Dirichlet distribution   References   Elkan, C. (2006) Clustering documents with an exponential-family approximation of the Dirichlet compound multinomial distribution . ICML, 289-296  Johnson, N. L., Kotz, S. and Balakrishnan, N. (1997) Discrete multivariate distributions (Vol. 165). New York: Wiley.  Kvam, P. and Day, D. (2001) The multivariate Polya distribution in combat modeling. Naval Research Logistics, 48, 1-17  Madsen, RE., Kauchak, D. and Elkan, C. (2005) Modeling Word Burstiness Using the Dirichlet Distribution . ICML, 545-552  Minka, T. (2003) Estimating a Dirichlet distribution . Technical report Microsoft Research. Includes Matlab code for fitting distributions to data.  Mosimann, J. E. (1962) On the compound multinomial distribution, the multivariate β-distribution, and correlations among proportions . Biometrika, 49(1-2), 65-82.  Wagner, U. and Taudes, A. (1986) A Multivariate Polya Model of Brand Choice and Purchase Incidence. Marketing Science, 5(3), 219-244.   "  Category:Multivariate discrete distributions  Category:Probability distributions  Category:Discrete distributions   