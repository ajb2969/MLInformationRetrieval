   Explained variation      Explained variation   In statistics , explained variation measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set. Often, variation is quantified as variance ; then, the more specific term explained variance can be used.  The complementary part of the total variation is called unexplained or residual .  Definition in terms of information gain  Information gain by better modelling  Following Kent (1983), 1 we use the Fraser information (Fraser 1965) 2       F   (  θ  )    =   ∫   d   r   g   (  r  )    ln  f    (  r  ;  θ  )           F  θ       d  r  g  r    f    r  θ       F(\theta)=\int\textrm{d}r\,g(r)\,\ln f(r;\theta)   where    g   (  r  )       g  r    g(r)   is the probability density of a random variable    R    R   R\,   , and    f   (  r  ;  θ  )       f   r  θ     f(r;\theta)\,   with    θ  ∈   Θ  i       θ   subscript  normal-Θ  i     \theta\in\Theta_{i}   (    i  =   0  ,   1        i   0  1     i=0,1\,   ) are two families of parametric models. Model family 0 is the simpler one, with a restricted parameter space     Θ  0   ⊂   Θ  1        subscript  normal-Θ  0    subscript  normal-Θ  1     \Theta_{0}\subset\Theta_{1}   .  Parameters are determined by maximum likelihood estimation ,       θ  i   =    arg max   θ  ∈   Θ  i     F   (  θ  )         subscript  θ  i      subscript  arg max    θ   subscript  normal-Θ  i     F  θ     \theta_{i}=\mbox{arg max}_{\theta\in\Theta_{i}}F(\theta)   .  The information gain of model 1 over model 0 is written as      Γ   (   θ  1   :   θ  0   )   =  2   [  F   (   θ  1   )   -  F   (   θ  0   )   ]      fragments  Γ   fragments  normal-(   subscript  θ  1   normal-:   subscript  θ  0   normal-)    2   fragments  normal-[  F   fragments  normal-(   subscript  θ  1   normal-)    F   fragments  normal-(   subscript  θ  0   normal-)   normal-]     \Gamma(\theta_{1}:\theta_{0})=2[F(\theta_{1})-F(\theta_{0})]\,   where a factor of 2 is included for convenience. Γ is always nonnegative; it measures the extent to which the best model of family 1 is better than the best model of family 0 in explaining g(r) .  Information gain by a conditional model  Assume a two-dimensional random variable    R  =   (  X  ,  Y  )       R   X  Y     R=(X,Y)   where X shall be considered as an explanatory variable, and Y as a dependent variable. Models of family 1 "explain" Y in terms of X ,      f   (  y  |  x  ;  θ  )      fragments  f   fragments  normal-(  y  normal-|  x  normal-;  θ  normal-)     f(y|x;\theta)   , whereas in family 0, X and Y are assumed to be independent. We define the randomness of Y by     D   (  Y  )    =   exp   [   -   2  F   (   θ  0   )     ]          D  Y         2  F   subscript  θ  0        D(Y)=\exp[-2F(\theta_{0})]   , and the randomness of Y , given X , by    D   (  Y  |  X  )   =  exp   [  -  2  F   (   θ  1   )   ]      fragments  D   fragments  normal-(  Y  normal-|  X  normal-)      fragments  normal-[   2  F   fragments  normal-(   subscript  θ  1   normal-)   normal-]     D(Y|X)=\exp[-2F(\theta_{1})]   . Then,       ρ  C  2   =  1  -  D   (  Y  |  X  )   /  D   (  Y  )      fragments   superscript   subscript  ρ  C   2    1   D   fragments  normal-(  Y  normal-|  X  normal-)    D   fragments  normal-(  Y  normal-)     \rho_{C}^{2}=1-D(Y|X)/D(Y)   can be interpreted as proportion of the data dispersion which is "explained" by X .  Special cases and generalized usage  For special models, the above definition yields particularly appealing results. Regrettably, these simplified definitions of explained variance are used even in situations where the underlying assumptions do not hold.  Linear regression  The fraction of variance unexplained is an established concept in the context of linear regression . The usual definition of the coefficient of determination is based on the fundamental concept of explained variance.  Correlation coefficient as measure of explained variance  Let X be a random vector, and Y a random variable that is modeled by a normal distribution with centre    μ  +    Ψ  T   X       μ     superscript  normal-Ψ  T   X     \mu+\Psi^{\textrm{T}}X   . In this case, the above-derived proportion of randomness    ρ  C  2     superscript   subscript  ρ  C   2    \rho_{C}^{2}   equals the squared correlation coefficient     R  2     superscript  R  2    R^{2}   .  Note the strong model assumptions: the centre of the Y distribution must be a linear function of X , and for any given x , the Y distribution must be normal. In other situations, it is generally not justified to interpret    R  2     superscript  R  2    R^{2}   as proportion of explained variance.  Explained variance in principal component analysis  "Explained variance" is routinely used in principal component analysis . The relation to the Fraser-Kent information gain remains to be clarified.  Criticism  As the fraction of "explained variance" equals the correlation coefficient    R  2     superscript  R  2    R^{2}   , it shares all the disadvantages of the latter: it reflects not only the quality of the regression, but also the distribution of the independent (conditioning) variables.  In the words of one critic: "Thus    R  2     superscript  R  2    R^{2}   gives the 'percentage of variance explained' by the regression, an expression that, for most social scientists, is of doubtful meaning but great rhetorical value. If this number is large, the regression gives a good fit, and there is little point in searching for additional variables. Other regression equations on different data sets are said to be less satisfactory or less powerful if their    R  2     superscript  R  2    R^{2}   is lower. Nothing about    R  2     superscript  R  2    R^{2}   supports these claims". 3 And, after constructing an example where    R  2     superscript  R  2    R^{2}   is enhanced just by jointly considering data from two different populations: "'Explained variance' explains nothing." 4 5  See also   Variance reduction   References  External links   Variance, explained and unexplained  Explained variance  Explained and Unexplained Variance on a graph   "  Category:Data analysis  Category:Regression analysis  Category:Statistics articles needing expert attention     ↩  ↩  ↩   ↩     