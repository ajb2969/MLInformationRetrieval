   Semidefinite embedding      Semidefinite embedding   Semidefinite embedding (SDE) or maximum variance unfolding (MVU) is an algorithm in computer science that uses semidefinite programming to perform non-linear dimensionality reduction of high-dimensional vectorial input data. MVU can be viewed as a non-linear generalization of Principal component analysis .  Non-linear dimensionality reduction algorithms attempt to map high-dimensional data onto a low-dimensional Euclidean  vector space . Maximum variance Unfolding is a member of the manifold learning family, which also include algorithms such as isomap and locally linear embedding . In manifold learning, the input data is assumed to be sampled from a low dimensional manifold that is embedded inside of a higher-dimensional vector space. The main intuition behind MVU is to exploit the local linearity of manifolds and create a mapping that preserves local neighborhoods at every point of the underlying manifold.  MVU creates a mapping from the high dimensional input vectors to some low dimensional Euclidean vector space in the following steps:  A neighborhood graph is created. Each input is connected with its k-nearest input vectors (according to Euclidean distance metric) and all k-nearest neighbors are connected with each other. If the data is sampled well enough, the resulting graph is a discrete approximation of the underlying manifold.  The neighborhood graph is "unfolded" with the help of semidefinite programming. Instead of learning the output vectors directly, the semidefinite programming aims to find an inner product matrix that maximizes the pairwise distances between any two inputs that are not connected in the neighborhood graph while preserving the nearest neighbors distances.  The low-dimensional embedding is finally obtained by application of multidimensional scaling on the learned inner product matrix.  The steps of applying semidefinite programming followed by a linear dimensionality reduction step to recover a low-dimensional embedding into a Euclidean space were first proposed by Linial, London, and Rabinovich.  Optimization Formulation  Let   X   X   X\,\!   be the original input and   Y   Y   Y\,\!   be the embedding. If    i  ,  j     i  j    i,j\,\!   are two neighbors, then the local isometry constraint that needs to be satisfied is:        |    X  i   -   X  j    |   2   =    |    Y  i   -   Y  j    |   2        superscript       subscript  X  i    subscript  X  j     2    superscript       subscript  Y  i    subscript  Y  j     2     |X_{i}-X_{j}|^{2}=|Y_{i}-Y_{j}|^{2}\,\!     Let    G  ,  K     G  K    G,K\,\!   be the Gram matrices of   X   X   X\,\!   and   Y   Y   Y\,\!   (i.e.      G   i  j    =    X  i   ⋅   X  j     ,    K   i  j    =    Y  i   ⋅   Y  j        formulae-sequence     subscript  G    i  j     normal-⋅   subscript  X  i    subscript  X  j        subscript  K    i  j     normal-⋅   subscript  Y  i    subscript  Y  j       G_{ij}=X_{i}\cdot X_{j},K_{ij}=Y_{i}\cdot Y_{j}\,\!   ). We can express the above constraint for every neighbor points    i  ,  j     i  j    i,j\,\!   in term of    G  ,  K     G  K    G,K\,\!   :         G   i  i    +   G   j  j     -   G   i  j    -   G   j  i     =     K   i  i    +   K   j  j     -   K   i  j    -   K   j  i              subscript  G    i  i     subscript  G    j  j      subscript  G    i  j     subscript  G    j  i          subscript  K    i  i     subscript  K    j  j      subscript  K    i  j     subscript  K    j  i       G_{ii}+G_{jj}-G_{ij}-G_{ji}=K_{ii}+K_{jj}-K_{ij}-K_{ji}\,\!     In addition, we also want to constraint the embedding   Y   Y   Y\,\!   to center at the origin:         ∑  i    Y  i    =  0   ⇔     (    ∑  i    Y  i    )   2   =  0  ⇒    ∑   i  ,  j      Y  i    Y  j     =  0  ⇒    ∑   i  ,  j     K   i  j     =  0      normal-⇔      subscript   i    subscript  Y  i    0        superscript    subscript   i    subscript  Y  i    2   0    normal-⇒      subscript    i  j       subscript  Y  i    subscript  Y  j          0    normal-⇒      subscript    i  j     subscript  K    i  j          0      \sum_{i}Y_{i}=0\Leftrightarrow(\sum_{i}Y_{i})^{2}=0\Rightarrow\sum_{i,j}Y_{i}Y%
 _{j}=0\Rightarrow\sum_{i,j}K_{ij}=0     As described above, except the distances of neighbor points are preserved, the algorithm aims to maximize the pairwise distance of every pair of points. The objective function to be maximized is:       T   (  Y  )    =     1   2  N       ∑   i  ,  j      |    Y  i   -   Y  j    |   2           T  Y       1    2  N      subscript    i  j     superscript       subscript  Y  i    subscript  Y  j     2       T(Y)=\dfrac{1}{2N}\sum_{i,j}|Y_{i}-Y_{j}|^{2}     Intuitively, maximizing the function above is equivalent to pulling the points as far away from each other as possible and therefore "unfold" the manifold. The local isometry constraint prevents the objective function from going to infinity. Proof:  Let    τ  =   m  a  x   {    η   i  j      |    Y  i   -   Y  j    |   2    }        τ    m  a  x      subscript  η    i  j     superscript       subscript  Y  i    subscript  Y  j     2        \tau=max\{\eta_{ij}|Y_{i}-Y_{j}|^{2}\}\,\!   where     η   i  j    =  1       subscript  η    i  j    1    \eta_{ij}=1\,\!   if i and j are neighbors and     η   i  j    =  0       subscript  η    i  j    0    \eta_{ij}=0\,\!   otherwise.  Since the graph has N points, the distance between any two points      |    Y  i   -   Y  j    |   2   ≤   N  τ        superscript       subscript  Y  i    subscript  Y  j     2     N  τ     |Y_{i}-Y_{j}|^{2}\leq N\tau\,\!   . We can then bound the objective function as follow:       T   (  Y  )    =    1   2  N      ∑   i  ,  j      |    Y  i   -   Y  j    |   2     ≤    1   2  N      ∑   i  ,  j      (   N  τ   )   2     =     N  3    τ  2    2           T  Y       1    2  N      subscript    i  j     superscript       subscript  Y  i    subscript  Y  j     2              1    2  N      subscript    i  j     superscript    N  τ   2               superscript  N  3    superscript  τ  2    2      T(Y)=\dfrac{1}{2N}\sum_{i,j}|Y_{i}-Y_{j}|^{2}\leq\dfrac{1}{2N}\sum_{i,j}(N\tau%
 )^{2}=\dfrac{N^{3}\tau^{2}}{2}\,\!     The objective function can be rewritten purely in the form of the Gram matrix:         T   (  Y  )        =    1   2  N      ∑   i  ,  j      |    Y  i   -   Y  j    |   2            =   1   2  N     (   ∑   i  ,  j     (   Y  i  2   +   Y  j  2   -   Y  i   ⋅   Y  j   -   Y  j   ⋅   Y  i   )            =    1   2  N     (      ∑   i  ,  j     Y  i  2    +    ∑   i  ,  j     Y  j  2     -    ∑   i  ,  j      Y  i   ⋅   Y  j     -    ∑   i  ,  j      Y  j   ⋅   Y  i      )            =    1   2  N     (      ∑   i  ,  j     Y  i  2    +    ∑   i  ,  j     Y  j  2     -  0  -  0   )            =    1  N    (    ∑  i    Y  i  2    )    =    1  N    (   T  r   (  K  )    )             T  Y     absent      1    2  N      subscript    i  j     superscript       subscript  Y  i    subscript  Y  j     2         missing-subexpression    fragments     1    2  N     fragments  normal-(   subscript    i  j     fragments  normal-(   superscript   subscript  Y  i   2     superscript   subscript  Y  j   2     subscript  Y  i   normal-⋅   subscript  Y  j     subscript  Y  j   normal-⋅   subscript  Y  i   normal-)        missing-subexpression     absent      1    2  N          subscript    i  j     superscript   subscript  Y  i   2      subscript    i  j     superscript   subscript  Y  j   2       subscript    i  j     normal-⋅   subscript  Y  i    subscript  Y  j       subscript    i  j     normal-⋅   subscript  Y  j    subscript  Y  i           missing-subexpression     absent      1    2  N          subscript    i  j     superscript   subscript  Y  i   2      subscript    i  j     superscript   subscript  Y  j   2     0  0        missing-subexpression       absent      1  N     subscript   i    superscript   subscript  Y  i   2              1  N     T  r  K         \begin{aligned}\displaystyle T(Y)&\displaystyle{}=\dfrac{1}{2N}\sum_{i,j}|Y_{i%
 }-Y_{j}|^{2}\\
 &\displaystyle{}=\dfrac{1}{2N}(\sum_{i,j}(Y_{i}^{2}+Y_{j}^{2}-Y_{i}\cdot Y_{j}%
 -Y_{j}\cdot Y_{i})\\
 &\displaystyle{}=\dfrac{1}{2N}(\sum_{i,j}Y_{i}^{2}+\sum_{i,j}Y_{j}^{2}-\sum_{i%
 ,j}Y_{i}\cdot Y_{j}-\sum_{i,j}Y_{j}\cdot Y_{i})\\
 &\displaystyle{}=\dfrac{1}{2N}(\sum_{i,j}Y_{i}^{2}+\sum_{i,j}Y_{j}^{2}-0-0)\\
 &\displaystyle{}=\dfrac{1}{N}(\sum_{i}Y_{i}^{2})=\dfrac{1}{N}(Tr(K))\\
 \end{aligned}\,\!     Finally, the optimization can be formulated as:  Maximize     T  r   (  K  )       T  r  K    Tr(K)\,\!     Subject to     K  ⪰  0     succeeds-or-equals  K  0    K\succeq 0\,\!   and     ∀  i   ,  j      for-all  i   j    \forall i,j\,\!   where      η   i  j    =  1   ,      G   i  i    +   G   j  j     -   G   i  j    -   G   j  i     =     K   i  i    +   K   j  j     -   K   i  j    -   K   j  i         formulae-sequence     subscript  η    i  j    1          subscript  G    i  i     subscript  G    j  j      subscript  G    i  j     subscript  G    j  i          subscript  K    i  i     subscript  K    j  j      subscript  K    i  j     subscript  K    j  i        \eta_{ij}=1,G_{ii}+G_{jj}-G_{ij}-G_{ji}=K_{ii}+K_{jj}-K_{ij}-K_{ji}\,\!     After the Gram matrix   K   K   K\,\!   is learned by semidefinite programming, the output   Y   Y   Y\,\!   can be obtained via Cholesky decomposition . In particular, the Gram matrix can be written as     K   i  j    =    ∑   α  =  1   N    (    λ  α    V   α  i     V   α  j     )         subscript  K    i  j      superscript   subscript     α  1    N      subscript  λ  α    subscript  V    α  i     subscript  V    α  j        K_{ij}=\sum_{\alpha=1}^{N}(\lambda_{\alpha}V_{\alpha i}V_{\alpha j})\,\!   where    V   α  i      subscript  V    α  i     V_{\alpha i}\,\!   is the i-th element of eigenvector    V  α     subscript  V  α    V_{\alpha}\,\!   of the eigenvalue    λ  α     subscript  λ  α    \lambda_{\alpha}\,\!   .  It follows that the   α   α   \alpha\,\!   -th element of the output    Y  i     subscript  Y  i    Y_{i}\,\!   is      λ  α     V   α  i           subscript  λ  α     subscript  V    α  i      \sqrt{\lambda_{\alpha}}V_{\alpha i}\,\!   .  Comparison to other methods  Semidefinite embedding is much better in revealing the underlying dimension of the data compared to LLE and Laplacian eigenmaps. It also guarantees that the nearest neighbors in the embedding is the same as the original nearest neighbor for each point while the other two methods do not. On the other hand, semidefinite embedding is much slower and harder to scale to large data.  Semidefinite embedding outperforms Isomap when the manifold is not a convex subset of the Euclidean space.  See also   Locally linear embedding   References   [ http://repository.upenn.edu/cgi/viewcontent.cgi?article=1000&context; ;=cis_papers Unsupervised learning of image manifolds by semidefinite programming] K. Q. Weinberger and L. K. Saul (2004). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-04), Washington D.C.  Unsupervised learning of image manifolds by semidefinite programming K. Q. Weinberger and L. K. Saul (2005), International Journal of Computer Vision - In Special Issue: Computer Vision and Pattern Recognition-CVPR 2005 Guest Editor(s): Aaron Bobick , Rama Chellappa , Larry Davis , pages 77–90, Volume 70, Number 1, Springer Netherlands  The geometry of graphs and some of its algorithmic applications , Nathan Linial , Eran London , Yuri Rabinovich , IEEE Symposium on Foundations of Computer Science.   External links   MVU Matlab code online   "  Category:Computational statistics  Category:Dimension reduction   