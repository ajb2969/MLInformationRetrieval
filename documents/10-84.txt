   Heteroscedasticity-consistent standard errors      Heteroscedasticity-consistent standard errors   The topic of heteroscedasticity-consistent (HC) standard errors arises in statistics and econometrics in the context of linear regression as well as time series analysis . The alternative names of Huberâ€“White standard errors , Eickerâ€“White or Eickerâ€“Huberâ€“White 1 are also frequently used in relation to the same ideas.  In regression and time-series modelling, basic forms of models make use of the assumption that the errors or disturbances u i have the same variance across all observation points. When this is not the case, the errors are said to be heteroscedastic, or to have heteroscedasticity , and this behaviour will be reflected in the residuals     u  i   ^     normal-^   subscript  u  i     \scriptstyle\widehat{u_{i}}   estimated from a fitted model. Heteroscedasticity-consistent standard errors are used to allow the fitting of a model that does contain heteroscedastic residuals. The first such approach was proposed by Huber (1967), and further improved procedures have been produced since for cross-sectional data, time-series data and GARCH estimation .  Definition  Assume that we are studying the linear regression model       Y  =     X  â€²   Î²   +  U    ,      Y       superscript  X  normal-â€²   Î²   U     Y=X^{\prime}\beta+U,\,     where X is the vector of explanatory variables and Î² is a k Ã— 1 column vector of parameters to be estimated.  The ordinary least squares (OLS) estimator is         Î²  ^    O  L  S    =     (    ğ•  â€²   ğ•   )    -  1     ğ•  â€²   ğ•    .       subscript   normal-^  Î²     O  L  S       superscript     superscript  ğ•  normal-â€²   ğ•     1     superscript  ğ•  normal-â€²   ğ•     \widehat{\beta}_{OLS}=(\mathbb{X}^{\prime}\mathbb{X})^{-1}\mathbb{X}^{\prime}%
 \mathbb{Y}.\,     where   ğ•   ğ•   \mathbb{X}   denotes the matrix of stacked    X  i  â€²     superscript   subscript  X  i   normal-â€²    X_{i}^{\prime}   values observed in the data.  If the sample errors have equal variance Ïƒ 2 and are uncorrelated , then the least-squares estimate of Î² is BLUE (best linear unbiased estimator), and its variance is easily estimated with         v   O  L  S     [    Î²  ^    O  L  S    ]    =    s  2     (    ğ•  â€²   ğ•   )    -  1      ,    s  2   =     âˆ‘  i     u  ^   i  2     n  -  k        formulae-sequence       subscript  v    O  L  S     delimited-[]   subscript   normal-^  Î²     O  L  S         superscript  s  2    superscript     superscript  ğ•  normal-â€²   ğ•     1         superscript  s  2       subscript   i    superscript   subscript   normal-^  u   i   2      n  k       v_{OLS}[\hat{\beta}_{OLS}]=s^{2}(\mathbb{X}^{\prime}\mathbb{X})^{-1},s^{2}=%
 \frac{\sum_{i}\hat{u}_{i}^{2}}{n-k}     where     u  ^   i     subscript   normal-^  u   i    \hat{u}_{i}   are regression residuals.  When the assumptions of     E   [   u   u  â€²    ]    =    Ïƒ  2    I  n          E   delimited-[]    u   superscript  u  normal-â€²         superscript  Ïƒ  2    subscript  I  n      E[uu^{\prime}]=\sigma^{2}I_{n}   are violated, the OLS estimator loses its desirable properties. Indeed,       V   [    Î²  ^    O  L  S    ]    =   V   [     (    ğ•  â€²   ğ•   )    -  1     ğ•  â€²   ğ•   ]    =     (    ğ•  â€²   ğ•   )    -  1     ğ•  â€²   Î£  ğ•    (    ğ•  â€²   ğ•   )    -  1             V   delimited-[]   subscript   normal-^  Î²     O  L  S        V   delimited-[]     superscript     superscript  ğ•  normal-â€²   ğ•     1     superscript  ğ•  normal-â€²   ğ•             superscript     superscript  ğ•  normal-â€²   ğ•     1     superscript  ğ•  normal-â€²   normal-Î£  ğ•   superscript     superscript  ğ•  normal-â€²   ğ•     1        V[\hat{\beta}_{OLS}]=V[(\mathbb{X}^{\prime}\mathbb{X})^{-1}\mathbb{X}^{\prime}%
 \mathbb{Y}]=(\mathbb{X}^{\prime}\mathbb{X})^{-1}\mathbb{X}^{\prime}\Sigma%
 \mathbb{X}(\mathbb{X}^{\prime}\mathbb{X})^{-1}     where    Î£  =   V   [  u  ]        normal-Î£    V   delimited-[]  u      \Sigma=V[u]   .  While the OLS point estimator remains unbiased, it is not "best" in the sense of having minimum mean square error, and the OLS variance estimator     v   O  L  S     [    Î²  ^    O  L  S    ]        subscript  v    O  L  S     delimited-[]   subscript   normal-^  Î²     O  L  S       v_{OLS}[\hat{\beta}_{OLS}]   does not provide a consistent estimate of the variance of the OLS estimates.  For any non-linear model (for instance Logit and Probit models), however, heteroscedasticity has more severe consequences: the maximum likelihood estimates of the parameters will be biased (in an unknown direction), as well as inconsistent (unless the likelihood function is modified to correctly take into account the precise form of heteroskedasticity). 2 As pointed out by Greene , â€œsimply computing a robust covariance matrix for an otherwise inconsistent estimator does not give it redemption.â€ 3  White's heteroscedasticity-consistent estimator  If the regression errors    u  i     subscript  u  i    u_{i}   are independent, but have distinct variances Ïƒ i 2 , then    Î£  =   diag   (   Ïƒ  1  2   ,  â€¦  ,   Ïƒ  n  2   )        normal-Î£   diag   superscript   subscript  Ïƒ  1   2   normal-â€¦   superscript   subscript  Ïƒ  n   2      \Sigma=\operatorname{diag}(\sigma_{1}^{2},\ldots,\sigma_{n}^{2})   which can be estimated with      Ïƒ  ^   i  2   =    u  ^   i  2        superscript   subscript   normal-^  Ïƒ   i   2    superscript   subscript   normal-^  u   i   2     \hat{\sigma}_{i}^{2}=\hat{u}_{i}^{2}   . This provides White's (1980) estimator, often referred to as HCE (heteroscedasticity-consistent estimator):       v   H  C  E     [    Î²  ^    O  L  S    ]        subscript  v    H  C  E     delimited-[]   subscript   normal-^  Î²     O  L  S       \displaystyle v_{HCE}[\hat{\beta}_{OLS}]     where as above   ğ•   ğ•   \mathbb{X}   denotes the matrix of stacked    X  i  â€²     superscript   subscript  X  i   normal-â€²    X_{i}^{\prime}   values from the data. The estimator can be derived in terms of the generalized method of moments (GMM).  Note that also often discussed in the literature (including in White's paper itself) is the covariance matrix     Î©  ^   n     subscript   normal-^  normal-Î©   n    \hat{\Omega}_{n}   of the    n      n    \sqrt{n}   -consistent limiting distribution:         n    (     Î²  ^   n   -  Î²   )     â†’  ğ‘‘    N   (  0  ,  Î©  )     ,      d  normal-â†’       n      subscript   normal-^  Î²   n   Î²      N   0  normal-Î©      \sqrt{n}(\hat{\beta}_{n}-\beta)\xrightarrow{d}N(0,\Omega),   where,       Î©  =   E    [   X   X  â€²    ]    -  1    V  a  r   [   X  u   ]   E    [   X   X  â€²    ]    -  1      ,      normal-Î©    E   superscript   delimited-[]    X   superscript  X  normal-â€²       1    V  a  r   delimited-[]    X  u    E   superscript   delimited-[]    X   superscript  X  normal-â€²       1       \Omega=E[XX^{\prime}]^{-1}Var[Xu]E[XX^{\prime}]^{-1},   and       Î©  ^   n     subscript   normal-^  normal-Î©   n    \displaystyle\hat{\Omega}_{n}   Thus,        Î©  ^   n   =    n  â‹…   v   H  C  E      [    Î²  ^    O  L  S    ]         subscript   normal-^  normal-Î©   n      normal-â‹…  n   subscript  v    H  C  E      delimited-[]   subscript   normal-^  Î²     O  L  S        \hat{\Omega}_{n}=n\cdot v_{HCE}[\hat{\beta}_{OLS}]   and         V  a  r   ^    [   X  u   ]    =    1  n     âˆ‘  i     X  i    X  i  â€²     u  ^   i  2      =    1  n    ğ•  â€²    diag   (    u  ^   1  2   ,  â€¦  ,    u  ^   n  2   )    ğ•            normal-^    V  a  r     delimited-[]    X  u         1  n     subscript   i      subscript  X  i    superscript   subscript  X  i   normal-â€²    superscript   subscript   normal-^  u   i   2               1  n    superscript  ğ•  normal-â€²    diag   superscript   subscript   normal-^  u   1   2   normal-â€¦   superscript   subscript   normal-^  u   n   2    ğ•      \widehat{Var}[Xu]=\frac{1}{n}\sum_{i}X_{i}X_{i}^{\prime}\hat{u}_{i}^{2}=\frac{%
 1}{n}\mathbb{X}^{\prime}\operatorname{diag}(\hat{u}_{1}^{2},\ldots,\hat{u}_{n}%
 ^{2})\mathbb{X}   . Precisely which covariance matrix is of concern should be a matter of context.  Alternative estimators have been proposed in MacKinnon & White (1985) that correct for unequal variances of regression residuals due to different leverage . Unlike the asymptotic White's estimator, their estimators are unbiased when the data are homoscedastic.  See also   Generalized least squares  Generalized estimating equations  White test â€” a test for whether heteroscedasticity is present.   Software   Stata : robust option applicable in many pseudo-likelihood based procedures. See online help for _robust option and regress command.  RATS : robusterrors option is available in many of the regression and optimization commands ( linreg , nlls , etc.).  EViews : EViews version 8 offers three different methods for robust least squares: M-estimation (Huber, 1973), S-estimation (Rousseeuw and Yohai, 1984), and MM-estimation (Yohai 1987). 1   References                      "  Category:Regression analysis  Category:Econometrics  Category:Simultaneous equation methods (econometrics)  Category:Mathematical and quantitative methods (economics)     Kleiber, C., Zeileis, A (2006) Applied Econometrics with R , UseR-2006 conference â†©  â†©  â†©     