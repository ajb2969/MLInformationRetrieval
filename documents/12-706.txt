   Kolmogorov structure function      Kolmogorov structure function   In 1973 Kolmogorov proposed a non-probabilistic approach to statistics and model selection. Let each data be a finite binary string and models be finite sets of binary strings. Consider model classes consisting of models of given maximal Kolmogorov complexity . The Kolmogorov structure function of an individual data string expresses the relation between the complexity level constraint on a model class and the least log-cardinality of a model in the class containing the data. The structure function determines all stochastic properties of the individual data string: for every constrained model class it determines the individual best-fitting model in the class irrespective of whether the true model is in the model class considered or not. In the classical case we talk about a set of data with a probability distribution, and the properties are those of the expectations. In contrast, here we deal with individual data strings and the properties of the individual string focussed on. In this setting, a property holds with certainty rather than with high probability as in the classical case. The Kolmogorov structure function precisely quantify the goodness-of-fit of an individual model with respect to individual data.  The Kolmogorov structure function is used in the algorithmic information theory , also known as the theory of Kolmogorov complexity , for describing the structure of a string by use of models of increasing complexity.  Kolmogorov's definition  The structure function was originally proposed by Kolmogorov in 1973 at a Soviet Information Theory symposium in Tallinn, but these results were not published 1 p. 182. But the results were announced in 2 in 1974, the only written record by Kolmogorov himself. One of his last scientific statements is (translated from the original Russian by L.A. Levin):   Contemporary definition  It is discussed in Cover and Thomas. 3 It is extensively studied in Vereshchagin and Vitanyi 4 where also the main properties are resolved. The Kolmogorov structure function can be written as        h  x    (  α  )    =    min  S    {    log   |  S  |    :    x  ∈  S   ,    K   (  S  )    ≤  α     }           subscript  h  x   α     subscript   S    normal-:      S     formulae-sequence    x  S       K  S   α        h_{x}(\alpha)=\min_{S}\{\log|S|:x\in S,K(S)\leq\alpha\}   where   x   x   x   is a binary string of length   n   n   n   with    x  ∈  S      x  S    x\in S   where   S   S   S   is a contemplated model (set of n-length strings) for   x   x   x   ,    K   (  S  )       K  S    K(S)   is the Kolmogorov complexity of   S   S   S   and   α   α   \alpha   is a nonnegative integer value bounding the complexity of the contemplated   S   S   S   's. Clearly, this function is nonincreasing and reaches     log   |   {  x  }   |    =  0           x     0    \log|\{x\}|=0   for    α  =    K   (  x  )    +  c       α      K  x   c     \alpha=K(x)+c   where   c   c   c   is the required number of bits to change   x   x   x   into    {  x  }     x    \{x\}   and    K   (  x  )       K  x    K(x)   is the Kolmogorov complexity of   x   x   x   .  The algorithmic sufficient statistic  We define a set   S   S   S   containing   x   x   x   such that      K   (  S  )   +  K   (  x  |  S  )   =  K   (  x  )   +  O   (  1  )      fragments  K   fragments  normal-(  S  normal-)    K   fragments  normal-(  x  normal-|  S  normal-)    K   fragments  normal-(  x  normal-)    O   fragments  normal-(  1  normal-)     K(S)+K(x|S)=K(x)+O(1)   . The function     h  x    (  α  )        subscript  h  x   α    h_{x}(\alpha)   never decreases more than a fixed independent constant below the diagonal called sufficiency line L defined by        L   (  α  )    +  α   =   K   (  x  )            L  α   α     K  x     L(\alpha)+\alpha=K(x)   . It is approached to within a constant distance by the graph of    h  x     subscript  h  x    h_{x}   for certain arguments (for instance, for    α  =    K   (  x  )    +  c       α      K  x   c     \alpha=K(x)+c   ). For these   α   α   \alpha   's we have     α  +    h  x    (  α  )     =    K   (  x  )    +   O   (  1  )           α     subscript  h  x   α        K  x     O  1      \alpha+h_{x}(\alpha)=K(x)+O(1)   and the associated model   S   S   S   (witness for     h  x    (  α  )        subscript  h  x   α    h_{x}(\alpha)   ) is called an optimal set for   x   x   x   , and its description of     K   (  S  )    ≤  α        K  S   α    K(S)\leq\alpha   bits is therefore an algorithmic sufficient statistic . We write `algorithmic' for `Kolmogorov complexity' by convention. The main properties of an algorithmic sufficient statistic are the following: If   S   S   S   is an algorithmic sufficient statistic for   x   x   x   , then        K   (  S  )    +   log   |  S  |     =    K   (  x  )    +   O   (  1  )             K  S       S         K  x     O  1      K(S)+\log|S|=K(x)+O(1)   . That is, the two-part description of   x   x   x   using the model   S   S   S   and as data-to-model code the index of   x   x   x   in the enumeration of   S   S   S   in    log   |  S  |         S     \log|S|   bits, is as concise as the shortest one-part code of   x   x   x   in    K   (  x  )       K  x    K(x)   bits. This can be easily seen as follows:      K   (  x  )   ≤  K   (  x  ,  S  )   +  O   (  1  )   ≤  K   (  S  )   +  K   (  x  |  S  )   +  O   (  1  )   ≤  K   (  S  )   +  log  |  S  |  +  O   (  1  )   ≤  K   (  x  )   +  O   (  1  )      fragments  K   fragments  normal-(  x  normal-)    K   fragments  normal-(  x  normal-,  S  normal-)    O   fragments  normal-(  1  normal-)    K   fragments  normal-(  S  normal-)    K   fragments  normal-(  x  normal-|  S  normal-)    O   fragments  normal-(  1  normal-)    K   fragments  normal-(  S  normal-)     normal-|  S  normal-|   O   fragments  normal-(  1  normal-)    K   fragments  normal-(  x  normal-)    O   fragments  normal-(  1  normal-)     K(x)\leq K(x,S)+O(1)\leq K(S)+K(x|S)+O(1)\leq K(S)+\log|S|+O(1)\leq K(x)+O(1)   ,  using straightforward inequalities and the sufficiency property, we find that    K   (  x  |  S  )   =  log  |  S  |  +  O   (  1  )      fragments  K   fragments  normal-(  x  normal-|  S  normal-)     normal-|  S  normal-|   O   fragments  normal-(  1  normal-)     K(x|S)=\log|S|+O(1)   . (For example, given    S  ∋  x      x  S    S\ni x   , we can describe   x   x   x   self-delimitingly (you can determine its end) in     log   |  S  |    +   O   (  1  )            S      O  1     \log|S|+O(1)   bits.) Therefore, the randomness deficiency     log  |  S  |  -  K   (  x  |  S  )      fragments   normal-|  S  normal-|   K   fragments  normal-(  x  normal-|  S  normal-)     \log|S|-K(x|S)   of   x   x   x   in   S   S   S   is a constant, which means that   x   x   x   is a typical (random) element of S. However, there can be models   S   S   S   containing   x   x   x   that are not sufficient statistics. An algorithmic sufficient statistic   S   S   S   for   x   x   x   has the additional property, apart from being a model of best fit, that     K   (  x  ,  S  )    =    K   (  x  )    +   O   (  1  )           K   x  S        K  x     O  1      K(x,S)=K(x)+O(1)   and therefore by the Kolmogorov complexity symmetry of information (the information about   x   x   x   in   S   S   S   is about the same as the information about   S   S   S   in x) we have    K   (  S  |   x  *   )   =  O   (  1  )      fragments  K   fragments  normal-(  S  normal-|   superscript  x    normal-)    O   fragments  normal-(  1  normal-)     K(S|x^{*})=O(1)   : the algorithmic sufficient statistic   S   S   S   is a model of best fit that is almost completely determined by   x   x   x   . (    x  *     superscript  x     x^{*}   is a shortest program for   x   x   x   .) The algorithmic sufficient statistic associated with the least such   α   α   \alpha   is called the algorithmic minimal sufficient statistic .  With respect to the picture: The MDL structure function     λ  x    (  α  )        subscript  λ  x   α    \lambda_{x}(\alpha)   is explained below. The Goodness-of-fit structure function     β  x    (  α  )        subscript  β  x   α    \beta_{x}(\alpha)   is the least randomness deficiency (see above) of any model    S  ∋  x      x  S    S\ni x   for   x   x   x   such that     K   (  S  )    ≤  α        K  S   α    K(S)\leq\alpha   . This structure function gives the goodness-of-fit of a model   S   S   S   (containing x) for the string x. When it is low the model fits well, and when it is high the model doesn't fit well. If      β  x    (  α  )    =  0         subscript  β  x   α   0    \beta_{x}(\alpha)=0   for some   α   α   \alpha   then there is a typical model    S  ∋  x      x  S    S\ni x   for   x   x   x   such that     K   (  S  )    ≤  α        K  S   α    K(S)\leq\alpha   and   x   x   x   is typical (random) for S. That is,   S   S   S   is the best-fitting model for x. For more details see 5 and especially 6 and. 7  Selection of properties  Within the constraints that the graph goes down at an angle of at least 45 degrees, that it starts at n and ends approximately at    K   (  x  )       K  x    K(x)   , every graph (up to a    O   (   log  n   )       O    n     O(\log n)   additive term in argument and value) is realized by the structure function of some data x and vice versa. Where the graph hits the diagonal first the argument (complexity) is that of the minimum sufficient statistic. It is incomputable to determine this place. See. 8  Main property  It is proved that at each level   α   α   \alpha   of complexity the structure function allows us to select the best model   S   S   S   for the individual string x within a strip of    O   (   log  n   )       O    n     O(\log n)   with certainty, not with great probability. 9  The MDL variant  The Minimum description length (MDL) function: The length of the minimal two-part code for x consisting of the model cost K(S) and the length of the index of x in S, in the model class of sets of given maximal Kolmogorov complexity   α   α   \alpha   , the complexity of S upper bounded by   α   α   \alpha   , is given by the MDL function or constrained MDL estimator:         λ  x    (  α  )    =    min  S    {    Λ   (  S  )    :    S  ∋  x   ,    K   (  S  )    ≤  α     }     ,         subscript  λ  x   α     subscript   S    normal-:    normal-Λ  S    formulae-sequence    x  S       K  S   α        \lambda_{x}(\alpha)=\min_{S}\{\Lambda(S):S\ni x,\;K(S)\leq\alpha\},   where     Λ   (  S  )    =    log   |  S  |    +   K   (  S  )     ≥    K   (  x  )    -   O   (  1  )             normal-Λ  S         S      K  S             K  x     O  1       \Lambda(S)=\log|S|+K(S)\geq K(x)-O(1)   is the total length of two-part code of x with help of model S.  Main property  It is proved that at each level   α   α   \alpha   of complexity the structure function allows us to select the best model S for the individual string x within a strip of    O   (   log  n   )       O    n     O(\log n)   with certainty, not with great probability. 10  Application in statistics  The mathematics developed above were taken as the foundation of MDL by its inventor Jorma Rissanen . 11  Probability models and the Kolmogorov structure function  For every computable probability distribution   P   P   P   it can be proved 12 that       -    log  P    (  x  )     =    log   |  S  |    +   O   (   log  n   )               P   x          S      O    n       -\log P(x)=\log|S|+O(\log n)   . For example, if   P   P   P   is the uniform distribution on the set   S   S   S   of strings of length   n   n   n   , then each    x  ∈  S      x  S    x\in S   has probability     P   (  x  )    =   1  /   |  S  |          P  x     1    S      P(x)=1/|S|   . In the general case of computable probability mass functions we incur a logarithmic additive error term. Kolmogorov's structure function becomes        h  x  ′    (  α  )    =    min  P    {    -    log  P    (  x  )     :     P   (  x  )    >  0   ,    K   (  P  )    ≤  α     }           subscript   superscript  h  normal-′   x   α     subscript   P    normal-:        P   x     formulae-sequence      P  x   0       K  P   α        h^{\prime}_{x}(\alpha)=\min_{P}\{-\log P(x):P(x)>0,K(P)\leq\alpha\}   where x is a binary string of length n with     -    log  P    (  x  )     >  0            P   x    0    -\log P(x)>0   where   P   P   P   is a contemplated model (computable probability of   n   n   n   -length strings) for   x   x   x   ,    K   (  P  )       K  P    K(P)   is the Kolmogorov complexity of   P   P   P   and   α   α   \alpha   is an integer value bounding the complexity of the contemplated   P   P   P   's. Clearly, this function is non-increasing and reaches     log   |   {  x  }   |    =  0           x     0    \log|\{x\}|=0   for    α  =    K   (  x  )    +  c       α      K  x   c     \alpha=K(x)+c   where c is the required number of bits to change   x   x   x   into    {  x  }     x    \{x\}   and    K   (  x  )       K  x    K(x)   is the Kolmogorov complexity of   x   x   x   . Then      h  x  ′    (  α  )    =     h  x    (  α  )    +   O   (   log  n   )            subscript   superscript  h  normal-′   x   α        subscript  h  x   α     O    n       h^{\prime}_{x}(\alpha)=h_{x}(\alpha)+O(\log n)   . For every complexity level   α   α   \alpha   the function     h  x  ′    (  α  )        subscript   superscript  h  normal-′   x   α    h^{\prime}_{x}(\alpha)   is the Kolmogorov complexity version of the maximum likelihood (ML).  Main property  It is proved that at each level   α   α   \alpha   of complexity the structure function allows us to select the best model   S   S   S   for the individual string   x   x   x   within a strip of    O   (   log  n   )       O    n     O(\log n)   with certainty, not with great probability. 13  The MDL variant and probability models  The MDL function: The length of the minimal two-part code for x consisting of the model cost K(P) and the length of    -    log  P    (  x  )            P   x     -\log P(x)   , in the model class of computable probability mass functions of given maximal Kolmogorov complexity   α   α   \alpha   , the complexity of P upper bounded by   α   α   \alpha   , is given by the MDL function or constrained MDL estimator:         λ  x  ′    (  α  )    =    min  P    {    Λ   (  P  )    :     P   (  x  )    >  0   ,    K   (  P  )    ≤  α     }     ,         subscript   superscript  λ  normal-′   x   α     subscript   P    normal-:    normal-Λ  P    formulae-sequence      P  x   0       K  P   α        \lambda^{\prime}_{x}(\alpha)=\min_{P}\{\Lambda(P):P(x)>0,\;K(P)\leq\alpha\},   where     Λ   (  P  )    =    -    log  P    (  x  )     +   K   (  P  )     ≥    K   (  x  )    -   O   (  1  )             normal-Λ  P           P   x      K  P             K  x     O  1       \Lambda(P)=-\log P(x)+K(P)\geq K(x)-O(1)   is the total length of two-part code of x with help of model P.  Main property  It is proved that at each level   α   α   \alpha   of complexity the MDL function allows us to select the best model P for the individual string x within a strip of    O   (   log  n   )       O    n     O(\log n)   with certainty, not with great probability. 14  Extension to rate distortion and denoising  It turns out that the approach can be extended to a theory of rate distortion of individual finite sequences and denoising of individual finite sequences 15 using Kolmogorov complexity. Experiments using real compressor programs have been carried out with success. 16 Here the assumption is that for natural data the Kolmogorov complexity is not far from the length of a compressed version using a good compressor.  References  Other literature      , Especially pp. 401–431 about the Kolmogorov structure function, and pp. 613–629 about rate distortion and denoising of individual sequences.      "  Category:Algorithmic information theory      [ http://www.mathnet.ru/php/archive.phtml?jrnid=rm&wshow; ;=issue&year;=1974&volume;=29&volume;_alt=&issue;=4&issue;_alt=178&option;_lang=eng Abstract of a talk for the Moscow Mathematical Society in Uspekhi Mat. Nauk Volume 29, Issue 4(178) in the Communications of the Moscow Mathematical Society page 155 (in the Russian edition, not translated into English)] ↩  ↩  ↩    ↩     ↩  [ http://scholar.google.com/scholar?hl=en&as;_sdt=0,5&q; ;=Shen+1983+Soviet+Math+Doklady A.Kh. Shen, The concept of (α, β)-stochasticity in the Kolmogorov sense, and its properties, Soviet Math. Dokl., 28:1(1983), 295--299] ↩    ↩  ↩     