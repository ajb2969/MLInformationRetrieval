   Linear least squares (mathematics)      Linear least squares (mathematics)  table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
   margin: 0; padding: 0; vertical-align: baseline; border: none; }
 <style>
 table.sourceCode { width: 100%; line-height: 100%; }
 td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
 td.sourceCode { padding-left: 5px; }
 code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
 code > span.dt { color: #902000; } /* DataType */
 code > span.dv { color: #40a070; } /* DecVal */
 code > span.bn { color: #40a070; } /* BaseN */
 code > span.fl { color: #40a070; } /* Float */
 code > span.ch { color: #4070a0; } /* Char */
 code > span.st { color: #4070a0; } /* String */
 code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
 code > span.ot { color: #007020; } /* Other */
 code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
 code > span.fu { color: #06287e; } /* Function */
 code > span.er { color: #ff0000; font-weight: bold; } /* Error */
 code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 code > span.cn { color: #880000; } /* Constant */
 code > span.sc { color: #4070a0; } /* SpecialChar */
 code > span.vs { color: #4070a0; } /* VerbatimString */
 code > span.ss { color: #bb6688; } /* SpecialString */
 code > span.im { } /* Import */
 code > span.va { color: #19177c; } /* Variable */
 code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
 code > span.op { color: #666666; } /* Operator */
 code > span.bu { } /* BuiltIn */
 code > span.ex { } /* Extension */
 code > span.pp { color: #bc7a00; } /* Preprocessor */
 code > span.at { color: #7d9029; } /* Attribute */
 code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
 code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
 code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
 code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */     In statistics and mathematics , linear least squares is an approach fitting a mathematical or statistical model to data in cases where the idealized value provided by the model for any data point is expressed linearly in terms of the unknown parameters of the model. The resulting fitted model can be used to summarize the data, to predict unobserved values from the same system, and to understand the mechanisms that may underlie the system.  Mathematically, linear least squares is the problem of approximately solving an overdetermined system of linear equations, where the best approximation is defined as that which minimizes the sum of squared differences between the data values and their corresponding modeled values. The approach is called "linear" least squares since the assumed function is linear in the parameters to be estimated. Linear least squares problems are convex and have a closed-form solution that is unique, provided that the number of data points used for fitting equals or exceeds the number of unknown parameters, except in special degenerate situations. In contrast, non-linear least squares problems generally must be solved by an iterative procedure , and the problems can be non-convex with multiple optima for the objective function. If prior distributions are available, then even an underdetermined system can be solved using the Bayesian MMSE estimator .  In statistics, linear least squares problems correspond to a particularly important type of statistical model called linear regression which arises as a particular form of regression analysis . One basic form of such a model is an ordinary least squares model. The present article concentrates on the mathematical aspects of linear least squares problems, with discussion of the formulation and interpretation of statistical regression models and statistical inferences related to these being dealt with in the articles just mentioned. See outline of regression analysis for an outline of the topic.  Example  (Figure)  A plot of the data points (in red), the least squares line of best fit (in blue), and the residuals (in green).   As a result of an experiment, four    (  x  ,  y  )     x  y    (x,y)   data points were obtained,     (  1  ,  6  )   ,     1  6    (1,6),        (  2  ,  5  )   ,     2  5    (2,5),        (  3  ,  7  )   ,     3  7    (3,7),   and    (  4  ,  10  )     4  10    (4,10)   (shown in red in the picture on the right). We hope to find a line    y  =    β  1   +    β  2   x        y     subscript  β  1      subscript  β  2   x      y=\beta_{1}+\beta_{2}x   that best fits these four points. In other words, we would like to find the numbers    β  1     subscript  β  1    \beta_{1}   and    β  2     subscript  β  2    \beta_{2}   that approximately solve the overdetermined linear system       β  1   +   1   β  2         subscript  β  1     1   subscript  β  2      \displaystyle\beta_{1}+1\beta_{2}   of four equations in two unknowns in some "best" sense.  The "error", at each point, between the curve fit and the data is the difference between the right- and left-hand sides of the equations above. The least squares approach to solving this problem is to try to make as small as possible the sum of the squares of these errors; that is, to find the minimum of the function       S   (   β  1   ,   β  2   )    =         S    subscript  β  1    subscript  β  2     absent    \displaystyle S(\beta_{1},\beta_{2})=     The minimum is determined by calculating the partial derivatives of    S   (   β  1   ,   β  2   )       S    subscript  β  1    subscript  β  2      S(\beta_{1},\beta_{2})   with respect to    β  1     subscript  β  1    \beta_{1}   and    β  2     subscript  β  2    \beta_{2}   and setting them to zero        ∂  S    ∂   β  1     =  0  =     8   β  1    +   20   β  2     -  56             S      subscript  β  1     0             8   subscript  β  1      20   subscript  β  2     56      \frac{\partial S}{\partial\beta_{1}}=0=8\beta_{1}+20\beta_{2}-56           ∂  S    ∂   β  2     =  0  =     20   β  1    +   60   β  2     -  154.             S      subscript  β  2     0             20   subscript  β  1      60   subscript  β  2     154.      \frac{\partial S}{\partial\beta_{2}}=0=20\beta_{1}+60\beta_{2}-154.     This results in a system of two equations in two unknowns, called the normal equations, which give, when solved       β  1   =  3.5       subscript  β  1   3.5    \beta_{1}=3.5          β  2   =  1.4       subscript  β  2   1.4    \beta_{2}=1.4     and the equation    y  =   3.5  +   1.4  x        y    3.5    1.4  x      y=3.5+1.4x   of the line of best fit. The residuals , that is, the discrepancies between the   y   y   y   values from the experiment and the   y   y   y   values calculated using the line of best fit are then found to be    1.1  ,    1.1   1.1,        -  1.3   ,      1.3    -1.3,        -  0.7   ,      0.7    -0.7,   and   0.9   0.9   0.9   (see the picture on the right). The minimum value of the sum of squares of the residuals is     S   (  3.5  ,  1.4  )    =    1.1  2   +    (   -  1.3   )   2   +    (   -  0.7   )   2   +   0.9  2    =  4.2.          S   3.5  1.4       superscript  1.1  2    superscript    1.3   2    superscript    0.7   2    superscript  0.9  2         4.2.     S(3.5,1.4)=1.1^{2}+(-1.3)^{2}+(-0.7)^{2}+0.9^{2}=4.2.     Using a quadratic model  (Figure)  The result of fitting a quadratic function    y  =    β  1   +    β  2   x   +    β  3     x  2          y     subscript  β  1      subscript  β  2   x      subscript  β  3    superscript  x  2       y=\beta_{1}+\beta_{2}x+\beta_{3}x^{2}\,   (in blue) through a set of data points    (   x  i   ,   y  i   )      subscript  x  i    subscript  y  i     (x_{i},y_{i})   (in red). In linear least squares the function need not be linear in the argument    x  ,    x   x,   but only in the parameters    β  j     subscript  β  j    \beta_{j}   that are determined to give the best fit.   Importantly, in "linear least squares", we are not restricted to using a line as the model as in the above example. For instance, we could have chosen the restricted quadratic model    y  =    β  1    x  2        y     subscript  β  1    superscript  x  2      y=\beta_{1}x^{2}   . This model is still linear in the    β  1     subscript  β  1    \beta_{1}   parameter, so we can still perform the same analysis, constructing a system of equations from the data points:     6   6   \displaystyle 6     The partial derivatives with respect to the parameters (this time there is only one) are again computed and set to 0:        ∂  S    ∂   β  1     =  0  =    708   β  1    -  498             S      subscript  β  1     0           708   subscript  β  1    498      \frac{\partial S}{\partial\beta_{1}}=0=708\beta_{1}-498     and solved       β  1   =  .703       subscript  β  1   .703    \beta_{1}=.703     leading to the resulting best fit model    y  =   .703   x  2        y    .703   superscript  x  2      y=.703x^{2}     The general problem  Consider an overdetermined system       ∑   j  =  1   n    X   i  j     β  j   =   y  i   ,   (  i  =  1  ,  2  ,  …  ,  m  )   ,     fragments   superscript   subscript     j  1    n    subscript  X    i  j     subscript  β  j     subscript  y  i   normal-,   fragments  normal-(  i   1  normal-,  2  normal-,  normal-…  normal-,  m  normal-)   normal-,    \sum_{j=1}^{n}X_{ij}\beta_{j}=y_{i},\ (i=1,2,\dots,m),     of m  linear equations in n unknown coefficients , β 1 , β 2 ,…, β n , with m > n . This can be written in matrix form as        𝐗  𝜷   =  𝐲   ,        𝐗  𝜷   𝐲    \mathbf{X}\boldsymbol{\beta}=\mathbf{y},     where        𝐗  =   [      X  11      X  12     ⋯     X   1  n         X  21      X  22     ⋯     X   2  n        ⋮    ⋮    ⋱    ⋮       X   m  1       X   m  2      ⋯     X   m  n       ]    ,    𝜷  =   [      β  1        β  2       ⋮       β  n      ]    ,   𝐲  =   [      y  1        y  2       ⋮       y  m      ]      .     formulae-sequence    𝐗     subscript  X  11    subscript  X  12   normal-⋯   subscript  X    1  n       subscript  X  21    subscript  X  22   normal-⋯   subscript  X    2  n      normal-⋮  normal-⋮  normal-⋱  normal-⋮     subscript  X    m  1     subscript  X    m  2    normal-⋯   subscript  X    m  n        formulae-sequence    𝜷     subscript  β  1      subscript  β  2     normal-⋮     subscript  β  n        𝐲     subscript  y  1      subscript  y  2     normal-⋮     subscript  y  m         \mathbf{X}=\begin{bmatrix}X_{11}&X_{12}&\cdots&X_{1n}\\
 X_{21}&X_{22}&\cdots&X_{2n}\\
 \vdots&\vdots&\ddots&\vdots\\
 X_{m1}&X_{m2}&\cdots&X_{mn}\end{bmatrix},\qquad\boldsymbol{\beta}=\begin{%
 bmatrix}\beta_{1}\\
 \beta_{2}\\
 \vdots\\
 \beta_{n}\end{bmatrix},\qquad\mathbf{y}=\begin{bmatrix}y_{1}\\
 y_{2}\\
 \vdots\\
 y_{m}\end{bmatrix}.     Such a system usually has no solution, so the goal is instead to find the coefficients β which fit the equations "best," in the sense of solving the quadratic  minimization problem        𝜷  ^   =       arg   min   𝜷    S   (  𝜷  )     ,       normal-^  𝜷      𝜷    arg  min    S  𝜷     \hat{\boldsymbol{\beta}}=\underset{\boldsymbol{\beta}}{\operatorname{arg\,min}%
 }\,S(\boldsymbol{\beta}),     where the objective function S is given by        S   (  𝜷  )    =    ∑   i  =  1   m     |    y  i   -    ∑   j  =  1   n     X   i  j     β  j      |   2    =    ∥   𝐲  -   𝐗  𝜷    ∥   2    .          S  𝜷     superscript   subscript     i  1    m    superscript       subscript  y  i     superscript   subscript     j  1    n      subscript  X    i  j     subscript  β  j       2          superscript   norm    𝐲    𝐗  𝜷     2      S(\boldsymbol{\beta})=\sum_{i=1}^{m}\bigl|y_{i}-\sum_{j=1}^{n}X_{ij}\beta_{j}%
 \bigr|^{2}=\bigl\|\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\bigr\|^{2}.     A justification for choosing this criterion is given in properties below. This minimization problem has a unique solution, provided that the n columns of the matrix X are linearly independent , given by solving the normal equations         (    𝐗  T   𝐗   )    𝜷  ^    =    𝐗  T   𝐲    .           superscript  𝐗  normal-T   𝐗    normal-^  𝜷       superscript  𝐗  normal-T   𝐲     (\mathbf{X}^{\rm T}\mathbf{X})\hat{\boldsymbol{\beta}}=\mathbf{X}^{\rm T}%
 \mathbf{y}.     The matrix     𝐗  T   𝐗       superscript  𝐗  normal-T   𝐗    \mathbf{X}^{\rm T}\mathbf{X}   is known as the Gramian matrix of   𝐗   𝐗   \mathbf{X}   , which possesses several nice properties such as being a positive semi-definite matrix .  Following is one generalized example which shows how to find a best fit line using least squares method (in two dimension). Later, a MATLAB code implementation of the same is also written which can help one visualize graphically.  Let us consider 3 points (x 1 ,y 1 ), (x 2 ,y 2 ) and (x 3 ,y 3 ) in a two dimensional frame. Now, assume that the best fit line is in the form: y = C + Dx. Substituting the three points in this equation will give us three equations.  These three equations can be written in the form of a matrix equation of the type Ax = b . Projection of matrix A on b gives us x. This can be represented by the formula :         (    𝐀  T   𝐀   )    𝜷  ^    =    𝐀  T   𝐲    .           superscript  𝐀  normal-T   𝐀    normal-^  𝜷       superscript  𝐀  normal-T   𝐲     (\mathbf{A}^{\rm T}\mathbf{A})\hat{\boldsymbol{\beta}}=\mathbf{A}^{\rm T}%
 \mathbf{y}.     We get beta from this expression; i.e.    𝜷  ^     normal-^  𝜷    \hat{\boldsymbol{\beta}}   is x. Hence, this follows that we have obtained C and D of the equation y = C + Dx . Hence, the best fit line obtained.  Matlab code implementation for better understanding in visuals:  1 %MATLAB code for finding the best fit line using least squares method  2 x=input('enter a')                      %input in the form of matrix, rows contain points  3     a=[1,x(1,1);1,x(2,1);1,x(3,1)]          %forming A of Ax=b  4     b=[x(1,2);x(2,2);x(3,2)]                %forming b of Ax=b  5     yy=inv(transpose(a)*a)*transpose(a)*b   %computing projection of matrix A on b, giving x  6 %plotting the best fit line  7   xx=linspace(1,10,50);  8   y=yy(1)+yy(2)*xx;  9   plot(xx,y)  10     %plotting the points(data) for which we found the best fit line  11 hold on  12    plot(x(2,1),x(2,2),'x')  13 hold on  14    plot(x(1,1),x(1,2),'x')  15 hold on  16    plot(x(3,1),x(3,2),'x')  17 hold off  Python code using the same variable naming as the Matlab code:  import numpy as np import matplotlib.pyplot as plt
 
 x = np.random.rand( 3 , 2 ) *  10 a = np.matrix([ [ 1 ,x[ 0 ][ 0 ]], [ 1 ,x[ 1 ][ 0 ]], [ 1 ,x[ 2 ][ 0 ]] ])
 b = np.matrix([ [x[ 0 ][ 1 ]], [x[ 1 ][ 1 ]], [x[ 2 ][ 1 ]] ])
 yy = (a.T * a).I * a.T * b
 xx = np.linspace( 1 , 10 , 50 )
 y = np.array(yy[ 0 ] + yy[ 1 ] * xx)
 
 plt.figure( 1 )
 plt.plot(xx, y.T, color = 'r' )
 plt.scatter([x[ 0 ][ 0 ], x[ 1 ][ 0 ], x[ 2 ][ 0 ] ], [x[ 0 ][ 1 ], x[ 1 ][ 1 ], x[ 2 ][ 1 ] ]) 
 plt.show()  Derivation of the normal equations  Define the   i   i   i   th residual to be       r  i   =    y  i   -    ∑   j  =  1   n     X   i  j     β  j           subscript  r  i      subscript  y  i     superscript   subscript     j  1    n      subscript  X    i  j     subscript  β  j        r_{i}=y_{i}-\sum_{j=1}^{n}X_{ij}\beta_{j}   .  Then   S   S   S   can be rewritten       S  =    ∑   i  =  1   m    r  i  2     .      S    superscript   subscript     i  1    m    superscript   subscript  r  i   2      S=\sum_{i=1}^{m}r_{i}^{2}.     S is minimized when its gradient vector is zero. (This follows by definition: if the gradient vector is not zero, there is a direction in which we can move to minimize it further - see maxima and minima .) The elements of the gradient vector are the partial derivatives of S with respect to the parameters:        ∂  S    ∂   β  j     =  2   ∑   i  =  1   m    r  i      ∂   r  i     ∂   β  j       (  j  =  1  ,  2  ,  …  ,  n  )   .     fragments      S      subscript  β  j      2   superscript   subscript     i  1    m    subscript  r  i        subscript  r  i       subscript  β  j      fragments  normal-(  j   1  normal-,  2  normal-,  normal-…  normal-,  n  normal-)   normal-.    \frac{\partial S}{\partial\beta_{j}}=2\sum_{i=1}^{m}r_{i}\frac{\partial r_{i}}%
 {\partial\beta_{j}}\ (j=1,2,\dots,n).     The derivatives are         ∂   r  i     ∂   β  j     =   -   X   i  j      .           subscript  r  i       subscript  β  j        subscript  X    i  j       \frac{\partial r_{i}}{\partial\beta_{j}}=-X_{ij}.     Substitution of the expressions for the residuals and the derivatives into the gradient equations gives        ∂  S    ∂   β  j     =  2   ∑   i  =  1   m    (   y  i   -   ∑   k  =  1   n    X   i  k     β  k   )    (  -   X   i  j    )    (  j  =  1  ,  2  ,  …  ,  n  )   .     fragments      S      subscript  β  j      2   superscript   subscript     i  1    m    fragments  normal-(   subscript  y  i     superscript   subscript     k  1    n    subscript  X    i  k     subscript  β  k   normal-)    fragments  normal-(    subscript  X    i  j    normal-)    fragments  normal-(  j   1  normal-,  2  normal-,  normal-…  normal-,  n  normal-)   normal-.    \frac{\partial S}{\partial\beta_{j}}=2\sum_{i=1}^{m}\left(y_{i}-\sum_{k=1}^{n}%
 X_{ik}\beta_{k}\right)(-X_{ij})\ (j=1,2,\dots,n).     Thus if    β  ^     normal-^  β    \hat{\beta}   minimizes S , we have      2   ∑   i  =  1   m    (   y  i   -   ∑   k  =  1   n    X   i  k      β  ^   k   )    (  -   X   i  j    )   =   0    (  j  =  1  ,  2  ,  …  ,  n  )   .     fragments  2   superscript   subscript     i  1    m    fragments  normal-(   subscript  y  i     superscript   subscript     k  1    n    subscript  X    i  k     subscript   normal-^  β   k   normal-)    fragments  normal-(    subscript  X    i  j    normal-)    0   fragments  normal-(  j   1  normal-,  2  normal-,  normal-…  normal-,  n  normal-)   normal-.    2\sum_{i=1}^{m}\left(y_{i}-\sum_{k=1}^{n}X_{ik}\hat{\beta}_{k}\right)(-X_{ij})%
 =0\ (j=1,2,\dots,n).     Upon rearrangement, we obtain the normal equations :       ∑   i  =  1   m    ∑   k  =  1   n    X   i  j     X   i  k      β  ^   k   =   ∑   i  =  1   m    X   i  j      y  i     (  j  =  1  ,  2  ,  …  ,  n  )   .     fragments   superscript   subscript     i  1    m    superscript   subscript     k  1    n    subscript  X    i  j     subscript  X    i  k     subscript   normal-^  β   k     superscript   subscript     i  1    m    subscript  X    i  j     subscript  y  i    fragments  normal-(  j   1  normal-,  2  normal-,  normal-…  normal-,  n  normal-)   normal-.    \sum_{i=1}^{m}\sum_{k=1}^{n}X_{ij}X_{ik}\hat{\beta}_{k}=\sum_{i=1}^{m}X_{ij}y_%
 {i}\ (j=1,2,\dots,n).     The normal equations are written in matrix notation as        (    𝐗  T   𝐗   )    𝜷  ^    =    𝐗  T   𝐲            superscript  𝐗  normal-T   𝐗    normal-^  𝜷       superscript  𝐗  normal-T   𝐲     (\mathbf{X}^{\mathrm{T}}\mathbf{X})\hat{\boldsymbol{\beta}}=\mathbf{X}^{%
 \mathrm{T}}\mathbf{y}   (where X T is the matrix transpose of X ).  The solution of the normal equations yields the vector    𝜷  ^     normal-^  𝜷    \hat{\boldsymbol{\beta}}   of the optimal parameter values.  Derivation directly in terms of matrices  The normal equations can be derived directly from a matrix representation of the problem as follows. The objective is to minimize        S   (  𝜷  )    =    ∥   𝐲  -   𝐗  𝜷    ∥   2   =     (   𝐲  -   𝐗  𝜷    )   T    (   𝐲  -   𝐗  𝜷    )    =      𝐲  T   𝐲   -    𝜷  T    𝐗  T   𝐲   -    𝐲  T   𝐗  𝜷    +    𝜷  T    𝐗  T   𝐗  𝜷     .          S  𝜷    superscript   norm    𝐲    𝐗  𝜷     2           superscript    𝐲    𝐗  𝜷    normal-T     𝐲    𝐗  𝜷                 superscript  𝐲  normal-T   𝐲      superscript  𝜷  normal-T    superscript  𝐗  normal-T   𝐲      superscript  𝐲  normal-T   𝐗  𝜷       superscript  𝜷  normal-T    superscript  𝐗  normal-T   𝐗  𝜷       S(\boldsymbol{\beta})=\bigl\|\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\bigr\|^{2%
 }=(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^{\rm T}(\mathbf{y}-\mathbf{X}%
 \boldsymbol{\beta})=\mathbf{y}^{\rm T}\mathbf{y}-\boldsymbol{\beta}^{\rm T}%
 \mathbf{X}^{\rm T}\mathbf{y}-\mathbf{y}^{\rm T}\mathbf{X}\boldsymbol{\beta}+%
 \boldsymbol{\beta}^{\rm T}\mathbf{X}^{\rm T}\mathbf{X}\boldsymbol{\beta}.     Note that      (    𝜷  T    𝐗  T   𝐲   )   T   =    𝐲  T   𝐗  𝜷        superscript     superscript  𝜷  normal-T    superscript  𝐗  normal-T   𝐲   normal-T      superscript  𝐲  normal-T   𝐗  𝜷     (\boldsymbol{\beta}^{\rm T}\mathbf{X}^{\rm T}\mathbf{y})^{\rm T}=\mathbf{y}^{%
 \rm T}\mathbf{X}\boldsymbol{\beta}   has the dimension 1x1 (the number of columns of   𝐲   𝐲   \mathbf{y}   ), so it is a scalar and equal to its own transpose, hence      𝜷  T    𝐗  T   𝐲   =    𝐲  T   𝐗  𝜷          superscript  𝜷  normal-T    superscript  𝐗  normal-T   𝐲      superscript  𝐲  normal-T   𝐗  𝜷     \boldsymbol{\beta}^{\rm T}\mathbf{X}^{\rm T}\mathbf{y}=\mathbf{y}^{\rm T}%
 \mathbf{X}\boldsymbol{\beta}   and the quantity to minimize becomes        S   (  𝜷  )    =      𝐲  T   𝐲   -   2   𝜷  T    𝐗  T   𝐲    +    𝜷  T    𝐗  T   𝐗  𝜷     .        S  𝜷          superscript  𝐲  normal-T   𝐲     2   superscript  𝜷  normal-T    superscript  𝐗  normal-T   𝐲       superscript  𝜷  normal-T    superscript  𝐗  normal-T   𝐗  𝜷      S(\boldsymbol{\beta})=\mathbf{y}^{\rm T}\mathbf{y}-2\boldsymbol{\beta}^{\rm T}%
 \mathbf{X}^{\rm T}\mathbf{y}+\boldsymbol{\beta}^{\rm T}\mathbf{X}^{\rm T}%
 \mathbf{X}\boldsymbol{\beta}.     Differentiating this with respect to   𝜷   𝜷   \boldsymbol{\beta}   and equating to zero to satisfy the first-order conditions gives         -    𝐗  T   𝐲    +    (    𝐗  T   𝐗   )   𝜷    =  0   ,             superscript  𝐗  normal-T   𝐲         superscript  𝐗  normal-T   𝐗   𝜷    0    -\mathbf{X}^{\rm T}\mathbf{y}+(\mathbf{X}^{\rm T}\mathbf{X}){\boldsymbol{\beta%
 }}=0,     which is equivalent to the above-given normal equations. A sufficient condition for satisfaction of the second-order conditions for a minimum is that   𝐗   𝐗   \mathbf{X}   have full column rank, in which case     𝐗  T   𝐗       superscript  𝐗  normal-T   𝐗    \mathbf{X}^{\rm T}\mathbf{X}   is positive definite .  Derivation without calculus  When     𝐗  T   𝐗       superscript  𝐗  normal-T   𝐗    \mathbf{X}^{\rm T}\mathbf{X}   is positive definite, the formula for the minimizing value of   𝜷   𝜷   \boldsymbol{\beta}   can be derived without the use of derivatives. The quantity       S   (  𝜷  )    =      𝐲  T   𝐲   -   2   𝜷  T    𝐗  T   𝐲    +    𝜷  T    𝐗  T   𝐗  𝜷          S  𝜷          superscript  𝐲  normal-T   𝐲     2   superscript  𝜷  normal-T    superscript  𝐗  normal-T   𝐲       superscript  𝜷  normal-T    superscript  𝐗  normal-T   𝐗  𝜷      S(\boldsymbol{\beta})=\mathbf{y}^{\rm T}\mathbf{y}-2\boldsymbol{\beta}^{\rm T}%
 \mathbf{X}^{\rm T}\mathbf{y}+\boldsymbol{\beta}^{\rm T}\mathbf{X}^{\rm T}%
 \mathbf{X}\boldsymbol{\beta}     can be written as         ⟨  𝜷  ,  𝜷  ⟩   -   2   ⟨  𝜷  ,     (    𝐗  T   𝐗   )    -  1     𝐗  T   𝐲   ⟩     +   ⟨     (    𝐗  T   𝐗   )    -  1     𝐗  T   𝐲   ,     (    𝐗  T   𝐗   )    -  1     𝐗  T   𝐲   ⟩   +  C   ,         𝜷  𝜷     2   𝜷     superscript     superscript  𝐗  normal-T   𝐗     1     superscript  𝐗  normal-T   𝐲          superscript     superscript  𝐗  normal-T   𝐗     1     superscript  𝐗  normal-T   𝐲      superscript     superscript  𝐗  normal-T   𝐗     1     superscript  𝐗  normal-T   𝐲    C    \langle\boldsymbol{\beta},\boldsymbol{\beta}\rangle-2\langle\boldsymbol{\beta}%
 ,(\mathbf{X}^{\rm T}\mathbf{X})^{-1}\mathbf{X}^{\rm T}\mathbf{y}\rangle+%
 \langle(\mathbf{X}^{\rm T}\mathbf{X})^{-1}\mathbf{X}^{\rm T}\mathbf{y},(%
 \mathbf{X}^{\rm T}\mathbf{X})^{-1}\mathbf{X}^{\rm T}\mathbf{y}\rangle+C,     where   C   C   C   depends only on   𝐲   𝐲   \mathbf{y}   and   𝐗   𝐗   \mathbf{X}   , and    ⟨  ⋅  ,  ⋅  ⟩     normal-⋅  normal-⋅    \langle\cdot,\cdot\rangle   is the inner product defined by        ⟨  x  ,  y  ⟩   =    x  T    (    𝐗  T   𝐗   )   y    .       x  y      superscript  x  normal-T      superscript  𝐗  normal-T   𝐗   y     \langle x,y\rangle=x^{\rm T}(\mathbf{X}^{\rm T}\mathbf{X})y.     It follows that    S   (  𝜷  )       S  𝜷    S(\boldsymbol{\beta})   is equal to       ⟨   𝜷  -     (    𝐗  T   𝐗   )    -  1     𝐗  T   𝐲    ,   𝜷  -     (    𝐗  T   𝐗   )    -  1     𝐗  T   𝐲    ⟩   +  C         𝜷     superscript     superscript  𝐗  normal-T   𝐗     1     superscript  𝐗  normal-T   𝐲      𝜷     superscript     superscript  𝐗  normal-T   𝐗     1     superscript  𝐗  normal-T   𝐲     C    \langle\boldsymbol{\beta}-(\mathbf{X}^{\rm T}\mathbf{X})^{-1}\mathbf{X}^{\rm T%
 }\mathbf{y},\boldsymbol{\beta}-(\mathbf{X}^{\rm T}\mathbf{X})^{-1}\mathbf{X}^{%
 \rm T}\mathbf{y}\rangle+C     and therefore minimized exactly when       𝜷  -     (    𝐗  T   𝐗   )    -  1     𝐗  T   𝐲    =  0.        𝜷     superscript     superscript  𝐗  normal-T   𝐗     1     superscript  𝐗  normal-T   𝐲    0.    \boldsymbol{\beta}-(\mathbf{X}^{\rm T}\mathbf{X})^{-1}\mathbf{X}^{\rm T}%
 \mathbf{y}=0.     Computation  A general approach to the least squares problem    min    ∥   𝐲  -   X  𝜷    ∥   2      min   superscript   norm    𝐲    X  𝜷     2     \operatorname{\,min}\,\big\|\mathbf{y}-X\boldsymbol{\beta}\big\|^{2}   can be described as follows. Suppose that we can find an n by m matrix S such that XS is an orthogonal projection onto the image of X . Then a solution to our minimization problem is given by      𝜷  =   S  𝐲       𝜷    S  𝐲     \boldsymbol{\beta}=S\mathbf{y}     simply because       X  𝜷   =   X   (   S  𝐲   )    =    (   X  S   )   𝐲           X  𝜷     X    S  𝐲             X  S   𝐲      X\boldsymbol{\beta}=X(S\mathbf{y})=(XS)\mathbf{y}     is exactly a sought for orthogonal projection of   𝐲   𝐲   \mathbf{y}   onto an image of X ( see the picture below and note that as explained in the next section the image of X is just a subspace generated by column vectors of X ). A few popular ways to find such a matrix S are described below.  Inverting the matrix of the normal equations (Or the Gramian matrix , in short)  The algebraic solution of the normal equations can be written as       𝜷  ^   =     (    𝐗  T   𝐗   )    -  1     𝐗  T   𝐲   =    𝐗  +   𝐲          normal-^  𝜷      superscript     superscript  𝐗  normal-T   𝐗     1     superscript  𝐗  normal-T   𝐲           superscript  𝐗    𝐲      \hat{\boldsymbol{\beta}}=(\mathbf{X}^{\rm T}\mathbf{X})^{-1}\mathbf{X}^{\rm T}%
 \mathbf{y}=\mathbf{X}^{+}\mathbf{y}     where X  + is the Moore–Penrose pseudoinverse of X . Although this equation is correct, and can work in many applications, it is not computationally efficient to invert the normal equations matrix. An exception occurs in numerical smoothing and differentiation where an analytical expression is required.  If the matrix X T X is well-conditioned and positive definite , implying that it has full rank , the normal equations can be solved directly by using the Cholesky decomposition  R T R , where R is an upper triangular matrix , giving:         R  T   R   𝜷  ^    =    X  T   𝐲    .         superscript  R  normal-T   R   normal-^  𝜷       superscript  X  normal-T   𝐲     R^{\rm T}R\hat{\boldsymbol{\beta}}=X^{\rm T}\mathbf{y}.     The solution is obtained in two stages, a forward substitution step, solving for z :         R  T   𝐳   =    X  T   𝐲    ,         superscript  R  normal-T   𝐳      superscript  X  normal-T   𝐲     R^{\rm T}\mathbf{z}=X^{\rm T}\mathbf{y},     followed by a backward substitution, solving for    𝜷  ^     normal-^  𝜷    \hat{\boldsymbol{\beta}}           R   𝜷  ^    =  𝐳   .        R   normal-^  𝜷    𝐳    R\hat{\boldsymbol{\beta}}=\mathbf{z}.     Both substitutions are facilitated by the triangular nature of R .  See example of linear regression for a worked-out numerical example with three parameters.  Orthogonal decomposition methods  Orthogonal decomposition methods of solving the least squares problem are slower than the normal equations method but are more numerically stable because they avoid forming the product X T X .  The residuals are written in matrix notation as       𝐫  =   𝐲  -   X   𝜷  ^      .      𝐫    𝐲    X   normal-^  𝜷       \mathbf{r}=\mathbf{y}-X\hat{\boldsymbol{\beta}}.     The matrix X is subjected to an orthogonal decomposition, e.g., the QR decomposition as follows.      X  =   Q  R       X    Q  R     X=QR   , where Q is an m × n  orthogonal matrix ( Q T Q=I ) and R is an n × n upper triangular matrix with     r   i  i    >  0       subscript  r    i  i    0    r_{ii}>0   .  The residual vector is left-multiplied by Q T .        Q  T   𝐫   =     Q  T   𝐲   -    (    Q  T   Q   )   R   𝜷  ^     =   [        (    Q  T   𝐲   )   n   -   R   𝜷  ^           (    Q  T   𝐲   )    m  -  n       ]   =   [     𝐮      𝐯     ]            superscript  Q  normal-T   𝐫        superscript  Q  normal-T   𝐲        superscript  Q  normal-T   Q   R   normal-^  𝜷               subscript     superscript  Q  normal-T   𝐲   n     R   normal-^  𝜷        subscript     superscript  Q  normal-T   𝐲     m  n             𝐮    𝐯       Q^{\rm T}\mathbf{r}=Q^{\rm T}\mathbf{y}-\left(Q^{\rm T}Q\right)R\hat{%
 \boldsymbol{\beta}}=\begin{bmatrix}\left(Q^{\rm T}\mathbf{y}\right)_{n}-R\hat{%
 \boldsymbol{\beta}}\\
 \left(Q^{\rm T}\mathbf{y}\right)_{m-n}\end{bmatrix}=\begin{bmatrix}\mathbf{u}%
 \\
 \mathbf{v}\end{bmatrix}     Because Q is orthogonal , the sum of squares of the residuals, s , may be written as:      s  =    ∥  𝐫  ∥   2   =    𝐫  T   𝐫   =    𝐫  T   Q   Q  T   𝐫   =     𝐮  T   𝐮   +    𝐯  T   𝐯          s   superscript   norm  𝐫   2           superscript  𝐫  normal-T   𝐫           superscript  𝐫  normal-T   Q   superscript  Q  normal-T   𝐫             superscript  𝐮  normal-T   𝐮      superscript  𝐯  normal-T   𝐯       s=\|\mathbf{r}\|^{2}=\mathbf{r}^{\rm T}\mathbf{r}=\mathbf{r}^{\rm T}QQ^{\rm T}%
 \mathbf{r}=\mathbf{u}^{\rm T}\mathbf{u}+\mathbf{v}^{\rm T}\mathbf{v}   Since v doesn't depend on β , the minimum value of s is attained when the upper block, u , is zero. Therefore the parameters are found by solving:        R   𝜷  ^    =    (    Q  T   𝐲   )   n    .        R   normal-^  𝜷     subscript     superscript  Q  normal-T   𝐲   n     R\hat{\boldsymbol{\beta}}=\left(Q^{\rm T}\mathbf{y}\right)_{n}.   These equations are easily solved as R is upper triangular.  An alternative decomposition of X is the singular value decomposition (SVD) 1      X  =   U  Σ   V  T        X    U  normal-Σ   superscript  V  normal-T      X=U\Sigma V^{\rm T}   ,  where U is m by m orthogonal matrix, V is n by n orthogonal matrix and   Σ   normal-Σ   \Sigma   is an m by n matrix with all its elements outside of the main diagonal equal to 0 . The pseudoinverse of   Σ   normal-Σ   \Sigma   is easily obtained by inverting its non-zero diagonal elements and transposing. Hence,        𝐗𝐗  +   =   U  Σ   V  T   V   Σ  +    U  T    =   U  P   U  T     ,         superscript  𝐗𝐗      U  normal-Σ   superscript  V  normal-T   V   superscript  normal-Σ     superscript  U  normal-T           U  P   superscript  U  normal-T       \mathbf{X}\mathbf{X}^{+}=U\Sigma V^{\rm T}V\Sigma^{+}U^{\rm T}=UPU^{\rm T},     where P is obtained from   Σ   normal-Σ   \Sigma   by replacing its non-zero diagonal elements with ones. Since      (   𝐗𝐗  +   )   *   =   𝐗𝐗  +        superscript   superscript  𝐗𝐗       superscript  𝐗𝐗      (\mathbf{X}\mathbf{X}^{+})^{*}=\mathbf{X}\mathbf{X}^{+}   (the property of pseudoinverse), the matrix    U  P   U  T       U  P   superscript  U  normal-T     UPU^{\rm T}   is an orthogonal projection onto the image (column-space) of X . In accordance with a general approach described in the introduction above (find XS which is an orthogonal projection),      S  =   𝐗  +       S   superscript  𝐗      S=\mathbf{X}^{+}   ,  and thus,      β  =   V   Σ  +    U  T   𝐲       β    V   superscript  normal-Σ     superscript  U  normal-T   𝐲     \beta=V\Sigma^{+}U^{\rm T}\mathbf{y}     is a solution of a least squares problem. This method is the most computationally intensive, but is particularly useful if the normal equations matrix, X T X , is very ill-conditioned (i.e. if its condition number multiplied by the machine's relative round-off error is appreciably large). In that case, including the smallest singular values in the inversion merely adds numerical noise to the solution. This can be cured with the truncated SVD approach, giving a more stable and exact answer, by explicitly setting to zero all singular values below a certain threshold and so ignoring them, a process closely related to factor analysis .  Properties of the least-squares estimators  The gradient equations at the minimum can be written as         (   𝐲  -   X   𝜷  ^     )   T   X   =  0.         superscript    𝐲    X   normal-^  𝜷     normal-T   X   0.    (\mathbf{y}-X\hat{\boldsymbol{\beta}})^{\rm T}X=0.     A geometrical interpretation of these equations is that the vector of residuals,    𝐲  -   X   𝜷  ^        𝐲    X   normal-^  𝜷      \mathbf{y}-X\hat{\boldsymbol{\beta}}   is orthogonal to the column space of X , since the dot product      (   𝐲  -   X   𝜷  ^     )   ⋅  X   𝐯       normal-⋅    𝐲    X   normal-^  𝜷     X   𝐯    (\mathbf{y}-X\hat{\boldsymbol{\beta}})\cdot X\mathbf{v}   is equal to zero for any conformal vector, v . This means that    𝐲  -   X   𝜷  ^        𝐲    X   bold-^  𝜷      \mathbf{y}-X\boldsymbol{\hat{\beta}}   is the shortest of all possible vectors    𝐲  -   X  𝜷       𝐲    X  𝜷     \mathbf{y}-X\boldsymbol{\beta}   , that is, the variance of the residuals is the minimum possible. This is illustrated at the right.  Introducing    𝜸  ^     normal-^  𝜸    \hat{\boldsymbol{\gamma}}   and a matrix K with the assumption that a matrix    [    X   K   ]     delimited-[]    X  K     [X\ K]   is non-singular and K T  X = 0 (cf. Orthogonal projections ), the residual vector should satisfy the following equation:        𝐫  ^   ≜   𝐲  -   X   𝜷  ^     =   K   𝜸  ^     .       normal-≜   normal-^  𝐫     𝐲    X   normal-^  𝜷            K   normal-^  𝜸       \hat{\mathbf{r}}\triangleq\mathbf{y}-X\hat{\boldsymbol{\beta}}=K\hat{{%
 \boldsymbol{\gamma}}}.   The equation and solution of linear least squares are thus described as follows:       𝐲  =    [     X    K     ]    (      𝜷  ^        𝜸  ^      )     ,      𝐲      X  K       normal-^  𝜷      normal-^  𝜸        \mathbf{y}=\begin{bmatrix}X&K\end{bmatrix}\begin{pmatrix}\hat{\boldsymbol{%
 \beta}}\\
 \hat{\boldsymbol{\gamma}}\end{pmatrix},           (      𝜷  ^        𝜸  ^      )   =     [     X    K     ]    -  1    𝐲   =    [        (    X  T   X   )    -  1     X  T           (    K  T   K   )    -  1     K  T       ]   𝐲    .           normal-^  𝜷      normal-^  𝜸        superscript    X  K      1    𝐲               superscript     superscript  X  normal-T   X     1     superscript  X  normal-T         superscript     superscript  K  normal-T   K     1     superscript  K  normal-T      𝐲      \begin{pmatrix}\hat{\boldsymbol{\beta}}\\
 \hat{\boldsymbol{\gamma}}\end{pmatrix}=\begin{bmatrix}X&K\end{bmatrix}^{-1}%
 \mathbf{y}=\begin{bmatrix}(X^{\rm T}X)^{-1}X^{\rm T}\\
 (K^{\rm T}K)^{-1}K^{\rm T}\end{bmatrix}\mathbf{y}.     If the experimental errors,    ϵ    ϵ   \epsilon\,   , are uncorrelated, have a mean of zero and a constant variance,   σ   σ   \sigma   , the Gauss-Markov theorem states that the least-squares estimator,    𝜷  ^     normal-^  𝜷    \hat{\boldsymbol{\beta}}   , has the minimum variance of all estimators that are linear combinations of the observations. In this sense it is the best, or optimal, estimator of the parameters. Note particularly that this property is independent of the statistical distribution function of the errors. In other words, the distribution function of the errors need not be a normal distribution . However, for some probability distributions, there is no guarantee that the least-squares solution is even possible given the observations; still, in such cases it is the best estimator that is both linear and unbiased.  For example, it is easy to show that the arithmetic mean of a set of measurements of a quantity is the least-squares estimator of the value of that quantity. If the conditions of the Gauss-Markov theorem apply, the arithmetic mean is optimal, whatever the distribution of errors of the measurements might be.  However, in the case that the experimental errors do belong to a normal distribution, the least-squares estimator is also a maximum likelihood estimator. 2  These properties underpin the use of the method of least squares for all types of data fitting, even when the assumptions are not strictly valid.  Limitations  An assumption underlying the treatment given above is that the independent variable, x , is free of error. In practice, the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored. When this is not the case, total least squares or more generally errors-in-variables models , or rigorous least squares , should be used. This can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure. 3 4  In some cases the (weighted) normal equations matrix X T X is ill-conditioned . When fitting polynomials the normal equations matrix is a Vandermonde matrix . Vandermonde matrices become increasingly ill-conditioned as the order of the matrix increases. In these cases, the least squares estimate amplifies the measurement noise and may be grossly inaccurate. Various regularization techniques can be applied in such cases, the most common of which is called ridge regression . If further information about the parameters is known, for example, a range of possible values of    𝜷  ^     normal-^  𝜷    \mathbf{\hat{\boldsymbol{\beta}}}   , then various techniques can be used to increase the stability of the solution. For example, see constrained least squares .  Another drawback of the least squares estimator is the fact that the norm of the residuals,    ∥   𝐲  -   X   𝜷  ^     ∥     norm    𝐲    X   normal-^  𝜷       \|\mathbf{y}-X\hat{\boldsymbol{\beta}}\|   is minimized, whereas in some cases one is truly interested in obtaining small error in the parameter    𝜷  ^     normal-^  𝜷    \mathbf{\hat{\boldsymbol{\beta}}}   , e.g., a small value of    ∥   𝜷  -   𝜷  ^    ∥     norm    𝜷   normal-^  𝜷      \|{\boldsymbol{\beta}}-\hat{\boldsymbol{\beta}}\|   . However, since the true parameter   𝜷   𝜷   {\boldsymbol{\beta}}   is necessarily unknown, this quantity cannot be directly minimized. If a prior probability on    𝜷  ^     normal-^  𝜷    \hat{\boldsymbol{\beta}}   is known, then a Bayes estimator can be used to minimize the mean squared error ,    E   {    ∥   𝜷  -   𝜷  ^    ∥   2   }       E    superscript   norm    𝜷   normal-^  𝜷     2      E\left\{\|{\boldsymbol{\beta}}-\hat{\boldsymbol{\beta}}\|^{2}\right\}   . The least squares method is often applied when no prior is known. Surprisingly, when several parameters are being estimated jointly, better estimators can be constructed, an effect known as Stein's phenomenon . For example, if the measurement error is Gaussian , several estimators are known which dominate , or outperform, the least squares technique; the best known of these is the James–Stein estimator . This is an example of more general shrinkage estimators that have been applied to regression problems.  Weighted linear least squares  In some cases the observations may be weighted—for example, they may not be equally reliable. In this case, one can minimize the weighted sum of squares:            arg   min   𝜷      ∑   i  =  1   m     w  i     |    y  i   -    ∑   j  =  1   n     X   i  j     β  j      |   2      =       arg   min   𝜷      ∥    W   1  /  2     (   𝐲  -   X  𝜷    )    ∥   2     .         𝜷    arg  min      superscript   subscript     i  1    m      subscript  w  i    superscript       subscript  y  i     superscript   subscript     j  1    n      subscript  X    i  j     subscript  β  j       2         𝜷    arg  min     superscript   norm     superscript  W    1  2      𝐲    X  𝜷      2      \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}}\,\sum_{i=1}^{m}w_{i}%
 \left|y_{i}-\sum_{j=1}^{n}X_{ij}\beta_{j}\right|^{2}=\underset{\boldsymbol{%
 \beta}}{\operatorname{arg\,min}}\,\big\|W^{1/2}(\mathbf{y}-X\boldsymbol{\beta}%
 )\big\|^{2}.     where w i > 0 is the weight of the i th observation, and W is the diagonal matrix of such weights.  The weights should, ideally, be equal to the reciprocal of the variance of the measurement. 5  6 The normal equations are then:         (    X  T   W  X   )    𝜷  ^    =    X  T   W  𝐲    .           superscript  X  normal-T   W  X    normal-^  𝜷       superscript  X  normal-T   W  𝐲     \left(X^{\rm T}WX\right)\hat{\boldsymbol{\beta}}=X^{\rm T}W\mathbf{y}.     This method is used in iteratively reweighted least squares .  Parameter errors and correlation  The estimated parameter values are linear combinations of the observed values        𝜷  ^   =     (    X  T   W  X   )    -  1     X  T   W  𝐲    .       normal-^  𝜷      superscript     superscript  X  normal-T   W  X     1     superscript  X  normal-T   W  𝐲     \hat{\boldsymbol{\beta}}=(X^{\rm T}WX)^{-1}X^{\rm T}W\mathbf{y}.\,     Therefore an expression for the residuals (i.e., the estimated errors in the observations) can be obtained by error propagation from the errors in the observations. Let the variance-covariance matrix for the observations be denoted by M and that of the parameters by M β . Then,        M  β   =     (    X  T   W  X   )    -  1     X  T   W  M   W  T   X    (    X  T    W  T   X   )    -  1      .       superscript  M  β      superscript     superscript  X  normal-T   W  X     1     superscript  X  normal-T   W  M   superscript  W  normal-T   X   superscript     superscript  X  normal-T    superscript  W  normal-T   X     1       M^{\beta}=(X^{\rm T}WX)^{-1}X^{\rm T}WMW^{\rm T}X(X^{\rm T}W^{\rm T}X)^{-1}.     When W = M −1 this simplifies to        M  β   =    (    X  T   W  X   )    -  1     .       superscript  M  β    superscript     superscript  X  normal-T   W  X     1      M^{\beta}=(X^{\rm T}WX)^{-1}.     When unit weights are used ( W = I ) it is implied that the experimental errors are uncorrelated and all equal: M = σ 2 I , where σ 2 is the variance of an observation, and I is the identity matrix . In this case σ 2 is approximated by    S   m  -  n       S    m  n     \frac{S}{m-n}   , where S is the minimum value of the objective function        M  β   =    S   m  -  n      (    X  T   X   )    -  1      .       superscript  M  β       S    m  n     superscript     superscript  X  normal-T   X     1       M^{\beta}=\frac{S}{m-n}(X^{\rm T}X)^{-1}.     The denominator, m − n , is the number of degrees of freedom ; see effective degrees of freedom for generalizations for the case of correlated observations. In all cases, the variance of the parameter    β  i     subscript  β  i    \beta_{i}   is given by    M   i  i   β     subscript   superscript  M  β     i  i     M^{\beta}_{ii}   and the covariance between parameters    β  i     subscript  β  i    \beta_{i}   and    β  j     subscript  β  j    \beta_{j}   is given by    M   i  j   β     subscript   superscript  M  β     i  j     M^{\beta}_{ij}   . Standard deviation is the square root of variance, and the correlation coefficient is given by     ρ   i  j    =    M   i  j   β   /   (    σ  i    σ  j    )         subscript  ρ    i  j       subscript   superscript  M  β     i  j       subscript  σ  i    subscript  σ  j       \rho_{ij}=M^{\beta}_{ij}/(\sigma_{i}\sigma_{j})   . These error estimates reflect only random errors in the measurements. The true uncertainty in the parameters is larger due to the presence of systematic errors which, by definition, cannot be quantified. Note that even though the observations may be un-correlated, the parameters are typically correlated .  Parameter confidence limits  It is often assumed , for want of any concrete evidence but often appealing to the central limit theorem -- see Normal distribution#Occurrence -- that the error on each observation belongs to a normal distribution with a mean of zero and standard deviation   σ   σ   \sigma   . Under that assumption the following probabilities can be derived for a single scalar parameter estimate in terms of its estimated standard error    s   e  β       s   subscript  e  β     se_{\beta}   (given here ):   68% that the interval     β  ^   ±   s   e  β       plus-or-minus   normal-^  β     s   subscript  e  β      \hat{\beta}\pm se_{\beta}   encompasses the true coefficient value  95% that the interval     β  ^   ±   2  s   e  β       plus-or-minus   normal-^  β     2  s   subscript  e  β      \hat{\beta}\pm 2se_{\beta}   encompasses the true coefficient value  99% that the interval     β  ^   ±   2.5  s   e  β       plus-or-minus   normal-^  β     2.5  s   subscript  e  β      \hat{\beta}\pm 2.5se_{\beta}   encompasses the true coefficient value   The assumption is not unreasonable when m >> n . If the experimental errors are normally distributed the parameters will belong to a Student's t-distribution with m − n  degrees of freedom . When m >> n Student's t-distribution approximates a normal distribution. Note, however, that these confidence limits cannot take systematic error into account. Also, parameter errors should be quoted to one significant figure only, as they are subject to sampling error . 7  When the number of observations is relatively small, Chebychev's inequality can be used for an upper bound on probabilities, regardless of any assumptions about the distribution of experimental errors: the maximum probabilities that a parameter will be more than 1, 2 or 3 standard deviations away from its expectation value are 100%, 25% and 11% respectively.  Residual values and correlation  The residuals are related to the observations by       𝐫  ^   =   𝐲  -   X   𝜷  ^     =   𝐲  -   H  𝐲    =    (   I  -  H   )   𝐲          normal-^  𝐫     𝐲    X   normal-^  𝜷            𝐲    H  𝐲             I  H   𝐲      \mathbf{\hat{r}}=\mathbf{y}-X\hat{\boldsymbol{\beta}}=\mathbf{y}-H\mathbf{y}=(%
 I-H)\mathbf{y}     where H is the idempotent matrix known as the hat matrix :      H  =   X    (    X  T   W  X   )    -  1     X  T   W       H    X   superscript     superscript  X  normal-T   W  X     1     superscript  X  normal-T   W     H=X\left(X^{\rm T}WX\right)^{-1}X^{\rm T}W     and I is the identity matrix . The variance-covariance matrix of the residuals, M r is given by        M  𝐫   =    (   I  -  H   )   M    (   I  -  H   )   T     .       superscript  M  𝐫       I  H   M   superscript    I  H   normal-T      M^{\mathbf{r}}=\left(I-H\right)M\left(I-H\right)^{\rm T}.     Thus the residuals are correlated, even if the observations are not.  When    W  =   M   -  1        W   superscript  M    1      W=M^{-1}   ,        M  𝐫   =    (   I  -  H   )   M    .       superscript  M  𝐫       I  H   M     M^{\mathbf{r}}=\left(I-H\right)M.     The sum of residual values is equal to zero whenever the model function contains a constant term. Left-multiply the expression for the residuals by X T :        X  T    𝐫  ^    =     X  T   𝐲   -    X  T   X   𝜷  ^     =     X  T   𝐲   -    (    X  T   X   )     (    X  T   X   )    -  1     X  T   𝐲    =  𝟎           superscript  X  normal-T    normal-^  𝐫         superscript  X  normal-T   𝐲      superscript  X  normal-T   X   normal-^  𝜷               superscript  X  normal-T   𝐲        superscript  X  normal-T   X    superscript     superscript  X  normal-T   X     1     superscript  X  normal-T   𝐲         0     X^{\rm T}\hat{\mathbf{r}}=X^{\rm T}\mathbf{y}-X^{\rm T}X\hat{\boldsymbol{\beta%
 }}=X^{\rm T}\mathbf{y}-(X^{\rm T}X)(X^{\rm T}X)^{-1}X^{\rm T}\mathbf{y}=%
 \mathbf{0}     Say, for example, that the first term of the model is a constant, so that     X   i  1    =  1       subscript  X    i  1    1    X_{i1}=1   for all i . In that case it follows that        ∑  i  m     X   i  1      r  ^   i     =    ∑  i  m     r  ^   i    =  0.          superscript   subscript   i   m      subscript  X    i  1     subscript   normal-^  r   i       superscript   subscript   i   m    subscript   normal-^  r   i         0.     \sum_{i}^{m}X_{i1}\hat{r}_{i}=\sum_{i}^{m}\hat{r}_{i}=0.     Thus, in the motivational example , above, the fact that the sum of residual values is equal to zero it is not accidental but is a consequence of the presence of the constant term, α, in the model.  If experimental error follows a normal distribution , then, because of the linear relationship between residuals and observations, so should residuals, 8 but since the observations are only a sample of the population of all possible observations, the residuals should belong to a Student's t-distribution . Studentized residuals are useful in making a statistical test for an outlier when a particular residual appears to be excessively large.  Objective function  The optimal value of the objective function, found by substituting in the optimal expression for the coefficient vector, can be written as       S  =    𝐲  T     (   I  -  H   )   T    (   I  -  H   )   𝐲   =    𝐲  T    (   I  -  H   )   𝐲    ,        S     superscript  𝐲  normal-T    superscript    I  H   normal-T     I  H   𝐲           superscript  𝐲  normal-T     I  H   𝐲      S=\mathbf{y}^{\rm T}(I-H)^{\rm T}(I-H)\mathbf{y}=\mathbf{y}^{\rm T}(I-H)%
 \mathbf{y},     the latter equality holding since ( I – H ) is symmetric and idempotent. It can be shown from this 9 that under an appropriate assignment of weights the expected value of S is m-n . If instead unit weights are assumed, the expected value of S is     (   m  -  n   )    σ  2         m  n    superscript  σ  2     (m-n)\sigma^{2}   , where    σ  2     superscript  σ  2    \sigma^{2}   is the variance of each observation.  If it is assumed that the residuals belong to a normal distribution, the objective function, being a sum of weighted squared residuals, will belong to a chi-squared (    χ  2     superscript  χ  2    \chi^{2}   ) distribution with m-n  degrees of freedom . Some illustrative percentile values of    χ  2     superscript  χ  2    \chi^{2}   are given in the following table. 10        m-n       χ  0.50  2     subscript   superscript  χ  2   0.50    \chi^{2}_{0.50}          χ  0.95  2     subscript   superscript  χ  2   0.95    \chi^{2}_{0.95}          χ  0.99  2     subscript   superscript  χ  2   0.99    \chi^{2}_{0.99}          10   9.34   18.3   23.2     25   24.3   37.7   44.3     100   99.3   124   136       These values can be used for a statistical criterion as to the goodness-of-fit . When unit weights are used, the numbers should be divided by the variance of an observation.  Constrained linear least squares  Often it is of interest to solve a linear least squares problem with an additional constraint on the solution. With constrained linear least squares, the original equation       𝐗  𝜷   =  𝐲        𝐗  𝜷   𝐲    \mathbf{X}\boldsymbol{\beta}=\mathbf{y}     must be satisfied (in the least squares sense) while also ensuring that some other property of   𝜷   𝜷   \boldsymbol{\beta}   is maintained. There are often special purpose algorithms for solving such problems efficiently. Some examples of constraints are given below:   Equality constrained least squares: the elements of   𝜷   𝜷   \boldsymbol{\beta}   must exactly satisfy     𝐋  𝜷   =  𝐝        𝐋  𝜷   𝐝    \mathbf{L}\boldsymbol{\beta}=\mathbf{d}     Regularized least squares: the elements of   𝜷   𝜷   \boldsymbol{\beta}   must satisfy     ∥    𝐋  𝜷   -  𝐝   ∥   ≤  ρ       norm      𝐋  𝜷   𝐝    ρ    \|\mathbf{L}\boldsymbol{\beta}-\mathbf{d}\|\leq\rho     Non-negative least squares (NNLS): The vector   𝜷   𝜷   \boldsymbol{\beta}   satisfies the vector inequality     𝜷  ≥  𝟎      𝜷  0    \boldsymbol{\beta}\geq\boldsymbol{0}   that is defined componentwise --- that is, each component must be either positive or zero.  Box-constrained least squares: The vector   𝜷   𝜷   \boldsymbol{\beta}   satisfies the vector inequalities      𝒍  𝒃   ≤  𝜷  ≤   𝒖  𝒃           𝒍  𝒃   𝜷         𝒖  𝒃      \boldsymbol{lb}\leq\boldsymbol{\beta}\leq\boldsymbol{ub}   , each of which is defined componentwise.  Integer constrained least squares: all elements of   𝜷   𝜷   \boldsymbol{\beta}   must be integer (instead of real numbers ).  Phase constrained least squares: all elements of   𝜷   𝜷   \boldsymbol{\beta}   must have the same phase (or must be real rather than complex numbers , i.e. phase = 0).   When the constraint only applies to some of the variables, the mixed problem may be solved using separable least squares by letting    𝐗  =   [    𝐗  𝟏    𝐗  𝟐    ]       𝐗   delimited-[]     subscript  𝐗  1    subscript  𝐗  2       \mathbf{X}=[\mathbf{X_{1}}\mathbf{X_{2}}]   and     β  T   =   [    β  𝟏    T    β  𝟐    T    ]        superscript  β  normal-T    delimited-[]     superscript   subscript  β  1   normal-T    superscript   subscript  β  2   normal-T       \mathbf{\beta}^{\rm T}=[\mathbf{\beta_{1}}^{\rm T}\mathbf{\beta_{2}}^{\rm T}]   represent the unconstrained (1) and constrained (2) components. Then substituting the least squares solution for    β  𝟏     subscript  β  1    \mathbf{\beta_{1}}   , i.e.        𝜷  𝟏   ^   =    𝐗  𝟏    +    (   𝐲  -    𝐗  𝟐    𝜷  𝟐     )         normal-^   subscript  𝜷  1       superscript   subscript  𝐗  1       𝐲     subscript  𝐗  2    subscript  𝜷  2        \hat{\boldsymbol{\beta_{1}}}=\mathbf{X_{1}}^{+}(\mathbf{y}-\mathbf{X_{2}}%
 \boldsymbol{\beta_{2}})     back into the original expression gives (following some rearrangement) an equation that can be solved as a purely constrained problem in    β  𝟐     subscript  β  2    \mathbf{\beta_{2}}   .        𝐏𝐗  𝟐    𝜷  𝟐    =  𝐏𝐲         subscript  𝐏𝐗  2    subscript  𝜷  2    𝐏𝐲    \mathbf{P}\mathbf{X_{2}}\boldsymbol{\beta_{2}}=\mathbf{P}\mathbf{y}     where    𝐏  :=   𝐈  -    𝐗  𝟏    𝐗  𝟏    +        assign  𝐏    𝐈     subscript  𝐗  1    superscript   subscript  𝐗  1         \mathbf{P}:=\mathbf{I}-\mathbf{X_{1}}\mathbf{X_{1}}^{+}   is a projection matrix . Following the constrained estimation of     𝜷  𝟐   ^     normal-^   subscript  𝜷  2     \hat{\boldsymbol{\beta_{2}}}   the vector     𝜷  𝟏   ^     normal-^   subscript  𝜷  1     \hat{\boldsymbol{\beta_{1}}}   is obtained from the expression above.  Typical uses and applications   Polynomial fitting : models are polynomials in an independent variable, x :  Straight line     f   (  x  ,  𝜷  )    =    β  1   +    β  2   x          f   x  𝜷       subscript  β  1      subscript  β  2   x      f(x,\boldsymbol{\beta})=\beta_{1}+\beta_{2}x   . 11  Quadratic     f   (  x  ,  𝜷  )    =    β  1   +    β  2   x   +    β  3    x  2           f   x  𝜷       subscript  β  1      subscript  β  2   x      subscript  β  3    superscript  x  2       f(x,\boldsymbol{\beta})=\beta_{1}+\beta_{2}x+\beta_{3}x^{2}   .  Cubic, quartic and higher polynomials. For regression with high-order polynomials , the use of orthogonal polynomials is recommended. 12   Numerical smoothing and differentiation — this is an application of polynomial fitting.  Multinomials in more than one independent variable, including surface fitting  Curve fitting with B-splines  13  Chemometrics , Calibration curve , Standard addition , Gran plot , analysis of mixtures   Uses in data fitting  The primary application of linear least squares is in data fitting . Given a set of m data points      y  1   ,   y  2   ,  …  ,   y  m    ,      subscript  y  1    subscript  y  2   normal-…   subscript  y  m     y_{1},y_{2},\dots,y_{m},   consisting of experimentally measured values taken at m values     x  1   ,   x  2   ,  …  ,   x  m       subscript  x  1    subscript  x  2   normal-…   subscript  x  m     x_{1},x_{2},\dots,x_{m}   of an independent variable (    x  i     subscript  x  i    x_{i}   may be scalar or vector quantities), and given a model function     y  =   f   (  x  ,  𝜷  )     ,      y    f   x  𝜷      y=f(x,\boldsymbol{\beta}),   with     𝜷  =   (   β  1   ,   β  2   ,  …  ,   β  n   )    ,      𝜷    subscript  β  1    subscript  β  2   normal-…   subscript  β  n      \boldsymbol{\beta}=(\beta_{1},\beta_{2},\dots,\beta_{n}),   it is desired to find the parameters    β  j     subscript  β  j    \beta_{j}   such that the model function "best" fits the data. In linear least squares, linearity is meant to be with respect to parameters     β  j   ,     subscript  β  j    \beta_{j},   so        f   (  x  ,  𝜷  )    =    ∑   j  =  1   n     β  j    ϕ  j    (  x  )      .        f   x  𝜷      superscript   subscript     j  1    n      subscript  β  j    subscript  ϕ  j   x      f(x,\boldsymbol{\beta})=\sum_{j=1}^{n}\beta_{j}\phi_{j}(x).     Here, the functions    ϕ  j     subscript  ϕ  j    \phi_{j}   may be nonlinear with respect to the variable x .  Ideally, the model function fits the data exactly, so       y  i   =   f   (   x  i   ,  𝜷  )         subscript  y  i     f    subscript  x  i   𝜷      y_{i}=f(x_{i},\boldsymbol{\beta})     for all     i  =   1  ,  2  ,  …  ,  m    .      i   1  2  normal-…  m     i=1,2,\dots,m.   This is usually not possible in practice, as there are more data points than there are parameters to be determined. The approach chosen then is to find the minimal possible value of the sum of squares of the residuals       r  i    (  𝜷  )   =   y  i   -  f   (   x  i   ,  𝜷  )   ,   (  i  =  1  ,  2  ,  …  ,  m  )      fragments   subscript  r  i    fragments  normal-(  β  normal-)     subscript  y  i    f   fragments  normal-(   subscript  x  i   normal-,  β  normal-)   normal-,   fragments  normal-(  i   1  normal-,  2  normal-,  normal-…  normal-,  m  normal-)     r_{i}(\boldsymbol{\beta})=y_{i}-f(x_{i},\boldsymbol{\beta}),\ (i=1,2,\dots,m)   so to minimize the function        S   (  𝜷  )    =    ∑   i  =  1   m     r  i  2    (  𝜷  )      .        S  𝜷     superscript   subscript     i  1    m      superscript   subscript  r  i   2   𝜷      S(\boldsymbol{\beta})=\sum_{i=1}^{m}r_{i}^{2}(\boldsymbol{\beta}).     After substituting for    r  i     subscript  r  i    r_{i}   and then for   f   f   f   , this minimization problem becomes the quadratic minimization problem above with        X   i  j    =    ϕ  j    (   x  i   )     ,       subscript  X    i  j       subscript  ϕ  j    subscript  x  i      X_{ij}=\phi_{j}(x_{i}),     and the best fit can be found by solving the normal equations.  Further discussion  The numerical methods for linear least squares are important because linear regression models are among the most important types of model, both as formal statistical models and for exploration of data-sets. The majority of statistical computer packages contain facilities for regression analysis that make use of linear least squares computations. Hence it is appropriate that considerable effort has been devoted to the task of ensuring that these computations are undertaken efficiently and with due regard to round-off error .  Individual statistical analyses are seldom undertaken in isolation, but rather are part of a sequence of investigatory steps. Some of the topics involved in considering numerical methods for linear least squares relate to this point. Thus important topics can be   Computations where a number of similar, and often nested , models are considered for the same data-set. That is, where models with the same dependent variable but different sets of independent variables are to be considered, for essentially the same set of data-points.  Computations for analyses that occur in a sequence, as the number of data-points increases.  Special considerations for very extensive data-sets.   Fitting of linear models by least squares often, but not always, arise in the context of statistical analysis . It can therefore be important that considerations of computation efficiency for such problems extend to all of the auxiliary quantities required for such analyses, and are not restricted to the formal solution of the linear least squares problem.  Rounding errors  Matrix calculations, like any other, are affected by rounding errors . An early summary of these effects, regarding the choice of computation methods for matrix inversion, was provided by Wilkinson. 14  See also   Line-line intersection#Nearest point to non-intersecting lines , an application   References  Further reading          External links   Least Squares Fitting – From MathWorld  Least Squares Fitting-Polynomial – From MathWorld   af:Kleinste-kwadratemetode  cs:Metoda nejmenších čtverců  de:Methode der kleinsten Quadrate  es:Mínimos cuadrados  fr:Méthode des moindres carrés  gl:Mínimos cadrados  gl:Mínimos cadrados lineais  it:Minimi Quadrati  he:שיטת הריבועים הפחותים  la:Methodus quadratorum minimorum  hu:Legkisebb négyzetek módszere  nl:Kleinste-kwadratenmethode  ja:最小二乗法  pl:Metoda najmniejszych kwadratów  pt:Método dos mínimos quadrados  ru:Метод наименьших квадратов  su:Kuadrat leutik  uk:Метод найменших квадратів  fi:Pienimmän neliösumman menetelmä  sv:Minstakvadratmetoden  tr:En küçük kareler yöntemi  ur:لکیری اقل مربعات  vi:Bình phương tối thiểu  vi:Bình phương tối thiểu tuyến tính  zh:最小二乘法 "  Category:Regression analysis  Category:Computational statistics  Category:Numerical linear algebra  Category:Least squares  Category:Articles with example MATLAB/Octave code  Category:Articles with example Python code     ↩  ↩  ↩  ↩  This implies that the observations are uncorrelated. If the observations are correlated , the expression    S  =    ∑  k     ∑  j     r  k    W   k  j      r  j           S    subscript   k     subscript   j      subscript  r  k    subscript  W    k  j     subscript  r  j        \textstyle S=\sum_{k}\sum_{j}r_{k}W_{kj}r_{j}\,   applies. In this case the weight matrix should ideally be equal to the inverse of the variance-covariance matrix of the observations. ↩  , chapter 3 ↩  ↩  ↩  ↩  ↩  ↩  ↩   Wilkinson, J.H. (1963) "Chapter 3: Matrix Computations", Rounding Errors in Algebraic Processes , London: Her Majesty's Stationery Office (National Physical Laboratory, Notes in Applied Science, No.32) ↩    