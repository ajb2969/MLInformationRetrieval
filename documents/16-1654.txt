   Generalized functional linear model      Generalized functional linear model  The '''generalized functional linear model''' ('''GFLM''') is an extension of the [[generalized linear model]] (GLM) that allows one to regress univariate responses of various types (continuous or discrete) on functional predictors, which are mostly random trajectories generated by a square-integrable  stochastic processes . Similarly to GLM, a link function relates the expected value of the response variable to a linear predictor, which in case of GFLM is obtained by forming the scalar product of the random predictor function   X   X   X   with a smooth parameter function   β   β   \beta   . Functional Linear Regression, Functional Poisson Regression and Functional Binomial Regression, with the important Functional Logistic Regression included, are special cases of GFLM. Applications of GFLM include classification and discrimination of stochastic processes and functional data . 1  Overview  A key aspect of GFLM is estimation and inference for the smooth parameter function   β   β   \beta   which is usually obtained by dimension reduction of the infinite dimensional functional predictor. A common method is to expand the predictor function   X   X   X   in an orthonormal  basis of L 2 space , the Hilbert space of square integrable functions with the simultaneous expansion of the parameter function in the same basis. This representation is then combined with a truncation step to reduce the contribution of the parameter function   β   β   \beta   in the linear predictor to a finite number of regression coefficients. Functional principal component analysis (FPCA) that employs the Karhunen–Loève expansion is a common and parsimonious approach to accomplish this. Other orthogonal expansions, like Fourier expansions and B-spline expansions may also be employed for the dimension reduction step. The Akaike information criterion (AIC) can be used for selecting the number of included components. Minimization of cross-validation prediction errors is another criteria often used in classification applications. Once the dimension of the predictor process has been reduced, the simplified linear predictor allows to use GLM and quasi-likelihood estimation techniques to obtain estimates of the finite dimensional regression coefficients which in turn provide an estimate of the parameter function   β   β   \beta   in the GFLM.  Model components  Linear predictor  The predictor functions      X   (  t  )    ,  t   ∈  T         X  t   t   T    \textstyle X(t),t\in T   , typically are square integrable stochastic processes on a real interval   T   T   T   and the unknown smooth parameter function      β   (  t  )    ,  t   ∈  T         β  t   t   T    \beta(t),t\in T   , is assumed to be square integrable on   T   T   T   . Given a real measure    d  w      d  w    dw   on   T   T   T   , the linear predictor is given by    η  =   α  +   ∫    X  c    (  t  )   β   (  t  )   d  w   (  t  )          η    α       superscript  X  c   t  β  t  d  w  t       \eta=\alpha+\int X^{c}(t)\beta(t)\,dw(t)   where      X  c    (  t  )    =    X   (  t  )    -   E   (   X   (  t  )    )            superscript  X  c   t       X  t     E    X  t       X^{c}(t)=X(t)-\text{E}(X(t))   is the centered predictor process and   α   α   \alpha   is a scalar that serves as an intercept.  Response variable and variance function  The outcome   Y   Y   Y   is typically a real valued random variable which may be either continuous or discrete. Often the conditional distribution of   Y   Y   Y   given the predictor process is specified within the exponential family . However it is also sufficient to consider the functional quasi-likelihood set up, where instead of the distribution of the response one specifies the conditional variance function ,    Var   (  Y  ∣  X  )   =   σ  2    (  μ  )      fragments  Var   fragments  normal-(  Y  normal-∣  X  normal-)     superscript  σ  2    fragments  normal-(  μ  normal-)     \rm{Var}(Y\mid X)=\sigma^{2}(\mu)   , as a function of the conditional mean,    E   (  Y  ∣  X  )   =  μ     fragments  E   fragments  normal-(  Y  normal-∣  X  normal-)    μ    \rm{E}(Y\mid X)=\mu   .  Link function  The link function   g   g   g   is a smooth invertible function, that relates the conditional mean of the response    E   (  Y  ∣  X  )   =  μ     fragments  E   fragments  normal-(  Y  normal-∣  X  normal-)    μ    \rm{E}(Y\mid X)=\mu   with the linear predictor    η  =   α  +   ∫    X  c    (  t  )   β   (  t  )   d  w   (  t  )          η    α       superscript  X  c   t  β  t  d  w  t       \eta=\alpha+\int X^{c}(t)\beta(t)\,dw(t)   . The relationship is given by    μ  =   g   (  η  )        μ    g  η     \mu=g(\eta)   .  Formulation  In order to implement the necessary dimension reduction, the centered predictor process     X  c    (  t  )        superscript  X  c   t    X^{c}(t)   and the parameter function    β   (  t  )       β  t    \beta(t)   are expanded as,         X  c    (  t  )    =    ∑   j  =  1   ∞     ξ  j    ρ  j    (  t  )   and  β   (  t  )     =    ∑   j  =  1   ∞     β  j    ρ  j    (  t  )      ,           superscript  X  c   t     superscript   subscript     j  1         subscript  ξ  j    subscript  ρ  j   t  and  β  t           superscript   subscript     j  1         subscript  β  j    subscript  ρ  j   t       X^{c}(t)=\sum_{j=1}^{\infty}\xi_{j}\rho_{j}(t)\text{ and }\beta(t)=\sum_{j=1}^%
 {\infty}\beta_{j}\rho_{j}(t),     where       ρ  j   ,  j   =  1   ,   2  ,  …      formulae-sequence      subscript  ρ  j   j   1    2  normal-…     \rho_{j},j=1,2,\ldots   is an orthonormal basis of the function space      L  2    (   d  w   )    ,       superscript  L  2     d  w     L^{2}(dw),   such that      ∫  T     ρ  j    (  t  )    ρ  k    (  t  )   d  w   (  t  )     =   δ   j  k          subscript   T      subscript  ρ  j   t   subscript  ρ  k   t  d  w  t     subscript  δ    j  k      \int_{T}\rho_{j}(t)\rho_{k}(t)\,dw(t)=\delta_{jk}   where     δ   j  k    =  1       subscript  δ    j  k    1    \delta_{jk}=1   if    j  =  k      j  k    j=k   and   0   0    otherwise.  The random variables    ξ  j     subscript  ξ  j    \xi_{j}   are given by     ξ  j   =   ∫    X  c    (  t  )    ρ  j    (  t  )   d  w   (  t  )          subscript  ξ  j        superscript  X  c   t   subscript  ρ  j   t  d  w  t      \xi_{j}=\int X^{c}(t)\rho_{j}(t)\,dw(t)   and the coefficients    β  j     subscript  β  j    \beta_{j}   as     β  j   =   ∫   β   (  t  )    ρ  j    (  t  )   d  w   (  t  )          subscript  β  j       β  t   subscript  ρ  j   t  d  w  t      \beta_{j}=\int\beta(t)\rho_{j}(t)\,dw(t)   for    j  =   1  ,  2  ,  …       j   1  2  normal-…     j=1,2,\ldots   .  Note that     E   (   ξ  j   )    =  0        E   subscript  ξ  j    0    \text{E}(\xi_{j})=0   and      ∑   j  =  1   ∞    β  j  2    <  ∞        superscript   subscript     j  1       superscript   subscript  β  j   2       \sum_{j=1}^{\infty}\beta_{j}^{2}<\infty   and denoting     σ  j  2   =   Var   (   ξ  j   )    =   E   (   ξ  j  2   )           superscript   subscript  σ  j   2     Var   subscript  ξ  j           E   superscript   subscript  ξ  j   2       \sigma_{j}^{2}=\text{Var}(\xi_{j})=\text{E}(\xi_{j}^{2})   , we also have      ∑   j  =  1   ∞    σ  j  2    =   ∫   E     (    X  c    (  t  )    )   2    d  w   (  t  )     <  ∞          superscript   subscript     j  1       superscript   subscript  σ  j   2        E   superscript     superscript  X  c   t   2   d  w  t             \sum_{j=1}^{\infty}\sigma_{j}^{2}=\int\text{E}(X^{c}(t))^{2}\,dw(t)<\infty   .  From the orthonormality of the basis functions    ρ  j     subscript  ρ  j    \rho_{j}   , it follows immediately that     ∫    X  c    (  t  )   β   (  t  )   d  w   (  t  )     =    ∑   j  =  1   ∞     β  j    ξ  j              superscript  X  c   t  β  t  d  w  t      superscript   subscript     j  1         subscript  β  j    subscript  ξ  j       \int X^{c}(t)\beta(t)\,dw(t)=\sum_{j=1}^{\infty}\beta_{j}\xi_{j}   .  The key step is then approximating    η  =   α  +   ∫    X  c    (  t  )   β   (  t  )   d  w   (  t  )      =   α  +    ∑   j  =  1   ∞     β  j    ξ  j            η    α       superscript  X  c   t  β  t  d  w  t            α    superscript   subscript     j  1         subscript  β  j    subscript  ξ  j         \eta=\alpha+\int X^{c}(t)\beta(t)\,dw(t)=\alpha+\sum_{j=1}^{\infty}\beta_{j}%
 \xi_{j}   by    η  ≈   α  +    ∑   j  =  1   p     β  j    ξ  j          η    α    superscript   subscript     j  1    p      subscript  β  j    subscript  ξ  j        \eta\approx\alpha+\sum_{j=1}^{p}\beta_{j}\xi_{j}   for a suitably chosen truncation point   p   p   p   .  FPCA gives the most parsimonious approximation of the linear predictor for a given number of basis functions as the eigenfunction basis explains more of the variation than any other set of basis functions.  For a differentiable link function with bounded first derivative, the approximation error of the   p   p   p   -truncated model i.e. the linear predictor trunctated to the summation of the first   p   p   p   components, is a constant multiple of     Var   (    ∑   j  =   p  +  1    ∞     β  j    ξ  j     )    =   E   (    (    ∑   j  =   p  +  1    ∞     β  j    ξ  j     )   2   )    =    ∑   j  =   p  +  1    ∞     β  j    σ  j  2             Var    superscript   subscript     j    p  1          subscript  β  j    subscript  ξ  j        E   superscript    superscript   subscript     j    p  1          subscript  β  j    subscript  ξ  j     2           superscript   subscript     j    p  1          subscript  β  j    superscript   subscript  σ  j   2        \text{Var}(\sum_{j=p+1}^{\infty}\beta_{j}\xi_{j})=\text{E}\left(\left(\sum_{j=%
 p+1}^{\infty}\beta_{j}\xi_{j}\right)^{2}\right)=\sum_{j=p+1}^{\infty}\beta_{j}%
 \sigma_{j}^{2}   .  A heuristic motivation for the truncation strategy derives from the fact that     E   (    (    ∑   j  =   p  +  1    ∞     β  j    ξ  j     )   2   )    =    ∑   j  =   p  +  1    ∞     β  j    σ  j  2     ≤    ∑   j  =   p  +  1    ∞      β  j  2      ∑   j  =   p  +  1    ∞    σ  j  2              E   superscript    superscript   subscript     j    p  1          subscript  β  j    subscript  ξ  j     2      superscript   subscript     j    p  1          subscript  β  j    superscript   subscript  σ  j   2            superscript   subscript     j    p  1          superscript   subscript  β  j   2     superscript   subscript     j    p  1        superscript   subscript  σ  j   2         \text{E}\left(\left(\sum_{j=p+1}^{\infty}\beta_{j}\xi_{j}\right)^{2}\right)=%
 \sum_{j=p+1}^{\infty}\beta_{j}\sigma_{j}^{2}\leq\sum_{j=p+1}^{\infty}\beta_{j}%
 ^{2}\ \sum_{j=p+1}^{\infty}\sigma_{j}^{2}   which is a consequence of the Cauchy–Schwarz inequality and by noting that the right hand side of the last inequality converges to 0 as    p  →  ∞     normal-→  p     p\rightarrow\infty   since both     ∑   j  =  1   ∞    β  j  2       superscript   subscript     j  1       superscript   subscript  β  j   2     \sum_{j=1}^{\infty}\beta_{j}^{2}   and     ∑   j  =  1   ∞    σ  j  2       superscript   subscript     j  1       superscript   subscript  σ  j   2     \sum_{j=1}^{\infty}\sigma_{j}^{2}   are finite.  For the special case of the eigenfunction basis, the sequence       σ  j  2   ,  j   =  1   ,   2  ,  …      formulae-sequence      superscript   subscript  σ  j   2   j   1    2  normal-…     \sigma_{j}^{2},j=1,2,\ldots   corresponds to the sequence of the eigenvalues of the covariance kernel      G   (  s  ,  t  )    =    Cov   (   X   (  s  )    ,   X   (  t  )    )    ,  s    ,   t  ∈  T      formulae-sequence      G   s  t       Cov     X  s     X  t     s      t  T     G(s,t)=\text{Cov}(X(s),X(t)),\ s,t\in T   .  For data with   n   n   n    i.i.d observations, setting     ξ  j  0   =  1       superscript   subscript  ξ  j   0   1    \xi_{j}^{0}=1   ,     β  0   =  α       subscript  β  0   α    \beta_{0}=\alpha   and     ξ  j  i   =   ∫    X  i    (  t  )    ρ  j    (  t  )   d  w   (  t  )          superscript   subscript  ξ  j   i        subscript  X  i   t   subscript  ρ  j   t  d  w  t      \xi_{j}^{i}=\int X_{i}(t)\rho_{j}(t)\,dw(t)   , the approximated linear predictors can be represented as      η  i   =    ∑   j  =  0   p     β  j    ξ  j  i      ,   i  =   1  ,  2  ,  …  ,  n       formulae-sequence     subscript  η  i     superscript   subscript     j  0    p      subscript  β  j    superscript   subscript  ξ  j   i        i   1  2  normal-…  n      \eta_{i}=\sum_{j=0}^{p}\beta_{j}\xi_{j}^{i},i=1,2,\ldots,n   which are related to the means through     μ  i   =   g   (   η  i   )         subscript  μ  i     g   subscript  η  i      \mu_{i}=g(\eta_{i})   .  Estimation  The main aim is to estimate the parameter function   β   β   \beta   .  Once   p   p   p   has been fixed, standard GLM and quasi-likelihood methods can be used for the   p   p   p   -truncated model to estimate     𝜷  T   =   (   β  0   ,   β  1   ,  …  ,   β  p   )        superscript  𝜷  T     subscript  β  0    subscript  β  1   normal-…   subscript  β  p      \boldsymbol{\beta}^{T}=(\beta_{0},\beta_{1},\ldots,\beta_{p})   by solving the estimating equation or the score equation      U   (  β  )    =  0.        U  β   0.    U(\beta)=0.     The vector valued score function turns out to be     U   (  β  )    =    ∑   i  =  1   n       (    Y  i   -   μ  i    )    g  ′    (   η  i   )    ξ  i    /   σ  2     (   μ  i   )           U  β     superscript   subscript     i  1    n            subscript  Y  i    subscript  μ  i     superscript  g  normal-′    subscript  η  i    subscript  ξ  i     superscript  σ  2     subscript  μ  i       U(\beta)=\sum_{i=1}^{n}(Y_{i}-\mu_{i})g^{\prime}(\eta_{i})\xi_{i}/\sigma^{2}(%
 \mu_{i})   which depends on   𝜷   𝜷   \boldsymbol{\beta}   through   μ   μ   \mu   and   η   η   \eta   .  Just as in GLM, the equation     U   (  β  )    =  0        U  β   0    U(\beta)=0   is solved using iterative methods like Newton–Raphson (NR) or Fisher scoring (FS) or iteratively reweighted least squares (IWLS) to get the estimate of the regression coefficients      ^   β       bold-^  absent   β    \boldsymbol{\hat{}}{\beta}   , leading to the estimate of the parameter function      β  ^    (  t  )    =     β  ^   o   +    ∑   j  =  1   p      β  ^   j    ρ  j    (  t  )             normal-^  β   t      subscript   normal-^  β   o     superscript   subscript     j  1    p      subscript   normal-^  β   j    subscript  ρ  j   t       \hat{\beta}(t)=\hat{\beta}_{o}+\sum_{j=1}^{p}\hat{\beta}_{j}\rho_{j}(t)   . When using the canonical link function, these methods are equivalent.  Results are available in the literature of   p   p   p   -truncated models as    p  →  ∞     normal-→  p     p\rightarrow\infty   which provide asymptotic inference for the deviation of the estimated parametric function from the true parametric function and also asymptotic tests for regression effects and asymptotic confidence regions .  Exponential family response  If the response variable    Y  i     subscript  Y  i    Y_{i}   , given     X  i   ∈    L  2    (  T  )         subscript  X  i      superscript  L  2   T     X_{i}\in L^{2}(T)   follows the one parameter exponential family, then its probability density function or probability mass function (as the case may be) is      f   (   y  i   ∣   X  i   )   =  exp   (      y  i    θ  i    -   b   (   θ  i   )     ϕ   +  c   (   y  i   ,  ϕ  )   )      fragments  f   fragments  normal-(   subscript  y  i   normal-∣   subscript  X  i   normal-)      fragments  normal-(         subscript  y  i    subscript  θ  i      b   subscript  θ  i     ϕ    c   fragments  normal-(   subscript  y  i   normal-,  ϕ  normal-)   normal-)     f(y_{i}\mid X_{i})=\exp\left(\frac{y_{i}\theta_{i}-b(\theta_{i})}{\phi}+c(y_{i%
 },\phi)\right)     for some functions   b   b   b   and   c   c   c   , where    θ  i     subscript  θ  i    \theta_{i}   is the canonical parameter, and   ϕ   ϕ   \phi   is a dispersion parameter which is typically assumed to be positive.  In the canonical set up,     η  i   =   α  +   ∫    X  i  c    (  t  )   β   (  t  )   d  w   (  t  )      =   θ  i          subscript  η  i     α       superscript   subscript  X  i   c   t  β  t  d  w  t           subscript  θ  i      \eta_{i}=\alpha+\int X_{i}^{c}(t)\beta(t)\,dw(t)=\theta_{i}   and from the properties of exponential family,         μ  i   =    b  ′    (   θ  i   )     ,    and so   μ  i    =    b  ′    (   η  i   )      .     formulae-sequence     subscript  μ  i      superscript  b  normal-′    subscript  θ  i         and so   subscript  μ  i       superscript  b  normal-′    subscript  η  i       \mu_{i}=b^{\prime}(\theta_{i}),\text{ and so }\mu_{i}=b^{\prime}(\eta_{i}).     Hence    b  ′     superscript  b  normal-′    b^{\prime}   serves as a link function and is called the canonical link function.  Note that    Var   (   y  i   )   =  ϕ   b  ′′    (   θ  i   )   =  ϕ   b  ′′    (   η  i   )   =  ϕ   g  ′    (   η  i   )   =  ϕ   g  ′    (   g   -  1     (   μ  i   )   )   )     fragments  Var   fragments  normal-(   subscript  y  i   normal-)    ϕ   superscript  b  ′′    fragments  normal-(   subscript  θ  i   normal-)    ϕ   superscript  b  ′′    fragments  normal-(   subscript  η  i   normal-)    ϕ   superscript  g  normal-′    fragments  normal-(   subscript  η  i   normal-)    ϕ   superscript  g  normal-′    fragments  normal-(   superscript  g    1     fragments  normal-(   subscript  μ  i   normal-)   normal-)   normal-)    \text{Var}(y_{i})=\phi b^{\prime\prime}(\theta_{i})=\phi b^{\prime\prime}(\eta%
 _{i})=\phi g^{\prime}(\eta_{i})=\phi g^{\prime}(g^{-1}(\mu_{i})))   is the corresponding variance function and   ϕ   ϕ   \phi   the dispersion parameter.  Special cases  Functional linear regression (FLR)  Functional linear regression, one of the most useful tools of functional data analysis, is an example of GFLM where the response variable is continuous and is often assumed to have a Normal distribution . The variance function is a constant function and the link function is identity. Under these assumption we find that the GFLM reduces to the FLR,      μ  =   E   (  Y  ∣  X  )    =  η  =   α  +   ∫    X  c    (  t  )   β   (  t  )   d  w   (  t  )            μ   normal-E  Y  X        η         α       superscript  X  c   t  β  t  d  w  t        \mu=\operatorname{E}(Y\mid X)=\eta=\alpha+\int X^{c}(t)\beta(t)\,dw(t)     Without the normality assumption, the constant variance function motivates the use of quasi-normal techniques.  Functional binary regression  When the response variable has binary outcomes ,i.e., 0 or 1, the distribution is usually chosen as Bernoulli , and then     μ  i   =  P   (   Y  i   =  1  ∣   X  i   )      fragments   subscript  μ  i    P   fragments  normal-(   subscript  Y  i    1  normal-∣   subscript  X  i   normal-)     \mu_{i}=P(Y_{i}=1\mid X_{i})   . Popular link functions are the expit function, which is the inverse of the logit function (functional logistic regression) and the probit function (functional probit regression). Any cumulative distribution function F has range [0,1] which is the range of binomial mean and so can be chosen as a link function. Another link function in this context is the complementary log–log function , which is an asymmetric link. The variance function for binary data is given by     Var   (   Y  i   )    =   ϕ   μ  i    (   1  -   μ  i    )         Var   subscript  Y  i      ϕ   subscript  μ  i     1   subscript  μ  i       \operatorname{Var}(Y_{i})=\phi\mu_{i}(1-\mu_{i})   where the dispersion parameter   ϕ   ϕ   \phi   is taken as 1 or alternatively the quasi-likelihood approach is used.  Functional Poisson regression  Another special case of GFLM occurs when the outcomes are counts, so that the distribution of the responses is assumed to be Poisson . The mean    μ  i     subscript  μ  i    \mu_{i}   is typically linked to the linear predictor    η  i     subscript  η  i    \eta_{i}   via a log-link, which is also the canonical link . The variance function is     Var   (   Y  i   )    =   ϕ   μ  i         Var   subscript  Y  i      ϕ   subscript  μ  i      \operatorname{Var}(Y_{i})=\phi\mu_{i}   , where the dispersion parameter   ϕ   ϕ   \phi   is 1, except when the data might be over-dispersed which is when the quasi-Poisson approach is used.  Extensions  Extensions of GFLM have been proposed for the cases where there are multiple predictor functions. 2 Another generalization is called the Semi Parametric Quasi-likelihood Regression (SPQR) 3 which considers the situation where the link and the variance functions are unknown and are estimated non-parametrically from the data. This situation can also be handled by single or multiple index models, using for example Sliced Inverse Regression (SIR).  Another extension in this domain is Functional Generalized Additive Model (FGAM)) 4 which is a generalization of generalized additive model (GAM) where         g   -  1     (   E   (  Y  ∣  X  )    )    =   α  +    ∑   j  =  1   p     f  j    (   ξ  j   )       ,         superscript  g    1     normal-E  Y  X      α    superscript   subscript     j  1    p      subscript  f  j    subscript  ξ  j        g^{-1}(\operatorname{E}(Y\mid X))=\alpha+\sum_{j=1}^{p}f_{j}(\xi_{j}),     where    ξ  j     subscript  ξ  j    \xi_{j}   are the expansion coefficients of the random predictor function   X   X   X   and each    f  j     subscript  f  j    f_{j}   is an unknown smooth function that has to be estimated and where     E   (    f  j    (   ξ  j   )    )    =  0.        E     subscript  f  j    subscript  ξ  j     0.    \text{E}(f_{j}(\xi_{j}))=0.   .  In general, estimation in FGAM requires combining IWLS with backfitting . However if the expansion coefficients are obtained as functional principal components, then in some cases (eg. Gaussian predictor function   X   X   X   ), they will be independent in which case backfitting is not needed, and one can use popular smoothing methods for estimating the unknown parameter functions    f  j     subscript  f  j    f_{j}   .  Application  A popular data set that has been used for a number of analysis in the domain of functional data analysis consists of the number of eggs laid daily until death of 1000 Mediterranean fruit flies (or medflies for short) 1 2 . The plot here shows the egg laying trajectories in the first 25 days of life of about 600 female medflies (those that have at least 20 remaining eggs in their lifetime). The red colored curves belong to those flies that will lay less than the median number of remaining eggs, while the blue colored curves belong to the flies that will lay more than the median number of remaining eggs after age 25. An related problem of classifying medflies as long-lived or short-lived based on the initial egg laying trajectories as predictors and the subsequent longevity of the flies as response has been studied with the GFLM 5  See also   Generalized linear model  Functional principal component analysis  Functional data analysis  Quasi-likelihood  Karhunen–Loève theorem  Stochastic processes  Lp space   References  "  Category:Generalized linear models     ↩  ↩   ↩      