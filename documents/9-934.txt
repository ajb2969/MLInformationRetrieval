   Hirschberg's algorithm      Hirschberg's algorithm   In computer science , Hirschberg's algorithm , named after its inventor, Dan Hirschberg , is a dynamic programming  algorithm that finds the optimal sequence alignment between two strings . Optimality is measured with the Levenshtein distance , defined to be the sum of the costs of insertions, replacements, deletions, and null actions needed to change one string into the other. Hirschberg's algorithm is simply described as a divide and conquer version of the Needleman–Wunsch algorithm . 1 Hirschberg's algorithm is commonly used in computational biology to find maximal global alignments of DNA and protein sequences.  Algorithm information  Hirschberg's algorithm is a generally applicable algorithm for optimal sequence alignment. BLAST and FASTA are suboptimal heuristics . If x and y are strings, where length( x ) = n and length( y ) = m , the Needleman-Wunsch algorithm finds an optimal alignment in O ( nm ) time, using O( nm ) space. Hirschberg's algorithm is a clever modification of the Needleman-Wunsch Algorithm which still takes O( nm ) time, but needs only O(min{ n , m }) space. 2 One application of the algorithm is finding sequence alignments of DNA or protein sequences. It is also a space-efficient way to calculate the longest common subsequence between two sets of data such as with the common diff tool.  The Hirschberg algorithm can be derived from the Needleman-Wunsch algorithm by observing that: 3   one can compute the optimal alignment score by only storing the current and previous row of the Needleman-Wunsch score matrix;  if     (  Z  ,  W  )   =   NW   (  X  ,  Y  )         Z  W    NW  X  Y     (Z,W)=\operatorname{NW}(X,Y)   is the optimal alignment of    (  X  ,  Y  )     X  Y    (X,Y)   , and    X  =    X  l   +   X  r        X     superscript  X  l    superscript  X  r      X=X^{l}+X^{r}   is an arbitrary partition of   X   X   X   , there exists a partition     Y  l   +   Y  r        superscript  Y  l    superscript  Y  r     Y^{l}+Y^{r}   of   Y   Y   Y   such that     NW   (  X  ,  Y  )    =    NW   (   X  l   ,   Y  l   )    +   NW   (   X  r   ,   Y  r   )          NW  X  Y      NW   superscript  X  l    superscript  Y  l     NW   superscript  X  r    superscript  Y  r       \operatorname{NW}(X,Y)=\operatorname{NW}(X^{l},Y^{l})+\operatorname{NW}(X^{r},%
 Y^{r})   .   Algorithm description      X  i     subscript  X  i    X_{i}   denotes the i-th character of   X   X   X   , where    1  <  i  ⩽   length   (  X  )          1  i        length  X      1   .    X   i  :  j      subscript  X   normal-:  i  j     X_{i:j}   denotes a substring of size     j  -  i   +  1        j  i   1    j-i+1   , ranging from i-th to the j-th character of   X   X   X   .    rev   (  X  )      rev  X    \operatorname{rev}(X)   is the reversed version of   X   X   X   .     X   X   X   and   Y   Y   Y   are sequences to be aligned. Let   x   x   x   be a character from   X   X   X   , and   y   y   y   be a character from   Y   Y   Y   . We assume that    Del   (  x  )      Del  x    \operatorname{Del}(x)   ,    Ins   (  y  )      Ins  y    \operatorname{Ins}(y)   and    Sub   (  x  ,  y  )      Sub  x  y    \operatorname{Sub}(x,y)   are well defined integer-valued functions. These functions represent the cost of deleting   x   x   x   , inserting   y   y   y   , and replacing   x   x   x   with   y   y   y   , respectively.  We define    NWScore   (  X  ,  Y  )      NWScore  X  Y    \operatorname{NWScore}(X,Y)   , which returns the last line of the Needleman-Wunsch score matrix    Score   (  i  ,  j  )       Score   i  j     \mathrm{Score}(i,j)   :   function NWScore(X,Y)  Score(0,0) = 0   for j=1 to length(Y)  Score(0,j) = Score(0,j-1) + Ins(Y j )   for i=1 to length(X)  Score(i,0) = Score(i-1,0) + Del(X i )   for j=1 to length(Y)  scoreSub = Score(i-1,j-1) + Sub(X i , Y j )  scoreDel = Score(i-1,j) + Del(X i )  scoreIns = Score(i,j-1) + Ins(Y j )  Score(i,j) = max(scoreSub, scoreDel, scoreIns)   end   end   for j=0 to length(Y)  LastLine(j) = Score(length(X),j)   return LastLine  Note that at any point,   NWScore   NWScore   \operatorname{NWScore}   only requires the two most recent rows of the score matrix. Thus,   NWScore   NWScore   \operatorname{NWScore}   can be implemented in    O   (   min   {   length   (  X  )    ,   length   (  Y  )    }    )       O   min   length  X    length  Y      O(\operatorname{min}\{\operatorname{length}(X),\operatorname{length}(Y)\})   space.  The Hirschberg algorithm follows:   function Hirschberg(X,Y)  Z = ""  W = ""   if length(X) == 0   for i=1 to length(Y)  Z = Z + '-'  W = W + Y i   end   else  if length(Y) == 0   for i=1 to length(X)  Z = Z + X i  W = W + '-'   end   else  if length(X) == 1 or length(Y) == 1  (Z,W) = NeedlemanWunsch(X,Y)   else  xlen = length(X)  xmid = length(X)/2  ylen = length(Y)    ScoreL = NWScore(X 1:xmid , Y)  ScoreR = NWScore(rev(X xmid+1:xlen ), rev(Y))  ymid = PartitionY(ScoreL, ScoreR)    (Z,W) = Hirschberg(X 1:xmid , y 1:ymid ) + Hirschberg(X xmid+1:xlen , Y ymid+1:ylen )   end   return (Z,W)  In the context of Observation (2), assume that     X  l   +   X  r        superscript  X  l    superscript  X  r     X^{l}+X^{r}   is a partition of   X   X   X   . Function   PartitionY   PartitionY   \mathrm{PartitionY}   returns index   ymid   ymid   \mathrm{ymid}   such that     Y  l   =   Y   1  :  ymid         superscript  Y  l    subscript  Y   normal-:  1  ymid      Y^{l}=Y_{1:\mathrm{ymid}}   and     Y  r   =   Y    ymid  +  1   :   length   (  Y  )           superscript  Y  r    subscript  Y   normal-:    ymid  1    length  Y       Y^{r}=Y_{\mathrm{ymid}+1:\operatorname{length}(Y)}   .   PartitionY   PartitionY   \mathrm{PartitionY}   is given by   function PartitionY(ScoreL, ScoreR)   return  arg  max ScoreL + rev(ScoreR)  Example  Let        X       =  AGTACGCA   ,       Y       =  TATGC   ,        Del   (  x  )         =   -  2    ,        Ins   (  y  )         =   -  2    ,        Sub   (  x  ,  y  )        =   {       +  2   ,       if  x   =  y         -  1   ,        if  x   ≠  y   .              X    absent  AGTACGCA     Y    absent  TATGC      Del  x     absent    2       Ins  y     absent    2       Sub  x  y     absent   cases    2       if  x   y     1       if  x   y        \begin{aligned}\displaystyle X&\displaystyle=\mathrm{AGTACGCA},\\
 \displaystyle Y&\displaystyle=\mathrm{TATGC},\\
 \displaystyle\operatorname{Del}(x)&\displaystyle=-2,\\
 \displaystyle\operatorname{Ins}(y)&\displaystyle=-2,\\
 \displaystyle\operatorname{Sub}(x,y)&\displaystyle=\begin{cases}+2,&\mbox{if }%
 x=y\\
 -1,&\mbox{if }x\neq y.\end{cases}\end{aligned}     The optimal alignment is given by  W = AGTACGCA  Z = --TATGC-  Indeed, this can be verified by backtracking its corresponding Needleman-Wunsch matrix:   T  A  T  G  C   0 -2  -4  -6  -8 -10   A  -2 -1   0  -2  -4  -6   G  -4 -3  -2  -1   0  -2   T -6 -2 -4   0  -2  -1   A -8  -4 0 -2  -1  -3   C -10  -6  -2 -1 -3   1   G -12  -8  -4  -3 1 -1   C -14 -10  -6  -5  -1 3   A -16 -12  -8  -7  -3 1  One starts with the top level call to    Hirschberg   (  AGTACGCA  ,  TATGC  )      Hirschberg  AGTACGCA  TATGC    \operatorname{Hirschberg}(\mathrm{AGTACGCA},\mathrm{TATGC})   . The call to    NWScore   (  AGTA  ,  Y  )      NWScore  AGTA  Y    \operatorname{NWScore}(\mathrm{AGTA},Y)   produces the following matrix:   T  A  T  G  C  0  -2  -4  -6  -8 -10   A -2  -1   0  -2  -4  -6   G -4  -3  -2  -1   0  -2   T -6  -2  -4   0  -2  -1   A -8  -4   0  -2  -1  -3  Likewise,    NWScore   (   rev   (  CGCA  )    ,   rev   (  Y  )    )      NWScore   rev  CGCA    rev  Y     \operatorname{NWScore}(\operatorname{rev}(\mathrm{CGCA}),\operatorname{rev}(Y))   generates the following matrix:   C  G  T  A  T  0 -2  -4  -6  -8 -10   A -2 -1  -3  -5  -4  -6   C -4  0  -2  -4  -6  -5   G -6 -2   2   0  -2  -4   C -8 -4   0   1  -1  -3  Their last lines are respectively  ScoreL = [ -8 -4  0 -2 -1 -3 ]  ScoreR = [ -8 -4  0  1 -1 -3 ]  PartitionY(ScoreL, ScoreR) = 2 , such that    X  =   AGTA  +  CGCA       X    AGTA  CGCA     X=\mathrm{AGTA}+\mathrm{CGCA}   and    Y  =   TA  +  TGC       Y    TA  TGC     Y=\mathrm{TA}+\mathrm{TGC}   .  The entire Hirschberg recursion (which we omit for brevity) produces the following tree:  (AGTACGCA,TATGC)  /              \  (AGTA,TA)            (CGCA,TGC)  /     \              /      \  (AG,)   (TA,TA)      (CG,TG)  (CA,C)  /   \        /   \  (T,T) (A,A)  (C,T) (G,G)  The leaves of the tree contain the optimal alignment.  See also   Needleman-Wunsch algorithm  Smith Waterman algorithm  Levenshtein distance  Longest Common Subsequence   References  "  Category:Sequence alignment algorithms  Category:Bioinformatics algorithms  Category:Articles with example pseudocode  Category:Dynamic programming     Hirschberg's algorithm ↩  http://www.cs.tau.ac.il/~rshamir/algmb/98/scribe/html/lec02/node10.html ↩  ↩     