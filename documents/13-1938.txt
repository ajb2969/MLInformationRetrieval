   Efficiency (statistics)      Efficiency (statistics)   In statistics , efficiency is a term used in the comparison of various statistical procedures and, in particular, it refers to a measure of the optimality of an estimator , of an experimental design , or of a hypothesis testing procedure. 1 Essentially, a more efficient estimator, experiment or test needs fewer observations than a less efficient one to achieve a given performance. This article primarily deals with efficiency of estimators.  The relative efficiency of two procedures is the ratio of their efficiencies, although often this term is used where the comparison is made between a given procedure and a notional "best possible" procedure. The efficiencies and the relative efficiency of two procedures theoretically depend on the sample size available for the given procedure, but it is often possible to use the asymptotic relative efficiency (defined as the limit of the relative efficiencies as the sample size grows) as the principal comparison measure.  Efficiencies are often defined using the variance or mean square error as the measure of desirability.  Estimators  The efficiency of an unbiased  estimator , T , of a parameter  Œ∏ is defined as 2       e   (  T  )    =     1  /  ‚Ñê    (  Œ∏  )     var   (  T  )           e  T         1  ‚Ñê   Œ∏     var  T      e(T)=\frac{1/\mathcal{I}(\theta)}{\mathrm{var}(T)}     where    ‚Ñê   (  Œ∏  )       ‚Ñê  Œ∏    \mathcal{I}(\theta)   is the Fisher information of the sample. Thus e ( T ) is the minimum possible variance for an unbiased estimator divided by its actual variance. The Cram√©r‚ÄìRao bound can be used to prove that e ( T )¬†‚â§¬†1.  Efficient estimators  If an unbiased  estimator of a parameter Œ∏ attains     e   (  T  )    =  1        e  T   1    e(T)=1   for all values of the parameter, then the estimator is called efficient.  Equivalently, the estimator achieves equality in the Cram√©r‚ÄìRao inequality for all Œ∏ .  An efficient estimator is also the minimum variance unbiased estimator (MVUE). This is because an efficient estimator maintains equality on the Cram√©r‚ÄìRao inequality for all parameter values, which means it attains the minimum variance for all parameters (the definition of the MVUE). The MVUE estimator, even if it exists, is not necessarily efficient, because "minimum" does not mean equality holds on the Cram√©r‚ÄìRao inequality.  Thus an efficient estimator need not exist, but if it does, it is the MVUE.  Asymptotic efficiency  For some estimators , they can attain efficiency asymptotically and are thus called asymptotically efficient estimators. This can be the case for some maximum likelihood estimators or for any estimators that attain equality of the Cram√©r‚ÄìRao bound asymptotically.  Example  Consider a sample of size   N   N   N   drawn from a normal distribution of mean   Œº   Œº   \mu   and unit variance , i.e.,      X  n   ‚àº   ùí©   (  Œº  ,  1  )     .     similar-to   subscript  X  n     ùí©   Œº  1      X_{n}\sim\mathcal{N}(\mu,1).     The sample mean ,    X  ¬Ø     normal-¬Ø  X    \overline{X}   , of the sample     X  1   ,   X  2   ,  ‚Ä¶  ,   X  N       subscript  X  1    subscript  X  2   normal-‚Ä¶   subscript  X  N     X_{1},X_{2},\ldots,X_{N}   , defined as        X  ¬Ø   =    1  N     ‚àë   n  =  1   N    X  n     ‚àº   ùí©   (  Œº  ,   1  N   )     .         normal-¬Ø  X       1  N     superscript   subscript     n  1    N    subscript  X  n       similar-to      ùí©   Œº    1  N        \overline{X}=\frac{1}{N}\sum_{n=1}^{N}X_{n}\sim\mathcal{N}\left(\mu,\frac{1}{N%
 }\right).     The variance of the mean, 1/ N (the square of the standard error ) is equal to the reciprocal of the Fisher information from the sample and thus, by the Cram√©r‚ÄìRao inequality , the sample mean is efficient in the sense that its efficiency is unity (100%).  Now consider the sample median ,    X  ~     normal-~  X    \widetilde{X}   . This is an unbiased and consistent estimator for   Œº   Œº   \mu   . For large   N   N   N   the sample median is approximately normally distributed with mean   Œº   Œº   \mu   and variance      œÄ  /  2   N   ,        œÄ  2   N    {\pi}/{2N},   i.e., 3        X  ~   ‚àº   ùí©   (  Œº  ,   œÄ   2  N    )     .     similar-to   normal-~  X     ùí©   Œº    œÄ    2  N        \widetilde{X}\sim\mathcal{N}\left(\mu,\frac{\pi}{2N}\right).   The efficiency for large   N   N   N   is thus        e   (   X  ~   )    =    (   1  N   )     (   œÄ   2  N    )    -  1     =   2  /  œÄ   ‚âà   64  %    .          e   normal-~  X        1  N    superscript    œÄ    2  N      1            2  œÄ         percent  64      e\left(\widetilde{X}\right)=\left(\frac{1}{N}\right)\left(\frac{\pi}{2N}\right%
 )^{-1}=2/\pi\approx 64\%.   Note that this is the asymptotic efficiency ‚Äî that is, the efficiency in the limit as sample size   N   N   N   tends to infinity. For finite values of    N  ,    N   N,   the efficiency is higher than this (for example, a sample size of 3 gives an efficiency of about 74%).  The sample mean is thus more efficient than the sample median in this example. However, there may be measures by which the median performs better. For example, the median is far more robust to outliers , so that if the Gaussian model is questionable or approximate, there may advantages to using the median (see Robust statistics ).  Dominant estimators  If    T  1     subscript  T  1    T_{1}   and    T  2     subscript  T  2    T_{2}   are estimators for the parameter   Œ∏   Œ∏   \theta   , then    T  1     subscript  T  1    T_{1}   is said to dominate     T  2     subscript  T  2    T_{2}   if:   its mean squared error (MSE) is smaller for at least some value of   Œ∏   Œ∏   \theta     the MSE does not exceed that of    T  2     subscript  T  2    T_{2}   for any value of Œ∏.   Formally,    T  1     subscript  T  1    T_{1}   dominates    T  2     subscript  T  2    T_{2}   if       E   [    (    T  1   -  Œ∏   )   2   ]    ‚â§   E   [    (    T  2   -  Œ∏   )   2   ]          normal-E   delimited-[]   superscript     subscript  T  1   Œ∏   2       normal-E   delimited-[]   superscript     subscript  T  2   Œ∏   2       \mathrm{E}\left[(T_{1}-\theta)^{2}\right]\leq\mathrm{E}\left[(T_{2}-\theta)^{2%
 }\right]     holds for all   Œ∏   Œ∏   \theta   , with strict inequality holding somewhere.  Relative efficiency  The relative efficiency of two estimators is defined as       e   (   T  1   ,   T  2   )    =    E   [    (    T  2   -  Œ∏   )   2   ]     E   [    (    T  1   -  Œ∏   )   2   ]           e    subscript  T  1    subscript  T  2         normal-E   delimited-[]   superscript     subscript  T  2   Œ∏   2       normal-E   delimited-[]   superscript     subscript  T  1   Œ∏   2        e(T_{1},T_{2})=\frac{\mathrm{E}\left[(T_{2}-\theta)^{2}\right]}{\mathrm{E}%
 \left[(T_{1}-\theta)^{2}\right]}     Although   e   e   e   is in general a function of   Œ∏   Œ∏   \theta   , in many cases the dependence drops out; if this is so,   e   e   e   being greater than one would indicate that    T  1     subscript  T  1    T_{1}   is preferable, whatever the true value of   Œ∏   Œ∏   \theta   .  An alternative to relative efficiency for comparing estimators, is the Pitman closeness criterion . This replaces the comparison of mean-squared-errors with comparing how often one estimator produces estimates closer to the true value than another estimator.  Estimators of u.i.d. variables  In the case that we are estimating the mean of uncorrelated, identically distributed variables we can take advantage of the fact that the variance of the sum is the sum of the variance . In this case Efficiency can be defined as the square of the Coefficient of variation , i.e., 4      E  ‚â°    (   œÉ  Œº   )   2       normal-E   superscript    œÉ  Œº   2     \mathrm{E}\equiv\left(\frac{\sigma}{\mu}\right)^{2}     Relative efficiency of two such estimators can thus be interpreted as the relative sample size of one required to achieve the certainty of the other. Proof:        E  1    E  2    =    s  1  2    s  2  2           subscript  normal-E  1    subscript  normal-E  2       superscript   subscript  s  1   2    superscript   subscript  s  2   2      \frac{\mathrm{E}_{1}}{\mathrm{E}_{2}}=\frac{s_{1}^{2}}{s_{2}^{2}}   . Now Because      s  1  2   =    n  1    œÉ  2     ,    s  2  2   =    n  2    œÉ  2        formulae-sequence     superscript   subscript  s  1   2      subscript  n  1    superscript  œÉ  2        superscript   subscript  s  2   2      subscript  n  2    superscript  œÉ  2       s_{1}^{2}=n_{1}\sigma^{2},\,s_{2}^{2}=n_{2}\sigma^{2}   we have      E  1    E  2    =    n  1    n  2           subscript  normal-E  1    subscript  normal-E  2       subscript  n  1    subscript  n  2      \frac{\mathrm{E}_{1}}{\mathrm{E}_{2}}=\frac{n_{1}}{n_{2}}   so the relative efficiency expresses the relative sample size of the first estimator needed to match the variance of the second.  Robustness  Efficiency of an estimator may change significantly if the distribution changes, often dropping. This is one of the motivations of robust statistics ‚Äì an estimator such as the sample mean is an efficient estimator of the population mean of a normal distribution, for example, but can be an inefficient estimator of a mixture distribution of two normal distributions with the same mean and different variances. For example, if a distribution is a combination of 98% N ( Œº,  œÉ ) and 2% N ( Œº, 10 œÉ ), the presence of extreme values from the latter distribution (often "contaminating outliers") significantly reduces the efficiency of the sample mean as an estimator of Œº. By contrast, the trimmed mean is less efficient for a normal distribution, but is more robust (less affected) by changes in distribution, and thus may be more efficient for a mixture distribution. Similarly, the shape of a distribution, such as skewness or heavy tails, can significantly reduce the efficiency of estimators that assume a symmetric distribution or thin tails.  Uses of inefficient estimators  While efficiency is a desirable quality of an estimator, it must be weighed against other desiderata, and an estimator that is efficient for certain distributions may well be inefficient for other distributions. Most significantly, estimators that are efficient for clean data from a simple distribution, such as the normal distribution (which is symmetric, unimodal, and has thin tails) may not be robust to contamination by outliers, and may be inefficient for more complicated distributions. In robust statistics , more importance is placed on robustness and applicability to a wide variety of distributions, rather than efficiency on a single distribution. M-estimators are a general class of solutions motivated by these concerns, yielding both robustness and high relative efficiency, though possibly lower efficiency than traditional estimators for some cases. These are potentially very computationally complicated, however.  A more traditional alternative are L-estimators , which are very simple statistics that are easy to compute and interpret, in many cases robust, and often sufficiently efficient for initial estimates. See applications of L-estimators for further discussion.  Hypothesis tests  For comparing significance tests , a meaningful measure of efficiency can be defined based on the sample size required for the test to achieve a given power .  Pitman efficiency 5 and Bahadur efficiency (or Hodges‚ÄìLehmann efficiency ) 6 7 relate to the comparison of the performance of statistical hypothesis testing procedures. The Encyclopedia of Mathematics provides a brief exposition of these three criteria.  Experimental design  For experimental designs, efficiency relates to the ability of a design to achieve the objective of the study with minimal expenditure of resources such as time and money. In simple cases, the relative efficiency of designs can be expressed as the ratio of the sample sizes required to achieve a given objective. 8  See optimal design for further discussion.  Notes  References       de:Effizienz (Statistik)  es:Eficiencia (estad√≠stica)  it:Efficienza (statistica)  pt:Efici√™ncia (estat√≠stica) "  Category:Statistical theory  Category:Statistical terminology     ‚Ü©  ‚Ü©  Williams, D. (2001) Weighing the Odds , CUP. ISBN 052100618X (p.165) ‚Ü©  ‚Ü©  ‚Ü©  Arcones M.A. "Bahadur efficiency of the likelihood ratio test" preprint ‚Ü©  Canay I.A. & Otsu, T. "Hodges-Lehmann Optimality for Testing Moment Condition Models" ‚Ü©  Dodge, Y. (2006) The Oxford Dictionary of Statistical Terms , OUP. ISBN 0-19-920613-9 ‚Ü©     