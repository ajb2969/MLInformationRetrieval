   Chebyshev's inequality      Chebyshev's inequality   In probability theory , Chebyshev's inequality (also spelled as Tchebysheff's inequality , ) guarantees that in any probability distribution , "nearly all" values are close to the mean — the precise statement being that no more than 1/ k 2 of the distribution's values can be more than k  standard deviations away from the mean (or equivalently, at least 1−1/ k 2 of the distribution's values are within k standard deviations of the mean). The rule is often called Chebyshev's theorem, about the range of standard deviations around the mean, in statistics. The inequality has great utility because it can be applied to completely arbitrary distributions (unknown except for mean and variance). For example, it can be used to prove the weak law of large numbers .  In practical usage, in contrast to the 68-95-99.7% rule , which applies to normal distributions , under Chebyshev's inequality a minimum of just 75% of values must lie within two standard deviations of the mean and 89% within three standard deviations. 1 2  The term Chebyshev's inequality may also refer to the Markov's inequality , especially in the context of analysis.  History  The theorem is named after Russian mathematician Pafnuty Chebyshev , although it was first formulated by his friend and colleague Irénée-Jules Bienaymé . 3 The theorem was first stated without proof by Bienaymé in 1853 4 and later proved by Chebyshev in 1867. 5 His student Andrey Markov provided another proof in his 1884 Ph.D. thesis. 6  Statement  Chebyshev's inequality is usually stated for random variables , but can be generalized to a statement about measure spaces .  Probabilistic statement  Let X (integrable) be a random variable with finite expected value  μ and finite non-zero variance  σ 2 . Then for any real number ,        Pr   (    |   X  -  μ   |   ≥   k  σ    )    ≤   1   k  2     .       Pr        X  μ      k  σ       1   superscript  k  2      \Pr(|X-\mu|\geq k\sigma)\leq\frac{1}{k^{2}}.   Only the case  is useful. When  the right hand     1   k  2    ≥  1.        1   superscript  k  2    1.    \frac{1}{k^{2}}\geq 1.   and the inequality is trivial as all probabilities are ≤ 1.  As an example, using  shows that the probability that values lie outside the interval  does not exceed 1/2.  Because it can be applied to completely arbitrary distributions (unknown except for mean and variance), the inequality generally gives a poor bound compared to what might be deduced if more aspects are known about the distribution involved.      k   Min % within k standard deviations of mean   Max % beyond k standard deviations from mean       1   | 0%   | 100%         2      2    \sqrt{2}      | 50%   | 50%     1.5   | 55.56%   | 44.44%     2   | 75%   | 25%     3   | 88.8889%   | 11.1111%     4   | 93.75%   | 6.25%     5   | 96%   | 4%     6   | 97.2222%   | 2.7778%     7   | 97.9592%   | 2.0408%     8   | 98.4375%   | 1.5625%     9   | 98.7654%   | 1.2346%     10   | 99%   | 1%     Measure-theoretic statement  Let ( X , Σ, μ) be a measure space , and let f be an extended real -valued measurable function defined on X . Then for any real number t > 0 and ''0        μ   (   {   x  ∈   X    :    |   f   (  x  )    |   ≥  t   }   )    ≤    1   t  p      ∫    |  f  |   >  t        |  f  |   p    d  μ      .        μ   conditional-set    x  X         f  x    t         1   superscript  t  p      subscript       f   t       superscript    f   p   d  μ       \mu(\{x\in X\,:\,\,|f(x)|\geq t\})\leq{1\over t^{p}}\int_{|f|>t}|f|^{p}\,d\mu.     More generally, if g is an extended real-valued measurable function, nonnegative and nondecreasing on the range of f , then        μ   (   {   x  ∈   X    :    f   (  x  )    ≥  t   }   )    ≤    1   g   (  t  )       ∫  X     g  ∘   f    d  μ      .        μ   conditional-set    x  X       f  x   t         1    g  t      subscript   X       g  f   d  μ       \mu(\{x\in X\,:\,\,f(x)\geq t\})\leq{1\over g(t)}\int_{X}g\circ f\,d\mu.     The previous statement then follows by defining    g   (  t  )       g  t    g(t)   as    t  2     superscript  t  2    t^{2}   if    t  ≥  0      t  0    t\geq 0   and   0   0    otherwise, and taking    |  f  |      f    |f|   instead of   f   f   f   .  Example  Suppose we randomly select a journal article from a source with an average of 1000 words per article, with a standard deviation of 200 words. We can then infer that the probability that it has between 600 and 1400 words (i.e. within k = 2 SDs of the mean) must be more than 75%, because there is less than  chance to be outside that range, by Chebyshev's inequality. But if we additionally know that the distribution is normal, we can say that is a 75% chance the word count is between 770 and 1230 (which is an even tighter bound).  Sharpness of bounds  As shown in the example above, the theorem typically provides rather loose bounds. However, these bounds cannot in general (remaining true for arbitrary distributions) be improved upon. The bounds are sharp for the following example: for any k ≥ 1,      X  =   {       -  1   ,      with probability    1   2   k  2            0  ,       with probability  1   -    1   k  2           1  ,      with probability    1   2   k  2               X   cases    1     with probability    1    2   superscript  k  2      0      with probability  1     1   superscript  k  2     1    with probability    1    2   superscript  k  2         X=\begin{cases}-1,&\text{with probability }\frac{1}{2k^{2}}\\
 0,&\text{with probability }1-\frac{1}{k^{2}}\\
 1,&\text{with probability }\frac{1}{2k^{2}}\end{cases}     For this distribution, mean μ = 0 and standard deviation σ = , so        Pr   (    |   X  -  μ   |   ≥   k  σ    )    =   Pr   (    |  Z  |   ≥  1   )    =   1   k  2     .         Pr        X  μ      k  σ      Pr      Z   1           1   superscript  k  2       \Pr(|X-\mu|\geq k\sigma)=\Pr(|Z|\geq 1)=\frac{1}{k^{2}}.   Chebyshev's inequality is an equality for precisely those distributions that are a linear transformation of this example.  Proof (of the two-sided version)  Probabilistic proof  Markov's inequality states that for any real-valued random variable Y and any positive number a , we have Pr(| Y | > a ) ≤ E(| Y |)/ a . One way to prove Chebyshev's inequality is to apply Markov's inequality to the random variable ( X − μ ) 2 }} with a = (σ k ) 2 .  It can also be proved directly. For any event A , let I A be the indicator random variable of A , i.e. I A equals 1 if A occurs and 0 otherwise. Then      Pr   (    |   X  -  μ   |   ≥   k  σ    )      Pr        X  μ      k  σ      \displaystyle\Pr(|X-\mu|\geq k\sigma)     The direct proof shows why the bounds are quite loose in typical cases: the number 1 to the right of "≥" is replaced by [( X − μ)/( k σ)] 2 to the left of "≥" whenever the latter exceeds 1. In some cases it exceeds 1 by a very wide margin.  Measure-theoretic proof  Fix   t   t   t   and let    A  t     subscript  A  t    A_{t}   be defined as     A  t   =   {   x  ∈  X   ∣    f   (  x  )    ≥  t   }        subscript  A  t    conditional-set    x  X       f  x   t      A_{t}=\{x\in X\mid f(x)\geq t\}   , and let    1   A  t      subscript  1   subscript  A  t     1_{A_{t}}   be the indicator function of the set    A  t     subscript  A  t    A_{t}   . Then, it is easy to check that, for any   x   x   x   ,       0  ≤   g   (  t  )    1   A  t     ≤   g   (   f   (  x  )    )    1   A  t      ,        0    g  t   subscript  1   subscript  A  t            g    f  x    subscript  1   subscript  A  t        0\leq g(t)1_{A_{t}}\leq g(f(x))\,1_{A_{t}},     since g is nondecreasing on the range of f , and therefore,      g   (  t  )   μ   (   A  t   )       g  t  μ   subscript  A  t     \displaystyle g(t)\mu(A_{t})     The desired inequality follows from dividing the above inequality by g ( t ).  Extensions  Several extensions of Chebyshev's inequality have been developed.  Asymmetric two-sided case  An asymmetric two-sided version of this inequality is also known. 7  When the distribution is known to be symmetric for any      k  1   +   k  2    =  0         subscript  k  1    subscript  k  2    0    k_{1}+k_{2}=0          Pr   (    k  1   <  X  <   k  2    )    ≥   1  -    4   σ  2      (    k  2   -   k  1    )   2          Pr       subscript  k  1   X        subscript  k  2        1      4   superscript  σ  2     superscript     subscript  k  2    subscript  k  1    2       \Pr(k_{1}   where σ 2 is the variance .  Similarly when the distribution is asymmetric or is unknown and      k  1   +   k  2    =   2  μ          subscript  k  1    subscript  k  2      2  μ     k_{1}+k_{2}=2\mu           Pr   (    k  1   <  X  <   k  2    )    ≥    4   [     (   μ  -   k  1    )    (    k  2   -  μ   )    -   σ  2    ]      (    k  2   -   k  1    )   2     ,       Pr       subscript  k  1   X        subscript  k  2          4   delimited-[]        μ   subscript  k  1       subscript  k  2   μ     superscript  σ  2       superscript     subscript  k  2    subscript  k  1    2      \Pr(k_{1}     where is the variance and   μ   μ   μ   is the mean .  Bivariate case  A version for the bivariate case is known. 8  Let be two random variables with means and finite variances respectively. Then       Pr   (    k  11   ≤   X  1   ≤   k  12    ,    k  21   ≤   X  2   ≤   k  22    )    ≥   1  -   ∑   T  i          Pr       subscript  k  11    subscript  X  1         subscript  k  12          subscript  k  21    subscript  X  2         subscript  k  22        1     subscript  T  i       \Pr(k_{11}\leq X_{1}\leq k_{12},k_{21}\leq X_{2}\leq k_{22})\geq 1-\sum T_{i}     where for i = 1, 2,        T  i   =     4   σ  i  2    +    [    2   μ  i    -   (    k   i  1    +   k   i  2     )    ]   2      (    k   i  2    -   k   i  1     )   2     .       subscript  T  i         4   superscript   subscript  σ  i   2     superscript   delimited-[]      2   subscript  μ  i       subscript  k    i  1     subscript  k    i  2       2     superscript     subscript  k    i  2     subscript  k    i  1     2      T_{i}=\frac{4\sigma_{i}^{2}+[2\mu_{i}-(k_{i1}+k_{i2})]^{2}}{(k_{i2}-k_{i1})^{2%
 }}.     Two correlated variables  Berge derived an inequality for two correlated variables . 9 Let   ρ   ρ   ρ   be the correlation coefficient between X 1 and X 2 and let σ i 2 be the variance of . Then      Pr   (   ⋂   i  =  1   2    [    |    X  i   -   μ  i    |    σ  i    <  k  ]   )   ≥  1  -    1  +    1  -   ρ  2       k  2    .     fragments  Pr   fragments  normal-(   superscript   subscript     i  1    2    fragments  normal-[         subscript  X  i    subscript  μ  i      subscript  σ  i     k  normal-]   normal-)    1       1      1   superscript  ρ  2       superscript  k  2    normal-.    \Pr\left(\bigcap_{i=1}^{2}\left[\frac{|X_{i}-\mu_{i}|}{\sigma_{i}}     Lal later obtained an alternative bound 10      Pr   (   ⋂   i  =  1   2    [    |    X  i   -   μ  i    |    σ  i    ≤   k  i   ]   )   ≥  1  -     k  1  2   +   k  2  2   +      (    k  1  2   +   k  2  2    )   2   -   4   k  1  2    k  2  2   ρ       2    (    k  1    k  2    )   2        fragments  Pr   fragments  normal-(   superscript   subscript     i  1    2    fragments  normal-[         subscript  X  i    subscript  μ  i      subscript  σ  i      subscript  k  i   normal-]   normal-)    1        superscript   subscript  k  1   2    superscript   subscript  k  2   2        superscript     superscript   subscript  k  1   2    superscript   subscript  k  2   2    2     4   superscript   subscript  k  1   2    superscript   subscript  k  2   2   ρ        2   superscript     subscript  k  1    subscript  k  2    2       \Pr\left(\bigcap_{i=1}^{2}\left[\frac{|X_{i}-\mu_{i}|}{\sigma_{i}}\leq k_{i}%
 \right]\right)\geq 1-\frac{k_{1}^{2}+k_{2}^{2}+\sqrt{(k_{1}^{2}+k_{2}^{2})^{2}%
 -4k_{1}^{2}k_{2}^{2}\rho}}{2(k_{1}k_{2})^{2}}     Isii derived a further generalisation. 11 Let      Z  =  Pr   (   (  -   k  1   <   X  1   <   k  2   )   ∩   (  -   k  1   <   X  2   <   k  2   )   )   ,  0  <   k  1   ≤   k  2   .     fragments  Z   Pr   fragments  normal-(   fragments  normal-(    subscript  k  1     subscript  X  1     subscript  k  2   normal-)     fragments  normal-(    subscript  k  1     subscript  X  2     subscript  k  2   normal-)   normal-)   normal-,  0    subscript  k  1     subscript  k  2   normal-.    Z=\Pr\left(\left(-k_{1}     and define:      λ  =      k  1    (   1  +  ρ   )    +     (   1  -   ρ  2    )    (    k  1  2   +  ρ   )         2   k  1    -  1   +  ρ        λ         subscript  k  1     1  ρ          1   superscript  ρ  2       superscript   subscript  k  1   2   ρ            2   subscript  k  1    1   ρ      \lambda=\frac{k_{1}(1+\rho)+\sqrt{(1-\rho^{2})(k_{1}^{2}+\rho)}}{2k_{1}-1+\rho}     There are now three cases.   Case A: If     2   k  1  2    >   1  -  ρ         2   superscript   subscript  k  1   2      1  ρ     2k_{1}^{2}>1-\rho   and      k  2   -   k  1    ≥   2  λ          subscript  k  2    subscript  k  1      2  λ     k_{2}-k_{1}\geq 2\lambda   then          Z  ≤    2   λ  2      2   λ  2    +  1  +  ρ     .      Z      2   superscript  λ  2        2   superscript  λ  2    1  ρ      Z\leq\frac{2\lambda^{2}}{2\lambda^{2}+1+\rho}.         Case B: If the conditions in case A are not met but and          2    (     k  1    k  2    -  1   )   2    ≥    2   (   1  -   ρ  2    )    +    (   1  -  ρ   )     (    k  2   -   k  1    )   2           2   superscript       subscript  k  1    subscript  k  2    1   2        2    1   superscript  ρ  2         1  ρ    superscript     subscript  k  2    subscript  k  1    2       2(k_{1}k_{2}-1)^{2}\geq 2(1-\rho^{2})+(1-\rho)(k_{2}-k_{1})^{2}       then      Z  ≤      (    k  2   -   k  1    )   2   +  4  +     16   (   1  -   ρ  2    )    +   8   (   1  -  ρ   )    (    k  2   -   k  1    )         (    k  1   +   k  2    )   2     .      Z       superscript     subscript  k  2    subscript  k  1    2   4        16    1   superscript  ρ  2       8    1  ρ      subscript  k  2    subscript  k  1         superscript     subscript  k  1    subscript  k  2    2      Z\leq\frac{(k_{2}-k_{1})^{2}+4+\sqrt{16(1-\rho^{2})+8(1-\rho)(k_{2}-k_{1})}}{(%
 k_{1}+k_{2})^{2}}.        Case C: If none of the conditions in cases A or B are satisfied then there is no universal bound other than 1.   Multivariate case  The general case is known as the Birnbaum–Raymond–Zuckerman inequality after the authors who proved it for two dimensions. 12       Pr   (     ∑   i  =  1   n      (    X  i   -   μ  i    )   2     σ  i  2    t  i  2      ≥   k  2    )    ≤    1   k  2      ∑   i  =  1   n    1   t  i  2           Pr      superscript   subscript     i  1    n      superscript     subscript  X  i    subscript  μ  i    2      superscript   subscript  σ  i   2    superscript   subscript  t  i   2       superscript  k  2         1   superscript  k  2      superscript   subscript     i  1    n     1   superscript   subscript  t  i   2        \Pr\left(\sum_{i=1}^{n}\frac{(X_{i}-\mu_{i})^{2}}{\sigma_{i}^{2}t_{i}^{2}}\geq
 k%
 ^{2}\right)\leq\frac{1}{k^{2}}\sum_{i=1}^{n}\frac{1}{t_{i}^{2}}     where is the   i   i   i   -th random variable, is the   i   i   i   -th mean and σ i 2 is the   i   i   i   -th variance.  If the variables are independent this inequality can be sharpened. 13       Pr   (     ⋂   i  =  1   n     |    X  i   -   μ  i    |    σ  i     ≤   k  i    )    ≥   ∏   (   1  -   1   k  i  2     )         Pr      superscript   subscript     i  1    n          subscript  X  i    subscript  μ  i      subscript  σ  i      subscript  k  i      product    1    1   superscript   subscript  k  i   2        \Pr\left(\bigcap_{i=1}^{n}\frac{|X_{i}-\mu_{i}|}{\sigma_{i}}\leq k_{i}\right)%
 \geq\prod\left(1-\frac{1}{k_{i}^{2}}\right)     Olkin and Pratt derived an inequality for   n   n   n   correlated variables. 14       Pr   (     ⋂   i  =  1   n     |    X  i   -   μ  i    |    σ  i     <   k  i    )    ≥   1  -    1   n  2      (    U   +     n  -  1       n    ∑  i    1   k  i  2      -  u      )   2          Pr      superscript   subscript     i  1    n          subscript  X  i    subscript  μ  i      subscript  σ  i      subscript  k  i       1      1   superscript  n  2     superscript      U         n  1          n    subscript   i     1   superscript   subscript  k  i   2      u      2       \Pr\left(\bigcap_{i=1}^{n}\frac{|X_{i}-\mu_{i}|}{\sigma_{i}}     where the sum is taken over the n variables and      !  !  ⋅  !  !     fragments    ⋅      {{!!}}⋅{{!!}}     A second related inequality has also been derived by Chen. 15 Let   X   X   X   be the dimension of the stochastic vector    E   (  X  )       E  X    E(X)   and let   X   X   X   be the mean of   S   S   S   . Let    k  >  0      k  0    k>0   be the covariance matrix and     Pr   (      (   X  -   E   (  X  )     )   T    S   -  1     (   X  -   E   (  X  )     )    <  k   )    ≥   1  -   N  k         Pr       superscript    X   normal-E  X    T    superscript  S    1      X   normal-E  X     k      1    N  k      \Pr\left((X-\operatorname{E}(X))^{T}S^{-1}(X-\operatorname{E}(X))   . Then     Y   Y   Y     where Y T is the transpose of   X   X   X   .  Infinite Dimensions  There is a straightforward extension of the vector version of Chebyshev's inequality to infinite dimensional settings. Let   𝒳   𝒳   \mathcal{X}   be a random variable which takes values in a Fréchet space    𝒳   𝒳   \mathcal{X}   (equipped with seminorms ⋅ {{!!}} α }} ). This includes most common settings of vector-valued random variables, e.g., when   X   X   X   is a Banach space (equipped with a single norm), a Hilbert space , or the finite-dimensional setting as described above.  Suppose that     E   (    ∥  X  ∥   α  2   )    <  ∞       normal-E   superscript   subscript   norm  X   α   2       \operatorname{E}\left(\|X\|_{\alpha}^{2}\right)<\infty   is of " strong order two ", meaning that     X   X   X     for every seminorm ⋅ {{!!}} α }} . This is a generalization of the requirement that    μ  ∈  𝒳      μ  𝒳    \mu\in\mathcal{X}   have finite variance, and is necessary for this strong form of Chebyshev's inequality in infinite dimensions. The terminology "strong order two" is due to Vakhania . 16  Let   X   X   X   be the Pettis integral of     σ  a   :=    E    ∥   X  -  μ   ∥   α  2        assign   subscript  σ  a       normal-E   superscript   subscript   norm    X  μ    α   2       \sigma_{a}:=\sqrt{\operatorname{E}\|X-\mu\|_{\alpha}^{2}}   (i.e., the vector generalization of the mean), and let      ∀  k  >  0  :  Pr   (  ∥  X  -  μ   ∥  α   ≥  k   σ  α   )   ≤   1   k  2    .     fragments  for-all  k   0  normal-:   Pr   fragments  normal-(  parallel-to  X   μ   subscript  parallel-to  α    k   subscript  σ  α   normal-)      1   superscript  k  2    normal-.    \forall k>0:\quad\Pr\left(\|X-\mu\|_{\alpha}\geq k\sigma_{\alpha}\right)\leq%
 \frac{1}{k^{2}}.     be the standard deviation with respect to the seminorm ⋅ {{!!}} α }} . In this setting we can state the following:   General Version of Chebyshev's Inequality.    X   X   X      Proof. The proof is straightforward, and essentially the same as the finitary version. If 0}} , then   μ   μ   μ   is constant (and equal to      ∥   X  -  μ   ∥   α   ≥   k   σ  α  2         subscript   norm    X  μ    α     k   superscript   subscript  σ  α   2      \|X-\mu\|_{\alpha}\geq k\sigma_{\alpha}^{2}   ) almost surely, so the inequality is trivial.  If      1  =     ∥   X  -  μ   ∥   α  2     ∥   X  -  μ   ∥   α  2        1     superscript   subscript   norm    X  μ    α   2    superscript   subscript   norm    X  μ    α   2      1=\tfrac{\|X-\mu\|_{\alpha}^{2}}{\|X-\mu\|_{\alpha}^{2}}     then X − μ {{!!}} α > 0}} , so we may safely divide by X − μ {{!!}} α }} . The crucial trick in Chebyshev's inequality is to recognize that    Pr   (     ∥   X  -  μ   ∥   α   ≥   k   σ  α     )      Pr     subscript   norm    X  μ    α     k   subscript  σ  α       \displaystyle\Pr\left(\|X-\mu\|_{\alpha}\geq k\sigma_{\alpha}\right)   .  The following calculations complete the proof:        Pr   (    |   X  -   E   (  X  )     |   ≥  k   )    ≤    E   (    |   X  -   E   (  X  )     |   n   )     k  n     ,    k  >  0   ,   n  ≥  2.       formulae-sequence     Pr        X   normal-E  X     k       normal-E   superscript      X   normal-E  X     n     superscript  k  n      formulae-sequence    k  0     n  2.      \Pr(|X-\operatorname{E}(X)|\geq k)\leq\frac{\operatorname{E}(|X-\operatorname{%
 E}(X)|^{n})}{k^{n}},\qquad k>0,n\geq 2.     Higher moments  An extension to higher moments is also possible:        Pr   (   X  ≥  ε   )    ≤    e   -   t  ε      E   (   e   t  X    )      ,   t  >  0.      formulae-sequence     Pr    X  ε       superscript  e      t  ε      normal-E   superscript  e    t  X         t  0.     \Pr(X\geq\varepsilon)\leq e^{-t\varepsilon}\operatorname{E}\left(e^{tX}\right)%
 ,\qquad t>0.     Exponential version  A related inequality sometimes known as the exponential Chebyshev's inequality 17 is the inequality      K   (  x  ,  t  )       K   x  t     K(x,t)     Let      K   (  x  ,  t  )    =   log   (   E   (   e   t  x    )    )     .        K   x  t       normal-E   superscript  e    t  x        K(x,t)=\log\left(\operatorname{E}\left(e^{tx}\right)\right).   be the cumulant generating function ,      K   (  x  ,  t  )       K   x  t     K(x,t)     Taking the Legendre–Fenchel transformation of      -   log   (   Pr   (   X  ≥  ε   )    )     ≤    sup  t    (    t  ε   -   K   (  x  ,  t  )     )     .           Pr    X  ε        subscript  supremum  t       t  ε     K   x  t        -\log(\Pr(X\geq\varepsilon))\leq\sup_{t}(t\varepsilon-K(x,t)).   and using the exponential Chebyshev's inequality we have       a  a   ,  b       a  a   b    aa,b     This inequality may be used to obtain exponential inequalities for unbounded variables. 18  Inequalities for bounded variables  If P( x ) has finite support based on the interval    M  =   m  a  x   (   |  a  |   ,   |  b  |   )        M    m  a  x     a     b       M=max(|a|,|b|)   , let   x   x   x   where | x | is the absolute value of    k  >  0      k  0    k>0   . If the mean of P( x ) is zero then for all        E   (    |  X  |   r   )    -   k  r     M  r    ≤   Pr   (    |  X  |   ≥  k   )    ≤    E   (    |  X  |   r   )     k  r     .             normal-E   superscript    X   r     superscript  k  r     superscript  M  r     Pr      X   k            normal-E   superscript    X   r     superscript  k  r       \frac{\operatorname{E}(|X|^{r})-k^{r}}{M^{r}}\leq\Pr(|X|\geq k)\leq\frac{%
 \operatorname{E}(|X|^{r})}{k^{r}}.    19      r  =  2      r  2    r=2     The second of these inequalities with    0  ≤  X  ≤  M      0  normal-≤  X  normal-≤  M    0≤X≤M   is the Chebyshev bound. The first provides a lower bound for the value of P( x ).  Sharp bounds for a bounded variate have been derived by Niemitalo 20  Let    M  >  0      M  0    M>0   where    P   (  |  X  -  m  |  ≥  k  s  )   ≤     g   N  +  1     (    N   k  2      N  -  1   +   k  2     )     N  +  1      (   N   N  +  1    )    1  /  2       fragments  P   fragments  normal-(  normal-|  X   m  normal-|   k  s  normal-)         subscript  g    N  1        N   superscript  k  2        N  1    superscript  k  2        N  1     superscript   fragments  normal-(    N    N  1    normal-)     1  2      P(|X-m|\geq ks)\leq\frac{g_{N+1}\left(\frac{Nk^{2}}{N-1+k^{2}}\right)}{N+1}%
 \left(\frac{N}{N+1}\right)^{1/2}   . Then   Case 1:           a  2   =    Q   (   Q  -  R   )     1  +   R   (   Q  -  R   )       .       superscript  a  2       Q    Q  R      1    R    Q  R        a^{2}=\frac{Q(Q-R)}{1+R(Q-R)}.         Case 2:           g  Q    (  x  )    =   R   if  R  is even           subscript  g  Q   x    R    if  R  is even      g_{Q}(x)=R\quad\text{if }R\text{ is even}         Case 3:     \Pr(X     Finite samples  Saw et al extended Chebyshev's inequality to cases where the population mean and variance are not known and may not exist, but you want to use the sample mean and sample standard deviation from N samples to bound the expected value of a new drawing from the same distribution. 21         g  Q    (  x  )    =  R     if  R  is odd and  x   <   a  2       formulae-sequence       subscript  g  Q   x   R       if  R  is odd and  x    superscript  a  2      g_{Q}(x)=R\quad\text{if }R\text{ is odd and }x     where X is a random variable which we have sampled N times, m is the sample mean, k is a constant and s is the sample standard deviation. g( x ) is defined as follows:  Let x ≥ 1, Q = N + 1, and R be the greatest integer less than Q / x . Let          g  Q    (  x  )    =   R  -  1      if  R  is odd and  x   ≥   a  2     .     formulae-sequence       subscript  g  Q   x     R  1        if  R  is odd and  x    superscript  a  2      g_{Q}(x)=R-1\quad\text{if }R\text{ is odd and }x\geq a^{2}.     Now      P   (  |  X  -  m  |  ≥  k  s  )   ≤   1    [   N   (   N  +  1   )    ]    1  /  2      [   (    N  -  1    k  2    +  1  )   ]      fragments  P   fragments  normal-(  normal-|  X   m  normal-|   k  s  normal-)      1   superscript   delimited-[]    N    N  1       1  2      fragments  normal-[   fragments  normal-(      N  1    superscript  k  2     1  normal-)   normal-]     P(|X-m|\geq ks)\leq\frac{1}{[N(N+1)]^{1/2}}\left[\left(\frac{N-1}{k^{2}}+1%
 \right)\right]         P   (  |  X  -  m  |  ≥  k  s  )   ≤    N  -  1   N    1   k  2      s  2    m  2    +   1  N   .     fragments  P   fragments  normal-(  normal-|  X   m  normal-|   k  s  normal-)        N  1   N     1   superscript  k  2       superscript  s  2    superscript  m  2       1  N   normal-.    P(|X-m|\geq ks)\leq\frac{N-1}{N}\frac{1}{k^{2}}\frac{s^{2}}{m^{2}}+\frac{1}{N}.         P   (  |  X  -  m  |  ≥  k  s  )   ≤   1   N  +  1    .     fragments  P   fragments  normal-(  normal-|  X   m  normal-|   k  s  normal-)      1    N  1    normal-.    P(|X-m|\geq ks)\leq\frac{1}{N+1}.     This inequality holds even when the population moments do not exist, and when the sample is only weakly exchangeably distributed; this criterion is met for randomised sampling. A table of values for the Saw–Yang–Mo inequality for finite sample sizes ( n The table allows the calculation of various confidence intervals for the mean, based on multiples, C, of the standard error of the mean as calculated from the sample. For example, Konijn shows that for n = 59, the 95 per cent confidence interval for the mean m is ( m - Cs , m + Cs ) where C = 4.447 x 1.006 = 4.47 (this is 2.28 times larger than the value found on the assumption of Normality showing the loss on precision resulting from ignorance of the precise nature of the distribution).  Kabán gives a somewhat less complex version of this inequality. 22      P   (  |  X  -  m  |  ≥  k  s  )   ≤   1    k  2    (   N  +  1   )     .     fragments  P   fragments  normal-(  normal-|  X   m  normal-|   k  s  normal-)      1     superscript  k  2     N  1     normal-.    P(|X-m|\geq ks)\leq\frac{1}{k^{2}(N+1)}.     If the standard deviation is a multiple of the mean then a further inequality can be derived, 23       Z  =    X  -   E   (  X  )      Var    (  X  )    1  /  2       .      Z      X   normal-E  X     fragments  Var   superscript   fragments  normal-(  X  normal-)     1  2        Z=\frac{X-\operatorname{E}(X)}{\operatorname{Var}(X)^{1/2}}.     A table of values for the Saw–Yang–Mo inequality for finite sample sizes ( n   For fixed N and large m the Saw–Yang–Mo inequality is approximately 24      P   (  Z  ≥  k  )   ≤   1   1  +   k  2     .     fragments  P   fragments  normal-(  Z   k  normal-)      1    1   superscript  k  2     normal-.    P(Z\geq k)\leq\frac{1}{1+k^{2}}.     Beasley et al have suggested a modification of this inequality 25      P   (  Z  ≥  k  )   ≤   1   2   k  2     .     fragments  P   fragments  normal-(  Z   k  normal-)      1    2   superscript  k  2     normal-.    P(Z\geq k)\leq\frac{1}{2k^{2}}.     In empirical testing this modification is conservative but appears to have low statistical power. Its theoretical basis currently remains unexplored.  Dependence of sample size  The bounds these inequalities give on a finite sample are less tight than those the Chebyshev inequality gives for a distribution. To illustrate this let the sample size n = 100 and let k = 3. Chebyshev's inequality states that at most approximately 11.11% of the distribution will lie outside these limits. Kabán's version of the inequality for a finite sample states that at most approximately 12.05% of the sample lies outside these limits. The dependence of the confidence intervals on sample size is further illustrated below.  For N = 10, the 95% confidence interval is approximately ±13.5789 standard deviations.  For N = 100 the 95% confidence interval is approximately ±4.9595 standard deviations; the 99% confidence interval is approximately ±140.0 standard deviations.  For N = 500 the 95% confidence interval is approximately ±4.5574 standard deviations; the 99% confidence interval is approximately ±11.1620 standard deviations.  For N = 1000 the 95% and 99% confidence intervals are approximately ±4.5141 and approximately ±10.5330 standard deviations respectively.  The Chebyshev inequality for the distribution gives 95% and 99% confidence intervals of approximately ±4.472 standard deviations and ±10 standard deviations respectively.  Samuelson's inequality  Although Chebyshev's inequality is the best possible bound for an arbitrary distribution, this is not necessarily true for finite samples. Samuelson's inequality states that all values of a sample will lie within √( N − 1) standard deviations of the mean. Chebyshev's bound improves as the sample size increases.  When N = 10, Samuelson's inequality states that all members of the sample lie within 3 standard deviations of the mean: in contrast Chebyshev's states that 99.5% of the sample lies within 13.5789 standard deviations of the mean.  When N = 100, Samuelson's inequality states that all members of the sample lie within approximately 9.9499 standard deviations of the mean: Chebyshev's states that 99% of the sample lies within 10 standard deviations of the mean.  When N = 500, Samuelson's inequality states that all members of the sample lie within approximately 22.3383 standard deviations of the mean: Chebyshev's states that 99% of the sample lies within 10 standard deviations of the mean.  Sharpened bounds  Chebyshev's inequality is important because of its applicability to any distribution. As a result of its generality it may not (and usually does not) provide as sharp a bound as alternative methods that can be used if the distribution of the random variable is known. To improve the sharpness of the bounds provided by Chebyshev's inequality a number of methods have been developed; for a review see eg. 26  Standardised variables  Sharpened bounds can be derived by first standardising the random variable. 27  Let X be a random variable with finite variance Var ( x ). Let Z be the standardised form defined as      P   (  Z  ≤  -  u  or  Z  ≥  v  )   ≤    4  +    (   u  -  v   )   2      (   u  +  v   )   2    .     fragments  P   fragments  normal-(  Z    u  or  Z   v  normal-)        4   superscript    u  v   2     superscript    u  v   2    normal-.    P(Z\leq-u\text{ or }Z\geq v)\leq\frac{4+(u-v)^{2}}{(u+v)^{2}}.     Cantelli's lemma is then       σ  +  2   =    ∑    (   x  -  m   )   2     n  -  1         superscript   subscript  σ    2        superscript    x  m   2      n  1      \sigma_{+}^{2}=\frac{\sum(x-m)^{2}}{n-1}     This inequality is sharp and is attained by k and −1/ k with probability 1/(1 + k 2 ) and k 2 /(1 + k 2 ) respectively.  If k > 1 and the distribution of X is symmetric then we have       σ  -  2   =    ∑    (   m  -  x   )   2     n  -  1         superscript   subscript  σ    2        superscript    m  x   2      n  1      \sigma_{-}^{2}=\frac{\sum(m-x)^{2}}{n-1}     Equality holds if and only if Z = − k , 0 or k with probabilities , and respectively. 28 An extension to a two-sided inequality is also possible.  Let u , v > 0. Then we have 29        σ  2   =    σ  +  2   +   σ  -  2     .       superscript  σ  2      superscript   subscript  σ    2    superscript   subscript  σ    2      \sigma^{2}=\sigma_{+}^{2}+\sigma_{-}^{2}.     Semivariances  An alternative method of obtaining sharper bounds is through the use of semivariances (partial moments). The upper ( σ + 2 ) and lower ( σ − 2 ) semivariances are defined        Pr   (   x  ≤   m  -   a   σ  -      )    ≤   1   a  2     .       Pr    x    m    a   subscript  σ          1   superscript  a  2      \Pr(x\leq m-a\sigma_{-})\leq\frac{1}{a^{2}}.          a  =    k  σ    σ  -     .      a      k  σ    subscript  σ       a=\frac{k\sigma}{\sigma_{-}}.     where m is the arithmetic mean of the sample, n is the number of elements in the sample and the sum for the upper (lower) semivariance is taken over the elements greater (less) than the mean.  The variance of the sample is the sum of the two semivariances        Pr   (   x  ≤   m  -   k  σ     )    ≤    1   k  2      σ  -  2    σ  2      .       Pr    x    m    k  σ          1   superscript  k  2       superscript   subscript  σ    2    superscript  σ  2       \Pr(x\leq m-k\sigma)\leq\frac{1}{k^{2}}\frac{\sigma_{-}^{2}}{\sigma^{2}}.     In terms of the lower semivariance Chebyshev's inequality can be written 30        σ  u  2   =   max   (   σ  -  2   ,   σ  +  2   )     ,       superscript   subscript  σ  u   2      superscript   subscript  σ    2    superscript   subscript  σ    2      \sigma_{u}^{2}=\max(\sigma_{-}^{2},\sigma_{+}^{2}),     Putting      Pr   (  |  x  ≤  m  -  k  σ  |  )   ≤   1   k  2      σ  u  2    σ  2    .     fragments  Pr   fragments  normal-(  normal-|  x   m   k  σ  normal-|  normal-)      1   superscript  k  2       superscript   subscript  σ  u   2    superscript  σ  2    normal-.    \Pr(|x\leq m-k\sigma|)\leq\frac{1}{k^{2}}\frac{\sigma_{u}^{2}}{\sigma^{2}}.     Chebyshev's inequality can now be written       σ  +  2   =   σ  -  2   =    1  2    σ  2           superscript   subscript  σ    2    superscript   subscript  σ    2            1  2    superscript  σ  2       \sigma_{+}^{2}=\sigma_{-}^{2}=\frac{1}{2}\sigma^{2}     A similar result can also be derived for the upper semivariance.  If we put        Pr   (   x  ≤   m  -   k  σ     )    ≤   1   2   k  2      .       Pr    x    m    k  σ        1    2   superscript  k  2       \Pr(x\leq m-k\sigma)\leq\frac{1}{2k^{2}}.     Chebyshev's inequality can be written      Y  =    α  X   +  β       Y      α  X   β     Y=\alpha X+\beta     Because σ u 2 ≤ σ 2 , use of the semivariance sharpens the original inequality.  If the distribution is known to be symmetric, then      α  =    2  k    b  -  a        α      2  k     b  a      \alpha=\frac{2k}{b-a}     and       β  =    -    (   b  +  a   )   k     b  -  a     .      β          b  a   k      b  a      \beta=\frac{-(b+a)k}{b-a}.     This result agrees with that derived using standardised variables.   Note: The inequality with the lower semivariance has been found to be of use in estimating downside risk in finance and agriculture. 31 32 33   Selberg's inequality  Selberg derived an inequality for P ( x ) when a ≤ x ≤ b . 34 To simplify the notation let       μ  Y   =    α   μ  X    +  β        subscript  μ  Y       α   subscript  μ  X    β     \mu_{Y}=\alpha\mu_{X}+\beta     where        σ  Y  2   =    α  2    σ  X  2     .       superscript   subscript  σ  Y   2      superscript  α  2    superscript   subscript  σ  X   2      \sigma_{Y}^{2}=\alpha^{2}\sigma_{X}^{2}.     and      P   (  |  Y  |  <  k  )   ≥     (   k  -   μ  Y    )   2      (   k  -   μ  Y    )   2   +   σ  Y  2     if   σ  Y  2   ≤   μ  Y    (  k  -   μ  Y   )      fragments  P   fragments  normal-(  normal-|  Y  normal-|   k  normal-)       superscript    k   subscript  μ  Y    2      superscript    k   subscript  μ  Y    2    superscript   subscript  σ  Y   2      if    superscript   subscript  σ  Y   2     subscript  μ  Y    fragments  normal-(  k    subscript  μ  Y   normal-)     P(|Y|     The result of this linear transformation is to make P ( a ≤ X ≤ b ) equal to P (| Y | ≤ k ).  The mean ( μ X ) and variance ( σ X ) of X are related to the mean ( μ Y ) and variance ( σ Y ) of Y :      P   (  |  Y  |  <  k  )   ≥  1  -     σ  Y  2   +   μ  Y  2     k  2    if   μ  Y    (  k  -   μ  Y   )   ≤   σ  Y  2   ≤   k  2   -   μ  Y  2      fragments  P   fragments  normal-(  normal-|  Y  normal-|   k  normal-)    1        superscript   subscript  σ  Y   2    superscript   subscript  μ  Y   2     superscript  k  2     if    subscript  μ  Y    fragments  normal-(  k    subscript  μ  Y   normal-)     superscript   subscript  σ  Y   2     superscript  k  2     superscript   subscript  μ  Y   2     P(|Y|         P   (  |  Y  |  <  k  )   ≥  0  if   k  2   -   μ  Y  2   ≤   σ  Y  2   .     fragments  P   fragments  normal-(  normal-|  Y  normal-|   k  normal-)    0   if    superscript  k  2     superscript   subscript  μ  Y   2     superscript   subscript  σ  Y   2   normal-.    P(|Y|     With this notation Selberg's inequality states that      P   (  X  -  μ  ≥  a  )   ≤    σ  2     σ  2   +   a  2        fragments  P   fragments  normal-(  X   μ   a  normal-)       superscript  σ  2      superscript  σ  2    superscript  a  2       P(X-\mu\geq a)\leq\frac{\sigma^{2}}{\sigma^{2}+a^{2}}           Pr   (    X  -  μ   ≥   k  σ    )    ≤   1   1  +   k  2      .       Pr      X  μ     k  σ       1    1   superscript  k  2       \Pr(X-\mu\geq k\sigma)\leq\frac{1}{1+k^{2}}.         X  =  1      X  1    X=1     These are known to be the best possible bounds. 35  Cantelli's inequality  Cantelli's inequality 36 due to Francesco Paolo Cantelli states that for a real random variable ( X ) with mean ( μ ) and variance ( σ 2 )       σ  2    1  +   σ  2         superscript  σ  2     1   superscript  σ  2      \frac{\sigma^{2}}{1+\sigma^{2}}     where a ≥ 0.  This inequality can be used to prove a one tailed variant of Chebyshev's inequality with k > 0 37      X  =   -   σ  2        X     superscript  σ  2      X=-\sigma^{2}     The bound on the one tailed variant is known to be sharp. To see this consider the random variable X that takes the values       1   1  +   σ  2     .      1    1   superscript  σ  2      \frac{1}{1+\sigma^{2}}.   with probability      |   μ  -  ν   |   ≤  σ   .          μ  ν    σ    \left|\mu-\nu\right|\leq\sigma.           Pr   (    X  -  μ   ≥  σ   )    ≤   1  2   ⟹   Pr   (   X  ≥   μ  +  σ    )    ≤   1  2    .         Pr      X  μ   σ      1  2         Pr    X    μ  σ            1  2      \Pr(X-\mu\geq\sigma)\leq\frac{1}{2}\implies\Pr(X\geq\mu+\sigma)\leq\frac{1}{2}.   with probability      Pr   (   X  ≤   μ  -  σ    )    ≤   1  2    .       Pr    X    μ  σ       1  2     \Pr(X\leq\mu-\sigma)\leq\frac{1}{2}.     Then E ( X ) = 0 and E ( X 2 ) = σ 2 and P ( X 2).   An application – distance between the mean and the median   The one-sided variant can be used to prove the proposition that for probability distributions having an expected value and a median , the mean and the median can never differ from each other by more than one standard deviation . To express this in symbols let μ , ν , and σ be respectively the mean, the median, and the standard deviation. Then      P   (  X  >  k  σ  )   ≤    κ  -   γ  2   -  1      (   κ  -   γ  2   -  1   )    (   1  +   k  2    )    +   (    k  2   -   k  γ   -  1   )     .     fragments  P   fragments  normal-(  X   k  σ  normal-)        κ   superscript  γ  2   1         κ   superscript  γ  2   1     1   superscript  k  2        superscript  k  2     k  γ   1     normal-.    P(X>k\sigma)\leq\frac{\kappa-\gamma^{2}-1}{(\kappa-\gamma^{2}-1)(1+k^{2})+(k^{%
 2}-k\gamma-1)}.     There is no need to assume that the variance is finite because this inequality is trivially true if the variance is infinite.  The proof is as follows. Setting k = 1 in the statement for the one-sided inequality gives:        [   X  -   E   (  X  )     ]    2  k    >  0       superscript   delimited-[]    X    E  X       2  k    0    [X-E(X)]^{2k}>0     Changing the sign of X and of μ , we get      E   (    [   X  -   E   (  X  )     ]    2  k    )       E   superscript   delimited-[]    X    E  X       2  k      E([X-E(X)]^{2k})     Thus the median is within one standard deviation of the mean.  A proof using Jensen's inequality also exists .  Bhattacharyya's inequality  Bhattacharyya 38 extended Cantelli's inequality using the third and fourth moments of the distribution.  Let μ = 0 and σ 2 be the variance. Let γ = E ( X 3 ) / σ 3 and κ = E ( X 4 ) / σ 4 .  If k 2 − k γ − 1 > 0 then      P   (  |  X  -  E   (  X  )   |  >  t    [  E    (  X  -  E   (  X  )   )    2  k    ]     1  /  2   k    )   ≤  min   [  1  ,   1   t   2  k     ]   .     fragments  P   fragments  normal-(  normal-|  X   E   fragments  normal-(  X  normal-)   normal-|   t   superscript   fragments  normal-[  E   superscript   fragments  normal-(  X   E   fragments  normal-(  X  normal-)   normal-)     2  k    normal-]       1  2   k    normal-)      fragments  normal-[  1  normal-,    1   superscript  t    2  k     normal-]   normal-.    P(|X-E(X)|>t[E(X-E(X))^{2k}]^{1/2k})\leq\min\left[1,\frac{1}{t^{2k}}\right].     The necessity of k 2 − k γ − 1 > 0 requires that k be reasonably large.  Mitzenmacher and Upfal's inequality  Mitzenmacher and Upfal 39 note that       Pr   (    X  -  μ   ≥   k  σ    )    ≤    [   1  +   k  2   +     (    k  2   -   k   θ  3    -  1   )   2     θ  4   -   θ  3  2   -  1     ]    -  1         Pr      X  μ     k  σ      superscript   delimited-[]    1   superscript  k  2      superscript     superscript  k  2     k   subscript  θ  3    1   2      subscript  θ  4    superscript   subscript  θ  3   2   1        1      \Pr(X-\mu\geq k\sigma)\leq\left[1+k^{2}+\frac{\left(k^{2}-k\theta_{3}-1\right)%
 ^{2}}{\theta_{4}-\theta_{3}^{2}-1}\right]^{-1}     for any real k > 0 and that       k  ≥     θ  3   +     θ  3  2   +  4     2    ,    θ  m   =    M  m    σ  m        formulae-sequence    k       subscript  θ  3        superscript   subscript  θ  3   2   4     2       subscript  θ  m      subscript  M  m    subscript  σ  m       k\geq\frac{\theta_{3}+\sqrt{\theta_{3}^{2}+4}}{2},\qquad\theta_{m}=\frac{M_{m}%
 }{\sigma_{m}}     is the k th central moment. They then show that for t > 0     m   m   m     For k = 2 we obtain Chebyshev's inequality. For t ≥ 1, k > 2 and assuming that the k th moment exists, this bound is tighter than Chebyshev's inequality.  Related inequalities  Several other related inequalities are also known.  Zelen's inequality  Zelen has shown that 40     σ   σ   σ     with     n   n   n     where is the      Pr   (       ∑   i  =  1   n    X  i    n   -  1   ≥   1  n    )    ≤   7  8    .       Pr          superscript   subscript     i  1    n    subscript  X  i    n   1     1  n       7  8     \Pr\left(\frac{\sum_{i=1}^{n}X_{i}}{n}-1\geq\frac{1}{n}\right)\leq\frac{7}{8}.   -th moment and   X   X   X   is the standard deviation.  He, Zhang and Zhang's inequality  For any collection of    a  ≤  X  ≤  b      a  normal-≤  X  normal-≤  b    a≤X≤b   non-negative independent random variables with expectation 1 41       E  X  X   =  0        E  X  X   0    EXX=0     Hoeffding's lemma  Let    s  >  0      s  0    s>0   be a random variable with      E   [   e   s  X    ]    ≤   e    1  8    s  2     (   b  -  a   )   2      .        E   delimited-[]   superscript  e    s  X       superscript  e      1  8    superscript  s  2    superscript    b  a   2       E\left[e^{sX}\right]\leq e^{\frac{1}{8}s^{2}(b-a)^{2}}.   and     Pr   (    |     ∑   i  =  1   n    X  i     n    |   ≤  1   )    ≥  0.5.       Pr          superscript   subscript     i  1    n    subscript  X  i      n     1    0.5.    \Pr\left(\left|\frac{\sum_{i=1}^{n}X_{i}}{\sqrt{n}}\right|\leq 1\right)\geq 0.5.   , then for any     P  r   >  0.31        P  r   0.31    Pr>0.31   , we have      P   (  |  X  |  ≥  k  )   ≤    4   E   (   X  2   )      9   k  2     if   k  2   ≥   4  3   E   (   X  2   )   ,     fragments  P   fragments  normal-(  normal-|  X  normal-|   k  normal-)        4   normal-E   superscript  X  2       9   superscript  k  2      if    superscript  k  2      4  3   normal-E   fragments  normal-(   superscript  X  2   normal-)   normal-,    P(|X|\geq k)\leq\frac{4\operatorname{E}(X^{2})}{9k^{2}}\quad\text{if}\quad k^{%
 2}\geq\frac{4}{3}\operatorname{E}(X^{2}),     Van Zuijlen's bound  Let be a set of independent Rademacher random variables : 1) {{=}} Pr( X i {{=}} −1) {{=}} 0.5}} . Then 42      P   (  |  X  |  ≥  k  )   ≤  1  -    k  2    3   E   (   X  2   )      if   k  2   ≤   4  3   E   (   X  2   )   .     fragments  P   fragments  normal-(  normal-|  X  normal-|   k  normal-)    1      superscript  k  2     3   normal-E   superscript  X  2       if    superscript  k  2      4  3   normal-E   fragments  normal-(   superscript  X  2   normal-)   normal-.    P(|X|\geq k)\leq 1-\frac{k^{2}}{3\operatorname{E}(X^{2})}\quad\text{if}\quad k%
 ^{2}\leq\frac{4}{3}\operatorname{E}(X^{2}).     The bound is sharp and better than that which can be derived from the normal distribution (approximately     σ  ≤  ω  ≤   2  σ    ,        σ  ω         2  σ      \sigma\leq\omega\leq 2\sigma,   ).  Unimodal distributions  A distribution function F is unimodal at ν if its cumulative distribution function is convex on (−∞, ν ) and concave on ( ν ,∞) 43 An empirical distribution can be tested for unimodality with the dip test . 44  In 1823 Gauss showed that for a unimodal distribution with a mode of zero 45        |   ν  -  μ   |   ≤     3  4    ω    .          ν  μ          3  4    ω     |\nu-\mu|\leq\sqrt{\frac{3}{4}}\omega.         P   (  |  X  |  ≥  k  )   ≤    (   r   r  +  1    )   r     E    (  |  X  |  )   r     k  r    if   k  r   ≥    r  r     (   r  +  1   )    r  +  1     E   (  |  X   |  r   )   ,     fragments  P   fragments  normal-(  normal-|  X  normal-|   k  normal-)     superscript   fragments  normal-(    r    r  1    normal-)   r      fragments  normal-E   superscript   fragments  normal-(  normal-|  X  normal-|  normal-)   r     superscript  k  r     if    superscript  k  r       superscript  r  r    superscript    r  1     r  1     normal-E   fragments  normal-(  normal-|  X   superscript  normal-|  r   normal-)   normal-,    P(|X|\geq k)\leq\left(\frac{r}{r+1}\right)^{r}\frac{\operatorname{E}(|X|)^{r}}%
 {k^{r}}\quad\text{if}\quad k^{r}\geq\frac{r^{r}}{(r+1)^{r+1}}\operatorname{E}(%
 |X|^{r}),     If the second condition holds then the second bound is always less than or equal to the first.  If the mode ( ν ) is not zero and the mean ( μ ) and standard deviation ( σ ) are both finite then denoting the root mean square deviation from the mode by ω , we have      P   (  |  X  |  ≥  k  )   ≤   (  1  -    [    k  r     (  r  +  1  )   E    (  |  X  |  )   r     ]    1  /  r    )   if   k  r   ≤    r  r     (   r  +  1   )    r  +  1     E   (  |  X   |  r   )   .     fragments  P   fragments  normal-(  normal-|  X  normal-|   k  normal-)     fragments  normal-(  1    superscript   fragments  normal-[     superscript  k  r    fragments   fragments  normal-(  r   1  normal-)   normal-E   superscript   fragments  normal-(  normal-|  X  normal-|  normal-)   r     normal-]     1  r    normal-)    if    superscript  k  r       superscript  r  r    superscript    r  1     r  1     normal-E   fragments  normal-(  normal-|  X   superscript  normal-|  r   normal-)   normal-.    P(|X|\geq k)\leq\left(1-\left[\frac{k^{r}}{(r+1)\operatorname{E}(|X|)^{r}}%
 \right]^{1/r}\right)\quad\text{if}\quad k^{r}\leq\frac{r^{r}}{(r+1)^{r+1}}%
 \operatorname{E}(|X|^{r}).     and      P   (  |  X  |  >  k  )   ≤  max   (    [   r    (   r  +  1   )   k    ]   r   E  |   X  r   |  ,   s    (   s  -  1   )    k  r     E  |   X  r   |  -   1   s  -  1    )      fragments  P   fragments  normal-(  normal-|  X  normal-|   k  normal-)      fragments  normal-(   superscript   fragments  normal-[    r      r  1   k    normal-]   r   E  normal-|   superscript  X  r   normal-|  normal-,    s      s  1    superscript  k  r     E  normal-|   superscript  X  r   normal-|     1    s  1    normal-)     P(|X|>k)\leq\max\left(\left[\frac{r}{(r+1)k}\right]^{r}E|X^{r}|,\frac{s}{(s-1)%
 k^{r}}E|X^{r}|-\frac{1}{s-1}\right)     Winkler in 1866 extended Gauss' inequality to r th moments 46 where r > 0 and the distribution is unimodal with a mode of zero:      P   (  X  ≥  k  )   ≤   1  2   -   k   2   3     if  0  ≤  k  ≤   2   3    ,     fragments  P   fragments  normal-(  X   k  normal-)      1  2      k    2    3      if   0   k     2    3    normal-,    P(X\geq k)\leq\frac{1}{2}-\frac{k}{2\sqrt{3}}\quad\text{if}\quad 0\leq k\leq%
 \frac{2}{\sqrt{3}},         P   (  X  ≥  k  )   ≤   2   9   k  2     if   2   3    ≤  k  ≤    2  N   3   .     fragments  P   fragments  normal-(  X   k  normal-)      2    9   superscript  k  2      if     2    3     k       2  N   3   normal-.    P(X\geq k)\leq\frac{2}{9k^{2}}\quad\text{if}\quad\frac{2}{\sqrt{3}}\leq k\leq%
 \frac{2N}{3}.     Gauss' bound has been subsequently sharpened and extended to apply to departures from the mean rather than the mode due to the Vysochanskiï–Petunin inequality.  The Vysochanskiï–Petunin inequality has been extended by Dharmadhikari and Joag-Dev 47        f   (  x  )    =    1   2   3     if      |  x  |   <   3       formulae-sequence      f  x      1    2    3     if        x     3      f(x)=\frac{1}{2\sqrt{3}}\quad\text{if}\quad|x|<\sqrt{3}     where s is a constant satisfying both s > r + 1 and s ( s − r − 1) = r r and r > 0.  It can be shown that these inequalities are the best possible and that further sharpening of the bounds requires that additional restrictions be placed on the distributions.  Unimodal symmetrical distributions  The bounds on this inequality can also be sharpened if the distribution is both unimodal and symmetrical . 48 An empirical distribution can be tested for symmetry with a number of tests including McWilliam's R*. 49 It is known that the variance of a unimodal symmetrical distribution with finite support [ a , b ] is less than or equal to ( b − a ) 2 / 12. 50  Let the distribution be supported on the finite interval [ − N , N ] and the variance be finite. Let the mode of the distribution be zero and rescale the variance to 1. Let k > 0 and assume k          f   (  x  )    =   0  if      |  x  |   ≥   3     .     formulae-sequence      f  x    0  if        x     3      f(x)=0\quad\text{if}\quad|x|\geq\sqrt{3}.             f  k    (  x  )    =    1   3  k    if      |  x  |   <    3  k   2     ,     formulae-sequence       subscript  f  k   x      1    3  k    if        x       3  k   2      f_{k}(x)=\frac{1}{3k}\quad\text{if}\quad|x|<\frac{3k}{2},     If 0          f  k    (  x  )    =   0  if      |  x  |   ≥    3  k   2     .     formulae-sequence       subscript  f  k   x    0  if        x       3  k   2      f_{k}(x)=0\quad\text{if}\quad|x|\geq\frac{3k}{2}.         P   (  |  X  -  μ  |  ≥  k  σ  )   ≤  1  -   k   3    if  k  ≤   2   3       fragments  P   fragments  normal-(  normal-|  X   μ  normal-|   k  σ  normal-)    1     k    3     if   k     2    3      P(|X-\mu|\geq k\sigma)\leq 1-\frac{k}{\sqrt{3}}\quad\text{if}\quad k\leq\frac{%
 2}{\sqrt{3}}     If 2 / √3  ( 1 - \beta_k ) \delta_0 ( x ) + \beta_k f_k( x ),  where β k = 4 / 3 k 2 , δ 0 is the Dirac delta function and where      P   (  |  X  -  μ  |  ≥  k  σ  )   ≤   4   9   k  2     if  k  >   2   3       fragments  P   fragments  normal-(  normal-|  X   μ  normal-|   k  σ  normal-)      4    9   superscript  k  2      if   k     2    3      P(|X-\mu|\geq k\sigma)\leq\frac{4}{9k^{2}}\quad\text{if}\quad k>\frac{2}{\sqrt%
 {3}}         P   (  |  X  -  μ  |  ≥  k  σ  )   ≤   1   3   k  2        fragments  P   fragments  normal-(  normal-|  X   μ  normal-|   k  σ  normal-)      1    3   superscript  k  2       P(|X-\mu|\geq k\sigma)\leq\frac{1}{3k^{2}}     The existence of these densities shows that the bounds are optimal. Since N is arbitrary these bounds apply to any value of N .  The Camp–Meidell's inequality is a related inequality. 51 For an absolutely continuous unimodal and symmetrical distribution      P   (  |  X  |  ≥  1  )   ≤  min   (  1  ,   σ  2   )      fragments  P   fragments  normal-(  normal-|  X  normal-|   1  normal-)      fragments  normal-(  1  normal-,   superscript  σ  2   normal-)     P(|X|\geq 1)\leq\min(1,\sigma^{2})         P   (  X  ≥  1  )   ≤    σ  2    1  +   σ  2        fragments  P   fragments  normal-(  X   1  normal-)       superscript  σ  2     1   superscript  σ  2       P(X\geq 1)\leq\frac{\sigma^{2}}{1+\sigma^{2}}     The second of these inequality is the same as the Vysochanskiï–Petunin inequality.  DasGupta has shown that if the distribution is known to be normal 52      P   (  |  X  |  >  ϵ  )   ≥     (   1  -   ϵ  2    )   2     ψ  -  1   +    (   1  -   ϵ  2    )   2     ≥     (   1  -   ϵ  2    )   2   ψ      fragments  P   fragments  normal-(  normal-|  X  normal-|   ϵ  normal-)       superscript    1   superscript  ϵ  2    2       ψ  1    superscript    1   superscript  ϵ  2    2         superscript    1   superscript  ϵ  2    2   ψ     P(|X|>\epsilon)\geq\frac{(1-\epsilon^{2})^{2}}{\psi-1+(1-\epsilon^{2})^{2}}%
 \geq\frac{(1-\epsilon^{2})^{2}}{\psi}     Notes   Effects of symmetry and unimodality   Symmetry of the distribution decreases the inequality's bounds by a factor of 2 while unimodality sharpens the bounds by a factor of 4/9.   Unimodal distributions   Because the mean and the mode in a unimodal distribution differ by at most √3 standard deviations 53 at most 5% of a symmetrical unimodal distribution lies outside (2√10 + 3√3)/3 standard deviations of the mean (approximately 3.840 standard deviations). This is sharper than the bounds provided by the Chebyshev inequality (approximately 4.472 standard deviations).  These bounds on the mean are less sharp than those that can be derived from symmetry of the distribution alone which shows that at most 5% of the distribution lies outside approximately 3.162 standard deviations of the mean. The Vysochanskiï–Petunin inequality further sharpens this bound by showing that for such a distribution that at most 5% of the distribution lies outside 4√5/3 (approximately 2.981) standard deviations of the mean.   Symmetrical unimodal distributions   For any symmetrical unimodal distribution:   approximately 5.784% of the distribution lies outside 1.96 standard deviations of the mode  5% of the distribution lies outside 2√10/3 (approximately 2.11) standard deviations of the mode   DasGupta's inequality states that for a normal distribution at least 95% lies within approximately 2.582 standard deviations of the mean. This is less sharp than the true figure (approximately 1.96 standard deviations of the mean).  Bounds for specific distributions  DasGupta has determined a set of best possible bounds for a normal distribution for this inequality. 54  Steliga and Szynal have extended these bounds to the Pareto distribution . 55  Zero means  When the mean ( μ ) is zero Chebyshev's inequality takes a simple form. Let σ 2 be the variance. Then      P   (  X  ≥  ϵ  )   ≥    C  0   ψ   -    C  1    ψ    ϵ  +    C  2    ψ   ψ     ϵ     fragments  P   fragments  normal-(  X   ϵ  normal-)       subscript  C  0   ψ       subscript  C  1     ψ    ϵ      subscript  C  2     ψ    ψ     ϵ    P(X\geq\epsilon)\geq\frac{C_{0}}{\psi}-\frac{C_{1}}{\sqrt{\psi}}\epsilon+\frac%
 {C_{2}}{\psi\sqrt{\psi}}\epsilon     With the same conditions Cantelli's inequality takes the form       C  0   =     2   3    -  3      (    ≊  0.464   )         subscript  C  0    annotated      2    3    3    approximately-equals-or-equals  absent  0.464      C_{0}=2\sqrt{3}-3\quad(\approxeq 0.464)     Unit variance  If in addition E ( X 2 ) = 1 and E ( X 4 ) = ψ then for any 0 ≤ ε ≤ 1 56       C  1   =  1.397       subscript  C  1   1.397    C_{1}=1.397     The first inequality is sharp.  It is also known that for a random variable obeying the above conditions that 57       C  2   =  0.0231       subscript  C  2   0.0231    C_{2}=0.0231     where      P   (  X  >  0  )   ≥    C  0   ψ      fragments  P   fragments  normal-(  X   0  normal-)       subscript  C  0   ψ     P(X>0)\geq\frac{C_{0}}{\psi}         ψ  ≥    3    3   +  1       (    ≊  1.098   )        ψ   annotated    3      3   1     approximately-equals-or-equals  absent  1.098      \psi\geq\frac{3}{\sqrt{3}+1}\quad(\approxeq 1.098)         ψ  ≤   3    3   +  1        ψ    3      3   1      \psi\leq\frac{3}{\sqrt{3}+1}     It is also known that 58      P   (  X  >  0  )   ≥   2   3  +  ψ  +      (   1  +  ψ   )   2   -  4         fragments  P   fragments  normal-(  X   0  normal-)      2    3  ψ       superscript    1  ψ   2   4        P(X>0)\geq\frac{2}{3+\psi+\sqrt{(1+\psi)^{2}-4}}     The value of C 0 is optimal and the bounds are sharp if        1   b  -  a       ∫  a  b     f   (  x  )   g   (  x  )   d  x     ≥    [    1   b  -  a       ∫  a  b     f   (  x  )   d  x     ]    [    1   b  -  a       ∫  a  b     g   (  x  )   d  x     ]            1    b  a      superscript   subscript   a   b     f  x  g  x  d  x        delimited-[]      1    b  a      superscript   subscript   a   b     f  x  d  x       delimited-[]      1    b  a      superscript   subscript   a   b     g  x  d  x         \frac{1}{b-a}\int_{a}^{b}\!f(x)g(x)\,dx\geq\left[\frac{1}{b-a}\int_{a}^{b}\!f(%
 x)\,dx\right]\left[\frac{1}{b-a}\int_{a}^{b}\!g(x)\,dx\right]     If      z  =    x  -    γ  6    (    x  2   -  1   )     +    x  72    [    2   γ  2    (    4   x  2    -  7   )    -   3  κ   (    x  2   -  3   )     ]    +  ⋯       z      x      γ  6      superscript  x  2   1         x  72    delimited-[]      2   superscript  γ  2       4   superscript  x  2    7      3  κ     superscript  x  2   3       normal-⋯     z=x-\frac{\gamma}{6}(x^{2}-1)+\frac{x}{72}[2\gamma^{2}(4x^{2}-7)-3\kappa(x^{2}%
 -3)]+\cdots     then the sharp bound is        δ  -    (   1  +  δ   )    log   (   1  +  δ   )      <    -   δ  2     2  +  δ     .        δ      1  δ       1  δ           superscript  δ  2      2  δ      \delta-(1+\delta)\log(1+\delta)<\frac{-\delta^{2}}{2+\delta}.     Integral Chebyshev inequality  There is a second (less well known) inequality also named after Chebyshev 59  If f , g : [ a , b ] → R are two monotonic  functions of the same monotonicity, then      P   (  X  >   (  1  +  δ  )   μ  )   ≤   e    -    δ  2   μ     2  +  δ     ,     fragments  P   fragments  normal-(  X    fragments  normal-(  1   δ  normal-)   μ  normal-)     superscript  e         superscript  δ  2   μ      2  δ     normal-,    P(X>(1+\delta)\mu)\leq e^{\frac{-\delta^{2}\mu}{2+\delta}},     If f and g are of opposite monotonicity, then the above inequality works in the reverse way.  This inequality is related to Jensen's inequality , 60  Kantorovich's inequality , 61 the Hermite–Hadamard inequality 62 and Walter's conjecture . 63  Other inequalities  There are also a number of other inequalities associated with Chebyshev   Chebyshev's sum inequality  Chebyshev–Markov–Stieltjes inequalities   Haldane's transformation  One use of Chebyshev's inequality in applications is to create confidence intervals for variates with an unknown distribution. Haldane noted, 64 using an equation derived by Kendall , 65 that if a variate ( x ) has a zero mean, unit variance and both finite skewness ( γ ) and kurtosis ( κ ) then the variate can be converted to a normally distributed standard score ( z ):      P   (  X  <   (  1  -  δ  )   μ  )   ≤   e    -    δ  2   μ     2  +  δ     .     fragments  P   fragments  normal-(  X    fragments  normal-(  1   δ  normal-)   μ  normal-)     superscript  e         superscript  δ  2   μ      2  δ     normal-.    P(X<(1-\delta)\mu)\leq e^{\frac{-\delta^{2}\mu}{2+\delta}}.     This transformation may be useful as an alternative to Chebyshev's inequality or as an adjunct to it for deriving confidence intervals for variates with unknown distributions.  While this transformation may be useful for moderately skewed and/or kurtotic distributions, it performs poorly when the distribution is markedly skewed and/or kurtotic.  Chernoff bounds  If the random variables may also be assumed to be independently distributed it is possible to obtain sharper bounds. Let δ > 0. Then 66  $$\delta - (1 + \delta) \log(1 + \delta) < \frac{ -\delta^2 }{ 2 + \delta } .$$  With this inequality it can be shown that  $$P(X > (1 + \delta) \mu) \le e^{ \frac{ -\delta^2 \mu }{ 2 + \delta } },$$  $$P(X < (1 - \delta) \mu) \le e^{ \frac{ -\delta^2 \mu }{ 2 + \delta } }.$$  where μ is the mean of the distribution. Further discussion may be found in the article on Chernoff bounds  Notes  The Environmental Protection Agency has suggested best practices for the use of Chebyshev's inequality for estimating confidence intervals. 67 This caution appears to be justified as its use in this context may be seriously misleading 1  See also   Chernoff bound — a bound on sums of random variables  Cornish–Fisher expansion  Eaton's inequality  Hoeffding's inequality — an exponential bound on the sum of a series of random variables  Kolmogorov's inequality  Proof of the weak law of large numbers using Chebyshev's inequality  Le Cam's theorem  Markov inequality  Dvoretzky–Kiefer–Wolfowitz inequality  Multidimensional Chebyshev's inequality  Paley–Zygmund inequality  Vysochanskiï–Petunin inequality — a stronger result applicable to unimodal probability distributions  Concentration inequality   References  Further reading   A. Papoulis (1991), Probability, Random Variables, and Stochastic Processes , 3rd ed. McGraw–Hill. ISBN 0-07-100870-5. pp. 113–114.  G. Grimmett and D. Stirzaker (2001), Probability and Random Processes , 3rd ed. Oxford. ISBN 0-19-857222-0. Section 7.3.   External links    Formal proof in the Mizar system .   "  Category:Articles containing proofs  Category:Probabilistic inequalities  Category:Probability theory  Category:Statistical inequalities     ↩  ↩  ↩  Bienaymé I.-J. (1853) Considérations àl'appui de la découverte de Laplace. Comptes Rendus de l'Académie des Sciences 37: 309–324 ↩  ↩  Markov A. (1884) On certain applications of algebraic continued fractions, Ph.D. thesis, St. Petersburg ↩  ↩  ↩  Berge P. O. (1938) A note on a form of Tchebycheff's theorem for two variables. Biometrika 29, 405–406 ↩  Lal D. N. (1955) A note on a form of Tchebycheﬀ's inequality for two or more variables. Sankhya 15(3):317–320 ↩  Isii K. (1959) On a method for generalizations of Tchebycheff's inequality. Ann Inst Stat Math 10: 65–88 ↩  ↩  ↩  ↩  ↩  Vakhania, Nikolai Nikolaevich. Probability distributions on linear spaces. New York: North Holland, 1981. ↩  Section 2.1 ↩  (the references for this article are corrected by ) ↩  Dufour (2003) Properties of moments of random variables ↩  Niemitalo O. (2012) One-sided Chebyshev-type inequalities for bounded probability distributions. ↩  ↩  ↩   ↩   Savage, I. Richard. "Probability inequalities of the Tchebycheff type." Journal of Research of the National Bureau of Standards-B. Mathematics and Mathematical Physics B 65 (1961): 211-222 ↩  ↩    ↩   ↩  Neave E. H., Ross M. N., Yang J. (2008) Distinguishing upside potential from downside risk. Management Research News. 32(1):26–36 ↩  ↩  ↩  Cantelli F. (1910) Intorno ad un teorema fondamentale della teoria del rischio. Bolletino dell Associazione degli Attuari Italiani ↩  Grimmett and Stirzaker, problem 7.11.9. Several proofs of this result can be found in Chebyshev's Inequalities by A. G. McDowell. ↩  ↩  ↩  Zelen M. (1954) Bounds on a distribution function that are functions of moments to order four. J Res Nat Bur Stand 53:377–381 ↩  ↩  Martien C. A. van Zuijlen (2011) On a conjecture concerning the sum of independent Rademacher random variables ↩  ↩  Hartigan J. A., Hartigan P. M. (1985) "The dip test of unimodality" . Annals of Statistics 13(1):70–84 ↩  Gauss C. F. Theoria Combinationis Observationum Erroribus Minimis Obnoxiae. Pars Prior. Pars Posterior. Supplementum. Theory of the Combination of Observations Least Subject to Errors. Part One. Part Two. Supplement. 1995. Translated by G. W. Stewart. Classics in Applied Mathematics Series, Society for Industrial and Applied Mathematics, Philadelphia ↩  Winkler A. (1886) Math-Natur theorie Kl. Akad. Wiss Wien Zweite Abt 53, 6–41 ↩  ↩  ↩  McWilliams T. P. (1990) "A distribution-free test for symmetry based on a runs statistic". Journal of the American Statistical Association 85(412):1130–1133 ↩  ↩  ↩  DasGupta A. (2000) Best constants in Chebychev inequalities with various applications. Metrika 5(1):185–200 ↩  ↩    Godwin H. J. (1964) Inequalities on distribution functions. (Chapter 3) New York, Hafner Pub. Co. ↩  Lesley F. D., Rotar V. I. (2003) Some remarks on lower bounds of Chebyshev's type for half-lines. J Inequalities Pure Appl Math 4(5) Art 96 ↩   ↩  ↩  ↩   ↩  ↩  Kendall M. G. (1943) The Advanced Theory of Statistics, 1. London ↩  ↩  ↩     