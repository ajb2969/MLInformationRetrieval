<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1267">Numerical stability</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Numerical stability</h1>
<hr/>

<p>In the <a href="mathematics" title="wikilink">mathematical</a> subfield of <a href="numerical_analysis" title="wikilink">numerical analysis</a>, <strong>numerical stability</strong> is a generally desirable property of <a href="numerical_algorithm" title="wikilink">numerical algorithms</a>. The precise definition of stability depends on the context. One is <a href="numerical_linear_algebra" title="wikilink">numerical linear algebra</a> and the other is algorithms for solving ordinary and partial differential equations by discrete approximation.</p>

<p>In numerical linear algebra the principal concern is instabilities caused by proximity to singularities of various kinds, such as very small or nearly colliding eigenvalues. On the other hand, in numerical algorithms for differential equations the concern is the growth of round-off errors and/or initially small fluctuations in initial data which might cause a large deviation of final answer from the exact solution.</p>

<p>Some numerical algorithms may damp out the small fluctuations (errors) in the input data; others might magnify such errors. Calculations that can be proven not to magnify approximation errors are called <em>numerically stable</em>. One of the common tasks of numerical analysis is to try to select algorithms which are <em>robust</em> – that is to say, do not produce a wildly different result for very small change in the input data.</p>

<p>An <a href="opposite_(semantics)" title="wikilink">opposite</a> phenomenon is <strong>instability</strong>. Typically, an algorithm involves an approximate method, and in some cases one could prove that the algorithm would approach the right solution in some limit. Even in this case, there is no guarantee that it would converge to the correct solution, because the floating-point round-off or truncation errors can be magnified, instead of damped, causing the deviation from the exact solution to grow exponentially.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="stability-in-numerical-linear-algebra">Stability in numerical linear algebra</h2>

<p>There are different ways to formalize the concept of stability. The following definitions of forward, backward, and mixed stability are often used in <a href="numerical_linear_algebra" title="wikilink">numerical linear algebra</a>.</p>
<figure><b>(Figure)</b>
<figcaption>Diagram showing the <strong>forward error</strong> Δ<em>y</em> and the <strong>backward error</strong> Δ<em>x</em>, and their relation to the exact solution map 

<math display="inline" id="Numerical_stability:0">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 and the numerical solution 

<math display="inline" id="Numerical_stability:1">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

*.</figcaption>
</figure>

<p>Consider the problem to be solved by the numerical algorithm as a <a href="function_(mathematics)" title="wikilink">function</a> 

<math display="inline" id="Numerical_stability:2">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 mapping the data 

<math display="inline" id="Numerical_stability:3">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 to the solution 

<math display="inline" id="Numerical_stability:4">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

. The result of the algorithm, say 

<math display="inline" id="Numerical_stability:5">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

*, will usually deviate from the "true" solution 

<math display="inline" id="Numerical_stability:6">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

. The main causes of error are <a href="round-off_error" title="wikilink">round-off error</a> and <a href="truncation_error" title="wikilink">truncation error</a>. The <em>forward error</em> of the algorithm is the difference between the result and the solution; in this case, 

<math display="inline" id="Numerical_stability:7">
 <semantics>
  <mrow>
   <mrow>
    <mi mathvariant="normal">Δ</mi>
    <mi>y</mi>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi>y</mi>
     <mo>*</mo>
     <mi mathvariant="normal">−</mi>
    </mrow>
    <mi>y</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>normal-Δ</ci>
     <ci>y</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <times></times>
      <ci>y</ci>
      <ci>normal-−</ci>
     </apply>
     <ci>y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Δy=y*−y
  </annotation>
 </semantics>
</math>

. The <em>backward error</em> is the smallest Δ

<math display="inline" id="Numerical_stability:8">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 such that 

<math display="inline" id="Numerical_stability:9">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>+</mo>
    <mi mathvariant="normal">Δ</mi>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>y</mi>
   <mo>*</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">f</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <plus></plus>
     <csymbol cd="unknown">Δ</csymbol>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">y</csymbol>
    <times></times>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x+Δx)=y*
  </annotation>
 </semantics>
</math>

; in other words, the backward error tells us what problem the algorithm actually solved. The forward and backward error are related by the <a href="condition_number" title="wikilink">condition number</a>: the forward error is at most as big in magnitude as the condition number multiplied by the magnitude of the backward error.</p>

<p>In many cases, it is more natural to consider the <a href="relative_error" title="wikilink">relative error</a></p>

<p>

<math display="block" id="Numerical_stability:10">
 <semantics>
  <mfrac>
   <mrow>
    <mo stretchy="false">|</mo>
    <mrow>
     <mi mathvariant="normal">Δ</mi>
     <mi>x</mi>
    </mrow>
    <mo stretchy="false">|</mo>
   </mrow>
   <mrow>
    <mo stretchy="false">|</mo>
    <mi>x</mi>
    <mo stretchy="false">|</mo>
   </mrow>
  </mfrac>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <abs></abs>
     <apply>
      <times></times>
      <ci>normal-Δ</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <apply>
     <abs></abs>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{|\Delta x|}{|x|}
  </annotation>
 </semantics>
</math>

 instead of the absolute error Δ

<math display="inline" id="Numerical_stability:11">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

.</p>

<p>The algorithm is said to be <em>backward stable</em> if the backward error is small for all inputs 

<math display="inline" id="Numerical_stability:12">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

. Of course, "small" is a relative term and its definition will depend on the context. Often, we want the error to be of the same order as, or perhaps only a few <a href="orders_of_magnitude" title="wikilink">orders of magnitude</a> bigger than, the <a href="unit_round-off" title="wikilink">unit round-off</a>.</p>
<figure><b>(Figure)</b>
<figcaption>Mixed stability combines the concepts of forward error and backward error.</figcaption>
</figure>

<p>The usual definition of numerical stability uses a more general concept, called <em>mixed stability</em>, which combines the forward error and the backward error. An algorithm is stable in this sense if it solves a nearby problem approximately, i.e., if there exists a Δ

<math display="inline" id="Numerical_stability:13">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 such that both Δ

<math display="inline" id="Numerical_stability:14">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 is small and 

<math display="inline" id="Numerical_stability:15">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>+</mo>
    <mi mathvariant="normal">Δ</mi>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi mathvariant="normal">−</mi>
   <mi>y</mi>
   <mo>*</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">f</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <plus></plus>
     <csymbol cd="unknown">Δ</csymbol>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">−</csymbol>
    <csymbol cd="unknown">y</csymbol>
    <times></times>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x+Δx)−y*
  </annotation>
 </semantics>
</math>

 is small. Hence, a backward stable algorithm is always stable.</p>

<p>An algorithm is <em>forward stable</em> if its forward error divided by the condition number of the problem is small. This means that an algorithm is forward stable if it has a forward error of magnitude similar to some backward stable algorithm.</p>
<h2 id="stability-in-numerical-differential-equations">Stability in numerical differential equations</h2>

<p>The above definitions are particularly relevant in situations where truncation errors are not important. In other contexts, for instance when solving <a href="differential_equation" title="wikilink">differential equations</a>, a different definition of numerical stability is used.</p>

<p>In <a href="Numerical_methods_for_ordinary_differential_equations" title="wikilink">numerical ordinary differential equations</a>, various concepts of numerical stability exist, for instance <a href="Stiff_equation#A-stability" title="wikilink">A-stability</a>. They are related to some concept of stability in the <a href="dynamical_system" title="wikilink">dynamical systems</a> sense, often <a href="Lyapunov_stability" title="wikilink">Lyapunov stability</a>. It is important to use a stable method when solving a <a href="stiff_equation" title="wikilink">stiff equation</a>.</p>

<p>Yet another definition is used in <a href="numerical_partial_differential_equations" title="wikilink">numerical partial differential equations</a>. An algorithm for solving a linear evolutionary <a href="partial_differential_equation" title="wikilink">partial differential equation</a> is stable if the <a href="total_variation" title="wikilink">total variation</a> of the numerical solution at a fixed time remains bounded as the step size goes to zero. The <a href="Lax_equivalence_theorem" title="wikilink">Lax equivalence theorem</a> states that an algorithm converges if it is consistent and stable (in this sense). Stability is sometimes achieved by including <a href="numerical_diffusion" title="wikilink">numerical diffusion</a>. Numerical diffusion is a mathematical term which ensures that roundoff and other errors in the calculation get spread out and do not add up to cause the calculation to "blow up". <a href="Von_Neumann_stability_analysis" title="wikilink">Von Neumann stability analysis</a> is a commonly used procedure for the stability analysis of <a href="finite_difference_method" title="wikilink">finite difference schemes</a> as applied to linear partial differential equations. These results do not hold for nonlinear PDEs, where a general, consistent definition of stability is complicated by many properties absent in linear equations.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Algorithms_for_calculating_variance" title="wikilink">Algorithms for calculating variance</a></li>
<li><a href="Stability_theory" title="wikilink">Stability theory</a></li>
<li><a href="Chaos_theory" title="wikilink">Chaos theory</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li><a href="Nicholas_J._Higham" title="wikilink">Nicholas J. Higham</a>, <em>Accuracy and Stability of Numerical Algorithms</em>, Society of Industrial and Applied Mathematics, Philadelphia, 1996. ISBN 0-89871-355-2.</li>
<li>Richard L. Burden and J. Douglas Faires, <em>Numerical Analysis 8th Edition</em>, Thomson Brooks/Cole, U.S., 2005. ISBN 0-534-39200-8</li>
</ul>

<p>"</p>

<p><a href="Category:Numerical_analysis" title="wikilink">Category:Numerical analysis</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
</ol>
</section>
</body>
</html>
