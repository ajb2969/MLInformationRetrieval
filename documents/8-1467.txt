   Beta-binomial distribution      Beta-binomial distribution   \!|  | kurtosis   = See text|  | entropy    =|  | mgf        =      F  1    2     (   -  n   ,  α  ;   α  +  β   ;   1  -   e  t    )        subscript   subscript  F  1   2      n   α    α  β     1   superscript  e  t       {}_{2}F_{1}(-n,\alpha;\alpha+\beta;1-e^{t})\!            for  t   <    log  e    (  2  )          for  t     subscript   e   2     \text{for }t<\log_{e}(2)    |  | char       =      F  1    2     (   -  n   ,  α  ;   α  +  β   ;   1  -   e   i  t     )        subscript   subscript  F  1   2      n   α    α  β     1   superscript  e    i  t        {}_{2}F_{1}(-n,\alpha;\alpha+\beta;1-e^{it})\!          for   |  t  |    <    log  e    (  2  )          for    t      subscript   e   2     \text{for }|t|<\log_{e}(2)    |  }}  In probability theory and statistics , the beta-binomial distribution is a family of discrete probability distributions on a finite support of non-negative integers arising when the probability of success in each of a fixed or known number of Bernoulli trials is either unknown or random. The beta-binomial distribution is the binomial distribution in which the probability of success at each trial is not fixed but random and follows the beta distribution . It is frequently used in Bayesian statistics , empirical Bayes methods and classical statistics as an overdispersed binomial distribution.  It reduces to the Bernoulli distribution as a special case when n = 1. For α = β = 1, it is the discrete uniform distribution from 0 to n . It also approximates the binomial distribution arbitrarily well for large α and β . The beta-binomial is a one-dimensional version of the Dirichlet-multinomial distribution , as the binomial and beta distributions are special cases of the multinomial and Dirichlet distributions , respectively.  Motivation and derivation  Beta-binomial distribution as a compound distribution  The Beta distribution is a conjugate distribution of the binomial distribution . This fact leads to an analytically tractable compound distribution where one can think of the   p   p   p   parameter in the binomial distribution as being randomly drawn from a beta distribution. Namely, if     X   X   \displaystyle X     where Bin(n,p) stands for the binomial distribution , and where p is a random variable with a beta distribution .      π   (  p  |  α  ,  β  )      fragments  π   fragments  normal-(  p  normal-|  α  normal-,  β  normal-)     \displaystyle\pi(p|\alpha,\beta)   then the compound distribution is given by      f   (  k  |  n  ,  α  ,  β  )      fragments  f   fragments  normal-(  k  normal-|  n  normal-,  α  normal-,  β  normal-)     \displaystyle f(k|n,\alpha,\beta)   Using the properties of the beta function , this can alternatively be written      f   (  k  |  n  ,  α  ,  β  )   =    Γ   (   n  +  1   )     Γ   (   k  +  1   )   Γ   (    n  -  k   +  1   )       Γ   (   k  +  α   )   Γ   (    n  -  k   +  β   )     Γ   (   n  +  α  +  β   )       Γ   (   α  +  β   )     Γ   (  α  )   Γ   (  β  )     .     fragments  f   fragments  normal-(  k  normal-|  n  normal-,  α  normal-,  β  normal-)        normal-Γ    n  1      normal-Γ    k  1   normal-Γ      n  k   1         normal-Γ    k  α   normal-Γ      n  k   β      normal-Γ    n  α  β         normal-Γ    α  β      normal-Γ  α  normal-Γ  β    normal-.    f(k|n,\alpha,\beta)=\frac{\Gamma(n+1)}{\Gamma(k+1)\Gamma(n-k+1)}\frac{\Gamma(k%
 +\alpha)\Gamma(n-k+\beta)}{\Gamma(n+\alpha+\beta)}\frac{\Gamma(\alpha+\beta)}{%
 \Gamma(\alpha)\Gamma(\beta)}.   It is within this context that the beta-binomial distribution appears often in Bayesian statistics: the beta-binomial is the posterior predictive distribution of a binomial random variable with a beta distribution  prior on the success probability.  Beta-binomial as an urn model  The beta-binomial distribution can also be motivated via an urn model for positive integer values of α and β, known as the Polya urn model . Specifically, imagine an urn containing α red balls and β black balls, where random draws are made. If a red ball is observed, then two red balls are returned to the urn. Likewise, if a black ball is drawn, then two black balls are returned to the urn. If this is repeated n times, then the probability of observing k red balls follows a beta-binomial distribution with parameters n,α and β.  Note that if the random draws are with simple replacement (no balls over and above the observed ball are added to the urn), then the distribution follows a binomial distribution and if the random draws are made without replacement, the distribution follows a hypergeometric distribution .  Moments and properties  The first three raw moments are         \begin{align}  \mu_1 & =\frac{n\alpha}{\alpha+\beta} \\[8pt]  \mu_2 & =\frac{n\alpha[n(1+\alpha)+\beta]}{(\alpha+\beta)(1+\alpha+\beta)}\\[8pt]  \mu_3 & =\frac{n\alpha[n^{2}(1+\alpha)(2+\alpha)+3n(1+\alpha)\beta+\beta(\beta-\alpha)]}{(\alpha+\beta)(1+\alpha+\beta)(2+\alpha+\beta)}  \end{align}   and the kurtosis is         \beta_2 = \frac{(\alpha + \beta)^2 (1+\alpha+\beta)}{n \alpha \beta( \alpha + \beta + 2)(\alpha + \beta + 3)(\alpha + \beta + n) } \left[ (\alpha + \beta)(\alpha + \beta - 1 + 6n) + 3 \alpha\beta(n - 2) + 6n^2 -\frac{3\alpha\beta n(6-n)}{\alpha + \beta} - \frac{18\alpha\beta n^{2}}{(\alpha+\beta)^2} \right].   Letting    π  =    α   α  +  β         π    α    α  β      \pi=\frac{\alpha}{\alpha+\beta}\!   we note, suggestively, that the mean can be written as         \mu = \frac{n\alpha}{\alpha+\beta}=n\pi \!  and the variance as         \sigma^2 = \frac{n\alpha\beta(\alpha+\beta+n)}{(\alpha+\beta)^2(\alpha+\beta+1)}  = n\pi(1-\pi) \frac{\alpha + \beta + n}{\alpha + \beta + 1} = n\pi(1-\pi)[1+(n-1)\rho]  \!  where    ρ  =    1   α  +  β  +  1         ρ    1    α  β  1      \rho=\tfrac{1}{\alpha+\beta+1}\!   is the pairwise correlation between the n Bernoulli draws and is called the over-dispersion parameter.  Recurrence relation      {       (   α  +  k   )    (   n  -  k   )   p   (  k  )    -    (   k  +  1   )   p   (   k  +  1   )    (     β  -  k   +  n   -  1   )     =  0   ,    p   (  0  )    =     (  β  )   n     (   α  +  β   )   n      }      formulae-sequence          α  k     n  k   p  k       k  1   p    k  1         β  k   n   1     0       p  0      subscript  β  n    subscript    α  β   n        \left\{(\alpha+k)(n-k)p(k)-(k+1)p(k+1)(\beta-k+n-1)=0,p(0)=\frac{(\beta)_{n}}{%
 (\alpha+\beta)_{n}}\right\}     Point estimates  Method of moments  The method of moments estimates can be gained by noting the first and second moments of the beta-binomial namely         \begin{align}  \mu_1 & =\frac{n\alpha}{\alpha+\beta} \\  \mu_2 & =\frac{n\alpha[n(1+\alpha)+\beta]}{(\alpha+\beta)(1+\alpha+\beta)}  \end{align}   and setting these raw moments equal to the first and second raw sample moments respectively         \begin{align}  \hat{\mu}_1 & = m_1 \\  \hat{\mu}_2 & =m_2  \end{align}  and solving for α and β we get         \begin{align}  \hat{\alpha} & =\frac{nm_1-m_2}{n(\frac{m_2}{m_1}-m_1-1)+m_1} \\  \hat{\beta} & =\frac{(n-m_1)(n-\frac{m_2}{m_1})}{n(\frac{m_2}{m_1}-m_1 - 1)+m_1}.  \end{align}  Note that these estimates can be non-sensically negative which is evidence that the data is either undispersed or underdispersed relative to the binomial distribution. In this case, the binomial distribution and the hypergeometric distribution are alternative candidates respectively.  Maximum likelihood estimation  While closed-form maximum likelihood estimates are impractical, given that the pdf consists of common functions (gamma function and/or Beta functions), they can be easily found via direct numerical optimization. Maximum likelihood estimates from empirical data can be computed using general methods for fitting multinomial Pólya distributions, methods for which are described in (Minka 2003). The R package VGAM through the function vglm, via maximum likelihood, facilitates the fitting of glm type models with responses distributed according to the beta-binomial distribution. Note also that there is no requirement that n is fixed throughout the observations.  Example  The following data gives the number of male children among the first 12 children of family size 13 in 6115 families taken from hospital records in 19th century Saxony (Sokal and Rohlf, p. 59 from Lindsey). The 13th child is ignored to assuage the effect of families non-randomly stopping when a desired gender is reached.      Males   0   1   2   3   4   5   6   7   8   9   10   11   12     Families   3   24   104   286   670   1033   1343   1112   829   478   181   45   7     We note the first two sample moments are         \begin{align}  m_1 & = 6.23\\  m_2 & = 42.31 \\  n & = 12  \end{align}  and therefore the method of moments estimates are         \begin{align}  \hat{\alpha} & = 34.1350\\  \hat{\beta} & = 31.6085.  \end{align}   The maximum likelihood estimates can be found numerically         \begin{align}  \hat\alpha_\mathrm{mle} & = 34.09558\\  \hat\beta_\mathrm{mle} & = 31.5715  \end{align}  and the maximized log-likelihood is         \log \mathcal{L} = -12492.9   from which we find the AIC         \mathit{AIC}=24989.74.   The AIC for the competing binomial model is AIC = 25070.34 and thus we see that the beta-binomial model provides a superior fit to the data i.e. there is evidence for overdispersion. Trivers and Willard posit a theoretical justification for heterogeneity (also known as " burstiness ") in gender-proneness among families (i.e. overdispersion).  The superior fit is evident especially among the tails      Males   0   1   2   3   4   5   6   7   8   9   10   11   12     Observed Families   3   24   104   286   670   1033   1343   1112   829   478   181   45   7     Predicted (Beta-Binomial)   2.3   22.6   104.8   310.9   655.7   1036.2   1257.9   1182.1   853.6   461.9   177.9   43.8   5.2     Predicted (Binomial p = 0.519215)   0.9   12.1   71.8   258.5   628.1   1085.2   1367.3   1265.6   854.2   410.0   132.8   26.1   2.3     Further Bayesian considerations  It is convenient to reparameterize the distributions so that the expected mean of the prior is a single parameter: Let      π   (  θ  |  μ  ,  M  )      fragments  π   fragments  normal-(  θ  normal-|  μ  normal-,  M  normal-)     \displaystyle\pi(\theta|\mu,M)     where         \begin{align}  \mu &= \frac{\alpha}{\alpha+\beta}  \\  M &= \alpha+\beta  \end{align}   so that         \begin{align}  \operatorname{E}(\theta|\mu,M)   & = \mu \\  \operatorname{Var}(\theta|\mu,M) & = \frac{\mu(1-\mu)}{M+1}.  \end{align}   The posterior distribution ρ(θ|k) is also a beta distribution:      ρ   (  θ  |  k  )      fragments  ρ   fragments  normal-(  θ  normal-|  k  normal-)     \displaystyle\rho(\theta|k)     And        E   (  θ  |  k  )    =    k  +   M  μ     n  +  M     .       normal-E  θ  k       k    M  μ      n  M      \operatorname{E}(\theta|k)=\frac{k+M\mu}{n+M}.     while the marginal distribution m ( k |μ, M ) is given by      m   (  k  |  μ  ,  M  )      fragments  m   fragments  normal-(  k  normal-|  μ  normal-,  M  normal-)     \displaystyle m(k|\mu,M)     Because the marginal is a complex, non-linear function of Gamma and Digamma functions, it is quite difficult to obtain a marginal maximum likelihood estimate (MMLE) for the mean and variance. Instead, we use the method of iterated expectations to find the expected value of the marginal moments.  Let us write our model as a two-stage compound sampling model. Let k i be the number of success out of n i trials for event i :         \begin{align}  k_i & \sim \operatorname{Bin}(n_i, \theta_i) \\  \theta_i & \sim \operatorname{Beta}(\mu,M),\ \mathrm{i.i.d.}  \end{align}   We can find iterated moment estimates for the mean and variance using the moments for the distributions in the two-stage model:         E   (   k  n   )    =   E   [   E   (   k  n   |  θ  )    ]    =   E   (  θ  )    =  μ         normal-E    k  n     normal-E   normal-E    k  n   θ          normal-E  θ        μ     \operatorname{E}\left(\frac{k}{n}\right)=\operatorname{E}\left[\operatorname{E%
 }\left(\left.\frac{k}{n}\right|\theta\right)\right]=\operatorname{E}(\theta)=\mu               \begin{align}  \operatorname{var}\left(\frac{k}{n}\right) & =  \operatorname{E}\left[\operatorname{var}\left(\left.\frac{k}{n}\right|\theta\right)\right] +  \operatorname{var}\left[\operatorname{E}\left(\left.\frac{k}{n}\right|\theta\right)\right] \\  & =  \operatorname{E}\left[\left(\left.\frac{1}{n}\right)\theta(1-\theta)\right|\mu,M\right] +  \operatorname{var}\left(\theta|\mu,M\right) \\  & =  \frac{1}{n}\left(\mu(1-\mu)\right) + \frac{n-1}{n}\frac{(\mu(1-\mu))}{M+1} \\  & =  \frac{\mu(1-\mu)}{n}\left(1+\frac{n-1}{M+1}\right).  \end{align}   (Here we have used the law of total expectation and the law of total variance .)  We want point estimates for   μ   μ   \mu   and   M   M   M   . The estimated mean    μ  ^     normal-^  μ    \hat{\mu}   is calculated from the sample          μ  ^   =      ∑   i  =  1   N     k  i       ∑   i  =  1   N     n  i      .       normal-^  μ       superscript   subscript     i  1    N    subscript  k  i      superscript   subscript     i  1    N    subscript  n  i       \hat{\mu}=\frac{\sum_{i=1}^{N}k_{i}}{\sum_{i=1}^{N}n_{i}}.        The estimate of the hyperparameter M is obtained using the moment estimates for the variance of the two-stage model:         s^2 = \frac{1}{N} \sum_{i=1}^N \operatorname{var}\left(\frac{k_{i}}{n_{i}} \right)  = \frac{1}{N} \sum_{i=1}^N \frac{\hat{\mu}(1-\hat{\mu})}{n_i}  \left[1+\frac{n_i-1}{\widehat{M}+1}\right]   Solving:          M  ^   =      μ  ^    (   1  -   μ  ^    )    -   s  2      s  2   -      μ  ^    (   1  -   μ  ^    )    N      ∑   i  =  1   N     1  /   n  i         ,       normal-^  M          normal-^  μ     1   normal-^  μ      superscript  s  2       superscript  s  2          normal-^  μ     1   normal-^  μ     N     superscript   subscript     i  1    N     1   subscript  n  i          \widehat{M}=\frac{\hat{\mu}(1-\hat{\mu})-s^{2}}{s^{2}-\frac{\hat{\mu}(1-\hat{%
 \mu})}{N}\sum_{i=1}^{N}1/n_{i}},        where    s^2 = \frac{N \sum_{i=1}^N n_i (\hat{\theta_i} - \hat{\mu})^2 }     {(N-1)\sum_{i=1}^N n_i }.  Since we now have parameter point estimates,    μ  ^     normal-^  μ    \hat{\mu}   and    M  ^     normal-^  M    \widehat{M}   , for the underlying distribution, we would like to find a point estimate     θ  ~   i     subscript   normal-~  θ   i    \tilde{\theta}_{i}   for the probability of success for event i . This is the weighted average of the event estimate      θ  i   ^   =    k  i   /   n  i         normal-^   subscript  θ  i       subscript  k  i    subscript  n  i      \hat{\theta_{i}}=k_{i}/n_{i}   and    μ  ^     normal-^  μ    \hat{\mu}   . Given our point estimates for the prior, we may now plug in these values to find a point estimate for the posterior          θ  i   ~   =  E   (  θ  |   k  i   )   =     k  i   +    M  ^    μ  ^       n  i   +   M  ^     =    M  ^     n  i   +   M  ^      μ  ^   +    n  i     n  i   +   M  ^       k  i    n  i    .     fragments   normal-~   subscript  θ  i     E   fragments  normal-(  θ  normal-|   subscript  k  i   normal-)         subscript  k  i      normal-^  M    normal-^  μ        subscript  n  i    normal-^  M         normal-^  M      subscript  n  i    normal-^  M      normal-^  μ       subscript  n  i      subscript  n  i    normal-^  M        subscript  k  i    subscript  n  i    normal-.    \tilde{\theta_{i}}=E(\theta|k_{i})=\frac{k_{i}+\widehat{M}\hat{\mu}}{n_{i}+%
 \widehat{M}}=\frac{\widehat{M}}{n_{i}+\widehat{M}}\hat{\mu}+\frac{n_{i}}{n_{i}%
 +\widehat{M}}\frac{k_{i}}{n_{i}}.        Shrinkage factors  We may write the posterior estimate as a weighted average:          θ  ~   i   =       B  ^   i     μ  ^    +    (   1  -    B  ^   i    )     θ  ^   i          subscript   normal-~  θ   i        subscript   normal-^  B   i    normal-^  μ        1   subscript   normal-^  B   i     subscript   normal-^  θ   i       \tilde{\theta}_{i}=\hat{B}_{i}\,\hat{\mu}+(1-\hat{B}_{i})\hat{\theta}_{i}        where     B  ^   i     subscript   normal-^  B   i    \hat{B}_{i}   is called the shrinkage factor .          B  i   ^   =    M  ^     M  ^   +   n  i          normal-^   subscript  B  i       normal-^  M      normal-^  M    subscript  n  i       \hat{B_{i}}=\frac{\hat{M}}{\hat{M}+n_{i}}        Related distributions        B  B   (  1  ,  1  ,  n  )    ∼   U   (  0  ,  n  )       similar-to    B  B   1  1  n      U   0  n      BB(1,1,n)\sim U(0,n)\,   where    U   (  a  ,  b  )       U   a  b     U(a,b)\,   is the discrete uniform distribution .   See also   Dirichlet-multinomial distribution   References    Minka, Thomas P. (2003). Estimating a Dirichlet distribution . Microsoft Technical Report.   External links   Using the Beta-binomial distribution to assess performance of a biometric identification device  Fastfit contains Matlab code for fitting Beta-Binomial distributions (in the form of two-dimensional Pólya distributions) to data.  Interactive graphic: Univariate Distribution Relationships  Beta-Binomial distribution package for R   "  Category:Conjugate prior distributions  Category:Discrete distributions  Category:Probability distributions   