


MinHash




MinHash

In computer science, MinHash (or the min-wise independent permutations locality sensitive hashing scheme) is a technique for quickly estimating how similar two sets are. The scheme was invented by ,1 and initially used in the AltaVista search engine to detect duplicate web pages and eliminate them from search results.2 It has also been applied in large-scale clustering problems, such as clustering documents by the similarity of their sets of words.3
Jaccard similarity and minimum hash values
The Jaccard similarity coefficient is a commonly used indicator of the similarity between two sets. For sets 
 
 
 
  and 
 
 
 
  it is defined to be the ratio of the number of elements of their intersection and the number of elements of their union:


 
  This value is 0 when the two sets are disjoint, 1 when they are equal, and strictly between 0 and 1 otherwise. Two sets are more similar (i.e. have relatively more members in common) when their Jaccard index is closer to 1. It is our goal to estimate 
 
 
 
  quickly, without explicitly computing the intersection and union.
Let 
 
 
 
  be a hash function that maps the members of 
 
 
 
  and 
 
 
 
  to distinct integers, and for any set 
 
 
 
  define  to be the minimal member of 
 
 
 
  with respect to 
 
 
 
 —that is, the member 
 
 
 
  of 
 
 
 
  with the minimum value of 
 
 
 
 . Now, if we apply  to both 
 
 
 
  and 
 
 
 
 , we will get the same value exactly when the element of the union 
 
 
 
  with minimum hash value lies in the intersection 
 
 
 
 . The probability of this being true is the ratio above, and therefore:




That is, the probability that  is true is equal to the similarity 
 
 
 
 , assuming randomly chosen sets 
 
 
 
  and 
 
 
 
 . In other words, if 
 
 
 
  is the random variable that is one when  and zero otherwise, then 
 
 
 
  is an unbiased estimator of 
 
 
 
 . 
 
 
 
  has too high a variance to be a useful estimator for the Jaccard similarity on its own—it is always zero or one. The idea of the MinHash scheme is to reduce this variance by averaging together several variables constructed in the same way.
Algorithm
Variant with many hash functions
The simplest version of the minhash scheme uses 
 
 
 
  different hash functions, where 
 
 
 
  is a fixed integer parameter, and represents each set 
 
 
 
  by the 
 
 
 
  values of  for these 
 
 
 
  functions.
To estimate 
 
 
 
  using this version of the scheme, let 
 
 
 
  be the number of hash functions for which , and use 
 
 
 
  as the estimate. This estimate is the average of 
 
 
 
  different 0-1 random variables, each of which is one when  and zero otherwise, and each of which is an unbiased estimator of 
 
 
 
 . Therefore, their average is also an unbiased estimator, and by standard Chernoff bounds for sums of 0-1 random variables, its expected error is 
 
 
 
 .4
Therefore, for any constant 
 
 
 
  there is a constant  such that the expected error of the estimate is at most 
 
 
 
 . For example, 400 hashes would be required to estimate 
 
 
 
  with an expected error less than or equal to .05.
Variant with a single hash function
It may be computationally expensive to compute multiple hash functions, but a related version of MinHash scheme avoids this penalty by using only a single hash function and uses it to select multiple values from each set rather than selecting only a single minimum value per hash function. Let 
 
 
 
  be a hash function, and let 
 
 
 
  be a fixed integer. If 
 
 
 
  is any set of 
 
 
 
  or more values in the domain of 
 
 
 
 , define  to be the subset of the 
 
 
 
  members of 
 
 
 
  that have the smallest values of 
 
 
 
 . This subset  is used as a signature for the set 
 
 
 
 , and the similarity of any two sets is estimated by comparing their signatures.
Specifically, let A and B be any two sets. Then  is a set of k elements of 
 
 
 
 , and if h is a random function then any subset of k elements is equally likely to be chosen; that is, 
 
 
 
  is a simple random sample of 
 
 
 
 . The subset  is the set of members of 
 
 
 
  that belong to the intersection 
 
 
 
 . Therefore, |
 
 
 
 |/
 
 
 
  is an unbiased estimator of 
 
 
 
 . The difference between this estimator and the estimator produced by multiple hash functions is that 
 
 
 
  always has exactly 
 
 
 
  members, whereas the multiple hash functions may lead to a smaller number of sampled elements due to the possibility that two different hash functions may have the same minima. However, when 
 
 
 
  is small relative to the sizes of the sets, this difference is negligible.
By standard Chernoff bounds for sampling without replacement, this estimator has expected error 
 
 
 
 , matching the performance of the multiple-hash-function scheme.
Time analysis
The estimator 
 
 
 
  can be computed in time 
 
 
 
  from the two signatures of the given sets, in either variant of the scheme. Therefore, when 
 
 
 
  and 
 
 
 
  are constants, the time to compute the estimated similarity from the signatures is also constant. The signature of each set can be computed in linear time on the size of the set, so when many pairwise similarities need to be estimated this method can lead to a substantial savings in running time compared to doing a full comparison of the members of each set. Specifically, for set size 
 
 
 
  the many hash variant takes 
 
 
 
  time. The single hash variant is generally faster, requiring 
 
 
 
  time to maintain the sorted list of minima.
Min-wise independent permutations
In order to implement the MinHash scheme as described above, one needs the hash function 
 
 
 
  to define a random permutation on 
 
 
 
  elements, where 
 
 
 
  is the total number of distinct elements in the union of all of the sets to be compared. But because there are 
 
 
 
  different permutations, it would require 
 
 
 
  bits just to specify a truly random permutation, an infeasibly large number for even moderate values of 
 
 
 
 . Because of this fact, by analogy to the theory of universal hashing, there has been significant work on finding a family of permutations that is "min-wise independent", meaning that for any subset of the domain, any element is equally likely to be the minimum. It has been established that a min-wise independent family of permutations must include at least


 
  different permutations, and therefore that it needs 
 
 
 
  bits to specify a single permutation, still infeasibly large.5
Because of this impracticality, two variant notions of min-wise independence have been introduced: restricted min-wise independent permutations families, and approximate min-wise independent families. Restricted min-wise independence is the min-wise independence property restricted to certain sets of cardinality at most 
 
 
 
 .6 Approximate min-wise independence has at most a fixed probability 
 
 
 
  of varying from full independence.7
Applications
The original applications for MinHash involved clustering and eliminating near-duplicates among web documents, represented as sets of the words occurring in those documents.89 Similar techniques have also been used for clustering and near-duplicate elimination for other types of data, such as images: in the case of image data, an image can be represented as a set of smaller subimages cropped from it, or as sets of more complex image feature descriptions.10
In data mining,  use MinHash as a tool for association rule learning. Given a database in which each entry has multiple attributes (viewed as a 0-1 matrix with a row per database entry and a column per attribute) they use MinHash-based approximations to the Jaccard index to identify candidate pairs of attributes that frequently co-occur, and then compute the exact value of the index for only those pairs to determine the ones whose frequencies of co-occurrence are below a given strict threshold.11
Other uses
The MinHash scheme may be seen as an instance of locality sensitive hashing, a collection of techniques for using hash functions to map large sets of objects down to smaller hash values in such a way that, when two objects have a small distance from each other, their hash values are likely to be the same. In this instance, the signature of a set may be seen as its hash value. Other locality sensitive hashing techniques exist for Hamming distance between sets and cosine distance between vectors; locality sensitive hashing has important applications in nearest neighbor search algorithms.12 For large distributed systems, and in particular MapReduce, there exist modified versions of MinHash to help compute similarities with no dependence on the point dimension.13
Evaluation and benchmarks
A large scale evaluation has been conducted by Google in 2006 14 to compare the performance of Minhash and Simhash15 algorithms. In 2007 Google reported using Simhash for duplicate detection for web crawling16 and using Minhash and LSH for Google News personalization.17
See also

Approximate string matching
Rolling hash
w-shingling
Tabulation hashing
Bloom filter
Count-Min sketch
Set cover problem
Levenshtein distance
String metric
Semantic hashingR. R. Salakhutdinov and G. E. Hinton. Semantic hashing. In SIGIR workshop on Information Retrieval and applications

of Graphical Models, 2007.

Spectral hashing18

External links

Mining of Massive Datasets, Ch. 3. Finding similar Items
Simple Simhashing
Set Similarity & MinHash - C# implementation
Minhash with LSH for all-pair search (C# implementation)
MinHash – Java implementation
MinHash – Scala implementation and a duplicate detection tool
All pairs similarity search (Google Research)
Distance and Similarity Measures(Wolfram Alpha)
Nilsimsa hash (Python implementation)
Simhash

References
"
Category:Hash functions Category:Clustering criteria Category:Hashing Category:Probabilistic data structures




.↩
.↩
.↩

.↩
.↩


; .↩
.↩
.↩
.↩
.↩
.↩
.↩
.↩
Weiss, Yair, Antonio Torralba, and Rob Fergus. "Spectral hashing." Advances in neural information processing systems. 2009.↩




