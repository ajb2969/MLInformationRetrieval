   Local case-control sampling      Local case-control sampling  In [[machine learning]], '''local case-control sampling''' {{cite journal|last1=Fithian|first1=William|last2=Hastie|first2 Trevor|title  Local case-control sampling: Efficient subsampling in imbalanced data sets|journal=The Annals of Statistics|date=2014|volume=42|issue=5|page=1693–1724|ref= http://arxiv.org/abs/1306.3706 }} is an algorithm used to reduce the complexity of training a logistic regression classifier. The algorithm reduces the training complexity by selecting a small subsample of the original dataset for training. It assumes the availability of a (unreliable) pilot estimation of the parameters. It then performs a single pass over the entire dataset using the pilot estimation to identify the most "surprising" samples. In practice, the pilot may come from prior knowledge or training using a subsample of the dataset. The algorithm is most effective when the underlying dataset is imbalanced. It exploits the structures of conditional imbalanced datasets more efficiently than alternative methods, such as case control sampling and weighted case control sampling.  Imbalanced datasets  In classification , a dataset is a set of N data points     (   x  i   ,   y  i   )    i  =  1   N     superscript   subscript    subscript  x  i    subscript  y  i      i  1    N    (x_{i},y_{i})_{i=1}^{N}   , where     x  i   ∈   ℝ  d        subscript  x  i    superscript  ℝ  d     x_{i}\in\mathbb{R}^{d}   is a feature vector,     y  i   ∈   {  0  ,  1  }        subscript  y  i    0  1     y_{i}\in\{0,1\}   is a label. Intuitively, a dataset is imbalanced when certain important statistical patterns are rare. The lack of observations of certain patterns does not always imply their irrelevance. For example, in medical studies of rare diseases, the small number of infected patients (cases) conveys the most valuable information for diagnosis and treatments.  Formally, an imbalanced dataset exhibits one or more of the following properties:   Marginal Imbalance . A dataset is marginally imbalanced if one class is rare compared to the other class. In other words,    ℙ   (  Y  =  1  )   ≈  0     fragments  P   fragments  normal-(  Y   1  normal-)    0    \mathbb{P}(Y=1)\approx 0   .  Conditional Imbalance . A dataset is conditionally imbalanced when it is easy to predict the correct labels in most cases. For example, if    X  ∈   {  0  ,  1  }       X   0  1     X\in\{0,1\}   , the dataset is conditionally imbalanced if    ℙ   (  Y  =  1  ∣  X  =  0  )   ≈  0     fragments  P   fragments  normal-(  Y   1  normal-∣  X   0  normal-)    0    \mathbb{P}(Y=1\mid X=0)\approx 0   and    ℙ   (  Y  =  1  ∣  X  =  1  )   ≈  1     fragments  P   fragments  normal-(  Y   1  normal-∣  X   1  normal-)    1    \mathbb{P}(Y=1\mid X=1)\approx 1   .   Algorithm outline  In logistic regression, given the model    θ  =   (  α  ,  β  )       θ   α  β     \theta=(\alpha,\beta)   , the prediction is made according to    ℙ   (  Y  =  1  ∣  X  ;  θ  )   =    p  ~   θ    (  x  )   =    exp   (   α  +    β  T   x    )     1  +   exp   (   α  +    β  T   x    )         fragments  P   fragments  normal-(  Y   1  normal-∣  X  normal-;  θ  normal-)     subscript   normal-~  p   θ    fragments  normal-(  x  normal-)          α     superscript  β  T   x       1      α     superscript  β  T   x         \mathbb{P}(Y=1\mid X;\theta)=\tilde{p}_{\theta}(x)=\frac{\exp(\alpha+\beta^{T}%
 x)}{1+\exp(\alpha+\beta^{T}x)}   . The local-case control sampling algorithm assumes the availability of a pilot model     θ  ~   =   (   α  ~   ,   β  ~   )        normal-~  θ     normal-~  α    normal-~  β      \tilde{\theta}=(\tilde{\alpha},\tilde{\beta})   . Given the pilot model, the algorithm performs a single pass over the entire dataset to select the subset of samples to include in training the logistic regression model. For a sample    (  x  ,  y  )     x  y    (x,y)   , define the acceptance probability as     a   (  x  ,  y  )    =   |   y  -     p  ~    θ  ~     (  x  )     |         a   x  y        y     subscript   normal-~  p    normal-~  θ    x       a(x,y)=|y-\tilde{p}_{\tilde{\theta}}(x)|   . The algorithm proceeds as follows:   Generate independent     z  i   ∼   Bernoulli   (   a   (   x  i   ,   y  i   )    )       similar-to   subscript  z  i     Bernoulli    a    subscript  x  i    subscript  y  i        z_{i}\sim\text{Bernoulli}(a(x_{i},y_{i}))   for    i  ∈   {  1  ,  …  ,  N  }       i   1  normal-…  N     i\in\{1,\ldots,N\}   .  Fit a logistic regression model to the subsample    S  =   {   (   x  i   ,   y  i   )   :    z  i   =  1   }       S   conditional-set    subscript  x  i    subscript  y  i       subscript  z  i   1      S=\{(x_{i},y_{i}):z_{i}=1\}   , obtaining the unadjusted estimates      θ  ^   S   =   (    α  ^   S   ,    β  ^   S   )        subscript   normal-^  θ   S     subscript   normal-^  α   S    subscript   normal-^  β   S      \hat{\theta}_{S}=(\hat{\alpha}_{S},\hat{\beta}_{S})   .  The output model is     θ  ^   =   (   α  ^   ,   β  ^   )        normal-^  θ     normal-^  α    normal-^  β      \hat{\theta}=(\hat{\alpha},\hat{\beta})   , where     α  ^   ←     α  ^   S   +   α  ~       normal-←   normal-^  α      subscript   normal-^  α   S    normal-~  α      \hat{\alpha}\leftarrow\hat{\alpha}_{S}+\tilde{\alpha}   and     β  ^   ←     β  ^   S   +   β  ~       normal-←   normal-^  β      subscript   normal-^  β   S    normal-~  β      \hat{\beta}\leftarrow\hat{\beta}_{S}+\tilde{\beta}   .   The algorithm can be understood as selecting samples that surprises the pilot model. Intuitively these samples are closer to the decision boundary of the classifier and is thus more informative.  Obtaining the pilot model  In practice, for cases where a pilot model is naturally available, the algorithm can be applied directly to reduce the complexity of training. In cases where a natural pilot is nonexistent, an estimate using a subsample selected through another sampling technique can be used instead. In the original paper describing the algorithm, the authors propose to use weighted case-control sampling with half the assigned sampling budget. For example, if the objective is to use a subsample with size    N  =  1000      N  1000    N=1000   , first estimate a model    θ  ~     normal-~  θ    \tilde{\theta}   using     N  h   =  500       subscript  N  h   500    N_{h}=500   samples from weighted case control sampling, then collect another     N  h   =  500       subscript  N  h   500    N_{h}=500   samples using local case-control sampling.  Larger or smaller sample size  It is possible to control the sample size by multiplying the acceptance probability with a constant   c   c   c   . For a larger sample size, pick    c  >  1      c  1    c>1   and adjust the acceptance probability to    min   (   c  a   (   x  i   ,   y  i   )    ,  1  )         c  a    subscript  x  i    subscript  y  i     1    \min(ca(x_{i},y_{i}),1)   . For a smaller sample size, the same strategy applies. In cases where the number of samples desired is precise, a convenient alternative method is to uniformly downsample from a larger subsample selected by local case-control sampling.  Properties  The algorithm has the following properties. When the pilot is consistent , the estimates using the samples from local case-control sampling is consistent even under model misspecification . If the model is correct then the algorithm has exactly twice the asymptotic variance of logistic regression on the full data set. For a larger sample size with    c  >  1      c  1    c>1   , the factor 2 is improved to    1  +   1  c       1    1  c     1+\frac{1}{c}   .  References  "  Category:Machine learning  Category:Log-linear models  Category:Regression analysis   