<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="830">Softmax function</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Softmax function</h1>
<hr/>

<p>In <a class="uri" href="mathematics" title="wikilink">mathematics</a>, in particular <a href="probability_theory" title="wikilink">probability theory</a> and related fields, the <strong>softmax</strong> function, or <strong>normalized exponential</strong>,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> is a generalization of the <a href="logistic_function" title="wikilink">logistic function</a> that "squashes" a 

<math display="inline" id="Softmax_function:0">
 <semantics>
  <mi>K</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>K</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K
  </annotation>
 </semantics>
</math>

-dimensional vector 

<math display="inline" id="Softmax_function:1">
 <semantics>
  <mi>𝐳</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐳</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{z}
  </annotation>
 </semantics>
</math>

 of arbitrary real values to a 

<math display="inline" id="Softmax_function:2">
 <semantics>
  <mi>K</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>K</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K
  </annotation>
 </semantics>
</math>

-dimensional vector 

<math display="inline" id="Softmax_function:3">
 <semantics>
  <mrow>
   <mi>σ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐳</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>σ</ci>
    <ci>𝐳</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma(\mathbf{z})
  </annotation>
 </semantics>
</math>

 of real values in the range (0, 1). The function is given by</p>

<p>

<math display="block" id="Softmax_function:4">
 <semantics>
  <mrow>
   <mrow>
    <mi>σ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>𝐳</mi>
      <mi>𝐣</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <msup>
     <mi>e</mi>
     <msub>
      <mi>z</mi>
      <mi>j</mi>
     </msub>
    </msup>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mi>k</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>K</mi>
     </msubsup>
     <msup>
      <mi>e</mi>
      <msub>
       <mi>z</mi>
       <mi>k</mi>
      </msub>
     </msup>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>σ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐳</ci>
      <ci>𝐣</ci>
     </apply>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>e</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>z</ci>
       <ci>j</ci>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>k</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>K</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>e</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>z</ci>
        <ci>k</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma(\mathbf{z_{j}})=\frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}}
  </annotation>
 </semantics>
</math>

    for <em>j</em> = 1, ..., <em>K</em>.</p>

<p>The softmax function is the gradient-log-normalizer of the <a href="Categorical_distribution" title="wikilink">categorical</a> <a href="probability_distribution" title="wikilink">probability distribution</a>. For this reason, the softmax function is used in various probabilistic <a href="multiclass_classification" title="wikilink">multiclass classification</a> methods including <a href="multinomial_logistic_regression" title="wikilink">multinomial logistic regression</a>,<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> multiclass <a href="linear_discriminant_analysis" title="wikilink">linear discriminant analysis</a>, <a href="naive_Bayes_classifier" title="wikilink">naive Bayes classifiers</a> and <a href="artificial_neural_network" title="wikilink">artificial neural networks</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> Specifically, in multinomial logistic regression and linear discriminant analysis, the input to the function is the result of 

<math display="inline" id="Softmax_function:5">
 <semantics>
  <mi>K</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>K</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K
  </annotation>
 </semantics>
</math>

 distinct <a href="linear_function" title="wikilink">linear functions</a>, and the predicted probability for the 

<math display="inline" id="Softmax_function:6">
 <semantics>
  <mi>j</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>j</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   j
  </annotation>
 </semantics>
</math>

'th class given a sample vector 

<math display="inline" id="Softmax_function:7">
 <semantics>
  <mi>𝐱</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐱</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}
  </annotation>
 </semantics>
</math>

 is:</p>

<p>

<math display="block" id="Softmax_function:8">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo>=</mo>
    <mi>j</mi>
    <mo stretchy="false">|</mo>
    <mi>𝐱</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <msup>
     <mi>e</mi>
     <mrow>
      <msup>
       <mi>𝐱</mi>
       <mi>𝖳</mi>
      </msup>
      <msub>
       <mi>𝐰</mi>
       <mi>j</mi>
      </msub>
     </mrow>
    </msup>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mi>k</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>K</mi>
     </msubsup>
     <msup>
      <mi>e</mi>
      <mrow>
       <msup>
        <mi>𝐱</mi>
        <mi>𝖳</mi>
       </msup>
       <msub>
        <mi>𝐰</mi>
        <mi>k</mi>
       </msub>
      </mrow>
     </msup>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">y</csymbol>
     <eq></eq>
     <csymbol cd="unknown">j</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>e</ci>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>𝐱</ci>
        <ci>𝖳</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>𝐰</ci>
        <ci>j</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>k</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>K</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>e</ci>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>𝐱</ci>
         <ci>𝖳</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>𝐰</ci>
         <ci>k</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(y=j|\mathbf{x})=\frac{e^{\mathbf{x}^{\mathsf{T}}\mathbf{w}_{j}}}{\sum_{k=1}^%
{K}e^{\mathbf{x}^{\mathsf{T}}\mathbf{w}_{k}}}
  </annotation>
 </semantics>
</math>

</p>

<p>This can be seen as the <a href="function_composition" title="wikilink">composition</a> of 

<math display="inline" id="Softmax_function:9">
 <semantics>
  <mi>K</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>K</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K
  </annotation>
 </semantics>
</math>

 linear functions 

<math display="inline" id="Softmax_function:10">
 <semantics>
  <mrow>
   <mrow>
    <mi>𝐱</mi>
    <mo>↦</mo>
    <mrow>
     <mrow>
      <msup>
       <mi>𝐱</mi>
       <mi>𝖳</mi>
      </msup>
      <msub>
       <mi>𝐰</mi>
       <mn>1</mn>
      </msub>
     </mrow>
     <mo>,</mo>
     <mi mathvariant="normal">…</mi>
    </mrow>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mi>𝐱</mi>
    <mo>↦</mo>
    <mrow>
     <msup>
      <mi>𝐱</mi>
      <mi>𝖳</mi>
     </msup>
     <msub>
      <mi>𝐰</mi>
      <mi>K</mi>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <csymbol cd="latexml">maps-to</csymbol>
     <ci>𝐱</ci>
     <list>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>𝐱</ci>
        <ci>𝖳</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>𝐰</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>normal-…</ci>
     </list>
    </apply>
    <apply>
     <csymbol cd="latexml">maps-to</csymbol>
     <ci>𝐱</ci>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>𝐱</ci>
       <ci>𝖳</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>𝐰</ci>
       <ci>K</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}\mapsto\mathbf{x}^{\mathsf{T}}\mathbf{w}_{1},\ldots,\mathbf{x}%
\mapsto\mathbf{x}^{\mathsf{T}}\mathbf{w}_{K}
  </annotation>
 </semantics>
</math>

 and the softmax function.</p>
<h2 id="artificial-neural-networks">Artificial neural networks</h2>

<p>In neural network simulations, the softmax function is often implemented at the final layer of a network used for classification. Such networks are then trained under a <a href="log_loss" title="wikilink">log loss</a> (or <a class="uri" href="cross-entropy" title="wikilink">cross-entropy</a>) regime, giving a non-linear variant of multinomial logistic regression.</p>

<p>Since the function maps a vector and a specific index <em>i</em> to a real value, the derivative needs to take the index into account:</p>

<p>

<math display="block" id="Softmax_function:11">
 <semantics>
  <mrow>
   <mrow>
    <mfrac>
     <mo>∂</mo>
     <mrow>
      <mo>∂</mo>
      <msub>
       <mi>q</mi>
       <mi>k</mi>
      </msub>
     </mrow>
    </mfrac>
    <mi>σ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mtext>𝐪</mtext>
     <mo>,</mo>
     <mi>i</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mi mathvariant="normal">…</mi>
   <mo>=</mo>
   <mrow>
    <mi>σ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mtext>𝐪</mtext>
     <mo>,</mo>
     <mi>i</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>δ</mi>
       <mrow>
        <mi>i</mi>
        <mi>k</mi>
       </mrow>
      </msub>
      <mo>-</mo>
      <mrow>
       <mi>σ</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mtext>𝐪</mtext>
        <mo>,</mo>
        <mi>k</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <apply>
       <divide></divide>
       <partialdiff></partialdiff>
       <apply>
        <partialdiff></partialdiff>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>q</ci>
         <ci>k</ci>
        </apply>
       </apply>
      </apply>
      <ci>σ</ci>
      <interval closure="open">
       <mtext>q</mtext>
       <ci>i</ci>
      </interval>
     </apply>
     <ci>normal-…</ci>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>σ</ci>
      <interval closure="open">
       <mtext>q</mtext>
       <ci>i</ci>
      </interval>
      <apply>
       <minus></minus>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>δ</ci>
        <apply>
         <times></times>
         <ci>i</ci>
         <ci>k</ci>
        </apply>
       </apply>
       <apply>
        <times></times>
        <ci>σ</ci>
        <interval closure="open">
         <mtext>q</mtext>
         <ci>k</ci>
        </interval>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{\partial}{\partial q_{k}}\sigma(\textbf{q},i)=\dots=\sigma(\textbf{q},i)%
(\delta_{ik}-\sigma(\textbf{q},k))
  </annotation>
 </semantics>
</math>

</p>

<p>Here, the <a href="Kronecker_delta" title="wikilink">Kronecker delta</a> is used for simplicity (cf. the derivative of a <a href="sigmoid_function" title="wikilink">sigmoid function</a>, being expressed via the function itself).</p>

<p>See <a href="Multinomial_logit" title="wikilink">Multinomial logit</a> for a probability model which uses the softmax activation function.</p>
<h2 id="reinforcement-learning">Reinforcement learning</h2>

<p>In the field of <a href="reinforcement_learning" title="wikilink">reinforcement learning</a>, a softmax function can be used to convert values into action probabilities. The function commonly used is:<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>

<math display="block" id="Softmax_function:12">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>P</mi>
     <mi>t</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>a</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mfrac>
     <mrow>
      <mi>exp</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mrow>
         <msub>
          <mi>q</mi>
          <mi>t</mi>
         </msub>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>a</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>/</mo>
        <mi>τ</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mrow>
      <msubsup>
       <mo largeop="true" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>n</mi>
      </msubsup>
      <mrow>
       <mi>exp</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mrow>
          <msub>
           <mi>q</mi>
           <mi>t</mi>
          </msub>
          <mrow>
           <mo stretchy="false">(</mo>
           <mi>i</mi>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
         <mo>/</mo>
         <mi>τ</mi>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mfrac>
    <mtext>,</mtext>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>P</ci>
      <ci>t</ci>
     </apply>
     <ci>a</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <apply>
       <exp></exp>
       <apply>
        <divide></divide>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>q</ci>
          <ci>t</ci>
         </apply>
         <ci>a</ci>
        </apply>
        <ci>τ</ci>
       </apply>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>n</ci>
       </apply>
       <apply>
        <exp></exp>
        <apply>
         <divide></divide>
         <apply>
          <times></times>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>q</ci>
           <ci>t</ci>
          </apply>
          <ci>i</ci>
         </apply>
         <ci>τ</ci>
        </apply>
       </apply>
      </apply>
     </apply>
     <mtext>,</mtext>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P_{t}(a)=\frac{\exp(q_{t}(a)/\tau)}{\sum_{i=1}^{n}\exp(q_{t}(i)/\tau)}\text{,}
  </annotation>
 </semantics>
</math>

</p>

<p>where the action value 

<math display="inline" id="Softmax_function:13">
 <semantics>
  <mrow>
   <msub>
    <mi>q</mi>
    <mi>t</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>a</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>q</ci>
     <ci>t</ci>
    </apply>
    <ci>a</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   q_{t}(a)
  </annotation>
 </semantics>
</math>

 corresponds to the expected reward of following action a and 

<math display="inline" id="Softmax_function:14">
 <semantics>
  <mi>τ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>τ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tau
  </annotation>
 </semantics>
</math>

 is called a temperature parameter (in allusion to <a href="chemical_kinetics" title="wikilink">chemical kinetics</a>). For high temperatures (

<math display="inline" id="Softmax_function:15">
 <semantics>
  <mrow>
   <mi>τ</mi>
   <mo>→</mo>
   <mi mathvariant="normal">∞</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>τ</ci>
    <infinity></infinity>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tau\to\infty
  </annotation>
 </semantics>
</math>

), all actions have nearly the same probability and the lower the temperature, the more expected rewards affect the probability. For a low temperature (

<math display="inline" id="Softmax_function:16">
 <semantics>
  <mrow>
   <mi>τ</mi>
   <mo>→</mo>
   <msup>
    <mn>0</mn>
    <mo>+</mo>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>τ</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cn type="integer">0</cn>
     <plus></plus>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tau\to 0^{+}
  </annotation>
 </semantics>
</math>

), the probability of the action with the highest expected reward tends to 1.</p>
<h2 id="softmax-normalization">Softmax Normalization</h2>

<p>Sigmoidal or Softmax normalization is a way of reducing the influence of extreme values or outliers in the data without removing them from the dataset. It is useful given outlier data, which we wish to include in the dataset while still preserving the significance of data within a standard deviation of the mean. The data are nonlinearly transformed using a sigmoidal function, either the logistic sigmoid function:<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> 

<math display="inline" id="Softmax_function:17">
 <semantics>
  <mrow>
   <msubsup>
    <mi>x</mi>
    <mi>i</mi>
    <mo>′</mo>
   </msubsup>
   <mo>≡</mo>
   <mfrac>
    <mn>1</mn>
    <mrow>
     <mn>1</mn>
     <mo>+</mo>
     <msup>
      <mi>e</mi>
      <mrow>
       <mo>-</mo>
       <mrow>
        <mo stretchy="false">(</mo>
        <mfrac>
         <mrow>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo>-</mo>
          <msub>
           <mi>μ</mi>
           <mi>i</mi>
          </msub>
         </mrow>
         <msub>
          <mi>σ</mi>
          <mi>i</mi>
         </msub>
        </mfrac>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </msup>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <equivalent></equivalent>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>i</ci>
     </apply>
     <ci>normal-′</ci>
    </apply>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <apply>
      <plus></plus>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>e</ci>
       <apply>
        <minus></minus>
        <apply>
         <divide></divide>
         <apply>
          <minus></minus>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>i</ci>
          </apply>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>μ</ci>
           <ci>i</ci>
          </apply>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>σ</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}^{\prime}\equiv\frac{1}{1+e^{-(\frac{x_{i}-\mu_{i}}{\sigma_{i}})}}
  </annotation>
 </semantics>
</math>

</p>

<p>or the hyperbolic tangent function, <em>tanh</em>:<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> 

<math display="inline" id="Softmax_function:18">
 <semantics>
  <mrow>
   <msubsup>
    <mi>x</mi>
    <mi>i</mi>
    <mo>′</mo>
   </msubsup>
   <mo>≡</mo>
   <mfrac>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <msup>
      <mi>e</mi>
      <mrow>
       <mo>-</mo>
       <mrow>
        <mo stretchy="false">(</mo>
        <mfrac>
         <mrow>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo>-</mo>
          <msub>
           <mi>μ</mi>
           <mi>i</mi>
          </msub>
         </mrow>
         <msub>
          <mi>σ</mi>
          <mi>i</mi>
         </msub>
        </mfrac>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </msup>
    </mrow>
    <mrow>
     <mn>1</mn>
     <mo>+</mo>
     <msup>
      <mi>e</mi>
      <mrow>
       <mo>-</mo>
       <mrow>
        <mo stretchy="false">(</mo>
        <mfrac>
         <mrow>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo>-</mo>
          <msub>
           <mi>μ</mi>
           <mi>i</mi>
          </msub>
         </mrow>
         <msub>
          <mi>σ</mi>
          <mi>i</mi>
         </msub>
        </mfrac>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </msup>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <equivalent></equivalent>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>i</ci>
     </apply>
     <ci>normal-′</ci>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>e</ci>
       <apply>
        <minus></minus>
        <apply>
         <divide></divide>
         <apply>
          <minus></minus>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>i</ci>
          </apply>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>μ</ci>
           <ci>i</ci>
          </apply>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>σ</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
     <apply>
      <plus></plus>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>e</ci>
       <apply>
        <minus></minus>
        <apply>
         <divide></divide>
         <apply>
          <minus></minus>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>i</ci>
          </apply>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>μ</ci>
           <ci>i</ci>
          </apply>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>σ</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}^{\prime}\equiv\frac{1-e^{-(\frac{x_{i}-\mu_{i}}{\sigma_{i}})}}{1+e^{-(%
\frac{x_{i}-\mu_{i}}{\sigma_{i}})}}
  </annotation>
 </semantics>
</math>

</p>

<p>The sigmoid function limits the normalized data to values between 0 and 1. The sigmoid function is almost linear near the mean and has smooth nonlinearity at both extremes, ensuring that all data points are within a limited range. This maintains the resolution of most values within a standard deviation of the mean.</p>

<p>The hyperbolic tangent function, <em>tanh</em>, limits the normalized data to values between -1 and 1. The hyperbolic tangent function is almost linear near the mean, but has a slope of half that of the sigmoid function. Like sigmoid, it has <a href="Smoothness" title="wikilink">smooth</a>, <a class="uri" href="monotonic" title="wikilink">monotonic</a> nonlinearity at both extremes. Also, like the sigmoid the function, it remains <a class="uri" href="differentiable" title="wikilink">differentiable</a> everywhere and the sign of the derivative (slope) is unaffected by the normalization. This ensures that optimization and numerical integration algorithms can continue to rely on the derivative to estimate changes to the output (normalized value) that will be produced by changes to the input in the region near any <a class="uri" href="linearisation" title="wikilink">linearisation</a> point.</p>
<h2 id="references">References</h2>
<references>
</references>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Rectifier_(neural_networks)" title="wikilink">Softplus</a></li>
<li><a href="Multinomial_logistic_regression" title="wikilink">Multinomial logistic regression</a></li>
</ul>

<p>"</p>

<p><a href="Category:Computational_neuroscience" title="wikilink">Category:Computational neuroscience</a> <a href="Category:Log-linear_models" title="wikilink">Category:Log-linear models</a> <a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3">ai-faq <a href="http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html">What is a softmax activation function?</a><a href="#fnref3">↩</a></li>
<li id="fn4">Sutton, R. S. and Barto A. G. <em>Reinforcement Learning: An Introduction</em>. The MIT Press, Cambridge, MA, 1998.<a href="http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node17.html">Softmax Action Selection</a><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
</ol>
</section>
</body>
</html>
