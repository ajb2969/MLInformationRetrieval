   Prais‚ÄìWinsten estimation      Prais‚ÄìWinsten estimation   In econometrics , Prais‚ÄìWinsten estimation is a procedure meant to take care of the serial correlation of type AR(1) in a linear model . Conceived by Sigbert Prais and Christopher Winsten in 1954, it is a modification of Cochrane‚ÄìOrcutt estimation in the sense that it does not lose the first observation and leads to more efficiency as a result.  Theory  Consider the model        y  t   =   Œ±  +    X  t   Œ≤   +   Œµ  t     ,       subscript  y  t     Œ±     subscript  X  t   Œ≤    subscript  Œµ  t      y_{t}=\alpha+X_{t}\beta+\varepsilon_{t},\,     where    y  t     subscript  y  t    y_{t}   is the time series of interest at time t ,   Œ≤   Œ≤   \beta   is a vector of coefficients,    X  t     subscript  X  t    X_{t}   is a matrix of explanatory variables , and    Œµ  t     subscript  Œµ  t    \varepsilon_{t}   is the error term . The error term can be serially correlated over time      Œµ  t   =    œÅ   Œµ   t  -  1     +   e  t     ,    |  œÅ  |   <  1      formulae-sequence     subscript  Œµ  t       œÅ   subscript  Œµ    t  1      subscript  e  t         œÅ   1     \varepsilon_{t}=\rho\varepsilon_{t-1}+e_{t},\ |\rho|<1   and    e  t     subscript  e  t    e_{t}   is a white noise. In addition to the Cochrane‚ÄìOrcutt procedure transformation, which is         y  t   -   œÅ   y   t  -  1      =    Œ±   (   1  -  œÅ   )    +   Œ≤   (    X  t   -   œÅ   X   t  -  1      )    +   e  t     .         subscript  y  t     œÅ   subscript  y    t  1          Œ±    1  œÅ      Œ≤     subscript  X  t     œÅ   subscript  X    t  1        subscript  e  t      y_{t}-\rho y_{t-1}=\alpha(1-\rho)+\beta(X_{t}-\rho X_{t-1})+e_{t}.\,     for t=2,3,...,T, Prais-Winsten procedure makes a reasonable transformation for t=1 in the following form          1  -   œÅ  2      y  1    =    Œ±    1  -   œÅ  2      +    (     1  -   œÅ  2      X  1    )   Œ≤   +     1  -   œÅ  2      Œµ  1      .            1   superscript  œÅ  2      subscript  y  1        Œ±      1   superscript  œÅ  2              1   superscript  œÅ  2      subscript  X  1    Œ≤         1   superscript  œÅ  2      subscript  Œµ  1       \sqrt{1-\rho^{2}}y_{1}=\alpha\sqrt{1-\rho^{2}}+\left(\sqrt{1-\rho^{2}}X_{1}%
 \right)\beta+\sqrt{1-\rho^{2}}\varepsilon_{1}.\,     Then the usual least squares estimation is done.  Estimation procedure  To do the estimation in a compact way it is directive to look at the auto-covariance function of the error term considered in the model above:         cov   (   Œµ  t   ,   Œµ   t  +  h    )    =    œÅ  h    1  -   œÅ  2      ,    for  h   =   0  ,   ¬±  1   ,   ¬±  2   ,   ‚Ä¶      .     formulae-sequence      cov    subscript  Œµ  t    subscript  Œµ    t  h         superscript  œÅ  h     1   superscript  œÅ  2          for  h    0   plus-or-minus  1    plus-or-minus  2   normal-‚Ä¶      \mathrm{cov}(\varepsilon_{t},\varepsilon_{t+h})=\frac{\rho^{h}}{1-\rho^{2}},%
 \text{ for }h=0,\pm 1,\pm 2,\dots\,.     Now is easy to see that the variance‚Äìcovariance matrix ,   ùõÄ   ùõÄ   \mathbf{\Omega}   , of the model is       ùõÄ  =   [       1   1  -   œÅ  2          œÅ   1  -   œÅ  2           œÅ  2    1  -   œÅ  2        ‚ãØ       œÅ   T  -  1     1  -   œÅ  2            œÅ   1  -   œÅ  2          1   1  -   œÅ  2          œÅ   1  -   œÅ  2        ‚ãØ       œÅ   T  -  2     1  -   œÅ  2             œÅ  2    1  -   œÅ  2          œÅ   1  -   œÅ  2          1   1  -   œÅ  2        ‚ãØ       œÅ   T  -  2     1  -   œÅ  2          ‚ãÆ    ‚ãÆ    ‚ãÆ    ‚ã±    ‚ãÆ         œÅ   T  -  1     1  -   œÅ  2           œÅ   T  -  2     1  -   œÅ  2           œÅ   T  -  3     1  -   œÅ  2        ‚ãØ      1   1  -   œÅ  2         ]    .      ùõÄ      1    1   superscript  œÅ  2       œÅ    1   superscript  œÅ  2        superscript  œÅ  2     1   superscript  œÅ  2     normal-‚ãØ     superscript  œÅ    T  1      1   superscript  œÅ  2         œÅ    1   superscript  œÅ  2       1    1   superscript  œÅ  2       œÅ    1   superscript  œÅ  2     normal-‚ãØ     superscript  œÅ    T  2      1   superscript  œÅ  2          superscript  œÅ  2     1   superscript  œÅ  2       œÅ    1   superscript  œÅ  2       1    1   superscript  œÅ  2     normal-‚ãØ     superscript  œÅ    T  2      1   superscript  œÅ  2       normal-‚ãÆ  normal-‚ãÆ  normal-‚ãÆ  normal-‚ã±  normal-‚ãÆ       superscript  œÅ    T  1      1   superscript  œÅ  2        superscript  œÅ    T  2      1   superscript  œÅ  2        superscript  œÅ    T  3      1   superscript  œÅ  2     normal-‚ãØ    1    1   superscript  œÅ  2         \mathbf{\Omega}=\begin{bmatrix}\frac{1}{1-\rho^{2}}&\frac{\rho}{1-\rho^{2}}&%
 \frac{\rho^{2}}{1-\rho^{2}}&\cdots&\frac{\rho^{T-1}}{1-\rho^{2}}\\
 \frac{\rho}{1-\rho^{2}}&\frac{1}{1-\rho^{2}}&\frac{\rho}{1-\rho^{2}}&\cdots&%
 \frac{\rho^{T-2}}{1-\rho^{2}}\\
 \frac{\rho^{2}}{1-\rho^{2}}&\frac{\rho}{1-\rho^{2}}&\frac{1}{1-\rho^{2}}&%
 \cdots&\frac{\rho^{T-2}}{1-\rho^{2}}\\
 \vdots&\vdots&\vdots&\ddots&\vdots\\
 \frac{\rho^{T-1}}{1-\rho^{2}}&\frac{\rho^{T-2}}{1-\rho^{2}}&\frac{\rho^{T-3}}{%
 1-\rho^{2}}&\cdots&\frac{1}{1-\rho^{2}}\end{bmatrix}.   Now having   œÅ   œÅ   \rho   (or an estimate of it), we see that,        Œò  ^   =     (    ùêô  ‚Ä≤    ùõÄ   -  1    ùêô   )    -  1     (    ùêô  ‚Ä≤    ùõÄ   -  1    ùêò   )     ,       normal-^  normal-Œò      superscript     superscript  ùêô  normal-‚Ä≤    superscript  ùõÄ    1    ùêô     1       superscript  ùêô  normal-‚Ä≤    superscript  ùõÄ    1    ùêò      \hat{\Theta}=(\mathbf{Z}^{\prime}\mathbf{\Omega}^{-1}\mathbf{Z})^{-1}(\mathbf{%
 Z}^{\prime}\mathbf{\Omega}^{-1}\mathbf{Y}),\,     where   ùêô   ùêô   \mathbf{Z}   is a matrix of observations on the independent variable ( X t , t =¬†1,¬†2,¬†..., T ) including a vector of ones,   ùêò   ùêò   \mathbf{Y}   is a vector stacking the observations on the dependent variable ( X t , t =¬†1,¬†2,¬†..., T ) and    Œò  ^     normal-^  normal-Œò    \hat{\Theta}   includes the model parameters.  Note  To see why the initial observation assumption stated by Prais‚ÄìWinsten (1954) is reasonable, considering the mechanics of general least square estimation procedure sketched above is helpful. The inverse of   ùõÄ   ùõÄ   \mathbf{\Omega}   can be decomposed as     ùõÄ   -  1    =    ùêÜ  ‚Ä≤   ùêÜ        superscript  ùõÄ    1       superscript  ùêÜ  normal-‚Ä≤   ùêÜ     \mathbf{\Omega}^{-1}=\mathbf{G}^{\prime}\mathbf{G}   with       ùêÜ  =   [       1  -   œÅ  2       0    0    ‚ãØ    0       -  œÅ     1    0    ‚ãØ    0      0     -  œÅ     1    ‚ãØ    0      ‚ãÆ    ‚ãÆ    ‚ãÆ    ‚ã±    ‚ãÆ      0    0    0    ‚ãØ    1     ]    .      ùêÜ        1   superscript  œÅ  2     0  0  normal-‚ãØ  0      œÅ   1  0  normal-‚ãØ  0    0    œÅ   1  normal-‚ãØ  0    normal-‚ãÆ  normal-‚ãÆ  normal-‚ãÆ  normal-‚ã±  normal-‚ãÆ    0  0  0  normal-‚ãØ  1      \mathbf{G}=\begin{bmatrix}\sqrt{1-\rho^{2}}&0&0&\cdots&0\\
 -\rho&1&0&\cdots&0\\
 0&-\rho&1&\cdots&0\\
 \vdots&\vdots&\vdots&\ddots&\vdots\\
 0&0&0&\cdots&1\end{bmatrix}.   A pre-multiplication of model in a matrix notation with this matrix gives the transformed model of Prais‚ÄìWinsten.  Restrictions  The error term is still restricted to be of an AR(1) type. If   œÅ   œÅ   \rho   is not known, a recursive procedure may be used to make the estimation feasible. See Cochrane‚ÄìOrcutt estimation .  References        "  Category:Econometrics  Category:Regression with time series structure   