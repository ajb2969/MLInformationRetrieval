   Symmetric matrix      Symmetric matrix   In linear algebra , a symmetric matrix is a square matrix that is equal to its transpose . Formally, matrix A is symmetric if       A  =   A  ⊤    .      A   superscript  A  top     A=A^{\top}.     Because equal matrices have equal dimensions, only square matrices can be symmetric.  The entries of a symmetric matrix are symmetric with respect to the main diagonal . So if the entries are written as A = ( a ij ), then a ij = a ji , for all indices i and j .  The following 3×3 matrix is symmetric:       [     1    7    3      7    4     -  5       3     -  5     6     ]   .      1  7  3    7  4    5     3    5   6     \begin{bmatrix}1&7&3\\
 7&4&-5\\
 3&-5&6\end{bmatrix}.     Every square diagonal matrix is symmetric, since all off-diagonal entries are zero. Similarly, each diagonal element of a skew-symmetric matrix must be zero, since each is its own negative.  In linear algebra, a real symmetric matrix represents a self-adjoint operator 1 over a real  inner product space . The corresponding object for a complex inner product space is a Hermitian matrix with complex-valued entries, which is equal to its conjugate transpose . Therefore, in linear algebra over the complex numbers, it is often assumed that a symmetric matrix refers to one which has real-valued entries. Symmetric matrices appear naturally in a variety of applications, and typical numerical linear algebra software makes special accommodations for them.  Properties  The sum and difference of two symmetric matrices is again symmetric, but this is not always true for the product : given symmetric matrices A and B , then AB is symmetric if and only if A and B  commute , i.e., if AB = BA . So for integer n , A n is symmetric if A is symmetric. If A −1 exists, it is symmetric if and only if A is symmetric.  Let Mat n denote the space of  matrices. A symmetric n × n matrix is determined by n ( n + 1)/2 scalars (the number of entries on or above the main diagonal ). Similarly, a skew-symmetric matrix is determined by n ( n − 1)/2 scalars (the number of entries above the main diagonal). If Sym n denotes the space of  symmetric matrices and Skew n the space of  skew-symmetric matrices then and }, i.e.        Mat  n   =    Sym  n   ⊕   Skew  n     ,       subscript  Mat  n    direct-sum   subscript  Sym  n    subscript  Skew  n      \mbox{Mat}_{n}=\mbox{Sym}_{n}\oplus\mbox{Skew}_{n},   where ⊕ denotes the direct sum . Let then       X  =     1  2    (   X  +   X  ⊤    )    +    1  2    (   X  -   X  ⊤    )      .      X        1  2     X   superscript  X  top         1  2     X   superscript  X  top        X=\frac{1}{2}(X+X^{\top})+\frac{1}{2}(X-X^{\top}).   Notice that and This is true for every square matrix  X with entries from any field whose characteristic is different from 2.  Any matrix congruent to a symmetric matrix is again symmetric: if X is a symmetric matrix then so is AXA T for any matrix A . A symmetric matrix is necessarily a normal matrix .  Real symmetric matrices  Denote by    ⟨  ⋅  ,  ⋅  ⟩     normal-⋅  normal-⋅    \langle\cdot,\cdot\rangle   the standard inner product on R n . The real n -by- n matrix A is symmetric if and only if         ⟨   A  x   ,  y  ⟩   =    ⟨  x  ,   A  y   ⟩    ∀  x     ,   y  ∈   ℝ  n     .     formulae-sequence       A  x   y     x    A  y     for-all  x       y   superscript  ℝ  n      \langle Ax,y\rangle=\langle x,Ay\rangle\quad\forall x,y\in\mathbb{R}^{n}.     Since this definition is independent of the choice of basis , symmetry is a property that depends only on the linear operator A and a choice of inner product . This characterization of symmetry is useful, for example, in differential geometry , for each tangent space to a manifold may be endowed with an inner product, giving rise to what is called a Riemannian manifold . Another area where this formulation is used is in Hilbert spaces .  The finite-dimensional spectral theorem says that any symmetric matrix whose entries are real can be diagonalized by an orthogonal matrix . More explicitly: For every symmetric real matrix A there exists a real orthogonal matrix Q such that D = Q T AQ is a diagonal matrix . Every symmetric matrix is thus, up to choice of an orthonormal basis , a diagonal matrix.  If A and B are n × n real symmetric matrices that commute, then they can be simultaneously diagonalized: there exists a basis of    ℝ  n     superscript  ℝ  n    \mathbb{R}^{n}   such that every element of the basis is an eigenvector for both A and B .  Every real symmetric matrix is Hermitian , and therefore all its eigenvalues are real. (In fact, the eigenvalues are the entries in the diagonal matrix D (above), and therefore D is uniquely determined by A up to the order of its entries.) Essentially, the property of being symmetric for real matrices corresponds to the property of being Hermitian for complex matrices.  Complex symmetric matrices  A complex symmetric matrix can be `diagonalized' using a unitary matrix : thus if A is a complex symmetric matrix, there is a unitary matrix U such that UAU t is a real diagonal matrix. This result is referred to as the Autonne–Takagi factorization . It was originally proved by Leon Autonne (1915) and Teiji Takagi (1925) and rediscovered with different proofs by several other mathematicians. 2 See:      , Lemma 1, page 12      In fact the matrix B = A * A is Hermitian and non-negative, so there is a unitary matrix V such that     V  †   B  V       superscript  V  normal-†   B  V    V^{\dagger}BV   is diagonal with non-negative real entries. Thus C = V t AV is complex symmetric with C * C real. Writing C = X + iY with X and Y real symmetric matrices, C * C = X 2 − Y 2 + i ( XY − YX ). Thus XY = YX . Since X and Y commute, there is a real orthogonal matrix W such that WXW t and WYW t are diagonal. Setting U = WV t , the matrix UAU t is diagonal. Post-multiplying U by a diagonal matrix the diagonal entries can be taken to be non-negative. Since their squares are the eigenvalues of A * A , they coincide with the singular values of A .  Decomposition  Using the Jordan normal form , one can prove that every square real matrix can be written as a product of two real symmetric matrices, and every square complex matrix can be written as a product of two complex symmetric matrices. 3  Every real non-singular matrix can be uniquely factored as the product of an orthogonal matrix and a symmetric positive definite matrix , which is called a polar decomposition . Singular matrices can also be factored, but not uniquely.  Cholesky decomposition states that every real positive-definite symmetric matrix A is a product of a lower-triangular matrix L and its transpose,    A  =   L   L  T        A    L   superscript  L  T      A=LL^{T}   . If the matrix is symmetric indefinite, it may be still decomposed as     P  A   P  T    =   L  T   L  T          P  A   superscript  P  T      L  T   superscript  L  T      PAP^{T}=LTL^{T}   where   P   P   P   is a permutation matrix (arising from the need to pivot ),   L   L   L   a lower unit triangular matrix,   T   T   T   a symmetric tridiagonal matrix, and   D   D   D   a direct sum of symmetric 1×1 and 2×2 blocks. 4  A complex symmetric matrix need not be diagonalizable by similarity; every real symmetric matrix is diagonalizable by a real orthogonal similarity.  Every complex symmetric matrix A can be diagonalized by unitary congruence      A  =   Q  Λ   Q  ⊤        A    Q  normal-Λ   superscript  Q  top      A=Q\Lambda Q^{\top}   where Q is an unitary matrix . If A is real, the matrix Q is a real orthogonal matrix , (the columns of which are eigenvectors of A ), and Λ is real and diagonal (having the eigenvalues of A on the diagonal). To see orthogonality, suppose   x   x   x   and   y   y   y   are eigenvectors corresponding to distinct eigenvalues    λ  1     subscript  λ  1    \lambda_{1}   ,    λ  2     subscript  λ  2    \lambda_{2}   . Then        λ  1    ⟨  x  ,  y  ⟩    =   ⟨   A  x   ,  y  ⟩   =   ⟨  x  ,   A  y   ⟩   =    λ  2    ⟨  x  ,  y  ⟩             subscript  λ  1    x  y       A  x   y         x    A  y            subscript  λ  2    x  y       \lambda_{1}\langle x,y\rangle=\langle Ax,y\rangle=\langle x,Ay\rangle=\lambda_%
 {2}\langle x,y\rangle   so that if     ⟨  x  ,  y  ⟩   ≠  0       x  y   0    \langle x,y\rangle\neq 0   then     λ  1   =   λ  2        subscript  λ  1    subscript  λ  2     \lambda_{1}=\lambda_{2}   , a contradiction; hence     ⟨  x  ,  y  ⟩   =  0       x  y   0    \langle x,y\rangle=0   .  Hessian  Symmetric n -by- n matrices of real functions appear as the Hessians of twice continuously differentiable functions of n real variables.  Every quadratic form  q on R n can be uniquely written in the form q ( x ) = x T A x with a symmetric n -by- n matrix A . Because of the above spectral theorem, one can then say that every quadratic form, up to the choice of an orthonormal basis of R n , "looks like"       q   (   x  1   ,  …  ,   x  n   )    =    ∑   i  =  1   n     λ  i    x  i  2           q    subscript  x  1   normal-…   subscript  x  n       superscript   subscript     i  1    n      subscript  λ  i    superscript   subscript  x  i   2       q(x_{1},\ldots,x_{n})=\sum_{i=1}^{n}\lambda_{i}x_{i}^{2}   with real numbers λ i . This considerably simplifies the study of quadratic forms, as well as the study of the level sets { x : q ( x ) = 1} which are generalizations of conic sections .  This is important partly because the second-order behavior of every smooth multi-variable function is described by the quadratic form belonging to the function's Hessian; this is a consequence of Taylor's theorem .  Symmetrizable matrix  An n -by- n matrix A is said to be symmetrizable if there exist an invertible diagonal matrix  D and symmetric matrix S such that  The transpose of a symmetrizable matrix is symmetrizable, since and  is symmetric. A matrix is symmetrizable if and only if the following conditions are met:        a   i  j    =  0       subscript  a    i  j    0    a_{ij}=0   implies     a   j  i    =  0       subscript  a    j  i    0    a_{ji}=0   for all     1  ≤  i  ≤  j  ≤  n   .        1  i       j       n     1\leq i\leq j\leq n.           a    i  1    i  2      a    i  2    i  3     …   a    i  k    i  1      =    a    i  2    i  1      a    i  3    i  2     …   a    i  1    i  k             subscript  a     subscript  i  1    subscript  i  2      subscript  a     subscript  i  2    subscript  i  3     normal-…   subscript  a     subscript  i  k    subscript  i  1         subscript  a     subscript  i  2    subscript  i  1      subscript  a     subscript  i  3    subscript  i  2     normal-…   subscript  a     subscript  i  1    subscript  i  k        a_{i_{1}i_{2}}a_{i_{2}i_{3}}\dots a_{i_{k}i_{1}}=a_{i_{2}i_{1}}a_{i_{3}i_{2}}%
 \dots a_{i_{1}i_{k}}   for any finite sequence     (   i  1   ,   i  2   ,  …  ,   i  k   )   .      subscript  i  1    subscript  i  2   normal-…   subscript  i  k     (i_{1},i_{2},\dots,i_{k}).      See also  Other types of symmetry or pattern in square matrices have special names; see for example:   Antimetric matrix  Centrosymmetric matrix  Circulant matrix  Covariance matrix  Coxeter matrix  Hankel matrix  Hilbert matrix  Persymmetric matrix  Skew-symmetric matrix  Sylvester's law of inertia  Toeplitz matrix   See also symmetry in mathematics .  Notes  References     External links    A brief introduction and proof of eigenvalue properties of the real symmetric matrix   "  Category:Matrices     ↩  ↩  ↩  ↩     