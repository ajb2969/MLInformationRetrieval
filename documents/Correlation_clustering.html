<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1995">Correlation clustering</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Correlation clustering</h1>
<hr/>

<p>Clustering is the problem of partitioning data points into groups based on their similarity. <strong>Correlation clustering</strong> provides a method for clustering a set of objects into the optimum number of clusters without specifying that number in advance.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="description-of-the-problem">Description of the problem</h2>

<p>In <a href="machine_learning" title="wikilink">machine learning</a>, <strong>correlation clustering</strong> or <strong>cluster editing</strong> operates in a scenario where the relationships between the objects are known instead of the actual representations of the objects. For example, given a <a href="weighted_graph" title="wikilink">weighted graph</a> 

<math display="inline" id="Correlation_clustering:0">
 <semantics>
  <mrow>
   <mi>G</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>V</mi>
    <mo>,</mo>
    <mi>E</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>G</ci>
    <interval closure="open">
     <ci>V</ci>
     <ci>E</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G=(V,E)
  </annotation>
 </semantics>
</math>

 where the edge weight indicates whether two nodes are similar (positive edge weight) or different (negative edge weight), the task is to find a clustering that either maximizes agreements (sum of positive edge weights within a cluster plus the absolute value of the sum of negative edge weights between clusters) or minimizes disagreements (absolute value of the sum of negative edge weights within a cluster plus the sum of positive edge weights across clusters). Unlike other clustering algorithms this does not require <a href="Determining_the_number_of_clusters_in_a_data_set" title="wikilink">choosing the number of clusters</a> 

<math display="inline" id="Correlation_clustering:1">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

 in advance because the objective, to minimize the sum of weights of the cut edges, is independent of the number of clusters.</p>

<p>It may not be possible to find a perfect clustering, where all similar items are in a cluster while all dissimilar ones are in different clusters. If the graph indeed admits a perfect clustering, then simply deleting all the negative edges and finding the connected components in the remaining graph will return the required clusters.</p>

<p>But, in general a graph may not have a perfect clustering. For example, given nodes <em>a,b,c</em> such that <em>a,b</em> and <em>a,c</em> are similar while <em>b,c</em> are dissimilar, a perfect clustering is not possible. In such cases, the task is to find a clustering that maximizes the number of agreements (number of + edges inside clusters minus the number of - edges between clusters) or minimizes the number of disagreements (the number of - edges inside clusters minus the number of + edges between clusters). This problem of maximizing the agreements is NP-complete (multiway cut problem reduces to maximizing weighted agreements and the problem of partitioning into triangles<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> can be reduced to the unweighted version).</p>
<h2 id="algorithms">Algorithms</h2>

<p>Bansal et al.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> discuss the NP-completeness proof and also present both a constant factor approximation algorithm and <a href="polynomial-time_approximation_scheme" title="wikilink">polynomial-time approximation scheme</a> to find the clusters in this setting. Ailon et al.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> propose a randomized 3-<a href="approximation_algorithm" title="wikilink">approximation algorithm</a> for the same problem.</p>

<p><code>
CC-Pivot(G=(V,E<sup>+</sup>,E<sup>−</sup>))
    Pick random pivot i ∈ V
    Set <math>C=\{i\}</math>, V'=Ø
    For all j ∈ V, j ≠ i;
        If (i,j) ∈ E<sup>+</sup> then
             Add j to C
        Else (If (i,j) ∈ E<sup>−</sup>)
             Add j to V'
    Let G' be the subgraph induced by V'
    Return clustering C,CC-Pivot(G')
</code></p>

<p>The authors show that the above algorithm is a 3-<a href="approximation_algorithm" title="wikilink">approximation algorithm</a> for correlation clustering.</p>

<p>Karpinski and Schudy<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> proved existence of a polynomial time approximation scheme (PTAS) for that problem on complete graphs and fixed number of clusters.</p>
<h2 id="optimal-number-of-clusters">Optimal number of clusters</h2>

<p>In 2011, it was shown by Bagon and Galun<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> that the optimization of the correlation clustering functional is closely related to well known discrete optimization methods. In their work they proposed a probabilistic analysis of the underlying implicit model that allows the correlation clustering functional to estimate the underlying number of clusters. This analysis suggests the functional assumes a uniform prior over all possible partitions regardless of their number of clusters. Thus, a non-uniform prior over the number of clusters emerges.</p>

<p>Several discrete optimization algorithms are proposed in this work that scales gracefully with the number of elements (experiments show results with more than 100,000 variables). The work of Bagon and Galun also evaluated the effectiveness of the recovery of the underlying number of clusters in several applications.</p>
<h2 id="correlation-clustering-data-mining">Correlation clustering (data mining)</h2>

<p><strong>Correlation clustering</strong> also relates to a different task, where <a href="correlation" title="wikilink">correlations</a> among attributes of <a href="feature_vector" title="wikilink">feature vectors</a> in a <a href="high-dimensional_space" title="wikilink">high-dimensional space</a> are assumed to exist guiding the <a href="cluster_analysis" title="wikilink">clustering process</a>. These correlations may be different in different clusters, thus a global <a class="uri" href="decorrelation" title="wikilink">decorrelation</a> cannot reduce this to traditional (uncorrelated) clustering.</p>

<p>Correlations among subsets of attributes result in different spatial shapes of clusters. Hence, the similarity between cluster objects is defined by taking into account the local correlation patterns. With this notion, the term has been introduced in <a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> simultaneously with the notion discussed above. Different methods for correlation clustering of this type are discussed in,<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> the relationship to different types of clustering is discussed in,<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> see also <a href="Clustering_high-dimensional_data" title="wikilink">Clustering high-dimensional data</a>.</p>

<p>Correlation clustering (according to this definition) can be shown to be closely related to <a class="uri" href="biclustering" title="wikilink">biclustering</a>. As in biclustering, the goal is to identify groups of objects that share a correlation in some of their attributes; where the correlation is usually typical for the individual clusters.</p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Cluster_analysis" title="wikilink">Category:Cluster analysis</a> <a href="Category:Computational_problems_in_graph_theory" title="wikilink">Category:Computational problems in graph theory</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Becker, Hila, <a href="http://www.cs.columbia.edu/~hila/clustering.pdf">"A Survey of Correlation Clustering", 5 May 2005</a><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6">Bagon, S.; Galun, M. (2011) <a href="http://arxiv.org/pdf/1112.2903v1.pdf">"Large Scale Correlation Clustering Optimization" </a><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
</ol>
</section>
</body>
</html>
