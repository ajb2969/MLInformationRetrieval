   Frankâ€“Wolfe algorithm      Frankâ€“Wolfe algorithm   The Frankâ€“Wolfe algorithm is an iterative  first-order  optimization  algorithm for constrained  convex optimization . Also known as the conditional gradient method , 1  reduced gradient algorithm and the convex combination algorithm , the method was originally proposed by Marguerite Frank and Philip Wolfe inÂ 1956. 2 In each iteration, the Frankâ€“Wolfe algorithm considers a linear approximation of the objective function, and moves slightly towards a minimizer of this linear function (taken over the same domain).  Problem statement  Suppose   ğ’Ÿ   ğ’Ÿ   \mathcal{D}   is a compact  convex set in a vector space and    f  :   ğ’Ÿ  â†’  â„      normal-:  f   normal-â†’  ğ’Ÿ  â„     f\colon\mathcal{D}\to\mathbb{R}   is a convex  differentiable  real-valued function . The Frankâ€“Wolfe algorithm solves the optimization problem   Minimize    f   (  ğ±  )       f  ğ±    f(\mathbf{x})     subject to    ğ±  âˆˆ  ğ’Ÿ      ğ±  ğ’Ÿ    \mathbf{x}\in\mathcal{D}   .   Algorithm  (Figure)  A step of the Frankâ€“Wolfe algorithm    Initialization: Let    k  â†  0     normal-â†  k  0    k\leftarrow 0   , and let     ğ±  0      subscript  ğ±  0    \mathbf{x}_{0}\!   be any point in   ğ’Ÿ   ğ’Ÿ   \mathcal{D}   .    Step 1.  Direction-finding subproblem: Find    ğ¬  k     subscript  ğ¬  k    \mathbf{s}_{k}   solving  Minimize     ğ¬  T    âˆ‡  f    (   ğ±  k   )        superscript  ğ¬  T    normal-âˆ‡  f    subscript  ğ±  k     \mathbf{s}^{T}\nabla f(\mathbf{x}_{k})     Subject to    ğ¬  âˆˆ  ğ’Ÿ      ğ¬  ğ’Ÿ    \mathbf{s}\in\mathcal{D}       (Interpretation: Minimize the linear approximation of the problem given by the first-order Taylor approximation of   f   f   f   around     ğ±  k      subscript  ğ±  k    \mathbf{x}_{k}\!   .)     Step 2.  Step size determination: Set    Î³  â†   2   k  +  2       normal-â†  Î³    2    k  2      \gamma\leftarrow\frac{2}{k+2}   , or alternatively find   Î³   Î³   \gamma   that minimizes    f   (    ğ±  k   +   Î³   (    ğ¬  k   -   ğ±  k    )     )       f     subscript  ğ±  k     Î³     subscript  ğ¬  k    subscript  ğ±  k        f(\mathbf{x}_{k}+\gamma(\mathbf{s}_{k}-\mathbf{x}_{k}))   subject to    0  â‰¤  Î³  â‰¤  1        0  Î³       1     0\leq\gamma\leq 1   .    Step 3.  Update: Let     ğ±   k  +  1    â†    ğ±  k   +   Î³   (    ğ¬  k   -   ğ±  k    )        normal-â†   subscript  ğ±    k  1       subscript  ğ±  k     Î³     subscript  ğ¬  k    subscript  ğ±  k        \mathbf{x}_{k+1}\leftarrow\mathbf{x}_{k}+\gamma(\mathbf{s}_{k}-\mathbf{x}_{k})   , let    k  â†   k  +  1      normal-â†  k    k  1     k\leftarrow k+1   and go to Step 1.   Properties  While competing methods such as gradient descent for constrained optimization require a projection step back to the feasible set in each iteration, the Frankâ€“Wolfe algorithm only needs the solution of a linear problem over the same set in each iteration, and automatically stays in the feasible set.  The convergence of the Frankâ€“Wolfe algorithm is sublinear in general: the error to the optimum is    O   (   1  /  k   )       O    1  k     O(1/k)   after k iterations. The same convergence rate can also be shown if the sub-problems are only solved approximately. 3  The iterates of the algorithm can always be represented as a sparse convex combination of the extreme points of the feasible set, which has helped to the popularity of the algorithm for sparse greedy optimization in machine learning and signal processing problems, 4 as well as for example the optimization of minimumâ€“cost flows in transportation networks . 5  If the feasible set is given by a set of linear constraints, then the subproblem to be solved in each iteration becomes a linear program .  While the worst-case convergence rate with    O   (   1  /  k   )       O    1  k     O(1/k)   can not be improved in general, faster convergence can be obtained for special problem classes, such as some strongly convex problems. 6  Lower bounds on the solution value, and primal-dual analysis  Since   f   f   f   is convex,    f   (  ğ²  )       f  ğ²    f(\mathbf{y})   is always above the tangent plane of   f   f   f   at any point    ğ±  âˆˆ  ğ’Ÿ      ğ±  ğ’Ÿ    \mathbf{x}\in\mathcal{D}   :       f   (  ğ²  )    â‰¥    f   (  ğ±  )    +     (   ğ²  -  ğ±   )   T    âˆ‡  f    (  ğ±  )           f  ğ²       f  ğ±      superscript    ğ²  ğ±   T    normal-âˆ‡  f   ğ±      f(\mathbf{y})\geq f(\mathbf{x})+(\mathbf{y}-\mathbf{x})^{T}\nabla f(\mathbf{x})     This holds in particular for the (unknown) optimal solution    ğ±  *     superscript  ğ±     \mathbf{x}^{*}   . The best lower bound with respect to a given point   ğ±   ğ±   \mathbf{x}   is given by       f   (   ğ±  *   )    â‰¥      min   ğ²  âˆˆ  D    f    (  ğ±  )    +     (   ğ²  -  ğ±   )   T    âˆ‡  f    (  ğ±  )     =     f   (  ğ±  )    -    ğ±  T    âˆ‡  f    (  ğ±  )     +     min   ğ²  âˆˆ  D     ğ²  T     âˆ‡  f    (  ğ±  )             f   superscript  ğ±           subscript     ğ²  D    f   ğ±      superscript    ğ²  ğ±   T    normal-âˆ‡  f   ğ±               f  ğ±      superscript  ğ±  T    normal-âˆ‡  f   ğ±        subscript     ğ²  D     superscript  ğ²  T     normal-âˆ‡  f   ğ±       f(\mathbf{x}^{*})\geq\min_{\mathbf{y}\in D}f(\mathbf{x})+(\mathbf{y}-\mathbf{x%
 })^{T}\nabla f(\mathbf{x})=f(\mathbf{x})-\mathbf{x}^{T}\nabla f(\mathbf{x})+%
 \min_{\mathbf{y}\in D}\mathbf{y}^{T}\nabla f(\mathbf{x})     The latter optimization problem is solved in every iteration of the Frankâ€“Wolfe algorithm, therefore the solution    ğ¬  k     subscript  ğ¬  k    \mathbf{s}_{k}   of the direction-finding subproblem of the   k   k   k   -th iteration can be used to determine increasing lower bounds    l  k     subscript  l  k    l_{k}   during each iteration by setting     l  0   =   -  âˆ        subscript  l  0         l_{0}=-\infty   and       l  k   :=   max   (   l   k  -  1    ,    f   (   ğ±  k   )    +     (    ğ¬  k   -   ğ±  k    )   T    âˆ‡  f    (   ğ±  k   )     )       assign   subscript  l  k      subscript  l    k  1        f   subscript  ğ±  k       superscript     subscript  ğ¬  k    subscript  ğ±  k    T    normal-âˆ‡  f    subscript  ğ±  k        l_{k}:=\max(l_{k-1},f(\mathbf{x}_{k})+(\mathbf{s}_{k}-\mathbf{x}_{k})^{T}%
 \nabla f(\mathbf{x}_{k}))   Such lower bounds on the unknown optimal value are important in practice because they can be used as a stopping criterion, and give an efficient certificate of the approximation quality in every iteration, since always     l  k   â‰¤   f   (   ğ±  *   )    â‰¤   f   (   ğ±  k   )           subscript  l  k     f   superscript  ğ±            f   subscript  ğ±  k       l_{k}\leq f(\mathbf{x}^{*})\leq f(\mathbf{x}_{k})   .  It has been shown that this corresponding duality gap , that is the difference between    f   (   ğ±  k   )       f   subscript  ğ±  k     f(\mathbf{x}_{k})   and the lower bound    l  k     subscript  l  k    l_{k}   , decreases with the same convergence rate, i.e.       f   (   ğ±  k   )    -   l  k    =   O   (   1  /  k   )     .          f   subscript  ğ±  k     subscript  l  k      O    1  k      f(\mathbf{x}_{k})-l_{k}=O(1/k).     Notes  Bibliography    (Overview paper)  The Frankâ€“Wolfe algorithm description   External links   Marguerite Frank giving a personal account of the history of the algorithm   See also   Proximal gradient methods   "  Category:Optimization algorithms and methods  Category:Iterative methods  Category:First order methods  Category:Gradient methods     â†©  â†©  â†©  â†©  â†©  â†©     