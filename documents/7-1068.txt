   Successive over-relaxation      Successive over-relaxation   In numerical linear algebra , the method of successive over-relaxation ( SOR ) is a variant of the Gauss–Seidel method for solving a linear system of equations , resulting in faster convergence. A similar method can be used for any slowly converging iterative process .  It was devised simultaneously by David M. Young, Jr. and by H. Frankel in 1950 for the purpose of automatically solving linear systems on digital computers. Over-relaxation methods had been used before the work of Young and Frankel. For instance, the method of Lewis Fry Richardson , and the methods developed by R. V. Southwell . However, these methods were designed for computation by human calculators, and they required some expertise to ensure convergence to the solution which made them inapplicable for programming on digital computers. These aspects are discussed in the thesis of David M. Young, Jr. 1  Formulation  Given a square system of n linear equations with unknown x :       A  𝐱   =  𝐛        A  𝐱   𝐛    A\mathbf{x}=\mathbf{b}     where:        A  =   [      a  11      a  12     ⋯     a   1  n         a  21      a  22     ⋯     a   2  n        ⋮    ⋮    ⋱    ⋮       a   n  1       a   n  2      ⋯     a   n  n       ]    ,    𝐱  =   [      x  1        x  2       ⋮       x  n      ]    ,   𝐛  =   [      b  1        b  2       ⋮       b  n      ]      .     formulae-sequence    A     subscript  a  11    subscript  a  12   normal-⋯   subscript  a    1  n       subscript  a  21    subscript  a  22   normal-⋯   subscript  a    2  n      normal-⋮  normal-⋮  normal-⋱  normal-⋮     subscript  a    n  1     subscript  a    n  2    normal-⋯   subscript  a    n  n        formulae-sequence    𝐱     subscript  x  1      subscript  x  2     normal-⋮     subscript  x  n        𝐛     subscript  b  1      subscript  b  2     normal-⋮     subscript  b  n         A=\begin{bmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\
 a_{21}&a_{22}&\cdots&a_{2n}\\
 \vdots&\vdots&\ddots&\vdots\\
 a_{n1}&a_{n2}&\cdots&a_{nn}\end{bmatrix},\qquad\mathbf{x}=\begin{bmatrix}x_{1}%
 \\
 x_{2}\\
 \vdots\\
 x_{n}\end{bmatrix},\qquad\mathbf{b}=\begin{bmatrix}b_{1}\\
 b_{2}\\
 \vdots\\
 b_{n}\end{bmatrix}.     Then A can be decomposed into a diagonal component D , and strictly lower and upper triangular components L and U :       A  =   D  +  L  +  U    ,      A    D  L  U     A=D+L+U,   where        D  =   [      a  11     0    ⋯    0      0     a  22     ⋯    0      ⋮    ⋮    ⋱    ⋮      0    0    ⋯     a   n  n       ]    ,    L  =   [     0    0    ⋯    0       a  21     0    ⋯    0      ⋮    ⋮    ⋱    ⋮       a   n  1       a   n  2      ⋯    0     ]    ,   U  =   [     0     a  12     ⋯     a   1  n        0    0    ⋯     a   2  n        ⋮    ⋮    ⋱    ⋮      0    0    ⋯    0     ]      .     formulae-sequence    D     subscript  a  11   0  normal-⋯  0    0   subscript  a  22   normal-⋯  0    normal-⋮  normal-⋮  normal-⋱  normal-⋮    0  0  normal-⋯   subscript  a    n  n        formulae-sequence    L    0  0  normal-⋯  0     subscript  a  21   0  normal-⋯  0    normal-⋮  normal-⋮  normal-⋱  normal-⋮     subscript  a    n  1     subscript  a    n  2    normal-⋯  0       U    0   subscript  a  12   normal-⋯   subscript  a    1  n      0  0  normal-⋯   subscript  a    2  n      normal-⋮  normal-⋮  normal-⋱  normal-⋮    0  0  normal-⋯  0        D=\begin{bmatrix}a_{11}&0&\cdots&0\\
 0&a_{22}&\cdots&0\\
 \vdots&\vdots&\ddots&\vdots\\
 0&0&\cdots&a_{nn}\end{bmatrix},\quad L=\begin{bmatrix}0&0&\cdots&0\\
 a_{21}&0&\cdots&0\\
 \vdots&\vdots&\ddots&\vdots\\
 a_{n1}&a_{n2}&\cdots&0\end{bmatrix},\quad U=\begin{bmatrix}0&a_{12}&\cdots&a_{%
 1n}\\
 0&0&\cdots&a_{2n}\\
 \vdots&\vdots&\ddots&\vdots\\
 0&0&\cdots&0\end{bmatrix}.     The system of linear equations may be rewritten as:        (   D  +   ω  L    )   𝐱   =    ω  𝐛   -    [    ω  U   +    (   ω  -  1   )   D    ]   𝐱            D    ω  L    𝐱       ω  𝐛      delimited-[]      ω  U       ω  1   D     𝐱      (D+\omega L)\mathbf{x}=\omega\mathbf{b}-[\omega U+(\omega-1)D]\mathbf{x}     for a constant ω > 1, called the relaxation factor .  The method of successive over-relaxation is an iterative technique that solves the left hand side of this expression for x , using previous value for x on the right hand side. Analytically, this may be written as:        𝐱   (   k  +  1   )    =     (   D  +   ω  L    )    -  1     (    ω  𝐛   -    [    ω  U   +    (   ω  -  1   )   D    ]    𝐱   (  k  )      )    =     L  w    𝐱   (  k  )     +  𝐜    ,         superscript  𝐱    k  1       superscript    D    ω  L      1        ω  𝐛      delimited-[]      ω  U       ω  1   D      superscript  𝐱  k                subscript  L  w    superscript  𝐱  k    𝐜      \mathbf{x}^{(k+1)}=(D+\omega L)^{-1}\big(\omega\mathbf{b}-[\omega U+(\omega-1)%
 D]\mathbf{x}^{(k)}\big)=L_{w}\mathbf{x}^{(k)}+\mathbf{c},     where    𝐱   (  k  )      superscript  𝐱  k    \mathbf{x}^{(k)}   is the k th approximation or iteration of   𝐱   𝐱   \mathbf{x}   and    𝐱   (   k  +  1   )      superscript  𝐱    k  1     \mathbf{x}^{(k+1)}   is the next or k + 1 iteration of   𝐱   𝐱   \mathbf{x}   . However, by taking advantage of the triangular form of ( D + ωL ), the elements of x ( k +1) can be computed sequentially using forward substitution :     A   A   A     The choice of relaxation factor ω is not necessarily easy, and depends upon the properties of the coefficient matrix. In 1947, Ostrowski proved that if     ρ   (   L  ω   )    <  1        ρ   subscript  L  ω    1    \rho(L_{\omega})<1   is symmetric and positive-definite then    0  <  ω  <  2        0  ω       2     0<\omega<2   for   A   A   A   . Thus convergence of the iteration process follows, but we are generally interested in faster convergence rather than just convergence.  Algorithm  Since elements can be overwritten as they are computed in this algorithm, only one storage vector is needed, and vector indexing is omitted. The algorithm goes as follows:  Inputs:   b   b   b   ,   ω   ω   ω   ,   ϕ   ϕ   \phi    Output   ϕ   ϕ   \phi     Choose an initial guess   i   i   i   to the solution  repeat until convergence   for    n   n   n    from 1 until     σ  ←  0     normal-←  σ  0    \sigma\leftarrow 0    do     j   j   j      for    n   n   n    from 1 until    j   j   j    do   if    i   i   i   ≠    σ  ←   σ  +    a   i  j     ϕ  j        normal-←  σ    σ     subscript  a    i  j     subscript  ϕ  j       \sigma\leftarrow\sigma+a_{ij}\phi_{j}    then     j   j   j      end if     end (     ϕ  i   ←     (   1  -  ω   )    ϕ  i    +    ω   a   i  i      (    b  i   -  σ   )        normal-←   subscript  ϕ  i         1  ω    subscript  ϕ  i        ω   subscript  a    i  i        subscript  b  i   σ       \phi_{i}\leftarrow(1-\omega)\phi_{i}+\frac{\omega}{a_{ii}}(b_{i}-\sigma)   -loop)     i   i   i       end (      (   1  -  ω   )    ϕ  i    +    ω   a   i  i      (    b  i   -  σ   )            1  ω    subscript  ϕ  i        ω   subscript  a    i  i        subscript  b  i   σ      (1-\omega)\phi_{i}+\frac{\omega}{a_{ii}}(b_{i}-\sigma)   -loop)  check if convergence is reached   end (repeat)  Note:      ϕ  i   +   ω   (      b  i   -  σ    a   i  i     -   ϕ  i    )         subscript  ϕ  i     ω         subscript  b  i   σ    subscript  a    i  i      subscript  ϕ  i       \phi_{i}+\omega\left(\frac{b_{i}-\sigma}{a_{ii}}-\phi_{i}\right)   can also be written     U  =   L  T    ,      U   superscript  L  T     U=L^{T},\,   , thus saving one multiplication in each iteration of the outer for -loop.  Symmetric successive over-relaxation  The version for symmetric matrices A , in which       P  =    (    D  ω   +  L   )    1   2  -  ω     D   -  1     (    D  ω   +  U   )     ,      P        D  ω   L     1    2  ω     superscript  D    1        D  ω   U      P=\left(\frac{D}{\omega}+L\right)\frac{1}{2-\omega}D^{-1}\left(\frac{D}{\omega%
 }+U\right),     is referred to as Symmetric Successive Over-Relaxation , or ( SSOR ), in which        𝐱   k  +  1    =    𝐱  k   -    γ  k    P   -  1     (    A   𝐱  k    -  𝐛   )      ,   k  ≥  0.      formulae-sequence     superscript  𝐱    k  1       superscript  𝐱  k      superscript  γ  k    superscript  P    1        A   superscript  𝐱  k    𝐛        k  0.     \mathbf{x}^{k+1}=\mathbf{x}^{k}-\gamma^{k}P^{-1}(A\mathbf{x}^{k}-\mathbf{b}),%
 \ k\geq 0.     and the iterative method is       x   n  +  1    =   f   (   x  n   )         subscript  x    n  1      f   subscript  x  n      x_{n+1}=f(x_{n})     The SOR and SSOR methods are credited to David M. Young, Jr. .  Other applications of the method  A similar technique can be used for any iterative method. If the original iteration had the form        x   n  +  1   SOR   =     (   1  -  ω   )    x  n  SOR    +   ω  f   (   x  n  SOR   )      .       subscript   superscript  x  SOR     n  1          1  ω    subscript   superscript  x  SOR   n      ω  f   subscript   superscript  x  SOR   n       x^{\mathrm{SOR}}_{n+1}=(1-\omega)x^{\mathrm{SOR}}_{n}+\omega f(x^{\mathrm{SOR}%
 }_{n}).     then the modified version would use     x   x   x     Note however that the formulation presented above, used for solving systems of linear equations, is not a special case of this formulation if      𝐱   (   k  +  1   )    =     (   1  -  ω   )    𝐱   (  k  )     +   ω   L  *   -  1     (   𝐛  -   U   𝐱   (  k  )      )      .       superscript  𝐱    k  1          1  ω    superscript  𝐱  k      ω   superscript   subscript  L      1      𝐛    U   superscript  𝐱  k         \mathbf{x}^{(k+1)}=(1-\omega)\mathbf{x}^{(k)}+\omega L_{*}^{-1}(\mathbf{b}-U%
 \mathbf{x}^{(k)}).   is considered to be the complete vector. If this formulation is used instead, the equation for calculating the next vector will look like      ω  >  1      ω  1    \omega>1     Values of    ω  <  1      ω  1    \omega<1   are used to speed up convergence of a slow-converging process, while values of   ω   ω   \omega   are often used to help establish convergence of a diverging iterative process or speed up the convergence of an overshooting process.  There are various methods that adaptively set the relaxation parameter $\omega$ based on the observed behavior of the converging process. Usually they help to reach a super-linear convergence for some problems but fail for the others.  See also   Jacobi method  Gaussian Belief Propagation  Matrix splitting   Notes  References      Abraham Berman, Robert J. Plemmons, Nonnegative Matrices in the Mathematical Sciences , 1994, SIAM. ISBN 0-89871-321-8.   Yousef Saad , Iterative Methods for Sparse Linear Systems , 1st edition, PWS, 1996.  Netlib 's copy of "Templates for the Solution of Linear Systems", by Barrett et al.  Richard S. Varga 2002 Matrix Iterative Analysis , Second ed. (of 1962 Prentice Hall edition), Springer-Verlag.  David M. Young, Jr.  Iterative Solution of Large Linear Systems , Academic Press, 1971. (reprinted by Dover, 2003)   External links   Module for the SOR Method  Tridiagonal linear system solver based on SOR, in C++   "  Category:Numerical linear algebra  Category:Articles with example pseudocode  Category:Relaxation (iterative methods)     ↩     