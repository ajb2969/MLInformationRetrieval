   Freivalds' algorithm      Freivalds' algorithm   Freivalds' algorithm (named after Rusins Freivalds) is a probabilistic randomized algorithm used to verify matrix multiplication . Given three n × n  matrices  A , B , and C , a general problem is to verify whether A × B = C . A naïve algorithm would compute the product A × B explicitly and compare term by term whether this product equals C . However, the best known matrix multiplication algorithm runs in O ( n 2.3729 ) time. 1 Freivalds' algorithm utilizes randomization in order to reduce this time bound to O ( n 2 ) 2  with high probability . In O (kn 2 ) time the algorithm can verify a matrix product with probability of failure less than    2   -  k      superscript  2    k     2^{-k}   .  The algorithm  Input  Three n × n  matrices  A , B , C .  Output  Yes, if A × B = C ; No, otherwise.  Procedure   Generate an n × 1 random 0/1 vector     r  →     normal-→  r    \vec{r}   .  Compute     P  →   =    A  ×   (   B   r  →    )    -   C   r  →          normal-→  P       A    B   normal-→  r       C   normal-→  r       \vec{P}=A\times(B\vec{r})-C\vec{r}   .  Output "Yes" if     P  →   =    (  0  ,  0  ,  …  ,  0  )   T        normal-→  P    superscript   0  0  normal-…  0   T     \vec{P}=(0,0,\ldots,0)^{T}   ; "No," otherwise.   Error  If A × B = C , then the algorithm always returns "Yes". If A × B ≠ C , then the probability that the algorithm returns "Yes" is less than or equal to one half. This is called one-sided error .  By iterating the algorithm k times and returning "Yes" only if all iterations yield "Yes", a runtime of O(kn 2 ) and error probability of ≤ 1/2 k is achieved.  Example  Suppose one wished to determine whether:        A  B   =    [     2    3      3    4     ]    [     1    0      1    2     ]     =  ?    [     6    5      8    7     ]   =  C   .          A  B       2  3    3  4      1  0    1  2        superscript   normal-?       6  5    8  7         C     AB=\begin{bmatrix}2&3\\
 3&4\end{bmatrix}\begin{bmatrix}1&0\\
 1&2\end{bmatrix}\stackrel{?}{=}\begin{bmatrix}6&5\\
 8&7\end{bmatrix}=C.   A random two-element vector with entries equal to 0 or 1 is selected — say     r  →   =   [     1      1     ]        normal-→  r     1    1      \vec{r}=\begin{bmatrix}1\\
 1\end{bmatrix}   — and used to compute:       A  ×   (   B   r  →    )    -   C   r  →          A    B   normal-→  r       C   normal-→  r      \displaystyle A\times(B\vec{r})-C\vec{r}   This yields the zero vector, suggesting the possibility that AB = C. However, if in a second trial the vector     r  →   =   [     1      0     ]        normal-→  r     1    0      \vec{r}=\begin{bmatrix}1\\
 0\end{bmatrix}   is selected, the result becomes:         A  ×   (   B   r  →    )    -   C   r  →     =     [     2    3      3    4     ]    (    [     1    0      1    2     ]    [     1      0     ]    )    -    [     6    5      8    7     ]    [     1      0     ]     =   [      -  1        -  1      ]    .            A    B   normal-→  r       C   normal-→  r           2  3    3  4        1  0    1  2      1    0          6  5    8  7      1    0               1       1        A\times(B\vec{r})-C\vec{r}=\begin{bmatrix}2&3\\
 3&4\end{bmatrix}\left(\begin{bmatrix}1&0\\
 1&2\end{bmatrix}\begin{bmatrix}1\\
 0\end{bmatrix}\right)-\begin{bmatrix}6&5\\
 8&7\end{bmatrix}\begin{bmatrix}1\\
 0\end{bmatrix}=\begin{bmatrix}-1\\
 -1\end{bmatrix}.   The result is nonzero, proving that in fact AB ≠ C.  There are four two-element 0/1 vectors, and half of them give the zero vector in this case (     r  →   =   [     0      0     ]        normal-→  r     0    0      \vec{r}=\begin{bmatrix}0\\
 0\end{bmatrix}   and     r  →   =   [     1      1     ]        normal-→  r     1    1      \vec{r}=\begin{bmatrix}1\\
 1\end{bmatrix}   ), so the chance of randomly selecting these in two trials (and falsely concluding that AB=C) is 1/2 2 or 1/4. In the general case, the proportion of r yielding the zero vector may be less than 1/2, and a larger number of trials (such as 20) would be used, rendering the probability of error very small.  Error analysis  Let p equal the probability of error. We claim that if A × B = C , then p = 0, and if A × B ≠ C , then p ≤ 1/2.  ===Case A × B = C ===      P  →     normal-→  P    \displaystyle\vec{P}     This is regardless of the value of    r  →     normal-→  r    \vec{r}   , since it uses only that      A  ×  B   -  C   =  0          A  B   C   0    A\times B-C=0   . Hence the probability for error in this case is:       Pr   [    P  →   ≠  0   ]    =  0       Pr     normal-→  P   0    0    \Pr[\vec{P}\neq 0]=0     Case A × B ≠ C  Let       P  →   =   D  ×   r  →    =    (   p  1   ,   p  2   ,  …  ,   p  n   )   T          normal-→  P     D   normal-→  r          superscript    subscript  p  1    subscript  p  2   normal-…   subscript  p  n    T      \vec{P}=D\times\vec{r}=(p_{1},p_{2},\dots,p_{n})^{T}     Where      D  =    A  ×  B   -  C   =   (   d   i  j    )         D      A  B   C         subscript  d    i  j       D=A\times B-C=(d_{ij})   .  Since     A  ×  B   ≠  C        A  B   C    A\times B\neq C   , we have that some element of   D   D   D   is nonzero. Suppose that the element     d   i  j    ≠  0       subscript  d    i  j    0    d_{ij}\neq 0   . By the definition of matrix multiplication , we have:       p  i   =    ∑   k  =  1   n     d   i  k     r  k     =     d   i  1     r  1    +  ⋯  +    d   i  j     r  j    +  ⋯  +    d   i  n     r  n     =     d   i  j     r  j    +  y          subscript  p  i     superscript   subscript     k  1    n      subscript  d    i  k     subscript  r  k               subscript  d    i  1     subscript  r  1    normal-⋯     subscript  d    i  j     subscript  r  j    normal-⋯     subscript  d    i  n     subscript  r  n               subscript  d    i  j     subscript  r  j    y      p_{i}=\sum_{k=1}^{n}d_{ik}r_{k}=d_{i1}r_{1}+\cdots+d_{ij}r_{j}+\cdots+d_{in}r_%
 {n}=d_{ij}r_{j}+y   .  For some constant   y   y   y   . Using Bayes' Theorem , we can partition over   y   y   y   :   We use that:       Pr   [    p  i   =  0   |   y  =  0   ]    =   Pr   [    r  j   =  0   ]    =   1  2          Pr     subscript  p  i   0     y  0     Pr     subscript  r  j   0           1  2      \Pr[p_{i}=0|y=0]=\Pr[r_{j}=0]=\frac{1}{2}          Pr   [    p  i   =  0   |   y  ≠  0   ]    =   Pr   [    r  j   =   1  and   d   i  j     =   -  y    ]    ≤   Pr   [    r  j   =  1   ]    =   1  2          Pr     subscript  p  i   0     y  0     Pr       subscript  r  j     1  italic- and   subscript  d    i  j            y            Pr     subscript  r  j   1           1  2      \Pr[p_{i}=0|y\neq 0]=\Pr[r_{j}=1\and d_{ij}=-y]\leq\Pr[r_{j}=1]=\frac{1}{2}     Plugging these in the equation (), we get:      Pr   [    p  i   =  0   ]      Pr     subscript  p  i   0     \displaystyle\Pr[p_{i}=0]     Therefore,        Pr   [    P  →   =  0   ]    =   Pr   [    p  0   =   0  and   p  1    =   0  and  …    ]    ≤   Pr   [    p  i   =  0   ]    ≤   1  2    .         Pr     normal-→  P   0     Pr       subscript  p  0     0  italic- and   subscript  p  1           0  italic- and  normal-…            Pr     subscript  p  i   0           1  2      \Pr[\vec{P}=0]=\Pr[p_{0}=0\and p_{1}=0\and\dots]\leq\Pr[p_{i}=0]\leq\frac{1}{2}.   This completes the proof.  Ramifications  Simple algorithmic analysis shows that the running time of this algorithm is O ( n 2 ), beating the classical deterministic algorithm's bound of O ( n 3 ). The error analysis also shows that if we run our algorithm  k times, we can achieve an error bound of less than    1   2  k       1   superscript  2  k     \frac{1}{2^{k}}   , an exponentially small quantity. The algorithm is also fast in practice due to wide availability of fast implementations for matrix-vector products. Therefore, utilization of randomized algorithms can speed up a very slow deterministic algorithm . In fact, the best known deterministic matrix multiplication verification algorithm known at the current time is a variant of the Coppersmith–Winograd algorithm with an asymptotic running time of O ( n 2.3729 ). 3  Freivalds' algorithm frequently arises in introductions to probabilistic algorithms due to its simplicity and how it illustrates the superiority of probabilistic algorithms in practice for some problems.  See also   Schwartz–Zippel lemma   References   Freivalds, R. (1977), “Probabilistic Machines Can Use Less Running Time”, IFIP Congress 1977, pp. 839–842.   "  Category:Articles containing proofs  Category:Matrix theory  Category:Randomized algorithms  Category:Matrix multiplication algorithms     ↩  ↩      