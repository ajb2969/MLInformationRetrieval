   Rateâ€“distortion theory      Rateâ€“distortion theory   Rateâ€“distortion theory is a major branch of information theory which provides the theoretical foundations for lossy data compression ; it addresses the problem of determining the minimal number of bits per symbol, as measured by the rate R , that should be communicated over a channel, so that the source (input signal) can be approximately reconstructed at the receiver (output signal) without exceeding a given distortion D .  Introduction  Rateâ€“distortion theory gives an analytical expression for how much compression can be achieved using lossy compression methods. Many of the existing audio, speech, image, and video compression techniques have transforms, quantization, and bit-rate allocation procedures that capitalize on the general shape of rateâ€“distortion functions.  Rateâ€“distortion theory was created by Claude Shannon in his foundational work on information theory.  In rateâ€“distortion theory, the rate is usually understood as the number of bits per data sample to be stored or transmitted. The notion of distortion is a subject of on-going discussion. In the most simple case (which is actually used in most cases), the distortion is defined as the expected value of the square of the difference between input and output signal (i.e., the mean squared error ). However, since we know that most lossy compression techniques operate on data that will be perceived by human consumers (listening to music, watching pictures and video) the distortion measure should preferably be modeled on human perception and perhaps aesthetics : much like the use of probability in lossless compression , distortion measures can ultimately be identified with loss functions as used in Bayesian estimation and decision theory . In audio compression, perceptual models (and therefore perceptual distortion measures) are relatively well developed and routinely used in compression techniques such as MP3 or Vorbis , but are often not easy to include in rateâ€“distortion theory. In image and video compression, the human perception models are less well developed and inclusion is mostly limited to the JPEG and MPEG weighting ( quantization , normalization ) matrix.  Rateâ€“distortion functions  The functions that relate the rate and distortion are found as the solution of the following minimization problem:         inf    Q   Y  |  X     (  y  |  x  )       I  Q    (  Y  ;  X  )    subject to    D  Q     â‰¤   D  *    .        subscript  infimum   fragments   subscript  Q   fragments  Y  normal-|  X     fragments  normal-(  y  normal-|  x  normal-)        subscript  I  Q    Y  X   subject to   subscript  D  Q      superscript  D      \inf_{Q_{Y|X}(y|x)}I_{Q}(Y;X)\ \mbox{subject to}\ D_{Q}\leq D^{*}.     Here Q Y | X ( y | x ), sometimes called a test channel, is the conditional  probability density function (PDF) of the communication channel output (compressed signal) Y for a given input (original signal) X , and I Q ( Y ; X ) is the mutual information between Y and X defined as      I   (  Y  ;  X  )   =  H   (  Y  )   -  H   (  Y  |  X  )      fragments  I   fragments  normal-(  Y  normal-;  X  normal-)    H   fragments  normal-(  Y  normal-)    H   fragments  normal-(  Y  normal-|  X  normal-)     I(Y;X)=H(Y)-H(Y|X)\,     where H ( Y ) and H ( Y | X ) are the entropy of the output signal Y and the conditional entropy of the output signal given the input signal, respectively:       H   (  Y  )    =   -    âˆ«   -  âˆž   âˆž     P  Y    (  y  )     log  2    (    P  Y    (  y  )    )    d  y           H  Y       superscript   subscript             subscript  P  Y   y    subscript   2      subscript  P  Y   y    d  y       H(Y)=-\int_{-\infty}^{\infty}P_{Y}(y)\log_{2}(P_{Y}(y))\,dy         H   (  Y  |  X  )   =  -   âˆ«   -  âˆž   âˆž    âˆ«   -  âˆž   âˆž    Q   Y  |  X     (  y  |  x  )    P  X    (  x  )    log  2    (   Q   Y  |  X     (  y  |  x  )   )   d   x   d  y  .     fragments  H   fragments  normal-(  Y  normal-|  X  normal-)      superscript   subscript           superscript   subscript           subscript  Q   fragments  Y  normal-|  X     fragments  normal-(  y  normal-|  x  normal-)    subscript  P  X    fragments  normal-(  x  normal-)    subscript   2    fragments  normal-(   subscript  Q   fragments  Y  normal-|  X     fragments  normal-(  y  normal-|  x  normal-)   normal-)   d  x  d  y  normal-.    H(Y|X)=-\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}Q_{Y|X}(y|x)P_{X}(x)\log%
 _{2}(Q_{Y|X}(y|x))\,dx\,dy.     The problem can also be formulated as a distortionâ€“rate function, where we find the infimum over achievable distortions for given rate constraint. The relevant expression is:         inf    Q   Y  |  X     (  y  |  x  )      E   [    D  Q    [  X  ,  Y  ]    ]    subject to    I  Q    (  Y  ;  X  )     â‰¤  R   .        subscript  infimum   fragments   subscript  Q   fragments  Y  normal-|  X     fragments  normal-(  y  normal-|  x  normal-)       E   delimited-[]     subscript  D  Q    X  Y     subject to   subscript  I  Q    Y  X     R    \inf_{Q_{Y|X}(y|x)}E[D_{Q}[X,Y]]\ \mbox{subject to}\ I_{Q}(Y;X)\leq R.     The two formulations lead to functions which are inverses of each other.  The mutual information can be understood as a measure for 'prior' uncertainty the receiver has about the sender's signal ( H(Y) ), diminished by the uncertainty that is left after receiving information about the sender's signal ( H ( Y | X )). Of course the decrease in uncertainty is due to the communicated amount of information, which is I ( Y ; X ).  As an example, in case there is no communication at all, then H ( Y | X ) = H ( Y ) and I ( Y ; X ) = 0. Alternatively, if the communication channel is perfect and the received signal Y is identical to the signal X at the sender, then H ( Y | X ) = 0 and I ( Y ; X ) = H ( Y ) = H ( X ).  In the definition of the rateâ€“distortion function, D Q and D * are the distortion between X and Y for a given Q Y | X ( y | x ) and the prescribed maximum distortion, respectively. When we use the mean squared error as distortion measure, we have (for amplitude-continuous signals ):       D  Q   =   âˆ«   -  âˆž   âˆž    âˆ«   -  âˆž   âˆž    P   X  ,  Y     (  x  ,  y  )      (  x  -  y  )   2    d   x   d  y  =   âˆ«   -  âˆž   âˆž    âˆ«   -  âˆž   âˆž    Q   Y  |  X     (  y  |  x  )    P  X    (  x  )      (  x  -  y  )   2    d   x   d  y  .     fragments   subscript  D  Q     superscript   subscript           superscript   subscript           subscript  P   X  Y     fragments  normal-(  x  normal-,  y  normal-)    superscript   fragments  normal-(  x   y  normal-)   2   d  x  d  y    superscript   subscript           superscript   subscript           subscript  Q   fragments  Y  normal-|  X     fragments  normal-(  y  normal-|  x  normal-)    subscript  P  X    fragments  normal-(  x  normal-)    superscript   fragments  normal-(  x   y  normal-)   2   d  x  d  y  normal-.    D_{Q}=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}P_{X,Y}(x,y)(x-y)^{2}\,dx%
 \,dy=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}Q_{Y|X}(y|x)P_{X}(x)(x-y)^{%
 2}\,dx\,dy.     As the above equations show, calculating a rateâ€“distortion function requires the stochastic description of the input X in terms of the PDF P X ( x ), and then aims at finding the conditional PDF Q Y | X ( y | x ) that minimize rate for a given distortion D * . These definitions can be formulated measure-theoretically to account for discrete and mixed random variables as well.  An analytical solution to this minimization problem is often difficult to obtain except in some instances for which we next offer two of the best known examples. The rateâ€“distortion function of any source is known to obey several fundamental properties, the most important ones being that it is a continuous , monotonically decreasing  convex (U) function and thus the shape for the function in the examples is typical (even measured rateâ€“distortion functions in real life tend to have very similar forms).  Although analytical solutions to this problem are scarce, there are upper and lower bounds to these functions including the famous Shannon lower bound (SLB), which in the case of squared error and memoryless sources, states that for arbitrary sources with finite differential entropy,       R   (  D  )    â‰¥    h   (  X  )    -   h   (  D  )           R  D       h  X     h  D      R(D)\geq h(X)-h(D)\,     where h(D) is the differential entropy of a Gaussian random variable with variance D. This lower bound is extensible to sources with memory and other distortion measures. One important feature of the SLB is that it is asymptotically tight in the low distortion regime for a wide class of sources and in some occasions, it actually coincides with the rateâ€“distortion function. Shannon Lower Bounds can generally be found if the distortion between any two numbers can be expressed as a function of the difference between the value of these two numbers.  The Blahutâ€“Arimoto algorithm , co-invented by Richard Blahut , is an elegant iterative technique for numerically obtaining rateâ€“distortion functions of arbitrary finite input/output alphabet sources and much work has been done to extend it to more general problem instances.  When working with stationary sources with memory, it is necessary to modify the definition of the rate distortion function and it must be understood in the sense of a limit taken over sequences of increasing lengths.       R   (  D  )    =    lim   n  â†’  âˆž      R  n    (  D  )           R  D     subscript    normal-â†’  n        subscript  R  n   D      R(D)=\lim_{n\rightarrow\infty}R_{n}(D)   where        R  n    (  D  )    =    1  n     inf    Q    Y  n   |   X  n     âˆˆ  ð’¬     I   (   Y  n   ,   X  n   )             subscript  R  n   D       1  n     subscript  infimum     subscript  Q   fragments   superscript  Y  n   normal-|   superscript  X  n     ð’¬      I    superscript  Y  n    superscript  X  n         R_{n}(D)=\frac{1}{n}\inf_{Q_{Y^{n}|X^{n}}\in\mathcal{Q}}I(Y^{n},X^{n})   and      ð’¬  =   {   Q    Y  n   |   X  n      (   Y  n   |   X  n   ,   X  0   )   :  E   [  d   (   X  n   ,   Y  n   )   ]   â‰¤  D  }      fragments  Q    fragments  normal-{   subscript  Q   fragments   superscript  Y  n   normal-|   superscript  X  n      fragments  normal-(   superscript  Y  n   normal-|   superscript  X  n   normal-,   subscript  X  0   normal-)   normal-:  E   fragments  normal-[  d   fragments  normal-(   superscript  X  n   normal-,   superscript  Y  n   normal-)   normal-]    D  normal-}     \mathcal{Q}=\{Q_{Y^{n}|X^{n}}(Y^{n}|X^{n},X_{0}):E[d(X^{n},Y^{n})]\leq D\}   where superscripts denote a complete sequence up to that time and the subscript 0 indicates initial state.  Memoryless (independent) Gaussian source  If we assume that P X ( x ) is Gaussian with variance Ïƒ 2 , and if we assume that successive samples of the signal X are stochastically independent (or equivalently, the source is memoryless , or the signal is uncorrelated ), we find the following analytical expression for the rateâ€“distortion function:      R   (  D  )   =   {         1  2      log  2    (    Ïƒ  x  2   /  D   )     ,       if  0   â‰¤  D  â‰¤   Ïƒ  x  2              0  ,        if  D   >   Ïƒ  x  2    .          fragments  R   fragments  normal-(  D  normal-)     fragments  normal-{        1  2     subscript   2      superscript   subscript  Ïƒ  x   2   D           if  0   D        superscript   subscript  Ïƒ  x   2       absent    0      if  D    superscript   subscript  Ïƒ  x   2         R(D)=\left\{\begin{matrix}\frac{1}{2}\log_{2}(\sigma_{x}^{2}/D),&\mbox{if }0%
 \leq D\leq\sigma_{x}^{2}\\
 \\
 0,&\mbox{if }D>\sigma_{x}^{2}.\end{matrix}\right.    1  The following figure shows what this function looks like:  (Figure)  Rate distortion function.png   Rateâ€“distortion theory tell us that 'no compression system exists that performs outside the gray area'. The closer a practical compression system is to the red (lower) bound, the better it performs. As a general rule, this bound can only be attained by increasing the coding block length parameter. Nevertheless, even at unit blocklengths one can often find good (scalar) quantizers that operate at distances from the rateâ€“distortion function that are practically relevant. 2  This rateâ€“distortion function holds only for Gaussian memoryless sources. It is known that the Gaussian source is the most "difficult" source to encode: for a given mean square error, it requires the greatest number of bits. The performance of a practical compression system working onâ€”sayâ€”images, may well be below the R(D) lower bound shown.  Connecting rate-distortion theory to channel capacity 3  Suppose we want to transmit information about a source to the user with a distortion not exceeding D . Rateâ€“distortion theory tells us that at least R ( D ) bits/symbol of information from the source must reach the user. We also know from Shannon's channel coding theorem that if the source entropy is H bits/symbol, and the channel capacity is C (where C      â†©   â†©     