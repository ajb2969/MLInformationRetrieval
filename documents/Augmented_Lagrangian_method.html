<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="782">Augmented Lagrangian method</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Augmented Lagrangian method</h1>
<hr/>

<p><strong>Augmented Lagrangian methods</strong> are a certain class of <a href="algorithm" title="wikilink">algorithms</a> for solving <a href="Constraint_(mathematics)" title="wikilink">constrained</a> <a href="optimization_(mathematics)" title="wikilink">optimization</a> problems. They have similarities to <a href="penalty_method" title="wikilink">penalty methods</a> in that they replace a constrained optimization problem by a series of unconstrained problems and add a penalty term to the <a href="objective_function" title="wikilink">objective</a>; the difference is that the augmented Lagrangian method adds yet another term, designed to mimic a <a href="Lagrange_multiplier" title="wikilink">Lagrange multiplier</a>. The augmented Lagrangian is not the same as the <a href="Lagrange_multiplier" title="wikilink">method of Lagrange multipliers</a>.</p>

<p>Viewed differently, the unconstrained objective is the <a href="Lagrange_multipliers#The_strong_Lagrangian_principle:_Lagrange_duality" title="wikilink">Lagrangian</a> of the constrained problem, with an additional penalty term (the <strong>augmentation</strong>).</p>

<p>The method was originally known as the <strong>method of multipliers</strong>, and was studied much in the 1970 and 1980s as a good alternative to penalty methods. It was first discussed by <a href="Magnus_Hestenes" title="wikilink">Magnus Hestenes</a> in 1969<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> and by <a href="Michael_J._D._Powell" title="wikilink">Powell</a> in 1969.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> The method was studied by <a href="R._Tyrrell_Rockafellar" title="wikilink">R. Tyrrell Rockafellar</a> in relation to <a href="Fenchel_duality" title="wikilink">Fenchel duality</a>, particularly in relation to proximal-point methods, <a href="regularization" title="wikilink">Moreau‚ÄìYosida regularization</a>, and <a href="monotone_operator" title="wikilink">maximal monotone operators</a>: These methods were used in <a href="structural_engineering" title="wikilink">structural optimization</a>. The method was also studied by <a href="Dimitri_Bertsekas" title="wikilink">Dimitri Bertsekas</a>, notably in his 1982 book,<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> together with extensions involving nonquadratic regularization functions, such as <a href="Bregman_divergence" title="wikilink">entropic regularization</a>, which gives rise to the "exponential method of multipliers," a method that handles inequality constraints with a twice differentiable augmented Lagrangian function.</p>

<p>Since the 1970s, <a href="sequential_quadratic_programming" title="wikilink">sequential quadratic programming</a> (SQP) and <a href="interior_point_method" title="wikilink">interior point methods</a> (IPM) have had increasing attention, in part because they more easily use <a href="sparse_matrix" title="wikilink">sparse matrix</a> <a href="subroutine" title="wikilink">subroutines</a> from <a href="numerical_linear_algebra" title="wikilink">numerical</a> <a href="numerical_software" title="wikilink">software libraries</a>, and in part because IPMs have proven complexity results via the theory of <a href="self-concordant_function" title="wikilink">self-concordant functions</a>. The augmented Lagrangian method was rejuvenated by the optimization systems <a href="Galahad_library" title="wikilink">LANCELOT</a> and <a class="uri" href="AMPL" title="wikilink">AMPL</a>, which allowed sparse matrix techniques to be used on seemingly dense but "partially separable" problems. The method is still useful for some problems.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> Around 2007, there was a resurgence of augmented Lagrangian methods in fields such as <a href="Total_variation_denoising" title="wikilink">total-variation denoising</a> and <a href="compressed_sensing" title="wikilink">compressed sensing</a>. In particular, a variant of the standard augmented Lagrangian method that uses partial updates (similar to the <a href="Gauss-Seidel_method" title="wikilink">Gauss-Seidel method</a> for solving linear equations) known as the <strong><a href="Augmented_Lagrangian_method#Alternating_direction_method_of_multipliers" title="wikilink">alternating direction method of multipliers</a></strong> or <strong>ADMM</strong> gained some attention.</p>
<h2 id="general-method">General method</h2>

<p>Let us say we are solving the following constrained problem:</p>

<p>

<math display="block" id="Augmented_Lagrangian_method:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>min</mi>
    <mi>f</mi>
   </mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>ùê±</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <min></min>
     <ci>f</ci>
    </apply>
    <ci>ùê±</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min f(\mathbf{x})
  </annotation>
 </semantics>
</math>

 subject to</p>

<p>

<math display="block" id="Augmented_Lagrangian_method:1">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mi>c</mi>
      <mi>i</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>ùê±</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mpadded width="+3.3pt">
      <mn>0</mn>
     </mpadded>
     <mrow>
      <mo>‚àÄ</mo>
      <mi>i</mi>
     </mrow>
    </mrow>
    <mo>‚àà</mo>
    <mi>I</mi>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>c</ci>
       <ci>i</ci>
      </apply>
      <ci>ùê±</ci>
     </apply>
     <apply>
      <times></times>
      <cn type="integer">0</cn>
      <apply>
       <csymbol cd="latexml">for-all</csymbol>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <in></in>
     <share href="#.cmml">
     </share>
     <ci>I</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   c_{i}(\mathbf{x})=0~{}\forall i\in I.
  </annotation>
 </semantics>
</math>

</p>

<p>This problem can be solved as a series of unconstrained minimization problems. For reference, we first list the <a href="penalty_method" title="wikilink">penalty method</a> approach:</p>

<p>

<math display="block" id="Augmented_Lagrangian_method:2">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>min</mi>
     <msub>
      <mi mathvariant="normal">Œ¶</mi>
      <mi>k</mi>
     </msub>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>ùê±</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>ùê±</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mpadded width="+3.3pt">
      <msub>
       <mi>Œº</mi>
       <mi>k</mi>
      </msub>
     </mpadded>
     <mrow>
      <mpadded width="+3.3pt">
       <munder>
        <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
        <mrow>
         <mi>i</mi>
         <mo>‚àà</mo>
         <mi>I</mi>
        </mrow>
       </munder>
      </mpadded>
      <mrow>
       <msub>
        <mi>c</mi>
        <mi>i</mi>
       </msub>
       <msup>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>ùê±</mi>
         <mo stretchy="false">)</mo>
        </mrow>
        <mn>2</mn>
       </msup>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <min></min>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-Œ¶</ci>
       <ci>k</ci>
      </apply>
     </apply>
     <ci>ùê±</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <ci>f</ci>
      <ci>ùê±</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Œº</ci>
       <ci>k</ci>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <in></in>
         <ci>i</ci>
         <ci>I</ci>
        </apply>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>c</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>ùê±</ci>
         <cn type="integer">2</cn>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min\Phi_{k}(\mathbf{x})=f(\mathbf{x})+\mu_{k}~{}\sum_{i\in I}~{}c_{i}(\mathbf%
{x})^{2}
  </annotation>
 </semantics>
</math>

 The penalty method solves this problem, then at the next iteration it re-solves the problem using a larger value of 

<math display="inline" id="Augmented_Lagrangian_method:3">
 <semantics>
  <msub>
   <mi>Œº</mi>
   <mi>k</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>Œº</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu_{k}
  </annotation>
 </semantics>
</math>

 (and using the old solution as the initial guess or "warm-start").</p>

<p>The augmented Lagrangian method uses the following unconstrained objective:</p>

<p>

<math display="block" id="Augmented_Lagrangian_method:4">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>min</mi>
     <msub>
      <mi mathvariant="normal">Œ¶</mi>
      <mi>k</mi>
     </msub>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>ùê±</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>ùê±</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>+</mo>
     <mrow>
      <mpadded width="+3.3pt">
       <mfrac>
        <msub>
         <mi>Œº</mi>
         <mi>k</mi>
        </msub>
        <mn>2</mn>
       </mfrac>
      </mpadded>
      <mrow>
       <mpadded width="+3.3pt">
        <munder>
         <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
         <mrow>
          <mi>i</mi>
          <mo>‚àà</mo>
          <mi>I</mi>
         </mrow>
        </munder>
       </mpadded>
       <mrow>
        <msub>
         <mi>c</mi>
         <mi>i</mi>
        </msub>
        <msup>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>ùê±</mi>
          <mo stretchy="false">)</mo>
         </mrow>
         <mn>2</mn>
        </msup>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
    <mo>-</mo>
    <mrow>
     <mpadded width="+3.3pt">
      <munder>
       <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
       <mrow>
        <mi>i</mi>
        <mo>‚àà</mo>
        <mi>I</mi>
       </mrow>
      </munder>
     </mpadded>
     <mrow>
      <msub>
       <mi>Œª</mi>
       <mi>i</mi>
      </msub>
      <msub>
       <mi>c</mi>
       <mi>i</mi>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>ùê±</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <min></min>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-Œ¶</ci>
       <ci>k</ci>
      </apply>
     </apply>
     <ci>ùê±</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <plus></plus>
      <apply>
       <times></times>
       <ci>f</ci>
       <ci>ùê±</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <divide></divide>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>Œº</ci>
         <ci>k</ci>
        </apply>
        <cn type="integer">2</cn>
       </apply>
       <apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <in></in>
          <ci>i</ci>
          <ci>I</ci>
         </apply>
        </apply>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>c</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <ci>ùê±</ci>
          <cn type="integer">2</cn>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <in></in>
        <ci>i</ci>
        <ci>I</ci>
       </apply>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Œª</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>c</ci>
        <ci>i</ci>
       </apply>
       <ci>ùê±</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min\Phi_{k}(\mathbf{x})=f(\mathbf{x})+\frac{\mu_{k}}{2}~{}\sum_{i\in I}~{}c_{%
i}(\mathbf{x})^{2}-\sum_{i\in I}~{}\lambda_{i}c_{i}(\mathbf{x})
  </annotation>
 </semantics>
</math>

 and after each iteration, in addition to updating 

<math display="inline" id="Augmented_Lagrangian_method:5">
 <semantics>
  <msub>
   <mi>Œº</mi>
   <mi>k</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>Œº</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu_{k}
  </annotation>
 </semantics>
</math>

, the variable 

<math display="inline" id="Augmented_Lagrangian_method:6">
 <semantics>
  <mi>Œª</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Œª</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda
  </annotation>
 </semantics>
</math>

 is also updated according to the rule</p>

<p>

<math display="block" id="Augmented_Lagrangian_method:7">
 <semantics>
  <mrow>
   <msub>
    <mi>Œª</mi>
    <mi>i</mi>
   </msub>
   <mo>‚Üê</mo>
   <mrow>
    <msub>
     <mi>Œª</mi>
     <mi>i</mi>
    </msub>
    <mo>-</mo>
    <mrow>
     <msub>
      <mi>Œº</mi>
      <mi>k</mi>
     </msub>
     <msub>
      <mi>c</mi>
      <mi>i</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>ùï©</mi>
       <mi>k</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-‚Üê</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>Œª</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Œª</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Œº</ci>
       <ci>k</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>c</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ùï©</ci>
       <ci>k</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda_{i}\leftarrow\lambda_{i}-\mu_{k}c_{i}(\mathbb{x}_{k})
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Augmented_Lagrangian_method:8">
 <semantics>
  <msub>
   <mi>ùï©</mi>
   <mi>k</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>ùï©</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbb{x}_{k}
  </annotation>
 </semantics>
</math>

 is the solution to the unconstrained problem at the <em>k</em>th step, i.e. 

<math display="inline" id="Augmented_Lagrangian_method:9">
 <semantics>
  <mrow>
   <msub>
    <mi>ùï©</mi>
    <mi>k</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <mtext>argmin</mtext>
    <msub>
     <mi mathvariant="normal">Œ¶</mi>
     <mi>k</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>ùê±</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>ùï©</ci>
     <ci>k</ci>
    </apply>
    <apply>
     <times></times>
     <mtext>argmin</mtext>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>normal-Œ¶</ci>
      <ci>k</ci>
     </apply>
     <ci>ùê±</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbb{x}_{k}=\text{argmin}\Phi_{k}(\mathbf{x})
  </annotation>
 </semantics>
</math>

</p>

<p>The variable 

<math display="inline" id="Augmented_Lagrangian_method:10">
 <semantics>
  <mi>Œª</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Œª</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda
  </annotation>
 </semantics>
</math>

 is an estimate of the <a href="Lagrange_multiplier" title="wikilink">Lagrange multiplier</a>, and the accuracy of this estimate improves at every step. The major advantage of the method is that unlike the <a href="penalty_method" title="wikilink">penalty method</a>, it is not necessary to take 

<math display="inline" id="Augmented_Lagrangian_method:11">
 <semantics>
  <mrow>
   <mi>Œº</mi>
   <mo>‚Üí</mo>
   <mi mathvariant="normal">‚àû</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-‚Üí</ci>
    <ci>Œº</ci>
    <infinity></infinity>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu\rightarrow\infty
  </annotation>
 </semantics>
</math>

 in order to solve the original constrained problem. Instead, because of the presence of the Lagrange multiplier term, 

<math display="inline" id="Augmented_Lagrangian_method:12">
 <semantics>
  <mi>Œº</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Œº</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu
  </annotation>
 </semantics>
</math>

 can stay much smaller.</p>

<p>The method can be extended to handle inequality constraints. For a discussion of practical improvements, see.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h2 id="comparison-with-penalty-methods">Comparison with penalty methods</h2>

<p>In <a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> it is suggested that the augmented Lagrangian method is generally preferred to the quadratic penalty method since there is little extra computational cost and the parameter 

<math display="inline" id="Augmented_Lagrangian_method:13">
 <semantics>
  <mi>Œº</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Œº</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu
  </annotation>
 </semantics>
</math>

 need not go to infinity, thus avoiding ill-conditioning this is used.</p>
<h2 id="alternating-direction-method-of-multipliers">Alternating direction method of multipliers</h2>

<p>The alternating direction method of multipliers (ADMM) is a variant of the augmented Lagrangian scheme that uses partial updates for the dual variables. This method is often applied to solve problems such as</p>

<p>

<math display="inline" id="Augmented_Lagrangian_method:14">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <msub>
       <mi>min</mi>
       <mi>x</mi>
      </msub>
      <mi>f</mi>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>g</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <min></min>
       <ci>x</ci>
      </apply>
      <ci>f</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <apply>
     <times></times>
     <ci>g</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{x}f(x)+g(x).
  </annotation>
 </semantics>
</math>

</p>

<p>This is equivalent to the constrained problem</p>

<p>

<math display="inline" id="Augmented_Lagrangian_method:15">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <mrow>
       <mrow>
        <msub>
         <mi>min</mi>
         <mrow>
          <mi>x</mi>
          <mo>,</mo>
          <mi>y</mi>
         </mrow>
        </msub>
        <mi>f</mi>
       </mrow>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>+</mo>
      <mrow>
       <mi>g</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo rspace="12.5pt">,</mo>
     <mtext>subject to</mtext>
     <mi>x</mi>
    </mrow>
    <mo>=</mo>
    <mi>y</mi>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <list>
     <apply>
      <plus></plus>
      <apply>
       <times></times>
       <apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <min></min>
         <list>
          <ci>x</ci>
          <ci>y</ci>
         </list>
        </apply>
        <ci>f</ci>
       </apply>
       <ci>x</ci>
      </apply>
      <apply>
       <times></times>
       <ci>g</ci>
       <ci>y</ci>
      </apply>
     </apply>
     <mtext>subject to</mtext>
     <ci>x</ci>
    </list>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \min_{x,y}f(x)+g(y),\quad\text{subject to}\quad x=y.
  </annotation>
 </semantics>
</math>

</p>

<p>Though this change may seem trivial, the problem can now be attacked using methods of constrained optimization (in particular, the augmented Lagrangian method), and the objective function is separable in <em>x</em> and <em>y</em>. The dual update requires solving a proximity function in <em>x</em> and <em>y</em> at the same time; the ADMM technique allows this problem to be solved approximately by first solving for <em>x</em> with <em>y</em> fixed, and then solving for <em>y</em> with <em>x</em> fixed. Rather than iterate until convergence (like the <a href="Jacobi_method" title="wikilink">Jacobi method</a>), the algorithm proceeds directly to updating the dual variable and then repeating the process. This is not equivalent to the exact minimization, but surprisingly, it can still be shown that this method converges to the right answer (under some assumptions). Because of this approximation, the algorithm is distinct from the pure augmented Lagrangian method.</p>

<p>The ADMM can be viewed as an application of the <a href="Douglas-Rachford_splitting_algorithm" title="wikilink">Douglas-Rachford splitting algorithm</a>, and the Douglas-Rachford algorithm is in turn an instance of the <a href="Proximal_point_algorithm" title="wikilink">Proximal point algorithm</a>; details can be found here.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> There are several modern software packages that solve <a href="Basis_pursuit" title="wikilink">Basis pursuit</a> and variants and use the ADMM; such packages include <a href="http://yall1.blogs.rice.edu/">YALL1</a> (2009), <a href="http://www.lx.it.pt/~mtf/SpaRSA/">SpaRSA</a> (2009) and <a href="http://cascais.lx.it.pt/~mafonso/salsa.html">SALSA</a> (2009).</p>
<h2 id="stochastic-optimization">Stochastic Optimization</h2>

<p>Stochastic optimization considers the problem of minimizing a loss function with access to noisy samples of the (gradient of the) function. The goal is to have an estimate of the optimal parameter (minimizer) per new sample. ADMM is originally a batch method. However, with some modifications it can also be used for stochastic optimization. Since in stochastic setting we only have access to noisy samples of gradient, we use an inexact approximation of the Lagrangian as</p>

<p>

<math display="inline" id="Augmented_Lagrangian_method:16">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mover accent="true">
       <mi class="ltx_font_mathcaligraphic">‚Ñí</mi>
       <mo stretchy="false">^</mo>
      </mover>
      <mrow>
       <mi>œÅ</mi>
       <mo>,</mo>
       <mi>k</mi>
      </mrow>
     </msub>
     <mo>=</mo>
     <mrow>
      <mrow>
       <msub>
        <mi>f</mi>
        <mn>1</mn>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <msub>
         <mi>x</mi>
         <mi>k</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>+</mo>
     </mrow>
     <mo><</mo>
     <mrow>
      <mrow>
       <mo>‚àá</mo>
       <mi>f</mi>
      </mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>x</mi>
        <mi>k</mi>
       </msub>
       <mo>,</mo>
       <msub>
        <mi>Œ∂</mi>
        <mrow>
         <mi>k</mi>
         <mo>+</mo>
         <mn>1</mn>
        </mrow>
       </msub>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mo>,</mo>
    <mrow>
     <mi>x</mi>
     <mo>></mo>
     <mrow>
      <mrow>
       <mrow>
        <mo>+</mo>
        <mrow>
         <mi>g</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>y</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
       </mrow>
       <mo>-</mo>
       <mrow>
        <msup>
         <mi>z</mi>
         <mi>T</mi>
        </msup>
        <mrow>
         <mo stretchy="false">(</mo>
         <mrow>
          <mrow>
           <mrow>
            <mi>A</mi>
            <mi>x</mi>
           </mrow>
           <mo>+</mo>
           <mrow>
            <mi>B</mi>
            <mi>y</mi>
           </mrow>
          </mrow>
          <mo>-</mo>
          <mi>c</mi>
         </mrow>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo>+</mo>
      <mrow>
       <mfrac>
        <mi>œÅ</mi>
        <mn>2</mn>
       </mfrac>
       <msup>
        <mrow>
         <mo>‚à•</mo>
         <mrow>
          <mrow>
           <mrow>
            <mi>A</mi>
            <mi>x</mi>
           </mrow>
           <mo>+</mo>
           <mrow>
            <mi>B</mi>
            <mi>y</mi>
           </mrow>
          </mrow>
          <mo>-</mo>
          <mi>c</mi>
         </mrow>
         <mo>‚à•</mo>
        </mrow>
        <mn>2</mn>
       </msup>
      </mrow>
      <mo>+</mo>
      <mfrac>
       <msup>
        <mrow>
         <mo>‚à•</mo>
         <mrow>
          <mi>x</mi>
          <mo>-</mo>
          <msub>
           <mi>x</mi>
           <mi>k</mi>
          </msub>
         </mrow>
         <mo>‚à•</mo>
        </mrow>
        <mn>2</mn>
       </msup>
       <mrow>
        <mn>2</mn>
        <msub>
         <mi>Œ∑</mi>
         <mrow>
          <mi>k</mi>
          <mo>+</mo>
          <mn>1</mn>
         </mrow>
        </msub>
       </mrow>
      </mfrac>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <and></and>
     <apply>
      <eq></eq>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-^</ci>
        <ci>‚Ñí</ci>
       </apply>
       <list>
        <ci>œÅ</ci>
        <ci>k</ci>
       </list>
      </apply>
      <apply>
       <csymbol cd="latexml">limit-from</csymbol>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>f</ci>
         <cn type="integer">1</cn>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>k</ci>
        </apply>
       </apply>
       <plus></plus>
      </apply>
     </apply>
     <apply>
      <lt></lt>
      <share href="#.cmml">
      </share>
      <apply>
       <times></times>
       <apply>
        <ci>normal-‚àá</ci>
        <ci>f</ci>
       </apply>
       <interval closure="open">
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>k</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>Œ∂</ci>
         <apply>
          <plus></plus>
          <ci>k</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
       </interval>
      </apply>
     </apply>
    </apply>
    <apply>
     <gt></gt>
     <ci>x</ci>
     <apply>
      <plus></plus>
      <apply>
       <minus></minus>
       <apply>
        <plus></plus>
        <apply>
         <times></times>
         <ci>g</ci>
         <ci>y</ci>
        </apply>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>z</ci>
         <ci>T</ci>
        </apply>
        <apply>
         <minus></minus>
         <apply>
          <plus></plus>
          <apply>
           <times></times>
           <ci>A</ci>
           <ci>x</ci>
          </apply>
          <apply>
           <times></times>
           <ci>B</ci>
           <ci>y</ci>
          </apply>
         </apply>
         <ci>c</ci>
        </apply>
       </apply>
      </apply>
      <apply>
       <times></times>
       <apply>
        <divide></divide>
        <ci>œÅ</ci>
        <cn type="integer">2</cn>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="latexml">norm</csymbol>
         <apply>
          <minus></minus>
          <apply>
           <plus></plus>
           <apply>
            <times></times>
            <ci>A</ci>
            <ci>x</ci>
           </apply>
           <apply>
            <times></times>
            <ci>B</ci>
            <ci>y</ci>
           </apply>
          </apply>
          <ci>c</ci>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </apply>
      <apply>
       <divide></divide>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="latexml">norm</csymbol>
         <apply>
          <minus></minus>
          <ci>x</ci>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <ci>k</ci>
          </apply>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
       <apply>
        <times></times>
        <cn type="integer">2</cn>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>Œ∑</ci>
         <apply>
          <plus></plus>
          <ci>k</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\mathcal{L}}_{\rho,k}=f_{1}(x_{k})+<\nabla f(x_{k},\zeta_{k+1}),x>+g(y)-z%
^{T}(Ax+By-c)+\frac{\rho}{2}\|Ax+By-c\|^{2}+\frac{\|x-x_{k}\|^{2}}{2\eta_{k+1}},
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Augmented_Lagrangian_method:17">
 <semantics>
  <msub>
   <mi>Œ∑</mi>
   <mrow>
    <mi>k</mi>
    <mo>+</mo>
    <mn>1</mn>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>Œ∑</ci>
    <apply>
     <plus></plus>
     <ci>k</ci>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \eta_{k+1}
  </annotation>
 </semantics>
</math>

 is a time-varying step size.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>

<p>The alternating direction method of multipliers (ADMM) is a popular method for online and distributed optimization on a large scale,<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> and is employed in many applications, e.g.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a><a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> ADMM is often applied to solve regularized problems, where the function optimization and regularization can be carried out locally, and then coordinated globally via constraints. Regularized optimization problems are especially relevant in the high dimensional regime since regularization is a natural mechanism to overcome ill-posedness and to encourage parsimony in the optimal solution, e.g., sparsity and low rank. Due to the efficiency of ADMM in solving regularized problems, it has a good potential for stochastic optimization in high dimensions. However, conventional stochastic ADMM methods suffer from curse of dimensionality. Their convergence rate is proportional to square of the dimension and in practice they scale poorly. See figure <a href="http://newport.eecs.uci.edu/anandkumar/Lab/Lab_sub/Projects_sub/Reason.png">REASON vs Stochastic ADMM</a></p>

<p>Recently, a general framework has been proposed for stochastic optimization in high-dimensions that solves this bottleneck by adding simple and cheap modifications to ADMM.,<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> The method is called <a class="uri" href="REASON" title="wikilink">REASON</a> (Regularized Epoch-based Admm for Stochastic Optimization in high-dimensioN). The modifications are in terms of added projection which goes a long way and results in logarithmic dimension dependency. REASON can be performed on any regularized optimization with any number of regularizers. The specific cases of sparse optimization framework and noisy decomposition framework are discussed further. In both cases, REASON obtains minimax optimal convergence rate. REASON provides the first online guarantees for noisy matrix decomposition. Experiment results show that in aforementioned cases, REASON outperforms state-of-the-art.</p>
<h2 id="software">Software</h2>

<p>Some well-known software packages that use the augmented Lagrangian method are <a href="Galahad_library" title="wikilink">LANCELOT</a> and <a href="PENOPT" title="wikilink">PENNON</a>. The software <a href="MINOS_(optimization_software)" title="wikilink">MINOS</a> also uses an augmented Lagrangian method for some types of problems. The code for <a class="uri" href="REASON" title="wikilink">REASON</a> is available online.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Penalty_method" title="wikilink">Penalty methods</a></li>
<li><a href="Barrier_method_(mathematics)" title="wikilink">Barrier method</a></li>
<li><a href="Barrier_function" title="wikilink">Barrier function</a></li>
<li><a href="Lagrange_multiplier" title="wikilink">Lagrange multiplier</a></li>
</ul>
<h2 id="references">References</h2>
<references>
</references>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li></li>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Optimization_algorithms_and_methods" title="wikilink">Category:Optimization algorithms and methods</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="Magnus_Hestenes" title="wikilink">M.R. Hestenes</a>, "Multiplier and gradient methods", <em>Journal of Optimization Theory and Applications</em>, 4, 1969, pp. 303‚Äì320<a href="#fnref1">‚Ü©</a></li>
<li id="fn2">M.J.D. Powell, "A method for nonlinear constraints in minimization problems", in <em>Optimization</em> ed. by R. Fletcher, Academic Press, New York, NY, 1969, pp. 283‚Äì298.<a href="#fnref2">‚Ü©</a></li>
<li id="fn3">Dimitri P. Bertsekas, <em>Constrained optimization and Lagrange multiplier methods</em>, Athena Scientific, 1996 (first published 1982)<a href="#fnref3">‚Ü©</a></li>
<li id="fn4">, chapter 17<a href="#fnref4">‚Ü©</a></li>
<li id="fn5"></li>
<li id="fn6"></li>
<li id="fn7"><a href="#fnref7">‚Ü©</a></li>
<li id="fn8"><a href="#fnref8">‚Ü©</a></li>
<li id="fn9"><a href="#fnref9">‚Ü©</a></li>
<li id="fn10"><a href="#fnref10">‚Ü©</a></li>
<li id="fn11"><a href="#fnref11">‚Ü©</a></li>
<li id="fn12"><a href="#fnref12">‚Ü©</a></li>
<li id="fn13"><a href="#fnref13">‚Ü©</a></li>
<li id="fn14"><a href="#fnref14">‚Ü©</a></li>
<li id="fn15"><a href="#fnref15">‚Ü©</a></li>
</ol>
</section>
</body>
</html>
