


Entropic vector




Entropic vector

The entropic vector or entropic function is a concept arising in information theory. Shannon's information entropy measures and their associated identities and inequalities (both constrained and unconstrained) have received a lot of attention over the past from the time Shannon introduced his concept of Information Entropy. A lot of inequalities and identities have been found and are available in standard Information Theory texts. But recent researchers have laid focus on trying to find all possible identities and inequalities (both constrained and unconstrained) on such entropies and characterize them. Entropic vector lays down the basic framework for such a study.
Definition
Let 
 
 
 
  be random variables, with 
 
 

A vector h in 
 
 
 
  is an entropic vector of order 
 
 
 
 
  if and only if there exists a tuple 
 
 
 
  with associated vector 
 
 
 
  defined by 
 
 
 
  where 
 
 
 
  y 
 
 
 
 
 . The set of all entropic vectors of order 
 
 
 
  is denoted by 
 
 

All the properties of entropic functions can be transposed to entropic vectors:


 
  is continuous
Given a deterministic random variable 
 
 
 
 , we have 
 
 

Given 
 
 
 
 , there exists a random variable 
 
 
 
  such that 
 
 

Given 
 
 
 
  a probability distribution on 
 
 
 
 
 , we have 
 
 

Example
Let X,Y be two independent random variables with discrete uniform distribution over the set 
 
 
 
 . Then



It follows that



The entropic vector is thus :



The region Γn*
The Shannon inequality and Γn
The entropy satisfies the properties






The Shannon inequality is



The entropy vector that satisfies the linear combination of this region is called 
 
 
 
 .
The region 
 
 
 
 
  has been studied recently, the cases for n = 1, 2, 3






if and only if n ∈ {1, 2, 3}  It is difficult harder con the case 
 
 
 
 , the number of inequalities given by monotone and submodularity properties increase when we increase n, however the relationship among entropic vectors, polymatroids, are an object of study for the information theory and there are other ways to characterize those relationships mentioned
The most important results for the characterization of 
 
 
 
  is not precisely about these set, but its topological clousure i.e. the set 
 
 
 
 
 , which says that 
 
 
 
  is a convex cone, other interesing characterization is that 
 
 
 
  (
 
 
 
  is the set of vectors that satisfy Shannon-type inequalities) for 
 
 
 
 , in other words the set of entropy vector is completely characterized by Sahnnon's Inequalities,1 for the case n = 4 fails this property,23 particularly by the Ingleton's inequality.4









The Matus theorem
On the year 1998 the Senior Member IEEE Zhen Zhang and Raymond W. Yeung
.5
show a new non-Shannon's inequality


 
  On the year 2007 Matus proved
6


 
  is not polihedral.
Entropy and groups
Group-charactizable vectors and quasi-uniform distribution
One way to charactize 
 
 
 
 
  is by looking at some special distributions.\\ Definition: A group characterizable vector h is also denoted to be
 
 

such that there exists a group 
 
 
 
  and subgroups 
 
 
 
  and for 
 
 




if 
 
 
 
  is not 
 
 
 
  and 0 otherwise. 
 
 
 
  .
Definition
 
 
 
  is the set of all group charactizable vectors is 
 
 
 
 
 , and we can describe better the set 
 
 

Theorem
 
 

Open problem
Given a vector 
 
 
 
 , is it possible to say if there exists 
 
 
 
  random variables such that their joint entropies are given by 
 
 
 
 
 ? It turns out that for 
 
 
 
  the problem has been solved. But for 
 
 
 
 , it still remains unsolved. Defining the set of all such vectors 
 
 
 
  that can be constructed from a set of 
 
 
 
  random variables as 
 
 
 
 
 , we see that a complete characterization of this space remains an unsolved mystery.
References



Thomas M. Cover, Joy A. Thomas. Elements of information theory New York: Wiley, 1991. ISBN 0-471-06259-6
Raymond Yeung. A First Course in Information Theory, Chapter 12, Information Inequalities, 2002, Print ISBN 0-306-46791-7

"
Category:Information theory













