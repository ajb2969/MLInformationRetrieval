   Marginal likelihood      Marginal likelihood   In statistics , a marginal likelihood function , or integrated likelihood , is a likelihood function in which some parameter variables have been marginalized . In the context of Bayesian statistics , it may also be referred to as the evidence or model evidence .  Given a set of independent identically distributed data points     ùïè  =   (   x  1   ,  ‚Ä¶  ,   x  n   )    ,      ùïè    subscript  x  1   normal-‚Ä¶   subscript  x  n      \mathbb{X}=(x_{1},\ldots,x_{n}),   where     x  i   ‚àº  p   (   x  i   |  Œ∏  )      fragments   subscript  x  i   similar-to  p   fragments  normal-(   subscript  x  i   normal-|  Œ∏  normal-)     x_{i}\sim p(x_{i}|\theta)   according to some probability distribution parameterized by Œ∏ , where Œ∏ itself is a random variable described by a distribution, i.e.    Œ∏  ‚àº  p   (  Œ∏  |  Œ±  )   ,     fragments  Œ∏  similar-to  p   fragments  normal-(  Œ∏  normal-|  Œ±  normal-)   normal-,    \theta\sim p(\theta|\alpha),   the marginal likelihood in general asks what the probability    p   (  ùïè  |  Œ±  )      fragments  p   fragments  normal-(  X  normal-|  Œ±  normal-)     p(\mathbb{X}|\alpha)   is, where Œ∏ has been marginalized out (integrated out):      p   (  ùïè  |  Œ±  )   =   ‚à´  Œ∏   p   (  ùïè  |  Œ∏  )   p   (  Œ∏  |  Œ±  )   d  Œ∏     fragments  p   fragments  normal-(  X  normal-|  Œ±  normal-)     subscript   Œ∏   p   fragments  normal-(  X  normal-|  Œ∏  normal-)   p   fragments  normal-(  Œ∏  normal-|  Œ±  normal-)   normal-d  Œ∏    p(\mathbb{X}|\alpha)=\int_{\theta}p(\mathbb{X}|\theta)\,p(\theta|\alpha)\ %
 \operatorname{d}\!\theta     The above definition is phrased in the context of Bayesian statistics . In classical ( frequentist ) statistics, the concept of marginal likelihood occurs instead in the context of a joint parameter Œ∏ =( œà , Œª ), where œà is the actual parameter of interest, and Œª is a non-interesting nuisance parameter . If there exists a probability distribution for Œª , it is often desirable to consider the likelihood function only in terms of œà , by marginalizing out Œª:      ‚Ñí   (  œà  ;  ùïè  )   =  p   (  ùïè  |  œà  )   =   ‚à´  Œª   p   (  ùïè  |  œà  ,  Œª  )   p   (  Œª  |  œà  )   d  Œª     fragments  L   fragments  normal-(  œà  normal-;  X  normal-)    p   fragments  normal-(  X  normal-|  œà  normal-)     subscript   Œª   p   fragments  normal-(  X  normal-|  œà  normal-,  Œª  normal-)   p   fragments  normal-(  Œª  normal-|  œà  normal-)   normal-d  Œª    \mathcal{L}(\psi;\mathbb{X})=p(\mathbb{X}|\psi)=\int_{\lambda}p(\mathbb{X}|%
 \psi,\lambda)\,p(\lambda|\psi)\ \operatorname{d}\!\lambda     Unfortunately, marginal likelihoods are generally difficult to compute. Exact solutions are known for a small class of distributions, particularly when the marginalized-out parameter is the conjugate prior of the distribution of the data. In other cases, some kind of numerical integration method is needed, either a general method such as Gaussian integration or a Monte Carlo method , or a method specialized to statistical problems such as the Laplace approximation , Gibbs sampling or the EM algorithm .  It is also possible to apply the above considerations to a single random variable (data point) x , rather than a set of observations. In a Bayesian context, this is equivalent to the prior predictive distribution of a data point.  Applications  Bayesian model comparison  In Bayesian model comparison , the marginalized variables are parameters for a particular type of model, and the remaining variable is the identity of the model itself. In this case, the marginalized likelihood is the probability of the data given the model type, not assuming any particular model parameters. Writing Œ∏ for the model parameters, the marginal likelihood for the model M is      p   (  x  |  M  )   =  ‚à´  p   (  x  |  Œ∏  ,  M  )   p   (  Œ∏  |  M  )   d  Œ∏     fragments  p   fragments  normal-(  x  normal-|  M  normal-)     p   fragments  normal-(  x  normal-|  Œ∏  normal-,  M  normal-)   p   fragments  normal-(  Œ∏  normal-|  M  normal-)   normal-d  Œ∏    p(x|M)=\int p(x|\theta,M)\,p(\theta|M)\,\operatorname{d}\!\theta     It is in this context that the term model evidence is normally used. This quantity is important because the posterior odds ratio for a model M 1 against another model M 2 involves a ratio of marginal likelihoods, the so-called Bayes factor :        p   (   M  1   |  x  )     p   (   M  2   |  x  )     =      p   (   M  1   )     p   (   M  2   )        p   (  x  |   M  1   )     p   (  x  |   M  2   )             fragments  p   fragments  normal-(   subscript  M  1   normal-|  x  normal-)     fragments  p   fragments  normal-(   subscript  M  2   normal-|  x  normal-)           p   subscript  M  1      p   subscript  M  2        fragments  p   fragments  normal-(  x  normal-|   subscript  M  1   normal-)     fragments  p   fragments  normal-(  x  normal-|   subscript  M  2   normal-)        \frac{p(M_{1}|x)}{p(M_{2}|x)}=\frac{p(M_{1})}{p(M_{2})}\,\frac{p(x|M_{1})}{p(x%
 |M_{2})}     which can be stated schematically as   posterior odds = prior odds √ó Bayes factor    See also   Empirical Bayes methods  Marginal probability  Lindley's paradox   References   Charles S. Bos. "A comparison of marginal likelihood computation methods". In W. H√§rdle and B. Ronz, editors, COMPSTAT 2002: Proceedings in Computational Statistics , pp.¬†111‚Äì117. 2002. (Available as a preprint on the web: 1 )  The on-line textbook: Information Theory, Inference, and Learning Algorithms , by David J.C. MacKay .   "  Category:Probability theory  Category:Bayesian statistics   