   Non-negative least squares      Non-negative least squares   In mathematical optimization , the problem of non-negative least squares ( NNLS ) is a constrained version of the least squares problem where the coefficients are not allowed to become negative. That is, given a matrix   ğ€   ğ€   \mathbf{A}   and a (column) vector of response variables    ğ²   ğ²   \mathbf{y}   , the goal is to find 1         arg   min   ğ±     âˆ¥   ğ€ğ±  -  ğ²   âˆ¥   2        subscript    arg  min   ğ±    subscript   norm    ğ€ğ±  ğ²    2     \operatorname*{arg\,min}_{\mathbf{x}}\|\mathbf{Ax}-\mathbf{y}\|_{2}   subject to    ğ±  â‰¥  0      ğ±  normal-â‰¥  0    \mathbf{x}â‰¥0   .  Here,    ğ±  â‰¥  0      ğ±  normal-â‰¥  0    \mathbf{x}â‰¥0   means that each component of the vector   ğ±   ğ±   \mathbf{x}   should be non-negative and    â€–  Â·  â€–  â‚‚      normal-â€–  normal-Â·  normal-â€–  normal-â‚‚    â€–Â·â€–â‚‚   denotes the Euclidean norm .  Non-negative least squares problems turn up as subproblems in matrix decomposition , e.g. in algorithms for PARAFAC 2 and non-negative matrix/tensor factorization . 3 The latter can be considered a generalization of NNLS. 4  Another generalization of NNLS is bounded-variable least squares (BLVS), with simultaneous upper and lower bounds    Î±  áµ¢  â‰¤  ğ±  áµ¢  â‰¤  Î²  áµ¢      Î±  áµ¢  normal-â‰¤  ğ±  áµ¢  normal-â‰¤  Î²  áµ¢    Î±áµ¢â‰¤\mathbf{x}áµ¢â‰¤Î²áµ¢   . 5  Quadratic programming version  The NNLS problem is equivalent to a quadratic programming problem          arg   min    ğ±  â‰¥  ğŸ      1  2    ğ±  ğ–³   ğğ±    +    ğœ  ğ–³   ğ±         subscript    arg  min     ğ±  0        1  2    superscript  ğ±  ğ–³   ğğ±       superscript  ğœ  ğ–³   ğ±     \operatorname*{arg\,min}_{\mathbf{x\geq 0}}\frac{1}{2}\mathbf{x}^{\mathsf{T}}%
 \mathbf{Q}\mathbf{x}+\mathbf{c}^{\mathsf{T}}\mathbf{x}   ,  where   ğ   ğ   \mathbf{Q}   =    ğ€  áµ€  ğ€      ğ€  áµ€  ğ€    \mathbf{A}áµ€\mathbf{A}   and   ğœ   ğœ   \mathbf{c}   =    âˆ’  ğ€  áµ€  ğ²      normal-âˆ’  ğ€  áµ€  ğ²    âˆ’\mathbf{A}áµ€\mathbf{y}   . This problem is convex as   ğ   ğ   \mathbf{Q}   is positive semidefinite and the non-negativity constraints form a convex feasible set. 6  Algorithms  The first widely used algorithm for solving this problem is an active set method published by Lawson and Hanson in their 1974 book Solving Least Squares Problems . 7 In pseudocode , this algorithm looks as follows: 8  #Â Inputs  AÂ :Â matrixÂ ofÂ shapeÂ (m,Â n)  yÂ :Â vectorÂ ofÂ lengthÂ m  tolÂ :Â toleranceÂ forÂ theÂ stoppingÂ criterion   #Â Initialization  PÂ â†Â âˆ…  RÂ â†Â {1,Â ...,Â n}  xÂ â†Â zero-vectorÂ ofÂ lengthÂ n  wÂ â†Â Aáµ€(yÂ âˆ’Â Ax)   while RÂ â‰ Â âˆ… and max(w)Â >Â tol  jÂ â†Â indexÂ ofÂ max(w)Â inÂ w  addÂ jÂ toÂ P  removeÂ jÂ fromÂ R  #Â Aá´¾Â isÂ AÂ restrictedÂ toÂ theÂ variablesÂ includedÂ inÂ P  sá´¾Â â†Â ((Aá´¾)áµ€Â Aá´¾)â»Â¹Â (Aá´¾)áµ€y   while min(sá´¾)Â â‰¤Â 0  Î±Â â†Â min(xáµ¢Â /Â (xáµ¢Â -Â sáµ¢)Â forÂ iÂ inÂ P,Â sáµ¢Â â‰¤Â 0)  xÂ â†Â xÂ +Â Î±(sÂ -Â x)  MoveÂ toÂ RÂ allÂ indexesÂ jÂ inÂ PÂ suchÂ thatÂ x j =Â 0  sá´¾Â â†Â ((Aá´¾)áµ€Â Aá´¾)â»Â¹Â (Aá´¾)áµ€y  sá´¿Â â†Â 0  xÂ â†Â s  wÂ â†Â Aáµ€(yÂ âˆ’Â Ax)  This algorithm takes a finite number of steps to reach a solution and smoothly improves its candidate solution as it goes (so it can find good approximate solutions when cut off at a reasonable number of iterations), but is very slow in practice, owing largely to the computation of the pseudoinverse      (    (   ğ€  á´¾   )   áµ€  ğ€  á´¾   )   â»  Â¹          ğ€  á´¾   áµ€  ğ€  á´¾   normal-â»  normal-Â¹    ((\mathbf{A}á´¾)áµ€\mathbf{A}á´¾)â»Â¹   . Variants of this algorithm are available in Matlab as the routine 9 and in SciPy as . 10  Many improved algorithms have been suggested since 1974. Fast NNLS (FNNLS) is an optimized version of the Lawsonâ€”Hanson algorithm. Other algorithms include variants of Landweber 's gradient descent method 11 and coordinate-wise optimization based on the quadratic programming problem above.  See also   M-matrix  Perronâ€“Frobenius theorem   References  "  Category:Least squares  Category:Mathematical optimization       â†©   â†©  â†©  â†©  â†©  â†©  â†©  â†©     