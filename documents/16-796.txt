


Non-negative least squares




Non-negative least squares

In mathematical optimization, the problem of non-negative least squares (NNLS) is a constrained version of the least squares problem where the coefficients are not allowed to become negative. That is, given a matrix 
 
 
 
  and a (column) vector of response variables

 
 , the goal is to find1


 
  subject to 
 
 
 
 .
Here, 
 
 
 
  means that each component of the vector 
 
 
 
  should be non-negative and 
 
 
 
  denotes the Euclidean norm.
Non-negative least squares problems turn up as subproblems in matrix decomposition, e.g. in algorithms for PARAFAC2 and non-negative matrix/tensor factorization.3 The latter can be considered a generalization of NNLS.4
Another generalization of NNLS is bounded-variable least squares (BLVS), with simultaneous upper and lower bounds 
 
 
 
 .5
Quadratic programming version
The NNLS problem is equivalent to a quadratic programming problem


 
 ,
where 
 
 
 
  = 
 
 
 
  and 
 
 
 
  = 
 
 
 
 . This problem is convex as 
 
 
 
  is positive semidefinite and the non-negativity constraints form a convex feasible set.6
Algorithms
The first widely used algorithm for solving this problem is an active set method published by Lawson and Hanson in their 1974 book Solving Least Squares Problems.7 In pseudocode, this algorithm looks as follows:8
# Inputs
A : matrix of shape (m, n)
y : vector of length m
tol : tolerance for the stopping criterion

# Initialization
P ← ∅
R ← {1, ..., n}
x ← zero-vector of length n
w ← Aᵀ(y − Ax)

while R ≠ ∅ and max(w) > tol
    j ← index of max(w) in w
    add j to P
    remove j from R
    # Aᴾ is A restricted to the variables included in P
    sᴾ ← ((Aᴾ)ᵀ Aᴾ)⁻¹ (Aᴾ)ᵀy
    while min(sᴾ) ≤ 0
        α ← min(xᵢ / (xᵢ - sᵢ) for i in P, sᵢ ≤ 0)
        x ← x + α(s - x)
        Move to R all indexes j in P such that xj = 0
        sᴾ ← ((Aᴾ)ᵀ Aᴾ)⁻¹ (Aᴾ)ᵀy
        sᴿ ← 0
    x ← s
    w ← Aᵀ(y − Ax)
This algorithm takes a finite number of steps to reach a solution and smoothly improves its candidate solution as it goes (so it can find good approximate solutions when cut off at a reasonable number of iterations), but is very slow in practice, owing largely to the computation of the pseudoinverse

 
 . Variants of this algorithm are available in Matlab as the routine 9 and in SciPy as .10
Many improved algorithms have been suggested since 1974. Fast NNLS (FNNLS) is an optimized version of the Lawson—Hanson algorithm. Other algorithms include variants of Landweber's gradient descent method11 and coordinate-wise optimization based on the quadratic programming problem above.
See also

M-matrix
Perron–Frobenius theorem

References
"
Category:Least squares Category:Mathematical optimization


















