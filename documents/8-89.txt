   Preconditioner      Preconditioner   In mathematics , preconditioning is the application of a transformation, called the preconditioner , that conditions a given problem into a form that is more suitable for numerical solving methods. Preconditioning is typically related to reducing a condition number of the problem. The preconditioned problem is then usually solved by an iterative method .  Preconditioning for linear systems  In linear algebra and numerical analysis , a preconditioner    P   P   P   of a matrix   A   A   A   is a matrix such that     P   -  1    A       superscript  P    1    A    P^{-1}A   has a smaller condition number than   A   A   A   . It is also common to call    T  =   P   -  1        T   superscript  P    1      T=P^{-1}   the preconditioner, rather than   P   P   P   , since   P   P   P   itself is rarely explicitly available. In modern preconditioning, the application of    T  =   P   -  1        T   superscript  P    1      T=P^{-1}   , i.e., multiplication of a column vector, or a block of column vectors, by    T  =   P   -  1        T   superscript  P    1      T=P^{-1}   , is commonly performed by rather sophisticated computer software packages in a matrix-free fashion , i.e., where neither   P   P   P   , nor    T  =   P   -  1        T   superscript  P    1      T=P^{-1}   (and often not even   A   A   A   ) are explicitly available in a matrix form.  Preconditioners are useful in iterative methods to solve a linear system     A  x   =  b        A  x   b    Ax=b   for   x   x   x   since the rate of convergence for most iterative linear solvers increases as the condition number of a matrix decreases as a result of preconditioning. Preconditioned iterative solvers typically outperform direct solvers, e.g., Gaussian elimination , for large, especially for sparse , matrices. Iterative solvers can be used as matrix-free methods , i.e. become the only choice if the coefficient matrix   A   A   A   is not stored explicitly, but is accessed by evaluating matrix-vector products.  Description  Instead of solving the original linear system above, one may solve either the right preconditioned system:       A   P   -  1    P  x   =  b        A   superscript  P    1    P  x   b    AP^{-1}Px=b     via solving       A   P   -  1    y   =  b        A   superscript  P    1    y   b    AP^{-1}y=b     for   y   y   y   and       P  x   =  y        P  x   y    Px=y     for   x   x   x   ; or the left preconditioned system:        P   -  1     (    A  x   -  b   )    =  0         superscript  P    1        A  x   b    0    P^{-1}(Ax-b)=0     both of which give the same solution as the original system so long as the preconditioner matrix   P   P   P   is nonsingular . The left preconditioning is more common.  The goal of this preconditioned system is to reduce the condition number of the left or right preconditioned system matrix     P   -  1    A       superscript  P    1    A    P^{-1}A   or     A   P   -  1     ,      A   superscript  P    1      AP^{-1},   respectively. The preconditioned matrix     P   -  1    A       superscript  P    1    A    P^{-1}A   or    A   P   -  1        A   superscript  P    1      AP^{-1}   is almost never explicitly formed. Only the action of applying the preconditioner solve operation    P   -  1      superscript  P    1     P^{-1}   to a given vector need to be computed in iterative methods.  Typically there is a trade-off in the choice of   P   P   P   . Since the operator    P   -  1      superscript  P    1     P^{-1}   must be applied at each step of the iterative linear solver, it should have a small cost (computing time) of applying the    P   -  1      superscript  P    1     P^{-1}   operation. The cheapest preconditioner would therefore be    P  =  I      P  I    P=I   since then      P   -  1    =  I   .       superscript  P    1    I    P^{-1}=I.   Clearly, this results in the original linear system and the preconditioner does nothing. At the other extreme, the choice    P  =  A      P  A    P=A   gives       P   -  1    A   =   A   P   -  1     =  I   ,           superscript  P    1    A     A   superscript  P    1          I     P^{-1}A=AP^{-1}=I,   which has optimal condition number of 1, requiring a single iteration for convergence; however in this case      P   -  1    =   A   -  1     ,       superscript  P    1     superscript  A    1      P^{-1}=A^{-1},   and applying the preconditioner is as difficult as solving the original system. One therefore chooses   P   P   P   as somewhere between these two extremes, in an attempt to achieve a minimal number of linear iterations while keeping the operator    P   -  1      superscript  P    1     P^{-1}   as simple as possible. Some examples of typical preconditioning approaches are detailed below.  Preconditioned iterative methods  Preconditioned iterative methods for      A  x   -  b   =  0          A  x   b   0    Ax-b=0   are, in most cases, mathematically equivalent to standard iterative methods applied to the preconditioned system      P   -  1     (    A  x   -  b   )    =  0.         superscript  P    1        A  x   b    0.    P^{-1}(Ax-b)=0.   For example, the standard Richardson iteration for solving      A  x   -  b   =  0          A  x   b   0    Ax-b=0   is        ùê±   n  +  1    =    ùê±  n   -    Œ≥  n    (    A   ùê±  n    -  ùêõ   )      ,   n  ‚â•  0.      formulae-sequence     subscript  ùê±    n  1       subscript  ùê±  n      subscript  Œ≥  n       A   subscript  ùê±  n    ùêõ        n  0.     \mathbf{x}_{n+1}=\mathbf{x}_{n}-\gamma_{n}(A\mathbf{x}_{n}-\mathbf{b}),\ n\geq
 0.     Applied to the preconditioned system       P   -  1     (    A  x   -  b   )    =  0   ,         superscript  P    1        A  x   b    0    P^{-1}(Ax-b)=0,   it turns into a preconditioned method        ùê±   n  +  1    =    ùê±  n   -    Œ≥  n    P   -  1     (    A   ùê±  n    -  ùêõ   )      ,   n  ‚â•  0.      formulae-sequence     subscript  ùê±    n  1       subscript  ùê±  n      subscript  Œ≥  n    superscript  P    1        A   subscript  ùê±  n    ùêõ        n  0.     \mathbf{x}_{n+1}=\mathbf{x}_{n}-\gamma_{n}P^{-1}(A\mathbf{x}_{n}-\mathbf{b}),%
 \ n\geq 0.     Examples of popular preconditioned iterative methods for linear systems include the preconditioned conjugate gradient method , the biconjugate gradient method , and generalized minimal residual method . Iterative methods, which use scalar products to compute the iterative parameters, require corresponding changes in the scalar product together with substituting      P   -  1     (    A  x   -  b   )    =  0         superscript  P    1        A  x   b    0    P^{-1}(Ax-b)=0   for      A  x   -  b   =  0.          A  x   b   0.    Ax-b=0.     Geometric interpretation  For a symmetric  positive definite matrix   A   A   A   the preconditioner   P   P   P   is typically chosen to be symmetric positive definite as well. The preconditioned operator     P   -  1    A       superscript  P    1    A    P^{-1}A   is then also symmetric positive definite, but with respect to the   P   P   P   -based scalar product . In this case, the desired effect in applying a preconditioner is to make the quadratic form of the preconditioned operator     P   -  1    A       superscript  P    1    A    P^{-1}A   with respect to the   P   P   P   -based scalar product to be nearly spherical 1 .  Variable and non-linear preconditioning  Denoting    T  =   P   -  1        T   superscript  P    1      T=P^{-1}   , we highlight that preconditioning is practically implemented as multiplying some vector   r   r   r   by   T   T   T   , i.e., computing the product     T  r   .      T  r    Tr.   In many applications,   T   T   T   is not given as a matrix, but rather as an operator    T   (  r  )       T  r    T(r)   acting on the vector   r   r   r   . Some popular preconditioners, however, change with   r   r   r   and the dependence on   r   r   r   may not be linear. Typical examples involve using non-linear iterative methods , e.g., the conjugate gradient method , as a part of the preconditioner construction. Such preconditioners may be practically very efficient, however, their behavior is hard to predict theoretically.  Spectrally equivalent preconditioning  The most common use of preconditioning is for iterative solution of linear systems resulting from approximations of partial differential equations . The better the approximation quality, the larger the matrix size is. In such a case, the goal of optimal preconditioning is, on the one side, to make the spectral condition number of     P   -  1    A       superscript  P    1    A    P^{-1}A   to be bounded from above by a constant independent of the matrix size, which is called spectrally equivalent preconditioning by D'yakonov . On the other hand, the cost of application of the    P   -  1      superscript  P    1     P^{-1}   should ideally be proportional (also independent of the matrix size) to the cost of multiplication of   A   A   A   by a vector.  Examples  Jacobi (or diagonal) preconditioner  The Jacobi preconditioner is one of the simplest forms of preconditioning, in which the preconditioner is chosen to be the diagonal of the matrix     P  =   diag   (  A  )     .      P    diag  A     P=\mathrm{diag}(A).   Assuming     A   i  i    ‚â†   0  ,   ‚àÄ  i         subscript  A    i  i     0   for-all  i      A_{ii}\neq 0,\forall i   , we get      P   i  j    -  1    =    Œ¥   i  j     A   i  j      .       subscript   superscript  P    1      i  j       subscript  Œ¥    i  j     subscript  A    i  j       P^{-1}_{ij}=\frac{\delta_{ij}}{A_{ij}}.   It is efficient for diagonally dominant matrices    A   A   A   .  SPAI  The Sparse Approximate Inverse preconditioner minimises      ‚à•    A  T   -  I   ‚à•   F   ,     subscript   norm      A  T   I    F    \|AT-I\|_{F},   where    ‚à•  ‚ãÖ   ‚à•  F      fragments  parallel-to  normal-‚ãÖ   subscript  parallel-to  F     \|\cdot\|_{F}   is the Frobenius matrix norm and    T  =   P   -  1        T   superscript  P    1      T=P^{-1}   is from some suitably constrained set of sparse matrices. Under the Frobenius norm, this reduces to solving numerous independent least-squares problems (one for every column). The entries in   T   T   T   must be restricted to some sparsity pattern or the problem becomes as hard and time consuming as finding the exact inverse of   A   A   A   . This method, as well as means to select sparsity patterns, were introduced by [M.J. Grote, T. Huckle, SIAM J. Sci. Comput . 18 (1997) 838‚Äì853].  Other preconditioners   Incomplete Cholesky factorization  Incomplete LU factorization  Successive over-relaxation   Symmetric successive over-relaxation   Multigrid#Multigrid_preconditioning   External links   Preconditioned Conjugate Gradient ‚Äì math-linux.com  Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods   Preconditioning for eigenvalue problems  Eigenvalue problems can be framed in several alternative ways, each leading to its own preconditioning. The traditional preconditioning is based on the so-called spectral transformations. Knowing (approximately) the targeted eigenvalue, one can compute the corresponding eigenvector by solving the related homogeneous linear system, thus allowing to use preconditioning for linear system. Finally, formulating the eigenvalue problem as optimization of the Rayleigh quotient brings preconditioned optimization techniques to the scene.  Spectral transformations  By analogy with linear systems, for an eigenvalue problem     A  x   =   Œª  x         A  x     Œª  x     Ax=\lambda x   one may be tempted to replace the matrix   A   A   A   with the matrix     P   -  1    A       superscript  P    1    A    P^{-1}A   using a preconditioner   P   P   P   . However, this makes sense only if the seeking eigenvectors of   A   A   A   and     P   -  1    A       superscript  P    1    A    P^{-1}A   are the same. This is the case for spectral transformations.  The most popular spectral transformation is the so-called shift-and-invert transformation, where for a given scalar   Œ±   Œ±   \alpha   , called the shift , the original eigenvalue problem     A  x   =   Œª  x         A  x     Œª  x     Ax=\lambda x   is replaced with the shift-and-invert problem       (   A  -   Œ±  I    )    -  1    x   =   Œº  x          superscript    A    Œ±  I      1    x     Œº  x     (A-\alpha I)^{-1}x=\mu x   . The eigenvectors are preserved, and one can solve the shift-and-invert problem by an iterative solver, e.g., the power iteration . This gives the Inverse iteration , which normally converges to the eigenvector, corresponding to the eigenvalue closest to the shift   Œ±   Œ±   \alpha   . The Rayleigh quotient iteration is a shift-and-invert method with a variable shift.  Spectral transformations are specific for eigenvalue problems and have no analogs for linear systems. They require accurate numerical calculation of the transformation involved, which becomes the main bottleneck for large problems.  General preconditioning  To make a close connection to linear systems, let us suppose that the targeted eigenvalue    Œª  ‚ãÜ     subscript  Œª  normal-‚ãÜ    \lambda_{\star}   is known (approximately). Then one can compute the corresponding eigenvector from the homogeneous linear system      (   A  -    Œª  ‚ãÜ   I    )   x   =  0          A     subscript  Œª  normal-‚ãÜ   I    x   0    (A-\lambda_{\star}I)x=0   . Using the concept of left preconditioning for linear systems, we obtain     T   (   A  -    Œª  ‚ãÜ   I    )   x   =  0        T    A     subscript  Œª  normal-‚ãÜ   I    x   0    T(A-\lambda_{\star}I)x=0   , where   T   T   T   is the preconditioner, which we can try to solve using the Richardson iteration       ùê±   n  +  1    =   ùê±  n   -   Œ≥  n   T   (  A  -   Œª  ‚ãÜ   I  )   )  ùê±    n   ,  n  ‚â•  0.     fragments   subscript  ùê±    n  1      subscript  ùê±  n     subscript  Œ≥  n   T   fragments  normal-(  A    subscript  Œª  normal-‚ãÜ   I  normal-)   normal-)  x   n   normal-,  n   0.    \mathbf{x}_{n+1}=\mathbf{x}_{n}-\gamma_{n}T(A-\lambda_{\star}I))\mathbf{x}_{n}%
 ,\ n\geq 0.     The ideal preconditioning  The Moore‚ÄìPenrose pseudoinverse     T  =    (   A  -    Œª  ‚ãÜ   I    )   +       T   superscript    A     subscript  Œª  normal-‚ãÜ   I        T=(A-\lambda_{\star}I)^{+}   is the preconditioner, which makes the Richardson iteration above converge in one step with     Œ≥  n   =  1       subscript  Œ≥  n   1    \gamma_{n}=1   , since    I  -     (   A  -    Œª  ‚ãÜ   I    )   +    (   A  -    Œª  ‚ãÜ   I    )        I     superscript    A     subscript  Œª  normal-‚ãÜ   I        A     subscript  Œª  normal-‚ãÜ   I       I-(A-\lambda_{\star}I)^{+}(A-\lambda_{\star}I)   , denoted by    P  ‚ãÜ     subscript  P  normal-‚ãÜ    P_{\star}   , is the orthogonal projector on the eigenspace, corresponding to    Œª  ‚ãÜ     subscript  Œª  normal-‚ãÜ    \lambda_{\star}   . The choice    T  =    (   A  -    Œª  ‚ãÜ   I    )   +       T   superscript    A     subscript  Œª  normal-‚ãÜ   I        T=(A-\lambda_{\star}I)^{+}   is impractical for three independent reasons. First,    Œª  ‚ãÜ     subscript  Œª  normal-‚ãÜ    \lambda_{\star}   is actually not known, although it can be replaced with its approximation     Œª  ~   ‚ãÜ     subscript   normal-~  Œª   normal-‚ãÜ    \tilde{\lambda}_{\star}   . Second, the exact Moore‚ÄìPenrose pseudoinverse requires the knowledge of the eigenvector, which we are trying to find. This can be somewhat circumvented by the use of the Jacobi‚ÄìDavidson preconditioner     T  =    (   I  -    P  ~   ‚ãÜ    )     (   A  -     Œª  ~   ‚ãÜ   I    )    -  1     (   I  -    P  ~   ‚ãÜ    )        T      I   subscript   normal-~  P   normal-‚ãÜ     superscript    A     subscript   normal-~  Œª   normal-‚ãÜ   I      1      I   subscript   normal-~  P   normal-‚ãÜ       T=(I-\tilde{P}_{\star})(A-\tilde{\lambda}_{\star}I)^{-1}(I-\tilde{P}_{\star})   , where     P  ~   ‚ãÜ     subscript   normal-~  P   normal-‚ãÜ    \tilde{P}_{\star}   approximates    P  ‚ãÜ     subscript  P  normal-‚ãÜ    P_{\star}   . Last, but not least, this approach requires accurate numerical solution of linear system with the system matrix    (   A  -     Œª  ~   ‚ãÜ   I    )      A     subscript   normal-~  Œª   normal-‚ãÜ   I     (A-\tilde{\lambda}_{\star}I)   , which becomes as expensive for large problems as the shift-and-invert method above. If the solution is not accurate enough, step two may be redundant.  Practical preconditioning  Let us first replace the theoretical value    Œª  ‚ãÜ     subscript  Œª  normal-‚ãÜ    \lambda_{\star}   in the Richardson iteration above with its current approximation    Œª  n     subscript  Œª  n    \lambda_{n}   to obtain a practical algorithm        ùê±   n  +  1    =    ùê±  n   -    Œ≥  n   T   (   A  -    Œª  n   I    )    ùê±  n      ,   n  ‚â•  0.      formulae-sequence     subscript  ùê±    n  1       subscript  ùê±  n      subscript  Œ≥  n   T    A     subscript  Œª  n   I     subscript  ùê±  n        n  0.     \mathbf{x}_{n+1}=\mathbf{x}_{n}-\gamma_{n}T(A-\lambda_{n}I)\mathbf{x}_{n},\ n%
 \geq 0.     A popular choice is     Œª  n   =   œÅ   (   x  n   )         subscript  Œª  n     œÅ   subscript  x  n      \lambda_{n}=\rho(x_{n})   using the Rayleigh quotient function    œÅ   (  ‚ãÖ  )       œÅ  normal-‚ãÖ    \rho(\cdot)   . Practical preconditioning may be as trivial as just using    T  =    (   d  i  a  g   (  A  )    )    -  1        T   superscript    d  i  a  g  A     1      T=(diag(A))^{-1}   or     T  =    (   d  i  a  g   (   A  -    Œª  n   I    )    )    -  1     .      T   superscript    d  i  a  g    A     subscript  Œª  n   I       1      T=(diag(A-\lambda_{n}I))^{-1}.   For some classes of eigenvalue problems the efficiency of    T  ‚âà   A   -  1        T   superscript  A    1      T\approx A^{-1}   has been demonstrated, both numerically and theoretically. The choice    T  ‚âà   A   -  1        T   superscript  A    1      T\approx A^{-1}   allows one to easily utilize for eigenvalue problems the vast variety of preconditioners developed for linear systems.  Due to the changing value    Œª  n     subscript  Œª  n    \lambda_{n}   , a comprehensive theoretical convergence analysis is much more difficult, compared to the linear systems case, even for the simplest methods, such as the Richardson iteration .  External links   Templates for the Solution of Algebraic Eigenvalue Problems: a Practical Guide   Preconditioning in optimization  In optimization , preconditioning is typically used to accelerate first-order  optimization  algorithms .  Description  For example, to find a local minimum of a real-valued function    F   (  ùê±  )       F  ùê±    F(\mathbf{x})   using gradient descent , one takes steps proportional to the negative of the gradient     -    ‚àá  F    (  ùêö  )           normal-‚àá  F   ùêö     -\nabla F(\mathbf{a})   (or of the approximate gradient) of the function at the current point:        ùê±   n  +  1    =    ùê±  n   -    Œ≥  n    ‚àá  F    (   ùê±  n   )      ,   n  ‚â•  0.      formulae-sequence     subscript  ùê±    n  1       subscript  ùê±  n      subscript  Œ≥  n    normal-‚àá  F    subscript  ùê±  n        n  0.     \mathbf{x}_{n+1}=\mathbf{x}_{n}-\gamma_{n}\nabla F(\mathbf{x}_{n}),\ n\geq 0.     The preconditioner is applied to the gradient:        ùê±   n  +  1    =    ùê±  n   -    Œ≥  n    P   -  1     ‚àá  F    (   ùê±  n   )      ,   n  ‚â•  0.      formulae-sequence     subscript  ùê±    n  1       subscript  ùê±  n      subscript  Œ≥  n    superscript  P    1     normal-‚àá  F    subscript  ùê±  n        n  0.     \mathbf{x}_{n+1}=\mathbf{x}_{n}-\gamma_{n}P^{-1}\nabla F(\mathbf{x}_{n}),\ n%
 \geq 0.     Preconditioning here can be viewed as changing the geometry of the vector space with the goal to make the level sets look like circles. In this case the preconditioned gradient aims closer to the point of the extrema as on the figure, which speeds up the convergence.  Connection to linear systems  The minimum of a quadratic function       F   (  ùê±  )    =     1  2    ùê±  T   A  ùê±   -    ùê±  T   ùêõ          F  ùê±         1  2    superscript  ùê±  T   A  ùê±      superscript  ùê±  T   ùêõ      F(\mathbf{x})=\frac{1}{2}\mathbf{x}^{T}A\mathbf{x}-\mathbf{x}^{T}\mathbf{b}   ,  where   ùê±   ùê±   \mathbf{x}   and   ùêõ   ùêõ   \mathbf{b}   are real column-vectors and   A   A   A   is a real symmetric  positive-definite matrix , is exactly the solution of the linear equation     A  ùê±   =  ùêõ        A  ùê±   ùêõ    A\mathbf{x}=\mathbf{b}   . Since      ‚àá  F    (  ùê±  )    =    A  ùê±   -  ùêõ          normal-‚àá  F   ùê±       A  ùê±   ùêõ     \nabla F(\mathbf{x})=A\mathbf{x}-\mathbf{b}   , the preconditioned gradient descent method of minimizing    F   (  ùê±  )       F  ùê±    F(\mathbf{x})   is        ùê±   n  +  1    =    ùê±  n   -    Œ≥  n    P   -  1     (    A   ùê±  n    -  ùêõ   )      ,   n  ‚â•  0.      formulae-sequence     subscript  ùê±    n  1       subscript  ùê±  n      subscript  Œ≥  n    superscript  P    1        A   subscript  ùê±  n    ùêõ        n  0.     \mathbf{x}_{n+1}=\mathbf{x}_{n}-\gamma_{n}P^{-1}(A\mathbf{x}_{n}-\mathbf{b}),%
 \ n\geq 0.     This is the preconditioned Richardson iteration for solving a system of linear equations .  Connection to eigenvalue problems  The minimum of the Rayleigh quotient        œÅ   (  ùê±  )    =     ùê±  T   A  ùê±     ùê±  T   ùê±     ,        œÅ  ùê±        superscript  ùê±  T   A  ùê±      superscript  ùê±  T   ùê±      \rho(\mathbf{x})=\frac{\mathbf{x}^{T}A\mathbf{x}}{\mathbf{x}^{T}\mathbf{x}},     where   ùê±   ùê±   \mathbf{x}   is a real non-zero column-vector and   A   A   A   is a real symmetric  positive-definite matrix , is the smallest eigenvalue of   A   A   A   , while the minimizer is the corresponding eigenvector . Since     ‚àá  œÅ    (  ùê±  )        normal-‚àá  œÅ   ùê±    \nabla\rho(\mathbf{x})   is proportional to     A  ùê±   -   œÅ   (  ùê±  )   ùê±         A  ùê±     œÅ  ùê±  ùê±     A\mathbf{x}-\rho(\mathbf{x})\mathbf{x}   , the preconditioned gradient descent method of minimizing    œÅ   (  ùê±  )       œÅ  ùê±    \rho(\mathbf{x})   is        ùê±   n  +  1    =    ùê±  n   -    Œ≥  n    P   -  1     (    A   ùê±  n    -   œÅ   (   ùê±  ùêß   )    ùê±  ùêß     )      ,   n  ‚â•  0.      formulae-sequence     subscript  ùê±    n  1       subscript  ùê±  n      subscript  Œ≥  n    superscript  P    1        A   subscript  ùê±  n      œÅ   subscript  ùê±  ùêß    subscript  ùê±  ùêß          n  0.     \mathbf{x}_{n+1}=\mathbf{x}_{n}-\gamma_{n}P^{-1}(A\mathbf{x}_{n}-\rho(\mathbf{%
 x_{n}})\mathbf{x_{n}}),\ n\geq 0.     This is an analog of preconditioned Richardson iteration for solving eigenvalue problems.  Variable preconditioning  In many cases, it may be beneficial to change the preconditioner at some or even every step of an iterative algorithm in order to accommodate for a changing shape of the level sets, as in        ùê±   n  +  1    =    ùê±  n   -    Œ≥  n    P  n   -  1     ‚àá  F    (   ùê±  n   )      ,   n  ‚â•  0.      formulae-sequence     subscript  ùê±    n  1       subscript  ùê±  n      subscript  Œ≥  n    superscript   subscript  P  n     1     normal-‚àá  F    subscript  ùê±  n        n  0.     \mathbf{x}_{n+1}=\mathbf{x}_{n}-\gamma_{n}P_{n}^{-1}\nabla F(\mathbf{x}_{n}),%
 \ n\geq 0.     One should have in mind, however, that constructing an efficient preconditioner is very often computationally expensive. The increased cost of updating the preconditioner can easily override the positive effect of faster convergence.  References       "  Category:Numerical linear algebra   