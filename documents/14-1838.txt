   Probability bounds analysis      Probability bounds analysis   Probability bounds analysis (PBA) is a collection of methods of uncertainty propagation for making qualitative and quantitative calculations in the face of uncertainties of various kinds. It is used to project partial information about random variables and other quantities through mathematical expressions. For instance, it computes sure bounds on the distribution of a sum, product, or more complex function, given only sure bounds on the distributions of the inputs. Such bounds are called probability boxes , and constrain cumulative probability distributions (rather than densities or mass functions ).  This bounding approach permits analysts to make calculations without requiring overly precise assumptions about parameter values, dependence among variables, or even distribution shape. Probability bounds analysis is essentially a combination of the methods of standard interval analysis and classical probability theory . Probability bounds analysis gives the same answer as interval analysis does when only range information is available. It also gives the same answers as Monte Carlo simulation does when information is abundant enough to precisely specify input distributions and their dependencies. Thus, it is a generalization of both interval analysis and probability theory.  The diverse methods comprising probability bounds analysis provide algorithms to evaluate mathematical expressions when there is uncertainty about the input values, their dependencies, or even the form of mathematical expression itself. The calculations yield results that are guaranteed to enclose all possible distributions of the output variable if the input p-boxes were also sure to enclose their respective distributions. In some cases, a calculated p-box will also be best-possible in the sense that the bounds could be no tighter without excluding some of the possible distributions.  P-boxes are usually merely bounds on possible distributions. The bounds often also enclose distributions that are not themselves possible. For instance, the set of probability distributions that could result from adding random values without the independence assumption from two (precise) distributions is generally a proper subset of all the distributions enclosed by the p-box computed for the sum. That is, there are distributions within the output p-box that could not arise under any dependence between the two input distributions. The output p-box will, however, always contain all distributions that are possible, so long as the input p-boxes were sure to enclose their respective underlying distributions. This property often suffices for use in risk analysis and other fields requiring calculations under uncertainty.  History of bounding probability  The idea of bounding probability has a very long tradition throughout the history of probability theory. Indeed, in 1854 George Boole used the notion of interval bounds on probability in his The Laws of Thought . 1 2 Also dating from the latter half of the 19th century, the inequality attributed to Chebyshev described bounds on a distribution when only the mean and variance of the variable are known, and the related inequality attributed to Markov found bounds on a positive variable when only the mean is known. Kyburg 3 reviewed the history of interval probabilities and traced the development of the critical ideas through the 20th century, including the important notion of incomparable probabilities favored by Keynes . Of particular note is Fr√©chet 's derivation in the 1930s of bounds on calculations involving total probabilities without dependence assumptions. Bounding probabilities has continued to the present day (e.g., Walley's theory of imprecise probability . 4 )  The methods of probability bounds analysis that could be routinely used in risk assessments were developed in the 1980s. Hailperin 5 described a computational scheme for bounding logical calculations extending the ideas of Boole. Yager 6 described the elementary procedures by which bounds on convolutions can be computed under an assumption of independence. At about the same time, Makarov, 7 and independently, R√ºschendorf 8 solved the problem, originally posed by Kolmogorov , of how to find the upper and lower bounds for the probability distribution of a sum of random variables whose marginal distributions, but not their joint distribution, are known. Frank et al. 9 generalized the result of Makarov and expressed it in terms of copulas . Since that time, formulas and algorithms for sums have been generalized and extended to differences, products, quotients and other binary and unary functions under various dependence assumptions. 10 11 12 13 14 15  Arithmetic expressions  Arithmetic expressions involving operations such as additions, subtractions, multiplications, divisions, minima, maxima, powers, exponentials, logarithms, square roots, absolute values, etc., are commonly used in risk analyses and uncertainty modeling. Convolution is the operation of finding the probability distribution of a sum of independent random variables specified by probability distributions. We can extend the term to finding distributions of other mathematical functions (products, differences, quotients, and more complex functions) and other assumptions about the intervariable dependencies. There are convenient algorithms for computing these generalized convolutions under a variety of assumptions about the dependencies among the inputs. 16 17 18 19  Mathematical details  Let  denote the space of distribution functions on the real numbers , i.e.,  = { D | D :  ‚Üí [0,1], D ( x ) ‚â§ D ( y ) whenever x 1, i 2 ], i 1 ‚â§ i 2 , i 1 , i 2 ‚àà }. Then a p-box is a quintuple { , F , m , v , F }, where , F ‚àà , while m , v ‚àà , and F ‚äÜ . This quintuple denotes the set of distribution functions F ‚àà F ‚äÜ  such that ( x ) ‚â§ F ( x ) ‚â§ F ( x ) for all x ‚àà }, and the mean and variance of F are in the intervals m and v respectively.  If F is a distribution function and B is a p-box , the notation F ‚àà B means that F is an element of B = { B 1 , B 2 , [ m 1 , m 2 ], [ v 1 , v 2 ], B }, that is, B 2 ( x ) ‚â§ F ( x ) ‚â§ B 1 ( x ), for all x ‚àà , E ( F ) ‚àà [ m 1 , m 2 ], V ( F ) ‚àà [ v 1 , v 2 ], and F ‚àà B . We sometimes say F is inside  B . In some cases, there may be no information about the moments or distribution family other than what is encoded in the two distribution functions that constitute the edges of the p-box. Then the quintuple representing the p-box { B 1 , B 2 , [‚àí‚àû,‚àû], [0,‚àû], ùîª} can be denoted more compactly as [ B 1 , B 2 ]. This notation harkens to that of intervals on the real line, except that the endpoints are distributions rather than points.  The notation X ~ F denotes the fact that X ‚àà is a random variable governed by the distribution function F , that is, F = F ( x ):‚Üí[0,1]:x‚ÜíPr( X ‚â§ x ).  Let us generalize the tilde notation for use with p-boxes. We will write X ~ B to mean that X is a random variable whose distribution function is unknown except that it is inside B . Thus, X ~ F ‚àà B can be contracted to X ~ B without mentioning the distribution function explicitly.  If X and Y are independent random variables with distributions F and G respectively, then X + Y = Z ~ H given by   H ( z ) = ‚à´ z=x+y  F ( x ) G ( y ) d z = ‚à´  F ( x ) G ( z ‚àí x ) d x = F * G .   This operation is called a convolution on F and G . The analogous operation on p-boxes is straightforward for sums. Suppose   X ~ A = [ A 1 , A 2 ] and  Y ~ B = [ B 1 , B 2 ].   If X and Y are stochastically independent, then the distribution of Z = X + Y is inside the p-box   [ A 1 * B 1 , A 2 * B 2 ].   Finding bounds on the distribution of sums Z = X + Y  without making any assumption about the dependence between X and Y is actually easier than the problem assuming independence. Makarov 20 21 22 showed that   Z ~ [ sup x+y=z max( F ( x ) + G ( y ) ‚àí 1, 0), inf x+y=z min( F ( x ) + G ( y ), 1) ] .   These bounds are implied by the Fr√©chet‚ÄìHoeffding  copula bounds. The problem can also be solved using the methods of mathematical programming . 23  The convolution under the intermediate assumption that X and Y have positive dependence is likewise easy to compute, as is the convolution under the extreme assumptions of perfect positive or perfect negative dependency between X and Y . 24  Generalized convolutions for other operations such as subtraction, multiplication, division, etc., can be derived using transformations. For instance, p-box subtraction A ‚àí B can be defined as A + (‚àí B ), where the negative of a p-box B =[ B 1 , B 2 ] is [ B 2 (‚àí x ), B 1 (‚àí x )].  Logical expressions  Logical or Boolean expressions involving conjunctions ( AND operations), disjunctions ( OR operations), exclusive disjunctions, equivalences, conditionals, etc. arise in the analysis of fault trees and event trees common in risk assessments. If the probabilities of events are characterized by intervals, as suggested by Boole 25 and Keynes 26 among others, these binary operations are straightforward to evaluate. For example, if the probability of an event A is in the interval P(A) = a = [0.2, 0.25], and the probability of the event B is in P(B) = b = [0.1, 0.3], then the probability of the conjunction is surely in the interval   P(A & B) = a √ó b    = [0.2, 0.25] √ó [0.1, 0.3]  = [0.2 √ó 0.1, 0.25 √ó 0.3]  = [0.02, 0.075]       so long as A and B can be assumed to be independent events. If they are not independent, we can still bound the conjunction using the classical Fr√©chet inequality . In this case, we can infer at least that the probability of the joint event A & B is surely within the interval   P(A & B) = env(max(0, a + b ‚àí1), min( a , b ))   = env(max(0, [0.2, 0.25]+[0.1, 0.3]‚àí1), min([0.2, 0.25], [0.1, 0.3]))  = env([max(0, 0.2+0.1‚Äì1), max(0, 0.25+0.3‚Äì1)], [min(0.2,0.1), min(0.25, 0.3)])  = env([0,0], [0.1, 0.25])  = [0, 0.25]       where env([ x 1 , x 2 ], [ y 1 , y 2 ]) is [min( x 1 , y 1 ), max( x 2 , y 2 )]. Likewise, the probability of the disjunction is surely in the interval   P(A v B) = a + b ‚àí a √ó b = 1 ‚àí (1 ‚àí a ) √ó (1 ‚àí b )   = 1 ‚àí (1 ‚àí [0.2, 0.25]) √ó (1 ‚àí [0.1, 0.3])  = 1 ‚àí [0.75, 0.8] √ó [0.7, 0.9]  = 1 ‚àí [0.525, 0.72]  = [0.28, 0.475]       if A and B are independent events. If they are not independent, the Fr√©chet inequality bounds the disjunction   P(A v B) = env(max( a , b ), min(1, a + b ))   = env(max([0.2, 0.25], [0.1, 0.3]), min(1, [0.2, 0.25] + [0.1, 0.3]))  = env([0.2, 0.3], [0.3, 0.55])  = [0.2, 0.55].       It is also possible to compute interval bounds on the conjunction or disjunction under other assumptions about the dependence between A and B. For instance, one might assume they are positively dependent, in which case the resulting interval is not as tight as the answer assuming independence but tighter than the answer given by the Fr√©chet inequality. Comparable calculations are used for other logical functions such as negation, exclusive disjunction, etc. When the Boolean expression to be evaluated becomes complex, it may be necessary to evaluate it using the methods of mathematical programming 27 to get best-possible bounds on the expression. If the probabilities of the events are characterized by probability distributions or p-boxes rather than intervals, then analogous calculations can be done to obtain distributional or p-box results characterizing the probability of the top event.  Magnitude comparisons  The probability that an uncertain number represented by a p-box D is less than zero is the interval Pr( D  F (0), FÃÖ (0)], where FÃÖ (0) is the left bound of the probability box D and F (0) is its right bound, both evaluated at zero. Two uncertain numbers represented by probability boxes may then be compared for numerical magnitude with the following encodings:   A < B = Pr( A ‚àí B < 0),  A > B = Pr( B ‚àí A < 0),  A ‚â§ B = Pr( A ‚àí B ‚â§ 0), and  A ‚â• B = Pr( B ‚àí A ‚â§ 0).   Thus the probability that A is less than B is the same as the probability that their difference is less than zero, and this probability can be said to be the value of the expression A Alvarez, D. A., 2006. On the calculation of the bounds of probability of events using infinite random sets. International Journal of Approximate Reasoning  43 : 241‚Äì267. 28 29 30 31 32 use sampling-based approaches to computing probability bounds, including Monte Carlo simulation , Latin hypercube methods or importance sampling . These approaches cannot assure mathematical rigor in the result because such simulation methods are approximations, although their performance can generally be improved simply by increasing the number of replications in the simulation. Thus, unlike the analytical theorems or methods based on mathematical programming, sampling-based calculations usually cannot produce verified computations . However, sampling-based methods can be very useful in addressing a variety of problems which are computationally difficult to solve analytically or even to rigorously bound. One important example is the use of Cauchy-deviate sampling to avoid the curse of dimensionality in propagating interval uncertainty through high-dimensional problems. 33  Relationship to other uncertainty propagation approaches  PBA belongs to a class of methods that use imprecise probabilities to simultaneously represent aleatoric and epistemic uncertainties . PBA is a generalization of both interval analysis and probabilistic convolution such as is commonly implemented with Monte Carlo simulation . PBA is also closely related to robust Bayes analysis , which is sometimes called Bayesian sensitivity analysis . PBA is an alternative to second-order Monte Carlo simulation .  Applications  See also   Probability box  Robust Bayes analysis  Imprecise probability  Second-order Monte Carlo simulation  Monte Carlo simulation  Interval analysis  Probability theory  Risk analysis   References  Further references       External links   Probability bounds analysis in environmental risk assessments  Intervals and probability distributions  Epistemic uncertainty project  The Society for Imprecise Probability: Theories and Applications   "  Category:Probability bounds analysis  Category:Risk analysis  Category:Mathematical analysis     ‚Ü©  ‚Ü©  Kyburg, H.E., Jr. (1999). Interval valued probabilities . SIPTA Documention on Imprecise Probability. ‚Ü©  ‚Ü©   Yager, R.R. (1986). Arithmetic and other operations on Dempster‚ÄìShafer structures. International Journal of Man-machine Studies  25 : 357‚Äì366. ‚Ü©  Makarov, G.D. (1981). Estimates for the distribution function of a sum of two random variables when the marginal distributions are fixed. Theory of Probability and Its Applications  26 : 803‚Äì806. ‚Ü©  R√ºschendorf, L. (1982). Random variables with maximum sums. Advances in Applied Probability  14 : 623‚Äì632. ‚Ü©  Frank, M.J., R.B. Nelsen and B. Schweizer (1987). Best-possible bounds for the distribution of a sum‚Äîa problem of Kolmogorov. Probability Theory and Related Fields  74 : 199‚Äì211. ‚Ü©  Williamson, R.C., and T. Downs (1990). Probabilistic arithmetic I: Numerical methods for calculating convolutions and dependency bounds. International Journal of Approximate Reasoning  4 : 89‚Äì158. ‚Ü©  Ferson, S., V. Kreinovich, L. Ginzburg, D.S. Myers, and K. Sentz. (2003). Constructing Probability Boxes and Dempster‚ÄìShafer Structures . SAND2002-4015. Sandia National Laboratories, Albuquerque, NM. ‚Ü©  Berleant, D. (1993). Automatically verified reasoning with both intervals and probability density functions. Interval Computations '''1993 (2) ''': 48‚Äì70. ‚Ü©  Berleant, D., G. Anderson, and C. Goodman-Strauss (2008). Arithmetic on bounded families of distributions: a DEnv algorithm tutorial. Pages 183‚Äì210 in Knowledge Processing with Interval and Soft Computing , edited by C. Hu, R.B. Kearfott, A. de Korvin and V. Kreinovich, Springer (ISBN 978-1-84800-325-5). ‚Ü©  Berleant, D., and C. Goodman-Strauss (1998). Bounding the results of arithmetic operations on random variables of unknown dependency using intervals. Reliable Computing  4 : 147‚Äì165. ‚Ü©  Ferson, S., R. Nelsen, J. Hajagos, D. Berleant, J. Zhang, W.T. Tucker, L. Ginzburg and W.L. Oberkampf (2004). Dependence in Probabilistic Modeling, Dempster‚ÄìShafer Theory, and Probability Bounds Analysis . Sandia National Laboratories, SAND2004-3072, Albuquerque, NM. ‚Ü©              Baraldi, P., Popescu, I. C., Zio, E., 2008. Predicting the time to failure of a randomly degrading component by a hybrid Monte Carlo and possibilistic method. IEEE Proc. International Conference on Prognostics and Health Management . ‚Ü©  Batarseh, O. G., Wang, Y., 2008. Reliable simulation with input uncertainties using an interval-based approach. IEEE Proc. Winter Simulation Conference . ‚Ü©  Roy, Christopher J., and Michael S. Balch (2012). A holistic approach to uncertainty quantification with application to supersonic nozzle thrust. International Journal for Uncertainty Quantification  2 (4): 363‚Äì81 . ‚Ü©  Zhang, H., Mullen, R. L., Muhanna, R. L. (2010). Interval Monte Carlo methods for structural reliability. Structural Safety  32 : 183‚Äì190. ‚Ü©  Zhang, H., Dai, H., Beer, M., Wang, W. (2012). Structural reliability analysis on the basis of small samples: an interval quasi-Monte Carlo method. Mechanical Systems and Signal Processing  37 (1‚Äì2): 137‚Äì51 . ‚Ü©  Trejo, R., Kreinovich, V. (2001). Error estimations for indirect measurements: randomized vs. deterministic algorithms for ‚Äòblack-box‚Äô programs . Handbook on Randomized Computing , S. Rajasekaran, P. Pardalos, J. Reif, and J. Rolim (eds.), Kluwer, 673‚Äì729. ‚Ü©     