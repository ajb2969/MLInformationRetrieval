   Semiparametric regression      Semiparametric regression   In statistics , semiparametric regression includes regression models that combine parametric and nonparametric models. They are often used in situations where the fully nonparametric model may not perform well or when the researcher wants to use a parametric model but the functional form with respect to a subset of the regressors or the density of the errors is not known. Semiparametric regression models are a particular type of semiparametric modelling and, since semiparametric models contain a parametric component, they rely on parametric assumptions and may be misspecified and inconsistent , just like a fully parametric model.  Methods  Many different semiparametric regression methods have been proposed and developed. The most popular methods are the partially linear, index and varying coefficient models.  Partially linear models  A partially linear model is given by         Y  i   =     X  i  ′   β   +   g   (   Z  i   )    +   u  i     ,   i  =   1  ,  …  ,  n     ,     formulae-sequence     subscript  Y  i        subscript   superscript  X  normal-′   i   β     g   subscript  Z  i     subscript  u  i       i   1  normal-…  n      Y_{i}=X^{\prime}_{i}\beta+g\left(Z_{i}\right)+u_{i},\,\quad i=1,\ldots,n,\,     where    Y  i     subscript  Y  i    Y_{i}   is the dependent variable,    X  i     subscript  X  i    X_{i}   and    Z  i     subscript  Z  i    Z_{i}   are    p  ×  1      p  1    p\times 1   vectors of explanatory variables,   β   β   \beta   is a    p  ×  1      p  1    p\times 1   vector of unknown parameters and     Z  i   ∈   R  q        subscript  Z  i    superscript  normal-R  q     Z_{i}\in\operatorname{R}^{q}   . The parametric part of the partially linear model is given by the parameter vector   β   β   \beta   while the nonparametric part is the unknown function    g   (   Z  i   )       g   subscript  Z  i     g\left(Z_{i}\right)   . The data is assumed to be i.i.d. with    E   (   u  i   |   X  i   ,   Z  i   )   =  0     fragments  E   fragments  normal-(   subscript  u  i   normal-|   subscript  X  i   normal-,   subscript  Z  i   normal-)    0    E\left(u_{i}|X_{i},Z_{i}\right)=0   and the model allows for a conditionally heteroskedastic error process    E   (   u  i  2   |  x  ,  z  )   =   σ  2    (  x  ,  z  )      fragments  E   fragments  normal-(   subscript   superscript  u  2   i   normal-|  x  normal-,  z  normal-)     superscript  σ  2    fragments  normal-(  x  normal-,  z  normal-)     E\left(u^{2}_{i}|x,z\right)=\sigma^{2}\left(x,z\right)   of unknown form. This type of model was proposed by Robinson (1988) and extended to handle categorical covariates by Racine and Liu (2007).  This method is implemented by obtaining a    n      n    \sqrt{n}   consistent estimator of   β   β   \beta   and then deriving an estimator of    g   (   Z  i   )       g   subscript  Z  i     g\left(Z_{i}\right)   from the nonparametric regression of     Y  i   -    X  i  ′    β  ^         subscript  Y  i      subscript   superscript  X  normal-′   i    normal-^  β      Y_{i}-X^{\prime}_{i}\hat{\beta}   on   z   z   z   using an appropriate nonparametric regression method. 1  Index models  A single index model takes the form       Y  =    g   (    X  ′    β  0    )    +  u    ,      Y      g     superscript  X  normal-′    subscript  β  0     u     Y=g\left(X^{\prime}\beta_{0}\right)+u,\,     where   Y   Y   Y   ,   X   X   X   and    β  0     subscript  β  0    \beta_{0}   are defined as earlier and the error term   u   u   u   satisfies    E   (  u  |  X  )   =  0     fragments  E   fragments  normal-(  u  normal-|  X  normal-)    0    E\left(u|X\right)=0   . The single index model takes its name from the parametric part of the model     x  ′   β       superscript  x  normal-′   β    x^{\prime}\beta   which is a scalar single index. The nonparametric part is the unknown function    g   (  ⋅  )       g  normal-⋅    g\left(\cdot\right)   .  Ichimura's method  The single index model method developed by Ichimura (1993) is as follows. Consider the situation in which   y   y   y   is continuous. Given a known form for the function    g   (  ⋅  )       g  normal-⋅    g\left(\cdot\right)   ,    β  0     subscript  β  0    \beta_{0}   could be estimated using the nonlinear least squares method to minimize the function        ∑   i  =  1      (    Y  i   -   g   (    X  i  ′   β   )     )   2    .      subscript     i  1     superscript     subscript  Y  i     g     subscript   superscript  X  normal-′   i   β     2     \sum_{i=1}\left(Y_{i}-g\left(X^{\prime}_{i}\beta\right)\right)^{2}.     Since the functional form of    g   (  ⋅  )       g  normal-⋅    g\left(\cdot\right)   is not known, we need to estimate it. For a given value for   β   β   \beta   an estimate of the function      G   (   X  i  ′   β  )   =  E   (   Y  i   |   X  i  ′   β  )   =  E   [  g   (   X  i  ′    β  o   )   |   X  i  ′   β  ]      fragments  G   fragments  normal-(   subscript   superscript  X  normal-′   i   β  normal-)    E   fragments  normal-(   subscript  Y  i   normal-|   subscript   superscript  X  normal-′   i   β  normal-)    E   fragments  normal-[  g   fragments  normal-(   subscript   superscript  X  normal-′   i    subscript  β  o   normal-)   normal-|   subscript   superscript  X  normal-′   i   β  normal-]     G\left(X^{\prime}_{i}\beta\right)=E\left(Y_{i}|X^{\prime}_{i}\beta\right)=E%
 \left[g\left(X^{\prime}_{i}\beta_{o}\right)|X^{\prime}_{i}\beta\right]     using kernel method. Ichimura (1993) proposes estimating    g   (    X  i  ′   β   )       g     subscript   superscript  X  normal-′   i   β     g\left(X^{\prime}_{i}\beta\right)   with         G  ^    -  i     (    X  i  ′   β   )    ,       subscript   normal-^  G     i       subscript   superscript  X  normal-′   i   β     \hat{G}_{-i}\left(X^{\prime}_{i}\beta\right),\,     the leave-one-out  nonparametric kernel estimator of    G   (    X  i  ′   β   )       G     subscript   superscript  X  normal-′   i   β     G\left(X^{\prime}_{i}\beta\right)   .  Klein and Spady's estimator  If the dependent variable   y   y   y   is binary and    X  i     subscript  X  i    X_{i}   and    u  i     subscript  u  i    u_{i}   are assumed to be independent , Klein and Spady (1993) propose a technique for estimating   β   β   \beta   using maximum likelihood methods. The log-likelihood function is given by        L   (  β  )    =     ∑  i     (   1  -   Y  i    )    ln   (   1  -     g  ^    -  i     (    X  i  ′   β   )     )      +    ∑  i     Y  i    ln   (     g  ^    -  i     (    X  i  ′   β   )    )        ,        L  β       subscript   i       1   subscript  Y  i        1     subscript   normal-^  g     i       subscript   superscript  X  normal-′   i   β          subscript   i      subscript  Y  i        subscript   normal-^  g     i       subscript   superscript  X  normal-′   i   β          L\left(\beta\right)=\sum_{i}\left(1-Y_{i}\right)\ln\left(1-\hat{g}_{-i}\left(X%
 ^{\prime}_{i}\beta\right)\right)+\sum_{i}Y_{i}\ln\left(\hat{g}_{-i}\left(X^{%
 \prime}_{i}\beta\right)\right),     where      g  ^    -  i     (    X  i  ′   β   )        subscript   normal-^  g     i       subscript   superscript  X  normal-′   i   β     \hat{g}_{-i}\left(X^{\prime}_{i}\beta\right)   is the leave-one-out estimator.  Smooth coefficient/varying coefficient models  Hastie and Tibshirani (1993) propose a smooth coefficient model given by        Y  i   =    α   (   Z  i   )    +    X  i  ′   β   (   Z  i   )    +   u  i    =     (   1  +   X  i  ′    )    (      α   (   Z  i   )         β   (   Z  i   )       )    +   u  i    =     W  i  ′   γ   (   Z  i   )    +   u  i     ,         subscript  Y  i       α   subscript  Z  i       subscript   superscript  X  normal-′   i   β   subscript  Z  i     subscript  u  i               1   subscript   superscript  X  normal-′   i        α   subscript  Z  i        β   subscript  Z  i        subscript  u  i              subscript   superscript  W  normal-′   i   γ   subscript  Z  i     subscript  u  i       Y_{i}=\alpha\left(Z_{i}\right)+X^{\prime}_{i}\beta\left(Z_{i}\right)+u_{i}=%
 \left(1+X^{\prime}_{i}\right)\left(\begin{array}[]{c}\alpha\left(Z_{i}\right)%
 \\
 \beta\left(Z_{i}\right)\end{array}\right)+u_{i}=W^{\prime}_{i}\gamma\left(Z_{i%
 }\right)+u_{i},     where    X  i     subscript  X  i    X_{i}   is a    k  ×  1      k  1    k\times 1   vector and    β   (  z  )       β  z    \beta\left(z\right)   is a vector of unspecified smooth functions of   z   z   z   .      γ   (  ⋅  )       γ  normal-⋅    \gamma\left(\cdot\right)   may be expressed as      γ   (   Z  i   )   =    (  E   [   W  i    W  i  ′   |   Z  i   ]   )    -  1    E   [   W  i    Y  i   |   Z  i   ]   .     fragments  γ   fragments  normal-(   subscript  Z  i   normal-)     superscript   fragments  normal-(  E   fragments  normal-[   subscript  W  i    subscript   superscript  W  normal-′   i   normal-|   subscript  Z  i   normal-]   normal-)     1    E   fragments  normal-[   subscript  W  i    subscript  Y  i   normal-|   subscript  Z  i   normal-]   normal-.    \gamma\left(Z_{i}\right)=\left(E\left[W_{i}W^{\prime}_{i}|Z_{i}\right]\right)^%
 {-1}E\left[W_{i}Y_{i}|Z_{i}\right].     See also   Nonparametric regression   Notes  References          "  Category:Regression analysis  Category:Non-parametric statistics     See Li and Racine (2007) for an in-depth look at nonparametric regression methods. ↩     