   Pointwise mutual information      Pointwise mutual information   Pointwise mutual information ( PMI ), 1 or point mutual information , is a measure of association used in information theory and statistics . In contrast to mutual information (MI) which builds upon PMI, it refers to single events, whereas MI refers to the average of all possible events.  Definition  The PMI of a pair of outcomes  x and y belonging to discrete random variables  X and Y quantifies the discrepancy between the probability of their coincidence given their joint distribution and their individual distributions, assuming independence . Mathematically:        pmi   (  x  ;  y  )    ≡   log    p   (  x  ,  y  )     p   (  x  )   p   (  y  )      =   log    p   (  x  |  y  )     p   (  x  )      =   log    p   (  y  |  x  )     p   (  y  )       .         pmi  x  y         p   x  y      p  x  p  y               fragments  p   fragments  normal-(  x  normal-|  y  normal-)      p  x               fragments  p   fragments  normal-(  y  normal-|  x  normal-)      p  y        \operatorname{pmi}(x;y)\equiv\log\frac{p(x,y)}{p(x)p(y)}=\log\frac{p(x|y)}{p(x%
 )}=\log\frac{p(y|x)}{p(y)}.     The mutual information (MI) of the random variables X and Y is the expected value of the PMI over all possible outcomes (with respect to the joint distribution    p   (  x  ,  y  )       p   x  y     p(x,y)   ).  The measure is symmetric (     pmi   (  x  ;  y  )    =   pmi   (  y  ;  x  )         pmi  x  y    pmi  y  x     \operatorname{pmi}(x;y)=\operatorname{pmi}(y;x)   ). It can take positive or negative values, but is zero if X and Y are independent . Note that even though PMI may be negative or positive, its expected outcome over all joint events (MI) is positive. PMI maximizes when X and Y are perfectly associated (i.e.    p   (  x  |  y  )      fragments  p   fragments  normal-(  x  normal-|  y  normal-)     p(x|y)   or    p   (  y  |  x  )   =  1     fragments  p   fragments  normal-(  y  normal-|  x  normal-)    1    p(y|x)=1   ), yielding the following bounds:        -  ∞   ≤   pmi   (  x  ;  y  )    ≤   min   [   -    log  p    (  x  )     ,   -    log  p    (  y  )     ]     .             pmi  x  y                p   x          p   y        -\infty\leq\operatorname{pmi}(x;y)\leq\min\left[-\log p(x),-\log p(y)\right].     Finally,    pmi   (  x  ;  y  )      pmi  x  y    \operatorname{pmi}(x;y)   will increase if    p   (  x  |  y  )      fragments  p   fragments  normal-(  x  normal-|  y  normal-)     p(x|y)   is fixed but    p   (  x  )       p  x    p(x)   decreases.  Here is an example to illustrate:      x   y   p ( x , y )       0   0   0.1     0   1   0.7     1   0   0.15     1   1   0.05     Using this table we can marginalize to get the following additional table for the individual distributions:       p ( x )   p ( y )       0   0.8   0.25     1   0.2   0.75     With this example, we can compute four values for    p  m  i   (  x  ;  y  )       p  m  i   x  y     pmi(x;y)   . Using base-2 logarithms:      pmi(x=0;y=0)   =   −1     pmi(x=0;y=1)   =   0.222392421     pmi(x=1;y=0)   =   1.584962501     pmi(x=1;y=1)   =   −1.584962501       (For reference, the mutual information     I   (  X  ;  Y  )      normal-I  X  Y    \operatorname{I}(X;Y)   would then be 0.214170945)  Similarities to mutual information  Pointwise Mutual Information has many of the same relationships as the mutual information. In particular,         pmi   (  x  ;  y  )      =       h   (  x  )    +   h   (  y  )     -   h   (  x  ,  y  )          =     h   (  x  )   -  h   (  x  |  y  )         =     h   (  y  )   -  h   (  y  |  x  )           pmi  x  y          h  x     h  y      h   x  y        missing-subexpression     fragments  h   fragments  normal-(  x  normal-)    h   fragments  normal-(  x  normal-|  y  normal-)       missing-subexpression     fragments  h   fragments  normal-(  y  normal-)    h   fragments  normal-(  y  normal-|  x  normal-)       \begin{aligned}\displaystyle\operatorname{pmi}(x;y)&\displaystyle=&%
 \displaystyle h(x)+h(y)-h(x,y)\\
 &\displaystyle=&\displaystyle h(x)-h(x|y)\\
 &\displaystyle=&\displaystyle h(y)-h(y|x)\end{aligned}     Where    h   (  x  )       h  x    h(x)   is the self-information , or    -   log  2   p   (  X  =  x  )      fragments    subscript   2   p   fragments  normal-(  X   x  normal-)     -\log_{2}p(X=x)   .  Normalized pointwise mutual information (npmi)  Pointwise mutual information can be normalized between [-1,+1] resulting in -1 (in the limit) for never occurring together, 0 for independence, and +1 for complete co-occurrence . 2      npmi   (  x  ;  y  )    =    pmi   (  x  ;  y  )     -   log   [   p   (  x  ,  y  )    ]           npmi  x  y      pmi  x  y         p   x  y         \operatorname{npmi}(x;y)=\frac{\operatorname{pmi}(x;y)}{-\log\left[p(x,y)%
 \right]}     Chain-rule for pmi  Like MI, 3 PMI follows the chain rule , that is,       pmi   (  x  ;   y  z   )    =    pmi   (  x  ;  y  )    +   pmi   (  x  ;  z  |  y  )          pmi  x    y  z       pmi  x  y    pmi  x  z  y      \operatorname{pmi}(x;yz)=\operatorname{pmi}(x;y)+\operatorname{pmi}(x;z|y)     This is easily proven by:       pmi   (  x  ;  y  )    +   pmi   (  x  ;  z  |  y  )         pmi  x  y    pmi  x  z  y     \displaystyle\operatorname{pmi}(x;y)+\operatorname{pmi}(x;z|y)     References     External links   Demo at Rensselaer MSR Server (PMI values normalized to be between 0 and 1)   "  Category:Information theory  Category:Statistical dependence  Category:Entropy and information     ↩  ↩  ↩     