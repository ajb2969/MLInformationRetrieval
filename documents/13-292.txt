   Extremum estimator      Extremum estimator   In statistics and econometrics , extremum estimators is a wide class of estimators for parametric models that are calculated through maximization (or minimization) of a certain objective function , which depends on the data. The general theory of extremum estimators was developed by .  Definition  An estimator    Œ∏  ^     normal-^  Œ∏    \scriptstyle\hat{\theta}   is called an extremum estimator , if there is an objective function      Q  ^   n     subscript   normal-^  Q   n    \scriptstyle\hat{Q}_{n}   such that        Œ∏  ^   =       arg   max    Œ∏  ‚àà  Œò       Q  ^   n    (  Œ∏  )     ,       normal-^  Œ∏        Œ∏  normal-Œò     arg  max     subscript   normal-^  Q   n   Œ∏     \hat{\theta}=\underset{\theta\in\Theta}{\operatorname{arg\;max}}\ \widehat{Q}_%
 {n}(\theta),   where Œò is the possible range of parameter values. Sometimes a slightly weaker definition is given:          Q  ^   n    (   Œ∏  ^   )    ‚â•       max   Œ∏  ‚àà  Œò       Q  ^   n     (  Œ∏  )    -    o  p    (  1  )      ,         subscript   normal-^  Q   n    normal-^  Œ∏          subscript     Œ∏  normal-Œò     subscript   normal-^  Q   n    Œ∏      subscript  o  p   1      \widehat{Q}_{n}(\hat{\theta})\geq\max_{\theta\in\Theta}\,\widehat{Q}_{n}(%
 \theta)-o_{p}(1),   where o p (1) is the variable converging in probability to zero . With this modification    Œ∏  ^     normal-^  Œ∏    \scriptstyle\hat{\theta}   doesn‚Äôt have to be the exact maximizer of the objective function, just be sufficiently close to it.  The theory of extremum estimators does not specify what the objective function should be. There are various types of objective functions suitable for different models, and this framework allows us to analyse the theoretical properties of such estimators from a unified perspective. The theory only specifies the properties that the objective function has to possess, and when one selects a particular objective function, he or she only has to verify that those properties are satisfied.  Consistency  (Figure)  When the set Œò is not compact ( in this example), then even if the objective function is uniquely maximized at Œ∏ 0 , this maximum may be not well-separated, in which case the estimator    Œ∏  ^     normal-^  Œ∏    \scriptscriptstyle\hat{\theta}   will fail to be consistent.   If the set Œò is compact and there is a limiting function  Q 0 ( Œ∏ ) such that:      Q  ^   n    (  Œ∏  )        subscript   normal-^  Q   n   Œ∏    \scriptstyle\hat{Q}_{n}(\theta)   converges to Q 0 ( Œ∏ ) in probability uniformly over Œò, and the function Q 0 ( Œ∏ ) is continuous and has a unique maximum at Œ∏ = Œ∏ 0 . If these conditions are satisfied then    Œ∏  ^     normal-^  Œ∏    \scriptstyle\hat{\theta}   is consistent for Œ∏ 0 . 1  The uniform convergence in probability of      Q  ^   n    (  Œ∏  )        subscript   normal-^  Q   n   Œ∏    \scriptstyle\hat{Q}_{n}(\theta)   means that        sup   Œ∏  ‚àà  Œò     |      Q  ^   n    (  Œ∏  )    -    Q  0    (  Œ∏  )     |     ‚Üí  ùëù   0.      p  normal-‚Üí     subscript  supremum    Œ∏  normal-Œò           subscript   normal-^  Q   n   Œ∏      subscript  Q  0   Œ∏      0.    \sup_{\theta\in\Theta}\big|\hat{Q}_{n}(\theta)-Q_{0}(\theta)\big|\ %
 \xrightarrow{p}\ 0.     The requirement for Œò to be compact can be replaced with a weaker assumption that the maximum of Q 0 was well-separated, that is there should not exist any points Œ∏ that are distant from Œ∏ 0 but such that Q 0 ( Œ∏ ) were close to Q 0 ( Œ∏ 0 ). Formally, it means that for any sequence { Œ∏ i } such that , it should be true that .  Asymptotic normality  Examples  See also  Notes  References      "  Category:Estimation theory     Newey & McFadden (1994), Theorem 2.1 ‚Ü©     