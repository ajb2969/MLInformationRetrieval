   Tsallis entropy      Tsallis entropy   In physics, the Tsallis entropy is a generalization of the standard Boltzmann–Gibbs entropy . It was introduced in 1988 by Constantino Tsallis 1 as a basis for generalizing the standard statistical mechanics. In the scientific literature, the physical relevance of the Tsallis entropy was occasionally debated. However, from the years 2000 on, an increasingly wide spectrum of natural, artificial and social complex systems have been identified which confirm the predictions and consequences that are derived from this nonadditive entropy, such as nonextensive statistical mechanics, 2 which generalizes the Boltzmann–Gibbs theory.  Among the various experimental verifications and applications presently available in the literature, the following ones deserve a special mention:   The distribution characterizing the motion of cold atoms in dissipative optical lattices, predicted in 2003 3 and observed in 2006. 4  The fluctuations of the magnetic field in the solar wind enabled the calculation of the q-triplet (or Tsallis triplet). 5  The velocity distributions in driven dissipative dusty plasma. 6  Spin glass relaxation. 7  Trapped ion interacting with a classical buffer gas . 8  High energy collisional experiments at LHC/CERN (CMS, ATLAS and ALICE detectors) 9 10 and RHIC/Brookhaven (STAR and PHENIX detectors). 11   Among the various available theoretical results which clarify the physical conditions under which Tsallis entropy and associated statistics apply, the following ones can be selected:   Anomalous diffusion . 12 13  Uniqueness theorem . 14  Sensitivity to initial conditions and entropy production at the edge of chaos. 15 16  Probability sets which make the nonadditive Tsallis entropy to be extensive in the thermodynamical sense. 17  Strongly quantum entangled systems and thermodynamics. 18  Thermostatistics of overdamped motion of interacting particles. 19 20  Nonlinear generalizations of the Schroedinger, Klein-Gordon and Dirac equations . 21   For further details a bibliography is available at http://tsallis.cat.cbpf.br/biblio.htm  Given a discrete set of probabilities    {   p  i   }      subscript  p  i     \{p_{i}\}   with the condition      ∑  i    p  i    =  1        subscript   i    subscript  p  i    1    \sum_{i}p_{i}=1   , and   q   q   q   any real number, the Tsallis entropy is defined as         S  q    (   p  i   )    =    k   q  -  1     (   1  -    ∑  i    p  i  q     )     ,         subscript  S  q    subscript  p  i        k    q  1      1    subscript   i    superscript   subscript  p  i   q        S_{q}({p_{i}})={k\over q-1}\left(1-\sum_{i}p_{i}^{q}\right),     where   q   q   q   is a real parameter sometimes called entropic-index . In the limit as    q  →  1     normal-→  q  1    q\to 1   , the usual Boltzmann–Gibbs entropy is recovered, namely        S   B  G    =    S  1    (  p  )    =   -   k    ∑  i     p  i    ln   p  i         .         subscript  S    B  G       subscript  S  1   p            k    subscript   i      subscript  p  i      subscript  p  i           S_{BG}=S_{1}(p)=-k\sum_{i}p_{i}\ln p_{i}.     For continuous probability distributions, we define the entropy as         S  q    [  p  ]    =    1   q  -  1     (   1  -   ∫      (   p   (  x  )    )   q    d  x     )     ,         subscript  S  q    delimited-[]  p        1    q  1      1       superscript    p  x   q   d  x        S_{q}[p]={1\over q-1}\left(1-\int(p(x))^{q}\,dx\right),     where    p   (  x  )       p  x    p(x)   is a probability density function .  The Tsallis Entropy has been used along with the Principle of maximum entropy to derive the Tsallis distribution .  Various relationships  The discrete Tsallis entropy satisfies       S  q   =   -    lim   x  →  1      D  q     ∑  i    p  i  x            subscript  S  q       subscript    normal-→  x  1       subscript  D  q     subscript   i    superscript   subscript  p  i   x         S_{q}=-\lim_{x\rightarrow 1}D_{q}\sum_{i}p_{i}^{x}     where D q is the q-derivative with respect to x . This may be compared to the standard entropy formula:      S  =   -    lim   x  →  1      d   d  x      ∑  i    p  i  x           S      subscript    normal-→  x  1        d    d  x      subscript   i    superscript   subscript  p  i   x         S=-\lim_{x\rightarrow 1}\frac{d}{dx}\sum_{i}p_{i}^{x}     Non-additivity  Given two independent systems A and B , for which the joint probability density satisfies        p   (  A  ,  B  )    =   p   (  A  )   p   (  B  )     ,        p   A  B      p  A  p  B     p(A,B)=p(A)p(B),\,     the Tsallis entropy of this system satisfies         S  q    (  A  ,  B  )    =     S  q    (  A  )    +    S  q    (  B  )    +    (   1  -  q   )    S  q    (  A  )    S  q    (  B  )      .         subscript  S  q    A  B         subscript  S  q   A      subscript  S  q   B       1  q    subscript  S  q   A   subscript  S  q   B      S_{q}(A,B)=S_{q}(A)+S_{q}(B)+(1-q)S_{q}(A)S_{q}(B).\,     From this result, it is evident that the parameter    |   1  -  q   |        1  q     |1-q|   is a measure of the departure from additivity. In the limit when q = 1,        S   (  A  ,  B  )    =    S   (  A  )    +   S   (  B  )      ,        S   A  B        S  A     S  B      S(A,B)=S(A)+S(B),\,     which is what is expected for an additive system. This property is sometimes referred to as "pseudo-additivity".  Exponential families  Many common distributions like the normal distribution belongs to the statistical exponential families. Tsallis entropy for an exponential family can be written 22 as        H  q  T    (    p  F    (  x  ;  θ  )    )    =    1   1  -  q     (     (   e    F   (   q  θ   )    -   q  F   (  θ  )      )    E  p    [   e    (   q  -  1   )   k   (  x  )     ]    -  1   )           subscript   superscript  H  T   q      subscript  p  F    x  θ         1    1  q         superscript  e      F    q  θ      q  F  θ      subscript  E  p    delimited-[]   superscript  e      q  1   k  x      1      H^{T}_{q}(p_{F}(x;\theta))=\frac{1}{1-q}\left((e^{F(q\theta)-qF(\theta)})E_{p}%
 [e^{(q-1)k(x)}]-1\right)   where F is log-normalizer and k the term indicating the carrier measure. For multivariate normal, term k is zero, and therefore the Tsallis entropy is in closed-form.  Generalised entropies  A number of interesting physical systems 23 abide to entropic functionals that are more general than the standard Tsallis entropy. Therefore, several physically meaningful generalisations have been introduced. The two most general of those are notably: Superstatistics, introduced by C. Beck and E.G.D. Cohen in 2003 24 and Spectral Statistics, introduced by G.A. Tsekouras and Constantino Tsallis in 2005. 25 Both these entropic forms have Tsallis and Boltzmann–Gibbs statistics as special cases; Spectral Statistics has been proven to at least contain Superstatistics and it has been conjectured to also cover some additional cases.  See also   Rényi entropy  Tsallis distribution   References    External links   Tsallis Statistics, Statistical Mechanics for Non-extensive Systems and Long-Range Interactions   "  Category:Probability theory  Category:Entropy and information  Category:Thermodynamic entropy  Category:Information theory     ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩     