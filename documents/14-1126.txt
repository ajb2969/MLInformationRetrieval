   Step detection      Step detection   (Figure)  Examples of signals that may contain steps corrupted by noise. (a) DNA copy-number ratios from microarray data, (b) cosmic ray intensity from a neutron monitor , (c) rotation speed against time of R. Sphaeroides  flagellar motor , and (d) red pixel intensity from a single scan line of a digital image.   In statistics and signal processing , step detection (also known as step smoothing , step filtering , shift detection , jump detection or edge detection ) is the process of finding abrupt changes (steps, jumps, shifts) in the mean level of a time series or signal. It is usually considered as a special case of the statistical method known as change detection or change point detection. Often, the step is small and the time series is corrupted by some kind of noise , and this makes the problem challenging because the step may be hidden by the noise. Therefore, statistical and/or signal processing algorithms are often required.  The step detection problem occurs in multiple scientific and engineering contexts, for example in statistical process control 1 (the control chart being the most directly related method), in exploration geophysics (where the problem is to segment a well-log recording into stratigraphic zones 2 ), in genetics (the problem of separating microarray data into similar copy-number regimes 3 ), and in biophysics (detecting state transitions in a molecular machine as recorded in time-position traces 4 ). For 2D signals, the related problem of edge detection has been studied intensively for image processing . 5  Algorithms  When the step detection must be performed as and when the data arrives, then online algorithms are usually used, 6 and it becomes a special case of sequential analysis . Such algorithms include the classical CUSUM method applied to changes in mean. 7  By contrast, offline algorithms are applied to the data potentially long after it has been received. Most offline algorithms for step detection in digital data can be categorised as top-down , bottom-up , sliding window , or global methods.  Top-down  These algorithms start with the assumption that there are no steps and introduce possible candidate steps one at a time, testing each candidate to find the one that minimizes some criteria (such as the least-squares fit of the estimated, underlying piecewise constant signal). An example is the stepwise jump placement algorithm, first studied in geophysical problems, 8 that has found recent uses in modern biophysics. 9  Bottom-up  Bottom-up algorithms take the "opposite" approach to top-down methods, first assuming that there is a step in between every sample in the digital signal, and then successively merging steps based on some criteria tested for every candidate merge.  Sliding window  By considering a small "window" of the signal, these algorithms look for evidence of a step occurring within the window. The window "slides" across the time series, one time step at a time. The evidence for a step is tested by statistical procedures, for example, by use of the two-sample Student's t-test . Alternatively, a nonlinear filter such as the median filter is applied to the signal. Filters such as these attempt to remove the noise whilst preserving the abrupt steps.  Global  Global algorithms consider the entire signal in one go, and attempt to find the steps in the signal by some kind of optimization procedure. Algorithms include wavelet methods, 10 and total variation denoising which uses methods from convex optimization . Where the steps can be modelled as a Markov chain , then Hidden Markov Models are also often used (a popular approach in the biophysics community 11 ). When there are only a few unique values of the mean, then k-means clustering can also be used.  Linear versus nonlinear signal processing methods for step detection  Because steps and (independent) noise have theoretically infinite bandwidth and so overlap in the Fourier basis , signal processing approaches to step detection generally do not use classical smoothing techniques such as the low pass filter . Instead, most algorithms are explicitly nonlinear or time-varying. 12  Step detection and piecewise constant signals  Because the aim of step detection is to find a series of instantaneous jumps in the mean of a signal, the wanted, underlying, mean signal is piecewise constant . For this reason, step detection can be profitably viewed as the problem of recovering a piecewise constant signal corrupted by noise. There are two complementary models for piecewise constant signals: as 0-degree splines with a few knots, or as level sets with a few unique levels. Many algorithms for step detection are therefore best understood as either 0-degree spline fitting, or level set recovery, methods.  Step detection as level set recovery  When there are only a few unique values of the mean, clustering techniques such as k-means clustering or mean-shift are appropriate. These techniques are best understood as methods for finding a level set description of the underlying piecewise constant signal.  Step detection as 0-degree spline fitting  Many algorithms explicitly fit 0-degree splines to the noisy signal in order to detect steps (including stepwise jump placement methods 13 14 ), but there are other popular algorithms that can also be seen to be spline fitting methods after some transformation, for example total variation denoising . 15  Generalized step detection by piecewise constant denoising  All the algorithms mentioned above have certain advantages and disadvantages in particular circumstances, yet, a surprisingly large number of these step detection algorithms are special cases of a more general algorithm. 16 This algorithm involves the minimization of a global functional: 17  Here, x i for i = 1, ...., N is the discrete-time input signal of length N , and m i is the signal output from the algorithm. The goal is to minimize H [ m ] with respect to the output signal m . The form of the function   Λ   normal-Λ   \scriptstyle\Lambda   determines the particular algorithm. For example, choosing:      Λ  =   1  2   |   x  i   -   m  j    |  2   I   (  i  -  j  =  0  )   +  γ  |   m  i   -   m  j   |  I   (  i  -  j  =  1  )      fragments  Λ     1  2   normal-|   subscript  x  i     subscript  m  j    superscript  normal-|  2   I   fragments  normal-(  i   j   0  normal-)    γ  normal-|   subscript  m  i     subscript  m  j   normal-|  I   fragments  normal-(  i   j   1  normal-)     \Lambda=\frac{1}{2}\left|x_{i}-m_{j}\right|^{2}I(i-j=0)+\gamma\left|m_{i}-m_{j%
 }\right|I(i-j=1)     where I ( S ) = 0 if the condition S is false, and one otherwise, obtains the total variation denoising algorithm with regularization parameter   γ   γ   \gamma   . Similarly:      Λ  =   min   {    1  2     |    m  i   -   m  j    |   2    ,  W  }        normal-Λ        1  2    superscript       subscript  m  i    subscript  m  j     2    W     \Lambda=\min\left\{\frac{1}{2}\left|m_{i}-m_{j}\right|^{2},W\right\}     leads to the mean shift algorithm, when using an adaptive step size Euler integrator initialized with the input signal x . 18 Here W > 0 is a parameter that determines the support of the mean shift kernel. Another example is:      Λ  =    1  -   exp   (   -    β    |    m  i   -   m  j    |   2    /  2    )     β   ⋅  I   (  |  i  -  j  |  ≤  W  )      fragments  Λ       1          β   superscript       subscript  m  i    subscript  m  j     2    2      β   normal-⋅  I   fragments  normal-(  normal-|  i   j  normal-|   W  normal-)     \Lambda=\frac{1-\exp(-\beta|m_{i}-m_{j}|^{2}/2)}{\beta}\cdot I(|i-j|\leq W)     leading to the bilateral filter , where    β  >  0      β  0    \scriptstyle\beta>0   is the tonal kernel parameter, and W is the spatial kernel support. Yet another special case is:      Λ  =   1  2   |   x  i   -   m  j    |  2   I   (  i  -  j  =  0  )   +  γ  |   m  i   -   m  j    |  0   I   (  i  -  j  =  1  )      fragments  Λ     1  2   normal-|   subscript  x  i     subscript  m  j    superscript  normal-|  2   I   fragments  normal-(  i   j   0  normal-)    γ  normal-|   subscript  m  i     subscript  m  j    superscript  normal-|  0   I   fragments  normal-(  i   j   1  normal-)     \Lambda=\frac{1}{2}\left|x_{i}-m_{j}\right|^{2}I(i-j=0)+\gamma\left|m_{i}-m_{j%
 }\right|^{0}I(i-j=1)     specifying a group of algorithms that attempt to greedily fit 0-degree splines to the signal. 19 20 Here,     |  x  |   0     superscript    x   0    \scriptstyle\left|x\right|^{0}   is defined as zero if x = 0, and one otherwise.  Many of the functionals in equation () defined by the particular choice of   Λ   normal-Λ   \scriptstyle\Lambda   are convex : they can be minimized using methods from convex optimization . Still others are non-convex but a range of algorithms for minimizing these functionals have been devised. 21  Step detection using the Potts model  A classical variational method for step detection is the Potts model. It is given by the non-convex optimization problem       u  *   =     arg    min   u  ∈    \R   N     γ      ∥   ∇  u   ∥   0    +    ∥   u  -  x   ∥   p  p         superscript  u            subscript     u   superscript  \R  N     γ     subscript   norm   normal-∇  u    0     superscript   subscript   norm    u  x    p   p      u^{*}=\arg\min_{u\in\R^{N}}\gamma\|\nabla u\|_{0}+\|u-x\|_{p}^{p}     The term      ∥   ∇  u   ∥   0   =   #   {  i  :    u  i   ≠   u   i  +  1     }         subscript   norm   normal-∇  u    0     normal-#   conditional-set  i     subscript  u  i    subscript  u    i  1         \|\nabla u\|_{0}=\#\{i:u_{i}\neq u_{i+1}\}   penalizes the number of jumps and the term      ∥   u  -  x   ∥   p  p   =    ∑   i  =  1   N     |    u  i   -   x  i    |   p         superscript   subscript   norm    u  x    p   p     superscript   subscript     i  1    N    superscript       subscript  u  i    subscript  x  i     p      \|u-x\|_{p}^{p}=\sum_{i=1}^{N}|u_{i}-x_{i}|^{p}   measures fidelity to data x . The parameter γ > 0 controls the tradeoff between regularity and data fidelity. Since the minimizer    u  *     superscript  u     u^{*}   is piecewise constant the steps are given by the non-zero locations of the gradient    ∇   u  *      normal-∇   superscript  u      \nabla u^{*}   . For    p  =  2      p  2    p=2   and    p  =  1      p  1    p=1   there are fast algorithms which give an exact solution of the Potts problem in    O   (   N  2   )       O   superscript  N  2     O(N^{2})   . 22 23 24  References  External links   PWCTools: Flexible Matlab software for step detection by piecewise constant denoising   "  Category:Change detection  Category:Nonlinear filters  Category:Signal processing  Category:Feature detection (computer vision)  Category:Time series analysis     ↩   ↩  ↩  ↩  ↩  Rodionov, S.N., 2005a: A brief overview of the regime shift detection methods. link to PDF In: Large-Scale Disturbances (Regime Shifts) and Recovery in Aquatic Ecosystems: Challenges for Management Toward Sustainability, V. Velikova and N. Chipev (Eds.), UNESCO-ROSTE/BAS Workshop on Regime Shifts, 14–16 June 2005, Varna, Bulgaria, 17-24. ↩  ↩  ↩  ↩  ↩  ↩    ↩   ↩      Mumford, D., & Shah, J. (1989). Optimal approximations by piecewise smooth functions and associated variational problems. Communications on pure and applied mathematics, 42(5), 577-685. ↩  Winkler, G., & Liebscher, V. (2002). Smoothers for discontinuous signals. Journal of Nonparametric Statistics, 14(1-2), 203-222. ↩  Friedrich et al. "Complexity penalized M-estimation: fast computation." Journal of Computational and Graphical Statistics 17.1 (2008): 201-224. ↩     