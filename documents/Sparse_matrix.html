<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="179">Sparse matrix</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Sparse matrix</h1>
<hr/>
<p>{| class="wikitable" align=right width=240px style="margin: 3px 0 5px 14px;" |</p>
<center>
<p><strong>Example of sparse matrix</strong></p>
</center>
<center>
<p><span class="LaTeX">$\left(\begin{smallmatrix}
11 & 22 & 0 & 0 & 0 & 0 & 0 \\
0 & 33 & 44 & 0 & 0 & 0 & 0 \\
0 & 0 & 55 & 66 & 77 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 88 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 99 \\
\end{smallmatrix}\right)$</span></p>
</center>
<p>|- |</p>
<center>
</center>
<p>|} </p>
<p>In <a href="numerical_analysis" title="wikilink">numerical analysis</a>, a <strong>sparse matrix</strong> is a <a href="matrix_(mathematics)" title="wikilink">matrix</a> in which most of the elements are zero. By contrast, if most of the elements are nonzero, then the matrix is considered <strong>dense</strong>. The fraction of non-zero elements over the total number of elements (i.e., that can fit into the matrix, say a matrix of dimension of m x n can accommodate m x n total number of elements) in a matrix is called the <strong>sparsity</strong> (<strong>density</strong>).</p>
<p>Conceptually, sparsity corresponds to systems which are loosely coupled. Consider a line of balls connected by springs from one to the next: this is a sparse system as only adjacent balls are coupled. By contrast, if the same line of balls had springs connecting each ball to all other balls, the system would correspond to a dense matrix. The concept of sparsity is useful in <a class="uri" href="combinatorics" title="wikilink">combinatorics</a> and application areas such as <a href="network_theory" title="wikilink">network theory</a>, which have a low density of significant data or connections.</p>
<p>Large sparse matrices often appear in <a class="uri" href="scientific" title="wikilink">scientific</a> or <a class="uri" href="engineering" title="wikilink">engineering</a> applications when solving <a href="partial_differential_equation" title="wikilink">partial differential equations</a>.</p>
<p>When storing and manipulating sparse matrices on a <a class="uri" href="computer" title="wikilink">computer</a>, it is beneficial and often necessary to use specialized <a href="algorithm" title="wikilink">algorithms</a> and <a href="data_structure" title="wikilink">data structures</a> that take advantage of the sparse structure of the matrix. Operations using standard dense-matrix structures and algorithms are slow and inefficient when applied to large sparse matrices as processing and <a href="Computer_memory" title="wikilink">memory</a> are wasted on the zeroes. Sparse data is by nature more easily <a href="data_compression" title="wikilink">compressed</a> and thus require significantly less <a href="computer_data_storage" title="wikilink">storage</a>. Some very large sparse matrices are infeasible to manipulate using standard dense-matrix algorithms.</p>
<h2 id="storing-a-sparse-matrix"> Storing a sparse matrix</h2>
<p>A matrix is typically stored as a two-dimensional array. Each entry in the array represents an element <mtpl></mtpl> of the matrix and is accessed by the two <a href="Array_data_structure" title="wikilink">indices</a> <span class="LaTeX">$i$</span> and <span class="LaTeX">$j$</span>. Conventionally, <span class="LaTeX">$i$</span> is the row index, numbered from top to bottom, and <span class="LaTeX">$j$</span> is the column index, numbered from left to right. For an <span class="LaTeX">$m × n$</span> matrix, the amount of memory required to store the matrix in this format is proportional to <span class="LaTeX">$m × n$</span> (disregarding the fact that the dimensions of the matrix also need to be stored).</p>
<p>In the case of a sparse matrix, substantial memory requirement reductions can be realized by storing only the non-zero entries. Depending on the number and distribution of the non-zero entries, different data structures can be used and yield huge savings in memory when compared to the basic approach. The caveat is that accessing the individual elements becomes more complex and additional structures are needed to be able to recover the original matrix unambiguously.</p>
<p>Formats can be divided into two groups:</p>
<ul>
<li>Those that support efficient modification, such as DOK (Dictionary of keys), LIL (List of lists), or COO (Coordinate list). These are typically used to construct the matrices.</li>
</ul>
<ul>
<li>Those that support efficient access and matrix operations, such as CSR (Compressed Sparse Row) or CSC (Compressed Sparse Column).</li>
</ul>
<h3 id="dictionary-of-keys-dok">Dictionary of keys (DOK)</h3>
<p>DOK consists of a <a href="associative_array" title="wikilink">dictionary</a> that maps <span class="LaTeX">$(row, column)$</span>-<a href="ordered_pair" title="wikilink">pairs</a> to the value of the elements. Elements that are missing from the dictionary are taken to be zero. The format is good for incrementally constructing a sparse matrix in random order, but poor for iterating over non-zero values in lexicographical order. One typically constructs a matrix in this format and then converts to another more efficient format for processing.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h3 id="list-of-lists-lil">List of lists (LIL)</h3>
<p>LIL stores one list per row, with each entry containing the column index and the value. Typically, these entries are kept sorted by column index for faster lookup. This is another format good for incremental matrix construction.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h3 id="coordinate-list-coo">Coordinate list (COO)</h3>
<p>COO stores a list of <span class="LaTeX">$(row, column, value)$</span> tuples. Ideally, the entries are sorted (by row index, then column index) to improve random access times. This is another format which is good for incremental matrix construction.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h3 id="yale">Yale</h3>
<p>The Yale sparse matrix format stores an initial sparse <span class="LaTeX">$m × n$</span> matrix, <span class="LaTeX">$\mathbf{ M } $</span>, in row form using three (one-dimensional) arrays <span class="LaTeX">$(A, IA, JA)$</span>. Let <span class="LaTeX">$NNZ$</span> denote the number of nonzero entries in <strong>M</strong>. (Note that <a href="Zero-based_numbering" title="wikilink">zero-based indices</a> shall be used here.)</p>
<ul>
<li>The array <span class="LaTeX">$A$</span> is of length <span class="LaTeX">$NNZ$</span> and holds all the nonzero entries of <strong>M</strong> in left-to-right top-to-bottom ("row-major") order.</li>
</ul>
<ul>
<li>The array <span class="LaTeX">$IA$</span> is of length <span class="LaTeX">$m + 1$</span> and contains the index in <span class="LaTeX">$A$</span> of the first element in each row, followed by the total number of nonzero elements <span class="LaTeX">$NNZ$</span>. <span class="LaTeX">$IA i i$</span> contains the index in <span class="LaTeX">$A$</span> of the first nonzero element of row <span class="LaTeX">$i$</span>. Row <span class="LaTeX">$i$</span> of the original matrix extends from <span class="LaTeX">$A IA A i i$</span> to <span class="LaTeX">$A IA A i i + 1 − 1 1$</span>, i.e. from the start of one row to the last index before the start of the next. The last entry, <span class="LaTeX">$IA m m$</span>, must be the number of elements in <span class="LaTeX">$A$</span>.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></li>
</ul>
<ul>
<li>The third array, <span class="LaTeX">$JA$</span>, contains the column index in <span class="LaTeX">$\mathbf{ M } $</span> of each element of <span class="LaTeX">$A$</span> and hence is of length <span class="LaTeX">$NNZ$</span> as well.</li>
</ul>
<p>For example, the matrix</p>
<dl>
<dd><dl>
<dd><math>\begin{pmatrix}
</math></dd>
</dl>
</dd>
</dl>
<p>0 & 0 & 0 & 0 \\ 5 & 8 & 0 & 0 \\ 0 & 0 & 3 & 0 \\ 0 & 6 & 0 & 0 \\ \end{pmatrix}</p>
<p>is a <span class="LaTeX">$4 × 4$</span> matrix with 4 nonzero elements, hence</p>
<p><code>   A  = [ 5 8 3 6 ]</code><br/>
<code>   IA = [ 0 0 2 3 4 ]</code><br/>
<code>   JA = [ 0 1 2 1 ]</code></p>
<p>So, in array <span class="LaTeX">$JA$</span>, the element "<span class="LaTeX">$5$</span>" from <span class="LaTeX">$A$</span> has column index <span class="LaTeX">$0$</span>, "<span class="LaTeX">$8$</span>" and "<span class="LaTeX">$6$</span>" have index <span class="LaTeX">$1$</span>, and element "<span class="LaTeX">$3$</span>" has index <span class="LaTeX">$2$</span>.</p>
<p>In this case the Yale representation contains 13 entries, compared to 16 in the original matrix. The Yale format saves on memory only when <span class="LaTeX">$NNZ . Another example, the matrix</span></p>
<p><span class="LaTeX">$$\begin{pmatrix}
10 & 20 &  0 &  0 &  0 &  0 \\
 0 & 30 &  0 & 40 &  0 &  0 \\
 0 &  0 & 50 & 60 & 70 &  0 \\
 0 &  0 &  0 &  0 &  0 & 80 \\
\end{pmatrix}$$</span></p>
<p>is a <span class="LaTeX">$4 × 6$</span> matrix (24 entries) with 8 nonzero elements, so</p>
<p><code>   A  = [ 10 20 30 40 50 60 70 80 ]</code><br/>
<code>   IA = [ 0 2 4 7 8 ]</code><br/>
<code>   JA = [ 0 1 1 3 2 3 4 5 ]</code></p>
<p>The whole is stored as 21 entries.</p>
<ul>
<li><span class="LaTeX">$IA$</span> splits the array <span class="LaTeX">$A$</span> into rows: <code>(10, 20) (30, 40) (50, 60, 70) (80)</code>;</li>
</ul>
<ul>
<li><span class="LaTeX">$JA$</span> aligns values in columns: <code>(10, 20, ...) (0, 30, 0, 40, ...)(0, 0, 50, 60, 70, 0) (0, 0, 0, 0, 0, 80)</code>.</li>
</ul>
<p>Note that in this format, the first value of <span class="LaTeX">$IA$</span> is always zero and the last is always <span class="LaTeX">$NNZ$</span>, so they are in some sense redundant. However, they can make accessing and traversing the array easier for the programmer.</p>
<h3 id="compressed-row-storage-crs-or-csr">Compressed row Storage (CRS or CSR)</h3>
<p><a href="http://netlib.org/linalg/html_templates/node91.html#SECTION00931100000000000000">CSR</a> is effectively identical to the Yale Sparse Matrix format, except that the column array is normally stored ahead of the row index array. I.e. CSR is <span class="LaTeX">$(val, col_ind, row_ptr)$</span>, where <span class="LaTeX">$val$</span> is an array of the (left-to-right, then top-to-bottom) non-zero values of the matrix; <span class="LaTeX">$col_ind$</span> is the column indices corresponding to the values; and, <span class="LaTeX">$row_ptr$</span> is the list of value indexes where each row starts. The name is based on the fact that row index information is compressed relative to the COO format. One typically uses another format (LIL, DOK, COO) for construction. This format is efficient for arithmetic operations, row slicing, and matrix-vector products. See <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html">scipy.sparse.csr_matrix</a>.</p>
<h3 id="compressed-sparse-column-csc-or-ccs">Compressed sparse column (CSC or CCS)</h3>
<p><a href="http://netlib.org/linalg/html_templates/node92.html#SECTION00931200000000000000">CSC</a> is similar to CSR except that values are read first by column, a row index is stored for each value, and column pointers are stored. I.e. CSC is <span class="LaTeX">$(val, row_ind, col_ptr)$</span>, where <span class="LaTeX">$val$</span> is an array of the (top-to-bottom, then left-to-right) non-zero values of the matrix; <span class="LaTeX">$row_ind$</span> is the row indices corresponding to the values; and, <span class="LaTeX">$col_ptr$</span> is the list of <span class="LaTeX">$val$</span> indexes where each column starts. The name is based on the fact that column index information is compressed relative to the COO format. One typically uses another format (LIL, DOK, COO) for construction. This format is efficient for arithmetic operations, column slicing, and matrix-vector products. See <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html">scipy.sparse.csc_matrix</a>. This is the traditional format for specifying a sparse matrix in MATLAB (via the <code>sparse</code> function).</p>
<h2 id="special-structure">Special structure</h2>
<h3 id="banded">Banded</h3>
<p>An important special type of sparse matrices is <a href="band_matrix" title="wikilink">band matrix</a>, defined as follows. The <em>lower bandwidth</em> of a matrix <span class="LaTeX">$\mathbf{ A } $</span> is the smallest number <span class="LaTeX">$p$</span> such that the entry <mtpl></mtpl> vanishes whenever <span class="LaTeX">$i > j + p$</span>. Similarly, the <em>upper bandwidth</em> is the smallest number <span class="LaTeX">$p$</span> such that <mtpl> 0}}</mtpl> whenever <span class="LaTeX">$i  . For example, a <a href="tridiagonal_matrix" title="wikilink">tridiagonal matrix</a> has lower bandwidth <span class="LaTeX">$1$</span> and upper bandwidth <span class="LaTeX">$1$</span>. As another example, the following sparse matrix has lower and upper bandwidth both equal to 3. Notice that zeros are represented with dots for clarity.</span></p>
<dl>
<dd><dl>
<dd><math>
</math></dd>
</dl>
</dd>
</dl>
<p>\left( \begin{smallmatrix}</p>
<p><code> X   &   X   &   X   & \cdot & \cdot & \cdot & \cdot & \\</code><br/>
<code> X   &   X   & \cdot &   X   &   X   & \cdot & \cdot & \\</code><br/>
<code> X   & \cdot &   X   & \cdot &   X   & \cdot & \cdot & \\</code></p>
<p>\cdot & X & \cdot & X & \cdot & X & \cdot & \\ \cdot & X & X & \cdot & X & X & X & \\ \cdot & \cdot & \cdot & X & X & X & \cdot & \\ \cdot & \cdot & \cdot & \cdot & X & \cdot & X & \\ \end{smallmatrix} \right) </p>
<p>Matrices with reasonably small upper and lower bandwidth are known as band matrices and often lend themselves to simpler algorithms than general sparse matrices; or one can sometimes apply dense matrix algorithms and gain efficiency simply by looping over a reduced number of indices.</p>
<p>By rearranging the rows and columns of a matrix <span class="LaTeX">$\mathbf{ A } $</span> it may be possible to obtain a matrix <span class="LaTeX">$\mathbf{ A } ′$</span> with a lower bandwidth. A number of algorithms are designed for <a href="Graph_bandwidth" title="wikilink">bandwidth minimization</a>.</p>
<h3 id="diagonal">Diagonal</h3>
<p>A very efficient structure for an extreme case of band matrices, the <a href="diagonal_matrix" title="wikilink">diagonal matrix</a>, is to store just the entries in the main diagonal as a one-dimensional array, so a diagonal <span class="LaTeX">$n × n$</span> matrix requires only <span class="LaTeX">$n$</span> entries.</p>
<h3 id="symmetric">Symmetric</h3>
<p>A symmetric sparse matrix arises as the <a href="adjacency_matrix" title="wikilink">adjacency matrix</a> of an <a href="undirected_graph" title="wikilink">undirected graph</a>; it can be stored efficiently as an <a href="adjacency_list" title="wikilink">adjacency list</a>.</p>
<h2 id="reducing-fill-in">Reducing fill-in</h2>
<p>The <strong>fill-in</strong> of a matrix are those entries which change from an initial zero to a non-zero value during the execution of an algorithm. To reduce the memory requirements and the number of arithmetic operations used during an algorithm it is useful to minimize the fill-in by switching rows and columns in the matrix. The <a href="symbolic_Cholesky_decomposition" title="wikilink">symbolic Cholesky decomposition</a> can be used to calculate the worst possible fill-in before doing the actual <a href="Cholesky_decomposition" title="wikilink">Cholesky decomposition</a>.</p>
<p>There are other methods than the <a href="Cholesky_decomposition" title="wikilink">Cholesky decomposition</a> in use. Orthogonalization methods (such as QR factorization) are common, for example, when solving problems by least squares methods. While the theoretical fill-in is still the same, in practical terms the "false non-zeros" can be different for different methods. And symbolic versions of those algorithms can be used in the same manner as the symbolic Cholesky to compute worst case fill-in.</p>
<h2 id="solving-sparse-matrix-equations">Solving sparse matrix equations</h2>
<p>Both <a href="Iterative_method" title="wikilink">iterative</a> and direct methods exist for sparse matrix solving.</p>
<p>Iterative methods, such as <a href="conjugate_gradient" title="wikilink">conjugate gradient</a> method and <a class="uri" href="GMRES" title="wikilink">GMRES</a> utilize fast computations of matrix-vector products <span class="LaTeX">$Ax_i$</span>, where matrix <span class="LaTeX">$A$</span> is sparse. The use of <a href="preconditioner" title="wikilink">preconditioners</a> can significantly accelerate convergence of such iterative methods.</p>
<h2 id="see-also">See also</h2>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><ul>
<li><a href="Matrix_representation" title="wikilink">Matrix representation</a></li>
<li><a href="Pareto_principle" title="wikilink">Pareto principle</a></li>
<li><a href="Ragged_matrix" title="wikilink">Ragged matrix</a></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><a href="Skyline_matrix" title="wikilink">Skyline matrix</a></li>
<li><a href="Sparse_array" title="wikilink">Sparse array</a></li>
<li><a href="Sparse_graph_code" title="wikilink">Sparse graph code</a></li>
</ul></td>
<td style="text-align: left;"><ul>
<li><a href="Sparse_file" title="wikilink">Sparse file</a></li>
<li><a href="Harwell-Boeing_file_format" title="wikilink">Harwell-Boeing file format</a></li>
<li><a href="Matrix_Market_exchange_formats" title="wikilink">Matrix Market exchange formats</a></li>
</ul></td>
</tr>
</tbody>
</table>
<h2 id="references">References</h2>
<ul>
<li></li>
<li></li>
<li>
<p>(This book, by a professor at the State University of New York at Stony Book, was the first book exclusively dedicated to Sparse Matrices. Graduate courses using this as a textbook were offered at that University in the early 1980s).</p></li>
<li></li>
<li></li>
<li>
<p>Also NOAA Technical Memorandum NOS NGS-4, National Geodetic Survey, Rockville, MD.</p></li>
</ul>
<h2 id="further-reading">Further reading</h2>
<ul>
<li></li>
<li></li>
<li><a href="http://www.cise.ufl.edu/research/sparse">Sparse Matrix Algorithms Research</a> at the University of Florida, containing the UF sparse matrix collection.</li>
<li><a href="http://www.small-project.eu">SMALL project</a> A EU-funded project on sparse models, algorithms and dictionary learning for large-scale data.</li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.solvingequations.net">Equations Solver Online</a></li>
<li><a href="http://purl.umn.edu/107467">Oral history interview with Harry M. Markowitz</a>, <a href="Charles_Babbage_Institute" title="wikilink">Charles Babbage Institute</a>, University of Minnesota. <a href="Harry_Markowitz" title="wikilink">Markowitz</a> discusses his development of <a href="portfolio_theory" title="wikilink">portfolio theory</a> (for which he received a Nobel Prize in Economics), <strong>sparse matrix methods</strong>, and his work at the <a href="RAND_Corporation" title="wikilink">RAND Corporation</a> and elsewhere on simulation software development (including computer language <a class="uri" href="SIMSCRIPT" title="wikilink">SIMSCRIPT</a>), modeling, and operations research.</li>
</ul>
<p>"</p>
<p><a href="Category:Sparse_matrices" title="wikilink">Category:Sparse matrices</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">See <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dok_matrix.html">scipy.org</a><a href="#fnref1">↩</a></li>
<li id="fn2">See <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html">scipy.org</a><a href="#fnref2">↩</a></li>
<li id="fn3">See <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html">scipy.org</a><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="http://netlib.org/linalg/html_templates/node91.html">netlib.org</a><a href="#fnref4">↩</a></li>
</ol>
</section>
</body>
</html>
