   Spectral density      Spectral density     The power spectrum of a time series     x   (  t  )       x  t    x(t)   describes how the variance of the data    x   (  t  )       x  t    x(t)   is distributed over the frequency domain , into spectral components which the series    x   (  t  )       x  t    x(t)   may be decomposed. This distribution of the variance may be described either by a measure    μ   μ   \mu   or by a statistical cumulative distribution function      S   (  f  )    =         S  f   absent    S(f)=   the power contributed by frequencies from 0 up to   f   f   f   . Given a band of frequencies    [  a  ,  b  )     a  b    [a,b)   , the amount of variance contributed to    x   (  t  )       x  t    x(t)   by frequencies lying within the interval    [  a  ,  b  )     a  b    [a,b)   is given by     S   (  b  )    -   S   (  a  )          S  b     S  a     S(b)-S(a)   . 1 Then   S   S   S   is called the spectral distribution function of   x   x   x   . Provided   S   S   S   is an absolutely continuous function, 2 then there exists a spectral density function     S  ′     superscript  S  normal-′    S^{\prime}   . In this case, the data or signal is said to possess an absolutely continuous spectrum. The spectral density at a frequency   f   f   f   gives the rate of variance contributed by frequencies in the immediate neighbourhood of   f   f   f   to the variance of   x   x   x   per unit frequency.  The nature of the spectrum of a function   x   x   x   gives useful information about the nature of   x   x   x   , for example, whether it is periodic or not. The study of the power spectrum is a kind of generalisation of Fourier analysis and applies to functions which do not possess Fourier transforms .  An analogous definition applies to a stochastic process     X   (  t  )       X  t    X(t)   . Furthermore, time may be either continuous or discrete.  Intuitively, the spectrum decomposes the content of a signal or of a stochastic process into the different frequencies present in that process, and helps identify periodicities. More specific terms which are used are the power spectrum , spectral density , power spectral density , or energy spectral density .  The variance of   x   x   x   has units which are the square of the units of   x   x   x   . Therefore, these are also the units of   μ   μ   \mu   or   S   S   S   , and so the units of the spectral density are the square of the units of   x   x   x   per unit frequency. In the case of the voltage of an electric signal,    x  2     superscript  x  2    x^{2}   is proportional, except that it has the wrong units, to the power of the signal (implicitly assuming a constant resistance), and so even in statistical applications which use different units, the spectral distribution function and density function are often referred to as the power spectral distribution function and the power spectral density function, although the word power is often omitted for brevity in contexts where no misunderstanding will arise. The use of the power spectrum is most important in statistical signal processing and in the branch of statistics consisting of the analysis of time series. It is, however, useful in many other branches of physics and engineering , and may involve other units. Usually the data is a function of time but they may be a function of spatial variables instead.  Explanation  Any signal that can be represented as an amplitude that varies with time has a corresponding frequency spectrum. This includes familiar concepts such as visible light ( color ), musical notes, radio/TV channels , and even the regular rotation of the earth. When these physical phenomena are represented in the form of a frequency spectrum, certain physical descriptions of their internal processes become much simpler. Often, the frequency spectrum clearly shows harmonics , visible as distinct spikes or lines at particular frequencies, that provide insight into the mechanisms that generate the entire signal.  In physics , the signal is usually a wave, such as an electromagnetic wave , random vibration , or an acoustic wave . The power spectral density (PSD) of the signal, when multiplied by the appropriate factor, describes the power contributed to the wave, by a frequency, per unit frequency. Power spectral density is commonly expressed in watts per hertz (W/Hz). 3  For voltage signals, it is customary to use units of V 2 Hz −1 for the PSD and V 2 s Hz −1 for the ESD ( energy spectral density ). 4 Often it is convenient to work with an amplitude spectral density (ASD), which is the square root of the PSD; the ASD of a voltage signal has units of V Hz −1/2 . 5 For random vibration analysis, units of g 2 Hz −1 are frequently used for the PSD of acceleration . Here g denotes the g-force . 6  Although it is not necessary to assign physical dimensions to the signal or its argument, in the following discussion the terms used will assume that the signal varies in time.  Preliminary conventions on notations for time series  The phrase time series has been defined as "... a collection of observations made sequentially in time." 7 But it is also used to refer to a stochastic process that functions as the underlying theoretical model for the process that generated the data (and thus includes consideration of all the other possible sequences of data that could have been observed, but were not). Furthermore, the 'time' can be either continuous or discrete. There are, therefore, four different but closely related definitions and formulas for the power spectrum of a time series.  If    X  n     subscript  X  n    X_{n}   (discrete time) or    X  t     subscript  X  t    X_{t}   (continuous time) is a stochastic process, we will refer to a possible time series of data coming from it as a sample or path or signal of the stochastic process. To avoid confusion, we will reserve the word process for a stochastic process, and use one of the words signal , or sample , to refer to a time series of data. However, the reader should be aware that in the engineering literature especially, this distinction is often conveyed instead by speaking of a deterministic signal (for the sample) but of a random signal for the process. Hence the value at a point in time of the deterministic signal is a number, denoted by    x   (  t  )       x  t    x(t)   , but the value at a point in time of a random signal is a random variable, denoted by the same symbol,    x   (  t  )       x  t    x(t)   . Nevertheless, this article will preserve the distinction and use capital letters for the random variables and processes.  For any random variable   X   X   X   , standard notations of angle brackets or   E   normal-E   \operatorname{E}   will be used for ensemble average , also known as statistical expectation , and   Var   Var   \operatorname{Var}   for the theoretical variance .  Motivating example  Suppose    x  n     subscript  x  n    x_{n}   , from    n  =  0      n  0    n=0   to    N  -  1      N  1    N-1   is a time series (discrete time) with zero mean. Suppose that it is a sum of a finite number of periodic components (all frequencies are positive):      x  n     subscript  x  n    \displaystyle x_{n}     The variance of    x  n     subscript  x  n    x_{n}   is, for a zero-mean function as above, given by     1  N     ∑   n  =  0    N  -  1     x  n  2          1  N     superscript   subscript     n  0      N  1     superscript   subscript  x  n   2      \frac{1}{N}\sum_{n=0}^{N-1}x_{n}^{2}   . If these data were samples taken from an electrical signal, this would be its average power (power is energy per unit time, so it is analogous to variance if energy is analogous to the amplitude squared).  Now, for simplicity, suppose the signal extends infinitely in time, so we pass to the limit as    N  →  ∞     normal-→  N     N\rightarrow\infty   . If the average power is bounded, which is almost always the case in reality, then the following limit exists and is the variance of the data.        lim   N  →  ∞      1  N     ∑   n  =  0    N  -  1     x  n  2      .      subscript    normal-→  N         1  N     superscript   subscript     n  0      N  1     superscript   subscript  x  n   2       \lim_{N\rightarrow\infty}\frac{1}{N}\sum_{n=0}^{N-1}x_{n}^{2}.     Again, for simplicity, we will pass to continuous time, and assume that the signal extends infinitely in time in both directions. Then these two formulas become       x   (  t  )    =    ∑  k     A  k   ⋅   sin   (    2  π   ν  k   t   +   ϕ  k    )            x  t     subscript   k    normal-⋅   subscript  A  k         2  π   subscript  ν  k   t    subscript  ϕ  k         x(t)=\sum_{k}A_{k}\cdot\sin(2\pi\nu_{k}t+\phi_{k})     and        lim   T  →  ∞      1   2  T      ∫   -  T   T    x    (  t  )   2   d  t      .      subscript    normal-→  T         1    2  T      superscript   subscript     T    T     x   superscript  t  2   d  t       \lim_{T\rightarrow\infty}\frac{1}{2T}\int_{-T}^{T}x(t)^{2}dt.     The root mean square of   sin     \sin   is    1  /   2       1    2     1/\sqrt{2}   , so the variance of     A  k    sin   (    2  π   ν  k   t   +   ϕ  k    )         subscript  A  k         2  π   subscript  ν  k   t    subscript  ϕ  k       A_{k}\sin(2\pi\nu_{k}t+\phi_{k})   is     A  k  2   /  2       superscript   subscript  A  k   2   2    A_{k}^{2}/2   . Hence, the contribution to the average power of    x   (  t  )       x  t    x(t)   coming from the component with frequency    ν  k     subscript  ν  k    \nu_{k}   is     A  k  2   /  2.       superscript   subscript  A  k   2   2.    A_{k}^{2}/2.   . All these contributions add up to the average power of    x   (  t  )       x  t    x(t)   .  Then the power as a function of frequency is     A  k  2   /  2       superscript   subscript  A  k   2   2    A_{k}^{2}/2   , and its statistical cumulative distribution function     S   (  ν  )       S  ν    S(\nu)   will be       S   (  ν  )    =    ∑   k  :    ν  k   <  ν       A  k  2   /  2.          S  ν     subscript    normal-:  k     subscript  ν  k   ν        superscript   subscript  A  k   2   2.      S(\nu)=\sum_{k:\nu_{k}<\nu}A_{k}^{2}/2.      S   S   S   is a step function , monotonically non-decreasing. Its jumps occur at the frequencies of the periodic components of   x   x   x   , and the value of each jump is the power or variance of that component.  The variance is the covariance of the data with itself. If we now consider the same data but with a lag of   τ   τ   \tau   , we can take the covariance of    x   (  t  )       x  t    x(t)   with    x   (   t  +  τ   )       x    t  τ     x(t+\tau)   , and define this to be the autocorrelation function    c   c   c   of the signal (or data)   x   x   x   :        c   (  τ  )    =    lim   T  →  ∞      1   2  T      ∫   -  T   T    x   (  t  )   x   (   t  +  τ   )   d  t       .        c  τ     subscript    normal-→  T         1    2  T      superscript   subscript     T    T     x  t  x    t  τ   d  t        c(\tau)=\lim_{T\rightarrow\infty}\frac{1}{2T}\int_{-T}^{T}x(t)x(t+\tau)dt.     If it exists, it is an even function of   τ   τ   \tau   . If the average power is bounded, then   c   c   c   exists everywhere, is finite, and is bounded by    c   (  0  )       c  0    c(0)   , which is the average power or variance of the data.  It can be shown that   c   c   c   can be decomposed into periodic components with the same periods as   x   x   x   :        c   (  τ  )    =    ∑  k     1  2    A  k  2    cos   (   2  π   ν  k   τ   )       .        c  τ     subscript   k       1  2    superscript   subscript  A  k   2       2  π   subscript  ν  k   τ        c(\tau)=\sum_{k}\frac{1}{2}A_{k}^{2}\cos(2\pi\nu_{k}\tau).     This is in fact the spectral decomposition of   c   c   c   over the different frequencies, and is related to the distribution of power of   x   x   x   over the frequencies: the amplitude of a frequency component of   c   c   c   is its contribution to the average power of the signal.  The power spectrum of this example is not continuous, and therefore does not have a derivative, and therefore this signal does not have a power spectral density function. In general, the power spectrum will usually be the sum of two parts: a line spectrum such as in this example, which is not continuous and does not have a density function, and a residue, which is absolutely continuous and does have a density function.  Definition  Energy spectral density  Energy spectral density describes how the energy of a signal or a time series is distributed with frequency. Here, the term energy is used in the generalized sense of signal processing; 8 that is, the energy of a signal    x   (  t  )       x  t    x(t)   is        ∫   -  ∞   ∞       |   x   (  t  )    |   2    d  t    .      superscript   subscript             superscript      x  t    2   d  t     \int\limits_{-\infty}^{\infty}|x(t)|^{2}\,dt.   The energy spectral density is most suitable for transients—that is, pulse-like signals—having a finite total energy. In this case, Parseval's theorem  9 gives us an alternate expression for the energy of the signal in terms of its Fourier transform ,       x  ^    (  f  )    =    ∫   -  ∞   ∞     e   -   2  π  i  f  t     x   (  t  )   d  t     .         normal-^  x   f     superscript   subscript             superscript  e      2  π  i  f  t     x  t  d  t      \hat{x}(f)=\int\limits_{-\infty}^{\infty}e^{-2\pi ift}x(t)dt.            ∫   -  ∞   ∞       |   x   (  t  )    |   2    d  t    =    ∫   -  ∞   ∞       |    x  ^    (  f  )    |   2    d  f     .        superscript   subscript             superscript      x  t    2   d  t      superscript   subscript             superscript       normal-^  x   f    2   d  f      \int\limits_{-\infty}^{\infty}|x(t)|^{2}\,dt=\int\limits_{-\infty}^{\infty}|%
 \hat{x}(f)|^{2}\,df.     Here   f   f   f   is the frequency in Hz, i.e., cycles per second. Often used is the angular frequency     ω  =   2  π  f       ω    2  π  f     \omega=2\pi f   . Since the integral on the right-hand side is the energy of the signal, the integrand     |    x  ^    (  f  )    |   2     superscript       normal-^  x   f    2    |\hat{x}(f)|^{2}   can be interpreted as a density function describing the energy per unit frequency contained in the signal at the frequency   f   f   f   . In light of this, the energy spectral density of a signal    x   (  t  )       x  t    x(t)   is defined as 10        S   x  x     (  f  )    =    |    x  ^    (  f  )    |   2          subscript  S    x  x    f    superscript       normal-^  x   f    2     S_{xx}(f)=|\hat{x}(f)|^{2}     As a physical example of how one might measure the energy spectral density of a signal, suppose    V   (  t  )       V  t    V(t)   represents the potential (in volts ) of an electrical pulse propagating along a transmission line of impedance    Z   Z   Z   , and suppose the line is terminated with a matched resistor (so that all of the pulse energy is delivered to the resistor and none is reflected back). By Ohm's law , the power delivered to the resistor at time   t   t   t   is equal to     V    (  t  )   2    /  Z        V   superscript  t  2    Z    V(t)^{2}/Z   , so the total energy is found by integrating     V    (  t  )   2    /  Z        V   superscript  t  2    Z    V(t)^{2}/Z   with respect to time over the duration of the pulse. To find the value of the energy spectral density     S   x  x     (  f  )        subscript  S    x  x    f    S_{xx}(f)   at frequency   f   f   f   , one could insert between the transmission line and the resistor a bandpass filter which passes only a narrow range of frequencies (    Δ  f      normal-Δ  f    \Delta f   , say) near the frequency of interest and then measure the total energy    E   (  f  )       E  f    E(f)   dissipated across the resistor. The value of the energy spectral density at   f   f   f   is then estimated to be      E   (  f  )    /  Δ   f          E  f   normal-Δ   f    E(f)/\Delta f   . In this example, since the power     V    (  t  )   2    /  Z        V   superscript  t  2    Z    V(t)^{2}/Z   has units of V 2 Ω −1 , the energy    E   (  f  )       E  f    E(f)   has units of V 2 s Ω −1 = J, and hence the estimate      E   (  f  )    /  Δ   f          E  f   normal-Δ   f    E(f)/\Delta f   of the energy spectral density has units of J Hz −1 , as required. In many situations, it is common to forgo the step of dividing by   Z   Z   Z   so that the energy spectral density instead has units of V 2 s Hz −1 .  This definition generalizes in a straightforward manner to a discrete signal with an infinite number of values    x  n     subscript  x  n    x_{n}   such as a signal sampled at discrete times     x  n   =   x   (    n   Δ  t   )         subscript  x  n     x    n  normal-Δ  t      x_{n}=x(n\,\Delta t)   :         S   x  x     (  f  )    =     (   Δ  t   )   2     |    ∑   n  =   -  ∞    ∞     x  n    e   -   2  π  i  f  n       |   2    =     (   Δ  t   )   2     x  ^   d    (  f  )     x  ^   d  *    (  f  )     ,           subscript  S    x  x    f      superscript    normal-Δ  t   2    superscript      superscript   subscript     n             subscript  x  n    superscript  e      2  π  i  f  n        2            superscript    normal-Δ  t   2    subscript   normal-^  x   d   f   superscript   subscript   normal-^  x   d     f      S_{xx}(f)=(\Delta t)^{2}\left|\sum_{n=-\infty}^{\infty}x_{n}e^{-2\pi ifn}%
 \right|^{2}=(\Delta t)^{2}\hat{x}_{d}(f)\hat{x}_{d}^{*}(f),     where      x  ^   d    (  f  )        subscript   normal-^  x   d   f    \hat{x}_{d}(f)   is the discrete Fourier transform of    x  n     subscript  x  n    x_{n}   and      x  ^   d  *    (  f  )        superscript   subscript   normal-^  x   d     f    \hat{x}_{d}^{*}(f)   is the complex conjugate of      x  ^   d    (  f  )        subscript   normal-^  x   d   f    \hat{x}_{d}(f)   . The sampling interval    Δ  t      normal-Δ  t    \Delta t   is needed to keep the correct physical units and to ensure that we recover the continuous case in the limit     Δ  t   →  0     normal-→    normal-Δ  t   0    \Delta t\rightarrow 0   ; however, in the mathematical sciences, the interval is often set to 1.  Power spectral density  The above definition of energy spectral density is most suitable for transients, i.e., pulse-like signals, for which the Fourier transforms of the signals exist. For continued signals that describe, for example, stationary physical processes, it makes more sense to define a power spectral density (PSD), which describes how the power of a signal or time series is distributed over the different frequencies, as in the simple example given previously. Here, power can be the actual physical power, or more often, for convenience with abstract signals, can be defined as the squared value of the signal. For example, statisticians study the variance of a set of data, but because of the analogy with electrical signals, it is customary to refer to it as the power spectrum even when it is not, physically speaking, power. The average power P of a signal    x   (  t  )       x  t    x(t)   is the following time average:       P  =    lim   T  →  ∞      1   2  T      ∫   -  T   T    x     (  t  )   2    d  t       .      P    subscript    normal-→  T         1    2  T      superscript   subscript     T    T     x   superscript  t  2   d  t        P=\lim_{T\rightarrow\infty}\frac{1}{2T}\int_{-T}^{T}x(t)^{2}\,dt.     The power of a signal may be finite even if the energy is infinite. For example, a 10-volt power supply connected to a 1 kΩ resistor delivers = 0.1 W of power at any given time; however, if the supply is allowed to operate for an infinite amount of time, it will deliver an infinite amount of energy (0.1 J each second for an infinite number of seconds).  In analyzing the frequency content of the signal    x   (  t  )       x  t    x(t)   , one might like to compute the ordinary Fourier transform     x  ^    (  ω  )        normal-^  x   ω    \hat{x}(\omega)   ; however, for many signals of interest this Fourier transform does not exist.{{#tag:ref|Some authors (e.g. Risken 11 ) still use the non-normalized Fourier transform in a formal way to formulate a definition of the power spectral density       ⟨    x  ^    (  ω  )     x  ^   ∗    (   ω  ′   )    ⟩   =   2   π   f   (  ω  )   δ   (   ω  -   ω  ′    )         delimited-⟨⟩     normal-^  x   ω   superscript   normal-^  x   normal-∗    superscript  ω  normal-′       2  π  f  ω  δ    ω   superscript  ω  normal-′       \langle\hat{x}(\omega)\hat{x}^{\ast}(\omega^{\prime})\rangle=2\pi\,f(\omega)\,%
 \delta(\omega-\omega^{\prime})   ,  where    δ   (   ω  -   ω  ′    )       δ    ω   superscript  ω  normal-′      \delta(\omega-\omega^{\prime})   is the Dirac delta function . Such formal statements may sometimes be useful to guide the intuition, but should always be used with utmost care.|group="N"}} Because of this, it is advantageous to work with a truncated Fourier transform      x  ^   T    (  ω  )        subscript   normal-^  x   T   ω    \hat{x}_{T}(\omega)   , where the signal is integrated only over a finite interval [0, T ]:          x  ^   T    (  ω  )    =    1    2  π  T       ∫  0  T    x   (  t  )     e   -   i  ω  t      d  t      .         subscript   normal-^  x   T   ω       1      2  π  T       superscript   subscript   0   T     x  t   superscript  e      i  ω  t     d  t       \hat{x}_{T}(\omega)=\frac{1}{\sqrt{2\pi T}}\int_{0}^{T}x(t)e^{-i\omega t}\,dt.     Note that       x  ^   T    (  ω  )    =      x  ^   T    (  f  )    /    2  π            subscript   normal-^  x   T   ω        subscript   normal-^  x   T   f       2  π       \hat{x}_{T}(\omega)=\hat{x}_{T}(f)/\sqrt{2\pi}   .  Then the power spectral density can be defined as 12 13         S   x  x     (  ω  )    =    lim   T  →  ∞     𝐄   [    |     x  ^   T    (  ω  )    |   2   ]      .         subscript  S    x  x    ω     subscript    normal-→  T       𝐄   delimited-[]   superscript       subscript   normal-^  x   T   ω    2        S_{xx}(\omega)=\lim_{T\rightarrow\infty}\mathbf{E}\left[|\hat{x}_{T}(\omega)|^%
 {2}\right].     Here E denotes the expected value ; explicitly, we have 14        𝐄   [    |     x  ^   T    (  ω  )    |   2   ]    =   𝐄   [    1  T     ∫  0  T     x  *    (  t  )     e   i  ω  t     d  t    ∫  0  T    x   (   t  ′   )     e   -   i  ω   t  ′       d   t  ′        ]    =    1  T     ∫  0  T     ∫  0  T    𝐄   [    x  *    (  t  )   x   (   t  ′   )    ]     e   i  ω   (   t  -   t  ′    )      d   t   d   t  ′        .          𝐄   delimited-[]   superscript       subscript   normal-^  x   T   ω    2       𝐄   delimited-[]      1  T     superscript   subscript   0   T      superscript  x    t   superscript  e    i  ω  t    d  t    superscript   subscript   0   T     x   superscript  t  normal-′    superscript  e      i  ω   superscript  t  normal-′      d   superscript  t  normal-′                   1  T     superscript   subscript   0   T     superscript   subscript   0   T     𝐄   delimited-[]     superscript  x    t  x   superscript  t  normal-′      superscript  e    i  ω    t   superscript  t  normal-′      d  t  d   superscript  t  normal-′          \mathbf{E}\left[|\hat{x}_{T}(\omega)|^{2}\right]=\mathbf{E}\left[\frac{1}{T}%
 \int\limits_{0}^{T}x^{*}(t)e^{i\omega t}\,dt\int\limits_{0}^{T}x(t^{\prime})e^%
 {-i\omega t^{\prime}}\,dt^{\prime}\right]=\frac{1}{T}\int\limits_{0}^{T}\int%
 \limits_{0}^{T}\mathbf{E}\left[x^{*}(t)x(t^{\prime})\right]e^{i\omega(t-t^{%
 \prime})}\,dt\,dt^{\prime}.     Using such formal reasoning, one may already guess that for a stationary random process , the power spectral density     S   x  x     (  ω  )        subscript  S    x  x    ω    S_{xx}(\omega)   and the autocorrelation function of this signal     γ   (  τ  )    =   ⟨   X   (  t  )   X   (   t  +  τ   )    ⟩         γ  τ    delimited-⟨⟩    X  t  X    t  τ       \gamma(\tau)=\langle X(t)X(t+\tau)\rangle   should be a Fourier transform pair. Provided that    γ   (  τ  )       γ  τ    \gamma(\tau)   is absolutely integrable, which is not always true, then         S   x  x     (  ω  )    =     ∫   -  ∞   ∞     γ   (  τ  )     e   -   i  ω  τ      d  τ    =    γ  ^    (  ω  )     .           subscript  S    x  x    ω     superscript   subscript            γ  τ   superscript  e      i  ω  τ     d  τ            normal-^  γ   ω      S_{xx}(\omega)=\int_{-\infty}^{\infty}\,\gamma(\tau)\,e^{-i\omega\tau}\,d\tau=%
 \hat{\gamma}(\omega).     The Wiener–Khinchin theorem makes sense of this formula for any wide-sense stationary process under weaker hypotheses   γ   γ   \gamma   does not need to be absolutely integrable, it only needs to exist. But the integral can no longer be interpreted as usual. The formula also makes sense if interpreted as involving distributions (in the sense of Laurent Schwartz , not in the sense of a statistical Cumulative distribution function ) instead of functions. If   γ   γ   \gamma   is continuous, Bochner's theorem can be used to prove that its Fourier transform exists as a positive measure , whose distribution function is F (but not necessarily as a function and not necessarily possessing a probability density).  Many authors use this equality to actually define the power spectral density. 15  The power of the signal in a given frequency band    [   ω  1   ,   ω  2   ]      subscript  ω  1    subscript  ω  2     [\omega_{1},\omega_{2}]   can be calculated by integrating over positive and negative frequencies,          ∫   ω  1    ω  2       S   x  x     (  ω  )     +    S   x  x     (   -  ω   )   d  ω    =    F   (   ω  2   )    -   F   (   -   ω  2    )             superscript   subscript    subscript  ω  1     subscript  ω  2       subscript  S    x  x    ω       subscript  S    x  x      ω   d  ω        F   subscript  ω  2      F     subscript  ω  2        \int_{\omega_{1}}^{\omega_{2}}\,S_{xx}(\omega)+S_{xx}(-\omega)\,d\omega=F(%
 \omega_{2})-F(-\omega_{2})     where   F   F   F   is the integrated spectrum whose derivative is    S   x  x      subscript  S    x  x     S_{xx}   .  More generally, similar techniques may be used to estimate a time-varying spectral density.  The definition of the power spectral density generalizes in a straightforward manner to finite time series    x  n     subscript  x  n    x_{n}   with    1  ≤  n  ≤  N        1  n       N     1\leq n\leq N   , such as a signal sampled at discrete times     x  n   =   x   (   n  Δ  t   )         subscript  x  n     x    n  normal-Δ  t      x_{n}=x(n\Delta t)   for a total measurement period    T  =   N  Δ  t       T    N  normal-Δ  t     T=N\Delta t   .        S   x  x     (  ω  )    =      (   Δ  t   )   2   T     |    ∑   n  =  1   N     x  n    e   -   i  ω  n       |   2           subscript  S    x  x    ω        superscript    normal-Δ  t   2   T    superscript      superscript   subscript     n  1    N      subscript  x  n    superscript  e      i  ω  n        2      S_{xx}(\omega)=\frac{(\Delta t)^{2}}{T}\left|\sum_{n=1}^{N}x_{n}e^{-i\omega n}%
 \right|^{2}   .  In a real-world application, one would typically average this single-measurement PSD over several repetitions of the measurement to obtain a more accurate estimate of the theoretical PSD of the physical process underlying the individual measurements. This computed PSD is sometimes called periodogram . One can prove that this periodogram converges to the true PSD when the averaging time interval T goes to infinity (Brown & Hwang 16 ) to approach the Power Spectral Density (PSD).  If two signals both possess power spectral densities, then a cross-spectral density can be calculated by using their cross-correlation function.  Properties of the power spectral density  Some properties of the PSD include: 17   The spectrum of a real valued process is an even function of frequency      S   x  x     (   -  ω   )    =    S   x  x     (  ω  )           subscript  S    x  x      ω       subscript  S    x  x    ω     S_{xx}(-\omega)=S_{xx}(\omega)   .  If the process is continuous and purely indeterministic, the autocovariance function can be reconstructed by using the Inverse Fourier transform  it describes the distribution of the variance over frequency. In particular,       Var   (   X  n   )    =   γ  0   =   2    ∫  0  ∞     S   x  x     (  ω  )   d  ω      .          Var   subscript  X  n     subscript  γ  0          2    superscript   subscript   0        subscript  S    x  x    ω  d  ω        \text{Var}(X_{n})=\gamma_{0}=2\int_{0}^{\infty}S_{xx}(\omega)d\omega.     It is a linear function of the autocovariance function in the sense that if   γ   γ   \gamma   is decomposed into two functions     γ   (  τ  )    =     α  1    γ  1    (  τ  )    +    α  2    γ  2    (  τ  )           γ  τ        subscript  α  1    subscript  γ  1   τ      subscript  α  2    subscript  γ  2   τ      \gamma(\tau)=\alpha_{1}\gamma_{1}(\tau)+\alpha_{2}\gamma_{2}(\tau)   , then      f  =     α  1    S    x  x   ,  1     +    α  2    S    x  x   ,  2       .      f       subscript  α  1    subscript  S     x  x   1        subscript  α  2    subscript  S     x  x   2        f=\alpha_{1}S_{xx,1}+\alpha_{2}S_{xx,2}.      The integrated spectrum or power spectral distribution     F   (  ω  )       F  ω    F(\omega)   is defined as 18        F   (  ω  )    =    ∫   -  ∞   ω     S   x  x     (   ω  ′   )   d   ω  ′      .        F  ω     superscript   subscript        ω      subscript  S    x  x     superscript  ω  normal-′   d   superscript  ω  normal-′       F(\omega)=\int_{-\infty}^{\omega}S_{xx}(\omega^{\prime})\,d\omega^{\prime}.     Cross-spectral density  Given two signals    x   (  t  )       x  t    x(t)   and    y   (  t  )       y  t    y(t)   , each of which possess power spectral densities     S   x  x     (  ω  )        subscript  S    x  x    ω    S_{xx}(\omega)   and     S   y  y     (  ω  )        subscript  S    y  y    ω    S_{yy}(\omega)   , it is possible to define a cross-spectral density (CSD) given by         S   x  y     (  ω  )    =    lim   T  →  ∞     𝐄   {     [    F  x  T    (  ω  )    ]   *    F  y  T    (  ω  )    }      .         subscript  S    x  y    ω     subscript    normal-→  T       𝐄      superscript   delimited-[]     superscript   subscript  F  x   T   ω       superscript   subscript  F  y   T   ω        S_{xy}(\omega)=\lim_{T\rightarrow\infty}\mathbf{E}\left\{\left[F_{x}^{T}(%
 \omega)\right]^{*}F_{y}^{T}(\omega)\right\}.     The cross-spectral density (or 'cross power spectrum') is thus the Fourier transform of the cross-correlation function.         S   x  y     (  ω  )    =    ∫   -  ∞   ∞     R   x  y     (  t  )    e   -   j  ω  t     d  t    =    ∫   -  ∞   ∞     [    ∫   -  ∞   ∞      x   (  τ  )    ⋅  y    (   τ  +  t   )   d  τ    ]    e   -   j  ω  t     d  t     ,           subscript  S    x  y    ω     superscript   subscript             subscript  R    x  y    t   superscript  e      j  ω  t     d  t           superscript   subscript             delimited-[]    superscript   subscript             normal-⋅    x  τ   y     τ  t   d  τ      superscript  e      j  ω  t     d  t       S_{xy}(\omega)=\int_{-\infty}^{\infty}R_{xy}(t)e^{-j\omega t}dt=\int_{-\infty}%
 ^{\infty}\left[\int_{-\infty}^{\infty}x(\tau)\cdot y(\tau+t)d\tau\right]\,e^{-%
 j\omega t}dt,     where     R   x  y     (  t  )        subscript  R    x  y    t    R_{xy}(t)   is the cross-correlation of    x   (  t  )       x  t    x(t)   and    y   (  t  )       y  t    y(t)   .  By an extension of the Wiener–Khinchin theorem, the Fourier transform of the cross-spectral density     S   x  y     (  ω  )        subscript  S    x  y    ω    S_{xy}(\omega)   is the cross-covariance function. 19 In light of this, the PSD is seen to be a special case of the CSD for     x   (  t  )    =   y   (  t  )          x  t     y  t     x(t)=y(t)   .  For discrete signals x n and y n , the relationship between the cross-spectral density and the cross-covariance is        S   x  y     (  ω  )    =    1   2  π      ∑   n  =   -  ∞    ∞     R   x  y     (  n  )    e   -   j  ω  n               subscript  S    x  y    ω       1    2  π      superscript   subscript     n             subscript  R    x  y    n   superscript  e      j  ω  n          S_{xy}(\omega)=\frac{1}{2\pi}\sum_{n=-\infty}^{\infty}R_{xy}(n)e^{-j\omega n}     Estimation  The goal of spectral density estimation is to estimate the spectral density of a random signal from a sequence of time samples. Depending on what is known about the signal, estimation techniques can involve parametric or non-parametric approaches, and may be based on time-domain or frequency-domain analysis. For example, a common parametric technique involves fitting the observations to an autoregressive model . A common non-parametric technique is the periodogram .  The spectral density is usually estimated using Fourier transform methods (such as the Welch method ), but other techniques such as the maximum entropy method can also be used.  Properties   The spectral density of    f   (  t  )       f  t    f(t)   and the autocorrelation of    f   (  t  )       f  t    f(t)   form a Fourier transform pair (for PSD versus ESD, different definitions of autocorrelation function are used). This result is known as Wiener–Khinchin theorem .    One of the results of Fourier analysis is Parseval's theorem which states that the area under the energy spectral density curve is equal to the area under the square of the magnitude of the signal, the total energy:            ∫   -  ∞   ∞       |   f   (  t  )    |   2    d  t    =    ∫   -  ∞   ∞    E  S  D   (  ω  )   d  ω     .        superscript   subscript             superscript      f  t    2   d  t      superscript   subscript            E  S  D  ω  d  ω      \int_{-\infty}^{\infty}\left|f(t)\right|^{2}\,dt=\int_{-\infty}^{\infty}ESD(%
 \omega)\,d\omega.         The above theorem holds true in the discrete cases as well. A similar result holds for power: the area under the power spectral density curve is equal to the total signal power, which is    R   (  0  )       R  0    R(0)   , the autocorrelation function at zero lag. This is also (up to a constant which depends on the normalization factors chosen in the definitions employed) the variance of the data comprising the signal.   Related concepts    Most spectrum graphs really display only the power spectral density. Sometimes (e.g., Bode plot , chirp ) the complete frequency spectrum is graphed in two parts, amplitude versus frequency and phase versus frequency (which contains the rest of the information from the frequency spectrum). The original function    f   (  t  )       f  t    f(t)   cannot be recovered from the amplitude spectral density part alone — information about the phase is lost. See spectral phase and phase noise .    The spectral centroid of a signal is the midpoint of its spectral density function, i.e. the frequency that divides the distribution into two equal parts.    The spectral edge frequency of a signal is an extension of the previous concept to any proportion instead of two equal parts.    The spectral density is a function of frequency, not a function of time. However, the spectral density of small windows of a longer signal may be calculated, and plotted versus time associated with the window. Such a graph is called a spectrogram . This is the basis of a number of spectral analysis techniques such as the short-time Fourier transform and wavelets .   Applications  Electrical engineering  The concept and use of the power spectrum of a signal is fundamental in electrical engineering , especially in electronic communication systems , including radio communications , radars , and related systems, plus passive [remote sensing] technology. Much effort has been expended and millions of dollars spent on developing and producing electronic instruments called " spectrum analyzers " for aiding electrical engineers and technicians in observing and measuring the power spectra of signals. The cost of a spectrum analyzer varies depending on its frequency range, its bandwidth , and its accuracy. The higher the frequency range ( S-band , C-band , X-band , Ku-band , K-band , Ka-band , etc.), the more difficult the components are to make, assemble, and test and the more expensive the spectrum analyzer is. Also, the wider the bandwidth that a spectrum analyzer possesses, the more costly that it is, and the capability for more accurate measurements increases costs as well.  The spectrum analyzer measures the magnitude of the short-time Fourier transform (STFT) of an input signal. If the signal being analyzed can be considered a stationary process, the STFT is a good smoothed estimate of its power spectral density. These devices work in low frequencies and with small bandwidths.  See also   Coherence (signal processing) , for use of the cross-spectral density .  Noise spectral density  Spectral density estimation  Spectral efficiency  Spectral power distribution  Brightness temperature  Colors of noise  Spectral leakage  Window function  Bispectrum   Notes  References  External links   Power Spectral Density Matlab scripts   "  Category:Frequency domain analysis  Category:Signal processing  Category:Waves  Category:Spectroscopy  Category:Scattering  Category:Fourier analysis  Category:Radio spectrum     or, equivalently, by the measure of that interval,    μ   (   [  a  ,  b  )   )       μ   a  b     \mu([a,b))   . ↩  or, equivalently,   μ   μ   \mu   is absolutely continuous with respect to Lebesgue measure . ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩   ↩  ↩  ↩  An Introduction to the Theory of Random Signals and Noise, Wilbur B. Davenport and Willian L. Root, IEEE Press, New York, 1987, ISBN 0-87942-235-1 ↩  ↩     