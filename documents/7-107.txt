   Mean squared prediction error      Mean squared prediction error   In statistics the mean squared prediction error of a smoothing or curve fitting procedure is the expected value of the squared difference between the fitted values    g  ^     normal-^  g    \widehat{g}   and the (unobservable) function g . If the smoothing procedure has operator matrix  L , then        MSPE   (  L  )    =   E   [    (    g   (   x  i   )    -    g  ^    (   x  i   )     )   2   ]     .       MSPE  L    normal-E   superscript      g   subscript  x  i       normal-^  g    subscript  x  i     2      \operatorname{MSPE}(L)=\operatorname{E}\left[\left(g(x_{i})-\widehat{g}(x_{i})%
 \right)^{2}\right].     The MSPE can be decomposed into two terms (just like mean squared error is decomposed into bias and variance ); however for MSPE one term is the sum of squared biases of the fitted values and another the sum of variances of the fitted values:        MSPE   (  L  )    =     ‚àë   i  =  1   n     (    E   [    g  ^    (   x  i   )    ]    -   g   (   x  i   )     )   2    +    ‚àë   i  =  1   n    var   [    g  ^    (   x  i   )    ]       .       MSPE  L       superscript   subscript     i  1    n    superscript     normal-E     normal-^  g    subscript  x  i       g   subscript  x  i     2      superscript   subscript     i  1    n    var     normal-^  g    subscript  x  i         \operatorname{MSPE}(L)=\sum_{i=1}^{n}\left(\operatorname{E}\left[\widehat{g}(x%
 _{i})\right]-g(x_{i})\right)^{2}+\sum_{i=1}^{n}\operatorname{var}\left[%
 \widehat{g}(x_{i})\right].     Note that knowledge of g is required in order to calculate MSPE exactly.  Estimation of MSPE  For the model     y  i   =    g   (   x  i   )    +   œÉ   Œµ  i          subscript  y  i       g   subscript  x  i      œÉ   subscript  Œµ  i       y_{i}=g(x_{i})+\sigma\varepsilon_{i}   where     Œµ  i   ‚àº   ùí©   (  0  ,  1  )       similar-to   subscript  Œµ  i     ùí©   0  1      \varepsilon_{i}\sim\mathcal{N}(0,1)   , one may write        MSPE   (  L  )    =     g  ‚Ä≤     (   I  -  L   )   ‚Ä≤    (   I  -  L   )   g   +    œÉ  2    tr   [    L  ‚Ä≤   L   ]       .       MSPE  L        superscript  g  normal-‚Ä≤    superscript    I  L   normal-‚Ä≤     I  L   g      superscript  œÉ  2    tr     superscript  L  normal-‚Ä≤   L        \operatorname{MSPE}(L)=g^{\prime}(I-L)^{\prime}(I-L)g+\sigma^{2}\operatorname{%
 tr}\left[L^{\prime}L\right].     The first term is equivalent to         ‚àë   i  =  1   n     (    E   [    g  ^    (   x  i   )    ]    -   g   (   x  i   )     )   2    =    E   [    ‚àë   i  =  1   n     (    y  i   -    g  ^    (   x  i   )     )   2    ]    -    œÉ  2    tr   [     (   I  -  L   )   ‚Ä≤    (   I  -  L   )    ]       .        superscript   subscript     i  1    n    superscript     normal-E     normal-^  g    subscript  x  i       g   subscript  x  i     2       normal-E    superscript   subscript     i  1    n    superscript     subscript  y  i      normal-^  g    subscript  x  i     2        superscript  œÉ  2    tr     superscript    I  L   normal-‚Ä≤     I  L         \sum_{i=1}^{n}\left(\operatorname{E}\left[\widehat{g}(x_{i})\right]-g(x_{i})%
 \right)^{2}=\operatorname{E}\left[\sum_{i=1}^{n}\left(y_{i}-\widehat{g}(x_{i})%
 \right)^{2}\right]-\sigma^{2}\operatorname{tr}\left[\left(I-L\right)^{\prime}%
 \left(I-L\right)\right].     Thus,        MSPE   (  L  )    =    E   [    ‚àë   i  =  1   n     (    y  i   -    g  ^    (   x  i   )     )   2    ]    -    œÉ  2    (   n  -   2   tr   [  L  ]      )      .       MSPE  L      normal-E    superscript   subscript     i  1    n    superscript     subscript  y  i      normal-^  g    subscript  x  i     2        superscript  œÉ  2     n    2   tr  L         \operatorname{MSPE}(L)=\operatorname{E}\left[\sum_{i=1}^{n}\left(y_{i}-%
 \widehat{g}(x_{i})\right)^{2}\right]-\sigma^{2}\left(n-2\operatorname{tr}\left%
 [L\right]\right).     If    œÉ  2     superscript  œÉ  2    \sigma^{2}   is known or well-estimated by     œÉ  ^   2     superscript   normal-^  œÉ   2    \widehat{\sigma}^{2}   , it becomes possible to estimate MSPE by         MSPE  ^    (  L  )    =     ‚àë   i  =  1   n     (    y  i   -    g  ^    (   x  i   )     )   2    -     œÉ  ^   2    (   n  -   2   tr   [  L  ]      )      .        normal-^  MSPE   L       superscript   subscript     i  1    n    superscript     subscript  y  i      normal-^  g    subscript  x  i     2       superscript   normal-^  œÉ   2     n    2   tr  L         \operatorname{\widehat{MSPE}}(L)=\sum_{i=1}^{n}\left(y_{i}-\widehat{g}(x_{i})%
 \right)^{2}-\widehat{\sigma}^{2}\left(n-2\operatorname{tr}\left[L\right]\right).     Colin Mallows advocated this method in the construction of his model selection statistic C p , which is a normalized version of the estimated MSPE:        C  p   =       ‚àë   i  =  1   n     (    y  i   -    g  ^    (   x  i   )     )   2      œÉ  ^   2    -  n   +   2   tr   [  L  ]       .       subscript  C  p           superscript   subscript     i  1    n    superscript     subscript  y  i      normal-^  g    subscript  x  i     2     superscript   normal-^  œÉ   2    n     2   tr  L       C_{p}=\frac{\sum_{i=1}^{n}\left(y_{i}-\widehat{g}(x_{i})\right)^{2}}{\widehat{%
 \sigma}^{2}}-n+2\operatorname{tr}\left[L\right].     where p comes from that fact that the number of parameters p estimated for a parametric smoother is given by    p  =   tr   [  L  ]        p   tr  L     p=\operatorname{tr}\left[L\right]   , and C is in honor of Cuthbert Daniel .  See also   Mean squared error  Errors and residuals in statistics  Law of total variance   "  Category:Point estimation performance  Category:Statistical deviation and dispersion  Category:Loss functions   