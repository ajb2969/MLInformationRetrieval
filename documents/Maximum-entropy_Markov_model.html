<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="116">Maximum-entropy Markov model</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Maximum-entropy Markov model</h1>
<hr/>

<p>In <a href="machine_learning" title="wikilink">machine learning</a>, a <strong>maximum-entropy Markov model</strong> (<strong>MEMM</strong>), or <strong>conditional Markov model</strong> (<strong>CMM</strong>), is a <a href="graphical_model" title="wikilink">graphical model</a> for <a href="sequence_labeling" title="wikilink">sequence labeling</a> that combines features of <a href="hidden_Markov_model" title="wikilink">hidden Markov models</a> (HMMs) and <a href="Maximum_entropy_probability_distribution" title="wikilink">maximum entropy</a> (MaxEnt) models. An MEMM is a <a href="discriminative_model" title="wikilink">discriminative model</a> that extends a standard <a href="maximum_entropy_classifier" title="wikilink">maximum entropy classifier</a> by assuming that the unknown values to be learnt are connected in a <a href="Markov_chain" title="wikilink">Markov chain</a> rather than being <a href="conditionally_independent" title="wikilink">conditionally independent</a> of each other. MEMMs find applications in <a href="natural_language_processing" title="wikilink">natural language processing</a>, specifically in <a href="part-of-speech_tagging" title="wikilink">part-of-speech tagging</a><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> and <a href="information_extraction" title="wikilink">information extraction</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h2 id="model">Model</h2>

<p>Suppose we have a sequence of observations 

<math display="inline" id="Maximum-entropy_Markov_model:0">
 <semantics>
  <mrow>
   <msub>
    <mi>O</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>O</mi>
    <mi>n</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>O</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>O</ci>
     <ci>n</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O_{1},\dots,O_{n}
  </annotation>
 </semantics>
</math>

 that we seek to tag with the labels 

<math display="inline" id="Maximum-entropy_Markov_model:1">
 <semantics>
  <mrow>
   <msub>
    <mi>S</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>S</mi>
    <mi>n</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>S</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>S</ci>
     <ci>n</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S_{1},\dots,S_{n}
  </annotation>
 </semantics>
</math>

that maximize the conditional probability 

<math display="inline" id="Maximum-entropy_Markov_model:2">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>S</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>S</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">|</mo>
    <msub>
     <mi>O</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>O</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>S</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>S</ci>
      <ci>n</ci>
     </apply>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>O</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>O</ci>
      <ci>n</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(S_{1},\dots,S_{n}|O_{1},\dots,O_{n})
  </annotation>
 </semantics>
</math>

. In a MEMM, this probability is factored into Markov transition probabilities, where the probability of transitioning to a particular label depends only on the observation at that position and the previous position's label:</p>

<p>

<math display="block" id="Maximum-entropy_Markov_model:3">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>S</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>S</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">|</mo>
    <msub>
     <mi>O</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>O</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <munderover>
    <mo largeop="true" movablelimits="false" symmetric="true">∏</mo>
    <mrow>
     <mi>t</mi>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mi>n</mi>
   </munderover>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>S</mi>
     <mi>t</mi>
    </msub>
    <mo stretchy="false">|</mo>
    <msub>
     <mi>S</mi>
     <mrow>
      <mi>t</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>O</mi>
     <mi>t</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>S</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>S</ci>
      <ci>n</ci>
     </apply>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>O</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>O</ci>
      <ci>n</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <csymbol cd="latexml">product</csymbol>
      <apply>
       <eq></eq>
       <ci>t</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>n</ci>
    </apply>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>S</ci>
      <ci>t</ci>
     </apply>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>S</ci>
      <apply>
       <minus></minus>
       <ci>t</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>O</ci>
      <ci>t</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-.</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(S_{1},\dots,S_{n}|O_{1},\dots,O_{n})=\prod_{t=1}^{n}P(S_{t}|S_{t-1},O_{t}).
  </annotation>
 </semantics>
</math>

 Each of these transition probabilities come from the same general distribution 

<math display="inline" id="Maximum-entropy_Markov_model:4">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>s</mi>
    <mo stretchy="false">|</mo>
    <msup>
     <mi>s</mi>
     <mo>′</mo>
    </msup>
    <mo>,</mo>
    <mi>o</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">s</csymbol>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-′</ci>
     </apply>
     <ci>normal-,</ci>
     <csymbol cd="unknown">o</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(s|s^{\prime},o)
  </annotation>
 </semantics>
</math>

. For each possible label value of the previous label 

<math display="inline" id="Maximum-entropy_Markov_model:5">
 <semantics>
  <msup>
   <mi>s</mi>
   <mo>′</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>s</ci>
    <ci>normal-′</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s^{\prime}
  </annotation>
 </semantics>
</math>

, the probability of a certain label 

<math display="inline" id="Maximum-entropy_Markov_model:6">
 <semantics>
  <mi>s</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>s</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s
  </annotation>
 </semantics>
</math>

 is modeled in the same way as a <a href="maximum_entropy_classifier" title="wikilink">maximum entropy classifier</a>:<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>

<math display="block" id="Maximum-entropy_Markov_model:7">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>s</mi>
    <mo stretchy="false">|</mo>
    <msup>
     <mi>s</mi>
     <mo>′</mo>
    </msup>
    <mo>,</mo>
    <mi>o</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <msub>
    <mi>P</mi>
    <msup>
     <mi>s</mi>
     <mo>′</mo>
    </msup>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>s</mi>
    <mo stretchy="false">|</mo>
    <mi>o</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mn>1</mn>
    <mrow>
     <mi>Z</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>o</mi>
      <mo>,</mo>
      <msup>
       <mi>s</mi>
       <mo>′</mo>
      </msup>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
   <mi>exp</mi>
   <mrow>
    <mo>(</mo>
    <munder>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mi>a</mi>
    </munder>
    <msub>
     <mi>λ</mi>
     <mi>a</mi>
    </msub>
    <msub>
     <mi>f</mi>
     <mi>a</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>o</mi>
     <mo>,</mo>
     <mi>s</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>)</mo>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">s</csymbol>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-′</ci>
     </apply>
     <ci>normal-,</ci>
     <csymbol cd="unknown">o</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>P</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-′</ci>
     </apply>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">s</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">o</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <apply>
      <times></times>
      <ci>Z</ci>
      <interval closure="open">
       <ci>o</ci>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>s</ci>
        <ci>normal-′</ci>
       </apply>
      </interval>
     </apply>
    </apply>
    <exp></exp>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <ci>a</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>λ</ci>
      <ci>a</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>f</ci>
      <ci>a</ci>
     </apply>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">o</csymbol>
      <ci>normal-,</ci>
      <csymbol cd="unknown">s</csymbol>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-.</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(s|s^{\prime},o)=P_{s^{\prime}}(s|o)=\frac{1}{Z(o,s^{\prime})}\exp\left(\sum_%
{a}\lambda_{a}f_{a}(o,s)\right).
  </annotation>
 </semantics>
</math>

 Here, the 

<math display="inline" id="Maximum-entropy_Markov_model:8">
 <semantics>
  <mrow>
   <msub>
    <mi>f</mi>
    <mi>a</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>o</mi>
    <mo>,</mo>
    <mi>s</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <ci>a</ci>
    </apply>
    <interval closure="open">
     <ci>o</ci>
     <ci>s</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{a}(o,s)
  </annotation>
 </semantics>
</math>

 are real-valued or categorical feature-functions, and 

<math display="inline" id="Maximum-entropy_Markov_model:9">
 <semantics>
  <mrow>
   <mi>Z</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>o</mi>
    <mo>,</mo>
    <msup>
     <mi>s</mi>
     <mo>′</mo>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>Z</ci>
    <interval closure="open">
     <ci>o</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-′</ci>
     </apply>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Z(o,s^{\prime})
  </annotation>
 </semantics>
</math>

 is a normalization term ensuring that the distribution sums to one. This form for the distribution corresponds to the <a href="maximum_entropy_probability_distribution" title="wikilink">maximum entropy probability distribution</a> satisfying the constraint that the empirical expectation for the feature is equal to the expectation given the model:</p>

<p>

<math display="block" id="Maximum-entropy_Markov_model:10">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mo>E</mo>
      <mi>e</mi>
     </msub>
     <mrow>
      <mo>[</mo>
      <mrow>
       <msub>
        <mi>f</mi>
        <mi>a</mi>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>o</mi>
        <mo>,</mo>
        <mi>s</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>]</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mrow>
      <msub>
       <mo>E</mo>
       <mi>p</mi>
      </msub>
      <mrow>
       <mo>[</mo>
       <mrow>
        <msub>
         <mi>f</mi>
         <mi>a</mi>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>o</mi>
         <mo>,</mo>
         <mi>s</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>]</mo>
      </mrow>
     </mrow>
     <mrow>
      <mtext>for all</mtext>
      <mi>a</mi>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>normal-E</ci>
      <ci>e</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>f</ci>
       <ci>a</ci>
      </apply>
      <interval closure="open">
       <ci>o</ci>
       <ci>s</ci>
      </interval>
     </apply>
    </apply>
    <list>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-E</ci>
       <ci>p</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>f</ci>
        <ci>a</ci>
       </apply>
       <interval closure="open">
        <ci>o</ci>
        <ci>s</ci>
       </interval>
      </apply>
     </apply>
     <apply>
      <times></times>
      <mtext>for all</mtext>
      <ci>a</ci>
     </apply>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{E}_{e}\left[f_{a}(o,s)\right]=\operatorname{E}_{p}\left[f_{a}(o,%
s)\right]\quad\text{ for all }a.
  </annotation>
 </semantics>
</math>

 The parameters 

<math display="inline" id="Maximum-entropy_Markov_model:11">
 <semantics>
  <msub>
   <mi>λ</mi>
   <mi>a</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>λ</ci>
    <ci>a</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda_{a}
  </annotation>
 </semantics>
</math>

 can be estimated using <a href="generalized_iterative_scaling" title="wikilink">generalized iterative scaling</a>.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> Furthermore, a variant of the <a href="Baum–Welch_algorithm" title="wikilink">Baum–Welch algorithm</a>, which is used for training HMMs, can be used to estimate parameters when training data has <a href="Semi-supervised_learning" title="wikilink">incomplete or missing labels</a>.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>

<p>The optimal state sequence 

<math display="inline" id="Maximum-entropy_Markov_model:12">
 <semantics>
  <mrow>
   <msub>
    <mi>S</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>S</mi>
    <mi>n</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>S</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>S</ci>
     <ci>n</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S_{1},\dots,S_{n}
  </annotation>
 </semantics>
</math>

 can be found using a very similar <a href="Viterbi_algorithm" title="wikilink">Viterbi algorithm</a> to the one used for HMMs. The dynamic program uses the forward probability:</p>

<p>

<math display="block" id="Maximum-entropy_Markov_model:13">
 <semantics>
  <mrow>
   <msub>
    <mi>α</mi>
    <mrow>
     <mi>t</mi>
     <mo>+</mo>
     <mn>1</mn>
    </mrow>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>s</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <munder>
    <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
    <mrow>
     <msup>
      <mi>s</mi>
      <mo>′</mo>
     </msup>
     <mo>∈</mo>
     <mi>S</mi>
    </mrow>
   </munder>
   <msub>
    <mi>α</mi>
    <mi>t</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <msup>
     <mi>s</mi>
     <mo>′</mo>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
   <msub>
    <mi>P</mi>
    <msup>
     <mi>s</mi>
     <mo>′</mo>
    </msup>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>s</mi>
    <mo stretchy="false">|</mo>
    <msub>
     <mi>o</mi>
     <mrow>
      <mi>t</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>α</ci>
     <apply>
      <plus></plus>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">s</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <sum></sum>
     <apply>
      <in></in>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>s</ci>
       <ci>normal-′</ci>
      </apply>
      <ci>S</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>α</ci>
     <ci>t</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-′</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>P</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>s</ci>
      <ci>normal-′</ci>
     </apply>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">s</csymbol>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>o</ci>
      <apply>
       <plus></plus>
       <ci>t</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-.</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha_{t+1}(s)=\sum_{s^{\prime}\in S}\alpha_{t}(s^{\prime})P_{s^{\prime}}(s|o%
_{t+1}).
  </annotation>
 </semantics>
</math>

</p>
<h2 id="strengths-and-weaknesses">Strengths and weaknesses</h2>

<p>An advantage of MEMMs rather than HMMs for sequence tagging is that they offer increased freedom in choosing features to represent observations. In sequence tagging situations, it is useful to use domain knowledge to design special-purpose features. In the original paper introducing MEMMs, the authors write that "when trying to extract previously unseen company names from a newswire article, the identity of a word alone is not very predictive; however, knowing that the word is capitalized, that is a noun, that it is used in an appositive, and that it appears near the top of the article would all be quite predictive (in conjunction with the context provided by the state-transition structure)."<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> Useful sequence tagging features, such as these, are often non-independent. Maximum entropy models do not assume independence between features, but generative observation models used in HMMs do.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> Therefore, MEMMs allow the user to specify lots of correlated, but informative features.</p>

<p>Another advantage of MEMMs versus HMMs and <a href="conditional_random_field" title="wikilink">conditional random fields</a> (CRFs) is that training can be considerably more efficient. In HMMs and CRFs, one needs to use some version of the <a href="forward–backward_algorithm" title="wikilink">forward–backward algorithm</a> as an inner loop in training. However, in MEMMs, estimating the parameters of the maximum-entropy distributions used for the transition probabilities can be done for each transition distribution in isolation.</p>

<p>A drawback of MEMMs is that they potentially suffer from the "label bias problem," where states with low-entropy transition distributions "effectively ignore their observations." Conditional random fields were designed to overcome this weakness,<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> which had already been recognised in the context of neural network-based Markov models in the early 1990s.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a><a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> Another source of label bias is that training is always done with respect to known previous tags, so the model struggles at test time when there is uncertainty in the previous tag.</p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Markov_models" title="wikilink">Category:Markov models</a> <a href="Category:Statistical_natural_language_processing" title="wikilink">Category:Statistical natural language processing</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"></li>
<li id="fn6"></li>
<li id="fn7"></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"></li>
</ol>
</section>
</body>
</html>
