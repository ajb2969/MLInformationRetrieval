   Backtracking line search      Backtracking line search   In (unconstrained) minimization , a backtracking line search , a search scheme based on the Armijo–Goldstein condition , is a line search method to determine the maximum amount to move along a given search direction. It involves starting with a relatively large estimate of the step size for movement along the search direction, and iteratively shrinking the step size (i.e., "backtracking") until a decrease of the objective function is observed that adequately corresponds to the decrease that is expected, based on the local gradient of the objective function.  Motivation  Given a starting position   𝐱   𝐱   \mathbf{x}   and a search direction   𝐩   𝐩   \mathbf{p}   , the task of a line search is to determine a step size   α   α   \alpha   that adequately reduces the objective function    f  :    ℝ  n   →  ℝ      normal-:  f   normal-→   superscript  ℝ  n   ℝ     f:\mathbb{R}^{n}\to\mathbb{R}   (assumed smooth ), i.e., to find a value of   α   α   \alpha   that reduces    f   (   𝐱  +    α   𝐩    )       f    𝐱    α  𝐩      f(\mathbf{x}+\alpha\,\mathbf{p})   relative to    f   (  𝐱  )       f  𝐱    f(\mathbf{x})   . However, it is usually undesirable to devote substantial resources to finding a value of   α   α   \alpha   to precisely minimize   f   f   f   . This is because the computing resources needed to find a more precise minimum along one particular direction could instead be employed to identify a better search direction. Once an improved starting point has been identified by the line search, another subsequent line search will ordinarily be performed in a new direction. The goal, then, is just to identify a value of   α   α   \alpha   that provides a reasonable amount of improvement in the objective function, rather than to find the actual minimizing value of   α   α   \alpha   .  The backtracking line search starts with a large estimate of   α   α   \alpha   and iteratively shrinks it. The shrinking continues until a value is found that is small enough to provide a decrease in the objective function that adequately matches the decrease that is expected to be achieved, based on the local function gradient      ∇  f    (  𝐱  )    .       normal-∇  f   𝐱    \nabla f(\mathbf{x})\,.     Define the local slope of the function of   α   α   \alpha   along the search direction   𝐩   𝐩   \mathbf{p}   as     m  =     𝐩  T     ∇  f    (  𝐱  )     .      m     superscript  𝐩  normal-T    normal-∇  f   𝐱     m=\mathbf{p}^{\mathrm{T}}\,\nabla f(\mathbf{x})\,.   It is assumed that   𝐩   𝐩   \mathbf{p}   is a direction in which some local decrease is possible, i.e., it is assumed that    m  <  0      m  0    m<0   .  Based on a selected control parameter     c   ∈   (  0  ,  1  )       c   0  1     c\,\in\,(0,1)   , the Armijo–Goldstein condition tests whether a step-wise movement from a current position   𝐱   𝐱   \mathbf{x}   to a modified position    𝐱  +    α   𝐩       𝐱    α  𝐩     \mathbf{x}+\alpha\,\mathbf{p}   achieves an adequately corresponding decrease in the objective function. The condition is fulfilled if      f   (   𝐱  +    α   𝐩    )    ≤    f   (  𝐱  )    +    α    c    m      .        f    𝐱    α  𝐩         f  𝐱     α  c  m      f(\mathbf{x}+\alpha\,\mathbf{p})\leq f(\mathbf{x})+\alpha\,c\,m\,.     This condition, when used appropriately as part of a line search, can ensure that the step size is not excessively large. However, this condition is not sufficient on its own to ensure that the step size is nearly optimal, since any value of   α   α   \displaystyle\alpha   that is sufficiently small will satisfy the condition.  Thus, the backtracking line search strategy starts with a relatively large step size, and repeatedly shrinks it by a factor     τ   ∈   (  0  ,  1  )       τ   0  1     \tau\,\in\,(0,1)   until the Armijo–Goldstein condition is fulfilled.  The search will terminate after a finite number of steps for any positive values of   c   c   c   and   τ   τ   \tau   that are less than 1. For example, Armijo used  for both   c   c   c   and   τ   τ   \tau   in a paper he published in 1966.  Algorithm  Starting with a maximum candidate step size value     α  0   >   0        subscript  α  0   0    \alpha_{0}>0\,   , using search control parameters     τ   ∈   (  0  ,  1  )       τ   0  1     \tau\,\in\,(0,1)   and     c   ∈   (  0  ,  1  )       c   0  1     c\,\in\,(0,1)   , the backtracking line search algorithm can be expressed as follows:   Set    t  =   -    c   m        t      c  m      t=-c\,m   and iteration counter     j   =  0      j  0    j\,=\,0   .  Until the condition is satisfied that       f   (  𝐱  )    -   f   (   𝐱  +     α  j    𝐩    )     ≥     α  j    t    ,          f  𝐱     f    𝐱     subscript  α  j   𝐩         subscript  α  j   t     f(\mathbf{x})-f(\mathbf{x}+\alpha_{j}\,\mathbf{p})\geq\alpha_{j}\,t,   repeatedly increment   j   j   j   and set      α  j   =    τ     α   j  -  1       .       subscript  α  j     τ   subscript  α    j  1       \alpha_{j}=\tau\,\alpha_{j-1}\,.     Return    α  j     subscript  α  j    \alpha_{j}   as the solution.   In other words, reduce    α  0     subscript  α  0    \alpha_{0}   by a factor of    τ    τ   \tau\,   in each iteration until the Armijo–Goldstein condition is fulfilled.  References       "  Category:Mathematical optimization   