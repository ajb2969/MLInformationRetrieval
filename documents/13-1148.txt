   Mean dependence      Mean dependence   In probability theory , a random variable  Y is said to be mean independent of random variable X if and only if E( Y | X ) = E( Y ) for all x such that ƒ 1 ( x ) is not equal to zero. Y is said to be mean dependent if E ( Y | X ) ≠ μ ( y ) for some x such that ƒ 1 ( x ) is not equal to zero.  According to  and , Stochastic independence implies mean independence, but the converse is not necessarily true.  Moreover, mean independence implies uncorrelation while the converse is not necessarily true.  The concept of mean independence is often used in econometrics to have a middle ground between the strong assumption of independent distributions     f   X  1    ⟂   f   X  2       perpendicular-to   subscript  f   subscript  X  1     subscript  f   subscript  X  2      f_{X_{1}}\perp{}f_{X_{2}}   and the weak assumption of uncorrelated variables     Cov   (   X  1   ,   X  2   )    =  0        Cov    subscript  X  1    subscript  X  2     0    \text{Cov}(X_{1},X_{2})=0   of a pair of random variables    X  1     subscript  X  1    X_{1}   and    X  2     subscript  X  2    X_{2}   .  If X , Y are two different random variables such that X is mean independent of Y and Z=f(X) , which means that Z is a function only of X , then Y and Z are mean independent.  References      "  Category:Probability theory  Category:Statistical terminology   