   Extensions of Fisher's method      Extensions of Fisher's method   In statistics , extensions of Fisher's method are a group of approaches that allow approximately valid statistical inferences to be made when the assumptions required for the direct application of Fisher's method are not valid. Fisher's method is a way of combining the information in the p-values from different statistical tests so as to form a single overall test: this method requires that the individual test statistics (or, more immediately, their resulting p-values) should be statistically independent .  Dependent statistics  A principle limitation of Fisher's method is its exclusive design to combine independent p-values, which renders it an unreliable technique to combine dependent p-values. To overcome this limitation, a number of methods were developed to extend its utility.  Known covariance  Brown's method  Fisher's method showed that the log-sum of k independent p-values follow a χ 2 -distribution with 2 k degrees of freedom: 1 2       X  =   -   2    ∑   i  =  1   k     log  e    (   p  i   )       ∼    χ  2    (   2  k   )     .        X      2    superscript   subscript     i  1    k     subscript   e    subscript  p  i         similar-to       superscript  χ  2     2  k       X=-2\sum_{i=1}^{k}\log_{e}(p_{i})\sim\chi^{2}(2k).     In the case that these p-values are not independent, Brown proposed the idea of approximating X using a scaled χ 2 -distribution, cχ 2 ( k’ ), with k’ degrees of freedom.  The mean and variance of this scaled χ 2 variable are:        E   [   c   χ  2    (   k  ′   )    ]    =   c   k  ′     ,       normal-E    c   superscript  χ  2    superscript  k  normal-′       c   superscript  k  normal-′      \operatorname{E}[c\chi^{2}(k^{\prime})]=ck^{\prime},           Var   [   c   χ  2    (   k  ′   )    ]    =   2   c  2    k  ′     .       Var    c   superscript  χ  2    superscript  k  normal-′       2   superscript  c  2    superscript  k  normal-′      \operatorname{Var}[c\chi^{2}(k^{\prime})]=2c^{2}k^{\prime}.     This approximation is shown to be accurate up to two moments.  Unknown covariance  Kost's method: t approximation  It should be noted that the method does require the test statistics' covariance structure to be known up to a scalar multiplicative constant. See reference 2.  References  "  Category:Multiple comparisons     ↩  ↩     