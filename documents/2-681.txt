   Skew-symmetric matrix      Skew-symmetric matrix   In mathematics, and in particular linear algebra , a skew-symmetric (or antisymmetric or antimetric 1 ) matrix is a square matrix  A whose transpose is also its negative; that is, it satisfies the condition If the entry in the  and  is a ij , i.e. then the skew symmetric condition is For example, the following matrix is skew-symmetric:       [     0    2     -  1        -  2     0     -  4       1    4    0     ]   .      0  2    1       2   0    4     1  4  0     \begin{bmatrix}0&2&-1\\
 -2&0&-4\\
 1&4&0\end{bmatrix}.     Properties  We assume that the underlying field is not of characteristic 2: that is, that  where 1 denotes the multiplicative identity and 0 the additive identity of the given field. Otherwise, a skew-symmetric matrix is just the same thing as a symmetric matrix .  Sums and scalar multiples of skew-symmetric matrices are again skew-symmetric. Hence, the skew-symmetric matrices form a vector space . Its dimension is n ( n ‚àí1)/2.  Let Mat n denote the space of  matrices. A skew-symmetric matrix is determined by n ( n ‚àí¬†1)/2 scalars (the number of entries above the main diagonal ); a symmetric matrix is determined by n ( n +¬†1)/2 scalars (the number of entries on or above the main diagonal). If Skew n denotes the space of  skew-symmetric matrices and Sym n denotes the space of  symmetric matrices and then since and }, i.e.        Mat  n   =    Skew  n   ‚äï   Sym  n     ,       subscript  Mat  n    direct-sum   subscript  Skew  n    subscript  Sym  n      \mbox{Mat}_{n}=\mbox{Skew}_{n}\oplus\mbox{Sym}_{n},   where ‚äï denotes the direct sum . Let then       A  =     1  2    (   A  -   A  ùñ≥    )    +    1  2    (   A  +   A  ùñ≥    )      .      A        1  2     A   superscript  A  ùñ≥         1  2     A   superscript  A  ùñ≥        A=\frac{1}{2}(A-A^{\mathsf{T}})+\frac{1}{2}(A+A^{\mathsf{T}}).   Notice that and This is true for every square matrix  A with entries from any field whose characteristic is different from 2.  Denote with    ‚ü®  ‚ãÖ  ,  ‚ãÖ  ‚ü©     normal-‚ãÖ  normal-‚ãÖ    \langle\cdot,\cdot\rangle   the standard inner product on R n . The real n -by- n matrix A is skew-symmetric if and only if         ‚ü®   A  x   ,  y  ‚ü©   =    -   ‚ü®  x  ,   A  y   ‚ü©     ‚àÄ  x     ,   y  ‚àà   ‚Ñù  n     .     formulae-sequence       A  x   y       x    A  y      for-all  x       y   superscript  ‚Ñù  n      \langle Ax,y\rangle=-\langle x,Ay\rangle\quad\forall x,y\in\mathbb{R}^{n}.   This is also equivalent to     ‚ü®  x  ,   A  x   ‚ü©   =  0       x    A  x    0    \langle x,Ax\rangle=0   for all x (one implication being obvious, the other a plain consequence of     ‚ü®   x  +  y   ,   A   (   x  +  y   )    ‚ü©   =  0         x  y     A    x  y     0    \langle x+y,A(x+y)\rangle=0   for all x and y). Since this definition is independent of the choice of basis , skew-symmetry is a property that depends only on the linear operator A and a choice of inner product .  All main diagonal entries of a skew-symmetric matrix must be zero, so the trace is zero. If is skew-symmetric, ; hence  3x3 skew symmetric matrices can be used to represent cross products as matrix multiplications.  Determinant  Let A be a n √ó n skew-symmetric matrix. The determinant of A satisfies   det( A ) = det( A T ) = det(‚àí A ) = (‚àí1) n det( A ).   In particular, if n is odd, and since the underlying field is not of characteristic 2, the determinant vanishes. This result is called Jacobi's theorem , after Carl Gustav Jacobi (Eves, 1980).  The even-dimensional case is more interesting. It turns out that the determinant of A for n even can be written as the square of a polynomial in the entries of A , which was first proved by Cayley: 2   det( A ) = Pf( A ) 2 .   This polynomial is called the Pfaffian of A and is denoted Pf( A ). Thus the determinant of a real skew-symmetric matrix is always non-negative. However this last fact can be proved in an elementary way as follows: the eigenvalues of a real skew-symmetric matrix are purely imaginary (see below) and to every eigenvalue there corresponds the conjugate eigenvalue with the same multiplicity; therefore, as the determinant is the product of the eigenvalues, each one repeated according to its multiplicity, it follows at once that the determinant, if it is not 0, is a positive real number.  The number of distinct terms s ( n ) in the expansion of the determinant of a skew-symmetric matrix of order n has been considered already by Cayley, Sylvester, and Pfaff. Due to cancellations, this number is quite small as compared the number of terms of a generic matrix of order n , which is n !. The sequence s ( n )  is   1, 0, 1, 0, 6, 0, 120, 0, 5250, 0, 395010, 0, ‚Ä¶   and it is encoded in the exponential generating function         ‚àë   n  =  0   ‚àû      s   (  n  )     n  !     x  n     =     (   1  -   x  2    )    -   1  4      exp   (    x  2   4   )      .        superscript   subscript     n  0            s  n     n     superscript  x  n        superscript    1   superscript  x  2        1  4          superscript  x  2   4       \sum_{n=0}^{\infty}\frac{s(n)}{n!}x^{n}=(1-x^{2})^{-\frac{1}{4}}\exp\left(%
 \frac{x^{2}}{4}\right).   The latter yields to the asymptotics (for n even)        s   (  n  )    =    œÄ   -   1  2      2   3  4    Œì   (   3  /  4   )     (   n  /  e   )    n  -   1  4      (   1  +   O   (   1  n   )     )     .        s  n      superscript  œÄ      1  2      superscript  2    3  4    normal-Œì    3  4    superscript    n  e     n    1  4       1    O    1  n        s(n)=\pi^{-\frac{1}{2}}2^{\frac{3}{4}}\Gamma\left(3/4\right)(n/e)^{n-\frac{1}{%
 4}}\left(1+O\big(\frac{1}{n}\big)\right).     The number of positive and negative terms are approximatively a half of the total, although their difference takes larger and larger positive and negative values as n increases .  Spectral theory  Since a matrix is similar to its own transpose, they must have the same eigenvalues. It follows that the eigenvalues of a skew-symmetric matrix always come in pairs ¬±Œª (except in the odd-dimensional case where there is an additional unpaired 0 eigenvalue). From the spectral theorem , for a real skew-symmetric matrix the nonzero eigenvalues are all pure imaginary and thus are of the form i Œª 1 , ‚àí i Œª 1 , i Œª 2 , ‚àí i Œª 2 , ‚Ä¶ where each of the Œª k are real.  Real skew-symmetric matrices are normal matrices (they commute with their adjoints ) and are thus subject to the spectral theorem , which states that any real skew-symmetric matrix can be diagonalized by a unitary matrix . Since the eigenvalues of a real skew-symmetric matrix are imaginary it is not possible to diagonalize one by a real matrix. However, it is possible to bring every skew-symmetric matrix to a block diagonal form by a special orthogonal transformation . 3 Specifically, every 2 n √ó¬†2 n real skew-symmetric matrix can be written in the form A = Q Œ£ Q T where Q is orthogonal and      Œ£  =   [        0     Œª  1        -   Œª  1      0       0    ‚ãØ    0      0       0     Œª  2        -   Œª  2      0          0      ‚ãÆ       ‚ã±    ‚ãÆ      0    0    ‚ãØ       0     Œª  r        -   Œª  r      0                        0         ‚ã±            0        ]       normal-Œ£      0   subscript  Œª  1        subscript  Œª  1    0    0  normal-‚ãØ  0    0    0   subscript  Œª  2        subscript  Œª  2    0    absent  0    normal-‚ãÆ  absent  normal-‚ã±  normal-‚ãÆ    0  0  normal-‚ãØ    0   subscript  Œª  r        subscript  Œª  r    0      absent  absent  absent  absent    0    absent  normal-‚ã±    absent  absent  0        \Sigma=\begin{bmatrix}\begin{matrix}0&\lambda_{1}\\
 -\lambda_{1}&0\end{matrix}&0&\cdots&0\\
 0&\begin{matrix}0&\lambda_{2}\\
 -\lambda_{2}&0\end{matrix}&&0\\
 \vdots&&\ddots&\vdots\\
 0&0&\cdots&\begin{matrix}0&\lambda_{r}\\
 -\lambda_{r}&0\end{matrix}\\
 &&&&\begin{matrix}0\\
 &\ddots\\
 &&0\end{matrix}\end{bmatrix}   for real Œª k . The nonzero eigenvalues of this matrix are ¬± i Œª k . In the odd-dimensional case Œ£ always has at least one row and column of zeros.  More generally, every complex skew-symmetric matrix can be written in the form A = U Œ£ U T where U is unitary and Œ£ has the block-diagonal form given above with complex Œª k . This is an example of the Youla decomposition of a complex square matrix. 4  Skew-symmetric and alternating forms  A skew-symmetric form  œÜ on a vector space  V over a field  K of arbitrary characteristic is defined to be a bilinear form   œÜ : V √ó V ‚Üí K    such that for all v , w in V ,   œÜ ( v , w ) = ‚àí œÜ ( w , v ).   This defines a form with desirable properties for vector spaces over fields of characteristic not equal to 2, but in a vector space over a field of characteristic 2, the definition is equivalent to that of a symmetric form, as every element is its own additive inverse.  Where the vector space  V is over a field of arbitrary characteristic including characteristic 2, we may define an alternating form as a bilinear form œÜ such that for all vectors v in V   œÜ ( v , v ) = 0.   This is equivalent to a skew-symmetric form when the field is not of characteristic 2 as seen from   0 = œÜ ( v + w , v + w ) = œÜ ( v , v ) + œÜ ( v , w ) + œÜ ( w , v ) + œÜ ( w , w ) = œÜ ( v , w ) + œÜ ( w , v ),   whence,   œÜ ( v , w ) = ‚àí œÜ ( w , v ).   A bilinear form œÜ will be represented by a matrix A such that œÜ ( v , w ) = v T Aw , once a basis of V is chosen, and conversely an n √ó n matrix A on K n gives rise to a form sending ( v , w ) to v T Aw . For each of symmetric, skew-symmetric and alternating forms, the representing matrices are symmetric, skew-symmetric and alternating respectively.  Infinitesimal rotations  Skew-symmetric matrices over the field of real numbers form the tangent space to the real orthogonal group O( n ) at the identity matrix; formally, the special orthogonal Lie algebra . In this sense, then, skew-symmetric matrices can be thought of as infinitesimal rotations .  Another way of saying this is that the space of skew-symmetric matrices forms the Lie algebra o( n ) of the Lie group O( n ). The Lie bracket on this space is given by the commutator :        [  A  ,  B  ]   =    A  B   -   B  A     .       A  B       A  B     B  A      [A,B]=AB-BA.\,     It is easy to check that the commutator of two skew-symmetric matrices is again skew-symmetric:         [  A  ,  B  ]   ùñ≥   =     B  ùñ≥    A  ùñ≥    -    A  ùñ≥    B  ùñ≥     =    B  A   -   A  B    =   -   [  A  ,  B  ]     .         superscript   A  B   ùñ≥        superscript  B  ùñ≥    superscript  A  ùñ≥       superscript  A  ùñ≥    superscript  B  ùñ≥              B  A     A  B            A  B       [A,B]^{\mathsf{T}}=B^{\mathsf{T}}A^{\mathsf{T}}-A^{\mathsf{T}}B^{\mathsf{T}}=%
 BA-AB=-[A,B]\,.     The matrix exponential of a skew-symmetric matrix A is then an orthogonal matrix  R :       R  =   exp   (  A  )    =    ‚àë   n  =  0   ‚àû     A  n    n  !      .        R    A          superscript   subscript     n  0         superscript  A  n     n        R=\exp(A)=\sum_{n=0}^{\infty}\frac{A^{n}}{n!}.     The image of the exponential map of a Lie algebra always lies in the connected component of the Lie group that contains the identity element. In the case of the Lie group O( n ), this connected component is the special orthogonal group SO( n ), consisting of all orthogonal matrices with determinant 1. So R = exp( A ) will have determinant +1. Moreover, since the exponential map of a connected compact Lie group is always surjective, it turns out that every orthogonal matrix with unit determinant can be written as the exponential of some skew-symmetric matrix. In the particular important case of dimension n =2, the exponential representation for an orthogonal matrix reduces to the well-known polar form of a complex number of unit modulus. Indeed, if n=2, a special orthogonal matrix has the form       [     a     -  b       b     a      ]   ,      a    b     b  a     \begin{bmatrix}a&-b\\
 b&\,a\end{bmatrix},   with a 2 +b 2 =1. Therefore, putting a =cos Œ∏ and b =sin Œ∏ , it can be written        [       cos   Œ∏      -    sin   Œ∏          sin   Œ∏       cos   Œ∏      ]   =   exp   (   Œ∏   [     0     -  1       1    0     ]    )     ,          Œ∏       Œ∏        Œ∏     Œ∏         Œ∏    0    1     1  0        \begin{bmatrix}\cos\,\theta&-\sin\,\theta\\
 \sin\,\theta&\,\cos\,\theta\end{bmatrix}=\exp\left(\theta\begin{bmatrix}0&-1\\
 1&\,0\end{bmatrix}\right),   which corresponds exactly to the polar form cos Œ∏ + i sin Œ∏ = e iŒ∏ of a complex number of unit modulus.  The exponential representation of an orthogonal matrix of order n can also be obtained starting from the fact that in dimension n any special orthogonal matrix R can be written as R = Q S Q T , where Q is orthogonal and S is a block diagonal matrix with    ‚åä   n  /  2   ‚åã        n  2     \scriptstyle\lfloor{n/2}\rfloor   blocks of order 2, plus one of order 1 if n is odd; since each single block of order 2 is also an orthogonal matrix, it admits an exponential form. Correspondingly, the matrix S writes as exponential of a skew-symmetric block matrix Œ£ of the form above, S=exp(Œ£), so that R = Q exp(Œ£)Q T = exp(Q Œ£ Q T ), exponential of the skew-symmetric matrix Q Œ£ Q T . Conversely, the surjectivity of the exponential map, together with the above-mentioned block-diagonalization for skew-symmetric matrices, implies the block-diagonalization for orthogonal matrices.  Coordinate-free  More intrinsically (i.e., without using coordinates), skew-symmetric linear transformations on a vector space V with an inner product may be defined as the bivectors on the space, which are sums of simple bivectors ( 2-blades )    v  ‚àß  w      v  w    v\wedge w   . The correspondence is given by the map      v  ‚àß  w   ‚Ü¶     v  *   ‚äó  w   -    w  *   ‚äó  v     ,     maps-to    v  w      tensor-product   superscript  v    w    tensor-product   superscript  w    v      v\wedge w\mapsto v^{*}\otimes w-w^{*}\otimes v,   where    v  *     superscript  v     v^{*}   is the covector dual to the vector   v   v   v   ; in orthonormal coordinates these are exactly the elementary skew-symmetric matrices. This characterization is used in interpreting the curl of a vector field (naturally a 2-vector) as an infinitesimal rotation or "curl", hence the name.  Skew-symmetrizable matrix  An n -by- n matrix A is said to be skew-symmetrizable if there exist an invertible diagonal matrix  D and skew-symmetric matrix S such that  For real  n -by- n matrices, sometimes the condition for D to have positive entries is added. 5  See also   Symmetric matrix  Skew-Hermitian matrix  Symplectic matrix  Symmetry in mathematics   References  Further reading       External links      Fortran  Fortran90   "  Category:Matrices     ‚Ü©  Reprintend in ‚Ü©  Voronov, Theodore. "Pfaffian." Concise Encyclopedia of Supersymmetry. Springer Netherlands, 2003. 298-298. ‚Ü©  ‚Ü©  ‚Ü©     