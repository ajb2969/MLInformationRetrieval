<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1657">Global optimization</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Global optimization</h1>
<hr/>

<p><strong>Global optimization</strong> is a branch of <a href="applied_mathematics" title="wikilink">applied mathematics</a> and <a href="numerical_analysis" title="wikilink">numerical analysis</a> that deals with the global <a href="optimization_(mathematics)" title="wikilink">optimization</a> of a <a href="function_(mathematics)" title="wikilink">function</a> or a <a href="Set_(mathematics)" title="wikilink">set</a> of functions according to some criteria. Typically, a set of bound and more general constraints is also present, and the decision variables are optimized considering also the constraints.</p>

<p>Global optimization is distinguished from regular <a href="optimization_(mathematics)" title="wikilink">optimization</a> by its focus on finding the maximum or minimum over all input values, as opposed to finding <em>local</em> minima or maxima.</p>
<h2 id="general">General</h2>

<p>A common (standard) model form is the <a href="maxima_and_minima" title="wikilink">minimization</a> of one <a href="real_number" title="wikilink">real</a>-valued function 

<math display="inline" id="Global_optimization:0">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 in the parameter-space 

<math display="inline" id="Global_optimization:1">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>x</mi>
    <mo stretchy="false">→</mo>
   </mover>
   <mo>∈</mo>
   <mi>P</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <ci>normal-→</ci>
     <ci>x</ci>
    </apply>
    <ci>P</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{x}\in P
  </annotation>
 </semantics>
</math>

, or its specified subset 

<math display="inline" id="Global_optimization:2">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>x</mi>
    <mo stretchy="false">→</mo>
   </mover>
   <mo>∈</mo>
   <mi>D</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <ci>normal-→</ci>
     <ci>x</ci>
    </apply>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{x}\in D
  </annotation>
 </semantics>
</math>

: here 

<math display="inline" id="Global_optimization:3">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D
  </annotation>
 </semantics>
</math>

 denotes the set defined by the constraints.</p>

<p>(The maximization of a real-valued function 

<math display="inline" id="Global_optimization:4">
 <semantics>
  <mrow>
   <mi>g</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>g</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g(x)
  </annotation>
 </semantics>
</math>


 is equivalent to the minimization of the function 

<math display="inline" id="Global_optimization:5">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>:=</mo>
   <mrow>
    <mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mo>-</mo>
       <mn>1</mn>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo>⋅</mo>
     <mi>g</mi>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">assign</csymbol>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <ci>normal-⋅</ci>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
      </apply>
      <ci>g</ci>
     </apply>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x):=(-1)\cdot g(x)
  </annotation>
 </semantics>
</math>

.)</p>

<p>In many nonlinear optimization problems, the objective function 

<math display="inline" id="Global_optimization:6">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 has a large number of <em>local</em> minima and maxima. Finding an arbitrary local optimum is relatively straightforward by using classical <em>local optimization</em> methods. Finding the global minimum (or maximum) of a function is far more difficult: symbolic (analytical) methods are frequently not applicable, and the use of numerical solution strategies often leads to very hard challenges.</p>
<h2 id="applications">Applications</h2>

<p>Typical examples of global optimization applications include:</p>
<ul>
<li><a href="Protein_structure_prediction" title="wikilink">Protein structure prediction</a> (minimize the energy/free energy function)</li>
<li><a href="Computational_phylogenetics" title="wikilink">Computational phylogenetics</a> (e.g., minimize the number of character transformations in the tree)</li>
<li><a href="Traveling_salesman_problem" title="wikilink">Traveling salesman problem</a> and electrical circuit design (minimize the path length)</li>
<li><a href="Chemical_engineering" title="wikilink">Chemical engineering</a> (e.g., analyzing the <a href="Gibbs_free_energy" title="wikilink">Gibbs free energy</a>)</li>
<li>Safety verification, <a href="safety_engineering" title="wikilink">safety engineering</a> (e.g., of mechanical structures, buildings)</li>
<li><a href="Worst_case" title="wikilink">Worst-case analysis</a></li>
<li>Mathematical problems (e.g., the <a href="Kepler_conjecture" title="wikilink">Kepler conjecture</a>)</li>
<li>Object packing (configuration design) problems</li>
<li>The starting point of several <a href="molecular_dynamics" title="wikilink">molecular dynamics</a> simulations consists of an initial optimization of the energy of the system to be simulated.</li>
<li><a href="Spin_glass" title="wikilink">Spin glasses</a></li>
<li>Calibration of <a href="radio_propagation_models" title="wikilink">radio propagation models</a> and of many other models in the sciences and engineering</li>
<li><a href="Curve_fitting" title="wikilink">Curve fitting</a> like <a href="non-linear_least_squares" title="wikilink">non-linear least squares</a> analysis and other generalizations, used in fitting model parameters to experimental data in chemistry, physics, economics, finance, medicine, astronomy, engineering.</li>
</ul>
<h2 id="deterministic-methods">Deterministic methods</h2>

<p>The most successful general strategies are:</p>
<h3 id="inner-and-outer-approximation">Inner and outer approximation</h3>

<p>In both of these strategies, set over which a function is to be optimized is approximated by polyhedra. In inner approximation, the polyhedra are contained in the set, while in outer approximation, the polyhedra contain the set.</p>
<h3 id="cutting-plane-methods">Cutting plane methods</h3>

<p>The <strong>cutting-plane method</strong> is an umbrella term for optimization methods which iteratively refine a <a href="feasible_set" title="wikilink">feasible set</a> or objective function by means of linear inequalities, termed <em>cuts</em>. Such procedures are popularly used to find <a class="uri" href="integer" title="wikilink">integer</a> solutions to <a href="mixed_integer_linear_programming" title="wikilink">mixed integer linear programming</a> (MILP) problems, as well as to solve general, not necessarily differentiable <a href="convex_optimization" title="wikilink">convex optimization</a> problems. The use of cutting planes to solve MILP was introduced by <a href="Ralph_E._Gomory" title="wikilink">Ralph E. Gomory</a> and <a href="Václav_Chvátal" title="wikilink">Václav Chvátal</a>.</p>
<h3 id="branch-and-bound-methods">Branch and bound methods</h3>

<p><strong>Branch and bound</strong> (<strong>BB</strong> or <strong>B&amp;B;</strong>) is an <a class="uri" href="algorithm" title="wikilink">algorithm</a> design paradigm for <a href="discrete_optimization" title="wikilink">discrete</a> and <a href="combinatorial_optimization" title="wikilink">combinatorial optimization</a> problems. A branch-and-bound algorithm consists of a systematic enumeration of candidate solutions by means of <a href="state_space_search" title="wikilink">state space search</a>: the set of candidate solutions is thought of as forming a <a href="Tree_(graph_theory)" title="wikilink">rooted tree</a> with the full set at the root. The algorithm explores <em>branches</em> of this tree, which represent subsets of the solution set. Before enumerating the candidate solutions of a branch, the branch is checked against upper and lower estimated <em>bounds</em> on the optimal solution, and is discarded if it cannot produce a better solution than the best one found so far by the algorithm.</p>
<h3 id="interval-methods">Interval methods</h3>

<p><strong>Interval arithmetic</strong>, <strong>interval mathematics</strong>, <strong>interval analysis</strong>, or <strong>interval computation</strong>, is a method developed by mathematicians since the 1950s and 1960s as an approach to putting bounds on <a href="rounding_error" title="wikilink">rounding errors</a> and <a href="measurement_error" title="wikilink">measurement errors</a> in <a href="numerical_analysis" title="wikilink">mathematical computation</a> and thus developing <a href="numerical_methods" title="wikilink">numerical methods</a> that yield reliable results. Interval arithmetic helps find reliable and guaranteed solutions to equations and optimization problems.</p>

<p>(see interalg from <a class="uri" href="OpenOpt" title="wikilink">OpenOpt</a> and GlobSol)</p>
<h3 id="methods-based-on-real-algebraic-geometry">Methods based on real algebraic geometry</h3>

<p><strong>Real algebra</strong> is the part of algebra which is relevant to real algebraic (and semialgebraic) geometry. It is mostly concerned with the study of <a href="ordered_field" title="wikilink">ordered fields</a> and <a href="ordered_ring" title="wikilink">ordered rings</a> (in particular <a href="real_closed_field" title="wikilink">real closed fields</a>) and their applications to the study of <a href="positive_polynomial" title="wikilink">positive polynomials</a> and <a href="Polynomial_SOS" title="wikilink">sums-of-squares of polynomials</a>. It can be used in <a href="convex_optimization" title="wikilink">convex optimization</a></p>
<h2 id="stochastic-methods">Stochastic methods</h2>

<p>Several Monte-Carlo-based algorithms exist:</p>
<h3 id="simulated-annealing">Simulated annealing</h3>

<p><em>Simulated annealing (SA)</em>' is a generic <a href="probabilistic_algorithm" title="wikilink">probabilistic</a> <a class="uri" href="metaheuristic" title="wikilink">metaheuristic</a> for the global optimization problem of locating a good approximation to the <a href="global_optimum" title="wikilink">global optimum</a> of a given <a href="function_(mathematics)" title="wikilink">function</a> in a large <a href="Mathematical_optimization#Optimization_problems" title="wikilink">search space</a>. It is often used when the search space is discrete (e.g., all tours that visit a given set of cities). For certain problems, simulated annealing may be more efficient than <a href="brute_force_search" title="wikilink">exhaustive enumeration</a> — provided that the goal is merely to find an acceptably good solution in a fixed amount of time, rather than the best possible solution.</p>
<h3 id="direct-monte-carlo-sampling">Direct Monte-Carlo sampling</h3>

<p>In this method, random simulations are used to find an approximate solution.</p>

<p>Example: The <a href="traveling_salesman_problem" title="wikilink">traveling salesman problem</a> is what is called a conventional optimization problem. That is, all the facts (distances between each destination point) needed to determine the optimal path to follow are known with certainty and the goal is to run through the possible travel choices to come up with the one with the lowest total distance. However, let's assume that instead of wanting to minimize the total distance traveled to visit each desired destination, we wanted to minimize the total time needed to reach each destination. This goes beyond conventional optimization since travel time is inherently uncertain (traffic jams, time of day, etc.). As a result, to determine our optimal path we would want to use simulation - optimization to first understand the range of potential times it could take to go from one point to another (represented by a probability distribution in this case rather than a specific distance) and then optimize our travel decisions to identify the best path to follow taking that uncertainty into account.</p>
<h3 id="stochastic-tunneling">Stochastic tunneling</h3>

<p><strong>Stochastic tunneling</strong> (STUN) is an approach to <a href="global_optimization" title="wikilink">global optimization</a> based on the <a href="Monte_Carlo_method" title="wikilink">Monte Carlo method</a>-<a href="Sampling_(signal_processing)" title="wikilink">sampling</a> of the function to be objective minimized in which the function is nonlinearly transformed to allow for easier tunneling among regions containing function minima. Easier tunneling allows for faster exploration of sample space and faster convergence to a good solution.</p>
<h3 id="parallel-tempering">Parallel tempering</h3>

<p><strong>Parallel tempering</strong>, also known as <strong>replica exchange MCMC sampling</strong>, is a <a class="uri" href="simulation" title="wikilink">simulation</a> method aimed at improving the dynamic properties of <a href="Monte_Carlo_method" title="wikilink">Monte Carlo method</a> simulations of physical systems, and of <a href="Markov_chain_Monte_Carlo" title="wikilink">Markov chain Monte Carlo</a> (MCMC) sampling methods more generally. The replica exchange method was originally devised by Swendsen,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> then extended by Geyer<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> and later developed, among others, by <a href="Giorgio_Parisi" title="wikilink">Giorgio Parisi</a>.,<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> Sugita and Okamoto formulated a <a href="molecular_dynamics" title="wikilink">molecular dynamics</a> version of parallel tempering:<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> this is usually known as replica-exchange molecular dynamics or REMD.</p>

<p>Essentially, one runs <em>N</em> copies of the system, randomly initialized, at different temperatures. Then, based on the Metropolis criterion one exchanges configurations at different temperatures. The idea of this method is to make configurations at high temperatures available to the simulations at low temperatures and vice versa. This results in a very robust ensemble which is able to sample both low and high energy configurations. In this way, thermodynamical properties such as the specific heat, which is in general not well computed in the canonical ensemble, can be computed with great precision.</p>
<h2 id="heuristics-and-metaheuristics">Heuristics and metaheuristics</h2>
<dl>
<dd><em>Main page: <a class="uri" href="Metaheuristic" title="wikilink">Metaheuristic</a></em>
</dd>
</dl>

<p>Other approaches include heuristic strategies to search the search space in a more or less intelligent way, including:</p>
<ul>
<li><a href="Evolutionary_algorithm" title="wikilink">Evolutionary algorithms</a> (e.g., <a href="genetic_algorithms" title="wikilink">genetic algorithms</a> and <a href="evolution_strategies" title="wikilink">evolution strategies</a>)</li>
<li><a href="Differential_evolution" title="wikilink">Differential evolution</a>, a method that <a href="optimization_(mathematics)" title="wikilink">optimizes</a> a problem by <a href="iterative_method" title="wikilink">iteratively</a> trying to improve a <a href="candidate_solution" title="wikilink">candidate solution</a> with regard to a given measure of quality</li>
<li><a href="Swarm_intelligence" title="wikilink">Swarm-based optimization algorithms</a> (e.g., <a href="particle_swarm_optimization" title="wikilink">particle swarm optimization</a>, <a href="Multi-swarm_optimization" title="wikilink">Multi-swarm optimization</a> and <a href="ant_colony_optimization" title="wikilink">ant colony optimization</a>)</li>
<li><a href="Memetic_algorithm" title="wikilink">Memetic algorithms</a>, combining global and local search strategies</li>
<li><a href="Reactive_search_optimization" title="wikilink">Reactive search optimization</a> (i.e. integration of sub-symbolic machine learning techniques into search heuristics)</li>
<li><a href="Graduated_optimization" title="wikilink">Graduated optimization</a>, a technique that attempts to solve a difficult optimization problem by initially solving a greatly simplified problem, and progressively transforming that problem (while optimizing) until it is equivalent to the difficult optimization problem.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a><ref name="mobahi2015">Hossein Mobahi, John W. Fisher III.</ref></li>
</ul>

<p><a href="http://people.csail.mit.edu/hmobahi/pubs/gaussian_convenv_2015.pdf">On the Link Between Gaussian Homotopy Continuation and Convex Envelopes</a>, In Lecture Notes in Computer Science (EMMCVPR 2015), Springer, 2015.</p>
<h2 id="response-surface-methodology-based-approaches">Response surface methodology based approaches</h2>
<ul>
<li><a class="uri" href="IOSO" title="wikilink">IOSO</a> Indirect Optimization based on Self-Organization</li>
<li><a href="Bayesian_optimization" title="wikilink">Bayesian optimization</a>, a sequential design strategy for global <a class="uri" href="optimization" title="wikilink">optimization</a> of black-box functions using <a href="Bayesian_statistics" title="wikilink">Bayesian statistics</a><a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></li>
</ul>
<h2 id="software">Software</h2>

<p><strong>1. Free and opensource:</strong></p>
<ul>
<li><a class="uri" href="OpenOpt" title="wikilink">OpenOpt</a></li>
</ul>

<p><strong>2. Commercial:</strong></p>
<ul>
<li><a class="uri" href="LIONsolver" title="wikilink">LIONsolver</a></li>
<li><a class="uri" href="TOMLAB" title="wikilink">TOMLAB</a> for Matlab</li>
<li><a href="http://www.midaco-solver.com/">MIDACO</a> Global optimization software (Excel, Matlab, Octave, Python, C/C++, R and Fortran)</li>
<li><a href="Optimus_platform" title="wikilink">Optimus platform</a></li>
<li>The <a href="NAG_Numerical_Library" title="wikilink">NAG Numerical Library</a> contains routines for both global and local optimization.</li>
<li>Demo global optimization software versions are available also for a number of commercial software products.</li>
<li><a href="http://www.optimalcomputing.be/index.php">XTREME</a> a single and multiple objective optimization software for nonlinear optimization (GUI, Excel, C/C++ API and Python)</li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Multidisciplinary_design_optimization" title="wikilink">Multidisciplinary design optimization</a></li>
<li><a href="Multiobjective_optimization" title="wikilink">Multiobjective optimization</a></li>
<li><a href="Optimization_(mathematics)" title="wikilink">Optimization (mathematics)</a></li>
</ul>
<h2 id="footnotes">Footnotes</h2>
<h2 id="references">References</h2>

<p>Deterministic global optimization:</p>
<ul>
<li>R. Horst, H. Tuy, Global Optimization: Deterministic Approaches, Springer, 1996.</li>
<li>R. Horst, P.M. Pardalos and N.V. Thoai, Introduction to Global Optimization, Second Edition. Kluwer Academic Publishers, 2000.</li>
<li><a href="http://www.mat.univie.ac.at/~neum/ms/glopt03.pdf">A.Neumaier, Complete Search in Continuous Global Optimization and Constraint Satisfaction, pp. 271-369 in: Acta Numerica 2004 (A. Iserles, ed.), Cambridge University Press 2004.</a></li>
<li>M. Mongeau, H. Karsenty, V. Rouzé and J.-B. Hiriart-Urruty, Comparison of public-domain software for black box global optimization. Optimization Methods &amp; Software 13(3), pp. 203–226, 2000.</li>
<li>J.D. Pintér, Global Optimization in Action - Continuous and Lipschitz Optimization: Algorithms, Implementations and Applications. Kluwer Academic Publishers, Dordrecht, 1996. Now distributed by Springer Science and Business Media, New York. This book also discusses stochastic global optimization methods.</li>
<li>L. Jaulin, M. Kieffer, O. Didrit, E. Walter (2001). Applied Interval Analysis. Berlin: Springer.</li>
<li>E.R. Hansen (1992), Global Optimization using Interval Analysis, Marcel Dekker, New York.</li>
<li>R.G. Strongin, Ya.D. Sergeyev (2000) Global optimization with non-convex constraints: Sequential and parallel algorithms, Kluwer Academic Publishers, Dordrecht.</li>
<li>Ya.D. Sergeyev, R.G. Strongin, D. Lera (2013) Introduction to global optimization exploiting space-filling curves, Springer, NY.</li>
</ul>

<p>For simulated annealing:</p>
<ul>
<li>S. Kirkpatrick, C.D. Gelatt, and M.P. Vecchi. <em>Science</em>, 220:671–680, 1983.</li>
</ul>

<p>For reactive search optimization:</p>
<ul>
<li><a href="Roberto_Battiti" title="wikilink">Roberto Battiti</a>, M. Brunato and F. Mascia, Reactive Search and Intelligent Optimization, Operations Research/Computer Science Interfaces Series, Vol. 45, Springer, November 2008. ISBN 978-0-387-09623-0</li>
</ul>

<p>For stochastic methods:</p>
<ul>
<li><a href="Anatoly_Zhigljavsky" title="wikilink">A. Zhigljavsky</a>. Theory of Global Random Search. Mathematics and its applications. Kluwer Academic Publishers. 1991.</li>
<li>K. Hamacher. Adaptation in Stochastic Tunneling Global Optimization of Complex Potential Energy Landscapes, <em>Europhys.Lett.</em> 74(6):944, 2006.</li>
<li>K. Hamacher and W. Wenzel. The Scaling Behaviour of Stochastic Minimization Algorithms in a Perfect Funnel Landscape. <em>Phys. Rev. E</em>, 59(1):938-941, 1999.</li>
<li>W. Wenzel and K. Hamacher. A Stochastic tunneling approach for global minimization. <em>Phys. Rev. Lett.</em>, 82(15):3003-3007, 1999.</li>
</ul>

<p>For parallel tempering:</p>
<ul>
<li>U. H. E. Hansmann. <em>Chem.Phys.Lett.</em>, 281:140, 1997.</li>
</ul>

<p>For continuation methods:</p>
<ul>
<li>Zhijun Wu. The effective energy transformation scheme as a special continuation approach to global optimization with application to molecular conformation. Technical Report, Argonne National Lab., IL (United States), November 1996.</li>
</ul>

<p>For general considerations on the dimensionality of the domain of definition of the objective function:</p>
<ul>
<li>K. Hamacher. On Stochastic Global Optimization of one-dimensional functions. <em>Physica A</em> 354:547-557, 2005.</li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.mat.univie.ac.at/~neum/glopt.html">A. Neumaier’s page on Global Optimization</a></li>
<li><a href="http://www.lix.polytechnique.fr/~liberti/teaching/dix/inf572-09/nonconvex_optimization.pdf">Introduction to global optimization by L. Liberti</a></li>
<li><a href="http://www.it-weise.de/projects/book.pdf">Free e-book by Thomas Weise</a></li>
</ul>

<p>"</p>

<p><a href="Category:Mathematical_optimization" title="wikilink">Category:Mathematical optimization</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Swendsen RH and Wang JS (1986) Replica Monte Carlo simulation of spin glasses Physical Review Letters 57 : 2607-2609<a href="#fnref1">↩</a></li>
<li id="fn2">C. J. Geyer, (1991) in <em>Computing Science and Statistics</em>, Proceedings of the 23rd Symposium on the Interface, American Statistical Association, New York, p. 156.<a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4">David J. Earl and Michael W. Deem (2005) <a href="http://www.rsc.org/Publishing/Journals/CP/article.asp?doi=b509983h">"Parallel tempering: Theory, applications, and new perspectives"</a>, <em>Phys. Chem. Chem. Phys.</em>, 7, 3910<a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8">Jonas Mockus (2013). Bayesian approach to global optimization: theory and applications. Kluwer Academic.<a href="#fnref8">↩</a></li>
</ol>
</section>
</body>
</html>
