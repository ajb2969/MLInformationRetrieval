   Evidence-based subjective logic      Evidence-based subjective logic   Evidence-based subjective logic (EBSL) is a variant 1 of subjective logic in which the transitivity of opinions (discounting) is handled by applying weights to the evidence underlying the opinions. Subjective Logic is based on Dempster–Shafer belief theory . The discounting rule in EBSL makes it possible to handle arbitrary trust networks.  Relation between evidence and opinions  Consider a proposition P. Let p be the amount of evidence supporting P, and n the amount of evidence supporting ¬P. We write the evidence as a vector ( p , n ). Let c be a positive constant representing a "unit" of evidence. An opinion ( b , d , u ) is formed on the basis of the evidence ( p , n ). There is a one-to-one mapping between the opinion and the evidence,       (  b  ,  d  ,  u  )   =    (  p  ,  n  ,  c  )    p  +  n  +  c     (  p  ,  n  )   =  c    (  b  ,  d  )   u   .   (  1  )      fragments   fragments  normal-(  b  normal-,  d  normal-,  u  normal-)       p  n  c     p  n  c    italic-   fragments  normal-(  p  normal-,  n  normal-)    c     b  d   u   normal-.  italic-   fragments  normal-(  1  normal-)     (b,d,u)=\frac{(p,n,c)}{p+n+c}\quad\quad(p,n)=c\frac{(b,d)}{u}.\quad\quad(1)     In the original literature on subjective logic the constant was set to c = 2. The mapping (1) is the unique solution of the following set of constraints, 2   b / d = p / n .  b + d + u = 1.  p + n = 0 implies u = 1.      p  →  ∞     normal-→  p     p\to\infty   implies    u  →  0     normal-→  u  0    u\to 0   .   Alternatively, (1) can be derived from an analysis of a posteriori probability distributions 3 ( beta distributions ). There are three "corner points" in opinion space: the full Belief B = (1,0,0), the full Disbelief D = (0,1,0), and full Uncertainty U = (0,0,1). Opinions on the line between B and D (including B and D) are called "dogmatic opinions". They have zero uncertainty, which is achievable only with an infinite amount of evidence. Dogmatic opinions are often excluded from the algebra.  Consensus / fusion  The consensus operation combines two opinions about the same predicate into one opinion by piling up the evidence. Let x = ( x b , x d , x u ) and y = ( y b , y d , y u ) be the opinions that are to be fused, and z = x ⊕ y the result. We denote their evidence vectors as ( p x , n x ), ( p y , n y ) and ( p z , n z ) respectively. In evidence space the consensus is straightforwardly defined as        (   p  z   ,   n  z   )   =    (   p  x   ,   n  x   )   +   (   p  y   ,   n  y   )     .        subscript  p  z    subscript  n  z        subscript  p  x    subscript  n  x      subscript  p  y    subscript  n  y       (p_{z},n_{z})=(p_{x},n_{x})+(p_{y},n_{y}).     In opinion space this yields       x  ⊕  y   =    (    p  x   +   p  y    ,    n  x   +   n  y    ,  c  )     p  x   +   p  y   +   n  x   +   n  y   +  c         direct-sum  x  y         subscript  p  x    subscript  p  y       subscript  n  x    subscript  n  y    c      subscript  p  x    subscript  p  y    subscript  n  x    subscript  n  y   c      x\oplus y=\frac{(p_{x}+p_{y},n_{x}+n_{y},c)}{p_{x}+p_{y}+n_{x}+n_{y}+c}     which using (1) can be rewritten as      x  ⊕  y  =    (     x  u    y  b    +    y  u    x  b     ,     x  u    y  d    +    y  u    x  d     ,    x  u    y  u    )      x  u   +   y  u    -    x  u    y  u      .   (  2  )      fragments  x  direct-sum  y           subscript  x  normal-u    subscript  y  normal-b       subscript  y  normal-u    subscript  x  normal-b          subscript  x  normal-u    subscript  y  normal-d       subscript  y  normal-u    subscript  x  normal-d        subscript  x  normal-u    subscript  y  normal-u          subscript  x  normal-u    subscript  y  normal-u       subscript  x  normal-u    subscript  y  normal-u      normal-.  italic-   fragments  normal-(  2  normal-)     x\oplus y=\frac{(x_{\rm u}y_{\rm b}+y_{\rm u}x_{\rm b},x_{\rm u}y_{\rm d}+y_{%
 \rm u}x_{\rm d},x_{\rm u}y_{\rm u})}{x_{\rm u}+y_{\rm u}-x_{\rm u}y_{\rm u}}.%
 \quad\quad(2)     The consensus rule can only be applied if the evidence underlying x and y is independent, otherwise double counting of evidence occurs.  Discounting  Old discounting rule (⊗)  The traditional discounting operation in Subjective Logic is denoted as ⊗ and defined as        x  ⊗  y    =  def    (    x  b    y  b    ,    x  b    y  d    ,   1  -    x  b    y  b    -    x  b    y  d     )    .      superscript   def    tensor-product  x  y       subscript  normal-x  normal-b    subscript  normal-y  normal-b       subscript  normal-x  normal-b    subscript  normal-y  normal-d      1     subscript  normal-x  normal-b    subscript  normal-y  normal-b       subscript  normal-x  normal-b    subscript  normal-y  normal-d        x\otimes y\stackrel{\rm def}{=}(x_{\rm b}y_{\rm b},x_{\rm b}y_{\rm d},1-x_{\rm
 b%
 }y_{\rm b}-x_{\rm b}y_{\rm d}).    This operation suffers from a number of serious problems, e.g.   There is no natural interpretation of the evidence underlying x ⊗ y when (1) is applied.    Consider the following. Alice has trust x in Bob. Bob gathers two independent evidence vectors, ( p 1 , n 1 ) and ( p 2 , n 2 ), about some proposition P. Scenario I: Bob forms two indendent opinions, y 1 and y 2 , based on the evidence. He sends y 1 and y 2 to Alice. Alice forms opinion     (   x  ⊗   y  1    )   ⊕   (   x  ⊗   y  2    )      direct-sum   tensor-product  x   subscript  y  1     tensor-product  x   subscript  y  2      (x\otimes y_{1})\oplus(x\otimes y_{2})   about P. Scenario II: Bob combines his evidence and forms opinion     y  1   ⊕   y  2      direct-sum   subscript  y  1    subscript  y  2     y_{1}\oplus y_{2}   . He then sends     y  1   ⊕   y  2      direct-sum   subscript  y  1    subscript  y  2     y_{1}\oplus y_{2}   to Alice. Alice forms opinion    x  ⊗   (    y  1   ⊕   y  2    )      tensor-product  x   direct-sum   subscript  y  1    subscript  y  2      x\otimes(y_{1}\oplus y_{2})   about P. It is obvious that these scenarios should yield the same result for Alice. Yet the traditional discounting rule gives:   x\otimes(y_1\oplus y_2)\neq (x\otimes y_1)\oplus(x\otimes y_2).  These problems make it very difficult to handle trust networks in subjective logic.  The product of a scalar and an opinion  Let x = ( x b , x d , x u ) be an opinion based on evidence ( p , n ). Let λ ≥ 0 be a scalar. The product λ ⋅ x is defined 4 as (λ p , λ n ) in evidence space, which corresponds to       λ  ⋅  x   =     (   λ   x  b    ,   λ   x  d    ,   x  u   )     λ   (    x  b   +   x  d    )    +   x  u      (  3  )         normal-⋅  λ  x         λ   subscript  x  normal-b      λ   subscript  x  normal-d     subscript  x  normal-u        λ     subscript  x  normal-b    subscript  x  normal-d      subscript  x  normal-u     3     \lambda\cdot x=\frac{(\lambda x_{\rm b},\lambda x_{\rm d},x_{\rm u})}{\lambda(%
 x_{\rm b}+x_{\rm d})+x_{\rm u}}\quad\quad(3)     in opinion space.  The discounting rule (☒) in EBSL  Let x and y be opinions. Let g be a mapping from opinion space to [0,1] satisfying g (B) = 1 and g (D) = 0.  In EBSL the discounting of y through x is denoted as x ☒ y and defined as 5        x   ⊠  y    =  def      g   (  x  )    ⋅  y   ,   (  4  )        superscript   def    normal-⊠  x  y     normal-⋅    normal-g  normal-x   normal-y   4     \quad x\boxtimes y\stackrel{\rm def}{=}g(x)\cdot y,\quad\quad(4)     with the "dot" product as specified in (3).  The function g can be chosen at will, depending on the context. The ☒ rule has a very simple interpretation in evidence space: Due to the disbelief and uncertainty present in x , only a fraction g ( x ) of the evidence in y survives.  The ☒ operation avoids all the inconsistencies of the ⊗ operation. The following properties hold,        x  ⊠   (    y  1   ⊕   y  2    )    =    (   x  ⊠   y  1    )   ⊕   (   x  ⊠   y  2    )         normal-⊠  x   direct-sum   subscript  y  1    subscript  y  2      direct-sum   normal-⊠  x   subscript  y  1     normal-⊠  x   subscript  y  2       x\boxtimes(y_{1}\oplus y_{2})=(x\boxtimes y_{1})\oplus(x\boxtimes y_{2})             x  1   ⊠   (    x  2   ⊠  y   )    =    x  2   ⊠   (    x  1   ⊠  y   )         normal-⊠   subscript  x  1    normal-⊠   subscript  x  2   y     normal-⊠   subscript  x  2    normal-⊠   subscript  x  1   y      x_{1}\boxtimes(x_{2}\boxtimes y)=x_{2}\boxtimes(x_{1}\boxtimes y)   .   There is no associativity , i.e.      x  1   ⊠   (    x  2   ⊠  y   )    ≠    (    x  1   ⊠   x  2    )   ⊠  y        normal-⊠   subscript  x  1    normal-⊠   subscript  x  2   y     normal-⊠   normal-⊠   subscript  x  1    subscript  x  2    y     x_{1}\boxtimes(x_{2}\boxtimes y)\neq(x_{1}\boxtimes x_{2})\boxtimes y   , in contrast to the ⊗ operation. This is not a problem, since the flow if information in a trust network has a well defined direction.  Also, we have         (    x  1   ⊕   x  2    )   ⊠  y   ≠    (    x  1   ⊠  y   )   ⊕   (    x  2   ⊠  y   )     .       normal-⊠   direct-sum   subscript  x  1    subscript  x  2    y    direct-sum   normal-⊠   subscript  x  1   y    normal-⊠   subscript  x  2   y      (x_{1}\oplus x_{2})\boxtimes y\neq(x_{1}\boxtimes y)\oplus(x_{2}\boxtimes y).     Computation of opinions in arbitrary trust networks  EBSL makes it possible to compute trust values even when the graph connecting the users in the trust network is complicated. This makes EBSL interesting e.g. for Reputation systems  Let A ij be the opinion that user i has about the trustworthiness of user j , based on direct evidence , e.g. direct interactions between i and j . We set A ii = U. Let every user publish these direct opinions in a reliable way; the matrix A is public and its integrity is guaranteed. Based on all the available trust information, direct as well as indirect, what should a user conclude about the trustworthiness of all the other users? In general this is a nontrivial problem because of the complicated connection graphs, in which loops may occur. The problem is to find a "reputation" matrix R that consistently combines the direct and indirect evidence. In EBSL the following "self-consistent" (self-containing) equation must be satisfied 6 by R ,      For  i  ≠  j  :   R   i  j    =   A   i  j    ⊕   ∑   k  :   k  ≠  i      R   i  k    ⊠   A   k  j     (  5  )      fragments  For  i   j  normal-:    subscript  R    i  j      subscript  A    i  j    direct-sum   subscript    normal-:  k    k  i      subscript  R    i  k    normal-⊠   subscript  A    k  j    italic-   fragments  normal-(  5  normal-)     \mbox{For }i\neq j:\quad R_{ij}=A_{ij}\oplus\sum_{k:\,k\neq i}R_{ik}\boxtimes A%
 _{kj}\quad\quad(5)           Diagonal:   R   i  i     =  B   .        Diagonal:   subscript  R    i  i     B    \mbox{Diagonal: }R_{ii}=B.     Here the "Σ" stands for ⊕ operations. The diagonal is set to full belief since everybody trusts himself implicitly, independent of other users' opinions. User i forms an opinion about j by combining his direct opinion A ij with other users' opinions A kj . The indirect evidence is weighted with a scalar that depends on the reputation of the intermediary: g ( R ik ).  Equation (5) can be written compactly in matrix form,      R  =  B  𝟏  ⊕   (  R  ⊠  A  )   .   (  6  )      fragments  R   B  1  direct-sum   fragments  normal-(  R  normal-⊠  A  normal-)   normal-.  italic-   fragments  normal-(  6  normal-)     R=B{\mathbf{1}}\oplus(R\boxtimes A).\quad\quad(6)    Here   𝟏   1   {\mathbf{1}}   is the identity matrix , and summation should be executed as ⊕. Eq.(6) is a fixed point equation similar to the case of Markov chains . It can be solved recursively . Let X be a square matrix with the same dimensions as A . Define a function f as       f   (  X  )    =    B  𝟏   ⊕   (   X  ⊠  A   )          f  X    direct-sum    B  1    normal-⊠  X  A      f(X)=B{\mathbf{1}}\oplus(X\boxtimes A)   .  Pick a starting matrix    X  0     subscript  X  0    X_{0}   and repeatedly apply f until the result does not change any more, i.e. a fixed point    R  =   f   (  R  )        R    f  R     R=f(R)   is reached. Independent of the choice of    X  0     subscript  X  0    X_{0}   , after one iteration the diagonal is B 1 and stays B 1 in all further iterations. If Eq.(6) were an ordinary matrix equation    R  =   𝟏  +   R  A        R    1    R  A      R={\mathbf{1}}+RA   for real-valued (or complex) A ij , it would have solution    R  =    (   𝟏  -  A   )    -  1    =   𝟏  +    ∑   k  ≥  1     A  k           R   superscript    1  A     1           1    subscript     k  1     superscript  A  k        R=({\mathbf{1}}-A)^{-1}={\mathbf{1}}+\sum_{k\geq 1}A^{k}   . However, the opinion algebra does not allow for such a simple expression. Instead we have  f 2 ( X 0 ) = B 1 ⊕ ((B 1 ⊕ ( X 0 ☒ A )) ☒ A )  f 3 ( X 0 ) = B 1 ⊕ ((B 1 ⊕ ( B 1 ⊕ ( X 0 ☒ A ) ☒ A )) ☒ A )  ...  which in general cannot be simplified.  References  "  Category:Probability theory  Category:Non-classical logic     B. Skoric, S.J.A. de Hoogh, N. Zannone. Flow-based reputation with uncertainty: Evidence-Based Subjective Logic . 2014. http://arxiv.org/abs/1402.3319 ↩   A. Jøsang. A Logic for Uncertain Probabilities. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems . 9(3), pp. 279–311, June 2001. PDF ↩        