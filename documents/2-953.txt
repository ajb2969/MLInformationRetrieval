   Covariance and contravariance of vectors      Covariance and contravariance of vectors   (Figure)  A vector v ( red ) represented by  in 3d general curvilinear coordinates ( q 1 , q 2 , q 3 ), a tuple of numbers to define point in a position space . Note the basis and cobasis do not coincide unless the basis is orthogonal . 1   In multilinear algebra and tensor analysis , covariance and contravariance describe how the quantitative description of certain geometric or physical entities changes with a change of basis . In physics, a basis is sometimes thought of as a set of reference axes. A change of scale on the reference axes corresponds to a change of units in the problem. For instance, in changing scale from meters to centimeters (that is, dividing the scale of the reference axes by 100), the components of a measured velocity vector will multiply by 100. Vectors exhibit this behavior of changing scale inversely to changes in scale to the reference axes: they are contravariant . As a result, vectors often have units of distance or distance times some other unit (like the velocity).  In contrast, dual vectors (also called covectors ) typically have units the inverse of distance or the inverse of distance times some other unit. An example of a dual vector is the gradient , which has units of a spatial derivative, or distance −1 . The components of dual vectors change in the same way as changes to scale of the reference axes: they are covariant . The components of vectors and covectors also transform in the same manner under more general changes in basis:   For a vector (such as a direction vector or velocity vector) to be basis-independent , the components of the vector must contra-vary with a change of basis to compensate. That is, the matrix that transforms the vector of components must be the inverse of the matrix that transforms the basis vectors. The components of vectors (as opposed to those of dual vectors) are said to be contravariant . Examples of vectors with contravariant components include the position of an object relative to an observer, or any derivative of position with respect to time, including velocity , acceleration , and jerk . In Einstein notation , contravariant components are denoted with upper indices as in      𝐯  =    v  i    𝐞  i     .      𝐯     superscript  v  i    subscript  𝐞  i      \mathbf{v}=v^{i}\mathbf{e}_{i}.     For a dual vector (also called a covector ) to be basis-independent, the components of the dual vector must co-vary with a change of basis to remain representing the same covector. That is, the components must be transformed by the same matrix as the change of basis matrix. The components of dual vectors (as opposed to those of vectors) are said to be covariant . Examples of covariant vectors generally appear when taking a gradient of a function. In Einstein notation , covariant components are denoted with lower indices as in      𝐯  =    v  i    𝐞  i     .      𝐯     subscript  v  i    superscript  𝐞  i      \mathbf{v}=v_{i}\mathbf{e}^{i}.      Curvilinear coordinate systems, such as cylindrical or spherical coordinates, are often used in physical and geometric problems. Associated to any coordinate system is a natural choice of coordinate basis for vectors based at each point of the space, and covariance and contravariance are particularly important for understanding how the coordinate description of a vector changes in passing from one coordinate system to another.  The terms covariant and contravariant were introduced by J.J. Sylvester in 1853 in order to study algebraic invariant theory . In this context, for instance, a system of simultaneous equations is contravariant in the variables. Tensors are objects in multilinear algebra that can have aspects of both covariance and contravariance. The use of both terms in the modern context of multilinear algebra is a specific example of corresponding notions in category theory .  Introduction  In physics, a vector typically arises as the outcome of a measurement or series of measurements, and is represented as a list (or tuple ) of numbers such as       (   v  1   ,   v  2   ,   v  3   )   .      subscript  v  1    subscript  v  2    subscript  v  3     (v_{1},v_{2},v_{3}).\,     The numbers in the list depend on the choice of coordinate system . For instance, if the vector represents position with respect to an observer ( position vector ), then the coordinate system may be obtained from a system of rigid rods, or reference axes, along which the components v 1 , v 2 , and v 3 are measured. For a vector to represent a geometric object, it must be possible to describe how it looks in any other coordinate system. That is to say, the components of the vectors will transform in a certain way in passing from one coordinate system to another.  A contravariant vector has components that "transform as the coordinates do" under changes of coordinates (and so inversely to the transformation of the reference axes), including rotation and dilation. The vector itself does not change under these operations ; instead, the components of the vector make a change that cancels the change in the spatial axes, in the same way that co-ordinates change. In other words, if the reference axes were rotated in one direction, the component representation of the vector would rotate in exactly the opposite way. Similarly, if the reference axes were stretched in one direction, the components of the vector, like the co-ordinates, would reduce in an exactly compensating way. Mathematically, if the coordinate system undergoes a transformation described by an invertible matrix  M , so that a coordinate vector  x is transformed to x ′ = M x , then a contravariant vector v must be similarly transformed via v ′ = M v . This important requirement is what distinguishes a contravariant vector from any other triple of physically meaningful quantities. For example, if v consists of the x , y , and z -components of velocity , then v is a contravariant vector: if the coordinates of space are stretched, rotated, or twisted, then the components of the velocity transform in the same way. Examples of contravariant vectors include displacement , velocity and acceleration . On the other hand, for instance, a triple consisting of the length, width, and height of a rectangular box could make up the three components of an abstract vector , but this vector would not be contravariant, since a change in coordinates on the space does not change the box's length, width, and height: instead these are scalars .  By contrast, a covariant vector has components that change oppositely to the coordinates or, equivalently, transform like the reference axes. For instance, the components of the gradient vector of a function       ∇  f   =      ∂  f    ∂   x  1       x  ^   1    +     ∂  f    ∂   x  2       x  ^   2    +     ∂  f    ∂   x  3       x  ^   3          normal-∇  f           f      superscript  x  1      superscript   normal-^  x   1          f      superscript  x  2      superscript   normal-^  x   2          f      superscript  x  3      superscript   normal-^  x   3       \nabla f=\frac{\partial f}{\partial x^{1}}\widehat{x}^{1}+\frac{\partial f}{%
 \partial x^{2}}\widehat{x}^{2}+\frac{\partial f}{\partial x^{3}}\widehat{x}^{3}   transform like the reference axes themselves. When only rotations of the axes are considered, the components of contravariant and covariant vectors behave in the same way. It is only when other transformations are allowed that the difference becomes apparent.  Definition  The general formulation of covariance and contravariance refers to how the components of a coordinate vector transform under a change of basis ( passive transformation ). Thus let V be a vector space of dimension n over the field of scalars  S , and let each of f = ( X 1 ,..., X n ) and f ' = ( Y 1 ,..., Y n ) be a basis of V . 2 Also, let the change of basis from f to f′ be given by  for some invertible  n × n matrix A with entries    a  j  i     subscript   superscript  a  i   j    a^{i}_{j}   . Here, each vector Y j of the f ' basis is a linear combination of the vectors X i of the f basis, so that        Y  j   =    ∑  i     a  j  i    X  i      .       subscript  Y  j     subscript   i      subscript   superscript  a  i   j    subscript  X  i       Y_{j}=\sum_{i}a^{i}_{j}X_{i}.     Contravariant transformation  A vector  v in V is expressed uniquely as a linear combination of the elements of the f basis as  where v  i [ f ] are scalars in S known as the components of v in the f basis. Denote the column vector of components of v by v [ f ]:       𝐯   [  𝐟  ]    =   [       v  1    [  𝐟  ]          v  2    [  𝐟  ]        ⋮        v  n    [  𝐟  ]       ]         𝐯   delimited-[]  𝐟         superscript  v  1    delimited-[]  𝐟         superscript  v  2    delimited-[]  𝐟      normal-⋮       superscript  v  n    delimited-[]  𝐟        \mathbf{v}[\mathbf{f}]=\begin{bmatrix}v^{1}[\mathbf{f}]\\
 v^{2}[\mathbf{f}]\\
 \vdots\\
 v^{n}[\mathbf{f}]\end{bmatrix}     so that () can be rewritten as a matrix product       v  =    𝐟   𝐯   [  𝐟  ]     .      v    𝐟  𝐯   delimited-[]  𝐟      v=\mathbf{f}\,\mathbf{v}[\mathbf{f}].     The vector v may also be expressed in terms of the f ' basis, so that       v  =     𝐟  ′    𝐯   [   𝐟  ′   ]     .      v     superscript  𝐟  normal-′   𝐯   delimited-[]   superscript  𝐟  normal-′       v=\mathbf{f^{\prime}}\,\mathbf{v}[\mathbf{f^{\prime}}].     However, since the vector v itself is invariant under the choice of basis,         𝐟   𝐯   [  𝐟  ]    =  v  =     𝐟  ′    𝐯   [   𝐟  ′   ]     .          𝐟  𝐯   delimited-[]  𝐟    v          superscript  𝐟  normal-′   𝐯   delimited-[]   superscript  𝐟  normal-′        \mathbf{f}\,\mathbf{v}[\mathbf{f}]=v=\mathbf{f^{\prime}}\,\mathbf{v}[\mathbf{f%
 ^{\prime}}].     The invariance of v combined with the relationship () between f and f ' implies that         𝐟   𝐯   [  𝐟  ]    =   𝐟   A   𝐯   [   𝐟  A   ]     ,        𝐟  𝐯   delimited-[]  𝐟      𝐟  A  𝐯   delimited-[]    𝐟  A       \mathbf{f}\,\mathbf{v}[\mathbf{f}]=\mathbf{f}A\,\mathbf{v}[\mathbf{f}A],     giving the transformation rule        𝐯   [   𝐟  A   ]    =    A   -  1    𝐯   [  𝐟  ]     .        𝐯   delimited-[]    𝐟  A        superscript  A    1    𝐯   delimited-[]  𝐟      \mathbf{v}[\mathbf{f}A]=A^{-1}\mathbf{v}[\mathbf{f}].     In terms of components,        v  i    [   𝐟  A   ]    =    ∑  j      a  ~   j  i    v  j    [  𝐟  ]            superscript  v  i    delimited-[]    𝐟  A       subscript   j      subscript   superscript   normal-~  a   i   j    superscript  v  j    delimited-[]  𝐟       v^{i}[\mathbf{f}A]=\sum_{j}\tilde{a}^{i}_{j}v^{j}[\mathbf{f}]     where the coefficients     a  ~   j  i     subscript   superscript   normal-~  a   i   j    \tilde{a}^{i}_{j}   are the entries of the inverse matrix of A .  Because the components of the vector v transform with the inverse of the matrix A , these components are said to transform contravariantly under a change of basis.  The way A relates the two pairs is depicted in the following informal diagram using an arrow. The reversal of the arrow indicates a contravariant change:      𝐟  ⟶   𝐟  ′      normal-⟶  𝐟   superscript  𝐟  normal-′     \mathbf{f}\longrightarrow\mathbf{f^{\prime}}          v   [  𝐟  ]    ⟵   v   [   𝐟  ′   ]       normal-⟵    v   delimited-[]  𝐟      v   delimited-[]   superscript  𝐟  normal-′       v[\mathbf{f}]\longleftarrow v[\mathbf{f^{\prime}}]     Covariant transformation  A linear functional α on V is expressed uniquely in terms of its components (scalars in S ) in the f basis as         α   (   X  i   )    =    α  i    [  𝐟  ]     ,   i  =   1  ,  2  ,  …  ,  n     .     formulae-sequence      α   subscript  X  i       subscript  α  i    delimited-[]  𝐟       i   1  2  normal-…  n      \alpha(X_{i})=\alpha_{i}[\mathbf{f}],\quad i=1,2,\dots,n.     These components are the action of α on the basis vectors X i of the f basis.  Under the change of basis from f to f ' (), the components transform so that  Denote the row vector of components of α by α [ f ]:       α   [  𝐟  ]    =   [        α  1    [  𝐟  ]    ,    α  2    [  𝐟  ]    ,  …  ,    α  n    [  𝐟  ]        ]         α   delimited-[]  𝐟          subscript  α  1    delimited-[]  𝐟       subscript  α  2    delimited-[]  𝐟    normal-…     subscript  α  n    delimited-[]  𝐟         \mathbf{\alpha}[\mathbf{f}]=\begin{bmatrix}\alpha_{1}[\mathbf{f}],\alpha_{2}[%
 \mathbf{f}],\dots,\alpha_{n}[\mathbf{f}]\end{bmatrix}     so that () can be rewritten as the matrix product        α   [   𝐟  A   ]    =   α   [  𝐟  ]   A    .        α   delimited-[]    𝐟  A       α   delimited-[]  𝐟   A     \alpha[\mathbf{f}A]=\alpha[\mathbf{f}]A.     Because the components of the linear functional α transform with the matrix A , these components are said to transform covariantly under a change of basis.  The way A relates the two pairs is depicted in the following informal diagram using an arrow. A covariant relationship is indicated since the arrows travel in the same direction:      𝐟  ⟶   𝐟  ′      normal-⟶  𝐟   superscript  𝐟  normal-′     \mathbf{f}\longrightarrow\mathbf{f^{\prime}}          α   [  𝐟  ]    ⟶   α   [   𝐟  ′   ]       normal-⟶    α   delimited-[]  𝐟      α   delimited-[]   superscript  𝐟  normal-′       \alpha[\mathbf{f}]\longrightarrow\alpha[\mathbf{f^{\prime}}]     Had a column vector representation been used instead, the transformation law would be the transpose         α  T    [   𝐟  A   ]    =    A  T    α  T    [  𝐟  ]     .         superscript  α  normal-T    delimited-[]    𝐟  A        superscript  A  normal-T    superscript  α  normal-T    delimited-[]  𝐟      \alpha^{\mathrm{T}}[\mathbf{f}A]=A^{\mathrm{T}}\alpha^{\mathrm{T}}[\mathbf{f}].     Coordinates  The choice of basis f on the vector space V defines uniquely a set of coordinate functions on V , by means of         x  i    [  𝐟  ]    (  v  )    =    v  i    [  𝐟  ]     .         superscript  x  i    delimited-[]  𝐟   v      superscript  v  i    delimited-[]  𝐟      x^{i}[\mathbf{f}](v)=v^{i}[\mathbf{f}].   The coordinates on V are therefore contravariant in the sense that         x  i    [   𝐟  A   ]    =    ∑   k  =  1   n      a  ~   k  i    x  k    [  𝐟  ]      .         superscript  x  i    delimited-[]    𝐟  A       superscript   subscript     k  1    n      subscript   superscript   normal-~  a   i   k    superscript  x  k    delimited-[]  𝐟       x^{i}[\mathbf{f}A]=\sum_{k=1}^{n}\tilde{a}^{i}_{k}x^{k}[\mathbf{f}].   Conversely, a system of n quantities v i that transform like the coordinates x i on V defines a contravariant vector. A system of n quantities that transform oppositely to the coordinates is then a covariant vector.  This formulation of contravariance and covariance is often more natural in applications in which there is a coordinate space (a manifold ) on which vectors live as tangent vectors or cotangent vectors . Given a local coordinate system x i on the manifold, the reference axes for the coordinate system are the vector fields         X  1   =    ∂   ∂   x  1     ,  …    ,    X  n   =   ∂   ∂   x  n       .     formulae-sequence     subscript  X  1          superscript  x  1     normal-…       subscript  X  n         superscript  x  n        X_{1}=\frac{\partial}{\partial x^{1}},\dots,X_{n}=\frac{\partial}{\partial x^{%
 n}}.   This gives rise to the frame f = ( X 1 ,..., X n ) at every point of the coordinate patch.  If y i is a different coordinate system and         Y  1   =    ∂   ∂   y  1     ,  …    ,    Y  n   =   ∂   ∂   y  n       ,     formulae-sequence     subscript  Y  1          superscript  y  1     normal-…       subscript  Y  n         superscript  y  n        Y_{1}=\frac{\partial}{\partial y^{1}},\dots,Y_{n}=\frac{\partial}{\partial y^{%
 n}},   then the frame f ' is related to the frame f by the inverse of the Jacobian matrix of the coordinate transition:         𝐟  ′   =   𝐟   J   -  1      ,   J  =    (    ∂   y  i     ∂   x  j     )     i  ,  j   =  1   n     .     formulae-sequence     superscript  𝐟  normal-′     𝐟   superscript  J    1        J   superscript   subscript       superscript  y  i       superscript  x  j        i  j   1    n      \mathbf{f}^{\prime}=\mathbf{f}J^{-1},\quad J=\left(\frac{\partial y^{i}}{%
 \partial x^{j}}\right)_{i,j=1}^{n}.   Or, in indices,        ∂   ∂   y  i     =    ∑   j  =  1   n      ∂   x  j     ∂   y  i      ∂   ∂   x  j        .            superscript  y  i       superscript   subscript     j  1    n          superscript  x  j       superscript  y  i           superscript  x  j         \frac{\partial}{\partial y^{i}}=\sum_{j=1}^{n}\frac{\partial x^{j}}{\partial y%
 ^{i}}\frac{\partial}{\partial x^{j}}.     A tangent vector is by definition a vector that is a linear combination of the coordinate partials    ∂  /   ∂   x  i            superscript  x  i      \partial/\partial x^{i}   . Thus a tangent vector is defined by       v  =    ∑   i  =  1   n     v  i    [  𝐟  ]    X  i     =    𝐟   𝐯   [  𝐟  ]     .        v    superscript   subscript     i  1    n      superscript  v  i    delimited-[]  𝐟    subscript  X  i            𝐟  𝐯   delimited-[]  𝐟       v=\sum_{i=1}^{n}v^{i}[\mathbf{f}]X_{i}=\mathbf{f}\ \mathbf{v}[\mathbf{f}].   Such a vector is contravariant with respect to change of frame. Under changes in the coordinate system, one has        𝐯   [   𝐟  ′   ]    =   𝐯   [   𝐟   J   -  1     ]    =    J   𝐯   [  𝐟  ]     .          𝐯   delimited-[]   superscript  𝐟  normal-′       𝐯   delimited-[]    𝐟   superscript  J    1              J  𝐯   delimited-[]  𝐟       \mathbf{v}[\mathbf{f}^{\prime}]=\mathbf{v}[\mathbf{f}J^{-1}]=J\,\mathbf{v}[%
 \mathbf{f}].   Therefore the components of a tangent vector transform via         v  i    [   𝐟  ′   ]    =    ∑   j  =  1   n      ∂   y  i     ∂   x  j      v  j    [  𝐟  ]      .         superscript  v  i    delimited-[]   superscript  𝐟  normal-′       superscript   subscript     j  1    n          superscript  y  i       superscript  x  j      superscript  v  j    delimited-[]  𝐟       v^{i}[\mathbf{f}^{\prime}]=\sum_{j=1}^{n}\frac{\partial y^{i}}{\partial x^{j}}%
 v^{j}[\mathbf{f}].   Accordingly, a system of n quantities v i depending on the coordinates that transform in this way on passing from one coordinate system to another is called a contravariant vector.  Covariant and contravariant components of a vector with a metric  In a vector space  V over a field K with a bilinear form (which may be referred to as the metric tensor ), there is little distinction between covariant and contravariant vectors, because the bilinear form allows covectors to be identified with vectors. That is, a vector v uniquely determines a covector α via       α   (  w  )    =   g   (  v  ,  w  )          α  w     g   v  w      \alpha(w)=g(v,w)   for all vectors w . Conversely, each covector α determines a unique vector v by this equation. Because of this identification of vectors with covectors, one may speak of the covariant components or contravariant components of a vector, that is, they are just representations of the same vector using reciprocal bases.  Given a basis f = ( X 1 ,..., X n ) of V , there is a unique reciprocal basis f # = ( Y 1 ,..., Y n ) of V determined by requiring that         Y  i    (   X  j   )    =   δ  j  i    ,         superscript  Y  i    subscript  X  j     subscript   superscript  δ  i   j     Y^{i}(X_{j})=\delta^{i}_{j},   the Kronecker delta . In terms of these bases, any vector v can be written in two ways:     v   v   \displaystyle v   The components v i [ f ] are the contravariant components of the vector v in the basis f , and the components v i [ f ] are the covariant components of v in the basis f . The terminology is justified because under a change of basis,         𝐯   [   𝐟  A   ]    =    A   -  1    𝐯   [  𝐟  ]     ,     𝐯  ♯    [   𝐟  A   ]    =    A  T    𝐯  ♯    [  𝐟  ]      .     formulae-sequence      𝐯   delimited-[]    𝐟  A        superscript  A    1    𝐯   delimited-[]  𝐟          superscript  𝐯  normal-♯    delimited-[]    𝐟  A        superscript  A  T    superscript  𝐯  normal-♯    delimited-[]  𝐟       \mathbf{v}[\mathbf{f}A]=A^{-1}\mathbf{v}[\mathbf{f}],\quad\mathbf{v}^{\sharp}[%
 \mathbf{f}A]=A^{T}\mathbf{v}^{\sharp}[\mathbf{f}].       Euclidean plane  In the Euclidean plane, the dot product allows for vectors to be identified with covectors. If     𝐞  1   ,   𝐞  2       subscript  𝐞  1    subscript  𝐞  2     \mathbf{e}_{1},\mathbf{e}_{2}   is a basis, then the dual basis     𝐞  1   ,   𝐞  2       superscript  𝐞  1    superscript  𝐞  2     \mathbf{e}^{1},\mathbf{e}^{2}   satisfies         𝐞  1   ⋅   𝐞  1    =  1   ,       normal-⋅   superscript  𝐞  1    subscript  𝐞  1    1    \displaystyle\mathbf{e}^{1}\cdot\mathbf{e}_{1}=1,   Thus, e 1 and e 2 are perpendicular to each other, as are e 2 and e 1 , and the lengths of e 1 and e 2 normalized against e 1 and e 2 , respectively.  Example  For example, 3 suppose that we are given a basis e 1 , e 2 consisting of a pair of vectors making a 45° angle with one another, such that e 1 has length 2 and e 2 has length 1. Then the dual basis vectors are given as follows:   e 2 is the result of rotating e 1 through an angle of 90° (where the sense is measured by assuming the pair e 1 , e 2 to be positively oriented), and then rescaling so that 1}} holds.  e 1 is the result of rotating e 2 through an angle of 90°, and then rescaling so that 1}} holds.   Applying these rules, we find       𝐞  1   =     1  2    𝐞  1    -    1   2     𝐞  2          superscript  𝐞  1         1  2    subscript  𝐞  1        1    2     subscript  𝐞  2       \mathbf{e}^{1}=\frac{1}{2}\mathbf{e}_{1}-\frac{1}{\sqrt{2}}\mathbf{e}_{2}   and        𝐞  2   =    -    1   2     𝐞  1     +   2   𝐞  2      .       superscript  𝐞  2           1    2     subscript  𝐞  1       2   subscript  𝐞  2       \mathbf{e}^{2}=-\frac{1}{\sqrt{2}}\mathbf{e}_{1}+2\mathbf{e}_{2}.     Thus the change of basis matrix in going from the original basis to the reciprocal basis is       R  =   [      1  /  2      -   1  /   2          -   1  /   2       2     ]    ,      R      1  2       1    2           1    2     2      R=\begin{bmatrix}1/2&-1/\sqrt{2}\\
 -1/\sqrt{2}&2\end{bmatrix},   since        [     𝐞  1     𝐞  2    ]   =    [     𝐞  1     𝐞  2    ]    [      1  /  2      -   1  /   2          -   1  /   2       2     ]     .       delimited-[]     superscript  𝐞  1    superscript  𝐞  2        delimited-[]     subscript  𝐞  1    subscript  𝐞  2         1  2       1    2           1    2     2       [\mathbf{e}^{1}\ \mathbf{e}^{2}]=[\mathbf{e}_{1}\ \mathbf{e}_{2}]\begin{%
 bmatrix}1/2&-1/\sqrt{2}\\
 -1/\sqrt{2}&2\end{bmatrix}.     For instance, the vector      v  =     3  2    𝐞  1    +   2   𝐞  2         v        3  2    subscript  𝐞  1      2   subscript  𝐞  2       v=\frac{3}{2}\mathbf{e}_{1}+2\mathbf{e}_{2}   is a vector with contravariant components        v  1   =   3  2    ,    v  2   =  2.      formulae-sequence     superscript  v  1     3  2       superscript  v  2   2.     v^{1}=\frac{3}{2},\quad v^{2}=2.   The covariant components are obtained by equating the two expressions for the vector v :      v  =     v  1    𝐞  1    +    v  2    𝐞  2     =     v  1    𝐞  1    +    v  2    𝐞  2           v       subscript  v  1    superscript  𝐞  1       subscript  v  2    superscript  𝐞  2               superscript  v  1    subscript  𝐞  1       superscript  v  2    subscript  𝐞  2        v=v_{1}\mathbf{e}^{1}+v_{2}\mathbf{e}^{2}=v^{1}\mathbf{e}_{1}+v^{2}\mathbf{e}_%
 {2}   so          [      v  1        v  2      ]       =    R   -  1     [      v  1        v  2      ]            =    [     4     2        2     1     ]    [      v  1        v  2      ]    =   [      6  +   2   2          2  +   3  /   2        ]       .         subscript  v  1      subscript  v  2       absent     superscript  R    1       superscript  v  1      superscript  v  2          missing-subexpression       absent      4    2       2   1       superscript  v  1      superscript  v  2               6    2    2         2    3    2            \begin{aligned}\displaystyle\begin{bmatrix}v_{1}\\
 v_{2}\end{bmatrix}&\displaystyle=R^{-1}\begin{bmatrix}v^{1}\\
 v^{2}\end{bmatrix}\\
 &\displaystyle=\begin{bmatrix}4&\sqrt{2}\\
 \sqrt{2}&1\end{bmatrix}\begin{bmatrix}v^{1}\\
 v^{2}\end{bmatrix}=\begin{bmatrix}6+2\sqrt{2}\\
 2+3/\sqrt{2}\end{bmatrix}\end{aligned}.     Three-dimensional Euclidean space  In the three-dimensional Euclidean space , one can also determine explicitly the dual basis to a given set of basis vectors  e 1 , e 2 , e 3 of E 3 that are not necessarily assumed to be orthogonal nor of unit norm. The contravariant (dual) basis vectors are:         𝐞  1   =     𝐞  2   ×   𝐞  3      𝐞  1   ⋅   (    𝐞  2   ×   𝐞  3    )      ;     𝐞  2   =     𝐞  3   ×   𝐞  1      𝐞  2   ⋅   (    𝐞  3   ×   𝐞  1    )      ;    𝐞  3   =     𝐞  1   ×   𝐞  2      𝐞  3   ⋅   (    𝐞  1   ×   𝐞  2    )        .     formulae-sequence     superscript  𝐞  1        subscript  𝐞  2    subscript  𝐞  3     normal-⋅   subscript  𝐞  1      subscript  𝐞  2    subscript  𝐞  3        formulae-sequence     superscript  𝐞  2        subscript  𝐞  3    subscript  𝐞  1     normal-⋅   subscript  𝐞  2      subscript  𝐞  3    subscript  𝐞  1          superscript  𝐞  3        subscript  𝐞  1    subscript  𝐞  2     normal-⋅   subscript  𝐞  3      subscript  𝐞  1    subscript  𝐞  2          \mathbf{e}^{1}=\frac{\mathbf{e}_{2}\times\mathbf{e}_{3}}{\mathbf{e}_{1}\cdot(%
 \mathbf{e}_{2}\times\mathbf{e}_{3})};\qquad\mathbf{e}^{2}=\frac{\mathbf{e}_{3}%
 \times\mathbf{e}_{1}}{\mathbf{e}_{2}\cdot(\mathbf{e}_{3}\times\mathbf{e}_{1})}%
 ;\qquad\mathbf{e}^{3}=\frac{\mathbf{e}_{1}\times\mathbf{e}_{2}}{\mathbf{e}_{3}%
 \cdot(\mathbf{e}_{1}\times\mathbf{e}_{2})}.     Even when the e i and e i are not orthonormal , they are still mutually dual:         𝐞  i   ⋅   𝐞  j    =   δ  j  i    ,       normal-⋅   superscript  𝐞  i    subscript  𝐞  j     subscript   superscript  δ  i   j     \mathbf{e}^{i}\cdot\mathbf{e}_{j}=\delta^{i}_{j},     Then the contravariant coordinates of any vector ''' v ''' can be obtained by the dot product of ''' v ''' with the contravariant basis vectors:         q  1   =   𝐯  ⋅   𝐞  1     ;     q  2   =   𝐯  ⋅   𝐞  2     ;    q  3   =   𝐯  ⋅   𝐞  3       .     formulae-sequence     superscript  q  1    normal-⋅  𝐯   superscript  𝐞  1      formulae-sequence     superscript  q  2    normal-⋅  𝐯   superscript  𝐞  2        superscript  q  3    normal-⋅  𝐯   superscript  𝐞  3        q^{1}=\mathbf{v}\cdot\mathbf{e}^{1};\qquad q^{2}=\mathbf{v}\cdot\mathbf{e}^{2}%
 ;\qquad q^{3}=\mathbf{v}\cdot\mathbf{e}^{3}.\,     Likewise, the covariant components of ''' v ''' can be obtained from the dot product of ''' v ''' with covariant basis vectors, viz.         q  1   =   𝐯  ⋅   𝐞  1     ;     q  2   =   𝐯  ⋅   𝐞  2     ;    q  3   =   𝐯  ⋅   𝐞  3       .     formulae-sequence     subscript  q  1    normal-⋅  𝐯   subscript  𝐞  1      formulae-sequence     subscript  q  2    normal-⋅  𝐯   subscript  𝐞  2        subscript  q  3    normal-⋅  𝐯   subscript  𝐞  3        q_{1}=\mathbf{v}\cdot\mathbf{e}_{1};\qquad q_{2}=\mathbf{v}\cdot\mathbf{e}_{2}%
 ;\qquad q_{3}=\mathbf{v}\cdot\mathbf{e}_{3}.\,     Then ''' v ''' can be expressed in two (reciprocal) ways, viz.      𝐯  =    q  i    𝐞  i    =     q  1    𝐞  1    +    q  2    𝐞  2    +    q  3     𝐞  3            𝐯     subscript  q  i    superscript  𝐞  i              subscript  q  1    superscript  𝐞  1       subscript  q  2    superscript  𝐞  2       subscript  q  3    superscript  𝐞  3        \mathbf{v}=q_{i}\mathbf{e}^{i}=q_{1}\mathbf{e}^{1}+q_{2}\mathbf{e}^{2}+q_{3}%
 \mathbf{e}^{3}\,   or       𝐯  =    q  i    𝐞  i    =     q  1    𝐞  1    +    q  2    𝐞  2    +    q  3    𝐞  3      .        𝐯     superscript  q  i    subscript  𝐞  i              superscript  q  1    subscript  𝐞  1       superscript  q  2    subscript  𝐞  2       superscript  q  3    subscript  𝐞  3        \mathbf{v}=q^{i}\mathbf{e}_{i}=q^{1}\mathbf{e}_{1}+q^{2}\mathbf{e}_{2}+q^{3}%
 \mathbf{e}_{3}.\,   Combining the above relations, we have      𝐯  =    (   𝐯  ⋅   𝐞  i    )    𝐞  i    =    (   𝐯  ⋅   𝐞  i    )     𝐞  i           𝐯     normal-⋅  𝐯   subscript  𝐞  i     superscript  𝐞  i            normal-⋅  𝐯   superscript  𝐞  i     subscript  𝐞  i       \mathbf{v}=(\mathbf{v}\cdot\mathbf{e}_{i})\mathbf{e}^{i}=(\mathbf{v}\cdot%
 \mathbf{e}^{i})\mathbf{e}_{i}\,   and we can convert from covariant to contravariant basis with       q  i   =   𝐯  ⋅   𝐞  i    =    (    q  j    𝐞  j    )   ⋅   𝐞  i    =    (    𝐞  j   ⋅   𝐞  i    )     q  j            subscript  q  i    normal-⋅  𝐯   subscript  𝐞  i          normal-⋅     superscript  q  j    subscript  𝐞  j     subscript  𝐞  i            normal-⋅   subscript  𝐞  j    subscript  𝐞  i     superscript  q  j       q_{i}=\mathbf{v}\cdot\mathbf{e}_{i}=(q^{j}\mathbf{e}_{j})\cdot\mathbf{e}_{i}=(%
 \mathbf{e}_{j}\cdot\mathbf{e}_{i})q^{j}\,   and        q  i   =   𝐯  ⋅   𝐞  i    =    (    q  j    𝐞  j    )   ⋅   𝐞  i    =    (    𝐞  j   ⋅   𝐞  i    )    q  j     .         superscript  q  i    normal-⋅  𝐯   superscript  𝐞  i          normal-⋅     subscript  q  j    superscript  𝐞  j     superscript  𝐞  i            normal-⋅   superscript  𝐞  j    superscript  𝐞  i     subscript  q  j       q^{i}=\mathbf{v}\cdot\mathbf{e}^{i}=(q_{j}\mathbf{e}^{j})\cdot\mathbf{e}^{i}=(%
 \mathbf{e}^{j}\cdot\mathbf{e}^{i})q_{j}.\,     The indices of covariant coordinates, vectors, and tensors are subscripts. If the contravariant basis vectors are orthonormal then they are equivalent to the covariant basis vectors, so there is no need to distinguish between the covariant and contravariant coordinates.  General Euclidean spaces  More generally, in an n -dimensional Euclidean space V , if a basis is       𝐞  1   ,  …  ,   𝐞  n       subscript  𝐞  1   normal-…   subscript  𝐞  n     \mathbf{e}_{1},\dots,\mathbf{e}_{n}   , the reciprocal basis is given by       𝐞  i   =    e   i  j     𝐞  j         superscript  𝐞  i      superscript  e    i  j     subscript  𝐞  j      \mathbf{e}^{i}=e^{ij}\mathbf{e}_{j}   where the coefficients e ij are the entries of the inverse matrix of        e   i  j    =    𝐞  i   ⋅   𝐞  j     .       subscript  e    i  j     normal-⋅   subscript  𝐞  i    subscript  𝐞  j      e_{ij}=\mathbf{e}_{i}\cdot\mathbf{e}_{j}.   Indeed, we then have         𝐞  i   ⋅   𝐞  k    =     e   i  j     𝐞  j    ⋅   𝐞  k    =    e   i  j     e   j  k     =   δ  k  i    .         normal-⋅   superscript  𝐞  i    subscript  𝐞  k     normal-⋅     superscript  e    i  j     subscript  𝐞  j     subscript  𝐞  k            superscript  e    i  j     subscript  e    j  k           subscript   superscript  δ  i   k      \mathbf{e}^{i}\cdot\mathbf{e}_{k}=e^{ij}\mathbf{e}_{j}\cdot\mathbf{e}_{k}=e^{%
 ij}e_{jk}=\delta^{i}_{k}.     The covariant and contravariant components of any vector      𝐯  =    q  i    𝐞  i    =    q  i     𝐞  i           𝐯     subscript  q  i    superscript  𝐞  i            superscript  q  i    subscript  𝐞  i       \mathbf{v}=q_{i}\mathbf{e}^{i}=q^{i}\mathbf{e}_{i}\,     are related as above by       q  i   =   𝐯  ⋅   𝐞  i    =    (    q  j    𝐞  j    )   ⋅   𝐞  i    =    q  j    e   j  i            subscript  q  i    normal-⋅  𝐯   subscript  𝐞  i          normal-⋅     superscript  q  j    subscript  𝐞  j     subscript  𝐞  i            superscript  q  j    subscript  e    j  i        q_{i}=\mathbf{v}\cdot\mathbf{e}_{i}=(q^{j}\mathbf{e}_{j})\cdot\mathbf{e}_{i}=q%
 ^{j}e_{ji}   and        q  i   =   𝐯  ⋅   𝐞  i    =    (    q  j    𝐞  j    )   ⋅   𝐞  i    =    q  j    e   j  i      .         superscript  q  i    normal-⋅  𝐯   superscript  𝐞  i          normal-⋅     subscript  q  j    superscript  𝐞  j     superscript  𝐞  i            subscript  q  j    superscript  e    j  i        q^{i}=\mathbf{v}\cdot\mathbf{e}^{i}=(q_{j}\mathbf{e}^{j})\cdot\mathbf{e}^{i}=q%
 _{j}e^{ji}.\,     Informal usage  In the field of physics , the adjective  covariant is often used informally as a synonym for invariant . For example, the Schrödinger equation does not keep its written form under the coordinate transformations of special relativity . Thus, a physicist might say that the Schrödinger equation is not covariant . In contrast, the Klein–Gordon equation and the Dirac equation do keep their written form under these coordinate transformations. Thus, a physicist might say that these equations are covariant .  Despite this usage of "covariant", it is more accurate to say that the Klein–Gordon and Dirac equations are invariant, and that the Schrödinger equation is not invariant. Additionally, to remove ambiguity, the transformation by which the invariance is evaluated should be indicated.  Because the components of vectors are contravariant and those of covectors are covariant, the vectors themselves are often referred to as being contravariant and the covectors as covariant.  Use in tensor analysis  The distinction between covariance and contravariance is particularly important for computations with tensors , which often have mixed variance . This means that they have both covariant and contravariant components, or both vector and dual vector components. The valence of a tensor is the number of variant and covariant terms, and in Einstein notation , covariant components have lower indices, while contravariant components have upper indices. The duality between covariance and contravariance intervenes whenever a vector or tensor quantity is represented by its components, although modern differential geometry uses more sophisticated index-free methods to represent tensors .  In tensor analysis , a covariant vector varies more or less reciprocally to a corresponding contravariant vector. Expressions for lengths, areas and volumes of objects in the vector space can then be given in terms of tensors with covariant and contravariant indices. Under simple expansions and contractions of the coordinates, the reciprocity is exact; under affine transformations the components of a vector intermingle on going between covariant and contravariant expression.  On a manifold , a tensor field will typically have multiple indices, of two sorts. By a widely followed convention, covariant indices are written as lower indices, whereas contravariant indices are upper indices. When the manifold is equipped with a metric , covariant and contravariant indices become very closely related to one another. Contravariant indices can be turned into covariant indices by contracting with the metric tensor. The reverse is possible by contracting with the (matrix) inverse of the metric tensor. Note that in general, no such relation exists in spaces not endowed with a metric tensor. Furthermore, from a more abstract standpoint, a tensor is simply "there" and its components of either kind are only calculational artifacts whose values depend on the chosen coordinates.  The explanation in geometric terms is that a general tensor will have contravariant indices as well as covariant indices, because it has parts that live in the tangent bundle as well as the cotangent bundle .  A contravariant vector is one which transforms like     d   x  μ     d  τ         d   superscript  x  μ      d  τ     \frac{dx^{\mu}}{d\tau}   , where     x  μ      superscript  x  μ    x^{\mu}\!   are the coordinates of a particle at its proper time     τ    τ   \tau\!   . A covariant vector is one which transforms like     ∂  ϕ    ∂   x  μ          ϕ      superscript  x  μ      \frac{\partial\phi}{\partial x^{\mu}}   , where    ϕ    ϕ   \phi\!   is a scalar field.  Algebra and geometry  In category theory , there are covariant functors and contravariant functors . The assignment of the dual space to a vector space is a standard example of a contravariant functor. Some constructions of multilinear algebra are of 'mixed' variance, which prevents them from being functors.  In differential geometry , the components of a vector relative to a basis of the tangent bundle are covariant if they change with the same linear transformation as a change of basis. They are contravariant if they change by the inverse transformation. This is sometimes a source of confusion for two distinct but related reasons. The first is that vectors whose components are covariant (called covectors or 1-forms ) actually pull back under smooth functions, meaning that the operation assigning the space of covectors to a smooth manifold is actually a contravariant functor. Likewise, vectors whose components are contravariant push forward under smooth mappings, so the operation assigning the space of (contravariant) vectors to a smooth manifold is a covariant functor. Secondly, in the classical approach to differential geometry, it is not bases of the tangent bundle that are the most primitive object, but rather changes in the coordinate system. Vectors with contravariant components transform in the same way as changes in the coordinates (because these actually change oppositely to the induced change of basis). Likewise, vectors with covariant components transform in the opposite way as changes in the coordinates.  See also   Covariant transformation  Change of basis  Active and passive transformation  Two-point tensor , a generalization allowing indices to reference multiple vector bases.  Mixed tensor   Notes  Notes  References    .   .   .   .   .   External links       Invariance, Contravariance, and Covariance   "  Category:Tensors  Category:Differential geometry  Category:Riemannian geometry  Category:Vectors     ↩  A basis f may here profitably be viewed as a linear isomorphism from R n to V . Regarding f as a row vector whose entries are the elements of the basis, the associated linear isomorphism is then     𝐱  ↦  𝐟𝐱   .     maps-to  𝐱  𝐟𝐱    \mathbf{x}\mapsto\mathbf{f}\mathbf{x}.    ↩  ↩     