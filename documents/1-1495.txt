   Geometric distribution      Geometric distribution   \! | kurtosis =    6  +     p  2    1  -  p         6     superscript  p  2     1  p      6+\frac{p^{2}}{1-p}\!   | entropy =       -    (   1  -  p   )     log  2    (   1  -  p   )      -   p    log  2   p     p               1  p     subscript   2     1  p        p    subscript   2   p     p    \tfrac{-(1-p)\log_{2}(1-p)-p\log_{2}p}{p}\!   | mgf =      p   e  t     1  -    (   1  -  p   )    e  t            p   superscript  e  t      1      1  p    superscript  e  t       \frac{pe^{t}}{1-(1-p)e^{t}}\!   ,   for     t  <   -   ln   (   1  -  p   )         t        1  p       t<-\ln(1-p)\!   | char =      p   e   i  t      1  -    (   1  -  p   )    e   i  t             p   superscript  e    i  t       1      1  p    superscript  e    i  t        \frac{pe^{it}}{1-(1-p)\,e^{it}}\!   | parameters2 =    0  <  p  ≤  1        0  p       1     0   success probability ( real ) | support2 = k failures where    k  ∈   {  0  ,  1  ,  2  ,  3  ,  …  }       k   0  1  2  3  normal-…     k\in\{0,1,2,3,\dots\}\!   | pdf2 =       (   1  -  p   )   k     p        superscript    1  p   k   p    (1-p)^{k}\,p\!   | cdf2 =    1  -     (   1  -  p   )    k  +  1         1   superscript    1  p     k  1      1-(1-p)^{k+1}\!   | mean2 =      1  -  p   p         1  p   p    \frac{1-p}{p}\!   | median2 =     ⌈    -  1     log  2    (   1  -  p   )     ⌉   -  1            1     subscript   2     1  p      1    \left\lceil\frac{-1}{\log_{2}(1-p)}\right\rceil\!-1    (not unique if    -   1  /    log  2    (   1  -  p   )           1    subscript   2     1  p       -1/\log_{2}(1-p)   is an integer) | mode2 =   0   0    | variance2 =      1  -  p    p  2          1  p    superscript  p  2     \frac{1-p}{p^{2}}\!   | skewness2 =      2  -  p     1  -  p           2  p       1  p      \frac{2-p}{\sqrt{1-p}}\!   | kurtosis2 =    6  +     p  2    1  -  p         6     superscript  p  2     1  p      6+\frac{p^{2}}{1-p}\!   | entropy2 =       -    (   1  -  p   )     log  2    (   1  -  p   )      -   p    log  2   p     p               1  p     subscript   2     1  p        p    subscript   2   p     p    \tfrac{-(1-p)\log_{2}(1-p)-p\log_{2}p}{p}\!   | mgf2 =     p   1  -    (   1  -  p   )    e  t          p    1      1  p    superscript  e  t       \frac{p}{1-(1-p)e^{t}}\!   | char2 =     p   1  -    (   1  -  p   )    e   i  t           p    1      1  p    superscript  e    i  t        \frac{p}{1-(1-p)\,e^{it}}\!   }} In probability theory and statistics , the geometric distribution is either of two discrete probability distributions :   The probability distribution of the number X of Bernoulli trials needed to get one success, supported on the set { 1, 2, 3, ...}    The probability distribution of the number Y = X − 1 of failures before the first success, supported on the set { 0, 1, 2, 3, ... }   Which of these one calls "the" geometric distribution is a matter of convention and convenience.  These two different geometric distributions should not be confused with each other. Often, the name shifted geometric distribution is adopted for the former one (distribution of the number X ); however, to avoid ambiguity, it is considered wise to indicate which is intended, by mentioning the support explicitly.  It’s the probability that the first occurrence of success requires k number of independent trials, each with success probability p. If the probability of success on each trial is p , then the probability that the k th trial (out of k trials) is the first success is       Pr   (   X  =  k   )    =      (   1  -  p   )    k  -  1      p         Pr    X  k       superscript    1  p     k  1    p     \Pr(X=k)=(1-p)^{k-1}\,p\,     for k = 1, 2, 3, ....  The above form of geometric distribution is used for modeling the number of trials up to and including the first success. By contrast, the following form of the geometric distribution is used for modeling the number of failures until the first success:       Pr   (   Y  =  k   )    =      (   1  -  p   )   k     p         Pr    Y  k       superscript    1  p   k   p     \Pr(Y=k)=(1-p)^{k}\,p\,     for k = 0, 1, 2, 3, ....  In either case, the sequence of probabilities is a geometric sequence .  For example, suppose an ordinary die is thrown repeatedly until the first time a "1" appears. The probability distribution of the number of times it is thrown is supported on the infinite set { 1, 2, 3, ... } and is a geometric distribution with p = 1/6.  Moments and cumulants  The expected value of a geometrically distributed random variable  X is 1/ p and the variance is (1 − p )/ p 2 :         E   (  X  )    =   1  p    ,    var   (  X  )    =    1  -  p    p  2      .     formulae-sequence      normal-E  X     1  p        var  X       1  p    superscript  p  2       \mathrm{E}(X)=\frac{1}{p},\qquad\mathrm{var}(X)=\frac{1-p}{p^{2}}.     Similarly, the expected value of the geometrically distributed random variable Y = X − 1 (where Y corresponds to the pmf listed in the right column) is q/ p = (1 − p )/ p , and its variance is (1 − p )/ p 2 :         E   (  Y  )    =    1  -  p   p    ,    var   (  Y  )    =    1  -  p    p  2      .     formulae-sequence      normal-E  Y       1  p   p        var  Y       1  p    superscript  p  2       \mathrm{E}(Y)=\frac{1-p}{p},\qquad\mathrm{var}(Y)=\frac{1-p}{p^{2}}.     Let μ = (1 − p )/ p be the expected value of Y . Then the cumulants     κ  n     subscript  κ  n    \kappa_{n}   of the probability distribution of Y satisfy the recursion        κ   n  +  1    =   μ   (   μ  +  1   )     d   κ  n     d  μ      .       subscript  κ    n  1      μ    μ  1       d   subscript  κ  n      d  μ       \kappa_{n+1}=\mu(\mu+1)\frac{d\kappa_{n}}{d\mu}.     Outline of proof: That the expected value is (1 − p )/ p can be shown in the following way. Let Y be as above. Then      E   (  Y  )       normal-E  Y    \displaystyle\mathrm{E}(Y)     (The interchange of summation and differentiation is justified by the fact that convergent power series  converge uniformly on compact subsets of the set of points where they converge.)  Parameter estimation  For both variants of the geometric distribution, the parameter p can be estimated by equating the expected value with the sample mean . This is the method of moments , which in this case happens to yield maximum likelihood estimates of p .  Specifically, for the first variant let k = k 1 , ..., k n be a sample where k i ≥ 1 for i = 1, ..., n . Then p can be estimated as        p  ^   =    (    1  n     ∑   i  =  1   n    k  i     )    -  1    =   n    ∑   i  =  1   n    k  i      .         normal-^  p    superscript      1  n     superscript   subscript     i  1    n    subscript  k  i       1           n    superscript   subscript     i  1    n    subscript  k  i        \widehat{p}=\left(\frac{1}{n}\sum_{i=1}^{n}k_{i}\right)^{-1}=\frac{n}{\sum_{i=%
 1}^{n}k_{i}}.\!     In Bayesian inference , the Beta distribution is the conjugate prior distribution for the parameter p . If this parameter is given a Beta( α , β ) prior , then the posterior distribution is       p  ∼   Beta   (   α  +  n   ,   β  +    ∑   i  =  1   n    (    k  i   -  1   )     )     .     similar-to  p    Beta     α  n     β    superscript   subscript     i  1    n      subscript  k  i   1         p\sim\mathrm{Beta}\left(\alpha+n,\ \beta+\sum_{i=1}^{n}(k_{i}-1)\right).\!     The posterior mean E[ p ] approaches the maximum likelihood estimate    p  ^     normal-^  p    \widehat{p}   as α and β approach zero.  In the alternative case, let k 1 , ..., k n be a sample where k i ≥ 0 for i = 1, ..., n . Then p can be estimated as        p  ^   =    (   1  +    1  n     ∑   i  =  1   n    k  i      )    -  1    =   n     ∑   i  =  1   n    k  i    +  n     .         normal-^  p    superscript    1      1  n     superscript   subscript     i  1    n    subscript  k  i        1           n      superscript   subscript     i  1    n    subscript  k  i    n       \widehat{p}=\left(1+\frac{1}{n}\sum_{i=1}^{n}k_{i}\right)^{-1}=\frac{n}{\sum_{%
 i=1}^{n}k_{i}+n}.\!     The posterior distribution of p given a Beta( α , β ) prior is       p  ∼   Beta   (   α  +  n   ,   β  +    ∑   i  =  1   n    k  i     )     .     similar-to  p    Beta     α  n     β    superscript   subscript     i  1    n    subscript  k  i         p\sim\mathrm{Beta}\left(\alpha+n,\ \beta+\sum_{i=1}^{n}k_{i}\right).\!     Again the posterior mean E[ p ] approaches the maximum likelihood estimate    p  ^     normal-^  p    \widehat{p}   as α and β approach zero.  Other properties   The probability-generating functions of X and Y are, respectively,          \begin{align} G_X(s) & = \frac{s\,p}{1-s\,(1-p)}, \\[10pt] G_Y(s) & = \frac{p}{1-s\,(1-p)}, \quad |s|   Like its continuous analogue (the exponential distribution ), the geometric distribution is memoryless . That means that if you intend to repeat an experiment until the first success, then, given that the first success has not yet occurred, the conditional probability distribution of the number of additional trials does not depend on how many failures have been observed. The die one throws or the coin one tosses does not have a "memory" of these failures. The geometric distribution is the only memoryless discrete distribution.    Among all discrete probability distributions supported on {1, 2, 3, ... } with given expected value μ, the geometric distribution X with parameter p = 1/μ is the one with the largest entropy .    The geometric distribution of the number Y of failures before the first success is infinitely divisible , i.e., for any positive integer n , there exist independent identically distributed random variables Y 1 , ..., Y n whose sum has the same distribution that Y has. These will not be geometrically distributed unless n = 1; they follow a negative binomial distribution .    The decimal digits of the geometrically distributed random variable Y are a sequence of independent (and not identically distributed) random variables. For example, the hundreds digit D has this probability distribution:           Pr   (   D  =  d   )    =    q   100  d     1  +   q  100   +   q  200   +  ⋯  +   q  900      ,       Pr    D  d       superscript  q    100  d      1   superscript  q  100    superscript  q  200   normal-⋯   superscript  q  900       \Pr(D=d)={q^{100d}\over 1+q^{100}+q^{200}+\cdots+q^{900}},       where q = 1 − p , and similarly for the other digits, and, more generally, similarly for numeral systems with other bases than 10. When the base is 2, this shows that a geometrically distributed random variable can be written as a sum of independent random variables whose probability distributions are indecomposable .    Golomb coding is the optimal prefix code for the geometric discrete distribution.    Recurrence relation       {       (   p  -  1   )    Pr   (  k  )     +   Pr   (   k  +  1   )     =  0   ,    Pr   (  0  )    =  p    }      formulae-sequence          p  1    Pr  k     Pr    k  1     0      Pr  0   p      \{(p-1)\Pr(k)+\Pr(k+1)=0,\Pr(0)=p\}     Related distributions   The geometric distribution Y is a special case of the negative binomial distribution , with r = 1. More generally, if Y 1 , ..., Y r are independent geometrically distributed variables with parameter p , then the sum         Z  =    ∑   m  =  1   r    Y  m        Z    superscript   subscript     m  1    r    subscript  Y  m      Z=\sum_{m=1}^{r}Y_{m}         follows a negative binomial distribution with parameters r and p . 1     The geometric distribution is a special case of discrete Compound Poisson distribution .    If Y 1 , ..., Y r are independent geometrically distributed variables (with possibly different success parameters p m ), then their minimum         W  =    min   m  ∈   1  ,  …  ,  r       Y  m         W    subscript     m   1  normal-…  r      subscript  Y  m      W=\min_{m\in 1,\dots,r}Y_{m}\,         is also geometrically distributed, with parameter     p  =   1  -    ∏  m    (   1  -   p  m    )      .      p    1    subscript  product  m     1   subscript  p  m        p=1-\prod_{m}(1-p_{m}).       Suppose 0 k has a Poisson distribution with expected value r  k / k . Then          ∑   k  =  1   ∞     k    X  k        superscript   subscript     k  1        k   subscript  X  k      \sum_{k=1}^{\infty}k\,X_{k}         has a geometric distribution taking values in the set {0, 1, 2, ...}, with expected value r /(1 − r ).    The exponential distribution is the continuous analogue of the geometric distribution. If X is an exponentially distributed random variable with parameter λ, then          Y  =   ⌊  X  ⌋    ,      Y    X     Y=\lfloor X\rfloor,         where    ⌊  ⌋     fragments  normal-⌊   normal-⌋    \lfloor\quad\rfloor   is the floor (or greatest integer) function, is a geometrically distributed random variable with parameter p = 1 − e − λ (thus λ = −ln(1 − p ) 2 ) and taking values in the set {0, 1, 2, ...}. This can be used to generate geometrically distributed pseudorandom numbers by first generating exponentially distributed pseudorandom numbers from a uniform pseudorandom number generator : then    ⌊    ln   (  U  )    /   ln   (   1  -  p   )     ⌋          U       1  p       \lfloor\ln(U)/\ln(1-p)\rfloor   is geometrically distributed with parameter   p   p   p   , if   U   U   U   is uniformly distributed in [0,1].    If    p  =   1  n       p    1  n     p=\frac{1}{n}   and    n  →  ∞     normal-→  n     n\rightarrow\infty   then we get an asymptotic exponential distribution with paramter of    λ  =   1  n       λ    1  n     \lambda=\frac{1}{n}      Since    P   (  X  >  a  )   =    (  1  -  p  )   a   =    (  1  -   1  n   )    n   1  n    (  a  )     =    [    (  1  -   1  n   )   n   ]     1  n    (  a  )      →   n  →  ∞       [   e   -  1    ]     1  n    (  a  )     =   e   -    1  n   a        fragments  P   fragments  normal-(  X   a  normal-)     superscript   fragments  normal-(  1   p  normal-)   a     superscript   fragments  normal-(  1     1  n   normal-)     n    1  n   a      superscript   fragments  normal-[   superscript   fragments  normal-(  1     1  n   normal-)   n   normal-]       1  n   a      normal-→  n     absent  normal-→     superscript   fragments  normal-[   superscript  e    1    normal-]       1  n   a      superscript  e        1  n   a       P\left(X>a\right)={{\left(1-p\right)}^{a}}={{\left(1-\frac{1}{n}\right)}^{n%
 \frac{1}{n}\left(a\right)}}={{\left[{{\left(1-\frac{1}{n}\right)}^{n}}\right]}%
 ^{\frac{1}{n}\left(a\right)}}\xrightarrow[n\to\infty]{}{{\left[{{e}^{-1}}%
 \right]}^{\frac{1}{n}\left(a\right)}}={{e}^{-\frac{1}{n}a}}     See also   Hypergeometric distribution  Coupon collector's problem  Compound Poisson distribution  Negative binomial distribution   References  External links    Geometric distribution on MathWorld .  Online geometric distribution calculator  Online calculator of Geometric distribution   "  Category:Discrete distributions  Category:Exponential family distributions  Category:Infinitely divisible probability distributions  Category:Probability distributions     Pitman, Jim. Probability (1993 edition). Springer Publishers. pp 372. ↩  http://www.wolframalpha.com/input/?i=inverse+p+%3D+1+-+e^-l ↩     