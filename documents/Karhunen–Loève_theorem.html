<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="631">Karhunen–Loève theorem</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Karhunen–Loève theorem</h1>
<hr>In the theory of [[stochastic process]]es, the '''Karhunen–Loève theorem'''  (named after [[Kari Karhunen]] and [[Michel Loève]]), also known as the '''Kosambi–Karhunen–Loève theorem'''<ref name="sapatnekar">{{Citation |last=Sapatnekar |first=Sachin |title= Overcoming variations in nanometer-scale technologies|journal= IEEE Journal on Emerging and Selected Topics in Circuits and Systems|volume= 1|year= 2011 |issue= 1|pages= 5–18 |doi=10.1109/jetcas.2011.2138250}}</ref><ref name="ghoman">{{Citation |last=Ghoman |first=Satyajit |last2= Wang|first2= Zhicun|last3=Chen |first3=PC |last4=Kapania|first4=Rakesh|title= A POD-based Reduced Order Design Scheme for Shape Optimization of Air Vehicles|booktitle=Proc of 53rd AIAA/ASME/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference, AIAA-2012-1808, Honolulu, Hawaii |year=2012 }}</ref> is a representation of a stochastic process as an in<p>finite linear combination of <a href="orthogonal_function" title="wikilink">orthogonal functions</a>, analogous to a <a href="Fourier_series" title="wikilink">Fourier series</a> representation of a function on a bounded interval. Stochastic processes given by infinite series of this form were first<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> considered by <a href="Damodar_Dharmananda_Kosambi" title="wikilink">Damodar Dharmananda Kosambi</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> There exist many such expansions of a stochastic process: if the process is indexed over <span class="LaTeX">$a a , b$</span>, any <a href="orthonormal_basis" title="wikilink">orthonormal basis</a> of <mtpl></mtpl> yields an expansion thereof in that form. The importance of the Karhunen–Loève theorem is that it yields the best such basis in the sense that it minimizes the total <a href="mean_squared_error" title="wikilink">mean squared error</a>.</p>
<p>In contrast to a Fourier series where the coefficients are fixed numbers and the expansion basis consists of <a href="trigonometric_function" title="wikilink">sinusoidal functions</a> (that is, <a class="uri" href="sine" title="wikilink">sine</a> and <a class="uri" href="cosine" title="wikilink">cosine</a> functions), the coefficients in the Karhunen–Loève theorem are <a href="random_variable" title="wikilink">random variables</a> and the expansion basis depends on the process. In fact, the orthogonal basis functions used in this representation are determined by the <a href="covariance_function" title="wikilink">covariance function</a> of the process. One can think that the Karhunen–Loève transform adapts to the process in order to produce the best possible basis for its expansion.</p>
<p>In the case of a <em>centered</em> stochastic process  (<em>centered</em> means <mtpl> 0}}</mtpl> for all <span class="LaTeX">$t ∈ a a , b$</span>) satisfying a technical continuity condition, <mtpl></mtpl> admits a decomposition</p>
<p><span class="LaTeX">$$X_t = \sum_{k=1}^\infty Z_k e_k(t)$$</span></p>
<p>where <mtpl></mtpl> are pairwise <a class="uri" href="uncorrelated" title="wikilink">uncorrelated</a> random variables and the functions <mtpl></mtpl> are continuous real-valued functions on <span class="LaTeX">$a a , b$</span> that are pairwise <a class="uri" href="orthogonal" title="wikilink">orthogonal</a> in <mtpl></mtpl>. It is therefore sometimes said that the expansion is <em>bi-orthogonal</em> since the random coefficients <mtpl></mtpl> are orthogonal in the probability space while the deterministic functions <mtpl></mtpl> are orthogonal in the time domain. The general case of a process <mtpl></mtpl> that is not centered can be brought back to the case of a centered process by considering <mtpl></mtpl> which is a centered process.</p>
<p>Moreover, if the process is <a href="Gaussian_process" title="wikilink">Gaussian</a>, then the random variables <mtpl></mtpl> are Gaussian and <a href="stochastically_independent" title="wikilink">stochastically independent</a>. This result generalizes the <em>Karhunen–Loève transform</em>. An important example of a centered real stochastic process on <span class="LaTeX">$0, 1 1$</span> is the <a href="Wiener_process" title="wikilink">Wiener process</a>; the Karhunen–Loève theorem can be used to provide a canonical orthogonal representation for it. In this case the expansion consists of sinusoidal functions.</p>
<p>The above expansion into uncorrelated random variables is also known as the <em>Karhunen–Loève expansion</em> or <em>Karhunen–Loève decomposition</em>. The <a href="statistic" title="wikilink">empirical</a> version (i.e., with the coefficients computed from a sample) is known as the <em>Karhunen–Loève transform</em> (KLT), <em><a href="principal_component_analysis" title="wikilink">principal component analysis</a></em>, <em>proper orthogonal decomposition (POD)</em>, <em><a href="Empirical_orthogonal_functions" title="wikilink">Empirical orthogonal functions</a></em> (a term used in <a class="uri" href="meteorology" title="wikilink">meteorology</a> and <a class="uri" href="geophysics" title="wikilink">geophysics</a>), or the <em><a href="Harold_Hotelling" title="wikilink">Hotelling</a> transform</em>.</p>
<h2 id="formulation">Formulation</h2>
<ul>
<li>Throughout this article, we will consider a square integrable zero-mean random process <mtpl></mtpl> defined over a probability space <span class="LaTeX">$(Ω, F , \mathbf{ P } )$</span> and indexed over a closed interval <span class="LaTeX">$a a , b$</span>, with covariance function <mtpl></mtpl>. We thus have:</li>
</ul>
<dl>
<dd><dl>
<dd><span class="LaTeX">$\forall t\in [a,b] \qquad X_t\in L^2(\Omega, F,\mathbf{P}),$</span>
</dd>
<dd><span class="LaTeX">$\forall t\in [a,b] \qquad \mathbf{E}[X_t]=0,$</span>
</dd>
<dd><span class="LaTeX">$\forall t,s \in [a,b] \qquad  K_X(s,t)=\mathbf{E}[X_s X_t].$</span>
</dd>
</dl>
</dd>
</dl>
<ul>
<li>We associate to <em>K</em><sub><em>X</em></sub> a <a href="linear_operator" title="wikilink">linear operator</a> <em>T</em><sub><em>K</em><sub><em>X</em></sub></sub> defined in the following way:</li>
</ul>
<dl>
<dd><dl>
<dd><math>\begin{cases}
</math></dd>
</dl>
</dd>
</dl>
<p>T_{K_X}: L^2([a,b]) \to L^2([a,b])\\ f \mapsto \int_a^b K_X(s,\cdot) f(s) ds \end{cases}</p>
<dl>
<dd>Since <em>T</em><sub><em>K</em><sub><em>X</em></sub></sub> is a linear operator, it makes sense to talk about its eigenvalues <em>λ<sub>k</sub></em> and eigenfunctions <em>e</em><sub><em>k</em></sub>, which are found solving the homogeneous Fredholm <a href="integral_equation" title="wikilink">integral equation</a> of the second kind
<p><span class="LaTeX">$$\int_a^b K_X(s,t) e_k(s)\,ds=\lambda_k e_k(t)$$</span></p>
</dd>
</dl>
<h2 id="statement-of-the-theorem">Statement of the theorem</h2>
<p><strong>Theorem</strong>. Let <mtpl></mtpl> be a zero-mean square integrable stochastic process defined over a probability space <span class="LaTeX">$(Ω, F , \mathbf{ P } )$</span> and indexed over a closed and bounded interval [<em>a</em>, <em>b</em>], with continuous covariance function <em>K</em><sub><em>X</em></sub>(<em>s</em>, <em>t</em>).</p>
<p>Then <em>K</em><sub><em>X</em></sub>(<em>s,t</em>) is a <a href="Mercer's_theorem" title="wikilink">Mercer kernel</a> and letting <em>e</em><sub><em>k</em></sub> be an orthonormal basis of <mtpl></mtpl> formed by the eigenfunctions of <em>T</em><sub><em>K</em><sub><em>X</em></sub></sub> with respective eigenvalues <mtpl></mtpl> admits the following representation</p>
<p><span class="LaTeX">$$X_t=\sum_{k=1}^\infty Z_k e_k(t)$$</span></p>
<p>where the convergence is in <a href="Convergence_of_random_variables#Convergence_in_mean" title="wikilink"><em>L</em><sup>2</sup></a>, uniform in <em>t</em> and</p>
<p><span class="LaTeX">$$Z_k=\int_a^b X_t e_k(t)\, dt$$</span></p>
<p>Furthermore, the random variables <em>Z</em><sub><em>k</em></sub> have zero-mean, are uncorrelated and have variance <em>λ<sub>k</sub></em></p>
<p><span class="LaTeX">$$\mathbf{E}[Z_k]=0,~\forall k\in\mathbb{N} \qquad \mbox{and}\qquad \mathbf{E}[Z_i Z_j]=\delta_{ij} \lambda_j,~\forall i,j\in \mathbb{N}$$</span></p>
<p>Note that by generalizations of Mercer's theorem we can replace the interval [<em>a</em>, <em>b</em>] with other compact spaces <em>C</em> and the Lebesgue measure on [<em>a</em>, <em>b</em>] with a Borel measure whose support is <em>C</em>.</p>
<h2 id="proof">Proof</h2>
<ul>
<li>The covariance function <em>K</em><sub><em>X</em></sub> satisfies the definition of a Mercer kernel. By <a href="Mercer's_theorem" title="wikilink">Mercer's theorem</a>, there consequently exists a set {<em>λ<sub>k</sub></em>, <em>e<sub>k</sub></em>(<em>t</em>)} of eigenvalues and eigenfunctions of T<sub><em>K</em><sub><em>X</em></sub></sub> forming an orthonormal basis of <em>L</em><sup>2</sup>([<em>a</em>,<em>b</em>]), and <em>K</em><sub><em>X</em></sub> can be expressed as</li>
</ul>
<dl>
<dd><dl>
<dd><span class="LaTeX">$K_X(s,t)=\sum_{k=1}^\infty \lambda_k e_k(s) e_k(t)$</span>
</dd>
</dl>
</dd>
</dl>
<ul>
<li>The process <em>X</em><sub><em>t</em></sub> can be expanded in terms of the eigenfunctions <em>e</em><sub><em>k</em></sub> as:</li>
</ul>
<dl>
<dd><dl>
<dd><span class="LaTeX">$X_t=\sum_{k=1}^\infty Z_k e_k(t)$</span>
</dd>
</dl>
</dd>
<dd>where the coefficients (random variables) <em>Z</em><sub><em>k</em></sub> are given by the projection of <em>X</em><sub><em>t</em></sub> on the respective eigenfunctions
<p><span class="LaTeX">$$Z_k=\int_a^b X_t e_k(t) \,dt$$</span></p>
</dd>
</dl>
<ul>
<li>We may then derive</li>
</ul>
<dl>
<dd><dl>
<dd><math>\begin{align}
</math></dd>
</dl>
</dd>
</dl>
<p>\mathbf{E}[Z_k] &=\mathbf{E}\left[\int_a^b X_t e_k(t) \,dt\right]=\int_a^b \mathbf{E}[X_t] e_k(t) dt=0 \\ [8pt] \mathbf{E}[Z_i Z_j]&=\mathbf{E}\left[ \int_a^b \int_a^b X_t X_s e_j(t)e_i(s) dt\, ds\right]\\ &=\int_a^b \int_a^b \mathbf{E}\left[X_t X_s\right] e_j(t)e_i(s) dt\, ds\\ &=\int_a^b \int_a^b K_X(s,t) e_j(t)e_i(s) dt \, ds\\ &=\int_a^b e_i(s)\left(\int_a^b K_X(s,t) e_j(t) dt\right) ds\\ &=\lambda_j \int_a^b e_i(s) e_j(s) ds\\ &=\delta_{ij}\lambda_j \end{align}</p>
<dl>
<dd>where we have used the fact that the <em>e</em><sub><em>k</em></sub> are eigenfunctions of <em>T</em><sub><em>K</em><sub><em>X</em></sub></sub> and are orthonormal.
</dd>
</dl>
<ul>
<li>Let us now show that the convergence is in <em>L</em><sup>2</sup>. Let</li>
</ul>
<dl>
<dd><dl>
<dd><span class="LaTeX">$S_N=\sum_{k=1}^N Z_k e_k(t).$</span>
</dd>
</dl>
</dd>
<dd>Then:
<p>:<math>\begin{align}</math></p>
</dd>
</dl>
<p>\mathbf{E} \left [\left |X_t-S_N \right |^2 \right ]&=\mathbf{E} \left [X_t^2 \right ]+\mathbf{E} \left [S_N^2 \right ] - 2\mathbf{E} \left [X_t S_N \right ]\\ &=K_X(t,t)+\mathbf{E}\left[\sum_{k=1}^N \sum_{l=1}^N Z_k Z_l e_k(t)e_l(t) \right] -2\mathbf{E}\left[X_t\sum_{k=1}^N Z_k e_k(t)\right]\\ &=K_X(t,t)+\sum_{k=1}^N \lambda_k e_k(t)^2 -2\mathbf{E}\left[\sum_{k=1}^N \int_a^b X_t X_s e_k(s) e_k(t) ds\right]\\ &=K_X(t,t)-\sum_{k=1}^N \lambda_k e_k(t)^2 \end{align}</p>
<dl>
<dd>which goes to 0 by Mercer's theorem.
</dd>
</dl>
<h2 id="properties-of-the-karhunenloève-transform">Properties of the Karhunen–Loève transform</h2>
<h3 id="special-case-gaussian-distribution">Special case: Gaussian distribution</h3>
<p>Since the limit in the mean of jointly Gaussian random variables is jointly Gaussian, and jointly Gaussian random (centered) variables are independent if and only if they are orthogonal, we can also conclude:</p>
<p><strong>Theorem</strong>. The variables <mtpl></mtpl> have a joint Gaussian distribution and are stochastically independent if the original process  is Gaussian.</p>
<p>In the Gaussian case, since the variables <mtpl></mtpl> are independent, we can say more:</p>
<p><span class="LaTeX">$$\lim_{N \to \infty} \sum_{i=1}^N e_i(t) Z_i(\omega) = X_t(\omega)$$</span> almost surely.</p>
<h3 id="the-karhunenloève-transform-decorrelates-the-process">The Karhunen–Loève transform decorrelates the process</h3>
<p>This is a consequence of the independence of the <mtpl></mtpl>.</p>
<h3 id="the-karhunenloève-expansion-minimizes-the-total-mean-square-error">The Karhunen–Loève expansion minimizes the total mean square error</h3>
<p>In the introduction, we mentioned that the truncated Karhunen–Loeve expansion was the best approximation of the original process in the sense that it reduces the total mean-square error resulting of its truncation. Because of this property, it is often said that the KL transform optimally compacts the energy.</p>
<p>More specifically, given any orthonormal basis {<em>f</em><sub><em>k</em></sub>} of <em>L</em><sup>2</sup>([<em>a</em>, <em>b</em>]), we may decompose the process <em>X<sub>t</sub></em> as:</p>
<p><span class="LaTeX">$$X_t(\omega)=\sum_{k=1}^\infty A_k(\omega) f_k(t)$$</span></p>
<p>where</p>
<p><span class="LaTeX">$$A_k(\omega)=\int_a^b X_t(\omega) f_k(t)\,dt$$</span></p>
<p>and we may approximate <em>X</em><sub><em>t</em></sub> by the finite sum</p>
<p><span class="LaTeX">$$\hat{X}_t(\omega)=\sum_{k=1}^N A_k(\omega) f_k(t)$$</span></p>
<p>for some integer <em>N</em>.</p>
<p><strong>Claim</strong>. Of all such approximations, the KL approximation is the one that minimizes the total mean square error (provided we have arranged the eigenvalues in decreasing order).</p>
<div class="NavFrame collapsed">
<div class="NavHead">
<p>[Proof]</p>
</div>
<div class="NavContent" style="text-align:left">
<p>Consider the error resulting from the truncation at the <em>N</em>-th term in the following orthonormal expansion:</p>
<p><span class="LaTeX">$$\varepsilon_N(t)=\sum_{k=N+1}^\infty A_k(\omega) f_k(t)$$</span> The mean-square error <em>ε</em><sub><em>N</em></sub><sup>2</sup>(<em>t</em>) can be written as:</p>
<p><span class="LaTeX">$$\begin{align}
\varepsilon_N^2(t)&=\mathbf{E} \left[\sum_{i=N+1}^\infty \sum_{j=N+1}^\infty A_i(\omega) A_j(\omega) f_i(t) f_j(t)\right]\\
&=\sum_{i=N+1}^\infty \sum_{j=N+1}^\infty \mathbf{E}\left[\int_a^b \int_a^b X_t X_s f_i(t)f_j(s) ds\, dt\right] f_i(t) f_j(t)\\
&=\sum_{i=N+1}^\infty \sum_{j=N+1}^\infty f_i(t) f_j(t) \int_a^b \int_a^b K_X(s,t) f_i(t)f_j(s) ds\, dt
\end{align}$$</span></p>
<p>We then integrate this last equality over [<em>a</em>, <em>b</em>]. The orthonormality of the <em>f<sub>k</sub></em> yields:</p>
<p><span class="LaTeX">$$\int_a^b \varepsilon_N^2(t) dt=\sum_{k=N+1}^\infty \int_a^b \int_a^b K_X(s,t) f_k(t)f_k(s) ds\, dt$$</span></p>
<p>The problem of minimizing the total mean-square error thus comes down to minimizing the right hand side of this equality subject to the constraint that the <em>f</em><sub><em>k</em></sub> be normalized. We hence introduce <mtpl></mtpl>, the Lagrangian multipliers associated with these constraints, and aim at minimizing the following function:</p>
<p><span class="LaTeX">$$Er[f_k(t),k\in\{N+1,\ldots\}]=\sum_{k=N+1}^\infty \int_a^b \int_a^b K_X(s,t) f_k(t)f_k(s) ds dt-\beta_k \left(\int_a^b f_k(t) f_k(t) dt -1\right)$$</span></p>
<p>Differentiating with respect to <em>f</em><sub><em>i</em></sub>(<em>t</em>) and setting the derivative to 0 yields:</p>
<p><span class="LaTeX">$$\frac{\partial Er}{\partial f_i(t)}=\int_a^b \left(\int_a^b K_X(s,t) f_i(s) ds -\beta_i f_i(t)\right)dt=0$$</span></p>
<p>which is satisfied in particular when</p>
<p><span class="LaTeX">$$\int_a^b K_X(s,t) f_i(s) \,ds =\beta_i f_i(t).$$</span></p>
<p>In other words when the <em>f</em><sub><em>k</em></sub> are chosen to be the eigenfunctions of <em>T</em><sub><em>K</em><sub><em>X</em></sub></sub>, hence resulting in the KL expansion.</p>
</div>
</div>
<h3 id="explained-variance">Explained variance</h3>
<p>An important observation is that since the random coefficients <em>Z</em><sub><em>k</em></sub> of the KL expansion are uncorrelated, the <a href="Variance#Sum_of_uncorrelated_variables_.28Bienaym.C3.A9_formula.29" title="wikilink">Bienaymé formula</a> asserts that the variance of <em>X</em><sub><em>t</em></sub> is simply the sum of the variances of the individual components of the sum:</p>
<p><span class="LaTeX">$$\mbox{Var}[X_t]=\sum_{k=0}^\infty e_k(t)^2 \mbox{Var}[Z_k]=\sum_{k=1}^\infty \lambda_k e_k(t)^2$$</span></p>
<p>Integrating over [<em>a</em>, <em>b</em>] and using the orthonormality of the <em>e</em><sub><em>k</em></sub>, we obtain that the total variance of the process is:</p>
<p><span class="LaTeX">$$\int_a^b \mbox{Var}[X_t] dt=\sum_{k=1}^\infty \lambda_k$$</span></p>
<p>In particular, the total variance of the <em>N</em>-truncated approximation is</p>
<p><span class="LaTeX">$$\sum_{k=1}^N \lambda_k.$$</span></p>
<p>As a result, the <em>N</em>-truncated expansion explains</p>
<p><span class="LaTeX">$$\frac{\sum_{k=1}^N \lambda_k}{\sum_{k=1}^\infty \lambda_k}$$</span></p>
<p>of the variance; and if we are content with an approximation that explains, say, 95% of the variance, then we just have to determine an <span class="LaTeX">$N\in\mathbb{N}$</span> such that</p>
<p><span class="LaTeX">$$\frac{\sum_{k=1}^N \lambda_k}{\sum_{k=1}^\infty \lambda_k} \geq 0.95$$</span>.</p>
<h3 id="the-karhunenloève-expansion-has-the-minimum-representation-entropy-property">The Karhunen–Loève expansion has the minimum representation entropy property</h3>
<h2 id="linear-karhunen-loeve-approximations">Linear Karhunen-Loeve Approximations</h2>
<p>Let us consider a whole class of signals we want to approximate over the first <span class="LaTeX">$M$</span> vectors of a basis. These signals are modeled as realizations of a random vector <span class="LaTeX">$Y n n$</span> of size <span class="LaTeX">$N$</span>. To optimize the approximation we design a basis that minimizes the average approximation error. This section proves that optimal bases are karhunen-loeve bases that diagonalize the covariance matrix of <span class="LaTeX">$Y$</span>. The random vector <span class="LaTeX">$Y$</span> can be decomposed in an orthogonal basis</p>
<p><span class="LaTeX">$$\left\{ g_m \right\}_{0\le m\le N}$$</span></p>
<p>as follows:</p>
<p><span class="LaTeX">$$Y=\sum_{m=0}^{N-1} \left\langle Y, g_m \right\rangle g_m,$$</span></p>
<p>where each</p>
<p><span class="LaTeX">$$\left\langle Y, g_m \right\rangle =\sum_{n=0}^{N-1}{Y[n]} g_m^* [n]$$</span></p>
<p>is a random variable. The approximation from the first <span class="LaTeX">$M ≤ N$</span> vectors of the basis is</p>
<p><span class="LaTeX">$$Y_M=\sum_{m=0}^{M-1} \left\langle Y, g_m \right\rangle g_m$$</span></p>
<p>The energy conservation in an orthogonal basis implies</p>
<p><span class="LaTeX">$$\varepsilon[M]= \mathbf{E} \left\{ \left\| Y- Y_M \right\|^2 \right\} =\sum_{m=M}^{N-1} \mathbf{E}\left\{ \left| \left\langle Y, g_m \right\rangle  \right|^2 \right\}$$</span></p>
<p>This error is related to the covariance of <span class="LaTeX">$Y$</span> defined by</p>
<p><span class="LaTeX">$$R[ n,m]=\mathbf{E} \left\{ Y[n] Y^*[m] \right\}$$</span></p>
<p>For any vector <span class="LaTeX">$x n n$</span> we denote by <span class="LaTeX">$K$</span> the covariance operator represented by this matrix,</p>
<p><span class="LaTeX">$$\mathbf{E}\left\{\left|\langle Y,x \rangle\right|^2\right\}=\langle Kx,x \rangle =\sum_{n=0}^{N-1}\sum_{m=0}^{N-1}R[n,m]x[n]x^*[m]$$</span></p>
<p>The error <span class="LaTeX">$ε M M$</span> is therefore a sum of the last <span class="LaTeX">$N − M$</span> coefficients of the covariance operator</p>
<p><span class="LaTeX">$$\varepsilon [M]=\sum_{m=M}^{N-1}{\left\langle K g_m, g_m \right\rangle }$$</span></p>
<p>The covariance operator <span class="LaTeX">$K$</span> is Hermitian and Positive and is thus diagonalized in an orthogonal basis called a Karhunen-Loeve basis. The following theorem states that a Karhunen-Loeve basis is optimal for linear approximations.</p>
<p><strong>Theorem (Optimality of Karhunen-Loeve Basis).</strong> Let <span class="LaTeX">$K$</span> be acovariance operator. For all <span class="LaTeX">$M ≥ 1$</span>, the approximation error</p>
<p><span class="LaTeX">$$\varepsilon [M]=\sum_{m=M}^{N-1}\left\langle K g_m, g_m \right\rangle$$</span></p>
<p>is minimum if and only if</p>
<p><span class="LaTeX">$$\left\{ g_m \right\}_{0\le m<n}< math=""> 

is a Karhunen-Loeve basis ordered by decreasing eigenvalues.

:<math>\left\langle K g_m, g_m \right\rangle \ge \left\langle Kg_{m+1}, g_{m+1} \right\rangle, \qquad 0\le m<n-1.< math="">

== Non-Linear Approximation in Bases ==
Linear approximations project the signal on M vectors a priori. The approximation can be made more precise by choosing the M orthogonal vectors depending on the signal properties. This section analyzes the general performance of these non-linear approximations. A signal <math>f\in \Eta$$</span> is approximated with M vectors selected adaptively in an orthonormal basis for <span class="LaTeX">$\Eta$</span></p>
<p><span class="LaTeX">$$\Beta =\left\{ g_m \right\}_{m\in \mathbb{N}}$$</span></p>
<p>Let <span class="LaTeX">$f_M$</span> be the projection of f over M vectors whose indices are in <mtpl></mtpl>:</p>
<p><span class="LaTeX">$$f_M=\sum_{m\in I_M} \left\langle f, g_m \right\rangle g_m$$</span></p>
<p>The approximation error is the sum of the remaining coefficients</p>
<p><span class="LaTeX">$$\varepsilon [M]=\left\{ \left\| f- f_M \right\|^2 \right\}=\sum_{m\notin I_M}^{N-1} \left\{ \left| \left\langle f, g_m \right\rangle  \right|^2 \right\}$$</span></p>
<p>To minimize this error, the indices in <mtpl></mtpl> must correspond to the M vectors having the largest inner product amplitude</p>
<p><span class="LaTeX">$$\left| \left\langle f, g_m \right\rangle  \right|.$$</span></p>
<p>These are the vectors that best correlate f. They can thus be interpreted as the main features of f. The resulting error is necessarily smaller than the error of a linear approximation which selects the M approximation vectors independently of f. Let us sort</p>
<p><span class="LaTeX">$$\left\{ \left| \left\langle f, g_m \right\rangle  \right| \right\}_{m\in \mathbb{N}}$$</span></p>
<p>in decreasing order</p>
<p><span class="LaTeX">$$\left| \left \langle f, g_{m_k} \right \rangle \right|\ge \left| \left \langle f, g_{m_{k+1}} \right \rangle  \right|.$$</span></p>
<p>The best non-linear approximation is</p>
<p><span class="LaTeX">$$f_M=\sum_{k=1}^M \left\langle f, g_{m_k} \right\rangle g_{m_k}$$</span></p>
<p>It can also be written as inner product thresholding:</p>
<p><span class="LaTeX">$$f_M=\sum_{m=0}^{\infty} \theta_T \left( \left\langle f, g_m \right\rangle  \right) g_m$$</span></p>
<p>with</p>
<p><span class="LaTeX">$$T=\left|\left\langle f, g_{m_M} \right \rangle\right|, \qquad \theta_T(x)= \begin{cases} x & |x|\ge T \\ 0 & |x| < T \end{cases}$$</span></p>
<p>The non-linear error is</p>
<p><span class="LaTeX">$$\varepsilon [M]=\left\{ \left\| f- f_M \right\|^2 \right\}=\sum_{k=M+1}^{\infty} \left\{ \left| \left\langle f, g_{m_k} \right\rangle  \right|^2 \right\}$$</span></p>
<p>this error goes quickly to zero as M increases, if the sorted values of <span class="LaTeX">$\left| \left\langle f, g_{m_k} \right\rangle  \right|$</span> have a fast decay as k increases. This decay is quantified by computing the <span class="LaTeX">$\Iota^\Rho$</span> norm of the signal inner products in B:</p>
<p><span class="LaTeX">$$\| f \|_{\Beta, p} =\left( \sum_{m=0}^{\infty} \left| \left\langle f, g_m \right\rangle  \right|^p \right)^{\frac{1}{p}}$$</span></p>
<p>The following theorem relates the decay of <span class="LaTeX">$ε M M$</span> to <span class="LaTeX">$\| f\|_{\Beta, p}$</span></p>
<p><strong>Theorem (decay of error).</strong> If <span class="LaTeX">$\| f\|_{\Beta ,p}<\infty$</span> with <span class="LaTeX">$p  then</span></p>
<p><span class="LaTeX">$$\varepsilon [M]\le \frac{\|f\|_{\Beta ,p}^2}{\frac{2}{p}-1} M^{1-\frac{2}{p}}$$</span> and</p>
<p><span class="LaTeX">$$\varepsilon [M]=o\left( M^{1-\frac{2}{p}} \right).$$</span> Conversely, if <span class="LaTeX">$\varepsilon [M]=o\left( M^{1-\frac{2}{p}} \right)$</span> then</p>
<p><span class="LaTeX">$\| f\|_{\Beta ,q}<\infty$</span> for any <span class="LaTeX">$q > p$</span>.</p>
<h3 id="non-optimality-of-karhunen-loéve-bases">Non-optimality of Karhunen-Loéve Bases</h3>
<p>To further illustrate the differences between linear and non-linear approximations, we study the decomposition of a simple non-Gaussian random vector in a Karhunen-Loéve basis. Processes whose realizations have a random translation are stationary. The Karhunen-Loéve basis is then a Fourier basis and we study its performance. To simplify the analysis, consider a random vector Y[n] of size N that is random shift modulo N of a deterministic signal f[n] of zero mean</p>
<p><span class="LaTeX">$$\sum_{n=0}^{N-1}f[n]=0$$</span></p>
<p><span class="LaTeX">$$Y[n]=f [ ( n-p)\bmod N ]$$</span></p>
<p>The random shift P is uniformly distributed on [0,N-1]:</p>
<p><span class="LaTeX">$$\Pr ( P=p )=\frac{1}{N}, \qquad 0\le p<n< math="">

Clearly

:<math>\mathbf{E}\{ Y[n]\}=\frac{1}{N} \sum_{p=0}^{N-1} f[( n-p)\bmod N]=0$$</span></p>
<p>and</p>
<p><span class="LaTeX">$$R[ n,k]=\mathbf{E} \{ Y[n]Y[k] \}=\frac{1}{N}\sum_{p=0}^{N-1} f[( n-p)\bmod N] f [(k-p)\bmod N ] =\frac{1}{N}f\Theta \bar{f}[ n-k], \quad \bar{f}[n]=f[ -n]$$</span></p>
<p>Hence</p>
<p><span class="LaTeX">$$R[ n,k]=R_Y[n-k], \qquad R_Y[k]=\frac{1}{N}f \Theta \bar{f}[k]$$</span></p>
<p>Since R<sub>Y</sub> is N periodic, Y is a circular stationary random vector. The covariance operator is a circular convolution with R<sub>Y</sub> and is therefore diagonalized in the discrete Fourier Karhunen-Loéve basis</p>
<p><span class="LaTeX">$$\left\{ \frac{1}{\sqrt{N}} e^{\frac{i2\pi mn}{N}} \right\}_{0\le m<n}.< math=""> 

The power spectrum is Fourier Transform of R<sub>Y</sub>:

:<math>P_Y[m]\hat{R}_Y[m]=\frac{1}{N} \left| \hat{f}[m] \right|^2$$</span></p>
<p><strong>Example:</strong> Consider an extreme case where <span class="LaTeX">$f[n]=\delta [n]-\delta [n-1]$</span>. A theorem stated above guarantees that the Fourier Karhunen-Loéve basis produces a smaller expected approximation error than a canonical basis of Diracs <span class="LaTeX">$\left\{g_m[n]=\delta[ n-m] \right\}_{0\le m<n}< math="">. Indeed we do not know a priori the abscissa of the non-zero coefficients of Y, so there is no particular Dirac that is better adapted to perform the approximation. But the Fourier vectors cover the whole support of Y and thus absorb a part of the signal energy.

:<math>\mathbf{E} \left\{ \left| \left\langle Y[n],\frac{1}{\sqrt{N}} e^{\frac{i2\pi mn}{N}} \right\rangle  \right|^2 \right\}=P_Y[m] = \frac{4}{N}\sin^2 \left(\frac{\pi k}{N} \right)$</span></p>
<p>Selecting higher frequency Fourier coefficients yields a better mean-square approximation than choosing a priori a few Dirac vectors to perform the approximation. The situation is totally different for non-linear approximations. If <span class="LaTeX">$f[n]=\delta[n]-\delta[n-1]$</span> then the discrete Fourier basis is extremely inefficient because f and hence Y have an energy that is almost uniformly spread among all Fourier vectors. In contrast, since f has only two non-zero coefficients in the Dirac basis, a non-linear approximation of Y with <span class="LaTeX">$M ≥ 2$</span> gives zero error.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="principal-component-analysis">Principal component analysis</h2>
<p>We have established the Karhunen–Loève theorem and derived a few properties thereof. We also noted that one hurdle in its application was the numerical cost of determining the eigenvalues and eigenfunctions of its covariance operator through the Fredholm integral equation of the second kind</p>
<p><span class="LaTeX">$$\int_a^b K_X(s,t) e_k(s)\,ds=\lambda_k e_k(t).$$</span></p>
<p>However, when applied to a discrete and finite process <span class="LaTeX">$\left(X_n\right)_{n\in\{1,\ldots,N\}}$</span>, the problem takes a much simpler form and standard algebra can be used to carry out the calculations.</p>
<p>Note that a continuous process can also be sampled at <em>N</em> points in time in order to reduce the problem to a finite version.</p>
<p>We henceforth consider a random <em>N</em>-dimensional vector <span class="LaTeX">$X=\left(X_1~X_2~\ldots~X_N\right)^T$</span>. As mentioned above, <em>X</em> could contain <em>N</em> samples of a signal but it can hold many more representations depending on the field of application. For instance it could be the answers to a survey or economic data in an econometrics analysis.</p>
<p>As in the continuous version, we assume that <em>X</em> is centered, otherwise we can let <span class="LaTeX">$X:=X-\mu_X$</span> (where <span class="LaTeX">$\mu_X$</span> is the <a href="mean_vector" title="wikilink">mean vector</a> of <em>X</em>) which is centered.</p>
<p>Let us adapt the procedure to the discrete case.</p>
<h3 id="covariance-matrix">Covariance matrix</h3>
<p>Recall that the main implication and difficulty of the KL transformation is computing the eigenvectors of the linear operator associated to the covariance function, which are given by the solutions to the integral equation written above.</p>
<p>Define Σ, the covariance matrix of <em>X</em>, as an <em>N</em> × <em>N</em> matrix whose elements are given by:</p>
<p><span class="LaTeX">$$\Sigma_{ij}= \mathbf{E}[X_i X_j],\qquad \forall i,j \in \{1,\ldots,N\}$$</span></p>
<p>Rewriting the above integral equation to suit the discrete case, we observe that it turns into:</p>
<p><span class="LaTeX">$$\sum_{i=1}^N \Sigma_{ij} e_j=\lambda e_i \quad \Leftrightarrow \quad \Sigma e=\lambda e$$</span></p>
<p>where <span class="LaTeX">$e=(e_1~e_2~\ldots~e_N)^T$</span> is an <em>N</em>-dimensional vector.</p>
<p>The integral equation thus reduces to a simple matrix eigenvalue problem, which explains why the PCA has such a broad domain of applications.</p>
<p>Since Σ is a positive definite symmetric matrix, it possesses a set of orthonormal eigenvectors forming a basis of <span class="LaTeX">$\R^N$</span>, and we write <span class="LaTeX">$\{\lambda_i,\phi_i\}_{i\in\{1,\ldots,N\}}$</span> this set of eigenvalues and corresponding eigenvectors, listed in decreasing values of <mtpl></mtpl>. Let also <span class="LaTeX">$Φ$</span> be the orthonormal matrix consisting of these eigenvectors:</p>
<p><span class="LaTeX">$$\begin{align}
\Phi &:=\left(\phi_1~\phi_2~\ldots~\phi_N\right)^T\\
\Phi^T \Phi &=I
\end{align}$$</span></p>
<h3 id="principal-component-transform">Principal component transform</h3>
<p>It remains to perform the actual KL transformation, called the <em>principal component transform</em> in this case. Recall that the transform was found by expanding the process with respect to the basis spanned by the eigenvectors of the covariance function. In this case, we hence have:</p>
<p><span class="LaTeX">$$X =\sum_{i=1}^N \langle \phi_i,X\rangle \phi_i =\sum_{i=1}^N \phi_i^T X \phi_i$$</span></p>
<p>In a more compact form, the principal component transform of <em>X</em> is defined by:</p>
<p><span class="LaTeX">$$\begin{cases} Y=\Phi^T X \\ X=\Phi Y \end{cases}$$</span></p>
<p>The <em>i</em>-th component of <em>Y</em> is <span class="LaTeX">$Y_i=\phi_i^T X$</span>, the projection of <em>X</em> on <span class="LaTeX">$\phi_i$</span> and the inverse transform <span class="LaTeX">$X = Φ Y$</span> yields the expansion of <span class="LaTeX">$X$</span> on the space spanned by the <span class="LaTeX">$\phi_i$</span>:</p>
<p><span class="LaTeX">$$X=\sum_{i=1}^N Y_i \phi_i=\sum_{i=1}^N \langle \phi_i,X\rangle \phi_i$$</span></p>
<p>As in the continuous case, we may reduce the dimensionality of the problem by truncating the sum at some <span class="LaTeX">$K\in\{1,\ldots,N\}$</span> such that</p>
<p><span class="LaTeX">$$\frac{\sum_{i=1}^K \lambda_i}{\sum_{i=1}^N \lambda_i}\geq \alpha$$</span></p>
<p>where α is the explained variance threshold we wish to set.</p>
<p>We can also reduce the dimensionality through the use of multilevel dominant eigenvector estimation (MDEE).<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<h2 id="examples">Examples</h2>
<h3 id="the-wiener-process">The Wiener process</h3>
<p>There are numerous equivalent characterizations of the <a href="Wiener_process" title="wikilink">Wiener process</a> which is a mathematical formalization of <a href="Brownian_motion" title="wikilink">Brownian motion</a>. Here we regard it as the centered standard Gaussian process <strong>W</strong><sub><em>t</em></sub> with covariance function</p>
<p><span class="LaTeX">$$K_{W}(t,s)  = \operatorname{Cov}(W_t,W_s) = \min (s,t).$$</span></p>
<p>We restrict the time domain to [<em>a</em>, <em>b</em>]=[0,1] without loss of generality.</p>
<p>The eigenvectors of the covariance kernel are easily determined. These are</p>
<p><span class="LaTeX">$$e_k(t) = \sqrt{2} \sin \left( \left(k - \tfrac{1}{2}\right) \pi t \right)$$</span> and the corresponding eigenvalues are</p>
<p><span class="LaTeX">$$\lambda_k = \frac{1}{(k -\frac{1}{2})^2 \pi^2}.$$</span></p>
<div class="NavFrame collapsed">
<div class="NavHead">
<p>[Proof]</p>
</div>
<div class="NavContent" style="text-align:left">
<p>In order to find the eigenvalues and eigenvectors, we need to solve the integral equation:</p>
<p><span class="LaTeX">$$\begin{align}
\int_a^b K_W(s,t) e(s) ds &=\lambda e(t)\qquad \forall t, 0\leq t\leq 1\\
\int_0^1 \min(s,t) e(s) ds &=\lambda e(t)\qquad \forall t, 0\leq t\leq 1 \\
\int_0^t s e(s) ds + t \int_t^1 e(s) ds &= \lambda e(t) \qquad \forall t, 0\leq t\leq 1
\end{align}$$</span></p>
<p>differentiating once with respect to <em>t</em> yields:</p>
<p><span class="LaTeX">$$\int_{t}^1 e(s) ds=\lambda e'(t)$$</span></p>
<p>a second differentiation produces the following differential equation:</p>
<p><span class="LaTeX">$$-e(t)=\lambda e''(t)$$</span></p>
<p>The general solution of which has the form:</p>
<p><span class="LaTeX">$$e(t)=A\sin\left(\frac{t}{\sqrt{\lambda}}\right)+B\cos\left(\frac{t}{\sqrt{\lambda}}\right)$$</span></p>
<p>where <em>A</em> and <em>B</em> are two constants to be determined with the boundary conditions. Setting <em>t</em>=0 in the initial integral equation gives <em>e</em>(0)=0 which implies that <em>B</em>=0 and similarly, setting <em>t</em>=1 in the first differentiation yields ''e' ''(1)=0, whence:</p>
<p><span class="LaTeX">$$\cos\left(\frac{1}{\sqrt{\lambda}}\right)=0$$</span></p>
<p>which in turn implies that eigenvalues of <em>T</em><sub><em>K</em><sub><em>X</em></sub></sub> are:</p>
<p><span class="LaTeX">$$\lambda_k=\left(\frac{1}{(k-\frac{1}{2})\pi}\right)^2,\qquad k\geq 1$$</span></p>
<p>The corresponding eigenfunctions are thus of the form:</p>
<p><span class="LaTeX">$$e_k(t)=A \sin\left((k-\frac{1}{2})\pi t\right),\qquad k\geq 1$$</span></p>
<p><em>A</em> is then chosen so as to normalize <em>e</em><sub><em>k</em></sub>:</p>
<p><span class="LaTeX">$$\int_0^1 e_k^2(t) dt=1\quad \implies\quad A=\sqrt{2}$$</span></p>
</div>
</div>
<p>This gives the following representation of the Wiener process:</p>
<p><strong>Theorem</strong>. There is a sequence {<em>Z</em><sub><em>i</em></sub>}<sub><em>i</em></sub> of independent Gaussian random variables with mean zero and variance 1 such that</p>
<p><span class="LaTeX">$$W_t = \sqrt{2} \sum_{k=1}^\infty Z_k \frac{\sin \left(\left(k - \frac{1}{2}\right) \pi t\right)}{ \left(k - \frac{1}{2}\right) \pi}.$$</span> Note that this representation is only valid for <span class="LaTeX">$t\in[0,1].$</span> On larger intervals, the increments are not independent. As stated in the theorem, convergence is in the L<sup>2</sup> norm and uniform in <em>t</em>.</p>
<h3 id="the-brownian-bridge">The Brownian bridge</h3>
<p>Similarly the <a href="Brownian_bridge" title="wikilink">Brownian bridge</a> <span class="LaTeX">$B_t=W_t-tW_1$</span> which is a <a href="stochastic_process" title="wikilink">stochastic process</a> with covariance function</p>
<p><span class="LaTeX">$$K_B(t,s)=\min(t,s)-ts$$</span> can be represented as the series</p>
<p><span class="LaTeX">$$B_t = \sum_{k=1}^\infty Z_k \frac{\sqrt{2} \sin(k \pi t)}{k \pi}$$</span></p>
<h2 id="applications">Applications</h2>
<p><a href="Adaptive_optics" title="wikilink">Adaptive optics</a> systems sometimes use K–L functions to reconstruct wave-front phase information (Dai 1996, JOSA A). Karhunen–Loève expansion is closely related to the <a href="Singular_Value_Decomposition" title="wikilink">Singular Value Decomposition</a>. The latter has myriad applications in image processing, radar, seismology, and the like. If one has independent vector observations from a vector valued stochastic process then the left singular vectors are <a href="maximum_likelihood" title="wikilink">maximum likelihood</a> estimates of the ensemble KL expansion.</p>
<h3 id="applications-in-signal-estimation-and-detection">Applications in signal estimation and detection</h3>
<h4 id="detection-of-a-known-continuous-signal-st">Detection of a known continuous signal S(t)</h4>
<p>In communication, we usually have to decide whether a signal from a noisy channel contains valuable information. The following hypothesis testing is used for detecting continuous signal s(t) from channel output X(t), N(t) is the channel noise, which is usually assumed zero mean gaussian process with correlation function <span class="LaTeX">$R_{N} (t, s) = E[N(t)N(s)]$</span></p>
<p><span class="LaTeX">$$H: X(t) = N(t),$$</span></p>
<p><span class="LaTeX">$$K: X(t) = N(t)+s(t), \quad t\in(0,T)$$</span></p>
<h4 id="signal-detection-in-white-noise">Signal detection in white noise</h4>
<p>When the channel noise is white, its correlation function is</p>
<p><span class="LaTeX">$$R_{N}(t) = \tfrac{1}{2} N_0 \delta (t),$$</span></p>
<p>and it has constant power spectrum density. In physically practical channel, the noise power is finite, so:</p>
<p><span class="LaTeX">$$S_{N}(f) = \begin{cases} \frac{N_0}{2} &|f|<w &="" 0="" \\="" |f|="">w \end{cases}</w>$$</span></p>
<p>Then the noise correlation function is sinc function with zeros at <span class="LaTeX">$\frac{n}{2\omega}, n \in \mathbf{Z}.$</span> Since are uncorrelated and gaussian, they are independent. Thus we can take samples from X(t) with time spacing</p>
<p><span class="LaTeX">$$\Delta t = \frac{n}{2\omega}$$</span> within (0,T).</p>
<p>Let <span class="LaTeX">$X_i = X(i\Delta t)$</span>. We have a total of <span class="LaTeX">$n = \frac{T}{\Delta t} = T(2\omega) = 2\omega T$</span> i.i.d samples <span class="LaTeX">$\{X_1, X_2,...,X_n\}$</span> to develop the likelihood-ratio test. Define signal <span class="LaTeX">$S_i = S(i\Delta t)$</span>, the problem becomes,</p>
<p><span class="LaTeX">$$H: X_i = N_i$$</span>,</p>
<p><span class="LaTeX">$$K: X_i = N_i + S_i, i = 1,2...n.$$</span></p>
<p>The log-likelihood ratio</p>
<p><span class="LaTeX">$$\mathcal{L}(\underline{x}) = \log\frac{\sum^n_{i=1} (2S_i x_i - S_i^2)}{2\sigma^2} \Leftrightarrow \Delta t \sum^n_{i = 1} S_i x_i = \sum^n_{i=1} S(i\Delta t)x(i\Delta t)\Delta t \gtrless \lambda_2$$</span>.</p>
<p>As <span class="LaTeX">$t → 0$</span>, let:</p>
<p><span class="LaTeX">$$G = \int^T_0 S(t)x(t)dt.$$</span></p>
<p>Then G is the test statistics and the <a href="Neyman–Pearson_lemma" title="wikilink">Neyman–Pearson optimum detector</a> is</p>
<p><span class="LaTeX">$$G(\underline{x}) > G_0 \Rightarrow K < G_0 \Rightarrow H.$$</span></p>
<p>As G is gaussian, we can characterize it by finding its mean and variances. Then we get</p>
<p><span class="LaTeX">$$H: G \sim N \left (0,\tfrac{1}{2}N_0E \right )$$</span></p>
<p><span class="LaTeX">$$K: G \sim N \left (E,\tfrac{1}{2}N_0E \right )$$</span></p>
<p>where</p>
<p><span class="LaTeX">$$\mathbf{E} = \int^T_{0} S^2(t)dt$$</span></p>
<p>is the signal energy.</p>
<p>The false alarm error</p>
<p><span class="LaTeX">$$\alpha = \int^{\infty}_{G_0} N \left (0, \tfrac{1}{2}N_0E \right )dG \Rightarrow G_0 = \sqrt{\tfrac{1}{2}N_0E} \Phi^{-1}(1-\alpha)$$</span></p>
<p>And the probability of detection:</p>
<p><span class="LaTeX">$$\beta = \int^{\infty}_{G_0} N \left (E, \tfrac{1}{2}N_0E \right )dG = 1-\Phi \left (\frac{G_0 - E}{\sqrt{\tfrac{1}{2}N_0 E}} \right ) = \Phi \left (\sqrt{\frac{2E}{N_0}} - \Phi^{-1}(1-\alpha) \right ),$$</span></p>
<p>where Φ is the cdf of standard normal gaussian variable.</p>
<h4 id="signal-detection-in-colored-noise">Signal detection in colored noise</h4>
<p>When N(t) is colored (correlated in time) gaussian noise with zero mean and covariance function <span class="LaTeX">$R_N(t,s) = E[X(t)X(s)],$</span> we cannot sample independent discrete observations by evenly spacing the time. Instead, we can use K–L expansion to uncorrelate the noise process and get independent gaussian observation 'samples'. The K–L expansion of N(t):</p>
<p><span class="LaTeX">$$N(t) = \sum^{\infty}_{i=1} N_i \Phi_i(t), \quad 0<t<t< math="">,

where <math>N_i =\int N(t)\Phi_i(t)dt$$</span> and the orthonormal bases <span class="LaTeX">$\{\Phi_i{t}\}$</span> are generated by kernel <span class="LaTeX">$R_N(t,s)$</span>, i.e., solution to</p>
<p><span class="LaTeX">$$\int ^T _0 R_N(t,s)\Phi_i(s)ds = \lambda_i \Phi_i(t), var[N_i] = \lambda_i$$</span>.</p>
<p>Do the expansion:</p>
<p><span class="LaTeX">$$S(t) = \sum^{\infty}_{i = 1}S_i\Phi_i(t)$$</span>,</p>
<p>where <span class="LaTeX">$S_i = \int^T _0 S(t)\Phi_i(t)dt, 0<t<t.< math="">, then

:<math>X_i = \int^T _0 X(t)\Phi_i(t) dt = N_i$</span></p>
<p>under H and <span class="LaTeX">$N_i + S_i$</span> under K. Let <span class="LaTeX">$\overline{X} = \{X_1,X_2,\dots\}$</span>, we have</p>
<p><span class="LaTeX">$$N_i$$</span> are independent gaussian r.v's with variance <span class="LaTeX">$\lambda_i$</span></p>
<dl>
<dd>under H<span class="LaTeX">$$\{X_i\}$$</span> are independent gaussian r.v's.
<p>:<math>f_H[x(t)|0<t<t] (-\frac{x_i^2}{2="" )</math></p>
</dd>
</dl>
<dl>
<dd>under K<span class="LaTeX">$$\{X_i - S_i\}$$</span> are independent gaussian r.v's.
<p>:<math>f_K[x(t)|0<t<t] (-\frac{(x_i="" )</math></p>
</dd>
</dl>
<p>Hence, the log-LR is given by</p>
<p><span class="LaTeX">$$\mathcal{L}(\underline{x}) = \sum^{\infty}_{i=1} \frac{2S_i x_i - S_i^2}{2\lambda_i}$$</span></p>
<p>and the optimum detector is</p>
<p><span class="LaTeX">$$G = \sum^{\infty}_{i=1} S_i x_i \lambda_i > G_0 \Rightarrow K, < G_0 \Rightarrow H.$$</span></p>
<p>Define</p>
<p><span class="LaTeX">$$k(t) = \sum^{\infty}_{i=1} \lambda_i S_i \Phi_i(t), 0<t<t,< math="">

then <math>G = \int^T _0 k(t)x(t)dt$$</span>.</p>
<h5 id="how-to-find-kt">How to find <em>k</em>(<em>t</em>)</h5>
<p>Since</p>
<p><span class="LaTeX">$$\int^T_0 R_N(t,s)k(s)ds = \sum^{\infty}_{i=1} \lambda_i S_i \int^T _0 R_N(t,s)\Phi_i (s) ds = \sum^{\infty}_{i=1} S_i \Phi_i(t) = S(t)$$</span>,</p>
<p>k(t) is the solution to</p>
<p><span class="LaTeX">$$\int^T_0 R_N(t,s)k(s)ds = S(t)$$</span>.</p>
<p>If N(t)is wide-sense stationary,</p>
<p><span class="LaTeX">$$\int^T_0 R_N(t-s)k(s)ds = S(t)$$</span>,</p>
<p>which is known as the <a href="Wiener–Hopf_equation" title="wikilink">Wiener–Hopf equation</a>. The equation can be solved by taking fourier transform, but not practically realizable since infinite spectrum needs spatial factorization. A special case which is easy to calculate k(t) is white gaussian noise.</p>
<p><span class="LaTeX">$$\int^T_0 \frac{N_0}{2}\delta(t-s)k(s)ds = S(t) \Rightarrow k(t) = C S(t), 0<t<t< math="">.

The corresponding impulse response is h(t) = k(T-t) = C S(T-t). Let C = 1, this is just the result we arrived at in previous section for detecting of signal in white noise.

=====Test threshold for Neyman–Pearson detector=====
Since X(t) is a gaussian process, 

:<math>G = \int^T_0 k(t)x(t)dt,$$</span></p>
<p>is a gaussian random variable that can be characterized by its mean and variance.</p>
<p><span class="LaTeX">$$\begin{align}
\mathbf{E}[G|H] &= \int^T_0 k(t)\mathbf{E}[x(t)|H]dt = 0 \\
\mathbf{E}[G|K] &= \int^T_0 k(t)\mathbf{E}[x(t)|K]dt = \int^T_0 k(t)S(t)dt \equiv \rho \\
\mathbf{E}[G^2|H] &= \int^T_0 \int^T_0 k(t)k(s) R_N(t,s)dtds = \int^T_0 k(t) \left (\int^T_0 k(s)R_N(t,s)ds \right )=\int^T_0 k(t)S(t)dt = \rho \\
\text{Var}[G|H] &= \mathbf{E}[G^2|H] - (\mathbf{E}[G|H])^2 = \rho \\
\mathbf{E}[G^2|K] &=\int^T_0\int^T_0k(t)k(s) \mathbf{E}[x(t)x(s)]dtds = \int^T_0\int^T_0k(t)k(s)(R_N(t,s) +S(t)S(s))dtds = \rho + \rho^2\\
\text{Var}[G|K] &= \mathbf{E}[G^2|K] - (\mathbf{E}[G|K])^2 = \rho + \rho^2 -\rho^2 = \rho
\end{align}$$</span></p>
<p>Hence, we obtain the distributions of <em>H</em> and <em>K</em>:</p>
<p><span class="LaTeX">$$H: G \sim N(0,\rho)$$</span></p>
<p><span class="LaTeX">$$K: G \sim N(\rho, \rho)$$</span></p>
<p>The false alarm error is</p>
<p><span class="LaTeX">$$\alpha = \int^{\infty}_{G_0} N(0,\rho)dG = 1 - \Phi \left (\frac{G_0}{\sqrt{\rho}} \right ).$$</span></p>
<p>So the test threshold for the Neyman–Pearson optimum detector is</p>
<p><span class="LaTeX">$$G_0 = \sqrt{\rho} \Phi^{-1} (1-\alpha)$$</span>.</p>
<p>Its power of detection is</p>
<p><span class="LaTeX">$$\beta = \int^{\infty}_{G_0} N(\rho, \rho)dG = \Phi \left (\sqrt{\rho} - \Phi^{-1}(1 - \alpha) \right )$$</span></p>
<p>When the noise is white gaussian process, the signal power is</p>
<p><span class="LaTeX">$$\rho = \int^T_0 k(t)S(t)dt = \int^T_0 S(t)^2 dt = E$$</span>.</p>
<h5 id="prewhitening">Prewhitening</h5>
<p>For some type of colored noise, a typical practise is to add a prewhitening filter before the matched filter to transform the colored noise into white noise. For example, N(t) is a wide-sense stationary colored noise with correlation function</p>
<p><span class="LaTeX">$$R_N(\tau) = \frac{B N_0}{4} e^{-B|\tau|}$$</span></p>
<p><span class="LaTeX">$$S_N(f) = \frac{N_0}{2(1+(\frac{w}{B})^2)}$$</span></p>
<p>The transfer function of prewhitening filter is</p>
<p><span class="LaTeX">$$H(f) = 1 + j \frac{w}{B}.$$</span></p>
<h4 id="detection-of-a-gaussian-random-signal-in-additive-white-gaussian-noise-awgn">Detection of a gaussian random signal in <a href="Additive_white_Gaussian_noise" title="wikilink">Additive white Gaussian noise (AWGN)</a></h4>
<p>When the signal we want to detect from the noisy channel is also random, for example, a white gaussian process X(t), we can still implement K–L expansion to get independent sequence of observation. In this case, the detection problem is described as follows:</p>
<p><span class="LaTeX">$$H_0 : Y(t) = N(t)$$</span></p>
<p><span class="LaTeX">$$H_1 : Y(t) = N(t) + X(t), 0<t<t. <="" math="">

X(t) is a random process with correlation function <math>R_X(t,s) = E\{X[t]X[s]\}$$</span></p>
<p>The K–L expansion of X(t) is</p>
<p><span class="LaTeX">$$X(t) = \sum^{\infty}_{i=1} X_i \Phi_i(t)$$</span>,</p>
<p>where</p>
<p><span class="LaTeX">$$X_i =\int^T_0 X(t) \Phi_i(t). \Phi(t)$$</span></p>
<p>are solutions to</p>
<p><span class="LaTeX">$$\int^T_0 R_X(t,s)\Phi_i(s)ds= \lambda_i \Phi_i(t)$$</span>.</p>
<p>So <span class="LaTeX">$X_i$</span>'s are independent sequence of r.v's with zero mean and variance <span class="LaTeX">$\lambda_i$</span>. Expanding Y(t) and N(t) by <span class="LaTeX">$\Phi_i(t)$</span>, we get</p>
<p><span class="LaTeX">$$Y_i = \int^T_0 Y(t)\Phi_i(t)dt = \int^T_0 [N(t) + X(t)]\Phi_i(t) = N_i + X_i$$</span>,</p>
<p>where</p>
<p><span class="LaTeX">$$N_i = \int^T_0 N(t)\Phi_i(t)dt.$$</span></p>
<p>As N(t) is gaussian white noise, <span class="LaTeX">$N_i$</span>'s are i.i.d sequence of r.v with zero mean and variance <span class="LaTeX">$\tfrac{1}{2}N_0$</span>, then the problem is simplified as follows,</p>
<p><span class="LaTeX">$$H_0: Y_i = N_i$$</span></p>
<p><span class="LaTeX">$$H_1: Y_i = N_i + X_i$$</span></p>
<p>The Neyman–Pearson optimal test:</p>
<p><span class="LaTeX">$$\Lambda = \frac{f_Y|H_1}{f_Y|H_0} = Ce^{-\sum^{\infty}_{i=1} \frac{y_i^2}{2} \frac{\lambda_i}{\tfrac{1}{2} N_0 (\tfrac{1}{2}N_0 + \lambda_i)} },$$</span></p>
<p>so the log-likelihood ratio</p>
<p><span class="LaTeX">$$\mathcal{L} = \ln(\Lambda) = K -\sum^{\infty}_{i=1}\tfrac{1}{2}y_i^2  \frac{\lambda_i}{\frac{N_0}{2}(\frac{N_0}{2} + \lambda_i)}$$</span>.</p>
<p>Since</p>
<p><span class="LaTeX">$$\hat{X_i} = \frac{\lambda_i}{\frac{N_0}{2}(\frac{N_0}{2} + \lambda_i)}$$</span></p>
<p>is just the minimum-mean-square estimate of <span class="LaTeX">$X_i$</span> given <span class="LaTeX">$Y_i$</span>'s,</p>
<p><span class="LaTeX">$$\mathcal{L} = K + \frac{1}{N_0} \sum^{\infty}_{i=1} Y_i \hat{X_i}$$</span>.</p>
<p>K–L expansion has the following property: If</p>
<p><span class="LaTeX">$$f(t) = \sum f_i \Phi_i(t), g(t) = \sum g_i \Phi_i(t)$$</span>,</p>
<p>where</p>
<p><span class="LaTeX">$$f_i = \int_0^T f(t) \Phi_i(t), g_i = \int_0^T g(t)\Phi_i(t).$$</span>,</p>
<p>then</p>
<p><span class="LaTeX">$$\sum^{\infty}_{i=1} f_i g_i = \int^T_0 g(t)f(t)dt$$</span>.</p>
<p>So let</p>
<p><span class="LaTeX">$$\hat{X(t|T)} = \sum^{\infty}_{i=1} \hat{X_i}\Phi_i(t), \quad \mathcal{L} = K + \frac{1}{N_0} \int^T_0 Y(t) \hat{X(t|T)}dt$$</span>.</p>
<p>Noncausal filter Q(t, s) can be used to get the estimate through</p>
<p><span class="LaTeX">$$\hat{X(t|T)} = \int^T_0 Q(t,s)Y(s)ds$$</span>.</p>
<p>By <a href="orthogonality_principle" title="wikilink">orthogonality principle</a>, Q(t,s) satisfies</p>
<p><span class="LaTeX">$$\int^T_0 Q(t,s)R_X(s,t)ds + \tfrac{N_0}{2} Q(t, \lambda) = R_X(t, \lambda), 0 < \lambda < T, 0<t<t. <="" math="">.

However for practical reason, it's necessary to further derive the causal filter h(t, s), where h(t, s) = 0 for s > t, to get estimate <math>\hat{X(t|t)}$$</span>. Specifically,</p>
<p><span class="LaTeX">$$Q(t,s) = h(t,s) + h(s, t) - \int^T_0 h(\lambda, t)h(s, \lambda)d\lambda$$</span>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Principal_component_analysis" title="wikilink">Principal component analysis</a></li>
<li><a href="Proper_orthogonal_decomposition" title="wikilink">Proper orthogonal decomposition</a></li>
<li><a href="Polynomial_chaos" title="wikilink">Polynomial chaos</a></li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li>Wu B., Zhu J., Najm F.(2005) "A Non-parametric Approach for Dynamic Range Estimation of Nonlinear Systems". In Proceedings of Design Automation Conference(841-844) 2005</li>
<li>Wu B., Zhu J., Najm F.(2006) "Dynamic Range Estimation". IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, Vol. 25 Issue:9 (1618-1636) 2006</li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><em>Mathematica</em> <a href="http://reference.wolfram.com/mathematica/ref/KarhunenLoeveDecomposition.html">KarhunenLoeveDecomposition</a> function.</li>
<li><em>E161: Computer Image Processing and Analysis</em> notes by Pr. Ruye Wang at <a href="Harvey_Mudd_College" title="wikilink">Harvey Mudd College</a> <a href="http://fourier.eng.hmc.edu/e161/lectures/klt/klt.html">1</a></li>
</ul>
<p><a href="fr:Transformée_de_Karhunen-Loève" title="wikilink">fr:Transformée de Karhunen-Loève</a>"</p>
<p><a href="Category:Estimation_theory" title="wikilink">Category:Estimation theory</a> <a href="Category:Probability_theorems" title="wikilink">Category:Probability theorems</a> <a href="Category:Signal_processing" title="wikilink">Category:Signal processing</a> <a href="Category:Stochastic_processes" title="wikilink">Category:Stochastic processes</a> <a href="Category:Statistical_theorems" title="wikilink">Category:Statistical theorems</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2">.<a href="#fnref2">↩</a></li>
<li id="fn3">A wavelet tour of signal processing-Stéphane Mallat<a href="#fnref3">↩</a></li>
<li id="fn4">X. Tang, “Texture information in run-length matrices,” IEEE Transactions on Image Processing, vol. 7, No. 11, pp. 1602- 1609, Nov. 1998<a href="#fnref4">↩</a></li>
</ol>
</section>
</body>
</html>
