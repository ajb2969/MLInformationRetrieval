   Eigendecomposition of a matrix      Eigendecomposition of a matrix   In the mathematical  discipline of linear algebra , eigendecomposition or sometimes spectral decomposition is the factorization of a matrix into a canonical form , whereby the matrix is represented in terms of its eigenvalues and eigenvectors . Only diagonalizable matrices can be factorized in this way.  Fundamental theory of matrix eigenvectors and eigenvalues  A (non-zero) vector v of dimension N is an eigenvector of a square ( N × N ) matrix A  if and only if it satisfies the linear equation      𝐀𝐯  =   λ  𝐯       𝐀𝐯    λ  𝐯     \mathbf{A}\mathbf{v}=\lambda\mathbf{v}   where λ is a scalar, termed the eigenvalue corresponding to v . That is, the eigenvectors are the vectors that the linear transformation A merely elongates or shrinks, and the amount that they elongate/shrink by is the eigenvalue. The above equation is called the eigenvalue equation or the eigenvalue problem .  This yields an equation for the eigenvalues       p   (  λ  )    :=   det   (   𝐀  -   λ  𝐈    )    =   0.        assign    p  λ       𝐀    λ  𝐈          0.     p\left(\lambda\right):=\det\left(\mathbf{A}-\lambda\mathbf{I}\right)=0.\!   We call p ( λ ) the characteristic polynomial , and the equation, called the characteristic equation , is an N th order polynomial equation in the unknown λ . This equation will have N λ distinct solutions, where 1 ≤ N λ ≤ N . The set of solutions, that is, the eigenvalues, is called the spectrum of A . 1 2 3  We can factor  p as       p   (  λ  )    =     (   λ  -   λ  1    )    n  1      (   λ  -   λ  2    )    n  2    ⋯    (   λ  -   λ  k    )    n  k     =   0.           p  λ      superscript    λ   subscript  λ  1     subscript  n  1     superscript    λ   subscript  λ  2     subscript  n  2    normal-⋯   superscript    λ   subscript  λ  k     subscript  n  k          0.     p\left(\lambda\right)=(\lambda-\lambda_{1})^{n_{1}}(\lambda-\lambda_{2})^{n_{2%
 }}\cdots(\lambda-\lambda_{k})^{n_{k}}=0.\!   The integer n i is termed the algebraic multiplicity of eigenvalue λ i . The algebraic multiplicities sum to N :         ∑   i  =  1    N  λ     n  i    =  N   .        superscript   subscript     i  1     subscript  N  λ     subscript  n  i    N    \sum\limits_{i=1}^{N_{\lambda}}{n_{i}}=N.     For each eigenvalue, λ i , we have a specific eigenvalue equation        (   𝐀  -    λ  i   𝐈    )   𝐯   =   0.           𝐀     subscript  λ  i   𝐈    𝐯   0.    \left(\mathbf{A}-\lambda_{i}\mathbf{I}\right)\mathbf{v}=0.\!   There will be 1 ≤ m i ≤ n i  linearly independent solutions to each eigenvalue equation. The m i solutions are the eigenvectors associated with the eigenvalue λ i . The integer m i is termed the geometric multiplicity of λ i . It is important to keep in mind that the algebraic multiplicity n i and geometric multiplicity m i may or may not be equal, but we always have m i ≤ n i . The simplest case is of course when m i = n i = 1. The total number of linearly independent eigenvectors, N v , can be calculated by summing the geometric multiplicities         ∑   i  =  1    N  λ     m  i    =   N  𝐯    .        superscript   subscript     i  1     subscript  N  λ     subscript  m  i     subscript  N  𝐯     \sum\limits_{i=1}^{N_{\lambda}}{m_{i}}=N_{\mathbf{v}}.   The eigenvectors can be indexed by eigenvalues, i.e. using a double index, with v i , j being the j th eigenvector for the i th eigenvalue. The eigenvectors can also be indexed using the simpler notation of a single index v k , with k = 1, 2, ..., N v .  Eigendecomposition of a matrix  Let A be a square ( N × N ) matrix with N  linearly independent eigenvectors,      q  i     (  i  =  1  ,  …  ,  N  )   .     fragments   subscript  q  i    fragments  normal-(  i   1  normal-,  normal-…  normal-,  N  normal-)   normal-.    q_{i}\,\,(i=1,\dots,N).   Then A can be factorized as      𝐀  =   𝐐  𝚲   𝐐   -  1         𝐀    𝐐  𝚲   superscript  𝐐    1       \mathbf{A}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1}   where Q is the square ( N × N ) matrix whose i th column is the eigenvector    q  i     subscript  q  i    q_{i}   of A and Λ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues, i.e. ,     Λ   i  i    =   λ  i        subscript  normal-Λ    i  i     subscript  λ  i     \Lambda_{ii}=\lambda_{i}   . Note that only diagonalizable matrices can be factorized in this way. For example, the defective matrix     (     1    1      0    1     )      1  1    0  1     \begin{pmatrix}1&1\\
 0&1\\
 \end{pmatrix}   cannot be diagonalized.  The eigenvectors      q  i     (  i  =  1  ,  …  ,  N  )      fragments   subscript  q  i    fragments  normal-(  i   1  normal-,  normal-…  normal-,  N  normal-)     q_{i}\,\,(i=1,\dots,N)   are usually normalized, but they need not be. A non-normalized set of eigenvectors,      v  i     (  i  =  1  ,  …  ,  N  )   ,     fragments   subscript  v  i    fragments  normal-(  i   1  normal-,  normal-…  normal-,  N  normal-)   normal-,    v_{i}\,\,(i=1,\dots,N),   can also be used as the columns of Q . That can be understood by noting that the magnitude of the eigenvectors in Q gets canceled in the decomposition by the presence of Q −1 .  Example  Taking a 2 × 2 real matrix    𝐀  =   [     1    0      1    3     ]       𝐀    1  0    1  3      \mathbf{A}=\begin{bmatrix}1&0\\
 1&3\\
 \end{bmatrix}   as an example to be decomposed into a diagonal matrix through multiplication of a non-singular matrix    𝐁  =   [     a    b      c    d     ]   ∈   ℝ   2  ×  2          𝐁    a  b    c  d          superscript  ℝ    2  2       \mathbf{B}=\begin{bmatrix}a&b\\
 c&d\\
 \end{bmatrix}\in\mathbb{R}^{2\times 2}   .  Then         [     a    b      c    d     ]    -  1     [     1    0      1    3     ]    [     a    b      c    d     ]    =   [     x    0      0    y     ]          superscript    a  b    c  d      1      1  0    1  3      a  b    c  d       x  0    0  y      \begin{bmatrix}a&b\\
 c&d\\
 \end{bmatrix}^{-1}\begin{bmatrix}1&0\\
 1&3\\
 \end{bmatrix}\begin{bmatrix}a&b\\
 c&d\\
 \end{bmatrix}=\begin{bmatrix}x&0\\
 0&y\\
 \end{bmatrix}   , for some real diagonal matrix    [     x    0      0    y     ]      x  0    0  y     \begin{bmatrix}x&0\\
 0&y\\
 \end{bmatrix}   .  Shifting   𝐁   𝐁   \mathbf{B}   to the right hand side:        [     1    0      1    3     ]    [     a    b      c    d     ]    =    [     a    b      c    d     ]    [     x    0      0    y     ]            1  0    1  3      a  b    c  d         a  b    c  d      x  0    0  y       \begin{bmatrix}1&0\\
 1&3\\
 \end{bmatrix}\begin{bmatrix}a&b\\
 c&d\\
 \end{bmatrix}=\begin{bmatrix}a&b\\
 c&d\\
 \end{bmatrix}\begin{bmatrix}x&0\\
 0&y\\
 \end{bmatrix}     The above equation can be decomposed into 2 simultaneous equations:      {        [     1    0      1    3     ]    [     a      c     ]    =   [      a  x        c  x      ]            [     1    0      1    3     ]    [     b      d     ]    =   [      b  y        d  y      ]           cases        1  0    1  3      a    c         a  x       c  x      otherwise        1  0    1  3      b    d         b  y       d  y      otherwise    \begin{cases}\begin{bmatrix}1&0\\
 1&3\end{bmatrix}\begin{bmatrix}a\\
 c\end{bmatrix}=\begin{bmatrix}ax\\
 cx\end{bmatrix}\\
 \begin{bmatrix}1&0\\
 1&3\end{bmatrix}\begin{bmatrix}b\\
 d\end{bmatrix}=\begin{bmatrix}by\\
 dy\end{bmatrix}\end{cases}     Factoring out the eigenvalues    x   x   x   and   y   y   y   :      {        [     1    0      1    3     ]    [     a      c     ]    =   x   [     a      c     ]             [     1    0      1    3     ]    [     b      d     ]    =   y   [     b      d     ]            cases        1  0    1  3      a    c       x    a    c      otherwise        1  0    1  3      b    d       y    b    d      otherwise    \begin{cases}\begin{bmatrix}1&0\\
 1&3\end{bmatrix}\begin{bmatrix}a\\
 c\end{bmatrix}=x\begin{bmatrix}a\\
 c\end{bmatrix}\\
 \begin{bmatrix}1&0\\
 1&3\end{bmatrix}\begin{bmatrix}b\\
 d\end{bmatrix}=y\begin{bmatrix}b\\
 d\end{bmatrix}\end{cases}     Letting      a  →   =   [     a      c     ]    ,    b  →   =   [     b      d     ]       formulae-sequence     normal-→  a     a    c        normal-→  b     b    d       \overrightarrow{a}=\begin{bmatrix}a\\
 c\end{bmatrix},\overrightarrow{b}=\begin{bmatrix}b\\
 d\end{bmatrix}   , this gives us two vector equations:      {       A   a  →    =   x   a  →            A   b  →    =   y   b  →            cases      A   normal-→  a      x   normal-→  a     otherwise      A   normal-→  b      y   normal-→  b     otherwise    \begin{cases}A\overrightarrow{a}=x\overrightarrow{a}\\
 A\overrightarrow{b}=y\overrightarrow{b}\end{cases}     And can be represented by a single vector equation involving 2 solutions as eigenvalues:      𝐀𝐮  =   λ  𝐮       𝐀𝐮    λ  𝐮     \mathbf{A}\mathbf{u}=\lambda\mathbf{u}     where   λ   λ   \lambda   represents the two eigenvalues   x   x   x   and   y   y   y   ,   𝐮   𝐮   \mathbf{u}   represents the vectors    a  →     normal-→  a    \overrightarrow{a}   and    b  →     normal-→  b    \overrightarrow{b}   .  Shifting    λ  𝐮      λ  𝐮    \lambda\mathbf{u}   to the left hand side and factorizing   𝐮   𝐮   \mathbf{u}   out        (   𝐀  -   λ  𝐈    )   𝐮   =  0          𝐀    λ  𝐈    𝐮   0    (\mathbf{A}-\lambda\mathbf{I})\mathbf{u}=0     Since   𝐁   𝐁   \mathbf{B}   is non-singular, it is essential that   𝐮   𝐮   \mathbf{u}   is non-zero. Therefore,       (   𝐀  -   λ  𝐈    )   =  𝟎        𝐀    λ  𝐈    0    (\mathbf{A}-\lambda\mathbf{I})=\mathbf{0}     Considering the determinant of    (   𝐀  -   λ  𝐈    )      𝐀    λ  𝐈     (\mathbf{A}-\lambda\mathbf{I})   ,       [      1  -  λ     0      1     3  -  λ      ]   =  0          1  λ   0    1    3  λ     0    \begin{bmatrix}1-\lambda&0\\
 1&3-\lambda\end{bmatrix}=0     Thus        (   1  -  λ   )    (   3  -  λ   )    =  0          1  λ     3  λ    0    (1-\lambda)(3-\lambda)=0     Giving us the solutions of the eigenvalues for the matrix   𝐀   𝐀   \mathbf{A}   as    λ  =  1      λ  1    \lambda=1   or    λ  =  3      λ  3    \lambda=3   , and the resulting diagonal matrix from the eigendecomposition of   𝐀   𝐀   \mathbf{A}   is thus    [     1    0      0    3     ]      1  0    0  3     \begin{bmatrix}1&0\\
 0&3\end{bmatrix}   .  Putting the solutions back into the above simultaneous equations      {        [     1    0      1    3     ]    [     a      c     ]    =   1   [     a      c     ]             [     1    0      1    3     ]    [     b      d     ]    =   3   [     b      d     ]            cases        1  0    1  3      a    c       1    a    c      otherwise        1  0    1  3      b    d       3    b    d      otherwise    \begin{cases}\begin{bmatrix}1&0\\
 1&3\end{bmatrix}\begin{bmatrix}a\\
 c\end{bmatrix}=1\begin{bmatrix}a\\
 c\end{bmatrix}\\
 \begin{bmatrix}1&0\\
 1&3\end{bmatrix}\begin{bmatrix}b\\
 d\end{bmatrix}=3\begin{bmatrix}b\\
 d\end{bmatrix}\end{cases}     Solving the equations, we have     a  =   -   2  c     ,   a  ∈  ℝ      formulae-sequence    a      2  c       a  ℝ     a=-2c,a\in\mathbb{R}   and     b  =  0   ,   d  ∈  ℝ      formulae-sequence    b  0     d  ℝ     b=0,d\in\mathbb{R}     Thus the matrix   𝐁   𝐁   \mathbf{B}   required for the eigendecomposition of   𝐀   𝐀   \mathbf{A}   is      [      -   2  c      0      c    d     ]   ,   [  c  ,  d  ]    ∈  ℝ             2  c    0    c  d     c  d    ℝ    \begin{bmatrix}-2c&0\\
 c&d\end{bmatrix},[c,d]\in\mathbb{R}   . i.e. :          [      -   2  c      0      c    d     ]    -  1     [     1    0      1    3     ]    [      -   2  c      0      c    d     ]    =   [     1    0      0    3     ]    ,    [  c  ,  d  ]   ∈  ℝ      formulae-sequence       superscript        2  c    0    c  d      1      1  0    1  3          2  c    0    c  d       1  0    0  3        c  d   ℝ     \begin{bmatrix}-2c&0\\
 c&d\\
 \end{bmatrix}^{-1}\begin{bmatrix}1&0\\
 1&3\\
 \end{bmatrix}\begin{bmatrix}-2c&0\\
 c&d\\
 \end{bmatrix}=\begin{bmatrix}1&0\\
 0&3\\
 \end{bmatrix},[c,d]\in\mathbb{R}     Matrix inverse via eigendecomposition  If matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is nonsingular and its inverse is given by       𝐀   -  1    =   𝐐   𝚲   -  1     𝐐   -  1          superscript  𝐀    1      𝐐   superscript  𝚲    1     superscript  𝐐    1       \mathbf{A}^{-1}=\mathbf{Q}\mathbf{\Lambda}^{-1}\mathbf{Q}^{-1}   Furthermore, because Λ is a diagonal matrix , its inverse is easy to calculate:        [   Λ   -  1    ]    i  i    =   1   λ  i         subscript   delimited-[]   superscript  normal-Λ    1       i  i      1   subscript  λ  i      \left[\Lambda^{-1}\right]_{ii}=\frac{1}{\lambda_{i}}     Practical implications 4  When eigendecomposition is used on a matrix of measured, real data , the inverse may be less valid when all eigenvalues are used unmodified in the form above. This is because as eigenvalues become relatively small, their contribution to the inversion is large. Those near zero or at the "noise" of the measurement system will have undue influence and could hamper solutions (detection) using the inverse.  Two mitigations have been proposed: 1) truncating small/zero eigenvalues, 2) extending the lowest reliable eigenvalue to those below it.  The first mitigation method is similar to a sparse sample of the original matrix, removing components that are not considered valuable. However, if the solution or detection process is near the noise level, truncating may remove components that influence the desired solution.  The second mitigation extends the eigenvalue so that lower values have much less influence over inversion, but do still contribute, such that solutions near the noise will still be found.  The reliable eigenvalue can be found by assuming that eigenvalues of extremely similar and low value are a good representation of measurement noise (which is assumed low for most systems).  If the eigenvalues are rank-sorted by value, then the reliable eigenvalue can be found by minimization of the Laplacian of the sorted eigenvalues: 5      min   |    ∇  2    λ  s    |           superscript  normal-∇  2    subscript  λ  s       \min|\nabla^{2}\lambda_{s}|     where the eigenvalues are subscripted with an 's' to denote being sorted. The position of the minimization is the lowest reliable eigenvalue. In measurement systems, the square root of this reliable eigenvalue is the average noise over the components of the system.  Functional calculus  The eigendecomposition allows for much easier computation of power series of matrices. If f ( x ) is given by       f   (  x  )    =    a  0   +    a  1   x   +    a  2    x  2    +  ⋯         f  x      subscript  a  0      subscript  a  1   x      subscript  a  2    superscript  x  2    normal-⋯     f(x)=a_{0}+a_{1}x+a_{2}x^{2}+\cdots   then we know that       f   (  𝐀  )    =   𝐐  f   (  𝚲  )    𝐐   -  1           f  𝐀     𝐐  f  𝚲   superscript  𝐐    1       f\left(\mathbf{A}\right)=\mathbf{Q}f\left(\mathbf{\Lambda}\right)\mathbf{Q}^{-1}   Because Λ is a diagonal matrix , functions of Λ are very easy to calculate:        [   f   (  𝚲  )    ]    i  i    =   f   (   λ  i   )         subscript   delimited-[]    f  𝚲      i  i      f   subscript  λ  i      \left[f\left(\mathbf{\Lambda}\right)\right]_{ii}=f\left(\lambda_{i}\right)   The off-diagonal elements of f ( Λ ) are zero; that is, f ( Λ ) is also a diagonal matrix. Therefore, calculating f ( A ) reduces to just calculating the function on each of the eigenvalues .  A similar technique works more generally with the holomorphic functional calculus , using       𝐀   -  1    =   𝐐   𝚲   -  1     𝐐   -  1          superscript  𝐀    1      𝐐   superscript  𝚲    1     superscript  𝐐    1       \mathbf{A}^{-1}=\mathbf{Q}\mathbf{\Lambda}^{-1}\mathbf{Q}^{-1}   from above . Once again, we find that        [   f   (  𝚲  )    ]    i  i    =   f   (   λ  i   )         subscript   delimited-[]    f  𝚲      i  i      f   subscript  λ  i      \left[f\left(\mathbf{\Lambda}\right)\right]_{ii}=f\left(\lambda_{i}\right)     Examples       𝐀  2   =    (   𝐐  𝚲   𝐐   -  1     )    (   𝐐  𝚲   𝐐   -  1     )    =   𝐐  𝚲   (    𝐐   -  1    𝐐   )   𝚲   𝐐   -  1     =   𝐐   𝚲  2    𝐐   -  1            superscript  𝐀  2       𝐐  𝚲   superscript  𝐐    1       𝐐  𝚲   superscript  𝐐    1             𝐐  𝚲     superscript  𝐐    1    𝐐   𝚲   superscript  𝐐    1            𝐐   superscript  𝚲  2    superscript  𝐐    1        \mathbf{A}^{2}=(\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1})(\mathbf{Q}\mathbf{%
 \Lambda}\mathbf{Q}^{-1})=\mathbf{Q}\mathbf{\Lambda}(\mathbf{Q}^{-1}\mathbf{Q})%
 \mathbf{\Lambda}\mathbf{Q}^{-1}=\mathbf{Q}\mathbf{\Lambda}^{2}\mathbf{Q}^{-1}          𝐀  n   =   𝐐   𝚲  n    𝐐   -  1          superscript  𝐀  n     𝐐   superscript  𝚲  n    superscript  𝐐    1       \mathbf{A}^{n}=\mathbf{Q}\mathbf{\Lambda}^{n}\mathbf{Q}^{-1}     Decomposition for special matrices  Normal matrices  A complex normal matrix (      A  *   A   =   A   A  *           superscript  A    A     A   superscript  A       A^{*}A=AA^{*}   ) has an orthogonal eigenvector basis, so a normal matrix can be decomposed as      𝐀  =   𝐔  𝚲   𝐔  *        𝐀    𝐔  𝚲   superscript  𝐔       \mathbf{A}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{*}   where U is a unitary matrix . Further, if A is Hermitian (    A  =   A  *       A   superscript  A      A=A^{*}   ), which implies that it is also complex normal, the diagonal matrix Λ has only real values, and if A is unitary, Λ takes all its values on the complex unit circle.  Real symmetric matrices  As a special case, for every N × N real symmetric matrix , the eigenvectors can be chosen such that they are real, orthogonal to each other and have norm one. Thus a real symmetric matrix A can be decomposed as      𝐀  =   𝐐  𝚲   𝐐  T        𝐀    𝐐  𝚲   superscript  𝐐  T      \mathbf{A}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{T}   where Q is an orthogonal matrix , and Λ is a diagonal matrix whose entries are the eigenvalues of A.  Useful facts  Useful facts regarding eigenvalues   The product of the eigenvalues is equal to the determinant of A        det   (  𝐀  )    =    ∏   i  =  1    N  λ      λ  i   n  i            𝐀     superscript   subscript  product    i  1     subscript  N  λ     superscript   subscript  λ  i    subscript  n  i       \det\left(\mathbf{A}\right)=\prod\limits_{i=1}^{N_{\lambda}}{\lambda_{i}^{n_{i%
 }}}\!   Note that each eigenvalue is raised to the power n i , the algebraic multiplicity .   The sum of the eigenvalues is equal to the trace of A        tr   (  𝐀  )    =    ∑   i  =  1    N  λ      n  i     λ  i           tr  𝐀     superscript   subscript     i  1     subscript  N  λ       subscript  n  i    subscript  λ  i       \operatorname{tr}\left(\mathbf{A}\right)=\sum\limits_{i=1}^{N_{\lambda}}{{n_{i%
 }}\lambda_{i}}\!   Note that each eigenvalue is multiplied by n i , the algebraic multiplicity .   If the eigenvalues of A are λ i , and A is invertible, then the eigenvalues of A −1 are simply λ i −1 .  If the eigenvalues of A are λ i , then the eigenvalues of f ( A ) are simply f (λ i ), for any holomorphic function  f .   Useful facts regarding eigenvectors   If A is Hermitian and full-rank, the basis of eigenvectors may be chosen to be mutually orthogonal . The eigenvalues are real.  The eigenvectors of A −1 are the same as the eigenvectors of A .  Eigenvectors are defined up to a phase, i.e. if     A  v   =   λ  v         A  v     λ  v     Av=\lambda v   then     e   i  θ    v       superscript  e    i  θ    v    e^{i\theta}v   is also an eigenvector, and specifically so is    -  v      v    -v   .  In the case of degenerate eigenvalues (and eigenvalue appearing more than once), the eigenvectors have an additional freedom of rotation, i.e. any linear (orthonormal) combination of eigenvectors sharing an eigenvalue (i.e. in the degenerate sub-space), are themselves eigenvectors (i.e. in the subspace).   Useful facts regarding eigendecomposition   A can be eigendecomposed if and only if        N  𝐯   =   N        subscript  N  𝐯   N    N_{\mathbf{v}}=N\,      If p (λ) has no repeated roots, i.e. N λ = N , then A can be eigendecomposed.    The statement " A can be eigendecomposed" does not imply that A has an inverse.    The statement " A has an inverse" does not imply that A can be eigendecomposed.   Useful facts regarding matrix inverse      𝐀   𝐀   \mathbf{A}   can be inverted if and only if        λ  i   ≠    0    ∀  i         subscript  λ  i     0   for-all  i      \lambda_{i}\neq 0\;\forall\,i      If     λ  i   ≠    0    ∀  i         subscript  λ  i     0   for-all  i      \lambda_{i}\neq 0\;\forall\,i    and      N  𝐯   =  N       subscript  N  𝐯   N    N_{\mathbf{v}}=N   , the inverse is given by        𝐀   -  1    =   𝐐   𝚲   -  1     𝐐   -  1          superscript  𝐀    1      𝐐   superscript  𝚲    1     superscript  𝐐    1       \mathbf{A}^{-1}=\mathbf{Q}\mathbf{\Lambda}^{-1}\mathbf{Q}^{-1}     Numerical computations  Numerical computation of eigenvalues  Suppose that we want to compute the eigenvalues of a given matrix. If the matrix is small, we can compute them symbolically using the characteristic polynomial . However, this is often impossible for larger matrices, in which case we must use a numerical method .  In practice, eigenvalues of large matrices are not computed using the characteristic polynomial. Computing the polynomial becomes expensive in itself, and exact (symbolic) roots of a high-degree polynomial can be difficult to compute and express: the Abel–Ruffini theorem implies that the roots of high-degree (5 or above) polynomials cannot in general be expressed simply using n th roots. Therefore, general algorithms to find eigenvectors and eigenvalues are iterative .  Iterative numerical algorithms for approximating roots of polynomials exist, such as Newton's method , but in general it is impractical to compute the characteristic polynomial and then apply these methods. One reason is that small round-off errors in the coefficients of the characteristic polynomial can lead to large errors in the eigenvalues and eigenvectors: the roots are an extremely ill-conditioned function of the coefficients. 6  A simple and accurate iterative method is the power method : a random vector   v   v   v   is chosen and a sequence of unit vectors is computed as        A  v    ∥   A  v   ∥    ,     A  2   v    ∥    A  2   v   ∥    ,     A  3   v    ∥    A  3   v   ∥    ,  …         A  v    norm    A  v          superscript  A  2   v    norm     superscript  A  2   v          superscript  A  3   v    norm     superscript  A  3   v     normal-…    \frac{Av}{\|Av\|},\frac{A^{2}v}{\|A^{2}v\|},\frac{A^{3}v}{\|A^{3}v\|},\dots   This sequence will almost always converge to an eigenvector corresponding to the eigenvalue of greatest magnitude, provided that v has a nonzero component of this eigenvector in the eigenvector basis (and also provided that there is only one eigenvalue of greatest magnitude). This simple algorithm is useful in some practical applications; for example, Google uses it to calculate the page rank of documents in their search engine. 7 Also, the power method is the starting point for many more sophisticated algorithms. For instance, by keeping not just the last vector in the sequence, but instead looking at the span of all the vectors in the sequence, one can get a better (faster converging) approximation for the eigenvector, and this idea is the basis of Arnoldi iteration . 8 Alternatively, the important QR algorithm is also based on a subtle transformation of a power method. 9  Numerical computation of eigenvectors  Once the eigenvalues are computed, the eigenvectors could be calculated by solving the equation        (   𝐀  -    λ  i   𝐈    )    𝐯   i  ,  j     =   0           𝐀     subscript  λ  i   𝐈     subscript  𝐯   i  j     0    \left(\mathbf{A}-\lambda_{i}\mathbf{I}\right)\mathbf{v}_{i,j}=0\!   using Gaussian elimination or any other method for solving matrix equations .  However, in practical large-scale eigenvalue methods, the eigenvectors are usually computed in other ways, as a byproduct of the eigenvalue computation. In power iteration , for example, the eigenvector is actually computed before the eigenvalue (which is typically computed by the Rayleigh quotient of the eigenvector). 10 In the QR algorithm for a Hermitian matrix (or any normal matrix ), the orthonormal eigenvectors are obtained as a product of the Q matrices from the steps in the algorithm. 11 (For more general matrices, the QR algorithm yields the Schur decomposition first, from which the eigenvectors can be obtained by a backsubstitution procedure. 12 ) For Hermitian matrices, the Divide-and-conquer eigenvalue algorithm is more efficient than the QR algorithm if both eigenvectors and eigenvalues are desired. 13  Additional topics  Generalized eigenspaces  Recall that the geometric multiplicity of an eigenvalue can be described as the dimension of the associated eigenspace, the nullspace of λI − A . The algebraic multiplicity can also be thought of as a dimension: it is the dimension of the associated generalized eigenspace (1st sense), which is the nullspace of the matrix (λI − A ) k for any sufficiently large k . That is, it is the space of generalized eigenvectors (1st sense), where a generalized eigenvector is any vector which eventually becomes 0 if λI − A is applied to it enough times successively. Any eigenvector is a generalized eigenvector, and so each eigenspace is contained in the associated generalized eigenspace. This provides an easy proof that the geometric multiplicity is always less than or equal to the algebraic multiplicity.  This usage should not be confused with the generalized eigenvalue problem described below.  Conjugate eigenvector  A conjugate eigenvector or coneigenvector is a vector sent after transformation to a scalar multiple of its conjugate, where the scalar is called the conjugate eigenvalue or coneigenvalue of the linear transformation. The coneigenvectors and coneigenvalues represent essentially the same information and meaning as the regular eigenvectors and eigenvalues, but arise when an alternative coordinate system is used. The corresponding equation is        A  v   =   λ   v  *     .        A  v     λ   superscript  v       Av=\lambda v^{*}.\,     For example, in coherent electromagnetic scattering theory, the linear transformation A represents the action performed by the scattering object, and the eigenvectors represent polarization states of the electromagnetic wave. In optics , the coordinate system is defined from the wave's viewpoint, known as the Forward Scattering Alignment (FSA), and gives rise to a regular eigenvalue equation, whereas in radar , the coordinate system is defined from the radar's viewpoint, known as the Back Scattering Alignment (BSA), and gives rise to a coneigenvalue equation.  Generalized eigenvalue problem  A generalized eigenvalue problem (2nd sense) is the problem of finding a vector v that obeys        A  𝐯   =   λ  B  𝐯          A  𝐯     λ  B  𝐯     A\mathbf{v}=\lambda B\mathbf{v}\quad\quad   where A and B are matrices. If v obeys this equation, with some λ, then we call v the generalized eigenvector of A and B (in the 2nd sense), and λ is called the generalized eigenvalue of A and B (in the 2nd sense) which corresponds to the generalized eigenvector v . The possible values of λ must obey the following equation       det   (   A  -   λ  B    )    =   0.           A    λ  B     0.    \det(A-\lambda B)=0.\,     In the case we can find    n  ∈  ℕ      n  ℕ    n\in\mathbb{N}   linearly independent vectors    {    𝐯  1    ,  …  ,   𝐯  n   }      subscript  𝐯  1   normal-…   subscript  𝐯  n     \{\mathbf{v}_{1}\ ,\dots,\mathbf{v}_{n}\}   so that for every    i  ∈   {  1  ,  …  ,  n  }       i   1  normal-…  n     i\in\{1,\dots,n\}   ,      A   𝐯  i    =    λ  i   B   𝐯  i           A   subscript  𝐯  i       subscript  λ  i   B   subscript  𝐯  i      A\mathbf{v}_{i}=\lambda_{i}B\mathbf{v}_{i}\quad   , where     λ  i   ∈  𝔽       subscript  λ  i   𝔽    \lambda_{i}\in\mathbb{F}   then we define the matrices P and D such that      P  =   (     |       |       𝐯  1     ⋯     𝐯  n       |       |     )   ≡   (       (   𝐯  1   )   1     ⋯      (   𝐯  n   )   1       ⋮       ⋮        (   𝐯  1   )   n     ⋯      (   𝐯  n   )   n      )         P    normal-|  absent  normal-|     subscript  𝐯  1   normal-⋯   subscript  𝐯  n     normal-|  absent  normal-|            subscript   subscript  𝐯  1   1   normal-⋯   subscript   subscript  𝐯  n   1     normal-⋮  absent  normal-⋮     subscript   subscript  𝐯  1   n   normal-⋯   subscript   subscript  𝐯  n   n        P=\begin{pmatrix}|&&|\\
 \mathbf{v}_{1}&\cdots&\mathbf{v}_{n}\\
 |&&|\\
 \end{pmatrix}\equiv\begin{pmatrix}(\mathbf{v}_{1})_{1}&\cdots&(\mathbf{v}_{n})%
 _{1}\\
 \vdots&&\vdots\\
 (\mathbf{v}_{1})_{n}&\cdots&(\mathbf{v}_{n})_{n}\\
 \end{pmatrix}           (  D  )    i  j    =   {       λ  i   ,       if  i   =  j        0  ,     else           subscript  D    i  j     cases   subscript  λ  i       if  i   j   0  else     (D)_{ij}=\begin{cases}\lambda_{i},&\text{if }i=j\\
 0,&\text{else}\end{cases}     Then the following equality holds      𝐀  =   𝐁𝐏𝐃𝐏   -  1        𝐀   superscript  𝐁𝐏𝐃𝐏    1      \mathbf{A}=\mathbf{B}\mathbf{P}\mathbf{D}\mathbf{P}^{-1}   And the proof is      𝐀𝐏  =   𝐀   (     |       |       𝐯  1     ⋯     𝐯  n       |       |     )    =   (     |       |       A   𝐯  1      ⋯     A   𝐯  n        |       |     )   =   (     |       |        λ  1   B   𝐯  1      ⋯      λ  n   B   𝐯  n        |       |     )   =    (     |       |       B   𝐯  1      ⋯     B   𝐯  n        |       |     )   𝐃   =  𝐁𝐏𝐃        𝐀𝐏    𝐀    normal-|  absent  normal-|     subscript  𝐯  1   normal-⋯   subscript  𝐯  n     normal-|  absent  normal-|            normal-|  absent  normal-|      A   subscript  𝐯  1    normal-⋯    A   subscript  𝐯  n      normal-|  absent  normal-|           normal-|  absent  normal-|       subscript  λ  1   B   subscript  𝐯  1    normal-⋯     subscript  λ  n   B   subscript  𝐯  n      normal-|  absent  normal-|             normal-|  absent  normal-|      B   subscript  𝐯  1    normal-⋯    B   subscript  𝐯  n      normal-|  absent  normal-|    𝐃        𝐁𝐏𝐃     \mathbf{A}\mathbf{P}=\mathbf{A}\begin{pmatrix}|&&|\\
 \mathbf{v}_{1}&\cdots&\mathbf{v}_{n}\\
 |&&|\\
 \end{pmatrix}=\begin{pmatrix}|&&|\\
 A\mathbf{v}_{1}&\cdots&A\mathbf{v}_{n}\\
 |&&|\\
 \end{pmatrix}=\begin{pmatrix}|&&|\\
 \lambda_{1}B\mathbf{v}_{1}&\cdots&\lambda_{n}B\mathbf{v}_{n}\\
 |&&|\\
 \end{pmatrix}=\begin{pmatrix}|&&|\\
 B\mathbf{v}_{1}&\cdots&B\mathbf{v}_{n}\\
 |&&|\\
 \end{pmatrix}\mathbf{D}=\mathbf{B}\mathbf{P}\mathbf{D}     And since P is invertible, we multiply the equation from the right by its inverse, finishing the proof.  The set of matrices of the form    A  −  λ  B      A  normal-−  λ  B    A−λB   , where   λ   λ   λ   is a complex number, is called a pencil ; the term matrix pencil can also refer to the pair ( A , B ) of matrices. 14 If B is invertible, then the original problem can be written in the form         B   -  1    A  𝐯   =   λ  𝐯           superscript  B    1    A  𝐯     λ  𝐯     B^{-1}A\mathbf{v}=\lambda\mathbf{v}\quad\quad   which is a standard eigenvalue problem. However, in most situations it is preferable not to perform the inversion, but rather to solve the generalized eigenvalue problem as stated originally. This is especially important if A and B are Hermitian matrices , since in this case     B   -  1    A       superscript  B    1    A    B^{-1}A   is not generally Hermitian and important properties of the solution are no longer apparent.  If A and B are Hermitian and B is a positive-definite matrix , the eigenvalues λ are real and eigenvectors v 1 and v 2 with distinct eigenvalues are B -orthogonal (      𝐯  1  *   B   𝐯  2    =  0         superscript   subscript  𝐯  1     B   subscript  𝐯  2    0    \mathbf{v}_{1}^{*}B\mathbf{v}_{2}=0   ). 15 Also, in this case it is guaranteed that there exists a basis of generalized eigenvectors (it is not a defective problem). 16 This case is sometimes called a Hermitian definite pencil or definite pencil . 17  See also   Matrix decomposition  List of matrices  Eigenvalue, eigenvector and eigenspace  Spectral theorem  Householder transformation  Frobenius covariant  Sylvester's formula  Eigenvalue perturbation   Notes  References           External links   Interactive program & tutorial of Spectral Decomposition .   "  Category:Linear algebra  Category:Matrix theory  Category:Matrix decompositions     ↩  ↩  ↩  ↩  ↩  ↩  Ipsen, Ilse, and Rebecca M. Wills, Analysis and Computation of Google's PageRank , 7th IMACS International Symposium on Iterative Methods in Scientific Computing, Fields Institute, Toronto, Canada, 5–8 May 2005. ↩      ↩   ↩  ↩       