   Run-time algorithm specialisation      Run-time algorithm specialisation   In computer science , run-time algorithm specialization is a methodology for creating efficient algorithms for costly computation tasks of certain kinds. The methodology originates in the field of automated theorem proving and, more specifically, in the Vampire theorem prover project.  The idea is inspired by the use of partial evaluation in optimising program translation. Many core operations in theorem provers exhibit the following pattern. Suppose that we need to execute some algorithm    𝑎𝑙𝑔   (  A  ,  B  )       𝑎𝑙𝑔   A  B     \mathit{alg}(A,B)   in a situation where a value of   A   A   A    is fixed for potentially many different values of    B   B   B   . In order to do this efficiently, we can try to find a specialization of   𝑎𝑙𝑔   𝑎𝑙𝑔   \mathit{alg}   for every fixed   A   A   A   , i.e., such an algorithm    𝑎𝑙𝑔  A     subscript  𝑎𝑙𝑔  A    \mathit{alg}_{A}   , that executing     𝑎𝑙𝑔  A    (  B  )        subscript  𝑎𝑙𝑔  A   B    \mathit{alg}_{A}(B)   is equivalent to executing    𝑎𝑙𝑔   (  A  ,  B  )       𝑎𝑙𝑔   A  B     \mathit{alg}(A,B)   .  The specialized algorithm may be more efficient than the generic one, since it can exploit some particular properties of the fixed value   A   A   A   . Typically,     𝑎𝑙𝑔  A    (  B  )        subscript  𝑎𝑙𝑔  A   B    \mathit{alg}_{A}(B)   can avoid some operations that    𝑎𝑙𝑔   (  A  ,  B  )       𝑎𝑙𝑔   A  B     \mathit{alg}(A,B)   would have to perform, if they are known to be redundant for this particular parameter   A   A   A   . In particular, we can often identify some tests that are true or false for   A   A   A   , unroll loops and recursion, etc.  Difference from partial evaluation  The key difference between run-time specialization and partial evaluation is that the values of   A   A   A   on which   𝑎𝑙𝑔   𝑎𝑙𝑔   \mathit{alg}   is specialised are not known statically, so the specialization takes place at run-time .  There is also an important technical difference. Partial evaluation is applied to algorithms explicitly represented as codes in some programming language. At run-time, we do not need any concrete representation of   𝑎𝑙𝑔   𝑎𝑙𝑔   \mathit{alg}   . We only have to imagine    𝑎𝑙𝑔   𝑎𝑙𝑔   \mathit{alg}    when we program the specialization procedure. All we need is a concrete representation of the specialized version    𝑎𝑙𝑔  A     subscript  𝑎𝑙𝑔  A    \mathit{alg}_{A}   . This also means that we cannot use any universal methods for specializing algorithms, which is usually the case with partial evaluation. Instead, we have to program a specialization procedure for every particular algorithm   𝑎𝑙𝑔   𝑎𝑙𝑔   \mathit{alg}   . An important advantage of doing so is that we can use some powerful ad hoc tricks exploiting peculiarities of   𝑎𝑙𝑔   𝑎𝑙𝑔   \mathit{alg}   and the representation of   A   A   A   and   B   B   B   , which are beyond the reach of any universal specialization methods.  Specialization with compilation  The specialized algorithm has to be represented in a form that can be interpreted. In many situations, usually when     𝑎𝑙𝑔  A    (  B  )        subscript  𝑎𝑙𝑔  A   B    \mathit{alg}_{A}(B)   is to be computed on many values   B   B   B   in a row, we can write    𝑎𝑙𝑔  A     subscript  𝑎𝑙𝑔  A    \mathit{alg}_{A}   as a code of a special abstract machine , and we often say that   A   A   A   is compiled . Then the code itself can be additionally optimized by answer-preserving transformations that rely only on the semantics of instructions of the abstract machine.  Instructions of the abstract machine can usually be represented as records. One field of such a record stores an integer tag that identifies the instruction type, other fields may be used for storing additional parameters of the instruction, for example a pointer to another instruction representing a label, if the semantics of the instruction requires a jump. All instructions of a code can be stored in an array, or list, or tree.  Interpretation is done by fetching instructions in some order, identifying their type and executing the actions associated with this type. In C or C++ we can use a switch statement to associate some actions with different instruction tags. Modern compilers usually compile a switch statement with integer labels from a narrow range rather efficiently by storing the address of the statement corresponding to a value   i   i   i   in the   i   i   i   -th cell of a special array. One can exploit this by taking values for instruction tags from a small interval of integers.  Data-and-algorithm specialization  There are situations when many instances of   A   A   A   are intended for long-term storage and the calls of    𝑎𝑙𝑔   (  A  ,  B  )       𝑎𝑙𝑔   A  B     \mathit{alg}(A,B)   occur with different   B   B   B   in an unpredictable order. For example, we may have to check    𝑎𝑙𝑔   (   A  1   ,   B  1   )       𝑎𝑙𝑔    subscript  A  1    subscript  B  1      \mathit{alg}(A_{1},B_{1})   first, then    𝑎𝑙𝑔   (   A  2   ,   B  2   )       𝑎𝑙𝑔    subscript  A  2    subscript  B  2      \mathit{alg}(A_{2},B_{2})   , then    𝑎𝑙𝑔   (   A  1   ,   B  3   )       𝑎𝑙𝑔    subscript  A  1    subscript  B  3      \mathit{alg}(A_{1},B_{3})   , and so on. In such circumstances, full-scale specialization with compilation may not be suitable due to excessive memory usage. However, we can sometimes find a compact specialized representation    A  ′     superscript  A  normal-′    A^{\prime}   for every   A   A   A   , that can be stored with, or instead of,   A   A   A   . We also define a variant    𝑎𝑙𝑔  ′     superscript  𝑎𝑙𝑔  normal-′    \mathit{alg}^{\prime}   that works on this representation and any call to    𝑎𝑙𝑔   (  A  ,  B  )       𝑎𝑙𝑔   A  B     \mathit{alg}(A,B)   is replaced by     𝑎𝑙𝑔  ′    (   A  ′   ,  B  )        superscript  𝑎𝑙𝑔  normal-′     superscript  A  normal-′   B     \mathit{alg}^{\prime}(A^{\prime},B)   , intended to do the same job faster.  See also   Psyco , a specializing run-time compiler for Python  multi-stage programming   References   A. Voronkov, "The Anatomy of Vampire: Implementing Bottom-Up Procedures with Code Trees", Journal of Automated Reasoning , 15(2), 1995 ( original idea )   Further reading   A. Riazanov and A. Voronkov, "Efficient Checking of Term Ordering Constraints", Proc. IJCAR 2004, Lecture Notes in Artificial Intelligence 3097, 2004 ( compact but self-contained illustration of the method )    A. Riazanov and A. Voronkov, Efficient Instance Retrieval with Standard and Relational Path Indexing , Information and Computation, 199(1-2), 2005 ( contains another illustration of the method )    A. Riazanov, "Implementing an Efficient Theorem Prover" , PhD thesis, The University of Manchester, 2003 ( contains the most comprehensive description of the method and many examples )   "  Category:Algorithms  Category:Software optimization   