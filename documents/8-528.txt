   Mean absolute difference      Mean absolute difference   The mean absolute difference is a measure of statistical dispersion equal to the average absolute difference of two independent values drawn from a probability distribution . A related statistic is the relative mean absolute difference , which is the mean absolute difference divided by the arithmetic mean , and equal to twice the Gini coefficient . The mean absolute difference is also known as the absolute mean difference (not to be confused with the absolute value of the mean signed difference ) and the Gini mean absolute difference . The mean absolute difference is sometimes denoted by Δ or as MD.  Definition  The mean absolute difference is defined as the "average" or "mean", formally the expected value , of the absolute difference of two random variables  X and Y  independently and identically distributed with the same (unknown) distribution henceforth called Q .       MD  :=   E   [   |   X  -  Y   |   ]     .     assign  MD    E   delimited-[]      X  Y        \mathrm{MD}:=E[|X-Y|].     Calculation  Specifically,   if Q has a discrete probability function  f ( y ), where y i , i = 1 to n , are the values with nonzero probabilities:          MD  =    ∑   i  =  1   n     ∑   j  =  1   n    f   (   y  i   )   f   (   y  j   )    |    y  i   -   y  j    |       .      MD    superscript   subscript     i  1    n     superscript   subscript     j  1    n     f   subscript  y  i   f   subscript  y  j        subscript  y  i    subscript  y  j          \mathrm{MD}=\sum_{i=1}^{n}\sum_{j=1}^{n}f(y_{i})f(y_{j})|y_{i}-y_{j}|.         if Q has a probability density function  f ( x ):          MD  =    ∫   -  ∞   ∞     ∫   -  ∞   ∞    f   (  x  )   f   (  y  )    |   x  -  y   |   d   x   d  y      .      MD    superscript   subscript            superscript   subscript            f  x  f  y      x  y    d  x  d  y       \mathrm{MD}=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x)\,f(y)\,|x-y|\,%
 dx\,dy.         if Q has a cumulative distribution function  F ( x ) with quantile function  F ( x ):          MD  =    ∫  0  1     ∫  0  1     |    F   (  x  )    -   F   (  y  )     |   d   x   d  y      .      MD    superscript   subscript   0   1     superscript   subscript   0   1           F  x     F  y     d  x  d  y       \mathrm{MD}=\int_{0}^{1}\int_{0}^{1}|F(x)-F(y)|\,dx\,dy.        For a random sample of size n of a population distributed according to Q , the (empirical) mean absolute difference of the sequence of sample values y i , i = 1 to n can be caluated as the arithmetic mean of the absolute value of all possible differences:       MD  =    1   n  2      ∑   i  =  1   n     ∑   j  =  1   n    |    y  i   -   y  j    |       .      MD      1   superscript  n  2      superscript   subscript     i  1    n     superscript   subscript     j  1    n        subscript  y  i    subscript  y  j          \mathrm{MD}=\frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}|y_{i}-y_{j}|.     Relative mean absolute difference  When the probability distribution has a finite and nonzero arithmetic mean , the relative mean absolute difference, sometimes denoted by Δ or RMD, is defined by        R  M  D   =    M  D   arithmetic mean    .        R  M  D       M  D   arithmetic mean     RMD=\frac{MD}{\mbox{arithmetic mean}}.     The relative mean absolute difference quantifies the mean absolute difference in comparison to the size of the mean and is a dimensionless quantity. The relative mean absolute difference is equal to twice the Gini coefficient which is defined in terms of the Lorenz curve . This relationship gives complementary perspectives to both the relative mean absolute difference and the Gini coefficient, including alternative ways of calculating their values.  Properties  The mean absolute difference is invariant to translations and negation, and varies proportionally to positive scaling. That is to say, if X is a random variable and c is a constant:   MD( X '' + c ) = MD( X ''),  MD(− X ) = MD( X ), and  MD( c  X ) = | c | MD( X ).   The relative mean absolute difference is invariant to positive scaling, commutes with negation, and varies under translation in proportion to the ratio of the original and translated arithmetic means. That is to say, if X is a random variable and c is a constant:   RMD( X '' + c ) = RMD( X ) · mean( X '')/(mean( X ) + c ) = RMD( X '') / (1 + c / mean( X '')) for c ≠ −mean( X ),  RMD(− X ) = −RMD( X ), and  RMD( c  X ) = RMD( X ) for c > 0.   If a random variable has a positive mean, then its relative mean absolute difference will always be greater than or equal to zero. If, additionally, the random variable can only take on values that are greater than or equal to zero, then its relative mean absolute difference will be less than 2.  Compared to standard deviation  The mean absolute difference is twice the L-scale (the second L-moment ), while the standard deviation is the square root of the variance about the mean (the second conventional central moment). The differences between L-moments and conventional moments are first seen in comparing the mean absolute difference and the standard deviation (the first L-moment and first conventional moment are both the mean).  Both the standard deviation and the mean absolute difference measure dispersion—how spread out are the values of a population or the probabilities of a distribution. The mean absolute difference is not defined in terms of a specific measure of central tendency, whereas the standard deviation is defined in terms of the deviation from the arithmetic mean. Because the standard deviation squares its differences, it tends to give more weight to larger differences and less weight to smaller differences compared to the mean absolute difference. When the arithmetic mean is finite, the mean absolute difference will also be finite, even when the standard deviation is infinite. See the examples for some specific comparisons.  The recently introduced distance standard deviation plays similar role to the mean absolute difference but the distance standard deviation works with centered distances. See also E-statistics .  Sample estimators  For a random sample S from a random variable X , consisting of n values y i , the statistic       M  D   (  S  )    =     ∑   i  =  1   n     ∑   j  =  1   n    |    y  i   -   y  j    |      n   (   n  -  1   )           M  D  S       superscript   subscript     i  1    n     superscript   subscript     j  1    n        subscript  y  i    subscript  y  j         n    n  1       MD(S)=\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}|y_{i}-y_{j}|}{n(n-1)}     is a consistent and unbiased  estimator of MD( X ). The statistic:       R  M  D   (  S  )    =     ∑   i  =  1   n     ∑   j  =  1   n    |    y  i   -   y  j    |       (   n  -  1   )     ∑   i  =  1   n    y  i            R  M  D  S       superscript   subscript     i  1    n     superscript   subscript     j  1    n        subscript  y  i    subscript  y  j           n  1     superscript   subscript     i  1    n    subscript  y  i        RMD(S)=\frac{\sum_{i=1}^{n}\sum_{j=1}^{n}|y_{i}-y_{j}|}{(n-1)\sum_{i=1}^{n}y_{%
 i}}   is a consistent  estimator of RMD( X ), but is not, in general, unbiased .  Confidence intervals for RMD( X ) can be calculated using bootstrap sampling techniques.  There does not exist, in general, an unbiased estimator for RMD( X ), in part because of the difficulty of finding an unbiased estimation for multiplying by the inverse of the mean. For example, even where the sample is known to be taken from a random variable X ( p ) for an unknown p , and X ( p ) − 1 has the Bernoulli distribution , so that Pr( X ( p ) = 1) = 1 − p and , then   RMD( X ( p )) = 2 p (1 − p )/(1 + p ).   But the expected value of any estimator R ( S ) of RMD( X ( p )) will be of the form:        E   (   R   (  S  )    )    =    ∑   i  =  0   n     p  i     (   1  -  p   )    n  -  i     r  i      ,       normal-E    R  S      superscript   subscript     i  0    n      superscript  p  i    superscript    1  p     n  i     subscript  r  i       \operatorname{E}(R(S))=\sum_{i=0}^{n}p^{i}(1-p)^{n-i}r_{i},     where the r  i are constants. So E( R ( S )) can never equal RMD( X ( p )) for all p between 0 and 1.  Examples      Examples of mean absolute difference and relative mean absolute difference   Distribution   Parameters   Mean   Standard deviation   Mean absolute difference   Relative mean absolute difference     Continuous uniform   a = 0 ; b = 1   1 / 2 = 0.5        1   12    ≈  0.2887        1    12    0.2887    \frac{1}{\sqrt{12}}\approx 0.2887      1 / 3 ≈ 0.3333   2 / 3 ≈ 0.6667     Normal   μ = 1 ; σ = 1   1   1        2   π    ≈  1.1284        2    π    1.1284    \frac{2}{\sqrt{\pi}}\approx 1.1284           2   π    ≈  1.1284        2    π    1.1284    \frac{2}{\sqrt{\pi}}\approx 1.1284        Exponential   λ = 1   1   1   1   1     Pareto   k > 1 ; x m = 1       k   k  -  1       k    k  1     \frac{k}{k-1}            1   k  -  1       k   k  -  2           1    k  1        k    k  2       \frac{1}{k-1}\,\sqrt{\frac{k}{k-2}}   (for k > 2)         2  k     (   k  -  1   )    (    2  k   -  1   )           2  k       k  1       2  k   1      \frac{2k}{(k-1)(2k-1)}\,           2    2  k   -  1        2      2  k   1     \frac{2}{2k-1}\,        Gamma   k ; θ   k θ         k    θ        k   θ    \sqrt{k}\,\theta          k  θ   (   2  -    I  0.5    (   k  +  1   ,  k  )     )       k  θ    2     subscript  I  0.5      k  1   k       k\theta(2-I_{0.5}(k+1,k))   †       2  -   4   I  0.5    (   k  +  1   ,  k  )        2    4   subscript  I  0.5      k  1   k      2-4I_{0.5}(k+1,k)   †     Gamma   k = 1 ; θ = 1   1   1   1   1     Gamma   k = 2 ; θ = 1   2        2   ≈  1.4142        2   1.4142    \sqrt{2}\approx 1.4142      3 / 2 = 1.5   3 / 4 = 0.75     Gamma   k = 3 ; θ = 1   3        3   ≈  1.7321        3   1.7321    \sqrt{3}\approx 1.7321      15 / 8 = 1.875   5 / 8 = 0.625     Gamma   k = 4 ; θ = 1   4   2   35 / 16 = 2.1875   35 / 64 = 0.546875     Bernoulli   0 ≤ p ≤ 1   p        p   (   1  -  p   )          p    1  p      \sqrt{p(1-p)}      2 p (1 − p )   2 (1 − p ) for p > 0     Student's t , 2 d.f.   ν = 2   0      ∞     \infty      π / √2 = 2.2214   undefined      †     I  z    (  x  ,  y  )        subscript  I  z    x  y     I_{z}(x,y)   is the regularized incomplete Beta function    See also   Mean deviation  Estimator  Coefficient of variation  L-moment   References            "  Category:Statistical deviation and dispersion  Category:Summary statistics  Category:Theory of probability distributions  Category:Scale statistics   