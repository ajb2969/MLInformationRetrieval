   Sliced inverse regression      Sliced inverse regression   Sliced inverse regression (SIR) is a tool for dimension reduction in the field of multivariate statistics .  In statistics , regression analysis is a popular way of studying the relationship between a response variable y and its explanatory variable    x  ¯     normal-¯  x    \underline{x}   , which is a p -dimensional vector. There are several approaches which come under the term of regression. For example parametric methods include multiple linear regression; non-parametric techniques include local smoothing .  With high-dimensional data (as p grows), the number of observations needed to use local smoothing methods escalates exponentially. Reducing the number of dimensions makes the operation computable. Dimension reduction aims to show only the most important directions of the data. SIR uses the inverse regression curve,    E   (    x  ¯    |  y  )      fragments  E   fragments  normal-(   normal-¯  x   normal-|  y  normal-)     E(\underline{x}\,|\,y)   to perform a weighted principal component analysis, with which one identifies the effective dimension reducing directions.  This article first introduces the reader to the subject of dimension reduction and how it is performed using the model here. There is then a short review on inverse regression, which later brings these pieces together.  Model  Given a response variable    Y    Y   \,Y   and a (random) vector    X  ∈    \R   p       X   superscript  \R  p     X\in\R^{p}   of explanatory variables, SIR is based on the model      Y  =    f   (    β  1  ⊤   X   ,  …  ,    β  k  ⊤   X   ,  ε  )     (  1  )        Y     f      superscript   subscript  β  1   top   X   normal-…     superscript   subscript  β  k   top   X   ε    1     Y=f(\beta_{1}^{\top}X,\ldots,\beta_{k}^{\top}X,\varepsilon)\quad\quad\quad%
 \quad\quad(1)     where     β  1   ,  …  ,   β  k       subscript  β  1   normal-…   subscript  β  k     \beta_{1},\ldots,\beta_{k}   are unknown projection vectors.    k    k   \,k   is an unknown number (the dimensionality of the space we try to reduce our data to) and, of course, as we want to reduce dimension, smaller than    p    p   \,p   .    f    f   \;f   is an unknown function on     \R    k  +  1      superscript  \R    k  1     \R^{k+1}   , as it only depends on    k    k   \,k   arguments, and   ε   ε   \varepsilon   is the error with    E   [  ε  |  X  ]   =  0     fragments  E   fragments  normal-[  ε  normal-|  X  normal-]    0    E[\varepsilon|X]=0   and finite variance    σ  2     superscript  σ  2    \sigma^{2}   . The model describes an ideal solution, where    Y    Y   \,Y   depends on    X  ∈    \R   p       X   superscript  \R  p     X\in\R^{p}   only through a    k    k   \,k   dimensional subspace. I.e. one can reduce to dimension of the explanatory variable from    p    p   \,p   to a smaller number    k    k   \,k   without losing any information.  An equivalent version of    (  1  )    1   \,(1)   is: the conditional distribution of    Y    Y   \,Y   given    X    X   \,X   depends on    X    X   \,X   only through the    k    k   \,k   dimensional random vector    (    β  1  ⊤   X   ,  …  ,    β  k  ⊤   X   )        superscript   subscript  β  1   top   X   normal-…     superscript   subscript  β  k   top   X     (\beta_{1}^{\top}X,\ldots,\beta_{k}^{\top}X)   . This perfectly reduced vector can be seen as informative as the original    X    X   \,X   in explaining    Y    Y   \,Y   .  The unknown      β   i  ′   s       superscript   subscript  β  i   normal-′   s    \,\beta_{i}^{\prime}s   are called the effective dimension reducing directions (EDR-directions). The space that is spanned by these vectors is denoted the effective dimension reducing space (EDR-space).  Relevant linear algebra background  To be able to visualize the model, note a short review on vector spaces:  For the definition of a vector space and some further properties I will refer to the article Linear Algebra and Gram-Schmidt Orthogonalization or any textbook in linear algebra and mention only the most important facts for understanding the model.  As the EDR-space is a    k    k   \,k   dimensional subspace, we need to know what a subspace is. A subspace of     \R   n     superscript  \R  n    \R^{n}   is defined as a subset    U  ∈    \R   n       U   superscript  \R  n     U\in\R^{n}   , if it holds that        a  ¯   ,   b  ¯    ∈  U  ⇒    a  ¯   +   b  ¯    ∈  U          normal-¯  a    normal-¯  b    U    normal-⇒       normal-¯  a    normal-¯  b         U     \underline{a},\underline{b}\in U\Rightarrow\underline{a}+\underline{b}\in U           a  ¯   ∈  U   ,   λ  ∈   \R   ⇒   λ   a  ¯    ∈  U      formulae-sequence     normal-¯  a   U       λ  \R    normal-⇒      λ   normal-¯  a         U      \underline{a}\in U,\lambda\in\R\Rightarrow\lambda\underline{a}\in U     Given       a  ¯   1   ,  …  ,    a  ¯   r    ∈    \R   n         subscript   normal-¯  a   1   normal-…   subscript   normal-¯  a   r     superscript  \R  n     \underline{a}_{1},\ldots,\underline{a}_{r}\in\R^{n}   , then    V  :=   L   (    a  ¯   1   ,  …  ,    a  ¯   r   )       assign  V    L    subscript   normal-¯  a   1   normal-…   subscript   normal-¯  a   r       V:=L(\underline{a}_{1},\ldots,\underline{a}_{r})   , the set of all linear combinations of these vectors, is called a linear subspace and is therefore a vector space. One says, the vectors      a  ¯   1   ,  …  ,    a  ¯   r       subscript   normal-¯  a   1   normal-…   subscript   normal-¯  a   r     \underline{a}_{1},\ldots,\underline{a}_{r}   span    V    V   \,V   . But the vectors that span a space    V    V   \,V   are not unique. This leads us to the concept of a basis and the dimension of a vector space:  A set    B  =   {    b  ¯   1   ,  …  ,    b  ¯   r   }       B    subscript   normal-¯  b   1   normal-…   subscript   normal-¯  b   r      B=\{\underline{b}_{1},\ldots,\underline{b}_{r}\}   of linear independent vectors of a vector space    V    V   \,V   is called basis of    V    V   \,V   , if it holds that      V  :=   L   (    b  ¯   1   ,  …  ,    b  ¯   r   )       assign  V    L    subscript   normal-¯  b   1   normal-…   subscript   normal-¯  b   r       V:=L(\underline{b}_{1},\ldots,\underline{b}_{r})     The dimension of     V      (    ∈    \R   n    )      annotated  V    absent   superscript  \R  n      \,V(\in\R^{n})   is equal to the maximum number of linearly independent vectors in    V    V   \,V   . A set of    n    n   \,n   linear independent vectors of     \R   n     superscript  \R  n    \R^{n}   set up a basis of     \R   n     superscript  \R  n    \R^{n}   . The dimension of a vector space is unique, as the basis itself is not. Several bases can span the same space. Of course also dependent vectors span a space, but the linear combinations of the latter can give only rise to the set of vectors lying on a straight line. As we are searching for a    k    k   \,k   dimensional subspace, we are interested in finding    k    k   \,k   linearly independent vectors that span the    k    k   \,k   dimensional subspace we want to project our data on.  Curse of dimensionality  The reason why we want to reduce the dimension of the data is due to the " curse of dimensionality " and of course, for graphical purposes. The curse of dimensionality is due to rapid increase in volume adding more dimensions to a (mathematical) space. For example, consider 100 observations from support    [  0  ,  1  ]     0  1    [0,1]   , which cover the interval quite well, and compare it to 100 observations from the corresponding   10   10   10   dimensional unit hypersquare, which are isolated points in a vast empty space. It is easy to draw inferences about the underlying properties of the data in the first case, whereas in the latter, it is not. For more information about the curse of dimensionality, see Curse of dimensionality .  Inverse regression  Computing the inverse regression curve (IR) means instead of looking for        E    [  Y  |  X  =  x  ]      fragments  E   fragments  normal-[  Y  normal-|  X   x  normal-]     \,E[Y|X=x]   , which is a curve in     \R   p     superscript  \R  p    \R^{p}      we calculate        E    [  X  |  Y  =  y  ]      fragments  E   fragments  normal-[  X  normal-|  Y   y  normal-]     \,E[X|Y=y]   , which is also a curve in     \R   p     superscript  \R  p    \R^{p}   , but consisting of    p    p   \,p   one-dimensional regressions.   The center of the inverse regression curve is located at     E    [  E   [  X  |  Y  ]   ]   =  E   [  X  ]      fragments  E   fragments  normal-[  E   fragments  normal-[  X  normal-|  Y  normal-]   normal-]    E   fragments  normal-[  X  normal-]     \,E[E[X|Y]]=E[X]   . Therefore, the centered inverse regression curve is        E    [  X  |  Y  =  y  ]   -  E   [  X  ]      fragments  E   fragments  normal-[  X  normal-|  Y   y  normal-]    E   fragments  normal-[  X  normal-]     \,E[X|Y=y]-E[X]      which is a    p    p   \,p   dimensional curve in     \R   p     superscript  \R  p    \R^{p}   . In what follows we will consider this centered inverse regression curve and we will see that it lies on a    k    k   \,k   dimensional subspace spanned by      Σ    x  x     β  i  ′   s       subscript  normal-Σ    x  x     superscript   subscript  β  i   normal-′   s    \,\Sigma_{xx}\beta_{i}\,^{\prime}s   .  But before seeing that this holds true, we will have a look at how the inverse regression curve is computed within the SIR-Algorithm, which will be introduced in detail later. What comes is the "sliced" part of SIR. We estimate the inverse regression curve by dividing the range of    Y    Y   \,Y   into    H    H   \,H   nonoverlapping intervals (slices), to afterwards compute the sample means      m  ^    h     subscript   normal-^  m   h    \,\hat{m}_{h}   of each slice. These sample means are used as a crude estimate of the IR-curve , denoted as     m    (  y  )       m  y    \,m(y)   . There are several ways to define the slices, either in a way that in each slice are equally much observations, or we define a fixed range for each slice, so that we then get different proportions of the      y   i  ′   s       superscript   subscript  y  i   normal-′   s    \,y_{i}\,^{\prime}s   that fall into each slice.  Inverse regression versus dimension reduction  As just mentioned, the centered inverse regression curve lies on a    k    k   \,k   dimensional subspace spanned by      Σ    x  x     β  i  ′   s       subscript  normal-Σ    x  x     superscript   subscript  β  i   normal-′   s    \,\Sigma_{xx}\beta_{i}\,^{\prime}s   (and therefore also the crude estimate we compute). This is the connection between our Model and Inverse Regression. We shall see that this is true, with only one condition on the design distribution that must hold. This condition is, that:      ∀   b  ¯   ∈    \R   p   :  E   [   b  ⊤   X  |   β  1  ⊤   X  =   β  1  ⊤   x  ,  …  ,   β  k  ⊤   X  =   β  k  ⊤   x  )   =   c  0   +   ∑   i  =  1   k    c  i    β  i  ⊤   x     fragments  for-all   normal-¯  b     superscript  \R  p   normal-:  E   fragments  normal-[   superscript  b  top   X  normal-|   superscript   subscript  β  1   top   X    superscript   subscript  β  1   top   x  normal-,  normal-…  normal-,   superscript   subscript  β  k   top   X    superscript   subscript  β  k   top   x  normal-)     subscript  c  0     superscript   subscript     i  1    k    subscript  c  i    superscript   subscript  β  i   top   x    \forall\,\underline{b}\in\R^{p}:\,E[b^{\top}X|\beta_{1}^{\top}X=\beta_{1}^{%
 \top}x,\ldots,\beta_{k}^{\top}X=\beta_{k}^{\top}x)=c_{0}+\sum_{i=1}^{k}c_{i}%
 \beta_{i}^{\top}x     I.e. the conditional expectation is linear in      β  1   X   ,  …  ,    β  k   X         subscript  β  1   X   normal-…     subscript  β  k   X     \beta_{1}X,\ldots,\beta_{k}X   , that is, for some constants     c  0   ,  …  ,   c  K       subscript  c  0   normal-…   subscript  c  K     c_{0},\ldots,c_{K}   . This condition is satisfied when the distribution of    X    X   \,X   is elliptically symmetric (e.g. the normal distribution). This seems to be a pretty strong requirement. It could help, for example, to closer examine the distribution of the data, so that outliers can be removed or clusters can be separated before analysis  Given this condition and    (  1  )    1   \,(1)   , it is indeed true that the centered inverse regression curve     E    [  X  |  Y  =  y  ]   -  E   [  X  ]      fragments  E   fragments  normal-[  X  normal-|  Y   y  normal-]    E   fragments  normal-[  X  normal-]     \,E[X|Y=y]-E[X]   is contained in the linear subspace spanned by      Σ    x  x     β  k    (  k  =  1  ,  …  ,  K  )      fragments   subscript  normal-Σ    x  x     subscript  β  k    fragments  normal-(  k   1  normal-,  normal-…  normal-,  K  normal-)     \,\Sigma_{xx}\beta_{k}(k=1,\ldots,K)   , where      Σ    x  x    =   C  o  v   (  X  )         subscript  normal-Σ    x  x      C  o  v  X     \,\Sigma_{xx}=Cov(X)   . The proof is provided by Duan and Li in Journal of the American Statistical Association (1991).  Estimation of the EDR-directions  After having had a look at all the theoretical properties, our aim is now to estimate the EDR-directions. For that purpose, we conduct a (weighted) principal component analysis for the sample means       m  ^    h  ′   s       superscript   subscript   normal-^  m   h   normal-′   s    \,\hat{m}_{h}\,^{\prime}s   , after having standardized    X    X   \,X   to     Z   =    Σ   x  x    -   1  /  2      {   X  -   E   (  X  )     }        Z     superscript   subscript  normal-Σ    x  x        1  2        X    E  X        \,Z=\Sigma_{xx}^{-1/2}\{X-E(X)\}   . Corresponding to the theorem above, the IR-curve      m   1    (  y  )   =  E   [  Z  |  Y  =  y  ]      fragments   subscript  m  1    fragments  normal-(  y  normal-)    E   fragments  normal-[  Z  normal-|  Y   y  normal-]     \,m_{1}(y)=E[Z|Y=y]   lies in the space spanned by    (   η  1   ,  …  ,   η  k   )      subscript  η  1   normal-…   subscript  η  k     \,(\eta_{1},\ldots,\eta_{k})   , where      η   i   =    Σ   x  x    1  /  2     β  i         subscript  η  i      subscript   superscript  normal-Σ    1  2      x  x     subscript  β  i      \,\eta_{i}=\Sigma^{1/2}_{xx}\beta_{i}   . (Due to the terminology introduced before, the      η   i  ′   s       superscript   subscript  η  i   normal-′   s    \,\eta_{i}\,^{\prime}s   are called the standardized effective dimension reducing directions .) As a consequence, the covariance matrix     c   o  v   [  E   [  Z  |  Y  ]   ]      fragments  c  o  v   fragments  normal-[  E   fragments  normal-[  Z  normal-|  Y  normal-]   normal-]     \,cov[E[Z|Y]]   is degenerate in any direction orthogonal to the      η   i  ′   s       superscript   subscript  η  i   normal-′   s    \,\eta_{i}\,^{\prime}s   . Therefore, the eigenvectors      η   k    (  k  =  1  ,  …  ,  K  )      fragments   subscript  η  k    fragments  normal-(  k   1  normal-,  normal-…  normal-,  K  normal-)     \,\eta_{k}(k=1,\ldots,K)   associated with the    K    K   \,K   largest eigenvalues are the standardized EDR-directions.  Back to PCA. That is, we calculate the estimate for     C   o  v   {    m  1    (  y  )    }       C  o  v      subscript  m  1   y      \,Cov\{m_{1}(y)\}   :       V  ^   =    n   -  1      ∑   i  =  1   S     n  s     z  ¯   s     z  ¯   s  ⊤           normal-^  V      superscript  n    1      superscript   subscript     i  1    S      subscript  n  s    subscript   normal-¯  z   s    superscript   subscript   normal-¯  z   s   top        \hat{V}=n^{-1}\sum_{i=1}^{S}n_{s}\bar{z}_{s}\bar{z}_{s}^{\top}     and identify the eigenvalues     λ  ^   i     subscript   normal-^  λ   i    \hat{\lambda}_{i}   and the eigenvectors     η  ^   i     subscript   normal-^  η   i    \hat{\eta}_{i}   of    V  ^     normal-^  V    \hat{V}   , which are the standardized EDR-directions. (For more details about that see next section: Algorithm.) Remember that the main idea of PC transformation is to find the most informative projections that maximize variance!  Note that in some situations SIR does not find the EDR-directions. One can overcome this difficulty by considering the conditional covariance     C   o  v   (  X  |  Y  )      fragments  C  o  v   fragments  normal-(  X  normal-|  Y  normal-)     \,Cov(X|Y)   . The principle remains the same as before, but one investigates the IR-curve with the conditional covariance instead of the conditional expectation. For further details and an example where SIR fails, see Härdle and Simar (2003).  Algorithm  The algorithm to estimate the EDR-directions via SIR is as follows. It is taken from the textbook Applied Multivariate Statistical Analysis (Härdle and Simar 2003)  1. Let     Σ    x  x      subscript  normal-Σ    x  x     \,\Sigma_{xx}   be the covariance matrix of    X    X   \,X   . Standardize    X    X   \,X   to       Z   =    Σ   x  x    -   1  /  2      {   X  -   E   (  X  )     }        Z     superscript   subscript  normal-Σ    x  x        1  2        X    E  X        \,Z=\Sigma_{xx}^{-1/2}\{X-E(X)\}     (We can therefore rewrite    (  1  )    1   \,(1)   as      Y  =   f   (    η  1  ⊤   Z   ,  …  ,    η  k  ⊤   Z   ,  ε  )        Y    f      superscript   subscript  η  1   top   Z   normal-…     superscript   subscript  η  k   top   Z   ε      Y=f(\eta_{1}^{\top}Z,\ldots,\eta_{k}^{\top}Z,\varepsilon)     where      η   k   =     β  k    Σ   x  x    1  /  2      ∀  k         subscript  η  k       subscript  β  k    superscript   subscript  normal-Σ    x  x      1  2      for-all  k      \,\eta_{k}=\beta_{k}\Sigma_{xx}^{1/2}\quad\forall\;k   For the standardized variable Z it holds that      E    [  Z  ]    =  0        E   delimited-[]  Z    0    \,E[Z]=0   and      C   o  v   (  Z  )    =  I        C  o  v  Z   I    \,Cov(Z)=I   .)  2. Divide the range of     y   i     subscript  y  i    \,y_{i}   into    S    S   \,S   nonoverlapping slices      H   s    (  s  =  1  ,  …  ,  S  )   .   n  s      fragments   subscript  H  s    fragments  normal-(  s   1  normal-,  normal-…  normal-,  S  normal-)   normal-.   subscript  n  s     \,H_{s}(s=1,\ldots,S).\;n_{s}   is the number of observations within each slice and     I    H  s      subscript  I   subscript  H  s     \,I_{H_{s}}   the indicator function for this slice:       n  s   =    ∑   i  =  1   n     I   H  s     (   y  i   )          subscript  n  s     superscript   subscript     i  1    n      subscript  I   subscript  H  s     subscript  y  i       n_{s}=\sum_{i=1}^{n}I_{H_{s}}(y_{i})     3. Compute the mean of     z   i     subscript  z  i    \,z_{i}   over all slices, which is a crude estimate      m  ^    1     subscript   normal-^  m   1    \,\hat{m}_{1}   of the inverse regression curve     m   1     subscript  m  1    \,m_{1}   :         z  ¯    s   =    n  s   -  1      ∑   i  =  1   n     z  i    I   H  s     (   y  i   )           subscript   normal-¯  z   s      superscript   subscript  n  s     1      superscript   subscript     i  1    n      subscript  z  i    subscript  I   subscript  H  s     subscript  y  i        \,\bar{z}_{s}=n_{s}^{-1}\sum_{i=1}^{n}z_{i}I_{H_{s}}(y_{i})     4. Calculate the estimate for     C   o  v   {    m  1    (  y  )    }       C  o  v      subscript  m  1   y      \,Cov\{m_{1}(y)\}   :        V  ^    =    n   -  1      ∑   i  =  1   S     n  s     z  ¯   s     z  ¯   s  ⊤           normal-^  V      superscript  n    1      superscript   subscript     i  1    S      subscript  n  s    subscript   normal-¯  z   s    superscript   subscript   normal-¯  z   s   top        \,\hat{V}=n^{-1}\sum_{i=1}^{S}n_{s}\bar{z}_{s}\bar{z}_{s}^{\top}     5. Identify the eigenvalues      λ  ^    i     subscript   normal-^  λ   i    \,\hat{\lambda}_{i}   and the eigenvectors      η  ^    i     subscript   normal-^  η   i    \,\hat{\eta}_{i}   of     V  ^      normal-^  V    \,\hat{V}   , which are the standardized EDR-directions.  6. Transform the standardized EDR-directions back to the original scale. The estimates for the EDR-directions are given by:         β  ^    i   =     Σ  ^    x  x    -   1  /  2       η  ^   i         subscript   normal-^  β   i      superscript   subscript   normal-^  normal-Σ     x  x        1  2      subscript   normal-^  η   i      \,\hat{\beta}_{i}=\hat{\Sigma}_{xx}^{-1/2}\hat{\eta}_{i}     (which are not necessarily orthogonal)  For examples, see the book by Härdle and Simar (2003).  See also   Curse of dimensionality   References   Li, K-C. (1991) "Sliced Inverse Regression for Dimension Reduction", Journal of the American Statistical Association , 86, 316–327 Jstor    Cook, R.D. and Sanford Weisberg, S. (1991) "Sliced Inverse Regression for Dimension Reduction: Comment", Journal of the American Statistical Association , 86, 328–332 Jstor    Härdle, W. and Simar, L. (2003) Applied Multivariate Statistical Analysis , Springer Verlag. ISBN 3-540-03079-4    Kurzfassung zur Vorlesung Mathematik II im Sommersemester 2005, A. Brandt   External links  "  Category:Regression analysis  Category:Dimension reduction   