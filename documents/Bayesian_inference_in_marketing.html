<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="102">Bayesian inference in marketing</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Bayesian inference in marketing</h1>
<hr/>

<p> <strong>Bayesian inference in marketing</strong> is the application of <a href="Bayes’_theorem" title="wikilink">Bayes’ theorem</a> to <a class="uri" href="marketing" title="wikilink">marketing</a>. Here, <a href="Bayesian_inference" title="wikilink">Bayesian inference</a> allows for decision making and <a href="market_research" title="wikilink">market research</a> evaluation under uncertainty and with limited data.</p>
<h2 id="introduction">Introduction</h2>

<p><a href="Bayes’_theorem" title="wikilink">Bayes’ theorem</a> is fundamental to <a href="Bayesian_inference" title="wikilink">Bayesian inference</a>. It is a subset of <a class="uri" href="statistics" title="wikilink">statistics</a>, providing a mathematical framework for forming <a class="uri" href="inferences" title="wikilink">inferences</a> through the concept of <a class="uri" href="probability" title="wikilink">probability</a>, in which evidence about the true state of the world is expressed in terms of degrees of belief through subjectively assessed numerical probabilities. Such a probability is known as a <a href="Bayesian_probability" title="wikilink">Bayesian probability</a>. The fundamental ideas and concepts behind Bayes’ theorem, and its use within Bayesian inference, have been developed and added to over the past centuries by <a href="Thomas_Bayes" title="wikilink">Thomas Bayes</a>, <a href="Richard_Price" title="wikilink">Richard Price</a> and <a href="Pierre_Simon_Laplace" title="wikilink">Pierre Simon Laplace</a> as well as numerous other mathematicians, statisticians and scientists.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Bayesian inference has experienced spikes in popularity as it has been seen as vague and controversial by rival <a href="Frequentist_probability" title="wikilink">frequentist</a> statisticians.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> In the past few decades Bayesian inference has become widespread in many scientific and social science fields such as <a class="uri" href="marketing" title="wikilink">marketing</a>. Bayesian inference allows for decision making and market research evaluation under uncertainty and limited data.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="bayes-theorem">Bayes’ theorem</h2>

<p>Bayesian probability specifies that there is some <a href="prior_probability" title="wikilink">prior probability</a>. Bayesian statisticians can use both an <a href="Objectivity_(philosophy)" title="wikilink">objective</a> and a <a href="Subjectivity" title="wikilink">subjective</a> approach when interpreting the prior probability, which is then updated in light of new relevant information. The concept is a manipulation of <a href="conditional_probabilities" title="wikilink">conditional probabilities</a>:<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>

<math display="block" id="Bayesian_inference_in_marketing:0">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>A</mi>
    <mi>B</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>A</mi>
    <mo stretchy="false">|</mo>
    <mi>B</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>B</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>B</mi>
    <mo stretchy="false">|</mo>
    <mi>A</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>A</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">A</csymbol>
     <csymbol cd="unknown">B</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">A</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">B</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">B</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">B</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">A</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">A</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(AB)=P(A|B)P(B)=P(B|A)P(A)
  </annotation>
 </semantics>
</math>

</p>

<p>Alternatively, a more simple understanding of the formula may be reached by substituting the events 

<math display="inline" id="Bayesian_inference_in_marketing:1">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Bayesian_inference_in_marketing:2">
 <semantics>
  <mi>B</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>B</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B
  </annotation>
 </semantics>
</math>

 to become respectively the hypothesis 

<math display="inline" id="Bayesian_inference_in_marketing:3">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mi>H</mi>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (H)
  </annotation>
 </semantics>
</math>

 and the <a class="uri" href="data" title="wikilink">data</a> 

<math display="inline" id="Bayesian_inference_in_marketing:4">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mi>D</mi>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (D)
  </annotation>
 </semantics>
</math>

. The rule allows for a judgment of the relative truth of the hypothesis given the data.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>

<p>This is done through the calculation shown below, where 

<math display="inline" id="Bayesian_inference_in_marketing:5">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">|</mo>
    <mi>H</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">H</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(D|H)
  </annotation>
 </semantics>
</math>

 is the <a href="likelihood_function" title="wikilink">likelihood function</a>. This assesses the probability of the observed data 

<math display="inline" id="Bayesian_inference_in_marketing:6">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mi>D</mi>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (D)
  </annotation>
 </semantics>
</math>

 arising from the hypothesis 

<math display="inline" id="Bayesian_inference_in_marketing:7">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mi>H</mi>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (H)
  </annotation>
 </semantics>
</math>

; 

<math display="inline" id="Bayesian_inference_in_marketing:8">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>H</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <ci>H</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(H)
  </annotation>
 </semantics>
</math>

 is the assigned prior probability or initial belief about the hypothesis; the denominator 

<math display="inline" id="Bayesian_inference_in_marketing:9">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <ci>D</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(D)
  </annotation>
 </semantics>
</math>

 is formed by the integrating or summing of 

<math display="inline" id="Bayesian_inference_in_marketing:10">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">|</mo>
    <mi>H</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>H</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">H</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">H</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(D|H)P(H)
  </annotation>
 </semantics>
</math>

; 

<math display="inline" id="Bayesian_inference_in_marketing:11">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>H</mi>
    <mo stretchy="false">|</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">H</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(H|D)
  </annotation>
 </semantics>
</math>

 is known as the <a href="Posterior_probability" title="wikilink">posterior</a> which is the recalculated probability, or updated belief about the hypothesis. It is a result of the prior beliefs as well as sample information. The posterior is a conditional distribution as the result of collecting or in consideration of new relevant data.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> 

<math display="inline" id="Bayesian_inference_in_marketing:12">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>H</mi>
    <mo stretchy="false">|</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>D</mi>
      <mo stretchy="false">|</mo>
      <mi>H</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>H</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>D</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">H</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">D</csymbol>
       <ci>normal-|</ci>
       <csymbol cd="unknown">H</csymbol>
       <ci>normal-)</ci>
      </cerror>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">H</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <apply>
      <times></times>
      <ci>P</ci>
      <ci>D</ci>
     </apply>
    </apply>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(H|D)=\frac{P(D|H)P(H)}{P(D)}
  </annotation>
 </semantics>
</math>

</p>

<p>To sum up this formula: the posterior probability of the hypothesis is equal to the prior probability of the hypothesis multiplied by the conditional probability of the evidence given the hypothesis, divided by the probability of the new evidence.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h2 id="use-in-marketing">Use in marketing</h2>
<h3 id="history">History</h3>

<p>While the concepts of Bayesian statistics are thought to date back to 1763, marketers’ exposure to the concepts are relatively recent, dating from 1959.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> Subsequently many books<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a><a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a><a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> and articles<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a><a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> have been written about the application of Bayesian statistics to marketing decision-making and <a href="market_research" title="wikilink">market research</a>. It was predicted that the Bayesian approach would be used widely in the marketing field but up until the mid-1980s the methods were considered impractical.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> The resurgence in the use of Bayesian methods is largely due to the developments over the last few decades in computational methods; and expanded availability of detailed marketplace data - primarily due to the birth of the <a href="world_wide_web" title="wikilink">world wide web</a> and explosion of the <a class="uri" href="internet" title="wikilink">internet</a>.</p>
<h3 id="application-in-marketing">Application in marketing</h3>

<p>Bayesian decision theory can be applied to all four areas of the <a href="marketing_mix" title="wikilink">marketing mix</a>.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> Assessments are made by a decision maker on the probabilities of events that determine the profitability of alternative actions where the outcomes are uncertain. Assessments are also made for the profit (utility) for each possible combination of action and event. The decision maker can decide how much research, if any, needs to be conducted in order to investigate the consequences associated with the courses of action under evaluation. This is done before a final decision is made, but it should be noted that in order to do this costs would be incurred, time used and may overall be unreliable. For each possible action, expected profit can be computed, that is a <a href="weighted_mean" title="wikilink">weighted mean</a> of the possible profits, the weights being the probabilities. The decision maker can then choose the action for which the expected profit is the highest. The theorem provides a formal reconciliation between judgment expressed <a href="Numerical_data" title="wikilink">quantitatively</a> in the prior distribution and the statistical evidence of the experiment.</p>
<h4 id="new-product-development">New product development</h4>

<p>The use of Bayesian <a href="decision_theory" title="wikilink">decision theory</a> in new product development allows for the use of subjective prior information. Bayes in new product development allows for the comparison of additional review project costs with the value of additional information in order to reduce the costs of uncertainty. The methodology used for this analysis is in the form of <a href="decision_trees" title="wikilink">decision trees</a> and ‘stop’/‘go’ procedures. If the predicted payoff (the posterior) is acceptable for the organisation the project should go ahead, if not, development should stop. By reviewing the posterior (which then becomes the new prior) on regular intervals throughout the development stage managers are able to make the best possible decision with the information available at hand. Although the review process may delay further development and increase costs, it can help greatly to reduce uncertainty in high risk decisions.</p>
<h4 id="pricing-decisions">Pricing decisions</h4>

<p><a href="Bayesian_decision_theory" title="wikilink">Bayesian decision theory</a> can be used in looking at pricing decisions. Field information such as retail and wholesale prices as well as the size of the market and market share are all incorporated into the prior information. Managerial judgement is included in order to evaluate different pricing strategies. This method of evaluating possible pricing strategies does have its limitations as it requires a number of assumptions to be made about the market place in which an organisation operates. As markets are dynamic environments it is often difficult to fully apply Bayesian decision theory to pricing strategies without simplifying the model.</p>
<h4 id="promotional-campaigns">Promotional campaigns</h4>

<p>When dealing with promotion a marketing manager must account for all the market complexities that are involved in a decision. As it is difficult to account for all aspects of the market, a manager should look to incorporate both experienced judgements from senior executives as well modifying these judgements in light of economically justifiable information gathering. An example of the application of Bayesian decision theory for promotional purposes could be the use of a test sample in order to assess the effectiveness of a promotion prior to a full scale role out. By combining prior subjective data about the occurrence of possible events with experimental empirical evidence gained through a test market, the resultant data can be used to make decisions under risk.</p>
<h4 id="channel-decisions-and-the-logistics-of-distribution">Channel decisions and the logistics of distribution</h4>

<p>Bayesian decision analysis can also be applied to the channel selection process. In order to help provide further information the method can be used that produces results in a profit or loss aspect. Prior information can include costs, expected profit, training expenses and any other costs relevant to the decision as well as managerial experience which can be displayed in a <a href="normal_distribution" title="wikilink">normal distribution</a>. Bayesian decision making under uncertainty lets a marketing manager assess his/her options for channel logistics by computing the most profitable method choice. A number of different costs can be entered into the model that helps to assess the ramifications of change in distribution method. Identifying and quantifying all of the relevant information for this process can be very time consuming and costly if the analysis delays possible future earnings.</p>
<h2 id="strengths">Strengths</h2>

<p>The Bayesian approach is superior to use in decision making when there is a high level of uncertainty or limited information in which to base decisions on and where expert opinion or historical knowledge is available. Bayes is also useful when explaining the findings in a probability- sense to people who are less familiar and comfortable with comprehending statistics. It is in this sense that Bayesian methods are thought of as having created a bridge between business judgments and statistics for the purpose of decision-making.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a></p>

<p>The three principle strengths of Bayes' theorem that have been identified by scholars are that it is prescriptive, complete and coherent.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> Prescriptive in that it is the theorem that is the simple prescription to the conclusions reached on the basis of evidence and reasoning for the consistent decision maker. It is complete because (for a given choice of model and prior distribution) the solution is often clear and unambiguous. It allows for the incorporation of prior information when available to increase the robustness of the solutions, as well as taking into consideration the costs and risks that are associated with choosing alternate decisions.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> Lastly Bayes theorem is coherent. It is considered the most appropriate way to update beliefs by welcoming the incorporation of new information, as is seen through the <a href="probability_distributions" title="wikilink">probability distributions</a> (see Savage<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a> and De Finetti<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a>). This is further complemented by the fact that Bayes inference satisfies the likelihood principle,<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> which states that models or inferences for datasets leading to the same likelihood function should generate the same statistical information. Bayes methods are more cost effective than the traditional frequentist take on marketing research and subsequent decision making. The probability can be assessed from a degree of belief before and after accounting for evidence, instead of calculating the probabilities of a certain decision by carrying out a large number of trials with each one producing an outcome from a set of possible outcomes. The planning and implementation of trials to see how a decision impacts in the ‘field’ e.g. observing consumers reaction to a relabeling of a product, is time consuming and costly, a method many firms cannot afford. In place of taking the frequentist route in aiming for a universally acceptable conclusion through <a class="uri" href="iteration" title="wikilink">iteration</a>,<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a> it is sometimes more effective to take advantage of all the information available to the firm to work out the ‘best’ decision at the time, and then subsequently when new knowledge is obtained, revise the posterior distribution to be then used as the prior, thus the inferences continue to <a href="logic" title="wikilink">logically</a> contribute to one another based on Bayes theorem.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a></p>
<h2 id="weaknesses">Weaknesses</h2>

<p>In marketing situations, it is important that the prior probability is (1) chosen correctly, and (2) is understood. A disadvantage to using Bayesian analysis is that there is no ‘correct’ way to choose a prior, therefore the inferences require a thorough analysis to translate the subjective prior beliefs into a mathematically formulated prior to ensure that the results will not be misleading and consequently lead to the disproportionate analysis of preposteriors.<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a> The subjective definition of probability and the selection and use of the priors have led to statisticians critiquing this subjective definition of probability that underlies the Bayesian approach.<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a> Bayesian probability is often found to be difficult when analysing and assessing probabilities due to its initial <a href="counter_intuitive" title="wikilink">counter intuitive</a> nature. Often when deciding between <a href="strategy" title="wikilink">strategies</a> based on a decision, they are interpreted as: where there is evidence X that shows condition A might hold true, is misread by judging A’s likelihood by how well the evidence X matches A, but crucially without considering the prior frequency of A.<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> In alignment with <a href="falsifiability" title="wikilink">Falsification</a>, which aims to question and falsify instead of prove hypotheses, where there is very strong evidence X, it does not necessarily mean there is a very high probability that A leads to B, but in fact should be interpreted as a very low probability of A not leading to B. In the field of marketing, behavioural experiments which have dealt with managerial decision- making,<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a><a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a> and <a href="risk_perception" title="wikilink">risk perception</a>,<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a><a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a> in consumer decisions have utilised the Bayesian model, or similar models, but found that it may not be relevant quantitatively in predicting human information processing behaviour. Instead the model has been proven as useful as a <a href="qualitative_data" title="wikilink">qualitative</a> means of describing how individuals combine new evidence with their predetermined judgements. Therefore “the model may have some value as a first approximation to the development of descriptive choice theory” in consumer and managerial instances.<a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a></p>
<h2 id="example">Example</h2>

<p>An <a class="uri" href="advertising" title="wikilink">advertising</a> manager is deciding whether or not to increase the advertising for a product in a particular <a href="Market_(economics)" title="wikilink">market</a>. The Bayes approach to this decision suggests: 1) These alternative courses of action for which the consequences are uncertain are a necessary condition in order to apply Bayes’; 2) The advertising manager will pick the course of action which allows him to achieve some objective i.e. a maximum return on his advertising investment in the form of profit; 3) He must determine the possible consequences of each action into some measure of success (or loss) with which a certain objective is achieved.</p>

<p>This 3 component example explains how the payoffs are conditional upon which outcomes occur. The advertising manager can characterize the outcomes based on past experience and knowledge and devise some possible events that are more likely to occur than others. He can then assign to these events prior probabilities, which would be in the form of numerical weights.<a class="footnoteRef" href="#fn32" id="fnref32"><sup>32</sup></a></p>

<p>He can test out his predictions (prior probabilities) through an <a class="uri" href="experiment" title="wikilink">experiment</a>. For example he can run a test campaign to decide if the total level of advertising should be in fact increased. Based on the outcome of the experiment he can re-evaluate his prior probability and make a decision on whether to go ahead with increasing the advertising in the market or not. However gathering this additional data is costly, time consuming and may not lead to perfectly reliable results. As a decision makers he has to deal with experimental and <a href="systematic_error" title="wikilink">systematic error</a> and this is where Bayes’ comes in.</p>

<p>It approaches the experimental problem by asking; is additional data required? If so, how much needs to be collected and by what means and finally, how does the decision maker revise his prior judgment in light of the results of the new experimental evidence? In this example the advertising manager can use the Bayesian approach to deal with his dilemma and update his prior judgments in light of new information he gains. He needs to take into account the profit (utility) attached to the alternative acts under different events and the value versus cost of information in order to make his optimal decision on how to proceed.</p>
<h2 id="bayes-in-computational-models">Bayes in computational models</h2>

<p><a href="Markov_Chain_Monte_Carlo" title="wikilink">Markov Chain Monte Carlo</a> (MCMC) is a flexible procedure designed to fit a variety of Bayesian models. It is the underlying method used in <a href="computer_simulation" title="wikilink">computational</a> software such as the <a class="uri" href="LaplacesDemon" title="wikilink">LaplacesDemon</a> <a href="R_(programming_language)" title="wikilink">R</a> Package and <a class="uri" href="WinBUGS" title="wikilink">WinBUGS</a>. The advancements and developments of these types of statistical software have allowed for the growth of Bayes by offering ease of calculation. This is achieved by the generation of samples from the posterior distributions, which are then used to produce a range of options or strategies which are allocated numerical weights. MCMC obtains these samples and produces summary and diagnostic statistics while also saving the posterior samples in the output. The decision maker can then assess the results from the output data set and choose the best option to proceed.<a class="footnoteRef" href="#fn33" id="fnref33"><sup>33</sup></a></p>
<h2 id="further-reading">Further reading</h2>

<p>For illustrative examples of Bayesian applications in marketing:</p>
<ul>
<li>Rossi, P. E. and Allenby, G. M. (1993) “A Bayesian Approach to estimating Household Parameters” Journal of Marketing Research 30 (2): 171-182.</li>
</ul>
<ul>
<li>Yang, S. and Allenby, G. M. (2003). “Modelling Interdependent Consumer Preferences”, Journal of Marketing Research 40 (3): 282-294.</li>
</ul>
<ul>
<li>Kim, J., Allenby, G. M. and Rossi, P. E. (2002) “Modelling Consumer Demand for Variety” Marketing Science 21 (3): 223-228.</li>
</ul>
<ul>
<li>Allenby, G. M., Shively, T., Yang, S. and Garratt, M. J. (2004). “A Choice Model for Packaged Goods: Dealing with Discrete Quantities and Quantity Discount” Marketing Science 23 (1): 95-108.</li>
</ul>
<ul>
<li>Green, P. E. and Frank, R. E. (1966). “Bayesian Statistics and Marketing Research”, Journal of the Royal Statistical Society, Series C 15 (3): 182 -a marketing manager deciding on whether to re label one of its products</li>
</ul>
<ul>
<li>Alderson, W., Green, P. E. (1964). Planning and Problem Solving in Marketing, Illinois: Richard D. Irwin Inc Illinois.</li>
</ul>
<ul>
<li>McGrayne, S. B. (2011). The Theory that Would Not Die, Yale University Press: New Haven; London.</li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Applications_of_Bayesian_inference" title="wikilink">Marketing</a> <a class="uri" href="Category:Marketing" title="wikilink">Category:Marketing</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">McGrayne, S. B. (2011). <em>The Theory that Would Not Die</em>, New Haven: Yale University Press.<a href="#fnref1">↩</a></li>
<li id="fn2">Green, P. E. and Frank, R. E. (1966). “Bayesian Statistics and Marketing Research”, <a href="Journal_of_the_Royal_Statistical_Society,_Series_C" title="wikilink">Journal of the Royal Statistical Society, Series C</a> 15 (3): 173-190. <a href="#fnref2">↩</a></li>
<li id="fn3">Olshausen B. A. (2004) “Bayesian Probability Theory” <a class="uri" href="http://redwood.berkeley.edu/bruno/npb163/bayes.pdf">http://redwood.berkeley.edu/bruno/npb163/bayes.pdf</a><a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"></li>
<li id="fn6"></li>
<li id="fn7">Paulos, J. A. (2011) “The Mathematics of Changing your Mind” Book review, New York Times. (August 5th p14 sunday book review)<a href="#fnref7">↩</a></li>
<li id="fn8"></li>
<li id="fn9">Chernoff, H. and Moses, L. E. (1959). Elementary Decision Theory. New York: Wiley; London: Chapman &amp; Hall<a href="#fnref9">↩</a></li>
<li id="fn10">Schlaifer, R. (1959). Probability and Statistics for Business Decisions, New York: McGraw Hill<a href="#fnref10">↩</a></li>
<li id="fn11">Rossi, P. E., Allenby, G. M. and McCulloch, R. (2005). <em>Bayesian Statistics and Marketing</em>, New York: Wiley<a href="#fnref11">↩</a></li>
<li id="fn12">Roberts, H. V. (1960). “The New Business Statistics”, <em>Journal of Business</em> 33 (1): 21-30.<a href="#fnref12">↩</a></li>
<li id="fn13">Pratt, J. W., Raiffa, H. and Schlaifer, R. (1964). ”The Foundations of Decision Under Uncertainty: An Elementary Exposition”, <em>Journal of the American Statistical Association</em> 59 (306): 353- 375<a href="#fnref13">↩</a></li>
<li id="fn14">Rossi, P.E., Allenby. G. M (2003) "Bayesian Statistics and Marketing" <em>Marketing Science</em> 22 (3): 304-328<a href="#fnref14">↩</a></li>
<li id="fn15">Alderson, W., Green, P. E. (1964) <em>Planning and Problem Solving in Marketing</em>. Richard D. Irwin Inc Illinois<a href="#fnref15">↩</a></li>
<li id="fn16">Roberts, H. V. (1963). “Bayesian Statistics in Marketing” Journal of Marketing 27 (1):1-4<a href="#fnref16">↩</a></li>
<li id="fn17">Little, R.(2006). “Calibrated Bayes: A Bayes/ Frequentist Roadmap”, The American Statistician 60 (3): 213-233<a href="#fnref17">↩</a></li>
<li id="fn18">Wald, A.(1950). “Statistical Decision Functions”, in: Kotz, S. And Johnson, N. L. (Eds.) (1992). Breakthroughs in Statistics: Foundations and Basic Theory, New York: Wiley<a href="#fnref18">↩</a></li>
<li id="fn19">Savage, L. J. (1954). The Foundations of Statistics, New York: Wiley<a href="#fnref19">↩</a></li>
<li id="fn20">De Finetti, B. (1974). The Theory of Probability, New York: Wiley<a href="#fnref20">↩</a></li>
<li id="fn21">Birnbaum, A. (1962). “On the Foundations of Statistical Inference”, Journal of the American Statistical Association, 57 (298), 269- 306. <http: stable="" www.jstor.org=""></http:><a href="#fnref21">↩</a></li>
<li id="fn22">Bradley, E(2005). “Bayesians, Frequentists, and Scientists”, Journal of the American Statistical Association 100 (469):1- 5. <a class="uri" href="http://search.proquest.com.ezproxy.otago.ac.nz/docview/274829688">http://search.proquest.com.ezproxy.otago.ac.nz/docview/274829688</a><a href="#fnref22">↩</a></li>
<li id="fn23">SAS Institute Inc. (2009). SAS/STAT® 9.2 User’s Guide, Second Edition, Cary, NC: SAS Institute Inc.http://support.sas.com/documentation/cdl/en/statug/63033/PDF/default/statug.pdf<a href="#fnref23">↩</a></li>
<li id="fn24"></li>
<li id="fn25">Little, Roderick (2006). “Calibrated Bayes: A Bayes/ Frequentist Roadmap”, The American Statistician 60 (3): 213-233<a href="#fnref25">↩</a></li>
<li id="fn26"></li>
<li id="fn27">Green, P. E., Peters, W. S. and Robinson, P. J. (1966). “A Behavioural Experiment in Decision- Making under Uncertainty”, Journal of Purchasing, 2: 18-31<a href="#fnref27">↩</a></li>
<li id="fn28">Starbuck, W. H. and Bass, F. M. (1965). An Experimental Study of Risk-taking and the Value of Information in a New Product Context. Institute Paper No. 117. Herman C. Krannert Graduate School of Industrial Administration, Perdue University<a href="#fnref28">↩</a></li>
<li id="fn29">Baur, R. A. (1960). “Consumer Behaviour as Risk Taking”, Proc. 43rd National Conference American Marketing Association., 389-398. Chicago: American Marketing Association<a href="#fnref29">↩</a></li>
<li id="fn30">Cox, D. F. and Rich, S. (1964). “Perceived Risk and Consumer Decision-making- The Case of Telephone Shopping”, Journal of Marketing Research, 1 (4): 32-39<a href="#fnref30">↩</a></li>
<li id="fn31"></li>
<li id="fn32">Green, P. E. (1962) “Bayesian Decision Theory in Advertising”. Journal of Advertising 33-42<a href="#fnref32">↩</a></li>
<li id="fn33"></li>
</ol>
</section>
</body>
</html>
