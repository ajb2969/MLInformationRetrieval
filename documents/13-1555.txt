   Asymptotic theory (statistics)      Asymptotic theory (statistics)   In statistics , asymptotic theory , or large sample theory , is a generic framework for assessment of properties of estimators and statistical tests . Within this framework it is typically assumed that the sample size  n grows indefinitely, and the properties of statistical procedures are evaluated in the limit as .  In practical applications, asymptotic theory is applied by treating the asymptotic results as approximately valid for finite sample sizes as well. Such approach is often criticized for not having any mathematical grounds behind it, yet it is used ubiquitously anyway. The importance of the asymptotic theory is that it often makes possible to carry out the analysis and state many results which cannot be obtained within the standard ‚Äúfinite-sample theory‚Äù .  Overview  Most statistical problems begin with a dataset of size  n . The asymptotic theory proceeds by assuming that it is possible to keep collecting additional data, so that the sample size would grow infinitely:      n  ‚Üí   ‚àû      normal-‚Üí  n     n\to\infty\,   Under this assumption many results can be obtained that are unavailable for samples of finite sizes. As an example consider the law of large numbers . This law states that for a sequence of iid random variables X 1 , X 2 , ‚Ä¶, the sample averages     X  ¬Ø   n     subscript   normal-¬Ø  X   n    \scriptstyle\overline{X}_{n}   converge in probability to the population mean E[ X i ] as n ‚Üí ‚àû. At the same time for finite n it is impossible to claim anything about the distribution of     X  ¬Ø   n     subscript   normal-¬Ø  X   n    \scriptstyle\overline{X}_{n}   if the distributions of individual X i ‚Äôs is unknown.  For various models slightly different modes of asymptotics may be used:   For cross-sectional data ( iid ) the new observations are sampled independently, from the same fixed distribution. This is the standard case of  asymptotics.  For longitudinal data ( time series ) the sampling method may differ from model to model. Sometimes the data is assumed to be ergodic , in other applications it can be integrated or cointegrated . In this case the asymptotic is again taken as the number of observations (usually denoted T in this case) goes to infinity: .  For panel data , it is commonly assumed that one dimension in the data ( T ) remains fixed, whereas the other dimension grows: , .   Besides these standard approaches, various other ‚Äúalternative‚Äù asymptotic approaches exist:   Within the local asymptotic normality framework, it is assumed that the value of the ‚Äútrue parameter‚Äù in the model varies slightly with n , such that the n -th model corresponds to      Œ∏  n    =   Œ∏  +   h  /   n          subscript  Œ∏  n     Œ∏    h    n       \scriptstyle\theta_{n}\,=\,\theta+h/\sqrt{n}   . This approach lets us study the regularity of estimators .  When statistical tests are studied for their power to distinguish against the alternatives that are close to the null hypothesis, it is done within the so-called ‚Äúlocal alternatives‚Äù framework: the null hypothesis is H 0 : Œ∏ = Œ∏ 0 , and the alternative is H 1 :     Œ∏   =    Œ∏  0   +   h  /   n         Œ∏     subscript  Œ∏  0     h    n       \scriptstyle\theta\,=\,\theta_{0}+h/\sqrt{n}   . This approach is especially popular for the unit root tests .  There are models where the dimension of the parameter space Œò n slowly expands with n , reflecting the fact that the more observations a statistician has, the more he is tempted to introduce additional parameters in the model. An example of this is the weak instruments asymptotic.  In kernel density estimation and kernel regression additional parameter ‚Äî the bandwidth h ‚Äî is assumed. In these models it is typically taken that h ‚Üí 0 as n ‚Üí ‚àû, however the rate of convergence must be chosen carefully, usually h ‚àù n ‚àí1/5 .   Modes of convergence of random variables  Asymptotic properties  Estimators   Consistency : a sequence of estimators is said to be consistent , if it converges in probability to the true value of the parameter being estimated:          Œ∏  ^   n      ‚Üí  ùëù     Œ∏  0       p  normal-‚Üí    subscript   normal-^  Œ∏   n    subscript  Œ∏  0     \hat{\theta}_{n}\ \xrightarrow{p}\ \theta_{0}   Generally an estimator is just some, more or less arbitrary, function of the data. The property of consistency requires that the estimator was estimating the quantity we intended it to. As such, it is the most important property in the estimation theory: estimators that are known to be inconsistent are never used in practice.   Asymptotic distribution : if it is possible to find sequences of non-random constants { a n }, { b n } (possibly depending on the value of Œ∏ 0 ), and a non-degenerate distribution G such that          b  n    (     Œ∏  ^   n   -   a  n    )      ‚Üí  ùëë    G   ,      d  normal-‚Üí      subscript  b  n      subscript   normal-^  Œ∏   n    subscript  a  n     G    b_{n}(\hat{\theta}_{n}-a_{n})\ \xrightarrow{d}\ G,   then the sequence of estimators     Œ∏  ^   n     subscript   normal-^  Œ∏   n    \scriptstyle\hat{\theta}_{n}   is said to have the asymptotic distribution  G .  Most often, the estimators encountered in practice have the asymptotically normal distribution, with , , and :         n    (     Œ∏  ^   n   -   Œ∏  0    )      ‚Üí  ùëë     ùí©   (  0  ,  V  )     .      d  normal-‚Üí       n      subscript   normal-^  Œ∏   n    subscript  Œ∏  0       ùí©   0  V      \sqrt{n}(\hat{\theta}_{n}-\theta_{0})\ \xrightarrow{d}\ \mathcal{N}(0,V).      Asymptotic confidence regions .  Regularity .   Asymptotic theorems   Law of large numbers  Central limit theorem  Slutsky‚Äôs theorem  Continuous mapping theorem   Notes  References      "  Category:Statistical theory  Category:Econometrics  Category:Asymptotic statistical theory   