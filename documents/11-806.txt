   Rademacher complexity      Rademacher complexity   In computational learning theory ( machine learning and theory of computation ), Rademacher complexity , named after Hans Rademacher , measures richness of a class of real-valued functions with respect to a probability distribution .  Given a training sample    S  =   (   z  1   ,   z  2   ,  ‚Ä¶  ,   z  m   )   ‚àà   Z  m         S    subscript  z  1    subscript  z  2   normal-‚Ä¶   subscript  z  m          superscript  Z  m      S=(z_{1},z_{2},\dots,z_{m})\in Z^{m}   , and a class   ‚Ñã   ‚Ñã   \mathcal{H}   of real-valued functions defined on a domain space   Z   Z   Z   , the empirical Rademacher complexity of   ‚Ñã   ‚Ñã   \mathcal{H}   is defined as:        ‚Ñõ  ^   S    (  ‚Ñã  )   =   2  m   ùîº   [   sup   h  ‚àà  ‚Ñã    |   ‚àë   i  =  1   m    œÉ  i   h   (   z  i   )   |  |  S  ]      fragments   subscript   normal-^  ‚Ñõ   S    fragments  normal-(  H  normal-)      2  m   E   fragments  normal-[   subscript  supremum    h  ‚Ñã    normal-|   superscript   subscript     i  1    m    subscript  œÉ  i   h   fragments  normal-(   subscript  z  i   normal-)   normal-|  normal-|  S  normal-]     \widehat{\mathcal{R}}_{S}(\mathcal{H})=\frac{2}{m}\mathbb{E}\left[\sup_{h\in%
 \mathcal{H}}\left|\sum_{i=1}^{m}\sigma_{i}h(z_{i})\right|\ \bigg|\ S\right]     where     œÉ  1   ,   œÉ  2   ,  ‚Ä¶  ,   œÉ  m       subscript  œÉ  1    subscript  œÉ  2   normal-‚Ä¶   subscript  œÉ  m     \sigma_{1},\sigma_{2},\dots,\sigma_{m}   are independent random variables drawn from the Rademacher distribution i.e.     Pr   (    œÉ  i   =   +  1    )    =   Pr   (    œÉ  i   =   -  1    )    =   1  /  2          Pr     subscript  œÉ  i     1      Pr     subscript  œÉ  i     1            1  2      \Pr(\sigma_{i}=+1)=\Pr(\sigma_{i}=-1)=1/2   for    i  =   1  ,  2  ,  ‚Ä¶  ,  m       i   1  2  normal-‚Ä¶  m     i=1,2,\dots,m   .  Let   P   P   P   be a probability distribution over   Z   Z   Z   . The Rademacher complexity of the function class   ‚Ñã   ‚Ñã   \mathcal{H}   with respect to   P   P   P   for sample size   m   m   m   is:        ‚Ñõ  m    (  ‚Ñã  )    =   ùîº   [     ‚Ñõ  ^   S    (  ‚Ñã  )    ]           subscript  ‚Ñõ  m   ‚Ñã     ùîº   delimited-[]     subscript   normal-^  ‚Ñõ   S   ‚Ñã       \mathcal{R}_{m}(\mathcal{H})=\mathbb{E}\left[\widehat{\mathcal{R}}_{S}(%
 \mathcal{H})\right]     where the above expectation is taken over an identically independently distributed (i.i.d.) sample    S  =   (   z  1   ,   z  2   ,  ‚Ä¶  ,   z  m   )       S    subscript  z  1    subscript  z  2   normal-‚Ä¶   subscript  z  m      S=(z_{1},z_{2},\dots,z_{m})   generated according to   P   P   P   .  One can show, for example, that there exists a constant   C   C   C   , such that any class of    {  0  ,  1  }     0  1    \{0,1\}   -indicator functions with Vapnik-Chervonenkis dimension    d   d   d   has Rademacher complexity upper-bounded by    C    d  m        C      d  m      C\sqrt{\frac{d}{m}}   .  Gaussian complexity  Gaussian complexity is a similar complexity with similar physical meanings, and can be obtained from the previous complexity using the random variables    g  i     subscript  g  i    g_{i}   instead of    œÉ  i     subscript  œÉ  i    \sigma_{i}   , where    g  i     subscript  g  i    g_{i}   are Gaussian  i.i.d. random variables with zero-mean and variance 1, i.e.     g  i   ‚àº   ùí©   (  0  ,  1  )       similar-to   subscript  g  i     ùí©   0  1      g_{i}\sim\mathcal{N}\left(0,1\right)   .  References   Peter L. Bartlett, Shahar Mendelson (2002) Rademacher and Gaussian Complexities: Risk Bounds and Structural Results . Journal of Machine Learning Research 3 463-482    Giorgio Gnecco, Marcello Sanguineti (2008) Approximation Error Bounds via Rademacher's Complexity . Applied Mathematical Sciences, Vol. 2, 2008, no. 4, 153 - 176   "  Category:Machine learning  Category:Decision theory  Category:Measures of complexity   