   Gradient boosting      Gradient boosting   Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees . It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable  loss function .  The idea of gradient boosting originated in the observation by Leo Breiman  1 that boosting can be interpreted as an optimization algorithm on a suitable cost function. Explicit regression gradient boosting algorithms were subsequently developed by Jerome H. Friedman 2 3 simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean . 4 5 The latter two papers introduced the abstract view of boosting algorithms as iterative functional gradient descent algorithms. That is, algorithms that optimize a cost functional over function space by iteratively choosing a function (weak hypothesis) that points in the negative gradient direction. This functional gradient view of boosting has led to the development of boosting algorithms in many areas of machine learning and statistics beyond regression and classification.  Informal introduction  (This section follows the exposition of gradient boosting by Li. 6 )  Like other boosting methods, gradient boosting combines weak learners into a single strong learner, in an iterative fashion. It is easiest to explain in the least-squares regression setting, where the goal is to learn a model   F   F   F   that predicts values     y  ^   =   F   (  x  )         normal-^  y     F  x     \hat{y}=F(x)   , minimizing the mean squared error      (    y  ^   -  y   )   2     superscript     normal-^  y   y   2    (\hat{y}-y)^{2}   to the true values   y   y   y   (averaged over some training set).  At each stage    1  â‰¤  m  â‰¤  M        1  m       M     1\leq m\leq M   of gradient boosting, it may be assumed that there is some imperfect model    F  m     subscript  F  m    F_{m}   (at the outset, a very weak model that just predicts the mean   y   y   y   in the training set could be used). The gradient boosting algorithm does not change    F  m     subscript  F  m    F_{m}   in any way; instead, it improves on it by constructing a new model that adds an estimator   h   h   h   to provide a better model      F   m  +  1     (  x  )    =     F  m    (  x  )    +   h   (  x  )            subscript  F    m  1    x        subscript  F  m   x     h  x      F_{m+1}(x)=F_{m}(x)+h(x)   . The question is now, how to find   h   h   h   ? The gradient boosting solution starts with the observation that a perfect   h   h   h   would imply       F   m  +  1    =     F  m    (  x  )    +   h   (  x  )     =  y         subscript  F    m  1         subscript  F  m   x     h  x         y     F_{m+1}=F_{m}(x)+h(x)=y     or, equivalently,       h   (  x  )    =   y  -    F  m    (  x  )           h  x     y     subscript  F  m   x      h(x)=y-F_{m}(x)   .  Therefore, gradient boosting will fit   h   h   h   to the residual     y  -    F  m    (  x  )        y     subscript  F  m   x     y-F_{m}(x)   . Like in other boosting variants, each    F   m  +  1      subscript  F    m  1     F_{m+1}   learns to correct its predecessor    F  m     subscript  F  m    F_{m}   . A generalization of this idea to other loss functions than squared error (and to classification and ranking problems) follows from the observation that residuals    y  -   F   (  x  )        y    F  x     y-F(x)   are the negative gradients of the squared error loss function     1  2     (   y  -   F   (  x  )     )   2         1  2    superscript    y    F  x    2     \frac{1}{2}(y-F(x))^{2}   . So, gradient boosting is a gradient descent algorithm; and generalizing it entails "plugging in" a different loss and its gradient.  Algorithm  In many supervised learning problems one has an output variable   y   y   y   and a vector of input variables   x   x   x   connected together via a joint probability distribution     P   (  x  ,  y  )       P   x  y     P(x,y)   . Using a training set    {   (   x  1   ,   y  1   )   ,  â€¦  ,   (   x  n   ,   y  n   )   }       subscript  x  1    subscript  y  1    normal-â€¦    subscript  x  n    subscript  y  n      \{(x_{1},y_{1}),\dots,(x_{n},y_{n})\}   of known values of   x   x   x   and corresponding values of   y   y   y   , the goal is to find an approximation     F  ^    (  x  )        normal-^  F   x    \hat{F}(x)   to a function   F   F   F   that minimizes the expected value of some specified loss function     L   (  y  ,   F   (  x  )    )       L   y    F  x      L(y,F(x))   :       F  *   =      arg  min   ð¹     ð”¼   x  ,  y     [   L   (  y  ,   F   (  x  )    )    ]         superscript  F       F        subscript  ð”¼   x  y     delimited-[]    L   y    F  x         F^{*}=\underset{F}{\arg\min}\,\mathbb{E}_{x,y}[L(y,F(x))]   .  Gradient boosting method assumes a real-valued y and seeks an approximation     F  ^    (  x  )        normal-^  F   x    \hat{F}(x)   in the form of a weighted sum of functions    h  áµ¢   (  x  )       h  áµ¢  x    háµ¢(x)   from some class â„‹, called base (or weak) learners:       F   (  x  )    =     âˆ‘   i  =  1   M     Î³  i    h  i    (  x  )     +  const         F  x       superscript   subscript     i  1    M      subscript  Î³  i    subscript  h  i   x    const     F(x)=\sum_{i=1}^{M}\gamma_{i}h_{i}(x)+\mbox{const}   .  In accordance with the empirical risk minimization principle, the method tries to find an approximation     F  ^    (  x  )        normal-^  F   x    \hat{F}(x)   that minimizes the average value of the loss function on the training set. It does so by starting with a model, consisting of a constant function      F   0    (  x  )        subscript  F  0   x    \!F_{0}(x)   , and incrementally expanding it in a greedy fashion:        F  0    (  x  )    =     arg  min   ð›¾     âˆ‘   i  =  1   n    L   (   y  i   ,  Î³  )             subscript  F  0   x      Î³         superscript   subscript     i  1    n     L    subscript  y  i   Î³        F_{0}(x)=\underset{\gamma}{\arg\min}\sum_{i=1}^{n}L(y_{i},\gamma)   ,        F  m    (  x  )    =     F   m  -  1     (  x  )    +      arg   min    f  âˆˆ  â„‹      âˆ‘   i  =  1   n    L   (   y  i   ,     F   m  -  1     (   x  i   )    +   f   (   x  i   )     )              subscript  F  m   x        subscript  F    m  1    x        f  â„‹     arg  min      superscript   subscript     i  1    n     L    subscript  y  i        subscript  F    m  1     subscript  x  i      f   subscript  x  i            F_{m}(x)=F_{m-1}(x)+\underset{f\in\mathcal{H}}{\operatorname{arg\,min}}\sum_{i%
 =1}^{n}L(y_{i},F_{m-1}(x_{i})+f(x_{i}))   ,  where   f   f   f   is restricted to be a function from the class â„‹ of base learner functions.  However, the problem of choosing at each step the best   f   f   f   for an arbitrary loss function   L   L   L   is a hard optimization problem in general, and so we'll "cheat" by solving a much easier problem instead.  The idea is to apply a steepest descent step to this minimization problem. If we only cared about predictions at the points of the training set, and   f   f   f   were unrestricted, we'd update the model per the following equation, where we view    L   (  y  ,  f  )       L   y  f     L(y,f)   not as a functional of   f   f   f   , but as a function of a vector of values      f    (   x  1   )    ,  â€¦  ,   f   (   x  n   )         f   subscript  x  1    normal-â€¦    f   subscript  x  n      \!f(x_{1}),\ldots,f(x_{n})   :         F  m    (  x  )    =     F   m  -  1     (  x  )    -    Î³  m     âˆ‘   i  =  1   n      âˆ‡  f   L    (   y  i   ,    F   m  -  1     (   x  i   )    )        ,         subscript  F  m   x        subscript  F    m  1    x      subscript  Î³  m     superscript   subscript     i  1    n       subscript  normal-âˆ‡  f   L     subscript  y  i      subscript  F    m  1     subscript  x  i           F_{m}(x)=F_{m-1}(x)-\gamma_{m}\sum_{i=1}^{n}\nabla_{f}L(y_{i},F_{m-1}(x_{i})),           Î³  m   =     arg  min   ð›¾     âˆ‘   i  =  1   n    L   (   y  i   ,     F   m  -  1     (   x  i   )    -   Î³     âˆ‚  L    (   y  i   ,    F   m  -  1     (   x  i   )    )      âˆ‚  f    (   x  i   )       )       .       subscript  Î³  m      Î³         superscript   subscript     i  1    n     L    subscript  y  i        subscript  F    m  1     subscript  x  i      Î³        L     subscript  y  i      subscript  F    m  1     subscript  x  i          f    subscript  x  i             \gamma_{m}=\underset{\gamma}{\arg\min}\sum_{i=1}^{n}L\left(y_{i},F_{m-1}(x_{i}%
 )-\gamma\frac{\partial L(y_{i},F_{m-1}(x_{i}))}{\partial f(x_{i})}\right).     But as   f   f   f   must come from a restricted class of functions (that's what allows us to generalize), we'll just choose the one that most closely approximates the gradient of   L   L   L   . Having chosen   f   f   f   , the multiplier   Î³   Î³   Î³   is then selected using line search just as shown in the second equation above.  In pseudocode, the generic gradient boosting method is: 7 8 Input: training set      {   (   x  i   ,   y  i   )   }    i  =  1   n   ,     superscript   subscript     subscript  x  i    subscript  y  i       i  1    n    \!\{(x_{i},y_{i})\}_{i=1}^{n},   a differentiable loss function      L    (  y  ,   F   (  x  )    )    ,      L   y    F  x      \!L(y,F(x)),   number of iterations     M  .     M   \!M.     Algorithm:   Initialize model with a constant value:        F  0    (  x  )    =     arg  min   ð›¾     âˆ‘   i  =  1   n    L   (   y  i   ,  Î³  )       .         subscript  F  0   x      Î³         superscript   subscript     i  1    n     L    subscript  y  i   Î³        F_{0}(x)=\underset{\gamma}{\arg\min}\sum_{i=1}^{n}L(y_{i},\gamma).     For   m   m   m   = 1 to   M   M   M   :  Compute so-called pseudo-residuals :        r   i  m    =   -    [     âˆ‚  L    (   y  i   ,   F   (   x  i   )    )      âˆ‚  F    (   x  i   )     ]     F   (  x  )    =    F   m  -  1     (  x  )          for  i   =   1  ,  â€¦  ,  n     .     formulae-sequence     subscript  r    i  m       subscript   delimited-[]        L     subscript  y  i     F   subscript  x  i          F    subscript  x  i          F  x      subscript  F    m  1    x           for  i    1  normal-â€¦  n      r_{im}=-\left[\frac{\partial L(y_{i},F(x_{i}))}{\partial F(x_{i})}\right]_{F(x%
 )=F_{m-1}(x)}\quad\mbox{for }i=1,\ldots,n.     Fit a base learner      h   m    (  x  )        subscript  h  m   x    \!h_{m}(x)   to pseudo-residuals, i.e. train it using the training set     {   (   x  i   ,   r   i  m    )   }    i  =  1   n     superscript   subscript     subscript  x  i    subscript  r    i  m        i  1    n    \{(x_{i},r_{im})\}_{i=1}^{n}   .  Compute multiplier     Î³   m     subscript  Î³  m    \!\gamma_{m}   by solving the following one-dimensional optimization problem:       Î³  m   =      arg   min   ð›¾     âˆ‘   i  =  1   n    L   (   y  i   ,     F   m  -  1     (   x  i   )    +   Î³   h  m    (   x  i   )     )       .       subscript  Î³  m      Î³    arg  min      superscript   subscript     i  1    n     L    subscript  y  i        subscript  F    m  1     subscript  x  i      Î³   subscript  h  m    subscript  x  i           \gamma_{m}=\underset{\gamma}{\operatorname{arg\,min}}\sum_{i=1}^{n}L\left(y_{i%
 },F_{m-1}(x_{i})+\gamma h_{m}(x_{i})\right).     Update the model:        F  m    (  x  )    =     F   m  -  1     (  x  )    +    Î³  m    h  m    (  x  )      .         subscript  F  m   x        subscript  F    m  1    x      subscript  Î³  m    subscript  h  m   x      F_{m}(x)=F_{m-1}(x)+\gamma_{m}h_{m}(x).      Output      F  M    (  x  )    .       subscript  F  M   x    F_{M}(x).      Gradient tree boosting  Gradient boosting is typically used with decision trees (especially CART trees) of a fixed size as base learners. For this special case Friedman proposes a modification to gradient boosting method which improves the quality of fit of each base learner.  Generic gradient boosting at the m -th step would fit a decision tree      h   m    (  x  )        subscript  h  m   x    \!h_{m}(x)   to pseudo-residuals. Let    J    J   \!J   be the number of its leaves. The tree partitions the input space into    J    J   \!J   disjoint regions      R    1  m    ,  â€¦  ,   R   J  m        subscript  R    1  m    normal-â€¦   subscript  R    J  m      \!R_{1m},\ldots,R_{Jm}   and predicts a constant value in each region. Using the indicator notation , the output of      h   m    (  x  )        subscript  h  m   x    \!h_{m}(x)   for input x can be written as the sum:       h  m    (  x  )   =   âˆ‘   j  =  1   J    b   j  m    I   (  x  âˆˆ   R   j  m    )   ,     fragments   subscript  h  m    fragments  normal-(  x  normal-)     superscript   subscript     j  1    J    subscript  b    j  m    I   fragments  normal-(  x    subscript  R    j  m    normal-)   normal-,    h_{m}(x)=\sum_{j=1}^{J}b_{jm}I(x\in R_{jm}),   where     b    j  m      subscript  b    j  m     \!b_{jm}   is the value predicted in the region     R    j  m      subscript  R    j  m     \!R_{jm}   . 9  Then the coefficients     b    j  m      subscript  b    j  m     \!b_{jm}   are multiplied by some value     Î³   m     subscript  Î³  m    \!\gamma_{m}   , chosen using line search so as to minimize the loss function, and the model is updated as follows:          F  m    (  x  )    =     F   m  -  1     (  x  )    +    Î³  m    h  m    (  x  )      ,    Î³  m   =      arg   min   ð›¾     âˆ‘   i  =  1   n    L   (   y  i   ,     F   m  -  1     (   x  i   )    +   Î³   h  m    (   x  i   )     )        .     formulae-sequence       subscript  F  m   x        subscript  F    m  1    x      subscript  Î³  m    subscript  h  m   x        subscript  Î³  m      Î³    arg  min      superscript   subscript     i  1    n     L    subscript  y  i        subscript  F    m  1     subscript  x  i      Î³   subscript  h  m    subscript  x  i            F_{m}(x)=F_{m-1}(x)+\gamma_{m}h_{m}(x),\quad\gamma_{m}=\underset{\gamma}{%
 \operatorname{arg\,min}}\sum_{i=1}^{n}L(y_{i},F_{m-1}(x_{i})+\gamma h_{m}(x_{i%
 })).     Friedman proposes to modify this algorithm so that it chooses a separate optimal value     Î³    j  m      subscript  Î³    j  m     \!\gamma_{jm}   for each of the tree's regions, instead of a single     Î³   m     subscript  Î³  m    \!\gamma_{m}   for the whole tree. He calls the modified algorithm "TreeBoost". The coefficients     b    j  m      subscript  b    j  m     \!b_{jm}   from the tree-fitting procedure can be then simply discarded and the model update rule becomes:       F  m    (  x  )   =   F   m  -  1     (  x  )   +   âˆ‘   j  =  1   J    Î³   j  m    I   (  x  âˆˆ   R   j  m    )   ,   Î³   j  m    =     arg   min   ð›¾    âˆ‘    x  i   âˆˆ   R   j  m      L   (   y  i   ,   F   m  -  1     (   x  i   )   +  Î³   h  m    (   x  i   )   )   .     fragments   subscript  F  m    fragments  normal-(  x  normal-)     subscript  F    m  1     fragments  normal-(  x  normal-)     superscript   subscript     j  1    J    subscript  Î³    j  m    I   fragments  normal-(  x    subscript  R    j  m    normal-)   normal-,   subscript  Î³    j  m      Î³    arg  min     subscript      subscript  x  i    subscript  R    j  m      L   fragments  normal-(   subscript  y  i   normal-,   subscript  F    m  1     fragments  normal-(   subscript  x  i   normal-)    Î³   subscript  h  m    fragments  normal-(   subscript  x  i   normal-)   normal-)   normal-.    F_{m}(x)=F_{m-1}(x)+\sum_{j=1}^{J}\gamma_{jm}I(x\in R_{jm}),\quad\gamma_{jm}=%
 \underset{\gamma}{\operatorname{arg\,min}}\sum_{x_{i}\in R_{jm}}L(y_{i},F_{m-1%
 }(x_{i})+\gamma h_{m}(x_{i})).     Size of trees      J    J   \!J   , the number of terminal nodes in trees, is the method's parameter which can be adjusted for a data set at hand. It controls the maximum allowed level of interaction between variables in the model. With     J   =  2      J  2    \!J=2   ( decision stumps ), no interaction between variables is allowed. With     J   =  3      J  3    \!J=3   the model may include effects of the interaction between up to two variables, and so on.  Hastie et al. 10 comment that typically    4  â‰¤  J  â‰¤  8        4  J       8     4\leq J\leq 8   work well for boosting and results are fairly insensitive to the choice of   J   J   J   in this range,    J  =  2      J  2    J=2   is insufficient for many applications, and    J  >  10      J  10    J>10   is unlikely to be required.  Regularization  Fitting the training set too closely can lead to degradation of the model's generalization ability. Several so-called regularization techniques reduce this overfitting effect by constraining the fitting procedure.  One natural regularization parameter is the number of gradient boosting iterations M (i.e. the number of trees in the model when the base learner is a decision tree). Increasing M reduces the error on training set, but setting it too high may lead to overfitting. An optimal value of M is often selected by monitoring prediction error on a separate validation data set. Besides controlling M , several other regularization techniques are used.  Shrinkage  An important part of gradient boosting method is regularization by shrinkage which consists in modifying the update rule as follows:          F  m    (  x  )    =     F   m  -  1     (  x  )    +    Î½  â‹…   Î³  m     h  m    (  x  )      ,   0  <  Î½  â‰¤  1    ,     formulae-sequence       subscript  F  m   x        subscript  F    m  1    x      normal-â‹…  Î½   subscript  Î³  m     subscript  h  m   x         0  Î½       1      F_{m}(x)=F_{m-1}(x)+\nu\cdot\gamma_{m}h_{m}(x),\quad 0<\nu\leq 1,   where parameter   Î½   Î½   \nu   is called the "learning rate".  Empirically it has been found that using small learning rates (such as    Î½  <  0.1      Î½  0.1    \nu<0.1   ) yields dramatic improvements in model's generalization ability over gradient boosting without shrinking (    Î½  =  1      Î½  1    \nu=1   ). 11 However, it comes at the price of increasing computational time both during training and querying: lower learning rate requires more iterations.  Stochastic gradient boosting  Soon after the introduction of gradient boosting Friedman proposed a minor modification to the algorithm, motivated by Breiman 's bagging method. 12 Specifically, he proposed that at each iteration of the algorithm, a base learner should be fit on a subsample of the training set drawn at random without replacement. 13 Friedman observed a substantial improvement in gradient boosting's accuracy with this modification.  Subsample size is some constant fraction f of the size of the training set. When f = 1, the algorithm is deterministic and identical to the one described above. Smaller values of f introduce randomness into the algorithm and help prevent overfitting , acting as a kind of regularization . The algorithm also becomes faster, because regression trees have to be fit to smaller datasets at each iteration. Friedman 14 obtained that     0.5   â‰¤  f  â‰¤  0.8        0.5  f       0.8     \!0.5\leq f\leq 0.8   leads to good results for small and moderate sized training sets. Therefore, f is typically set to 0.5, meaning that one half of the training set is used to build each base learner.  Also, like in bagging, subsampling allows one to define an out-of-bag estimate of the prediction performance improvement by evaluating predictions on those observations which were not used in the building of the next base learner. Out-of-bag estimates help avoid the need for an independent validation dataset, but often underestimate actual performance improvement and the optimal number of iterations. 15  Number of observations in leaves  Gradient tree boosting implementations often also use regularization by limiting the minimum number of observations in trees' terminal nodes (this parameter is called n.minobsinnode in the R gbm package 16 ). It is used in the tree building process by ignoring any splits that lead to nodes containing fewer than this number of training set instances.  Imposing this limit helps to reduce variance in predictions at leaves.  Penalize Complexity of Tree  Another useful regularization techniques for gradient boosted trees is to penalize model complexity of the learned model. 17 The model complexity can be defined proportional number of leaves in the learned trees. The jointly optimization of loss and model complexity corresponds to a post-pruning algorithm to remove branches that fail to reduce the loss by a threshold. Other kinds of regularization such as l2 penalty on the leave values can also be added to avoid overfitting.  Usage  Recently, gradient boosting has gained some popularity in the field of learning to rank . The commercial web search engines Yahoo 18 and Yandex 19 use variants of gradient boosting in their machine-learned ranking engines.  Names  The method goes by a variety of names. Friedman introduced his regression technique as a "Gradient Boosting Machine" (GBM). 20 Mason, Baxter et. el. described the generalized abstract class of algorithms as "functional gradient boosting". 21 22  A popular open-source implementation 23 for R calls it "Generalized Boosting Model". Commercial implementations from Salford Systems use the names "Multiple Additive Regression Trees" (MART) and TreeNet, both trademarked.  See also   AdaBoost  Random forest   References  "  Category:Decision trees  Category:Ensemble learning     Brieman, L. " Arcing The Edge " (June 1997) â†©  Friedman, J. H. " Greedy Function Approximation: A Gradient Boosting Machine. " (February 1999) â†©  Friedman, J. H. " Stochastic Gradient Boosting. " (March 1999) â†©  â†©  â†©  â†©    Note: in case of usual CART trees, the trees are fitted using least-squares loss, and so the coefficient    b   j  m      subscript  b    j  m     b_{jm}   for the region    R   j  m      subscript  R    j  m     R_{jm}   is equal to just the value of output variable, averaged over all training instances in    R   j  m      subscript  R    j  m     R_{jm}   . â†©  â†©    Note that this is different from bagging, which samples with replacement because it uses samples of the same size as the training set. â†©  Friedman, J. H. " Stochastic Gradient Boosting. " (March 1999) â†©  Ridgeway, Greg (2007). Generalized Boosted Models: A guide to the gbm package. â†©   Tianqi Chen. Introduction to Boosted Trees â†©  Cossock, David and Zhang, Tong (2008). Statistical Analysis of Bayes Optimal Subset Ranking , page 14. â†©  [ http://webmaster.ya.ru/replies.xml?item_no=5707&ncrnd; ;=5118 Yandex corporate blog entry about new ranking model "Snezhinsk"] (in Russian) â†©         