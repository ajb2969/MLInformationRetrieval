   Probit model      Probit model   In statistics , a probit model is a type of regression where the dependent variable can only take two values, for example married or not married. The name is from prob ability + un it . 1 The purpose of the model is to estimate the probability that an observation with particular characteristics will fall into a specific one of the categories; moreover, if estimated probabilities greater than 1/2 are treated as classifying an observation into a predicted category, the probit model is a type of binary classification model.  A probit model is a popular specification for an ordinal 2 or a binary response model . As such it treats the same set of problems as does logistic regression using similar techniques. The probit model, which employs a probit  link function , is most often estimated using the standard maximum likelihood procedure, such an estimation being called a probit regression .  Probit models were introduced by Chester Bliss in 1934; 3 a fast method for computing maximum likelihood estimates for them was proposed by Ronald Fisher as an appendix to Bliss' work in 1935. 4  Conceptual framework  Suppose response variable Y is binary , that is it can have only two possible outcomes which we will denote as 1 and 0. For example Y may represent presence/absence of a certain condition, success/failure of some device, answer yes/no on a survey, etc. We also have a vector of regressors  X , which are assumed to influence the outcome Y . Specifically, we assume that the model takes the form        Pr   (   Y  =  1   ∣  X  )    =   Φ   (    X  ′   β   )     ,       Pr    Y  1   X     normal-Φ     superscript  X  normal-′   β      \Pr(Y=1\mid X)=\Phi(X^{\prime}\beta),   where Pr denotes probability , and Φ is the Cumulative Distribution Function ( CDF ) of the standard normal distribution . The parameters β are typically estimated by maximum likelihood .  It is possible to motivate the probit model as a latent variable model . Suppose there exists an auxiliary random variable        Y  ∗   =     X  ′   β   +  ε    ,       superscript  Y  normal-∗        superscript  X  normal-′   β   ε     Y^{\ast}=X^{\prime}\beta+\varepsilon,\,   where ε ~ N (0, 1). Then Y can be viewed as an indicator for whether this latent variable is positive:      Y  =   {     1       if   Y  ∗    >     0   i.e.   -  ε   <    X  ′   β    ,       0    otherwise.          Y   cases  1        if   superscript  Y  normal-∗        0  i.e.   ε           superscript  X  normal-′   β     0  otherwise.     Y=\begin{cases}1&\text{if }Y^{\ast}>0\ \text{ i.e. }-\varepsilon     The use of the standard normal distribution causes no loss of generality compared with using an arbitrary mean and standard deviation because adding a fixed amount to the mean can be compensated by subtracting the same amount from the intercept, and multiplying the standard deviation by a fixed amount can be compensated by multiplying the weights by the same amount.  To see that the two models are equivalent, note that      Pr   (   Y  =  1   ∣  X  )      Pr    Y  1   X    \displaystyle\Pr(Y=1\mid X)     Model estimation  Maximum likelihood estimation  Suppose data set     {   y  i   ,   x  i   }    i  =  1   n     superscript   subscript    subscript  y  i    subscript  x  i      i  1    n    \{y_{i},x_{i}\}_{i=1}^{n}   contains n independent statistical units corresponding to the model above. Then their joint log-likelihood function is        ln  ℒ    (  β  )    =    ∑   i  =  1   n    (     y  i    ln  Φ    (    x  i  ′   β   )    +    (   1  -   y  i    )     ln    (   1  -   Φ   (    x  i  ′   β   )     )      )            ℒ   β     superscript   subscript     i  1    n        subscript  y  i     normal-Φ      superscript   subscript  x  i   normal-′   β        1   subscript  y  i        1    normal-Φ     superscript   subscript  x  i   normal-′   β           \ln\mathcal{L}(\beta)=\sum_{i=1}^{n}\bigg(y_{i}\ln\Phi(x_{i}^{\prime}\beta)+(1%
 -y_{i})\ln\!\big(1-\Phi(x_{i}^{\prime}\beta)\big)\bigg)   The estimator    β  ^     normal-^  β    \hat{\beta}   which maximizes this function will be consistent , asymptotically normal and efficient provided that E[ XX '] exists and is not singular. It can be shown that this log-likelihood function is globally concave in β , and therefore standard numerical algorithms for optimization will converge rapidly to the unique maximum.  Asymptotic distribution for    β  ^     normal-^  β    \hat{\beta}   is given by         n    (    β  ^   -  β   )      →  𝑑     𝒩   (  0  ,   Ω   -  1    )     ,      d  normal-→       n      normal-^  β   β      𝒩   0   superscript  normal-Ω    1        \sqrt{n}(\hat{\beta}-\beta)\ \xrightarrow{d}\ \mathcal{N}(0,\,\Omega^{-1}),   where       Ω  =   E   [      φ  2    (    X  ′   β   )     Φ   (    X  ′   β   )    (   1  -   Φ   (    X  ′   β   )     )     X   X  ′    ]     ,    Ω  ^   =    1  n     ∑   i  =  1   n       φ  2    (    x  i  ′    β  ^    )     Φ   (    x  i  ′    β  ^    )    (   1  -   Φ   (    x  i  ′    β  ^    )     )      x  i    x  i  ′          formulae-sequence    normal-Ω   normal-E         superscript  φ  2      superscript  X  normal-′   β      normal-Φ     superscript  X  normal-′   β     1    normal-Φ     superscript  X  normal-′   β       X   superscript  X  normal-′         normal-^  normal-Ω       1  n     superscript   subscript     i  1    n          superscript  φ  2      subscript   superscript  x  normal-′   i    normal-^  β       normal-Φ     subscript   superscript  x  normal-′   i    normal-^  β      1    normal-Φ     subscript   superscript  x  normal-′   i    normal-^  β         subscript  x  i    subscript   superscript  x  normal-′   i         \Omega=\operatorname{E}\bigg[\frac{\varphi^{2}(X^{\prime}\beta)}{\Phi(X^{%
 \prime}\beta)(1-\Phi(X^{\prime}\beta))}XX^{\prime}\bigg],\qquad\hat{\Omega}=%
 \frac{1}{n}\sum_{i=1}^{n}\frac{\varphi^{2}(x^{\prime}_{i}\hat{\beta})}{\Phi(x^%
 {\prime}_{i}\hat{\beta})(1-\Phi(x^{\prime}_{i}\hat{\beta}))}x_{i}x^{\prime}_{i}   and φ = Φ ' is the Probability Density Function ( PDF ) of standard normal distribution.  Berkson's minimum chi-square method  This method can be applied only when there are many observations of response variable    y  i     subscript  y  i    y_{i}   having the same value of the vector of regressors    x  i     subscript  x  i    x_{i}   (such situation may be referred to as "many observations per cell"). More specifically, the model can be formulated as follows.  Suppose among n observations     {   y  i   ,   x  i   }    i  =  1   n     superscript   subscript    subscript  y  i    subscript  x  i      i  1    n    \{y_{i},x_{i}\}_{i=1}^{n}   there are only T distinct values of the regressors, which can be denoted as    {   x   (  1  )    ,  …  ,   x   (  T  )    }      subscript  x  1   normal-…   subscript  x  T     \{x_{(1)},\ldots,x_{(T)}\}   . Let    n  t     subscript  n  t    n_{t}   be the number of observations with      x  i   =   x   (  t  )     ,       subscript  x  i    subscript  x  t     x_{i}=x_{(t)},   and    r  t     subscript  r  t    r_{t}   the number of such observations with     y  i   =  1       subscript  y  i   1    y_{i}=1   . We assume that there are indeed "many" observations per each "cell": for each     t  ,    lim   n  →  ∞      n  t   /  n     =   c  t   >  0         t    subscript    normal-→  n        subscript  n  t   n      subscript  c  t        0     t,\lim_{n\rightarrow\infty}n_{t}/n=c_{t}>0   .  Denote        p  ^   t   =    r  t   /   n  t         subscript   normal-^  p   t      subscript  r  t    subscript  n  t      \hat{p}_{t}=r_{t}/n_{t}           σ  ^   t  2   =    1   n  t        p  ^   t    (   1  -    p  ^   t    )      φ  2    (    Φ   -  1     (    p  ^   t   )    )           superscript   subscript   normal-^  σ   t   2       1   subscript  n  t         subscript   normal-^  p   t     1   subscript   normal-^  p   t        superscript  φ  2      superscript  normal-Φ    1     subscript   normal-^  p   t         \hat{\sigma}_{t}^{2}=\frac{1}{n_{t}}\frac{\hat{p}_{t}(1-\hat{p}_{t})}{\varphi^%
 {2}\big(\Phi^{-1}(\hat{p}_{t})\big)}     Then Berkson's minimum chi-square estimator is a generalized least squares estimator in a regression of     Φ   -  1     (    p  ^   t   )        superscript  normal-Φ    1     subscript   normal-^  p   t     \Phi^{-1}(\hat{p}_{t})   on    x   (  t  )      subscript  x  t    x_{(t)}   with weights     σ  ^   t   -  2      superscript   subscript   normal-^  σ   t     2     \hat{\sigma}_{t}^{-2}   :       β  ^   =     (    ∑   t  =  1   T      σ  ^   t   -  2     x   (  t  )     x   (  t  )   ′     )    -  1      ∑   t  =  1   T      σ  ^   t   -  2     x   (  t  )     Φ   -  1     (    p  ^   t   )           normal-^  β      superscript    superscript   subscript     t  1    T      superscript   subscript   normal-^  σ   t     2     subscript  x  t    subscript   superscript  x  normal-′   t       1      superscript   subscript     t  1    T      superscript   subscript   normal-^  σ   t     2     subscript  x  t    superscript  normal-Φ    1     subscript   normal-^  p   t        \hat{\beta}=\Bigg(\sum_{t=1}^{T}\hat{\sigma}_{t}^{-2}x_{(t)}x^{\prime}_{(t)}%
 \Bigg)^{-1}\sum_{t=1}^{T}\hat{\sigma}_{t}^{-2}x_{(t)}\Phi^{-1}(\hat{p}_{t})     It can be shown that this estimator is consistent (as n →∞ and T fixed), asymptotically normal and efficient. Its advantage is the presence of a closed-form formula for the estimator. However, it is only meaningful to carry out this analysis when individual observations are not available, only their aggregated counts    r  t     subscript  r  t    r_{t}   ,    n  t     subscript  n  t    n_{t}   , and    x   (  t  )      subscript  x  t    x_{(t)}   (for example in the analysis of voting behavior).  Gibbs sampling  Gibbs sampling of a probit model is possible because regression models typically use normal prior distributions over the weights, and this distribution is conjugate with the normal distribution of the errors (and hence of the latent variables Y * ). The model can be described as     𝜷   𝜷   \displaystyle\boldsymbol{\beta}     From this, we can determine the full conditional densities needed:     𝐁   𝐁   \displaystyle\mathbf{B}     The result for β is given in the article on Bayesian linear regression , although specified with different notation.  The only trickiness is in the last two equations. The notation    [    y  i  ∗   <  0   ]     delimited-[]     superscript   subscript  y  i   normal-∗   0     [y_{i}^{\ast}<0]   is the Iverson bracket , sometimes written    ℐ   (   y  i  ∗   <  0  )      fragments  I   fragments  normal-(   superscript   subscript  y  i   normal-∗    0  normal-)     \mathcal{I}(y_{i}^{\ast}<0)   or similar. It indicates that the distribution must be truncated within the given range, and rescaled appropriately. In this particular case, a truncated normal distribution arises. Sampling from this distribution depends on how much is truncated. If a large fraction of the original mass remains, sampling can be easily done with rejection sampling — simply sample a number from the non-truncated distribution, and reject it if it falls outside the restriction imposed by the truncation. If sampling from only a small fraction of the original mass, however (e.g. if sampling from one of the tails of the normal distribution — for example if     𝐱  i  ′   𝜷       subscript   superscript  𝐱  normal-′   i   𝜷    \mathbf{x}^{\prime}_{i}\boldsymbol{\beta}   is around 3 or more, and a negative sample is desired), then this will be inefficient and it becomes necessary to fall back on other sampling algorithms. General sampling from the truncated normal can be achieved using approximations to the normal CDF and the probit function , and R has a function rtnorm() for generating truncated-normal samples.  Model evaluation  The suitability of an estimated binary model can be evaluated by counting the number of true observations equaling 1, and the number equaling zero, for which the model assigns a correct predicted classification by treating any estimated probability above 1/2 (or, below 1/2), as an assignment of a prediction of 1 (or, of 0). See here for details.  See also   Generalized linear model  Limited dependent variable  Multivariate probit models  Ordered probit and Ordered logit model  Separation (statistics)  Multinomial probit   References  Further reading        External links    by Mark Thoma   "  Category:Regression analysis  Category:Classification algorithms     Oxford English Dictionary , 3rd ed. s.v. probit (article dated June 2007): ↩  Ordinal probit regression model UCLA Academic Technology Services http://www.ats.ucla.edu/stat/stata/dae/ologit.htm ↩  ↩  ↩     