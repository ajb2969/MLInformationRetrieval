   Generalized minimum-distance decoding      Generalized minimum-distance decoding   In coding theory , generalized minimum-distance (GMD) decoding provides an efficient algorithm for decoding concatenated codes , which is based on using an errors -and- erasures decoder for the outer code .  A naive decoding algorithm for concatenated codes can not be an optimal way of decoding because it does not take into account the information that maximum likelihood decoding (MLD) gives. In other words, in the naive algorithm, inner received codewords are treated the same regardless of the difference between their hamming distances . Intuitively, the outer decoder should place higher confidence in symbols whose inner encodings are close to the received word. David Forney in 1966 devised a better algorithm called generalized minimum distance (GMD) decoding which makes use of those information better. This method is achieved by measuring confidence of each received codeword, and erasing symbols whose confidence is below a desired value. And GMD decoding algorithm was one of the first examples of soft-decision decoders . We will present three versions of the GMD decoding algorithm. The first two will be randomized algorithms while the last one will be a deterministic algorithm .  Setup   Hamming distance : Given two vectors      u  ,  v   ‚àà   ‚àë  n        u  v    superscript   n     u,v\in\sum^{n}   the Hamming distance between u and v, denoted by    Œî   (  u  ,  v  )       normal-Œî   u  v     \Delta(u,v)   , is defined to be the number of positions in which u and v differ.  Minimum distance : Let    C  ‚äÜ   ‚àë  n       C   superscript   n     C\subseteq\sum^{n}   be a code . The minimum distance of code C is defined to be    d  =    min  Œî    (   c  1   ,   c  2   )        d      normal-Œî     subscript  c  1    subscript  c  2       d=\min{\Delta(c_{1},c_{2})}   where     c  1   ‚â†   c  2   ‚àà  C         subscript  c  1    subscript  c  2        C     c_{1}\neq c_{2}\in C     Code concatenation : Given    m  =   (   m  1   ,  ‚Ä¶  ,   m  K   )   ‚àà    [  Q  ]   K         m    subscript  m  1   normal-‚Ä¶   subscript  m  K          superscript   delimited-[]  Q   K      m=(m_{1},\ldots,m_{K})\in[Q]^{K}   , consider two codes which we call outer code and inner code     C  out   =    [  Q  ]   K   ‚Üí    [  Q  ]   N   ,   C  in   :    [  q  ]   k   ‚Üí    [  q  ]   n      fragments   subscript  C  out     superscript   fragments  normal-[  Q  normal-]   K   normal-‚Üí   superscript   fragments  normal-[  Q  normal-]   N   normal-,   subscript  C  in   normal-:   superscript   fragments  normal-[  q  normal-]   k   normal-‚Üí   superscript   fragments  normal-[  q  normal-]   n     C_{\text{out}}=[Q]^{K}\rightarrow[Q]^{N},C_{\text{in}}:[q]^{k}\rightarrow[q]^{n}   , and their distances are   D   D   D   and   d   d   d   . A concatenated code can be achieved by       C  out   ‚àò   C  in     (  m  )    =   (    C  in    (    C  out     (  m  )   1    )    ,  ‚Ä¶  ,    C  in    (    C  out     (  m  )   N    )    )            subscript  C  out    subscript  C  in    m       subscript  C  in      subscript  C  out    subscript  m  1     normal-‚Ä¶     subscript  C  in      subscript  C  out    subscript  m  N        C_{\text{out}}\circ C_{\text{in}}(m)=(C_{\text{in}}(C_{\text{out}}(m)_{1}),%
 \ldots,C_{\text{in}}(C_{\text{out}}(m)_{N}))   where      C  out    (  m  )    =   (   (    C  out     (  m  )   1    ,  ‚Ä¶  ,    (  m  )   N   )   )          subscript  C  out   m       subscript  C  out    subscript  m  1    normal-‚Ä¶   subscript  m  N      C_{\text{out}}(m)=((C_{\text{out}}(m)_{1},\ldots,(m)_{N}))   . Finally we will take    C  out     subscript  C  out    C_{\text{out}}   to be RS code , which has an errors and erasure decoder, and    K  =   O   (   log  N   )        K    O    N      K=O(\log{N})   , which in turn implies that MLD on the inner code will be poly(   N   N   N   ) time.  Maximum likelihood decoding(MLD) : MLD is a decoding method for error correcting codes, which outputs the codeword closest to the received word in Hamming distance. The MLD function denoted by     D   M  L  D    :    ‚àë  n   ‚Üí  C      normal-:   subscript  D    M  L  D     normal-‚Üí   superscript   n   C     D_{MLD}:\sum^{n}\rightarrow C   is defined as follows. For every    y  ‚àà   ‚àë  n       y   subscript   n     y\in\sum_{n}   ,      D   M  L  D     (  y  )    =    arg    min   c  ‚àà  C    Œî     (  c  ,  y  )           subscript  D    M  L  D    y         subscript     c  C    normal-Œî     c  y      D_{MLD}(y)=\arg\min_{c\in C}\Delta(c,y)   .  Probability density function : A probability distribution     Pr   [  ‚àô  ]      Pr  normal-‚àô    \Pr[\bullet]   on a sample space   S   S   S   is a mapping from events of   S   S   S   to real numbers such that     Pr   [  A  ]    ‚â•  0       Pr  A   0    \Pr[A]\geq 0   for any event   A   A   A   ,     Pr   [  S  ]    =  1       Pr  S   1    \Pr[S]=1   , and     Pr   [   A  ‚à™  B   ]    =    Pr   [  A  ]    +   Pr   [  B  ]          Pr    A  B       Pr  A    Pr  B      \Pr[A\cup B]=\Pr[A]+\Pr[B]   for any two mutually exclusive events   A   A   A   and   B   B   B     Expected value : The expected value of a discrete random variable    X   X   X   is    ùîº  =    ‚àë  x    Pr   [   X  =  x   ]         ùîº    subscript   x    Pr    X  x       \mathbb{E}=\sum_{x}\Pr[X=x]   .   Randomized algorithm  Consider the received word    ùê≤  =   (   y  1   ,  ‚Ä¶  ,   y  N   )   ‚àà    [   q  n   ]   N         ùê≤    subscript  y  1   normal-‚Ä¶   subscript  y  N          superscript   delimited-[]   superscript  q  n    N      \mathbf{y}=(y_{1},\ldots,y_{N})\in[q^{n}]^{N}   which corrupted by noisy channel . The following is the algorithm description for the general case. In this algorithm, we can decode y by just declaring an erasure at every bad position and running the errors and erasure decoding algorithm for    C  out     subscript  C  out    C_{\text{out}}   on the resulting vector.  Randomized_Decoder '''Given : '''    ùê≤  =   (   y  1   ,  ‚Ä¶  ,   y  N   )   ‚àà    [   q  n   ]   N         ùê≤    subscript  y  1   normal-‚Ä¶   subscript  y  N          superscript   delimited-[]   superscript  q  n    N      \mathbf{y}=(y_{1},\dots,y_{N})\in[q^{n}]^{N}   .   For every    1  ‚â§  i  ‚â§  N        1  i       N     1\leq i\leq N   , compute     y  i  ‚Ä≤   =   M  L   D   C  in     (   y  i   )         superscript   subscript  y  i   normal-‚Ä≤     M  L   subscript  D   subscript  C  in     subscript  y  i      y_{i}^{\prime}=MLD_{C_{\text{in}}}(y_{i})   .  Set     œâ  i   =   min   (   Œî   (    C  in    (   y  i  ‚Ä≤   )    ,   y  i   )    ,   d  2   )         subscript  œâ  i       normal-Œî      subscript  C  in    superscript   subscript  y  i   normal-‚Ä≤     subscript  y  i       d  2      \omega_{i}=\min(\Delta(C_{\text{in}}(y_{i}^{\prime}),y_{i}),{d\over 2})   .  For every    1  ‚â§  i  ‚â§  N        1  i       N     1\leq i\leq N   , repeat : With probability     2   œâ  i    d        2   subscript  œâ  i    d    2\omega_{i}\over d   , set     y  i  ‚Ä≤‚Ä≤   ‚Üê      normal-‚Üê   superscript   subscript  y  i   ‚Ä≤‚Ä≤   absent    y_{i}^{\prime\prime}\leftarrow   ?, otherwise set     y  i  ‚Ä≤‚Ä≤   =   y  i  ‚Ä≤        superscript   subscript  y  i   ‚Ä≤‚Ä≤    superscript   subscript  y  i   normal-‚Ä≤     y_{i}^{\prime\prime}=y_{i}^{\prime}   .  Run errors and erasure algorithm for    C  out     subscript  C  out    C_{\text{out}}   on     ùê≤  ‚Ä≤‚Ä≤   =   (   y  1  ‚Ä≤‚Ä≤   ,  ‚Ä¶  ,   y  N  ‚Ä≤‚Ä≤   )        superscript  ùê≤  ‚Ä≤‚Ä≤     superscript   subscript  y  1   ‚Ä≤‚Ä≤   normal-‚Ä¶   superscript   subscript  y  N   ‚Ä≤‚Ä≤      \mathbf{y}^{\prime\prime}=(y_{1}^{\prime\prime},\ldots,y_{N}^{\prime\prime})   .   Theorem 1.  Let y be a received word such that there exists a codeword     ùêú  =   (   c  1   ,  ‚Ä¶  ,   c  N   )   ‚àà    C  out   ‚àò   C  in    ‚äÜ    [   q  n   ]   N         ùêú    subscript  c  1   normal-‚Ä¶   subscript  c  N            subscript  C  out    subscript  C  in          superscript   delimited-[]   superscript  q  n    N      \mathbf{c}=(c_{1},\ldots,c_{N})\in C_{\text{out}}\circ{C_{\text{in}}}\subseteq%
 [q^{n}]^{N}    such that      Œî   (  ùêú  ,  ùê≤  )    <    D  d   2         normal-Œî   ùêú  ùê≤        D  d   2     \Delta(\mathbf{c},\mathbf{y})<\frac{Dd}{2}   . Then the deterministic GMD algorithm outputs    ùêú   ùêú   \mathbf{c}   .  Note that a naive decoding algorithm for concatenated codes can correct up to     D  d   4        D  d   4    Dd\over 4   errors.  Lemma 1.  Let the assumption in Theorem 1 hold. And if     ùê≤  ‚Ä≤‚Ä≤     superscript  ùê≤  ‚Ä≤‚Ä≤    \mathbf{y^{\prime\prime}}    has     e  ‚Ä≤     superscript  e  normal-‚Ä≤    e^{\prime}    errors and     s  ‚Ä≤     superscript  s  normal-‚Ä≤    s^{\prime}    erasures(when compared with    ùêú   ùêú   \mathbf{c}    ) after  Step 1 , then      ùîº   [    2   e  ‚Ä≤    +   s  ‚Ä≤    ]    <  D        ùîº   delimited-[]      2   superscript  e  normal-‚Ä≤     superscript  s  normal-‚Ä≤      D    \mathbb{E}[2e^{\prime}+s^{\prime}]   .  If      2   e  ‚Ä≤    +   s  ‚Ä≤    <  D          2   superscript  e  normal-‚Ä≤     superscript  s  normal-‚Ä≤    D    2e^{\prime}+s^{\prime}   , then the algorithm in Step 2 will output   ùêú   ùêú   \mathbf{c}   . The lemma above says that in expectation, this is indeed the case. Note that this is not enough to prove Theorem 1 , but can be crucial in developing future variations of the algorithm.  Proof of lemma 1. For every    1  ‚â§  i  ‚â§  N        1  i       N     1\leq i\leq N   , define     e  i   =   Œî   (   y  i   ,   c  i   )         subscript  e  i     normal-Œî    subscript  y  i    subscript  c  i       e_{i}=\Delta(y_{i},c_{i})   . This implies that        ‚àë   i  =  1   N    e  i    <     D  d   2    (  1  )          superscript   subscript     i  1    N    subscript  e  i         D  d   2   1     \sum_{i=1}^{N}e_{i}<\frac{Dd}{2}\qquad\qquad(1)     Next for every    1  ‚â§  i  ‚â§  N        1  i       N     1\leq i\leq N   , we define two indicator variables :      X   =   i  ?   1     fragments  X   subscript   superscript   normal-?   i   1    X{{}_{i}^{?}}=1   iff      y  i  ‚Ä≤‚Ä≤   =  ?   ,       superscript   subscript  y  i   ‚Ä≤‚Ä≤   normal-?    y_{i}^{\prime\prime}=?,     and      X   =   i  e   1     fragments  X   subscript   superscript   e   i   1    X{{}_{i}^{e}}=1   iff      C  in    (   y  i  ‚Ä≤‚Ä≤   )    ‚â†   c  i          subscript  C  in    superscript   subscript  y  i   ‚Ä≤‚Ä≤     subscript  c  i     C_{\text{in}}(y_{i}^{\prime\prime})\neq c_{i}     and        y  i  ‚Ä≤‚Ä≤   ‚â†  ?   .       superscript   subscript  y  i   ‚Ä≤‚Ä≤   normal-?    y_{i}^{\prime\prime}\neq?.     We claim that we are done if we can show that for every    1  ‚â§  i  ‚â§  N        1  i       N     1\leq i\leq N   :      ùîº   [  2  X   +   i  e   X   ]   i  ?    ‚â§    2   e  i    d    (  2  )      fragments  E   fragments  normal-[  2  X   subscript   superscript   e   i   X   subscript   superscript  normal-]  normal-?   i         2   subscript  e  i    d   italic-   fragments  normal-(  2  normal-)     \mathbb{E}[2X{{}_{i}^{e}+X{{}_{i}^{?}}}]\leq{{2e_{i}}\over d}\qquad\qquad(2)     Clearly, by definition     e  ‚Ä≤   =   ‚àë  i   X    i  e      fragments   superscript  e  normal-‚Ä≤     subscript   i   X   subscript   superscript  absent  e   i     e^{\prime}=\sum_{i}X{{}_{i}^{e}}   and     s  ‚Ä≤   =   ‚àë  i   X    i  ?      fragments   superscript  s  normal-‚Ä≤     subscript   i   X   subscript   superscript  absent  normal-?   i     s^{\prime}=\sum_{i}X{{}_{i}^{?}}   . Further, by the linearity of expectation, we get     ùîº   [    2   e  ‚Ä≤    +   s  ‚Ä≤    ]    ‚â§    2  d     ‚àë  i    e  i     <  D          ùîº   delimited-[]      2   superscript  e  normal-‚Ä≤     superscript  s  normal-‚Ä≤          2  d     subscript   i    subscript  e  i          D     \mathbb{E}[2e^{\prime}+s^{\prime}]\leq{2\over d}\sum_{i}e_{i}   . We consider two cases to prove (2)     i  ‚Ä≤   t  h       superscript  i  normal-‚Ä≤   t  h    i^{\prime}th   block is correctly decoded( Case 1 ),     i  ‚Ä≤   t  h       superscript  i  normal-‚Ä≤   t  h    i^{\prime}th   block is incorrectly decoded( Case 2 )  Case 1:     (    c  i   =    C  in    (   y  i  ‚Ä≤   )     )       subscript  c  i      subscript  C  in    superscript   subscript  y  i   normal-‚Ä≤      (c_{i}=C_{\text{in}}(y_{i}^{\prime}))     Note that if     y  i  ‚Ä≤‚Ä≤   =  ?       superscript   subscript  y  i   ‚Ä≤‚Ä≤   normal-?    y_{i}^{\prime\prime}=?   then     X  i  e   =  0       superscript   subscript  X  i   e   0    X_{i}^{e}=0   , and     Pr   [    y  i  ‚Ä≤‚Ä≤   =  ?   ]    =    2   œâ  i    d        Pr     superscript   subscript  y  i   ‚Ä≤‚Ä≤   normal-?        2   subscript  œâ  i    d     \Pr[y_{i}^{\prime\prime}=?]={2\omega_{i}\over d}   implies     ùîº   [   X  i  ?   ]    =   Pr   [    X  i  ?   =  1   ]    =    2   œâ  i    d           ùîº   delimited-[]   superscript   subscript  X  i   normal-?      Pr     superscript   subscript  X  i   normal-?   1             2   subscript  œâ  i    d      \mathbb{E}[X_{i}^{?}]=\Pr[X_{i}^{?}=1]={2\omega_{i}\over d}   , and     ùîº   [   X  i  e   ]    =   Pr   [    X  i  e   =  1   ]    =  0          ùîº   delimited-[]   superscript   subscript  X  i   e      Pr     superscript   subscript  X  i   e   1         0     \mathbb{E}[X_{i}^{e}]=\Pr[X_{i}^{e}=1]=0   .  Further, by definition we have       œâ  i   =   min   (   Œî   (    C  in    (   y  i  ‚Ä≤   )    ,   y  i   )    ,   d  2   )    ‚â§   Œî   (    C  in    (   y  i  ‚Ä≤   )    ,   y  i   )    =   Œî   (   c  i   ,   y  i   )    =   e  i          subscript  œâ  i       normal-Œî      subscript  C  in    superscript   subscript  y  i   normal-‚Ä≤     subscript  y  i       d  2           normal-Œî      subscript  C  in    superscript   subscript  y  i   normal-‚Ä≤     subscript  y  i            normal-Œî    subscript  c  i    subscript  y  i           subscript  e  i      \omega_{i}=\min(\Delta(C_{\text{in}}(y_{i}^{\prime}),y_{i}),{d\over 2})\leq%
 \Delta(C_{\text{in}}(y_{i}^{\prime}),y_{i})=\Delta(c_{i},y_{i})=e_{i}     Case 2:     (    c  i   ‚â†    C  in    (   y  i  ‚Ä≤   )     )       subscript  c  i      subscript  C  in    superscript   subscript  y  i   normal-‚Ä≤      (c_{i}\neq C_{\text{in}}(y_{i}^{\prime}))     In this case,       ùîº   [   X  i  ?   ]    =    2   œâ  i    d         ùîº   delimited-[]   superscript   subscript  X  i   normal-?         2   subscript  œâ  i    d     \mathbb{E}[X_{i}^{?}]={2\omega_{i}\over d}   and      ùîº   [   X  i  e   ]    =   Pr   [    X  i  e   =  1   ]    =   1  -    2   œâ  i    d     .          ùîº   delimited-[]   superscript   subscript  X  i   e      Pr     superscript   subscript  X  i   e   1           1      2   subscript  œâ  i    d       \mathbb{E}[X_{i}^{e}]=\Pr[X_{i}^{e}=1]=1-{2\omega_{i}\over d}.     Since     c  i   ‚â†    C  in    (   y  i  ‚Ä≤   )         subscript  c  i      subscript  C  in    superscript   subscript  y  i   normal-‚Ä≤      c_{i}\neq C_{\text{in}}(y_{i}^{\prime})   ,      e  i   +   œâ  i    ‚â•  d         subscript  e  i    subscript  œâ  i    d    e_{i}+\omega_{i}\geq d   . This follows another case analysis when    (    œâ  i   =   Œî   (    C  in    (   y  i  ‚Ä≤   )    ,   y  i   )    <   d  2    )         subscript  œâ  i     normal-Œî      subscript  C  in    superscript   subscript  y  i   normal-‚Ä≤     subscript  y  i            d  2      (\omega_{i}=\Delta(C_{\text{in}}(y_{i}^{\prime}),y_{i})<{d\over 2})   or not.  Finally, this implies        ùîº   [    2   X  i  e    +   X  i  ?    ]    =   2  -    2   œâ  i    d    ‚â§    2   e  i    d    .          ùîº   delimited-[]      2   superscript   subscript  X  i   e     superscript   subscript  X  i   normal-?        2      2   subscript  œâ  i    d             2   subscript  e  i    d      \mathbb{E}[2X_{i}^{e}+X_{i}^{?}]=2-{2\omega_{i}\over d}\leq{2e_{i}\over d}.     In the following sections, we will finally show that the deterministic version of the algorithm above can do unique decoding of     C  out   ‚àò   C  in        subscript  C  out    subscript  C  in     C_{\text{out}}\circ C_{\text{in}}   up to half its design distance.  Modified randomized algorithm  Note that, in the previous version of the GMD algorithm in step "3", we do not really need to use "fresh" randomness for each   i   i   i   . Now we come up with another randomized version of the GMD algorithm that uses the same randomness for every   i   i   i   . This idea follows the algorithm below.  Modified_Randomized_Decoder '''Given : '''    ùê≤  =   (   y  1   ,  ‚Ä¶  ,   y  N   )   ‚àà    [   q  n   ]   N         ùê≤    subscript  y  1   normal-‚Ä¶   subscript  y  N          superscript   delimited-[]   superscript  q  n    N      \mathbf{y}=(y_{1},\ldots,y_{N})\in[q^{n}]^{N}   , pick    Œ∏  ‚àà   [  0  ,  1  ]       Œ∏   0  1     \theta\in[0,1]   at random. Then every for every    1  ‚â§  i  ‚â§  N        1  i       N     1\leq i\leq N   :   Set     y  i  ‚Ä≤   =   M  L   D   C  in     (   y  i   )         superscript   subscript  y  i   normal-‚Ä≤     M  L   subscript  D   subscript  C  in     subscript  y  i      y_{i}^{\prime}=MLD_{C_{\text{in}}}(y_{i})   .  Compute     œâ  i   =   min   (   Œî   (    C  in    (   y  i  ‚Ä≤   )    ,   y  i   )    ,   d  2   )         subscript  œâ  i       normal-Œî      subscript  C  in    superscript   subscript  y  i   normal-‚Ä≤     subscript  y  i       d  2      \omega_{i}=\min(\Delta(C_{\text{in}}(y_{i}^{\prime}),y_{i}),{d\over 2})   .  If   Œ∏   Œ∏   \theta   {2\omega_i \over d}, set     y  i  ‚Ä≤‚Ä≤   ‚Üê      normal-‚Üê   superscript   subscript  y  i   ‚Ä≤‚Ä≤   absent    y_{i}^{\prime\prime}\leftarrow   ?, otherwise set     y  i  ‚Ä≤‚Ä≤   =   y  i  ‚Ä≤        superscript   subscript  y  i   ‚Ä≤‚Ä≤    superscript   subscript  y  i   normal-‚Ä≤     y_{i}^{\prime\prime}=y_{i}^{\prime}   .  Run errors and erasure algorithm for    C  out     subscript  C  out    C_{\text{out}}   on     ùê≤  ‚Ä≤‚Ä≤   =   (   y  1  ‚Ä≤‚Ä≤   ,  ‚Ä¶  ,   y  N  ‚Ä≤‚Ä≤   )        superscript  ùê≤  ‚Ä≤‚Ä≤     superscript   subscript  y  1   ‚Ä≤‚Ä≤   normal-‚Ä¶   superscript   subscript  y  N   ‚Ä≤‚Ä≤      \mathbf{y}^{\prime\prime}=(y_{1}^{\prime\prime},\ldots,y_{N}^{\prime\prime})   .   For the proof of Lemma 1 , we only use the randomness to show that        Pr   [    y  i  ‚Ä≤‚Ä≤   =  ?   ]    =    2   œâ  i    d    .       Pr     superscript   subscript  y  i   ‚Ä≤‚Ä≤   normal-?        2   subscript  œâ  i    d     \Pr[y_{i}^{\prime\prime}=?]={2\omega_{i}\over d}.     In this version of the GMD algorithm, we note that        Pr   [    y  i  ‚Ä≤‚Ä≤   =  ?   ]    =   Pr   [   Œ∏  ‚àà   [  0  ,    2   œâ  i    d   ]    ]    =    2   œâ  i    d    .         Pr     superscript   subscript  y  i   ‚Ä≤‚Ä≤   normal-?     Pr    Œ∏   0      2   subscript  œâ  i    d               2   subscript  œâ  i    d      \Pr[y_{i}^{\prime\prime}=?]=\Pr[\theta\in[0,{2\omega_{i}\over d}]]={2\omega_{i%
 }\over d}.     The second equality above follows from the choice of   Œ∏   Œ∏   \theta   . The proof of Lemma 1 can be also used to show    ùîº   [    2   e  ‚Ä≤    +   s  ‚Ä≤    ]       ùîº   delimited-[]      2   superscript  e  normal-‚Ä≤     superscript  s  normal-‚Ä≤       \mathbb{E}[2e^{\prime}+s^{\prime}]   D for version2 of GMD. In the next section, we will see how to get a deterministic version of the GMD algorithm by choosing Œ∏ from a polynomially sized set as opposed to the current infinite set    [  0  ,  1  ]     0  1    [0,1]   .  Deterministic algorithm  Let    Q  =    {  0  ,  1  }   ‚à™   {    2   œâ  1    d   ,  ‚Ä¶  ,    2   œâ  N    d   }        Q     0  1        2   subscript  œâ  1    d   normal-‚Ä¶      2   subscript  œâ  N    d       Q=\{0,1\}\cup\{{2\omega_{1}\over d},\ldots,{2\omega_{N}\over d}\}   . Since for each     i  ,   œâ  i    =   min   (   Œî   (   ùê≤  ùê¢  ‚Ä≤   ,   ùê≤  ùê¢   )    ,   d  2   )         i   subscript  œâ  i        normal-Œî    superscript   subscript  ùê≤  ùê¢   normal-‚Ä≤    subscript  ùê≤  ùê¢       d  2      i,\omega_{i}=\min(\Delta(\mathbf{y_{i}^{\prime}},\mathbf{y_{i}}),{d\over 2})   , we have      Q  =    {  0  ,  1  }   ‚à™   {   q  1   ,  ‚Ä¶  ,   q  m   }        Q     0  1     subscript  q  1   normal-‚Ä¶   subscript  q  m       Q=\{0,1\}\cup\{q_{1},\ldots,q_{m}\}     where     q  1   <   q  2   <  ‚ãØ  <   q  m          subscript  q  1    subscript  q  2        normal-‚ãØ        subscript  q  m      q_{1}   for some    m  ‚â§   ‚åä   d  2   ‚åã       m      d  2      m\leq\left\lfloor\frac{d}{2}\right\rfloor   . Note that for every    Œ∏  ‚àà   [   q  i   ,   q   i  +  1    ]       Œ∏    subscript  q  i    subscript  q    i  1       \theta\in[q_{i},q_{i+1}]   , the step 1 of the second version of randomized algorithm outputs the same    ùê≤  ‚Ä≤‚Ä≤     superscript  ùê≤  ‚Ä≤‚Ä≤    \mathbf{y^{\prime\prime}}   . Thus, we need to consider all possible value of    Œ∏  ‚àà  Q      Œ∏  Q    \theta\in Q   . This gives the deterministic algorithm below.  Deterministic_Decoder ''' Given : '''    ùê≤  =   (   y  1   ,  ‚Ä¶  ,   y  N   )   ‚àà    [   q  n   ]   N         ùê≤    subscript  y  1   normal-‚Ä¶   subscript  y  N          superscript   delimited-[]   superscript  q  n    N      \mathbf{y}=(y_{1},\ldots,y_{N})\in[q^{n}]^{N}   , for every    Œ∏  ‚àà  Q      Œ∏  Q    \theta\in Q   , repeat the following.   Compute     y  i  ‚Ä≤   =   M  L   D   C  in     (   y  i   )         superscript   subscript  y  i   normal-‚Ä≤     M  L   subscript  D   subscript  C  in     subscript  y  i      y_{i}^{\prime}=MLD_{C_{\text{in}}}(y_{i})   for    1  ‚â§  i  ‚â§  N        1  i       N     1\leq i\leq N   .  Set     œâ  i   =   min   (   Œî   (    C  in    (   y  i  ‚Ä≤   )    ,   y  i   )    ,   d  2   )         subscript  œâ  i       normal-Œî      subscript  C  in    superscript   subscript  y  i   normal-‚Ä≤     subscript  y  i       d  2      \omega_{i}=\min(\Delta(C_{\text{in}}(y_{i}^{\prime}),y_{i}),{d\over 2})   for every    1  ‚â§  i  ‚â§  N        1  i       N     1\leq i\leq N   .  If   Œ∏   Œ∏   \theta   {2\omega_i \over d}, set     y  i  ‚Ä≤‚Ä≤   ‚Üê      normal-‚Üê   superscript   subscript  y  i   ‚Ä≤‚Ä≤   absent    y_{i}^{\prime\prime}\leftarrow   ?, otherwise set     y  i  ‚Ä≤‚Ä≤   =   y  i  ‚Ä≤        superscript   subscript  y  i   ‚Ä≤‚Ä≤    superscript   subscript  y  i   normal-‚Ä≤     y_{i}^{\prime\prime}=y_{i}^{\prime}   .  Run errors-and-erasures algorithm for    C  out     subscript  C  out    C_{\text{out}}   on     ùê≤  ‚Ä≤‚Ä≤   =   (   y  1  ‚Ä≤‚Ä≤   ,  ‚Ä¶  ,   y  N  ‚Ä≤‚Ä≤   )        superscript  ùê≤  ‚Ä≤‚Ä≤     superscript   subscript  y  1   ‚Ä≤‚Ä≤   normal-‚Ä¶   superscript   subscript  y  N   ‚Ä≤‚Ä≤      \mathbf{y^{\prime\prime}}=(y_{1}^{\prime\prime},\ldots,y_{N}^{\prime\prime})   . Let    c  Œ∏     subscript  c  Œ∏    c_{\theta}   be the codeword in     C  out   ‚àò   C  in        subscript  C  out    subscript  C  in     C_{\text{out}}\circ C_{\text{in}}   corresponding to the output of the algorithm, if any.  Among all the    c  Œ∏     subscript  c  Œ∏    c_{\theta}   output in 4, output the one closest to   ùê≤   ùê≤   \mathbf{y}      Every loop of 1~4 can be run in polynomial time , the algorithm above can also be computed in polynomial time. Specifically, each call to an errors and erasures decoder of    O   (    N  Q   n   O   (  1  )      +   N   T  out     )       O      N  Q   superscript  n    O  1       N   subscript  T  out       O(NQn^{O(1)}+NT_{\text{out}})   time. Finally, the runtime of the algorithm above is    T  out     subscript  T  out    T_{\text{out}}   where $T_\text{out}$ is the running time of the outer errors and erasures decoder.  See also   Concatenated codes  Reed Solomon error correction  Welch Berlekamp algorithm   References   University at Buffalo Lecture Notes on Coding Theory ‚Äì Atri Rudra  MIT Lecture Notes on Essential Coding Theory ‚Äì Madhu Sudan  University of Washington ‚Äì Venkatesan Guruswami  G. David Forney. Generalized Minimum Distance decoding. IEEE Transactions on Information Theory , 12:125‚Äì131, 1966   "   Category:Error detection and correction  Category:Coding theory  Category:Finite fields  Category:Information theory   