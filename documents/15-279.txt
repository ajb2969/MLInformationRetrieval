   Discrete Universal Denoiser      Discrete Universal Denoiser   In information theory and signal processing , the Discrete Universal Denoiser (DUDE) is a denoising scheme for recovering sequences over a finite alphabet, which have been corrupted by a discrete memoryless channel . The DUDE was proposed in 2005 by Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdú and Marcelo J. Weinberger . 1  Overview  The Discrete Universal Denoiser  2 (DUDE) is a denoising scheme that estimates an unknown signal     x  n   =   (    x  1   …   x  n    )        superscript  x  n      subscript  x  1   normal-…   subscript  x  n      x^{n}=\left(x_{1}\ldots x_{n}\right)   over a finite alphabet from a noisy version     z  n   =   (    z  1   …   z  n    )        superscript  z  n      subscript  z  1   normal-…   subscript  z  n      z^{n}=\left(z_{1}\ldots z_{n}\right)   . While most denoising schemes in the signal processing and statistics literature deal with signals over an infinite alphabet (notably, real-valued signals), the DUDE addresses the finite alphabet case. The noisy version    z  n     superscript  z  n    z^{n}   is assumed to be generated by transmitting    x  n     superscript  x  n    x^{n}   through a known discrete memoryless channel .  For a fixed context length parameter   k   k   k   , the DUDE counts of the occurrences of all the strings of length     2  k   +  1        2  k   1    2k+1   appearing in    z  n     superscript  z  n    z^{n}   . The estimated value     x  ^   i     subscript   normal-^  x   i    \hat{x}_{i}   is determined based the two-sided length-   k   k   k    context     (   z   i  -  k    ,  …  ,   z   i  -  1    ,   z   i  +  1    ,  …  ,   z   i  +  k    )      subscript  z    i  k    normal-…   subscript  z    i  1     subscript  z    i  1    normal-…   subscript  z    i  k      \left(z_{i-k},\ldots,z_{i-1},z_{i+1},\ldots,z_{i+k}\right)   of    z  i     subscript  z  i    z_{i}   , taking into account all the other tokens in    z  n     superscript  z  n    z^{n}   with the same context, as well as the known channel matrix and the loss function being used.  The idea underlying the DUDE is best illustrated when    x  n     superscript  x  n    x^{n}   is a realization of a random vector    X  n     superscript  X  n    X^{n}   . If the conditional distribution     X  i   |   Z   i  -  k    ,  …  ,   Z   i  -  1    ,   Z   i  +  1    ,  …  ,   Z   i  +  k       fragments   subscript  X  i   normal-|   subscript  Z    i  k    normal-,  normal-…  normal-,   subscript  Z    i  1    normal-,   subscript  Z    i  1    normal-,  normal-…  normal-,   subscript  Z    i  k      X_{i}|Z_{i-k},\ldots,Z_{i-1},Z_{i+1},\ldots,Z_{i+k}   , namely the distribution of the noiseless symbol    X  i     subscript  X  i    X_{i}   conditional on its noisy context    (   Z   i  -  k    ,  …  ,   Z   i  -  1    ,   Z   i  +  1    ,  …  ,   Z   i  +  k    )      subscript  Z    i  k    normal-…   subscript  Z    i  1     subscript  Z    i  1    normal-…   subscript  Z    i  k      \left(Z_{i-k},\ldots,Z_{i-1},Z_{i+1},\ldots,Z_{i+k}\right)   was available, the optimal estimator     X  ^   i     subscript   normal-^  X   i    \hat{X}_{i}   would be the Bayes Response to     X  i   |   Z   i  -  k    ,  …  ,   Z   i  -  1    ,   Z   i  +  1    ,  …  ,   Z   i  +  k       fragments   subscript  X  i   normal-|   subscript  Z    i  k    normal-,  normal-…  normal-,   subscript  Z    i  1    normal-,   subscript  Z    i  1    normal-,  normal-…  normal-,   subscript  Z    i  k      X_{i}|Z_{i-k},\ldots,Z_{i-1},Z_{i+1},\ldots,Z_{i+k}   . Fortunately, when the channel matrix is known and non-degenerate, this conditional distribution can be expressed in terms of the conditional distribution     Z  i   |   Z   i  -  k    ,  …  ,   Z   i  -  1    ,   Z   i  +  1    ,  …  ,   Z   i  +  k       fragments   subscript  Z  i   normal-|   subscript  Z    i  k    normal-,  normal-…  normal-,   subscript  Z    i  1    normal-,   subscript  Z    i  1    normal-,  normal-…  normal-,   subscript  Z    i  k      Z_{i}|Z_{i-k},\ldots,Z_{i-1},Z_{i+1},\ldots,Z_{i+k}   , namely the distribution of the noisy symbol    Z  i     subscript  Z  i    Z_{i}   conditional on its noisy context. This conditional distribution, in turn, can be estimated from an individual observed noisy signal    Z  n     superscript  Z  n    Z^{n}   by virtue of the Law of Large Numbers , provided   n   n   n   is ``large enough''.  Applying the DUDE scheme with a context length   k   k   k   to a sequence of length   n   n   n   over a finite alphabet   𝒵   𝒵   \mathcal{Z}   requires    O   (  n  )       O  n    O(n)   operations and space    O   (   min   (  n  ,    |  𝒵  |    2  k    )    )       O    n   superscript    𝒵     2  k       O\left(\min(n,|\mathcal{Z}|^{2k})\right)   .  Under certain assumptions, the DUDE is a universal scheme in the sense of asymptotically performing as well as an optimal denoiser, which has oracle access to the unknown sequence. More specifically, assume that the denoising performance is measured using a given single-character fidelity criterion, and consider the regime where the sequence length   n   n   n   tends to infinity and the context length    k  =   k  n       k   subscript  k  n     k=k_{n}   tends to infinity “not too fast”. In the stochastic setting, where a doubly-infinite sequence noiseless sequence   𝐱   𝐱   \mathbf{x}   is a realization of a stationary process   𝐗   𝐗   \mathbf{X}   , the DUDE asymptotically performs, in expectation, as well as the best denoiser, which has oracle access to the source distribution   𝐗   𝐗   \mathbf{X}   . In the single-sequence, or “semi-stochastic” setting with a fixed doubly-infinite sequence   𝐱   𝐱   \mathbf{x}   , the DUDE asymptotically performs as well as the best “sliding window” denoiser, namely any denoiser that determines     x  ^   i     subscript   normal-^  x   i    \hat{x}_{i}   from the window    (   z   i  -  k    ,  …  ,   z   i  +  k    )      subscript  z    i  k    normal-…   subscript  z    i  k      \left(z_{i-k},\ldots,z_{i+k}\right)   , which has oracle access to   𝐱   𝐱   \mathbf{x}   .  The discrete denoising problem  (Figure)  Block diagram description of the discrete denoising problem   Let   𝒳   𝒳   \mathcal{X}   be the finite alphabet of a fixed but unknown original “noiseless” sequence     x  n   =   (   x  1   ,  …  ,   x  n   )   ∈   𝒳  n          superscript  x  n     subscript  x  1   normal-…   subscript  x  n          superscript  𝒳  n      x^{n}=\left(x_{1},\ldots,x_{n}\right)\in\mathcal{X}^{n}   . The sequence is fed into a discrete memoryless channel (DMC). The DMC operates on each symbol    x  i     subscript  x  i    x_{i}   independently, producing a corresponding random symbol    Z  i     subscript  Z  i    Z_{i}   in a finite alphabet   𝒵   𝒵   \mathcal{Z}   . The DMC is known and given as a   𝒳   𝒳   \mathcal{X}   -by-   𝒵   𝒵   \mathcal{Z}   Markov matrix   Π   normal-Π   \Pi   , whose entries are    π   (  x  ,  z  )   =  ℙ   (  Z  =   z   |  X  =  x  )      fragments  π   fragments  normal-(  x  normal-,  z  normal-)    P   fragments  normal-(  Z   z  normal-|  X   x  normal-)     \pi(x,z)=\mathbb{P}\left(Z=z\,|\,X=x\right)   . It is convenient to write    π  z     subscript  π  z    \pi_{z}   for the   z   z   z   -column of   Π   normal-Π   \Pi   . The DMC produces a random noisy sequence     Z  n   =   (   z  1   ,  …  ,   z  n   )   ∈   𝒵  n          superscript  Z  n     subscript  z  1   normal-…   subscript  z  n          superscript  𝒵  n      Z^{n}=\left(z_{1},\ldots,z_{n}\right)\in\mathcal{Z}^{n}   . A specific realization of this random vector will be denoted by    z  n     superscript  z  n    z^{n}   . A denoiser is a function      X  ^   n   :    𝒵  n   →   𝒳  n       normal-:   superscript   normal-^  X   n    normal-→   superscript  𝒵  n    superscript  𝒳  n      \hat{X}^{n}:\mathcal{Z}^{n}\to\mathcal{X}^{n}   that attempts to recover the noiseless sequence    x  n     superscript  x  n    x^{n}   from a distorted version    z  n     superscript  z  n    z^{n}   . A specific denoised sequence is denoted by      x  ^   n   =     X  ^   n    (   z  n   )    =   (     X  ^   1    (   z  n   )    ,  …  ,     X  ^   n    (   z  n   )    )          superscript   normal-^  x   n      superscript   normal-^  X   n    superscript  z  n             subscript   normal-^  X   1    superscript  z  n    normal-…     subscript   normal-^  X   n    superscript  z  n        \hat{x}^{n}=\hat{X}^{n}\left(z^{n}\right)=\left(\hat{X}_{1}(z^{n}),\ldots,\hat%
 {X}_{n}(z^{n})\right)   . The problem of choosing the denoiser     X  ^   n     superscript   normal-^  X   n    \hat{X}^{n}   is known as signal estimation, filtering or smoothing . To compare candidate denoisers, we choose a single-symbol fidelity criterion    Λ  :    𝒳  ×  𝒳   →   [  0  ,  ∞  )       normal-:  normal-Λ   normal-→    𝒳  𝒳    0       \Lambda:\mathcal{X}\times\mathcal{X}\to[0,\infty)   (for example, the Hamming loss) and define the per-symbol loss of the denoiser     X  ^   n     superscript   normal-^  X   n    \hat{X}^{n}   at    (   x  n   ,   z  n   )      superscript  x  n    superscript  z  n     (x^{n},z^{n})   by            L    X  ^   n     (   x  n   ,   z  n   )    =     1  n       ∑   i  =  1   n     Λ   (    x  i    ,     X  ^   i    (   z  n   )    )       .              subscript  L   superscript   normal-^  X   n      superscript  x  n    superscript  z  n         1  n     superscript   subscript     i  1    n     normal-Λ    subscript  x  i      subscript   normal-^  X   i    superscript  z  n            \begin{aligned}\displaystyle L_{\hat{X}^{n}}\left(x^{n},z^{n}\right)=\frac{1}{%
 n}\sum_{i=1}^{n}\Lambda\left(x_{i}\,,\,\hat{X}_{i}(z^{n})\right)\,.\end{aligned}     Ordering the elements of the alphabet   𝒳   𝒳   \mathcal{X}   by    𝒳  =   (   a  1   ,  …  ,   a   |  𝒳  |    )       𝒳    subscript  a  1   normal-…   subscript  a    𝒳       \mathcal{X}=\left(a_{1},\ldots,a_{|\mathcal{X}|}\right)   , the fidelity criterion can be given by a    |  𝒳  |      𝒳    |\mathcal{X}|   -by-    |  𝒳  |      𝒳    |\mathcal{X}|   matrix, with columns of the form           λ   x  ^    =   (      Λ   (   a  1   ,   x  ^   )        ⋮       Λ   (   a   |  𝒳  |    ,   x  ^   )       )    .            subscript  λ   normal-^  x        normal-Λ    subscript  a  1    normal-^  x       normal-⋮      normal-Λ    subscript  a    𝒳     normal-^  x           \begin{aligned}\displaystyle\lambda_{\hat{x}}=\left(\begin{array}[]{c}\Lambda(%
 a_{1},\hat{x})\\
 \vdots\\
 \Lambda(a_{|\mathcal{X}|},\hat{x})\end{array}\right)\,.\end{aligned}     The DUDE scheme  Step 1: Calculating the empirical distribution in each context  The DUDE corrects symbols according to their context. The context length   k   k   k   used is a tuning parameter of the scheme. For     k  +  1   ≤  i  ≤   n  -  k           k  1   i         n  k      k+1\leq i\leq n-k   , define the left context of the   i   i   i   -th symbol in    z  n     superscript  z  n    z^{n}   by      l  k    (   z  n   ,  i  )    =   (   z   i  -  k    ,  …  ,   z   i  -  1    )          superscript  l  k     superscript  z  n   i      subscript  z    i  k    normal-…   subscript  z    i  1       l^{k}(z^{n},i)=\left(z_{i-k},\ldots,z_{i-1}\right)   and the corresponding right context as      r  k    (   z  n   ,  i  )    =   (   z   i  +  1    ,  …  ,   z   i  +  k    )          superscript  r  k     superscript  z  n   i      subscript  z    i  1    normal-…   subscript  z    i  k       r^{k}(z^{n},i)=\left(z_{i+1},\ldots,z_{i+k}\right)   . A two-sided context is a combination    (   l  k   ,   r  k   )      superscript  l  k    superscript  r  k     (l^{k},r^{k})   of a left and a right context.  The first step of the DUDE scheme is to calculate the empirical distribution of symbols in each possible two-sided context along the noisy sequence    z  n     superscript  z  n    z^{n}   . Formally, a given two-sided context     (   l  k   ,   r  k   )   ∈    𝒵  k   ×   𝒵  k          superscript  l  k    superscript  r  k       superscript  𝒵  k    superscript  𝒵  k      (l^{k},r^{k})\in\mathcal{Z}^{k}\times\mathcal{Z}^{k}   that appears once or more along    z  n     superscript  z  n    z^{n}   determines an empirical probability distribution over   𝒵   𝒵   \mathcal{Z}   , whose value at the symbol   z   z   z   is           μ   (   z  n   ,   l  k   ,   r  k   )    [  z  ]    =      |   {    k  +  1   ≤  i  ≤   n  -   k     |    (   z   i  -  k    ,  …  ,   z   i  +  k    )   =    l  k   z   r  k     }   |    |   {    k  +  1   ≤  i  ≤   n  -   k     |     l  k    (   z  n   ,  i  )    =    l  k   and   r  k    (   z  n   ,  i  )    =   r  k    }   |       .             μ    superscript  z  n    superscript  l  k    superscript  r  k     delimited-[]  z         conditional-set        k  1   i         n  k         subscript  z    i  k    normal-…   subscript  z    i  k        superscript  l  k   z   superscript  r  k          conditional-set        k  1   i         n  k            superscript  l  k     superscript  z  n   i       superscript  l  k   and   superscript  r  k     superscript  z  n   i          superscript  r  k            \begin{aligned}\displaystyle\mu\left(z^{n},l^{k},r^{k}\right)[z]=\frac{\Big|%
 \left\{k+1\leq i\leq n-k\,\,|\,\,(z_{i-k},\ldots,z_{i+k})=l^{k}zr^{k}\right\}%
 \Big|}{\Big|\left\{k+1\leq i\leq n-k\,\,|\,\,l^{k}(z^{n},i)=l^{k}\text{ and }r%
 ^{k}(z^{n},i)=r^{k}\right\}\Big|}\,.\end{aligned}     Thus, the first step of the DUDE scheme with context length   k   k   k   is to scan the input noisy sequence    z  n     superscript  z  n    z^{n}   once, and store the length-    |  𝒵  |      𝒵    |\mathcal{Z}|   empirical distribution vector    μ   (   z  n   ,   l  k   ,   r  k   )       μ    superscript  z  n    superscript  l  k    superscript  r  k      \mu\left(z^{n},l^{k},r^{k}\right)   (or its non-normalized version, the count vector) for each two-sided context found along    z  n     superscript  z  n    z^{n}   . Since there are at most     N   n  ,  k    =   min   (  n  ,    |  𝒵  |    2  k    )         subscript  N   n  k      n   superscript    𝒵     2  k       N_{n,k}=\min\left(n,|\mathcal{Z}|^{2k}\right)   possible two-sided contexts along    z  n     superscript  z  n    z^{n}   , this step requires    O   (  n  )       O  n    O(n)   operations and storage    O   (   N   n  ,  k    )       O   subscript  N   n  k      O(N_{n,k})   .  Step 2: Calculating the Bayes response to each context  Denote the column of single-symbol fidelity criterion   Λ   normal-Λ   \Lambda   , corresponding to the symbol     x  ^   ∈  𝒳       normal-^  x   𝒳    \hat{x}\in\mathcal{X}   , by    λ   x  ^      subscript  λ   normal-^  x     \lambda_{\hat{x}}   . We define the Bayes Response to any vector   𝐯   𝐯   \mathbf{v}   of length    |  𝒳  |      𝒳    |\mathcal{X}|   with non-negative entries as             X  ^    B  a  y  e  s     (  𝐯  )    =    argmin    x  ^   ∈  𝒳     λ   x  ^   ⊤    𝐯     .              subscript   normal-^  X     B  a  y  e  s    𝐯      subscript  argmin     normal-^  x   𝒳     superscript   subscript  λ   normal-^  x    top   𝐯       \begin{aligned}\displaystyle\hat{X}_{Bayes}(\mathbf{v})=\text{argmin}_{\hat{x}%
 \in\mathcal{X}}\lambda_{\hat{x}}^{\top}\mathbf{v}\,.\end{aligned}     This definition is motivated in the background below.  The second step of the DUDE scheme is to calculate, for each two-sided context    (   l  k   ,   r  k   )      superscript  l  k    superscript  r  k     (l^{k},r^{k})   observed in the previous step along    z  n     superscript  z  n    z^{n}   , and for each symbol    z  ∈  𝒵      z  𝒵    z\in\mathcal{Z}   observed in each context (namely, any   z   z   z   such that     l  r   z   r  k        superscript  l  r   z   superscript  r  k     l^{r}zr^{k}   is a substring of    z  n     superscript  z  n    z^{n}   ) the Bayes response to the vector      Π    -  ⊤    μ   (    z  n    ,    l  k    ,   r  k   )    ⊙   π  z      direct-product     superscript  normal-Π    absent  top    μ    superscript  z  n    superscript  l  k    superscript  r  k      subscript  π  z     \Pi^{-\top}\mu\left(z^{n}\,,\,l^{k}\,,\,r^{k}\right)\odot\pi_{z}   , namely           g   (   l  k   ,  z  ,   r  k   )    :=     X  ^    B  a  y  e  s     (     Π    -  ⊤    μ   (    z  n    ,    l  k    ,   r  k   )    ⊙   π  z    )     .          assign    g    superscript  l  k   z   superscript  r  k        subscript   normal-^  X     B  a  y  e  s     direct-product     superscript  normal-Π    absent  top    μ    superscript  z  n    superscript  l  k    superscript  r  k      subscript  π  z         \begin{aligned}\displaystyle g(l^{k},z,r^{k}):=\hat{X}_{Bayes}\left(\Pi^{-\top%
 }\mu\left(z^{n}\,,\,l^{k}\,,\,r^{k}\right)\odot\pi_{z}\right)\,.\end{aligned}     Note that the sequence    z  n     superscript  z  n    z^{n}   and the context length   k   k   k   are implicit. Here,    π  z     subscript  π  z    \pi_{z}   is the   z   z   z   -column of   Π   normal-Π   \Pi   and for vectors   𝐚   𝐚   \mathbf{a}   and   𝐛   𝐛   \mathbf{b}   ,    𝐚  ⊙  𝐛     direct-product  𝐚  𝐛    \mathbf{a}\odot\mathbf{b}   denotes their Schur (entrywise) product, defined by      (   𝐚  ⊙  𝐛   )   i   =    a  i    b  i         subscript   direct-product  𝐚  𝐛   i      subscript  a  i    subscript  b  i      \left(\mathbf{a}\odot\mathbf{b}\right)_{i}=a_{i}b_{i}   . Matrix multiplication is evaluated before the Schur product, so that      Π    -  ⊤    μ   ⊙   π  z      direct-product     superscript  normal-Π    absent  top    μ    subscript  π  z     \Pi^{-\top}\mu\odot\pi_{z}   stands for     (    Π    -  ⊤    μ   )   ⊙   π  z      direct-product     superscript  normal-Π    absent  top    μ    subscript  π  z     (\Pi^{-\top}\mu)\odot\pi_{z}   .  This formula assumed that the channel matrix   Π   normal-Π   \Pi   is square (     |  𝒳  |   =   |  𝒵  |         𝒳     𝒵     |\mathcal{X}|=|\mathcal{Z}|   ) and invertible. When     |  𝒳  |   ≤   |  𝒵  |         𝒳     𝒵     |\mathcal{X}|\leq|\mathcal{Z}|   and   Π   normal-Π   \Pi   is not invertible, under the reasonable assumption that it has full row rank, we replace     (   Π  ⊤   )    -  1      superscript   superscript  normal-Π  top     1     (\Pi^{\top})^{-1}   above with its Moore-Penrose pseudo-inverse      (   Π   Π  ⊤    )    -  1    Π       superscript    normal-Π   superscript  normal-Π  top      1    normal-Π    \left(\Pi\Pi^{\top}\right)^{-1}\Pi   and calculate instead           g   (   l  k   ,  z  ,   r  k   )    :=     X  ^    B  a  y  e  s     (      (   Π   Π  ⊤    )    -  1    Π  μ   (   z  n   ,   l  k   ,   r  k   )    ⊙   π  z    )     .          assign    g    superscript  l  k   z   superscript  r  k        subscript   normal-^  X     B  a  y  e  s     direct-product     superscript    normal-Π   superscript  normal-Π  top      1    normal-Π  μ    superscript  z  n    superscript  l  k    superscript  r  k      subscript  π  z         \begin{aligned}\displaystyle g(l^{k},z,r^{k}):=\hat{X}_{Bayes}\left((\Pi\Pi^{%
 \top})^{-1}\Pi\mu\left(z^{n},l^{k},r^{k}\right)\odot\pi_{z}\right)\,.\end{aligned}     By caching the inverse or pseudo-inverse    Π    -  ⊤      superscript  normal-Π    absent  top     \Pi^{-\top}   , and the values     λ   x  ^    ⊙   π  z      direct-product   subscript  λ   normal-^  x     subscript  π  z     \lambda_{\hat{x}}\odot\pi_{z}   for the relevant pairs     (   x  ^   ,  z  )   ∈   𝒳  ×  𝒵         normal-^  x   z     𝒳  𝒵     (\hat{x},z)\in\mathcal{X}\times\mathcal{Z}   , this step requires    O   (   N   k  ,  n    )       O   subscript  N   k  n      O(N_{k,n})   operations and    O   (   N   k  ,  n    )       O   subscript  N   k  n      O(N_{k,n})   storage.  Step 3: Estimating each symbol by the Bayes response to its context  The third and final step of the DUDE scheme is to scan    z  n     superscript  z  n    z^{n}   again and compute the actual denoised sequence       X  ^   n    (   z  n   )    =   (     X  ^   1    (   z  n   )    ,  …  ,     X  ^   n    (   z  n   )    )          superscript   normal-^  X   n    superscript  z  n        subscript   normal-^  X   1    superscript  z  n    normal-…     subscript   normal-^  X   n    superscript  z  n       \hat{X}^{n}(z^{n})=\left(\hat{X}_{1}(z^{n}),\ldots,\hat{X}_{n}(z^{n})\right)   . The denoised symbol chosen to replace    z  i     subscript  z  i    z_{i}   is the Bayes response to the two-sided context of the symbol, namely             X  ^   i    (   z  n   )    :=   g   (    l  k    (   z  n   ,  i  )    ,    z  i    ,    r  k    (   z  n   ,  i  )    )     .          assign     subscript   normal-^  X   i    superscript  z  n      g      superscript  l  k     superscript  z  n   i     subscript  z  i      superscript  r  k     superscript  z  n   i          \begin{aligned}\displaystyle\hat{X}_{i}(z^{n}):=g\left(l^{k}(z^{n},i)\,,\,z_{i%
 }\,,\,r^{k}(z^{n},i)\right)\,.\end{aligned}     This step requires    O   (  n  )       O  n    O(n)   operations and used the data structure constructed in the previous step.  In summary, the entire DUDE requires    O   (  n  )       O  n    O(n)   operations and    O   (   N   k  ,  n    )       O   subscript  N   k  n      O(N_{k,n})   storage.  Asymptotic optimality properties  The DUDE is designed to be universally optimal, namely optimal (is some sense, under some assumptions) regardless of the original sequence    x  n     superscript  x  n    x^{n}   .  Let      X  ^    D  U  D  E   n   :    𝒵  n   →   𝒳  n       normal-:   subscript   superscript   normal-^  X   n     D  U  D  E     normal-→   superscript  𝒵  n    superscript  𝒳  n      \hat{X}^{n}_{DUDE}:\mathcal{Z}^{n}\to\mathcal{X}^{n}   denote a sequence of DUDE schemes, as described above, where     X  ^    D  U  D  E   n     subscript   superscript   normal-^  X   n     D  U  D  E     \hat{X}^{n}_{DUDE}   uses a context length    k  n     subscript  k  n    k_{n}   that is implicit in the notation. We only require that      lim   n  →  ∞     k  n    =  ∞        subscript    normal-→  n      subscript  k  n       \lim_{n\to\infty}k_{n}=\infty   and that      k  n     |  𝒵  |    2   K  n      =   o   (   n   log  n    )           subscript  k  n    superscript    𝒵     2   subscript  K  n        o    n    n       k_{n}|\mathcal{Z}|^{2K_{n}}=o\left(\frac{n}{\log n}\right)   .  For a stationary source  Denote by    𝒟  n     subscript  𝒟  n    \mathcal{D}_{n}   the set of all   n   n   n   -block denoisers, namely all maps      X  ^   n   :    𝒵  n   →   𝒳  n       normal-:   superscript   normal-^  X   n    normal-→   superscript  𝒵  n    superscript  𝒳  n      \hat{X}^{n}:\mathcal{Z}^{n}\to\mathcal{X}^{n}   .  Let   𝐗   𝐗   \mathbf{X}   be an unknown stationary source and   𝐙   𝐙   \mathbf{Z}   be the distribution of the corresponding noisy sequence. Then            lim   n  →  ∞     𝐄   [    L    X  ^    D  U  D  E   n     (   X  n   ,   Z  n   )    ]     =    lim   n  →  ∞       min     X  ^   n   ∈   𝒟  n     𝐄    [    L    X  ^   n     (   X  n   ,   Z  n   )    ]      ,             subscript    normal-→  n       𝐄   delimited-[]     subscript  L   subscript   superscript   normal-^  X   n     D  U  D  E       superscript  X  n    superscript  Z  n          subscript    normal-→  n         subscript      superscript   normal-^  X   n    subscript  𝒟  n     𝐄    delimited-[]     subscript  L   superscript   normal-^  X   n      superscript  X  n    superscript  Z  n            \begin{aligned}\displaystyle\lim_{n\to\infty}\mathbf{E}\left[L_{\hat{X}^{n}_{%
 DUDE}}\left(X^{n},Z^{n}\right)\right]=\lim_{n\to\infty}\min_{\hat{X}^{n}\in%
 \mathcal{D}_{n}}\mathbf{E}\left[L_{\hat{X}^{n}}\left(X^{n},Z^{n}\right)\right]%
 \,,\end{aligned}     and both limits exist. If, in addition the source   𝐗   𝐗   \mathbf{X}   is ergodic, then            lim sup   n  →  ∞      L    X  ^    D  U  D  E   n     (   X  n   ,   Z  n   )     =     lim   n  →  ∞       min     X  ^   n   ∈   𝒟  n     𝐄    [    L    X  ^   n     (   X  n   ,   Z  n   )    ]     ,   almost surely     .             subscript  limit-supremum   normal-→  n        subscript  L   subscript   superscript   normal-^  X   n     D  U  D  E       superscript  X  n    superscript  Z  n         subscript    normal-→  n         subscript      superscript   normal-^  X   n    subscript  𝒟  n     𝐄    delimited-[]     subscript  L   superscript   normal-^  X   n      superscript  X  n    superscript  Z  n        almost surely       \begin{aligned}\displaystyle\limsup_{n\to\infty}L_{\hat{X}^{n}_{DUDE}}\left(X^%
 {n},Z^{n}\right)=\lim_{n\to\infty}\min_{\hat{X}^{n}\in\mathcal{D}_{n}}\mathbf{%
 E}\left[L_{\hat{X}^{n}}\left(X^{n},Z^{n}\right)\right]\,,\,\text{ almost %
 surely}\,.\end{aligned}     For an individual sequence  Denote by    𝒟   n  ,  k      subscript  𝒟   n  k     \mathcal{D}_{n,k}   the set of all   n   n   n   -block   k   k   k   -th order sliding window denoisers, namely all maps      X  ^   n   :   𝒵  →  𝒳      normal-:   superscript   normal-^  X   n    normal-→  𝒵  𝒳     \hat{X}^{n}:\mathcal{Z}\to\mathcal{X}   of the form       X  ^   i    (   z  n   )    =   f   (   z   i  -  k    ,  …  ,   z   i  +  k    )           subscript   normal-^  X   i    superscript  z  n      f    subscript  z    i  k    normal-…   subscript  z    i  k        \hat{X}_{i}(z^{n})=f\left(z_{i-k},\ldots,z_{i+k}\right)   with    f  :    𝒵    2  k   +  1    →  𝒳      normal-:  f   normal-→   superscript  𝒵      2  k   1    𝒳     f:\mathcal{Z}^{2k+1}\to\mathcal{X}   arbitrary.  Let    𝐱  ∈   𝒳  ∞       𝐱   superscript  𝒳      \mathbf{x}\in\mathcal{X}^{\infty}   be an unknown noiseless sequence stationary source and   𝐙   𝐙   \mathbf{Z}   be the distribution of the corresponding noisy sequence. Then            lim   n  →  ∞     [     L    X  ^    D  U  D  E   n     (   x  n   ,   Z  n   )    -     min     X  ^   n   ∈   𝒟   n  ,  k       L    X  ^   n      (   x  n   ,   Z  n   )     ]    =    0   ,   almost surely     .             subscript    normal-→  n      delimited-[]       subscript  L   subscript   superscript   normal-^  X   n     D  U  D  E       superscript  x  n    superscript  Z  n         subscript      superscript   normal-^  X   n    subscript  𝒟   n  k       subscript  L   superscript   normal-^  X   n       superscript  x  n    superscript  Z  n         0  almost surely       \begin{aligned}\displaystyle\lim_{n\to\infty}\left[L_{\hat{X}^{n}_{DUDE}}\left%
 (x^{n},Z^{n}\right)-\min_{\hat{X}^{n}\in\mathcal{D}_{n,k}}L_{\hat{X}^{n}}\left%
 (x^{n},Z^{n}\right)\right]=0\,,\,\text{ almost surely}\,.\end{aligned}     Non-asymptotic performance  Let     X  ^   k  n     subscript   superscript   normal-^  X   n   k    \hat{X}^{n}_{k}   denote the DUDE on with context length   k   k   k   defined on   n   n   n   -blocks. Then there exist explicit constants     A  ,  C   >  0       A  C   0    A,C>0   and    B  >  1      B  1    B>1   that depend on    (  Π  ,  Λ  )     normal-Π  normal-Λ    \left(\Pi,\Lambda\right)   alone, such that for any    n  ,  k     n  k    n,k   and any     x  n   ∈   𝒳  n        superscript  x  n    superscript  𝒳  n     x^{n}\in\mathcal{X}^{n}   we have             A   n       B  k     ≤   𝐄   [     L    X  ^   k  n     (   x  n   ,   Z  n   )    -     min     X  ^   n   ∈   𝒟   n  ,  k       L    X  ^   n      (   x  n   ,   Z  n   )     ]    ≤    k     C   n        |  𝒵  |   k      ,                 A    n     superscript  B  k      𝐄   delimited-[]       subscript  L   subscript   superscript   normal-^  X   n   k      superscript  x  n    superscript  Z  n         subscript      superscript   normal-^  X   n    subscript  𝒟   n  k       subscript  L   superscript   normal-^  X   n       superscript  x  n    superscript  Z  n                 k     C    n     superscript    𝒵   k         \begin{aligned}\displaystyle\frac{A}{\sqrt{n}}B^{k}\,\leq\mathbf{E}\left[L_{%
 \hat{X}^{n}_{k}}\left(x^{n},Z^{n}\right)-\min_{\hat{X}^{n}\in\mathcal{D}_{n,k}%
 }L_{\hat{X}^{n}}\left(x^{n},Z^{n}\right)\right]\leq\sqrt{k}\frac{C}{\sqrt{n}}|%
 \mathcal{Z}|^{k}\,,\end{aligned}     where    Z  n     superscript  Z  n    Z^{n}   is the noisy sequence corresponding to    x  n     superscript  x  n    x^{n}   (whose randomness is due to the channel alone) 3 .  In fact holds with the same constants    A  ,  B     A  B    A,B   as above for any    n   n   n   -block denoiser      X  ^   n   ∈   𝒟  n        superscript   normal-^  X   n    superscript  𝒟  n     \hat{X}^{n}\in\mathcal{D}^{n}   . 4 The lower bound proof requires that the channel matrix   Π   normal-Π   \Pi   be square and the pair    (  Π  ,  Λ  )     normal-Π  normal-Λ    \left(\Pi,\Lambda\right)   satisfies a certain technical condition.  Background  To motivate the particular definition of the DUDE using the Bayes response to a particular vector, we now find the optimal denoiser in the non-universal case, where the unknown sequence    x  n     superscript  x  n    x^{n}   is a realization of a random vector    X  n     superscript  X  n    X^{n}   , whose distribution is known.  Consider first the case    n  =  1      n  1    n=1   . Since the joint distribution of    (  X  ,  Z  )     X  Z    (X,Z)   is known, given the observed noisy symbol   z   z   z   , the unknown symbol    X  ∈  𝒳      X  𝒳    X\in\mathcal{X}   is distributed according to the known distribution    ℙ   (  X  =  x  |  Z  =  z  )      fragments  P   fragments  normal-(  X   x  normal-|  Z   z  normal-)     \mathbb{P}(X=x|Z=z)   . By ordering the elements of   𝒳   𝒳   \mathcal{X}   , we can describe this conditional distribution on   𝒳   𝒳   \mathcal{X}   using a probability vector    𝐏   X  |  z      subscript  𝐏   fragments  X  normal-|  z     \mathbf{P}_{X|z}   , indexed by   𝒳   𝒳   \mathcal{X}   , whose   x   x   x   -entry is    ℙ   (  X  =  x  |  Z  =  z  )      fragments  P   fragments  normal-(  X   x  normal-|  Z   z  normal-)     \mathbb{P}\left(X=x|Z=z\right)   . Clearly the expected loss for the choice of estimated symbol    x  ^     normal-^  x    \hat{x}   is     λ   x  ^   ⊤    𝐏   X  |  z         superscript   subscript  λ   normal-^  x    top    subscript  𝐏   fragments  X  normal-|  z      \lambda_{\hat{x}}^{\top}\mathbf{P}_{X|z}   .  Define the Bayes Envelope of a probability vector   𝐯   𝐯   \mathbf{v}   , describing a probability distribution on   𝒳   𝒳   \mathcal{X}   , as the minimal expected loss     U   (  𝐯  )    =    min    x  ^   ∈  𝒳      𝐯  ⊤    λ   x  ^            U  𝐯     subscript      normal-^  x   𝒳       superscript  𝐯  top    subscript  λ   normal-^  x        U(\mathbf{v})=\min_{\hat{x}\in\mathcal{X}}\mathbf{v}^{\top}\lambda_{\hat{x}}   , and the Bayes Response to   𝐯   𝐯   \mathbf{v}   as the prediction that achieves this minimum,       X  ^    B  a  y  e  s     (  𝐯  )    =    argmin    x  ^   ∈  𝒳     𝐯  ⊤    λ   x  ^            subscript   normal-^  X     B  a  y  e  s    𝐯      subscript  argmin     normal-^  x   𝒳     superscript  𝐯  top    subscript  λ   normal-^  x       \hat{X}_{Bayes}(\mathbf{v})=\text{argmin}_{\hat{x}\in\mathcal{X}}\mathbf{v}^{%
 \top}\lambda_{\hat{x}}   . Observe that the Bayes response is scale invariant in the sense that       X  ^    B  a  y  e  s     (  𝐯  )    =     X  ^    B  a  y  e  s     (   α  𝐯   )           subscript   normal-^  X     B  a  y  e  s    𝐯      subscript   normal-^  X     B  a  y  e  s      α  𝐯      \hat{X}_{Bayes}(\mathbf{v})=\hat{X}_{Bayes}(\alpha\mathbf{v})   for    α  >  0      α  0    \alpha>0   .  For the case    n  =  1      n  1    n=1   , then, the optimal denoiser is      X  ^    (  z  )    =     X  ^    B  a  y  e  s     (   𝐏   X  |  z    )           normal-^  X   z      subscript   normal-^  X     B  a  y  e  s     subscript  𝐏   fragments  X  normal-|  z       \hat{X}(z)=\hat{X}_{Bayes}\left(\mathbf{P}_{X|z}\right)   . This optimal denoiser can be expressed using the marginal distribution of   Z   Z   Z   alone, as follows. When the channel matrix   Π   normal-Π   \Pi   is invertible, we have     𝐏   X  |  z    ∝     Π    -  ⊤     P  Z    ⊙   π  z       proportional-to   subscript  𝐏   fragments  X  normal-|  z     direct-product     superscript  normal-Π    absent  top     subscript  P  Z     subscript  π  z      \mathbf{P}_{X|z}\propto\Pi^{-\top}P_{Z}\odot\pi_{z}   where    π  z     subscript  π  z    \pi_{z}   is the   z   z   z   -th column of   Π   normal-Π   \Pi   . This implies that the optimal denoiser is given equivalently by      X  ^    (  z  )    =     X  ^    B  a  y  e  s     (     Π    -  ⊤     𝐏  Z    ⊙   π  z    )           normal-^  X   z      subscript   normal-^  X     B  a  y  e  s     direct-product     superscript  normal-Π    absent  top     subscript  𝐏  Z     subscript  π  z       \hat{X}(z)=\hat{X}_{Bayes}\left(\Pi^{-\top}\mathbf{P}_{Z}\odot\pi_{z}\right)   . When     |  𝒳  |   ≤   |  𝒵  |         𝒳     𝒵     |\mathcal{X}|\leq|\mathcal{Z}|   and   Π   normal-Π   \Pi   is not invertible, under the reasonable assumption that it has full row rank, we can replace    Π   -  1      superscript  normal-Π    1     \Pi^{-1}   with its Moore-Penrose pseudo-inverse and obtain         X  ^    (  z  )    =     X  ^    B  a  y  e  s     (      (   Π   Π  ⊤    )    -  1    Π   𝐏  Z    ⊙   π  z    )     .         normal-^  X   z      subscript   normal-^  X     B  a  y  e  s     direct-product     superscript    normal-Π   superscript  normal-Π  top      1    normal-Π   subscript  𝐏  Z     subscript  π  z       \hat{X}(z)=\hat{X}_{Bayes}\left((\Pi\Pi^{\top})^{-1}\Pi\mathbf{P}_{Z}\odot\pi_%
 {z}\right)\,.     Turning now to arbitrary   n   n   n   , the optimal denoiser      X  ^    o  p  t     (   z  n   )        superscript   normal-^  X     o  p  t     superscript  z  n     \hat{X}^{opt}(z^{n})   (with minimal expected loss) is therefore given by the Bayes response to    𝐏    X  i   |   z  n       subscript  𝐏   fragments   subscript  X  i   normal-|   superscript  z  n      \mathbf{P}_{X_{i}|z^{n}}              X  ^   i   o  p  t     (   z  n   )    =     X  ^    B  a  y  e  s     𝐏    X  i   |   z  n      =    argmin    x  ^   ∈  𝒳     λ   x  ^   ⊤     𝐏    X  i   |   z  n        ,                subscript   superscript   normal-^  X     o  p  t    i    superscript  z  n       subscript   normal-^  X     B  a  y  e  s     subscript  𝐏   fragments   subscript  X  i   normal-|   superscript  z  n              subscript  argmin     normal-^  x   𝒳     superscript   subscript  λ   normal-^  x    top    subscript  𝐏   fragments   subscript  X  i   normal-|   superscript  z  n           \begin{aligned}\displaystyle\hat{X}^{opt}_{i}(z^{n})=\hat{X}_{Bayes}\mathbf{P}%
 _{X_{i}|z^{n}}=\text{argmin}_{\hat{x}\in\mathcal{X}}\lambda_{\hat{x}}^{\top}%
 \mathbf{P}_{X_{i}|z^{n}}\,,\end{aligned}     where    𝐏    X  i   |   z  n       subscript  𝐏   fragments   subscript  X  i   normal-|   superscript  z  n      \mathbf{P}_{X_{i}|z^{n}}   is a vector indexed by   𝒳   𝒳   \mathcal{X}   , whose   x   x   x   -entry is    ℙ   (   X  i   =  x  |   Z  n   =   z  n   )      fragments  P   fragments  normal-(   subscript  X  i    x  normal-|   superscript  Z  n     superscript  z  n   normal-)     \mathbb{P}\left(X_{i}=x|Z^{n}=z^{n}\right)   . The conditional probability vector    𝐏    X  i   |   z  n       subscript  𝐏   fragments   subscript  X  i   normal-|   superscript  z  n      \mathbf{P}_{X_{i}|z^{n}}   is hard to compute. A derivation analogous to the case    n  =  1      n  1    n=1   above shows that the optimal denoiser admits an alternative representation, namely       X  ^   i   o  p  t     (   z  n   )    =     X  ^    B  a  y  e  s     (     Π    -  ⊤     𝐏    Z  i   ,   z   n  \  i       ⊙   π   z  i     )           subscript   superscript   normal-^  X     o  p  t    i    superscript  z  n       subscript   normal-^  X     B  a  y  e  s     direct-product     superscript  normal-Π    absent  top     subscript  𝐏    subscript  Z  i    superscript  z   normal-\  n  i        subscript  π   subscript  z  i        \hat{X}^{opt}_{i}(z^{n})=\hat{X}_{Bayes}\left(\Pi^{-\top}\mathbf{P}_{Z_{i},z^{%
 n\backslash i}}\odot\pi_{z_{i}}\right)   , where     z   n  \  i    =   (   z  1   ,  …  ,   z   i  -  1    ,   z   i  +  1    ,  …  ,   z  n   )   ∈   𝒵   n  -  1           superscript  z   normal-\  n  i      subscript  z  1   normal-…   subscript  z    i  1     subscript  z    i  1    normal-…   subscript  z  n          superscript  𝒵    n  1       z^{n\backslash i}=\left(z_{1},\ldots,z_{i-1},z_{i+1},\ldots,z_{n}\right)\in%
 \mathcal{Z}^{n-1}   is a given vector and    𝐏    Z  i   ,   z   n  \  i        subscript  𝐏    subscript  Z  i    superscript  z   normal-\  n  i       \mathbf{P}_{Z_{i},z^{n\backslash i}}   is the probability vector indexed by   𝒵   𝒵   \mathcal{Z}   whose   z   z   z   -entry is    ℙ   (   (   Z  1   ,  …  ,   Z  n   )   =   (   z  1   ,  …  ,   z   i  -  1    ,  z  ,   z   i  +  1    ,  …  ,   z  n   )   )   .     fragments  P   fragments  normal-(   fragments  normal-(   subscript  Z  1   normal-,  normal-…  normal-,   subscript  Z  n   normal-)     fragments  normal-(   subscript  z  1   normal-,  normal-…  normal-,   subscript  z    i  1    normal-,  z  normal-,   subscript  z    i  1    normal-,  normal-…  normal-,   subscript  z  n   normal-)   normal-)   normal-.    \mathbb{P}\left((Z_{1},\ldots,Z_{n})=(z_{1},\ldots,z_{i-1},z,z_{i+1},\ldots,z_%
 {n})\right)\,.   Again,    Π    -  ⊤      superscript  normal-Π    absent  top     \Pi^{-\top}   is replaced by a pseudo-inverse if   Π   normal-Π   \Pi   is not square or not invertible.  When the distribution of   X   X   X   (and therefore, of   Z   Z   Z   ) is not available, the DUDE replaces the unknown vector    𝐏    Z  i   ,   z   n  \  i        subscript  𝐏    subscript  Z  i    superscript  z   normal-\  n  i       \mathbf{P}_{Z_{i},z^{n\backslash i}}   with an empirical estimate obtained along the noisy sequence    z  n     superscript  z  n    z^{n}   itself, namely with    μ   (   Z  i   ,    l  k    (   Z  n   ,  i  )    ,    r  k    (   Z  n   ,  i  )    )       μ    subscript  Z  i      superscript  l  k     superscript  Z  n   i       superscript  r  k     superscript  Z  n   i       \mu\left(Z_{i},l^{k}(Z^{n},i),r^{k}(Z^{n},i)\right)   . This leads to the above definition of the DUDE.  While the convergence arguments behind the optimality properties above are more subtle, we note that the above, combined with the Birkhoff Ergodic Theorem , is enough to prove that for a stationary ergodic source, the DUDE with context-length   k   k   k   is asymptotically optimal all   k   k   k   -th order sliding window denoisers.  Extensions  The basic DUDE as described here assumes a signal with a one-dimensional index set over a finite alphabet, a known memoryless channel and a context length that is fixed in advance. Relaxations of each of these assumptions have been considered in turn. 5 Specifically:   Infinite alphabets   A. Dembo and T. Weissman. Universal denoising for the ﬁnite-input-general-output channel. IEEE Trans. Inform. Theory, 51(4):1507–1517, April 2005. 6 7 8   Channels with memory   C. D. Giurcaneanu and B. Yu. Eﬃcient algorithms for discrete universal denoising for channels with memory. In Proc. of the 2005 IEEE Intl. Symp. on Inform. Theory, (ISIT’05), Adelaide, Australia, Sept. 2005. 9   Unknown channel matrix   G. M. Gemelos, S. Sigurjonsson, T. Weissman. Universal minimax discrete denoising under channel uncertainty. IEEE Trans. Inform. Theory, 52:3476–3497, 2006. 10   Variable context and adaptive choice of context length   E. Ordentlich, M.J. Weinberger, and T. Weissman. Multi-directional context sets with applications to universal denoising and compression. In Proc. of the 2005 IEEE Intl. Symp. on Inform. Theory, (ISIT’05), Adelaide, Australia, Sept. 2005. 11 12 13   Two-dimensional signals   E. Ordentlich, G. Seroussi, S. Verd´u, M.J. Weinberger, and T. Weissman. A universal discrete image denoiser and its application to binary images. In Proc. IEEE International Conference on Image Processing, Barcelona, Catalonia, Spain, September 2003.  Applications  Application to image denoising  A DUDE-based framework for grayscale image denoising 14 achieves state-of-the-art denoising for impulse-type noise channels (e.g., "salt and pepper" or "M-ary symmetric" noise), and good performance on the Gaussian channel (comparable to the Non-local means image denoising scheme on this channel). A different DUDE variant applicable to grayscale images is presented in. 15  Application to channel decoding of uncompressed sources  The DUDE has led to universal algorithms for channel decoding of uncompressed sources . 16  References  "  Category:Noise reduction     T. Weissman, E. Ordentlich, G. Seroussi, S. Verdu ́, and M.J. Weinberger. Universal discrete denoising: Known channel. IEEE Transactions on Information Theory,, 51(1):5–28, 2005. ↩   K. Viswanathan and E. Ordentlich. Lower limits of discrete universal denoising. IEEE Transactions on Information Theory, 55(3):1374–1386, 2009. ↩   ↩  K. Sivaramakrishnan and T. Weissman. Universal denoising of discrete-time continuous amplitude signals. In Proc. of the 2006 IEEE Intl. Symp. on Inform. Theory, (ISIT’06), Seattle, WA, USA, July 2006. ↩  G. Motta, E. Ordentlich, I. Ramírez, G. Seroussi, and M. Weinberger, “The DUDE framework for continuous tone image denoising,” IEEE Transactions on Image Processing, 20, No. 1, January 2011. ↩  K. Sivaramakrishnan and T. Weissman. Universal denoising of continuous amplitude signals with applications to images. In Proc. of IEEE International Conference on Image Processing, Atlanta, GA, USA, October 2006, pp. 2609–2612 ↩  R. Zhang and T. Weissman. Discrete denoising for channels with memory. Communications in Information and Systems (CIS), 5(2):257–288, 2005. ↩  G. M. Gemelos, S. Sigurjonsson and T. Weissman. Algorithms for discrete denoising under channel uncertainty. IEEE Trans. Signal Processing, 54(6):2263–2276, June 2006. ↩  J. Yu and S. Verd´u. Schemes for bidirectional modeling of discrete stationary sources. IEEE Trans. Inform. Theory, 52(11):4789–4807, 2006. ↩  S. Chen, S. N. Diggavi, S. Dusad and S. Muthukrishnan. Eﬃcient string matching algorithms for combinatorial universal denoising. In Proc. of IEEE Data Compression Conference (DCC), Snowbird, Utah, March 2005. ↩  G. Gimel’farb. Adaptive context for a discrete universal denoiser. In Proc. Structural, Syntactic, and Statistical Pattern Recognition, Joint IAPR International Workshops, SSPR 2004 and SPR 2004, Lisbon, Portugal, August 18–20, pp. 477–485 ↩    E. Ordentlich, G. Seroussi, S. Verdú, and K. Viswanathan, "Universal Algorithms for Channel Decoding of Uncompressed Sources," IEEE Trans. Information Theory, vol. 54, no. 5, pp. 2243-2262, May 2008 ↩     