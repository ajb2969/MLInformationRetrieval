   Sample complexity      Sample complexity   In machine learning , sample complexity is the number of examples needed for the estimate of a target function to be within a given error rate. 1 The sample complexity of a machine learning algorithm characterizes its rate of consistency .  Mathematical Setup  Given a set of samples     S  n   =   {   (   x  1   ,   y  1   )   ,  ‚Ä¶  ,   (   x  n   ,   y  n   )   }        subscript  S  n      subscript  x  1    subscript  y  1    normal-‚Ä¶    subscript  x  n    subscript  y  n       S_{n}=\{(x_{1},y_{1}),\ldots,(x_{n},y_{n})\}   drawn i.i.d. according to a distribution   œÅ   œÅ   \rho   from some input space    ùí≥  √ó  ùí¥      ùí≥  ùí¥    \mathcal{X}\times\mathcal{Y}   , a supervised learning algorithm chooses a function    f  :   ùí≥  ‚Üí  ùí¥      normal-:  f   normal-‚Üí  ùí≥  ùí¥     f:\mathcal{X}\to\mathcal{Y}   from some hypothesis class   ‚Ñã   ‚Ñã   \mathcal{H}   . A desirable property of the algorithm is that it chooses functions with small expected prediction error with respect to   œÅ   œÅ   \rho   and some loss function    V  :    ùí¥  √ó  ùí¥   ‚Üí   ‚Ñù  +       normal-:  V   normal-‚Üí    ùí¥  ùí¥    subscript  ‚Ñù       V:\mathcal{Y}\times\mathcal{Y}\to\mathbb{R}_{+}   . Specifically, it is desirable to have a consistent algorithm, or an algorithm that generates functions whose expected risks converge to the best possible expected risk. Formally, let        ‚Ñõ  ‚Ñã  *   =    inf   f  ‚àà  ‚Ñã     ùîº  œÅ    [   V   (   f   (  x  )    ,  y  )    ]     ,       subscript   superscript  ‚Ñõ    ‚Ñã        f  ‚Ñã   infimum    subscript  ùîº  œÅ    delimited-[]    V     f  x   y        \mathcal{R}^{*}_{\mathcal{H}}=\underset{f\in\mathcal{H}}{\inf}\mathbb{E}_{\rho%
 }[V(f(x),y)],     and let    f  n     subscript  f  n    f_{n}   be the functions generated by an algorithm as the number of data points   n   n   n   grows. The algorithm is consistent if       lim   n  ‚Üí  ‚àû     ‚Ñô   S  n     (   ùîº  œÅ    [  V   (  f   (  x  )   ,  y  )   ]   -   ‚Ñõ  ‚Ñã  *   >  œµ  )   ,     fragments    normal-‚Üí  n       subscript  ‚Ñô   subscript  S  n     fragments  normal-(   subscript  ùîº  œÅ    fragments  normal-[  V   fragments  normal-(  f   fragments  normal-(  x  normal-)   normal-,  y  normal-)   normal-]     subscript   superscript  ‚Ñõ    ‚Ñã    œµ  normal-)   normal-,    \underset{n\to\infty}{\lim}\mathbb{P}_{S_{n}}(\mathbb{E}_{\rho}[V(f(x),y)]-%
 \mathcal{R}^{*}_{\mathcal{H}}>\epsilon),     for all    œµ  >  0      œµ  0    \epsilon>0   , where    ‚Ñô   S  n      subscript  ‚Ñô   subscript  S  n     \mathbb{P}_{S_{n}}   denotes the probability measure    œÅ  n     superscript  œÅ  n    \rho^{n}   .  The consistency property is nice, but it says nothing about how fast the expected risks converge. Since in practice one always deals with finite data, it is important to answer the question of how many samples are needed to achieve a risk that is close, in the   œµ   œµ   \epsilon   sense, to the best possible for the function class. The notion of sample complexity answers this question. The sample complexity of a learning algorithm is a function    n   (  œÅ  ,  œµ  ,  Œ¥  )       n   œÅ  œµ  Œ¥     n(\rho,\epsilon,\delta)   such that for all    n  ‚â•   n   (  œÅ  ,  œµ  ,  Œ¥  )        n    n   œÅ  œµ  Œ¥      n\geq n(\rho,\epsilon,\delta)   ,       ‚Ñô   S  n     (   ùîº  œÅ    [  V   (  f   (  x  )   ,  y  )   ]   -   ‚Ñõ  ‚Ñã  *   ‚â§  œµ  )   ‚â•  1  -  Œ¥  .     fragments   subscript  ‚Ñô   subscript  S  n     fragments  normal-(   subscript  ùîº  œÅ    fragments  normal-[  V   fragments  normal-(  f   fragments  normal-(  x  normal-)   normal-,  y  normal-)   normal-]     subscript   superscript  ‚Ñõ    ‚Ñã    œµ  normal-)    1   Œ¥  normal-.    \mathbb{P}_{S_{n}}(\mathbb{E}_{\rho}[V(f(x),y)]-\mathcal{R}^{*}_{\mathcal{H}}%
 \leq\epsilon)\geq 1-\delta.     In words, the sample complexity    n   (  œÅ  ,  œµ  ,  Œ¥  )       n   œÅ  œµ  Œ¥     n(\rho,\epsilon,\delta)   defines the rate of consistency of the algorithm. Given a desired accuracy   œµ   œµ   \epsilon   and confidence   Œ¥   Œ¥   \delta   , one needs at most    n   (  œÅ  ,  œµ  ,  Œ¥  )       n   œÅ  œµ  Œ¥     n(\rho,\epsilon,\delta)   samples to guarantee that the expected risk of the output function is within   œµ   œµ   \epsilon   of the best possible expected risk with probability at least    1  -  Œ¥      1  Œ¥    1-\delta   . 2  No Free Lunch Theorem (Machine Learning)  Optimistically one could hope for a stronger notion of sample complexity that is independent of the distribution   œÅ   œÅ   \rho   on the input and output spaces. However, it has been shown that without restrictions on the hypothesis class   ‚Ñã   ‚Ñã   \mathcal{H}   , there always exists "bad" distributions for which the sample complexity is arbitrarily large. 3 Thus in order to make statements about the rate of convergence of the quantity        sup  ùúå     ‚Ñô   S  n     (   ùîº  œÅ    [  V   (  f   (  x  )   ,  y  )   ]   -   ‚Ñõ  ‚Ñã  *   >  œµ  )   ,     fragments   œÅ  supremum    subscript  ‚Ñô   subscript  S  n     fragments  normal-(   subscript  ùîº  œÅ    fragments  normal-[  V   fragments  normal-(  f   fragments  normal-(  x  normal-)   normal-,  y  normal-)   normal-]     subscript   superscript  ‚Ñõ    ‚Ñã    œµ  normal-)   normal-,    \underset{\rho}{\sup}\ \mathbb{P}_{S_{n}}(\mathbb{E}_{\rho}[V(f(x),y)]-%
 \mathcal{R}^{*}_{\mathcal{H}}>\epsilon),     one must either   Constrain the set of probability distributions   œÅ   œÅ   \rho   , e.g. via a parametric approach, or  Constrain the set   ‚Ñã   ‚Ñã   \mathcal{H}   to be small, as in distribution free approaches.   The latter approach leads to concepts such as VC dimension and Rademacher complexity which control the complexity of the space   ‚Ñã   ‚Ñã   \mathcal{H}   . A smaller hypothesis space introduces more bias into the inference process, meaning that    ‚Ñõ  ‚Ñã  *     subscript   superscript  ‚Ñõ    ‚Ñã    \mathcal{R}^{*}_{\mathcal{H}}   may be larger than the best possible expected risk in a larger space. However, by restricting the complexity of the hypothesis space it becomes possible for an algorithm to produce functions converging in expected risk to    ‚Ñõ  ‚Ñã  *     subscript   superscript  ‚Ñõ    ‚Ñã    \mathcal{R}^{*}_{\mathcal{H}}   . This trade-off leads to the concept of regularization . 4  Other Settings  In addition to the supervised learning setting, sample complexity is relevant to semi-supervised learning problems including active learning , 5 where the algorithm can ask for labels to specifically chosen inputs in order to reduce the cost of obtaining many labels. The concept of sample complexity also shows up in reinforcement learning , 6  online learning , and unsupervised algorithms, e.g. for dictionary learning . 7  References  "  Category:Machine learning     ‚Ü©  ‚Ü©  ‚Ü©    ‚Ü©  ‚Ü©     