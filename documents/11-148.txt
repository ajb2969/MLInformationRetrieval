   Lexical density      Lexical density   In computational linguistics , lexical density constitutes the estimated measure of content per functional (grammatical) and lexical units ( lexemes ) in total. It is used in discourse analysis as a descriptive parameter which varies with register and genre. Spoken texts tend to have a lower lexical density than written ones, for example.  Lexical density may be determined thus:       L  d   =    (    N  lex   /  N   )   Ã—  100        subscript  L  d        subscript  N  lex   N   100     L_{d}=(N_{\mathrm{lex}}/N)\times 100     Where:      L  d     subscript  L  d    L_{d}   = the analysed text's lexical density      N  lex     subscript  N  lex    N_{\mathrm{lex}}   = the number of lexical word tokens (nouns, adjectives, verbs, adverbs) in the analysed text     N   N   N   = the number of all tokens (total number of words) in the analysed text  (The variable symbols applied herein are by no means conventional; they were arbitrarily chosen for the nonce to illustrate the example in question.)  See also   Content analysis  Pierce's type-token distinction   Further reading   Ure, J (1971). Lexical density and register differentiation. In G. Perren and J.L.M. Trim (eds), Applications of Linguistics , London: Cambridge University Press. 443-452.   External links   Lexical density 'Textalyser'   "  Category:Computational linguistics  Category:Applied linguistics   