<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1856">C4.5 algorithm</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>C4.5 algorithm</h1>
<hr/>

<p><strong>C4.5</strong> is an algorithm used to generate a <a href="decision_tree_learning" title="wikilink">decision tree</a> developed by <a href="Ross_Quinlan" title="wikilink">Ross Quinlan</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> C4.5 is an extension of Quinlan's earlier <a href="ID3_algorithm" title="wikilink">ID3 algorithm</a>. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a <a href="Statistical_classification" title="wikilink">statistical classifier</a>.</p>

<p>It became quite popular after ranking #1 in the <em>Top 10 Algorithms in Data Mining</em> pre-eminent paper published by <a href="Springer_Science+Business_Media" title="wikilink">Springer</a> <a href="Lecture_Notes_in_Computer_Science" title="wikilink">LNCS</a> in <a class="uri" href="2008" title="wikilink">2008</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h2 id="algorithm">Algorithm</h2>

<p>C4.5 builds decision trees from a set of training data in the same way as <a href="ID3_algorithm" title="wikilink">ID3</a>, using the concept of <a href="Entropy_(information_theory)" title="wikilink">information entropy</a>. The training data is a set 

<math display="inline" id="C4.5_algorithm:0">
 <semantics>
  <mrow>
   <mi>S</mi>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>s</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>s</mi>
     <mn>2</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>S</ci>
    <list>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>s</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>s</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>normal-…</ci>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S={s_{1},s_{2},...}
  </annotation>
 </semantics>
</math>

 of already classified samples. Each sample 

<math display="inline" id="C4.5_algorithm:1">
 <semantics>
  <msub>
   <mi>s</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>s</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s_{i}
  </annotation>
 </semantics>
</math>

 consists of a p-dimensional vector 

<math display="inline" id="C4.5_algorithm:2">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <msub>
    <mi>x</mi>
    <mrow>
     <mn>1</mn>
     <mo>,</mo>
     <mi>i</mi>
    </mrow>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>x</mi>
    <mrow>
     <mn>2</mn>
     <mo>,</mo>
     <mi>i</mi>
    </mrow>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>x</mi>
    <mrow>
     <mi>p</mi>
     <mo>,</mo>
     <mi>i</mi>
    </mrow>
   </msub>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <vector>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <list>
      <cn type="integer">1</cn>
      <ci>i</ci>
     </list>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <list>
      <cn type="integer">2</cn>
      <ci>i</ci>
     </list>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <list>
      <ci>p</ci>
      <ci>i</ci>
     </list>
    </apply>
   </vector>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x_{1,i},x_{2,i},...,x_{p,i})
  </annotation>
 </semantics>
</math>

, where the 

<math display="inline" id="C4.5_algorithm:3">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>j</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{j}
  </annotation>
 </semantics>
</math>

 represent attributes or features of the sample, as well as the class in which 

<math display="inline" id="C4.5_algorithm:4">
 <semantics>
  <msub>
   <mi>s</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>s</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s_{i}
  </annotation>
 </semantics>
</math>

 falls.</p>

<p>At each node of the tree, C4.5 chooses the attribute of the data that most effectively splits its set of samples into subsets enriched in one class or the other. The splitting criterion is the normalized <a href="information_gain" title="wikilink">information gain</a> (difference in <a href="Entropy_(information_theory)" title="wikilink">entropy</a>). The attribute with the highest normalized information gain is chosen to make the decision. The C4.5 algorithm then recurs on the smaller sublists.</p>

<p>This algorithm has a few base cases.</p>
<ul>
<li>All the samples in the list belong to the same class. When this happens, it simply creates a leaf node for the decision tree saying to choose that class.</li>
<li>None of the features provide any information gain. In this case, C4.5 creates a decision node higher up the tree using the expected value of the class.</li>
<li>Instance of previously-unseen class encountered. Again, C4.5 creates a decision node higher up the tree using the expected value.</li>
</ul>
<h3 id="pseudocode">Pseudocode</h3>

<p>In <a class="uri" href="pseudocode" title="wikilink">pseudocode</a>, the general algorithm for building decision trees is:<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<ol>
<li>Check for base cases</li>
<li>For each attribute <em>a</em>
<ol>
<li>Find the normalized information gain ratio from splitting on <em>a</em></li>
</ol></li>
<li>Let <em>a_best</em> be the attribute with the highest normalized information gain</li>
<li>Create a decision <em>node</em> that splits on <em>a_best</em></li>
<li>Recur on the sublists obtained by splitting on <em>a_best</em>, and add those nodes as children of <em>node</em></li>
</ol>
<h2 id="implementations">Implementations</h2>

<p><strong>J48</strong> is an <a href="open_source" title="wikilink">open source</a> <a href="Java_(programming_language)" title="wikilink">Java</a> implementation of the C4.5 algorithm in the <a href="Weka_(machine_learning)" title="wikilink">weka</a> <a href="data_mining" title="wikilink">data mining</a> tool.</p>
<h2 id="improvements-from-id.3-algorithm">Improvements from ID.3 algorithm</h2>

<p>C4.5 made a number of improvements to ID3. Some of these are:</p>
<ul>
<li>Handling both continuous and discrete attributes - In order to handle continuous attributes, C4.5 creates a threshold and then splits the list into those whose attribute value is above the threshold and those that are less than or equal to it.<ref>J. R. Quinlan. Improved use of continuous attributes in c4.5. Journal of Artificial Intelligence Research, 4:77-90, 1996.</ref></li>
</ul>

<p></p>
<ul>
<li>Handling training data with missing attribute values - C4.5 allows attribute values to be marked as ? for missing. Missing attribute values are simply not used in gain and entropy calculations.</li>
<li>Handling attributes with differing costs.</li>
<li>Pruning trees after creation - C4.5 goes back through the tree once it's been created and attempts to remove branches that do not help by replacing them with leaf nodes.</li>
</ul>
<h2 id="improvements-in-c5.0see5-algorithm">Improvements in C5.0/See5 algorithm</h2>

<p>Quinlan went on to create C5.0 and See5 (C5.0 for Unix/Linux, See5 for Windows) which he markets commercially. C5.0 offers a number of improvements on C4.5. Some of these are:<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<ul>
<li>Speed - C5.0 is significantly faster than C4.5 (several orders of magnitude)</li>
<li>Memory usage - C5.0 is more memory efficient than C4.5</li>
<li>Smaller decision trees - C5.0 gets similar results to C4.5 with considerably smaller decision trees.</li>
<li>Support for <a href="Boosting_(meta-algorithm)" title="wikilink">boosting</a> - Boosting improves the trees and gives them more accuracy.</li>
<li>Weighting - C5.0 allows you to weight different cases and misclassification types.</li>
<li>Winnowing - a C5.0 option automatically <a href="Winnow_(algorithm)" title="wikilink">winnows</a> the attributes to remove those that may be unhelpful.</li>
</ul>

<p>Source for a single-threaded Linux version of C5.0 is available under the GPL.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="ID3_algorithm" title="wikilink">ID3 algorithm</a></li>
</ul>
<h2 id="references">References</h2>
<references>
</references>
<h2 id="external-links">External links</h2>
<ul>
<li>Original implementation on Ross Quinlan's homepage: <a href="http://www.rulequest.com/Personal/"></a><a class="uri" href="http://www.rulequest.com/Personal/">http://www.rulequest.com/Personal/</a></li>
<li><a href="http://www.rulequest.com/see5-info.html">See5 and C5.0</a></li>
</ul>

<p>"</p>

<p><a href="Category:Classification_algorithms" title="wikilink">Category:Classification algorithms</a> <a href="Category:Decision_trees" title="wikilink">Category:Decision trees</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Quinlan, J. R. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993.<a href="#fnref1">↩</a></li>
<li id="fn2"><a href="http://www.cs.umd.edu/~samir/498/10Algorithms-08.pdf">Umd.edu - Top 10 Algorithms in Data Mining</a><a href="#fnref2">↩</a></li>
<li id="fn3">S.B. Kotsiantis, Supervised Machine Learning: A Review of Classification Techniques, Informatica 31(2007) 249-268, 2007<a href="#fnref3">↩</a></li>
<li id="fn4"><a href="http://www.rulequest.com/see5-comparison.html">Is See5/C5.0 Better Than C4.5?</a><a href="#fnref4">↩</a></li>
<li id="fn5">M. Kuhn and K. Johnson, Applied Predictive Modeling, Springer 2013<a href="#fnref5">↩</a></li>
</ol>
</section>
</body>
</html>
