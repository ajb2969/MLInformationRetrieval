   Probability axioms      Probability axioms   In Kolmogorov's probability theory , the probability  P of some event  E , denoted    P   (  E  )       P  E    P(E)   , is usually defined such that P satisfies the Kolmogorov axioms , named after the famous Russian  mathematician  Andrey Kolmogorov , which are described below.  These assumptions can be summarised as follows: Let (Ω, F , P ) be a measure space with P (Ω)=1. Then (Ω, F , P ) is a probability space , with sample space Ω, event space F and probability measure P .  An alternative approach to formalising probability, favoured by some Bayesians , is given by Cox's theorem .  Axioms  First axiom  The probability of an event is a non-negative real number:        P   (  E  )    ∈  ℝ   ,     P   (  E  )    ≥  0     ∀  E   ∈  F       formulae-sequence      P  E   ℝ    formulae-sequence      P  E   0      for-all  E   F      P(E)\in\mathbb{R},P(E)\geq 0\qquad\forall E\in F     where   F   F   F   is the event space. In particular,    P   (  E  )       P  E    P(E)   is always finite, in contrast with more general measure theory . Theories which assign negative probability relax the first axiom.  Second axiom  This is the assumption of unit measure : that the probability that some elementary event in the entire sample space will occur is 1. More specifically, there are no elementary events outside the sample space.       P   (  Ω  )    =  1.        P  normal-Ω   1.    P(\Omega)=1.     This is often overlooked in some mistaken probability calculations; if you cannot precisely define the whole sample space, then the probability of any subset cannot be defined either.  Third axiom  This is the assumption of σ-additivity :   Any countable sequence of disjoint (synonymous with mutually exclusive ) events     E  1   ,   E  2   ,  …      subscript  E  1    subscript  E  2   normal-…    E_{1},E_{2},...   satisfies       P   (    ⋃   i  =  1   ∞    E  i    )    =    ∑   i  =  1   ∞    P   (   E  i   )      .        P    superscript   subscript     i  1       subscript  E  i       superscript   subscript     i  1        P   subscript  E  i       P\left(\bigcup_{i=1}^{\infty}E_{i}\right)=\sum_{i=1}^{\infty}P(E_{i}).       Some authors consider merely finitely additive probability spaces, in which case one just needs an algebra of sets , rather than a σ-algebra . Quasiprobability distributions in general relax the third axiom.  Consequences  From the Kolmogorov axioms, one can deduce other useful rules for calculating probabilities.  The probability of the empty set       P   (  ∅  )    =  0.        P    0.    P(\varnothing)=0.     Monotonicity          if   A   ⊆  B     then   P   (  A  )     ≤   P   (  B  )      .     formulae-sequence     if  A   B      then    P  A      P  B      \quad\text{if}\quad A\subseteq B\quad\text{then}\quad P(A)\leq P(B).     The numeric bound  It immediately follows from the monotonicity property that        0  ≤   P   (  E  )    ≤  1     ∀  E   ∈  F    .     formulae-sequence      0    P  E        1       for-all  E   F     0\leq P(E)\leq 1\qquad\forall E\in F.     Proofs  The proofs of these properties are both interesting and insightful. They illustrate the power of the third axiom, and its interaction with the remaining two axioms. When studying axiomatic  probability theory , many deep consequences follow from merely these three axioms. In order to verify the monotonicity property, we set     E  1   =  A       subscript  E  1   A    E_{1}=A   and     E  2   =   B  \  A        subscript  E  2    normal-\  B  A     E_{2}=B\backslash A   , where     A   ⊆   B  and   E  i    =  ∅        A    B  and   subscript  E  i             \quad A\subseteq B\text{ and }E_{i}=\varnothing   for    i  ≥  3      i  3    i\geq 3   . It is easy to see that the sets    E  i     subscript  E  i    E_{i}   are pairwise disjoint and      E  1   ∪   E  2   ∪  …   =  B         subscript  E  1    subscript  E  2   normal-…   B    E_{1}\cup E_{2}\cup\ldots=B   . Hence, we obtain from the third axiom that         P   (  A  )    +   P   (   B  \  A   )    +    ∑   i  =  3   ∞    P   (  ∅  )      =   P   (  B  )     .          P  A     P   normal-\  B  A      superscript   subscript     i  3        P        P  B     P(A)+P(B\backslash A)+\sum_{i=3}^{\infty}P(\varnothing)=P(B).   Since the left-hand side of this equation is a series of non-negative numbers, and that it converges to    P   (  B  )       P  B    P(B)   which is finite, we obtain both     P   (  A  )    ≤   P   (  B  )          P  A     P  B     P(A)\leq P(B)   and     P   (  ∅  )    =  0        P    0    P(\varnothing)=0   . The second part of the statement is seen by contradiction: if     P   (  ∅  )    =  a        P    a    P(\varnothing)=a   then the left hand side is not less than        ∑   i  =  3   ∞    P   (   E  i   )     =    ∑   i  =  3   ∞    P   (  ∅  )     =    ∑   i  =  3   ∞   a   =   {     0       if  a   =  0   ,       ∞      if  a   >  0.               superscript   subscript     i  3        P   subscript  E  i       superscript   subscript     i  3        P            superscript   subscript     i  3      a         cases  0      if  a   0        if  a   0.       \sum_{i=3}^{\infty}P(E_{i})=\sum_{i=3}^{\infty}P(\varnothing)=\sum_{i=3}^{%
 \infty}a=\begin{cases}0&\text{if }a=0,\\
 \infty&\text{if }a>0.\end{cases}   If    a  >  0      a  0    a>0   then we obtain a contradiction, because the sum does not exceed    P   (  B  )       P  B    P(B)   which is finite. Thus,    a  =  0      a  0    a=0   . We have shown as a byproduct of the proof of monotonicity that     P   (  ∅  )    =  0        P    0    P(\varnothing)=0   .  Further consequences  Another important property is:        P   (   A  ∪  B   )    =     P   (  A  )    +   P   (  B  )     -   P   (   A  ∩  B   )      .        P    A  B          P  A     P  B      P    A  B       P(A\cup B)=P(A)+P(B)-P(A\cap B).     This is called the addition law of probability, or the sum rule. That is, the probability that A  or  B will happen is the sum of the probabilities that A will happen and that B will happen, minus the probability that both A  and  B will happen. The proof of this is as follows:       P   (   A  ∪  B   )    =    P   (  A  )    +   P   (   B  ∖   (   A  ∩  B   )    )           P    A  B        P  A     P    B    A  B        P(A\cup B)=P(A)+P(B\setminus(A\cap B))\,\,   (by Axiom 3) now,     P   (  B  )    =    P   (   B  ∖   (   A  ∩  B   )    )    +   P   (   A  ∩  B   )           P  B       P    B    A  B       P    A  B       P(B)=P(B\setminus(A\cap B))+P(A\cap B)   .  Eliminating    P   (   B  ∖   (   A  ∩  B   )    )       P    B    A  B      P(B\setminus(A\cap B))   from both equations gives us the desired result.  This can be extended to the inclusion-exclusion principle .       P   (   E  c   )    =   P   (   Ω  ∖  E   )    =   1  -   P   (  E  )             P   superscript  E  c      P    normal-Ω  E           1    P  E       P\left(E^{c}\right)=P(\Omega\setminus E)=1-P(E)     That is, the probability that any event will not happen (or the event's complement ) is 1 minus the probability that it will.  Simple example: coin toss  Consider a single coin-toss, and assume that the coin will either land heads (H) or tails (T) (but not both). No assumption is made as to whether the coin is fair.  We may define:      Ω  =   {  H  ,  T  }       normal-Ω   H  T     \Omega=\{H,T\}         F  =   {  ∅  ,   {  H  }   ,   {  T  }   ,   {  H  ,  T  }   }       F     H    T    H  T      F=\{\varnothing,\{H\},\{T\},\{H,T\}\}     Kolmogorov's axioms imply that:       P   (  ∅  )    =  0        P    0    P(\varnothing)=0   The probability of neither heads nor tails, is 0.       P   (   {  H  ,  T  }   )    =  1        P   H  T    1    P(\{H,T\})=1   The probability of either heads or tails, is 1.        P   (   {  H  }   )    +   P   (   {  T  }   )     =  1          P   H      P   T     1    P(\{H\})+P(\{T\})=1   The sum of the probability of heads and the probability of tails, is 1.  See also   Law of total probability  Measure (mathematics)  Borel Algebra  σ-Algebra  Probability theory  Set theory  Conditional probability  Quasiprobability  Fully probabilistic design   Further reading   Von Plato, Jan, 2005, "Grundbegriffe der Wahrscheinlichkeitsrechnung" in Grattan-Guinness, I. , ed., Landmark Writings in Western Mathematics . Elsevier: 960-69. (in English)    External links   Kolmogorov`s probability calculus , Stanford Encyclopedia of Philosophy.  Formal definition of probability in the Mizar system , and the [ http://mmlquery.mizar.org/cgi-bin/mmlquery/emacs_search?input =(symbol+Probability+%7C+notation+%7C+constructor+%7C+occur+%7C+th)+ordered+by+number+of+ref list of theorems] formally proved about it.   "  Category:Probability theory  Category:Mathematical axioms   