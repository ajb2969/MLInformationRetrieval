   Bühlmann model      Bühlmann model   In credibility theory , a branch of study in actuarial science , the Bühlmann model is a random effects model (or "variance components model" or hierarchical linear model ) used in to determine the appropriate premium for a group of insurance contracts. The model is named after Hans Bühlmann who first published a description in 1967. 1  Model description  Consider i risks which generate random losses for which historical data of m recent claims are available (indexed by j ). A premium for the i th risk is to be determined based on the expected value of claims. A linear estimator which minimizes the mean square error is sought. Write   X ij for the j -th claim on the i -th risk (we assume that all claims for i -th risk are independent and identically distributed )        X  ¯   i   =     1  m      ∑   j  =  1   m    X   i  j           subscript   normal-¯  X   i       1  m     superscript   subscript     j  1    m    subscript  X    i  j        \scriptstyle\bar{X}_{i}=\frac{1}{m}\sum_{j=1}^{m}X_{ij}   for the average value.      Θ  i     subscript  normal-Θ  i    \Theta_{i}   - the parameter for the distribution of the i-th risk       m   (  ϑ  )    =   E   [   X   i  j    |    Θ  i   =  ϑ   ]          m  ϑ    normal-E   subscript  X    i  j       subscript  normal-Θ  i   ϑ      m(\vartheta)=\operatorname{E}\left[X_{ij}|\Theta_{i}=\vartheta\right]         Π  =   E   (   m   (  ϑ  )    |   X   i  1    ,   X   i  2    ,   …   X   i  m     )        normal-Π   normal-E    m  ϑ    subscript  X    i  1     subscript  X    i  2      normal-…   subscript  X    i  m        \Pi=\operatorname{E}(m(\vartheta)|X_{i1},X_{i2},...X_{im})   - premium for the i-th risk      μ  =  (  m   (  ϑ  )   )     fragments  μ   normal-(  m   fragments  normal-(  ϑ  normal-)   normal-)    \mu=\operatorname{(}m(\vartheta))           s  2    (  ϑ  )    =   Var   [   X   i  j    |    Θ  i   =  ϑ   ]           superscript  s  2   ϑ    Var   subscript  X    i  j       subscript  normal-Θ  i   ϑ      s^{2}(\vartheta)=\operatorname{Var}\left[X_{ij}|\Theta_{i}=\vartheta\right]          σ  2   =   E   [    s  2    (  ϑ  )    ]         superscript  σ  2    normal-E     superscript  s  2   ϑ      \sigma^{2}=\operatorname{E}\left[s^{2}(\vartheta)\right]          v  2   =   Var   [   m   (  ϑ  )    ]         superscript  v  2    Var    m  ϑ      v^{2}=\operatorname{Var}\left[m(\vartheta)\right]      Note    m   (  ϑ  )       m  ϑ    m(\vartheta)   and     s  2    (  ϑ  )        superscript  s  2   ϑ    s^{2}(\vartheta)   are functions of random parameter   ϑ   ϑ   \vartheta     The Bühlmann model is the solution for the problem:         arg   min     a   i  0    ,   a   i  1    ,  …  ,   a   i  m       E   [    (     a   i  0    +    ∑   j  =  1   m     a   i  j     X   i  j       -  Π   )   2   ]           subscript  a    i  0     subscript  a    i  1    normal-…   subscript  a    i  m       arg  min     normal-E   superscript       subscript  a    i  0      superscript   subscript     j  1    m      subscript  a    i  j     subscript  X    i  j       normal-Π   2      \underset{a_{i0},a_{i1},...,a_{im}}{\operatorname{arg\,min}}\operatorname{E}%
 \left[\left(a_{i0}+\sum_{j=1}^{m}a_{ij}X_{ij}-\Pi\right)^{2}\right]     where     a   i  0    +    ∑   j  =  1   m     a   i  j     X   i  j           subscript  a    i  0      superscript   subscript     j  1    m      subscript  a    i  j     subscript  X    i  j        a_{i0}+\sum_{j=1}^{m}a_{ij}X_{ij}   is the estimator of premium   Π   normal-Π   \Pi   and arg min represents the parameter values which minimize the expression.  Model solution  The solution for the problem is:       Z    X  ¯   i    +    (   1  -  Z   )   μ         Z   subscript   normal-¯  X   i        1  Z   μ     Z\bar{X}_{i}+(1-Z)\mu     where:      Z  =   1   1  +    σ  2     v  2   m          Z    1    1     superscript  σ  2      superscript  v  2   m        Z=\frac{1}{1+\frac{\sigma^{2}}{v^{2}m}}     We can give this result the interpretation, that Z part of the premium is based on the information that we have about the specific risk, and (1-Z) part is based on the information that we have about the whole population.  Proof  The following proof is slightly different from the one in the original paper. It is also more general, because it considers all linear estimators, while original proof considers only estimators based on average claim. Proof can be found on this site. 2  Lemma: The problem can be stated alternatively as:      f  =   E   [    (     a   i  0    +    ∑   j  =  1   m     a   i  j     X   i  j       -   m   (  ϑ  )     )   2   ]    →   m  i  n         f   normal-E   superscript       subscript  a    i  0      superscript   subscript     j  1    m      subscript  a    i  j     subscript  X    i  j         m  ϑ    2      normal-→      m  i  n      f=\operatorname{E}\left[\left(a_{i0}+\sum_{j=1}^{m}a_{ij}X_{ij}-m(\vartheta)%
 \right)^{2}\right]\rightarrow min     Proof:      E   [    (     a   i  0    +    ∑   j  =  1   m     a   i  j     X   i  j       -   m   (  ϑ  )     )   2   ]      normal-E   superscript       subscript  a    i  0      superscript   subscript     j  1    m      subscript  a    i  j     subscript  X    i  j         m  ϑ    2     \operatorname{E}\left[\left(a_{i0}+\sum_{j=1}^{m}a_{ij}X_{ij}-m(\vartheta)%
 \right)^{2}\right]        =    E   [    (     a   i  0    +    ∑   j  =  1   m     a   i  j     X   i  j       -  Π   )   2   ]    +   E   [    (    m   (  ϑ  )    -  Π   )   2   ]    +   2  E   [    (     a   i  0    +    ∑   j  =  1   m     a   i  j     X   i  j       -  Π   )    (    m   (  ϑ  )    -  Π   )    ]         absent     normal-E   superscript       subscript  a    i  0      superscript   subscript     j  1    m      subscript  a    i  j     subscript  X    i  j       normal-Π   2     normal-E   superscript      m  ϑ   normal-Π   2      2  E   delimited-[]         subscript  a    i  0      superscript   subscript     j  1    m      subscript  a    i  j     subscript  X    i  j       normal-Π       m  ϑ   normal-Π         =\operatorname{E}\left[\left(a_{i0}+\sum_{j=1}^{m}a_{ij}X_{ij}-\Pi\right)^{2}%
 \right]+\operatorname{E}\left[\left(m(\vartheta)-\Pi\right)^{2}\right]+2E\left%
 [\left(a_{i0}+\sum_{j=1}^{m}a_{ij}X_{ij}-\Pi\right)\left(m(\vartheta)-\Pi%
 \right)\right]        =    E   [    (     a   i  0    +    ∑   j  =  1   m     a   i  j     X   i  j       -  Π   )   2   ]    +   E   [    (    m   (  ϑ  )    -  Π   )   2   ]         absent     normal-E   superscript       subscript  a    i  0      superscript   subscript     j  1    m      subscript  a    i  j     subscript  X    i  j       normal-Π   2     normal-E   superscript      m  ϑ   normal-Π   2       =\operatorname{E}\left[\left(a_{i0}+\sum_{j=1}^{m}a_{ij}X_{ij}-\Pi\right)^{2}%
 \right]+\operatorname{E}\left[\left(m(\vartheta)-\Pi\right)^{2}\right]     The last equation follows from the fact that    E   [    (     a   i  0    +    ∑   j  =  1   m     a   i  j     X   i  j       -  Π   )    (    m   (  ϑ  )    -  Π   )    ]      normal-E         subscript  a    i  0      superscript   subscript     j  1    m      subscript  a    i  j     subscript  X    i  j       normal-Π       m  ϑ   normal-Π      \operatorname{E}\left[\left(a_{i0}+\sum_{j=1}^{m}a_{ij}X_{ij}-\Pi\right)\left(%
 m(\vartheta)-\Pi\right)\right]        E  Θ    {   E  X    [   (   a   i  0    +   ∑   j  =  1   m    a   i  j     X   i  j    -  Π  )    (  m   (  ϑ  )   -  Π  )   |   X   i  1    ,   X   i  2    ,   X   i  m    ]   }      fragments   subscript  normal-E  normal-Θ    fragments  normal-{   subscript  E  X    fragments  normal-[   fragments  normal-(   subscript  a    i  0      superscript   subscript     j  1    m    subscript  a    i  j     subscript  X    i  j     Π  normal-)    fragments  normal-(  m   fragments  normal-(  ϑ  normal-)    Π  normal-)   normal-|   subscript  X    i  1    normal-,   subscript  X    i  2    normal-,   subscript  X    i  m    normal-]   normal-}     \operatorname{E}_{\Theta}\left\{E_{X}\left[\left(a_{i0}+\sum_{j=1}^{m}a_{ij}X_%
 {ij}-\Pi\right)\left(m(\vartheta)-\Pi\right)|X_{i1},X_{i2},X_{im}\right]\right\}        =    (     a   i  0    +    ∑   j  =  1   m     a   i  j     X   i  j       -  Π   )     E  Θ    {    E  X    [   (    m   (  ϑ  )    -  Π   )   |   X   i  1    ,   X   i  2    ,   X   i  m    ]    }     =  0        absent         subscript  a    i  0      superscript   subscript     j  1    m      subscript  a    i  j     subscript  X    i  j       normal-Π     subscript  normal-E  normal-Θ     subscript  normal-E  X       m  ϑ   normal-Π    subscript  X    i  1     subscript  X    i  2     subscript  X    i  m            0     =\left(a_{i0}+\sum_{j=1}^{m}a_{ij}X_{ij}-\Pi\right)\operatorname{E}_{\Theta}%
 \left\{\operatorname{E}_{X}\left[\left(m(\vartheta)-\Pi\right)|X_{i1},X_{i2},X%
 _{im}\right]\right\}=0     We are using here the law of total expectation and the fact, that    Π  =   E   (   m   (  ϑ  )    |   X   i  1    ,   X   i  2    ,   …   X   i  m     )        normal-Π   normal-E    m  ϑ    subscript  X    i  1     subscript  X    i  2      normal-…   subscript  X    i  m        \Pi=\operatorname{E}(m(\vartheta)|X_{i1},X_{i2},...X_{im})     In our previous equation, we decompose minimized function in the sum of two expressions. The second expression does not depend on parameters used in minimization. Therefore, minimizing the function is the same as minimizing the first part of the sum.  Let us find critical points of the function        1  2     ∂  f    ∂   a  01      =           1  2       f      subscript  a  01      absent    \frac{1}{2}\frac{\partial f}{\partial a_{01}}=        E   [     a   i  0    +    ∑   j  =  1   m     a   i  j     X   i  j       -   m   (  ϑ  )     ]    =     a   i  0    +    ∑   j  =  1   m     a   i  j     E   (   X   i  j    )       -   E   (   m   (  ϑ  )    )     =    a   i  0    -    (     ∑   j  =  1   m    a   i  j     -  1   )   μ           normal-E       subscript  a    i  0      superscript   subscript     j  1    m      subscript  a    i  j     subscript  X    i  j         m  ϑ          subscript  a    i  0      superscript   subscript     j  1    m      subscript  a    i  j     normal-E   subscript  X    i  j         normal-E    m  ϑ             subscript  a    i  0          superscript   subscript     j  1    m    subscript  a    i  j     1   μ       \operatorname{E}\left[a_{i0}+\sum_{j=1}^{m}a_{ij}X_{ij}-m(\vartheta)\right]=a_%
 {i0}+\sum_{j=1}^{m}a_{ij}\operatorname{E}(X_{ij})-\operatorname{E}(m(\vartheta%
 ))=a_{i0}-\left(\sum_{j=1}^{m}a_{ij}-1\right)\mu        a   i  0    =    (     ∑   j  =  1   m    a   i  j     -  1   )   μ        subscript  a    i  0          superscript   subscript     j  1    m    subscript  a    i  j     1   μ     a_{i0}=\left(\sum_{j=1}^{m}a_{ij}-1\right)\mu     For    k  ≠  0      k  0    k\neq 0   we have:        1  2     ∂  f    ∂   a   i  k       =   E   [    X   i  k     (     a   i  0    +    ∑   j  =  1   m     a   i  j     X   i  j       -   m   (  ϑ  )     )    ]            1  2       f      subscript  a    i  k        normal-E     subscript  X    i  k         subscript  a    i  0      superscript   subscript     j  1    m      subscript  a    i  j     subscript  X    i  j         m  ϑ        \frac{1}{2}\frac{\partial f}{\partial a_{ik}}=\operatorname{E}\left[X_{ik}%
 \left(a_{i0}+\sum_{j=1}^{m}a_{ij}X_{ij}-m(\vartheta)\right)\right]        =      E   [   X   i  k    ]     a   i  0     +    ∑    j  =  1   ,   j  ≠  k    m     a   i  j     E   [    X   i  k     X   i  j     ]      +    a   i  k     E   [   X   i  k   2   ]      -   E   [    X   i  k    m   (  ϑ  )    ]     =  0        absent        normal-E   subscript  X    i  k      subscript  a    i  0       superscript   subscript    formulae-sequence    j  1     j  k     m      subscript  a    i  j     normal-E     subscript  X    i  k     subscript  X    i  j           subscript  a    i  k     normal-E   subscript   superscript  X  2     i  k        normal-E     subscript  X    i  k    m  ϑ          0     =\operatorname{E}\left[X_{ik}\right]a_{i0}+\sum_{j=1,j\neq k}^{m}a_{ij}%
 \operatorname{E}[X_{ik}X_{ij}]+a_{ik}\operatorname{E}[X^{2}_{ik}]-%
 \operatorname{E}[X_{ik}m(\vartheta)]=0     We can simplify derivative, noting that:      E   [   X   i  j     X   i  k    ]   =  E   [  E   [   X   i  j     X   i  k    |  ϑ  ]   ]   =  E   [  c  o  v   (   X   i  j     X   i  k    |  ϑ  )   +  E   (   X   i  j    |  ϑ  )   E   (   X   i  k    |  ϑ  )   ]   =  E   [    (  m   (  ϑ  )   )   2   ]   =   v  2   +   μ  2      fragments  normal-E   fragments  normal-[   subscript  X    i  j     subscript  X    i  k    normal-]    normal-E   fragments  normal-[  normal-E   fragments  normal-[   subscript  X    i  j     subscript  X    i  k    normal-|  ϑ  normal-]   normal-]    normal-E   fragments  normal-[  c  o  v   fragments  normal-(   subscript  X    i  j     subscript  X    i  k    normal-|  ϑ  normal-)    normal-E   fragments  normal-(   subscript  X    i  j    normal-|  ϑ  normal-)   normal-E   fragments  normal-(   subscript  X    i  k    normal-|  ϑ  normal-)   normal-]    normal-E   fragments  normal-[   superscript   fragments  normal-(  m   fragments  normal-(  ϑ  normal-)   normal-)   2   normal-]     superscript  v  2     superscript  μ  2     \operatorname{E}[X_{ij}X_{ik}]=\operatorname{E}[\operatorname{E}[X_{ij}X_{ik}|%
 \vartheta]]=\operatorname{E}[cov(X_{ij}X_{ik}|\vartheta)+\operatorname{E}(X_{%
 ij}|\vartheta)\operatorname{E}(X_{ik}|\vartheta)]=\operatorname{E}[(m(%
 \vartheta))^{2}]=v^{2}+\mu^{2}     and       E   [   X   i  k   2   ]    =   E   [   E   [   X   i  k   2   |  ϑ  ]    ]    =   E   [     s  2    (  ϑ  )    +    (   m   (  ϑ  )    )   2    ]    =    σ  2   +   v  2   +   μ  2           normal-E   subscript   superscript  X  2     i  k      normal-E   normal-E   subscript   superscript  X  2     i  k    ϑ          normal-E       superscript  s  2   ϑ    superscript    m  ϑ   2             superscript  σ  2    superscript  v  2    superscript  μ  2       \operatorname{E}[X^{2}_{ik}]=\operatorname{E}[\operatorname{E}[X^{2}_{ik}|%
 \vartheta]]=\operatorname{E}[s^{2}(\vartheta)+(m(\vartheta))^{2}]=\sigma^{2}+v%
 ^{2}+\mu^{2}     and      E   [   X   i  k    m   (  ϑ  )   ]   =  E   [  E   [   X   i  k    m   (  ϑ  )   |   Θ  i   ]   =  E   [    (  m   (  ϑ  )   )   2   ]   =   v  2   +   μ  2       fragments  normal-E   fragments  normal-[   subscript  X    i  k    m   fragments  normal-(  ϑ  normal-)   normal-]    normal-E   fragments  normal-[  normal-E   fragments  normal-[   subscript  X    i  k    m   fragments  normal-(  ϑ  normal-)   normal-|   subscript  normal-Θ  i   normal-]    normal-E   fragments  normal-[   superscript   fragments  normal-(  m   fragments  normal-(  ϑ  normal-)   normal-)   2   normal-]     superscript  v  2     superscript  μ  2      \operatorname{E}[X_{ik}m(\vartheta)]=\operatorname{E}[\operatorname{E}[X_{ik}m%
 (\vartheta)|\Theta_{i}]=\operatorname{E}[(m(\vartheta))^{2}]=v^{2}+\mu^{2}     Taking above equations and inserting into derivative, we have:       1  2     ∂  f    ∂   a   i  k            1  2       f      subscript  a    i  k        \frac{1}{2}\frac{\partial f}{\partial a_{ik}}        =      (   1  -    ∑   j  =  1   m    a   i  j      )    μ  2    +    ∑    j  =  1   ,   j  ≠  k    m     a   i  j     (    v  2   +   μ  2    )     +    a   i  k     (    σ  2   +   v  2   +   μ  2    )     -   (    v  2   +   μ  2    )    =     a   i  k     σ  2    -    (   1  -    ∑   j  =  1   m    a   i  j      )    v  2     =  0        absent          1    superscript   subscript     j  1    m    subscript  a    i  j       superscript  μ  2      superscript   subscript    formulae-sequence    j  1     j  k     m      subscript  a    i  j       superscript  v  2    superscript  μ  2         subscript  a    i  k       superscript  σ  2    superscript  v  2    superscript  μ  2         superscript  v  2    superscript  μ  2               subscript  a    i  k     superscript  σ  2        1    superscript   subscript     j  1    m    subscript  a    i  j       superscript  v  2          0     =\left(1-\sum_{j=1}^{m}a_{ij}\right)\mu^{2}+\sum_{j=1,j\neq k}^{m}a_{ij}(v^{2}%
 +\mu^{2})+a_{ik}(\sigma^{2}+v^{2}+\mu^{2})-(v^{2}+\mu^{2})=a_{ik}\sigma^{2}-%
 \left(1-\sum_{j=1}^{m}a_{ij}\right)v^{2}=0         σ  2    a   i  k     =    v  2    (   1  -    ∑   j  =  1   m    a   i  j      )           superscript  σ  2    subscript  a    i  k        superscript  v  2     1    superscript   subscript     j  1    m    subscript  a    i  j         \sigma^{2}a_{ik}=v^{2}\left(1-\sum_{j=1}^{m}a_{ij}\right)     Right side doesn't depend on k. Therefore all    a   i  k      subscript  a    i  k     a_{ik}   are constant       a   i  1    =   a   i  2    =  …  =   a   i  m    =    v  2     σ  2   +   m   v  2             subscript  a    i  1     subscript  a    i  2         normal-…        subscript  a    i  m            superscript  v  2      superscript  σ  2     m   superscript  v  2         a_{i1}=a_{i2}=...=a_{im}=\frac{v^{2}}{\sigma^{2}+mv^{2}}     From the solution for    a   i  0      subscript  a    i  0     a_{i0}   we have       a   i  0    =    (   1  -   m   a   i  k      )   μ   =    (   1  -    m   v  2      σ  2   +   m   v  2       )   μ          subscript  a    i  0        1    m   subscript  a    i  k      μ            1      m   superscript  v  2       superscript  σ  2     m   superscript  v  2       μ      a_{i0}=(1-ma_{ik})\mu=\left(1-\frac{mv^{2}}{\sigma^{2}+mv^{2}}\right)\mu     Finally, the best estimator is        a   i  0    +    ∑   j  =  1   m     a   i  j     X   i  j       =      m   v  2      σ  2   +   m   v  2        X  i   ¯    +    (   1  -    m   v  2      σ  2   +   m   v  2       )   μ    =    Z    X  i   ¯    +    (   1  -  Z   )   μ             subscript  a    i  0      superscript   subscript     j  1    m      subscript  a    i  j     subscript  X    i  j               m   superscript  v  2       superscript  σ  2     m   superscript  v  2       normal-¯   subscript  X  i         1      m   superscript  v  2       superscript  σ  2     m   superscript  v  2       μ             Z   normal-¯   subscript  X  i         1  Z   μ       a_{i0}+\sum_{j=1}^{m}a_{ij}X_{ij}=\frac{mv^{2}}{\sigma^{2}+mv^{2}}\bar{X_{i}}+%
 \left(1-\frac{mv^{2}}{\sigma^{2}+mv^{2}}\right)\mu=Z\bar{X_{i}}+(1-Z)\mu     References       "  Category:Actuarial science  Category:Analysis of variance     ↩  url = http://www.math.ku.dk/~schmidli/rt.pdf ↩     