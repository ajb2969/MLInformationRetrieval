<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1693">PAQ</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>PAQ</h1>
<hr/>
<figure><b>(Figure)</b>
<figcaption>A sample session of PAQ8O</figcaption>
</figure>

<p><strong>PAQ</strong> is a series of <a href="lossless_data_compression" title="wikilink">lossless data compression</a> archivers that have gone through collaborative development to top rankings on several benchmarks measuring compression ratio (although at the expense of speed and memory usage). Specialized versions of PAQ have won the <a href="Hutter_Prize" title="wikilink">Hutter Prize</a> and the <a href="Calgary_Challenge" title="wikilink">Calgary Challenge</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> PAQ is <a href="free_software" title="wikilink">free software</a> distributed under the <a href="GNU_General_Public_License" title="wikilink">GNU General Public License</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h2 id="algorithm">Algorithm</h2>

<p>PAQ uses a <a href="context_mixing" title="wikilink">context mixing</a> algorithm. Context mixing is related to <a href="Prediction_by_Partial_Matching" title="wikilink">Prediction by Partial Matching</a> (PPM) in that the compressor is divided into a predictor and an <a href="arithmetic_code" title="wikilink">arithmetic coder</a>, but differs in that the next-symbol prediction is computed using a weighed combination of probability estimates from a large number of models conditioned on different contexts. Unlike PPM, a context doesn't need to be contiguous. Most PAQ versions collect next-symbol statistics for the following contexts:</p>
<ul>
<li><a href="n-gram" title="wikilink"><em>n</em>-grams</a>. The context is the last  bytes before the predicted symbol (as in PPM).</li>
<li>whole word n-grams, ignoring case and nonalphabetic characters (useful in text files).</li>
<li>"sparse" contexts, for example, the second and fourth bytes preceding the predicted symbol (useful in some binary formats).</li>
<li>"analog" contexts, consisting of the high order bits of previous 8 or 16 bit words (useful for multimedia files).</li>
<li>two-dimensional contexts (useful for images, tables, and spreadsheets). The row length is determined by finding the stride length of repeating byte patterns.</li>
<li>specialized models, such as x86 executables, or <a href="Windows_Bitmap" title="wikilink">BMP</a>, <a class="uri" href="TIFF" title="wikilink">TIFF</a>, or <a class="uri" href="JPEG" title="wikilink">JPEG</a> images. These models are active only when the particular file type is detected.</li>
</ul>

<p>All PAQ versions predict and compress one bit at a time, but differ in the details of the models and how the predictions are combined and postprocessed. Once the next-bit probability is determined, it is encoded by <a href="arithmetic_coding" title="wikilink">arithmetic coding</a>. There are three methods for combining predictions, depending on the version.</p>
<ul>
<li>In PAQ1 through PAQ3, each prediction is represented as a pair of bit counts 

<math display="inline" id="PAQ:0">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <msub>
    <mi>n</mi>
    <mn>0</mn>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>n</mi>
    <mn>1</mn>
   </msub>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="open">
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>n</ci>
     <cn type="integer">0</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>n</ci>
     <cn type="integer">1</cn>
    </apply>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (n_{0},n_{1})
  </annotation>
 </semantics>
</math>

. These counts are combined by weighted summation, with greater weights given to longer contexts.</li>
<li>In PAQ4 through PAQ6, the predictions are combined as before, but the weights assigned to each model are adjusted to favor the more accurate models.</li>
<li>In PAQ7 and later, each model outputs a probability rather than a pair of counts. The probabilities are combined using an <a href="artificial_neural_network" title="wikilink">artificial neural network</a>.</li>
</ul>

<p>PAQ1SSE and later versions postprocess the prediction using secondary symbol estimation (SSE). The combined prediction and a small context are used to look up a new prediction in a table. After the bit is encoded, the table entry is adjusted to reduce the prediction error. SSE stages can be pipelined with different contexts, or computed in parallel with the outputs averaged.</p>
<h3 id="arithmetic-coding">Arithmetic coding</h3>

<p>A string <em>s</em> is compressed to the shortest byte string representing a base 256 <a href="big_endian" title="wikilink">big endian</a> number <em>x</em> in the range [0, 1] such that P(r 2P(r = s) bits. The length of <em>s</em> is stored in the archive header.</p>

<p>The <a href="Arithmetic_coding" title="wikilink">arithmetic coder</a> in PAQ is implemented by maintaining for each prediction a lower and upper bound on <em>x</em>, initially [0, 1]. After each prediction, the current range is split into two parts in proportion to <em>P(0)</em> and <em>P(1)</em>, the probability that the next bit of <em>s</em> will be a 0 or 1, respectively, given the previous bits of <em>s</em>. The next bit is then encoded by selecting the corresponding subrange to be the new range.</p>

<p>The number <em>x</em> is decompressed back to string <em>s</em> by making an identical series of bit predictions (since the previous bits of <em>s</em> are known). The range is split as with compression. The portion containing <em>x</em> becomes the new range, and the corresponding bit is appended to <em>s</em>.</p>

<p>In PAQ, the lower and upper bounds of the range are represented in 3 parts. The most significant base-256 digits are identical, so they can be written as the leading bytes of <em>x</em>. The next 4 bytes are kept in memory, such that the leading byte is different. The trailing bits are assumed to be all zeros for the lower bound and all ones for the upper bound. Compression is terminated by writing one more byte from the lower bound.</p>
<h3 id="adaptive-model-weighting">Adaptive model weighting</h3>

<p>In PAQ versions through PAQ6, each model maps a set of distinct contexts to a pair of counts, 

<math display="inline" id="PAQ:1">
 <semantics>
  <msub>
   <mi>n</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>n</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n_{0}
  </annotation>
 </semantics>
</math>

, a count of zero bits, and 

<math display="inline" id="PAQ:2">
 <semantics>
  <msub>
   <mi>n</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>n</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n_{1}
  </annotation>
 </semantics>
</math>

, a count of 1 bits. In order to favor recent history, half of the count over 2 is discarded when the opposite bit is observed. For example, if the current state associated with a context is 

<math display="inline" id="PAQ:3">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>n</mi>
     <mn>0</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>n</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>12</mn>
    <mo>,</mo>
    <mn>3</mn>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>n</ci>
      <cn type="integer">0</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>n</ci>
      <cn type="integer">1</cn>
     </apply>
    </interval>
    <interval closure="open">
     <cn type="integer">12</cn>
     <cn type="integer">3</cn>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (n_{0},n_{1})=(12,3)
  </annotation>
 </semantics>
</math>

 and a 1 is observed, then the counts are updated to (7,4).</p>

<p>A bit is arithmetic coded with space proportional to its probability, either P(1) or P(0) = 1 − P(1). The probabilities are computed by weighted addition of the 0 and 1 counts:</p>
<ul>
<li>S<sub>0</sub> = Σ<sub>i</sub> w<sub>i</sub> n<sub>0i</sub></li>
<li>S<sub>1</sub> = Σ<sub>i</sub> w<sub>i</sub> n<sub>1i</sub></li>
<li>S = S<sub>0</sub> + S<sub>1</sub></li>
<li>P(0) = S<sub>0</sub> / S</li>
<li>P(1) = S<sub>1</sub> / S</li>
</ul>

<p>where 

<math display="inline" id="PAQ:4">
 <semantics>
  <msub>
   <mi>w</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}
  </annotation>
 </semantics>
</math>

 is the weight of the i'th model. Through PAQ3, the weights were fixed and set in an ad-hoc manner. (Order-n contexts had a weight of n².) Beginning with PAQ4, the weights were adjusted adaptively in the direction that would reduce future errors in the same context set. If the bit to be coded is <em>y</em>, then the weight adjustment is:</p>
<ul>
<li>n<sub>i</sub> = n<sub>0i</sub> + n<sub>1i</sub></li>
<li>error = y – P(1)</li>
<li>w<sub>i</sub> ← w<sub>i</sub> + [(S n<sub>1i</sub> - S<sub>1</sub> n<sub>i</sub>) / (S<sub>0</sub> S<sub>1</sub>)] error</li>
</ul>
<h3 id="neural-network-mixing">Neural network mixing</h3>

<p>Beginning with PAQ7, each model outputs a prediction (instead of a pair of counts). These predictions are averaged in the logistic domain:</p>
<ul>
<li>x<sub>i</sub> = stretch(P<sub>i</sub>(1))</li>
<li>P(1) = squash(Σ<sub>i</sub> w<sub>i</sub> x<sub>i</sub>)</li>
</ul>

<p>where P(1) is the probability that the next bit will be a 1, P<sub>i</sub>(1) is the probability estimated by the <em>i'th</em> model, and</p>
<ul>
<li>stretch(x) = ln(x / (1 - x))</li>
<li>squash(x) = 1 / (1 + e<sup>-x</sup>) (inverse of stretch).</li>
</ul>

<p>After each prediction, the model is updated by adjusting the weights to minimize coding cost.</p>
<ul>
<li>w<sub>i</sub> ← w<sub>i</sub> + η x<sub>i</sub> (y - P(1))</li>
</ul>

<p>where η is the learning rate (typically 0.002 to 0.01), <em>y</em> is the predicted bit, and (y - P(1)) is the prediction error. The weight update algorithm differs from <a class="uri" href="backpropagation" title="wikilink">backpropagation</a> in that the terms P(1)P(0) are dropped. This is because the goal of the neural network is to minimize coding cost, not <a href="root_mean_square" title="wikilink">root mean square</a> error.</p>

<p>Most versions of PAQ use a small context to select among sets of weights for the neural network. Some versions use multiple networks whose outputs are combined with one more network prior to the SSE stages. Furthermore, for each input prediction there may be several inputs which are <a class="uri" href="nonlinear" title="wikilink">nonlinear</a> functions of P<sub>i</sub>(1) in addition to <em>stretch(P(1))</em></p>
<h3 id="context-modeling">Context modeling</h3>

<p>Each model partitions the known bits of <em>s</em> into a set of contexts, and maps each context to a bit history represented by an 8 bit state. In versions through PAQ6, the state represents a pair of counters (<em>n</em><sub>0</sub>, <em>n</em><sub>1</sub>). In PAQ7 and later versions under certain conditions, the state also represents the value of the last bit or the entire sequence. The states are mapped to probabilities using a 256 entry table for each model. After a prediction by the model, the table entry is adjusted slightly (typically by 0.4%) to reduce the prediction error.</p>

<p>In all PAQ8 versions, the representable states are as follows:</p>
<ul>
<li>The exact bit sequence for up to 4 bits.</li>
<li>A pair of counts and an indicator of the most recent bit for sequences of 5 to 15 bits.</li>
<li>A pair of counts for sequences of 16 to 41 bits.</li>
</ul>

<p>To keep the number of states to 256, the following limits are placed on the representable counts: (41, 0), (40, 1), (12, 2), (5, 3), (4, 4), (3, 5), (2, 12), (1, 40), (0, 41). If a count exceeds this limit, then the next state is one chosen to have a similar ratio of <em>n</em><sub>0</sub> to <em>n</em><sub>1</sub>. Thus, if the current state is (<em>n</em><sub>0</sub> = 4, <em>n</em><sub>1</sub> = 4, last bit = 0) and a 1 is observed, then the new state is not (<em>n</em><sub>0</sub> = 4, <em>n</em><sub>1</sub> = 5, last bit = 1). Rather, it is (<em>n</em><sub>0</sub> = 3, n<sub>1</sub> = 4, last bit = 1).</p>

<p>Most context models are implemented as <a href="hash_table" title="wikilink">hash tables</a>. Some small contexts are implemented as direct <a href="lookup_table" title="wikilink">lookup tables</a>.</p>
<h3 id="text-preprocessing">Text preprocessing</h3>

<p>Some versions of PAQ, in particular PAsQDa, PAQAR (both PAQ6 derivatives), and PAQ8HP1 through PAQ8HP8 (PAQ8 derivatives and <a href="Hutter_prize" title="wikilink">Hutter prize</a> recipients) preprocess text files by looking up words in an external dictionary and replacing them with 1–3 byte codes. In addition, uppercase letters are encoded with a special character followed by the lower case letter. In the PAQ8HP series, the dictionary is organized by grouping syntactically and semantically related words together. This allows models to use just the most significant bits of the dictionary codes as context.</p>
<h2 id="comparison">Comparison</h2>

<p>The following table is a sample from the <a href="http://mattmahoney.net/dc/text.html">Large Text Compression Benchmark</a> by Matt Mahoney that consists of a file consisting of 10<sup>9</sup> bytes (1<a href="Gigabyte" title="wikilink">GB</a> or 0.931<a class="uri" href="GiB" title="wikilink">GiB</a>) of <a class="uri" href="Wikipedia" title="wikilink">Wikipedia</a> English text.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">
<p>Program</p></th>
<th style="text-align: left;">
<p>Compressed size (bytes)</p></th>
<th style="text-align: left;">
<p>% of original size</p></th>
<th style="text-align: left;">
<p>Compression time (<a href="second" title="wikilink">s</a>)</p></th>
<th style="text-align: left;">
<p>Memory (MiB)</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p>PAQ8HP8</p></td>
<td style="text-align: left;">
<p>133,423,109</p></td>
<td style="text-align: left;">
<p>{{#expr: ((133423109/1000000000)*100) round 2}}</p></td>
<td style="text-align: left;">
<p>64 639</p></td>
<td style="text-align: left;">
<p>1849</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p><a class="uri" href="PPMd" title="wikilink">PPMd</a></p></td>
<td style="text-align: left;">
<p>183,976,014</p></td>
<td style="text-align: left;">
<p>{{#expr: ((183976014/1000000000)*100) round 2}}</p></td>
<td style="text-align: left;">
<p>880</p></td>
<td style="text-align: left;">
<p>256</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p><a class="uri" href="bzip2" title="wikilink">bzip2</a></p></td>
<td style="text-align: left;">
<p>254,007,875</p></td>
<td style="text-align: left;">
<p>{{#expr: ((254007875/1000000000)*100) round 2}}</p></td>
<td style="text-align: left;">
<p>379</p></td>
<td style="text-align: left;">
<p>8</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p><a href="Infozip" title="wikilink">InfoZIP</a></p></td>
<td style="text-align: left;">
<p>322,649,703</p></td>
<td style="text-align: left;">
<p>{{#expr: ((322649703/1000000000)*100) round 2}}</p></td>
<td style="text-align: left;">
<p>104</p></td>
<td style="text-align: left;">
<p>0.1</p></td>
</tr>
</tbody>
</table>

<p>See <a href="Comparison_of_file_archivers" title="wikilink">Comparison of file archivers</a> for a list of file compression benchmarks.</p>
<h2 id="history">History</h2>

<p>The following lists the major enhancements to the PAQ algorithm. In addition, there have been a large number of incremental improvements, which are omitted.</p>
<ul>
<li><strong>PAQ1</strong> was released on January 6, 2002 by Matt Mahoney. It used fixed weights and did not include an analog or sparse model.</li>
</ul>
<ul>
<li><strong>PAQ1SSE/PAQ2</strong> was released on May 11, 2003 by Serge Osnach. It significantly improved compression by adding a SSE stage between the predictor and encoder. SSE (secondary symbol estimation) inputs a short context and the current prediction and outputs a new prediction from a table. The table entry is then adjusted to reflect the actual bit value.</li>
</ul>
<ul>
<li><strong>PAQ3N</strong>, released October 9, 2003 added a sparse model.</li>
</ul>
<ul>
<li><strong>PAQ4</strong>, released November 15, 2003 by Matt Mahoney used adaptive weighting. <strong>PAQ5</strong> (December 18, 2003) and <strong>PAQ6</strong> (December 30, 2003) were minor improvements, including a new analog model. At this point, PAQ was competitive with the best PPM compressors and attracted the attention of the data compression community, which resulted in a large number of incremental improvements through April 2004. Berto Destasio tuned the models and adjusted the bit count discounting schedule. Johan de Bock made improvements to the user interface. David A. Scott made improvements to the arithmetic coder. Fabio Buffoni made speed improvements.</li>
</ul>
<ul>
<li>During the period May 20, 2004 through July 27, 2004, Alexander Ratushnyak released seven versions of <strong>PAQAR</strong>, which made significant compression improvements by adding many new models, multiple mixers with weights selected by context, adding an SSE stage to each mixer output, and adding a preprocessor to improve the compression of Intel executable files. PAQAR stood as the top-ranked compressor through the end of 2004 but was significantly slower than prior PAQ versions.</li>
</ul>
<ul>
<li>During the period January 18, 2005 through February 7, 2005, Przemyslaw Skibinski released four versions of <strong>PASqDa</strong>, based on PAQ6 and PAQAR with the addition of an English dictionary preprocessor. It achieved the top ranking on the Calgary corpus but not on most other benchmarks.</li>
</ul>
<ul>
<li>A modified version of <strong>PAQ6</strong> won the <a href="Calgary_Challenge" title="wikilink">Calgary Challenge</a> on January 10, 2004 by Matt Mahoney. This was bettered by ten subsequent versions of PAQAR by Alexander Ratushnyak. The most recent was submitted on June 5, 2006, consisting of compressed data and program source code totaling 589,862 bytes.</li>
</ul>
<ul>
<li><strong>PAQ7</strong> was released December 2005 by Matt Mahoney. PAQ7 is a complete rewrite of PAQ6 and variants (PAQAR, PAsQDa). Compression ratio was similar to PAQAR but 3 times faster. However it lacked x86 and a dictionary, so it did not compress Windows executables and English text files as well as PAsQDa. It does include models for color BMP, TIFF and JPEG files, so compresses these files better. The primary difference from PAQ6 is it uses a neural network to combine models rather than a gradient descent mixer. Another feature is PAQ7's ability to compress embedded jpeg and bitmap images in Excel-, Word- and pdf-files.</li>
</ul>
<ul>
<li><strong>PAQ8A</strong> was released on January 27, 2006, <strong>PAQ8C</strong> on February 13, 2006. These were experimental pre-release of anticipated PAQ8. It fixed several issues in PAQ7 (poor compression in some cases). PAQ8A also included model for compressing (x86) executables.</li>
</ul>
<ul>
<li><strong>PAQ8F</strong> was released on February 28, 2006. PAQ8F had 3 improvements over PAQ8A: a more memory efficient context model, a new indirect context model to improve compression, and a new user interface to support drag and drop in Windows. It does not use an English dictionary like the PAQ8B/C/D/E variants.</li>
</ul>
<ul>
<li><strong>PAQ8G</strong> was released March 3, 2006 by Przemyslaw Skibinski. PAQ8G is PAQ8F with dictionaries added and some other improvements as a redesigned TextFilter (which does not decrease compression performance on non-textual files)</li>
</ul>
<ul>
<li><strong>PAQ8H</strong> was released on March 22, 2006 by Alexander Ratushnyak and updated on March 24, 2006. PAQ8H is based on PAQ8G with some improvements to the model.</li>
</ul>
<ul>
<li><strong>PAQ8I</strong> was released on August 18, 2006 by Pavel L. Holoborodko, with bug fixes on August 24, September 4, and September 13. It added a grayscale image model for <a href="Portable_bitmap#PGM_example" title="wikilink">PGM</a> files.</li>
</ul>
<ul>
<li><strong>PAQ8J</strong> was released on November 13, 2006 by Bill Pettis. It was based on <strong>PAQ8F</strong> with some text model improvements taken from PAQ8HP5. Thus, it did not include the text dictionaries from <strong>PAQ8G</strong> or PGM model from <strong>PAQ8I</strong>.</li>
</ul>
<ul>
<li>Serge Osnach released a series of modeling improvements: <strong>PAQ8JA</strong> on November 16, 2006, <strong>PAQ8JB</strong> on November 21, and <strong>PAQ8JC</strong> on November 28.</li>
</ul>
<ul>
<li><strong>PAQ8JD</strong> was released on December 30, 2006 by Bill Pettis. This version has since been ported to 32 bit <a href="Microsoft_Windows" title="wikilink">Windows</a> for several processors, and 32 and 64 bit <a class="uri" href="Linux" title="wikilink">Linux</a>.</li>
</ul>
<ul>
<li><strong>PAQ8K</strong> was released on February 13, 2007 by Bill Pettis. It includes additional models for binary files.</li>
</ul>
<ul>
<li><strong>PAQ8L</strong> was released on March 8, 2007 by Matt Mahoney. It is based on PAQ8JD and adds a <a href="Dynamic_Markov_Compression" title="wikilink">DMC</a> model.</li>
</ul>
<ul>
<li><strong>PAQ8O</strong> was released on August 24, 2007 by Andreas Morphis. Contains improved <a href="BMP_file_format" title="wikilink">BMP</a> and <a class="uri" href="JPEG" title="wikilink">JPEG</a> models over PAQ8L. Can be optionally compiled with <a class="uri" href="SSE2" title="wikilink">SSE2</a> support and for 64-bit Linux. The algorithm has notable performance benefits under 64-bit OS.</li>
</ul>
<ul>
<li><strong>PAQ8P</strong> was released on August 25, 2008 by Andreas Morphis. Contains improved BMP model and adds a <a class="uri" href="WAV" title="wikilink">WAV</a> model.</li>
</ul>
<ul>
<li><strong>PAQ8PX</strong> was released on April 25, 2009 by Jan Ondrus. It contains various improvements like better <a class="uri" href="WAV" title="wikilink">WAV</a> compression and <a class="uri" href="EXE" title="wikilink">EXE</a> compression.</li>
</ul>
<ul>
<li><strong>PAQ8KX</strong> was released on July 15, 2009 by Jan Ondrus. It is a combination of PAQ8K with PAQ8PX.</li>
</ul>
<ul>
<li><strong>PAQ8PF</strong> was released on September 9, 2009 by LovePimple without source code (which the <a class="uri" href="GPL" title="wikilink">GPL</a> license requires). It compresses 7% worse, but is 7 times faster compared to PAQ8PX v66 (measured with 1 MB English text)</li>
</ul>
<ul>
<li><strong>PAQ9A</strong> was released on December 31, 2007 by Matt Mahoney. A new experimental version. It does not include models for specific file types, has an LZP preprocessor and supports files over 2 GB.</li>
</ul>
<ul>
<li><strong><a class="uri" href="ZPAQ" title="wikilink">ZPAQ</a></strong> was released on March 12, 2009 by Matt Mahoney. It uses a new archive format designed so that future ZPAQ variants may preserve the ability to decompress existing archives (the various PAQ variants listed above are not backwards compatible in this fashion). It achieves this by specifying the decompression algorithm in a bytecode program that is stored in each created archive file.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></li>
</ul>
<h3 id="hutter-prizes">Hutter Prizes</h3>

<p>The series <strong>PAQ8HP1</strong> through <strong>PAQ8HP8</strong> were released by Alexander Ratushnyak from August 21, 2006 through January 18, 2007 as <a href="Hutter_Prize" title="wikilink">Hutter Prize</a> submissions. The Hutter Prize is a text compression contest using a 100 MB English and XML data set derived from Wikipedia's source. The PAQ8HP series was forked from PAQ8H. The programs include text preprocessing dictionaries and models tuned specifically to the benchmark. All non-text models were removed. The dictionaries were organized to group syntactically and semantically related words and to group words by common suffix. The former strategy improves compression because related words (which are likely to appear in similar context) can be modeled on the high order bits of their dictionary codes. The latter strategy makes the dictionary easier to compress. The size of the decompression program and compressed dictionary is included in the contest ranking.</p>

<p>On October 27, 2006, it was announced<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> that <strong>PAQ8HP5</strong> won a <a href="Hutter_Prize" title="wikilink">Hutter Prize for Lossless Compression of Human Knowledge</a> of <a href="Euro" title="wikilink">€</a>3,416.</p>

<p>On June 30, 2007, Ratushnyak's <strong>paq8hp12</strong> was awarded a second Hutter prize of €1732,<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> improving upon his previous record by 3.46%.</p>
<h2 id="paq-derivations">PAQ derivations</h2>

<p>Being <a href="free_software" title="wikilink">free software</a>, PAQ can be modified and redistributed by anyone who has a copy. This has allowed other authors to <a href="Fork_(software_development)" title="wikilink">fork</a> the PAQ compression engine and add new features such as a <a href="graphical_user_interface" title="wikilink">graphical user interface</a> or better speed (at the expense of compression ratio). Notable PAQ derivatives include:</p>
<ul>
<li><strong>WinUDA 0.291</strong>, based on PAQ6 but faster<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></li>
<li><strong>UDA 0.301</strong>, based on PAQ8I algorithm<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></li>
</ul>
<ul>
<li><strong><a href="KGB_Archiver" title="wikilink">KGB</a></strong>, based on PAQ6<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> (beta version is based on PAQ7).</li>
</ul>
<ul>
<li><strong>Emilcont</strong> based on PAQ6<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></li>
<li><strong><a class="uri" href="Peazip" title="wikilink">Peazip</a></strong> GUI frontend (for Windows and Linux) for <abbr title='"Lite" PAQ, single file compressor'>LPAQ</abbr><a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> and various PAQ8* algorithms<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></li>
<li>PWCM (PAQ weighted context mixing) is an independently developed closed source implementation of the PAQ algorithm used in <a class="uri" href="WinRK" title="wikilink">WinRK</a>.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></li>
<li><strong>PerfectCompress<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></strong> Is a compression software which features UCA (ULTRA Compressed Archive). A compression format that featured PAQ8PX v42 to v65 and that now can use PAQ8PF, PAQ8KX, or PAQ8PXPRE as the default UCA Compressor. In addition, PerfectCompress can compress files to PAQ8PX v42 to v67, and ZPAQ, and as of version 6.0, can compress files to LPAQ and PAQ8PF beta 1 to beta 3. PerfectCompress v6.10 introduced support compression for the recently released PAQ8PXPRE. PerfectCompress 6.12 introduces support for the PAQ8KX series.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></li>
<li><strong><a class="uri" href="FrontPAQ" title="wikilink">FrontPAQ</a></strong>, small gui for PAQ. Latest version is FrontPAQ v8 supporting PAQ8PX, PAQ8PF, and FP8</li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="List_of_archive_formats" title="wikilink">List of archive formats</a></li>
<li><a href="Comparison_of_file_archivers" title="wikilink">Comparison of file archivers</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="further-reading">Further reading</h2>
<ul>
<li>David Salomon, Giovanni Motta, (with contributions by David Bryant), <em>Handbook of Data Compression</em>, 5th edition, Springer, 2009, ISBN 1-84882-902-7, 5.15 PAQ, pp. 314–319</li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li></li>
<li><a href="http://powerpaq.sourceforge.net/">Compiled linux binaries</a> - Linux command-line executables download.</li>
</ul>

<p>"</p>

<p><a href="Category:Free_data_compression_software" title="wikilink">Category:Free data compression software</a> <a href="Category:Lossless_compression_algorithms" title="wikilink">Category:Lossless compression algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4">James Bowery. <a href="http://groups.google.com/group/Hutter-Prize/browse_frm/thread/3f3f80c76dd14513/">Alexander Ratushnyak Wins First Hutter Prize Payout</a>. Published October 27, 2006. Retrieved October 30, 2006. <a href="#fnref4">↩</a></li>
<li id="fn5"><a class="uri" href="http://prize.hutter1.net/award2.gif">http://prize.hutter1.net/award2.gif</a><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="http://wex.cn/dwing/mycomp.htm">dwing's homepage</a><a href="#fnref6">↩</a></li>
<li id="fn7"></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"><a href="#fnref11">↩</a></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"><a href="#fnref14">↩</a></li>
</ol>
</section>
</body>
</html>
