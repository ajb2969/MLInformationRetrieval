<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1188">Logistic regression</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Logistic regression</h1>
<hr/>

<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, <strong>logistic regression</strong>, or <strong>logit regression</strong>, or <strong>logit model</strong><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> is a <a class="uri" href="regression" title="wikilink">regression</a> model where the dependent variable is categorical. This article is about the case when it is binary, while problems with more than two categories are referred to as <a href="multinomial_logistic_regression" title="wikilink">multinomial logistic regression</a>, or, if the multiple categories are <a href="Level_of_measurement#Ordinal_type" title="wikilink">ordered</a>, as <a href="ordinal_logistic_regression" title="wikilink">ordinal logistic regression</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>Logistic regression was developed by statistician <a href="David_Cox_(statistician)" title="wikilink">David Cox</a> in 1958<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> (although much work was done in the single independent variable case almost two decades earlier). The binary logistic model is used to predict a binary response based on one or more predictor (or independent) variables (features), making it a <a href="probabilistic_classification" title="wikilink">probabilistic classification</a> model in the parlance of <a href="machine_learning" title="wikilink">machine learning</a>,<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> or a <a href="Discrete_choice" title="wikilink">qualitative response/discrete choice model</a> in the terminology of <a class="uri" href="economics" title="wikilink">economics</a>.</p>

<p>The probabilities describing the possible outcomes of a single trial are modeled as a function of the explanatory (predictor) variables, using a <a href="logistic_function" title="wikilink">logistic function</a>. Frequently (and hereafter in this article) "logistic regression" is used to refer specifically to the problem in which the dependent variable is <a href="Binary_variable" title="wikilink">binary</a>—that is, the number of available categories is two— Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities. Thus, it treats the same set of problems as does <a href="probit_regression" title="wikilink">probit regression</a> using similar techniques; the first assumes a <a href="logistic_function" title="wikilink">logistic function</a> and the second a standard <a href="normal_distribution" title="wikilink">normal distribution</a> function.</p>

<p>Logistic regression can be seen as a special case of <a href="generalized_linear_model" title="wikilink">generalized linear model</a> and thus analogous to <a href="linear_regression" title="wikilink">linear regression</a>. The model of logistic regression, however, is based on quite different assumptions (about the relationship between dependent and independent variables) from those of linear regression. In particular the key differences of these two models can be seen in the following two features of logistic regression. First, the conditional distribution 

<math display="inline" id="Logistic_regression:0">
<semantics>
<mrow>
<mi>p</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>y</mi>
<mo>∣</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<csymbol cd="unknown">p</csymbol>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-(</ci>
<csymbol cd="unknown">y</csymbol>
<ci>normal-∣</ci>
<csymbol cd="unknown">x</csymbol>
<ci>normal-)</ci>
</cerror>
</cerror>
</annotation-xml>
<annotation encoding="application/x-tex">
   p(y\mid x)
  </annotation>
</semantics>
</math>

 is a <a href="Bernoulli_distribution" title="wikilink">Bernoulli distribution</a> rather than a <a href="Gaussian_distribution" title="wikilink">Gaussian distribution</a>, because the dependent variable is binary. Second, the estimated probabilities are restricted to [0,1] through the <a href="logistic_function" title="wikilink">logistic distribution function</a> because logistic regression predicts the <strong>probability</strong> of the instance being positive.</p>

<p>Logistic regression is an alternative to Fisher's 1936 classification method, <a href="linear_discriminant_analysis" title="wikilink">linear discriminant analysis</a>.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> If the assumptions of linear discriminant analysis hold, application of Bayes' rule to reverse the conditioning results in the logistic model, so if linear discriminant assumptions are true, logistic regression assumptions must hold. The converse is not true, so the logistic model has fewer assumptions than discriminant analysis and makes no assumption on the distribution of the independent variables.</p>
<h2 id="fields-and-example-applications">Fields and example applications</h2>

<p>Logistic regression is used widely in many fields, including the medical and social sciences. For example, the Trauma and Injury Severity Score (TRISS), which is widely used to predict mortality in injured patients, was originally developed by Boyd et al. using logistic regression.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> Many other medical scales used to assess severity of a patient have been developed using logistic regression.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a><a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a><a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> Logistic regression may be used to predict whether a patient has a given disease (e.g. <a href="Diabetes_mellitus" title="wikilink">diabetes</a>; <a href="Coronary_artery_disease" title="wikilink">coronary heart disease</a>), based on observed characteristics of the patient (age, sex, <a href="body_mass_index" title="wikilink">body mass index</a>, results of various <a href="blood_test" title="wikilink">blood tests</a>, etc.; age, blood cholesterol level, systolic blood pressure, relative weight, blood hemoglobin level, smoking (at 3 levels), and abnormal electrocardiogram.).<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a><a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> Another example might be to predict whether an American voter will vote Democratic or Republican, based on age, income, sex, race, state of residence, votes in previous elections, etc.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> The technique can also be used in <a class="uri" href="engineering" title="wikilink">engineering</a>, especially for predicting the probability of failure of a given process, system or product.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a><a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> It is also used in <a class="uri" href="marketing" title="wikilink">marketing</a> applications such as prediction of a customer's propensity to purchase a product or halt a subscription, etc. In <a class="uri" href="economics" title="wikilink">economics</a> it can be used to predict the likelihood of a person's choosing to be in the labor force, and a business application would be to predict the likelihood of a homeowner defaulting on a <a class="uri" href="mortgage" title="wikilink">mortgage</a>. <a href="Conditional_random_field" title="wikilink">Conditional random fields</a>, an extension of logistic regression to sequential data, are used in <a href="natural_language_processing" title="wikilink">natural language processing</a>.</p>
<h2 id="basics">Basics</h2>

<p>Logistic regression can be binomial, ordinal or multinomial. Binomial or binary logistic regression deals with situations in which the observed outcome for a <a href="dependent_variable" title="wikilink">dependent variable</a> can have only two possible types (for example, "dead" vs. "alive" or "win" vs. "loss"). <a href="multinomial_logit" title="wikilink">Multinomial logistic regression</a> deals with situations where the outcome can have three or more possible types (e.g., "disease A" vs. "disease B" vs. "disease C") that are not ordered. <a href="Ordinal_logistic_regression" title="wikilink">Ordinal logistic regression</a> deals with dependent variables that are ordered. In binary logistic regression, the outcome is usually coded as "0" or "1", as this leads to the most straightforward interpretation.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> If a particular observed outcome for the dependent variable is the noteworthy possible outcome (referred to as a "success" or a "case") it is usually coded as "1" and the contrary outcome (referred to as a "failure" or a "noncase") as "0". Logistic regression is used to predict the <a class="uri" href="odds" title="wikilink">odds</a> of being a case based on the values of the <a href="independent_variable" title="wikilink">independent variables</a> (predictors). The odds are defined as the probability that a particular outcome is a case divided by the probability that it is a noncase.</p>

<p>Like other forms of <a href="regression_analysis" title="wikilink">regression analysis</a>, logistic regression makes use of one or more predictor variables that may be either continuous or categorical. Unlike ordinary linear regression, however, logistic regression is used for predicting binary dependent variables (treating the dependent variable as the outcome of a <a href="Bernoulli_trial" title="wikilink">Bernoulli trial</a>) rather than a continuous outcome. Given this difference, the assumptions of linear regression are violated. In particular, the residuals cannot be normally distributed. In addition, linear regression may make nonsensical predictions for a binary dependent variable. What is needed is a way to convert a binary variable into a continuous one that can take on any real value (negative or positive). To do that logistic regression first takes the <a class="uri" href="odds" title="wikilink">odds</a> of the event happening for different levels of each independent variable, then takes the ratio of those odds (which is continuous but cannot be negative) and then takes the <a class="uri" href="logarithm" title="wikilink">logarithm</a> of that ratio. This is referred to as <a class="uri" href="logit" title="wikilink">logit</a> or log-odds) to create a continuous criterion as a transformed version of the dependent variable.</p>

<p>Thus the logit transformation is referred to as the <em>link function</em> in logistic regression—although the dependent variable in logistic regression is binomial, the logit is the continuous criterion upon which linear regression is conducted.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a></p>

<p>The logit of success is then fitted to the predictors using <a href="linear_regression" title="wikilink">linear regression</a> analysis. The predicted value of the logit is converted back into predicted odds via the inverse of the natural logarithm, namely the <a href="exponential_function" title="wikilink">exponential function</a>. Thus, although the observed dependent variable in logistic regression is a zero-or-one variable, the logistic regression estimates the odds, as a continuous variable, that the dependent variable is a success (a case). In some applications the odds are all that is needed. In others, a specific yes-or-no prediction is needed for whether the dependent variable is or is not a case; this categorical prediction can be based on the computed odds of a success, with predicted odds above some chosen cutoff value being translated into a prediction of a success.</p>
<h2 id="logistic-function-odds-odds-ratio-and-logit">Logistic function, odds, odds ratio, and logit</h2>
<figure><b>(Figure)</b>
<figcaption>Figure 1. The logistic function 

<math display="inline" id="Logistic_regression:1">
<semantics>
<mrow>
<mi>σ</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>t</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<times></times>
<ci>σ</ci>
<ci>t</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \sigma(t)
  </annotation>
</semantics>
</math>

; note that 

<math display="inline" id="Logistic_regression:2">
<semantics>
<mrow>
<mrow>
<mi>σ</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>t</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>∈</mo>
<mrow>
<mo stretchy="false">[</mo>
<mn>0</mn>
<mo>,</mo>
<mn>1</mn>
<mo stretchy="false">]</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<in></in>
<apply>
<times></times>
<ci>σ</ci>
<ci>t</ci>
</apply>
<interval closure="closed">
<cn type="integer">0</cn>
<cn type="integer">1</cn>
</interval>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \sigma(t)\in[0,1]
  </annotation>
</semantics>
</math>

 for all 

<math display="inline" id="Logistic_regression:3">
<semantics>
<mi>t</mi>
<annotation-xml encoding="MathML-Content">
<ci>t</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   t
  </annotation>
</semantics>
</math>

.</figcaption>
</figure>
<h3 id="definition-of-the-logistic-function">Definition of the logistic function</h3>

<p>An explanation of logistic regression begins with an explanation of the <a href="logistic_function" title="wikilink">logistic function</a>. The logistic function is useful because it can take an input with any value from negative to positive infinity, whereas the output always takes values between zero and one<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a> and hence is interpretable as a probability. The logistic function 

<math display="inline" id="Logistic_regression:4">
<semantics>
<mrow>
<mi>σ</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>t</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<times></times>
<ci>σ</ci>
<ci>t</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \sigma(t)
  </annotation>
</semantics>
</math>


 is defined as follows:</p>

<p>
<math display="block" id="Logistic_regression:5">
<semantics>
<mrow>
<mrow>
<mrow>
<mi>σ</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>t</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mfrac>
<msup>
<mi>e</mi>
<mi>t</mi>
</msup>
<mrow>
<msup>
<mi>e</mi>
<mi>t</mi>
</msup>
<mo>+</mo>
<mn>1</mn>
</mrow>
</mfrac>
<mo>=</mo>
<mfrac>
<mn>1</mn>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<mo>-</mo>
<mi>t</mi>
</mrow>
</msup>
</mrow>
</mfrac>
</mrow>
<mo>,</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<and></and>
<apply>
<eq></eq>
<apply>
<times></times>
<ci>σ</ci>
<ci>t</ci>
</apply>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<ci>t</ci>
</apply>
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<ci>t</ci>
</apply>
<cn type="integer">1</cn>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<divide></divide>
<cn type="integer">1</cn>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<minus></minus>
<ci>t</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \sigma(t)=\frac{e^{t}}{e^{t}+1}=\frac{1}{1+e^{-t}},
  </annotation>
</semantics>
</math>
</p>

<p>A graph of the logistic function is shown in Figure 1.</p>

<p>If 

<math display="inline" id="Logistic_regression:6">
<semantics>
<mi>t</mi>
<annotation-xml encoding="MathML-Content">
<ci>t</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   t
  </annotation>
</semantics>
</math>

 is viewed as a linear function of an <a href="dependent_and_independent_variables" title="wikilink">explanatory variable</a>
<math display="inline" id="Logistic_regression:7">
<semantics>
<mi>x</mi>
<annotation-xml encoding="MathML-Content">
<ci>x</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   x
  </annotation>
</semantics>
</math>

 (or of a linear combination of explanatory variables), then we express 

<math display="inline" id="Logistic_regression:8">
<semantics>
<mi>t</mi>
<annotation-xml encoding="MathML-Content">
<ci>t</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   t
  </annotation>
</semantics>
</math>

 as follows:</p>

<p>
<math display="block" id="Logistic_regression:9">
<semantics>
<mrow>
<mi>t</mi>
<mo>=</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<mi>x</mi>
</mrow>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<ci>t</ci>
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
<ci>x</ci>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   t=\beta_{0}+\beta_{1}x
  </annotation>
</semantics>
</math>
</p>

<p>And the logistic function can now be written as:</p>

<p>
<math display="block" id="Logistic_regression:10">
<semantics>
<mrow>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mfrac>
<mn>1</mn>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<mo>-</mo>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<mi>x</mi>
</mrow>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</msup>
</mrow>
</mfrac>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<times></times>
<ci>F</ci>
<ci>x</ci>
</apply>
<apply>
<divide></divide>
<cn type="integer">1</cn>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<minus></minus>
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
<ci>x</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   F(x)=\frac{1}{1+e^{-(\beta_{0}+\beta_{1}x)}}
  </annotation>
</semantics>
</math>
</p>

<p>Note that 

<math display="inline" id="Logistic_regression:11">
<semantics>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<times></times>
<ci>F</ci>
<ci>x</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   F(x)
  </annotation>
</semantics>
</math>

 is interpreted as the probability of the dependent variable equaling a "success" or "case" rather than a failure or non-case. It's clear that the <a href="Dependent_and_independent_variables" title="wikilink">response variables</a>
<math display="inline" id="Logistic_regression:12">
<semantics>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   Y_{i}
  </annotation>
</semantics>
</math>

 are not identically distributed

<math display="block" id="Logistic_regression:13">
<semantics>
<mrow>
<mi>P</mi>
<mrow>
<mo stretchy="false">(</mo>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mn>1</mn>
<mo>∣</mo>
<mi>X</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<csymbol cd="unknown">P</csymbol>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-(</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<eq></eq>
<cn type="integer">1</cn>
<ci>normal-∣</ci>
<csymbol cd="unknown">X</csymbol>
<ci>normal-)</ci>
</cerror>
</cerror>
</annotation-xml>
<annotation encoding="application/x-tex">
   P(Y_{i}=1\mid X)
  </annotation>
</semantics>
</math>

 differs from one data point 

<math display="inline" id="Logistic_regression:14">
<semantics>
<msub>
<mi>X</mi>
<mi>i</mi>
</msub>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>X</ci>
<ci>i</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   X_{i}
  </annotation>
</semantics>
</math>


 to another, though they are independent given <a href="design_matrix" title="wikilink">design matrix</a>
<math display="inline" id="Logistic_regression:15">
<semantics>
<mi>X</mi>
<annotation-xml encoding="MathML-Content">
<ci>X</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   X
  </annotation>
</semantics>
</math>

 and shared with parameters 

<math display="inline" id="Logistic_regression:16">
<semantics>
<mi>β</mi>
<annotation-xml encoding="MathML-Content">
<ci>β</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   \beta
  </annotation>
</semantics>
</math>

.<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a></p>
<h3 id="definition-of-the-inverse-of-the-logistic-function">Definition of the inverse of the logistic function</h3>

<p>We can now define the inverse of the logistic function, 

<math display="inline" id="Logistic_regression:17">
<semantics>
<mi>g</mi>
<annotation-xml encoding="MathML-Content">
<ci>g</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   g
  </annotation>
</semantics>
</math>

, the <a class="uri" href="logit" title="wikilink">logit</a> (log odds):</p>

<p>
<math display="block" id="Logistic_regression:18">
<semantics>
<mrow>
<mrow>
<mrow>
<mi>g</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mi>ln</mi>
<mfrac>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mrow>
<mn>1</mn>
<mo>-</mo>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
</mfrac>
</mrow>
<mo>=</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<mi>x</mi>
</mrow>
</mrow>
</mrow>
<mo>,</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<and></and>
<apply>
<eq></eq>
<apply>
<times></times>
<ci>g</ci>
<apply>
<times></times>
<ci>F</ci>
<ci>x</ci>
</apply>
</apply>
<apply>
<ln></ln>
<apply>
<divide></divide>
<apply>
<times></times>
<ci>F</ci>
<ci>x</ci>
</apply>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<times></times>
<ci>F</ci>
<ci>x</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
<ci>x</ci>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   g(F(x))=\ln\frac{F(x)}{1-F(x)}=\beta_{0}+\beta_{1}x,
  </annotation>
</semantics>
</math>
</p>

<p>and equivalently:</p>

<p>
<math display="block" id="Logistic_regression:19">
<semantics>
<mrow>
<mrow>
<mfrac>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mrow>
<mn>1</mn>
<mo>-</mo>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
</mfrac>
<mo>=</mo>
<msup>
<mi>e</mi>
<mrow>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<mi>x</mi>
</mrow>
</mrow>
</msup>
</mrow>
<mo>.</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<divide></divide>
<apply>
<times></times>
<ci>F</ci>
<ci>x</ci>
</apply>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<times></times>
<ci>F</ci>
<ci>x</ci>
</apply>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
<ci>x</ci>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \frac{F(x)}{1-F(x)}=e^{\beta_{0}+\beta_{1}x}.
  </annotation>
</semantics>
</math>
</p>
<h3 id="interpretation-of-these-terms">Interpretation of these terms</h3>

<p>In the above equations, the terms are as follows:</p>
<ul>
<li>
<math display="inline" id="Logistic_regression:20">
<semantics>
<mrow>
<mi>g</mi>
<mrow>
<mo stretchy="false">(</mo>
<mo>⋅</mo>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<times></times>
<ci>g</ci>
<ci>normal-⋅</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   g(\cdot)
  </annotation>
</semantics>
</math>

 refers to the logit function. The equation for 

<math display="inline" id="Logistic_regression:21">
<semantics>
<mrow>
<mi>g</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<times></times>
<ci>g</ci>
<apply>
<times></times>
<ci>F</ci>
<ci>x</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   g(F(x))
  </annotation>
</semantics>
</math>

 illustrates that the <a class="uri" href="logit" title="wikilink">logit</a> (i.e., log-odds or natural logarithm of the odds) is equivalent to the linear regression expression.</li>
<li>
<math display="inline" id="Logistic_regression:22">
<semantics>
<mi>ln</mi>
<annotation-xml encoding="MathML-Content">
<ln></ln>
</annotation-xml>
<annotation encoding="application/x-tex">
   \ln
  </annotation>
</semantics>
</math>

 denotes the natural logarithm.</li>
<li>
<math display="inline" id="Logistic_regression:23">
<semantics>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<times></times>
<ci>F</ci>
<ci>x</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   F(x)
  </annotation>
</semantics>
</math>

 is the probability that the dependent variable equals a case, given some linear combination 

<math display="inline" id="Logistic_regression:24">
<semantics>
<mi>x</mi>
<annotation-xml encoding="MathML-Content">
<ci>x</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   x
  </annotation>
</semantics>
</math>


 of the predictors. The formula for 

<math display="inline" id="Logistic_regression:25">
<semantics>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<times></times>
<ci>F</ci>
<ci>x</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   F(x)
  </annotation>
</semantics>
</math>

 illustrates that the probability of the dependent variable equaling a case is equal to the value of the logistic function of the linear regression expression. This is important in that it shows that the value of the linear regression expression can vary from negative to positive infinity and yet, after transformation, the resulting expression for the probability 

<math display="inline" id="Logistic_regression:26">
<semantics>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<times></times>
<ci>F</ci>
<ci>x</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   F(x)
  </annotation>
</semantics>
</math>

 ranges between 0 and 1.</li>
<li>
<math display="inline" id="Logistic_regression:27">
<semantics>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \beta_{0}
  </annotation>
</semantics>
</math>

 is the <a href="Y-intercept" title="wikilink">intercept</a> from the linear regression equation (the value of the criterion when the predictor is equal to zero).</li>
<li>
<math display="inline" id="Logistic_regression:28">
<semantics>
<mrow>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<mi>x</mi>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
<ci>x</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \beta_{1}x
  </annotation>
</semantics>
</math>

 is the regression coefficient multiplied by some value of the predictor.</li>
<li>base 

<math display="inline" id="Logistic_regression:29">
<semantics>
<mi>e</mi>
<annotation-xml encoding="MathML-Content">
<ci>e</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   e
  </annotation>
</semantics>
</math>


 denotes the exponential function.</li>
</ul>
<h3 id="definition-of-the-odds">Definition of the odds</h3>

<p>The odds of the dependent variable equaling a case (given some linear combination 

<math display="inline" id="Logistic_regression:30">
<semantics>
<mi>x</mi>
<annotation-xml encoding="MathML-Content">
<ci>x</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   x
  </annotation>
</semantics>
</math>

 of the predictors) is equivalent to the exponential function of the linear regression expression. This illustrates how the <a class="uri" href="logit" title="wikilink">logit</a> serves as a link function between the probability and the linear regression expression. Given that the logit ranges between negative and positive infinity, it provides an adequate criterion upon which to conduct linear regression and the logit is easily converted back into the odds.<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a></p>

<p>So we define odds of the dependent variable equaling a case (given some linear combination 

<math display="inline" id="Logistic_regression:31">
<semantics>
<mi>x</mi>
<annotation-xml encoding="MathML-Content">
<ci>x</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   x
  </annotation>
</semantics>
</math>

 of the predictors) as follows:</p>

<p>
<math display="block" id="Logistic_regression:32">
<semantics>
<mrow>
<mrow>
<mtext>odds</mtext>
<mo>=</mo>
<msup>
<mi>e</mi>
<mrow>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<mi>x</mi>
</mrow>
</mrow>
</msup>
</mrow>
<mo>.</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<mtext>odds</mtext>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
<ci>x</ci>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \text{odds}=e^{\beta_{0}+\beta_{1}x}.
  </annotation>
</semantics>
</math>
</p>
<h3 id="definition-of-the-odds-ratio">Definition of the odds ratio</h3>

<p>For a continuous independent variable the odds ratio can be defined as:</p>

<p>
<math display="block" id="Logistic_regression:33">
<semantics>
<mrow>
<mrow>
<mi>O</mi>
<mi>R</mi>
</mrow>
<mo>=</mo>
<mrow>
<mrow>
<mo>odds</mo>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mi>x</mi>
<mo>+</mo>
<mn>1</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>/</mo>
<mrow>
<mo>odds</mo>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<mo>=</mo>
<mfrac>
<mfrac>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mi>x</mi>
<mo>+</mo>
<mn>1</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mrow>
<mn>1</mn>
<mo>-</mo>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mi>x</mi>
<mo>+</mo>
<mn>1</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
</mfrac>
<mfrac>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mrow>
<mn>1</mn>
<mo>-</mo>
<mrow>
<mi>F</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
</mfrac>
</mfrac>
<mo>=</mo>
<mrow>
<msup>
<mi>e</mi>
<mrow>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mi>x</mi>
<mo>+</mo>
<mn>1</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
</msup>
<mo>/</mo>
<msup>
<mi>e</mi>
<mrow>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<mi>x</mi>
</mrow>
</mrow>
</msup>
</mrow>
<mo>=</mo>
<msup>
<mi>e</mi>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
</msup>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<and></and>
<apply>
<eq></eq>
<apply>
<times></times>
<ci>O</ci>
<ci>R</ci>
</apply>
<apply>
<divide></divide>
<apply>
<ci>odds</ci>
<apply>
<plus></plus>
<ci>x</ci>
<cn type="integer">1</cn>
</apply>
</apply>
<apply>
<ci>odds</ci>
<ci>x</ci>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<divide></divide>
<apply>
<divide></divide>
<apply>
<times></times>
<ci>F</ci>
<apply>
<plus></plus>
<ci>x</ci>
<cn type="integer">1</cn>
</apply>
</apply>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<times></times>
<ci>F</ci>
<apply>
<plus></plus>
<ci>x</ci>
<cn type="integer">1</cn>
</apply>
</apply>
</apply>
</apply>
<apply>
<divide></divide>
<apply>
<times></times>
<ci>F</ci>
<ci>x</ci>
</apply>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<times></times>
<ci>F</ci>
<ci>x</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<plus></plus>
<ci>x</ci>
<cn type="integer">1</cn>
</apply>
</apply>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
<ci>x</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   OR=\operatorname{odds}(x+1)/\operatorname{odds}(x)=\frac{\frac{F(x+1)}{1-F(x+1%
)}}{\frac{F(x)}{1-F(x)}}=e^{\beta_{0}+\beta_{1}(x+1)}/e^{\beta_{0}+\beta_{1}x}%
=e^{\beta_{1}}
  </annotation>
</semantics>
</math>
</p>

<p>This exponential relationship provides an interpretation for 

<math display="inline" id="Logistic_regression:34">
<semantics>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \beta_{1}
  </annotation>
</semantics>
</math>

: The odds multiply by 

<math display="inline" id="Logistic_regression:35">
<semantics>
<msup>
<mi>e</mi>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
</msup>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   e^{\beta_{1}}
  </annotation>
</semantics>
</math>

 for every 1-unit increase in x.<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a></p>

<p>For a binary independent variable the odds ratio is defined as 

<math display="inline" id="Logistic_regression:36">
<semantics>
<mfrac>
<mrow>
<mi>a</mi>
<mi>d</mi>
</mrow>
<mrow>
<mi>b</mi>
<mi>c</mi>
</mrow>
</mfrac>
<annotation-xml encoding="MathML-Content">
<apply>
<divide></divide>
<apply>
<times></times>
<ci>a</ci>
<ci>d</ci>
</apply>
<apply>
<times></times>
<ci>b</ci>
<ci>c</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \frac{ad}{bc}
  </annotation>
</semantics>
</math>

 where a, b, c and d are cells in a 2x2 <a href="contingency_table" title="wikilink">contingency table</a>.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a></p>
<h3 id="multiple-explanatory-variables">Multiple explanatory variables</h3>

<p>If there are multiple explanatory variables, the above expression 

<math display="inline" id="Logistic_regression:37">
<semantics>
<mrow>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<mi>x</mi>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
<ci>x</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \beta_{0}+\beta_{1}x
  </annotation>
</semantics>
</math>

 can be revised to 

<math display="inline" id="Logistic_regression:38">
<semantics>
<mrow>
<mrow>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<msub>
<mi>x</mi>
<mn>1</mn>
</msub>
</mrow>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>2</mn>
</msub>
<msub>
<mi>x</mi>
<mn>2</mn>
</msub>
</mrow>
<mo>+</mo>
<mi mathvariant="normal">⋯</mi>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mi>m</mi>
</msub>
<msub>
<mi>x</mi>
<mi>m</mi>
</msub>
</mrow>
</mrow>
<mo>.</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>x</ci>
<cn type="integer">1</cn>
</apply>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">2</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>x</ci>
<cn type="integer">2</cn>
</apply>
</apply>
<ci>normal-⋯</ci>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<ci>m</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>x</ci>
<ci>m</ci>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\cdots+\beta_{m}x_{m}.
  </annotation>
</semantics>
</math>

 Then when this is used in the equation relating the logged odds of a success to the values of the predictors, the linear regression will be a <a href="multiple_regression" title="wikilink">multiple regression</a> with <em>m</em> explanators; the parameters 

<math display="inline" id="Logistic_regression:39">
<semantics>
<msub>
<mi>β</mi>
<mi>j</mi>
</msub>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<ci>j</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \beta_{j}
  </annotation>
</semantics>
</math>

 for all <em>j</em> = 0, 1, 2, ..., <em>m</em> are all estimated.</p>
<h2 id="model-fitting">Model fitting</h2>
<h3 id="estimation">Estimation</h3>

<p>Because the model can be expressed as a <a href="generalized_linear_model" title="wikilink">generalized linear model</a> (see <a href="Logistic_regression#As_a_generalized_linear_model" title="wikilink">below</a>), for 0<p< p="">
<h4 id="maximum-likelihood-estimation">Maximum likelihood estimation</h4>

<p>The regression coefficients are usually estimated using <a href="maximum_likelihood" title="wikilink">maximum likelihood</a> estimation.<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a> Unlike linear regression with normally distributed residuals, it is not possible to find a closed-form expression for the coefficient values that maximize the likelihood function, so that an iterative process must be used instead; for example <a href="Newton's_method" title="wikilink">Newton's method</a>. This process begins with a tentative solution, revises it slightly to see if it can be improved, and repeats this revision until improvement is minute, at which point the process is said to have converged.<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a></p>

<p>In some instances the model may not reach convergence. Nonconvergence of a model indicates that the coefficients are not meaningful because the iterative process was unable to find appropriate solutions. A failure to converge may occur for a number of reasons: having a large ratio of predictors to cases, <a class="uri" href="multicollinearity" title="wikilink">multicollinearity</a>, <a href="sparse_matrix" title="wikilink">sparseness</a>, or complete separation.</p>
<ul>
<li>Having a large ratio of variables to cases results in an overly conservative Wald statistic (discussed below) and can lead to nonconvergence.</li>
<li>Multicollinearity refers to unacceptably high correlations between predictors. As multicollinearity increases, coefficients remain unbiased but standard errors increase and the likelihood of model convergence decreases.<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> To detect multicollinearity amongst the predictors, one can conduct a linear regression analysis with the predictors of interest for the sole purpose of examining the tolerance statistic <a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a> used to assess whether multicollinearity is unacceptably high.</li>
<li>Sparseness in the data refers to having a large proportion of empty cells (cells with zero counts). Zero cell counts are particularly problematic with categorical predictors. With continuous predictors, the model can infer values for the zero cell counts, but this is not the case with categorical predictors. The model will not converge with zero cell counts for categorical predictors because the natural logarithm of zero is an undefined value, so that final solutions to the model cannot be reached. To remedy this problem, researchers may collapse categories in a theoretically meaningful way or add a constant to all cells.<a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a></li>
<li>Another numerical problem that may lead to a lack of convergence is complete separation, which refers to the instance in which the predictors perfectly predict the criterion – all cases are accurately classified. In such instances, one should reexamine the data, as there is likely some kind of error.<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a></li>
</ul>

<p>As a general rule of thumb, logistic regression models require a minimum of about 10 events per explaining variable (where <em>event</em> denotes the cases belonging to the less frequent category in the dependent variable).<a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a></p>
<h4 id="minimum-chi-squared-estimator-for-grouped-data">Minimum chi-squared estimator for grouped data</h4>

<p>While individual data will have a dependent variable with a value of zero or one for every observation, with <a href="grouped_data" title="wikilink">grouped data</a> one observation is on a group of people who all share the same characteristics (e.g., demographic characteristics); in this case the researcher observes the proportion of people in the group for whom the response variable falls into one category or the other. If this proportion is neither zero nor one for any group, the minimum chi-squared estimator involves using <a href="weighted_least_squares" title="wikilink">weighted least squares</a> to estimate a linear model in which the dependent variable is the logit of the proportion: that is, the log of the ratio of the fraction in one group to the fraction in the other group.<a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a></p>
<h3 id="evaluating-goodness-of-fit">Evaluating goodness of fit</h3>

<p><a href="Goodness_of_fit" title="wikilink">Goodness of fit</a> in linear regression models is generally measured using the <a href="R_square" title="wikilink">R<sup>2</sup></a>. Since this has no direct analog in logistic regression, various methods<a class="footnoteRef" href="#fn32" id="fnref32"><sup>32</sup></a> including the following can be used instead.</p>
<h4 id="deviance-and-likelihood-ratio-tests">Deviance and likelihood ratio tests</h4>

<p>In linear regression analysis, one is concerned with partitioning variance via the <a href="Partition_of_sums_of_squares" title="wikilink">sum of squares</a> calculations – variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance. In logistic regression analysis, <a href="Deviance_(statistics)" title="wikilink">deviance</a> is used in lieu of sum of squares calculations.<a class="footnoteRef" href="#fn33" id="fnref33"><sup>33</sup></a> Deviance is analogous to the sum of squares calculations in linear regression<a class="footnoteRef" href="#fn34" id="fnref34"><sup>34</sup></a> and is a measure of the lack of fit to the data in a logistic regression model.<a class="footnoteRef" href="#fn35" id="fnref35"><sup>35</sup></a> When a "saturated" model is available (a model with a theoretically perfect fit), deviance is calculated by comparing a given model with the saturated model.<a class="footnoteRef" href="#fn36" id="fnref36"><sup>36</sup></a> This computation give the <a href="likelihood-ratio_test" title="wikilink">likelihood-ratio test</a>:.<a class="footnoteRef" href="#fn37" id="fnref37"><sup>37</sup></a></p>

<p>
<math display="block" id="Logistic_regression:40">
<semantics>
<mrow>
<mrow>
<mi>D</mi>
<mo>=</mo>
<mrow>
<mo>-</mo>
<mrow>
<mn>2</mn>
<mrow>
<mi>ln</mi>
<mfrac>
<mtext>likelihood of the fitted model</mtext>
<mtext>likelihood of the saturated model</mtext>
</mfrac>
</mrow>
</mrow>
</mrow>
</mrow>
<mo>.</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<ci>D</ci>
<apply>
<minus></minus>
<apply>
<times></times>
<cn type="integer">2</cn>
<apply>
<ln></ln>
<apply>
<divide></divide>
<mtext>likelihood of the fitted model</mtext>
<mtext>likelihood of the saturated model</mtext>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   D=-2\ln\frac{\text{likelihood of the fitted model}}{\text{likelihood of the %
saturated model}}.
  </annotation>
</semantics>
</math>
</p>

<p>In the above equation <em>D</em> represents the deviance and ln represents the natural logarithm. The log of the likelihood ratio (the ratio of the fitted model to the saturated model) will produce a negative value, so the product is multiplied by negative two times its natural logarithm to produce a value with an approximate <a href="chi-squared_distribution" title="wikilink">chi-squared distribution</a>.<a class="footnoteRef" href="#fn38" id="fnref38"><sup>38</sup></a> Smaller values indicate better fit as the fitted model deviates less from the saturated model. When assessed upon a chi-square distribution, nonsignificant chi-square values indicate very little unexplained variance and thus, good model fit. Conversely, a significant chi-square value indicates that a significant amount of the variance is unexplained.</p>

<p>When the saturated model is not available (a common case), deviance is calculated simply as (-2)x(log likelihood of the fitted model), and the reference to the saturated model's log likelihood can be removed from all that follows without harm.</p>

<p>Two measures of deviance are particularly important in logistic regression: null deviance and model deviance. The null deviance represents the difference between a model with only the intercept (which means "no predictors") and the saturated model. The model deviance represents the difference between a model with at least one predictor and the saturated model.<a class="footnoteRef" href="#fn39" id="fnref39"><sup>39</sup></a> In this respect, the null model provides a baseline upon which to compare predictor models. Given that deviance is a measure of the difference between a given model and the saturated model, smaller values indicate better fit. Thus, to assess the contribution of a predictor or set of predictors, one can subtract the model deviance from the null deviance and assess the difference on a 

<math display="inline" id="Logistic_regression:41">
<semantics>
<mrow>
<msubsup>
<mi>χ</mi>
<mrow>
<mi>s</mi>
<mo>-</mo>
<mi>p</mi>
</mrow>
<mn>2</mn>
</msubsup>
<mo>,</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>χ</ci>
<cn type="integer">2</cn>
</apply>
<apply>
<minus></minus>
<ci>s</ci>
<ci>p</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \chi^{2}_{s-p},
  </annotation>
</semantics>
</math>

 chi-square distribution with <a href="Degrees_of_freedom_(statistics)" title="wikilink">degrees of freedom</a><a class="footnoteRef" href="#fn40" id="fnref40"><sup>40</sup></a> equal to the difference in the number of parameters estimated.</p>

<p>Let</p>

<p>
<math display="block" id="Logistic_regression:42">
<semantics>
<mrow>
<msub>
<mi>D</mi>
<mtext>null</mtext>
</msub>
<mo>=</mo>
<mrow>
<mo>-</mo>
<mrow>
<mn>2</mn>
<mrow>
<mi>ln</mi>
<mfrac>
<mtext>likelihood of null model</mtext>
<mtext>likelihood of the saturated model</mtext>
</mfrac>
</mrow>
</mrow>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>D</ci>
<mtext>null</mtext>
</apply>
<apply>
<minus></minus>
<apply>
<times></times>
<cn type="integer">2</cn>
<apply>
<ln></ln>
<apply>
<divide></divide>
<mtext>likelihood of null model</mtext>
<mtext>likelihood of the saturated model</mtext>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   D_{\text{null}}=-2\ln\frac{\text{likelihood of null model}}{\text{likelihood %
of the saturated model}}
  </annotation>
</semantics>
</math>
</p>

<p>
<math display="block" id="Logistic_regression:43">
<semantics>
<mrow>
<mrow>
<msub>
<mi>D</mi>
<mtext>fitted</mtext>
</msub>
<mo>=</mo>
<mrow>
<mo>-</mo>
<mrow>
<mn>2</mn>
<mrow>
<mi>ln</mi>
<mfrac>
<mtext>likelihood of fitted model</mtext>
<mtext>likelihood of the saturated model</mtext>
</mfrac>
</mrow>
</mrow>
</mrow>
</mrow>
<mo>.</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>D</ci>
<mtext>fitted</mtext>
</apply>
<apply>
<minus></minus>
<apply>
<times></times>
<cn type="integer">2</cn>
<apply>
<ln></ln>
<apply>
<divide></divide>
<mtext>likelihood of fitted model</mtext>
<mtext>likelihood of the saturated model</mtext>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   D_{\text{fitted}}=-2\ln\frac{\text{likelihood of fitted model}}{\text{%
likelihood of the saturated model}}.
  </annotation>
</semantics>
</math>
</p>

<p>Then</p>

<p>
<math display="inline" id="Logistic_regression:44">
<semantics>
<mrow>
<msub>
<mi>D</mi>
<mtext>null</mtext>
</msub>
<mo>-</mo>
<msub>
<mi>D</mi>
<mtext>fitted</mtext>
</msub>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<minus></minus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>D</ci>
<mtext>null</mtext>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>D</ci>
<mtext>fitted</mtext>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \displaystyle D_{\text{null}}-D_{\text{fitted}}
  </annotation>
</semantics>
</math>
</p>

<p>If the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improved model fit. This is analogous to the <em>F</em>-test used in linear regression analysis to assess the significance of prediction.<a class="footnoteRef" href="#fn41" id="fnref41"><sup>41</sup></a></p>
<h4 id="pseudo-r2s">Pseudo-R<sup>2</sup>s</h4>

<p>In linear regression the squared multiple correlation, <em>R</em><sup>2</sup> is used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors.<a class="footnoteRef" href="#fn42" id="fnref42"><sup>42</sup></a> In logistic regression analysis, there is no agreed upon analogous measure, but there are several competing measures each with limitations.<a class="footnoteRef" href="#fn43" id="fnref43"><sup>43</sup></a> Three of the most commonly used indices are examined on this page beginning with the likelihood ratio <em>R</em><sup>2</sup>, <em>R</em><sup>2</sup><sub>L</sub>:<a class="footnoteRef" href="#fn44" id="fnref44"><sup>44</sup></a></p>

<p>
<math display="block" id="Logistic_regression:45">
<semantics>
<mrow>
<mrow>
<msubsup>
<mi>R</mi>
<mtext>L</mtext>
<mn>2</mn>
</msubsup>
<mo>=</mo>
<mfrac>
<mrow>
<msub>
<mi>D</mi>
<mtext>null</mtext>
</msub>
<mo>-</mo>
<msub>
<mi>D</mi>
<mtext>fitted</mtext>
</msub>
</mrow>
<msub>
<mi>D</mi>
<mtext>null</mtext>
</msub>
</mfrac>
</mrow>
<mo>.</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>R</ci>
<cn type="integer">2</cn>
</apply>
<mtext>L</mtext>
</apply>
<apply>
<divide></divide>
<apply>
<minus></minus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>D</ci>
<mtext>null</mtext>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>D</ci>
<mtext>fitted</mtext>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>D</ci>
<mtext>null</mtext>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   R^{2}_{\text{L}}=\frac{D_{\text{null}}-D_{\text{fitted}}}{D_{\text{null}}}.
  </annotation>
</semantics>
</math>
</p>

<p>This is the most analogous index to the squared multiple correlation in linear regression.<a class="footnoteRef" href="#fn45" id="fnref45"><sup>45</sup></a> It represents the proportional reduction in the deviance wherein the deviance is treated as a measure of variation analogous but not identical to the <a class="uri" href="variance" title="wikilink">variance</a> in <a href="linear_regression" title="wikilink">linear regression</a> analysis.<a class="footnoteRef" href="#fn46" id="fnref46"><sup>46</sup></a> One limitation of the likelihood ratio <em>R</em><sup>2</sup> is that it is not monotonically related to the odds ratio,<a class="footnoteRef" href="#fn47" id="fnref47"><sup>47</sup></a> meaning that it does not necessarily increase as the odds ratio increases and does not necessarily decrease as the odds ratio decreases.</p>

<p>The Cox and Snell <em>R</em><sup>2</sup> is an alternative index of goodness of fit related to the <em>R</em><sup>2</sup> value from linear regression. The Cox and Snell index is problematic as its maximum value is .75, when the <a class="uri" href="variance" title="wikilink">variance</a> is at its maximum (.25). The Nagelkerke <em>R</em><sup>2</sup> provides a correction to the Cox and Snell <em>R</em><sup>2</sup> so that the maximum value is equal to one. Nevertheless, the Cox and Snell and likelihood ratio <em>R</em><sup>2</sup>s show greater agreement with each other than either does with the Nagelkerke <em>R</em><sup>2</sup>.<a class="footnoteRef" href="#fn48" id="fnref48"><sup>48</sup></a> Of course, this might not be the case for values exceeding .75 as the Cox and Snell index is capped at this value. The likelihood ratio <em>R</em><sup>2</sup> is often preferred to the alternatives as it is most analogous to <em>R</em><sup>2</sup> in <a href="linear_regression" title="wikilink">linear regression</a>, is independent of the base rate (both Cox and Snell and Nagelkerke <em>R</em><sup>2</sup>s increase as the proportion of cases increase from 0 to .5) and varies between 0 and 1.</p>

<p>A word of caution is in order when interpreting pseudo-<em>R</em><sup>2</sup> statistics. The reason these indices of fit are referred to as <em>pseudo</em> <em>R</em><sup>2</sup> is that they do not represent the proportionate reduction in error as the <em>R</em><sup>2</sup> in <a href="linear_regression" title="wikilink">linear regression</a> does.<a class="footnoteRef" href="#fn49" id="fnref49"><sup>49</sup></a> Linear regression assumes <a class="uri" href="homoscedasticity" title="wikilink">homoscedasticity</a>, that the error variance is the same for all values of the criterion. Logistic regression will always be <a class="uri" href="heteroscedastic" title="wikilink">heteroscedastic</a> – the error variances differ for each value of the predicted score. For each value of the predicted score there would be a different value of the proportionate reduction in error. Therefore, it is inappropriate to think of <em>R</em><sup>2</sup> as a proportionate reduction in error in a universal sense in logistic regression.<a class="footnoteRef" href="#fn50" id="fnref50"><sup>50</sup></a></p>
<h4 id="hosmerlemeshow-test">Hosmer–Lemeshow test</h4>

<p>The <a href="Hosmer–Lemeshow_test" title="wikilink">Hosmer–Lemeshow test</a> uses a test statistic that asymptotically follows a <a href="chi-squared_distribution" title="wikilink">
<math display="inline" id="Logistic_regression:46">
<semantics>
<msup>
<mi>χ</mi>
<mn>2</mn>
</msup>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>χ</ci>
<cn type="integer">2</cn>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \chi^{2}
  </annotation>
</semantics>
</math>

 distribution</a> to assess whether or not the observed event rates match expected event rates in subgroups of the model population.</p>
<h4 id="evaluating-binary-classification-performance">Evaluating binary classification performance</h4>

<p>If the estimated probabilities are to be used to <a href="binary_classification" title="wikilink">classify</a> each observation of independent variable values as predicting the category that the dependent variable is found in, the various <a href="#Model_suitability" title="wikilink">methods below</a> for judging the model's suitability in out-of-sample forecasting can also be used on the data that were used for estimation—<a href="Accuracy_and_precision#In_binary_classification" title="wikilink">accuracy, precision</a> (also called <a href="positive_predictive_value" title="wikilink">positive predictive value</a>), <a href="Precision_and_recall" title="wikilink">recall</a> (also called <a href="Sensitivity_and_specificity" title="wikilink">sensitivity</a>), <a href="Sensitivity_and_specificity" title="wikilink">specificity</a> and <a href="negative_predictive_value" title="wikilink">negative predictive value</a>. In each of these evaluative methods, an aspect of the model's effectiveness in assigning instances to the correct categories is measured.</p>
<h2 id="coefficients">Coefficients</h2>

<p>After fitting the model, it is likely that researchers will want to examine the contribution of individual predictors. To do so, they will want to examine the regression coefficients. In linear regression, the regression coefficients represent the change in the criterion for each unit change in the predictor.<a class="footnoteRef" href="#fn51" id="fnref51"><sup>51</sup></a> In logistic regression, however, the regression coefficients represent the change in the logit for each unit change in the predictor. Given that the logit is not intuitive, researchers are likely to focus on a predictor's effect on the exponential function of the regression coefficient – the odds ratio (see <a href="logistic_regression#Definition" title="wikilink">definition</a>). In linear regression, the significance of a regression coefficient is assessed by computing a <em>t</em> test. In logistic regression, there are several different tests designed to assess the significance of an individual predictor, most notably the likelihood ratio test and the Wald statistic.</p>
<h3 id="likelihood-ratio-test">Likelihood ratio test</h3>

<p>The <a href="likelihood-ratio_test" title="wikilink">likelihood-ratio test</a> discussed above to assess model fit is also the recommended procedure to assess the contribution of individual "predictors" to a given model.<a class="footnoteRef" href="#fn52" id="fnref52"><sup>52</sup></a><a class="footnoteRef" href="#fn53" id="fnref53"><sup>53</sup></a><a class="footnoteRef" href="#fn54" id="fnref54"><sup>54</sup></a> In the case of a single predictor model, one simply compares the deviance of the predictor model with that of the null model on a chi-square distribution with a single degree of freedom. If the predictor model has a significantly smaller deviance (c.f chi-square using the difference in degrees of freedom of the two models), then one can conclude that there is a significant association between the "predictor" and the outcome. Although some common statistical packages (e.g. SPSS) do provide likelihood ratio test statistics, without this computationally intensive test it would be more difficult to assess the contribution of individual predictors in the multiple logistic regression case. To assess the contribution of individual predictors one can enter the predictors hierarchically, comparing each new model with the previous to determine the contribution of each predictor.<a class="footnoteRef" href="#fn55" id="fnref55"><sup>55</sup></a> There is some debate among statisticians about the appropriateness of so-called "stepwise" procedures. The fear is that they may not preserve nominal statistical properties and may become misleading.[<a class="uri" href="http://www.amazon.com/Regression-Modeling-Strategies-Applications-Statistics/dp/1441929185/ref=sr_1_2?ie=UTF8&amp;qid">http://www.amazon.com/Regression-Modeling-Strategies-Applications-Statistics/dp/1441929185/ref=sr_1_2?ie=UTF8&amp;qid;</a>;=1339171287&amp;sr;=8-2]</p>
<h3 id="wald-statistic">Wald statistic</h3>

<p>Alternatively, when assessing the contribution of individual predictors in a given model, one may examine the significance of the <a href="Wald_test" title="wikilink">Wald statistic</a>. The Wald statistic, analogous to the <em>t</em>-test in linear regression, is used to assess the significance of coefficients. The Wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as a chi-square distribution.<a class="footnoteRef" href="#fn56" id="fnref56"><sup>56</sup></a>
<math display="inline" id="Logistic_regression:47">
<semantics>
<mrow>
<msub>
<mi>W</mi>
<mi>j</mi>
</msub>
<mo>=</mo>
<mfrac>
<msubsup>
<mi>B</mi>
<mi>j</mi>
<mn>2</mn>
</msubsup>
<mrow>
<mi>S</mi>
<msubsup>
<mi>E</mi>
<msub>
<mi>B</mi>
<mi>j</mi>
</msub>
<mn>2</mn>
</msubsup>
</mrow>
</mfrac>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>W</ci>
<ci>j</ci>
</apply>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>B</ci>
<cn type="integer">2</cn>
</apply>
<ci>j</ci>
</apply>
<apply>
<times></times>
<ci>S</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>E</ci>
<cn type="integer">2</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>B</ci>
<ci>j</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   W_{j}=\frac{B^{2}_{j}}{SE^{2}_{B_{j}}}
  </annotation>
</semantics>
</math>
</p>

<p>Although several statistical packages (e.g., SPSS, SAS) report the Wald statistic to assess the contribution of individual predictors, the Wald statistic has limitations. When the regression coefficient is large, the standard error of the regression coefficient also tends to be large increasing the probability of <a href="Type_I_and_Type_II_errors" title="wikilink">Type-II error</a>. The Wald statistic also tends to be biased when data are sparse.<a class="footnoteRef" href="#fn57" id="fnref57"><sup>57</sup></a></p>
<h3 id="case-control-sampling">Case-control sampling</h3>

<p>Suppose cases are rare. Then we might wish to sample them more frequently than their prevalence in the population. For example, suppose there is a disease that affects 1 person in 10,000 and to collect our data we need to do a complete physical. It may be too expensive to do thousands of physicals of healthy people in order to obtain data for only a few diseased individuals. Thus, we may evaluate more diseased individuals. This is also called unbalanced data. As a rule of thumb, sampling controls at a rate of five times the number of cases will produce sufficient control data.<a class="footnoteRef" href="#fn58" id="fnref58"><sup>58</sup></a></p>

<p>If we form a logistic model from such data, if the model is correct, the 

<math display="inline" id="Logistic_regression:48">
<semantics>
<msub>
<mi>β</mi>
<mi>j</mi>
</msub>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<ci>j</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \beta_{j}
  </annotation>
</semantics>
</math>

 parameters are all correct except for 

<math display="inline" id="Logistic_regression:49">
<semantics>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \beta_{0}
  </annotation>
</semantics>
</math>

. We can correct 

<math display="inline" id="Logistic_regression:50">
<semantics>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \beta_{0}
  </annotation>
</semantics>
</math>

 if we know the true prevalence as follows:<a class="footnoteRef" href="#fn59" id="fnref59"><sup>59</sup></a>
<math display="inline" id="Logistic_regression:51">
<semantics>
<mrow>
<mover accent="true">
<msubsup>
<mi>β</mi>
<mn>0</mn>
<mo>*</mo>
</msubsup>
<mo stretchy="false">^</mo>
</mover>
<mo>=</mo>
<mrow>
<mrow>
<mover accent="true">
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo stretchy="false">^</mo>
</mover>
<mo>+</mo>
<mrow>
<mi>log</mi>
<mfrac>
<mi>π</mi>
<mrow>
<mn>1</mn>
<mo>-</mo>
<mi>π</mi>
</mrow>
</mfrac>
</mrow>
</mrow>
<mo>-</mo>
<mrow>
<mi>log</mi>
<mfrac>
<mover accent="true">
<mi>π</mi>
<mo stretchy="false">~</mo>
</mover>
<mrow>
<mn>1</mn>
<mo>-</mo>
<mover accent="true">
<mi>π</mi>
<mo stretchy="false">~</mo>
</mover>
</mrow>
</mfrac>
</mrow>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<ci>normal-^</ci>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<times></times>
</apply>
</apply>
<apply>
<minus></minus>
<apply>
<plus></plus>
<apply>
<ci>normal-^</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
</apply>
<apply>
<log></log>
<apply>
<divide></divide>
<ci>π</ci>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<ci>π</ci>
</apply>
</apply>
</apply>
</apply>
<apply>
<log></log>
<apply>
<divide></divide>
<apply>
<ci>normal-~</ci>
<ci>π</ci>
</apply>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<ci>normal-~</ci>
<ci>π</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \hat{\beta_{0}^{*}}=\hat{\beta_{0}}+\log{{\pi}\over{1-\pi}}-\log{{\tilde{\pi}}%
\over{1-\tilde{\pi}}}
  </annotation>
</semantics>
</math>
</p>

<p>where 

<math display="inline" id="Logistic_regression:52">
<semantics>
<mi>π</mi>
<annotation-xml encoding="MathML-Content">
<ci>π</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   \pi
  </annotation>
</semantics>
</math>

 is the true prevalence and 

<math display="inline" id="Logistic_regression:53">
<semantics>
<mover accent="true">
<mi>π</mi>
<mo stretchy="false">~</mo>
</mover>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>normal-~</ci>
<ci>π</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \tilde{\pi}
  </annotation>
</semantics>
</math>

 is the prevalence in the sample.</p>
<h2 id="formal-mathematical-specification">Formal mathematical specification</h2>

<p>There are various equivalent specifications of logistic regression, which fit into different types of more general models. These different specifications allow for different sorts of useful generalizations.</p>
<h3 id="setup">Setup</h3>

<p>The basic setup of logistic regression is the same as for standard <a href="linear_regression" title="wikilink">linear regression</a>.</p>

<p>It is assumed that we have a series of <em>N</em> observed data points. Each data point <em>i</em> consists of a set of <em>m</em> explanatory variables <em>x</em><sub>1,<em>i</em></sub> ... <em>x</em><sub><em>m,i</em></sub> (also called <a href="independent_variable" title="wikilink">independent variables</a>, predictor variables, input variables, features, or attributes), and an associated <a class="uri" href="binary-valued" title="wikilink">binary-valued</a> outcome variable <em>Y</em><sub><em>i</em></sub> (also known as a <a href="dependent_variable" title="wikilink">dependent variable</a>, response variable, output variable, outcome variable or class variable), i.e. it can assume only the two possible values 0 (often meaning "no" or "failure") or 1 (often meaning "yes" or "success"). The goal of logistic regression is to explain the relationship between the explanatory variables and the outcome, so that an outcome can be predicted for a new set of explanatory variables.</p>

<p>Some examples:</p>
<ul>
<li>The observed outcomes are the presence or absence of a given disease (e.g. diabetes) in a set of patients, and the explanatory variables might be characteristics of the patients thought to be pertinent (sex, race, age, <a href="blood_pressure" title="wikilink">blood pressure</a>, <a href="body-mass_index" title="wikilink">body-mass index</a>, etc.).</li>
<li>The observed outcomes are the votes (e.g. <a href="Democratic_Party_(United_States)" title="wikilink">Democratic</a> or <a href="Republican_Party_(United_States)" title="wikilink">Republican</a>) of a set of people in an election, and the explanatory variables are the demographic characteristics of each person (e.g. sex, race, age, income, etc.). In such a case, one of the two outcomes is arbitrarily coded as 1, and the other as 0.</li>
</ul>

<p>As in linear regression, the outcome variables <em>Y</em><sub><em>i</em></sub> are assumed to depend on the explanatory variables <em>x</em><sub>1,<em>i</em></sub> ... <em>x</em><sub><em>m,i</em></sub>.</p>
<dl>
<dt>Explanatory variables</dt>
</dl>

<p>As shown above in the above examples, the explanatory variables may be of any <a href="statistical_data_type" title="wikilink">type</a>: <a class="uri" href="real-valued" title="wikilink">real-valued</a>, <a href="binary_variable" title="wikilink">binary</a>, <a href="categorical_variable" title="wikilink">categorical</a>, etc. The main distinction is between <a href="continuous_variable" title="wikilink">continuous variables</a> (such as income, age and <a href="blood_pressure" title="wikilink">blood pressure</a>) and <a href="discrete_variable" title="wikilink">discrete variables</a> (such as sex or race). Discrete variables referring to more than two possible choices are typically coded using <a href="Dummy_variable_(statistics)" title="wikilink">dummy variables</a> (or <a href="indicator_variable" title="wikilink">indicator variables</a>), that is, separate explanatory variables taking the value 0 or 1 are created for each possible value of the discrete variable, with a 1 meaning "variable does have the given value" and a 0 meaning "variable does not have that value". For example, a four-way discrete variable of <a href="blood_type" title="wikilink">blood type</a> with the possible values "A, B, AB, O" can be converted to four separate two-way dummy variables, "is-A, is-B, is-AB, is-O", where only one of them has the value 1 and all the rest have the value 0. This allows for separate regression coefficients to be matched for each possible value of the discrete variable. (In a case like this, only three of the four dummy variables are independent of each other, in the sense that once the values of three of the variables are known, the fourth is automatically determined. Thus, it is necessary to encode only three of the four possibilities as dummy variables. This also means that when all four possibilities are encoded, the overall model is not <a class="uri" href="identifiable" title="wikilink">identifiable</a> in the absence of additional constraints such as a regularization constraint. Theoretically, this could cause problems, but in reality almost all logistic regression models are fitted with regularization constraints.)</p>
<dl>
<dt>Outcome variables</dt>
</dl>

<p>Formally, the outcomes <em>Y</em><sub><em>i</em></sub> are described as being <a href="Bernoulli_distribution" title="wikilink">Bernoulli-distributed</a> data, where each outcome is determined by an unobserved probability <em>p</em><sub><em>i</em></sub> that is specific to the outcome at hand, but related to the explanatory variables. This can be expressed in any of the following equivalent forms:</p>

<p>
<math display="inline" id="Logistic_regression:54">
<semantics>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>∣</mo>
<msub>
<mi>x</mi>
<mrow>
<mn>1</mn>
<mo>,</mo>
<mi>i</mi>
</mrow>
</msub>
<mo>,</mo>
<mi mathvariant="normal">…</mi>
<mo>,</mo>
<msub>
<mi>x</mi>
<mrow>
<mi>m</mi>
<mo>,</mo>
<mi>i</mi>
</mrow>
</msub>
</mrow>
<annotation-xml encoding="MathML-Content">
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<ci>normal-∣</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>x</ci>
<list>
<cn type="integer">1</cn>
<ci>i</ci>
</list>
</apply>
<ci>normal-,</ci>
<ci>normal-…</ci>
<ci>normal-,</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>x</ci>
<list>
<ci>m</ci>
<ci>i</ci>
</list>
</apply>
</cerror>
</annotation-xml>
<annotation encoding="application/x-tex">
   \displaystyle Y_{i}\mid x_{1,i},\ldots,x_{m,i}
  </annotation>
</semantics>
</math>
</p>

<p>The meanings of these four lines are:</p>
<ol>
<li>The first line expresses the <a href="probability_distribution" title="wikilink">probability distribution</a> of each <em>Y</em><sub><em>i</em></sub>: Conditioned on the explanatory variables, it follows a <a href="Bernoulli_distribution" title="wikilink">Bernoulli distribution</a> with parameters <em>p</em><sub><em>i</em></sub>, the probability of the outcome of 1 for trial <em>i</em>. As noted above, each separate trial has its own probability of success, just as each trial has its own explanatory variables. The probability of success <em>p</em><sub><em>i</em></sub> is not observed, only the outcome of an individual Bernoulli trial using that probability.</li>
<li>The second line expresses the fact that the <a href="expected_value" title="wikilink">expected value</a> of each <em>Y</em><sub><em>i</em></sub> is equal to the probability of success <em>p</em><sub><em>i</em></sub>, which is a general property of the Bernoulli distribution. In other words, if we run a large number of Bernoulli trials using the same probability of success <em>p</em><sub><em>i</em></sub>, then take the average of all the 1 and 0 outcomes, then the result would be close to <em>p</em><sub><em>i</em></sub>. This is because doing an average this way simply computes the proportion of successes seen, which we expect to converge to the underlying probability of success.</li>
<li>The third line writes out the <a href="probability_mass_function" title="wikilink">probability mass function</a> of the Bernoulli distribution, specifying the probability of seeing each of the two possible outcomes.</li>
<li>The fourth line is another way of writing the probability mass function, which avoids having to write separate cases and is more convenient for certain types of calculations. This relies on the fact that <em>Y</em><sub><em>i</em></sub> can take only the value 0 or 1. In each case, one of the exponents will be 1, "choosing" the value under it, while the other is 0, "canceling out" the value under it. Hence, the outcome is either <em>p</em><sub><em>i</em></sub> or 1 − <em>p</em><sub><em>i</em></sub>, as in the previous line.</li>
</ol>
<dl>
<dt>Linear predictor function</dt>
</dl>

<p>The basic idea of logistic regression is to use the mechanism already developed for <a href="linear_regression" title="wikilink">linear regression</a> by modeling the probability <em>p</em><sub><em>i</em></sub> using a <a href="linear_predictor_function" title="wikilink">linear predictor function</a>, i.e. a <a href="linear_combination" title="wikilink">linear combination</a> of the explanatory variables and a set of <a href="regression_coefficient" title="wikilink">regression coefficients</a> that are specific to the model at hand but the same for all trials. The linear predictor function 

<math display="inline" id="Logistic_regression:55">
<semantics>
<mrow>
<mi>f</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>i</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<times></times>
<ci>f</ci>
<ci>i</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   f(i)
  </annotation>
</semantics>
</math>

 for a particular data point <em>i</em> is written as:</p>

<p>
<math display="block" id="Logistic_regression:56">
<semantics>
<mrow>
<mrow>
<mrow>
<mi>f</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>i</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<msub>
<mi>x</mi>
<mrow>
<mn>1</mn>
<mo>,</mo>
<mi>i</mi>
</mrow>
</msub>
</mrow>
<mo>+</mo>
<mi mathvariant="normal">⋯</mi>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mi>m</mi>
</msub>
<msub>
<mi>x</mi>
<mrow>
<mi>m</mi>
<mo>,</mo>
<mi>i</mi>
</mrow>
</msub>
</mrow>
</mrow>
</mrow>
<mo>,</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<times></times>
<ci>f</ci>
<ci>i</ci>
</apply>
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>x</ci>
<list>
<cn type="integer">1</cn>
<ci>i</ci>
</list>
</apply>
</apply>
<ci>normal-⋯</ci>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<ci>m</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>x</ci>
<list>
<ci>m</ci>
<ci>i</ci>
</list>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   f(i)=\beta_{0}+\beta_{1}x_{1,i}+\cdots+\beta_{m}x_{m,i},
  </annotation>
</semantics>
</math>
</p>

<p>where 

<math display="inline" id="Logistic_regression:57">
<semantics>
<mrow>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo>,</mo>
<mi mathvariant="normal">…</mi>
<mo>,</mo>
<msub>
<mi>β</mi>
<mi>m</mi>
</msub>
</mrow>
<annotation-xml encoding="MathML-Content">
<list>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<ci>normal-…</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<ci>m</ci>
</apply>
</list>
</annotation-xml>
<annotation encoding="application/x-tex">
   \beta_{0},\ldots,\beta_{m}
  </annotation>
</semantics>
</math>

 are <a href="regression_coefficient" title="wikilink">regression coefficients</a> indicating the relative effect of a particular explanatory variable on the outcome.</p>

<p>The model is usually put into a more compact form as follows:</p>
<ul>
<li>The regression coefficients <em>β</em><sub>0</sub>, <em>β</em><sub>1</sub>, ..., <em>β</em><sub><em>m</em></sub> are grouped into a single vector <strong><em>β</em></strong> of size <em>m</em> + 1.</li>
<li>For each data point <em>i</em>, an additional explanatory pseudo-variable <em>x</em><sub>0,<em>i</em></sub> is added, with a fixed value of 1, corresponding to the <a href="Y-intercept" title="wikilink">intercept</a> coefficient <em>β</em><sub>0</sub>.</li>
<li>The resulting explanatory variables <em>x</em><sub>0,<em>i</em></sub>, <em>x</em><sub>1,<em>i</em></sub>, ..., <em>x</em><sub><em>m,i</em></sub> are then grouped into a single vector <strong><em>X<sub>i</sub></em></strong> of size <em>m</em> + 1.</li>
</ul>

<p>This makes it possible to write the linear predictor function as follows:</p>

<p>
<math display="block" id="Logistic_regression:58">
<semantics>
<mrow>
<mrow>
<mrow>
<mi>f</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>i</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</mrow>
<mo>,</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<times></times>
<ci>f</ci>
<ci>i</ci>
</apply>
<apply>
<ci>normal-⋅</ci>
<ci>𝜷</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   f(i)=\boldsymbol{\beta}\cdot\mathbf{X}_{i},
  </annotation>
</semantics>
</math>
</p>

<p>using the notation for a <a href="dot_product" title="wikilink">dot product</a> between two vectors.</p>
<h3 id="as-a-generalized-linear-model">As a generalized linear model</h3>

<p>The particular model used by logistic regression, which distinguishes it from standard <a href="linear_regression" title="wikilink">linear regression</a> and from other types of <a href="regression_analysis" title="wikilink">regression analysis</a> used for <a class="uri" href="binary-valued" title="wikilink">binary-valued</a> outcomes, is the way the probability of a particular outcome is linked to the linear predictor function:</p>

<p>
<math display="block" id="Logistic_regression:59">
<semantics>
<mrow>
<mo>logit</mo>
<mrow>
<mo stretchy="false">(</mo>
<mi>𝔼</mi>
<mrow>
<mo stretchy="false">[</mo>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>∣</mo>
<msub>
<mi>x</mi>
<mrow>
<mn>1</mn>
<mo>,</mo>
<mi>i</mi>
</mrow>
</msub>
<mo>,</mo>
<mi mathvariant="normal">…</mi>
<mo>,</mo>
<msub>
<mi>x</mi>
<mrow>
<mi>m</mi>
<mo>,</mo>
<mi>i</mi>
</mrow>
</msub>
<mo stretchy="false">]</mo>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
<mo>=</mo>
<mo>logit</mo>
<mrow>
<mo stretchy="false">(</mo>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
<mo stretchy="false">)</mo>
</mrow>
<mo>=</mo>
<mi>ln</mi>
<mrow>
<mo>(</mo>
<mfrac>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
<mrow>
<mn>1</mn>
<mo>-</mo>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
</mrow>
</mfrac>
<mo>)</mo>
</mrow>
<mo>=</mo>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo>+</mo>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<msub>
<mi>x</mi>
<mrow>
<mn>1</mn>
<mo>,</mo>
<mi>i</mi>
</mrow>
</msub>
<mo>+</mo>
<mi mathvariant="normal">⋯</mi>
<mo>+</mo>
<msub>
<mi>β</mi>
<mi>m</mi>
</msub>
<msub>
<mi>x</mi>
<mrow>
<mi>m</mi>
<mo>,</mo>
<mi>i</mi>
</mrow>
</msub>
</mrow>
<annotation-xml encoding="MathML-Content">
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>logit</ci>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-(</ci>
<csymbol cd="unknown">E</csymbol>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-[</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<ci>normal-∣</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>x</ci>
<list>
<cn type="integer">1</cn>
<ci>i</ci>
</list>
</apply>
<ci>normal-,</ci>
<ci>normal-…</ci>
<ci>normal-,</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>x</ci>
<list>
<ci>m</ci>
<ci>i</ci>
</list>
</apply>
<ci>normal-]</ci>
</cerror>
<ci>normal-)</ci>
</cerror>
<eq></eq>
<ci>logit</ci>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-(</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
<ci>normal-)</ci>
</cerror>
<eq></eq>
<ln></ln>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-(</ci>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
<ci>normal-)</ci>
</cerror>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>x</ci>
<list>
<cn type="integer">1</cn>
<ci>i</ci>
</list>
</apply>
<plus></plus>
<ci>normal-⋯</ci>
<plus></plus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<ci>m</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>x</ci>
<list>
<ci>m</ci>
<ci>i</ci>
</list>
</apply>
</cerror>
</annotation-xml>
<annotation encoding="application/x-tex">
   \operatorname{logit}(\mathbb{E}[Y_{i}\mid x_{1,i},\ldots,x_{m,i}])=%
\operatorname{logit}(p_{i})=\ln\left(\frac{p_{i}}{1-p_{i}}\right)=\beta_{0}+%
\beta_{1}x_{1,i}+\cdots+\beta_{m}x_{m,i}
  </annotation>
</semantics>
</math>
</p>

<p>Written using the more compact notation described above, this is:</p>

<p>
<math display="block" id="Logistic_regression:60">
<semantics>
<mrow>
<mo>logit</mo>
<mrow>
<mo stretchy="false">(</mo>
<mi>𝔼</mi>
<mrow>
<mo stretchy="false">[</mo>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>∣</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
<mo stretchy="false">]</mo>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
<mo>=</mo>
<mo>logit</mo>
<mrow>
<mo stretchy="false">(</mo>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
<mo stretchy="false">)</mo>
</mrow>
<mo>=</mo>
<mi>ln</mi>
<mrow>
<mo>(</mo>
<mfrac>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
<mrow>
<mn>1</mn>
<mo>-</mo>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
</mrow>
</mfrac>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
<annotation-xml encoding="MathML-Content">
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>logit</ci>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-(</ci>
<csymbol cd="unknown">E</csymbol>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-[</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<ci>normal-∣</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
<ci>normal-]</ci>
</cerror>
<ci>normal-)</ci>
</cerror>
<eq></eq>
<ci>logit</ci>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-(</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
<ci>normal-)</ci>
</cerror>
<eq></eq>
<ln></ln>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-(</ci>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
<ci>normal-)</ci>
</cerror>
<eq></eq>
<csymbol cd="unknown">β</csymbol>
<ci>normal-⋅</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</cerror>
</annotation-xml>
<annotation encoding="application/x-tex">
   \operatorname{logit}(\mathbb{E}[Y_{i}\mid\mathbf{X}_{i}])=\operatorname{logit}%
(p_{i})=\ln\left(\frac{p_{i}}{1-p_{i}}\right)=\boldsymbol{\beta}\cdot\mathbf{X%
}_{i}
  </annotation>
</semantics>
</math>
</p>

<p>This formulation expresses logistic regression as a type of <a href="generalized_linear_model" title="wikilink">generalized linear model</a>, which predicts variables with various types of <a href="probability_distribution" title="wikilink">probability distributions</a> by fitting a linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable.</p>

<p>The intuition for transforming using the logit function (the natural log of the odds) was explained above. It also has the practical effect of converting the probability (which is bounded to be between 0 and 1) to a variable that ranges over 

<math display="inline" id="Logistic_regression:61">
<semantics>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mo>-</mo>
<mi mathvariant="normal">∞</mi>
</mrow>
<mo>,</mo>
<mrow>
<mo>+</mo>
<mi mathvariant="normal">∞</mi>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<interval closure="open">
<apply>
<minus></minus>
<infinity></infinity>
</apply>
<apply>
<plus></plus>
<infinity></infinity>
</apply>
</interval>
</annotation-xml>
<annotation encoding="application/x-tex">
   (-\infty,+\infty)
  </annotation>
</semantics>
</math>

 — thereby matching the potential range of the linear prediction function on the right side of the equation.</p>

<p>Note that both the probabilities <em>p</em><sub><em>i</em></sub> and the regression coefficients are unobserved, and the means of determining them is not part of the model itself. They are typically determined by some sort of optimization procedure, e.g. <a href="maximum_likelihood_estimation" title="wikilink">maximum likelihood estimation</a>, that finds values that best fit the observed data (i.e. that give the most accurate predictions for the data already observed), usually subject to <a href="regularization_(mathematics)" title="wikilink">regularization</a> conditions that seek to exclude unlikely values, e.g. extremely large values for any of the regression coefficients. The use of a regularization condition is equivalent to doing <a href="maximum_a_posteriori" title="wikilink">maximum a posteriori</a> (MAP) estimation, an extension of maximum likelihood. (Regularization is most commonly done using <a href="Ridge_regression" title="wikilink">a squared regularizing function</a>, which is equivalent to placing a zero-mean <a href="Gaussian_distribution" title="wikilink">Gaussian</a> <a href="prior_distribution" title="wikilink">prior distribution</a> on the coefficients, but other regularizers are also possible.) Whether or not regularization is used, it is usually not possible to find a closed-form solution; instead, an iterative numerical method must be used, such as <a href="iteratively_reweighted_least_squares" title="wikilink">iteratively reweighted least squares</a> (IRLS) or, more commonly these days, a <a href="quasi-Newton_method" title="wikilink">quasi-Newton method</a> such as the <a href="L-BFGS" title="wikilink">L-BFGS method</a>.</p>

<p>The interpretation of the <em>β</em><sub><em>j</em></sub> parameter estimates is as the additive effect on the log of the <a class="uri" href="odds" title="wikilink">odds</a> for a unit change in the <em>j</em>th explanatory variable. In the case of a dichotomous explanatory variable, for instance gender, 

<math display="inline" id="Logistic_regression:62">
<semantics>
<msup>
<mi>e</mi>
<mi>β</mi>
</msup>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<ci>β</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   e^{\beta}
  </annotation>
</semantics>
</math>

 is the estimate of the odds of having the outcome for, say, males compared with females.</p>

<p>An equivalent formula uses the inverse of the logit function, which is the <a href="logistic_function" title="wikilink">logistic function</a>, i.e.:</p>

<p>
<math display="block" id="Logistic_regression:63">
<semantics>
<mrow>
<mi>𝔼</mi>
<mrow>
<mo stretchy="false">[</mo>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>∣</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
<mo stretchy="false">]</mo>
</mrow>
<mo>=</mo>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<msup>
<mo>logit</mo>
<mrow>
<mo>-</mo>
<mn>1</mn>
</mrow>
</msup>
<mrow>
<mo stretchy="false">(</mo>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
<mo stretchy="false">)</mo>
</mrow>
<mo>=</mo>
<mfrac>
<mn>1</mn>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<mo>-</mo>
<mrow>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</mrow>
</msup>
</mrow>
</mfrac>
</mrow>
<annotation-xml encoding="MathML-Content">
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<csymbol cd="unknown">E</csymbol>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-[</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<ci>normal-∣</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
<ci>normal-]</ci>
</cerror>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>logit</ci>
<apply>
<minus></minus>
<cn type="integer">1</cn>
</apply>
</apply>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-(</ci>
<csymbol cd="unknown">β</csymbol>
<ci>normal-⋅</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
<ci>normal-)</ci>
</cerror>
<eq></eq>
<apply>
<divide></divide>
<cn type="integer">1</cn>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<minus></minus>
<apply>
<ci>normal-⋅</ci>
<ci>𝜷</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</cerror>
</annotation-xml>
<annotation encoding="application/x-tex">
   \mathbb{E}[Y_{i}\mid\mathbf{X}_{i}]=p_{i}=\operatorname{logit}^{-1}(%
\boldsymbol{\beta}\cdot\mathbf{X}_{i})=\frac{1}{1+e^{-\boldsymbol{\beta}\cdot%
\mathbf{X}_{i}}}
  </annotation>
</semantics>
</math>
</p>

<p>The formula can also be written as a <a href="probability_distribution" title="wikilink">probability distribution</a> (specifically, using a <a href="probability_mass_function" title="wikilink">probability mass function</a>):</p>

<p>
<math display="block" id="Logistic_regression:64">
<semantics>
<mrow>
<mrow>
<mo>Pr</mo>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<msub>
<mi>y</mi>
<mi>i</mi>
</msub>
</mrow>
<mo>∣</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mmultiscripts>
<mi>p</mi>
<mi>i</mi>
<none></none>
<none></none>
<msub>
<mi>y</mi>
<mi>i</mi>
</msub>
</mmultiscripts>
<msup>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mn>1</mn>
<mo>-</mo>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
<mrow>
<mn>1</mn>
<mo>-</mo>
<msub>
<mi>y</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
</mrow>
<mo>=</mo>
<mrow>
<msup>
<mrow>
<mo>(</mo>
<mfrac>
<msup>
<mi>e</mi>
<mrow>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
</mrow>
</mfrac>
<mo>)</mo>
</mrow>
<msub>
<mi>y</mi>
<mi>i</mi>
</msub>
</msup>
<msup>
<mrow>
<mo>(</mo>
<mrow>
<mn>1</mn>
<mo>-</mo>
<mfrac>
<msup>
<mi>e</mi>
<mrow>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
</mrow>
</mfrac>
</mrow>
<mo>)</mo>
</mrow>
<mrow>
<mn>1</mn>
<mo>-</mo>
<msub>
<mi>y</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
</mrow>
<mo>=</mo>
<mfrac>
<msup>
<mi>e</mi>
<mrow>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
<mo>⋅</mo>
<msub>
<mi>y</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
</mrow>
</mfrac>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<and></and>
<apply>
<eq></eq>
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>y</ci>
<ci>i</ci>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>y</ci>
<ci>i</ci>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
</apply>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>y</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<ci>𝜷</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<ci>𝜷</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>y</ci>
<ci>i</ci>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<ci>𝜷</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<ci>𝜷</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>y</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<ci>𝜷</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>y</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<ci>𝜷</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \operatorname{Pr}(Y_{i}=y_{i}\mid\mathbf{X}_{i})={p_{i}}^{y_{i}}(1-p_{i})^{1-y%
_{i}}=\left(\frac{e^{\boldsymbol{\beta}\cdot\mathbf{X}_{i}}}{1+e^{\boldsymbol{%
\beta}\cdot\mathbf{X}_{i}}}\right)^{y_{i}}\left(1-\frac{e^{\boldsymbol{\beta}%
\cdot\mathbf{X}_{i}}}{1+e^{\boldsymbol{\beta}\cdot\mathbf{X}_{i}}}\right)^{1-y%
_{i}}=\frac{e^{\boldsymbol{\beta}\cdot\mathbf{X}_{i}\cdot y_{i}}}{1+e^{%
\boldsymbol{\beta}\cdot\mathbf{X}_{i}}}
  </annotation>
</semantics>
</math>
</p>
<h3 id="as-a-latent-variable-model">As a latent-variable model</h3>

<p>The above model has an equivalent formulation as a <a href="latent-variable_model" title="wikilink">latent-variable model</a>. This formulation is common in the theory of <a href="discrete_choice" title="wikilink">discrete choice</a> models, and makes it easier to extend to certain more complicated models with multiple, correlated choices, as well as to compare logistic regression to the closely related <a href="probit_model" title="wikilink">probit model</a>.</p>

<p>Imagine that, for each trial <em>i</em>, there is a continuous <a href="latent_variable" title="wikilink">latent variable</a> <em>Y</em><sub><em>i</em></sub><sup>*</sup> (i.e. an unobserved <a href="random_variable" title="wikilink">random variable</a>) that is distributed as follows:</p>

<p>
<math display="block" id="Logistic_regression:65">
<semantics>
<mrow>
<msubsup>
<mi>Y</mi>
<mi>i</mi>
<mo>∗</mo>
</msubsup>
<mo>=</mo>
<mrow>
<mrow>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
<mo>+</mo>
<mpadded width="+1.7pt">
<mi>ε</mi>
</mpadded>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<ci>normal-∗</ci>
</apply>
<apply>
<plus></plus>
<apply>
<ci>normal-⋅</ci>
<ci>𝜷</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
<ci>ε</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   Y_{i}^{\ast}=\boldsymbol{\beta}\cdot\mathbf{X}_{i}+\varepsilon\,
  </annotation>
</semantics>
</math>

 where</p>

<p>
<math display="block" id="Logistic_regression:66">
<semantics>
<mrow>
<mi>ε</mi>
<mo>∼</mo>
<mrow>
<mo>Logistic</mo>
<mrow>
<mo stretchy="false">(</mo>
<mn>0</mn>
<mo>,</mo>
<mn>1</mn>
<mo rspace="4.2pt" stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="latexml">similar-to</csymbol>
<ci>ε</ci>
<apply>
<ci>Logistic</ci>
<cn type="integer">0</cn>
<cn type="integer">1</cn>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \varepsilon\sim\operatorname{Logistic}(0,1)\,
  </annotation>
</semantics>
</math>

 i.e. the latent variable can be written directly in terms of the linear predictor function and an additive random <a href="error_variable" title="wikilink">error variable</a> that is distributed according to a standard <a href="logistic_distribution" title="wikilink">logistic distribution</a>.</p>

<p>Then <em>Y</em><sub><em>i</em></sub> can be viewed as an indicator for whether this latent variable is positive:</p>

<p>
<math display="block" id="Logistic_regression:67">
<semantics>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mrow>
<mo>{</mo>
<mtable displaystyle="true">
<mtr>
<mtd columnalign="left">
<mn>1</mn>
</mtd>
<mtd columnalign="left">
<mrow>
<mrow>
<mrow>
<mtext>if</mtext>
<msubsup>
<mi>Y</mi>
<mi>i</mi>
<mo>∗</mo>
</msubsup>
</mrow>
<mo>&gt;</mo>
<mrow>
<mrow>
<mpadded width="+5pt">
<mn>0</mn>
</mpadded>
<mtext>i.e.</mtext>
</mrow>
<mo>-</mo>
<mi>ε</mi>
</mrow>
<mo>&lt;</mo>
<mrow>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</mrow>
<mo>,</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd columnalign="left">
<mn>0</mn>
</mtd>
<mtd columnalign="left">
<mtext>otherwise.</mtext>
</mtd>
</mtr>
</mtable>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="latexml">cases</csymbol>
<cn type="integer">1</cn>
<apply>
<and></and>
<apply>
<gt></gt>
<apply>
<times></times>
<mtext>if</mtext>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<ci>normal-∗</ci>
</apply>
</apply>
<apply>
<minus></minus>
<apply>
<times></times>
<cn type="integer">0</cn>
<mtext>i.e.</mtext>
</apply>
<ci>ε</ci>
</apply>
</apply>
<apply>
<lt></lt>
<share href="#.cmml">
</share>
<apply>
<ci>normal-⋅</ci>
<ci>𝜷</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
<cn type="integer">0</cn>
<mtext>otherwise.</mtext>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   Y_{i}=\begin{cases}1&amp;\text{if }Y_{i}^{\ast}&gt;0\ \text{ i.e. }-\varepsilon&lt;%
\boldsymbol{\beta}\cdot\mathbf{X}_{i},\\
0&amp;\text{otherwise.}\end{cases}
  </annotation>
</semantics>
</math>
</p>

<p>The choice of modeling the error variable specifically with a standard logistic distribution, rather than a general logistic distribution with the location and scale set to arbitrary values, seems restrictive, but in fact it is not. It must be kept in mind that we can choose the regression coefficients ourselves, and very often can use them to offset changes in the parameters of the error variable's distribution. For example, a logistic error-variable distribution with a non-zero location parameter <em>μ</em> (which sets the mean) is equivalent to a distribution with a zero location parameter, where <em>μ</em> has been added to the intercept coefficient. Both situations produce the same value for <em>Y</em><sub><em>i</em></sub><sup>*</sup> regardless of settings of explanatory variables. Similarly, an arbitrary scale parameter <em>s</em> is equivalent to setting the scale parameter to 1 and then dividing all regression coefficients by <em>s</em>. In the latter case, the resulting value of <em>Y</em><sub><em>i</em></sub><sup><em>*</em></sup> will be smaller by a factor of <em>s</em> than in the former case, for all sets of explanatory variables — but critically, it will always remain on the same side of 0, and hence lead to the same <em>Y</em><sub><em>i</em></sub> choice.</p>

<p>(Note that this predicts that the irrelevancy of the scale parameter may not carry over into more complex models where more than two choices are available.)</p>

<p>It turns out that this formulation is exactly equivalent to the preceding one, phrased in terms of the <a href="generalized_linear_model" title="wikilink">generalized linear model</a> and without any <a href="latent_variable" title="wikilink">latent variables</a>. This can be shown as follows, using the fact that the <a href="cumulative_distribution_function" title="wikilink">cumulative distribution function</a> (CDF) of the standard <a href="logistic_distribution" title="wikilink">logistic distribution</a> is the <a href="logistic_function" title="wikilink">logistic function</a>, which is the inverse of the <a href="logit_function" title="wikilink">logit function</a>, i.e.</p>

<p>
<math display="block" id="Logistic_regression:68">
<semantics>
<mrow>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mi>ε</mi>
<mo>&lt;</mo>
<mi>x</mi>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<msup>
<mo>logit</mo>
<mrow>
<mo>-</mo>
<mn>1</mn>
</mrow>
</msup>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<ci>Pr</ci>
<apply>
<lt></lt>
<ci>ε</ci>
<ci>x</ci>
</apply>
</apply>
<apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>logit</ci>
<apply>
<minus></minus>
<cn type="integer">1</cn>
</apply>
</apply>
<ci>x</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Pr(\varepsilon<x)=\operatorname{logit}^{-1}(x) <="" annotation="">
</x)=\operatorname{logit}^{-1}(x)></annotation></semantics>
</math>
</p>

<p>Then:</p>

<p>
<math display="inline" id="Logistic_regression:69">
<semantics>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mo>∣</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \displaystyle\Pr(Y_{i}=1\mid\mathbf{X}_{i})
  </annotation>
</semantics>
</math>
</p>

<p>This formulation—which is standard in <a href="discrete_choice" title="wikilink">discrete choice</a> models—makes clear the relationship between logistic regression (the "logit model") and the <a href="probit_model" title="wikilink">probit model</a>, which uses an error variable distributed according to a standard <a href="normal_distribution" title="wikilink">normal distribution</a> instead of a standard logistic distribution. Both the logistic and normal distributions are symmetric with a basic unimodal, "bell curve" shape. The only difference is that the logistic distribution has somewhat <a href="heavy-tailed_distribution" title="wikilink">heavier tails</a>, which means that it is less sensitive to outlying data (and hence somewhat more <a href="robust_statistics" title="wikilink">robust</a> to model mis-specifications or erroneous data).</p>
<h3 id="as-a-two-way-latent-variable-model">As a two-way latent-variable model</h3>

<p>Yet another formulation uses two separate latent variables:</p>

<p>
<math display="inline" id="Logistic_regression:70">
<semantics>
<msubsup>
<mi>Y</mi>
<mi>i</mi>
<mrow>
<mn>0</mn>
<mo>∗</mo>
</mrow>
</msubsup>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<list>
<cn type="integer">0</cn>
<ci>normal-∗</ci>
</list>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \displaystyle Y_{i}^{0\ast}
  </annotation>
</semantics>
</math>
</p>

<p>where</p>

<p>
<math display="inline" id="Logistic_regression:71">
<semantics>
<msub>
<mi>ε</mi>
<mn>0</mn>
</msub>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>ε</ci>
<cn type="integer">0</cn>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \displaystyle\varepsilon_{0}
  </annotation>
</semantics>
</math>
</p>

<p>where <em>EV</em><sub>1</sub>(0,1) is a standard type-1 <a href="extreme_value_distribution" title="wikilink">extreme value distribution</a>: i.e.</p>

<p>
<math display="block" id="Logistic_regression:72">
<semantics>
<mrow>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>ε</mi>
<mn>0</mn>
</msub>
<mo>=</mo>
<mi>x</mi>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>ε</mi>
<mn>1</mn>
</msub>
<mo>=</mo>
<mi>x</mi>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<msup>
<mi>e</mi>
<mrow>
<mo>-</mo>
<mi>x</mi>
</mrow>
</msup>
<msup>
<mi>e</mi>
<mrow>
<mo>-</mo>
<msup>
<mi>e</mi>
<mrow>
<mo>-</mo>
<mi>x</mi>
</mrow>
</msup>
</mrow>
</msup>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<and></and>
<apply>
<eq></eq>
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>ε</ci>
<cn type="integer">0</cn>
</apply>
<ci>x</ci>
</apply>
</apply>
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>ε</ci>
<cn type="integer">1</cn>
</apply>
<ci>x</ci>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<minus></minus>
<ci>x</ci>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<minus></minus>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<minus></minus>
<ci>x</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Pr(\varepsilon_{0}=x)=\Pr(\varepsilon_{1}=x)=e^{-x}e^{-e^{-x}}
  </annotation>
</semantics>
</math>
</p>

<p>Then</p>

<p>
<math display="block" id="Logistic_regression:73">
<semantics>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mrow>
<mo>{</mo>
<mtable displaystyle="true">
<mtr>
<mtd columnalign="left">
<mn>1</mn>
</mtd>
<mtd columnalign="left">
<mrow>
<mrow>
<mrow>
<mtext>if</mtext>
<msubsup>
<mi>Y</mi>
<mi>i</mi>
<mrow>
<mn>1</mn>
<mo>∗</mo>
</mrow>
</msubsup>
</mrow>
<mo>&gt;</mo>
<msubsup>
<mi>Y</mi>
<mi>i</mi>
<mrow>
<mn>0</mn>
<mo>∗</mo>
</mrow>
</msubsup>
</mrow>
<mo>,</mo>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd columnalign="left">
<mn>0</mn>
</mtd>
<mtd columnalign="left">
<mtext>otherwise.</mtext>
</mtd>
</mtr>
</mtable>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="latexml">cases</csymbol>
<cn type="integer">1</cn>
<apply>
<gt></gt>
<apply>
<times></times>
<mtext>if</mtext>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<list>
<cn type="integer">1</cn>
<ci>normal-∗</ci>
</list>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<list>
<cn type="integer">0</cn>
<ci>normal-∗</ci>
</list>
</apply>
</apply>
<cn type="integer">0</cn>
<mtext>otherwise.</mtext>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   Y_{i}=\begin{cases}1&amp;\text{if }Y_{i}^{1\ast}&gt;Y_{i}^{0\ast},\\
0&amp;\text{otherwise.}\end{cases}
  </annotation>
</semantics>
</math>
</p>

<p>This model has a separate latent variable and a separate set of regression coefficients for each possible outcome of the dependent variable. The reason for this separation is that it makes it easy to extend logistic regression to multi-outcome categorical variables, as in the <a href="multinomial_logit" title="wikilink">multinomial logit</a> model. In such a model, it is natural to model each possible outcome using a different set of regression coefficients. It is also possible to motivate each of the separate latent variables as the theoretical <a class="uri" href="utility" title="wikilink">utility</a> associated with making the associated choice, and thus motivate logistic regression in terms of <a href="utility_theory" title="wikilink">utility theory</a>. (In terms of utility theory, a rational actor always chooses the choice with the greatest associated utility.) This is the approach taken by economists when formulating <a href="discrete_choice" title="wikilink">discrete choice</a> models, because it both provides a theoretically strong foundation and facilitates intuitions about the model, which in turn makes it easy to consider various sorts of extensions. (See the example below.)</p>

<p>The choice of the type-1 <a href="extreme_value_distribution" title="wikilink">extreme value distribution</a> seems fairly arbitrary, but it makes the mathematics work out, and it may be possible to justify its use through <a href="rational_choice_theory" title="wikilink">rational choice theory</a>.</p>

<p>It turns out that this model is equivalent to the previous model, although this seems non-obvious, since there are now two sets of regression coefficients and error variables, and the error variables have a different distribution. In fact, this model reduces directly to the previous one with the following substitutions:</p>

<p>
<math display="block" id="Logistic_regression:74">
<semantics>
<mrow>
<mi>𝜷</mi>
<mo>=</mo>
<mrow>
<msub>
<mi>𝜷</mi>
<mn>1</mn>
</msub>
<mo>-</mo>
<msub>
<mi>𝜷</mi>
<mn>0</mn>
</msub>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<ci>𝜷</ci>
<apply>
<minus></minus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<cn type="integer">0</cn>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \boldsymbol{\beta}=\boldsymbol{\beta}_{1}-\boldsymbol{\beta}_{0}
  </annotation>
</semantics>
</math>
</p>

<p>
<math display="block" id="Logistic_regression:75">
<semantics>
<mrow>
<mi>ε</mi>
<mo>=</mo>
<mrow>
<msub>
<mi>ε</mi>
<mn>1</mn>
</msub>
<mo>-</mo>
<msub>
<mi>ε</mi>
<mn>0</mn>
</msub>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<ci>ε</ci>
<apply>
<minus></minus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>ε</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>ε</ci>
<cn type="integer">0</cn>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \varepsilon=\varepsilon_{1}-\varepsilon_{0}
  </annotation>
</semantics>
</math>

 An intuition for this comes from the fact that, since we choose based on the maximum of two values, only their difference matters, not the exact values — and this effectively removes one <a href="Degrees_of_freedom_(statistics)" title="wikilink">degree of freedom</a>. Another critical fact is that the difference of two type-1 extreme-value-distributed variables is a logistic distribution, i.e. if 

<math display="inline" id="Logistic_regression:76">
<semantics>
<mrow>
<mrow>
<mi>ε</mi>
<mo>=</mo>
<mrow>
<msub>
<mi>ε</mi>
<mn>1</mn>
</msub>
<mo>-</mo>
<msub>
<mi>ε</mi>
<mn>0</mn>
</msub>
</mrow>
<mo>∼</mo>
<mrow>
<mo>Logistic</mo>
<mrow>
<mo stretchy="false">(</mo>
<mn>0</mn>
<mo>,</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<mo>.</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<and></and>
<apply>
<eq></eq>
<ci>ε</ci>
<apply>
<minus></minus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>ε</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>ε</ci>
<cn type="integer">0</cn>
</apply>
</apply>
</apply>
<apply>
<csymbol cd="latexml">similar-to</csymbol>
<share href="#.cmml">
</share>
<apply>
<ci>Logistic</ci>
<cn type="integer">0</cn>
<cn type="integer">1</cn>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \varepsilon=\varepsilon_{1}-\varepsilon_{0}\sim\operatorname{Logistic}(0,1).
  </annotation>
</semantics>
</math>
</p>

<p>We can demonstrate the equivalent as follows:</p>

<p>
<math display="inline" id="Logistic_regression:77">
<semantics>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mo>∣</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \displaystyle\Pr(Y_{i}=1\mid\mathbf{X}_{i})
  </annotation>
</semantics>
</math>
</p>
<h4 id="example">Example</h4>

<p>As an example, consider a province-level election where the choice is between a right-of-center party, a left-of-center party, and a secessionist party (e.g. the <a href="Parti_Québécois" title="wikilink">Parti Québécois</a>, which wants <a class="uri" href="Quebec" title="wikilink">Quebec</a> to secede from <a class="uri" href="Canada" title="wikilink">Canada</a>). We would then use three latent variables, one for each choice. Then, in accordance with <a href="utility_theory" title="wikilink">utility theory</a>, we can then interpret the latent variables as expressing the <a class="uri" href="utility" title="wikilink">utility</a> that results from making each of the choices. We can also interpret the regression coefficients as indicating the strength that the associated factor (i.e. explanatory variable) has in contributing to the utility — or more correctly, the amount by which a unit change in an explanatory variable changes the utility of a given choice. A voter might expect that the right-of-center party would lower taxes, especially on rich people. This would give low-income people no benefit, i.e. no change in utility (since they usually don't pay taxes); would cause moderate benefit (i.e. somewhat more money, or moderate utility increase) for middle-incoming people; and would cause significant benefits for high-income people. On the other hand, the left-of-center party might be expected to raise taxes and offset it with increased welfare and other assistance for the lower and middle classes. This would cause significant positive benefit to low-income people, perhaps weak benefit to middle-income people, and significant negative benefit to high-income people. Finally, the secessionist party would take no direct actions on the economy, but simply secede. A low-income or middle-income voter might expect basically no clear utility gain or loss from this, but a high-income voter might expect negative utility, since he/she is likely to own companies, which will have a harder time doing business in such an environment and probably lose money.</p>

<p>These intuitions can be expressed as follows:</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">

<p>Estimated strength of regression coefficient for different outcomes (party choices) and different values of explanatory variables</p></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">

<p>High-income</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">

<p>Middle-income</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">

<p>Low-income</p></td>
</tr>
<tr class="even">
</tr>
</tbody>
</table>

<p>This clearly shows that</p>
<ol>
<li>Separate sets of regression coefficients need to exist for each choice. When phrased in terms of utility, this can be seen very easily. Different choices have different effects on net utility; furthermore, the effects vary in complex ways that depend on the characteristics of each individual, so there need to be separate sets of coefficients for each characteristic, not simply a single extra per-choice characteristic.</li>
<li>Even though income is a continuous variable, its effect on utility is too complex for it to be treated as a single variable. Either it needs to be directly split up into ranges, or higher powers of income need to be added so that <a href="polynomial_regression" title="wikilink">polynomial regression</a> on income is effectively done.</li>
</ol>
<h3 id="as-a-log-linear-model">As a "log-linear" model</h3>

<p>Yet another formulation combines the two-way latent variable formulation above with the original formulation higher up without latent variables, and in the process provides a link to one of the standard formulations of the <a href="multinomial_logit" title="wikilink">multinomial logit</a>.</p>

<p>Here, instead of writing the <a class="uri" href="logit" title="wikilink">logit</a> of the probabilities <em>p</em><sub><em>i</em></sub> as a linear predictor, we separate the linear predictor into two, one for each of the two outcomes:</p>

<p>
<math display="inline" id="Logistic_regression:78">
<semantics>
<mrow>
<mi>ln</mi>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mn>0</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ln></ln>
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<cn type="integer">0</cn>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \displaystyle\ln\Pr(Y_{i}=0)
  </annotation>
</semantics>
</math>
</p>

<p>Note that two separate sets of regression coefficients have been introduced, just as in the two-way latent variable model, and the two equations appear a form that writes the <a class="uri" href="logarithm" title="wikilink">logarithm</a> of the associated probability as a linear predictor, with an extra term 

<math display="inline" id="Logistic_regression:79">
<semantics>
<mrow>
<mo>-</mo>
<mrow>
<mi>l</mi>
<mi>n</mi>
<mi>Z</mi>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<minus></minus>
<apply>
<times></times>
<ci>l</ci>
<ci>n</ci>
<ci>Z</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   -lnZ
  </annotation>
</semantics>
</math>

 at the end. This term, as it turns out, serves as the <a href="normalizing_factor" title="wikilink">normalizing factor</a> ensuring that the result is a distribution. This can be seen by exponentiating both sides:</p>

<p>
<math display="inline" id="Logistic_regression:80">
<semantics>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mn>0</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<cn type="integer">0</cn>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \displaystyle\Pr(Y_{i}=0)
  </annotation>
</semantics>
</math>
</p>

<p>In this form it is clear that the purpose of <em>Z</em> is to ensure that the resulting distribution over <em>Y</em><sub><em>i</em></sub> is in fact a <a href="probability_distribution" title="wikilink">probability distribution</a>, i.e. it sums to 1. This means that <em>Z</em> is simply the sum of all un-normalized probabilities, and by dividing each probability by <em>Z</em>, the probabilities become "<a href="normalizing_constant" title="wikilink">normalized</a>". That is:</p>

<p>
<math display="block" id="Logistic_regression:81">
<semantics>
<mrow>
<mi>Z</mi>
<mo>=</mo>
<mrow>
<msup>
<mi>e</mi>
<mrow>
<msub>
<mi>𝜷</mi>
<mn>0</mn>
</msub>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<msub>
<mi>𝜷</mi>
<mn>1</mn>
</msub>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<ci>Z</ci>
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   Z=e^{\boldsymbol{\beta}_{0}\cdot\mathbf{X}_{i}}+e^{\boldsymbol{\beta}_{1}\cdot%
\mathbf{X}_{i}}
  </annotation>
</semantics>
</math>
</p>

<p>and the resulting equations are</p>

<p>
<math display="inline" id="Logistic_regression:82">
<semantics>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mn>0</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<cn type="integer">0</cn>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \displaystyle\Pr(Y_{i}=0)
  </annotation>
</semantics>
</math>
</p>

<p>Or generally:</p>

<p>
<math display="block" id="Logistic_regression:83">
<semantics>
<mrow>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mi>c</mi>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mfrac>
<msup>
<mi>e</mi>
<mrow>
<msub>
<mi>𝜷</mi>
<mi>c</mi>
</msub>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
<mrow>
<msub>
<mo largeop="true" symmetric="true">∑</mo>
<mi>h</mi>
</msub>
<msup>
<mi>e</mi>
<mrow>
<msub>
<mi>𝜷</mi>
<mi>h</mi>
</msub>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
</mrow>
</mfrac>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<ci>c</ci>
</apply>
</apply>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<ci>c</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
<apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<sum></sum>
<ci>h</ci>
</apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<ci>h</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Pr(Y_{i}=c)=\frac{e^{\boldsymbol{\beta}_{c}\cdot\mathbf{X}_{i}}}{\sum_{h}e^{%
\boldsymbol{\beta}_{h}\cdot\mathbf{X}_{i}}}
  </annotation>
</semantics>
</math>
</p>

<p>This shows clearly how to generalize this formulation to more than two outcomes, as in <a href="multinomial_logit" title="wikilink">multinomial logit</a>. Note that this general formulation is exactly the <a href="Softmax_function" title="wikilink">Softmax function</a> as in</p>

<p>
<math display="block" id="Logistic_regression:84">
<semantics>
<mrow>
<mrow>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mi>c</mi>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mo>softmax</mo>
<mrow>
<mo stretchy="false">(</mo>
<mi>c</mi>
<mo>,</mo>
<mrow>
<msub>
<mi>𝜷</mi>
<mn>0</mn>
</msub>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
<mo>,</mo>
<mrow>
<msub>
<mi>𝜷</mi>
<mn>1</mn>
</msub>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
<mo>,</mo>
<mi mathvariant="normal">…</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<mo>.</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<ci>c</ci>
</apply>
</apply>
<apply>
<ci>softmax</ci>
<ci>c</ci>
<apply>
<ci>normal-⋅</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
<apply>
<ci>normal-⋅</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
<ci>normal-…</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Pr(Y_{i}=c)=\operatorname{softmax}(c,\boldsymbol{\beta}_{0}\cdot\mathbf{X}_{i%
},\boldsymbol{\beta}_{1}\cdot\mathbf{X}_{i},\dots).
  </annotation>
</semantics>
</math>
</p>

<p>In order to prove that this is equivalent to the previous model, note that the above model is overspecified, in that 

<math display="inline" id="Logistic_regression:85">
<semantics>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mn>0</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<cn type="integer">0</cn>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Pr(Y_{i}=0)
  </annotation>
</semantics>
</math>

 and 

<math display="inline" id="Logistic_regression:86">
<semantics>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<cn type="integer">1</cn>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Pr(Y_{i}=1)
  </annotation>
</semantics>
</math>

 cannot be independently specified: rather 

<math display="inline" id="Logistic_regression:87">
<semantics>
<mrow>
<mrow>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mn>0</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>+</mo>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<mo>=</mo>
<mn>1</mn>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<plus></plus>
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<cn type="integer">0</cn>
</apply>
</apply>
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<cn type="integer">1</cn>
</apply>
</apply>
</apply>
<cn type="integer">1</cn>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Pr(Y_{i}=0)+\Pr(Y_{i}=1)=1
  </annotation>
</semantics>
</math>

 so knowing one automatically determines the other. As a result, the model is <a class="uri" href="nonidentifiable" title="wikilink">nonidentifiable</a>, in that multiple combinations of <strong><em>β</em></strong><sub>0</sub> and <strong><em>β</em></strong><sub>1</sub> will produce the same probabilities for all possible explanatory variables. In fact, it can be seen that adding any constant vector to both of them will produce the same probabilities:</p>

<p>
<math display="inline" id="Logistic_regression:88">
<semantics>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<cn type="integer">1</cn>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \displaystyle\Pr(Y_{i}=1)
  </annotation>
</semantics>
</math>
</p>

<p>As a result, we can simplify matters, and restore identifiability, by picking an arbitrary value for one of the two vectors. We choose to set 

<math display="inline" id="Logistic_regression:89">
<semantics>
<mrow>
<msub>
<mi>𝜷</mi>
<mn>0</mn>
</msub>
<mo>=</mo>
<mn>0.</mn>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<cn type="integer">0</cn>
</apply>
<cn type="float">0.</cn>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \boldsymbol{\beta}_{0}=\mathbf{0}.
  </annotation>
</semantics>
</math>

 Then,</p>

<p>
<math display="block" id="Logistic_regression:90">
<semantics>
<mrow>
<msup>
<mi>e</mi>
<mrow>
<msub>
<mi>𝜷</mi>
<mn>0</mn>
</msub>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
<mo>=</mo>
<msup>
<mi>e</mi>
<mrow>
<mn>𝟎</mn>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
<mo>=</mo>
<mn>1</mn>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<and></and>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<cn type="integer">0</cn>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<cn type="integer">1</cn>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   e^{\boldsymbol{\beta}_{0}\cdot\mathbf{X}_{i}}=e^{\mathbf{0}\cdot\mathbf{X}_{i}%
}=1
  </annotation>
</semantics>
</math>
</p>

<p>and so</p>

<p>
<math display="block" id="Logistic_regression:91">
<semantics>
<mrow>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mfrac>
<msup>
<mi>e</mi>
<mrow>
<msub>
<mi>𝜷</mi>
<mn>1</mn>
</msub>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<msub>
<mi>𝜷</mi>
<mn>1</mn>
</msub>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
</mrow>
</mfrac>
<mo>=</mo>
<mfrac>
<mn>1</mn>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<mo>-</mo>
<mrow>
<msub>
<mi>𝜷</mi>
<mn>1</mn>
</msub>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</mrow>
</msup>
</mrow>
</mfrac>
<mo>=</mo>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<and></and>
<apply>
<eq></eq>
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<cn type="integer">1</cn>
</apply>
</apply>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<ci>normal-⋅</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<divide></divide>
<cn type="integer">1</cn>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<minus></minus>
<apply>
<ci>normal-⋅</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Pr(Y_{i}=1)=\frac{e^{\boldsymbol{\beta}_{1}\cdot\mathbf{X}_{i}}}{1+e^{%
\boldsymbol{\beta}_{1}\cdot\mathbf{X}_{i}}}=\frac{1}{1+e^{-\boldsymbol{\beta}_%
{1}\cdot\mathbf{X}_{i}}}=p_{i}
  </annotation>
</semantics>
</math>
</p>

<p>which shows that this formulation is indeed equivalent to the previous formulation. (As in the two-way latent variable formulation, any settings where 

<math display="inline" id="Logistic_regression:92">
<semantics>
<mrow>
<mi>𝜷</mi>
<mo>=</mo>
<mrow>
<msub>
<mi>𝜷</mi>
<mn>1</mn>
</msub>
<mo>-</mo>
<msub>
<mi>𝜷</mi>
<mn>0</mn>
</msub>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<ci>𝜷</ci>
<apply>
<minus></minus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝜷</ci>
<cn type="integer">0</cn>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \boldsymbol{\beta}=\boldsymbol{\beta}_{1}-\boldsymbol{\beta}_{0}
  </annotation>
</semantics>
</math>

 will produce equivalent results.)</p>

<p>Note that most treatments of the <a href="multinomial_logit" title="wikilink">multinomial logit</a> model start out either by extending the "log-linear" formulation presented here or the two-way latent variable formulation presented above, since both clearly show the way that the model could be extended to multi-way outcomes. In general, the presentation with latent variables is more common in <a class="uri" href="econometrics" title="wikilink">econometrics</a> and <a href="political_science" title="wikilink">political science</a>, where <a href="discrete_choice" title="wikilink">discrete choice</a> models and <a href="utility_theory" title="wikilink">utility theory</a> reign, while the "log-linear" formulation here is more common in <a href="computer_science" title="wikilink">computer science</a>, e.g. <a href="machine_learning" title="wikilink">machine learning</a> and <a href="natural_language_processing" title="wikilink">natural language processing</a>.</p>
<h3 id="as-a-single-layer-perceptron">As a single-layer perceptron</h3>

<p>The model has an equivalent formulation</p>

<p>
<math display="block" id="Logistic_regression:93">
<semantics>
<mrow>
<mrow>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mfrac>
<mn>1</mn>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<mo>-</mo>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>0</mn>
</msub>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mn>1</mn>
</msub>
<msub>
<mi>x</mi>
<mrow>
<mn>1</mn>
<mo>,</mo>
<mi>i</mi>
</mrow>
</msub>
</mrow>
<mo>+</mo>
<mi mathvariant="normal">⋯</mi>
<mo>+</mo>
<mrow>
<msub>
<mi>β</mi>
<mi>k</mi>
</msub>
<msub>
<mi>x</mi>
<mrow>
<mi>k</mi>
<mo>,</mo>
<mi>i</mi>
</mrow>
</msub>
</mrow>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</msup>
</mrow>
</mfrac>
</mrow>
<mo>.</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
<apply>
<divide></divide>
<cn type="integer">1</cn>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<minus></minus>
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">0</cn>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<cn type="integer">1</cn>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>x</ci>
<list>
<cn type="integer">1</cn>
<ci>i</ci>
</list>
</apply>
</apply>
<ci>normal-⋯</ci>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>β</ci>
<ci>k</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>x</ci>
<list>
<ci>k</ci>
<ci>i</ci>
</list>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   p_{i}=\frac{1}{1+e^{-(\beta_{0}+\beta_{1}x_{1,i}+\cdots+\beta_{k}x_{k,i})}}.\,
  </annotation>
</semantics>
</math>
</p>

<p>This functional form is commonly called a single-layer <a class="uri" href="perceptron" title="wikilink">perceptron</a> or single-layer <a href="artificial_neural_network" title="wikilink">artificial neural network</a>. A single-layer neural network computes a continuous output instead of a <a href="step_function" title="wikilink">step function</a>. The derivative of <em>p<sub>i</sub></em> with respect to <em>X</em> = (<em>x</em><sub>1</sub>, ..., <em>x</em><sub><em>k</em></sub>) is computed from the general form:</p>

<p>
<math display="block" id="Logistic_regression:94">
<semantics>
<mrow>
<mi>y</mi>
<mo>=</mo>
<mfrac>
<mn>1</mn>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<mo>-</mo>
<mrow>
<mi>f</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>X</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
</msup>
</mrow>
</mfrac>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<ci>y</ci>
<apply>
<divide></divide>
<cn type="integer">1</cn>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<minus></minus>
<apply>
<times></times>
<ci>f</ci>
<ci>X</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   y=\frac{1}{1+e^{-f(X)}}
  </annotation>
</semantics>
</math>
</p>

<p>where <em>f</em>(<em>X</em>) is an <a href="analytic_function" title="wikilink">analytic function</a> in <em>X</em>. With this choice, the single-layer neural network is identical to the logistic regression model. This function has a continuous derivative, which allows it to be used in <a class="uri" href="backpropagation" title="wikilink">backpropagation</a>. This function is also preferred because its derivative is easily calculated:</p>

<p>
<math display="block" id="Logistic_regression:95">
<semantics>
<mrow>
<mrow>
<mfrac>
<mrow>
<mi mathvariant="normal">d</mi>
<mi>y</mi>
</mrow>
<mrow>
<mi mathvariant="normal">d</mi>
<mi>X</mi>
</mrow>
</mfrac>
<mo>=</mo>
<mrow>
<mi>y</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mn>1</mn>
<mo>-</mo>
<mi>y</mi>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
<mfrac>
<mrow>
<mi mathvariant="normal">d</mi>
<mi>f</mi>
</mrow>
<mrow>
<mi mathvariant="normal">d</mi>
<mi>X</mi>
</mrow>
</mfrac>
</mrow>
</mrow>
<mo>.</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<divide></divide>
<apply>
<times></times>
<ci>normal-d</ci>
<ci>y</ci>
</apply>
<apply>
<times></times>
<ci>normal-d</ci>
<ci>X</ci>
</apply>
</apply>
<apply>
<times></times>
<ci>y</ci>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<ci>y</ci>
</apply>
<apply>
<divide></divide>
<apply>
<times></times>
<ci>normal-d</ci>
<ci>f</ci>
</apply>
<apply>
<times></times>
<ci>normal-d</ci>
<ci>X</ci>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \frac{\mathrm{d}y}{\mathrm{d}X}=y(1-y)\frac{\mathrm{d}f}{\mathrm{d}X}.\,
  </annotation>
</semantics>
</math>
</p>
<h3 id="in-terms-of-binomial-data">In terms of binomial data</h3>

<p>A closely related model assumes that each <em>i</em> is associated not with a single Bernoulli trial but with <em>n</em><sub><em>i</em></sub> <a href="independent_identically_distributed" title="wikilink">independent identically distributed</a> trials, where the observation <em>Y</em><sub><em>i</em></sub> is the number of successes observed (the sum of the individual Bernoulli-distributed random variables), and hence follows a <a href="binomial_distribution" title="wikilink">binomial distribution</a>:</p>

<p>
<math display="block" id="Logistic_regression:96">
<semantics>
<mrow>
<mrow>
<mpadded width="+5pt">
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
</mpadded>
<mo>∼</mo>
<mrow>
<mo>Bin</mo>
<mrow>
<mo stretchy="false">(</mo>
<msub>
<mi>n</mi>
<mi>i</mi>
</msub>
<mo>,</mo>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<mo>,</mo>
<mrow>
<mrow>
<mtext>for</mtext>
<mi>i</mi>
</mrow>
<mo>=</mo>
<mrow>
<mn>1</mn>
<mo>,</mo>
<mi mathvariant="normal">…</mi>
<mo>,</mo>
<mi>n</mi>
</mrow>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<csymbol cd="ambiguous">formulae-sequence</csymbol>
<apply>
<csymbol cd="latexml">similar-to</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<apply>
<ci>Bin</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>n</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<apply>
<times></times>
<mtext>for</mtext>
<ci>i</ci>
</apply>
<list>
<cn type="integer">1</cn>
<ci>normal-…</ci>
<ci>n</ci>
</list>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   Y_{i}\ \sim\operatorname{Bin}(n_{i},p_{i}),\text{ for }i=1,\dots,n
  </annotation>
</semantics>
</math>
</p>

<p>An example of this distribution is the fraction of seeds (<em>p</em><sub><em>i</em></sub>) that germinate after <em>n</em><sub><em>i</em></sub> are planted.</p>

<p>In terms of <a href="expected_value" title="wikilink">expected values</a>, this model is expressed as follows:</p>

<p>
<math display="block" id="Logistic_regression:97">
<semantics>
<mrow>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<mi>𝔼</mi>
<mrow>
<mo>[</mo>
<mpadded width="+1.7pt">
<mfrac>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<msub>
<mi>n</mi>
<mi>i</mi>
</msub>
</mfrac>
</mpadded>
<mo rspace="4.2pt">|</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
<mo>]</mo>
</mrow>
<mo>,</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
<eq></eq>
<csymbol cd="unknown">E</csymbol>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-[</ci>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>n</ci>
<ci>i</ci>
</apply>
</apply>
<ci>normal-|</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
<ci>normal-]</ci>
</cerror>
<ci>normal-,</ci>
</cerror>
</annotation-xml>
<annotation encoding="application/x-tex">
   p_{i}=\mathbb{E}\left[\left.\frac{Y_{i}}{n_{i}}\,\right|\,\mathbf{X}_{i}\right],
  </annotation>
</semantics>
</math>
</p>

<p>so that</p>

<p>
<math display="block" id="Logistic_regression:98">
<semantics>
<mrow>
<mo>logit</mo>
<mrow>
<mo>(</mo>
<mi>𝔼</mi>
<mrow>
<mo>[</mo>
<mpadded width="+1.7pt">
<mfrac>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<msub>
<mi>n</mi>
<mi>i</mi>
</msub>
</mfrac>
</mpadded>
<mo rspace="4.2pt">|</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
<mo>]</mo>
</mrow>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mo>logit</mo>
<mrow>
<mo stretchy="false">(</mo>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
<mo stretchy="false">)</mo>
</mrow>
<mo>=</mo>
<mi>ln</mi>
<mrow>
<mo>(</mo>
<mfrac>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
<mrow>
<mn>1</mn>
<mo>-</mo>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
</mrow>
</mfrac>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
<mo>,</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>logit</ci>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-(</ci>
<csymbol cd="unknown">E</csymbol>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-[</ci>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>n</ci>
<ci>i</ci>
</apply>
</apply>
<ci>normal-|</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
<ci>normal-]</ci>
</cerror>
<ci>normal-)</ci>
</cerror>
<eq></eq>
<ci>logit</ci>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-(</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
<ci>normal-)</ci>
</cerror>
<eq></eq>
<ln></ln>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-(</ci>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
<ci>normal-)</ci>
</cerror>
<eq></eq>
<csymbol cd="unknown">β</csymbol>
<ci>normal-⋅</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
<ci>normal-,</ci>
</cerror>
</annotation-xml>
<annotation encoding="application/x-tex">
   \operatorname{logit}\left(\mathbb{E}\left[\left.\frac{Y_{i}}{n_{i}}\,\right|\,%
\mathbf{X}_{i}\right]\right)=\operatorname{logit}(p_{i})=\ln\left(\frac{p_{i}}%
{1-p_{i}}\right)=\boldsymbol{\beta}\cdot\mathbf{X}_{i},
  </annotation>
</semantics>
</math>
</p>

<p>Or equivalently:</p>

<p>
<math display="block" id="Logistic_regression:99">
<semantics>
<mrow>
<mrow>
<mo>Pr</mo>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>=</mo>
<msub>
<mi>y</mi>
<mi>i</mi>
</msub>
</mrow>
<mo>∣</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mrow>
<mo>(</mo>
<mtable columnspacing="0.4em" rowspacing="0.2ex">
<mtr>
<mtd>
<msub>
<mi>n</mi>
<mi>i</mi>
</msub>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>y</mi>
<mi>i</mi>
</msub>
</mtd>
</mtr>
</mtable>
<mo>)</mo>
</mrow>
<msubsup>
<mi>p</mi>
<mi>i</mi>
<msub>
<mi>y</mi>
<mi>i</mi>
</msub>
</msubsup>
<msup>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mn>1</mn>
<mo>-</mo>
<msub>
<mi>p</mi>
<mi>i</mi>
</msub>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
<mrow>
<msub>
<mi>n</mi>
<mi>i</mi>
</msub>
<mo>-</mo>
<msub>
<mi>y</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
</mrow>
<mo>=</mo>
<mrow>
<mrow>
<mo>(</mo>
<mtable columnspacing="0.4em" rowspacing="0.2ex">
<mtr>
<mtd>
<msub>
<mi>n</mi>
<mi>i</mi>
</msub>
</mtd>
</mtr>
<mtr>
<mtd>
<msub>
<mi>y</mi>
<mi>i</mi>
</msub>
</mtd>
</mtr>
</mtable>
<mo>)</mo>
</mrow>
<msup>
<mrow>
<mo>(</mo>
<mfrac>
<mn>1</mn>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<mo>-</mo>
<mrow>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</mrow>
</msup>
</mrow>
</mfrac>
<mo>)</mo>
</mrow>
<msub>
<mi>y</mi>
<mi>i</mi>
</msub>
</msup>
<msup>
<mrow>
<mo>(</mo>
<mrow>
<mn>1</mn>
<mo>-</mo>
<mfrac>
<mn>1</mn>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<mo>-</mo>
<mrow>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
</mrow>
</msup>
</mrow>
</mfrac>
</mrow>
<mo>)</mo>
</mrow>
<mrow>
<msub>
<mi>n</mi>
<mi>i</mi>
</msub>
<mo>-</mo>
<msub>
<mi>y</mi>
<mi>i</mi>
</msub>
</mrow>
</msup>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<and></and>
<apply>
<eq></eq>
<apply>
<ci>Pr</ci>
<apply>
<eq></eq>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>y</ci>
<ci>i</ci>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
<apply>
<times></times>
<apply>
<csymbol cd="latexml">binomial</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>n</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>y</ci>
<ci>i</ci>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>y</ci>
<ci>i</ci>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>p</ci>
<ci>i</ci>
</apply>
</apply>
<apply>
<minus></minus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>n</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>y</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<times></times>
<apply>
<csymbol cd="latexml">binomial</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>n</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>y</ci>
<ci>i</ci>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<divide></divide>
<cn type="integer">1</cn>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<minus></minus>
<apply>
<ci>normal-⋅</ci>
<ci>𝜷</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>y</ci>
<ci>i</ci>
</apply>
</apply>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<apply>
<minus></minus>
<cn type="integer">1</cn>
<apply>
<divide></divide>
<cn type="integer">1</cn>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<minus></minus>
<apply>
<ci>normal-⋅</ci>
<ci>𝜷</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
<apply>
<minus></minus>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>n</ci>
<ci>i</ci>
</apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>y</ci>
<ci>i</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \operatorname{Pr}(Y_{i}=y_{i}\mid\mathbf{X}_{i})={n_{i}\choose y_{i}}p_{i}^{y_%
{i}}(1-p_{i})^{n_{i}-y_{i}}={n_{i}\choose y_{i}}\left(\frac{1}{1+e^{-%
\boldsymbol{\beta}\cdot\mathbf{X}_{i}}}\right)^{y_{i}}\left(1-\frac{1}{1+e^{-%
\boldsymbol{\beta}\cdot\mathbf{X}_{i}}}\right)^{n_{i}-y_{i}}
  </annotation>
</semantics>
</math>
</p>

<p>This model can be fit using the same sorts of methods as the above more basic model.</p>
<h2 id="bayesian-logistic-regression">Bayesian logistic regression</h2>
<figure><b>(Figure)</b>
<figcaption>Comparison of <a href="logistic_function" title="wikilink">logistic function</a> with a scaled inverse <a href="probit_function" title="wikilink">probit function</a> (i.e. the <a href="cumulative_distribution_function" title="wikilink">CDF</a> of the <a href="normal_distribution" title="wikilink">normal distribution</a>), comparing 

<math display="inline" id="Logistic_regression:100">
<semantics>
<mrow>
<mi>σ</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<times></times>
<ci>σ</ci>
<ci>x</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \sigma(x)
  </annotation>
</semantics>
</math>

 vs. 

<math display="inline" id="Logistic_regression:101">
<semantics>
<mrow>
<mi mathvariant="normal">Φ</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msqrt>
<mfrac>
<mi>π</mi>
<mn>8</mn>
</mfrac>
</msqrt>
<mi>x</mi>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<times></times>
<ci>normal-Φ</ci>
<apply>
<times></times>
<apply>
<root></root>
<apply>
<divide></divide>
<ci>π</ci>
<cn type="integer">8</cn>
</apply>
</apply>
<ci>x</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Phi(\sqrt{\frac{\pi}{8}}x)
  </annotation>
</semantics>
</math>

, which makes the slopes the same at the origin. This shows the <a href="heavy-tailed_distribution" title="wikilink">heavier tails</a> of the logistic distribution.</figcaption>
</figure>

<p>In a <a href="Bayesian_statistics" title="wikilink">Bayesian statistics</a> context, <a href="prior_distribution" title="wikilink">prior distributions</a> are normally placed on the regression coefficients, usually in the form of <a href="Gaussian_distribution" title="wikilink">Gaussian distributions</a>. Unfortunately, the Gaussian distribution is not the <a href="conjugate_prior" title="wikilink">conjugate prior</a> of the <a href="likelihood_function" title="wikilink">likelihood function</a> in logistic regression. As a result, the <a href="posterior_distribution" title="wikilink">posterior distribution</a> is difficult to calculate, even using standard simulation algorithms (e.g. <a href="Gibbs_sampling" title="wikilink">Gibbs sampling</a>).</p>

<p>There are various possibilities:</p>
<ul>
<li>Don't do a proper Bayesian analysis, but simply compute a <a href="maximum_a_posteriori" title="wikilink">maximum a posteriori</a> point estimate of the parameters. This is common, for example, in "maximum entropy" classifiers in <a href="machine_learning" title="wikilink">machine learning</a>.</li>
<li>Use a more general approximation method such as the <a href="Metropolis–Hastings_algorithm" title="wikilink">Metropolis–Hastings algorithm</a>.</li>
<li>Draw a Markov chain Monte Carlo sample from the exact posterior by using the Independent Metropolis–Hastings algorithm with heavy-tailed multivariate candidate distribution found by matching the mode and curvature at the mode of the normal approximation to the posterior and then using the Student’s t shape with low degrees of freedom.<a class="footnoteRef" href="#fn60" id="fnref60"><sup>60</sup></a> This is shown to have excellent convergence properties.</li>
<li>Use a <a href="latent_variable_model" title="wikilink">latent variable model</a> and approximate the logistic distribution using a more tractable distribution, e.g. a <a href="Student's_t-distribution" title="wikilink">Student's t-distribution</a> or a <a href="mixture_density" title="wikilink">mixture</a> of <a href="normal_distribution" title="wikilink">normal distributions</a>.</li>
<li>Do <a href="probit_regression" title="wikilink">probit regression</a> instead of logistic regression. This is actually a special case of the previous situation, using a <a href="normal_distribution" title="wikilink">normal distribution</a> in place of a Student's t, mixture of normals, etc. This will be less accurate but has the advantage that probit regression is extremely common, and a ready-made Bayesian implementation may already be available.</li>
<li>Use the <a href="Laplace_approximation" title="wikilink">Laplace approximation</a> of the posterior distribution.<a class="footnoteRef" href="#fn61" id="fnref61"><sup>61</sup></a> This approximates the posterior with a Gaussian distribution. This is not a terribly good approximation, but it suffices if all that is desired is an estimate of the posterior mean and variance. In such a case, an approximation scheme such as <a href="variational_Bayes" title="wikilink">variational Bayes</a> can be used.<a class="footnoteRef" href="#fn62" id="fnref62"><sup>62</sup></a></li>
</ul>
<h3 id="gibbs-sampling-with-an-approximating-distribution">Gibbs sampling with an approximating distribution</h3>

<p>As shown above, logistic regression is equivalent to a <a href="latent_variable_model" title="wikilink">latent variable model</a> with an <a href="error_variable" title="wikilink">error variable</a> distributed according to a standard <a href="logistic_distribution" title="wikilink">logistic distribution</a>. The overall distribution of the latent variable 

<math display="inline" id="Logistic_regression:102">
<semantics>
<mrow>
<msub>
<mi>Y</mi>
<mi>i</mi>
</msub>
<mo>∗</mo>
</mrow>
<annotation-xml encoding="MathML-Content">
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>Y</ci>
<ci>i</ci>
</apply>
<ci>normal-∗</ci>
</cerror>
</annotation-xml>
<annotation encoding="application/x-tex">
   Y_{i}\ast
  </annotation>
</semantics>
</math>

 is also a logistic distribution, with the mean equal to 

<math display="inline" id="Logistic_regression:103">
<semantics>
<mrow>
<mi>𝜷</mi>
<mo>⋅</mo>
<msub>
<mi>𝐗</mi>
<mi>i</mi>
</msub>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>normal-⋅</ci>
<ci>𝜷</ci>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<ci>𝐗</ci>
<ci>i</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \boldsymbol{\beta}\cdot\mathbf{X}_{i}
  </annotation>
</semantics>
</math>

 (i.e. the fixed quantity added to the error variable). This model considerably simplifies the application of techniques such as <a href="Gibbs_sampling" title="wikilink">Gibbs sampling</a>. However, sampling the regression coefficients is still difficult, because of the lack of <a href="conjugate_prior" title="wikilink">conjugacy</a> between the normal and logistic distributions. Changing the prior distribution over the regression coefficients is of no help, because the logistic distribution is not in the <a href="exponential_family" title="wikilink">exponential family</a> and thus has no <a href="conjugate_prior" title="wikilink">conjugate prior</a>.</p>

<p>One possibility is to use a more general <a href="Markov_chain_Monte_Carlo" title="wikilink">Markov chain Monte Carlo</a> technique, such as the <a href="Metropolis–Hastings_algorithm" title="wikilink">Metropolis–Hastings algorithm</a>, which can sample arbitrary distributions. Another possibility, however, is to replace the logistic distribution with a similar-shaped distribution that is easier to work with using Gibbs sampling. In fact, the logistic and normal distributions have a similar shape, and thus one possibility is simply to have normally distributed errors. Because the normal distribution is conjugate to itself, sampling the regression coefficients becomes easy. In fact, this model is exactly the model used in <a href="probit_regression" title="wikilink">probit regression</a>.</p>

<p>However, the normal and logistic distributions differ in that the logistic has <a href="heavy-tailed_distribution" title="wikilink">heavier tails</a>. As a result, it is more <a href="robust_statistics" title="wikilink">robust</a> to inaccuracies in the underlying model (which are inevitable, in that the model is essentially always an approximation) or to errors in the data. Probit regression loses some of this robustness.</p>

<p>Another alternative is to use errors distributed as a <a href="Student's_t-distribution" title="wikilink">Student's t-distribution</a>. The Student's t-distribution has heavy tails, and is easy to sample from because it is the <a href="compound_distribution" title="wikilink">compound distribution</a> of a normal distribution with variance distributed as an <a href="inverse_gamma_distribution" title="wikilink">inverse gamma distribution</a>. In other words, if a normal distribution is used for the error variable, and another <a href="latent_variable" title="wikilink">latent variable</a>, following an inverse gamma distribution, is added corresponding to the variance of this error variable, the <a href="marginal_distribution" title="wikilink">marginal distribution</a> of the error variable will follow a Student's t distribution. Because of the various conjugacy relationships, all variables in this model are easy to sample from.</p>

<p>The Student's t distribution that best approximates a standard logistic distribution can be determined by <a href="method_of_moments_(statistics)" title="wikilink">matching the moments</a> of the two distributions. The Student's t distribution has three parameters, and since the <a class="uri" href="skewness" title="wikilink">skewness</a> of both distributions is always 0, the first four moments can all be matched, using the following equations:</p>

<p>
<math display="inline" id="Logistic_regression:104">
<semantics>
<mi>μ</mi>
<annotation-xml encoding="MathML-Content">
<ci>μ</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   \displaystyle\mu
  </annotation>
</semantics>
</math>
</p>

<p>This yields the following values:</p>

<p>
<math display="inline" id="Logistic_regression:105">
<semantics>
<mi>μ</mi>
<annotation-xml encoding="MathML-Content">
<ci>μ</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   \displaystyle\mu
  </annotation>
</semantics>
</math>
</p>

<p>The following graphs compare the standard logistic distribution with the Student's t distribution that matches the first four moments using the above-determined values, as well as the normal distribution that matches the first two moments. Note how much closer the Student's t distribution agrees, especially in the tails. Beyond about two standard deviations from the mean, the logistic and normal distributions diverge rapidly, but the logistic and Student's t distributions don't start diverging significantly until more than 5 standard deviations away.</p>

<p>(Another possibility, also amenable to Gibbs sampling, is to approximate the logistic distribution using a <a href="mixture_density" title="wikilink">mixture density</a> of <a href="normal_distribution" title="wikilink">normal distributions</a>.)</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><figure><b>(Figure)</b>
<figcaption>Comparison of logistic and approximating distributions (t, normal).</figcaption>
</figure></td>
<td style="text-align: left;"><figure><b>(Figure)</b>
<figcaption>Tails of distributions.</figcaption>
</figure></td>
</tr>
<tr class="even">
<td style="text-align: left;"><figure><b>(Figure)</b>
<figcaption>Further tails of distributions.</figcaption>
</figure></td>
<td style="text-align: left;"><figure><b>(Figure)</b>
<figcaption>Extreme tails of distributions.</figcaption>
</figure></td>
</tr>
</tbody>
</table>
<h2 id="extensions">Extensions</h2>

<p>There are large numbers of extensions:</p>
<ul>
<li><a href="Multinomial_logistic_regression" title="wikilink">Multinomial logistic regression</a> (or <strong>multinomial logit</strong>) handles the case of a multi-way <a href="categorical_variable" title="wikilink">categorical</a> dependent variable (with unordered values, also called "classification"). Note that the general case of having dependent variables with more than two values is termed <em>polytomous regression</em>.</li>
<li><a href="Ordered_logistic_regression" title="wikilink">Ordered logistic regression</a> (or <strong>ordered logit</strong>) handles <a href="Levels_of_measurement#Ordinal_measurement" title="wikilink">ordinal</a> dependent variables (ordered values).</li>
<li><a href="Mixed_logit" title="wikilink">Mixed logit</a> is an extension of multinomial logit that allows for correlations among the choices of the dependent variable.</li>
<li>An extension of the logistic model to sets of interdependent variables is the <a href="conditional_random_field" title="wikilink">conditional random field</a>.</li>
</ul>
<h2 id="model-suitability">Model suitability</h2>

<p>A way to measure a model's suitability is to assess the model against a set of data that was not used to create the model.<a class="footnoteRef" href="#fn63" id="fnref63"><sup>63</sup></a> The class of techniques is called <a href="cross-validation_(statistics)" title="wikilink">cross-validation</a>. This holdout model assessment method is particularly valuable when data are collected in different settings (e.g., at different times or places) or when models are assumed to be generalizable.</p>

<p>To measure the suitability of a binary regression model, one can classify both the actual value and the predicted value of each observation as either 0 or 1.<a class="footnoteRef" href="#fn64" id="fnref64"><sup>64</sup></a> The predicted value of an observation can be set equal to 1 if the estimated probability that the observation equals 1 is above 

<math display="inline" id="Logistic_regression:106">
<semantics>
<mfrac>
<mn>1</mn>
<mn>2</mn>
</mfrac>
<annotation-xml encoding="MathML-Content">
<apply>
<divide></divide>
<cn type="integer">1</cn>
<cn type="integer">2</cn>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \frac{1}{2}
  </annotation>
</semantics>
</math>

, and set equal to 0 if the estimated probability is below 

<math display="inline" id="Logistic_regression:107">
<semantics>
<mfrac>
<mn>1</mn>
<mn>2</mn>
</mfrac>
<annotation-xml encoding="MathML-Content">
<apply>
<divide></divide>
<cn type="integer">1</cn>
<cn type="integer">2</cn>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \frac{1}{2}
  </annotation>
</semantics>
</math>

. Here logistic regression is being used as a <a href="binary_classification" title="wikilink">binary classification</a> model. There are four possible combined classifications:</p>
<ol>
<li>prediction of 0 when the holdout sample has a 0 (True Negatives, the number of which is TN)</li>
<li>prediction of 0 when the holdout sample has a 1 (False Negatives, the number of which is FN)</li>
<li>prediction of 1 when the holdout sample has a 0 (False Positives, the number of which is FP)</li>
<li>prediction of 1 when the holdout sample has a 1 (True Positives, the number of which is TP)</li>
</ol>

<p>These classifications are used to calculate <a href="Accuracy_and_precision#In_binary_classification" title="wikilink">accuracy, precision</a> (also called <a href="positive_predictive_value" title="wikilink">positive predictive value</a>), <a href="Precision_and_recall" title="wikilink">recall</a> (also called <a href="Sensitivity_and_specificity" title="wikilink">sensitivity</a>), <a href="Sensitivity_and_specificity" title="wikilink">specificity</a> and <a href="negative_predictive_value" title="wikilink">negative predictive value</a>:</p>

<p>
<math display="block" id="Logistic_regression:108">
<semantics>
<mrow>
<mtext>Accuracy</mtext>
<mo>=</mo>
<mfrac>
<mrow>
<mrow>
<mi>T</mi>
<mi>P</mi>
</mrow>
<mo>+</mo>
<mrow>
<mi>T</mi>
<mi>N</mi>
</mrow>
</mrow>
<mrow>
<mrow>
<mi>T</mi>
<mi>P</mi>
</mrow>
<mo>+</mo>
<mrow>
<mi>F</mi>
<mi>P</mi>
</mrow>
<mo>+</mo>
<mrow>
<mi>F</mi>
<mi>N</mi>
</mrow>
<mo>+</mo>
<mrow>
<mi>T</mi>
<mi>N</mi>
</mrow>
</mrow>
</mfrac>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<mtext>Accuracy</mtext>
<apply>
<divide></divide>
<apply>
<plus></plus>
<apply>
<times></times>
<ci>T</ci>
<ci>P</ci>
</apply>
<apply>
<times></times>
<ci>T</ci>
<ci>N</ci>
</apply>
</apply>
<apply>
<plus></plus>
<apply>
<times></times>
<ci>T</ci>
<ci>P</ci>
</apply>
<apply>
<times></times>
<ci>F</ci>
<ci>P</ci>
</apply>
<apply>
<times></times>
<ci>F</ci>
<ci>N</ci>
</apply>
<apply>
<times></times>
<ci>T</ci>
<ci>N</ci>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \text{Accuracy}=\frac{TP+TN}{TP+FP+FN+TN}
  </annotation>
</semantics>
</math>

 = fraction of observations with correct predicted classification</p>

<p>
<math display="block" id="Logistic_regression:109">
<semantics>
<mrow>
<mtext>Precision</mtext>
<mo>=</mo>
<mtext>Positive predictive value</mtext>
<mo>=</mo>
<mpadded width="+1.7pt">
<mfrac>
<mrow>
<mi>T</mi>
<mi>P</mi>
</mrow>
<mrow>
<mrow>
<mi>T</mi>
<mi>P</mi>
</mrow>
<mo>+</mo>
<mrow>
<mi>F</mi>
<mi>P</mi>
</mrow>
</mrow>
</mfrac>
</mpadded>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<and></and>
<apply>
<eq></eq>
<mtext>Precision</mtext>
<mtext>Positive predictive value</mtext>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<divide></divide>
<apply>
<times></times>
<ci>T</ci>
<ci>P</ci>
</apply>
<apply>
<plus></plus>
<apply>
<times></times>
<ci>T</ci>
<ci>P</ci>
</apply>
<apply>
<times></times>
<ci>F</ci>
<ci>P</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \text{Precision}=\text{Positive predictive value}=\frac{TP}{TP+FP}\,
  </annotation>
</semantics>
</math>

 = Fraction of predicted positives that are correct</p>

<p>
<math display="block" id="Logistic_regression:110">
<semantics>
<mrow>
<mtext>Negative predictive value</mtext>
<mo>=</mo>
<mfrac>
<mrow>
<mi>T</mi>
<mi>N</mi>
</mrow>
<mrow>
<mrow>
<mi>T</mi>
<mi>N</mi>
</mrow>
<mo>+</mo>
<mrow>
<mi>F</mi>
<mi>N</mi>
</mrow>
</mrow>
</mfrac>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<mtext>Negative predictive value</mtext>
<apply>
<divide></divide>
<apply>
<times></times>
<ci>T</ci>
<ci>N</ci>
</apply>
<apply>
<plus></plus>
<apply>
<times></times>
<ci>T</ci>
<ci>N</ci>
</apply>
<apply>
<times></times>
<ci>F</ci>
<ci>N</ci>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \text{Negative predictive value}=\frac{TN}{TN+FN}
  </annotation>
</semantics>
</math>

 = fraction of predicted negatives that are correct</p>

<p>
<math display="block" id="Logistic_regression:111">
<semantics>
<mrow>
<mtext>Recall</mtext>
<mo>=</mo>
<mtext>Sensitivity</mtext>
<mo>=</mo>
<mpadded width="+1.7pt">
<mfrac>
<mrow>
<mi>T</mi>
<mi>P</mi>
</mrow>
<mrow>
<mrow>
<mi>T</mi>
<mi>P</mi>
</mrow>
<mo>+</mo>
<mrow>
<mi>F</mi>
<mi>N</mi>
</mrow>
</mrow>
</mfrac>
</mpadded>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<and></and>
<apply>
<eq></eq>
<mtext>Recall</mtext>
<mtext>Sensitivity</mtext>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<divide></divide>
<apply>
<times></times>
<ci>T</ci>
<ci>P</ci>
</apply>
<apply>
<plus></plus>
<apply>
<times></times>
<ci>T</ci>
<ci>P</ci>
</apply>
<apply>
<times></times>
<ci>F</ci>
<ci>N</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \text{Recall}=\text{Sensitivity}=\frac{TP}{TP+FN}\,
  </annotation>
</semantics>
</math>

 = fraction of observations that are actually 1 with a correct predicted classification</p>

<p>
<math display="block" id="Logistic_regression:112">
<semantics>
<mrow>
<mtext>Specificity</mtext>
<mo>=</mo>
<mfrac>
<mrow>
<mi>T</mi>
<mi>N</mi>
</mrow>
<mrow>
<mrow>
<mi>T</mi>
<mi>N</mi>
</mrow>
<mo>+</mo>
<mrow>
<mi>F</mi>
<mi>P</mi>
</mrow>
</mrow>
</mfrac>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<mtext>Specificity</mtext>
<apply>
<divide></divide>
<apply>
<times></times>
<ci>T</ci>
<ci>N</ci>
</apply>
<apply>
<plus></plus>
<apply>
<times></times>
<ci>T</ci>
<ci>N</ci>
</apply>
<apply>
<times></times>
<ci>F</ci>
<ci>P</ci>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \text{Specificity}=\frac{TN}{TN+FP}
  </annotation>
</semantics>
</math>

 = fraction of observations that are actually 0 with a correct predicted classification</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Logistic_function" title="wikilink">Logistic function</a></li>
<li><a href="Discrete_choice" title="wikilink">Discrete choice</a></li>
<li><a href="Jarrow–Turnbull_model" title="wikilink">Jarrow–Turnbull model</a></li>
<li><a href="Limited_dependent_variable" title="wikilink">Limited dependent variable</a></li>
<li><a href="Multinomial_logit" title="wikilink">Multinomial logit model</a></li>
<li><a href="Ordered_logit" title="wikilink">Ordered logit</a></li>
<li><a href="Hosmer–Lemeshow_test" title="wikilink">Hosmer–Lemeshow test</a></li>
<li><a href="Brier_score" title="wikilink">Brier score</a></li>
<li><a href="MLPACK_(C++_library)" title="wikilink">MLPACK</a> - contains a <a class="uri" href="C++" title="wikilink">C++</a> implementation of logistic regression</li>
<li><a href="Local_case-control_sampling" title="wikilink">Local case-control sampling</a></li>
<li><a href="Logistic_model_tree" title="wikilink">Logistic model tree</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="further-reading">Further reading</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li>

<p>by <a href="Mark_Thoma" title="wikilink">Mark Thoma</a></p></li>
<li><a href="http://www.appricon.com/index.php/logistic-regression-analysis.html">Logistic Regression Interpretation</a></li>
<li><a href="http://www.omidrouhani.com/research/logisticregression/html/logisticregression.htm">Logistic Regression tutorial</a></li>
<li><a href="http://www.simafore.com/blog/?Tag=logistic+regression">Using open source software for building Logistic Regression models</a></li>
<li><a href="http://www.biomedicalstatistics.info/en/prognosis/logistic.html">Logistic regression. Biomedical statistics</a></li>
</ul>

<p>"</p>

<p><a href="Category:Classification_algorithms" title="wikilink">Category:Classification algorithms</a> <a href="Category:Log-linear_models" title="wikilink">Category:Log-linear models</a> <a href="Category:Regression_analysis" title="wikilink">Category:Regression analysis</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8">Kologlu M., Elker D., Altun H., Sayek I. Valdation of MPI and OIA II in two different groups of patients with secondary peritonitis // Hepato-Gastroenterology. – 2001. – Vol. 48, № 37. – P. 147-151.<a href="#fnref8">↩</a></li>
<li id="fn9">Biondo S., Ramos E., Deiros M. et al. Prognostic factors for mortality in left colonic peritonitis: a new scoring system // J. Am. Coll. Surg. – 2000. – Vol. 191, № 6. – Р. 635-642.<a href="#fnref9">↩</a></li>
<li id="fn10">Marshall J.C., Cook D.J., Christou N.V. et al. Multiple Organ Dysfunction Score: A reliable descriptor of a complex clinical outcome // Crit. Care Med. – 1995. – Vol. 23. – P. 1638-1652.<a href="#fnref10">↩</a></li>
<li id="fn11">Le Gall J.-R., Lemeshow S., Saulnier F. A new Simplified Acute Physiology Score (SAPS II) based on a European/North American multicenter study // JAMA. – 1993. – Vol. 270. – P. 2957-2963.<a href="#fnref11">↩</a></li>
<li id="fn12"></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"></li>
<li id="fn15"><a href="#fnref15">↩</a></li>
<li id="fn16"><a href="#fnref16">↩</a></li>
<li id="fn17"></li>
<li id="fn18"></li>
<li id="fn19"></li>
<li id="fn20"></li>
<li id="fn21"></li>
<li id="fn22"><a class="uri" href="http://www.planta.cn/forum/files_planta/introduction_to_categorical_data_analysis_805.pdf">http://www.planta.cn/forum/files_planta/introduction_to_categorical_data_analysis_805.pdf</a><a href="#fnref22">↩</a></li>
<li id="fn23"><a href="#fnref23">↩</a></li>
<li id="fn24"></li>
<li id="fn25">Menard ch 1.3<a href="#fnref25">↩</a></li>
<li id="fn26"></li>
<li id="fn27"></li>
<li id="fn28"></li>
<li id="fn29"></li>
<li id="fn30"><a href="#fnref30">↩</a></li>
<li id="fn31"></li>
<li id="fn32"><a href="#fnref32">↩</a></li>
<li id="fn33"></li>
<li id="fn34"></li>
<li id="fn35"></li>
<li id="fn36"></li>
<li id="fn37"></li>
<li id="fn38"></li>
<li id="fn39"></li>
<li id="fn40"></li>
<li id="fn41"></li>
<li id="fn42"></li>
<li id="fn43"></li>
<li id="fn44"></li>
<li id="fn45"></li>
<li id="fn46"></li>
<li id="fn47"></li>
<li id="fn48"></li>
<li id="fn49"></li>
<li id="fn50"></li>
<li id="fn51"></li>
<li id="fn52"></li>
<li id="fn53"></li>
<li id="fn54"></li>
<li id="fn55"></li>
<li id="fn56"></li>
<li id="fn57"></li>
<li id="fn58"><a class="uri" href="https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/classification.pdf">https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/classification.pdf</a> slide 16<a href="#fnref58">↩</a></li>
<li id="fn59"></li>
<li id="fn60"></li>
<li id="fn61"><a href="#fnref61">↩</a></li>
<li id="fn62"><a href="#fnref62">↩</a></li>
<li id="fn63">Jonathan Mark and Michael A. Goldberg (2001). Multiple Regression Analysis and Mass Assessment: A Review of the Issues. The Appraisal Journal, Jan. pp. 89–109<a href="#fnref63">↩</a></li>
<li id="fn64"><a href="#fnref64">↩</a></li>
</ol>
</section>
</p<></p></body>
</html>
