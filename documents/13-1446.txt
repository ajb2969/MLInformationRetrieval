   Self-Similarity of Network Data Analysis      Self-Similarity of Network Data Analysis   In computer networks , self-similarity is a feature of network data transfer dynamics. When modeling network data dynamics the traditional time series models, such as an autoregressive moving average model (ARMA(p, q)), are not appropriate. This is because these models only provide a finite number of parameters in the model and thus interaction in a finite time window, but the network data usually have a long-range dependent temporal structure. A self-similar process is one way of modeling network data dynamics with such a long range correlation. This article defines and describes network data transfer dynamics in the context of a self-similar process. Properties of the process are shown and methods are given for graphing and estimating parameters modeling the self-similarity of network data.  Definition  Suppose   X   X   X   be a weakly stationary (2nd-order stationary) process with mean   μ   μ   \mu   , variance    σ  2     superscript  σ  2    \sigma^{2}   , and autocorrelation function    γ   (  t  )       γ  t    \gamma(t)   . Assume that the autocorrelation function    γ   (  t  )       γ  t    \gamma(t)   has the form     γ   (  t  )    →    t   -  β    L   (  t  )       normal-→    γ  t      superscript  t    β    L  t     \gamma(t)\rightarrow t^{-\beta}L(t)   as    t  →  ∞     normal-→  t     t\to\infty   , where    0  <  β  <  1        0  β       1     0<\beta<1   and    L   (  t  )       L  t    L(t)   is a slowly varying function at infinity , that is      lim   t  →  ∞      L   (   t  x   )     L   (  t  )      =  1        subscript    normal-→  t         L    t  x      L  t     1    \lim_{t\to\infty}\frac{L(tx)}{L(t)}=1   for all    x  >  0      x  0    x>0   . For example,     L   (  t  )    =   c  o  n  s  t         L  t     c  o  n  s  t     L(t)=const   and     L   (  t  )    =   log   (  t  )          L  t     t     L(t)=\log(t)   are slowly varying functions. Let     X  k   (  m  )    =    1   m  H     (    X     k  m   -  m   +  1    +  ⋯  +   X   k  m     )         superscript   subscript  X  k   m       1   superscript  m  H       subscript  X        k  m   m   1    normal-⋯   subscript  X    k  m        X_{k}^{(m)}=\frac{1}{m^{H}}(X_{km-m+1}+\cdot\cdot\cdot+X_{km})   , where    k  =   1  ,  2  ,  3  ,  …       k   1  2  3  normal-…     k=1,2,3,\ldots   , denote an aggregated point series over non-overlapping blocks of size   m   m   m   , for each   m   m   m   is a positive integer .  Exactly self-similar process      X   X   X   is called an exactly self-similar process if there exists a self-similar parameter   H   H   H   such that    X  k   (  m  )      superscript   subscript  X  k   m    X_{k}^{(m)}   has the same distribution as   X   X   X   . An example of exactly self-similar process with   H   H   H   is Fractional Gaussian Noise (FGN) with \frac{1}{2} .   Definition:Fractional Gaussian Noise (FGN) {{-}}      X   (  t  )    =     B  H    (   t  +  1   )    -    B  H    (  t  )      ,    ∀  t   ≥  1      formulae-sequence      X  t        subscript  B  H     t  1       subscript  B  H   t        for-all  t   1     X(t)=B_{H}(t+1)-B_{H}(t),~{}\forall t\geq 1   is called the Fractional Gaussian Noise, where     B  H    (  ⋅  )        subscript  B  H   normal-⋅    B_{H}(\cdot)   is a Fractional Brownian motion . 1  exactly second order self-similar process      X   X   X   is called an exactly second order self-similar process if there exists a self-similar parameter   H   H   H   such that    X  k   (  m  )      superscript   subscript  X  k   m    X_{k}^{(m)}   has the same variance and autocorrelation as   X   X   X   .   asymptotic second order self-similar process      X   X   X   is called an asymptotic second order self-similar process with self-similar parameter   H   H   H   if      γ   (  m  )     (  t  )    →    1  2    [      (   t  +  1   )    2  H    -   2   t   2  H      +    (   t  -  1   )    2  H     ]       normal-→     superscript  γ  m   t       1  2    delimited-[]       superscript    t  1     2  H      2   superscript  t    2  H       superscript    t  1     2  H         \gamma^{(m)}(t)\to\frac{1}{2}[(t+1)^{2H}-2t^{2H}+(t-1)^{2H}]   as    m  →  ∞     normal-→  m     m\to\infty   ,     ∀  t   =   1  ,  2  ,  3  ,  …        for-all  t    1  2  3  normal-…     ~{}\forall t=1,2,3,\ldots      Some relative situations of Self-Similar Processes  Long-Range-Dependence(LRD)  Suppose    X   (  t  )       X  t    X(t)   be a weakly stationary (2nd-order stationary) process with mean   μ   μ   \mu   and variance    σ  2     superscript  σ  2    \sigma^{2}   . The Autocorrelation Function (ACF) of lag   t   t   t   is given by     γ   (  t  )    =    cov   (   X   (  h  )    ,   X   (   h  +  t   )    )     σ  2    =    E   [    (    X   (  h  )    -  μ   )    (    X   (   h  +  t   )    -  μ   )    ]     σ  2            γ  t       cov     X  h     X    h  t       superscript  σ  2             E   delimited-[]        X  h   μ       X    h  t    μ       superscript  σ  2       \gamma(t)={\mathrm{cov}(X(h),X(h+t))\over\sigma^{2}}={E[(X(h)-\mu)(X(h+t)-\mu)%
 ]\over\sigma^{2}}   {{-}} Definition: {{-}} A weakly stationary process is said to be "Long-Range-Dependence" if      ∑   t  =  0   ∞    |   γ   (  t  )    |    =  ∞        superscript   subscript     t  0          γ  t        \sum_{t=0}^{\infty}|\gamma(t)|=\infty     A process which satisfies     γ   (  t  )    →    t   -  β    L   (  t  )       normal-→    γ  t      superscript  t    β    L  t     \gamma(t)\rightarrow t^{-\beta}L(t)   as    t  →  ∞     normal-→  t     t\to\infty   is said to have long-range dependence. The spectral density function of long-range dependence follows a power law near the origin. Equivalently to     γ   (  t  )    →    t   -  β    L   (  t  )       normal-→    γ  t      superscript  t    β    L  t     \gamma(t)\rightarrow t^{-\beta}L(t)   ,   X   X   X   has long-range dependence if the spectral density function of autocorrelation function,      f  t    (  w  )    =    ∑   t  =  0   ∞    γ   (  t  )    e   i  w  t             subscript  f  t   w     superscript   subscript     t  0        γ  t   superscript  e    i  w  t        f_{t}(w)=\sum_{t=0}^{\infty}\gamma(t)e^{iwt}   , has the form of     w   -  γ    L   (  w  )        superscript  w    γ    L  w    w^{-\gamma}L(w)   as    w  →  0     normal-→  w  0    w\to 0   where    0  <  γ  <  1        0  γ       1     0<\gamma<1   ,   L   L   L   is slowly varying at 0.  also see  Slowly decaying variances       X   (  m  )    =    1  m    (    X  1   +  ⋯  +   X  m    )         superscript  X  m       1  m      subscript  X  1   normal-⋯   subscript  X  m       X^{(m)}=\frac{1}{m}(X_{1}+\cdot\cdot\cdot+X_{m})    When an autocorrelation function of a self-similar process satisfies     γ   (  t  )    →    t   -  β    L   (  t  )       normal-→    γ  t      superscript  t    β    L  t     \gamma(t)\rightarrow t^{-\beta}L(t)   as    t  →  ∞     normal-→  t     t\to\infty   , that means it also satisfies     V  a  r   (   X   (  m  )    )    →   a   m   -  β        normal-→    V  a  r   superscript  X  m      a   superscript  m    β       Var(X^{(m)})\to am^{-\beta}   as    m  →  ∞     normal-→  m     m\to\infty   , where   a   a   a   is a finite positive constant independent of m, and 0<βX is Fractional Gaussian Noise. Consider the series     X   (  1  )    ,  …  ,   X   (  n  )        subscript  X  1   normal-…   subscript  X  n     X_{(1)},\ldots,X_{(n)}   , and let     Y   (  n  )    =    ∑   i  =  1   n    X   (  i  )          subscript  Y  n     superscript   subscript     i  1    n    subscript  X  i      Y_{(n)}=\sum_{i=1}^{n}X_{(i)}   . The sample variance of    X   (  i  )      subscript  X  i    X_{(i)}   is      S  2    (  n  )    =     1  n     ∑   i  =  1   n    X   (  i  )   2     -     (   1  n   )   2    Y  n  2            superscript  S  2   n         1  n     superscript   subscript     i  1    n    subscript   superscript  X  2   i        superscript    1  n   2    subscript   superscript  Y  2   n       S^{2}(n)=\frac{1}{n}\sum_{i=1}^{n}X^{2}_{(i)}-(\frac{1}{n})^{2}Y^{2}_{n}   {{-}} Definition:R/S statistic        R  S    (  n  )    =    1   S   (  n  )      [     max   0  ≤  t  ≤  n     (    Y  t   -    t  n    Y  n     )    -    min   0  ≤  t  ≤  n     (    Y  t   -    t  n    Y  n     )     ]            R  S   n       1    S  n     delimited-[]      subscript       0  t       n        subscript  Y  t       t  n    subscript  Y  n        subscript       0  t       n        subscript  Y  t       t  n    subscript  Y  n           \frac{R}{S}(n)=\frac{1}{S(n)}[\max_{0\leq t\leq n}(Y_{t}-\frac{t}{n}Y_{n})-%
 \min_{0\leq t\leq n}(Y_{t}-\frac{t}{n}Y_{n})]    {{-}} If    X   (  i  )      subscript  X  i    X_{(i)}   is FGN, then     E   (    R  S    (  n  )    )    →    C  H   ×   n  H       normal-→    E      R  S   n       subscript  C  H    superscript  n  H      E(\frac{R}{S}(n))\to C_{H}\times n^{H}    Consider fitting a regression model :     l  o  g   R  S    (  n  )    =    l  o  g   (   C  H   )    +   H  l  o  g   (  n  )    +   ϵ  n          l  o  g    R  S   n       l  o  g   subscript  C  H      H  l  o  g  n    subscript  ϵ  n      log\frac{R}{S}(n)=log(C_{H})+Hlog(n)+\epsilon_{n}   , where     ϵ  n   ∼   N   (  0  ,   σ  2   )       normal-∼   subscript  ϵ  n     N   0   superscript  σ  2       \epsilon_{n}\thicksim N(0,\sigma^{2})    In particular for a time series of length   N   N   N   divide the time series data into   k   k   k   groups each of size    N  k      N  k    \frac{N}{k}   , compute     R  S    (  n  )         R  S   n    \frac{R}{S}(n)   for each group. Thus for each n we have   k   k   k   pairs of data (     l  o  g   (  n  )    ,   l  o  g   R  S    (  n  )         l  o  g  n     l  o  g    R  S   n     log(n),log\frac{R}{S}(n)   ).There are   k   k   k   points for each   n   n   n   , so we can fit a regression model to estimate   H   H   H   more accurately. If the solpe of the regression line is between 0.5~1, it is a self-similar process.  Variance-time plot  Variance of the sample mean is given by      V  a  r   (    X  ¯   n   )    →   c   n    2  H   -  2      ,    ∀  c   >  0      formulae-sequence   normal-→    V  a  r   subscript   normal-¯  X   n      c   superscript  n      2  H   2         for-all  c   0     Var(\bar{X}_{n})\to cn^{2H-2},~{}\forall c>0   . For estimating H, calculate sample means       X  ¯   1   ,    X  ¯   2   ,  ⋯  ,    X  ¯    m  k        subscript   normal-¯  X   1    subscript   normal-¯  X   2   normal-⋯   subscript   normal-¯  X    subscript  m  k      \bar{X}_{1},\bar{X}_{2},\cdots,\bar{X}_{m_{k}}   for    m  k     subscript  m  k    m_{k}   sub-series of length   k   k   k   . Overall mean can be given by      X  ¯    (  k  )    =    1   m  k      ∑   i  =  1    m  k       X  ¯   i    (  k  )             normal-¯  X   k       1   subscript  m  k      superscript   subscript     i  1     subscript  m  k       subscript   normal-¯  X   i   k       \bar{X}(k)=\frac{1}{m_{k}}\sum_{i=1}^{m_{k}}\bar{X}_{i}(k)   , sample variance      S  2    (  k  )    =    1    m  k   -  1      ∑   i  =  1    m  k      (      X  ¯   i    (  k  )    -    X  ¯    (  k  )     )   2            superscript  S  2   k       1     subscript  m  k   1      superscript   subscript     i  1     subscript  m  k     superscript       subscript   normal-¯  X   i   k      normal-¯  X   k    2       S^{2}(k)=\frac{1}{m_{k}-1}\sum_{i=1}^{m_{k}}(\bar{X}_{i}(k)-\bar{X}(k))^{2}   . The variance-time plots are obtained by plotting     log   S  2     (  k  )          superscript  S  2    k    \log S^{2}(k)   against    log  k      k    \log k   and we can fit a simple least square line through the resulting points in the plane ignoring the small values of k.  For large values of   k   k   k   , the points in the plot are expected to be scattered around a straight line with a negative slope     2  H   -  2        2  H   2    2H-2   .For short-range dependence or independence among the observations, the slope of the straight line is equal to -1. Self-similarity can be inferred from the values of the estimated slope which is asymptotically between –1 and 0, and an estimate for the degree of self-similarity is given by      H  ^   =   1  +    1  2    (   s  l  o  p  e   )      .       normal-^  H     1      1  2     s  l  o  p  e       \hat{H}=1+\frac{1}{2}(slope).     Periodogram-based analysis  Whittle's approximate maximum likelihood estimator ( MLE ) is applied to solve the Hurst's parameter via the spectral density of   X   X   X   . It is not only a tool for visualizing the Hurst's parameter, but also a method to do some statistical inference about the parameters via the asymptotic properties of the MLE. In particular,   X   X   X   follows a Gaussian process . Let the spectral density of   X   X   X   ,      f  x    (  w  ;  θ  )    =    σ  ϵ  2    f  x    (  w  ;   (  1  ,  η  )   )           subscript  f  x    w  θ       superscript   subscript  σ  ϵ   2    subscript  f  x    w   1  η       f_{x}(w;\theta)=\sigma_{\epsilon}^{2}f_{x}(w;(1,\eta))   , where     θ  =   (   σ  ϵ  2   ,  η  )   =   (   σ  ϵ  2   ,  H  ,   θ  3   ,  …  ,   θ  k   )    ,   H  =    γ  +  1   2       formulae-sequence      θ    superscript   subscript  σ  ϵ   2   η          superscript   subscript  σ  ϵ   2   H   subscript  θ  3   normal-…   subscript  θ  k        H      γ  1   2      \theta=(\sigma_{\epsilon}^{2},\eta)=(\sigma_{\epsilon}^{2},H,\theta_{3},\ldots%
 ,\theta_{k}),H=\frac{\gamma+1}{2}   , and     θ  3   ,  …  ,   θ  k       subscript  θ  3   normal-…   subscript  θ  k     \theta_{3},\ldots,\theta_{k}   construct a short-range time series autoregression (AR) model, that is     X  j   =     ∑   i  =  1   k     α  i    X   j  -  i      +   ϵ  j         subscript  X  j       superscript   subscript     i  1    k      subscript  α  i    subscript  X    j  i       subscript  ϵ  j      X_{j}=\sum_{i=1}^{k}\alpha_{i}X_{j-i}+\epsilon_{j}   , with     V  a  r   (   ϵ  j   )    =   σ  ϵ  2         V  a  r   subscript  ϵ  j     superscript   subscript  σ  ϵ   2     Var(\epsilon_{j})=\sigma_{\epsilon}^{2}   .  Thus, the Whittle's estimator    η  ^     normal-^  η    \hat{\eta}   of   η   η   \eta   minimizes the function     Q   (  η  )    =    ∫   -  π   π       I   (  w  )     f   (  w  ;   (  1  ,  η  )   )      d  w          Q  η     superscript   subscript     π    π         I  w     f   w   1  η      d  w      Q(\eta)=\int_{-\pi}^{\pi}\frac{I(w)}{f(w;(1,\eta))}\,dw   , where I(w) denotes the periodogram of X as      (   2  π  n   )    -  1      |    ∑   j  =  1   n     X  j    e   i  w  j      |   2        superscript    2  π  n     1     superscript      superscript   subscript     j  1    n      subscript  X  j    superscript  e    i  w  j       2     (2\pi n)^{-1}|\sum_{j=1}^{n}X_{j}e^{iwj}|^{2}   and      σ  ^   2   =    ∫   -  π   π       I   (  w  )     f   (  w  ;   (  1  ,   η  ^   )   )      d  w         superscript   normal-^  σ   2     superscript   subscript     π    π         I  w     f   w   1   normal-^  η       d  w      \hat{\sigma}^{2}=\int_{-\pi}^{\pi}\frac{I(w)}{f(w;(1,\hat{\eta}))}\,dw   . These integrations can be assessed by Riemann sum. {{-}} Then     n   1  /  2     (    θ  ^   -  θ   )        superscript  n    1  2       normal-^  θ   θ     n^{1/2}(\hat{\theta}-\theta)   asymptotically follows a normal distribution if    X  j     subscript  X  j    X_{j}   can be expressed as a form of an infinite moving average model.{{-}} {{-}} To estimate   H   H   H   , first, one has to calculate this periodogram. Since     I  n    (  w  )        subscript  I  n   w    I_{n}(w)   is an estimator of the spectral density, a series with long-range dependence should have a periodogram, which is proportional to     |  λ  |    1  -   2  H       superscript    λ     1    2  H      |\lambda|^{1-2H}   close to the origin. The periodogram plot is obtained by ploting    log   (    I  n    (  w  )    )          subscript  I  n   w     \log(I_{n}(w))   against    log   (  w  )       w    \log(w)   . Then fitting a regression model of the    log   (    I  n    (  w  )    )          subscript  I  n   w     \log(I_{n}(w))   on the    log   (  w  )       w    \log(w)   should give a slope of    β  ^     normal-^  β    \hat{\beta}   . The slope of the fitted straight line is also the estimation of    1  -   2  H       1    2  H     1-2H   . Thus, the estimation    H  ^     normal-^  H    \hat{H}   is obtained.  Note: There are two common problems when we apply the periodogram method. First, if the data does not follow a Gaussian distribution, transformation of the data can solve this kind of problems. Second, the sample spectrum which deviates from the assumed spectral density is another one. An aggregation method is suggested to solve this problem. If   X   X   X   is a Gaussian process and the spectral density function of   X   X   X   satisfies     w   -  γ    L   (  w  )        superscript  w    γ    L  w    w^{-\gamma}L(w)   as    w  →  ∞     normal-→  w     w\to\infty   , the function,        m   -  H     L   -   1  2      (  m  )     ∑   i  =     (   j  -  1   )   m   +  1    m    k   (    X  i   -   E   (   |   X  i   |   )     )      ,  j   =  1   ,   2  ,  …  ,   [   n  m   ]       formulae-sequence        superscript  m    H     superscript  L      1  2     m    superscript   subscript     i        j  1   m   1     m     k     subscript  X  i     E     subscript  X  i         j   1    2  normal-…   delimited-[]    n  m       m^{-H}L^{-\frac{1}{2}}(m)\sum_{i=(j-1)m+1}^{m}k(X_{i}-E(|X_{i}|)),~{}j=1,2,%
 \ldots,[\tfrac{n}{m}]   , converges in distribution to FGN as    m  →  ∞     normal-→  m     m\to\infty   .  References   P. Whittle, "Estimation and information in stationary time series", Art. Mat. 2, 423-434, 1953.  K. PARK,W. WILLINGER, Self-Similar Network Traffic and Performance Evaluation, WILEY,2000.  W. E. Leland, W. Willinger, M. S. Taqqu, D. V. Wilson, "On the self-similar nature of Ethernet traffic",ACM SIGCOMM Computer Communication Review 25,202-213,1995.  W. Willinger, M. S. Taqqu, W. E. Leland, D. V. Wilson, "Self-Similarity in High-Speed Packet Traffic: Analysis and Modeling of Ethernet Traffic Measurements",Statistical Science 10,67-85,1995.   "  Category:Computer network analysis     W. E. Leland, W. Willinger, M. S. Taqqu, D. V. Wilson, "On the self-similar nature of Ethernet traffic",ACM SIGCOMM Computer Communication Review 25,202-213,1995. ↩     