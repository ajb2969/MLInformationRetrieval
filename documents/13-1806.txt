   Least squares support vector machine      Least squares support vector machine   Least squares support vector machines (LS-SVM) are least squares versions of support vector machines (SVM), which are a set of related supervised learning methods that analyze data and recognize patterns, and which are used for classification and regression analysis . In this version one finds the solution by solving a set of linear equations instead of a convex quadratic programming (QP) problem for classical SVMs. Least squares SVM classifiers, were proposed by Suykens and Vandewalle. 1 LS-SVMs are a class of kernel-based learning methods .  From support vector machine to least squares support vector machine  Given a training set     {   x  i   ,   y  i   }    i  =  1   N     superscript   subscript    subscript  x  i    subscript  y  i      i  1    N    \{x_{i},y_{i}\}_{i=1}^{N}   with input data     x  i   ∈   ℝ  n        subscript  x  i    superscript  ℝ  n     x_{i}\in\mathbb{R}^{n}   and corresponding binary class labels     y  i   ∈   {   -  1   ,   +  1   }        subscript  y  i      1     1      y_{i}\in\{-1,+1\}   , the SVM 2 classifier, according to Vapnik ’s original formulation, satisfies the following conditions:  (Figure)  The spiral data     y  i   =  1       subscript  y  i   1    y_{i}=1   for blue data point     y  i   =   -  1        subscript  y  i     1     y_{i}=-1   for red data point       {          w  T   ϕ   (   x  i   )    +  b   ≥  1   ,        if   y  i    =   +  1    ,            w  T   ϕ   (   x  i   )    +  b   ≤   -  1    ,       if   y  i    =   -  1.          cases         superscript  w  T   ϕ   subscript  x  i    b   1      if   subscript  y  i      1           superscript  w  T   ϕ   subscript  x  i    b     1       if   subscript  y  i      1.      \begin{cases}w^{T}\phi(x_{i})+b\geq 1,&\text{if }\quad y_{i}=+1,\\
 w^{T}\phi(x_{i})+b\leq-1,&\text{if }\quad y_{i}=-1.\end{cases}     Which is equivalent to          y  i    [     w  T   ϕ   (   x  i   )    +  b   ]    ≥  1   ,   i  =   1  ,  …  ,   N      ,     formulae-sequence       subscript  y  i    delimited-[]       superscript  w  T   ϕ   subscript  x  i    b     1     i   1  normal-…  N      y_{i}\left[{w^{T}\phi(x_{i})+b}\right]\geq 1,\quad i=1,\ldots,N\,,     where    ϕ   (  x  )       ϕ  x    \phi(x)   is the nonlinear map from original space to the high (and possibly infinite) dimensional space.  Inseparable data  In case such a separating hyperplane does not exist, we introduce so-called slack variables    ξ  i     subscript  ξ  i    \xi_{i}   such that      {         y  i    [     w  T   ϕ   (   x  i   )    +  b   ]    ≥   1  -   ξ  i     ,       i  =   1  ,  …  ,  N    ,          ξ  i   ≥  0   ,       i  =   1  ,  …  ,  N    .         cases       subscript  y  i    delimited-[]       superscript  w  T   ϕ   subscript  x  i    b       1   subscript  ξ  i       i   1  normal-…  N       subscript  ξ  i   0     i   1  normal-…  N      \begin{cases}y_{i}\left[{w^{T}\phi(x_{i})+b}\right]\geq 1-\xi_{i},&i=1,\ldots,%
 N,\\
 \xi_{i}\geq 0,&i=1,\ldots,N.\end{cases}     According to the structural risk minimization principle, the risk bound is minimized by the following minimization problem:         min   J  1     (  w  ,  ξ  )    =     1  2    w  T   w   +   c    ∑   i  =  1   N    ξ  i       ,           subscript  J  1     w  ξ          1  2    superscript  w  T   w     c    superscript   subscript     i  1    N    subscript  ξ  i        \min J_{1}(w,\xi)=\frac{1}{2}w^{T}w+c\sum\limits_{i=1}^{N}{\xi_{i}},         Subject to   {         y  i    [     w  T   ϕ   (   x  i   )    +  b   ]    ≥   1  -   ξ  i     ,       i  =   1  ,  …  ,  N    ,          ξ  i   ≥  0   ,       i  =   1  ,  …  ,  N    ,           Subject to   cases       subscript  y  i    delimited-[]       superscript  w  T   ϕ   subscript  x  i    b       1   subscript  ξ  i       i   1  normal-…  N       subscript  ξ  i   0     i   1  normal-…  N       \text{Subject to }\begin{cases}y_{i}\left[{w^{T}\phi(x_{i})+b}\right]\geq 1-%
 \xi_{i},&i=1,\ldots,N,\\
 \xi_{i}\geq 0,&i=1,\ldots,N,\end{cases}     To solve this problem, we could construct the Lagrangian function :         L  1    (  w  ,  b  ,  ξ  ,  α  ,  β  )    =     1  2    w  T   w   +   c    ∑   i  =  1   N    ξ  i     +    ∑   i  =  1   N     α  i    {      y  i    [     w  T   ϕ   (   x  i   )    +  b   ]    -  1   +   ξ  i    }     +    ∑   i  =  1   N     β  i    ξ  i       ,         subscript  L  1    w  b  ξ  α  β          1  2    superscript  w  T   w     c    superscript   subscript     i  1    N    subscript  ξ  i       superscript   subscript     i  1    N      subscript  α  i           subscript  y  i    delimited-[]       superscript  w  T   ϕ   subscript  x  i    b     1    subscript  ξ  i         superscript   subscript     i  1    N      subscript  β  i    subscript  ξ  i        L_{1}(w,b,\xi,\alpha,\beta)=\frac{1}{2}w^{T}w+c\sum\limits_{i=1}^{N}{\xi_{i}}+%
 \sum\limits_{i=1}^{N}\alpha_{i}\left\{y_{i}\left[{w^{T}\phi(x_{i})+b}\right]-1%
 +\xi_{i}\right\}+\sum\limits_{i=1}^{N}\beta_{i}\xi_{i},     where     α  i   ≥  0  ,   β  i   ≥   0    (  i  =  1  ,  …  ,  N  )      fragments   subscript  α  i    0  normal-,   subscript  β  i    0   fragments  normal-(  i   1  normal-,  normal-…  normal-,  N  normal-)     \alpha_{i}\geq 0,{\rm}\beta_{i}\geq 0\;(i=1,\ldots,N)   are the Lagrangian multipliers . The optimal point will be in the saddle point of the Lagrangian function, and then we obtain      {           ∂   L  1     ∂  w     =   0  →     w  =     ∑   i  =  1   N      α  i    y  i   ϕ   (   x  i   )       ,              ∂   L  1     ∂  b     =   0  →        ∑   i  =  1   N      α  i    y  i     =  0    ,               ∂   L  1     ∂   ξ  i      =   0  →     0  ≤   α  i   ≤  c    ,   i  =   1  ,  …  ,  N     .          cases   formulae-sequence         subscript  L  1      w     0  normal-→      w    superscript   subscript     i  1    N      subscript  α  i    subscript  y  i   ϕ   subscript  x  i       otherwise   formulae-sequence         subscript  L  1      b     0  normal-→        superscript   subscript     i  1    N      subscript  α  i    subscript  y  i     0    otherwise   formulae-sequence   formulae-sequence         subscript  L  1       subscript  ξ  i      0  normal-→        0   subscript  α  i        c       i   1  normal-…  N     otherwise    \begin{cases}\frac{\partial L_{1}}{\partial w}=0\quad\to\quad w=\sum\limits_{i%
 =1}^{N}\alpha_{i}y_{i}\phi(x_{i}),\\
 \frac{\partial L_{1}}{\partial b}=0\quad\to\quad\sum\limits_{i=1}^{N}\alpha_{i%
 }y_{i}=0,\\
 \frac{\partial L_{1}}{\partial\xi_{i}}=0\quad\to\quad 0\leq\alpha_{i}\leq c,\;%
 i=1,\ldots,N.\end{cases}     By substituting   w   w   w   by its expression in the Lagrangian formed from the appropriate objective and constraints, we will get the following quadratic programming problem:         max    Q  1     (  α  )    =    -    1  2     ∑    i  ,  j   =  1   N     α  i    α  j    y  i    y  j   K   (   x  i   ,   x  j   )       +    ∑   i  =  1   N    α  i              subscript  Q  1    α           1  2     superscript   subscript      i  j   1    N      subscript  α  i    subscript  α  j    subscript  y  i    subscript  y  j   K    subscript  x  i    subscript  x  j          superscript   subscript     i  1    N    subscript  α  i       \max\;Q_{1}(\alpha)\;=-\frac{1}{2}\sum\limits_{i,j=1}^{N}{\alpha_{i}\alpha_{j}%
 y_{i}y_{j}K(x_{i},x_{j})}+\sum\limits_{i=1}^{N}{\alpha_{i}}     where     K   (   x  i   ,   x  j   )    =   ⟨   ϕ   (   x  i   )    ,   ϕ   (   x  j   )    ⟩         K    subscript  x  i    subscript  x  j        ϕ   subscript  x  i      ϕ   subscript  x  j       K(x_{i},x_{j})=\left\langle{\phi(x_{i}),\phi(x_{j})}\right\rangle   is called the kernel function . Solving this QP problem subject to constraints in (8), we will get the hyperplane in the high-dimensional space and hence the classifier in the original space.  Least squares SVM formulation  The least squares version of the SVM classifier is obtained by reformulating the minimization problem as:         min   J  2     (  w  ,  b  ,  e  )    =     μ  2    w  T   w   +    ζ  2     ∑   i  =  1   N    e   c  ,  i   2       ,           subscript  J  2     w  b  e          μ  2    superscript  w  T   w       ζ  2     superscript   subscript     i  1    N    superscript   subscript  e   c  i    2        \min J_{2}(w,b,e)=\frac{\mu}{2}w^{T}w+\frac{\zeta}{2}\sum\limits_{i=1}^{N}{e_{%
 c,i}^{2}},     subject to the equality constraints:          y  i    [     w  T   ϕ   (   x  i   )    +  b   ]    =   1  -   e   c  ,  i      ,   i  =   1  ,  …  ,  N     .     formulae-sequence       subscript  y  i    delimited-[]       superscript  w  T   ϕ   subscript  x  i    b       1   subscript  e   c  i        i   1  normal-…  N      y_{i}\left[{w^{T}\phi(x_{i})+b}\right]=1-e_{c,i},\quad i=1,\ldots,N.     The least squares SVM (LS-SVM) classifier formulation above implicitly corresponds to a regression interpretation with binary targets     y  i   =   ±  1        subscript  y  i    plus-or-minus  1     y_{i}=\pm 1   .  Using     y  i  2   =  1       superscript   subscript  y  i   2   1    y_{i}^{2}=1   , we have         ∑   i  =  1   N    e   c  ,  i   2    =    ∑   i  =  1   N     (    y  i    e   c  ,  i     )   2    =    ∑   i  =  1   N    e  i  2    =    ∑   i  =  1   N     (    y  i   -   (     w  T   ϕ   (   x  i   )    +  b   )    )   2     ,          superscript   subscript     i  1    N    superscript   subscript  e   c  i    2      superscript   subscript     i  1    N    superscript     subscript  y  i    subscript  e   c  i     2           superscript   subscript     i  1    N    superscript   subscript  e  i   2           superscript   subscript     i  1    N    superscript     subscript  y  i        superscript  w  T   ϕ   subscript  x  i    b    2       \sum\limits_{i=1}^{N}{e_{c,i}^{2}}=\sum\limits_{i=1}^{N}{(y_{i}e_{c,i})^{2}}=%
 \sum\limits_{i=1}^{N}{e_{i}^{2}}=\sum\limits_{i=1}^{N}{\left({y_{i}-(w^{T}\phi%
 (x_{i})+b)}\right)}^{2},     with      e  i   =    y  i   -   (     w  T   ϕ   (   x  i   )    +  b   )     .       subscript  e  i      subscript  y  i        superscript  w  T   ϕ   subscript  x  i    b      e_{i}=y_{i}-(w^{T}\phi(x_{i})+b).   Notice, that this error would also make sense for least squares data fitting, so that the same end results holds for the regression case.  Hence the LS-SVM classifier formulation is equivalent to         J   2    (  w  ,  b  ,  e  )    =    μ   E  W    +   ζ   E  D            subscript  J  2    w  b  e        μ   subscript  E  W      ζ   subscript  E  D       \;J_{2}(w,b,e)=\mu E_{W}+\zeta E_{D}     with     E  W   =    1  2    w  T   w        subscript  E  W       1  2    superscript  w  T   w     E_{W}=\frac{1}{2}w^{T}w   and      E  D   =    1  2     ∑   i  =  1   N    e  i  2     =    1  2     ∑   i  =  1   N     (    y  i   -   (     w  T   ϕ   (   x  i   )    +  b   )    )   2      .         subscript  E  D       1  2     superscript   subscript     i  1    N    superscript   subscript  e  i   2              1  2     superscript   subscript     i  1    N    superscript     subscript  y  i        superscript  w  T   ϕ   subscript  x  i    b    2        E_{D}=\frac{1}{2}\sum\limits_{i=1}^{N}{e_{i}^{2}}=\frac{1}{2}\sum\limits_{i=1}%
 ^{N}{\left({y_{i}-(w^{T}\phi(x_{i})+b)}\right)}^{2}.     (Figure)  The result of the LS-SVM classifier   Both   μ   μ   \mu   and   ζ   ζ   \zeta   should be considered as hyperparameters to tune the amount of regularization versus the sum squared error. The solution does only depend on the ratio    γ  =   ζ  /  μ       γ    ζ  μ     \gamma=\zeta/\mu   , therefore the original formulation uses only   γ   γ   \gamma   as tuning parameter. We use both   μ   μ   \mu   and   ζ   ζ   \zeta   as parameters in order to provide a Bayesian interpretation to LS-SVM.  The solution of LS-SVM regressor will be obtained after we construct the Lagrangian function :      {         L  2    (  w  ,  b  ,  e  ,  α  )    =     J  2    (  w  ,  e  )    -     ∑   i  =  1   N      α  i    {     [     w  T   ϕ   (   x  i   )    +  b   ]   +   e  i    -   y  i    }       ,           =       1  2     w  T   w   +     γ  2       ∑   i  =  1   N     e  i  2      -     ∑   i  =  1   N      α  i    {     [     w  T   ϕ   (   x  i   )    +  b   ]   +   e  i    -   y  i    }       ,          cases       subscript  L  2    w  b  e  α         subscript  J  2    w  e      superscript   subscript     i  1    N      subscript  α  i         delimited-[]       superscript  w  T   ϕ   subscript  x  i    b     subscript  e  i     subscript  y  i         otherwise    absent          1  2    superscript  w  T   w       γ  2     superscript   subscript     i  1    N    superscript   subscript  e  i   2        superscript   subscript     i  1    N      subscript  α  i         delimited-[]       superscript  w  T   ϕ   subscript  x  i    b     subscript  e  i     subscript  y  i         otherwise    \begin{cases}L_{2}(w,b,e,\alpha)\;=J_{2}(w,e)-\sum\limits_{i=1}^{N}\alpha_{i}%
 \left\{{\left[{w^{T}\phi(x_{i})+b}\right]+e_{i}-y_{i}}\right\},\\
 \quad\quad\quad\quad\quad\;=\frac{1}{2}w^{T}w+\frac{\gamma}{2}\sum\limits_{i=1%
 }^{N}e_{i}^{2}-\sum\limits_{i=1}^{N}\alpha_{i}\left\{\left[w^{T}\phi(x_{i})+b%
 \right]+e_{i}-y_{i}\right\},\end{cases}     where     α  i   ∈  ℝ       subscript  α  i   ℝ    \alpha_{i}\in\mathbb{R}   are the Lagrange multipliers. The conditions for optimality are      {           ∂   L  2     ∂  w     =   0  →     w  =     ∑   i  =  1   N      α  i   ϕ   (   x  i   )       ,              ∂   L  2     ∂  b     =   0  →        ∑   i  =  1   N     α  i    =  0    ,              ∂   L  2     ∂   e  i      =   0  →       α  i   =   γ   e  i     ,   i  =   1  ,  …  ,  N      ,              ∂   L  2     ∂   α  i      =   0  →       y  i   =     w  T   ϕ   (   x  i   )    +  b  +   e  i     ,   i  =   1  ,  …  ,  N      .          cases   formulae-sequence         subscript  L  2      w     0  normal-→      w    superscript   subscript     i  1    N      subscript  α  i   ϕ   subscript  x  i       otherwise   formulae-sequence         subscript  L  2      b     0  normal-→        superscript   subscript     i  1    N    subscript  α  i    0    otherwise   formulae-sequence         subscript  L  2       subscript  e  i      0  normal-→     formulae-sequence     subscript  α  i     γ   subscript  e  i       i   1  normal-…  N      otherwise   formulae-sequence         subscript  L  2       subscript  α  i      0  normal-→     formulae-sequence     subscript  y  i        superscript  w  T   ϕ   subscript  x  i    b   subscript  e  i       i   1  normal-…  N      otherwise    \begin{cases}\frac{\partial L_{2}}{\partial w}=0\quad\to\quad w=\sum\limits_{i%
 =1}^{N}\alpha_{i}\phi(x_{i}),\\
 \frac{\partial L_{2}}{\partial b}=0\quad\to\quad\sum\limits_{i=1}^{N}\alpha_{i%
 }=0,\\
 \frac{\partial L_{2}}{\partial e_{i}}=0\quad\to\quad\alpha_{i}=\gamma e_{i},\;%
 i=1,\ldots,N,\\
 \frac{\partial L_{2}}{\partial\alpha_{i}}=0\quad\to\quad y_{i}=w^{T}\phi(x_{i}%
 )+b+e_{i},\,i=1,\ldots,N.\end{cases}     Elimination of   w   w   w   and   e   e   e   will yield a linear system instead of a quadratic programming problem:         [     0     1  N  T        1  N      Ω  +    γ   -  1     I  N        ]    [     b      α     ]    =   [     0      Y     ]    ,         delimited-[]    0   superscript   subscript  1  N   T      subscript  1  N     normal-Ω     superscript  γ    1     subscript  I  N         delimited-[]    b    α       delimited-[]    0    Y       \left[\begin{matrix}0&1_{N}^{T}\\
 1_{N}&\Omega+\gamma^{-1}I_{N}\end{matrix}\right]\left[\begin{matrix}b\\
 \alpha\end{matrix}\right]=\left[\begin{matrix}0\\
 Y\end{matrix}\right],     with    Y  =    [   y  1   ,  …  ,   y  N   ]   T       Y   superscript    subscript  y  1   normal-…   subscript  y  N    T     Y=[y_{1},\ldots,y_{N}]^{T}   ,     1  N   =    [  1  ,  …  ,  1  ]   T        subscript  1  N    superscript   1  normal-…  1   T     1_{N}=[1,\ldots,1]^{T}   and    α  =    [   α  1   ,  …  ,   α  N   ]   T       α   superscript    subscript  α  1   normal-…   subscript  α  N    T     \alpha=[\alpha_{1},\ldots,\alpha_{N}]^{T}   . Here,    I  N     subscript  I  N    I_{N}   is an    N  ×  N      N  N    N\times N    identity matrix , and    Ω  ∈   ℝ   N  ×  N        normal-Ω   superscript  ℝ    N  N      \Omega\in\mathbb{R}^{N\times N}   is the kernel matrix defined by     Ω   i  j    =   ϕ    (   x  i   )   T   ϕ   (   x  j   )    =   K   (   x  i   ,   x  j   )           subscript  normal-Ω    i  j      ϕ   superscript   subscript  x  i   T   ϕ   subscript  x  j           K    subscript  x  i    subscript  x  j        \Omega_{ij}=\phi(x_{i})^{T}\phi(x_{j})=K(x_{i},x_{j})   .  Kernel function K  For the kernel function K (•, •) one typically has the following choices:   Linear kernel      K   (  x  ,   x  i   )    =    x  i  T   x    ,        K   x   subscript  x  i        superscript   subscript  x  i   T   x     K(x,x_{i})=x_{i}^{T}x,     Polynomial kernel of degree   d   d   d         K   (  x  ,   x  i   )    =    (   1  +     x  i  T   x   /  c    )   d    ,        K   x   subscript  x  i      superscript    1       superscript   subscript  x  i   T   x   c    d     K(x,x_{i})=\left({1+x_{i}^{T}x/c}\right)^{d},     Radial basis function RBF kernel      K   (  x  ,   x  i   )    =   exp   (   -     ∥   x  -   x  i    ∥   2   /   σ  2     )     ,        K   x   subscript  x  i            superscript   norm    x   subscript  x  i     2    superscript  σ  2        K(x,x_{i})=\exp\left({-\left\|{x-x_{i}}\right\|^{2}/\sigma^{2}}\right),     MLP kernel      K   (  x  ,   x  i   )    =   tanh   (     k    x  i  T   x   +  θ   )     ,        K   x   subscript  x  i           k   superscript   subscript  x  i   T   x   θ      K(x,x_{i})=\tanh\left({k\,x_{i}^{T}x+\theta}\right),      where   d   d   d   ,   c   c   c   ,   σ   σ   \sigma   ,   k   k   k   and   θ   θ   \theta   are constants. Notice that the Mercer condition holds for all     c  ,  σ   ∈   ℝ  +        c  σ    superscript  ℝ      c,\sigma\in\mathbb{R}^{+}   and    d  ∈  N      d  N    d\in N   values in the polynomial and RBF case, but not for all possible choices of   k   k   k   and   θ   θ   \theta   in the MLP case. The scale parameters   c   c   c   ,   σ   σ   \sigma   and   k   k   k   determine the scaling of the inputs in the polynomial, RBF and MLP kernel function . This scaling is related to the bandwidth of the kernel in statistics , where it is shown that the bandwidth is an important parameter of the generalization behavior of a kernel method.  Bayesian interpretation for LS-SVM  A Bayesian interpretation of the SVM has been proposed by Smola et al. They showed that the use of different kernels in SVM can be regarded as defining different prior probability distributions on the functional space, as     P   [  f  ]    ∝   exp   (   -   β    ∥    P  ^   f   ∥   2     )       proportional-to    P   delimited-[]  f          β   superscript   norm     normal-^  P   f    2        P[f]\propto\exp\left({-\beta\left\|{\hat{P}f}\right\|^{2}}\right)   . Here    β  >  0      β  0    \beta>0   is a constant and    P  ^     normal-^  P    \hat{P}   is the regularization operator corresponding to the selected kernel.  A general Bayesian evidence framework was developed by MacKay, 3 4 5 and MacKay has used it to the problem of regression, forward neural network and classification network. Provided data set   D   D   D   , a model   𝕄   𝕄   \mathbb{M}   with parameter vector   w   w   w   and a so-called hyperparameter or regularization parameter   λ   λ   \lambda   , Bayesian inference is constructed with 3 levels of inference:   In level 1, for a given value of   λ   λ   \lambda   , the first level of inference infers the posterior distribution of   w   w   w   by Bayesian rule     p(w|D,\lambda ,\mathbb{M}) \propto p(D|w,\mathbb{M})p(w|\lambda ,\mathbb{M})       The second level of inference determines the value of   λ   λ   \lambda   , by maximizing         p   (  λ  |  D  ,  𝕄  )   ∝  p   (  D  |  λ  ,  𝕄  )   p   (  λ  |  𝕄  )      fragments  p   fragments  normal-(  λ  normal-|  D  normal-,  M  normal-)   proportional-to  p   fragments  normal-(  D  normal-|  λ  normal-,  M  normal-)   p   fragments  normal-(  λ  normal-|  M  normal-)     p(\lambda|D,\mathbb{M})\propto p(D|\lambda,\mathbb{M})p(\lambda|\mathbb{M})         The third level of inference in the evidence framework ranks different models by examining their posterior probabilities     p(\mathbb{M}|D) \propto p(D|\mathbb{M})p(\mathbb{M}).      We can see that Bayesian evidence framework is a unified theory for learning the model and model selection. Kwok used the Bayesian evidence framework to interpret the formulation of SVM and model selection. And he also applied Bayesian evidence framework to support vector regression.  Now, given the data points     {   x  i   ,   y  i   }    i  =  1   N     superscript   subscript    subscript  x  i    subscript  y  i      i  1    N    \{x_{i},y_{i}\}_{i=1}^{N}   and the hyperparameters   μ   μ   \mu   and   ζ   ζ   \zeta   of the model   𝕄   𝕄   \mathbb{M}   , the model parameters   w   w   w   and   b   b   b   are estimated by maximizing the posterior    p   (  w  ,  b  |  D  ,  log  μ  ,  log  ζ  ,  𝕄  )      fragments  p   fragments  normal-(  w  normal-,  b  normal-|  D  normal-,   μ  normal-,   ζ  normal-,  M  normal-)     p(w,b|D,\log\mu,\log\zeta,\mathbb{M})   . Applying Bayes’ rule, we obtain:      p   (  w  ,  b  |  D  ,  log  μ  ,  log  ζ  ,  𝕄  )   =    p   (  D  |  w  ,  b  ,  log  μ  ,  log  ζ  ,  𝕄  )   p   (  w  ,  b  |  log  μ  ,  log  ζ  ,  𝕄  )     p   (  D  |  log  μ  ,  log  ζ  ,  𝕄  )     .     fragments  p   fragments  normal-(  w  normal-,  b  normal-|  D  normal-,   μ  normal-,   ζ  normal-,  M  normal-)       fragments  p   fragments  normal-(  D  normal-|  w  normal-,  b  normal-,   μ  normal-,   ζ  normal-,  M  normal-)   p   fragments  normal-(  w  normal-,  b  normal-|   μ  normal-,   ζ  normal-,  M  normal-)     fragments  p   fragments  normal-(  D  normal-|   μ  normal-,   ζ  normal-,  M  normal-)     normal-.    p(w,b|D,\log\mu,\log\zeta,\mathbb{M})=\frac{{p(D|w,b,\log\mu,\log\zeta,\mathbb%
 {M})p(w,b|\log\mu,\log\zeta,\mathbb{M})}}{{p(D|\log\mu,\log\zeta,\mathbb{M})}}.     Where    p   (  D  |  log  μ  ,  log  ζ  ,  𝕄  )      fragments  p   fragments  normal-(  D  normal-|   μ  normal-,   ζ  normal-,  M  normal-)     p(D|\log\mu,\log\zeta,\mathbb{M})   is a normalizing constant such the integral over all possible   w   w   w   and   b   b   b   is equal to 1. We assume   w   w   w   and   b   b   b   are independent of the hyperparameter   ζ   ζ   \zeta   , and are conditional independent, i.e., we assume      p   (  w  ,  b  |  log  μ  ,  log  ζ  ,  𝕄  )   =  p   (  w  |  log  μ  ,  𝕄  )   p   (  b  |  log   σ  b   ,  𝕄  )   .     fragments  p   fragments  normal-(  w  normal-,  b  normal-|   μ  normal-,   ζ  normal-,  M  normal-)    p   fragments  normal-(  w  normal-|   μ  normal-,  M  normal-)   p   fragments  normal-(  b  normal-|    subscript  σ  b   normal-,  M  normal-)   normal-.    p(w,b|\log\mu,\log\zeta,\mathbb{M})=p(w|\log\mu,\mathbb{M})p(b|\log\sigma_{b},%
 \mathbb{M}).     When     σ  b   →  ∞     normal-→   subscript  σ  b      \sigma_{b}\to\infty   , the distribution of   b   b   b   will approximate a uniform distribution. Furthermore, we assume   w   w   w   and   b   b   b   are Gaussian distribution, so we obtain the a priori distribution of   w   w   w   and   b   b   b   with     σ  b   →  ∞     normal-→   subscript  σ  b      \sigma_{b}\to\infty   to be:          p   (  w  ,  b  |  log  μ  ,  )   =    (   μ   2  π    )     n  f   2    exp   (  -   μ  2    w  T   w  )    1    2  π   σ  b      exp   (  -    b  2    2   σ  b     )          ∝     (   μ   2  π    )     n  f   2     exp   (   -    μ  2    w  T   w    )         .       fragments  p   fragments  normal-(  w  normal-,  b  normal-|   μ  normal-,  normal-)     superscript   fragments  normal-(    μ    2  π    normal-)      subscript  n  f   2      fragments  normal-(     μ  2    superscript  w  T   w  normal-)     1      2  π   subscript  σ  b        fragments  normal-(      superscript  b  2     2   subscript  σ  b     normal-)       proportional-to  absent     superscript    μ    2  π       subscript  n  f   2            μ  2    superscript  w  T   w          \begin{array}[]{l}p(w,b|\log\mu,)=\left({\frac{\mu}{{2\pi}}}\right)^{\frac{{n_%
 {f}}}{2}}\exp\left({-\frac{\mu}{2}w^{T}w}\right)\frac{1}{{\sqrt{2\pi\sigma_{b}%
 }}}\exp\left({-\frac{{b^{2}}}{{2\sigma_{b}}}}\right)\\
 \quad\quad\quad\quad\quad\quad\quad\propto\left({\frac{\mu}{{2\pi}}}\right)^{%
 \frac{{n_{f}}}{2}}\exp\left({-\frac{\mu}{2}w^{T}w}\right)\end{array}.     Here    n  f     subscript  n  f    n_{f}   is the dimensionality of the feature space, same as the dimensionality of   w   w   w   .  The probability of    p   (  D  |  w  ,  b  ,  log  μ  ,  log  ζ  ,  𝕄  )      fragments  p   fragments  normal-(  D  normal-|  w  normal-,  b  normal-,   μ  normal-,   ζ  normal-,  M  normal-)     p(D|w,b,\log\mu,\log\zeta,\mathbb{M})   is assumed to depend only on    w  ,  b  ,  ζ     w  b  ζ    w,b,\zeta   and   𝕄   𝕄   \mathbb{M}   . We assume that the data points are independently identically distributed (i.i.d.), so that:      p   (  D  |  w  ,  b  ,  log  ζ  ,  𝕄  )   =   ∏   i  =  1   N   p   (   x  i   ,   y  i   |  w  ,  b  ,  log  ζ  ,  𝕄  )   .     fragments  p   fragments  normal-(  D  normal-|  w  normal-,  b  normal-,   ζ  normal-,  M  normal-)     superscript   subscript  product    i  1    N   p   fragments  normal-(   subscript  x  i   normal-,   subscript  y  i   normal-|  w  normal-,  b  normal-,   ζ  normal-,  M  normal-)   normal-.    p(D|w,b,\log\zeta,\mathbb{M})=\prod\limits_{i=1}^{N}{p(x_{i},y_{i}|w,b,\log%
 \zeta,\mathbb{M})}.     In order to obtain the least square cost function, it is assumed that the probability of a data point is proportional to:      p   (   x  i   ,   y  i   |  w  ,  b  ,  log  ζ  ,  𝕄  )   ∝  p   (   e  i   |  w  ,  b  ,  log  ζ  ,  𝕄  )   .     fragments  p   fragments  normal-(   subscript  x  i   normal-,   subscript  y  i   normal-|  w  normal-,  b  normal-,   ζ  normal-,  M  normal-)   proportional-to  p   fragments  normal-(   subscript  e  i   normal-|  w  normal-,  b  normal-,   ζ  normal-,  M  normal-)   normal-.    p(x_{i},y_{i}|w,b,\log\zeta,\mathbb{M})\propto p(e_{i}|w,b,\log\zeta,\mathbb{M%
 }).     A Gaussian distribution is taken for the errors     e  i   =    y  i   -   (     w  T   ϕ   (   x  i   )    +  b   )         subscript  e  i      subscript  y  i        superscript  w  T   ϕ   subscript  x  i    b      e_{i}=y_{i}-(w^{T}\phi(x_{i})+b)   as:      p   (   e  i   |  w  ,  b  ,  log  ζ  ,  𝕄  )   =    ζ   2  π     exp   (  -    ζ   e  i  2    2   )   .     fragments  p   fragments  normal-(   subscript  e  i   normal-|  w  normal-,  b  normal-,   ζ  normal-,  M  normal-)        ζ    2  π       fragments  normal-(       ζ   superscript   subscript  e  i   2    2   normal-)   normal-.    p(e_{i}|w,b,\log\zeta,\mathbb{M})=\sqrt{\frac{\zeta}{{2\pi}}}\exp\left({-\frac%
 {{\zeta e_{i}^{2}}}{2}}\right).     It is assumed that the   w   w   w   and   b   b   b   are determined in such a way that the class centers     m  ^   -     subscript   normal-^  m      \hat{m}_{-}   and     m  ^   +     subscript   normal-^  m      \hat{m}_{+}   are mapped onto the target -1 and +1, respectively. The projections      w  T   ϕ   (  x  )    +  b         superscript  w  T   ϕ  x   b    w^{T}\phi(x)+b   of the class elements    ϕ   (  x  )       ϕ  x    \phi(x)   follow a multivariate Gaussian distribution, which have variance    1  /  ζ      1  ζ    1/\zeta   .  Combining the preceding expressions, and neglecting all constants, Bayes’ rule becomes      p   (  w  ,  b  |  D  ,  log  μ  ,  log  ζ  ,  𝕄  )   ∝  exp   (  -   μ  2    w  T   w  -   ζ  2    ∑   i  =  1   N    e  i  2   )   =  exp   (  -   J  2    (  w  ,  b  )   )   .     fragments  p   fragments  normal-(  w  normal-,  b  normal-|  D  normal-,   μ  normal-,   ζ  normal-,  M  normal-)   proportional-to    fragments  normal-(     μ  2    superscript  w  T   w     ζ  2    superscript   subscript     i  1    N    superscript   subscript  e  i   2   normal-)      fragments  normal-(    subscript  J  2    fragments  normal-(  w  normal-,  b  normal-)   normal-)   normal-.    p(w,b|D,\log\mu,\log\zeta,\mathbb{M})\propto\exp(-\frac{\mu}{2}w^{T}w-\frac{%
 \zeta}{2}\sum\limits_{i=1}^{N}{e_{i}^{2}})=\exp(-J_{2}(w,b)).     The maximum posterior density estimates    w   M  P      subscript  w    M  P     w_{MP}   and    b   M  P      subscript  b    M  P     b_{MP}   are then be obtained by minimizing the negative logarithm of (26), so we arrive (10).  References    Bibliography   J. A. K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, J. Vandewalle, Least Squares Support Vector Machines, World Scientific Pub. Co., Singapore, 2002. ISBN 981-238-151-1  Suykens J.A.K., Vandewalle J., Least squares support vector machine classifiers, Neural Processing Letters , vol. 9, no. 3, Jun. 1999, pp. 293–300.  Vladimir Vapnik. The Nature of Statistical Learning Theory . Springer-Verlag, 1995. ISBN 0-387-98780-0  MacKay, D. J. C., Probable networks and plausible predictions—A review of practical Bayesian methods for supervised neural networks. Network: Computation in Neural Systems , vol. 6, 1995, pp. 469–505.   External links   www.esat.kuleuven.be/sista/lssvmlab/ "Least squares support vector machine Lab (LS-SVMlab) toolbox contains Matlab/C implementations for a number of LS-SVM algorithms."  www.kernel-machines.org "Support Vector Machines and Kernel based methods (Smola & Schölkopf)."  www.gaussianprocess.org "Gaussian Processes: Data modeling using Gaussian Process priors over functions for regression and classification (MacKay, Williams)"  www.support-vector.net "Support Vector Machines and kernel based methods (Cristianini)"  dlib : Contains a least-squares SVM implementation for large-scale datasets.   "  Category:Support vector machines  Category:Classification algorithms  Category:Statistical classification  Category:Least squares     Suykens, J.A.K.; Vandewalle, J. (1999) "Least squares support vector machine classifiers", Neural Processing Letters , 9 (3), 293-300. ↩  Vapnik, V. The nature of statistical learning theory. Springer-Verlag, New York, 1995 ↩  MacKay, D.J.C. Bayesian Interpolation. Neural Computation, 4(3): 415-447, May 1992. ↩  MacKay, D.J.C. A practical Bayesian framework for backpropagation networks. Neural Computation, 4(3): 448-472, May 1992. ↩  MacKay, D.J.C. The evidence framework applied to classification networks. Neural Computation, 4(5): 720-736, Sept. 1992. ↩     