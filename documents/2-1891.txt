   Rayleigh quotient      Rayleigh quotient   In mathematics , for a given complex Hermitian matrix  M and nonzero vector  x , the Rayleigh quotient 1     R   (  M  ,  x  )       R   M  x     R(M,x)   , is defined as: 2 3        R   (  M  ,  x  )    :=     x  *   M  x     x  *   x     .     assign    R   M  x         superscript  x    M  x      superscript  x    x      R(M,x):={x^{*}Mx\over x^{*}x}.     For real matrices and vectors, the condition of being Hermitian reduces to that of being symmetric , and the conjugate transpose     x  *     superscript  x     x^{*}   to the usual transpose     x  ′     superscript  x  normal-′    x^{\prime}   . Note that     R   (  M  ,   c  x   )    =   R   (  M  ,  x  )          R   M    c  x       R   M  x      R(M,cx)=R(M,x)   for any non-zero real scalar c . Recall that a Hermitian (or real symmetric) matrix has real eigenvalues . It can be shown that, for a given matrix, the Rayleigh quotient reaches its minimum value    λ  min     subscript  λ     \lambda_{\min}   (the smallest eigenvalue of M ) when x is    v  min     subscript  v     v_{\min}   (the corresponding eigenvector ). Similarly,     R   (  M  ,  x  )    ≤   λ  max         R   M  x     subscript  λ      R(M,x)\leq\lambda_{\max}   and     R   (  M  ,   v  max   )    =   λ  max         R   M   subscript  v       subscript  λ      R(M,v_{\max})=\lambda_{\max}   .  The Rayleigh quotient is used in the min-max theorem to get exact values of all eigenvalues. It is also used in eigenvalue algorithms to obtain an eigenvalue approximation from an eigenvector approximation. Specifically, this is the basis for Rayleigh quotient iteration .  The range of the Rayleigh quotient (for matrix that is not necessarily Hermitian) is called a numerical range , (or spectrum in functional analysis). When the matrix is Hermitian, the numerical range is equal to the spectral norm. Still in functional analysis,    λ  max     subscript  λ     \lambda_{\max}   is known as the spectral radius . In the context of C*-algebras or algebraic quantum mechanics, the function that to M associates the Rayleigh-Ritz quotient R(M,x) for a fixed x and M varying through the algebra would be referred to as "vector state" of the algebra.  Bounds for Hermitian   M   M   M     As stated in introduction     R   (  M  ,  x  )    ∈   [   λ  min   ,   λ  max   ]         R   M  x      subscript  λ     subscript  λ       R(M,x)\in\left[\lambda_{\min},\lambda_{\max}\right]   . This is immediate after observing that the Rayleigh quotient is a weighted average of eigenvalues of M :       R   (  M  ,  x  )    =     x  *   M  x     x  *   x    =     ∑   i  =  1   n     λ  i    y  i  2       ∑   i  =  1   n    y  i  2             R   M  x         superscript  x    M  x      superscript  x    x             superscript   subscript     i  1    n      subscript  λ  i    superscript   subscript  y  i   2       superscript   subscript     i  1    n    superscript   subscript  y  i   2        R(M,x)={x^{*}Mx\over x^{*}x}=\frac{\sum_{i=1}^{n}\lambda_{i}y_{i}^{2}}{\sum_{i%
 =1}^{n}y_{i}^{2}}     where    (   λ  i   ,   v  i   )      subscript  λ  i    subscript  v  i     (\lambda_{i},v_{i})   is the   i   i   i   th eigenpair after orthonormalization and     y  i   =    v  i  *   x        subscript  y  i      superscript   subscript  v  i     x     y_{i}=v_{i}^{*}x   is the   i   i   i   th coordinate of x in the eigenbasis. It is then easy to verify that the bounds are attained at the corresponding eigenvectors     v  min   ,   v  max       subscript  v     subscript  v      v_{\min},v_{\max}   .  The fact that the quotient is a weighted average of the eigenvalues can be used to identify the second, the third, ... largest eigenvalues. Let     λ   m  a  x    =   λ  1   ≥   λ  2   ≥  …  ≥   λ  n   =   λ   m  i  n           subscript  λ    m  a  x     subscript  λ  1         subscript  λ  2        normal-…        subscript  λ  n         subscript  λ    m  i  n       \lambda_{max}=\lambda_{1}\geq\lambda_{2}\geq...\geq\lambda_{n}=\lambda_{min}   be the eigenvalues in decreasing order. If   x   x   x   is constrained to be orthogonal to    v  1     subscript  v  1    v_{1}   , in which case     y  1   =    v  1  *   x   =  0         subscript  y  1      superscript   subscript  v  1     x        0     y_{1}=v_{1}^{*}x=0   , then    R   (  M  ,  x  )       R   M  x     R(M,x)   has the maximum    λ  2     subscript  λ  2    \lambda_{2}   , which is achieved when    x  =   v  2       x   subscript  v  2     x=v_{2}   .  Special case of covariance matrices  An empirical covariance matrix  M can be represented as the product A ' A of the data matrix  A pre-multiplied by its transpose A '. Being a positive semi-definite matrix, M has non-negative eigenvalues, and orthogonal (or othogonalisable) eigenvectors, which can be demonstrated as follows.  Firstly, that the eigenvalues    λ  i     subscript  λ  i    \lambda_{i}   are non-negative:       M   v  i    =    A  ′   A   v  i    =    λ  i    v  i            M   subscript  v  i       superscript  A  normal-′   A   subscript  v  i            subscript  λ  i    subscript  v  i       Mv_{i}=A^{\prime}Av_{i}=\lambda_{i}v_{i}          ⇒    v  i  ′    A  ′   A   v  i    =    v  i  ′    λ  i    v  i         normal-⇒  absent     superscript   subscript  v  i   normal-′    superscript  A  normal-′   A   subscript  v  i            superscript   subscript  v  i   normal-′    subscript  λ  i    subscript  v  i       \Rightarrow v_{i}^{\prime}A^{\prime}Av_{i}=v_{i}^{\prime}\lambda_{i}v_{i}          ⇒    ∥   A   v  i    ∥   2   =    λ  i     ∥   v  i   ∥   2         normal-⇒  absent   superscript   norm    A   subscript  v  i     2           subscript  λ  i    superscript   norm   subscript  v  i    2       \Rightarrow\left\|Av_{i}\right\|^{2}=\lambda_{i}\left\|v_{i}\right\|^{2}          ⇒   λ  i   =     ∥   A   v  i    ∥   2     ∥   v  i   ∥   2    ≥  0.       normal-⇒  absent   subscript  λ  i           superscript   norm    A   subscript  v  i     2    superscript   norm   subscript  v  i    2         0.     \Rightarrow\lambda_{i}=\frac{\left\|Av_{i}\right\|^{2}}{\left\|v_{i}\right\|^{%
 2}}\geq 0.     Secondly, that the eigenvectors v i are orthogonal to one another:        M    v  i    =    λ  i    v  i          M   subscript  v  i       subscript  λ  i    subscript  v  i      \displaystyle\qquad\qquad Mv_{i}=\lambda_{i}v_{i}     If the eigenvalues are different – in the case of multiplicity, the basis can be orthogonalized.  To now establish that the Rayleigh quotient is maximised by the eigenvector with the largest eigenvalue, consider decomposing an arbitrary vector x on the basis of the eigenvectors v i :       x  =    ∑   i  =  1   n     α  i    v  i      ,      x    superscript   subscript     i  1    n      subscript  α  i    subscript  v  i       x=\sum_{i=1}^{n}\alpha_{i}v_{i},     where       α  i   =     x  ′    v  i      v  i  ′    v  i     =    ⟨  x  ,   v  i   ⟩     ∥   v  i   ∥   2           subscript  α  i        superscript  x  normal-′    subscript  v  i       superscript   subscript  v  i   normal-′    subscript  v  i             x   subscript  v  i     superscript   norm   subscript  v  i    2       \alpha_{i}=\frac{x^{\prime}v_{i}}{v_{i}^{\prime}v_{i}}=\frac{\langle x,v_{i}%
 \rangle}{\left\|v_{i}\right\|^{2}}     is the coordinate of x orthogonally projected onto v i . Therefore we have:       R   (  M  ,  x  )    =     x  ′    A  ′   A  x     x  ′   x    =      (    ∑   j  =  1   n     α  j    v  j     )   ′    (    A  ′   A   )    (    ∑   i  =  1   n     α  i    v  i     )       (    ∑   j  =  1   n     α  j    v  j     )   ′    (    ∑   i  =  1   n     α  i    v  i     )             R   M  x         superscript  x  normal-′    superscript  A  normal-′   A  x      superscript  x  normal-′   x              superscript    superscript   subscript     j  1    n      subscript  α  j    subscript  v  j     normal-′      superscript  A  normal-′   A     superscript   subscript     i  1    n      subscript  α  i    subscript  v  i         superscript    superscript   subscript     j  1    n      subscript  α  j    subscript  v  j     normal-′     superscript   subscript     i  1    n      subscript  α  i    subscript  v  i          R(M,x)=\frac{x^{\prime}A^{\prime}Ax}{x^{\prime}x}=\frac{\left(\sum_{j=1}^{n}%
 \alpha_{j}v_{j}\right)^{\prime}\left(A^{\prime}A\right)\left(\sum_{i=1}^{n}%
 \alpha_{i}v_{i}\right)}{\left(\sum_{j=1}^{n}\alpha_{j}v_{j}\right)^{\prime}%
 \left(\sum_{i=1}^{n}\alpha_{i}v_{i}\right)}     which, by orthogonality of the eigenvectors, becomes:       R   (  M  ,  x  )    =     ∑   i  =  1   n     α  i  2    λ  i       ∑   i  =  1   n    α  i  2     =    ∑   i  =  1   n     λ  i      (    x  ′    v  i    )   2     (    x  ′   x   )    (    v  i  ′    v  i    )               R   M  x        superscript   subscript     i  1    n      superscript   subscript  α  i   2    subscript  λ  i       superscript   subscript     i  1    n    superscript   subscript  α  i   2            superscript   subscript     i  1    n      subscript  λ  i      superscript     superscript  x  normal-′    subscript  v  i    2        superscript  x  normal-′   x      superscript   subscript  v  i   normal-′    subscript  v  i           R(M,x)=\frac{\sum_{i=1}^{n}\alpha_{i}^{2}\lambda_{i}}{\sum_{i=1}^{n}\alpha_{i}%
 ^{2}}=\sum_{i=1}^{n}\lambda_{i}\frac{(x^{\prime}v_{i})^{2}}{(x^{\prime}x)(v_{i%
 }^{\prime}v_{i})}     The last representation establishes that the Rayleigh quotient is the sum of the squared cosines of the angles formed by the vector x and each eigenvector v i , weighted by corresponding eigenvalues.  If a vector x maximizes    R   (  M  ,  x  )       R   M  x     R(M,x)   , then any non-zero scalar multiple kx also maximizes R , so the problem can be reduced to the Lagrange problem of maximizing     ∑   i  =  1   n     α  i  2    λ  i        superscript   subscript     i  1    n      superscript   subscript  α  i   2    subscript  λ  i      \sum_{i=1}^{n}\alpha_{i}^{2}\lambda_{i}   under the constraint that      ∑   i  =  1   n    α  i  2    =  1        superscript   subscript     i  1    n    superscript   subscript  α  i   2    1    \sum_{i=1}^{n}\alpha_{i}^{2}=1   .  Define:  α }} . This then becomes a linear program , which always attains its maximum at one of the corners of the domain. A maximum point will have     α  1   =   ±  1        subscript  α  1    plus-or-minus  1     \alpha_{1}=\pm 1   and     α  i   =  0       subscript  α  i   0    \alpha_{i}=0   for all i > 1 (when the eigenvalues are ordered by decreasing magnitude).  Thus, as advertised, the Rayleigh quotient is maximised by the eigenvector with the largest eigenvalue.  Formulation using Lagrange multipliers  Alternatively, this result can be arrived at by the method of Lagrange multipliers . The problem is to find the critical points of the function       R   (  M  ,  x  )    =    x  T   M  x         R   M  x       superscript  x  T   M  x     R(M,x)=x^{T}Mx   , subject to the constraint      ∥  x  ∥   2   =    x  T   x   =  1.         superscript   norm  x   2      superscript  x  T   x        1.     \|x\|^{2}=x^{T}x=1.   I.e. to find the critical points of        ℒ   (  x  )    =     x  T   M  x   -   λ   (     x  T   x   -  1   )      ,        ℒ  x        superscript  x  T   M  x     λ       superscript  x  T   x   1       \mathcal{L}(x)=x^{T}Mx-\lambda\left(x^{T}x-1\right),   where λ is a Lagrange multiplier. The stationary points of    ℒ   (  x  )       ℒ  x    \mathcal{L}(x)   occur at        d  ℒ   (  x  )     d  x    =  0          d  ℒ  x     d  x    0    \frac{d\mathcal{L}(x)}{dx}=0          ∴     2   x  T    M  T    -   2  λ   x  T     =  0      therefore  absent        2   superscript  x  T    superscript  M  T      2  λ   superscript  x  T     0     \therefore 2x^{T}M^{T}-2\lambda x^{T}=0          ∴    M  x   =   λ  x       therefore  absent      M  x     λ  x      \therefore Mx=\lambda x   and        R   (  M  ,  x  )    =     x  T   M  x     x  T   x    =   λ     x  T   x     x  T   x     =  λ   .          R   M  x         superscript  x  T   M  x      superscript  x  T   x           λ       superscript  x  T   x      superscript  x  T   x          λ     R(M,x)=\frac{x^{T}Mx}{x^{T}x}=\lambda\frac{x^{T}x}{x^{T}x}=\lambda.     Therefore, the eigenvectors     x  1   ,  ⋯  ,   x  n       subscript  x  1   normal-⋯   subscript  x  n     x_{1},\cdots,x_{n}   of M are the critical points of the Rayleigh Quotient and their corresponding eigenvalues     λ  1   ,  ⋯  ,   λ  n       subscript  λ  1   normal-⋯   subscript  λ  n     \lambda_{1},\cdots,\lambda_{n}   are the stationary values of R .  This property is the basis for principal components analysis and canonical correlation .  Use in Sturm–Liouville theory  Sturm–Liouville theory concerns the action of the linear operator       L   (  y  )    =    1   w   (  x  )      (    -    d   d  x     [   p   (  x  )     d  y    d  x     ]     +   q   (  x  )   y    )          L  y       1    w  x            d    d  x     delimited-[]    p  x      d  y     d  x          q  x  y       L(y)=\frac{1}{w(x)}\left(-\frac{d}{dx}\left[p(x)\frac{dy}{dx}\right]+q(x)y\right)     on the inner product space defined by       ⟨   y  1   ,   y  2   ⟩   =    ∫  a  b    w   (  x  )    y  1    (  x  )    y  2    (  x  )   d  x          subscript  y  1    subscript  y  2      superscript   subscript   a   b     w  x   subscript  y  1   x   subscript  y  2   x  d  x      \langle{y_{1},y_{2}}\rangle=\int_{a}^{b}w(x)y_{1}(x)y_{2}(x)\,dx     of functions satisfying some specified boundary conditions at a and b . In this case the Rayleigh quotient is         ⟨  y  ,   L  y   ⟩    ⟨  y  ,  y  ⟩    =     ∫  a  b    y   (  x  )    (    -    d   d  x     [   p   (  x  )     d  y    d  x     ]     +   q   (  x  )   y   (  x  )     )   d  x      ∫  a  b    w   (  x  )   y    (  x  )   2   d  x      .         y    L  y     y  y        superscript   subscript   a   b     y  x          d    d  x     delimited-[]    p  x      d  y     d  x          q  x  y  x    d  x      superscript   subscript   a   b     w  x  y   superscript  x  2   d  x       \frac{\langle{y,Ly}\rangle}{\langle{y,y}\rangle}=\frac{\int_{a}^{b}y(x)\left(-%
 \frac{d}{dx}\left[p(x)\frac{dy}{dx}\right]+q(x)y(x)\right)dx}{\int_{a}^{b}{w(x%
 )y(x)^{2}}dx}.     This is sometimes presented in an equivalent form, obtained by separating the integral in the numerator and using integration by parts :        ⟨  y  ,   L  y   ⟩    ⟨  y  ,  y  ⟩         y    L  y     y  y     \displaystyle\frac{\langle{y,Ly}\rangle}{\langle{y,y}\rangle}     Generalizations   For a given pair ( A , B ) of matrices, and a given non-zero vector x , the generalized Rayleigh quotient is defined as:          R   (  A  ,  B  ;  x  )    :=     x  *   A  x     x  *   B  x     .     assign    R   A  B  x         superscript  x    A  x      superscript  x    B  x      R(A,B;x):=\frac{x^{*}Ax}{x^{*}Bx}.       The Generalized Rayleigh Quotient can be reduced to the Rayleigh Quotient    R   (  D  ,    C  *   x   )       R   D     superscript  C    x      R(D,C^{*}x)   through the transformation    D  =    C   -  1    A   C   *    -  1         D     superscript  C    1    A   superscript   superscript  C      1       D=C^{-1}A{C^{*}}^{-1}   where    C   C  *       C   superscript  C      CC^{*}   is the Cholesky decomposition of the Hermitian positive-definite matrix B .     For a given pair ( x , y ) of non-zero vectors, and a given Hermitian matrix H , the generalized Rayleigh quotient can be defined as:         R   (  H  ;  x  ,  y  )    :=        y  *   H  x        y  *   y   ⋅   x  *    x      assign    R   H  x  y       normal-⋅         superscript  y    H  x     absent     superscript  y    y    superscript  x     x     R(H;x,y):=\frac{y^{*}Hx}{\sqrt{}}{y^{*}y\cdot x^{*}x}       which coincides with R(H,x) when x = y .    See also   Field of values  Min-max theorem   References    Further reading   Shi Yu, Léon-Charles Tranchevent, Bart Moor, Yves Moreau, [ http://books.google.com/books?id=U6-ubGYgf7QC&dq; ;='Rayleigh%E2%80%93Ritz+ratio%22+Rayleigh+quotient&source;=gbs_navlinks_s Kernel-based Data Fusion for Machine Learning: Methods and Applications in Bioinformatics and Text Mining] , Ch. 2, Springer, 2011.   "  Category:Linear algebra     Also known as the Rayleigh–Ritz ratio ; named after Walther Ritz and Lord Rayleigh . ↩  Horn, R. A. and C. A. Johnson. 1985. Matrix Analysis . Cambridge University Press. pp. 176–180. ↩  Parlet B. N. The symmetric eigenvalue problem , SIAM, Classics in Applied Mathematics,1998 ↩     