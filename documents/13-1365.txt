   Regulatory feedback network      Regulatory feedback network   Regulatory Feedback Networks describe a class of neural networks related to Virtual Lateral Inhibition (named to distinguish it from lateral inhibition ) that perform inference using negative feedback . J. Reggia, “Virtual lateral inhibition in parallel  activation models of associative memory,” in Proc. 9th International  Joint Conference on Artificial Intelligence., Aug. 1985, pp. 244-248. 1 2 The feedback is implemented during recognition and during recognition connectivity parameters are not changed. Thus this is completely separate from learning/training (e.g. supervised learning or unsupervised learning ). This is also different from models of spatial attention . Instead, these networks determine the relevance of inputs through a "conservation of information principle".  How the network functions  The computational basis of conservation of information is that an input should not pass more information than is justified to the next layer. Thus inputs are regulated by the outputs they activate. Subsequently, each input’s contribution (i.e. salience ) is adjusted through feedback regulation by its associated outputs. The amplitudes of the adjusted inputs are propagated to the output layer. A new salience is re-evaluated based on the new output activity (through feedback). This can be iterated until the networks reach steady state. 3 At every step, the role of salience is  to maintain the relation where: the total activity of outputs connected  to an input will be equivalent to the input’s amplitude. 4 5  How the network is used  These networks are best suited for nodes with binary connections. 6 7 8 Instead of weights determining the relevance of connections, input salience is adjusted at the time of recognition. For example, a node representing car may connect to features wheels, door, and bumper. A node representing bicycle may connect to features wheels, pedals, and chain. Given wheels, the network will determine how relevant the wheels are to either the bicycle or car nodes during recognition.  Benefits/costs  This model displays unparalleled performance given simultaneous patterns, addressing combinatorial explosions associated with simultaneous patterns. 9 10  The model can also generate solutions composed of multiple output nodes with minimal overlap. 11 12 This property groups patterns together in a manner that suggests a way out of a fundamental recognition conundrum called the binding problem ('unity of perception' version).  In contrast to conventional neural networks or machine learning methods these networks cannot be guaranteed to be able to capture any arbitrary pattern. However for the patterns they can capture, they show these properties.  Implementation  Suppose there are fuzzy-type input features   x   x   x   and output nodes   y   y   y   . Each output node    y  j     subscript  y  j    y_{j}   is defined by set of feedforward binary connections    F   F  j       F   subscript  F  j     FF_{j}   from   x   x   x   's. It also has a set of symmetrical feedback connections    F  B      F  B    FB   that implement negative feedback . Due to the symmetry each member of    F   F  j       F   subscript  F  j     FF_{j}   (a connection from input to output) has a corresponding member in    F  B      F  B    FB   (a connection from the same output to same input) that returns and inhibits the input. Lets label    F   B  i       F   subscript  B  i     FB_{i}   the set of connections that return to an    x  i     subscript  x  i    x_{i}   .    |   F   F  j    |        F   subscript  F  j      |FF_{j}|   is the number of connections to    y  j     subscript  y  j    y_{j}   . Lets label    s  i     subscript  s  i    s_{i}   the salience of input    x  i     subscript  x  i    x_{i}   . Then the activity of  the output node is determined by:        y  j    (   t  +   Δ  t    )    =      y  j    (  t  )     |   F   F  j    |      ∑   k  ∈   F   F  j       s  k            subscript  y  j     t    normal-Δ  t            subscript  y  j   t       F   subscript  F  j        subscript     k    F   subscript  F  j       subscript  s  k       y_{j}(t+\Delta t)=\frac{y_{j}(t)}{|FF_{j}|}\sum_{k\in FF_{j}}s_{k}   . The salience value    s  i     subscript  s  i    s_{i}   of a given    x  i     subscript  x  i    x_{i}   is determined by:      s  i   =    x  i      ∑   r  ∈   F   B  i         y  r    (  t  )           subscript  s  i      subscript  x  i     subscript     r    F   subscript  B  i         subscript  y  r   t       s_{i}=\frac{x_{i}}{\sum_{r\in{FB_{i}}}y_{r}(t)}   . These equations can be iterated until the network reaches steady state.  See also   Visual perception  Visual Object Recognition in Cognitive Neuroscience  Bag of words model in computer vision  Computational neuroscience   References    "  Category:Control theory  Category:Computational neuroscience     Mcfadden, F. E. (1995). "Convergence of Competitive Activation Models Based on Virtual Lateral Inhibition." Neural Networks 8(6): 865-875. ↩  Achler, T. (2002). Input Shunt Networks. Neurocomputing, 44, 249-255. ↩    Achler  T.,  Amir  E.,  “Input  Feedback  Networks:  Classification  and  Inference  Based  on  Network  Structure”  Artificial  General  Intelligence  2008  pdf ↩    Achler T., Omar C., Amir E., “Shedding Weights: More With Less”, IEEE Proc. International Joint Conference on Neural Networks, 2008 pdf ↩   Achler T., Vural C., Amir, E., "Counting with Biologically Inspired Regulatory Feedback Networks”, IEEE Proc. International Joint Conference on Neural Networks, 2009 pdf ↩   Achler T., “Using Non-Oscillatory Dynamics to Disambiguate Simultaneous Patterns”, IEEE Proc. International Joint Conference on Neural Networks, 2009 pdf ↩     