   Probability-generating function      Probability-generating function   In probability theory , the probability generating function of a discrete random variable is a power series representation (the generating function ) of the probability mass function of the random variable. Probability generating functions are often employed for their succinct description of the sequence of probabilities Pr( X = i ) in the probability mass function for a random variable  X , and to make available the well-developed theory of power series with non-negative coefficients.  Definition  Univariate case  If X is a discrete random variable taking values in the non-negative integers {0,1, ...}, then the probability generating function of X is defined as 1        G   (  z  )    =   E   (   z  X   )    =    ∑   x  =  0   ∞    p   (  x  )    z  x      ,          G  z    normal-E   superscript  z  X           superscript   subscript     x  0        p  x   superscript  z  x        G(z)=\operatorname{E}(z^{X})=\sum_{x=0}^{\infty}p(x)z^{x},   where p is the probability mass function of X . Note that the subscripted notations G X and p X are often used to emphasize that these pertain to a particular random variable X , and to its distribution. The power series converges absolutely at least for all complex numbers  z with | z | ≤ 1; in many examples the radius of convergence is larger.  Multivariate case  If ( X 1 ,..., X d )}} is a discrete random variable taking values in the d -dimensional non-negative integer lattice {0,1, ...} d , then the probability generating function of X is defined as        G   (  z  )    =   G   (   z  1   ,  …  ,   z  d   )    =   E   (    z  1   X  1    ⋯   z  d   X  d     )    =    ∑     x  1   ,  …  ,   x  d    =  0   ∞    p   (   x  1   ,  …  ,   x  d   )    z  1   x  1    ⋯   z  d   x  d       ,          G  z     G    subscript  z  1   normal-…   subscript  z  d           normal-E     superscript   subscript  z  1    subscript  X  1    normal-⋯   superscript   subscript  z  d    subscript  X  d             superscript   subscript       subscript  x  1   normal-…   subscript  x  d    0        p    subscript  x  1   normal-…   subscript  x  d     superscript   subscript  z  1    subscript  x  1    normal-⋯   superscript   subscript  z  d    subscript  x  d         G(z)=G(z_{1},\ldots,z_{d})=\operatorname{E}\bigl(z_{1}^{X_{1}}\cdots z_{d}^{X_%
 {d}}\bigr)=\sum_{x_{1},\ldots,x_{d}=0}^{\infty}p(x_{1},\ldots,x_{d})z_{1}^{x_{%
 1}}\cdots z_{d}^{x_{d}},   where p is the probability mass function of X . The power series converges absolutely at least for all complex vectors ( z 1 ,..., z d ) ∈ ℂ d }} with .  Properties  Power series  Probability generating functions obey all the rules of power series with non-negative coefficients. In particular, G (1 − ) = 1, where G (1 − ) = lim z→1 G ( z ) from below , since the probabilities must sum to one. So the radius of convergence of any probability generating function must be at least 1, by Abel's theorem for power series with non-negative coefficients.  Probabilities and expectations  The following properties allow the derivation of various basic quantities related to X :  1. The probability mass function of X is recovered by taking derivatives of G        p   (  k  )    =   Pr   (   X  =  k   )    =     G   (  k  )     (  0  )     k  !     .          p  k    Pr    X  k              superscript  G  k   0     k       p(k)=\operatorname{Pr}(X=k)=\frac{G^{(k)}(0)}{k!}.     2. It follows from Property 1 that if random variables X and Y have probability generating functions that are equal, G X = G Y , then p X = p Y . That is, if X and Y have identical probability generating functions, then they have identical distributions.  3. The normalization of the probability density function can be expressed in terms of the generating function by       E   (  1  )    =   G   (   1  -   )    =    ∑   i  =  0   ∞    f   (  i  )     =  1.         normal-E  1     G   superscript  1            superscript   subscript     i  0        f  i         1.     \operatorname{E}(1)=G(1^{-})=\sum_{i=0}^{\infty}f(i)=1.     The expectation of X is given by        E   (  X  )    =    G  ′    (   1  -   )     .       normal-E  X      superscript  G  normal-′    superscript  1       \operatorname{E}\left(X\right)=G^{\prime}(1^{-}).     More generally, the k th  factorial moment ,    E   (   X   (   X  -  1   )   ⋯   (    X  -  k   +  1   )    )       E    X    X  1   normal-⋯      X  k   1      \textrm{E}(X(X-1)\cdots(X-k+1))   of X is given by        E   (    X  !     (   X  -  k   )   !    )    =    G   (  k  )     (   1  -   )     ,   k  ≥  0.      formulae-sequence      E      X       X  k         superscript  G  k    superscript  1        k  0.     \textrm{E}\left(\frac{X!}{(X-k)!}\right)=G^{(k)}(1^{-}),\quad k\geq 0.     So the variance of X is given by        Var   (  X  )    =      G  ′′    (   1  -   )    +    G  ′    (   1  -   )     -    [    G  ′    (   1  -   )    ]   2     .       Var  X          superscript  G  ′′    superscript  1        superscript  G  normal-′    superscript  1       superscript   delimited-[]     superscript  G  normal-′    superscript  1      2      \operatorname{Var}(X)=G^{\prime\prime}(1^{-})+G^{\prime}(1^{-})-\left[G^{%
 \prime}(1^{-})\right]^{2}.     4.      G  X    (   e  t   )    =    M  X    (  t  )           subscript  G  X    superscript  e  t       subscript  M  X   t     G_{X}(e^{t})=M_{X}(t)   where X is a random variable,     G  X    (  t  )        subscript  G  X   t    G_{X}(t)   is the probability generating function (of X ) and     M  X    (  t  )        subscript  M  X   t    M_{X}(t)   is the moment-generating function (of X ) .  Functions of independent random variables  Probability generating functions are particularly useful for dealing with functions of independent random variables. For example:   If X 1 , X 2 , ..., X n is a sequence of independent (and not necessarily identically distributed) random variables, and           S  n   =    ∑   i  =  1   n     a  i    X  i      ,       subscript  S  n     superscript   subscript     i  1    n      subscript  a  i    subscript  X  i       S_{n}=\sum_{i=1}^{n}a_{i}X_{i},         where the a i are constants, then the probability generating function is given by            G   S  n     (  z  )    =   E   (   z   S  n    )    =   E   (   z     ∑   i  =  1   n     a  i    X  i   ,    )    =    G   X  1     (   z   a  1    )    G   X  2     (   z   a  2    )   ⋯   G   X  n     (   z   a  n    )     .           subscript  G   subscript  S  n    z    normal-E   superscript  z   subscript  S  n           normal-E   superscript  z   fragments   superscript   subscript     i  1    n    subscript  a  i    subscript  X  i   normal-,             subscript  G   subscript  X  1     superscript  z   subscript  a  1     subscript  G   subscript  X  2     superscript  z   subscript  a  2    normal-⋯   subscript  G   subscript  X  n     superscript  z   subscript  a  n        G_{S_{n}}(z)=\operatorname{E}(z^{S_{n}})=\operatorname{E}(z^{\sum_{i=1}^{n}a_{%
 i}X_{i},})=G_{X_{1}}(z^{a_{1}})G_{X_{2}}(z^{a_{2}})\cdots G_{X_{n}}(z^{a_{n}}).         For example, if           S  n   =    ∑   i  =  1   n    X  i     ,       subscript  S  n     superscript   subscript     i  1    n    subscript  X  i      S_{n}=\sum_{i=1}^{n}X_{i},         then the probability generating function, G Sn ( z ), is given by            G   S  n     (  z  )    =    G   X  1     (  z  )    G   X  2     (  z  )   ⋯   G   X  n     (  z  )     .         subscript  G   subscript  S  n    z      subscript  G   subscript  X  1    z   subscript  G   subscript  X  2    z  normal-⋯   subscript  G   subscript  X  n    z     G_{S_{n}}(z)=G_{X_{1}}(z)G_{X_{2}}(z)\cdots G_{X_{n}}(z).         It also follows that the probability generating function of the difference of two independent random variables S = X 1 − X 2 is            G  S    (  z  )    =    G   X  1     (  z  )    G   X  2     (   1  /  z   )     .         subscript  G  S   z      subscript  G   subscript  X  1    z   subscript  G   subscript  X  2      1  z      G_{S}(z)=G_{X_{1}}(z)G_{X_{2}}(1/z).         Suppose that N is also an independent, discrete random variable taking values on the non-negative integers, with probability generating function G N . If the X 1 , X 2 , ..., X N are independent and identically distributed with common probability generating function G X , then            G   S  N     (  z  )    =    G  N    (    G  X    (  z  )    )     .         subscript  G   subscript  S  N    z      subscript  G  N      subscript  G  X   z      G_{S_{N}}(z)=G_{N}(G_{X}(z)).         This can be seen, using the law of total expectation , as follows:            G   S  N     (  z  )    =   E   (   z   S  N    )    =   E   (   z     ∑   i  =  1   N     X  i     )    =   E   (   E   (   z     ∑   i  =  1   N     X  i     |  N  )    )    =   E   (    (    G  X    (  z  )    )   N   )    =    G  N    (    G  X    (  z  )    )     .           subscript  G   subscript  S  N    z    normal-E   superscript  z   subscript  S  N           normal-E   superscript  z    superscript   subscript     i  1    N    subscript  X  i            normal-E   normal-E   superscript  z    superscript   subscript     i  1    N    subscript  X  i     N          normal-E   superscript     subscript  G  X   z   N            subscript  G  N      subscript  G  X   z       G_{S_{N}}(z)=\operatorname{E}(z^{S_{N}})=\operatorname{E}(z^{\sum_{i=1}^{N}X_{%
 i}})=\operatorname{E}\big(\operatorname{E}(z^{\sum_{i=1}^{N}X_{i}}|N)\big)=%
 \operatorname{E}\big((G_{X}(z))^{N}\big)=G_{N}(G_{X}(z)).         This last fact is useful in the study of Galton–Watson processes .    Suppose again that N is also an independent, discrete random variable taking values on the non-negative integers, with probability generating function G N and probability density     f  i   =   Pr   {   N  =  i   }         subscript  f  i    Pr    N  i      f_{i}=\Pr\{N=i\}   . If the X 1 , X 2 , ..., X N are independent, but not identically distributed random variables, where    G   X  i      subscript  G   subscript  X  i     G_{X_{i}}   denotes the probability generating function of    X  i     subscript  X  i    X_{i}   , then            G   S  N     (  z  )    =    ∑   i  ≥  1      f  i     ∏   k  =  1   i     G   X  i     (  z  )        .         subscript  G   subscript  S  N    z     subscript     i  1       subscript  f  i     superscript   subscript  product    k  1    i      subscript  G   subscript  X  i    z        G_{S_{N}}(z)=\sum_{i\geq 1}f_{i}\prod_{k=1}^{i}G_{X_{i}}(z).         For identically distributed X i this simplifies to the identity stated before. The general case is sometimes useful to obtain a decomposition of S N by means of generating functions.   Examples   The probability generating function of a constant random variable , i.e. one with Pr( X = c ) = 1, is           G   (  z  )    =   (   z  c   )    .        G  z    superscript  z  c     G(z)=\left(z^{c}\right).\,         The probability generating function of a binomial random variable , the number of successes in n trials, with probability p of success in each trial, is           G   (  z  )    =    [    (   1  -  p   )   +   p  z    ]   n    .        G  z    superscript   delimited-[]      1  p     p  z     n     G(z)=\left[(1-p)+pz\right]^{n}.\,         Note that this is the n -fold product of the probability generating function of a Bernoulli random variable with parameter p .    The probability generating function of a negative binomial random variable on {0,1,2 ...}, the number of failures until the r th success with probability of success in each trial p , is           G   (  z  )    =    (    p  z    1  -    (   1  -  p   )   z     )   r    .        G  z    superscript      p  z     1      1  p   z     r     G(z)=\left(\frac{pz}{1-(1-p)z}\right)^{r}.         (Convergence for     |  z  |   <   1   1  -  p          z     1    1  p      |z|<\frac{1}{1-p}   ).    Note that this is the r -fold product of the probability generating function of a geometric random variable with parameter 1− p on {0,1,2 ...}.    The probability generating function of a Poisson random variable with rate parameter λ is           G   (  z  )    =   e   λ   (   z  -  1   )      .        G  z    superscript  e    λ    z  1       G(z)=\textrm{e}^{\lambda(z-1)}.\;\,        Related concepts  The probability generating function is an example of a generating function of a sequence: see also formal power series . It is equivalent to, and sometimes called, the z-transform of the probability mass function.  Other generating functions of random variables include the moment-generating function , the characteristic function and the cumulant generating function .  Notes  References   Johnson, N.L.; Kotz, S.; Kemp, A.W. (1993) Univariate Discrete distributions (2nd edition). Wiley. ISBN 0-471-54897-9 (Section 1.B9)   "  Category:Theory of probability distributions  Category:Generating functions     http://www.am.qub.ac.uk/users/g.gribakin/sor/Chap3.pdf ↩     