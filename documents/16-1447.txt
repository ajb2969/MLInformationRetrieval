   Occam learning      Occam learning   In Occam Learning , named after Occam's razor , a probably approximately correct (PAC) learning algorithm is evaluated based on its succinctness and performance on the training set, rather than directly on its predictive power on a test set. Occam learnability is equivalent to PAC learnability.  Definitions of Occam learning and succinctness  The succinctness of a concept   c   c   c   in concept class    C   C   C   can be expressed by the length    s  i  z  e   (  c  )       s  i  z  e  c    size(c)   of the shortest bit string that can represent   c   c   c   in   C   C   C   . Occam learning connects the succinctness of a learning algorithm's output to its predictive power on unseen data.  Let   C   C   C   and   H   H   H   be concept classes containing target concepts and hypotheses respectively and let sample set   S   S   S   contain   m   m   m   samples each containing   n   n   n   bits. Then, for constants    α  ≥  0      α  0    \alpha\geq 0   and    0  ≤  β  <  1        0  β       1     0\leq\beta<1   , a learning algorithm   L   L   L   is an (α,β)-Occam algorithm for   C   C   C   using   H   H   H   if, given   S   S   S   labeled according to    c  ∈  C      c  C    c\in C   ,   L   L   L   outputs a hypothesis    h  ∈  H      h  H    h\in H   such that   h   h   h   is consistent with   c   c   c   on   S   S   S   (that is,      h   (  x  )    =   c   (  x  )     ,    ∀  x   ∈  S      formulae-sequence      h  x     c  x       for-all  x   S     h(x)=c(x),\forall x\in S   ) and     s  i  z  e   (  h  )    ≤     (    n  ⋅  s   i  z  e   (  c  )    )   α    m  β          s  i  z  e  h      superscript     normal-⋅  n  s   i  z  e  c   α    superscript  m  β      size(h)\leq(n\cdot size(c))^{\alpha}m^{\beta}   . 1 2  Such an algorithm   L   L   L   is called an efficient (α,β)-Occam algorithm if it runs in time polynomial in   n   n   n   ,   m   m   m   , and    s  i  z  e   (  c  )       s  i  z  e  c    size(c)   .  Equivalence of Occam and PAC learning  Any efficient Occam algorithm is also an efficient PAC learning algorithm. Specifically, an efficient (α,β)-Occam algorithm   L   L   L   for   C   C   C   using   H   H   H   , given   m   m   m   samples of   n   n   n   bits each, such that    m  ≥   α   (     1  ϵ    log   1  δ     +    (      (  n  ⋅  s  i  z  e   (  c  )   )   α   )   ϵ   )    1   1  -  β      )        m    α        1  ϵ       1  δ      superscript     fragments   superscript   fragments  normal-(  n  normal-⋅  s  i  z  e   fragments  normal-(  c  normal-)   normal-)   α   normal-)   ϵ     1    1  β         m\geq\alpha\left(\frac{1}{\epsilon}\log\frac{1}{\delta}+\left(\frac{(n\cdot
 size%
 (c))^{\alpha})}{\epsilon}\right)^{\frac{1}{1-\beta}}\right)   will return    h  ∈  H      h  H    h\in H   such that     e  r  r  o  r   (  h  )    ≤  ϵ        e  r  r  o  r  h   ϵ    error(h)\leq\epsilon   with probability at least    1  -  δ      1  δ    1-\delta   . More generally, there exists a constant    b  >  0      b  0    b>0   such that given    m  ≥    1   b  ϵ     (    log   |  H  |    +   log   1  δ     )        m      1    b  ϵ          H        1  δ        m\geq\frac{1}{b\epsilon}\left(\log|H|+\log\frac{1}{\delta}\right)   examples   L   L   L   will output    h  ∈  H      h  H    h\in H   such that     e  r  r  o  r   (  h  )    ≤  ϵ        e  r  r  o  r  h   ϵ    error(h)\leq\epsilon   with probability at least    1  -  δ      1  δ    1-\delta   . 3  Any PAC learning algorithm is also an Occam algorithm. 4  Improving sample complexity for common problems  Though Occam and PAC learnability are equivalent, the Occam framework can be used to produce tighter bounds on the sample complexity of classical problems including conjunctions, 5 conjunctions with few relevant variables, 6 and decision lists. 7  Extensions  Occam algorithms have also been shown to be successful for PAC learning in the presence of errors, 8 9 probabilistic concepts, 10 function learning 11 and Markovian non-independent examples. 12  See also   Structural Risk Minimization  Computational learning theory   References  "  Category:Theoretical computer science  Category:Computational learning theory  Category:Machine learning     Kearns, M. J., & Vazirani, U. V. (1994). An introduction to computational learning theory, chapter 2. MIT press. ↩  Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1987). Occam's razor . Information processing letters, 24(6), 377-380. ↩   Board, R., & Pitt, L. (1990, April). On the necessity of Occam algorithms. In Proceedings of the twenty-second annual ACM symposium on Theory of computing (pp. 54-63). ACM. ↩   Haussler, D. (1988). Quantifying inductive bias: AI learning algorithms and Valiant's learning framework . Artificial intelligence, 36(2), 177-221. ↩  Rivest, R. L. (1987). Learning decision lists. Machine learning , 2(3), 229-246. ↩  Angluin, D., & Laird, P. (1988). Learning from noisy examples. Machine Learning, 2(4), 343-370. ↩  Kearns, M., & Li, M. (1993). Learning in the presence of malicious errors. SIAM Journal on Computing, 22(4), 807-837. ↩  Kearns, M. J., & Schapire, R. E. (1990, October). Efficient distribution-free learning of probabilistic concepts . In Foundations of Computer Science, 1990. Proceedings., 31st Annual Symposium on (pp. 382-391). IEEE. ↩  Natarajan, B. K. (1993, August). Occam's razor for functions. In Proceedings of the sixth annual conference on Computational learning theory (pp. 370-376). ACM. ↩  Aldous, D., & Vazirani, U. (1990, October). A Markovian extension of Valiant's learning model . In Foundations of Computer Science, 1990. Proceedings., 31st Annual Symposium on (pp. 392-396). IEEE. ↩     