<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="104">Least squares</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Least squares</h1>
<hr/>

<p>The method of <strong>least squares</strong> is a standard approach in <a href="regression_analysis" title="wikilink">regression analysis</a> to the approximate solution of <a href="overdetermined_system" title="wikilink">overdetermined systems</a>, i.e., sets of equations in which there are more equations than unknowns. "Least squares" means that the overall solution minimizes the sum of the squares of the errors made in the results of every single equation.</p>

<p>The most important application is in <a href="curve_fitting" title="wikilink">data fitting</a>. The best fit in the least-squares sense minimizes the sum of squared <a href="errors_and_residuals_in_statistics" title="wikilink">residuals</a>, a residual being the difference between an observed value and the fitted value provided by a model. When the problem has substantial uncertainties in the <a href="independent_variable" title="wikilink">independent variable</a> (the <em>x</em> variable), then simple regression and least squares methods have problems; in such cases, the methodology required for fitting <a href="errors-in-variables_models" title="wikilink">errors-in-variables models</a> may be considered instead of that for least squares.</p>

<p>Least squares problems fall into two categories: linear or <a href="ordinary_least_squares" title="wikilink">ordinary least squares</a> and <a href="non-linear_least_squares" title="wikilink">non-linear least squares</a>, depending on whether or not the residuals are linear in all unknowns. The linear least-squares problem occurs in statistical <a href="regression_analysis" title="wikilink">regression analysis</a>; it has a <a href="closed-form_solution" title="wikilink">closed-form solution</a>. The non-linear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases.</p>

<p><a href="Polynomial_least_squares" title="wikilink">Polynomial least squares</a> describes the variance in a prediction of the dependent variable as a function of the independent variable and the deviations from the fitted curve.</p>

<p>When the observations come from an <a href="exponential_family" title="wikilink">exponential family</a> and mild conditions are satisfied, least-squares estimates and <a href="Maximum_likelihood" title="wikilink">maximum-likelihood</a> estimates are identical.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> The method of least squares can also be derived as a <a href="method_of_moments_(statistics)" title="wikilink">method of moments</a> estimator.</p>

<p>The following discussion is mostly presented in terms of <a class="uri" href="linear" title="wikilink">linear</a> functions but the use of least-squares is valid and practical for more general families of functions. Also, by iteratively applying local <a href="quadratic_approximation" title="wikilink">quadratic approximation</a> to the likelihood (through the <a href="Fisher_information" title="wikilink">Fisher information</a>), the least-squares method may be used to fit a <a href="generalized_linear_model" title="wikilink">generalized linear model</a>.</p>

<p>For the topic of approximating a function by a sum of others using an objective function based on squared distances, see <a href="least_squares_(function_approximation)" title="wikilink">least squares (function approximation)</a>.</p>
<figure><b>(Figure)</b>
<figcaption>The result of fitting a set of data points with a quadratic function.</figcaption>
</figure>

<p>The least-squares method is usually credited to <a href="Carl_Friedrich_Gauss" title="wikilink">Carl Friedrich Gauss</a> (1795),<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> but it was first published by <a href="Adrien-Marie_Legendre" title="wikilink">Adrien-Marie Legendre</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> </p>
<h2 id="history">History</h2>
<h3 id="context">Context</h3>

<p>The method of least squares grew out of the fields of <a class="uri" href="astronomy" title="wikilink">astronomy</a> and <a class="uri" href="geodesy" title="wikilink">geodesy</a> as scientists and mathematicians sought to provide solutions to the challenges of navigating the Earth's oceans during the <a href="Age_of_Exploration" title="wikilink">Age of Exploration</a>. The accurate description of the behavior of celestial bodies was the key to enabling ships to sail in open seas, where sailors could no longer rely on land sightings for navigation.</p>

<p>The method was the culmination of several advances that took place during the course of the eighteenth century:<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<ul>
<li>The combination of different observations as being the best estimate of the true value; errors decrease with aggregation rather than increase, perhaps first expressed by <a href="Roger_Cotes" title="wikilink">Roger Cotes</a> in 1722.</li>
<li>The combination of different observations taken under the <em>same</em> conditions contrary to simply trying one's best to observe and record a single observation accurately. The approach was known as the method of averages. This approach was notably used by <a href="Tobias_Mayer" title="wikilink">Tobias Mayer</a> while studying the <a href="libration" title="wikilink">librations</a> of the moon in 1750, and by <a href="Pierre-Simon_Laplace" title="wikilink">Pierre-Simon Laplace</a> in his work in explaining the differences in motion of <a class="uri" href="Jupiter" title="wikilink">Jupiter</a> and <a class="uri" href="Saturn" title="wikilink">Saturn</a> in 1788.</li>
<li>The combination of different observations taken under <em>different</em> conditions. The method came to be known as the method of least absolute deviation. It was notably performed by <a href="Roger_Joseph_Boscovich" title="wikilink">Roger Joseph Boscovich</a> in his work on the shape of the earth in 1757 and by <a href="Pierre-Simon_Laplace" title="wikilink">Pierre-Simon Laplace</a> for the same problem in 1799.</li>
<li>The development of a criterion that can be evaluated to determine when the solution with the minimum error has been achieved. Laplace tried to specify a mathematical form of the <a class="uri" href="probability" title="wikilink">probability</a> density for the errors and define a method of estimation that minimizes the error of estimation. For this purpose, Laplace used a symmetric two-sided exponential distribution we now call <a href="Laplace_distribution" title="wikilink">Laplace distribution</a> to model the error distribution, and used the sum of absolute deviation as error of estimation. He felt these to be the simplest assumptions he could make, and he had hoped to obtain the arithmetic mean as the best estimate. Instead, his estimator was the posterior median.</li>
</ul>
<h3 id="the-method">The method</h3>
<figure><b>(Figure)</b>
<figcaption><a href="Carl_Friedrich_Gauss" title="wikilink">Carl Friedrich Gauss</a></figcaption>
</figure>

<p>The first clear and concise exposition of the method of least squares was published by <a href="Adrien-Marie_Legendre" title="wikilink">Legendre</a> in 1805.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> The technique is described as an algebraic procedure for fitting linear equations to data and Legendre demonstrates the new method by analyzing the same data as Laplace for the shape of the earth. The value of Legendre's method of least squares was immediately recognized by leading astronomers and geodesists of the time.</p>

<p>In 1809 <a href="Carl_Friedrich_Gauss" title="wikilink">Carl Friedrich Gauss</a> published his method of calculating the orbits of celestial bodies. In that work he claimed to have been in possession of the method of least squares since 1795. This naturally led to a priority dispute with Legendre. However, to Gauss's credit, he went beyond Legendre and succeeded in connecting the method of least squares with the principles of probability and to the <a href="normal_distribution" title="wikilink">normal distribution</a>. He had managed to complete Laplace's program of specifying a mathematical form of the probability density for the observations, depending on a finite number of unknown parameters, and define a method of estimation that minimizes the error of estimation. Gauss showed that arithmetic mean is indeed the best estimate of the location parameter by changing both the probability density and the method of estimation. He then turned the problem around by asking what form the density should have and what method of estimation should be used to get the arithmetic mean as estimate of the location parameter. In this attempt, he invented the normal distribution.</p>

<p>An early demonstration of the strength of <a href="Gauss'_Method" title="wikilink">Gauss' Method</a> came when it was used to predict the future location of the newly discovered asteroid <a href="Ceres_(dwarf_planet)" title="wikilink">Ceres</a>. On 1 January 1801, the Italian astronomer <a href="Giuseppe_Piazzi" title="wikilink">Giuseppe Piazzi</a> discovered Ceres and was able to track its path for 40 days before it was lost in the glare of the sun. Based on this data, astronomers desired to determine the location of Ceres after it emerged from behind the sun without solving the complicated <a href="Kepler's_laws_of_planetary_motion" title="wikilink">Kepler's nonlinear equations</a> of planetary motion. The only predictions that successfully allowed Hungarian astronomer <a href="Franz_Xaver_von_Zach" title="wikilink">Franz Xaver von Zach</a> to relocate Ceres were those performed by the 24-year-old Gauss using least-squares analysis.</p>

<p>In 1810, after reading Gauss's work, Laplace, after proving the <a href="central_limit_theorem" title="wikilink">central limit theorem</a>, used it to give a large sample justification for the method of least square and the normal distribution. In 1822, Gauss was able to state that the least-squares approach to regression analysis is optimal in the sense that in a linear model where the errors have a mean of zero, are uncorrelated, and have equal variances, the best linear unbiased estimator of the coefficients is the least-squares estimator. This result is known as the <a href="Gauss–Markov_theorem" title="wikilink">Gauss–Markov theorem</a>.</p>

<p>The idea of least-squares analysis was also independently formulated by the American <a href="Robert_Adrain" title="wikilink">Robert Adrain</a> in 1808. In the next two centuries workers in the theory of errors and in statistics found many different ways of implementing least squares.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h2 id="problem-statement">Problem statement</h2>

<p>The objective consists of adjusting the parameters of a model function to best fit a data set. A simple data set consists of <em>n</em> points (data pairs) 

<math display="inline" id="Least_squares:0">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <msub>
    <mi>x</mi>
    <mi>i</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>y</mi>
    <mi>i</mi>
   </msub>
   <mo rspace="0.8pt" stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="open">
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <ci>i</ci>
    </apply>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x_{i},y_{i})\!
  </annotation>
 </semantics>
</math>

, <em>i</em> = 1, ..., <em>n</em>, where 

<math display="inline" id="Least_squares:1">
 <semantics>
  <mpadded width="-1.7pt">
   <msub>
    <mi>x</mi>
    <mi>i</mi>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}\!
  </annotation>
 </semantics>
</math>

 is an <a href="independent_variable" title="wikilink">independent variable</a> and 

<math display="inline" id="Least_squares:2">
 <semantics>
  <mpadded width="-1.7pt">
   <msub>
    <mi>y</mi>
    <mi>i</mi>
   </msub>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>y</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{i}\!
  </annotation>
 </semantics>
</math>

 is a <a href="dependent_variable" title="wikilink">dependent variable</a> whose value is found by observation. The model function has the form 

<math display="inline" id="Least_squares:3">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>β</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>β</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x,\beta)
  </annotation>
 </semantics>
</math>

, where <em>m</em> adjustable parameters are held in the vector 

<math display="inline" id="Least_squares:4">
 <semantics>
  <mi>𝜷</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝜷</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \boldsymbol{\beta}
  </annotation>
 </semantics>
</math>

. The goal is to find the parameter values for the model which "best" fits the data. The least squares method finds its optimum when the sum, <em>S</em>, of squared residuals</p>

<p>

<math display="block" id="Least_squares:5">
 <semantics>
  <mrow>
   <mi>S</mi>
   <mo>=</mo>
   <mrow>
    <munderover>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>n</mi>
    </munderover>
    <mmultiscripts>
     <mi>r</mi>
     <mi>i</mi>
     <none></none>
     <none></none>
     <mn>2</mn>
    </mmultiscripts>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>S</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>n</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>r</ci>
       <ci>i</ci>
      </apply>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S=\sum_{i=1}^{n}{r_{i}}^{2}
  </annotation>
 </semantics>
</math>

 is a minimum. A <a href="errors_and_residuals_in_statistics" title="wikilink">residual</a> is defined as the difference between the actual value of the dependent variable and the value predicted by the model.</p>

<p>

<math display="block" id="Least_squares:6">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>r</mi>
     <mi>i</mi>
    </msub>
    <mo>=</mo>
    <mrow>
     <msub>
      <mi>y</mi>
      <mi>i</mi>
     </msub>
     <mo>-</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>x</mi>
        <mi>i</mi>
       </msub>
       <mo>,</mo>
       <mi>𝜷</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>r</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <times></times>
      <ci>f</ci>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
       <ci>𝜷</ci>
      </interval>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r_{i}=y_{i}-f(x_{i},\boldsymbol{\beta}).
  </annotation>
 </semantics>
</math>

</p>

<p>An example of a model is that of the straight line in two dimensions. Denoting the intercept as 

<math display="inline" id="Least_squares:7">
 <semantics>
  <msub>
   <mi>β</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>β</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \beta_{0}
  </annotation>
 </semantics>
</math>

 and the slope as 

<math display="inline" id="Least_squares:8">
 <semantics>
  <msub>
   <mi>β</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>β</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \beta_{1}
  </annotation>
 </semantics>
</math>

, the model function is given by 

<math display="inline" id="Least_squares:9">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo>,</mo>
     <mi>𝜷</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>β</mi>
     <mn>0</mn>
    </msub>
    <mo>+</mo>
    <mrow>
     <msub>
      <mi>β</mi>
      <mn>1</mn>
     </msub>
     <mi>x</mi>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <interval closure="open">
      <ci>x</ci>
      <ci>𝜷</ci>
     </interval>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>β</ci>
      <cn type="integer">0</cn>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>β</ci>
       <cn type="integer">1</cn>
      </apply>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x,\boldsymbol{\beta})=\beta_{0}+\beta_{1}x
  </annotation>
 </semantics>
</math>

. See <a href="Linear_least_squares_(mathematics)#Motivational_example" title="wikilink">linear least squares</a> for a fully worked out example of this model.</p>

<p>A data point may consist of more than one independent variable. For example, when fitting a plane to a set of height measurements, the plane is a function of two independent variables, <em>x</em> and <em>z</em>, say. In the most general case there may be one or more independent variables and one or more dependent variables at each data point.</p>
<h2 id="limitations">Limitations</h2>

<p>This regression formulation considers only residuals in the dependent variable. There are two rather different contexts in which different implications apply:</p>
<ul>
<li>Regression for prediction. Here a model is fitted to provide a prediction rule for application in a similar situation to which the data used for fitting apply. Here the dependent variables corresponding to such future application would be subject to the same types of observation error as those in the data used for fitting. It is therefore logically consistent to use the least-squares prediction rule for such data.</li>
<li>Regression for fitting a "true relationship". In standard <a href="regression_analysis" title="wikilink">regression analysis</a>, that leads to fitting by least squares, there is an implicit assumption that errors in the <a href="independent_variable" title="wikilink">independent variable</a> are zero or strictly controlled so as to be negligible. When errors in the <a href="independent_variable" title="wikilink">independent variable</a> are non-negligible, <a href="Errors-in-variables_models" title="wikilink">models of measurement error</a> can be used; such methods can lead to <a href="parameter_estimation" title="wikilink">parameter estimates</a>, <a href="hypothesis_testing" title="wikilink">hypothesis testing</a> and <a href="confidence_interval" title="wikilink">confidence intervals</a> that take into account the presence of observation errors in the independent variables.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> An alternative approach is to fit a model by <a href="total_least_squares" title="wikilink">total least squares</a>; this can be viewed as taking a pragmatic approach to balancing the effects of the different sources of error in formulating an objective function for use in model-fitting.</li>
</ul>
<h2 id="solving-the-least-squares-problem">Solving the least squares problem</h2>

<p>The <a href="Maxima_and_minima" title="wikilink">minimum</a> of the sum of squares is found by setting the <a class="uri" href="gradient" title="wikilink">gradient</a> to zero. Since the model contains <em>m</em> parameters, there are <em>m</em> gradient equations:</p>

<p>

<math display="block" id="Least_squares:10">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mfrac>
      <mrow>
       <mo>∂</mo>
       <mi>S</mi>
      </mrow>
      <mrow>
       <mo>∂</mo>
       <msub>
        <mi>β</mi>
        <mi>j</mi>
       </msub>
      </mrow>
     </mfrac>
     <mo>=</mo>
     <mrow>
      <mn>2</mn>
      <mrow>
       <munder>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mi>i</mi>
       </munder>
       <mrow>
        <msub>
         <mi>r</mi>
         <mi>i</mi>
        </msub>
        <mfrac>
         <mrow>
          <mo>∂</mo>
          <msub>
           <mi>r</mi>
           <mi>i</mi>
          </msub>
         </mrow>
         <mrow>
          <mo>∂</mo>
          <msub>
           <mi>β</mi>
           <mi>j</mi>
          </msub>
         </mrow>
        </mfrac>
       </mrow>
      </mrow>
     </mrow>
     <mo>=</mo>
     <mn>0</mn>
    </mrow>
    <mo rspace="7.5pt">,</mo>
    <mrow>
     <mi>j</mi>
     <mo>=</mo>
     <mrow>
      <mn>1</mn>
      <mo>,</mo>
      <mi mathvariant="normal">…</mi>
      <mo>,</mo>
      <mi>m</mi>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <and></and>
     <apply>
      <eq></eq>
      <apply>
       <divide></divide>
       <apply>
        <partialdiff></partialdiff>
        <ci>S</ci>
       </apply>
       <apply>
        <partialdiff></partialdiff>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>β</ci>
         <ci>j</ci>
        </apply>
       </apply>
      </apply>
      <apply>
       <times></times>
       <cn type="integer">2</cn>
       <apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <ci>i</ci>
        </apply>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>r</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <divide></divide>
          <apply>
           <partialdiff></partialdiff>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>r</ci>
            <ci>i</ci>
           </apply>
          </apply>
          <apply>
           <partialdiff></partialdiff>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>β</ci>
            <ci>j</ci>
           </apply>
          </apply>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
     <apply>
      <eq></eq>
      <share href="#.cmml">
      </share>
      <cn type="integer">0</cn>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <ci>j</ci>
     <list>
      <cn type="integer">1</cn>
      <ci>normal-…</ci>
      <ci>m</ci>
     </list>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{\partial S}{\partial\beta_{j}}=2\sum_{i}r_{i}\frac{\partial r_{i}}{%
\partial\beta_{j}}=0,\ j=1,\ldots,m,
  </annotation>
 </semantics>
</math>

</p>

<p>and since 

<math display="inline" id="Least_squares:11">
 <semantics>
  <mrow>
   <msub>
    <mi>r</mi>
    <mi>i</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>y</mi>
     <mi>i</mi>
    </msub>
    <mo>-</mo>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
      <mo>,</mo>
      <mi>𝜷</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>r</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <times></times>
      <ci>f</ci>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
       <ci>𝜷</ci>
      </interval>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r_{i}=y_{i}-f(x_{i},\boldsymbol{\beta})
  </annotation>
 </semantics>
</math>

, the gradient equations become</p>

<p>

<math display="block" id="Least_squares:12">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <mo>-</mo>
      <mrow>
       <mn>2</mn>
       <mrow>
        <munder>
         <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
         <mi>i</mi>
        </munder>
        <mrow>
         <msub>
          <mi>r</mi>
          <mi>i</mi>
         </msub>
         <mfrac>
          <mrow>
           <mrow>
            <mo>∂</mo>
            <mi>f</mi>
           </mrow>
           <mrow>
            <mo stretchy="false">(</mo>
            <msub>
             <mi>x</mi>
             <mi>i</mi>
            </msub>
            <mo>,</mo>
            <mi>𝜷</mi>
            <mo stretchy="false">)</mo>
           </mrow>
          </mrow>
          <mrow>
           <mo>∂</mo>
           <msub>
            <mi>β</mi>
            <mi>j</mi>
           </msub>
          </mrow>
         </mfrac>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
     <mo>=</mo>
     <mn>0</mn>
    </mrow>
    <mo rspace="7.5pt">,</mo>
    <mrow>
     <mi>j</mi>
     <mo>=</mo>
     <mrow>
      <mn>1</mn>
      <mo>,</mo>
      <mi mathvariant="normal">…</mi>
      <mo>,</mo>
      <mi>m</mi>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <eq></eq>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <cn type="integer">2</cn>
       <apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <ci>i</ci>
        </apply>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>r</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <divide></divide>
          <apply>
           <times></times>
           <apply>
            <partialdiff></partialdiff>
            <ci>f</ci>
           </apply>
           <interval closure="open">
            <apply>
             <csymbol cd="ambiguous">subscript</csymbol>
             <ci>x</ci>
             <ci>i</ci>
            </apply>
            <ci>𝜷</ci>
           </interval>
          </apply>
          <apply>
           <partialdiff></partialdiff>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>β</ci>
            <ci>j</ci>
           </apply>
          </apply>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
     <cn type="integer">0</cn>
    </apply>
    <apply>
     <eq></eq>
     <ci>j</ci>
     <list>
      <cn type="integer">1</cn>
      <ci>normal-…</ci>
      <ci>m</ci>
     </list>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   -2\sum_{i}r_{i}\frac{\partial f(x_{i},\boldsymbol{\beta})}{\partial\beta_{j}}=%
0,\ j=1,\ldots,m.
  </annotation>
 </semantics>
</math>

</p>

<p>The gradient equations apply to all least squares problems. Each particular problem requires particular expressions for the model and its partial derivatives.</p>
<h3 id="linear-least-squares">Linear least squares</h3>

<p>A regression model is a linear one when the model comprises a <a href="linear_combination" title="wikilink">linear combination</a> of the parameters, i.e.,</p>

<p>

<math display="block" id="Least_squares:13">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo>,</mo>
      <mi>β</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>j</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>m</mi>
     </munderover>
     <mrow>
      <msub>
       <mi>β</mi>
       <mi>j</mi>
      </msub>
      <msub>
       <mi>ϕ</mi>
       <mi>j</mi>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <interval closure="open">
      <ci>x</ci>
      <ci>β</ci>
     </interval>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>j</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>m</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>β</ci>
       <ci>j</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ϕ</ci>
       <ci>j</ci>
      </apply>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x,\beta)=\sum_{j=1}^{m}\beta_{j}\phi_{j}(x),
  </annotation>
 </semantics>
</math>

</p>

<p>where the function 

<math display="inline" id="Least_squares:14">
 <semantics>
  <msub>
   <mi>ϕ</mi>
   <mi>j</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>ϕ</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \phi_{j}
  </annotation>
 </semantics>
</math>

 is a function of 

<math display="inline" id="Least_squares:15">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

.</p>

<p>Letting</p>

<p>

<math display="block" id="Least_squares:16">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>X</mi>
     <mrow>
      <mi>i</mi>
      <mi>j</mi>
     </mrow>
    </msub>
    <mo>=</mo>
    <mfrac>
     <mrow>
      <mrow>
       <mo>∂</mo>
       <mi>f</mi>
      </mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>x</mi>
        <mi>i</mi>
       </msub>
       <mo>,</mo>
       <mi>𝜷</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mrow>
      <mo>∂</mo>
      <msub>
       <mi>β</mi>
       <mi>j</mi>
      </msub>
     </mrow>
    </mfrac>
    <mo>=</mo>
    <mrow>
     <msub>
      <mi>ϕ</mi>
      <mi>j</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>X</ci>
      <apply>
       <times></times>
       <ci>i</ci>
       <ci>j</ci>
      </apply>
     </apply>
     <apply>
      <divide></divide>
      <apply>
       <times></times>
       <apply>
        <partialdiff></partialdiff>
        <ci>f</ci>
       </apply>
       <interval closure="open">
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
        <ci>𝜷</ci>
       </interval>
      </apply>
      <apply>
       <partialdiff></partialdiff>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>β</ci>
        <ci>j</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ϕ</ci>
       <ci>j</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X_{ij}=\frac{\partial f(x_{i},\boldsymbol{\beta})}{\partial\beta_{j}}=\phi_{j}%
(x_{i}),
  </annotation>
 </semantics>
</math>

</p>

<p>we can then see that in that case the least square estimate (or estimator, in the context of a random sample), 

<math display="inline" id="Least_squares:17">
 <semantics>
  <mi>𝜷</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝜷</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \boldsymbol{\beta}
  </annotation>
 </semantics>
</math>

 is given by</p>

<p>

<math display="block" id="Least_squares:18">
 <semantics>
  <mrow>
   <mrow>
    <mover accent="true">
     <mi>𝜷</mi>
     <mo mathvariant="bold" stretchy="false">^</mo>
    </mover>
    <mo>=</mo>
    <mrow>
     <msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <msup>
         <mi>X</mi>
         <mi>T</mi>
        </msup>
        <mi>X</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mrow>
       <mo>-</mo>
       <mn>1</mn>
      </mrow>
     </msup>
     <msup>
      <mi>X</mi>
      <mi>T</mi>
     </msup>
     <mi>𝒚</mi>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>bold-^</ci>
     <ci>𝜷</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>X</ci>
        <ci>T</ci>
       </apply>
       <ci>X</ci>
      </apply>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>X</ci>
      <ci>T</ci>
     </apply>
     <ci>𝒚</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \boldsymbol{\hat{\beta}}=(X^{T}X)^{-1}X^{T}\boldsymbol{y}.
  </annotation>
 </semantics>
</math>

</p>

<p>For a derivation of this estimate see <a href="Linear_least_squares_(mathematics)" title="wikilink">Linear least squares (mathematics)</a>.</p>
<h3 id="non-linear-least-squares">Non-linear least squares</h3>

<p>There is no closed-form solution to a non-linear least squares problem. Instead, numerical algorithms are used to find the value of the parameters 

<math display="inline" id="Least_squares:19">
 <semantics>
  <mi>β</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>β</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \beta
  </annotation>
 </semantics>
</math>

 that minimizes the objective. Most algorithms involve choosing initial values for the parameters. Then, the parameters are refined iteratively, that is, the values are obtained by successive approximation:</p>

<p>

<math display="block" id="Least_squares:20">
 <semantics>
  <mrow>
   <mrow>
    <mmultiscripts>
     <mi>β</mi>
     <mi>j</mi>
     <none></none>
     <none></none>
     <mrow>
      <mi>k</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
    </mmultiscripts>
    <mo>=</mo>
    <mrow>
     <mmultiscripts>
      <mi>β</mi>
      <mi>j</mi>
      <none></none>
      <none></none>
      <mi>k</mi>
     </mmultiscripts>
     <mo>+</mo>
     <mrow>
      <mi mathvariant="normal">Δ</mi>
      <msub>
       <mi>β</mi>
       <mi>j</mi>
      </msub>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>β</ci>
      <ci>j</ci>
     </apply>
     <apply>
      <plus></plus>
      <ci>k</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>β</ci>
       <ci>j</ci>
      </apply>
      <ci>k</ci>
     </apply>
     <apply>
      <times></times>
      <ci>normal-Δ</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>β</ci>
       <ci>j</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   {\beta_{j}}^{k+1}={\beta_{j}}^{k}+\Delta\beta_{j},
  </annotation>
 </semantics>
</math>

 where <em>k</em> is an iteration number, and the vector of increments 

<math display="inline" id="Least_squares:21">
 <semantics>
  <mrow>
   <mi mathvariant="normal">Δ</mi>
   <msub>
    <mi>β</mi>
    <mi>j</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-Δ</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>β</ci>
     <ci>j</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Delta\beta_{j}
  </annotation>
 </semantics>
</math>

 is called the shift vector. In some commonly used algorithms, at each iteration the model may be linearized by approximation to a first-order <a href="Taylor_series" title="wikilink">Taylor series</a> expansion about 

<math display="inline" id="Least_squares:22">
 <semantics>
  <msup>
   <mi>𝜷</mi>
   <mi>k</mi>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>𝜷</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \boldsymbol{\beta}^{k}
  </annotation>
 </semantics>
</math>

:</p>

<p>

<math display="inline" id="Least_squares:23">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mi>i</mi>
    </msub>
    <mo>,</mo>
    <mi>𝜷</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>i</ci>
     </apply>
     <ci>𝜷</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle f(x_{i},\boldsymbol{\beta})
  </annotation>
 </semantics>
</math>


</p>

<p>The <a href="Jacobian_matrix_and_determinant" title="wikilink">Jacobian</a> <strong>J</strong> is a function of constants, the independent variable <em>and</em> the parameters, so it changes from one iteration to the next. The residuals are given by</p>

<p>

<math display="block" id="Least_squares:24">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>r</mi>
     <mi>i</mi>
    </msub>
    <mo>=</mo>
    <mrow>
     <msub>
      <mi>y</mi>
      <mi>i</mi>
     </msub>
     <mo>-</mo>
     <mrow>
      <msup>
       <mi>f</mi>
       <mi>k</mi>
      </msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>x</mi>
        <mi>i</mi>
       </msub>
       <mo>,</mo>
       <mi>𝜷</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>-</mo>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>k</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>m</mi>
      </munderover>
      <mrow>
       <msub>
        <mi>J</mi>
        <mrow>
         <mi>i</mi>
         <mi>k</mi>
        </mrow>
       </msub>
       <mi mathvariant="normal">Δ</mi>
       <msub>
        <mi>β</mi>
        <mi>k</mi>
       </msub>
      </mrow>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mi mathvariant="normal">Δ</mi>
      <msub>
       <mi>y</mi>
       <mi>i</mi>
      </msub>
     </mrow>
     <mo>-</mo>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>j</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>m</mi>
      </munderover>
      <mrow>
       <msub>
        <mi>J</mi>
        <mrow>
         <mi>i</mi>
         <mi>j</mi>
        </mrow>
       </msub>
       <mi mathvariant="normal">Δ</mi>
       <msub>
        <mi>β</mi>
        <mi>j</mi>
       </msub>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>r</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>f</ci>
        <ci>k</ci>
       </apply>
       <interval closure="open">
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
        <ci>𝜷</ci>
       </interval>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>k</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>m</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>J</ci>
         <apply>
          <times></times>
          <ci>i</ci>
          <ci>k</ci>
         </apply>
        </apply>
        <ci>normal-Δ</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>β</ci>
         <ci>k</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>normal-Δ</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <ci>i</ci>
       </apply>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>j</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>m</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>J</ci>
         <apply>
          <times></times>
          <ci>i</ci>
          <ci>j</ci>
         </apply>
        </apply>
        <ci>normal-Δ</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>β</ci>
         <ci>j</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r_{i}=y_{i}-f^{k}(x_{i},\boldsymbol{\beta})-\sum_{k=1}^{m}J_{ik}\Delta\beta_{k%
}=\Delta y_{i}-\sum_{j=1}^{m}J_{ij}\Delta\beta_{j}.
  </annotation>
 </semantics>
</math>

</p>

<p>To minimize the sum of squares of 

<math display="inline" id="Least_squares:25">
 <semantics>
  <msub>
   <mi>r</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>r</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r_{i}
  </annotation>
 </semantics>
</math>

, the gradient equation is set to zero and solved for 

<math display="inline" id="Least_squares:26">
 <semantics>
  <mrow>
   <mi mathvariant="normal">Δ</mi>
   <msub>
    <mi>β</mi>
    <mi>j</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-Δ</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>β</ci>
     <ci>j</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Delta\beta_{j}
  </annotation>
 </semantics>
</math>

:</p>

<p>

<math display="block" id="Least_squares:27">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mo>-</mo>
     <mrow>
      <mn>2</mn>
      <mrow>
       <munderover>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mrow>
         <mi>i</mi>
         <mo>=</mo>
         <mn>1</mn>
        </mrow>
        <mi>n</mi>
       </munderover>
       <mrow>
        <msub>
         <mi>J</mi>
         <mrow>
          <mi>i</mi>
          <mi>j</mi>
         </mrow>
        </msub>
        <mrow>
         <mo>(</mo>
         <mrow>
          <mrow>
           <mi mathvariant="normal">Δ</mi>
           <msub>
            <mi>y</mi>
            <mi>i</mi>
           </msub>
          </mrow>
          <mo>-</mo>
          <mrow>
           <munderover>
            <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
            <mrow>
             <mi>k</mi>
             <mo>=</mo>
             <mn>1</mn>
            </mrow>
            <mi>m</mi>
           </munderover>
           <mrow>
            <msub>
             <mi>J</mi>
             <mrow>
              <mi>i</mi>
              <mi>k</mi>
             </mrow>
            </msub>
            <mi mathvariant="normal">Δ</mi>
            <msub>
             <mi>β</mi>
             <mi>k</mi>
            </msub>
           </mrow>
          </mrow>
         </mrow>
         <mo>)</mo>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mn>0</mn>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <cn type="integer">2</cn>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>n</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>J</ci>
         <apply>
          <times></times>
          <ci>i</ci>
          <ci>j</ci>
         </apply>
        </apply>
        <apply>
         <minus></minus>
         <apply>
          <times></times>
          <ci>normal-Δ</ci>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>y</ci>
           <ci>i</ci>
          </apply>
         </apply>
         <apply>
          <apply>
           <csymbol cd="ambiguous">superscript</csymbol>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <sum></sum>
            <apply>
             <eq></eq>
             <ci>k</ci>
             <cn type="integer">1</cn>
            </apply>
           </apply>
           <ci>m</ci>
          </apply>
          <apply>
           <times></times>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>J</ci>
            <apply>
             <times></times>
             <ci>i</ci>
             <ci>k</ci>
            </apply>
           </apply>
           <ci>normal-Δ</ci>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>β</ci>
            <ci>k</ci>
           </apply>
          </apply>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   -2\sum_{i=1}^{n}J_{ij}\left(\Delta y_{i}-\sum_{k=1}^{m}J_{ik}\Delta\beta_{k}%
\right)=0,
  </annotation>
 </semantics>
</math>

</p>

<p>which, on rearrangement, become <em>m</em> simultaneous linear equations, the <strong>normal equations</strong>:</p>

<p>

<math display="block" id="Least_squares:28">
 <semantics>
  <mrow>
   <munderover>
    <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
    <mrow>
     <mi>i</mi>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mi>n</mi>
   </munderover>
   <munderover>
    <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
    <mrow>
     <mi>k</mi>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mi>m</mi>
   </munderover>
   <msub>
    <mi>J</mi>
    <mrow>
     <mi>i</mi>
     <mi>j</mi>
    </mrow>
   </msub>
   <msub>
    <mi>J</mi>
    <mrow>
     <mi>i</mi>
     <mi>k</mi>
    </mrow>
   </msub>
   <mi mathvariant="normal">Δ</mi>
   <msub>
    <mi>β</mi>
    <mi>k</mi>
   </msub>
   <mo>=</mo>
   <munderover>
    <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
    <mrow>
     <mi>i</mi>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mi>n</mi>
   </munderover>
   <msub>
    <mi>J</mi>
    <mrow>
     <mi>i</mi>
     <mi>j</mi>
    </mrow>
   </msub>
   <mi mathvariant="normal">Δ</mi>
   <msub>
    <mi>y</mi>
    <mi>i</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>j</mi>
    <mo>=</mo>
    <mn>1</mn>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <mi>m</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <eq></eq>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>n</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <eq></eq>
       <ci>k</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>m</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>J</ci>
     <apply>
      <times></times>
      <ci>i</ci>
      <ci>j</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>J</ci>
     <apply>
      <times></times>
      <ci>i</ci>
      <ci>k</ci>
     </apply>
    </apply>
    <csymbol cd="unknown">Δ</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>β</ci>
     <ci>k</ci>
    </apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <eq></eq>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>n</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>J</ci>
     <apply>
      <times></times>
      <ci>i</ci>
      <ci>j</ci>
     </apply>
    </apply>
    <csymbol cd="unknown">Δ</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <ci>i</ci>
    </apply>
    <ci>italic-</ci>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">j</csymbol>
     <eq></eq>
     <cn type="integer">1</cn>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <csymbol cd="unknown">m</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-.</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{i=1}^{n}\sum_{k=1}^{m}J_{ij}J_{ik}\Delta\beta_{k}=\sum_{i=1}^{n}J_{ij}%
\Delta y_{i}\qquad(j=1,\ldots,m).
  </annotation>
 </semantics>
</math>

</p>

<p>The normal equations are written in matrix notation as</p>

<p>

<math display="block" id="Least_squares:29">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <mo>(</mo>
      <mrow>
       <msup>
        <mi>𝐉</mi>
        <mi>𝐓</mi>
       </msup>
       <mi>𝐉</mi>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mi>𝚫</mi>
     <mi>𝜷</mi>
    </mrow>
    <mo>=</mo>
    <mrow>
     <msup>
      <mi>𝐉</mi>
      <mi>𝐓</mi>
     </msup>
     <mi>𝚫</mi>
     <mi>𝐲</mi>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>𝐉</ci>
       <ci>𝐓</ci>
      </apply>
      <ci>𝐉</ci>
     </apply>
     <ci>𝚫</ci>
     <ci>𝜷</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>𝐉</ci>
      <ci>𝐓</ci>
     </apply>
     <ci>𝚫</ci>
     <ci>𝐲</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{\left(J^{T}J\right)\Delta\boldsymbol{\beta}=J^{T}\Delta y}.\,
  </annotation>
 </semantics>
</math>

</p>

<p>These are the defining equations of the <a href="Gauss–Newton_algorithm" title="wikilink">Gauss–Newton algorithm</a>.</p>
<h3 id="differences-between-linear-and-non-linear-least-squares">Differences between linear and non-linear least squares</h3>
<ul>
<li>The model function, <em>f</em>, in LLSQ (linear least squares) is a linear combination of parameters of the form 

<math display="inline" id="Least_squares:30">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <msub>
      <mi>X</mi>
      <mrow>
       <mi>i</mi>
       <mn>1</mn>
      </mrow>
     </msub>
     <msub>
      <mi>β</mi>
      <mn>1</mn>
     </msub>
    </mrow>
    <mo>+</mo>
    <mrow>
     <msub>
      <mi>X</mi>
      <mrow>
       <mi>i</mi>
       <mn>2</mn>
      </mrow>
     </msub>
     <msub>
      <mi>β</mi>
      <mn>2</mn>
     </msub>
    </mrow>
    <mo>+</mo>
    <mi mathvariant="normal">⋯</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>f</ci>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>X</ci>
       <apply>
        <times></times>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>β</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>X</ci>
       <apply>
        <times></times>
        <ci>i</ci>
        <cn type="integer">2</cn>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>β</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
     <ci>normal-⋯</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f=X_{i1}\beta_{1}+X_{i2}\beta_{2}+\cdots
  </annotation>
 </semantics>
</math>

 The model may represent a straight line, a parabola or any other linear combination of functions. In NLLSQ (non-linear least squares) the parameters appear as functions, such as 

<math display="inline" id="Least_squares:31">
 <semantics>
  <mrow>
   <msup>
    <mi>β</mi>
    <mn>2</mn>
   </msup>
   <mo>,</mo>
   <msup>
    <mi>e</mi>
    <mrow>
     <mi>β</mi>
     <mi>x</mi>
    </mrow>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>β</ci>
     <cn type="integer">2</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>e</ci>
     <apply>
      <times></times>
      <ci>β</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \beta^{2},e^{\beta x}
  </annotation>
 </semantics>
</math>

 and so forth. If the derivatives 

<math display="inline" id="Least_squares:32">
 <semantics>
  <mrow>
   <mrow>
    <mo>∂</mo>
    <mi>f</mi>
   </mrow>
   <mo>/</mo>
   <mrow>
    <mo>∂</mo>
    <msub>
     <mi>β</mi>
     <mi>j</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <partialdiff></partialdiff>
     <ci>f</ci>
    </apply>
    <apply>
     <partialdiff></partialdiff>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>β</ci>
      <ci>j</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \partial f/\partial\beta_{j}
  </annotation>
 </semantics>
</math>

 are either constant or depend only on the values of the independent variable, the model is linear in the parameters. Otherwise the model is non-linear.</li>
<li>Algorithms for finding the solution to a NLLSQ problem require initial values for the parameters, LLSQ does not.</li>
<li>Like LLSQ, solution algorithms for NLLSQ often require that the Jacobian be calculated. Analytical expressions for the partial derivatives can be complicated. If analytical expressions are impossible to obtain either the partial derivatives must be calculated by numerical approximation or an estimate must be made of the Jacobian.</li>
<li>In NLLSQ non-convergence (failure of the algorithm to find a minimum) is a common phenomenon whereas the LLSQ is globally concave so non-convergence is not an issue.</li>
<li>NLLSQ is usually an iterative process. The iterative process has to be terminated when a convergence criterion is satisfied. LLSQ solutions can be computed using direct methods, although problems with large numbers of parameters are typically solved with iterative methods, such as the <a class="uri" href="Gauss–Seidel" title="wikilink">Gauss–Seidel</a> method.</li>
<li>In LLSQ the solution is unique, but in NLLSQ there may be multiple minima in the sum of squares.</li>
<li>Under the condition that the errors are uncorrelated with the predictor variables, LLSQ yields unbiased estimates, but even under that condition NLLSQ estimates are generally biased.</li>
</ul>

<p>These differences must be considered whenever the solution to a non-linear least squares problem is being sought.</p>
<h2 id="least-squares-regression-analysis-and-statistics">Least squares, regression analysis and statistics</h2>

<p>The method of least squares is often used to generate estimators and other statistics in regression analysis.</p>

<p>Consider a simple example drawn from physics. A spring should obey <a href="Hooke's_law" title="wikilink">Hooke's law</a> which states that the extension of a spring 

<math display="inline" id="Least_squares:33">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 is proportional to the force, <em>F</em>, applied to it.</p>

<p>

<math display="block" id="Least_squares:34">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>F</mi>
     <mo>,</mo>
     <mi>k</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>k</mi>
    <mpadded width="-1.7pt">
     <mi>F</mi>
    </mpadded>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>y</ci>
     <apply>
      <times></times>
      <ci>f</ci>
      <interval closure="open">
       <ci>F</ci>
       <ci>k</ci>
      </interval>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>k</ci>
      <ci>F</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=f(F,k)=kF\!
  </annotation>
 </semantics>
</math>

 constitutes the model, where <em>F</em> is the independent variable. To estimate the <a href="force_constant" title="wikilink">force constant</a>, <em>k</em>, a series of <em>n</em> measurements with different forces will produce a set of data, 

<math display="inline" id="Least_squares:35">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>F</mi>
       <mi>i</mi>
      </msub>
      <mo>,</mo>
      <msub>
       <mi>y</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo rspace="7.5pt">,</mo>
     <mi>i</mi>
    </mrow>
    <mo>=</mo>
    <mn>1</mn>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <mpadded width="-1.7pt">
     <mi>n</mi>
    </mpadded>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <eq></eq>
     <list>
      <interval closure="open">
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>F</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <ci>i</ci>
       </apply>
      </interval>
      <ci>i</ci>
     </list>
     <cn type="integer">1</cn>
    </apply>
    <list>
     <ci>normal-…</ci>
     <ci>n</ci>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (F_{i},y_{i}),\ i=1,\dots,n\!
  </annotation>
 </semantics>
</math>

, where <em>y<sub>i</sub></em> is a measured spring extension. Each experimental observation will contain some error. If we denote this error 

<math display="inline" id="Least_squares:36">
 <semantics>
  <mi>ε</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ε</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \varepsilon
  </annotation>
 </semantics>
</math>

, we may specify an empirical model for our observations,</p>

<p>

<math display="block" id="Least_squares:37">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>y</mi>
     <mi>i</mi>
    </msub>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mi>k</mi>
      <msub>
       <mi>F</mi>
       <mi>i</mi>
      </msub>
     </mrow>
     <mo>+</mo>
     <msub>
      <mi>ε</mi>
      <mi>i</mi>
     </msub>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <ci>k</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>F</ci>
       <ci>i</ci>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>ε</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{i}=kF_{i}+\varepsilon_{i}.\,
  </annotation>
 </semantics>
</math>

</p>

<p>There are many methods we might use to estimate the unknown parameter <em>k</em>. Noting that the <em>n</em> equations in the <em>m</em> variables in our data comprise an <a href="overdetermined_system" title="wikilink">overdetermined system</a> with one unknown and <em>n</em> equations, we may choose to estimate <em>k</em> using least squares. The sum of squares to be minimized is</p>

<p>

<math display="block" id="Least_squares:38">
 <semantics>
  <mrow>
   <mrow>
    <mi>S</mi>
    <mo>=</mo>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </munderover>
     <msup>
      <mrow>
       <mo>(</mo>
       <mrow>
        <msub>
         <mi>y</mi>
         <mi>i</mi>
        </msub>
        <mo>-</mo>
        <mrow>
         <mi>k</mi>
         <msub>
          <mi>F</mi>
          <mi>i</mi>
         </msub>
        </mrow>
       </mrow>
       <mo>)</mo>
      </mrow>
      <mn>2</mn>
     </msup>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>S</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>n</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <minus></minus>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <times></times>
        <ci>k</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>F</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S=\sum_{i=1}^{n}\left(y_{i}-kF_{i}\right)^{2}.
  </annotation>
 </semantics>
</math>

</p>

<p>The least squares estimate of the force constant, <em>k</em>, is given by</p>

<p>

<math display="block" id="Least_squares:39">
 <semantics>
  <mrow>
   <mrow>
    <mover accent="true">
     <mi>k</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mo>=</mo>
    <mfrac>
     <mrow>
      <msub>
       <mo largeop="true" symmetric="true">∑</mo>
       <mi>i</mi>
      </msub>
      <mrow>
       <msub>
        <mi>F</mi>
        <mi>i</mi>
       </msub>
       <msub>
        <mi>y</mi>
        <mi>i</mi>
       </msub>
      </mrow>
     </mrow>
     <mrow>
      <msub>
       <mo largeop="true" symmetric="true">∑</mo>
       <mi>i</mi>
      </msub>
      <mmultiscripts>
       <mi>F</mi>
       <mi>i</mi>
       <none></none>
       <none></none>
       <mn>2</mn>
      </mmultiscripts>
     </mrow>
    </mfrac>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>normal-^</ci>
     <ci>k</ci>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <ci>i</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>F</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>F</ci>
        <ci>i</ci>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{k}=\frac{\sum_{i}F_{i}y_{i}}{\sum_{i}{F_{i}}^{2}}.
  </annotation>
 </semantics>
</math>

</p>

<p>Here it is assumed that application of the force <strong><em>causes</em></strong> the spring to expand and, having derived the force constant by least squares fitting, the extension can be predicted from Hooke's law.</p>

<p>In regression analysis the researcher specifies an empirical model. For example, a very common model is the straight line model which is used to test if there is a linear relationship between dependent and independent variable. If a linear relationship is found to exist, the variables are said to be <a class="uri" href="correlated" title="wikilink">correlated</a>. However, <a href="Correlation_does_not_imply_causation" title="wikilink">correlation does not prove causation</a>, as both variables may be correlated with other, hidden, variables, or the dependent variable may "reverse" cause the independent variables, or the variables may be otherwise spuriously correlated. For example, suppose there is a correlation between deaths by drowning and the volume of ice cream sales at a particular beach. Yet, both the number of people going swimming and the volume of ice cream sales increase as the weather gets hotter, and presumably the number of deaths by drowning is correlated with the number of people going swimming. Perhaps an increase in swimmers causes both the other variables to increase.</p>

<p>In order to make statistical tests on the results it is necessary to make assumptions about the nature of the experimental errors. A common (but not necessary) assumption is that the errors belong to a normal distribution. The <a href="central_limit_theorem" title="wikilink">central limit theorem</a> supports the idea that this is a good approximation in many cases.</p>
<ul>
<li>The <a href="Gauss–Markov_theorem" title="wikilink">Gauss–Markov theorem</a>. In a linear model in which the errors have <a href="expected_value" title="wikilink">expectation</a> zero conditional on the independent variables, are <a class="uri" href="uncorrelated" title="wikilink">uncorrelated</a> and have equal <a href="variance" title="wikilink">variances</a>, the best linear <a class="uri" href="unbiased" title="wikilink">unbiased</a> estimator of any linear combination of the observations, is its least-squares estimator. "Best" means that the least squares estimators of the parameters have minimum variance. The assumption of equal variance is valid when the errors all belong to the same distribution.</li>
<li>In a linear model, if the errors belong to a normal distribution the least squares estimators are also the <a href="maximum_likelihood_estimator" title="wikilink">maximum likelihood estimators</a>.</li>
</ul>

<p>However, if the errors are not normally distributed, a <a href="central_limit_theorem" title="wikilink">central limit theorem</a> often nonetheless implies that the parameter estimates will be approximately normally distributed so long as the sample is reasonably large. For this reason, given the important property that the error mean is independent of the independent variables, the distribution of the error term is not an important issue in regression analysis. Specifically, it is not typically important whether the error term follows a normal distribution.</p>

<p>In a least squares calculation with unit weights, or in linear regression, the variance on the <em>j</em>th parameter, denoted 

<math display="inline" id="Least_squares:40">
 <semantics>
  <mrow>
   <mo>var</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mover accent="true">
      <mi>β</mi>
      <mo stretchy="false">^</mo>
     </mover>
     <mi>j</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>var</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <ci>normal-^</ci>
      <ci>β</ci>
     </apply>
     <ci>j</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{var}(\hat{\beta}_{j})
  </annotation>
 </semantics>
</math>

, is usually estimated with</p>

<p>

<math display="block" id="Least_squares:41">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mtext>var</mtext>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mover accent="true">
        <mi>β</mi>
        <mo stretchy="false">^</mo>
       </mover>
       <mi>j</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <msup>
      <mi>σ</mi>
      <mn>2</mn>
     </msup>
     <msub>
      <mrow>
       <mo>(</mo>
       <msup>
        <mrow>
         <mo>[</mo>
         <mrow>
          <msup>
           <mi>X</mi>
           <mi>T</mi>
          </msup>
          <mi>X</mi>
         </mrow>
         <mo>]</mo>
        </mrow>
        <mrow>
         <mo>-</mo>
         <mn>1</mn>
        </mrow>
       </msup>
       <mo>)</mo>
      </mrow>
      <mrow>
       <mi>j</mi>
       <mi>j</mi>
      </mrow>
     </msub>
    </mrow>
    <mo>≈</mo>
    <mrow>
     <mfrac>
      <mi>S</mi>
      <mrow>
       <mi>n</mi>
       <mo>-</mo>
       <mi>m</mi>
      </mrow>
     </mfrac>
     <msub>
      <mrow>
       <mo>(</mo>
       <msup>
        <mrow>
         <mo>[</mo>
         <mrow>
          <msup>
           <mi>X</mi>
           <mi>T</mi>
          </msup>
          <mi>X</mi>
         </mrow>
         <mo>]</mo>
        </mrow>
        <mrow>
         <mo>-</mo>
         <mn>1</mn>
        </mrow>
       </msup>
       <mo>)</mo>
      </mrow>
      <mrow>
       <mi>j</mi>
       <mi>j</mi>
      </mrow>
     </msub>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <mtext>var</mtext>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-^</ci>
        <ci>β</ci>
       </apply>
       <ci>j</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>σ</ci>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="latexml">delimited-[]</csymbol>
         <apply>
          <times></times>
          <apply>
           <csymbol cd="ambiguous">superscript</csymbol>
           <ci>X</ci>
           <ci>T</ci>
          </apply>
          <ci>X</ci>
         </apply>
        </apply>
        <apply>
         <minus></minus>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <apply>
        <times></times>
        <ci>j</ci>
        <ci>j</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <approx></approx>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <apply>
       <divide></divide>
       <ci>S</ci>
       <apply>
        <minus></minus>
        <ci>n</ci>
        <ci>m</ci>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="latexml">delimited-[]</csymbol>
         <apply>
          <times></times>
          <apply>
           <csymbol cd="ambiguous">superscript</csymbol>
           <ci>X</ci>
           <ci>T</ci>
          </apply>
          <ci>X</ci>
         </apply>
        </apply>
        <apply>
         <minus></minus>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <apply>
        <times></times>
        <ci>j</ci>
        <ci>j</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \text{var}(\hat{\beta}_{j})=\sigma^{2}\left(\left[X^{T}X\right]^{-1}\right)_{%
jj}\approx\frac{S}{n-m}\left(\left[X^{T}X\right]^{-1}\right)_{jj},
  </annotation>
 </semantics>
</math>

 where the true residual variance σ<sup>2</sup> is replaced by an estimate based on the minimised value of the sum of squares objective function <em>S</em>. The denominator, <em>n</em> − <em>m</em>, is the <a href="Degrees_of_freedom_(statistics)" title="wikilink">statistical degrees of freedom</a>; see <a href="Degrees_of_freedom_(statistics)#Effective_degrees_of_freedom" title="wikilink">effective degrees of freedom</a> for generalizations.</p>

<p><a href="Confidence_limits" title="wikilink">Confidence limits</a> can be found if the <a href="probability_distribution" title="wikilink">probability distribution</a> of the parameters is known, or an asymptotic approximation is made, or assumed. Likewise statistical tests on the residuals can be made if the probability distribution of the residuals is known or assumed. The probability distribution of any linear combination of the dependent variables can be derived if the probability distribution of experimental errors is known or assumed. Inference is particularly straightforward if the errors are assumed to follow a normal distribution, which implies that the parameter estimates and residuals will also be normally distributed conditional on the values of the independent variables.</p>
<h2 id="weighted-least-squares">Weighted least squares</h2>

<p>A special case of <a href="generalized_least_squares" title="wikilink">generalized least squares</a> called <strong>weighted least squares</strong> occurs when all the off-diagonal entries of <em>Ω</em> (the correlation matrix of the residuals) are null; the <a href="variance" title="wikilink">variances</a> of the observations (along the covariance matrix diagonal) may still be unequal (<a class="uri" href="heteroskedasticity" title="wikilink">heteroskedasticity</a>).</p>

<p>The expressions given above are based on the implicit assumption that the errors are uncorrelated with each other and with the independent variables and have equal variance. The <a href="Gauss–Markov_theorem" title="wikilink">Gauss–Markov theorem</a> shows that, when this is so, 

<math display="inline" id="Least_squares:42">
 <semantics>
  <mover accent="true">
   <mi>𝜷</mi>
   <mo stretchy="false">^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>𝜷</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\boldsymbol{\beta}}
  </annotation>
 </semantics>
</math>

 is a <a href="best_linear_unbiased_estimator" title="wikilink">best linear unbiased estimator</a> (BLUE). If, however, the measurements are uncorrelated but have different uncertainties, a modified approach might be adopted. <a href="Alexander_Aitken" title="wikilink">Aitken</a> showed that when a weighted sum of squared residuals is minimized, 

<math display="inline" id="Least_squares:43">
 <semantics>
  <mover accent="true">
   <mi>𝜷</mi>
   <mo stretchy="false">^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>𝜷</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\boldsymbol{\beta}}
  </annotation>
 </semantics>
</math>

 is the <a href="Best_linear_unbiased_estimator" title="wikilink">BLUE</a> if each weight is equal to the reciprocal of the variance of the measurement</p>

<p>

<math display="block" id="Least_squares:44">
 <semantics>
  <mrow>
   <mrow>
    <mi>S</mi>
    <mo>=</mo>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </munderover>
     <mrow>
      <msub>
       <mi>W</mi>
       <mrow>
        <mi>i</mi>
        <mi>i</mi>
       </mrow>
      </msub>
      <mmultiscripts>
       <mi>r</mi>
       <mi>i</mi>
       <none></none>
       <none></none>
       <mn>2</mn>
      </mmultiscripts>
     </mrow>
    </mrow>
   </mrow>
   <mo rspace="22.5pt">,</mo>
   <mrow>
    <msub>
     <mi>W</mi>
     <mrow>
      <mi>i</mi>
      <mi>i</mi>
     </mrow>
    </msub>
    <mo>=</mo>
    <mfrac>
     <mn>1</mn>
     <mmultiscripts>
      <mi>σ</mi>
      <mi>i</mi>
      <none></none>
      <none></none>
      <mn>2</mn>
     </mmultiscripts>
    </mfrac>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <eq></eq>
     <ci>S</ci>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>W</ci>
        <apply>
         <times></times>
         <ci>i</ci>
         <ci>i</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>r</ci>
         <ci>i</ci>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>W</ci>
      <apply>
       <times></times>
       <ci>i</ci>
       <ci>i</ci>
      </apply>
     </apply>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>σ</ci>
        <ci>i</ci>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S=\sum_{i=1}^{n}W_{ii}{r_{i}}^{2},\qquad W_{ii}=\frac{1}{{\sigma_{i}}^{2}}
  </annotation>
 </semantics>
</math>

 The gradient equations for this sum of squares are</p>

<p>

<math display="block" id="Least_squares:45">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mo>-</mo>
     <mrow>
      <mn>2</mn>
      <mrow>
       <munder>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mi>i</mi>
       </munder>
       <mrow>
        <msub>
         <mi>W</mi>
         <mrow>
          <mi>i</mi>
          <mi>i</mi>
         </mrow>
        </msub>
        <mfrac>
         <mrow>
          <mrow>
           <mo>∂</mo>
           <mi>f</mi>
          </mrow>
          <mrow>
           <mo stretchy="false">(</mo>
           <msub>
            <mi>x</mi>
            <mi>i</mi>
           </msub>
           <mo>,</mo>
           <mi>𝜷</mi>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
         <mrow>
          <mo>∂</mo>
          <msub>
           <mi>β</mi>
           <mi>j</mi>
          </msub>
         </mrow>
        </mfrac>
        <msub>
         <mi>r</mi>
         <mi>i</mi>
        </msub>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mn>0</mn>
   </mrow>
   <mo rspace="22.5pt">,</mo>
   <mrow>
    <mi>j</mi>
    <mo>=</mo>
    <mrow>
     <mn>1</mn>
     <mo>,</mo>
     <mi mathvariant="normal">…</mi>
     <mo>,</mo>
     <mi>n</mi>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <eq></eq>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <cn type="integer">2</cn>
       <apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <ci>i</ci>
        </apply>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>W</ci>
          <apply>
           <times></times>
           <ci>i</ci>
           <ci>i</ci>
          </apply>
         </apply>
         <apply>
          <divide></divide>
          <apply>
           <times></times>
           <apply>
            <partialdiff></partialdiff>
            <ci>f</ci>
           </apply>
           <interval closure="open">
            <apply>
             <csymbol cd="ambiguous">subscript</csymbol>
             <ci>x</ci>
             <ci>i</ci>
            </apply>
            <ci>𝜷</ci>
           </interval>
          </apply>
          <apply>
           <partialdiff></partialdiff>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>β</ci>
            <ci>j</ci>
           </apply>
          </apply>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>r</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
     <cn type="integer">0</cn>
    </apply>
    <apply>
     <eq></eq>
     <ci>j</ci>
     <list>
      <cn type="integer">1</cn>
      <ci>normal-…</ci>
      <ci>n</ci>
     </list>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   -2\sum_{i}W_{ii}\frac{\partial f(x_{i},\boldsymbol{\beta})}{\partial\beta_{j}}%
r_{i}=0,\qquad j=1,\ldots,n
  </annotation>
 </semantics>
</math>

</p>

<p>which, in a linear least squares system give the modified normal equations,</p>

<p>

<math display="block" id="Least_squares:46">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>n</mi>
      </munderover>
      <mrow>
       <munderover>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mrow>
         <mi>k</mi>
         <mo>=</mo>
         <mn>1</mn>
        </mrow>
        <mi>m</mi>
       </munderover>
       <mrow>
        <msub>
         <mi>X</mi>
         <mrow>
          <mi>i</mi>
          <mi>j</mi>
         </mrow>
        </msub>
        <msub>
         <mi>W</mi>
         <mrow>
          <mi>i</mi>
          <mi>i</mi>
         </mrow>
        </msub>
        <msub>
         <mi>X</mi>
         <mrow>
          <mi>i</mi>
          <mi>k</mi>
         </mrow>
        </msub>
        <msub>
         <mover accent="true">
          <mi>β</mi>
          <mo stretchy="false">^</mo>
         </mover>
         <mi>k</mi>
        </msub>
       </mrow>
      </mrow>
     </mrow>
     <mo>=</mo>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>n</mi>
      </munderover>
      <mrow>
       <msub>
        <mi>X</mi>
        <mrow>
         <mi>i</mi>
         <mi>j</mi>
        </mrow>
       </msub>
       <msub>
        <mi>W</mi>
        <mrow>
         <mi>i</mi>
         <mi>i</mi>
        </mrow>
       </msub>
       <msub>
        <mi>y</mi>
        <mi>i</mi>
       </msub>
      </mrow>
     </mrow>
    </mrow>
    <mo rspace="22.5pt">,</mo>
    <mrow>
     <mi>j</mi>
     <mo>=</mo>
     <mrow>
      <mn>1</mn>
      <mo>,</mo>
      <mi mathvariant="normal">…</mi>
      <mo>,</mo>
      <mpadded width="+1.7pt">
       <mi>m</mi>
      </mpadded>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <eq></eq>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>k</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>m</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>X</ci>
         <apply>
          <times></times>
          <ci>i</ci>
          <ci>j</ci>
         </apply>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>W</ci>
         <apply>
          <times></times>
          <ci>i</ci>
          <ci>i</ci>
         </apply>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>X</ci>
         <apply>
          <times></times>
          <ci>i</ci>
          <ci>k</ci>
         </apply>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <apply>
          <ci>normal-^</ci>
          <ci>β</ci>
         </apply>
         <ci>k</ci>
        </apply>
       </apply>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>X</ci>
        <apply>
         <times></times>
         <ci>i</ci>
         <ci>j</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>W</ci>
        <apply>
         <times></times>
         <ci>i</ci>
         <ci>i</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <ci>j</ci>
     <list>
      <cn type="integer">1</cn>
      <ci>normal-…</ci>
      <ci>m</ci>
     </list>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{i=1}^{n}\sum_{k=1}^{m}X_{ij}W_{ii}X_{ik}\hat{\beta}_{k}=\sum_{i=1}^{n}X_%
{ij}W_{ii}y_{i},\qquad j=1,\ldots,m\,.
  </annotation>
 </semantics>
</math>

</p>

<p>When the observational errors are uncorrelated and the weight matrix, <strong>W</strong>, is diagonal, these may be written as</p>

<p>

<math display="block" id="Least_squares:47">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <mo>(</mo>
      <mrow>
       <msup>
        <mi>𝐗</mi>
        <mi>𝐓</mi>
       </msup>
       <mi>𝐖𝐗</mi>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mover accent="true">
      <mi>𝜷</mi>
      <mo stretchy="false">^</mo>
     </mover>
    </mrow>
    <mo>=</mo>
    <mrow>
     <msup>
      <mi>𝐗</mi>
      <mi>𝐓</mi>
     </msup>
     <mi>𝐖𝐲</mi>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>𝐗</ci>
       <ci>𝐓</ci>
      </apply>
      <ci>𝐖𝐗</ci>
     </apply>
     <apply>
      <ci>normal-^</ci>
      <ci>𝜷</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>𝐗</ci>
      <ci>𝐓</ci>
     </apply>
     <ci>𝐖𝐲</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{\left(X^{T}WX\right)\hat{\boldsymbol{\beta}}=X^{T}Wy}.
  </annotation>
 </semantics>
</math>

</p>

<p>If the errors are correlated, the resulting estimator is the BLUE if the weight matrix is equal to the inverse of the <a href="variance-covariance_matrix" title="wikilink">variance-covariance matrix</a> of the observations.</p>

<p>When the errors are uncorrelated, it is convenient to simplify the calculations to factor the weight matrix as 

<math display="inline" id="Least_squares:48">
 <semantics>
  <mrow>
   <msub>
    <mi>𝐰</mi>
    <mi>𝐢𝐢</mi>
   </msub>
   <mo>=</mo>
   <msqrt>
    <msub>
     <mi>𝐖</mi>
     <mi>𝐢𝐢</mi>
    </msub>
   </msqrt>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>𝐰</ci>
     <ci>𝐢𝐢</ci>
    </apply>
    <apply>
     <root></root>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐖</ci>
      <ci>𝐢𝐢</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w_{ii}}=\sqrt{\mathbf{W_{ii}}}
  </annotation>
 </semantics>
</math>

. The normal equations can then be written as</p>

<p>

<math display="block" id="Least_squares:49">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mo>(</mo>
     <mrow>
      <msup>
       <mi>𝐗</mi>
       <mrow>
        <mi mathvariant="normal">′</mi>
        <mi>𝐓</mi>
       </mrow>
      </msup>
      <msup>
       <mi>𝐗</mi>
       <mo>′</mo>
      </msup>
     </mrow>
     <mo>)</mo>
    </mrow>
    <mover accent="true">
     <mi>𝜷</mi>
     <mo stretchy="false">^</mo>
    </mover>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msup>
     <mi>𝐗</mi>
     <mrow>
      <mi mathvariant="normal">′</mi>
      <mi>𝐓</mi>
     </mrow>
    </msup>
    <mpadded width="+1.7pt">
     <msup>
      <mi>𝐲</mi>
      <mo>′</mo>
     </msup>
    </mpadded>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>𝐗</ci>
       <list>
        <ci>normal-′</ci>
        <ci>𝐓</ci>
       </list>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>𝐗</ci>
       <ci>normal-′</ci>
      </apply>
     </apply>
     <apply>
      <ci>normal-^</ci>
      <ci>𝜷</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>𝐗</ci>
      <list>
       <ci>normal-′</ci>
       <ci>𝐓</ci>
      </list>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>𝐲</ci>
      <ci>normal-′</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{\left(X^{\prime T}X^{\prime}\right)\hat{\boldsymbol{\beta}}=X^{\prime T%
}y^{\prime}}\,
  </annotation>
 </semantics>
</math>

</p>

<p>where</p>

<p>

<math display="block" id="Least_squares:50">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msup>
      <mi>𝐗</mi>
      <mo>′</mo>
     </msup>
     <mo>=</mo>
     <mi>𝐰𝐗</mi>
    </mrow>
    <mo>,</mo>
    <mrow>
     <msup>
      <mi>𝐲</mi>
      <mo>′</mo>
     </msup>
     <mo>=</mo>
     <mi>𝐰𝐲</mi>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>𝐗</ci>
      <ci>normal-′</ci>
     </apply>
     <ci>𝐰𝐗</ci>
    </apply>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>𝐲</ci>
      <ci>normal-′</ci>
     </apply>
     <ci>𝐰𝐲</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{X^{\prime}}=\mathbf{wX},\mathbf{y^{\prime}}=\mathbf{wy}.\,
  </annotation>
 </semantics>
</math>

</p>

<p>For non-linear least squares systems a similar argument shows that the normal equations should be modified as follows.</p>

<p>

<math display="block" id="Least_squares:51">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <mo>(</mo>
      <mrow>
       <msup>
        <mi>𝐉</mi>
        <mi>𝐓</mi>
       </msup>
       <mi>𝐖𝐉</mi>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mi>𝚫</mi>
     <mi>β</mi>
    </mrow>
    <mo>=</mo>
    <mrow>
     <msup>
      <mi>𝐉</mi>
      <mi>𝐓</mi>
     </msup>
     <mi>𝐖</mi>
     <mi>𝚫</mi>
     <mi>𝐲</mi>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>𝐉</ci>
       <ci>𝐓</ci>
      </apply>
      <ci>𝐖𝐉</ci>
     </apply>
     <ci>𝚫</ci>
     <ci>β</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>𝐉</ci>
      <ci>𝐓</ci>
     </apply>
     <ci>𝐖</ci>
     <ci>𝚫</ci>
     <ci>𝐲</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{\left(J^{T}WJ\right)\boldsymbol{\Delta}\beta=J^{T}W\boldsymbol{\Delta}%
y}.\,
  </annotation>
 </semantics>
</math>

</p>

<p>Note that for empirical tests, the appropriate <strong>W</strong> is not known for sure and must be estimated. For this <a href="feasible_generalized_least_squares" title="wikilink">feasible generalized least squares</a> (FGLS) techniques may be used.</p>
<h2 id="relationship-to-principal-components">Relationship to principal components</h2>

<p>The first <a href="Principal_component_analysis" title="wikilink">principal component</a> about the mean of a set of points can be represented by that line which most closely approaches the data points (as measured by squared distance of closest approach, i.e. perpendicular to the line). In contrast, linear least squares tries to minimize the distance in the 

<math display="inline" id="Least_squares:52">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 direction only. Thus, although the two use a similar error metric, linear least squares is a method that treats one dimension of the data preferentially, while PCA treats all dimensions equally.</p>
<h2 id="regularized-versions">Regularized versions</h2>
<h3 id="tikhonov-regularization">Tikhonov regularization</h3>

<p>In some contexts a <a href="Regularization_(machine_learning)" title="wikilink">regularized</a> version of the least squares solution may be preferable. <a href="Tikhonov_regularization" title="wikilink">Tikhonov regularization</a> (or <a href="ridge_regression" title="wikilink">ridge regression</a>) adds a constraint that 

<math display="inline" id="Least_squares:53">
 <semantics>
  <msup>
   <mrow>
    <mo>∥</mo>
    <mi>β</mi>
    <mo>∥</mo>
   </mrow>
   <mn>2</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <ci>β</ci>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|\beta\|^{2}
  </annotation>
 </semantics>
</math>

, the <a href="L2-norm" title="wikilink">L<sup>2</sup>-norm</a> of the parameter vector, is not greater than a given value. Equivalently, it may solve an unconstrained minimization of the least-squares penalty with 

<math display="inline" id="Least_squares:54">
 <semantics>
  <mrow>
   <mi>α</mi>
   <msup>
    <mrow>
     <mo>∥</mo>
     <mi>β</mi>
     <mo>∥</mo>
    </mrow>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>α</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <ci>β</ci>
     </apply>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha\|\beta\|^{2}
  </annotation>
 </semantics>
</math>

 added, where 

<math display="inline" id="Least_squares:55">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

 is a constant (this is the <a href="Lagrange_multipliers" title="wikilink">Lagrangian</a> form of the constrained problem). In a <a href="Bayesian_statistics" title="wikilink">Bayesian</a> context, this is equivalent to placing a zero-mean normally distributed <a href="prior_distribution" title="wikilink">prior</a> on the parameter vector.</p>
<h3 id="lasso-method">Lasso method</h3>

<p>An alternative <a href="Regularization_(machine_learning)" title="wikilink">regularized</a> version of least squares is <em>lasso</em> (least absolute shrinkage and selection operator), which uses the constraint that 

<math display="inline" id="Least_squares:56">
 <semantics>
  <msub>
   <mrow>
    <mo>∥</mo>
    <mi>β</mi>
    <mo>∥</mo>
   </mrow>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <ci>β</ci>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|\beta\|_{1}
  </annotation>
 </semantics>
</math>

, the <a href="L1-norm" title="wikilink">L<sup>1</sup>-norm</a> of the parameter vector, is no greater than a given value.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a><a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> (As above, this is equivalent to an unconstrained minimization of the least-squares penalty with 

<math display="inline" id="Least_squares:57">
 <semantics>
  <mrow>
   <mi>α</mi>
   <msub>
    <mrow>
     <mo>∥</mo>
     <mi>β</mi>
     <mo>∥</mo>
    </mrow>
    <mn>1</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>α</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <ci>β</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha\|\beta\|_{1}
  </annotation>
 </semantics>
</math>

 added.) In a <a href="Bayesian_statistics" title="wikilink">Bayesian</a> context, this is equivalent to placing a zero-mean <a href="Laplace_distribution" title="wikilink">Laplace</a> <a href="prior_distribution" title="wikilink">prior distribution</a> on the parameter vector.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> The optimization problem may be solved using <a href="quadratic_programming" title="wikilink">quadratic programming</a> or more general <a href="convex_optimization" title="wikilink">convex optimization</a> methods, as well as by specific algorithms such as the <a href="least_angle_regression" title="wikilink">least angle regression</a> algorithm.</p>

<p>One of the prime differences between Lasso and ridge regression is that in ridge regression, as the penalty is increased, all parameters are reduced while still remaining non-zero, while in Lasso, increasing the penalty will cause more and more of the parameters to be driven to zero. This is an advantage of Lasso over ridge regression, as driving parameters to zero deselects the features from the regression. Thus, Lasso automatically selects more relevant features and discards the others, whereas Ridge regression never fully discards any features. Some feature selection techniques are developed based on the LASSO including Bolasso which bootstraps samples,<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> and FeaLect which analyzes the regression coefficients corresponding to different values of 

<math display="inline" id="Least_squares:58">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

 to score all the features.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></p>

<p>The L<sup>1</sup>-regularized formulation is useful in some contexts due to its tendency to prefer solutions with fewer nonzero parameter values, effectively reducing the number of variables upon which the given solution is dependent.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> For this reason, the Lasso and its variants are fundamental to the field of <a href="compressed_sensing" title="wikilink">compressed sensing</a>. An extension of this approach is <a href="elastic_net_regularization" title="wikilink">elastic net regularization</a>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Adjustment_of_observations" title="wikilink">Adjustment of observations</a></li>
<li><a href="Minimum_mean_square_error" title="wikilink">Bayesian MMSE estimator</a></li>
<li><a href="Gauss–Markov_theorem" title="wikilink">Best linear unbiased estimator</a> (BLUE)</li>
<li><a href="Best_linear_unbiased_prediction" title="wikilink">Best linear unbiased prediction</a> (BLUP)</li>
<li><a href="Gauss–Markov_theorem" title="wikilink">Gauss–Markov theorem</a></li>
<li><a href="L2_norm" title="wikilink"><em>L</em><sub>2</sub> norm</a></li>
<li><a href="Least_absolute_deviation" title="wikilink">Least absolute deviation</a></li>
<li><a href="Measurement_uncertainty" title="wikilink">Measurement uncertainty</a></li>
<li><a href="Proximal_gradient_methods_for_learning" title="wikilink">Proximal gradient methods for learning</a></li>
<li><a href="Quadratic_loss_function" title="wikilink">Quadratic loss function</a></li>
<li><a href="Root_mean_square" title="wikilink">Root mean square</a></li>
<li><a href="Squared_deviations" title="wikilink">Squared deviations</a></li>
</ul>
<h2 id="references">References</h2>
<references>
</references>
<h2 id="further-reading">Further reading</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Least_squares" title="wikilink"> </a> <a href="Category:Regression_analysis" title="wikilink">Category:Regression analysis</a> <a href="Category:Single-equation_methods_(econometrics)" title="wikilink">Category:Single-equation methods (econometrics)</a> <a href="Category:Mathematical_and_quantitative_methods_(economics)" title="wikilink">Category:Mathematical and quantitative methods (economics)</a> <a href="Category:Mathematical_optimization" title="wikilink">Category:Mathematical optimization</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7">For a good introduction to error-in-variables, please see <a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"><a href="#fnref11">↩</a></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"></li>
</ol>
</section>
</body>
</html>
