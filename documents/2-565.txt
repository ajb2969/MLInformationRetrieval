   Square matrix      Square matrix   In mathematics , a square matrix is a matrix with the same number of rows and columns. An n -by- n matrix is known as a square matrix of order n . Any two square matrices of the same order can be added and multiplied.  Square matrices are often used to represent simple linear transformations , such as shearing or rotation . For example, if R is a square matrix representing a rotation ( rotation matrix ) and v is a column vector describing the position of a point in space, the product Rv yields another column vector describing the position of that point after that rotation. If v is a row vector , the same transformation can be obtained using vR T , where R T is the transpose of R .  Main diagonal  The entries a ii ( i = 1, ..., n ) form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix. For instance, the main diagonal of the 4-by-4 matrix above contains the elements a 11 =¬†9, a 22 =¬†11, a 33 =¬†4, a 44 =¬†10.  The diagonal of a square matrix from the top right to the bottom left corner is called antidiagonal or counterdiagonal .  Special kinds        Name   Example with n = 3       Diagonal matrix       [      a  11     0    0      0     a  22     0      0    0     a  33      ]       subscript  a  11   0  0    0   subscript  a  22   0    0  0   subscript  a  33      \begin{bmatrix}a_{11}&0&0\\
 0&a_{22}&0\\
 0&0&a_{33}\end{bmatrix}        Lower triangular matrix       [      a  11     0    0       a  21      a  22     0       a  31      a  32      a  33      ]       subscript  a  11   0  0     subscript  a  21    subscript  a  22   0     subscript  a  31    subscript  a  32    subscript  a  33      \begin{bmatrix}a_{11}&0&0\\
 a_{21}&a_{22}&0\\
 a_{31}&a_{32}&a_{33}\end{bmatrix}        Upper triangular matrix       [      a  11      a  12      a  13       0     a  22      a  23       0    0     a  33      ]       subscript  a  11    subscript  a  12    subscript  a  13     0   subscript  a  22    subscript  a  23     0  0   subscript  a  33      \begin{bmatrix}a_{11}&a_{12}&a_{13}\\
 0&a_{22}&a_{23}\\
 0&0&a_{33}\end{bmatrix}          Diagonal or triangular matrix  If all entries outside the main diagonal are zero, A is called a diagonal matrix . If only all entries above (or below) the main diagonal are zero, A is called a lower (or upper) triangular matrix .  Identity matrix  The identity matrix  I n of size n is the n -by- n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, e.g.         I  1   =   [     1     ]    ,     I  2   =    [     1    0      0    1     ]   ,  ‚ãØ    ,    I  n   =   [     1    0    ‚ãØ    0      0    1    ‚ãØ    0      ‚ãÆ    ‚ãÆ    ‚ã±    ‚ãÆ      0    0    ‚ãØ    1     ]      .     formulae-sequence     subscript  I  1     1      formulae-sequence     subscript  I  2      1  0    0  1    normal-‚ãØ       subscript  I  n     1  0  normal-‚ãØ  0    0  1  normal-‚ãØ  0    normal-‚ãÆ  normal-‚ãÆ  normal-‚ã±  normal-‚ãÆ    0  0  normal-‚ãØ  1        I_{1}=\begin{bmatrix}1\end{bmatrix},\ I_{2}=\begin{bmatrix}1&0\\
 0&1\end{bmatrix},\ \cdots,\ I_{n}=\begin{bmatrix}1&0&\cdots&0\\
 0&1&\cdots&0\\
 \vdots&\vdots&\ddots&\vdots\\
 0&0&\cdots&1\end{bmatrix}.   It is a square matrix of order n , and also a special kind of diagonal matrix . It is called identity matrix because multiplication with it leaves a matrix unchanged:    AI n = I m A = A for any m -by- n matrix A .    Symmetric or skew-symmetric matrix  A square matrix A that is equal to its transpose, i.e., A = A T , is a symmetric matrix . If instead, A was equal to the negative of its transpose, i.e., A = ‚àí A T , then A is a skew-symmetric matrix . In complex matrices, symmetry is often replaced by the concept of Hermitian matrices , which satisfy A ‚àó = A , where the star or asterisk denotes the conjugate transpose of the matrix, i.e., the transpose of the complex conjugate of A .  By the spectral theorem , real symmetric matrices and complex Hermitian matrices have an eigenbasis ; i.e., every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real. 1 This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below .  Invertible matrix and its inverse  A square matrix A is called invertible or non-singular if there exists a matrix B such that   AB = BA = I n . 2 3    If B exists, it is unique and is called the inverse matrix of A , denoted A ‚àí1 .  Definite matrix      Positive definite   Indefinite           [      1  /  4     0      0     1  /  4      ]        1  4   0    0    1  4      \begin{bmatrix}1/4&0\\
 0&1/4\\
 \end{bmatrix}          [      1  /  4     0      0     -   1  /  4       ]        1  4   0    0      1  4       \begin{bmatrix}1/4&0\\
 0&-1/4\end{bmatrix}        Q ( x , y ) = 1/4 x 2 + 1/4 y 2   Q ( x , y ) = 1/4 x 2 ‚àí 1/4 y 2     Points such that Q ( x , y )¬†=¬†1 ( Ellipse ).   Points such that Q ( x , y )¬†=¬†1 ( Hyperbola ).     A symmetric n √ó n -matrix is called'' positive-definite '' (respectively negative-definite; indefinite), if for all nonzero vectors x ‚àà R n the associated quadratic form given by   Q ( x ) = x T Ax    takes only positive values (respectively only negative values; both some negative and some positive values). 4 If the quadratic form takes only non-negative (respectively only non-positive) values, the symmetric matrix is called positive-semidefinite (respectively negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.  A symmetric matrix is positive-definite if and only if all its eigenvalues are positive. 5 The table at the right shows two possibilities for 2-by-2 matrices.  Allowing as input two different vectors instead yields the bilinear form associated to A :   B A ( x , y ) = x T Ay . 6    Orthogonal matrix  An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal  unit vectors (i.e., orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse :        A  T   =   A   -  1     ,       superscript  A  normal-T    superscript  A    1      A^{\mathrm{T}}=A^{-1},\,   which entails         A  T   A   =   A   A  T    =  I   ,           superscript  A  normal-T   A     A   superscript  A  normal-T         I     A^{\mathrm{T}}A=AA^{\mathrm{T}}=I,\,   where I is the identity matrix .  An orthogonal matrix A is necessarily invertible (with inverse ), unitary ( ), and normal (). The determinant of any orthogonal matrix is either +1 or ‚àí1. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation , every orthogonal matrix with determinant +1 is a pure rotation , while every orthogonal matrix with determinant¬†‚àí1 is either a pure reflection , or a composition of reflection and rotation.  The complex analogue of an orthogonal matrix is a unitary matrix .  Operations  Trace  The trace , tr( A ) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above , the trace of the product of two matrices is independent of the order of the factors:   tr( AB ) = tr( BA ).   This is immediate from the definition of matrix multiplication:        tr   (  ùñ†ùñ°  )    =     ‚àë   i  =  1   m       ‚àë   j  =  1   n      A   i  j     B   j  i       =   tr   (  ùñ°ùñ†  )     .         tr  ùñ†ùñ°     superscript   subscript     i  1    m     superscript   subscript     j  1    n      subscript  A    i  j     subscript  B    j  i             tr  ùñ°ùñ†      \scriptstyle\operatorname{tr}(\mathsf{AB})=\sum_{i=1}^{m}\sum_{j=1}^{n}A_{ij}B%
 _{ji}=\operatorname{tr}(\mathsf{BA}).   Also, the trace of a matrix is equal to that of its transpose, i.e.,    tr( A ) = tr( A T ).    Determinant  (Figure)  A linear transformation on R 2 given by the indicated matrix. The determinant of this matrix is ‚àí1, as the area of the green parallelogram at the right is 1, but the map reverses the orientation , since it turns the counterclockwise orientation of the vectors to a clockwise one.   The determinant det( A ) or | A | of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R 2 ) or volume (in R 3 ) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.  The determinant of 2-by-2 matrices is given by        det   [     a     b  ;       c     d  ;      ]    =    a  d   -   b  c     .          a  b    c  d         a  d     b  c      \det\begin{bmatrix}a&b\\
 c&d\end{bmatrix}=ad-bc.   The determinant of 3-by-3 matrices involves 6 terms ( rule of Sarrus ). The more lengthy Leibniz formula generalises these two formulae to all dimensions. 7  The determinant of a product of square matrices equals the product of their determinants:    det( AB ) = det( A ) ¬∑ det( B ). 8    Adding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by ‚àí1. 9 Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors , i.e., determinants of smaller matrices. 10 This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule , where the division of the determinants of two related square matrices equates to the value of each of the system's variables. 11  Eigenvalues and eigenvectors  A number Œª and a non-zero vector v satisfying   Av = Œª v    are called an eigenvalue and an eigenvector of A , respectively. 12 13 The number Œª is an eigenvalue of an n √ó n -matrix A if and only if A ‚àíŒª I n is not invertible, which is equivalent to       det   (   ùñ†  -   Œª  ùñ®    )    =  0.          ùñ†    Œª  ùñ®     0.    \det(\mathsf{A}-\lambda\mathsf{I})=0.    14 The polynomial p A in an indeterminate  X given by evaluation the determinant det( X I n ‚àí A ) is called the characteristic polynomial of A . It is a monic polynomial of degree  n . Therefore the polynomial equation p A (Œª)¬†=¬†0 has at most n different solutions, i.e., eigenvalues of the matrix. 15 They may be complex even if the entries of A are real. According to the Cayley‚ÄìHamilton theorem , p A ( A ) = 0 , that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix .  Notes  "       ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  Eigen means "own" in German and in Dutch . ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©     