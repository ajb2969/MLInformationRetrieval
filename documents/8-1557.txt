   Bayesian multivariate linear regression      Bayesian multivariate linear regression   In statistics , Bayesian multivariate linear regression is a Bayesian approach to multivariate linear regression , i.e. linear regression where the predicted outcome is a vector of correlated random variables rather than a single scalar random variable. A more general treatment of this approach can be found in the article MMSE estimator .  Details  Consider a regression problem where the dependent variable to be predicted is not a single real-valued scalar but an m -length vector of correlated real numbers. As in the standard regression setup, there are n observations, where each observation i consists of k -1 explanatory variables , grouped into a vector    ğ±  i     subscript  ğ±  i    \mathbf{x}_{i}   of length k (where a dummy variable with a value of 1 has been added to allow for an intercept coefficient). This can be viewed as a set of m related regression problems for each observation i :       y   i  ,  1    =     ğ±  i  T    ğœ·  1    +   Ïµ   i  ,  1          subscript  y   i  1         superscript   subscript  ğ±  i   normal-T    subscript  ğœ·  1     subscript  Ïµ   i  1       y_{i,1}=\mathbf{x}_{i}^{\rm T}\boldsymbol{\beta}_{1}+\epsilon_{i,1}        â‹¯   normal-â‹¯   \cdots          y   i  ,  m    =     ğ±  i  T    ğœ·  m    +   Ïµ   i  ,  m          subscript  y   i  m         superscript   subscript  ğ±  i   normal-T    subscript  ğœ·  m     subscript  Ïµ   i  m       y_{i,m}=\mathbf{x}_{i}^{\rm T}\boldsymbol{\beta}_{m}+\epsilon_{i,m}     where the set of errors    {   Ïµ   i  ,  1    ,  â€¦  ,   Ïµ   i  ,  m    }      subscript  Ïµ   i  1    normal-â€¦   subscript  Ïµ   i  m      \{\epsilon_{i,1},\ldots,\epsilon_{i,m}\}   are all correlated. Equivalently, it can be viewed as a single regression problem where the outcome is a row vector     ğ²  i  T     superscript   subscript  ğ²  i   normal-T    \mathbf{y}_{i}^{\rm T}   and the regression coefficient vectors are stacked next to each other, as follows:        ğ²  i  T   =     ğ±  i  T   ğ   +   Ïµ  i  T     .       superscript   subscript  ğ²  i   normal-T        superscript   subscript  ğ±  i   normal-T   ğ    superscript   subscript  bold-italic-Ïµ  i   normal-T      \mathbf{y}_{i}^{\rm T}=\mathbf{x}_{i}^{\rm T}\mathbf{B}+\boldsymbol{\epsilon}_%
 {i}^{\rm T}.     The coefficient matrix B is a    k  Ã—  m      k  m    k\times m   matrix where the coefficient vectors     ğœ·  1   ,  â€¦  ,   ğœ·  m       subscript  ğœ·  1   normal-â€¦   subscript  ğœ·  m     \boldsymbol{\beta}_{1},\ldots,\boldsymbol{\beta}_{m}   for each regression problem are stacked horizontally:       ğ  =   [       (           ğœ·  1           )   â‹¯   (           ğœ·  m           )       ]   =   [       (      Î²   1  ,  1        â‹®       Î²   k  ,  1       )   â‹¯   (      Î²   1  ,  m        â‹®       Î²   k  ,  m       )       ]    .        ğ        absent     subscript  ğœ·  1     absent    normal-â‹¯    absent     subscript  ğœ·  m     absent                   subscript  Î²   1  1      normal-â‹®     subscript  Î²   k  1      normal-â‹¯     subscript  Î²   1  m      normal-â‹®     subscript  Î²   k  m            \mathbf{B}=\begin{bmatrix}\begin{pmatrix}\\
 \boldsymbol{\beta}_{1}\\
 \\
 \end{pmatrix}\cdots\begin{pmatrix}\\
 \boldsymbol{\beta}_{m}\\
 \\
 \end{pmatrix}\end{bmatrix}=\begin{bmatrix}\begin{pmatrix}\beta_{1,1}\\
 \vdots\\
 \beta_{k,1}\\
 \end{pmatrix}\cdots\begin{pmatrix}\beta_{1,m}\\
 \vdots\\
 \beta_{k,m}\\
 \end{pmatrix}\end{bmatrix}.     The noise vector    Ïµ  i     subscript  bold-italic-Ïµ  i    \boldsymbol{\epsilon}_{i}   for each observation i is jointly normal, so that the outcomes for a given observation are correlated:        Ïµ  i   âˆ¼   N   (  0  ,   ğšº  Ïµ  2   )     .     similar-to   subscript  bold-italic-Ïµ  i     N   0   superscript   subscript  ğšº  Ïµ   2       \boldsymbol{\epsilon}_{i}\sim N(0,\boldsymbol{\Sigma}_{\epsilon}^{2}).     We can write the entire regression problem in matrix form as:       ğ˜  =   ğ—ğ  +  ğ„    ,      ğ˜    ğ—ğ  ğ„     \mathbf{Y}=\mathbf{X}\mathbf{B}+\mathbf{E},     where Y and E are    n  Ã—  m      n  m    n\times m   matrices. The design matrix  X is an    n  Ã—  k      n  k    n\times k   matrix with the observations stacked vertically, as in the standard linear regression setup:       ğ—  =   [      ğ±  1  T        ğ±  2  T       â‹®       ğ±  n  T      ]   =   [      x   1  ,  1      â‹¯     x   1  ,  k         x   2  ,  1      â‹¯     x   2  ,  k        â‹®    â‹±    â‹®       x   n  ,  1      â‹¯     x   n  ,  k       ]    .        ğ—     subscript   superscript  ğ±  normal-T   1      subscript   superscript  ğ±  normal-T   2     normal-â‹®     subscript   superscript  ğ±  normal-T   n             subscript  x   1  1    normal-â‹¯   subscript  x   1  k       subscript  x   2  1    normal-â‹¯   subscript  x   2  k      normal-â‹®  normal-â‹±  normal-â‹®     subscript  x   n  1    normal-â‹¯   subscript  x   n  k         \mathbf{X}=\begin{bmatrix}\mathbf{x}^{\rm T}_{1}\\
 \mathbf{x}^{\rm T}_{2}\\
 \vdots\\
 \mathbf{x}^{\rm T}_{n}\end{bmatrix}=\begin{bmatrix}x_{1,1}&\cdots&x_{1,k}\\
 x_{2,1}&\cdots&x_{2,k}\\
 \vdots&\ddots&\vdots\\
 x_{n,1}&\cdots&x_{n,k}\end{bmatrix}.     The classical, frequentists linear least squares solution is to simply estimate the matrix of regression coefficients    ğ  ^     normal-^  ğ    \hat{\mathbf{B}}   using the Moore-Penrose  pseudoinverse :       ğ  ^   =     (    ğ—  T   ğ—   )    -  1     ğ—  T   ğ˜        normal-^  ğ      superscript     superscript  ğ—  normal-T   ğ—     1     superscript  ğ—  normal-T   ğ˜     \hat{\mathbf{B}}=(\mathbf{X}^{\rm T}\mathbf{X})^{-1}\mathbf{X}^{\rm T}\mathbf{Y}   .  To obtain the Bayesian solution, we need to specify the conditional likelihood and then find the appropriate conjugate prior. As with the univariate case of linear Bayesian regression , we will find that we can specify a natural conditional conjugate prior (which is scale dependent).  Let us write our conditional likelihood as 1      Ï   (  ğ„  |   ğšº  Ïµ   )   âˆ    (   ğšº  Ïµ  2   )    -   n  /  2     exp   (  -   1  2   tr   (   ğ„  T   ğ„   ğšº  Ïµ   -  1    )   )   ,     fragments  Ï   fragments  normal-(  E  normal-|   subscript  ğšº  Ïµ   normal-)   proportional-to   superscript   fragments  normal-(   superscript   subscript  ğšº  Ïµ   2   normal-)       n  2       fragments  normal-(     1  2   tr   fragments  normal-(   superscript  ğ„  normal-T   E   superscript   subscript  ğšº  Ïµ     1    normal-)   normal-)   normal-,    \rho(\mathbf{E}|\boldsymbol{\Sigma}_{\epsilon})\propto(\boldsymbol{\Sigma}_{%
 \epsilon}^{2})^{-n/2}\exp(-\frac{1}{2}{\rm tr}(\mathbf{E}^{\rm T}\mathbf{E}%
 \boldsymbol{\Sigma}_{\epsilon}^{-1})),     writing the error   ğ„   ğ„   \mathbf{E}   in terms of     ğ˜  ,  ğ—   ,     ğ˜  ğ—    \mathbf{Y},\mathbf{X},   and   ğ   ğ   \mathbf{B}   yields      Ï   (  ğ˜  |  ğ—  ,  ğ  ,   ğšº  Ïµ   )   âˆ    (   ğšº  Ïµ  2   )    -   n  /  2     exp   (  -   1  2   tr   (    (  ğ˜  -  ğ—ğ  )   T    (  ğ˜  -  ğ—ğ  )    ğšº  Ïµ   -  1    )   )   ,     fragments  Ï   fragments  normal-(  Y  normal-|  X  normal-,  B  normal-,   subscript  ğšº  Ïµ   normal-)   proportional-to   superscript   fragments  normal-(   superscript   subscript  ğšº  Ïµ   2   normal-)       n  2       fragments  normal-(     1  2   tr   fragments  normal-(   superscript   fragments  normal-(  Y   XB  normal-)   normal-T    fragments  normal-(  Y   XB  normal-)    superscript   subscript  ğšº  Ïµ     1    normal-)   normal-)   normal-,    \rho(\mathbf{Y}|\mathbf{X},\mathbf{B},\boldsymbol{\Sigma}_{\epsilon})\propto(%
 \boldsymbol{\Sigma}_{\epsilon}^{2})^{-n/2}\exp(-\frac{1}{2}{\rm tr}((\mathbf{Y%
 }-\mathbf{X}\mathbf{\mathbf{B}})^{\rm T}(\mathbf{Y}-\mathbf{X}\mathbf{\mathbf{%
 B}})\boldsymbol{\Sigma}_{\epsilon}^{-1})),     We seek a natural conjugate priorâ€”a joint density    Ï   (  ğ  ,   Î£  Ïµ   )       Ï   ğ   subscript  normal-Î£  Ïµ      \rho(\mathbf{B},\Sigma_{\epsilon})   which is of the same functional form as the likelihood. Since the likelihood is quadratic in   ğ   ğ   \mathbf{B}   , we re-write the likelihood so it is normal in    (   ğ  -   ğ  ^    )      ğ   normal-^  ğ     (\mathbf{B}-\hat{\mathbf{B}})   (the deviation from classical sample estimate).  Using the same technique as with Bayesian linear regression , we decompose the exponential term using a matrix-form of the sum-of-squares technique. Here, however, we will also need to use the Matrix Differential Calculus ( Kronecker product and vectorization transformations).  First, let us apply sum-of-squares to obtain new expression for the likelihood:      Ï   (  ğ˜  |  ğ—  ,  ğ  ,   ğšº  Ïµ   )   âˆ   ğšº  Ïµ   -    (   n  -  k   )   /  2     exp   (  -  tr   (   1  2    ğ’  T   ğ’   ğšº  Ïµ   -  1    )   )     (   ğšº  Ïµ  2   )    -   k  /  2     exp   (  -   1  2   tr   (    (  ğ  -   ğ  ^   )   T    ğ—  T   ğ—   (  ğ  -   ğ  ^   )    ğšº  Ïµ   -  1    )   )   ,     fragments  Ï   fragments  normal-(  Y  normal-|  X  normal-,  B  normal-,   subscript  ğšº  Ïµ   normal-)   proportional-to   superscript   subscript  ğšº  Ïµ         n  k   2       fragments  normal-(   tr   fragments  normal-(    1  2    superscript  ğ’  normal-T   S   superscript   subscript  ğšº  Ïµ     1    normal-)   normal-)    superscript   fragments  normal-(   superscript   subscript  ğšº  Ïµ   2   normal-)       k  2       fragments  normal-(     1  2   tr   fragments  normal-(   superscript   fragments  normal-(  B    normal-^  ğ   normal-)   normal-T    superscript  ğ—  normal-T   X   fragments  normal-(  B    normal-^  ğ   normal-)    superscript   subscript  ğšº  Ïµ     1    normal-)   normal-)   normal-,    \rho(\mathbf{Y}|\mathbf{X},\mathbf{B},\boldsymbol{\Sigma}_{\epsilon})\propto%
 \boldsymbol{\Sigma}_{\epsilon}^{-(n-k)/2}\exp(-{\rm tr}(\frac{1}{2}\mathbf{S}^%
 {\rm T}\mathbf{S}\boldsymbol{\Sigma}_{\epsilon}^{-1}))(\boldsymbol{\Sigma}_{%
 \epsilon}^{2})^{-k/2}\exp(-\frac{1}{2}{\rm tr}((\mathbf{B}-\hat{\mathbf{B}})^{%
 \rm T}\mathbf{X}^{\rm T}\mathbf{X}(\mathbf{B}-\hat{\mathbf{B}})\boldsymbol{%
 \Sigma}_{\epsilon}^{-1})),         ğ’  =   ğ˜  -    ğ  ^   ğ—        ğ’    ğ˜     normal-^  ğ   ğ—      \mathbf{S}=\mathbf{Y}-\hat{\mathbf{B}}\mathbf{X}     We would like to develop a conditional form for the priors:      Ï   (  ğ  ,   ğšº  Ïµ   )   =  Ï   (   ğšº  Ïµ   )   Ï   (  ğ  |   ğšº  Ïµ   )   ,     fragments  Ï   fragments  normal-(  B  normal-,   subscript  ğšº  Ïµ   normal-)    Ï   fragments  normal-(   subscript  ğšº  Ïµ   normal-)   Ï   fragments  normal-(  B  normal-|   subscript  ğšº  Ïµ   normal-)   normal-,    \rho(\mathbf{B},\boldsymbol{\Sigma}_{\epsilon})=\rho(\boldsymbol{\Sigma}_{%
 \epsilon})\rho(\mathbf{B}|\boldsymbol{\Sigma}_{\epsilon}),     where    Ï   (   ğšº  Ïµ   )       Ï   subscript  ğšº  Ïµ     \rho(\boldsymbol{\Sigma}_{\epsilon})   is an inverse-Wishart distribution and    Ï   (  ğ  |   ğšº  Ïµ   )      fragments  Ï   fragments  normal-(  B  normal-|   subscript  ğšº  Ïµ   normal-)     \rho(\mathbf{B}|\boldsymbol{\Sigma}_{\epsilon})   is some form of normal distribution in the matrix   ğ   ğ   \mathbf{B}   . This is accomplished using the vectorization transformation, which converts the likelihood from a function of the matrices    ğ  ,   ğ  ^      ğ   normal-^  ğ     \mathbf{B},\hat{\mathbf{B}}   to a function of the vectors     ğœ·  =   vec   (  ğ  )     ,    ğœ·  ^   =   vec   (   ğ  ^   )        formulae-sequence    ğœ·    vec  ğ       normal-^  ğœ·     vec   normal-^  ğ       \boldsymbol{\beta}={\rm vec}(\mathbf{B}),\hat{\boldsymbol{\beta}}={\rm vec}(%
 \hat{\mathbf{B}})   .  Write       tr   (     (   ğ  -   ğ  ^    )   T    ğ—  T   ğ—   (   ğ  -   ğ  ^    )    ğšº  Ïµ   -  1     )    =   vec    (   ğ  -   ğ  ^    )   T   vec   (    ğ—  T   ğ—   (   ğ  -   ğ  ^    )    ğšº  Ïµ   -  1     )          tr     superscript    ğ   normal-^  ğ    normal-T    superscript  ğ—  normal-T   ğ—    ğ   normal-^  ğ     superscript   subscript  ğšº  Ïµ     1        vec   superscript    ğ   normal-^  ğ    normal-T   vec     superscript  ğ—  normal-T   ğ—    ğ   normal-^  ğ     superscript   subscript  ğšº  Ïµ     1        {\rm tr}((\mathbf{B}-\hat{\mathbf{B}})^{\rm T}\mathbf{X}^{\rm T}\mathbf{X}(%
 \mathbf{B}-\hat{\mathbf{B}})\boldsymbol{\Sigma}_{\epsilon}^{-1})={\rm vec}(%
 \mathbf{B}-\hat{\mathbf{B}})^{\rm T}{\rm vec}(\mathbf{X}^{\rm T}\mathbf{X}(%
 \mathbf{B}-\hat{\mathbf{B}})\boldsymbol{\Sigma}_{\epsilon}^{-1})     Let        vec   (    ğ—  T   ğ—   (   ğ  -   ğ  ^    )    ğšº  Ïµ   -  1     )    =    (     ğšº  Ïµ   -  1    âŠ—   ğ—  T    ğ—   )   vec   (   ğ  -   ğ  ^    )     ,        vec     superscript  ğ—  normal-T   ğ—    ğ   normal-^  ğ     superscript   subscript  ğšº  Ïµ     1           tensor-product   superscript   subscript  ğšº  Ïµ     1     superscript  ğ—  normal-T    ğ—   vec    ğ   normal-^  ğ       {\rm vec}(\mathbf{X}^{\rm T}\mathbf{X}(\mathbf{B}-\hat{\mathbf{B}})\boldsymbol%
 {\Sigma}_{\epsilon}^{-1})=(\boldsymbol{\Sigma}_{\epsilon}^{-1}\otimes\mathbf{X%
 }^{\rm T}\mathbf{X}){\rm vec}(\mathbf{B}-\hat{\mathbf{B}}),   where    ğ€  âŠ—  ğ     tensor-product  ğ€  ğ    \mathbf{A}\otimes\mathbf{B}   denotes the Kronecker product of matrices A and B , a generalization of the outer product which multiplies an    m  Ã—  n      m  n    m\times n   matrix by a    p  Ã—  q      p  q    p\times q   matrix to generate an      m  p   Ã—  n   q          m  p   n   q    mp\times nq   matrix, consisting of every combination of products of elements from the two matrices.  Then      vec    (   ğ  -   ğ  ^    )   T    (     ğšº  Ïµ   -  1    âŠ—   ğ—  T    ğ—   )   vec   (   ğ  -   ğ  ^    )       vec   superscript    ğ   normal-^  ğ    normal-T      tensor-product   superscript   subscript  ğšº  Ïµ     1     superscript  ğ—  normal-T    ğ—   vec    ğ   normal-^  ğ      {\rm vec}(\mathbf{B}-\hat{\mathbf{B}})^{\rm T}(\boldsymbol{\Sigma}_{\epsilon}^%
 {-1}\otimes\mathbf{X}^{\rm T}\mathbf{X}){\rm vec}(\mathbf{B}-\hat{\mathbf{B}})            =     (   ğœ·  -   ğœ·  ^    )   T    (     ğšº  Ïµ   -  1    âŠ—   ğ—  T    ğ—   )    (   ğœ·  -   ğœ·  ^    )        absent     superscript    ğœ·   normal-^  ğœ·    normal-T      tensor-product   superscript   subscript  ğšº  Ïµ     1     superscript  ğ—  normal-T    ğ—     ğœ·   normal-^  ğœ·       =(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^{\rm T}(\boldsymbol{\Sigma}_{%
 \epsilon}^{-1}\otimes\mathbf{X}^{\rm T}\mathbf{X})(\boldsymbol{\beta}-\hat{%
 \boldsymbol{\beta}})        which will lead to a likelihood which is normal in    (   ğœ·  -   ğœ·  ^    )      ğœ·   normal-^  ğœ·     (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})   .  With the likelihood in a more tractable form, we can now find a natural (conditional) conjugate prior.  See also   Bayesian linear regression  Matrix normal distribution   References   Peter E. Rossi, Greg M. Allenby, and Robert McCulloch, Bayesian Statistics and Marketing , John Wiley & Sons, Ltd, 2006   "  Multivariate linear regression  Category:Regression analysis     Peter E. Rossi, Greg M. Allenby, Rob McCulloch. Bayesian Statistics and Marketing . John Wiley & Sons, 2012, p. 32. â†©     