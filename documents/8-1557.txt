   Bayesian multivariate linear regression      Bayesian multivariate linear regression   In statistics , Bayesian multivariate linear regression is a Bayesian approach to multivariate linear regression , i.e. linear regression where the predicted outcome is a vector of correlated random variables rather than a single scalar random variable. A more general treatment of this approach can be found in the article MMSE estimator .  Details  Consider a regression problem where the dependent variable to be predicted is not a single real-valued scalar but an m -length vector of correlated real numbers. As in the standard regression setup, there are n observations, where each observation i consists of k -1 explanatory variables , grouped into a vector    𝐱  i     subscript  𝐱  i    \mathbf{x}_{i}   of length k (where a dummy variable with a value of 1 has been added to allow for an intercept coefficient). This can be viewed as a set of m related regression problems for each observation i :       y   i  ,  1    =     𝐱  i  T    𝜷  1    +   ϵ   i  ,  1          subscript  y   i  1         superscript   subscript  𝐱  i   normal-T    subscript  𝜷  1     subscript  ϵ   i  1       y_{i,1}=\mathbf{x}_{i}^{\rm T}\boldsymbol{\beta}_{1}+\epsilon_{i,1}        ⋯   normal-⋯   \cdots          y   i  ,  m    =     𝐱  i  T    𝜷  m    +   ϵ   i  ,  m          subscript  y   i  m         superscript   subscript  𝐱  i   normal-T    subscript  𝜷  m     subscript  ϵ   i  m       y_{i,m}=\mathbf{x}_{i}^{\rm T}\boldsymbol{\beta}_{m}+\epsilon_{i,m}     where the set of errors    {   ϵ   i  ,  1    ,  …  ,   ϵ   i  ,  m    }      subscript  ϵ   i  1    normal-…   subscript  ϵ   i  m      \{\epsilon_{i,1},\ldots,\epsilon_{i,m}\}   are all correlated. Equivalently, it can be viewed as a single regression problem where the outcome is a row vector     𝐲  i  T     superscript   subscript  𝐲  i   normal-T    \mathbf{y}_{i}^{\rm T}   and the regression coefficient vectors are stacked next to each other, as follows:        𝐲  i  T   =     𝐱  i  T   𝐁   +   ϵ  i  T     .       superscript   subscript  𝐲  i   normal-T        superscript   subscript  𝐱  i   normal-T   𝐁    superscript   subscript  bold-italic-ϵ  i   normal-T      \mathbf{y}_{i}^{\rm T}=\mathbf{x}_{i}^{\rm T}\mathbf{B}+\boldsymbol{\epsilon}_%
 {i}^{\rm T}.     The coefficient matrix B is a    k  ×  m      k  m    k\times m   matrix where the coefficient vectors     𝜷  1   ,  …  ,   𝜷  m       subscript  𝜷  1   normal-…   subscript  𝜷  m     \boldsymbol{\beta}_{1},\ldots,\boldsymbol{\beta}_{m}   for each regression problem are stacked horizontally:       𝐁  =   [       (           𝜷  1           )   ⋯   (           𝜷  m           )       ]   =   [       (      β   1  ,  1        ⋮       β   k  ,  1       )   ⋯   (      β   1  ,  m        ⋮       β   k  ,  m       )       ]    .        𝐁        absent     subscript  𝜷  1     absent    normal-⋯    absent     subscript  𝜷  m     absent                   subscript  β   1  1      normal-⋮     subscript  β   k  1      normal-⋯     subscript  β   1  m      normal-⋮     subscript  β   k  m            \mathbf{B}=\begin{bmatrix}\begin{pmatrix}\\
 \boldsymbol{\beta}_{1}\\
 \\
 \end{pmatrix}\cdots\begin{pmatrix}\\
 \boldsymbol{\beta}_{m}\\
 \\
 \end{pmatrix}\end{bmatrix}=\begin{bmatrix}\begin{pmatrix}\beta_{1,1}\\
 \vdots\\
 \beta_{k,1}\\
 \end{pmatrix}\cdots\begin{pmatrix}\beta_{1,m}\\
 \vdots\\
 \beta_{k,m}\\
 \end{pmatrix}\end{bmatrix}.     The noise vector    ϵ  i     subscript  bold-italic-ϵ  i    \boldsymbol{\epsilon}_{i}   for each observation i is jointly normal, so that the outcomes for a given observation are correlated:        ϵ  i   ∼   N   (  0  ,   𝚺  ϵ  2   )     .     similar-to   subscript  bold-italic-ϵ  i     N   0   superscript   subscript  𝚺  ϵ   2       \boldsymbol{\epsilon}_{i}\sim N(0,\boldsymbol{\Sigma}_{\epsilon}^{2}).     We can write the entire regression problem in matrix form as:       𝐘  =   𝐗𝐁  +  𝐄    ,      𝐘    𝐗𝐁  𝐄     \mathbf{Y}=\mathbf{X}\mathbf{B}+\mathbf{E},     where Y and E are    n  ×  m      n  m    n\times m   matrices. The design matrix  X is an    n  ×  k      n  k    n\times k   matrix with the observations stacked vertically, as in the standard linear regression setup:       𝐗  =   [      𝐱  1  T        𝐱  2  T       ⋮       𝐱  n  T      ]   =   [      x   1  ,  1      ⋯     x   1  ,  k         x   2  ,  1      ⋯     x   2  ,  k        ⋮    ⋱    ⋮       x   n  ,  1      ⋯     x   n  ,  k       ]    .        𝐗     subscript   superscript  𝐱  normal-T   1      subscript   superscript  𝐱  normal-T   2     normal-⋮     subscript   superscript  𝐱  normal-T   n             subscript  x   1  1    normal-⋯   subscript  x   1  k       subscript  x   2  1    normal-⋯   subscript  x   2  k      normal-⋮  normal-⋱  normal-⋮     subscript  x   n  1    normal-⋯   subscript  x   n  k         \mathbf{X}=\begin{bmatrix}\mathbf{x}^{\rm T}_{1}\\
 \mathbf{x}^{\rm T}_{2}\\
 \vdots\\
 \mathbf{x}^{\rm T}_{n}\end{bmatrix}=\begin{bmatrix}x_{1,1}&\cdots&x_{1,k}\\
 x_{2,1}&\cdots&x_{2,k}\\
 \vdots&\ddots&\vdots\\
 x_{n,1}&\cdots&x_{n,k}\end{bmatrix}.     The classical, frequentists linear least squares solution is to simply estimate the matrix of regression coefficients    𝐁  ^     normal-^  𝐁    \hat{\mathbf{B}}   using the Moore-Penrose  pseudoinverse :       𝐁  ^   =     (    𝐗  T   𝐗   )    -  1     𝐗  T   𝐘        normal-^  𝐁      superscript     superscript  𝐗  normal-T   𝐗     1     superscript  𝐗  normal-T   𝐘     \hat{\mathbf{B}}=(\mathbf{X}^{\rm T}\mathbf{X})^{-1}\mathbf{X}^{\rm T}\mathbf{Y}   .  To obtain the Bayesian solution, we need to specify the conditional likelihood and then find the appropriate conjugate prior. As with the univariate case of linear Bayesian regression , we will find that we can specify a natural conditional conjugate prior (which is scale dependent).  Let us write our conditional likelihood as 1      ρ   (  𝐄  |   𝚺  ϵ   )   ∝    (   𝚺  ϵ  2   )    -   n  /  2     exp   (  -   1  2   tr   (   𝐄  T   𝐄   𝚺  ϵ   -  1    )   )   ,     fragments  ρ   fragments  normal-(  E  normal-|   subscript  𝚺  ϵ   normal-)   proportional-to   superscript   fragments  normal-(   superscript   subscript  𝚺  ϵ   2   normal-)       n  2       fragments  normal-(     1  2   tr   fragments  normal-(   superscript  𝐄  normal-T   E   superscript   subscript  𝚺  ϵ     1    normal-)   normal-)   normal-,    \rho(\mathbf{E}|\boldsymbol{\Sigma}_{\epsilon})\propto(\boldsymbol{\Sigma}_{%
 \epsilon}^{2})^{-n/2}\exp(-\frac{1}{2}{\rm tr}(\mathbf{E}^{\rm T}\mathbf{E}%
 \boldsymbol{\Sigma}_{\epsilon}^{-1})),     writing the error   𝐄   𝐄   \mathbf{E}   in terms of     𝐘  ,  𝐗   ,     𝐘  𝐗    \mathbf{Y},\mathbf{X},   and   𝐁   𝐁   \mathbf{B}   yields      ρ   (  𝐘  |  𝐗  ,  𝐁  ,   𝚺  ϵ   )   ∝    (   𝚺  ϵ  2   )    -   n  /  2     exp   (  -   1  2   tr   (    (  𝐘  -  𝐗𝐁  )   T    (  𝐘  -  𝐗𝐁  )    𝚺  ϵ   -  1    )   )   ,     fragments  ρ   fragments  normal-(  Y  normal-|  X  normal-,  B  normal-,   subscript  𝚺  ϵ   normal-)   proportional-to   superscript   fragments  normal-(   superscript   subscript  𝚺  ϵ   2   normal-)       n  2       fragments  normal-(     1  2   tr   fragments  normal-(   superscript   fragments  normal-(  Y   XB  normal-)   normal-T    fragments  normal-(  Y   XB  normal-)    superscript   subscript  𝚺  ϵ     1    normal-)   normal-)   normal-,    \rho(\mathbf{Y}|\mathbf{X},\mathbf{B},\boldsymbol{\Sigma}_{\epsilon})\propto(%
 \boldsymbol{\Sigma}_{\epsilon}^{2})^{-n/2}\exp(-\frac{1}{2}{\rm tr}((\mathbf{Y%
 }-\mathbf{X}\mathbf{\mathbf{B}})^{\rm T}(\mathbf{Y}-\mathbf{X}\mathbf{\mathbf{%
 B}})\boldsymbol{\Sigma}_{\epsilon}^{-1})),     We seek a natural conjugate prior—a joint density    ρ   (  𝐁  ,   Σ  ϵ   )       ρ   𝐁   subscript  normal-Σ  ϵ      \rho(\mathbf{B},\Sigma_{\epsilon})   which is of the same functional form as the likelihood. Since the likelihood is quadratic in   𝐁   𝐁   \mathbf{B}   , we re-write the likelihood so it is normal in    (   𝐁  -   𝐁  ^    )      𝐁   normal-^  𝐁     (\mathbf{B}-\hat{\mathbf{B}})   (the deviation from classical sample estimate).  Using the same technique as with Bayesian linear regression , we decompose the exponential term using a matrix-form of the sum-of-squares technique. Here, however, we will also need to use the Matrix Differential Calculus ( Kronecker product and vectorization transformations).  First, let us apply sum-of-squares to obtain new expression for the likelihood:      ρ   (  𝐘  |  𝐗  ,  𝐁  ,   𝚺  ϵ   )   ∝   𝚺  ϵ   -    (   n  -  k   )   /  2     exp   (  -  tr   (   1  2    𝐒  T   𝐒   𝚺  ϵ   -  1    )   )     (   𝚺  ϵ  2   )    -   k  /  2     exp   (  -   1  2   tr   (    (  𝐁  -   𝐁  ^   )   T    𝐗  T   𝐗   (  𝐁  -   𝐁  ^   )    𝚺  ϵ   -  1    )   )   ,     fragments  ρ   fragments  normal-(  Y  normal-|  X  normal-,  B  normal-,   subscript  𝚺  ϵ   normal-)   proportional-to   superscript   subscript  𝚺  ϵ         n  k   2       fragments  normal-(   tr   fragments  normal-(    1  2    superscript  𝐒  normal-T   S   superscript   subscript  𝚺  ϵ     1    normal-)   normal-)    superscript   fragments  normal-(   superscript   subscript  𝚺  ϵ   2   normal-)       k  2       fragments  normal-(     1  2   tr   fragments  normal-(   superscript   fragments  normal-(  B    normal-^  𝐁   normal-)   normal-T    superscript  𝐗  normal-T   X   fragments  normal-(  B    normal-^  𝐁   normal-)    superscript   subscript  𝚺  ϵ     1    normal-)   normal-)   normal-,    \rho(\mathbf{Y}|\mathbf{X},\mathbf{B},\boldsymbol{\Sigma}_{\epsilon})\propto%
 \boldsymbol{\Sigma}_{\epsilon}^{-(n-k)/2}\exp(-{\rm tr}(\frac{1}{2}\mathbf{S}^%
 {\rm T}\mathbf{S}\boldsymbol{\Sigma}_{\epsilon}^{-1}))(\boldsymbol{\Sigma}_{%
 \epsilon}^{2})^{-k/2}\exp(-\frac{1}{2}{\rm tr}((\mathbf{B}-\hat{\mathbf{B}})^{%
 \rm T}\mathbf{X}^{\rm T}\mathbf{X}(\mathbf{B}-\hat{\mathbf{B}})\boldsymbol{%
 \Sigma}_{\epsilon}^{-1})),         𝐒  =   𝐘  -    𝐁  ^   𝐗        𝐒    𝐘     normal-^  𝐁   𝐗      \mathbf{S}=\mathbf{Y}-\hat{\mathbf{B}}\mathbf{X}     We would like to develop a conditional form for the priors:      ρ   (  𝐁  ,   𝚺  ϵ   )   =  ρ   (   𝚺  ϵ   )   ρ   (  𝐁  |   𝚺  ϵ   )   ,     fragments  ρ   fragments  normal-(  B  normal-,   subscript  𝚺  ϵ   normal-)    ρ   fragments  normal-(   subscript  𝚺  ϵ   normal-)   ρ   fragments  normal-(  B  normal-|   subscript  𝚺  ϵ   normal-)   normal-,    \rho(\mathbf{B},\boldsymbol{\Sigma}_{\epsilon})=\rho(\boldsymbol{\Sigma}_{%
 \epsilon})\rho(\mathbf{B}|\boldsymbol{\Sigma}_{\epsilon}),     where    ρ   (   𝚺  ϵ   )       ρ   subscript  𝚺  ϵ     \rho(\boldsymbol{\Sigma}_{\epsilon})   is an inverse-Wishart distribution and    ρ   (  𝐁  |   𝚺  ϵ   )      fragments  ρ   fragments  normal-(  B  normal-|   subscript  𝚺  ϵ   normal-)     \rho(\mathbf{B}|\boldsymbol{\Sigma}_{\epsilon})   is some form of normal distribution in the matrix   𝐁   𝐁   \mathbf{B}   . This is accomplished using the vectorization transformation, which converts the likelihood from a function of the matrices    𝐁  ,   𝐁  ^      𝐁   normal-^  𝐁     \mathbf{B},\hat{\mathbf{B}}   to a function of the vectors     𝜷  =   vec   (  𝐁  )     ,    𝜷  ^   =   vec   (   𝐁  ^   )        formulae-sequence    𝜷    vec  𝐁       normal-^  𝜷     vec   normal-^  𝐁       \boldsymbol{\beta}={\rm vec}(\mathbf{B}),\hat{\boldsymbol{\beta}}={\rm vec}(%
 \hat{\mathbf{B}})   .  Write       tr   (     (   𝐁  -   𝐁  ^    )   T    𝐗  T   𝐗   (   𝐁  -   𝐁  ^    )    𝚺  ϵ   -  1     )    =   vec    (   𝐁  -   𝐁  ^    )   T   vec   (    𝐗  T   𝐗   (   𝐁  -   𝐁  ^    )    𝚺  ϵ   -  1     )          tr     superscript    𝐁   normal-^  𝐁    normal-T    superscript  𝐗  normal-T   𝐗    𝐁   normal-^  𝐁     superscript   subscript  𝚺  ϵ     1        vec   superscript    𝐁   normal-^  𝐁    normal-T   vec     superscript  𝐗  normal-T   𝐗    𝐁   normal-^  𝐁     superscript   subscript  𝚺  ϵ     1        {\rm tr}((\mathbf{B}-\hat{\mathbf{B}})^{\rm T}\mathbf{X}^{\rm T}\mathbf{X}(%
 \mathbf{B}-\hat{\mathbf{B}})\boldsymbol{\Sigma}_{\epsilon}^{-1})={\rm vec}(%
 \mathbf{B}-\hat{\mathbf{B}})^{\rm T}{\rm vec}(\mathbf{X}^{\rm T}\mathbf{X}(%
 \mathbf{B}-\hat{\mathbf{B}})\boldsymbol{\Sigma}_{\epsilon}^{-1})     Let        vec   (    𝐗  T   𝐗   (   𝐁  -   𝐁  ^    )    𝚺  ϵ   -  1     )    =    (     𝚺  ϵ   -  1    ⊗   𝐗  T    𝐗   )   vec   (   𝐁  -   𝐁  ^    )     ,        vec     superscript  𝐗  normal-T   𝐗    𝐁   normal-^  𝐁     superscript   subscript  𝚺  ϵ     1           tensor-product   superscript   subscript  𝚺  ϵ     1     superscript  𝐗  normal-T    𝐗   vec    𝐁   normal-^  𝐁       {\rm vec}(\mathbf{X}^{\rm T}\mathbf{X}(\mathbf{B}-\hat{\mathbf{B}})\boldsymbol%
 {\Sigma}_{\epsilon}^{-1})=(\boldsymbol{\Sigma}_{\epsilon}^{-1}\otimes\mathbf{X%
 }^{\rm T}\mathbf{X}){\rm vec}(\mathbf{B}-\hat{\mathbf{B}}),   where    𝐀  ⊗  𝐁     tensor-product  𝐀  𝐁    \mathbf{A}\otimes\mathbf{B}   denotes the Kronecker product of matrices A and B , a generalization of the outer product which multiplies an    m  ×  n      m  n    m\times n   matrix by a    p  ×  q      p  q    p\times q   matrix to generate an      m  p   ×  n   q          m  p   n   q    mp\times nq   matrix, consisting of every combination of products of elements from the two matrices.  Then      vec    (   𝐁  -   𝐁  ^    )   T    (     𝚺  ϵ   -  1    ⊗   𝐗  T    𝐗   )   vec   (   𝐁  -   𝐁  ^    )       vec   superscript    𝐁   normal-^  𝐁    normal-T      tensor-product   superscript   subscript  𝚺  ϵ     1     superscript  𝐗  normal-T    𝐗   vec    𝐁   normal-^  𝐁      {\rm vec}(\mathbf{B}-\hat{\mathbf{B}})^{\rm T}(\boldsymbol{\Sigma}_{\epsilon}^%
 {-1}\otimes\mathbf{X}^{\rm T}\mathbf{X}){\rm vec}(\mathbf{B}-\hat{\mathbf{B}})            =     (   𝜷  -   𝜷  ^    )   T    (     𝚺  ϵ   -  1    ⊗   𝐗  T    𝐗   )    (   𝜷  -   𝜷  ^    )        absent     superscript    𝜷   normal-^  𝜷    normal-T      tensor-product   superscript   subscript  𝚺  ϵ     1     superscript  𝐗  normal-T    𝐗     𝜷   normal-^  𝜷       =(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})^{\rm T}(\boldsymbol{\Sigma}_{%
 \epsilon}^{-1}\otimes\mathbf{X}^{\rm T}\mathbf{X})(\boldsymbol{\beta}-\hat{%
 \boldsymbol{\beta}})        which will lead to a likelihood which is normal in    (   𝜷  -   𝜷  ^    )      𝜷   normal-^  𝜷     (\boldsymbol{\beta}-\hat{\boldsymbol{\beta}})   .  With the likelihood in a more tractable form, we can now find a natural (conditional) conjugate prior.  See also   Bayesian linear regression  Matrix normal distribution   References   Peter E. Rossi, Greg M. Allenby, and Robert McCulloch, Bayesian Statistics and Marketing , John Wiley & Sons, Ltd, 2006   "  Multivariate linear regression  Category:Regression analysis     Peter E. Rossi, Greg M. Allenby, Rob McCulloch. Bayesian Statistics and Marketing . John Wiley & Sons, 2012, p. 32. ↩     