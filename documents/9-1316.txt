   Entropy rate      Entropy rate   In the mathematical theory of probability , the entropy rate or source information rate of a stochastic process is, informally, the time density of the average information in a stochastic process. For stochastic processes with a countable index, the entropy rate H ( X ) is the limit of the joint entropy of n members of the process X k divided by n , as n  tends to  infinity :       H   (  X  )    =    lim   n  →  ∞      1  n   H   (   X  1   ,   X  2   ,   …   X  n    )           H  X     subscript    normal-→  n         1  n   H    subscript  X  1    subscript  X  2     normal-…   subscript  X  n         H(X)=\lim_{n\to\infty}\frac{1}{n}H(X_{1},X_{2},\dots X_{n})     when the limit exists. An alternative, related quantity is:       H  ′    (  X  )   =   lim   n  →  ∞    H   (   X  n   |   X   n  -  1    ,   X   n  -  2    ,  …   X  1   )      fragments   superscript  H  normal-′    fragments  normal-(  X  normal-)     subscript    normal-→  n     H   fragments  normal-(   subscript  X  n   normal-|   subscript  X    n  1    normal-,   subscript  X    n  2    normal-,  normal-…   subscript  X  1   normal-)     H^{\prime}(X)=\lim_{n\to\infty}H(X_{n}|X_{n-1},X_{n-2},\dots X_{1})     For strongly stationary stochastic processes,     H   (  X  )    =    H  ′    (  X  )          H  X      superscript  H  normal-′   X     H(X)=H^{\prime}(X)   . The entropy rate can be thought of as a general property of stochastic sources; this is the asymptotic equipartition property .  Entropy rates for Markov chains  Since a stochastic process defined by a Markov chain that is irreducible and aperiodic and persistent has a stationary distribution , the entropy rate is independent of the initial distribution.  For example, for such a Markov chain Y k defined on a countable number of states, given the transition matrix  P ij , H ( Y ) is given by:       H   (  Y  )    =   -    ∑   i  j      μ  i    P   i  j     log   P   i  j              H  Y       subscript     i  j       subscript  μ  i    subscript  P    i  j       subscript  P    i  j          \displaystyle H(Y)=-\sum_{ij}\mu_{i}P_{ij}\log P_{ij}     where μ i is the stationary distribution of the chain.  A simple consequence of this definition is that an i.i.d.  stochastic process has an entropy rate that is the same as the entropy of any individual member of the process.  See also   Information source (mathematics)  Markov information source  Asymptotic equipartition property   References   Cover, T. and Thomas, J. (1991) Elements of Information Theory, John Wiley and Sons, Inc., ISBN 0-471-06259-6 [ http://www3.interscience.wiley.com/cgi-bin/bookhome/110438582?CRETRY=1&SRETRY; ;=0]   External links   Systems Analysis, Modelling and Prediction (SAMP), University of Oxford  MATLAB code for estimating information-theoretic quantities for stochastic processes.   "  Category:Information theory  Category:Entropy  Category:Markov models  Category:Temporal rates   