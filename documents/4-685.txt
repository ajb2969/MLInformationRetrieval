   Concentration of measure      Concentration of measure   In mathematics , concentration of measure (about a median ) is a principle that is applied in measure theory , probability and combinatorics , and has consequences for other fields such as Banach space theory. Informally, it states that "A random variable that depends in a Lipschitz way on many independent variables (but not too much on any of them) is essentially constant". 1  The c.o.m. phenomenon was put forth in the early 1970s by Vitali Milman in his works on the local theory of Banach spaces , extending an idea going back to the work of Paul Lévy . 2 3 It was further developed in the works of Milman and Gromov , Maurey , Pisier , Shechtman , Talagrand , Ledoux, and others.  The general setting  Let    (  X  ,  d  ,  μ  )     X  d  μ    (X,d,\mu)   be a metric measure space,     μ   (  X  )    =  1        μ  X   1    \mu(X)=1   . Let        α   (  ϵ  )    =   sup   {   μ   (   X  ∖   A  ϵ    )    |    μ   (  A  )    ≥   1  /  2    }     ,        α  ϵ    supremum   conditional-set    μ    X   subscript  A  ϵ         μ  A     1  2        \alpha(\epsilon)=\sup\left\{\mu(X\setminus A_{\epsilon})\,|\,\mu(A)\geq 1/2%
 \right\},   where       A  ϵ   =   {   x   |    d   (  x  ,  A  )    <  ϵ   }        subscript  A  ϵ    conditional-set  x      d   x  A    ϵ      A_{\epsilon}=\left\{x\,|\,d(x,A)<\epsilon\right\}   is the   ϵ   ϵ   \epsilon   - extension of a set   A   A   A   .  The function    α   (  ⋅  )       α  normal-⋅    \alpha(\cdot)   is called the concentration rate of the space   X   X   X   . The following equivalent definition has many applications:      α   (  ϵ  )   =  sup   {  μ   (   {  F  ≥  𝑀  +  ϵ  }   )   }   ,     fragments  α   fragments  normal-(  ϵ  normal-)    supremum   fragments  normal-{  μ   fragments  normal-(   fragments  normal-{  F   M   ϵ  normal-}   normal-)   normal-}   normal-,    \alpha(\epsilon)=\sup\left\{\mu(\{F\geq\mathop{M}+\epsilon\})\right\},   where the supremum is over all 1-Lipschitz functions    F  :   X  →  ℝ      normal-:  F   normal-→  X  ℝ     F:X\to\mathbb{R}   , and the median (or Levy mean)    M  =    M  e  d   F       M     M  e  d   F     M=\mathop{Med}F   is defined by the inequalities      μ   {  F  ≥  M  }   ≥  1  /  2  ,  μ   {  F  ≤  M  }   ≥  1  /  2.     fragments  μ   fragments  normal-{  F   M  normal-}    1   2  normal-,  μ   fragments  normal-{  F   M  normal-}    1   2.    \mu\{F\geq M\}\geq 1/2,\,\mu\{F\leq M\}\geq 1/2.     Informally, the space   X   X   X   exhibits a concentration phenomenon if    α   (  ϵ  )       α  ϵ    \alpha(\epsilon)   decays very fast as   ϵ   ϵ   \epsilon   grows. More formally, a family of metric measure spaces    (   X  n   ,   d  n   ,   μ  n   )      subscript  X  n    subscript  d  n    subscript  μ  n     (X_{n},d_{n},\mu_{n})   is called a Lévy family if the corresponding concentration rates    α  n     subscript  α  n    \alpha_{n}   satisfy        ∀  ϵ   >    0    α  n    (  ϵ  )    →    0    as   n   →  ∞   ,         for-all  ϵ     0   subscript  α  n   ϵ     normal-→      0  as  n     normal-→        \forall\epsilon>0\,\,\alpha_{n}(\epsilon)\to 0{\rm\;as\;}n\to\infty,   and a normal Lévy family if       ∀  ϵ   >    0    α  n    (  ϵ  )    ≤   C   exp   (   -   c  n   ϵ  2     )            for-all  ϵ     0   subscript  α  n   ϵ          C        c  n   superscript  ϵ  2          \forall\epsilon>0\,\,\alpha_{n}(\epsilon)\leq C\exp(-cn\epsilon^{2})   for some constants     c  ,  C   >  0       c  C   0    c,C>0   . For examples see below.  Concentration on the sphere  The first example goes back to Paul Lévy . According to the spherical isoperimetric inequality , among all subsets   A   A   A   of the sphere    S  n     superscript  S  n    S^{n}   with prescribed spherical measure      σ  n    (  A  )        subscript  σ  n   A    \sigma_{n}(A)   , the spherical cap       {   x  ∈   S  n    |    dist   (  x  ,   x  0   )    ≤  R   }   ,     conditional-set    x   superscript  S  n        dist   x   subscript  x  0     R     \left\{x\in S^{n}|\mathrm{dist}(x,x_{0})\leq R\right\},   for suitable   R   R   R   , has the smallest   ϵ   ϵ   \epsilon   -extension    A  ϵ     subscript  A  ϵ    A_{\epsilon}   (for any    ϵ  >  0      ϵ  0    \epsilon>0   ).  Applying this to sets of measure      σ  n    (  A  )    =   1  /  2          subscript  σ  n   A     1  2     \sigma_{n}(A)=1/2   (where      σ  n    (   S  n   )    =  1         subscript  σ  n    superscript  S  n    1    \sigma_{n}(S^{n})=1   ), one can deduce the following concentration inequality :        σ  n    (   A  ϵ   )    ≥   1  -   C   exp   (   -   c  n   ϵ  2     )             subscript  σ  n    subscript  A  ϵ      1    C        c  n   superscript  ϵ  2          \sigma_{n}(A_{\epsilon})\geq 1-C\exp(-cn\epsilon^{2})   , where    C  ,  c     C  c    C,c   are universal constants. Therefore     (   S  n   )   n     subscript   superscript  S  n   n    (S^{n})_{n}   meet the definition above of a normal Lévy family.  Vitali Milman applied this fact to several problems in the local theory of Banach spaces, in particular, to give a new proof of Dvoretzky's theorem .  Other examples   Talagrand's concentration inequality  Gaussian isoperimetric inequality   Footnotes    Further reading      A. A. Giannopoulos and V. Milman, Concentration property on probability spaces , Advances in Mathematics 156 (2000), 77-106.   "  Category:Measure theory  Category:Asymptotic geometric analysis     Michel Talagand, A New Look at Independence, The Annals of Probability, 1996, Vol. 24, No.1, 1-34 ↩  " The concentration of     f  ∗    (  μ  )        subscript  f  normal-∗   μ    f_{\ast}(\mu)   , ubiquitous in the probability theory and statistical mechanics, was brought to geometry (starting from Banach spaces) by Vitali Milman, following the earlier work by Paul Lévy " - M. Gromov , Spaces and questions, GAFA 2000 (Tel Aviv, 1999), Geom. Funct. Anal. 2000, Special Volume, Part I, 118–161. ↩  " The idea of concentration of measure (which was discovered by V.Milman) is arguably one of the great ideas of analysis in our times. While its impact on Probability is only a small part of the whole picture, this impact should not be ignored. " - M. Talagrand , A new look at independence, Ann. Probab. 24 (1996), no. 1, 1–34. ↩     