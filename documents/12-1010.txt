   Conditional mutual information      Conditional mutual information   (Figure)  Venn diagram of information theoretic measures for three variables x, y, and z, represented by the lower left, lower right, and upper circles, respectively. The conditional mutual informations    I   (  x  ;  z  |  y  )      fragments  I   fragments  normal-(  x  normal-;  z  normal-|  y  normal-)     I(x;z|y)   ,    I   (  y  ;  z  |  x  )      fragments  I   fragments  normal-(  y  normal-;  z  normal-|  x  normal-)     I(y;z|x)   and    I   (  x  ;  y  |  z  )      fragments  I   fragments  normal-(  x  normal-;  y  normal-|  z  normal-)     I(x;y|z)   are represented by the yellow, magenta, and cyan regions, respectively.   In probability theory , and in particular, information theory , the conditional mutual information 1 2 is, in its most basic form, the expected value of the mutual information of two random variables given the value of a third.  Definition  For discrete random variables    X  ,    X   X,       Y  ,    Y   Y,   and    Z  ,    Z   Z,   we define      I   (  X  ;  Y  |  Z  )   =   ùîº  Z    (  I   (  X  ;  Y  )   |  Z  )   =   ‚àë   z  ‚àà  Z     p  Z    (  z  )    ‚àë   y  ‚àà  Y     ‚àë   x  ‚àà  X     p   X  ,  Y  |  Z     (  x  ,  y  |  z  )   log     p   X  ,  Y  |  Z     (  x  ,  y  |  z  )      p   X  |  Z     (  x  |  z  )    p   Y  |  Z     (  y  |  z  )     ,     fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)     subscript  ùîº  Z    fragments  normal-(  I   fragments  normal-(  X  normal-;  Y  normal-)   normal-|  Z  normal-)     subscript     z  Z     subscript  p  Z    fragments  normal-(  z  normal-)    subscript     y  Y     subscript     x  X     subscript  p   fragments  X  normal-,  Y  normal-|  Z     fragments  normal-(  x  normal-,  y  normal-|  z  normal-)       fragments   subscript  p   fragments  X  normal-,  Y  normal-|  Z     fragments  normal-(  x  normal-,  y  normal-|  z  normal-)     fragments   subscript  p   fragments  X  normal-|  Z     fragments  normal-(  x  normal-|  z  normal-)    subscript  p   fragments  Y  normal-|  Z     fragments  normal-(  y  normal-|  z  normal-)     normal-,    I(X;Y|Z)=\mathbb{E}_{Z}\big(I(X;Y)|Z\big)=\sum_{z\in Z}p_{Z}(z)\sum_{y\in Y}%
 \sum_{x\in X}p_{X,Y|Z}(x,y|z)\log\frac{p_{X,Y|Z}(x,y|z)}{p_{X|Z}(x|z)p_{Y|Z}(y%
 |z)},   where the marginal, joint, and/or conditional probability mass functions are denoted by   p   p   p   with the appropriate subscript. This can be simplified as      I   (  X  ;  Y  |  Z  )   =   ‚àë   z  ‚àà  Z     ‚àë   y  ‚àà  Y     ‚àë   x  ‚àà  X     p   X  ,  Y  ,  Z     (  x  ,  y  ,  z  )   log     p  Z    (  z  )    p   X  ,  Y  ,  Z     (  x  ,  y  ,  z  )      p   X  ,  Z     (  x  ,  z  )    p   Y  ,  Z     (  y  ,  z  )     .     fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)     subscript     z  Z     subscript     y  Y     subscript     x  X     subscript  p   X  Y  Z     fragments  normal-(  x  normal-,  y  normal-,  z  normal-)         subscript  p  Z   z   subscript  p   X  Y  Z     x  y  z       subscript  p   X  Z     x  z    subscript  p   Y  Z     y  z     normal-.    I(X;Y|Z)=\sum_{z\in Z}\sum_{y\in Y}\sum_{x\in X}p_{X,Y,Z}(x,y,z)\log\frac{p_{Z%
 }(z)p_{X,Y,Z}(x,y,z)}{p_{X,Z}(x,z)p_{Y,Z}(y,z)}.   Alternatively, we may write 3 in terms of joint and conditional entropies as      I   (  X  ;  Y  |  Z  )   =  H   (  X  ,  Z  )   +  H   (  Y  ,  Z  )   -  H   (  X  ,  Y  ,  Z  )   -  H   (  Z  )   =  H   (  X  |  Z  )   -  H   (  X  |  Y  ,  Z  )   .     fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)    H   fragments  normal-(  X  normal-,  Z  normal-)    H   fragments  normal-(  Y  normal-,  Z  normal-)    H   fragments  normal-(  X  normal-,  Y  normal-,  Z  normal-)    H   fragments  normal-(  Z  normal-)    H   fragments  normal-(  X  normal-|  Z  normal-)    H   fragments  normal-(  X  normal-|  Y  normal-,  Z  normal-)   normal-.    I(X;Y|Z)=H(X,Z)+H(Y,Z)-H(X,Y,Z)-H(Z)=H(X|Z)-H(X|Y,Z).   This can be rewritten to show its relationship to mutual information      I   (  X  ;  Y  |  Z  )   =  I   (  X  ;  Y  ,  Z  )   -  I   (  X  ;  Z  )      fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)    I   fragments  normal-(  X  normal-;  Y  normal-,  Z  normal-)    I   fragments  normal-(  X  normal-;  Z  normal-)     I(X;Y|Z)=I(X;Y,Z)-I(X;Z)   usually rearranged as the chain rule for mutual information      I   (  X  ;  Y  ,  Z  )   =  I   (  X  ;  Z  )   +  I   (  X  ;  Y  |  Z  )      fragments  I   fragments  normal-(  X  normal-;  Y  normal-,  Z  normal-)    I   fragments  normal-(  X  normal-;  Z  normal-)    I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)     I(X;Y,Z)=I(X;Z)+I(X;Y|Z)   Another equivalent form of the above is      I   (  X  ;  Y  |  Z  )   =  H   (  Z  |  X  )   +  H   (  X  )   +  H   (  Z  |  Y  )   +  H   (  Y  )   -  H   (  Z  |  X  ,  Y  )   -  H   (  X  ,  Y  )   -  H   (  Z  )   =  I   (  X  ;  Y  )   +  H   (  Z  |  X  )   +  H   (  Z  |  Y  )   -  H   (  Z  |  X  ,  Y  )   -  H   (  Z  )      fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)    H   fragments  normal-(  Z  normal-|  X  normal-)    H   fragments  normal-(  X  normal-)    H   fragments  normal-(  Z  normal-|  Y  normal-)    H   fragments  normal-(  Y  normal-)    H   fragments  normal-(  Z  normal-|  X  normal-,  Y  normal-)    H   fragments  normal-(  X  normal-,  Y  normal-)    H   fragments  normal-(  Z  normal-)    I   fragments  normal-(  X  normal-;  Y  normal-)    H   fragments  normal-(  Z  normal-|  X  normal-)    H   fragments  normal-(  Z  normal-|  Y  normal-)    H   fragments  normal-(  Z  normal-|  X  normal-,  Y  normal-)    H   fragments  normal-(  Z  normal-)     I(X;Y|Z)=H(Z|X)+H(X)+H(Z|Y)+H(Y)-H(Z|X,Y)-H(X,Y)-H(Z)=I(X;Y)+H(Z|X)+H(Z|Y)-H(Z%
 |X,Y)-H(Z)     Conditioning on a third random variable may either increase or decrease the mutual information: that is, the difference    I   (  X  ;  Y  |  Z  )   -  I   (  X  ;  Y  )      fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)    I   fragments  normal-(  X  normal-;  Y  normal-)     I(X;Y|Z)-I(X;Y)   , called the interaction information , may be positive, negative, or zero, but it is always true that      I   (  X  ;  Y  |  Z  )   ‚â•  0     fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)    0    I(X;Y|Z)\geq 0   for discrete, jointly distributed random variables X , Y , Z . This result has been used as a basic building block for proving other inequalities in information theory , in particular, those known as Shannon-type inequalities.  Like mutual information, conditional mutual information can be expressed as a Kullback‚ÄìLeibler divergence :      I   (  X  ;  Y  |  Z  )   =   D  KL    [  p   (  X  ,  Y  ,  Z  )   ‚à•  p   (  X  |  Z  )   p   (  Y  |  Z  )   p   (  Z  )   ]   .     fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)     subscript  D  KL    fragments  normal-[  p   fragments  normal-(  X  normal-,  Y  normal-,  Z  normal-)   parallel-to  p   fragments  normal-(  X  normal-|  Z  normal-)   p   fragments  normal-(  Y  normal-|  Z  normal-)   p   fragments  normal-(  Z  normal-)   normal-]   normal-.    I(X;Y|Z)=D_{\mathrm{KL}}[p(X,Y,Z)\|p(X|Z)p(Y|Z)p(Z)].     Or as an expected value of simpler Kullback‚ÄìLeibler divergences:      I   (  X  ;  Y  |  Z  )   =   ‚àë   z  ‚àà  Z    p   (  Z  =  z  )    D  KL    [  p   (  X  ,  Y  |  z  )   ‚à•  p   (  X  |  z  )   p   (  Y  |  z  )   ]   ,     fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)     subscript     z  Z    p   fragments  normal-(  Z   z  normal-)    subscript  D  KL    fragments  normal-[  p   fragments  normal-(  X  normal-,  Y  normal-|  z  normal-)   parallel-to  p   fragments  normal-(  X  normal-|  z  normal-)   p   fragments  normal-(  Y  normal-|  z  normal-)   normal-]   normal-,    I(X;Y|Z)=\sum_{z\in Z}p(Z=z)D_{\mathrm{KL}}[p(X,Y|z)\|p(X|z)p(Y|z)],         I   (  X  ;  Y  |  Z  )   =   ‚àë   y  ‚àà  Y    p   (  Y  =  y  )    D  KL    [  p   (  X  ,  Z  |  y  )   ‚à•  p   (  X  |  Z  )   p   (  Z  |  y  )   ]   .     fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)     subscript     y  Y    p   fragments  normal-(  Y   y  normal-)    subscript  D  KL    fragments  normal-[  p   fragments  normal-(  X  normal-,  Z  normal-|  y  normal-)   parallel-to  p   fragments  normal-(  X  normal-|  Z  normal-)   p   fragments  normal-(  Z  normal-|  y  normal-)   normal-]   normal-.    I(X;Y|Z)=\sum_{y\in Y}p(Y=y)D_{\mathrm{KL}}[p(X,Z|y)\|p(X|Z)p(Z|y)].     More general definition  A more general definition of conditional mutual information, applicable to random variables with continuous or other arbitrary distributions, will depend on the concept of regular conditional probability . (See also. 4 5 )  Let    (  Œ©  ,  ‚Ñ±  ,  ùîì  )     normal-Œ©  ‚Ñ±  ùîì    (\Omega,\mathcal{F},\mathfrak{P})   be a probability space , and let the random variables X , Y , and Z each be defined as a Borel-measurable function from   Œ©   normal-Œ©   \Omega   to some state space endowed with a topological structure.  Consider the Borel measure (on the œÉ-algebra generated by the open sets) in the state space of each random variable defined by assigning each Borel set the   ùîì   ùîì   \mathfrak{P}   -measure of its preimage in   ‚Ñ±   ‚Ñ±   \mathcal{F}   . This is called the pushforward measure        X  *   ùîì   =   ùîì   (    X   -  1     (  ‚ãÖ  )    )     .         subscript  X    ùîì     ùîì     superscript  X    1    normal-‚ãÖ      X_{*}\mathfrak{P}=\mathfrak{P}\big(X^{-1}(\cdot)\big).   The support of a random variable is defined to be the topological support of this measure, i.e.       supp   X   =    supp    X  *   ùîì    .        supp  X     supp   subscript  X    ùîì     \mathrm{supp}\,X=\mathrm{supp}\,X_{*}\mathfrak{P}.     Now we can formally define the conditional probability measure given the value of one (or, via the product topology , more) of the random variables. Let   M   M   M   be a measurable subset of    Œ©  ,    normal-Œ©   \Omega,   (i.e.     M  ‚àà  ‚Ñ±   ,      M  ‚Ñ±    M\in\mathcal{F},   ) and let     x  ‚àà    supp   X    .      x    supp  X     x\in\mathrm{supp}\,X.   Then, using the disintegration theorem :      ùîì   (  M  |  X  =  x  )   =   lim   U  ‚àã  x      ùîì   (  M  ‚à©   {  X  ‚àà  U  }   )     ùîì   (   {  X  ‚àà  U  }   )     and  ùîì   (  M  |  X  )   =   ‚à´  M   d  ùîì   (  œâ  |  X  =  X   (  œâ  )   )   ,     fragments  P   fragments  normal-(  M  normal-|  X   x  normal-)     subscript     x  U       fragments  P   fragments  normal-(  M    fragments  normal-{  X   U  normal-}   normal-)     fragments  P   fragments  normal-(   fragments  normal-{  X   U  normal-}   normal-)     italic-  and  italic-  P   fragments  normal-(  M  normal-|  X  normal-)     subscript   M   d  P   fragments  normal-(  œâ  normal-|  X   X   fragments  normal-(  œâ  normal-)   normal-)   normal-,    \mathfrak{P}(M|X=x)=\lim_{U\ni x}\frac{\mathfrak{P}(M\cap\{X\in U\})}{%
 \mathfrak{P}(\{X\in U\})}\qquad\textrm{and}\qquad\mathfrak{P}(M|X)=\int_{M}d%
 \mathfrak{P}\big(\omega|X=X(\omega)\big),   where the limit is taken over the open neighborhoods   U   U   U   of   x   x   x   , as they are allowed to become arbitrarily smaller with respect to set inclusion .  Finally we can define the conditional mutual information via Lebesgue integration :      I   (  X  ;  Y  |  Z  )   =   ‚à´  Œ©   log    d  ùîì   (  œâ  |  X  ,  Z  )   d  ùîì   (  œâ  |  Y  ,  Z  )     d  ùîì   (  œâ  |  Z  )   d  ùîì   (  œâ  |  X  ,  Y  ,  Z  )     d  ùîì   (  œâ  )   ,     fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)     subscript   normal-Œ©       fragments  d  P   fragments  normal-(  œâ  normal-|  X  normal-,  Z  normal-)   d  P   fragments  normal-(  œâ  normal-|  Y  normal-,  Z  normal-)     fragments  d  P   fragments  normal-(  œâ  normal-|  Z  normal-)   d  P   fragments  normal-(  œâ  normal-|  X  normal-,  Y  normal-,  Z  normal-)     d  P   fragments  normal-(  œâ  normal-)   normal-,    I(X;Y|Z)=\int_{\Omega}\log\frac{d\mathfrak{P}(\omega|X,Z)\,d\mathfrak{P}(%
 \omega|Y,Z)}{d\mathfrak{P}(\omega|Z)\,d\mathfrak{P}(\omega|X,Y,Z)}d\mathfrak{P%
 }(\omega),   where the integrand is the logarithm of a Radon‚ÄìNikodym derivative involving some of the conditional probability measures we have just defined.  Note on notation  In an expression such as    I   (  A  ;  B  |  C  )   ,     fragments  I   fragments  normal-(  A  normal-;  B  normal-|  C  normal-)   normal-,    I(A;B|C),       A  ,    A   A,       B  ,    B   B,   and   C   C   C   need not necessarily be restricted to representing individual random variables, but could also represent the joint distribution of any collection of random variables defined on the same probability space . As is common in probability theory , we may use the comma to denote such a joint distribution, e.g.    I   (   A  0   ,   A  1   ;   B  1   ,   B  2   ,   B  3   |   C  0   ,   C  1   )   .     fragments  I   fragments  normal-(   subscript  A  0   normal-,   subscript  A  1   normal-;   subscript  B  1   normal-,   subscript  B  2   normal-,   subscript  B  3   normal-|   subscript  C  0   normal-,   subscript  C  1   normal-)   normal-.    I(A_{0},A_{1};B_{1},B_{2},B_{3}|C_{0},C_{1}).   Hence the use of the semicolon (or occasionally a colon or even a wedge   ‚àß     \wedge   ) to separate the principal arguments of the mutual information symbol. (No such distinction is necessary in the symbol for joint entropy , since the joint entropy of any number of random variables is the same as the entropy of their joint distribution.)  Multivariate mutual information  The conditional mutual information can be used to inductively define a multivariate mutual information in a set- or measure-theoretic sense in the context of information diagrams . In this sense we define the multivariate mutual information as follows:      I   (   X  1   ;  ‚Ä¶  ;   X   n  +  1    )   =  I   (   X  1   ;  ‚Ä¶  ;   X  n   )   -  I   (   X  1   ;  ‚Ä¶  ;   X  n   |   X   n  +  1    )   ,     fragments  I   fragments  normal-(   subscript  X  1   normal-;  normal-‚Ä¶  normal-;   subscript  X    n  1    normal-)    I   fragments  normal-(   subscript  X  1   normal-;  normal-‚Ä¶  normal-;   subscript  X  n   normal-)    I   fragments  normal-(   subscript  X  1   normal-;  normal-‚Ä¶  normal-;   subscript  X  n   normal-|   subscript  X    n  1    normal-)   normal-,    I(X_{1};\ldots;X_{n+1})=I(X_{1};\ldots;X_{n})-I(X_{1};\ldots;X_{n}|X_{n+1}),   where      I   (   X  1   ;  ‚Ä¶  ;   X  n   |   X   n  +  1    )   =   ùîº   X   n  +  1      (  I   (   X  1   ;  ‚Ä¶  ;   X  n   )   |   X   n  +  1    )   .     fragments  I   fragments  normal-(   subscript  X  1   normal-;  normal-‚Ä¶  normal-;   subscript  X  n   normal-|   subscript  X    n  1    normal-)     subscript  ùîº   subscript  X    n  1      fragments  normal-(  I   fragments  normal-(   subscript  X  1   normal-;  normal-‚Ä¶  normal-;   subscript  X  n   normal-)   normal-|   subscript  X    n  1    normal-)   normal-.    I(X_{1};\ldots;X_{n}|X_{n+1})=\mathbb{E}_{X_{n+1}}\big(I(X_{1};\ldots;X_{n})|X%
 _{n+1}\big).   This definition is identical to that of interaction information except for a change in sign in the case of an odd number of random variables. A complication is that this multivariate mutual information (as well as the interaction information) can be positive, negative, or zero, which makes this quantity difficult to interpret intuitively. In fact, for n random variables, there are     2  n   -  1       superscript  2  n   1    2^{n}-1   degrees of freedom for how they might be correlated in an information-theoretic sense, corresponding to each non-empty subset of these variables. These degrees of freedom are bounded by various Shannon- and non-Shannon-type inequalities in information theory .  References    "  Category:Information theory  Category:Entropy and information     ‚Ü©  ‚Ü©  K. Makarychev et al. A new class of non-Shannon-type inequalities for entropies. Communications in Information and Systems, Vol. 2, No. 2, pp. 147‚Äì166, December 2002 PDF ‚Ü©  Regular Conditional Probability on PlanetMath ‚Ü©  D. Leao, Jr. et al. Regular conditional probability, disintegration of probability and Radon spaces. Proyecciones. Vol. 23, No. 1, pp. 15‚Äì29, May 2004, Universidad Cat√≥lica del Norte, Antofagasta, Chile PDF ‚Ü©     