   Bayesian experimental design      Bayesian experimental design   Bayesian experimental design provides a general probability-theoretical framework from which other theories on experimental design can be derived. It is based on Bayesian inference to interpret the observations/data acquired during the experiment. This allows accounting for both any prior knowledge on the parameters to be determined as well as uncertainties in observations.  The theory of Bayesian experimental design is to a certain extent based on the theory for making optimal decisions under uncertainty . The aim when designing an experiment is to maximize the expected utility of the experiment outcome. The utility is most commonly defined in terms of a measure of the accuracy of the information provided by the experiment (e.g. the Shannon information or the negative variance ), but may also involve factors such as the financial cost of performing the experiment. What will be the optimal experiment design depends on the particular utility criterion chosen.  Relations to more specialized optimal design theory  Linear theory  If the model is linear, the prior probability density function (PDF) is homogeneous and observational errors are normally distributed , the theory simplifies to the classical optimal experimental design theory .  Approximate normality  In numerous publications on Bayesian experimental design, it is (often implicitly) assumed that all posterior PDFs will be approximately normal. This allows for the expected utility to be calculated using linear theory, averaging over the space of model parameters, an approach reviewed in . Caution must however be taken when applying this method, since approximate normality of all possible posteriors is difficult to verify, even in cases of normal observational errors and uniform prior PDF.  Posterior distribution  Recently, increased computational resources allow inference of the posterior distribution of model parameters, which can directly be used for experiment design.  proposed an approach that uses the posterior predictive distribution to assess the effect of new measurements on prediction uncertainty, while  suggest maximizing the mutual information between parameters, predictions and potential new experiments.  Mathematical formulation         Notation     nowrap|    θ    θ   \theta\,        nowrap|    y    y   y\,        nowrap|    ξ    ξ   \xi\,        nowrap|    p   (  y  |  θ  ,  ξ  )      fragments  p   fragments  normal-(  y  normal-|  θ  normal-,  ξ  normal-)     p(y|\theta,\xi)\,        nowrap|    p   (  θ  )       p  θ    p(\theta)\,        nowrap|    p   (  y  |  ξ  )      fragments  p   fragments  normal-(  y  normal-|  ξ  normal-)     p(y|\xi)\,        nowrap|    p   (  θ  |  y  ,  ξ  )      fragments  p   fragments  normal-(  θ  normal-|  y  normal-,  ξ  normal-)     p(\theta|y,\xi)\,        nowrap|    U   (  ξ  )       U  ξ    U(\xi)\,        nowrap|    U   (  y  ,  ξ  )       U   y  ξ     U(y,\xi)\,           Given a vector   θ   θ   \theta   of parameters to determine, a prior PDF     p   (  θ  )       p  θ    p(\theta)   over those parameters and a PDF    p   (  y  |  θ  ,  ξ  )      fragments  p   fragments  normal-(  y  normal-|  θ  normal-,  ξ  normal-)     p(y|\theta,\xi)   for making observation   y   y   y   , given parameter values   θ   θ   \theta   and an experiment design   ξ   ξ   \xi   , the posterior PDF can be calculated using Bayes' theorem      p   (  θ  |  y  ,  ξ  )   =     p   (  y  |  θ  ,  ξ  )   p   (  θ  )     p   (  y  |  ξ  )      ,     fragments  p   fragments  normal-(  θ  normal-|  y  normal-,  ξ  normal-)       fragments  p   fragments  normal-(  y  normal-|  θ  normal-,  ξ  normal-)   p   fragments  normal-(  θ  normal-)     fragments  p   fragments  normal-(  y  normal-|  ξ  normal-)     normal-,    p(\theta|y,\xi)=\frac{p(y|\theta,\xi)p(\theta)}{p(y|\xi)}\,,     where    p   (  y  |  ξ  )      fragments  p   fragments  normal-(  y  normal-|  ξ  normal-)     p(y|\xi)   is the marginal probability density in observation space      p   (  y  |  ξ  )   =  ∫  p   (  θ  )   p   (  y  |  θ  ,  ξ  )   d   θ   .     fragments  p   fragments  normal-(  y  normal-|  ξ  normal-)     p   fragments  normal-(  θ  normal-)   p   fragments  normal-(  y  normal-|  θ  normal-,  ξ  normal-)   d  θ  normal-.    p(y|\xi)=\int{p(\theta)p(y|\theta,\xi)d\theta}\,.     The expected utility of an experiment with design   ξ   ξ   \xi   can then be defined      U   (  ξ  )   =  ∫  p   (  y  |  ξ  )   U   (  y  ,  ξ  )   d   y   ,     fragments  U   fragments  normal-(  ξ  normal-)     p   fragments  normal-(  y  normal-|  ξ  normal-)   U   fragments  normal-(  y  normal-,  ξ  normal-)   d  y  normal-,    U(\xi)=\int{p(y|\xi)U(y,\xi)dy}\,,   where    U   (  y  ,  ξ  )       U   y  ξ     U(y,\xi)   is some real-valued functional of the posterior PDF     p   (  θ  |  y  ,  ξ  )      fragments  p   fragments  normal-(  θ  normal-|  y  normal-,  ξ  normal-)     p(\theta|y,\xi)   after making observation   y   y   y   using an experiment design   ξ   ξ   \xi   .  Gain in Shannon information as utility  Utility may be defined as the prior-posterior gain in Shannon information      U   (  y  ,  ξ  )   =  ∫  log   (  p   (  θ  |  y  ,  ξ  )   )   p   (  θ  |  y  ,  ξ  )   d  θ  -  ∫  log   (  p   (  θ  )   )   p   (  θ  )   d   θ   .     fragments  U   fragments  normal-(  y  normal-,  ξ  normal-)       fragments  normal-(  p   fragments  normal-(  θ  normal-|  y  normal-,  ξ  normal-)   normal-)   p   fragments  normal-(  θ  normal-|  y  normal-,  ξ  normal-)   d  θ      fragments  normal-(  p   fragments  normal-(  θ  normal-)   normal-)   p   fragments  normal-(  θ  normal-)   d  θ  normal-.    U(y,\xi)=\int{\log(p(\theta|y,\xi))p(\theta|y,\xi)d\theta}-\int{\log(p(\theta)%
 )p(\theta)d\theta}\,.   Note also that      U   (  y  ,  ξ  )   =   D   K  L     (  p   (  θ  |  y  ,  ξ  )   ∥  p   (  θ  |  ξ  )   )   ,     fragments  U   fragments  normal-(  y  normal-,  ξ  normal-)     subscript  D    K  L     fragments  normal-(  p   fragments  normal-(  θ  normal-|  y  normal-,  ξ  normal-)   parallel-to  p   fragments  normal-(  θ  normal-|  ξ  normal-)   normal-)   normal-,    U(y,\xi)=D_{KL}(p(\theta|y,\xi)\|p(\theta|\xi))\,,   the Kullback–Leibler divergence of the prior from the posterior distribution.  noted that the expected utility will then be coordinate-independent and can be written in two forms          U   (  ξ  )       =  ∫  ∫  log   (  p   (  θ  |  y  ,  ξ  )   )   p   (  θ  ,  y  |  ξ  )   d  θ  d  y  -  ∫  log   (  p   (  θ  )   )   p   (  θ  )   d  θ         =  ∫  ∫  log   (  p   (  y  |  θ  ,  ξ  )   )   p   (  θ  ,  y  |  ξ  )   d  y  d  θ  -  ∫  log   (  p   (  y  |  ξ  )   )   p   (  y  |  ξ  )   d  y  ,            U  ξ    fragments       fragments  normal-(  p   fragments  normal-(  θ  normal-|  y  normal-,  ξ  normal-)   normal-)   p   fragments  normal-(  θ  normal-,  y  normal-|  ξ  normal-)   d  θ  d  y      fragments  normal-(  p   fragments  normal-(  θ  normal-)   normal-)   p   fragments  normal-(  θ  normal-)   d  θ      missing-subexpression    fragments       fragments  normal-(  p   fragments  normal-(  y  normal-|  θ  normal-,  ξ  normal-)   normal-)   p   fragments  normal-(  θ  normal-,  y  normal-|  ξ  normal-)   d  y  d  θ      fragments  normal-(  p   fragments  normal-(  y  normal-|  ξ  normal-)   normal-)   p   fragments  normal-(  y  normal-|  ξ  normal-)   d  y  normal-,      \begin{aligned}\displaystyle U(\xi)&\displaystyle=\int{\int{\log(p(\theta|y,%
 \xi))p(\theta,y|\xi)d\theta}dy}-\int{\log(p(\theta))p(\theta)d\theta}\\
 &\displaystyle=\int{\int{\log(p(y|\theta,\xi))p(\theta,y|\xi)dy}d\theta}-\int{%
 \log(p(y|\xi))p(y|\xi)dy},\end{aligned}\,     of which the latter can be evaluated without the need for evaluating individual posterior PDFs    p   (  θ  |  y  ,  ξ  )      fragments  p   fragments  normal-(  θ  normal-|  y  normal-,  ξ  normal-)     p(\theta|y,\xi)   for all possible observations   y   y   y   . Worth noting is that the first term on the second equation line will not depend on the design   ξ   ξ   \xi   , as long as the observational uncertainty doesn't. On the other hand, the integral of    p   (  θ  )    log  p    (  θ  )       p  θ    p   θ    p(\theta)\log p(\theta)   in the first form is constant for all   ξ   ξ   \xi   , so if the goal is to choose the design with the highest utility, the term need not be computed at all. Several authors have considered numerical techniques for evaluating and optimizing this criterion, e.g.  and . Note that        U   (  ξ  )    =   I   (  θ  ;  y  )     ,        U  ξ     I   θ  y      U(\xi)=I(\theta;y)\,,   the expected information gain being exactly the mutual information between the parameter θ and the observation y .  also derived just such a utility function for a gambler seeking to profit maximally from side information in a horse race ; Kelly's situation is identical to the foregoing, with the side information, or "private wire" taking the place of the experiment.  See also   Optimal Designs  Active Learning   References                       "  Experimental design  Category:Design of experiments  Category:Statistical methods  Category:Optimal decisions  Category:Operations research  Category:Industrial engineering  Category:Systems engineering  Category:Information, knowledge, and uncertainty   