   Regularization (mathematics)      Regularization (mathematics)   Regularization , in mathematics and statistics and particularly in the fields of machine learning and inverse problems , refers to a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting . This information is usually of the form of a penalty for complexity, such as restrictions for smoothness or bounds on the vector space norm .  A theoretical justification for regularization is that it attempts to impose Occam's razor on the solution. From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters.  The same idea arose in many fields of science . For example, the least-squares method can be viewed as a very simple form of regularization. A simple form of regularization applied to integral equations , generally termed Tikhonov regularization after Andrey Nikolayevich Tikhonov , is essentially a trade-off between fitting the data and reducing a norm of the solution. More recently, non-linear regularization methods, including total variation regularization have become popular.  Regularization in statistics and machine learning  In statistics and machine learning, regularization methods are used for model selection, in particular to prevent overfitting by penalizing models with extreme parameter values. The most common variants in machine learning are    L  ₁      L  normal-₁    L₁   and    L  ₂      L  normal-₂    L₂   regularization, which can be added to learning algorithms that minimize a loss function     E   (  X  ,  Y  )       E   X  Y     E(X,Y)   by instead minimizing     E   (  X  ,  Y  )    +   α  ‖  w  ‖         E   X  Y      α  normal-‖  w  normal-‖     E(X,Y)+α‖w‖   , where   w   w   w   is the model's weight vector, ‖·‖ is either the    L  ₁      L  normal-₁    L₁   norm or the squared    L  ₂      L  normal-₂    L₂   norm, and α is a free parameter that needs to be tuned empirically (typically by cross-validation ; see hyperparameter optimization ). This method applies to many models. When applied in linear regression , the resulting models are termed lasso or ridge regression , but regularization is also employed in (binary and multiclass ) logistic regression , neural nets , support vector machines , conditional random fields and some matrix decomposition methods.    L  ₂      L  normal-₂    L₂   regularization may also be called "weight decay", in particular in the setting of neural nets.      L  ₁      L  normal-₁    L₁   regularization is often preferred because it produces sparse models and thus performs feature selection within the learning algorithm, but since the    L  ₁      L  normal-₁    L₁   norm is not differentiable, it may require changes to learning algorithms, in particular gradient-based learners. 1 2  Bayesian learning methods make use of a prior probability that (usually) gives lower probability to more complex models. Well-known model selection techniques include the Akaike information criterion (AIC), minimum description length (MDL), and the Bayesian information criterion (BIC). Alternative methods of controlling overfitting not involving regularization include cross-validation .  Regularization can be used to fine-tune model complexity using an augmented error function with cross-validation. The data sets used in complex models can produce a levelling-off of validation as complexity of the models increases. Training data sets errors decrease while the validation data set error remains constant. Regularization introduces a second factor which weights the penalty against more complex models with an increasing variance in the data errors. This gives an increasing penalty as model complexity increases. 3  Examples of applications of different methods of regularization to the linear model are:      Model   Fit measure   Entropy measure 4 5       AIC / BIC        ∥   Y  -   X  β    ∥   2     subscript   norm    Y    X  β     2    \|Y-X\beta\|_{2}           ∥  β  ∥   0     subscript   norm  β   0    \|\beta\|_{0}        Ridge regression        ∥   Y  -   X  β    ∥   2     subscript   norm    Y    X  β     2    \|Y-X\beta\|_{2}           ∥  β  ∥   2     subscript   norm  β   2    \|\beta\|_{2}        Lasso {{Cite journal   last = Tibshirani   first = Robert     Basis pursuit denoising        ∥   Y  -   X  β    ∥   2     subscript   norm    Y    X  β     2    \|Y-X\beta\|_{2}          λ    ∥  β  ∥   1       λ   subscript   norm  β   1     \lambda\|\beta\|_{1}        Rudin-Osher-Fatemi model (TV)        ∥   Y  -   X  β    ∥   2     subscript   norm    Y    X  β     2    \|Y-X\beta\|_{2}          λ    ∥   ∇  β   ∥   1       λ   subscript   norm   normal-∇  β    1     \lambda\|\nabla\beta\|_{1}        Potts model        ∥   Y  -   X  β    ∥   2     subscript   norm    Y    X  β     2    \|Y-X\beta\|_{2}          λ    ∥   ∇  β   ∥   0       λ   subscript   norm   normal-∇  β    0     \lambda\|\nabla\beta\|_{0}        RLAD {{Cite conference   author = Li Wang, Michael D. Gordon & Ji Zhu   title = Regularized Least Absolute Deviations Regression and an Efficient Algorithm for Parameter Tuning     Dantzig Selector {{Cite journal   last = Candes   first = Emmanuel | authorlink = Emmanuel Candès     SLOPE {{Cite journal   author = Małgorzata Bogdan, Ewout van den Berg, Weijie Su & Emmanuel J. Candes   title = Statistical estimation and testing via the ordered L1 norm     A linear combination of the LASSO and ridge regression methods is elastic net regularization .  Ensemble-based regularization  In inverse problem theory, an optimization problem is usually solved to generate a model that provides a good match to observed data. In this context, a regularization term is used to preserve prior information about the model and prevent over-fitting and convergence to a model that matches the data but does not predict well. Ensemble-based regularization is based on utilizing an ensemble (i.e., a set) of realizations from the prior probability distribution function (pdf) to construct a regularization term . 6 This regularization is flexible as it is based on representing the prior pdf using a set of realizations, instead of using, say a mean and covariance matrix for Gaussian distribution.  See also   Bayesian interpretation of regularization  Regularization by spectral filtering   Notes  References   A. Neumaier, Solving ill-conditioned and singular linear systems: A tutorial on regularization, SIAM Review 40 (1998), 636-666. Available in pdf from author's website .   de:Regularisierung "  Category:Mathematical analysis  Category:Inverse problems     ↩  ↩  ↩  ↩  ↩  ↩     