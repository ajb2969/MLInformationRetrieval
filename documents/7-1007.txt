   Jensen–Shannon divergence      Jensen–Shannon divergence  In [[probability theory]] and [[statistics]], the '''[[Johan Jensen (mathematician)|Jensen]]–[[Claude Shannon|Shannon]] divergence''' is a popular method of measuring the similarity between two [[probability distribution]]s.  It is also known as '''information radius''' ('''IRad''') {{cite book |author=Hinrich Schütze; Christopher D. Manning|title=Foundations of Statistical Natural Language Processing |publisher=MIT Press |location=Cambridge, Mass |year=1999 |isbn=0-262-13360-1 |url=http://nlp.stanford.edu/fsnlp/ |doi= |page=304}} or '''total divergence to the average'''. {{cite journal|title=Similarity-Based Methods For Word Sense Disambiguation|journal=Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics|year=1997|first=Ido|last=Dagan|author2=Lillian Lee |author3=Fernando Pere ira |volume=|issue=|pages=pp. 56–63|id= |url= http://citeseer.ist.psu.edu/dagan97similaritybased.html|accessdate=2008-03-09|doi=10.3115/979617.979625 }} It is based on the Kullback–Leibler divergence , with some notable (and useful) differences, including that it is symmetric and it is always a finite value. The square root of the Jensen–Shannon divergence is a metric often referred to as Jensen-Shannon distance. 1 2  Definition  Consider the set     M  +  1    (  A  )        superscript   subscript  M    1   A    M_{+}^{1}(A)   of probability distributions where A is a set provided with some σ-algebra of measurable subsets. In particular we can take A to be a finite or countable set with all subsets being measurable.  The Jensen–Shannon divergence (JSD)        M  +  1    (  A  )    ×   M  +  1     (  A  )    →   [  0  ,  ∞  )      normal-→         superscript   subscript  M    1   A    superscript   subscript  M    1    A    0      M_{+}^{1}(A)\times M_{+}^{1}(A)\rightarrow[0,\infty{})   is a symmetrized and smoothed version of the Kullback–Leibler divergence     D   (  P  ∥  Q  )      fragments  D   fragments  normal-(  P  parallel-to  Q  normal-)     D(P\parallel Q)   . It is defined by      JSD   (  P  ∥  Q  )   =   1  2   D   (  P  ∥  M  )   +   1  2   D   (  Q  ∥  M  )      fragments  JSD   fragments  normal-(  P  parallel-to  Q  normal-)      1  2   D   fragments  normal-(  P  parallel-to  M  normal-)      1  2   D   fragments  normal-(  Q  parallel-to  M  normal-)     {\rm JSD}(P\parallel Q)=\frac{1}{2}D(P\parallel M)+\frac{1}{2}D(Q\parallel M)     where    M  =    1  2    (   P  +  Q   )        M      1  2     P  Q      M=\frac{1}{2}(P+Q)     A more general definition, allowing for the comparison of more than two probability distributions, is:        JSD    π  1   ,  …  ,   π  n      (   P  1   ,   P  2   ,  …  ,   P  n   )    =    H   (    ∑   i  =  1   n     π  i    P  i     )    -    ∑   i  =  1   n     π  i   H   (   P  i   )             subscript  JSD    subscript  π  1   normal-…   subscript  π  n       subscript  P  1    subscript  P  2   normal-…   subscript  P  n         H    superscript   subscript     i  1    n      subscript  π  i    subscript  P  i        superscript   subscript     i  1    n      subscript  π  i   H   subscript  P  i        {\rm JSD}_{\pi_{1},\ldots,\pi_{n}}(P_{1},P_{2},\ldots,P_{n})=H\left(\sum_{i=1}%
 ^{n}\pi_{i}P_{i}\right)-\sum_{i=1}^{n}\pi_{i}H(P_{i})     where     π  1   ,  …  ,   π  n       subscript  π  1   normal-…   subscript  π  n     \pi_{1},\ldots,\pi_{n}   are weights that are selected for the probability distributions     P  1   ,   P  2   ,  …  ,   P  n       subscript  P  1    subscript  P  2   normal-…   subscript  P  n     P_{1},P_{2},\ldots,P_{n}   and    H   (  P  )       H  P    H(P)   is the Shannon entropy for distribution   P   P   P   . For the two-distribution case described above,         P  1   =  P   ,     P  2   =  Q   ,    π  1   =   π  2   =   1  2      .     formulae-sequence     subscript  P  1   P    formulae-sequence     subscript  P  2   Q        subscript  π  1    subscript  π  2          1  2        P_{1}=P,P_{2}=Q,\pi_{1}=\pi_{2}=\frac{1}{2}.     Bounds  The Jensen–Shannon divergence is bounded by 1, given that one uses the base 2 logarithm. 3      0  ≤  JSD   (  P  ∥  Q  )   ≤  1     fragments  0   JSD   fragments  normal-(  P  parallel-to  Q  normal-)    1    0\leq{\rm JSD}(P\parallel Q)\leq 1     For log base e, or ln, which is commonly used in statistical thermodynamics, the upper bound is ln(2):      0  ≤  JSD   (  P  ∥  Q  )   ≤  ln   (  2  )      fragments  0   JSD   fragments  normal-(  P  parallel-to  Q  normal-)      fragments  normal-(  2  normal-)     0\leq{\rm JSD}(P\parallel Q)\leq\ln(2)     Relation to mutual information  The Jensen–Shannon divergence is the mutual information between a random variable   X   X   X   associated to a mixture distribution between   P   P   P   and   Q   Q   Q   and the binary indicator variable   Z   Z   Z   that is used to switch between   P   P   P   and   Q   Q   Q   to produce the mixture. Let   X   X   X   be some abstract function on the underlying set of events that discriminates well between events, and choose the value of   X   X   X   according to   P   P   P   if    Z  =  0      Z  0    Z=0   and according to   Q   Q   Q   if    Z  =  1      Z  1    Z=1   . That is, we are choosing   X   X   X   according to the probability measure    M  =    (   P  +  Q   )   /  2       M      P  Q   2     M=(P+Q)/2   , and its distribution is the mixture distribution. We compute      I   (  X  ;  Z  )       I   X  Z     \displaystyle I(X;Z)   It follows from the above result that the Jensen–Shannon divergence is bounded by 0 and 1 because mutual information is non-negative and bounded by     H   (  Z  )    =  1        H  Z   1    H(Z)=1   . The JSD is not always bounded by 0 and 1: the upper limit of 1 arises here because we are considering the specific case involving the binary variable   Z   Z   Z   .  One can apply the same principle to a joint distribution and the product of its two marginal distribution (in analogy to Kullback–Leibler divergence and mutual information) and to measure how reliably one can decide if a given response comes from the joint distribution or the product distribution—subject to the assumption that these are the only two possibilities. 4  Quantum Jensen–Shannon divergence  The generalization of probability distributions on density matrices allows to define quantum Jensen–Shannon divergence (QJSD). 5 6 It is defined for a set of density matrices     (   ρ  1   ,  …  ,   ρ  n   )      subscript  ρ  1   normal-…   subscript  ρ  n     (\rho_{1},\ldots,\rho_{n})   and probability distribution    π  =   (   π  1   ,  …  ,   π  n   )       π    subscript  π  1   normal-…   subscript  π  n      \pi=(\pi_{1},\ldots,\pi_{n})   as       QJSD   (   ρ  1   ,  …  ,   ρ  n   )    =    S   (    ∑   i  =  1   n     π  i    ρ  i     )    -    ∑   i  =  1   n     π  i   S   (   ρ  i   )            QJSD    subscript  ρ  1   normal-…   subscript  ρ  n         S    superscript   subscript     i  1    n      subscript  π  i    subscript  ρ  i        superscript   subscript     i  1    n      subscript  π  i   S   subscript  ρ  i        {\rm QJSD}(\rho_{1},\ldots,\rho_{n})=S\left(\sum_{i=1}^{n}\pi_{i}\rho_{i}%
 \right)-\sum_{i=1}^{n}\pi_{i}S(\rho_{i})     where    S   (   π  i   )       S   subscript  π  i     S(\pi_{i})   is the von Neumann entropy . This quantity was introduced in quantum information theory, where it is called the Holevo information: it gives the upper bound for amount of classical information encoded by the quantum states    (   ρ  1   ,  …  ,   ρ  n   )      subscript  ρ  1   normal-…   subscript  ρ  n     (\rho_{1},\ldots,\rho_{n})   under the prior distribution   π   π   \pi   (see Holevo's theorem ) 7 Quantum Jensen–Shannon divergence for    π  =   (   1  2   ,   1  2   )       π     1  2     1  2      \pi=\left(\frac{1}{2},\frac{1}{2}\right)   and two density matrices is a symmetric function, everywhere defined, bounded and equal to zero only if two density matrices are the same. It is a square of a metric for pure states 8 but it is unknown whether the metric property holds in general. 9 The Bures metric is closely related to the quantum JS divergence; it is the quantum analog of the Fisher information metric .  Applications  The Jensen–Shannon divergence has been applied in bioinformatics and genome comparison , 10 11 in protein surface comparison, 12 in the social sciences, 13 in the quantitative study of history, 14 and in machine learning. 15  Notes  Further reading      External links   Ruby gem for calculating JS divergence  Python code for calculating JS divergence   "  Category:Statistical distance measures     ↩  ↩   ↩  ↩  ↩  . English translation: Probl. Inf. Transm ., 9, 177–183 (1975)) ↩  ↩        Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, "Generative Adversarial Networks", NIPS 2014. http://arxiv.org/abs/1406.2661 ↩     