


Information distance




Information distance

Information distance is the distance between two finite objects (represented as computer files) expressed as the number of bits in the shortest program which transforms one object into the other one or vice versa on a universal computer. This is an extension of Kolmogorov complexity.1 The Kolmogorov complexity of a single finite object is the information in that object; the information distance between a pair of finite objects is the minimum information required to go from one object to the other or vice versa. Information distance was first defined and investigated in 2 based on thermodynamic principles, see also.3 Subsequently it achieved final form in.4 It is applied in the normalized compression distance and the normalized Google distance.
Properties
Formally the information distance 
 
 
 
  between 
 
 
 
  and 
 
 
 
  is defined by


 
  with 
 
 
 
  a finite binary program for the fixed universal computer with as inputs finite binary strings 
 
 
 
 . In 5 it is proven that 
 
 
 
  with


 
  where 
 
 
 
  is the Kolmogorov complexity defined by 6 of the prefix type.7 This 
 
 
 
  is the important quantity.
Universality
Let 
 
 
 
  be the class of upper semicomputable distances

 
  that satisfy the density condition


 
  This excludes irrelevant distances such as 
 
 
 
  for 
 
 
 
 ; it takes care that if the distance growth then the number of objects within that distance of a geven object grows. If 
 
 
 
  then 
 
 
 
  up to a constant additive term.8
Metricity
The distance 
 
 
 
  is a metric up to an additive 
 
 
 
  term in the metric (in)equalities.9
Maximum overlap
If 
 
 
 
 , then there is a program 
 
 
 
  of length 
 
 
 
  that converts 
 
 
 
  to 
 
 
 
 , and a program 
 
 
 
  of length 
 
 
 
  such that the program 
 
 
 
  converts 
 
 
 
  to 
 
 
 
 . (The programs are of the self-delimiting format which means that one can decide where one program ends and the other begins in concatenation of the programs.) That is, the shortest programs to convert between two objects can be made maximally overlapping: For 
 
 
 
  it can be divided into a program that converts object 
 
 
 
  to object 
 
 
 
 , and another program which concatenated with the first converts 
 
 
 
  to 
 
 
 
  while the concatenation of these two programs is a shortest program to convert between these objects.10
Minimum overlap
The programs to convert between objacts 
 
 
 
  and 
 
 
 
  can also be made minimal overlapping. There exists a program 
 
 
 
  of length 
 
 
 
  up to an additive term of 
 
 
 
  that maps 
 
 
 
  to 
 
 
 
  and has small complexity when 
 
 
 
  is known (
 
 
 
 ). Interchanging the two objects we have the other program11 Having in mind the parallelism between Shannon information theory and Kolmogorov complexity theory, one can say that this result is parallel to the Slepian-Wolf and Körner–Imre Csiszár–Marton theorems.
Applications
Theoretical
The result of An.A. Muchnik on minimum overlap above is an important theoretical application showing that certain codes exist: to go to finite target object from any object there is a program which almost only depends on the target object! This result is fairly precise and the error term cannot be significantly improved.12 Information distance was material in the textbook,13 it occurs in the Encyclopedia on Distances.14
Practical
To determine the similarity of objects such as genomes, languages, music, internet attacks and worms, software programs, and so on, information distance is normalized and the Kolmogorov complexity terms approximated by real-world compressors (the Kolmogorov complexity is a lower bound to the length in bits of a compressed version of the object). The result is the normalized compression distance (NCD) between the objects. This pertains to objects given as computer files like the genome of a mouse or text of a book. If the objects are just given by name such as `Einstein' or `table' or the name of a book or the name `mouse', compression does not make sense. We need outside information about what the name means. Using a data base (such as the internet) and a means to search the database (such as a search engine like Google) provides this information. Every search engine on a data base that provides aggregate page counts can be used in the normalized Google distance (NGD).
References
Related literature



"
Category:Statistical distance measures



[http://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid;;=ppi&paperid;=68&option;_lang=eng A.N. Kolmogorov, Three approaches to the quantitative definition of information, Problems Inform. Transmission, 1:1(1965), 1–7]↩
M. Li, P.M.B. Vitanyi, Theory of Thermodynamics of Computation, Proc. IEEE Physics of Computation Workshop, Dallas, Texas, USA, 1992, 42–46↩
M. Li, P.M.B. Vitanyi, Reversibility and Adiabatic Computation: Trading Time and Space for Energy, Proc. R. Soc. Lond. A 9 April 1996 vol. 452 no. 1947 769–789↩
[http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber;=681318&url;=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D681318 C.H. Bennett, P. Gacs, M. Li, P.M.B. Vitanyi, W. Zurek, Information distance, IEEE Transactions on Information Theory, 44:4(1998), 1407–1423]↩


[http://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid;;=ppi&paperid;=1039&option;_lang=eng L.A. Levin, Laws of Information Conservation (Nongrowth) and Aspects of the Foundation of Probability Theory, Problems Inform. Transmission, 10:3(1974), 30–35]↩



A.A. Muchnik, Conditional complexity and codes, Theoretical Computer Science, 271: 1–2(2002), 97–109↩
[http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber;=856744&url;=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D856744 N.K Vereshchagin, M.V. Vyugin, Independent minimum length programs to translate between given strings, Proc. 15th Ann. Conf. Computational Complexity, 2000, 138–144]↩
[http://books.google.nl/books?hl=nl&lr;;=&id;=ziLLYu7oIkQC&oi;=fnd&pg;=PR3&ots;=8HPfnQFmII&sig;=vgXU_u9DnWV798WrHLmNVB3fG_E#v=onepage&q;&f;=false M.Hutter, Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability, Springer, 1998]↩
[http://download.springer.com/static/pdf/223/chp%253A10.1007%252F978-3-642-00234-2_1.pdf?auth66=1385145284_cd50f0c0bc5fc787fc689e492711df23&ext;;=.pdf M.M. Deza, E Deza, Encyclopedia of Distances, Springer, 2009]↩




