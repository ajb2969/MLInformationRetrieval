   Method of moments (probability theory)      Method of moments (probability theory)   In probability theory , the method of moments is a way of proving convergence in distribution by proving convergence of a sequence of moment sequences. 1 Suppose X is a random variable and that all of the moments      E   (   X  k   )      normal-E   superscript  X  k     \operatorname{E}(X^{k})\,     exist. Further suppose the probability distribution of X is completely determined by its moments, i.e., there is no other probability distribution with the same sequence of moments (cf. the problem of moments ). If        lim   n  →  ∞     E   (   X  n  k   )     =   E   (   X  k   )          subscript    normal-→  n      normal-E   superscript   subscript  X  n   k      normal-E   superscript  X  k      \lim_{n\to\infty}\operatorname{E}(X_{n}^{k})=\operatorname{E}(X^{k})\,     for all values of k , then the sequence { X n } converges to X in distribution.  The method of moments was introduced by Pafnuty Chebyshev for proving the central limit theorem ; Chebyshev cited earlier contributions by Irénée-Jules Bienaymé . 2 More recently, it has been applied by Eugene Wigner to prove Wigner's semicircle law , and has since found numerous applications in the theory of random matrices . 3  Notes  "  Category:Probability theory     ↩  ↩  ↩     