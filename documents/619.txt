   Variance-based sensitivity analysis      Variance-based sensitivity analysis  '''Variance-based sensitivity analysis''' is a form of global [[sensitivity analysis]]. Saltelli, A., Ratto, M., Andres, T., Campolongo, F., Cariboni, J., Gatelli, D. Saisana, M., and Tarantola, S., 2008, ''Global Sensitivity Analysis. The Primer'', John Wiley & Sons. Working within a probabilistic framework, it decomposes the variance of the output of the model or system into fractions which can be attributed to inputs or sets of inputs. For example, given a model with two inputs and one output, one might find that 70% of the output variance is caused by the variance in the first input, 20% by the variance in the second, and 10% due to Interactions between the two. These percentages are directly interpreted as measures of sensitivity. Variance-based measures of sensitivity are attractive because they measure sensitivity across the whole input space (i.e. it is a global method), they can deal with nonlinear responses, and they can measure the effect of interactions in non- additive systems. 1  Decomposition of Variance  From a black box perspective, any model may be viewed as a function Y = f ( X ), where X is a vector of d uncertain model inputs { X 1 , X 2 , ... X d }, and Y is a chosen univariate model output (note that this approach examines scalar model outputs, but multiple outputs can be analysed by multiple independent sensitivity analyses). Furthermore, it will be assumed that the inputs are independently and uniformly distributed within the unit hypercube, i.e.     X  i   ∈   [  0  ,  1  ]        subscript  X  i    0  1     X_{i}\in[0,1]   for    i  =   1  ,  2  ,  …  ,  d       i   1  2  normal-…  d     i=1,2,...,d   . This incurs no loss of generality because any input space can be transformed onto this unit hypercube. f ( X ) may be decomposed in the following way, 2       f  0   =   E   (  Y  )         subscript  f  0     E  Y     f_{0}=E(Y)     i.e. all the terms in the functional decomposition are orthogonal . This leads to definitions of the terms of the functional decomposition in terms of conditional expected values,       f  i    (   X  i   )   =  E   (  Y  |   X  i   )   -   f  0      fragments   subscript  f  i    fragments  normal-(   subscript  X  i   normal-)    E   fragments  normal-(  Y  normal-|   subscript  X  i   normal-)     subscript  f  0     f_{i}(X_{i})=E(Y|X_{i})-f_{0}          f   i  j     (   X  i   ,   X  j   )   =  E   (  Y  |   X  i   ,   X  j   )   -   f  0   -   f  i   -   f  j      fragments   subscript  f    i  j     fragments  normal-(   subscript  X  i   normal-,   subscript  X  j   normal-)    E   fragments  normal-(  Y  normal-|   subscript  X  i   normal-,   subscript  X  j   normal-)     subscript  f  0     subscript  f  i     subscript  f  j     f_{ij}(X_{i},X_{j})=E(Y|X_{i},X_{j})-f_{0}-f_{i}-f_{j}          V   i  j    =   Var   X   i  j      (   E   𝐗    ∼   i  j       (  Y  ∣   X   i  j    )   )      fragments   subscript  V    i  j      subscript  Var   subscript  X    i  j      fragments  normal-(   subscript  E   subscript  X   similar-to  absent    i  j       fragments  normal-(  Y  normal-∣   subscript  X    i  j    normal-)   normal-)     V_{ij}=\operatorname{Var}_{X_{ij}}\left(E_{\textbf{X}_{\sim ij}}\left(Y\mid X_%
 {ij}\right)\right)     From which it can be seen that f i is the effect of varying X i alone (known as the main effect of X i ), and f ij is the effect of varying X i and X j simultaneously, additional to the effect of their individual variations . This is known as a second-order interaction . Higher-order terms have analogous definitions.  Now, further assuming that the f ( X ) is square-integrable , the functional decomposition may be squared and integrated to give,       S  i   =    V  i    Var   (  Y  )          subscript  S  i      subscript  V  i    Var  Y      S_{i}=\frac{V_{i}}{\operatorname{Var}(Y)}   ,        ∑   i  =  1   d    S   T  i     ≥  1        superscript   subscript     i  1    d    subscript  S    T  i     1    \sum_{i=1}^{d}S_{Ti}\geq 1     and so on. The X ~ i notation indicates the set of all variables except  X i . The above variance decomposition shows how the variance of the model output can be decomposed into terms attributable to each input, as well as the interaction effects between them. Together, all terms sum to the total variance of the model output.  First-order indices  A direct variance-based measure of sensitivity S i , called the "first-order sensitivity index", or "main effect index" is stated as follows, 3       Var   X  i     (   E   𝐗    ∼  i      (  Y  |   X  i   )   )   ≈   1  N    ∑   j  =  1   N   f    (  𝐁  )   j    (  f    (   𝐀  B  i   )   j   -  f    (  𝐀  )   j   )      fragments   subscript  Var   subscript  X  i     fragments  normal-(   subscript  E   subscript  𝐗   similar-to  absent  i      fragments  normal-(  Y  normal-|   subscript  X  i   normal-)   normal-)      1  N    superscript   subscript     j  1    N   f   subscript   fragments  normal-(  B  normal-)   j    fragments  normal-(  f   subscript   fragments  normal-(   subscript   superscript  𝐀  i   B   normal-)   j    f   subscript   fragments  normal-(  A  normal-)   j   normal-)     \operatorname{Var}_{X_{i}}(E_{\mathbf{X}_{\sim i}}(Y|X_{i}))\approx{\frac{1}{N%
 }\sum_{j=1}^{N}f\left(\mathbf{B}\right)_{j}\left(f\left(\mathbf{A}^{i}_{B}%
 \right)_{j}-f\left(\mathbf{A}\right)_{j}\right)}     This is the contribution to the output variance of the main effect of X i , therefore it measures the effect of varying X i  alone , but averaged over variations in other input parameters. It is standardised by the total variance to provide a fractional contribution. Higher-order interaction indices S ij , S ijk and so on can be formed by dividing other terms in the variance decomposition by Var( Y ). Note that this has the implication that,        E   𝐗    ∼  i      (    Var   X  i     (  Y  ∣   𝐗    ∼  i    )    )    ≈    1   2  N      ∑   j  =  1   N     (    f    (  𝐀  )   j    -   f    (   𝐀  B  i   )   j     )   2            subscript  E   subscript  𝐗   similar-to  absent  i       subscript  Var   subscript  X  i    Y   subscript  𝐗   similar-to  absent  i          1    2  N      superscript   subscript     j  1    N    superscript      f   subscript  𝐀  j      f   subscript   subscript   superscript  𝐀  i   B   j     2       E_{\mathbf{X}_{\sim i}}\left(\operatorname{Var}_{X_{i}}\left(Y\mid\mathbf{X}_{%
 \sim i}\right)\right)\approx{\frac{1}{2N}\sum_{j=1}^{N}\left(f\left(\mathbf{A}%
 \right)_{j}-f\left(\mathbf{A}^{i}_{B}\right)_{j}\right)^{2}}     Note that unlike the S i ,  $$\sum_{i=1}^d S_{Ti} \geq 1$$  due to the fact that the interaction effect between e.g. X i and X j is counted in both S Ti  and  S Tj In fact, the sum of the S Ti will only be equal to 1 when the model is purely additive .  Calculation of indices  For analytically tractable functions, the indices above may be calculated analytically by evaluating the integrals in the decomposition. However, in the vast majority of cases they are estimated - this is usually done by the Monte Carlo method .  Sampling sequences  thumb | right | 500px | An example of construction of A B i matrices with d =3 and N =4. The Monte Carlo approach involves generating a sequence of randomly distributed points inside the unit hypercube (strictly speaking these will be pseudorandom ). In practice, it is common to substitute random sequences with low-discrepancy sequences to improve the efficiency of the estimators. This is then known as the Quasi-Monte Carlo method . Some low-discrepancy sequences commonly used in sensitivity analysis include the Sobol sequence and the Latin hypercube design.  Procedure  To calculate the indices using the (Quasi) Monte Carlo method, the following steps are used: 4   Generate an N x2 d sample matrix, i.e. each row is a sample point in the hyperspace of 2 d dimensions. This should be done with respect to the probability distributions of the input variables.  Use the first d columns of the matrix as matrix A , and the remaining d columns as matrix B . This effectively gives two independent samples of N points in the d -dimensional unit hypercube.  Build d further N x d matrices A B i , for i = 1,2,...,d, such that the i th column of A B i is equal to the i th column of B , and the remaining columns are from A .  The A , B , and the d  A B i matrices in total specify N ( d +2) points in the input space (one for each row). Run the model at each design point in the A , B , and A B i matrices, giving a total of N ( d +2) model evaluations - the corresponding f( A ), f( B ) and f( A B i ) values.  Calculate the sensitivity indices using the estimators below.   The accuracy of the estimators is of course dependent on N . The value of N can be chosen by sequentially adding points and calculating the indices until the estimated values reach some acceptable convergence. For this reason, when using low-discrepancy sequences, it can be advantageous to use those that allow sequential addition of points (such as the Sobol sequence), as compared to those that do not (such as Latin hypercube sequences).  Estimators  There are a number of possible Monte Carlo estimators available for both indices. Two that are currently in general use are, 5  $$\operatorname{Var}_{X_i}(E_{\mathbf{X}_{\sim i}}(Y|X_i)) \approx
   { \frac {1}{N}  \sum_{j=1}^{N}  f \left (  \mathbf{B} \right )_{j}  \left ( f   \left ( \mathbf{A}^i_B \right )_{j} -   f \left ( \mathbf{A} \right )_{j}  \right ) }$$  and  $$E_{\mathbf{X}_{\sim i}}\left(\operatorname{Var}_{X_i}\left(Y \mid \mathbf{X}_{\sim i} \right) \right) \approx {\frac {1}{2N}\sum_{j=1}^{N}
 \left( f   \left ( \mathbf{A} \right )_{j} - f  \left (\mathbf{A}^i_B \right )_{j}\right )^2
 }$$  for the estimation of for estimation of the S i and the S Ti respectively.  Computational Expense  For the estimation of the S i and the S Ti for all input variables, N ( d +2) model runs are required. Since N is often of the order of hundreds or thousands of runs, computational expense can quickly become a problem when the model takes a significant amount of time for a single run. In such cases, there are a number of techniques available to reduce the computational cost of estimating sensitivity indices, such as emulators , HDMR and FAST .  See also   Sensitivity analysis  Monte Carlo method  Quasi-Monte Carlo method  Sobol sequence   References  "  Category:Scientific modeling       Saltelli,  A.,  Annoni,  P.,  2010,  How  to  avoid  a  perfunctory  sensitivity  analysis,  Environmental  Modeling  and  Software  25 ,  1508-1517. ↩  Sobol’, I. (1990). Sensitivity estimates for nonlinear mathematical models. Matematicheskoe Modelirovanie  2 , 112–118. in Russian, translated in English in Sobol’ , I. (1993). Sensitivity analysis for non-linear mathematical models. Mathematical Modeling & Computational Experiment (Engl. Transl.) , 1993, 1 , 407–414. ↩    Andrea Saltelli, Paola Annoni, Ivano Azzini, Francesca Campolongo, Marco Ratto, and Stefano Tarantola. Variance based sensitivity analysis of model output. Design and estimator for the total sensitivity index. Computer Physics Communications, 181(2):259{270, 2010 ↩     