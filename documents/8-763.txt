   Finite difference method      Finite difference method   In mathematics , finite-difference methods (FDM) are numerical methods for solving differential equations by approximating them with difference equations , in which finite differences approximate the derivatives . FDMs are thus discretization methods.  Today, FDMs are the dominant approach to numerical solutions of partial differential equations . 1  Derivation from Taylor's polynomial  First, assuming the function whose derivatives are to be approximated is properly-behaved, by Taylor's theorem , we can create a Taylor Series expansion        f   (    x  0   +  h   )    =    f   (   x  0   )    +      f  ′    (   x  0   )     1  !    h   +      f   (  2  )     (   x  0   )     2  !     h  2    +  ⋯  +      f   (  n  )     (   x  0   )     n  !     h  n    +    R  n    (  x  )      ,        f     subscript  x  0   h        f   subscript  x  0           superscript  f  normal-′    subscript  x  0      1    h          superscript  f  2    subscript  x  0      2     superscript  h  2    normal-⋯         superscript  f  n    subscript  x  0      n     superscript  h  n       subscript  R  n   x      f(x_{0}+h)=f(x_{0})+\frac{f^{\prime}(x_{0})}{1!}h+\frac{f^{(2)}(x_{0})}{2!}h^{%
 2}+\cdots+\frac{f^{(n)}(x_{0})}{n!}h^{n}+R_{n}(x),     where n ! denotes the factorial of n , and R n ( x ) is a remainder term, denoting the difference between the Taylor polynomial of degree n and the original function. We will derive an approximation for the first derivative of the function "f" by first truncating the Taylor polynomial:        f   (    x  0   +  h   )    =    f   (   x  0   )    +    f  ′    (   x  0   )   h   +    R  1    (  x  )      ,        f     subscript  x  0   h        f   subscript  x  0       superscript  f  normal-′    subscript  x  0   h      subscript  R  1   x      f(x_{0}+h)=f(x_{0})+f^{\prime}(x_{0})h+R_{1}(x),     Setting, x 0 =a we have,        f   (   a  +  h   )    =    f   (  a  )    +    f  ′    (  a  )   h   +    R  1    (  x  )      ,        f    a  h        f  a      superscript  f  normal-′   a  h      subscript  R  1   x      f(a+h)=f(a)+f^{\prime}(a)h+R_{1}(x),     Dividing across by h gives:        f   (   a  +  h   )    h   =     f   (  a  )    h   +    f  ′    (  a  )    +     R  1    (  x  )    h            f    a  h    h         f  a   h      superscript  f  normal-′   a        subscript  R  1   x   h      {f(a+h)\over h}={f(a)\over h}+f^{\prime}(a)+{R_{1}(x)\over h}     Solving for f'(a):        f  ′    (  a  )    =      f   (   a  +  h   )    -   f   (  a  )     h   -     R  1    (  x  )    h           superscript  f  normal-′   a           f    a  h      f  a    h        subscript  R  1   x   h      f^{\prime}(a)={f(a+h)-f(a)\over h}-{R_{1}(x)\over h}     Assuming that     R  1    (  x  )        subscript  R  1   x    R_{1}(x)   is sufficiently small, the approximation of the first derivative of "f" is:         f  ′    (  a  )    ≈     f   (   a  +  h   )    -   f   (  a  )     h    .         superscript  f  normal-′   a         f    a  h      f  a    h     f^{\prime}(a)\approx{f(a+h)-f(a)\over h}.     Accuracy and order  The error in a method's solution is defined as the difference between its approximation and the exact analytical solution. The two sources of error in finite difference methods are round-off error , the loss of precision due to computer rounding of decimal quantities, and truncation error or discretization error , the difference between the exact solution of the finite difference equation and the exact quantity assuming perfect arithmetic (that is, assuming no round-off).  To use a finite difference method to approximate the solution to a problem, one must first discretize the problem's domain. This is usually done by dividing the domain into a uniform grid (see image to the right). Note that this means that finite-difference methods produce sets of discrete numerical approximations to the derivative, often in a "time-stepping" manner.  An expression of general interest is the local truncation error of a method. Typically expressed using Big-O notation , local truncation error refers to the error from a single application of a method. That is, it is the quantity      f  ′    (   x  i   )    -   f  i  ′          superscript  f  normal-′    subscript  x  i     subscript   superscript  f  normal-′   i     f^{\prime}(x_{i})-f^{\prime}_{i}   if     f  ′    (   x  i   )        superscript  f  normal-′    subscript  x  i     f^{\prime}(x_{i})   refers to the exact value and    f  i  ′     subscript   superscript  f  normal-′   i    f^{\prime}_{i}   to the numerical approximation. The remainder term of a Taylor polynomial is convenient for analyzing the local truncation error. Using the Lagrange form of the remainder from the Taylor polynomial for    f   (    x  0   +  h   )       f     subscript  x  0   h     f(x_{0}+h)   , which is        R  n    (    x  0   +  h   )    =      f   (   n  +  1   )     (  ξ  )      (   n  +  1   )   !      (  h  )    n  +  1            subscript  R  n      subscript  x  0   h           superscript  f    n  1    ξ       n  1      superscript  h    n  1       R_{n}(x_{0}+h)=\frac{f^{(n+1)}(\xi)}{(n+1)!}(h)^{n+1}   , where     x  0   <  ξ  <    x  0   +  h          subscript  x  0   ξ          subscript  x  0   h      x_{0}<\xi   ,  the dominant term of the local truncation error can be discovered. For example, again using the forward-difference formula for the first derivative, knowing that     f   (   x  i   )    =   f   (    x  0   +   i  h    )          f   subscript  x  i      f     subscript  x  0     i  h       f(x_{i})=f(x_{0}+ih)   ,        f   (    x  0   +   i  h    )    =    f   (   x  0   )    +    f  ′    (   x  0   )   i  h   +      f  ′′    (  ξ  )     2  !      (   i  h   )   2      ,        f     subscript  x  0     i  h         f   subscript  x  0       superscript  f  normal-′    subscript  x  0   i  h          superscript  f  ′′   ξ     2     superscript    i  h   2       f(x_{0}+ih)=f(x_{0})+f^{\prime}(x_{0})ih+\frac{f^{\prime\prime}(\xi)}{2!}(ih)^%
 {2},     and with some algebraic manipulation, this leads to          f   (    x  0   +   i  h    )    -   f   (   x  0   )      i  h    =     f  ′    (   x  0   )    +      f  ′′    (  ξ  )     2  !    i  h     ,            f     subscript  x  0     i  h       f   subscript  x  0       i  h         superscript  f  normal-′    subscript  x  0           superscript  f  ′′   ξ     2    i  h      \frac{f(x_{0}+ih)-f(x_{0})}{ih}=f^{\prime}(x_{0})+\frac{f^{\prime\prime}(\xi)}%
 {2!}ih,     and further noting that the quantity on the left is the approximation from the finite difference method and that the quantity on the right is the exact quantity of interest plus a remainder, clearly that remainder is the local truncation error. A final expression of this example and its order is:          f   (    x  0   +   i  h    )    -   f   (   x  0   )      i  h    =     f  ′    (   x  0   )    +   O   (  h  )      .            f     subscript  x  0     i  h       f   subscript  x  0       i  h         superscript  f  normal-′    subscript  x  0      O  h      \frac{f(x_{0}+ih)-f(x_{0})}{ih}=f^{\prime}(x_{0})+O(h).     This means that, in this case, the local truncation error is proportional to the step sizes. The quality and duration of simulated FDM solution depends on the discretization equation selection and the step sizes (time and space steps). The data quality and simulation duration increase significantly with smaller step size. 2 Therefore, a reasonable balance between data quality and simulation duration is necessary for practical usage. Large time steps are favourable to increase simulation speed in many practice, however too large time steps may create instabilities and affecting the data quality. 3 4 5  The von Neumann method ( Fourier stability analysis) usually applied to determine the numerical model stability. 6 7 8 9 10  Example: ordinary differential equation  For example, consider the ordinary differential equation        u  ′    (  x  )    =    3  u   (  x  )    +   2.           superscript  u  normal-′   x       3  u  x   2.     u^{\prime}(x)=3u(x)+2.\,   The Euler method for solving this equation uses the finite difference quotient         u   (   x  +  h   )    -   u   (  x  )     h   ≈    u  ′    (  x  )              u    x  h      u  x    h      superscript  u  normal-′   x     \frac{u(x+h)-u(x)}{h}\approx u^{\prime}(x)   to approximate the differential equation by first substituting in for u'(x) then applying a little algebra (multiplying both sides by h, and then adding u(x) to both sides) to get        u   (   x  +  h   )    =    u   (  x  )    +   h   (    3  u   (  x  )    +  2   )      .        u    x  h        u  x     h      3  u  x   2       u(x+h)=u(x)+h(3u(x)+2).\,   The last equation is a finite-difference equation, and solving this equation gives an approximate solution to the differential equation.  Example: The heat equation  Consider the normalized heat equation in one dimension, with homogeneous Dirichlet boundary conditions       U  t   =    U   x  x          subscript  U  t    subscript  U    x  x      U_{t}=U_{xx}\,          U   (  0  ,  t  )    =   U   (  1  ,  t  )    =   0           U   0  t      U   1  t         0     U(0,t)=U(1,t)=0\,   (boundary condition)       U   (  x  ,  0  )    =    U  0    (  x  )          U   x  0       subscript  U  0   x     U(x,0)=U_{0}(x)\,   (initial condition)  One way to numerically solve this equation is to approximate all the derivatives by finite differences. We partition the domain in space using a mesh     x  0   ,  …  ,   x  J       subscript  x  0   normal-…   subscript  x  J     x_{0},...,x_{J}   and in time using a mesh     t  0   ,  …  .  ,   t  N      fragments   subscript  t  0   normal-,  normal-…  normal-.  normal-,   subscript  t  N     t_{0},....,t_{N}   . We assume a uniform partition both in space and in time, so the difference between two consecutive space points will be h and between two consecutive time points will be k . The points       u   (   x  j   ,   t  n   )    =   u  j  n         u    subscript  x  j    subscript  t  n      superscript   subscript  u  j   n     u(x_{j},t_{n})=u_{j}^{n}     will represent the numerical approximation of     u   (   x  j   ,   t  n   )    .      u    subscript  x  j    subscript  t  n      u(x_{j},t_{n}).     Explicit method  Using a forward difference  at time    t  n     subscript  t  n    t_{n}    and a second-order central difference for the space derivative at position    x  j     subscript  x  j    x_{j}   ( FTCS ) we get the recurrence equation:          u  j   n  +  1    -   u  j  n    k   =      u   j  +  1   n   -   2   u  j  n     +   u   j  -  1   n     h  2     .           superscript   subscript  u  j     n  1     superscript   subscript  u  j   n    k          superscript   subscript  u    j  1    n     2   superscript   subscript  u  j   n      superscript   subscript  u    j  1    n     superscript  h  2      \frac{u_{j}^{n+1}-u_{j}^{n}}{k}=\frac{u_{j+1}^{n}-2u_{j}^{n}+u_{j-1}^{n}}{h^{2%
 }}.\,     This is an explicit method for solving the one-dimensional heat equation .  We can obtain    u  j   n  +  1      superscript   subscript  u  j     n  1     u_{j}^{n+1}   from the other values this way:       u  j   n  +  1    =     (   1  -   2  r    )    u  j  n    +   r   u   j  -  1   n    +   r   u   j  +  1   n          superscript   subscript  u  j     n  1          1    2  r     superscript   subscript  u  j   n      r   superscript   subscript  u    j  1    n      r   superscript   subscript  u    j  1    n       u_{j}^{n+1}=(1-2r)u_{j}^{n}+ru_{j-1}^{n}+ru_{j+1}^{n}     where     r  =   k  /   h  2     .      r    k   superscript  h  2      r=k/h^{2}.     So, with this recurrence relation, and knowing the values at time n , one can obtain the corresponding values at time n +1.    u  0  n     superscript   subscript  u  0   n    u_{0}^{n}   and    u  J  n     superscript   subscript  u  J   n    u_{J}^{n}   must be replaced by the boundary conditions, in this example they are both 0.  This explicit method is known to be numerically stable and convergent whenever    r  ≤   1  /  2       r    1  2     r\leq 1/2   . 11 The numerical errors are proportional to the time step and the square of the space step:       Δ  u   =    O   (  k  )    +   O   (   h  2   )           normal-Δ  u       O  k     O   superscript  h  2       \Delta u=O(k)+O(h^{2})\,     Implicit method  If we use the backward difference  at time    t   n  +  1      subscript  t    n  1     t_{n+1}    and a second-order central difference for the space derivative at position    x  j     subscript  x  j    x_{j}   (The Backward Time, Centered Space Method "BTCS") we get the recurrence equation:          u  j   n  +  1    -   u  j  n    k   =      u   j  +  1    n  +  1    -   2   u  j   n  +  1      +   u   j  -  1    n  +  1      h  2     .           superscript   subscript  u  j     n  1     superscript   subscript  u  j   n    k          superscript   subscript  u    j  1      n  1      2   superscript   subscript  u  j     n  1       superscript   subscript  u    j  1      n  1      superscript  h  2      \frac{u_{j}^{n+1}-u_{j}^{n}}{k}=\frac{u_{j+1}^{n+1}-2u_{j}^{n+1}+u_{j-1}^{n+1}%
 }{h^{2}}.\,     This is an implicit method for solving the one-dimensional heat equation .  We can obtain    u  j   n  +  1      superscript   subscript  u  j     n  1     u_{j}^{n+1}   from solving a system of linear equations:         (   1  +   2  r    )    u  j   n  +  1     -   r   u   j  -  1    n  +  1     -   r   u   j  +  1    n  +  1      =   u  j  n             1    2  r     superscript   subscript  u  j     n  1       r   superscript   subscript  u    j  1      n  1       r   superscript   subscript  u    j  1      n  1       superscript   subscript  u  j   n     (1+2r)u_{j}^{n+1}-ru_{j-1}^{n+1}-ru_{j+1}^{n+1}=u_{j}^{n}     The scheme is always numerically stable and convergent but usually more numerically intensive than the explicit method as it requires solving a system of numerical equations on each time step. The errors are linear over the time step and quadratic over the space step:        Δ  u   =    O   (  k  )    +   O   (   h  2   )      .        normal-Δ  u       O  k     O   superscript  h  2       \Delta u=O(k)+O(h^{2}).\,     Crank–Nicolson method  Finally if we use the central difference at time    t   n  +   1  /  2       subscript  t    n    1  2      t_{n+1/2}   and a second-order central difference for the space derivative at position    x  j     subscript  x  j    x_{j}   ("CTCS") we get the recurrence equation:          u  j   n  +  1    -   u  j  n    k   =    1  2    (       u   j  +  1    n  +  1    -   2   u  j   n  +  1      +   u   j  -  1    n  +  1      h  2    +      u   j  +  1   n   -   2   u  j  n     +   u   j  -  1   n     h  2     )     .           superscript   subscript  u  j     n  1     superscript   subscript  u  j   n    k       1  2            superscript   subscript  u    j  1      n  1      2   superscript   subscript  u  j     n  1       superscript   subscript  u    j  1      n  1      superscript  h  2           superscript   subscript  u    j  1    n     2   superscript   subscript  u  j   n      superscript   subscript  u    j  1    n     superscript  h  2        \frac{u_{j}^{n+1}-u_{j}^{n}}{k}=\frac{1}{2}\left(\frac{u_{j+1}^{n+1}-2u_{j}^{n%
 +1}+u_{j-1}^{n+1}}{h^{2}}+\frac{u_{j+1}^{n}-2u_{j}^{n}+u_{j-1}^{n}}{h^{2}}%
 \right).\,     This formula is known as the Crank–Nicolson method .  We can obtain    u  j   n  +  1      superscript   subscript  u  j     n  1     u_{j}^{n+1}   from solving a system of linear equations:         (   2  +   2  r    )    u  j   n  +  1     -   r   u   j  -  1    n  +  1     -   r   u   j  +  1    n  +  1      =     (   2  -   2  r    )    u  j  n    +   r   u   j  -  1   n    +   r   u   j  +  1   n               2    2  r     superscript   subscript  u  j     n  1       r   superscript   subscript  u    j  1      n  1       r   superscript   subscript  u    j  1      n  1            2    2  r     superscript   subscript  u  j   n      r   superscript   subscript  u    j  1    n      r   superscript   subscript  u    j  1    n       (2+2r)u_{j}^{n+1}-ru_{j-1}^{n+1}-ru_{j+1}^{n+1}=(2-2r)u_{j}^{n}+ru_{j-1}^{n}+%
 ru_{j+1}^{n}     The scheme is always numerically stable and convergent but usually more numerically intensive as it requires solving a system of numerical equations on each time step. The errors are quadratic over both the time step and the space step:        Δ  u   =    O   (   k  2   )    +   O   (   h  2   )      .        normal-Δ  u       O   superscript  k  2      O   superscript  h  2       \Delta u=O(k^{2})+O(h^{2}).\,     Usually the Crank–Nicolson scheme is the most accurate scheme for small time steps. The explicit scheme is the least accurate and can be unstable, but is also the easiest to implement and the least numerically intensive. The implicit scheme works the best for large time steps.  See also   Finite element method  Finite difference  Finite difference time domain  Stencil (numerical analysis)  Finite difference coefficients  Five-point stencil  Lax–Richtmyer theorem  Finite difference methods for option pricing  Upwind differencing scheme for convection  Central differencing scheme   References   K.W. Morton and D.F. Mayers, Numerical Solution of Partial Differential Equations, An Introduction . Cambridge University Press, 2005.  Autar Kaw and E. Eric Kalu, Numerical Methods with Applications , (2008) 1 . Contains a brief, engineering-oriented introduction to FDM (for ODEs) in Chapter 08.07 .     .  Randall J. LeVeque , Finite Difference Methods for Ordinary and Partial Differential Equations , SIAM, 2007.   External links   List of Internet Resources for the Finite Difference Method for PDEs   Various lectures and lecture notes   Finite-Difference Method in Electromagnetics (see and listen to lecture 9)  Lecture Notes Shih-Hung Chen, National Central University  Finite Difference Method for Boundary Value Problems  Numerical Methods for time-dependent Partial Differential Equations   "  Category:Finite differences  Category:Numerical differential equations     ↩  ↩  ↩  ↩  ↩     ↩  ↩  Crank, J. The Mathematics of Diffusion . 2nd Edition, Oxford, 1975, p. 143. ↩     