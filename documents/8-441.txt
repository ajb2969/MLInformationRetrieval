   Sensitivity and specificity      Sensitivity and specificity   Sensitivity and specificity are statistical measures of the performance of a binary classification  test , also known in statistics as classification function :   Sensitivity (also called the true positive rate , or the recall in some fields) measures the proportion of positives that are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition).  Specificity (also called the true negative rate ) measures the proportion of negatives that are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition).   For any test, there is usually a trade-off between the measures. For instance, in an airport security setting in which one is testing for potential threats to safety, scanners may be set to trigger on low-risk items like belt buckles and keys (low specificity), in order to reduce the risk of missing objects that do pose a threat to the aircraft and those aboard (high sensitivity). This trade-off can be represented graphically as a receiver operating characteristic curve.  A perfect predictor would be described as 100% sensitive (e.g., all sick are identified as sick) and 100% specific (e.g., all healthy are not identified as sick); however, theoretically any predictor will possess a minimum error bound known as the Bayes error rate .  Definitions      Terminology and derivations from a confusion matrix     true positive (TP)  eqv. with hit  true negative (TN)  eqv. with correct rejection  false positive (FP)  eqv. with false alarm , Type I error   false negative (FN)  eqv. with miss, Type II error      sensitivity or true positive rate (TPR)  eqv. with hit rate , recall       𝑇𝑃𝑅  =   𝑇𝑃  /  P   =   𝑇𝑃  /   (   𝑇𝑃  +  𝐹𝑁   )          𝑇𝑃𝑅    𝑇𝑃  P          𝑇𝑃    𝑇𝑃  𝐹𝑁       \mathit{TPR}=\mathit{TP}/P=\mathit{TP}/(\mathit{TP}+\mathit{FN})     specificity (SPC) or true negative rate      𝑆𝑃𝐶  =   𝑇𝑁  /  N   =   𝑇𝑁  /   (   𝑇𝑁  +  𝐹𝑃   )          𝑆𝑃𝐶    𝑇𝑁  N          𝑇𝑁    𝑇𝑁  𝐹𝑃       \mathit{SPC}=\mathit{TN}/N=\mathit{TN}/(\mathit{TN}+\mathit{FP})     precision or positive predictive value (PPV)      𝑃𝑃𝑉  =   𝑇𝑃  /   (   𝑇𝑃  +  𝐹𝑃   )        𝑃𝑃𝑉    𝑇𝑃    𝑇𝑃  𝐹𝑃      \mathit{PPV}=\mathit{TP}/(\mathit{TP}+\mathit{FP})     negative predictive value (NPV)      𝑁𝑃𝑉  =   𝑇𝑁  /   (   𝑇𝑁  +  𝐹𝑁   )        𝑁𝑃𝑉    𝑇𝑁    𝑇𝑁  𝐹𝑁      \mathit{NPV}=\mathit{TN}/(\mathit{TN}+\mathit{FN})     fall-out or false positive rate (FPR)      𝐹𝑃𝑅  =   𝐹𝑃  /  N   =   𝐹𝑃  /   (   𝐹𝑃  +  𝑇𝑁   )    =   1  -  𝑆𝑃𝐶         𝐹𝑃𝑅    𝐹𝑃  N          𝐹𝑃    𝐹𝑃  𝑇𝑁           1  𝑆𝑃𝐶      \mathit{FPR}=\mathit{FP}/N=\mathit{FP}/(\mathit{FP}+\mathit{TN})=1-\mathit{SPC}     false negative rate (FNR)      𝐹𝑁𝑅  =   𝐹𝑁  /   (   𝑇𝑃  +  𝐹𝑁   )    =   1  -  𝑇𝑃𝑅         𝐹𝑁𝑅    𝐹𝑁    𝑇𝑃  𝐹𝑁           1  𝑇𝑃𝑅      \mathit{FNR}=\mathit{FN}/(\mathit{TP}+\mathit{FN})=1-\mathit{TPR}     false discovery rate (FDR)      𝐹𝐷𝑅  =   𝐹𝑃  /   (   𝑇𝑃  +  𝐹𝑃   )    =   1  -  𝑃𝑃𝑉         𝐹𝐷𝑅    𝐹𝑃    𝑇𝑃  𝐹𝑃           1  𝑃𝑃𝑉      \mathit{FDR}=\mathit{FP}/(\mathit{TP}+\mathit{FP})=1-\mathit{PPV}        accuracy (ACC)      𝐴𝐶𝐶  =    (   𝑇𝑃  +  𝑇𝑁   )   /   (   𝑇𝑃  +  𝐹𝑃  +  𝐹𝑁  +  𝑇𝑁   )        𝐴𝐶𝐶      𝑇𝑃  𝑇𝑁     𝑇𝑃  𝐹𝑃  𝐹𝑁  𝑇𝑁      \mathit{ACC}=(\mathit{TP}+\mathit{TN})/(\mathit{TP}+\mathit{FP}+\mathit{FN}+%
 \mathit{TN})     F1 score  is the harmonic mean of precision and sensitivity       F1  =    2  𝑇𝑃   /   (    2  𝑇𝑃   +  𝐹𝑃  +  𝐹𝑁   )        italic-F1      2  𝑇𝑃       2  𝑇𝑃   𝐹𝑃  𝐹𝑁      \mathit{F1}=2\mathit{TP}/(2\mathit{TP}+\mathit{FP}+\mathit{FN})     Matthews correlation coefficient (MCC)  \frac{ \mathit{TP} \times \mathit{TN} - \mathit{FP} \times \mathit{FN} } {\sqrt{ (\mathit{TP}+\mathit{FP}) ( \mathit{TP} + \mathit{FN} ) ( \mathit{TN} + \mathit{FP} ) ( \mathit{TN} + \mathit{FN} ) } }     Informedness       𝑇𝑃𝑅  +  𝑆𝑃𝐶   -  1        𝑇𝑃𝑅  𝑆𝑃𝐶   1    \mathit{TPR}+\mathit{SPC}-1     Markedness       𝑃𝑃𝑉  +  𝑁𝑃𝑉   -  1        𝑃𝑃𝑉  𝑁𝑃𝑉   1    \mathit{PPV}+\mathit{NPV}-1      ; Sources: Fawcett (2006) and Powers (2011). 1 2     Imagine a study evaluating a new test that screens people for a disease. Each person taking the test either has or does not have the disease. The test outcome can be positive (predicting that the person has the disease) or negative (predicting that the person does not have the disease). The test results for each subject may or may not match the subject's actual status. In that setting:   True positive: Sick people correctly diagnosed as sick  False positive: Healthy people incorrectly identified as sick  True negative: Healthy people correctly identified as healthy  False negative: Sick people incorrectly identified as healthy   In general, Positive = identified and negative = rejected. Therefore:   True positive = correctly identified  False positive = incorrectly identified  True negative = correctly rejected  False negative = incorrectly rejected   Let us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 contingency table or confusion matrix , as follows:  Sensitivity  Sensitivity relates to the test's ability to correctly detect patients who do have a condition. Consider the example of a medical test used to identify a disease. Sensitivity of the test is the proportion of people known to have the disease, who test positive for it. Mathematically, this can be expressed as:       =    number of true positives   number of true positives  +  number of false negatives         absent    number of true positives    number of true positives  number of false negatives      \displaystyle=\frac{\text{number of true positives}}{\text{number of true %
 positives}+\text{number of false negatives}}     A negative result in a test with high sensitivity is useful for ruling out disease. A high sensitivity test is reliable when its result is negative, since it rarely misdiagnoses those who have the disease. A test with 100% sensitivity will recognize all patients with the disease by testing positive. A negative test result would definitively rule out presence of the disease in a patient.  A positive result in a test with high sensitivity is not useful for ruling in disease. Suppose a 'bogus' test kit is designed to show only one reading, positive. When used on diseased patients, all patients test positive, giving the test 100% sensitivity. However, sensitivity by definition does not take into account false positives. The bogus test also returns positive on all healthy patients, giving it a false positive rate of 100%, rendering it useless for diagnosing or "ruling in" the disease.  Sensitivity is not the same as the precision or positive predictive value (ratio of true positives to combined true and false positives), which is as much a statement about the proportion of actual positives in the population being tested as it is about the test.  The calculation of sensitivity does not take into account indeterminate test results. If a test cannot be repeated, indeterminate samples either should be excluded from the analysis (the number of exclusions should be stated when quoting sensitivity) or can be treated as false negatives (which gives the worst-case value for sensitivity and may therefore underestimate it).  A test with high sensitivity has a low type II error rate. In non-medical contexts, sensitivity is sometimes called recall .  Specificity  Specificity relates to the test's ability to correctly detect patients without a condition. Consider the example of a medical test for diagnosing a disease. Specificity of a test is the proportion of healthy patients known not to have the disease, who will test negative for it. Mathematically, this can also be written as:       =    number of true negatives   number of true negatives  +  number of false positives         absent    number of true negatives    number of true negatives  number of false positives      \displaystyle=\frac{\text{number of true negatives}}{\text{number of true %
 negatives}+\text{number of false positives}}     Positive result in a test with high specificity is useful for ruling in disease. The test rarely gives positive results in healthy patients. A test with 100% specificity will read negative, and accurately exclude disease from all healthy patients. A positive result will highlight a high probability of the presence of disease. 3  Negative result in a test with high specificity is not useful for ruling out disease. Assume a 'bogus' test is designed to read only negative. This is administered to healthy patients, and reads negative on all of them. This will give the test a specificity of 100%. Specificity by definition does not take into account false negatives. The same test will also read negative on diseased patients, therefore it has a false negative rate of 100%, and will be useless for ruling out disease.  A test with a high specificity has a low type I error rate.  Graphical illustration  File:HighSensitivity LowSpecificity 1401x1050.png|High sensitivity and low specificity File:LowSensitivity HighSpecificity 1400x1050.png|Low sensitivity and high specificity  Medical examples  In medical diagnosis , test sensitivity is the ability of a test to correctly identify those with the disease (true positive rate), whereas test specificity is the ability of the test to correctly identify those without the disease (true negative rate). If 100 patients known to have a disease were tested, and 43 test positive, then the test has 43% sensitivity. If 100 with no disease are tested and 96 return a negative result, then the test has 96% specificity. Sensitivity and specificity are prevalence-independent test characteristics, as their values are intrinsic to the test and do not depend on the disease prevalence in the population of interest. 4 Positive and negative predictive values, but not sensitivity or specificity, are values influenced by the prevalence of disease in the population that is being tested.  Misconceptions  It is often claimed that a highly specific test is effective at ruling in a disease when positive, while a highly sensitive test is deemed effective at ruling out a disease when negative. 5 6 This has led to the widely used mnemonics SPIN and SNOUT, according to which a highly SPecific test, when Positive, rules IN disease (SP-P-IN), and a highly 'SeNsitive' test, when Negative rules OUT disease (SN-N-OUT). Both rules of thumb are, however, inferentially misleading, as the diagnostic power of any test is determined by both its sensitivity and its specificity. 7 8 9  The tradeoff between Specificity and Sensitivity is explored in ROC analysis as a trade off between TPR and FPR (that is Recall and Fallout). 10 Giving them equal weight optimizes Informedness = Specificity+Sensitivity-1 = TPR-FPR, the magnitude of which gives the probability of an informed decision between the two classes (>0 represents appropriate use of information, 0 represents chance-level performance,  Sensitivity index  The sensitivity index or ''d' '' (pronounced 'dee-prime') is a statistic used in signal detection theory . It provides the separation between the means of the signal and the noise distributions, compared against the standard deviation of the noise distribution. For normally distributed signal and noise with mean and standard deviations    μ  S     subscript  μ  S    \mu_{S}   and    σ  S     subscript  σ  S    \sigma_{S}   , and    μ  N     subscript  μ  N    \mu_{N}   and    σ  N     subscript  σ  N    \sigma_{N}   , respectively, d' is defined as:       d  ′   =     μ  S   -   μ  N       1  2    (    σ  S  2   +   σ  N  2    )           superscript  d  normal-′        subscript  μ  S    subscript  μ  N          1  2      superscript   subscript  σ  S   2    superscript   subscript  σ  N   2         d^{\prime}=\frac{\mu_{S}-\mu_{N}}{\sqrt{\frac{1}{2}(\sigma_{S}^{2}+\sigma_{N}^%
 {2})}}    11  An estimate of d' can be also found from measurements of the hit rate and false-alarm rate. It is calculated as:    d ' = Z (hit rate) - Z (false alarm rate), 12      where function Z ( p ), p ∈ [0,1], is the inverse of the cumulative Gaussian distribution .  ''d' '' is a dimensionless statistic. A higher d ' indicates that the signal can be more readily detected.  Worked example  Estimation of errors in quoted sensitivity or specificity  Sensitivity and specificity values alone may be highly misleading. The 'worst-case' sensitivity or specificity must be calculated in order to avoid reliance on experiments with few results. For example, a particular test may easily show 100% sensitivity if tested against the gold standard four times, but a single additional test against the gold standard that gave a poor result would imply a sensitivity of only 80%. A common way to do this is to state the binomial proportion confidence interval , often calculated using a Wilson score interval.  Confidence intervals for sensitivity and specificity can be calculated, giving the range of values within which the correct value lies at a given confidence level (e.g., 95%). 13  Terminology in information retrieval  In information retrieval , the positive predictive value is called precision , and sensitivity is called recall . Unlike the Specificity vs Sensitivity tradeoff, these measures are both independent of the number of true negatives, which is generally unknown and much larger than the actual numbers of relevant and retrieved documents. This assumption of very large numbers of true negatives versus positives is rare in other applications. 14  The F-score can be used as a single measure of performance of the test for the positive class. The F-score is the harmonic mean of precision and recall:      F  =   2  ×    precision  ×  recall    precision  +  recall         F    2      precision  recall     precision  recall       F=2\times\frac{\text{precision}\times\text{recall}}{\text{precision}+\text{%
 recall}}     In the traditional language of statistical hypothesis testing , the sensitivity of a test is called the statistical power of the test, although the word power in that context has a more general usage that is not applicable in the present context. A sensitive test will have fewer Type II errors .  See also  References  Further reading      External links   Vassar College's Sensitivity/Specificity Calculator   "  Category:Accuracy and precision  Category:Bioinformatics  Category:Biostatistics  Category:Cheminformatics  Category:Medical statistics  Category:Statistical ratios  Category:Statistical theory  Category:Summary statistics for contingency tables     ↩   ↩  ↩  ↩  ↩  ↩  ↩  ↩   ↩  ↩  ↩      