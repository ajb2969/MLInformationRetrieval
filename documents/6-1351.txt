   Isotonic regression      Isotonic regression   (Figure)  An example of isotonic regression   In numerical analysis , isotonic regression ( IR ) involves finding a weighted least-squares fit    x  ∈   ℝ  n       x   superscript  ℝ  n     x\in\mathbb{R}^{n}   to a vector     a  ∈   ℝ  n       a   superscript  ℝ  n     a\in\mathbb{R}^{n}   with weights vector    w  ∈   ℝ  n       w   superscript  ℝ  n     w\in\mathbb{R}^{n}   subject to a set of non-contradictory constraints of the kind     x  i   ≥   x  j        subscript  x  i    subscript  x  j     x_{i}\geq x_{j}   .  Such constraints define partial order or total order and can be represented as a directed graph     G  =   (  N  ,  E  )       G   N  E     G=(N,E)   , where N is the set of variables involved, and E is the set of pairs (i, j) for each constraint     x  i   ≥   x  j        subscript  x  i    subscript  x  j     x_{i}\geq x_{j}   . Thus, the IR problem corresponds to the following quadratic program (QP):      min    ∑   i  =  1   n     w  i     (    x  i   -   a  i    )   2            superscript   subscript     i  1    n      subscript  w  i    superscript     subscript  x  i    subscript  a  i    2       \min\sum_{i=1}^{n}w_{i}(x_{i}-a_{i})^{2}         subject to   x  i    ≥     x  j    for all   (  i  ,  j  )    ∈  E   .          subject to   subscript  x  i       subscript  x  j   for all   i  j         E     \text{subject to }x_{i}\geq x_{j}~{}\text{ for all }(i,j)\in E.     In the case when    G  =   (  N  ,  E  )       G   N  E     G=(N,E)   is a total order , a simple iterative algorithm for solving this QP is called the pool adjacent violators algorithm (PAVA). Best and Chakravarti (1990) have studied the problem as an active set identification problem, and have proposed a primal algorithm in O(n), the same complexity as the PAVA, which can be seen as a dual algorithm. 1  IR has applications in statistical inference , for example, to fit of an isotonic curve to mean experimental results when an order is expected. A benefit of isotonic regression is that it does not assume any form for the target function, such as linearity assumed by linear regression .  Another application is nonmetric multidimensional scaling , 2 where a low-dimensional embedding for data points is sought such that order of distances between points in the embedding matches order of dissimilarity between points. Isotonic regression is used iteratively to fit ideal distances to preserve relative dissimilarity order.  Isotonic regression is also sometimes referred to as monotonic regression . Correctly speaking, isotonic is used when the direction of the trend is strictly increasing, while monotonic could imply a trend that is either strictly increasing or strictly decreasing.  Isotonic Regression under the    L  p     subscript  L  p    L_{p}   for    p  >  0      p  0    p>0   is defined as follows:      min    ∑   i  =  1   n     w  i     |    x  i   -   a  i    |   p            superscript   subscript     i  1    n      subscript  w  i    superscript       subscript  x  i    subscript  a  i     p       \min\sum_{i=1}^{n}w_{i}|x_{i}-a_{i}|^{p}          subject    to    x  i    ≥     x  j    for all   (  i  ,  j  )    ∈  E   .          subject  to   subscript  x  i       subscript  x  j   for all   i  j         E     \mathrm{subject~{}to~{}}x_{i}\geq x_{j}~{}\text{ for all }(i,j)\in E.     Simply ordered case  To illustrate the above, let     x  1   ≤   x  2   ≤  …  ≤   x  n          subscript  x  1    subscript  x  2        normal-…        subscript  x  n      x_{1}\leq x_{2}\leq\ldots\leq x_{n}   , and     f   (   x  1   )    ≤   f   (   x  2   )    ≤  …  ≤   f   (   x  n   )            f   subscript  x  1      f   subscript  x  2         normal-…         f   subscript  x  n       f(x_{1})\leq f(x_{2})\leq\ldots\leq f(x_{n})   , and     w  i   ≥  0       subscript  w  i   0    w_{i}\geq 0   .  The isotonic estimator,    g  *     superscript  g     g^{*}   , minimizes the weighted least squares-like condition:       min  g     ∑   i  =  1   n     w  i     (    g   (   x  i   )    -   f   (   x  i   )     )   2          subscript   g     superscript   subscript     i  1    n      subscript  w  i    superscript      g   subscript  x  i      f   subscript  x  i     2       \min_{g}\sum_{i=1}^{n}w_{i}(g(x_{i})-f(x_{i}))^{2}     Where   g   g   g   is the unknown function we are estimating, and   f   f   f   is a known function.  Software has been developed in the R statistical package for computing isotone (monotonic) regression. 3  References  Further reading        "  Category:Regression analysis  Category:Nonparametric regression  Category:Non-parametric Bayesian methods  Category:Numerical analysis     ↩  ↩  ↩     