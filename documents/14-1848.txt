   Nonparametric skew      Nonparametric skew  In [[statistics]] and [[probability theory]], the '''nonparametric skew''' is a statistic occasionally used with random  variables that take real values. 1 2 It is a measure of the skewness of a random variable's distribution —that is, the distribution's tendency to "lean" to one side or the other of the mean .  Its calculation does not require any knowledge of the form of the underlying distribution—hence the name nonparametric . It has some desirable properties: it is zero for any symmetric  distribution ; it is  unaffected by a scale shift; and it reveals either left- or right-skewness equally well. Although its use has been mentioned in older textbooks 3 4 it appears to have gone out of fashion. In statistical samples it has been shown to be less powerful 5 than the usual measures of skewness in detecting departures of the population from normality . 6  Properties  Definition  The nonparametric skew is defined as      S  =    μ  -  ν   σ       S      μ  ν   σ     S=\frac{\mu-\nu}{\sigma}     where the mean ( µ ), median ( ν ) and standard deviation ( σ ) of the population have their usual meanings.  Properties  The nonparametric skew is one third of the Pearson 2 skewness coefficient and lies between −1 and +1 for any distribution. 7 8 This range is implied by the fact that the mean lies within one standard deviation of any median. 9  Under an affine transformation of the variable ( X ), the value of S does not change except for a possible change in sign. In symbols       S   (    a  X   +  b   )    =     sign   (  a  )    S    (  X  )          S      a  X   b        sign  a   S   X     S(aX+b)=\operatorname{sign}(a)\,S(X)     where a ≠ 0 and b are constants and S ( X ) is the nonparametric skew of the variable X .  Sharper bounds  The bounds of this statistic ( ±1 ) were sharpened by Majindar 10 who showed that its absolute value is bounded by       2    (   p  q   )    1  /  2       (   p  +  q   )    1  /  2          2   superscript    p  q     1  2      superscript    p  q     1  2      \frac{2(pq)^{1/2}}{(p+q)^{1/2}}     with      p  =   Pr   (   X  >   E   (  X  )     )        p   Pr    X   normal-E  X       p=\Pr(X>\operatorname{E}(X))     and       q  =   Pr   (   X  <   E   (  X  )     )     ,      q   Pr    X   normal-E  X       q=\Pr(X<\operatorname{E}(X)),     where X is a random variable with finite variance , E () is the expectation operator and Pr () is the probability of the event occurring.  When p = q = 0.5 the absolute value of this statistic is bounded by 1. With p = 0.1 and p = 0.01, the statistic's absolute value is bounded by 0.6 and 0.199 respectively.  Extensions  It is also known that 11        |   μ  -   ν  0    |   ≤   E   (   |   X  -   ν  0    |   )    ≤   E   (   |   X  -  μ   |   )    ≤  σ   ,            μ   subscript  ν  0      normal-E      X   subscript  ν  0            normal-E      X  μ          σ     |\mu-\nu_{0}|\leq\operatorname{E}(|X-\nu_{0}|)\leq\operatorname{E}(|X-\mu|)%
 \leq\sigma,     where ν 0 is any median and E (.) is the expectation operator .  It has been shown that        |   μ  -   x  q    |   σ   ≤   max   (     (   1  -  q   )   q    ,    q   (   1  -  q   )     )              μ   subscript  x  q     σ           1  q   q        q    1  q        \frac{|\mu-x_{q}|}{\sigma}\leq\max\left(\sqrt{\frac{(1-q)}{q}},\sqrt{\frac{q}{%
 (1-q)}}\right)     where x q is the q th  quantile . 12 Quantiles lie between 0 and 1: the median (the 0.5 quantile) has q = 0.5. This inequality has also been used to define a measure of skewness. 13  This latter inequality has been sharpened further. 14       μ  -   σ     1  -  q   q      ≤   x  q   ≤   μ  +   σ    q   1  -  q               μ    σ        1  q   q       subscript  x  q          μ    σ      q    1  q          \mu-\sigma\sqrt{\frac{1-q}{q}}\leq x_{q}\leq\mu+\sigma\sqrt{\frac{q}{1-q}}   Another extension for a distribution with a finite mean has been published: 15       μ  -    1   2  q     E   |   X  -  μ   |      ≤   x  q   ≤   μ  +    1   (   2  -   2  q    )     E   |   X  -  μ   |              μ      1    2  q     normal-E      X  μ        subscript  x  q          μ      1    2    2  q      normal-E      X  μ          \mu-\frac{1}{2q}\operatorname{E}|X-\mu|\leq x_{q}\leq\mu+\frac{1}{(2-2q)}%
 \operatorname{E}|X-\mu|     The bounds in this last pair of inequalities are attained when     Pr   (   X  =  a   )    =  q       Pr    X  a    q    \Pr(X=a)=q   and     Pr   (   X  =  b   )    =   1  -  q        Pr    X  b      1  q     \Pr(X=b)=1-q   for fixed numbers a r is the r th  order statistic , m the sample mean and s the sample standard deviation corrected for degrees of freedom, 16       |   m  -   x  r    |   s   ≤   max   [      (   n  -  1   )    (   r  -  1   )     n   (    n  -  r   +  1   )      ,      (   n  -  1   )    (   n  -  r   )     n  r     ]              m   subscript  x  r     s     max           n  1     r  1      n      n  r   1              n  1     n  r      n  r         \frac{|m-x_{r}|}{s}\leq\text{max}\left[\sqrt{\frac{(n-1)(r-1)}{n(n-r+1)}},%
 \sqrt{\frac{(n-1)(n-r)}{nr}}\right]     Replacing r with n / 2 gives the result appropriate for the sample median: 17       |   m  -  a   |   s   ≤      n  2   -  n    n  2     =     n  -  1   n                m  a    s          superscript  n  2   n    superscript  n  2                n  1   n       \frac{|m-a|}{s}\leq\sqrt{\frac{n^{2}-n}{n^{2}}}=\sqrt{\frac{n-1}{n}}     where a is the sample median.  Statistical tests  Hotelling and Solomons considered the distribution of the test statistic 18      D  =    n   (   m  -  a   )    s       D      n    m  a    s     D=\frac{n(m-a)}{s}     where n is the sample size, m is the sample mean, a is the sample median and s is the sample's standard deviation.  Statistical tests of D have assumed that the null hypothesis being tested is that the distribution is symmetric .  Gastwirth estimated the asymptotic variance of n −1/2 D . 19 If the distribution is unimodal and symmetric about 0, the asymptotic variance lies between 1/4 and 1. Assuming a conservative estimate (putting the variance equal to 1) can lead to a true level of significance well below the nominal level.  Assuming that the underlying distribution is symmetric Cabilio and Masaro have shown that the distribution of S is asymptotically normal. 20 The asymptotic variance depends on the underlying distribution: for the normal distribution, the asymptotic variance of ( S √ n ) is 0.5708.  Assuming that the underlying distribution is symmetric, by considering the distribution of values above and below the median Zheng and Gastwirth have argued that 21        2  n     (    m  -  a   s   )           2  n        m  a   s     \sqrt{2n}\left(\frac{m-a}{s}\right)     where n is the sample size, is distributed as a t distribution .  Related statistics  Mira studied the distribution of the difference between the mean and the median. 22        γ  1   =   2   (   m  -  a   )     ,       subscript  γ  1     2    m  a      \gamma_{1}=2(m-a),     where m is the sample mean and a is the median. If the underlying distribution is symmetrical γ 1 itself is asymptotically normal. This statistic had been earlier suggested by Bonferroni. 23  Assuming a symmetric underlying distribution, a modification of S was studied by Miao, Gel and Gastwirth who modified the standard deviation to create their statistic. 24      J  =    1  n     π  2     ∑   |    X  i   -  a   |         J      1  n       π  2           subscript  X  i   a        J=\frac{1}{n}\sqrt{\frac{\pi}{2}}\sum{|X_{i}-a|}     where X i are the sample values, || is the absolute value and the sum is taken over all n sample values.  The test statistic was       T  =    m  -  a   J    .      T      m  a   J     T=\frac{m-a}{J}.     The scaled statistic ( T √ n ) is asymptotically normal with a mean of zero for a symmetric distribution. Its asymptotic variance depends on the underlying distribution: the limiting values are, for the normal distribution  = 0.5708 and, for the t distribution with three degrees of freedom ,  = 0.9689. 25  Values for individual distributions  Symmetric distributions  For symmetric probability distributions the value of the nonparametric skew is 0.  Asymmetric distributions  It is positive for right skewed distributions and negative for left skewed distributions. Absolute values ≥ 0.2 indicate marked skewness.  It may be difficult to determine S for some distributions. This is usually because a closed form for the median is not known: examples of such distributions include the gamma distribution , inverse-chi-squared distribution , the inverse-gamma distribution and the scaled inverse chi-squared distribution .  The following values for S are known:   Beta distribution : 1 Kerman J (2011) "A closed-form approximation for the median of the beta distribution".         S  =    1  3      (   α  -   2  β    )     (   α  +  β  +  1   )    1  /  2       (    α  +  β   -   2  /  3    )     (   α  β   )    1  /  2           S      1  3         α    2  β     superscript    α  β  1     1  2           α  β     2  3     superscript    α  β     1  2         S=\frac{1}{3}\frac{(\alpha-2\beta)(\alpha+\beta+1)^{1/2}}{(\alpha+\beta-2/3)(%
 \alpha\beta)^{1/2}}         If 1 < β < α then the positions of α and β are reversed in the formula. S is always < 0.    Binomial distribution : varies. If the mean is an integer then S = 0. If the mean is not an integer S may have either sign or be zero. 26 It is bounded by ±min{ max{ p , 1 − p }, log e 2 } / σ where σ is the standard deviation of the binomial distribution. 27    Burr distribution :    Birnbaum–Saunders distribution :         S  =   2    β  2    (   4  +   5   α  2     )         S    2     superscript  β  2     4    5   superscript  α  2         S=\frac{2}{\beta^{2}(4+5\alpha^{2})}         where α is the shape parameter and β is the location parameter.    Cantor distribution :           -  4   3   ≤  S  ≤   4  3             4   3   S         4  3      \frac{-4}{3}\leq S\leq\frac{4}{3}         Chi square distribution : Although S ≥ 0 its value depends on the numbers of degrees of freedom ( k ).         S  ≈    1  -    (   1  -   2  k    )   3    2       S      1   superscript    1    2  k    3    2     S\approx\frac{1-(1-\frac{2}{k})^{3}}{2}         Dagum distribution :    Exponential distribution :         S  =   1  -    log  e    (  2  )     ≈  0.31        S    1    subscript   e   2         0.31     S=1-\log_{e}(2)\approx 0.31         Exponential distribution with two parameters: 28         S  =   1  -    log  e    (  2  )     ≈  0.31        S    1    subscript   e   2         0.31     S=1-\log_{e}(2)\approx 0.31         Exponential-logarithmic distribution         S  =   -     p  o  l  y  l  o  g   (  2  ,   1  -  p   )    +    ln   (   1  +   p    )     ln  p       -   [    2  p  o  l  y  l  o  g   (  3  ,   1  -  p   )    +   p  o  l  y  l  o   g  2    (  2  ,   1  -  p   )     ]           S          p  o  l  y  l  o  g   2    1  p           1    p       p          delimited-[]      2  p  o  l  y  l  o  g   3    1  p       p  o  l  y  l  o   superscript  g  2    2    1  p             S=-\frac{polylog(2,1-p)+\ln(1+\sqrt{p})\ln p}{\sqrt{-[2polylog(3,1-p)+polylog^%
 {2}(2,1-p)]}}         Here S is always > 0.    Exponentially modified Gaussian distribution :         0  ≤  S  ≤   1  -    log  e    (  2  )           0  S         1    subscript   e   2       0\leq S\leq 1-\log_{e}(2)         F distribution with n and n  degrees of freedom ( n > 4 ): 29         S  =     n   -   3  /  2        n  -  4    n  -  2      +   O   (   n   -   5  /  2     )         S       superscript  n      3  2           n  4     n  2        O   superscript  n      5  2         S=n^{-3/2}\sqrt{\frac{n-4}{n-2}}+O(n^{-5/2})         Fréchet distribution : The variance of this distribution is defined only for α > 2.     S = \frac{ \Gamma \left( 1 - \frac{ 1 }{ \alpha } \right) - \frac{ 1 }{ \sqrt{\alpha} \log_e( 2 ) } }     \sqrt { \Gamma \left( 1 - \frac{ 2 }{ \alpha } \right ) - \left( \Gamma \left( 1 - \frac{ 1 }{ \alpha } \right ) \right )^2 }   Gamma distribution : The median can only be determined approximately for this distribution. 30 If the shape parameter α is ≥ 1 then         S  ≈   β    3  α   +  0.2        S    β      3  α   0.2      S\approx\frac{\beta}{3\alpha+0.2}         where β > 0 is the rate parameter. Here S is always > 0.    Generalized normal distribution version 2         S  =   -     exp   (    -   k  2    2   )    -  1      exp   (    k  2   2   )    -  1          S               superscript  k  2    2    1            superscript  k  2   2    1        S=-\frac{\exp(\frac{-k^{2}}{2})-1}{\sqrt{\exp(\frac{k^{2}}{2})-1}}         S is always < 0.    Generalized Pareto distribution : S is defined only when the shape parameter ( k ) is < 1/2. S is < 0 for this distribution.         S  =    (      2  k   -  1   k   -   2  k    )     (   1  -   2  k    )   0.5        S           superscript  2  k   1   k    superscript  2  k     superscript    1    2  k    0.5      S=\left(\frac{2^{k}-1}{k}-2^{k}\right)(1-2k)^{0.5}         Gumbel distribution :            6    [   γ  +    log  e    (    log  e    (  2  )    )     ]    π   ≈  0.1643            6    delimited-[]    γ    subscript   e     subscript   e   2       π   0.1643    \frac{\sqrt{6}[\gamma+\log_{e}(\log_{e}(2))]}{\pi}\approx 0.1643         where γ is Euler's constant . 31     Half-normal distribution : 32         S  ≈     2   -   0.6745   π       π  -  2     ≈  0.36279        S        2     0.6745    π         π  2          0.36279     S\approx\frac{\sqrt{2}-0.6745\sqrt{\pi}}{\sqrt{\pi-2}}\approx 0.36279         Kumaraswamy distribution    Log-logistic distribution (Fisk distribution): Let β be the shape parameter. The variance and mean of this distribution are only defined when β > 2. To simplify the notation let b = β /   π   π   \pi   .         S  =    b  -   sin   (  b  )        b   tan   (  b  )     -   b  2          S      b    b          b    b     superscript  b  2        S=\frac{b-\sin(b)}{\sqrt{b\tan(b)-b^{2}}}         The standard deviation does not exist for values of b > 4.932 (approximately). For values for which the standard deviation is defined, S is > 0.    Log-normal distribution : With mean ( μ ) and variance ( σ 2 )         S  =   1    (    e    σ  2   2    +  1   )    (   e   μ  +   σ  2     )         S    1       superscript  e     superscript  σ  2   2    1    superscript  e    μ   superscript  σ  2         S=\frac{1}{(e^{\frac{\sigma^{2}}{2}}+1)(e^{\mu+\sigma^{2}})}         Log-Weibull distribution : 33         S  ≈     [     log  e    (    log  e    (  2  )    )    -  0.5772   ]    6    π   ≈   -  0.1643         S       delimited-[]      subscript   e     subscript   e   2    0.5772      6    π          0.1643      S\approx\frac{[\log_{e}(\log_{e}(2))-0.5772]\sqrt{6}}{\pi}\approx-0.1643         Lomax distribution : S is defined only for α > 2         S  =     (   α  -  1   )    (   α  -  2   )    (   1  -    (   α  -  1   )    (    2   1  /  α    -  1   )     )     α   1  /  2         S        α  1     α  2     1      α  1      superscript  2    1  α    1       superscript  α    1  2       S=\frac{(\alpha-1)(\alpha-2)(1-(\alpha-1)(2^{1/\alpha}-1))}{\alpha^{1/2}}         Maxwell–Boltzmann distribution : 34         S  ≈     2   -   1.5382  Γ   (   3  2   )       2   (    Γ   (   5  2   )    -   Γ   (   3  2   )     )      ≈  0.0854        S        2     1.5382  normal-Γ    3  2         2      normal-Γ    5  2      normal-Γ    3  2             0.0854     S\approx\frac{\sqrt{2}-1.5382\Gamma(\frac{3}{2})}{\sqrt{2(\Gamma(\frac{5}{2})-%
 \Gamma(\frac{3}{2}))}}\approx 0.0854         Nakagami distribution         S  =   -  1       S    1     S=-1         Pareto distribution : for α > 2 where α is the shape parameter of the distribution,          S  =    (   α  -    2   1  /  α     [   α  -  1   ]     )     (    α  -  2   α   )    1  /  2      ,      S      α     superscript  2    1  α     delimited-[]    α  1       superscript      α  2   α     1  2       S=(\alpha-2^{1/\alpha}[\alpha-1])(\frac{\alpha-2}{\alpha})^{1/2},         and S is always > 0.    Poisson distribution :           -    log  e    (  2  )      λ   1  2     ≤  S  ≤   1   3   λ   1  2                  subscript   e   2     superscript  λ    1  2     S         1    3   superscript  λ    1  2         \frac{-\log_{e}(2)}{\lambda^{\frac{1}{2}}}\leq S\leq\frac{1}{3\lambda^{\frac{1%
 }{2}}}         where λ is the parameter of the distribution. 35     Rayleigh distribution :         S  =     2   4  -  π      [     (   π  2   )   0.5   -    log  e    (  4  )     ]    ≈  0.1251        S        2    4  π      delimited-[]     superscript    π  2   0.5     subscript   e   4           0.1251     S=\sqrt{\frac{2}{4-\pi}}[(\frac{\pi}{2})^{0.5}-\log_{e}(4)]\approx 0.1251         Weibull distribution :          S  =    Γ   (  1  +  1  /  k  )   -   log  e     (  2  )    1  /  k       (    Γ   (   1  +   2  /  k    )    -   Γ   (   1  +   1  /  k    )     )    1  /  2      ,      S     fragments  Γ   fragments  normal-(  1   1   k  normal-)     subscript   e    superscript   fragments  normal-(  2  normal-)     1  k      superscript      normal-Γ    1    2  k       normal-Γ    1    1  k        1  2       S=\frac{\Gamma(1+1/k)-\log_{e}(2)^{1/k}}{(\Gamma(1+2/k)-\Gamma(1+1/k))^{1/2}},         where k is the shape parameter of the distribution. Here S is always > 0.   History  In 1895 Pearson first suggested measuring skewness by standardizing the difference between the mean and the mode , 36 giving        μ  -  θ   σ   ,        μ  θ   σ    \frac{\mu-\theta}{\sigma},     where μ , θ and σ is the mean, mode and standard deviation of the distribution respectively. Estimates of the population mode from the sample data may be difficult but the difference between the mean and the mode for many distributions is approximately three times the difference between the mean and the median 37 which suggested to Pearson a second skewness coefficient:        3   (   μ  -  ν   )    σ   ,        3    μ  ν    σ    \frac{3(\mu-\nu)}{\sigma},     where ν is the median of the distribution. Bowley dropped the factor 3 is from this formula in 1901 leading to the nonparametric skew statistic.  The relationship between the median, the mean and the mode was first noted by Pearson when he was investigating his type III distributions.  Relationships between the mean, median and mode  For an arbitrary distribution the mode, median and mean may appear in any order. 38 39 40  Analyses have been made of some of the relationships between the mean, median, mode and standard deviation. 41 and these relationships place some restrictions of the sign and magnitude of the nonparametric skew.  A simple example illustrating these relationships is the binomial distribution with n = 10 and p = 0.09. 42 This distribution when plotted has a long right tail. The mean (0.9) is to the left of the median (1) but the skew (0.906) as defined by the third standardized moment is positive. In contrast the nonparametric skew is -0.110.  Pearson's rule  The rule that for some distributions the difference between the mean and the mode is three times that between the mean and the median is due to Pearson who discovered it while investigating his Type 3 distributions. It is often applied to slightly asymmetric distributions that resemble a normal distribution but it is not always true.  In 1895 Pearson noted that for what is now known as the gamma distribution that the relation 43       ν  -  θ   =   2   (   μ  -  ν   )          ν  θ     2    μ  ν      \nu-\theta=2(\mu-\nu)     where θ , ν and µ are the mode, median and mean of the distribution respectively was approximately true for distributions with a large shape parameter.  Doodson in 1917 proved that the median lies between the mode and the mean for moderately skewed distributions with finite fourth moments. 44 This relationship holds for all the Pearson distributions and all of these distributions have a positive nonparametric skew.  Doodson also noted that for this family of distributions to a good approximation,       θ  =    3  ν   -   2  μ     ,      θ      3  ν     2  μ      \theta=3\nu-2\mu,     where θ , ν and µ are the mode, median and mean of the distribution respectively. Doodson's approximation was further investigated and confirmed by Haldane . 45 Haldane noted that in samples with identical and independent variates with a third cumulant had sample means that obeyed Pearson's relationship for large sample sizes. Haldane required a number of conditions for this relationship to hold including the existence of an Edgeworth expansion and the uniqueness of both the median and the mode. Under these conditions he found that mode and the median converged to 1/2 and 1/6 of the third moment respectively. This result was confirmed by Hall under weaker conditions using characteristic functions . 46  Doodson's relationship was studied by Kendall and Stuart in the log-normal distribution for which they found an exact relationship close to it. 47  Hall also showed that for a distribution with regularly varying tails and exponent α that 48       μ  -  θ   =   α   (   μ  -  ν   )          μ  θ     α    μ  ν      \mu-\theta=\alpha(\mu-\nu)     Unimodal distributions  Gauss showed in 1823 that for a unimodal distribution 49      σ  ≤  ω  ≤   2  σ         σ  ω         2  σ      \sigma\leq\omega\leq 2\sigma     and        |   ν  -  μ   |   ≤     3  4    ω    ,          ν  μ          3  4    ω     |\nu-\mu|\leq\sqrt{\frac{3}{4}}\omega,     where ω is the root mean square deviation from the mode.  For a large class of unimodal distributions that are positively skewed the mode, median and mean fall in that order. 50 Conversely for a large class of unimodal distributions that are negatively skewed the mean is less than the median which in turn is less than the mode. In symbols for these positively skewed unimodal distributions      θ  ≤  ν  ≤  μ        θ  ν       μ     \theta\leq\nu\leq\mu     and for these negatively skewed unimodal distributions      μ  ≤  ν  ≤  θ        μ  ν       θ     \mu\leq\nu\leq\theta     This class includes the important F, beta and gamma distributions.  This rule does not hold for the unimodal Weibull distribution. 51  For a unimodal distribution the following bounds are known and are sharp: 52         |   θ  -  μ   |   σ   ≤   3    ,            θ  μ    σ     3     \frac{|\theta-\mu|}{\sigma}\leq\sqrt{3},            |   ν  -  μ   |   σ   ≤   0.6    ,            ν  μ    σ     0.6     \frac{|\nu-\mu|}{\sigma}\leq\sqrt{0.6},            |   θ  -  ν   |   σ   ≤   3    ,            θ  ν    σ     3     \frac{|\theta-\nu|}{\sigma}\leq\sqrt{3},     where μ , ν and θ are the mean, median and mode respectively.  The middle bound limits the nonparametric skew of a unimodal distribution to approximately ±0.775.  van Zwet condition  The following inequality,       θ  ≤  ν  ≤  μ   ,        θ  ν       μ     \theta\leq\nu\leq\mu,     where θ , ν and µ is the mode, median and mean of the distribution respectively, holds if         F   (   ν  -  x   )    +   F   (   ν  +  x   )     ≥   1  for all  x    ,          F    ν  x      F    ν  x       1  for all  x     F(\nu-x)+F(\nu+x)\geq 1\text{ for all }x,     where F is the cumulative distribution function of the distribution. 53 These conditions have since been generalised 54 and extended to discrete distributions. 55 Any distribution for which this holds has either a zero or a positive nonparametric skew.  Notes  Ordering of skewness  In 1964 van Zwet proposed a series of axioms for ordering measures of skewness. 56 The nonparametric skew does not satisfy these axioms.  Benford's law  Benford's law is an empirical law concerning the distribution of digits in a list of numbers. It has been suggested that random variates from distributions with a positive nonparametric skew will obey this law. 57  Relation to Bowley's coefficient  This statistic can be derived from Bowley's coefficient of skewness 58       S   K  2    =      Q  3   +   Q  1    -   2   Q  2       Q  3   -   Q  1           S   subscript  K  2           subscript  Q  3    subscript  Q  1      2   subscript  Q  2        subscript  Q  3    subscript  Q  1       SK_{2}=\frac{Q_{3}+Q_{1}-2Q_{2}}{Q_{3}-Q_{1}}     where Q i is the ith quartile of the distribution.  Hinkley generalised this 59       S  K   =       F   -  1     (   1  -  α   )    +    F   -  1     (  α  )     -   2   Q  2       Q  3   -   Q  1           S  K            superscript  F    1      1  α       superscript  F    1    α      2   subscript  Q  2        subscript  Q  3    subscript  Q  1       SK=\frac{F^{-1}(1-\alpha)+F^{-1}(\alpha)-2Q_{2}}{Q_{3}-Q_{1}}     where   α   α   \alpha   lies between 0 and 0.5. Bowley's coefficient is a special case with   α   α   \alpha   equal to 0.25.  Groeneveld and Meeden 60 removed the dependence on by integrating over it.       S   K  3    =    μ  -   Q  2     E   |   y  -   Q  2    |           S   subscript  K  3        μ   subscript  Q  2      E      y   subscript  Q  2         SK_{3}=\frac{\mu-Q_{2}}{E|y-Q_{2}|}     The denominator is a measure of dispersion. Replacing the denominator with the standard deviation we obtain the nonparametric skew.  References  "  Category:Summary statistics     Arnold  BC,  Groeneveld  RA  (1995)  Measuring  skewness  with  respect  to  the  mode.  The  American  Statistician  49  (1)  34–38  DOI:10.1080/00031305.1995.10476109 ↩  Rubio  F.J.;  Steel  M.F.J.  (2012)  "On  the  Marshall–Olkin  transformation  as  a  skewing  mechanism".  Computational  Statistics  &  Data  Analysis  Preprint ↩  Yule G.U. ; Kendall M.G. (1950) An Introduction to the Theory of Statistics . 3rd edition. Harper Publishing Company pp 162–163 ↩  Hildebrand DK (1986) Statistical thinking for behavioral scientists. Boston: Duxbury ↩  Tabor J (2010) Investigating the Investigative Task: Testing for skewness - An investigation of different test statistics and their power to detect skewness. J Stat Ed 18: 1–13 ↩  ↩  Hotelling H, Solomons LM (1932) The limits of a measure of skewness. Annals Math Stat 3, 141–114 ↩  Garver (1932) Concerning the limits of a mesuare of skewness. Ann Math Stats 3(4) 141–142 ↩  O’Cinneide CA (1990) The mean is within one standard deviation of any median. Amer Statist 44, 292–293 ↩  Majindar KN (1962) "Improved bounds on a measure of skewness". Annals of Mathematical Statistics , 33, 1192–1194 ↩  Mallows CCC, Richter D (1969) "Inequalities of Chebyschev type involving conditional expectations". Annals of Mathematical Statistics , 40:1922–1932 ↩   Dziubinska R, Szynal D (1996) On functional measures of skewness. Applicationes Mathematicae 23(4) 395–403 ↩  Dharmadhikari SS (1991) Bounds on on quantiles: a comment on O'Cinneide. The Am Statist 45: 257-58 ↩  Gilat D, Hill TP(1993) Quantile-locating functions and the distance between the mean and quantiles. Statistica Neerlandica 47 (4) 279–283 DOI: 10.1111/j.1467-9574.1993.tb01424.x [ http://digitalcommons.calpoly.edu/cgi/viewcontent.cgi?article=1037&context; ;=rgp_rsr] ↩  David HA (1991) Mean minus median: A comment on O'Cinneide. The Am Statist 45: 257 ↩  Joarder AH, Laradji A (2004) Some inequalities in descriptive statistics. Technical Report Series TR 321 ↩   Gastwirth JL (1971) "On the sign test for symmetry". Journal of the American Statistical Association 66:821–823 ↩  Cabilio P, Masaro J (1996) "A simple test of symmetry about an unknown median". Canandian Journal of Statistics-Revue Canadienne De Statistique , 24:349–361 ↩  Zheng T, Gastwirth J (2010) "On bootstrap tests of symmetry about an unknown median". Journal of Data Science , 8(3): 413–427 ↩  Mira A (1999) "Distribution-free test for symmetry based on Bonferroni’s measure", Journal of Applied Statistics , 26:959–972 ↩  Bonferroni CE (1930) Elementi di statistica generale . Seeber, Firenze ↩  Miao W, Gel YR, Gastwirth JL (2006) "A new test of symmetry about an unknown median". In: Hsiung A, Zhang C-H, Ying Z, eds. Random Walk, Sequential Analysis and Related Topics — A Festschrift in honor of Yuan-Shih Chow . World Scientific; Singapore ↩   Kaas R, Buhrman JM (1980) Mean, median and mode in binomial distributions. Statistica Neerlandica 34 (1) 13–18 ↩  Hamza K (1995) "The smallest uniform upper bound on the distance between the mean and the median of the binomial and Poisson distributions". Statistics and Probability Letters , 23 (1) 21–25 ↩  http://web.ipac.caltech.edu/staff/fmasci/home/statistics_refs/UsefulDistributions.pdf ↩  Terrell GR (1986) "Pearson's rule for sample medians". Technical Report 86-2 ↩  Banneheka BMSG, Ekanayake GEMUPD (2009) A new point estimator for the median of Gamma distribution. Viyodaya J Science 14:95–103 ↩  Ferguson T. "Asymptotic Joint Distribution of Sample Mean and a Sample Quantile" , Unpublished ↩     Choi KP (1994) "On the medians of Gamma distributions and an equation of Ramanujan". Proc Amer Math Soc 121 (1) 245–251 ↩  Pearson K (1895) Contributions to the Mathematical Theory of Evolution–II. Skew variation in homogenous material. Phil Trans Roy Soc A. 186: 343–414 ↩  Stuart A, Ord JK (1994) Kendall’s advanced theory of statistics. Vol 1. Distribution theory . 6th Edition. Edward Arnold, London ↩  Relationship between the mean, median, mode, and standard deviation in a unimodal distribution ↩  von Hippel, Paul T. (2005) "Mean, Median, and Skew: Correcting a Textbook Rule" , Journal of Statistics Education , 13(2) ↩  Dharmadhikari SW, Joag-dev K (1983) Mean, Median, Mode III. Statistica Neerlandica, 33: 165–168 ↩  Bottomly, H.(2002,2006) "Relationship between the mean, median, mode, and standard deviation in a unimodal distribution" Personal webpage ↩  Lesser LM (2005). "Letter to the editor" , [comment on von Hippel (2005)]. Journal of Statistics Education 13(2). ↩  Pearson K (1895) "Contributions to the Mathematical Theory of Evolution–II. Skew variation in homogenous material". Phil Trans Roy Soc A. 186: 343–414 ↩  Doodson AT (1917) "Relation of the mode, median and mean in frequency functions". Biometrika , 11 (4) 425–429 ↩  Haldane JBS (1942) "The mode and median of a nearly normal distribution with given cumulants". Biometrika , 32: 294–299 ↩  Hall P (1980) "On the limiting behaviour of the mode and median of a sum of independent random variables". Annals of Probability 8: 419–430 ↩  Kendall M.G., Stuart A. (1958) The advanced theory of statistics . p53 Vol 1. Griffin. London ↩  Hall P. (1980) "On the limiting behaviour of the mode and median of a sum of independent random variables". Annals of Probability 8: 419–430 ↩  Gauss C.F. Theoria Combinationis Observationum Erroribus Minimis Obnoxiae. Pars Prior. Pars Posterior. Supplementum. Theory of the Combination of Observations Least Subject to Errors. Part One. Part Two. Supplement. 1995. Translated by G.W. Stewart. Classics in Applied Mathematics Series, Society for Industrial and Applied Mathematics, Philadelphia ↩  MacGillivray HL (1981) The mean, median, mode inequality and skewness for a class of densities. Aust J Stat 23(2) 247–250 ↩  Groeneveld RA (1986) Skewness for the Weibull family. Statistica Neerlandica 40: 135–140 ↩  Johnson NL, Rogers CA (1951) "The moment problem for unimodal distributions". Annals of Mathematical Statistics , 22 (3) 433–439 ↩  van Zwet W.R. (1979) "Mean, median, mode II". Statistica Neerlandica 33(1) 1–5 ↩  Dharmadhikari SW, Joag-dev K (1983) Mean, median, mode III. Statistica Neerlandica 37 (4) 165–168 ↩  Abdous B, Theodorescu R (1998) Mean, median, mode IV. Statistica Neerlandica. 52 (3) 356–359 ↩  van Zwet, W.R. (1964) "Convex transformations of random variables". Mathematics Centre Tract , 7, Mathematisch Centrum, Amsterdam ↩  Durtschi C, Hillison W, Pacini C (2004) The effective use of Benford’s Law to assist in detecting fraud in accounting data. J Forensic Accounting 5: 17–34 ↩  Bowley AL (1920) Elements of statistics. New York: Charles Scribner's Sons ↩  Hinkley DV (1975) On power transformations to symmetry. Biometrika 62: 101–111 ↩  Groeneveld RA, Meeden G (1984) Measuring skewness and kurtosis. The Statistician, 33: 391–399 ↩     