   Representer theorem      Representer theorem   In statistical learning theory , a representer theorem is any of several related results stating that a minimizer    f  *     superscript  f     f^{*}   of a regularized empirical risk function defined over a reproducing kernel Hilbert space can be represented as a finite linear combination of kernel products evaluated on the input points in the training set data.  Formal Statement  The following Representer Theorem and its proof are due to Sch√∂lkopf , Herbrich, and Smola:  Theorem: Let   ùí≥   ùí≥   \mathcal{X}   be a nonempty set and   k   k   k   a positive-definite real-valued kernel on    ùí≥  √ó  ùí≥      ùí≥  ùí≥    \mathcal{X}\times\mathcal{X}   with corresponding reproducing kernel Hilbert space    H  k     subscript  H  k    H_{k}   . Given a training sample      (   x  1   ,   y  1   )   ,  ‚Ä¶  ,   (   x  n   ,   y  n   )    ‚àà   ùí≥  √ó   \R           subscript  x  1    subscript  y  1    normal-‚Ä¶    subscript  x  n    subscript  y  n       ùí≥  \R     (x_{1},y_{1}),\ldots,(x_{n},y_{n})\in\mathcal{X}\times\R   , a strictly monotonically increasing real-valued function    g  :    [  0  ,  ‚àû  )   ‚Üí   \R       normal-:  g   normal-‚Üí   0    \R     g\colon[0,\infty)\to\R   , and an arbitrary empirical risk function    E  :     (   ùí≥  √ó    \R   2    )   n   ‚Üí    \R   ‚à™   {  ‚àû  }        normal-:  E   normal-‚Üí   superscript    ùí≥   superscript  \R  2    n     \R         E\colon(\mathcal{X}\times\R^{2})^{n}\to\R\cup\{\infty\}   , then for any     f  *   ‚àà   H  k        superscript  f     subscript  H  k     f^{*}\in H_{k}   satisfying       f  *   =     argmin   f  ‚àà   H  k      {    E   (   (   x  1   ,   y  1   ,   f   (   x  1   )    )   ,  ‚Ä¶  ,   (   x  n   ,   y  n   ,   f   (   x  n   )    )   )    +   g   (   ‚à•  f  ‚à•   )     }    ,   (  *  )         superscript  f       subscript  argmin    f   subscript  H  k         E     subscript  x  1    subscript  y  1     f   subscript  x  1     normal-‚Ä¶    subscript  x  n    subscript  y  n     f   subscript  x  n         g   norm  f          f^{*}=\operatorname{argmin}_{f\in H_{k}}\left\{E\left((x_{1},y_{1},f(x_{1})),.%
 ..,(x_{n},y_{n},f(x_{n}))\right)+g\left(\lVert f\rVert\right)\right\},\quad(*)       f  *     superscript  f     f^{*}   admits a representation of the form:         f  *    (  ‚ãÖ  )    =    ‚àë   i  =  1   n     Œ±  i   k   (  ‚ãÖ  ,   x  i   )      ,         superscript  f    normal-‚ãÖ     superscript   subscript     i  1    n      subscript  Œ±  i   k   normal-‚ãÖ   subscript  x  i        f^{*}(\cdot)=\sum_{i=1}^{n}\alpha_{i}k(\cdot,x_{i}),     where     Œ±  i   ‚àà   \R        subscript  Œ±  i   \R    \alpha_{i}\in\R   for all    1  ‚â§  i  ‚â§  n        1  i       n     1\leq i\leq n   .  Proof: Define a mapping      œÜ  :  ùí≥     normal-:  œÜ  ùí≥    \displaystyle\varphi\colon\mathcal{X}     (so that     œÜ   (  x  )    =   k   (  ‚ãÖ  ,  x  )          œÜ  x     k   normal-‚ãÖ  x      \varphi(x)=k(\cdot,x)   is itself a map    ùí≥  ‚Üí   \R      normal-‚Üí  ùí≥  \R    \mathcal{X}\to\R   ). Since   k   k   k   is reproducing kernel, then        œÜ   (  x  )    (   x  ‚Ä≤   )    =   k   (   x  ‚Ä≤   ,  x  )    =   ‚ü®   œÜ   (   x  ‚Ä≤   )    ,   œÜ   (  x  )    ‚ü©    ,          œÜ  x   superscript  x  normal-‚Ä≤      k    superscript  x  normal-‚Ä≤   x            œÜ   superscript  x  normal-‚Ä≤      œÜ  x       \varphi(x)(x^{\prime})=k(x^{\prime},x)=\langle\varphi(x^{\prime}),\varphi(x)\rangle,   where    ‚ü®  ‚ãÖ  ,  ‚ãÖ  ‚ü©     normal-‚ãÖ  normal-‚ãÖ    \langle\cdot,\cdot\rangle   is the inner product on    H  k     subscript  H  k    H_{k}   .  Given any     x  1   ,  ‚Ä¶  ,   x  n       subscript  x  1   normal-‚Ä¶   subscript  x  n     x_{1},...,x_{n}   , one can use orthogonal projection to decompose any    f  ‚àà   H  k       f   subscript  H  k     f\in H_{k}   into a sum of two function, one lying in    span   {   œÜ   (   x  1   )    ,  ‚Ä¶  ,   œÜ   (   x  n   )    }      span    œÜ   subscript  x  1    normal-‚Ä¶    œÜ   subscript  x  n      \operatorname{span}\left\{\varphi(x_{1}),...,\varphi(x_{n})\right\}   , and the other lying in the orthogonal complement:       f  =     ‚àë   i  =  1   n     Œ±  i   œÜ   (   x  i   )     +  v    ,      f      superscript   subscript     i  1    n      subscript  Œ±  i   œÜ   subscript  x  i     v     f=\sum_{i=1}^{n}\alpha_{i}\varphi(x_{i})+v,   where     ‚ü®  v  ,   œÜ   (   x  i   )    ‚ü©   =  0       v    œÜ   subscript  x  i     0    \langle v,\varphi(x_{i})\rangle=0   for all   i   i   i   .  The above orthogonal decomposition and the reproducing property together show that applying   f   f   f   to any training point    x  j     subscript  x  j    x_{j}   produces        f   (   x  j   )    =   ‚ü®     ‚àë   i  =  1   n     Œ±  i   œÜ   (   x  i   )     +  v   ,   œÜ   (   x  j   )    ‚ü©   =    ‚àë   i  =  1   n     Œ±  i    ‚ü®   œÜ   (   x  i   )    ,   œÜ   (   x  j   )    ‚ü©      ,          f   subscript  x  j         superscript   subscript     i  1    n      subscript  Œ±  i   œÜ   subscript  x  i     v     œÜ   subscript  x  j            superscript   subscript     i  1    n      subscript  Œ±  i      œÜ   subscript  x  i      œÜ   subscript  x  j          f(x_{j})=\left\langle\sum_{i=1}^{n}\alpha_{i}\varphi(x_{i})+v,\varphi(x_{j})%
 \right\rangle=\sum_{i=1}^{n}\alpha_{i}\langle\varphi(x_{i}),\varphi(x_{j})\rangle,     which we observe is independent of   v   v   v   . Consequently, the value of the empirical risk   E   E   E   in (*) is likewise independent of   v   v   v   . For the second term (the regularization term), since   v   v   v   is orthogonal to     ‚àë   i  =  1   n     Œ±  i   œÜ   (   x  i   )        superscript   subscript     i  1    n      subscript  Œ±  i   œÜ   subscript  x  i      \sum_{i=1}^{n}\alpha_{i}\varphi(x_{i})   and   g   g   g   is strictly monotonic, we have      g   (   ‚à•  f  ‚à•   )       g   norm  f     \displaystyle g\left(\lVert f\rVert\right)     Therefore setting    v  =  0      v  0    v=0   does not affect the first term of (*), while it strictly decreasing the second term. Consequently, any minimizer    f  *     superscript  f     f^{*}   in (*) must have    v  =  0      v  0    v=0   , i.e., it must be of the form         f  *    (  ‚ãÖ  )    =    ‚àë   i  =  1   n     Œ±  i   œÜ   (   x  i   )     =    ‚àë   i  =  1   n     Œ±  i   k   (  ‚ãÖ  ,   x  i   )      ,           superscript  f    normal-‚ãÖ     superscript   subscript     i  1    n      subscript  Œ±  i   œÜ   subscript  x  i            superscript   subscript     i  1    n      subscript  Œ±  i   k   normal-‚ãÖ   subscript  x  i         f^{*}(\cdot)=\sum_{i=1}^{n}\alpha_{i}\varphi(x_{i})=\sum_{i=1}^{n}\alpha_{i}k(%
 \cdot,x_{i}),     which is the desired result.  Generalizations  The Theorem stated above is a particular example of a family of results that are collectively referred to as "Representer Theorems"; here we describe several such.  The first statement of a Representer Theorem was due to Kimeldorf and Wahba for the special case in which      E   (   (   x  1   ,   y  1   ,   f   (   x  1   )    )   ,  ‚Ä¶  ,   (   x  n   ,   y  n   ,   f   (   x  n   )    )   )       E     subscript  x  1    subscript  y  1     f   subscript  x  1     normal-‚Ä¶    subscript  x  n    subscript  y  n     f   subscript  x  n        \displaystyle E\left((x_{1},y_{1},f(x_{1})),...,(x_{n},y_{n},f(x_{n}))\right)     for    Œª  >  0      Œª  0    \lambda>0   . Sch√∂lkopf, Herbrich, and Smola generalized this result by relaxing the assumption of the squared-loss cost and allowing the regularizer to be any strictly monotonically increasing function    g   (  ‚ãÖ  )       g  normal-‚ãÖ    g(\cdot)   of the Hilbert space norm.  It is possible to generalize further by augmenting the regularized empirical risk function through the addition of unpenalized offset terms. For example, Sch√∂lkopf, Herbrich, and Smola also consider the minimization        f  ~   *   =    argmin   {    E   (   (   x  1   ,   y  1   ,    f  ~    (   x  1   )    )   ,  ‚Ä¶  ,   (   x  n   ,   y  n   ,    f  ~    (   x  n   )    )   )    +   g   (   ‚à•  f  ‚à•   )     ‚à£    f  ~   =   f  +  h   ‚àà    H  k   ‚äï   span   {   œà  p   ‚à£   1  ‚â§  p  ‚â§  M   }      }    ,   (  ‚Ä†  )         superscript   normal-~  f       argmin      E     subscript  x  1    subscript  y  1      normal-~  f    subscript  x  1     normal-‚Ä¶    subscript  x  n    subscript  y  n      normal-~  f    subscript  x  n         g   norm  f          normal-~  f     f  h         direct-sum   subscript  H  k    span   subscript  œà  p       1  p       M         normal-‚Ä†     \tilde{f}^{*}=\operatorname{argmin}\left\{E\left((x_{1},y_{1},\tilde{f}(x_{1})%
 ),...,(x_{n},y_{n},\tilde{f}(x_{n}))\right)+g\left(\lVert f\rVert\right)\mid%
 \tilde{f}=f+h\in H_{k}\oplus\operatorname{span}\{\psi_{p}\mid 1\leq p\leq M\}%
 \right\},\quad(\dagger)     i.e., we consider functions of the form     f  ~   =   f  +  h        normal-~  f     f  h     \tilde{f}=f+h   , where    f  ‚àà   H  k       f   subscript  H  k     f\in H_{k}   and   h   h   h   is an unpenalized function lying in the span of a finite set of real-valued functions    {   œà  p   :  ùí≥  ‚Üí   \R   ‚à£  1  ‚â§  p  ‚â§  M  }     fragments  normal-{   subscript  œà  p   normal-:  X  normal-‚Üí  \R  normal-‚à£  1   p   M  normal-}    \{\psi_{p}\colon\mathcal{X}\to\R\mid 1\leq p\leq M\}   . Under the assumption that the    m  √ó  M      m  M    m\times M   matrix     (    œà  p    (   x  i   )    )    i  p      subscript     subscript  œà  p    subscript  x  i      i  p     \left(\psi_{p}(x_{i})\right)_{ip}   has rank   M   M   M   , they show that the minimizer     f  ~   *     superscript   normal-~  f      \tilde{f}^{*}   in    (  ‚Ä†  )    normal-‚Ä†   (\dagger)   admits a representation of the form         f  ~   *    (  ‚ãÖ  )    =     ‚àë   i  =  1   n     Œ±  i   k   (  ‚ãÖ  ,   x  i   )     +    ‚àë   p  =  1   M     Œ≤  p    œà  p    (  ‚ãÖ  )             superscript   normal-~  f     normal-‚ãÖ       superscript   subscript     i  1    n      subscript  Œ±  i   k   normal-‚ãÖ   subscript  x  i        superscript   subscript     p  1    M      subscript  Œ≤  p    subscript  œà  p   normal-‚ãÖ       \tilde{f}^{*}(\cdot)=\sum_{i=1}^{n}\alpha_{i}k(\cdot,x_{i})+\sum_{p=1}^{M}%
 \beta_{p}\psi_{p}(\cdot)     where      Œ±  i   ,   Œ≤  p    ‚àà   \R         subscript  Œ±  i    subscript  Œ≤  p    \R    \alpha_{i},\beta_{p}\in\R   and the    Œ≤  p     subscript  Œ≤  p    \beta_{p}   are all uniquely determined.  The conditions under which a Representer Theorem exists were investigated by Argyriou, Miccheli, and Pontil, who proved the following:  Theorem: Let   ùí≥   ùí≥   \mathcal{X}   be a nonempty set,   k   k   k   a positive-definite real-valued kernel on    ùí≥  √ó  ùí≥      ùí≥  ùí≥    \mathcal{X}\times\mathcal{X}   with corresponding reproducing kernel Hilbert space    H  k     subscript  H  k    H_{k}   , and let    R  :    H  k   ‚Üí   \R       normal-:  R   normal-‚Üí   subscript  H  k   \R     R\colon H_{k}\to\R   be a differentiable regularization function. Then given a training sample      (   x  1   ,   y  1   )   ,  ‚Ä¶  ,   (   x  n   ,   y  n   )    ‚àà   ùí≥  √ó   \R           subscript  x  1    subscript  y  1    normal-‚Ä¶    subscript  x  n    subscript  y  n       ùí≥  \R     (x_{1},y_{1}),...,(x_{n},y_{n})\in\mathcal{X}\times\R   and an arbitrary empirical risk function    E  :     (   ùí≥  √ó    \R   2    )   m   ‚Üí    \R   ‚à™   {  ‚àû  }        normal-:  E   normal-‚Üí   superscript    ùí≥   superscript  \R  2    m     \R         E\colon(\mathcal{X}\times\R^{2})^{m}\to\R\cup\{\infty\}   , a minimizer       f  *   =     argmin   f  ‚àà   H  k      {    E   (   (   x  1   ,   y  1   ,   f   (   x  1   )    )   ,  ‚Ä¶  ,   (   x  n   ,   y  n   ,   f   (   x  n   )    )   )    +   R   (  f  )     }     (  ‚Ä°  )         superscript  f       subscript  argmin    f   subscript  H  k         E     subscript  x  1    subscript  y  1     f   subscript  x  1     normal-‚Ä¶    subscript  x  n    subscript  y  n     f   subscript  x  n         R  f     normal-‚Ä°     f^{*}=\operatorname{argmin}_{f\in H_{k}}\left\{E\left((x_{1},y_{1},f(x_{1})),.%
 ..,(x_{n},y_{n},f(x_{n}))\right)+R(f)\right\}\quad(\ddagger)     of the regularized empirical risk minimization problem admits a representation of the form         f  *    (  ‚ãÖ  )    =    ‚àë   i  =  1   n     Œ±  i   k   (  ‚ãÖ  ,   x  i   )      ,         superscript  f    normal-‚ãÖ     superscript   subscript     i  1    n      subscript  Œ±  i   k   normal-‚ãÖ   subscript  x  i        f^{*}(\cdot)=\sum_{i=1}^{n}\alpha_{i}k(\cdot,x_{i}),     where     Œ±  i   ‚àà   \R        subscript  Œ±  i   \R    \alpha_{i}\in\R   for all    1  ‚â§  i  ‚â§  n        1  i       n     1\leq i\leq n   , if and only if there exists a nondecreasing function    h  :    [  0  ,  ‚àû  )   ‚Üí   \R       normal-:  h   normal-‚Üí   0    \R     h\colon[0,\infty)\to\R   for which        R   (  f  )    =   h   (   ‚à•  f  ‚à•   )     .        R  f     h   norm  f      R(f)=h(\lVert f\rVert).     Effectively, this result provides a necessary and sufficient condition on a differentiable regularizer    R   (  ‚ãÖ  )       R  normal-‚ãÖ    R(\cdot)   under which the corresponding regularized empirical risk minimization    (  ‚Ä°  )    normal-‚Ä°   (\ddagger)   will have a Representer Theorem. In particular, this shows that a broad class of regularized risk minimizations (much broader than those originally considered by Kimeldorf and Wahba) have Representer Theorems.  Applications  Representer theorems are useful from a practical standpoint because they dramatically simplify the regularized empirical risk minimization problem    (  ‚Ä°  )    normal-‚Ä°   (\ddagger)   . In most interesting applications, the search domain    H  k     subscript  H  k    H_{k}   for the minimization will be an infinite-dimensional subspace of     L  2    (  ùí≥  )        superscript  L  2   ùí≥    L^{2}(\mathcal{X})   , and therefore the search (as written) does not admit implementation on finite-memory and finite-precision computers. In contrast, the representation of     f  *    (  ‚ãÖ  )        superscript  f    normal-‚ãÖ    f^{*}(\cdot)   afforded by a representer theorem reduces the original (infinite-dimensional) minimization problem to a search for the optimal   n   n   n   -dimensional vector of coefficients    Œ±  =   (   Œ±  1   ,  ‚Ä¶  ,   Œ±  n   )   ‚àà    \R   n         Œ±    subscript  Œ±  1   normal-‚Ä¶   subscript  Œ±  n          superscript  \R  n      \alpha=(\alpha_{1},...,\alpha_{n})\in\R^{n}   ;   Œ±   Œ±   \alpha   can then be obtained by applying any standard function minimization algorithm. Consequently, representer theorems provide the theoretical basis for the reduction of the general machine learning problem to algorithms that can actually be implemented on computers in practice.  See also   Mercer's theorem   References        "  Category:Computational learning theory  Category:Theoretical computer science  Category:Machine learning  Category:Hilbert space   