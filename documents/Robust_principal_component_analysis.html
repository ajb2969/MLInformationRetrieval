<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="929">Robust principal component analysis</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Robust principal component analysis</h1>
<hr/>

<p><strong>Robust Principal Component Analysis (RPCA)</strong> is a modification of the widely used statistical procedure <a href="Principal_component_analysis" title="wikilink">Principal component analysis (PCA)</a> which works well with respect to <em>grossly</em> corrupted observations. A number of different approaches exist for Robust PCA, including an idealized version of Robust PCA, which aims to recover a low-rank matrix L<sub>0</sub> from highly corrupted measurements M = L<sub>0</sub> +S<sub>0</sub>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> This decomposition in low-rank and sparse matrices can be achieved by techniques such as Principal Component Pursuit method (PCP),<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Stable PCP,<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> Quantized PCP ,<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> Block based PCP,<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> and Local PCP.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> Then, optimization methods are used such as the Augmented Lagrange Multiplier Method (ALM<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a>), Alternating Direction Method (ADM<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a>), Fast Alternating Minimization (FAM<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a>) or Iteratively Reweighted Least Squares (IRLS <a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a><a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a>). Bouwmans and Zahzah have made a complete survey <a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> in 2014.</p>
<h2 id="algorithms">Algorithms</h2>
<h3 id="non-convex-method">Non-convex method</h3>

<p>The state-of-the-art guaranteed algorithm for the robust PCA problem (with the input matrix being 

<math display="inline" id="Robust_principal_component_analysis:0">
 <semantics>
  <mrow>
   <mi>M</mi>
   <mo>=</mo>
   <mrow>
    <mi>L</mi>
    <mo>+</mo>
    <mi>S</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>M</ci>
    <apply>
     <plus></plus>
     <ci>L</ci>
     <ci>S</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   M=L+S
  </annotation>
 </semantics>
</math>

) is an alternating minimization type algorithm.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> The computational complexity is 

<math display="inline" id="Robust_principal_component_analysis:1">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo>(</mo>
    <mrow>
     <mi>m</mi>
     <mi>n</mi>
     <msup>
      <mi>r</mi>
      <mn>2</mn>
     </msup>
     <mrow>
      <mi>log</mi>
      <mfrac>
       <mn>1</mn>
       <mi>ϵ</mi>
      </mfrac>
     </mrow>
    </mrow>
    <mo>)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <times></times>
     <ci>m</ci>
     <ci>n</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>r</ci>
      <cn type="integer">2</cn>
     </apply>
     <apply>
      <log></log>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>ϵ</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O\left(mnr^{2}\log\frac{1}{\epsilon}\right)
  </annotation>
 </semantics>
</math>

 where the input is the superposition of a low-rank (of rank 

<math display="inline" id="Robust_principal_component_analysis:2">
 <semantics>
  <mi>r</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>r</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r
  </annotation>
 </semantics>
</math>

) and a sparse matrix of dimension 

<math display="inline" id="Robust_principal_component_analysis:3">
 <semantics>
  <mrow>
   <mi>m</mi>
   <mo>×</mo>
   <mi>n</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>m</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m\times n
  </annotation>
 </semantics>
</math>


 and 

<math display="inline" id="Robust_principal_component_analysis:4">
 <semantics>
  <mi>ϵ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ϵ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \epsilon
  </annotation>
 </semantics>
</math>

 is the desired accuracy of the recovered solution, i.e., 

<math display="inline" id="Robust_principal_component_analysis:5">
 <semantics>
  <mrow>
   <msub>
    <mrow>
     <mo>∥</mo>
     <mrow>
      <mover accent="true">
       <mi>L</mi>
       <mo>^</mo>
      </mover>
      <mo>-</mo>
      <mi>L</mi>
     </mrow>
     <mo>∥</mo>
    </mrow>
    <mi>F</mi>
   </msub>
   <mo>≤</mo>
   <mi>ϵ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <apply>
       <minus></minus>
       <apply>
        <ci>normal-^</ci>
        <ci>L</ci>
       </apply>
       <ci>L</ci>
      </apply>
     </apply>
     <ci>F</ci>
    </apply>
    <ci>ϵ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|\widehat{L}-L\|_{F}\leq\epsilon
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Robust_principal_component_analysis:6">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

 is the true low-rank component and 

<math display="inline" id="Robust_principal_component_analysis:7">
 <semantics>
  <mover accent="true">
   <mi>L</mi>
   <mo>^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>L</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \widehat{L}
  </annotation>
 </semantics>
</math>

 is the estimated or recovered low-rank component. Intuitively, this algorithm performs projections of the residual on to the set of low-rank matrices (via the SVD operation) and sparse matrices (via entry-wise hard thresholding) in an alternating manner - that is, low-rank projection of the difference the input matrix and the sparse matrix obtained at a given iteration followed by sparse projection of the difference of the input matrix and the low-rank matrix obtained in the previous step, and iterating the two steps until convergence.</p>
<h3 id="convex-relaxation">Convex relaxation</h3>

<p>This method consists of relaxing the rank constraint in the optimization problem to the nuclear norm and the sparsity constraint to 

<math display="inline" id="Robust_principal_component_analysis:8">
 <semantics>
  <msub>
   <mi mathvariant="normal">ℓ</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-ℓ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ell_{1}
  </annotation>
 </semantics>
</math>


-norm. The resulting program can be solved using methods such as the method of Augmented Lagrange Multipliers.</p>
<h2 id="applications">Applications</h2>

<p>RPCA has many real life important applications particularly when the data under study can naturally be modeled as a low-rank plus a sparse contribution. Following examples are inspired by contemporary challenges in computer science, and depending on the applications, either the low-rank component or the sparse component could be the object of interest:</p>
<h3 id="video-surveillance">Video Surveillance</h3>

<p>Given a sequence of surveillance video frames, it is often required to identify the activities that stand out from the background. If we stack the video frames as columns of a matrix M, then the low-rank component L<sub>0</sub> naturally corresponds to the stationary background and the sparse component S<sub>0</sub> captures the moving objects in the foreground.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> A systematic evaluation and comparative analysis of several algorithms on a large scale dataset in video surveillance can be found in a complete review.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> (For more information: <a class="uri" href="http://sites.google.com/site/rpcaforegrounddetection/">http://sites.google.com/site/rpcaforegrounddetection/</a>)</p>
<h3 id="face-recognition">Face Recognition</h3>

<p>Images of a convex, <a href="Lambertian_reflectance" title="wikilink">Lambertian surface</a> under varying illuminations span a low-dimensional subspace.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> This is one of the reasons for effectiveness of low-dimensional models for imagery data. In particular, it is easy to approximate images of a human’s face by a low-dimensional subspace. To be able to correctly retrieve this subspace is crucial in many applications such as face recognition and alignment. It turns out that RPCA can be applied successfully to this problem to exactly recover the face.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a></p>
<h2 id="resources-datasets-and-codes">Resources, Datasets and Codes</h2>
<h3 id="university-of-illinois-website">University of Illinois Website</h3>

<p>This website provides MATLAB packages to solve the RPCA optimization problem by different methods. All of the codes are Copyright 2009 Perception and Decision Lab, University of Illinois at Urbana-Champaign, and Microsoft Research Asia, Beijing. (For more information: <a class="uri" href="http://perception.csl.illinois.edu/matrix-rank/sample_code.html">http://perception.csl.illinois.edu/matrix-rank/sample_code.html</a>)</p>
<h3 id="low-rank-and-sparse-tools-for-background-modeling-and-subtraction-in-videos">Low-Rank and Sparse tools for Background Modeling and Subtraction in Videos</h3>

<p>The LRSLibrary (A. Sobral, Univ. La Rochelle, France) provides a collection of low-rank and sparse decomposition algorithms in MATLAB. The library was designed for motion segmentation in videos, but it can be also used or adapted for other computer vision problems. Currently the LRSLibrary contains a total of 64 matrix-based and tensor-based algorithms. The LRSLibrary was tested successfully in MATLAB R2013b both x86 and x64 versions. (For more information: <a class="uri" href="https://github.com/andrewssobral/lrslibrary#lrslibrary">https://github.com/andrewssobral/lrslibrary#lrslibrary</a>)</p>
<h3 id="bgs-website">BGS Website</h3>

<p>The Background Subtraction Web Site (T. Bouwmans, Univ. La Rochelle, France) contains a full list of <a href="http://sites.google.com/site/backgroundsubtraction/recent-background-modeling/background-modeling-via-rpca">references</a> on RPCA algorithms which can be applied to video surveillance, links to available <a href="https://sites.google.com/site/backgroundsubtraction/test-sequences">datasets</a> and <a href="https://sites.google.com/site/backgroundsubtraction/available-implementation">codes</a>. (For more information: <a class="uri" href="http://sites.google.com/site/backgroundsubtraction/Home">http://sites.google.com/site/backgroundsubtraction/Home</a>)</p>
<h2 id="references">References</h2>
<references>
</references>

<p>"</p>

<p><a href="Category:Matrix_decompositions" title="wikilink">Category:Matrix decompositions</a> <a href="Category:Multivariate_statistics" title="wikilink">Category:Multivariate statistics</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"></li>
<li id="fn2"></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"><a href="#fnref11">↩</a></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"></li>
<li id="fn14"><a href="#fnref14">↩</a></li>
<li id="fn15"><a href="#fnref15">↩</a></li>
<li id="fn16"><a href="#fnref16">↩</a></li>
<li id="fn17"><a href="#fnref17">↩</a></li>
<li id="fn18"></li>
</ol>
</section>
</body>
</html>
