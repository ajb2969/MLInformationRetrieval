<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="285">Bootstrap aggregating</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Bootstrap aggregating</h1>
<hr/>

<p><strong>Bootstrap aggregating</strong>, also called <strong>bagging</strong>, is a <a href="Ensemble_learning" title="wikilink">machine learning ensemble</a> <a class="uri" href="meta-algorithm" title="wikilink">meta-algorithm</a> designed to improve the stability and accuracy of <a href="machine_learning" title="wikilink">machine learning</a> algorithms used in <a href="statistical_classification" title="wikilink">statistical classification</a> and <a href="Regression_analysis" title="wikilink">regression</a>. It also reduces <a class="uri" href="variance" title="wikilink">variance</a> and helps to avoid <a class="uri" href="overfitting" title="wikilink">overfitting</a>. Although it is usually applied to <a href="Decision_tree_learning" title="wikilink">decision tree</a> methods, it can be used with any type of method. Bagging is a special case of the <a href="Ensemble_learning" title="wikilink">model averaging</a> approach.</p>
<h2 id="description-of-the-technique">Description of the technique</h2>

<p>Given a standard <a href="training_set" title="wikilink">training set</a> <em>D</em> of size <em>n</em>, bagging generates <em>m</em> new training sets 

<math display="inline" id="Bootstrap_aggregating:0">
 <semantics>
  <msub>
   <mi>D</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>D</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{i}
  </annotation>
 </semantics>
</math>

, each of size <em>n′</em>, by <a href="Sampling_(statistics)" title="wikilink">sampling</a> from <em>D</em> <a href="Probability_distribution#With_finite_support" title="wikilink">uniformly</a> and <a href="Sampling_(statistics)#Replacement_of_selected_units" title="wikilink">with replacement</a>. By sampling with replacement, some observations may be repeated in each 

<math display="inline" id="Bootstrap_aggregating:1">
 <semantics>
  <msub>
   <mi>D</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>D</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{i}
  </annotation>
 </semantics>
</math>

. If <em>n<a href="prime_(symbol)" title="wikilink">′</a></em>=<em>n</em>, then for large <em>n</em> the set 

<math display="inline" id="Bootstrap_aggregating:2">
 <semantics>
  <msub>
   <mi>D</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>D</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{i}
  </annotation>
 </semantics>
</math>

 is expected to have the fraction (1 - 1/<em><a href="e_(mathematical_constant)" title="wikilink">e</a></em>) (≈63.2%) of the unique examples of <em>D</em>, the rest being duplicates.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> This kind of sample is known as a <a href="bootstrap_(statistics)" title="wikilink">bootstrap</a> sample. The <em>m</em> models are fitted using the above <em>m</em> bootstrap samples and combined by averaging the output (for regression) or voting (for classification).</p>

<p>Bagging leads to "improvements for unstable procedures" (Breiman, 1996), which include, for example, <a href="artificial_neural_networks" title="wikilink">artificial neural networks</a>, <a href="classification_and_regression_tree" title="wikilink">classification and regression trees</a>, and subset selection in <a href="linear_regression" title="wikilink">linear regression</a> (Breiman, 1994). An interesting application of bagging showing improvement in preimage learning is provided here.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> On the other hand, it can mildly degrade the performance of stable methods such as K-nearest neighbors (Breiman, 1996).</p>
<h2 id="example-ozone-data">Example: Ozone data</h2>

<p>To illustrate the basic principles of bagging, below is an analysis on the relationship between <a class="uri" href="ozone" title="wikilink">ozone</a> and temperature (data from <a href="Peter_Rousseeuw" title="wikilink">Rousseeuw</a> and Leroy (1986), available at <a href="classic_data_sets" title="wikilink">classic data sets</a>, analysis done in <a href="R_(programming_language)" title="wikilink">R</a>).</p>

<p>The relationship between temperature and ozone in this data set is apparently non-linear, based on the scatter plot. To mathematically describe this relationship, <a href="local_regression" title="wikilink">LOESS</a> smoothers (with span 0.5) are used. Instead of building a single smoother from the complete data set, 100 <a href="bootstrap_(statistics)" title="wikilink">bootstrap</a> samples of the data were drawn. Each sample is different from the original data set, yet resembles it in distribution and variability. For each bootstrap sample, a LOESS smoother was fit. Predictions from these 100 smoothers were then made across the range of the data. The first 10 predicted smooth fits appear as grey lines in the figure below. The lines are clearly very <em>wiggly</em> and they overfit the data - a result of the span being too low.</p>

<p>By taking the average of 100 smoothers, each fitted to a subset of the original data set, we arrive at one bagged predictor (red line). Clearly, the mean is more stable and there is less <a href="overfitting" title="wikilink">overfit</a>.</p>

<p><a class="uri" href="image:ozone.png" title="wikilink">image:ozone.png</a></p>
<h2 id="bagging-for-nearest-neighbour-classifiers">Bagging for nearest neighbour classifiers</h2>

<p>The <a href="Bayes_classifier" title="wikilink">risk</a> of a 1 nearest neighbour (1NN) classifier is at most twice the risk of the <a href="Bayes_classifier" title="wikilink">Bayes classifier</a>,<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> but there are no guarantees that this classifier will be <a href="Bayes_classifier" title="wikilink">consistent</a>. By careful choice of the size of the resamples, bagging can lead to substantial improvements of the performance of the 1NN classifier. By taking a large number of resamples of the data of size 

<math display="inline" id="Bootstrap_aggregating:3">
 <semantics>
  <msup>
   <mi>n</mi>
   <mo>′</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>n</ci>
    <ci>normal-′</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n^{\prime}
  </annotation>
 </semantics>
</math>

, the bagged nearest neighbour classifier will be consistent provided 

<math display="inline" id="Bootstrap_aggregating:4">
 <semantics>
  <mrow>
   <msup>
    <mi>n</mi>
    <mo>′</mo>
   </msup>
   <mo>→</mo>
   <mi mathvariant="normal">∞</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>n</ci>
     <ci>normal-′</ci>
    </apply>
    <infinity></infinity>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n^{\prime}\to\infty
  </annotation>
 </semantics>
</math>

 diverges but 

<math display="inline" id="Bootstrap_aggregating:5">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>n</mi>
     <mo>′</mo>
    </msup>
    <mo>/</mo>
    <mi>n</mi>
   </mrow>
   <mo>→</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <apply>
     <divide></divide>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>n</ci>
      <ci>normal-′</ci>
     </apply>
     <ci>n</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n^{\prime}/n\to 0
  </annotation>
 </semantics>
</math>

 as the sample size 

<math display="inline" id="Bootstrap_aggregating:6">
 <semantics>
  <mrow>
   <mi>n</mi>
   <mo>→</mo>
   <mi mathvariant="normal">∞</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>n</ci>
    <infinity></infinity>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n\to\infty
  </annotation>
 </semantics>
</math>

.</p>

<p>Under infinite simulation, the bagged nearest neighbour classifier can be viewed as a <a href="weighted_nearest_neighbour_classifier" title="wikilink">weighted nearest neighbour classifier</a>. Suppose that the feature space is 

<math display="inline" id="Bootstrap_aggregating:7">
 <semantics>
  <mi>d</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d
  </annotation>
 </semantics>
</math>

 dimensional and denote by 

<math display="inline" id="Bootstrap_aggregating:8">
 <semantics>
  <msubsup>
   <mi>C</mi>
   <mrow>
    <mi>n</mi>
    <mo>,</mo>
    <msup>
     <mi>n</mi>
     <mo>′</mo>
    </msup>
   </mrow>
   <mrow>
    <mi>b</mi>
    <mi>n</mi>
    <mi>n</mi>
   </mrow>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>C</ci>
     <apply>
      <times></times>
      <ci>b</ci>
      <ci>n</ci>
      <ci>n</ci>
     </apply>
    </apply>
    <list>
     <ci>n</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>n</ci>
      <ci>normal-′</ci>
     </apply>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   C^{bnn}_{n,n^{\prime}}
  </annotation>
 </semantics>
</math>

 the bagged nearest neighbour classifier based on a training set of size 

<math display="inline" id="Bootstrap_aggregating:9">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

, with resamples of size 

<math display="inline" id="Bootstrap_aggregating:10">
 <semantics>
  <msup>
   <mi>n</mi>
   <mo>′</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>n</ci>
    <ci>normal-′</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n^{\prime}
  </annotation>
 </semantics>
</math>

. In the infinite sampling case, under certain regularity conditions on the class distributions, the <a href="Bayes_classifier" title="wikilink">excess risk</a> has the following asymptotic expansion<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>

<p>

<math display="block" id="Bootstrap_aggregating:11">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <msub>
       <mi class="ltx_font_mathcaligraphic">ℛ</mi>
       <mi class="ltx_font_mathcaligraphic">ℛ</mi>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <msubsup>
        <mi>C</mi>
        <mrow>
         <mi>n</mi>
         <mo>,</mo>
         <msup>
          <mi>n</mi>
          <mo>′</mo>
         </msup>
        </mrow>
        <mrow>
         <mi>b</mi>
         <mi>n</mi>
         <mi>n</mi>
        </mrow>
       </msubsup>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>-</mo>
     <mrow>
      <msub>
       <mi class="ltx_font_mathcaligraphic">ℛ</mi>
       <mi class="ltx_font_mathcaligraphic">ℛ</mi>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <msup>
        <mi>C</mi>
        <mrow>
         <mi>B</mi>
         <mi>a</mi>
         <mi>y</mi>
         <mi>e</mi>
         <mi>s</mi>
        </mrow>
       </msup>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mo>(</mo>
      <mrow>
       <mrow>
        <msub>
         <mi>B</mi>
         <mn>1</mn>
        </msub>
        <mfrac>
         <msup>
          <mi>n</mi>
          <mo>′</mo>
         </msup>
         <mi>n</mi>
        </mfrac>
       </mrow>
       <mo>+</mo>
       <mrow>
        <msub>
         <mi>B</mi>
         <mn>2</mn>
        </msub>
        <mfrac>
         <mn>1</mn>
         <msup>
          <mrow>
           <mo stretchy="false">(</mo>
           <msup>
            <mi>n</mi>
            <mo>′</mo>
           </msup>
           <mo stretchy="false">)</mo>
          </mrow>
          <mrow>
           <mn>4</mn>
           <mo>/</mo>
           <mi>d</mi>
          </mrow>
         </msup>
        </mfrac>
       </mrow>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mrow>
      <mo stretchy="false">{</mo>
      <mrow>
       <mn>1</mn>
       <mo>+</mo>
       <mrow>
        <mi>o</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mn>1</mn>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo stretchy="false">}</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ℛ</ci>
       <ci>ℛ</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>C</ci>
        <apply>
         <times></times>
         <ci>b</ci>
         <ci>n</ci>
         <ci>n</ci>
        </apply>
       </apply>
       <list>
        <ci>n</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>n</ci>
         <ci>normal-′</ci>
        </apply>
       </list>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ℛ</ci>
       <ci>ℛ</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>C</ci>
       <apply>
        <times></times>
        <ci>B</ci>
        <ci>a</ci>
        <ci>y</ci>
        <ci>e</ci>
        <ci>s</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <plus></plus>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>B</ci>
        <cn type="integer">1</cn>
       </apply>
       <apply>
        <divide></divide>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>n</ci>
         <ci>normal-′</ci>
        </apply>
        <ci>n</ci>
       </apply>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>B</ci>
        <cn type="integer">2</cn>
       </apply>
       <apply>
        <divide></divide>
        <cn type="integer">1</cn>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <ci>n</ci>
          <ci>normal-′</ci>
         </apply>
         <apply>
          <divide></divide>
          <cn type="integer">4</cn>
          <ci>d</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
     <set>
      <apply>
       <plus></plus>
       <cn type="integer">1</cn>
       <apply>
        <times></times>
        <ci>o</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
     </set>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{R}_{\mathcal{R}}(C^{bnn}_{n,n^{\prime}})-\mathcal{R}_{\mathcal{R}}(C^%
{Bayes})=\left(B_{1}\frac{n^{\prime}}{n}+B_{2}\frac{1}{(n^{\prime})^{4/d}}%
\right)\{1+o(1)\},
  </annotation>
 </semantics>
</math>

 for some constants 

<math display="inline" id="Bootstrap_aggregating:12">
 <semantics>
  <msub>
   <mi>B</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>B</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B_{1}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Bootstrap_aggregating:13">
 <semantics>
  <msub>
   <mi>B</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>B</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B_{2}
  </annotation>
 </semantics>
</math>

. The optimal choice of 

<math display="inline" id="Bootstrap_aggregating:14">
 <semantics>
  <msup>
   <mi>n</mi>
   <mo>′</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>n</ci>
    <ci>normal-′</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n^{\prime}
  </annotation>
 </semantics>
</math>

, that balances the two terms in the asymptotic expansion, is given by 

<math display="inline" id="Bootstrap_aggregating:15">
 <semantics>
  <mrow>
   <msup>
    <mi>n</mi>
    <mo>′</mo>
   </msup>
   <mo>=</mo>
   <mrow>
    <mi>B</mi>
    <msup>
     <mi>n</mi>
     <mrow>
      <mi>d</mi>
      <mo>/</mo>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>d</mi>
        <mo>+</mo>
        <mn>4</mn>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </msup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>n</ci>
     <ci>normal-′</ci>
    </apply>
    <apply>
     <times></times>
     <ci>B</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>n</ci>
      <apply>
       <divide></divide>
       <ci>d</ci>
       <apply>
        <plus></plus>
        <ci>d</ci>
        <cn type="integer">4</cn>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n^{\prime}=Bn^{d/(d+4)}
  </annotation>
 </semantics>
</math>

 for some constant 

<math display="inline" id="Bootstrap_aggregating:16">
 <semantics>
  <mi>B</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>B</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B
  </annotation>
 </semantics>
</math>

.</p>
<h2 id="history">History</h2>

<p>Bagging (<strong>B</strong>ootstrap <strong>agg</strong>regat<strong>ing</strong>) was proposed by <a href="Leo_Breiman" title="wikilink">Leo Breiman</a> in 1994 to improve the classification by combining classifications of randomly generated training sets. See Breiman, 1994. Technical Report No. 421.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Boosting_(meta-algorithm)" title="wikilink">Boosting (meta-algorithm)</a></li>
<li><a href="Bootstrapping_(statistics)" title="wikilink">Bootstrapping (statistics)</a></li>
<li><a href="Cross-validation_(statistics)" title="wikilink">Cross-validation (statistics)</a></li>
<li><a href="Random_forest" title="wikilink">Random forest</a></li>
<li><a href="Random_subspace_method" title="wikilink">Random subspace method</a> (attribute bagging)</li>
</ul>
<h2 id="references">References</h2>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Ensemble_learning" title="wikilink">Category:Ensemble learning</a> <a href="Category:Machine_learning_algorithms" title="wikilink">Category:Machine learning algorithms</a> <a href="Category:Computational_statistics" title="wikilink">Category:Computational statistics</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Aslam, Javed A.; Popa, Raluca A.; and Rivest, Ronald L. (2007); <a href="http://people.csail.mit.edu/rivest/pubs/APR07.pdf"><em>On Estimating the Size and Confidence of a Statistical Audit</em></a>, Proceedings of the Electronic Voting Technology Workshop (EVT '07), Boston, MA, August 6, 2007. More generally, when drawing with replacement <em>n′</em> values out of a set of <em>n</em> (different and equally likely), the expected number of unique draws is 

<math display="inline" id="Bootstrap_aggregating:17">
 <semantics>
  <mrow>
   <mi>n</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <msup>
      <mi>e</mi>
      <mrow>
       <mo>-</mo>
       <mrow>
        <msup>
         <mi>n</mi>
         <mo>′</mo>
        </msup>
        <mo>/</mo>
        <mi>n</mi>
       </mrow>
      </mrow>
     </msup>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>n</ci>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>e</ci>
      <apply>
       <minus></minus>
       <apply>
        <divide></divide>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>n</ci>
         <ci>normal-′</ci>
        </apply>
        <ci>n</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n(1-e^{-n^{\prime}/n})
  </annotation>
 </semantics>
</math>

.<a href="#fnref1">↩</a></li>
<li id="fn2">Sahu, A., Runger, G., Apley, D., Image denoising with a multi-phase kernel principal component approach and an ensemble version, IEEE Applied Imagery Pattern Recognition Workshop, pp.1-7, 2011.<a href="#fnref2">↩</a></li>
<li id="fn3">Shinde, Amit, Anshuman Sahu, Daniel Apley, and George Runger. "Preimages for Variation Patterns from Kernel PCA and Bagging." IIE Transactions, Vol.46, Iss.5, 2014<a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
</ol>
</section>
</body>
</html>
