


Polynomial interpolation




Polynomial interpolation

In numerical analysis, polynomial interpolation is the interpolation of a given data set by a polynomial: given some points, find a polynomial which goes exactly through these points.
Applications
Polynomials can be used to approximate complicated curves, for example, the shapes of letters in typography, given a few points. A relevant application is the evaluation of the natural logarithm and trigonometric functions: pick a few known data points, create a lookup table, and interpolate between those data points. This results in significantly faster computations. Polynomial interpolation also forms the basis for algorithms in numerical quadrature and numerical ordinary differential equations.
Polynomial interpolation is also essential to perform sub-quadratic multiplication and squaring such as Karatsuba multiplication and Toom–Cook multiplication, where an interpolation through points on a polynomial which defines the product yields the product itself. For example, given a = f(x) = a0x0 + a1x1 + ... and b = g(x) = b0x0 + b1x1 + ..., the product ab is equivalent to W(x) = f(x)g(x). Finding points along W(x) by substituting x for small values in f(x) and g(x) yields points on the curve. Interpolation based on those points will yield the terms of W(x) and subsequently the product ab. In the case of Karatsuba multiplication this technique is substantially faster than quadratic multiplication, even for modest-sized inputs. This is especially true when implemented in parallel hardware.
Definition
Given a set of 
 
 
 
  data points  where no two  are the same, one is looking for a polynomial 
 
 
 
  of degree at most 
 
 
 
  with the property



The unisolvence theorem  states that such a polynomial p exists and is unique, and can be proved by the Vandermonde matrix, as described below.
The theorem states that for 
 
 
 
  interpolation nodes , polynomial interpolation defines a linear bijection



where Πn is the vector space of polynomials (defined on any interval containing the nodes) of degree at most 
 
 
 
 .
Constructing the interpolation polynomial
 Suppose that the interpolation polynomial is in the form


 
  The statement that p interpolates the data points means that


 
  If we substitute equation (1) in here, we get a system of linear equations in the coefficients . The system in matrix-vector form reads



We have to solve this system for  to construct the interpolant p(x). The matrix on the left is commonly referred to as a Vandermonde matrix.
The condition number of the Vandermonde matrix may be large,1 causing large errors when computing the coefficients  if the system of equations is solved using Gaussian elimination.
Several authors have therefore proposed algorithms which exploit the structure of the Vandermonde matrix to compute numerically stable solutions in O(n2) operations instead of the O(n3) required by Gaussian elimination.234 These methods rely on constructing first a Newton interpolation of the polynomial and then converting it to the monomial form above.
Alternatively, we may write down the polynomial immediately in terms of Lagrange polynomials:



For matrix arguments, this formula is called Sylvester's formula and the matrix-valued Lagrange polynomials are the Frobenius covariants.
Uniqueness of the interpolating polynomial
Proof 1
Suppose we interpolate through 
 
 
 
 
  data points with an at-most 
 
 
 
  degree polynomial p(x) (we need at least 
 
 
 
  datapoints or else the polynomial cannot be fully solved for). Suppose also another polynomial exists also of degree at most 
 
 
 
  that also interpolates the 
 
 
 
  points; call it q(x).
Consider 
 
 
 
 
 . We know,

r(x) is a polynomial
r(x) has degree at most 
 
 
 
 , since p(x) and q(x) are no higher than this and we are just subtracting them.
At the 
 
 
 
  data points, 
 
 
 
 . Therefore r(x) has 
 
 
 
  roots.

But r(x) is an polynomial of degree 
 
 
 
 
 . It has one root too many. Formally, if r(x) is any non-zero polynomial, it must be writable as 
 
 
 
 , for some constant A. By distributivity, the 
 
 
x's multiply together to give leading term 
 
 
 
 , i.e. one degree higher than the maximum we set. So the only way r(x) can exist is if 
 
 
 
 , or equivalently, 
 
 
 
 
 .



So q(x) (which could be any polynomial, so long as it interpolates the points) is identical with p(x), and p(x) is unique.
Proof 2
Given the Vandermonde matrix used above to construct the interpolant, we can set up the system



To prove that V is nonsingular we use the Vandermonde determinant formula:


 
  is never zero, therefore V is nonsingular and the system has a unique solution.
Either way this means that no matter what method we use to do our interpolation: direct, Lagrange etc., (assuming we can do all our calculations perfectly) we will always get the same polynomial.
Non-Vandermonde solutions
We are trying to construct our unique interpolation polynomial in the vector space Πn of polynomials of degree 
 
 
 
 
 . When using a monomial basis for Πn we have to solve the Vandermonde matrix to construct the coefficients  for the interpolation polynomial. This can be a very costly operation (as counted in clock cycles of a computer trying to do the job). By choosing another basis for Πn we can simplify the calculation of the coefficients but then we have to do additional calculations when we want to express the interpolation polynomial in terms of a monomial basis.
One method is to write the interpolation polynomial in the Newton form and use the method of divided differences to construct the coefficients, e.g. Neville's algorithm. The cost is O(n2) operations, while Gaussian elimination costs O(n3) operations. Furthermore, you only need to do O(n) extra work if an extra point is added to the data set, while for the other methods, you have to redo the whole computation.
Another method is to use the Lagrange form of the interpolation polynomial. The resulting formula immediately shows that the interpolation polynomial exists under the conditions stated in the above theorem. Lagrange formula is to be preferred to Vandermonde formula when we are not interested in computing the coefficients of the polynomial, but in computing the value of p(x) in a given x not in the original data set. In this case, we can reduce complexity to O(n2).5
The Bernstein form was used in a constructive proof of the Weierstrass approximation theorem by Bernstein and has nowadays gained great importance in computer graphics in the form of Bézier curves.
Interpolation error
When interpolating a given function f by a polynomial of degree 
 
 
 
  at the nodes x0,...,xn we get the error



where



is the notation for divided differences.
If f is 
 
 
 
  times continuously differentiable on a closed interval I and 
 
 
 
 
  be a polynomial of degree at most 
 
 
 
  that interpolates f at 
 
 
 
  distinct points {xi} (i=0,1,...,n) in that interval. Then for each x in the interval there exists 
 
 
 
  in that interval such that



Proof
Set the error term as



and set up an auxiliary function:



where



Since  are roots of 
 
 
 
  and 
 
 
 
 , we have  Y(xi) {{=}} 0}}, which means 
 
 
 
 
  has 
 
 
 
  roots. From Rolle's theorem, 
 
 
 
  has 
 
 
 
  roots, then 
 
 
 
  has one root 
 
 
 
 
 , where 
 
 
 
  is in the interval 
 
 
 
 .
So we can get



Since 
 
 
 
  is a polynomial of degree at most 
 
 
 
 , then



Thus



Since 
 
 
 
  is the root of 
 
 
 
 , so



Therefore


 
 .
Thus the remainder term in the Lagrange form of the Taylor theorem is a special case of interpolation error when all interpolation nodes  are identical.6
In the case of equally spaced interpolation nodes 
 
 
 
 , it follows that the interpolation error is O
 
 
 
 . However, this assumes that 
 
 
 
  is dominated by 
 
 
 
 
 , i.e. 
 
 
 
 . In several cases, this is not true and the error actually increases as 
 
 
 
  (see Runge's phenomenon). That question is treated in the section Convergence properties.
The above error bound suggests choosing the interpolation points  such that the product



is as small as possible. The Chebyshev nodes achieve this.
Lebesgue constants

See the main article: Lebesgue constant.


We fix the interpolation nodes x0, ..., xn and an interval [a, b] containing all the interpolation nodes. The process of interpolation maps the function f to a polynomial p. This defines a mapping X from the space C([a, b]) of all continuous functions on [a, b] to itself. The map X is linear and it is a projection on the subspace Πn of polynomials of degree n or less.
The Lebesgue constant L is defined as the operator norm of X. One has (a special case of Lebesgue's lemma):



In other words, the interpolation polynomial is at most a factor (L + 1) worse than the best possible approximation. This suggests that we look for a set of interpolation nodes that makes L small. In particular, we have for Chebyshev nodes:



We conclude again that Chebyshev nodes are a very good choice for polynomial interpolation, as the growth in n is exponential for equidistant nodes. However, those nodes are not optimal.
Convergence properties
It is natural to ask, for which classes of functions and for which interpolation nodes the sequence of interpolating polynomials converges to the interpolated function as 
 
 
 
 ? Convergence may be understood in different ways, e.g. pointwise, uniform or in some integral norm.
The situation is rather bad for equidistant nodes, in that uniform convergence is not even guaranteed for infinitely differentiable functions. One classical example, due to Carl Runge, is the function f(x) = 1 / (1 + x2) on the interval 
 
 
 
 . The interpolation error  f − pn{{!!}}∞}} grows without bound as 
 
 
 
 . Another example is the function f(x) = |x| on the interval 
 
 
 
 , for which the interpolating polynomials do not even converge pointwise except at the three points x = ±1, 0.7
One might think that better convergence properties may be obtained by choosing different interpolation nodes. The following result seems to give a rather encouraging answer:

Theorem. For any function f(x) continuous on an interval [a,b] there exists a table of nodes for which the sequence of interpolating polynomials 
 
 
 
 
  converges to f(x) uniformly on [a,b].
 

Proof. It's clear that the sequence of polynomials of best approximation 
 
 
 
  converges to f(x) uniformly (due to Weierstrass approximation theorem). Now we have only to show that each 
 
 
 
  may be obtained by means of interpolation on certain nodes. But this is true due to a special property of polynomials of best approximation known from the Chebyshev alternation theorem. Specifically, we know that such polynomials should intersect f(x) at least 
 
 
 
  times. Choosing the points of intersection as interpolation nodes we obtain the interpolating polynomial coinciding with the best approximation polynomial.
The defect of this method, however, is that interpolation nodes should be calculated anew for each new function f(x), but the algorithm is hard to be implemented numerically. Does there exist a single table of nodes for which the sequence of interpolating polynomials converge to any continuous function f(x)? The answer is unfortunately negative:

Theorem. For any table of nodes there is a continuous function f(x) on an interval [a, b] for which the sequence of interpolating polynomials diverges on [a,b].8


The proof essentially uses the lower bound estimation of the Lebesgue constant, which we defined above to be the operator norm of Xn (where Xn is the projection operator on Πn). Now we seek a table of nodes for which



Due to the Banach–Steinhaus theorem, this is only possible when norms of Xn are uniformly bounded, which cannot be true since we know that



For example, if equidistant points are chosen as interpolation nodes, the function from Runge's phenomenon demonstrates divergence of such interpolation. Note that this function is not only continuous but even infinitely times differentiable on 
 
 
 
 . For better Chebyshev nodes, however, such an example is much harder to find due to the following result:

Theorem. For every absolutely continuous function on 
 
 
 
  the sequence of interpolating polynomials constructed on Chebyshev nodes converges to f(x) uniformly.
 

Related concepts
Runge's phenomenon shows that for high values of $n$, the interpolation polynomial may oscillate wildly between the data points. This problem is commonly resolved by the use of spline interpolation. Here, the interpolant is not a polynomial but a spline: a chain of several polynomials of a lower degree.
Interpolation of periodic functions by harmonic functions is accomplished by Fourier transform. This can be seen as a form of polynomial interpolation with harmonic base functions, see trigonometric interpolation and trigonometric polynomial.
Hermite interpolation problems are those where not only the values of the polynomial p at the nodes are given, but also all derivatives up to a given order. This turns out to be equivalent to a system of simultaneous polynomial congruences, and may be solved by means of the Chinese remainder theorem for polynomials. Birkhoff interpolation is a further generalization where only derivatives of some orders are prescribed, not necessarily all orders from 0 to a k.
Collocation methods for the solution of differential and integral equations are based on polynomial interpolation.
The technique of rational function modeling is a generalization that considers ratios of polynomial functions.
At last, multivariate interpolation for higher dimensions.
See also

Newton series

Notes
References










External links


ALGLIB has an implementations in C++ / C# / VBA / Pascal.
GSL has a polynomial interpolation code in C
Interpolating Polynomial by Stephen Wolfram, the Wolfram Demonstrations Project.

"
Category:Interpolation Category:Polynomials Category:Articles containing proofs



↩
↩
↩
↩
R.Bevilaqua, D. Bini, M.Capovani and O. Menchi (2003). Appunti di Calcolo Numerico. Chapter 5, p. 89. Servizio Editoriale Universitario Pisa - Azienda Regionale Diritto allo Studio Universitario.↩
↩
 attributes the last example to .↩
 attributes this theorem to .↩




