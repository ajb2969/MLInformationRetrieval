   Law of total cumulance      Law of total cumulance   In probability theory and mathematical  statistics , the law of total cumulance is a generalization to cumulants of the law of total probability , the law of total expectation , and the law of total variance . It has applications in the analysis of time series . It was introduced by David Brillinger. 1  It is most transparent when stated in its most general form, for joint cumulants, rather than for cumulants of a specified order for just one random variable . In general, we have      κ   (   X  1   ,  …  ,   X  n   )   =   ∑  π   κ   (  κ   (   X  i   :  i  ∈  B  ∣  Y  )   :  B  ∈  π  )   ,     fragments  κ   fragments  normal-(   subscript  X  1   normal-,  normal-…  normal-,   subscript  X  n   normal-)     subscript   π   κ   fragments  normal-(  κ   fragments  normal-(   subscript  X  i   normal-:  i   B  normal-∣  Y  normal-)   normal-:  B   π  normal-)   normal-,    \kappa(X_{1},\dots,X_{n})=\sum_{\pi}\kappa(\kappa(X_{i}:i\in B\mid Y):B\in\pi),     where   κ( X 1 , ..., X n ) is the joint cumulant of n random variables X 1 , ..., X n , and    the sum is over all partitions    π   π   \pi   of the set { 1, ..., n } of indices, and    " B ∈ π" means B runs through the whole list of "blocks" of the partition π, and    κ( X i : i ∈ B | Y ) is a conditional cumulant given the value of the random variable Y . It is therefore a random variable in its own right—a function of the random variable Y .   Examples  ===The special case of just one random variable and n = 2 or 3===  Only in case n = either 2 or 3 is the n th cumulant the same as the n th central moment . The case n = 2 is well-known (see law of total variance ). Below is the case n = 3. The notation μ 3 means the third central moment.       μ  3    (  X  )   =  E   (   μ  3    (  X  ∣  Y  )   )   +   μ  3    (  E   (  X  ∣  Y  )   )   +   3   cov   (  E   (  X  ∣  Y  )   ,  var   (  X  ∣  Y  )   )   .     fragments   subscript  μ  3    fragments  normal-(  X  normal-)    E   fragments  normal-(   subscript  μ  3    fragments  normal-(  X  normal-∣  Y  normal-)   normal-)     subscript  μ  3    fragments  normal-(  E   fragments  normal-(  X  normal-∣  Y  normal-)   normal-)    3  cov   fragments  normal-(  E   fragments  normal-(  X  normal-∣  Y  normal-)   normal-,  var   fragments  normal-(  X  normal-∣  Y  normal-)   normal-)   normal-.    \mu_{3}(X)=E(\mu_{3}(X\mid Y))+\mu_{3}(E(X\mid Y))+3\,\operatorname{cov}(E(X%
 \mid Y),\operatorname{var}(X\mid Y)).\,     General 4th-order joint cumulants  For general 4th-order cumulants, the rule gives a sum of 15 terms, as follows:      κ   (   X  1   ,   X  2   ,   X  3   ,   X  4   )       κ    subscript  X  1    subscript  X  2    subscript  X  3    subscript  X  4      \kappa(X_{1},X_{2},X_{3},X_{4})\,           =  κ   (  κ   (   X  1   ,   X  2   ,   X  3   ,   X  4   ∣  Y  )   )      fragments   κ   fragments  normal-(  κ   fragments  normal-(   subscript  X  1   normal-,   subscript  X  2   normal-,   subscript  X  3   normal-,   subscript  X  4   normal-∣  Y  normal-)   normal-)     =\kappa(\kappa(X_{1},X_{2},X_{3},X_{4}\mid Y))\,          \left.\begin{matrix}     & {}+\kappa(\kappa(X_1,X_2,X_3\mid Y),\kappa(X_4\mid Y)) \\ \\ & {}+\kappa(\kappa(X_1,X_2,X_4\mid Y),\kappa(X_3\mid Y)) \\ \\ & {}+\kappa(\kappa(X_1,X_3,X_4\mid Y),\kappa(X_2\mid Y)) \\ \\ & {}+\kappa(\kappa(X_2,X_3,X_4\mid Y),\kappa(X_1\mid Y)) \end{matrix}\right\}(\mathrm{partitions}\ \mathrm{of}\ \mathrm{the}\ 3+1\ \mathrm{form})    \left.\begin{matrix}     & {}+\kappa(\kappa(X_1,X_2\mid Y),\kappa(X_3,X_4\mid Y)) \\ \\ & {}+\kappa(\kappa(X_1,X_3\mid Y),\kappa(X_2,X_4\mid Y)) \\ \\ & {}+\kappa(\kappa(X_1,X_4\mid Y),\kappa(X_2,X_3\mid Y))\end{matrix}\right\}(\mathrm{partitions}\ \mathrm{of}\ \mathrm{the}\ 2+2\ \mathrm{form})    \left.\begin{matrix}     & {}+\kappa(\kappa(X_1,X_2\mid Y),\kappa(X_3\mid Y),\kappa(X_4\mid Y)) \\ \\ & {}+\kappa(\kappa(X_1,X_3\mid Y),\kappa(X_2\mid Y),\kappa(X_4\mid Y)) \\ \\ & {}+\kappa(\kappa(X_1,X_4\mid Y),\kappa(X_2\mid Y),\kappa(X_3\mid Y)) \\ \\ & {}+\kappa(\kappa(X_2,X_3\mid Y),\kappa(X_1\mid Y),\kappa(X_4\mid Y)) \\ \\ & {}+\kappa(\kappa(X_2,X_4\mid Y),\kappa(X_1\mid Y),\kappa(X_3\mid Y)) \\ \\ & {}+\kappa(\kappa(X_3,X_4\mid Y),\kappa(X_1\mid Y),\kappa(X_2\mid Y)) \end{matrix}\right\}(\mathrm{partitions}\ \mathrm{of}\ \mathrm{the}\ 2+1+1\ \mathrm{form})        +  κ   (  κ   (   X  1   ∣  Y  )   ,  κ   (   X  2   ∣  Y  )   ,  κ   (   X  3   ∣  Y  )   ,  κ   (   X  4   ∣  Y  )   )   .     fragments   κ   fragments  normal-(  κ   fragments  normal-(   subscript  X  1   normal-∣  Y  normal-)   normal-,  κ   fragments  normal-(   subscript  X  2   normal-∣  Y  normal-)   normal-,  κ   fragments  normal-(   subscript  X  3   normal-∣  Y  normal-)   normal-,  κ   fragments  normal-(   subscript  X  4   normal-∣  Y  normal-)   normal-)   normal-.    {}+\kappa(\kappa(X_{1}\mid Y),\kappa(X_{2}\mid Y),\kappa(X_{3}\mid Y),\kappa(X%
 _{4}\mid Y)).\,        Cumulants of compound Poisson random variables  Suppose Y has a Poisson distribution with expected value 1, and X is the sum of Y  independent copies of W .       X  =    ∑   y  =  1   Y    W  y     .      X    superscript   subscript     y  1    Y    subscript  W  y      X=\sum_{y=1}^{Y}W_{y}.\,     All of the cumulants of the Poisson distribution are equal to each other, and so in this case are equal to 1. Also recall that if random variables W 1 , ..., W m are independent , then the n th cumulant is additive:         κ  n    (    W  1   +  ⋯  +   W  m    )    =     κ  n    (   W  1   )    +  ⋯  +    κ  n    (   W  m   )      .         subscript  κ  n      subscript  W  1   normal-⋯   subscript  W  m          subscript  κ  n    subscript  W  1    normal-⋯     subscript  κ  n    subscript  W  m       \kappa_{n}(W_{1}+\cdots+W_{m})=\kappa_{n}(W_{1})+\cdots+\kappa_{n}(W_{m}).\,     We will find the 4th cumulant of X . We have:        κ  4    (  X  )    =   κ   (  X  ,  X  ,  X  ,  X  )           subscript  κ  4   X     κ   X  X  X  X      \kappa_{4}(X)=\kappa(X,X,X,X)\,           =   κ  1    (   κ  4    (  X  ∣  Y  )   )   +  4  κ   (   κ  3    (  X  ∣  Y  )   ,   κ  1    (  X  ∣  Y  )   )   +  3   κ  2    (   κ  2    (  X  ∣  Y  )   )      fragments    subscript  κ  1    fragments  normal-(   subscript  κ  4    fragments  normal-(  X  normal-∣  Y  normal-)   normal-)    4  κ   fragments  normal-(   subscript  κ  3    fragments  normal-(  X  normal-∣  Y  normal-)   normal-,   subscript  κ  1    fragments  normal-(  X  normal-∣  Y  normal-)   normal-)    3   subscript  κ  2    fragments  normal-(   subscript  κ  2    fragments  normal-(  X  normal-∣  Y  normal-)   normal-)     =\kappa_{1}(\kappa_{4}(X\mid Y))+4\kappa(\kappa_{3}(X\mid Y),\kappa_{1}(X\mid Y%
 ))+3\kappa_{2}(\kappa_{2}(X\mid Y))\,              +  6  κ   (   κ  2    (  X  ∣  Y  )   ,   κ  1    (  X  ∣  Y  )   ,   κ  1    (  X  ∣  Y  )   )   +   κ  4    (   κ  1    (  X  ∣  Y  )   )      fragments   6  κ   fragments  normal-(   subscript  κ  2    fragments  normal-(  X  normal-∣  Y  normal-)   normal-,   subscript  κ  1    fragments  normal-(  X  normal-∣  Y  normal-)   normal-,   subscript  κ  1    fragments  normal-(  X  normal-∣  Y  normal-)   normal-)     subscript  κ  4    fragments  normal-(   subscript  κ  1    fragments  normal-(  X  normal-∣  Y  normal-)   normal-)     {}+6\kappa(\kappa_{2}(X\mid Y),\kappa_{1}(X\mid Y),\kappa_{1}(X\mid Y))+\kappa%
 _{4}(\kappa_{1}(X\mid Y))\,          =\kappa_1(Y\kappa_4(W))+4\kappa(Y\kappa_3(W),Y\kappa_1(W))     +3\kappa_2(Y\kappa_2(W))\,    {}+6\kappa(Y\kappa_2(W),Y\kappa_1(W),Y\kappa_1(W))     +\kappa_4(Y\kappa_1(W))\,    =\kappa_4(W)\kappa_1(Y)+4\kappa_3(W)\kappa_1(W)\kappa_2(Y)     +3\kappa_2(W)^2 \kappa_2(Y)\,         +   6   κ  2    (  W  )    κ  1     (  W  )   2    κ  3    (  Y  )     +    κ  1     (  W  )   4    κ  4    (  Y  )            6   subscript  κ  2   W   subscript  κ  1    superscript  W  2    subscript  κ  3   Y       subscript  κ  1    superscript  W  4    subscript  κ  4   Y     {}+6\kappa_{2}(W)\kappa_{1}(W)^{2}\kappa_{3}(Y)+\kappa_{1}(W)^{4}\kappa_{4}(Y)\,          =\kappa_4(W)+4\kappa_3(W)\kappa_1(W)     +3\kappa_2(W)^2+6\kappa_2(W) \kappa_1(W)^2+\kappa_1(W)^4.\,         =   E   (   W  4   )        absent    E   superscript  W  4      =E(W^{4})\,   (the punch line—see the explanation below).     We recognize this last sum as the sum over all partitions of the set { 1, 2, 3, 4 }, of the product over all blocks of the partition, of cumulants of W of order equal to the size of the block. That is precisely the 4th raw moment of W (see cumulant for a more leisurely discussion of this fact). Hence the moments of W are the cumulants of X .  In this way we see that every moment sequence is also a cumulant sequence (the converse cannot be true, since cumulants of even order ≥ 4 are in some cases negative, and also because the cumulant sequence of the normal distribution is not a moment sequence of any probability distribution).  Conditioning on a Bernoulli random variable  Suppose Y = 1 with probability p and Y = 0 with probability q = 1 − p . Suppose the conditional probability distribution of X given Y is F if Y = 1 and G if Y = 0. Then we have        κ  n    (  X  )    =    p   κ  n    (  F  )    +   q   κ  n    (  G  )    +    ∑   π  <   1  ^       κ   |  π  |     (  Y  )     ∏   B  ∈  π     (     κ   |  B  |     (  F  )    -    κ   |  B  |     (  G  )     )              subscript  κ  n   X       p   subscript  κ  n   F     q   subscript  κ  n   G     subscript     π   normal-^  1        subscript  κ    π    Y    subscript  product    B  π         subscript  κ    B    F      subscript  κ    B    G          \kappa_{n}(X)=p\kappa_{n}(F)+q\kappa_{n}(G)+\sum_{\pi<\widehat{1}}\kappa_{%
 \left|\pi\right|}(Y)\prod_{B\in\pi}(\kappa_{\left|B\right|}(F)-\kappa_{\left|B%
 \right|}(G))     where    π  <   1  ^       π   normal-^  1     \pi<\widehat{1}   means π is a partition of the set { 1, ..., n } that is finer than the coarsest partition – the sum is over all partitions except that one. For example, if n = 3, then we have         κ  3    (  X  )    =    p   κ  3    (  F  )    +   q   κ  3    (  G  )    +   3  p  q   (     κ  2    (  F  )    -    κ  2    (  G  )     )    (     κ  1    (  F  )    -    κ  1    (  G  )     )    +   p  q   (   q  -  p   )     (     κ  1    (  F  )    -    κ  1    (  G  )     )   3      .         subscript  κ  3   X       p   subscript  κ  3   F     q   subscript  κ  3   G     3  p  q       subscript  κ  2   F      subscript  κ  2   G         subscript  κ  1   F      subscript  κ  1   G       p  q    q  p    superscript       subscript  κ  1   F      subscript  κ  1   G    3       \kappa_{3}(X)=p\kappa_{3}(F)+q\kappa_{3}(G)+3pq(\kappa_{2}(F)-\kappa_{2}(G))(%
 \kappa_{1}(F)-\kappa_{1}(G))+pq(q-p)(\kappa_{1}(F)-\kappa_{1}(G))^{3}.\,     References  "  Category:Algebra of random variables  Category:Theory of probability distributions  Category:Statistical theorems  Category:Statistical laws     David Brillinger, "The calculation of cumulants via conditioning", Annals of the Institute of Statistical Mathematics , Vol. 21 (1969), pp. 215–218. ↩     