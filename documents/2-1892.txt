


Markov property




Markov property

(Figure)
A single realisation of three-dimensional Brownian motion for times 0 ≤ t ≤ 2. Brownian motion has the Markov property, as the velocity of the particle does not depend on its past velocity.

In probability theory and statistics, the term Markov property refers to the memoryless property of a stochastic process. It is named after the Russian mathematician Andrey Markov.1
A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it. A process with this property is called a Markov process. The term strong Markov property is similar to the Markov property, except that the meaning of "present" is defined in terms of a random variable known as a stopping time. Both the terms "Markov property" and "strong Markov property" have been used in connection with a particular "memoryless" property of the exponential distribution.2
The term Markov assumption is used to describe a model where the Markov property is assumed to hold, such as a hidden Markov model.
A Markov random field3 extends this property to two or more dimensions or to random variables defined for an interconnected network of items. An example of a model for such a field is the Ising model.
A discrete-time stochastic process satisfying the Markov property is known as a Markov chain.
Introduction
A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present values) depends only upon the present state; that is, given the present, the future does not depend on the past. A process with this property is said to be Markovian or a Markov process. The most famous Markov process is a Markov chain. Brownian motion is another well-known Markov process.
History
Definition
Let 
 
 
 
  be a probability space with a filtration

 
 , for some (totally ordered) index set 
 
 
 
 ; and let 
 
 
 
  be a measurable space. A 
 
 
 
 -valued stochastic process 
 
 
 
  adapted to the filtration is said to possess the Markov property if, for each 
 
 
 
  and each 
 
 
 
  with 
 
 
4
In the case where 
 
 
 
  is a discrete set with the discrete sigma algebra and 
 
 
 
 , this can be reformulated as follows:


 
 .
Alternative formulations
Alternatively, the Markov property can be formulated as follows.



for all 
 
 
 
  and 
 
 
 
  bounded and measurable.
Strong Markov property
Suppose that 
 
 
 
  is a stochastic process on a probability space

 
  with natural filtration

 
 . For any 
 
 
 
 , we can define the germ sigma algebra 
 
 
 
  to be the intersection of all 
 
 
 
  for 
 
 
 
 . Then for any stopping time

 
  on 
 
 
 
 , we can define


 
 .
Then 
 
 
 
  is said to have the strong Markov property if, for each stopping time

 
 , conditioned on the event 
 
 
 
 , we have that for each 
 
 
 
 , 
 
 
 
  is independent of 
 
 
 
  given 
 
 
 
 .
The strong Markov property implies the ordinary Markov property, since by taking the stopping time $\tau=t$, the ordinary Markov property can be deduced.
Examples
Assume that an urn contains two red balls and one green ball. One ball was drawn yesterday, one ball was drawn today, and the final ball will be drawn tomorrow. All of the draws are "without replacement".
Suppose you know that today's ball was red, but you have no information about yesterday's ball. The chance that tomorrow's ball will be red is 1/2. That's because the only two remaining outcomes for this random experiment are:




Day

Outcome 1

Outcome 2





Yesterday

Red

Green



Today

Red

Red



Tomorrow

Green

Red



On the other hand, if you know that both today and yesterday's balls were red, then you are guaranteed to get a green ball tomorrow.
This discrepancy shows that the probability distribution for tomorrow's color depends not only on the present value, but is also affected by information about the past. This stochastic process of observed colors doesn't have the Markov property. Using the same experiment above, if sampling "without replacement" is changed to sampling "with replacement," the process of observed colors will have the Markov property.5
An application of the Markov property in a generalized form is in Markov chain Monte Carlo computations in the context of Bayesian statistics.
See also

Markov chain
Markov blanket
Markov decision process
Causal Markov condition
Markov model
Chapman–Kolmogorov equation

References
"
Category:Markov models Category:Markov processes



Markov, A. A. (1954). Theory of Algorithms. [Translated by Jacques J. Schorr-Kon and PST staff] Imprint Moscow, Academy of Sciences of the USSR, 1954 [Jerusalem, Israel Program for Scientific Translations, 1961; available from Office of Technical Services, United States Department of Commerce] Added t.p. in Russian Translation of Works of the Mathematical Institute, Academy of Sciences of the USSR, v. 42. Original title: Teoriya algorifmov. [QA248.M2943 Dartmouth College library. U.S. Dept. of Commerce, Office of Technical Services, number OTS 60-51085.]
Feller, W. (1971) Introduction to Probability Theory and Its Applications, Vol II (2nd edition),Wiley. ISBN 0-471-25709-5 (pages 9 and 20)
Dodge, Y. (2003) The Oxford Dictionary of Statistical Terms OUP. ISBN 0-19-850994-4
Durrett, Rick. Probability: Theory and Examples. Fourth Edition. Cambridge: Cambridge University Press, 2010.
http://math.stackexchange.com/questions/89394/example-of-a-stochastic-process-which-does-not-have-the-markov-property




