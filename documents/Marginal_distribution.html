<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="1350">Marginal distribution</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Marginal distribution</h1>
<hr/>
<p>In <a href="probability_theory" title="wikilink">probability theory</a> and <a class="uri" href="statistics" title="wikilink">statistics</a>, the <strong>marginal distribution</strong> of a <a class="uri" href="subset" title="wikilink">subset</a> of a collection of <a href="random_variable" title="wikilink">random variables</a> is the <a href="probability_distribution" title="wikilink">probability distribution</a> of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables. This contrasts with a <a href="conditional_distribution" title="wikilink">conditional distribution</a>, which gives the probabilities contingent upon the values of the other variables.</p>
<p>The term <strong>marginal variable</strong> is used to refer to those variables in the subset of variables being retained. These terms are dubbed "marginal" because they used to be found by summing values in a table along rows or columns, and writing the sum in the margins of the table.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> The distribution of the marginal variables (the marginal distribution) is obtained by <strong>marginalizing</strong> over the distribution of the variables being discarded, and the discarded variables are said to have been <strong>marginalized out</strong>.</p>
<p>The context here is that the theoretical studies being undertaken, or the <a href="data_analysis" title="wikilink">data analysis</a> being done, involves a wider set of random variables but that attention is being limited to a reduced number of those variables. In many applications an analysis may start with a given collection of random variables, then first extend the set by defining new ones (such as the sum of the original random variables) and finally reduce the number by placing interest in the marginal distribution of a subset (such as the sum). Several different analyses may be done, each treating a different subset of variables as the marginal variables.</p>
<h2 id="two-variable-case">Two-variable case</h2>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><p>|</p></th>
<th style="text-align: left;"><p>x<sub>1</sub></p></th>
<th style="text-align: left;"><p>x<sub>2</sub></p></th>
<th style="text-align: left;"><p>x<sub>3</sub></p></th>
<th style="text-align: left;"><p>x<sub>4</sub></p></th>
<th style="text-align: left;"><p>p<sub>y</sub>(Y)↓</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><p>|y<sub>1</sub></p></td>
<td style="text-align: left;"><p>|<sup>4</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p><sup>2</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p><sup>1</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p><sup>1</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p>| <sup>8</sup>⁄<sub>32</sub></p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>|y<sub>2</sub></p></td>
<td style="text-align: left;"><p>| <sup>2</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p><sup>4</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p><sup>1</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p><sup>1</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p>| <sup>8</sup>⁄<sub>32</sub></p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>|y<sub>3</sub></p></td>
<td style="text-align: left;"><p>| <sup>2</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p><sup>2</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p><sup>2</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p><sup>2</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p>| <sup>8</sup>⁄<sub>32</sub></p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>|y<sub>4</sub></p></td>
<td style="text-align: left;"><p>| <sup>8</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p>0</p></td>
<td style="text-align: left;"><p>0</p></td>
<td style="text-align: left;"><p>0</p></td>
<td style="text-align: left;"><p>| <sup>8</sup>⁄<sub>32</sub></p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>p<sub>x</sub>(X) →</p></td>
<td style="text-align: left;"><p>| <sup>16</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p><sup>8</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p><sup>4</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p><sup>4</sup>⁄<sub>32</sub></p></td>
<td style="text-align: left;"><p>| <sup>32</sup>⁄<sub>32</sub></p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>Joint and marginal distributions of a pair of discrete, random variables X,Y having nonzero <a href="mutual_information" title="wikilink">mutual information</a> I(X; Y). The values of the joint distribution are in the 4×4 square, and the values of the marginal distributions are along the right and bottom margins.</p></td>
</tr>
</tbody>
</table>
<p>Given two <a href="random_variable" title="wikilink">random variables</a> <em>X</em> and <em>Y</em> whose <a href="joint_distribution" title="wikilink">joint distribution</a> is known, the marginal distribution of <em>X</em> is simply the <a href="probability_distribution" title="wikilink">probability distribution</a> of <em>X</em> averaging over information about <em>Y</em>. It is the probability distribution of <em>X</em> when the value of <em>Y</em> is not known. This is typically calculated by summing or integrating the <a href="joint_probability" title="wikilink">joint probability</a> distribution over <em>Y</em>.</p>
<p>For <a href="discrete_random_variable" title="wikilink">discrete random variables</a>, the marginal <a href="probability_mass_function" title="wikilink">probability mass function</a> can be written as Pr(<em>X</em> = <em>x</em>). This is</p>
<p><span class="LaTeX">$$\Pr(X=x) = \sum_{y} \Pr(X=x,Y=y) = \sum_{y} \Pr(X=x|Y=y) \Pr(Y=y),$$</span></p>
<p>where Pr(<em>X</em> = <em>x</em>,<em>Y</em> = <em>y</em>) is the <a href="joint_distribution" title="wikilink">joint distribution</a> of <em>X</em> and <em>Y</em>, while Pr(<em>X</em> = <em>x</em>|<em>Y</em> = <em>y</em>) is the <a href="conditional_distribution" title="wikilink">conditional distribution</a> of <em>X</em> given <em>Y</em>. In this case, the variable <em>Y</em> has been marginalized out.</p>
<p>Bivariate marginal and joint probabilities for discrete random variables are often displayed as <a href="Frequency_distribution#Joint_frequency_distributions" title="wikilink">two-way tables</a>.</p>
<p>Similarly for <a href="continuous_random_variable" title="wikilink">continuous random variables</a>, the marginal <a href="probability_density_function" title="wikilink">probability density function</a> can be written as <em>p</em><sub><em>X</em></sub>(<em>x</em>). This is</p>
<p><span class="LaTeX">$$p_{X}(x) = \int_y p_{X,Y}(x,y) \, \operatorname{d}\!y = \int_y p_{X|Y}(x|y) \, p_Y(y) \, \operatorname{d}\!y ,$$</span></p>
<p>where <em>p</em><sub><em>X</em>,<em>Y</em></sub>(<em>x</em>,<em>y</em>) gives the joint distribution of <em>X</em> and <em>Y</em>, while <em>p</em><sub><em>X</em>|<em>Y</em></sub>(<em>x</em>|<em>y</em>) gives the conditional distribution for <em>X</em> given <em>Y</em>. Again, the variable <em>Y</em> has been marginalized out.</p>
<p>Note that a marginal probability can always be written as an <a href="expected_value" title="wikilink">expected value</a>:</p>
<p><span class="LaTeX">$$p_{X}(x) = \int_y p_{X|Y}(x|y) \, p_Y(y) \, \operatorname{d}\!y = \mathbb{E}_{Y} [p_{X|Y}(x|Y)]$$</span></p>
<p>Intuitively, the marginal probability of <em>X</em> is computed by examining the conditional probability of <em>X</em> given a particular value of <em>Y</em>, and then averaging this conditional probability over the distribution of all values of <em>Y</em>.</p>
<p>This follows from the definition of expected value, i.e. in general</p>
<p><span class="LaTeX">$$\mathbb{E}_Y [f(Y)] = \int_y f(y) p_Y(y) \, \operatorname{d}\!y$$</span></p>
<h2 id="real-world-example">Real-world example</h2>
<p>Suppose that the probability that a pedestrian will be hit by a car while crossing the road at a pedestrian crossing without paying attention to the traffic light is to be computed. Let H be a <a href="discrete_random_variable" title="wikilink">discrete random variable</a> taking one value from {Hit, Not Hit}. Let L be a discrete random variable taking one value from {Red, Yellow, Green}.</p>
<p>Realistically, H will be dependent on L. That is, P(H = Hit) and P(H = Not Hit) will take different values depending on whether L is red, yellow or green. A person is, for example, far more likely to be hit by a car when trying to cross while the lights for cross traffic are green than if they are red. In other words, for any given possible pair of values for H and L, one must consider the <a href="joint_probability_distribution" title="wikilink">joint probability distribution</a> of H and L to find the probability of that pair of events occurring together if the pedestrian ignores the state of the light.</p>
<p>However, in trying to calculate the <strong>marginal probability</strong> P(H=hit), what we are asking for is the probability that H=Hit in the situation in which we don't actually know the particular value of L and in which the pedestrian ignores the state of the light. In general a pedestrian can be hit if the lights are red OR if the lights are yellow OR if the lights are green. So in this case the answer for the marginal probability can be found by summing P(H,L) for all possible values of L, with each value of L weighted by its probability of occurring.</p>
<p>Here is a table showing the conditional probabilities of being hit, depending on the state of the lights. (Note that the columns in this table must add up to 1 because the probability of being hit or not hit is 1 regardless of the state of the light.)</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><p>Conditional distribution: P(H|L)</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>H=Not Hit</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>H=Hit</p></td>
</tr>
</tbody>
</table>
<p>To find the joint probability distribution, we need more data. Let's say that P(L=red) = 0.2, P(L=yellow) = 0.1, and P(L=green) = 0.7. Multiplying each column in the conditional distribution by the probability of that column occurring, we find the joint probability distribution of H and L, given in the central 2×3 block of entries. (Note that the cells in this 2×3 block add up to 1).</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><p>Joint distribution: P(H,L)</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>H=Not Hit</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>H=Hit</p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>Total</p></td>
</tr>
</tbody>
</table>
<p>The marginal probability P(H=Hit) is the sum along the H=Hit row of this joint distribution table, as this is the probability of being hit when the lights are red OR yellow OR green. Similarly, the marginal probability that P(H=Not Hit) is the sum of the H=Not Hit row.</p>
<h2 id="multivariate-distributions">Multivariate distributions</h2>
<figure><b>(Figure)</b>
<figcaption>Many samples from a bivariate normal distribution. The marginal distributions are shown in red and blue. The marginal distribution of X is also approximated by creating a histogram of the X coordinates without consideration of the Y coordinates.</figcaption>
</figure>
<p>For <a href="multivariate_distribution" title="wikilink">multivariate distributions</a>, formulae similar to those above apply with the symbols <em>X</em> and/or <em>Y</em> being interpreted as vectors. In particular, each summation or integration would be over all variables except those contained in <em>X</em>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Joint_probability_distribution" title="wikilink">Joint probability distribution</a></li>
<li><a href="Wasserstein_metric" title="wikilink">Wasserstein metric</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li></li>
<li></li>
</ul>
<p>"</p>
<p><a href="Category:Probability_theory" title="wikilink">Category:Probability theory</a> <a href="Category:Theory_of_probability_distributions" title="wikilink">Category:Theory of probability distributions</a> <a href="Category:Statistical_terminology" title="wikilink">Category:Statistical terminology</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Trumpler and Weaver (1962), pp. 32–33.<a href="#fnref1">↩</a></li>
</ol>
</section>
</body>
</html>
