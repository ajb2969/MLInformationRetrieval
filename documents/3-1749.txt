   CramÃ©râ€“Rao bound      CramÃ©râ€“Rao bound  In [[estimation theory]] and [[statistics]], the '''CramÃ©râ€“Rao bound (CRB)''' or '''CramÃ©râ€“Rao lower bound (CRLB)''', named in honor of [[Harald CramÃ©r]] and [[Calyampudi Radhakrishna Rao] ] who were among the first to derive it, 1 2 3 expresses a lower bound on the variance of estimators of a deterministic parameter. The bound is also known as the CramÃ©râ€“Rao inequality or the information inequality .  In its simplest form, the bound states that the variance of any unbiased estimator is at least as high as the inverse of the Fisher information . An unbiased estimator which achieves this lower bound is said to be (fully) efficient . Such a solution achieves the lowest possible mean squared error among all unbiased methods, and is therefore the minimum variance unbiased (MVU) estimator. However, in some cases, no unbiased technique exists which achieves the bound. This may occur even when an MVU estimator exists.  The CramÃ©râ€“Rao bound can also be used to bound the variance of biased estimators of given bias. In some cases, a biased approach can result in both a variance and a mean squared error that are below the unbiased CramÃ©râ€“Rao lower bound; see estimator bias .  Statement  The Cramerâ€“Rao bound is stated in this section for several increasingly general cases, beginning with the case in which the parameter is a scalar and its estimator is unbiased . All versions of the bound require certain regularity conditions, which hold for most well-behaved distributions. These conditions are listed later in this section .  Scalar unbiased case  Suppose   Î¸   Î¸   \theta   is an unknown deterministic parameter which is to be estimated from measurements   x   x   x   , distributed according to some probability density function     f   (  x  ;  Î¸  )       f   x  Î¸     f(x;\theta)   . The variance of any unbiased estimator    Î¸  ^     normal-^  Î¸    \hat{\theta}   of   Î¸   Î¸   \theta   is then bounded by the reciprocal of the Fisher information     I   (  Î¸  )       I  Î¸    I(\theta)   :       var   (   Î¸  ^   )    â‰¥   1   I   (  Î¸  )           var   normal-^  Î¸      1    I  Î¸      \mathrm{var}(\hat{\theta})\geq\frac{1}{I(\theta)}   where the Fisher information    I   (  Î¸  )       I  Î¸    I(\theta)   is defined by       I   (  Î¸  )    =   E   [    (     âˆ‚  â„“    (  x  ;  Î¸  )     âˆ‚  Î¸    )   2   ]    =   -   E   [      âˆ‚  2   â„“    (  x  ;  Î¸  )     âˆ‚   Î¸  2     ]             I  Î¸     normal-E   delimited-[]   superscript        normal-â„“    x  Î¸      Î¸    2              normal-E   delimited-[]        superscript   2   normal-â„“    x  Î¸       superscript  Î¸  2           I(\theta)=\mathrm{E}\left[\left(\frac{\partial\ell(x;\theta)}{\partial\theta}%
 \right)^{2}\right]=-\mathrm{E}\left[\frac{\partial^{2}\ell(x;\theta)}{\partial%
 \theta^{2}}\right]   and     â„“   (  x  ;  Î¸  )    =   log   (   f   (  x  ;  Î¸  )    )          normal-â„“   x  Î¸        f   x  Î¸       \ell(x;\theta)=\log(f(x;\theta))   is the natural logarithm of the likelihood function and   E   normal-E   \mathrm{E}   denotes the expected value (over   x   x   x   ).  The efficiency of an unbiased estimator    Î¸  ^     normal-^  Î¸    \hat{\theta}   measures how close this estimator's variance comes to this lower bound; estimator efficiency is defined as       e   (   Î¸  ^   )    =    I    (  Î¸  )    -  1      var   (   Î¸  ^   )           e   normal-^  Î¸        I   superscript  Î¸    1       var   normal-^  Î¸       e(\hat{\theta})=\frac{I(\theta)^{-1}}{{\rm var}(\hat{\theta})}     or the minimum possible variance for an unbiased estimator divided by its actual variance. The CramÃ©râ€“Rao lower bound thus gives       e   (   Î¸  ^   )    â‰¤  1.        e   normal-^  Î¸    1.    e(\hat{\theta})\leq 1.     General scalar case  A more general form of the bound can be obtained by considering a biased estimator    T   (  X  )       T  X    T(X)   of the parameter   Î¸   Î¸   \theta   . Here, biasedness is understood as stating that     E   {   T   (  X  )    }    =   Ïˆ   (  Î¸  )          E     T  X       Ïˆ  Î¸     E\{T(X)\}=\psi(\theta)   . In this case, the bound is given by       var   (  T  )    â‰¥     [    Ïˆ  â€²    (  Î¸  )    ]   2    I   (  Î¸  )           var  T      superscript   delimited-[]     superscript  Ïˆ  normal-â€²   Î¸    2     I  Î¸      \mathrm{var}(T)\geq\frac{[\psi^{\prime}(\theta)]^{2}}{I(\theta)}   where     Ïˆ  â€²    (  Î¸  )        superscript  Ïˆ  normal-â€²   Î¸    \psi^{\prime}(\theta)   is the derivative of    Ïˆ   (  Î¸  )       Ïˆ  Î¸    \psi(\theta)   (by   Î¸   Î¸   \theta   ), and    I   (  Î¸  )       I  Î¸    I(\theta)   is the Fisher information defined above.  Bound on the variance of biased estimators  Apart from being a bound on estimators of functions of the parameter, this approach can be used to derive a bound on the variance of biased estimators with a given bias, as follows. Consider an estimator    Î¸  ^     normal-^  Î¸    \hat{\theta}   with bias     b   (  Î¸  )    =    E   {   Î¸  ^   }    -  Î¸         b  Î¸       E    normal-^  Î¸     Î¸     b(\theta)=E\{\hat{\theta}\}-\theta   , and let     Ïˆ   (  Î¸  )    =    b   (  Î¸  )    +  Î¸         Ïˆ  Î¸       b  Î¸   Î¸     \psi(\theta)=b(\theta)+\theta   . By the result above, any unbiased estimator whose expectation is    Ïˆ   (  Î¸  )       Ïˆ  Î¸    \psi(\theta)   has variance greater than or equal to       (    Ïˆ  â€²    (  Î¸  )    )   2   /  I    (  Î¸  )          superscript     superscript  Ïˆ  normal-â€²   Î¸   2   I   Î¸    (\psi^{\prime}(\theta))^{2}/I(\theta)   . Thus, any estimator    Î¸  ^     normal-^  Î¸    \hat{\theta}   whose bias is given by a function    b   (  Î¸  )       b  Î¸    b(\theta)   satisfies        var   (   Î¸  ^   )    â‰¥     [   1  +    b  â€²    (  Î¸  )     ]   2    I   (  Î¸  )      .        var   normal-^  Î¸       superscript   delimited-[]    1     superscript  b  normal-â€²   Î¸     2     I  Î¸      \mathrm{var}\left(\hat{\theta}\right)\geq\frac{[1+b^{\prime}(\theta)]^{2}}{I(%
 \theta)}.   The unbiased version of the bound is a special case of this result, with     b   (  Î¸  )    =  0        b  Î¸   0    b(\theta)=0   .  It's trivial to have a small variance âˆ’ an "estimator" that is constant has a variance of zero. But from the above equation we find that the mean squared error of a biased estimator is bounded by        E   (    (    Î¸  ^   -  Î¸   )   2   )    â‰¥      [   1  +    b  â€²    (  Î¸  )     ]   2    I   (  Î¸  )     +   b    (  Î¸  )   2      ,        normal-E   superscript     normal-^  Î¸   Î¸   2         superscript   delimited-[]    1     superscript  b  normal-â€²   Î¸     2     I  Î¸      b   superscript  Î¸  2       \mathrm{E}\left((\hat{\theta}-\theta)^{2}\right)\geq\frac{[1+b^{\prime}(\theta%
 )]^{2}}{I(\theta)}+b(\theta)^{2},     using the standard decomposition of the MSE. Note, however, that this bound can be less than the unbiased CramÃ©râ€“Rao bound 1/ I (Î¸). See the example of estimating variance below.  Multivariate case  Extending the CramÃ©râ€“Rao bound to multiple parameters, define a parameter column vector      ğœ½  =    [   Î¸  1   ,   Î¸  2   ,  â€¦  ,   Î¸  d   ]   T   âˆˆ   â„  d         ğœ½   superscript    subscript  Î¸  1    subscript  Î¸  2   normal-â€¦   subscript  Î¸  d    T         superscript  â„  d      \boldsymbol{\theta}=\left[\theta_{1},\theta_{2},\dots,\theta_{d}\right]^{T}\in%
 \mathbb{R}^{d}   with probability density function    f   (  x  ;  ğœ½  )       f   x  ğœ½     f(x;\boldsymbol{\theta})   which satisfies the two regularity conditions below.  The Fisher information matrix is a    d  Ã—  d      d  d    d\times d   matrix with element    I   m  ,  k      subscript  I   m  k     I_{m,k}   defined as        I   m  ,  k    =   E   [    âˆ‚   âˆ‚   Î¸  m      log  f    (  x  ;  ğœ½  )    âˆ‚   âˆ‚   Î¸  k      log  f    (  x  ;  ğœ½  )    ]    =   -   E   [     âˆ‚  2     âˆ‚   Î¸  m     âˆ‚   Î¸  k       log  f    (  x  ;  ğœ½  )    ]      .         subscript  I   m  k      normal-E   delimited-[]          subscript  Î¸  m       f    x  ğœ½         subscript  Î¸  k       f    x  ğœ½               normal-E   delimited-[]       superscript   2        subscript  Î¸  m       subscript  Î¸  k        f    x  ğœ½          I_{m,k}=\mathrm{E}\left[\frac{\partial}{\partial\theta_{m}}\log f\left(x;%
 \boldsymbol{\theta}\right)\frac{\partial}{\partial\theta_{k}}\log f\left(x;%
 \boldsymbol{\theta}\right)\right]=-\mathrm{E}\left[\frac{\partial^{2}}{%
 \partial\theta_{m}\partial\theta_{k}}\log f\left(x;\boldsymbol{\theta}\right)%
 \right].     Let    ğ‘»   (  X  )       ğ‘»  X    \boldsymbol{T}(X)   be an estimator of any vector function of parameters,     ğ‘»   (  X  )    =    (    T  1    (  X  )    ,  â€¦  ,    T  d    (  X  )    )   T         ğ‘»  X    superscript      subscript  T  1   X   normal-â€¦     subscript  T  d   X    T     \boldsymbol{T}(X)=(T_{1}(X),\ldots,T_{d}(X))^{T}   , and denote its expectation vector    E   [   ğ‘»   (  X  )    ]       normal-E   delimited-[]    ğ‘»  X      \mathrm{E}[\boldsymbol{T}(X)]   by    ğ   (  ğœ½  )       ğ  ğœ½    \boldsymbol{\psi}(\boldsymbol{\theta})   . The CramÃ©râ€“Rao bound then states that the covariance matrix of    ğ‘»   (  X  )       ğ‘»  X    \boldsymbol{T}(X)   satisfies        cov  ğœ½    (   ğ‘»   (  X  )    )    â‰¥      âˆ‚  ğ    (  ğœ½  )     âˆ‚  ğœ½      [   I   (  ğœ½  )    ]    -  1      (     âˆ‚  ğ    (  ğœ½  )     âˆ‚  ğœ½    )   T           subscript  cov  ğœ½     ğ‘»  X            ğ   ğœ½     ğœ½     superscript   delimited-[]    I  ğœ½      1     superscript        ğ   ğœ½     ğœ½    T      \mathrm{cov}_{\boldsymbol{\theta}}\left(\boldsymbol{T}(X)\right)\geq\frac{%
 \partial\boldsymbol{\psi}\left(\boldsymbol{\theta}\right)}{\partial\boldsymbol%
 {\theta}}[I\left(\boldsymbol{\theta}\right)]^{-1}\left(\frac{\partial%
 \boldsymbol{\psi}\left(\boldsymbol{\theta}\right)}{\partial\boldsymbol{\theta}%
 }\right)^{T}   where   The matrix inequality    A  â‰¥  B      A  B    A\geq B   is understood to mean that the matrix    A  -  B      A  B    A-B   is positive semidefinite , and        âˆ‚  ğ    (  ğœ½  )    /   âˆ‚  ğœ½           ğ   ğœ½     ğœ½     \partial\boldsymbol{\psi}(\boldsymbol{\theta})/\partial\boldsymbol{\theta}   is the Jacobian matrix whose    i  j      i  j    ij   element is given by      âˆ‚   Ïˆ  i     (  ğœ½  )    /   âˆ‚   Î¸  j             subscript  Ïˆ  i    ğœ½      subscript  Î¸  j      \partial\psi_{i}(\boldsymbol{\theta})/\partial\theta_{j}   .   If    ğ‘»   (  X  )       ğ‘»  X    \boldsymbol{T}(X)   is an unbiased estimator of   ğœ½   ğœ½   \boldsymbol{\theta}   (i.e.,     ğ   (  ğœ½  )    =  ğœ½        ğ  ğœ½   ğœ½    \boldsymbol{\psi}\left(\boldsymbol{\theta}\right)=\boldsymbol{\theta}   ), then the CramÃ©râ€“Rao bound reduces to         cov  ğœ½    (   ğ‘»   (  X  )    )    â‰¥   I    (  ğœ½  )    -  1      .         subscript  cov  ğœ½     ğ‘»  X      I   superscript  ğœ½    1       \mathrm{cov}_{\boldsymbol{\theta}}\left(\boldsymbol{T}(X)\right)\geq I\left(%
 \boldsymbol{\theta}\right)^{-1}.     If it is inconvenient to compute the inverse of the Fisher information matrix , then one can simply take the reciprocal of the corresponding diagonal element to find a (possibly loose) lower bound (For the Bayesian case, see eqn. (11) of Bobrovsky, Mayer-Wolf, Zakai, "Some classes of global Cramer-Rao bounds", Ann. Stats., 15(4):1421-38, 1987).         var  ğœ½    (    T  m    (  X  )    )    =    [    cov  ğœ½    (   ğ‘»   (  X  )    )    ]    m  m    â‰¥    [   I    (  ğœ½  )    -  1     ]    m  m    â‰¥    (    [   I   (  ğœ½  )    ]    m  m    )    -  1     .           subscript  var  ğœ½      subscript  T  m   X     subscript   delimited-[]     subscript  cov  ğœ½     ğ‘»  X       m  m          subscript   delimited-[]    I   superscript  ğœ½    1        m  m          superscript   subscript   delimited-[]    I  ğœ½      m  m      1       \mathrm{var}_{\boldsymbol{\theta}}\left(T_{m}(X)\right)=\left[\mathrm{cov}_{%
 \boldsymbol{\theta}}\left(\boldsymbol{T}(X)\right)\right]_{mm}\geq\left[I\left%
 (\boldsymbol{\theta}\right)^{-1}\right]_{mm}\geq\left(\left[I\left(\boldsymbol%
 {\theta}\right)\right]_{mm}\right)^{-1}.     Regularity conditions  The bound relies on two weak regularity conditions on the probability density function ,    f   (  x  ;  Î¸  )       f   x  Î¸     f(x;\theta)   , and the estimator    T   (  X  )       T  X    T(X)   :   The Fisher information is always defined; equivalently, for all   x   x   x   such that     f   (  x  ;  Î¸  )    >  0        f   x  Î¸    0    f(x;\theta)>0   ,          âˆ‚   âˆ‚  Î¸     log  f    (  x  ;  Î¸  )            Î¸      f    x  Î¸     \frac{\partial}{\partial\theta}\log f(x;\theta)       exists, and is finite.    The operations of integration with respect to   x   x   x   and differentiation with respect to   Î¸   Î¸   \theta   can be interchanged in the expectation of   T   T   T   ; that is,          \frac{\partial}{\partial\theta}  \left[  \intÂ T(x)Â f(x;\theta)Â \,dx  \right]  =  \intÂ T(x)  \left[  \frac{\partial}{\partial\theta}Â f(x;\theta)  \right]  \,dx    whenever the right-hand side is finite.  This condition can often be confirmed by using the fact that integration and differentiation can be swapped when either of the following cases hold:  The function    f   (  x  ;  Î¸  )       f   x  Î¸     f(x;\theta)   has bounded support in   x   x   x   , and the bounds do not depend on   Î¸   Î¸   \theta   ;  The function    f   (  x  ;  Î¸  )       f   x  Î¸     f(x;\theta)   has infinite support, is continuously differentiable , and the integral converges uniformly for all   Î¸   Î¸   \theta   .     Simplified form of the Fisher information  Suppose, in addition, that the operations of integration and differentiation can be swapped for the second derivative of    f   (  x  ;  Î¸  )       f   x  Î¸     f(x;\theta)   as well, i.e.,          âˆ‚  2    âˆ‚   Î¸  2      [   âˆ«   T   (  x  )   f   (  x  ;  Î¸  )   d  x    ]    =   âˆ«   T   (  x  )    [     âˆ‚  2    âˆ‚   Î¸  2     f   (  x  ;  Î¸  )    ]   d  x     .           superscript   2      superscript  Î¸  2      delimited-[]      T  x  f   x  Î¸   d  x          T  x   delimited-[]       superscript   2      superscript  Î¸  2     f   x  Î¸     d  x      \frac{\partial^{2}}{\partial\theta^{2}}\left[\int T(x)f(x;\theta)\,dx\right]=%
 \int T(x)\left[\frac{\partial^{2}}{\partial\theta^{2}}f(x;\theta)\right]\,dx.   In this case, it can be shown that the Fisher information equals        I   (  Î¸  )    =   -   E   [     âˆ‚  2    âˆ‚   Î¸  2      log  f    (  X  ;  Î¸  )    ]      .        I  Î¸       normal-E   delimited-[]       superscript   2      superscript  Î¸  2       f    X  Î¸         I(\theta)=-\mathrm{E}\left[\frac{\partial^{2}}{\partial\theta^{2}}\log f(X;%
 \theta)\right].   The CramÃ¨râ€“Rao bound can then be written as        var   (   Î¸  ^   )    â‰¥   1   I   (  Î¸  )     =   1   -   E   [     âˆ‚  2    âˆ‚   Î¸  2      log  f    (  X  ;  Î¸  )    ]       .          var   normal-^  Î¸      1    I  Î¸           1      normal-E   delimited-[]       superscript   2      superscript  Î¸  2       f    X  Î¸           \mathrm{var}\left(\widehat{\theta}\right)\geq\frac{1}{I(\theta)}=\frac{1}{-%
 \mathrm{E}\left[\frac{\partial^{2}}{\partial\theta^{2}}\log f(X;\theta)\right]}.   In some cases, this formula gives a more convenient technique for evaluating the bound.  Single-parameter proof  The following is a proof of the general scalar case of the CramÃ©râ€“Rao bound described above . Assume that    T  =   t   (  X  )        T    t  X     T=t(X)   is an unbiased estimator for the value    Ïˆ   (  Î¸  )       Ïˆ  Î¸    \psi(\theta)   (based on the observations   X   X   X   ), and so     E   (  T  )    =   Ïˆ   (  Î¸  )          normal-E  T     Ïˆ  Î¸     {\rm E}(T)=\psi(\theta)   . The goal is to prove that, for all   Î¸   Î¸   \theta   ,        var   (   t   (  X  )    )    â‰¥     [    Ïˆ  â€²    (  Î¸  )    ]   2    I   (  Î¸  )      .        var    t  X       superscript   delimited-[]     superscript  Ïˆ  normal-â€²   Î¸    2     I  Î¸      {\rm var}(t(X))\geq\frac{[\psi^{\prime}(\theta)]^{2}}{I(\theta)}.     Let   X   X   X   be a random variable with probability density function    f   (  x  ;  Î¸  )       f   x  Î¸     f(x;\theta)   . Here    T  =   t   (  X  )        T    t  X     T=t(X)   is a statistic , which is used as an estimator for    Ïˆ   (  Î¸  )       Ïˆ  Î¸    \psi(\theta)   . Define   V   V   V   as the score :      V  =    âˆ‚   âˆ‚  Î¸     ln  f    (  X  ;  Î¸  )    =    1   f   (  X  ;  Î¸  )      âˆ‚   âˆ‚  Î¸    f   (  X  ;  Î¸  )          V         Î¸      f    X  Î¸             1    f   X  Î¸          Î¸    f   X  Î¸       V=\frac{\partial}{\partial\theta}\ln f(X;\theta)=\frac{1}{f(X;\theta)}\frac{%
 \partial}{\partial\theta}f(X;\theta)     where the chain rule is used in the final equality above. Then the expectation of   V   V   V   , written    E   (  V  )       normal-E  V    {\rm E}(V)   , is zero. This is because:       E   (  V  )    =    âˆ«  x    f   (  x  ;  Î¸  )    [    1   f   (  x  ;  Î¸  )      âˆ‚   âˆ‚  Î¸    f   (  x  ;  Î¸  )    ]   d  x    =    âˆ‚   âˆ‚  Î¸      âˆ«  x    f   (  x  ;  Î¸  )   d  x     =  0          normal-E  V     subscript   x     f   x  Î¸    delimited-[]      1    f   x  Î¸          Î¸    f   x  Î¸     d  x                Î¸      subscript   x     f   x  Î¸   d  x          0     {\rm E}\left(V\right)=\int_{x}f(x;\theta)\left[\frac{1}{f(x;\theta)}\frac{%
 \partial}{\partial\theta}f(x;\theta)\right]dx=\frac{\partial}{\partial\theta}%
 \int_{x}f(x;\theta)dx=0     where the integral and partial derivative have been interchanged (justified by the second regularity condition).  If we consider the covariance     cov   (  V  ,  T  )       cov   V  T     {\rm cov}(V,T)   of   V   V   V   and   T   T   T   , we have     cov   (  V  ,  T  )    =   E   (   V  T   )          cov   V  T      normal-E    V  T      {\rm cov}(V,T)={\rm E}(VT)   , because     E   (  V  )    =  0        normal-E  V   0    {\rm E}(V)=0   . Expanding this expression we have       cov   (  V  ,  T  )    =   E   (   T  â‹…   [    1   f   (  X  ;  Î¸  )      âˆ‚   âˆ‚  Î¸    f   (  X  ;  Î¸  )    ]    )    =    âˆ«  x    t   (  x  )    [    âˆ‚   âˆ‚  Î¸    f   (  x  ;  Î¸  )    ]   d  x    =    âˆ‚   âˆ‚  Î¸     [    âˆ«  x    t   (  x  )   f   (  x  ;  Î¸  )   d  x    ]    =    Ïˆ  â€²    (  Î¸  )            cov   V  T      normal-E   normal-â‹…  T   delimited-[]      1    f   X  Î¸          Î¸    f   X  Î¸              subscript   x     t  x   delimited-[]         Î¸    f   x  Î¸     d  x                Î¸     delimited-[]    subscript   x     t  x  f   x  Î¸   d  x              superscript  Ïˆ  normal-â€²   Î¸      {\rm cov}(V,T)={\rm E}\left(T\cdot\left[\frac{1}{f(X;\theta)}\frac{\partial}{%
 \partial\theta}f(X;\theta)\right]\right)=\int_{x}t(x)\left[\frac{\partial}{%
 \partial\theta}f(x;\theta)\right]\,dx=\frac{\partial}{\partial\theta}\left[%
 \int_{x}t(x)f(x;\theta)\,dx\right]=\psi^{\prime}(\theta)     again because the integration and differentiation operations commute (second condition).  The Cauchyâ€“Schwarz inequality shows that        var   (  T  )   var   (  V  )     â‰¥   |   cov   (  V  ,  T  )    |   =   |    Ïˆ  â€²    (  Î¸  )    |             var  T  var  V        cov   V  T               superscript  Ïˆ  normal-â€²   Î¸       \sqrt{{\rm var}(T){\rm var}(V)}\geq\left|{\rm cov}(V,T)\right|=\left|\psi^{%
 \prime}(\theta)\right|     therefore       var   (  T  )    â‰¥     [    Ïˆ  â€²    (  Î¸  )    ]   2    var   (  V  )     =     [    Ïˆ  â€²    (  Î¸  )    ]   2    I   (  Î¸  )             var  T      superscript   delimited-[]     superscript  Ïˆ  normal-â€²   Î¸    2     var  V            superscript   delimited-[]     superscript  Ïˆ  normal-â€²   Î¸    2     I  Î¸       {\rm var}(T)\geq\frac{[\psi^{\prime}(\theta)]^{2}}{{\rm var}(V)}=\frac{[\psi^{%
 \prime}(\theta)]^{2}}{I(\theta)}   which proves the proposition.  Examples  Multivariate normal distribution  For the case of a d -variate normal distribution      ğ’™  âˆ¼    N  d    (   ğ   (  ğœ½  )    ,   ğ‘ª   (  ğœ½  )    )       similar-to  ğ’™     subscript  N  d      ğ  ğœ½     ğ‘ª  ğœ½       \boldsymbol{x}\sim N_{d}\left(\boldsymbol{\mu}\left(\boldsymbol{\theta}\right)%
 ,{\boldsymbol{C}}\left(\boldsymbol{\theta}\right)\right)   the Fisher information matrix has elements 4       I   m  ,  k    =      âˆ‚   ğ  T     âˆ‚   Î¸  m      ğ‘ª   -  1      âˆ‚  ğ    âˆ‚   Î¸  k      +    1  2   tr   (    ğ‘ª   -  1      âˆ‚  ğ‘ª    âˆ‚   Î¸  m      ğ‘ª   -  1      âˆ‚  ğ‘ª    âˆ‚   Î¸  k      )          subscript  I   m  k             superscript  ğ  T       subscript  Î¸  m      superscript  ğ‘ª    1        ğ      subscript  Î¸  k          1  2   tr     superscript  ğ‘ª    1        ğ‘ª      subscript  Î¸  m      superscript  ğ‘ª    1        ğ‘ª      subscript  Î¸  k          I_{m,k}=\frac{\partial\boldsymbol{\mu}^{T}}{\partial\theta_{m}}{\boldsymbol{C}%
 }^{-1}\frac{\partial\boldsymbol{\mu}}{\partial\theta_{k}}+\frac{1}{2}\mathrm{%
 tr}\left({\boldsymbol{C}}^{-1}\frac{\partial{\boldsymbol{C}}}{\partial\theta_{%
 m}}{\boldsymbol{C}}^{-1}\frac{\partial{\boldsymbol{C}}}{\partial\theta_{k}}\right)   where "tr" is the trace .  For example, let    w   [  n  ]       w   delimited-[]  n     w[n]   be a sample of   N   N   N   independent observations) with unknown mean   Î¸   Î¸   \theta   and known variance    Ïƒ  2     superscript  Ïƒ  2    \sigma^{2}           w   [  n  ]    âˆ¼    â„•  N    (   Î¸  ğŸ   ,    Ïƒ  2   ğ‘°   )     .     similar-to    w   delimited-[]  n       subscript  â„•  N      Î¸  1      superscript  Ïƒ  2   ğ‘°       w[n]\sim\mathbb{N}_{N}\left(\theta{\boldsymbol{1}},\sigma^{2}{\boldsymbol{I}}%
 \right).   Then the Fisher information is a scalar given by        I   (  Î¸  )    =     (     âˆ‚  ğ    (  Î¸  )     âˆ‚  Î¸    )   T    ğ‘ª   -  1     (     âˆ‚  ğ    (  Î¸  )     âˆ‚  Î¸    )    =    âˆ‘   i  =  1   N    1   Ïƒ  2     =   N   Ïƒ  2     ,          I  Î¸      superscript        ğ   Î¸     Î¸    T    superscript  ğ‘ª    1          ğ   Î¸     Î¸            subscript   superscript   N     i  1      1   superscript  Ïƒ  2            N   superscript  Ïƒ  2       I(\theta)=\left(\frac{\partial\boldsymbol{\mu}(\theta)}{\partial\theta}\right)%
 ^{T}{\boldsymbol{C}}^{-1}\left(\frac{\partial\boldsymbol{\mu}(\theta)}{%
 \partial\theta}\right)=\sum^{N}_{i=1}\frac{1}{\sigma^{2}}=\frac{N}{\sigma^{2}},   and so the CramÃ©râ€“Rao bound is        var   (   Î¸  ^   )    â‰¥    Ïƒ  2   N    .        var   normal-^  Î¸       superscript  Ïƒ  2   N     \mathrm{var}\left(\hat{\theta}\right)\geq\frac{\sigma^{2}}{N}.     Normal variance with known mean  Suppose X is a normally distributed random variable with known mean   Î¼   Î¼   \mu   and unknown variance    Ïƒ  2     superscript  Ïƒ  2    \sigma^{2}   . Consider the following statistic:       T  =     âˆ‘   i  =  1   n     (    X  i   -  Î¼   )   2    n    .      T      superscript   subscript     i  1    n    superscript     subscript  X  i   Î¼   2    n     T=\frac{\sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}}{n}.     Then T is unbiased for    Ïƒ  2     superscript  Ïƒ  2    \sigma^{2}   , as     E   (  T  )    =   Ïƒ  2         E  T    superscript  Ïƒ  2     E(T)=\sigma^{2}   . What is the variance of T ?       var   (  T  )    =    var    (   X  -  Î¼   )   2    n   =    1  n    [    E   {    (   X  -  Î¼   )   4   }    -    (   E   {    (   X  -  Î¼   )   2   }    )   2    ]            var  T       var   superscript    X  Î¼   2    n            1  n    delimited-[]      E    superscript    X  Î¼   4      superscript    E    superscript    X  Î¼   2     2         \mathrm{var}(T)=\frac{\mathrm{var}(X-\mu)^{2}}{n}=\frac{1}{n}\left[E\left\{(X-%
 \mu)^{4}\right\}-\left(E\left\{(X-\mu)^{2}\right\}\right)^{2}\right]     (the second equality follows directly from the definition of variance). The first term is the fourth moment about the mean and has value    3    (   Ïƒ  2   )   2       3   superscript   superscript  Ïƒ  2   2     3(\sigma^{2})^{2}   ; the second is the square of the variance, or     (   Ïƒ  2   )   2     superscript   superscript  Ïƒ  2   2    (\sigma^{2})^{2}   . Thus        var   (  T  )    =    2    (   Ïƒ  2   )   2    n    .        var  T       2   superscript   superscript  Ïƒ  2   2    n     \mathrm{var}(T)=\frac{2(\sigma^{2})^{2}}{n}.     Now, what is the Fisher information in the sample? Recall that the score  V is defined as      V  =    âˆ‚   âˆ‚   Ïƒ  2      log  L    (   Ïƒ  2   ,  X  )        V          superscript  Ïƒ  2       L     superscript  Ïƒ  2   X      V=\frac{\partial}{\partial\sigma^{2}}\log L(\sigma^{2},X)     where   L   L   L   is the likelihood function . Thus in this case,      V  =    âˆ‚   âˆ‚   Ïƒ  2      log   [    1    2  Ï€   Ïƒ  2       e   -      (   X  -  Î¼   )   2   /  2    Ïƒ  2       ]     =      (   X  -  Î¼   )   2    2    (   Ïƒ  2   )   2     -   1   2   Ïƒ  2            V          superscript  Ïƒ  2           1      2  Ï€   superscript  Ïƒ  2       superscript  e         superscript    X  Î¼   2   2    superscript  Ïƒ  2                   superscript    X  Î¼   2     2   superscript   superscript  Ïƒ  2   2       1    2   superscript  Ïƒ  2         V=\frac{\partial}{\partial\sigma^{2}}\log\left[\frac{1}{\sqrt{2\pi\sigma^{2}}}%
 e^{-(X-\mu)^{2}/{2\sigma^{2}}}\right]=\frac{(X-\mu)^{2}}{2(\sigma^{2})^{2}}-%
 \frac{1}{2\sigma^{2}}     where the second equality is from elementary calculus. Thus, the information in a single observation is just minus the expectation of the derivative of V , or       I  =   -   E   (    âˆ‚  V    âˆ‚   Ïƒ  2     )     =   -   E   (    -     (   X  -  Î¼   )   2     (   Ïƒ  2   )   3     +   1   2    (   Ïƒ  2   )   2      )     =     Ïƒ  2     (   Ïƒ  2   )   3    -   1   2    (   Ïƒ  2   )   2      =   1   2    (   Ïƒ  2   )   2      .        I      E      V      superscript  Ïƒ  2                E         superscript    X  Î¼   2    superscript   superscript  Ïƒ  2   3       1    2   superscript   superscript  Ïƒ  2   2                  superscript  Ïƒ  2    superscript   superscript  Ïƒ  2   3      1    2   superscript   superscript  Ïƒ  2   2             1    2   superscript   superscript  Ïƒ  2   2        I=-E\left(\frac{\partial V}{\partial\sigma^{2}}\right)=-E\left(-\frac{(X-\mu)^%
 {2}}{(\sigma^{2})^{3}}+\frac{1}{2(\sigma^{2})^{2}}\right)=\frac{\sigma^{2}}{(%
 \sigma^{2})^{3}}-\frac{1}{2(\sigma^{2})^{2}}=\frac{1}{2(\sigma^{2})^{2}}.     Thus the information in a sample of   n   n   n   independent observations is just   n   n   n   times this, or     n   2    (   Ïƒ  2   )   2     .      n    2   superscript   superscript  Ïƒ  2   2      \frac{n}{2(\sigma^{2})^{2}}.     The Cramer Rao bound states that        var   (  T  )    â‰¥   1  I    .        var  T     1  I     \mathrm{var}(T)\geq\frac{1}{I}.     In this case, the inequality is saturated (equality is achieved), showing that the estimator is efficient .  However, we can achieve a lower mean squared error using a biased estimator. The estimator       T  =     âˆ‘   i  =  1   n     (    X  i   -  Î¼   )   2     n  +  2     .      T      superscript   subscript     i  1    n    superscript     subscript  X  i   Î¼   2      n  2      T=\frac{\sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}}{n+2}.     obviously has a smaller variance, which is in fact        var   (  T  )    =    2  n    (   Ïƒ  2   )   2      (   n  +  2   )   2     .        var  T       2  n   superscript   superscript  Ïƒ  2   2     superscript    n  2   2      \mathrm{var}(T)=\frac{2n(\sigma^{2})^{2}}{(n+2)^{2}}.     Its bias is        (   1  -   n   n  +  2     )    Ïƒ  2    =    2   Ïƒ  2     n  +  2            1    n    n  2      superscript  Ïƒ  2        2   superscript  Ïƒ  2      n  2      \left(1-\frac{n}{n+2}\right)\sigma^{2}=\frac{2\sigma^{2}}{n+2}     so its mean squared error is       MSE   (  T  )    =    (     2  n     (   n  +  2   )   2    +   4    (   n  +  2   )   2     )     (   Ïƒ  2   )   2    =    2    (   Ïƒ  2   )   2     n  +  2            MSE  T           2  n    superscript    n  2   2      4   superscript    n  2   2      superscript   superscript  Ïƒ  2   2             2   superscript   superscript  Ïƒ  2   2      n  2       \mathrm{MSE}(T)=\left(\frac{2n}{(n+2)^{2}}+\frac{4}{(n+2)^{2}}\right)(\sigma^{%
 2})^{2}=\frac{2(\sigma^{2})^{2}}{n+2}     which is clearly less than the CramÃ©râ€“Rao bound found above.  When the mean is not known, the minimum mean squared error estimate of the variance of a sample from Gaussian distribution is achieved by dividing by n +Â 1, rather than n âˆ’Â 1 or n +Â 2.  See also   Chapmanâ€“Robbins bound  Kullback's inequality   References and notes  Further reading     . Chapter 3.   . Section 3.1.3.   External links   FandPLimitTool a GUI-based software to calculate the Fisher information and Cramer-Rao Lower Bound with application to single-molecule microscopy.   "  Category:Articles containing proofs  Category:Statistical inequalities  Category:Estimation theory     â†©  â†©  â†©  â†©     