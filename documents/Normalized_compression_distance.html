<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="630">Normalized compression distance</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Normalized compression distance</h1>
<hr/>

<p><strong>Normalized compression distance</strong> is way of measuring the <a href="Equality_(mathematics)" title="wikilink">similarity</a> between two objects, be it two documents, two letters, two emails, two music scores, two languages, two programs, two pictures, two systems, two genomes, to name a few. Such a measurement should not be application dependent or arbitrary. A reasonable definition for the similarity between two objects is how difficult it is to transform them into each other.</p>
<h2 id="information-distance">Information distance</h2>

<p>We assume that the objects one talks about are finite <a href="binary_sequence" title="wikilink">strings of 0s and 1s</a>. Thus we mean <a href="string_similarity" title="wikilink">string similarity</a>. Every computer file is of this form, that is, if an object is a file in a computer it is of this form. One can define the <a href="information_distance" title="wikilink">information distance</a> between strings 

<math display="inline" id="Normalized_compression_distance:0">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Normalized_compression_distance:1">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 as the length of the shortest program 

<math display="inline" id="Normalized_compression_distance:2">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

 that computes 

<math display="inline" id="Normalized_compression_distance:3">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 from 

<math display="inline" id="Normalized_compression_distance:4">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 and vice versa. This shortest program is in a fixed programming language. For technical reasons one uses the theoretical notion of <a href="Turing_machines" title="wikilink">Turing machines</a>. Moreover, to express the length of 

<math display="inline" id="Normalized_compression_distance:5">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

 one uses the notion of <a href="Kolmogorov_complexity" title="wikilink">Kolmogorov complexity</a>. Then, it has been shown <a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>

<p>

<math display="block" id="Normalized_compression_distance:6">
 <semantics>
  <mrow>
   <mo stretchy="false">|</mo>
   <mi>p</mi>
   <mo stretchy="false">|</mo>
   <mo>=</mo>
   <mi>max</mi>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>K</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo>∣</mo>
     <mi>y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>,</mo>
    <mi>K</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>y</mi>
     <mo>∣</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <ci>normal-|</ci>
    <csymbol cd="unknown">p</csymbol>
    <ci>normal-|</ci>
    <eq></eq>
    <max></max>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-{</ci>
     <csymbol cd="unknown">K</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">x</csymbol>
      <ci>normal-∣</ci>
      <csymbol cd="unknown">y</csymbol>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-,</ci>
     <csymbol cd="unknown">K</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">y</csymbol>
      <ci>normal-∣</ci>
      <csymbol cd="unknown">x</csymbol>
      <ci>normal-)</ci>
     </cerror>
     <ci>normal-}</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   |p|=\max\{K(x\mid y),K(y\mid x)\}
  </annotation>
 </semantics>
</math>

 up to logarithmic additive terms which can be ignored. This information distance is shown to be a <a href="metric_(mathematics)" title="wikilink">metric</a> (it satisfies the metric inequalities up to a logarithmic additive term), is universal (it minorizes every computable distance as computed for example from features up to a constant additive term).<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h3 id="normalized-information-distance-similarity-metric">Normalized information distance (similarity metric)</h3>

<p>The information distance is absolute, but if we want to express similarity, then we are more interested in relative ones. For example, if two strings of length 1,000,000 differ by 1000 bits, then we are inclined to think that those strings are relatively more similar than two strings of 1000 bits that have that distance. Hence we need to normalize to obtain a similarity metric. This way one obtains the normalized information distance (NID),</p>

<p>

<math display="block" id="Normalized_compression_distance:7">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>N</mi>
     <mi>I</mi>
     <mi>D</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo>,</mo>
      <mi>y</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mfrac>
     <mrow>
      <mi>max</mi>
      <mrow>
       <mo stretchy="false">{</mo>
       <mi>K</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>∣</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
       </mrow>
       <mo>,</mo>
       <mi>K</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>y</mi>
        <mo>∣</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
       </mrow>
       <mo stretchy="false">}</mo>
      </mrow>
     </mrow>
     <mrow>
      <mi>max</mi>
      <mrow>
       <mo stretchy="false">{</mo>
       <mrow>
        <mi>K</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>,</mo>
       <mrow>
        <mi>K</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>y</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo stretchy="false">}</mo>
      </mrow>
     </mrow>
    </mfrac>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>N</ci>
     <ci>I</ci>
     <ci>D</ci>
     <interval closure="open">
      <ci>x</ci>
      <ci>y</ci>
     </interval>
    </apply>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <max></max>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-{</ci>
       <csymbol cd="unknown">K</csymbol>
       <cerror>
        <csymbol cd="ambiguous">fragments</csymbol>
        <ci>normal-(</ci>
        <csymbol cd="unknown">x</csymbol>
        <ci>normal-∣</ci>
        <csymbol cd="unknown">y</csymbol>
        <ci>normal-)</ci>
       </cerror>
       <ci>normal-,</ci>
       <csymbol cd="unknown">K</csymbol>
       <cerror>
        <csymbol cd="ambiguous">fragments</csymbol>
        <ci>normal-(</ci>
        <csymbol cd="unknown">y</csymbol>
        <ci>normal-∣</ci>
        <csymbol cd="unknown">x</csymbol>
        <ci>normal-)</ci>
       </cerror>
       <ci>normal-}</ci>
      </cerror>
     </cerror>
     <apply>
      <max></max>
      <apply>
       <times></times>
       <ci>K</ci>
       <ci>x</ci>
      </apply>
      <apply>
       <times></times>
       <ci>K</ci>
       <ci>y</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   NID(x,y)=\frac{\max\{K{(x\mid y)},K{(y\mid x)}\}}{\max\{K(x),K(y)\}},
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Normalized_compression_distance:8">
 <semantics>
  <mrow>
   <mi>K</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>∣</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">K</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K(x\mid y)
  </annotation>
 </semantics>
</math>

 is <a href="algorithmic_information" title="wikilink">algorithmic information</a> of 

<math display="inline" id="Normalized_compression_distance:9">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 given 

<math display="inline" id="Normalized_compression_distance:10">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 as input. The NID is called `the similarity metric.' since the function 

<math display="inline" id="Normalized_compression_distance:11">
 <semantics>
  <mrow>
   <mi>N</mi>
   <mi>I</mi>
   <mi>D</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>N</ci>
    <ci>I</ci>
    <ci>D</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   NID(x,y)
  </annotation>
 </semantics>
</math>

 has been shown to satisfy the basic requirements for a metric distance measure.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> However, it is not computable or even semicomputable.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h2 id="normalized-compression-distance">Normalized compression distance</h2>

<p>While the NID metric is not computable, it has an abundance of applications. Simply approximating 

<math display="inline" id="Normalized_compression_distance:12">
 <semantics>
  <mi>K</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>K</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K
  </annotation>
 </semantics>
</math>

 by real-world compressors, with 

<math display="inline" id="Normalized_compression_distance:13">
 <semantics>
  <mrow>
   <mi>Z</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>Z</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Z(x)
  </annotation>
 </semantics>
</math>

 is the binary length of the file 

<math display="inline" id="Normalized_compression_distance:14">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 compressed with compressor Z (for example "<a class="uri" href="gzip" title="wikilink">gzip</a>", "<a class="uri" href="bzip2" title="wikilink">bzip2</a>", "<a href="Prediction_by_partial_matching" title="wikilink">PPMZ</a>") in order to make NID easy to apply.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> <a href="Paul_Vitanyi" title="wikilink">Vitanyi</a> and <a href="Rudi_Cilibrasi" title="wikilink">Cilibrasi</a> rewrote the NID to obtain the Normalized Compression Distance (NCD)</p>

<p>

<math display="block" id="Normalized_compression_distance:15">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>N</mi>
     <mi>C</mi>
     <msub>
      <mi>D</mi>
      <mi>Z</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo>,</mo>
      <mi>y</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mfrac>
     <mrow>
      <mrow>
       <mi>Z</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mi>x</mi>
         <mi>y</mi>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>-</mo>
      <mrow>
       <mi>min</mi>
       <mrow>
        <mo stretchy="false">{</mo>
        <mrow>
         <mi>Z</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>x</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>,</mo>
        <mrow>
         <mi>Z</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>y</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo stretchy="false">}</mo>
       </mrow>
      </mrow>
     </mrow>
     <mrow>
      <mi>max</mi>
      <mrow>
       <mo stretchy="false">{</mo>
       <mrow>
        <mi>Z</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>,</mo>
       <mrow>
        <mi>Z</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>y</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo stretchy="false">}</mo>
      </mrow>
     </mrow>
    </mfrac>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>N</ci>
     <ci>C</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>D</ci>
      <ci>Z</ci>
     </apply>
     <interval closure="open">
      <ci>x</ci>
      <ci>y</ci>
     </interval>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>Z</ci>
       <apply>
        <times></times>
        <ci>x</ci>
        <ci>y</ci>
       </apply>
      </apply>
      <apply>
       <min></min>
       <apply>
        <times></times>
        <ci>Z</ci>
        <ci>x</ci>
       </apply>
       <apply>
        <times></times>
        <ci>Z</ci>
        <ci>y</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <max></max>
      <apply>
       <times></times>
       <ci>Z</ci>
       <ci>x</ci>
      </apply>
      <apply>
       <times></times>
       <ci>Z</ci>
       <ci>y</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   NCD_{Z}(x,y)=\frac{Z(xy)-\min\{Z(x),Z(y)\}}{\max\{Z(x),Z(y)\}}.
  </annotation>
 </semantics>
</math>

<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> The NCD is actually a family of distances parametrized with the compressor Z. The better Z is, the closer the NCD approaches the NID, and the better the results are.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>
<h3 id="applications">Applications</h3>

<p>The normalized compression distance has been used to fully automatically reconstruct language and phylogenetic trees.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a><a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> It can also be used for new applications of general <a href="cluster_analysis" title="wikilink">clustering</a> and <a href="statistical_classification" title="wikilink">classification</a> of natural data in arbitrary domains,<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> for clustering of heterogeneous data,<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> and for <a href="anomaly_detection" title="wikilink">anomaly detection</a> across domains.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> The NID and NCD have been applied to numerous subjects, including music classification,<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> to analyze network traffic and cluster computer worms and viruses,<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> authorship attribution,<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> gene expression dynamics,<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> predicting useful versus useless stem cells,<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> critical networks,<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a> image registration,<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> question-answer systems.<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a></p>
<h3 id="performance">Performance</h3>

<p>Researchers from the <a class="uri" href="datamining" title="wikilink">datamining</a> community use NCD and variants as "parameter-free, feature-free" data-mining tools.<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a> One group have experimentally tested a closely related metric on a large variety of sequence benchmarks. Comparing their compression method with 51 major methods found in 7 major data-mining conferences over the past decade, they established superiority of the compression method for clustering heterogeneous data, and for anomaly detection, and competitiveness in clustering domain data.</p>

<p>NCD has an advantage of being <a href="Robust_statistics" title="wikilink">robust</a> to noise.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a> However, although NCD appears "parameter-free", practical questions include which compressor to use in computing the NCD and other possible problems.<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a></p>
<h2 id="normalized-google-distance"><a href="Normalized_Google_distance" title="wikilink">Normalized Google distance</a></h2>

<p>Objects can be given literally, like the literal four-letter <a href="Mouse_Genome_Database" title="wikilink">genome of a mouse</a>, or the literal text of <a href="War_and_Peace" title="wikilink">War and Peace</a> by Tolstoy. For simplicity we take it that all meaning of the object is represented by the literal object itself. Objects can also be given by name, like "the four-letter genome of a mouse," or "the text of `War and Peace' by Tolstoy." There are also objects that cannot be given literally, but only by name, and that acquire their meaning from their contexts in background common knowledge in humankind, like "home" or "red." We are interested in <a href="semantic_similarity" title="wikilink">semantic similarity</a>. Using code-word lengths obtained from the page-hit counts returned by Google from the web, we obtain a semantic distance using the NCD formula and viewing Google as a compressor useful for data mining, text comprehension, classification, and translation. The associated NCD, called the <a href="normalized_Google_distance" title="wikilink">normalized Google distance</a> (NGD) can be rewritten as</p>

<p>

<math display="block" id="Normalized_compression_distance:16">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>N</mi>
     <mi>G</mi>
     <mi>D</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo>,</mo>
      <mi>y</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mfrac>
     <mrow>
      <mrow>
       <mi>max</mi>
       <mrow>
        <mo stretchy="false">{</mo>
        <mrow>
         <mrow>
          <mi>log</mi>
          <mi>f</mi>
         </mrow>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>x</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>,</mo>
        <mrow>
         <mrow>
          <mi>log</mi>
          <mi>f</mi>
         </mrow>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>y</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo stretchy="false">}</mo>
       </mrow>
      </mrow>
      <mo>-</mo>
      <mrow>
       <mrow>
        <mi>log</mi>
        <mi>f</mi>
       </mrow>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mrow>
      <mrow>
       <mi>log</mi>
       <mi>N</mi>
      </mrow>
      <mo>-</mo>
      <mrow>
       <mi>min</mi>
       <mrow>
        <mo stretchy="false">{</mo>
        <mrow>
         <mrow>
          <mi>log</mi>
          <mi>f</mi>
         </mrow>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>x</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>,</mo>
        <mrow>
         <mrow>
          <mi>log</mi>
          <mi>f</mi>
         </mrow>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>y</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo stretchy="false">}</mo>
       </mrow>
      </mrow>
     </mrow>
    </mfrac>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>N</ci>
     <ci>G</ci>
     <ci>D</ci>
     <interval closure="open">
      <ci>x</ci>
      <ci>y</ci>
     </interval>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <minus></minus>
      <apply>
       <max></max>
       <apply>
        <times></times>
        <apply>
         <log></log>
         <ci>f</ci>
        </apply>
        <ci>x</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <log></log>
         <ci>f</ci>
        </apply>
        <ci>y</ci>
       </apply>
      </apply>
      <apply>
       <times></times>
       <apply>
        <log></log>
        <ci>f</ci>
       </apply>
       <interval closure="open">
        <ci>x</ci>
        <ci>y</ci>
       </interval>
      </apply>
     </apply>
     <apply>
      <minus></minus>
      <apply>
       <log></log>
       <ci>N</ci>
      </apply>
      <apply>
       <min></min>
       <apply>
        <times></times>
        <apply>
         <log></log>
         <ci>f</ci>
        </apply>
        <ci>x</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <log></log>
         <ci>f</ci>
        </apply>
        <ci>y</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   NGD(x,y)=\frac{\max\{\log f(x),\log f(y)\}-\log f(x,y)}{\log N-\min\{\log f(x)%
,\log f(y)\}},
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Normalized_compression_distance:17">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)
  </annotation>
 </semantics>
</math>

 denotes the number of pages containing the search term 

<math display="inline" id="Normalized_compression_distance:18">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

, and 

<math display="inline" id="Normalized_compression_distance:19">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x,y)
  </annotation>
 </semantics>
</math>

 denotes the number of pages containing both 

<math display="inline" id="Normalized_compression_distance:20">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Normalized_compression_distance:21">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

,) as returned by Google or any search engine capable of returning an aggregate page count. The number 

<math display="inline" id="Normalized_compression_distance:22">
 <semantics>
  <mi>N</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>N</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N
  </annotation>
 </semantics>
</math>

 can be set to the number of pages indexed although it is more proper to count each page according to the number of search terms or phrases it contains. As rule of the thumb one can multiply the number of pages by, say, a thousand...<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a></p>
<h2 id="software-implementation">Software implementation</h2>

<p>For a publicly available open-source downloadable software tool CompLearn, for both NCD and NGD, see <a class="uri" href="http://www.complearn.org">http://www.complearn.org</a>.</p>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="Ming_Li" title="wikilink">M. Li</a> and P. Vitanyi, An Introduction to Kolmogorov Complexity and Its Applications,Springer-Verlag, New York, 3rd Edition 2008,</li>
</ul>

<p>"</p>

<p><a href="Category:Articles_created_via_the_Article_Wizard" title="wikilink">Category:Articles created via the Article Wizard</a> <a href="Category:Statistical_distance_measures" title="wikilink">Category:Statistical distance measures</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">[<a class="uri" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp">http://ieeexplore.ieee.org/xpl/login.jsp?tp</a>=&amp;arnumber;=681318&amp;url;=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D681318 C.H. Bennett, P. Gacs, M. Li, P.M.B. Vitányi, and W. Zurek, Information Distance, IEEE Trans. Inform. Theory, IT-44:4(1998) 1407–1423]<a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="http://www.sciencedirect.com/science/article/pii/S0022000010001029">S.A. Terwijn, L. Torenvliet, and P.M.B. Vitanyi, Nonapproximability of the Normalized Information Distance, J. Comput. System Sciences, 77:4(2011), 738–742</a><a href="#fnref5">↩</a></li>
<li id="fn6"></li>
<li id="fn7"></li>
<li id="fn8"></li>
<li id="fn9"></li>
<li id="fn10"></li>
<li id="fn11"></li>
<li id="fn12"></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"></li>
<li id="fn15"><a href="#fnref15">↩</a></li>
<li id="fn16"><a href="#fnref16">↩</a></li>
<li id="fn17"><a href="#fnref17">↩</a></li>
<li id="fn18"><a href="#fnref18">↩</a></li>
<li id="fn19"><a href="#fnref19">↩</a></li>
<li id="fn20"><a href="#fnref20">↩</a></li>
<li id="fn21"><a href="#fnref21">↩</a></li>
<li id="fn22"></li>
<li id="fn23"><a href="#fnref23">↩</a></li>
<li id="fn24"><a href="#fnref24">↩</a></li>
<li id="fn25"><a href="#fnref25">↩</a></li>
</ol>
</section>
</body>
</html>
