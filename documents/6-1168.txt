   Symmetry in mathematics      Symmetry in mathematics    Symmetry occurs not only in geometry , but also in other branches of mathematics. Symmetry is a type of invariance : the property that something does not change under a set of transformations .  Given a structured object X of any sort, a symmetry is a mapping of the object onto itself which preserves the structure. This occurs in many cases; for example, if X is a set with no additional structure, a symmetry is a bijective map from the set to itself, giving rise to permutation groups . If the object X is a set of points in the plane with its metric structure or any other metric space , a symmetry is a bijection of the set to itself which preserves the distance between each pair of points (an isometry ).  In general, every kind of structure in mathematics will have its own kind of symmetry, many of which are listed in this article.  Symmetry in geometry  The types of symmetry considered in basic geometry (like reflection and rotation symmetry) are described more fully in the main article on symmetry .  Symmetry in calculus  Even and odd functions  Even functions  Let f ( x ) be a real -valued function of a real variable. Then f is even if the following equation holds for all x and -x in the domain of f :        f   (  x  )    =   f   (   -  x   )     .        f  x     f    x      f(x)=f(-x).\,     Geometrically speaking, the graph face of an even function is symmetric with respect to the y -axis, meaning that its graph remains unchanged after reflection about the y -axis.  Examples of even functions are x , x 2 , x 4 , cos ( x ), and cosh ( x ).  Odd functions  Again, let f ( x ) be a real -valued function of a real variable. Then f is odd if the following equation holds for all x and -x in the domain of f :        -   f   (  x  )     =   f   (   -  x   )     ,          f  x      f    x      -f(x)=f(-x)\,,     or        f   (  x  )    +   f   (   -  x   )     =  0 .          f  x     f    x     0 .    f(x)+f(-x)=0\,.     Geometrically, the graph of an odd function has rotational symmetry with respect to the origin , meaning that its graph remains unchanged after rotation of 180 degrees about the origin.  Examples of odd functions are x , x 3 , sin ( x ), sinh ( x ), and erf ( x ).  Integrating  The integral of an odd function from − A to + A is zero (where A is finite, and the function has no vertical asymptotes between − A and A ).  The integral of an even function from − A to + A is twice the integral from 0 to + A (where A is finite, and the function has no vertical asymptotes between − A and A . This also holds true when A is infinite, but only if the integral converges).  Series   The Maclaurin series of an even function includes only even powers.  The Maclaurin series of an odd function includes only odd powers.  The Fourier series of a periodic even function includes only cosine terms.  The Fourier series of a periodic odd function includes only sine terms.   Symmetry in linear algebra  Symmetry in matrices  In linear algebra , a symmetric matrix is a square matrix that is equal to its transpose . Formally, matrix A is symmetric if      A  =   A  ⊤       A   superscript  A  top     A=A^{\top}     and, because the definition of matrix equality demands equality of their dimensions, only square matrices can be symmetric.  The entries of a symmetric matrix are symmetric with respect to the main diagonal . So if the entries are written as A = ( a ij ), then a ij = a ji , for all indices i and j .  The following 3×3 matrix is symmetric:       [     1    7    3      7    4     -  5       3     -  5     6     ]   .      1  7  3    7  4    5     3    5   6     \begin{bmatrix}1&7&3\\
 7&4&-5\\
 3&-5&6\end{bmatrix}.     Every square diagonal matrix is symmetric, since all off-diagonal entries are zero. Similarly, each diagonal element of a skew-symmetric matrix must be zero, since each is its own negative.  In linear algebra, a real symmetric matrix represents a self-adjoint operator over a real  inner product space . The corresponding object for a complex inner product space is a Hermitian matrix with complex-valued entries, which is equal to its conjugate transpose . Therefore, in linear algebra over the complex numbers, it is often assumed that a symmetric matrix refers to one which has real-valued entries. Symmetric matrices appear naturally in a variety of applications, and typical numerical linear algebra software makes special accommodations for them.  Symmetry in abstract algebra  Symmetric groups  The symmetric group  S n on a finite set of n symbols is the group whose elements are all the permutations of the n symbols, and whose group operation is the composition of such permutations, which are treated as bijective functions from the set of symbols to itself. 1 Since there are n ! ( n  factorial ) possible permutations of a set of n symbols, it follows that the order (the number of elements) of the symmetric group S n is n !.  Symmetric polynomials  A symmetric polynomial is a polynomial  P ( X 1 , X 2 , …, X n ) in n variables, such that if any of the variables are interchanged, one obtains the same polynomial. Formally, P is a symmetric polynomial , if for any permutation σ of the subscripts 1, 2, ..., n one has P ( X σ(1) , X σ(2) , …, X σ( n ) ) = P ( X 1 , X 2 , …, X n ).  Symmetric polynomials arise naturally in the study of the relation between the roots of a polynomial in one variable and its coefficients, since the coefficients can be given by polynomial expressions in the roots, and all roots play a similar role in this setting. From this point of view the elementary symmetric polynomials are the most fundamental symmetric polynomials. A theorem states that any symmetric polynomial can be expressed in terms of elementary symmetric polynomials, which implies that every symmetric  polynomial expression in the roots of a monic polynomial can alternatively be given as a polynomial expression in the coefficients of the polynomial.  Examples  In two variables X 1 , X 2 one has symmetric polynomials like         X  1  3   +   X  2  3    -  7         superscript   subscript  X  1   3    superscript   subscript  X  2   3    7    X_{1}^{3}+X_{2}^{3}-7          4   X  1  2    X  2  2    +    X  1  3    X  2    +    X  1    X  2  3    +    (    X  1   +   X  2    )   4         4   superscript   subscript  X  1   2    superscript   subscript  X  2   2       superscript   subscript  X  1   3    subscript  X  2       subscript  X  1    superscript   subscript  X  2   3     superscript     subscript  X  1    subscript  X  2    4     4X_{1}^{2}X_{2}^{2}+X_{1}^{3}X_{2}+X_{1}X_{2}^{3}+(X_{1}+X_{2})^{4}      and in three variables X 1 , X 2 , X 3 one has for instance         X  1    X  2    X  3    -   2   X  1    X  2    -   2   X  1    X  3    -   2   X  2     X  3            subscript  X  1    subscript  X  2    subscript  X  3      2   subscript  X  1    subscript  X  2      2   subscript  X  1    subscript  X  3      2   subscript  X  2    subscript  X  3      X_{1}X_{2}X_{3}-2X_{1}X_{2}-2X_{1}X_{3}-2X_{2}X_{3}\,      Symmetric tensors  In mathematics , a symmetric tensor is tensor that is invariant under a permutation of its vector arguments:       T   (   v  1   ,   v  2   ,  …  ,   v  r   )    =   T   (   v   σ  1    ,   v   σ  2    ,  …  ,   v   σ  r    )          T    subscript  v  1    subscript  v  2   normal-…   subscript  v  r       T    subscript  v    σ  1     subscript  v    σ  2    normal-…   subscript  v    σ  r        T(v_{1},v_{2},\dots,v_{r})=T(v_{\sigma 1},v_{\sigma 2},\dots,v_{\sigma r})   for every permutation σ of the symbols {1,2,..., r }. Alternatively, an r th order symmetric tensor represented in coordinates as a quantity with r indices satisfies        T    i  1    i  2   …   i  r     =   T    i   σ  1     i   σ  2    …   i   σ  r       .       subscript  T     subscript  i  1    subscript  i  2   normal-…   subscript  i  r      subscript  T     subscript  i    σ  1     subscript  i    σ  2    normal-…   subscript  i    σ  r        T_{i_{1}i_{2}\dots i_{r}}=T_{i_{\sigma 1}i_{\sigma 2}\dots i_{\sigma r}}.     The space of symmetric tensors of rank r on a finite-dimensional vector space is naturally isomorphic to the dual of the space of homogeneous polynomials of degree r on V . Over fields of characteristic zero , the graded vector space of all symmetric tensors can be naturally identified with the symmetric algebra on V . A related concept is that of the antisymmetric tensor or alternating form . Symmetric tensors occur widely in engineering , physics and mathematics .  Galois theory  Given a polynomial, it may be that some of the roots are connected by various algebraic equations . For example, it may be that for two of the roots, say A and B , that . The central idea of Galois theory is to consider those permutations (or rearrangements) of the roots having the property that any algebraic equation satisfied by the roots is still satisfied after the roots have been permuted. An important proviso is that we restrict ourselves to algebraic equations whose coefficients are rational numbers . Thus, Galois theory studies the symmetries inherent in algebraic equations.  Automorphisms of algebraic objects  In abstract algebra , an automorphism is an isomorphism from a mathematical object to itself. It is, in some sense, a symmetry of the object, and a way of mapping the object to itself while preserving all of its structure. The set of all automorphisms of an object forms a group , called the automorphism group . It is, loosely speaking, the symmetry group of the object.  Examples   In set theory , an arbitrary permutation of the elements of a set X is an automorphism. The automorphism group of X is also called the symmetric group on X .  In elementary arithmetic , the set of integers , Z , considered as a group under addition, has a unique nontrivial automorphism: negation. Considered as a ring , however, it has only the trivial automorphism. Generally speaking, negation is an automorphism of any abelian group , but not of a ring or field.  A group automorphism is a group isomorphism from a group to itself. Informally, it is a permutation of the group elements such that the structure remains unchanged. For every group G there is a natural group homomorphism G → Aut( G ) whose image is the group Inn( G ) of inner automorphisms and whose kernel is the center of G . Thus, if G has trivial center it can be embedded into its own automorphism group.     In linear algebra , an endomorphism of a vector space  V is a linear operator  V → V . An automorphism is an invertible linear operator on V . When the vector space is finite-dimensional, the automorphism group of V is the same as the general linear group , GL( V ).  A field automorphism is a bijective  ring homomorphism from a field to itself. In the cases of the rational numbers ( Q ) and the real numbers ( R ) there are no nontrivial field automorphisms. Some subfields of R have nontrivial field automorphisms, which however do not extend to all of R (because they cannot preserve the property of a number having a square root in R ). In the case of the complex numbers , C , there is a unique nontrivial automorphism that sends R into R : complex conjugation , but there are infinitely ( uncountably ) many "wild" automorphisms (assuming the axiom of choice ). 2 Field automorphisms are important to the theory of field extensions , in particular Galois extensions . In the case of a Galois extension L / K the subgroup of all automorphisms of L fixing K pointwise is called the Galois group of the extension.   Symmetry in representation theory  Symmetry in quantum mechanics: bosons and fermions  In quantum mechanics, bosons have representatives that are symmetric under permutation operators, and fermions have antisymmetric representatives.  This implies the Pauli exclusion principle for fermions. In fact, the Pauli exclusion principle with a single-valued many-particle wavefunction is equivalent to requiring the wavefunction to be antisymmetric. An antisymmetric two-particle state is represented as a sum of states in which one particle is in state    |  x  ⟩     ket  x    \scriptstyle|x\rangle   and the other in state    |  y  ⟩     ket  y    \scriptstyle|y\rangle   :       |  ψ  ⟩   =    ∑   x  ,  y     A   (  x  ,  y  )    |   x  ,  y   ⟩          ket  ψ     subscript    x  y      A   x  y    ket   x  y        |\psi\rangle=\sum_{x,y}A(x,y)|x,y\rangle     and antisymmetry under exchange means that . This implies that , which is Pauli exclusion. It is true in any basis, since unitary changes of basis keep antisymmetric matrices antisymmetric, although strictly speaking, the quantity  is not a matrix but an antisymmetric rank-two tensor .  Conversely, if the diagonal quantities  are zero in every basis , then the wavefunction component:       A   (  x  ,  y  )    =   ⟨  ψ  |   x  ,  y   ⟩   =    ⟨  ψ  |    (    |  x  ⟩   ⊗   |  y  ⟩    )            A   x  y     inner-product  ψ   x  y            bra  ψ    tensor-product   ket  x    ket  y        A(x,y)=\langle\psi|x,y\rangle=\langle\psi|(|x\rangle\otimes|y\rangle)     is necessarily antisymmetric. To prove it, consider the matrix element:       ⟨  ψ  |    (    (    |  x  ⟩   +   |  y  ⟩    )   ⊗   (    |  x  ⟩   +   |  y  ⟩    )    )        bra  ψ    tensor-product     ket  x    ket  y       ket  x    ket  y       \langle\psi|((|x\rangle+|y\rangle)\otimes(|x\rangle+|y\rangle))\,     This is zero, because the two particles have zero probability to both be in the superposition state     |  x  ⟩   +   |  y  ⟩        ket  x    ket  y     \scriptstyle|x\rangle+|y\rangle   . But this is equal to       ⟨  ψ  |   x  ,  x   ⟩   +   ⟨  ψ  |   x  ,  y   ⟩   +   ⟨  ψ  |   y  ,  x   ⟩   +   ⟨  ψ  |   y  ,  y   ⟩        inner-product  ψ   x  x     inner-product  ψ   x  y     inner-product  ψ   y  x     inner-product  ψ   y  y      \langle\psi|x,x\rangle+\langle\psi|x,y\rangle+\langle\psi|y,x\rangle+\langle%
 \psi|y,y\rangle\,     The first and last terms on the right hand side are diagonal elements and are zero, and the whole sum is equal to zero. So the wavefunction matrix elements obey:        ⟨  ψ  |   x  ,  y   ⟩   +   ⟨  ψ  |   y  ,  x   ⟩    =   0          inner-product  ψ   x  y     inner-product  ψ   y  x     0    \langle\psi|x,y\rangle+\langle\psi|y,x\rangle=0\,   .  or       A   (  x  ,  y  )    =   -   A   (  y  ,  x  )           A   x  y        A   y  x       A(x,y)=-A(y,x)\,     Symmetry in set theory  Symmetric relation  We call a relation symmetric if every time the relation stands from A to B, it stands too from B to A. Note that symmetry is not the exact opposite of antisymmetry .  Symmetry in metric spaces  Isometries of a space  An isometry is a distance -preserving map between metric spaces . Given a metric space, or a set and scheme for assigning distances between elements of the set, an isometry is a transformation which maps elements to another metric space such that the distance between the elements in the new metric space is equal to the distance between the elements in the original metric space. In a two-dimensional or three-dimensional space, two geometric figures are congruent if they are related by an isometry: related by either a rigid motion , or a composition of a rigid motion and a reflection . Up to a relation by a rigid motion, they are equal if related by a direct isometry .  Isometries were used to build a unfying symmetry definition working in geometry and for functions, probability distributions, matrices, strings, graphs, etc. 3  Symmetries of differential equations  A symmetry of a differential equation is a transformation that leaves the differential equation invariant. Knowledge of such symmetries may help solve the differential equation.  A Lie symmetry of a system of differential equations is a continuous symmetry of the system of differential equations. Knowledge of a Lie symmetry can be used to simplify an ordinary differential equation through reduction of order . 4  For ordinary differential equations , knowledge of an appropriate set of Lie symmetries allows one to explicitly calculate a set of first integrals, yielding a complete solution without integration.  Symmetries may be found by solving a related set of ordinary differential equations. 5 Solving these equations is often much simpler than solving the original differential equations.  Symmetry in probability  In the case of a finite number of possible outcomes, symmetry with respect to permutations (relabelings) implies a discrete uniform distribution .  In the case of a real interval of possible outcomes, symmetry with respect to interchanging sub-intervals of equal length corresponds to a continuous uniform distribution .  In other cases, such as "taking a random integer" or "taking a random real number", there are no probability distributions at all symmetric with respect to relabellings or to exchange of equally long subintervals. Other reasonable symmetries do not single out one particular distribution, or in other words, there is not a unique probability distribution providing maximum symmetry.  There is one type of isometry in one dimension that may leave the probability distribution unchanged, that is reflection in a point, for example zero.  A possible symmetry for randomness with positive outcomes is that the former applies for the logarithm, i.e., the outcome and its reciprocal have the same distribution. However this symmetry does not single out any particular distribution uniquely.  For a "random point" in a plane or in space, one can choose an origin, and consider a probability distribution with circular or spherical symmetry, respectively.  See also   Use of symmetry in integration  Invariance (mathematics)   References  Bibliography   Hermann Weyl , Symmetry. Reprint of the 1952 original. Princeton Science Library. Princeton University Press, Princeton, NJ, 1989. viii+168 pp. ISBN 0-691-02374-3  Mark Ronan, Symmetry and the Monster , Oxford University Press, 2006. ISBN 978-0-19-280723-6 (Concise introduction for lay reader)  Marcus du Sautoy , Finding Moonshine: a Mathematician's Journey through Symmetry , Fourth Estate , 2009   "  Category:Symmetry     Jacobson (2009), p. 31. ↩  ↩  ↩  ↩      