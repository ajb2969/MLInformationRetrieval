


Differential dynamic programming




Differential dynamic programming

Differential dynamic programming (DDP) is an optimal control algorithm of the trajectory optimization class. The algorithm was introduced in 1966 by Mayne1 and subsequently analysed in Jacobson and Mayne's eponymous book.2 The algorithm uses locally-quadratic models of the dynamics and cost functions, and displays quadratic convergence. It is closely related to Pantoja's step-wise Newton's method.34
Finite-horizon discrete-time problems
The dynamics
describe the evolution of the state 
 
 
 
  given the control 
 
 
 
  from time 
 
 
 
  to time 
 
 
 
 . The total cost

 
  is the sum of running costs 
 
 
 
  and final cost 
 
 
 
 , incurred when starting from state 
 
 
 
  and applying the control sequence 
 
 
 
  until the horizon is reached:



where 
 
 
 
 , and the 
 
 
 
  for 
 
 
 
  are given by . The solution of the optimal control problem is the minimizing control sequence 
 
 
Trajectory optimization means finding 
 
 
 
  for a particular 
 
 
 
 , rather than for all possible initial states.
Dynamic programming
Let 
 
 
 
  be the partial control sequence 
 
 
 
  and define the cost-to-go

 
  as the partial sum of costs from 
 
 
 
  to 
 
 
 
 :



The optimal cost-to-go or value function at time 
 
 
 
  is the cost-to-go given the minimizing control sequence:



Setting 
 
 
 
 , the dynamic programming principle reduces the minimization over an entire sequence of controls to a sequence of minimizations over a single control, proceeding backwards in time:
[\ell(\mathbf{x},\mathbf{u}) + V(\mathbf{f}(\mathbf{x},\mathbf{u}),i+1)].|2}}
This is the Bellman equation.
Differential dynamic programming
DDP proceeds by iteratively performing a backward pass on the nominal trajectory to generate a new control sequence, and then a forward-pass to compute and evaluate a new nominal trajectory. We begin with the backward pass. If



is the argument of the 
 
 
 
  operator in , let 
 
 
 
  be the variation of this quantity around the 
 
 
 
 -th 
 
 
 
  pair:



and expand to second order
& Q_{\mathbf{x}\mathbf{u}}\\ Q_\mathbf{u} & Q_{\mathbf{u}\mathbf{x}} & Q_{\mathbf{u}\mathbf{u}} \end{bmatrix} \begin{bmatrix} 1\\ \delta\mathbf{x}\\ \delta\mathbf{u} \end{bmatrix} |3}}
The 
 
 
 
  notation used here is a variant of the notation of Morimoto where subscripts denote differentiation in denominator layout.5
Dropping the index 
 
 
 
  for readability, primes denoting the next time-step 
 
 
 
 , the expansion coefficients are



The last terms in the last three equations denote contraction of a vector with a tensor. Minimizing the quadratic approximation  with respect to 
 
 
 
  we have
^* = \operatorname{argmin}\limits_{\delta \mathbf{u}}Q(\delta \mathbf{x},\delta \mathbf{u})=-Q_{\mathbf{u}\mathbf{u}}^{-1}(Q_\mathbf{u}+Q_{\mathbf{u}\mathbf{x}}\delta \mathbf{x}), |4}}
giving an open-loop term 
 
 
 
  and a feedback gain term 
 
 
 
 . Plugging the result back into , we now have a quadratic model of the value at time 
 
 
 
 :



Recursively computing the local quadratic models of 
 
 
 
  and the control modifications 
 
 
 
 , from 
 
 
 
  down to 
 
 
 
 , constitutes the backward pass. As above, the Value is initialized with 
 
 
 
 . Once the backward pass is completed, a forward pass computes a new trajectory:



The backward passes and forward passes are iterated until convergence.
Regularization and line-search
Differential dynamic programming is a second-order algorithm like Newton's method. It therefore takes large steps toward the minimum and often requires regularization and/or line-search to achieve convergence 6 .7 Regularization in the DDP context means ensuring that the 
 
 
 
  matrix in  is positive definite. Line-search in DDP amounts to scaling the open-loop control modification 
 
 
 
  by some 
 
 
 
 .
See also

Optimal control

References
External links

A Python implementation of DDP
A MATLAB implementation of DDP

"
Category:Articles created via the Article Wizard Category:Dynamic programming














