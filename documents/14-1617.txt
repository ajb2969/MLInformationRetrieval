   Differential dynamic programming      Differential dynamic programming   Differential dynamic programming (DDP) is an optimal control algorithm of the trajectory optimization class. The algorithm was introduced in 1966 by Mayne 1 and subsequently analysed in Jacobson and Mayne's eponymous book. 2 The algorithm uses locally-quadratic models of the dynamics and cost functions, and displays quadratic convergence . It is closely related to Pantoja's step-wise Newton's method. 3 4  Finite-horizon discrete-time problems  The dynamics  describe the evolution of the state   ğ±   ğ±   \textstyle\mathbf{x}   given the control   ğ®   ğ®   \mathbf{u}   from time   i   i   i   to time    i  +  1      i  1    i+1   . The total cost     J  0     subscript  J  0    J_{0}   is the sum of running costs   â„“   normal-â„“   \textstyle\ell   and final cost    â„“  f     subscript  normal-â„“  f    \ell_{f}   , incurred when starting from state   ğ±   ğ±   \mathbf{x}   and applying the control sequence    ğ”  â‰¡   {   ğ®  0   ,    ğ®  1   â€¦   ,   ğ®   N  -  1    }       ğ”    subscript  ğ®  0      subscript  ğ®  1   normal-â€¦    subscript  ğ®    N  1       \mathbf{U}\equiv\{\mathbf{u}_{0},\mathbf{u}_{1}\dots,\mathbf{u}_{N-1}\}   until the horizon is reached:         J  0    (  ğ±  ,  ğ”  )    =     âˆ‘   i  =  0    N  -  1     â„“   (   ğ±  i   ,   ğ®  i   )     +    â„“  f    (   ğ±  N   )      ,         subscript  J  0    ğ±  ğ”        superscript   subscript     i  0      N  1      normal-â„“    subscript  ğ±  i    subscript  ğ®  i         subscript  normal-â„“  f    subscript  ğ±  N       J_{0}(\mathbf{x},\mathbf{U})=\sum_{i=0}^{N-1}\ell(\mathbf{x}_{i},\mathbf{u}_{i%
 })+\ell_{f}(\mathbf{x}_{N}),     where     ğ±  0   â‰¡  ğ±       subscript  ğ±  0   ğ±    \mathbf{x}_{0}\equiv\mathbf{x}   , and the    ğ±  i     subscript  ğ±  i    \mathbf{x}_{i}   for    i  >  0      i  0    i>0   are given by . The solution of the optimal control problem is the minimizing control sequence       ğ”  *    (  ğ±  )    â‰¡     argmin  ğ”    J  0     (  ğ±  ,  ğ”  )     .         superscript  ğ”    ğ±       subscript  argmin  ğ”    subscript  J  0     ğ±  ğ”      \mathbf{U}^{*}(\mathbf{x})\equiv\operatorname{argmin}_{\mathbf{U}}J_{0}(%
 \mathbf{x},\mathbf{U}).    Trajectory optimization means finding     ğ”  *    (  ğ±  )        superscript  ğ”    ğ±    \mathbf{U}^{*}(\mathbf{x})   for a particular   ğ±   ğ±   \mathbf{x}   , rather than for all possible initial states.  Dynamic programming  Let    ğ”  i     subscript  ğ”  i    \mathbf{U}_{i}   be the partial control sequence     ğ”  i   â‰¡   {   ğ®  i   ,    ğ®   i  +  1    â€¦   ,   ğ®   N  -  1    }        subscript  ğ”  i     subscript  ğ®  i      subscript  ğ®    i  1    normal-â€¦    subscript  ğ®    N  1       \mathbf{U}_{i}\equiv\{\mathbf{u}_{i},\mathbf{u}_{i+1}\dots,\mathbf{u}_{N-1}\}   and define the cost-to-go     J  i     subscript  J  i    J_{i}   as the partial sum of costs from   i   i   i   to   N   N   N   :         J  i    (  ğ±  ,   ğ”  i   )    =     âˆ‘   j  =  i    N  -  1     â„“   (   ğ±  j   ,   ğ®  j   )     +    â„“  f    (   ğ±  N   )      .         subscript  J  i    ğ±   subscript  ğ”  i         superscript   subscript     j  i      N  1      normal-â„“    subscript  ğ±  j    subscript  ğ®  j         subscript  normal-â„“  f    subscript  ğ±  N       J_{i}(\mathbf{x},\mathbf{U}_{i})=\sum_{j=i}^{N-1}\ell(\mathbf{x}_{j},\mathbf{u%
 }_{j})+\ell_{f}(\mathbf{x}_{N}).     The optimal cost-to-go or value function at time   i   i   i   is the cost-to-go given the minimizing control sequence:        V   (  ğ±  ,  i  )    â‰¡     min   ğ”  i     J  i     (  ğ±  ,   ğ”  i   )     .        V   ğ±  i        subscript    subscript  ğ”  i     subscript  J  i     ğ±   subscript  ğ”  i       V(\mathbf{x},i)\equiv\min_{\mathbf{U}_{i}}J_{i}(\mathbf{x},\mathbf{U}_{i}).     Setting     V   (  ğ±  ,  N  )    â‰¡    â„“  f    (   ğ±  N   )          V   ğ±  N       subscript  normal-â„“  f    subscript  ğ±  N      V(\mathbf{x},N)\equiv\ell_{f}(\mathbf{x}_{N})   , the dynamic programming principle reduces the minimization over an entire sequence of controls to a sequence of minimizations over a single control, proceeding backwards in time:  [\ell(\mathbf{x},\mathbf{u}) + V(\mathbf{f}(\mathbf{x},\mathbf{u}),i+1)].| 2 }}  This is the Bellman equation .  Differential dynamic programming  DDP proceeds by iteratively performing a backward pass on the nominal trajectory to generate a new control sequence, and then a forward-pass to compute and evaluate a new nominal trajectory. We begin with the backward pass. If       â„“   (  ğ±  ,  ğ®  )    +   V   (   ğŸ   (  ğ±  ,  ğ®  )    ,   i  +  1   )          normal-â„“   ğ±  ğ®      V     ğŸ   ğ±  ğ®      i  1       \ell(\mathbf{x},\mathbf{u})+V(\mathbf{f}(\mathbf{x},\mathbf{u}),i+1)     is the argument of the    min   [  ]           \min[]   operator in , let   Q   Q   Q   be the variation of this quantity around the   i   i   i   -th    (  ğ±  ,  ğ®  )     ğ±  ğ®    (\mathbf{x},\mathbf{u})   pair:       Q   (   Î´  ğ±   ,   Î´  ğ®   )    â‰¡         Q     Î´  ğ±     Î´  ğ®     absent    \displaystyle Q(\delta\mathbf{x},\delta\mathbf{u})\equiv     and expand to second order  & Q_{\mathbf{x}\mathbf{u}}\\ Q_\mathbf{u} & Q_{\mathbf{u}\mathbf{x}} & Q_{\mathbf{u}\mathbf{u}} \end{bmatrix} \begin{bmatrix} 1\\ \delta\mathbf{x}\\ \delta\mathbf{u} \end{bmatrix} | 3 }}  The   Q   Q   Q   notation used here is a variant of the notation of Morimoto where subscripts denote differentiation in denominator layout. 5  Dropping the index   i   i   i   for readability, primes denoting the next time-step     V  â€²   â‰¡   V   (   i  +  1   )         superscript  V  normal-â€²     V    i  1      V^{\prime}\equiv V(i+1)   , the expansion coefficients are      Q  ğ±     subscript  Q  ğ±    \displaystyle Q_{\mathbf{x}}     The last terms in the last three equations denote contraction of a vector with a tensor. Minimizing the quadratic approximation  with respect to    Î´  ğ®      Î´  ğ®    \delta\mathbf{u}   we have  ^* = \operatorname{argmin}\limits_{\delta \mathbf{u}}Q(\delta \mathbf{x},\delta \mathbf{u})=-Q_{\mathbf{u}\mathbf{u}}^{-1}(Q_\mathbf{u}+Q_{\mathbf{u}\mathbf{x}}\delta \mathbf{x}), | 4 }}  giving an open-loop term    ğ¤  =   -    Q  ğ®ğ®   -  1     Q  ğ®         ğ¤       superscript   subscript  Q  ğ®ğ®     1     subscript  Q  ğ®       \mathbf{k}=-Q_{\mathbf{u}\mathbf{u}}^{-1}Q_{\mathbf{u}}   and a feedback gain term    ğŠ  =   -    Q  ğ®ğ®   -  1     Q  ğ®ğ±         ğŠ       superscript   subscript  Q  ğ®ğ®     1     subscript  Q  ğ®ğ±       \mathbf{K}=-Q_{\mathbf{u}\mathbf{u}}^{-1}Q_{\mathbf{u}\mathbf{x}}   . Plugging the result back into , we now have a quadratic model of the value at time   i   i   i   :      Î”  V   (  i  )       normal-Î”  V  i    \displaystyle\Delta V(i)     Recursively computing the local quadratic models of    V   (  i  )       V  i    V(i)   and the control modifications    {   ğ¤   (  i  )    ,   ğŠ   (  i  )    }       ğ¤  i     ğŠ  i     \{\mathbf{k}(i),\mathbf{K}(i)\}   , from    i  =   N  -  1       i    N  1     i=N-1   down to    i  =  1      i  1    i=1   , constitutes the backward pass. As above, the Value is initialized with     V   (  ğ±  ,  N  )    â‰¡    â„“  f    (   ğ±  N   )          V   ğ±  N       subscript  normal-â„“  f    subscript  ğ±  N      V(\mathbf{x},N)\equiv\ell_{f}(\mathbf{x}_{N})   . Once the backward pass is completed, a forward pass computes a new trajectory:       ğ±  ^    (  1  )        normal-^  ğ±   1    \displaystyle\hat{\mathbf{x}}(1)     The backward passes and forward passes are iterated until convergence.  Regularization and line-search  Differential dynamic programming is a second-order algorithm like Newton's method . It therefore takes large steps toward the minimum and often requires regularization and/or line-search to achieve convergence 6 . 7 Regularization in the DDP context means ensuring that the    Q  ğ®ğ®     subscript  Q  ğ®ğ®    Q_{\mathbf{u}\mathbf{u}}   matrix in  is positive definite . Line-search in DDP amounts to scaling the open-loop control modification   ğ¤   ğ¤   \mathbf{k}   by some    0  <  Î±  <  1        0  Î±       1     0<\alpha<1   .  See also   Optimal control   References  External links   A Python implementation of DDP  A MATLAB implementation of DDP   "  Category:Articles created via the Article Wizard  Category:Dynamic programming     â†©  â†©  â†©  â†©  â†©  â†©  â†©     