<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1449">Loss functions for classification</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Loss functions for classification</h1>
<hr/>

<p> In <a href="machine_learning" title="wikilink">machine learning</a> and <a href="mathematical_optimization" title="wikilink">mathematical optimization</a>, <strong>loss functions for classification</strong> are computationally feasible <a href="loss_functions" title="wikilink">loss functions</a> representing the price paid for inaccuracy of predictions in classification problems.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Given 

<math display="inline" id="Loss_functions_for_classification:0">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 as the vector space of all possible inputs, and <em>Y</em> = {–1,1} as the vector space of all possible outputs, we wish to find a function 

<math display="inline" id="Loss_functions_for_classification:1">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>:</mo>
   <mrow>
    <mi>X</mi>
    <mo>↦</mo>
    <mi>ℝ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>f</ci>
    <apply>
     <csymbol cd="latexml">maps-to</csymbol>
     <ci>X</ci>
     <ci>ℝ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f:X\mapsto\mathbb{R}
  </annotation>
 </semantics>
</math>

 which best maps 

<math display="inline" id="Loss_functions_for_classification:2">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(\vec{x})
  </annotation>
 </semantics>
</math>

 to 

<math display="inline" id="Loss_functions_for_classification:3">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same 

<math display="inline" id="Loss_functions_for_classification:4">
 <semantics>
  <mover accent="true">
   <mi>x</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{x}
  </annotation>
 </semantics>
</math>

 to generate different 

<math display="inline" id="Loss_functions_for_classification:5">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> As a result, the goal of the learning problem is to minimize expected risk, defined as</p>

<p>

<math display="block" id="Loss_functions_for_classification:6">
 <semantics>
  <mrow>
   <mrow>
    <mi>I</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mi>f</mi>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mo largeop="true" symmetric="true">∫</mo>
     <mrow>
      <mi>X</mi>
      <mo>⊗</mo>
      <mi>Y</mi>
     </mrow>
    </msub>
    <mrow>
     <mi>V</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>f</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mover accent="true">
         <mi>x</mi>
         <mo stretchy="false">→</mo>
        </mover>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>,</mo>
      <mi>y</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>p</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mo>,</mo>
      <mi>y</mi>
      <mo rspace="4.2pt" stretchy="false">)</mo>
     </mrow>
     <mi>d</mi>
     <mpadded width="+1.7pt">
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
     </mpadded>
     <mi>d</mi>
     <mi>y</mi>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>I</ci>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <ci>f</ci>
     </apply>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <int></int>
      <apply>
       <csymbol cd="latexml">tensor-product</csymbol>
       <ci>X</ci>
       <ci>Y</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>V</ci>
      <interval closure="open">
       <apply>
        <times></times>
        <ci>f</ci>
        <apply>
         <ci>normal-→</ci>
         <ci>x</ci>
        </apply>
       </apply>
       <ci>y</ci>
      </interval>
      <ci>p</ci>
      <interval closure="open">
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <ci>y</ci>
      </interval>
      <ci>d</ci>
      <apply>
       <ci>normal-→</ci>
       <ci>x</ci>
      </apply>
      <ci>d</ci>
      <ci>y</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I[f]=\displaystyle\int_{X\otimes Y}V(f(\vec{x}),y)p(\vec{x},y)\,d\vec{x}\,dy
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Loss_functions_for_classification:7">
 <semantics>
  <mrow>
   <mi>V</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>V</ci>
    <interval closure="open">
     <apply>
      <times></times>
      <ci>f</ci>
      <apply>
       <ci>normal-→</ci>
       <ci>x</ci>
      </apply>
     </apply>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(f(\vec{x}),y)
  </annotation>
 </semantics>
</math>

 represents the loss function, and 

<math display="inline" id="Loss_functions_for_classification:8">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>p</ci>
    <interval closure="open">
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(\vec{x},y)
  </annotation>
 </semantics>
</math>

 represents the probability distribution of the data, which can equivalently be written using <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a> as</p>

<p>

<math display="block" id="Loss_functions_for_classification:9">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo>∣</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
     <ci>normal-,</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-∣</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-.</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(\vec{x},y)=p(y\mid\vec{x})p(\vec{x}).
  </annotation>
 </semantics>
</math>

</p>

<p>In practice, the probability distribution 

<math display="inline" id="Loss_functions_for_classification:10">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>p</ci>
    <interval closure="open">
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(\vec{x},y)
  </annotation>
 </semantics>
</math>

 is unknown. Consequently, utilizing a training set of 

<math display="inline" id="Loss_functions_for_classification:11">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 <a href="iid" title="wikilink">independently and identically distributed</a> samples</p>

<p>

<math display="block" id="Loss_functions_for_classification:12">
 <semantics>
  <mrow>
   <mi>S</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mn>1</mn>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>y</mi>
      <mn>1</mn>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mi>n</mi>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>y</mi>
      <mi>n</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>S</ci>
    <set>
     <interval closure="open">
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <cn type="integer">1</cn>
      </apply>
     </interval>
     <ci>normal-…</ci>
     <interval closure="open">
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>n</ci>
      </apply>
     </interval>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S=\{(\vec{x}_{1},y_{1}),\dots,(\vec{x}_{n},y_{n})\}
  </annotation>
 </semantics>
</math>

</p>

<p>drawn from the data <a href="sample_space" title="wikilink">sample space</a>, one seeks to <a href="empirical_risk_minimization" title="wikilink"> minimize empirical risk</a></p>

<p>

<math display="block" id="Loss_functions_for_classification:13">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mi>I</mi>
      <mi>S</mi>
     </msub>
     <mrow>
      <mo stretchy="false">[</mo>
      <mi>f</mi>
      <mo stretchy="false">]</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mfrac>
      <mn>1</mn>
      <mi>n</mi>
     </mfrac>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>n</mi>
      </munderover>
      <mrow>
       <mi>V</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mi>f</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <msub>
           <mover accent="true">
            <mi>x</mi>
            <mo stretchy="false">→</mo>
           </mover>
           <mi>i</mi>
          </msub>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>,</mo>
        <msub>
         <mi>y</mi>
         <mi>i</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>I</ci>
      <ci>S</ci>
     </apply>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <ci>f</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>n</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <times></times>
       <ci>V</ci>
       <interval closure="open">
        <apply>
         <times></times>
         <ci>f</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <apply>
           <ci>normal-→</ci>
           <ci>x</ci>
          </apply>
          <ci>i</ci>
         </apply>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>i</ci>
        </apply>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I_{S}[f]=\frac{1}{n}\sum_{i=1}^{n}V(f(\vec{x}_{i}),y_{i}).
  </annotation>
 </semantics>
</math>

</p>

<p>as a proxy for expected risk.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> (See <a href="statistical_learning_theory" title="wikilink">statistical learning theory</a> for a more detailed description.)</p>

<p>For computational ease, it is standard practice to write <a href="loss_functions" title="wikilink">loss functions</a> as functions of only one variable. Within classification, loss functions are generally written solely in terms of the product of the true classifier 

<math display="inline" id="Loss_functions_for_classification:14">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 and the predicted value 

<math display="inline" id="Loss_functions_for_classification:15">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(\vec{x})
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> Selection of a loss function within this framework</p>

<p>

<math display="block" id="Loss_functions_for_classification:16">
 <semantics>
  <mrow>
   <mrow>
    <mi>V</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mover accent="true">
        <mi>x</mi>
        <mo stretchy="false">→</mo>
       </mover>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>,</mo>
     <mi>y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>ϕ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mo>-</mo>
      <mrow>
       <mi>y</mi>
       <mi>f</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mover accent="true">
         <mi>x</mi>
         <mo stretchy="false">→</mo>
        </mover>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>V</ci>
     <interval closure="open">
      <apply>
       <times></times>
       <ci>f</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
      <ci>y</ci>
     </interval>
    </apply>
    <apply>
     <times></times>
     <ci>ϕ</ci>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>y</ci>
       <ci>f</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(f(\vec{x}),y)=\phi(-yf(\vec{x}))
  </annotation>
 </semantics>
</math>

 impacts the optimal 

<math display="inline" id="Loss_functions_for_classification:17">
 <semantics>
  <msubsup>
   <mi>f</mi>
   <mi>S</mi>
   <mo>*</mo>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>f</ci>
     <times></times>
    </apply>
    <ci>S</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f^{*}_{S}
  </annotation>
 </semantics>
</math>

 which <a href="empirical_risk_minimization" title="wikilink">minimizes empirical risk</a>, as well as the computational complexity of the learning algorithm.</p>

<p>Given the binary nature of classification, a natural selection for a loss function (assuming equal cost for <a href="false_positives_and_false_negatives" title="wikilink">false positives and false negatives</a>) would be the 0–1 <a href="indicator_function" title="wikilink">indicator function</a> which takes the value of 0 if the predicted classification equals that of the true class or a 1 if the predicted classification does not match the true class. This selection is modeled by</p>

<p>

<math display="block" id="Loss_functions_for_classification:18">
 <semantics>
  <mrow>
   <mrow>
    <mi>V</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mover accent="true">
        <mi>x</mi>
        <mo stretchy="false">→</mo>
       </mover>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>,</mo>
     <mi>y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>H</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mo>-</mo>
      <mrow>
       <mi>y</mi>
       <mi>f</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mover accent="true">
         <mi>x</mi>
         <mo stretchy="false">→</mo>
        </mover>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>V</ci>
     <interval closure="open">
      <apply>
       <times></times>
       <ci>f</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
      <ci>y</ci>
     </interval>
    </apply>
    <apply>
     <times></times>
     <ci>H</ci>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>y</ci>
       <ci>f</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(f(\vec{x}),y)=H(-yf(\vec{x}))
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Loss_functions_for_classification:19">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H
  </annotation>
 </semantics>
</math>

 indicates the <a href="Heaviside_step_function" title="wikilink">Heaviside step function</a>. However, this loss function is non-convex and non-smooth, and solving for the optimal solution is an <a class="uri" href="NP-hard" title="wikilink">NP-hard</a> combinatorial optimization problem.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> As a result, it is better to substitute continuous, convex <strong>loss function surrogates</strong> which are tractable for commonly used learning algorithms. In addition to their computational tractability, one can show that the solutions to the learning problem using these loss surrogates allows for the recovery of the actual solution to the original classification problem.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> Some of these surrogates are described below.</p>
<h2 id="bounds-for-classification">Bounds for classification</h2>

<p>Utilizing <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a>, it can be shown that the optimal 

<math display="inline" id="Loss_functions_for_classification:20">
 <semantics>
  <msup>
   <mi>f</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>f</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f^{*}
  </annotation>
 </semantics>
</math>

 for a binary classification problem is equivalent to</p>

<p>

<math display="block" id="Loss_functions_for_classification:21">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>f</mi>
     <mo>*</mo>
    </msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mo rspace="5.3pt" stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo rspace="5.3pt">=</mo>
   <mrow>
    <mo>{</mo>
    <mtable displaystyle="true">
     <mtr>
      <mtd columnalign="left">
       <mn>1</mn>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mtext>if</mtext>
        <mi>p</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mn>1</mn>
         <mo>∣</mo>
         <mover accent="true">
          <mi>x</mi>
          <mo stretchy="false">→</mo>
         </mover>
         <mo stretchy="false">)</mo>
        </mrow>
        <mo>></mo>
        <mi>p</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mo>-</mo>
         <mn>1</mn>
         <mo>∣</mo>
         <mover accent="true">
          <mi>x</mi>
          <mo stretchy="false">→</mo>
         </mover>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mtext>if</mtext>
        <mi>p</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mn>1</mn>
         <mo>∣</mo>
         <mover accent="true">
          <mi>x</mi>
          <mo stretchy="false">→</mo>
         </mover>
         <mo stretchy="false">)</mo>
        </mrow>
        <mo><</mo>
        <mi>p</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mo>-</mo>
         <mn>1</mn>
         <mo>∣</mo>
         <mover accent="true">
          <mi>x</mi>
          <mo stretchy="false">→</mo>
         </mover>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mtd>
     </mtr>
    </mtable>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>f</ci>
      <times></times>
     </apply>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="latexml">cases</csymbol>
     <cn type="integer">1</cn>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <mtext>if</mtext>
      <csymbol cd="unknown">p</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <cn type="integer">1</cn>
       <ci>normal-∣</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
      <gt></gt>
      <csymbol cd="unknown">p</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <minus></minus>
       <cn type="integer">1</cn>
       <ci>normal-∣</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
     </apply>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <mtext>if</mtext>
      <csymbol cd="unknown">p</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <cn type="integer">1</cn>
       <ci>normal-∣</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
      <lt></lt>
      <csymbol cd="unknown">p</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <minus></minus>
       <cn type="integer">1</cn>
       <ci>normal-∣</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f^{*}(\vec{x})\;=\;\begin{cases}1&\text{if }p(1\mid\vec{x})>p(-1\mid\vec{x})\\
-1&\text{if }p(1\mid\vec{x})<p(-1\mid\vec{x})\end{cases}
  </annotation>
 </semantics>
</math>

 (when 

<math display="inline" id="Loss_functions_for_classification:22">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>1</mn>
    <mo>∣</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>≠</mo>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mo>-</mo>
    <mn>1</mn>
    <mo>∣</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="integer">1</cn>
     <ci>normal-∣</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <neq></neq>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <minus></minus>
     <cn type="integer">1</cn>
     <ci>normal-∣</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(1\mid\vec{x})\neq p(-1\mid\vec{x})
  </annotation>
 </semantics>
</math>

). Furthermore, it can be shown that for any convex loss function 

<math display="inline" id="Loss_functions_for_classification:23">
 <semantics>
  <mrow>
   <mi>V</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>y</mi>
     <msub>
      <mi>f</mi>
      <mn>0</mn>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>V</ci>
    <apply>
     <times></times>
     <ci>y</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>f</ci>
      <cn type="integer">0</cn>
     </apply>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(yf_{0}(\vec{x}))
  </annotation>
 </semantics>
</math>

 for which 

<math display="inline" id="Loss_functions_for_classification:24">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>f</mi>
     <mn>0</mn>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>≠</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <neq></neq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>f</ci>
      <cn type="integer">0</cn>
     </apply>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{0}(\vec{x})\neq 0
  </annotation>
 </semantics>
</math>

 and which is decreasing in a neighborhood of 0,</p>

<p>

<math display="block" id="Loss_functions_for_classification:25">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>f</mi>
     <mo>*</mo>
    </msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>sgn</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>f</mi>
       <mn>0</mn>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <mover accent="true">
        <mi>x</mi>
        <mo stretchy="false">→</mo>
       </mover>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>f</ci>
      <times></times>
     </apply>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <apply>
     <ci>sgn</ci>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>f</ci>
       <cn type="integer">0</cn>
      </apply>
      <apply>
       <ci>normal-→</ci>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f^{*}(\vec{x})=\operatorname{sgn}(f_{0}(\vec{x}))
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Loss_functions_for_classification:26">
 <semantics>
  <mo>sgn</mo>
  <annotation-xml encoding="MathML-Content">
   <ci>sgn</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{sgn}
  </annotation>
 </semantics>
</math>

 is the <a href="sign_function" title="wikilink">sign function</a> (for proof see <a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a>). Note also that 

<math display="inline" id="Loss_functions_for_classification:27">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>f</mi>
     <mn>0</mn>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>≠</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <neq></neq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>f</ci>
      <cn type="integer">0</cn>
     </apply>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{0}(\vec{x})\neq 0
  </annotation>
 </semantics>
</math>

 in practice when the loss function is differentiable at the origin. This fact confers a consistency property upon all convex loss functions; specifically, all convex loss functions will lead to consistent results with the 0–1 loss function given the presence of infinite data. Consequently, we can bound the difference of any of these convex loss function from expected risk.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<h2 id="simplifying-expected-risk-for-classification">Simplifying expected risk for classification</h2>

<p>Given the properties of binary classification properties, it is possible to simplify the calculation of expected risk from the integral specified above. Specifically,</p>

<p>

<math display="inline" id="Loss_functions_for_classification:28">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>f</mi>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>I</ci>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <ci>f</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle I[f]
  </annotation>
 </semantics>
</math>


</p>

<p>The second equality follows from the properties described above. The third equality follows since 

<math display="inline" id="Loss_functions_for_classification:29">
 <semantics>
  <mover accent="true">
   <mi>x</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{x}
  </annotation>
 </semantics>
</math>

 is simply data and since 

<math display="inline" id="Loss_functions_for_classification:30">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mo largeop="true" symmetric="true">∫</mo>
     <mi>X</mi>
    </msub>
    <mrow>
     <mi>p</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo rspace="4.2pt" stretchy="false">)</mo>
     </mrow>
     <mi>d</mi>
     <mi>x</mi>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <int></int>
      <ci>X</ci>
     </apply>
     <apply>
      <times></times>
      <ci>p</ci>
      <ci>x</ci>
      <ci>d</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \int_{X}p(x)\,dx=1
  </annotation>
 </semantics>
</math>

. Finally, the fourth equality follows from the fact that 1 and −1 are the only possible values for 

<math display="inline" id="Loss_functions_for_classification:31">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

, and the fifth because 

<math display="inline" id="Loss_functions_for_classification:32">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mo>-</mo>
    <mn>1</mn>
    <mo>∣</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
   <mo>-</mo>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>1</mn>
    <mo>∣</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <minus></minus>
     <cn type="integer">1</cn>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="integer">1</cn>
    <minus></minus>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="integer">1</cn>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(-1\mid x)=1-p(1\mid x)
  </annotation>
 </semantics>
</math>

. As a result, one can solve for the minimizers of 

<math display="inline" id="Loss_functions_for_classification:33">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>f</mi>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>I</ci>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <ci>f</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I[f]
  </annotation>
 </semantics>
</math>

 for any convex loss functions with these properties by differentiating the last equality with respect to 

<math display="inline" id="Loss_functions_for_classification:34">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 and setting the derivative equal to 0. Thus, minimizers for all of the loss function surrogates described below are easily obtained as functions of only 

<math display="inline" id="Loss_functions_for_classification:35">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(\vec{x})
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Loss_functions_for_classification:36">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>1</mn>
    <mo>∣</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="integer">1</cn>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(1\mid x)
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h2 id="square-loss">Square loss</h2>

<p>While more commonly used in regression, the square loss function can be re-written as a function 

<math display="inline" id="Loss_functions_for_classification:37">
 <semantics>
  <mrow>
   <mi>ϕ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>y</mi>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>ϕ</ci>
    <apply>
     <times></times>
     <ci>y</ci>
     <ci>f</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \phi(yf(\vec{x}))
  </annotation>
 </semantics>
</math>

 and utilized for classification. Defined as</p>

<p>

<math display="block" id="Loss_functions_for_classification:38">
 <semantics>
  <mrow>
   <mrow>
    <mi>V</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mover accent="true">
        <mi>x</mi>
        <mo stretchy="false">→</mo>
       </mover>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>,</mo>
     <mi>y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mn>1</mn>
      <mo>-</mo>
      <mrow>
       <mi>y</mi>
       <mi>f</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mover accent="true">
         <mi>x</mi>
         <mo stretchy="false">→</mo>
        </mover>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>V</ci>
     <interval closure="open">
      <apply>
       <times></times>
       <ci>f</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
      <ci>y</ci>
     </interval>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
      <apply>
       <times></times>
       <ci>y</ci>
       <ci>f</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(f(\vec{x}),y)=(1-yf(\vec{x}))^{2}
  </annotation>
 </semantics>
</math>

 the square loss function is both convex and smooth and matches the 0–1 <a href="indicator_function" title="wikilink">indicator function</a> when 

<math display="inline" id="Loss_functions_for_classification:39">
 <semantics>
  <mrow>
   <mrow>
    <mi>y</mi>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>y</ci>
     <ci>f</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   yf(\vec{x})=0
  </annotation>
 </semantics>
</math>

 and when 

<math display="inline" id="Loss_functions_for_classification:40">
 <semantics>
  <mrow>
   <mrow>
    <mi>y</mi>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>y</ci>
     <ci>f</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   yf(\vec{x})=1
  </annotation>
 </semantics>
</math>

. However, the square loss function tends to penalize outliers excessively, leading to slower convergence rates (with regards to sample complexity) than for the logistic loss or hinge loss functions.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> In addition, functions which yield high values of 

<math display="inline" id="Loss_functions_for_classification:41">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(\vec{x})
  </annotation>
 </semantics>
</math>

 for some 

<math display="inline" id="Loss_functions_for_classification:42">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>∈</mo>
   <mi>X</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>x</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x\in X
  </annotation>
 </semantics>
</math>

 will perform poorly with the square loss function, since high values of 

<math display="inline" id="Loss_functions_for_classification:43">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>y</ci>
    <ci>f</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   yf(\vec{x})
  </annotation>
 </semantics>
</math>

 will be penalized severely, regardless of whether the signs of 

<math display="inline" id="Loss_functions_for_classification:44">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Loss_functions_for_classification:45">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(\vec{x})
  </annotation>
 </semantics>
</math>

 match.</p>

<p>A benefit of the square loss function is that its structure lends itself to easy cross validation of regularization parameters. Specifically for <a href="Tikhonov_regularization" title="wikilink">Tikhonov regularization</a>, one can solve for the regularization parameter using leave-one-out <a href="cross-validation_(statistics)" title="wikilink">cross-validation</a> in the same time as it would take to solve a single problem.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></p>

<p>The minimizer of 

<math display="inline" id="Loss_functions_for_classification:46">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>f</mi>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>I</ci>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <ci>f</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I[f]
  </annotation>
 </semantics>
</math>

 for the square loss function is</p>

<p>

<math display="block" id="Loss_functions_for_classification:47">
 <semantics>
  <mrow>
   <msubsup>
    <mi>f</mi>
    <mtext>Square</mtext>
    <mo>*</mo>
   </msubsup>
   <mo>=</mo>
   <mn>2</mn>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>1</mn>
    <mo>∣</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>-</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>f</ci>
      <times></times>
     </apply>
     <mtext>Square</mtext>
    </apply>
    <eq></eq>
    <cn type="integer">2</cn>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="integer">1</cn>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <minus></minus>
    <cn type="integer">1</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f^{*}_{\text{Square}}=2p(1\mid x)-1
  </annotation>
 </semantics>
</math>

 This function notably equals 

<math display="inline" id="Loss_functions_for_classification:48">
 <semantics>
  <msup>
   <mi>f</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>f</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f^{*}
  </annotation>
 </semantics>
</math>

 for the 0–1 loss function when 

<math display="inline" id="Loss_functions_for_classification:49">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>1</mn>
    <mo>∣</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="integer">1</cn>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="integer">1</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(1\mid x)=1
  </annotation>
 </semantics>
</math>

 or 

<math display="inline" id="Loss_functions_for_classification:50">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>1</mn>
    <mo>∣</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="integer">1</cn>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="integer">0</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(1\mid x)=0
  </annotation>
 </semantics>
</math>

, but predicts a value between the two classifications when the classification of 

<math display="inline" id="Loss_functions_for_classification:51">
 <semantics>
  <mover accent="true">
   <mi>x</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{x}
  </annotation>
 </semantics>
</math>

 is not known with absolute certainty.</p>
<h2 id="hinge-loss">Hinge loss</h2>

<p>The hinge loss function is defined as</p>

<p>

<math display="block" id="Loss_functions_for_classification:52">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>V</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>f</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mover accent="true">
         <mi>x</mi>
         <mo stretchy="false">→</mo>
        </mover>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>,</mo>
      <mi>y</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mi>max</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mn>0</mn>
      <mo>,</mo>
      <mrow>
       <mn>1</mn>
       <mo>-</mo>
       <mrow>
        <mi>y</mi>
        <mi>f</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mover accent="true">
          <mi>x</mi>
          <mo stretchy="false">→</mo>
         </mover>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <msub>
     <mrow>
      <mo stretchy="false">|</mo>
      <mrow>
       <mn>1</mn>
       <mo>-</mo>
       <mrow>
        <mi>y</mi>
        <mi>f</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mover accent="true">
          <mi>x</mi>
          <mo stretchy="false">→</mo>
         </mover>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo stretchy="false">|</mo>
     </mrow>
     <mo>+</mo>
    </msub>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <ci>V</ci>
      <interval closure="open">
       <apply>
        <times></times>
        <ci>f</ci>
        <apply>
         <ci>normal-→</ci>
         <ci>x</ci>
        </apply>
       </apply>
       <ci>y</ci>
      </interval>
     </apply>
     <apply>
      <max></max>
      <cn type="integer">0</cn>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <apply>
        <times></times>
        <ci>y</ci>
        <ci>f</ci>
        <apply>
         <ci>normal-→</ci>
         <ci>x</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <abs></abs>
       <apply>
        <minus></minus>
        <cn type="integer">1</cn>
        <apply>
         <times></times>
         <ci>y</ci>
         <ci>f</ci>
         <apply>
          <ci>normal-→</ci>
          <ci>x</ci>
         </apply>
        </apply>
       </apply>
      </apply>
      <plus></plus>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(f(\vec{x}),y)=\max(0,1-yf(\vec{x}))=|1-yf(\vec{x})|_{+}.
  </annotation>
 </semantics>
</math>

</p>

<p>The hinge loss provides a relatively tight, convex upper bound on the 0–1 <a href="indicator_function" title="wikilink">indicator function</a>. Specifically, the hinge loss equals the 0–1 <a href="indicator_function" title="wikilink">indicator function</a> when 

<math display="inline" id="Loss_functions_for_classification:53">
 <semantics>
  <mrow>
   <mrow>
    <mo>sgn</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mover accent="true">
        <mi>x</mi>
        <mo stretchy="false">→</mo>
       </mover>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mi>y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>sgn</ci>
     <apply>
      <times></times>
      <ci>f</ci>
      <apply>
       <ci>normal-→</ci>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{sgn}(f(\vec{x}))=y
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Loss_functions_for_classification:54">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">|</mo>
    <mrow>
     <mi>y</mi>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo stretchy="false">|</mo>
   </mrow>
   <mo>≥</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <geq></geq>
    <apply>
     <abs></abs>
     <apply>
      <times></times>
      <ci>y</ci>
      <ci>f</ci>
      <apply>
       <ci>normal-→</ci>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   |yf(\vec{x})|\geq 1
  </annotation>
 </semantics>
</math>

. In addition, the empirical risk minimization of this loss is equivalent to the classical formulation for <a href="support_vector_machines" title="wikilink">support vector machines</a> (SVMs). Correctly classified points lying outside the margin boundaries of the support vectors are not penalized, whereas points within the margin boundaries or on the wrong side of the hyperplane are penalized in a linear fashion compared to their distance from the correct boundary.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></p>

<p>While the hinge loss function is both convex and continuous, it is not smooth (that is not differentiable) at 

<math display="inline" id="Loss_functions_for_classification:55">
 <semantics>
  <mrow>
   <mrow>
    <mi>y</mi>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>y</ci>
     <ci>f</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   yf(\vec{x})=1
  </annotation>
 </semantics>
</math>

. Consequently, the hinge loss function cannot be used with <a href="gradient_descent" title="wikilink">gradient descent</a> methods or <a href="stochastic_gradient_descent" title="wikilink">stochastic gradient descent</a> methods which rely on differentiability over the entire domain. However, the hinge loss does have a subgradient at 

<math display="inline" id="Loss_functions_for_classification:56">
 <semantics>
  <mrow>
   <mrow>
    <mi>y</mi>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>y</ci>
     <ci>f</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   yf(\vec{x})=1
  </annotation>
 </semantics>
</math>

, which allows for the utilization of <a href="subgradient_method" title="wikilink"> subgradient descent methods</a>.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> SVMs utilizing the hinge loss function can also be solved using <a href="quadratic_programming" title="wikilink">quadratic programming</a>.</p>

<p>The minimizer of 

<math display="inline" id="Loss_functions_for_classification:57">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>f</mi>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>I</ci>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <ci>f</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I[f]
  </annotation>
 </semantics>
</math>

 for the hinge loss function is</p>

<p>

<math display="block" id="Loss_functions_for_classification:58">
 <semantics>
  <mrow>
   <mrow>
    <msubsup>
     <mi>f</mi>
     <mtext>Hinge</mtext>
     <mo>*</mo>
    </msubsup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mo rspace="5.3pt" stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo rspace="5.3pt">=</mo>
   <mrow>
    <mo>{</mo>
    <mtable displaystyle="true">
     <mtr>
      <mtd columnalign="left">
       <mn>1</mn>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mtext>if</mtext>
        <mi>p</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mn>1</mn>
         <mo>∣</mo>
         <mover accent="true">
          <mi>x</mi>
          <mo stretchy="false">→</mo>
         </mover>
         <mo stretchy="false">)</mo>
        </mrow>
        <mo>></mo>
        <mi>p</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mo>-</mo>
         <mn>1</mn>
         <mo>∣</mo>
         <mover accent="true">
          <mi>x</mi>
          <mo stretchy="false">→</mo>
         </mover>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mtext>if</mtext>
        <mi>p</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mn>1</mn>
         <mo>∣</mo>
         <mover accent="true">
          <mi>x</mi>
          <mo stretchy="false">→</mo>
         </mover>
         <mo stretchy="false">)</mo>
        </mrow>
        <mo><</mo>
        <mi>p</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mo>-</mo>
         <mn>1</mn>
         <mo>∣</mo>
         <mover accent="true">
          <mi>x</mi>
          <mo stretchy="false">→</mo>
         </mover>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mtd>
     </mtr>
    </mtable>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>f</ci>
       <times></times>
      </apply>
      <mtext>Hinge</mtext>
     </apply>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="latexml">cases</csymbol>
     <cn type="integer">1</cn>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <mtext>if</mtext>
      <csymbol cd="unknown">p</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <cn type="integer">1</cn>
       <ci>normal-∣</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
      <gt></gt>
      <csymbol cd="unknown">p</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <minus></minus>
       <cn type="integer">1</cn>
       <ci>normal-∣</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
     </apply>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <mtext>if</mtext>
      <csymbol cd="unknown">p</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <cn type="integer">1</cn>
       <ci>normal-∣</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
      <lt></lt>
      <csymbol cd="unknown">p</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <minus></minus>
       <cn type="integer">1</cn>
       <ci>normal-∣</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f^{*}_{\text{Hinge}}(\vec{x})\;=\;\begin{cases}1&\text{if }p(1\mid\vec{x})>p(-%
1\mid\vec{x})\\
-1&\text{if }p(1\mid\vec{x})<p(-1\mid\vec{x})\end{cases}
  </annotation>
 </semantics>
</math>

</p>

<p>when 

<math display="inline" id="Loss_functions_for_classification:59">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>1</mn>
    <mo>∣</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>≠</mo>
   <mn>0.5</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="integer">1</cn>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <neq></neq>
    <cn type="float">0.5</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(1\mid x)\neq 0.5
  </annotation>
 </semantics>
</math>

, which matches that of the 0–1 indicator function. This conclusion makes the hinge loss quite attractive, as bounds can be placed on the difference between expected risk and the sign of hinge loss function.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>
<h2 id="logistic-loss">Logistic loss</h2>

<p>The logistic loss function is defined as</p>

<p>

<math display="block" id="Loss_functions_for_classification:60">
 <semantics>
  <mrow>
   <mrow>
    <mi>V</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>f</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mover accent="true">
        <mi>x</mi>
        <mo stretchy="false">→</mo>
       </mover>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>,</mo>
     <mi>y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mrow>
      <mi>ln</mi>
      <mn>2</mn>
     </mrow>
    </mfrac>
    <mrow>
     <mi>ln</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mn>1</mn>
       <mo>+</mo>
       <msup>
        <mi>e</mi>
        <mrow>
         <mo>-</mo>
         <mrow>
          <mi>y</mi>
          <mi>f</mi>
          <mrow>
           <mo stretchy="false">(</mo>
           <mover accent="true">
            <mi>x</mi>
            <mo stretchy="false">→</mo>
           </mover>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
        </mrow>
       </msup>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>V</ci>
     <interval closure="open">
      <apply>
       <times></times>
       <ci>f</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
      <ci>y</ci>
     </interval>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <apply>
       <ln></ln>
       <cn type="integer">2</cn>
      </apply>
     </apply>
     <apply>
      <ln></ln>
      <apply>
       <plus></plus>
       <cn type="integer">1</cn>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>e</ci>
        <apply>
         <minus></minus>
         <apply>
          <times></times>
          <ci>y</ci>
          <ci>f</ci>
          <apply>
           <ci>normal-→</ci>
           <ci>x</ci>
          </apply>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(f(\vec{x}),y)=\frac{1}{\ln 2}\ln(1+e^{-yf(\vec{x})})
  </annotation>
 </semantics>
</math>

</p>

<p>This function displays a similar convergence rate to the hinge loss function, and since it is continuous, <a href="gradient_descent" title="wikilink">gradient descent</a> methods can be utilized. However, the logistic loss function does not assign zero penalty to any points. Instead, functions which correctly classify points with high confidence, that is high values of 

<math display="inline" id="Loss_functions_for_classification:61">
 <semantics>
  <mrow>
   <mo stretchy="false">|</mo>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mover accent="true">
      <mi>x</mi>
      <mo stretchy="false">→</mo>
     </mover>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo stretchy="false">|</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <abs></abs>
    <apply>
     <times></times>
     <ci>f</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   |f(\vec{x})|
  </annotation>
 </semantics>
</math>

, are penalized less. This structure leads the logistic loss function to be very sensitive to outliers in the data. In short, the logistic loss function holds some balance between the computational attractiveness of the square loss function and the direct applicability of the hinge loss function.</p>

<p>The minimizer of 

<math display="inline" id="Loss_functions_for_classification:62">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>f</mi>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>I</ci>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <ci>f</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I[f]
  </annotation>
 </semantics>
</math>

 for the logistic loss function is</p>

<p>

<math display="block" id="Loss_functions_for_classification:63">
 <semantics>
  <mrow>
   <mrow>
    <msubsup>
     <mi>f</mi>
     <mtext>Logistic</mtext>
     <mo>*</mo>
    </msubsup>
    <mo>=</mo>
    <mrow>
     <mi>ln</mi>
     <mrow>
      <mo>(</mo>
      <mfrac>
       <mrow>
        <mi>p</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mn>1</mn>
         <mo>∣</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mrow>
        <mn>1</mn>
        <mo>-</mo>
        <mi>p</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mn>1</mn>
         <mo>∣</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mfrac>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>f</ci>
      <times></times>
     </apply>
     <mtext>Logistic</mtext>
    </apply>
    <apply>
     <ln></ln>
     <apply>
      <divide></divide>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <csymbol cd="unknown">p</csymbol>
       <cerror>
        <csymbol cd="ambiguous">fragments</csymbol>
        <ci>normal-(</ci>
        <cn type="integer">1</cn>
        <ci>normal-∣</ci>
        <csymbol cd="unknown">x</csymbol>
        <ci>normal-)</ci>
       </cerror>
      </cerror>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <cn type="integer">1</cn>
       <minus></minus>
       <csymbol cd="unknown">p</csymbol>
       <cerror>
        <csymbol cd="ambiguous">fragments</csymbol>
        <ci>normal-(</ci>
        <cn type="integer">1</cn>
        <ci>normal-∣</ci>
        <csymbol cd="unknown">x</csymbol>
        <ci>normal-)</ci>
       </cerror>
      </cerror>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f^{*}_{\text{Logistic}}=\ln\left(\frac{p(1\mid x)}{1-p(1\mid x)}\right).
  </annotation>
 </semantics>
</math>

</p>

<p>This function is undefined when 

<math display="inline" id="Loss_functions_for_classification:64">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>1</mn>
    <mo>∣</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="integer">1</cn>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="integer">1</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(1\mid x)=1
  </annotation>
 </semantics>
</math>

 or 

<math display="inline" id="Loss_functions_for_classification:65">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>1</mn>
    <mo>∣</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="integer">1</cn>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="integer">0</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(1\mid x)=0
  </annotation>
 </semantics>
</math>

 (tending toward ∞ and −∞ respectively), but predicts a smooth curve which grows when 

<math display="inline" id="Loss_functions_for_classification:66">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>1</mn>
    <mo>∣</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="integer">1</cn>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(1\mid x)
  </annotation>
 </semantics>
</math>

 increases and equals 0 when 

<math display="inline" id="Loss_functions_for_classification:67">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>1</mn>
    <mo>∣</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>0.5</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="integer">1</cn>
     <ci>normal-∣</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="float">0.5</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(1\mid x)=0.5
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Statistical_learning_theory" title="wikilink">Statistical learning theory</a></li>
<li><a href="Loss_function" title="wikilink">Loss function</a></li>
<li><a href="Support_vector_machine" title="wikilink">Support vector machine</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Machine_learning_algorithms" title="wikilink">Category:Machine learning algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"></li>
<li id="fn9"></li>
<li id="fn10"></li>
<li id="fn11"></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"></li>
<li id="fn14"></li>
<li id="fn15"></li>
<li id="fn16"></li>
</ol>
</section>
</body>
</html>
