   Draft:Uniform Stability and Generalization in learning theory      Draft:Uniform Stability and Generalization in learning theory    -- Brandojazz ( talk ) 01:57, 8 December 2014 (UTC)  Uniform Stability and Generalization in learning theory  The notion of stability is one which captures how much a predictive function changes when the training set is changed slightly. When an algorithm is uniformly stable, intuitively, it means that the algorithm is resistant, in the "worst case", to a change in the training set. In other words, for all training sets and any changes to this training set, the predictive accuracy should not change in a noticeable way. Hence, since the algorithm didn't change that much, then its error shouldn't change (too much) either.  In this article we will show that if a learning algorithm   L   L   L   is uniformly stable in a precise sense, then it will generalize. i.e. the error in the training set will be a true reflection of the quality of our predictions on unseen data points. In other words, if the algorithm is resistant to small perturbations in the training set as defined by uniform stability, then we can mathematically show that the learning algorithm will be able to make good predictions on sample points that are inside our training set as well as samples that are not.  Motivation  Why would someone want their learning algorithm to be stable in any sense? One can intuitively motivate the concept of stability with the analogy of a scientific theory, where the power of our predictor lies in the predictive power of the scientific theory. If we had a good scientific theory and thus a good predictor, then it should have been able to see general trends in the data that inspired this theory. In other words, small changes to the data that inspired this theory, should not affect the theory as a whole to much, because the theory was able to extract the general trends effectively in a predictive manner. If it was a good theory in the first place, then small changes in the training data should not affect its success to predict unseen data samples. Similarly, if we have a learning algorithm that is able to generalize on unseen data points, then if the algorithm was "good" at learning from the data it had, then small perturbations on that data should not affect its predictive power too much. This intuitive idea can actually be made mathematically rigorous with uniform stability. If a learning algorithm is (uniformly) stable then, from the previous argument, it makes sense that it should have good predictive power on data it has already seen and data it has not yet seen. i.e. if the learning algorithm is stable, it should be able to generalize.  Preliminary definitions  This article can be heavy in notation and its important to clarify that. The notation has already been established in the following wikipedia article. Please refer to that for details on the notation.  Formal Definition of Uniform Stability  An algorithm   L   L   L   has uniform stability β with respect to the loss function V if the following holds:        ∀  S   ∈   Z  m    ,     ∀  i   ∈   {  1  ,  …  ,  m  }    ,     sup   z  ∈  Z     |    V   (   f  S   ,  z  )    -   V   (   f   S  i    ,  z  )     |    ≤  β       formulae-sequence     for-all  S    superscript  Z  m     formulae-sequence     for-all  i    1  normal-…  m        subscript  supremum    z  Z          V    subscript  f  S   z      V    subscript  f   superscript  S  i    z       β      \forall S\in Z^{m},\forall i\in\{1,...,m\},\sup_{z\in Z}|V(f_{S},z)-V(f_{S^{i}%
 },z)|\leq\beta     A probabilistic version of uniform stability β is:      ∀  S  ∈   Z  m   ,  ∀  i  ∈   {  1  ,  …  ,  m  }   ,   ℙ  S    {   sup   z  ∈  Z    |  V   (   f  S   ,  z  )   -  V   (   f   S  i    ,  z  )   |  ≤  β  }   ≥  1  -  δ     fragments  for-all  S    superscript  Z  m   normal-,  for-all  i    fragments  normal-{  1  normal-,  normal-…  normal-,  m  normal-}   normal-,   subscript  ℙ  S    fragments  normal-{   subscript  supremum    z  Z    normal-|  V   fragments  normal-(   subscript  f  S   normal-,  z  normal-)    V   fragments  normal-(   subscript  f   superscript  S  i    normal-,  z  normal-)   normal-|   β  normal-}    1   δ    \forall S\in Z^{m},\forall i\in\{1,...,m\},\mathbb{P}_{S}\{\sup_{z\in Z}|V(f_{%
 S},z)-V(f_{S^{i}},z)|\leq\beta\}\geq 1-\delta     Final Results of Uniform Stability and Generalization  If a learning algorithm   L   L   L   is uniformly stable and also has a bounded loss function, then with confidence    1  -  δ      1  δ    1-\delta   the difference between the empirical risk and the generalization error will be upper bounded as follows:        I   [   f  S   ]    -    I  S    [   f  S   ]     ≤    β  n   +    (    2  n   β  n    +  M   )      l  n   (   2  δ   )     2  n               I   delimited-[]   subscript  f  S        subscript  I  S    delimited-[]   subscript  f  S         subscript  β  n         2  n   subscript  β  n    M         l  n    2  δ      2  n         I[f_{S}]-I_{S}[f_{S}]\leq\beta_{n}+(2n\beta_{n}+M)\sqrt{\frac{ln(\frac{2}{%
 \delta})}{2n}}     Specifically, if one has a uniformly stable learning algorithm with     β  n   =   O   (   1  n   )         subscript  β  n     O    1  n      \beta_{n}=O\left(\frac{1}{n}\right)   , then the upper bound becomes:        I   [   f  S   ]    -    I  S    [   f  S   ]     ≤   O   (   1   n    )            I   delimited-[]   subscript  f  S        subscript  I  S    delimited-[]   subscript  f  S        O    1    n       I[f_{S}]-I_{S}[f_{S}]\leq O\left(\frac{1}{\sqrt{n}}\right)     which clearly approaches zero as n approaches infinity. Therefore, the empirical risk is equal to the generalization error for sufficiently large and thus, the empirical risk is a good proxy for the generalization error. In this case, we say that the learning algorithm generalizes.  Remarks  Why minimizing the empirical risk is a good idea for large training sets  Notice that this result is basically saying that for sufficiently large n, the empirical risk and the generalization error are approximately equal. Therefore, this means that given enough training data, finding a predictor that minimizes the empirical risk will actually also minimize the generalization error. Therefore, for a stable learning algorithm, minimizing the empirical risk is actually a good procedure for minimizing generalization (since the two are approximately equal for sufficiently large n).  Unfortunately, this is obviously an asymptotic bound and therefore, large values of n are needed. However, this result does justify why minimizing the empirical risk for large training sets might be a good idea.  Justification for bounded loss function  One immediate argument that one could hold against such a theoretical result is an argument against the bounded loss function. In reality, we never choose a bounded loss function. For example, the squared loss, is not bounded for all values on the real line. However, the important argument made to justify such an argument is that in reality, we will never actually expect to observe every value of the real line. Within a realistic domain of values that any reasonable training set might have, we will observe finite bounded numbers   z   z   z   . Therefore, since we don't expect to get any value of the real line, we approximately have a bounded loss function.  Example of a Uniformly Stable  Tikhonov Regularization  Recall tikhonov regularization to be:       f  S  λ   =   a  r  g    min   f  ∈  ℋ     (     1  n     ∑   i  =  1   n    V   (   f   (   x  i   )    ,   y  i   )      +   λ    ||  f  ||    ℝ  k   2     )          subscript   superscript  f  λ   S     a  r  g    subscript     f  ℋ          1  n     subscript   superscript   n     i  1      V     f   subscript  x  i     subscript  y  i         λ   subscript   superscript   norm  f   2    superscript  ℝ  k          f^{\lambda}_{S}=arg\min_{f\in\mathcal{H}}\left(\frac{1}{n}\sum^{n}_{i=1}V(f(x_%
 {i}),y_{i})+\lambda||f||^{2}_{\mathbb{R}^{k}}\right)     it can be shown that tikhonov regularization is uniformly stable. If that is true then tikhonov regularization is proved to generalize.  To prove that Tikhonov regularization is stable we only need to show these three statements to be true:  1) we assume that the loss is Lipschitz continuous     |    V   (    f  i    (  x  )    ,   y  ′   )    -   V   (    f  2    (  x  )    ,   y  ′   )     |   ≤   L    ||    f  1   -   f  2    ||   ∞              V      subscript  f  i   x    superscript  y  normal-′       V      subscript  f  2   x    superscript  y  normal-′         L   subscript   norm     subscript  f  1    subscript  f  2          |V(f_{i}(x),y^{\prime})-V(f_{2}(x),y^{\prime})|\leq L||f_{1}-f_{2}||_{\infty}     2) we need that the hypothesis class to be over reproducing kernel Hilbert spaces (RKHS)      ||   f  -   f  ′    ||   ∞   ≤   κ    ||   f  -   f  ′    ||    ℝ  K          subscript   norm    f   superscript  f  normal-′         κ   subscript   norm    f   superscript  f  normal-′      superscript  ℝ  K       ||f-f^{\prime}||_{\infty}\leq\kappa||f-f^{\prime}||_{\mathbb{R}^{K}}   for any     f  ,   f  ′    ∈  ℋ       f   superscript  f  normal-′    ℋ    f,f^{\prime}\in\mathcal{H}     3) finally we need the following lemma to hold      ||    f  S  λ   -   f   S   i  ,  z    λ    ||    ℝ  K   2   ≤    L    ||    f  S  λ   -   f   S   i  ,  z    λ    ||   ∞     λ  n         subscript   superscript   norm     subscript   superscript  f  λ   S    subscript   superscript  f  λ    superscript  S   i  z       2    superscript  ℝ  K        L   subscript   norm     subscript   superscript  f  λ   S    subscript   superscript  f  λ    superscript  S   i  z            λ  n      ||f^{\lambda}_{S}-f^{\lambda}_{S^{i,z}}||^{2}_{\mathbb{R}^{K}}\leq\frac{L||f^{%
 \lambda}_{S}-f^{\lambda}_{S^{i,z}}||_{\infty}}{\lambda n}     If the above holds and the loss function is upper bounded by M, then the generalization bound has the following form:       |    I   [   f  S  λ   ]    -    I  S    [   f  S  λ   ]     |   ≤      L  2    κ  2     λ  n    +    (     2   L  2    κ  2     λ  n    +  M   )      2  l  n   (   2  δ   )    n                I   delimited-[]   subscript   superscript  f  λ   S        subscript  I  S    delimited-[]   subscript   superscript  f  λ   S              superscript  L  2    superscript  κ  2      λ  n            2   superscript  L  2    superscript  κ  2      λ  n    M         2  l  n    2  δ    n        |I[f^{\lambda}_{S}]-I_{S}[f^{\lambda}_{S}]|\leq\frac{L^{2}\kappa^{2}}{\lambda n%
 }+(\frac{2L^{2}\kappa^{2}}{\lambda n}+M)\sqrt{\frac{2ln(\frac{2}{\delta})}{n}}     Therefore, with confidence    1  -  δ      1  δ    1-\delta   , tikhonov regularization generalizes as n goes to infinity.  Remarks on Bound  Notice that keeping   λ   λ   \lambda   fixed as n increases, the generalization tightens as    O   (   1   n    )       O    1    n      O\left(\frac{1}{\sqrt{n}}\right)   . However, fixing   λ   λ   \lambda   keeps our hypothesis spaced fixed. However, as we get more data, we want   λ   λ   \lambda   to get smaller. However, if   λ   λ   \lambda   gets smaller too quickly, then the bounds have the potential to become vacuous.  Statement  If a learning algorithm   L   L   L   is uniformly stable, then as the number of training points approaches infinity the empirical risk approaches the generalization error with high probability i.e.        lim   n  →  ∞      I  S    [   f  S   ]     =   I   [   f  S   ]          subscript    normal-→  n        subscript  I  S    delimited-[]   subscript  f  S        I   delimited-[]   subscript  f  S       \lim_{n\to\infty}I_{S}[f_{S}]=I[f_{S}]     with high probability.  Therefore, our goal will be to show that if uniform stability holds for   L   L   L   , then the difference between the empirical error and the generalization error will go to zero with high probability i.e.:      P  r   [  |   I  S    [   f  S   ]   -  I   [   f  S   ]   |  ≤  ϵ   (  n  )   ]   ≥  1  -  δ     fragments  P  r   fragments  normal-[  normal-|   subscript  I  S    fragments  normal-[   subscript  f  S   normal-]    I   fragments  normal-[   subscript  f  S   normal-]   normal-|   ϵ   fragments  normal-(  n  normal-)   normal-]    1   δ    Pr[|I_{S}[f_{S}]-I[f_{S}]|\leq\epsilon(n)]\geq 1-\delta     where    ϵ   (  n  )       ϵ  n    \epsilon(n)   will be an upper bound that approaches zero as n approaches infinity.  Proof  Let's begin the proof by using the fact that our learning algorithm is uniformly stable. If the learning algorithm   L   L   L   is uniformly stable, then using McDiarmid's inequality, and setting the generalization error to be the functional yields:      ∀   (  S  ,  z  )   ∈   Z   n  +  1    ,  ∀  i  ∈   {  1  ,  …  ,  n  }   ,   sup   z  ∈  Z    |  I   [   f  S   ]   -  I   [   f   S   i  ,  z     ]   |  ≤  β  ⟹  P   (  |  I   [   f  S   ]   -   𝔼  S    [  I   [   f  S   ]   ]   |  ≥  ϵ  )   ≤  2   e   (    -   2   ϵ  2      n   β  2     )       fragments  for-all   fragments  normal-(  S  normal-,  z  normal-)     superscript  Z    n  1    normal-,  for-all  i    fragments  normal-{  1  normal-,  normal-…  normal-,  n  normal-}   normal-,   subscript  supremum    z  Z    normal-|  I   fragments  normal-[   subscript  f  S   normal-]    I   fragments  normal-[   subscript  f   superscript  S   i  z     normal-]   normal-|   β   P   fragments  normal-(  normal-|  I   fragments  normal-[   subscript  f  S   normal-]     subscript  𝔼  S    fragments  normal-[  I   fragments  normal-[   subscript  f  S   normal-]   normal-]   normal-|   ϵ  normal-)    2   superscript  e        2   superscript  ϵ  2       n   superscript  β  2        \forall(S,z)\in Z^{n+1},\forall i\in\{1,...,n\},\sup_{z\in Z}|I[f_{S}]-I[f_{S^%
 {i,z}}]|\leq\beta\implies P(|I[f_{S}]-\mathbb{E}_{S}[I[f_{S}]]|\geq\epsilon)%
 \leq 2e^{\left(\frac{-2\epsilon^{2}}{n\beta^{2}}\right)}     Which says that the generalization error will be close to the expected generalization error over training sets with high probability. If we change the above probabilistic statement to its confidence form and demand to have    1  -  δ      1  δ    1-\delta   confidence that    I   [   f  S   ]       I   delimited-[]   subscript  f  S      I[f_{S}]   and     𝔼  S    [   I   [   f  S   ]    ]        subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S        \mathbb{E}_{S}[I[f_{S}]]   are close, then after the bound equal to   δ   δ   \delta   and some lines of algebra, its easy to verify that   ϵ   ϵ   \epsilon   must be at most:      ϵ  =   n  β     l  n   (   2  δ   )     2  n          ϵ    n  β        l  n    2  δ      2  n        \epsilon=n\beta\sqrt{\frac{ln(\frac{2}{\delta})}{2n}}     Therefore, we have with confidence    1  -  δ      1  δ    1-\delta   that:       |    I   [   f  S   ]    -    𝔼  S    [   I   [   f  S   ]    ]     |   ≤   n  β     l  n   (   2  δ   )     2  n      ⟹   I   [   f  S   ]    ≤     𝔼  S    [   I   [   f  S   ]    ]    +   n  β     l  n   (   2  δ   )     2  n                   I   delimited-[]   subscript  f  S        subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S           n  β        l  n    2  δ      2  n             I   delimited-[]   subscript  f  S               subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S         n  β        l  n    2  δ      2  n          |I[f_{S}]-\mathbb{E}_{S}[I[f_{S}]]|\leq n\beta\sqrt{\frac{ln(\frac{2}{\delta})%
 }{2n}}\implies I[f_{S}]\leq\mathbb{E}_{S}[I[f_{S}]]+n\beta\sqrt{\frac{ln(\frac%
 {2}{\delta})}{2n}}     The above statement is an upper bound on the generalization error, which can be turned into the desired bound (i.e. one bounding the difference of the empirical risk and generalization error) easily by subtracting the empirical risk     I  S    [   f  S   ]        subscript  I  S    delimited-[]   subscript  f  S      I_{S}[f_{S}]   from both sides of the inequality yielding:        I   [   f  S   ]    -    I  S    [   f  S   ]     ≤      𝔼  S    [   I   [   f  S   ]    ]    -    I  S    [   f  S   ]     +   n  β     l  n   (   2  δ   )     2  n               I   delimited-[]   subscript  f  S        subscript  I  S    delimited-[]   subscript  f  S             subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S          subscript  I  S    delimited-[]   subscript  f  S        n  β        l  n    2  δ      2  n         I[f_{S}]-I_{S}[f_{S}]\leq\mathbb{E}_{S}[I[f_{S}]]-I_{S}[f_{S}]+n\beta\sqrt{%
 \frac{ln(\frac{2}{\delta})}{2n}}     the LHS is exactly what we need to conclude the proof, however we need to finish upper bounding the RHS, specifically we need to upper bound      𝔼  S    [   I   [   f  S   ]    ]    -    I  S    [   f  S   ]           subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S          subscript  I  S    delimited-[]   subscript  f  S       \mathbb{E}_{S}[I[f_{S}]]-I_{S}[f_{S}]   to conclude the proof.  The upper bound for      𝔼  S    [   I   [   f  S   ]    ]    -    I  S    [   f  S   ]           subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S          subscript  I  S    delimited-[]   subscript  f  S       \mathbb{E}_{S}[I[f_{S}]]-I_{S}[f_{S}]   is exactly:         𝔼  S    [   I   [   f  S   ]    ]    -    I  S    [   f  S   ]     ≤   β  +    (    n  β   +  M   )      l  n   (   2  δ   )     2  n                subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S          subscript  I  S    delimited-[]   subscript  f  S        β        n  β   M         l  n    2  δ      2  n         \mathbb{E}_{S}[I[f_{S}]]-I_{S}[f_{S}]\leq\beta+(n\beta+M)\sqrt{\frac{ln(\frac{%
 2}{\delta})}{2n}}     where M is the upper bound on the loss function    V   (  f  ,  z  )       V   f  z     V(f,z)   .  If that is true then the proof concludes that:        I   [   f  S   ]    -    I  S    [   f  S   ]     ≤   β  +    (    2  n  β   +  M   )      l  n   (   2  δ   )     2  n               I   delimited-[]   subscript  f  S        subscript  I  S    delimited-[]   subscript  f  S        β        2  n  β   M         l  n    2  δ      2  n         I[f_{S}]-I_{S}[f_{S}]\leq\beta+(2n\beta+M)\sqrt{\frac{ln(\frac{2}{\delta})}{2n}}     Which gives the desired upper bound and as long as the upper bound decreases as n increases, then the generalization error and empirical risk can be made arbitrarily close.  We will finish the proof by proving what we need in the following lemma:  Lemma  For a bounded loss function    V   (  f  ,  z  )       V   f  z     V(f,z)   (with upper bound M) and a uniformly stable learning algorithm the following upper bound holds with confidence    1  -  δ      1  δ    1-\delta          𝔼  S    [   I   [   f  S   ]    ]    -    I  S    [   f  S   ]     ≤   β  +    (    n  β   +  M   )      l  n   (   2  δ   )     2  n                subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S          subscript  I  S    delimited-[]   subscript  f  S        β        n  β   M         l  n    2  δ      2  n         \mathbb{E}_{S}[I[f_{S}]]-I_{S}[f_{S}]\leq\beta+(n\beta+M)\sqrt{\frac{ln(\frac{%
 2}{\delta})}{2n}}     Proof  Notice that the above statement is the difference of     I  S    [   f  S   ]        subscript  I  S    delimited-[]   subscript  f  S      I_{S}[f_{S}]   and the expected value of a different quanity. This suggests that setting     I  S    [   f  S   ]        subscript  I  S    delimited-[]   subscript  f  S      I_{S}[f_{S}]   to be the functional in McDiarmids inequality might be good first step to yield the above result. Thus, we wull use McDiarmid's inequality and let the functional F in McDiarmid's be     I  S    [   f  S   ]        subscript  I  S    delimited-[]   subscript  f  S      I_{S}[f_{S}]   . If that is our choice of functional then we must show the following first        ∀  i   ,    sup   S  ,  z     |     I  S    [   f  S   ]    -    I   S   i  ,  z      [   f   S   i  ,  z     ]     |     ≤   c  i         for-all  i     subscript  supremum   S  z           subscript  I  S    delimited-[]   subscript  f  S        subscript  I   superscript  S   i  z      delimited-[]   subscript  f   superscript  S   i  z            subscript  c  i     \forall i,\sup_{S,z}|I_{S}[f_{S}]-I_{S^{i,z}}[f_{S^{i,z}}]|\leq c_{i}     if we want the probabilistic upper bound on     𝔼  S    [    I  S    [   f  S   ]    ]        subscript  𝔼  S    delimited-[]     subscript  I  S    delimited-[]   subscript  f  S        \mathbb{E}_{S}[I_{S}[f_{S}]]   to hold.  Let's search for the upper bound    c  i     subscript  c  i    c_{i}   by considering the LHS of the above inequality and expanding it:        ∀  i   ,    sup   S  ,  z     |     I  S    [   f  S   ]    -    I   S   i  ,  z      [   f  S   i  ,  z    ]     |     =   |     1  n     ∑   i  =  1   n    V   (   f  s   ,   z  i   )      -   (     1  n     ∑   j  ≠  i     V   (   f   S   i  ,  z     ,   z  j   )      +    1  n   V   (   f   S   i  ,  z     ,  z  )     )    |         for-all  i     subscript  supremum   S  z           subscript  I  S    delimited-[]   subscript  f  S        subscript  I   superscript  S   i  z      delimited-[]   superscript   subscript  f  S    i  z                  1  n     subscript   superscript   n     i  1      V    subscript  f  s    subscript  z  i             1  n     subscript     j  i      V    subscript  f   superscript  S   i  z      subscript  z  j           1  n   V    subscript  f   superscript  S   i  z     z         \forall i,\sup_{S,z}|I_{S}[f_{S}]-I_{S^{i,z}}[f_{S}^{i,z}]|=|\frac{1}{n}\sum^{%
 n}_{i=1}V(f_{s},z_{i})-(\frac{1}{n}\sum_{j\neq i}V(f_{S^{i,z}},z_{j})+\frac{1}%
 {n}V(f_{S^{i,z}},z))|     by triangle inequality and pairing up the samples points     z  i   ≠   z  j        subscript  z  i    subscript  z  j     z_{i}\neq z_{j}   in the summations we get:       ≤     1  n     ∑   j  ≠  i     |    V   (   f  S   ,   z  j   )    -   V   (   f   S   i  ,  z     ,   z  j   )     |     +    1  n    |    V   (   f  S   ,   z  i   )    -   V   (   f   S   i  ,  z     ,  z  )     |         absent        1  n     subscript     j  i          V    subscript  f  S    subscript  z  j       V    subscript  f   superscript  S   i  z      subscript  z  j             1  n         V    subscript  f  S    subscript  z  i       V    subscript  f   superscript  S   i  z     z          \leq\frac{1}{n}\sum_{j\neq i}|V(f_{S},z_{j})-V(f_{S^{i,z}},z_{j})|+\frac{1}{n}%
 |V(f_{S},z_{i})-V(f_{S^{i,z}},z)|     We divided the terms that way because the first term     V   (   f  S   ,   z  j   )    -   V   (   f   S   i  ,  z     ,   z  j   )          V    subscript  f  S    subscript  z  j       V    subscript  f   superscript  S   i  z      subscript  z  j       V(f_{S},z_{j})-V(f_{S^{i,z}},z_{j})   can be upper bounded by   β   β   \beta   because of the stability of our learning algorithm. Thus,        V   (   f  S   ,   z  j   )    -   V   (   f   S   i  ,  z     ,   z  j   )     ≤  β          V    subscript  f  S    subscript  z  j       V    subscript  f   superscript  S   i  z      subscript  z  j      β    V(f_{S},z_{j})-V(f_{S^{i,z}},z_{j})\leq\beta     and the second term can only be bounded by the the upper bound of the loss function (because we are evaluating the loss at two different points with different training points):        V   (   f  S   ,   z  i   )    -   V   (   f   S   i  ,  z     ,  z  )     ≤  M          V    subscript  f  S    subscript  z  i       V    subscript  f   superscript  S   i  z     z     M    V(f_{S},z_{i})-V(f_{S^{i,z}},z)\leq M     Combining both terms yields the desired upper bound (to use McDirmad's afterwards):       ≤      n  -  1   n   β   +   M  n    ≤   β  +   M  n          absent          n  1   n   β     M  n           β    M  n       \leq\frac{n-1}{n}\beta+\frac{M}{n}\leq\beta+\frac{M}{n}     With this last result we can apply McDiarmid's inequality and the constants    c  i     subscript  c  i    c_{i}   at the beginning of the lemma are     β  +   M  n    ,   ∀  i        β    M  n     for-all  i     \beta+\frac{M}{n},\forall i   . Thus we have:      ∀  i  ,   sup   S  ,  z    |   I  S    [   f  S   ]   -   I   S   i  ,  z      [   f  S   i  ,  z    ]   |  ≤  β  +   M  n   ⟹  P  r   [  |   I  S    [   f  S   ]   -   𝔼  S    [   I  S    [   f  S   ]   ]   |  ≥  ϵ  ]   ≤  2   e    -   2  n   ϵ  2       (    n  β   +  M   )   2        fragments  for-all  i  normal-,   subscript  supremum   S  z    normal-|   subscript  I  S    fragments  normal-[   subscript  f  S   normal-]     subscript  I   superscript  S   i  z      fragments  normal-[   superscript   subscript  f  S    i  z    normal-]   normal-|   β     M  n    P  r   fragments  normal-[  normal-|   subscript  I  S    fragments  normal-[   subscript  f  S   normal-]     subscript  𝔼  S    fragments  normal-[   subscript  I  S    fragments  normal-[   subscript  f  S   normal-]   normal-]   normal-|   ϵ  normal-]    2   superscript  e        2  n   superscript  ϵ  2      superscript      n  β   M   2       \forall i,\sup_{S,z}|I_{S}[f_{S}]-I_{S^{{i,z}}}[f_{S}^{i,z}]|\leq\beta+\frac{M%
 }{n}\implies Pr[|I_{S}[f_{S}]-\mathbb{E}_{S}[I_{S}[f_{S}]]|\geq\epsilon]\leq 2%
 e^{\frac{-2n\epsilon^{2}}{(n\beta+M)^{2}}}     Switching the above bound to its confidence form and requiring it to have confidence    1  -  δ      1  δ    1-\delta   yields   ϵ   ϵ   \epsilon   to be:      ϵ  =    (    n  β   +  M   )      l  n   (   2  δ   )     2  n          ϵ        n  β   M         l  n    2  δ      2  n        \epsilon=(n\beta+M)\sqrt{\frac{ln(\frac{2}{\delta})}{2n}}     Therefore, from the probabilistic bound implied by McDiarmid's inequality we have:       |     I  S    [   f  S   ]    -    𝔼  S    [    I  S    [   f  S   ]    ]     |   ≤    (    n  β   +  M   )      l  n   (   2  δ   )     2  n      ⟹    I  S    [   f  S   ]    ≤     𝔼  S    [    I  S    [   f  S   ]    ]    +    (    n  β   +  M   )      l  n   (   2  δ   )     2  n                    subscript  I  S    delimited-[]   subscript  f  S        subscript  𝔼  S    delimited-[]     subscript  I  S    delimited-[]   subscript  f  S               n  β   M         l  n    2  δ      2  n              subscript  I  S    delimited-[]   subscript  f  S               subscript  𝔼  S    delimited-[]     subscript  I  S    delimited-[]   subscript  f  S             n  β   M         l  n    2  δ      2  n          |I_{S}[f_{S}]-\mathbb{E}_{S}[I_{S}[f_{S}]]|\leq(n\beta+M)\sqrt{\frac{ln(\frac{%
 2}{\delta})}{2n}}\implies I_{S}[f_{S}]\leq\mathbb{E}_{S}[I_{S}[f_{S}]]+(n\beta%
 +M)\sqrt{\frac{ln(\frac{2}{\delta})}{2n}}     Which isn't exactly what we wanted to prove by the lemma. However we can subtract     𝔼  S    [   I   [   f  S   ]    ]        subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S        \mathbb{E}_{S}[I[f_{S}]]   from both sides of the inequality to get it in the form of the original lemma statement. Doing that yields:         I  S    [   f  S   ]    -    𝔼  S    [   I   [   f  S   ]    ]     ≤      𝔼  S    [    I  S    [   f  S   ]    ]    -    𝔼  S    [   I   [   f  S   ]    ]     +    (    n  β   +  M   )      l  n   (   2  δ   )     2  n                subscript  I  S    delimited-[]   subscript  f  S        subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S               subscript  𝔼  S    delimited-[]     subscript  I  S    delimited-[]   subscript  f  S          subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S              n  β   M         l  n    2  δ      2  n         I_{S}[f_{S}]-\mathbb{E}_{S}[I[f_{S}]]\leq\mathbb{E}_{S}[I_{S}[f_{S}]]-\mathbb{%
 E}_{S}[I[f_{S}]]+(n\beta+M)\sqrt{\frac{ln(\frac{2}{\delta})}{2n}}     Which will conclude the proof of the lemma if we can upper bound      𝔼  S    [    I  S    [   f  S   ]    ]    -    𝔼  S    [   I   [   f  S   ]    ]           subscript  𝔼  S    delimited-[]     subscript  I  S    delimited-[]   subscript  f  S          subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S         \mathbb{E}_{S}[I_{S}[f_{S}]]-\mathbb{E}_{S}[I[f_{S}]]   .  Which is actually simple to show its upper bounded is   β   β   \beta   . First notice that:         𝔼  S    [    I  S    [   f  S   ]    ]    -    𝔼  S    [   I   [   f  S   ]    ]     =     𝔼  z    [    𝔼  S    [    I  S    [   f  S   ]    ]    ]    -    𝔼  S    [   I   [   f  S   ]    ]     =     𝔼  z    [    𝔼  S    [    1  n     ∑   i  =  1   n    V   (  f  ,   z  i   )      ]    ]    -    𝔼  S    [    𝔼  z    [   V   (   f  S   ,  z  )    ]    ]                subscript  𝔼  S    delimited-[]     subscript  I  S    delimited-[]   subscript  f  S          subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S             subscript  𝔼  z    delimited-[]     subscript  𝔼  S    delimited-[]     subscript  I  S    delimited-[]   subscript  f  S            subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S                  subscript  𝔼  z    delimited-[]     subscript  𝔼  S    delimited-[]      1  n     subscript   superscript   n     i  1      V   f   subscript  z  i              subscript  𝔼  S    delimited-[]     subscript  𝔼  z    delimited-[]    V    subscript  f  S   z            \mathbb{E}_{S}[I_{S}[f_{S}]]-\mathbb{E}_{S}[I[f_{S}]]=\mathbb{E}_{z}[\mathbb{E%
 }_{S}[I_{S}[f_{S}]]]-\mathbb{E}_{S}[I[f_{S}]]=\mathbb{E}_{z}[\mathbb{E}_{S}[%
 \frac{1}{n}\sum^{n}_{i=1}V(f,z_{i})]]-\mathbb{E}_{S}[\mathbb{E}_{z}[V(f_{S},z)]]     Then by linearity of expectation and some simple algebra we have:        𝔼  z    [    𝔼  S    [     1  n     ∑   i  =  1   n    V   (  f  ,   z  i   )      -   V   (   f  S   ,  z  )     ]    ]    =    𝔼  z    [    𝔼  S    [    1  n     ∑   i  =  1   n    (    V   (   f   S   i  ,  z     ,  z  )    -   V   (   f  S   ,  z  )     )     ]    ]           subscript  𝔼  z    delimited-[]     subscript  𝔼  S    delimited-[]        1  n     subscript   superscript   n     i  1      V   f   subscript  z  i         V    subscript  f  S   z            subscript  𝔼  z    delimited-[]     subscript  𝔼  S    delimited-[]      1  n     subscript   superscript   n     i  1        V    subscript  f   superscript  S   i  z     z      V    subscript  f  S   z             \mathbb{E}_{z}[\mathbb{E}_{S}[\frac{1}{n}\sum^{n}_{i=1}V(f,z_{i})-V(f_{S},z)]]%
 =\mathbb{E}_{z}[\mathbb{E}_{S}[\frac{1}{n}\sum^{n}_{i=1}\left(V(f_{S^{i,z}},z)%
 -V(f_{S},z))\right]]     Then by stability we know that each term      V   (   f   S   i  ,  z     ,  z  )    -   V   (   f  S   ,  z  )     ≤  β          V    subscript  f   superscript  S   i  z     z      V    subscript  f  S   z     β    V(f_{S^{i,z}},z)-V(f_{S},z)\leq\beta   . Thus:         𝔼  S    [    I  S    [   f  S   ]    ]    -    𝔼  S    [   I   [   f  S   ]    ]     ≤  β           subscript  𝔼  S    delimited-[]     subscript  I  S    delimited-[]   subscript  f  S          subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S        β    \mathbb{E}_{S}[I_{S}[f_{S}]]-\mathbb{E}_{S}[I[f_{S}]]\leq\beta     Which concludes the proof of the lemma and shows that with confidence    1  -  δ      1  δ    1-\delta   we have:         𝔼  S    [   I   [   f  S   ]    ]    -    I  S    [   f  S   ]     ≤   β  +    (    n  β   +  M   )      l  n   (   2  δ   )     2  n                subscript  𝔼  S    delimited-[]    I   delimited-[]   subscript  f  S          subscript  I  S    delimited-[]   subscript  f  S        β        n  β   M         l  n    2  δ      2  n         \mathbb{E}_{S}[I[f_{S}]]-I_{S}[f_{S}]\leq\beta+(n\beta+M)\sqrt{\frac{ln(\frac{%
 2}{\delta})}{2n}}     References  1  2  3  4 "     Lorenzo Rosasco, Tomaso Poggio: "Class slides on Stability": http://www.mit.edu/~9.520/fall14/slides/class15/class15_stability.pdf ↩  Lorenzo Rosasco, Tomaso Poggio: "Class slides on Stability of Tikhonov ": http://www.mit.edu/~9.520/fall14/slides/class16/class16_stability.pdf ↩  Lorenzo Rosasco, Tomaso Poggio: "9.520: Statistical Learning Theory and Applications Fall 2014": http://www.mit.edu/~9.520/fall14/ ↩  Rosasco, Lorenzo (2014). Consistency, Learnability and Regularization. Lecture Notes for MIT Course 9.520. ↩     