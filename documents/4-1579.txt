   Kernel (linear algebra)      Kernel (linear algebra)   In linear algebra and functional analysis , the kernel (also null space or nullspace ) of a linear map between two vector spaces  V and W , is the set of all elements v of V for which , where 0 denotes the zero vector in W . That is, in set-builder notation ,       ker   (  L  )    =    {   𝐯  ∈  V   |    L   (  𝐯  )    =  𝟎   }   .        kernel  L      conditional-set    𝐯  V       L  𝐯   0    .     \ker(L)=\left\{\mathbf{v}\in V|L(\mathbf{v})=\mathbf{0}\right\}\text{.}     Properties of the Kernel  The kernel of L is a linear subspace of the domain  V . 1 In the linear map , two elements of V have the same image in W if and only if their difference lies in the kernel of L :        L   (   𝐯  1   )    =    L   (   𝐯  2   )    ⇔      L   (    𝐯  1   -   𝐯  2    )    =   𝟎  .       formulae-sequence      L   subscript  𝐯  1       L   subscript  𝐯  2    normal-⇔        L     subscript  𝐯  1    subscript  𝐯  2       0  .      L(\mathbf{v}_{1})=L(\mathbf{v}_{2})\;\;\;\;\Leftrightarrow\;\;\;\;L(\mathbf{v}%
 _{1}-\mathbf{v}_{2})=\mathbf{0}\text{.}   It follows that the image of L is isomorphic to the quotient of V by the kernel:       im   (  L  )    ≅    V  /   ker   (  L  )     .        im  L       V   kernel  L    .     \mathop{\mathrm{im}}(L)\cong V/\ker(L)\text{.}   This implies the rank–nullity theorem :        dim   (   ker  L   )    +   dim   (   im  L   )     =   dim    (  V  )    .            dimension   kernel  L     dimension   im  L      dimension    V  .      \dim(\ker L)+\dim(\mathop{\mathrm{im}}L)=\dim(V)\text{.}\,   where, by “rank” we mean the dimension of the image of L , and by “nullity” that of the kernel of L .  When V is an inner product space , the quotient  can be identified with the orthogonal complement in V of ker( L ). This is the generalization to linear operators of the row space , or coimage, of a matrix.  Application to modules  The notion of kernel applies to the homomorphisms of modules , the latter being a generalization of the vector space over a field to that over a ring . The domain of the mapping is a " right  free module ", and the kernel constitutes a " submodule ". Here, the concepts of rank and nullity do not necessarily apply.  The kernel in functional analysis  If V and W are topological vector spaces (and W is finite-dimensional) then a linear operator L : V → W is continuous if and only if the kernel of L is a closed subspace of V .  Representation as matrix multiplication  Consider a linear map represented as a m × n matrix A with coefficients in a field  K (typically the field of the real numbers or of the complex numbers ) and operating on column vectors x with n components over K . The kernel of this linear map is the set of solutions to the equation , where 0 is understood as the zero vector . The dimension of the kernel of A is called the nullity of A . In set-builder notation ,        N   (  A  )    =   Null   (  A  )    =   ker   (  A  )    =   {   𝐱  ∈   K  n    |    A  𝐱   =  𝟎   }    .         normal-N  A    Null  A         ker  A         conditional-set    𝐱   superscript  K  n        A  𝐱   0       \operatorname{N}(A)=\operatorname{Null}(A)=\operatorname{ker}(A)=\left\{%
 \mathbf{x}\in K^{n}|A\mathbf{x}=\mathbf{0}\right\}.   The matrix equation is equivalent to a homogeneous system of linear equations :        A  𝐱   =   𝟎    ⇔       a  11    x  1       +       a  12    x  2         +   ⋯    +        a   1  n     x  n       =      0        a  21    x  1       +       a  22    x  2         +   ⋯    +        a   2  n     x  n       =      0      ⋮       ⋮       ⋮         ⋮         a   m  1     x  1       +       a   m  2     x  2         +   ⋯    +        a   m  n     x  n       =       0  .         normal-⇔      A  𝐱   0        subscript  a  11    subscript  x  1     missing-subexpression     missing-subexpression      subscript  a  12    subscript  x  2     missing-subexpression    limit-from    normal-⋯      missing-subexpression      subscript  a    1  n     subscript  x  n     missing-subexpression     missing-subexpression    missing-subexpression   0       subscript  a  21    subscript  x  1     missing-subexpression     missing-subexpression      subscript  a  22    subscript  x  2     missing-subexpression    limit-from    normal-⋯      missing-subexpression      subscript  a    2  n     subscript  x  n     missing-subexpression     missing-subexpression    missing-subexpression   0    normal-⋮   missing-subexpression    missing-subexpression    missing-subexpression   normal-⋮   missing-subexpression    missing-subexpression    missing-subexpression   normal-⋮   missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression   normal-⋮       subscript  a    m  1     subscript  x  1     missing-subexpression     missing-subexpression      subscript  a    m  2     subscript  x  2     missing-subexpression    limit-from    normal-⋯      missing-subexpression      subscript  a    m  n     subscript  x  n     missing-subexpression     missing-subexpression    missing-subexpression     0  .       A\mathbf{x}=\mathbf{0}\;\;\Leftrightarrow\;\;\begin{aligned}\displaystyle a_{1%
 1}x_{1}&&\displaystyle\;+&&\displaystyle a_{12}x_{2}&&\displaystyle\;+\;\cdots%
 \;+&&\displaystyle a_{1n}x_{n}&&\displaystyle\;=&&&\displaystyle 0\\
 \displaystyle a_{21}x_{1}&&\displaystyle\;+&&\displaystyle a_{22}x_{2}&&%
 \displaystyle\;+\;\cdots\;+&&\displaystyle a_{2n}x_{n}&&\displaystyle\;=&&&%
 \displaystyle 0\\
 \displaystyle\vdots&&&&\displaystyle\vdots&&&&\displaystyle\vdots&&&&&%
 \displaystyle\;\vdots\\
 \displaystyle a_{m1}x_{1}&&\displaystyle\;+&&\displaystyle a_{m2}x_{2}&&%
 \displaystyle\;+\;\cdots\;+&&\displaystyle a_{mn}x_{n}&&\displaystyle\;=&&&%
 \displaystyle 0\text{.}\\
 \end{aligned}   Thus the kernel of A is the same as the solution set to the above homogeneous equations.  Subspace properties  The kernel of an  matrix A over a field K is a linear subspace of K n . That is, the kernel of A , the set Null( A ), has the following three properties:   Null( A ) always contains the zero vector , since .  If  and , then . This follows from the distributivity of matrix multiplication over addition.  If  and c is a scalar , then , since .   The Row Space of a Matrix  The product A x can be written in terms of the dot product of vectors as follows:        A  𝐱   =   [       𝐚  1   ⋅  𝐱         𝐚  2   ⋅  𝐱       ⋮        𝐚  m   ⋅  𝐱      ]    .        A  𝐱      normal-⋅   subscript  𝐚  1   𝐱      normal-⋅   subscript  𝐚  2   𝐱     normal-⋮     normal-⋅   subscript  𝐚  m   𝐱       A\mathbf{x}=\begin{bmatrix}\mathbf{a}_{1}\cdot\mathbf{x}\\
 \mathbf{a}_{2}\cdot\mathbf{x}\\
 \vdots\\
 \mathbf{a}_{m}\cdot\mathbf{x}\end{bmatrix}.   Here a 1 , ... , a m denote the rows of the matrix A . It follows that x is in the kernel of A if and only if x is orthogonal (or perpendicular) to each of the row vectors of A (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).  The row space , or coimage, of a matrix A is the span of the row vectors of A . By the above reasoning, the kernel of A is the orthogonal complement to the row space. That is, a vector x lies in the kernel of A if and only if it is perpendicular to every vector in the row space of A .  The dimension of the row space of A is called the rank of A , and the dimension of the kernel of A is called the nullity of A . These quantities are related by the rank–nullity theorem         rank   (  A  )    +   nullity   (  A  )     =  n   .         rank  A    nullity  A    n    \operatorname{rank}(A)+\operatorname{nullity}(A)=n.     Left null space  The left null space , or cokernel, of a matrix A consists of all vectors x such that x T A = 0 T , where T denotes the transpose of a column vector. The left null space of A is the same as the kernel of A T . The left null space of A is the orthogonal complement to the column space of A , and is dual to the cokernel of the associated linear transformation. The kernel, the row space, the column space, and the left null space of A are the four fundamental subspaces associated to the matrix A .  Nonhomogeneous systems of linear equations  The kernel also plays a role in the solution to a nonhomogeneous system of linear equations:       A  𝐱   =   𝐛  or       a  11    x  1       +       a  12    x  2         +   ⋯    +        a   1  n     x  n       =       b  1         a  21    x  1       +       a  22    x  2         +   ⋯    +        a   2  n     x  n       =       b  2       ⋮       ⋮       ⋮         ⋮         a   m  1     x  1       +       a   m  2     x  2         +   ⋯    +        a   m  n     x  n       =       b  m             A  𝐱    𝐛  or       subscript  a  11    subscript  x  1     missing-subexpression     missing-subexpression      subscript  a  12    subscript  x  2     missing-subexpression    limit-from    normal-⋯      missing-subexpression      subscript  a    1  n     subscript  x  n     missing-subexpression     missing-subexpression    missing-subexpression    subscript  b  1        subscript  a  21    subscript  x  1     missing-subexpression     missing-subexpression      subscript  a  22    subscript  x  2     missing-subexpression    limit-from    normal-⋯      missing-subexpression      subscript  a    2  n     subscript  x  n     missing-subexpression     missing-subexpression    missing-subexpression    subscript  b  2     normal-⋮   missing-subexpression    missing-subexpression    missing-subexpression   normal-⋮   missing-subexpression    missing-subexpression    missing-subexpression   normal-⋮   missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression   normal-⋮       subscript  a    m  1     subscript  x  1     missing-subexpression     missing-subexpression      subscript  a    m  2     subscript  x  2     missing-subexpression    limit-from    normal-⋯      missing-subexpression      subscript  a    m  n     subscript  x  n     missing-subexpression     missing-subexpression    missing-subexpression    subscript  b  m        A\mathbf{x}=\mathbf{b}\;\;\;\;\;\;\text{or}\;\;\;\;\;\;\begin{aligned}%
 \displaystyle a_{11}x_{1}&&\displaystyle\;+&&\displaystyle a_{12}x_{2}&&%
 \displaystyle\;+\;\cdots\;+&&\displaystyle a_{1n}x_{n}&&\displaystyle\;=&&&%
 \displaystyle b_{1}\\
 \displaystyle a_{21}x_{1}&&\displaystyle\;+&&\displaystyle a_{22}x_{2}&&%
 \displaystyle\;+\;\cdots\;+&&\displaystyle a_{2n}x_{n}&&\displaystyle\;=&&&%
 \displaystyle b_{2}\\
 \displaystyle\vdots&&&&\displaystyle\vdots&&&&\displaystyle\vdots&&&&&%
 \displaystyle\;\vdots\\
 \displaystyle a_{m1}x_{1}&&\displaystyle\;+&&\displaystyle a_{m2}x_{2}&&%
 \displaystyle\;+\;\cdots\;+&&\displaystyle a_{mn}x_{n}&&\displaystyle\;=&&&%
 \displaystyle b_{m}\\
 \end{aligned}   If u and v are two possible solutions to the above equation, then       A   (   𝐮  -  𝐯   )    =    A  𝐮   -   A  𝐯    =   𝐛  -  𝐛   =   𝟎           A    𝐮  𝐯        A  𝐮     A  𝐯           𝐛  𝐛        0     A(\mathbf{u}-\mathbf{v})=A\mathbf{u}-A\mathbf{v}=\mathbf{b}-\mathbf{b}=\mathbf%
 {0}\,   Thus, the difference of any two solutions to the equation A x = b lies in the kernel of A .  It follows that any solution to the equation A x = b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel. That is, the solution set to the equation ''A x = b is       {   𝐯  +  𝐱   |    A  𝐯   =   𝐛  ∧  𝐱   ∈   Null   (  A  )     }   ,     conditional-set    𝐯  𝐱         A  𝐯     𝐛  𝐱         Null  A       \left\{\mathbf{v}+\mathbf{x}|A\mathbf{v}=\mathbf{b}\land\mathbf{x}\in%
 \operatorname{Null}(A)\right\},   Geometrically, this says that the solution set to A x = b is the translation of the kernel of A by the vector v . See also Fredholm alternative and flat (geometry) .  Illustration  We give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.  Consider the matrix       A  =   [     2    3    5       -  4     2    3     ]    .      A    2  3  5      4   2  3      A=\begin{bmatrix}\,\,\,2&3&5\\
 -4&2&3\end{bmatrix}.   The kernel of this matrix consists of all vectors ( x , y , z ) ∈ R 3 for which         [     2    3    5       -  4     2    3     ]    [     x      y      z     ]    =   [     0      0     ]    ,          2  3  5      4   2  3      x    y    z       0    0      \begin{bmatrix}\,\,\,2&3&5\\
 -4&2&3\end{bmatrix}\begin{bmatrix}x\\
 y\\
 z\end{bmatrix}=\begin{bmatrix}0\\
 0\end{bmatrix},   which can be expressed as a homogeneous system of linear equations involving x , y , and z :      2  x      2  x    \displaystyle 2x   which can be written in matrix form as:       [     2    3    5    0       -  4     2    3    0     ]   .     delimited-[]    2  3  5  0      4   2  3  0      \left[\begin{array}[]{ccc|c}2&3&5&0\\
 -4&2&3&0\end{array}\right].    Gauss–Jordan elimination reduces this to:       [     1    0     1  /  16     0      0    1     13  /  8     0     ]   .     delimited-[]    1  0    1  16   0    0  1    13  8   0      \left[\begin{array}[]{ccc|c}1&0&1/16&0\\
 0&1&13/8&0\end{array}\right].   Rewriting yields:      x  =       x  absent    \displaystyle x=   Now we can express an element of the kernel:        [     x      y      z     ]   =   c   [      -   1  /  16         -   13  /  8        1     ]     .        x    y    z      c        1  16          13  8      1       \begin{bmatrix}x\\
 y\\
 z\end{bmatrix}=c\begin{bmatrix}-1/16\\
 -13/8\\
 1\end{bmatrix}.   for c a scalar .  Since c is a free variable , this can be expressed equally well as,        [     x      y      z     ]   =   c   [      -  1        -  26       16     ]     .        x    y    z      c      1       26     16       \begin{bmatrix}x\\
 y\\
 z\end{bmatrix}=c\begin{bmatrix}-1\\
 -26\\
 16\end{bmatrix}.   The kernel of A is precisely the solution set to these equations (in this case, a line through the origin in R 3 ); the vector (−1,−26,16) T constitutes a basis of the kernel of A . Thus, the nullity of A is 1.  Note also that the following dot products are zero:          [     2    3    5     ]   ⋅   [      -  1        -  26       16     ]    =   0  and       [      -  4     2    3     ]   ⋅   [      -  1        -  26       16     ]    =  0    ,     formulae-sequence     normal-⋅   delimited-[]    2  3  5         1       26     16      0  and       normal-⋅   delimited-[]      4   2  3         1       26     16     0     \left[\begin{array}[]{ccc}2&3&5\end{array}\right]\cdot\begin{bmatrix}-1\\
 -26\\
 16\end{bmatrix}=0\quad\mathrm{and}\quad\left[\begin{array}[]{ccc}-4&2&3\end{%
 array}\right]\cdot\begin{bmatrix}-1\\
 -26\\
 16\end{bmatrix}=0\mathrm{,}   which illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.  These two (linearly independent) row vectors span the row space of A , a plane orthogonal to the vector (−1,−26,16) T .  With the rank of A 2, the nullity of A 1, and the dimension of A 3, we have an illustration of the rank-nullity theorem.  Examples   If L : R m → R n , then the kernel of L is the solution set to a homogeneous system of linear equations . As in the above illustration, if L is the operator:          L   (   x  1   ,   x  2   ,   x  3   )    =   (    2   x  1    +   3   x  2    +   5   x  3     ,    -   4   x  1     +   2   x  2    +   3   x  3     )         L    subscript  x  1    subscript  x  2    subscript  x  3          2   subscript  x  1      3   subscript  x  2      5   subscript  x  3           4   subscript  x  1       2   subscript  x  2      3   subscript  x  3        L(x_{1},x_{2},x_{3})=(2x_{1}+3x_{2}+5x_{3},\;-4x_{1}+2x_{2}+3x_{3})       then the kernel of L is the set of solutions to the equations : \begin{alignat}{7}    2x_1 &\;+\;& 3x_2 &\;+\;& 5x_3 &\;=\;& 0 \\  -4x_1 &\;+\;& 2x_2 &\;+\;& 3x_3 &\;=\;& 0  \end{alignat}   Let C [0,1] denote the vector space of all continuous real-valued functions on the interval [0,1], and define L : C [0,1] → R by the rule          L   (  f  )    =   f   (  0.3  )    .          L  f     f  0.3  .     L(f)=f(0.3)\text{.}\,       Then the kernel of L consists of all functions f ∈ C [0,1] for which f (0.3) = 0.    Let C ∞ ( R ) be the vector space of all infinitely differentiable functions R → R , and let D : C ∞ ( R ) → C ∞ ( R ) be the differentiation operator :          D   (  f  )    =     d  f    d  x    .         D  f         d  f     d  x    .     D(f)=\frac{df}{dx}\text{.}       Then the kernel of D consists of all functions in C ∞ ( R ) whose derivatives are zero, i.e. the set of all constant functions .    Let R ∞ be the direct product of infinitely many copies of R , and let s : R ∞ → R ∞ be the shift operator          s   (   x  1   ,   x  2   ,   x  3   ,   x  4   ,  …  )    =    (   x  2   ,   x  3   ,   x  4   ,  …  )   .         s    subscript  x  1    subscript  x  2    subscript  x  3    subscript  x  4   normal-…        subscript  x  2    subscript  x  3    subscript  x  4   normal-…   .     s(x_{1},x_{2},x_{3},x_{4},\ldots)=(x_{2},x_{3},x_{4},\ldots)\text{.}       Then the kernel of s is the one-dimensional subspace consisting of all vectors ( x 1 , 0, 0, ...).    If V is an inner product space and W is a subspace, the kernel of the orthogonal projection  V → W is the orthogonal complement to W in V .   Computation by Gaussian elimination  A basis of the kernel of a matrix may be computed by Gaussian elimination .  For this purpose, given an m × n matrix A , we construct first the row augmented matrix      [     A      I     ]   ,     delimited-[]    A    I      \left[\begin{array}[]{c}A\\
 \hline I\end{array}\right],   where   I   I   I   is the n × n  identity matrix .  Computing its column echelon form by Gaussian elimination (or any other suitable method), we get a matrix     [     B      C     ]   .     delimited-[]    B    C      \left[\begin{array}[]{c}B\\
 \hline C\end{array}\right].   A basis of the kernel of A consists in the non-zero columns of C such that the corresponding column of B is a zero column .  In fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.  For example, suppose that       A  =   [      1    0     -  3     0    2     -  8       0    1    5    0     -  1     4      0    0    0    1    7     -  9       0    0    0    0    0    0      ]    .      A   delimited-[]    1  0    3   0  2    8     0  1  5  0    1   4    0  0  0  1  7    9     0  0  0  0  0  0       A=\left[\begin{array}[]{cccccc}1&0&-3&0&2&-8\\
 0&1&5&0&-1&4\\
 0&0&0&1&7&-9\\
 0&0&0&0&0&0\end{array}\,\right].   Then        [     A      I     ]   =   [     1    0     -  3     0    2     -  8       0    1    5    0     -  1     4      0    0    0    1    7     -  9       0    0    0    0    0    0      1    0    0    0    0    0      0    1    0    0    0    0      0    0    1    0    0    0      0    0    0    1    0    0      0    0    0    0    1    0      0    0    0    0    0    1     ]    .       delimited-[]    A    I      delimited-[]    1  0    3   0  2    8     0  1  5  0    1   4    0  0  0  1  7    9     0  0  0  0  0  0    1  0  0  0  0  0    0  1  0  0  0  0    0  0  1  0  0  0    0  0  0  1  0  0    0  0  0  0  1  0    0  0  0  0  0  1       \left[\begin{array}[]{c}A\\
 \hline I\end{array}\right]=\left[\begin{array}[]{cccccc}1&0&-3&0&2&-8\\
 0&1&5&0&-1&4\\
 0&0&0&1&7&-9\\
 0&0&0&0&0&0\\
 \hline 1&0&0&0&0&0\\
 0&1&0&0&0&0\\
 0&0&1&0&0&0\\
 0&0&0&1&0&0\\
 0&0&0&0&1&0\\
 0&0&0&0&0&1\end{array}\right].     Putting the upper part in column echelon form by column operations on the whole matrix gives        [     B      C     ]   =   [     1    0    0    0    0    0      0    1    0    0    0    0      0    0    1    0    0    0      0    0    0    0    0    0      1    0    0    3     -  2     8      0    1    0     -  5     1     -  4       0    0    0    1    0    0      0    0    1    0     -  7     9      0    0    0    0    1    0      0    0    0    0    0    1     ]    .       delimited-[]    B    C      delimited-[]    1  0  0  0  0  0    0  1  0  0  0  0    0  0  1  0  0  0    0  0  0  0  0  0    1  0  0  3    2   8    0  1  0    5   1    4     0  0  0  1  0  0    0  0  1  0    7   9    0  0  0  0  1  0    0  0  0  0  0  1       \left[\begin{array}[]{c}B\\
 \hline C\end{array}\right]=\left[\begin{array}[]{cccccc}1&0&0&0&0&0\\
 0&1&0&0&0&0\\
 0&0&1&0&0&0\\
 0&0&0&0&0&0\\
 \hline 1&0&0&3&-2&8\\
 0&1&0&-5&1&-4\\
 0&0&0&1&0&0\\
 0&0&1&0&-7&9\\
 0&0&0&0&1&0\\
 0&0&0&0&0&1\end{array}\right].     The last three columns of B are zero columns. Therefore, the three last vectors of C ,       [     3       -  5       1      0      0      0     ]   ,   [      -  2       1      0       -  7       1      0     ]   ,   [     8       -  4       0      9      0      1     ]       delimited-[]    3      5     1    0    0    0      delimited-[]      2     1    0      7     1    0      delimited-[]    8      4     0    9    0    1       \left[\!\!\begin{array}[]{r}3\\
 -5\\
 1\\
 0\\
 0\\
 0\end{array}\right],\;\left[\!\!\begin{array}[]{r}-2\\
 1\\
 0\\
 -7\\
 1\\
 0\end{array}\right],\;\left[\!\!\begin{array}[]{r}8\\
 -4\\
 0\\
 9\\
 0\\
 1\end{array}\right]   are a basis of the kernel of A .  Since column operations correspond to pre-multiplication by invertible matrices, the fact that    [     A      I     ]     delimited-[]    A    I      \left[\begin{array}[]{c}A\\
 \hline I\end{array}\right]   reduces to    [     B      C     ]     delimited-[]    B    C      \left[\begin{array}[]{c}B\\
 \hline C\end{array}\right]   tells us that     A  C   =  B        A  C   B    AC=B   . That is, the action of   A   A   A   via (the columns of)   C   C   C   corresponds to the action of   B   B   B   . Since   B   B   B   is in column-echelon form, it acts trivially on only the elementary basis elements corresponding to zero columns in   B   B   B   . Since the action of   B   B   B   corresponds to the action of   A   A   A   via columns of   C   C   C   , the corresponding columns of   C   C   C   must then be null columns for   A   A   A   , and must form a basis of the null space of   A   A   A   by the rank-nullity theorem.  Numerical computation  The problem of computing the kernel on a computer depends on the nature of the coefficients.  Exact coefficients  If the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic , which reduces the problem to a similar one over a finite field .  For coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gröbner basis computation, better algorithms are known, which have roughly the same computational complexity , but are faster and behave better with modern computer hardware .  Floating point computation  For matrices whose entries are floating-point numbers , the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors , a floating-point matrix has almost always a full rank , even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned , i.e. it has a low condition number . 2  Even for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.  See also   System of linear equations  Row and column spaces  Row reduction  Four fundamental subspaces  Vector space  Linear subspace  Linear operator  Function space  Fredholm alternative   Notes  References          Lloyd N. Trefethen and David Bau, III, Numerical Linear Algebra, SIAM 1997, ISBN 978-0-89871-361-9 online version   External links    Gilbert Strang , MIT Linear Algebra Lecture on the Four Fundamental Subspaces at Google Video, from MIT OpenCourseWare  Khan Academy , Introduction to the Null Space of a Matrix   "  Category:Linear algebra  Category:Functional analysis  Category:Matrices  Category:Numerical linear algebra     Linear algebra, as discussed in this article, is a very well established mathematical discipline for which there are many sources. Almost all of the material in this article can be found in Lay 2005, Meyer 2001, and Strang 2005. ↩  https://www.math.ohiou.edu/courses/math3600/lecture11.pdf ↩     