   Multimodal learning      Multimodal learning   The information in real world usually comes as different modalities. For example, images are usually associated with tags and text explanations; texts contain images to more clearly express the main idea of the article. Different modalities are characterized by very different statistical properties. For Instance, image is usually represented as pixel intensities or outputs of feature extractors, while texts are represented as discrete word count vectors. Due to distinct statistical properties of different information resources, it is very important to discover the relationship between different modalities. Multimodal learning is a good model to represent the joint representations of different modalities. The multimodal learning model is also capable to fill missing modality given the observed ones. The multimodal learning model combines two Deep Boltzmann Machines each corresponds to one modality. An additional hidden layer is placed on top of the two Boltzmann Machines to give the joint representation.  Motivation  A lot of models/algorithms have been implemented to retrieve and classify a certain type of data, e.g. image or text. However, data usually come with different modalities which carry different information. For example, it is very common to caption an image to convey the information not presented by this image. Similarly, sometimes it is more straightforward to use a image to description the information which may not be obvious from texts. As a results, if some different words appear in similar images, these words are very likely used to describe the same thing. Conversely, if some words are used in different images, these images may represent the same object. Thus, it is important to invite a novel model which is able to jointly represent the information such that the model can capture the correlation structure between different modalities. Moreover, it should also be able to recover missing modalities given observed ones, e.g. predicting possible image object according to text description. The Multimodal Deep Boltzmann Machine model satisfies the above purposes.  Background: Boltzmann Machine  A Boltzmann machine is a type of stochastic neural network invented by Geoffrey Hinton and Terry Sejnowski in 1985. Boltzmann machines can be seen as the stochastic , generative counterpart of Hopfield nets . They are named after the Boltzmann distribution in statistical mechanics. The units in Boltzmann machines are divided into two groups-visible units and hidden units. General Boltzmann Machines allow connection between any units. However, learning is impractical using general Boltzmann Machines because the computational time is exponential to the size of the machine. A more efficient architecture is called Restricted Boltzmann Machine where connection is only allowed between hidden unit and visible unit, which is described in the next section.  Restricted Boltzmann Machine  A Restricted Boltzmann Machine 1 is an undirected graphical model with stochastic visible variable and stochastic hidden variables. Each visible variable is connected to each hidden variable. The energy function of the model is defined as       E   (  𝐯  ,  𝐡  ;  θ  )    =    -    ∑   i  =  1   D     ∑   j  =  1   F     W   i  j     v  i    h  j       -    ∑   i  =  1   D     b  i    v  i     -    ∑   j  =  1   F     a  j    h  j            E   𝐯  𝐡  θ          superscript   subscript     i  1    D     superscript   subscript     j  1    F      subscript  W    i  j     subscript  v  i    subscript  h  j         superscript   subscript     i  1    D      subscript  b  i    subscript  v  i       superscript   subscript     j  1    F      subscript  a  j    subscript  h  j        E(\mathbf{v},\mathbf{h};\theta)=-\sum_{i=1}^{D}\sum_{j=1}^{F}W_{ij}v_{i}h_{j}-%
 \sum_{i=1}^{D}b_{i}v_{i}-\sum_{j=1}^{F}a_{j}h_{j}   where    θ  =   {  𝐯  ,  𝐡  ;  θ  }       θ   𝐯  𝐡  θ     \theta=\{\mathbf{v},\mathbf{h};\theta\}   are model parameters    W   i  j      subscript  W    i  j     W_{ij}   represents the symmetric interaction term between visible unit   i   i   i   and hidden unit   j   j   j   ;    b  i     subscript  b  i    b_{i}   and    a  j     subscript  a  j    a_{j}   are bias terms. The joint distribution of the system is defined as       P   (  𝐯  ;  θ  )    =    1   𝒵   (  θ  )       ∑  𝐡    exp   (   -   E   (  𝐯  ,  𝐡  ;  θ  )     )            P   𝐯  θ        1    𝒵  θ      subscript   𝐡     exp      E   𝐯  𝐡  θ          P(\mathbf{v};\theta)=\frac{1}{\mathcal{Z}(\theta)}\sum_{\mathbf{h}}\mathrm{exp%
 }(-E(\mathbf{v},\mathbf{h};\theta))   where    𝒵   (  θ  )       𝒵  θ    \mathcal{Z}(\theta)   is a normalizing constant. The conditional distribution over hidden   𝐡   𝐡   \mathbf{h}   and   𝐯   𝐯   \mathbf{v}   can be derived as logistic function in terms of model parameters.      P   (  𝐡  |  𝐯  ;  θ  )   =   ∏   j  =  1   F   p   (   h  j   |  𝐯  )      fragments  P   fragments  normal-(  h  normal-|  v  normal-;  θ  normal-)     superscript   subscript  product    j  1    F   p   fragments  normal-(   subscript  h  j   normal-|  v  normal-)     P(\mathbf{h}|\mathbf{v};\theta)=\prod_{j=1}^{F}p(h_{j}|\mathbf{v})   , with    p   (   h  j   =  1  |  𝐯  )   =  g   (   ∑   i  =  1   D    W   i  j     v  i   +   a  j   )      fragments  p   fragments  normal-(   subscript  h  j    1  normal-|  v  normal-)    g   fragments  normal-(   superscript   subscript     i  1    D    subscript  W    i  j     subscript  v  i     subscript  a  j   normal-)     p(h_{j}=1|\mathbf{v})=g(\sum_{i=1}^{D}W_{ij}v_{i}+a_{j})         P   (  𝐯  |  𝐡  ;  θ  )   =   ∏   i  =  1   D   p   (   v  i   |  𝐯  )      fragments  P   fragments  normal-(  v  normal-|  h  normal-;  θ  normal-)     superscript   subscript  product    i  1    D   p   fragments  normal-(   subscript  v  i   normal-|  v  normal-)     P(\mathbf{v}|\mathbf{h};\theta)=\prod_{i=1}^{D}p(v_{i}|\mathbf{v})   , with    p   (   v  i   =  1  |  𝐡  )   =  g   (   ∑   j  =  1   F    W   i  j     h  j   +   b  i   )      fragments  p   fragments  normal-(   subscript  v  i    1  normal-|  h  normal-)    g   fragments  normal-(   superscript   subscript     j  1    F    subscript  W    i  j     subscript  h  j     subscript  b  i   normal-)     p(v_{i}=1|\mathbf{h})=g(\sum_{j=1}^{F}W_{ij}h_{j}+b_{i})   where     g   (  x  )    =   1   (   1  +   exp   (   -  x   )     )          g  x     1    1    exp    x        g(x)=\frac{1}{(1+\mathrm{exp}(-x))}   is the logistic function.  The derivative of the log-likelihood with respect to the model parameters can be decomposed as the difference between the model's expectation and data-dependent expectation .  Gaussian-Bernoulli RBM  Gaussian-Bernoulli RBMs 2 are a variant of Restricted Boltzmann Machine used for modeling real-valued vectors such as pixel intensities. It is usually used to model the image data. The energy of the system of the Gaussian-Bernoulli RBM is defined as       E   (  𝐯  ,  𝐡  ;  θ  )    =     ∑   i  =  1   D      (    v  i   -   b  i    )   2    2   σ  i  2      -    ∑   i  =  1   D     ∑   j  =  1   F      v  i    σ  i     W   i  j     v  i    h  j      -    ∑   i  =  1   D     b  i    v  i     -    ∑   j  =  1   F     a  j    h  j            E   𝐯  𝐡  θ        superscript   subscript     i  1    D      superscript     subscript  v  i    subscript  b  i    2     2   superscript   subscript  σ  i   2        superscript   subscript     i  1    D     superscript   subscript     j  1    F        subscript  v  i    subscript  σ  i     subscript  W    i  j     subscript  v  i    subscript  h  j        superscript   subscript     i  1    D      subscript  b  i    subscript  v  i       superscript   subscript     j  1    F      subscript  a  j    subscript  h  j        E(\mathbf{v},\mathbf{h};\theta)=\sum_{i=1}^{D}\frac{(v_{i}-b_{i})^{2}}{2\sigma%
 _{i}^{2}}-\sum_{i=1}^{D}\sum_{j=1}^{F}\frac{v_{i}}{\sigma_{i}}W_{ij}v_{i}h_{j}%
 -\sum_{i=1}^{D}b_{i}v_{i}-\sum_{j=1}^{F}a_{j}h_{j}   where    θ  =   {  𝐚  ,  𝐛  ,  𝐰  ,  σ  }       θ   𝐚  𝐛  𝐰  σ     \theta=\{\mathbf{a},\mathbf{b},\mathbf{w},\mathbf{\sigma}\}   are the model parameters. The joint distribution is defined the same as the one in Restricted Boltzmann Machine . The conditional distributions now become      P   (  𝐡  |  𝐯  ;  θ  )   =   ∏   j  =  1   F   p   (   h  j   |  𝐯  )      fragments  P   fragments  normal-(  h  normal-|  v  normal-;  θ  normal-)     superscript   subscript  product    j  1    F   p   fragments  normal-(   subscript  h  j   normal-|  v  normal-)     P(\mathbf{h}|\mathbf{v};\theta)=\prod_{j=1}^{F}p(h_{j}|\mathbf{v})   , with    p   (   h  j   =  1  |  𝐯  )   =  g   (   ∑   i  =  1   D    W   i  j      v  i    σ  i    +   a  j   )      fragments  p   fragments  normal-(   subscript  h  j    1  normal-|  v  normal-)    g   fragments  normal-(   superscript   subscript     i  1    D    subscript  W    i  j       subscript  v  i    subscript  σ  i      subscript  a  j   normal-)     p(h_{j}=1|\mathbf{v})=g(\sum_{i=1}^{D}W_{ij}\frac{v_{i}}{\sigma_{i}}+a_{j})         P   (  𝐯  |  𝐡  ;  θ  )   =   ∏   i  =  1   D   p   (   v  i   |  𝐡  )      fragments  P   fragments  normal-(  v  normal-|  h  normal-;  θ  normal-)     superscript   subscript  product    i  1    D   p   fragments  normal-(   subscript  v  i   normal-|  h  normal-)     P(\mathbf{v}|\mathbf{h};\theta)=\prod_{i=1}^{D}p(v_{i}|\mathbf{h})   , with    p   (   v  i   |  𝐡  )   ∼  𝒩   (   σ  i    ∑   j  =  1   F    W   i  j     h  j   +   b  i   ,   σ  i  2   )      fragments  p   fragments  normal-(   subscript  v  i   normal-|  h  normal-)   similar-to  N   fragments  normal-(   subscript  σ  i    superscript   subscript     j  1    F    subscript  W    i  j     subscript  h  j     subscript  b  i   normal-,   superscript   subscript  σ  i   2   normal-)     p(v_{i}|\mathbf{h})\sim\mathcal{N}(\sigma_{i}\sum_{j=1}^{F}W_{ij}h_{j}+b_{i},%
 \sigma_{i}^{2})   In Gaussian-Bernoulli RBM, the visible unit conditioned on hidden units is modeled as a Gaussian distribution.  Replicated Softmax Model  The Replicated Softmax Model  3 is also an variant of Restricted Boltzmann Machine and commonly used to model word count vectors in a document. In a typical text mining problem, let   K   K   K   be the dictionary size, and   M   M   M   be the number of words in the document. Let   𝐕   𝐕   \mathbf{V}   be a    M  ×  K      M  K    M\times K   binary matrix with     v   i  k    =  1       subscript  v    i  k    1    v_{ik}=1   only when the    i   t  h      superscript  i    t  h     i^{th}   word in the document is the    k   t  h      superscript  k    t  h     k^{th}   word in the dictionary.     v  ^   k     subscript   normal-^  v   k    \hat{v}_{k}   denotes the count for the    k   t  h      superscript  k    t  h     k^{th}   word in the dictionary. The energy of the state    {  𝐕  ,  𝐡  }     𝐕  𝐡    \{\mathbf{V},\mathbf{h}\}   for a document contains   M   M   M   words is defined as       E   (  𝐕  ,  𝐡  )    =    -    ∑   j  =  1   F     ∑   k  =  1   K     W   j  k      v  ^   k    h  j       -    ∑   k  =  1   K     b  k     v  ^   k     -   M    ∑   j  =  1   F     a  j    h  j             E   𝐕  𝐡          superscript   subscript     j  1    F     superscript   subscript     k  1    K      subscript  W    j  k     subscript   normal-^  v   k    subscript  h  j         superscript   subscript     k  1    K      subscript  b  k    subscript   normal-^  v   k       M    superscript   subscript     j  1    F      subscript  a  j    subscript  h  j         E(\mathbf{V},\mathbf{h})=-\sum_{j=1}^{F}\sum_{k=1}^{K}W_{jk}\hat{v}_{k}h_{j}-%
 \sum_{k=1}^{K}b_{k}\hat{v}_{k}-M\sum_{j=1}^{F}a_{j}h_{j}   The conditional distributions are given by      p   (   h  j   =  1  |  𝐕  )   =  g   (  M   a  j   +   ∑   k  =  1   K     v  ^   k    W   j  k    )      fragments  p   fragments  normal-(   subscript  h  j    1  normal-|  V  normal-)    g   fragments  normal-(  M   subscript  a  j     superscript   subscript     k  1    K    subscript   normal-^  v   k    subscript  W    j  k    normal-)     p(h_{j}=1|\mathbf{V})=g(Ma_{j}+\sum_{k=1}^{K}\hat{v}_{k}W_{jk})         p   (   v   i  k    =  1  |  𝐡  )   =    exp   (   b  k   +   ∑   j  =  1   F    h  j    W   j  k        ∑   q  =  1   K   exp   (   b  q   +   ∑   j  =  1   F    h  j    W   j  q       )     fragments  p   fragments  normal-(   subscript  v    i  k     1  normal-|  h  normal-)       fragments  exp   fragments  normal-(   subscript  b  k     superscript   subscript     j  1    F    subscript  h  j    subscript  W    j  k       fragments   superscript   subscript     q  1    K   exp   fragments  normal-(   subscript  b  q     superscript   subscript     j  1    F    subscript  h  j    subscript  W    j  q       normal-)    p(v_{ik}=1|\mathbf{h})=\frac{\mathrm{exp}(b_{k}+\sum_{j=1}^{F}h_{j}W_{jk}}{%
 \sum_{q=1}^{K}\mathrm{exp}(b_{q}+\sum_{j=1}^{F}h_{j}W_{jq}})     Deep Boltzmann Machines  A Deep Boltzmann Machine  4 has a sequence of layers of hidden units.There are only connections between adjacent hidden layers, as well as between visible units and hidden units in the first hidden layer. The energy function of the system adds layer interaction terms to the energy function of general Restricted Boltzmann Machine and is defined by        E   (  𝐯  ,  𝐡  ;  θ  )    =        -     ∑   i  =  1   D       ∑   j  =  1    F  1       W   i  j    (  1  )     v  i    h  j   (  1  )        -     ∑   j  =  1    F  1        ∑   l  =  1    F  2       W   j  l    (  2  )     h  j   (  1  )     h  l   (  2  )               -     ∑   l  =  1    F  2        ∑   p  =  1    F  3       W   l  p    (  3  )     h  l   (  2  )     h  p   (  3  )        -     ∑   i  =  1   D      b  i    v  i     -     ∑   j  =  1    F  1       b  j   (  1  )     h  j   (  1  )      -     ∑   l  =  1    F  2       b  l   (  2  )     h  l   (  2  )      -     ∑   p  =  1    F  3       b  p   (  3  )     h  p   (  3  )                 E   𝐯  𝐡  θ    absent         superscript   subscript     i  1    D     superscript   subscript     j  1     subscript  F  1       superscript   subscript  W    i  j    1    subscript  v  i    superscript   subscript  h  j   1         superscript   subscript     j  1     subscript  F  1      superscript   subscript     l  1     subscript  F  2       superscript   subscript  W    j  l    2    superscript   subscript  h  j   1    superscript   subscript  h  l   2          missing-subexpression         superscript   subscript     l  1     subscript  F  2      superscript   subscript     p  1     subscript  F  3       superscript   subscript  W    l  p    3    superscript   subscript  h  l   2    superscript   subscript  h  p   3         superscript   subscript     i  1    D      subscript  b  i    subscript  v  i       superscript   subscript     j  1     subscript  F  1       superscript   subscript  b  j   1    superscript   subscript  h  j   1       superscript   subscript     l  1     subscript  F  2       superscript   subscript  b  l   2    superscript   subscript  h  l   2       superscript   subscript     p  1     subscript  F  3       superscript   subscript  b  p   3    superscript   subscript  h  p   3         \begin{aligned}\displaystyle E({\mathbf{v},\mathbf{h};\theta})=&\displaystyle-%
 \sum_{i=1}^{D}\sum_{j=1}^{F_{1}}W_{ij}^{(1)}v_{i}h_{j}^{(1)}-\sum_{j=1}^{F_{1}%
 }\sum_{l=1}^{F_{2}}W_{jl}^{(2)}h_{j}^{(1)}h_{l}^{(2)}\\
 &\displaystyle-\sum_{l=1}^{F_{2}}\sum_{p=1}^{F_{3}}W_{lp}^{(3)}h_{l}^{(2)}h_{p%
 }^{(3)}-\sum_{i=1}^{D}b_{i}v_{i}-\sum_{j=1}^{F_{1}}b_{j}^{(1)}h_{j}^{(1)}-\sum%
 _{l=1}^{F_{2}}b_{l}^{(2)}h_{l}^{(2)}-\sum_{p=1}^{F_{3}}b_{p}^{(3)}h_{p}^{(3)}%
 \end{aligned}     The joint distribution is       P   (  𝐯  ;  θ  )    =    1   𝒵   (  θ  )       ∑  𝐡    exp   (   -   E   (  𝐯  ,   𝐡   (  1  )    ,   𝐡   (  2  )    ,   𝐡   (  3  )    ;  θ  )     )            P   𝐯  θ        1    𝒵  θ      subscript   𝐡     exp      E   𝐯   superscript  𝐡  1    superscript  𝐡  2    superscript  𝐡  3   θ          P(\mathbf{v};\theta)=\frac{1}{\mathcal{Z}(\theta)}\sum_{\mathbf{h}}\mathrm{exp%
 }(-E(\mathbf{v},\mathbf{h}^{(1)},\mathbf{h}^{(2)},\mathbf{h}^{(3)};\theta))     Multimodal Deep Boltzmann Machines  Multimodal Deep Boltzmann Machine  5 6 uses an image-text bi-modal DBM where the image pathway is modeled as Gaussian-Bernoulli DBM and text pathway as Replicated Softmax DBM, and each DBM has two hidden layers and one visible layer. The two DBMs join together at an additional top hidden layer. The joint distribution over the multi-modal inputs defined as       P   (   𝐯  m   ,   𝐯  t   ;  θ  )       =    ∑    𝐡   (   2  m   )    ,   𝐡   (   2  t   )    ,   𝐡   (  3  )       P   (   𝐡   (   2  m   )    ,   𝐡   (   2  t   )    ,   𝐡   (  3  )    )    (    ∑   𝐡   (   1  m   )      P   (   𝐯  m   ,   𝐡   (   1  m   )    |   𝐡   (   2  m   )    )   )    (    ∑   𝐡   (   1  t   )      P   (   𝐯  t   ,   𝐡   (   1  t   )    |   𝐡   (   2  t   )    )   )          =    1    𝒵  M    (  θ  )        ∑  𝐡    exp   (    ∑   k  j      W   k  j    (   1  t   )     v  k  t    h  j   (   1  t   )             +     ∑   j  l       W   j  l    (   2  t   )     h  j   (   1  t   )     h  l   (   2  t   )       +     ∑  k      b  k  t    v  k  t     +   M     ∑  j      b  j   (   1  t   )     h  j   (   1  t   )       +     ∑  l      b  l   (   2  t   )     h  l   (   2  t   )              -     ∑  i        (    v  i  m   -   b  i  m    )   2    2   σ  2        +     ∑   i  j         v  i  m    σ  i      W   i  j    (   1  m   )     h  j   (   1  m   )              +     ∑   j  l       W   j  l    (   2  m   )     h  j   (   1  m   )     h  l   (   2  m   )       +     ∑  j      b  j   (   1  m   )     h  j   (   1  m   )      +     ∑  l      b  l   (   2  m   )     h  l    (   2  m   )             +     ∑   l  p       W   (   3  t   )     h  l   (   2  t   )     h  p   (  3  )       +     ∑   l  p       W   (   3  m   )     h  l   (   2  m   )     h  p   (  3  )      +     ∑  p      b  p   (  3  )     h  p   (  3  )               P    superscript  𝐯  m    superscript  𝐯  t   θ     fragments    subscript     superscript  𝐡    2  m     superscript  𝐡    2  t     superscript  𝐡  3     P   fragments  normal-(   superscript  𝐡    2  m    normal-,   superscript  𝐡    2  t    normal-,   superscript  𝐡  3   normal-)    fragments  normal-(   subscript    superscript  𝐡    1  m     P   fragments  normal-(   subscript  𝐯  m   normal-,   superscript  𝐡    1  m    normal-|   superscript  𝐡    2  m    normal-)   normal-)    fragments  normal-(   subscript    superscript  𝐡    1  t     P   fragments  normal-(   superscript  𝐯  t   normal-,   superscript  𝐡    1  t    normal-|   superscript  𝐡    2  t    normal-)   normal-)       missing-subexpression    fragments     1     subscript  𝒵  M   θ     subscript   𝐡   exp   fragments  normal-(   subscript     k  j     superscript   subscript  W    k  j      1  t     superscript   subscript  v  k   t    superscript   subscript  h  j     1  t         missing-subexpression         subscript     j  l       superscript   subscript  W    j  l      2  t     superscript   subscript  h  j     1  t     superscript   subscript  h  l     2  t         subscript   k      superscript   subscript  b  k   t    superscript   subscript  v  k   t       M    subscript   j      superscript   subscript  b  j     1  t     superscript   subscript  h  j     1  t         subscript   l      superscript   subscript  b  l     2  t     superscript   subscript  h  l     2  t          missing-subexpression         subscript   i      superscript     superscript   subscript  v  i   m    superscript   subscript  b  i   m    2     2   superscript  σ  2         subscript     i  j         superscript   subscript  v  i   m    subscript  σ  i     superscript   subscript  W    i  j      1  m     superscript   subscript  h  j     1  m          missing-subexpression         subscript     j  l       superscript   subscript  W    j  l      2  m     superscript   subscript  h  j     1  m     superscript   subscript  h  l     2  m         subscript   j      superscript   subscript  b  j     1  m     superscript   subscript  h  j     1  m        subscript   l      superscript   subscript  b  l     2  m     subscript  h  l     2  m         missing-subexpression         subscript     l  p       superscript  W    3  t     superscript   subscript  h  l     2  t     superscript   subscript  h  p   3        subscript     l  p       superscript  W    3  m     superscript   subscript  h  l     2  m     superscript   subscript  h  p   3       subscript   p      superscript   subscript  b  p   3    superscript   subscript  h  p   3         \begin{aligned}\displaystyle P(\mathbf{v}^{m},\mathbf{v}^{t};\theta)&%
 \displaystyle=\sum_{\mathbf{h}^{(2m)},\mathbf{h}^{(2t)},\mathbf{h}^{(3)}}P(%
 \mathbf{h}^{(2m)},\mathbf{h}^{(2t)},\mathbf{h}^{(3)})(\sum_{\mathbf{h}^{(1m)}}%
 P(\mathbf{v}_{m},\mathbf{h}^{(1m)}|\mathbf{h}^{(2m)}))(\sum_{\mathbf{h}^{(1t)}%
 }P(\mathbf{v}^{t},\mathbf{h}^{(1t)}|\mathbf{h}^{(2t)}))\\
 &\displaystyle=\frac{1}{\mathcal{Z}_{M}(\theta)}\sum_{\mathbf{h}}\mathrm{exp}(%
 \sum_{kj}W_{kj}^{(1t)}v_{k}^{t}h_{j}^{(1t)}\\
 &\displaystyle+\sum_{jl}W_{jl}^{(2t)}h_{j}^{(1t)}h_{l}^{(2t)}+\sum_{k}b_{k}^{t%
 }v_{k}^{t}+M\sum_{j}b_{j}^{(1t)}h_{j}^{(1t)}+\sum_{l}b_{l}^{(2t)}h_{l}^{(2t)}%
 \\
 &\displaystyle-\sum_{i}\frac{(v_{i}^{m}-b_{i}^{m})^{2}}{2\sigma^{2}}+\sum_{ij}%
 \frac{v_{i}^{m}}{\sigma_{i}}W_{ij}^{(1m)}h_{j}^{(1m)}\\
 &\displaystyle+\sum_{jl}W_{jl}^{(2m)}h_{j}^{(1m)}h_{l}^{(2m)}+\sum_{j}b_{j}^{(%
 1m)}h_{j}^{(1m)}+\sum_{l}b_{l}^{(2m)}h_{l}{(2m)}\\
 &\displaystyle+\sum_{lp}W^{(3t)}h_{l}^{(2t)}h_{p}^{(3)}+\sum_{lp}W^{(3m)}h_{l}%
 ^{(2m)}h_{p}^{(3)}+\sum_{p}b_{p}^{(3)}h_{p}^{(3)}\end{aligned}     The conditional distributions over the visible and hidden units are      p   (   h  j   (   1  m   )    =  1  |   𝐯  m   ,   𝐡   (   2  m   )    )   =  g   (   ∑   i  =  1   D    W   i  j    (   1  m   )      v  i  m    σ  i    +   ∑   l  =  1    F  2  m     W   j  l    (   2  m   )     h  l   (   2  m   )    +   b  j   (   1  m   )    )      fragments  p   fragments  normal-(   superscript   subscript  h  j     1  m     1  normal-|   superscript  𝐯  m   normal-,   superscript  𝐡    2  m    normal-)    g   fragments  normal-(   superscript   subscript     i  1    D    superscript   subscript  W    i  j      1  m       superscript   subscript  v  i   m    subscript  σ  i      superscript   subscript     l  1     superscript   subscript  F  2   m     superscript   subscript  W    j  l      2  m     superscript   subscript  h  l     2  m      superscript   subscript  b  j     1  m    normal-)     p(h_{j}^{(1m)}=1|\mathbf{v}^{m},\mathbf{h}^{(2m)})=g(\sum_{i=1}^{D}W_{ij}^{(1m%
 )}\frac{v_{i}^{m}}{\sigma_{i}}+\sum_{l=1}^{F_{2}^{m}}W_{jl}^{(2m)}h_{l}^{(2m)}%
 +b_{j}^{(1m)})         p   (   h  l   (   2  m   )    =  1  |   𝐡   (   1  m   )    ,   𝐡   (  3  )    )   =  g   (   ∑   j  =  1    F  1  m     W   j  l    (   2  m   )     h  j   (   1  m   )    +   ∑   p  =  1    F  3     W   l  p    (   3  m   )     h  p   (  3  )    +   b  l   (   2  m   )    )      fragments  p   fragments  normal-(   superscript   subscript  h  l     2  m     1  normal-|   superscript  𝐡    1  m    normal-,   superscript  𝐡  3   normal-)    g   fragments  normal-(   superscript   subscript     j  1     superscript   subscript  F  1   m     superscript   subscript  W    j  l      2  m     superscript   subscript  h  j     1  m      superscript   subscript     p  1     subscript  F  3     superscript   subscript  W    l  p      3  m     superscript   subscript  h  p   3     superscript   subscript  b  l     2  m    normal-)     p(h_{l}^{(2m)}=1|\mathbf{h}^{(1m)},\mathbf{h}^{(3)})=g(\sum_{j=1}^{F_{1}^{m}}W%
 _{jl}^{(2m)}h_{j}^{(1m)}+\sum_{p=1}^{F_{3}}W_{lp}^{(3m)}h_{p}^{(3)}+b_{l}^{(2m%
 )})         p   (   h  j   (   1  t   )    =  1  |   𝐯  t   ,   𝐡   (   2  t   )    )   =  g   (   ∑   k  =  1   K    W   k  l    (   1  t   )     v  k   (  t  )    +   ∑   l  =  1    F  2  t     W   j  l    (   2  t   )     h  l   (   2  t   )    +  M   b  j   (   1  t   )    )      fragments  p   fragments  normal-(   superscript   subscript  h  j     1  t     1  normal-|   superscript  𝐯  t   normal-,   superscript  𝐡    2  t    normal-)    g   fragments  normal-(   superscript   subscript     k  1    K    superscript   subscript  W    k  l      1  t     superscript   subscript  v  k   t     superscript   subscript     l  1     superscript   subscript  F  2   t     superscript   subscript  W    j  l      2  t     superscript   subscript  h  l     2  t     M   superscript   subscript  b  j     1  t    normal-)     p(h_{j}^{(1t)}=1|\mathbf{v}^{t},\mathbf{h}^{(2t)})=g(\sum_{k=1}^{K}W_{kl}^{(1t%
 )}v_{k}^{(t)}+\sum_{l=1}^{F_{2}^{t}}W_{jl}^{(2t)}h_{l}^{(2t)}+Mb_{j}^{(1t)})         p   (   h  l   (   2  t   )    =  1  |   𝐡   (   1  t   )    ,   𝐡   (  3  )    )   =  g   (   ∑   j  =  1    F  1  t     W   j  l    (   2  t   )     h  j   (   1  t   )    +   ∑   p  =  1    F  3     W   l  p    (   3  t   )     h  p   (  3  )    +   b  l   (   2  t   )    )      fragments  p   fragments  normal-(   superscript   subscript  h  l     2  t     1  normal-|   superscript  𝐡    1  t    normal-,   superscript  𝐡  3   normal-)    g   fragments  normal-(   superscript   subscript     j  1     superscript   subscript  F  1   t     superscript   subscript  W    j  l      2  t     superscript   subscript  h  j     1  t      superscript   subscript     p  1     subscript  F  3     superscript   subscript  W    l  p      3  t     superscript   subscript  h  p   3     superscript   subscript  b  l     2  t    normal-)     p(h_{l}^{(2t)}=1|\mathbf{h}^{(1t)},\mathbf{h}^{(3)})=g(\sum_{j=1}^{F_{1}^{t}}W%
 _{jl}^{(2t)}h_{j}^{(1t)}+\sum_{p=1}^{F_{3}}W_{lp}^{(3t)}h_{p}^{(3)}+b_{l}^{(2t%
 )})         p   (   h  p   3  )    =  1  |   𝐡   (  2  )    )   =  g   (   ∑   l  =  1    F  2  m     W   l  p    (   3  m   )     h  l   (   2  m   )    +   ∑   l  =  1    F  2  t     W   l  p    (   3  t   )     h  l   (   2  t   )    +   b  p   (  3  )    )      fragments  p   fragments  normal-(   superscript   subscript  h  p    fragments  3  normal-)     1  normal-|   superscript  𝐡  2   normal-)    g   fragments  normal-(   superscript   subscript     l  1     superscript   subscript  F  2   m     superscript   subscript  W    l  p      3  m     superscript   subscript  h  l     2  m      superscript   subscript     l  1     superscript   subscript  F  2   t     superscript   subscript  W    l  p      3  t     superscript   subscript  h  l     2  t      superscript   subscript  b  p   3   normal-)     p(h_{p}^{3)}=1|\mathbf{h}^{(2)})=g(\sum_{l=1}^{F_{2}^{m}}W_{lp}^{(3m)}h_{l}^{(%
 2m)}+\sum_{l=1}^{F_{2}^{t}}W_{lp}^{(3t)}h_{l}^{(2t)}+b_{p}^{(3)})         p   (   v   i  k   t   =  1  |   𝐡   (   1  t   )    )   =    exp   (     ∑   j  =  1    F  1  t      h  j   (   1  t   )     W   j  k    (   1  t   )      +   b  k  t    )      ∑   q  =  1   K    exp   (     ∑   j  =  1    F  1  t      h  j   (   1  t   )     W   j  q    (   1  t   )      +   b  k  t    )         fragments  p   fragments  normal-(   superscript   subscript  v    i  k    t    1  normal-|   superscript  𝐡    1  t    normal-)        exp      superscript   subscript     j  1     superscript   subscript  F  1   t       superscript   subscript  h  j     1  t     superscript   subscript  W    j  k      1  t       superscript   subscript  b  k   t       superscript   subscript     q  1    K     exp      superscript   subscript     j  1     superscript   subscript  F  1   t       superscript   subscript  h  j     1  t     superscript   subscript  W    j  q      1  t       superscript   subscript  b  k   t         p(v_{ik}^{t}=1|\mathbf{h}^{(1t)})=\frac{\mathrm{exp}(\sum_{j=1}^{F_{1}^{t}}h_{%
 j}^{(1t)}W_{jk}^{(1t)}+b_{k}^{t})}{\sum_{q=1}^{K}\mathrm{exp}(\sum_{j=1}^{F_{1%
 }^{t}}h_{j}^{(1t)}W_{jq}^{(1t)}+b_{k}^{t})}         p   (   v  i  m   |   𝐡   (   1  m   )    )   ∼  𝒩   (   σ  i    ∑   j  =  1    F  1  m     W   i  j    (   1  m   )     h  j   (   1  m   )    +   b  i  m   ,   σ  i  2   )      fragments  p   fragments  normal-(   superscript   subscript  v  i   m   normal-|   superscript  𝐡    1  m    normal-)   similar-to  N   fragments  normal-(   subscript  σ  i    superscript   subscript     j  1     superscript   subscript  F  1   m     superscript   subscript  W    i  j      1  m     superscript   subscript  h  j     1  m      superscript   subscript  b  i   m   normal-,   superscript   subscript  σ  i   2   normal-)     p(v_{i}^{m}|\mathbf{h}^{(1m)})\sim\mathcal{N}(\sigma_{i}\sum_{j=1}^{F_{1}^{m}}%
 W_{ij}^{(1m)}h_{j}^{(1m)}+b_{i}^{m},\sigma_{i}^{2})     Inference and Learning  Exact maximum likelihood learning in this model is intractable, but approximate learning of DBMs can be carried out by using a variational approach, where mean-field inference is used to estimate data-dependent expectations and an MCMC based stochastic approximation procedure is used to approximate the model’s expected sufficient statistics. 7  Application  Multimodal Deep Boltzmann Machines is successfully used in classification and missing data retrieval. The classification accuracy of Multimodal Deep Boltzmann Machine outperforms Support vector machine , Latent Dirichlet allocation and Deep belief network , when models are tested on data with both image-text modalities or with single modality. Multimodal Deep Boltzmann Machine is also able to predict the missing modality given the observed ones with reasonably good precision.  See also   Hopfield network  Deep belief network  Boltzmann machine  Restricted Boltzmann machine  Markov Random Field  Markov chain Monte Carlo   References        "  Category:Artificial neural networks     ↩  ↩  ↩  ↩  ↩  ↩  ↩     