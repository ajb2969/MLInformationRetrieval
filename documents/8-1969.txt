   ADALINE      ADALINE   (Figure)  Learning inside a single layer ADALINE   ADALINE ( Adaptive Linear Neuron or later Adaptive Linear Element ) is an early single-layer artificial neural network and the name of the physical device that implemented this network. 1 2 3 4 5 The network uses memistors . It was developed by Professor Bernard Widrow and his graduate student Ted Hoff at Stanford University in 1960. It is based on the McCulloch–Pitts neuron . It consists of a weight, a bias and a summation function.  The difference between Adaline and the standard ( McCulloch–Pitts ) perceptron is that in the learning phase the weights are adjusted according to the weighted sum of the inputs (the net). In the standard perceptron, the net is passed to the activation ( transfer ) function and the function's output is used for adjusting the weights.  There also exists an extension known as Madaline .  Definition  Adaline is a multiple layer neural network with multiple nodes where each node accepts multiple inputs and generates one output. Given the following variables:as   x is the input vector  w is the weight vector  n is the number of inputs     θ   θ   \theta   some constant  y is the output of the model   then we find that the output is    y  =     ∑   j  =  1   n     x  j    w  j     +  θ       y      superscript   subscript     j  1    n      subscript  x  j    subscript  w  j     θ     y=\sum_{j=1}^{n}x_{j}w_{j}+\theta   . If we further assume that        x   n  +  1    =  1       subscript  x    n  1    1    x_{n+1}=1          w   n  +  1    =  θ       subscript  w    n  1    θ    w_{n+1}=\theta      then the output further reduces to the dot product of x and w    y  =   x  ⋅  w       y   normal-⋅  x  w     y=x\cdot w     Learning algorithm  Let us assume:      η   η   \eta   is the learning rate (some positive constant)     y   y   y   is the output of the model     o   o   o   is the target (desired) output   then the weights are updated as follows    w  ←   w  +   η   (   o  -  y   )   x       normal-←  w    w    η    o  y   x      w\leftarrow w+\eta(o-y)x   . The ADALINE converges to the least squares error which is    E  =    (   o  -  y   )   2       E   superscript    o  y   2     E=(o-y)^{2}   . 6 This update rule is in fact the stochastic gradient descent update for linear regression . 7  References  External links     "  Category:Artificial neural networks     ↩  Youtube: widrowlms: Science in Action ↩  1960: An adaptive "ADALINE" neuron using chemical "memistors" ↩  Youtube: widrowlms: The LMS algorithm and ADALINE. Part I - The LMS algorithm ↩  Youtube: widrowlms: The LMS algorithm and ADALINE. Part II - ADALINE and memistor ADALINE ↩  ↩  ↩     