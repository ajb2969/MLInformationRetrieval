   Kaczmarz method      Kaczmarz method   The Kaczmarz method or Kaczmarz's algorithm is an iterative algorithm for solving linear equation systems      A  x   =  b        A  x   b    Ax=b   . It was first discovered by the Polish mathematician Stefan Kaczmarz , 1 and was rediscovered in the field of image reconstruction from projections by Richard Gordon , Robert Bender, and Gabor Herman in 1970, where it is called the Algebraic Reconstruction Technique (ART). 2 ART includes the positivity constraint, making it nonlinear. 3  The Kaczmarz method is applicable to any linear system of equations, but its computational advantage relative to other methods depends on the system being sparse . It has been demonstrated to be superior, in some biomedical imaging applications, to other methods such as the filtered backprojection method. 4  It has many applications ranging from computed tomography (CT) to signal processing . It can be obtained also by applying to the hyperplanes, described by the linear system, the method of successive projections onto convex sets (POCS). 5 6  Algorithm 1: Kaczmarz algorithm  Let     A  x   =  b        A  x   b    Ax=b   be a linear system, let   m   m   m   the number of rows of A,    a  i     subscript  a  i    a_{i}   be the   i   i   i   th row of complex-valued matrix   A   A   A   , and let    x  0     subscript  x  0    x_{0}   be arbitrary complex-valued initial approximation to the solution of     A  x   =  b        A  x   b    Ax=b   . For    k  =   0  ,  1  ,  ‚Ä¶       k   0  1  normal-‚Ä¶     k=0,1,...   compute:       x   k  +  1    =    x  k   +      b  i   -   ‚ü®   a  i   ,   x  k   ‚ü©      ‚à•   a  i   ‚à•   2      a  i   ¬Ø          subscript  x    k  1       subscript  x  k          subscript  b  i     subscript  a  i    subscript  x  k      superscript   norm   subscript  a  i    2     normal-¬Ø   subscript  a  i        x_{k+1}=x_{k}+\frac{b_{i}-\langle a_{i},x_{k}\rangle}{\lVert a_{i}\rVert^{2}}%
 \overline{a_{i}}     where    i  =    k   mod   m  +  1        i   modulo  k    m  1      i=k\,\bmod\,m+1   and     a  i   ¬Ø     normal-¬Ø   subscript  a  i     \overline{a_{i}}   denotes complex conjugation of    a  i     subscript  a  i    a_{i}   .  If the linear system is consistent,    x  k     subscript  x  k    x_{k}   converges to the minimum-norm solution, provided that the iterations start with the zero vector.  A more general algorithm can be defined using a relaxation parameter    Œª  k     subscript  Œª  k    \lambda_{k}          x   k  +  1    =    x  k   +    Œª  k      b  i   -   ‚ü®   a  i   ,   x  k   ‚ü©      ‚à•   a  i   ‚à•   2      a  i   ¬Ø          superscript  x    k  1       superscript  x  k      subscript  Œª  k        subscript  b  i     subscript  a  i    superscript  x  k      superscript   norm   subscript  a  i    2     normal-¬Ø   subscript  a  i        x^{k+1}=x^{k}+\lambda_{k}\frac{b_{i}-\langle a_{i},x^{k}\rangle}{\lVert a_{i}%
 \rVert^{2}}\overline{a_{i}}     There are versions of the method that converge to a regularized weighted least squares solution when applied to a system of inconsistent equations and, at least as far as initial behavior is concerned, at a lesser cost than other iterative methods, such as the conjugate gradient method . 7  Algorithm 2: Randomized Kaczmarz algorithm  Recently, a randomized version of the Kaczmarz method for overdetermined linear systems was introduced by Strohmer and Vershynin 8 in which the i -th equation is selected randomly with probability proportional to     ‚à•   a  i   ‚à•   2     superscript   norm   subscript  a  i    2    \lVert a_{i}\rVert^{2}   .  This method can be seen as a particular case of stochastic gradient descent . 9  Under such circumstances    x  k     subscript  x  k    x_{k}   converges exponentially fast to the solution of     A  x   =  b        A  x   b    Ax=b   , and the rate of convergence depends only on the scaled condition number     Œ∫   (  A  )       Œ∫  A    \kappa(A)   .  Theorem  Let   x   x   x   be the solution of     A  x   =  b        A  x   b    Ax=b   . Then Algorithm 1 converges to   x   x   x   in expectation, with the average error:        E    ‚à•    x  k   -  x   ‚à•   2    ‚â§     (   1  -   Œ∫    (  A  )    -  2      )   k   ‚ãÖ    ‚à•    x  0   -  x   ‚à•   2     .        E   superscript   norm     subscript  x  k   x    2     normal-‚ãÖ   superscript    1    Œ∫   superscript  A    2      k    superscript   norm     subscript  x  0   x    2      E{\lVert x_{k}-x\rVert^{2}}\leq(1-\kappa(A)^{-2})^{k}\cdot{\lVert x_{0}-x%
 \rVert^{2}}.     Proof  We have        ‚àë   j  =  1   m     |   ‚ü®  z  ,   a  j   ‚ü©   |   2    ‚â•      ‚à•  z  ‚à•   2     ‚à•   A   -  1    ‚à•   2     (  1  )          superscript   subscript     j  1    m    superscript     z   subscript  a  j     2        superscript   norm  z   2    superscript   norm   superscript  A    1     2    1     \sum_{j=1}^{m}|\langle z,a_{j}\rangle|^{2}\geq\frac{\lVert z\rVert^{2}}{\lVert
 A%
 ^{-1}\rVert^{2}}\qquad\qquad\qquad\qquad(1)   for all     z  ‚àà   ‚ÑÇ  n    .      z   superscript  ‚ÑÇ  n     z\in\mathbb{C}^{n}.     Using the fact that      ‚à•  A  ‚à•   2   =    ‚àë   j  =  1   m     ‚à•   a  j   ‚à•   2         superscript   norm  A   2     superscript   subscript     j  1    m    superscript   norm   subscript  a  j    2      {\lVert A\rVert^{2}}=\sum_{j=1}^{m}{\lVert a_{j}\rVert^{2}}   we can write (1) as            ‚àë   j  =  1   m         ‚à•   a  j   ‚à•   2     ‚à•  A  ‚à•   2       |   ‚ü®  z  ,     a  j    ‚à•   a  j   ‚à•     ‚ü©   |   2     ‚â•    Œ∫    (  A  )    -  2      ‚à•  z  ‚à•   2     (  2  )               superscript   subscript     j  1    m        superscript   norm   subscript  a  j    2    superscript   norm  A   2     superscript     z     subscript  a  j    norm   subscript  a  j       2        Œ∫   superscript  A    2     superscript   norm  z   2    2       \begin{aligned}\displaystyle\sum_{j=1}^{m}\frac{{\lVert a_{j}\rVert^{2}}}{%
 \lVert A\rVert^{2}}\left|\left\langle z,\frac{a_{j}}{\lVert a_{j}\rVert}\right%
 \rangle\right|^{2}\geq\kappa(A)^{-2}{\lVert z\rVert^{2}}\qquad\qquad\qquad%
 \qquad(2)\end{aligned}   for all     z  ‚àà   ‚ÑÇ  n    .      z   superscript  ‚ÑÇ  n     z\in\mathbb{C}^{n}.     The main point of the proof is to view the left hand side in (2) as an expectation of some random variable. Namely, recall that the solution space of the    j  -   t  h       j    t  h     j-th   equation of     A  x   =  b        A  x   b    Ax=b   is the hyperplane    y  :    ‚ü®  y  ,   a  j   ‚ü©   =   b  j       normal-:  y     y   subscript  a  j     subscript  b  j      {y:\langle y,a_{j}\rangle=b_{j}}   , whose normal is      a  j     ‚à•   a  j   ‚à•   2    .       subscript  a  j    superscript   norm   subscript  a  j    2     \frac{a_{j}}{\lVert a_{j}\rVert^{2}}.   DeÔ¨Åne a random vector Z whose values are the normals to all the equations of     A  x   =  b        A  x   b    Ax=b   , with probabilities as in our algorithm:      Z  =    a  j    ‚à•   a  j   ‚à•        Z     subscript  a  j    norm   subscript  a  j       Z=\frac{a_{j}}{\lVert a_{j}\rVert}   with probability         ‚à•   a  j   ‚à•   2     ‚à•  A  ‚à•   2    j   =  1   ,   ‚ãØ  ,  m      formulae-sequence        superscript   norm   subscript  a  j    2    superscript   norm  A   2    j   1    normal-‚ãØ  m     \frac{\lVert a_{j}\rVert^{2}}{\lVert A\rVert^{2}}\qquad\qquad\qquad j=1,\cdots,m     Then (2) says that          ùîº    |   ‚ü®  z  ,  Z  ‚ü©   |   2    ‚â•    Œ∫    (  A  )    -  2      ‚à•  z  ‚à•   2     (  3  )               ùîº   superscript     z  Z    2       Œ∫   superscript  A    2     superscript   norm  z   2    3       \begin{aligned}\displaystyle\mathbb{E}|\langle z,Z\rangle|^{2}\geq\kappa(A)^{-%
 2}{\lVert z\rVert^{2}}\qquad\qquad(3)\end{aligned}   for all     z  ‚àà   ‚ÑÇ  n    .      z   superscript  ‚ÑÇ  n     z\in\mathbb{C}^{n}.     The orthogonal projection   P   P   P   onto the solution space of a random equation of     A  x   =  b        A  x   b    Ax=b   is given by      P  z   =   z  -    ‚ü®   z  -  x   ,  Z  ‚ü©   Z     .        P  z     z       z  x   Z   Z      Pz=z-\langle z-x,Z\rangle Z.     Now we are ready to analyze our algorithm. We want to show that the error     ‚à•    x  k   -  x   ‚à•   2     superscript   norm     subscript  x  k   x    2    {\lVert x_{k}-x\rVert^{2}}   reduces at each step in average (conditioned on the previous steps) by at least the factor of     (   1  -   Œ∫    (  A  )    -  2      )   .      1    Œ∫   superscript  A    2       (1-\kappa(A)^{-2}).   The next approximation    x  k     subscript  x  k    x_{k}   is computed from    x   k  -  1      subscript  x    k  1     x_{k-1}   as      x  k   =    P  k    x   k  -  1      ,       subscript  x  k      subscript  P  k    subscript  x    k  1       x_{k}=P_{k}x_{k-1},   where     P  1   ,   P  2   ,  ‚ãØ      subscript  P  1    subscript  P  2   normal-‚ãØ    P_{1},P_{2},\cdots   are independent realizations of the random projection    P  .    P   P.   The vector     x   k  -  1    -   x  k        subscript  x    k  1     subscript  x  k     x_{k-1}-x_{k}   is in the kernel of     P  k   .     subscript  P  k    P_{k}.   It is orthogonal to the solution space of the equation onto which    P  k     subscript  P  k    P_{k}   projects, which contains the vector     x  k   -  x       subscript  x  k   x    x_{k}-x   (recall that   x   x   x   is the solution to all equations). The orthogonality of these two vectors then yields       ‚à•    x  k   -  x   ‚à•   2   =     ‚à•    x   k  -  1    -  x   ‚à•   2   -    ‚à•    x   k  -  1    -   x  k    ‚à•   2     .       superscript   norm     subscript  x  k   x    2      superscript   norm     subscript  x    k  1    x    2    superscript   norm     subscript  x    k  1     subscript  x  k     2      {\lVert x_{k}-x\rVert^{2}}={\lVert x_{k-1}-x\rVert^{2}}-{\lVert x_{k-1}-x_{k}%
 \rVert^{2}}.   To complete the proof, we have to bound     ‚à•    x   k  -  1    -   x  k    ‚à•   2     superscript   norm     subscript  x    k  1     subscript  x  k     2    {\lVert x_{k-1}-x_{k}\rVert^{2}}   from below. By the deÔ¨Ånition of    x  k     subscript  x  k    x_{k}   , we have     ‚à•    x   k  -  1    -   x  k    ‚à•   =   ‚ü®    x   k  -  1    -  x   ,   Z  k   ‚ü©        norm     subscript  x    k  1     subscript  x  k         subscript  x    k  1    x    subscript  Z  k      {\lVert x_{k-1}-x_{k}\rVert}=\langle x_{k-1}-x,Z_{k}\rangle     where     Z  1   ,   Z  2   ,  ‚ãØ      subscript  Z  1    subscript  Z  2   normal-‚ãØ    Z_{1},Z_{2},\cdots   are independent realizations of the random vector    Z  .    Z   Z.     Thus       ‚à•    x  k   -  x   ‚à•   2   =    (   1  -    |   ‚ü®     x   k  -  1    -  x    ‚à•    x   k  -  1    -  x   ‚à•    ,   Z  k   ‚ü©   |   2    )     ‚à•    x   k  -  1    -  x   ‚à•   2     .       superscript   norm     subscript  x  k   x    2       1   superscript          subscript  x    k  1    x    norm     subscript  x    k  1    x      subscript  Z  k     2     superscript   norm     subscript  x    k  1    x    2      {\lVert x_{k}-x\rVert^{2}}=\left(1-\left|\left\langle\frac{x_{k-1}-x}{\lVert x%
 _{k-1}-x\rVert},Z_{k}\right\rangle\right|^{2}\right){\lVert x_{k-1}-x\rVert^{2%
 }}.     Now we take the expectation of both sides conditional upon the choice of the random vectors     Z  1   ,  ‚ãØ  ,   Z   k  -  1        subscript  Z  1   normal-‚ãØ   subscript  Z    k  1      Z_{1},\cdots,Z_{k-1}   (hence we Ô¨Åx the choice of the random projections     P  1   ,  ‚ãØ  ,   P   k  -  1        subscript  P  1   normal-‚ãØ   subscript  P    k  1      P_{1},\cdots,P_{k-1}   and thus the random vectors     x  1   ,  ‚ãØ  ,   x   k  -  1        subscript  x  1   normal-‚ãØ   subscript  x    k  1      x_{1},\cdots,x_{k-1}   and we average over the random vector    Z  k     subscript  Z  k    Z_{k}   ). Then         ùîº    Z  1   ,  ‚ãØ  ,   Z   k  -  1        ‚à•    x  k   -  x   ‚à•   2    =    (   1  -    ùîº    Z  1   ,  ‚ãØ  ,   Z   k  -  1        |   ‚ü®     x   k  -  1    -  x    ‚à•    x   k  -  1    -  x   ‚à•    ,   Z  k   ‚ü©   |   2     )     ‚à•    x   k  -  1    -  x   ‚à•   2     .         subscript  ùîº    subscript  Z  1   normal-‚ãØ   subscript  Z    k  1       superscript   norm     subscript  x  k   x    2        1     subscript  ùîº    subscript  Z  1   normal-‚ãØ   subscript  Z    k  1       superscript          subscript  x    k  1    x    norm     subscript  x    k  1    x      subscript  Z  k     2      superscript   norm     subscript  x    k  1    x    2      \mathbb{E}_{{Z_{1},\cdots,Z_{k-1}}}{\lVert x_{k}-x\rVert^{2}}=\left(1-\mathbb{%
 E}_{{Z_{1},\cdots,Z_{k-1}}}\left|\left\langle\frac{x_{k-1}-x}{\lVert x_{k-1}-x%
 \rVert},Z_{k}\right\rangle\right|^{2}\right){\lVert x_{k-1}-x\rVert^{2}}.     By (3) and the independence,         ùîº    Z  1   ,  ‚ãØ  ,   Z   k  -  1        ‚à•    x  k   -  x   ‚à•   2    ‚â§    (   1  -   Œ∫    (  A  )    -  2      )     ‚à•    x   k  -  1    -  x   ‚à•   2     .         subscript  ùîº    subscript  Z  1   normal-‚ãØ   subscript  Z    k  1       superscript   norm     subscript  x  k   x    2        1    Œ∫   superscript  A    2       superscript   norm     subscript  x    k  1    x    2      \mathbb{E}_{{Z_{1},\cdots,Z_{k-1}}}{\lVert x_{k}-x\rVert^{2}}\leq(1-\kappa(A)^%
 {-2}){\lVert x_{k-1}-x\rVert^{2}}.     Taking the full expectation of both sides, we conclude that        ùîº    ‚à•    x  k   -  x   ‚à•   2    ‚â§    (   1  -   Œ∫    (  A  )    -  2      )   ùîº    ‚à•    x   k  -  1    -  x   ‚à•   2     .        ùîº   superscript   norm     subscript  x  k   x    2        1    Œ∫   superscript  A    2      ùîº   superscript   norm     subscript  x    k  1    x    2      \mathbb{E}{\lVert x_{k}-x\rVert^{2}}\leq(1-\kappa(A)^{-2})\mathbb{E}{\lVert x_%
 {k-1}-x\rVert^{2}}.      ‚ñ†   normal-‚ñ†   \blacksquare     The superiority of this selection was illustrated with the reconstruction of a bandlimited function from its nonuniformly spaced sampling values. However, it has been pointed out 10 that the reported success by Strohmer and Vershynin depends on the specific choices that were made there in translating the underlying problem, whose geometrical nature is to find a common point of a set of hyperplanes , into a system of algebraic equations. There will always be legitimate algebraic representations of the underlying problem for which the selection method in 11 will perform in an inferior manner. 12 13 14  Notes  References                  External links   1 A randomized Kaczmarz algorithm with exponential convergence  2 Comments on the randomized Kaczmarz method   "  Category:Medical imaging  Category:Numerical linear algebra     ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  See  and references therein. ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©     ‚Ü©     