


Kaczmarz method




Kaczmarz method

The Kaczmarz method or Kaczmarz's algorithm is an iterative algorithm for solving linear equation systems

 
 . It was first discovered by the Polish mathematician Stefan Kaczmarz,1 and was rediscovered in the field of image reconstruction from projections by Richard Gordon, Robert Bender, and Gabor Herman in 1970, where it is called the Algebraic Reconstruction Technique (ART).2 ART includes the positivity constraint, making it nonlinear.3
The Kaczmarz method is applicable to any linear system of equations, but its computational advantage relative to other methods depends on the system being sparse. It has been demonstrated to be superior, in some biomedical imaging applications, to other methods such as the filtered backprojection method.4
It has many applications ranging from computed tomography (CT) to signal processing. It can be obtained also by applying to the hyperplanes, described by the linear system, the method of successive projections onto convex sets (POCS).56
Algorithm 1: Kaczmarz algorithm
Let 
 
 
 
  be a linear system, let 
 
 
 
  the number of rows of A, 
 
 
 
  be the 
 
 
 
 th row of complex-valued matrix 
 
 
 
 , and let 
 
 
 
  be arbitrary complex-valued initial approximation to the solution of 
 
 
 
 . For 
 
 
 
  compute:



where 
 
 
 
  and 
 
 
 
  denotes complex conjugation of 
 
 
 
 .
If the linear system is consistent, 
 
 
 
  converges to the minimum-norm solution, provided that the iterations start with the zero vector.
A more general algorithm can be defined using a relaxation parameter 
 
 




There are versions of the method that converge to a regularized weighted least squares solution when applied to a system of inconsistent equations and, at least as far as initial behavior is concerned, at a lesser cost than other iterative methods, such as the conjugate gradient method.7
Algorithm 2: Randomized Kaczmarz algorithm
Recently, a randomized version of the Kaczmarz method for overdetermined linear systems was introduced by Strohmer and Vershynin8 in which the i-th equation is selected randomly with probability proportional to 
 
 
 
 .
This method can be seen as a particular case of stochastic gradient descent .9
Under such circumstances 
 
 
 
  converges exponentially fast to the solution of 
 
 
 
 , and the rate of convergence depends only on the scaled condition number

 
 .
Theorem
Let 
 
 
 
  be the solution of 
 
 
 
 . Then Algorithm 1 converges to 
 
 
 
  in expectation, with the average error:



Proof
We have


 
  for all 
 
 

Using the fact that 
 
 
 
  we can write (1) as


 
  for all 
 
 

The main point of the proof is to view the left hand side in (2) as an expectation of some random variable. Namely, recall that the solution space of the 
 
 
 
  equation of 
 
 
 
  is the hyperplane 
 
 
 
 , whose normal is 
 
 
 
  Deﬁne a random vector Z whose values are the normals to all the equations of 
 
 
 
 , with probabilities as in our algorithm:


 
  with probability 
 
 

Then (2) says that


 
 
  for all 
 
 

The orthogonal projection 
 
 
 
  onto the solution space of a random equation of 
 
 
 
  is given by 
 
 

Now we are ready to analyze our algorithm. We want to show that the error 
 
 
 
 
  reduces at each step in average (conditioned on the previous steps) by at least the factor of 
 
 
 
  The next approximation 
 
 
 
  is computed from 
 
 
 
  as 
 
 
 
  where 
 
 
 
 
  are independent realizations of the random projection 
 
 
 
  The vector 
 
 
 
  is in the kernel of 
 
 
 
  It is orthogonal to the solution space of the equation onto which 
 
 
 
  projects, which contains the vector 
 
 
 
 
  (recall that 
 
 
 
  is the solution to all equations). The orthogonality of these two vectors then yields 
 
 
 
  To complete the proof, we have to bound 
 
 
 
  from below. By the deﬁnition of 
 
 
 
 , we have 
 
 

where 
 
 
 
  are independent realizations of the random vector 
 
 

Thus 
 
 

Now we take the expectation of both sides conditional upon the choice of the random vectors 
 
 
 
  (hence we ﬁx the choice of the random projections 
 
 
 
 
  and thus the random vectors 
 
 
 
  and we average over the random vector 
 
 
 
 ). Then



By (3) and the independence,



Taking the full expectation of both sides, we conclude that




The superiority of this selection was illustrated with the reconstruction of a bandlimited function from its nonuniformly spaced sampling values. However, it has been pointed out10 that the reported success by Strohmer and Vershynin depends on the specific choices that were made there in translating the underlying problem, whose geometrical nature is to find a common point of a set of hyperplanes, into a system of algebraic equations. There will always be legitimate algebraic representations of the underlying problem for which the selection method in 11 will perform in an inferior manner.121314
Notes
References
















External links

1 A randomized Kaczmarz algorithm with exponential convergence
2 Comments on the randomized Kaczmarz method

"
Category:Medical imaging Category:Numerical linear algebra









See  and references therein.











