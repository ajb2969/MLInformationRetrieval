   Well-behaved statistic      Well-behaved statistic   A well-behaved statistic is a term sometimes used in the theory of statistics to describe part of a procedure. This usage is broadly similar to the use of well-behaved in more general mathematics. It is essentially an assumption about the formulation of an estimation procedure (which entails the specification of an estimator or statistic ) that is used to avoid giving extensive details about what conditions need to hold. In particular it means that the statistic is not an unusual one in the context being studied. Due to this, the meaning attributed to well-behaved statistic may vary from context to context.  The present article is mainly concerned with the context of data mining procedures applied to statistical inference and, in particular, to the group of computationally intensive procedure that have been called algorithmic inference .  Algorithmic inference  In algorithmic inference , the property of a statistic that is of most relevance is the pivoting step which allows to transference of probability-considerations from the sample distribution to the distribution of the parameters representing the population distribution in such a way that the conclusion of this statistical inference step is compatible with the sample actually observed.  By default, capital letters (such as U , X ) will denote random variables and small letters ( u , x ) their corresponding realizations and with gothic letters (such as    𝔘  ,  𝔛     𝔘  𝔛    \mathfrak{U},\mathfrak{X}   ) the domain where the variable takes specifications. Facing a sample    𝒙  =   {   x  1   ,  …  ,   x  m   }       𝒙    subscript  x  1   normal-…   subscript  x  m      \boldsymbol{x}=\{x_{1},\ldots,x_{m}\}   , given a sampling mechanism     (   g  θ   ,  Z  )      subscript  g  θ   Z    (g_{\theta},Z)   , with   θ   θ   \theta   scalar, for the random variable X , we have       𝒙  =   {    g  θ    (   z  1   )    ,  …  ,    g  θ    (   z  m   )    }    .      𝒙      subscript  g  θ    subscript  z  1    normal-…     subscript  g  θ    subscript  z  m       \boldsymbol{x}=\{g_{\theta}(z_{1}),\ldots,g_{\theta}(z_{m})\}.   The sampling mechanism    (   g  θ   ,  𝒛  )      subscript  g  θ   𝒛    (g_{\theta},\boldsymbol{z})   , of the statistic s , as a function ? of    {   x  1   ,  …  ,   x  m   }      subscript  x  1   normal-…   subscript  x  m     \{x_{1},\ldots,x_{m}\}   with specifications in   𝔖   𝔖   \mathfrak{S}   , has an explaining function defined by the master equation:       s  =   ρ   (   x  1   ,  …  ,   x  m   )    =   ρ   (    g  θ    (   z  1   )    ,  …  ,    g  θ    (   z  m   )    )    =   h   (  θ  ,   z  1   ,  …  ,   z  m   )     ,   (  1  )      formulae-sequence      s    ρ    subscript  x  1   normal-…   subscript  x  m            ρ      subscript  g  θ    subscript  z  1    normal-…     subscript  g  θ    subscript  z  m             h   θ   subscript  z  1   normal-…   subscript  z  m       1    s=\rho(x_{1},\ldots,x_{m})=\rho(g_{\theta}(z_{1}),\ldots,g_{\theta}(z_{m}))=h(%
 \theta,z_{1},\ldots,z_{m}),\qquad\qquad\qquad(1)     for suitable seeds    𝒛  =   {   z  1   ,  …  ,   z  m   }       𝒛    subscript  z  1   normal-…   subscript  z  m      \boldsymbol{z}=\{z_{1},\ldots,z_{m}\}   and parameter ?  Well-behaved  In order to derive the distribution law of the parameter T , compatible with   𝒙   𝒙   \boldsymbol{x}   , the statistic must obey some technical properties. Namely, a statistic s is said to be well-behaved if it satisfies the following three statements:   monotonicity . A uniformly monotone relation exists between s and ? for any fixed seed    {   z  1   ,  …  ,   z  m   }      subscript  z  1   normal-…   subscript  z  m     \{z_{1},\ldots,z_{m}\}   – so as to have a unique solution of (1);  well-defined . On each observed s the statistic is well defined for every value of ?, i.e. any sample specification     {   x  1   ,  …  ,   x  m   }   ∈   𝔛  m         subscript  x  1   normal-…   subscript  x  m     superscript  𝔛  m     \{x_{1},\ldots,x_{m}\}\in\mathfrak{X}^{m}   such that     ρ   (   x  1   ,  …  ,   x  m   )    =  s        ρ    subscript  x  1   normal-…   subscript  x  m     s    \rho(x_{1},\ldots,x_{m})=s   has a probability density different from 0 – so as to avoid considering a non-surjective mapping from    𝔛  m     superscript  𝔛  m    \mathfrak{X}^{m}   to   𝔖   𝔖   \mathfrak{S}   , i.e. associating via   s   s   s   to a sample    {   x  1   ,  …  ,   x  m   }      subscript  x  1   normal-…   subscript  x  m     \{x_{1},\ldots,x_{m}\}   a ? that could not generate the sample itself;  local sufficiency .    {    θ  ˘   1   ,  …  ,    θ  ˘   N   }      subscript   normal-˘  θ   1   normal-…   subscript   normal-˘  θ   N     \{\breve{\theta}_{1},\ldots,\breve{\theta}_{N}\}   constitutes a true T sample for the observed s , so that the same probability distribution can be attributed to each sampled value. Now,      θ  ˘   j   =    h   -  1     (  s  ,    z  ˘   1  j   ,  …  ,    z  ˘   m  j   )         subscript   normal-˘  θ   j      superscript  h    1     s   superscript   subscript   normal-˘  z   1   j   normal-…   superscript   subscript   normal-˘  z   m   j       \breve{\theta}_{j}=h^{-1}(s,\breve{z}_{1}^{j},\ldots,\breve{z}_{m}^{j})   is a solution of (1) with the seed    {    z  ˘   1  j   ,  …  ,    z  ˘   m  j   }      superscript   subscript   normal-˘  z   1   j   normal-…   superscript   subscript   normal-˘  z   m   j     \{\breve{z}_{1}^{j},\ldots,\breve{z}_{m}^{j}\}   . Since the seeds are equally distributed, the sole caveat comes from their independence or, conversely from their dependence on ? itself. This check can be restricted to seeds involved by s , i.e. this drawback can be avoided by requiring that the distribution of    {    Z  1   ,  …  ,   Z  m    |   S  =  s   }     conditional-set    subscript  Z  1   normal-…   subscript  Z  m      S  s     \{Z_{1},\ldots,Z_{m}|S=s\}   is independent of ?. An easy way to check this property is by mapping seed specifications into    x  i     subscript  x  i    x_{i}   s specifications. The mapping of course depends on ?, but the distribution of    {    X  1   ,  …  ,   X  m    |   S  =  s   }     conditional-set    subscript  X  1   normal-…   subscript  X  m      S  s     \{X_{1},\ldots,X_{m}|S=s\}   will not depend on ?, if the above seed independence holds – a condition that looks like a local sufficiency of the statistic S .   Example  For instance, for both the Bernoulli distribution with parameter p and the exponential distribution with parameter ? the statistic     ∑   i  =  1   m    x  i       superscript   subscript     i  1    m    subscript  x  i     \sum_{i=1}^{m}x_{i}   is well-behaved. The satisfaction of the above three properties is straightforward when looking at both explaining functions      g  p    (  u  )    =  1         subscript  g  p   u   1    g_{p}(u)=1   if    u  ≤  p      u  p    u\leq p   , 0 otherwise in the case of the Bernoulli random variable, and      g  λ    (  u  )    =   -   log   u  /  λ            subscript  g  λ   u         u  λ       g_{\lambda}(u)=-\log u/\lambda   for the Exponential random variable, giving rise to statistics       s  p   =    ∑   i  =  1   m     I   [  0  ,  p  ]     (   u  i   )          subscript  s  p     superscript   subscript     i  1    m      subscript  I   0  p     subscript  u  i       s_{p}=\sum_{i=1}^{m}I_{[0,p]}(u_{i})   and        s  λ   =   -    1  λ     ∑   i  =  1   m    log   u  i        .       subscript  s  λ         1  λ     superscript   subscript     i  1    m      subscript  u  i         s_{\lambda}=-\frac{1}{\lambda}\sum_{i=1}^{m}\log u_{i}.     Vice versa , in the case of X following a continuous uniform distribution on    [  0  ,  A  ]     0  A    [0,A]   the same statistics do not meet the second requirement. For instance, the observed sample    {  c  ,   c  /  2   ,   c  /  3   }     c    c  2     c  3     \{c,c/2,c/3\}   gives     s  A  ′   =    11  /  6   c        subscript   superscript  s  normal-′   A       11  6   c     s^{\prime}_{A}=11/6c   . But the explaining function of this X is      g  a    (  u  )    =   u  a          subscript  g  a   u     u  a     g_{a}(u)=ua   . Hence a master equation     s  A   =    ∑   i  =  1   m     u  i   a         subscript  s  A     superscript   subscript     i  1    m      subscript  u  i   a      s_{A}=\sum_{i=1}^{m}u_{i}a   would produce with a U sample    {  0.8  ,  0.8  ,  0.8  }     0.8  0.8  0.8    \{0.8,0.8,0.8\}   and a solution     a  ˘   =   0.76  c        normal-˘  a     0.76  c     \breve{a}=0.76c   . This conflicts with the observed sample since the first observed value should result greater than the right extreme of the X range. The statistic     s  A   =   max   {   x  1   ,  …  ,   x  m   }         subscript  s  A      subscript  x  1   normal-…   subscript  x  m      s_{A}=\max\{x_{1},\ldots,x_{m}\}   is well-behaved in this case.  Analogously, for a random variable X following the Pareto distribution with parameters K and A (see Pareto example for more detail of this case),       s  1   =    ∑   i  =  1   m    log   x  i          subscript  s  1     superscript   subscript     i  1    m      subscript  x  i       s_{1}=\sum_{i=1}^{m}\log x_{i}   and       s  2   =    min   i  =   1  ,  …  ,  m      {   x  i   }         subscript  s  2     subscript     i   1  normal-…  m      subscript  x  i      s_{2}=\min_{i=1,\ldots,m}\{x_{i}\}   can be used as joint statistics for these parameters.  As a general statement that holds under weak conditions, sufficient statistics are well-behaved with respect to the related parameters. The table below gives sufficient / Well-behaved statistics for the parameters of some of the most commonly used probability distributions.      Common distribution laws together with related sufficient and well-behaved statistics.   Distribution   Definition of density function   Sufficient/Well-behaved statistic     Uniform discrete        f   (  x  ;  n  )    =    1  /  n    I   {  1  ,  2  ,  …  ,  n  }     (  x  )          f   x  n        1  n    subscript  I   1  2  normal-…  n    x     f(x;n)=1/nI_{\{1,2,\ldots,n\}}(x)           s  n   =    max  i    x  i         subscript  s  n     subscript   i    subscript  x  i      s_{n}=\max_{i}x_{i}        Bernoulli        f   (  x  ;  p  )    =    p  x     (   1  -  p   )    1  -  x     I   {  0  ,  1  }     (  x  )          f   x  p       superscript  p  x    superscript    1  p     1  x     subscript  I   0  1    x     f(x;p)=p^{x}(1-p)^{1-x}I_{\{0,1\}}(x)           s  P   =    ∑   i  =  1   m    x  i         subscript  s  P     superscript   subscript     i  1    m    subscript  x  i      s_{P}=\sum_{i=1}^{m}x_{i}        Binomial        f   (  x  ;  n  ,  p  )    =    (      n      x      )    p  x     (   1  -  p   )    n  -  x     I   0  ,  1  ,  …  ,  n     (  x  )          f   x  n  p       binomial  n  x    superscript  p  x    superscript    1  p     n  x     subscript  I   0  1  normal-…  n    x     f(x;n,p)={\left({{n}\atop{x}}\right)}p^{x}(1-p)^{n-x}I_{0,1,\ldots,n}(x)           s  P   =    ∑   i  =  1   m    x  i         subscript  s  P     superscript   subscript     i  1    m    subscript  x  i      s_{P}=\sum_{i=1}^{m}x_{i}        Geometric        f   (  x  ;  p  )    =   p    (   1  -  p   )   x    I   {  0  ,  1  ,  …  }     (  x  )          f   x  p      p   superscript    1  p   x    subscript  I   0  1  normal-…    x     f(x;p)=p(1-p)^{x}I_{\{0,1,\ldots\}}(x)           s  P   =    ∑   i  =  1   m    x  i         subscript  s  P     superscript   subscript     i  1    m    subscript  x  i      s_{P}=\sum_{i=1}^{m}x_{i}        Poisson        f   (  x  ;  μ  )    =      e   -   μ  x      μ  x    /   x  !     I   {  0  ,  1  ,  …  }     (  x  )          f   x  μ           superscript  normal-e      μ  x      superscript  μ  x      x     subscript  I   0  1  normal-…    x     f(x;\mu)=\mathrm{e}^{-\mu x}\mu^{x}/x!I_{\{0,1,\ldots\}}(x)           s  M   =    ∑   i  =  1   m    x  i         subscript  s  M     superscript   subscript     i  1    m    subscript  x  i      s_{M}=\sum_{i=1}^{m}x_{i}        Uniform continuous        f   (  x  ;  a  ,  b  )    =    1  /   (   b  -  a   )     I   [  a  ,  b  ]     (  x  )          f   x  a  b        1    b  a     subscript  I   a  b    x     f(x;a,b)=1/(b-a)I_{[a,b]}(x)            s  A   =    min  i    x  i     ;    s  B   =    max  i    x  i        formulae-sequence     subscript  s  A     subscript   i    subscript  x  i        subscript  s  B     subscript   i    subscript  x  i       s_{A}=\min_{i}x_{i};s_{B}=\max_{i}x_{i}        Negative exponential        f   (  x  ;  λ  )    =   λ   e   -   λ  x      I   [  0  ,  ∞  ]     (  x  )          f   x  λ      λ   superscript  normal-e      λ  x      subscript  I   0     x     f(x;\lambda)=\lambda\mathrm{e}^{-\lambda x}I_{[0,\infty]}(x)           s  Λ   =    ∑   i  =  1   m    x  i         subscript  s  normal-Λ     superscript   subscript     i  1    m    subscript  x  i      s_{\Lambda}=\sum_{i=1}^{m}x_{i}        Pareto        f   (  x  ;  a  ,  k  )    =    a  k     (   x  k   )     -  a   -  1     I   [  k  ,  ∞  ]     (  x  )          f   x  a  k        a  k    superscript    x  k       a   1     subscript  I   k     x     f(x;a,k)=\frac{a}{k}\left(\frac{x}{k}\right)^{-a-1}I_{[k,\infty]}(x)            s  A   =    ∑   i  =  1   m    log   x  i      ;    s  K   =    min  i    x  i        formulae-sequence     subscript  s  A     superscript   subscript     i  1    m      subscript  x  i         subscript  s  K     subscript   i    subscript  x  i       s_{A}=\sum_{i=1}^{m}\log x_{i};s_{K}=\min_{i}x_{i}        Gaussian        f   (  x  ,  μ  ,  σ  )    =    1  /   (     2  π    σ   )     e   -    (   x  -   μ  2    )   /   (   2   σ  2    )             f   x  μ  σ        1        2  π    σ     superscript  normal-e        x   superscript  μ  2      2   superscript  σ  2          f(x,\mu,\sigma)=1/(\sqrt{2\pi}\sigma)\mathrm{e}^{-(x-\mu^{2})/(2\sigma^{2})}            s  M   =    ∑   i  =  1   m    x  i     ;    s  Σ   =     ∑   i  =  1   m     (    x  i   -   x  ¯    )   2         formulae-sequence     subscript  s  M     superscript   subscript     i  1    m    subscript  x  i        subscript  s  normal-Σ       superscript   subscript     i  1    m    superscript     subscript  x  i    normal-¯  x    2        s_{M}=\sum_{i=1}^{m}x_{i};s_{\Sigma}=\sqrt{\sum_{i=1}^{m}(x_{i}-\bar{x})^{2}}        Gamma        f   (  x  ;  r  ,  λ  )    =    λ  /  Γ    (  r  )     (   λ  x   )    r  -  1     e   -   λ  x      I   [  0  ,  ∞  ]     (  x  )          f   x  r  λ        λ  normal-Γ   r   superscript    λ  x     r  1     superscript  normal-e      λ  x      subscript  I   0     x     f(x;r,\lambda)=\lambda/\Gamma(r)(\lambda x)^{r-1}\mathrm{e}^{-\lambda x}I_{[0,%
 \infty]}(x)            s  Λ   =    ∑   i  =  1   m    x  i     ;    s  K   =    ∏   i  =  1   m    x  i        formulae-sequence     subscript  s  normal-Λ     superscript   subscript     i  1    m    subscript  x  i        subscript  s  K     superscript   subscript  product    i  1    m    subscript  x  i       s_{\Lambda}=\sum_{i=1}^{m}x_{i};s_{K}=\prod_{i=1}^{m}x_{i}        References      "  Category:Statistical inference   