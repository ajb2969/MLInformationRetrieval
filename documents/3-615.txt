   Normalizing constant      Normalizing constant   The concept of a normalizing constant arises in probability theory and a variety of other areas of mathematics .  Definition and examples  In probability theory , a normalizing constant is a constant by which an everywhere non-negative function must be multiplied so the area under its graph is 1, e.g., to make it a probability density function or a probability mass function . 1 2 For example, if we define        p   (  x  )    =   e   -    x  2   /  2      ,   x  ∈   (   -  ∞   ,  ∞  )       formulae-sequence      p  x    superscript  e       superscript  x  2   2        x            p(x)=e^{-x^{2}/2},x\in(-\infty,\infty)     we have         ∫   -  ∞   ∞    p   (  x  )   d  x    =    ∫   -  ∞   ∞      e   -    x  2   /  2      d  x    =    2   π      ,          superscript   subscript            p  x  d  x      superscript   subscript             superscript  e       superscript  x  2   2     d  x             2  π       \int_{-\infty}^{\infty}p(x)\,dx=\int_{-\infty}^{\infty}e^{-x^{2}/2}\,dx=\sqrt{%
 2\pi\,},     if we define function    φ   (  x  )       φ  x    \varphi(x)   as       φ   (  x  )    =    1    2   π      p   (  x  )    =    1    2   π       e   -    x  2   /  2              φ  x       1      2  π     p  x            1      2  π      superscript  e       superscript  x  2   2         \varphi(x)=\frac{1}{\sqrt{2\pi\,}}p(x)=\frac{1}{\sqrt{2\pi\,}}e^{-x^{2}/2}     so that        ∫   -  ∞   ∞    φ   (  x  )   d  x    =    ∫   -  ∞   ∞     1    2   π        e   -    x  2   /  2      d  x    =  1          superscript   subscript            φ  x  d  x      superscript   subscript              1      2  π      superscript  e       superscript  x  2   2     d  x         1     \int_{-\infty}^{\infty}\varphi(x)\,dx=\int_{-\infty}^{\infty}\frac{1}{\sqrt{2%
 \pi\,}}e^{-x^{2}/2}\,dx=1     Function    φ   (  x  )       φ  x    \varphi(x)   is a probability density function. 3 This is the density of the standard normal distribution . ( Standard , in this case, means the expected value is 0 and the variance is 1.)  And constant    1    2   π         1      2  π      \frac{1}{\sqrt{2\pi\,}}   is the normalizing constant of function    p   (  x  )       p  x    p(x)   .  Similarly,         ∑   n  =  0   ∞     λ  n    n  !     =   e  λ    ,        superscript   subscript     n  0         superscript  λ  n     n      superscript  e  λ     \sum_{n=0}^{\infty}\frac{\lambda^{n}}{n!}=e^{\lambda},     and consequently       f   (  n  )    =     λ  n    e   -  λ      n  !          f  n        superscript  λ  n    superscript  e    λ       n      f(n)=\frac{\lambda^{n}e^{-\lambda}}{n!}     is a probability mass function on the set of all nonnegative integers. 4 This is the probability mass function of the Poisson distribution with expected value λ.  Note that if the probability density function is a function of various parameters, so too will be its normalizing constant. The parametrised normalizing constant for the Boltzmann distribution plays a central role in statistical mechanics . In that context, the normalizing constant is called the partition function .  Bayes' theorem  Bayes' theorem says that the posterior probability measure is proportional to the product of the prior probability measure and the likelihood function . Proportional to implies that one must multiply or divide by a normalizing constant to assign measure 1 to the whole space, i.e., to get a probability measure. In a simple discrete case we have      P   (   H  0   |  D  )   =    P   (  D  |   H  0   )   P   (   H  0   )     P   (  D  )        fragments  P   fragments  normal-(   subscript  H  0   normal-|  D  normal-)       fragments  P   fragments  normal-(  D  normal-|   subscript  H  0   normal-)   P   fragments  normal-(   subscript  H  0   normal-)      P  D      P(H_{0}|D)=\frac{P(D|H_{0})P(H_{0})}{P(D)}     where P(H 0 ) is the prior probability that the hypothesis is true; P(D|H 0 ) is the conditional probability of the data given that the hypothesis is true, but given that the data are known it is the likelihood of the hypothesis (or its parameters) given the data; P(H 0 |D) is the posterior probability that the hypothesis is true given the data. P(D) should be the probability of producing the data, but on its own is difficult to calculate, so an alternative way to describe this relationship is as one of proportionality:      P   (   H  0   |  D  )   ∝  P   (  D  |   H  0   )   P   (   H  0   )   .     fragments  P   fragments  normal-(   subscript  H  0   normal-|  D  normal-)   proportional-to  P   fragments  normal-(  D  normal-|   subscript  H  0   normal-)   P   fragments  normal-(   subscript  H  0   normal-)   normal-.    P(H_{0}|D)\propto P(D|H_{0})P(H_{0}).     Since P(H|D) is a probability, the sum over all possible (mutually exclusive) hypotheses should be 1, leading to the conclusion that      P   (   H  0   |  D  )   =    P   (  D  |   H  0   )   P   (   H  0   )       ∑  i    P   (  D  |   H  i   )   P   (   H  i   )     .     fragments  P   fragments  normal-(   subscript  H  0   normal-|  D  normal-)       fragments  P   fragments  normal-(  D  normal-|   subscript  H  0   normal-)   P   fragments  normal-(   subscript  H  0   normal-)     fragments   subscript   i   P   fragments  normal-(  D  normal-|   subscript  H  i   normal-)   P   fragments  normal-(   subscript  H  i   normal-)     normal-.    P(H_{0}|D)=\frac{P(D|H_{0})P(H_{0})}{\displaystyle\sum_{i}P(D|H_{i})P(H_{i})}.     In this case, the reciprocal of the value      P   (  D  )   =   ∑  i   P   (  D  |   H  i   )   P   (   H  i   )      fragments  P   fragments  normal-(  D  normal-)     subscript   i   P   fragments  normal-(  D  normal-|   subscript  H  i   normal-)   P   fragments  normal-(   subscript  H  i   normal-)     P(D)=\sum_{i}P(D|H_{i})P(H_{i})\;     is the normalizing constant . 5 It can be extended from countably many hypotheses to uncountably many by replacing the sum by an integral.  Non-probabilistic uses  The Legendre polynomials are characterized by orthogonality with respect to the uniform measure on the interval [− 1, 1] and the fact that they are normalized so that their value at 1 is 1. The constant by which one multiplies a polynomial so its value at 1 is 1 is a normalizing constant.  Orthonormal functions are normalized such that       ⟨   f  i   ,   f  j   ⟩   =   δ   i  ,  j          subscript  f  i    subscript  f  j     subscript  δ   i  j      \langle f_{i},\,f_{j}\rangle=\,\delta_{i,j}   with respect to some inner product .  The constant 1/√2 is used to establish the hyperbolic functions cosh and sinh from the lengths of the adjacent and opposite sides of a hyperbolic triangle .  Notes  References   Continuous Distributions at Department of Mathematical Sciences: University of Alabama in Huntsville    "  Category:Probability theory  Category:1 (number)     Continuous Distributions at University of Alabama. ↩  Feller, 1968, p. 22. ↩  Feller, 1968, p. 174. ↩  Feller, 1968, p. 156. ↩  Feller, 1968, p. 124. ↩     