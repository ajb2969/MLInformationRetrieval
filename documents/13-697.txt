   Iterative proportional fitting      Iterative proportional fitting   The iterative proportional fitting procedure ( IPFP , also known as biproportional fitting in statistics, RAS algorithm 1 in economics and matrix raking or matrix scaling in computer science) is an iterative algorithm for estimating cell values of a contingency table such that the marginal totals remain fixed and the estimated table decomposes into an outer product .  First introduced by Deming and Stephan in 1940 2 (they proposed IPFP as an algorithm leading to a minimizer of the Pearson X-squared statistic , which it does not , 3 and even failed to prove convergence), it has seen various extensions and related research. A rigorous proof of convergence by means of differential geometry is due to Fienberg (1970). 4 He interpreted the family of contingency tables of constant crossproduct ratios as a particular ( IJ − 1)-dimensional manifold of constant interaction and showed that the IPFP is a fixed-point iteration on that manifold. Nevertheless, he assumed strictly positive observations. Generalization to tables with zero entries is still considered a hard and only partly solved problem.  An exhaustive treatment of the algorithm and its mathematical foundations can be found in the book of Bishop et al. (1975). 5 The first general proof of convergence, built on non-trivial measure theoretic theorems and entropy minimization, is due to Csiszár (1975). 6 Relatively new results on convergence and error behavior have been published by Pukelsheim and Simeone (2009) . 7 They proved simple necessary and sufficient conditions for the convergence of the IPFP for arbitrary two-way tables (i.e. tables with zero entries) by analysing an    L  1     subscript  L  1    L_{1}   -error function.  Other general algorithms can be modified to yield the same limit as the IPFP, for instance the Newton–Raphson method and the EM algorithm . In most cases, IPFP is preferred due to its computational speed, numerical stability and algebraic simplicity.  Algorithm 1 (classical IPFP)  Given a two-way ( I × J )-table of counts    (   x   i  j    )     subscript  x    i  j     (x_{ij})   , where the cell values are assumed to be Poisson or multinomially distributed, we wish to estimate a decomposition      m  ^    i  j    =    a  i    b  j         subscript   normal-^  m     i  j       subscript  a  i    subscript  b  j      \hat{m}_{ij}=a_{i}b_{j}   for all i and j such that    (    m  ^    i  j    )     subscript   normal-^  m     i  j     (\hat{m}_{ij})   is the maximum likelihood estimate (MLE) of the expected values    (   m   i  j    )     subscript  m    i  j     (m_{ij})   leaving the marginals     x   i  +    =    ∑  j     x   i  j           subscript  x   limit-from  i       subscript   j    subscript  x    i  j       \textstyle x_{i+}=\sum_{j}x_{ij}\,   and     x   +  j    =    ∑  i     x   i  j           subscript  x    j      subscript   i    subscript  x    i  j       \textstyle x_{+j}=\sum_{i}x_{ij}\,   fixed. The assumption that the table factorizes in such a manner is known as the model of independence (I-model). Written in terms of a log-linear model , we can write this assumption as      log    m   i  j     =   u  +   v  i   +   w  j   +   z   i  j            subscript  m    i  j       u   subscript  v  i    subscript  w  j    subscript  z    i  j       \log\ m_{ij}=u+v_{i}+w_{j}+z_{ij}   , where     m   i  j    :=   𝔼   (   x   i  j    )       assign   subscript  m    i  j      𝔼   subscript  x    i  j       m_{ij}:=\mathbb{E}(x_{ij})   ,      ∑  i    v  i    =    ∑  j    w  j    =  0          subscript   i    subscript  v  i      subscript   j    subscript  w  j         0     \sum_{i}v_{i}=\sum_{j}w_{j}=0   and the interaction term vanishes, that is     z   i  j    =  0       subscript  z    i  j    0    z_{ij}=0   for all i and j .  Choose initial values      m  ^    i  j    (  0  )    :=  1     assign   superscript   subscript   normal-^  m     i  j    0   1    \hat{m}_{ij}^{(0)}:=1   (different choices of initial values may lead to changes in convergence behavior), and for    η  ≥  1      η  1    \eta\geq 1   set        m  ^    i  j    (    2  η   -  1   )    =      m  ^    i  j    (    2  η   -  2   )     x   i  +       ∑   k  =  1   J     m  ^    i  k    (    2  η   -  2   )           superscript   subscript   normal-^  m     i  j        2  η   1         superscript   subscript   normal-^  m     i  j        2  η   2     subscript  x   limit-from  i        superscript   subscript     k  1    J    superscript   subscript   normal-^  m     i  k        2  η   2        \hat{m}_{ij}^{(2\eta-1)}=\frac{\hat{m}_{ij}^{(2\eta-2)}x_{i+}}{\sum_{k=1}^{J}%
 \hat{m}_{ik}^{(2\eta-2)}}            m  ^    i  j    (   2  η   )    =      m  ^    i  j    (    2  η   -  1   )     x   +  j       ∑   k  =  1   I     m  ^    k  j    (    2  η   -  1   )       .       superscript   subscript   normal-^  m     i  j      2  η         superscript   subscript   normal-^  m     i  j        2  η   1     subscript  x    j       superscript   subscript     k  1    I    superscript   subscript   normal-^  m     k  j        2  η   1        \hat{m}_{ij}^{(2\eta)}=\frac{\hat{m}_{ij}^{(2\eta-1)}x_{+j}}{\sum_{k=1}^{I}%
 \hat{m}_{kj}^{(2\eta-1)}}.     Notes:   Convergence does not depend on the actual distribution. Distributional assumptions are necessary for inferring that the limit     (    m  ^    i  j    )   :=    lim   η  →  ∞     (    m  ^    i  j    (  η  )    )       assign   subscript   normal-^  m     i  j      subscript    normal-→  η      subscript   superscript   normal-^  m   η     i  j       (\hat{m}_{ij}):=\lim_{\eta\rightarrow\infty}(\hat{m}^{(\eta)}_{ij})   is an MLE indeed.    IPFP can be manipulated to generate any positive marginals be replacing    x   i  +      subscript  x   limit-from  i      x_{i+}   by the desired row marginal    u  i     subscript  u  i    u_{i}   (analogously for the column marginals).    IPFP can be extended to fit the model of quasi-independence (Q-model), where     m   i  j    =  0       subscript  m    i  j    0    m_{ij}=0   is known a priori for     (  i  ,  j  )   ∈  S       i  j   S    (i,j)\in S   . Only the initial values have to be changed: Set      m  ^    i  j    (  0  )    =  0       superscript   subscript   normal-^  m     i  j    0   0    \hat{m}_{ij}^{(0)}=0   if     (  i  ,  j  )   ∈  S       i  j   S    (i,j)\in S   and 1 otherwise.   Algorithm 2 (factor estimation)  Assume the same setting as in the classical IPFP. Alternatively, we can estimate the row and column factors separately: Choose initial values      b  ^   j   (  0  )    :=  1     assign   superscript   subscript   normal-^  b   j   0   1    \hat{b}_{j}^{(0)}:=1   , and for    η  ≥  1      η  1    \eta\geq 1   set         a  ^   i   (  η  )    =    x   i  +      ∑  j     b  ^   j   (   η  -  1   )       ,       superscript   subscript   normal-^  a   i   η      subscript  x   limit-from  i       subscript   j    superscript   subscript   normal-^  b   j     η  1        \hat{a}_{i}^{(\eta)}=\frac{x_{i+}}{\sum_{j}\hat{b}_{j}^{(\eta-1)}},           b  ^   j   (  η  )    =    x   +  j      ∑  i     a  ^   i   (  η  )           superscript   subscript   normal-^  b   j   η      subscript  x    j      subscript   i    superscript   subscript   normal-^  a   i   η       \hat{b}_{j}^{(\eta)}=\frac{x_{+j}}{\sum_{i}\hat{a}_{i}^{(\eta)}}     Setting      m  ^    i  j    (   2  η   )    =     a  ^   i   (  η  )      b  ^   j   (  η  )          superscript   subscript   normal-^  m     i  j      2  η       superscript   subscript   normal-^  a   i   η    superscript   subscript   normal-^  b   j   η      \hat{m}_{ij}^{(2\eta)}=\hat{a}_{i}^{(\eta)}\hat{b}_{j}^{(\eta)}   , the two variants of the algorithm are mathematically equivalent (can be seen by formal induction).  Notes:   In matrix notation, we can write     (    m  ^    i  j    )   =    a  ^     b  ^   T         subscript   normal-^  m     i  j       normal-^  a    superscript   normal-^  b   T      (\hat{m}_{ij})=\hat{a}\hat{b}^{T}   , where     a  ^   =    (    a  ^   1   ,  …  ,    a  ^   I   )   T   =    lim   η  →  ∞      a  ^    (  η  )            normal-^  a    superscript    subscript   normal-^  a   1   normal-…   subscript   normal-^  a   I    T          subscript    normal-→  η      superscript   normal-^  a   η       \hat{a}=(\hat{a}_{1},\ldots,\hat{a}_{I})^{T}=\lim_{\eta\rightarrow\infty}\hat{%
 a}^{(\eta)}   and     b  ^   =    (    b  ^   1   ,  …  ,    b  ^   J   )   T   =    lim   η  →  ∞      b  ^    (  η  )            normal-^  b    superscript    subscript   normal-^  b   1   normal-…   subscript   normal-^  b   J    T          subscript    normal-→  η      superscript   normal-^  b   η       \hat{b}=(\hat{b}_{1},\ldots,\hat{b}_{J})^{T}=\lim_{\eta\rightarrow\infty}\hat{%
 b}^{(\eta)}   .  The factorization is not unique, since it is     m   i  j    =    a  i    b  j    =    (   γ   a  i    )    (    1  γ    b  j    )           subscript  m    i  j       subscript  a  i    subscript  b  j             γ   subscript  a  i        1  γ    subscript  b  j        m_{ij}=a_{i}b_{j}=(\gamma a_{i})(\frac{1}{\gamma}b_{j})   for all    γ  >  0      γ  0    \gamma>0   .  The factor totals remain constant, i.e.      ∑  i     a  ^   i   (  η  )     =    ∑  i     a  ^   i   (  1  )           subscript   i    superscript   subscript   normal-^  a   i   η      subscript   i    superscript   subscript   normal-^  a   i   1      \sum_{i}\hat{a}_{i}^{(\eta)}=\sum_{i}\hat{a}_{i}^{(1)}   for all    η  ≥  1      η  1    \eta\geq 1   and      ∑  j     b  ^   j   (  η  )     =    ∑  j     b  ^   j   (  0  )           subscript   j    superscript   subscript   normal-^  b   j   η      subscript   j    superscript   subscript   normal-^  b   j   0      \sum_{j}\hat{b}_{j}^{(\eta)}=\sum_{j}\hat{b}_{j}^{(0)}   for all    η  ≥  0      η  0    \eta\geq 0   .  To fit the Q-model, where     m   i  j    =  0       subscript  m    i  j    0    m_{ij}=0   a priori for     (  i  ,  j  )   ∈  S       i  j   S    (i,j)\in S   , set     δ   i  j    =  0       subscript  δ    i  j    0    \delta_{ij}=0   if (    i  ,  j  )  ∈  S     fragments  i  normal-,  j  normal-)   S    i,j)\in S   and     δ   i  j    =  1       subscript  δ    i  j    1    \delta_{ij}=1   otherwise. Then            a  ^   i   (  η  )    =    x   i  +       ∑  j      δ   i  j      b  ^   j   (   η  -  1   )        ,       superscript   subscript   normal-^  a   i   η      subscript  x   limit-from  i       subscript   j      subscript  δ    i  j     superscript   subscript   normal-^  b   j     η  1         \hat{a}_{i}^{(\eta)}=\frac{x_{i+}}{\sum_{j}\delta_{ij}\hat{b}_{j}^{(\eta-1)}},                b  ^   j   (  η  )    =    x   +  j       ∑  i      δ   i  j      a  ^   i   (  η  )            superscript   subscript   normal-^  b   j   η      subscript  x    j      subscript   i      subscript  δ    i  j     superscript   subscript   normal-^  a   i   η        \hat{b}_{j}^{(\eta)}=\frac{x_{+j}}{\sum_{i}\delta_{ij}\hat{a}_{i}^{(\eta)}}                m  ^    i  j    (   2  η   )    =    δ   i  j      a  ^   i   (  η  )      b  ^   j   (  η  )          superscript   subscript   normal-^  m     i  j      2  η       subscript  δ    i  j     superscript   subscript   normal-^  a   i   η    superscript   subscript   normal-^  b   j   η      \hat{m}_{ij}^{(2\eta)}=\delta_{ij}\hat{a}_{i}^{(\eta)}\hat{b}_{j}^{(\eta)}        Obviously, the I-model is a particular case of the Q-model.  Algorithm 3 (RAS)  The Problem: Let    M  :=   (   m   i  j    (  0  )    )   ∈   ℝ   I  ×  J         assign  M   subscript   superscript  m  0     i  j          superscript  ℝ    I  J       M:=(m^{(0)}_{ij})\in\mathbb{R}^{I\times J}   be the initial matrix with nonnegative entries,    u  ∈   ℝ  I       u   superscript  ℝ  I     u\in\mathbb{R}^{I}   a vector of specified row marginals (e.i. row sums) and    v  ∈   ℝ  J       v   superscript  ℝ  J     v\in\mathbb{R}^{J}   a vector of column marginals. We wish to compute a matrix     M  ^   =   (    m  ^    i  j    )   ∈   ℝ   I  ×  J           normal-^  M    subscript   normal-^  m     i  j          superscript  ℝ    I  J       \hat{M}=(\hat{m}_{ij})\in\mathbb{R}^{I\times J}   similar to M and consistent with the predefined marginals, meaning        m  ^    i  +    =    ∑   j  =  1   n     m  ^    i  j     =   u  i          subscript   normal-^  m    limit-from  i       superscript   subscript     j  1    n    subscript   normal-^  m     i  j           subscript  u  i      \hat{m}_{i+}=\sum_{j=1}^{n}\hat{m}_{ij}=u_{i}     and        m  ^    +  j    =    ∑   i  =  1   m     m  ^    i  j     =   v  j          subscript   normal-^  m     j      superscript   subscript     i  1    m    subscript   normal-^  m     i  j           subscript  v  j      \hat{m}_{+j}=\sum_{i=1}^{m}\hat{m}_{ij}=v_{j}     Define the diagonalization operator     d  i  a  g   :    ℝ  k   ⟶   ℝ   k  ×  k        normal-:    d  i  a  g    normal-⟶   superscript  ℝ  k    superscript  ℝ    k  k       diag:\mathbb{R}^{k}\longrightarrow\mathbb{R}^{k\times k}   , which produces a (diagonal) matrix with its input vector on the main diagonal and zero elsewhere. Then, for    η  ≥  0      η  0    \eta\geq 0   , set       M   (    2  η   +  1   )    =   diag   (   r   (   η  +  1   )    )    M   (   2  η   )          superscript  M      2  η   1      diag   superscript  r    η  1     superscript  M    2  η       M^{(2\eta+1)}=\text{diag}(r^{(\eta+1)})M^{(2\eta)}          M   (    2  η   +  2   )    =    M   (    2  η   +  1   )    diag   (   s   (   η  +  1   )    )         superscript  M      2  η   2       superscript  M      2  η   1    diag   superscript  s    η  1       M^{(2\eta+2)}=M^{(2\eta+1)}\text{diag}(s^{(\eta+1)})     where       r  i   η  +  1    =    u  i     ∑  j    m   i  j    (   2  η   )           superscript   subscript  r  i     η  1       subscript  u  i     subscript   j    superscript   subscript  m    i  j      2  η        r_{i}^{\eta+1}=\frac{u_{i}}{\sum_{j}m_{ij}^{(2\eta)}}          s  j   η  +  1    =    v  j     ∑  i    m   i  j    (    2  η   +  1   )           superscript   subscript  s  j     η  1       subscript  v  j     subscript   i    superscript   subscript  m    i  j        2  η   1        s_{j}^{\eta+1}=\frac{v_{j}}{\sum_{i}m_{ij}^{(2\eta+1)}}     Finally, we obtain      M  ^   =    lim   η  →  ∞     M   (  η  )      .       normal-^  M     subscript    normal-→  η      superscript  M  η      \hat{M}=\lim_{\eta\rightarrow\infty}M^{(\eta)}.     Discussion and comparison of the algorithms  Although RAS seems to be the solution of an entirely different problem, it is indeed identical to the classical IPFP. In practice, one would not implement actual matrix multiplication, since diagonal matrices are involved. Reducing the operations to the necessary ones, it can easily be seen that RAS does the same as IPFP. The vaguely demanded 'similarity' can be explained as follows: IPFP (and thus RAS) maintains the crossproduct ratios, e.i.          m   i  j    (  0  )     m   h  k    (  0  )       m   i  k    (  0  )     m   h  j    (  0  )      =       m   i  j    (  η  )     m   h  k    (  η  )       m   i  k    (  η  )     m   h  j    (  η  )        ∀  η    ≥   0  and  i   ≠  h   ,   j  ≠  k      formulae-sequence           subscript   superscript  m  0     i  j     subscript   superscript  m  0     h  k        subscript   superscript  m  0     i  k     subscript   superscript  m  0     h  j             subscript   superscript  m  η     i  j     subscript   superscript  m  η     h  k        subscript   superscript  m  η     i  k     subscript   superscript  m  η     h  j       for-all  η           0  and  i        h      j  k     \frac{m^{(0)}_{ij}m^{(0)}_{hk}}{m^{(0)}_{ik}m^{(0)}_{hj}}=\frac{m^{(\eta)}_{ij%
 }m^{(\eta)}_{hk}}{m^{(\eta)}_{ik}m^{(\eta)}_{hj}}\ \forall\ \eta\geq 0\text{ %
 and }i\neq h,\quad j\neq k     since      m   i  j    (  η  )    =    a  i   (  η  )     b  j   (  η  )      .       subscript   superscript  m  η     i  j       superscript   subscript  a  i   η    superscript   subscript  b  j   η      m^{(\eta)}_{ij}=a_{i}^{(\eta)}b_{j}^{(\eta)}.     This property is sometimes called structure conservation and directly leads to the geometrical interpretation of contingency tables and the proof of convergence in the seminal paper of Fienberg (1970).  Nevertheless, direct factor estimation (algorithm 2) is under all circumstances the best way to deal with IPF: Whereas classical IPFP needs        I  J   (   2  +  J   )    +   I  J   (   2  +  I   )     =     I  2   J   +   I   J  2    +   4  I   J             I  J    2  J      I  J    2  I          superscript  I  2   J     I   superscript  J  2      4  I  J      IJ(2+J)+IJ(2+I)=I^{2}J+IJ^{2}+4IJ\,     elementary operations in each iteration step (including a row and a column fitting step), factor estimation needs only        I   (   1  +  J   )    +   J   (   1  +  I   )     =    2  I  J   +  I  +   J            I    1  J      J    1  I         2  I  J   I  J     I(1+J)+J(1+I)=2IJ+I+J\,     operations being at least one order in magnitude faster than classical IPFP.  Existence and uniqueness of MLEs  Necessary and sufficient conditions for the existence and uniqueness of MLEs are complicated in the general case (see 8 ), but sufficient conditions for 2-dimensional tables are simple:   the marginals of the observed table do not vanish (that is,      x   i  +    >  0   ,    x   +  j    >  0      formulae-sequence     subscript  x   limit-from  i     0      subscript  x    j    0     x_{i+}>0,\ x_{+j}>0   ) and  the observed table is inseparable (e.i. the table does not permute to a block-diagonal shape).   If unique MLEs exist, IPFP exhibits linear convergence in the worst case (Fienberg 1970), but exponential convergence has also been observed (Pukelsheim and Simeone 2009). If a direct estimator (i.e. a closed form of    (    m  ^    i  j    )     subscript   normal-^  m     i  j     (\hat{m}_{ij})   ) exists, IPFP converges after 2 iterations. If unique MLEs do not exist, IPFP converges toward the so-called extended MLEs by design (Haberman 1974), but convergence may be arbitrarily slow and often computationally infeasible.  If all observed values are strictly positive, existence and uniqueness of MLEs and therefore convergence is ensured.  Goodness of fit  Checking if the assumption of independence is adequate, one uses the Pearson X-squared statistic       X  2   =    ∑   i  ,  j       (    x   i  j    -    m   i  j    ^    )   2     m   i  j    ^          superscript  X  2     subscript    i  j       superscript     subscript  x    i  j     normal-^   subscript  m    i  j      2    normal-^   subscript  m    i  j         X^{2}=\sum_{i,j}\frac{(x_{ij}-\hat{m_{ij}})^{2}}{\hat{m_{ij}}}     or alternatively the likelihood-ratio test ( G-test ) statistic      G  =   2    ∑   i  ,  j      x   i  j      log     x   i  j      m  ^    i  j             G    2    subscript    i  j       subscript  x    i  j         subscript  x    i  j     subscript   normal-^  m     i  j           G=2\sum_{i,j}x_{ij}\log\ \frac{x_{ij}}{\hat{m}_{ij}}   .  Both statistics are asymptotically     \Chi   r  2     subscript   superscript  \Chi  2   r    \Chi^{2}_{r}   -distributed, where    r  =    (   I  -  1   )    (   J  -  1   )        r      I  1     J  1      r=(I-1)(J-1)   is the number of degrees of freedom. That is, if the p-values     1  -     \Chi   r  2    (   X  2   )        1     subscript   superscript  \Chi  2   r    superscript  X  2      1-\Chi^{2}_{r}(X^{2})   and    1  -     \Chi   r  2    (  G  )        1     subscript   superscript  \Chi  2   r   G     1-\Chi^{2}_{r}(G)   are not too small (> 0.05 for instance), there is no indication to discard the hypothesis of independence.  Interpretation  If the rows correspond to different values of property A, and the columns correspond to different values of property B, and the hypothesis of independence is not discarded, the properties A and B are considered independent.  Example  Consider a table of observations (taken from the entry on contingency tables ):        | right-handed   left-handed   TOTAL     male   43   9   52     female   44   4   48     TOTAL   87   13   100      For executing the classical IPFP, we first initialize the matrix with ones, leaving the marginals untouched:        | right-handed   left-handed   TOTAL     male   1   1   52     female   1   1   48     TOTAL   87   13   100      Of course, the marginal sums do not correspond to the matrix anymore, but this is fixed in the next two iterations of IPFP. The first iteration deals with the row sums:        | right-handed   left-handed   TOTAL     male   26   26   52     female   24   24   48     TOTAL   87   13   100      Note that, by definition, the row sums always constitute a perfect match after odd iterations, as do the column sums for even ones. The subsequent iteration updates the matrix column-wise:        | right-handed   left-handed   TOTAL     male   45.24   6.76   52     female   41.76   6.24   48     TOTAL   87   13   100      Now, both row and column sums of the matrix match the given marginals again.  The p-value of this matrix approximates to     p   (   X  2   )    ≈  0.1824671        p   superscript  X  2    0.1824671    p(X^{2})\approx 0.1824671   , meaning: gender and left-handedness/right-handedness can be considered independent.  Implementation  The R package mipfp (currently in version 2.0) provides a multi-dimensional implementation of the traditional iterative proportional fitting procedure. 9 The package allows the updating of a N -dimensional array with respect to given target marginal distributions (which, in turn can be multi-dimensional).  Notes  "  Category:Categorical data  Category:Statistical algorithms     ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩     