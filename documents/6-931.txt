   Lanczos algorithm      Lanczos algorithm   The Lanczos algorithm is an iterative algorithm devised by Cornelius Lanczos 1 that is an adaptation of power methods to find the most useful eigenvalues and eigenvectors of an    n   t  h      superscript  n    t  h     n^{th}   order linear system with a limited number of operations,   m   m   m   , where   m   m   m   is much smaller than   n   n   n   . Although computationally efficient in principle, the method as initially formulated was not useful, due to its numerical instability. In 1970, Ojalvo and Newman 2 showed how to make the method numerically stable and applied it to the solution of very large engineering structures subjected to dynamic loading. This was achieved using a method for purifying the vectors to any degree of accuracy, which when not performed, produced a series of vectors that were highly contaminated by those associated with the lowest natural frequencies. In their original work, these authors also suggested how to select a starting vector (i.e. use a random number generator to select each element of the starting vector) and suggested an empirically determined method for determining   m   m   m   , the reduced number of vectors (i.e. it should be selected to be approximately 1 ½ times the number of accurate eigenvalues desired). Soon thereafter their work was followed by Paige 3 4 who also provided an error analysis. In 1988, Ojalvo 5 produced a more detailed history of this algorithm and an efficient eigenvalue error test. Currently, the method is widely used in a variety of technical fields and has seen a number of variations.  Power method for finding eigenvalues  The power method for finding the largest eigenvalue of a matrix    A    A   A\,   can be summarized by noting that if     x  0      subscript  x  0    x_{0}\,   is a random vector and     x   n  +  1    =   A    x  n          subscript  x    n  1      A   subscript  x  n      x_{n+1}=Ax_{n}\,   , then in the large   n   n   n   limit,     x  n   /   ∥   x  n   ∥        subscript  x  n    norm   subscript  x  n      x_{n}/\|x_{n}\|   approaches the normed eigenvector corresponding to the largest magnitude eigenvalue.  If    A  =   U    diag   (   σ  i   )      U  ′          A    U    diag   subscript  σ  i     superscript  U  normal-′       A=U\operatorname{diag}(\sigma_{i})U^{\prime}\,   is the eigendecomposition of    A    A   A\,   , then     A  n   =   U    diag   (   σ  i  n   )     U  ′          superscript  A  n     U    diag   superscript   subscript  σ  i   n     superscript  U  normal-′       A^{n}=U\operatorname{diag}(\sigma_{i}^{n})U^{\prime}   . As    n    n   n\,   gets very large, the diagonal matrix of eigenvalues will be dominated by whichever eigenvalue is largest (neglecting the case of two or more equally large eigenvalues, of course). As this happens,       x  n  *    x   n  +  1     /   x  n  *      x  n             superscript   subscript  x  n      subscript  x    n  1      superscript   subscript  x  n       subscript  x  n     x_{n}^{*}x_{n+1}/{x_{n}^{*}x_{n}}\,   will converge to the largest eigenvalue and     x  n   /   ∥   x  n   ∥        subscript  x  n    norm   subscript  x  n      x_{n}/\|x_{n}\|\,   to the associated eigenvector. If the largest eigenvalue is multiple, then     x  n      subscript  x  n    x_{n}\,   will converge to a vector in the subspace spanned by the eigenvectors associated with those largest eigenvalues. Having found the first eigenvector/value, one can then successively restrict the algorithm to the null space of the known eigenvectors to get the second largest eigenvector/values and so on.  In practice, this simple algorithm does not work very well for computing very many of the eigenvectors because any round-off error will tend to introduce slight components of the more significant eigenvectors back into the computation, degrading the accuracy of the computation. Pure power methods also can converge slowly, even for the first eigenvector.  Lanczos method  During the procedure of applying the power method, while getting the ultimate eigenvector     A   n  -  1    v       superscript  A    n  1    v    A^{n-1}v   , we also got a series of vectors        A  j   v   ,  j   =  0   ,   1  ,  ⋯  ,   n  -  2       formulae-sequence        superscript  A  j   v   j   0    1  normal-⋯    n  2      A^{j}v,\,j=0,1,\cdots,n-2   which were eventually discarded. As   n   n   n   is often taken to be quite large, this can result in a large amount of disregarded information. More advanced algorithms, such as Arnoldi's algorithm and the Lanczos algorithm, save this information and use the Gram–Schmidt process or Householder algorithm to reorthogonalize them into a basis spanning the Krylov subspace corresponding to the matrix   A   A   A   .  The algorithm  The Lanczos algorithm can be viewed as a simplified Arnoldi's algorithm in that it applies to Hermitian matrices . The   m   m   m   'th step of the algorithm transforms the matrix   A   A   A   into a tridiagonal matrix     T   m  m      subscript  T    m  m     T_{mm}   ; when   m   m   m   is equal to the dimension of   A   A   A   ,    T   m  m      subscript  T    m  m     T_{mm}   is similar to   A   A   A   .  Definitions  We hope to calculate the tridiagonal and symmetric matrix      T   m  m    =    V  m  *   A   V  m     .       subscript  T    m  m       superscript   subscript  V  m     A   subscript  V  m      T_{mm}=V_{m}^{*}AV_{m}.     The diagonal elements are denoted by     α  j   =   t   j  j         subscript  α  j    subscript  t    j  j      \alpha_{j}=t_{jj}   , and the off-diagonal elements are denoted by     β  j   =   t    j  -  1   ,  j         subscript  β  j    subscript  t     j  1   j      \beta_{j}=t_{j-1,j}   .  Note that     t    j  -  1   ,  j    =   t   j  ,   j  -  1          subscript  t     j  1   j     subscript  t   j    j  1       t_{j-1,j}=t_{j,j-1}   , due to its symmetry.  Iteration  (Note: Following these steps alone will not give you the correct eigenvalue and eigenvectors. More consideration must be applied to correct for the numerical errors. See the section Numerical stability in the following.)  There are in principle four ways to write the iteration procedure. Paige[1972] and other works show that the following procedure is the most numerically stable. 6 7      v  1   ←      normal-←   subscript  v  1   absent    v_{1}\leftarrow\,   random vector with norm 1.        v  0   ←   0      normal-←   subscript  v  0   0    v_{0}\leftarrow 0\,          β  1   ←   0      normal-←   subscript  β  1   0    \beta_{1}\leftarrow 0\,      for      j  =   1  ,  2  ,  ⋯  ,   m  -   1         j   1  2  normal-⋯    m  1      j=1,2,\cdots,m-1\,          w  j   ←   A    v  j        normal-←   subscript  w  j     A   subscript  v  j      w_{j}\leftarrow Av_{j}\,        α  j   ←    w  j   ⋅    v  j        normal-←   subscript  α  j    normal-⋅   subscript  w  j    subscript  v  j      \alpha_{j}\leftarrow w_{j}\cdot v_{j}\,        w  j   ←    w  j   -    α  j    v  j    -    β  j     v   j  -  1          normal-←   subscript  w  j      subscript  w  j      subscript  α  j    subscript  v  j       subscript  β  j    subscript  v    j  1        w_{j}\leftarrow w_{j}-\alpha_{j}v_{j}-\beta_{j}v_{j-1}\,        β   j  +  1    ←   ∥   w  j   ∥      normal-←   subscript  β    j  1     norm   subscript  w  j      \beta_{j+1}\leftarrow\left\|w_{j}\right\|\,        v   j  +  1    ←    w  j   /    β   j  +  1         normal-←   subscript  v    j  1       subscript  w  j    subscript  β    j  1       v_{j+1}\leftarrow w_{j}/\beta_{j+1}\,      endfor          w  m   ←   A    v  m        normal-←   subscript  w  m     A   subscript  v  m      w_{m}\leftarrow Av_{m}\,          α  m   ←    w  m   ⋅    v  m        normal-←   subscript  α  m    normal-⋅   subscript  w  m    subscript  v  m      \alpha_{m}\leftarrow w_{m}\cdot v_{m}\,      return  Here,    x  ⋅  y     normal-⋅  x  y    x\cdot y   represents the dot product of vectors   x   x   x   and   y   y   y   .  After the iteration, we get the    α  j     subscript  α  j    \alpha_{j}   and    β  j     subscript  β  j    \beta_{j}   which construct a tridiagonal matrix       T   m  m    =   (      α  1      β  2              0       β  2      α  2      β  3                    β  3      α  3     ⋱                  ⋱    ⋱     β   m  -  1                     β   m  -  1       α   m  -  1       β  m       0              β  m      α  m      )        subscript  T    m  m       subscript  α  1    subscript  β  2   absent  absent  absent  0     subscript  β  2    subscript  α  2    subscript  β  3   absent  absent  absent    absent   subscript  β  3    subscript  α  3   normal-⋱  absent  absent    absent  absent  normal-⋱  normal-⋱   subscript  β    m  1    absent    absent  absent  absent   subscript  β    m  1     subscript  α    m  1     subscript  β  m     0  absent  absent  absent   subscript  β  m    subscript  α  m       T_{mm}=\begin{pmatrix}\alpha_{1}&\beta_{2}&&&&0\\
 \beta_{2}&\alpha_{2}&\beta_{3}&&&\\
 &\beta_{3}&\alpha_{3}&\ddots&&\\
 &&\ddots&\ddots&\beta_{m-1}&\\
 &&&\beta_{m-1}&\alpha_{m-1}&\beta_{m}\\
 0&&&&\beta_{m}&\alpha_{m}\\
 \end{pmatrix}     The vectors    v  j     subscript  v  j    v_{j}   ( Lanczos vectors ) generated on the fly construct the transformation matrix       V  m   =   (   v  1   ,   v  2   ,  ⋯  ,   v  m   )        subscript  V  m     subscript  v  1    subscript  v  2   normal-⋯   subscript  v  m      V_{m}=\left(v_{1},v_{2},\cdots,v_{m}\right)   ,  which is useful for calculating the eigenvectors (see below). In practice, it could be saved after generation (but takes a lot of memory), or could be regenerated when needed, as long as one keeps the first vector    v  1     subscript  v  1    v_{1}   . At each iteration the algorithm executes a matrix-vector multiplication and 7n further ﬂoating point operations.  Solve for eigenvalues and eigenvectors  After the matrix    T   m  m      subscript  T    m  m     T_{mm}   is calculated, one can solve its eigenvalues    λ  i   (  m  )      superscript   subscript  λ  i   m    \lambda_{i}^{(m)}   and their corresponding eigenvectors    u  i   (  m  )      superscript   subscript  u  i   m    u_{i}^{(m)}   (for example, using the QR algorithm or Multiple Relatively Robust Representations (MRRR)). The eigenvalues and eigenvectors of   T   T   T   can be obtained in as little as    𝒪   (   m  2   )       𝒪   superscript  m  2     \mathcal{O}(m^{2})   work with MRRR; obtaining just the eigenvalues is much simpler and can be done in    𝒪   (   m  2   )       𝒪   superscript  m  2     \mathcal{O}(m^{2})   work with spectral bisection.  It can be proved that the eigenvalues are approximate eigenvalues of the original matrix   A   A   A   .  The Ritz eigenvectors    y  i     subscript  y  i    y_{i}   of   A   A   A   can be calculated by     y  i   =    V  m    u  i   (  m  )          subscript  y  i      subscript  V  m    superscript   subscript  u  i   m      y_{i}=V_{m}u_{i}^{(m)}   , where    V  m     subscript  V  m    V_{m}   is the transformation matrix whose column vectors are     v  1   ,   v  2   ,  ⋯  ,   v  m       subscript  v  1    subscript  v  2   normal-⋯   subscript  v  m     v_{1},v_{2},\cdots,v_{m}   .  Numerical stability  Stability means how much the algorithm will be affected (i.e. will it produce the approximate result close to the original one) if there are small numerical errors introduced and accumulated. Numerical stability is the central criterion for judging the usefulness of implementing an algorithm on a computer with roundoff.  For the Lanczos algorithm, it can be proved that with exact arithmetic , the set of vectors     v  1   ,   v  2   ,  ⋯  ,   v   m  +  1        subscript  v  1    subscript  v  2   normal-⋯   subscript  v    m  1      v_{1},v_{2},\cdots,v_{m+1}   constructs an orthonormal basis, and the eigenvalues/vectors solved are good approximations to those of the original matrix. However, in practice (as the calculations are performed in floating point arithmetic where inaccuracy is inevitable), the orthogonality is quickly lost and in some cases the new vector could even be linearly dependent on the set that is already constructed. As a result, some of the eigenvalues of the resultant tridiagonal matrix may not be approximations to the original matrix. Therefore, the Lanczos algorithm is not very stable.  Users of this algorithm must be able to find and remove those "spurious" eigenvalues. Practical implementations of the Lanczos algorithm go in three directions to fight this stability issue: 8 9   Prevent the loss of orthogonality  Recover the orthogonality after the basis is generated  After the good and "spurious" eigenvalues are all identified, remove the spurious ones.   Variations  Variations on the Lanczos algorithm exist where the vectors involved are tall, narrow matrices instead of vectors and the normalizing constants are small square matrices. These are called "block" Lanczos algorithms and can be much faster on computers with large numbers of registers and long memory-fetch times.  Many implementations of the Lanczos algorithm restart after a certain number of iterations. One of the most influential restarted variations is the implicitly restarted Lanczos method, 10 which is implemented in ARPACK . 11 This has led into a number of other restarted variations such as restarted Lanczos bidiagonalization. 12 Another successful restarted variation is the Thick-Restart Lanczos method, 13 which has been implemented in a software package called TRLan. 14  Nullspace over a finite field  In 1995, Peter Montgomery published an algorithm, based on the Lanczos algorithm, for finding elements of the nullspace of a large sparse matrix over GF(2) ; since the set of people interested in large sparse matrices over finite fields and the set of people interested in large eigenvalue problems scarcely overlap, this is often also called the block Lanczos algorithm without causing unreasonable confusion.  Applications  Lanczos algorithms are very attractive because the multiplication by    A    A   A\,   is the only large-scale linear operation. Since weighted-term text retrieval engines implement just this operation, the Lanczos algorithm can be applied efficiently to text documents (see Latent Semantic Indexing ). Eigenvectors are also important for large-scale ranking methods such as the HITS algorithm developed by Jon Kleinberg , or the PageRank algorithm used by Google.  Lanczos algorithms are also used in Condensed Matter Physics as a method for solving Hamiltonians of strongly correlated electron systems . 15  Lanczos algorithm has also been used in the formulation of the Levenberg-Marquardt or the Gauss-Newton optimization for solving nonlinear inverse problems (such as generating computational models of oil and gas reservoirs given observed production data). 16  Implementations  The NAG Library contains several routines 17 for the solution of large scale linear systems and eigenproblems which use the Lanczos algorithm.  MATLAB and GNU Octave come with ARPACK built-in. Both stored and implicit matrices can be analyzed through the eigs() function ( Matlab / Octave ).  A Matlab implementation of the Lanczos algorithm (note precision issues) is available as a part of the Gaussian Belief Propagation Matlab Package . The GraphLab 18 collaborative filtering library incorporates a large scale parallel implementation of the Lanczos algorithm (in C++) for multicore.  The PRIMME library also implements a Lanczos like algorithm.  References  External links   Golub and van Loan give very good descriptions of the various forms of Lanczos algorithms in their book Matrix Computations  Andrew Ng et al., an analysis of PageRank  Lanczos and conjugate gradient methods B. A. LaMacchia and A. M. Odlyzko, Solving Large Sparse Linear Systems Over Finite Fields.   "  Category:Numerical linear algebra     Lanczos, C. "An iteration method for the solution of the eigenvalue problem of linear differential and integral operators", J. Res. Nat’l Bur. Std. 45, 225-282 (1950). ↩  Ojalvo, I.U. and Newman, M.,"Vibration modes of large structures by an automatic matrix-reduction method", AIAA J., 8 (7), 1234-1239 (1970). ↩  Paige, C.C., "The computation of eigenvalues and eigenvectors of very large sparse matrices", the U. of London Ph.D. thesis (1971). ↩  Paige, C.C., "Computational variants of the Lanczos method for the eigenproblem", J. Inst. Maths Applics 10, 373-381 (1972). ↩  Ojalvo, I.U., "Origins and advantages of Lanczos vectors for large dynamic systems", Proc. 6th Modal Analysis Conference (IMAC), Kissimmee, FL, 489-494 (1988). ↩  ↩  ↩    ↩  ↩  ↩  ↩  ↩  ↩  History matching production data and uncertainty assessment with an efficient TSVD parameterization algorithm, Journal of Petroleum Science and Engineering http://www.sciencedirect.com/science/article/pii/S0920410513003227 ↩  ↩  GraphLab ↩     