<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="746">BLEU</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>BLEU</h1>
<hr/>

<p>__NOTOC__ <strong>BLEU</strong> (<strong>Bilingual Evaluation Understudy</strong>) is an algorithm for evaluating the quality of text which has been <a href="machine_translation" title="wikilink">machine-translated</a> from one <a href="natural_language" title="wikilink">natural language</a> to another. Quality is considered to be the correspondence between a machine's output and that of a human: "the closer a machine translation is to a professional human translation, the better it is" – this is the central idea behind BLEU. BLEU was one of the first <a href="Metric_(mathematics)" title="wikilink">metrics</a> to achieve a high <a class="uri" href="correlation" title="wikilink">correlation</a> with human judgements of quality, and remains one of the most popular automated and inexpensive metrics.</p>

<p>Scores are calculated for individual translated segments—generally sentences—by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole <a href="text_corpus" title="wikilink">corpus</a> to reach an estimate of the translation's overall quality. Intelligibility or grammatical correctness are not taken into account.</p>

<p>BLEU is designed to approximate human judgement at a corpus level, and performs badly if used to evaluate the quality of individual sentences.</p>

<p>BLEU’s output is always a number between 0 and 1. This value indicates how similar the candidate and reference texts are, with values closer to 1 representing more similar texts. However, few human translations will attain a score of 1. The candidate texts must be identical to a reference translation. For this reason, it is not necessary to attain a score of 1. Because there are more opportunities to match, adding additional reference translations will increase the BLEU score.</p>
<h2 id="algorithm">Algorithm</h2>

<p>BLEU uses a modified form of <a href="Precision_(information_retrieval)" title="wikilink">precision</a> to compare a candidate translation against multiple reference translations. The metric modifies simple precision since machine translation systems have been known to generate more words than are in a reference text. This is illustrated in the following example from Papineni et al. (2002),</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p>Example of poor machine translation output with high precision</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>Candidate</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>Reference 1</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>Reference 2</p></td>
</tr>
<tr class="odd">
</tr>
</tbody>
</table>

<p>Of the seven words in the candidate translation, all of them appear in the reference translations. Thus the candidate text is given a unigram precision of,</p>

<p>

<math display="block" id="BLEU:0">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mo>=</mo>
   <mfrac>
    <mi>m</mi>
    <msub>
     <mi>w</mi>
     <mi>t</mi>
    </msub>
   </mfrac>
   <mo>=</mo>
   <mfrac>
    <mn>7</mn>
    <mn>7</mn>
   </mfrac>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>P</ci>
     <apply>
      <divide></divide>
      <ci>m</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <ci>t</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <divide></divide>
      <cn type="integer">7</cn>
      <cn type="integer">7</cn>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P=\frac{m}{w_{t}}=\frac{7}{7}=1
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="BLEU:1">
 <semantics>
  <mpadded lspace="3.3pt" width="+3.3pt">
   <mi>m</mi>
  </mpadded>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ~{}m
  </annotation>
 </semantics>
</math>

 is number of words from the candidate that are found in the reference, and 

<math display="inline" id="BLEU:2">
 <semantics>
  <msub>
   <mpadded lspace="3.3pt" width="+3.3pt">
    <mi>w</mi>
   </mpadded>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ~{}w_{t}
  </annotation>
 </semantics>
</math>

 is the total number of words in the candidate. This is a perfect score, despite the fact that the candidate translation above retains little of the content of either of the references.</p>

<p>The modification that BLEU makes is fairly straightforward. For each word in the candidate translation, the algorithm takes its maximum total count, 

<math display="inline" id="BLEU:3">
 <semantics>
  <msub>
   <mpadded lspace="3.3pt" width="+3.3pt">
    <mi>m</mi>
   </mpadded>
   <mrow>
    <mi>m</mi>
    <mi>a</mi>
    <mi>x</mi>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>m</ci>
    <apply>
     <times></times>
     <ci>m</ci>
     <ci>a</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ~{}m_{max}
  </annotation>
 </semantics>
</math>


, in any of the reference translations. In the example above, the word "the" appears twice in reference 1, and once in reference 2. Thus 

<math display="inline" id="BLEU:4">
 <semantics>
  <mrow>
   <msub>
    <mpadded lspace="3.3pt" width="+3.3pt">
     <mi>m</mi>
    </mpadded>
    <mrow>
     <mi>m</mi>
     <mi>a</mi>
     <mi>x</mi>
    </mrow>
   </msub>
   <mo>=</mo>
   <mn>2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>m</ci>
     <apply>
      <times></times>
      <ci>m</ci>
      <ci>a</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ~{}m_{max}=2
  </annotation>
 </semantics>
</math>

.</p>

<p>For the candidate translation, the count 

<math display="inline" id="BLEU:5">
 <semantics>
  <msub>
   <mi>m</mi>
   <mi>w</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>m</ci>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m_{w}
  </annotation>
 </semantics>
</math>

 of each word is clipped to a maximum of 

<math display="inline" id="BLEU:6">
 <semantics>
  <msub>
   <mi>m</mi>
   <mrow>
    <mi>m</mi>
    <mi>a</mi>
    <mi>x</mi>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>m</ci>
    <apply>
     <times></times>
     <ci>m</ci>
     <ci>a</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m_{max}
  </annotation>
 </semantics>
</math>

 for that word. In this case, "the" has 

<math display="inline" id="BLEU:7">
 <semantics>
  <mrow>
   <msub>
    <mpadded lspace="3.3pt" width="+3.3pt">
     <mi>m</mi>
    </mpadded>
    <mi>w</mi>
   </msub>
   <mo>=</mo>
   <mn>7</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>m</ci>
     <ci>w</ci>
    </apply>
    <cn type="integer">7</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ~{}m_{w}=7
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="BLEU:8">
 <semantics>
  <mrow>
   <msub>
    <mpadded lspace="3.3pt" width="+3.3pt">
     <mi>m</mi>
    </mpadded>
    <mrow>
     <mi>m</mi>
     <mi>a</mi>
     <mi>x</mi>
    </mrow>
   </msub>
   <mo>=</mo>
   <mn>2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>m</ci>
     <apply>
      <times></times>
      <ci>m</ci>
      <ci>a</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ~{}m_{max}=2
  </annotation>
 </semantics>
</math>


, thus 

<math display="inline" id="BLEU:9">
 <semantics>
  <msub>
   <mpadded lspace="3.3pt" width="+3.3pt">
    <mi>m</mi>
   </mpadded>
   <mi>w</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>m</ci>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ~{}m_{w}
  </annotation>
 </semantics>
</math>

 is clipped to 2. 

<math display="inline" id="BLEU:10">
 <semantics>
  <msub>
   <mpadded lspace="3.3pt" width="+3.3pt">
    <mi>m</mi>
   </mpadded>
   <mi>w</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>m</ci>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ~{}m_{w}
  </annotation>
 </semantics>
</math>

 is then summed over all words in the candidate. This sum is then divided by the total number of words in the candidate translation. In the above example, the modified unigram precision score would be:</p>

<p>

<math display="block" id="BLEU:11">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mo>=</mo>
   <mfrac>
    <mn>2</mn>
    <mn>7</mn>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>P</ci>
    <apply>
     <divide></divide>
     <cn type="integer">2</cn>
     <cn type="integer">7</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P=\frac{2}{7}
  </annotation>
 </semantics>
</math>

</p>

<p>In practice, however, using individual words as the unit of comparison is not optimal. Instead, BLEU computes the same modified precision metric using <a href="n-gram" title="wikilink">n-grams</a>. The length which has the "highest correlation with monolingual human judgements" was found to be four. The unigram scores are found to account for the adequacy of the translation, how much information is retained. The longer 

<math display="inline" id="BLEU:12">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

-gram scores account for the fluency of the translation, or to what extent it reads like "good English".</p>

<p>Another problem with BLEU scores is that they tend to favor short translations, which can produce very high precision scores, even using modified precision. An example of a candidate translation for the same references as above might be:</p>
<dl>
<dd>the cat
</dd>
</dl>

<p>In this example, the modified unigram precision would be,</p>

<p>

<math display="block" id="BLEU:13">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mo>=</mo>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mn>2</mn>
    </mfrac>
    <mo>+</mo>
    <mfrac>
     <mn>1</mn>
     <mn>2</mn>
    </mfrac>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mn>2</mn>
    <mn>2</mn>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>P</ci>
     <apply>
      <plus></plus>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <divide></divide>
      <cn type="integer">2</cn>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P=\frac{1}{2}+\frac{1}{2}=\frac{2}{2}
  </annotation>
 </semantics>
</math>

</p>

<p>as the word 'the' and the word 'cat' appear once each in the candidate, and the total number of words is two. The modified bigram precision would be 

<math display="inline" id="BLEU:14">
 <semantics>
  <mrow>
   <mn>1</mn>
   <mo>/</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <cn type="integer">1</cn>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1/1
  </annotation>
 </semantics>
</math>

 as the bigram, "the cat" appears once in the candidate. It has been pointed out that precision is usually twinned with <a href="Recall_(information_retrieval)" title="wikilink">recall</a> to overcome this problem , as the unigram recall of this example would be 

<math display="inline" id="BLEU:15">
 <semantics>
  <mrow>
   <mn>2</mn>
   <mo>/</mo>
   <mn>6</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <cn type="integer">2</cn>
    <cn type="integer">6</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   2/6
  </annotation>
 </semantics>
</math>

 or 

<math display="inline" id="BLEU:16">
 <semantics>
  <mrow>
   <mn>2</mn>
   <mo>/</mo>
   <mn>7</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <cn type="integer">2</cn>
    <cn type="integer">7</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   2/7
  </annotation>
 </semantics>
</math>

. The problem being that as there are multiple reference translations, a bad translation could easily have an inflated recall, such as a translation which consisted of all the words in each of the references.</p>

<p>To produce a score for the whole corpus the modified precision scores for the segments are combined using the <a href="geometric_mean" title="wikilink">geometric mean</a> multiplied by a brevity penalty to prevent very short candidates from receiving too high a score. Let 

<math display="inline" id="BLEU:17">
 <semantics>
  <mi>r</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>r</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r
  </annotation>
 </semantics>
</math>

 be the total length of the reference corpus, and 

<math display="inline" id="BLEU:18">
 <semantics>
  <mi>c</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>c</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   c
  </annotation>
 </semantics>
</math>


 the total length of the translation corpus. If 

<math display="inline" id="BLEU:19">
 <semantics>
  <mrow>
   <mi>c</mi>
   <mo>≤</mo>
   <mi>r</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <ci>c</ci>
    <ci>r</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   c\leq r
  </annotation>
 </semantics>
</math>

, the brevity penalty applies, defined to be 

<math display="inline" id="BLEU:20">
 <semantics>
  <msup>
   <mi>e</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mrow>
      <mi>r</mi>
      <mo>/</mo>
      <mi>c</mi>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>e</ci>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <divide></divide>
      <ci>r</ci>
      <ci>c</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   e^{(1-r/c)}
  </annotation>
 </semantics>
</math>

. (In the case of multiple reference sentences, 

<math display="inline" id="BLEU:21">
 <semantics>
  <mi>r</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>r</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r
  </annotation>
 </semantics>
</math>

 is taken to be the sum of the lengths of the sentences whose lengths are closest to the lengths of the candidate sentences. However, in the version of the metric used by <a href="NIST_(metric)" title="wikilink">NIST</a> evaluations prior to 2009, the shortest reference sentence had been used instead.)</p>

<p>iBLEU is an interactive version of BLEU that allows a user to visually examine the BLEU scores obtained by the candidate translations. It also allows comparing two different systems in a visual and interactive manner which is useful for system development.</p>
<h2 id="performance">Performance</h2>

<p>BLEU has frequently been reported as correlating well with human judgement, and remains a benchmark for the assessment of any new evaluation metric. There are however a number of criticisms that have been voiced. It has been noted that although in principle capable of evaluating translations of any language, BLEU cannot in its present form deal with languages lacking word boundaries.</p>

<p>It has been argued that although BLEU has significant advantages, there is no guarantee that an increase in BLEU score is an indicator of improved translation quality. There is an inherent, systemic problem with any metric based on comparing with one or a few reference translations: in real life, sentences can be translated in many different ways, sometimes with no overlap. Therefore, the approach of comparing by how much any given translation result by a computer differs from just a few human translations is flawed. HyTER is another automated MT metric that compares to very many translations in a reference grammar defined by human translators;<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> the drawback is then that the human effort involved in correctly defining the combinatorially many ways to render the meaning of the translation in practice means HyTER also is only an approximation.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="F1_Score" title="wikilink">F-Measure</a></li>
<li><a href="NIST_(metric)" title="wikilink">NIST (metric)</a></li>
<li><a class="uri" href="METEOR" title="wikilink">METEOR</a></li>
<li><a href="ROUGE_(metric)" title="wikilink">ROUGE (metric)</a></li>
<li><a href="Word_error_rate" title="wikilink">Word Error Rate (WER)</a></li>
<li><a href="Noun-Phrase_Chunking" title="wikilink">Noun-Phrase Chunking</a></li>
<li><a href="Translation_Error_Rate" title="wikilink">Translation Error Rate</a></li>
</ul>
<h2 id="notes">Notes</h2>
<ol>
<li>

<p>Papineni, K., et al. (2002)</p></li>
<li>

<p>Papineni, K., et al. (2002)</p></li>
<li>

<p>Coughlin, D. (2003)</p></li>
<li>

<p>Papineni, K., et al. (2002)</p></li>
<li>

<p>Papineni, K., et al. (2002)</p></li>
<li>

<p>Papineni, K., et al. (2002)</p></li>
<li>

<p>Papineni, K., et al. (2002)</p></li>
<li>

<p>Coughlin, D. (2003)</p></li>
<li>

<p>Doddington, G. (2002)</p></li>
<li>

<p>Denoual, E. and Lepage, Y. (2005)</p></li>
<li>

<p>Callison-Burch, C., Osborne, M. and Koehn, P. (2006)</p></li>
<li>

<p>Lee, A. and Przybocki, M. (2005)</p></li>
<li>

<p>Callison-Burch, C., Osborne, M. and Koehn, P. (2006)</p></li>
<li>

<p>Lin, C. and Och, F. (2004)</p></li>
<li>

<p>Callison-Burch, C., Osborne, M. and Koehn, P. (2006)</p></li>
<li>

<p>Madnani, N. (2011)</p></li>
</ol>
<h2 id="references">References</h2>
<ul>
<li></li>
<li>Papineni, K., Roukos, S., Ward, T., Henderson, J and Reeder, F. (2002). “<a href="http://mt-archive.info/HLT-2002-Papineni.pdf">Corpus-based Comprehensive and Diagnostic MT Evaluation: Initial Arabic, Chinese, French, and Spanish Results</a>” in Proceedings of Human Language Technology 2002, San Diego, pp. 132–137</li>
<li>Callison-Burch, C., Osborne, M. and Koehn, P. (2006) "<a href="http://www.cs.jhu.edu/~ccb/publications/re-evaluating-the-role-of-bleu-in-mt-research.pdf">Re-evaluating the Role of BLEU in Machine Translation Research</a>" in <em>11th Conference of the European Chapter of the Association for Computational Linguistics: EACL 2006</em> pp. 249–256</li>
<li>Doddington, G. (2002) "<a href="http://www.nist.gov/speech/tests/mt/doc/ngram-study.pdf">Automatic evaluation of machine translation quality using n-gram cooccurrence statistics</a>" in <em>Proceedings of the Human Language Technology Conference (HLT), San Diego, CA</em> pp. 128–132</li>
<li>Coughlin, D. (2003) "<a href="http://www.mt-archive.info/MTS-2003-Coughlin.pdf">Correlating Automated and Human Assessments of Machine Translation Quality</a>" in <em>MT Summit IX, New Orleans, USA</em> pp. 23–27</li>
<li>Denoual, E. and Lepage, Y. (2005) "<a href="http://www.mt-archive.info/IJCNLP-2005-Denoual.pdf">BLEU in characters: towards automatic MT evaluation in languages without word delimiters</a>" in <em>Companion Volume to the Proceedings of the Second International Joint Conference on Natural Language Processing</em> pp. 81–86</li>
<li>Lee, A. and Przybocki, M. (2005) NIST 2005 machine translation evaluation official results</li>
<li>Lin, C. and Och, F. (2004) "<a href="http://www.mt-archive.info/ACL-2004-Lin.pdf">Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics</a>" in <em>Proceedings of the 42nd Annual Meeting of the Association of Computational Linguistics</em>.</li>
<li>Madnani, N. (2011). "[<a class="uri" href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6061334&amp;tag">http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6061334&amp;tag;</a>;=1 iBLEU: Interactively Scoring and Debugging Statistical Machine Translation Systems]" in "Proceedings of the Fifth IEEE International Conference on Semantic Computing (Demos), Palo Alto, CA" pp. 213–214</li>
</ul>

<p>"</p>

<p><a href="Category:Evaluation_of_machine_translation" title="wikilink">Category:Evaluation of machine translation</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
</ol>
</section>
</body>
</html>
