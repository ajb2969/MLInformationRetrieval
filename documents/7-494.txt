   Rosenbrock function      Rosenbrock function   In mathematical optimization , the Rosenbrock function is a non- convex function used as a performance test problem for optimization algorithms introduced by Howard H. Rosenbrock in 1960. 1 It is also known as Rosenbrock's valley or Rosenbrock's banana function .  The global minimum is inside a long, narrow, parabolic shaped flat valley. To find the valley is trivial. To converge to the global minimum , however, is difficult.  The function is defined by       f   (  x  ,  y  )    =     (   a  -  x   )   2   +   b    (   y  -   x  2    )   2           f   x  y       superscript    a  x   2     b   superscript    y   superscript  x  2    2       f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}     It has a global minimum at     (  x  ,  y  )   =   (  a  ,   a  2   )        x  y    a   superscript  a  2      (x,y)=(a,a^{2})   , where     f   (  x  ,  y  )    =  0        f   x  y    0    f(x,y)=0   . Usually    a  =  1      a  1    a=1   and    b  =  100      b  100    b=100   .  Multidimensional generalisations  Two variants are commonly encountered. One is the sum of    N  /  2      N  2    N/2   uncoupled 2D Rosenbrock problems,        f   (  ùê±  )    =   f   (   x  1   ,   x  2   ,  ‚Ä¶  ,   x  N   )    =    ‚àë   i  =  1    N  /  2     [    100    (    x    2  i   -  1   2   -   x   2  i     )   2    +    (    x    2  i   -  1    -  1   )   2    ]     .          f  ùê±     f    subscript  x  1    subscript  x  2   normal-‚Ä¶   subscript  x  N            superscript   subscript     i  1      N  2     delimited-[]      100   superscript     superscript   subscript  x      2  i   1    2    subscript  x    2  i     2     superscript     subscript  x      2  i   1    1   2         f(\mathbf{x})=f(x_{1},x_{2},\dots,x_{N})=\sum_{i=1}^{N/2}\left[100(x_{2i-1}^{2%
 }-x_{2i})^{2}+(x_{2i-1}-1)^{2}\right].    2  This variant is only defined for even   N   N   N   and has predictably simple solutions.  A more involved variant is         f   (  ùê±  )    =    ‚àë   i  =  1    N  -  1     [     (   1  -   x  i    )   2   +   100    (    x   i  +  1    -   x  i  2    )   2     ]       ‚àÄ  x   ‚àà   ‚Ñù  N     .     formulae-sequence      f  ùê±     superscript   subscript     i  1      N  1     delimited-[]     superscript    1   subscript  x  i    2     100   superscript     subscript  x    i  1     superscript   subscript  x  i   2    2           for-all  x    superscript  ‚Ñù  N      f(\mathbf{x})=\sum_{i=1}^{N-1}\left[(1-x_{i})^{2}+100(x_{i+1}-x_{i}^{2})^{2}%
 \right]\quad\forall x\in\mathbb{R}^{N}.    3  This variant has been shown to have exactly one minimum for    N  =  3      N  3    N=3   (at    (  1  ,  1  ,  1  )     1  1  1    (1,1,1)   ) and exactly two minima for    4  ‚â§  N  ‚â§  7        4  N       7     4\leq N\leq 7   ‚Äîthe global minimum of all ones and a local minimum near     (   x  1   ,   x  2   ,  ‚Ä¶  ,   x  N   )   =   (   -  1   ,  1  ,  ‚Ä¶  ,  1  )         subscript  x  1    subscript  x  2   normal-‚Ä¶   subscript  x  N       1   1  normal-‚Ä¶  1     (x_{1},x_{2},\dots,x_{N})=(-1,1,\dots,1)   . This result is obtained by setting the gradient of the function equal to zero, noticing that the resulting equation is a rational function of   x   x   x   . For small   N   N   N   the polynomials can be determined exactly and Sturm's theorem can be used to determine the number of real roots, while the roots can be bounded in the region of     |   x  i   |   <  2.4         subscript  x  i    2.4    |x_{i}|<2.4   . 4 For larger   N   N   N   this method breaks down due to the size of the coefficients involved.  Stationary points  Many of the stationary points of the function exhibit a regular pattern when plotted. 5 This structure can be exploited to locate them.  An example of optimization  The Rosenbrock function can be efficiently optimized by adapting appropriate coordinate system without using any gradient information and without building local approximation models (in contrast to many derivate-free optimizers). The following figure illustrates an example of 2-dimensional Rosenbrock function optimization by adaptive coordinate descent from starting point     x  0   =   (   -  3   ,   -  4   )        subscript  x  0      3     4      x_{0}=(-3,-4)   . The solution with the function value    10   -  10      superscript  10    10     10^{-10}   can be found after 325 function evaluations.  (Figure)  Rosenbrock.png   See also   Test functions for optimization   Notes  References     External links   Rosenbrock function plot in 3D  Minimizing the Rosenbrock Function by Michael Croucher, The Wolfram Demonstrations Project .    "  Category:Mathematical optimization     ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©      