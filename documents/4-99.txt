   Reproducing kernel Hilbert space      Reproducing kernel Hilbert space   (Figure)  Figure illustrates related but varying approaches to viewing RKHS   In functional analysis (a branch of mathematics ), a '''reproducing kernel Hilbert space '''(RKHS) is a Hilbert space associated with a kernel that reproduces every function in the space or, equivalently, where every evaluation functional is bounded. The reproducing kernel was first introduced in the 1907 work of Stanisław Zaremba concerning boundary value problems for harmonic and biharmonic functions. James Mercer simultaneously examined functions which satisfy the reproducing property in the theory of integral equations. The idea of the reproducing kernel remained untouched for nearly twenty years until it appeared in the dissertations of Gábor Szegő , Stefan Bergman , and Salomon Bochner . The subject was eventually systematically developed in the early 1950s by Nachman Aronszajn and Stefan Bergman. 1  These spaces have wide applications, including complex analysis , harmonic analysis , and quantum mechanics . Reproducing kernel Hilbert spaces are particularly important in the field of statistical learning theory because of the celebrated Representer theorem which states that every function in an RKHS can be written as a linear combination of the kernel function evaluated at the training points. This is a practically useful result as it effectively simplifies the empirical risk minimization problem from an infinite dimensional to a finite dimensional optimization problem.  For ease of understanding, we provide the framework for real-valued Hilbert spaces. The theory can be easily extended to spaces of complex-valued functions and hence include the many important examples of reproducing kernel Hilbert spaces that are spaces of analytic functions . 2  Definition  Let X be an arbitrary set and H a Hilbert space of real-valued functions on X . The evaluation functional over the Hilbert space of functions H is a linear functional that evaluates each function at a point x ,        L  x   :   f  ↦   f   (  x  )     ∀  f    ∈  H    .     normal-:   subscript  L  x      maps-to  f    f  x    for-all  f         H      L_{x}:f\mapsto f(x)\text{ }\forall f\in H.     We say that H is a reproducing kernel Hilbert space if    L  x     subscript  L  x    L_{x}   is a continuous function for any   f   f   f   in   H   H   H   or, equivalently, if    L  x     subscript  L  x    L_{x}   is a bounded operator so that for any   f   f   f   in   H   H   H   there exists some M > 0 such that  While property () is the weakest condition that ensures both the existence of an inner product and the evaluation of every function in H at every point in the domain, it does not lend itself to easy application in practice. A more intuitive definition of the RKHS can be obtained by observing that this property guarantees that the evaluation functional can be represented by taking the inner product of   f   f   f   with a function    K  x     subscript  K  x    K_{x}   in H . This function is the so-called reproducing kernel for the Hilbert space H from which the RKHS takes its name. More formally, the Riesz representation theorem implies that for all x in X there exists a unique element    K  x     subscript  K  x    K_{x}   of H with the reproducing property,  Since    K  x     subscript  K  x    K_{x}   is itself a function in H we have that for each x in X         K  x    (  y  )    =    ⟨   K  y   ,   K  x   ⟩   H    .         subscript  K  x   y    subscript    subscript  K  y    subscript  K  x    H     K_{x}(y)=\langle K_{y},\ K_{x}\rangle_{H}.     This allows us to define the reproducing kernel of H as a function    K  :    X  ×  X   →  ℝ      normal-:  K   normal-→    X  X   ℝ     K:X\times X\to\mathbb{R}   by        K   (  x  ,  y  )    =    K  x    (  y  )     .        K   x  y       subscript  K  x   y     K(x,y)=K_{x}(y).     From this definition it is easy to see that a function    K  :    X  ×  X   →  ℝ      normal-:  K   normal-→    X  X   ℝ     K:X\times X\to\mathbb{R}   is a reproducing kernel if it is both symmetric and positive definite , i.e.        ∑    i  ,  j   =  1   n     c  i    c  j   K   (   x  i   ,   x  j   )     ≥  0        superscript   subscript      i  j   1    n      subscript  c  i    subscript  c  j   K    subscript  x  i    subscript  x  j      0    \sum_{i,j=1}^{n}c_{i}c_{j}K(x_{i},x_{j})\geq 0     for any      n  ∈   ℕ  ,   x  1   ,  …    ,     x  n   ∈   X  ,   and   c  1    ,  …    ,    c  n   ∈  ℝ     .     formulae-sequence    n   ℕ   subscript  x  1   normal-…     formulae-sequence     subscript  x  n    X    and   subscript  c  1    normal-…       subscript  c  n   ℝ      n\in\mathbb{N},x_{1},\dots,x_{n}\in X,\text{ and }c_{1},\dots,c_{n}\in\mathbb{%
 R}.    3  Example  The space of bandlimited functions    H   H   H   is a RKHS. Let      H  =   {   f  ∈    L  2    (  ℝ  )     |     supp   (  F  )    ⊂   [   -  a   ,  a  ]    ,   a  <  ∞    }       H   conditional-set    f     subscript  L  2   ℝ     formulae-sequence     supp  F      a   a      a        H=\{f\in L_{2}(\mathbb{R})|\operatorname{supp}(F)\subset[-a,a],a<\infty\}     where     F   (  ω  )    =   ∫   f   (  x  )    e   -   i  ω  x     d  x          F  ω       f  x   superscript  e      i  ω  x     d  x      F(\omega)=\int f(x)e^{-i\omega x}dx   is the Fourier transform of   f   f   f   . One can show that if    f  ∈  H      f  H    f\in H   then       f   (  x  )    =    1   2  π      ∫   -  a   a    ϕ   (  ω  )    e   i  x  ω    d  ω           f  x       1    2  π      superscript   subscript     a    a     ϕ  ω   superscript  e    i  x  ω    d  ω       f(x)=\frac{1}{2\pi}\int_{-a}^{a}\phi(\omega)e^{ix\omega}d\omega     for some    ϕ  ∈    L  2    [   -  a   ,  a  ]        ϕ     subscript  L  2      a   a      \phi\in L_{2}[-a,a]   . It then follows by the Cauchy-Schwarz inequality and Plancherel's Theorem that        |   f   (  x  )    |   ≤      1   2  π      ∫   -  a   a    1  d  ω         1   2  π      ∫   -  a   a      |   ϕ   (  ω  )    |   2   d  ω       =     a  π     ∥  f  ∥     .            f  x            1    2  π      superscript   subscript     a    a     1  d  ω            1    2  π      superscript   subscript     a    a      superscript      ϕ  ω    2   d  ω                  a  π     norm  f       |f(x)|\leq\sqrt{\frac{1}{2\pi}\int_{-a}^{a}1d\omega}\sqrt{\frac{1}{2\pi}\int_{%
 -a}^{a}|\phi(\omega)|^{2}d\omega}=\sqrt{\frac{a}{\pi}}\|f\|.     As this inequality shows that the evaluation functional is bounded and   H   H   H   is also a Hilbert space,   H   H   H   is indeed a RKHS.  The kernel function    K  x     subscript  K  x    K_{x}   in this case is given by        K  x    (  y  )    =    a  π    sinc   (   a   (   y  -  x   )    )     =    sin   (   a   (   y  -  x   )    )     π   (   y  -  x   )              subscript  K  x   y       a  π    sinc    a    y  x                 a    y  x       π    y  x        K_{x}(y)=\frac{a}{\pi}\operatorname{sinc}(a(y-x))=\frac{\sin(a(y-x))}{\pi(y-x)}   .  Note, that    K  x     subscript  K  x    K_{x}   in this case is the "bandlimited version" of the Dirac delta distribution and that    K  x     subscript  K  x    K_{x}   converges to    δ   (  ⋅  -  x  )      fragments  δ   fragments  normal-(  normal-⋅   x  normal-)     \delta(\cdot-x)   in the weak sense, as explained in the entry for the sinc function .  Moore-Aronszajn Theorem  We have seen how a reproducing kernel Hilbert space defines a reproducing kernel function that is both symmetric and positive definite . The Moore-Aronszajn theorem goes in the other direction; it states that every symmetric, positive definite kernel defines a unique reproducing kernel Hilbert space. The theorem first appeared in Aronszajn's Theory of Reproducing Kernels , although he attributes it to E. H. Moore .   Theorem . Suppose K is a symmetric, positive definite kernel on a set X . Then there is a unique Hilbert space of functions on X for which K is a reproducing kernel.   Proof . For all x in X , define K x = K ( x , ⋅ ). Let H 0 be the linear span of { K x : x ∈ X }. Define an inner product on H 0 by        ⟨    ∑   j  =  1   n     b  j    K   y  j      ,    ∑   i  =  1   m     a  i    K   x  i      ⟩   =    ∑   i  =  1   m     ∑   j  =  1   n     a  i    b  j   K   (   y  j   ,   x  i   )       .         superscript   subscript     j  1    n      subscript  b  j    subscript  K   subscript  y  j        superscript   subscript     i  1    m      subscript  a  i    subscript  K   subscript  x  i         superscript   subscript     i  1    m     superscript   subscript     j  1    n      subscript  a  i    subscript  b  j   K    subscript  y  j    subscript  x  i         \left\langle\sum_{j=1}^{n}b_{j}K_{y_{j}},\sum_{i=1}^{m}a_{i}K_{x_{i}}\right%
 \rangle=\sum_{i=1}^{m}\sum_{j=1}^{n}{a_{i}}b_{j}K(y_{j},x_{i}).     The symmetry of this inner product follows from the symmetry of K and the non-degeneracy follows from the fact that K is positive definite.  Let H be the completion of H 0 with respect to this inner product. Then H consists of functions of the form       f   (  x  )    =    ∑   i  =  1   ∞     a  i    K   x  i     (  x  )           f  x     superscript   subscript     i  1         subscript  a  i    subscript  K   subscript  x  i    x      f(x)=\sum_{i=1}^{\infty}a_{i}K_{x_{i}}(x)     where      ∑   i  =  1   ∞     a  i  2   K   (   x  i   ,   x  i   )     <  ∞        superscript   subscript     i  1         superscript   subscript  a  i   2   K    subscript  x  i    subscript  x  i         \sum_{i=1}^{\infty}a_{i}^{2}K(x_{i},x_{i})<\infty   . The fact that the above sum converges for every x follows from the Cauchy-Schwarz inequality .  Now we can check the reproducing property ():        ⟨  f  ,   K  x   ⟩   =   ⟨    ∑   i  =  1   ∞     a  i    K   x  i      ,   K  x   ⟩   =    ∑   i  =  1   ∞     a  i   K   (   x  i   ,  x  )     =   f   (  x  )     .         f   subscript  K  x       superscript   subscript     i  1         subscript  a  i    subscript  K   subscript  x  i       subscript  K  x           superscript   subscript     i  1         subscript  a  i   K    subscript  x  i   x            f  x      \langle f,K_{x}\rangle=\left\langle\sum_{i=1}^{\infty}a_{i}K_{x_{i}},K_{x}%
 \right\rangle=\sum_{i=1}^{\infty}a_{i}K(x_{i},x)=f(x).     To prove uniqueness, let G be another Hilbert space of functions for which K is a reproducing kernel. For any x and y in X , () implies that         ⟨   K  x   ,   K  y   ⟩   H   =   K   (  x  ,  y  )    =    ⟨   K  x   ,   K  y   ⟩   G    .         subscript    subscript  K  x    subscript  K  y    H     K   x  y          subscript    subscript  K  x    subscript  K  y    G      \langle K_{x},K_{y}\rangle_{H}=K(x,y)=\langle K_{x},K_{y}\rangle_{G}.\,     By linearity,      ⟨  ⋅  ,  ⋅  ⟩   H   =    ⟨  ⋅  ,  ⋅  ⟩   G        subscript   normal-⋅  normal-⋅   H    subscript   normal-⋅  normal-⋅   G     \langle\cdot,\cdot\rangle_{H}=\langle\cdot,\cdot\rangle_{G}   on the span of { K x : x ∈ X }. Then G = H by the uniqueness of the completion.  Integral Operators and Mercer's Theorem  We may characterize a symmetric positive definite kernel   K   K   K   via the integral operator using Mercer's theorem and obtain an additional view of the RKHS. Let   X   X   X   be a compact space equipped with a strictly positive finite Borel measure    μ   μ   \mu   and    K  :    X  ×  X   →  ℝ      normal-:  K   normal-→    X  X   ℝ     K:X\times X\to\mathbb{R}   a continuous, symmetric, and positive definite function. Define the integral operator     T  K   :     L  2    (  X  )    →    L  2    (  X  )        normal-:   subscript  T  K    normal-→     subscript  L  2   X      subscript  L  2   X      T_{K}:L_{2}(X)\rightarrow L_{2}(X)   as        [    T  K   f   ]    (  ⋅  )    =    ∫  X    K   (  ⋅  ,  t  )   f   (  t  )   d  μ   (  t  )            delimited-[]     subscript  T  K   f    normal-⋅     subscript   X     K   normal-⋅  t   f  t  d  μ  t      [T_{K}f](\cdot)=\int_{X}K(\cdot,t)f(t)\,d\mu(t)     where     L  2    (  X  )        subscript  L  2   X    L_{2}(X)   is the space of square integrable functions with respect to   μ   μ   \mu   .  Mercer's theorem states that the spectral decomposition of the integral operator    T  K     subscript  T  K    T_{K}   of   K   K   K   yields a series representation of   K   K   K   in terms of the eigenvalues and eigenfunctions of    T  K     subscript  T  K    T_{K}   . This then implies that   K   K   K   is a reproducing kernel so that the corresponding RKHS can be defined in terms of these eigenvalues and eigenfunctions. We provide the details below.  Under these assumptions    T  k     subscript  T  k    T_{k}   is a compact, continuous, self-adjoint, and positive operator. The spectral theorem for self-adjoint operators implies that there is an at most countable decreasing sequence      (   σ  i   )   i   ≥  0       subscript   subscript  σ  i   i   0    (\sigma_{i})_{i}\geq 0   such that      lim   i  →  ∞     σ  i    =  0        subscript    normal-→  i      subscript  σ  i    0    \lim_{i\to\infty}\sigma_{i}=0   and      T  K    ϕ  i    (  x  )    =    σ  i    ϕ  i    (  x  )           subscript  T  K    subscript  ϕ  i   x      subscript  σ  i    subscript  ϕ  i   x     T_{K}\phi_{i}(x)=\sigma_{i}\phi_{i}(x)   , where the    {   ϕ  i   }      subscript  ϕ  i     \{\phi_{i}\}   form an orthonormal basis of     L  2    (  X  )        subscript  L  2   X    L_{2}(X)   . By the positivity    T  k     subscript  T  k    T_{k}   ,     σ  i   >   0    ∀  i         subscript  σ  i     0    for-all  i      \sigma_{i}>0\text{ }\forall i   . One can also show that    T  k     subscript  T  k    T_{k}   maps continuously into the space of continuous functions    C   (  X  )       C  X    C(X)   and therefore we may choose continuous functions as the eigenvectors, that is,     ϕ  i   ∈   C   (  X  )     ∀  i         subscript  ϕ  i     C  X    for-all  i      \phi_{i}\in C(X)\text{ }\forall i   . Then by Mercer's theorem   K   K   K   may be written in terms of the eigenvalues and continuous eigenfunctions as       K   (  x  ,  y  )    =    ∑   j  =  1   ∞      σ  j     ϕ  j    (  x  )    ϕ  j    (  y  )           K   x  y      superscript   subscript     j  1         subscript  σ  j    subscript  ϕ  j   x   subscript  ϕ  j   y      K(x,y)=\sum_{j=1}^{\infty}\sigma_{j}\,\phi_{j}(x)\,\phi_{j}(y)     for all    x  ,  y     x  y    x,y   in   X   X   X   such that      lim   n  →  ∞      sup   u  ,  v     |    K   (  u  ,  v  )    -    ∑   j  =  1   n      σ  j     ϕ  j    (  u  )    ϕ  j    (  v  )      |     =  0.        subscript    normal-→  n       subscript  supremum   u  v          K   u  v      superscript   subscript     j  1    n      subscript  σ  j    subscript  ϕ  j   u   subscript  ϕ  j   v        0.    \lim_{n\to\infty}\sup_{u,v}|K(u,v)-\sum_{j=1}^{n}\sigma_{j}\,\phi_{j}(u)\,\phi%
 _{j}(v)|=0.   This above series representation is referred to as a Mercer kernel or Mercer representation of   K   K   K   .  Furthermore, it can be shown that the RKHS   H   H   H   of   K   K   K   is given by      H  =   {  f  ∈   L  2    (  X  )   |   ∑   i  =  1   ∞      ⟨  f  ,   ϕ  i   ⟩   2    σ  i    <  ∞  }      fragments  H    fragments  normal-{  f    subscript  L  2    fragments  normal-(  X  normal-)   normal-|   superscript   subscript     i  1         superscript   f   subscript  ϕ  i    2    subscript  σ  i      normal-}     H=\left\{f{\in}L_{2}(X)\mathrel{\Bigg|}\sum_{i=1}^{\infty}\frac{\left\langle f%
 ,\phi_{i}\right\rangle^{2}}{\sigma_{i}}<\infty\right\}     where the inner product of   H   H   H   given by       ⟨  f  ,  g  ⟩   H   =    ∑   i  =  1   ∞       ⟨  f  ,   ϕ  i   ⟩    L  2      ⟨  g  ,   ϕ  i   ⟩    L  2      σ  i      .       subscript   f  g   H     superscript   subscript     i  1           subscript   f   subscript  ϕ  i     subscript  L  2     subscript   g   subscript  ϕ  i     subscript  L  2      subscript  σ  i       \left\langle f,g\right\rangle_{H}=\sum_{i=1}^{\infty}\frac{\left\langle f,\phi%
 _{i}\right\rangle_{L_{2}}\left\langle g,\phi_{i}\right\rangle_{L_{2}}}{\sigma_%
 {i}}.   This representation of the RKHS has application in probability and statistics, for example to the Karhunen-Loeve representation for stochastic processes and kernel PCA .  Feature Maps  A feature map is a map    φ  :   X  →  F      normal-:  φ   normal-→  X  F     \varphi:X\rightarrow F   , where   F   F   F   is a Hilbert space which we will call the feature space. The first sections presented the connection between bounded/continuous evaluation functions, positive definite functions, and integral operators and in this section we provide another representation of the RKHS in terms of feature maps.  We first note that every feature map defines a kernel via   Clearly   K   K   K   is symmetric and positive definiteness follows from the properties of inner product in   F   F   F   . Conversely, every positive definite function and corresponding reproducing kernel Hilbert space has infinitely many associated feature maps such that () holds.  For example, we can trivially take    F  =  H      F  H    F=H   and     φ   (  x  )    =   K  x         φ  x    subscript  K  x     \varphi(x)=K_{x}   for all    x  ∈  X      x  X    x\in X   . Then () is satisfied by the reproducing property. Another classical example of a feature map relates to the previous section regarding integral operators by taking    F  =   ℓ  2       F   superscript  normal-ℓ  2     F=\ell^{2}   and     φ   (  x  )    =    (     σ  i     ϕ  i    (  x  )    )   i         φ  x    subscript       subscript  σ  i     subscript  ϕ  i   x   i     \varphi(x)=(\sqrt{\sigma_{i}}\phi_{i}(x))_{i}   .  This connection between kernels and feature maps provides us with a new way to understand positive definite functions and hence reproducing kernels as inner products in   H   H   H   . Moreover, every feature map can naturally define a RKHS by means of the definition of a positive definite function.  Lastly, feature maps allow us to construct function spaces that reveal another perspective on the RKHS. Consider the linear space       H  φ   =   {  f  :  X  →  ℝ  |  ∃  w  ∈  F  ,  f   (  x  )   =    ⟨  w  ,  φ   (  x  )   ⟩   F   ,  ∀   x  ∈  X  }   .     fragments   subscript  H  φ     fragments  normal-{  f  normal-:  X  normal-→  R  normal-|   w   F  normal-,  f   fragments  normal-(  x  normal-)     subscript   fragments  normal-⟨  w  normal-,  φ   fragments  normal-(  x  normal-)   normal-⟩   F   normal-,  for-all   x   X  normal-}   normal-.    H_{\varphi}=\{f:X\to\mathbb{R}|\exists w\in F,f(x)=\langle w,\varphi(x)\rangle%
 _{F},\forall\text{ }x\in X\}.     We can define a norm on    H  φ     subscript  H  φ    H_{\varphi}   by         ||  f  ||   φ   =   inf   {    ||  w  ||   F   :    w  ∈  F   ,     f   (  x  )    =    ⟨  w  ,   φ   (  x  )    ⟩   F    ,    ∀    x    ∈  X     }     .       subscript   norm  f   φ     inf   conditional-set   subscript   norm  w   F    formulae-sequence    w  F    formulae-sequence      f  x    subscript   w    φ  x    F       for-all     x    X         ||f||_{\varphi}=\text{inf}\{||w||_{F}:w\in F,f(x)=\langle w,\varphi(x)\rangle_%
 {F},\forall\text{ }x\in X\}.     It can be shown that    H  φ     subscript  H  φ    H_{\varphi}   is a RKHS with kernel defined by     K   (  x  ,  y  )    =   ⟨   φ   (  x  )    ,   φ   (  y  )    ⟩         K   x  y       φ  x     φ  y      K(x,y)=\langle\varphi(x),\varphi(y)\rangle   . This representation implies that the elements of the RKHS are inner products of elements in the feature space and can accordingly be seen as hyperplanes. This view of the RKHS is related to the kernel trick in machine learning. 4  Properties  The following properties of RKHSs may be useful to readers.   Let     (   X  i   )    i  =  1   p     superscript   subscript   subscript  X  i     i  1    p    (X_{i})_{i=1}^{p}   be a sequence of sets and     (   K  i   )    i  =  1   p     superscript   subscript   subscript  K  i     i  1    p    (K_{i})_{i=1}^{p}   be a collection of corresponding positive definite functions on     (   X  i   )    i  =  1   p     superscript   subscript   subscript  X  i     i  1    p    (X_{i})_{i=1}^{p}   . It then follows that        K   (   (   x  1   ,  …  ,   x  p   )   ,   (   y  1   ,  …  ,   y  p   )   )    =    K  1    (   x  1   ,   y  1   )   …   K  p    (   x  p   ,   y  p   )          K     subscript  x  1   normal-…   subscript  x  p      subscript  y  1   normal-…   subscript  y  p         subscript  K  1     subscript  x  1    subscript  y  1    normal-…   subscript  K  p     subscript  x  p    subscript  y  p       K((x_{1},\dots,x_{p}),(y_{1},\dots,y_{p}))=K_{1}(x_{1},y_{1})\dots K_{p}(x_{p}%
 ,y_{p})     is a kernel on    X  =    X  1   ×  …  ×   X  p        X     subscript  X  1   normal-…   subscript  X  p      X=X_{1}\times\dots\times X_{p}   .   Let     X  0   ⊂  X       subscript  X  0   X    X_{0}\subset X   , then the restriction of   K   K   K   to     X  0   ×   X  0        subscript  X  0    subscript  X  0     X_{0}\times X_{0}   is also a reproducing kernel.    Consider a normalized kernel   K   K   K   such that     K   (  x  ,  x  )    =  1        K   x  x    1    K(x,x)=1   for all    x  ∈  X      x  X    x\in X   . Define a pseudo-metric on X as         d  K    (  x  ,  y  )    =    ||    K  x   -   K  y    ||   H  2   =   2   (   1  -   K   (  x  ,  y  )     )     ∀  x    ∈  X           subscript  d  K    x  y     superscript   subscript   norm     subscript  K  x    subscript  K  y     H   2          2    1    K   x  y       for-all  x         X     d_{K}(x,y)=||K_{x}-K_{y}||_{H}^{2}=2(1-K(x,y))\text{ }\forall x\in X   .  By the Cauchy–Schwarz inequality ,         K    (  x  ,  y  )   2    ≤   K   (  x  ,  x  )   K   (  y  ,  y  )     ∀  x     ,   y  ∈  X    .     formulae-sequence      K   superscript   x  y   2      K   x  x   K   y  y     for-all  x       y  X     K(x,y)^{2}\leq K(x,x)K(y,y)\text{ }\forall x,y\in X.     This inequality allows us to view   K   K   K   as a measure of similarity between inputs. If     x  ,  y   ∈  X       x  y   X    x,y\in X   are similar then    K   (  x  ,  y  )       K   x  y     K(x,y)   will be closer to 1 while if     x  ,  y   ∈  X       x  y   X    x,y\in X   are dissimilar then    K   (  x  ,  y  )       K   x  y     K(x,y)   will be closer to 0.   The closure of the span of    {   K  x   |   x  ∈  X   }     conditional-set   subscript  K  x     x  X     \{K_{x}|x\in X\}   coincides with   H   H   H   . 5   Examples  Common examples of kernels include:   Linear Kernel :        K   (  x  ,  y  )    =   ⟨  x  ,  y  ⟩         K   x  y     x  y     K(x,y)=\langle x,y\rangle      Polynomial Kernel :         K   (  x  ,  y  )    =    (    α   ⟨  x  ,  y  ⟩    +  1   )   d    ,    α  ∈  ℝ   ,   d  ∈  ℕ       formulae-sequence      K   x  y     superscript      α   x  y    1   d     formulae-sequence    α  ℝ     d  ℕ      K(x,y)=(\alpha\langle x,y\rangle+1)^{d},\alpha{\in}\mathbb{R},d{\in}\mathbb{N}     Other common examples are kernels which satisfy     K   (  x  ,  y  )    =   K   (   ∥   x  -  y   ∥   )          K   x  y      K   norm    x  y       K(x,y)=K(\|x-y\|)   . These are the radial basis function kernels.   Radial Basis Function Kernels :   :* Gaussian Kernel :    Sometimes referred to as the Radial basis function kernel , or squared exponential kernel        K   (  x  ,  y  )    =   e   -     ∥   x  -  y   ∥   2    2   σ  2        ,   σ  >  0      formulae-sequence      K   x  y     superscript  e       superscript   norm    x  y    2     2   superscript  σ  2          σ  0     K(x,y)=e^{-\frac{\|x-y\|^{2}}{2\sigma^{2}}},\sigma>0        :* Laplacian Kernel :          K   (  x  ,  y  )    =   e   -    ∥   x  -  y   ∥   σ      ,   σ  >  0      formulae-sequence      K   x  y     superscript  e       norm    x  y    σ        σ  0     K(x,y)=e^{-\frac{\|x-y\|}{\sigma}},\sigma>0        We also provide examples of Bergman kernels . Let X be finite and let H consist of all complex-valued functions on X . Then an element of H can be represented as an array of complex numbers. If the usual inner product is used, then K x is the function whose value is 1 at x and 0 everywhere else, and K(x,y) can be thought of as an identity matrix since K(x,y)=1 when x=y and K(x,y)=0 otherwise. In this case, H is isomorphic to C n .  The case of X = D is more sophisticated, here the Bergman space  H 2 ( D ) is the space of square-integrable  holomorphic functions on D . It can be shown that the reproducing kernel for H 2 ( D ) is        K   (  x  ,  y  )    =    1  π    1    (   1  -   x   y  ¯     )   2      .        K   x  y        1  π     1   superscript    1    x   normal-¯  y     2       K(x,y)=\frac{1}{\pi}\frac{1}{(1-x\overline{y})^{2}}.     Lastly, the space of band limited functions   f   f   f   in     L  2    (  ℝ  )        superscript  L  2   ℝ    L^{2}(\mathbb{R})   with bandwidth   π   π   \pi   are a RKHS with reproducing kernel        K   (  x  ,  y  )    =     sin  π    (   x  -  y   )     π   (   x  -  y   )      .        K   x  y          π     x  y      π    x  y       K(x,y)=\frac{\sin\pi(x-y)}{\pi(x-y)}.     Extension to Vector-Valued Functions  In this section we extend the definition of the RKHS to spaces of vector-valued functions as this extension is particularly important in multi-task learning and manifold regularization. The main difference is that the reproducing kernel   Γ   normal-Γ   \Gamma   is a symmetric function that is now a positive semi-definite matrix for any    x  ,  y     x  y    x,y   in   X   X   X   . More formally, we define a vector-valued RKHS (vvRKHS) as a Hilbert space of functions    f  :   X  →   ℝ  T       normal-:  f   normal-→  X   superscript  ℝ  T      f:X\to\mathbb{R}^{T}   such that for all    c  ∈   ℝ  T       c   superscript  ℝ  T     c\in\mathbb{R}^{T}   and    x  ∈  X      x  X    x\in X           Γ  x   c   (  y  )    =   Γ   (  x  ,  y  )   c   ∈   H  for  y   ∈  X           subscript  normal-Γ  x   c  y     normal-Γ   x  y   c          H  for  y        X     \Gamma_{x}c(y)=\Gamma(x,y)c\in H\text{ for }y\in X     and      {    Γ  x   c   :    x  ∈  X   ,   c  ∈   ℝ  T     }     conditional-set     subscript  normal-Γ  x   c    formulae-sequence    x  X     c   superscript  ℝ  T       \{\Gamma_{x}c:x\in X,c\in\mathbb{R}^{T}\}     This second property parallels the reproducing property for the scalar-valued case. We note that this definition can also be connected to integral operators, bounded evaluation functions, and feature maps as we saw for the scalar-valued RKHS. We can equivalently define the vvRKHS as a vector-valued Hilbert space with a bounded evaluation functional and show that this implies the existence of a unique reproducing kernel by the Riesz Representation theorem. Mercer's theorem can also be extended to address the vector-valued setting and we can therefore obtain a feature map view of the vvRKHS. Lastly, it can also be shown that the closure of the span of   H   H   H   coincides with    Λ  =   {  1  ,  …  ,  T  }       normal-Λ   1  normal-…  T     \Lambda=\{1,\dots,T\}   , another property similar to the scalar-valued case.  We can gain intuition for the vvRKHS by taking a component-wise perspective on these spaces. In particular, we find that every vvRKHS is isometrically isomorphic to a scalar-valued RKHS on a particular input space. Let    X  ×  Λ      X  normal-Λ    X\times\Lambda   . Consider the space    {   γ   (  x  ,  t  )    :    x  ∈  X   ,   t  ∈  Λ    }     conditional-set   subscript  γ   x  t     formulae-sequence    x  X     t  normal-Λ      \{\gamma_{(x,t)}:x\in X,t\in\Lambda\}   and the corresponding reproducing kernel  As noted above, the RKHS associated to this reproducing kernel is given by the closure of the span of       γ    (  x  ,  t  )     (  y  ,  s  )    =   γ   (   (  x  ,  t  )   ,   (  y  ,  s  )   )           subscript  γ   x  t     y  s      γ    x  t    y  s       \ \gamma_{(x,t)}(y,s)=\gamma((x,t),(y,s))   where      (  x  ,  t  )   ,   (  y  ,  s  )    ∈   X  ×  Λ         x  t    y  s      X  normal-Λ     (x,t),(y,s)\in X\times\Lambda   for every set of pairs      Γ    (  x  ,  y  )    (  t  ,  s  )     =   γ   (   (  x  ,  t  )   ,   (  y  ,  s  )   )     .        normal-Γ   subscript   x  y    t  s       γ    x  t    y  s       \Gamma(x,y)_{(t,s)}=\gamma((x,t),(y,s)).   .  The connection to the scalar-valued RKHS can then be made by the fact that every matrix-valued kernel can be identified with a kernel of the form of () via      D  :    H  Γ   →   H  γ       normal-:  D   normal-→   subscript  H  normal-Γ    subscript  H  γ      D:H_{\Gamma}\to H_{\gamma}     Moreover, every kernel with the form of () defines a matrix-valued kernel with the above expression. Now letting the map    e  t     subscript  e  t    e_{t}   be defined as      t   t  h      superscript  t    t  h     t^{th}     where    ℝ  T     superscript  ℝ  T    \mathbb{R}^{T}   is the   D   D   D   component of the canonical basis for    H  Γ     subscript  H  normal-Γ    H_{\Gamma}   , one can show that    H  γ     subscript  H  γ    H_{\gamma}   is bijective and an isometry between   T   T   T   and     γ   (   (  x  ,  t  )   ,   (  y  ,  s  )   )    =   K   (  x  ,  y  )    K  T    (  t  ,  s  )          γ    x  t    y  s       K   x  y    subscript  K  T    t  s      \gamma((x,t),(y,s))=K(x,y)K_{T}(t,s)   .  While this view of the vvRKHS can be quite useful in multi-task learning, it should be noted that this isometry does not reduce the study of the vector-valued case to that of the scalar-valued case. In fact, this isometry procedure can make both the scalar-valued kernel and the input space too difficult to work with in practice as properties of the original kernels are often lost. 6  7  8  An important class of matrix-valued reproducing kernels are separable kernels which can factorized as the product of a scalar valued kernel and a    x  ,  y     x  y    x,y   -dimensional symmetric positive semi-definite matrix. In light of our previous discussion these kernels are of the form     X   X   X     for all    t  ,  s     t  s    t,s   in   T   T   T   and $t,s$ in $T$ . As the scalar-valued kernel encodes dependencies between the inputs, we can observe that the matrix-valued kernel encodes dependencies among both the inputs and the outputs.  We lastly remark that the above theory can be further extended to spaces of functions with values in function spaces but obtaining kernels for these spaces is a more difficult task. 9  See also   Positive definite kernel  Mercer's theorem  Kernel trick  Kernel embedding of distributions  Representer theorem   Notes  References   Alvarez, Mauricio, Rosasco, Lorenzo and Lawrence, Neil, “Kernels for Vector-Valued Functions: a Review,” http://arxiv.org/abs/1106.6251 , June 2011.   Berlinet, Alain and Thomas, Christine. Reproducing kernel Hilbert spaces in Probability and Statistics , Kluwer Academic Publishers, 2004.   De Vito, Ernest, Umanita, Veronica, and Villa, Silvia. "An extension of Mercer theorem to vector-valued measurable kernels," http://arxiv.org/pdf/1110.4017.pdf , June 2013.  Durrett, Greg. 9.520 Course Notes, Massachusetts Institute of Technology, http://www.mit.edu/~9.520/scribe-notes/class03_gdurett.pdf , February 2010.   Okutmustur, Baver. “Reproducing Kernel Hilbert Spaces,” Ph.D. dissertation, Bilkent University, http://www.thesis.bilkent.edu.tr/0002953.pdf , August 2005.  Paulsen, Vern. “An introduction to the theory of reproducing kernel Hilbert spaces,” http://www.math.uh.edu/∼vern/rkhs.pdf .   Rosasco, Lorenzo and Poggio, Thomas. "A Regularization Tour of Machine Learning - MIT 9.520 Lecture Notes" Manuscript, Dec. 2014.  Wahba, Grace , Spline Models for Observational Data , SIAM , 1990.  Zhang, Haizhang, Xu, Yuesheng, and Zhang, Qinghui (2012). "Refinement of Operator-valued Reproducing Kernels." Journal of Machine Learning Research 13 91-136.   "  Category:Hilbert space     Okutmustur ↩  Paulson ↩  Durrett ↩  Rosasco ↩  Rosasco ↩  De Vito ↩  Zhang ↩  Alvarez ↩  Rosasco ↩     