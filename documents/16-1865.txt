   Cascade Learning Based on Adaboost      Cascade Learning Based on Adaboost   The Boosting Algorithms for Detector Cascade Learning 1 2 is proposed by Mohammad Saberian and Nuno Vasconcelos 3 in 2014, it is based on Viola–Jones object detection framework . 4  Motivation of Improvement  Paul Viola and Michael Jones propose great ideas in cascade learning classifier based on Ad- aboost. However, we can see, how to determine the number of stage in cascade classifier and the number of feature per stage to construct Adaboost classifier is hard to get. In Viola and Jones, Rapid Object Detection using a Boosted Cascade of Simple Features, just give a crude way to determine configuration: number of features in the first five layers of the detector is 1, 10, 25, 25 and 50 features respectively. The remaining layers have increasingly more features.  It is an important topic discussed in Boosting Algorithms for Detector Cascade Learning . In this work, they address the problem of automatically learning both the configuration and the stages of a high detection rate detector cascade. This is accomplished with the fast cascade boosting (FCBoost) algorithm, which is derived from a Lagrangian risk that trades-off detection performs and speed. FCBoost optimizes this risk with respect to a predictor that complies with the sequential decision making structure of the cascade architecture.  Contents of Algorithm  last-stage  The first family of cascade predictors that we consider is derived from the generator         g  1    [   f  1   ,   f  2   ]    (  x  )    =     f  1    (  x  )   u   [   -    f  1    (  x  )     ]    +   u   [    f  1    (  x  )    ]    f  2    (  x  )     =   {       f  1    (  x  )        if   f  1    (  x  )    <  0         f  2    (  x  )        if   f  1    (  x  )    >  0                subscript  g  1     subscript  f  1    subscript  f  2    x        subscript  f  1   x  u   delimited-[]       subscript  f  1   x        u   delimited-[]     subscript  f  1   x     subscript  f  2   x          cases     subscript  f  1   x       if   subscript  f  1   x   0      subscript  f  2   x       if   subscript  f  1   x   0       g_{1}[f_{1},f_{2}](x)=f_{1}(x)u[-f_{1}(x)]+u[f_{1}(x)]f_{2}(x)=\begin{cases}f_%
 {1}(x)&\mbox{if }f_{1}(x)<0\\
 f_{2}(x)&\mbox{if }f_{1}(x)>0\end{cases}      The associated predictor recursion is         g  2    [   f  1   ,   f  2   ]    (  x  )    =     f  1    (  x  )   u   [   -    f  1    (  x  )     ]    +   u   [    f  1    (  x  )    ]    f  1    (  x  )    f  2    (  x  )     =   {       f  1    (  x  )        if   f  1    (  x  )    <  0         f  2    (  x  )    f  1    (  x  )        if   f  1    (  x  )    >  0                subscript  g  2     subscript  f  1    subscript  f  2    x        subscript  f  1   x  u   delimited-[]       subscript  f  1   x        u   delimited-[]     subscript  f  1   x     subscript  f  1   x   subscript  f  2   x          cases     subscript  f  1   x       if   subscript  f  1   x   0      subscript  f  2   x   subscript  f  1   x       if   subscript  f  1   x   0       g_{2}[f_{1},f_{2}](x)=f_{1}(x)u[-f_{1}(x)]+u[f_{1}(x)]f_{1}(x)f_{2}(x)=\begin{%
 cases}f_{1}(x)&\mbox{if }f_{1}(x)<0\\
 f_{2}(x)f_{1}(x)&\mbox{if }f_{1}(x)>0\end{cases}      Multiplicative Cascades  The second family of cascade predictors has generator        ℒ   [  f  ]    =     ℛ  E    [  F  ]    +   η   ℛ  C    [  F  ]           ℒ   delimited-[]  f         subscript  ℛ  E    delimited-[]  F      η   subscript  ℛ  C    delimited-[]  F       \mathcal{L}[f]=\mathcal{R}_{E}[F]+\eta\mathcal{R}_{C}[F]      The associated predictor recursion is       F   (  x  )       F  x    F(x)      Learning the Cascade Configuration  This consists of determining the number of cascade stages and the number of weak learners per stage. In this work, they start by assuming that the number of cascade stages is known, and concentrate on the composition of theses stages. By minimizing Lagrangian risk to accomplish this goal, the Lagrangian risk is a trade-off about searching most accurate detector under a complexity constraint. Because more accurate detector, more complexity it has.        R  E    [  F  ]        subscript  R  E    delimited-[]  F     R_{E}[F]      where     g  k  *    (  x  )        superscript   subscript  g  k     x    g_{k}^{*}(x)   and     g  k  *    (  x  )        superscript   subscript  g  k     x    g_{k}^{*}(x)   are the cascade predictor and classification risk, We have assumed that the number of cascade stages is known at first. However this is usually not the case, there is a need for a procedure that learns this component of the cascade configuration. In the work, they adopt a greedy strategy, where cascade stages are added by the boosting algorithm itself, whenever this leads to a reduction of the risk. It is assumed that a new stage, or predictor g, can only be added at the end of the existing cascade. But it is often to meet problem that no update is taken, so they introduce the concept of neutral predictors.  The FCBoost Algorithm  FCBoost is initialized with a neutral predictor. At each iteration, it finds the best update      f   k  +  1     (  x  )    =    f  k    (  x  )    =   w   (  x  )             subscript  f    k  1    x      subscript  f  k   x          w  x      f_{k+1}(x)=f_{k}(x)=w(x)   (which a best weak learner for stage k) for each of the cascade stages and the best stage to add at the end of the cascade. It then selects the stage k whose update $g_k^*(x)$ most reduces the Lagrangian L[F]. If k is the newly added stage, a new stage is created and appended to the cascade. The only parameters are the multiplier η in Lagrange risk, which gives the relative importance of cascade speed and accuracy for the cascade designer.  From algorithm of FCBoost above, it can automatically determine both the cascade configuration(number of stages and number of weak learners per stage through iteration) and the predictor of each stage, so as to optimize the trade-off between detection accuracy and complex- ity through given parameter η.  The two cascade predictor(last-stage and multiplicative cascade) cover the two predominant cascade structures, first (last-stage) is the independent stage, i.e. stage predictors are designed independently(Viola and Jones, 2001). The second(multiplicative) is embedded stage where predictors of consecutive stages are related by   $f_{k+1}(x) = f_k(x) = w(x)$   and w(x) is a single or linear combination of weak learners.  last-stage:  itr: 1) g1  itr: 2) g1 -> g1 + g2  itr: 3) g1 + g3 -> g1 + g2  itr: 4) g1 + g3 -> g1 + g2 -> g1 + g2 + g4  Multiplicative Cascades:  itr: 1) 1 + g1  itr: 2) 1 + g1 -> 1 + g2  itr: 3) 1 + g1 + g3 -> 1 + g2  itr: 4) 1 + g1 + g3 -> 1 + g2 -> 1 + g4  Conclusion  FCBoost is the new cascade boosting algorithm proposed by Saberian and Vasconcelos. It can determine cascade learning configuration automatically by just given a Lagrange multiplier η (which is relative importance of detect accurate and speed). And in algorithm, it is very efficient to implement and have good performances compared to original work.  References  "     P. Bartlett and M. Traskin. Adaboost is consistent. Journal of Machine Learning Research,8:2347–2368, December 2007. ↩  S. Brubaker, M. Mullin, and J. Rehg. On the design of cascades of boosted ensembles for face detection. International Journal of Computer Vision, 77:65–86, 2008. ↩  Boosting Algorithms for Detector Cascade Learning ↩  Rapid object detection using a boosted cascade of simple features ↩     