<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1413">Cache-oblivious algorithm</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Cache-oblivious algorithm</h1>
<hr/>

<p>In <a class="uri" href="computing" title="wikilink">computing</a>, a <strong>cache-oblivious algorithm</strong> (or cache-transcendent algorithm) is an <a class="uri" href="algorithm" title="wikilink">algorithm</a> designed to take advantage of a <a href="CPU_cache" title="wikilink">CPU cache</a> without having the size of the cache (or the length of the <a href="cache_line" title="wikilink">cache lines</a>, etc.) as an explicit parameter. An <strong>optimal cache-oblivious algorithm</strong> is a cache-oblivious algorithm that uses the cache optimally (in an <a href="asymptotic_notation" title="wikilink">asymptotic</a> sense, ignoring constant factors). Thus, a cache oblivious algorithm is designed to perform well, without modification, on multiple machines with different cache sizes, or for a <a href="memory_hierarchy" title="wikilink">memory hierarchy</a> with different levels of cache having different sizes. Cache-oblivious algorithms are contrasted with explicit <em><a href="Loop_tiling" title="wikilink">blocking</a>,</em> as in <a href="loop_nest_optimization" title="wikilink">loop nest optimization</a>, which explicitly breaks a problem into blocks that are optimally sized for a given cache.</p>

<p>Optimal cache-oblivious algorithms are known for the <a href="Cooley–Tukey_FFT_algorithm" title="wikilink">Cooley–Tukey FFT algorithm</a>, <a href="Cache-oblivious_matrix_multiplication" title="wikilink">matrix multiplication</a>, <a href="Funnelsort" title="wikilink">sorting</a>, <a href="matrix_transposition" title="wikilink">matrix transposition</a>, and several other problems. Because these algorithms are only optimal in an asymptotic sense (ignoring constant factors), further machine-specific tuning may be required to obtain nearly optimal performance in an absolute sense. The goal of cache-oblivious algorithms is to reduce the amount of such tuning that is required.</p>

<p>Typically, a cache-oblivious algorithm works by a <a href="recursion" title="wikilink">recursive</a> <a href="divide_and_conquer_algorithm" title="wikilink">divide and conquer algorithm</a>, where the problem is divided into smaller and smaller subproblems. Eventually, one reaches a subproblem size that fits into cache, regardless of the cache size. For example, an optimal cache-oblivious matrix multiplication is obtained by recursively dividing each matrix into four sub-matrices to be multiplied, multiplying the submatrices in a <a class="uri" href="depth-first" title="wikilink">depth-first</a> fashion. In tuning for a specific machine, one may use a <a href="hybrid_algorithm" title="wikilink">hybrid algorithm</a> which uses blocking tuned for the specific cache sizes at the bottom level, but otherwise uses the cache-oblivious algorithm.</p>
<h2 id="history">History</h2>

<p>The idea (and name) for cache-oblivious algorithms was conceived by <a href="Charles_E._Leiserson" title="wikilink">Charles E. Leiserson</a> as early as 1996 and first published by <a href="Harald_Prokop" title="wikilink">Harald Prokop</a> in his master's thesis at the <a href="Massachusetts_Institute_of_Technology" title="wikilink">Massachusetts Institute of Technology</a> in 1999. There were many predecessors, typically analyzing specific problems; these are discussed in detail in Frigo et al. 1999. Early examples cited include Singleton 1969 for a recursive Fast Fourier Transform, similar ideas in Aggarwal et al. 1987, Frigo 1996 for matrix multiplication and LU decomposition, and <a href="Todd_Veldhuizen" title="wikilink">Todd Veldhuizen</a> 1996 for matrix algorithms in the <a class="uri" href="Blitz++" title="wikilink">Blitz++</a> library.</p>
<h2 id="idealized-cache-model">Idealized cache model</h2>

<p>Cache-oblivious algorithms are typically analyzed using an idealized model of the cache, sometimes called the <strong>cache-oblivious model</strong>. This model is much easier to analyze than a real cache's characteristics (which have complicated associativity, replacement policies, etcetera), but in many cases is provably within a constant factor of a more realistic cache's performance.</p>

<p>In particular, the cache-oblivious model is an <a href="abstract_machine" title="wikilink">abstract machine</a> (i.e. a theoretical model of computation). It is similar to the <a href="RAM_model" title="wikilink">RAM machine model</a> which replaces the <a href="Turing_machine" title="wikilink">Turing machine</a>'s infinite tape with an infinite array. Each location within the array can be accessed in 

<math display="inline" id="Cache-oblivious_algorithm:0">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>1</mn>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(1)
  </annotation>
 </semantics>
</math>

 time, similar to the <a href="Random_access_memory" title="wikilink">Random access memory</a> on a real computer. Unlike the RAM machine model, it also introduces a cache: a second level of storage between the RAM and the CPU. The other differences between the two models are listed below. In the cache-oblivious model:</p>
<ul>
<li>Memory is broken into lines of 

<math display="inline" id="Cache-oblivious_algorithm:1">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

 words each</li>
<li>A load or a store between main memory and a CPU register may now be serviced from the cache.</li>
<li>If a load or a store cannot be serviced from the cache, it is called a <em>cache miss</em>.</li>
<li>A cache miss results in one line being loaded from main memory into the cache. Namely, if the CPU tries to access word 

<math display="inline" id="Cache-oblivious_algorithm:2">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Cache-oblivious_algorithm:3">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>


 is the line containing 

<math display="inline" id="Cache-oblivious_algorithm:4">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

, then 

<math display="inline" id="Cache-oblivious_algorithm:5">
 <semantics>
  <mi>b</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>b</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b
  </annotation>
 </semantics>
</math>

 is loaded into the cache. If the cache was previously full, then a line will be evicted as well (see replacement policy below).</li>
<li>The cache holds 

<math display="inline" id="Cache-oblivious_algorithm:6">
 <semantics>
  <mi>Z</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Z</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Z
  </annotation>
 </semantics>
</math>

 words, where 

<math display="inline" id="Cache-oblivious_algorithm:7">
 <semantics>
  <mrow>
   <mi>Z</mi>
   <mo>=</mo>
   <mrow>
    <mi mathvariant="normal">Ω</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msup>
      <mi>L</mi>
      <mn>2</mn>
     </msup>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>Z</ci>
    <apply>
     <times></times>
     <ci>normal-Ω</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>L</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Z=\Omega(L^{2})
  </annotation>
 </semantics>
</math>

. This is also known as the <em>tall cache assumption</em>.</li>
<li>The cache is fully associative: each line can be loaded into any location in the cache.</li>
<li>The replacement policy is optimal. In other words, the cache is assumed to be given the entire sequence of memory accesses during algorithm execution. If it needs to evict a line at time 

<math display="inline" id="Cache-oblivious_algorithm:8">
 <semantics>
  <mi>t</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>t</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   t
  </annotation>
 </semantics>
</math>


, it will look into its sequence of future requests and evict the line that is accessed furthest in the future. This can be emulated in practice with the <a href="Least_Recently_Used" title="wikilink">Least Recently Used</a> policy, which is shown to be within a small constant factor of the offline optimal replacement strategy (Frigo et al., 1999, Sleator and Tarjan, 1985).</li>
</ul>

<p>To measure the complexity of an algorithm that executes within the cache-oblivious model, we can measure the familiar (running time) <em>work</em> complexity 

<math display="inline" id="Cache-oblivious_algorithm:9">
 <semantics>
  <mrow>
   <mi>W</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>n</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>W</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W(n)
  </annotation>
 </semantics>
</math>

. However, we can also measure the <em>cache complexity</em>, 

<math display="inline" id="Cache-oblivious_algorithm:10">
 <semantics>
  <mrow>
   <mi>Q</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>n</mi>
    <mo>,</mo>
    <mi>L</mi>
    <mo>,</mo>
    <mi>Z</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>Q</ci>
    <vector>
     <ci>n</ci>
     <ci>L</ci>
     <ci>Z</ci>
    </vector>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Q(n,L,Z)
  </annotation>
 </semantics>
</math>

, the number of cache misses that the algorithm will experience.</p>

<p>The goal for creating a good cache-oblivious algorithm is to match the work complexity of some optimal RAM model algorithm while minimizing 

<math display="inline" id="Cache-oblivious_algorithm:11">
 <semantics>
  <mrow>
   <mi>Q</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>n</mi>
    <mo>,</mo>
    <mi>L</mi>
    <mo>,</mo>
    <mi>Z</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>Q</ci>
    <vector>
     <ci>n</ci>
     <ci>L</ci>
     <ci>Z</ci>
    </vector>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Q(n,L,Z)
  </annotation>
 </semantics>
</math>

. Furthermore, unlike the <a href="external-memory_model" title="wikilink">external-memory model</a>, which shares many of the listed features, we would like our algorithm to be independent of cache parameters (

<math display="inline" id="Cache-oblivious_algorithm:12">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Cache-oblivious_algorithm:13">
 <semantics>
  <mi>Z</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Z</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Z
  </annotation>
 </semantics>
</math>


). The benefit of such an algorithm is that what is efficient on a cache-oblivious machine is likely to be efficient across many real machines without fine tuning for particular real machine parameters. Frigo et al. showed that for many problems, an optimal cache-oblivious algorithm will also be optimal for a machine with more than two <a href="memory_hierarchy" title="wikilink">memory hierarchy</a> levels.</p>
<h2 id="examples">Examples</h2>

<p>For example, it is possible to design a variant of <a href="unrolled_linked_list" title="wikilink">unrolled linked lists</a> which is cache-oblivious and allows list traversal of 

<math display="inline" id="Cache-oblivious_algorithm:14">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 elements in 

<math display="inline" id="Cache-oblivious_algorithm:15">
 <semantics>
  <mrow>
   <mi>n</mi>
   <mo>/</mo>
   <mi>L</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <ci>n</ci>
    <ci>L</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n/L
  </annotation>
 </semantics>
</math>

 time, where 

<math display="inline" id="Cache-oblivious_algorithm:16">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

 is the cache size in elements. For a fixed 

<math display="inline" id="Cache-oblivious_algorithm:17">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

, this is 

<math display="inline" id="Cache-oblivious_algorithm:18">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>n</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(n)
  </annotation>
 </semantics>
</math>


 time. However, the advantage of the algorithm is that it can scale to take advantage of larger cache line sizes (larger values of 

<math display="inline" id="Cache-oblivious_algorithm:19">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

).</p>

<p>The simplest cache-oblivious algorithm presented in Frigo et al. is an out-of-place <a href="matrix_transpose" title="wikilink">matrix transpose</a> operation (<a href="in-place_matrix_transposition" title="wikilink">in-place algorithms</a> have also been devised for transposition, but are much more complicated for non-square matrices). Given <em>m</em>×<em>n</em> array <strong>A</strong> and <em>n</em>×<em>m</em> array <strong>B</strong>, we would like to store the transpose of 

<math display="inline" id="Cache-oblivious_algorithm:20">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A
  </annotation>
 </semantics>
</math>

 in 

<math display="inline" id="Cache-oblivious_algorithm:21">
 <semantics>
  <mi>B</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>B</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B
  </annotation>
 </semantics>
</math>

. The naive solution traverses one array in row-major order and another in column-major. The result is that when the matrices are large, we get a cache miss on every step of the column-wise traversal. The total number of cache misses is 

<math display="inline" id="Cache-oblivious_algorithm:22">
 <semantics>
  <mrow>
   <mi mathvariant="normal">Θ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>m</mi>
     <mi>n</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-Θ</ci>
    <apply>
     <times></times>
     <ci>m</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Theta(mn)
  </annotation>
 </semantics>
</math>

.</p>

<p> The cache-oblivious algorithm has optimal work complexity 

<math display="inline" id="Cache-oblivious_algorithm:23">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>m</mi>
     <mi>n</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <times></times>
     <ci>m</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(mn)
  </annotation>
 </semantics>
</math>


 and optimal cache complexity 

<math display="inline" id="Cache-oblivious_algorithm:24">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mn>1</mn>
     <mo>+</mo>
     <mrow>
      <mrow>
       <mi>m</mi>
       <mi>n</mi>
      </mrow>
      <mo>/</mo>
      <mi>L</mi>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <plus></plus>
     <cn type="integer">1</cn>
     <apply>
      <divide></divide>
      <apply>
       <times></times>
       <ci>m</ci>
       <ci>n</ci>
      </apply>
      <ci>L</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(1+mn/L)
  </annotation>
 </semantics>
</math>

. The basic idea is to reduce the transpose of two large matrices into the transpose of small (sub)matrices. We do this by dividing the matrices in half along their larger dimension until we just have to perform the transpose of a matrix that will fit into the cache. Because the cache size is not known to the algorithm, the matrices will continue to be divided recursively even after this point, but these further subdivisions will be in cache. Once the dimensions 

<math display="inline" id="Cache-oblivious_algorithm:25">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Cache-oblivious_algorithm:26">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 are small enough so an <em>input</em> array of size 

<math display="inline" id="Cache-oblivious_algorithm:27">
 <semantics>
  <mrow>
   <mi>m</mi>
   <mo>×</mo>
   <mi>n</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>m</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m\times n
  </annotation>
 </semantics>
</math>

 and an output array of size 

<math display="inline" id="Cache-oblivious_algorithm:28">
 <semantics>
  <mrow>
   <mi>n</mi>
   <mo>×</mo>
   <mi>m</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>n</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n\times m
  </annotation>
 </semantics>
</math>


 fit into the cache, both row-major and column-major traversals result in 

<math display="inline" id="Cache-oblivious_algorithm:29">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>m</mi>
     <mi>n</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <times></times>
     <ci>m</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(mn)
  </annotation>
 </semantics>
</math>

 work and 

<math display="inline" id="Cache-oblivious_algorithm:30">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mrow>
      <mi>m</mi>
      <mi>n</mi>
     </mrow>
     <mo>/</mo>
     <mi>L</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <ci>m</ci>
      <ci>n</ci>
     </apply>
     <ci>L</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(mn/L)
  </annotation>
 </semantics>
</math>

 cache misses. By using this divide and conquer approach we can achieve the same level of complexity for the overall matrix.</p>

<p>(In principle, one could continue dividing the matrices until a base case of size 1×1 is reached, but in practice one uses a larger base case (e.g. 16×16) in order to <a href="Amortized_analysis" title="wikilink">amortize</a> the overhead of the recursive subroutine calls.)</p>

<p>Most cache-oblivious algorithms rely on a divide-and-conquer approach. They reduce the problem, so that it eventually fits in cache no matter how small the cache is, and end the recursion at some small size determined by the function-call overhead and similar cache-unrelated optimizations, and then use some cache-efficient access pattern to merge the results of these small, solved problems.</p>
<h2 id="references">References</h2>
<ul>
<li>Harald Prokop. <a href="http://supertech.csail.mit.edu/papers/Prokop99.pdf">Cache-Oblivious Algorithms</a>. Masters thesis, MIT. 1999.</li>
<li>M. Frigo, C.E. Leiserson, H. Prokop, and S. Ramachandran. Cache-oblivious algorithms. In <em>Proceedings of the 40th IEEE Symposium on Foundations of Computer Science</em> (FOCS 99), p.285-297. 1999. <a href="http://ieeexplore.ieee.org/iel5/6604/17631/00814600.pdf?arnumber=814600">Extended abstract at IEEE</a>, <a href="http://citeseer.ist.psu.edu/307799.html">at Citeseer</a>.</li>
<li><a href="Erik_Demaine" title="wikilink">Erik Demaine</a>. <a href="http://theory.csail.mit.edu/classes/6.897/spring03/scribe_notes/L15/lecture15.pdf">Review of the Cache-Oblivious Model</a>. Notes for MIT Computer Science 6.897: Advanced Data Structures.</li>
<li><a href="http://compgeom.com/~piyush">Piyush Kumar</a>. <a href="http://www.compgeom.com/co-chap/chap.pdf">Cache-Oblivious Algorithms</a>. Algorithms for Memory Hierarchies, LNCS 2625, pages 193-212, Springer Verlag.</li>
<li>Daniel Sleator, Robert Tarjan. Amortized Efficiency of List Update and Paging Rules. In <em>Communications of the ACM</em>, Volume 28, Number 2, p.202-208. Feb 1985.</li>
<li><a href="Erik_Demaine" title="wikilink">Erik Demaine</a>. <a href="http://erikdemaine.org/papers/BRICS2002/">Cache-Oblivious Algorithms and Data Structures</a>, in Lecture Notes from the EEF Summer School on Massive Data Sets, BRICS, University of Aarhus, Denmark, June 27–July 1, 2002.</li>
</ul>

<p>"</p>

<p><a href="Category:Models_of_computation" title="wikilink">Category:Models of computation</a> <a href="Category:Cache_(computing)" title="wikilink">Category:Cache (computing)</a> <a href="Category:Analysis_of_algorithms" title="wikilink">Category:Analysis of algorithms</a></p>
</body>
</html>
