   Kernel perceptron      Kernel perceptron   In machine learning , the kernel perceptron is a variant of the popular perceptron learning algorithm that can learn kernel machines , i.e. non-linear classifiers that employ a kernel function to compute the similarity of unseen samples to training samples. The algorithm was invented in 1964, 1 making it the first kernel classification learner. 2  Preliminaries  The perceptron algorithm  The perceptron algorithm is an old but popular online learning algorithm that operates by a principle called "error-driven learning". It iteratively improves a model by running it on training samples, then updating the model whenever it finds it has made an incorrect classification with respect to a supervised signal. The model learned by the standard perceptron algorithm is a linear binary classifier: a vector of weights   𝐰   𝐰   \mathbf{w}   (and optionally an intercept term   b   b   b   , omitted here for simplicity) that is used to classify a sample vector   𝐱   𝐱   \mathbf{x}   as class "one" or class "minus one" according to       y  ^   =   sgn   (    𝐰  ⊤   𝐱   )         normal-^  y    sgn     superscript  𝐰  top   𝐱      \hat{y}=\operatorname{sgn}(\mathbf{w}^{\top}\mathbf{x})     where a zero is arbitrarily mapped to one or minus one. (The " hat " on   ŷ   ŷ   ŷ   denotes an estimated value.)  In pseudocode, the perceptron algorithm is given by:   Initialize   𝐰   𝐰   \mathbf{w}   to an all-zero vector of length   p   p   p   , the number of predictors (features).  For some fixed number of iterations, or until some stopping criterion is met:  For each training example    𝐱  ᵢ      𝐱  ᵢ    \mathbf{x}ᵢ   with ground truth label }:  Let sgn( w T  x ᵢ )}} .  If    ŷ  ≠  y  ᵢ      ŷ  normal-≠  y  ᵢ    ŷ≠yᵢ   , update     𝐰  ←  𝐰   +   y  ᵢ  𝐱  ᵢ         𝐰  normal-←  𝐰     y  ᵢ  𝐱  ᵢ     \mathbf{w}←\mathbf{w}+yᵢ\mathbf{x}ᵢ   .       Kernel machines  By contrast with the linear models learned by the perceptron, a kernel machine 3 is a classifier that stores a subset of its training examples , associates with each a weight , and makes decisions for new samples   𝐱   𝐱   \mathbf{x}   by evaluating      sgn    ∑  i     α  i    y  i   K   (   𝐱  i   ,   𝐱  ′   )         sgn    subscript   i      subscript  α  i    subscript  y  i   K    subscript  𝐱  i    superscript  𝐱  normal-′        \operatorname{sgn}\sum_{i}\alpha_{i}y_{i}K(\mathbf{x}_{i},\mathbf{x^{\prime}})   .  Here,   K   K   K   is some kernel function. Formally, a kernel function is a non-negative semidefinite kernel (see Mercer's condition ), representing an inner product between samples in a high-dimensional space, as if the samples had been expanded to include additional features by a function   Φ   normal-Φ   Φ   :    K   (  𝐱  ,  𝐱  )   =  Φ   (  𝐱  )   ·  Φ   (  𝐱  )   )     fragments  K   fragments  normal-(  x  normal-,  x  normal-)    Φ   fragments  normal-(  x  normal-)   ·  Φ   fragments  normal-(  x  normal-)   normal-)    K(\mathbf{x},\mathbf{x})=Φ(\mathbf{x})·Φ(\mathbf{x}))   . Intuitively, it can be thought of as a similarity function between samples, so the kernel machine establishes the class of a new sample by weighted comparison to the training set. Each function    𝐱  ↦  K   (  𝐱  ᵢ  ,  𝐱  )   )     fragments  x  ↦  K   fragments  normal-(  x  ᵢ  normal-,  x  normal-)   normal-)    \mathbf{x}↦K(\mathbf{x}ᵢ,\mathbf{x}))   serves as a basis function in the classification.  Algorithm  To derive a kernelized version of the perceptron algorithm, we must first formulate it in dual form , starting from the observation that the weight vector   𝐰   𝐰   \mathbf{w}   can be expressed as a linear combination of the   n   n   n   training samples. The equation for the weight vector is      𝐰  =    ∑  i  n     α  i    y  i    𝐱  i         𝐰    superscript   subscript   i   n      subscript  α  i    subscript  y  i    subscript  𝐱  i       \mathbf{w}=\sum_{i}^{n}\alpha_{i}y_{i}\mathbf{x}_{i}     where is the number of times was misclassified, forcing an update . Using this result, we can formulate the dual perceptron algorithm, which loops through the samples as before, making predictions, but instead of storing and updating a weight vector   𝐰   𝐰   \mathbf{w}   , it updates a "mistake counter" vector   α   α   \mathbf{α}   . We must also rewrite the prediction formula to get rid of   𝐰   𝐰   \mathbf{w}   :      y  ^     normal-^  y    \displaystyle\hat{y}     Plugging these two equations into the training loop turn it into the dual perceptron algorithm.  Finally, we can replace the dot product in the dual perceptron by an arbitrary kernel function, to get the effect of a feature map   Φ   normal-Φ   Φ   without computing    Φ   (  𝐱  )       normal-Φ  𝐱    Φ(\mathbf{x})   explicitly for any samples. Doing this yields the kernel perceptron algorithm: 4   Initialize   α   α   \mathbf{α}   to an all-zeros vector of length   n   n   n   , the number of training samples.  For some fixed number of iterations, or until some stopping criterion is met:  For each training example    𝐱  ,  y     𝐱  y    \mathbf{x},y   :  Let     y  ^   =   sgn    ∑  i  n     α  i    y  i   K   (   𝐱  i   ,  𝐱  )           normal-^  y     sgn    superscript   subscript   i   n      subscript  α  i    subscript  y  i   K    subscript  𝐱  i   𝐱        \hat{y}=\operatorname{sgn}\sum_{i}^{n}\alpha_{i}y_{i}K(\mathbf{x}_{i},\mathbf{%
 x})     If    ŷ  ≠  y      ŷ  normal-≠  y    ŷ≠y   , perform an update:           Variants and extensions  One problem with the kernel perceptron, as presented above, is that it does not learn sparse kernel machines. Initially, all the    α  ᵢ      α  ᵢ    αᵢ   are zero so that evaluating the decision function to get   ŷ   ŷ   ŷ   requires no kernel evaluations at all, but each update increments a single    α  ᵢ      α  ᵢ    αᵢ   , making the evaluation increasingly more costly. Moreover, when the kernel perceptron is used in an online setting, the number of non-zero    α  ᵢ      α  ᵢ    αᵢ   and thus the evaluation cost grow linearly in the number of examples presented to the algorithm.  The forgetron variant of the kernel perceptron was suggested to deal with this problem. It maintains an active set of examples with non-zero    α  ᵢ      α  ᵢ    αᵢ   , removing ("forgetting") examples from the active set when it exceeds a pre-determined budget and "shrinking" (lowering the weight of) old examples as new ones are promoted to non-zero    α  ᵢ      α  ᵢ    αᵢ   . 5  Another problem with the kernel perceptron is that it does not regularize , making it vulnerable to overfitting . The NORMA online kernel learning algorithm can be regarded as a generalization of the kernel perceptron algorithm with regularization. 6 The sequential minimal optimization (SMO) algorithm used to learn support vector machines can also be regarded as a generalization of the kernel perceptron. 7  The voted perceptron algorithm of Freund and Schapire also extends to the kernelized case, 8 giving generalization bounds comparable to the kernel SVM. 9  References  "  Category:Kernel methods for machine learning  Category:Statistical classification     Cited in ↩  ↩  Schölkopf, Bernhard; and Smola, Alexander J.; Learning with Kernels , MIT Press, Cambridge, MA, 2002. ISBN 0-262-19475-9 ↩  ↩  ↩  ↩   ↩      