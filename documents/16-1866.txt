   De-sparsified lasso      De-sparsified lasso   De-sparsified lasso contributes to construct confidence intervals and statistical tests for single or low-dimensional components of a large parameter vector in high-dimensional model. 1  1 High-dimensional linear model      Y  =    X   Œ≤  0    +  œµ       Y      X   superscript  Œ≤  0    œµ     Y=X\beta^{0}+\epsilon   with    n  √ó  p      n  p    n\times p   design matrix    X  =  :   [   X  1   ,  ‚Ä¶  ,   X  p   ]      fragments  X   normal-:   fragments  normal-[   subscript  X  1   normal-,  normal-‚Ä¶  normal-,   subscript  X  p   normal-]     X=:[X_{1},...,X_{p}]   (    n  √ó  p      n  p    n\times p   vectors    X  j     subscript  X  j    X_{j}   ),    œµ  ‚àº    N  n    (  0  ,    œÉ  œµ  2   I   )       similar-to  œµ     subscript  N  n    0     subscript   superscript  œÉ  2   œµ   I       \epsilon\sim N_{n}(0,\sigma^{2}_{\epsilon}I)   independent of   X   X   X   and unknown regression    p  √ó  1      p  1    p\times 1   vector    Œ≤  0     superscript  Œ≤  0    \beta^{0}   .  The usual method to find the parameter is by Lasso:       Œ≤  ^   n    (  Œª  )    =       a  r  g  m  i  n    Œ≤  ‚àà   ‚Ñù  p       1   2  n      ‚à•   Y  -   X  Œ≤    ‚à•   2  2    +   Œª    ‚à•  Œ≤  ‚à•   1            superscript   normal-^  Œ≤   n   Œª          Œ≤   superscript  ‚Ñù  p      a  r  g  m  i  n      1    2  n     subscript   superscript   norm    Y    X  Œ≤     2   2      Œª   subscript   norm  Œ≤   1       \hat{\beta}^{n}(\lambda)=\underset{\beta\in\mathbb{R}^{p}}{argmin}\ \frac{1}{2%
 n}\left\|Y-X\beta\right\|^{2}_{2}+\lambda\left\|\beta\right\|_{1}     The de-sparsified lasso is a method modified from the Lasso estimator which fulfills the Karush-Kuhn-Tucker conditions 2 is as follows:         Œ≤  ^   n    (  Œª  ,  M  )    =      Œ≤  ^   n    (  Œª  )    +    1  n   M   X  T    (   Y  -   X    Œ≤  ^   n    (  Œª  )     )            superscript   normal-^  Œ≤   n    Œª  M         superscript   normal-^  Œ≤   n   Œª       1  n   M   superscript  X  T     Y    X   superscript   normal-^  Œ≤   n   Œª        \hat{\beta}^{n}(\lambda,M)=\hat{\beta}^{n}(\lambda)+\frac{1}{n}MX^{T}(Y-X\hat{%
 \beta}^{n}(\lambda))     where    M  ‚àà   R  (   p  √ó  p  )     fragments  M    superscript  R  normal-(   p   p  normal-)    M\in R^{(}p\times p)   is an arbitrary matrix. The matrix   M   M   M   is generated using a surrogate inverse covariance matrix.  2 Generalized linear model  Desparsifying    l  1     subscript  l  1    l_{1}   - norm penalized estimators and corresponding theory can also be applied to models with convex loss functions such as generalized linear models.  Consider the following    1  √ó  p      1  p    1\times p   vectors of covariables     x  i   ‚àà  œá  ‚äÇ   R  p          subscript  x  i   œá        superscript  R  p      x_{i}\in\chi\subset R^{p}   and univariate responses     y  i   ‚àà  Y  ‚äÇ  R         subscript  y  i   Y       R     y_{i}\in Y\subset R   for    i  =   1  ,  ‚Ä¶  ,  n       i   1  normal-‚Ä¶  n     i=1,...,n     we have a loss function     œÅ  Œ≤    (  y  ,  x  )   =  œÅ   (  y  ,  x  Œ≤  )    (  Œ≤  ‚àà   R  p   )      fragments   subscript  œÅ  Œ≤    fragments  normal-(  y  normal-,  x  normal-)    œÅ   fragments  normal-(  y  normal-,  x  Œ≤  normal-)    fragments  normal-(  Œ≤    superscript  R  p   normal-)     \rho_{\beta}(y,x)=\rho(y,x\beta)(\beta\in R^{p})   which is assumed to be strictly convex function in    Œ≤  ‚àà   R  p       Œ≤   superscript  R  p     \beta\in R^{p}     The    l  1     subscript  l  1    l_{1}   -norm regularized estimator is     Œ≤  ^   =     a  r  g  m  i  n   ùõΩ    (     P  n    œÅ  Œ≤    +   Œª    ‚à•  Œ≤  ‚à•   1     )         normal-^  Œ≤      Œ≤    a  r  g  m  i  n         subscript  P  n    subscript  œÅ  Œ≤      Œª   subscript   norm  Œ≤   1        \hat{\beta}=\underset{\beta}{argmin}(P_{n}\rho_{\beta}+\lambda\left\|\beta%
 \right\|_{1})     Similarly, the Lasso for node wise regression with matrix input is defined as follows: Denote by    Œ£  ^     normal-^  normal-Œ£    \hat{\Sigma}   a matrix which we want to approximately invert using nodewise lasso.  The de-sparsified    l  1     subscript  l  1    l_{1}   -norm regularized estimator is as follows:      Œ≥  j   ^   :=    a  r  g  m  i  n    Œ≥  ‚àà   R   p  -  1       (    Œ£  ^    j  ,  j    -  2    Œ£  ^    j  ,    /  j     Œ≥  +   Œ≥  T     Œ£  ^      /  j   ,    /  j     Œ≥  +  2   Œª  j   ‚à•  Œ≥   ‚à•  1       fragments   normal-^   subscript  Œ≥  j    assign     Œ≥   superscript  R    p  1       a  r  g  m  i  n     fragments  normal-(   subscript   normal-^  normal-Œ£    j  j     2   subscript   normal-^  normal-Œ£    j    absent  j     Œ≥    superscript  Œ≥  T    subscript   normal-^  normal-Œ£      absent  j     absent  j     Œ≥   2   subscript  Œª  j   normal-‚à•  Œ≥   subscript  normal-‚à•  1      \hat{\gamma_{j}}:=\underset{\gamma\in R^{p-1}}{argmin}(\hat{\Sigma}_{j,j}-2%
 \hat{\Sigma}_{j,/j}\gamma+\gamma^{T}\hat{\Sigma}_{/j,/j}\gamma+2\lambda_{j}%
 \left\|\gamma\right\|_{1}     where     Œ£  ^    j  ,    /  j       subscript   normal-^  normal-Œ£    j    absent  j      \hat{\Sigma}_{j,/j}   denotes the   j   j   j   th row of    Œ£  ^     normal-^  normal-Œ£    \hat{\Sigma}   without the diagonal element    (  j  ,  j  )     j  j    (j,j)   , and     Œ£  ^      /  j   ,    /  j       subscript   normal-^  normal-Œ£      absent  j     absent  j      \hat{\Sigma}_{/j,/j}   is the sub matrix without the   j   j   j   th row and   j   j   j   th column.  References  "     ‚Ü©  ‚Ü©     