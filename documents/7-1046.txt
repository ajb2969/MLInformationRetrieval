   Pi system      Pi system   In mathematics , a π-system (or pi-system ) on a set Ω is a collection P of certain subsets of Ω, such that   P is non-empty.    A ∩ B ∈ P whenever A and B are in P .   That is, P is a non-empty family of subsets of Ω that is closed under finite intersections . The importance of π-systems arise from the fact that if two probability measures agree on a π-system, then they agree on the σ-algebra generated by that π-system. Moreover, if other properties, such as equality of integrals, hold for the π-system, then they hold for the generated σ-algebra as well. This is the case whenever the collection of subsets for which the property holds is a λ-system . π-systems are also useful for checking independence of random variables.  This is desirable because in practice, π-systems are often simpler to work with than σ-algebras. For example, it may be awkward to work with σ-algebras generated by infinitely many sets    σ   (   E  1   ,   E  2   ,  …  )       σ    subscript  E  1    subscript  E  2   normal-…     \sigma(E_{1},E_{2},\ldots)   . So instead we may examine the union of all σ-algebras generated by finitely many sets     ⋃  n    σ   (   E  1   ,  …  ,   E  n   )        subscript   n     σ    subscript  E  1   normal-…   subscript  E  n       \bigcup_{n}\sigma(E_{1},\ldots,E_{n})   . This forms a π-system that generates the desired σ-algebra. Another example is the collection of all interval subsets of the real line, along with the empty set, which is a π-system that generates the very important Borel σ-algebra of subsets of the real line.  Examples   On the real line   ℝ   ℝ   \mathbb{R}   , the intervals    (   -  ∞   ,  a  ]         a    (-\infty,a]   form a π-system. Similarly, the intervals    (  a  ,  b  ]     a  b    (a,b]   form a π-system, if the empty set is also included.  The topology (collection of open subsets) of any topological space is a π-system.  For any collection Σ of subsets of Ω, there exists a π-system    ℐ  Σ     subscript  ℐ  normal-Σ    \mathcal{I}_{\Sigma}   which is the unique smallest π-system of Ω to contain every element of Σ, and is called the π-system '' generated '' by Σ.  For any measurable function    f  :   Ω  →  ℝ      normal-:  f   normal-→  normal-Ω  ℝ     f\colon\Omega\rightarrow\mathbb{R}   , the set     ℐ  f   =   {    f   -  1     (   (   -  ∞   ,  x  ]   )    :   x  ∈  ℝ   }        subscript  ℐ  f    conditional-set     superscript  f    1         x      x  ℝ      \mathcal{I}_{f}=\left\{f^{-1}\left(\left(-\infty,x\right]\right)\colon x\in%
 \mathbb{R}\right\}   defines a π-system, and is called the π-system '' generated '' by f . (Alternatively,    {    A  1   ×   A  2    :     A  1   ∈   P  1    ,    A  2   ∈   P  2     }     conditional-set     subscript  A  1    subscript  A  2     formulae-sequence     subscript  A  1    subscript  P  1       subscript  A  2    subscript  P  2       \{A_{1}\times A_{2}:A_{1}\in P_{1},A_{2}\in P_{2}\}   .)  If P 1 and P 2 are π-systems for Ω 1 and Ω 2 , respectively, then    Ω  ∈  D      normal-Ω  D    \Omega\in D   is a π-system for the product space Ω 1 ×Ω 2 .  Any σ-algebra is a π-system.   Relationship to λ-Systems  A λ-system on Ω is a set D of subsets of Ω, satisfying       A  ∈  D      A  D    A\in D   ,  if     A  c   ∈  D       superscript  A  c   D    A^{c}\in D   then     A  1   ,   A  2   ,   A  3   ,  …      subscript  A  1    subscript  A  2    subscript  A  3   normal-…    A_{1},A_{2},A_{3},\dots   ,  if   D   D   D   is a sequence of disjoint subsets in      ∪   n  =  1   ∞    A  n    ∈  D        superscript   subscript     n  1       subscript  A  n    D    \cup_{n=1}^{\infty}A_{n}\in D   then   D   D   D   .   Whilst it is true that any σ-algebra satisfies the properties of being both a π-system and a λ-system, it is not true that any π-system is a λ-system, and moreover it is not true that any π-system is a σ-algebra. However, a useful classification is that any set system which is both a λ-system and a π-system is a σ-algebra. This is used as a step in proving the π-λ theorem.  The π-λ Theorem  Let    ℐ  ⊆  D      ℐ  D    \mathcal{I}\subseteq D   be a λ-system, and let   D   D   D   be a π-system contained in    σ   (  ℐ  )       σ  ℐ    \sigma(\mathcal{I})   . The π-λ Theorem 1 states that the σ-algebra   ℐ   ℐ   \mathcal{I}   generated by   D   D   D   is contained in     σ   (  ℐ  )    ⊂  D        σ  ℐ   D    \sigma(\mathcal{I})\subset D        D  =   {   A  ∈   σ   (  I  )     :     μ  1    (  A  )    =    μ  2    (  A  )     }    .      D   conditional-set    A    σ  I         subscript  μ  1   A      subscript  μ  2   A       D=\left\{A\in\sigma(I)\colon\mu_{1}(A)=\mu_{2}(A)\right\}.   .  The π-λ theorem can be used to prove many elementary measure theoretic results. For instance, it is used in proving the uniqueness claim of the Carathéodory extension theorem for σ-finite measures. 2  The π-λ theorem is closely related to the monotone class theorem , which provides a similar relationship between monotone classes and algebras, and can be used to derive many of the same results. Since π-systems are simpler classes than algebras, it can be easier to identify the sets that are in them while, on the other hand, checking whether the property under consideration determines a λ-system is often relatively easy. Despite the difference between the two theorems, the π-λ theorem is sometimes referred to as the monotone class theorem. 3  Example  Let μ 1 , μ 2 : F → R be two measures on the σ-algebra F , and suppose that F  =  σ ( I ) is generated by a π-system I . If   μ 1 ( A ) = μ 2 ( A ), ∀ A ∈  I , and  μ 1 ( Ω ) = μ 2 ( Ω ) <  ∞ ,   then μ 1 = μ 2 . This is the uniqueness statement of the Carathéodory extension theorem for finite measures. If this result does not seem very remarkable, consider the fact that it usually is very difficult or even impossible to fully describe every set in the σ-algebra, and so the problem of equating measures would be completely hopeless without such a tool.  Idea of Proof 4 Define the collection of sets      X  :    (  Ω  ,  ℱ  ,  ℙ  )   →  ℝ      normal-:  X   normal-→   normal-Ω  ℱ  ℙ   ℝ     X\colon(\Omega,\mathcal{F},\mathbb{P})\rightarrow\mathbb{R}   By the first assumption, μ 1 and μ 2 agree on I and thus I  ⊆  D . By the second assumption, Ω ∈  D , and it can further be shown that D is a λ-system. It follows from the π-λ theorem that σ ( I ) ⊆  D  ⊆  σ ( I ), and so D = σ ( I ). That is to say, the measures agree on σ ( I ).  π-Systems in Probability  π-systems are more commonly used in the study of probability theory than in the general field of measure theory. This is primarily due to probabilistic notions such as independence, though it may also be a consequence of the fact that the π-λ theorem was proven by the probabilist Eugene Dynkin . Standard measure theory texts typically prove the same results via monotone classes, rather than π-systems.  Equality in Distribution  The π-λ theorem motivates the common definition of the probability distribution of a random variable      F  X    (  a  )   =  ℙ   [  X  ≤  a  ]   ,  a  ∈  ℝ     fragments   subscript  F  X    fragments  normal-(  a  normal-)    P   fragments  normal-[  X   a  normal-]   normal-,  a   R    F_{X}(a)=\mathbb{P}\left[X\leq a\right],\qquad a\in\mathbb{R}   in terms of its cumulative distribution function . Recall that the cumulative distribution of a random variable is defined as         ℒ  X    (  B  )    =   ℙ   [    X   -  1     (  B  )    ]     ,   B  ∈   ℬ   (  ℝ  )        formulae-sequence       subscript  ℒ  X   B     ℙ   delimited-[]     superscript  X    1    B        B    ℬ  ℝ      \mathcal{L}_{X}(B)=\mathbb{P}\left[X^{-1}(B)\right],\qquad B\in\mathcal{B}(%
 \mathbb{R})   , whereas the seemingly more general law of the variable is the probability measure      ℬ   (  ℝ  )       ℬ  ℝ    \mathcal{B}(\mathbb{R})   , where    X  :   (  Ω  ,  ℱ  ,  ℙ  )      normal-:  X   normal-Ω  ℱ  ℙ     X\colon(\Omega,\mathcal{F},\mathbb{P})   is the Borel σ-algebra. We say that the random variables    Y  :    (   Ω  ~   ,   ℱ  ~   ,   ℙ  ~   )   →  ℝ      normal-:  Y   normal-→    normal-~  normal-Ω    normal-~  ℱ    normal-~  ℙ    ℝ     Y\colon(\tilde{\Omega},\tilde{\mathcal{F}},\tilde{\mathbb{P}})\rightarrow%
 \mathbb{R}   , and    X   =  𝒟   Y      superscript   𝒟   X  Y    X\stackrel{\mathcal{D}}{=}Y   (on two possibly different probability spaces) are equal in distribution (or law ),    ℒ  X     subscript  ℒ  X    \mathcal{L}_{X}   , if they have the same cumulative distribution functions, F X = F Y . The motivation for the definition stems from the observation that if F X = F Y , then that is exactly to say that    ℒ  Y     subscript  ℒ  Y    \mathcal{L}_{Y}   and    {   (   -  ∞   ,  a  ]   :   a  ∈  ℝ   }     conditional-set       a     a  ℝ     \left\{(-\infty,a]\colon a\in\mathbb{R}\right\}   agree on the π-system    ℬ   (  ℝ  )       ℬ  ℝ    \mathcal{B}(\mathbb{R})   which generates     ℒ  X   =   ℒ  Y        subscript  ℒ  X    subscript  ℒ  Y     \mathcal{L}_{X}=\mathcal{L}_{Y}   , and so by the example above    (  Ω  ,  ℱ  ,  ℙ  )     normal-Ω  ℱ  ℙ    (\Omega,\mathcal{F},\mathbb{P})   .  A similar result holds for the joint distribution of a random vector. For example, suppose X and Y are two random variables defined on the same probability space    ℐ  X     subscript  ℐ  X    \mathcal{I}_{X}   , with respectively generated π-systems    ℐ  Y     subscript  ℐ  Y    \mathcal{I}_{Y}   and     F   X  ,  Y     (  a  ,  b  )   =  ℙ   [  X  ≤  a  ,  Y  ≤  b  ]   =  ℙ   [   X   -  1     (   (  -  ∞  ,  a  ]   )   ∩   Y   -  1     (   (  -  ∞  ,  b  ]   )   ]   ,  a  ,  b  ∈  ℝ     fragments   subscript  F   X  Y     fragments  normal-(  a  normal-,  b  normal-)    P   fragments  normal-[  X   a  normal-,  Y   b  normal-]    P   fragments  normal-[   superscript  X    1     fragments  normal-(   fragments  normal-(    normal-,  a  normal-]   normal-)     superscript  Y    1     fragments  normal-(   fragments  normal-(    normal-,  b  normal-]   normal-)   normal-]   normal-,  a  normal-,  b   R    F_{X,Y}(a,b)=\mathbb{P}\left[X\leq a,Y\leq b\right]=\mathbb{P}\left[X^{-1}((-%
 \infty,a])\cap Y^{-1}((-\infty,b])\right],\qquad a,b\in\mathbb{R}   . The joint cumulative distribution function of ( X , Y ) is      A  =    X   -  1     (   (   -  ∞   ,  a  ]   )    ∈   ℐ  X         A     superscript  X    1         a          subscript  ℐ  X      A=X^{-1}((-\infty,a])\in\mathcal{I}_{X}   . However,    B  =    Y   -  1     (   (   -  ∞   ,  b  ]   )    ∈   ℐ  Y         B     superscript  Y    1         b          subscript  ℐ  Y      B=Y^{-1}((-\infty,b])\in\mathcal{I}_{Y}   and     ℐ   X  ,  Y    =   {   A  ∩  B   :    A  ∈   ℐ  X    ,   B  ∈   ℐ  Y     }        subscript  ℐ   X  Y     conditional-set    A  B    formulae-sequence    A   subscript  ℐ  X      B   subscript  ℐ  Y        \mathcal{I}_{X,Y}=\{A\cap B:A\in\mathcal{I}_{X},\,B\in\mathcal{I}_{Y}\}   . Since        (   X  t   )    t  ∈  T    ,    (   Y  t   )    t  ∈  T        subscript   subscript  X  t     t  T     subscript   subscript  Y  t     t  T      (X_{t})_{t\in T},(Y_{t})_{t\in T}   is a π-system generated by the random pair ( X , Y ), the π-λ theorem is used to show that the joint cumulative distribution function suffices to determine the joint law of ( X , Y ). In other words, ( X , Y ) and ( W , Z ) have the same distribution if and only if they have the same joint cumulative distribution function.  In the theory of stochastic processes, two processes       t  1   ,  …  ,   t  n    ∈  T   ,   n  ∈  ℕ      formulae-sequence      subscript  t  1   normal-…   subscript  t  n    T     n  ℕ     t_{1},\ldots,t_{n}\in T,\,n\in\mathbb{N}   are known to be equal in distribution if and only if they agree on all finite-dimensional distributions. i.e. for all     (   X   t  1    ,  …  ,   X   t  n    )    =  𝒟    (   Y   t  1    ,  …  ,   Y   t  n    )       superscript   𝒟     subscript  X   subscript  t  1    normal-…   subscript  X   subscript  t  n       subscript  Y   subscript  t  1    normal-…   subscript  Y   subscript  t  n       (X_{t_{1}},\ldots,X_{t_{n}})\stackrel{\mathcal{D}}{=}(Y_{t_{1}},\ldots,Y_{t_{n%
 }})   .      (  Ω  ,  ℱ  ,  ℙ  )     normal-Ω  ℱ  ℙ    (\Omega,\mathcal{F},\mathbb{P})   . The proof of this is another application of the π-λ theorem. 5  Independent Random Variables  The theory of π-system plays an important role in the probabilistic notion of independence . If X and Y are two random variables defined on the same probability space     ℐ  X   ,   ℐ  Y       subscript  ℐ  X    subscript  ℐ  Y     \mathcal{I}_{X},\mathcal{I}_{Y}   then the random variables are independent if and only if their π-systems       ℙ   [   A  ∩  B   ]    =   ℙ   [  A  ]   ℙ   [  B  ]     ,     ∀  A   ∈   ℐ  X    ,   B  ∈   ℐ  Y      ,     formulae-sequence      ℙ   delimited-[]    A  B       ℙ   delimited-[]  A   ℙ   delimited-[]  B      formulae-sequence     for-all  A    subscript  ℐ  X      B   subscript  ℐ  Y       \mathbb{P}\left[A\cap B\right]=\mathbb{P}\left[A\right]\mathbb{P}\left[B\right%
 ],\qquad\forall A\in\mathcal{I}_{X},\,B\in\mathcal{I}_{Y},   satisfy       ℐ  X   ,   ℐ  Y       subscript  ℐ  X    subscript  ℐ  Y     \mathcal{I}_{X},\mathcal{I}_{Y}   which is to say that    Z  =   (   Z  1   ,   Z  2   )       Z    subscript  Z  1    subscript  Z  2      Z=(Z_{1},Z_{2})   are independent. This actually is a special case of the use of π-systems for determining the distribution of ( X , Y ).  Example  Let      Z  1   ,   Z  2    ∼   𝒩   (  0  ,  1  )       similar-to    subscript  Z  1    subscript  Z  2      𝒩   0  1      Z_{1},Z_{2}\sim\mathcal{N}(0,1)   , where     R  =     Z  1  2   +   Z  2  2      ,   Θ  =    tan   -  1     (    Z  2   /   Z  1    )        formulae-sequence    R       superscript   subscript  Z  1   2    superscript   subscript  Z  2   2        normal-Θ    superscript     1       subscript  Z  2    subscript  Z  1        R=\sqrt{Z_{1}^{2}+Z_{2}^{2}},\qquad\Theta=\tan^{-1}(Z_{2}/Z_{1})   are iid standard normal random variables. Define the radius and argument (arctan) variables     R   R   R   . Then   Θ   normal-Θ   \Theta   and     ℐ  R   ,   ℐ  Θ       subscript  ℐ  R    subscript  ℐ  normal-Θ     \mathcal{I}_{R},\mathcal{I}_{\Theta}   are independent random variables.  To prove this, it is sufficient to show that the π-systems    ℙ   [  R  ≤  ρ  ,  Θ  ≤  θ  ]   =  ℙ   [  R  ≤  ρ  ]   ℙ   [  Θ  ≤  θ  ]   ∀  ρ  ∈   [  0  ,  ∞  )   ,  θ  ∈   [  0  ,  2  π  ]   .     fragments  P   fragments  normal-[  R   ρ  normal-,  Θ   θ  normal-]    P   fragments  normal-[  R   ρ  normal-]   P   fragments  normal-[  Θ   θ  normal-]    for-all  ρ    fragments  normal-[  0  normal-,   normal-)   normal-,  θ    fragments  normal-[  0  normal-,  2  π  normal-]   normal-.    \mathbb{P}[R\leq\rho,\Theta\leq\theta]=\mathbb{P}[R\leq\rho]\mathbb{P}[\Theta%
 \leq\theta]\quad\forall\rho\in[0,\infty),\,\theta\in[0,2\pi].   are independent: i.e.       ρ  ∈   [  0  ,  ∞  )    ,   θ  ∈   [  0  ,   2  π   ]       formulae-sequence    ρ   0       θ   0    2  π       \rho\in[0,\infty),\,\theta\in[0,2\pi]   Confirming that this is the case is an exercise in changing variables. Fix   Z   Z   Z   , then the probability can be expressed as an integral of the probability density function of    ℙ   [  R  ≤  ρ  ,  Θ  ≤  θ  ]      fragments  P   fragments  normal-[  R   ρ  normal-,  Θ   θ  normal-]     \displaystyle\mathbb{P}[R\leq\rho,\Theta\leq\theta]   .      σ   (   E  1   ,   E  2   ,  …  )       σ    subscript  E  1    subscript  E  2   normal-…     \sigma(E_{1},E_{2},\ldots)     See also   λ-systems  σ-algebra  Monotone class theorem  Probability distribution  Independence   Notes    References      "  Category:Measure theory  Category:Set families     Kallenberg, Foundations Of Modern Probability, p.2 ↩  Durrett, Probability Theory and Examples, p.404 ↩    Kallenberg, Foundations Of Modern probability, p. 48 ↩     