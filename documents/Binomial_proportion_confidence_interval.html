<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="652">Binomial proportion confidence interval</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Binomial proportion confidence interval</h1>
<hr/>
<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, a <strong>binomial proportion confidence interval</strong> is a <a href="confidence_interval" title="wikilink">confidence interval</a> for a proportion in a <a href="statistical_population" title="wikilink">statistical population</a>. It uses the proportion estimated in a <a href="statistical_sample" title="wikilink">statistical sample</a> and allows for <a href="sampling_error" title="wikilink">sampling error</a>. There are several formulas for a binomial confidence interval, but all of them rely on the assumption of a <a href="binomial_distribution" title="wikilink">binomial distribution</a>. In general, a binomial distribution applies when an experiment is repeated a fixed number of times, each trial of the experiment has two possible outcomes (labeled arbitrarily success and failure), the probability of success is the same for each trial, and the trials are <a href="statistically_independent" title="wikilink">statistically independent</a>.</p>
<p>A simple example of a binomial distribution is the set of various possible outcomes, and their probabilities, for the number of heads observed when a (not necessarily fair) coin is flipped ten times. The observed binomial proportion is the fraction of the flips which turn out to be heads. Given this observed proportion, the confidence interval for the true proportion innate in that coin is a range of possible proportions which may contain the true proportion. A 95% confidence interval for the proportion, for instance, will contain the true proportion 95% of the times that the procedure for constructing the confidence interval is employed.</p>
<p>There are several ways to compute a confidence interval for a binomial proportion. The normal approximation interval is the simplest formula, and the one introduced in most basic Statistics classes and textbooks. This formula, however, is based on an approximation that does not always work well. Several competing formulas are available that perform better, especially for situations with a small sample size and a proportion very close to zero or one. The choice of interval will depend on how important it is to use a simple and easy-to-explain interval versus the desire for better accuracy.</p>
<h2 id="normal-approximation-interval">Normal approximation interval</h2>
<p>The most commonly used formula for a binomial confidence interval relies on approximating the distribution of error about a binomially-distributed observation, <span class="LaTeX">$\hat p$</span>, with a <a href="normal_distribution" title="wikilink">normal distribution</a>. However, although this distribution is frequently confused with a <a href="binomial_distribution" title="wikilink">binomial distribution</a>, it should be noted that the error distribution itself is not binomial,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> and hence other methods (below) are preferred.</p>
<p>The approximation is usually justified by the <a href="central_limit_theorem" title="wikilink">central limit theorem</a>. The formula is</p>
<p><span class="LaTeX">$$\hat p \pm z \sqrt{\frac{1}{n}\hat p \left(1 - \hat p \right)}$$</span></p>
<p>where <span class="LaTeX">$\hat p$</span> is the proportion of successes in a <a href="Bernoulli_trial" title="wikilink">Bernoulli trial</a> process estimated from the statistical sample, <span class="LaTeX">$z$</span> is the <span class="LaTeX">$\scriptstyle 1 - \frac{1}{2}\alpha$</span> <a href="Percentile_rank" title="wikilink">percentile</a> of a <a href="standard_normal_distribution" title="wikilink">standard normal distribution</a>, <span class="LaTeX">$\alpha$</span> is the error percentile and <em>n</em> is the sample size. For example, for a 95% confidence level the error (<span class="LaTeX">$\alpha$</span>) is 5%, so <span class="LaTeX">$\scriptstyle 1 - \frac{1}{2}\alpha$</span> = 0.975 and <span class="LaTeX">$z$</span> = 1.96.</p>
<p>The <a href="central_limit_theorem" title="wikilink">central limit theorem</a> applies poorly to this distribution with a sample size less than 30 or where the proportion is close to 0 or 1. The normal approximation fails totally when the sample proportion is exactly zero or exactly one. A frequently cited rule of thumb is that the normal approximation is a reasonable one as long as <em>np</em> > 5 and <em>n</em>(1 − <em>p</em>) > 5; see Brown et al. 2001.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<p>An important theoretical derivation of this confidence interval involves the inversion of a hypothesis test. Under this formulation, the confidence interval represents those values of the population parameter that would have large <em>p</em>-values if they were tested as a hypothesized population proportion. The collection of values, <span class="LaTeX">$\theta$</span>, for which the normal approximation is valid can be represented as</p>
<p><span class="LaTeX">$$\left\{ \theta \bigg| y \le \frac{\hat p - \theta}{\sqrt{\frac{1}{n}\hat p \left(1 - \hat p\right)}} \le z \right\}$$</span></p>
<p>where <span class="LaTeX">$y$</span> is the <span class="LaTeX">$\scriptstyle \frac{1}{2}\alpha$</span> <a href="Percentile_rank" title="wikilink">percentile</a> of a <a href="standard_normal_distribution" title="wikilink">standard normal distribution</a>.</p>
<p>Since the test in the middle of the inequality is a <a href="Wald_test" title="wikilink">Wald test</a>, the normal approximation interval is sometimes called the <a href="Abraham_Wald" title="wikilink">Wald</a> interval, but <a href="Pierre-Simon_Laplace" title="wikilink">Pierre-Simon Laplace</a> first described it in his 1812 book <em>Théorie analytique des probabilités</em> (page 283).</p>
<h2 id="wilson-score-interval">Wilson score interval</h2>
<p>The Wilson interval is an improvement (the actual <a href="coverage_probability" title="wikilink">coverage probability</a> is closer to the nominal value) over the normal approximation interval and was first developed by <a href="Edwin_Bidwell_Wilson" title="wikilink">Edwin Bidwell Wilson</a> (1927).<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<p><span class="LaTeX">$$\frac{1}{1 + \frac{1}{n} z^2}
  \left[
    \hat p + \frac{1}{2n} z^2 \pm
    z \sqrt{
      \frac{1}{n}\hat p \left(1 - \hat p\right) +
      \frac{1}{4n^2}z^2
    } 
   \right]$$</span></p>
<p>This interval has good properties even for a small number of trials and/or an extreme probability.</p>
<p>These properties obtain from its derivation from the binomial model. Consider a binomial population probability <span class="LaTeX">$P$</span>, whose distribution may be approximated by the <a href="normal_distribution" title="wikilink">normal distribution</a> with standard deviation <span class="LaTeX">$\scriptstyle \sqrt{\frac{1}{n}P \left(1 - P \right)}$</span>. However, the distribution of true values about an observation is not binomial. Rather, an observation <span class="LaTeX">$\hat p$</span> will have an error interval with a lower bound equal to <span class="LaTeX">$P$</span> when <span class="LaTeX">$\hat p$</span> is at the equivalent normal interval upper bound (i.e. for the same <span class="LaTeX">$\alpha$</span>) of <span class="LaTeX">$P$</span>, and vice versa.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<p>The Wilson interval can also be derived from <a href="Pearson's_chi-squared_test" title="wikilink">Pearson's chi-squared test</a> with two categories. The resulting interval</p>
<p><span class="LaTeX">$$\left\{ \theta \bigg| y \le
  \frac{\hat p - \theta}{\sqrt{\frac{1}{n} \theta \left({1 - \theta} \right)}} \le
  z \right\}$$</span></p>
<p>can then be solved for <span class="LaTeX">$\theta$</span> to produce the Wilson interval. The test in the middle of the inequality is a <a href="score_test" title="wikilink">score test</a>, so the Wilson interval is sometimes called the Wilson score interval.</p>
<p>The center of the Wilson interval</p>
<p><span class="LaTeX">$$\frac
    {\hat p + \frac{1}{2n} z^2}
    {     1 + \frac{1}{n}  z^2}$$</span></p>
<p>can be shown to be a weighted average of <span class="LaTeX">$\hat p = \scriptstyle \frac{X}{n}$</span> and <span class="LaTeX">$\scriptstyle \frac{1}{2}$</span>, with <span class="LaTeX">$\hat p$</span> receiving greater weight as the sample size increases. For the 95% interval, the Wilson interval is nearly identical to the normal approximation interval using <span class="LaTeX">$\tilde p \,=\, \scriptstyle \frac{X + 2}{n + 4}$</span> instead of <span class="LaTeX">$\hat p$</span>.</p>
<h3 id="wilson-score-interval-with-continuity-correction">Wilson score interval with continuity correction</h3>
<p>The Wilson interval may be modified by employing a <a href="continuity_correction" title="wikilink">continuity correction</a>, in order to align the <em>minimum</em> <a href="coverage_probability" title="wikilink">coverage probability</a> (rather than the <em>average</em>) with the nominal value.</p>
<p>Just as the Wilson interval mirrors <a href="Pearson's_chi-squared_test" title="wikilink">Pearson's chi-squared test</a>, the Wilson interval with continuity correction mirrors the equivalent <a href="Yates's_correction_for_continuity" title="wikilink">Yates' chi-squared test</a>.</p>
<p>The following formulae for the lower and upper bounds of the Wilson score interval with continuity correction <span class="LaTeX">$( w^- , w^+ )$</span> are derived from Newcombe (1998).<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<p><span class="LaTeX">$$w^- = \operatorname{max}\left\{0, \frac { 2n\hat p + z^2 - [z \sqrt{z^2 - \frac{1}{n} + 4n\hat p(1 -\hat p)+(4\hat p - 2)}+1] }
               { 2(n+z^2) }\right\}$$</span></p>
<p><span class="LaTeX">$$w^+ = \operatorname{min}\left\{1, \frac { 2n\hat p + z^2 + [z \sqrt{z^2 - \frac{1}{n} + 4n\hat p(1 -\hat p)-(4\hat p - 2)}+1] }
               { 2(n+z^2) }\right\}$$</span></p>
<h2 id="jeffreys-interval">Jeffreys interval</h2>
<p>The <em>Jeffreys interval</em> has a Bayesian derivation, but it has good frequentist properties. In particular, it has coverage properties that are similar to the Wilson interval, but it is one of the few intervals with the advantage of being <em>equal-tailed</em> (e.g., for a 95% confidence interval, the probabilities of the interval lying above or below the true value are both close to 2.5%). In contrast, the Wilson interval has a systematic bias such that it is centred too close to p = 0.5.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<p>The Jeffreys interval is the Bayesian <a href="credible_interval" title="wikilink">credible interval</a> obtained when using the <a href="non-informative_prior" title="wikilink">non-informative</a> <a href="Jeffreys_prior" title="wikilink">Jeffreys prior</a> for the binomial proportion <span class="LaTeX">$p$</span>. The <a href="Jeffreys_prior#Bernoulli_trial" title="wikilink">Jeffreys prior for this problem</a> is a <a href="Beta_distribution" title="wikilink">Beta distribution</a> with parameters <span class="LaTeX">$(1/2, 1/2)$</span>. After observing <span class="LaTeX">$x$</span> successes in <span class="LaTeX">$n$</span> trials, the <a href="posterior_distribution" title="wikilink">posterior distribution</a> for <span class="LaTeX">$p$</span> is a Beta distribution with parameters <span class="LaTeX">$( x + 1/2, n – x + 1/2)$</span>.</p>
<p>When <span class="LaTeX">$x ≠0$</span> and <span class="LaTeX">$x ≠ n$</span>, the Jeffreys interval is taken to be the <span class="LaTeX">$100(1 – α )%$</span> equal-tailed posterior probability interval, i.e., the <span class="LaTeX">$α / 2$</span> and <span class="LaTeX">$1 – α / 2$</span> quantiles of a Beta distribution with parameters <span class="LaTeX">$( x + 1/2, n – x + 1/2)$</span>. These quantiles need to be computed numerically, although this is reasonably simple with modern statistical software.</p>
<p>In order to avoid the coverage probability tending to zero when <span class="LaTeX">$p → 0$</span> or <span class="LaTeX">$1$</span>, when <span class="LaTeX">$x = 0$</span> the upper limit is calculated as before but the lower limit is set to 0, and when <span class="LaTeX">$x = n$</span> the lower limit is calculated as before but the upper limit is set to 1.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h2 id="clopper-pearson-interval">Clopper-Pearson interval</h2>
<p>The Clopper-Pearson interval is an early and very common method for calculating binomial confidence intervals.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> This is often called an 'exact' method, but that is because it is based on the cumulative probabilities of the binomial distribution (i.e., exactly the correct distribution rather than an approximation), but the intervals are not exact in the way that one might assume: the discontinuous nature of the binomial distribution precludes any interval with exact coverage for all population proportions. The Clopper-Pearson interval can be written as</p>
<p><span class="LaTeX">$$S_{\le} \cap S_{\ge}
\mathrm{~~or~equivalently~~}
( \inf S_{\ge}\,,\, \sup S_{\le} )$$</span></p>
<p>with</p>
<p><span class="LaTeX">$$S_{\le} := \left\{ \theta \Big | P \left[ \mathrm{Bin}\left( n; \theta \right) \le X \right] > \frac{\alpha}{2} \right\}
\mathrm{~~and~~}
S_{\ge} := \left\{ \theta \Big | P \left[ \mathrm{Bin}\left( n; \theta \right) \ge X \right] > \frac{\alpha}{2} \right\},$$</span></p>
<p>where <em>0 ≤ X ≤ n</em> is the number of successes observed in the sample and Bin(<em>n</em>; θ) is a binomial random variable with <em>n</em> trials and probability of success θ.</p>
<p>Because of a relationship between the cumulative binomial distribution and the <a href="beta_distribution" title="wikilink">beta distribution</a>, the Clopper-Pearson interval is sometimes presented in an alternate format that uses quantiles from the beta distribution.</p>
<p><span class="LaTeX">$$B\left(\frac{\alpha}{2}; x, n - x + 1\right) < \theta <  B\left(1 - \frac{\alpha}{2}; x + 1, n - x\right)$$</span></p>
<p>where <em>x</em> is the number of successes, <em>n</em> is the number of trials, and <em>B</em>(<em>p</em>; <em>v</em>,<em>w</em>) is the <em>p</em>th <a href="Cumulative_distribution_function#Inverse_distribution_function_.28quantile_function.29" title="wikilink">quantile</a> from a beta distribution with shape parameters <em>v</em> and <em>w</em>. The beta distribution is, in turn, related to the <a class="uri" href="F-distribution" title="wikilink">F-distribution</a> so a third formulation of the Clopper-Pearson interval can be written using F percentiles:</p>
<p><span class="LaTeX">$$\left( 1 + \frac{n - x + 1}{x\,\,F\!\left[1 - \frac{1}{2}\alpha; 2x, 2(n - x + 1)\right]} \right)^{-1}<
  \theta <
  \left( 1 + \frac{n - x}{\left[x + 1\right]\,F\!\left[\frac{\alpha}{2}; 2(x + 1), 2(n - x)\right]} \right)^{-1}$$</span></p>
<p>where <em>x</em> is the number of successes, <em>n</em> is the number of trials, and <em>F(c; d1, d2)</em> is the <em>1 - c</em> quantile from an F-distribution with <em>d1</em> and <em>d2</em> degrees of freedom.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<p>The Clopper-Pearson interval is an exact interval since it is based directly on the binomial distribution rather than any approximation to the binomial distribution. This interval never has less than the nominal coverage for any population proportion, but that means that it is usually conservative. For example, the true coverage rate of a 95% Clopper-Pearson interval may be well above 95%, depending on n and θ. Thus the interval may be wider than it needs to be to achieve 95% confidence. In contrast, it is worth noting that other confidence bounds may be narrower than their nominal confidence with, i.e., the Normal Approximation (or "Standard") Interval, Wilson Interval,<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> Agresti-Coull Interval,<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> etc., with a nominal coverage of 95% may in fact cover less than 95%.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></p>
<h2 id="agresti-coull-interval">Agresti-Coull Interval</h2>
<p>The Agresti-Coull interval is also another approximate binomial confidence interval.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></p>
<p>Given <span class="LaTeX">$X$</span> successes in <span class="LaTeX">$n$</span> trials, define</p>
<p><span class="LaTeX">$$\tilde{n} = n + z^2$$</span></p>
<p>and</p>
<p><span class="LaTeX">$$\tilde{p} = \frac{1}{\tilde{n}}\left(X + \frac{1}{2}z^2\right)$$</span></p>
<p>Then, a confidence interval for <span class="LaTeX">$p$</span> is given by</p>
<p><span class="LaTeX">$$\tilde{p} \pm z
    \sqrt{\frac{1}{\tilde{n}}\tilde{p}\left(1 - \tilde{p} \right)}$$</span></p>
<p>where <span class="LaTeX">$z$</span> is the <span class="LaTeX">$1 - \frac{1}{2}\alpha$</span> percentile of a standard normal distribution, as before. For example, for a 95% confidence interval, let <span class="LaTeX">$\alpha = 0.05$</span>, so <span class="LaTeX">$z$</span> = 1.96 and <span class="LaTeX">$z^2$</span> = 3.84. If we use 2 instead of 1.96 for <span class="LaTeX">$z$</span>, this is the "add 2 successes and 2 failures" interval in <a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></p>
<h2 id="arcsine-transformation">Arcsine transformation</h2>
<p>Let <em>X</em> be the number of successes in <em>n</em> trials and let <em>p</em> = <em>X</em>/<em>n</em>. The variance of <em>p</em> is</p>
<p><span class="LaTeX">$$var(p) = \frac{ p ( 1 - p ) }{ n }$$</span></p>
<p>Using the arc sine transform the variance of the arcsine of <em>p</em> is<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>
<p><span class="LaTeX">$$var(\arcsin( \sqrt { p } ) ) \approx \frac{ var( p ) }{ 4 p( 1 - p ) } = \frac{ p( 1 - p ) }{ 4n p( 1 - p ) } = \frac{ 1 }{ 4n }$$</span></p>
<p>So, confidence interval itself has the following form:</p>
<p><span class="LaTeX">$$\sin^2\left(\arcsin(\sqrt{p}) - \frac{z}{2\sqrt{n}}\right) < \theta < \sin^2\left(\arcsin(\sqrt{p}) + \frac{z}{2\sqrt{n}}\right)$$</span></p>
<p>where <span class="LaTeX">$z$</span> is <span class="LaTeX">$1 - \alpha/2$</span> percentile of a standard normal distribution</p>
<p>This method may be used to estimate the variance of <em>p</em> but its use is problematic when <em>p</em> is close to 0 or 1.</p>
<h2 id="ta-transform">t<sub>a</sub> transform</h2>
<p>Let <em>p</em> be the proportion of successes. For 0 ≤ a ≤ 2</p>
<p><span class="LaTeX">$$t_{ a } = \log\left( \frac{ p^{ a } }{ ( 1 - p )^{ 2 - a } } \right) = a \log( p ) - ( 2 - a )\log( 1 - p )$$</span></p>
<p>This family is a generalisation of the logit transform which is a special case with <em>a</em> = 1 and can be used to transform a proportional data distribution to an approximately <a href="normal_distribution" title="wikilink">normal distribution</a>. The parameter <em>a</em> has to be estimated for the data set.</p>
<h2 id="special-cases">Special cases</h2>
<p>In medicine, the <a href="Rule_of_three_(medicine)" title="wikilink">rule of three</a> is used to provide a simple way of stating an approximate 95% confidence interval for <em>p</em>, in the special case that no successes (<span class="LaTeX">$\hat p = 0$</span>) have been observed.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> The interval is .</p>
<p>By symmetry, one could expect for only successes (<span class="LaTeX">$\hat p = 1$</span>), the interval is .</p>
<h2 id="comparison-of-different-intervals">Comparison of different intervals</h2>
<p>There are several research papers that compare these and other confidence intervals for the binomial proportion.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a><a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a><a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a><a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> Both Agresti and Coull (1998)<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> and Ross (2003)<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a> point out that exact methods such as the Clopper-Pearson interval may not work as well as certain approximations.</p>
<p>Many of these intervals can be calculated in <a href="R_(programming_language)" title="wikilink">R</a> using the <a href="http://cran.r-project.org/web/packages/binom/index.html">binom</a> package.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Coverage_probability" title="wikilink">Coverage probability</a></li>
<li><a href="Estimation_theory" title="wikilink">Estimation theory</a></li>
</ul>
<h2 id="references">References</h2>
<p>"</p>
<p><a href="Category:Statistical_theory" title="wikilink">Category:Statistical theory</a> <a href="Category:Statistical_approximations" title="wikilink">Category:Statistical approximations</a> <a href="Category:Statistical_intervals" title="wikilink">Category:Statistical intervals</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"></li>
<li id="fn6">Cai TT. One-sided confidence intervals in discrete distributions. Journal of Statistical Planning and Inference 2005;131:63-88.<a href="#fnref6">↩</a></li>
<li id="fn7"></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"></li>
<li id="fn10"></li>
<li id="fn11"><a href="#fnref11">↩</a></li>
<li id="fn12"></li>
<li id="fn13"></li>
<li id="fn14"></li>
<li id="fn15">Shao J (1998) Mathematical statistics. Springer. New York, New York, USA<a href="#fnref15">↩</a></li>
<li id="fn16">Steve Simon (2010) <a href="http://www.pmean.com/01/zeroevents.html">"Confidence interval with zero events"</a>, The Children's Mercy Hospital, Kansas City, Mo. (website: "Ask Professor Mean at <a href="http://www.childrensmercy.org/stats/">Stats topics or Medical Research</a>)<a href="#fnref16">↩</a></li>
<li id="fn17"></li>
<li id="fn18"></li>
<li id="fn19"></li>
<li id="fn20"></li>
<li id="fn21"></li>
<li id="fn22"></li>
</ol>
</section>
</body>
</html>
