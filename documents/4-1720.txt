   Dirichlet distribution      Dirichlet distribution  table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
   margin: 0; padding: 0; vertical-align: baseline; border: none; }
 <style>
 table.sourceCode { width: 100%; line-height: 100%; }
 td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
 td.sourceCode { padding-left: 5px; }
 code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
 code > span.dt { color: #902000; } /* DataType */
 code > span.dv { color: #40a070; } /* DecVal */
 code > span.bn { color: #40a070; } /* BaseN */
 code > span.fl { color: #40a070; } /* Float */
 code > span.ch { color: #4070a0; } /* Char */
 code > span.st { color: #4070a0; } /* String */
 code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
 code > span.ot { color: #007020; } /* Other */
 code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
 code > span.fu { color: #06287e; } /* Function */
 code > span.er { color: #ff0000; font-weight: bold; } /* Error */
 code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 code > span.cn { color: #880000; } /* Constant */
 code > span.sc { color: #4070a0; } /* SpecialChar */
 code > span.vs { color: #4070a0; } /* VerbatimString */
 code > span.ss { color: #bb6688; } /* SpecialString */
 code > span.im { } /* Import */
 code > span.va { color: #19177c; } /* Variable */
 code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
 code > span.op { color: #666666; } /* Operator */
 code > span.bu { } /* BuiltIn */
 code > span.ex { } /* Extension */
 code > span.pp { color: #bc7a00; } /* Preprocessor */
 code > span.at { color: #7d9029; } /* Attribute */
 code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
 code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
 code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
 code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */     In probability and statistics , the Dirichlet distribution (after Peter Gustav Lejeune Dirichlet ), often denoted    Dir   (  𝜶  )      Dir  𝜶    \operatorname{Dir}(\boldsymbol{\alpha})   , is a family of continuous  multivariate  probability distributions parameterized by a vector   𝜶   𝜶   \boldsymbol{\alpha}   of positive reals . It is the multivariate generalization of the beta distribution . 1 Dirichlet distributions are very often used as prior distributions in Bayesian statistics , and in fact the Dirichlet distribution is the conjugate prior of the categorical distribution and multinomial distribution . That is, its probability density function returns the belief that the probabilities of K rival events are    x  i     subscript  x  i    x_{i}   given that each event has been observed     α  i   -  1       subscript  α  i   1    \alpha_{i}-1   times.  The infinite-dimensional generalization of the Dirichlet distribution is the Dirichlet process .  Probability density function  (Figure)  Illustrating how the log of the density function changes when K =3 as we change the vector α from α =(0.3, 0.3, 0.3) to (2.0, 2.0, 2.0), keeping all the individual    α  i     subscript  α  i    \alpha_{i}   's equal to each other.   The Dirichlet distribution of order K ≥ 2 with parameters α 1 , ..., α K > 0 has a probability density function with respect to Lebesgue measure on the Euclidean space  R K −1 given by        f   (   x  1   ,  ⋯  ,   x  K   ;   α  1   ,  ⋯  ,   α  K   )    =    1   B   (  α  )       ∏   i  =  1   K    x  i    α  i   -  1       ,        f    subscript  x  1   normal-⋯   subscript  x  K    subscript  α  1   normal-⋯   subscript  α  K         1    normal-B  α      superscript   subscript  product    i  1    K    superscript   subscript  x  i      subscript  α  i   1        f\left(x_{1},\cdots,x_{K};\alpha_{1},\cdots,\alpha_{K}\right)=\frac{1}{\mathrm%
 {B}(\alpha)}\prod_{i=1}^{K}x_{i}^{\alpha_{i}-1},     on the open ( K − 1)-dimensional simplex defined by:      x   ;  1   ,  ⋯  ,   x   K  -  1    >  0     fragments  x   subscript  normal-;  1   normal-,  normal-⋯  normal-,   subscript  x    K  1     0    \displaystyle x;_{1},\cdots,x_{K-1}>0     and zero elsewhere.  The normalizing constant is the multinomial Beta function , which can be expressed in terms of the gamma function :         B   (  𝜶  )    =     ∏   i  =  1   K    Γ   (   α  i   )      Γ   (    ∑   i  =  1   K    α  i    )      ,   𝜶  =   (   α  1   ,  ⋯  ,   α  K   )     .     formulae-sequence      normal-B  𝜶       superscript   subscript  product    i  1    K     normal-Γ   subscript  α  i       normal-Γ    superscript   subscript     i  1    K    subscript  α  i         𝜶    subscript  α  1   normal-⋯   subscript  α  K       \mathrm{B}(\boldsymbol{\alpha})=\frac{\prod_{i=1}^{K}\Gamma(\alpha_{i})}{%
 \Gamma\left(\sum_{i=1}^{K}\alpha_{i}\right)},\qquad\boldsymbol{\alpha}=(\alpha%
 _{1},\cdots,\alpha_{K}).     Support  The support of the Dirichlet distribution is the set of K -dimensional vectors   𝒙   𝒙   \boldsymbol{x}   whose entries are real numbers in the interval (0,1); furthermore,      ∥  𝒙  ∥   1   =  1       subscript   norm  𝒙   1   1    \|\boldsymbol{x}\|_{1}=1   , i.e. the sum of the coordinates is 1. These can be viewed as the probabilities of a K -way categorical event. Another way to express this is that the domain of the Dirichlet distribution is itself a set of probability distributions , specifically the set of K -dimensional discrete distributions . Note that the technical term for the set of points in the support of a K -dimensional Dirichlet distribution is the open  standard ( K −1)-simplex , 2 which is a generalization of a triangle , embedded in the next-higher dimension. For example, with K =3, the support is an equilateral triangle embedded in a downward-angle fashion in three-dimensional space, with vertices at (1,0,0), (0,1,0) and (0,0,1), i.e. touching each of the coordinate axes at a point 1 unit away from the origin.  Special cases  A very common special case is the symmetric Dirichlet distribution , where all of the elements making up the parameter vector   𝜶   𝜶   \boldsymbol{\alpha}   have the same value. Symmetric Dirichlet distributions are often used when a Dirichlet prior is called for, since there typically is no prior knowledge favoring one component over another. Since all elements of the parameter vector have the same value, the distribution alternatively can be parametrized by a single scalar value α , called the concentration parameter . The density function then simplifies to        f   (   x  1   ,  …  ,   x   K  -  1    ;  α  )    =     Γ   (   α  K   )     Γ    (  α  )   K       ∏   i  =  1   K    x  i   α  -  1       .        f    subscript  x  1   normal-…   subscript  x    K  1    α          normal-Γ    α  K      normal-Γ   superscript  α  K       superscript   subscript  product    i  1    K    superscript   subscript  x  i     α  1        f(x_{1},\dots,x_{K-1};\alpha)=\frac{\Gamma(\alpha K)}{\Gamma(\alpha)^{K}}\prod%
 _{i=1}^{K}x_{i}^{\alpha-1}.     When α =1, the symmetric Dirichlet distribution is equivalent to a uniform distribution over the open standard ( K −1)-simplex , i.e. it is uniform over all points in its support . This particular distribution is known as the flat Dirichlet distribution . Values of the concentration parameter above 1 prefer variates that are dense, evenly distributed distributions, i.e. all the values within a single sample are similar to each other. Values of the concentration parameter below 1 prefer sparse distributions, i.e. most of the values within a single sample will be close to 0, and the vast majority of the mass will be concentrated in a few of the values.  More generally, the parameter vector is sometimes written as the product    α  𝒏      α  𝒏    \alpha\boldsymbol{n}   of a ( scalar ) concentration parameter  α and a ( vector ) base measure     𝒏  =   (   n  1   ,  …  ,   n  K   )       𝒏    subscript  n  1   normal-…   subscript  n  K      \boldsymbol{n}=(n_{1},\dots,n_{K})   where   𝒏   𝒏   \boldsymbol{n}   lies within the ( K −1)-simplex (i.e.: its coordinates    n  i     subscript  n  i    n_{i}   sum to one). The concentration parameter in this case is larger by a factor of K than the concentration parameter for a symmetric Dirichlet distribution described above. This construction ties in with concept of a base measure when discussing Dirichlet processes and is often used in the topic modelling literature.     If we define the concentration parameter as the sum of the Dirichlet parameters for each dimension, the Dirichlet distribution with concentration parameter K , the dimension of the distribution, is the uniform distribution on the K −1 simplex.     Properties  Moments  Let    X  =   (   X  1   ,  …  ,   X  K   )   ∼   Dir   (  α  )          X    subscript  X  1   normal-…   subscript  X  K      similar-to     Dir  α      X=(X_{1},\ldots,X_{K})\sim\operatorname{Dir}(\alpha)   , meaning that the first K – 1 components have the above density and     X  K   =   1  -    ∑   i  =  1    K  -  1     X  i          subscript  X  K     1    superscript   subscript     i  1      K  1     subscript  X  i       X_{K}=1-\sum_{i=1}^{K-1}X_{i}   .  Let        α  0   =    ∑   i  =  1   K    α  i     .       subscript  α  0     superscript   subscript     i  1    K    subscript  α  i      \alpha_{0}=\sum_{i=1}^{K}\alpha_{i}.     Then 3 4        E   [   X  i   ]    =    α  i    α  0     ,        normal-E   delimited-[]   subscript  X  i        subscript  α  i    subscript  α  0      \mathrm{E}[X_{i}]=\frac{\alpha_{i}}{\alpha_{0}},           Var   [   X  i   ]    =     α  i    (    α  0   -   α  i    )      α  0  2    (    α  0   +  1   )      .        Var   delimited-[]   subscript  X  i          subscript  α  i      subscript  α  0    subscript  α  i        superscript   subscript  α  0   2      subscript  α  0   1       \mathrm{Var}[X_{i}]=\frac{\alpha_{i}(\alpha_{0}-\alpha_{i})}{\alpha_{0}^{2}(%
 \alpha_{0}+1)}.     Furthermore, if    i  ≠  j      i  j    i\neq j           Cov   [   X  i   ,   X  j   ]    =    -    α  i    α  j       α  0  2    (    α  0   +  1   )      .        Cov    subscript  X  i    subscript  X  j            subscript  α  i    subscript  α  j        superscript   subscript  α  0   2      subscript  α  0   1       \mathrm{Cov}[X_{i},X_{j}]=\frac{-\alpha_{i}\alpha_{j}}{\alpha_{0}^{2}(\alpha_{%
 0}+1)}.     Note that the matrix so defined is singular .  More generally, moments of Dirichlet-distributed random variables can be expressed as 5        E   [    ∏   i  =  1   K    x  i   β  i     ]    =    B   (   𝜶  +  𝜷   )     B   (  𝜶  )     =     Γ   (    ∑   i  =  1   K    α  i    )     Γ   (     ∑   i  =  1   K    α  i    +   β  i    )     ×    ∏   i  =  1   K     Γ   (    α  i   +   β  i    )     Γ   (   α  i   )        .          E   delimited-[]    superscript   subscript  product    i  1    K    superscript   subscript  x  i    subscript  β  i           B    𝜶  𝜷      B  𝜶               normal-Γ    superscript   subscript     i  1    K    subscript  α  i       normal-Γ      superscript   subscript     i  1    K    subscript  α  i     subscript  β  i        superscript   subscript  product    i  1    K       normal-Γ     subscript  α  i    subscript  β  i       normal-Γ   subscript  α  i          E\left[\prod_{i=1}^{K}x_{i}^{\beta_{i}}\right]=\frac{B\left(\boldsymbol{\alpha%
 }+\boldsymbol{\beta}\right)}{B\left(\boldsymbol{\alpha}\right)}=\frac{\Gamma%
 \left(\sum_{i=1}^{K}\alpha_{i}\right)}{\Gamma\left(\sum_{i=1}^{K}\alpha_{i}+%
 \beta_{i}\right)}\times\prod_{i=1}^{K}\frac{\Gamma\left(\alpha_{i}+\beta_{i}%
 \right)}{\Gamma\left(\alpha_{i}\right)}.     Mode  The mode of the distribution is 6 the vector ( x 1 , ..., x K ) with        x  i   =     α  i   -  1     α  0   -  K     ,    α  i   >  1.      formulae-sequence     subscript  x  i        subscript  α  i   1      subscript  α  0   K        subscript  α  i   1.     x_{i}=\frac{\alpha_{i}-1}{\alpha_{0}-K},\qquad\alpha_{i}>1.     Marginal distributions  The marginal distributions are beta distributions : 7        X  i   ∼   Beta   (   α  i   ,    (    ∑   j  =  1   K    α  j    )   -   α  i    )     .     similar-to   subscript  X  i    Beta   subscript  α  i       superscript   subscript     j  1    K    subscript  α  j     subscript  α  i       X_{i}\sim\operatorname{Beta}\left(\alpha_{i},\left(\sum\nolimits_{j=1}^{K}{%
 \alpha_{j}}\right)-\alpha_{i}\right).     Conjugate to categorical/multinomial  The Dirichlet distribution is the conjugate prior distribution of the categorical distribution (a generic discrete probability distribution with a given number of possible outcomes) and multinomial distribution (the distribution over observed counts of each possible category in a set of categorically distributed observations). This means that if a data point has either a categorical or multinomial distribution, and the prior distribution of the distribution's parameter (the vector of probabilities that generates the data point) is distributed as a Dirichlet, then the posterior distribution of the parameter is also a Dirichlet. Intuitively, in such a case, starting from what we know about the parameter prior to observing the data point, we then can update our knowledge based on the data point and end up with a new distribution of the same form as the old one. This means that we can successively update our knowledge of a parameter by incorporating new observations one at a time, without running into mathematical difficulties.  Formally, this can be expressed as follows. Given a model        𝜶    =     (   α  1   ,  ⋯  ,   α  K   )     =    concentration hyperparameter       𝐩  ∣  𝜶     =     (   p  1   ,  ⋯  ,   p  K   )     ∼     Dir   (  K  ,  𝜶  )         𝕏  ∣  𝐩     =     (   𝐱  1   ,  ⋯  ,   𝐱  K   )     ∼     Cat   (  K  ,  𝐩  )          𝜶     subscript  α  1   normal-⋯   subscript  α  K     concentration hyperparameter     fragments  p  normal-∣  α      subscript  p  1   normal-⋯   subscript  p  K    similar-to   Dir  K  𝜶      fragments  X  normal-∣  p      subscript  𝐱  1   normal-⋯   subscript  𝐱  K    similar-to   Cat  K  𝐩      \begin{array}[]{rcccl}\boldsymbol{\alpha}&=&\left(\alpha_{1},\cdots,\alpha_{K}%
 \right)&=&\text{concentration hyperparameter}\\
 \mathbf{p}\mid\boldsymbol{\alpha}&=&\left(p_{1},\cdots,p_{K}\right)&\sim&%
 \operatorname{Dir}(K,\boldsymbol{\alpha})\\
 \mathbb{X}\mid\mathbf{p}&=&\left(\mathbf{x}_{1},\cdots,\mathbf{x}_{K}\right)&%
 \sim&\operatorname{Cat}(K,\mathbf{p})\end{array}     then the following holds:        𝐜    =     (   c  1   ,  ⋯  ,   c  K   )     =     number of occurrences of category  i        𝐩  ∣  𝕏  ,  𝜶     ∼     Dir   (  K  ,   𝐜  +  𝜶   )      =     Dir   (  K  ,    c  1   +   α  1    ,  ⋯  ,    c  K   +   α  K    )          𝐜     subscript  c  1   normal-⋯   subscript  c  K       number of occurrences of category  i      fragments  p  normal-∣  X  normal-,  α   similar-to   Dir  K    𝐜  𝜶      Dir  K     subscript  c  1    subscript  α  1    normal-⋯     subscript  c  K    subscript  α  K        \begin{array}[]{rcccl}\mathbf{c}&=&\left(c_{1},\cdots,c_{K}\right)&=&\text{%
 number of occurrences of category }i\\
 \mathbf{p}\mid\mathbb{X},\boldsymbol{\alpha}&\sim&\operatorname{Dir}(K,\mathbf%
 {c}+\boldsymbol{\alpha})&=&\operatorname{Dir}\left(K,c_{1}+\alpha_{1},\cdots,c%
 _{K}+\alpha_{K}\right)\end{array}     This relationship is used in Bayesian statistics to estimate the underlying parameter p of a categorical distribution given a collection of N samples. Intuitively, we can view the hyperprior vector α as pseudocounts , i.e. as representing the number of observations in each category that we have already seen. Then we simply add in the counts for all the new observations (the vector c ) in order to derive the posterior distribution.  In Bayesian mixture models and other hierarchical Bayesian models with mixture components, Dirichlet distributions are commonly used as the prior distributions for the categorical variables appearing in the models. See the section on applications below for more information.  Relation to Dirichlet-multinomial distribution  In a model where a Dirichlet prior distribution is placed over a set of categorical-valued observations, the marginal  joint distribution of the observations (i.e. the joint distribution of the observations, with the prior parameter marginalized out ) is a Dirichlet-multinomial distribution . This distribution plays an important role in hierarchical Bayesian models , because when doing inference over such models using methods such as Gibbs sampling or variational Bayes , Dirichlet prior distributions are often marginalized out. See the article on this distribution for more details.  Entropy  If X is a Dir( α ) random variable, then the exponential family differential identities can be used to get an analytic expression for the expectation of    log   (   X  i   )        subscript  X  i     \log(X_{i})   and its associated covariance matrix:       E   [   log   (   X  i   )    ]    =    ψ   (   α  i   )    -   ψ   (   α  0   )          normal-E     subscript  X  i         ψ   subscript  α  i      ψ   subscript  α  0       \operatorname{E}[\log(X_{i})]=\psi(\alpha_{i})-\psi(\alpha_{0})     and       Cov   [   log   (   X  i   )    ,   log   (   X  j   )    ]    =     ψ  ′    (   α  i   )    δ   i  j     -    ψ  ′    (   α  0   )          Cov     subscript  X  i       subscript  X  j          superscript  ψ  normal-′    subscript  α  i    subscript  δ    i  j        superscript  ψ  normal-′    subscript  α  0       \operatorname{Cov}[\log(X_{i}),\log(X_{j})]=\psi^{\prime}(\alpha_{i})\delta_{%
 ij}-\psi^{\prime}(\alpha_{0})     where   ψ   ψ   \psi   is the digamma function ,    ψ  ′     superscript  ψ  normal-′    \psi^{\prime}   is the trigamma function , and    δ   i  j      subscript  δ    i  j     \delta_{ij}   is the Kronecker delta . The formula for    E   [   log   (   X  i   )    ]      normal-E     subscript  X  i      \operatorname{E}[\log(X_{i})]   yields the following formula for the information entropy of X :       H   (  X  )    =      log  B    (  𝜶  )    +    (    α  0   -  K   )   ψ   (   α  0   )     -    ∑   j  =  1   K     (    α  j   -  1   )   ψ   (   α  j   )            H  X           normal-B   𝜶        subscript  α  0   K   ψ   subscript  α  0       superscript   subscript     j  1    K        subscript  α  j   1   ψ   subscript  α  j        H(X)=\log\mathrm{B}(\boldsymbol{\alpha})+(\alpha_{0}-K)\psi(\alpha_{0})-\sum_{%
 j=1}^{K}(\alpha_{j}-1)\psi(\alpha_{j})     The spectrum of Rényi information for values other than    λ  =  1      λ  1    \lambda=1   is given by 8        F  R    (  λ  )    =     (   1  -  λ   )    -  1     (     -    log  B    (  α  )     +    ∑   j  =  1   K     log  Γ    (    λ   (    α  i   -  1   )    +  1   )      -    log  Γ    (    λ   (    α  0   -  d   )    +  d   )     )           subscript  F  R   λ      superscript    1  λ     1              normal-B   α      superscript   subscript     j  1    K       normal-Γ       λ     subscript  α  i   1    1          normal-Γ       λ     subscript  α  0   d    d        F_{R}(\lambda)=(1-\lambda)^{-1}\left(-\log\mathrm{B}(\alpha)+\sum_{j=1}^{K}%
 \log\Gamma(\lambda(\alpha_{i}-1)+1)-\log\Gamma(\lambda(\alpha_{0}-d)+d)\right)   and the information entropy is the limit as   λ   λ   \lambda   goes to 1.  Aggregation  If      X  =   (   X  1   ,  ⋯  ,   X  K   )   ∼   Dir   (   α  1   ,  ⋯  ,   α  K   )          X    subscript  X  1   normal-⋯   subscript  X  K      similar-to     Dir   subscript  α  1   normal-⋯   subscript  α  K       X=(X_{1},\cdots,X_{K})\sim\operatorname{Dir}(\alpha_{1},\cdots,\alpha_{K})     then, if the random variables with subscripts i and j are dropped from the vector and replaced by their sum,        X  ′   =   (   X  1   ,  ⋯  ,    X  i   +   X  j    ,  ⋯  ,   X  K   )   ∼   Dir   (   α  1   ,  ⋯  ,    α  i   +   α  j    ,  ⋯  ,   α  K   )     .         superscript  X  normal-′     subscript  X  1   normal-⋯     subscript  X  i    subscript  X  j    normal-⋯   subscript  X  K      similar-to     Dir   subscript  α  1   normal-⋯     subscript  α  i    subscript  α  j    normal-⋯   subscript  α  K       X^{\prime}=(X_{1},\cdots,X_{i}+X_{j},\cdots,X_{K})\sim\operatorname{Dir}\left(%
 \alpha_{1},\cdots,\alpha_{i}+\alpha_{j},\cdots,\alpha_{K}\right).     This aggregation property may be used to derive the marginal distribution of    X  i     subscript  X  i    X_{i}   mentioned above.  Neutrality  If    X  =   (   X  1   ,  …  ,   X  K   )   ∼   Dir   (  α  )          X    subscript  X  1   normal-…   subscript  X  K      similar-to     Dir  α      X=(X_{1},\ldots,X_{K})\sim\operatorname{Dir}(\alpha)   , then the vector X is said to be neutral 9 in the sense that X K is independent of    X   (   -  K   )      superscript  X    K     X^{(-K)}    10 where        X   (   -  K   )    =   (    X  1    1  -   X  K     ,    X  2    1  -   X  K     ,  ⋯  ,    X   K  -  1     1  -   X  K     )    ,       superscript  X    K        subscript  X  1     1   subscript  X  K        subscript  X  2     1   subscript  X  K     normal-⋯     subscript  X    K  1      1   subscript  X  K        X^{(-K)}=\left(\frac{X_{1}}{1-X_{K}},\frac{X_{2}}{1-X_{K}},\cdots,\frac{X_{K-1%
 }}{1-X_{K}}\right),     and similarly for removing any of     X  2   ,  …  ,   X   K  -  1        subscript  X  2   normal-…   subscript  X    K  1      X_{2},\ldots,X_{K-1}   . Observe that any permutation of X is also neutral (a property not possessed by samples drawn from a generalized Dirichlet distribution .) 11  Characteristic function  The characteristic function of the Dirichlet distribution is a confluent form of the Lauricella hypergeometric series . It is given by Phillips 12 as       C  F   (   s  1   ,  …  ,   s   k  -  1    )    =   𝔼   (   e   i   (     s  1    x  1    +  ⋯  +    s   k  -  1     x   k  -  1      )     )    =    Ψ   [   k  -  1   ]     (   α  1   ,  …  ,   α  k   ;  α  ;   i   s  1    ,   …  i   s   k  -  1     )            C  F    subscript  s  1   normal-…   subscript  s    k  1        𝔼   superscript  e    i       subscript  s  1    subscript  x  1    normal-⋯     subscript  s    k  1     subscript  x    k  1                 superscript  normal-Ψ   delimited-[]    k  1       subscript  α  1   normal-…   subscript  α  k   α    i   subscript  s  1      normal-…  i   subscript  s    k  1          CF\left(s_{1},\ldots,s_{k-1}\right)=\mathbb{E}\left(e^{i\left(s_{1}x_{1}+%
 \cdots+s_{k-1}x_{k-1}\right)}\right)=\Psi^{\left[k-1\right]}\left(\alpha_{1},%
 \ldots,\alpha_{k};\alpha;is_{1},\ldots is_{k-1}\right)   where         Ψ   [  m  ]     (   a  1   ,  …  ,   a  m   ;  c  ;   z  1   ,   …   z  m    )    =   ∑      (  a  )    k  1    ⋯     (   a  m   )    k  m      z  1   a  1    ⋯   z  m   a  m         (  c  )   k      k  1   !   ⋯    k  m   !       .         superscript  normal-Ψ   delimited-[]  m      subscript  a  1   normal-…   subscript  a  m   c   subscript  z  1     normal-…   subscript  z  m             subscript  a   subscript  k  1    normal-⋯   subscript   subscript  a  m    subscript  k  m     superscript   subscript  z  1    subscript  a  1    normal-⋯   superscript   subscript  z  m    superscript  a  m        subscript  c  k      subscript  k  1    normal-⋯     subscript  k  m         \Psi^{\left[m\right]}\left(a_{1},\ldots,a_{m};c;z_{1},\ldots z_{m}\right)=\sum%
 \frac{\left(a\right)_{k_{1}}\cdots\left(a_{m}\right)_{k_{m}}\,z_{1}^{a_{1}}%
 \cdots z_{m}^{a^{m}}}{\left(c\right)_{k}\,k_{1}!\cdots k_{m}!}.   The sum is over non-negative integers     k  1   ,  …  ,   k  m       subscript  k  1   normal-…   subscript  k  m     k_{1},\ldots,k_{m}   and    k  =    k  1   +  ⋯  +   k  m        k     subscript  k  1   normal-⋯   subscript  k  m      k=k_{1}+\cdots+k_{m}   . Phillips goes on to state that this form is "inconvenient for numerical calculation" and gives an alternative in terms of a complex path integral :       Ψ   [  m  ]    =     Γ   (  c  )     2  π  i      ∫  L      e  t      t     a  1   +  ⋯  +   a  m    -  c       ∏   j  =  1   m       (   t  -   z  j    )    -   a  j      d  t            superscript  normal-Ψ   delimited-[]  m          normal-Γ  c     2  π  i      subscript   L      superscript  e  t    superscript  t       subscript  a  1   normal-⋯   subscript  a  m    c      superscript   subscript  product    j  1    m      superscript    t   subscript  z  j       subscript  a  j     d  t         \Psi^{\left[m\right]}=\frac{\Gamma(c)}{2\pi i}\int_{L}e^{t}\,t^{a_{1}+\cdots+a%
 _{m}-c}\,\prod_{j=1}^{m}\left(t-z_{j}\right)^{-a_{j}}\,dt   where L denotes any path in the complex plane originating at    -  ∞         -\infty   , encircling in the positive direction all the singularities of the integrand and returning to    -  ∞         -\infty   .  Related distributions  For K independently distributed Gamma distributions:        Y  1   ∼    Gamma   (   α  1   ,  θ  )    ,  ⋯    ,    Y  K   ∼   Gamma   (   α  K   ,  θ  )        formulae-sequence   similar-to   subscript  Y  1     Gamma   subscript  α  1   θ   normal-⋯     similar-to   subscript  Y  K    Gamma   subscript  α  K   θ      Y_{1}\sim\operatorname{Gamma}(\alpha_{1},\theta),\cdots,Y_{K}\sim\operatorname%
 {Gamma}(\alpha_{K},\theta)     we have: 13       V  =    ∑   i  =  1   K    Y  i    ∼   Gamma   (    ∑   i  =  1   K    α  i    ,  θ  )     ,        V    superscript   subscript     i  1    K    subscript  Y  i      similar-to     Gamma    superscript   subscript     i  1    K    subscript  α  i    θ      V=\sum_{i=1}^{K}Y_{i}\sim\operatorname{Gamma}\left(\sum_{i=1}^{K}\alpha_{i},%
 \theta\right),          X  =   (   X  1   ,  ⋯  ,   X  K   )   =   (    Y  1   V   ,  ⋯  ,    Y  K   V   )   ∼   Dir   (   α  1   ,  ⋯  ,   α  K   )     .        X    subscript  X  1   normal-⋯   subscript  X  K             subscript  Y  1   V   normal-⋯     subscript  Y  K   V      similar-to     Dir   subscript  α  1   normal-⋯   subscript  α  K       X=(X_{1},\cdots,X_{K})=\left(\frac{Y_{1}}{V},\cdots,\frac{Y_{K}}{V}\right)\sim%
 \operatorname{Dir}\left(\alpha_{1},\cdots,\alpha_{K}\right).     Although the X i s are not independent from one another, they can be seen to be generated from a set of K independent gamma random variable. 14 Unfortunately, since the sum V is lost in forming X (in fact it can be shown that V is stochastically independent of X ), it is not possible to recover the original gamma random variables from these values alone. Nevertheless, because independent random variables are simpler to work with, this reparametrization can still be useful for proofs about properties of the Dirichlet distribution.  Applications  Dirichlet distributions are most commonly used as the prior distribution of categorical variables or multinomial variables in Bayesian mixture models and other hierarchical Bayesian models . (Note that in many fields, such as in natural language processing , categorical variables are often imprecisely called "multinomial variables". Such a usage is liable to cause confusion, just as if Bernoulli distributions and binomial distributions were commonly conflated.)  Inference over hierarchical Bayesian models is often done using Gibbs sampling , and in such a case, instances of the Dirichlet distribution are typically marginalized out of the model by integrating out the Dirichlet random variable . This causes the various categorical variables drawn from the same Dirichlet random variable to become correlated, and the joint distribution over them assumes a Dirichlet-multinomial distribution , conditioned on the hyperparameters of the Dirichlet distribution (the concentration parameters ). One of the reasons for doing this is that Gibbs sampling of the Dirichlet-multinomial distribution is extremely easy; see that article for more information.  Random number generation  Gamma distribution  With a source of Gamma-distributed random variates, one can easily sample a random vector    x  =   (   x  1   ,  ⋯  ,   x  K   )       x    subscript  x  1   normal-⋯   subscript  x  K      x=(x_{1},\cdots,x_{K})   from the K -dimensional Dirichlet distribution with parameters    (   α  1   ,  ⋯  ,   α  K   )      subscript  α  1   normal-⋯   subscript  α  K     (\alpha_{1},\cdots,\alpha_{K})   . First, draw K independent random samples     y  1   ,  …  ,   y  K       subscript  y  1   normal-…   subscript  y  K     y_{1},\ldots,y_{K}   from Gamma distributions each with density        Gamma   (   α  i   ,  1  )    =      y  i    α  i   -  1      e   -   y  i       Γ   (   α  i   )      ,        Gamma    subscript  α  i   1         superscript   subscript  y  i      subscript  α  i   1     superscript  e     subscript  y  i        normal-Γ   subscript  α  i       \textrm{Gamma}(\alpha_{i},1)=\frac{y_{i}^{\alpha_{i}-1}\;e^{-y_{i}}}{\Gamma(%
 \alpha_{i})},\!     and then set        x  i   =    y  i     ∑   j  =  1   K    y  j      .       subscript  x  i      subscript  y  i     superscript   subscript     j  1    K    subscript  y  j       x_{i}=\frac{y_{i}}{\sum_{j=1}^{K}y_{j}}.     Below is example Python code to draw the sample:  params = [a1, a2, ..., ak]
 sample = [random.gammavariate(a, 1 ) for a in params]
 sample = [v / sum (sample) for v in sample]  Marginal beta distributions  A less efficient algorithm 15 relies on the univariate marginal and conditional distributions being beta and proceeds as follows. Simulate    x  1     subscript  x  1    x_{1}   from      Beta   (   α  1   ,    ∑   i  =  2   K    α  i    )       Beta    subscript  α  1     superscript   subscript     i  2    K    subscript  α  i       \textrm{Beta}\left(\alpha_{1},\sum_{i=2}^{K}\alpha_{i}\right)     Then simulate     x  2   ,  …  ,   x   K  -  1        subscript  x  2   normal-…   subscript  x    K  1      x_{2},\ldots,x_{K-1}   in order, as follows. For    j  =   2  ,  …  ,   K  -  1        j   2  normal-…    K  1      j=2,\ldots,K-1   , simulate    ϕ  j     subscript  ϕ  j    \phi_{j}   from       Beta   (   α  j   ,    ∑   i  =   j  +  1    K    α  i    )    ,      Beta    subscript  α  j     superscript   subscript     i    j  1     K    subscript  α  i       \textrm{Beta}\left(\alpha_{j},\sum_{i=j+1}^{K}\alpha_{i}\right),     and let        x  j   =    (   1  -    ∑   i  =  1    j  -  1     x  i     )    ϕ  j     .       subscript  x  j       1    superscript   subscript     i  1      j  1     subscript  x  i      subscript  ϕ  j      x_{j}=\left(1-\sum_{i=1}^{j-1}x_{i}\right)\phi_{j}.     Finally, set        x  K   =   1  -    ∑   i  =  1    K  -  1     x  i      .       subscript  x  K     1    superscript   subscript     i  1      K  1     subscript  x  i       x_{K}=1-\sum_{i=1}^{K-1}x_{i}.     This iterative procedure corresponds closely to the "string cutting" intuition described below.  Below is example Python code to draw the sample:  params = [a1, a2, ..., ak]
 xs = [random.betavariate(params[ 0 ], sum (params[ 1 :]))] for j in  range ( 1 , len (params) - 1 ):
     phi = random.betavariate(params[j], sum (params[j +1 :]))
     xs.append(( 1 - sum (xs)) * phi)
 xs.append( 1 - sum (xs))  Intuitive interpretations of the parameters  The concentration parameter  Dirichlet distributions are very often used as prior distributions in Bayesian inference . The simplest and perhaps most common type of Dirichlet prior is the symmetric Dirichlet distribution, where all parameters are equal. This corresponds to the case where you have no prior information to favor one component over any other. As described above, the single value α to which all parameters are set is called the concentration parameter . If the sample space of the Dirichlet distribution is interpreted as a discrete probability distribution , then intuitively the concentration parameter can be thought of as determining how "concentrated" the probability mass of a sample from a Dirichlet distribution is likely to be. With a value much less than 1, the mass will be highly concentrated in a few components, and all the rest will have almost no mass. With a value much greater than 1, the mass will be dispersed almost equally among all the components. See the article on the concentration parameter for further discussion.  String cutting  One example use of the Dirichlet distribution is if one wanted to cut strings (each of initial length 1.0) into K pieces with different lengths, where each piece had a designated average length, but allowing some variation in the relative sizes of the pieces. The α / α 0 values specify the mean lengths of the cut pieces of string resulting from the distribution. The variance around this mean varies inversely with α 0 .  (Figure)  Example of Dirichlet(1/2,1/3,1/6) distribution   Pólya's urn  Consider an urn containing balls of K different colors. Initially, the urn contains α 1 balls of color 1, α 2 balls of color 2, and so on. Now perform N draws from the urn, where after each draw, the ball is placed back into the urn with an additional ball of the same color. In the limit as N approaches infinity, the proportions of different colored balls in the urn will be distributed as Dir( α 1 ,..., α K ). 16  For a formal proof, note that the proportions of the different colored balls form a bounded [0,1] K -valued martingale , hence by the martingale convergence theorem , these proportions converge almost surely and in mean to a limiting random vector. To see that this limiting vector has the above Dirichlet distribution, check that all mixed moments agree.  Note that each draw from the urn modifies the probability of drawing a ball of any one color from the urn in the future. This modification diminishes with the number of draws, since the relative effect of adding a new ball to the urn diminishes as the urn accumulates increasing numbers of balls. This "diminishing returns" effect can also help explain how small α values yield Dirichlet distributions with most of the probability mass concentrated around a single point on the simplex.  See also   Generalized Dirichlet distribution  Grouped Dirichlet distribution  Inverted Dirichlet distribution  Latent Dirichlet allocation  Dirichlet process   References  External links    Dirichlet Distribution  How to estimate the parameters of the compound Dirichlet distribution (Pólya distribution) using expectation-maximization (EM)   Dirichlet Random Measures, Method of Construction via Compound Poisson Random Variables, and Exchangeability Properties of the resulting Gamma Distribution   "  Category:Multivariate continuous distributions  Category:Conjugate prior distributions  Category:Exponential family distributions  Category:Probability distributions  Category:Continuous distributions     (Chapter 49: Dirichlet and Inverted Dirichlet Distributions) ↩  ↩  Eq. (49.9) on page 488 of Kotz, Balakrishnan & Johnson (2000). Continuous Multivariate Distributions. Volume 1: Models and Applications. New York: Wiley. ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  See Kotz, Balakrishnan & Johnson (2000), Section 8.5, "Connor and Mosimann's Generalization", pp. 519–521. ↩  P. C. B. Phillips 1988. "The characteristic function of the Dirichlet and multivariate F distribution", Cowles Foundation discussion paper 985 ↩  ↩   ↩  ↩    