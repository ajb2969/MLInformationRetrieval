   Neyman‚ÄìPearson lemma      Neyman‚ÄìPearson lemma   In statistics , the Neyman‚ÄìPearson lemma , named after Jerzy Neyman and Egon Pearson , states that when performing a hypothesis test between two simple hypotheses  H 0 : Œ∏ = Œ∏ 0 and H 1 : Œ∏ = Œ∏ 1 , then the likelihood-ratio test which rejects H 0 in favour of H 1 when       Œõ   (  x  )    =    L   (   Œ∏  0   ‚à£  x  )     L   (   Œ∏  1   ‚à£  x  )     ‚â§  Œ∑          normal-Œõ  x      fragments  L   fragments  normal-(   subscript  Œ∏  0   normal-‚à£  x  normal-)     fragments  L   fragments  normal-(   subscript  Œ∏  1   normal-‚à£  x  normal-)          Œ∑     \Lambda(x)=\frac{L(\theta_{0}\mid x)}{L(\theta_{1}\mid x)}\leq\eta   where      P   (  Œõ   (  X  )   ‚â§  Œ∑  ‚à£   H  0   )   =  Œ±     fragments  P   fragments  normal-(  Œõ   fragments  normal-(  X  normal-)    Œ∑  normal-‚à£   subscript  H  0   normal-)    Œ±    P(\Lambda(X)\leq\eta\mid H_{0})=\alpha     is the most powerful test at significance level Œ± for a threshold Œ∑. If the test is most powerful for all     Œ∏  1   ‚àà   Œò  1        subscript  Œ∏  1    subscript  normal-Œò  1     \theta_{1}\in\Theta_{1}   , it is said to be uniformly most powerful (UMP) for alternatives in the set     Œò  1      subscript  normal-Œò  1    \Theta_{1}\,   .  In practice, the likelihood ratio is often used directly to construct tests ‚Äî see Likelihood-ratio test . However it can also be used to suggest particular test-statistics that might be of interest or to suggest simplified tests ‚Äî for this, one considers algebraic manipulation of the ratio to see if there are key statistics in it related to the size of the ratio (i.e. whether a large statistic corresponds to a small ratio or to a large one).  Proof  Define the rejection region of the null hypothesis for the NP test as       R   N  P    =   {  x  :     L   (   Œ∏  0   |  x  )     L   (   Œ∏  1   |  x  )     ‚â§  Œ∑   }        subscript  R    N  P     conditional-set  x       fragments  L   fragments  normal-(   subscript  Œ∏  0   normal-|  x  normal-)     fragments  L   fragments  normal-(   subscript  Œ∏  1   normal-|  x  normal-)     Œ∑      R_{NP}=\left\{x:\frac{L(\theta_{0}|x)}{L(\theta_{1}|x)}\leq\eta\right\}   where   Œ∑   Œ∑   \eta   is chosen so that     P   (   R   N  P    ,   Œ∏  0   )    =   Œ±         P    subscript  R    N  P     subscript  Œ∏  0     Œ±    P(R_{NP},\theta_{0})=\alpha\,   .  Any other test will have a different rejection region that we define as    R  A     subscript  R  A    R_{A}   . Furthermore, define the probability of the data falling in region R, given parameter   Œ∏   Œ∏   \theta   as      P   (  R  ,  Œ∏  )   =   ‚à´  R   L   (  Œ∏  |  x  )   d  x  ,     fragments  P   fragments  normal-(  R  normal-,  Œ∏  normal-)     subscript   R   L   fragments  normal-(  Œ∏  normal-|  x  normal-)   d  x  normal-,    P(R,\theta)=\int_{R}L(\theta|x)\,dx,     For the test with critical region    R  A     subscript  R  A    R_{A}   to have level   Œ±   Œ±   \alpha   , it must be true that    Œ±  ‚â•   P   (   R  A   ,   Œ∏  0   )        Œ±    P    subscript  R  A    subscript  Œ∏  0       \alpha\geq P(R_{A},\theta_{0})   , hence       Œ±  =   P   (   R   N  P    ,   Œ∏  0   )    ‚â•   P   (   R  A   ,   Œ∏  0   )     .        Œ±    P    subscript  R    N  P     subscript  Œ∏  0            P    subscript  R  A    subscript  Œ∏  0        \alpha=P(R_{NP},\theta_{0})\geq P(R_{A},\theta_{0})\,.     It will be useful to break these down into integrals over distinct regions:        P   (   R   N  P    ,  Œ∏  )    =    P   (    R   N  P    ‚à©   R  A    ,  Œ∏  )    +   P   (    R   N  P    ‚à©   R  A  c    ,  Œ∏  )      ,        P    subscript  R    N  P    Œ∏        P      subscript  R    N  P     subscript  R  A    Œ∏      P      subscript  R    N  P     superscript   subscript  R  A   c    Œ∏       P(R_{NP},\theta)=P(R_{NP}\cap R_{A},\theta)+P(R_{NP}\cap R_{A}^{c},\theta),     and        P   (   R  A   ,  Œ∏  )    =    P   (    R   N  P    ‚à©   R  A    ,  Œ∏  )    +   P   (    R   N  P   c   ‚à©   R  A    ,  Œ∏  )      .        P    subscript  R  A   Œ∏        P      subscript  R    N  P     subscript  R  A    Œ∏      P      superscript   subscript  R    N  P    c    subscript  R  A    Œ∏       P(R_{A},\theta)=P(R_{NP}\cap R_{A},\theta)+P(R_{NP}^{c}\cap R_{A},\theta).     Setting    Œ∏  =   Œ∏  0       Œ∏   subscript  Œ∏  0     \theta=\theta_{0}   , these two expressions and the above inequality yield that        P   (    R   N  P    ‚à©   R  A  c    ,   Œ∏  0   )    ‚â•   P   (    R   N  P   c   ‚à©   R  A    ,   Œ∏  0   )     .        P      subscript  R    N  P     superscript   subscript  R  A   c     subscript  Œ∏  0       P      superscript   subscript  R    N  P    c    subscript  R  A     subscript  Œ∏  0       P(R_{NP}\cap R_{A}^{c},\theta_{0})\geq P(R_{NP}^{c}\cap R_{A},\theta_{0}).     Comparing the powers of the two tests,    P   (   R   N  P    ,   Œ∏  1   )       P    subscript  R    N  P     subscript  Œ∏  1      P(R_{NP},\theta_{1})   and    P   (   R  A   ,   Œ∏  1   )       P    subscript  R  A    subscript  Œ∏  1      P(R_{A},\theta_{1})   , one can see that         P   (   R   N  P    ,   Œ∏  1   )    ‚â•   P   (   R  A   ,   Œ∏  1   )     ‚áî    P   (    R   N  P    ‚à©   R  A  c    ,   Œ∏  1   )    ‚â•   P   (    R   N  P   c   ‚à©   R  A    ,   Œ∏  1   )      .     iff      P    subscript  R    N  P     subscript  Œ∏  1       P    subscript  R  A    subscript  Œ∏  1          P      subscript  R    N  P     superscript   subscript  R  A   c     subscript  Œ∏  1       P      superscript   subscript  R    N  P    c    subscript  R  A     subscript  Œ∏  1        P(R_{NP},\theta_{1})\geq P(R_{A},\theta_{1})\iff P(R_{NP}\cap R_{A}^{c},\theta%
 _{1})\geq P(R_{NP}^{c}\cap R_{A},\theta_{1}).     Now by the definition of    R   N  P      subscript  R    N  P     R_{NP}   ,      P   (   R   N  P    ‚à©   R  A  c   ,   Œ∏  1   )   =   ‚à´    R   N  P    ‚à©   R  A  c     L   (   Œ∏  1   |  x  )   d  x  ‚â•   1  Œ∑    ‚à´    R   N  P    ‚à©   R  A  c     L   (   Œ∏  0   |  x  )   d  x  =   1  Œ∑   P   (   R   N  P    ‚à©   R  A  c   ,   Œ∏  0   )      fragments  P   fragments  normal-(   subscript  R    N  P      superscript   subscript  R  A   c   normal-,   subscript  Œ∏  1   normal-)     subscript      subscript  R    N  P     superscript   subscript  R  A   c     L   fragments  normal-(   subscript  Œ∏  1   normal-|  x  normal-)   d  x     1  Œ∑    subscript      subscript  R    N  P     superscript   subscript  R  A   c     L   fragments  normal-(   subscript  Œ∏  0   normal-|  x  normal-)   d  x     1  Œ∑   P   fragments  normal-(   subscript  R    N  P      superscript   subscript  R  A   c   normal-,   subscript  Œ∏  0   normal-)     P(R_{NP}\cap R_{A}^{c},\theta_{1})=\int_{R_{NP}\cap R_{A}^{c}}L(\theta_{1}|x)%
 \,dx\geq\frac{1}{\eta}\int_{R_{NP}\cap R_{A}^{c}}L(\theta_{0}|x)\,dx=\frac{1}{%
 \eta}P(R_{NP}\cap R_{A}^{c},\theta_{0})         ‚â•   1  Œ∑   P   (   R   N  P   c   ‚à©   R  A   ,   Œ∏  0   )   =   1  Œ∑    ‚à´    R   N  P   c   ‚à©   R  A     L   (   Œ∏  0   |  x  )   d  x  ‚â•   ‚à´    R   N  P   c   ‚à©   R  A     L   (   Œ∏  1   |  x  )   d  x  =  P   (   R   N  P   c   ‚à©   R  A   ,   Œ∏  1   )   .     fragments     1  Œ∑   P   fragments  normal-(   superscript   subscript  R    N  P    c     subscript  R  A   normal-,   subscript  Œ∏  0   normal-)      1  Œ∑    subscript      superscript   subscript  R    N  P    c    subscript  R  A     L   fragments  normal-(   subscript  Œ∏  0   normal-|  x  normal-)   d  x    subscript      superscript   subscript  R    N  P    c    subscript  R  A     L   fragments  normal-(   subscript  Œ∏  1   normal-|  x  normal-)   d  x   P   fragments  normal-(   superscript   subscript  R    N  P    c     subscript  R  A   normal-,   subscript  Œ∏  1   normal-)   normal-.    \geq\frac{1}{\eta}P(R_{NP}^{c}\cap R_{A},\theta_{0})=\frac{1}{\eta}\int_{R_{NP%
 }^{c}\cap R_{A}}L(\theta_{0}|x)\,dx\geq\int_{R_{NP}^{c}\cap R_{A}}L(\theta_{1}%
 |x)dx=P(R_{NP}^{c}\cap R_{A},\theta_{1}).     Hence the inequality holds.  Example  Let     X  1   ,  ‚Ä¶  ,   X  n       subscript  X  1   normal-‚Ä¶   subscript  X  n     X_{1},\dots,X_{n}   be a random sample from the    ùí©   (  Œº  ,   œÉ  2   )       ùí©   Œº   superscript  œÉ  2      \mathcal{N}(\mu,\sigma^{2})   distribution where the mean   Œº   Œº   \mu   is known, and suppose that we wish to test for     H  0   :    œÉ  2   =   œÉ  0  2       normal-:   subscript  H  0      superscript  œÉ  2    superscript   subscript  œÉ  0   2      H_{0}:\sigma^{2}=\sigma_{0}^{2}   against     H  1   :    œÉ  2   =   œÉ  1  2       normal-:   subscript  H  1      superscript  œÉ  2    superscript   subscript  œÉ  1   2      H_{1}:\sigma^{2}=\sigma_{1}^{2}   . The likelihood for this set of normally distributed data is        L   (   œÉ  2   ;  ùê±  )    ‚àù     (   œÉ  2   )    -   n  /  2      exp   {   -     ‚àë   i  =  1   n     (    x  i   -  Œº   )   2     2   œÉ  2      }      .     proportional-to    L    superscript  œÉ  2   ùê±       superscript   superscript  œÉ  2       n  2             superscript   subscript     i  1    n    superscript     subscript  x  i   Œº   2      2   superscript  œÉ  2          L\left(\sigma^{2};\mathbf{x}\right)\propto\left(\sigma^{2}\right)^{-n/2}\exp%
 \left\{-\frac{\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}}{2\sigma^{2}}\right\}.     We can compute the likelihood ratio to find the key statistic in this test and its effect on the test's outcome:        Œõ   (  ùê±  )    =    L   (   œÉ  0  2   ;  ùê±  )     L   (   œÉ  1  2   ;  ùê±  )     =     (    œÉ  0  2    œÉ  1  2    )    -   n  /  2      exp   {   -    1  2    (    œÉ  0   -  2    -   œÉ  1   -  2     )     ‚àë   i  =  1   n     (    x  i   -  Œº   )   2      }      .          normal-Œõ  ùê±       L    superscript   subscript  œÉ  0   2   ùê±      L    superscript   subscript  œÉ  1   2   ùê±             superscript     superscript   subscript  œÉ  0   2    superscript   subscript  œÉ  1   2        n  2             1  2      superscript   subscript  œÉ  0     2     superscript   subscript  œÉ  1     2       superscript   subscript     i  1    n    superscript     subscript  x  i   Œº   2           \Lambda(\mathbf{x})=\frac{L\left(\sigma_{0}^{2};\mathbf{x}\right)}{L\left(%
 \sigma_{1}^{2};\mathbf{x}\right)}=\left(\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}}%
 \right)^{-n/2}\exp\left\{-\frac{1}{2}(\sigma_{0}^{-2}-\sigma_{1}^{-2})\sum_{i=%
 1}^{n}\left(x_{i}-\mu\right)^{2}\right\}.     This ratio only depends on the data through     ‚àë   i  =  1   n     (    x  i   -  Œº   )   2       superscript   subscript     i  1    n    superscript     subscript  x  i   Œº   2     \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}   . Therefore, by the Neyman‚ÄìPearson lemma, the most powerful test of this type of hypothesis for this data will depend only on     ‚àë   i  =  1   n     (    x  i   -  Œº   )   2       superscript   subscript     i  1    n    superscript     subscript  x  i   Œº   2     \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}   . Also, by inspection, we can see that if     œÉ  1  2   >   œÉ  0  2        superscript   subscript  œÉ  1   2    superscript   subscript  œÉ  0   2     \sigma_{1}^{2}>\sigma_{0}^{2}   , then    Œõ   (  ùê±  )       normal-Œõ  ùê±    \Lambda(\mathbf{x})   is a decreasing function of     ‚àë   i  =  1   n     (    x  i   -  Œº   )   2       superscript   subscript     i  1    n    superscript     subscript  x  i   Œº   2     \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}   . So we should reject    H  0     subscript  H  0    H_{0}   if     ‚àë   i  =  1   n     (    x  i   -  Œº   )   2       superscript   subscript     i  1    n    superscript     subscript  x  i   Œº   2     \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}   is sufficiently large. The rejection threshold depends on the size of the test. In this example, the test statistic can be shown to be a scaled Chi-square distributed random variable and an exact critical value can be obtained.  Application in economics  A variant of the Neyman‚ÄìPearson lemma has found an application in the seemingly-unrelated domain of economy with land. One of the fundamental problems in consumer theory is calculating the demand function of the consumer given the prices. In particular, given a heterogeneous land-estate, a price measure over the land, and a subjective utility measure over the land, the consumer's problem is to calculate the best land parcel that he can buy ‚Äì i.e, the land parcel with the largest utility, whose price is at most his budget. It turns out that this problem is very similar to the problem of finding the most powerful stastistical test, and so the Neyman‚ÄìPearson lemma can be used. 1  See also   Statistical power   References    cnx.org: Neyman‚ÄìPearson criterion   External links   Cosma Shalizi, a professor of statistics at Carnegie Mellon University, gives an intuitive derivation of the Neyman‚ÄìPearson Lemma using ideas from economics   "  Category:Statistical theorems  Category:Statistical tests  Category:Articles containing proofs     ‚Ü©     