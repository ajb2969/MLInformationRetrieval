   Neymanâ€“Pearson lemma      Neymanâ€“Pearson lemma   In statistics , the Neymanâ€“Pearson lemma , named after Jerzy Neyman and Egon Pearson , states that when performing a hypothesis test between two simple hypotheses  H 0 : Î¸ = Î¸ 0 and H 1 : Î¸ = Î¸ 1 , then the likelihood-ratio test which rejects H 0 in favour of H 1 when       Î›   (  x  )    =    L   (   Î¸  0   âˆ£  x  )     L   (   Î¸  1   âˆ£  x  )     â‰¤  Î·          normal-Î›  x      fragments  L   fragments  normal-(   subscript  Î¸  0   normal-âˆ£  x  normal-)     fragments  L   fragments  normal-(   subscript  Î¸  1   normal-âˆ£  x  normal-)          Î·     \Lambda(x)=\frac{L(\theta_{0}\mid x)}{L(\theta_{1}\mid x)}\leq\eta   where      P   (  Î›   (  X  )   â‰¤  Î·  âˆ£   H  0   )   =  Î±     fragments  P   fragments  normal-(  Î›   fragments  normal-(  X  normal-)    Î·  normal-âˆ£   subscript  H  0   normal-)    Î±    P(\Lambda(X)\leq\eta\mid H_{0})=\alpha     is the most powerful test at significance level Î± for a threshold Î·. If the test is most powerful for all     Î¸  1   âˆˆ   Î˜  1        subscript  Î¸  1    subscript  normal-Î˜  1     \theta_{1}\in\Theta_{1}   , it is said to be uniformly most powerful (UMP) for alternatives in the set     Î˜  1      subscript  normal-Î˜  1    \Theta_{1}\,   .  In practice, the likelihood ratio is often used directly to construct tests â€” see Likelihood-ratio test . However it can also be used to suggest particular test-statistics that might be of interest or to suggest simplified tests â€” for this, one considers algebraic manipulation of the ratio to see if there are key statistics in it related to the size of the ratio (i.e. whether a large statistic corresponds to a small ratio or to a large one).  Proof  Define the rejection region of the null hypothesis for the NP test as       R   N  P    =   {  x  :     L   (   Î¸  0   |  x  )     L   (   Î¸  1   |  x  )     â‰¤  Î·   }        subscript  R    N  P     conditional-set  x       fragments  L   fragments  normal-(   subscript  Î¸  0   normal-|  x  normal-)     fragments  L   fragments  normal-(   subscript  Î¸  1   normal-|  x  normal-)     Î·      R_{NP}=\left\{x:\frac{L(\theta_{0}|x)}{L(\theta_{1}|x)}\leq\eta\right\}   where   Î·   Î·   \eta   is chosen so that     P   (   R   N  P    ,   Î¸  0   )    =   Î±         P    subscript  R    N  P     subscript  Î¸  0     Î±    P(R_{NP},\theta_{0})=\alpha\,   .  Any other test will have a different rejection region that we define as    R  A     subscript  R  A    R_{A}   . Furthermore, define the probability of the data falling in region R, given parameter   Î¸   Î¸   \theta   as      P   (  R  ,  Î¸  )   =   âˆ«  R   L   (  Î¸  |  x  )   d  x  ,     fragments  P   fragments  normal-(  R  normal-,  Î¸  normal-)     subscript   R   L   fragments  normal-(  Î¸  normal-|  x  normal-)   d  x  normal-,    P(R,\theta)=\int_{R}L(\theta|x)\,dx,     For the test with critical region    R  A     subscript  R  A    R_{A}   to have level   Î±   Î±   \alpha   , it must be true that    Î±  â‰¥   P   (   R  A   ,   Î¸  0   )        Î±    P    subscript  R  A    subscript  Î¸  0       \alpha\geq P(R_{A},\theta_{0})   , hence       Î±  =   P   (   R   N  P    ,   Î¸  0   )    â‰¥   P   (   R  A   ,   Î¸  0   )     .        Î±    P    subscript  R    N  P     subscript  Î¸  0            P    subscript  R  A    subscript  Î¸  0        \alpha=P(R_{NP},\theta_{0})\geq P(R_{A},\theta_{0})\,.     It will be useful to break these down into integrals over distinct regions:        P   (   R   N  P    ,  Î¸  )    =    P   (    R   N  P    âˆ©   R  A    ,  Î¸  )    +   P   (    R   N  P    âˆ©   R  A  c    ,  Î¸  )      ,        P    subscript  R    N  P    Î¸        P      subscript  R    N  P     subscript  R  A    Î¸      P      subscript  R    N  P     superscript   subscript  R  A   c    Î¸       P(R_{NP},\theta)=P(R_{NP}\cap R_{A},\theta)+P(R_{NP}\cap R_{A}^{c},\theta),     and        P   (   R  A   ,  Î¸  )    =    P   (    R   N  P    âˆ©   R  A    ,  Î¸  )    +   P   (    R   N  P   c   âˆ©   R  A    ,  Î¸  )      .        P    subscript  R  A   Î¸        P      subscript  R    N  P     subscript  R  A    Î¸      P      superscript   subscript  R    N  P    c    subscript  R  A    Î¸       P(R_{A},\theta)=P(R_{NP}\cap R_{A},\theta)+P(R_{NP}^{c}\cap R_{A},\theta).     Setting    Î¸  =   Î¸  0       Î¸   subscript  Î¸  0     \theta=\theta_{0}   , these two expressions and the above inequality yield that        P   (    R   N  P    âˆ©   R  A  c    ,   Î¸  0   )    â‰¥   P   (    R   N  P   c   âˆ©   R  A    ,   Î¸  0   )     .        P      subscript  R    N  P     superscript   subscript  R  A   c     subscript  Î¸  0       P      superscript   subscript  R    N  P    c    subscript  R  A     subscript  Î¸  0       P(R_{NP}\cap R_{A}^{c},\theta_{0})\geq P(R_{NP}^{c}\cap R_{A},\theta_{0}).     Comparing the powers of the two tests,    P   (   R   N  P    ,   Î¸  1   )       P    subscript  R    N  P     subscript  Î¸  1      P(R_{NP},\theta_{1})   and    P   (   R  A   ,   Î¸  1   )       P    subscript  R  A    subscript  Î¸  1      P(R_{A},\theta_{1})   , one can see that         P   (   R   N  P    ,   Î¸  1   )    â‰¥   P   (   R  A   ,   Î¸  1   )     â‡”    P   (    R   N  P    âˆ©   R  A  c    ,   Î¸  1   )    â‰¥   P   (    R   N  P   c   âˆ©   R  A    ,   Î¸  1   )      .     iff      P    subscript  R    N  P     subscript  Î¸  1       P    subscript  R  A    subscript  Î¸  1          P      subscript  R    N  P     superscript   subscript  R  A   c     subscript  Î¸  1       P      superscript   subscript  R    N  P    c    subscript  R  A     subscript  Î¸  1        P(R_{NP},\theta_{1})\geq P(R_{A},\theta_{1})\iff P(R_{NP}\cap R_{A}^{c},\theta%
 _{1})\geq P(R_{NP}^{c}\cap R_{A},\theta_{1}).     Now by the definition of    R   N  P      subscript  R    N  P     R_{NP}   ,      P   (   R   N  P    âˆ©   R  A  c   ,   Î¸  1   )   =   âˆ«    R   N  P    âˆ©   R  A  c     L   (   Î¸  1   |  x  )   d  x  â‰¥   1  Î·    âˆ«    R   N  P    âˆ©   R  A  c     L   (   Î¸  0   |  x  )   d  x  =   1  Î·   P   (   R   N  P    âˆ©   R  A  c   ,   Î¸  0   )      fragments  P   fragments  normal-(   subscript  R    N  P      superscript   subscript  R  A   c   normal-,   subscript  Î¸  1   normal-)     subscript      subscript  R    N  P     superscript   subscript  R  A   c     L   fragments  normal-(   subscript  Î¸  1   normal-|  x  normal-)   d  x     1  Î·    subscript      subscript  R    N  P     superscript   subscript  R  A   c     L   fragments  normal-(   subscript  Î¸  0   normal-|  x  normal-)   d  x     1  Î·   P   fragments  normal-(   subscript  R    N  P      superscript   subscript  R  A   c   normal-,   subscript  Î¸  0   normal-)     P(R_{NP}\cap R_{A}^{c},\theta_{1})=\int_{R_{NP}\cap R_{A}^{c}}L(\theta_{1}|x)%
 \,dx\geq\frac{1}{\eta}\int_{R_{NP}\cap R_{A}^{c}}L(\theta_{0}|x)\,dx=\frac{1}{%
 \eta}P(R_{NP}\cap R_{A}^{c},\theta_{0})         â‰¥   1  Î·   P   (   R   N  P   c   âˆ©   R  A   ,   Î¸  0   )   =   1  Î·    âˆ«    R   N  P   c   âˆ©   R  A     L   (   Î¸  0   |  x  )   d  x  â‰¥   âˆ«    R   N  P   c   âˆ©   R  A     L   (   Î¸  1   |  x  )   d  x  =  P   (   R   N  P   c   âˆ©   R  A   ,   Î¸  1   )   .     fragments     1  Î·   P   fragments  normal-(   superscript   subscript  R    N  P    c     subscript  R  A   normal-,   subscript  Î¸  0   normal-)      1  Î·    subscript      superscript   subscript  R    N  P    c    subscript  R  A     L   fragments  normal-(   subscript  Î¸  0   normal-|  x  normal-)   d  x    subscript      superscript   subscript  R    N  P    c    subscript  R  A     L   fragments  normal-(   subscript  Î¸  1   normal-|  x  normal-)   d  x   P   fragments  normal-(   superscript   subscript  R    N  P    c     subscript  R  A   normal-,   subscript  Î¸  1   normal-)   normal-.    \geq\frac{1}{\eta}P(R_{NP}^{c}\cap R_{A},\theta_{0})=\frac{1}{\eta}\int_{R_{NP%
 }^{c}\cap R_{A}}L(\theta_{0}|x)\,dx\geq\int_{R_{NP}^{c}\cap R_{A}}L(\theta_{1}%
 |x)dx=P(R_{NP}^{c}\cap R_{A},\theta_{1}).     Hence the inequality holds.  Example  Let     X  1   ,  â€¦  ,   X  n       subscript  X  1   normal-â€¦   subscript  X  n     X_{1},\dots,X_{n}   be a random sample from the    ğ’©   (  Î¼  ,   Ïƒ  2   )       ğ’©   Î¼   superscript  Ïƒ  2      \mathcal{N}(\mu,\sigma^{2})   distribution where the mean   Î¼   Î¼   \mu   is known, and suppose that we wish to test for     H  0   :    Ïƒ  2   =   Ïƒ  0  2       normal-:   subscript  H  0      superscript  Ïƒ  2    superscript   subscript  Ïƒ  0   2      H_{0}:\sigma^{2}=\sigma_{0}^{2}   against     H  1   :    Ïƒ  2   =   Ïƒ  1  2       normal-:   subscript  H  1      superscript  Ïƒ  2    superscript   subscript  Ïƒ  1   2      H_{1}:\sigma^{2}=\sigma_{1}^{2}   . The likelihood for this set of normally distributed data is        L   (   Ïƒ  2   ;  ğ±  )    âˆ     (   Ïƒ  2   )    -   n  /  2      exp   {   -     âˆ‘   i  =  1   n     (    x  i   -  Î¼   )   2     2   Ïƒ  2      }      .     proportional-to    L    superscript  Ïƒ  2   ğ±       superscript   superscript  Ïƒ  2       n  2             superscript   subscript     i  1    n    superscript     subscript  x  i   Î¼   2      2   superscript  Ïƒ  2          L\left(\sigma^{2};\mathbf{x}\right)\propto\left(\sigma^{2}\right)^{-n/2}\exp%
 \left\{-\frac{\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}}{2\sigma^{2}}\right\}.     We can compute the likelihood ratio to find the key statistic in this test and its effect on the test's outcome:        Î›   (  ğ±  )    =    L   (   Ïƒ  0  2   ;  ğ±  )     L   (   Ïƒ  1  2   ;  ğ±  )     =     (    Ïƒ  0  2    Ïƒ  1  2    )    -   n  /  2      exp   {   -    1  2    (    Ïƒ  0   -  2    -   Ïƒ  1   -  2     )     âˆ‘   i  =  1   n     (    x  i   -  Î¼   )   2      }      .          normal-Î›  ğ±       L    superscript   subscript  Ïƒ  0   2   ğ±      L    superscript   subscript  Ïƒ  1   2   ğ±             superscript     superscript   subscript  Ïƒ  0   2    superscript   subscript  Ïƒ  1   2        n  2             1  2      superscript   subscript  Ïƒ  0     2     superscript   subscript  Ïƒ  1     2       superscript   subscript     i  1    n    superscript     subscript  x  i   Î¼   2           \Lambda(\mathbf{x})=\frac{L\left(\sigma_{0}^{2};\mathbf{x}\right)}{L\left(%
 \sigma_{1}^{2};\mathbf{x}\right)}=\left(\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}}%
 \right)^{-n/2}\exp\left\{-\frac{1}{2}(\sigma_{0}^{-2}-\sigma_{1}^{-2})\sum_{i=%
 1}^{n}\left(x_{i}-\mu\right)^{2}\right\}.     This ratio only depends on the data through     âˆ‘   i  =  1   n     (    x  i   -  Î¼   )   2       superscript   subscript     i  1    n    superscript     subscript  x  i   Î¼   2     \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}   . Therefore, by the Neymanâ€“Pearson lemma, the most powerful test of this type of hypothesis for this data will depend only on     âˆ‘   i  =  1   n     (    x  i   -  Î¼   )   2       superscript   subscript     i  1    n    superscript     subscript  x  i   Î¼   2     \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}   . Also, by inspection, we can see that if     Ïƒ  1  2   >   Ïƒ  0  2        superscript   subscript  Ïƒ  1   2    superscript   subscript  Ïƒ  0   2     \sigma_{1}^{2}>\sigma_{0}^{2}   , then    Î›   (  ğ±  )       normal-Î›  ğ±    \Lambda(\mathbf{x})   is a decreasing function of     âˆ‘   i  =  1   n     (    x  i   -  Î¼   )   2       superscript   subscript     i  1    n    superscript     subscript  x  i   Î¼   2     \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}   . So we should reject    H  0     subscript  H  0    H_{0}   if     âˆ‘   i  =  1   n     (    x  i   -  Î¼   )   2       superscript   subscript     i  1    n    superscript     subscript  x  i   Î¼   2     \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}   is sufficiently large. The rejection threshold depends on the size of the test. In this example, the test statistic can be shown to be a scaled Chi-square distributed random variable and an exact critical value can be obtained.  Application in economics  A variant of the Neymanâ€“Pearson lemma has found an application in the seemingly-unrelated domain of economy with land. One of the fundamental problems in consumer theory is calculating the demand function of the consumer given the prices. In particular, given a heterogeneous land-estate, a price measure over the land, and a subjective utility measure over the land, the consumer's problem is to calculate the best land parcel that he can buy â€“ i.e, the land parcel with the largest utility, whose price is at most his budget. It turns out that this problem is very similar to the problem of finding the most powerful stastistical test, and so the Neymanâ€“Pearson lemma can be used. 1  See also   Statistical power   References    cnx.org: Neymanâ€“Pearson criterion   External links   Cosma Shalizi, a professor of statistics at Carnegie Mellon University, gives an intuitive derivation of the Neymanâ€“Pearson Lemma using ideas from economics   "  Category:Statistical theorems  Category:Statistical tests  Category:Articles containing proofs     â†©     