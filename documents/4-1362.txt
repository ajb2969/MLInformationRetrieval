   Coefficient of variation      Coefficient of variation   In probability theory and statistics , the coefficient of variation ( CV ), also known as relative standard deviation ( RSD ), is a standardized measure of dispersion of a probability distribution or frequency distribution . It is often expressed as a percentage, and is defined as the ratio of the standard deviation     σ    σ   \ \sigma   to the mean     μ    μ   \ \mu   (or its absolute value ,    |  μ  |      μ    |\mu|   ). The CV or RSD is widely used in analytical chemistry to express the precision and repeatability of an assay . It is also commonly used in fields such as engineering or physics when doing quality assurance studies and ANOVA gauge R&R; .  Definition  The coefficient of variation (CV) is defined as the ratio of the standard deviation    σ    σ   \ \sigma   to the mean    μ    μ   \ \mu   : 1       c  v   =   σ  μ        subscript  c  v     σ  μ     c_{v}=\frac{\sigma}{\mu}   It shows the extent of variability in relation to the mean of the population.  The coefficient of variation should be computed only for data measured on a ratio scale , as these are the measurements that can only take non-negative values. The coefficient of variation may not have any meaning for data on an interval scale . 2 For example, most temperature scales (e.g., Celsius, Fahrenheit etc.) are interval scales that can take both positive and negative values, whereas the Kelvin temperature can never be less than zero, which is the complete absence of thermal energy. Hence, the Kelvin scale is a ratio scale. While the standard deviation (SD) can be derived on both the Kelvin and the Celsius scale (with both leading to the same SDs), the CV is only relevant as a measure of relative variability for the Kelvin scale.  Measurements that are log-normally distributed exhibit stationary CV; in contrast, SD varies depending upon the expected value of measurements.  A nonparametric possibility is the quartile coefficient of dispersion , i.e. interquartile range      Q  3   -   Q  1        subscript  Q  3    subscript  Q  1     {Q_{3}-Q_{1}}   divided by the median     Q  2     subscript  Q  2    {Q_{2}}   .  Examples  A data set of [100, 100, 100] has constant values. Its standard deviation is 0 and average is 100:   100% × 0 / 100 = 0%   A data set of [90, 100, 110] has more variability. Its standard deviation is 8.16 and its average is 100:   100% × 8.16 / 100 = 8.16%   A data set of [1, 5, 6, 8, 10, 40, 65, 88] has more variability again. Its standard deviation is 30.78 and its average is 27.875:   100% × 30.78 / 27.875 = 110.4%   Examples of misuse  To see why the coefficient of variation should not be applied to interval level data, compare the same set of temperatures in Celsius and Fahrenheit:  Celsius: [0, 10, 20, 30, 40] Fahrenheit: [32, 50, 68, 86, 104]  The CV of the first set is 15.81/20 = 0.79. For the second set (which are the same temperatures) it is 28.46/68 = 0.42.  Estimation  When only a sample of data from a population is available, the population CV can be estimated using the ratio of the sample standard deviation     s    s   s\,   to the sample mean    x  ¯     normal-¯  x    \bar{x}   :        c  v   ^   =   s   x  ¯         normal-^   subscript  c  v      s   normal-¯  x      \widehat{c_{v}}=\frac{s}{\bar{x}}     But this estimator, when applied to a small or moderately sized sample, tends to be too low: it is a biased estimator . For normally distributed data, an unbiased estimator 3 for a sample of size n is:         c  v   ^   *   =    (   1  +   1   4  n     )     c  v   ^         superscript   normal-^   subscript  c  v          1    1    4  n      normal-^   subscript  c  v       \widehat{c_{v}}^{*}=\bigg(1+\frac{1}{4n}\bigg)\widehat{c_{v}}     Log-normal data  In many applications, it can be assumed that data are log-normally distributed (evidenced by the presence of skewness in the sampled data). 4 In such cases, a more accurate estimate, derived from the properties of the log-normal distribution , 5 6 7 is defined as:         c  v   ^    l  n    =      e   s   l  n     2     -  1         subscript   normal-^   subscript  c  v      l  n         superscript  e   superscript   subscript  s    l  n    2    1      \widehat{c_{v}}_{ln}=\sqrt{e^{{s_{ln}}^{2}}\!\!-1}     where     s   l  n       subscript  s    l  n     {s_{ln}}\,   is the sample standard deviation of the data after a natural log transformation. (In the event that measurements are recorded using any other logarithmic base, b, their standard deviation     s  b      subscript  s  b    s_{b}\,   is converted to base e using     s   l  n    =    s  b   l  n   (  b  )         subscript  s    l  n       subscript  s  b   l  n  b     s_{ln}=s_{b}ln(b)\,   , and the formula for       c  v   ^    l  n       subscript   normal-^   subscript  c  v      l  n     \widehat{c_{v}}_{ln}\,   remains the same. 8 ) This estimate is sometimes referred to as the “geometric CV” 9 10 in order to distinguish it from the simple estimate above. However, "geometric coefficient of variation" has also been defined by Kirkwood 11 as:       G  C   V  K    =     e   s   l  n      -  1         G  C   subscript  V  K       superscript  e   subscript  s    l  n     1     GCV_{K}={e^{s_{ln}}\!\!-1}     This term was intended to be analogous to the coefficient of variation, for describing multiplicative variation in log-normal data, but this definition of GCV has no theoretical basis as an estimate of     c  v      subscript  c  v    c_{v}\,   itself.  For many practical purposes (such as sample size determination and calculation of confidence intervals ) it is     s   l  n       subscript  s    l  n     s_{ln}\,   which is of most use in the context of log-normally distributed data. If necessary, this can be derived from an estimate of     c  v      subscript  c  v    c_{v}\,   or GCV by inverting the corresponding formula.  Comparison to standard deviation  Advantages  The coefficient of variation is useful because the standard deviation of data must always be understood in the context of the mean of the data. In contrast, the actual value of the CV is independent of the unit in which the measurement has been taken, so it is a dimensionless number . For comparison between data sets with different units or widely different means, one should use the coefficient of variation instead of the standard deviation.  Disadvantages   When the mean value is close to zero, the coefficient of variation will approach infinity and is therefore sensitive to small changes in the mean. This is often the case if the values do not originate from a ratio scale.  Unlike the standard deviation, it cannot be used directly to construct confidence intervals for the mean.  CVs are not an ideal index of the certainty of a measurement when the number of replicates varies across samples because CV is invariant to the number of replicates while certainty of the mean improves with increasing replicates. In this case standard error in percent is suggested to be superior. 12   Applications  The coefficient of variation is also common in applied probability fields such as renewal theory , queueing theory , and reliability theory . In these fields, the exponential distribution is often more important than the normal distribution . The standard deviation of an exponential distribution is equal to its mean, so its coefficient of variation is equal to 1. Distributions with CV  1 (such as a hyper-exponential distribution ) are considered high-variance. Some formulas in these fields are expressed using the squared coefficient of variation , often abbreviated SCV. In modeling, a variation of the CV is the CV(RMSD). Essentially the CV(RMSD) replaces the standard deviation term with the Root Mean Square Deviation (RMSD) . While many natural processes indeed show a correlation between the average value and the amount of variation around it, accurate sensor devices need to be designed in such a way that the coefficient of variation is close to zero, i.e., yielding a constant absolute error over their working range.  In actuarial science , the CV is known as unitized risk 13  Laboratory measures of intra-assay and inter-assay CVs  CV measures are often used as quality controls for quantitative laboratory assays . While intra-assay and inter-assay CVs might be assumed to be calculated by simply averaging CV values across CV values for multiple samples within one assay or by averaging multiple inter-assay CV estimates, it has been suggested that these practices are incorrect and that a more complex computational process is required. 14 It has also been noted that CV values are not an ideal index of the certainty of a measurement when the number of replicates varies across samples − in this case standard error in percent is suggested to be superior. 15  Distribution  Provided that negative and small positive values of the sample mean occur with negligible frequency, the probability distribution of the coefficient of variation for a sample of size n has been shown by Hendricks and Robey 16 to be        d   F   c  v     =    2    π   1  /  2    Γ   (    n  -  1   2   )      e   -    n   2    (   σ  μ   )   2       c  v    2    1  +   c  v    2          c  v     n  -  2      (   1  +   c  v    2    )    n  /  2       ∑  ′     i  =  0    ′      n  -  1    ′         (   n  -  1   )   !   Γ   (    n  -  i   2   )       (   n  -  1  -  i   )   !    i  !       n   i  /  2      2   i  /  2      (   σ  μ   )   i      1    (   1  +   c  v    2    )    i  /  2     d   c  v     ,        d   subscript  F   subscript  c  v         2     superscript  π    1  2    normal-Γ      n  1   2      superscript  e        n    2   superscript    σ  μ   2        superscript   subscript  c  v   2     1   superscript   subscript  c  v   2           superscript   subscript  c  v     n  2     superscript    1   superscript   subscript  c  v   2      n  2      superscript   subscript   superscript   normal-′     i  0      n  1            n  1    normal-Γ      n  i   2          n  1  i      i        superscript  n    i  2       superscript  2    i  2     superscript    σ  μ   i       1   superscript    1   superscript   subscript  c  v   2      i  2     d   subscript  c  v      dF_{c_{v}}=\frac{2}{\pi^{1/2}\Gamma\Big(\frac{n-1}{2}\Big)}e^{-\frac{n}{2(%
 \frac{\sigma}{\mu})^{2}}\frac{{c_{v}}^{2}}{1+{c_{v}}^{2}}}\frac{{c_{v}}^{n-2}}%
 {(1+{c_{v}}^{2})^{n/2}}\sideset{}{{}^{\prime}}{\sum}_{i=0}^{n-1}\frac{(n-1)!%
 \Gamma\Big(\frac{n-i}{2}\Big)}{(n-1-i)!i!}\frac{n^{i/2}}{2^{i/2}(\frac{\sigma}%
 {\mu})^{i}}\frac{1}{(1+{c_{v}}^{2})^{i/2}}dc_{v},     where the symbol    ∑  ′     superscript   normal-′    \sideset{}{{}^{\prime}}{\sum}   indicates that the summation is over only even values of , i.e., if n is odd, sum over even values of i and if n is even, sum only over odd values of i .  This is useful, for instance, in the construction of hypothesis tests or confidence intervals . Statistical inference for the coefficient of variation in normally distributed data is often based on McKay's chi-square approximation for the coefficient of variation 17 18 19 20  Alternative  According to Liu (2012), 21 Lehmann (1986) 22 "also derived the sample distribution of CV in order to give an exact method for the construction of a confidence interval for CV;" it is based on a non-central t-distribution .  Similar ratios  Standardized moments are similar ratios,     μ  k   /   σ  k        subscript  μ  k    superscript  σ  k     {\mu_{k}}/{\sigma^{k}}   where    μ  k     subscript  μ  k    \mu_{k}   is the k th moment about the mean, which are also dimensionless and scale invariant. The variance-to-mean ratio ,     σ  2   /  μ       superscript  σ  2   μ    \sigma^{2}/\mu   , is another similar ratio, but is not dimensionless, and hence not scale invariant. See Normalization (statistics) for further ratios.  In signal processing , particularly image processing , the reciprocal ratio    μ  /  σ      μ  σ    \mu/\sigma   is referred to as the signal to noise ratio in general and signal-to-noise ratio (imaging) in particular.   Efficiency ,     σ  2   /   μ  2        superscript  σ  2    superscript  μ  2     \sigma^{2}/\mu^{2}     Standardized moment ,     μ  k   /   σ  k        subscript  μ  k    superscript  σ  k     \mu_{k}/\sigma^{k}     Variance-to-mean ratio (or relative variance),     σ  2   /  μ       superscript  σ  2   μ    \sigma^{2}/\mu     Fano factor ,     σ  W  2   /   μ  W        subscript   superscript  σ  2   W    subscript  μ  W     \sigma^{2}_{W}/\mu_{W}   (windowed VMR)  Relative Standard Error   See also   Omega ratio  Sampling (statistics)   References  ru:Вариация (статистика)#Относительные показатели "  Category:Theory of probability distributions  Category:Statistical deviation and dispersion  Category:Statistical terminology  Category:Statistical ratios     ↩  ↩  Sokal RR & Rohlf FJ. Biometry (3rd Ed). New York: Freeman, 1995. p. 58. ISBN 0-7167-2411-1 ↩  ↩  ↩  ↩  ↩  ↩  Sawant,S.; Mohan, N. (2011) "FAQ: Issues with Efficacy Analysis of Clinical Trial Data Using SAS" , PharmaSUG2011 , Paper PO08 ↩  Schiff MH, et al. Ann Rheum Dis 2014;0:1–3. ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  . ↩  ↩  [ http://scholarworks.gsu.edu/cgi/viewcontent.cgi?article=1116&context; ;=math_theses], p.3 ↩  Lehmann, E. L. (1986), Testing Statistical Hypothesis. 2nd ed. New York: Wiley. ↩     