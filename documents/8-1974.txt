   Robust optimization      Robust optimization   Robust optimization is a field of optimization theory that deals with optimization problems in which a certain measure of robustness is sought against uncertainty that can be represented as deterministic variability in the value of the parameters of the problem itself and/or its solution.  History  The origins of robust optimization date back to the establishment of modern decision theory in the 1950s and the use of worst case analysis and Wald's maximin model as a tool for the treatment of severe uncertainty. It became a discipline of its own in the 1970s with parallel developments in several scientific and technological fields. Over the years, it has been applied in statistics , but also in operations research , 1  control theory , 2  finance , 3  portfolio management 4  logistics , 5  manufacturing engineering , 6  chemical engineering , 7  medicine , 8 and computer science . In engineering problems, these formulations often take the name of "Robust Design Optimization", RDO or "Reliability Based Design Optimization", RBDO.  Example 1  Consider the following linear programming problem           max   x  ,  y      {    3  x   +   2  y    }      subject   to   x  ,  y   ‚â•  0   ;      c  x   +   d  y    ‚â§  10   ,    ‚àÄ   (  c  ,  d  )    ‚àà  P       formulae-sequence       subscript    x  y        3  x     2  y       subject  to   x  y   0    formulae-sequence        c  x     d  y    10      for-all   c  d    P      \max_{x,y}\ \{3x+2y\}\ \ \mathrm{subject\ to}\ \ x,y\geq 0;cx+dy\leq 10,%
 \forall(c,d)\in P   where   P   P   P   is a given subset of    ‚Ñù  2     superscript  ‚Ñù  2    \mathbb{R}^{2}   .  What makes this a 'robust optimization' problem is the     ‚àÄ   (  c  ,  d  )    ‚àà  P       for-all   c  d    P    \forall(c,d)\in P   clause in the constraints. Its implication is that for a pair    (  x  ,  y  )     x  y    (x,y)   to be admissible, the constraint      c  x   +   d  y    ‚â§  10          c  x     d  y    10    cx+dy\leq 10   must be satisfied by the worst      (  c  ,  d  )   ‚àà  P       c  d   P    (c,d)\in P   pertaining to    (  x  ,  y  )     x  y    (x,y)   , namely the pair     (  c  ,  d  )   ‚àà  P       c  d   P    (c,d)\in P   that maximizes the value of     c  x   +   d  y         c  x     d  y     cx+dy   for the given value of    (  x  ,  y  )     x  y    (x,y)   .  If the parameter space   P   P   P   is finite (consisting of finitely many elements), then this robust optimization problem itself is a linear programming problem: for each     (  c  ,  d  )   ‚àà  P       c  d   P    (c,d)\in P   there is a linear constraint      c  x   +   d  y    ‚â§  10          c  x     d  y    10    cx+dy\leq 10   .  If   P   P   P   is not a finite set, then this problem is a linear semi-infinite programming problem, namely a linear programming problem with finitely many (2) decision variables and infinitely many constraints.  Classification  There are a number of classification criteria for robust optimization problems/models. In particular, one can distinguish between problems dealing with local and global models of robustness; and between probabilistic and non-probabilistic models of robustness. Modern robust optimization deals primarily with non-probabilistic models of robustness that are worst case oriented and as such usually deploy Wald's maximin models .  Local robustness  There are cases where robustness is sought against small perturbations in a nominal value of a parameter. A very popular model of local robustness is the radius of stability model:        œÅ  ^    (  x  ,   u  ^   )    :=     max   œÅ  ‚â•  0      {   œÅ  :    u  ‚àà   S   (  x  )     ,    ‚àÄ  u   ‚àà   B   (  œÅ  ,   u  ^   )       }       assign     normal-^  œÅ    x   normal-^  u       subscript     œÅ  0     normal-:  œÅ   formulae-sequence    u    S  x       for-all  u     B   œÅ   normal-^  u           \hat{\rho}(x,\hat{u}):=\max_{\rho\geq 0}\ \{\rho:u\in S(x),\forall u\in B(\rho%
 ,\hat{u})\}     where    u  ^     normal-^  u    \hat{u}   denotes the nominal value of the parameter,    B   (  œÅ  ,   u  ^   )       B   œÅ   normal-^  u      B(\rho,\hat{u})   denotes a ball of radius   œÅ   œÅ   \rho   centered at    u  ^     normal-^  u    \hat{u}   and    S   (  x  )       S  x    S(x)   denotes the set of values of   u   u   u   that satisfy given stability/performance conditions associated with decision   x   x   x   .  In words, the robustness (radius of stability) of decision   x   x   x   is the radius of the largest ball centered at    u  ^     normal-^  u    \hat{u}   all of whose elements satisfy the stability requirements imposed on   x   x   x   . The picture is this:  (Figure)  Local robustness.png   where the rectangle    U   (  x  )       U  x    U(x)   represents the set of all the values   u   u   u   associated with decision   x   x   x   .  Global robustness  Consider the simple abstract robust optimization problem        max   x  ‚àà  X      {    f   (  x  )    :     g   (  x  ,  u  )    ‚â§  b   ,    ‚àÄ  u   ‚àà  U     }       subscript     x  X     normal-:    f  x    formulae-sequence      g   x  u    b      for-all  u   U       \max_{x\in X}\ \{f(x):g(x,u)\leq b,\forall u\in U\}     where   U   U   U   denotes the set of all possible values of   u   u   u   under consideration.  This is a global robust optimization problem in the sense that the robustness constraint      g   (  x  ,  u  )    ‚â§  b   ,    ‚àÄ  u   ‚àà  U      formulae-sequence      g   x  u    b      for-all  u   U     g(x,u)\leq b,\forall u\in U   represents all the possible values of   u   u   u   .  The difficulty is that such a "global" constraint can be too demanding in that there is no    x  ‚àà  X      x  X    x\in X   that satisfies this constraint. But even if such an    x  ‚àà  X      x  X    x\in X   exists, the constraint can be too "conservative" in that it yields a solution    x  ‚àà  X      x  X    x\in X   that generates a very small payoff    f   (  x  )       f  x    f(x)   that is not representative of the performance of other decisions in   X   X   X   . For instance, there could be an     x  ‚Ä≤   ‚àà  X       superscript  x  normal-‚Ä≤   X    x^{\prime}\in X   that only slightly violates the robustness constraint but yields a very large payoff    f   (   x  ‚Ä≤   )       f   superscript  x  normal-‚Ä≤     f(x^{\prime})   . In such cases it might be necessary to relax a bit the robustness constraint and/or modify the statement of the problem.  Example 2  Consider the case where the objective is to satisfy a constraint      g   (  x  ,  u  )    ‚â§  b   ,        g   x  u    b    g(x,u)\leq b,   . where    x  ‚àà  X      x  X    x\in X   denotes the decision variable and   u   u   u   is a parameter whose set of possible values in   U   U   U   . If there is no    x  ‚àà  X      x  X    x\in X   such that      g   (  x  ,  u  )    ‚â§  b   ,    ‚àÄ  u   ‚àà  U      formulae-sequence      g   x  u    b      for-all  u   U     g(x,u)\leq b,\forall u\in U   , then the following intuitive measure of robustness suggests itself:        œÅ   (  x  )    :=     max   Y  ‚äÜ  U      {    s  i  z  e   (  Y  )    :     g   (  x  ,  u  )    ‚â§  b   ,    ‚àÄ  u   ‚àà  Y     }     ,   x  ‚àà  X      formulae-sequence   assign    œÅ  x     subscript     Y  U     normal-:    s  i  z  e  Y    formulae-sequence      g   x  u    b      for-all  u   Y         x  X     \rho(x):=\max_{Y\subseteq U}\ \{size(Y):g(x,u)\leq b,\forall u\in Y\}\ ,\ x\in
 X     where    s  i  z  e   (  Y  )       s  i  z  e  Y    size(Y)   denotes an appropriate measure of the "size" of set   Y   Y   Y   . For example, if   U   U   U   is a finite set, then    s  i  z  e   (  Y  )       s  i  z  e  Y    size(Y)   could be defined as the cardinality of set   Y   Y   Y   .  In words, the robustness of decision is the size of the largest subset of   U   U   U   for which the constraint     g   (  x  ,  u  )    ‚â§  b        g   x  u    b    g(x,u)\leq b   is satisfied for each   u   u   u   in this set. An optimal decision is then a decision whose robustness is the largest.  This yields the following robust optimization problem:        max    x  ‚àà  X   ,   Y  ‚äÜ  U       {    s  i  z  e   (  Y  )    :     g   (  x  ,  u  )    ‚â§  b   ,    ‚àÄ  u   ‚àà  Y     }       subscript    formulae-sequence    x  X     Y  U      normal-:    s  i  z  e  Y    formulae-sequence      g   x  u    b      for-all  u   Y       \max_{x\in X,Y\subseteq U}\ \{size(Y):g(x,u)\leq b,\forall u\in Y\}     This intuitive notion of global robustness is not used often in practice because the robust optimization problems that it induces are usually (not always) very difficult to solve.  Example 3  Consider the robust optimization problem       z   (  U  )    :=     max   x  ‚àà  X      {    f   (  x  )    :     g   (  x  ,  u  )    ‚â§  b   ,    ‚àÄ  u   ‚àà  U     }       assign    z  U     subscript     x  X     normal-:    f  x    formulae-sequence      g   x  u    b      for-all  u   U        z(U):=\max_{x\in X}\ \{f(x):g(x,u)\leq b,\forall u\in U\}   where   g   g   g   is a real-valued function on    X  √ó  U      X  U    X\times U   , and assume that there is no feasible solution to this problem because the robustness constraint      g   (  x  ,  u  )    ‚â§  b   ,    ‚àÄ  u   ‚àà  U      formulae-sequence      g   x  u    b      for-all  u   U     g(x,u)\leq b,\forall u\in U   is too demanding.  To overcome this difficult, let   ùí©   ùí©   \mathcal{N}   be a relatively small subset of   U   U   U   representing "normal" values of   u   u   u   and consider the following robust optimization problem:       z   (  ùí©  )    :=     max   x  ‚àà  X      {    f   (  x  )    :     g   (  x  ,  u  )    ‚â§  b   ,    ‚àÄ  u   ‚àà  ùí©     }       assign    z  ùí©     subscript     x  X     normal-:    f  x    formulae-sequence      g   x  u    b      for-all  u   ùí©        z(\mathcal{N}):=\max_{x\in X}\ \{f(x):g(x,u)\leq b,\forall u\in\mathcal{N}\}     Since   ùí©   ùí©   \mathcal{N}   is much smaller than   U   U   U   , its optimal solution may not perform well on a large portion of   U   U   U   and therefore may not be robust against the variability of   u   u   u   over   U   U   U   .  One way to fix this difficulty is to relax the constraint     g   (  x  ,  u  )    ‚â§  b        g   x  u    b    g(x,u)\leq b   for values of   u   u   u   outside the set   ùí©   ùí©   \mathcal{N}   in a controlled manner so that larger violations are allowed as the distance of   u   u   u   from   ùí©   ùí©   \mathcal{N}   increases. For instance, consider the relaxed robustness constraint        g   (  x  ,  u  )    ‚â§   b  +    Œ≤  ‚ãÖ  d   i  s  t   (  u  ,  ùí©  )      ,    ‚àÄ  u   ‚àà  U      formulae-sequence      g   x  u      b     normal-‚ãÖ  Œ≤  d   i  s  t   u  ùí©         for-all  u   U     g(x,u)\leq b+\beta\cdot dist(u,\mathcal{N})\ ,\ \forall u\in U     where    Œ≤  ‚â•  0      Œ≤  0    \beta\geq 0   is a control parameter and    d  i  s  t   (  u  ,  ùí©  )       d  i  s  t   u  ùí©     dist(u,\mathcal{N})   denotes the distance of   u   u   u   from   ùí©   ùí©   \mathcal{N}   . Thus, for    Œ≤  =  0      Œ≤  0    \beta=0   the relaxed robustness constraint reduces back to the original robustness constraint. This yields the following (relaxed) robust optimization problem:       z   (  ùí©  ,  U  )    :=     max   x  ‚àà  X      {    f   (  x  )    :     g   (  x  ,  u  )    ‚â§   b  +    Œ≤  ‚ãÖ  d   i  s  t   (  u  ,  ùí©  )      ,    ‚àÄ  u   ‚àà  U     }       assign    z   ùí©  U      subscript     x  X     normal-:    f  x    formulae-sequence      g   x  u      b     normal-‚ãÖ  Œ≤  d   i  s  t   u  ùí©         for-all  u   U        z(\mathcal{N},U):=\max_{x\in X}\ \{f(x):g(x,u)\leq b+\beta\cdot dist(u,%
 \mathcal{N})\ ,\ \forall u\in U\}     The function    d  i  s  t      d  i  s  t    dist   is defined in such a manner that        d  i  s  t   (  u  ,  ùí©  )    ‚â•  0   ,    ‚àÄ  u   ‚àà  U      formulae-sequence      d  i  s  t   u  ùí©    0      for-all  u   U     dist(u,\mathcal{N})\geq 0,\forall u\in U     and        d  i  s  t   (  u  ,  ùí©  )    =  0   ,    ‚àÄ  u   ‚àâ  ùí©      formulae-sequence      d  i  s  t   u  ùí©    0      for-all  u   ùí©     dist(u,\mathcal{N})=0,\forall u\not\in\mathcal{N}     and therefore the optimal solution to the relaxed problem satisfies the original constraint     g   (  x  ,  u  )    ‚â§  b        g   x  u    b    g(x,u)\leq b   for all values of   u   u   u   in   ùí©   ùí©   \mathcal{N}   . In addition, it also satisfies the relaxed constraint       g   (  x  ,  u  )    ‚â§   b  +    Œ≤  ‚ãÖ  d   i  s  t   (  u  ,  ùí©  )           g   x  u      b     normal-‚ãÖ  Œ≤  d   i  s  t   u  ùí©       g(x,u)\leq b+\beta\cdot dist(u,\mathcal{N})     outside   ùí©   ùí©   \mathcal{N}   .  Non-probabilistic robust optimization models  The dominating paradigm in this area of robust optimization is Wald's maximin model , namely        max   x  ‚àà  X      min   u  ‚àà   U   (  x  )      f     (  x  ,  u  )         subscript     x  X      subscript     u    U  x     f     x  u     \max_{x\in X}\min_{u\in U(x)}f(x,u)     where the   max     \max   represents the decision maker, the   min     \min   represents Nature, namely uncertainty ,   X   X   X   represents the decision space and    U   (  x  )       U  x    U(x)   denotes the set of possible values of   u   u   u   associated with decision   x   x   x   . This is the classic format of the generic model, and is often referred to as minimax or maximin optimization problem. The non-probabilistic ( deterministic ) model has been and is being extensively used for robust optimization especially in the field of signal processing. 9 10 11  The equivalent mathematical programming (MP) of the classic format above is        max    x  ‚àà  X   ,   v  ‚àà  ‚Ñù       {   v  :    v  ‚â§   f   (  x  ,  u  )     ,    ‚àÄ  u   ‚àà   U   (  x  )       }       subscript    formulae-sequence    x  X     v  ‚Ñù      normal-:  v   formulae-sequence    v    f   x  u        for-all  u     U  x        \max_{x\in X,v\in\mathbb{R}}\ \{v:v\leq f(x,u),\forall u\in U(x)\}     Constraints can be incorporated explicitly in these models. The generic constrained classic format is       max   x  ‚àà  X       min   u  ‚àà   U   (  x  )        {    f   (  x  ,  u  )    :     g   (  x  ,  u  )    ‚â§  b   ,    ‚àÄ  u   ‚àà   U   (  x  )       }        subscript     x  X      subscript     u    U  x      normal-:    f   x  u     formulae-sequence      g   x  u    b      for-all  u     U  x         \max_{x\in X}\min_{u\in U(x)}\ \{f(x,u):g(x,u)\leq b,\forall u\in U(x)\}     The equivalent constrained MP format is        max    x  ‚àà  X   ,   v  ‚àà  ‚Ñù       {   v  :    v  ‚â§   f   (  x  ,  u  )     ,     g   (  x  ,  u  )    ‚â§  b   ,    ‚àÄ  u   ‚àà   U   (  x  )        }       subscript    formulae-sequence    x  X     v  ‚Ñù      normal-:  v   formulae-sequence    v    f   x  u      formulae-sequence      g   x  u    b      for-all  u     U  x         \max_{x\in X,v\in\mathbb{R}}\ \{v:v\leq f(x,u),g(x,u)\leq b,\forall u\in U(x)\}     Probabilistic robust optimization models  These models quantify the uncertainty in the "true" value of the parameter of interest by probability distribution functions. They have been traditionally classified as stochastic programming and stochastic optimization models.  Robust counterpart  The solution method to many robust program involves creating a deterministic equivalent, called the robust counterpart. The practical difficulty of a robust program depends on if its robust counterpart is computationally tractable. 12  Applications  Robust optimization for oil field development planning  Many of the optimization problems in science and engineering involve nonlinear objective functions with uncertain model. In these cases, robust optimization is applied to optimize the expected objective (sample average) over a set of realizations generated using Monte Carlo simulation. For expensive function evaluations, model selection is used to reduce the number of realizations. Techniques such as out-of-sample validation is used to reduce the number of required realizations. Recently, optimization with sample validation (OSV) (also referred to as "multilevel optimization with validation", MLOV) is proposed to significantly reduce the computational cost in robust optimization for expensive function evaluations. Robust optimization using OSV has been applied for optimization of hydrocarbon field development planning. 13  See also   Stability radius  Minimax  Minimax estimator  Minimax regret  Robust statistics  Robust decision making  Stochastic programming  Stochastic optimization  Info-gap decision theory  Probabilistic-based design optimization  Taguchi methods   References  Further reading   H.J. Greenberg. Mathematical Programming Glossary. World Wide Web, http://glossary.computing.society.informs.org/ , 1996-2006. Edited by the INFORMS Computing Society.  Ben-Tal, A., Nemirovski, A. (1998). Robust Convex Optimization. Mathematics of Operations Research 23, 769-805.    Ben-Tal A., El Ghaoui, L. and Nemirovski, A. (2006). Mathematical Programming, Special issue on Robust Optimization, Volume 107(1-2).  Ben-Tal A., El Ghaoui, L. and Nemirovski, A. (2009). Robust Optimization. Princeton Series in Applied Mathematics, Princeton University Press.         Kouvelis P. and Yu G. (1997). Robust Discrete Optimization and Its Applications, Kluwer.      Rustem B. and Howe M. (2002). Algorithms for Worst-case Design and Applications to Risk Management, Princeton University Press.       Wald, A. (1950). Statistical Decision Functions, John Wiley, NY.   External links   ROME: Robust Optimization Made Easy  [ http://robust.moshe-online.com : Robust Decision-Making Under Severe Uncertainty]   "  Category:Mathematical optimization     ‚Ü©  ‚Ü©  [ http://books.google.it/books?id=p6UHHfkQ9Y8C&lpg; ;=PR11&ots;=AqlJfX5Z0X&dq;=economics%20robust%20optimization&lr;&hl;=it&pg;=PR11#v=onepage&q;&f;=false%20 Robust portfolio optimization] ‚Ü©  Md. Asadujjaman and Kais Zaman, "Robust Portfolio Optimization under Data Uncertainty" 15th National Statistical Conference, December 2014, Dhaka, Bangladesh. ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  M. Danish Nisar. "Minimax Robustness in Signal Processing for Communications" , Shaker Verlag, ISBN 978-3-8440-0332-1, August 2011. ‚Ü©  Ben-Tal A., El Ghaoui, L. and Nemirovski, A. (2009). Robust Optimization. Princeton Series in Applied Mathematics, Princeton University Press, 9-16. ‚Ü©  ‚Ü©     