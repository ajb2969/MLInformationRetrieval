   Averaged one-dependence estimators      Averaged one-dependence estimators   Averaged one-dependence estimators ( AODE ) is a probabilistic classification learning technique. It was developed to address the attribute-independence problem of the popular naive Bayes classifier . It frequently develops substantially more accurate classifiers than naive Bayes at the cost of a modest increase in the amount of computation. 1  The AODE classifier  AODE seeks to estimate the probability of each class y given a specified set of features x 1 , ... x n , P( y | x 1 , ... x n ). To do so it uses the formula       P  ^    (  y  ∣   x  1   ,  …   x  n   )   =     ∑   i  :   1  ≤  i  ≤   n  ∧   F   (   x  i   )     ≥  m      P  ^    (  y  ,   x  i   )    ∏   j  =  1   n    P  ^    (   x  j   ∣  y  ,   x  i   )      ∑    y  ′   ∈  Y     ∑   i  :   1  ≤  i  ≤   n  ∧   F   (   x  i   )     ≥  m      P  ^    (   y  ′   ,   x  i   )    ∏   j  =  1   n    P  ^    (   x  j   ∣   y  ′   ,   x  i   )        fragments   normal-^  P    fragments  normal-(  y  normal-∣   subscript  x  1   normal-,  normal-…   subscript  x  n   normal-)       fragments   subscript    normal-:  i      1  i         n    F   subscript  x  i          m       normal-^  P    fragments  normal-(  y  normal-,   subscript  x  i   normal-)    superscript   subscript  product    j  1    n    normal-^  P    fragments  normal-(   subscript  x  j   normal-∣  y  normal-,   subscript  x  i   normal-)     fragments   subscript      superscript  y  normal-′   Y     subscript    normal-:  i      1  i         n    F   subscript  x  i          m       normal-^  P    fragments  normal-(   superscript  y  normal-′   normal-,   subscript  x  i   normal-)    superscript   subscript  product    j  1    n    normal-^  P    fragments  normal-(   subscript  x  j   normal-∣   superscript  y  normal-′   normal-,   subscript  x  i   normal-)       \hat{P}(y\mid x_{1},\ldots x_{n})=\frac{\sum_{i:1\leq i\leq n\wedge F(x_{i})%
 \geq m}\hat{P}(y,x_{i})\prod_{j=1}^{n}\hat{P}(x_{j}\mid y,x_{i})}{\sum_{y^{%
 \prime}\in Y}\sum_{i:1\leq i\leq n\wedge F(x_{i})\geq m}\hat{P}(y^{\prime},x_{%
 i})\prod_{j=1}^{n}\hat{P}(x_{j}\mid y^{\prime},x_{i})}   where     P  ^    (  ⋅  )        normal-^  P   normal-⋅    \hat{P}(\cdot)   denotes an estimate of    P   (  ⋅  )       P  normal-⋅    P(\cdot)   ,    F   (  ⋅  )       F  normal-⋅    F(\cdot)   is the frequency with which the argument appears in the sample data and m is a user specified minimum frequency with which a term must appear in order to be used in the outer summation. In recent practice m is usually set at 1.  Derivation of the AODE classifier  We seek to estimate P( y | x 1 , ... x n ). By the definition of conditional probability      P   (  y  ∣   x  1   ,  …   x  n   )   =    P   (  y  ,   x  1   ,   …   x  n    )     P   (   x  1   ,   …   x  n    )     .     fragments  P   fragments  normal-(  y  normal-∣   subscript  x  1   normal-,  normal-…   subscript  x  n   normal-)        P   y   subscript  x  1     normal-…   subscript  x  n        P    subscript  x  1     normal-…   subscript  x  n       normal-.    P(y\mid x_{1},\ldots x_{n})=\frac{P(y,x_{1},\ldots x_{n})}{P(x_{1},\ldots x_{n%
 })}.     For any    1  ≤  i  ≤  n        1  i       n     1\leq i\leq n   ,      P   (  y  ,   x  1   ,  …   x  n   )   =  P   (  y  ,   x  i   )   P   (   x  1   ,  …   x  n   ∣  y  ,   x  i   )   .     fragments  P   fragments  normal-(  y  normal-,   subscript  x  1   normal-,  normal-…   subscript  x  n   normal-)    P   fragments  normal-(  y  normal-,   subscript  x  i   normal-)   P   fragments  normal-(   subscript  x  1   normal-,  normal-…   subscript  x  n   normal-∣  y  normal-,   subscript  x  i   normal-)   normal-.    P(y,x_{1},\ldots x_{n})=P(y,x_{i})P(x_{1},\ldots x_{n}\mid y,x_{i}).     Under an assumption that x 1 , ... x n are independent given y and x i , it follows that      P   (  y  ,   x  1   ,  …   x  n   )   =  P   (  y  ,   x  i   )    ∏   j  =  1   n   P   (   x  j   ∣  y  ,   x  i   )   .     fragments  P   fragments  normal-(  y  normal-,   subscript  x  1   normal-,  normal-…   subscript  x  n   normal-)    P   fragments  normal-(  y  normal-,   subscript  x  i   normal-)    superscript   subscript  product    j  1    n   P   fragments  normal-(   subscript  x  j   normal-∣  y  normal-,   subscript  x  i   normal-)   normal-.    P(y,x_{1},\ldots x_{n})=P(y,x_{i})\prod_{j=1}^{n}P(x_{j}\mid y,x_{i}).     This formula defines a special form of One Dependence Estimator (ODE), a variant of the naive Bayes classifier that makes the above independence assumption that is weaker (and hence potentially less harmful) than the naive Bayes' independence assumption. In consequence, each ODE should create a less biased estimator than naive Bayes. However, because the base probability estimates are each conditioned by two variables rather than one, they are formed from less data (the training examples that satisfy both variables) and hence are likely to have more variance. AODE reduces this variance by averaging the estimates of all such ODEs.  Features of the AODE classifier  Like naive Bayes, AODE does not perform model selection and does not use tuneable parameters. As a result, it has low variance. It supports incremental learning whereby the classifier can be updated efficiently with information from new examples as they become available. It predicts class probabilities rather than simply predicting a single class, allowing the user to determine the confidence with which each classification can be made. Its probabilistic model can directly handle situations where some data are missing.  AODE has computational complexity    O   (   l   n  2    )       O    l   superscript  n  2      O(ln^{2})   at training time and    O   (   k   n  2    )       O    k   superscript  n  2      O(kn^{2})   at classification time, where n is the number of features, l is the number of training examples and k is the number of classes. This makes it infeasible for application to high-dimensional data. However, within that limitation, it is linear with respect to the number of training examples and hence can efficiently process large numbers of training examples.  Implementations  The free Weka machine learning suite includes an implementation of AODE.  See also   Cluster-weighted modeling   References  "  Category:Classification algorithms  Category:Bayesian statistics  Category:Statistical classification     Webb, G. I., J. Boughton, and Z. Wang (2005). "Not So Naive Bayes: Aggregating One-Dependence Estimators" . Machine Learning , 58(1), 5–24. ↩     