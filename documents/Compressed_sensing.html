<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1509">Compressed sensing</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Compressed sensing</h1><hr/>

<p><strong>Compressed sensing</strong> (also known as <strong>compressive sensing</strong>, <strong>compressive sampling</strong>, or <strong>sparse sampling</strong>) is a <a href="signal_processing" title="wikilink">signal processing</a> technique for efficiently acquiring and reconstructing a <a href="Signal_(electronics)" title="wikilink">signal</a>, by finding solutions to <a href="Underdetermined_system" title="wikilink">underdetermined linear systems</a>. This is based on the principle that, through optimization, the sparsity of a signal can be exploited to recover it from far fewer samples than required by the <a href="Nyquist–Shannon_sampling_theorem" title="wikilink">Shannon-Nyquist sampling theorem</a>. There are two conditions under which recovery is possible.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> The first one is <a class="uri" href="sparsity" title="wikilink">sparsity</a> which requires the signal to be sparse in some domain. The second one is <a href="Coherence_(signal_processing)" title="wikilink">incoherence</a> which is applied through the isometric property which is sufficient for sparse signals.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> <a href="Magnetic_resonance_imaging" title="wikilink">MRI</a> is a prominent application.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> The first multirole software based on the compressed sensing theory is designed by Lablanche &amp; Company.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>
<h2 id="overview">Overview</h2>

<p>A common goal of the engineering field of <a href="signal_processing" title="wikilink">signal processing</a> is to reconstruct a signal from a series of sampling measurements. In general, this task is impossible because there is no way to reconstruct a signal during the times that the signal is not measured. Nevertheless, with prior knowledge or assumptions about the signal, it turns out to be possible to perfectly reconstruct a signal from a series of measurements. Over time, engineers have improved their understanding of which assumptions are practical and how they can be generalized.</p>

<p>An early breakthrough in signal processing was the <a href="Nyquist–Shannon_sampling_theorem" title="wikilink">Nyquist–Shannon sampling theorem</a>. It states that if the signal's highest frequency is less than half of the sampling rate, then the signal can be reconstructed perfectly. The main idea is that with prior knowledge about constraints on the signal’s frequencies, fewer samples are needed to reconstruct the signal.</p>

<p>Around 2004, <a href="Emmanuel_Candès" title="wikilink">Emmanuel Candès</a>, <a href="Terence_Tao" title="wikilink">Terence Tao</a>, and <a href="David_Donoho" title="wikilink">David Donoho</a> proved that given knowledge about a signal's <a class="uri" href="sparsity" title="wikilink">sparsity</a>, the signal may be reconstructed with even fewer samples than the sampling theorem requires.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a><a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> This idea is the basis of compressed sensing.</p>
<h2 id="history">History</h2>

<p>Compressed sensing relies on <a href="Lp_space" title="wikilink">L1</a> techniques, which several other scientific fields have used historically.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> In statistics, the <a href="least_squares" title="wikilink">least squares</a> method was complemented by the <a href="Lp_norm" title="wikilink">

<math display="inline" id="Compressed_sensing:0">
 <semantics>
  <msup>
   <mi>L</mi>
   <mn>1</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>L</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L^{1}
  </annotation>
 </semantics>
</math>

-norm</a>, which was introduced by <a href="Pierre-Simon_Laplace" title="wikilink">Laplace</a>. Following the introduction of <a href="linear_programming" title="wikilink">linear programming</a> and <a href="George_Dantzig" title="wikilink">Dantzig</a>'s <a href="simplex_algorithm" title="wikilink">simplex algorithm</a>, the 

<math display="inline" id="Compressed_sensing:1">
 <semantics>
  <msup>
   <mi>L</mi>
   <mn>1</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>L</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L^{1}
  </annotation>
 </semantics>
</math>

-norm was used in <a href="computational_statistics" title="wikilink">computational statistics</a>. In statistical theory, the 

<math display="inline" id="Compressed_sensing:2">
 <semantics>
  <msup>
   <mi>L</mi>
   <mn>1</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>L</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L^{1}
  </annotation>
 </semantics>
</math>

-norm was used by <a href="George_W._Brown" title="wikilink">George W. Brown</a> and later writers on <a href="median-unbiased_estimator" title="wikilink">median-unbiased estimators</a>. It was used by Peter J. Huber and others working on <a href="robust_statistics" title="wikilink">robust statistics</a>. The 

<math display="inline" id="Compressed_sensing:3">
 <semantics>
  <msup>
   <mi>L</mi>
   <mn>1</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>L</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L^{1}
  </annotation>
 </semantics>
</math>

-norm was also used in signal processing, for example, in the 1970s, when seismologists constructed images of reflective layers within the earth based on data that did not seem to satisfy the <a href="Nyquist–Shannon_sampling_theorem" title="wikilink">Nyquist–Shannon criterion</a>.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> It was used in <a href="matching_pursuit" title="wikilink">matching pursuit</a> in 1993, the <a href="Lasso_regression" title="wikilink">LASSO estimator</a> by <a href="Robert_Tibshirani" title="wikilink">Robert Tibshirani</a> in 1996<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> and <a href="basis_pursuit" title="wikilink">basis pursuit</a> in 1998.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> There were theoretical results describing when these algorithms recovered sparse solutions, but the required type and number of measurements were sub-optimal and subsequently greatly improved by compressed sensing.</p>

<p>At first glance, compressed sensing might seem to violate <a href="Nyquist–Shannon_sampling_theorem" title="wikilink">the sampling theorem</a>, because compressed sensing depends on the <a href="Sparse_matrix" title="wikilink">sparsity</a> of the signal in question and not its highest frequency. This is a misconception, because the sampling theorem guarantees perfect reconstruction given sufficient, not necessary, conditions. A sampling method fundamentally different from classical fixed-rate sampling cannot "violate" the sampling theorem. Sparse signals with high frequency components can be highly under-sampled using compressed sensing compared to classical fixed-rate sampling.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>
<h2 id="method">Method</h2>
<h3 id="underdetermined-linear-system">Underdetermined linear system</h3>

<p>An <a href="underdetermined_system" title="wikilink">underdetermined system</a> of linear equations has more unknowns than equations and generally has an infinite number of solutions. In order to choose a solution to such a system, one must impose extra constraints or conditions (such as smoothness) as appropriate.</p>

<p>In compressed sensing, one adds the constraint of sparsity, allowing only solutions which have a small number of nonzero coefficients. Not all underdetermined systems of linear equations have a sparse solution. However, if there is a unique sparse solution to the underdetermined system, then the compressed sensing framework allows the recovery of that solution.</p>
<h3 id="solution-reconstruction-method">Solution / reconstruction method</h3>

<p>Compressed sensing takes advantage of the redundancy in many interesting signals—they are not pure noise. In particular, many signals are <a href="sparse_matrix" title="wikilink">sparse</a>, that is, they contain many coefficients close to or equal to zero, when represented in some domain.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> This is the same insight used in many forms of <a href="lossy_compression" title="wikilink">lossy compression</a>.</p>

<p>Compressed sensing typically starts with taking a weighted linear combination of samples also called compressive measurements in a <a href="Basis_(linear_algebra)" title="wikilink">basis</a> different from the basis in which the signal is known to be sparse. The results found by <a href="Emmanuel_Candès" title="wikilink">Emmanuel Candès</a>, <a href="Justin_Romberg" title="wikilink">Justin Romberg</a>, <a href="Terence_Tao" title="wikilink">Terence Tao</a> and <a href="David_Donoho" title="wikilink">David Donoho</a>, showed that the number of these compressive measurements can be small and still contain nearly all the useful information. Therefore, the task of converting the image back into the intended domain involves solving an underdetermined <a href="matrix_equation" title="wikilink">matrix equation</a> since the number of compressive measurements taken is smaller than the number of pixels in the full image. However, adding the constraint that the initial signal is sparse enables one to solve this underdetermined <a href="system_of_linear_equations" title="wikilink">system of linear equations</a>.</p>

<p>The least-squares solution to such problems is to minimize the <a href="L2_norm" title="wikilink">

<math display="inline" id="Compressed_sensing:4">
 <semantics>
  <msup>
   <mi>L</mi>
   <mn>2</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>L</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L^{2}
  </annotation>
 </semantics>
</math>

 norm</a>—that is, minimize the amount of energy in the system. This is usually simple mathematically (involving only a <a href="matrix_multiplication" title="wikilink">matrix multiplication</a> by the <a class="uri" href="pseudo-inverse" title="wikilink">pseudo-inverse</a> of the basis sampled in). However, this leads to poor results for many practical applications, for which the unknown coefficients have nonzero energy.</p>

<p>To enforce the sparsity constraint when solving for the underdetermined system of linear equations, one can minimize the number of nonzero components of the solution. The function counting the number of non-zero components of a vector was called the <a href="L0_norm" title="wikilink">

<math display="inline" id="Compressed_sensing:5">
 <semantics>
  <msup>
   <mi>L</mi>
   <mn>0</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>L</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L^{0}
  </annotation>
 </semantics>
</math>

 "norm"</a> by David Donoho.</p>

<p><a href="Emmanuel_Candès" title="wikilink">Candès</a>. et al., proved that for many problems it is probable that the <a href="L1_norm" title="wikilink">

<math display="inline" id="Compressed_sensing:6">
 <semantics>
  <msup>
   <mi>L</mi>
   <mn>1</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>L</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L^{1}
  </annotation>
 </semantics>
</math>

 norm</a> is equivalent to the <a href="L0_norm" title="wikilink">

<math display="inline" id="Compressed_sensing:7">
 <semantics>
  <msup>
   <mi>L</mi>
   <mn>0</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>L</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L^{0}
  </annotation>
 </semantics>
</math>

 norm</a>, in a technical sense: This equivalence result allows one to solve the 

<math display="inline" id="Compressed_sensing:8">
 <semantics>
  <msup>
   <mi>L</mi>
   <mn>1</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>L</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L^{1}
  </annotation>
 </semantics>
</math>

 problem, which is easier than the 

<math display="inline" id="Compressed_sensing:9">
 <semantics>
  <msup>
   <mi>L</mi>
   <mn>0</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>L</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L^{0}
  </annotation>
 </semantics>
</math>

 problem. Finding the candidate with the smallest 

<math display="inline" id="Compressed_sensing:10">
 <semantics>
  <msup>
   <mi>L</mi>
   <mn>1</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>L</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L^{1}
  </annotation>
 </semantics>
</math>

 norm can be expressed relatively easily as a <a href="linear_program" title="wikilink">linear program</a>, for which efficient solution methods already exist.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> When measurements may contain a finite amount of noise, <a href="basis_pursuit_denoising" title="wikilink">basis pursuit denoising</a> is preferred over linear programming, since it preserves sparsity in the face of noise and can be solved faster than an exact linear program.</p>
<h3 id="total-variation-based-cs-reconstruction">Total Variation based CS reconstruction</h3>
<h4 id="motivation-and-applications">Motivation and Applications</h4>
<h5 id="role-of-tv-regularization">Role of TV regularization</h5>

<p><a href="Total_variation" title="wikilink">Total variation</a> can be seen as a <a class="uri" href="non-negative" title="wikilink">non-negative</a> <a href="real_number" title="wikilink">real</a>-valued <a href="functional_(mathematics)" title="wikilink">functional</a> defined on the space of <a href="real_number" title="wikilink">real-valued</a> <a href="function_(mathematics)" title="wikilink">functions</a> (for the case of functions of one variable) or on the space of <a href="integrable_function" title="wikilink">integrable functions</a> (for the case of functions of several variables). For signals, especially, <a href="total_variation" title="wikilink">total variation</a> refers to the integral of the absolute <a class="uri" href="gradient" title="wikilink">gradient</a> of the signal. In signal and image reconstruction, it is applied as <a href="total_variation_regularization" title="wikilink">total variation regularization</a> where the underlying principle is that signals with excessive details have high total variation and that removing these details, while retaining important information such as edges, would reduce the total variation of the signal and make the signal subject closer to the original signal in the problem.</p>

<p>For the purpose of signal and image reconstruction, 

<math display="inline" id="Compressed_sensing:11">
 <semantics>
  <mrow>
   <mi>l</mi>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>l</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   l1
  </annotation>
 </semantics>
</math>

 minimization models are used. Other approaches also include the least-squares as has been discussed before in this article. These methods are extremely slow and return a not-so-perfect reconstruction of the signal. The current CS Regularization models attempt to address this problem by incorporating sparsity priors of the original image, one of which is the total variation (TV). Conventional TV approaches are designed to give piece-wise constant solutions. Some of these include (as discussed ahead) - constrained l1-minimization which uses an iterative scheme. This method, though fast, subsequently leads to over-smoothing of edges resulting in blurred image edges.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> TV methods with iterative re-weighting have been implemented to reduce the influence of large gradient value magnitudes in the images. This has been used in computed tomography reconstruction as a method known as edge-preserving total variation. However, as gradient magnitudes are used for estimation of relative penalty weights between the data fidelity and regularization terms, this method is not robust to noise and artifacts and accurate enough for CS image/signal reconstruction and, therefore, fails to preserve smaller structures.</p>

<p>Recent progress on this problem involves using an iteratively directional TV refinement for CS reconstruction.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a> This method would have 2 stages: the first stage would estimate and refine the initial orientation field - which is defined as a noisy point-wise initial estimate, through edge-detection, of the given image. In the second stage, the CS reconstruction model is presented by utilizing directional TV regularizer. More details about these TV-based approaches - iteratively reweighted l1 minimization, edge-preserving TV and iterative model using directional orientation field and TV- are provided below.</p>
<h4 id="existing-approaches">Existing approaches</h4>
<h5 id="iteratively-reweighted-l_1-minimization">Iteratively reweighted 

<math display="inline" id="Compressed_sensing:12">
 <semantics>
  <msub>
   <mi>l</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>l</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   l_{1}
  </annotation>
 </semantics>
</math>

 minimization <a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a></h5>

<p> In the CS reconstruction models using constrained 

<math display="inline" id="Compressed_sensing:13">
 <semantics>
  <msub>
   <mi>l</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>l</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   l_{1}
  </annotation>
 </semantics>
</math>

 minimization, larger coefficients are penalized heavily in the 

<math display="inline" id="Compressed_sensing:14">
 <semantics>
  <msub>
   <mi>l</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>l</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   l_{1}
  </annotation>
 </semantics>
</math>

 norm. It was proposed to have a weighted formulation of 

<math display="inline" id="Compressed_sensing:15">
 <semantics>
  <msub>
   <mi>l</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>l</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   l_{1}
  </annotation>
 </semantics>
</math>

 minimization designed to more democratically penalize nonzero coefficients. An iterative algorithm is used for constructing the appropriate weights.<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> Each iteration requires solving one 

<math display="inline" id="Compressed_sensing:16">
 <semantics>
  <msub>
   <mi>l</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>l</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   l_{1}
  </annotation>
 </semantics>
</math>

 minimization problem by finding the local minimum of a concave penalty function that more closely resembles the 

<math display="inline" id="Compressed_sensing:17">
 <semantics>
  <msub>
   <mi>l</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>l</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   l_{0}
  </annotation>
 </semantics>
</math>

 norm. An additional parameter, usually to avoid any sharp transitions in the penalty function curve, is introduced into the iterative equation to ensure stability and so that a zero estimate in one iteration does not necessarily lead to a zero estimate in the next iteration. The method essentially involves using the current solution for computing the weights to be used in the next iteration.</p>
<h6 id="advantages-and-disadvantages">Advantages and disadvantages</h6>

<p>Early iterations may find inaccurate sample estimates, however this method will down-sample these at a later stage to give more weight to the smaller non-zero signal estimates. One of the disadvantages is the need for defining a valid starting point as a global minimum might not be obtained every time due to the concavity of the function. Another disadvantage is that this method tends to uniformly penalize the image gradient irrespective of the underlying image structures. This causes over-smoothing of edges, especially those of low contrast regions,subsequently leading to loss of low contrast information.The advantages of this method include: reduction of the sampling rate for sparse signals; reconstruction of the image while being robust to the removal of noise and other artifacts; and use of very few iterations. This can also help in recovering images with sparse gradients.</p>

<p>In the figure shown below, <strong>P1</strong> refers to the first-step of the iterative reconstruction process, of the projection matrix <strong>P</strong> of the fan-beam geometry, which is constrained by the data fidelity term. This may contain noise and artifacts as no regularization is performed. The minimization of <strong>P1</strong> is solved through the conjugate gradient least squares method. <strong>P2</strong> refers to the second step of the iterative reconstruction process wherein it utilizes the edge-preserving total variation regularization term to remove noise and artifacts, and thus improve the quality of the reconstructed image/signal. The minimization of <strong>P2</strong> is done through a simple gradient descent method. Convergence is determined by testing, after each iteration, for image positivity, by checking if 

<math display="inline" id="Compressed_sensing:18">
 <semantics>
  <mrow>
   <msup>
    <mi>f</mi>
    <mrow>
     <mi>k</mi>
     <mo>-</mo>
     <mn>1</mn>
    </mrow>
   </msup>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>f</ci>
     <apply>
      <minus></minus>
      <ci>k</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f^{k-1}=0
  </annotation>
 </semantics>
</math>

 for the case when 

<math display="inline" id="Compressed_sensing:19">
 <semantics>
  <mrow>
   <msup>
    <mi>f</mi>
    <mrow>
     <mi>k</mi>
     <mo>-</mo>
     <mn>1</mn>
    </mrow>
   </msup>
   <mo><</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <lt></lt>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>f</ci>
     <apply>
      <minus></minus>
      <ci>k</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f^{k-1}<0
  </annotation>
 </semantics>
</math>

 (Note that 

<math display="inline" id="Compressed_sensing:20">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 refers to the different x-ray linear attenuation coefficients at different voxels of the patient image).</p>
<h5 id="edge-preserving-total-variation-tv-based-compressed-sensing">Edge-preserving total variation (TV) based compressed sensing<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a></h5>

<p> This is an iterative CT reconstruction algorithm with edge-preserving TV regularization to reconstruct CT images from highly undersampled data obtained at low dose CT through low current levels (milliampere). In order to reduce the imaging dose, one of the approaches used is to reduce the number of x-ray projections acquired by the scanner detectors. However, this insufficient projection data which is used to reconstruct the CT image can cause streaking artifacts. Furthermore, using these insufficient projections in standard TV algorithms end up making the problem under-determined and thus leading to infinitely many possible solutions. In this method, an additional penalty weighted function is assigned to the original TV norm. This allows for easier detection of sharp discontinuities in intensity in the images and thereby adapt the weight to store the recovered edge information during the process of signal/image reconstruction. The parameter 

<math display="inline" id="Compressed_sensing:21">
 <semantics>
  <mi>σ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>σ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma
  </annotation>
 </semantics>
</math>

 controls the amount of smoothing applied to the pixels at the edges to differentiate them from the non-edge pixels. The value of 

<math display="inline" id="Compressed_sensing:22">
 <semantics>
  <mi>σ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>σ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma
  </annotation>
 </semantics>
</math>

 is changed adaptively based on the values of the histogram of the gradient magnitude so that a certain percentage of pixels have gradient values larger than 

<math display="inline" id="Compressed_sensing:23">
 <semantics>
  <mi>σ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>σ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma
  </annotation>
 </semantics>
</math>

. The edge-preserving total variation term, thus, becomes sparser and this speeds up the implementation. A two-step iteration process known as forward-backward splitting algorithm is used.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a> The optimization problem is split into two sub-problems which are then solved with the conjugate gradient least squares method<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a> and the simple gradient descent method respectively. The method is stopped when the desired convergence has been achieved or if the maximum number of iterations is reached.</p>
<h5 id="advantages-and-disadvantages-1">Advantages and disadvantages</h5>

<p>Some of the disadvantages of this method are the absence of smaller structures in the reconstructed image and degradation of image resolution. This edge preserving TV algorithm, however, requires fewer iterations than the conventional TV algorithm.<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a> Analyzing the horizontal and vertical intensity profiles of the reconstructed images, it can be seen that there are sharp jumps at edge points and negligible, minor fluctuation at non-edge points. Thus, this method leads to low relative error and higher correlation as compared to the TV method. It also effectively suppresses and removes any form of image noise and image artifacts such as streaking.</p>
<h5 id="iterative-model-using-a-directional-orientation-field-and-directional-total-variation">Iterative model using a directional orientation field and directional total variation<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a></h5>

<p>To prevent over-smoothing of edges and texture details and to obtain a reconstructed CS image which is accurate and robust to noise and artifacts, this method is used. First, an initial estimate of the noisy point-wise orientation field of the image 

<math display="inline" id="Compressed_sensing:24">
 <semantics>
  <mi>I</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>I</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Compressed_sensing:25">
 <semantics>
  <mover accent="true">
   <mi>d</mi>
   <mo stretchy="false">^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>d</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{d}
  </annotation>
 </semantics>
</math>

, is obtained. This noisy orientation field is defined so that it can be refined at a later stage to reduce the noise influences in orientation field estimation.A coarse orientation field estimation is then introduced based on structure tensor which is formulated as:<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a> 

<math display="inline" id="Compressed_sensing:26">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>J</mi>
     <mi>ρ</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mo>∇</mo>
      <msub>
       <mi>I</mi>
       <mi>σ</mi>
      </msub>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>G</mi>
     <mi>ρ</mi>
    </msub>
    <mo>*</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mrow>
       <mo>∇</mo>
       <msub>
        <mi>I</mi>
        <mi>σ</mi>
       </msub>
      </mrow>
      <mo>⊗</mo>
      <mrow>
       <mo>∇</mo>
       <msub>
        <mi>I</mi>
        <mi>σ</mi>
       </msub>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>(</mo>
    <mtable>
     <mtr>
      <mtd columnalign="center">
       <msub>
        <mi>J</mi>
        <mn>11</mn>
       </msub>
      </mtd>
      <mtd columnalign="center">
       <msub>
        <mi>J</mi>
        <mn>12</mn>
       </msub>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="center">
       <msub>
        <mi>J</mi>
        <mn>12</mn>
       </msub>
      </mtd>
      <mtd columnalign="center">
       <msub>
        <mi>J</mi>
        <mn>22</mn>
       </msub>
      </mtd>
     </mtr>
    </mtable>
    <mo>)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>J</ci>
       <ci>ρ</ci>
      </apply>
      <apply>
       <ci>normal-∇</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>I</ci>
        <ci>σ</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>G</ci>
       <ci>ρ</ci>
      </apply>
      <apply>
       <csymbol cd="latexml">tensor-product</csymbol>
       <apply>
        <ci>normal-∇</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>I</ci>
         <ci>σ</ci>
        </apply>
       </apply>
       <apply>
        <ci>normal-∇</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>I</ci>
         <ci>σ</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <matrix>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>J</ci>
        <cn type="integer">11</cn>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>J</ci>
        <cn type="integer">12</cn>
       </apply>
      </matrixrow>
      <matrixrow>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>J</ci>
        <cn type="integer">12</cn>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>J</ci>
        <cn type="integer">22</cn>
       </apply>
      </matrixrow>
     </matrix>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   J_{\rho}(\nabla I_{\sigma})=G_{\rho}*(\nabla I_{\sigma}\otimes\nabla I_{\sigma%
})=\begin{pmatrix}J_{11}&J_{12}\\
J_{12}&J_{22}\end{pmatrix}
  </annotation>
 </semantics>
</math>

. Here, 

<math display="inline" id="Compressed_sensing:27">
 <semantics>
  <msub>
   <mi>J</mi>
   <mi>ρ</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>J</ci>
    <ci>ρ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   J_{\rho}
  </annotation>
 </semantics>
</math>

 refers to the structure tensor related with the image pixel point (i,j) having standard deviation 

<math display="inline" id="Compressed_sensing:28">
 <semantics>
  <mi>ρ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ρ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \rho
  </annotation>
 </semantics>
</math>

. 

<math display="inline" id="Compressed_sensing:29">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>

 refers to the Gaussian kernel 

<math display="inline" id="Compressed_sensing:30">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mn>0</mn>
   <mo>,</mo>
   <msup>
    <mi>ρ</mi>
    <mn>2</mn>
   </msup>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="open">
    <cn type="integer">0</cn>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>ρ</ci>
     <cn type="integer">2</cn>
    </apply>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (0,\rho^{2})
  </annotation>
 </semantics>
</math>

 with standard deviation 

<math display="inline" id="Compressed_sensing:31">
 <semantics>
  <mi>ρ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ρ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \rho
  </annotation>
 </semantics>
</math>

. 

<math display="inline" id="Compressed_sensing:32">
 <semantics>
  <mi>σ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>σ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma
  </annotation>
 </semantics>
</math>

 refers to the manually defined parameter for the image 

<math display="inline" id="Compressed_sensing:33">
 <semantics>
  <mi>I</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>I</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I
  </annotation>
 </semantics>
</math>

 below which the edge detection is insensitive to noise. 

<math display="inline" id="Compressed_sensing:34">
 <semantics>
  <mrow>
   <mo>∇</mo>
   <msub>
    <mi>I</mi>
    <mi>σ</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-∇</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>I</ci>
     <ci>σ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \nabla I_{\sigma}
  </annotation>
 </semantics>
</math>

 refers to the gradient of the image 

<math display="inline" id="Compressed_sensing:35">
 <semantics>
  <mi>I</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>I</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Compressed_sensing:36">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mrow>
    <mrow>
     <mo>∇</mo>
     <msub>
      <mi>I</mi>
      <mi>σ</mi>
     </msub>
    </mrow>
    <mo>⊗</mo>
    <mrow>
     <mo>∇</mo>
     <msub>
      <mi>I</mi>
      <mi>σ</mi>
     </msub>
    </mrow>
   </mrow>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">tensor-product</csymbol>
    <apply>
     <ci>normal-∇</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>I</ci>
      <ci>σ</ci>
     </apply>
    </apply>
    <apply>
     <ci>normal-∇</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>I</ci>
      <ci>σ</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (\nabla I_{\sigma}\otimes\nabla I_{\sigma})
  </annotation>
 </semantics>
</math>

 refers to the tensor product obtained by using this gradient.</p>

<p>The structure tensor obtained is convolved with a Gaussian kernel 

<math display="inline" id="Compressed_sensing:37">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>

 to improve the accuracy of the orientation estimate with 

<math display="inline" id="Compressed_sensing:38">
 <semantics>
  <mi>σ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>σ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma
  </annotation>
 </semantics>
</math>

 being set to high values to account for the unknown noise levels. For every pixel (i,j) in the image, the structure tensor J is a symmetric and positive semi-definite matrix. Convolving all the pixels in the image with 

<math display="inline" id="Compressed_sensing:39">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>

, gives orthonormal eigen vectors ω and υ of the 

<math display="inline" id="Compressed_sensing:40">
 <semantics>
  <mi>J</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>J</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   J
  </annotation>
 </semantics>
</math>

 matrix. ω points in the direction of the dominant orientation having the largest contrast and υ points in the direction of the structure orientation having the smallest contrast. The orientation field coarse initial estimation 

<math display="inline" id="Compressed_sensing:41">
 <semantics>
  <mover accent="true">
   <mi>d</mi>
   <mo stretchy="false">^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>d</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{d}
  </annotation>
 </semantics>
</math>

 is defined as 

<math display="inline" id="Compressed_sensing:42">
 <semantics>
  <mover accent="true">
   <mi>d</mi>
   <mo stretchy="false">^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>d</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{d}
  </annotation>
 </semantics>
</math>

 = υ. This estimate is accurate at strong edges. However, at weak edges or on regions with noise, its reliability decreases.</p>

<p>To overcome this drawback, a refined orientation model is defined in which the data term reduces the effect of noise and improves accuracy while the second penalty term with the L2-norm is a fidelity term which ensures accuracy of initial coarse estimation.</p>

<p>This orientation field is introduced into the directional total variation optimization model for CS reconstruction through the equation

<math display="block" id="Compressed_sensing:43">
 <semantics>
  <mrow>
   <mrow>
    <mi>m</mi>
    <mi>i</mi>
    <msub>
     <mi>n</mi>
     <merror class="ltx_ERROR undefined undefined">
      <mtext>\Chi</mtext>
     </merror>
    </msub>
    <msub>
     <mrow>
      <mo fence="true">∥</mo>
      <mrow>
       <mo>∇</mo>
       <mrow>
        <merror class="ltx_ERROR undefined undefined">
         <mtext>\Chi</mtext>
        </merror>
        <mo>∙</mo>
        <mi>d</mi>
       </mrow>
      </mrow>
      <mo fence="true">∥</mo>
     </mrow>
     <mn>1</mn>
    </msub>
   </mrow>
   <mo>+</mo>
   <mrow>
    <mpadded width="+5pt">
     <mfrac>
      <mi>λ</mi>
      <mn>2</mn>
     </mfrac>
    </mpadded>
    <msubsup>
     <mrow>
      <mo fence="true">∥</mo>
      <mrow>
       <mi>Y</mi>
       <mo>-</mo>
       <mrow>
        <mi mathvariant="normal">Φ</mi>
        <merror class="ltx_ERROR undefined undefined">
         <mtext>\Chi</mtext>
        </merror>
       </mrow>
      </mrow>
      <mo fence="true">∥</mo>
     </mrow>
     <mn>2</mn>
     <mn>2</mn>
    </msubsup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <ci>m</ci>
     <ci>i</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>n</ci>
      <mtext>\Chi</mtext>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="latexml">norm</csymbol>
       <apply>
        <ci>normal-∇</ci>
        <apply>
         <ci>normal-∙</ci>
         <mtext>\Chi</mtext>
         <ci>d</ci>
        </apply>
       </apply>
      </apply>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <ci>λ</ci>
      <cn type="integer">2</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="latexml">norm</csymbol>
        <apply>
         <minus></minus>
         <ci>Y</ci>
         <apply>
          <times></times>
          <ci>normal-Φ</ci>
          <mtext>\Chi</mtext>
         </apply>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   min_{\Chi}\lVert\nabla\Chi\bullet d\rVert_{1}+\frac{\lambda}{2}\ \lVert Y-\Phi%
\Chi\rVert^{2}_{2}
  </annotation>
 </semantics>
</math>

. 

<math display="inline" id="Compressed_sensing:44">
 <semantics>
  <merror class="ltx_ERROR undefined undefined">
   <mtext>\Chi</mtext>
  </merror>
  <annotation-xml encoding="MathML-Content">
   <mtext>\Chi</mtext>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Chi
  </annotation>
 </semantics>
</math>

 is the objective signal which needs to be recovered. Y is the corresponding measurement vector, d is the iterative refined orientation field and 

<math display="inline" id="Compressed_sensing:45">
 <semantics>
  <mi mathvariant="normal">Φ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-Φ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Phi
  </annotation>
 </semantics>
</math>

 is the CS measurement matrix. This method undergoes a few iterations ultimately leading to convergence.

<math display="inline" id="Compressed_sensing:46">
 <semantics>
  <mover accent="true">
   <mi>d</mi>
   <mo stretchy="false">^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>d</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{d}
  </annotation>
 </semantics>
</math>

 is the orientation field approximate estimation of the reconstructed image 

<math display="inline" id="Compressed_sensing:47">
 <semantics>
  <msup>
   <mi>X</mi>
   <mrow>
    <mi>k</mi>
    <mo>-</mo>
    <mn>1</mn>
   </mrow>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>X</ci>
    <apply>
     <minus></minus>
     <ci>k</ci>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X^{k-1}
  </annotation>
 </semantics>
</math>

 from the previous iteration (in order to check for convergence and the subsequent optical performance, the previous iteration is used). For the two vector fields represented by 

<math display="inline" id="Compressed_sensing:48">
 <semantics>
  <merror class="ltx_ERROR undefined undefined">
   <mtext>\Chi</mtext>
  </merror>
  <annotation-xml encoding="MathML-Content">
   <mtext>\Chi</mtext>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Chi
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Compressed_sensing:49">
 <semantics>
  <mi>d</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Compressed_sensing:50">
 <semantics>
  <mrow>
   <merror class="ltx_ERROR undefined undefined">
    <mtext>\Chi</mtext>
   </merror>
   <mo>∙</mo>
   <mi>d</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-∙</ci>
    <mtext>\Chi</mtext>
    <ci>d</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Chi\bullet d
  </annotation>
 </semantics>
</math>

 refers to the multiplication of respective horizontal and vertical vector elements of 

<math display="inline" id="Compressed_sensing:51">
 <semantics>
  <merror class="ltx_ERROR undefined undefined">
   <mtext>\Chi</mtext>
  </merror>
  <annotation-xml encoding="MathML-Content">
   <mtext>\Chi</mtext>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Chi
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Compressed_sensing:52">
 <semantics>
  <mi>d</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d
  </annotation>
 </semantics>
</math>

 followed by their subsequent addition. These equations are reduced to a series of convex minimization problems which are then solved with a combination of variable splitting and augmented Lagrangian (FFT-based fast solver with a closed form solution) methods.<a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a> It (Augmented Lagrangian) is considered equivalent to the split Bregman iteration which ensures convergence of this method. The orientation field, d is defined as being equal to 

<math display="inline" id="Compressed_sensing:53">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <msub>
    <mi>d</mi>
    <mi>h</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>d</mi>
    <mi>v</mi>
   </msub>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="open">
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>d</ci>
     <ci>h</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>d</ci>
     <ci>v</ci>
    </apply>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (d_{h},d_{v})
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Compressed_sensing:54">
 <semantics>
  <mrow>
   <msub>
    <mi>d</mi>
    <mi>h</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>d</mi>
    <mi>v</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>d</ci>
     <ci>h</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>d</ci>
     <ci>v</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d_{h},d_{v}
  </annotation>
 </semantics>
</math>

 define the horizontal and vertical estimates of 

<math display="inline" id="Compressed_sensing:55">
 <semantics>
  <mi>d</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d
  </annotation>
 </semantics>
</math>

.</p>
<figure><b>(Figure)</b>
<figcaption>Augmented Lagrangian method for orientation field and iterative directional field refinement models</figcaption>
</figure>

<p>The Augmented Lagrangian method for the orientation field, 

<math display="inline" id="Compressed_sensing:56">
 <semantics>
  <mrow>
   <mrow>
    <mi>m</mi>
    <mi>i</mi>
    <msub>
     <mi>n</mi>
     <merror class="ltx_ERROR undefined undefined">
      <mtext>\Chi</mtext>
     </merror>
    </msub>
    <msub>
     <mrow>
      <mo fence="true">∥</mo>
      <mrow>
       <mo>∇</mo>
       <mrow>
        <merror class="ltx_ERROR undefined undefined">
         <mtext>\Chi</mtext>
        </merror>
        <mo>∙</mo>
        <mi>d</mi>
       </mrow>
      </mrow>
      <mo fence="true">∥</mo>
     </mrow>
     <mn>1</mn>
    </msub>
   </mrow>
   <mo>+</mo>
   <mrow>
    <mpadded width="+5pt">
     <mfrac>
      <mi>λ</mi>
      <mn>2</mn>
     </mfrac>
    </mpadded>
    <msubsup>
     <mrow>
      <mo fence="true">∥</mo>
      <mrow>
       <mi>Y</mi>
       <mo>-</mo>
       <mrow>
        <mi mathvariant="normal">Φ</mi>
        <merror class="ltx_ERROR undefined undefined">
         <mtext>\Chi</mtext>
        </merror>
       </mrow>
      </mrow>
      <mo fence="true">∥</mo>
     </mrow>
     <mn>2</mn>
     <mn>2</mn>
    </msubsup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <ci>m</ci>
     <ci>i</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>n</ci>
      <mtext>\Chi</mtext>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="latexml">norm</csymbol>
       <apply>
        <ci>normal-∇</ci>
        <apply>
         <ci>normal-∙</ci>
         <mtext>\Chi</mtext>
         <ci>d</ci>
        </apply>
       </apply>
      </apply>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <ci>λ</ci>
      <cn type="integer">2</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="latexml">norm</csymbol>
        <apply>
         <minus></minus>
         <ci>Y</ci>
         <apply>
          <times></times>
          <ci>normal-Φ</ci>
          <mtext>\Chi</mtext>
         </apply>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   min_{\Chi}\lVert\nabla\Chi\bullet d\rVert_{1}+\frac{\lambda}{2}\ \lVert Y-\Phi%
\Chi\rVert^{2}_{2}
  </annotation>
 </semantics>
</math>

, involves initializing 

<math display="inline" id="Compressed_sensing:57">
 <semantics>
  <mrow>
   <msub>
    <mi>d</mi>
    <mi>h</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>d</mi>
    <mi>v</mi>
   </msub>
   <mo>,</mo>
   <mi>H</mi>
   <mo>,</mo>
   <mi>V</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>d</ci>
     <ci>h</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>d</ci>
     <ci>v</ci>
    </apply>
    <ci>H</ci>
    <ci>V</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d_{h},d_{v},H,V
  </annotation>
 </semantics>
</math>

 and then finding the approximate minimizer of 

<math display="inline" id="Compressed_sensing:58">
 <semantics>
  <msub>
   <mi>L</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>L</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L_{1}
  </annotation>
 </semantics>
</math>

 with respect to these variables. The Lagrangian multipliers are then updated and the iterative process is stopped when convergence is achieved. For the iterative directional total variation refinement model, the augmented lagrangian method involves initializing 

<math display="inline" id="Compressed_sensing:59">
 <semantics>
  <mrow>
   <merror class="ltx_ERROR undefined undefined">
    <mtext>\Chi</mtext>
   </merror>
   <mo>,</mo>
   <mi>P</mi>
   <mo>,</mo>
   <mi>Q</mi>
   <mo>,</mo>
   <msub>
    <mi>λ</mi>
    <mi>P</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>λ</mi>
    <mi>Q</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <mtext>\Chi</mtext>
    <ci>P</ci>
    <ci>Q</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>λ</ci>
     <ci>P</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>λ</ci>
     <ci>Q</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Chi,P,Q,\lambda_{P},\lambda_{Q}
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a></p>

<p>Here, 

<math display="inline" id="Compressed_sensing:60">
 <semantics>
  <mrow>
   <mi>H</mi>
   <mo>,</mo>
   <mi>V</mi>
   <mo>,</mo>
   <mi>P</mi>
   <mo>,</mo>
   <mi>Q</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <ci>H</ci>
    <ci>V</ci>
    <ci>P</ci>
    <ci>Q</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H,V,P,Q
  </annotation>
 </semantics>
</math>

 are newly introduced variables where 

<math display="inline" id="Compressed_sensing:61">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H
  </annotation>
 </semantics>
</math>

 = 

<math display="inline" id="Compressed_sensing:62">
 <semantics>
  <mrow>
   <mo>∇</mo>
   <msub>
    <mi>d</mi>
    <mi>h</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-∇</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>d</ci>
     <ci>h</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \nabla d_{h}
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Compressed_sensing:63">
 <semantics>
  <mi>V</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>V</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V
  </annotation>
 </semantics>
</math>

 = 

<math display="inline" id="Compressed_sensing:64">
 <semantics>
  <mrow>
   <mo>∇</mo>
   <msub>
    <mi>d</mi>
    <mi>v</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-∇</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>d</ci>
     <ci>v</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \nabla d_{v}
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Compressed_sensing:65">
 <semantics>
  <mi>P</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>P</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P
  </annotation>
 </semantics>
</math>

 = 

<math display="inline" id="Compressed_sensing:66">
 <semantics>
  <mrow>
   <mo>∇</mo>
   <merror class="ltx_ERROR undefined undefined">
    <mtext>\Chi</mtext>
   </merror>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-∇</ci>
    <mtext>\Chi</mtext>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \nabla\Chi
  </annotation>
 </semantics>
</math>

, and 

<math display="inline" id="Compressed_sensing:67">
 <semantics>
  <mi>Q</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Q</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Q
  </annotation>
 </semantics>
</math>

 = 

<math display="inline" id="Compressed_sensing:68">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mo>∙</mo>
   <mi>d</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-∙</ci>
    <ci>P</ci>
    <ci>d</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P\bullet d
  </annotation>
 </semantics>
</math>

. 

<math display="inline" id="Compressed_sensing:69">
 <semantics>
  <mrow>
   <msub>
    <mi>λ</mi>
    <mi>H</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>λ</mi>
    <mi>V</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>λ</mi>
    <mi>P</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>λ</mi>
    <mi>Q</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>λ</ci>
     <ci>H</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>λ</ci>
     <ci>V</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>λ</ci>
     <ci>P</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>λ</ci>
     <ci>Q</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda_{H},\lambda_{V},\lambda_{P},\lambda_{Q}
  </annotation>
 </semantics>
</math>

 are the Lagrangian multipliers for 

<math display="inline" id="Compressed_sensing:70">
 <semantics>
  <mrow>
   <mi>H</mi>
   <mo>,</mo>
   <mi>V</mi>
   <mo>,</mo>
   <mi>P</mi>
   <mo>,</mo>
   <mi>Q</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <ci>H</ci>
    <ci>V</ci>
    <ci>P</ci>
    <ci>Q</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H,V,P,Q
  </annotation>
 </semantics>
</math>

. For each iteration, the approximate minimizer of 

<math display="inline" id="Compressed_sensing:71">
 <semantics>
  <msub>
   <mi>L</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>L</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L_{2}
  </annotation>
 </semantics>
</math>

 with respect to variables (

<math display="inline" id="Compressed_sensing:72">
 <semantics>
  <mrow>
   <merror class="ltx_ERROR undefined undefined">
    <mtext>\Chi</mtext>
   </merror>
   <mo>,</mo>
   <mi>P</mi>
   <mo>,</mo>
   <mi>Q</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <mtext>\Chi</mtext>
    <ci>P</ci>
    <ci>Q</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Chi,P,Q
  </annotation>
 </semantics>
</math>

) is calculated. And as in the field refinement model, the lagrangian multipliers are updated and the iterative process is stopped when convergence is achieved.</p>

<p>For the orientation field refinement model, the Lagrangian multipliers are updated in the iterative process as follows:</p>

<p>

<math display="inline" id="Compressed_sensing:73">
 <semantics>
  <mrow>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>λ</mi>
      <mi>H</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mi>k</mi>
   </msup>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>λ</mi>
      <mi>H</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mi>k</mi>
     <mo>-</mo>
     <mn>1</mn>
    </mrow>
   </msup>
   <mo>+</mo>
   <msub>
    <mi>γ</mi>
    <mi>H</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <msup>
     <mi>H</mi>
     <mi>k</mi>
    </msup>
    <mo>-</mo>
    <mo>∇</mo>
    <msup>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>d</mi>
       <mi>h</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>k</mi>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>H</ci>
      </apply>
      <ci>normal-)</ci>
     </cerror>
     <ci>k</ci>
    </apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>H</ci>
      </apply>
      <ci>normal-)</ci>
     </cerror>
     <apply>
      <minus></minus>
      <ci>k</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <plus></plus>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>γ</ci>
     <ci>H</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>H</ci>
      <ci>k</ci>
     </apply>
     <minus></minus>
     <ci>normal-∇</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>d</ci>
        <ci>h</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
      <ci>k</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (\lambda_{H})^{k}=(\lambda_{H})^{k-1}+\gamma_{H}(H^{k}-\nabla(d_{h})^{k})
  </annotation>
 </semantics>
</math>

 

<math display="inline" id="Compressed_sensing:74">
 <semantics>
  <mrow>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>λ</mi>
      <mi>V</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mi>k</mi>
   </msup>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>λ</mi>
      <mi>V</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mi>k</mi>
     <mo>-</mo>
     <mn>1</mn>
    </mrow>
   </msup>
   <mo>+</mo>
   <msub>
    <mi>γ</mi>
    <mi>V</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <msup>
     <mi>V</mi>
     <mi>k</mi>
    </msup>
    <mo>-</mo>
    <mo>∇</mo>
    <msup>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>d</mi>
       <mi>v</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>k</mi>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>V</ci>
      </apply>
      <ci>normal-)</ci>
     </cerror>
     <ci>k</ci>
    </apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>V</ci>
      </apply>
      <ci>normal-)</ci>
     </cerror>
     <apply>
      <minus></minus>
      <ci>k</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <plus></plus>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>γ</ci>
     <ci>V</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>V</ci>
      <ci>k</ci>
     </apply>
     <minus></minus>
     <ci>normal-∇</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>d</ci>
        <ci>v</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
      <ci>k</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (\lambda_{V})^{k}=(\lambda_{V})^{k-1}+\gamma_{V}(V^{k}-\nabla(d_{v})^{k})
  </annotation>
 </semantics>
</math>

</p>

<p>For the iterative directional total variation refinement model, the Lagrangian multipliers are updated as follows:</p>

<p>

<math display="inline" id="Compressed_sensing:75">
 <semantics>
  <mrow>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>λ</mi>
      <mi>P</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mi>k</mi>
   </msup>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>λ</mi>
      <mi>P</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mi>k</mi>
     <mo>-</mo>
     <mn>1</mn>
    </mrow>
   </msup>
   <mo>+</mo>
   <msub>
    <mi>γ</mi>
    <mi>P</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <msup>
     <mi>P</mi>
     <mi>k</mi>
    </msup>
    <mo>-</mo>
    <mo>∇</mo>
    <msup>
     <mrow>
      <mo stretchy="false">(</mo>
      <merror class="ltx_ERROR undefined undefined">
       <mtext>\Chi</mtext>
      </merror>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>k</mi>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>P</ci>
      </apply>
      <ci>normal-)</ci>
     </cerror>
     <ci>k</ci>
    </apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>P</ci>
      </apply>
      <ci>normal-)</ci>
     </cerror>
     <apply>
      <minus></minus>
      <ci>k</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <plus></plus>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>γ</ci>
     <ci>P</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>P</ci>
      <ci>k</ci>
     </apply>
     <minus></minus>
     <ci>normal-∇</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <mtext>\Chi</mtext>
       <ci>normal-)</ci>
      </cerror>
      <ci>k</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (\lambda_{P})^{k}=(\lambda_{P})^{k-1}+\gamma_{P}(P^{k}-\nabla(\Chi)^{k})
  </annotation>
 </semantics>
</math>

 

<math display="inline" id="Compressed_sensing:76">
 <semantics>
  <mrow>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>λ</mi>
      <mi>Q</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mi>k</mi>
   </msup>
   <mo>=</mo>
   <mrow>
    <msup>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>λ</mi>
       <mi>Q</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <mrow>
      <mi>k</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msup>
    <mo>+</mo>
    <mrow>
     <msub>
      <mi>γ</mi>
      <mi>Q</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <msup>
        <mi>Q</mi>
        <mi>k</mi>
       </msup>
       <mo>-</mo>
       <mrow>
        <msup>
         <mi>P</mi>
         <mi>k</mi>
        </msup>
        <mo>∙</mo>
        <mi>d</mi>
       </mrow>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>λ</ci>
      <ci>Q</ci>
     </apply>
     <ci>k</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>Q</ci>
      </apply>
      <apply>
       <minus></minus>
       <ci>k</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>γ</ci>
       <ci>Q</ci>
      </apply>
      <apply>
       <minus></minus>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>Q</ci>
        <ci>k</ci>
       </apply>
       <apply>
        <ci>normal-∙</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>P</ci>
         <ci>k</ci>
        </apply>
        <ci>d</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (\lambda_{Q})^{k}=(\lambda_{Q})^{k-1}+\gamma_{Q}(Q^{k}-P^{k}\bullet d)
  </annotation>
 </semantics>
</math>

</p>

<p>Here, 

<math display="inline" id="Compressed_sensing:77">
 <semantics>
  <mrow>
   <msub>
    <mi>γ</mi>
    <mi>H</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>γ</mi>
    <mi>V</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>γ</mi>
    <mi>P</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>γ</mi>
    <mi>Q</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>γ</ci>
     <ci>H</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>γ</ci>
     <ci>V</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>γ</ci>
     <ci>P</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>γ</ci>
     <ci>Q</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma_{H},\gamma_{V},\gamma_{P},\gamma_{Q}
  </annotation>
 </semantics>
</math>

 are positive constants.</p>
<h5 id="advantages-and-disadvantages-2">Advantages and disadvantages</h5>

<p>Based on Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics and known ground-truth images for testing performance, it is concluded that iterative directional total variation has a better reconstructed performance than the non-iterative methods in preserving edge and texture areas. The orientation field refinement model plays a major role in this improvement in performance as it increases the number of directionless pixels in the flat area while enhancing the orientation field consistency in the regions with edges.</p>
<h2 id="applications">Applications</h2>

<p>The field of compressive sensing is related to several topics in signal processing and computational mathematics, such as <a href="underdetermined_system" title="wikilink">underdetermined linear-systems</a>, <a href="group_testing" title="wikilink">group testing</a>, heavy hitters, <a href="sparse_coding" title="wikilink">sparse coding</a>, <a class="uri" href="multiplexing" title="wikilink">multiplexing</a>, sparse sampling, and finite rate of innovation. Its broad scope and generality has enabled several innovative CS-enhanced approaches in signal processing and compression, solution of inverse problems, design of radiating systems, radar and through-the-wall imaging, and antenna characterization.<a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a> Imaging techniques having a strong affinity with compressive sensing include <a href="coded_aperture" title="wikilink">coded aperture</a> and <a href="computational_photography" title="wikilink">computational photography</a>. Implementations of compressive sensing in hardware at different <a href="technology_readiness_level" title="wikilink">technology readiness levels</a> is available.<a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a></p>

<p>Conventional CS reconstruction uses sparse signals (usually sampled at a rate less than the Nyquist sampling rate) for reconstruction through constrained 

<math display="inline" id="Compressed_sensing:78">
 <semantics>
  <msub>
   <mi>l</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>l</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   l_{1}
  </annotation>
 </semantics>
</math>

 minimization. One of the earliest applications of such an approach was in reflection seismology which used sparse reflected signals from band-limited data for tracking changes between sub-surface layers.<a class="footnoteRef" href="#fn32" id="fnref32"><sup>32</sup></a> When the LASSO model came into prominence in the 1990s as a statistical method for selection of sparse models,<a class="footnoteRef" href="#fn33" id="fnref33"><sup>33</sup></a> this method was further used in computational harmonic analysis for sparse signal representation from over-complete dictionaries. Some of the other applications include incoherent sampling of radar pulses. The work by <em>Boyd et al.</em><a class="footnoteRef" href="#fn34" id="fnref34"><sup>34</sup></a> has applied the LASSO model- for selection of sparse models- towards analog to digital converters (the current ones use a sampling rate higher than the Nyquist rate along with the quantized Shannon representation). This would involve a parallel architecture in which the polarity of the analog signal changes at a high rate followed by digitizing the integral at the end of each time-interval to obtain the converted digital signal.</p>
<h3 id="photography">Photography</h3>

<p>Compressed sensing is used in a mobile phone camera sensor. The approach allows a reduction in image acquisition energy per image by as much as a factor of 15 at the cost of complex decompression algorithms; the computation may require an off-device implementation.<a class="footnoteRef" href="#fn35" id="fnref35"><sup>35</sup></a></p>

<p>Compressed sensing is used in single-pixel cameras from <a href="Rice_University" title="wikilink">Rice University</a>.<a class="footnoteRef" href="#fn36" id="fnref36"><sup>36</sup></a> <a href="Bell_Labs" title="wikilink">Bell Labs</a> employed the technique in a lensless single-pixel camera that takes stills using repeated snapshots of randomly chosen apertures from a grid. Image quality improves with the number of snapshots, and generally requires a small fraction of the data of conventional imaging, while eliminating lens/focus-related aberrations.<a class="footnoteRef" href="#fn37" id="fnref37"><sup>37</sup></a><a class="footnoteRef" href="#fn38" id="fnref38"><sup>38</sup></a></p>
<h3 id="holography">Holography</h3>

<p>Compressed sensing can be used to improve image reconstruction in <a class="uri" href="holography" title="wikilink">holography</a> by increasing the number of <a href="voxel" title="wikilink">voxels</a> one can infer from a single hologram.<a class="footnoteRef" href="#fn39" id="fnref39"><sup>39</sup></a><a class="footnoteRef" href="#fn40" id="fnref40"><sup>40</sup></a><a class="footnoteRef" href="#fn41" id="fnref41"><sup>41</sup></a> It is also used for image retrieval from undersampled measurements in optical <a class="footnoteRef" href="#fn42" id="fnref42"><sup>42</sup></a><a class="footnoteRef" href="#fn43" id="fnref43"><sup>43</sup></a> and millimeter-wave <a class="footnoteRef" href="#fn44" id="fnref44"><sup>44</sup></a> holography.</p>
<h3 id="facial-recognition">Facial recognition</h3>

<p>Compressed sensing is being used in facial recognition applications.<a class="footnoteRef" href="#fn45" id="fnref45"><sup>45</sup></a></p>
<h3 id="magnetic-resonance-imaging">Magnetic resonance imaging</h3>

<p>Compressed sensing has been used <a class="footnoteRef" href="#fn46" id="fnref46"><sup>46</sup></a><a class="footnoteRef" href="#fn47" id="fnref47"><sup>47</sup></a> to shorten <a href="magnetic_resonance_imaging" title="wikilink">magnetic resonance imaging</a> scanning sessions on conventional hardware.<a class="footnoteRef" href="#fn48" id="fnref48"><sup>48</sup></a><a class="footnoteRef" href="#fn49" id="fnref49"><sup>49</sup></a><a class="footnoteRef" href="#fn50" id="fnref50"><sup>50</sup></a> Reconstruction methods includes ISTA, FISTA, SISTA, ePRESS,<a class="footnoteRef" href="#fn51" id="fnref51"><sup>51</sup></a> EWISTARS,<a class="footnoteRef" href="#fn52" id="fnref52"><sup>52</sup></a> etc. Compressed sensing addresses the issue of high scan time by enabling faster acquisition by measuring fewer Fourier coefficients. This produces a high-quality image with relatively lower scan time. Another application (also discussed ahead) is for CT reconstruction with fewer X-ray projections. Compressed sensing, in this case, removes the high spatial gradient parts - mainly, image noise and artifacts. This holds tremendous potential as one can obtain high-resolution CT images at low radiation doses (through lower current-mA settings).<a class="footnoteRef" href="#fn53" id="fnref53"><sup>53</sup></a></p>
<h3 id="network-tomography">Network tomography</h3>

<p>Compressed sensing has showed outstanding results in the application of <a href="network_tomography" title="wikilink">network tomography</a> to <a href="network_management" title="wikilink">network management</a>. <a href="Network_delay" title="wikilink">Network delay</a> estimation and <a href="network_congestion" title="wikilink">network congestion</a> detection can both be modeled as underdetermined <a href="System_of_linear_equations" title="wikilink">systems of linear equations</a> where the coefficient matrix is the network routing matrix. Moreover, in the <a class="uri" href="Internet" title="wikilink">Internet</a>, network routing matrices usually satisfy the criterion for using compressed sensing.<a class="footnoteRef" href="#fn54" id="fnref54"><sup>54</sup></a></p>
<h3 id="shortwave-infrared-cameras">Shortwave-infrared cameras</h3>

<p>Commercial shortwave-infrared cameras based upon compressed sensing are available.<a class="footnoteRef" href="#fn55" id="fnref55"><sup>55</sup></a> These cameras have light sensitivity from 0.9 <a class="uri" href="µm" title="wikilink">µm</a> to 1.7 µm, which are wavelengths invisible to the human eye.</p>
<h3 id="aperture-synthesis-in-radio-astronomy">Aperture synthesis in radio astronomy</h3>

<p>In the field of <a href="radio_astronomy" title="wikilink">radio astronomy</a>, compressed sensing has been proposed for deconvolving an interferometric image.<a class="footnoteRef" href="#fn56" id="fnref56"><sup>56</sup></a> In fact, the <a href="CLEAN_(algorithm)" title="wikilink">Högbom CLEAN algorithm</a> that has been in use for the deconvolution of radio images since 1974, is similar to compressed sensing's matching pursuit algorithm.</p>
<h2 id="notes">Notes</h2>
<h2 id="see-also">See also</h2>
<ul>
<li><a class="uri" href="Noiselet" title="wikilink">Noiselet</a></li>
<li><a href="Sparse_approximation" title="wikilink">Sparse approximation</a></li>
<li><a href="Sparse_coding" title="wikilink">Sparse coding</a></li>
<li><a href="Low-density_parity-check_code" title="wikilink">Low-density parity-check code</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="further-reading">Further reading</h2>
<ul>
<li>"The Fundamentals of Compressive Sensing" <a href="http://www.brainshark.com/brainshark/brainshark.net/portal/title.aspx?pid=zCdz10BfTRz0z0">Part 1</a>, <a href="http://www.brainshark.com/brainshark/brainshark.net/portal/title.aspx?pid=zCgzXgcEKz0z0">Part 2</a> and <a href="http://www.brainshark.com/brainshark/brainshark.net/portal/title.aspx?pid=zAvz9F41cz0z0">Part 3</a>: video tutorial by Mark Davenport, Georgia Tech. at <a href="http://www.brainshark.com/sps">SigView, the IEEE Signal Processing Society Tutorial Library</a>.</li>
<li><a href="http://www.wired.com/magazine/2010/02/ff_algorithm/all/1">Using Math to Turn Lo-Res Datasets Into Hi-Res Samples</a> Wired Magazine article</li>
<li><a href="http://dsp.rice.edu/cs">Compressive Sensing Resources</a> at <a href="Rice_University" title="wikilink">Rice University</a>.</li>
<li><a href="http://igorcarron.googlepages.com/cs">Compressed Sensing: The Big Picture</a></li>
<li><a href="http://igorcarron.googlepages.com/compressedsensinghardware">A list of different hardware implementation of Compressive Sensing</a></li>
<li><a href="http://compressedsensing.googlepages.com/home">Compressed Sensing 2.0</a></li>
<li><a href="http://www.ams.org/happening-series/hap7-pixel.pdf">Compressed Sensing Makes Every Pixel Count</a> – article in the AMS <em>What's Happening in the Mathematical Sciences</em> series</li>
<li><a href="http://nuit-blanche.blogspot.com/search/label/CS">Nuit Blanche</a> A blog on Compressive Sensing featuring the most recent information on the subject (preprints, presentations, Q/As)</li>
<li><a href="http://igorcarron.googlepages.com/csvideos">Online Talks focused on Compressive Sensing</a></li>
<li><a href="http://ugcs.caltech.edu/~srbecker/wiki/Main_Page">Wiki on sparse reconstruction</a></li>
<li><a href="http://stemblab.github.io/intuitive-cs/">Intuitive Compressive Sensing</a></li>
</ul>

<p>"</p>

<p><a href="Category:Information_theory" title="wikilink">Category:Information theory</a> <a href="Category:Signal_processing" title="wikilink">Category:Signal processing</a> <a href="Category:Linear_algebra" title="wikilink">Category:Linear algebra</a> <a href="Category:Regression_analysis" title="wikilink">Category:Regression analysis</a> <a href="Category:Mathematical_optimization" title="wikilink">Category:Mathematical optimization</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="http://nuit-blanche.blogspot.com/2009/09/cs.html">CS: Compressed Genotyping, DNA Sudoku - Harnessing high throughput sequencing for multiplexed specimen analysis</a><a href="#fnref1">↩</a></li>
<li id="fn2">For most large underdetermined systems of linear equations the minimal 𝓁1-norm solution is also the sparsest solution; See Donoho, David L, Communications on pure and applied mathematics, 59, 797 (2006) <a href="#fnref2">↩</a></li>
<li id="fn3"><a href="http://www.brainshark.com/brainshark/brainshark.net/portal/title.aspx?pid=zCdz10BfTRz0z0">M. Davenport, "The Fundamentals of Compressive Sensing", SigView, April 12, 2013.</a><a href="#fnref3">↩</a></li>
<li id="fn4">Sparse MRI: The application of compressed sensing for rapid MR imaging; See Lustig, Michael and Donoho, David and Pauly, John M, Magnetic resonance in medicine, 58(6), 1182-1195 (2007) <a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="http://www.theengineer.co.uk/news/news-analysis/picture-of-health/1001485.article">Compressive sampling makes medical imaging safer</a><a href="#fnref6">↩</a></li>
<li id="fn7">Novel Sampling Strategies for Sparse MR Image Reconstruction; See Wang, Q.; Zenge M., Cetingul H.E., Mueller E., Nadar M.S., International Symposium of Magnetic Resonance in Medicine (2014) <a class="uri" href="http://www.healthcare.siemens.com/siemens_hwem-hwem_ssxa_websites-context-root/wcm/idc/siemens_hwem-hwem_ssxa_websites-context-root/wcm/idc/groups/public/@global/@imaging/@mri/documents/download/MRI-ISMRM-2014-ABSTRACT-WANG-2-01412769.pdf">http://www.healthcare.siemens.com/siemens_hwem-hwem_ssxa_websites-context-root/wcm/idc/siemens_hwem-hwem_ssxa_websites-context-root/wcm/idc/groups/public/@global/@imaging/@mri/documents/download/MRI-ISMRM-2014-ABSTRACT-WANG-2-01412769.pdf</a><a href="#fnref7">↩</a></li>
<li id="fn8"><a class="uri" href="http://www.lablanche-and-co.com">http://www.lablanche-and-co.com</a><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"><a href="http://2.bp.blogspot.com/_0ZCyAOBrUtA/TTwqLEeLvdI/AAAAAAAAEXI/7S0_SnWoC0E/s1600/l1-minimization.JPG">List of L1 regularization ideas</a> from Vivek Goyal, Alyson Fletcher, Sundeep Rangan, <a href="http://www.math.uiuc.edu/%7Elaugesen/imaha10/goyal_talk.pdf">The Optimistic Bayesian: Replica Method Analysis of Compressed Sensing</a><a href="#fnref11">↩</a></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14">"Atomic decomposition by basis pursuit", by Scott Shaobing Chen, David L. Donoho, Michael, A. Saunders. SIAM Journal on Scientific Computing<a href="#fnref14">↩</a></li>
<li id="fn15"><a href="#fnref15">↩</a></li>
<li id="fn16">Candès, E.J., &amp; Wakin, M.B., <em>An Introduction To Compressive Sampling</em>, IEEE Signal Processing Magazine, V.21, March 2008 [<a class="uri" href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp">http://ieeexplore.ieee.org/stamp/stamp.jsp?tp</a>=&amp;arnumber;=4472240&amp;isnumber;=4472102]<a href="#fnref16">↩</a></li>
<li id="fn17"><a href="http://www.acm.caltech.edu/l1magic/">L1-MAGIC is a collection of MATLAB routines</a><a href="#fnref17">↩</a></li>
<li id="fn18"></li>
<li id="fn19"></li>
<li id="fn20"><a href="#fnref20">↩</a></li>
<li id="fn21">Lange, K.: Optimization, Springer Texts in Statistics. Springer, New York (2004)<a href="#fnref21">↩</a></li>
<li id="fn22"><a href="#fnref22">↩</a></li>
<li id="fn23"><a href="#fnref23">↩</a></li>
<li id="fn24"><a href="#fnref24">↩</a></li>
<li id="fn25"></li>
<li id="fn26"><a class="uri" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6588871">http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6588871</a><a href="#fnref26">↩</a></li>
<li id="fn27"><a href="#fnref27">↩</a></li>
<li id="fn28"></li>
<li id="fn29"><a href="#fnref29">↩</a></li>
<li id="fn30"><a href="#fnref30">↩</a></li>
<li id="fn31">Compressive Sensing Hardware, <a class="uri" href="http://sites.google.com/site/igorcarron2/compressedsensinghardware">http://sites.google.com/site/igorcarron2/compressedsensinghardware</a><a href="#fnref31">↩</a></li>
<li id="fn32">Taylor, H.L., Banks, S.C., McCoy, J.F. "Deconvolution with the 1 norm. <em>Geophysics</em> 44(1), 39–52 (1979)<a href="#fnref32">↩</a></li>
<li id="fn33">Tibshirani, R. "Regression shrinkage and selection via the lasso. <em>J. R. Stat. Soc. B</em> 58(1), 267–288 (1996)<a href="#fnref33">↩</a></li>
<li id="fn34"></li>
<li id="fn35"><a href="#fnref35">↩</a></li>
<li id="fn36"><a href="#fnref36">↩</a></li>
<li id="fn37"><a href="#fnref37">↩</a></li>
<li id="fn38"><a href="#fnref38">↩</a></li>
<li id="fn39"><a href="#fnref39">↩</a></li>
<li id="fn40"><a href="#fnref40">↩</a></li>
<li id="fn41"><a href="#fnref41">↩</a></li>
<li id="fn42"><a href="#fnref42">↩</a></li>
<li id="fn43"><a href="#fnref43">↩</a></li>
<li id="fn44"><a href="#fnref44">↩</a></li>
<li id="fn45"><a href="http://www.wired.com/science/discoveries/news/2008/03/new_face_recognition">Engineers Test Highly Accurate Face Recognition</a><a href="#fnref45">↩</a></li>
<li id="fn46"></li>
<li id="fn47"></li>
<li id="fn48"><a href="#fnref48">↩</a></li>
<li id="fn49"><a href="http://nuit-blanche.blogspot.com/2010/03/why-compressed-sensing-is-not-csi.html">Why Compressed Sensing is NOT a CSI "Enhance" technology ... yet !</a><a href="#fnref49">↩</a></li>
<li id="fn50"><a href="http://nuit-blanche.blogspot.com/2010/03/surely-you-must-be-joking-mr.html">Surely You Must Be Joking Mr. Screenwriter</a><a href="#fnref50">↩</a></li>
<li id="fn51"><a href="#fnref51">↩</a></li>
<li id="fn52"><a href="#fnref52">↩</a></li>
<li id="fn53"><a href="#fnref53">↩</a></li>
<li id="fn54">[Network tomography via compressed sensing|<a class="uri" href="http://www.ee.washington.edu/research/funlab/Publications/2010/CS-Tomo.pdf">http://www.ee.washington.edu/research/funlab/Publications/2010/CS-Tomo.pdf</a>]<a href="#fnref54">↩</a></li>
<li id="fn55"><a href="#fnref55">↩</a></li>
<li id="fn56"><a href="http://mnras.oxfordjournals.org/content/395/3/1733|Compressed">sensing imaging techniques for radio interferometry</a><a href="#fnref56">↩</a></li>
</ol>
</section>
</body>
</html>
