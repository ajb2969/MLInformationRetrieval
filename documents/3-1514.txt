


Prediction interval




Prediction interval

In statistical inference, specifically predictive inference, a prediction interval is an estimate of an interval in which future observations will fall, with a certain probability, given what has already been observed. Prediction intervals are often used in regression analysis.
Prediction intervals are used in both frequentist statistics and Bayesian statistics: a prediction interval bears the same relationship to a future observation that a frequentist confidence interval or Bayesian credible interval bears to an unobservable population parameter: prediction intervals predict the distribution of individual future points, whereas confidence intervals and credible intervals of parameters predict the distribution of estimates of the true population mean or other quantity of interest that cannot be observed. Prediction intervals are also present in forecasts. It is difficult to estimate the prediction intervals of forecasts that have contrary series.1
Introduction
For example, if one makes the parametric assumption that the underlying distribution is a normal distribution, and has a sample set {X1, ..., Xn}, then confidence intervals and credible intervals may be used to estimate the population mean μ and population standard deviation σ of the underlying population, while prediction intervals may be used to estimate the value of the next sample variable, Xn+1.
Alternatively, in Bayesian terms, a prediction interval can be described as a credible interval for the variable itself, rather than for a parameter of the distribution thereof.
The concept of prediction intervals need not be restricted to inference about a single future sample value but can be extended to more complicated cases. For example, in the context of river flooding where analyses are often based on annual values of the largest flow within the year, there may be interest in making inferences about the largest flood likely to be experienced within the next 50 years.
Since prediction intervals are only concerned with past and future observations, rather than unobservable population parameters, they are advocated as a better method than confidence intervals by some statisticians, such as Seymour Geisser, following the focus on observables by Bruno de Finetti.
Normal distribution
Given a sample from a normal distribution, whose parameters are unknown, it is possible to give prediction intervals in the frequentist sense, i.e., an interval [a, b] based on statistics of the sample such that on repeated experiments, Xn+1 falls in the interval the desired percentage of the time; one may call these "predictive confidence intervals".2
A general technique of frequentist prediction intervals is to find and compute a pivotal quantity of the observables X1, ..., Xn, Xn+1 – meaning a function of observables and parameters whose probability distribution does not depend on the parameters – that can be inverted to give a probability of the future observation Xn+1 falling in some interval computed in terms of the observed values so far, 
 
 
 
  Such a pivotal quantity, depending only on observables, is called an ancillary statistic.3 The usual method of constructing pivotal quantities is to take the difference of two variables that depend on location, so that location cancels out, and then take the ratio of two variables that depend on scale, so that scale cancels out. The most familiar pivotal quantity is the Student's t-statistic, which can be derived by this method and is used in the sequel.
Known mean, known variance
A prediction interval [l,u] for a future observation X in a normal distribution N(µ,σ2) with known mean and variance may easily be calculated from


 
 , the standard score of X, is standard normal distributed.
Hence


 
  or



with z the quantile in the standard normal distribution for which:







Prediction
  interval

z





75%

0.674



90%

1.645



95%

1.966



99%

2.587



(Figure)
Prediction interval (on the y-axis) given from z (the quantile of the standard score, on the x-axis). The y-axis is logarithmically compressed (but the values on it are not modified).

The prediction interval is conventionally written as:



For example, to calculate the 95% prediction interval for a normal distribution with a mean (µ) of 5 and a standard deviation (σ) of 1, then z is approximately 2. Therefore, the lower limit of the prediction interval is approximately 5 ‒ (2·1) = 3, and the upper limit is approximately 5 + (2·1) = 7, thus giving a prediction interval of approximately 3 to 7.
(Figure)
Diagram showing the cumulative distribution function for the normal distribution with mean (µ) 0 and variance (σ2) 1. In addition to the quantile function, the prediction interval for any standard score can be calculated by (1 − (1 − Φµ,σ2(standard score))·2). For example, a standard score of x = 1.96 gives Φµ,σ2(1.96) = 0.9750 corresponding to a prediction interval of (1 − (1 − 0.9750)·2) = 0.9500 = 95%.

Estimation of parameters
For a distribution with unknown parameters, a direct approach to prediction is to estimate the parameters and then use the associated quantile function – for example, one could use the sample mean 
 
 
 
  as estimate for μ and the sample variance s2 as an estimate for σ2. Note that there are two natural choices for s2 here – dividing by 
 
 
 
  yields an unbiased estimate, while dividing by n yields the maximum likelihood estimator, and either might be used. One then uses the quantile function with these estimated parameters 
 
 
 
  to give a prediction interval.
This approach is usable, but the resulting interval will not have the repeated sampling interpretation8 – it is not a predictive confidence interval.
For the sequel, use the sample mean:


 
  and the (unbiased) sample variance:



Unknown mean, known variance
Given9 a normal distribution with unknown mean μ but known variance 1, the sample mean 
 
 
 
  of the observations 
 
 
 
  has distribution 
 
 
 
 
  while the future observation 
 
 
 
  has distribution 
 
 
 
  Taking the difference of these cancels the μ and yields a normal distribution of variance 
 
 
 
  thus


 
  Solving for 
 
 
 
 
  gives the prediction distribution 
 
 
 
  from which one can compute intervals as before. This is a predictive confidence interval in the sense that if one uses a quantile range of 100p%, then on repeated applications of this computation, the future observation 
 
 
 
  will fall in the predicted interval 100p% of the time.
Notice that this prediction distribution is more conservative than using the estimated mean 
 
 
 
  and known variance 1, as this uses variance 
 
 
 
 , hence yields wider intervals. This is necessary for the desired confidence interval property to hold.
Known mean, unknown variance
Conversely, given a normal distribution with known mean 0 but unknown variance 
 
 
 
 
 , the sample variance 
 
 
 
  of the observations 
 
 
 
  has, up to scale, a 

 
  distribution; more precisely:


 
  while the future observation 
 
 
 
  has distribution 
 
 
 
  Taking the ratio of the future observation and the sample standard deviation cancels the σ, yielding a Student's t-distribution with n–1 degrees of freedom:


 
  Solving for 
 
 
 
  gives the prediction distribution 
 
 
 
  from which one can compute intervals as before.
Notice that this prediction distribution is more conservative than using a normal distribution with the estimated standard deviation 
 
 
 
  and known mean 0, as it uses the t-distribution instead of the normal distribution, hence yields wider intervals. This is necessary for the desired confidence interval property to hold.
Unknown mean, unknown variance
Combining the above for a normal distribution 
 
 
 
  with both μ and σ2 unknown yields the following ancillary statistic:10


 
  This simple combination is possible because the sample mean and sample variance of the normal distribution are independent statistics; this is only true for the normal distribution, and in fact characterizes the normal distribution.
Solving for 
 
 
 
  yields the prediction distribution



The probability of 
 
 
 
  falling in a given interval is then:



where Ta is the 100((1 + p)/2)th percentile of Student's t-distribution with n − 1 degrees of freedom. Therefore the numbers



are the endpoints of a 100p% prediction interval for 
 
 
 
 .
Non-parametric methods
One can compute prediction intervals without any assumptions on the population; formally, this is a non-parametric method.11
Suppose one randomly draws a sample of two observations X1 and X2 from a population in which values are assumed to have a continuous probability distribution

What is the probability that X2 > X1?
 

The answer is exactly 50%, regardless of the underlying population – the probability of picking 3 and then 7 is the same as picking 7 and then 3, regardless of the particular probability of picking 3 or 7. Thus, if one picks a single sample X1, then 50% of the time the next sample will be greater, which yields (X1, +∞) as a 50% prediction interval for X2. Similarly, 50% of the time it will be smaller, which yields another 50% prediction interval for X2, namely (−∞, X1). Note that the assumption of a continuous distribution avoids the possibililty that values might be exactly equal; this would complicate matters.
Similarly, if one has a sample {X1, ..., Xn} then the probability that the next observation Xn+1 will be the largest is 1/(n + 1), since all observations have equal probability of being the maximum. In the same way, the probability that Xn+1 will be the smallest is 1/(n + 1). The other (n − 1)/(n + 1) of the time, Xn+1 falls between the sample maximum and sample minimum of the sample {X1, ..., Xn}. Thus, denoting the sample maximum and minimum by M and m, this yields an (n − 1)/(n + 1) prediction interval of [m, M].
For example, if n = 19, then [m, M] gives an 18/20 = 90% prediction interval – 90% of the time, the 20th observation falls between the smallest and largest observation seen heretofore. Likewise, n = 39 gives a 95% prediction interval, and n = 199 gives a 99% prediction interval.
More generally, if X(j) and X(k) are order statistics of the sample with j (j), X(k)] is a prediction interval for Xn+1 with coverage probability (significance level) equal to (n + 1 - 2j) / (n + 1).
One can visualize this by drawing the n samples on a line, which divides the line into n + 1 sections (n − 1 segments between samples, and 2 intervals going to infinity at both ends), and noting that Xn+1 has an equal chance of landing in any of these n + 1 sections. Thus one can also pick any k of these sections and give a k/(n + 1) prediction interval (or set, if the sections are not consecutive). For instance, if n = 2, then the probability that X3 will land between the existing 2 observations is 1/3.
Notice that while this gives the probability that a future observation will fall in a range, it does not give any estimate as to where in a segment it will fall – notably, if it falls outside the range of observed values, it may be far outside the range. See extreme value theory for further discussion. Formally, this applies not just to sampling from a population, but to any exchangeable sequence of random variables, not necessarily independent or identically distributed.
Contrast with other intervals
Contrast with confidence intervals
Note that in the formula for the predictive confidence interval no mention is made of the unobservable parameters μ and σ of population mean and standard deviation – the observed sample statistics 
 
 
 
  and 
 
 
 
  of sample mean and standard deviation are used, and what is estimated is the outcome of future samples.
Rather than using sample statistics as estimators of population parameters and applying confidence intervals to these estimates, one considers "the next sample" 
 
 
 
  as itself a statistic, and computes its sampling distribution.
In parameter confidence intervals, one estimates population parameters; if one wishes to interpret this as prediction of the next sample, one models "the next sample" as a draw from this estimated population, using the (estimated) population distribution. By contrast, in predictive confidence intervals, one uses the sampling distribution of (a statistic of) n or n+1 samples from such a population, and the population distribution is not directly used, though the assumption about its form (though not the values of its parameters) is used in computing the sampling distribution.
Contrast with tolerance intervals
Applications
Prediction intervals are commonly used as definitions of reference ranges, such as reference ranges for blood tests to give an idea of whether a blood test is normal or not. For this purpose, the most commonly used prediction interval is the 95% prediction interval, and a reference range based on it can be called a standard reference range.
Regression analysis
A common application of prediction intervals is to regression analysis.
Suppose the data is being modeled by a straight line regression:


 
  where 
 
 
 
  is the response variable, 
 
 
 
  is the explanatory variable, εi is a random error term, and 
 
 
 
 
  and 
 
 
 
  are parameters.
Given estimates 
 
 
 
  and 
 
 
 
  for the parameters, such as from a simple linear regression, the predicted response value yd for a given explanatory value xd is


 
  (the point on the regression line), while the actual response would be


 
 
  The point estimate

 
  is called the mean response, and is an estimate of the expected value of yd,


A prediction interval instead gives an interval in which one expects yd to fall; this is not necessary if the actual parameters α and β are known (together with the error term εi), but if one is estimating from a sample, then one may use the standard error of the estimates for the intercept and slope ($\hat\alpha$ and $\hat\beta$), as well as their correlation, to compute a prediction interval.
In regression, Faraway (2002, p. 39) makes a distinction between intervals for predictions of the mean response vs. for predictions of observed response—affecting essentially the inclusion or not of the unity term within the square root in the expansion factors above; see 12 for details.
Bayesian statistics
Seymour Geisser, a proponent of predictive inference, gives predictive applications of Bayesian statistics.13
In Bayesian statistics, one can compute (Bayesian) prediction intervals from the posterior probability of the random variable, as a credible interval. In theoretical work, credible intervals are not often calculated for the prediction of future events, but for inference of parameters – i.e., credible intervals of a parameter, not for the outcomes of the variable itself. However, particularly where applications are concerned with possible extreme values of yet to be observed cases, credible intervals for such values can be of practical importance.
See also

Extrapolation
Posterior probability
Prediction
Prediction band
Regression analysis
Seymour Geisser
Trend estimation

Further reading

"Confidence intervals for predictions", chap. 3.5 in 14




ISO 16269-8 Standard Interpretation of Data, Part 8, Determination of Prediction Intervals

References
"
Category:Statistical terminology Category:Statistical inference Category:Regression analysis Category:Statistical intervals



↩
↩
↩
Page 472 (Table A2) in: ↩



↩
↩
↩
Prediction Intervals, Statistics @ SUNY Oswego↩
J. J. Faraway, "Practical Regression and Anova using R"↩
↩





