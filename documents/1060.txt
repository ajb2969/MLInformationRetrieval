   Eigenvalue perturbation      Eigenvalue perturbation   In mathematics, an eigenvalue perturbation problem is that of finding the eigenvectors and eigenvalues of a system that is perturbed from one with known eigenvectors and eigenvalues. This is useful for studying how sensitive the original system's eigenvectors and eigenvalues are to changes in the system. This type of analysis popularized by Lord Rayleigh , in his investigation of harmonic vibrations of a string perturbed by small inhomogeneities. 1  The derivations in this article are essentially self-contained and can be found in many texts on numerical linear algebra 2 or numerical functional analysis.  Example  Suppose we have solutions to the generalized eigenvalue problem ,       𝐊  0    𝐱   0  i    =   λ   0  i     𝐌  0    𝐱   0  i    .   (  0  )      fragments   subscript  𝐊  0    subscript  𝐱    0  i      subscript  λ    0  i     subscript  𝐌  0    subscript  𝐱    0  i    normal-.  italic-   fragments  normal-(  0  normal-)     \mathbf{K}_{0}\mathbf{x}_{0i}=\lambda_{0i}\mathbf{M}_{0}\mathbf{x}_{0i}.\qquad%
 (0)     where    𝐊  0     subscript  𝐊  0    \mathbf{K}_{0}   and    𝐌  0     subscript  𝐌  0    \mathbf{M}_{0}   are matrices. That is, we know the eigenvalues and eigenvectors for    i  =   1  ,  …  ,  N       i   1  normal-…  N     i=1,...,N   . Now suppose we want to change the matrices by a small amount. That is, we want to find the eigenvalues and eigenvectors of       𝐊𝐱  i   =     λ  i    𝐌𝐱  i     (  1  )         subscript  𝐊𝐱  i       subscript  λ  i    subscript  𝐌𝐱  i    1     \mathbf{K}\mathbf{x}_{i}=\lambda_{i}\mathbf{M}\mathbf{x}_{i}\qquad(1)   where     𝐊   𝐊   \displaystyle\mathbf{K}     with the perturbations    δ  𝐊      δ  𝐊    \delta\mathbf{K}   and    δ  𝐌      δ  𝐌    \delta\mathbf{M}   much smaller than   𝐊   𝐊   \mathbf{K}   and   𝐌   𝐌   \mathbf{M}   respectively. Then we expect the new eigenvalues and eigenvectors to be similar to the original, plus small perturbations:      λ  i     subscript  λ  i    \displaystyle\lambda_{i}     Steps  We assume that the matrices are symmetric and positive definite , and assume we have scaled the eigenvectors such that        𝐱   0  j   ⊤    𝐌  0    𝐱   0  i     =    δ   i  j     (  2  )           superscript   subscript  𝐱    0  j    top    subscript  𝐌  0    subscript  𝐱    0  i       subscript  δ    i  j    2     \mathbf{x}_{0j}^{\top}\mathbf{M}_{0}\mathbf{x}_{0i}=\delta_{ij}\qquad(2)     where is the Kronecker delta . Now we want to solve the equation        𝐊𝐱  i   =    λ  i    𝐌𝐱  i     .       subscript  𝐊𝐱  i      subscript  λ  i    subscript  𝐌𝐱  i      \mathbf{K}\mathbf{x}_{i}=\lambda_{i}\mathbf{M}\mathbf{x}_{i}.     Substituting, we get         (    𝐊  0   +   δ  𝐊    )    (    𝐱   0  i    +   δ   𝐱  i     )    =    (    λ   0  i    +   δ   λ  i     )    (    𝐌  0   +   δ  𝐌    )    (    𝐱   0  i    +   δ   𝐱  i     )     ,           subscript  𝐊  0     δ  𝐊       subscript  𝐱    0  i      δ   subscript  𝐱  i           subscript  λ    0  i      δ   subscript  λ  i        subscript  𝐌  0     δ  𝐌       subscript  𝐱    0  i      δ   subscript  𝐱  i        (\mathbf{K}_{0}+\delta\mathbf{K})(\mathbf{x}_{0i}+\delta\mathbf{x}_{i})=\left(%
 \lambda_{0i}+\delta\lambda_{i}\right)\left(\mathbf{M}_{0}+\delta\mathbf{M}%
 \right)\left(\mathbf{x}_{0i}+\delta\mathbf{x}_{i}\right),     which expands to       𝐊  0    𝐱   0  i         subscript  𝐊  0    subscript  𝐱    0  i      \displaystyle\mathbf{K}_{0}\mathbf{x}_{0i}     Canceling from (1) leaves         δ   𝐊𝐱   0  i     +    𝐊  0   δ   𝐱  i    +   δ  𝐊  δ   𝐱  i     =     λ   0  i     𝐌  0   δ   𝐱  i    +    λ   0  i    δ   𝐌𝐱   0  i     +   δ   λ  i    𝐌  0    𝐱   0  i     +    λ   0  i    δ  𝐌  δ   𝐱  i    +   δ   λ  i   δ   𝐌𝐱   0  i     +   δ   λ  i    𝐌  0   δ   𝐱  i    +   δ   λ  i   δ  𝐌  δ   𝐱  i      .          δ   subscript  𝐊𝐱    0  i        subscript  𝐊  0   δ   subscript  𝐱  i      δ  𝐊  δ   subscript  𝐱  i          subscript  λ    0  i     subscript  𝐌  0   δ   subscript  𝐱  i       subscript  λ    0  i    δ   subscript  𝐌𝐱    0  i       δ   subscript  λ  i    subscript  𝐌  0    subscript  𝐱    0  i        subscript  λ    0  i    δ  𝐌  δ   subscript  𝐱  i      δ   subscript  λ  i   δ   subscript  𝐌𝐱    0  i       δ   subscript  λ  i    subscript  𝐌  0   δ   subscript  𝐱  i      δ   subscript  λ  i   δ  𝐌  δ   subscript  𝐱  i       \displaystyle\delta\mathbf{K}\mathbf{x}_{0i}+\mathbf{K}_{0}\delta\mathbf{x}_{i%
 }+\delta\mathbf{K}\delta\mathbf{x}_{i}=\lambda_{0i}\mathbf{M}_{0}\delta\mathbf%
 {x}_{i}+\lambda_{0i}\delta\mathbf{M}\mathbf{x}_{0i}+\delta\lambda_{i}\mathbf{M%
 }_{0}\mathbf{x}_{0i}+\lambda_{0i}\delta\mathbf{M}\delta\mathbf{x}_{i}+\delta%
 \lambda_{i}\delta\mathbf{M}\mathbf{x}_{0i}+\delta\lambda_{i}\mathbf{M}_{0}%
 \delta\mathbf{x}_{i}+\delta\lambda_{i}\delta\mathbf{M}\delta\mathbf{x}_{i}.     Removing the higher-order terms, this simplifies to       𝐊  0   δ   𝐱  i   +  δ   𝐊𝐱   0  i    =   λ   0  i     𝐌  0   δ   𝐱  i   +   λ   0  i    δ  𝐌   x   0  i    +  δ   λ  i    𝐌  0    𝐱   0  i    .   (  3  )      fragments   subscript  𝐊  0   δ   subscript  𝐱  i    δ   subscript  𝐊𝐱    0  i      subscript  λ    0  i     subscript  𝐌  0   δ   subscript  𝐱  i     subscript  λ    0  i    δ  M   subscript  normal-x    0  i     δ   subscript  λ  i    subscript  𝐌  0    subscript  𝐱    0  i    normal-.  italic-   fragments  normal-(  3  normal-)     \mathbf{K}_{0}\delta\mathbf{x}_{i}+\delta\mathbf{K}\mathbf{x}_{0i}=\lambda_{0i%
 }\mathbf{M}_{0}\delta\mathbf{x}_{i}+\lambda_{0i}\delta\mathbf{M}\mathrm{x}_{0i%
 }+\delta\lambda_{i}\mathbf{M}_{0}\mathbf{x}_{0i}.\qquad(3)     When the matrix is symmetric, the unperturbed eigenvectors are orthogonal and so we use them as a basis for the perturbed eigenvectors. That is, we want to construct       δ   𝐱  i    =     ∑   j  =  1   N     ε   i  j     𝐱   0  j       (  4  )          δ   subscript  𝐱  i       superscript   subscript     j  1    N      subscript  ε    i  j     subscript  𝐱    0  j      4     \delta\mathbf{x}_{i}=\sum_{j=1}^{N}\varepsilon_{ij}\mathbf{x}_{0j}\qquad(4)     where the are small constants that are to be determined. Substituting (4) into (3) and rearranging gives        𝐊  0      ∑   j  =  1   N      ε   i  j     𝐱   0  j       +   δ   𝐊𝐱   0  i            subscript  𝐊  0     superscript   subscript     j  1    N      subscript  ε    i  j     subscript  𝐱    0  j         δ   subscript  𝐊𝐱    0  i       \displaystyle\mathbf{K}_{0}\sum_{j=1}^{N}\varepsilon_{ij}\mathbf{x}_{0j}+%
 \delta\mathbf{K}\mathbf{x}_{0i}     Because the eigenvectors are -orthogonal when is positive definite, we can remove the summations by left multiplying by    𝐱   0  i   ⊤     superscript   subscript  𝐱    0  i    top    \mathbf{x}_{0i}^{\top}   :          𝐱   0  i   ⊤    ε   i  i     λ   0  i     𝐌  0    𝐱   0  i     +    𝐱   0  i   ⊤   δ   𝐊𝐱   0  i      =     λ   0  i     𝐱   0  i   ⊤    𝐌  0    ε   i  i     𝐱   0  i     +    λ   0  i     𝐱   0  i   ⊤   δ   𝐌𝐱   0  i     +   δ   λ  i    𝐱   0  i   ⊤    𝐌  0    𝐱   0  i       .           superscript   subscript  𝐱    0  i    top    subscript  ε    i  i     subscript  λ    0  i     subscript  𝐌  0    subscript  𝐱    0  i        superscript   subscript  𝐱    0  i    top   δ   subscript  𝐊𝐱    0  i           subscript  λ    0  i     superscript   subscript  𝐱    0  i    top    subscript  𝐌  0    subscript  ε    i  i     subscript  𝐱    0  i        subscript  λ    0  i     superscript   subscript  𝐱    0  i    top   δ   subscript  𝐌𝐱    0  i       δ   subscript  λ  i    superscript   subscript  𝐱    0  i    top    subscript  𝐌  0    subscript  𝐱    0  i        \mathbf{x}_{0i}^{\top}\varepsilon_{ii}\lambda_{0i}\mathbf{M}_{0}\mathbf{x}_{0i%
 }+\mathbf{x}_{0i}^{\top}\delta\mathbf{K}\mathbf{x}_{0i}=\lambda_{0i}\mathbf{x}%
 _{0i}^{\top}\mathbf{M}_{0}\varepsilon_{ii}\mathbf{x}_{0i}+\lambda_{0i}\mathbf{%
 x}_{0i}^{\top}\delta\mathbf{M}\mathbf{x}_{0i}+\delta\lambda_{i}\mathbf{x}_{0i}%
 ^{\top}\mathbf{M}_{0}\mathbf{x}_{0i}.     By use of equation (1) again:       𝐱   0  i   ⊤    𝐊  0    ε   i  i     𝐱   0  i    +   𝐱   0  i   ⊤   δ   𝐊𝐱   0  i    =   λ   0  i     𝐱   0  i   ⊤    𝐌  0    ε   i  i     𝐱   0  i    +   λ   0  i     𝐱   0  i   ⊤   δ   𝐌𝐱   0  i    +  δ   λ  i    𝐱   0  i   ⊤    𝐌  0    𝐱   0  i    .   (  6  )      fragments   superscript   subscript  𝐱    0  i    top    subscript  𝐊  0    subscript  ε    i  i     subscript  𝐱    0  i      superscript   subscript  𝐱    0  i    top   δ   subscript  𝐊𝐱    0  i      subscript  λ    0  i     superscript   subscript  𝐱    0  i    top    subscript  𝐌  0    subscript  ε    i  i     subscript  𝐱    0  i      subscript  λ    0  i     superscript   subscript  𝐱    0  i    top   δ   subscript  𝐌𝐱    0  i     δ   subscript  λ  i    superscript   subscript  𝐱    0  i    top    subscript  𝐌  0    subscript  𝐱    0  i    normal-.  italic-   fragments  normal-(  6  normal-)     \mathbf{x}_{0i}^{\top}\mathbf{K}_{0}\varepsilon_{ii}\mathbf{x}_{0i}+\mathbf{x}%
 _{0i}^{\top}\delta\mathbf{K}\mathbf{x}_{0i}=\lambda_{0i}\mathbf{x}_{0i}^{\top}%
 \mathbf{M}_{0}\varepsilon_{ii}\mathbf{x}_{0i}+\lambda_{0i}\mathbf{x}_{0i}^{%
 \top}\delta\mathbf{M}\mathbf{x}_{0i}+\delta\lambda_{i}\mathbf{x}_{0i}^{\top}%
 \mathbf{M}_{0}\mathbf{x}_{0i}.\qquad(6)     The two terms containing are equal because left-multiplying (1) by    𝐱   0  i   ⊤     superscript   subscript  𝐱    0  i    top    \mathbf{x}_{0i}^{\top}   gives         𝐱   0  i   ⊤    𝐊  0    𝐱   0  i     =    λ   0  i     𝐱   0  i   ⊤    𝐌  0    𝐱   0  i      .         superscript   subscript  𝐱    0  i    top    subscript  𝐊  0    subscript  𝐱    0  i        subscript  λ    0  i     superscript   subscript  𝐱    0  i    top    subscript  𝐌  0    subscript  𝐱    0  i       \mathbf{x}_{0i}^{\top}\mathbf{K}_{0}\mathbf{x}_{0i}=\lambda_{0i}\mathbf{x}_{0i%
 }^{\top}\mathbf{M}_{0}\mathbf{x}_{0i}.     Canceling those terms in (6) leaves         𝐱   0  i   ⊤   δ   𝐊𝐱   0  i     =     λ   0  i     𝐱   0  i   ⊤   δ   𝐌𝐱   0  i     +   δ   λ  i    𝐱   0  i   ⊤    𝐌  0    𝐱   0  i       .         superscript   subscript  𝐱    0  i    top   δ   subscript  𝐊𝐱    0  i          subscript  λ    0  i     superscript   subscript  𝐱    0  i    top   δ   subscript  𝐌𝐱    0  i       δ   subscript  λ  i    superscript   subscript  𝐱    0  i    top    subscript  𝐌  0    subscript  𝐱    0  i        \mathbf{x}_{0i}^{\top}\delta\mathbf{K}\mathbf{x}_{0i}=\lambda_{0i}\mathbf{x}_{%
 0i}^{\top}\delta\mathbf{M}\mathbf{x}_{0i}+\delta\lambda_{i}\mathbf{x}_{0i}^{%
 \top}\mathbf{M}_{0}\mathbf{x}_{0i}.     Rearranging gives       δ   λ  i    =     𝐱   0  i   ⊤    (    δ  𝐊   -    λ   0  i    δ  𝐌    )    𝐱   0  i       𝐱   0  i   ⊤    𝐌  0    𝐱   0  i            δ   subscript  λ  i         subscript   superscript  𝐱  top     0  i        δ  𝐊      subscript  λ    0  i    δ  𝐌     subscript  𝐱    0  i        superscript   subscript  𝐱    0  i    top    subscript  𝐌  0    subscript  𝐱    0  i        \delta\lambda_{i}=\frac{\mathbf{x}^{\top}_{0i}\left(\delta\mathbf{K}-\lambda_{%
 0i}\delta\mathbf{M}\right)\mathbf{x}_{0i}}{\mathbf{x}_{0i}^{\top}\mathbf{M}_{0%
 }\mathbf{x}_{0i}}     But by (2), this denominator is equal to 1. Thus        δ   λ  i    =    𝐱   0  i   ⊤    (    δ  𝐊   -    λ   0  i    δ  𝐌    )    𝐱   0  i      .        δ   subscript  λ  i       subscript   superscript  𝐱  top     0  i        δ  𝐊      subscript  λ    0  i    δ  𝐌     subscript  𝐱    0  i       \delta\lambda_{i}=\mathbf{x}^{\top}_{0i}\left(\delta\mathbf{K}-\lambda_{0i}%
 \delta\mathbf{M}\right)\mathbf{x}_{0i}.     Then, by left-multiplying equation (5) by :         ε   i  k    =     𝐱   0  k   ⊤    (    δ  𝐊   -    λ   0  i    δ  𝐌    )    𝐱   0  i       λ   0  i    -   λ   0  k       ,   i  ≠  k    .     formulae-sequence     subscript  ε    i  k         subscript   superscript  𝐱  top     0  k        δ  𝐊      subscript  λ    0  i    δ  𝐌     subscript  𝐱    0  i        subscript  λ    0  i     subscript  λ    0  k         i  k     \varepsilon_{ik}=\frac{\mathbf{x}^{\top}_{0k}\left(\delta\mathbf{K}-\lambda_{0%
 i}\delta\mathbf{M}\right)\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0k}},\qquad i%
 \neq k.     Or by changing the name of the indices:         ε   i  j    =     𝐱   0  j   ⊤    (    δ  𝐊   -    λ   0  i    δ  𝐌    )    𝐱   0  i       λ   0  i    -   λ   0  j       ,   i  ≠  j    .     formulae-sequence     subscript  ε    i  j         subscript   superscript  𝐱  top     0  j        δ  𝐊      subscript  λ    0  i    δ  𝐌     subscript  𝐱    0  i        subscript  λ    0  i     subscript  λ    0  j         i  j     \varepsilon_{ij}=\frac{\mathbf{x}^{\top}_{0j}\left(\delta\mathbf{K}-\lambda_{0%
 i}\delta\mathbf{M}\right)\mathbf{x}_{0i}}{\lambda_{0i}-\lambda_{0j}},\qquad i%
 \neq j.     To find , use the fact that:        𝐱  i  ⊤    𝐌𝐱  i    =  1         subscript   superscript  𝐱  top   i    subscript  𝐌𝐱  i    1    \mathbf{x}^{\top}_{i}\mathbf{M}\mathbf{x}_{i}=1     implies:        ε   i  i    =   -     1  2     𝐱   0  i   ⊤   δ   𝐌𝐱   0  i       .       subscript  ε    i  i          1  2    subscript   superscript  𝐱  top     0  i    δ   subscript  𝐌𝐱    0  i        \varepsilon_{ii}=-\tfrac{1}{2}\mathbf{x}^{\top}_{0i}\delta\mathbf{M}\mathbf{x}%
 _{0i}.     Summary      λ  i     subscript  λ  i    \displaystyle\lambda_{i}     for infinitesimal    δ  K      δ  K    δK   and    δ  M      δ  M    δM   (the high order terms in (3) being negligible)  Results  This means it is possible to efficiently do a sensitivity analysis on as a function of changes in the entries of the matrices. (Recall that the matrices are symmetric and so changing will also change , hence the term.)        ∂   λ  i     ∂   𝐊   (   k  ℓ   )             subscript  λ  i       subscript  𝐊    k  normal-ℓ       \displaystyle\frac{\partial\lambda_{i}}{\partial\mathbf{K}_{(k\ell)}}     Similarly        ∂   𝐱  i     ∂   𝐊   (   k  ℓ   )             subscript  𝐱  i       subscript  𝐊    k  normal-ℓ       \displaystyle\frac{\partial\mathbf{x}_{i}}{\partial\mathbf{K}_{(k\ell)}}     Existence of eigenvectors  Note that in the above example we assumed that both the unperturbed and the perturbed systems involved symmetric matrices , which guaranteed the existence of   N   N   N   linearly independent eigenvectors. An eigenvalue problem involving non-symmetric matrices is not guaranteed to have   N   N   N   linearly independent eigenvectors, though a sufficient condition is that   𝐊   𝐊   \mathbf{K}   and   𝐌   𝐌   \mathbf{M}   be simultaneously diagonalisable .  See also   Perturbation theory (quantum mechanics)  Bauer–Fike theorem   References       Further reading     "  Category:Perturbation theory  Category:Linear algebra  Category:Numerical linear algebra     ↩  ↩     