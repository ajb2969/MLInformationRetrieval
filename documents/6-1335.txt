   Delta method      Delta method   In statistics , the delta method is a result concerning the approximate probability distribution for a function of an asymptotically normal statistical estimator from knowledge of the limiting variance of that estimator.  Univariate delta method  While the delta method generalizes easily to a multivariate setting, careful motivation of the technique is more easily demonstrated in univariate terms. Roughly, if there is a sequence of random variables satisfying         n    [    X  n   -  θ   ]      →  𝐷     𝒩   (  0  ,   σ  2   )     ,      D  normal-→       n    delimited-[]     subscript  X  n   θ       𝒩   0   superscript  σ  2       {\sqrt{n}[X_{n}-\theta]\,\xrightarrow{D}\,\mathcal{N}(0,\sigma^{2})},     where θ and σ 2 are finite valued constants and    →  𝐷     D  normal-→    \xrightarrow{D}   denotes convergence in distribution , then        n    [    g   (   X  n   )    -   g   (  θ  )     ]      →  𝐷     𝒩   (  0  ,    σ  2     [    g  ′    (  θ  )    ]   2    )        D  normal-→       n    delimited-[]      g   subscript  X  n      g  θ        𝒩   0     superscript  σ  2    superscript   delimited-[]     superscript  g  normal-′   θ    2        {\sqrt{n}[g(X_{n})-g(\theta)]\,\xrightarrow{D}\,\mathcal{N}(0,\sigma^{2}[g^{%
 \prime}(\theta)]^{2})}     for any function g satisfying the property that    g  ′   (  θ  )       g  normal-′  θ    g′(θ)   exists, is non-zero valued, and is polynomially bounded with the random variable. 1  Proof in the univariate case  Demonstration of this result is fairly straightforward under the assumption that    g  ′   (  θ  )       g  normal-′  θ    g′(θ)   is continuous . To begin, we use the mean value theorem :        g   (   X  n   )    =    g   (  θ  )    +    g  ′    (   θ  ~   )    (    X  n   -  θ   )      ,        g   subscript  X  n        g  θ      superscript  g  normal-′    normal-~  θ      subscript  X  n   θ       g(X_{n})=g(\theta)+g^{\prime}(\tilde{\theta})(X_{n}-\theta),   where    θ  ~     normal-~  θ    \tilde{\theta}   lies between and θ . Note that since      X  n      →  𝑃    θ      P  normal-→    subscript  X  n   θ    X_{n}\,\xrightarrow{P}\,\theta   and     X  n   <   θ  ~   <  θ         subscript  X  n    normal-~  θ        θ     X_{n}<\tilde{\theta}<\theta   , it must be that      θ  ~      →  𝑃    θ      P  normal-→    normal-~  θ   θ    \tilde{\theta}\,\xrightarrow{P}\,\theta   and since    g  ′   (  θ  )       g  normal-′  θ    g′(θ)   is continuous, applying the continuous mapping theorem yields         g  ′    (   θ  ~   )      →  𝑃      g  ′    (  θ  )     ,      P  normal-→      superscript  g  normal-′    normal-~  θ       superscript  g  normal-′   θ     g^{\prime}(\tilde{\theta})\,\xrightarrow{P}\,g^{\prime}(\theta),   where    →  𝑃     P  normal-→    \xrightarrow{P}   denotes convergence in probability .  Rearranging the terms and multiplying by    n      n    \sqrt{n}   gives         n    [    g   (   X  n   )    -   g   (  θ  )     ]    =    g  ′    (   θ  ~   )    n    [    X  n   -  θ   ]     .          n    delimited-[]      g   subscript  X  n      g  θ         superscript  g  normal-′    normal-~  θ     n    delimited-[]     subscript  X  n   θ       \sqrt{n}[g(X_{n})-g(\theta)]=g^{\prime}\left(\tilde{\theta}\right)\sqrt{n}[X_{%
 n}-\theta].   Since        n    [    X  n   -  θ   ]     →  𝐷    𝒩   (  0  ,   σ  2   )        D  normal-→       n    delimited-[]     subscript  X  n   θ       𝒩   0   superscript  σ  2       {\sqrt{n}[X_{n}-\theta]\xrightarrow{D}\mathcal{N}(0,\sigma^{2})}   by assumption, it follows immediately from appeal to Slutsky's Theorem that         n    [    g   (   X  n   )    -   g   (  θ  )     ]     →  𝐷    𝒩   (  0  ,    σ  2     [    g  ′    (  θ  )    ]   2    )     .      D  normal-→       n    delimited-[]      g   subscript  X  n      g  θ        𝒩   0     superscript  σ  2    superscript   delimited-[]     superscript  g  normal-′   θ    2        {\sqrt{n}[g(X_{n})-g(\theta)]\xrightarrow{D}\mathcal{N}(0,\sigma^{2}[g^{\prime%
 }(\theta)]^{2})}.   This concludes the proof.  Proof with an explicit order of approximation  Alternatively, one can add one more step at the end, to obtain the order of approximation :       n    [    g   (   X  n   )    -   g   (  θ  )     ]         n    delimited-[]      g   subscript  X  n      g  θ       \displaystyle\sqrt{n}[g(X_{n})-g(\theta)]   This suggests that the error in the approximation converges to 0 in probability.  Multivariate delta method  By definition, a consistent  estimator  B  converges in probability to its true value β , and often a central limit theorem can be applied to obtain asymptotic normality :         n    (   B  -  β   )      →  𝐷     N   (  0  ,  Σ  )     ,      D  normal-→       n     B  β      N   0  normal-Σ      \sqrt{n}\left(B-\beta\right)\,\xrightarrow{D}\,N\left(0,\Sigma\right),     where n is the number of observations and Σ is a (symmetric positive semi-definite) covariance matrix. Suppose we want to estimate the variance of a function h of the estimator B . Keeping only the first two terms of the Taylor series , and using vector notation for the gradient , we can estimate h(B) as       h   (  B  )    ≈    h   (  β  )    +     ∇  h     (  β  )   T    ⋅   (   B  -  β   )           h  B       h  β    normal-⋅     normal-∇  h    superscript  β  T      B  β       h(B)\approx h(\beta)+\nabla h(\beta)^{T}\cdot(B-\beta)     which implies the variance of h(B) is approximately      Var   (   h   (  B  )    )      Var    h  B     \displaystyle\operatorname{Var}\left(h(B)\right)     One can use the mean value theorem (for real-valued functions of many variables) to see that this does not rely on taking first order approximation.  The delta method therefore implies that        n    (    h   (  B  )    -   h   (  β  )     )      →  𝐷     N   (  0  ,      ∇  h     (  β  )   T    ⋅  Σ  ⋅   ∇  h     (  β  )    )        D  normal-→       n       h  B     h  β       N   0     normal-⋅     normal-∇  h    superscript  β  T    normal-Σ   normal-∇  h    β       \sqrt{n}\left(h(B)-h(\beta)\right)\,\xrightarrow{D}\,N\left(0,\nabla h(\beta)^%
 {T}\cdot\Sigma\cdot\nabla h(\beta)\right)     or in univariate terms,         n    (    h   (  B  )    -   h   (  β  )     )      →  𝐷     N   (  0  ,    σ  2   ⋅    (    h  ′    (  β  )    )   2    )     .      D  normal-→       n       h  B     h  β       N   0   normal-⋅   superscript  σ  2    superscript     superscript  h  normal-′   β   2        \sqrt{n}\left(h(B)-h(\beta)\right)\,\xrightarrow{D}\,N\left(0,\sigma^{2}\cdot%
 \left(h^{\prime}(\beta)\right)^{2}\right).     Example  Suppose X n is Binomial with parameters p and n . Since         n    [     X  n   n   -  p   ]      →  𝐷     N   (  0  ,   p   (   1  -  p   )    )     ,      D  normal-→       n    delimited-[]       subscript  X  n   n   p       N   0    p    1  p        {\sqrt{n}\left[\frac{X_{n}}{n}-p\right]\,\xrightarrow{D}\,N(0,p(1-p))},     we can apply the Delta method with     g   (  θ  )    =   l  o  g   (  θ  )          g  θ     l  o  g  θ     g(θ)=log(θ)   to see        n    [    log   (    X  n   n   )    -   log   (  p  )     ]      →  𝐷     N   (  0  ,   p   (   1  -  p   )     [   1  /  p   ]   2    )        D  normal-→       n    delimited-[]         subscript  X  n   n      p        N   0    p    1  p    superscript   delimited-[]    1  p    2        {\sqrt{n}\left[\log\left(\frac{X_{n}}{n}\right)-\log(p)\right]\,\xrightarrow{D%
 }\,N(0,p(1-p)[1/p]^{2})}     Hence, the variance of    log   (    X  n   n   )          subscript  X  n   n     \log\left(\frac{X_{n}}{n}\right)   is approximately        1  -  p     p   n    .        1  p     p  n     \frac{1-p}{p\,n}.     Moreover, if    p  ^     normal-^  p    \hat{p}   and    q  ^     normal-^  q    \hat{q}   are estimates of different group rates from independent samples of sizes n and m respectively, then the logarithm of the estimated relative risk      p  ^    q  ^        normal-^  p    normal-^  q     \frac{\hat{p}}{\hat{q}}   is approximately normally distributed with variance that can be estimated by         1  -   p  ^       p  ^    n    +    1  -   q  ^       q  ^    m     .          1   normal-^  p       normal-^  p   n        1   normal-^  q       normal-^  q   m      \frac{1-\hat{p}}{\hat{p}\,n}+\frac{1-\hat{q}}{\hat{q}\,m}.     This is useful to construct a hypothesis test or to make a confidence interval for the relative risk.  Note  The delta method is often used in a form that is essentially identical to that above, but without the assumption that or B is asymptotically normal. Often the only context is that the variance is "small". The results then just give approximations to the means and covariances of the transformed quantities. For example, the formulae presented in Klein (1953, p. 258) are:       Var   (   h  r   )    =        Var   subscript  h  r    absent    \displaystyle\operatorname{Var}\left(h_{r}\right)=     where is the r th element of h ( B ) and B i is the i th element of B . The only difference is that Klein stated these as identities, whereas they are actually approximations.  See also   Taylor expansions for the moments of functions of random variables  Variance-stabilizing transformation   References   Casella, G. and Berger, R. L. (2002), Statistical Inference, 2nd ed.  Cramér, H. (1946), Mathematical Methods of Statistics, p. 353.  Davison, A. C. (2003), Statistical Models, pp. 33–35.  Greene, W. H. (2003), Econometric Analysis, 5th ed., pp. 913f.  Klein, L. R. (1953), A Textbook of Econometrics, p. 258.  Oehlert, G. W. (1992), A Note on the Delta Method, The American Statistician , Vol. 46, No. 1, p. 27-29. http://www.jstor.org/stable/2684406  Lecture notes  More lecture notes  Explanation from Stata software corporation   de:Statistischer Test#Asymptotisches Verhalten des Tests "  Category:Econometrics  Category:Statistical approximations  Category:Articles containing proofs     Oehlert, G. W. (1992). A note on the delta method. The American Statistician, 46(1), 27-29. ↩     