   Law of total variance      Law of total variance   In probability theory , the law of total variance 1 or variance decomposition formula , also known as Eve's law , states that if X and Y are random variables on the same probability space , and the variance of Y is finite, then        Var   [  Y  ]    =     E  X    (   Var   [  Y  ∣  X  ]    )    +    Var  X    (   E   [  Y  ∣  X  ]    )      .       Var  Y       subscript  normal-E  X    Var  Y  X      subscript  Var  X    normal-E  Y  X       \operatorname{Var}[Y]=\operatorname{E}_{X}(\operatorname{Var}[Y\mid X])+%
 \operatorname{Var}_{X}(\operatorname{E}[Y\mid X]).\,     Some writers on probability call this the "conditional variance formula". In language perhaps better known to statisticians than to probabilists, the two terms are the "explained" and the "unexplained" components of the variance respecively (cf. fraction of variance unexplained , explained variation ). In actuarial science , specifically credibility theory , the first component is called the expected value of the process variance ( EVPV ) and the second is called the variance of the hypothetical means ( VHM ). 2  There is a general variance decomposition formula for c ≥ 2 components (see below). 3 For example, with two conditioning random variables:        Var   [  Y  ]    =    E   (   Var   [  Y  ∣   X  1   ,   X  2   ]    )    +   E   (   Var   [   E   [  Y  ∣   X  1   ,   X  2   ]    ∣   X  1   ]    )    +   Var   (   E   [  Y  ∣   X  1   ]    )      ,       Var  Y      normal-E   Var  Y   subscript  X  1    subscript  X  2      normal-E   Var   normal-E  Y   subscript  X  1    subscript  X  2     subscript  X  1      Var   normal-E  Y   subscript  X  1        \operatorname{Var}[Y]=\operatorname{E}(\operatorname{Var}[Y\mid X_{1},X_{2}])+%
 \operatorname{E}(\operatorname{Var}[\operatorname{E}[Y\mid X_{1},X_{2}]\mid X_%
 {1}])+\operatorname{Var}(\operatorname{E}[Y\mid X_{1}]),\,     which follows from the law of total conditional variance: 4        Var   [  Y  ∣   X  1   ]    =    E   (   Var   [  Y  ∣   X  1   ,   X  2   ]    ∣   X  1   )    +   Var   (   E   [  Y  ∣   X  1   ,   X  2   ]    ∣   X  1   )      .       Var  Y   subscript  X  1       normal-E   Var  Y   subscript  X  1    subscript  X  2     subscript  X  1     Var   normal-E  Y   subscript  X  1    subscript  X  2     subscript  X  1       \operatorname{Var}[Y\mid X_{1}]=\operatorname{E}(\operatorname{Var}[Y\mid X_{1%
 },X_{2}]\mid X_{1})+\operatorname{Var}(\operatorname{E}[Y\mid X_{1},X_{2}]\mid
 X%
 _{1}).\,     Note that the conditional expected value is a random variable in its own right, whose value depends on the value of X . Notice that the conditional expected value of Y given the event  X = x is a function of x (this is where adherence to the conventional and rigidly case-sensitive notation of probability theory becomes important!). If we write E( Y | X = x ) = g ( x ) then the random variable  is just g ( X ). Similar comments apply to the conditional variance .  One special case, (similar to the Law of total expectation ) states that if     A  1   ,   A  2   ,  …  ,   A  n       subscript  A  1    subscript  A  2   normal-…   subscript  A  n     A_{1},A_{2},\ldots,A_{n}   is a partition of the whole outcome space, i.e. these events are mutually exclusive and exhaustive, then      Var   (  X  )   =   ∑   i  =  1   n   Var   (  X  ∣   A  i   )   P   (   A  i   )   +   ∑   i  =  1   n   E    (  X  ∣   A  i   )   2    (  1  -  P   (   A  i   )   )   P   (   A  i   )   -  2   ∑   i  =  1   n    ∑   j  =  1    i  -  1    E   (  X  ∣   A  i   )   P   (   A  i   )   E   (  X  ∣   A  j   )   P   (   A  j   )   .     fragments  Var   fragments  normal-(  X  normal-)     superscript   subscript     i  1    n   Var   fragments  normal-(  X  normal-∣   subscript  A  i   normal-)   normal-P   fragments  normal-(   subscript  A  i   normal-)     superscript   subscript     i  1    n   normal-E   superscript   fragments  normal-(  X  normal-∣   subscript  A  i   normal-)   2    fragments  normal-(  1   normal-P   fragments  normal-(   subscript  A  i   normal-)   normal-)   normal-P   fragments  normal-(   subscript  A  i   normal-)    2   superscript   subscript     i  1    n    superscript   subscript     j  1      i  1    normal-E   fragments  normal-(  X  normal-∣   subscript  A  i   normal-)   normal-P   fragments  normal-(   subscript  A  i   normal-)   normal-E   fragments  normal-(  X  normal-∣   subscript  A  j   normal-)   normal-P   fragments  normal-(   subscript  A  j   normal-)   normal-.    \operatorname{Var}(X)=\sum_{i=1}^{n}{\operatorname{Var}(X\mid A_{i})%
 \operatorname{P}(A_{i})}+\sum_{i=1}^{n}{\operatorname{E}(X\mid A_{i})^{2}(1-%
 \operatorname{P}(A_{i}))\operatorname{P}(A_{i})}-2\sum_{i=1}^{n}\sum_{j=1}^{i-%
 1}\operatorname{E}(X\mid A_{i})\operatorname{P}(A_{i})\operatorname{E}(X\mid A%
 _{j})\operatorname{P}(A_{j}).     Proof  The law of total variance can be proved using the law of total expectation . 5 First,       Var   [  Y  ]    =    E   [   Y  2   ]    -    [   E   [  Y  ]    ]   2         Var  Y      normal-E   superscript  Y  2     superscript   delimited-[]   normal-E  Y    2      \operatorname{Var}[Y]=\operatorname{E}[Y^{2}]-[\operatorname{E}[Y]]^{2}     from the definition of variance. Then we apply the law of total expectation to each term by conditioning on the random variable X :         =      E  X     [   E   [   Y  2   ∣  X  ]    ]    -    [    E  X    [   E   [  Y  ∣  X  ]    ]    ]   2        absent      subscript  normal-E  X    normal-E   superscript  Y  2   X     superscript   delimited-[]    subscript  normal-E  X    normal-E  Y  X     2      =\operatorname{E}_{X}\!\left[\operatorname{E}[Y^{2}\mid X]\right]-[%
 \operatorname{E}_{X}[\operatorname{E}[Y\mid X]]]^{2}        Now we rewrite the conditional second moment of Y in terms of its variance and first moment:         =      E  X     [    Var   [  Y  ∣  X  ]    +    [   E   [  Y  ∣  X  ]    ]   2    ]    -    [    E  X    [   E   [  Y  ∣  X  ]    ]    ]   2        absent      subscript  normal-E  X      Var  Y  X    superscript   delimited-[]   normal-E  Y  X    2      superscript   delimited-[]    subscript  normal-E  X    normal-E  Y  X     2      =\operatorname{E}_{X}\!\left[\operatorname{Var}[Y\mid X]+[\operatorname{E}[Y%
 \mid X]]^{2}\right]-[\operatorname{E}_{X}[\operatorname{E}[Y\mid X]]]^{2}        Since the expectation of a sum is the sum of expectations, the terms can now be regrouped:         =     E  X    [   Var   [  Y  ∣  X  ]    ]    +   (     E  X    [    [   E   [  Y  ∣  X  ]    ]   2   ]    -    [    E  X    [   E   [  Y  ∣  X  ]    ]    ]   2    )        absent      subscript  normal-E  X    Var  Y  X        subscript  normal-E  X    superscript   delimited-[]   normal-E  Y  X    2     superscript   delimited-[]    subscript  normal-E  X    normal-E  Y  X     2       =\operatorname{E}_{X}[\operatorname{Var}[Y\mid X]]+\left(\operatorname{E}_{X}[%
 [\operatorname{E}[Y\mid X]]^{2}]-[\operatorname{E}_{X}[\operatorname{E}[Y\mid X%
 ]]]^{2}\right)        Finally, we recognize the terms in parentheses as the variance of the conditional expectation E[ Y | X ]:         =     E  X    [   Var   [  Y  ∣  X  ]    ]    +    Var  X    [   E   [  Y  ∣  X  ]    ]         absent      subscript  normal-E  X    Var  Y  X      subscript  Var  X    normal-E  Y  X       =\operatorname{E}_{X}[\operatorname{Var}[Y\mid X]]+\operatorname{Var}_{X}[%
 \operatorname{E}[Y\mid X]]        General variance decomposition applicable to dynamic systems  The following formula shows how to apply the general, measure theoretic variance decomposition formula 6 to stochastic dynamic systems. Let Y ( t ) be the value of a system variable at time t . Suppose we have the internal histories ( natural filtrations )     H   1  t    ,   H   2  t    ,  …  ,   H    c  -  1   ,  t        subscript  H    1  t     subscript  H    2  t    normal-…   subscript  H     c  1   t      H_{1t},H_{2t},\ldots,H_{c-1,t}   , each one corresponding to the history (trajectory) of a different collection of system variables. The collections need not be disjoint. The variance of Y ( t ) can be decomposed, for all times t , into c ≥ 2 components as follows:        Var   [   Y   (  t  )    ]    =    E   (   Var   [   Y   (  t  )    ∣   H   1  t    ,   H   2  t    ,  …  ,   H    c  -  1   ,  t    ]    )    +    ∑   j  =  2    c  -  1     E   (   Var   [   E   [   Y   (  t  )    ∣   H   1  t    ,   H   2  t    ,  …  ,   H   j  t    ]    ∣   H   1  t    ,   H   2  t    ,  …  ,   H    j  -  1   ,  t    ]    )     +   Var   (   E   [   Y   (  t  )    ∣   H   1  t    ]    )      .       Var    Y  t       normal-E   Var    Y  t    subscript  H    1  t     subscript  H    2  t    normal-…   subscript  H     c  1   t        superscript   subscript     j  2      c  1     normal-E   Var   normal-E    Y  t    subscript  H    1  t     subscript  H    2  t    normal-…   subscript  H    j  t      subscript  H    1  t     subscript  H    2  t    normal-…   subscript  H     j  1   t        Var   normal-E    Y  t    subscript  H    1  t         \operatorname{Var}[Y(t)]=\operatorname{E}(\operatorname{Var}[Y(t)\mid H_{1t},H%
 _{2t},\ldots,H_{c-1,t}])+\sum_{j=2}^{c-1}\operatorname{E}(\operatorname{Var}[%
 \operatorname{E}[Y(t)\mid H_{1t},H_{2t},\ldots,H_{jt}]\mid H_{1t},H_{2t},%
 \ldots,H_{j-1,t}])+\operatorname{Var}(\operatorname{E}[Y(t)\mid H_{1t}]).\,     The decomposition is not unique. It depends on the order of the conditioning in the sequential decomposition.  The square of the correlation and explained (or informational) variation  In cases where ( Y , X ) are such that the conditional expected value is linear; i.e., in cases where        E   (  Y  ∣  X  )    =    a  X   +  b    ,       normal-E  Y  X       a  X   b     \operatorname{E}(Y\mid X)=aX+b,\,     it follows from the bilinearity of Cov(-,-) that      a  =    Cov   (  Y  ,  X  )     Var   (  X  )         a     Cov  Y  X    Var  X      a={\operatorname{Cov}(Y,X)\over\operatorname{Var}(X)}     and      b  =    E   (  Y  )    -     Cov   (  Y  ,  X  )     Var   (  X  )      E   (  X  )          b     normal-E  Y        Cov  Y  X    Var  X     normal-E  X       b=\operatorname{E}(Y)-{\operatorname{Cov}(Y,X)\over\operatorname{Var}(X)}%
 \operatorname{E}(X)     and the explained component of the variance divided by the total variance is just the square of the correlation between Y and X ; i.e., in such cases,        Var   (   E   (  Y  ∣  X  )    )     Var   (  Y  )     =  Corr    (  X  ,  Y  )   2   .     fragments     Var   normal-E  Y  X     Var  Y     Corr   superscript   fragments  normal-(  X  normal-,  Y  normal-)   2   normal-.    {\operatorname{Var}(\operatorname{E}(Y\mid X))\over\operatorname{Var}(Y)}=%
 \operatorname{Corr}(X,Y)^{2}.\,     One example of this situation is when ( X , Y ) have a bivariate normal (Gaussian) distribution.  More generally, when the conditional expectation  is a non-linear function of X       ι   Y  ∣  X    =    Var   (   E   (  Y  ∣  X  )    )     Var   (  Y  )     =  Corr    (  E   (  Y  ∣  X  )   ,  Y  )   2   ,     fragments   subscript  ι   fragments  Y  normal-∣  X        Var   normal-E  Y  X     Var  Y     Corr   superscript   fragments  normal-(  normal-E   fragments  normal-(  Y  normal-∣  X  normal-)   normal-,  Y  normal-)   2   normal-,    \iota_{Y\mid X}={\operatorname{Var}(\operatorname{E}(Y\mid X))\over%
 \operatorname{Var}(Y)}=\operatorname{Corr}(\operatorname{E}(Y\mid X),Y)^{2},\,    7  which can be estimated as the R squared from a non-linear regression of Y on X , using data drawn from the joint distribution of ( X , Y ). When  has a Gaussian distribution (and is an invertible function of X ), or Y itself has a (marginal) Gaussian distribution, this explained component of variation sets a lower bound on the mutual information : 8        I   (  Y  ;  X  )    ≥   ln   (    [   1  -   ι   Y  ∣  X     ]    -   1  /  2     )     .       normal-I  Y  X      superscript   delimited-[]    1   subscript  ι   fragments  Y  normal-∣  X          1  2        \operatorname{I}(Y;X)\geq\ln([1-\iota_{Y\mid X}]^{-1/2}).\,     Higher moments  A similar law for the third central moment  μ 3 says       μ  3    (  Y  )   =  E   (   μ  3    (  Y  ∣  X  )   )   +   μ  3    (  E   (  Y  ∣  X  )   )   +   3   cov   (  E   (  Y  ∣  X  )   ,  var   (  Y  ∣  X  )   )   .     fragments   subscript  μ  3    fragments  normal-(  Y  normal-)    normal-E   fragments  normal-(   subscript  μ  3    fragments  normal-(  Y  normal-∣  X  normal-)   normal-)     subscript  μ  3    fragments  normal-(  normal-E   fragments  normal-(  Y  normal-∣  X  normal-)   normal-)    3  cov   fragments  normal-(  normal-E   fragments  normal-(  Y  normal-∣  X  normal-)   normal-,  var   fragments  normal-(  Y  normal-∣  X  normal-)   normal-)   normal-.    \mu_{3}(Y)=\operatorname{E}(\mu_{3}(Y\mid X))+\mu_{3}(\operatorname{E}(Y\mid X%
 ))+3\,\operatorname{cov}(\operatorname{E}(Y\mid X),\operatorname{var}(Y\mid X)%
 ).\,     For higher cumulants , a simple and elegant generalization exists. See law of total cumulance .  See also   Law of total covariance , a generalization  Law of propagation of errors   References   9     (Problem 34.10(b))   "  Category:Algebra of random variables  Category:Statistical deviation and dispersion  Category:Articles containing proofs  Category:Theory of probability distributions  Category:Statistical theorems  Category:Statistical laws     Neil A. Weiss, A Course in Probability , Addison–Wesley, 2005, pages 385–386. ↩  ↩  Bowsher, C.G. and P.S. Swain, Proc Natl Acad Sci USA, 2012: 109, E1320–29. ↩   Neil A. Weiss, A Course in Probability , Addison–Wesley, 2005, pages 380–383. ↩     ↩     