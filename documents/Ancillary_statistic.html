<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="204">Ancillary statistic</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Ancillary statistic</h1>
<hr/>

<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, an <strong>ancillary statistic</strong> is a <a class="uri" href="statistic" title="wikilink">statistic</a> whose <a href="sampling_distribution" title="wikilink">sampling distribution</a> does not depend on the parameters of the model. An ancillary statistic is a <a href="pivotal_quantity" title="wikilink">pivotal quantity</a> that is also a statistic. Ancillary statistics can be used to construct <a href="prediction_interval" title="wikilink">prediction intervals</a>.</p>

<p>This concept was introduced by the statistical geneticist Sir <a href="Ronald_Fisher" title="wikilink">Ronald Fisher</a>.</p>
<h2 id="example">Example</h2>

<p>Suppose <em>X</em><sub>1</sub>, ..., <em>X</em><sub><em>n</em></sub> are <a href="Independent_identically-distributed_random_variables" title="wikilink">independent and identically distributed</a>, and are <a href="normal_distribution" title="wikilink">normally distributed</a> with unknown <a href="expected_value" title="wikilink">expected value</a> <em>μ</em> and known <a class="uri" href="variance" title="wikilink">variance</a> 1. Let</p>

<p>

<math display="block" id="Ancillary_statistic:0">
 <semantics>
  <mrow>
   <msub>
    <mover accent="true">
     <mi>X</mi>
     <mo>¯</mo>
    </mover>
    <mi>n</mi>
   </msub>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <msub>
      <mi>X</mi>
      <mn>1</mn>
     </msub>
     <mo rspace="4.2pt">+</mo>
     <mpadded width="+1.7pt">
      <mi mathvariant="normal">⋯</mi>
     </mpadded>
     <mo rspace="4.2pt">+</mo>
     <msub>
      <mi>X</mi>
      <mi>n</mi>
     </msub>
    </mrow>
    <mi>n</mi>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <ci>normal-¯</ci>
      <ci>X</ci>
     </apply>
     <ci>n</ci>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <plus></plus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>X</ci>
       <cn type="integer">1</cn>
      </apply>
      <ci>normal-⋯</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>X</ci>
       <ci>n</ci>
      </apply>
     </apply>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \overline{X}_{n}=\frac{X_{1}+\,\cdots\,+X_{n}}{n}
  </annotation>
 </semantics>
</math>

</p>

<p>be the <a href="Arithmetic_mean" title="wikilink">sample mean</a>.</p>

<p>The following statistical measures of dispersion of the sample</p>
<ul>
<li><a href="Range_(statistics)" title="wikilink">Range</a>: max(<em>X</em><sub>1</sub>, ..., <em>X</em><sub><em>n</em></sub>) − min(<em>X</em><sub>1</sub>, ..., <em>X<sub>n</sub></em>)</li>
<li><a href="Interquartile_range" title="wikilink">Interquartile range</a>: <em>Q</em><sub>3</sub> − <em>Q</em><sub>1</sub></li>
<li><a href="Sample_variance" title="wikilink">Sample variance</a>:</li>
</ul>
<dl>
<dd><dl>
<dd>

<math display="inline" id="Ancillary_statistic:1">
 <semantics>
  <mrow>
   <msup>
    <mover accent="true">
     <mi>σ</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mn>2</mn>
   </msup>
   <mo rspace="4.2pt">:=</mo>
   <mfrac>
    <mstyle scriptlevel="-1">
     <mrow>
      <mo largeop="true" symmetric="true">∑</mo>
      <msup>
       <mrow>
        <mo>(</mo>
        <mrow>
         <msub>
          <mi>X</mi>
          <mi>i</mi>
         </msub>
         <mo>-</mo>
         <mover accent="true">
          <mi>X</mi>
          <mo>¯</mo>
         </mover>
        </mrow>
        <mo>)</mo>
       </mrow>
       <mn>2</mn>
      </msup>
     </mrow>
    </mstyle>
    <mi>n</mi>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">assign</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <ci>normal-^</ci>
      <ci>σ</ci>
     </apply>
     <cn type="integer">2</cn>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <sum></sum>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>X</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <ci>normal-¯</ci>
         <ci>X</ci>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\sigma}^{2}:=\,\frac{\sum\left(X_{i}-\overline{X}\right)^{2}}{n}
  </annotation>
 </semantics>
</math>


</dd>
</dl>
</dd>
</dl>

<p>are all <em>ancillary statistics</em>, because their sampling distributions do not change as <em>μ</em> changes. Computationally, this is because in the formulas, the <em>μ</em> terms cancel – adding a constant number to a distribution (and all samples) changes its sample maximum and minimum by the same amount, so it does not change their difference, and likewise for others: these measures of dispersion do not depend on location.</p>

<p>Conversely, given i.i.d. normal variables with known mean 1 and unknown variance <em>σ</em><sup>2</sup>, the sample mean 

<math display="inline" id="Ancillary_statistic:2">
 <semantics>
  <mover accent="true">
   <mi>X</mi>
   <mo>¯</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-¯</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \overline{X}
  </annotation>
 </semantics>
</math>

 is <em>not</em> an ancillary statistic of the variance, as the sampling distribution of the sample mean is <em>N</em>(1, <em>σ</em><sup>2</sup>/<em>n</em>), which does depend on <em>σ</em> <sup>2</sup> – this measure of location (specifically, its <a href="standard_error" title="wikilink">standard error</a>) depends on dispersion.</p>
<h2 id="ancillary-complement">Ancillary complement</h2>

<p>Given a statistic <em>T</em> that is not <a href="Sufficiency_(statistics)" title="wikilink">sufficient</a>, an <strong>ancillary complement</strong> is a statistic <em>U</em> that is ancillary and such that (<em>T</em>, <em>U</em>) is sufficient.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Intuitively, an ancillary complement "adds the missing information" (without duplicating any).</p>

<p>The statistic is particularly useful if one takes <em>T</em> to be a <a href="maximum_likelihood_estimator" title="wikilink">maximum likelihood estimator</a>, which in general will not be sufficient; then one can ask for an ancillary complement. In this case, Fisher argues that one must condition on an ancillary complement to determine information content: one should consider the <a href="Fisher_information" title="wikilink">Fisher information</a> content of <em>T</em> to not be the marginal of <em>T</em>, but the conditional distribution of <em>T</em>, given <em>U</em>: how much information does <em>T</em> <em>add</em>? This is not possible in general, as no ancillary complement need exist, and if one exists, it need not be unique, nor does a maximum ancillary complement exist.</p>
<h3 id="example-1">Example</h3>

<p>In <a class="uri" href="baseball" title="wikilink">baseball</a>, suppose a scout observes a batter in <em>N</em> at-bats. Suppose (unrealistically) that the number <em>N</em> is chosen by some random process that is <a href="statistical_independence" title="wikilink">independent</a> of the batter's ability – say a coin is tossed after each at-bat and the result determines whether the scout will stay to watch the batter's next at-bat. The eventual data are the number <em>N</em> of at-bats and the number <em>X</em> of hits: the data (<em>X</em>, <em>N</em>) are a sufficient statistic. The observed <a href="batting_average" title="wikilink">batting average</a> <em>X</em>/<em>N</em> fails to convey all of the information available in the data because it fails to report the number <em>N</em> of at-bats (e.g., a batting average of 0.40, which is <a href="List_of_Major_League_Baseball_batting_champions" title="wikilink">very high</a>, based on only five at-bats does not inspire anywhere near as much confidence in the player's ability than a 0.40 average based on 100 at-bats). The number <em>N</em> of at-bats is an ancillary statistic because</p>
<ul>
<li>It is a part of the observable data (it is a <em>statistic</em>), and</li>
<li>Its probability distribution does not depend on the batter's ability, since it was chosen by a random process independent of the batter's ability.</li>
</ul>

<p>This ancillary statistic is an <strong>ancillary complement</strong> to the observed batting average <em>X</em>/<em>N</em>, i.e., the batting average <em>X</em>/<em>N</em> is not a <a href="sufficiency_(statistics)" title="wikilink">sufficient statistic</a>, in that it conveys less than all of the relevant information in the data, but conjoined with <em>N</em>, it becomes sufficient.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Basu's_theorem" title="wikilink">Basu's theorem</a></li>
<li><a href="Prediction_interval" title="wikilink">Prediction interval</a></li>
<li><a href="Group_family" title="wikilink">Group family</a></li>
<li><a href="Conditionality_principle" title="wikilink">Conditionality principle</a></li>
</ul>
<h2 id="notes">Notes</h2>

<p>"</p>

<p><a href="Category:Statistical_theory" title="wikilink">Category:Statistical theory</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="http://www.utstat.toronto.edu/dfraser/documents/237.pdf">Ancillary Statistics: A Review</a> by M. Ghosh, N. Reid and D.A.S. Fraser<a href="#fnref1">↩</a></li>
</ol>
</section>
</body>
</html>
