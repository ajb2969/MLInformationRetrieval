   MM algorithm      MM algorithm   The MM algorithm is an iterative optimization method which exploits the convexity of a function in order to find their maxima or minima. The MM stands for “Majorize-Minimization” or “Minorize-Maximization”, depending on whether you're doing maximization or minimization. MM itself is not an algorithm, but a description of how to construct an optimization algorithm .  The EM algorithm can be treated as a special case of the MM algorithm. 1 However, in the EM algorithm complex conditional expectation and extensive analytical skills are usually involved, while in the MM algorithm convexity and inequalities are our major focus, and it is relatively easier to understand and apply in most of the cases.  History  The original idea of the MM algorithm can be dated back at least to 1970 when Ortega and Rheinboldt were doing their studies related to line search methods. 2 The same idea kept reappearing under different guises in different areas until 2000 when Hunter and Lange put forth "MM" as general frame work. 3 Recently studies have shown that it can be used in a wide range of context, like mathematics , statistics , machine learning , engineering , etc.  How it works  MM algorithm works by finding a surrogate function that minorizes or majorizes the objective function. Optimizing the surrogate functions will drive the objective function upward or downward until a local optimum is reached.  Take the minorize-maximization version for example.  Let    f   (  θ  )       f  θ    f(\theta)   be the objective concave function we want to maximize. At the   m   m   m   step of the algorithm,    m  =   0  ,  1...       m   0  1...     m=0,1...   , the constructed function    g   (  θ  |   θ  m   )      fragments  g   fragments  normal-(  θ  normal-|   subscript  θ  m   normal-)     g(\theta|\theta_{m})   will be called the minorized version of the objective function (the surrogate function) at    θ  m     subscript  θ  m    \theta_{m}   if       g   (  θ  |   θ  m   )      fragments  g   fragments  normal-(  θ  normal-|   subscript  θ  m   normal-)     g(\theta|\theta_{m})    ≤     f   (  θ  )       f  θ    f(\theta)    for all    θ   θ   \theta         g   (   θ  m   |   θ  m   )   =  f   (   θ  m   )      fragments  g   fragments  normal-(   subscript  θ  m   normal-|   subscript  θ  m   normal-)    f   fragments  normal-(   subscript  θ  m   normal-)     g(\theta_{m}|\theta_{m})=f(\theta_{m})     Then we maximize    g   (  θ  |   θ  m   )      fragments  g   fragments  normal-(  θ  normal-|   subscript  θ  m   normal-)     g(\theta|\theta_{m})   instead of    f   (  θ  )       f  θ    f(\theta)   , and let        θ   m  +  1    =  arg   max  θ   g   (  θ  |   θ  m   )      fragments   subscript  θ    m  1       subscript   θ   g   fragments  normal-(  θ  normal-|   subscript  θ  m   normal-)     \theta_{m+1}=\arg\max_{\theta}g(\theta|\theta_{m})     The above iterative method will guarantee that    f   (   θ  m   )       f   subscript  θ  m     f(\theta_{m})   will converge to a local optimum or a saddle point as   m   m   m   goes to infinity. 4 By the construction we have       f   (   θ   m  +  1    )       f   subscript  θ    m  1      f(\theta_{m+1})    ≥     g   (   θ   m  +  1    |   θ  m   )      fragments  g   fragments  normal-(   subscript  θ    m  1    normal-|   subscript  θ  m   normal-)     g(\theta_{m+1}|\theta_{m})    ≥     g   (   θ  m   |   θ  m   )   =  f   (   θ  m   )      fragments  g   fragments  normal-(   subscript  θ  m   normal-|   subscript  θ  m   normal-)    f   fragments  normal-(   subscript  θ  m   normal-)     g(\theta_{m}|\theta_{m})=f(\theta_{m})     The marching of    θ  m     subscript  θ  m    \theta_{m}   and the surrogate functions relative to the objective function is shown on the Figure  We can just flip the image upside down, and that would be the methodology while we are doing Majorize-Minimization .  Ways to construct surrogate functions  Basically, we can use any inequalities to construct the desired majorized/minorized version of the objective function, but there are several typical choices   Jensen's inequality  Convexity inequality  Cauchy–Schwarz inequality  Inequality of arithmetic and geometric means   References  "  Category:Optimization algorithms and methods     ↩  ↩  ↩  Wu, C. F. Jeff (Mar 1983). "On the Convergence Properties of the EM Algorithm" ↩     