<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="338">Negentropy</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Negentropy</h1>
<hr>The '''negentropy''', also '''negative entropy''', '''syntropy''', '''extropy''', '''ectropy''' or '''entaxy''',<ref>Wiener, Norbert</ref> of a [[living system]] is the [[entropy]] that it exports to keep its own entropy low; it lies at the intersection of [[entropy and life]]. The concept and phrase "negative entropy" was introduced by [[Erwin Schrödinger]] in his 1944 popular-science book ''[[What is Life? (Schrödinger)|What is Life?]]''<r
<p>ef&gt;Schrödinger, Erwin, <em>What is Life - the Physical Aspect of the Living Cell</em>, Cambridge University Press, 1944 Later, <a href="Léon_Brillouin" title="wikilink">Léon Brillouin</a> shortened the phrase to <em>negentropy</em>,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> to express it in a more "positive" way: a living system imports negentropy and stores it.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> In 1974, <a href="Albert_Szent-Györgyi" title="wikilink">Albert Szent-Györgyi</a> proposed replacing the term <em>negentropy</em> with <em>syntropy</em>. That term may have originated in the 1940s with the Italian mathematician <a href="Luigi_Fantappiè" title="wikilink">Luigi Fantappiè</a>, who tried to construct a unified theory of <a class="uri" href="biology" title="wikilink">biology</a> and <a class="uri" href="physics" title="wikilink">physics</a>. <a href="Buckminster_Fuller" title="wikilink">Buckminster Fuller</a> tried to popularize this usage, but <em>negentropy</em> remains common.</r
<p></hr></body></html>

<p>In a note to <a href="What_is_Life?" title="wikilink">What is Life?</a> Schrödinger explained his use of this phrase. </p>

<p>Indeed, negentropy has been used by biologists as the basis for purpose or direction in life, namely cooperative or moral instincts.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>In 2009, Mahulikar &amp; Herwig redefined negentropy of a dynamically ordered sub-system as the specific entropy deficit of the ordered sub-system relative to its surrounding chaos.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> Thus, negentropy has units [J/kg-K] when defined based on specific entropy per unit mass, and [K<sup>−1</sup>] when defined based on specific entropy per unit energy. This definition enabled: <em>i</em>) scale-invariant thermodynamic representation of dynamic order existence, <em>ii</em>) formulation of physical principles exclusively for dynamic order existence and evolution, and <em>iii</em>) mathematical interpretation of Schrödinger's negentropy debt.</p>
<h2 id="information-theory">Information theory</h2>

<p>In <a href="information_theory" title="wikilink">information theory</a> and <a class="uri" href="statistics" title="wikilink">statistics</a>, negentropy is used as a measure of distance to normality.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a><a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> Out of all <a href="Distribution_(mathematics)" title="wikilink">distributions</a> with a given mean and variance, the normal or <a href="Gaussian_distribution" title="wikilink">Gaussian distribution</a> is the one with the highest entropy. Negentropy measures the difference in entropy between a given distribution and the Gaussian distribution with the same mean and variance. Thus, negentropy is always nonnegative, is invariant by any linear invertible change of coordinates, and vanishes <a href="if_and_only_if" title="wikilink">if and only if</a> the signal is Gaussian.</p>

<p>Negentropy is defined as</p>

<p>

<math display="block" id="Negentropy:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>J</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>p</mi>
      <mi>x</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi>S</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>ϕ</mi>
       <mi>x</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>-</mo>
    <mrow>
     <mi>S</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>p</mi>
       <mi>x</mi>
      </msub>
      <mo rspace="4.2pt" stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>J</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>p</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <ci>S</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ϕ</ci>
       <ci>x</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>S</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>p</ci>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   J(p_{x})=S(\phi_{x})-S(p_{x})\,
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Negentropy:1">
 <semantics>
  <mrow>
   <mi>S</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>ϕ</mi>
     <mi>x</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>S</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>ϕ</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S(\phi_{x})
  </annotation>
 </semantics>
</math>

 is the <a href="differential_entropy" title="wikilink">differential entropy</a> of the Gaussian density with the same <a class="uri" href="mean" title="wikilink">mean</a> and <a class="uri" href="variance" title="wikilink">variance</a> as 

<math display="inline" id="Negentropy:2">
 <semantics>
  <msub>
   <mi>p</mi>
   <mi>x</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>p</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p_{x}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Negentropy:3">
 <semantics>
  <mrow>
   <mi>S</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>p</mi>
     <mi>x</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>S</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>p</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S(p_{x})
  </annotation>
 </semantics>
</math>


 is the differential entropy of 

<math display="inline" id="Negentropy:4">
 <semantics>
  <msub>
   <mi>p</mi>
   <mi>x</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>p</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p_{x}
  </annotation>
 </semantics>
</math>

:</p>

<p>

<math display="block" id="Negentropy:5">
 <semantics>
  <mrow>
   <mrow>
    <mi>S</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>p</mi>
      <mi>x</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <mo largeop="true" symmetric="true">∫</mo>
     <mrow>
      <msub>
       <mi>p</mi>
       <mi>x</mi>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>u</mi>
       <mo stretchy="false">)</mo>
      </mrow>
      <mrow>
       <mi>log</mi>
       <msub>
        <mi>p</mi>
        <mi>x</mi>
       </msub>
      </mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>u</mi>
       <mo stretchy="false">)</mo>
      </mrow>
      <mi>d</mi>
      <mi>u</mi>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>S</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>p</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <int></int>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>p</ci>
        <ci>x</ci>
       </apply>
       <ci>u</ci>
       <apply>
        <log></log>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>p</ci>
         <ci>x</ci>
        </apply>
       </apply>
       <ci>u</ci>
       <ci>d</ci>
       <ci>u</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S(p_{x})=-\int p_{x}(u)\log p_{x}(u)du
  </annotation>
 </semantics>
</math>

</p>

<p>Negentropy is used in <a class="uri" href="statistics" title="wikilink">statistics</a> and <a href="signal_processing" title="wikilink">signal processing</a>. It is related to network <a href="Information_entropy" title="wikilink">entropy</a>, which is used in <a href="Independent_Component_Analysis" title="wikilink">Independent Component Analysis</a>.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a><a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h2 id="correlation-between-statistical-negentropy-and-gibbs-free-energy">Correlation between statistical negentropy and Gibbs' free energy</h2>

<p> There is a physical quantity closely linked to <a href="Thermodynamic_free_energy" title="wikilink">free energy</a> (<a href="free_enthalpy" title="wikilink">free enthalpy</a>), with a unit of entropy and isomorphic to negentropy known in statistics and information theory. In 1873, <a href="Josiah_Willard_Gibbs" title="wikilink">Willard Gibbs</a> created a diagram illustrating the concept of free energy corresponding to <a href="free_enthalpy" title="wikilink">free enthalpy</a>. On the diagram one can see the quantity called <a href="capacity_for_entropy" title="wikilink">capacity for entropy</a>. The said quantity is the amount of entropy that may be increased without changing an internal energy or increasing its volume.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> In other words, it is a difference between maximum possible, under assumed conditions, entropy and its actual entropy. It corresponds exactly to the definition of negentropy adopted in statistics and information theory. A similar physical quantity was introduced in 1869 by <a href="François_Jacques_Dominique_Massieu" title="wikilink">Massieu</a> for the <a href="isothermal_process" title="wikilink">isothermal process</a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a><a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> (both quantities differs just with a figure sign) and then <a href="Max_Planck" title="wikilink">Planck</a> for the <a href="Isothermal_process" title="wikilink">isothermal</a>-<a href="Isobaric_process" title="wikilink">isobaric</a> process.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> More recently, the Massieu-Planck <a href="thermodynamic_potential" title="wikilink">thermodynamic potential</a>, known also as <em><a href="free_entropy" title="wikilink">free entropy</a></em>, has been shown to play a great role in the so-called entropic formulation of <a href="statistical_mechanics" title="wikilink">statistical mechanics</a>,<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> applied among the others in molecular biology<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> and thermodynamic non-equilibrium processes.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a></p>
<dl>
<dd><dl>
<dd><strong>

<math display="inline" id="Negentropy:6">
 <semantics>
  <mrow>
   <mi>J</mi>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>S</mi>
     <mi>max</mi>
    </msub>
    <mo>-</mo>
    <mi>S</mi>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mi mathvariant="normal">Φ</mi>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <mi>k</mi>
     <mrow>
      <mi>ln</mi>
      <mpadded width="+1.7pt">
       <mi>Z</mi>
      </mpadded>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>J</ci>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>S</ci>
       <max></max>
      </apply>
      <ci>S</ci>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <minus></minus>
      <ci>normal-Φ</ci>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>k</ci>
       <apply>
        <ln></ln>
        <ci>Z</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   J=S_{\max}-S=-\Phi=-k\ln Z\,
  </annotation>
 </semantics>
</math>

</strong>
</dd>
</dl>
</dd>
</dl>
<dl>
<dd><dl>
<dd>where:
</dd>
<dd>

<math display="inline" id="Negentropy:7">
 <semantics>
  <mi>J</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>J</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   J
  </annotation>
 </semantics>
</math>

 - negentropy (Gibbs "capacity for entropy")
</dd>
<dd>

<math display="inline" id="Negentropy:8">
 <semantics>
  <mi mathvariant="normal">Φ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-Φ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Phi
  </annotation>
 </semantics>
</math>


 – <a href="Free_entropy" title="wikilink">Massieu potential</a>
</dd>
<dd>

<math display="inline" id="Negentropy:9">
 <semantics>
  <mi>Z</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Z</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Z
  </annotation>
 </semantics>
</math>

 - <a href="Partition_function_(statistical_mechanics)" title="wikilink">partition function</a>
</dd>
<dd>

<math display="inline" id="Negentropy:10">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

 - <a href="Boltzmann_constant" title="wikilink">Boltzmann constant</a>
</dd>
</dl>
</dd>
</dl>
<h2 id="risk-management">Risk management</h2>

<p>In <a href="risk_management" title="wikilink">risk management</a>, negentropy is the force that seeks to achieve effective organizational behavior and lead to a steady predictable state.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a></p>
<h2 id="brillouins-negentropy-principle-of-information">Brillouin's negentropy principle of information</h2>

<p>In 1953, Brillouin derived a general equation<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> stating that the changing of an information bit value requires at least kT ln(2) energy. This is the same energy as the work <a href="Leó_Szilárd" title="wikilink">Leó Szilárd</a>'s engine produces in the idealistic case. In his book,<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> he further explored this problem concluding that any cause of this bit value change (measurement, decision about a yes/no question, erasure, display, etc.) will require the same amount of energy.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a class="uri" href="Exergy" title="wikilink">Exergy</a></li>
<li><a class="uri" href="Extropy" title="wikilink">Extropy</a></li>
<li><a href="Free_entropy" title="wikilink">Free entropy</a></li>
<li><a href="Entropy_in_thermodynamics_and_information_theory" title="wikilink">Entropy in thermodynamics and information theory</a></li>
</ul>
<h2 id="notes">Notes</h2>

<p>"</p>

<p><a href="Category:Thermodynamic_entropy" title="wikilink">Category:Thermodynamic entropy</a> <a href="Category:Entropy_and_information" title="wikilink">Category:Entropy and information</a> <a href="Category:Statistical_deviation_and_dispersion" title="wikilink">Category:Statistical deviation and dispersion</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Brillouin, Leon: (1953) "Negentropy Principle of Information", <em>J. of Applied Physics</em>, v. <strong>24(9)</strong>, pp. 1152-1163<a href="#fnref1">↩</a></li>
<li id="fn2">Léon Brillouin, <em>La science et la théorie de l'information</em>, Masson, 1959<a href="#fnref2">↩</a></li>
<li id="fn3">Mae-Wan Ho, <a href="http://www.i-sis.org.uk/negentr.php">What is (Schrödinger's) Negentropy?</a>, Bioelectrodynamics Laboratory, Open university Walton Hall, Milton Keynes<a href="#fnref3">↩</a></li>
<li id="fn4"><a href="Jeremy_Griffith" title="wikilink">Jeremy Griffith</a>. 2011. <em>What is the Meaning of Life?</em>. In <em>The Book of Real Answers to Everything!</em> ISBN 9781741290073. From <a class="uri" href="http://www.worldtransformation.com/what-is-the-meaning-of-life/">http://www.worldtransformation.com/what-is-the-meaning-of-life/</a><a href="#fnref4">↩</a></li>
<li id="fn5">Mahulikar, S.P. &amp; Herwig, H.: (2009) "Exact thermodynamic principles for dynamic order existence and evolution in chaos", <em>Chaos, Solitons &amp; Fractals</em>, v. <strong>41(4)</strong>, pp. 1939-1948<a href="#fnref5">↩</a></li>
<li id="fn6">Aapo Hyvärinen, <a href="http://www.cis.hut.fi/aapo/papers/NCS99web/node32.html">Survey on Independent Component Analysis, node32: Negentropy</a>, Helsinki University of Technology Laboratory of Computer and Information Science<a href="#fnref6">↩</a></li>
<li id="fn7">Aapo Hyvärinen and Erkki Oja, <a href="http://www.cis.hut.fi/aapo/papers/IJCNN99_tutorialweb/node14.html">Independent Component Analysis: A Tutorial, node14: Negentropy</a>, Helsinki University of Technology Laboratory of Computer and Information Science<a href="#fnref7">↩</a></li>
<li id="fn8">Ruye Wang, <a href="http://fourier.eng.hmc.edu/e161/lectures/ica/node4.html">Independent Component Analysis, node4: Measures of Non-Gaussianity</a><a href="#fnref8">↩</a></li>
<li id="fn9">P. Comon, Independent Component Analysis - a new concept?, <em>Signal Processing</em>, <strong>36</strong> 287-314, 1994.<a href="#fnref9">↩</a></li>
<li id="fn10">Didier G. Leibovici and Christian Beckmann, <a href="http://www.fmrib.ox.ac.uk/analysis/techrep/tr01dl1/tr01dl1/tr01dl1.html">An introduction to Multiway Methods for Multi-Subject fMRI experiment</a>, FMRIB Technical Report 2001, Oxford Centre for Functional Magnetic Resonance Imaging of the Brain (FMRIB), Department of Clinical Neurology, University of Oxford, John Radcliffe Hospital, Headley Way, Headington, Oxford, UK.<a href="#fnref10">↩</a></li>
<li id="fn11">Willard Gibbs, <a href="http://www.ufn.ru/ufn39/ufn39_4/Russian/r394b.pdf">A Method of Geometrical Representation of the Thermodynamic Properties of Substances by Means of Surfaces</a>, <em>Transactions of the Connecticut Academy</em>, 382-404 (1873)<a href="#fnref11">↩</a></li>
<li id="fn12">Massieu, M. F. (1869a). Sur les fonctions caractéristiques des divers fluides. <em>C. R. Acad. Sci.</em> LXIX:858-862.<a href="#fnref12">↩</a></li>
<li id="fn13">Massieu, M. F. (1869b). Addition au precedent memoire sur les fonctions caractéristiques. <em>C. R. Acad. Sci.</em> LXIX:1057-1061.<a href="#fnref13">↩</a></li>
<li id="fn14">Massieu, M. F. (1869), <em>Compt. Rend.</em> <strong>69</strong> (858): 1057.<a href="#fnref14">↩</a></li>
<li id="fn15">Planck, M. (1945). <em>Treatise on Thermodynamics</em>. Dover, New York.<a href="#fnref15">↩</a></li>
<li id="fn16">Antoni Planes, Eduard Vives, <a href="http://www.ecm.ub.es/condensed/eduard/papers/massieu/node2.html">Entropic Formulation of Statistical Mechanics</a>, Entropic variables and Massieu-Planck functions 2000-10-24 Universitat de Barcelona<a href="#fnref16">↩</a></li>
<li id="fn17">John A. Scheilman, <a href="http://www.biophysj.org/cgi/reprint/73/6/2960.pdf">Temperature, Stability, and the Hydrophobic Interaction</a>, <em>Biophysical Journal</em> <strong>73</strong> (December 1997), 2960-2964, Institute of Molecular Biology, University of Oregon, Eugene, Oregon 97403 USA<a href="#fnref17">↩</a></li>
<li id="fn18">Z. Hens and X. de Hemptinne, <a href="http://arxiv.org/pdf/chao-dyn/9604008">Non-equilibrium Thermodynamics approach to Transport Processes in Gas Mixtures</a>, Department of Chemistry, Catholic University of Leuven, Celestijnenlaan 200 F, B-3001 Heverlee, Belgium<a href="#fnref18">↩</a></li>
<li id="fn19"><a href="http://www.kent.ac.uk/scarr/events/Grinberg-%20(2).pdf">Pedagogical Risk and Governmentality: Shantytowns in Argentina in the 21st Century</a> (see p. 4).<a href="#fnref19">↩</a></li>
<li id="fn20">Leon Brillouin, The negentropy principle of information, <em>J. Applied Physics</em> <strong>24</strong>, 1152-1163 1953<a href="#fnref20">↩</a></li>
<li id="fn21">Leon Brillouin, <em>Science and Information theory</em>, Dover, 1956<a href="#fnref21">↩</a></li>
</ol>
</section>


