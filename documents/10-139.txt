   Hellinger distance      Hellinger distance   In probability and statistics , the Hellinger distance (also called Bhattacharyya distance as this was originally introduced by Anil Kumar Bhattacharya ) is used to quantify the similarity between two probability distributions . It is a type of f -divergence . The Hellinger distance is defined in terms of the Hellinger integral , which was introduced by Ernst Hellinger in 1909. 1 2  Definition  Measure theory  To define the Hellinger distance in terms of measure theory , let P and Q denote two probability measures that are absolutely continuous with respect to a third probability measure Œª. The square of the Hellinger distance between P and Q is defined as the quantity         H  2    (  P  ,  Q  )    =    1  2    ‚à´     (      d  P    d  Œª     -     d  Q    d  Œª      )   2   d  Œª      .         superscript  H  2    P  Q        1  2        superscript          d  P     d  Œª           d  Q     d  Œª      2   d  Œª       H^{2}(P,Q)=\frac{1}{2}\displaystyle\int\left(\sqrt{\frac{dP}{d\lambda}}-\sqrt{%
 \frac{dQ}{d\lambda}}\right)^{2}d\lambda.     Here, dP / dŒª and dQ / d Œª are the Radon‚ÄìNikodym derivatives of P and Q respectively. This definition does not depend on Œª, so the Hellinger distance between P and Q does not change if Œª is replaced with a different probability measure with respect to which both P and Q are absolutely continuous. For compactness, the above formula is often written as         H  2    (  P  ,  Q  )    =    1  2    ‚à´    (     d  P    -    d  Q     )   2      .         superscript  H  2    P  Q        1  2      superscript        d  P        d  Q     2       H^{2}(P,Q)=\frac{1}{2}\int\left(\sqrt{dP}-\sqrt{dQ}\right)^{2}.     Probability theory using Lebesgue measure  To define the Hellinger distance in terms of elementary probability theory, we take Œª to be Lebesgue measure , so that dP / dŒª and dQ / d Œª are simply probability density functions . If we denote the densities as f and g , respectively, the squared Hellinger distance can be expressed as a standard calculus integral         H  2    (  P  ,  Q  )    =    1  2    ‚à´      (     f   (  x  )     -    g   (  x  )      )   2    d  x     =   1  -   ‚à´      f   (  x  )   g   (  x  )      d  x      ,           superscript  H  2    P  Q        1  2        superscript        f  x        g  x     2   d  x            1          f  x  g  x    d  x        H^{2}(P,Q)=\frac{1}{2}\int\left(\sqrt{f(x)}-\sqrt{g(x)}\right)^{2}\,dx=1-\int%
 \sqrt{f(x)g(x)}\,dx,     where the second form can be obtained by expanding the square and using the fact that the integral of a probability density over its domain equals 1.  The Hellinger distance H ( P , Q ) satisfies the property (derivable from the Cauchy‚ÄìSchwarz inequality )      0  ‚â§   H   (  P  ,  Q  )    ‚â§  1.        0    H   P  Q         1.     0\leq H(P,Q)\leq 1.     Discrete distributions  For two discrete probability distributions    P  =   (   p  1   ,  ‚Ä¶  ,   p  k   )       P    subscript  p  1   normal-‚Ä¶   subscript  p  k      P=(p_{1},\ldots,p_{k})   and    Q  =   (   q  1   ,  ‚Ä¶  ,   q  k   )       Q    subscript  q  1   normal-‚Ä¶   subscript  q  k      Q=(q_{1},\ldots,q_{k})   , their Hellinger distance is defined as        H   (  P  ,  Q  )    =     1   2        ‚àë   i  =  1   k     (     p  i    -    q  i     )   2       ,        H   P  Q        1    2        superscript   subscript     i  1    k    superscript       subscript  p  i       subscript  q  i     2        H(P,Q)=\frac{1}{\sqrt{2}}\;\sqrt{\sum_{i=1}^{k}(\sqrt{p_{i}}-\sqrt{q_{i}})^{2}},     which is directly related to the Euclidean norm of the difference of the square root vectors, i.e.        H   (  P  ,  Q  )    =     1   2       ‚à•    P   -   Q    ‚à•   2     .        H   P  Q        1    2     subscript   norm      P     Q     2      H(P,Q)=\frac{1}{\sqrt{2}}\;\bigl\|\sqrt{P}-\sqrt{Q}\bigr\|_{2}.     Connection with the statistical distance  The Hellinger distance    H   (  P  ,  Q  )       H   P  Q     H(P,Q)   and the total variation distance (or statistical distance)    Œ¥   (  P  ,  Q  )       Œ¥   P  Q     \delta(P,Q)   are related as follows: 3         H  2    (  P  ,  Q  )    ‚â§   Œ¥   (  P  ,  Q  )    ‚â§    2   H   (  P  ,  Q  )     .           superscript  H  2    P  Q      Œ¥   P  Q             2   H   P  Q       H^{2}(P,Q)\leq\delta(P,Q)\leq\sqrt{2}H(P,Q)\,.     These inequalities follow immediately from the inequalities between the 1-norm and the 2-norm .  Properties  The Hellinger distance forms a bounded  metric on the space of probability distributions over a given probability space .  The maximum distance 1 is achieved when P assigns probability zero to every set to which Q assigns a positive probability, and vice versa.  Sometimes the factor 1/2 in front of the integral is omitted, in which case the Hellinger distance ranges from zero to the square root of two.  The Hellinger distance is related to the Bhattacharyya coefficient     B  C   (  P  ,  Q  )       B  C   P  Q     BC(P,Q)   as it can be defined as        H   (  P  ,  Q  )    =    1  -   B  C   (  P  ,  Q  )       .        H   P  Q        1    B  C   P  Q        H(P,Q)=\sqrt{1-BC(P,Q)}.     Hellinger distances are used in the theory of sequential and asymptotic statistics . 4 5  Examples  The squared Hellinger distance between two normal distributions      P   ‚àº   ùí©   (   Œº  1   ,   œÉ  1  2   )       similar-to  P    ùí©    subscript  Œº  1    superscript   subscript  œÉ  1   2       \scriptstyle P\,\sim\,\mathcal{N}(\mu_{1},\sigma_{1}^{2})   and     Q   ‚àº   ùí©   (   Œº  2   ,   œÉ  2  2   )       similar-to  Q    ùí©    subscript  Œº  2    superscript   subscript  œÉ  2   2       \scriptstyle Q\,\sim\,\mathcal{N}(\mu_{2},\sigma_{2}^{2})   is:         H  2    (  P  ,  Q  )    =   1  -       2   œÉ  1    œÉ  2      œÉ  1  2   +   œÉ  2  2        e   -    1  4      (    Œº  1   -   Œº  2    )   2     œÉ  1  2   +   œÉ  2  2           .         superscript  H  2    P  Q      1          2   subscript  œÉ  1    subscript  œÉ  2       superscript   subscript  œÉ  1   2    superscript   subscript  œÉ  2   2       superscript  e        1  4      superscript     subscript  Œº  1    subscript  Œº  2    2      superscript   subscript  œÉ  1   2    superscript   subscript  œÉ  2   2            H^{2}(P,Q)=1-\sqrt{\frac{2\sigma_{1}\sigma_{2}}{\sigma_{1}^{2}+\sigma_{2}^{2}}%
 }\,e^{-\frac{1}{4}\frac{(\mu_{1}-\mu_{2})^{2}}{\sigma_{1}^{2}+\sigma_{2}^{2}}}.     The squared Hellinger distance between two exponential distributions      P   ‚àº   Exp   (  Œ±  )       similar-to  P    Exp  Œ±     \scriptstyle P\,\sim\,\rm{Exp}(\alpha)   and     Q   ‚àº   Exp   (  Œ≤  )       similar-to  Q    Exp  Œ≤     \scriptstyle Q\,\sim\,\rm{Exp}(\beta)   is:         H  2    (  P  ,  Q  )    =   1  -    2    Œ±  Œ≤      Œ±  +  Œ≤      .         superscript  H  2    P  Q      1      2      Œ±  Œ≤       Œ±  Œ≤       H^{2}(P,Q)=1-\frac{2\sqrt{\alpha\beta}}{\alpha+\beta}.     The squared Hellinger distance between two Weibull distributions      P   ‚àº   W   (  k  ,  Œ±  )       similar-to  P    normal-W   normal-k  Œ±      \scriptstyle P\,\sim\,\rm{W}(k,\alpha)   and     Q   ‚àº   W   (  k  ,  Œ≤  )       similar-to  Q    normal-W   normal-k  Œ≤      \scriptstyle Q\,\sim\,\rm{W}(k,\beta)   (where   k   k   k   is a common shape parameter and     Œ±   ,  Œ≤     Œ±  Œ≤    \alpha\,,\beta   are the scale parameters respectively):         H  2    (  P  ,  Q  )    =   1  -    2    (   Œ±  Œ≤   )    k  /  2       Œ±  k   +   Œ≤  k       .         superscript  H  2    P  Q      1      2   superscript    Œ±  Œ≤     k  2        superscript  Œ±  k    superscript  Œ≤  k        H^{2}(P,Q)=1-\frac{2(\alpha\beta)^{k/2}}{\alpha^{k}+\beta^{k}}.     The squared Hellinger distance between two Poisson distributions with rate parameters   Œ±   Œ±   \alpha   and   Œ≤   Œ≤   \beta   , so that     P   ‚àº   Poisson   (  Œ±  )       similar-to  P    Poisson  Œ±     \scriptstyle P\,\sim\,\rm{Poisson}(\alpha)   and     Q   ‚àº   Poisson   (  Œ≤  )       similar-to  Q    Poisson  Œ≤     \scriptstyle Q\,\sim\,\rm{Poisson}(\beta)   , is:         H  2    (  P  ,  Q  )    =   1  -   e   -    1  2     (    Œ±   -   Œ≤    )   2        .         superscript  H  2    P  Q      1   superscript  e        1  2    superscript      Œ±     Œ≤    2         H^{2}(P,Q)=1-e^{-\frac{1}{2}(\sqrt{\alpha}-\sqrt{\beta})^{2}}.     The squared Hellinger distance between two Beta distributions      P   ‚àº   Beta   (   a  1   ,   b  1   )       similar-to  P    Beta    subscript  a  1    subscript  b  1       \scriptstyle P\,\sim\,\text{Beta}(a_{1},b_{1})   and     Q   ‚àº   Beta   (   a  2   ,   b  2   )       similar-to  Q    Beta    subscript  a  2    subscript  b  2       \scriptstyle Q\,\sim\,\text{Beta}(a_{2},b_{2})   is:        H  2    (  P  ,  Q  )    =   1  -    B   (     a  1   +   a  2    2   ,     b  1   +   b  2    2   )      B   (   a  1   ,   b  1   )   B   (   a  2   ,   b  2   )              superscript  H  2    P  Q      1      B        subscript  a  1    subscript  a  2    2        subscript  b  1    subscript  b  2    2         B    subscript  a  1    subscript  b  1    B    subscript  a  2    subscript  b  2          H^{2}(P,Q)=1-\frac{B\left(\frac{a_{1}+a_{2}}{2},\frac{b_{1}+b_{2}}{2}\right)}{%
 \sqrt{B(a_{1},b_{1})B(a_{2},b_{2})}}   where   B   B   B   is the Beta function .  See also   Kullback‚ÄìLeibler divergence  Fisher information metric   Notes  References       "  Category:Probability theory  Category:F-divergences  Category:Statistical distance measures  Category:String similarity measures     ‚Ü©  ‚Ü©  Harsha's lecture notes on communication complexity ‚Ü©  Erik Torgerson (1991) Comparison of Statistical Experiments , volume 36 of Encyclopedia of Mathematics. Cambridge University Press. ‚Ü©  ‚Ü©     