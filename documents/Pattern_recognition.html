<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="290">Pattern recognition</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Pattern recognition</h1>
<hr/>
<p><strong>Pattern recognition</strong> is a branch of <a href="machine_learning" title="wikilink">machine learning</a> that focuses on the recognition of patterns and regularities in data, although it is in some cases considered to be nearly synonymous with machine learning.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Pattern recognition systems are in many cases trained from labeled "training" data (<a href="supervised_learning" title="wikilink">supervised learning</a>), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (<a href="unsupervised_learning" title="wikilink">unsupervised learning</a>).</p>
<p>The terms pattern recognition, machine learning, <a href="data_mining" title="wikilink">data mining</a> and <a href="knowledge_discovery_in_databases" title="wikilink">knowledge discovery in databases</a> (KDD) are hard to separate, as they largely overlap in their scope. Machine learning is the common term for supervised learning methods and originates from <a href="artificial_intelligence" title="wikilink">artificial intelligence</a>, whereas KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition has its origins in <a class="uri" href="engineering" title="wikilink">engineering</a>, and the term is popular in the context of <a href="computer_vision" title="wikilink">computer vision</a>: a leading computer vision conference is named <a href="Conference_on_Computer_Vision_and_Pattern_Recognition" title="wikilink">Conference on Computer Vision and Pattern Recognition</a>. In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern; whereas machine learning traditionally focuses on maximizing the recognition rates. Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics; and have become increasingly similar by integrating developments and ideas from each other.</p>
<p>In <a href="machine_learning" title="wikilink">machine learning</a>, <strong>pattern recognition</strong> is the assignment of a label to a given input value. In statistics, <a href="Linear_discriminant_analysis" title="wikilink">discriminant analysis</a> was introduced for this same purpose in 1936. An example of pattern recognition is <a href="classification_(machine_learning)" title="wikilink">classification</a>, which attempts to assign each input value to one of a given set of <em>classes</em> (for example, determine whether a given email is "spam" or "non-spam"). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are <a href="regression_analysis" title="wikilink">regression</a>, which assigns a real-valued output to each input; <a href="sequence_labeling" title="wikilink">sequence labeling</a>, which assigns a class to each member of a sequence of values (for example, <a href="part_of_speech_tagging" title="wikilink">part of speech tagging</a>, which assigns a <a href="part_of_speech" title="wikilink">part of speech</a> to each word in an input sentence); and <a class="uri" href="parsing" title="wikilink">parsing</a>, which assigns a <a href="parse_tree" title="wikilink">parse tree</a> to an input sentence, describing the <a href="syntactic_structure" title="wikilink">syntactic structure</a> of the sentence.</p>
<p>Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform "most likely" matching of the inputs, taking into account their statistical variation. This is opposed to <em><a href="pattern_matching" title="wikilink">pattern matching</a></em> algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is <a href="regular_expression" title="wikilink">regular expression</a> matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many <a href="text_editor" title="wikilink">text editors</a> and <a href="word_processor" title="wikilink">word processors</a>. In contrast to pattern recognition, pattern matching is generally not considered a type of machine learning, although pattern-matching algorithms (especially with fairly general, carefully tailored patterns) can sometimes succeed in providing similar-quality output to the sort provided by pattern-recognition algorithms.</p>
<h2 id="overview">Overview</h2>
<p>Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. <em><a href="Supervised_learning" title="wikilink">Supervised learning</a></em> assumes that a set of <em>training data</em> (the <em><a href="training_set" title="wikilink">training set</a></em>) has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. A learning procedure then generates a <em>model</em> that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of "simple", in accordance with <a href="Occam's_Razor" title="wikilink">Occam's Razor</a>, discussed below). <a href="Unsupervised_learning" title="wikilink">Unsupervised learning</a>, on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> A combination of the two that has recently been explored is <a href="semi-supervised_learning" title="wikilink">semi-supervised learning</a>, which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). Note that in cases of unsupervised learning, there may be no training data at all to speak of; in other words, the data to be labeled <em>is</em> the training data.</p>
<p>Note that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. For example, the unsupervised equivalent of classification is normally known as <em><a href="data_clustering" title="wikilink">clustering</a></em>, based on the common perception of the task as involving no training data to speak of, and of grouping the input data into <em>clusters</em> based on some inherent similarity measure (e.g. the <a class="uri" href="distance" title="wikilink">distance</a> between instances, considered as vectors in a multi-dimensional <a href="vector_space" title="wikilink">vector space</a>), rather than assigning each input instance into one of a set of pre-defined classes. Note also that in some fields, the terminology is different: For example, in <a href="community_ecology" title="wikilink">community ecology</a>, the term "classification" is used to refer to what is commonly known as "clustering".</p>
<p>The piece of input data for which an output value is generated is formally termed an <em>instance</em>. The instance is formally described by a <a href="feature_vector" title="wikilink">vector</a> of <em>features</em>, which together constitute a description of all known characteristics of the instance. (These feature vectors can be seen as defining points in an appropriate <a href="space_(mathematics)" title="wikilink">multidimensional space</a>, and methods for manipulating vectors in <a href="vector_space" title="wikilink">vector spaces</a> can be correspondingly applied to them, such as computing the <a href="dot_product" title="wikilink">dot product</a> or the angle between two vectors.) Typically, features are either <a href="categorical_data" title="wikilink">categorical</a> (also known as <a href="nominal_data" title="wikilink">nominal</a>, i.e., consisting of one of a set of unordered items, such as a gender of "male" or "female", or a blood type of "A", "B", "AB" or "O"), <a href="ordinal_data" title="wikilink">ordinal</a> (consisting of one of a set of ordered items, e.g., "large", "medium" or "small"), <a href="integer" title="wikilink">integer-valued</a> (e.g., a count of the number of occurrences of a particular word in an email) or <a href="real_number" title="wikilink">real-valued</a> (e.g., a measurement of blood pressure). Often, categorical and ordinal data are grouped together; likewise for integer-valued and real-valued data. Furthermore, many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be <em>discretized</em> into groups (e.g., less than 5, between 5 and 10, or greater than 10).</p>
<h3 id="probabilistic-classifiers">Probabilistic classifiers</h3>
<p>Many common pattern recognition algorithms are <em>probabilistic</em> in nature, in that they use <a href="statistical_inference" title="wikilink">statistical inference</a> to find the best label for a given instance. Unlike other algorithms, which simply output a "best" label, often probabilistic algorithms also output a <a class="uri" href="probability" title="wikilink">probability</a> of the instance being described by the given label. In addition, many probabilistic algorithms output a list of the <em>N</em>-best labels with associated probabilities, for some value of <em>N</em>, instead of simply a single best label. When the number of possible labels is fairly small (e.g., in the case of <a href="classification_(machine_learning)" title="wikilink">classification</a>), <em>N</em> may be set so that the probability of all possible labels is output. Probabilistic algorithms have many advantages over non-probabilistic algorithms:</p>
<ul>
<li>They output a confidence value associated with their choice. (Note that some other algorithms may also output confidence values, but in general, only for probabilistic algorithms is this value mathematically grounded in <a href="probability_theory" title="wikilink">probability theory</a>. Non-probabilistic confidence values can in general not be given any specific meaning, and only used to compare against other confidence values output by the same algorithm.)</li>
<li>Correspondingly, they can <em>abstain</em> when the confidence of choosing any particular output is too low.</li>
<li>Because of the probabilities output, probabilistic pattern-recognition algorithms can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of <em>error propagation</em>.</li>
</ul>
<h3 id="how-many-feature-variables-are-important">How many feature variables are important?</h3>
<p><a href="Feature_selection" title="wikilink">Feature selection</a> algorithms attempt to directly prune out redundant or irrelevant features. A general introduction to <a href="feature_selection" title="wikilink">feature selection</a> which summarizes approaches and challenges, has been given.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> The complexity of feature-selection is, because of its non-monotonous character, an <a href="optimization_problem" title="wikilink">optimization problem</a> where given a total of <span class="LaTeX">$n$</span> features the <a class="uri" href="powerset" title="wikilink">powerset</a> consisting of all <span class="LaTeX">$2^n-1$</span> subsets of features need to be explored. The <a href="Branch_and_bound" title="wikilink">Branch-and-Bound algorithm</a> <a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> does reduce this complexity but is intractable for medium to large values of the number of available features <span class="LaTeX">$n$</span>. For a large-scale comparison of feature-selection algorithms see .<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<p>Techniques to transform the raw feature vectors (<strong>feature extraction</strong>) are sometimes used prior to application of the pattern-matching algorithm. For example, <a href="feature_extraction" title="wikilink">feature extraction</a> algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as <a href="principal_components_analysis" title="wikilink">principal components analysis</a> (PCA). The distinction between <strong>feature selection</strong> and <strong>feature extraction</strong> is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features.</p>
<h2 id="problem-statement-supervised-version">Problem statement (supervised version)</h2>
<p>Formally, the problem of <a href="supervised_learning" title="wikilink">supervised</a> pattern recognition can be stated as follows: Given an unknown function <span class="LaTeX">$g:\mathcal{X}\rightarrow\mathcal{Y}$</span> (the <em>ground truth</em>) that maps input instances <span class="LaTeX">$\boldsymbol{x} \in \mathcal{X}$</span> to output labels <span class="LaTeX">$y \in \mathcal{Y}$</span>, along with training data <span class="LaTeX">$\mathbf{D} = \{(\boldsymbol{x}_1,y_1),\dots,(\boldsymbol{x}_n, y_n)\}$</span> assumed to represent accurate examples of the mapping, produce a function <span class="LaTeX">$h:\mathcal{X}\rightarrow\mathcal{Y}$</span> that approximates as closely as possible the correct mapping <span class="LaTeX">$g$</span>. (For example, if the problem is filtering spam, then <span class="LaTeX">$\boldsymbol{x}_i$</span> is some representation of an email and <span class="LaTeX">$y$</span> is either "spam" or "non-spam"). In order for this to be a well-defined problem, "approximates as closely as possible" needs to be defined rigorously. In <a href="decision_theory" title="wikilink">decision theory</a>, this is defined by specifying a <a href="loss_function" title="wikilink">loss function</a> that assigns a specific value to "loss" resulting from producing an incorrect label. The goal then is to minimize the <a href="expected_value" title="wikilink">expected</a> loss, with the expectation taken over the <a href="probability_distribution" title="wikilink">probability distribution</a> of <span class="LaTeX">$\mathcal{X}$</span>. In practice, neither the distribution of <span class="LaTeX">$\mathcal{X}$</span> nor the ground truth function <span class="LaTeX">$g:\mathcal{X}\rightarrow\mathcal{Y}$</span> are known exactly, but can be computed only empirically by collecting a large number of samples of <span class="LaTeX">$\mathcal{X}$</span> and hand-labeling them using the correct value of <span class="LaTeX">$\mathcal{Y}$</span> (a time-consuming process, which is typically the limiting factor in the amount of data of this sort that can be collected). The particular loss function depends on the type of label being predicted. For example, in the case of <a href="classification_(machine_learning)" title="wikilink">classification</a>, the simple <a href="zero-one_loss_function" title="wikilink">zero-one loss function</a> is often sufficient. This corresponds simply to assigning a loss of 1 to any incorrect labeling and implies that the optimal classifier minimizes the <a href="Bayes_error_rate" title="wikilink">error rate</a> on independent test data (i.e. counting up the fraction of instances that the learned function <span class="LaTeX">$h:\mathcal{X}\rightarrow\mathcal{Y}$</span> labels wrongly, which is equivalent to maximizing the number of correctly classified instances). The goal of the learning procedure is then to minimize the error rate (maximize the <a class="uri" href="correctness" title="wikilink">correctness</a>) on a "typical" test set.</p>
<p>For a probabilistic pattern recognizer, the problem is instead to estimate the probability of each possible output label given a particular input instance, i.e., to estimate a function of the form</p>
<p><span class="LaTeX">$$p({\rm label}|\boldsymbol{x},\boldsymbol\theta) = f\left(\boldsymbol{x};\boldsymbol{\theta}\right)$$</span> where the <a href="feature_vector" title="wikilink">feature vector</a> input is <span class="LaTeX">$\boldsymbol{x}$</span>, and the function <em>f</em> is typically parameterized by some parameters <span class="LaTeX">$\boldsymbol{\theta}$</span>.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> In a <a href="discriminative_model" title="wikilink">discriminative</a> approach to the problem, <em>f</em> is estimated directly. In a <a href="generative_model" title="wikilink">generative</a> approach, however, the inverse probability <span class="LaTeX">$p({\boldsymbol{x}|\rm label})$</span> is instead estimated and combined with the <a href="prior_probability" title="wikilink">prior probability</a> <span class="LaTeX">$p({\rm label}|\boldsymbol\theta)$</span> using <a href="Bayes'_rule" title="wikilink">Bayes' rule</a>, as follows:</p>
<p><span class="LaTeX">$$p({\rm label}|\boldsymbol{x},\boldsymbol\theta) = \frac{p({\boldsymbol{x}|\rm label,\boldsymbol\theta}) p({\rm label|\boldsymbol\theta})}{\sum_{L \in \text{all labels}} p(\boldsymbol{x}|L) p(L|\boldsymbol\theta)}.$$</span></p>
<p>When the labels are <a href="continuous_distribution" title="wikilink">continuously distributed</a> (e.g., in <a href="regression_analysis" title="wikilink">regression analysis</a>), the denominator involves <a href="integral" title="wikilink">integration</a> rather than summation:</p>
<p><span class="LaTeX">$$p({\rm label}|\boldsymbol{x},\boldsymbol\theta) = \frac{p({\boldsymbol{x}|\rm label,\boldsymbol\theta}) p({\rm label|\boldsymbol\theta})}{\int_{L \in \text{all labels}} p(\boldsymbol{x}|L) p(L|\boldsymbol\theta) \operatorname{d}L}.$$</span></p>
<p>The value of <span class="LaTeX">$\boldsymbol\theta$</span> is typically learned using <a href="maximum_a_posteriori" title="wikilink">maximum a posteriori</a> (MAP) estimation. This finds the best value that simultaneously meets two conflicting objects: To perform as well as possible on the training data (smallest <a href="Bayes_error_rate" title="wikilink">error-rate</a>) and to find the simplest possible model. Essentially, this combines <a href="maximum_likelihood" title="wikilink">maximum likelihood</a> estimation with a <a href="regularization_(mathematics)" title="wikilink">regularization</a> procedure that favors simpler models over more complex models. In a <a href="Bayesian_inference" title="wikilink">Bayesian</a> context, the regularization procedure can be viewed as placing a <a href="prior_probability" title="wikilink">prior probability</a> <span class="LaTeX">$p(\boldsymbol\theta)$</span> on different values of <span class="LaTeX">$\boldsymbol\theta$</span>. Mathematically:</p>
<p><span class="LaTeX">$$\boldsymbol\theta^* = \arg \max_{\boldsymbol\theta} p(\boldsymbol\theta|\mathbf{D})$$</span></p>
<p>where <span class="LaTeX">$\boldsymbol\theta^*$</span> is the value used for <span class="LaTeX">$\boldsymbol\theta$</span> in the subsequent evaluation procedure, and <span class="LaTeX">$p(\boldsymbol\theta|\mathbf{D})$</span>, the <a href="posterior_probability" title="wikilink">posterior probability</a> of <span class="LaTeX">$\boldsymbol\theta$</span>, is given by</p>
<p><span class="LaTeX">$$p(\boldsymbol\theta|\mathbf{D}) = \left[\prod_{i=1}^n p(y_i|\boldsymbol{x}_i,\boldsymbol\theta) \right] p(\boldsymbol\theta).$$</span></p>
<p>In the <a href="Bayesian_statistics" title="wikilink">Bayesian</a> approach to this problem, instead of choosing a single parameter vector <span class="LaTeX">$\boldsymbol{\theta}^*$</span>, the probability of a given label for a new instance <span class="LaTeX">$\boldsymbol{x}$</span> is computed by integrating over all possible values of <span class="LaTeX">$\boldsymbol\theta$</span>, weighted according to the posterior probability:</p>
<p><span class="LaTeX">$$p({\rm label}|\boldsymbol{x}) = \int p({\rm label}|\boldsymbol{x},\boldsymbol\theta)p(\boldsymbol{\theta}|\mathbf{D}) \operatorname{d}\boldsymbol{\theta}.$$</span></p>
<h3 id="frequentist-or-bayesian-approach-to-pattern-recognition">Frequentist or Bayesian approach to pattern recognition?</h3>
<p>The first pattern classifier – the linear discriminant presented by <a href="Fisher_discriminant_analysis" title="wikilink">Fisher</a> – was developed in the <a href="Frequentist_inference" title="wikilink">Frequentist</a> tradition. The frequentist approach entails that the model parameters are considered unknown, but objective. The parameters are then computed (estimated) from the collected data. For the linear discriminant, these parameters are precisely the mean vectors and the <a href="Covariance_matrix" title="wikilink">Covariance matrix</a>. Also the probability of each class <span class="LaTeX">$p({\rm label}|\boldsymbol\theta)$</span> is estimated from the collected dataset. Note that the usage of ‘<a href="Bayes_rule" title="wikilink">Bayes rule</a>’ in a pattern classifier does not make the classification approach Bayesian.</p>
<p><a href="Bayesian_inference" title="wikilink">Bayesian statistics</a> has its origin in Greek philosophy where a distinction was already made between the ‘<a href="A_priori_and_a_posteriori" title="wikilink">a priori</a>’ and the ‘<a href="A_priori_and_a_posteriori" title="wikilink">a posteriori</a>’ knowledge. Later <a href="A_priori_and_a_posteriori#Immanuel_Kant" title="wikilink">Kant</a> defined his distinction between what is a priori known – before observation – and the empirical knowledge gained from observations. In a Bayesian pattern classifier, the class probabilities <span class="LaTeX">$p({\rm label}|\boldsymbol\theta)$</span> can be chosen by the user, which are then a priori. Moreover, experience quantified as a priori parameter values can be weighted with empirical observations – using e.g., the <a href="Beta_distribution" title="wikilink">Beta-</a> (<a href="Conjugate_prior_distribution" title="wikilink">conjugate prior</a>) and <a href="Dirichlet_distribution" title="wikilink">Dirichlet-distributions</a>. The Bayesian approach facilitates a seamless intermixing between expert knowledge in the form of subjective probabilities, and objective observations.</p>
<p>Probabilistic pattern classifiers can be used according to a frequentist or a Bayesian approach.</p>
<h2 id="uses">Uses</h2>
<p> Within medical science, pattern recognition is the basis for <a href="computer-aided_diagnosis" title="wikilink">computer-aided diagnosis</a> (CAD) systems. CAD describes a procedure that supports the doctor's interpretations and findings.  Other typical applications of pattern recognition techniques are automatic <a href="speech_recognition" title="wikilink">speech recognition</a>, <a href="document_classification" title="wikilink">classification of text into several categories</a> (e.g., spam/non-spam email messages), the <a href="handwriting_recognition" title="wikilink">automatic recognition of handwritten postal codes</a> on postal envelopes, automatic recognition of images of human faces, or handwriting image extraction from medical forms.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> The last two examples form the subtopic <a href="image_analysis" title="wikilink">image analysis</a> of pattern recognition that deals with digital images as input to pattern recognition systems.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<p>Optical character recognition is a classic example of the application of a pattern classifier, see <a href="http://cmp.felk.cvut.cz/cmp/software/stprtool/examples/ocr_system.gif">OCR-example</a>. The method of signing one's name was captured with stylus and overlay starting in 1990. The strokes, speed, relative min, relative max, acceleration and pressure is used to uniquely identify and confirm identity. Banks were first offered this technology, but were content to collect from the FDIC for any bank fraud and did not want to inconvenience customers..</p>
<p><a href="Artificial_neural_networks" title="wikilink">Artificial neural networks</a> (neural net classifiers) and <a href="Deep_Learning" title="wikilink">Deep Learning</a> have many real-world applications in image processing, a few examples:</p>
<ul>
<li>identification and authentication: e.g., license plate recognition,<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> fingerprint analysis and face detection/verification;<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></li>
<li>medical diagnosis: e.g., screening for cervical cancer (Papnet)<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> or breast tumors;</li>
<li>defence: various navigation and guidance systems, target recognition systems, shape recognition technology etc.</li>
</ul>
<p>For a discussion of the aforementioned applications of neural networks in image processing, see e.g.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></p>
<p>In psychology, pattern recognition, making sense of and identifying the objects we see is closely related to perception, which explains how the sensory inputs we receive are made meaningful. Pattern recognition can be thought of in two different ways: the first being template matching and the second being feature detection. A template is a pattern used to produce items of the same proportions. The template-matching hypothesis suggests that incoming stimuli are compared with templates in the long term memory. If there is a match, the stimulus is identified. Feature detection models, such as the Pandemonium system for classifying letters (Selfridge, 1959), suggest that the stimuli are broken down into their component parts for identification. For example, a capital E has three horizontal lines and one vertical line.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></p>
<h2 id="algorithms">Algorithms</h2>
<p>Algorithms for pattern recognition depend on the type of label output, on whether learning is supervised or unsupervised, and on whether the algorithm is statistical or non-statistical in nature. Statistical algorithms can further be categorized as <a href="generative_model" title="wikilink">generative</a> or <a href="discriminative_model" title="wikilink">discriminative</a>.</p>
<h3 id="classification-algorithms-supervised-algorithms-predicting-categorical-labels"><a href="Classification_(machine_learning)" title="wikilink">Classification</a> algorithms (<a href="supervised_learning" title="wikilink">supervised</a> algorithms predicting <a href="categorical_data" title="wikilink">categorical</a> labels)</h3>
<p>Parametric:<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>
<ul>
<li><a href="Linear_discriminant_analysis" title="wikilink">Linear discriminant analysis</a></li>
<li><a href="Quadratic_classifier" title="wikilink">Quadratic discriminant analysis</a></li>
<li><a href="Maximum_entropy_classifier" title="wikilink">Maximum entropy classifier</a> (aka <a href="logistic_regression" title="wikilink">logistic regression</a>, <a href="multinomial_logistic_regression" title="wikilink">multinomial logistic regression</a>): Note that logistic regression is an algorithm for classification, despite its name. (The name comes from the fact that logistic regression uses an extension of a linear regression model to model the probability of an input being in a particular class.)</li>
</ul>
<p>Nonparametric:<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a></p>
<ul>
<li><a href="Decision_tree" title="wikilink">Decision trees</a>, <a href="decision_list" title="wikilink">decision lists</a></li>
<li><a href="Variable_kernel_density_estimation#Use_for_statistical_classification" title="wikilink">Kernel estimation</a> and <a class="uri" href="K-nearest-neighbor" title="wikilink">K-nearest-neighbor</a> algorithms</li>
<li><a href="Naive_Bayes_classifier" title="wikilink">Naive Bayes classifier</a></li>
<li><a href="Neural_network" title="wikilink">Neural networks</a> (multi-layer perceptrons)</li>
<li><a href="Perceptron" title="wikilink">Perceptrons</a></li>
<li><a href="Support_vector_machine" title="wikilink">Support vector machines</a></li>
<li><a href="Gene_expression_programming" title="wikilink">Gene expression programming</a></li>
</ul>
<h3 id="clustering-algorithms-unsupervised-algorithms-predicting-categorical-labels"><a href="Cluster_analysis" title="wikilink">Clustering</a> algorithms (<a href="unsupervised_learning" title="wikilink">unsupervised</a> algorithms predicting <a href="categorical_data" title="wikilink">categorical</a> labels)</h3>
<ul>
<li>Categorical <a href="mixture_model" title="wikilink">mixture models</a></li>
<li><a href="Deep_learning" title="wikilink">Deep learning methods</a></li>
<li><a href="Hierarchical_clustering" title="wikilink">Hierarchical clustering</a> (agglomerative or divisive)</li>
<li><a href="K-means_clustering" title="wikilink">K-means clustering</a></li>
<li><a href="Correlation_clustering" title="wikilink">Correlation clustering</a></li>
<li><a href="Kernel_principal_component_analysis" title="wikilink">Kernel principal component analysis</a> (Kernel PCA)</li>
</ul>
<h3 id="ensemble-learning-algorithms-supervised-meta-algorithms-for-combining-multiple-learning-algorithms-together"><a href="Ensemble_learning" title="wikilink">Ensemble learning</a> algorithms (supervised <a href="meta-algorithm" title="wikilink">meta-algorithms</a> for combining multiple learning algorithms together)</h3>
<ul>
<li><a href="Boosting_(meta-algorithm)" title="wikilink">Boosting (meta-algorithm)</a></li>
<li><a href="Bootstrap_aggregating" title="wikilink">Bootstrap aggregating</a> ("bagging")</li>
<li><a href="Ensemble_averaging" title="wikilink">Ensemble averaging</a></li>
<li><a href="Mixture_of_experts" title="wikilink">Mixture of experts</a>, <a href="hierarchical_mixture_of_experts" title="wikilink">hierarchical mixture of experts</a></li>
</ul>
<h3 id="general-algorithms-for-predicting-arbitrarily-structured-sets-of-labels">General algorithms for predicting arbitrarily-structured (sets of) labels</h3>
<ul>
<li><a href="Bayesian_network" title="wikilink">Bayesian networks</a></li>
<li><a href="Markov_random_field" title="wikilink">Markov random fields</a></li>
</ul>
<h3 id="multilinear-subspace-learning-algorithms-predicting-labels-of-multidimensional-data-using-tensor-representations"><a href="Multilinear_subspace_learning" title="wikilink">Multilinear subspace learning</a> algorithms (predicting labels of multidimensional data using <a class="uri" href="tensor" title="wikilink">tensor</a> representations)</h3>
<p>Unsupervised:</p>
<ul>
<li><a href="Multilinear_principal_component_analysis" title="wikilink">Multilinear principal component analysis</a> (MPCA)</li>
</ul>
<h3 id="real-valued-sequence-labeling-algorithms-predicting-sequences-of-real-valued-labels">Real-valued <a href="sequence_labeling" title="wikilink">sequence labeling</a> algorithms (predicting sequences of <a href="real_number" title="wikilink">real-valued</a> labels)</h3>
<p>Supervised (?):</p>
<ul>
<li><a href="Kalman_filter" title="wikilink">Kalman filters</a></li>
<li><a href="Particle_filter" title="wikilink">Particle filters</a></li>
</ul>
<h3 id="regression-algorithms-predicting-real-valued-labels"><a href="Regression_analysis" title="wikilink">Regression</a> algorithms (predicting <a href="real_number" title="wikilink">real-valued</a> labels)</h3>
<p>Supervised:</p>
<ul>
<li><a href="Gaussian_process_regression" title="wikilink">Gaussian process regression</a> (kriging)</li>
<li><a href="Linear_regression" title="wikilink">Linear regression</a> and extensions</li>
<li><a href="Neural_network" title="wikilink">Neural networks</a> and <a href="Deep_learning" title="wikilink">Deep learning methods</a></li>
</ul>
<p>Unsupervised:</p>
<ul>
<li><a href="Independent_component_analysis" title="wikilink">Independent component analysis</a> (ICA)</li>
<li><a href="Principal_components_analysis" title="wikilink">Principal components analysis</a> (PCA)</li>
</ul>
<h3 id="sequence-labeling-algorithms-predicting-sequences-of-categorical-labels"><a href="Sequence_labeling" title="wikilink">Sequence labeling</a> algorithms (predicting sequences of <a href="categorical_data" title="wikilink">categorical</a> labels)</h3>
<p>Supervised:</p>
<ul>
<li><a href="Conditional_random_field" title="wikilink">Conditional random fields</a> (CRFs)</li>
<li><a href="Hidden_Markov_model" title="wikilink">Hidden Markov models</a> (HMMs)</li>
<li><a href="Maximum_entropy_Markov_model" title="wikilink">Maximum entropy Markov models</a> (MEMMs)</li>
<li><a href="Recurrent_neural_networks" title="wikilink">Recurrent neural networks</a></li>
</ul>
<p>Unsupervised:</p>
<ul>
<li><a href="Hidden_Markov_model" title="wikilink">Hidden Markov models</a> (HMMs)</li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Adaptive_resonance_theory" title="wikilink">Adaptive resonance theory</a></li>
<li><a href="Cache_language_model" title="wikilink">Cache language model</a></li>
<li><a href="Compound_term_processing" title="wikilink">Compound term processing</a></li>
<li><a href="Computer-aided_diagnosis" title="wikilink">Computer-aided diagnosis</a></li>
<li><a href="Data_mining" title="wikilink">Data mining</a></li>
<li><a href="Deep_Learning" title="wikilink">Deep Learning</a></li>
<li><a href="List_of_numerical_analysis_software" title="wikilink">List of numerical analysis software</a></li>
<li><a href="List_of_numerical_libraries" title="wikilink">List of numerical libraries</a></li>
<li><a href="Machine_learning" title="wikilink">Machine learning</a></li>
<li><a href="Multilinear_subspace_learning" title="wikilink">Multilinear subspace learning</a></li>
<li><a class="uri" href="Neocognitron" title="wikilink">Neocognitron</a></li>
<li><a class="uri" href="Perception" title="wikilink">Perception</a></li>
<li><a href="Perceptual_learning" title="wikilink">Perceptual learning</a></li>
<li><a href="Predictive_analytics" title="wikilink">Predictive analytics</a></li>
<li><a href="Prior_knowledge_for_pattern_recognition" title="wikilink">Prior knowledge for pattern recognition</a></li>
<li><a href="Sequence_mining" title="wikilink">Sequence mining</a></li>
<li><a href="Template_matching" title="wikilink">Template matching</a></li>
<li><a href="Contextual_image_classification" title="wikilink">Contextual image classification</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="further-reading">Further reading</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li><a href="http://www.egmont-petersen.nl/classifiers.htm">An introductory tutorial to classifiers (introducing the basic terms, with numeric example)</a></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.iapr.org">The International Association for Pattern Recognition</a></li>
<li><a href="http://cgm.cs.mcgill.ca/~godfried/teaching/pr-web.html">List of Pattern Recognition web sites</a></li>
<li><a href="http://www.jprr.org">Journal of Pattern Recognition Research</a></li>
<li><a href="http://www.docentes.unal.edu.co/morozcoa/docs/pr.php">Pattern Recognition Info</a></li>
<li><a href="http://www.sciencedirect.com/science/journal/00313203">Pattern Recognition</a> (Journal of the Pattern Recognition Society)</li>
<li><a href="http://www.worldscinet.com/ijprai/mkt/archive.shtml">International Journal of Pattern Recognition and Artificial Intelligence</a></li>
<li><a href="http://www.inderscience.com/ijapr">International Journal of Applied Pattern Recognition</a></li>
<li><a href="http://www.openpr.org.cn/">Open Pattern Recognition Project</a>, intended to be an open source platform for sharing algorithms of pattern recognition</li>
</ul>
<p>"</p>
<p><a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a> <a href="Category:Formal_sciences" title="wikilink">Category:Formal sciences</a> <a href="Category:Pattern_recognition" title="wikilink">Category:Pattern recognition</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2">.<a href="#fnref2">↩</a></li>
<li id="fn3">Isabelle Guyon Clopinet, André Elisseeff (2003). <em>An Introduction to Variable and Feature Selection</em>. The Journal of Machine Learning Research, Vol. 3, 1157-1182. <a href="http://www-vis.lbl.gov/~romano/mlgroup/papers/guyon03a.pdf">Link</a><a href="#fnref3">↩</a></li>
<li id="fn4">.<a href="#fnref4">↩</a></li>
<li id="fn5">.<a href="#fnref5">↩</a></li>
<li id="fn6">For <a href="linear_discriminant_analysis" title="wikilink">linear discriminant analysis</a> the parameter vector <span class="LaTeX">$\boldsymbol\theta$</span> consists of the two mean vectors <span class="LaTeX">$\boldsymbol\mu_1$</span> and <span class="LaTeX">$\boldsymbol\mu_2$</span> and the common <a href="covariance_matrix" title="wikilink">covariance matrix</a> <span class="LaTeX">$\boldsymbol\Sigma$</span>.<a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8">Richard O. Duda, <a href="Peter_E._Hart" title="wikilink">Peter E. Hart</a>, David G. Stork (2001) <em>Pattern classification</em> (2nd edition), Wiley, New York, ISBN 0-471-05669-3<a href="#fnref8">↩</a></li>
<li id="fn9">R. Brunelli, <em>Template Matching Techniques in Computer Vision: Theory and Practice</em>, Wiley, ISBN 978-0-470-51706-2, 2009<a href="#fnref9">↩</a></li>
<li id="fn10"><a href="http://anpr-tutorial.com/">THE AUTOMATIC NUMBER PLATE RECOGNITION TUTORIAL</a> <a class="uri" href="http://anpr-tutorial.com/">http://anpr-tutorial.com/</a><a href="#fnref10">↩</a></li>
<li id="fn11"><a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/usr/mitchell/ftp/faces.html">Neural Networks for Face Recognition</a> Companion to Chapter 4 of the textbook Machine Learning.<a href="#fnref11">↩</a></li>
<li id="fn12"><a href="http://health-asia.org/papnet-for-cervical-screening/">PAPNET For Cervical Screening</a> <a class="uri" href="http://health-asia.org/papnet-for-cervical-screening/">http://health-asia.org/papnet-for-cervical-screening/</a><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"><a href="#fnref14">↩</a></li>
<li id="fn15">Assuming known distributional shape of feature distributions per class, such as the <a href="Gaussian_distribution" title="wikilink">Gaussian</a> shape.<a href="#fnref15">↩</a></li>
<li id="fn16">No distributional assumption regarding shape of feature distributions per class.<a href="#fnref16">↩</a></li>
</ol>
</section>
</body>
</html>
