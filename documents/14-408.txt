   Basis pursuit denoising      Basis pursuit denoising   In mathematics and machine learning , basis pursuit denoising (BPDN) is one approach to solving a mathematical optimization problem of the form:          min  x    1  2      ∥   y  -   A  x    ∥   2  2    +   λ    ∥  x  ∥   1     .          subscript   x     1  2     subscript   superscript   norm    y    A  x     2   2      λ   subscript   norm  x   1      \min_{x}\frac{1}{2}\|y-Ax\|^{2}_{2}+\lambda\|x\|_{1}.     where   λ   λ   \lambda   is a parameter that controls the trade-off between sparsity and reconstruction fidelity,   x   x   x   is an    N  ×  1      N  1    N\times 1   solution vector,   y   y   y   is an    M  ×  1      M  1    M\times 1   vector of observations,   A   A   A   is an    M  ×  N      M  N    M\times N   transform matrix and    M  <  N      M  N    M   . This is an instance of convex optimization and also of quadratic programming .  Some authors refer to basis pursuit denoising as the following closely related problem:        min  x      ∥  x  ∥   1     subject    to     ∥   y  -   A  x    ∥   2  2    ≤  δ         subscript   x    subscript   norm  x   1   subject  to   subscript   superscript   norm    y    A  x     2   2    δ    \min_{x}\|x\|_{1}\;\textrm{subject}\ \textrm{to}\;\;\|y-Ax\|^{2}_{2}\leq\delta   which, for any given   λ   λ   \lambda   , is equivalent to the unconstrained formulation for some (usually unknown a priori ) value of   δ   δ   \delta   . The two problems are quite similar, but most specialized numerical algorithms can only solve the unconstrained formulation.  Either type of basis pursuit denoising solves a regularization problem with a trade-off between having a small residual (making   y   y   y   close to    A  x      A  x    Ax   in terms of an    L  2     subscript  L  2    L_{2}   distance) and making   x   x   x   simple in the    L  1     subscript  L  1    L_{1}   sense. It can be thought of as a mathematical statement of Occam's razor , finding the simplest possible explanation (     min  x     ∥  x  ∥   1        subscript   x    subscript   norm  x   1     \min_{x}\|x\|_{1}   ) capable of accounting for the observations (     min  x     ∥   y  -   A  x    ∥   2  2        subscript   x    subscript   superscript   norm    y    A  x     2   2     \min_{x}\|y-Ax\|^{2}_{2}   ).  Exact solutions to basis pursuit denoising are often the best computationally tractable approximation of an underdetermined system of equations. Basis pursuit denoising has potential applications in statistics (c.f. the LASSO method of regularization ), image compression and compressed sensing .  As    λ  →  ∞     normal-→  λ     \lambda\rightarrow\infty   (or when    δ  =  0      δ  0    \delta=0   ), this problem becomes basis pursuit .  Solving basis pursuit denoising  The problem is a convex quadratic problem, so it can be solved by many general solvers, such as interior point methods . For very large problems, many specialized methods that are faster than interior point methods have been proposed.  Several popular methods for solving basis pursuit denoising include the in-crowd algorithm (a fast solver for large, sparse problems 1 ), homotopy continuation , fixed-point continuation (a special case of the forward backward algorithm ) and spectral projected gradient for L1 minimization (which actually solves LASSO , a related problem).  References  External links   A list of BPDN solvers at the sparse- and low-rank approximation wiki .   "  Category:Mathematical optimization     See The In-Crowd Algorithm for Fast Basis Pursuit Denoising , IEEE Trans Sig Proc 59 (10), Oct 1 2011, pp. 4595 - 4605, 1 , demo MATLAB code available 2 ↩     