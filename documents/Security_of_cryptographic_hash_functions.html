<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1440">Security of cryptographic hash functions</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Security of cryptographic hash functions</h1>
<hr/>

<p>In <a class="uri" href="cryptography" title="wikilink">cryptography</a>, <a href="cryptographic_hash_functions" title="wikilink">cryptographic hash functions</a> can be divided into two main categories. In the first category are those functions whose designs are based on a mathematical problem and thus their security follows from rigorous mathematical proofs, <a href="Computational_complexity_theory" title="wikilink">complexity theory</a> and <a href="Reduction_(complexity)" title="wikilink">formal reduction</a>. These functions are called Provably Secure Cryptographic Hash Functions. However this does not mean that such a function could not be broken. To construct them is very difficult and only a few examples were introduced. The practical use is limited.</p>

<p>In the second category are functions that are not based on mathematical problems but on an ad hoc basis, where the bits of the message are mixed to produce the hash. They are then believed to be hard to break, but no such formal proof is given. Almost all widely spread hash functions fall in this category. Some of these functions are already broken and are no longer in use.</p>
<h2 id="types-of-security-of-hash-functions">Types of security of hash functions</h2>

<p>Generally, the <em>basic</em> security of <a href="cryptographic_hash_functions" title="wikilink">cryptographic hash functions</a> can be seen from three different angles: pre-image resistance, second pre-image resistance and collision resistance.</p>
<ul>
<li><strong>Pre-image resistance</strong>: given a hash h it should be <em>hard</em> to find any message m such that h = hash(m). This concept is related to that of one way function. Functions that lack this property are vulnerable to <a href="pre-image_attack" title="wikilink">pre-image attacks</a>.</li>
<li><strong>Second pre-image resistance</strong>: given an input m1, it should be <em>hard</em> to find another input, m2 (not equal to m1) such that hash(m1) = hash(m2). This property is sometimes referred to as weak collision resistance. Functions that lack this property are vulnerable to <a href="second_pre-image_attack" title="wikilink">second pre-image attacks</a>.</li>
<li><strong>Collision resistance</strong>: it should be <em>hard</em> to find two different messages m1 and m2 such that hash(m1) = hash(m2). Such a pair is called a (cryptographic) hash collision. This property is sometimes referred to as strong collision resistance. It requires a hash value at least twice as long as what is required for pre-image resistance, otherwise collisions may be found by a <a href="birthday_attack" title="wikilink">birthday attack</a>.</li>
</ul>
<h3 id="the-meaning-of-hard">The meaning of "hard"</h3>

<p>The basic question is the meaning of "<strong>hard</strong>". There are two approaches to answer this question. First is the intuitive/practical approach: "hard means that it is almost certainly beyond the reach of any adversary who must be prevented from breaking the system for as long as the security of the system is deemed important."</p>

<p>The second approach is theoretical and is based on the <a href="computational_complexity_theory" title="wikilink">computational complexity theory</a>. If problem A is hard, there exists a formal <a href="Reduction_(complexity)" title="wikilink">security reduction</a> from a problem which is widely considered unsolvable in <a href="polynomial_time" title="wikilink">polynomial time</a>, such as <a href="integer_factorization" title="wikilink">integer factorization</a> problem or <a href="discrete_logarithm" title="wikilink">discrete logarithm</a> problem.</p>

<p>However, non-existence of a polynomial time algorithm does not automatically ensure that the system is secure. The difficulty of a problem also depends on its size. For example, <a href="RSA_public_key_cryptography" title="wikilink">RSA public key cryptography</a> relies on the difficulty of <a href="integer_factorization" title="wikilink">integer factorization</a>. However, it is considered secure only with keys that are at least 1024 bits large.</p>
<h2 id="cryptographic-hash-functions">Cryptographic hash functions</h2>

<p>Most hash functions are built on an ad hoc basis, where the bits of the message are nicely mixed to produce the hash. Various <a href="bitwise_operation" title="wikilink">bitwise operations</a> (e.g. rotations), <a href="Modular_arithmetic" title="wikilink">modular additions</a> and <a href="One-way_compression_function" title="wikilink">compression functions</a> are used in iterative mode to ensure high complexity and pseudo-randomness of the output. In this way, the security is very hard to prove and the proof is usually not done. Only a few years ago, one of the most popular hash functions, <a class="uri" href="SHA-1" title="wikilink">SHA-1</a>, was shown to be less secure than its length suggested: collisions could be found in only 2<sup>51</sup><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> tests, rather than the brute-force number of 2<sup>80</sup>.</p>

<p>In other words, most of the hash functions in use nowadays are not provably collision-resistant. These hashes are not based on purely mathematical functions. This approach results generally in more effective hashing functions, but with the risk that a weakness of such a function will be eventually used to find collisions. The famous case is <a class="uri" href="MD5" title="wikilink">MD5</a>.</p>

<p>Meaning of "hard to find collision" in these cases means "almost certainly beyond the reach of any adversary who must be prevented from breaking the system for as long as the security of the system is deemed important." The meaning of the term is therefore somewhat dependent on the application, since the effort that a malicious agent may put into the task is usually proportional to his expected gain.</p>
<h2 id="provably-secure-hash-functions">Provably secure hash functions</h2>

<p>In this approach, we base the security of hash function on some hard mathematical problem and we prove that finding collisions of the hash functions is as hard as breaking the underlying problem. This gives much stronger security than just relying on complex mixing of bits as in the classical approach.</p>

<p>A cryptographic hash function has <strong>provable security against collision attacks</strong> if finding collisions is provably <a href="Polynomial-time_reduction" title="wikilink">polynomial-time reducible</a> from problem P which is supposed to be unsolvable in polynomial time. The function is then called provably secure, or just provable.</p>

<p>It means that if finding collisions would be feasible in polynomial time by algorithm A, we could find and use polynomial time algorithm R (reduction algorithm) that would use algorithm A to solve problem P, which is widely supposed to be unsolvable in polynomial time. That is a contradiction. This means, that finding collisions cannot be easier than solving P.</p>

<p>Hash functions with the proof of their security are based on mathematical functions.</p>
<h3 id="hard-problems">Hard problems</h3>

<p>Examples of problems, that are assumed to be not solvable in polynomial time</p>
<ul>
<li><a href="Discrete_logarithm" title="wikilink">Discrete Logarithm Problem</a></li>
<li><a href="Quadratic_residue" title="wikilink">Finding Modular Square Roots</a></li>
<li><a href="Integer_factorization" title="wikilink">Integer Factorization Problem</a></li>
<li><a href="Subset_sum_problem" title="wikilink">Subset Sum Problem</a></li>
</ul>
<h3 id="downsides-of-provable-approach">Downsides of provable approach</h3>
<ul>
<li>Current collision-resistant hash algorithms that have provable security <a href="Reduction_(complexity)" title="wikilink">reductions</a> are too inefficient to be used in practice. In comparison to classical hash functions, they tend to be relatively slow and do not always meet all of criteria traditionally expected of cryptographic hashes. <a href="Very_smooth_hash" title="wikilink">Very smooth hash</a> is an example.</li>
<li>Constructing a hash function with provable security is much more difficult than using a classical approach where we just hope that the complex mixing of bits in the hashing algorithm is strong enough to prevent adversary from finding collisions.</li>
<li>The proof is often a reduction to a problem with asymptotically hard <a href="Worst-case_complexity" title="wikilink">worst-case</a> or <a href="average-case_complexity" title="wikilink">average-case complexity</a>. Worst-case measures the difficulty of solving pathological cases rather than typical cases of the underlying problem. Even a reduction to a problem with hard average complexity offers only limited security as there still can be an algorithm that easily solves the problem for a subset of the problem space. For example, early versions of <a href="Fast_Syndrome_Based_Hash" title="wikilink">Fast Syndrome Based Hash</a> turned out to be insecure. This problem was solved in the latest version.</li>
</ul>

<p><a class="uri" href="SWIFFT" title="wikilink">SWIFFT</a> is an example of a hash function that circumvents these security problems. It can be shown that for any algorithm that can break SWIFFT with probability P within an estimated time T, we can find an algorithm that solves the <em>worst-case</em> scenario of a certain difficult mathematical problem within time T' depending on T and P.</p>
<h3 id="example-of-impractical-provably-secure-hash-function">Example of (impractical) Provably Secure Hash Function</h3>

<p>Hash(<em>m</em>) = <em>x</em><sup><em>m</em></sup> mod <em>n</em> where <em>n</em> is hard to factor composite number, and <em>x</em> is some prespecified base value. A collision <em>x</em><sup><em>m1</em></sup> congruent to <em>x</em><sup><em>m2</em></sup> reveals a multiple <em>m1 - m2</em> of the order of <em>x</em>. Such information can be used to factor <em>n</em> in polynomial time assuming certain properties of <em>x</em>.</p>

<p>But the algorithm is quite inefficient because it requires on average 1.5 multiplications modulo <em>n</em> per message-bit.</p>
<h3 id="more-practical-provably-secure-hash-functions">More practical provably secure hash functions</h3>
<ul>
<li><a href="Very_smooth_hash" title="wikilink">VSH - Very Smooth Hash function</a> - a provably secure collision-resistant hash function assuming the hardness of finding nontrivial modular square roots modulo composite number 

<math display="inline" id="Security_of_cryptographic_hash_functions:0">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 (this is proven to be as hard as <a href="Integer_factorization" title="wikilink">factoring</a> 

<math display="inline" id="Security_of_cryptographic_hash_functions:1">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

).</li>
<li><a class="uri" href="MuHASH" title="wikilink">MuHASH</a></li>
<li><a href="Elliptic_curve_only_hash" title="wikilink">ECOH - Elliptic Curve Only hash function</a> - based on the concept of Elliptic curves, Subset Sum Problem and summation of polynomials. The security proof of the collision resistance was based on weakened assumptions and eventually a second pre-image attack was found.</li>
<li><a href="Fast_Syndrome_Based_Hash" title="wikilink">FSB - Fast Syndrome-Based hash function</a> - it can be proven that breaking FSB is at least as difficult as solving a certain NP-complete problem known as Regular Syndrome Decoding.</li>
<li><a class="uri" href="SWIFFT" title="wikilink">SWIFFT</a> - SWIFFT is based on the <a href="Fast_Fourier_transform" title="wikilink">Fast Fourier transform</a> and is provably collision resistant, under a relatively mild assumption about the worst-case difficulty of finding short vectors in cyclic/<a href="Ideal_lattice_cryptography" title="wikilink">ideal lattices</a>.</li>
<li><a href="Chaum,_van_Heijst,_Pfitzmann_hash_function" title="wikilink">Chaum, van Heijst, Pfitzmann hash function</a> - A compression function where finding collisions is as hard as solving the <a href="discrete_logarithm" title="wikilink">discrete logarithm problem</a> in a finite group 

<math display="inline" id="Security_of_cryptographic_hash_functions:2">
 <semantics>
  <msub>
   <mi>F</mi>
   <mrow>
    <mrow>
     <mn>2</mn>
     <mi>p</mi>
    </mrow>
    <mo>+</mo>
    <mn>1</mn>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>F</ci>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <cn type="integer">2</cn>
      <ci>p</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F_{2p+1}
  </annotation>
 </semantics>
</math>

.</li>
<li><a href="Knapsack-based_hash_functions" title="wikilink">Knapsack-based hash functions</a> - A family of hash functions based on the <a href="Knapsack_problem" title="wikilink">Knapsack problem</a>.</li>
<li><a href="The_Zémor-Tillich_hash_function" title="wikilink">The Zémor-Tillich hash function</a> - A family of hash functions that rely on the arithmetic of the group of matrices <a href="Special_linear_group" title="wikilink">SL2</a>. Finding collisions is at least as difficult as finding factorization of certain elements in this group. This is supposed to be hard, at least <a class="uri" href="PSPACE-complete" title="wikilink">PSPACE-complete</a>. For this hash, an attack was eventually discovered with a time complexity close to 

<math display="inline" id="Security_of_cryptographic_hash_functions:3">
 <semantics>
  <msup>
   <mn>2</mn>
   <mrow>
    <mi>n</mi>
    <mo>/</mo>
    <mn>2</mn>
   </mrow>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <cn type="integer">2</cn>
    <apply>
     <divide></divide>
     <ci>n</ci>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   2^{n/2}
  </annotation>
 </semantics>
</math>

. This beat by far the birthday bound and ideal pre-image complexities which are 

<math display="inline" id="Security_of_cryptographic_hash_functions:4">
 <semantics>
  <msup>
   <mn>2</mn>
   <mrow>
    <mrow>
     <mn>3</mn>
     <mi>n</mi>
    </mrow>
    <mo>/</mo>
    <mn>2</mn>
   </mrow>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <cn type="integer">2</cn>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <cn type="integer">3</cn>
      <ci>n</ci>
     </apply>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   2^{3n/2}
  </annotation>
 </semantics>
</math>


 and 

<math display="inline" id="Security_of_cryptographic_hash_functions:5">
 <semantics>
  <msup>
   <mn>2</mn>
   <mrow>
    <mn>3</mn>
    <mi>n</mi>
   </mrow>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <cn type="integer">2</cn>
    <apply>
     <times></times>
     <cn type="integer">3</cn>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   2^{3n}
  </annotation>
 </semantics>
</math>

 for the Zémor-Tillich hash function. As the attacks include a birthday search in a reduced set of size 

<math display="inline" id="Security_of_cryptographic_hash_functions:6">
 <semantics>
  <mrow>
   <mn>2</mn>
   <mi>n</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <cn type="integer">2</cn>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   2n
  </annotation>
 </semantics>
</math>

 they indeed do not destroy the idea of provable security of invalidate the scheme but rather suggest that the initial parameters were too small.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></li>
<li><a href="Hash_functions_from_Sigma_Protocols" title="wikilink">Hash functions from Sigma Protocols</a> - there exists a general way of constructing a provably secure hash, specifically from any (suitable) <a href="Proof_of_knowledge#Sigma_protocols" title="wikilink">sigma protocol</a>. A faster version of <a href="Very_smooth_hash" title="wikilink">VSH</a> (called VSH*) could be obtained in this way.</li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Cryptographic_hash_functions" title="wikilink">Category:Cryptographic hash functions</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a class="uri" href="http://eprint.iacr.org/2008/469.pdf">http://eprint.iacr.org/2008/469.pdf</a><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
</ol>
</section>
</body>
</html>
