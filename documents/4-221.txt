


Matrix exponential




Matrix exponential

In mathematics, the matrix exponential is a matrix function on square matrices analogous to the ordinary exponential function. Abstractly, the matrix exponential gives the connection between a matrix Lie algebra and the corresponding Lie group.
Let $X$ be an $n × n$ real or complex matrix. The exponential of $X$, denoted by  or $exp( X )$, is the $n × n$ matrix given by the power series
$$e^X = \sum_{k=0}^\infty{1 \over k!}X^k.$$
The above series always converges, so the exponential of $X$ is well-defined. If $X$ is a 1×1 matrix the matrix exponential of $X$ is a 1×1 matrix whose single element is the ordinary exponential of the single element of $X$.
Properties
Let $X$ and $Y$ be $n × n$ complex matrices and let $a$ and $b$ be arbitrary complex numbers. We denote the $n × n$ identity matrix by $I$ and the zero matrix by 0. The matrix exponential satisfies the following properties:

 I}}
 e(a + b)X}}
 I}}
If $XY = YX$ then  eYeX {{=}} e(X + Y).}}
If $Y$ is invertible then  YeXY−1.}}
 (exp X)T}}, where  denotes the transpose of $X$. It follows that if $X$ is symmetric then  is also symmetric, and that if $X$ is skew-symmetric then  is orthogonal.
 (exp X)∗}}, where  denotes the conjugate transpose of $X$. It follows that if $X$ is Hermitian then  is also Hermitian, and that if $X$ is skew-Hermitian then  is unitary.
A Laplace transform of matrix exponentials amounts to the resolvent,  I / (I−X)}}.

Linear differential equation systems
One of the reasons for the importance of the matrix exponential is that it can be used to solve systems of linear ordinary differential equations. The solution of
$$\frac{d}{dt} y(t) = Ay(t), \quad y(0) = y_0,$$ where $A$ is a constant matrix, is given by
$$y(t) = e^{At} y_0. \,$$ The matrix exponential can also be used to solve the inhomogeneous equation
$$\frac{d}{dt} y(t) = Ay(t) + z(t), \quad y(0) = y_0.$$ See the section on applications below for examples.
There is no closed-form solution for differential equations of the form
$$\frac{d}{dt} y(t) = A(t) \, y(t), \quad y(0) = y_0,$$ where $A$ is not constant, but the Magnus series gives the solution as an infinite sum.
The exponential of sums
For any real numbers (scalars) $x$ and $y$ we know that the exponential function satisfies  ex ey}}. The same is true for commuting matrices. If matrices $X$ and $Y$ commute (meaning that $XY = YX$), then
$$e^{X+Y} = e^Xe^Y ~.$$
However, for matrices that do not commute the above equality does not necessarily hold. In this case the Baker–Campbell–Hausdorff formula can be used to calculate .
The converse is not true in general. The equation  eX eY}} does not imply that $X$ and $Y$ commute.
For Hermitian matrices there are two notable theorems related to the trace of matrix exponentials.
Golden–Thompson inequality
If $A$ and $H$ are Hermitian matrices, then
$$\operatorname{tr}\exp(A+H) \leq \operatorname{tr}(\exp(A)\exp(H)).$$1 Note that there is no requirement of commutativity. There are counterexamples to show that the Golden–Thompson inequality cannot be extended to three matrices – and, in any event, $tr(exp( A )exp( B )exp( C ))$ is not guaranteed to be real for Hermitian $A$, $B$, $C$. However, the next theorem accomplishes this in one sense.
Lieb's theorem
The Lieb's theorem, named after Elliott H. Lieb, states that, for a fixed Hermitian matrix $H$, the function
$$f(A) = \operatorname{tr} \,\exp \left (H + \log A \right)$$ is concave on the cone of positive-definite matrices.2
The exponential map
Note that the exponential of a matrix is always an invertible matrix. The inverse matrix of  is given by . This is analogous to the fact that the exponential of a complex number is always nonzero. The matrix exponential then gives us a map
$$\exp \colon M_n(\mathbb C) \to \mathrm{GL}(n,\mathbb C)$$ from the space of all n×n matrices to the general linear group of degree $n$, i.e. the group of all n×n invertible matrices. In fact, this map is surjective which means that every invertible matrix can be written as the exponential of some other matrix (for this, it is essential to consider the field C of complex numbers and not R).
For any two matrices $X$ and $Y$,
$$\| e^{X+Y} - e^X \| \le \|Y\| e^{\|X\|} e^{\|Y\|},$$
where || · || denotes an arbitrary matrix norm. It follows that the exponential map is continuous and Lipschitz continuous on compact subsets of .
The map
$$t \mapsto e^{tX}, \qquad t \in \mathbb R$$ defines a smooth curve in the general linear group which passes through the identity element at t = 0.
In fact, this gives a one-parameter subgroup of the general linear group since
$$e^{tX}e^{sX} = e^{(t+s)X}.\,$$
The derivative of this curve (or tangent vector) at a point t is given by
$$\frac{d}{dt}e^{tX} = Xe^{tX} = e^{tX}X. \qquad (1)$$ The derivative at t = 0 is just the matrix X, which is to say that X generates this one-parameter subgroup.
More generally,3 for a generic $t$-dependent exponent, $X(t)$, 
Taking the above expression  outside the integral sign and expanding the integrand with the help of the Hadamard lemma one can obtain the following useful expression for the derivative of the matrix exponent,
$$\left(\frac{d}{dt}e^{X(t)}\right)e^{-X(t)} = \frac{d}{dt}X(t) + \frac{1}{2!}[X(t),\frac{d}{dt}X(t)] + \frac{1}{3!}[X(t),[X(t),\frac{d}{dt}X(t)]]+\cdots$$ Note that the coefficients in the expression above are different from what appears in the exponential. For a closed form, see derivative of the exponential map.
The determinant of the matrix exponential
By Jacobi's formula, for any complex square matrix the following trace identity holds: 
In addition to providing a computational tool, this formula demonstrates that a matrix exponential is always an invertible matrix. This follows from the fact that the right hand side of the above equation is always non-zero, and so , which implies that  must be invertible.
In the real-valued case, the formula also exhibits the map
$$\exp \colon M_n(\mathbb R) \to \mathrm{GL}(n,\mathbb R)$$ to not be surjective, in contrast to the complex case mentioned earlier. This follows from the fact that, for real-valued matrices, the right-hand side of the formula is always positive, while there exist invertible matrices with a negative determinant.
Computing the matrix exponential
Finding reliable and accurate methods to compute the matrix exponential is difficult, and this is still a topic of considerable current research in mathematics and numerical analysis. Matlab, GNU Octave, and SciPy all use the Padé approximant.456
Diagonalizable case
If a matrix is diagonal:
$$A=\begin{bmatrix} a_1 & 0 & \ldots & 0 \\
 0 & a_2 & \ldots & 0  \\ \vdots & \vdots & \ddots & \vdots \\
 0 & 0 & \ldots & a_n \end{bmatrix}$$,
then its exponential can be obtained by exponentiating each entry on the main diagonal:
$$e^A=\begin{bmatrix} e^{a_1} & 0 & \ldots & 0 \\
 0 & e^{a_2} & \ldots & 0  \\ \vdots & \vdots & \ddots & \vdots \\
 0 & 0 & \ldots & e^{a_n} \end{bmatrix}$$.
This also allows one to exponentiate diagonalizable matrices. If  and $D$ is diagonal, then . Application of Sylvester's formula yields the same result. (To see this, note that addition and multiplication, hence also exponentiation, of diagonal matrices is equivalent to element-wise addition and multiplication, and hence exponentiation; in particular, the "one-dimensional" exponentiation is felt element-wise for the diagonal case.)
Projection case
If $P$ is a projection matrix (i.e. is idempotent), its matrix exponential is . This may be derived by expansion of the definition of the exponential function and by use of the idempotency of $P$:
$$e^P = \sum_{k=0}^{\infty} \frac{P^k}{k!}=I+\left(\sum_{k=1}^{\infty} \frac{1}{k!}\right)P=I+(e-1)P ~.$$
Rotation case
For a simple rotation in which the perpendicular unit vectors $a$ and $b$ specify a plane,7 the rotation matrix $R$ can be expressed in terms of a similar exponential function involving a generator $G$ and angle $θ$.89
$$G=ba^\mathsf{T}-ab^\mathsf{T} \qquad a^\mathsf{T}b=0$$
$$-G^2=aa^\mathsf{T}+bb^\mathsf{T}=P \qquad P^2=P \qquad PG=GP=G ~,$$
$$\begin{align}
 R\left( \theta  \right) &= {{e}^{G\theta }}=I+G\sin (\theta ) +{{G}^{2}}(1- \cos (\theta ) ) \\ 
  &=I-P+P\cos (\theta )+G\sin (\theta ) ~.\\ 
 \end{align}$$
The formula for the exponential results from reducing the powers of $G$ in the series expansion and identifying the respective series coefficients of  and $G$ with $−cos( θ )$ and $sin( θ )$ respectively. The second expression here for  is the same as the expression for $R ( θ )$ in the article containing the derivation of the generator,  eGθ}}.
In two dimensions, if $a= \left ( \begin{smallmatrix}1\\0\end{smallmatrix} \right )$ and $b= \left ( \begin{smallmatrix}0\\1\end{smallmatrix} \right )$, then $G= \left ( \begin{smallmatrix}0&-1\\1&0\end{smallmatrix} \right )$, $G^2= \left ( \begin{smallmatrix}-1&0\\0&-1\end{smallmatrix} \right )$, and
$$R(\theta)= \left ( \begin{matrix}\cos(\theta)&-\sin(\theta)\\ \sin(\theta)&\cos(\theta)\end{matrix} \right )=I \cos(\theta) + G \sin(\theta)$$ reduces to the standard matrix for a plane rotation.
The matrix  projects a vector onto the $ab$-plane and the rotation only affects this part of the vector. An example illustrating this is a rotation of $30° = π/6$ in the plane spanned by $a$ and $b$,
$\begin{align}
   & a=\left( \begin{matrix}
    1  \\
    0  \\
    0  \\
 \end{matrix} \right)\quad b=\frac{1}{\sqrt{5}}\left( \begin{matrix}
    0  \\
    1  \\
    2  \\
 \end{matrix} \right) \\ 
  & I=\left( \begin{matrix}
    1 & 0 & 0  \\
    0 & 1 & 0  \\
    0 & 0 & 1  \\
 \end{matrix} \right)\quad G=\frac{1}{\sqrt{5}}\left( \begin{matrix}
    0 & -1 & -2  \\
    1 & 0 & 0  \\
    2 & 0 & 0  \\
 \end{matrix} \right) \\
  & P=-{{G}^{2}}=\frac{1}{5}\left( \begin{matrix}
    5 & 0 & 0  \\
    0 & 1 & 2  \\
    0 & 2 & 4  \\
 \end{matrix} \right)\quad P\left( \begin{matrix}
    1  \\
    2  \\
    3  \\
 \end{matrix} \right)=\frac{1}{5}\left( \begin{matrix}
    5  \\
    8  \\
    16  \\
 \end{matrix} \right)=a+\frac{8}{\sqrt{5}}b \\
  & \theta =\frac{\pi}{6} \quad \Rightarrow \quad R=\frac{1}{10}\left( \begin{matrix}
    5\sqrt{3} & -\sqrt{5} & -2\sqrt{5}  \\
    \sqrt{5} & 8+\sqrt{3} & -4+2\sqrt{3}  \\
    2\sqrt{5} & -4+2\sqrt{3} & 2+4\sqrt{3}  \\
 \end{matrix} \right) \\
 \end{align}$
Let $ N = I − P$, so  and its products with $P$ and $G$ are zero. This will allow us to evaluate powers of $R$.
$\begin{align} 
  & R\left( \frac{\pi }{6} \right)=N+P\frac{\sqrt{3}}{2}+G\frac{1}{2}\quad \quad R{{\left( \frac{\pi }{6} 
 
 \right)}^{2}}=N+P\frac{1}{2}+G\frac{\sqrt{3}}{2} \\ 
  & R{{\left( \frac{\pi }{6} \right)}^{3}}=N+G\quad \quad R{{\left( \frac{\pi }{6} \right)}^{6}}=N-P\quad 
 
 \quad R{{\left( \frac{\pi }{6} \right)}^{12}}=N+P=I \\ 
 \end{align}$ 
Nilpotent case
A matrix N is nilpotent if Nq = 0 for some integer q. In this case, the matrix exponential eN can be computed directly from the series expansion, as the series terminates after a finite number of terms:
$$e^N = I + N + \frac{1}{2}N^2 + \frac{1}{6}N^3 + \cdots + \frac{1}{(q-1)!}N^{q-1} ~.$$
Generalization
When the minimal polynomial of a matrix X can be factored into a product of first degree polynomials, it can be expressed as a sum
$$X = A + N \,$$ where

A is diagonalizable
N is nilpotent
A commutes with N (i.e. AN = NA)

This is the Jordan–Chevalley decomposition.
This means that we can compute the exponential of X by reducing to the previous two cases:
$$e^X = e^{A+N} = e^A e^N. \,$$ Note that we need the commutativity of A and N for the last step to work.
Another (closely related) method if the field is algebraically closed is to work with the Jordan form of X. Suppose that X = PJP −1 where J is the Jordan form of X. Then
$$e^{X}=Pe^{J}P^{-1}.\,$$
Also, since
$$J=J_{a_1}(\lambda_1)\oplus J_{a_2}(\lambda_2)\oplus\cdots\oplus J_{a_n}(\lambda_n),$$
$$\begin{align}
 e^{J} & {} = \exp \big( J_{a_1}(\lambda_1)\oplus J_{a_2}(\lambda_2)\oplus\cdots\oplus J_{a_n}(\lambda_n) \big) \\
 & {} = \exp \big( J_{a_1}(\lambda_1) \big) \oplus \exp \big( J_{a_2}(\lambda_2) \big) \oplus\cdots\oplus \exp \big( J_{a_k}(\lambda_k) \big).
 \end{align}$$
Therefore, we need only know how to compute the matrix exponential of a Jordan block. But each Jordan block is of the form
$$J_{a}(\lambda) = \lambda I + N \,$$ where N is a special nilpotent matrix. The matrix exponential of this block is given by
$$e^{\lambda I + N} = e^{\lambda}e^N. \,$$
Evaluation by Laurent series
By virtue of the Cayley–Hamilton theorem the matrix exponential is expressible as a polynomial of order $n$−1.
If $P$ and  are nonzero polynomials in one variable, such that $P ( A ) = 0$, and if the meromorphic function
$$f(z)=\frac{e^{t z}-Q_t(z)}{P(z)}$$ is entire, then
$$e^{t A} = Q_t(A)$$. To prove this, multiply the first of the two above equalities by $P ( z )$ and replace $z$ by $A$.
Such a polynomial  can be found as follows−−see Sylvester's formula. Letting $a$ be a root of $P$,  is solved from the product of $P$ by the principal part of the Laurent series of $f$ at $a$: It is proportional to the relevant Frobenius covariant. Then the sum St of the Qa,t, where $a$ runs over all the roots of $P$, can be taken as a particular . All the other Qt will be obtained by adding a multiple of $P$ to . In particular, , the Lagrange-Sylvester polynomial, is the only  whose degree is less than that of $P$.
Example: Consider the case of an arbitrary 2-by-2 matrix,
$$A:=\begin{bmatrix}
 a & b \\
 c & d \end{bmatrix}.$$
The exponential matrix , by virtue of the Cayley–Hamilton theorem, must be of the form


$e^{tA}=s_0(t)\,I+s_1(t)\,A$.
 



(For any complex number $z$ and any C-algebra $B$, we denote again by $z$ the product of $z$ by the unit of $B$.)
Let $α$ and $β$ be the roots of the characteristic polynomial of $A$,
$$P(z)=z^2-(a+d)\ z+ ad-bc= (z-\alpha)(z-\beta) ~ .$$
Then we have
$$S_t(z)= e^{\alpha t} \frac{z-\beta}{\alpha-\beta}   + e^{\beta t} \frac{z-\alpha}{\beta-\alpha}  ~,$$ and hence
$$s_0(t)=\frac{\alpha\,e^{\beta t}
 -\beta\,e^{\alpha t}}{\alpha-\beta},\quad
 s_1(t)=\frac{e^{\alpha t}-e^{\beta t}}{\alpha-\beta}\quad$$ if $α ≠ β$; while, if $α = β$,
$$S_t(z)= e^{\alpha t} ( 1+ t (z-\alpha  ))  ~,$$ so that
$$s_0(t)=(1-\alpha\,t)\,e^{\alpha t},\quad
 s_1(t)=t\,e^{\alpha t}~.$$
Defining
$$s \equiv \frac{\alpha + \beta}{2}=\frac{\operatorname{tr} A}{2}~, \qquad \qquad  q\equiv \frac{\alpha-\beta}{2}=\pm\sqrt{-\det\left(A-s I\right)},$$ we have
$$s_0(t) = e^{s t}\left(\cosh (q t) - s \frac{\sinh (q t)}{q}\right),\qquad s_1(t) =e^{s t}\frac{\sinh(q t)}{q},$$ where $sin( qt )/ q$ is 0 if $t$ = 0, and $t$ if $q$ = 0. Thus,  Thus, as indicated above, the matrix $A$ having decomposed into the sum of two mutually commuting pieces, the traceful piece and the traceless piece,
$$A= sI + (A-sI)~,$$ the matrix exponential reduces to a plain product of the exponentials of the two respective pieces. This is a formula often used in physics, as it amounts to the analog of Euler's formula for Pauli spin matrices, that is rotations of the doublet representation of the group SU(2).
The polynomial  can also be given the following "interpolation" characterization. Define , and $n$ ≡ deg$P$. Then  is the unique degree $ polynomial which satisfies  et(k)(a)}} whenever $k$ is less than the multiplicity of $a$ as a root of $P$. We assume, as we obviously can, that $P$ is the minimal polynomial of $A$. We further assume that $A$ is a diagonalizable matrix. In particular, the roots of $P$ are simple, and the "interpolation" characterization indicates that  is given by the Lagrange interpolation formula, so it is the Lagrange−Sylvester polynomial .
At the other extreme, if  (z−a)n}}, then
$$S_t=e^{at}\ \sum_{k=0}^{n-1}\ \frac{t^k}{k!}\ (z-a)^k ~.$$ The simplest case not covered by the above observations is when $P=(z-a)^2\,(z-b)$ with $a ≠ b$, which yields
$$S_t=e^{at}\ \frac{z-b}{a-b}\ \Bigg(1+\left(t+\frac{1}{b-a}\right)(z-a)\Bigg)+e^{bt}\ \frac{(z-a)^2}{(b-a)^2}\quad.$$
Evaluation by implementation of Sylvester's formula
A practical, expedited computation of the above reduces to the following rapid steps. Recall from above that an n-by-n matrix $exp( tA )$ amounts to a linear combination of the first $n$−1 powers of $A$ by the Cayley-Hamilton theorem. For diagonalizable matrices, as illustrated above, e.g. in the 2 by 2 case, Sylvester's formula yields  Bα exp(tα)+Bβ exp(tβ)}}, where the $B$s are the Frobenius covariants of $A$.
It is easiest, however, to simply solve for these $B$s directly, by evaluating this expression and its first derivative at $t$=0, in terms of $A$ and $I$, to find the same answer as above.
But this simple procedure also works for defective matrices, in a generalization due to Buchheim.10 This is illustrated here for a 4-by-4 example of a matrix which is not diagonalizable, and the $B$s are not projection matrices.
Consider
$$A = 
 \begin{pmatrix}
 1 & 1 & 0 & 0 \\
 0 & 1 & 1 & 0 \\
 0 & 0 & 1 & -1/8 \\
 0 & 0 & 1/2 & 1/2
 \end{pmatrix}  ~,$$ with eigenvalues 3/4}} and 1}}, each with a multiplicity of two.
Consider the exponential of each eigenvalue multiplied by $t$, . Multiply each such by the corresponding undetermined coefficient matrix . If the eigenvalues have an algebraic multiplicity greater than 1, then repeat the process, but now multiplying by an extra factor of $t$ for each repetition, to ensure linear independence. (If one eigenvalue had a multiplicity of three, then there would be the three terms$$B_{i_1} e^{\lambda_i t}, ~ B_{i_2} t e^{\lambda_i t}, ~ B_{i_3} t^2 e^{\lambda_i t}$$. By contrast, when all eigenvalues are distinct, the $B$s are just the Frobenius covariants, and solving for them as below just amounts to the inversion of the Vandermonde matrix of these 4 eigenvalues.)
Sum all such terms, here four such:
$$e^{A t} = B_{1_1} e^{\lambda_1 t} + B_{1_2} t e^{\lambda_1 t} + B_{2_1} e^{\lambda_2 t} + B_{2_2} t e^{\lambda_2 t} ,$$
$$e^{A t} = B_{1_1} e^{3/4 t} + B_{1_2} t e^{3/4 t} + B_{2_1} e^{1 t} + B_{2_2} t e^{1 t}$$.
To solve for all of the unknown matrices $B$ in terms of the first three powers of $A$ and the identity, we need four equations, the above one providing one such at $t$ =0. Further, differentiate it with respect to $t$,
$$A e^{A t} = 3/4 B_{1_1} e^{3/4 t} + \left( 3/4 t + 1 \right) B_{1_2} e^{3/4 t} + 1 B_{2_1} e^{1 t} + \left(1 t + 1 \right) B_{2_2} e^{1 t}   ~,$$ and again,
$$\begin{align}
 A^2 e^{A t} =& (3/4)^2 B_{1_1} e^{3/4 t} + \left( (3/4)^2 t + ( 3/4 + 1 \cdot 3/4) \right) B_{1_2} e^{3/4 t} + B_{2_1} e^{1 t}\\ +& \left(1^2 t + (1 + 1 \cdot 1 )\right) B_{2_2} e^{1 t} \\  =& (3/4)^2 B_{1_1} e^{3/4 t} + \left( (3/4)^2 t + 3/2 \right) B_{1_2} e^{3/4 t} + B_{2_1} e^{t} + \left(t + 2\right) B_{2_2} e^{t} ~,
 \end{align}$$ and once more,
$$\begin{align}
 A^3 e^{A t} =& (3/4)^3 B_{1_1} e^{3/4 t} + \left( (3/4)^3 t + ( (3/4)^2 + (3/2) \cdot 3/4) ) \right) B_{1_2} e^{3/4 t}\\ +& B_{2_1} e^{1 t} + \left(1^3 t + (1 + 2) \cdot 1 \right) B_{2_2} e^{1 t} \\ =&  (3/4)^3 B_{1_1} e^{3/4 t}\! + \left( (3/4)^3 t\! + 27/16 ) \right) B_{1_2} e^{3/4 t}\! + B_{2_1} e^{t}\! + \left(t + 3\cdot 1\right) B_{2_2} e^{t}
 \end{align}$$. (In the general case, $n$−1 derivatives need be taken.)
Setting $t$=0 in these four equations, the four coefficient matrices $B$s may be solved for,
$$\begin{align}
 I =& B_{1_1} + B_{2_1} \\
 A =& 3/4 B_{1_1} + B_{1_2} + B_{2_1} + B_{2_2} \\
 A^2 =& (3/4)^2 B_{1_1} + (3/2) B_{1_2} + B_{2_1} + 2 B_{2_2} \\
 A^3 =& (3/4)^3 B_{1_1} + (27/16) B_{1_2} + B_{2_1} + 3 B_{2_2}
 \end{align}$$ , to yield
$$\begin{align}
 B_{1_1} =& 128 A^3 - 366 A^2 + 288 A - 80 I \\
 B_{1_2} =& 16 A^3 - 44 A^2 + 40 A - 12 I \\
 B_{2_1} =&-128 A^3 + 366 A^2 - 288 A + 80 I\\
 B_{2_2} =& 16 A^3 - 40 A^2 + 33 A - 9 I
 \end{align}$$ .
Substituting with the value for $A$ yields the coefficient matrices
$$\begin{align}
 B_{1_1} =& \begin{pmatrix}0 & 0 & 48 & -16\\ 0 & 0 & -8 & 2\\ 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 1\end{pmatrix}\\
 B_{1_2} =& \begin{pmatrix}0 & 0 & 4 & -2\\ 0 & 0 & -1 & 1/2\\ 0 & 0 & 1/4 & -1/8\\ 0 & 0 & 1/2 & -1/4 \end{pmatrix}\\
 B_{2_1} =& \begin{pmatrix}1 & 0 & -48 & 16\\ 0 & 1 & 8 & -2\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\end{pmatrix}\\
 B_{2_2} =& \begin{pmatrix}0 & 1 & 8 & -2\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\end{pmatrix}  
 \end{align}$$ so the final answer is
$${e}^{tA}\!=\!\begin{pmatrix}{e}^{t} & t{e}^{t} & \left( 8t-48\right) {e}^{t}\!+\left( 4t+48\right){e}^{3t/4} & \left( 16-2\,t\right){e}^{t}\!+\left( -2t-16\right){e}^{3t/4}\\ 0 & {e}^{t} & 8{e}^{t}\!+\left( -t-8\right) {e}^{3t/4} & -\frac{4{e}^{t}+\left(-t-4\right){e}^{3t/4}}{2}\\ 0 & 0 & \frac{\left( t+4\right) {e}^{3t/4}}{4} & -\frac{t {e}^{3t/4}}{8}\\ 0 & 0 & \frac{t{e}^{3t/4}}{2} & -\frac{\left( t-4\right) {e}^{3t/4}}{4}\end{pmatrix}$$.
The procedure is much shorter than Putzer's algorithm sometimes utilized in such cases.
Illustrations
Suppose that we want to compute the exponential of
$$B=\begin{bmatrix}
 21 & 17 & 6 \\
 -5 & -1 & -6 \\
 4 & 4 & 16 \end{bmatrix}.$$
Its Jordan form is
$$J = P^{-1}BP = \begin{bmatrix}
 4 & 0 & 0 \\
 0 & 16 & 1 \\
 0 & 0 & 16 \end{bmatrix},$$
where the matrix P is given by
$$P=\begin{bmatrix}
 -\frac14 & 2 & \frac54 \\
 \frac14 & -2 & -\frac14 \\
 0 & 4 & 0 \end{bmatrix}.$$
Let us first calculate exp(J). We have
$$J=J_1(4)\oplus J_2(16) \,$$
The exponential of a 1×1 matrix is just the exponential of the one entry of the matrix, so exp(J1(4)) = [e4]. The exponential of J2(16) can be calculated by the formula e(λI + N) = eλ eN mentioned above; this yields11
$$\begin{align}
 \exp \left( \begin{bmatrix} 16 & 1 \\ 0 & 16 \end{bmatrix} \right)
 & = e^{16} \exp \left( \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \right) \\[6pt]
 & = e^{16} \left(\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} + \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} + {1 \over 2!}\begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} + \cdots \right)
 = \begin{bmatrix} e^{16} & e^{16} \\ 0 & e^{16} \end{bmatrix}.
 \end{align}$$
Therefore, the exponential of the original matrix B is
$$\begin{align}
 \exp(B)
 & = P \exp(J) P^{-1}
 = P \begin{bmatrix} e^4 & 0 & 0 \\ 0 & e^{16} & e^{16} \\ 0 & 0 & e^{16}  \end{bmatrix} P^{-1} \\[6pt]
 & = {1\over 4} \begin{bmatrix}
    13e^{16} - e^4 & 13e^{16} - 5e^4 & 2e^{16} - 2e^4 \\
    -9e^{16} + e^4 & -9e^{16} + 5e^4 & -2e^{16} + 2e^4 \\
    16e^{16}       & 16e^{16}        & 4e^{16}
 \end{bmatrix}.
 \end{align}$$
Applications
Linear differential equations
The matrix exponential has applications to systems of linear differential equations. (See also matrix differential equation.) Recall from earlier in this article that a homogeneous differential equation of the form
$$\mathbf{y}' = A\mathbf{y}$$ has solution .
If we consider the vector
$$\mathbf{y}(t) = \begin{pmatrix} y_1(t) \\ \vdots \\y_n(t) \end{pmatrix}   ~,$$ we can express a system of inhomogeneous coupled linear differential equations as
$$\mathbf{y}'(t) = A\mathbf{y}(t)+\mathbf{b}(t).$$ Making an ansatz to use an integrating factor of  and multiplying throughout, yields
$$e^{-At}\mathbf{y}'-e^{-At}A\mathbf{y} = e^{-At}\mathbf{b}$$
$$e^{-At}\mathbf{y}'-Ae^{-At}\mathbf{y} = e^{-At}\mathbf{b}$$
$$\frac{d}{dt} (e^{-At}\mathbf{y}) = e^{-At}\mathbf{b}~.$$
The second step is possible due to the fact that, if $AB = BA$, then  BeAt}}. So, calculating  leads to the solution to the system, by simply integrating the third step in $t$s.
Example (homogeneous)
Consider the system
$$\begin{matrix}
 x' &=& 2x&-y&+z \\
 y' &=&   &3y&-1z \\
 z' &=& 2x&+y&+3z  ~.\end{matrix}$$
The associated defective matrix is
$$A=\begin{bmatrix}
 2 & -1 &  1 \\
 0 &  3 & -1 \\
 2 &  1 &  3 \end{bmatrix}  ~.$$
The matrix exponential is
$$e^{tA}=\frac{1}{2}\begin{bmatrix}
     e^{2t}(1+e^{2t}-2t)  & -2te^{2t}    &  e^{2t}(-1+e^{2t}) \\
    -e^{2t}(-1+e^{2t}-2t) & 2(t+1)e^{2t} & -e^{2t}(-1+e^{2t}) \\
     e^{2t}(-1+e^{2t}+2t) & 2te^{2t}     &  e^{2t}(1+e^{2t})  \end{bmatrix}~,$$
so that the general solution of the homogeneous system is
$$\begin{bmatrix}x \\y \\ z\end{bmatrix}=
 \frac{x(0)}{2}\begin{bmatrix}e^{2t}(1+e^{2t}-2t) \\-e^{2t}(-1+e^{2t}-2t)\\e^{2t}(-1+e^{2t}+2t)\end{bmatrix}
 +\frac{y(0)}{2}\begin{bmatrix}-2te^{2t}\\2(t+1)e^{2t}\\2te^{2t}\end{bmatrix}
 +\frac{z(0)}{2}\begin{bmatrix}e^{2t}(-1+e^{2t})\\-e^{2t}(-1+e^{2t})\\e^{2t}(1+e^{2t})\end{bmatrix} ~,$$ amounting to
$$\begin{align}
 2x & = x(0)(e^{2t}(1+e^{2t}-2t)) + y(0) (-2te^{2t}) + z(0)(e^{2t}(-1+e^{2t})) \\
 2y & = x(0)(-e^{2t}(-1+e^{2t}-2t)) + y(0)(2(t+1)e^{2t}) + z(0)(-e^{2t}(-1+e^{2t})) \\
 2z & = x(0)(e^{2t}(-1+e^{2t}+2t)) + y(0)(2te^{2t}) + z(0)(e^{2t}(1+e^{2t}))  ~.
 \end{align}$$
Example (inhomogeneous)
Consider now the inhomogeneous system
$$\begin{matrix}
 x' &=& 2x & - & y & + & z & + & e^{2t} \\
 y' &=&    &   & 3y& - & z & \\
 z' &=& 2x & + & y & + & 3z & + & e^{2t} \end{matrix} ~.$$
We again have
$$A= \left[ \begin{array}{rrr}
 2 & -1 &  1 \\
 0 &  3 & -1 \\
 2 &  1 &  3 \end{array} \right] ~,$$ and
$$\mathbf{b}=e^{2t}\begin{bmatrix}1 \\0\\1\end{bmatrix}.$$
From before, we already have the general solution to the homogeneous equation. Since the sum of the homogeneous and particular solutions give the general solution to the inhomogeneous problem, we now only need find the particular solution.
We have, by above,
$$\mathbf{y}_p = e^{tA}\int_0^t e^{(-u)A}\begin{bmatrix}e^{2u} \\0\\e^{2u}\end{bmatrix}\,du+e^{tA}\mathbf{c}$$
$$\mathbf{y}_p = e^{tA}\int_0^t
 \begin{bmatrix}
      2e^u - 2ue^{2u} & -2ue^{2u}    & 0 \\  \\
 -2e^u + 2(u+1)e^{2u} & 2(u+1)e^{2u} & 0 \\  \\
             2ue^{2u} & 2ue^{2u}     & 2e^u\end{bmatrix}\begin{bmatrix}e^{2u} \\0\\e^{2u}\end{bmatrix}\,du+e^{tA}\mathbf{c}$$
$$\mathbf{y}_p = e^{tA}\int_0^t
 \begin{bmatrix}
 e^{2u}( 2e^u - 2ue^{2u}) \\  \\
   e^{2u}(-2e^u + 2(1 + u)e^{2u}) \\  \\
   2e^{3u} + 2ue^{4u}\end{bmatrix}\,du+e^{tA}\mathbf{c}$$
$$\mathbf{y}_p = e^{tA}\begin{bmatrix}
 -{1 \over 24}e^{3t}(3e^t(4t-1)-16) \\  \\
 {1 \over 24}e^{3t}(3e^t(4t+4)-16) \\  \\
 {1 \over 24}e^{3t}(3e^t(4t-1)-16)\end{bmatrix}+
 \begin{bmatrix}
      2e^t - 2te^{2t} & -2te^{2t}    & 0 \\  \\
 -2e^t + 2(t+1)e^{2t} & 2(t+1)e^{2t} & 0 \\  \\
             2te^{2t} & 2te^{2t}     & 2e^t\end{bmatrix}\begin{bmatrix}c_1 \\c_2 \\c_3\end{bmatrix} ~,$$ which could be further simplified to get the requisite particular solution determined through variation of parameters. Note c = yp(0). For more rigor, see the following generalization.
Inhomogeneous case generalization: variation of parameters
For the inhomogeneous case, we can use integrating factors (a method akin to variation of parameters). We seek a particular solution of the form  exp(tA) z (t) }},
$$\begin{align}
 \mathbf{y}_p'(t) & = (e^{tA})'\mathbf{z}(t)+e^{tA}\mathbf{z}'(t) \\[6pt]
 & = Ae^{tA}\mathbf{z}(t)+e^{tA}\mathbf{z}'(t) \\[6pt]
 & = A\mathbf{y}_p(t)+e^{tA}\mathbf{z}'(t)~.
 \end{align}$$
For  to be a solution,
$$\begin{align}
 e^{tA}\mathbf{z}'(t) & = \mathbf{b}(t) \\[6pt]
 \mathbf{z}'(t) & = (e^{tA})^{-1}\mathbf{b}(t) \\[6pt]
 \mathbf{z}(t) & = \int_0^t e^{-uA}\mathbf{b}(u)\,du+\mathbf{c} ~.
 \end{align}$$
Thus,
$$\begin{align}
 \mathbf{y}_p(t) & {} = e^{tA}\int_0^t e^{-uA}\mathbf{b}(u)\,du+e^{tA}\mathbf{c} \\
 & {} = \int_0^t e^{(t-u)A}\mathbf{b}(u)\,du+e^{tA}\mathbf{c}
 \end{align} ~,$$ where $\mathbf{ c } $ is determined by the initial conditions of the problem.
More precisely, consider the equation
$$Y'-A\ Y=F(t)$$ with the initial condition  Y0}}, where $A$ is an $n$ by $n$ complex matrix,
$F$ is a continuous function from some open interval $I$ to ℂn,
$t_0$ is a point of $I$, and
$Y_0$ is a vector of ℂn.
Left-multiplying the above displayed equality by  yields
$$Y(t)=e^{(t-t_0)A}\ Y_0+\int_{t_0}^t e^{(t-x)A}\ F(x)\ dx  ~.$$
We claim that the solution to the equation
$$P(d/dt)\ y = f(t)$$ with the initial conditions $y^{(k)}(t_0)=y_k$ for 0 ≤ $k  is
$$y(t)=\sum_{k=0}^{n-1}\ y_k\ s_k(t-t_0)+\int_{t_0}^t s_{n-1}(t-x)\ f(x)\ dx ~,$$ where the notation is as follows:
$P\in\mathbb{C}[X]$ is a monic polynomial of degree $n > 0$,
$f$ is a continuous complex valued function defined on some open interval $I$,
$t_0$ is a point of $I$,
$y_k$ is a complex number, and
 is the coefficient of $X^k$ in the polynomial denoted by $S_t\in\mathbb{C}[X]$ in Subsection Evaluation by Laurent series above.
To justify this claim, we transform our order $n$ scalar equation into an order one vector equation by the usual reduction to a first order system. Our vector equation takes the form
$$\frac{dY}{dt}-A\ Y=F(t),\quad Y(t_0)=Y_0,$$
where $A$ is the transpose companion matrix of $P$. We solve this equation as explained above, computing the matrix exponentials by the observation made in Subsection Alternative above.
In the case $n$ = 2 we get the following statement. The solution to
$$y''-(\alpha+\beta)\ y'
 +\alpha\,\beta\ y=f(t),\quad
 y(t_0)=y_0,\quad y'(t_0)=y_1$$ is
$$y(t)=y_0\ s_0(t-t_0)+y_1\ s_1(t-t_0)
 +\int_{t_0}^t s_1(t-x)\,f(x)\ dx,$$ where the functions  and  are as in Subsection Evaluation by Laurent series above.
See also

Matrix function
Matrix logarithm
Exponential function
Exponential map (Lie theory)
Magnus expansion
Derivative of the exponential map
Vector flow
Golden–Thompson inequality
Phase-type distribution
Lie product formula
Baker–Campbell–Hausdorff formula
Frobenius covariant
Sylvester's formula

References


.

.




External links


Module for the Matrix Exponential

"
Category:Matrix theory Category:Lie groups Category:Exponentials




 




in a Euclidean space


Rinehart, R. F. (1955). "The equivalence of definitions of a matric function". The American Mathematical Monthly, 62 (6), 395-414.
This can be generalized; in general, the exponential of Jn(a) is an upper triangular matrix with ea/0! on the main diagonal, ea/1! on the one above, ea/2! on the next one, and so on.




