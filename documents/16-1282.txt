   First-order second-moment method      First-order second-moment method   In probability theory, the first-order second-moment (FOSM) method , also referenced as mean value first-order second-moment (MVFOSM) method , is a probabilistic method to determine the stochastic moments of a function with random input variables. The name is based on the derivation, which uses a first-order  Taylor series and the first and second moments of the input variables. 1  Approximation  Consider the objective function    g   (  x  )       g  x    g(x)   , where the input vector   x   x   x   is a realization of the random vector   X   X   X   with probability density function      f  X    (  x  )        subscript  f  X   x    f_{X}(x)   . As   X   X   X   is randomly distributed, also   g   g   g   is randomly distributed. Following the FOSM method, the mean value of   g   g   g   is approximated by       μ  g   ≈   g   (  μ  )         subscript  μ  g     g  μ     \mu_{g}\approx g(\mu)     The variance of   g   g   g   is approximated by       σ  g  2   ≈    ∑   i  =  1   n     ∑   j  =  1   n       ∂  g    (  μ  )     ∂   x  i        ∂  g    (  μ  )     ∂   x  j      cov   (   X  i   ,   X  j   )            subscript   superscript  σ  2   g     superscript   subscript     i  1    n     superscript   subscript     j  1    n           g   μ      subscript  x  i           g   μ      subscript  x  j      cov   subscript  X  i    subscript  X  j         \sigma^{2}_{g}\approx\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{\partial g(\mu)}{%
 \partial x_{i}}\frac{\partial g(\mu)}{\partial x_{j}}\operatorname{cov}(X_{i},%
 X_{j})     where   n   n   n   is the length/dimension of   x   x   x   and      ∂  g    (  μ  )     ∂   x  i            g   μ      subscript  x  i      \frac{\partial g(\mu)}{\partial x_{i}}   is the partial derivative of   g   g   g   at the mean vector   μ   μ   \mu   with respect to the i -th entry of   x   x   x   .  Derivation  The objective function is approximated by a Taylor series at the mean vector   μ   μ   \mu   .       g   (  x  )    =    g   (  μ  )    +    ∑   i  =  1   n       ∂  g    (  μ  )     ∂   x  i      (    x  i   -   μ  i    )     +    1  2     ∑   i  =  1   n     ∑   j  =  1   n        ∂  2   g    (  μ  )      ∂    x  i      ∂   x  j       (    x  i   -   μ  i    )    (    x  j   -   μ  j    )       +  ⋯         g  x       g  μ     superscript   subscript     i  1    n           g   μ      subscript  x  i        subscript  x  i    subscript  μ  i          1  2     superscript   subscript     i  1    n     superscript   subscript     j  1    n           superscript   2   g   μ        subscript  x  i       subscript  x  j         subscript  x  i    subscript  μ  i       subscript  x  j    subscript  μ  j        normal-⋯     g(x)=g(\mu)+\sum_{i=1}^{n}\frac{\partial g(\mu)}{\partial x_{i}}(x_{i}-\mu_{i}%
 )+\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{\partial^{2}g(\mu)}{\partial x_%
 {i}\,\partial x_{j}}(x_{i}-\mu_{i})(x_{j}-\mu_{j})+\cdots     The mean value of   g   g   g   is given by the integral       μ  g   =   E   [   g   (  x  )    ]    =    ∫   -  ∞   ∞    g   (  x  )    f  X    (  x  )   d  x           subscript  μ  g     E   delimited-[]    g  x            superscript   subscript            g  x   subscript  f  X   x  d  x       \mu_{g}=E[g(x)]=\int_{-\infty}^{\infty}g(x)f_{X}(x)\,dx     Inserting the first-order Taylor series yields      μ  g     subscript  μ  g    \displaystyle\par
 \mu_{g}     The variance of   g   g   g   is given by the integral        σ  g  2   =   E   (    [    g   (  x  )    -   μ  g    ]   2   )    =    ∫   -  ∞   ∞      [    g   (  x  )    -   μ  g    ]   2    f  X    (  x  )   d  x     .         superscript   subscript  σ  g   2     E   superscript   delimited-[]      g  x    subscript  μ  g     2           superscript   subscript             superscript   delimited-[]      g  x    subscript  μ  g     2    subscript  f  X   x  d  x       \sigma_{g}^{2}=E([g(x)-\mu_{g}]^{2})=\int_{-\infty}^{\infty}[g(x)-\mu_{g}]^{2}%
 f_{X}(x)\,dx.     According to the computational formula for the variance , this can be written as       σ  g  2   =   E   (    [    g   (  x  )    -   μ  g    ]   2   )    =    E   (   g    (  x  )   2    )    -   μ  g  2    =     ∫   -  ∞   ∞    g    (  x  )   2    f  X    (  x  )   d  x    -   μ  g  2           superscript   subscript  σ  g   2     E   superscript   delimited-[]      g  x    subscript  μ  g     2             E    g   superscript  x  2      superscript   subscript  μ  g   2             superscript   subscript            g   superscript  x  2    subscript  f  X   x  d  x     superscript   subscript  μ  g   2       \sigma_{g}^{2}=E([g(x)-\mu_{g}]^{2})=E(g(x)^{2})-\mu_{g}^{2}=\int_{-\infty}^{%
 \infty}g(x)^{2}f_{X}(x)\,dx-\mu_{g}^{2}     Inserting the Taylor series yields      σ  g  2     superscript   subscript  σ  g   2    \displaystyle\sigma_{g}^{2}     Higher-order approaches  The following abbreviations are introduced.        g  μ   =   g   (  μ  )     ,     g   ,  i    =     ∂  g    (  μ  )     ∂   x  i      ,     g   ,  i  j    =      ∂  2   g    (  μ  )      ∂    x  i      ∂   x  j       ,    μ   i  ,  j    =   E   [    (    x  i   -   μ  i    )   j   ]          formulae-sequence     subscript  g  μ     g  μ     formulae-sequence     subscript  g   fragments  normal-,  i          g   μ      subscript  x  i       formulae-sequence     subscript  g   fragments  normal-,  i  j          superscript   2   g   μ        subscript  x  i       subscript  x  j          subscript  μ   i  j      E   delimited-[]   superscript     subscript  x  i    subscript  μ  i    j          g_{\mu}=g(\mu),\quad g_{,i}=\frac{\partial g(\mu)}{\partial x_{i}},\quad g_{,%
 ij}=\frac{\partial^{2}g(\mu)}{\partial x_{i}\,\partial x_{j}},\quad\mu_{i,j}=E%
 [(x_{i}-\mu_{i})^{j}]     In the following, the entries of the random vector   X   X   X   are assumed to be independent. Considering also the second-order terms of the Taylor expansion, the approximation of the mean value is given by       μ  g   ≈    g  μ   +    1  2     ∑   i  =  1   n      g   ,  i  i      μ   i  ,  2             subscript  μ  g      subscript  g  μ       1  2     superscript   subscript     i  1    n      subscript  g   fragments  normal-,  i  i     subscript  μ   i  2          \mu_{g}\approx g_{\mu}+\frac{1}{2}\sum_{i=1}^{n}g_{,ii}\;\mu_{i,2}     The second-order approximation of the variance is given by      σ  g  2     superscript   subscript  σ  g   2    \displaystyle\sigma_{g}^{2}     The skewness of   g   g   g   can be determined from the third central moment     μ   g  ,  3      subscript  μ   g  3     \mu_{g,3}   . When considering only linear terms of the Taylor series, but higher-order moments, the third central moment is approximated by       μ   g  ,  3    ≈    ∑   i  =  1   n      g   ,  i   3     μ   i  ,  3           subscript  μ   g  3      superscript   subscript     i  1    n      superscript   subscript  g   fragments  normal-,  i    3    subscript  μ   i  3        \mu_{g,3}\approx\sum_{i=1}^{n}g_{,i}^{3}\;\mu_{i,3}   For the second-order approximations of the third central moment as well as for the derivation of all higher-order approximations see Appendix D of Ref. 2 Taking into account the quadratic terms of the Taylor series and the third moments of the input variables is referred to as second-order third-moment method. 3 However, the full second-order approach of the variance (given above) also includes fourth-order moments of input parameters, and the full second-order approach of the skewness 6th-order moments 4  Practical application  There are several examples in the literature where the FOSM method is employed to estimate the stochastic distribution of the buckling load of axially compressed structures (see e.g. Ref. 5 6 7 8 ). For structures which are very sensitive to deviations from the ideal structure (like cylindrical shells) it has been proposed to use the FOSM method as a design approach. Often the applicability is checked by comparison with a Monte Carlo simulation . In engineering practice, the objective function often is not given as analytic expression, but for instance as a result of a finite-element simulation. Then the derivatives of the objective function need to be estimated by the central differences method. The number of evaluations of the objective function equals     2  n   +  1        2  n   1    2n+1   . Depending on the number of random variables this still can mean a significantly smaller number of evaluations than performing a Monte Carlo simulation. However, when using the FOSM method as a design procedure, a lower bound shall be estimated, which is actually not given by the FOSM approach. Therefore, a type of distribution needs to be assumed for the distribution of the objective function, taking into account the approximated mean value and standard deviation.  References    "  Category:Probabilistic models  Category:Stochastic algorithms     A. Haldar and S. Mahadevan, Probability, Reliability, and Statistical Methods in Engineering Design. John Wiley & Sons New York/Chichester, UK, 2000. ↩  B. Kriegesmann, "Probabilistic Design of Thin-Walled Fiber Composite Structures", Mitteilungen des Instituts für Statik und Dynamik der Leibniz Universität Hannover 15/2012, ISSN 1862-4650, Gottfried Wilhelm Leibniz Universität Hannover, Hannover, Germany, 2012, PDF; 10,2MB . ↩  Y. J. Hong, J. Xing, and J. B. Wang, "A Second-Order Third-Moment Method for Calculating the Reliability of Fatigue", Int. J. Press. Vessels Pip., 76 (8), pp 567–570, 1999. ↩   I. Elishakoff, S. van Manen, P. G. Vermeulen, and J. Arbocz, "First-Order Second-Moment Analysis of the Buckling of Shells with Random Imperfections", AIAA J., 25 (8), pp 1113–1117, 1987. ↩  I. Elishakoff, "Uncertain Buckling: Its Past, Present and Future", Int. J. Solids Struct., 37 (46–47), pp 6869–6889, Nov. 2000. ↩  J. Arbocz and M. W. Hilburger, "Toward a Probabilistic Preliminary Design Criterion for Buckling Critical Composite Shells", AIAA J., 43 (8), pp 1823–1827, 2005. ↩  B. Kriegesmann, R. Rolfes, C. Hühne, and A. Kling, "Fast Probabilistic Design Procedure for Axially Compressed Composite Cylinders", Compos. Struct., 93, pp 3140–3149, 2011. ↩     