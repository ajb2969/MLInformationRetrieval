   Optimal estimation      Optimal estimation   In applied statistics, optimal estimation is a regularized  matrix  inverse method based on Bayes theorem . It is used very commonly in the geosciences , particularly for atmospheric sounding . A matrix inverse problem looks like this:       𝐀   x  →    =   y  →         𝐀   normal-→  x     normal-→  y     \mathbf{A}\vec{x}=\vec{y}   The essential concept is to transform the matrix, A , into a conditional probability and the variables,    x  →     normal-→  x    \vec{x}   and    y  →     normal-→  y    \vec{y}   into probability distributions by assuming Gaussian statistics and using empirically-determined covariance matrices.  Derivation  Typically, one expects the statistics of most measurements to be Gaussian . So for example for    P   (   y  →   |   x  →   )      fragments  P   fragments  normal-(   normal-→  y   normal-|   normal-→  x   normal-)     P(\vec{y}|\vec{x})   , we can write:      P   (   y  →   |   x  →   )   =   1     (   2  π   )     m  n   /  2     |   𝑺  𝒚   |     exp   [  -   1  2     (  𝑨   x  →   -   y  →   )   T    𝑺  𝒚     -  1     (  𝑨   x  →   -   y  →   )   ]      fragments  P   fragments  normal-(   normal-→  y   normal-|   normal-→  x   normal-)      1     superscript    2  π       m  n   2       subscript  𝑺  𝒚        fragments  normal-[     1  2    superscript   fragments  normal-(  A   normal-→  x     normal-→  y   normal-)   T    superscript   subscript  𝑺  𝒚     1     fragments  normal-(  A   normal-→  x     normal-→  y   normal-)   normal-]     P(\vec{y}|\vec{x})=\frac{1}{(2\pi)^{mn/2}|\boldsymbol{S_{y}}|}\exp\left[-\frac%
 {1}{2}(\boldsymbol{A}\vec{x}-\vec{y})^{T}\boldsymbol{S_{y}}^{-1}(\boldsymbol{A%
 }\vec{x}-\vec{y})\right]     where m and n are the numbers of elements in    x  →     normal-→  x    \vec{x}   and    y  →     normal-→  y    \vec{y}   respectively   𝑨   𝑨   \boldsymbol{A}   is the matrix to be solved (the linear or linearised forward model) and    𝑺  𝒚     subscript  𝑺  𝒚    \boldsymbol{S_{y}}   is the covariance matrix of the vector    y  →     normal-→  y    \vec{y}   . This can be similarly done for    x  →     normal-→  x    \vec{x}   :       P   (   x  →   )    =    1     (   2  π   )    m  /  2     |   𝑺   𝒙  𝒂    |      exp   [   -    1  2     (    x  →   -    x  a   ^    )   T    𝑺   𝒙  𝒂      -  1     (    x  →   -    x  a   ^    )     ]           P   normal-→  x        1     superscript    2  π     m  2       subscript  𝑺   subscript  𝒙  𝒂               1  2    superscript     normal-→  x    normal-^   subscript  x  a     T    superscript   subscript  𝑺   subscript  𝒙  𝒂      1       normal-→  x    normal-^   subscript  x  a           P(\vec{x})=\frac{1}{(2\pi)^{m/2}|\boldsymbol{S_{x_{a}}}|}\exp\left[-\frac{1}{2%
 }(\vec{x}-\widehat{x_{a}})^{T}\boldsymbol{S_{x_{a}}}^{-1}(\vec{x}-\widehat{x_{%
 a}})\right]     Here    P   (   x  →   )       P   normal-→  x     P(\vec{x})   is taken to be the so-called "a-priori" distribution     x  a   ^     normal-^   subscript  x  a     \widehat{x_{a}}   denotes the a-priori values for    x  →     normal-→  x    \vec{x}   while    𝑺   𝒙  𝒂      subscript  𝑺   subscript  𝒙  𝒂     \boldsymbol{S_{x_{a}}}   is its covariance matrix.  The nice thing about the Gaussian distributions is that only two parameters are needed to describe them and so the whole problem can be converted once again to matrices. Assuming that    P   (   x  →   |   y  →   )      fragments  P   fragments  normal-(   normal-→  x   normal-|   normal-→  y   normal-)     P(\vec{x}|\vec{y})   takes the following form:      P   (   x  →   |   y  →   )   =   1     (   2  π   )     m  n   /  2     |   𝑺  𝒙   |     exp   [  -   1  2     (   x  →   -   x  ^   )   T    𝑺  𝒙     -  1     (   x  →   -   x  ^   )   ]      fragments  P   fragments  normal-(   normal-→  x   normal-|   normal-→  y   normal-)      1     superscript    2  π       m  n   2       subscript  𝑺  𝒙        fragments  normal-[     1  2    superscript   fragments  normal-(   normal-→  x     normal-^  x   normal-)   T    superscript   subscript  𝑺  𝒙     1     fragments  normal-(   normal-→  x     normal-^  x   normal-)   normal-]     P(\vec{x}|\vec{y})=\frac{1}{(2\pi)^{mn/2}|\boldsymbol{S_{x}}|}\exp\left[-\frac%
 {1}{2}(\vec{x}-\widehat{x})^{T}\boldsymbol{S_{x}}^{-1}(\vec{x}-\widehat{x})\right]       P   (   y  →   )       P   normal-→  y     P(\vec{y})   may be neglected since, for a given value of    x  →     normal-→  x    \vec{x}   , it is simply a constant scaling term. Now it is possible to solve for both the expectation value of    x  →     normal-→  x    \vec{x}   ,    x  ^     normal-^  x    \widehat{x}   , and for its covariance matrix by equating    P   (   x  →   |   y  →   )      fragments  P   fragments  normal-(   normal-→  x   normal-|   normal-→  y   normal-)     P(\vec{x}|\vec{y})   and    P   (   y  →   |   x  →   )   P   (   x  →   )      fragments  P   fragments  normal-(   normal-→  y   normal-|   normal-→  x   normal-)   P   fragments  normal-(   normal-→  x   normal-)     P(\vec{y}|\vec{x})P(\vec{x})   . This produces the following equations:       𝑺  𝒙   =    (     𝑨  T    𝑺  𝒚   -  𝟏    𝑨   +   𝑺   𝒙  𝒂    -  𝟏     )    -  1         subscript  𝑺  𝒙    superscript       superscript  𝑨  T    superscript   subscript  𝑺  𝒚     1    𝑨    superscript   subscript  𝑺   subscript  𝒙  𝒂      1       1      \boldsymbol{S_{x}}=(\boldsymbol{A}^{T}\boldsymbol{S_{y}^{-1}}\boldsymbol{A}+%
 \boldsymbol{S_{x_{a}}^{-1}})^{-1}          x  ^   =     x  a   ^   +    𝑺  𝒙    𝑨  T    𝑺  𝒚     -  1     (    y  →   -   𝑨    x  a   ^     )          normal-^  x      normal-^   subscript  x  a       subscript  𝑺  𝒙    superscript  𝑨  T    superscript   subscript  𝑺  𝒚     1       normal-→  y     𝑨   normal-^   subscript  x  a          \widehat{x}=\widehat{x_{a}}+\boldsymbol{S_{x}}\boldsymbol{A}^{T}\boldsymbol{S_%
 {y}}^{-1}(\vec{y}-\boldsymbol{A}\widehat{x_{a}})     Because we are using Gaussians, the expected value is equivalent to the maximum likely value, and so this is also a form of maximum likelihood estimation.  Typically with optimal estimation, in addition to the vector of retrieved quantities, one extra matrix is returned along with the covariance matrix. This is sometimes called the resolution matrix or the averaging kernel and is calculated as follows:      𝑹  =     (     𝑨  T    𝑺  𝒚     -  1    𝑨   +   𝑺   𝒙  𝒂      -  1     )    -  1     𝑨  T    𝑺  𝒚     -  1    𝑨       𝑹     superscript       superscript  𝑨  T    superscript   subscript  𝑺  𝒚     1    𝑨    superscript   subscript  𝑺   subscript  𝒙  𝒂      1       1     superscript  𝑨  T    superscript   subscript  𝑺  𝒚     1    𝑨     \boldsymbol{R}=(\boldsymbol{A}^{T}\boldsymbol{S_{y}}^{-1}\boldsymbol{A}+%
 \boldsymbol{S_{x_{a}}}^{-1})^{-1}\boldsymbol{A}^{T}\boldsymbol{S_{y}}^{-1}%
 \boldsymbol{A}     This tells us, for a given element of the retrieved vector, how much of the other elements of the vector are mixed in. In the case of a retrieval of profile information, it typical indicates the altitude resolution for a given altitude. For instance if the resolution vectors for all the altitudes contain non-zero elements (to a numerical tolerance) in their four nearest neighbours, then the altitude resolution is only one fourth that of the actual grid size.  References           "  Category:Inverse problems  Category:Remote sensing   