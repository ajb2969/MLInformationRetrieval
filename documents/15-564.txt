   Multicanonical ensemble      Multicanonical ensemble   In statistics and physics , multicanonical ensemble (also called multicanonical sampling or flat histogram ) is a Markov chain Monte Carlo sampling technique that uses the Metropolisâ€“Hastings algorithm to compute integrals where the integrand has a rough landscape with multiple local minima . It samples states according to the inverse of the density of states , 1 which has to be known a priori or be computed using other techniques like the Wang and Landau algorithm . 2 Multicanonical sampling is an important technique for spin systems like the Ising model or spin glasses . 3 4 5  Motivation  In systems with a large number of degrees of freedom, like spin systems, Monte Carlo integration is required. In this integration, importance sampling and in particular the Metropolis algorithm , is a very important technique. 6 However, the Metropolis algorithm samples states according to    exp   (   -   Î²  E    )           Î²  E      \exp(-\beta E)   where beta is the inverse of the temperature. This means that an energy barrier of    Î”  E      normal-Î”  E    \Delta E   on the energy spectrum is exponentially difficult to overcome. 7 Systems with multiple local energy minima like the Potts model become hard to sample as the algorithm gets stuck in the system's local minima. 8 This motivates other approaches, namely, other sampling distributions.  Overview  Multicanonical ensemble uses the Metropolisâ€“Hastings algorithm with a sampling distribution given by the inverse of the density of states of the system, contrary to the sampling distribution    exp   (   -   Î²  E    )           Î²  E      \exp(-\beta E)   of the Metropolis algorithm. 9 With this choice, on average, the number of states sampled at each energy is constant, i.e. it is a simulation with a "flat histogram" on energy. This leads to an algorithm for which the energy barriers are no longer difficult to overcome. Another advantage over the Metropolis algorithm is that the sampling is independent of the temperature of the system, which means that one simulation allows the estimation of thermodynamical variables for all temperatures (thus the name "multicanonical": several temperatures). This is a great improvement in the study of first order phase transitions . 10  The biggest problem in performing a multicanonical ensemble is that the density of states has to be known a priori . 11 12 One important contribution to multicanonical sampling was the Wang and Landau algorithm , which asymptotically converges to a multicanonical ensemble while calculating the density of states during the convergence. 13  The multicanonical ensemble is not restricted to physical systems. It can be employed on abstract systems which have a cost function F . By using the density of states with respect to F, the method becomes general for computing higher-dimensional integrals or finding local minima. 14  Motivation  Consider a system and it phase-space   Î©   normal-Î©   \Omega   characterized by a configuration    ð’“  âˆˆ  Î©      ð’“  normal-Î©    \boldsymbol{r}\in\Omega   and a "cost" function F from the system's phase-space to a one-dimensional space   Î“   normal-Î“   \Gamma        F   (  Î©  )    =  Î“  =   [   Î“  min   ,   Î“  max   ]           F  normal-Î©   normal-Î“         subscript  normal-Î“     subscript  normal-Î“        F(\Omega)=\Gamma=[\Gamma_{\min},\Gamma_{\max}]   , the spectrum of F .      {{show   example:   The Ising model with N sites is an example of such a system; the phase-space is a discrete phase-space defined by all possible configurations of N spins    ð’“  =   (   Ïƒ  1   ,  â€¦  ,   Ïƒ  i   ,  â€¦  ,   Ïƒ  N   )       ð’“    subscript  Ïƒ  1   normal-â€¦   subscript  Ïƒ  i   normal-â€¦   subscript  Ïƒ  N      \boldsymbol{r}=(\sigma_{1},\ldots,\sigma_{i},\ldots,\sigma_{N})   where     Ïƒ  i   âˆˆ   {   -  1   ,  1  }        subscript  Ïƒ  i      1   1     \sigma_{i}\in\{-1,1\}   . The cost function is the Hamiltonian of the system:        H   (  ð’“  )    =   -    âˆ‘   âŸ¨    i   j   âŸ©      J   i  j     (   1  -    Ïƒ  i    Ïƒ  j     )       ,        H  ð’“       subscript    delimited-âŸ¨âŸ©    i  j        subscript  J    i  j      1     subscript  Ïƒ  i    subscript  Ïƒ  j          H(\boldsymbol{r})=-\sum_{\langle i~{}j\rangle}J_{ij}(1-\sigma_{i}\sigma_{j}),   where    J   i  j      subscript  J    i  j     J_{ij}   is the sum over neighborhoods and    Î“  =   [   E  min   ,   E  max   ]       normal-Î“    subscript  E     subscript  E       \Gamma=[E_{\min},E_{\max}]   is the interaction matrix. The energy spectrum is    J   i  j      subscript  J    i  j     J_{ij}   which, in this case, depends on the particular    J   i  j      subscript  J    i  j     J_{ij}   used. If all     E  min   =  0       subscript  E    0    E_{\min}=0   are 1 (the ferromagnetic Ising model),     E  max   =   2  D  N        subscript  E      2  D  N     E_{\max}=2DN   (e.g. all spins are 1.) and    Î“  âˆˆ  â„¤      normal-Î“  â„¤    \Gamma\in\mathbb{Z}   (half spins are up, half spins are down). Also notice that in this system,    âŸ¨  Q  âŸ©     delimited-âŸ¨âŸ©  Q    \langle Q\rangle   }}     The computation of an average quantity     âŸ¨  Q  âŸ©   =    1  V     âˆ«  Î©    Q   (  ð’“  )   p   (  ð’“  )   d  ð’“          delimited-âŸ¨âŸ©  Q       1  V     subscript   normal-Î©     Q  ð’“  p  ð’“  d  ð’“       \langle Q\rangle=\frac{1}{V}\int_{\Omega}Q(\boldsymbol{r})p(\boldsymbol{r})\,d%
 \boldsymbol{r}   over the phase-space requires the evaluation of an integral:      p   (  ð’“  )       p  ð’“    p(\boldsymbol{r})     where      p   (  ð’“  )    =    1  V     âˆ«  Î©    p   (  ð’“  )   d  ð’“      .        p  ð’“       1  V     subscript   normal-Î©     p  ð’“  d  ð’“       p(\boldsymbol{r})=\frac{1}{V}\int_{\Omega}p(\boldsymbol{r})\,d\boldsymbol{r}.   is weight of each state per volume,       Ï   (  f  )    =    1  V     âˆ«  Î©    Î´   (   f  -   F   (  ð’“  )     )   d  ð’“           Ï  f       1  V     subscript   normal-Î©     Î´    f    F  ð’“    d  ð’“       \rho(f)=\frac{1}{V}\int_{\Omega}\delta(f-F(\boldsymbol{r}))\,d\boldsymbol{r}     The density of states in respect with F is given by       F   (  ð’“  )    =   F  ð’“         F  ð’“    subscript  F  ð’“     F(\boldsymbol{r})=F_{\boldsymbol{r}}     which means that if both Q and p do not depend on the particular state but only on the particular F's value of the state       Q   (  ð’“  )    =   Q   (   F  ð’“   )     ,    p   (  ð’“  )    =   p   (   F  ð’“   )      ,     formulae-sequence      Q  ð’“     Q   subscript  F  ð’“         p  ð’“     p   subscript  F  ð’“       Q(\boldsymbol{r})=Q(F_{\boldsymbol{r}}),p(\boldsymbol{r})=p(F_{\boldsymbol{r}}),   ,      âŸ¨  Q  âŸ©     delimited-âŸ¨âŸ©  Q    \langle Q\rangle     the formula for    âŸ¨  Q  âŸ©     delimited-âŸ¨âŸ©  Q    \displaystyle\langle Q\rangle   can be integrated over f by adding a dirac delta function ,     Î©   normal-Î©   \Omega     i.e. the knowledge of the density of states allows the computation of averages over F using a one-dimensional integral instead of a multidimensional integral as it is the projection of the number of states on   Î“   normal-Î“   \Gamma   in   Î²   Î²   \beta   .      {{show   example:   A system in contact with a heat bath at inverse temperature     âŸ¨  E  âŸ©   =    1  V     âˆ«  Î©    H   (  ð’“  )      e   -   Î²  H   (  ð’“  )      Z    d  ð’“          delimited-âŸ¨âŸ©  E       1  V     subscript   normal-Î©     H  ð’“     superscript  e      Î²  H  ð’“     Z   d  ð’“       \langle E\rangle=\frac{1}{V}\int_{\Omega}H(\boldsymbol{r})\frac{e^{-\beta H(%
 \boldsymbol{r})}}{Z}\,d\boldsymbol{r}   is a clear example for computing this kind of integral. For instance, the mean energy of the system is weighted by the Boltzmann factor :       Z  =    1  V     âˆ«  Î©      e   -   Î²  H   (  ð’“  )       d  ð’“      .      Z      1  V     subscript   normal-Î©      superscript  e      Î²  H  ð’“     d  ð’“       Z=\frac{1}{V}\int_{\Omega}e^{-\beta H(\boldsymbol{r})}\,d\boldsymbol{r}.   where      Ï   (  E  )       Ï  E    \rho(E)   The density of states    âŸ¨  E  âŸ©     delimited-âŸ¨âŸ©  E    \langle E\rangle   can used to compute     âŸ¨  E  âŸ©   =    âˆ«   E  min    E  max     E    e   -   Î²  E     Z   Ï   (  E  )   d  E         delimited-âŸ¨âŸ©  E     superscript   subscript    subscript  E      subscript  E       E     superscript  e      Î²  E     Z   Ï  E  d  E      \langle E\rangle=\int_{E_{\min}}^{E_{\max}}E\frac{e^{-\beta E}}{Z}\rho(E)\,dE   :      âŸ¨  Q  âŸ©     delimited-âŸ¨âŸ©  Q    \langle Q\rangle   }}     Because the number of states can be high for systems with high number of degrees of freedom, an analytical expression can be hard to obtain and the computation of     ð’“  i   âˆˆ  Î©       subscript  ð’“  i   normal-Î©    \boldsymbol{r}_{i}\in\Omega   can be expensive. Typically, because the problem is a multidimensional integral, Monte Carlo integration is normally employed. On the simplest formulation, the method chooses N uniform random states      Q  Â¯   N   =    1  N     âˆ‘   i  =  0   N    Q   (   ð’“  i   )   p   (   ð’“  i   )           subscript   normal-Â¯  Q   N       1  N     superscript   subscript     i  0    N     Q   subscript  ð’“  i   p   subscript  ð’“  i        \overline{Q}_{N}=\frac{1}{N}\sum_{i=0}^{N}Q(\boldsymbol{r}_{i})p(\boldsymbol{r%
 }_{i})   , and uses the estimator      âŸ¨  Q  âŸ©     delimited-âŸ¨âŸ©  Q    \langle Q\rangle     for computing     Q  Â¯   N     subscript   normal-Â¯  Q   N    \overline{Q}_{N}       âŸ¨  Q  âŸ©     delimited-âŸ¨âŸ©  Q    \langle Q\rangle   converges almost surely to       lim   N  â†’  âˆž      Q  Â¯   N    =   âŸ¨  Q  âŸ©    .        subscript    normal-â†’  N      subscript   normal-Â¯  Q   N     delimited-âŸ¨âŸ©  Q     \lim_{N\rightarrow\infty}\overline{Q}_{N}=\langle Q\rangle.   by the strong law of large numbers :       Q  Â¯   N     subscript   normal-Â¯  Q   N    \overline{Q}_{N}     One typical problem of this convergence is that the variance of Q can be very high, which leads to a high computational effort to achieve reasonable results.      {{show   example   On the previous example, the states that mostly contribute to the integral are the ones with low energy. If the states are sampled uniformly, on average, the number of states which are sampled with energy E is given by the density of states. This density of states can be centered far away from the energy's minima and thus the average can be difficult to obtain. }}     To improve this convergence, the Metropolisâ€“Hastings algorithm was proposed. Generally, Monte Carlo methods' idea is to use importance sampling to improve the convergence of the estimator    P   (  ð’“  )       P  ð’“    P(\boldsymbol{r})   by sampling states according to an arbitrary distribution      Q  Â¯   N   =    1  X     âˆ‘   i  =  0   N    Q   (   ð’“  i   )    P   -  1     (   ð’“  i   )   p   (   ð’“  i   )           subscript   normal-Â¯  Q   N       1  X     superscript   subscript     i  0    N     Q   subscript  ð’“  i    superscript  P    1     subscript  ð’“  i   p   subscript  ð’“  i        \overline{Q}_{N}=\frac{1}{X}\sum_{i=0}^{N}Q(\boldsymbol{r}_{i})P^{-1}(%
 \boldsymbol{r}_{i})p(\boldsymbol{r}_{i})   (notice the capital P , different from p ), and use the appropriate estimator:      X  =    âˆ‘   i  =  0   N     P   -  1     (   ð’“  i   )         X    superscript   subscript     i  0    N      superscript  P    1     subscript  ð’“  i       X=\sum_{i=0}^{N}P^{-1}(\boldsymbol{r}_{i})     where     P   (  ð’“  )    =    p  Boltzmann    (  ð’“  )    =    e   -   Î²  F   (  ð’“  )         âˆ«  Î©     d  ð’“   e   -   Î²  F   (  ð’“  )                 P  ð’“      subscript  p  Boltzmann   ð’“           superscript  e      Î²  F  ð’“       subscript   normal-Î©     d  ð’“   superscript  e      Î²  F  ð’“           P(\boldsymbol{r})=p_{\mathrm{Boltzmann}}(\boldsymbol{r})=\frac{e^{-\beta F(%
 \boldsymbol{r})}}{\int_{\Omega}\,d\boldsymbol{r}e^{-\beta F(\boldsymbol{r})}}   .  Notice that when P is a uniform distribution, this estimator equals the one used on a uniform sampling, as it should.  One important choice of P is to define an arbitrary temperature, and use it equals to Boltzmann factor with the energy being the cost function:       P   (  ð’“  )    =   1   Ï   (   F   (  ð’“  )    )           P  ð’“     1    Ï    F  ð’“       P(\boldsymbol{r})=\frac{1}{\rho(F(\boldsymbol{r}))}     I.e. the lower the cost function of a particular state, the more likely it is to be sampled.  Historically, this occurred because the original idea 15 was exactly to use Metropolisâ€“Hastings algorithm to compute averages on a system in contact with a heat bath where the weight is given by the Boltzmann factor. On these systems, the choice of states according to it led to a considerable improvement on studying physical systems. 16  However, it is not true that the sampling distribution must equals the weight distribution. One reason for this is that Metropolisâ€“Hastings algorithm fails to converge when the cost function has multiple minima. 17 The reason is that the algorithm uses a random walk with local steps. I.e. the random walk normally performs steps whose energy difference is of order 1. This means that the computational cost to the algorithm leave a specific region with a local minimum exponentially increases with the cost function's value of the minimum. I.e. the deeper the minimum, the more time the algorithm spends there, and harder (exponentially with deep) it will leave.  This is the motivation to introduce a multicanonic ensemble. The idea is to avoid becoming stuck on local minima of the cost function by making them "invisible" to the sampling technique.  Multicanonic ensemble  A multicanonic ensemble is choosing the sampling distribution used in the importance sampling to be       Ï   (  f  )    =    1  V     âˆ«  Î©    Î´   (    F   (  ð’“  )    -  f   )   d  ð’“           Ï  f       1  V     subscript   normal-Î©     Î´      F  ð’“   f   d  ð’“       \rho(f)=\frac{1}{V}\int_{\Omega}\delta(F(\boldsymbol{r})-f)\,d\boldsymbol{r}     where       P   (  f  )    =    1    f  max   -   f  min       âˆ«  Î©    Î´   (   f  -   F   (  ð’“  )     )   P   (  ð’“  )   d  ð’“     =    1    f  max   -   f  min      1  V   Ï   (  f  )     âˆ«  Î©    Î´   (   f  -   F   (  ð’“  )     )   d  ð’“     =   1    f  max   -   f  min     =  constant          P  f       1     subscript  f     subscript  f        subscript   normal-Î©     Î´    f    F  ð’“    P  ð’“  d  ð’“              1     subscript  f     subscript  f        1  V   Ï  f    subscript   normal-Î©     Î´    f    F  ð’“    d  ð’“            1     subscript  f     subscript  f           constant     P(f)=\frac{1}{f_{\max}-f_{\min}}\int_{\Omega}\delta(f-F(\boldsymbol{r}))P(%
 \boldsymbol{r})\,d\boldsymbol{r}=\frac{1}{f_{\max}-f_{\min}}\frac{1}{V}\rho(f)%
 \int_{\Omega}\delta(f-F(\boldsymbol{r}))\,d\boldsymbol{r}=\frac{1}{f_{\max}-f_%
 {\min}}=\text{constant}     is the density of states of the system with respect to the cost function ( V is the phase-space volume). The consequence of this choice is that the probability to sample a state and it has cost function f is      P   (  ð’“  )       P  ð’“    P(\boldsymbol{r})     which motivates the name "flat histogram". I.e. all costs are equally sampled, and thus there are no barriers. For systems in contact with a heat bath, there is another important advantage: because the sampling is independent of the temperature, one simulation is enough to study all temperatures (thus the name "multicanonic": several temperatures).      {{show   example:   On the ferromagnetic Ising model with N sites (exemplified on previous section), the density of states can be analytically computed. In this case, a multicanonic ensemble can be used to compute any other quantity Q by sampling the system according to    Q  Â¯     normal-Â¯  Q    \overline{Q}   and using the proper estimator    P   (  ð’“  )       P  ð’“    P(\boldsymbol{r})   defined on the previous section. }}     Tunneling time and critical slowing down  Like in any other Monte Carlo method, there are correlations of the samples being drawn from     Ï„   t  t    âˆ   N  2      proportional-to   subscript  Ï„    t  t     superscript  N  2     \tau_{tt}\propto N^{2}   . A typical measurement of the correlation is the tunneling time . The tunneling time is defined by the number of Markov steps (of the Markov chain) the simulation needs to perform a round-trip between the minimum and maximum of the spectrum of F . One motivation to use the tunneling time is that when it crosses the spectra, it passes through the region of the maximum of the density of states, thus de-correlating the process. On the other hand using round-trips ensures that the system visits all the spectrum.  Because the histogram is flat on the variable F , a multicanonic ensemble can be seen as a diffusion process (i.e. a random walk) on the one-dimensional line of F values. Detailed balance of the process dictates that there is no drift on the process. 18 This implies that the tunneling time, in local dynamics, should scale as a diffusion process, and thus the tunneling time should scale quadratically with the size of the spectrum, N :      N   2  +  z      superscript  N    2  z     N^{2+z}     However, in some systems (the Ising model being the most paradigmatic), the scaling suffers from critical slowing down: it is    z  >  0      z  0    z>0   where $z>0$ depends on the particular system. 19  Non-local dynamics were developed to improve the scaling to a quadratic scaling 20 (see wolff algorithm ), beating the critical slowing down. However, it is still an open question whether there is a local dynamics that does not suffer from critical slowing down in spin systems like Ising model.  References  "  Category:Monte Carlo methods  Category:Statistical algorithms  Category:Computational physics                            