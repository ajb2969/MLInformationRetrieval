   Generalized vector space model      Generalized vector space model   The Generalized vector space model is a generalization of the vector space model used in information retrieval . Many classifiers, especially those which are related to document or text classification, use the TFIDF basis of VSM. However, this is where the similarity between the models ends - the generalized model uses the results of the TFIDF dictionary to generate similarity metrics based on distance or angle difference, rather than centroid based classification. Wong et al. 1 presented an analysis of the problems that the pairwise orthogonality assumption of the vector space model (VSM) creates. From here they extended the VSM to the generalized vector space model (GVSM).  Definitions  GVSM introduces a term to term correlations, which deprecate the pairwise orthogonality assumption. More specifically, the factor considered a new space, where each term vector t i was expressed as a linear combination of 2 n vectors m r where r = 1...2 n .  For a document d k and a query q the similarity function now becomes:       s  i  m   (   d  k   ,  q  )    =     ∑   j  =  1   n     ∑   i  =  1   n      w   i  ,  k    *   w   j  ,  q    *   t  i    ⋅   t  j          ∑   i  =  1   n    w   i  ,  k   2     *     ∑   i  =  1   n    w   i  ,  q   2             s  i  m    subscript  d  k   q        superscript   subscript     j  1    n     superscript   subscript     i  1    n    normal-⋅     subscript  w   i  k     subscript  w   j  q     subscript  t  i     subscript  t  j            superscript   subscript     i  1    n    superscript   subscript  w   i  k    2         superscript   subscript     i  1    n    superscript   subscript  w   i  q    2         sim(d_{k},q)=\frac{\sum_{j=1}^{n}\sum_{i=1}^{n}w_{i,k}*w_{j,q}*t_{i}\cdot t_{j%
 }}{\sqrt{\sum_{i=1}^{n}w_{i,k}^{2}}*\sqrt{\sum_{i=1}^{n}w_{i,q}^{2}}}     where t i and t j are now vectors of a 2 n dimensional space.  Term correlation     t  i   ⋅   t  j      normal-⋅   subscript  t  i    subscript  t  j     t_{i}\cdot t_{j}   can be implemented in several ways. For an example, Wong et al. uses the term occurrence frequency matrix obtained from automatic indexing as input to their algorithm. The term occurrence and the output is the term correlation between any pair of index terms.  Semantic information on GVSM  There are at least two basic directions for embedding term to term relatedness, other than exact keyword matching, into a retrieval model:   compute semantic correlations between terms  compute frequency co-occurrence statistics from large corpora   Recently Tsatsaronis 2 focused on the first approach.  They measure semantic relatedness ( SR ) using a thesaurus ( O ) like WordNet . It considers the path length, captured by compactness ( SCM ), and the path depth, captured by semantic path elaboration ( SPE ). They estimate the     t  i   ⋅   t  j      normal-⋅   subscript  t  i    subscript  t  j     t_{i}\cdot t_{j}   inner product by:        t  i   ⋅   t  j    =   S  R   (   (   t  i   ,   t  j   )   ,   (   s  i   ,   s  j   )   ,  O  )         normal-⋅   subscript  t  i    subscript  t  j      S  R     subscript  t  i    subscript  t  j      subscript  s  i    subscript  s  j    O      t_{i}\cdot t_{j}=SR((t_{i},t_{j}),(s_{i},s_{j}),O)     where s i and s j are senses of terms t i and t j respectively, maximizing      S  C  M   ⋅  S   P  E       normal-⋅    S  C  M   S   P  E    SCM\cdot SPE   .  References  "  Category:Vector space model     ↩  ↩     