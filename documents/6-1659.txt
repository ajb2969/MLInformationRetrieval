   Euler method      Euler method   In mathematics and computational science , the Euler method is a SN-order numerical procedure for solving ordinary differential equations (ODEs) with a given initial value . It is the most basic explicit method for numerical integration of ordinary differential equations and is the simplest Runge‚ÄìKutta method . The Euler method is named after Leonhard Euler , who treated it in his book Institutionum calculi integralis (published 1768‚Äì70). 1  The Euler method is a first-order method, which means that the local error (error per step) is proportional to the square of the step size, and the global error (error at a given time) is proportional to the step size. The Euler method often serves as the basis to construct more complex methods.  Informal geometrical description  Consider the problem of calculating the shape of an unknown curve which starts at a given point and satisfies a given differential equation. Here, a differential equation can be thought of as a formula by which the slope of the tangent line to the curve can be computed at any point on the curve, once the position of that point has been calculated.  The idea is that while the curve is initially unknown, its starting point, which we denote by     A  0   ,     subscript  A  0    A_{0},   is known (see the picture on top right). Then, from the differential equation, the slope to the curve at    A  0     subscript  A  0    A_{0}   can be computed, and so, the tangent line.  Take a small step along that tangent line up to a point     A  1   .     subscript  A  1    A_{1}.   Along this small step, the slope does not change too much, so    A  1     subscript  A  1    A_{1}   will be close to the curve. If we pretend that    A  1     subscript  A  1    A_{1}   is still on the curve, the same reasoning as for the point    A  0     subscript  A  0    A_{0}   above can be used. After several steps, a polygonal curve      A  0    A  1    A  2    A  3   ‚Ä¶       subscript  A  0    subscript  A  1    subscript  A  2    subscript  A  3   normal-‚Ä¶    A_{0}A_{1}A_{2}A_{3}\dots   is computed. In general, this curve does not diverge too far from the original unknown curve, and the error between the two curves can be made small if the step size is small enough and the interval of computation is finite. 2  Formulation of the method  Suppose that we want to approximate the solution of the initial value problem          y  ‚Ä≤    (  t  )    =   f   (  t  ,   y   (  t  )    )     ,    y   (   t  0   )    =   y  0     .     formulae-sequence       superscript  y  normal-‚Ä≤   t     f   t    y  t          y   subscript  t  0     subscript  y  0      y^{\prime}(t)=f(t,y(t)),\qquad\qquad y(t_{0})=y_{0}.     Choose a value   h   h   h   for the size of every step and set     t  n   =    t  0   +   n  h         subscript  t  n      subscript  t  0     n  h      t_{n}=t_{0}+nh   . Now, one step of the Euler method from    t  n     subscript  t  n    t_{n}   to     t   n  +  1    =    t  n   +  h        subscript  t    n  1       subscript  t  n   h     t_{n+1}=t_{n}+h   is 3        y   n  +  1    =    y  n   +   h  f   (   t  n   ,   y  n   )      .       subscript  y    n  1       subscript  y  n     h  f    subscript  t  n    subscript  y  n        y_{n+1}=y_{n}+hf(t_{n},y_{n}).     The value of    y  n     subscript  y  n    y_{n}   is an approximation of the solution to the ODE at time    t  n     subscript  t  n    t_{n}        y  n   ‚âà   y   (   t  n   )         subscript  y  n     y   subscript  t  n      y_{n}\approx y(t_{n})   . The Euler method is explicit , i.e. the solution    y   n  +  1      subscript  y    n  1     y_{n+1}   is an explicit function of    y  i     subscript  y  i    y_{i}   for    i  ‚â§  n      i  n    i\leq n   .  While the Euler method integrates a first-order ODE, any ODE of order N can be represented as a first-order ODE: to treat the equation        y   (  N  )     (  t  )    =   f   (  t  ,   y   (  t  )    ,    y  ‚Ä≤    (  t  )    ,  ‚Ä¶  ,    y   (   N  -  1   )     (  t  )    )           superscript  y  N   t     f   t    y  t      superscript  y  normal-‚Ä≤   t   normal-‚Ä¶     superscript  y    N  1    t       y^{(N)}(t)=f(t,y(t),y^{\prime}(t),\ldots,y^{(N-1)}(t))   ,  we introduce auxiliary variables       z  1    (  t  )    =   y   (  t  )     ,      z  2    (  t  )    =     y  ‚Ä≤    (  t  )    ,  ‚Ä¶    ,     z  N    (  t  )    =    y   (   N  -  1   )     (  t  )         formulae-sequence       subscript  z  1   t     y  t     formulae-sequence       subscript  z  2   t       superscript  y  normal-‚Ä≤   t   normal-‚Ä¶         subscript  z  N   t      superscript  y    N  1    t       z_{1}(t)=y(t),z_{2}(t)=y^{\prime}(t),\ldots,z_{N}(t)=y^{(N-1)}(t)   and obtain the equivalent equation        ùê≥  ‚Ä≤    (  t  )    =   (       z  1  ‚Ä≤    (  t  )        ‚ãÆ        z   N  -  1   ‚Ä≤    (  t  )          z  N  ‚Ä≤    (  t  )       )   =   (       y  ‚Ä≤    (  t  )        ‚ãÆ        y   (   N  -  1   )     (  t  )          y   (  N  )     (  t  )       )   =   (       z  2    (  t  )        ‚ãÆ        z  N    (  t  )         f   (  t  ,    z  1    (  t  )    ,  ‚Ä¶  ,    z  N    (  t  )    )       )            superscript  ùê≥  normal-‚Ä≤   t        superscript   subscript  z  1   normal-‚Ä≤   t     normal-‚ãÆ       superscript   subscript  z    N  1    normal-‚Ä≤   t        superscript   subscript  z  N   normal-‚Ä≤   t               superscript  y  normal-‚Ä≤   t     normal-‚ãÆ       superscript  y    N  1    t        superscript  y  N   t               subscript  z  2   t     normal-‚ãÆ       subscript  z  N   t       f   t     subscript  z  1   t   normal-‚Ä¶     subscript  z  N   t          \mathbf{z}^{\prime}(t)=\begin{pmatrix}z_{1}^{\prime}(t)\\
 \vdots\\
 z_{N-1}^{\prime}(t)\\
 z_{N}^{\prime}(t)\end{pmatrix}=\begin{pmatrix}y^{\prime}(t)\\
 \vdots\\
 y^{(N-1)}(t)\\
 y^{(N)}(t)\end{pmatrix}=\begin{pmatrix}z_{2}(t)\\
 \vdots\\
 z_{N}(t)\\
 f(t,z_{1}(t),\ldots,z_{N}(t))\end{pmatrix}     This is a first-order system in the variable    ùê≥   (  t  )       ùê≥  t    \mathbf{z}(t)   and can be handled by Euler's method or, in fact, by any other scheme for first-order systems. 4  Example  Given the initial value problem         y  ‚Ä≤   =  y   ,    y   (  0  )    =  1    ,     formulae-sequence     superscript  y  normal-‚Ä≤   y       y  0   1     y^{\prime}=y,\quad y(0)=1,     we would like to use the Euler method to approximate    y   (  4  )       y  4    y(4)   . 5  === Using step size equal to 1 ( h = 1) ===  The Euler method is       y   n  +  1    =   y  n   +  h  f   (   t  n   ,   y  n   )   .     fragments   subscript  y    n  1      subscript  y  n    h  f   fragments  normal-(   subscript  t  n   normal-,   subscript  y  n   normal-)   normal-.  italic-    y_{n+1}=y_{n}+hf(t_{n},y_{n}).\qquad\qquad     so first we must compute    f   (   t  0   ,   y  0   )       f    subscript  t  0    subscript  y  0      f(t_{0},y_{0})   . In this simple differential equation, the function   f   f   f   is defined by     f   (  t  ,  y  )    =  y        f   t  y    y    f(t,y)=y   . We have        f   (   t  0   ,   y  0   )    =   f   (  0  ,  1  )    =  1.           f    subscript  t  0    subscript  y  0       f   0  1         1.     f(t_{0},y_{0})=f(0,1)=1.\qquad\qquad     By doing the above step, we have found the slope of the line that is tangent to the solution curve at the point    (  0  ,  1  )     0  1    (0,1)   . Recall that the slope is defined as the change in   y   y   y   divided by the change in   t   t   t   , or      Œî  y   /  Œî   t          normal-Œî  y   normal-Œî   t    \Delta y/\Delta t   .  The next step is to multiply the above value by the step size   h   h   h   , which we take equal to one here:         h  ‚ãÖ  f    (   y  0   )    =   1  ‚ãÖ  1   =  1.            normal-‚ãÖ  h  f    subscript  y  0     normal-‚ãÖ  1  1        1.     h\cdot f(y_{0})=1\cdot 1=1.\qquad\qquad     Since the step size is the change in   t   t   t   , when we multiply the step size and the slope of the tangent, we get a change in   y   y   y   value. This value is then added to the initial   y   y   y   value to obtain the next value to be used for computations.         y  0   +   h  f   (   y  0   )     =   y  1   =   1  +   1  ‚ãÖ  1    =  2.            subscript  y  0     h  f   subscript  y  0      subscript  y  1          1   normal-‚ãÖ  1  1         2.     y_{0}+hf(y_{0})=y_{1}=1+1\cdot 1=2.\qquad\qquad     The above steps should be repeated to find    y  2     subscript  y  2    y_{2}   ,    y  3     subscript  y  3    y_{3}   and    y  4     subscript  y  4    y_{4}   .      y  2     subscript  y  2    \displaystyle y_{2}     Due to the repetitive nature of this algorithm, it can be helpful to organize computations in a chart form, as seen below, to avoid making errors.           n   n   n          y  n     subscript  y  n    y_{n}          t  n     subscript  t  n    t_{n}          f   (   t  n   ,   y  n   )       f    subscript  t  n    subscript  y  n      f(t_{n},y_{n})         h   h   h          Œî  y      normal-Œî  y    \Delta y          y   n  +  1      subscript  y    n  1     y_{n+1}          0   1   0   1   1   1   2     1   2   1   2   1   2   4     2   4   2   4   1   4   8     3   8   3   8   1   8   16       The conclusion of this computation is that     y  4   =  16       subscript  y  4   16    y_{4}=16   . The exact solution of the differential equation is     y   (  t  )    =   e  t         y  t    superscript  e  t     y(t)=e^{t}   , so     y   (  4  )    =   e  4   ‚âà  54.598          y  4    superscript  e  4        54.598     y(4)=e^{4}\approx 54.598   . Thus, the approximation of the Euler method is not very good in this case. However, as the figure shows, its behaviour is qualitatively right.  Using other step sizes  As suggested in the introduction, the Euler method is more accurate if the step size   h   h   h   is smaller. The table below shows the result with different step sizes. The top row corresponds to the example in the previous section, and the second row is illustrated in the figure.        step size   result of Euler's method   error       1   16   38.598     0.25   35.53   19.07     0.1   45.26   9.34     0.05   49.56   5.04     0.025   51.98   2.62     0.0125   53.26   1.34       The error recorded in the last column of the table is the difference between the exact solution at    t  =  4      t  4    t=4   and the Euler approximation. In the bottom of the table, the step size is half the step size in the previous row, and the error is also approximately half the error in the previous row. This suggests that the error is roughly proportional to the step size, at least for fairly small values of the step size. This is true in general, also for other equations; see the section Global truncation error for more details.  Other methods, such as the midpoint method also illustrated in the figures, behave more favourably: the error of the midpoint method is roughly proportional to the square of the step size. For this reason, the Euler method is said to be a first-order method, while the midpoint method is second order.  We can extrapolate from the above table that the step size needed to get an answer that is correct to three decimal places is approximately 0.00001, meaning that we need 400,000 steps. This large number of steps entails a high computational cost. For this reason, people usually employ alternative, higher-order methods such as Runge‚ÄìKutta methods or linear multistep methods , especially if a high accuracy is desired. 6  Derivation  The Euler method can be derived in a number of ways. Firstly, there is the geometrical description mentioned above.  Another possibility is to consider the Taylor expansion of the function   y   y   y   around    t  0     subscript  t  0    t_{0}   :        y   (    t  0   +  h   )    =    y   (   t  0   )    +   h   y  ‚Ä≤    (   t  0   )    +    1  2    h  2    y  ‚Ä≤‚Ä≤    (   t  0   )    +   O   (   h  3   )      .        y     subscript  t  0   h        y   subscript  t  0      h   superscript  y  normal-‚Ä≤    subscript  t  0        1  2    superscript  h  2    superscript  y  ‚Ä≤‚Ä≤    subscript  t  0      O   superscript  h  3       y(t_{0}+h)=y(t_{0})+hy^{\prime}(t_{0})+\frac{1}{2}h^{2}y^{\prime\prime}(t_{0})%
 +O(h^{3}).     The differential equation states that     y  ‚Ä≤   =   f   (  t  ,  y  )         superscript  y  normal-‚Ä≤     f   t  y      y^{\prime}=f(t,y)   . If this is substituted in the Taylor expansion and the quadratic and higher-order terms are ignored, the Euler method arises. 7 The Taylor expansion is used below to analyze the error committed by the Euler method, and it can be extended to produce Runge‚ÄìKutta methods .  A closely related derivation is to substitute the forward finite difference formula for the derivative,        y  ‚Ä≤    (   t  0   )    ‚âà     y   (    t  0   +  h   )    -   y   (   t  0   )     h          superscript  y  normal-‚Ä≤    subscript  t  0          y     subscript  t  0   h      y   subscript  t  0     h     y^{\prime}(t_{0})\approx\frac{y(t_{0}+h)-y(t_{0})}{h}     in the differential equation     y  ‚Ä≤   =   f   (  t  ,  y  )         superscript  y  normal-‚Ä≤     f   t  y      y^{\prime}=f(t,y)   . Again, this yields the Euler method. 8 A similar computation leads to the midpoint rule and the backward Euler method .  Finally, one can integrate the differential equation from    t  0     subscript  t  0    t_{0}   to     t  0   +  h       subscript  t  0   h    t_{0}+h   and apply the fundamental theorem of calculus to get:         y   (    t  0   +  h   )    -   y   (   t  0   )     =    ‚à´   t  0     t  0   +  h     f   (  t  ,   y   (  t  )    )   d  t     .          y     subscript  t  0   h      y   subscript  t  0       superscript   subscript    subscript  t  0       subscript  t  0   h      f   t    y  t    normal-d  t      y(t_{0}+h)-y(t_{0})=\int_{t_{0}}^{t_{0}+h}f(t,y(t))\,\mathrm{d}t.     Now approximate the integral by the left-hand rectangle method (with only one rectangle):         ‚à´   t  0     t  0   +  h     f   (  t  ,   y   (  t  )    )   d  t    ‚âà   h  f   (   t  0   ,   y   (   t  0   )    )     .        superscript   subscript    subscript  t  0       subscript  t  0   h      f   t    y  t    normal-d  t      h  f    subscript  t  0     y   subscript  t  0        \int_{t_{0}}^{t_{0}+h}f(t,y(t))\,\mathrm{d}t\approx hf(t_{0},y(t_{0})).     Combining both equations, one finds again the Euler method. 9 This line of thought can be continued to arrive at various linear multistep methods .  Local truncation error  The local truncation error of the Euler method is error made in a single step. It is the difference between the numerical solution after one step,    y  1     subscript  y  1    y_{1}   , and the exact solution at time     t  1   =    t  0   +  h        subscript  t  1      subscript  t  0   h     t_{1}=t_{0}+h   . The numerical solution is given by       y  1   =   y  0   +  h  f   (   t  0   ,   y  0   )   .     fragments   subscript  y  1     subscript  y  0    h  f   fragments  normal-(   subscript  t  0   normal-,   subscript  y  0   normal-)   normal-.     y_{1}=y_{0}+hf(t_{0},y_{0}).\quad     For the exact solution, we use the Taylor expansion mentioned in the section Derivation above:        y   (    t  0   +  h   )    =    y   (   t  0   )    +   h   y  ‚Ä≤    (   t  0   )    +    1  2    h  2    y  ‚Ä≤‚Ä≤    (   t  0   )    +   O   (   h  3   )      .        y     subscript  t  0   h        y   subscript  t  0      h   superscript  y  normal-‚Ä≤    subscript  t  0        1  2    superscript  h  2    superscript  y  ‚Ä≤‚Ä≤    subscript  t  0      O   superscript  h  3       y(t_{0}+h)=y(t_{0})+hy^{\prime}(t_{0})+\frac{1}{2}h^{2}y^{\prime\prime}(t_{0})%
 +O(h^{3}).     The local truncation error (LTE) introduced by the Euler method is given by the difference between these equations:       LTE  =    y   (    t  0   +  h   )    -   y  1    =     1  2    h  2    y  ‚Ä≤‚Ä≤    (   t  0   )    +   O   (   h  3   )      .        LTE      y     subscript  t  0   h     subscript  y  1               1  2    superscript  h  2    superscript  y  ‚Ä≤‚Ä≤    subscript  t  0      O   superscript  h  3        \mathrm{LTE}=y(t_{0}+h)-y_{1}=\frac{1}{2}h^{2}y^{\prime\prime}(t_{0})+O(h^{3}).     This result is valid if   y   y   y   has a bounded third derivative. 10  This shows that for small   h   h   h   , the local truncation error is approximately proportional to    h  2     superscript  h  2    h^{2}   . This makes the Euler method less accurate (for small   h   h   h   ) than other higher-order techniques such as Runge-Kutta methods and linear multistep methods , for which the local truncation error is proportial to a higher power of the step size.  A slightly different formulation for the local truncation error can be obtained by using the Lagrange form for the remainder term in Taylor's theorem . If   y   y   y   has a continuous second derivative, then there exists a    Œæ  ‚àà   [   t  0   ,    t  0   +  h   ]       Œæ    subscript  t  0      subscript  t  0   h      \xi\in[t_{0},t_{0}+h]   such that       LTE  =    y   (    t  0   +  h   )    -   y  1    =    1  2    h  2    y  ‚Ä≤‚Ä≤    (  Œæ  )     .        LTE      y     subscript  t  0   h     subscript  y  1             1  2    superscript  h  2    superscript  y  ‚Ä≤‚Ä≤   Œæ      \mathrm{LTE}=y(t_{0}+h)-y_{1}=\frac{1}{2}h^{2}y^{\prime\prime}(\xi).    11  In the above expressions for the error, the second derivative of the unknown exact solution   y   y   y   can be replaced by an expression involving the right-hand side of the differential equation. Indeed, it follows from the equation     y  ‚Ä≤   =   f   (  t  ,  y  )         superscript  y  normal-‚Ä≤     f   t  y      y^{\prime}=f(t,y)   that         y  ‚Ä≤‚Ä≤    (   t  0   )    =      ‚àÇ  f    ‚àÇ  t     (   t  0   ,   y   (   t  0   )    )    +     ‚àÇ  f    ‚àÇ  y     (   t  0   ,   y   (   t  0   )    )   f   (   t  0   ,   y   (   t  0   )    )      .         superscript  y  ‚Ä≤‚Ä≤    subscript  t  0            f     t      subscript  t  0     y   subscript  t  0            f     y      subscript  t  0     y   subscript  t  0     f    subscript  t  0     y   subscript  t  0         y^{\prime\prime}(t_{0})={\partial f\over\partial t}(t_{0},y(t_{0}))+{\partial f%
 \over\partial y}(t_{0},y(t_{0}))\,f(t_{0},y(t_{0})).    12  Global truncation error  The global truncation error is the error at a fixed time   t   t   t   , after however many steps the methods needs to take to reach that time from the initial time. The global truncation error is the cumulative effect of the local truncation errors committed in each step. 13 The number of steps is easily determined to be     (   t  -   t  0    )   /  h        t   subscript  t  0    h    (t-t_{0})/h   , which is proportional to    1  /  h      1  h    1/h   , and the error committed in each step is proportional to    h  2     superscript  h  2    h^{2}   (see the previous section). Thus, it is to be expected that the global truncation error will be proportional to   h   h   h   . 14  This intuitive reasoning can be made precise. If the solution   y   y   y   has a bounded second derivative and   f   f   f   is Lipschitz continuous in its second argument, then the global truncation error (GTE) is bounded by        |  GTE  |   ‚â§     h  M    2  L     (    e   L   (   t  -   t  0    )     -  1   )           GTE         h  M     2  L       superscript  e    L    t   subscript  t  0      1      |\text{GTE}|\leq\frac{hM}{2L}(e^{L(t-t_{0})}-1)\qquad\qquad     where   M   M   M   is an upper bound on the second derivative of   y   y   y   on the given interval and   L   L   L   is the Lipschitz constant of   f   f   f   . 15  The precise form of this bound of little practical importance, as in most cases the bound vastly overestimates the actual error committed by the Euler method. 16 What is important is that it shows that the global truncation error is (approximately) proportional to   h   h   h   . For this reason, the Euler method is said to be first order. 17  Numerical stability  The Euler method can also be numerically unstable , especially for stiff equations , meaning that the numerical solution grows very large for equations where the exact solution does not. This can be illustrated using the linear equation        y  ‚Ä≤   =   -   2.3  y     ,    y   (  0  )    =  1.      formulae-sequence     superscript  y  normal-‚Ä≤       2.3  y         y  0   1.     y^{\prime}=-2.3y,\qquad y(0)=1.   The exact solution is     y   (  t  )    =   e   -   2.3  t           y  t    superscript  e      2.3  t       y(t)=e^{-2.3t}   , which decays to zero as    t  ‚Üí  ‚àû     normal-‚Üí  t     t\to\infty   . However, if the Euler method is applied to this equation with step size    h  =  1      h  1    h=1   , then the numerical solution is qualitatively wrong: it oscillates and grows (see the figure). This is what it means to be unstable. If a smaller step size is used, for instance    h  =  0.7      h  0.7    h=0.7   , then the numerical solution does decay to zero.  If the Euler method is applied to the linear equation     y  ‚Ä≤   =   k  y        superscript  y  normal-‚Ä≤     k  y     y^{\prime}=ky   , then the numerical solution is unstable if the product    h  k      h  k    hk   is outside the region       {   z  ‚àà  ùêÇ   ‚à£    |   z  +  1   |   ‚â§  1   }   ,     conditional-set    z  ùêÇ         z  1    1     \{z\in\mathbf{C}\mid|z+1|\leq 1\},   illustrated on the right. This region is called the (linear) instability region. 18 In the example,   k   k   k   equals ‚àí2.3, so if    h  =  1      h  1    h=1   then     h  k   =   -  2.3         h  k     2.3     hk=-2.3   which is outside the stability region, and thus the numerical solution is unstable.  This limitation ‚Äîalong with its slow convergence of error with h ‚Äî means that the Euler method is not often used, except as a simple example of numerical integration.  Rounding errors  The discussion up to now has ignored the consequences of rounding error . In step n of the Euler method, the rounding error is roughly of the magnitude Œµ y n where Œµ is the machine epsilon . Assuming that the rounding errors are all of approximately the same size, the combined rounding error in N steps is roughly N Œµ y 0 if all errors points in the same direction. Since the number of steps is inversely proportional to the step size h , the total rounding error is proportional to Œµ / h . In reality, however, it is extremely unlikely that all rounding errors point in the same direction. If instead it is assumed that the rounding errors are independent rounding variables, then the total rounding error is proportional to    Œµ  /   h       Œµ    h     \varepsilon/\sqrt{h}   . 19  Thus, for extremely small values of the step size, the truncation error will be small but the effect of rounding error may be big. Most of the effect of rounding error can be easily avoided if compensated summation is used in the formula for the Euler method. 20  Modifications and extensions  A simple modification of the Euler method which eliminates the stability problems noted in the previous section is the backward Euler method :        y   n  +  1    =    y  n   +   h  f   (   t   n  +  1    ,   y   n  +  1    )      .       subscript  y    n  1       subscript  y  n     h  f    subscript  t    n  1     subscript  y    n  1         y_{n+1}=y_{n}+hf(t_{n+1},y_{n+1}).   This differs from the (standard, or forward) Euler method in that the function   f   f   f   is evaluated at the end point of the step, instead of the starting point. The backward Euler method is an implicit method , meaning that the formula for the backward Euler method has    y   n  +  1      subscript  y    n  1     y_{n+1}   on both sides, so when applying the backward Euler method we have to solve an equation. This makes the implementation more costly.  Other modifications of the Euler method that help with stability yield the exponential Euler method or the semi-implicit Euler method .  More complicated methods can achieve a higher order (and more accuracy). One possibility is to use more function evaluations. This is illustrated by the midpoint method which is already mentioned in this article:        y   n  +  1    =    y  n   +   h  f   (    t  n   +     1  2    h    ,    y  n   +     1  2    h  f   (   t  n   ,   y  n   )     )      .       subscript  y    n  1       subscript  y  n     h  f      subscript  t  n       1  2   h       subscript  y  n       1  2   h  f    subscript  t  n    subscript  y  n           y_{n+1}=y_{n}+hf\Big(t_{n}+\tfrac{1}{2}h,y_{n}+\tfrac{1}{2}hf(t_{n},y_{n})\Big).   This leads to the family of Runge‚ÄìKutta methods .  The other possibility is to use more past values, as illustrated by the two-step Adams‚ÄìBashforth method:        y   n  +  1    =     y  n   +     3  2    h  f   (   t  n   ,   y  n   )     -     1  2    h  f   (   t   n  -  1    ,   y   n  -  1    )      .       subscript  y    n  1         subscript  y  n       3  2   h  f    subscript  t  n    subscript  y  n          1  2   h  f    subscript  t    n  1     subscript  y    n  1         y_{n+1}=y_{n}+\tfrac{3}{2}hf(t_{n},y_{n})-\tfrac{1}{2}hf(t_{n-1},y_{n-1}).   This leads to the family of linear multistep methods .  See also   Dynamic errors of numerical methods of ODE discretization  Gradient descent similarly uses finite steps, here to find minima of functions  List of Runge-Kutta methods  Linear multistep method  Numerical integration (for calculating definite integrals)  Numerical methods for ordinary differential equations   Notes  References    .   .   .   .    .   .   External links   Euler's Method for O.D.E.'s , by John H. Matthews, California State University at Fullerton.  On line calculator for Euler's method by www.mathstools.com  Euler method implementations in different languages by Rosetta Code   "  Category:Numerical differential equations  Category:Runge‚ÄìKutta methods  Category:First order methods     ; ‚Ü©  ; ‚Ü©  ; ‚Ü©  ; ‚Ü©  See also ‚Ü©  ‚Ü©  ; ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ; ‚Ü©  ‚Ü©  ‚Ü©  ; ‚Ü©  ‚Ü©  ‚Ü©     