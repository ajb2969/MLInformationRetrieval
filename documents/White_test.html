<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1762">White test</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>White test</h1>
<hr/>

<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, the <strong>White test</strong> is a <a href="statistical_test" title="wikilink">statistical test</a> that establishes whether the <a href="errors_and_residuals_in_statistics" title="wikilink">residual</a> <a class="uri" href="variance" title="wikilink">variance</a> of a variable in a <a href="regression_model" title="wikilink">regression model</a> is constant: that is for <a class="uri" href="homoskedasticity" title="wikilink">homoskedasticity</a>.</p>

<p>This test, and an estimator for <a href="heteroskedasticity-consistent_standard_errors" title="wikilink">heteroskedasticity-consistent standard errors</a>, were proposed by <a href="Halbert_White" title="wikilink">Halbert White</a> in 1980.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> These methods have become extremely widely used, making this paper one of the most cited articles in economics.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>In cases where the White test statistics is statistically significant, heteroskedasticity may not necessarily be the cause, but specification errors. In other words, “The White test can be a test of heteroskedasticity or specification error or both." If no cross product terms are introduced in the White test procedure, then this is a pure test of pure heteroskedasticity. If cross product are introduced in model, then it is a test of both heteroskedasticity and specification bias.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="testing-constant-variance">Testing constant variance</h2>

<p>To test for constant variance one undertakes an auxiliary regression analysis: this regresses the squared residuals from the original regression model onto a set of <a href="regressor" title="wikilink">regressors</a> that contain the original regressors along with their squares and cross-products. One then inspects the <em>R</em><sup>2</sup>. The <a href="Lagrange_multiplier_test" title="wikilink">Lagrange multiplier (LM) test</a> statistic is the product of the <em>R</em><sup>2</sup> value and sample size:</p>

<p>

<math display="block" id="White_test:0">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mpadded lspace="5pt" width="+5pt">
      <mi>L</mi>
     </mpadded>
     <mi>M</mi>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mi>n</mi>
     <mo>⋅</mo>
     <msup>
      <mi>R</mi>
      <mn>2</mn>
     </msup>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>L</ci>
     <ci>M</ci>
    </apply>
    <apply>
     <ci>normal-⋅</ci>
     <ci>n</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>R</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ LM=n\cdot R^{2}.
  </annotation>
 </semantics>
</math>

 This follows a <a href="chi-squared_distribution" title="wikilink">chi-squared distribution</a>, with degrees of freedom equal to P-1, where P is the number of estimated parameters (in the auxiliary regression).</p>

<p>The logic of the test is as follows. First, the squared residuals from the original model serve as a proxy for the variance of the error term at each observation. (The error term is assumed to have a mean of zero, and the <a class="uri" href="variance" title="wikilink">variance</a> of a zero-mean random variable is just the expectation of its square.) The independent variables in the auxiliary regression account for the possibility that the error variance depends on the values of the original regressors in some way (linear or quadratic). If the error term in the original model is in fact homoskedastic (has a constant variance) then the coefficients in the auxiliary regression (besides the constant) should be statistically indistinguishable from zero and the <em>R</em><sup>2</sup> should be “small". Conversely, a “large" <em>R</em><sup>2</sup> (scaled by the sample size so that it follows the chi-squared distribution) counts against the hypothesis of homoskedasticity.</p>

<p>An alternative to the White test is the <a href="Breusch–Pagan_test" title="wikilink">Breusch–Pagan test</a>. Under certain conditions and a modification of one of the tests, they can be found to be algebraically equivalent.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>If homoskedasticity is rejected one can use <a href="heteroskedasticity-consistent_standard_errors" title="wikilink">heteroskedasticity-consistent standard errors</a>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a class="uri" href="Heteroskedasticity" title="wikilink">Heteroskedasticity</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="further-reading">Further reading</h2>
<ul>
<li></li>
<li></li>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Statistical_tests" title="wikilink">Category:Statistical tests</a> <a href="Category:Regression_diagnostics" title="wikilink">Category:Regression diagnostics</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
</ol>
</section>
</body>
</html>
