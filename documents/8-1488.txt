   Sequential quadratic programming      Sequential quadratic programming   Sequential quadratic programming ( SQP ) is an iterative method for nonlinear optimization . SQP methods are used on problems for which the objective function and the constraints are twice continuously differentiable .  SQP methods solve a sequence of optimization subproblems, each of which optimizes a quadratic model of the objective subject to a linearization of the constraints. If the problem is unconstrained, then the method reduces to Newton's method for finding a point where the gradient of the objective vanishes. If the problem has only equality constraints, then the method is equivalent to applying Newton's method to the first-order optimality conditions, or Karush–Kuhn–Tucker conditions , of the problem. SQP methods have been implemented in many packages, including NPSOL , SNOPT , NLPQL , OPSYC , OPTIMA , MATLAB , GNU Octave and SQP.  Algorithm basics  Consider a nonlinear programming problem of the form:         min  x      f   (  x  )        s.t.      b   (  x  )    ≥  0          c   (  x  )    =  0.          subscript   x     f  x     s.t.      b  x   0      missing-subexpression       c  x   0.      \begin{array}[]{rl}\min\limits_{x}&f(x)\\
 \mbox{s.t.}&b(x)\geq 0\\
 &c(x)=0.\end{array}     The Lagrangian for this problem is;        ℒ   (  x  ,  λ  ,  σ  )    =    f   (  x  )    -    λ  T   b   (  x  )    -    σ  T   c   (  x  )      ,        ℒ   x  λ  σ        f  x      superscript  λ  T   b  x      superscript  σ  T   c  x      \mathcal{L}(x,\lambda,\sigma)=f(x)-\lambda^{T}b(x)-\sigma^{T}c(x),     where   λ   λ   \lambda   and   σ   σ   \sigma   are Lagrange multipliers . At an iterate    x  k     subscript  x  k    x_{k}   , a basic sequential quadratic programming algorithm defines an appropriate search direction    d  k     subscript  d  k    d_{k}   as a solution to the quadratic programming subproblem         min  d       f   (   x  k   )    +    ∇  f     (   x  k   )   T   d   +     1  2     d  T     ∇   x  x   2   ℒ    (   x  k   ,   λ  k   ,   σ  k   )   d          s  .  t   .        b   (   x  k   )    +    ∇  b     (   x  k   )   T   d    ≥  0           c   (   x  k   )    +    ∇  c     (   x  k   )   T   d    =  0.          subscript   d       f   subscript  x  k       normal-∇  f    superscript   subscript  x  k   T   d       1  2    superscript  d  T     superscript   subscript  normal-∇    x  x    2   ℒ     subscript  x  k    subscript  λ  k    subscript  σ  k    d       formulae-sequence  normal-s  normal-t         b   subscript  x  k       normal-∇  b    superscript   subscript  x  k   T   d    0      missing-subexpression         c   subscript  x  k       normal-∇  c    superscript   subscript  x  k   T   d    0.      \begin{array}[]{rl}\min\limits_{d}&f(x_{k})+\nabla f(x_{k})^{T}d+\tfrac{1}{2}d%
 ^{T}\nabla_{xx}^{2}\mathcal{L}(x_{k},\lambda_{k},\sigma_{k})d\\
 \mathrm{s.t.}&b(x_{k})+\nabla b(x_{k})^{T}d\geq 0\\
 &c(x_{k})+\nabla c(x_{k})^{T}d=0.\end{array}     Note that the term    f   (   x  k   )       f   subscript  x  k     f(x_{k})   in the expression above may be left out for the minimization problem, since it is constant.  See also   Sequential linear programming  Secant method  Newton's method   References      External links   Sequential Quadratic Programming at NEOS guide   "  Category:Optimization algorithms and methods   