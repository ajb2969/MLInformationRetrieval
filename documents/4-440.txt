   Cross-correlation      Cross-correlation   In signal processing , cross-correlation is a measure of similarity of two series as a function of the lag of one relative to the other. This is also known as a sliding dot product or sliding inner-product . It is commonly used for searching a long signal for a shorter, known feature. It has applications in pattern recognition , single particle analysis , electron tomography , averaging , cryptanalysis , and neurophysiology .  For continuous functions f and g , the cross-correlation is defined as :         (   f  ‚ãÜ  g   )    (  œÑ  )     =  def     ‚à´   -  ‚àû   ‚àû     f  *    (  t  )   g   (   t  +  œÑ   )   d  t     ,      superscript   def      normal-‚ãÜ  f  g   œÑ     superscript   subscript             superscript  f    t  g    t  œÑ   d  t      (f\star g)(\tau)\ \stackrel{\mathrm{def}}{=}\int_{-\infty}^{\infty}f^{*}(t)\ g%
 (t+\tau)\,dt,     where    f  *     superscript  f     f^{*}   denotes the complex conjugate of   f   f   f   and   œÑ   œÑ   \tau   is the lag.  Similarly, for discrete functions, the cross-correlation is defined as :         (   f  ‚ãÜ  g   )    [  n  ]     =  def     ‚àë   m  =   -  ‚àû    ‚àû     f  *    [  m  ]   g   [   m  +  n   ]      .      superscript   def      normal-‚ãÜ  f  g    delimited-[]  n      superscript   subscript     m             superscript  f     delimited-[]  m   g   delimited-[]    m  n        (f\star g)[n]\ \stackrel{\mathrm{def}}{=}\sum_{m=-\infty}^{\infty}f^{*}[m]\ g[%
 m+n].     The cross-correlation is similar in nature to the convolution of two functions.  In an autocorrelation , which is the cross-correlation of a signal with itself, there will always be a peak equal to unity at a lag of zero.  In probability and statistics , the term cross-correlations is used for referring to the correlations between the entries of two random vectors  X and Y , while the autocorrelations of a random vector X are considered to be the correlations between the entries of X itself, those forming the correlation matrix (matrix of correlations) of X . This is analogous to the distinction between autocovariance of a random vector and cross-covariance of two random vectors. One more distinction to point out is that in probability and statistics the definition of correlation always includes a standardising factor in such a way that correlations have values between ‚àí1 and +1.  If   X   X   X   and   Y   Y   Y   are two independent  random variables with probability density functions  f and g , respectively, then the probability density of the difference    Y  -  X      Y  X    Y-X   is formally given by the cross-correlation (in the signal-processing sense)    f  ‚ãÜ  g     normal-‚ãÜ  f  g    f\star g   ; however this terminology is not used in probability and statistics. In contrast, the convolution     f  *  g      f  g    f*g   (equivalent to the cross-correlation of f ( t ) and g (‚àí t ) ) gives the probability density function of the sum    X  +  Y      X  Y    X+Y   .  Explanation  As an example, consider two real valued functions   f   f   f   and   g   g   g   differing only by an unknown shift along the x-axis. One can use the cross-correlation to find how much   g   g   g   must be shifted along the x-axis to make it identical to   f   f   f   . The formula essentially slides the   g   g   g   function along the x-axis, calculating the integral of their product at each position. When the functions match, the value of    (   f  ‚ãÜ  g   )     normal-‚ãÜ  f  g    (f\star g)   is maximized. This is because when peaks (positive areas) are aligned, they make a large contribution to the integral. Similarly, when troughs (negative areas) align, they also make a positive contribution to the integral because the product of two negative numbers is positive.  With complex-valued functions    f   f   f   and   g   g   g   , taking the conjugate of   f   f   f   ensures that aligned peaks (or aligned troughs) with imaginary components will contribute positively to the integral.  In econometrics , lagged cross-correlation is sometimes referred to as cross-autocorrelation. 1  Properties   The cross-correlation of functions f ( t ) and g ( t ) is equivalent to the convolution of f * (‚àí t ) and g ( t ). That is:           f  ‚ãÜ  g   =     f  *    (   -  t   )    *  g    .       normal-‚ãÜ  f  g        superscript  f      t    g     f\star g=f^{*}(-t)*g.         If f is a Hermitian function , then      f  ‚ãÜ  g   =   f  *  g    .       normal-‚ãÜ  f  g     f  g     f\star g=f*g.              (   f  ‚ãÜ  g   )   ‚ãÜ   (   f  ‚ãÜ  g   )    =    (   f  ‚ãÜ  f   )   ‚ãÜ   (   g  ‚ãÜ  g   )     .       normal-‚ãÜ   normal-‚ãÜ  f  g    normal-‚ãÜ  f  g     normal-‚ãÜ   normal-‚ãÜ  f  f    normal-‚ãÜ  g  g      (f\star g)\star(f\star g)=(f\star f)\star(g\star g).       Analogous to the convolution theorem , the cross-correlation satisfies           ‚Ñ±   {   f  ‚ãÜ  g   }    =      (   ‚Ñ±   {  f  }    )   *   ‚ãÖ  ‚Ñ±    {  g  }     ,        ‚Ñ±    normal-‚ãÜ  f  g        normal-‚ãÖ   superscript    ‚Ñ±   f      ‚Ñ±    g      \mathcal{F}\{f\star g\}=(\mathcal{F}\{f\})^{*}\cdot\mathcal{F}\{g\},         where   ‚Ñ±   ‚Ñ±   \mathcal{F}   denotes the Fourier transform , and an asterisk again indicates the complex conjugate. Coupled with fast Fourier transform algorithms, this property is often exploited for the efficient numerical computation of cross-correlations (see circular cross-correlation ).    The cross-correlation is related to the spectral density (see Wiener‚ÄìKhinchin theorem ).    The cross-correlation of a convolution of f and h with a function g is the convolution of the cross-correlation of f and g with the kernel h :            (   f  *  h   )   ‚ãÜ  g   =    h   (  -  )    *   (   f  ‚ãÜ  g   )     .       normal-‚ãÜ    f  h   g       h     normal-‚ãÜ  f  g      (f*h)\star g=h(-)*(f\star g).        Time series analysis  In time series analysis , as applied in statistics and signal processing , the cross-correlation between two time series describes the normalized cross-covariance function.  Let    (   X  t   ,   Y  t   )      subscript  X  t    subscript  Y  t     (X_{t},Y_{t})   represent a pair of stochastic processes that are jointly wide-sense stationary . Then the cross-covariance and the cross-correlation are given by        cross-covariance          Œ≥   X  Y     (  œÑ  )    =   E   [    (    X  t   -   Œº  X    )    (    Y   t  +  œÑ    -   Œº  Y    )    ]     ,         subscript  Œ≥    X  Y    œÑ    normal-E       subscript  X  t    subscript  Œº  X       subscript  Y    t  œÑ     subscript  Œº  Y        \gamma_{XY}(\tau)=\operatorname{E}[(X_{t}-\mu_{X})(Y_{t+\tau}-\mu_{Y})],        cross-correlation          œÅ   X  Y     (  œÑ  )    =    E   [    (    X  t   -   Œº  X    )    (    Y   t  +  œÑ    -   Œº  Y    )    ]    /   (    œÉ  X    œÉ  Y    )     ,         subscript  œÅ    X  Y    œÑ      normal-E       subscript  X  t    subscript  Œº  X       subscript  Y    t  œÑ     subscript  Œº  Y         subscript  œÉ  X    subscript  œÉ  Y       \rho_{XY}(\tau)=\operatorname{E}[(X_{t}-\mu_{X})\,(Y_{t+\tau}-\mu_{Y})]/(%
 \sigma_{X}\sigma_{Y}),          where    Œº  X     subscript  Œº  X    \mu_{X}   and    œÉ  X     subscript  œÉ  X    \sigma_{X}   are the mean and standard deviation of the process    (   X  t   )     subscript  X  t    (X_{t})   , which are constant over time due to stationarity; and similarly for    (   Y  t   )     subscript  Y  t    (Y_{t})   , respectively. That the cross-covariance and cross-correlation are independent of t is precisely the additional information (beyond being individually wide-sense stationary) conveyed by the requirement that    (   X  t   ,   Y  t   )      subscript  X  t    subscript  Y  t     (X_{t},Y_{t})   are jointly wide-sense stationary.  The cross-correlation of a pair of jointly wide sense stationary  stochastic process can be estimated by averaging the product of samples measured from one process and samples measured from the other (and its time shifts). The samples included in the average can be an arbitrary subset of all the samples in the signal (e.g., samples within a finite time window or a sub-sampling of one of the signals). For a large number of samples, the average converges to the true cross-correlation.  Time delay analysis  Cross-correlations are useful for determining the time delay between two signals, e.g. for determining time delays for the propagation of acoustic signals across a microphone array. 2 3 After calculating the cross-correlation between the two signals, the maximum (or minimum if the signals are negatively correlated) of the cross-correlation function indicates the point in time where the signals are best aligned, i.e. the time delay between the two signals is determined by the argument of the maximum, or arg max of the cross-correlation , as in       œÑ  delay   =      arg   max   ùë°    (    (   f  ‚ãÜ  g   )    (  t  )    )         subscript  œÑ  delay      t    arg  max       normal-‚ãÜ  f  g   t      \tau_{\mathrm{delay}}=\underset{t}{\operatorname{arg\,max}}((f\star g)(t))     Normalized cross-correlation  For image-processing applications in which the brightness of the image and template can vary due to lighting and exposure conditions, the images can be first normalized. This is typically done at every step by subtracting the mean and dividing by the standard deviation . That is, the cross-correlation of a template,    t   (  x  ,  y  )       t   x  y     t(x,y)   with a subimage    f   (  x  ,  y  )       f   x  y     f(x,y)   is       1  n     ‚àë   x  ,  y       (    f   (  x  ,  y  )    -   f  ¬Ø    )    (    t   (  x  ,  y  )    -   t  ¬Ø    )      œÉ  f    œÉ  t            1  n     subscript    x  y            f   x  y     normal-¬Ø  f        t   x  y     normal-¬Ø  t        subscript  œÉ  f    subscript  œÉ  t        \frac{1}{n}\sum_{x,y}\frac{(f(x,y)-\overline{f})(t(x,y)-\overline{t})}{\sigma_%
 {f}\sigma_{t}}   . where   n   n   n   is the number of pixels in    t   (  x  ,  y  )       t   x  y     t(x,y)   and    f   (  x  ,  y  )       f   x  y     f(x,y)   ,    f  ¬Ø     normal-¬Ø  f    \overline{f}   is the average of f and    œÉ  f     subscript  œÉ  f    \sigma_{f}   is standard deviation of f . In functional analysis terms, this can be thought of as the dot product of two normalized vectors . That is, if       F   (  x  ,  y  )    =    f   (  x  ,  y  )    -   f  ¬Ø          F   x  y        f   x  y     normal-¬Ø  f      F(x,y)=f(x,y)-\overline{f}   and       T   (  x  ,  y  )    =    t   (  x  ,  y  )    -   t  ¬Ø          T   x  y        t   x  y     normal-¬Ø  t      T(x,y)=t(x,y)-\overline{t}   then the above sum is equal to      ‚ü®   F   ‚à•  F  ‚à•    ,   T   ‚à•  T  ‚à•    ‚ü©       F   norm  F      T   norm  T      \left\langle\frac{F}{\|F\|},\frac{T}{\|T\|}\right\rangle   where    ‚ü®  ‚ãÖ  ,  ‚ãÖ  ‚ü©     normal-‚ãÖ  normal-‚ãÖ    \langle\cdot,\cdot\rangle   is the inner product and    ‚à•  ‚ãÖ  ‚à•     fragments  parallel-to  normal-‚ãÖ  parallel-to    \|\cdot\|   is the L ¬≤ norm . Thus, if f and t are real matrices, their normalized cross-correlation equals the cosine of the angle between the unit vectors F and T , being thus 1 if and only if F equals T multiplied by a positive scalar.  Normalized correlation is one of the methods used for template matching , a process used for finding incidences of a pattern or object within an image. It is also the 2-dimensional version of Pearson product-moment correlation coefficient .  Nonlinear systems  Caution must be applied when using cross correlation for nonlinear systems. In certain circumstances, which depend on the properties of the input, cross correlation between the input and output of a system with nonlinear dynamics can be completely blind to certain nonlinear effects. 4 This problem arises because some quadratic moments can equal zero and this can incorrectly suggest that there is little "correlation" (in the sense of statistical dependence) between two signals, when in fact the two signals are strongly related by nonlinear dynamics.  See also   Autocorrelation  Autocovariance  Coherence  Convolution  Correlation  Cross-covariance  Cross-spectrum  Covariance mapping  Digital image correlation  Phase correlation  Scaled correlation  Spectral density  Wiener‚ÄìKhinchin theorem   References  Further reading     External links   Cross Correlation from Mathworld  http://scribblethink.org/Work/nvisionInterface/nip.html  http://www.phys.ufl.edu/LIGO/stochastic/sign05.pdf  http://www.staff.ncl.ac.uk/oliver.hinton/eee305/Chapter6.pdf   "  Category:Bilinear operators  Category:Covariance and correlation  Category:Signal processing  Category:Time domain analysis     ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©     