   Chapman–Robbins bound      Chapman–Robbins bound   In statistics , the Chapman–Robbins bound or Hammersley–Chapman–Robbins bound is a lower bound on the variance of estimators of a deterministic parameter. It is a generalization of the Cramér–Rao bound ; compared to the Cramér–Rao bound, it is both tighter and applicable to a wider range of problems. However, it is usually more difficult to compute.  The bound was independently discovered by John Hammersley in 1950, 1 and by Douglas Chapman and Herbert Robbins in 1951. 2  Statement  Let be an unknown, deterministic parameter, and let be a random variable, interpreted as a measurement of θ . Suppose the probability density function of X is given by p ( x ; θ ). It is assumed that p ( x ; θ ) is well-defined and that  for all values of x and θ .  Suppose δ ( X ) is an unbiased estimate of an arbitrary scalar function of θ , i.e.,        E   {   δ   (  X  )    }    =   g   (  θ  )   for all  θ    .        E     δ  X       g  θ  for all  θ     E\{\delta(X)\}=g(\theta)\text{ for all }\theta.\,     The Chapman–Robbins bound then states that        Var   (   δ   (  X  )    )    ≥    sup  Δ      [    g   (   θ  +  Δ   )    -   g   (  θ  )     ]   2     E  θ     [     p   (  X  ;   θ  +  Δ   )     p   (  X  ;  θ  )     -  1   ]   2       .        Var    δ  X      subscript  supremum  normal-Δ      superscript   delimited-[]      g    θ  normal-Δ      g  θ     2      subscript  E  θ    superscript   delimited-[]        p   X    θ  normal-Δ       p   X  θ     1    2        \mathrm{Var}(\delta(X))\geq\sup_{\Delta}\frac{\left[g(\theta+\Delta)-g(\theta)%
 \right]^{2}}{E_{\theta}\left[\tfrac{p(X;\theta+\Delta)}{p(X;\theta)}-1\right]^%
 {2}}.     Note that the denominator in the lower bound above is exactly the     χ  2     superscript  χ  2    \chi^{2}   -divergence of    p   (  ⋅  ;   θ  +  Δ   )       p   normal-⋅    θ  normal-Δ      p(\cdot;\theta+\Delta)   with respect to    p   (  ⋅  ;  θ  )       p   normal-⋅  θ     p(\cdot;\theta)   .  Relation to Cramér–Rao bound  The Chapman–Robbins bound converges to the Cramér–Rao bound when , assuming the regularity conditions of the Cramér–Rao bound hold. This implies that, when both bounds exist, the Chapman–Robbins version is always at least as tight as the Cramér–Rao bound; in many cases, it is substantially tighter.  The Chapman–Robbins bound also holds under much weaker regularity conditions. For example, no assumption is made regarding differentiability of the probability density function p ( x ; θ ). When p ( x ; θ ) is non-differentiable, the Fisher information is not defined, and hence the Cramér–Rao bound does not exist.  See also   Cramér–Rao bound  Estimation theory   References  Further reading     "  Category:Statistical inequalities  Category:Estimation theory     ↩  ↩     