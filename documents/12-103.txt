   Multinomial test      Multinomial test   In statistics , the multinomial test is the test of the null hypothesis that the parameters of a multinomial distribution equal specified values. It is used for categorical data; see Read and Cressie. 1  We begin with a sample of   N   N   N   items each of which has been observed to fall into one of   k   k   k   categories. We can define    𝐱  =   (   x  1   ,   x  2   ,  …  ,   x  k   )       𝐱    subscript  x  1    subscript  x  2   normal-…   subscript  x  k      \mathbf{x}=(x_{1},x_{2},\dots,x_{k})   as the observed numbers of items in each cell. Hence      ∑   i  =  1   k    x  i    =  N        superscript   subscript     i  1    k    subscript  x  i    N    \textstyle\sum_{i=1}^{k}x_{i}=N   .  Next, we define a vector of parameters     H  0   :   π  =   (   π  1   ,   π  2   ,  …  ,   π  k   )       normal-:   subscript  H  0     π    subscript  π  1    subscript  π  2   normal-…   subscript  π  k       H_{0}:\mathbf{\pi}=(\pi_{1},\pi_{2},\dots,\pi_{k})   , where       ∑   i  =  1   k     π  i    =  1        superscript   subscript     i  1    k    subscript  π  i    1    \textstyle\sum_{i=1}^{k}\pi_{i}=1   . These are the parameter values under the null hypothesis .  The exact probability of the observed configuration   𝐱   𝐱   \mathbf{x}   under the null hypothesis is given by      Pr    (  𝐱  )   𝟎   =  N  !   ∏   i  =  1   k     π  i   x  i      x  i   !    .     fragments  Pr   subscript   fragments  normal-(  x  normal-)   0    N    superscript   subscript  product    i  1    k      superscript   subscript  π  i    subscript  x  i       subscript  x  i     normal-.    \Pr(\mathbf{x)_{0}}=N!\prod_{i=1}^{k}\frac{\pi_{i}^{x_{i}}}{x_{i}!}.     The significance probability for the test is the probability of occurrence of the data set observed, or of a data set less likely than that observed, if the null hypothesis is true. Using an exact test , this is calculated as       Pr   (  𝐬𝐢𝐠  )    =    ∑   y  :    P  r   (  𝐲  )    ≤   P  r    (  𝐱  )   𝟎        Pr   (  𝐲  )          Pr  𝐬𝐢𝐠     subscript    normal-:  y      P  r  𝐲     P  r   subscript  𝐱  0        Pr  𝐲      \Pr(\mathbf{sig})=\sum_{y:Pr(\mathbf{y})\leq Pr(\mathbf{x)_{0}}}\Pr(\mathbf{y})     where the sum ranges over all outcomes as likely as, or less likely than, that observed. In practice this becomes computationally onerous as   k   k   k   and   N   N   N   increase so it is probably only worth using exact tests for small samples. For larger samples, asymptotic approximations are accurate enough and easier to calculate.  One of these approximations is the likelihood ratio . We set up an alternative hypothesis under which each value    π  i     subscript  π  i    \pi_{i}   is replaced by its maximum likelihood estimate     p  i   =    x  i   /  N        subscript  p  i      subscript  x  i   N     p_{i}=x_{i}/N   . The exact probability of the observed configuration   𝐱   𝐱   \mathbf{x}   under the alternative hypothesis is given by      Pr    (  𝐱  )   𝐀   =  N  !   ∏   i  =  1   k     p  i   x  i      x  i   !    .     fragments  Pr   subscript   fragments  normal-(  x  normal-)   𝐀    N    superscript   subscript  product    i  1    k      superscript   subscript  p  i    subscript  x  i       subscript  x  i     normal-.    \Pr(\mathbf{x)_{A}}=N!\prod_{i=1}^{k}\frac{p_{i}^{x_{i}}}{x_{i}!}.     The natural logarithm of the ratio between these two probabilities multiplied by    -  2      2    -2   is then the statistic for the likelihood ratio test        -   2   ln   (   L  R   )      =   -   2     ∑   i  =  1   k      x  i    ln   (    π  i   /   p  i    )         .          2      L  R          2    superscript   subscript     i  1    k      subscript  x  i        subscript  π  i    subscript  p  i           -2\ln(LR)=\textstyle-2\sum_{i=1}^{k}x_{i}\ln(\pi_{i}/p_{i}).     If the null hypothesis is true, then as   N   N   N   increases, the distribution of    -   2   ln   (   L  R   )           2      L  R       -2\ln(LR)   converges to that of chi-squared with    k  -  1      k  1    k-1   degrees of freedom. However it has long been known (e.g. Lawley 1956) that for finite sample sizes, the moments of    -   2   ln   (   L  R   )           2      L  R       -2\ln(LR)   are greater than those of chi-squared, thus inflating the probability of type I errors (false positives). The difference between the moments of chi-squared and those of the test statistic are a function of    N   -  1      superscript  N    1     N^{-1}   . Williams (1976) showed that the first moment can be matched as far as    N   -  2      superscript  N    2     N^{-2}   if the test statistic is divided by a factor given by        q  1   =   1  +      ∑   i  =  1   k    π  i   -  1     -  1    6  N   (   k  -  1   )       .       subscript  q  1     1        superscript   subscript     i  1    k    superscript   subscript  π  i     1     1     6  N    k  1        q_{1}=1+\frac{\sum_{i=1}^{k}\pi_{i}^{-1}-1}{6N(k-1)}.     In the special case where the null hypothesis is that all the values    π  i     subscript  π  i    \pi_{i}   are equal to    1  /  k      1  k    1/k   (i.e. it stipulates a uniform distribution), this simplifies to        q  1   =   1  +    k  +  1    6  N      .       subscript  q  1     1      k  1     6  N       q_{1}=1+\frac{k+1}{6N}.     Subsequently, Smith et al. (1981) derived a dividing factor which matches the first moment as far as    N   -  3      superscript  N    3     N^{-3}   . For the case of equal values of    π  i     subscript  π  i    \pi_{i}   , this factor is        q  2   =   1  +    k  +  1    6  N    +    k  2    6   N  2       .       subscript  q  2     1      k  1     6  N       superscript  k  2     6   superscript  N  2        q_{2}=1+\frac{k+1}{6N}+\frac{k^{2}}{6N^{2}}.     The null hypothesis can also be tested by using Pearson's chi-squared test       χ  2   =    ∑   i  =  1   k      (    x  i   -   E  i    )   2    E  i          superscript  χ  2     superscript   subscript     i  1    k      superscript     subscript  x  i    subscript  E  i    2    subscript  E  i       \chi^{2}=\sum_{i=1}^{k}{(x_{i}-E_{i})^{2}\over E_{i}}     where     E  i   =   N   π  i         subscript  E  i     N   subscript  π  i      E_{i}=N\pi_{i}   is the expected number of cases in category   i   i   i   under the null hypothesis. This statistic also converges to a chi-squared distribution with    k  -  1      k  1    k-1   degrees of freedom when the null hypothesis is true but does so from below, as it were, rather than from above as    -   2   ln   (   L  R   )           2      L  R       -2\ln(LR)   does, so may be preferable to the uncorrected version of    -   2   ln   (   L  R   )           2      L  R       -2\ln(LR)   for small samples.  References         "  Category:Categorical data  Category:Statistical tests  Category:Non-parametric statistics     Read, T. R. C. and Cressie, N. A. C. (1988). Goodness-of-fit statistics for discrete multivariate data. New York: Springer-Verlag. ISBN 0-387-96682-X. ↩     