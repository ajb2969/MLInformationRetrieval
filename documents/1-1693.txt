   Partial derivative      Partial derivative   In mathematics , a partial derivative of a function of several variables is its derivative with respect to one of those variables, with the others held constant (as opposed to the total derivative , in which all variables are allowed to vary). Partial derivatives are used in vector calculus and differential geometry .  The partial derivative of a function f ( x , y , ...) with respect to the variable x is variously denoted by        f  x  ′   ,   f  x   ,    ∂  x   f   ,    ∂   ∂  x    f   ,   or    ∂  f    ∂  x      .      subscript   superscript  f  normal-′   x    subscript  f  x     subscript   x   f          x    f     or      f     x       f^{\prime}_{x},\ f_{x},\ \partial_{x}f,\frac{\partial}{\partial x}f,\text{ or %
 }\frac{\partial f}{\partial x}.     Since in general a partial derivative is a function of the same arguments as was the original function, this functional dependence is sometimes explicitly included in the notation, as in         f  x    (  x  ,  y  ,  …  )    ,     ∂  f    ∂  x     (  x  ,  y  ,  …  )     .        subscript  f  x    x  y  normal-…          f     x     x  y  normal-…      f_{x}(x,y,...),\ \frac{{\partial f}}{{\partial x}}(x,y,...).     The partial-derivative symbol is ∂ . One of the first known uses of the symbol in mathematics is by Marquis de Condorcet from 1770, who used it for partial differences. The modern partial derivative notation is by Adrien-Marie Legendre (1786), though he later abandoned it; Carl Gustav Jacob Jacobi re-introduced the symbol in 1841. 1  Introduction  Suppose that ƒ is a function of more than one variable. For instance,       z  =   f   (  x  ,  y  )    =    x  2   +   x  y   +   y  2     .        z    f   x  y            superscript  x  2     x  y    superscript  y  2       z=f(x,y)=\,\!x^{2}+xy+y^{2}.\,     . For the partial derivative at  that leaves y constant, the corresponding tangent line is parallel to the xz -plane.  | image2    = X2+X+1.svg  | caption2  = A slice of the graph above showing the function in the xz -plane at . Note that the two axes are shown here with different scales. The slope of the tangent line is 3.  }}  The graph of this function defines a surface in Euclidean space . To every point on this surface, there are an infinite number of tangent lines . Partial differentiation is the act of choosing one of these lines and finding its slope . Usually, the lines of most interest are those that are parallel to the xz -plane, and those that are parallel to the yz -plane (which result from holding either y or x constant, respectively.)  To find the slope of the line tangent to the function at P that is parallel to the xz -plane, the y variable is treated as constant. The graph and this plane are shown on the right. On the graph below it, we see the way the function looks on the plane . By finding the derivative of the equation while assuming that y is a constant, the slope of ƒ at the point  is found to be:        ∂  z    ∂  x    =    2  x   +  y           z     x        2  x   y     \frac{\partial z}{\partial x}=2x+y     So at , by substitution, the slope is 3. Therefore        ∂  z    ∂  x    =  3          z     x    3    \frac{\partial z}{\partial x}=3     at the point . That is, the partial derivative of z with respect to x at  is 3, as shown in the graph.  Definition  Basic definition  The function f can be reinterpreted as a family of functions of one variable indexed by the other variables:        f   (  x  ,  y  )    =    f  y    (  x  )    =    x  2   +   x  y   +   y  2     .          f   x  y       subscript  f  y   x           superscript  x  2     x  y    superscript  y  2       f(x,y)=f_{y}(x)=\,\!x^{2}+xy+y^{2}.\,     In other words, every value of y defines a function, denoted f y , which is a function of one variable x . 2 That is,         f  y    (  x  )    =    x  2   +   x  y   +   y  2     .         subscript  f  y   x      superscript  x  2     x  y    superscript  y  2      f_{y}(x)=x^{2}+xy+y^{2}.\,     Once a value of y is chosen, say a , then f ( x , y ) determines a function f a which traces a curve x 2 + ax + a 2 on the xz plane:         f  a    (  x  )    =    x  2   +   a  x   +   a  2     .         subscript  f  a   x      superscript  x  2     a  x    superscript  a  2      f_{a}(x)=x^{2}+ax+a^{2}.\,     In this expression, a is a constant , not a variable , so f a is a function of only one real variable, that being x . Consequently, the definition of the derivative for a function of one variable applies:         f  a  ′    (  x  )    =    2  x   +  a    .         superscript   subscript  f  a   normal-′   x       2  x   a     f_{a}^{\prime}(x)=2x+a.\,     The above procedure can be performed for any choice of a . Assembling the derivatives together into a function gives a function which describes the variation of f in the x direction:          ∂  f    ∂  x     (  x  ,  y  )    =    2  x   +  y    .            f     x     x  y        2  x   y     \frac{\partial f}{\partial x}(x,y)=2x+y.\,     This is the partial derivative of f with respect to x . Here ∂ is a rounded d called the partial derivative symbol . To distinguish it from the letter d , ∂ is sometimes pronounced "tho" or "partial"  In general, the partial derivative of a function f ( x 1 ,..., x n ) in the direction x i at the point ( a 1 ,..., a n ) is defined to be:          ∂  f    ∂   x  i      (   a  1   ,  …  ,   a  n   )    =    lim   h  →  0       f   (   a  1   ,  …  ,    a  i   +  h   ,  …  ,   a  n   )    -   f   (   a  1   ,  …  ,   a  i   ,  …  ,   a  n   )     h     .            f      subscript  x  i       subscript  a  1   normal-…   subscript  a  n       subscript    normal-→  h  0          f    subscript  a  1   normal-…     subscript  a  i   h   normal-…   subscript  a  n       f    subscript  a  1   normal-…   subscript  a  i   normal-…   subscript  a  n      h      \frac{\partial f}{\partial x_{i}}(a_{1},\ldots,a_{n})=\lim_{h\to 0}\frac{f(a_{%
 1},\ldots,a_{i}+h,\ldots,a_{n})-f(a_{1},\ldots,a_{i},\dots,a_{n})}{h}.     In the above difference quotient, all the variables except x i are held fixed. That choice of fixed values determines a function of one variable      f    a  1   ,  …  ,   a   i  -  1    ,   a   i  +  1    ,  …  ,   a  n      (   x  i   )    =   f   (   a  1   ,  …  ,   a   i  -  1    ,   x  i   ,   a   i  +  1    ,  …  ,   a  n   )           subscript  f    subscript  a  1   normal-…   subscript  a    i  1     subscript  a    i  1    normal-…   subscript  a  n      subscript  x  i      f    subscript  a  1   normal-…   subscript  a    i  1     subscript  x  i    subscript  a    i  1    normal-…   subscript  a  n       f_{a_{1},\ldots,a_{i-1},a_{i+1},\ldots,a_{n}}(x_{i})=f(a_{1},\ldots,a_{i-1},x_%
 {i},a_{i+1},\ldots,a_{n})   , and by definition,          d   f    a  1   ,  …  ,   a   i  -  1    ,   a   i  +  1    ,  …  ,   a  n       d   x  i      (   a  i   )    =     ∂  f    ∂   x  i      (   a  1   ,  …  ,   a  n   )     .            d   subscript  f    subscript  a  1   normal-…   subscript  a    i  1     subscript  a    i  1    normal-…   subscript  a  n        d   subscript  x  i      subscript  a  i          f      subscript  x  i       subscript  a  1   normal-…   subscript  a  n       \frac{df_{a_{1},\ldots,a_{i-1},a_{i+1},\ldots,a_{n}}}{dx_{i}}(a_{i})=\frac{%
 \partial f}{\partial x_{i}}(a_{1},\ldots,a_{n}).     In other words, the different choices of a index a family of one-variable functions just as in the example above. This expression also shows that the computation of partial derivatives reduces to the computation of one-variable derivatives.  An important example of a function of several variables is the case of a scalar-valued function  f ( x 1 ,... x n ) on a domain in Euclidean space    ℝ  n     superscript  ℝ  n    \mathbb{R}^{n}   (e.g., on    ℝ  2     superscript  ℝ  2    \mathbb{R}^{2}   or    ℝ  3     superscript  ℝ  3    \mathbb{R}^{3}   ). In this case f has a partial derivative ∂ f /∂ x j with respect to each variable x j . At the point a , these partial derivatives define the vector         ∇  f    (  a  )    =   (     ∂  f    ∂   x  1      (  a  )    ,  …  ,     ∂  f    ∂   x  n      (  a  )    )    .         normal-∇  f   a          f      subscript  x  1     a   normal-…        f      subscript  x  n     a      \nabla f(a)=\left(\frac{\partial f}{\partial x_{1}}(a),\ldots,\frac{\partial f%
 }{\partial x_{n}}(a)\right).     This vector is called the gradient of f at a . If f is differentiable at every point in some domain, then the gradient is a vector-valued function ∇ f which takes the point a to the vector ∇ f ( a ). Consequently, the gradient produces a vector field .  A common abuse of notation is to define the del operator (∇) as follows in three-dimensional Euclidean space     ℝ  3     superscript  ℝ  3    \mathbb{R}^{3}   with unit vectors      𝐢  ^   ,   𝐣  ^   ,   𝐤  ^       normal-^  𝐢    normal-^  𝐣    normal-^  𝐤     \mathbf{\hat{i}},\mathbf{\hat{j}},\mathbf{\hat{k}}   :      ∇  =     [   ∂   ∂  x    ]    𝐢  ^    +    [   ∂   ∂  y    ]    𝐣  ^    +    [   ∂   ∂  z    ]    𝐤  ^         normal-∇       delimited-[]       x      normal-^  𝐢       delimited-[]       y      normal-^  𝐣       delimited-[]       z      normal-^  𝐤       \nabla=\bigg[{\frac{\partial}{\partial x}}\bigg]\mathbf{\hat{i}}+\bigg[{\frac{%
 \partial}{\partial y}}\bigg]\mathbf{\hat{j}}+\bigg[{\frac{\partial}{\partial z%
 }}\bigg]\mathbf{\hat{k}}   Or, more generally, for n -dimensional Euclidean space    ℝ  n     superscript  ℝ  n    \mathbb{R}^{n}   with coordinates (x 1 , x 2 , x 3 ,...,x n ) and unit vectors (      𝐞  ^   𝟏   ,    𝐞  ^   𝟐   ,    𝐞  ^   𝟑   ,  …  ,    𝐞  ^   𝐧       subscript   normal-^  𝐞   1    subscript   normal-^  𝐞   2    subscript   normal-^  𝐞   3   normal-…   subscript   normal-^  𝐞   𝐧     \mathbf{\hat{e}_{1}},\mathbf{\hat{e}_{2}},\mathbf{\hat{e}_{3}},\dots,\mathbf{%
 \hat{e}_{n}}   ):      ∇  =    ∑   j  =  1   n     [   ∂   ∂   x  j     ]     𝐞  ^   𝐣     =     [   ∂   ∂   x  1     ]     𝐞  ^   𝟏    +    [   ∂   ∂   x  2     ]     𝐞  ^   𝟐    +    [   ∂   ∂   x  3     ]     𝐞  ^   𝟑    +  …  +    [   ∂   ∂   x  n     ]     𝐞  ^   𝐧           normal-∇    superscript   subscript     j  1    n      delimited-[]        subscript  x  j       subscript   normal-^  𝐞   𝐣               delimited-[]        subscript  x  1       subscript   normal-^  𝐞   1       delimited-[]        subscript  x  2       subscript   normal-^  𝐞   2       delimited-[]        subscript  x  3       subscript   normal-^  𝐞   3    normal-…     delimited-[]        subscript  x  n       subscript   normal-^  𝐞   𝐧        \nabla=\sum_{j=1}^{n}\bigg[{\frac{\partial}{\partial x_{j}}}\bigg]\mathbf{\hat%
 {e}_{j}}=\bigg[{\frac{\partial}{\partial x_{1}}}\bigg]\mathbf{\hat{e}_{1}}+%
 \bigg[{\frac{\partial}{\partial x_{2}}}\bigg]\mathbf{\hat{e}_{2}}+\bigg[{\frac%
 {\partial}{\partial x_{3}}}\bigg]\mathbf{\hat{e}_{3}}+\dots+\bigg[{\frac{%
 \partial}{\partial x_{n}}}\bigg]\mathbf{\hat{e}_{n}}     Formal definition  Like ordinary derivatives, the partial derivative is defined as a limit . Let U be an open subset of R n and f : U → R a function. The partial derivative of f at the point a = ( a 1 , ..., a n ) ∈ U with respect to the i -th variable a i is defined as        ∂   ∂   a  i     f   (  𝐚  )    =    lim   h  →  0       f   (   a  1   ,  …  ,   a   i  -  1    ,    a  i   +  h   ,   a   i  +  1    ,  …  ,   a  n   )    -   f   (   a  1   ,  …  ,   a  i   ,  …  ,   a  n   )     h                subscript  a  i     f  𝐚     subscript    normal-→  h  0          f    subscript  a  1   normal-…   subscript  a    i  1       subscript  a  i   h    subscript  a    i  1    normal-…   subscript  a  n       f    subscript  a  1   normal-…   subscript  a  i   normal-…   subscript  a  n      h      \frac{\partial}{\partial a_{i}}f(\mathbf{a})=\lim_{h\rightarrow 0}{f(a_{1},%
 \dots,a_{i-1},a_{i}+h,a_{i+1},\dots,a_{n})-f(a_{1},\dots,a_{i},\dots,a_{n})%
 \over h}     Even if all partial derivatives ∂ f /∂ a i ( a ) exist at a given point a , the function need not be continuous there. However, if all partial derivatives exist in a neighborhood of a and are continuous there, then f is totally differentiable in that neighborhood and the total derivative is continuous. In this case, it is said that f is a C 1 function. This can be used to generalize for vector valued functions ( f : U → R ' m ) by carefully using a componentwise argument.  The partial derivative     ∂  f    ∂  x         f     x     \frac{\partial f}{\partial x}   can be seen as another function defined on U and can again be partially differentiated. If all mixed second order partial derivatives are continuous at a point (or on a set), f is termed a C 2 function at that point (or on that set); in this case, the partial derivatives can be exchanged by Clairaut's theorem :          ∂  2   f     ∂    x  i      ∂   x  j      =     ∂  2   f     ∂    x  j      ∂   x  i       .          superscript   2   f        subscript  x  i       subscript  x  j          superscript   2   f        subscript  x  j       subscript  x  i        \frac{\partial^{2}f}{\partial x_{i}\,\partial x_{j}}=\frac{\partial^{2}f}{%
 \partial x_{j}\,\partial x_{i}}.     Examples  Geometry  The volume  V of a cone depends on the cone's height  h and its radius  r according to the formula        V   (  r  ,  h  )    =    π   r  2   h   3    .        V   r  h        π   superscript  r  2   h   3     V(r,h)=\frac{\pi r^{2}h}{3}.     The partial derivative of V with respect to r is         ∂  V    ∂  r    =    2  π  r  h   3    ,          V     r        2  π  r  h   3     \frac{\partial V}{\partial r}=\frac{2\pi rh}{3},     which represents the rate with which a cone's volume changes if its radius is varied and its height is kept constant. The partial derivative with respect to h is         ∂  V    ∂  h    =    π   r  2    3    ,          V     h        π   superscript  r  2    3     \frac{\partial V}{\partial h}=\frac{\pi r^{2}}{3},     which represents the rate with which the volume changes if its height is varied and its radius is kept constant.  By contrast, the total derivative of V with respect to r and h are respectively        d  V    d  r    =       2  π  r  h   3   ⏞     ∂  V    ∂  r     +       π   r  2    3   ⏞     ∂  V    ∂  h       d  h    d  r             normal-d  V    normal-d  r       superscript   normal-⏞      2  π  r  h   3        V     r        superscript   normal-⏞      π   superscript  r  2    3        V     h        normal-d  h    normal-d  r        \frac{\operatorname{d}V}{\operatorname{d}r}=\overbrace{\frac{2\pi rh}{3}}^{%
 \frac{\partial V}{\partial r}}+\overbrace{\frac{\pi r^{2}}{3}}^{\frac{\partial
 V%
 }{\partial h}}\frac{\operatorname{d}h}{\operatorname{d}r}     and        d  V    d  h    =       π   r  2    3   ⏞     ∂  V    ∂  h     +       2  π  r  h   3   ⏞     ∂  V    ∂  r       d  r    d  h             normal-d  V    normal-d  h       superscript   normal-⏞      π   superscript  r  2    3        V     h        superscript   normal-⏞      2  π  r  h   3        V     r        normal-d  r    normal-d  h        \frac{\operatorname{d}V}{\operatorname{d}h}=\overbrace{\frac{\pi r^{2}}{3}}^{%
 \frac{\partial V}{\partial h}}+\overbrace{\frac{2\pi rh}{3}}^{\frac{\partial V%
 }{\partial r}}\frac{\operatorname{d}r}{\operatorname{d}h}     The difference between the total and partial derivative is the elimination of indirect dependencies between variables in partial derivatives.  If (for some arbitrary reason) the cone's proportions have to stay the same, and the height and radius are in a fixed ratio k ,       k  =   h  r   =    d  h    d  r     .        k    h  r           normal-d  h    normal-d  r       k=\frac{h}{r}=\frac{\operatorname{d}h}{\operatorname{d}r}.     This gives the total derivative with respect to r :        d  V    d  r    =     2  π  r  h   3   +     π   r  2    3   k           normal-d  V    normal-d  r          2  π  r  h   3         π   superscript  r  2    3   k      \frac{\operatorname{d}V}{\operatorname{d}r}=\frac{2\pi rh}{3}+\frac{\pi r^{2}}%
 {3}k     which simplifies to:        d  V    d  r    =   k  π   r  2           normal-d  V    normal-d  r      k  π   superscript  r  2      \frac{\operatorname{d}V}{\operatorname{d}r}=k\pi r^{2}     Similarly, the total derivative with respect to h is:        d  V    d  h    =   π   r  2           normal-d  V    normal-d  h      π   superscript  r  2      \frac{\operatorname{d}V}{\operatorname{d}h}=\pi r^{2}     The total derivative with respect to both r and h of the volume intended as scalar function of these two variables is given by the gradient vector     ∇  V   =   (    ∂  V    ∂  r    ,    ∂  V    ∂  h    )   =   (    2  3   π  r  h   ,    1  3   π   r  2    )          normal-∇  V        V     r        V     h               2  3   π  r  h       1  3   π   superscript  r  2        \nabla V=\left(\frac{\partial V}{\partial r},\frac{\partial V}{\partial h}%
 \right)=\left(\frac{2}{3}\pi rh,\frac{1}{3}\pi r^{2}\right)   .  Optimization  Partial derivatives appear in any calculus-based optimization problem with more than one choice variable. For example, in economics a firm may wish to maximize profit π( x , y ) with respect to the choice of the quantities x and y of two different types of output. The first order conditions for this optimization are π x = 0 = π y . Since both partial derivatives π x and π y will generally themselves be functions of both arguments x and y , these two first order conditions form a system of two equations in two unknowns .  Science and engineering  Equations involving an unknown function's partial derivatives are called partial differential equations . These equations are used to mathematically approximate many physical phenomena like fluid flows , force in a spring , nerve conduction and are frequently encountered in physics , engineering , and other sciences and applied disciplines.  Economics  Partial derivatives play a prominent role in economics , in which most functions describing economic behavior posit that the behavior depends on more than one variable. For example, a societal consumption function may describe the amount spent on consumer goods as depending on both income and wealth; the marginal propensity to consume is then the partial derivative of the consumption function with respect to income.  Notation  For the following examples, let f be a function in x , y and z .  First-order partial derivatives:         ∂  f    ∂  x    =   f  x   =    ∂  x   f    .            f     x     subscript  f  x          subscript   x   f      \frac{\partial f}{\partial x}=f_{x}=\partial_{x}f.     Second-order partial derivatives:          ∂  2   f    ∂   x  2     =   f   x  x    =    ∂   x  x    f    .            superscript   2   f      superscript  x  2      subscript  f    x  x           subscript     x  x    f      \frac{\partial^{2}f}{\partial x^{2}}=f_{xx}=\partial_{xx}f.     Second-order mixed derivatives :          ∂  2   f     ∂   y     ∂  x     =    ∂   ∂  y     (    ∂  f    ∂  x    )    =    (   f  x   )   y   =   f   x  y    =    ∂   y  x    f    .            superscript   2   f       y     x            y        f     x           subscript   subscript  f  x   y         subscript  f    x  y           subscript     y  x    f      \frac{\partial^{2}f}{\partial y\,\partial x}=\frac{\partial}{\partial y}\left(%
 \frac{\partial f}{\partial x}\right)=(f_{x})_{y}=f_{xy}=\partial_{yx}f.     Higher-order partial and mixed derivatives:          ∂   i  +  j  +  k    f     ∂    x  i      ∂    y  j      ∂   z  k      =   f   (  i  ,  j  ,  k  )     .          superscript     i  j  k    f        superscript  x  i       superscript  y  j       superscript  z  k       superscript  f   i  j  k      \frac{\partial^{i+j+k}f}{\partial x^{i}\,\partial y^{j}\,\partial z^{k}}=f^{(i%
 ,j,k)}.     When dealing with functions of multiple variables, some of these variables may be related to each other, and it may be necessary to specify explicitly which variables are being held constant. In fields such as statistical mechanics , the partial derivative of f with respect to x , holding y and z constant, is often expressed as        (    ∂  f    ∂  x    )    y  ,  z    .     subscript      f     x     y  z     \left(\frac{\partial f}{\partial x}\right)_{y,z}.     Antiderivative analogue  There is a concept for partial derivatives that is analogous to antiderivatives for regular derivatives. Given a partial derivative, it allows for the partial recovery of the original function.  Consider the example of      ∂  z    ∂  x    =    2  x   +  y           z     x        2  x   y     \frac{\partial z}{\partial x}=2x+y   . The "partial" integral can be taken with respect to x (treating y as constant, in a similar manner to partial differentiation):      z  =   ∫      ∂  z    ∂  x     d  x    =    x  2   +   x  y   +   g   (  y  )           z          z     x    d  x            superscript  x  2     x  y     g  y       z=\int\frac{\partial z}{\partial x}\,dx=x^{2}+xy+g(y)   Here, the "constant" of integration is no longer a constant, but instead a function of all the variables of the original function except x . The reason for this is that all the other variables are treated as constant when taking the partial derivative, so any function which does not involve   x   x   x   will disappear when taking the partial derivative, and we have to account for this when we take the antiderivative. The most general way to represent this is to have the "constant" represent an unknown function of all the other variables.  Thus the set of functions     x  2   +   x  y   +   g   (  y  )         superscript  x  2     x  y     g  y     x^{2}+xy+g(y)   , where g is any one-argument function, represents the entire set of functions in variables x , y that could have produced the x -partial derivative 2 x + y .  If all the partial derivatives of a function are known (for example, with the gradient ), then the antiderivatives can be matched via the above process to reconstruct the original function up to a constant.  Higher order partial derivatives  Second and higher order partial derivatives are defined analogously to the higher order derivatives of univariate functions. For the function    f   (  x  ,  y  ,  …  )       f   x  y  normal-…     f(x,y,...)   the "own" second partial derivative with respect to x is simply the partial derivative of the partial derivative (both with respect to x ): 3          ∂  2   f    ∂   x  2     ≡   ∂     ∂  f   /   ∂  x     ∂  x     ≡    ∂   f  x     ∂  x    ≡   f   x  x     .            superscript   2   f      superscript  x  2             f     x      x               subscript  f  x      x          subscript  f    x  x       \frac{{\partial^{2}f}}{{\partial x^{2}}}\equiv\partial\frac{{\partial f/%
 \partial x}}{{\partial x}}\equiv\frac{{\partial f_{x}}}{{\partial x}}\equiv f_%
 {{xx}}.     The cross partial derivative with respect to x and y is obtained by taking the partial derivative of f with respect to x , and then taking the partial derivative of the result with respect to y , to obtain          ∂  2   f     ∂  x    ∂  y     ≡   ∂     ∂  f   /   ∂  x     ∂  y     ≡    ∂   f  x     ∂  y    ≡   f   x  y     .            superscript   2   f       x     y             f     x      y               subscript  f  x      y          subscript  f    x  y       \frac{{\partial^{2}f}}{{\partial x\partial y}}\equiv\partial\frac{{\partial f/%
 \partial x}}{{\partial y}}\equiv\frac{{\partial f_{x}}}{{\partial y}}\equiv f_%
 {{xy}}.     Schwarz' theorem states that if the second derivatives are continuous the expression for the cross partial derivative is unaffected by which variable the partial derivative is taken with respect to first and which is taken second. That is,         ∂  2   f     ∂  x    ∂  y     =     ∂  2   f     ∂  y    ∂  x             superscript   2   f       x     y         superscript   2   f       y     x       \frac{{\partial^{2}f}}{{\partial x\partial y}}=\frac{{\partial^{2}f}}{{%
 \partial y\partial x}}     or equivalently      f   x  y    =   f   y  x     .       subscript  f    x  y     subscript  f    y  x      f_{{xy}}=f_{{yx}}.     Own and cross partial derivatives appear in the Hessian matrix which is used in the second order conditions in optimization problems.  See also   d'Alembertian operator  Chain rule  Curl (mathematics)  Directional derivative  Divergence  Exterior derivative  Gradient  Jacobian matrix and determinant  Laplacian  Symmetry of second derivatives  Triple product rule , also known as the cyclic chain rule.   Notes    External links    Partial Derivatives at MathWorld   "  Category:Multivariable calculus  Category:Differential operators     ↩  This can also be expressed as the adjointness between the product space and function space constructions. ↩  Chiang, Alpha C.  Fundamental Methods of Mathematical Economics , McGraw-Hill, third edition, 1984. ↩     