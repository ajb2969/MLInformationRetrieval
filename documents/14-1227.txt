   Convolution of probability distributions      Convolution of probability distributions   The convolution of probability distributions arises in probability theory and statistics as the operation in terms of probability distributions that corresponds to the addition of independent  random variables and, by extension, to forming linear combinations of random variables. The operation here is a special case of convolution in the context of probability distributions.  Introduction  The probability distribution of the sum of two or more independent  random variables is the convolution of their individual distributions. The term is motivated by the fact that the probability mass function or probability density function of a sum of random variables is the convolution of their corresponding probability mass functions or probability density functions respectively. Many well known distributions have simple convolutions: see List of convolutions of probability distributions  Example derivation  There are several ways of derive formulae for the convolution of probability distributions. Often the manipulation of integrals can be avoided by use of some type of generating function . Such methods can also be useful in deriving properties of the resulting distribution, such as moments, even if an explicit formula for the distribution itself cannot be derived.  One of the straightforward techniques is to use characteristic functions , which always exists and are unique to a given distribution.  Convolution of Bernoulli distributions  The convolution of two i.i.d. Bernoulli random variables is a Binomial random variable. That is, in a shorthand notation,         ∑   i  =  1   2    Bernoulli   (  p  )     ∼   Binomial   (  2  ,  p  )     .     similar-to    superscript   subscript     i  1    2     Bernoulli  p      Binomial   2  p      \sum_{i=1}^{2}\mathrm{Bernoulli}(p)\sim\mathrm{Binomial}(2,p).     To show this let       Z  ∼   Binomial   (  2  ,  p  )     .     similar-to  Z    Binomial   2  p      Z\sim\mathrm{Binomial}(2,p)\,\!.   Also, let Z denote a generic binomial random variable:       X  1   and   X  2        subscript  X  1   and   subscript  X  2     X_{1}\text{ and }X_{2}     Using probability mass functions  As    ℙ   [  Y  =  n  ]      fragments  P   fragments  normal-[  Y   n  normal-]     \displaystyle\mathbb{P}[Y=n]   are independent,       (      n      k      )   =  0       binomial  n  k   0    {\textstyle\left({{n}\atop{k}}\right)}=0     Here, use was made of the fact that    X  k     subscript  X  k    X_{k}   for k > n in the last but three equality, and of Pascal's rule in the second last equality.  Using characteristic functions  The characteristic function of each   Z   Z   Z   and of       φ   X  k     (  t  )    =    1  -  p   +   p   e   i  t          φ  Z    (  t  )    =    (    1  -  p   +   p   e   i  t      )   2       formulae-sequence       subscript  φ   subscript  X  k    t       1  p     p   superscript  e    i  t            subscript  φ  Z   t    superscript      1  p     p   superscript  e    i  t      2      \varphi_{X_{k}}(t)=1-p+pe^{it}\qquad\varphi_{Z}(t)=\left(1-p+pe^{it}\right)^{2}   is       φ  Y    (  t  )        subscript  φ  Y   t    \displaystyle\varphi_{Y}(t)   where t is within some neighborhood of zero.      X  k     subscript  X  k    X_{k}     The expectation of the product is the product of the expectations since each   Y   Y   Y   is independent. Since   Z   Z   Z   and $Z$ have the same characteristic function, they must have the same distribution.  References     "  Category:Theory of probability distributions   