   Moore–Penrose pseudoinverse      Moore–Penrose pseudoinverse  In [[mathematics]], and in particular [[linear algebra]], a '''pseudoinverse''' {{math|''A'' + }} of a  [[matrix (mathematics)|matrix]] {{math|''A''}} is a [[Generalized inverse|generalization]] of the [[inverse matrix]]. {{cite book | last=Ben-Israel | first = Adi |author2=[[Thomas N.E. Greville]]  | title=Generalized Inverses | isbn=0-387-00293-6 | publisher=[[Springer Science+Business Media|Springer-Verlag]] | year=2003}} The most widely known type of matrix pseudoinverse is the '''Moore–Penrose pseudoinverse''', which was independently described by [[E. H. Moore]] {{cite journal | last=Moore | first=E. H. | authorlink=E. H. Moore | title=On the reciprocal of the general algebraic matrix | journal=[[Bulletin of the American Mathematical Society]] | volume=26 |issue=9| pages=394–395 | year=1920 | url =http://projecteuclid.org/euclid.bams/1183425340 | doi = 10.1090/S0002-9 904-1920-03322-7 }} in 1920, Arne Bjerhammar 1 in 1951 and Roger Penrose 2 in 1955. Earlier, Fredholm had introduced the concept of a pseudoinverse of integral operators in 1903. When referring to a matrix, the term pseudoinverse, without further specification, is often used to indicate the Moore–Penrose pseudoinverse. The term generalized inverse is sometimes used as a synonym for pseudoinverse.  A common use of the Moore–Penrose pseudoinverse (hereafter, just pseudoinverse) is to compute a 'best fit' ( least squares ) solution to a system of linear equations that lacks a unique solution (see below under § Applications ). Another use is to find the minimum ( Euclidean ) norm solution to a system of linear equations with multiple solutions. The pseudoinverse facilitates the statement and proof of results in linear algebra.  The pseudoinverse is defined and unique for all matrices whose entries are real or complex numbers. It can be computed using the singular value decomposition .  Notation  In the following discussion, the following conventions are adopted.      K   K   K   will denote one of the fields of real or complex numbers, denoted    ℝ  ,  ℂ     ℝ  ℂ    \mathbb{R},\,\mathbb{C}   , respectively. The vector space of    m  ×  n      m  n    m\times n   matrices over   K   K   K   is denoted by    M   (  m  ,  n  ;  K  )       normal-M   m  n  K     \mathrm{M}(m,n;K)   .  For    A  ∈   M   (  m  ,  n  ;  K  )        A    normal-M   m  n  K      A\in\mathrm{M}(m,n;K)   ,    A  T     superscript  A  normal-T    A^{\mathrm{T}}   and    A  *     superscript  A     A^{*}   denote the transpose and Hermitian transpose (also called conjugate transpose ) respectively. If    K  =  ℝ      K  ℝ    K=\mathbb{R}   , then     A  *   =   A  T        superscript  A     superscript  A  normal-T     A^{*}=A^{\mathrm{T}}   .  For    A  ∈   M   (  m  ,  n  ;  K  )        A    normal-M   m  n  K      A\in\mathrm{M}(m,n;K)   , then    im   (  A  )      im  A    \operatorname{im}(A)   denotes the range (image) of   A   A   A   (the space spanned by the column vectors of   A   A   A   ) and    ker   (  A  )      ker  A    \operatorname{ker}(A)   denotes the kernel (null space) of   A   A   A   .  Finally, for any positive integer   n   n   n   ,     I  n   ∈   M   (  n  ,  n  ;  K  )         subscript  I  n     normal-M   n  n  K      I_{n}\in\mathrm{M}(n,n;K)   denotes the    n  ×  n      n  n    n\times n    identity matrix .   Definition  For    A  ∈   M   (  m  ,  n  ;  K  )        A    normal-M   m  n  K      A\in\mathrm{M}(m,n;K)   , a pseudoinverse of   A   A   A   is defined as a matrix     A  +   ∈   M   (  n  ,  m  ;  K  )         superscript  A      normal-M   n  m  K      A^{+}\in\mathrm{M}(n,m;K)   satisfying all of the following four criteria: 3 4        A   A  +   A   =  A        A   superscript  A    A   A    AA^{+}A=A\,\!   ( need not be the general identity matrix, but it maps all column vectors of   A   A   A   to themselves);        A  +   A   A  +    =   A  +          superscript  A    A   superscript  A      superscript  A      A^{+}AA^{+}=A^{+}\,\!   ( is a weak inverse for the multiplicative semigroup );        (   A   A  +    )   *   =   A   A  +         superscript    A   superscript  A         A   superscript  A       (AA^{+})^{*}=AA^{+}\,\!   ( is Hermitian ); and        (    A  +   A   )   *   =    A  +   A        superscript     superscript  A    A        superscript  A    A     (A^{+}A)^{*}=A^{+}A\,\!   ( is also Hermitian).       A  +     superscript  A     A^{+}   exists for any matrix,   A   A   A   , but when the latter has full rank ,    A  +     superscript  A     A^{+}   can be expressed as a simple algebraic formula.  In particular, when   A   A   A   has full column rank (and thus matrix     A  *   A       superscript  A    A    A^{*}A   is invertible),    A  +     superscript  A     A^{+}   can be computed as:        A  +   =     (    A  *   A   )    -  1      A  *      .       superscript  A       superscript     superscript  A    A     1     superscript  A       A^{+}=(A^{*}A)^{-1}A^{*}\,.     This particular pseudoinverse constitutes a left inverse , since, in this case,      A  +   A   =  I         superscript  A    A   I    A^{+}A=I   .  When   A   A   A   has full row rank (matrix    A   A  *       A   superscript  A      AA^{*}   is invertible),    A  +     superscript  A     A^{+}   can be computed as:        A  +   =    A  *      (   A   A  *    )    -  1       .       superscript  A       superscript  A     superscript    A   superscript  A       1       A^{+}=A^{*}(AA^{*})^{-1}\,.     This is a right inverse , as     A   A  +    =  I        A   superscript  A     I    AA^{+}=I   .  Properties  Proofs for some of these facts may be found on a separate page here .  Existence and uniqueness   The pseudoinverse exists and is unique: for any matrix   A   A   A\,\!   , there is precisely one matrix    A  +     superscript  A     A^{+}\,\!   , that satisfies the four properties of the definition. 5   A matrix satisfying the first condition of the definition is known as a generalized inverse . If the matrix also satisfies the second definition, it is called a generalized reflexive inverse . Generalized inverses always exist but are not in general unique. Uniqueness is a consequence of the last two conditions.  Basic properties   If   A   A   A\,\!   has real entries, then so does    A  +     superscript  A     A^{+}\,\!   .  If   A   A   A\,\!   is invertible , its pseudoinverse is its inverse. That is     A  +   =   A   -  1         superscript  A     superscript  A    1      A^{+}=A^{-1}\,\!   . 6  The pseudoinverse of a zero matrix is its transpose.  The pseudoinverse of the pseudoinverse is the original matrix      (   A  +   )   +   =  A       superscript   superscript  A      A    (A^{+})^{+}=A\,\!   . 7  Pseudoinversion commutes with transposition, conjugation, and taking the conjugate transpose: 8             (   A  T   )   +   =    (   A  +   )   T    ,      (    A  ¯    )   +   =    A  +   ¯    ,     (   A  *   )   +   =    (   A  +   )   *      .     formulae-sequence     superscript   superscript  A  normal-T      superscript   superscript  A    normal-T     formulae-sequence     superscript   normal-¯  A      normal-¯   superscript  A         superscript   superscript  A       superscript   superscript  A          (A^{\mathrm{T}})^{+}=(A^{+})^{\mathrm{T}},~{}~{}(\,\overline{A}\,)^{+}=%
 \overline{A^{+}},~{}~{}(A^{*})^{+}=(A^{+})^{*}.\,\!         The pseudoinverse of a scalar multiple of   A   A   A   is the reciprocal multiple of :           (   α  A   )   +   =    α   -  1     A  +         superscript    α  A        superscript  α    1     superscript  A       (\alpha A)^{+}=\alpha^{-1}A^{+}\,\!   for    α  ≠  0.      α  0.    \alpha\neq 0.        Identities  The following identities can be used to cancel certain subexpressions or expand expressions involving pseudoinverses. Proofs for these properties can be found in the proofs subpage .    \begin{array}{lclll}     A^+ &=& A^+ & A^{+*} & A^*\\ A^+ &=& A^* & A^{+*} & A^+\\ A &=& A^{+*}& A^* & A \\ A &=& A & A^* & A^{+*}\\ A^* &=& A^* & A & A^+\\ A^* &=& A^+ & A & A^*\\ \end{array}  Reduction to Hermitian case  The computation of the pseudoinverse is reducible to its construction in the Hermitian case. This is possible through the equivalences:        A  +   =     (    A  *   A   )   +    A  *         superscript  A       superscript     superscript  A    A      superscript  A       A^{+}=(A^{*}A)^{+}A^{*}\,\!          A  +   =    A  *     (   A   A  *    )   +         superscript  A       superscript  A     superscript    A   superscript  A          A^{+}=A^{*}(AA^{*})^{+}\,\!      as     A  *   A       superscript  A    A    A^{*}A   and    A   A  *       A   superscript  A      AA^{*}   are obviously Hermitian.  Products  If     A  ∈   M   (  m  ,  n  ;  K  )     ,   B  ∈   M   (  n  ,  p  ;  K  )        formulae-sequence    A    normal-M   m  n  K       B    normal-M   n  p  K       A\in\mathrm{M}(m,n;K),~{}B\in\mathrm{M}(n,p;K)\,   and either,      A   A   A\,\!   has orthonormal columns (i.e.,      A  *   A   =    I  n           superscript  A    A    subscript  I  n     A^{*}A=I_{n}\,   ) or,     B   B   B\,\!   has orthonormal rows (i.e.,     B   B  *    =    I  n          B   superscript  B      subscript  I  n     BB^{*}=I_{n}\,   ) or,     A   A   A\,\!   has all columns linearly independent (full column rank) and   B   B   B\,\!   has all rows linearly independent (full row rank) or,      B  =   A  *       B   superscript  A      B=A^{*}\,\!   (i.e.,   B   B   B   is the conjugate transpose of   A   A   A   ),   then        (   A  B   )   +   ≡    B  +    A  +         superscript    A  B        superscript  B     superscript  A       (AB)^{+}\equiv B^{+}A^{+}\,\!   .  The last property yields the equivalences:       (   A   A  *    )   +     superscript    A   superscript  A        \displaystyle(AA^{*})^{+}     Projectors      P  =   A   A  +        P    A   superscript  A       P=AA^{+}\,\!   and    Q  =    A  +   A       Q     superscript  A    A     Q=A^{+}A\,\!   are orthogonal projection operators – that is, they are Hermitian (    P  =   P  *       P   superscript  P      P=P^{*}\,\!   ,    Q  =   Q  *       Q   superscript  Q      Q=Q^{*}\,\!   ) and idempotent (     P  2   =  P       superscript  P  2   P    P^{2}=P\,\!   and     Q  2   =  Q       superscript  Q  2   Q    Q^{2}=Q\,\!   ). The following hold:        P  A   =  A  =   A  Q           P  A   A         A  Q      PA=A=AQ\,\!   and      A  +   P   =   A  +   =   Q   A  +             superscript  A    P    superscript  A           Q   superscript  A        A^{+}P=A^{+}=QA^{+}\,\!        P   P   P\,\!   is the orthogonal projector onto the range of   A   A   A\,\!   (which equals the orthogonal complement of the kernel of    A  *     superscript  A     A^{*}\,\!   ).     Q   Q   Q\,\!   is the orthogonal projector onto the range of    A  *     superscript  A     A^{*}\,\!   (which equals the orthogonal complement of the kernel of   A   A   A\,\!   ).      (   I  -  P   )      I  P    (I-P)\,\!   is the orthogonal projector onto the kernel of    A  *     superscript  A     A^{*}\,\!   .      (   I  -  Q   )      I  Q    (I-Q)\,\!   is the orthogonal projector onto the kernel of   A   A   A\,\!   . 9   Geometric construction  If we view the matrix as a linear map    A  :    K  n   →   K  m       normal-:  A   normal-→   superscript  K  n    superscript  K  m      A:K^{n}\to K^{m}   over a field   K   K   K   then     A  +   :    K  m   →   K  n       normal-:   superscript  A     normal-→   superscript  K  m    superscript  K  n      A^{+}:K^{m}\to K^{n}   can be decomposed as follows. We write   ⊕   direct-sum   \oplus   for the direct sum ,   ⟂   perpendicular-to   \perp   for the orthogonal complement ,   ker   ker   \operatorname{ker}   for the kernel of a map, and   ran   ran   \operatorname{ran}   for the image of a map. Notice that     K  n   =     (   ker  A   )   ⟂   ⊕   ker  A         superscript  K  n    direct-sum   superscript   ker  A   perpendicular-to    ker  A      K^{n}=(\operatorname{ker}A)^{\perp}\oplus\operatorname{ker}A   and     K  m   =    ran  A   ⊕    (   ran  A   )   ⟂         superscript  K  m    direct-sum   ran  A    superscript   ran  A   perpendicular-to      K^{m}=\operatorname{ran}A\oplus(\operatorname{ran}A)^{\perp}   . The restriction    A  :     (   ker  A   )   ⟂   →   ran  A       normal-:  A   normal-→   superscript   ker  A   perpendicular-to    ran  A      A:(\operatorname{ker}A)^{\perp}\to\operatorname{ran}A   is then an isomorphism. These imply that    A  +     superscript  A     A^{+}   is defined on    ran  A     ran  A    \operatorname{ran}A   to be the inverse of this isomorphism, and on     (   ran  A   )   ⟂     superscript   ran  A   perpendicular-to    (\operatorname{ran}A)^{\perp}   to be zero.  In other words: To find     A  +   b       superscript  A    b    A^{+}b   for given   b   b   b   in , first project   b   b   b   orthogonally onto the range of   A   A   A   , finding a point    p   (  b  )       p  b    p(b)   in the range. Then form , i.e. find those vectors in that   A   A   A   sends to    p   (  b  )       p  b    p(b)   . This will be an affine subspace of parallel to the kernel of   A   A   A   . The element of this subspace that has the smallest length (i.e. is closest to the origin) is the answer     A  +   b       superscript  A    b    A^{+}b   we are looking for. It can be found by taking an arbitrary member of  and projecting it orthogonally onto the orthogonal complement of the kernel of   A   A   A   .  This description is closely related to the Minimum norm solution to a linear system .  Subspaces      ker   (   A  +   )      ker   superscript  A      \displaystyle\operatorname{ker}(A^{+})     Limit relations   The pseudoinverse are limits:        A  +   =    lim   δ  ↘  0       (     A  *   A   +   δ  I    )    -  1     A  *     =    lim   δ  ↘  0      A  *     (    A   A  *    +   δ  I    )    -  1             superscript  A      subscript    normal-↘  δ  0       superscript       superscript  A    A     δ  I      1     superscript  A             subscript    normal-↘  δ  0       superscript  A     superscript      A   superscript  A       δ  I      1         A^{+}=\lim_{\delta\searrow 0}(A^{*}A+\delta I)^{-1}A^{*}=\lim_{\delta\searrow 0%
 }A^{*}(AA^{*}+\delta I)^{-1}      (see Tikhonov regularization ). These limits exist even if     (   A   A  *    )    -  1      superscript    A   superscript  A       1     (AA^{*})^{-1}\,\!   or     (    A  *   A   )    -  1      superscript     superscript  A    A     1     (A^{*}A)^{-1}\,\!   do not exist. 10    Continuity   In contrast to ordinary matrix inversion, the process of taking pseudoinverses is not continuous : if the sequence    (   A  n   )     subscript  A  n    (A_{n})   converges to the matrix   A   A   A   (in the maximum norm or Frobenius norm , say), then need not converge to . However, if all the matrices have the same rank, will converge to . 11   Derivative  The derivative of a real valued pseudoinverse matrix which has constant rank at a point   x   x   x   may be calculated in terms of the derivative of the original matrix: 12        d   d  x     A  +    (  x  )    =    -    A  +    (    d   d  x    A   )     A  +      +    A  +    A   +  T     (    d   d  x     A  T    )    (   1  -   A   A  +     )    +    (   1  -    A  +   A    )    (    d   d  x     A  T    )    A   +  T     A  +             normal-d    normal-d  x     superscript  A    x          superscript  A        normal-d    normal-d  x    A    superscript  A         superscript  A     superscript  A    T        normal-d    normal-d  x     superscript  A  T      1    A   superscript  A           1     superscript  A    A        d    d  x     superscript  A  T     superscript  A    T     superscript  A        \frac{\mathrm{d}}{\mathrm{d}x}A^{+}(x)=-A^{+}\left(\frac{\mathrm{d}}{\mathrm{d%
 }x}A\right)A^{+}~{}+~{}A^{+}A^{+\text{T}}\left(\frac{\mathrm{d}}{\mathrm{d}x}A%
 ^{\text{T}}\right)\left(1-AA^{+}\right)~{}+~{}\left(1-A^{+}A\right)\left(\frac%
 {\text{d}}{\text{d}x}A^{\text{T}}\right)A^{+\text{T}}A^{+}     Special cases  Scalars  It is also possible to define a pseudoinverse for scalars and vectors. This amounts to treating these as matrices. The pseudoinverse of a scalar   x   x   x   is zero if   x   x   x   is zero and the reciprocal of   x   x   x   otherwise:       x  +   =   {      0  ,        if  x   =  0   ;         x   -  1    ,      otherwise  .          fragments   superscript  x      fragments  normal-{    0      if  x   0      superscript  x    1    otherwise       x^{+}=\left\{\begin{matrix}0,&\mbox{if }x=0;\\
 x^{-1},&\mbox{otherwise}.\end{matrix}\right.     Vectors  The pseudoinverse of the null (all zero) vector is the transposed null vector. The pseudoinverse of a non-null vector is the conjugate transposed vector divided by its squared magnitude:       x  +   =   {       0  T   ,        if  x   =  0   ;           x  *     x  *   x     ,      otherwise  .          fragments   superscript  x      fragments  normal-{     superscript  0  normal-T       if  x   0        superscript  x       superscript  x    x    otherwise       x^{+}=\left\{\begin{matrix}0^{\mathrm{T}},&\mbox{if }x=0;\\
 {x^{*}\over x^{*}x},&\mbox{otherwise}.\end{matrix}\right.     Linearly independent columns  If the columns of   A   A   A\,\!   are linearly independent (so that    m  ≥  n      m  n    m\geq n   ), then     A  *   A       superscript  A    A    A^{*}A\,\!   is invertible. In this case, an explicit formula is: 13       A  +   =     (    A  *   A   )    -  1     A  *         superscript  A       superscript     superscript  A    A     1     superscript  A       A^{+}=(A^{*}A)^{-1}A^{*}\,\!   . It follows that    A  +     superscript  A     A^{+}\,\!   is then a left inverse of   A   A   A\,\!   :      A  +   A   =   I  n          superscript  A    A    subscript  I  n     A^{+}A=I_{n}\,\!   .  Linearly independent rows  If the rows of   A   A   A\,\!   are linearly independent (so that    m  ≤  n      m  n    m\leq n   ), then    A   A  *       A   superscript  A      AA^{*}   is invertible. In this case, an explicit formula is:       A  +   =    A  *     (   A   A  *    )    -  1          superscript  A       superscript  A     superscript    A   superscript  A       1       A^{+}=A^{*}(AA^{*})^{-1}\,\!   . It follows that    A  +     superscript  A     A^{+}\,\!   is a right inverse of   A   A   A\,\!   :     A   A  +    =   I  m         A   superscript  A      subscript  I  m     AA^{+}=I_{m}\,\!   .  Orthonormal columns or rows  This is a special case of either full column rank or full row rank (treated above). If   A   A   A\,\!   has orthonormal columns (      A  *   A   =   I  n          superscript  A    A    subscript  I  n     A^{*}A=I_{n}\,\!   ) or orthonormal rows (     A   A  *    =   I  m         A   superscript  A      subscript  I  m     AA^{*}=I_{m}\,\!   ), then     A  +   =   A  *        superscript  A     superscript  A      A^{+}=A^{*}\,\!   .  Circulant matrices  For a circulant matrix    C   C   C\,\!   , the singular value decomposition is given by the Fourier transform , that is the singular values are the Fourier coefficients. Let   ℱ   ℱ   \mathcal{F}   be the Discrete Fourier Transform (DFT) matrix , then 14     C   C   \displaystyle C     Construction  Rank decomposition  Let    r  ≤   min   (  m  ,  n  )        r    m  n     r\leq\min(m,n)   denote the rank of    A  ∈   M   (  m  ,  n  ;  K  )        A    normal-M   m  n  K      A\in\mathrm{M}(m,n;K)\,\!   . Then   A   A   A\,\!   can be (rank) decomposed as    A  =   B  C       A    B  C     A=BC\,\!   where    B  ∈   M   (  m  ,  r  ;  K  )        B    normal-M   m  r  K      B\in\mathrm{M}(m,r;K)\,\!   and    C  ∈   M   (  r  ,  n  ;  K  )        C    normal-M   r  n  K      C\in\mathrm{M}(r,n;K)\,\!   are of rank   r   r   r   . Then     A  +   =    C  +    B  +    =    C  *     (   C   C  *    )    -  1      (    B  *   B   )    -  1     B  *           superscript  A       superscript  C     superscript  B             superscript  C     superscript    C   superscript  C       1     superscript     superscript  B    B     1     superscript  B        A^{+}=C^{+}B^{+}=C^{*}(CC^{*})^{-1}(B^{*}B)^{-1}B^{*}\,\!   .  The QR method  For    K  =  ℝ      K  ℝ    K=\mathbb{R}\,\!   or    K  =  ℂ      K  ℂ    K=\mathbb{C}\,\!   computing the product    A   A  *       A   superscript  A      AA^{*}   or     A  *   A       superscript  A    A    A^{*}A   and their inverses explicitly is often a source of numerical rounding errors and computational cost in practice. An alternative approach using the QR decomposition of   A   A   A\,\!   may be used instead.  Considering the case when   A   A   A\,\!   is of full column rank, so that     A  +   =     (    A  *   A   )    -  1     A  *         superscript  A       superscript     superscript  A    A     1     superscript  A       A^{+}=(A^{*}A)^{-1}A^{*}\,\!   . Then the Cholesky decomposition       A  *   A   =    R  *   R          superscript  A    A      superscript  R    R     A^{*}A=R^{*}R\,\!   , where   R   R   R\,\!   is an upper triangular matrix , may be used. Multiplication by the inverse is then done easily by solving a system with multiple right-hand sides,        A  +   =      (    A  *   A   )    -  1     A  *    ⇔        (    A  *   A   )    A  +    =    A  *   ⇔       R  *   R   A  +    =   A  *        formulae-sequence     superscript  A        superscript     superscript  A    A     1     superscript  A     normal-⇔     formulae-sequence         superscript  A    A    superscript  A       superscript  A    normal-⇔         superscript  R    R   superscript  A      superscript  A        A^{+}=(A^{*}A)^{-1}A^{*}\quad\Leftrightarrow\quad(A^{*}A)A^{+}=A^{*}\quad%
 \Leftrightarrow\quad R^{*}RA^{+}=A^{*}   which may be solved by forward substitution followed by back substitution .  The Cholesky decomposition may be computed without forming     A  *   A       superscript  A    A    A^{*}A\,\!   explicitly, by alternatively using the QR decomposition of    A  =   Q  R       A    Q  R     A=QR\,\!   , where    Q    Q   Q\,\,\!   has orthonormal columns,      Q  *   Q   =  I         superscript  Q    Q   I    Q^{*}Q=I   , and   R   R   R\,\!   is upper triangular. Then        A  *    A    =     (   Q  R   )   *    (   Q  R   )    =    R  *    Q  *   Q   R    =    R  *   R            superscript  A    A      superscript    Q  R       Q  R            superscript  R     superscript  Q    Q  R           superscript  R    R      A^{*}A\,=\,(QR)^{*}(QR)\,=\,R^{*}Q^{*}QR\,=\,R^{*}R   , so   R   R   R   is the Cholesky factor of     A  *   A       superscript  A    A    A^{*}A   .  The case of full row rank is treated similarly by using the formula     A  +   =    A  *     (   A   A  *    )    -  1          superscript  A       superscript  A     superscript    A   superscript  A       1       A^{+}=A^{*}(AA^{*})^{-1}\,\!   and using a similar argument, swapping the roles of   A   A   A   and    A  *     superscript  A     A^{*}   .  Singular value decomposition (SVD)  A computationally simple and accurate way to compute the pseudo inverse is by using the singular value decomposition . 15 16 17 If    A  =   U  Σ   V  *        A    U  normal-Σ   superscript  V       A=U\Sigma V^{*}   is the singular value decomposition of   A   A   A   , then     A  +   =   V   Σ  +    U  *         superscript  A      V   superscript  normal-Σ     superscript  U       A^{+}=V\Sigma^{+}U^{*}   . For a rectangular diagonal matrix such as   Σ   normal-Σ   \Sigma   , we get the pseudo inverse by taking the reciprocal of each non-zero element on the diagonal, leaving the zeros in place, and then transposing the matrix. In numerical computation, only elements larger than some small tolerance are taken to be nonzero, and the others are replaced by zeros. For example, in the MATLAB , GNU Octave , or NumPy function pinv , the tolerance is taken to be    t  =   ε  ⋅  m  a  x   (  m  ,  n  )   ⋅  m  a  x   (  Σ  )        t    ε  normal-⋅  m  a  x   m  n   normal-⋅  m  a  x  normal-Σ     t=ε⋅max(m,n)⋅max(Σ)   , where ε is the machine epsilon .  The computational cost of this method is dominated by the cost of computing the SVD, which is several times higher than matrix–matrix multiplication, even if a state-of-the art implementation (such as that of LAPACK ) is used.  The above procedure shows why taking the pseudo inverse is not a continuous operation: if the original matrix   A   A   A   has a singular value 0 (a diagonal entry of the matrix   Σ   normal-Σ   \Sigma   above), then modifying   A   A   A   slightly may turn this zero into a tiny positive number, thereby affecting the pseudo inverse dramatically as we now have to take the reciprocal of a tiny number.  Block matrices  Optimized approaches exist for calculating the pseudoinverse of block structured matrices.  The iterative method of Ben-Israel and Cohen  Another method for computing the pseudoinverse uses the recursion        A   i  +  1    =    2   A  i    -    A  i   A   A  i      ,       subscript  A    i  1        2   subscript  A  i       subscript  A  i   A   subscript  A  i       A_{i+1}=2A_{i}-A_{i}AA_{i},\,   which is sometimes referred to as hyper-power sequence. This recursion produces a sequence converging quadratically to the pseudoinverse of   A   A   A   if it is started with an appropriate    A  0     subscript  A  0    A_{0}   satisfying      A  0   A   =    (    A  0   A   )   *          subscript  A  0   A    superscript     subscript  A  0   A       A_{0}A=(A_{0}A)^{*}   . The choice     A  0   =   α   A  *         subscript  A  0     α   superscript  A       A_{0}=\alpha A^{*}   (where    0  <  α  <    2  /   σ  1  2     (  A  )          0  α           2   subscript   superscript  σ  2   1    A      0<\alpha<2/\sigma^{2}_{1}(A)   , with     σ  1    (  A  )        subscript  σ  1   A    \sigma_{1}(A)   denoting the largest singular value of   A   A   A   ) 18 has been argued not to be competitive to the method using the SVD mentioned above, because even for moderately ill-conditioned matrices it takes a long time before    A  i     subscript  A  i    A_{i}   enters the region of quadratic convergence. 19 However, if started with    A  0     subscript  A  0    A_{0}   already close to the Moore–Penrose pseudoinverse and      A  0   A   =    (    A  0   A   )   *          subscript  A  0   A    superscript     subscript  A  0   A       A_{0}A=(A_{0}A)^{*}   , for example     A  0   :=     (     A  *   A   +   δ  I    )    -  1     A  *       assign   subscript  A  0      superscript       superscript  A    A     δ  I      1     superscript  A       A_{0}:=(A^{*}A+\delta I)^{-1}A^{*}   , convergence is fast (quadratic).  Updating the pseudoinverse  For the cases where   A   A   A   has full row or column rank, and the inverse of the correlation matrix (    A   A  *       A   superscript  A      AA^{*}   for   A   A   A   with full row rank or     A  *   A       superscript  A    A    A^{*}A   for full column rank) is already known, the pseudoinverse for matrices related to   A   A   A   can be computed by applying the Sherman–Morrison–Woodbury formula to update the inverse of the correlation matrix, which may need less work. In particular, if the related matrix differs from the original one by only a changed, added or deleted row or column, incremental algorithms 20 21 exist that exploit the relationship.  Similarly, it is possible to update the Cholesky factor when a row or column is added, without creating the inverse of the correlation matrix explicitly. However, updating the pseudoinverse in the general rank-deficient case is much more complicated. 22 23  Software libraries  The package NumPy provides a pseudoinverse calculation through its functions matrix.I and linalg.pinv ; its pinv uses the SVD-based algorithm. SciPy adds a function scipy.linalg.pinv that uses a least-squares solver. High quality implementations of SVD, QR, and back substitution are available in standard libraries , such as LAPACK . Writing one's own implementation of SVD is a major programming project that requires a significant numerical expertise . In special circumstances, such as parallel computing or embedded computing , however, alternative implementations by QR or even the use of an explicit inverse might be preferable, and custom implementations may be unavoidable.  Applications  Linear least-squares  The pseudoinverse provides a least squares solution to a system of linear equations . 24 For    A  ∈   M   (  m  ,  n  ;  K  )        A    normal-M   m  n  K      A\in\mathrm{M}(m,n;K)\,\!   , given a system of linear equations        A  x   =  b   ,        A  x   b    Ax=b,\,     in general, a vector   x   x   x   that solves the system may not exist, or if one does exist, it may not be unique. The pseudoinverse solves the "least-squares" problem as follows:        ∀  x   ∈   K  n        for-all  x    superscript  K  n     \forall x\in K^{n}\,\!   , we have      ∥    A  x   -  b   ∥   2   ≥    ∥    A  z   -  b   ∥   2        subscript   norm      A  x   b    2    subscript   norm      A  z   b    2     \|Ax-b\|_{2}\geq\|Az-b\|_{2}   where    z  =    A  +   b       z     superscript  A    b     z=A^{+}b   and    ∥  ⋅   ∥  2      fragments  parallel-to  normal-⋅   subscript  parallel-to  2     \|\cdot\|_{2}   denotes the Euclidean norm . This weak inequality holds with equality if and only if    x  =     A  +   b   +    (   I  -    A  +   A    )   w        x       superscript  A    b       I     superscript  A    A    w      x=A^{+}b+(I-A^{+}A)w   for any vector w ; this provides an infinitude of minimizing solutions unless A has full column rank, in which case    (   I  -    A  +   A    )      I     superscript  A    A     (I-A^{+}A)   is a zero matrix. 25 The solution with minimum Euclidean norm is    z  .    z   z.    26   This result is easily extended to systems with multiple right-hand sides, when the Euclidean norm is replaced by the Frobenius norm. Let    B  ∈   M   (  m  ,  p  ;  K  )        B    normal-M   m  p  K      B\in\mathrm{M}(m,p;K)   .        ∀  X   ∈   M   (  n  ,  p  ;  K  )         for-all  X     normal-M   n  p  K      \forall X\in\mathrm{M}(n,p;K)\,\!   , we have      ∥    A  X   -  B   ∥   F   ≥    ∥    A  Z   -  B   ∥   F        subscript   norm      A  X   B    normal-F    subscript   norm      A  Z   B    normal-F     \|AX-B\|_{\mathrm{F}}\geq\|AZ-B\|_{\mathrm{F}}   where    Z  =    A  +   B       Z     superscript  A    B     Z=A^{+}B   and    ∥  ⋅   ∥  F      fragments  parallel-to  normal-⋅   subscript  parallel-to  normal-F     \|\cdot\|_{\mathrm{F}}   denotes the Frobenius norm .   Obtaining all solutions of a linear system  If the linear system       A  x   =   b         A  x   b    Ax=b\,     has any solutions, they are all given by 27      x  =     A  +   b   +    [   I  -    A  +   A    ]   w        x       superscript  A    b      delimited-[]    I     superscript  A    A     w      x=A^{+}b+[I-A^{+}A]w     for arbitrary vector w . Solution(s) exist if and only if     A   A  +   b   =  b        A   superscript  A    b   b    AA^{+}b=b   . 28 If the latter holds, then the solution is unique if and only if A has full column rank, in which case    [   I  -    A  +   A    ]     delimited-[]    I     superscript  A    A      [I-A^{+}A]   is a zero matrix. If solutions exist but A does not have full column rank, then we have an indeterminate system , all of whose infinitude of solutions are given by this last equation. This solution is deeply connected to the Udwadia–Kalaba equation of classical mechanics to forces of constraint that do not obey D'Alembert's principle .  Minimum norm solution to a linear system  For linear systems      A  x   =  b   ,        A  x   b    Ax=b,\,   with non-unique solutions (such as under-determined systems), the pseudoinverse may be used to construct the solution of minimum Euclidean norm      ∥  x  ∥   2     subscript   norm  x   2    \|x\|_{2}   among all solutions.   If     A  x   =   b         A  x   b    Ax=b\,   is satisfiable, the vector    z  =    A  +   b       z     superscript  A    b     z=A^{+}b   is a solution, and satisfies      ∥  z  ∥   2   ≤    ∥  x  ∥   2        subscript   norm  z   2    subscript   norm  x   2     \|z\|_{2}\leq\|x\|_{2}   for all solutions.   This result is easily extended to systems with multiple right-hand sides, when the Euclidean norm is replaced by the Frobenius norm. Let    B  ∈   M   (  m  ,  p  ;  K  )        B    normal-M   m  p  K      B\in\mathrm{M}(m,p;K)\,\!   .   If     A  X   =   B         A  X   B    AX=B\,   is satisfiable, the matrix    Z  =    A  +   B       Z     superscript  A    B     Z=A^{+}B   is a solution, and satisfies      ∥  Z  ∥   F   ≤    ∥  X  ∥   F        subscript   norm  Z   normal-F    subscript   norm  X   normal-F     \|Z\|_{\mathrm{F}}\leq\|X\|_{\mathrm{F}}   for all solutions.   Condition number  Using the pseudoinverse and a matrix norm , one can define a condition number for any matrix:        cond   (  A  )    =    ∥  A  ∥    ∥   A  +   ∥     .        cond  A      norm  A    norm   superscript  A        \mbox{cond}(A)=\|A\|\|A^{+}\|.   A large condition number implies that the problem of finding least-squares solutions to the corresponding system of linear equations is ill-conditioned in the sense that small errors in the entries of   A   A   A   can lead to huge errors in the entries of the solution. 29  Generalizations  In order to solve more general least-squares problems, one can define Moore–Penrose pseudoinverses for all continuous linear operators between two Hilbert spaces  and , using the same four conditions as in our definition above. It turns out that not every continuous linear operator has a continuous linear pseudoinverse in this sense. 30 Those that do are precisely the ones whose range is closed in .  In abstract algebra , a Moore–Penrose pseudoinverse may be defined on a *-regular semigroup . This abstract definition coincides with the one in linear algebra.  See also   Proofs involving the Moore–Penrose pseudoinverse  Drazin inverse  Hat matrix  Inverse element  Linear least squares (mathematics)  Pseudo-determinant  Von Neumann regular ring   References  External links   Interactive program & tutorial of Moore–Penrose Pseudoinverse     The Moore–Penrose Pseudoinverse. A Tutorial Review of the Theory  Online Moore-Penrose Inverse calculator   "  Category:Matrix theory  Category:Singular value decomposition  Category:Numerical linear algebra     ↩  ↩   ↩   . ↩      ↩  http://mathoverflow.net/questions/25778/analytical-formula-for-numerical-derivative-of-the-matrix-pseudo-inverse ↩   ↩    Linear Systems & Pseudo-Inverse ↩  pdf ↩  ↩  ↩  , Mohammad Emtiyaz, "Updating Inverse of a Matrix When a Column is Added/Removed" 1 ↩  Meyer, Carl D., Jr. Generalized inverses and ranks of block matrices. SIAM J. Appl. Math. 25 (1973), 597–602 ↩  Meyer, Carl D., Jr. Generalized inversion of modified matrices. SIAM J. Appl. Math. 24 (1973), 315–323 ↩  ↩  Planitz, M., "Inconsistent systems of linear equations", Mathematical Gazette 63, October 1979, 181–185. ↩   James, M., "The generalised inverse", Mathematical Gazette 62, June 1978, 109–114. ↩    Roland Hagen, Steffen Roch, Bernd Silbermann. C*-algebras and Numerical Analysis , CRC Press, 2001. Section 2.1.2. ↩     