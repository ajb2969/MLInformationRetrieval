   Optimistic knowledge gradient      Optimistic knowledge gradient   In statistics The optimistic knowledge gradient 1 is a new approximation policy proposed by Xi Chen, Qihang Lin and Dengyong Zhou in 2013. This policy is created to solve the challenge of computationally intractable of large size of optimal computing budget allocation problem in binary/multi-class crowd labeling where each label from the crowd has a certain cost. 2  Motivation  The optimal computing budget allocation problem is formulated as a Bayesian Markov decision process 3 (MDP) and is solved by using the dynamic programming (DP) algorithm where the Optimistic knowledge gradient policy is used to solve the computationally intractable of the dynamic programming 4 (DP) algorithm.  Consider a budget allocation issue in crowdsourcing . The particular crowdsourcing problem we considering is crowd labeling. Crowd labeling is a large amount of labeling tasks which are hard to solve by machine, turn out to easy to solve by human beings, then we just outsourced to an unidentified group of random people in a distributed environment.  Methodology  We want to finish this labeling tasks rely on the power of the crowd hopefully. For example, suppose we want to identify a picture according to the people in a picture is adult or not, this is a Bernoulli labeling problem, and all of us can do in one or two seconds, this is a easy task for human being. However, if we have tens of thousands picture like this, then this is no longer the easy task any more. That's why we need to rely on crowdsourcing framework to make this fast. Crowdsourcing framework of this consists of two steps. Step one, we just dynamically acquire from the crowd for items. On the other sides, this is dynamic procedure. We don't just send out this picture to everyone and we focus every response, instead, we do this in quantity. We are going to decide which picture we send it in the next, and which worker we are going to hire in the crowd in the next. According to his or her historical labeling results. And each picture can be send to multiple workers and every worker can also work on different pictures. Then after we collect enough number of labels for different picture, we go to the second steps where we want to infer true label of each picture based on the collected labels. So there are multiple ways we can do inference. For instance, the simplest we can do this is just majority vote. The problem is that no free lunch, we have to pays for worker for each label he or she provides and we only have a limited project budget. So the question is how to spend the limited budget in a smart way.  Challenging  Before show mathematic model, the paper mentions what kinds of challenging we are facing.  challenging 1  First of all, the items are very different difficult level to compute label, for example, go back the forward example, some picture are easily to classify, usually you will see very consistent label from the crowd. However, some ambiguous pictures, usually people disagree with each other, and label you receive highly inconsistent. So usually we spend more money on this ambiguous task.  challenging 2  And another difficulty we often have is that the worker are not perfect, sometimes this worker are not responsible, they just provide the random label, therefore, of course, we would not spend our budget on this no reliable workers. Now the problem is both the difficulty of the pictures and the reliability of the worker we completely unknown at the beginning. We can only estimate them during the procedure. Therefore, we are naturally facing to exploration and exploitation, and our goal is to give a reasonable good policy to spend money to the right way‚Äìmaximize the overall accuracy of final inferred labels.  Mathematical model  For the mathematical model, we have the K items,    i  =   {  1  ,  2  ,  ‚Ä¶  ,  k  }       i   1  2  normal-‚Ä¶  k     i=\{1,2,\ldots,k\}   , and total budget is T and we assume each label cost 1 so we are going to have T labels eventually. We assume each items has true label    Z  i     subscript  Z  i    Z_{i}   which positive or negative, this binomial cases and we can extended to multiple class, labeling cases, this a singular idea. And the positive set    H  *     superscript  H     H^{*}   is defined as the set of items whose true label is positive. And    Œ∏  i     subscript  Œ∏  i    \theta_{i}   also defined a soft-label,    Œ∏  i     subscript  Œ∏  i    \theta_{i}   for each item which number between 0 and 1, and we define    Œ∏  i     subscript  Œ∏  i    \theta_{i}   as underlying probability of being labeled as positive by a member randomly picked from a group of perfect workers.  In this first case, we assume for every worker is perfect, it means they all reliable, but being perfect doesn‚Äôt means this worker gives the same answer or right answer. It just means they will try their best to figure out the best answer in their mind, and suppose everyone is perfect worker, just randomly picked one of them, and with    Œ∏  i     subscript  Œ∏  i    \theta_{i}   probability, we going to get a guy who believe this one is positive. That is how we explain    Œ∏  i     subscript  Œ∏  i    \theta_{i}   . So we are assume a label    Y  i     subscript  Y  i    Y_{i}   is drawn from Bernoulli(    Œ∏  i     subscript  Œ∏  i    \theta_{i}   ), and    Œ∏  i     subscript  Œ∏  i    \theta_{i}   must be consistent with the true label, which means    Œ∏  i     subscript  Œ∏  i    \theta_{i}   is greater or equal to 0.5 if and only if this item is positive with a true positive label. So our goal is to learn H*, the set of positive items. In other word, we want to make an inferred positive set H based on collected labels to maximize:       ‚àë   i  =  1   k    (     ùüè   (  i  ‚àà  H  )     ùüè   (  i  ‚àà   H  ‚ãÜ   )     +    ùüè   (  i  ‚àâ  H  )     ùüè   (  i  ‚àâ   H  ‚ãÜ   )      )       superscript   subscript     i  1    k        subscript  1   fragments  normal-(  i   H  normal-)     subscript  1   fragments  normal-(  i    superscript  H  normal-‚ãÜ   normal-)        subscript  1   fragments  normal-(  i   H  normal-)     subscript  1   fragments  normal-(  i    superscript  H  normal-‚ãÜ   normal-)        \sum_{i=1}^{k}(\textbf{1}_{(i\in H)}\textbf{1}_{(i\in H^{\star})}+\textbf{1}_{%
 (i\notin H)}\textbf{1}_{(i\notin H^{\star})})     It can also be written as:       |   H  ‚à©   H  ‚ãÜ    |   +   |    H  c   ‚à©   H    ‚ãÜ  c     |           H   superscript  H  normal-‚ãÜ          superscript  H  c    superscript  H   normal-‚ãÜ  absent  c        |H\cap H^{\star}|+|H^{c}\cap H^{\star c}|     step1: Bayesian decision process  Before show the Bayesian framework, the paper use an example to mention why we choose Bayesian instead of frequency approach, such that we can propose some posterior of prior distribution on the soft-label    Œ∏  i     subscript  Œ∏  i    \theta_{i}   . We assume each    Œ∏  i     subscript  Œ∏  i    \theta_{i}   is drawn from a known Beta prior:     Œ∏  i   ‚àº   Beta   (   a  i  o   ,   b  i  o   )       similar-to   subscript  Œ∏  i     Beta    superscript   subscript  a  i   o    superscript   subscript  b  i   o       \theta_{i}\sim\mathrm{Beta}(a_{i}^{o},b_{i}^{o})     And the matrix:       s  o   =    ‚ü®   (   a  i  o   ,   b  i  o   )   ‚ü©    i  =  1   k   ‚àà   ùêë   k  √ó  2           superscript  s  o    superscript   subscript   delimited-‚ü®‚ü©    superscript   subscript  a  i   o    superscript   subscript  b  i   o       i  1    k         superscript  R    k  2       s^{o}=\left\langle(a_{i}^{o},b_{i}^{o})\right\rangle_{i=1}^{k}\in\textbf{R}^{k%
 \times 2}     So we know that the Bernoulli conjugate of beta, so once we get a new label for item i, we going to update posterior distribution, the beta distribution by:       Œ∏  i   ‚àº   Beta   (   a  i  t   ,   b  i  t   )       similar-to   subscript  Œ∏  i     Beta    superscript   subscript  a  i   t    superscript   subscript  b  i   t       \theta_{i}\sim\mathrm{Beta}(a_{i}^{t},b_{i}^{t})          y  i   ‚à£   Œ∏  i   ‚àº  Bernoulli   (   Œ∏  i   )      fragments   subscript  y  i   normal-‚à£   subscript  Œ∏  i   similar-to  Bernoulli   fragments  normal-(   subscript  Œ∏  i   normal-)     y_{i}\mid\theta_{i}\sim\mathrm{Bernoulli}(\theta_{i})          Œ∏  i   ‚à£   y  i   =  1  ‚àº  Beta   (   a  i  t   +  1  ,   b  i  t   )      fragments   subscript  Œ∏  i   normal-‚à£   subscript  y  i    1  similar-to  Beta   fragments  normal-(   superscript   subscript  a  i   t    1  normal-,   superscript   subscript  b  i   t   normal-)     \theta_{i}\mid y_{i}=1\sim\mathrm{Beta}(a_{i}^{t}+1,b_{i}^{t})          Œ∏  i   ‚à£   y  i   =  -  1  ‚àº  Beta   (   a  i  t   +  1  ,   b  i  t   )      fragments   subscript  Œ∏  i   normal-‚à£   subscript  y  i     1  similar-to  Beta   fragments  normal-(   superscript   subscript  a  i   t    1  normal-,   superscript   subscript  b  i   t   normal-)     \theta_{i}\mid y_{i}=-1\sim\mathrm{Beta}(a_{i}^{t}+1,b_{i}^{t})     Depending on the label is positive or negative.  Here is the whole procedure in the high level, we have T stage,    0  ‚â§  t  ‚â§   T  -  1         0  t         T  1      0\leq t\leq T-1   . And in current stage we look at matrix S, which summarized the posterior distribution information for all the    Œ∏  i     subscript  Œ∏  i    \theta_{i}          s  t   =    ‚ü®   (   a  i  t   ,   b  i  t   )   ‚ü©    i  =  1   k   ‚àà   ùêë   k  √ó  2           superscript  s  t    superscript   subscript   delimited-‚ü®‚ü©    superscript   subscript  a  i   t    superscript   subscript  b  i   t       i  1    k         superscript  R    k  2       s^{t}=\left\langle(a_{i}^{t},b_{i}^{t})\right\rangle_{i=1}^{k}\in\textbf{R}^{k%
 \times 2}     We are going to make a decision, choose the next item to label    i  t     subscript  i  t    i_{t}   ,     i  t   ‚àà   {  1  ,  2  ,  ‚Ä¶  ,  k  }        subscript  i  t    1  2  normal-‚Ä¶  k     i_{t}\in\{1,2,\ldots,k\}   .  And depending what the label is positive or negative, we add a matrix to getting a label:       Œ∏  i   ‚àº   Beta   (   a  i  t   ,   b  i  t   )       similar-to   subscript  Œ∏  i     Beta    superscript   subscript  a  i   t    superscript   subscript  b  i   t       \theta_{i}\sim\mathrm{Beta}(a_{i}^{t},b_{i}^{t})          y  i   ‚à£   Œ∏  i   ‚àº  Bernoulli   (   Œ∏  i   )      fragments   subscript  y  i   normal-‚à£   subscript  Œ∏  i   similar-to  Bernoulli   fragments  normal-(   subscript  Œ∏  i   normal-)     y_{i}\mid\theta_{i}\sim\mathrm{Bernoulli}(\theta_{i})          Œ∏  i   ‚à£   y  i   =  1  ‚àº  Beta   (   a  i  t   +  1  ,   b  i  t   )      fragments   subscript  Œ∏  i   normal-‚à£   subscript  y  i    1  similar-to  Beta   fragments  normal-(   superscript   subscript  a  i   t    1  normal-,   superscript   subscript  b  i   t   normal-)     \theta_{i}\mid y_{i}=1\sim\mathrm{Beta}(a_{i}^{t}+1,b_{i}^{t})          Œ∏  i   ‚à£   y  i   =  -  1  ‚àº  Beta   (   a  i  t   +  1  ,   b  i  t   )      fragments   subscript  Œ∏  i   normal-‚à£   subscript  y  i     1  similar-to  Beta   fragments  normal-(   superscript   subscript  a  i   t    1  normal-,   superscript   subscript  b  i   t   normal-)     \theta_{i}\mid y_{i}=-1\sim\mathrm{Beta}(a_{i}^{t}+1,b_{i}^{t})     Above all, this is the whole framework.  step2: Inference on positive set  When the t labels are collected, we can make an inference about the positive set H t based on posterior distribution given by S t      H  t     subscript  H  t    \displaystyle H_{t}     So here become the Bernoulli selection problem, we just take to look at the probability of being positive or being negative conditional    S  t     subscript  S  t    S_{t}   to see is greater than 0.5 or not, if it is greater than 0.5, then we prove this item into the current infer positive set    H  t     subscript  H  t    H_{t}   so this is a cost form for current optimal solution    H  t     subscript  H  t    H_{t}   based on the information in    S  t     subscript  S  t    S_{t}   .  After know what is optimal solution, then the paper show what is the optimal value. Plug   t   t   t   in the optimal function,       h   (  x  )    =   max   (  x  ,   1  -  x   )          h  x     x    1  x      h(x)=\max(x,1-x)     This function is just a single function which choose the larger one between the conditional probability of being positive and being negative. Once we get one more label for item i, we take a difference between this value, before and after we get a new label, we can see this conditional probability can actually simplify as follows:      R   (   s  t   ,   i  t   ,   y   i  t    )       R    superscript  s  t    subscript  i  t    subscript  y   subscript  i  t       \displaystyle R(s^{t},i_{t},y_{i_{t}})     The positive item being positive only depends on the beta posterior, therefore, if only the function of parameter of beta distribution function are a and b , as       h   (   Pr   (   a   i  t    t  +  1    ,   b   i  t    t  +  1    )    )    -   h   (   Pr   (   a   i  t   t   ,   b   i  t   t   )    )          h   Pr   superscript   subscript  a   subscript  i  t      t  1     superscript   subscript  b   subscript  i  t      t  1        h   Pr   superscript   subscript  a   subscript  i  t    t    superscript   subscript  b   subscript  i  t    t       h(\Pr(a_{i_{t}}^{t+1},b_{i_{t}}^{t+1}))-h(\Pr(a_{i_{t}}^{t},b_{i_{t}}^{t}))     One more label for this particular item, we double change the posterior function, so all of this items can be cancel except 1, so this is the change for whole accuracy and we defined as stage-wise reward: improvement the inference accuracy by one more sample. Of course this label have two positive value, we‚Äôve get positive label or negative label, take average for this two, get expect reward. We just choose item to be label such that the expect reward is maximized using Knowledge Gradient :      i  t     subscript  i  t    \displaystyle i_{t}     They are multiple items, let us know how do we break the ties. If we break the tie deterministically, which means we choose the smallest index. We are going to have a problem because this is not consistent which means the positive stage    H  t     subscript  H  t    H_{t}   does not converge to the true positive stage    H  *     superscript  H     H^{*}   .  So we can also try to break the ties randomly, it works, however, we will see the performance is almost like uniform sampling, is the best reward. The writer‚Äôs policy is kinds of more greedy, instead of choosing the average in stage once reward, we can actually calculate the larger one, the max of the two stage possible reward, so Optimistic Knowledge Gradient :       i  t   =    argmax   i  ‚àà   {  1  ,  ‚Ä¶  ,  k  }      (    R  +    (   S  t   ,  i  )    )    =   max   (   R   (   S  t   ,  i  ,  1  )    ,   R   (   S  t   ,  i  ,   -  1   )    )           subscript  i  t     subscript  argmax    i   1  normal-‚Ä¶  k        superscript  R      superscript  S  t   i              R    superscript  S  t   i  1      R    superscript  S  t   i    1         i_{t}=\operatorname{argmax}\limits_{i\in\{1,\ldots,k\}}(R^{+}(S^{t},i))=\max(R%
 (S^{t},i,1),R(S^{t},i,-1))     And we know under optimistic knowledge gradient, the final inference accuracy converge to 100%. Above is based on every worker is perfect, however, in practice, workers are not always responsible. So if in imperfect workers, we assume K items,    1  ‚â§  i  ‚â§  k        1  i       k     1\leq i\leq k   .       Œ∏  i   ‚àà   (  0  ,  1  )   ‚àº   Bet  a   (   a  i  o   ,   b  i  o   )           subscript  Œ∏  i    0  1     similar-to      Bet  a    superscript   subscript  a  i   o    superscript   subscript  b  i   o        \theta_{i}\in(0,1)\sim\mathrm{Bet}a(a_{i}^{o},b_{i}^{o})     The probability of item   i   i   i   being labeled as positive by a perfect worker. M workers,    1  ‚â§  j  ‚â§  M        1  j       M     1\leq j\leq M   ,     œÅ  j   ‚àà   (  0  ,  1  )   ‚àº   Beta   (   c  j  o   ,   d  j  o   )           subscript  œÅ  j    0  1     similar-to      Beta    superscript   subscript  c  j   o    superscript   subscript  d  j   o        \rho_{j}\in(0,1)\sim\mathrm{Beta}(c_{j}^{o},d_{j}^{o})   The probability of worker   j   j   j   giving the same label as a perfect worker. Distribution of the label    Z   i  j      subscript  Z    i  j     Z_{ij}   from worker   j   j   j   to item   i   i   i   :       Pr   (    Z   i  j    =  1   ‚à£   Œ∏  i   ,   œÅ  j   )    =     Pr   (    Z   i  j    =  1   ‚à£    Y  i   =  1   )     Pr   (    Y  i   =  1   )     +    Pr   (    Z   i  j    =  1   ‚à£    Y  i   =   -  1    )     Pr   (    Y  i   =   -  1    )      =    œÅ  j    Œ∏  i   t   (   1  -   œÅ  j    )    (   1  -   Œ∏  i    )           Pr     subscript  Z    i  j    1    subscript  Œ∏  i    subscript  œÅ  j         Pr     subscript  Z    i  j    1      subscript  Y  i   1     Pr     subscript  Y  i   1        Pr     subscript  Z    i  j    1      subscript  Y  i     1      Pr     subscript  Y  i     1               subscript  œÅ  j    subscript  Œ∏  i   t    1   subscript  œÅ  j      1   subscript  Œ∏  i        \Pr(Z_{ij}=1\mid\theta_{i},\rho_{j})=\Pr(Z_{ij}=1\mid Y_{i}=1)\Pr(Y_{i}=1)+\Pr%
 (Z_{ij}=1\mid Y_{i}=-1)\Pr(Y_{i}=-1)=\rho_{j}\theta_{i}t(1-\rho_{j})(1-\theta_%
 {i})     And the action space is that      Pr   (   Z   i  j    =  1  ‚à£   Œ∏  i   ,   œÅ  j   )   =  P  r   (   Z   i  j    =  1  ‚à£   Y  i   =  1  )   Pr   (   Y  i   =  1  )   +  Pr   (   Z   i  j    =  1  ‚à£   Y  i   =  -  1  )   Pr   (   Y  i   =  -  1  )   =   œÅ  j    Œ∏  i   t   (  1  -   œÅ  j   )    (  1  -   Œ∏  i   )   =   œÅ  j    Œ∏  i   t   (  1  -   œÅ  j   )    (  1  -   Œ∏  i   )   ,     fragments  Pr   fragments  normal-(   subscript  Z    i  j     1  normal-‚à£   subscript  Œ∏  i   normal-,   subscript  œÅ  j   normal-)    P  r   fragments  normal-(   subscript  Z    i  j     1  normal-‚à£   subscript  Y  i    1  normal-)   Pr   fragments  normal-(   subscript  Y  i    1  normal-)    Pr   fragments  normal-(   subscript  Z    i  j     1  normal-‚à£   subscript  Y  i     1  normal-)   Pr   fragments  normal-(   subscript  Y  i     1  normal-)     subscript  œÅ  j    subscript  Œ∏  i   t   fragments  normal-(  1    subscript  œÅ  j   normal-)    fragments  normal-(  1    subscript  Œ∏  i   normal-)     subscript  œÅ  j    subscript  Œ∏  i   t   fragments  normal-(  1    subscript  œÅ  j   normal-)    fragments  normal-(  1    subscript  Œ∏  i   normal-)   normal-,    \Pr(Z_{ij}=1\mid\theta_{i},\rho_{j})=Pr(Z_{ij}=1\mid Y_{i}=1)\Pr(Y_{i}=1)+\Pr(%
 Z_{ij}=1\mid Y_{i}=-1)\Pr(Y_{i}=-1)=\rho_{j}\theta_{i}t(1-\rho_{j})(1-\theta_{%
 i})=\rho_{j}\theta_{i}t(1-\rho_{j})(1-\theta_{i}),     where     (  i  ,  j  )   ‚àà    {  1  ,  2  ,  ‚Ä¶  ,  k  }   √ó   {  1  ,  2  ,  ‚Ä¶  ,  M  }         i  j      1  2  normal-‚Ä¶  k    1  2  normal-‚Ä¶  M      \qquad\qquad(i,j)\in\{1,2,\ldots,k\}\times\{1,2,\ldots,M\}   , label matrix     Z   i  j    ‚àà   {   -  1   ,  1  }        subscript  Z    i  j       1   1     Z_{ij}\in\{-1,1\}     It is difficult to calculate, so we can use Variational Bayesian methods 5 of    Pr   (   i  ‚àà   H  ‚ãÜ    ‚à£   S  t   )      Pr    i   superscript  H  normal-‚ãÜ     superscript  S  t     \Pr(i\in H^{\star}\mid S^{t})     References  "     1 Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling Xi Chen, Qihang Lin, Dengyong Zhou; 16(Jan):1‚àí46, 2015. ‚Ü©  2 Proceedings of the 30-th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR:W&CP; volume 28. Xi Chen, Qihang Lin, Dengyong Zhou ‚Ü©  * Learning to Solve Markovian Decision Processes by Satinder P. Singh ‚Ü©  An Introduction to Dynamic Programming ‚Ü©  * Variational-Bayes Repository A repository of papers, software, and links related to the use of variational methods for approximate Bayesian learning ‚Ü©     