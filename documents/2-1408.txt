   Diagonalizable matrix      Diagonalizable matrix   In linear algebra , a square matrix  A is called diagonalizable if it is similar to a diagonal matrix , i.e., if there exists an invertible matrix  P such that P −1 AP is a diagonal matrix. If V is a finite- dimensional  vector space , then a linear map  T : V → V is called diagonalizable if there exists an ordered basis of V with respect to which T is represented by a diagonal matrix. Diagonalization is the process of finding a corresponding diagonal matrix for a diagonalizable matrix or linear map. 1 A square matrix that is not diagonalizable is called defective .  Diagonalizable matrices and maps are of interest because diagonal matrices are especially easy to handle: their eigenvalues and eigenvectors are known and one can raise a diagonal matrix to a power by simply raising the diagonal entries to that same power. Geometrically, a diagonalizable matrix is an inhomogeneous dilation (or anisotropic scaling ) — it scales the space, as does a homogeneous dilation , but by a different factor in each direction, determined by the scale factors on each axis (diagonal entries).  Characterization  The fundamental fact about diagonalizable maps and matrices is expressed by the following:   An n × n matrix A over the field  F is diagonalizable if and only if the sum of the dimensions of its eigenspaces is equal to n , which is the case if and only if there exists a basis of F n consisting of eigenvectors of A . If such a basis has been found, one can form the matrix P having these basis vectors as columns, and P −1 AP will be a diagonal matrix. The diagonal entries of this matrix are the eigenvalues of A .  A linear map T : V → V is diagonalizable if and only if the sum of the dimensions of its eigenspaces is equal to dim( V ), which is the case if and only if there exists a basis of V consisting of eigenvectors of T . With respect to such a basis, T will be represented by a diagonal matrix. The diagonal entries of this matrix are the eigenvalues of T .   Another characterization: A matrix or linear map is diagonalizable over the field F if and only if its minimal polynomial is a product of distinct linear factors over F . (Put in another way, a matrix is diagonalizable if and only if all of its elementary divisors are linear.)  The following sufficient (but not necessary) condition is often useful.   An n × n matrix A is diagonalizable over the field F if it has n distinct eigenvalues in F , i.e. if its characteristic polynomial has n distinct roots in F ; however, the converse may be false. Let us consider          [      -  1     3     -  1        -  3     5     -  1        -  3     3    1     ]   ,        1   3    1       3   5    1       3   3  1     \begin{bmatrix}-1&3&-1\\
 -3&5&-1\\
 -3&3&1\end{bmatrix},       which has eigenvalues 1, 2, 2 (not all distinct) and is diagonalizable with diagonal form ( similar to A )     [     1    0    0      0    2    0      0    0    2     ]      1  0  0    0  2  0    0  0  2     \begin{bmatrix}1&0&0\\
 0&2&0\\
 0&0&2\end{bmatrix}      and change of basis matrix  P       [     1    1     -  1       1    1    0      1    0    3     ]   .      1  1    1     1  1  0    1  0  3     \begin{bmatrix}1&1&-1\\
 1&1&0\\
 1&0&3\end{bmatrix}.      The converse fails when A has an eigenspace of dimension higher than 1. In this example, the eigenspace of A associated with the eigenvalue 2 has dimension 2.    A linear map T : V → V with n = dim( V ) is diagonalizable if it has n distinct eigenvalues, i.e. if its characteristic polynomial has n distinct roots in F .   Let A be a matrix over F . If A is diagonalizable, then so is any power of it. Conversely, if A is invertible, F is algebraically closed, and A n is diagonalizable for some n that is not an integer multiple of the characteristic of F , then A is diagonalizable. Proof: If A n is diagonalizable, then A is annihilated by some polynomial     (    x  n   -   λ  1    )   ⋯   (    x  n   -   λ  k    )          superscript  x  n    subscript  λ  1    normal-⋯     superscript  x  n    subscript  λ  k      (x^{n}-\lambda_{1})\cdots(x^{n}-\lambda_{k})   , which has no multiple root (since     λ  j   ≠  0       subscript  λ  j   0    \lambda_{j}\neq 0   ) and is divided by the minimal polynomial of A .  As a rule of thumb, over C almost every matrix is diagonalizable. More precisely: the set of complex n × n matrices that are not diagonalizable over C , considered as a subset of C n × n , has Lebesgue measure zero. One can also say that the diagonalizable matrices form a dense subset with respect to the Zariski topology : the complement lies inside the set where the discriminant of the characteristic polynomial vanishes, which is a hypersurface . From that follows also density in the usual ( strong ) topology given by a norm . The same is not true over R .  The Jordan–Chevalley decomposition expresses an operator as the sum of its semisimple (i.e., diagonalizable) part and its nilpotent part. Hence, a matrix is diagonalizable if and only if its nilpotent part is zero. Put in another way, a matrix is diagonalizable if each block in its Jordan form has no nilpotent part; i.e., each "block" is a one-by-one matrix.  Diagonalization  If a matrix A can be diagonalized, that is,         P   -  1    A  P   =   (      λ  1           λ  2             ⋱                λ  n      )    ,         superscript  P    1    A  P      subscript  λ  1     absent   subscript  λ  2     absent  absent  normal-⋱    absent  absent  absent   subscript  λ  n       P^{-1}AP=\begin{pmatrix}\lambda_{1}\\
 &\lambda_{2}\\
 &&\ddots\\
 &&&\lambda_{n}\end{pmatrix},     then:        A  P   =   P   (      λ  1           λ  2             ⋱                λ  n      )     .        A  P     P     subscript  λ  1     absent   subscript  λ  2     absent  absent  normal-⋱    absent  absent  absent   subscript  λ  n        AP=P\begin{pmatrix}\lambda_{1}\\
 &\lambda_{2}\\
 &&\ddots\\
 &&&\lambda_{n}\end{pmatrix}.     Writing P as a block matrix of its column vectors     α  →   i     subscript   normal-→  α   i    \vec{\alpha}_{i}          P  =   (       α  →   1       α  →   2     ⋯      α  →   n      )    ,      P     subscript   normal-→  α   1    subscript   normal-→  α   2   normal-⋯   subscript   normal-→  α   n       P=\begin{pmatrix}\vec{\alpha}_{1}&\vec{\alpha}_{2}&\cdots&\vec{\alpha}_{n}\end%
 {pmatrix},     the above equation can be rewritten as      A    α  →   i   =   λ  i     α  →   i    (  i  =  1  ,  2  ,  ⋯  ,  n  )   .     fragments  A   subscript   normal-→  α   i     subscript  λ  i    subscript   normal-→  α   i   italic-   fragments  normal-(  i   1  normal-,  2  normal-,  normal-⋯  normal-,  n  normal-)   normal-.    A\vec{\alpha}_{i}=\lambda_{i}\vec{\alpha}_{i}\qquad(i=1,2,\cdots,n).     So the column vectors of P are right eigenvectors of A , and the corresponding diagonal entry is the corresponding eigenvalue . The invertibility of P also suggests that the eigenvectors are linearly independent and form a basis of F n . This is the necessary and sufficient condition for diagonalizability and the canonical approach of diagonalization. The row vectors of P −1 are the left eigenvectors of A .  When the matrix A is a Hermitian matrix (resp. symmetric matrix ), eigenvectors of A can be chosen to form an orthonormal basis of C n (resp. R n ). Under such circumstance P will be a unitary matrix (resp. orthogonal matrix ) and P −1 equals the conjugate transpose (resp. transpose ) of P .  Simultaneous diagonalization  A set of matrices are said to be simultaneously diagonalizable if there exists a single invertible matrix P such that P −1 AP is a diagonal matrix for every A in the set. The following theorem characterises simultaneously diagonalisable matrices: A set of diagonalizable matrices commutes if and only if the set is simultaneously diagonalisable. 2  The set of all n × n diagonalisable matrices (over C ) with n > 1 is not simultaneously diagonalisable. For instance, the matrices       [     1    0      0    0     ]   and   [     1    1      0    0     ]        1  0    0  0    and    1  1    0  0      \begin{bmatrix}1&0\\
 0&0\end{bmatrix}\quad\text{and}\quad\begin{bmatrix}1&1\\
 0&0\end{bmatrix}     are diagonalizable but not simultaneously diagonalizable because they do not commute.  A set consists of commuting normal matrices if and only if it is simultaneously diagonalisable by a unitary matrix ; that is, there exists a unitary matrix U such that U*AU is diagonal for every A in the set.  In the language of Lie theory , a set of simultaneously diagonalisable matrices generate a toral Lie algebra .  Examples  Diagonalizable matrices   Involutions are diagonalisable over the reals (and indeed any field of characteristic not 2), with ±1 on the diagonal  Finite order endomorphisms are diagonalisable over C (or any algebraically closed field where the characteristic of the field does not divide the order of the endomorphism) with roots of unity on the diagonal. This follows since the minimal polynomial is separable , because the roots of unity are distinct.  Projections are diagonalizable, with 0's and 1's on the diagonal.  Real symmetric matrices are diagonalizable by orthogonal matrices ; i.e., given a real symmetric matrix A , Q T AQ is diagonal for some orthogonal matrix Q . More generally, matrices are diagonalizable by unitary matrices if and only if they are normal . In the case of the real symmetric matrix, we see that    A  =   A  T       A   superscript  A  T     A=A^{T}   , so clearly     A   A  T    =    A  T   A         A   superscript  A  T       superscript  A  T   A     AA^{T}=A^{T}A   holds. Examples of normal matrices are real symmetric (or skew-symmetric ) matrices (e.g. covariance matrices) and Hermitian matrices (or skew-Hermitian matrices). See spectral theorems for generalizations to infinite-dimensional vector spaces.   Matrices that are not diagonalizable  In general, a rotation matrix is not diagonalizable over the reals, but all rotation matrices are diagonalizable over the complex field. Even if a matrix is not diagonalizable, it is always possible to "do the best one can", and find a matrix with the same properties consisting of eigenvalues on the leading diagonal, and either ones or zeroes on the superdiagonal - known as Jordan normal form .  Some matrices are not diagonalizable over any field, most notably nonzero nilpotent matrices . This happens more generally if the algebraic and geometric multiplicities of an eigenvalue do not coincide. For instance, consider       C  =   [     0    1      0    0     ]    .      C    0  1    0  0      C=\begin{bmatrix}0&1\\
 0&0\end{bmatrix}.     This matrix is not diagonalizable: there is no matrix U such that U −1 CU is a diagonal matrix. Indeed, C has one eigenvalue (namely zero) and this eigenvalue has algebraic multiplicity 2 and geometric multiplicity 1.  Some real matrices are not diagonalizable over the reals. Consider for instance the matrix       B  =   [     0    1       -  1     0     ]    .      B    0  1      1   0      B=\begin{bmatrix}0&1\\
 -1&0\end{bmatrix}.     The matrix B does not have any real eigenvalues, so there is no real matrix Q such that Q −1 BQ is a diagonal matrix. However, we can diagonalize B if we allow complex numbers. Indeed, if we take       Q  =   [     1    i      i    1     ]    ,      Q    1  i    i  1      Q=\begin{bmatrix}1&\textrm{i}\\
 \textrm{i}&1\end{bmatrix},     then Q −1 BQ is diagonal.  Note that the above examples show that the sum of diagonalizable matrices need not be diagonalizable.  How to diagonalize a matrix  Consider a matrix       A  =   [     1    2    0      0    3    0      2     -  4     2     ]    .      A    1  2  0    0  3  0    2    4   2      A=\begin{bmatrix}1&2&0\\
 0&3&0\\
 2&-4&2\end{bmatrix}.     This matrix has eigenvalues        λ  1   =  3   ,     λ  2   =  2   ,    λ  3   =  1.       formulae-sequence     subscript  λ  1   3    formulae-sequence     subscript  λ  2   2      subscript  λ  3   1.      \lambda_{1}=3,\quad\lambda_{2}=2,\quad\lambda_{3}=1.     A is a 3×3 matrix with 3 different eigenvalues; therefore, it is diagonalizable. Note that if there are exactly n distinct eigenvalues in an n × n matrix then this matrix is diagonalizable.  These eigenvalues are the values that will appear in the diagonalized form of matrix A , so by finding the eigenvalues of A we have diagonalized it. We could stop here, but it is a good check to use the eigenvectors to diagonalize A .  The eigenvectors of A are         v  1   =   [      -  1        -  1       2     ]    ,     v  2   =   [     0      0      1     ]    ,    v  3   =   [      -  1       0      2     ]      .     formulae-sequence     subscript  v  1       1       1     2      formulae-sequence     subscript  v  2     0    0    1        subscript  v  3       1     0    2        v_{1}=\begin{bmatrix}-1\\
 -1\\
 2\end{bmatrix},\quad v_{2}=\begin{bmatrix}0\\
 0\\
 1\end{bmatrix},\quad v_{3}=\begin{bmatrix}-1\\
 0\\
 2\end{bmatrix}.     One can easily check that      A   v  k    =    λ  k    v  k     .        A   subscript  v  k       subscript  λ  k    subscript  v  k      Av_{k}=\lambda_{k}v_{k}.     Now, let P be the matrix with these eigenvectors as its columns:       P  =   [      -  1     0     -  1        -  1     0    0      2    1    2     ]    .      P      1   0    1       1   0  0    2  1  2      P=\begin{bmatrix}-1&0&-1\\
 -1&0&0\\
 2&1&2\end{bmatrix}.     Note there is no preferred order of the eigenvectors in P ; changing the order of the eigenvectors in P just changes the order of the eigenvalues in the diagonalized form of A . 3  Then P diagonalizes A , as a simple computation confirms, having calculated P −1 using any suitable method :         P   -  1    A  P   =    [     0     -  1     0      2    0    1       -  1     1    0     ]    [     1    2    0      0    3    0      2     -  4     2     ]    [      -  1     0     -  1        -  1     0    0      2    1    2     ]    =   [     3    0    0      0    2    0      0    0    1     ]    .           superscript  P    1    A  P       0    1   0    2  0  1      1   1  0      1  2  0    0  3  0    2    4   2        1   0    1       1   0  0    2  1  2            3  0  0    0  2  0    0  0  1       P^{-1}AP=\begin{bmatrix}0&-1&0\\
 2&0&1\\
 -1&1&0\end{bmatrix}\begin{bmatrix}1&2&0\\
 0&3&0\\
 2&-4&2\end{bmatrix}\begin{bmatrix}-1&0&-1\\
 -1&0&0\\
 2&1&2\end{bmatrix}=\begin{bmatrix}3&0&0\\
 0&2&0\\
 0&0&1\end{bmatrix}.     Note that the eigenvalues    λ  k     subscript  λ  k    \lambda_{k}   appear in the diagonal matrix.  An application  Diagonalization can be used to compute the powers of a matrix A efficiently, provided the matrix is diagonalizable. Suppose we have found that        P   -  1    A  P   =  D         superscript  P    1    A  P   D    P^{-1}AP=D     is a diagonal matrix. Then, as the matrix product is associative,      A  k     superscript  A  k    \displaystyle A^{k}     and the latter is easy to calculate since it only involves the powers of a diagonal matrix. This approach can be generalized to matrix exponential and other matrix functions since they can be defined as power series.  This is particularly useful in finding closed form expressions for terms of linear recursive sequences , such as the Fibonacci numbers .  Particular application  For example, consider the following matrix:       M  =   [     a     b  -  a       0     b  ;      ]    .      M    a    b  a     0  b      M=\begin{bmatrix}a&b-a\\
 0&b\end{bmatrix}.     Calculating the various powers of M reveals a surprising pattern:        M  2   =   [      a  2       b  2   -   a  2        0     b   ;  2       ]    ,     M  3   =   [      a  3       b  3   -   a  3        0     b   ;  3       ]    ,    M  4   =    [      a  4       b  4   -   a  4        0     b   ;  4       ]   ,  …        formulae-sequence     superscript  M  2      superscript  a  2      superscript  b  2    superscript  a  2      0   fragments  b   superscript  normal-;  2        formulae-sequence     superscript  M  3      superscript  a  3      superscript  b  3    superscript  a  3      0   fragments  b   superscript  normal-;  3          superscript  M  4       superscript  a  4      superscript  b  4    superscript  a  4      0   fragments  b   superscript  normal-;  4      normal-…       M^{2}=\begin{bmatrix}a^{2}&b^{2}-a^{2}\\
 0&b^{2}\end{bmatrix},\quad M^{3}=\begin{bmatrix}a^{3}&b^{3}-a^{3}\\
 0&b^{3}\end{bmatrix},\quad M^{4}=\begin{bmatrix}a^{4}&b^{4}-a^{4}\\
 0&b^{4}\end{bmatrix},\quad\ldots     The above phenomenon can be explained by diagonalizing M . To accomplish this, we need a basis of R 2 consisting of eigenvectors of M . One such eigenvector basis is given by        𝐮  =   [     1      0     ]   =   𝐞  1    ,   𝐯  =   [     1      1     ]   =    𝐞  1   +   𝐞  2      ,     formulae-sequence      𝐮    1    0          subscript  𝐞  1         𝐯    1    1            subscript  𝐞  1    subscript  𝐞  2        \mathbf{u}=\begin{bmatrix}1\\
 0\end{bmatrix}=\mathbf{e}_{1},\quad\mathbf{v}=\begin{bmatrix}1\\
 1\end{bmatrix}=\mathbf{e}_{1}+\mathbf{e}_{2},     where e i denotes the standard basis of R n . The reverse change of basis is given by         𝐞  1   =  𝐮   ,    𝐞  2   =   𝐯  -  𝐮     .     formulae-sequence     subscript  𝐞  1   𝐮      subscript  𝐞  2     𝐯  𝐮      \mathbf{e}_{1}=\mathbf{u},\qquad\mathbf{e}_{2}=\mathbf{v}-\mathbf{u}.     Straightforward calculations show that         M  𝐮   =   a  𝐮    ,    M  𝐯   =   b  𝐯     .     formulae-sequence      M  𝐮     a  𝐮        M  𝐯     b  𝐯      M\mathbf{u}=a\mathbf{u},\qquad M\mathbf{v}=b\mathbf{v}.     Thus, a and b are the eigenvalues corresponding to u and v , respectively. By linearity of matrix multiplication, we have that          M  n   𝐮   =     a  n    𝐮    ,     M  n   𝐯   =     b  n    𝐯     .     formulae-sequence       superscript  M  n   𝐮      superscript  a  n   𝐮         superscript  M  n   𝐯      superscript  b  n   𝐯      M^{n}\mathbf{u}=a^{n}\,\mathbf{u},\qquad M^{n}\mathbf{v}=b^{n}\,\mathbf{v}.     Switching back to the standard basis, we have         M  n    𝐞  1    =    M  n   𝐮   =    a  n    𝐞  1     ,           superscript  M  n    subscript  𝐞  1       superscript  M  n   𝐮           superscript  a  n    subscript  𝐞  1       M^{n}\mathbf{e}_{1}=M^{n}\mathbf{u}=a^{n}\mathbf{e}_{1},            M  n    𝐞  2    =    M  n    (   𝐯  -  𝐮   )    =     b  n   𝐯   -    a  n   𝐮    =     (    b  n   -   a  n    )    𝐞  1    +    b  n    𝐞  2      .           superscript  M  n    subscript  𝐞  2       superscript  M  n     𝐯  𝐮              superscript  b  n   𝐯      superscript  a  n   𝐮                superscript  b  n    superscript  a  n     subscript  𝐞  1       superscript  b  n    subscript  𝐞  2        M^{n}\mathbf{e}_{2}=M^{n}(\mathbf{v}-\mathbf{u})=b^{n}\mathbf{v}-a^{n}\mathbf{%
 u}=(b^{n}-a^{n})\mathbf{e}_{1}+b^{n}\mathbf{e}_{2}.     The preceding relations, expressed in matrix form, are        M  n   =   [      a  n       b  n   -   a  n        0     b   ;  n       ]    ,       superscript  M  n      superscript  a  n      superscript  b  n    superscript  a  n      0   fragments  b   superscript  normal-;  n        M^{n}=\begin{bmatrix}a^{n}&b^{n}-a^{n}\\
 0&b^{n}\end{bmatrix},     thereby explaining the above phenomenon.  Quantum mechanical application  In quantum mechanical and quantum chemical computations matrix diagonalization is one of the most frequently applied numerical processes. The basic reason is that the time-independent Schrödinger equation is an eigenvalue equation, albeit in most of the physical situations on an infinite dimensional space (a Hilbert space ). A very common approximation is to truncate Hilbert space to finite dimension, after which the Schrödinger equation can be formulated as an eigenvalue problem of a real symmetric, or complex Hermitian, matrix. Formally this approximation is founded on the variational principle , valid for Hamiltonians that are bounded from below. But also first-order perturbation theory for degenerate states leads to a matrix eigenvalue problem.  See also   Defective matrix  Scaling (geometry)  Triangular matrix  Semisimple operator  Diagonalizable group  Jordan normal form  Weight module – associative algebra generalization   Notes  References     External links    On line tool on matrix diagonalizations and Jordan form by www.mathstools.com   "  Category:Matrices     Horn & Johnson 1985 ↩  Horn & Johnson 1985, pp. 51–53 ↩  ↩     