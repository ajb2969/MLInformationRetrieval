   Stein's method      Stein's method   Stein's method is a general method in probability theory to obtain bounds on the distance between two probability distributions with respect to a probability metric . It was introduced by Charles Stein , who first published it in 1972, 1 to obtain a bound between the distribution of a sum of   m   m   m   -dependent sequence of random variables and a standard normal distribution in the Kolmogorov (uniform) metric and hence to prove not only a central limit theorem , but also bounds on the rates of convergence for the given metric.  History  At the end of the 1960s, unsatisfied with the by-then known proofs of a specific central limit theorem , Charles Stein developed a new way of proving the theorem for his statistics lecture. 2 His seminal paper 3 was presented in 1970 at the sixth Berkeley Symposium and published in the corresponding proceedings.  Later, his Ph.D. student Louis Chen Hsiao Yun modified the method so as to obtain approximation results for the Poisson distribution , 4 therefore the Stein method applied to the problem of Poisson approximation is often referred to as the Stein-Chen method .  Probably the most important contributions are the monograph by Stein (1986), where he presents his view of the method and the concept of auxiliary randomisation , in particular using exchangeable pairs , and the articles by Barbour (1988) and Götze (1991), who introduced the so-called generator interpretation , which made it possible to easily adapt the method to many other probability distributions. An important contribution was also an article by Bolthausen (1984) on the so-called combinatorial central limit theorem .  In the 1990s the method was adapted to a variety of distributions, such as Gaussian processes by Barbour (1990), the binomial distribution by Ehm (1991), Poisson processes by Barbour and Brown (1992), the Gamma distribution by Luk (1994), and many others.  The basic approach  Probability metrics  Stein's method is a way to bound the distance between two probability distributions using a specific probability metric .  Let the metric be given in the form        (  1.1  )    d   (  P  ,  Q  )     =    sup   h  ∈  ℋ     |    ∫   h  d  P    -   ∫   h  d  Q     |    =    sup   h  ∈  ℋ     |    E  h   (  W  )    -   E  h   (  Y  )     |           1.1    d   P  Q       subscript  supremum    h  ℋ            h  d  P        h  d  Q              subscript  supremum    h  ℋ          E  h  W     E  h  Y         (1.1)\quad d(P,Q)=\sup_{h\in\mathcal{H}}\left|\int hdP-\int hdQ\right|=\sup_{h%
 \in\mathcal{H}}\left|Eh(W)-Eh(Y)\right|   Here,   P   P   P   and   Q   Q   Q   are probability measures on a measurable space    𝒳   𝒳   \mathcal{X}   ,   W   W   W   and   Y   Y   Y   are random variables with distribution   P   P   P   and   Q   Q   Q   respectively,   E   E   E   is the usual expectation operator and   ℋ   ℋ   \mathcal{H}   is a set of functions from   𝒳   𝒳   \mathcal{X}   to the set of real numbers. Set   ℋ   ℋ   \mathcal{H}   has to be large enough, so that the above definition indeed yields a metric .  Important examples are the total variation metric , where we let   ℋ   ℋ   \mathcal{H}   consist of all the indicator functions of measurable sets, the Kolmogorov (uniform) metric for probability measures on the real numbers, where we consider all the half-line indicator functions, and the Lipschitz (first order Wasserstein; Kantorovich) metric , where the underlying space is itself a metric space and we take the set   ℋ   ℋ   \mathcal{H}   to be all Lipschitz-continuous functions with Lipschitz-constant 1. However, note that not every metric can be represented in the form (1.1).  In what follows   P   P   P   is a complicated distribution (e.g., the distribution of a sum of dependent random variables), which we want to approximate by a much simpler and tractable distribution   Q   Q   Q   (e.g., the standard normal distribution).  The Stein operator  We assume now that the distribution   Q   Q   Q   is a fixed distribution; in what follows we shall in particular consider the case where   Q   Q   Q   is the standard normal distribution, which serves as a classical example.  First of all, we need an operator   𝒜   𝒜   \mathcal{A}   , which acts on functions   f   f   f   from   𝒳   𝒳   \mathcal{X}   to the set of real numbers and 'characterizes' distribution   Q   Q   Q   in the sense that the following equivalence holds:          (  2.1  )    E   (   𝒜  f   )    (  Y  )     =   0  for all  f     ⇔   Y  has distribution  Q     .     formulae-sequence     2.1    E    𝒜  f   Y      0  for all  f     iff    Y  has distribution  Q      (2.1)\quad E(\mathcal{A}f)(Y)=0\text{ for all }f\quad\iff\quad Y\text{ has %
 distribution }Q.   We call such an operator the Stein operator .  For the standard normal distribution, Stein's lemma yields such an operator:         (  2.2  )    E   (     f  ′    (  Y  )    -   Y  f   (  Y  )     )     =   0  for all  f   ∈   C  b  1     ⇔   Y  has standard normal distribution.       formulae-sequence       2.2    E       superscript  f  normal-′   Y     Y  f  Y        0  for all  f         superscript   subscript  C  b   1      iff    Y  has standard normal distribution.      (2.2)\quad E\left(f^{\prime}(Y)-Yf(Y)\right)=0\text{ for all }f\in C_{b}^{1}%
 \quad\iff\quad Y\text{ has standard normal distribution.}   Thus, we can take         (  2.3  )     (   𝒜  f   )    (  x  )     =     f  ′    (  x  )    -   x  f   (  x  )      .       2.3      𝒜  f   x         superscript  f  normal-′   x     x  f  x      (2.3)\quad(\mathcal{A}f)(x)=f^{\prime}(x)-xf(x).   There are in general infinitely many such operators and it still remains an open question, which one to choose. However, it seems that for many distributions there is a particular good one, like (2.3) for the normal distribution.  There are different ways to find Stein operators (cf. Novak, 5 ch. 12).  The Stein equation     P   P   P   is close to   Q   Q   Q   with respect to   d   d   d   if the difference of expectations in (1.1) is close to 0. We hope now that the operator   𝒜   𝒜   \mathcal{A}   exhibits the same behavior: if    P  =  Q      P  Q    P=Q   then     E   (   𝒜  f   )    (  W  )    =  0        E    𝒜  f   W   0    E(\mathcal{A}f)(W)=0   , and hopefully if    P  ≈  Q      P  Q    P\approx Q   we have     E   (   𝒜  f   )    (  W  )    ≈  0        E    𝒜  f   W   0    E(\mathcal{A}f)(W)\approx 0   .  It is usually possible to define function    f  =   f  h       f   subscript  f  h     f=f_{h}   such that          (  3.1  )     (   𝒜  f   )    (  x  )     =    h   (  x  )    -   E   [   h   (  Y  )    ]       for all  x    .     formulae-sequence     3.1      𝒜  f   x        h  x     E   delimited-[]    h  Y         for all  x     (3.1)\quad(\mathcal{A}f)(x)=h(x)-E[h(Y)]\qquad\text{ for all }x.   We call (3.1) the Stein equation . Replacing   x   x   x   by   W   W   W   and taking expectation with respect to   W   W   W   , we get         (  3.2  )    E   (   𝒜  f   )    (  W  )     =    E   [   h   (  W  )    ]    -   E   [   h   (  Y  )    ]      .       3.2    E    𝒜  f   W        E   delimited-[]    h  W       E   delimited-[]    h  Y        (3.2)\quad E(\mathcal{A}f)(W)=E[h(W)]-E[h(Y)].   Now all the effort is worth only if the left-hand side of (3.2) is easier to bound than the right hand side. This is, surprisingly, often the case.  If   Q   Q   Q   is the standard normal distribution and we use (2.3), then the corresponding Stein equation is          (  3.3  )      f  ′    (  x  )    -   x  f   (  x  )      =    h   (  x  )    -   E   [   h   (  Y  )    ]       for all  x    .     formulae-sequence     3.3       superscript  f  normal-′   x     x  f  x         h  x     E   delimited-[]    h  Y         for all  x     (3.3)\quad f^{\prime}(x)-xf(x)=h(x)-E[h(Y)]\qquad\text{for all }x.     If probability distribution Q has an absolutely continuous (with respect to the Lebesgue measure) density q, then (Novak (2011), ch. 12)         (  3.4  )     (   𝒜  f   )    (  x  )     =     f  ′    (  x  )    +     f   (  x  )    q  ′    (  x  )    /  q    (  x  )      .       3.4      𝒜  f   x         superscript  f  normal-′   x         f  x   superscript  q  normal-′   x   q   x      (3.4)\quad(\mathcal{A}f)(x)=f^{\prime}(x)+f(x)q^{\prime}(x)/q(x).     Solving the Stein equation  Analytic methods . Equation (3.3) can be easily solved explicitly:         (  4.1  )    f   (  x  )     =    e    x  2   /  2      ∫   -  ∞   x     [    h   (  s  )    -   E  h   (  Y  )     ]    e   -    s  2   /  2     d  s      .       4.1    f  x       superscript  e     superscript  x  2   2      superscript   subscript        x      delimited-[]      h  s     E  h  Y      superscript  e       superscript  s  2   2     d  s       (4.1)\quad f(x)=e^{x^{2}/2}\int_{-\infty}^{x}[h(s)-Eh(Y)]e^{-s^{2}/2}ds.     Generator method . If   𝒜   𝒜   \mathcal{A}   is the generator of a Markov process     (   Z  t   )    t  ≥  0      subscript   subscript  Z  t     t  0     (Z_{t})_{t\geq 0}   (see Barbour (1988), Götze (1991)), then the solution to (3.2) is         (  4.2  )    f   (  x  )     =   -    ∫  0  ∞     [     E  x   h   (   Z  t   )    -   E  h   (  Y  )     ]   d  t      ,       4.2    f  x        superscript   subscript   0        delimited-[]       superscript  E  x   h   subscript  Z  t      E  h  Y     d  t       (4.2)\quad f(x)=-\int_{0}^{\infty}[E^{x}h(Z_{t})-Eh(Y)]dt,   where    E  x     superscript  E  x    E^{x}   denotes expectation with respect to the process   Z   Z   Z   being started in   x   x   x   . However, one still has to prove that the solution (4.2) exists for all desired functions    h  ∈  ℋ      h  ℋ    h\in\mathcal{H}   .  Properties of the solution to the Stein equation  Usually, one tries to give bounds on   f   f   f   and its derivatives (or differences) in terms of   h   h   h   and its derivatives (or differences), that is, inequalities of the form         (  5.1  )    ||    D  k   f   ||    ≤    C   k  ,  l     ||    D  l   h   ||     ,       5.1   norm     superscript  D  k   f        subscript  C   k  l     norm     superscript  D  l   h       (5.1)\quad||D^{k}f||\leq C_{k,l}||D^{l}h||,   for some specific      k  ,  l   =  0   ,   1  ,  2  ,  …      formulae-sequence     k  l   0    1  2  normal-…     k,l=0,1,2,\dots   (typically    k  ≥  l      k  l    k\geq l   or    k  ≥   l  -  1       k    l  1     k\geq l-1   , respectively, depending on the form of the Stein operator), where often    |  |  ⋅  |  |     fragments  normal-|  normal-|  normal-⋅  normal-|  normal-|    ||\cdot||   is the supremum norm. Here,    D  k     superscript  D  k    D^{k}   denotes the differential operator , but in discrete settings it usually refers to a difference operator . The constants    C   k  ,  l      subscript  C   k  l     C_{k,l}   may contain the parameters of the distribution   Q   Q   Q   . If there are any, they are often referred to as Stein factors .  In the case of (4.1) one can prove for the supremum norm that          (  5.2  )     ||  f  ||   ∞    ≤   min   {     π  /  2      ||  h  ||   ∞    ,   2    ||   h  ′   ||   ∞    }     ,      ||   f  ′   ||   ∞   ≤   min   {   2    ||  h  ||   ∞    ,   4    ||   h  ′   ||   ∞    }     ,     ||   f  ′′   ||   ∞   ≤   2    ||   h  ′   ||   ∞       ,     formulae-sequence     5.2   subscript   norm  f              π  2     subscript   norm  h        2   subscript   norm   superscript  h  normal-′          formulae-sequence     subscript   norm   superscript  f  normal-′          2   subscript   norm  h        4   subscript   norm   superscript  h  normal-′            subscript   norm   superscript  f  ′′        2   subscript   norm   superscript  h  normal-′           (5.2)\quad||f||_{\infty}\leq\min\{\sqrt{\pi/2}||h||_{\infty},2||h^{\prime}||_{%
 \infty}\},\quad||f^{\prime}||_{\infty}\leq\min\{2||h||_{\infty},4||h^{\prime}|%
 |_{\infty}\},\quad||f^{\prime\prime}||_{\infty}\leq 2||h^{\prime}||_{\infty},   where the last bound is of course only applicable if   h   h   h   is differentiable (or at least Lipschitz-continuous, which, for example, is not the case if we regard the total variation metric or the Kolmogorov metric!). As the standard normal distribution has no extra parameters, in this specific case the constants are free of additional parameters.  If we have bounds in the general form (5.1), we usually are able to treat many probability metrics together. One can often start with the next step below, if bounds of the form (5.1) are already available (which is the case for many distributions).  An abstract approximation theorem  We are now in a position to bound the left hand side of (3.1). As this step heavily depends on the form of the Stein operator, we directly regard the case of the standard normal distribution.  At this point we could directly plug in random variable   W   W   W   , which we want to approximate, and try to find upper bounds. However, it is often fruitful to formulate a more general theorem. Let us consider here the case of local dependence.  Assume that    W  =    ∑   i  =  1   n    X  i        W    superscript   subscript     i  1    n    subscript  X  i      W=\sum_{i=1}^{n}X_{i}   is a sum of random variables such that the     E   [  W  ]    =  0        E   delimited-[]  W    0    E[W]=0   and variance      var   [  W  ]    =  1       var  W   1    \operatorname{var}[W]=1   . Assume that, for every    i  =   1  ,  …  ,  n       i   1  normal-…  n     i=1,\dots,n   , there is a set     A  i   ⊂   {  1  ,  2  ,  …  ,  n  }        subscript  A  i    1  2  normal-…  n     A_{i}\subset\{1,2,\dots,n\}   , such that    X  i     subscript  X  i    X_{i}   is independent of all the random variables    X  j     subscript  X  j    X_{j}   with    j  ∉   A  i       j   subscript  A  i     j\not\in A_{i}   . We call this set the 'neighborhood' of    X  i     subscript  X  i    X_{i}   . Likewise let     B  i   ⊂   {  1  ,  2  ,  …  ,  n  }        subscript  B  i    1  2  normal-…  n     B_{i}\subset\{1,2,\dots,n\}   be a set such that all    X  j     subscript  X  j    X_{j}   with    j  ∈   A  i       j   subscript  A  i     j\in A_{i}   are independent of all    X  k     subscript  X  k    X_{k}   ,    k  ∉   B  i       k   subscript  B  i     k\not\in B_{i}   . We can think of    B  i     subscript  B  i    B_{i}   as the neighbors in the neighborhood of    X  i     subscript  X  i    X_{i}   , a second-order neighborhood, so to speak. For a set    A  ⊂   {  1  ,  2  ,  …  ,  n  }       A   1  2  normal-…  n     A\subset\{1,2,\dots,n\}   define now the sum     X  A   :=    ∑   j  ∈  A     X  j       assign   subscript  X  A     subscript     j  A     subscript  X  j      X_{A}:=\sum_{j\in A}X_{j}   .  Using Taylor expansion, it is possible to prove that        (  6.1  )    |   E   (     f  ′    (  W  )    -   W  f   (  W  )     )    |    ≤     ||   f  ′′   ||   ∞     ∑   i  =  1   n    (     1  2   E   |    X  i    X   A  i   2    |    +   E   |    X  i    X   A  i     X    B  i   ∖   A  i      |    +   E   |    X  i    X   A  i     |   E   |   X   B  i    |     )          6.1      E       superscript  f  normal-′   W     W  f  W          subscript   norm   superscript  f  ′′        superscript   subscript     i  1    n         1  2   E       subscript  X  i    superscript   subscript  X   subscript  A  i    2        E       subscript  X  i    subscript  X   subscript  A  i     subscript  X     subscript  B  i    subscript  A  i          E       subscript  X  i    subscript  X   subscript  A  i      E     subscript  X   subscript  B  i           (6.1)\quad\left|E(f^{\prime}(W)-Wf(W))\right|\leq||f^{\prime\prime}||_{\infty}%
 \sum_{i=1}^{n}\left(\frac{1}{2}E|X_{i}X_{A_{i}}^{2}|+E|X_{i}X_{A_{i}}X_{B_{i}%
 \setminus A_{i}}|+E|X_{i}X_{A_{i}}|E|X_{B_{i}}|\right)     Note that, if we follow this line of argument, we can bound (1.1) only for functions where     ||   h  ′   ||   ∞     subscript   norm   superscript  h  normal-′       ||h^{\prime}||_{\infty}   is bounded because of the third inequality of (5.2) (and in fact, if   h   h   h   has discontinuities, so will    f  ′′     superscript  f  ′′    f^{\prime\prime}   ). To obtain a bound similar to (6.1) which contains only the expressions     ||  f  ||   ∞     subscript   norm  f      ||f||_{\infty}   and     ||   f  ′   ||   ∞     subscript   norm   superscript  f  normal-′       ||f^{\prime}||_{\infty}   , the argument is much more involved and the result is not as simple as (6.1); however, it can be done.  Theorem A . If   W   W   W   is as described above, we have for the Lipschitz metric    d  W     subscript  d  W    d_{W}   that         (  6.2  )     d  W    (   ℒ   (  W  )    ,   N   (  0  ,  1  )    )     ≤   2    ∑   i  =  1   n    (     1  2   E   |    X  i    X   A  i   2    |    +   E   |    X  i    X   A  i     X    B  i   ∖   A  i      |    +   E   |    X  i    X   A  i     |   E   |   X   B  i    |     )      .       6.2     subscript  d  W      ℒ  W     N   0  1         2    superscript   subscript     i  1    n         1  2   E       subscript  X  i    superscript   subscript  X   subscript  A  i    2        E       subscript  X  i    subscript  X   subscript  A  i     subscript  X     subscript  B  i    subscript  A  i          E       subscript  X  i    subscript  X   subscript  A  i      E     subscript  X   subscript  B  i           (6.2)\quad d_{W}(\mathcal{L}(W),N(0,1))\leq 2\sum_{i=1}^{n}\left(\frac{1}{2}E|%
 X_{i}X_{A_{i}}^{2}|+E|X_{i}X_{A_{i}}X_{B_{i}\setminus A_{i}}|+E|X_{i}X_{A_{i}}%
 |E|X_{B_{i}}|\right).    Proof . Recall that the Lipschitz metric is of the form (1.1) where the functions   h   h   h   are Lipschitz-continuous with Lipschitz-constant 1, thus     ||   h  ′   ||   ≤  1       norm   superscript  h  normal-′    1    ||h^{\prime}||\leq 1   . Combining this with (6.1) and the last bound in (5.2) proves the theorem.  Thus, roughly speaking, we have proved that, to calculate the Lipschitz-distance between a   W   W   W   with local dependence structure and a standard normal distribution, we only need to know the third moments of    X  i     subscript  X  i    X_{i}   and the size of the neighborhoods    A  i     subscript  A  i    A_{i}   and    B  i     subscript  B  i    B_{i}   .  Application of the theorem  We can treat the case of sums of independent and identically distributed random variables with Theorem A.  Assume that     E   X  i    =  0        E   subscript  X  i    0    EX_{i}=0   ,     v  a  r   X  i    =  1        v  a  r   subscript  X  i    1    varX_{i}=1   and    W  =    n   -   1  /  2      ∑   X  i         W     superscript  n      1  2        subscript  X  i       W=n^{-1/2}\sum X_{i}   . We can take     A  i   =   B  i   =   {  i  }          subscript  A  i    subscript  B  i         i      A_{i}=B_{i}=\{i\}   . From Theorem A we obtain that         (  7.1  )     d  W    (   ℒ   (  W  )    ,   N   (  0  ,  1  )    )     ≤    5  E    |   X  1   |   3     n   1  /  2      .       7.1     subscript  d  W      ℒ  W     N   0  1           5  E   superscript     subscript  X  1    3     superscript  n    1  2       (7.1)\quad d_{W}(\mathcal{L}(W),N(0,1))\leq\frac{5E|X_{1}|^{3}}{n^{1/2}}.     Connections to other methods   Lindeberg's device . Lindeberg (1922) introduced a device, where the difference        E  h   (    X  1   +  …  +   X  n    )    -   E  h   (    Y  1   +  …  +   Y  n    )          E  h     subscript  X  1   normal-…   subscript  X  n       E  h     subscript  Y  1   normal-…   subscript  Y  n       Eh(X_{1}+...+X_{n})-Eh(Y_{1}+...+Y_{n})   is represented as a sum of step by step differences.   Tikhomirov's method . Clearly the approach via (1.1) and (3.1) does not involve characteristic functions . However, Tikhomirov (1980) presented a proof of a central limit theorem based on characteristic functions and a differential operator similar to (2.3). The basic observation is that the characteristic function    ψ   (  t  )       ψ  t    \psi(t)   of the standard normal distribution satisfies the differential equation       ψ  ′    (  t  )    +   t  ψ   (  t  )     =  0           superscript  ψ  normal-′   t     t  ψ  t    0    \psi^{\prime}(t)+t\psi(t)=0   for all   t   t   t   . Thus, if the characteristic function     ψ  W    (  t  )        subscript  ψ  W   t    \psi_{W}(t)   of   W   W   W   is such that       ψ  W  ′    (  t  )    +   t   ψ  W    (  t  )     ≈  0           subscript   superscript  ψ  normal-′   W   t     t   subscript  ψ  W   t    0    \psi^{\prime}_{W}(t)+t\psi_{W}(t)\approx 0   we expect that      ψ  W    (  t  )    ≈   ψ   (  t  )           subscript  ψ  W   t     ψ  t     \psi_{W}(t)\approx\psi(t)   and hence that   W   W   W   is close to the normal distribution. Tikhomirov states in his paper that he was inspired by Stein's seminal paper.   Notes  References               Literature  The following text is advanced, and gives a comprehensive overview of the normal case     Another advanced book, but having some introductory character, is     A standard reference is the book by Stein,     which contains a lot of interesting material, but may be a little hard to understand at first reading.  Despite its age, there are few standard introductory books about Stein's method available. The following recent textbook has a chapter (Chapter 2) devoted to introducing Stein's method:     Although the book     is by large parts about Poisson approximation, it contains nevertheless a lot of information about the generator approach, in particular in the context of Poisson process approximation.  The following textbook has a chapter (Chapter 10) devoted to introducing Stein's method of Poisson approximation:     "  Category:Statistical distance measures  Category:Theory of probability distributions     ↩  Charles Stein: The Invariant, the Direct and the "Pretentious" . Interview given in 2003 in Singapore ↩   ↩  Novak S.Y. (2011) Extreme value methods with applications to finance. London: CRC. ISBN 978-1-43983-574-6. ↩     