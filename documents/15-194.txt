   Bayesian interpretation of kernel regularization      Bayesian interpretation of kernel regularization   In machine learning , kernel methods arise from the assumption of an inner product space or similarity structure on inputs. For some such methods, such as SVMs , the original formulation and its regularization were not Bayesian in nature. It is helpful to understand them from a Bayesian perspective. Because the kernels are not necessarily positive semidefinite, the underlying structure may not be inner product spaces, but instead more general reproducing kernel Hilbert spaces . In Bayesian probability kernel methodsare a key component of Gaussian processes , where the kernel function is known as the covariance function. Kernel methods have traditionally been used in supervised learning problems where the input space is usually a space of vectors while the output space is a space of scalars . More recently these methods have been extended to problems that deal with multiple outputs such as in multi-task learning . 1  In this article we analyze the connections between the regularization and the Bayesian point of view for kernel methods in the case of scalar outputs. A mathematical equivalence between the regularization and the Bayesian point of view is easily proved in cases where the reproducing kernel Hilbert space is finite-dimensional . The infinite-dimensional case raises subtle mathematical issues; we will consider here the finite-dimensional case. We start with a brief review of the main ideas underlying kernel methods for scalar learning, and briefly introduce the concepts of regularization and Gaussian processes. We then show how both points of view arrive at essentially equivalent estimators, and show the connection that ties them together.  The Supervised Learning Problem  The classical supervised learning problem requires estimating the output for some new input point    𝐱  ′     superscript  𝐱  normal-′    \mathbf{x}^{\prime}   by learning a scalar-valued estimator     f  ^    (   𝐱  ′   )        normal-^  f    superscript  𝐱  normal-′     \hat{f}(\mathbf{x}^{\prime})   on the basis of a training set   S   S   S   consisting of   n   n   n   input-output pairs,     S  =   (  𝐗  ,  𝐘  )   =   (   𝐱  1   ,   y  1   )    ,   …  ,   (   𝐱  n   ,   y  n   )       formulae-sequence      S   𝐗  𝐘          subscript  𝐱  1    subscript  y  1       normal-…    subscript  𝐱  n    subscript  y  n       S=(\mathbf{X},\mathbf{Y})=(\mathbf{x}_{1},y_{1}),\ldots,(\mathbf{x}_{n},y_{n})   . 2 Given a symmetric and positive bivariate function    k   (  ⋅  ,  ⋅  )       k   normal-⋅  normal-⋅     k(\cdot,\cdot)   called a kernel , one of the most popular estimators in machine learning is given by  where    𝐊  ≡   k   (  𝐗  ,  𝐗  )        𝐊    k   𝐗  𝐗      \mathbf{K}\equiv k(\mathbf{X},\mathbf{X})   is the kernel matrix with entries     𝐊   i  j    =   k   (   𝐱  i   ,   𝐱  j   )         subscript  𝐊    i  j      k    subscript  𝐱  i    subscript  𝐱  j       \mathbf{K}_{ij}=k(\mathbf{x}_{i},\mathbf{x}_{j})   ,    𝐤  =    [   k   (   𝐱  1   ,   𝐱  ′   )    ,  …  ,   k   (   𝐱  n   ,   𝐱  ′   )    ]   ⊤       𝐤   superscript     k    subscript  𝐱  1    superscript  𝐱  normal-′     normal-…    k    subscript  𝐱  n    superscript  𝐱  normal-′      top     \mathbf{k}=[k(\mathbf{x}_{1},\mathbf{x}^{\prime}),\ldots,k(\mathbf{x}_{n},%
 \mathbf{x}^{\prime})]^{\top}   , and    𝐘  =    [   y  1   ,  …  ,   y  n   ]   ⊤       𝐘   superscript    subscript  y  1   normal-…   subscript  y  n    top     \mathbf{Y}=[y_{1},\ldots,y_{n}]^{\top}   . We will see how this estimator can be derived both from a regularization and a Bayesian perspective.  A Regularization Perspective  The main assumption in the regularization perspective is that the set of functions   ℱ   ℱ   \mathcal{F}   is assumed to belong to a reproducing kernel Hilbert space    ℋ  k     subscript  ℋ  k    \mathcal{H}_{k}   . 3 4 5 6  Reproducing Kernel Hilbert Space  A reproducing kernel Hilbert space (RKHS)    ℋ  k     subscript  ℋ  k    \mathcal{H}_{k}   is a Hilbert space of functions defined by a symmetric , positive-definite function     k  :    𝒳  ×  𝒳   →  ℝ      normal-:  k   normal-→    𝒳  𝒳   ℝ     k:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}   called the reproducing kernel such that the function    k   (  𝐱  ,  ⋅  )       k   𝐱  normal-⋅     k(\mathbf{x},\cdot)   belongs to    ℋ  k     subscript  ℋ  k    \mathcal{H}_{k}   for all    𝐱  ∈  𝒳      𝐱  𝒳    \mathbf{x}\in\mathcal{X}   . 7 8 9 There are three main properties make an RKHS appealing:  1. The reproducing property , which gives name to the space,         f   (  𝐱  )    =    ⟨  f  ,   k   (  𝐱  ,  ⋅  )    ⟩   k    ,    ∀  f   ∈   ℋ  k     ,     formulae-sequence      f  𝐱    subscript   f    k   𝐱  normal-⋅     k       for-all  f    subscript  ℋ  k      f(\mathbf{x})=\langle f,k(\mathbf{x},\cdot)\rangle_{k},\quad\forall\ f\in%
 \mathcal{H}_{k},     where     ⟨  ⋅  ,  ⋅  ⟩   k     subscript   normal-⋅  normal-⋅   k    \langle\cdot,\cdot\rangle_{k}   is the inner product in    ℋ  k     subscript  ℋ  k    \mathcal{H}_{k}   .  2. Functions in an RKHS are in the closure of the linear combination of the kernel at given points,       f   (  𝐱  )    =    ∑  i    k   (   𝐱  i   ,  𝐱  )    c  i           f  𝐱     subscript   i     k    subscript  𝐱  i   𝐱    subscript  c  i       f(\mathbf{x})=\sum_{i}k(\mathbf{x}_{i},\mathbf{x})c_{i}   .  This allows the construction in a unified framework of both linear and generalized linear models.  3. The squared norm in an RKHS can be written as        ∥  f  ∥   k  2   =    ∑   i  ,  j     k   (   𝐱  i   ,   𝐱  j   )    c  i    c  j          superscript   subscript   norm  f   k   2     subscript    i  j      k    subscript  𝐱  i    subscript  𝐱  j     subscript  c  i    subscript  c  j       \|f\|_{k}^{2}=\sum_{i,j}k(\mathbf{x}_{i},\mathbf{x}_{j})c_{i}c_{j}     and could be viewed as measuring the complexity of the function.  The Regularized Functional  The estimator is derived as the minimizer of the regularized functional  where    f  ∈   ℋ  k       f   subscript  ℋ  k     f\in\mathcal{H}_{k}   and    ∥  ⋅   ∥  k      fragments  parallel-to  normal-⋅   subscript  parallel-to  k     \|\cdot\|_{k}   is the norm in    ℋ  k     subscript  ℋ  k    \mathcal{H}_{k}   . The first term in this functional, which measures the average of the squares of the errors between the    f   (   𝐱  i   )       f   subscript  𝐱  i     f(\mathbf{x}_{i})   and the    y  i     subscript  y  i    y_{i}   , is called the empirical risk and represents the cost we pay by predicting    f   (   𝐱  i   )       f   subscript  𝐱  i     f(\mathbf{x}_{i})   for the true value    y  i     subscript  y  i    y_{i}   . The second term in the functional is the squared norm in a RKHS multiplied by a weight   λ   λ   \lambda   and serves the purpose of stabilizing the problem 10 11 as well as of adding a trade-off between fitting and complexity of the estimator. 12 The weight   λ   λ   \lambda   , called the regularizer , determines the degree to which instability and complexity of the estimator should be penalized (higher penalty for increasing value of   λ   λ   \lambda   ).  Derivation of the Estimator  The explicit form of the estimator in equation () is derived in two steps. First, the representer theorem 13 14 15 states that the minimizer of the functional () can always be written as a linear combination of the kernels centered at the training-set points,  for some    𝐜  ∈   ℝ  n       𝐜   superscript  ℝ  n     \mathbf{c}\in\mathbb{R}^{n}   . The explicit form of the coefficients    𝐜  =    [   c  1   ,  …  ,   c  n   ]   ⊤       𝐜   superscript    subscript  c  1   normal-…   subscript  c  n    top     \mathbf{c}=[c_{1},\ldots,c_{n}]^{\top}   can be found by substituting for    f   (  ⋅  )       f  normal-⋅    f(\cdot)   in the functional (). For a function of the form in equation (), we have that          ∥  f  ∥   k  2        =    ⟨  f  ,  f  ⟩   k    ,           =    ⟨     ∑   i  =  1   N      c  i   k   (   𝐱  i   ,  ⋅  )     ,     ∑   j  =  1   N      c  j   k   (   𝐱  j   ,  ⋅  )     ⟩   k    ,           =     ∑   i  =  1   N       ∑   j  =  1   N      c  i    c  j     ⟨   k   (   𝐱  i   ,  ⋅  )    ,   k   (   𝐱  j   ,  ⋅  )    ⟩   k       ,           =     ∑   i  =  1   N       ∑   j  =  1   N      c  i    c  j   k   (   𝐱  i   ,   𝐱  j   )       ,           =    𝐜  ⊤   𝐊𝐜    .          superscript   subscript   norm  f   k   2     absent   subscript   f  f   k       missing-subexpression     absent   subscript     superscript   subscript     i  1    N      subscript  c  i   k    subscript  𝐱  i   normal-⋅       superscript   subscript     j  1    N      subscript  c  j   k    subscript  𝐱  j   normal-⋅      k       missing-subexpression     absent    superscript   subscript     i  1    N     superscript   subscript     j  1    N      subscript  c  i    subscript  c  j    subscript     k    subscript  𝐱  i   normal-⋅      k    subscript  𝐱  j   normal-⋅     k          missing-subexpression     absent    superscript   subscript     i  1    N     superscript   subscript     j  1    N      subscript  c  i    subscript  c  j   k    subscript  𝐱  i    subscript  𝐱  j           missing-subexpression     absent     superscript  𝐜  top   𝐊𝐜       \begin{aligned}\displaystyle\|f\|_{k}^{2}&\displaystyle=\langle f,f\rangle_{k}%
 ,\\
 &\displaystyle=\left\langle\sum_{i=1}^{N}c_{i}k(\mathbf{x}_{i},\cdot),\sum_{j=%
 1}^{N}c_{j}k(\mathbf{x}_{j},\cdot)\right\rangle_{k},\\
 &\displaystyle=\sum_{i=1}^{N}\sum_{j=1}^{N}c_{i}c_{j}\langle k(\mathbf{x}_{i},%
 \cdot),k(\mathbf{x}_{j},\cdot)\rangle_{k},\\
 &\displaystyle=\sum_{i=1}^{N}\sum_{j=1}^{N}c_{i}c_{j}k(\mathbf{x}_{i},\mathbf{%
 x}_{j}),\\
 &\displaystyle=\mathbf{c}^{\top}\mathbf{K}\mathbf{c}.\end{aligned}     We can rewrite the functional () as         1  n     ∥   𝐲  -  𝐊𝐜   ∥   2    +   λ   𝐜  ⊤   𝐊𝐜    .          1  n    superscript   norm    𝐲  𝐊𝐜    2      λ   superscript  𝐜  top   𝐊𝐜     \frac{1}{n}\|\mathbf{y}-\mathbf{K}\mathbf{c}\|^{2}+\lambda\mathbf{c}^{\top}%
 \mathbf{K}\mathbf{c}.     This functional is convex in   𝐜   𝐜   \mathbf{c}   and therefore we can find its minimum by setting the gradient with respect to   𝐜   𝐜   \mathbf{c}   to zero,          -     1  n    𝐊   (   𝐘  -  𝐊𝐜   )     +   λ  𝐊𝐜         =  0   ,         (   𝐊  +   λ  n  𝐈    )   𝐜        =  𝐘   ,       𝐜       =     (   𝐊  +   λ  n  𝐈    )    -  1    𝐘    .                 1  n   𝐊    𝐘  𝐊𝐜       λ  𝐊𝐜      absent  0         𝐊    λ  n  𝐈    𝐜     absent  𝐘     𝐜    absent     superscript    𝐊    λ  n  𝐈      1    𝐘       \begin{aligned}\displaystyle-\frac{1}{n}\mathbf{K}(\mathbf{Y}-\mathbf{K}%
 \mathbf{c})+\lambda\mathbf{K}\mathbf{c}&\displaystyle=0,\\
 \displaystyle(\mathbf{K}+\lambda n\mathbf{I})\mathbf{c}&\displaystyle=\mathbf{%
 Y},\\
 \displaystyle\mathbf{c}&\displaystyle=(\mathbf{K}+\lambda n\mathbf{I})^{-1}%
 \mathbf{Y}.\end{aligned}     Substituting this expression for the coefficients in equation (), we obtain the estimator stated previously in equation (),         f  ^    (   𝐱  ′   )    =    𝐤  ⊤     (   𝐊  +   λ  n  𝐈    )    -  1    𝐘    .         normal-^  f    superscript  𝐱  normal-′       superscript  𝐤  top    superscript    𝐊    λ  n  𝐈      1    𝐘     \hat{f}(\mathbf{x}^{\prime})=\mathbf{k}^{\top}(\mathbf{K}+\lambda n\mathbf{I})%
 ^{-1}\mathbf{Y}.     A Bayesian Perspective  The notion of a kernel plays a crucial role in Bayesian probability as the covariance function of a stochastic process called the Gaussian process .  A Review of Bayesian Probability  As part of the Bayesian framework, the Gaussian process specifies the prior distribution that describes the prior beliefs about the properties of the function being modeled. These beliefs are updated after taking into account observational data by means of a likelihood function that relates the prior beliefs to the observations. Taken together, the prior and likelihood lead to an updated distribution called the posterior distribution that is customarily used for predicting test cases.  The Gaussian Process  A Gaussian process (GP) is a stochastic process in which any finite number of random variables that are sampled follow a joint Normal distribution . 16 The mean vector and covariance matrix of the Gaussian distribution completely specify the GP. GPs are usually used as a priori distribution for functions, and as such the mean vector and covariance matrix can be viewed as functions, where the covariance function is also called the kernel of the GP. Let a function   f   f   f   follow a Gaussian process with mean function   m   m   m   and kernel function   k   k   k   ,       f  ∼   𝒢  𝒫   (  m  ,  k  )     .     similar-to  f    𝒢  𝒫   m  k      f\sim\mathcal{GP}(m,k).     In terms of the underlying Gaussian distribution, we have that for any finite set    𝐗  =    {   𝐱  i   }    i  =  1   n       𝐗   superscript   subscript    subscript  𝐱  i      i  1    n     \mathbf{X}=\{\mathbf{x}_{i}\}_{i=1}^{n}   if we let     f   (  𝐗  )    =    [   f   (   𝐱  1   )    ,  …  ,   f   (   𝐱  n   )    ]   ⊤         f  𝐗    superscript     f   subscript  𝐱  1    normal-…    f   subscript  𝐱  n     top     f(\mathbf{X})=[f(\mathbf{x}_{1}),\ldots,f(\mathbf{x}_{n})]^{\top}   then        f   (  𝐗  )    ∼   𝒩   (  𝐦  ,  𝐊  )     ,     similar-to    f  𝐗     𝒩   𝐦  𝐊      f(\mathbf{X})\sim\mathcal{N}(\mathbf{m},\mathbf{K}),     where    𝐦  =   m   (  𝐗  )    =    [   m   (   𝐱  1   )    ,  …  ,   m   (   𝐱  N   )    ]   ⊤         𝐦    m  𝐗         superscript     m   subscript  𝐱  1    normal-…    m   subscript  𝐱  N     top      \mathbf{m}=m(\mathbf{X})=[m(\mathbf{x}_{1}),\ldots,m(\mathbf{x}_{N})]^{\top}   is the mean vector and    𝐊  =   k   (  𝐗  ,  𝐗  )        𝐊    k   𝐗  𝐗      \mathbf{K}=k(\mathbf{X},\mathbf{X})   is the covariance matrix of the multivariate Gaussian distribution.  Derivation of the Estimator  In a regression context, the likelihood function is usually assumed to be a Gaussian distribution and the observations to be independent and identically distributed (iid),      p   (  y  |  f  ,  𝐱  ,   σ  2   )   =  𝒩   (  f   (  𝐱  )   ,   σ  2   )   .     fragments  p   fragments  normal-(  y  normal-|  f  normal-,  x  normal-,   superscript  σ  2   normal-)    N   fragments  normal-(  f   fragments  normal-(  x  normal-)   normal-,   superscript  σ  2   normal-)   normal-.    p(y|f,\mathbf{x},\sigma^{2})=\mathcal{N}(f(\mathbf{x}),\sigma^{2}).     This assumption corresponds to the observations being corrupted with zero-mean Gaussian noise with variance    σ  2     superscript  σ  2    \sigma^{2}   . The iid assumption makes it possible to factorize the likelihood function over the data points given the set of inputs   𝐗   𝐗   \mathbf{X}   and the variance of the noise    σ  2     superscript  σ  2    \sigma^{2}   , and thus the posterior distribution can be computed analytically. For a test input vector    𝐱  ′     superscript  𝐱  normal-′    \mathbf{x}^{\prime}   , given the training data    S  =   {  𝐗  ,  𝐘  }       S   𝐗  𝐘     S=\{\mathbf{X},\mathbf{Y}\}   , the posterior distribution is given by      p   (  f   (   𝐱  ′   )   |  S  ,   𝐱  ′   ,  ϕ  )   =  𝒩   (  m   (   𝐱  ′   )   ,   σ  2    (   𝐱  ′   )   )   ,     fragments  p   fragments  normal-(  f   fragments  normal-(   superscript  𝐱  normal-′   normal-)   normal-|  S  normal-,   superscript  𝐱  normal-′   normal-,  ϕ  normal-)    N   fragments  normal-(  m   fragments  normal-(   superscript  𝐱  normal-′   normal-)   normal-,   superscript  σ  2    fragments  normal-(   superscript  𝐱  normal-′   normal-)   normal-)   normal-,    p(f(\mathbf{x}^{\prime})|S,\mathbf{x}^{\prime},\boldsymbol{\phi})=\mathcal{N}(%
 m(\mathbf{x}^{\prime}),\sigma^{2}(\mathbf{x}^{\prime})),     where   ϕ   bold-italic-ϕ   \boldsymbol{\phi}   denotes the set of parameters which include the variance of the noise    σ  2     superscript  σ  2    \sigma^{2}   and any parameters from the covariance function   k   k   k   and where         m   (   𝐱  ′   )         =    𝐤  ⊤     (   𝐊  +    σ  2   𝐈    )    -  1    𝐘    ,         σ  2    (   𝐱  ′   )         =    k   (   𝐱  ′   ,   𝐱  ′   )    -    𝐤  ⊤     (   𝐊  +    σ  2   𝐈    )    -  1    𝐤     .           m   superscript  𝐱  normal-′      absent     superscript  𝐤  top    superscript    𝐊     superscript  σ  2   𝐈      1    𝐘         superscript  σ  2    superscript  𝐱  normal-′      absent      k    superscript  𝐱  normal-′    superscript  𝐱  normal-′        superscript  𝐤  top    superscript    𝐊     superscript  σ  2   𝐈      1    𝐤        \begin{aligned}\displaystyle m(\mathbf{x}^{\prime})&\displaystyle=\mathbf{k}^{%
 \top}(\mathbf{K}+\sigma^{2}\mathbf{I})^{-1}\mathbf{Y},\\
 \displaystyle\sigma^{2}(\mathbf{x}^{\prime})&\displaystyle=k(\mathbf{x}^{%
 \prime},\mathbf{x}^{\prime})-\mathbf{k}^{\top}(\mathbf{K}+\sigma^{2}\mathbf{I}%
 )^{-1}\mathbf{k}.\end{aligned}     The Connection Between Regularization and Bayes  A connection between regularization theory and Bayesian theory can only be achieved in the case of finite dimensional RKHS . Under this assumption, regularization theory and Bayesian theory are connected through Gaussian process prediction. 17 18  In the finite dimensional case, every RKHS can be described in terms of a feature map    Φ  :   𝒳  →   ℝ  p       normal-:  normal-Φ   normal-→  𝒳   superscript  ℝ  p      \Phi:\mathcal{X}\rightarrow\mathbb{R}^{p}   such that 19       k   (  𝐱  ,   𝐱  ′   )    =    ∑   i  =  1   p     Φ  i    (  𝐱  )    Φ  i    (   𝐱  ′   )      .        k   𝐱   superscript  𝐱  normal-′       superscript   subscript     i  1    p      superscript  normal-Φ  i   𝐱   superscript  normal-Φ  i    superscript  𝐱  normal-′       k(\mathbf{x},\mathbf{x}^{\prime})=\sum_{i=1}^{p}\Phi^{i}(\mathbf{x})\Phi^{i}(%
 \mathbf{x}^{\prime}).     Functions in the RKHS with kernel   𝐊   𝐊   \mathbf{K}   can be then be written as         f  𝐰    (  𝐱  )    =    ∑   i  =  1   p     𝐰  i    Φ  i    (  𝐱  )     =   ⟨  𝐰  ,   Φ   (  𝐱  )    ⟩    ,           subscript  f  𝐰   𝐱     superscript   subscript     i  1    p      superscript  𝐰  i    superscript  normal-Φ  i   𝐱          𝐰    normal-Φ  𝐱       f_{\mathbf{w}}(\mathbf{x})=\sum_{i=1}^{p}\mathbf{w}^{i}\Phi^{i}(\mathbf{x})=%
 \langle\mathbf{w},\Phi(\mathbf{x})\rangle,     and we also have that         ∥   f  𝐰   ∥   k   =   ∥  𝐰  ∥    .       subscript   norm   subscript  f  𝐰    k    norm  𝐰     \|f_{\mathbf{w}}\|_{k}=\|\mathbf{w}\|.     We can now build a Gaussian process by assuming    𝐰  =    [   w  1   ,  …  ,   w  p   ]   ⊤       𝐰   superscript    superscript  w  1   normal-…   superscript  w  p    top     \mathbf{w}=[w^{1},\ldots,w^{p}]^{\top}   to be distributed according to a multivariate Gaussian distribution with zero mean and identity covariance matrix,       𝐰  ∼   𝒩   (  0  ,  𝐈  )    ∝   exp   (   -    ∥  𝐰  ∥   2    )     .       similar-to  𝐰    𝒩   0  𝐈      proportional-to         superscript   norm  𝐰   2        \mathbf{w}\sim\mathcal{N}(0,\mathbf{I})\propto\exp(-\|\mathbf{w}\|^{2}).     If we assume a Gaussian likelihood we have      P   (  𝐘  |  𝐗  ,  f  )   =  𝒩   (  f   (  𝐗  )   ,   σ  2   𝐈  )   ∝  exp   (  -   1   σ  2    ∥   f  𝐰    (  𝐗  )   -  𝐘   ∥  2   )   ,     fragments  P   fragments  normal-(  Y  normal-|  X  normal-,  f  normal-)    N   fragments  normal-(  f   fragments  normal-(  X  normal-)   normal-,   superscript  σ  2   I  normal-)   proportional-to    fragments  normal-(     1   superscript  σ  2    parallel-to   subscript  f  𝐰    fragments  normal-(  X  normal-)    Y   superscript  parallel-to  2   normal-)   normal-,    P(\mathbf{Y}|\mathbf{X},f)=\mathcal{N}(f(\mathbf{X}),\sigma^{2}\mathbf{I})%
 \propto\exp\left(-\frac{1}{\sigma^{2}}\|f_{\mathbf{w}}(\mathbf{X})-\mathbf{Y}%
 \|^{2}\right),     where     f  𝐰    (  𝐗  )   =   (   ⟨  𝐰  ,  Φ   (   𝐱  1   )   ⟩   ,  …  ,   ⟨  𝐰  ,  Φ   (   𝐱  n   ⟩   )       fragments   subscript  f  𝐰    fragments  normal-(  X  normal-)     fragments  normal-(   fragments  normal-⟨  w  normal-,  Φ   fragments  normal-(   subscript  𝐱  1   normal-)   normal-⟩   normal-,  normal-…  normal-,   fragments  normal-⟨  w  normal-,  Φ   fragments  normal-(   subscript  𝐱  n   normal-⟩   normal-)      f_{\mathbf{w}}(\mathbf{X})=(\langle\mathbf{w},\Phi(\mathbf{x}_{1})\rangle,%
 \ldots,\langle\mathbf{w},\Phi(\mathbf{x}_{n}\rangle)   . The resulting posterior distribution is the given by      P   (  f  |  𝐗  ,  𝐘  )   ∝  exp   (  -   1   σ  2    ∥   f  𝐰    (  𝐗  )   -  𝐘   ∥  n  2   +  ∥  𝐰   ∥  2   )      fragments  P   fragments  normal-(  f  normal-|  X  normal-,  Y  normal-)   proportional-to    fragments  normal-(     1   superscript  σ  2    parallel-to   subscript  f  𝐰    fragments  normal-(  X  normal-)    Y   superscript   subscript  parallel-to  n   2    parallel-to  w   superscript  parallel-to  2   normal-)     P(f|\mathbf{X},\mathbf{Y})\propto\exp\left(-\frac{1}{\sigma^{2}}\|f_{\mathbf{w%
 }}(\mathbf{X})-\mathbf{Y}\|_{n}^{2}+\|\mathbf{w}\|^{2}\right)     We can see that a maximum posterior (MAP) estimate is equivalent to the minimization problem defining Tikhonov regularization , where in the Bayesian case the regularization parameter is related to the noise variance.  From a philosophical perspective, the loss function in a regularization setting plays a different role than the likelihood function in the Bayesian setting. Whereas the loss function measures the error that is incurred when predicting    f   (  𝐱  )       f  𝐱    f(\mathbf{x})   in place of   y   y   y   , the likelihood function measures how likely the observations are from the model that was assumed to be true in the generative process. From a mathematical perspective, however, the formulations of the regularization and Bayesian frameworks make the loss function and the likelihood function to have the same mathematical role of promoting the inference of functions   f   f   f   that approximate the labels   y   y   y   as much as possible.  References  "  Category:Bayesian statistics  Category:Machine learning  Category:Probability theory     ↩  ↩    ↩  ↩  ↩  ↩  ↩     ↩  ↩  ↩   ↩  ↩      