<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1346">Image segmentation</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Image segmentation</h1>
<hr/>

<p> In <a href="computer_vision" title="wikilink">computer vision</a>, <strong>image segmentation</strong> is the process of partitioning a <a href="digital_image" title="wikilink">digital image</a> into multiple segments (<a href="Set_(mathematics)" title="wikilink">sets</a> of <a href="pixel" title="wikilink">pixels</a>, also known as superpixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.</p>

<p>The result of image segmentation is a set of segments that collectively cover the entire image, or a set of <a href="Contour_line" title="wikilink">contours</a> extracted from the image (see <a href="edge_detection" title="wikilink">edge detection</a>). Each of the pixels in a region are similar with respect to some characteristic or computed property, such as <a class="uri" href="color" title="wikilink">color</a>, <a href="luminous_intensity" title="wikilink">intensity</a>, or <a href="Image_texture" title="wikilink">texture</a>. Adjacent regions are significantly different with respect to the same characteristic(s).<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> When applied to a stack of images, typical in <a href="medical_imaging" title="wikilink">medical imaging</a>, the resulting contours after image segmentation can be used to create 3D reconstructions with the help of interpolation algorithms like <a href="Marching_cubes" title="wikilink">Marching cubes</a>.</p>
<h2 id="applications">Applications</h2>

<p>Some of the practical applications of image segmentation are:</p>
<ul>
<li><a href="Content-based_image_retrieval" title="wikilink">Content-based image retrieval</a></li>
<li><a href="Machine_vision" title="wikilink">Machine vision</a></li>
<li><a href="Medical_imaging" title="wikilink">Medical imaging</a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a>
<ul>
<li>Locate tumors and other pathologies<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></li>
<li>Measure tissue volumes</li>
<li>Diagnosis, study of anatomical structure</li>
<li>Surgery planning</li>
<li>Virtual surgery simulation</li>
<li>Intra-surgery navigation</li>
</ul></li>
<li><a href="Object_detection" title="wikilink">Object detection</a><a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a>
<ul>
<li><a href="Pedestrian_detection" title="wikilink">Pedestrian detection</a></li>
<li><a href="Face_detection" title="wikilink">Face detection</a></li>
<li>Brake light detection</li>
<li>Locate objects in satellite images (roads, forests, crops, etc.)</li>
</ul></li>
<li>Recognition Tasks
<ul>
<li><a href="Face_recognition" title="wikilink">Face recognition</a></li>
<li><a href="Fingerprint_recognition" title="wikilink">Fingerprint recognition</a></li>
<li><a href="Iris_recognition" title="wikilink">Iris recognition</a></li>
</ul></li>
<li>Traffic control systems</li>
<li><a href="Video_surveillance" title="wikilink">Video surveillance</a></li>
</ul>

<p>Several general-purpose <a href="algorithm" title="wikilink">algorithms</a> and techniques have been developed for image segmentation. To be useful, these techniques must typically be combined with a domain's specific knowledge in order to effectively solve the domain's segmentation problems.</p>
<h2 id="thresholding">Thresholding</h2>

<p>The simplest method of image segmentation is called the <a href="Thresholding_(image_processing)" title="wikilink">thresholding</a> method. This method is based on a clip-level (or a threshold value) to turn a gray-scale image into a binary image. There is also a <a href="balanced_histogram_thresholding" title="wikilink">balanced histogram thresholding</a>.</p>

<p>The key of this method is to select the threshold value (or values when multiple-levels are selected). Several popular methods are used in industry including the maximum entropy method, <a href="Otsu's_method" title="wikilink">Otsu's method</a> (maximum variance), and <a class="uri" href="k-means" title="wikilink">k-means</a> clustering.</p>

<p>Recently, methods have been developed for thresholding computed tomography (CT) images. The key idea is that, unlike <a href="Otsu's_method" title="wikilink">Otsu's method</a>, the thresholds are derived from the radiographs instead of the (reconstructed) image<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> .<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>

<p>New methods suggested the usage of multi-dimensional fuzzy rule-based non-linear thresholds. In these works decision over each pixel's membership to a segment is based on multi-dimensional rules derived from fuzzy logic and evolutionary algorithms based on image lighting environment and application.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></p>
<h2 id="clustering-methods">Clustering methods</h2>

<p>The <a href="K-means_algorithm" title="wikilink">K-means algorithm</a> is an <a class="uri" href="iterative" title="wikilink">iterative</a> technique that is used to <a href="Cluster_analysis" title="wikilink">partition an image</a> into <em>K</em> clusters.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> The basic <a class="uri" href="algorithm" title="wikilink">algorithm</a> is</p>
<ol>
<li>Pick <em>K</em> cluster centers, either <a href="random" title="wikilink">randomly</a> or based on some <a class="uri" href="heuristic" title="wikilink">heuristic</a></li>
<li>Assign each pixel in the image to the cluster that minimizes the <a class="uri" href="distance" title="wikilink">distance</a> between the pixel and the cluster center</li>
<li>Re-compute the cluster centers by averaging all of the pixels in the cluster</li>
<li>Repeat steps 2 and 3 until convergence is attained (i.e. no pixels change clusters)</li>
</ol>

<p>In this case, <a class="uri" href="distance" title="wikilink">distance</a> is the squared or absolute difference between a pixel and a cluster center. The difference is typically based on pixel <a href="Hue" title="wikilink">color</a>, <a href="Brightness" title="wikilink">intensity</a>, <a href="Texture_(computer_graphics)" title="wikilink">texture</a>, and location, or a weighted combination of these factors. <em>K</em> can be selected manually, <a href="random" title="wikilink">randomly</a>, or by a <a class="uri" href="heuristic" title="wikilink">heuristic</a>. This algorithm is guaranteed to converge, but it may not return the <a href="Global_optimum" title="wikilink">optimal</a> solution. The quality of the solution depends on the initial set of clusters and the value of <em>K</em>.</p>
<h2 id="compression-based-methods">Compression-based methods</h2>

<p>Compression based methods postulate that the optimal segmentation is the one that minimizes, over all possible segmentations, the coding length of the data.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> The connection between these two concepts is that segmentation tries to find patterns in an image and any regularity in the image can be used to compress it. The method describes each segment by its texture and boundary shape. Each of these components is modeled by a probability distribution function and its coding length is computed as follows:</p>
<ol>
<li>The boundary encoding leverages the fact that regions in natural images tend to have a smooth contour. This prior is used by <a href="Huffman_coding" title="wikilink">Huffman coding</a> to encode the difference <a href="chain_code" title="wikilink">chain code</a> of the contours in an image. Thus, the smoother a boundary is, the shorter coding length it attains.</li>
<li>Texture is encoded by <a href="lossy_compression" title="wikilink">lossy compression</a> in a way similar to <a href="minimum_description_length" title="wikilink">minimum description length</a> (MDL) principle, but here the length of the data given the model is approximated by the number of samples times the <a href="Entropy_(information_theory)" title="wikilink">entropy</a> of the model. The texture in each region is modeled by a <a href="multivariate_normal_distribution" title="wikilink">multivariate normal distribution</a> whose entropy has closed form expression. An interesting property of this model is that the estimated entropy bounds the true entropy of the data from above. This is because among all distributions with a given mean and covariance, normal distribution has the largest entropy. Thus, the true coding length cannot be more than what the algorithm tries to minimize.</li>
</ol>

<p>For any given segmentation of an image, this scheme yields the number of bits required to encode that image based on the given segmentation. Thus, among all possible segmentations of an image, the goal is to find the segmentation which produces the shortest coding length. This can be achieved by a simple agglomerative clustering method. The distortion in the lossy compression determines the coarseness of the segmentation and its optimal value may differ for each image. This parameter can be estimated heuristically from the contrast of textures in an image. For example, when the textures in an image are similar, such as in camouflage images, stronger sensitivity and thus lower quantization is required.</p>
<h2 id="histogram-based-methods">Histogram-based methods</h2>

<p><a class="uri" href="Histogram" title="wikilink">Histogram</a>-based methods are very efficient compared to other image segmentation methods because they typically require only one pass through the <a href="pixel" title="wikilink">pixels</a>. In this technique, a histogram is computed from all of the pixels in the image, and the peaks and valleys in the histogram are used to locate the <a href="Cluster_analysis" title="wikilink">clusters</a> in the image.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> <a href="Hue" title="wikilink">Color</a> or <a href="Brightness" title="wikilink">intensity</a> can be used as the measure.</p>

<p>A refinement of this technique is to <a href="Recursion_(computer_science)" title="wikilink">recursively</a> apply the histogram-seeking method to clusters in the image in order to divide them into smaller clusters. This operation is repeated with smaller and smaller clusters until no more clusters are formed.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a><a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></p>

<p>One disadvantage of the histogram-seeking method is that it may be difficult to identify significant peaks and valleys in the image.</p>

<p>Histogram-based approaches can also be quickly adapted to apply to multiple frames, while maintaining their single pass efficiency. The histogram can be done in multiple fashions when multiple frames are considered. The same approach that is taken with one frame can be applied to multiple, and after the results are merged, peaks and valleys that were previously difficult to identify are more likely to be distinguishable. The histogram can also be applied on a per-pixel basis where the resulting information is used to determine the most frequent color for the pixel location. This approach segments based on active objects and a static environment, resulting in a different type of segmentation useful in <a href="Video_tracking" title="wikilink">Video tracking</a>.</p>
<h2 id="edge-detection">Edge detection</h2>

<p><a href="Edge_detection" title="wikilink">Edge detection</a> is a well-developed field on its own within image processing. Region boundaries and edges are closely related, since there is often a sharp adjustment in intensity at the region boundaries. Edge detection techniques have therefore been used as the base of another segmentation technique.</p>

<p>The edges identified by edge detection are often disconnected. To segment an object from an image however, one needs closed region boundaries. The desired edges are the boundaries between such objects or spatial-taxons.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> <a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a></p>

<p>Spatial-taxons<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> are information granules.,<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> consisting of a crisp pixel region, stationed at abstraction levels within a hierarchical nested scene architecture. They are similar to the Gestalt psychological designation of figure-ground, but are extended to include foreground, object groups, objects and salient object parts. Edge detection methods can be applied to the spatial-taxon region, in the same manner they would be applied to a silhouette. This method is particularly useful when the disconnected edge is part of an illusory contour<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a><a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a></p>

<p>Segmentation methods can also be applied to edges obtained from edge detectors. Lindeberg and Li<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a> developed an integrated method that segments edges into straight and curved edge segments for parts-based object recognition, based on a minimum description length (M<sub>DL</sub>) criterion that was optimized by a split-and-merge-like method with candidate breakpoints obtained from complementary junction cues to obtain more likely points at which to consider partitions into different segments.</p>
<h2 id="dual-clustering-method">Dual clustering method</h2>

<p>This method is a combination of three characteristics of the image: partition of the image based on histogram analysis is checked by high compactness of the clusters (objects), and high gradients of their borders. For that purpose two spaces has to be introduced: one space is the one-dimensional histogram of brightness H = H(B), the second space – the dual 3-dimensional space of the original image itself B = B(x, y). The first space allows to measure how compact is distributed the brightness of the image by calculating minimal clustering kmin. Threshold brightness T corresponding to kmin defines the binary (black-and-white) image – bitmap b = φ(x, y), where φ(x, y) = 0, if B(x, y) DC =G/(k-L) has to be calculated (where k is difference in brightness between the object and the background, L is length of all borders, and G is mean gradient on the borders). Maximum of MDC defines the segmentation.<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a></p>
<h2 id="region-growing-methods">Region-growing methods</h2>

<p><a class="uri" href="Region-growing" title="wikilink">Region-growing</a> methods rely mainly on the assumption that the neighboring pixels within one region have similar values. The common procedure is to compare one pixel with its neighbors. If a similarity criterion is satisfied, the pixel can be set to belong to the cluster as one or more of its neighbors. The selection of the similarity criterion is significant and the results are influenced by noise in all instances.</p>

<p>The method of Statistical Region Merging<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> (SRM) starts by building the graph of pixels using 4-connectedness with edges weighted by the absolute value of the intensity difference. Initially each pixel forms a single pixel region. SRM then sorts those edges in a priority queue and decide whether or not to merge the current regions belonging to the edge pixels using a statistical predicate.</p>

<p>One <a class="uri" href="region-growing" title="wikilink">region-growing</a> method is the seeded region growing method. This method takes a set of seeds as input along with the image. The seeds mark each of the objects to be segmented. The regions are iteratively grown by comparison of all unallocated neighboring pixels to the regions. The difference between a pixel's intensity value and the region's mean, 

<math display="inline" id="Image_segmentation:0">
 <semantics>
  <mi>δ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>δ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \delta
  </annotation>
 </semantics>
</math>

, is used as a measure of similarity. The pixel with the smallest difference measured in this way is assigned to the respective region. This process continues until all pixels are assigned to a region. Because seeded region growing requires seeds as additional input, the segmentation results are dependent on the choice of seeds, and noise in the image can cause the seeds to be poorly placed.</p>

<p>Another <a class="uri" href="region-growing" title="wikilink">region-growing</a> method is the unseeded region growing method. It is a modified algorithm that does not require explicit seeds. It starts with a single region 

<math display="inline" id="Image_segmentation:1">
 <semantics>
  <msub>
   <mi>A</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>A</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A_{1}
  </annotation>
 </semantics>
</math>

—the pixel chosen here does not markedly influence the final segmentation. At each iteration it considers the neighboring pixels in the same way as seeded region growing. It differs from seeded region growing in that if the minimum 

<math display="inline" id="Image_segmentation:2">
 <semantics>
  <mi>δ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>δ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \delta
  </annotation>
 </semantics>
</math>

 is less than a predefined threshold 

<math display="inline" id="Image_segmentation:3">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>

 then it is added to the respective region 

<math display="inline" id="Image_segmentation:4">
 <semantics>
  <msub>
   <mi>A</mi>
   <mi>j</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>A</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A_{j}
  </annotation>
 </semantics>
</math>

. If not, then the pixel is considered different from all current regions 

<math display="inline" id="Image_segmentation:5">
 <semantics>
  <msub>
   <mi>A</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>A</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A_{i}
  </annotation>
 </semantics>
</math>

 and a new region 

<math display="inline" id="Image_segmentation:6">
 <semantics>
  <msub>
   <mi>A</mi>
   <mrow>
    <mi>n</mi>
    <mo>+</mo>
    <mn>1</mn>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>A</ci>
    <apply>
     <plus></plus>
     <ci>n</ci>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A_{n+1}
  </annotation>
 </semantics>
</math>

 is created with this pixel.</p>

<p>One variant of this technique, proposed by <a class="uri" href="Haralick" title="wikilink">Haralick</a> and Shapiro (1985),<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a> is based on pixel <a href="Brightness" title="wikilink">intensities</a>. The <a href="Arithmetic_mean" title="wikilink">mean</a> and <a class="uri" href="scatter" title="wikilink">scatter</a> of the region and the intensity of the candidate pixel are used to compute a test statistic. If the test statistic is sufficiently small, the pixel is added to the region, and the region’s mean and scatter are recomputed. Otherwise, the pixel is rejected, and is used to form a new region.</p>

<p>A special region-growing method is called 

<math display="inline" id="Image_segmentation:7">
 <semantics>
  <mi>λ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>λ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda
  </annotation>
 </semantics>
</math>

-connected segmentation (see also <a class="uri" href="lambda-connectedness" title="wikilink">lambda-connectedness</a>). It is based on pixel <a href="Brightness" title="wikilink">intensities</a> and neighborhood-linking paths. A degree of connectivity (connectedness) is calculated based on a path that is formed by pixels. For a certain value of 

<math display="inline" id="Image_segmentation:8">
 <semantics>
  <mi>λ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>λ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda
  </annotation>
 </semantics>
</math>

, two pixels are called 

<math display="inline" id="Image_segmentation:9">
 <semantics>
  <mi>λ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>λ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda
  </annotation>
 </semantics>
</math>

-connected if there is a path linking those two pixels and the connectedness of this path is at least 

<math display="inline" id="Image_segmentation:10">
 <semantics>
  <mi>λ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>λ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda
  </annotation>
 </semantics>
</math>

. 

<math display="inline" id="Image_segmentation:11">
 <semantics>
  <mi>λ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>λ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda
  </annotation>
 </semantics>
</math>

-connectedness is an equivalence relation.<a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a></p>

<p>Split-and-merge segmentation is based on a <a class="uri" href="quadtree" title="wikilink">quadtree</a> partition of an image. It is sometimes called quadtree segmentation.</p>

<p>This method starts at the root of the tree that represents the whole image. If it is found non-uniform (not homogeneous), then it is split into four son squares (the splitting process), and so on. If, in contrast, four son squares are homogeneous, they are merged as several connected components (the merging process). The node in the tree is a segmented node. This process continues recursively until no further splits or merges are possible.<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a><a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a> When a special data structure is involved in the implementation of the algorithm of the method, its time complexity can reach 

<math display="inline" id="Image_segmentation:12">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>n</mi>
     <mrow>
      <mi>log</mi>
      <mi>n</mi>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <times></times>
     <ci>n</ci>
     <apply>
      <log></log>
      <ci>n</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(n\log n)
  </annotation>
 </semantics>
</math>

, an optimal algorithm of the method.<a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a></p>
<h2 id="partial-differential-equation-based-methods">Partial differential equation-based methods</h2>

<p>Using a <a href="partial_differential_equation" title="wikilink">partial differential equation</a> (PDE)-based method and solving the PDE equation by a numerical scheme, one can segment the image.<a class="footnoteRef" href="#fn32" id="fnref32"><sup>32</sup></a> Curve propagation is a popular technique in this category, with numerous applications to object extraction, object tracking, stereo reconstruction, etc. The central idea is to evolve an initial curve towards the lowest potential of a cost function, where its definition reflects the task to be addressed. As for most <a href="inverse_problems" title="wikilink">inverse problems</a>, the minimization of the cost functional is non-trivial and imposes certain smoothness constraints on the solution, which in the present case can be expressed as geometrical constraints on the evolving curve.</p>
<h3 id="parametric-methods">Parametric methods</h3>

<p><a class="uri" href="Lagrangian" title="wikilink">Lagrangian</a> techniques are based on parameterizing the contour according to some sampling strategy and then evolve each element according to image and internal terms. Such techniques are fast and efficient, however the original "purely parametric" formulation (due to Kass, <a href="Andrew_Witkin" title="wikilink">Witkin</a> and <a href="Demetri_Terzopoulos" title="wikilink">Terzopoulos</a> in 1987 and known as "<a href="Snake_(computer_vision)" title="wikilink">snakes</a>"), is generally criticized for its limitations regarding the choice of sampling strategy, the internal geometric properties of the curve, topology changes (curve splitting and merging), addressing problems in higher dimensions, etc.. Nowadays, efficient "discretized" formulations have been developed to address these limitations while maintaining high efficiency. In both cases, energy minimization is generally conducted using a steepest-gradient descent, whereby derivatives are computed using, e.g., finite differences.</p>
<h3 id="level-set-methods">Level set methods</h3>

<p>The <a href="level_set_method" title="wikilink">level set method</a> was initially proposed to track moving interfaces by Osher and Sethian in 1988 and has spread across various imaging domains in the late 90s. It can be used to efficiently address the problem of curve/surface/etc. propagation in an implicit manner. The central idea is to represent the evolving contour using a signed function whose zero corresponds to the actual contour. Then, according to the motion equation of the contour, one can easily derive a similar flow for the implicit surface that when applied to the zero level will reflect the propagation of the contour. The level set method affords numerous advantages: it is implicit, is parameter-free, provides a direct way to estimate the geometric properties of the evolving structure, allows for change of topology, and is intrinsic. It can be used to define an optimization framework, as proposed by Zhao, Merriman and Osher in 1996. One can conclude that it is a very convenient framework for addressing numerous applications of computer vision and medical image analysis.<a class="footnoteRef" href="#fn33" id="fnref33"><sup>33</sup></a> Research into various <a href="level_set_data_structures" title="wikilink">level set data structures</a> has led to very efficient implementations of this method.</p>
<h3 id="fast-marching-methods">Fast marching methods</h3>

<p>The <a href="fast_marching_method" title="wikilink">fast marching method</a> has been used in image segmentation,<a class="footnoteRef" href="#fn34" id="fnref34"><sup>34</sup></a> and this model has been improved (permitting a both positive and negative speed propagation speed) in an approach called the generalized fast marching method.<a class="footnoteRef" href="#fn35" id="fnref35"><sup>35</sup></a></p>
<h2 id="variational-methods">Variational methods</h2>

<p>The goal of variational methods is to find a segmentation which is optimal with respect to a specific energy functional. The functionals consist of a data fitting term and a regularizing terms. A classical representative is the <a href="Potts_model" title="wikilink">Potts model</a> defined for an image 

<math display="inline" id="Image_segmentation:13">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 by</p>

<p>

<math display="block" id="Image_segmentation:14">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <munder>
       <mo movablelimits="false">argmin</mo>
       <mi>u</mi>
      </munder>
      <mi>γ</mi>
     </mrow>
     <msub>
      <mrow>
       <mo>∥</mo>
       <mrow>
        <mo>∇</mo>
        <mi>u</mi>
       </mrow>
       <mo>∥</mo>
      </mrow>
      <mn>0</mn>
     </msub>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mo largeop="true" symmetric="true">∫</mo>
     <mrow>
      <msup>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mi>u</mi>
         <mo>-</mo>
         <mi>f</mi>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
       <mn>2</mn>
      </msup>
      <mi>d</mi>
      <mi>x</mi>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>argmin</ci>
       <ci>u</ci>
      </apply>
      <ci>γ</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="latexml">norm</csymbol>
       <apply>
        <ci>normal-∇</ci>
        <ci>u</ci>
       </apply>
      </apply>
      <cn type="integer">0</cn>
     </apply>
    </apply>
    <apply>
     <int></int>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <ci>u</ci>
        <ci>f</ci>
       </apply>
       <cn type="integer">2</cn>
      </apply>
      <ci>d</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname*{argmin}_{u}\gamma\|\nabla u\|_{0}+\int(u-f)^{2}dx.
  </annotation>
 </semantics>
</math>

 A minimizer 

<math display="inline" id="Image_segmentation:15">
 <semantics>
  <msup>
   <mi>u</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>u</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   u^{*}
  </annotation>
 </semantics>
</math>

 is a piecewise constant image which has an optimal tradeoff between the squared L2 distance to the given image 

<math display="inline" id="Image_segmentation:16">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 and the total length of its jump set. The jump set of 

<math display="inline" id="Image_segmentation:17">
 <semantics>
  <msup>
   <mi>u</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>u</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   u^{*}
  </annotation>
 </semantics>
</math>

 defines a segmentation. The relative weight of the energies is tuned by the parameter 

<math display="inline" id="Image_segmentation:18">
 <semantics>
  <mrow>
   <mi>γ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>γ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma>0
  </annotation>
 </semantics>
</math>

. The binary variant of the Potts model, i.e., if the range of 

<math display="inline" id="Image_segmentation:19">
 <semantics>
  <mi>u</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>u</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   u
  </annotation>
 </semantics>
</math>

 is restricted to two values, is often called Chan-Vese model.<a class="footnoteRef" href="#fn36" id="fnref36"><sup>36</sup></a> An important generalization is the Mumford-Shah model <a class="footnoteRef" href="#fn37" id="fnref37"><sup>37</sup></a> given by</p>

<p>

<math display="block" id="Image_segmentation:20">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <munder>
      <mo movablelimits="false">argmin</mo>
      <mrow>
       <mi>u</mi>
       <mo>,</mo>
       <mi>K</mi>
      </mrow>
     </munder>
     <mrow>
      <mi>γ</mi>
      <msup>
       <mrow>
        <mo stretchy="false">|</mo>
        <mrow>
         <mi>K</mi>
         <mrow>
          <mo stretchy="false">|</mo>
          <mrow>
           <mo>+</mo>
           <mrow>
            <mi>μ</mi>
            <msub>
             <mo largeop="true" symmetric="true">∫</mo>
             <msup>
              <mi>K</mi>
              <mi>C</mi>
             </msup>
            </msub>
           </mrow>
          </mrow>
          <mo stretchy="false">|</mo>
         </mrow>
         <mrow>
          <mo>∇</mo>
          <mi>u</mi>
         </mrow>
        </mrow>
        <mo stretchy="false">|</mo>
       </mrow>
       <mn>2</mn>
      </msup>
      <mi>d</mi>
      <mi>x</mi>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mo largeop="true" symmetric="true">∫</mo>
     <mrow>
      <msup>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mi>u</mi>
         <mo>-</mo>
         <mi>f</mi>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
       <mn>2</mn>
      </msup>
      <mi>d</mi>
      <mi>x</mi>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>argmin</ci>
      <list>
       <ci>u</ci>
       <ci>K</ci>
      </list>
     </apply>
     <apply>
      <times></times>
      <ci>γ</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <abs></abs>
        <apply>
         <times></times>
         <ci>K</ci>
         <apply>
          <abs></abs>
          <apply>
           <plus></plus>
           <apply>
            <times></times>
            <ci>μ</ci>
            <apply>
             <csymbol cd="ambiguous">subscript</csymbol>
             <int></int>
             <apply>
              <csymbol cd="ambiguous">superscript</csymbol>
              <ci>K</ci>
              <ci>C</ci>
             </apply>
            </apply>
           </apply>
          </apply>
         </apply>
         <apply>
          <ci>normal-∇</ci>
          <ci>u</ci>
         </apply>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
      <ci>d</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <apply>
     <int></int>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <ci>u</ci>
        <ci>f</ci>
       </apply>
       <cn type="integer">2</cn>
      </apply>
      <ci>d</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname*{argmin}_{u,K}\gamma|K|+\mu\int_{K^{C}}|\nabla u|^{2}dx+\int(u-f%
)^{2}dx.
  </annotation>
 </semantics>
</math>

 The functional value is the sum of the total length of the segmentation curve 

<math display="inline" id="Image_segmentation:21">
 <semantics>
  <mi>K</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>K</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   K
  </annotation>
 </semantics>
</math>

, the smoothness of the approximation 

<math display="inline" id="Image_segmentation:22">
 <semantics>
  <mi>u</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>u</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   u
  </annotation>
 </semantics>
</math>

, and its distance to the original image 

<math display="inline" id="Image_segmentation:23">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

. The weight of the smoothness penalty is adjusted by 

<math display="inline" id="Image_segmentation:24">
 <semantics>
  <mrow>
   <mi>μ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>μ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu>0
  </annotation>
 </semantics>
</math>

. The Potts model is often called piecewise constant Mumford-Shah model as it can be seen as the degenerate case 

<math display="inline" id="Image_segmentation:25">
 <semantics>
  <mrow>
   <mi>μ</mi>
   <mo>→</mo>
   <mi mathvariant="normal">∞</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>μ</ci>
    <infinity></infinity>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu\to\infty
  </annotation>
 </semantics>
</math>

. The optimization problems are known to be NP-hard in general but near-minimizing strategies work well in practice. Classical algorithms are <a href="Graduated_optimization" title="wikilink">graduated non-convexity</a> and <a href="Mumford–Shah_functional" title="wikilink">Ambrosio-Tortorelli approximation</a>.</p>
<h2 id="graph-partitioning-methods">Graph partitioning methods</h2>

<p><a href="Graph_(data_structure)" title="wikilink">Graph</a> partitioning methods are an effective tools for image segmentation since they model the impact of pixel neighborhoods on a given cluster of pixels or pixel, under the assumption of homogeneity in images. In these methods, the image is modeled as a weighted, <a href="undirected_graph" title="wikilink">undirected graph</a>. Usually a pixel or a group of pixels are associated with <a href="Vertex_(graph_theory)" title="wikilink">nodes</a> and <a href="Glossary_of_graph_theory#Basics" title="wikilink">edge</a> weights define the (dis)similarity between the neighborhood pixels. The graph (image) is then partitioned according to a criterion designed to model "good" clusters. Each partition of the nodes (pixels) output from these algorithms are considered an object segment in the image. Some popular algorithms of this category are normalized cuts,<a class="footnoteRef" href="#fn38" id="fnref38"><sup>38</sup></a> <a href="random_walker_(computer_vision)" title="wikilink">random walker</a>,<a class="footnoteRef" href="#fn39" id="fnref39"><sup>39</sup></a> minimum cut,<a class="footnoteRef" href="#fn40" id="fnref40"><sup>40</sup></a> isoperimetric partitioning,<a class="footnoteRef" href="#fn41" id="fnref41"><sup>41</sup></a> <a href="minimum_spanning_tree-based_segmentation" title="wikilink">minimum spanning tree-based segmentation</a>,<a class="footnoteRef" href="#fn42" id="fnref42"><sup>42</sup></a> and <a href="segmentation-based_object_categorization" title="wikilink">segmentation-based object categorization</a>.</p>
<h3 id="markov-random-fields">Markov Random Fields</h3>

<p>The application of <a href="Markov_random_fields" title="wikilink">Markov random fields</a> (MRF) for images was suggested in early 1984 by Geman and Geman.<a class="footnoteRef" href="#fn43" id="fnref43"><sup>43</sup></a> Their strong mathematical foundation and ability to provide a global optima even when defined on local features proved to be the foundation for novel research in the domain of image analysis, de-noising and segmentation. MRFs are completely characterized by their prior probability distributions, marginal probability distributions, <a href="Clique_(graph_theory)" title="wikilink">cliques</a>, smoothing constraint as well as criterion for updating values. The criterion for image segmentation using MRFs is restated as finding the labelling scheme which has maximum probability for a given set of features. The broad categories of image segmentation using MRFs are supervised and unsupervised segmentation.</p>
<h4 id="supervised-image-segmentation-using-mrf-and-map">Supervised Image Segmentation using MRF and MAP</h4>

<p>In terms of image segmentation, the function that MRFs seek to maximize is the probability of identifying a labelling scheme given a particular set of features are detected in the image. This is a restatement of the <a href="Maximum_a_posteriori_estimation" title="wikilink">Maximum a posteriori estimation</a> method.</p>
<figure><b>(Figure)</b>
<figcaption>MRF neighborhood for a chosen pixel</figcaption>
</figure>

<p>The generic algorithm for image segmentation using MAP is given below:</p>

<p><code>1. Define the neighborhood of each feature (random variable in MRF terms).</code><br/>
<code>   Generally this includes 1st order or 2nd order neighbors.</code><br/>
<code>2. Set initial probabilities </code>

<math display="inline" id="Image_segmentation:26">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>f</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">f</csymbol>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(f
  </annotation>
 </semantics>
</math>

<sub><code>i</code></sub>

<math display="inline" id="Image_segmentation:27">
 <semantics>
  <mo stretchy="false">)</mo>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-)</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   )
  </annotation>
 </semantics>
</math>

<code> for each feature as 0 or 1, where </code>

<math display="inline" id="Image_segmentation:28">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

<sub><code>i</code></sub>

<math display="inline" id="Image_segmentation:29">
 <semantics>
  <mrow>
   <mi>ϵ</mi>
   <mi mathvariant="normal">Σ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>ϵ</ci>
    <ci>normal-Σ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \epsilon\Sigma
  </annotation>
 </semantics>
</math>

<code> is the set containing features extracted</code><br/>
<code>   for pixel </code>

<math display="inline" id="Image_segmentation:30">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

<code> and define an initial set of clusters.</code><br/>
<code>3. Using the training data compute the mean (</code>

<math display="inline" id="Image_segmentation:31">
 <semantics>
  <mi>μ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>μ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu
  </annotation>
 </semantics>
</math>

<sub><code>l</code><sub><code>i</code></sub></sub><code>) and variance (</code>

<math display="inline" id="Image_segmentation:32">
 <semantics>
  <mi>σ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>σ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma
  </annotation>
 </semantics>
</math>

<sub><code>l</code><sub><code>i</code></sub></sub><code>) for each label. This is termed as class statistics.</code><br/>
<code>4. Compute the marginal distribution for the given labeling scheme </code>

<math display="inline" id="Image_segmentation:33">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>f</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">f</csymbol>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(f
  </annotation>
 </semantics>
</math>

<sub><code>i</code></sub>

<math display="inline" id="Image_segmentation:34">
 <semantics>
  <mrow>
   <mo stretchy="false">|</mo>
   <mi>l</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <ci>normal-|</ci>
    <csymbol cd="unknown">l</csymbol>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   |l
  </annotation>
 </semantics>
</math>

<sub><code>i</code></sub>

<math display="inline" id="Image_segmentation:35">
 <semantics>
  <mo stretchy="false">)</mo>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-)</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   )
  </annotation>
 </semantics>
</math>

<code> using </code><a href="Bayes'_theorem" title="wikilink"><code>Bayes'</code> <code>theorem</code></a><br/>
<code>   and the class statistics calculated earlier. A Gaussian model is used for the marginal distribution.</code><br/>
<code>   </code>

<math display="inline" id="Image_segmentation:36">
 <semantics>
  <mrow>
   <mfrac>
    <mn>1</mn>
    <mrow>
     <mi>σ</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>l</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <msqrt>
      <mrow>
       <mn>2</mn>
       <mi>π</mi>
      </mrow>
     </msqrt>
    </mrow>
   </mfrac>
   <msup>
    <mi>e</mi>
    <mrow>
     <mo>-</mo>
     <mfrac>
      <msup>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <msub>
          <mi>f</mi>
          <mi>i</mi>
         </msub>
         <mo>-</mo>
         <mrow>
          <mi>μ</mi>
          <mrow>
           <mo stretchy="false">(</mo>
           <msub>
            <mi>l</mi>
            <mi>i</mi>
           </msub>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
       <mn>2</mn>
      </msup>
      <mrow>
       <mn>2</mn>
       <mi>σ</mi>
       <msup>
        <mrow>
         <mo stretchy="false">(</mo>
         <msub>
          <mi>l</mi>
          <mi>i</mi>
         </msub>
         <mo stretchy="false">)</mo>
        </mrow>
        <mn>2</mn>
       </msup>
      </mrow>
     </mfrac>
    </mrow>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <apply>
      <times></times>
      <ci>σ</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>l</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <root></root>
       <apply>
        <times></times>
        <cn type="integer">2</cn>
        <ci>π</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>e</ci>
     <apply>
      <minus></minus>
      <apply>
       <divide></divide>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <minus></minus>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>f</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <times></times>
          <ci>μ</ci>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>l</ci>
           <ci>i</ci>
          </apply>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
       <apply>
        <times></times>
        <cn type="integer">2</cn>
        <ci>σ</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>l</ci>
          <ci>i</ci>
         </apply>
         <cn type="integer">2</cn>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{1}{\sigma(l_{i})\sqrt{2\pi}}e^{-\frac{(f_{i}-\mu(l_{i}))^{2}}{2\sigma(l_%
{i})^{2}}}
  </annotation>
 </semantics>
</math>

<br/>
<code>5. Calculate the probability of each class label given the neighborhood defined previously.</code><br/>
<code>   </code><a href="Clique_(graph_theory)" title="wikilink"><code>Clique</code></a><code> potentials are used to model the social impact in labeling.</code><br/>
<code>6. Iterate over new prior probabilities and redefine clusters such that these probabilities are maximized.</code><br/>
<code>   This is done using a variety of optimization algorithms described below.</code><br/>
<code>7. Stop when probability is maximized and labeling scheme does not change.</code><br/>
<code>   The calculations can be implemented in </code><a href="Log-likelihood" title="wikilink"><code>log</code> <code>likelihood</code></a><code> terms as well.</code></p>
<h4 id="optimization-algorithms">Optimization algorithms</h4>

<p>Each optimization algorithm is an adaptation of models from a variety of fields and they are set apart by their unique cost functions. The common trait of cost functions is to penalize change in pixel value as well as difference in pixel label when compared to labels of neighboring pixels.</p>
<h5 id="iterated-conditional-modesgradient-descent">Iterated Conditional Modes/Gradient Descent</h5>

<p>The <a href="Iterated_conditional_modes" title="wikilink">ICM</a> algorithm tries to reconstruct the ideal labeling scheme by changing the values of each pixel over each iteration and evaluating the energy of the new labeling scheme using the cost function given below,</p>

<p>

<math display="inline" id="Image_segmentation:37">
 <semantics>
  <mrow>
   <mi>α</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mn>1</mn>
    <mo>-</mo>
    <mi>δ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>l</mi>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">α</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <cn type="integer">1</cn>
     <minus></minus>
     <csymbol cd="unknown">δ</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-(</ci>
      <csymbol cd="unknown">l</csymbol>
     </cerror>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha(1-\delta(l
  </annotation>
 </semantics>
</math>

<sub>i</sub> 

<math display="inline" id="Image_segmentation:38">
 <semantics>
  <mrow>
   <mo>-</mo>
   <mi>l</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <ci>l</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   -l
  </annotation>
 </semantics>
</math>

<sub>initial i</sub>

<math display="inline" id="Image_segmentation:39">
 <semantics>
  <mrow>
   <mo stretchy="false">)</mo>
   <mo>+</mo>
   <mi>β</mi>
   <mi mathvariant="normal">Σ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <ci>normal-)</ci>
    <plus></plus>
    <csymbol cd="unknown">β</csymbol>
    <csymbol cd="unknown">Σ</csymbol>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   )+\beta\Sigma
  </annotation>
 </semantics>
</math>

<sub>q 

<math display="inline" id="Image_segmentation:40">
 <semantics>
  <mi>ϵ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ϵ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \epsilon
  </annotation>
 </semantics>
</math>

 N(i)</sub>

<math display="inline" id="Image_segmentation:41">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mn>1</mn>
   <mo>-</mo>
   <mi>δ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>l</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <ci>normal-(</ci>
    <cn type="integer">1</cn>
    <minus></minus>
    <csymbol cd="unknown">δ</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">l</csymbol>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (1-\delta(l
  </annotation>
 </semantics>
</math>

<sub>i</sub>

<math display="inline" id="Image_segmentation:42">
 <semantics>
  <mrow>
   <mo>,</mo>
   <mi>l</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <ci>normal-,</ci>
    <csymbol cd="unknown">l</csymbol>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ,l
  </annotation>
 </semantics>
</math>

<sub>q(i)</sub>

<math display="inline" id="Image_segmentation:43">
 <semantics>
  <mrow>
   <mo stretchy="false">)</mo>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <ci>normal-)</ci>
    <ci>normal-)</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   ))
  </annotation>
 </semantics>
</math>

.</p>

<p>where 

<math display="inline" id="Image_segmentation:44">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

 is the penalty for change in pixel label and 

<math display="inline" id="Image_segmentation:45">
 <semantics>
  <mi>β</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>β</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \beta
  </annotation>
 </semantics>
</math>

 is the penalty for difference in label between neighboring pixels and chosen pixel. Here 

<math display="inline" id="Image_segmentation:46">
 <semantics>
  <mrow>
   <mi>N</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>i</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>N</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N(i)
  </annotation>
 </semantics>
</math>

 is neighborhood of pixel i and 

<math display="inline" id="Image_segmentation:47">
 <semantics>
  <mi>δ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>δ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \delta
  </annotation>
 </semantics>
</math>

 is the Kronecker delta function. A major issue with ICM is that, similar to gradient descent, it has a tendency to rest over local maxima and thus not obtain a globally optimal labeling scheme.</p>
<h5 id="simulated-annealingsa"><a href="Simulated_annealing" title="wikilink">Simulated Annealing(SA)</a></h5>

<p>Derived as an analogue of annealing in metallurgy, SA uses change in pixel label over iterations and estimates the difference in energy of each newly formed graph to the initial data. If the newly formed graph is more profitable, in terms of low energy cost, given by:</p>

<p>

<math display="inline" id="Image_segmentation:48">
 <semantics>
  <mrow>
   <mrow>
    <mi mathvariant="normal">Δ</mi>
    <mi>U</mi>
   </mrow>
   <mo>=</mo>
   <mi>U</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>normal-Δ</ci>
     <ci>U</ci>
    </apply>
    <ci>U</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Delta U=U
  </annotation>
 </semantics>
</math>

<sup>new</sup>

<math display="inline" id="Image_segmentation:49">
 <semantics>
  <mrow>
   <mo>-</mo>
   <mi>U</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <ci>U</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   -U
  </annotation>
 </semantics>
</math>

<sup>old</sup></p>

<p>

<math display="block" id="Image_segmentation:50">
 <semantics>
  <mrow>
   <msub>
    <mi>l</mi>
    <mi>i</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <mo>{</mo>
    <mtable displaystyle="true">
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <msubsup>
         <mi>l</mi>
         <mi>i</mi>
         <mrow>
          <mi>n</mi>
          <mi>e</mi>
          <mi>w</mi>
         </mrow>
        </msubsup>
        <mo>,</mo>
       </mrow>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mrow>
          <mtext>if</mtext>
          <mrow>
           <mi mathvariant="normal">Δ</mi>
           <mi>U</mi>
          </mrow>
         </mrow>
         <mo>≤</mo>
         <mn>0</mn>
        </mrow>
        <mo>,</mo>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <msubsup>
         <mi>l</mi>
         <mi>i</mi>
         <mrow>
          <mi>n</mi>
          <mi>e</mi>
          <mi>w</mi>
         </mrow>
        </msubsup>
        <mo>,</mo>
       </mrow>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mrow>
          <mrow>
           <mtext>if</mtext>
           <mrow>
            <mi mathvariant="normal">Δ</mi>
            <mi>U</mi>
           </mrow>
          </mrow>
          <mo>></mo>
          <mn>0</mn>
         </mrow>
         <mrow>
          <mrow>
           <mtext>and</mtext>
           <mi>δ</mi>
          </mrow>
          <mo><</mo>
          <msup>
           <mi>e</mi>
           <mrow>
            <mo>-</mo>
            <mrow>
             <mrow>
              <mi mathvariant="normal">Δ</mi>
              <mi>U</mi>
             </mrow>
             <mo>/</mo>
             <mi>T</mi>
            </mrow>
           </mrow>
          </msup>
         </mrow>
        </mrow>
        <mo>,</mo>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <msubsup>
        <mi>l</mi>
        <mi>i</mi>
        <mrow>
         <mi>o</mi>
         <mi>l</mi>
         <mi>d</mi>
        </mrow>
       </msubsup>
      </mtd>
      <mtd></mtd>
     </mtr>
    </mtable>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>l</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="latexml">cases</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>l</ci>
       <apply>
        <times></times>
        <ci>n</ci>
        <ci>e</ci>
        <ci>w</ci>
       </apply>
      </apply>
      <ci>i</ci>
     </apply>
     <apply>
      <leq></leq>
      <list>
       <mtext>if</mtext>
       <apply>
        <times></times>
        <ci>normal-Δ</ci>
        <ci>U</ci>
       </apply>
      </list>
      <cn type="integer">0</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>l</ci>
       <apply>
        <times></times>
        <ci>n</ci>
        <ci>e</ci>
        <ci>w</ci>
       </apply>
      </apply>
      <ci>i</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">formulae-sequence</csymbol>
      <apply>
       <gt></gt>
       <list>
        <mtext>if</mtext>
        <apply>
         <times></times>
         <ci>normal-Δ</ci>
         <ci>U</ci>
        </apply>
       </list>
       <cn type="integer">0</cn>
      </apply>
      <apply>
       <lt></lt>
       <list>
        <mtext>and</mtext>
        <ci>δ</ci>
       </list>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>e</ci>
        <apply>
         <minus></minus>
         <apply>
          <divide></divide>
          <apply>
           <times></times>
           <ci>normal-Δ</ci>
           <ci>U</ci>
          </apply>
          <ci>T</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>l</ci>
       <apply>
        <times></times>
        <ci>o</ci>
        <ci>l</ci>
        <ci>d</ci>
       </apply>
      </apply>
      <ci>i</ci>
     </apply>
     <mtext>otherwise</mtext>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   l_{i}=\begin{cases}l^{new}_{i},&\text{if}\ \ \Delta U<=0,\\
l^{new}_{i},&\text{if}\ \ \Delta U>0\ \ \text{and}\ \ \delta<e^{-\Delta U/T},%
\\
l^{old}_{i}\end{cases}
  </annotation>
 </semantics>
</math>

</p>

<p>the algorithm selects the newly formed graph. Simulated annealing requires the input of temperature schedules which directly affects the speed of convergence of the system, as well as energy threshold for minimization to occur.</p>
<h5 id="alternative-algorithms">Alternative Algorithms</h5>

<p>A range of other methods exist for solving simple as well as higher order MRFs. They include Maximization of Posterior Marginal, Multi-scale MAP estimation,<a class="footnoteRef" href="#fn44" id="fnref44"><sup>44</sup></a> Multiple Resolution segmentation<a class="footnoteRef" href="#fn45" id="fnref45"><sup>45</sup></a> and more. Apart from likelihood estimates, graph-cut using maximum flow<a class="footnoteRef" href="#fn46" id="fnref46"><sup>46</sup></a> and other highly constrained graph based methods<a class="footnoteRef" href="#fn47" id="fnref47"><sup>47</sup></a><a class="footnoteRef" href="#fn48" id="fnref48"><sup>48</sup></a> exist for solving MRFs.</p>
<h4 id="unsupervised-image-segmentation-using-mrf-and-expectation-maximization">Unsupervised Image Segmentation using MRF and Expectation Maximization</h4>

<p>A subset of unsupervised machine learning, the <a href="Expectation–maximization_algorithm" title="wikilink">Expectation–maximization algorithm</a> is utilized to iteratively estimate the a posterior probabilities and distributions of labeling when no training data is available and no estimate of segmentation model can be formed. A general approach is to use histograms to represent the features of an image and proceed as outlined briefly in the 3-step algorithm mentioned below,</p>

<p>1. A random estimate of the model parameters (same as in supervised learning) is utilized.</p>

<p>2. E-Step: Estimate class statistics based on the random segmentation model defined. Using these, compute the conditional probability of belonging to a label given the feature set is calculated using naive <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a>.</p>

<p>

<math display="inline" id="Image_segmentation:51">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>λ</mi>
    <mo stretchy="false">|</mo>
    <msub>
     <mi>f</mi>
     <mi>i</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>f</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">|</mo>
      <mi>λ</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>λ</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <msub>
      <mi mathvariant="normal">Σ</mi>
      <mrow>
       <mi>λ</mi>
       <mi>ϵ</mi>
       <mi mathvariant="normal">Λ</mi>
      </mrow>
     </msub>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>f</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">|</mo>
      <mi>λ</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>λ</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">λ</csymbol>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>f</ci>
      <ci>i</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>f</ci>
        <ci>i</ci>
       </apply>
       <ci>normal-|</ci>
       <csymbol cd="unknown">λ</csymbol>
       <ci>normal-)</ci>
      </cerror>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">λ</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-Σ</ci>
       <apply>
        <times></times>
        <ci>λ</ci>
        <ci>ϵ</ci>
        <ci>normal-Λ</ci>
       </apply>
      </apply>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>f</ci>
        <ci>i</ci>
       </apply>
       <ci>normal-|</ci>
       <csymbol cd="unknown">λ</csymbol>
       <ci>normal-)</ci>
      </cerror>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">λ</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
    </apply>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(\lambda|f_{i})=\frac{P(f_{i}|\lambda)P(\lambda)}{\Sigma_{\lambda\epsilon%
\Lambda}P(f_{i}|\lambda)P(\lambda)}
  </annotation>
 </semantics>
</math>

 Here 

<math display="inline" id="Image_segmentation:52">
 <semantics>
  <mrow>
   <mi>λ</mi>
   <mi>ϵ</mi>
   <mi mathvariant="normal">Λ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>λ</ci>
    <ci>ϵ</ci>
    <ci>normal-Λ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda\epsilon\Lambda
  </annotation>
 </semantics>
</math>

 is the set of all possible labels.</p>

<p>3. M-Step: The established relevance of a given feature set to a labeling scheme is now used to compute the a priori estimate of a given label in the second part of the algorithm. Since the actual number of total labels is unknown (from a training data set), a hidden estimate of the number of labels given by the user is utilized in computations.</p>

<p>

<math display="inline" id="Image_segmentation:53">
 <semantics>
  <mrow>
   <mrow>
    <mi>P</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>λ</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <msub>
      <mi mathvariant="normal">Σ</mi>
      <mrow>
       <mi>λ</mi>
       <mi>ϵ</mi>
       <mi mathvariant="normal">Λ</mi>
      </mrow>
     </msub>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>λ</mi>
      <mo stretchy="false">|</mo>
      <msub>
       <mi>f</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mo stretchy="false">|</mo>
     <mi mathvariant="normal">Ω</mi>
     <mo stretchy="false">|</mo>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>P</ci>
     <ci>λ</ci>
    </apply>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-Σ</ci>
       <apply>
        <times></times>
        <ci>λ</ci>
        <ci>ϵ</ci>
        <ci>normal-Λ</ci>
       </apply>
      </apply>
      <csymbol cd="unknown">P</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">λ</csymbol>
       <ci>normal-|</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>f</ci>
        <ci>i</ci>
       </apply>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <apply>
      <abs></abs>
      <ci>normal-Ω</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(\lambda)=\frac{\Sigma_{\lambda\epsilon\Lambda}P(\lambda|f_{i})}{|\Omega|}
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Image_segmentation:54">
 <semantics>
  <mi mathvariant="normal">Ω</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-Ω</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Omega
  </annotation>
 </semantics>
</math>

 is the set of all possible features.</p>
<figure><b>(Figure)</b>
<figcaption>Segmentation of color image using HMRF-EM model</figcaption>
</figure>
<h4 id="disadvantages-of-map-and-em-based-image-segmentation">Disadvantages of MAP and EM based image segmentation</h4>

<p>1. Exact MAP estimates cannot be easily computed.</p>

<p>2. Approximate MAP estimates are computationally expensive to calculate.</p>

<p>3. Extension to multi-class labeling degrades performance and increases storage required.</p>

<p>4. Reliable estimation of parameters for EM is required for global optima to be achieved.</p>

<p>5. Based on method of optimization, segmentation may cluster to local minima.</p>
<h4 id="implementations-of-mrf-based-image-segmentation">Implementations of MRF based Image segmentation</h4>

<p>1. <a href="http://www.mathworks.com/matlabcentral/fileexchange/37530-hmrf-em-image">HMRF-EM</a> Implementation of EM algorithm based image segmentation.</p>

<p>2. <a href="https://www.inf.u-szeged.hu/~kato/software/mrfdemo.html">Zoltan Kato’s</a> implementation of supervised image segmentation using MRFs.<a class="footnoteRef" href="#fn49" id="fnref49"><sup>49</sup></a><a class="footnoteRef" href="#fn50" id="fnref50"><sup>50</sup></a></p>

<p>3. <a href="https://engineering.purdue.edu/~bouman/grad-labs/MAP-Segmentation/">Purdue University’s</a> implementation of Discrete MRF and their application to segmentation.</p>

<p>4. <a href="http://www.mathworks.com/matlabcentral/fileexchange/39553-gmm-hmrf">Gaussian Mixture Model based HMRF</a> segmentation in MATLAB.</p>

<p>5. <a href="http://www.cs.cmu.edu/~mohitg/segmentation.htm">CMU’s</a> implementation of multiple graph cut based segmentation algorithms.</p>
<h2 id="watershed-transformation">Watershed transformation</h2>

<p>The <a href="Watershed_(algorithm)" title="wikilink">watershed transformation</a> considers the gradient magnitude of an image as a topographic surface. Pixels having the highest gradient magnitude intensities (GMIs) correspond to watershed lines, which represent the region boundaries. Water placed on any pixel enclosed by a common watershed line flows downhill to a common local intensity minimum (LIM). Pixels draining to a common minimum form a catch basin, which represents a segment.</p>
<h2 id="model-based-segmentation">Model based segmentation</h2>

<p>The central assumption of such an approach is that structures of interest/organs have a repetitive form of geometry. Therefore, one can seek for a probabilistic model towards explaining the variation of the shape of the organ and then when segmenting an image impose constraints using this model as prior. Such a task involves (i) registration of the training examples to a common pose, (ii) probabilistic representation of the variation of the registered samples, and (iii) statistical inference between the model and the image. State of the art methods in the literature for knowledge-based segmentation involve active shape and appearance models, active contours and deformable templates and level-set based methods. </p>
<h2 id="multi-scale-segmentation">Multi-scale segmentation</h2>

<p>Image segmentations are computed at multiple scales in <a href="scale_space" title="wikilink">scale space</a> and sometimes propagated from coarse to fine scales; see <a href="scale-space_segmentation" title="wikilink">scale-space segmentation</a>.</p>

<p>Segmentation criteria can be arbitrarily complex and may take into account global as well as local criteria. A common requirement is that each region must be connected in some sense.</p>
<h3 id="one-dimensional-hierarchical-signal-segmentation">One-dimensional hierarchical signal segmentation</h3>

<p>Witkin's seminal work<a class="footnoteRef" href="#fn51" id="fnref51"><sup>51</sup></a><a class="footnoteRef" href="#fn52" id="fnref52"><sup>52</sup></a> in scale space included the notion that a one-dimensional signal could be unambiguously segmented into regions, with one scale parameter controlling the scale of segmentation.</p>

<p>A key observation is that the zero-crossings of the second derivatives (minima and maxima of the first derivative or slope) of multi-scale-smoothed versions of a signal form a nesting tree, which defines hierarchical relations between segments at different scales. Specifically, slope extrema at coarse scales can be traced back to corresponding features at fine scales. When a slope maximum and slope minimum annihilate each other at a larger scale, the three segments that they separated merge into one segment, thus defining the hierarchy of segments.</p>
<h3 id="image-segmentation-and-primal-sketch">Image segmentation and primal sketch</h3>

<p>There have been numerous research works in this area, out of which a few have now reached a state where they can be applied either with interactive manual intervention (usually with application to medical imaging) or fully automatically. The following is a brief overview of some of the main research ideas that current approaches are based upon.</p>

<p>The nesting structure that Witkin described is, however, specific for one-dimensional signals and does not trivially transfer to higher-dimensional images. Nevertheless, this general idea has inspired several other authors to investigate coarse-to-fine schemes for image segmentation. Koenderink<a class="footnoteRef" href="#fn53" id="fnref53"><sup>53</sup></a> proposed to study how iso-intensity contours evolve over scales and this approach was investigated in more detail by Lifshitz and Pizer.<a class="footnoteRef" href="#fn54" id="fnref54"><sup>54</sup></a> Unfortunately, however, the intensity of image features changes over scales, which implies that it is hard to trace coarse-scale image features to finer scales using iso-intensity information.</p>

<p>Lindeberg<a class="footnoteRef" href="#fn55" id="fnref55"><sup>55</sup></a><a class="footnoteRef" href="#fn56" id="fnref56"><sup>56</sup></a> studied the problem of linking local extrema and saddle points over scales, and proposed an image representation called the scale-space primal sketch which makes explicit the relations between structures at different scales, and also makes explicit which image features are stable over large ranges of scale including locally appropriate scales for those. Bergholm proposed to detect edges at coarse scales in scale-space and then trace them back to finer scales with manual choice of both the coarse detection scale and the fine localization scale.</p>

<p>Gauch and Pizer<a class="footnoteRef" href="#fn57" id="fnref57"><sup>57</sup></a> studied the complementary problem of ridges and valleys at multiple scales and developed a tool for interactive image segmentation based on multi-scale watersheds. The use of multi-scale watershed with application to the gradient map has also been investigated by Olsen and Nielsen<a class="footnoteRef" href="#fn58" id="fnref58"><sup>58</sup></a> and been carried over to clinical use by Dam<a class="footnoteRef" href="#fn59" id="fnref59"><sup>59</sup></a> Vincken et al.<a class="footnoteRef" href="#fn60" id="fnref60"><sup>60</sup></a> proposed a hyperstack for defining probabilistic relations between image structures at different scales. The use of stable image structures over scales has been furthered by Ahuja<a class="footnoteRef" href="#fn61" id="fnref61"><sup>61</sup></a><a class="footnoteRef" href="#fn62" id="fnref62"><sup>62</sup></a> and his co-workers into a fully automated system. A fully automatic brain segmentation algorithm based on closely related ideas of multi-scale watersheds has been presented by Undeman and Lindeberg<a class="footnoteRef" href="#fn63" id="fnref63"><sup>63</sup></a> and been extensively tested in brain databases.</p>

<p>These ideas for multi-scale image segmentation by linking image structures over scales have also been picked up by Florack and Kuijper.<a class="footnoteRef" href="#fn64" id="fnref64"><sup>64</sup></a> Bijaoui and Rué<a class="footnoteRef" href="#fn65" id="fnref65"><sup>65</sup></a> associate structures detected in scale-space above a minimum noise threshold into an object tree which spans multiple scales and corresponds to a kind of feature in the original signal. Extracted features are accurately reconstructed using an iterative conjugate gradient matrix method.</p>
<h2 id="semi-automatic-segmentation">Semi-automatic segmentation</h2>

<p>In one kind of segmentation, the user outlines the region of interest with the mouse clicks and algorithms are applied so that the path that best fits the edge of the image is shown.</p>

<p>Techniques like <a href="Simple_Interactive_Object_Extraction" title="wikilink">SIOX</a>, <a href="Livewire_Segmentation_Technique" title="wikilink">Livewire</a>, Intelligent Scissors or IT-SNAPS are used in this kind of segmentation. In an alternative kind of semi-automatic segmentation, the algorithms return a spatial-taxon (i.e. foreground, object-group, object or object-part) selected by the user or designated via prior probabilities.<a class="footnoteRef" href="#fn66" id="fnref66"><sup>66</sup></a><a class="footnoteRef" href="#fn67" id="fnref67"><sup>67</sup></a></p>
<h2 id="trainable-segmentation">Trainable segmentation</h2>

<p>Most segmentation methods are based only on color information of pixels in the image. Humans use much more knowledge than this when doing image segmentation, but implementing this knowledge would cost considerable computation time and would require a huge domain-knowledge database, which is currently not available. In addition to traditional segmentation methods, there are trainable segmentation methods which can model some of this knowledge.</p>

<p>Neural Network segmentation relies on processing small areas of an image using an <a href="artificial_neural_network" title="wikilink">artificial neural network</a><a class="footnoteRef" href="#fn68" id="fnref68"><sup>68</sup></a> or a set of neural networks. After such processing the decision-making mechanism marks the areas of an image accordingly to the category recognized by the neural network. A type of network designed especially for this is the <a href="Kohonen_map" title="wikilink">Kohonen map</a>.</p>

<p><a href="Pulse-coupled_networks" title="wikilink">Pulse-coupled neural networks (PCNNs)</a> are neural models proposed by modeling a cat’s visual cortex and developed for high-performance biomimetic image processing. In 1989, Eckhorn introduced a neural model to emulate the mechanism of a cat’s visual cortex. The Eckhorn model provided a simple and effective tool for studying the visual cortex of small mammals, and was soon recognized as having significant application potential in image processing. In 1994, the Eckhorn model was adapted to be an image processing algorithm by Johnson, who termed this algorithm Pulse-Coupled Neural Network. Over the past decade, PCNNs have been utilized for a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, noise reduction, and so on. A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel’s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be utilized for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.</p>

<p><strong>Open-source implementations of trainable segmentation</strong>:</p>
<ul>
<li><a href="http://fiji.sc/wiki/index.php/Trainable_Weka_Segmentation">Trainable Segmentation (in Java)</a></li>
<li><a href="http://www.burgsys.com/image-processing-software-free.php">IMMI</a></li>
</ul>
<h2 id="other-methods">Other methods</h2>

<p>There are many other methods of segmentation like <a href="multispectral_segmentation" title="wikilink">multispectral segmentation</a> or connectivity-based segmentation based on <a href="Diffusion_MRI#Diffusion_tensor_imaging" title="wikilink">DTI images</a>.<a class="footnoteRef" href="#fn69" id="fnref69"><sup>69</sup></a></p>
<h2 id="segmentation-benchmarking">Segmentation benchmarking</h2>

<p>Several segmentation benchmarks are available for comparing the performance of segmentation methods with the state-of-the-art segmentation methods on standardized sets:</p>
<ul>
<li><a href="http://mosaic.utia.cas.cz">Prague On-line Texture Segmentation Benchmark</a><a class="footnoteRef" href="#fn70" id="fnref70"><sup>70</sup></a></li>
<li><a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/">The Berkeley Segmentation Dataset and Benchmark</a><ref></ref></li>
</ul>

<p></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Computer_vision" title="wikilink">Computer vision</a></li>
<li><a href="Image-based_meshing" title="wikilink">Image-based meshing</a></li>
<li><a href="Range_image_segmentation" title="wikilink">Range image segmentation</a></li>
<li><a href="Vector_quantization" title="wikilink">Vector quantization</a></li>
<li><a href="Image_quantization" title="wikilink">Image quantization</a></li>
<li><a href="Color_quantization" title="wikilink">Color quantization</a></li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<ul>
<li><a href="http://instrumentation.hit.bg/Papers/2008-02-02%203D%20Multistage%20Entropy.htm">3D Entropy Based Image Segmentation</a></li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://csc.fsksm.utm.my/syed/projects/image-processing.html">Some sample code that performs basic segmentation</a>, by Syed Zainudeen. University Technology of Malaysia.</li>
<li><a href="http://rd.springer.com/article/10.1007/s11075-008-9183-x">Generalized Fast Marching method</a> by Forcadel et al. [2008] for applications in image segmentation.</li>
<li><a href="http://www.iprg.co.in">Image Processing Research Group</a> An Online Open Image Processing Research Community.</li>
<li><a href="https://www.mathworks.com/discovery/image-segmentation.html">Segmentation methods in image processing and analysis</a></li>
</ul>

<p>"</p>

<p><a href="Category:Image_segmentation" title="wikilink">*</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Linda G. Shapiro and George C. Stockman (2001): “Computer Vision”, pp 279-325, New Jersey, Prentice-Hall, ISBN 0-13-030796-3<a href="#fnref1">↩</a></li>
<li id="fn2">Barghout, Lauren, and Lawrence W. Lee. "Perceptual information processing system." Paravue Inc. U.S. Patent Application 10/618,543, filed July 11, 2003.<a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6">W. Wu, A. Y. C. Chen, L. Zhao and J. J. Corso (2014): "Brain Tumor detection and segmentation in a CRF framework with pixel-pairwise affinity and super pixel-level features", International Journal of Computer Aided Radiology and Surgery, pp. 241-253, Vol. 9.<a href="#fnref6">↩</a></li>
<li id="fn7">E. B. George and M. Karnan (2012): "MR Brain image segmentation using Bacteria Foraging Optimization Algorithm", <em>International Journal of Engineering and Technology</em>, Vol. 4.<a href="#fnref7">↩</a></li>
<li id="fn8">J. A. Delmerico, P. David and J. J. Corso (2011): "Building façade detection, segmentation and parameter estimation for mobile robot localization and guidance", International Conference on Intelligent Robots and Systems, pp. 1632-1639.<a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10">K J. Batenburg, and J. Sijbers, "Optimal Threshold Selection for Tomogram Segmentation by Projection Distance Minimization", <em>IEEE Transactions on Medical Imaging</em>, vol. 28, no. 5, pp. 676-686, June, 2009 <a href="http://www.visielab.ua.ac.be/publications/optimal-threshold-selection-tomogram-segmentation-projection-distance-minimization">PDF</a><a href="#fnref10">↩</a></li>
<li id="fn11">A. Kashanipour, N Milani, A. Kashanipour, H. Eghrary," Robust Color Classification Using Fuzzy Rule-Based Particle Swarm Optimization", <em>IEEE Congress on Image and Signal Processing</em>, vol. 2, pp. 110-114 , May, 2008 [<a class="uri" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp">http://ieeexplore.ieee.org/xpl/login.jsp?tp</a>=&amp;arnumber;=4566278&amp;url;=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4566278]<a href="#fnref11">↩</a></li>
<li id="fn12">Barghout, Lauren, and Jacob Sheynin. "Real-world scene perception and perceptual organization: Lessons from Computer Vision." Journal of Vision 13.9 (2013): 709-709.<a href="#fnref12">↩</a></li>
<li id="fn13">Hossein Mobahi, Shankar Rao, Allen Yang, Shankar Sastry and Yi Ma. <a href="http://perception.csl.illinois.edu/coding/papers/MobahiH2011-IJCV.pdf">Segmentation of Natural Images by Texture and Boundary Compression</a>, International Journal of Computer Vision (IJCV), 95 (1), pg. 86-98, Oct. 2011.<a href="#fnref13">↩</a></li>
<li id="fn14">Shankar Rao, Hossein Mobahi, Allen Yang, Shankar Sastry and Yi Ma <a href="http://perception.csl.illinois.edu/coding/papers/RaoS2009-ACCV.pdf">Natural Image Segmentation with Adaptive Texture and Boundary Encoding</a>, Proceedings of the Asian Conference on Computer Vision (ACCV) 2009, H. Zha, R.-i. Taniguchi, and S. Maybank (Eds.), Part I, LNCS 5994, pp. 135--146, Springer.<a href="#fnref14">↩</a></li>
<li id="fn15"></li>
<li id="fn16"></li>
<li id="fn17"><a href="#fnref17">↩</a></li>
<li id="fn18"><a href="R._Kimmel_and_A.M._Bruckstein." title="wikilink">R. Kimmel and A.M. Bruckstein.</a> <a class="uri" href="http://www.cs.technion.ac.il/~ron/PAPERS/Paragios_chapter2003.pdf">http://www.cs.technion.ac.il/~ron/PAPERS/Paragios_chapter2003.pdf</a>, <em>International Journal of Computer Vision</em> 2003; 53(3):225-243.<a href="#fnref18">↩</a></li>
<li id="fn19"><a href="R._Kimmel" title="wikilink">R. Kimmel</a>, <a class="uri" href="http://www.cs.technion.ac.il/~ron/PAPERS/laplacian_ijcv2003.pdf">http://www.cs.technion.ac.il/~ron/PAPERS/laplacian_ijcv2003.pdf</a>, chapter in Geometric Level Set Methods in Imaging, Vision and Graphics, (S. Osher, N. Paragios, Eds.), Springer Verlag, 2003. ISBN 0387954880<a href="#fnref19">↩</a></li>
<li id="fn20">Barghout, Lauren. Visual Taxometric approach Image Segmentation using Fuzzy-Spatial Taxon Cut Yields Contextually Relevant Regions. Communications in Computer and Information Science (CCIS). Springer-Verlag. 2014<a href="#fnref20">↩</a></li>
<li id="fn21">Witold Pedrycz (Editor), Andrzej Skowron (Co-Editor), Vladik Kreinovich (Co-Editor). Handbook of Granular Computing. Wiley 2008<a href="#fnref21">↩</a></li>
<li id="fn22">Barghout, Lauren (2014). Vision. Global Conceptual Context Changes Local Contrast Processing (Ph.D. Dissertation 2003). Updated to include Computer Vision Techniques. Scholars' Press. ISBN 978-3-639-70962-9.<a href="#fnref22">↩</a></li>
<li id="fn23">Barghout, Lauren, and Lawrence Lee. "Perceptual information processing system." Google Patents<a href="#fnref23">↩</a></li>
<li id="fn24"><a href="#fnref24">↩</a></li>
<li id="fn25"><a href="http://gth.krammerbuch.at/sites/default/files/articles/AHAH%20callback/01_Guberman_KORR.pdf"></a><a href="Guberman_Shelia_(Shelija)" title="wikilink">Shelia Guberman</a>, Vadim V. Maximov, Alex Pashintsev Gestalt and Image Understanding. GESTALT THEORY 2012, Vol. 34, No.2, 143-166.<a href="#fnref25">↩</a></li>
<li id="fn26">R. Nock and F. Nielsen, Statistical Region Merging, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol 26, No 11, pp 1452-1458, 2004.<a href="#fnref26">↩</a></li>
<li id="fn27"></li>
<li id="fn28">L. Chen, H. D. Cheng, and J. Zhang, Fuzzy subfiber and its application to seismic lithology classification, Information Sciences: Applications, Vol 1, No 2, pp 77-95, 1994.<a href="#fnref28">↩</a></li>
<li id="fn29">S.L. Horowitz and T. Pavlidis, Picture Segmentation by a Directed Split and Merge Procedure, Proc. ICPR, 1974, Denmark, pp.424-433.<a href="#fnref29">↩</a></li>
<li id="fn30">S.L. Horowitz and T. Pavlidis, Picture Segmentation by a Tree Traversal Algorithm, Journal of the ACM, 23 (1976), pp. 368-388.<a href="#fnref30">↩</a></li>
<li id="fn31">L. Chen, <a href="http://www.spclab.com/research/lambda/lambdaConn91.pdf">The lambda-connected segmentation and the optimal algorithm for split-and-merge segmentation</a>, Chinese J. Computers, 14(1991), pp 321-331<a href="#fnref31">↩</a></li>
<li id="fn32"><a href="V._Caselles,_R._Kimmel,_and_G._Sapiro." title="wikilink">V. Caselles, R. Kimmel, and G. Sapiro.</a> Geodesic active contours. International Journal of Computer Vision, 22(1):61-79, 1997. <a class="uri" href="http://www.cs.technion.ac.il/~ron/PAPERS/CasKimSap_IJCV1997.pdf">http://www.cs.technion.ac.il/~ron/PAPERS/CasKimSap_IJCV1997.pdf</a><a href="#fnref32">↩</a></li>
<li id="fn33">S. Osher and N. Paragios. <a href="http://www.mas.ecp.fr/vision/Personnel/nikos/osher-paragios/">Geometric Level Set Methods in Imaging Vision and Graphics</a>, Springer Verlag, ISBN 0-387-95488-0, 2003.<a href="#fnref33">↩</a></li>
<li id="fn34"><a href="#fnref34">↩</a></li>
<li id="fn35"><a href="#fnref35">↩</a></li>
<li id="fn36"><a href="#fnref36">↩</a></li>
<li id="fn37"><a href="David_Mumford" title="wikilink">David Mumford</a> and Jayant Shah (1989): Optimal approximations by piecewise smooth functions and associated variational problems, <em>Communications on Pure and Applied Mathematics</em>, pp 577-685, Vol. 42, No. 5<a href="#fnref37">↩</a></li>
<li id="fn38">Jianbo Shi and <a href="Jitendra_Malik" title="wikilink">Jitendra Malik</a> (2000): <a href="http://www.cs.cmu.edu/~jshi/papers/pami_ncut.pdf">"Normalized Cuts and Image Segmentation"</a>, <em>IEEE Transactions on pattern analysis and machine intelligence</em>, pp 888-905, Vol. 22, No. 8<a href="#fnref38">↩</a></li>
<li id="fn39">Leo Grady (2006): <a href="http://www.cns.bu.edu/~lgrady/grady2006random.pdf">"Random Walks for Image Segmentation"</a>, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, pp. 1768–1783, Vol. 28, No. 11<a href="#fnref39">↩</a></li>
<li id="fn40">Z. Wu and R. Leahy (1993): <a href="ftp://sipi.usc.edu/pub/leahy/pdfs/MAP93.pdf">"An optimal graph theoretic approach to data clustering: Theory and its application to image segmentation"</a>, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, pp. 1101–1113, Vol. 15, No. 11<a href="#fnref40">↩</a></li>
<li id="fn41">Leo Grady and Eric L. Schwartz (2006): <a href="http://www.cns.bu.edu/~lgrady/grady2006isoperimetric.pdf">"Isoperimetric Graph Partitioning for Image Segmentation"</a>, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, pp. 469–475, Vol. 28, No. 3<a href="#fnref41">↩</a></li>
<li id="fn42">C. T. Zahn (1971): <a href="http://web.cse.msu.edu/~cse802/Papers/zahn.pdf">"Graph-theoretical methods for detecting and describing gestalt clusters"</a>, <em>IEEE Transactions on Computers</em>, pp. 68–86, Vol. 20, No. 1<a href="#fnref42">↩</a></li>
<li id="fn43">S. Geman and D. Geman (1984): "Stochastic relaxation, Gibbs Distributions and Bayesian Restoration of Images", IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 721-741, Vol. 6, No.6.<a href="#fnref43">↩</a></li>
<li id="fn44">A. Bouman and M. Shapiro (2002): "A multiscale Random field model for Bayesian image segmentation", IEEE Transactions on Image Processing, pp. 162-177, Vol. 3.<a href="#fnref44">↩</a></li>
<li id="fn45">J. Liu and Y. H. Yang (1994): "Multiresolution color image segmentation", IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 689-700, Vol. 16.<a href="#fnref45">↩</a></li>
<li id="fn46">S. Vicente, V. Kolmogrov and C. Rother (2008): "Graph cut based image segmentation with connectivity priors", CVPR<a href="#fnref46">↩</a></li>
<li id="fn47">Corso, Z. Tu, and A. Yuille (2008): "MRF Labelling with Graph-Shifts Algorithm", Proceedings of International workshop on combinatorial Image Analysis<a href="#fnref47">↩</a></li>
<li id="fn48">B. J. Frey and D. MacKayan (1997): "A Revolution: Belief propagation in Graphs with Cycles", Proceedings of Neural Information Processing Systems (NIPS)<a href="#fnref48">↩</a></li>
<li id="fn49">Z. Kato, J. Zerubia and M. Berthod (1992): "Satellite Image classification using modified metropolis dynamic", Proceedings of International Conference on Acoustics, Speech and Signal Processing, pp. 573-576, Vol. 3.<a href="#fnref49">↩</a></li>
<li id="fn50">Z. Kato, "Modelisations markoviennes multiresolutions en vision par ordinateur", Thesis 1994.<a href="#fnref50">↩</a></li>
<li id="fn51">Witkin, A. P. "Scale-space filtering", Proc. 8th Int. Joint Conf. Art. Intell., Karlsruhe, Germany,1019–1022, 1983.<a href="#fnref51">↩</a></li>
<li id="fn52">A. Witkin, "Scale-space filtering: A new approach to multi-scale description," in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing (<a class="uri" href="ICASSP" title="wikilink">ICASSP</a>), vol. 9, San Diego, CA, Mar. 1984, pp. 150–153.<a href="#fnref52">↩</a></li>
<li id="fn53">Koenderink, Jan "The structure of images", Biological Cybernetics, 50:363–370, 1984<a href="#fnref53">↩</a></li>
<li id="fn54">[<a class="uri" href="http://portal.acm.org/citation.cfm?id=80964&amp;dl">http://portal.acm.org/citation.cfm?id=80964&amp;dl;</a>;=GUIDE&amp;coll;=GUIDE Lifshitz, L. and Pizer, S.: A multiresolution hierarchical approach to image segmentation based on intensity extrema, IEEE Transactions on Pattern Analysis and Machine Intelligence, 12:6, 529–540, 1990.]<a href="#fnref54">↩</a></li>
<li id="fn55"><a href="http://www.nada.kth.se/~tony/abstracts/Lin92-IJCV.html">Lindeberg, T.: Detecting salient blob-like image structures and their scales with a scale-space primal sketch: A method for focus-of-attention, International Journal of Computer Vision, 11(3), 283–318, 1993.</a><a href="#fnref55">↩</a></li>
<li id="fn56"><a href="http://www.nada.kth.se/~tony/book.html">Lindeberg, Tony, Scale-Space Theory in Computer Vision, Kluwer Academic Publishers, 1994</a>, ISBN 0-7923-9418-6<a href="#fnref56">↩</a></li>
<li id="fn57">[<a class="uri" href="http://portal.acm.org/citation.cfm?coll=GUIDE&amp;dl">http://portal.acm.org/citation.cfm?coll=GUIDE&amp;dl;</a>;=GUIDE&amp;id;=628490 Gauch, J. and Pizer, S.: Multiresolution analysis of ridges and valleys in grey-scale images, IEEE Transactions on Pattern Analysis and Machine Intelligence, 15:6 (June 1993), pages: 635–646, 1993.]<a href="#fnref57">↩</a></li>
<li id="fn58">Olsen, O. and Nielsen, M.: Multi-scale gradient magnitude watershed segmentation, Proc. of ICIAP 97, Florence, Italy, Lecture Notes in Computer Science, pages 6–13. Springer Verlag, September 1997.<a href="#fnref58">↩</a></li>
<li id="fn59">Dam, E., Johansen, P., Olsen, O. Thomsen,, A. Darvann, T. , Dobrzenieck, A., Hermann, N., Kitai, N., Kreiborg, S., Larsen, P., Nielsen, M.: "Interactive multi-scale segmentation in clinical use" in European Congress of Radiology 2000.<a href="#fnref59">↩</a></li>
<li id="fn60">Vincken, K., Koster, A. and Viergever, M.: , IEEE Transactions on Pattern Analysis and Machine Intelligence, 19:2, pp. 109–120, 1997.]<a href="#fnref60">↩</a></li>
<li id="fn61"><a href="http://vision.ai.uiuc.edu/~msingh/segmen/seg/MSS.html">M. Tabb and N. Ahuja, Unsupervised multiscale image segmentation by integrated edge and region detection, IEEE Transactions on Image Processing, Vol. 6, No. 5, 642–655, 1997.</a><a href="#fnref61">↩</a></li>
<li id="fn62"><a href="http://www.springerlink.com/content/44627w1458284738/">E. Akbas and N. Ahuja, "From ramp discontinuities to segmentation tree"</a><a href="#fnref62">↩</a></li>
<li id="fn63"><a href="http://www.csc.kth.se/cvap/abstracts/cvap285.html">C. Undeman and T. Lindeberg (2003) "Fully Automatic Segmentation of MRI Brain Images using Probabilistic Anisotropic Diffusion and Multi-Scale Watersheds", Proc. Scale-Space'03, Isle of Skye, Scotland, Springer Lecture Notes in Computer Science, volume 2695, pages 641--656.</a><a href="#fnref63">↩</a></li>
<li id="fn64">Florack, L. and Kuijper, A.: The topological structure of scale-space images, Journal of Mathematical Imaging and Vision, 12:1, 65–79, 2000.<a href="#fnref64">↩</a></li>
<li id="fn65"><a href="http://dx.doi.org/10.1016/0165-1684(95)00093-4">Bijaoui, A., Rué, F.: 1995, A Multiscale Vision Model, <em>Signal Processing</em> <strong>46</strong>, 345</a><a href="#fnref65">↩</a></li>
<li id="fn66">Barghout, Lauren. Visual Taxometric Approach to Image Segmentation using Fuzzy-Spatial Taxon Cut Yields Contextually Relevant Regions. IPMU 2014, Part II. A. Laurent et al (Eds.) CCIS 443, pp 163-173. Springer International Publishing Switzerland<a href="#fnref66">↩</a></li>
<li id="fn67"><a href="#fnref67">↩</a></li>
<li id="fn68"><a href="Mahinda_Pathegama" title="wikilink">Mahinda Pathegama</a> &amp; Ö Göl (2004): "Edge-end pixel extraction for edge-based image segmentation", <em>Transactions on Engineering, Computing and Technology,</em> vol. 2, pp 213–216, ISSN 1305-5313<a href="#fnref68">↩</a></li>
<li id="fn69">Menke, RA, Jbabdi, S, Miller, KL, Matthews, PM and Zarei, M.: , Neuroimage, 52:4, pp. 1175–80, 2010.]<a href="#fnref69">↩</a></li>
<li id="fn70">Haindl, M. – Mikeš, S. <a href="http://dx.doi.org/10.1109/ICPR.2008.4761118">Texture Segmentation Benchmark</a>, Proc. of the 19th Int. Conference on Pattern Recognition. IEEE Computer Society, 2008, pp. 1–4 ISBN 978-1-4244-2174-9 ISSN 1051-4651<a href="#fnref70">↩</a></li>
</ol>
</section>
</body>
</html>
