   Ordinal regression      Ordinal regression   In statistics , ordinal regression is a type of regression analysis used for predicting an ordinal variable , i.e. a variable whose value exists on an arbitrary scale where only the relative ordering between different values is significant. It can be considered an intermediate problem in between (metric) regression and classification . 1 Ordinal regression turns up often in the social sciences , for example in the modeling of human levels of preference (on a scale from, say, 1–5 for "very poor" through "excellent"), as well as in information retrieval . In machine learning , ordinal regression may also be called ranking learning . 2  Linear models for ordinal regression  Ordinal regression can be performed using a generalized linear model (GLM) that fits both a coefficient vector and a set of thresholds to a dataset. Suppose one has a set of observations, represented by length-   p   p   p   vectors through , with associated responses  through , where each is an ordinal variable on a scale    1  ,  …  ,  K     1  normal-…  K    1,...,K   . To this data, one fits a length-   p   p   p   coefficient vector   𝐰   𝐰   \mathbf{w}   and a set of thresholds with the property that . This set of thresholds divides the real number line into   K   K   K   disjoint segments, corresponding to the   K   K   K   response levels.  The model can now be formulated as       Pr   (   y  ≤  k   |  𝐱  )    =   σ   (    θ  i   -   𝐰  ⋅  𝐱    )         Pr    y  k   𝐱     σ     subscript  θ  i    normal-⋅  𝐰  𝐱       \Pr(y\leq k|\mathbf{x})=\sigma(\theta_{i}-\mathbf{w}\cdot\mathbf{x})     or, the cumulative probability of the response   y   y   y   being at most   k   k   k   is given by a function   σ   σ   σ   (the inverse link function ) applied to a linear function of   𝐱   𝐱   \mathbf{x}   . Several choices exist for   σ   σ   σ   ; the logistic function       σ   (    θ  i   -   𝐰  ⋅  𝐱    )    =   1   1  +   e    𝐰  ⋅  𝐱   -   θ  i             σ     subscript  θ  i    normal-⋅  𝐰  𝐱       1    1   superscript  e     normal-⋅  𝐰  𝐱    subscript  θ  i         \sigma(\theta_{i}-\mathbf{w}\cdot\mathbf{x})=\frac{1}{1+e^{\mathbf{w}\cdot%
 \mathbf{x}-\theta_{i}}}     gives the ordered logit model, while using the probit function gives the ordered probit model. A third option is to use an exponential function\       σ   (    θ  i   -   𝐰  ⋅  𝐱    )    =   exp   (   -   exp   (    θ  i   -   𝐰  ⋅  𝐱    )     )          σ     subscript  θ  i    normal-⋅  𝐰  𝐱              subscript  θ  i    normal-⋅  𝐰  𝐱         \sigma(\theta_{i}-\mathbf{w}\cdot\mathbf{x})=\exp(-\exp(\theta_{i}-\mathbf{w}%
 \cdot\mathbf{x}))     which gives the proportional hazards model . 3  Latent variable model  The probit version of the above model can be justified by assuming the existence of a real-valued latent variable (unobserved quantity)    y  *     fragments  y     y*   , determined by 4       y  *   =    𝐰  ⋅  𝐱   +  ε        superscript  y       normal-⋅  𝐰  𝐱   ε     y^{*}=\mathbf{w}\cdot\mathbf{x}+\varepsilon     where   ε   ε   ε   is normally distributed with zero mean and unit variance, conditioned on   𝐱   𝐱   \mathbf{x}   . The response variable   y   y   y   results from an "incomplete measurement" of    y  *     fragments  y     y*   , where one only determines the interval into which    y  *     fragments  y     y*   falls:      y  =   {         1    if    y  *    ≤   θ  1    ,            2    if    θ  1    <   y  *   ≤   θ  2    ,           3    if    θ  2    <   y  *   ≤   θ  3         ⋮           K    if    θ   K  -  1     <   y  *    .            y   cases      1  if   superscript  y      subscript  θ  1    otherwise        2  if   subscript  θ  1     superscript  y          subscript  θ  2     otherwise        3  if   subscript  θ  2     superscript  y          subscript  θ  3     otherwise  normal-⋮  otherwise      K  if   subscript  θ    K  1      superscript  y     otherwise     y=\begin{cases}1~{}~{}\text{if}~{}~{}y^{*}\leq\theta_{1},\\
 2~{}~{}\text{if}~{}~{}\theta_{1}     Defining -∞}} and ∞}} , the above can be summarized as    y  =  k      y  k    y=k    if and only if  .  From these assumptions, one can derive the conditional distribution of   y   y   y   as      P   (  y  =  k  |  𝐱  )      fragments  P   fragments  normal-(  y   k  normal-|  x  normal-)     \displaystyle P(y=k|\mathbf{x})     where   Φ   normal-Φ   Φ   is the cumulative distribution function of the standard normal distribution, and takes on the role of the inverse link function   σ   σ   σ   . The log-likelihood of the model for a single training example , can now be stated as      log  ℒ   (  𝐰  ,  θ  |   𝐱  i   ,   y  i   )   =   ∑   k  =  1   K    [   y  i   =  k  ]   log   [  Φ   (   θ  k   -  𝐰  ⋅  𝐱  )   -  Φ   (   θ   k  -  1    -  𝐰  ⋅  𝐱  )   ]      fragments   L   fragments  normal-(  w  normal-,  θ  normal-|   subscript  𝐱  i   normal-,   subscript  y  i   normal-)     superscript   subscript     k  1    K    fragments  normal-[   subscript  y  i    k  normal-]     fragments  normal-[  Φ   fragments  normal-(   subscript  θ  k    w  normal-⋅  x  normal-)    Φ   fragments  normal-(   subscript  θ    k  1     w  normal-⋅  x  normal-)   normal-]     \log\mathcal{L}(\mathbf{w},\mathbf{\theta}|\mathbf{x}_{i},y_{i})=\sum_{k=1}^{K%
 }[y_{i}=k]\log[\Phi(\theta_{k}-\mathbf{w}\cdot\mathbf{x})-\Phi(\theta_{k-1}-%
 \mathbf{w}\cdot\mathbf{x})]     (using the Iverson bracket   k ]}} .) The log-likelihood of the ordered logit model is analogous, using the logistic function instead of   Φ   normal-Φ   Φ   . 5  Alternative models  In machine learning, alternatives to the latent-variable models of ordinal regression have been proposed. An early result was PRank, a variant of the perceptron algorithm that found multiple parallel hyperplanes separating the various ranks; its output is a weight vector   𝐰   𝐰   \mathbf{w}   and a sorted vector of    K  −  1      K  normal-−  1    K−1   thresholds   θ   θ   \mathbf{θ}   , as in the ordered logit/probit models. The prediction rule for this model is to output the smallest rank   k   k   k   such that . 6  Other methods rely on the principle of large-margin learning that also underlies support vector machines . 7 8  Another approach is given by Rennie and Srebro, who, realizing that "even just evaluating the likelihood of a predictor is not straight-forward" in the ordered logit and ordered probit models, propose fitting ordinal regression models by adapting common loss functions from classification (such as the hinge loss and log loss ) to the ordinal case. 9  Notes  References  Further reading       "  Category:Regression analysis  Category:Categorical data     ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩     