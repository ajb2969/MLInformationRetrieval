<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="538">Bias–variance tradeoff</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Bias–variance tradeoff</h1>
<hr/>

<p>In <a class="uri" href="statistics" title="wikilink">statistics</a> and <a href="machine_learning" title="wikilink">machine learning</a>, the <strong>bias–variance tradeoff</strong> (or <strong>dilemma</strong>) is the problem of simultaneously minimizing two sources of <a href="Errors_and_residuals_in_statistics" title="wikilink">error</a> that prevent <a href="supervised_learning" title="wikilink">supervised learning</a> algorithms from generalizing beyond their <a href="training_set" title="wikilink">training set</a>:</p>
<ul>
<li>The <a href="Bias_of_an_estimator" title="wikilink"><em>bias</em></a> is error from erroneous assumptions in the learning <a class="uri" href="algorithm" title="wikilink">algorithm</a>. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).</li>
<li>The <em><a class="uri" href="variance" title="wikilink">variance</a></em> is error from sensitivity to small fluctuations in the training set. High variance can cause <a class="uri" href="overfitting" title="wikilink">overfitting</a>: modeling the random <a href="Noise_(signal_processing)" title="wikilink">noise</a> in the training data, rather than the intended outputs.</li>
</ul>

<p>The <strong>bias–variance decomposition</strong> is a way of analyzing a learning algorithm's <a href="expected_value" title="wikilink">expected</a> generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the <em>irreducible error</em>, resulting from noise in the problem itself.</p>

<p>This tradeoff applies to all forms of <a href="supervised_learning" title="wikilink">supervised learning</a>: <a href="Statistical_classification" title="wikilink">classification</a>, <a href="Regression_analysis" title="wikilink">regression</a> (function fitting),<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> and <a href="Structured_prediction" title="wikilink">structured output learning</a>. It has also been invoked to explain the effectiveness of heuristics in human learning.</p>
<h2 id="motivation">Motivation</h2>

<p>The bias–variance tradeoff is a central problem in supervised learning. Ideally, one wants to <a href="Model_selection" title="wikilink">choose a model</a> that both accurately captures the regularities in its training data, but also <a href="Generalization" title="wikilink">generalizes</a> well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well, but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that don't tend to overfit, but may <em>underfit</em> their training data, failing to capture important regularities.</p>

<p>Models with low bias are usually more complex (e.g. higher-order regression polynomials), enabling them to represent the training set more accurately. In the process, however, they may also represent a large <a href="Noise_(signal_processing)" title="wikilink">noise</a> component in the training set, making their predictions less accurate - despite their added complexity. In contrast, models with higher bias tend to be relatively simple (low-order or even linear regression polynomials), but may produce lower variance predictions when applied beyond the training set.</p>
<h2 id="biasvariance-decomposition-of-squared-error">Bias–variance decomposition of squared error</h2>

<p>Suppose that we have a training set consisting of a set of points 

<math display="inline" id="Bias–variance_tradeoff:0">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>x</mi>
    <mi>n</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>n</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{1},\dots,x_{n}
  </annotation>
 </semantics>
</math>

 and real values 

<math display="inline" id="Bias–variance_tradeoff:1">
 <semantics>
  <msub>
   <mi>y</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>y</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{i}
  </annotation>
 </semantics>
</math>

 associated with each point 

<math display="inline" id="Bias–variance_tradeoff:2">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}
  </annotation>
 </semantics>
</math>

. We assume that there is a functional, but noisy relation 

<math display="inline" id="Bias–variance_tradeoff:3">
 <semantics>
  <mrow>
   <msub>
    <mi>y</mi>
    <mi>i</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mi>ϵ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <ci>f</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>i</ci>
      </apply>
     </apply>
     <ci>ϵ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{i}=f(x_{i})+\epsilon
  </annotation>
 </semantics>
</math>

, where the noise, 

<math display="inline" id="Bias–variance_tradeoff:4">
 <semantics>
  <mi>ϵ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ϵ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \epsilon
  </annotation>
 </semantics>
</math>

, has zero mean and variance 

<math display="inline" id="Bias–variance_tradeoff:5">
 <semantics>
  <msup>
   <mi>σ</mi>
   <mn>2</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>σ</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma^{2}
  </annotation>
 </semantics>
</math>

.</p>

<p>We want to find a function 

<math display="inline" id="Bias–variance_tradeoff:6">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>f</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <ci>normal-^</ci>
     <ci>f</ci>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{f}(x)
  </annotation>
 </semantics>
</math>

, that approximates the true function 

<math display="inline" id="Bias–variance_tradeoff:7">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=f(x)
  </annotation>
 </semantics>
</math>

 as well as possible, by means of some learning algorithm. We make "as well as possible" precise by measuring the <a href="mean_squared_error" title="wikilink">mean squared error</a> between 

<math display="inline" id="Bias–variance_tradeoff:8">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Bias–variance_tradeoff:9">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>f</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <ci>normal-^</ci>
     <ci>f</ci>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{f}(x)
  </annotation>
 </semantics>
</math>

: we want 

<math display="inline" id="Bias–variance_tradeoff:10">
 <semantics>
  <msup>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>y</mi>
     <mo>-</mo>
     <mrow>
      <mover accent="true">
       <mi>f</mi>
       <mo stretchy="false">^</mo>
      </mover>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mn>2</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <apply>
     <minus></minus>
     <ci>y</ci>
     <apply>
      <times></times>
      <apply>
       <ci>normal-^</ci>
       <ci>f</ci>
      </apply>
      <ci>x</ci>
     </apply>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (y-\hat{f}(x))^{2}
  </annotation>
 </semantics>
</math>

 to be minimal, both for 

<math display="inline" id="Bias–variance_tradeoff:11">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>x</mi>
    <mi>n</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>n</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{1},\dots,x_{n}
  </annotation>
 </semantics>
</math>

 <em>and for points outside of our sample</em>. Of course, we cannot hope to do so perfectly, since the 

<math display="inline" id="Bias–variance_tradeoff:12">
 <semantics>
  <msub>
   <mi>y</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>y</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{i}
  </annotation>
 </semantics>
</math>

 contain noise 

<math display="inline" id="Bias–variance_tradeoff:13">
 <semantics>
  <mi>ϵ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ϵ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \epsilon
  </annotation>
 </semantics>
</math>

; this means we must be prepared to accept an <em>irreducible error</em> in any function we come up with.</p>

<p>Finding an 

<math display="inline" id="Bias–variance_tradeoff:14">
 <semantics>
  <mover accent="true">
   <mi>f</mi>
   <mo stretchy="false">^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>f</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{f}
  </annotation>
 </semantics>
</math>

 that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function 

<math display="inline" id="Bias–variance_tradeoff:15">
 <semantics>
  <mover accent="true">
   <mi>f</mi>
   <mo stretchy="false">^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>f</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{f}
  </annotation>
 </semantics>
</math>

 we select, we can decompose its <a href="expected_value" title="wikilink">expected</a> error on an unseen sample 

<math display="inline" id="Bias–variance_tradeoff:16">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 as follows:<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>

<math display="inline" id="Bias–variance_tradeoff:17">
 <semantics>
  <mrow>
   <mi mathvariant="normal">E</mi>
   <mrow>
    <mo maxsize="160%" minsize="160%">[</mo>
    <msup>
     <mrow>
      <mo maxsize="120%" minsize="120%">(</mo>
      <mrow>
       <mi>y</mi>
       <mo>-</mo>
       <mrow>
        <mover accent="true">
         <mi>f</mi>
         <mo stretchy="false">^</mo>
        </mover>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo maxsize="120%" minsize="120%">)</mo>
     </mrow>
     <mn>2</mn>
    </msup>
    <mo maxsize="160%" minsize="160%">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-E</ci>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <minus></minus>
       <ci>y</ci>
       <apply>
        <times></times>
        <apply>
         <ci>normal-^</ci>
         <ci>f</ci>
        </apply>
        <ci>x</ci>
       </apply>
      </apply>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\mathrm{E}\Big[\big(y-\hat{f}(x)\big)^{2}\Big]
  </annotation>
 </semantics>
</math>


</p>

<p>Where:</p>

<p>

<math display="inline" id="Bias–variance_tradeoff:18">
 <semantics>
  <mrow>
   <mrow>
    <mi>Bias</mi>
    <mrow>
     <mo maxsize="120%" minsize="120%">[</mo>
     <mrow>
      <mover accent="true">
       <mi>f</mi>
       <mo stretchy="false">^</mo>
      </mover>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo maxsize="120%" minsize="120%">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi mathvariant="normal">E</mi>
     <mrow>
      <mo maxsize="120%" minsize="120%">[</mo>
      <mrow>
       <mover accent="true">
        <mi>f</mi>
        <mo stretchy="false">^</mo>
       </mover>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo maxsize="120%" minsize="120%">]</mo>
     </mrow>
    </mrow>
    <mo>-</mo>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>Bias</ci>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <apply>
       <times></times>
       <apply>
        <ci>normal-^</ci>
        <ci>f</ci>
       </apply>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <ci>normal-E</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <times></times>
        <apply>
         <ci>normal-^</ci>
         <ci>f</ci>
        </apply>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>f</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\mathrm{Bias}\big[\hat{f}(x)\big]=\mathrm{E}\big[\hat{f}(x)\big]-%
f(x)
  </annotation>
 </semantics>
</math>


</p>

<p>and</p>

<p>

<math display="inline" id="Bias–variance_tradeoff:19">
 <semantics>
  <mrow>
   <mrow>
    <mi>Var</mi>
    <mrow>
     <mo maxsize="120%" minsize="120%">[</mo>
     <mrow>
      <mover accent="true">
       <mi>f</mi>
       <mo stretchy="false">^</mo>
      </mover>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo maxsize="120%" minsize="120%">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi mathvariant="normal">E</mi>
    <mrow>
     <mo maxsize="160%" minsize="160%">[</mo>
     <msup>
      <mrow>
       <mo maxsize="120%" minsize="120%">(</mo>
       <mrow>
        <mrow>
         <mover accent="true">
          <mi>f</mi>
          <mo stretchy="false">^</mo>
         </mover>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>x</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>-</mo>
        <mrow>
         <mi mathvariant="normal">E</mi>
         <mrow>
          <mo stretchy="false">[</mo>
          <mrow>
           <mover accent="true">
            <mi>f</mi>
            <mo stretchy="false">^</mo>
           </mover>
           <mrow>
            <mo stretchy="false">(</mo>
            <mi>x</mi>
            <mo stretchy="false">)</mo>
           </mrow>
          </mrow>
          <mo stretchy="false">]</mo>
         </mrow>
        </mrow>
       </mrow>
       <mo maxsize="120%" minsize="120%">)</mo>
      </mrow>
      <mn>2</mn>
     </msup>
     <mo maxsize="160%" minsize="160%">]</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>Var</ci>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <apply>
       <times></times>
       <apply>
        <ci>normal-^</ci>
        <ci>f</ci>
       </apply>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>normal-E</ci>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <apply>
         <times></times>
         <apply>
          <ci>normal-^</ci>
          <ci>f</ci>
         </apply>
         <ci>x</ci>
        </apply>
        <apply>
         <times></times>
         <ci>normal-E</ci>
         <apply>
          <csymbol cd="latexml">delimited-[]</csymbol>
          <apply>
           <times></times>
           <apply>
            <ci>normal-^</ci>
            <ci>f</ci>
           </apply>
           <ci>x</ci>
          </apply>
         </apply>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\mathrm{Var}\big[\hat{f}(x)\big]=\mathrm{E}\Big[\big(\hat{f}(x)-%
\mathrm{E}[\hat{f}(x)]\big)^{2}\Big]
  </annotation>
 </semantics>
</math>


</p>

<p>The expectation ranges over different choices of the training set 

<math display="inline" id="Bias–variance_tradeoff:20">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>x</mi>
    <mi>n</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>y</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>y</mi>
    <mi>n</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>n</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <ci>n</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{1},\dots,x_{n},y_{1},\dots,y_{n}
  </annotation>
 </semantics>
</math>

, all sampled from the same distribution. The three terms represent:</p>
<ul>
<li>the square of the <em>bias</em> of the learning method, which can be thought of the error caused by the simplifying assumptions built into the method. E.g., when approximating a non-linear function 

<math display="inline" id="Bias–variance_tradeoff:21">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)
  </annotation>
 </semantics>
</math>

 using a learning method for <a href="linear_model" title="wikilink">linear models</a>, there will be error in the estimates 

<math display="inline" id="Bias–variance_tradeoff:22">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>f</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <ci>normal-^</ci>
     <ci>f</ci>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{f}(x)
  </annotation>
 </semantics>
</math>

 due to this assumption;</li>
<li>the <em>variance</em> of the learning method, or, intuitively, how much the learning method 

<math display="inline" id="Bias–variance_tradeoff:23">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>f</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <ci>normal-^</ci>
     <ci>f</ci>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{f}(x)
  </annotation>
 </semantics>
</math>

 will move around its mean;</li>
<li>the irreducible error 

<math display="inline" id="Bias–variance_tradeoff:24">
 <semantics>
  <msup>
   <mi>σ</mi>
   <mn>2</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>σ</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma^{2}
  </annotation>
 </semantics>
</math>

. Since all three terms are non-negative, this forms a lower bound on the expected error on unseen samples.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></li>
</ul>

<p>The more complex the model 

<math display="inline" id="Bias–variance_tradeoff:25">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>f</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <ci>normal-^</ci>
     <ci>f</ci>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{f}(x)
  </annotation>
 </semantics>
</math>

 is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model "move" more to capture the data points, and hence its variance will be larger.</p>
<h3 id="derivation">Derivation</h3>

<p>The derivation of the bias–variance decomposition for squared error proceeds as follows.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> For notational convenience, abbreviate 

<math display="inline" id="Bias–variance_tradeoff:26">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>=</mo>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>f</ci>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f=f(x)
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Bias–variance_tradeoff:27">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>f</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mo>=</mo>
   <mrow>
    <mover accent="true">
     <mi>f</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>normal-^</ci>
     <ci>f</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <ci>normal-^</ci>
      <ci>f</ci>
     </apply>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{f}=\hat{f}(x)
  </annotation>
 </semantics>
</math>

. First, note that for any random variable 

<math display="inline" id="Bias–variance_tradeoff:28">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

, we have</p>

<p>

<math display="inline" id="Bias–variance_tradeoff:29">
 <semantics>
  <mrow>
   <mi mathvariant="normal">E</mi>
   <mrow>
    <mo stretchy="false">[</mo>
    <msup>
     <mi>X</mi>
     <mn>2</mn>
    </msup>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-E</ci>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>X</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\mathrm{E}[X^{2}]
  </annotation>
 </semantics>
</math>


</p>

<p>Since 

<math display="inline" id="Bias–variance_tradeoff:30">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 is deterministic</p>

<p>

<math display="inline" id="Bias–variance_tradeoff:31">
 <semantics>
  <mrow>
   <mn>0</mn>
   <mo>=</mo>
   <mrow>
    <mi>Var</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mi>f</mi>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi mathvariant="normal">E</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>f</mi>
        <mo>-</mo>
        <mrow>
         <mi mathvariant="normal">E</mi>
         <mrow>
          <mo stretchy="false">[</mo>
          <mi>f</mi>
          <mo stretchy="false">]</mo>
         </mrow>
        </mrow>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mn>2</mn>
     </msup>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>⇒</mo>
   <mrow>
    <mi>f</mi>
    <mo>-</mo>
    <mrow>
     <mi mathvariant="normal">E</mi>
     <mrow>
      <mo stretchy="false">[</mo>
      <mi>f</mi>
      <mo stretchy="false">]</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
   <mo>⇒</mo>
   <mrow>
    <mi mathvariant="normal">E</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mi>f</mi>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mi>f</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <cn type="integer">0</cn>
     <apply>
      <times></times>
      <ci>Var</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <ci>f</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>normal-E</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <minus></minus>
         <ci>f</ci>
         <apply>
          <times></times>
          <ci>normal-E</ci>
          <apply>
           <csymbol cd="latexml">delimited-[]</csymbol>
           <ci>f</ci>
          </apply>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <ci>normal-⇒</ci>
     <share href="#.cmml">
     </share>
     <apply>
      <minus></minus>
      <ci>f</ci>
      <apply>
       <times></times>
       <ci>normal-E</ci>
       <apply>
        <csymbol cd="latexml">delimited-[]</csymbol>
        <ci>f</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <cn type="integer">0</cn>
    </apply>
    <apply>
     <ci>normal-⇒</ci>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>normal-E</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <ci>f</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <ci>f</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle 0=\mathrm{Var}[f]=\mathrm{E}[(f-\mathrm{E}[f])^{2}]\Rightarrow f%
-\mathrm{E}[f]=0\Rightarrow\mathrm{E}[f]=f
  </annotation>
 </semantics>
</math>


.</p>

<p>This, given 

<math display="inline" id="Bias–variance_tradeoff:32">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mrow>
    <mi>f</mi>
    <mo>+</mo>
    <mi>ϵ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <apply>
     <plus></plus>
     <ci>f</ci>
     <ci>ϵ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=f+\epsilon
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Bias–variance_tradeoff:33">
 <semantics>
  <mrow>
   <mrow>
    <mi mathvariant="normal">E</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mi>ϵ</mi>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>normal-E</ci>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <ci>ϵ</ci>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{E}[\epsilon]=0
  </annotation>
 </semantics>
</math>

, implies 

<math display="inline" id="Bias–variance_tradeoff:34">
 <semantics>
  <mrow>
   <mrow>
    <mi mathvariant="normal">E</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mi>y</mi>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi mathvariant="normal">E</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mrow>
      <mi>f</mi>
      <mo>+</mo>
      <mi>ϵ</mi>
     </mrow>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi mathvariant="normal">E</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mi>f</mi>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mi>f</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <ci>normal-E</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <ci>y</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>normal-E</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <plus></plus>
        <ci>f</ci>
        <ci>ϵ</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>normal-E</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <ci>f</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <ci>f</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{E}[y]=\mathrm{E}[f+\epsilon]=\mathrm{E}[f]=f
  </annotation>
 </semantics>
</math>

.</p>

<p>Also, since 

<math display="inline" id="Bias–variance_tradeoff:35">
 <semantics>
  <mrow>
   <mrow>
    <mi>Var</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mi>ϵ</mi>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mi>σ</mi>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>Var</ci>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <ci>ϵ</ci>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>σ</ci>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{Var}[\epsilon]=\sigma^{2}
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="inline" id="Bias–variance_tradeoff:36">
 <semantics>
  <mrow>
   <mrow>
    <mi>Var</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mi>y</mi>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi mathvariant="normal">E</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>y</mi>
        <mo>-</mo>
        <mrow>
         <mi mathvariant="normal">E</mi>
         <mrow>
          <mo stretchy="false">[</mo>
          <mi>y</mi>
          <mo stretchy="false">]</mo>
         </mrow>
        </mrow>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mn>2</mn>
     </msup>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi mathvariant="normal">E</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>y</mi>
        <mo>-</mo>
        <mi>f</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mn>2</mn>
     </msup>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi mathvariant="normal">E</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mrow>
         <mi>f</mi>
         <mo>+</mo>
         <mi>ϵ</mi>
        </mrow>
        <mo>-</mo>
        <mi>f</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mn>2</mn>
     </msup>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi mathvariant="normal">E</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <msup>
      <mi>ϵ</mi>
      <mn>2</mn>
     </msup>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi>Var</mi>
     <mrow>
      <mo stretchy="false">[</mo>
      <mi>ϵ</mi>
      <mo stretchy="false">]</mo>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi mathvariant="normal">E</mi>
     <msup>
      <mrow>
       <mo stretchy="false">[</mo>
       <mi>ϵ</mi>
       <mo stretchy="false">]</mo>
      </mrow>
      <mn>2</mn>
     </msup>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mi>σ</mi>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <ci>Var</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <ci>y</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>normal-E</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <minus></minus>
         <ci>y</ci>
         <apply>
          <times></times>
          <ci>normal-E</ci>
          <apply>
           <csymbol cd="latexml">delimited-[]</csymbol>
           <ci>y</ci>
          </apply>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>normal-E</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <minus></minus>
         <ci>y</ci>
         <ci>f</ci>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>normal-E</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <minus></minus>
         <apply>
          <plus></plus>
          <ci>f</ci>
          <ci>ϵ</ci>
         </apply>
         <ci>f</ci>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>normal-E</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>ϵ</ci>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <plus></plus>
      <apply>
       <times></times>
       <ci>Var</ci>
       <apply>
        <csymbol cd="latexml">delimited-[]</csymbol>
        <ci>ϵ</ci>
       </apply>
      </apply>
      <apply>
       <times></times>
       <ci>normal-E</ci>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="latexml">delimited-[]</csymbol>
         <ci>ϵ</ci>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>σ</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\mathrm{Var}[y]=\mathrm{E}[(y-\mathrm{E}[y])^{2}]=\mathrm{E}[(y-f%
)^{2}]=\mathrm{E}[(f+\epsilon-f)^{2}]=\mathrm{E}[\epsilon^{2}]=\mathrm{Var}[%
\epsilon]+\mathrm{E}[\epsilon]^{2}=\sigma^{2}
  </annotation>
 </semantics>
</math>


</p>

<p>Thus, since 

<math display="inline" id="Bias–variance_tradeoff:37">
 <semantics>
  <mi>ϵ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ϵ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \epsilon
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Bias–variance_tradeoff:38">
 <semantics>
  <mover accent="true">
   <mi>f</mi>
   <mo stretchy="false">^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>f</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{f}
  </annotation>
 </semantics>
</math>

 are independent, we can write</p>

<p>

<math display="inline" id="Bias–variance_tradeoff:39">
 <semantics>
  <mrow>
   <mi mathvariant="normal">E</mi>
   <mrow>
    <mo maxsize="120%" minsize="120%">[</mo>
    <msup>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>y</mi>
       <mo>-</mo>
       <mover accent="true">
        <mi>f</mi>
        <mo stretchy="false">^</mo>
       </mover>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mn>2</mn>
    </msup>
    <mo maxsize="120%" minsize="120%">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-E</ci>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <minus></minus>
       <ci>y</ci>
       <apply>
        <ci>normal-^</ci>
        <ci>f</ci>
       </apply>
      </apply>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\mathrm{E}\big[(y-\hat{f})^{2}\big]
  </annotation>
 </semantics>
</math>


</p>

<p><a class="uri" href="Q.E.D." title="wikilink">Q.E.D.</a></p>
<h2 id="application-to-classification">Application to classification</h2>

<p>The bias–variance decomposition was originally formulated for least-squares regression. For the case of <a href="statistical_classification" title="wikilink">classification</a> under the 0-1 loss (misclassification rate), it's possible to find a similar decomposition.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> Alternatively, if the classification problem can be phrased as <a href="probabilistic_classification" title="wikilink">probabilistic classification</a>, then the expected squared error of the predicted probabilities with respect to the true probabilities can be decomposed as before.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h2 id="approaches">Approaches</h2>

<p><a href="Dimensionality_reduction" title="wikilink">Dimensionality reduction</a> and <a href="feature_selection" title="wikilink">feature selection</a> can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance, e.g.:</p>
<ul>
<li>(<a href="Generalized_linear_model" title="wikilink">Generalized</a>) linear models can be <a href="Regularization_(mathematics)" title="wikilink">regularized</a> to decrease their variance at the cost of increasing their bias <a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></li>
<li>In <a href="artificial_neural_network" title="wikilink">artificial neural networks</a>, the variance increases and the bias decreases with the number of hidden units.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> Like in GLMs, regularization is typically applied.</li>
<li>In <a href="k-nearest_neighbor" title="wikilink">k-nearest neighbor</a> models, a high value of 

<math display="inline" id="Bias–variance_tradeoff:40">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

 leads to high bias and low variance (see below).</li>
<li>In <a href="Instance-based_learning" title="wikilink">Instance-based learning</a>, regularization can be achieved varying the mixture of <a href="prototype" title="wikilink">prototypes</a> and <a href="exemplar" title="wikilink">exemplars</a>.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></li>
<li>In <a href="decision_tree" title="wikilink">decision trees</a>, the depth of the tree determines the variance. Decision trees are commonly pruned to control variance.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></li>
</ul>

<p>One way of resolving the trade-off is to use <a href="mixture_models" title="wikilink">mixture models</a> and <a href="ensemble_learning" title="wikilink">ensemble learning</a>.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a><a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> For example, <a href="Boosting_(machine_learning)" title="wikilink">boosting</a> combines many "weak" (high bias) models in an ensemble that has lower bias than the individual models, while <a href="Bootstrap_aggregating" title="wikilink">bagging</a> combines "strong" learners in a way that reduces their variance.</p>
<h3 id="k-nearest-neighbors">K-nearest neighbors</h3>

<p>In the case of <a href="k-nearest_neighbors_algorithm" title="wikilink">

<math display="inline" id="Bias–variance_tradeoff:41">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

-nearest neighbors regression</a>, a <a href="closed-form_expression" title="wikilink">closed-form expression</a> exists that relates the bias–variance decomposition to the parameter 

<math display="inline" id="Bias–variance_tradeoff:42">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

:<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></p>

<p>

<math display="block" id="Bias–variance_tradeoff:43">
 <semantics>
  <mrow>
   <mrow>
    <mi mathvariant="normal">E</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>y</mi>
        <mo>-</mo>
        <mrow>
         <mover accent="true">
          <mi>f</mi>
          <mo stretchy="false">^</mo>
         </mover>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>x</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mn>2</mn>
     </msup>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msup>
     <mrow>
      <mo>(</mo>
      <mrow>
       <mrow>
        <mi>f</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>-</mo>
       <mrow>
        <mfrac>
         <mn>1</mn>
         <mi>k</mi>
        </mfrac>
        <mrow>
         <munderover>
          <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
          <mrow>
           <mi>i</mi>
           <mo>=</mo>
           <mn>1</mn>
          </mrow>
          <mi>k</mi>
         </munderover>
         <mrow>
          <mi>f</mi>
          <mrow>
           <mo stretchy="false">(</mo>
           <mrow>
            <msub>
             <mi>N</mi>
             <mi>i</mi>
            </msub>
            <mrow>
             <mo stretchy="false">(</mo>
             <mi>x</mi>
             <mo stretchy="false">)</mo>
            </mrow>
           </mrow>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
        </mrow>
       </mrow>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mn>2</mn>
    </msup>
    <mo>+</mo>
    <mfrac>
     <msup>
      <mi>σ</mi>
      <mn>2</mn>
     </msup>
     <mi>k</mi>
    </mfrac>
    <mo>+</mo>
    <msup>
     <mi>σ</mi>
     <mn>2</mn>
    </msup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>normal-E</ci>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <ci>y</ci>
        <apply>
         <times></times>
         <apply>
          <ci>normal-^</ci>
          <ci>f</ci>
         </apply>
         <ci>x</ci>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <minus></minus>
       <apply>
        <times></times>
        <ci>f</ci>
        <ci>x</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <divide></divide>
         <cn type="integer">1</cn>
         <ci>k</ci>
        </apply>
        <apply>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <sum></sum>
           <apply>
            <eq></eq>
            <ci>i</ci>
            <cn type="integer">1</cn>
           </apply>
          </apply>
          <ci>k</ci>
         </apply>
         <apply>
          <times></times>
          <ci>f</ci>
          <apply>
           <times></times>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>N</ci>
            <ci>i</ci>
           </apply>
           <ci>x</ci>
          </apply>
         </apply>
        </apply>
       </apply>
      </apply>
      <cn type="integer">2</cn>
     </apply>
     <apply>
      <divide></divide>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>σ</ci>
       <cn type="integer">2</cn>
      </apply>
      <ci>k</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>σ</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{E}[(y-\hat{f}(x))^{2}]=\left(f(x)-\frac{1}{k}\sum_{i=1}^{k}f(N_{i}(x))%
\right)^{2}+\frac{\sigma^{2}}{k}+\sigma^{2}
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Bias–variance_tradeoff:44">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>N</mi>
     <mn>1</mn>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <mrow>
    <msub>
     <mi>N</mi>
     <mi>k</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>N</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>x</ci>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>N</ci>
      <ci>k</ci>
     </apply>
     <ci>x</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N_{1}(x),\dots,N_{k}(x)
  </annotation>
 </semantics>
</math>

 are the 

<math display="inline" id="Bias–variance_tradeoff:45">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

 nearest neighbors of 

<math display="inline" id="Bias–variance_tradeoff:46">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 in the training set. The bias (first term) is a monotone rising function of 

<math display="inline" id="Bias–variance_tradeoff:47">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

, while the variance (second term) drops off as 

<math display="inline" id="Bias–variance_tradeoff:48">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

 is increased. In fact, under "reasonable assumptions" the bias of the first-nearest neighbor (1-NN) estimator vanishes entirely as the size of the training set approaches infinity.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a></p>
<h2 id="application-to-human-learning">Application to human learning</h2>

<p>While widely discussed in the context of machine learning, the bias-variance dilemma has been examined in the context of <a href="Cognitive_science" title="wikilink">human cognition</a>, most notably by <a href="Gerd_Gigerenzer" title="wikilink">Gerd Gigerenzer</a> and co-workers in the context of learned heuristics. They have argued (see references below) that the human brain resolves the dilemma in the case of the typically sparse, poorly-characterised training-sets provided by experience by adopting high-bias/low variance heuristics. This reflects the fact that a zero-bias approach has poor generalisability to new situations, and also unreasonably presumes precise knowledge of the true state of the world. The resulting heuristics are relatively simple, but produce better inferences in a wider variety of situations.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a></p>

<p>Geman et al.<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> argue that that the bias-variance dilemma implies that abilities such as generic object recognition cannot be learned from scratch, but require a certain degree of “hard wiring” that is later tuned by experience. This is because model-free approaches to inference require impractically large training sets if they are to avoid high variance.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Bias_of_an_estimator" title="wikilink">Bias of an estimator</a></li>
<li><a href="Gauss–Markov_theorem" title="wikilink">Gauss–Markov theorem</a></li>
<li><a href="Hyperparameter_optimization" title="wikilink">Hyperparameter optimization</a></li>
<li><a href="Minimum-variance_unbiased_estimator" title="wikilink">Minimum-variance unbiased estimator</a></li>
<li><a href="Model_selection" title="wikilink">Model selection</a></li>
<li><a href="Regression_model_validation" title="wikilink">Regression model validation</a></li>
<li><a href="Supervised_learning" title="wikilink">Supervised learning</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a class="uri" href="http://scott.fortmann-roe.com/docs/BiasVariance.html">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></li>
</ul>

<p>"</p>

<p><a class="uri" href="Category:Dilemmas" title="wikilink">Category:Dilemmas</a> <a href="Category:Model_selection" title="wikilink">Category:Model selection</a> <a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a> <a href="Category:Statistical_classification" title="wikilink">Category:Statistical classification</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2">Bias–variance decomposition, In Encyclopedia of Machine Learning. Eds. Claude Sammut, Geoffrey I. Webb. Springer 2011. pp. 100-101<a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"><a href="#fnref11">↩</a></li>
<li id="fn12"></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"></li>
<li id="fn15">Jo-Anne Ting, Sethu Vijaykumar, Stefan Schaal, Locally Weighted Regression for Control. In Encyclopedia of Machine Learning. Eds. Claude Sammut, Geoffrey I. Webb. Springer 2011. p. 615<a href="#fnref15">↩</a></li>
<li id="fn16">Scott Fortmann-Roe. Understanding the Bias–Variance Tradeoff. 2012. <a class="uri" href="http://scott.fortmann-roe.com/docs/BiasVariance.html">http://scott.fortmann-roe.com/docs/BiasVariance.html</a><a href="#fnref16">↩</a></li>
<li id="fn17"></li>
<li id="fn18"></li>
<li id="fn19"><a href="#fnref19">↩</a></li>
<li id="fn20"></li>
</ol>
</section>
</body>
</html>
