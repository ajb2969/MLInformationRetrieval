   Volterra series      Volterra series   The Volterra series is a model for non-linear behavior similar to the Taylor series . It differs from the Taylor series in its ability to capture 'memory' effects. The Taylor series can be used for approximating the response of a nonlinear system to a given input if the output of this system depends strictly on the input at that particular time. In the Volterra series the output of the nonlinear system depends on the input to the system at all other times. This provides the ability to capture the 'memory' effect of devices like capacitors and inductors.  It has been applied in the fields of medicine (biomedical engineering) and biology, especially neuroscience. It is also used in electrical engineering to model intermodulation distortion in many devices including power amplifiers and frequency mixers . Its main advantage lies in its generality: it can represent a wide range of systems. Thus it is sometimes considered a non-parametric model.  In mathematics , a Volterra series denotes a functional expansion of a dynamic, nonlinear , time-invariant functional . Volterra series are frequently used in system identification . The Volterra series, which is used to prove the Volterra theorem, is an infinite sum of multidimensional convolutional integrals.  History  The Volterra series is a modernized version of the theory of analytic functionals due to the Italian mathematician Vito Volterra in work dating from 1887. 1  Norbert Wiener became interested in this theory in the 1920s from contact with Volterra's student Paul Lévy . He applied his theory of Brownian motion to the integration of Volterra analytic functionals. The use of Volterra series for system analysis originated from a restricted 1942 wartime report 2 of Wiener, then professor of mathematics at MIT . It used the series to make an approximate analysis of the effect of radar noise in a nonlinear receiver circuit. The report became public after the war. 3 As a general method of analysis of nonlinear systems, Volterra series came into use after about 1957 as the result of a series of reports, at first privately circulated, from MIT and elsewhere. 4 The name Volterra series came into use a few years later.  Mathematical theory  The theory of Volterra series can be viewed from two different perspectives: either one considers an operator mapping between two real (or complex) function spaces or a functional mapping from a real (or complex) function space into the real (or complex) numbers. The latter, functional perspective is in more frequent use, due to the assumed time-invariance of the system.  Continuous time  A continuous time-invariant system with x ( t ) as input and y ( t ) as output can be expanded in Volterra series as:      y   (  t  )   =   h  0   +   ∑   n  =  1   N    ∫  a  b   ⋯   ∫  a  b    h  n    (   τ  1   ,  .  .  ,   τ  n   )    ∏   j  =  1   n   x   (  t  -   τ  j   )   d   τ  j      fragments  y   fragments  normal-(  t  normal-)     subscript  h  0     superscript   subscript     n  1    N    superscript   subscript   a   b   normal-⋯   superscript   subscript   a   b    subscript  h  n    fragments  normal-(   subscript  τ  1   normal-,  normal-.  normal-.  normal-,   subscript  τ  n   normal-)    subscript   superscript  product  n     j  1    x   fragments  normal-(  t    subscript  τ  j   normal-)   d   subscript  τ  j     y(t)=h_{0}+\sum_{n=1}^{N}{\int_{a}^{b}\cdots\int_{a}^{b}{h_{n}(\tau_{1},.\,.\,%
 ,\tau_{n})\prod^{n}_{j=1}{x(t-\tau_{j})d\tau_{j}}}}     Here the constant term    h  0     subscript  h  0    h_{0}   on the right hand side is usually taken to be zero by suitable choice of output level   y   y   y   . The function     h  n    (   τ  1   ,  .  .  ,   τ  n   )      fragments   subscript  h  n    fragments  normal-(   subscript  τ  1   normal-,  normal-.  normal-.  normal-,   subscript  τ  n   normal-)     h_{n}(\tau_{1},.\,.\,,\tau_{n})   is called the n -th order Volterra kernel . It can be regarded as a higher-order impulse response of the system. The integrals are usually taken over an infinite range. Since normally the output depends only on past values of the input, the integrals may in this case equivalently be taken over a half-infinite range.  If N is finite, the series is said to be truncated . If a,b , and N are finite, the series is called doubly-finite .  Sometimes the n -th order term is divided by n!, a convention which is convenient when taking the output of one Volterra system as the input of another ('cascading').  The causality condition : Since in any physically realizable system the output can only depend on previous values of the input, the kernels     h  n    (   t  1   ,   t  2   ,  …  ,   t  n   )        subscript  h  n     subscript  t  1    subscript  t  2   normal-…   subscript  t  n      h_{n}(t_{1},t_{2},\ldots,t_{n})   will be zero if any of the variables     t  1   ,   t  2   ,  …  ,   t  n       subscript  t  1    subscript  t  2   normal-…   subscript  t  n     t_{1},t_{2},\ldots,t_{n}   are negative. The integrals may then be written over the half range from zero to infinity. So if the operator is causal,    a  ≥  0      a  0    a\geq 0   .  Fréchet's approximation theorem : The use of the Volterra series to represent a time-invariant functional relation is often justified by appealing to a theorem due to Fréchet . This theorem states that a time-invariant functional relation (satisfying certain very general conditions) can be approximated uniformly and to an arbitrary degree of precision by a sufficiently high finite order Volterra series. Among other conditions, the set of admissible input functions    x   (  t  )       x  t    x(t)   for which the approximation will hold is required to be compact . It is usually taken to be an equicontinuous , uniformly bounded set of functions, which is compact by the Arzelà–Ascoli theorem . In many physical situations, this assumption about the input set is a reasonable one. The theorem, however, gives no indication as to how many terms are needed for a good approximation, which is an essential question in applications.  Discrete time  This is similar to the continuous-time case:      y   (  n  )   =   h  0   +   ∑   p  =  1   P    ∑    τ  1   =  a   b   ⋯   ∑    τ  p   =  a   b    h  p    (   τ  1   ,  .  .  ,   τ  p   )    ∏   j  =  1   p   x   (  n  -   τ  j   )   ,     fragments  y   fragments  normal-(  n  normal-)     subscript  h  0     superscript   subscript     p  1    P    superscript   subscript      subscript  τ  1   a    b   normal-⋯   superscript   subscript      subscript  τ  p   a    b    subscript  h  p    fragments  normal-(   subscript  τ  1   normal-,  normal-.  normal-.  normal-,   subscript  τ  p   normal-)    subscript   superscript  product  p     j  1    x   fragments  normal-(  n    subscript  τ  j   normal-)   normal-,    y(n)=h_{0}+\sum_{p=1}^{P}{\sum_{\tau_{1}=a}^{b}\cdots\sum_{\tau_{p}=a}^{b}{h_{%
 p}(\tau_{1},.\,.\,,\tau_{p})\prod^{p}_{j=1}{x(n-\tau_{j})}}},        h  p    (   τ  1   ,  .  .  ,   τ  p   )      fragments   subscript  h  p    fragments  normal-(   subscript  τ  1   normal-,  normal-.  normal-.  normal-,   subscript  τ  p   normal-)     h_{p}(\tau_{1},.\,.\,,\tau_{p})   are called discrete-time Volterra kernels.  If P is finite, the series operator is said to be truncated. If a,b and P are finite the series operator is called doubly-finite Volterra series. If    a  ≥  0      a  0    a\geq 0   the operator is said to be causal .  We can always consider, without loss of the generality, the kernel     h  p    (   τ  1   ,  .  .  ,   τ  p   )      fragments   subscript  h  p    fragments  normal-(   subscript  τ  1   normal-,  normal-.  normal-.  normal-,   subscript  τ  p   normal-)     h_{p}(\tau_{1},.\,.\,,\tau_{p})   as symmetrical. In fact, for the commutativity of the multiplication it is always possible to symmetrize it by forming a new kernel taken as the average of the kernels for all permutations of the variables     τ  1   ,  .  .  ,   τ  p      fragments   subscript  τ  1   normal-,  normal-.  normal-.  normal-,   subscript  τ  p     \tau_{1},.\,.\,,\tau_{p}   .  For a causal system with symmetrical kernels we can rewrite the nth term approximately in triangular form     ∑    τ  1   =  0   M    ∑    τ  2   =   τ  1    M   ⋯   ∑    τ  p   =   τ   p  -  1     M    h  p    (   τ  1   ,  .  .  ,   τ  p   )    ∏   j  =  1   p   x   (  n  -   τ  j   )   .     fragments   superscript   subscript      subscript  τ  1   0    M    superscript   subscript      subscript  τ  2    subscript  τ  1     M   normal-⋯   superscript   subscript      subscript  τ  p    subscript  τ    p  1      M    subscript  h  p    fragments  normal-(   subscript  τ  1   normal-,  normal-.  normal-.  normal-,   subscript  τ  p   normal-)    subscript   superscript  product  p     j  1    x   fragments  normal-(  n    subscript  τ  j   normal-)   normal-.    \sum_{\tau_{1}=0}^{M}\sum_{\tau_{2}=\tau_{1}}^{M}\cdots\sum_{\tau_{p}=\tau_{p-%
 1}}^{M}{h_{p}(\tau_{1},.\,.\,,\tau_{p})\prod^{p}_{j=1}{x(n-\tau_{j})}}.     Methods to estimate the kernel coefficients  Estimating the Volterra coefficients individually is complicated since the basis functionals of the Volterra series are correlated. This leads to the problem of simultaneously solving a set of integral-equations for the coefficients. Hence, estimation of Volterra coefficients is generally performed by estimating the coefficients of an orthogonalized series, e.g. the Wiener series , and then recomputing the coefficients of the original Volterra series. The Volterra series main appeal over the orthogonalized series lies in its intuitive, canonical structure, i.e. all interactions of the input have one fixed degree. The orthogonalized basis functionals will generally be quite complicated.  An important aspect, with respect to which the following methods differ is whether the orthogonalization of the basis functionals is to be performed over the idealized specification of the input signal (e.g. gaussian, white noise) or over the actual realization of the input (i.e. the pseudo-random, bounded, almost-white version of gaussian white noise, or any other stimulus). The latter methods, despite their lack of mathematical elegance, have been shown to be more flexible (as arbitrary inputs can be easily accommodated) and precise (due to the effect that the idealized version of the input signal is not always realizable).  Crosscorrelation method  This method, developed by Lee & Schetzen, orthogonalizes with respect to the actual mathematical description of the signal, i.e. the projection onto the new basis functionals is based on the knowledge of the moments of the random signal.  To allow identification orthogonalization, Volterra series must be rearranged in terms of orthogonal non-homogeneous G operators ( Wiener series ):       y   (  n  )    =    ∑  p     H  p   x   (  n  )     ≡    ∑  p     G  p   x   (  n  )             y  n     subscript   p      subscript  H  p   x  n           subscript   p      subscript  G  p   x  n       y(n)=\sum_{p}{H_{p}x(n)}\equiv\sum_{p}{G_{p}x(n)}     The G operators can be defined by the following        E   {    H  i   x   (  n  )    G  j   x   (  n  )    }    =  0   ;   i  <  j      formulae-sequence      E      subscript  H  i   x  n   subscript  G  j   x  n     0     i  j     E\{H_{i}x(n)G_{j}x(n)\}=0;\qquad i         E   {    G  i   x   (  n  )    G  j   x   (  n  )    }    =  0   ;   i  ≠  j      formulae-sequence      E      subscript  G  i   x  n   subscript  G  j   x  n     0     i  j     E\{G_{i}x(n)G_{j}x(n)\}=0;\qquad i\neq j     whenever     H  i   x   (  n  )        subscript  H  i   x  n    H_{i}x(n)   is arbitrary omogeneous Volterra, x(n) is a Stationary white noise with zero mean and variance A .  Recalling that every Volterra functional is orthogonal to all Wiener functional of greater order, and considering the following Volterra functional        H   p  ¯   *   x   (  n  )    =    ∏   j  =  1    p  ¯     x   (   n  -   τ  j    )            subscript   superscript  H     normal-¯  p    x  n     subscript   superscript  product   normal-¯  p      j  1      x    n   subscript  τ  j        H^{*}_{\overline{p}}x(n)=\prod^{\overline{p}}_{j=1}{x(n-\tau_{j})}     we can write       E   {   y   (  n  )    H   p  ¯   *   x   (  n  )    }    =   E   {    ∑   p  =  0   ∞     G  p   x   (  n  )    H   p  ¯   *   x   (  n  )     }          E     y  n   subscript   superscript  H     normal-¯  p    x  n       E     superscript   subscript     p  0         subscript  G  p   x  n   subscript   superscript  H     normal-¯  p    x  n        E\left\{y(n)H^{*}_{\overline{p}}x(n)\right\}=E\left\{\sum_{p=0}^{\infty}{G_{p}%
 x(n)H^{*}_{\overline{p}}x(n)}\right\}     If x is SWN,     τ  1   ≠   τ  2   ≠  …  ≠   τ  P          subscript  τ  1    subscript  τ  2        normal-…        subscript  τ  P      \tau_{1}\neq\tau_{2}\neq\ldots\neq\tau_{P}   and by letting    A  =   σ  x  2       A   subscript   superscript  σ  2   x     A=\sigma^{2}_{x}   , we have:      E   {  y   (  n  )    ∏   j  =  1    p  ¯    x   (  n  -   τ  j   )   }   =  E   {   G   p  ¯    x   (  n  )    ∏   j  =  1    p  ¯    x   (  n  -   τ  j   )   }   =   p  ¯   !   A   p  ¯     k   p  ¯     (   τ  1   ,  .  .  ,   τ   p  ¯    )      fragments  E   fragments  normal-{  y   fragments  normal-(  n  normal-)    subscript   superscript  product   normal-¯  p      j  1    x   fragments  normal-(  n    subscript  τ  j   normal-)   normal-}    E   fragments  normal-{   subscript  G   normal-¯  p    x   fragments  normal-(  n  normal-)    subscript   superscript  product   normal-¯  p      j  1    x   fragments  normal-(  n    subscript  τ  j   normal-)   normal-}     normal-¯  p     superscript  A   normal-¯  p     subscript  k   normal-¯  p     fragments  normal-(   subscript  τ  1   normal-,  normal-.  normal-.  normal-,   subscript  τ   normal-¯  p    normal-)     E\left\{y(n)\prod^{\overline{p}}_{j=1}{x(n-\tau_{j})}\right\}=E\left\{G_{%
 \overline{p}}x(n)\prod^{\overline{p}}_{j=1}{x(n-\tau_{j})}\right\}=\overline{p%
 }!A^{\overline{p}}k_{\overline{p}}(\tau_{1},.\,.\,,\tau_{\overline{p}})     So if we exclude the diagonal elements,     τ  i   ≠    τ  j   ,   ∀  i   ,  j        subscript  τ  i     subscript  τ  j    for-all  i   j     {\tau_{i}\neq\tau_{j},\,\forall i,j}   , it is       k  p    (   τ  1   ,  .  .  ,   τ  p   )   =    E   {   y   (  n  )   x   (   n  -   τ  1    )   ⋯  x   (   n  -   τ  p    )    }      p  !    A  p     .     fragments   subscript  k  p    fragments  normal-(   subscript  τ  1   normal-,  normal-.  normal-.  normal-,   subscript  τ  p   normal-)        E     y  n  x    n   subscript  τ  1    normal-⋯  x    n   subscript  τ  p           p    superscript  A  p     normal-.    k_{p}(\tau_{1},.\,.\,,\tau_{p})=\frac{E\left\{{y(n)x(n-\tau_{1})\cdots x(n-%
 \tau_{p})}\right\}}{{p!A^{p}}}.     If we want to consider the diagonal points, the solution proposed by Lee and Schetzen is:       k  p    (   τ  1   ,  .  .  ,   τ  p   )   =    E   {    (    y   (  n  )    -      ∑   m  =  0    p  -  1        G  m   x   (  n  )      )   x   (   n  -   τ  1    )   ⋯  x   (   n  -   τ  p    )    }      p  !    A  p        fragments   subscript  k  p    fragments  normal-(   subscript  τ  1   normal-,  normal-.  normal-.  normal-,   subscript  τ  p   normal-)        E         y  n     superscript   subscript     m  0      p  1       subscript  G  m   x  n     x    n   subscript  τ  1    normal-⋯  x    n   subscript  τ  p           p    superscript  A  p       k_{p}(\tau_{1},.\,.\,,\tau_{p})\!=\!\frac{E\left\{{\left({y(n)\!-\!\!\!\sum%
 \limits_{m=0}^{p-1}{\!G_{m}x(n)}}\!\!\right)\!x(n-\tau_{1})\cdots x(n-\tau_{p}%
 )}\right\}}{p!A^{p}}     Efficient formulas and references for diagonal kernel point estimation can be found in  M. Pirani, S. Orcioni, and C. Turchetti,  ``Diagonal kernel point estimation of n-th order discrete Volterra-Wiener systems,''EURASIP Journal on Applied Signal Processing, vol. 2004, no. 12, pp. 1807--1816, Sept. 2004.  and . 5  Advances in Crosscorrelation method  In the traditional orthogonal algorithm, using inputs with high    σ  x     subscript  σ  x    \sigma_{x}   has the advantage of stimulating high order nonlinearity, so as to achieve more accurate high order kernel identification. As a drawback, the use of high    σ  x     subscript  σ  x    \sigma_{x}   values causes high identification error in lower order kernels, as shown in , 6 mainly due to nonideality of the input and truncation errors.  On the contrary the use of lower    σ  x     subscript  σ  x    \sigma_{x}   in the identification process can lead to a better estimation of lower order kernel, but can be insufficient to stimulate high order nonlinearity.  This phenomenon, that can be called locality of truncated Volterra series, can be revealed by calculating the output error of a series as a function of different variances of input. This test can be repeated with series identified with different input variances, obtaining different curves, each with a minimun in correspondence of the variance used in the identification.  To overcome this limitation, a low    σ  x     subscript  σ  x    \sigma_{x}   value should be used for the lower order kernel and gradually increased for higher order kernels. This is not a theoretical problem in Wiener kernel identification, since the Wiener functional are orthogonal to each other, but an appropriate normalization is needed in Wiener to Volterra conversion formulas for taking into account the use of different variances. Furthermore new Wiener to Volterra conversion formulas are needed.  The traditional Wiener kernel identification should be changed as follows: 7      k  0   (  0  )    =   E   {    y   (  0  )     (  n  )    }         superscript   subscript  k  0   0     E      superscript  y  0   n       k_{0}^{(0)}=E\{y^{(0)}(n)\}         k  1   (  1  )     (   τ  1   )    =    1   A  1    E   {    y   (  1  )     (  n  )    x   (  1  )     (   n  -   τ  1    )    }           superscript   subscript  k  1   1    subscript  τ  1        1   subscript  A  1    E      superscript  y  1   n   superscript  x  1     n   subscript  τ  1         k_{1}^{(1)}(\tau_{1})=\frac{1}{A_{1}}E\left\{y^{(1)}(n)\,x^{(1)}(n-\tau_{1})\right\}         k  2   (  2  )     (   τ  1   ,   τ  2   )    =     1    2  !    A  2  2       {    E   {    y   (  2  )     (  n  )     ∏   i  =  1   2     x   (  2  )     (   n  -   τ  i    )      }    -    A  2    k  0   (  2  )     δ    τ  1    τ  2       }           superscript   subscript  k  2   2     subscript  τ  1    subscript  τ  2         1      2    superscript   subscript  A  2   2          E      superscript  y  2   n    superscript   subscript  product    i  1    2      superscript  x  2     n   subscript  τ  i            subscript  A  2    superscript   subscript  k  0   2    subscript  δ     subscript  τ  1    subscript  τ  2           k_{2}^{(2)}(\tau_{1},\tau_{2})=\frac{1}{2!A_{2}^{2}}\,\left\{E\left\{y^{(2)}(n%
 )\,\prod_{i=1}^{2}{x^{(2)}(n-\tau_{i})}\right\}-A_{2}k_{0}^{(2)}\delta_{\tau_{%
 1}\tau_{2}}\right\}         k  3   (  3  )     (   τ  1   ,   τ  2   ,   τ  3   )    =    1    3  !    A  3  3      {    E   {    y   (  3  )     (  n  )     ∏   i  =  1   3     x   (  3  )     (   n  -   τ  i    )      }    -    A  3  2    [     k  1   (  3  )     (   τ  1   )    δ    τ  2    τ  3      +    k  1   (  3  )     (   τ  2   )    δ    τ  1    τ  3      +    k  1   (  3  )     (   τ  3   )    δ    τ  1    τ  2       ]     }           superscript   subscript  k  3   3     subscript  τ  1    subscript  τ  2    subscript  τ  3         1      3    superscript   subscript  A  3   3          E      superscript  y  3   n    superscript   subscript  product    i  1    3      superscript  x  3     n   subscript  τ  i            superscript   subscript  A  3   2    delimited-[]       superscript   subscript  k  1   3    subscript  τ  1    subscript  δ     subscript  τ  2    subscript  τ  3         superscript   subscript  k  1   3    subscript  τ  2    subscript  δ     subscript  τ  1    subscript  τ  3         superscript   subscript  k  1   3    subscript  τ  3    subscript  δ     subscript  τ  1    subscript  τ  2              k_{3}^{(3)}(\tau_{1},\tau_{2},\tau_{3})=\frac{1}{3!A_{3}^{3}}\left\{E\left\{y^%
 {(3)}(n)\,\prod_{i=1}^{3}{x^{(3)}(n-\tau_{i})}\right\}-A_{3}^{2}\left[k_{1}^{(%
 3)}(\tau_{1})\delta_{\tau_{2}\tau_{3}}+k_{1}^{(3)}(\tau_{2})\delta_{\tau_{1}%
 \tau_{3}}+k_{1}^{(3)}(\tau_{3})\delta_{\tau_{1}\tau_{2}}\right]\right\}   In the above formulas the impulse functions are introduced for the identification of diagonal kernel points. If the Wiener kernels are extracted with the new formulas, the following Wiener to Volterra formulas (explicited up the fifth order) are needed:       h  5   =   k  5   (  5  )         subscript  h  5    superscript   subscript  k  5   5     h_{5}=\,k_{5}^{(5)}        h  4   =   k  4   (  4  )         subscript  h  4    superscript   subscript  k  4   4     h_{4}=\,k_{4}^{(4)}        h  3   =    k  3   (  3  )    -   10   A  3     ∑   τ  4      k  5   (  5  )     (   τ  1   ,   τ  2   ,   τ  3   ,   τ  4   ,   τ  4   )            subscript  h  3      superscript   subscript  k  3   3     10   subscript  A  3     subscript    subscript  τ  4       superscript   subscript  k  5   5     subscript  τ  1    subscript  τ  2    subscript  τ  3    subscript  τ  4    subscript  τ  4          h_{3}=\,k_{3}^{(3)}-10A_{3}\sum_{\tau_{4}}{k_{5}^{(5)}(\tau_{1},\tau_{2},\tau_%
 {3},\tau_{4},\tau_{4})}        h  2   =    k  2   (  2  )    -   6   A  2     ∑   τ  3      k  4   (  4  )     (   τ  1   ,   τ  2   ,   τ  3   ,   τ  3   )            subscript  h  2      superscript   subscript  k  2   2     6   subscript  A  2     subscript    subscript  τ  3       superscript   subscript  k  4   4     subscript  τ  1    subscript  τ  2    subscript  τ  3    subscript  τ  3          h_{2}=\,k_{2}^{(2)}-6A_{2}\sum_{\tau_{3}}{k_{4}^{(4)}(\tau_{1},\tau_{2},\tau_{%
 3},\tau_{3})}        h  1   =     k  1   (  1  )    -   3   A  1     ∑   τ  2      k  3   (  3  )     (   τ  1   ,   τ  2   ,   τ  2   )       +   15   A  1  2     ∑   τ  2      ∑   τ  3      k  5   (  5  )     (   τ  1   ,   τ  2   ,   τ  2   ,   τ  3   ,   τ  3   )             subscript  h  1        superscript   subscript  k  1   1     3   subscript  A  1     subscript    subscript  τ  2       superscript   subscript  k  3   3     subscript  τ  1    subscript  τ  2    subscript  τ  2          15   superscript   subscript  A  1   2     subscript     τ  2      subscript    subscript  τ  3       superscript   subscript  k  5   5     subscript  τ  1    subscript  τ  2    subscript  τ  2    subscript  τ  3    subscript  τ  3           h_{1}=\,k_{1}^{(1)}-3A_{1}\sum_{\tau_{2}}{k_{3}^{(3)}(\tau_{1},\tau_{2},\tau_{%
 2})}+15A_{1}^{2}\sum_{\tau 2}{\sum_{\tau_{3}}{k_{5}^{(5)}(\tau_{1},\tau_{2},%
 \tau_{2},\tau_{3},\tau_{3})}}        h  0   =     k  0   (  0  )    -    A  0     ∑   τ  1      k  2   (  2  )     (   τ  1   ,   τ  1   )       +   3   A  0  2     ∑   τ  1      ∑   τ  2      k  4   (  4  )     (   τ  1   ,   τ  1   ,   τ  2   ,   τ  2   )             subscript  h  0        superscript   subscript  k  0   0      subscript  A  0     subscript    subscript  τ  1       superscript   subscript  k  2   2     subscript  τ  1    subscript  τ  1          3   superscript   subscript  A  0   2     subscript    subscript  τ  1      subscript    subscript  τ  2       superscript   subscript  k  4   4     subscript  τ  1    subscript  τ  1    subscript  τ  2    subscript  τ  2           h_{0}=\,k_{0}^{(0)}-A_{0}\sum_{\tau_{1}}{k_{2}^{(2)}(\tau_{1},\tau_{1})}+3A_{0%
 }^{2}\sum_{\tau_{1}}{\sum_{\tau_{2}}{k_{4}^{(4)}(\tau_{1},\tau_{1},\tau_{2},%
 \tau_{2})}}     As can be seen, the drawback with respect to the classic formula is that for the identification of the n-order kernel, all lower kernels must be identified again with the higher variance. However an outstanding improvement in the output MSE will be obtained if the Wiener and Volterra kernels are obtained with the new formulas, as can be seen in. 8  Exact orthogonal algorithm  This method and its more efficient version (Fast Orthogonal Algorithm) were invented by Korenberg . 9 In this method the orthogonalization is performed empirically over the actual input. It has been shown to perform more precisely than the Crosscorrelation method. Another advantage is that arbitrary inputs can be used for the orthogonalization and that fewer data-points suffice to reach a desired level of accuracy. Also, estimation can be performed incrementally until some criterion is fulfilled.  Linear regression  Linear regression is a standard tool from linear analysis. Hence, one of its main advantages is the widespread existence of standard tools for solving linear regressions efficiently. It has some educational value, since it highlights the basic property of Volterra series: linear combination of non-linear basis-functionals. For estimation the order of the original should be known, since the volterra basis-functionals are not orthogonal and estimation can thus not be performed incrementally.  Kernel method  This method was invented by Franz & Schölkopf and is based on statistical learning theory . Consequently, this approach is also based on minimizing the empirical error (often called empirical risk minimization). Franz and Schölkopf proposed that the kernel method could essentially replace the Volterra series representation, although noting that the latter is more intuitive.  Differential sampling  This method was developed by van Hemmen and coworkers and utilizes Dirac delta functions to sample the Volterra coefficients.  See also   Wiener series   References  Further reading   Barrett J.F: Bibliography of Volterra series, Hermite functional expansions, and related subjects . Dept. Electr. Engrg, Univ.Tech. Eindhoven, NL 1977, T-H report 77-E-71. (Chronological listing of early papers to 1977) URL: http://alexandria.tue.nl/extra1/erap/publichtml/7704263.pdf  Bussgang, J.J.; Ehrman, L.; Graham, J.W: Analysis of nonlinear systems with multiple inputs, Proc. IEEE, vol.62, no.8, pp. 1088–1119, Aug. 1974  Giannakis G.B & Serpendin E: A bibliography on nonlinear system identification. Signal Processing, 81 2001 533–580. (Alphabetic listing to 2001) www.elsevier.nl/locate/sigpro  Korenberg M.J. Hunter I.W: The Identification of Nonlinear Biological Systems: Volterra Kernel Approaches , Annals Biomedical Engineering (1996), Volume 24, Number 2.  Kuo Y L: Frequency-domain analysis of weakly nonlinear networks , IEEE Trans. Circuits & Systems, vol.CS-11(4) Aug 1977; vol.CS-11(5) Oct 1977 2–6.  Rugh W J: Nonlinear System Theory: The Volterra–Wiener Approach. Baltimore 1981 (Johns Hopkins Univ Press) http://rfic.eecs.berkeley.edu/~niknejad/ee242/pdf/volterra_book.pdf  Schetzen M: The Volterra and Wiener Theories of Nonlinear Systems , New York: Wiley, 1980.   "  Category:Mathematical series  Category:Functional analysis     Vito Volterra. Theory of Functionals and of Integrals and Integro-Differential Equations. Madrid 1927 (Spanish), translated version reprinted New York: Dover Publications, 1959. ↩  Wiener N: Response of a nonlinear device to noise. Radiation Lab MIT 1942, restricted. report V-16, no 129 (112 pp). Declassified Jul 1946, Published as rep. no. PB-1-58087, U.S. Dept. Commerce. URL: http://www.dtic.mil/dtic/tr/fulltext/u2/a800212.pdf ↩  Ikehara S: A method of Wiener in a nonlinear circuit. MIT Dec 10 1951, tech. rep. no 217, Res. Lab. Electron. ↩  Early MIT reports by Brilliant, Zames, George, Hause, Chesler can be found on dspace.mit.edu. ↩  S. Orcioni, M. Pirani, and C. Turchetti, ``Advances in Lee-Schetzen method for Volterra filter identification,''Multidimensional Systems and Signal Processing, vol. 16, no. 3, pp. 265--284, 2005. ↩  Simone Orcioni. Improving the approximation ability of Volterra series identified with a cross-correlation method, Nonlinear Dynamics, 2014, DOI: 10.1007/s11071-014-1631-7. URL http://link.springer.com/content/pdf/10.1007%2Fs11071-014-1631-7.pdf ↩    Korenberg, M.J., Bruder, S.B., McIlroy, P.J.: Exact orthogonal kernel estimation from finite data records: extending Wiener’s identification of nonlinear systems. Ann. Biomed. Eng.16, 201–214 (1988) ↩     