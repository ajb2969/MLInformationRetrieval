   Draft:Kernels on Graph      Draft:Kernels on Graph    A graph can be represented in the form of an adjacency matrix , which encodes local links information between nodes. Community detection requires the global similarity information between pairwise nodes. Given an adjacency matrix, kernels on graph can be used to derive global similarity matrix . There're different ways to construct kernels on graph. In the following, we will summarize different methods and their respective assumptions.  Motivation  Community detection is a common procedure in network analysis. Different clustering algorithms, such as spectral clustering , hierarchical clustering , can be applied to group nodes into communities. The performance of these algorithms is largely determined by the construction of similarity matrix, which encodes certain notion of distance metric. It has been proved that the difference between many clustering algorithms results from different choices of similarity metric. 1 . Therefore, it will be meaningful to investigate the right way to construct similarity matrix in various networks.  Adjacency and Laplacian matrix of a graph  In graph theory, adjacency matrix   A   A   A   is defined to represent a graph. In particular, for a simple graph   ùêÜ   ùêÜ   \mathbf{G}   ,     A   i  j    =  1       subscript  A    i  j    1    A_{ij}=1   if   i   i   i   -th node and   j   j   j   -th node are connected through a link,     A   i  j    =  0       subscript  A    i  j    0    A_{ij}=0   otherwise. Diagonal elements    A   i  i      subscript  A    i  i     A_{ii}   are set to be zeros. Laplacian matrix 2 is defined as:      L  =   D  -  A       L    D  A     L=D-A     where   D   D   D   is degree matrix . Each diagonal element    D   i  i      subscript  D    i  i     D_{ii}   is defined to be the degree of   i   i   i   -th node and all the off-diagonal elements are zeros. Laplacian matrix is an important concept in spectral graph theory .  Kernel methods  Kernel methods, such as support vector machine , gaussian process , are widely used in machine learning and pattern recognition . Given inputs    x  1     subscript  x  1    x_{1}   and    x  2     subscript  x  2    x_{2}   , a kernel function     k   (   x  1   ,   x  2   )    =     œï   (   x  1   )    ‚ãÖ  œï    (   x  2   )          k    subscript  x  1    subscript  x  2        normal-‚ãÖ    œï   subscript  x  1    œï    subscript  x  2      k(x_{1},x_{2})=\phi(x_{1})\cdot\phi(x_{2})   can be defined to measure the similarity between    x  1     subscript  x  1    x_{1}   and    x  2     subscript  x  2    x_{2}   . Kernel function implicitly maps data from original space into high dimensional space     œï   (   x  1   )    ,   œï   (   x  2   )         œï   subscript  x  1      œï   subscript  x  2      \phi(x_{1}),\phi(x_{2})   such that data can be well separated in the new space. The kernel function need to be positive-definite in order to guarantee the existence of mapping function   œï   œï   \phi   .  Kernels on graph  Von Neumann kernel 3  Similar to Latent_semantic indexing , Neumann kernels is firstly proposed to compute the similarity between documents. Each document is represented by the terms occurring in the document. In the context of document clustering, the adjacency matrix of document graph is derived from document correlation matrix. Then Von Neumann diffusion kernel can be written as       K   V  N    =    ‚àë   k  =  0   ‚àû     Œ≥  k    A  k     =    (   ùêà  -   Œ≥  A    )    -  1           subscript  K    V  N      superscript   subscript     k  0         superscript  Œ≥  k    superscript  A  k           superscript    ùêà    Œ≥  A      1       K_{VN}=\sum_{k=0}^{\infty}\gamma^{k}A^{k}=(\mathbf{I}-\gamma A)^{-1}   Instead of only considering immediately links between nodes in adjacency matrix, ,    A   i  j   k     subscript   superscript  A  k     i  j     A^{k}_{ij}   is the number of paths between   i   i   i   -th node and   j   j   j   -th node with   k   k   k   steps in the co-citation graph. In this way, the kernel    K   V  N      subscript  K    V  N     K_{VN}   integrates all possible paths between pairwise nodes in arbitrary number of steps. Apparently shorter paths should have higher weights. The Von Neumann kernel gives longer path smaller weights with decay factor   Œ≥   Œ≥   \gamma   . The limitation of Von Neumann diffusion kernel is the similarity between nodes    i  ,  j     i  j    i,j   is determined by the number of nodes commonly citing    i  ,  j     i  j    i,j   . While nodes citing only one of them are ignored 4 .  Regularized Laplacian kernel 5  The regularized Laplacian kernel is defined as       K   R  L    =    ‚àë   k  =  0   ‚àû     Œ≥  k     (   -  L   )   k     =    (   I  +   Œ≥  L    )    -  1           subscript  K    R  L      superscript   subscript     k  0         superscript  Œ≥  k    superscript    L   k           superscript    I    Œ≥  L      1       K_{RL}=\sum_{k=0}^{\infty}\gamma^{k}(-L)^{k}=(I+\gamma L)^{-1}   Here adjacency matrix is replaced by negative Laplacian matrix. Therefore to interpret this formulation, the counting of paths is carried out on negative Laplacian matrix. In this case, the self-loop edges are also taken into account and have negative weights. Tradeoff between node importance and relatedness can be achieved by controlling   Œ≥   Œ≥   \gamma    6  Diffusion kernel 7 8  A diffusion model can be defined on a graph. Suppose each node has     x  i    (  t  )        subscript  x  i   t    x_{i}(t)   amount of particles at time   t   t   t   and it diffuses to node   j   j   j   in its neighborhood with rate    A   i  j      subscript  A    i  j     A_{ij}   . Therefore, during a small time interval    Œî  t      normal-Œî  t    \Delta t   ,     x  i    A   i  j    Œî  t       subscript  x  i    subscript  A    i  j    normal-Œî  t    x_{i}A_{ij}\Delta t   amount of particles is transferred from   i   i   i   -th node to   j   j   j   -th node. Consider both the amount of input and output during    Œî  t      normal-Œî  t    \Delta t   , the rate equation for   i   i   i   -th node is given by         x  i    (   t  +   Œî  t    )    -    x  i    (  t  )     =     ‚àë   j  =  1   n     x  j    A   j  i    Œî  t    -    ‚àë   j  =  1   n     x  i    A   i  j    Œî  t              subscript  x  i     t    normal-Œî  t        subscript  x  i   t        superscript   subscript     j  1    n      subscript  x  j    subscript  A    j  i    normal-Œî  t      superscript   subscript     j  1    n      subscript  x  i    subscript  A    i  j    normal-Œî  t       x_{i}(t+\Delta t)-x_{i}(t)=\sum_{j=1}^{n}x_{j}A_{ji}\Delta t-\sum_{j=1}^{n}x_{%
 i}A_{ij}\Delta t   . Solve for    ùê±   (  t  )       ùê±  t    \mathbf{x}(t)   , we have     ùê±   (  t  )    =   ùê±   (  0  )    exp   (   -   L  t    )           ùê±  t     ùê±  0        L  t        \mathbf{x}(t)=\mathbf{x}(0)\exp(-Lt)   , where    ùê±   (  0  )       ùê±  0    \mathbf{x}(0)   is the initial amount of particles at time    t  =  0      t  0    t=0   . This leads to diffusion kernel       K   D  L    =   exp   (   -   Œ≥  L    )         subscript  K    D  L          Œ≥  L       K_{DL}=\exp(-\gamma L)   If negative Laplacian matrix is replaced by adjacency matrix, a new kernel can be derived       K   D  A    =   exp   (   Œ≥  A   )    =    ‚àë   k  =  0   ‚àû      Œ≥  k    A  k     k  !            subscript  K    D  A        Œ≥  A           superscript   subscript     k  0           superscript  Œ≥  k    superscript  A  k      k        K_{DA}=\exp(\gamma A)=\sum_{k=0}^{\infty}\frac{\gamma^{k}A^{k}}{k!}   The difference between    K   D  A      subscript  K    D  A     K_{DA}   and Von Neumann kernel is the decay factor changes from   Œ≥   Œ≥   \gamma   to     Œ≥  k   /   k  !        superscript  Œ≥  k     k     \gamma^{k}/k!   .  Commute-time kernel 9  Consider random walk on the graph and define   P   P   P   as the transition probability matrix, where     P   i  j    =    A   i  j    /    ‚àë   j  =  1   n    A   i  j           subscript  P    i  j       subscript  A    i  j      superscript   subscript     j  1    n    subscript  A    i  j        P_{ij}=A_{ij}/\sum_{j=1}^{n}A_{ij}   . The average commute time between node    i  ,  j     i  j    i,j   , which is the average number of steps that a random walker, starting from node   i   i   i   , will take before arriving node   j   j   j   for the first time, and go back to   i   i   i   , can be computed from       n   (  i  ,  j  )    =    V  G     (    e  i   -   e  j    )   T    L  +    (    e  i   -   e  j    )          n   i  j       subscript  V  G    superscript     subscript  e  i    subscript  e  j    T    superscript  L       subscript  e  i    subscript  e  j       n(i,j)=V_{G}(e_{i}-e_{j})^{T}L^{+}(e_{i}-e_{j})   where     e  i   ,   e  j       subscript  e  i    subscript  e  j     e_{i},e_{j}   are representations of node    i  ,  j     i  j    i,j   ,    V  G     subscript  V  G    V_{G}   is the volume of the graph,    L  +     superscript  L     L^{+}   is Moore-Penrose pseudoinverse of Laplacian matrix   L   L   L   .    n   (  i  ,  j  )       n   i  j     n(i,j)   is the Mahalanobis distance between node    i  ,  j     i  j    i,j   with covariance matrix    L  +     superscript  L     L^{+}   . Accordingly, the commute time kernel can be defined as       K   C  T    =   L  +        subscript  K    C  T     superscript  L      K_{CT}=L^{+}     Random-walk-with-restart similarity kernel 10  In random walk that defines commute-time kernel, besides jumping from node   i   i   i   to its neighbor   j   j   j   proportional to edge weights, the walker is allowed to stay at node   i   i   i   with probability    1  -  Œ≥      1  Œ≥    1-\gamma   . From the final stable state, the random-walk-with-restart similarity kernel can be extracted       K   R  W  R    =     (   D  -   Œ≥  A    )    -  1    D        subscript  K    R  W  R       superscript    D    Œ≥  A      1    D     K_{RWR}=(D-\gamma A)^{-1}D     Markov diffusion kernel 11  The distance between two nodes can be defined by comparing their influence to the graph. If two nodes diffuse through the graph in the same way, then the distance between them is zero. Given the averaging visiting rate     x   i  k     (  t  )        subscript  x    i  k    t    x_{ik}(t)   , which is the probability of finding the random walker starting from state   i   i   i   in state   k   k   k   after   t   t   t   steps, the Markov diffusion distance between node    i  ,  j     i  j    i,j   at time   t   t   t   is        d   i  j     (  t  )    =    ‚àë   k  =  1   n     (     x   i  k     (  t  )    -    x   j  k     (  t  )     )   2    =     (    e  i   -   e  j    )   T   Z   (  t  )    Z  T    (  t  )    (    e  i   -   e  j    )             subscript  d    i  j    t     superscript   subscript     k  1    n    superscript       subscript  x    i  k    t      subscript  x    j  k    t    2            superscript     subscript  e  i    subscript  e  j    T   Z  t   superscript  Z  T   t     subscript  e  i    subscript  e  j        d_{ij}(t)=\sum_{k=1}^{n}(x_{ik}(t)-x_{jk}(t))^{2}=(e_{i}-e_{j})^{T}Z(t)Z^{T}(t%
 )(e_{i}-e_{j})   Thus, the Markov diffusion kernel can be defined as        K   M  D     (  t  )    =   Z   (  t  )    Z  T    (  t  )           subscript  K    M  D    t     Z  t   superscript  Z  T   t     K_{MD}(t)=Z(t)Z^{T}(t)   with     Z   (  t  )    =    1  t     ‚àë   œÑ  =  1   t    P  œÑ     =    1  t     (   I  -  P   )    -  1     (   I  -   P  t    )   P           Z  t       1  t     superscript   subscript     œÑ  1    t    superscript  P  œÑ              1  t    superscript    I  P     1      I   superscript  P  t    P      Z(t)=\frac{1}{t}\sum_{\tau=1}^{t}P^{\tau}=\frac{1}{t}(I-P)^{-1}(I-P^{t})P   If the Euclidean distance in    d   i  j      subscript  d    i  j     d_{ij}   is replaced with Kullback‚ÄìLeibler divergence , then Relative-entropy diffusion kernel 12 can be derived.  References      "      Yan, S., Xu, D., Zhang, B., Zhang, H. J., Yang, Q., & Lin, S. (2007). Graph embedding and extensions: a general framework for dimensionality reduction. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(1), 40-51. ‚Ü©  . ‚Ü©  Kandola, J., Cristianini, N., & Shawe-taylor, J. S. (2002). Learning semantic similarity. In Advances in neural information processing systems (pp. 657-664). ‚Ü©  Ito, T., Shimbo, M., Kudo, T., & Matsumoto, Y. (2005, August). Application of kernels to link analysis. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining (pp. 586-592). ACM. ‚Ü©  Chebotarev, P., & Shamis, E. (2006). The matrix-forest theorem and measuring relations in small social groups. arXiv preprint math/0602070. ‚Ü©  Ito, T., Shimbo, M., Kudo, T., & Matsumoto, Y. (2005, August). Application of kernels to link analysis. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining (pp. 586-592). ACM. ‚Ü©  Kondor, R. I., & Lafferty, J. (2002, July). Diffusion kernels on graphs and other discrete input spaces. In ICML (Vol. 2, pp. 315-322). ‚Ü©  Smola, A. J., & Kondor, R. (2003). Kernels and regularization on graphs. In Learning theory and kernel machines (pp. 144-158). Springer Berlin Heidelberg. ‚Ü©  Saerens, M., Fouss, F., Yen, L., & Dupont, P. (2004). The principal components analysis of a graph, and its relationships to spectral clustering. In Machine Learning: ECML 2004 (pp. 371-383). Springer Berlin Heidelberg. ‚Ü©  Pan, J. Y., Yang, H. J., Faloutsos, C., & Duygulu, P. (2004, August). Automatic multimedia cross-modal correlation discovery. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 653-658). ACM. ‚Ü©  Nadler, B., Lafon, S., Coifman, R. R., & Kevrekidis, I. G. (2005). Diffusion maps, spectral clustering and eigenfunctions of Fokker-Planck operators. arXiv preprint math/0506090. ‚Ü©  Fouss, F., Francoisse, K., Yen, L., Pirotte, A., & Saerens, M. (2012). An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification. Neural Networks, 31, 53-72. ‚Ü©     