


Draft:Kernels on Graph




Draft:Kernels on Graph


A  graph can be represented in the form of an adjacency matrix, which encodes local links information between nodes.  Community detection requires the global similarity information between pairwise nodes. Given an adjacency matrix,  kernels on graph can be used to derive global similarity matrix. There're different ways to construct kernels on graph. In the following, we will summarize different methods and their respective assumptions.
Motivation
Community detection is a common procedure in network analysis. Different  clustering algorithms, such as spectral clustering, hierarchical clustering, can be applied to group nodes into communities. The performance of these algorithms is largely determined by the construction of similarity matrix, which encodes certain notion of distance metric. It has been proved that the difference between many clustering algorithms results from different choices of similarity metric.1. Therefore, it will be meaningful to investigate the right way to construct similarity matrix in various networks.
Adjacency and Laplacian matrix of a graph
In graph theory, adjacency matrix 
 
 
 
  is defined to represent a graph. In particular, for a simple graph 
 
 
 
 , 
 
 
 
  if 
 
 
 
 -th node and 
 
 
 
 -th node are connected through a link, 
 
 
 
  otherwise. Diagonal elements 
 
 
 
  are set to be zeros. Laplacian matrix2 is defined as:



where 
 
 
 
  is degree matrix. Each diagonal element 
 
 
 
  is defined to be the degree of 
 
 
 
 -th node and all the off-diagonal elements are zeros. Laplacian matrix is an important concept in spectral graph theory.
Kernel methods
Kernel methods, such as support vector machine, gaussian process, are widely used in machine learning and pattern recognition. Given inputs 
 
 
 
  and 
 
 
 
 , a kernel function 
 
 
 
  can be defined to measure the similarity between 
 
 
 
  and 
 
 
 
 . Kernel function implicitly maps data from original space into high dimensional space 
 
 
 
  such that data can be well separated in the new space. The kernel function need to be  positive-definite in order to guarantee the existence of mapping function 
 
 
 
 .
Kernels on graph
Von Neumann kernel 3
Similar to Latent_semantic indexing, Neumann kernels is firstly proposed to compute the similarity between documents. Each document is represented by the terms occurring in the document. In the context of document clustering, the adjacency matrix of document graph is derived from document correlation matrix. Then Von Neumann diffusion kernel can be written as


 
  Instead of only considering immediately links between nodes in adjacency matrix, , 
 
 
 
  is the number of paths between 
 
 
 
 -th node and 
 
 
 
 -th node with 
 
 
 
  steps in the co-citation graph. In this way, the kernel 
 
 
 
  integrates all possible paths between pairwise nodes in arbitrary number of steps. Apparently shorter paths should have higher weights. The Von Neumann kernel gives longer path smaller weights with decay factor 
 
 
 
 . The limitation of Von Neumann diffusion kernel is the similarity between nodes 
 
 
 
  is determined by the number of nodes commonly citing 
 
 
 
 . While nodes citing only one of them are ignored4.
Regularized Laplacian kernel5
The regularized Laplacian kernel is defined as


 
  Here adjacency matrix is replaced by negative Laplacian matrix. Therefore to interpret this formulation, the counting of paths is carried out on negative Laplacian matrix. In this case, the self-loop edges are also taken into account and have negative weights. Tradeoff between node importance and relatedness can be achieved by controlling 
 
 
6
Diffusion kernel78
A  diffusion model can be defined on a graph. Suppose each node has 
 
 
 
  amount of particles at time 
 
 
 
  and it diffuses to node 
 
 
 
  in its neighborhood with rate 
 
 
 
 . Therefore, during a small time interval 
 
 
 
 , 
 
 
 
  amount of particles is transferred from 
 
 
 
 -th node to 
 
 
 
 -th node. Consider both the amount of input and output during 
 
 
 
 , the rate equation for 
 
 
 
 -th node is given by


 
 . Solve for 
 
 
 
 , we have 
 
 
 
 , where 
 
 
 
  is the initial amount of particles at time 
 
 
 
 . This leads to diffusion kernel


 
  If negative Laplacian matrix is replaced by adjacency matrix, a new kernel can be derived


 
  The difference between 
 
 
 
  and Von Neumann kernel is the decay factor changes from 
 
 
 
  to 
 
 
 
 .
Commute-time kernel 9
Consider random walk on the graph and define 
 
 
 
  as the transition probability matrix, where 
 
 
 
 . The average commute time between node 
 
 
 
 , which is the average number of steps that a random walker, starting from node 
 
 
 
 , will take before arriving node 
 
 
 
  for the first time, and go back to 
 
 
 
 , can be computed from


 
  where 
 
 
 
  are representations of node 
 
 
 
 , 
 
 
 
  is the volume of the graph, 
 
 
 
  is Moore-Penrose pseudoinverse of Laplacian matrix 
 
 
 
 . 
 
 
 
  is the Mahalanobis distance between node 
 
 
 
  with covariance matrix 
 
 
 
 . Accordingly, the commute time kernel can be defined as



Random-walk-with-restart similarity kernel 10
In random walk that defines commute-time kernel, besides jumping from node 
 
 
 
  to its neighbor 
 
 
 
  proportional to edge weights, the walker is allowed to stay at node 
 
 
 
  with probability 
 
 
 
 . From the final stable state, the random-walk-with-restart similarity kernel can be extracted



Markov diffusion kernel 11
The distance between two nodes can be defined by comparing their influence to the graph. If two nodes diffuse through the graph in the same way, then the distance between them is zero. Given the averaging visiting rate 
 
 
 
 , which is the probability of finding the random walker starting from state 
 
 
 
  in state 
 
 
 
  after 
 
 
 
  steps, the Markov diffusion distance between node 
 
 
 
  at time 
 
 
 
  is


 
  Thus, the Markov diffusion kernel can be defined as


 
  with 
 
 
 
  If the Euclidean distance in 
 
 
 
  is replaced with Kullbackâ€“Leibler divergence, then Relative-entropy diffusion kernel12 can be derived.
References




"
 




Yan, S., Xu, D., Zhang, B., Zhang, H. J., Yang, Q., & Lin, S. (2007). Graph embedding and extensions: a general framework for dimensionality reduction. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(1), 40-51.
.
Kandola, J., Cristianini, N., & Shawe-taylor, J. S. (2002). Learning semantic similarity. In Advances in neural information processing systems (pp. 657-664).
Ito, T., Shimbo, M., Kudo, T., & Matsumoto, Y. (2005, August). Application of kernels to link analysis. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining (pp. 586-592). ACM.
Chebotarev, P., & Shamis, E. (2006). The matrix-forest theorem and measuring relations in small social groups. arXiv preprint math/0602070.
Ito, T., Shimbo, M., Kudo, T., & Matsumoto, Y. (2005, August). Application of kernels to link analysis. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining (pp. 586-592). ACM.
Kondor, R. I., & Lafferty, J. (2002, July). Diffusion kernels on graphs and other discrete input spaces. In ICML (Vol. 2, pp. 315-322).
Smola, A. J., & Kondor, R. (2003). Kernels and regularization on graphs. In Learning theory and kernel machines (pp. 144-158). Springer Berlin Heidelberg.
Saerens, M., Fouss, F., Yen, L., & Dupont, P. (2004). The principal components analysis of a graph, and its relationships to spectral clustering. In Machine Learning: ECML 2004 (pp. 371-383). Springer Berlin Heidelberg.
Pan, J. Y., Yang, H. J., Faloutsos, C., & Duygulu, P. (2004, August). Automatic multimedia cross-modal correlation discovery. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 653-658). ACM.
Nadler, B., Lafon, S., Coifman, R. R., & Kevrekidis, I. G. (2005). Diffusion maps, spectral clustering and eigenfunctions of Fokker-Planck operators. arXiv preprint math/0506090.
Fouss, F., Francoisse, K., Yen, L., Pirotte, A., & Saerens, M. (2012). An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification. Neural Networks, 31, 53-72.




