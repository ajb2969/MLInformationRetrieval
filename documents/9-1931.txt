   Algebraic formula for the variance      Algebraic formula for the variance   In probability theory and statistics , there are several algebraic formulae for the variance available for deriving the variance of a random variable . The usefulness of these depends on what is already known about the random variable; for example a random variable may be defined in terms of its probability density function or by construction from other random variables. The context here is that of deriving algebraic expressions for the theoretical variance of a random variable, in contrast to questions of estimating the variance of a population from sample data for which there are special considerations in implementing computational algorithms .  In terms of raw moments  If the raw moments E( X ) and E( X  2 ) of a random variable  X are known (where E( X ) is the expected value of X ), then Var( X ) is given by        Var   (  X  )    =    E   (   X  2   )    -    [   E   (  X  )    ]   2     .       Var  X      normal-E   superscript  X  2     superscript   delimited-[]   normal-E  X    2      \operatorname{Var}(X)=\operatorname{E}(X^{2})-[\operatorname{E}(X)]^{2}.     The result is called the König – Huygens formula in French-language literature 1 and known as Steiner translation theorem in Germany. 2  There is a corresponding formula for use in estimation of the variance from sample data, that can be of use in hand calculations. This is a closely related identity that is structured to create an unbiased estimate of the population variance         σ  ^   2   =    1   N  -  1      ∑   i  =  1   N     (    x  i   -   x  ¯    )   2     =    N   N  -  1     (     1  N    (    ∑   i  =  1   N    x  i  2    )    -    x  ¯   2    )    ≡    1   N  -  1     (    (    ∑   i  =  1   N    x  i  2    )   -   N    (   x  ¯   )   2     )     .         superscript   normal-^  σ   2       1    N  1      superscript   subscript     i  1    N    superscript     subscript  x  i    normal-¯  x    2              N    N  1          1  N     superscript   subscript     i  1    N    superscript   subscript  x  i   2      superscript   normal-¯  x   2              1    N  1        superscript   subscript     i  1    N    superscript   subscript  x  i   2      N   superscript   normal-¯  x   2         \hat{\sigma}^{2}=\frac{1}{N-1}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}=\frac{N}{N-1}%
 \left(\frac{1}{N}\left(\sum_{i=1}^{N}x_{i}^{2}\right)-\bar{x}^{2}\right)\equiv%
 \frac{1}{N-1}\left(\left(\sum_{i=1}^{N}x_{i}^{2}\right)-N\left(\bar{x}\right)^%
 {2}\right).     However, use of these formulas can be unwise in practice when using floating point arithmetic with limited precision: subtracting two values having a similar magnitude can lead to catastrophic cancellation , 3 and thus causing a loss of significance when    E    (  X  )   2   ≫  Var   (  X  )      fragments  normal-E   superscript   fragments  normal-(  X  normal-)   2   much-greater-than  Var   fragments  normal-(  X  normal-)     \operatorname{E}(X)^{2}\gg\operatorname{Var}(X)   . There exist other numerically stable algorithms for calculating variance for use with floating point numbers.  Proof  The computational formula for the population variance follows in a straightforward manner from the linearity of expected values and the definition of variance:      Var   (  X  )      Var  X    \displaystyle\operatorname{Var}(X)     Generalization to covariance  This formula can be generalized for covariance , with two random variables X i and X j :       Cov   (   X  i   ,   X  j   )    =    E   (    X  i    X  j    )    -    E   (   X  i   )     E   (   X  j   )           Cov   subscript  X  i    subscript  X  j       normal-E     subscript  X  i    subscript  X  j        normal-E   subscript  X  i     normal-E   subscript  X  j        \operatorname{Cov}(X_{i},X_{j})=\operatorname{E}(X_{i}X_{j})-\operatorname{E}(%
 X_{i})\operatorname{E}(X_{j})     as well as for the n by n  covariance matrix of a random vector of length n :      Var   (  𝐗  )   =  E   (   𝐗𝐗  ⊤   )   -  E   (  𝐗  )   E    (  𝐗  )   ⊤      fragments  Var   fragments  normal-(  X  normal-)    normal-E   fragments  normal-(   superscript  𝐗𝐗  top   normal-)    normal-E   fragments  normal-(  X  normal-)   normal-E   superscript   fragments  normal-(  X  normal-)   top     \operatorname{Var}(\mathbf{X})=\operatorname{E}(\mathbf{XX^{\top}})-%
 \operatorname{E}(\mathbf{X})\operatorname{E}(\mathbf{X})^{\top}     and for the n by m  cross-covariance matrix between two random vectors of length n and m :      Cov   (  𝐗  ,  𝐘  )   =  E   (   𝐗𝐘  ⊤   )   -  E   (  𝐗  )   E    (  𝐘  )   ⊤      fragments  Cov   fragments  normal-(  X  normal-,  Y  normal-)    normal-E   fragments  normal-(   superscript  𝐗𝐘  top   normal-)    normal-E   fragments  normal-(  X  normal-)   normal-E   superscript   fragments  normal-(  Y  normal-)   top     \operatorname{Cov}(\textbf{X},\textbf{Y})=\operatorname{E}(\mathbf{XY^{\top}})%
 -\operatorname{E}(\mathbf{X})\operatorname{E}(\mathbf{Y})^{\top}     where expectations are taken element-wise and    𝐗  =   {   X  1   ,   X  2   ,  …  ,   X  n   }       𝐗    subscript  X  1    subscript  X  2   normal-…   subscript  X  n      \mathbf{X}=\{X_{1},X_{2},\ldots,X_{n}\}   and    𝐘  =   {   Y  1   ,   Y  2   ,  …  ,   Y  m   }       𝐘    subscript  Y  1    subscript  Y  2   normal-…   subscript  Y  m      \mathbf{Y}=\{Y_{1},Y_{2},\ldots,Y_{m}\}   are random vectors of respective lengths n and m .  Note that this formula suffers from the same loss of significance as the formula for variance if used for calculating estimates of the covariance.  See also   Standard deviation: Identities and mathematical properties   References  "  Category:Statistical deviation and dispersion     In French: formule de Koenig–Huygens. See e.g. ↩  In German: Verschiebungssatz von Steiner. See e.g. . ↩  Donald E. Knuth (1998). The Art of Computer Programming , volume 2: Seminumerical Algorithms , 3rd edn., p. 232. Boston: Addison-Wesley. ↩     