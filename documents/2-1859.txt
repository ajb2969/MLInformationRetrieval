   Gaussian process      Gaussian process   In probability theory and statistics , Gaussian processes are a family of statistical distributions (not necessarily stochastic processes in which time plays a role). In a Gaussian process, every point in some input space is associated with a normally distributed  random variable . Moreover, every finite collection of those random variables has a multivariate normal distribution . The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions.  The concept of Gaussian processes is named after Carl Friedrich Gauss because it is based on the notion of the Gaussian distribution ( normal distribution ). Gaussian processes can be seen as an infinite-dimensional generalization of multivariate normal distributions.  Gaussian processes are important in statistical modelling because of properties inherited from the normal. For example, if a random process is modeled as a Gaussian process, the distributions of various derived quantities can be obtained explicitly. Such quantities include the average value of the process over a range of times and the error in estimating the average using sample values at a small set of times.  Definition  A Gaussian process is a statistical distribution  X t , t ∈ T , for which any finite linear combination of samples has a joint Gaussian distribution . More accurately, any linear functional applied to the sample function X t will give a normally distributed result. Notation-wise, one can write X ~ GP( m , K ), meaning the random function  X is distributed as a GP with mean function m and covariance function  K . 1 When the input vector t is two- or multi-dimensional, a Gaussian process might be also known as a Gaussian random field . 2  Some authors 3 assume the random variables  X t have mean zero; this greatly simplifies calculations without loss of generality and allows the mean square properties of the process to be entirely determined by the covariance function  K . 4  Alternative definitions  Alternatively, a process is Gaussian if and only if for every finite set of indices      t  1   ,  …  ,   t  k       subscript  t  1   normal-…   subscript  t  k     t_{1},\ldots,t_{k}   in the index set   T   T   T          𝐗    t  1   ,  …  ,   t  k     =   (   𝐗   t  1    ,  …  ,   𝐗   t  k    )        subscript  𝐗    subscript  t  1   normal-…   subscript  t  k       subscript  𝐗   subscript  t  1    normal-…   subscript  𝐗   subscript  t  k       {\mathbf{X}}_{t_{1},\ldots,t_{k}}=(\mathbf{X}_{t_{1}},\ldots,\mathbf{X}_{t_{k}})     is a multivariate Gaussian  random variable . Using characteristic functions of random variables, the Gaussian property can be formulated as follows    {     X  t   ;  t   ∈  T   }         subscript  X  t   t   T     \left\{X_{t};t\in T\right\}   is Gaussian if and only if, for every finite set of indices     t  1   ,  …  ,   t  k       subscript  t  1   normal-…   subscript  t  k     t_{1},\ldots,t_{k}   , there are real valued    σ   ℓ  j      subscript  σ    normal-ℓ  j     \sigma_{\ell j}   ,    μ  ℓ     subscript  μ  normal-ℓ    \mu_{\ell}   with     σ   i  i    >  0       subscript  σ    i  i    0    \sigma_{ii}>0   such that        E   (   exp   (    i     ∑   ℓ  =  1   k      s  ℓ     𝐗   t  ℓ       )    )    =   exp   (    -     1  2      ∑   ℓ  ,  j      σ   ℓ  j     s  ℓ    s  j       +   i    ∑  ℓ     μ  ℓ    s  ℓ       )     .       normal-E      i    superscript   subscript     normal-ℓ  1    k      subscript  s  normal-ℓ    subscript  𝐗   subscript  t  normal-ℓ                   1  2     subscript    normal-ℓ  j       subscript  σ    normal-ℓ  j     subscript  s  normal-ℓ    subscript  s  j         i    subscript   normal-ℓ      subscript  μ  normal-ℓ    subscript  s  normal-ℓ          \operatorname{E}\left(\exp\left(i\ \sum_{\ell=1}^{k}s_{\ell}\ \mathbf{X}_{t_{%
 \ell}}\right)\right)=\exp\left(-\frac{1}{2}\,\sum_{\ell,j}\sigma_{\ell j}s_{%
 \ell}s_{j}+i\sum_{\ell}\mu_{\ell}s_{\ell}\right).     The numbers    σ   ℓ  j      subscript  σ    normal-ℓ  j     \sigma_{\ell j}   and    μ  ℓ     subscript  μ  normal-ℓ    \mu_{\ell}   can be shown to be the covariances and means of the variables in the process. 5  Covariance functions  A key fact of Gaussian processes is that they can be completely defined by their second-order statistics. 6 Thus, if a Gaussian process is assumed to have mean zero, defining the covariance function completely defines the process' behaviour. The covariance matrix K between all the pair of points x and x ' specifies a distribution on functions and is known as the Gram matrix . Importantly, because every valid covariance function is a scalar product of vectors, by construction the matrix K is a non-negative definite matrix . Equivalently, the covariance function K is a non-negative definite function in the sense that for every pair x and x ', K ( x , x ') ≥ 0; if K (,) > 0 then K is called positive definite . Importantly the non-negative definiteness of K enables its spectral decomposition using the Karhunen–Loeve expansion . Basic aspects that can be defined through the covariance function are the process' stationarity , isotropy , smoothness and periodicity . 7 8  Stationarity refers to the process' behaviour regarding the separation of any two points x and ''x' ''. If the process is stationary, it depends on their separation, x − x ', while if non-stationary it depends on the actual position of the points x and x '; an example of a stationary process is the Ornstein–Uhlenbeck process . On the contrary, the special case of an Ornstein–Uhlenbeck process, a Brownian motion process, is non-stationary.  If the process depends only on | x − x '|, the Euclidean distance (not the direction) between x and x ' then the process is considered isotropic. A process that is concurrently stationary and isotropic is considered to be homogeneous ; 9 in practice these properties reflect the differences (or rather the lack of them) in the behaviour of the process given the location of the observer.  Ultimately Gaussian processes translate as taking priors on functions and the smoothness of these priors can be induced by the covariance function. 10 If we expect that for "near-by" input points x and ''x' '' their corresponding output points y and ''y' '' to be "near-by" also, then the assumption of smoothness is present. If we wish to allow for significant displacement then we might choose a rougher covariance function. Extreme examples of the behaviour is the Ornstein–Uhlenbeck covariance function and the squared exponential where the former is never differentiable and the latter infinitely differentiable.  Periodicity refers to inducing periodic patterns within the behaviour of the process. Formally, this is achieved by mapping the input x to a two dimensional vector u ( x ) = (cos( x ), sin( x )).  Usual covariance functions  There are a number of common covariance functions: 11   Constant      K  C    (  x  ,   x  ′   )    =  C         subscript  K  C    x   superscript  x  normal-′     C    K_{\text{C}}(x,x^{\prime})=C     Linear      K  L    (  x  ,   x  ′   )    =    x  T    x  ′           subscript  K  L    x   superscript  x  normal-′        superscript  x  T    superscript  x  normal-′      K_{\text{L}}(x,x^{\prime})=x^{T}x^{\prime}     Gaussian Noise      K  GN    (  x  ,   x  ′   )    =    σ  2    δ   x  ,   x  ′             subscript  K  GN    x   superscript  x  normal-′        superscript  σ  2    subscript  δ   x   superscript  x  normal-′        K_{\text{GN}}(x,x^{\prime})=\sigma^{2}\delta_{x,x^{\prime}}     Squared Exponential      K  SE    (  x  ,   x  ′   )    =   exp   (   -     |  d  |   2    2   l  2      )           subscript  K  SE    x   superscript  x  normal-′            superscript    d   2     2   superscript  l  2         K_{\text{SE}}(x,x^{\prime})=\exp\Big(-\frac{|d|^{2}}{2l^{2}}\Big)     Ornstein–Uhlenbeck      K  OU    (  x  ,   x  ′   )    =   exp   (   -    |  d  |   l    )           subscript  K  OU    x   superscript  x  normal-′             d   l       K_{\text{OU}}(x,x^{\prime})=\exp\Big(-\frac{|d|}{l}\Big)     Matérn      K  Matern    (  x  ,   x  ′   )    =     2   1  -  ν     Γ   (  ν  )       (      2  ν     |  d  |    l   )   ν    K  ν    (      2  ν     |  d  |    l   )           subscript  K  Matern    x   superscript  x  normal-′          superscript  2    1  ν      normal-Γ  ν     superscript          2  ν      d    l   ν    subscript  K  ν           2  ν      d    l      K_{\text{Matern}}(x,x^{\prime})=\frac{2^{1-\nu}}{\Gamma(\nu)}\Big(\frac{\sqrt{%
 2\nu}|d|}{l}\Big)^{\nu}K_{\nu}\Big(\frac{\sqrt{2\nu}|d|}{l}\Big)     Periodic      K  P    (  x  ,   x  ′   )    =   exp   (   -    2    sin  2    (   d  2   )      l  2     )           subscript  K  P    x   superscript  x  normal-′             2    superscript   2     d  2      superscript  l  2        K_{\text{P}}(x,x^{\prime})=\exp\Big(-\frac{2\sin^{2}(\frac{d}{2})}{l^{2}}\Big)     Rational Quadratic       K  RQ    (  x  ,   x  ′   )    =    (   1  +    |  d  |   2    )    -  α     ,   α  ≥  0      formulae-sequence       subscript  K  RQ    x   superscript  x  normal-′      superscript    1   superscript    d   2      α       α  0     K_{\text{RQ}}(x,x^{\prime})=(1+|d|^{2})^{-\alpha},\quad\alpha\geq 0      Here    d  =   x  -   x  ′        d    x   superscript  x  normal-′      d=x-x^{\prime}   . The parameter   l   l   l   is the characteristic length-scale of the process (practically, "how close" two points   x   x   x   and    x  ′     superscript  x  normal-′    x^{\prime}   have to be to influence each other significantly), δ is the Kronecker delta and σ the standard deviation of the noise fluctuations. Moreover,    K  ν     subscript  K  ν    K_{\nu}   is the modified Bessel function of order   ν   ν   \nu   and   Γ   normal-Γ   \Gamma   is the gamma function evaluated for   ν   ν   \nu   . Importantly, a complicated covariance function can be defined as a linear combination of other simpler covariance functions in order to incorporate different insights about the data-set at hand.  Clearly, the inferential results are dependent on the values of the hyperparameters θ (e.g.   l   l   l   and σ ) defining the model's behaviour. A popular choice for θ is to provide maximum a posteriori (MAP) estimates of it by maximizing the marginal likelihood of the process; the marginalization being done over the observed process values   y   y   y   . 12 This approach is also known as maximum likelihood II , evidence maximization , or Empirical Bayes . 13  Brownian Motion as the Integral of Gaussian processes  A Wiener process (aka brownian motion) is the integral of a white noise Gaussian process. It is not stationary , but it has stationary increments.  The Ornstein–Uhlenbeck process is a stationary Gaussian process.  The Brownian bridge is the integral of a Gaussian process whose increments are not independent .  The fractional Brownian motion is the integral of a Gaussian process whose covariance function is a generalisation of Wiener process.  Applications  A Gaussian process can be used as a prior probability distribution over functions in Bayesian inference . 14 15 Given any set of N points in the desired domain of your functions, take a multivariate Gaussian whose covariance matrix parameter is the Gram matrix of your N points with some desired kernel , and sample from that Gaussian.  Inference of continuous values with a Gaussian process prior is known as Gaussian process regression, or kriging ; extending Gaussian process regression to multiple target variables is known as cokriging . 16 Gaussian processes are thus useful as a powerful non-linear multivariate interpolation tool. Additionally, Gaussian process regression can be extended to address learning tasks in both supervised (e.g. probabilistic classification 17 ) and unsupervised (e.g. manifold learning 18 ) learning frameworks.  Gaussian process prediction  When concerned with a general Gaussian process regression problem, it is assumed that for a Gaussian process f observed at coordinates x, the vector of values f(x) is just one sample from a multivariate Gaussian distribution of dimension equal to number of observed coordinates |x| . Therefore under the assumption of a zero-mean distribution, f (x) ∼ N (0, K(θ,x,x')) , where K(θ,x,x') is the covariance matrix between all possible pairs (x,x') for a given set of hyperparameters θ. 19 As such the log marginal likelihood is:      log  p   (  f   (  x  )   |  θ  ,  x  )   =  -   1  2   f    (  x  )   T   K    (  θ  ,  x  ,   x  ′   )    -  1    f   (  x  )   -   1  2   log  det   (  K   (  θ  ,  x  ,   x  ′   )   )   -    |  x  |   2   log  2  π     fragments   p   fragments  normal-(  f   fragments  normal-(  x  normal-)   normal-|  θ  normal-,  x  normal-)       1  2   f   superscript   fragments  normal-(  x  normal-)   T   K   superscript   fragments  normal-(  θ  normal-,  x  normal-,   superscript  x  normal-′   normal-)     1    f   fragments  normal-(  x  normal-)      1  2      fragments  normal-(  K   fragments  normal-(  θ  normal-,  x  normal-,   superscript  x  normal-′   normal-)   normal-)        x   2    2  π    \log p(f(x)|\theta,x)=-\frac{1}{2}f(x)^{T}K(\theta,x,x^{\prime})^{-1}f(x)-%
 \frac{1}{2}\log\det(K(\theta,x,x^{\prime}))-\frac{|x|}{2}\log 2\pi   and maximizing this marginal likelihood towards θ provides the complete specification of the Gaussian process f . One can briefly note at this point that the first term corresponds to a penalty term for a model's failure to fit observed values and the second term to a penalty term that increases proportionally to a model's complexity. Having specified θ making predictions about unobserved values f(x*) at coordinates x* is then only a matter of drawing samples from the predictive distribution p(y*|x*,f(x),x) = N(y*|A,B) where the posterior mean estimate A is defined as:      A  =   K   (  θ  ,   x  *   ,  x  )   K    (  θ  ,  x  ,   x  ′   )    -  1    f   (  x  )        A    K   θ   superscript  x    x   K   superscript   θ  x   superscript  x  normal-′      1    f  x     A=K(\theta,x^{*},x)K(\theta,x,x^{\prime})^{-1}f(x)   and the posterior variance estimate B is defined as:      B  =    K   (  θ  ,   x  *   ,   x  *   )    -   K   (  θ  ,   x  *   ,  x  )   K    (  θ  ,  x  ,   x  ′   )    -  1    K    (  θ  ,   x  *   ,  x  )   T         B      K   θ   superscript  x     superscript  x        K   θ   superscript  x    x   K   superscript   θ  x   superscript  x  normal-′      1    K   superscript   θ   superscript  x    x   T       B=K(\theta,x^{*},x^{*})-K(\theta,x^{*},x)K(\theta,x,x^{\prime})^{-1}K(\theta,x%
 ^{*},x)^{T}   where K(θ,x*,x) is the covariance between the new coordinate of estimation x* and all other observed coordinates x for a given hyperparameter vector θ, K(θ,x,x') and f(x) are defined as before and K(θ,x*,x*) is the variance at point x* as dictated by θ . It is important to note that practically the posterior mean estimate f(x*) (the "point estimate") is just a linear combination of the observations f(x) ; in a similar manner the variance of f(x*) is actually independent of the observations f(x) . A known bottleneck in Gaussian process prediction is that the computational complexity of prediction is cubic in the number of points |x| and as such can become unfeasible for larger data sets. 20 Works on sparse Gaussian processes, that usually are based on the idea of building a representative set for the given process f , try to circumvent this issue. 21 22  See also   Bayes linear statistics  Gaussian random field  Bayesian interpretation of regularization  Kriging   Notes  External links   www.GaussianProcess.com  The Gaussian Processes Web Site, including the text of Rasmussen and Williams' Gaussian Processes for Machine Learning  A gentle introduction to Gaussian processes  A Review of Gaussian Random Fields and Correlation Functions   Software   Yelp MOE - A black box optimization engine using Gaussian process learning  ooDACE - A flexible object-oriented Kriging matlab toolbox.  GPstuff - Gaussian process toolbox for Matlab and Octave  GPy - A Gaussian processes framework in Python  Interactive Gaussian process regression demo  Basic Gaussian process library written in C++11   Video tutorials   Gaussian Process Basics by David MacKay  Learning with Gaussian Processes by Carl Edward Rasmussen  Bayesian inference and Gaussian processes by Carl Edward Rasmussen   "  Category:Stochastic processes  Category:Kernel methods for machine learning  Category:Non-parametric Bayesian methods     ↩  ↩  ↩  ↩  ↩   ↩  ↩  ↩       ↩  ↩      ↩  ↩     