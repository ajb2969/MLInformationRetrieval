


Universal approximation theorem




Universal approximation theorem

In the mathematical theory of artificial neural networks, the universal approximation theorem states1 that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters; it does not touch upon the algorithmic learnability of those parameters.
One of the first versions of the theorem was proved by George Cybenko in 1989 for sigmoid activation functions.2
Kurt Hornik showed in 19913 that it is not the specific choice of the activation function, but rather the multilayer feedforward architecture itself which gives neural networks the potential of being universal approximators. The output units are always assumed to be linear. For notational convenience, only the single output case will be shown. The general case can easily be deduced from the single output case.
Formal statement
The theorem4567 in mathematical terms:

Let 
 
 
 
  be a nonconstant, bounded, and monotonically-increasing continuous function. Let 
 
 
 
  denote the m-dimensional unit hypercube

 
 . The space of continuous functions on 
 
 
 
  is denoted by 
 
 
 
 . Then, given any function 
 
 
 
  and 
 
 
 
 , there exist an integer 
 
 
 
  and real constants 
 
 
 
 , where 
 
 
 
  such that we may define:



as an approximate realization of the function 
 
 
 
  where 
 
 
 
 
  is independent of 
 
 
 
 ; that is,



for all 
 
 
 
 . In other words, functions of the form 
 
 
 
  are dense in 
 
 
 
 
 .

It obviously holds replacing 
 
 
 
  with any compact subset of 
 
 
 
 .
References
"
Category:Theorems in discrete mathematics Category:Artificial neural networks Category:Neural networks Category:Network architecture Category:Networks Category:Information, knowledge, and uncertainty



Balázs Csanád Csáji. Approximation with Artificial Neural Networks; Faculty of Sciences; Eötvös Loránd University, Hungary
Cybenko., G. (1989) "Approximations by superpositions of sigmoidal functions", Mathematics of Control, Signals, and Systems, 2 (4), 303-314
Kurt Hornik (1991) "Approximation Capabilities of Multilayer Feedforward Networks", Neural Networks, 4(2), 251–257


Haykin, Simon (1998). Neural Networks: A Comprehensive Foundation, Volume 2, Prentice Hall. ISBN 0-13-273350-1.
Hassoun, M. (1995) Fundamentals of Artificial Neural Networks MIT Press, p. 48




