   Bennett's inequality      Bennett's inequality   In probability theory , Bennett's inequality provides an upper bound on the probability that the sum of independent random variables deviates from its expected value by more than any specified amount. Bennett's inequality was proved by George Bennett of the University of New South Wales in 1962. 1  Let be independent random variables , and assume (for simplicity but without loss of generality ) they all have zero expected value. Further assume X i {{!}} ≤ a }}  almost surely for all   i   i   i   , and let        σ  2   =    1  n     ∑   i  =  1   n    Var   (   X  i   )       .       superscript  σ  2       1  n     superscript   subscript     i  1    n    Var   subscript  X  i        \sigma^{2}=\frac{1}{n}\sum_{i=1}^{n}\operatorname{Var}(X_{i}).   Then for any    t  ≥  0      t  normal-≥  0    t≥0   ,        Pr   (     ∑   i  =  1   n    X  i    >  t   )    ≤   exp   (   -     n   σ  2     a  2    h   (    a  t    n   σ  2     )     )     ,       Pr      superscript   subscript     i  1    n    subscript  X  i    t              n   superscript  σ  2     superscript  a  2    h      a  t     n   superscript  σ  2          \Pr\left(\sum_{i=1}^{n}X_{i}>t\right)\leq\exp\left(-\frac{n\sigma^{2}}{a^{2}}h%
 \left(\frac{at}{n\sigma^{2}}\right)\right),     where     h   (  u  )    =    (   1  +  u   )   l  o  g   (   1  +  u   )   –  u         h  u       1  u   l  o  g    1  u   normal-–  u     h(u)=(1+u)log(1+u)–u   . 2  See also Freedman (1975) 3 and Fan et al. (2012) 4 for a martingale version of Bennett's inequality and its improvement, respectively.  See also   Bernstein inequalities (probability theory)  Hoeffding's inequality  Azuma's inequality  McDiarmid's inequality  Markov inequality  Dvoretzky–Kiefer–Wolfowitz inequality  Chebyshev's inequality  Concentration inequality   References    "  Category:Probabilistic inequalities     ↩  ↩  ↩  ↩     