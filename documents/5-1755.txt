   Broyden–Fletcher–Goldfarb–Shanno algorithm      Broyden–Fletcher–Goldfarb–Shanno algorithm   In numerical  optimization , the Broyden–Fletcher–Goldfarb–Shanno ( BFGS ) algorithm is an iterative method for solving unconstrained nonlinear optimization problems.  The BFGS method approximates  Newton's method , a class of hill-climbing optimization techniques that seeks a stationary point of a (preferably twice continuously differentiable) function. For such problems, a necessary condition for optimality is that the gradient be zero. Newton's method and the BFGS methods are not guaranteed to converge unless the function has a quadratic Taylor expansion near an optimum . These methods use both the first and second derivatives of the function. However, BFGS has proven to have good performance even for non-smooth optimizations. 1  In quasi-Newton methods , the Hessian matrix of second derivatives doesn't need to be evaluated directly. Instead, the Hessian matrix is approximated using rank-one updates specified by gradient evaluations (or approximate gradient evaluations). Quasi-Newton methods are generalizations of the secant method to find the root of the first derivative for multidimensional problems. In multi-dimensional problems, the secant equation does not specify a unique solution, and quasi-Newton methods differ in how they constrain the solution. The BFGS method is one of the most popular members of this class. 2 Also in common use is L-BFGS , which is a limited-memory version of BFGS that is particularly suited to problems with very large numbers of variables (e.g., >1000). The BFGS-B 3 variant handles simple box constraints.  Rationale  The search direction p k at stage k is given by the solution of the analogue of the Newton equation        B  k    𝐩  k    =   -    ∇  f    (   𝐱  k   )            subscript  B  k    subscript  𝐩  k         normal-∇  f    subscript  𝐱  k       B_{k}\mathbf{p}_{k}=-\nabla f(\mathbf{x}_{k})     where    B  k     subscript  B  k    B_{k}   is an approximation to the Hessian matrix which is updated iteratively at each stage, and     ∇  f    (   𝐱  k   )        normal-∇  f    subscript  𝐱  k     \nabla f(\mathbf{x}_{k})   is the gradient of the function evaluated at x k . A line search in the direction p k is then used to find the next point x k +1 . Instead of requiring the full Hessian matrix at the point x k +1 to be computed as B k +1 , the approximate Hessian at stage k is updated by the addition of two matrices.       B   k  +  1    =    B  k   +   U  k   +   V  k         subscript  B    k  1       subscript  B  k    subscript  U  k    subscript  V  k      B_{k+1}=B_{k}+U_{k}+V_{k}\,\!     Both U k and V k are symmetric rank-one matrices but have different (matrix) bases. The symmetric rank one assumption here means that we may write      C  =   𝐚𝐛  T       C   superscript  𝐚𝐛  normal-T     C=\mathbf{a}\mathbf{b}^{\mathrm{T}}     So equivalently, U k and V k construct a rank-two update matrix which is robust against the scale problem often suffered in the gradient descent searching ( e.g. , in Broyden's method ).  The quasi-Newton condition imposed on this update is         B   k  +  1     (    𝐱   k  +  1    -   𝐱  k    )    =     ∇  f    (   𝐱   k  +  1    )    -    ∇  f    (   𝐱  k   )      .         subscript  B    k  1       subscript  𝐱    k  1     subscript  𝐱  k          normal-∇  f    subscript  𝐱    k  1        normal-∇  f    subscript  𝐱  k       B_{k+1}(\mathbf{x}_{k+1}-\mathbf{x}_{k})=\nabla f(\mathbf{x}_{k+1})-\nabla f(%
 \mathbf{x}_{k}).     Algorithm  From an initial guess    𝐱  0     subscript  𝐱  0    \mathbf{x}_{0}   and an approximate Hessian matrix    B  0     subscript  B  0    B_{0}   the following steps are repeated as    𝐱  k     subscript  𝐱  k    \mathbf{x}_{k}   converges to the solution.   Obtain a direction    𝐩  k     subscript  𝐩  k    \mathbf{p}_{k}   by solving       B  k    𝐩  k    =   -    ∇  f    (   𝐱  k   )      .         subscript  B  k    subscript  𝐩  k         normal-∇  f    subscript  𝐱  k       B_{k}\mathbf{p}_{k}=-\nabla f(\mathbf{x}_{k}).     Perform a line search to find an acceptable stepsize    α  k     subscript  α  k    \alpha_{k}   in the direction found in the first step, then update      𝐱   k  +  1    =    𝐱  k   +    α  k    𝐩  k      .       subscript  𝐱    k  1       subscript  𝐱  k      subscript  α  k    subscript  𝐩  k       \mathbf{x}_{k+1}=\mathbf{x}_{k}+\alpha_{k}\mathbf{p}_{k}.     Set      𝐬  k   =    α  k    𝐩  k     .       subscript  𝐬  k      subscript  α  k    subscript  𝐩  k      \mathbf{s}_{k}=\alpha_{k}\mathbf{p}_{k}.           𝐲  k   =     ∇  f    (   𝐱   k  +  1    )    -    ∇  f    (   𝐱  k   )      .       subscript  𝐲  k        normal-∇  f    subscript  𝐱    k  1        normal-∇  f    subscript  𝐱  k       \mathbf{y}_{k}={\nabla f(\mathbf{x}_{k+1})-\nabla f(\mathbf{x}_{k})}.           B   k  +  1    =     B  k   +     𝐲  k    𝐲  k  T      𝐲  k  T    𝐬  k      -     B  k    𝐬  k    𝐬  k  T    B  k      𝐬  k  T    B  k    𝐬  k       .       subscript  B    k  1         subscript  B  k        subscript  𝐲  k    superscript   subscript  𝐲  k   normal-T       superscript   subscript  𝐲  k   normal-T    subscript  𝐬  k           subscript  B  k    subscript  𝐬  k    superscript   subscript  𝐬  k   normal-T    subscript  B  k       superscript   subscript  𝐬  k   normal-T    subscript  B  k    subscript  𝐬  k        B_{k+1}=B_{k}+\frac{\mathbf{y}_{k}\mathbf{y}_{k}^{\mathrm{T}}}{\mathbf{y}_{k}^%
 {\mathrm{T}}\mathbf{s}_{k}}-\frac{B_{k}\mathbf{s}_{k}\mathbf{s}_{k}^{\mathrm{T%
 }}B_{k}}{\mathbf{s}_{k}^{\mathrm{T}}B_{k}\mathbf{s}_{k}}.          f   (  𝐱  )       f  𝐱    f(\mathbf{x})   denotes the objective function to be minimized. Convergence can be checked by observing the norm of the gradient,    |    ∇  f    (   𝐱  k   )    |         normal-∇  f    subscript  𝐱  k      \left|\nabla f(\mathbf{x}_{k})\right|   . Practically,    B  0     subscript  B  0    B_{0}   can be initialized with     B  0   =  I       subscript  B  0   I    B_{0}=I   , so that the first step will be equivalent to a gradient descent , but further steps are more and more refined by    B  k     subscript  B  k    B_{k}   , the approximation to the Hessian.  The first step of the algorithm is carried out using the inverse of the matrix    B  k     subscript  B  k    B_{k}   , which is usually obtained efficiently by applying the Sherman–Morrison formula to the fifth line of the algorithm, giving        B   k  +  1    -  1    =     (   I  -     s  k    y  k  T      y  k  T    s  k      )    B  k   -  1     (   I  -     y  k    s  k  T      y  k  T    s  k      )    +     s  k    s  k  T       y  k  T     s  k       .       superscript   subscript  B    k  1      1          I       subscript  s  k    superscript   subscript  y  k   T       superscript   subscript  y  k   T    subscript  s  k       superscript   subscript  B  k     1      I       subscript  y  k    superscript   subscript  s  k   T       superscript   subscript  y  k   T    subscript  s  k            subscript  s  k    superscript   subscript  s  k   T       superscript   subscript  y  k   T    subscript  s  k        B_{k+1}^{-1}=\left(I-\frac{s_{k}y_{k}^{T}}{y_{k}^{T}s_{k}}\right)B_{k}^{-1}%
 \left(I-\frac{y_{k}s_{k}^{T}}{y_{k}^{T}s_{k}}\right)+\frac{s_{k}s_{k}^{T}}{y_{%
 k}^{T}\,s_{k}}.     This can be computed efficiently without temporary matrices, recognizing that    B  k   -  1      superscript   subscript  B  k     1     B_{k}^{-1}   is symmetric, and that     𝐲  k  T    B  k   -  1     𝐲  k        superscript   subscript  𝐲  k   normal-T    superscript   subscript  B  k     1     subscript  𝐲  k     \mathbf{y}_{k}^{\mathrm{T}}B_{k}^{-1}\mathbf{y}_{k}   and     𝐬  k  T    𝐲  k        superscript   subscript  𝐬  k   normal-T    subscript  𝐲  k     \mathbf{s}_{k}^{\mathrm{T}}\mathbf{y}_{k}   are scalar, using an expansion such as        B   k  +  1    -  1    =     B  k   -  1    +     (     𝐬  k  T    𝐲  k    +    𝐲  k  T    B  k   -  1     𝐲  k     )    (    𝐬  k    𝐬  k  T    )      (    𝐬  k  T    𝐲  k    )   2     -      B  k   -  1     𝐲  k    𝐬  k  T    +    𝐬  k    𝐲  k  T    B  k   -  1        𝐬  k  T    𝐲  k       .       superscript   subscript  B    k  1      1         superscript   subscript  B  k     1             superscript   subscript  𝐬  k   normal-T    subscript  𝐲  k       superscript   subscript  𝐲  k   normal-T    superscript   subscript  B  k     1     subscript  𝐲  k        subscript  𝐬  k    superscript   subscript  𝐬  k   normal-T      superscript     superscript   subscript  𝐬  k   normal-T    subscript  𝐲  k    2            superscript   subscript  B  k     1     subscript  𝐲  k    superscript   subscript  𝐬  k   normal-T       subscript  𝐬  k    superscript   subscript  𝐲  k   normal-T    superscript   subscript  B  k     1         superscript   subscript  𝐬  k   normal-T    subscript  𝐲  k        B_{k+1}^{-1}=B_{k}^{-1}+\frac{(\mathbf{s}_{k}^{\mathrm{T}}\mathbf{y}_{k}+%
 \mathbf{y}_{k}^{\mathrm{T}}B_{k}^{-1}\mathbf{y}_{k})(\mathbf{s}_{k}\mathbf{s}_%
 {k}^{\mathrm{T}})}{(\mathbf{s}_{k}^{\mathrm{T}}\mathbf{y}_{k})^{2}}-\frac{B_{k%
 }^{-1}\mathbf{y}_{k}\mathbf{s}_{k}^{\mathrm{T}}+\mathbf{s}_{k}\mathbf{y}_{k}^{%
 \mathrm{T}}B_{k}^{-1}}{\mathbf{s}_{k}^{\mathrm{T}}\mathbf{y}_{k}}.     In statistical estimation problems (such as maximum likelihood or Bayesian inference), credible intervals or confidence intervals for the solution can be estimated from the inverse of the final Hessian matrix. However, these quantities are technically defined by the true Hessian matrix, and the BFGS approximation may not converge to the true Hessian matrix.  Implementations  The GSL implements BFGS as gsl_multimin_fdfminimizer_vector_bfgs2 . Ceres Solver implements both BFGS and L-BFGS . In SciPy , the scipy.optimize.fmin_bfgs function implements BFGS. It is also possible to run BFGS using any of the L-BFGS algorithms by setting the parameter L to a very large number.  Octave uses BFGS with a double-dogleg approximation to the cubic line search.  In the MATLAB Optimization Toolbox , the fminunc function uses BFGS with cubic line search when the problem size is set to "medium scale."  A high-precision arithmetic version of BFGS ( pBFGS ), implemented in C++ and integrated with the high-precision arithmetic package ARPREC is robust against numerical instability (e.g. round-off errors).  Another C++ implementation of BFGS, along with L-BFGS, L-BFGS-B, CG, and Newton's method) using Eigen (C++ library) are available on github under the MIT License  here .  BFGS and L-BFGS are also implemented in C as part of the open-source Gnu Regression, Econometrics and Time-series Library ( gretl ).  See also   Quasi-Newton methods  Davidon–Fletcher–Powell formula  L-BFGS  Gradient descent  Nelder–Mead method  Pattern search (optimization)  BHHH algorithm   Notes    Bibliography              External links   Source code of high-precision BFGS A C++ source code of BFGS with high-precision arithmetic   "  Category:Optimization algorithms and methods     ↩  , page 24 ↩  ↩     