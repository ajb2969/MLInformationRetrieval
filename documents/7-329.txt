   Kernel method      Kernel method   In machine learning , kernel methods are a class of algorithms for pattern analysis , whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters , rankings , principal components , correlations , classifications ) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map : in contrast, kernel methods require only a user-specified kernel , i.e., a similarity function over pairs of data points in raw representation.  Kernel methods owe their name to the use of kernel functions , which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates. This approach is called the " kernel trick ". Kernel functions have been introduced for sequence data, graphs , text, images, as well as vectors.  Algorithms capable of operating with kernels include the kernel perceptron , support vector machines (SVM), Gaussian processes , principal components analysis (PCA), canonical correlation analysis , ridge regression , spectral clustering , linear adaptive filters and many others. Any linear model can be turned into a non-linear model by applying the kernel trick to the model: replacing its features (predictors) by a kernel function.  Most kernel algorithms are based on convex optimization or eigenproblems and are statistically well-founded. Typically, their statistical properties are analyzed using statistical learning theory (for example, using Rademacher complexity ).  Motivation and informal explanation  Kernel methods can be thought of as instance-based learners : rather than learning some fixed set of parameters corresponding to the features of their inputs, they instead "remember" the   i   i   i   -th training example    (   ğ±  i   ,   y  i   )      subscript  ğ±  i    subscript  y  i     (\mathbf{x}_{i},y_{i})   by learning a corresponding weight    w  i     subscript  w  i    w_{i}   . Prediction for unlabeled inputs, i.e., those not in the training set, is treated by the application of a similarity function    k   k   k   , called a kernel , between the unlabeled input    ğ±  â€²     superscript  ğ±  normal-â€²    \mathbf{x^{\prime}}   and each of the training inputs    ğ±  i     subscript  ğ±  i    \mathbf{x}_{i}   . For instance, a kernelized binary classifier typically computes a weighted sum of similarities       y  ^   =   sgn    âˆ‘   i  =  1   n     w  i    y  i   k   (   ğ±  i   ,   ğ±  â€²   )           normal-^  y     sgn    superscript   subscript     i  1    n      subscript  w  i    subscript  y  i   k    subscript  ğ±  i    superscript  ğ±  normal-â€²         \hat{y}=\operatorname{sgn}\sum_{i=1}^{n}w_{i}y_{i}k(\mathbf{x}_{i},\mathbf{x^{%
 \prime}})   ,  where        y  ^   âˆˆ   {   -  1   ,   +  1   }        normal-^  y      1     1      \hat{y}\in\{-1,+1\}   is the kernelized binary classifier's predicted label for the unlabeled input    ğ±  â€²     superscript  ğ±  normal-â€²    \mathbf{x^{\prime}}   whose hidden true label   y   y   y   is of interest;      k  :    ğ’³  Ã—  ğ’³   â†’  â„      normal-:  k   normal-â†’    ğ’³  ğ’³   â„     k\colon\mathcal{X}\times\mathcal{X}\to\mathbb{R}   is the kernel function that measures similarity between any pair of inputs     ğ±  ,   ğ±  â€²    âˆˆ  ğ’³       ğ±   superscript  ğ±  normal-â€²    ğ’³    \mathbf{x},\mathbf{x^{\prime}}\in\mathcal{X}   ;  the sum ranges over the   n   n   n   labeled examples     {   (   ğ±  i   ,   y  i   )   }    i  =  1   n     superscript   subscript     subscript  ğ±  i    subscript  y  i       i  1    n    \{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}   in the classifier's training set, with     y  i   âˆˆ   {   -  1   ,   +  1   }        subscript  y  i      1     1      y_{i}\in\{-1,+1\}   ;  the     w  i   âˆˆ  â„       subscript  w  i   â„    w_{i}\in\mathbb{R}   are the weights for the training examples, as determined by the learning algorithm;  the sign function    sgn   sgn   \operatorname{sgn}   determines whether the predicted classification    y  ^     normal-^  y    \hat{y}   comes out positive or negative.   Kernel classifiers were described as early as the 1960s, with the invention of the kernel perceptron . 1 They rose to great prominence with the popularity of the support vector machine (SVM) in the 1990s, when the SVM was found to be competitive with neural networks on tasks such as handwriting recognition .  Mathematics  The kernel trick avoids the explicit mapping that is needed to get linear learning algorithms to learn a nonlinear function or decision boundary . For all   ğ±   ğ±   \mathbf{x}   and    ğ±  â€²     superscript  ğ±  normal-â€²    \mathbf{x^{\prime}}   in the input space   ğ’³   ğ’³   \mathcal{X}   , certain functions    k   (  ğ±  ,   ğ±  â€²   )       k   ğ±   superscript  ğ±  normal-â€²      k(\mathbf{x},\mathbf{x^{\prime}})   can be expressed as an inner product in another space   ğ’±   ğ’±   \mathcal{V}   . The function    k  :    ğ’³  Ã—  ğ’³   â†’  â„      normal-:  k   normal-â†’    ğ’³  ğ’³   â„     k\colon\mathcal{X}\times\mathcal{X}\to\mathbb{R}   is often referred to as a kernel or a kernel function ; the word "kernel" is used in different ways throughout mathematics.  If one is insightful regarding a particular machine learning problem, one may manually construct a "feature map"    Ï†  :   ğ’³  â†’  ğ’±      normal-:  Ï†   normal-â†’  ğ’³  ğ’±     \varphi\colon\mathcal{X}\to\mathcal{V}   such that       k   (  ğ±  ,   ğ±  â€²   )    =    âŸ¨   Ï†   (  ğ±  )    ,   Ï†   (   ğ±  â€²   )    âŸ©   ğ’±         k   ğ±   superscript  ğ±  normal-â€²      subscript     Ï†  ğ±     Ï†   superscript  ğ±  normal-â€²     ğ’±     k(\mathbf{x},\mathbf{x^{\prime}})=\langle\varphi(\mathbf{x}),\varphi(\mathbf{x%
 ^{\prime}})\rangle_{\mathcal{V}}   and verify that     âŸ¨  â‹…  ,  â‹…  âŸ©   ğ’±     subscript   normal-â‹…  normal-â‹…   ğ’±    \langle\cdot,\cdot\rangle_{\mathcal{V}}   is indeed an inner product. In fact, an explicit representation for   Ï†   Ï†   \varphi   is not required: it suffices to show that   ğ’±   ğ’±   \mathcal{V}   is an inner product space . Conveniently, based on Mercer's theorem , it suffices to equip   ğ’³   ğ’³   \mathcal{X}   with one's choice of measure and verify that   k   k   k   satisfies Mercer's condition .  Mercer's theorem is stated in a general mathematical setting with implications in the theory of integral equations . However, the general statement is more than what is required for understanding the kernel trick. Given a finite observation set   X   X   X   , one can select the counting measure      Î¼   (  T  )    =   |  T  |         Î¼  T     T     \mu(T)=|T|   for all    T  âŠ‚  X      T  X    T\subset X   . Then the integral in Mercer's theorem reduces to a simple summation        âˆ‘   i  =  1   n     âˆ‘   j  =  1   n    k   (   ğ±  i   ,   ğ±  j   )    c  i    c  j      â‰¥  0        superscript   subscript     i  1    n     superscript   subscript     j  1    n     k    subscript  ğ±  i    subscript  ğ±  j     subscript  c  i    subscript  c  j      0    \sum_{i=1}^{n}\sum_{j=1}^{n}k(\mathbf{x}_{i},\mathbf{x}_{j})c_{i}c_{j}\geq 0   for all finite sequences of points    (   ğ±  1   ,  â€¦  ,   ğ±  n   )      subscript  ğ±  1   normal-â€¦   subscript  ğ±  n     (\mathbf{x}_{1},\ldots,\mathbf{x}_{n})   in   ğ’³   ğ’³   \mathcal{X}   and all choices of   n   n   n   real-valued coefficients    (   c  1   ,  â€¦  ,   c  n   )      subscript  c  1   normal-â€¦   subscript  c  n     (c_{1},\dots,c_{n})   (cf. positive definite kernel ).  Some algorithms that depend on arbitrary relationships in the native space   ğ’³   ğ’³   \mathcal{X}   would, in fact, have a linear interpretation in a different setting: the range space of   Ï†   Ï†   \varphi   . The linear interpretation gives us insight about the algorithm. Furthermore, there is often no need to compute   Ï†   Ï†   \varphi   directly during computation, as is the case with support vector machines . Some cite this running time shortcut as the primary benefit. Researchers also use it to justify the meanings and properties of existing algorithms.  Theoretically, a Gram matrix     ğŠ  âˆˆ   â„   n  Ã—  n        ğŠ   superscript  â„    n  n      \mathbf{K}\in\mathbb{R}^{n\times n}   with respect to    {   ğ±  1   ,  â€¦  ,   ğ±  n   }      subscript  ğ±  1   normal-â€¦   subscript  ğ±  n     \{\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\}   (sometimes also called a "kernel matrix" 2 ), where    ğŠ  =    (   k   (   ğ±  i   ,   ğ±  j   )    )    i  j        ğŠ   subscript    k    subscript  ğ±  i    subscript  ğ±  j       i  j      \mathbf{K}=(k(\mathbf{x}_{i},\mathbf{x}_{j}))_{ij}   , must be positive semi-definite (PSD) . 3 Empirically, for machine learning heuristics, choices of a function   k   k   k   that do not satisfy Mercer's condition may still perform reasonably if   k   k   k   at least approximates the intuitive idea of similarity. 4 Regardless of whether   k   k   k   is a Mercer kernel,   k   k   k   may still be referred to as a "kernel".  If the kernel function   k   k   k   is also a covariance function as used in Gaussian processes , then the Gram matrix   ğŠ   ğŠ   \mathbf{K}   can also be called a covariance matrix . 5  Finally, suppose that   ğŠ   ğŠ   \mathbf{K}   is a square matrix. Then     ğŠ  T   ğŠ       superscript  ğŠ  normal-T   ğŠ    \mathbf{K}^{\mathrm{T}}\mathbf{K}   is a PSD matrix.  Applications  Application areas of kernel methods are diverse and include geostatistics , 6  kriging , inverse distance weighting , 3D reconstruction , bioinformatics , chemoinformatics , information extraction and handwriting recognition .  Popular kernels   Fisher kernel  Graph kernels  Polynomial kernel  RBF kernel  String kernels   See also   Kernel regression  Kernel smoothing  Kernel methods for vector output   Notes  References      External links   Kernel-Machines Org â€”community website  www.support-vector-machines.org  (Literature, Review, Software, Links related to Support Vector Machines - Academic Site)  onlineprediction.net Kernel Methods Article   "  Category:Kernel methods for machine learning  Category:Geostatistics  Category:Classification algorithms     Cited in â†©  â†©  â†©  http://www.svms.org/mercer/ â†©  â†©  â†©     