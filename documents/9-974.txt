   Stress majorization      Stress majorization   Stress majorization is an optimization strategy used in multidimensional scaling (MDS) where, for a set of n  m -dimensional data items, a configuration X of n points in ''r(\sigma(X)  . Usually r is 2 or 3, i.e. the (r x n) matrix X lists points in 2- or 3-dimensional Euclidean space so that the result may be visualised (i.e. an MDS plot). The function   σ   σ   \sigma   is a cost or loss function that measures the squared differences between ideal (   m   m   m   -dimensional) distances and actual distances in r -dimensional space. It is defined as:      (  i  ,  j  )     i  j    (i,j)   is a weight for the measurement between a pair of points     d   i  j     (  X  )        subscript  d    i  j    X    d_{ij}(X)   ,   i   i   i   is the euclidean distance between   j   j   j   and    δ   i  j      subscript  δ    i  j     \delta_{ij}   and   m   m   m   is the ideal distance between the points (their separation) in the    w   i  j      subscript  w    i  j     w_{ij}   -dimensional data space. Note that   X   X   X   can be used to specify a degree of confidence in the similarity between points (e.g. 0 can be specified if there is no information for a particular pair).  A configuration    σ   (  X  )       σ  X    \sigma(X)   which minimizes   m   m   m   gives a plot in which points that are close together correspond to points that are also close together in the original    σ   (  X  )       σ  X    \sigma(X)   -dimensional data space.  There are many ways that   σ   σ   \sigma   could be minimized. For example, Kruskal 1 recommended an iterative steepest descent approach. However, a significantly better (in terms of guarantees on, and rate of, convergence) method for minimizing stress was introduced by Jan de Leeuw . 2 De Leeuw's iterative majorization method at each step minimizes a simple convex function which both bounds   σ   σ   \sigma   from above and touches the surface of   Z   Z   Z   at a point   σ   σ   \sigma   , called the supporting point . In convex analysis such a function is called a majorizing function. This iterative majorization process is also referred to as the SMACOF algorithm ("Scaling by majorizing a convex function").  The SMACOF algorithm  The stress function     X  ′   V  X       superscript  X  normal-′   V  X    X^{\prime}VX   can be expanded as follows:       b   i  j    =   -     w   i  j     δ   i  j       d   i  j     (  Z  )           subscript  b    i  j           subscript  w    i  j     subscript  δ    i  j        subscript  d    i  j    Z       b_{ij}=-\frac{w_{ij}\delta_{ij}}{d_{ij}(Z)}   and the second term is quadratic in X (i.e. for the Hessian matrix V the second term is equivalent to tr        d   i  j     (  Z  )    ≠  0   ,   i  ≠  j      formulae-sequence       subscript  d    i  j    Z   0     i  j     d_{ij}(Z)\neq 0,i\neq j   ) and therefore relatively easily solved. The third term is bounded by:       b   i  j    =  0       subscript  b    i  j    0    b_{ij}=0   has:         d   i  j     (  Z  )    =  0   ,   i  ≠  j      formulae-sequence       subscript  d    i  j    Z   0     i  j     d_{ij}(Z)=0,i\neq j   for     b   i  i    =   -    ∑    j  =  1   ,   j  ≠  i    n    b   i  j           subscript  b    i  i        superscript   subscript    formulae-sequence    j  1     j  i     n    subscript  b    i  j        b_{ii}=-\sum_{j=1,j\neq i}^{n}b_{ij}     and    τ   (  X  ,  Z  )       τ   X  Z     \tau(X,Z)   for     σ   (  X  )    =    C  +   tr    X  ′   V  X     -    2    tr    X  ′   B     (  X  )   X          σ  X       C   tr     superscript  X  normal-′   V  X       2   tr     superscript  X  normal-′   B    X  X      \sigma(X)=C+\,\operatorname{tr}\,X^{\prime}VX-2\,\operatorname{tr}\,X^{\prime}%
 B(X)X     and     ≤    C  +   tr    X  ′   V  X     -    2    tr    X  ′   B     (  Z  )   Z    =   τ   (  X  ,  Z  )          absent      C   tr     superscript  X  normal-′   V  X       2   tr     superscript  X  normal-′   B    Z  Z           τ   X  Z       \leq C+\,\operatorname{tr}\,X^{\prime}VX-2\,\operatorname{tr}\,X^{\prime}B(Z)Z%
 =\tau(X,Z)   .  Proof of this inequality is by the Cauchy-Schwarz inequality, see Borg 3 (pp. 152–153).  Thus, we have a simple quadratic function    Z  ←   X   k  -  1       normal-←  Z   superscript  X    k  1      Z\leftarrow X^{k-1}   that majorizes stress:       X  k   ←     min  X   τ    (  X  ,  Z  )       normal-←   superscript  X  k       subscript   X   τ    X  Z      X^{k}\leftarrow\min_{X}\tau(X,Z)           σ   (   X   k  -  1    )    -   σ   (   X  k   )     <  ϵ          σ   superscript  X    k  1       σ   superscript  X  k     ϵ    \sigma(X^{k-1})-\sigma(X^{k})<\epsilon     The iterative minimization procedure is then:   at the k th step we set    δ   i  j      subscript  δ    i  j     \delta_{ij}         w   i  j      subscript  w    i  j     w_{ij}     stop if    δ   i  j    -  α      superscript   subscript  δ    i  j      α     \delta_{ij}^{-\alpha}   otherwise repeat.   This algorithm has been shown to decrease stress monotonically (see de Leeuw 4 ).  Use in graph drawing  Stress majorization and algorithms similar to SMACOF also have application in the field of graph drawing . 5 6 That is, one can find a reasonably aesthetically appealing layout for a network or graph by minimizing a stress function over the positions of the nodes in the graph. In this case, the   α   α   \alpha   are usually set to the graph-theoretic distances between nodes i and j and the weights    α  =  2      α  2    \alpha=2   are taken to be $\delta_{ij}^{-\alpha}$ . Here, $\alpha$ is chosen as a trade-off between preserving long- or short-range ideal distances. Good results have been shown for $\alpha=2$ . 7  References  "  Category:Graph drawing  Category:Multivariate statistics  Category:Mathematical optimization  Category:Mathematical analysis     . ↩  . ↩  . ↩   . ↩  . ↩  . ↩     