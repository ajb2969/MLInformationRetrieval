<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="620">Multidimensional scaling</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Multidimensional scaling</h1>
<hr/>
<p> <strong>Multidimensional scaling</strong> (<strong>MDS</strong>) is a means of visualizing the level of similarity of individual cases of a dataset. It refers to a set of related <a href="Ordination_(statistics)" title="wikilink">ordination</a> techniques used in <a href="information_visualization" title="wikilink">information visualization</a>, in particular to display the information contained in a <a href="distance_matrix" title="wikilink">distance matrix</a>. An MDS <a class="uri" href="algorithm" title="wikilink">algorithm</a> aims to place each object in <em>N</em>-<a href="dimension" title="wikilink">dimensional</a> space such that the between-object distances are preserved as well as possible. Each object is then assigned <a href="coordinate" title="wikilink">coordinates</a> in each of the <em>N</em> dimensions. The number of dimensions of an MDS plot <em>N</em> can exceed 2 and is specified <a href="a_priori_and_a_posteriori" title="wikilink">a priori</a>. Choosing <em>N</em>=2 optimizes the object locations for a two-dimensional <a class="uri" href="scatterplot" title="wikilink">scatterplot</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="types">Types</h2>
<p>MDS algorithms fall into a <a href="Taxonomy_(general)" title="wikilink">taxonomy</a>, depending on the meaning of the input matrix:</p>
<dl>
<dt>Classical multidimensional scaling: Also known as Principal Coordinates Analysis, Torgerson Scaling or Torgerson–Gower scaling. Takes an input matrix giving dissimilarities between pairs of items and outputs a coordinate matrix whose configuration minimizes a loss function called <em>strain</em>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><br/>
Metric multidimensional scaling: A superset of classical MDS that generalizes the optimization procedure to a variety of loss functions and input matrices of known distances with weights and so on. A useful loss function in this context is called <em>stress</em>, which is often minimized using a procedure called <a href="stress_majorization" title="wikilink">stress majorization</a>.<br/>
Non-metric multidimensional scaling: In contrast to metric MDS, non-metric MDS finds both a <a class="uri" href="non-parametric" title="wikilink">non-parametric</a> <a class="uri" href="monotonic" title="wikilink">monotonic</a> relationship between the dissimilarities in the item-item matrix and the Euclidean distances between items, and the location of each item in the low-dimensional space. The relationship is typically found using <a href="isotonic_regression" title="wikilink">isotonic regression</a>.<br/>
*<a href="Louis_Guttman" title="wikilink">Louis Guttman</a>'s <strong>smallest space analysis</strong> (SSA) is an example of a non-metric MDS procedure.<br/>
Generalized multidimensional scaling: An extension of metric multidimensional scaling, in which the target space is an arbitrary smooth non-Euclidean space. In cases where the dissimilarities are distances on a surface and the target space is another surface, GMDS allows finding the minimum-distortion embedding of one surface into another.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></dt>
</dl>
<h2 id="details">Details</h2>
<p>The data to be analyzed is a collection of <span class="LaTeX">$I$</span> objects (colors, faces, stocks, . . .) on which a <em>distance function</em> is defined,</p>
<p><span class="LaTeX">$$\delta_{i,j} :=$$</span> distance between <span class="LaTeX">$i$</span>-th and <span class="LaTeX">$j$</span>-th objects.</p>
<p>These distances are the entries of the <em>dissimilarity matrix</em></p>
<p><span class="LaTeX">$$\Delta := 
\begin{pmatrix}
\delta_{1,1} & \delta_{1,2} & \cdots & \delta_{1,I} \\
\delta_{2,1} & \delta_{2,2} & \cdots & \delta_{2,I} \\
\vdots & \vdots & & \vdots \\
\delta_{I,1} & \delta_{I,2} & \cdots & \delta_{I,I}
\end{pmatrix}.$$</span></p>
<p>The goal of MDS is, given <span class="LaTeX">$\Delta$</span>, to find <span class="LaTeX">$I$</span> vectors <span class="LaTeX">$x_1,\ldots,x_I \in \mathbb{R}^N$</span> such that</p>
<p><span class="LaTeX">$$\|x_i - x_j\| \approx \delta_{i,j}$$</span> for all <span class="LaTeX">$i,j\in {1,\dots,I}$</span>,</p>
<p>where <span class="LaTeX">$\|\cdot\|$</span> is a <a href="Norm_(mathematics)" title="wikilink">vector norm</a>. In classical MDS, this norm is the <a href="Euclidean_distance" title="wikilink">Euclidean distance</a>, but, in a broader sense, it may be a <a href="metric_(mathematics)" title="wikilink">metric</a> or arbitrary distance function.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<p>In other words, MDS attempts to find an <a class="uri" href="embedding" title="wikilink">embedding</a> from the <span class="LaTeX">$I$</span> objects into <span class="LaTeX">$\mathbb{R}^N$</span> such that distances are preserved. If the dimension <span class="LaTeX">$N$</span> is chosen to be 2 or 3, we may plot the vectors <span class="LaTeX">$x_i$</span> to obtain a visualization of the similarities between the <span class="LaTeX">$I$</span> objects. Note that the vectors <span class="LaTeX">$x_i$</span> are not unique: With the Euclidean distance, they may be arbitrarily translated, rotated, and reflected, since these transformations do not change the pairwise distances <span class="LaTeX">$\|x_i - x_j\|$</span>.</p>
<p>(Note: The symbol <span class="LaTeX">$\mathbb{R}$</span> indicates the set of <a href="real_numbers" title="wikilink">real numbers</a>, and the notation <span class="LaTeX">$\mathbb{R}^N$</span> refers to the Cartesian product of <span class="LaTeX">$N$</span> copies of <span class="LaTeX">$\mathbb{R}$</span>, which is an <span class="LaTeX">$N$</span>-dimensional vector space over the field of the real numbers.)</p>
<p>There are various approaches to determining the vectors <span class="LaTeX">$x_i$</span>. Usually, MDS is formulated as an <a href="optimization_(mathematics)" title="wikilink">optimization problem</a>, where <span class="LaTeX">$(x_1,\ldots,x_I)$</span> is found as a minimizer of some cost function, for example,</p>
<p><span class="LaTeX">$$\min_{x_1,\ldots,x_I} \sum_{i<j} (="" )^2.="" -="" <="" \,="" \delta_{i,j}="" \|x_i="" math="" x_j\|="">

A solution may then be found by numerical optimization techniques.  For some particularly chosen cost functions, minimizers can be stated analytically in terms of matrix [[Eigendecomposition of a matrix|eigendecompositions]].{{citation needed|date=September 2012}}

==Procedure==
There are several steps in conducting MDS research:
# '''Formulating the problem''' – What variables do you want to compare?  How many variables do you want to compare? More than 20 is often considered cumbersome. {{Citation needed|date=May 2012}}  Fewer than 8 (4 pairs) will not give valid results. {{Citation needed|date=May 2012}} What purpose is the study to be used for?
# '''Obtaining input data''' – Respondents are asked a series of questions. For each product pair, they are asked to rate similarity (usually on a 7 point [[Likert scale]] from very similar to very dissimilar). The first question could be for Coke/Pepsi for example, the next for Coke/Hires rootbeer, the next for Pepsi/Dr Pepper, the next for Dr Pepper/Hires rootbeer, etc. The number of questions is a function of the number of brands and can be calculated as <math>Q = N (N - 1) / 2$$</span> where <em>Q</em> is the number of questions and <em>N</em> is the number of brands. This approach is referred to as the “Perception data : direct approach”. There are two other approaches. There is the “Perception data : derived approach” in which products are decomposed into attributes that are rated on a <a href="semantic_differential" title="wikilink">semantic differential</a> scale. The other is the “Preference data approach” in which respondents are asked their preference rather than similarity.</p>
<ol>
<li><strong>Running the MDS statistical program</strong> – Software for running the procedure is available in many software for statistics. Often there is a choice between Metric MDS (which deals with interval or ratio level data), and Nonmetric MDS (which deals with ordinal data).</li>
<li><strong>Decide number of dimensions</strong> – The researcher must decide on the number of dimensions they want the computer to create. The more dimensions, the better the statistical fit, but the more difficult it is to interpret the results.</li>
<li><strong>Mapping the results and defining the dimensions</strong> – The statistical program (or a related module) will map the results. The map will plot each product (usually in two-dimensional space). The proximity of products to each other indicate either how similar they are or how preferred they are, depending on which approach was used. How the dimensions of the embedding actually correspond to dimensions of system behavior, however, are not necessarily obvious. Here, a subjective judgment about the correspondence can be made (see <a href="perceptual_mapping" title="wikilink">perceptual mapping</a>).</li>
<li><strong>Test the results for reliability and validity</strong> – Compute <a class="uri" href="R-squared" title="wikilink">R-squared</a> to determine what proportion of variance of the scaled data can be accounted for by the MDS procedure. An R-square of 0.6 is considered the minimum acceptable level.  An R-square of 0.8 is considered good for metric scaling and .9 is considered good for non-metric scaling. Other possible tests are Kruskal’s Stress, split data tests, data stability tests (i.e., eliminating one brand), and test-retest reliability.</li>
<li><strong>Report the results comprehensively</strong> – Along with the mapping, at least distance measure (e.g., <a href="Sorenson_index" title="wikilink">Sorenson index</a>, <a href="Jaccard_index" title="wikilink">Jaccard index</a>) and reliability (e.g., stress value) should be given. It is also very advisable to give the algorithm (e.g., Kruskal, Mather), which is often defined by the program used (sometimes replacing the algorithm report), if you have given a start configuration or had a random choice, the number of runs, the assessment of dimensionality, the <a href="Monte_Carlo" title="wikilink">Monte Carlo method</a> results, the number of iterations, the assessment of stability, and the proportional variance of each axis (r-square).</li>
</ol>
<h2 id="applications">Applications</h2>
<p>Applications include <a href="scientific_visualisation" title="wikilink">scientific visualisation</a> and <a href="data_mining" title="wikilink">data mining</a> in fields such as <a href="cognitive_science" title="wikilink">cognitive science</a>, <a href="information_science" title="wikilink">information science</a>, <a class="uri" href="psychophysics" title="wikilink">psychophysics</a>, <a class="uri" href="psychometrics" title="wikilink">psychometrics</a>, <a class="uri" href="marketing" title="wikilink">marketing</a> and <a class="uri" href="ecology" title="wikilink">ecology</a>. New applications arise in the scope of autonomous wireless nodes that populate a space or an area. MDS may apply as a real time enhanced approach to monitoring and managing such populations.</p>
<p>Furthermore, MDS has been used extensively in <a class="uri" href="geostatistics" title="wikilink">geostatistics</a>, for modeling the spatial variability of the patterns of an image (by representing them as points in a lower-dimensional space),<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> and <a href="natural_language_processing" title="wikilink">natural language processing</a>, for modeling the semantic and affective relatedness of natural language concepts (by representing them as points in a 100-dimensional vector space).<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h3 id="marketing">Marketing</h3>
<p>In <a class="uri" href="marketing" title="wikilink">marketing</a>, MDS is a statistical technique for taking the preferences and perceptions of respondents and representing them on a visual grid, called <a href="perceptual_mapping" title="wikilink">perceptual maps</a>. By mapping multiple attributes and multiple brands at the same time, a greater understanding of the marketplace and of consumers' perceptions can be achieved, as compared with a basic two attribute perceptual map.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h3 id="bioinformatics">Bioinformatics</h3>
<p>MDS is becoming a popular method used in sequence clustering and visualization. In <a class="uri" href="bioinformatics" title="wikilink">bioinformatics</a>, MDS is used to reduce the dimensionality by giving the dissimilarity scores from each pair of sequences. These disimilarity scores are usually calculated using <a href="sequence_alignment" title="wikilink">Sequence Alignment</a>. By mapping each sequence from the high dimensional space to a visually acceptable space (such as 2D/3D space), the correlations between each sequence cluster can be observed easily. <a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>
<h3 id="comparison-and-advantages">Comparison and advantages</h3>
<p>Potential customers are asked to compare pairs of <a href="product_(business)" title="wikilink">products</a> and make judgments about their similarity. Whereas other techniques (such as <a href="factor_analysis" title="wikilink">factor analysis</a>, <a href="discriminant_analysis_(in_marketing)" title="wikilink">discriminant analysis</a>, and <a href="conjoint_analysis_(in_marketing)" title="wikilink">conjoint analysis</a>) obtain underlying dimensions from responses to product attributes identified by the researcher, MDS obtains the underlying dimensions from respondents’ judgments about the similarity of products. This is an important advantage.  It does not depend on researchers’ judgments. It does not require a list of attributes to be shown to the respondents. The underlying dimensions come from respondents’ judgments about pairs of products. Because of these advantages, MDS is the most common technique used in perceptual mapping. </p>
<h2 id="implementations">Implementations</h2>
<ul>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html">MDS</a> in <a href="scikit-learn" title="wikilink">sklearn</a> (<a href="Python_(programming_language)" title="wikilink">Python</a>)</li>
<li><em>cmdscale</em> in ''<a href="R_(programming_language)" title="wikilink">R</a></li>
<li><em>NMS</em> in <a href="http://www.pcord.com">PC-ORD, Multivariate Analysis of Ecological Data</a></li>
<li><a href="Orange_(software)" title="wikilink">Orange</a>, a free data mining software suite, module <a href="http://www.ailab.si/orange/doc/modules/orngMDS.htm">orngMDS</a></li>
<li><a href="http://www.uv.es/visualstats/Book">ViSta</a> has implementations of MDS by Forrest W. Young. Interactive graphics allow exploring the results of MDS in detail.</li>
<li><a href="http://www.usabilitest.com/CardSorting">usabiliTEST's Online Card Sorting</a> software is utilizing MDS to plot the data collected from the participants of usability tests.</li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Positioning_(marketing)" title="wikilink">Positioning (marketing)</a></li>
<li><a href="Perceptual_mapping" title="wikilink">Perceptual mapping</a></li>
<li><a href="Product_management" title="wikilink">Product management</a></li>
<li><a class="uri" href="Marketing" title="wikilink">Marketing</a></li>
<li><a href="Generalized_multidimensional_scaling" title="wikilink">Generalized multidimensional scaling</a> (GMDS)</li>
<li><a href="Data_clustering" title="wikilink">Data clustering</a></li>
<li><a href="Factor_analysis" title="wikilink">Factor analysis</a></li>
<li><a href="Discriminant_analysis" title="wikilink">Discriminant analysis</a></li>
<li><a href="Dimensionality_reduction" title="wikilink">Dimensionality reduction</a></li>
<li><a href="Nonlinear_dimensionality_reduction" title="wikilink">Nonlinear dimensionality reduction</a></li>
<li><a href="Distance_geometry" title="wikilink">Distance geometry</a></li>
<li><a href="Cayley–Menger_determinant" title="wikilink">Cayley–Menger determinant</a></li>
</ul>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.perceptualmaps.com/map-format/multi-attribute-perceptual-maps/">Making MDS Maps on Excel</a></li>
<li><a href="http://www.newmdsx.com/">NewMDSX: Multidimensional Scaling Software</a></li>
<li><a href="http://www.granular.com/MDS/">MDS page</a></li>
<li><a href="http://codingplayground.blogspot.com/2009/05/multidimension-scaling.html">MDS in C++</a> by Antonio Gulli</li>
<li><a href="http://orange.biolab.si/doc/modules/orngMDS.htm">The orngMDS module</a> for MDS from <a href="Orange_(software)" title="wikilink">Orange (software)</a></li>
</ul>
<p>"</p>
<p><a href="Category:Multivariate_statistics" title="wikilink">Category:Multivariate statistics</a> <a href="Category:Market_research" title="wikilink">Category:Market research</a> <a class="uri" href="Category:Psychometrics" title="wikilink">Category:Psychometrics</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="Joseph_Kruskal" title="wikilink">Kruskal, J. B.</a>, and Wish, M. (1978), <em>Multidimensional Scaling</em>, Sage University Paper series on Quantitative Application in the Social Sciences, 07-011. Beverly Hills and London: Sage Publications.<a href="#fnref4">↩</a></li>
<li id="fn5">Honarkhah, M and Caers, J, 2010, <em><a href="http://dx.doi.org/10.1007/s11004-010-9276-7">Stochastic Simulation of Patterns Using Distance-Based Pattern Modeling</a></em>, Mathematical Geosciences, 42: 487–517<a href="#fnref5">↩</a></li>
<li id="fn6">Cambria, E, Song, Y, Wang, H and Howard, N, 2013, '<a href="http://dx.doi.org/10.1109/MIS.2012.118">Semantic multi-dimensional scaling for open-domain sentiment analysis</a>", IEEE Intelligent Systems<a href="#fnref6">↩</a></li>
<li id="fn7">Fripp G, 2014, <a href="http://www.perceptualmaps.com/map-format/multi-attribute-perceptual-maps/">Understanding Perceptual Maps for Marketing</a><a href="#fnref7">↩</a></li>
<li id="fn8">Yang R, 2012, <a href="http://dl.acm.org/citation.cfm?id=2382978">DACIDR: deterministic annealed clustering with interpolative dimension reduction using a large collection of 16S rRNA sequences</a><a href="#fnref8">↩</a></li>
</ol>
</section>
</body>
</html>
