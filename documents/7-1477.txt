   Cook's distance      Cook's distance  In [[statistics]], '''Cook's distance''' or '''Cook's ''D''''' is a commonly used e stimate of the influence of a data point when performing least squares regression analysis . 1 In a practical ordinary least squares analysis, Cook's distance can be used in several ways: to indicate data points that are particularly worth checking for validity; to indicate regions of the design space where it would be good to be able to obtain more data points. It is named after the American statistician R. Dennis Cook , who introduced the concept in 1977. 2 3  Definition  Cook's distance measures the effect of deleting a given observation. Data points with large residuals ( outliers ) and/or high leverage may distort the outcome and accuracy of a regression. Points with a large Cook's distance are considered to merit closer examination in the analysis. It is calculated as:        D  i   =     ∑   j  =  1   n     (      Y  ^   j    -    Y  ^    j   (  i  )      )   2      p   MSE     ,       subscript  D  i       superscript   subscript     j  1    n    superscript     subscript   normal-^  Y   j    subscript   normal-^  Y     j  i     2      p  MSE      D_{i}=\frac{\sum_{j=1}^{n}(\hat{Y}_{j}\ -\hat{Y}_{j(i)})^{2}}{p\ \mathrm{MSE}},     where:        Y  ^   j      subscript   normal-^  Y   j    \hat{Y}_{j}\,   is the prediction from the full regression model for observation j ;        Y  ^    j   (  i  )        subscript   normal-^  Y     j  i     \hat{Y}_{j(i)}\,   is the prediction for observation j from a refitted regression model in which observation i has been omitted;     p   p   p   is the number of fitted parameters in the model;      MSE    MSE   \mathrm{MSE}\,   is the mean square error of the regression model.  The following are the algebraically equivalent expressions (in case of simple linear regression ):        D  i   =     e  i  2     p   MSE     [    h   i  i      (   1  -   h   i  i     )   2    ]     ,       subscript  D  i        superscript   subscript  e  i   2     p  MSE     delimited-[]     subscript  h    i  i     superscript    1   subscript  h    i  i     2        D_{i}=\frac{e_{i}^{2}}{p\ \mathrm{MSE}}\left[\frac{h_{ii}}{(1-h_{ii})^{2}}%
 \right],           D  i   =      (    β  ^   -    β  ^    (   -  i   )     )   T    (    X  T   X   )    (    β  ^   -    β  ^    (   -  i   )     )      (   1  +  p   )    s  2      ,       subscript  D  i        superscript     normal-^  β    superscript   normal-^  β     i     T      superscript  X  T   X      normal-^  β    superscript   normal-^  β     i          1  p    superscript  s  2       D_{i}=\frac{(\hat{\beta}-\hat{\beta}^{(-i)})^{T}(X^{T}X)(\hat{\beta}-\hat{%
 \beta}^{(-i)})}{(1+p)s^{2}},     where:       h   i  i       subscript  h    i  i     h_{ii}\,   is the leverage , i.e., the i-th diagonal element of the hat matrix     𝐗    (    𝐗  T   𝐗   )    -  1     𝐗  T       𝐗   superscript     superscript  𝐗  T   𝐗     1     superscript  𝐗  T     \mathbf{X}\left(\mathbf{X}^{T}\mathbf{X}\right)^{-1}\mathbf{X}^{T}   ;       e  i      subscript  e  i    e_{i}\,   is the residual (i.e., the difference between the observed value and the value fitted by the proposed model).  Detecting highly influential observations  There are different opinions regarding what cut-off values to use for spotting highly influential points . A simple operational guideline of     D  i   >  1       subscript  D  i   1    D_{i}>1   has been suggested. 4 Others have indicated that     D  i   >   4  /  n        subscript  D  i     4  n     D_{i}>4/n   , where   n   n   n   is the number of observations, might be used. 5  A conservative approach relies on the fact that Cook's distance has the form W/p, where W is formally identical to the Wald statistic that one uses for testing that     H  0   :    β  i   =   β  0       normal-:   subscript  H  0      subscript  β  i    subscript  β  0      H_{0}:\beta_{i}=\beta_{0}   using some     β  ^    [   -  i   ]      subscript   normal-^  β    delimited-[]    i      \hat{\beta}_{[-i]}   . Recalling that W/p has an    F   p  ,   n  -  p       subscript  F   p    n  p      F_{p,n-p}   distribution (with p and n-p degrees of freedom), we see that Cook's distance is equivalent to the F statistic for testing this hypothesis, and we can thus use    F   p  ,   n  -  p   ,   1  -  α       subscript  F   p    n  p     1  α      F_{p,n-p,1-\alpha}   as a threshold.  Interpretation  Specifically    D  i     subscript  D  i    D_{i}   can be interpreted as the distance one's estimates move within the confidence ellipsoid that represents a region of plausible values for the parameters. This is shown by an alternative but equivalent representation of Cook's distance in terms of changes to the estimates of the regression parameters between the cases where the particular observation is either included or excluded from the regression analysis.  See also   Outlier  Leverage (statistics)  Partial leverage  DFFITS  Studentized residual   References  Further reading       "  Category:Regression diagnostics  Category:Statistical outliers  Category:Statistical distance measures     ↩  ↩  ↩  ↩  ↩     