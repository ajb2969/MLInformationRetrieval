   Separation principle      Separation principle   In control theory , a separation principle , more formally known as a principle of separation of estimation and control , states that under some assumptions the problem of designing an optimal feedback controller for a stochastic system can be solved by designing an optimal observer for the state of the system, which feeds into an optimal deterministic controller for the system. Thus the problem can be broken into two separate parts, which facilitates the design.  As an example of such a principle, it has been proved that if a stable  observer and stable state feedback are designed for a linear time-invariant system , then the combined observer and feedback will be stable. The separation principle does not hold in general (for example for non-linear systems). Another example is the separation of the linear-quadratic-Gaussian control solution into the Kalman filter and optimal controller for a linear-quadratic regulator . A separation principle also exists for the control of quantum systems.  Proof of separation principle for deterministic LTI systems  Consider a deterministic LTI system:          x  ˙    (  t  )        =    A  x   (  t  )    +   B  u   (  t  )           y   (  t  )        =   C  x   (  t  )              normal-˙  x   t     absent      A  x  t     B  u  t         y  t     absent    C  x  t       \begin{aligned}\displaystyle\dot{x}(t)&\displaystyle=Ax(t)+Bu(t)\\
 \displaystyle y(t)&\displaystyle=Cx(t)\end{aligned}     where      u   (  t  )       u  t    u(t)   represents the input signal,      y   (  t  )       y  t    y(t)   represents the output signal, and      x   (  t  )       x  t    x(t)   represents the internal state of the system.  We can design an observer of the form        x  ^   ˙   =     (   A  -   L  C    )    x  ^    +   B  u   +   L   y          normal-˙   normal-^  x          A    L  C     normal-^  x      B  u     L  y      \dot{\hat{x}}=(A-LC)\hat{x}+Bu+Ly\,     and state feedback        u   (  t  )    =   -   K    x  ^       .        u  t       K   normal-^  x       u(t)=-K\hat{x}\,.     Define the error e :       e  =   x  -    x  ^      .      e    x   normal-^  x      e=x-\hat{x}\,.     Then       e  ˙   =    (   A  -   L  C    )    e         normal-˙  e       A    L  C    e     \dot{e}=(A-LC)e\,           u   (  t  )    =   -   K   (   x  -  e   )      .        u  t       K    x  e       u(t)=-K(x-e)\,.     Now we can write the closed-loop dynamics as        [      x  ˙        e  ˙      ]   =    [      A  -   B  K       B  K       0     A  -   L  C       ]    [     x      e     ]     .         normal-˙  x      normal-˙  e           A    B  K      B  K     0    A    L  C        x    e       \begin{bmatrix}\dot{x}\\
 \dot{e}\\
 \end{bmatrix}=\begin{bmatrix}A-BK&BK\\
 0&A-LC\\
 \end{bmatrix}\begin{bmatrix}x\\
 e\\
 \end{bmatrix}.     Since this is triangular , the eigenvalues are just those of A − BK together with those of A − LC . 1 Thus the stability of the observer and feedback are independent .  References   Brezinski, Claude. Computational Aspects of Linear Control (Numerical Methods and Algorithms) . Springer, 2002.   "  Category:Control theory  Category:Stochastic control     Proof can be found in this math.stackexchange question. ↩     