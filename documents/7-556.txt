   Fano's inequality      Fano's inequality   In information theory , Fano's inequality (also known as the Fano converse and the Fano lemma ) relates the average information lost in a noisy channel to the probability of the categorization error. It was derived by Robert Fano in the early 1950s while teaching a Ph.D. seminar in information theory at MIT , and later recorded in his 1961 textbook.  It is used to find a lower bound on the error probability of any decoder as well as the lower bounds for minimax risks in density estimation .  Let the random variables  X and Y represent input and output messages with a joint probability     P   (  x  ,  y  )       P   x  y     P(x,y)   . Let e represent an occurrence of error; i.e., that    X  ‚â†   X  ~       X   normal-~  X     X\neq\tilde{X}   , being     X  ~   =   f   (  Y  )         normal-~  X     f  Y     \tilde{X}=f(Y)   a noise approximate version of   X   X   X   . Fano's inequality is      H   (  X  |  Y  )   ‚â§  H   (  e  )   +  P   (  e  )   log   (  |  ùí≥  |  -  1  )   ,     fragments  H   fragments  normal-(  X  normal-|  Y  normal-)    H   fragments  normal-(  e  normal-)    P   fragments  normal-(  e  normal-)     fragments  normal-(  normal-|  X  normal-|   1  normal-)   normal-,    H(X|Y)\leq H(e)+P(e)\log(|\mathcal{X}|-1),     where   ùí≥   ùí≥   \mathcal{X}   denotes the support of X ,      H   (  X  |  Y  )   =  -   ‚àë   i  ,  j    P   (   x  i   ,   y  j   )   log  P   (   x  i   |   y  j   )      fragments  H   fragments  normal-(  X  normal-|  Y  normal-)      subscript    i  j    P   fragments  normal-(   subscript  x  i   normal-,   subscript  y  j   normal-)    P   fragments  normal-(   subscript  x  i   normal-|   subscript  y  j   normal-)     H\left(X|Y\right)=-\sum_{i,j}P(x_{i},y_{j})\log P\left(x_{i}|y_{j}\right)     is the conditional entropy ,      P   (  e  )   =  P   (  X  ‚â†   X  ~   )      fragments  P   fragments  normal-(  e  normal-)    P   fragments  normal-(  X    normal-~  X   normal-)     P(e)=P(X\neq\tilde{X})     is the probability of the communication error, and       H   (  e  )    =    -   P   (  e  )    log  P    (  e  )     -    (   1  -   P   (  e  )     )    log   (   1  -   P   (  e  )     )            H  e         P  e    P   e        1    P  e        1    P  e         H(e)=-P(e)\log P(e)-(1-P(e))\log(1-P(e))     is the corresponding binary entropy .  Alternative formulation  Let X be a random variable with density equal to one of    r  +  1      r  1    r+1   possible densities     f  1   ,  ‚Ä¶  ,   f   r  +  1        subscript  f  1   normal-‚Ä¶   subscript  f    r  1      f_{1},\ldots,f_{r+1}   . Furthermore, the Kullback‚ÄìLeibler divergence between any pair of densities cannot be too large,       D   K  L     (   f  i   ‚à•   f  j   )   ‚â§  Œ≤     fragments   subscript  D    K  L     fragments  normal-(   subscript  f  i   parallel-to   subscript  f  j   normal-)    Œ≤    D_{KL}(f_{i}\|f_{j})\leq\beta   for all     i  ‚â†  j   .      i  j    i\not=j.     Let     œà   (  X  )    ‚àà   {  1  ,  ‚Ä¶  ,   r  +  1   }         œà  X    1  normal-‚Ä¶    r  1      \psi(X)\in\{1,\ldots,r+1\}   be an estimate of the index. Then       sup  i    P  i    (  œà   (  X  )   ‚â†  i  )   ‚â•  1  -    Œ≤  +   log  2     log  r       fragments   subscript  supremum  i    subscript  P  i    fragments  normal-(  œà   fragments  normal-(  X  normal-)    i  normal-)    1       Œ≤    2      r      \sup_{i}P_{i}(\psi(X)\not=i)\geq 1-\frac{\beta+\log 2}{\log r}   where    P  i     subscript  P  i    P_{i}   is the probability induced by    f  i     subscript  f  i    f_{i}     Generalization  The following generalization is due to Ibragimov and Khasminskii (1979), Assouad and Birge (1983).  Let F be a class of densities with a subclass of r +¬†1 densities ∆í Œ∏ such that for any Œ∏ ‚â† Œ∏ ‚Ä≤         ‚à•    f  Œ∏   -   f   Œ∏  ‚Ä≤     ‚à•    L  1    ‚â•  Œ±   ,       subscript   norm     subscript  f  Œ∏    subscript  f   superscript  Œ∏  normal-‚Ä≤       subscript  L  1    Œ±    \|f_{\theta}-f_{\theta^{\prime}}\|_{L_{1}}\geq\alpha,\,          D   K  L     (   f  Œ∏   ‚à•   f   Œ∏  ‚Ä≤    )   ‚â§  Œ≤  .     fragments   subscript  D    K  L     fragments  normal-(   subscript  f  Œ∏   parallel-to   subscript  f   superscript  Œ∏  normal-‚Ä≤    normal-)    Œ≤  normal-.    D_{KL}(f_{\theta}\|f_{\theta^{\prime}})\leq\beta.\,     Then in the worst case the expected value of error of estimation is bound from below,        sup   f  ‚àà  ùêÖ     E    ‚à•    f  n   -  f   ‚à•    L  1      ‚â•    Œ±  2    (   1  -     n  Œ≤   +   log  2     log  r     )          subscript  supremum    f  ùêÖ      E   subscript   norm     subscript  f  n   f     subscript  L  1          Œ±  2     1        n  Œ≤     2      r        \sup_{f\in\mathbf{F}}E\|f_{n}-f\|_{L_{1}}\geq\frac{\alpha}{2}\left(1-\frac{n%
 \beta+\log 2}{\log r}\right)     where ∆í n is any density estimator based on a sample of size n .  References   P. Assouad, "Deux remarques sur l'estimation", Comptes Rendus de L'Academie des Sciences de Paris , Vol. 296, pp.¬†1021‚Äì1024, 1983.  L. Birge, "Estimating a density under order restrictions: nonasymptotic minimax risk", Technical report, UER de Sciences √âconomiques, Universite Paris X, Nanterre, France, 1983.  T. Cover, J. Thomas, Elements of Information Theory . pp.¬†43.  L. Devroye, A Course in Density Estimation . Progress in probability and statistics, Vol 14. Boston, Birkhauser, 1987. ISBN 0-8176-3365-0, ISBN 3-7643-3365-0.  R. Fano, Transmission of information; a statistical theory of communications. Cambridge, Massachusetts, M.I.T. Press, 1961. ISBN 0-262-06001-9  R. Fano, Fano inequality  Scholarpedia , 2008.  I. A. Ibragimov, R. Z. Has‚Ä≤minskii, Statistical estimation, asymptotic theory . Applications of Mathematics, vol. 16, Springer-Verlag, New York, 1981. ISBN 0-387-90523-5   "  Category:Information theory  Category:Inequalities   