   Two-way analysis of variance      Two-way analysis of variance   In statistics , the two-way analysis of variance (ANOVA) is an extension of the one-way ANOVA that examines the influence of two different categorical  independent variables on one continuous  dependent variable . The two-way ANOVA not only aims at assessing the main effect of each independent variable but also if there is any interaction between them.  History  In 1925, Ronald Fisher mentions the two-way ANOVA in his celebrated book from 1925 , Statistical Methods for Research Workers (chapters 7 and 8). In 1934 , Frank Yates published procedures for the unbalanced case. 1 Since then, an extensive literature has been produced, reviewed in 1993 by Yasunori Fujikoshi . 2 In 2005 , Andrew Gelman proposed a different approach of ANOVA, viewed as a multilevel model . 3  Data set  Let us imagine a data set for which a dependent variable may be influenced by two factors which are potential sources of variation. The first factor has   I   I   I   levels  and the second has   J   J   J   levels . Each combination    (  i  ,  j  )     i  j    (i,j)   defines a treatment , for a total of    I  √ó  J      I  J    I\times J   treatments. We represent the number of replicates for treatment    (  i  ,  j  )     i  j    (i,j)   by    n   i  j      subscript  n    i  j     n_{ij}   , and let   k   k   k   be the index of the replicate in this treatment .  From these data, we can build a contingency table , where     n   i  +    =    ‚àë   j  =  1   J    n   i  j          subscript  n   limit-from  i       superscript   subscript     j  1    J    subscript  n    i  j       n_{i+}=\sum_{j=1}^{J}n_{ij}   and     n   +  j    =    ‚àë   i  =  1   I    n   i  j          subscript  n    j      superscript   subscript     i  1    I    subscript  n    i  j       n_{+j}=\sum_{i=1}^{I}n_{ij}   , and the total number of replicates is equal to    n  =    ‚àë   i  ,  j     n   i  j     =    ‚àë  i    n   i  +     =    ‚àë  j    n   +  j           n    subscript    i  j     subscript  n    i  j            subscript   i    subscript  n   limit-from  i             subscript   j    subscript  n    j        n=\sum_{i,j}n_{ij}=\sum_{i}n_{i+}=\sum_{j}n_{+j}   .  The experimental design is balanced if each treatment has the same number of replicates,   K   K   K   . In such a case, the design is also said to be orthogonal , allowing to fully distinguish the effects of both factors. We hence can write      ‚àÄ  i   ,    j    n   i  j      =  K        for-all  i     j   subscript  n    i  j      K    \forall i,j\;n_{ij}=K   , and      ‚àÄ  i   ,    j    n   i  j      =     n   i  +    √ó   n   +  j     n         for-all  i     j   subscript  n    i  j           subscript  n   limit-from  i      subscript  n    j     n     \forall i,j\;n_{ij}=\frac{n_{i+}\times n_{+j}}{n}   .  Model  Upon observing variation among all   n   n   n   data points, for instance via a histogram , " probability may be used to describe such variation". 4 Let us hence denote by    Y   i  j  k      subscript  Y    i  j  k     Y_{ijk}   the random variable which observed value    y   i  j  k      subscript  y    i  j  k     y_{ijk}   is the   k   k   k   -th measure for treatment    (  i  ,  j  )     i  j    (i,j)   . The two-way ANOVA models all these variables as varying independently and normally around a mean,    Œº   i  j      subscript  Œº    i  j     \mu_{ij}   , with a constant variance,    œÉ  2     superscript  œÉ  2    \sigma^{2}   ( homoscedasticity ):        Y   i  j  k     |   Œº   i  j    ,    œÉ  2      ‚àº   i  .  i  .  d  .     ùí©   (   Œº   i  j    ,   œÉ  2   )      fragments   subscript  Y    i  j  k    normal-|   subscript  Œº    i  j    normal-,   superscript  œÉ  2     fragments  i  normal-.  i  normal-.  d  normal-.   similar-to   N   fragments  normal-(   subscript  Œº    i  j    normal-,   superscript  œÉ  2   normal-)     Y_{ijk}\,|\,\mu_{ij},\sigma^{2}\;\overset{i.i.d.}{\sim}\;\mathcal{N}(\mu_{ij},%
 \sigma^{2})   .  Specifically, the mean of the response variable is modeled as a linear combination of the explanatory variables:       Œº   i  j    =   Œº  +   Œ±  i   +   Œ≤  j   +   Œ≥   i  j          subscript  Œº    i  j      Œº   subscript  Œ±  i    subscript  Œ≤  j    subscript  Œ≥    i  j       \mu_{ij}=\mu+\alpha_{i}+\beta_{j}+\gamma_{ij}   ,  where   Œº   Œº   \mu   is the grand mean,    Œ±  i     subscript  Œ±  i    \alpha_{i}   is the additive main effect of level   i   i   i   from the first factor (i-th row in the contigency table),    Œ≤  j     subscript  Œ≤  j    \beta_{j}   is the additive main effect of level   j   j   j   from the second factor (j-th column in the contigency table) and    Œ≥   i  j      subscript  Œ≥    i  j     \gamma_{ij}   is the non-additive interaction effect of treatment    (  i  ,  j  )     i  j    (i,j)   from both factors (cell at row i and column j in the contigency table).  An other, equivalent way of describing the two-way ANOVA is by mentioning that, besides the variation explained by the factors, there remains some statistical noise . This amount of unexplained variation is handled via the introduction of one random variable per data point,    œµ   i  j  k      subscript  œµ    i  j  k     \epsilon_{ijk}   , called error . These   n   n   n   random variables are seen as deviations from the means, and are assumed to be independent and normally distributed:       Y   i  j  k    =    Œº   i  j    +    œµ   i  j  k    with   œµ   i  j  k     ‚àº   i  .  i  .  d  .    ùí©   (  0  ,   œÉ  2   )          subscript  Y    i  j  k       subscript  Œº    i  j       subscript  œµ    i  j  k    with   subscript  œµ    i  j  k      fragments  i  normal-.  i  normal-.  d  normal-.   similar-to   ùí©   0   superscript  œÉ  2        Y_{ijk}=\mu_{ij}+\epsilon_{ijk}\text{ with }\epsilon_{ijk}\overset{i.i.d.}{%
 \sim}\mathcal{N}(0,\sigma^{2})   .  Assumptions  Following Gelman and Hill, the assumptions of the ANOVA, and more generally the general linear model , are, in decreasing order of importance: 5   the data points are relevant with respect to the scientific question under investigation;  the mean of the response variable is influenced additively (if not interaction term) and linearly by the factors;  the errors are independent;  the errors have the same variance;  the errors are normally distributed.   Parameter estimation  To ensure identifiability of parameters, we can add the following "sum-to-zero" constraints:        ‚àë  i    Œ±  i    =    ‚àë  j    Œ≤  j    =    ‚àë  i     ‚àë  j    Œ≥   i  j      =  0          subscript   i    subscript  Œ±  i      subscript   j    subscript  Œ≤  j           subscript   i     subscript   j    subscript  Œ≥    i  j           0     \sum_{i}\alpha_{i}=\sum_{j}\beta_{j}=\sum_{i}\sum_{j}\gamma_{ij}=0     Hypothesis testing  In the classical approach, testing null hypotheses (that the factors have no effect) is achieved via their significance which requires calculating sums of squares .  Testing if the interaction term is significant can be difficult because of the potentially-large number of degrees of freedom . 6  See also   Analysis of variance  One-way ANOVA  F test ( Includes a one-way ANOVA example )  Repeated measures ANOVA  Multivariate analysis of variance (MANOVA)  Tukey's test of additivity  Mixed model   References     Notes  "  Category:Statistical methods  Category:Regression analysis  Category:Analysis of variance     ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©     