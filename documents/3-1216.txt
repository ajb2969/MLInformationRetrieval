


Information geometry




Information geometry

Information geometry is a branch of mathematics that applies the techniques of differential geometry to the field of probability theory. This is done by taking probability distributions for a statistical model as the points of a Riemannian manifold, forming a statistical manifold. The Fisher information metric provides the Riemannian metric.
Information geometry reached maturity through the work of Shun'ichi Amari and other Japanese mathematicians in the 1980s. Amari and Nagaoka's book, Methods of Information Geometry,1 is cited by most works of the relatively young field due to its broad coverage of significant developments attained using the methods of information geometry up to the year 2000. Many of these developments were previously only available in Japanese-language publications.
Introduction
The following introduction is based on Methods of Information Geometry.2
Information and probability
Define an n-set to be a set V with cardinality 
 
 
 
 . To choose an element v (value, state, point, outcome) from an n-set V, one needs to specify 
 
 
 
 
  b-sets (default b=2), if one disregards all but the cardinality. That is, 
 
 
nats of information are required to specify v; equivalently, 
 
 
bits are needed.
By considering the occurrences 
 
 
 
  of values from 
 
 
 
 , one has an alternate way to refer to 
 
 
 
 
 , through 
 
 
 
 . First, one chooses an occurrence 
 
 
 
 , which requires information of 
 
 
 
  bits. To specify v, one subtracts the excess information used to choose one 
 
 
 
  from all those linked to 
 
 
 
 
 , this is 
 
 
 
 . Then, 
 
 
 
  is the number of 
 
 
 
  portions fitting into 
 
 
 
 . Thus, one needs 
 
 
 
 
  bits to choose one of them. So the information (variable size, code length, number of bits) needed to refer to 
 
 
 
 , considering its occurrences in a message is



Finally, 
 
 
 
  is the normalized portion of information needed to code all occurrences of one 
 
 
 
 . The averaged code length over all values is 
 
 
 
 
 . 
 
 
 
  is called the entropy of a random variable

 
 .
Statistical model, Parameters
With a probability distribution

 
  one looks at a variable 
 
 
 
  through an observation context like a message or an experimental setup.
The context can often be identified by a set of parameters through combinatorial reasoning. The parameters can have an arbitrary number of dimensions and can be very local or less so, as long as the context given by a certain 
 
 
 
 
  produces every value of 
 
 
 
 , i.e. the support

 
  does not change as function of 
 
 
 
 . Every 
 
 
 
  determines one probability distribution for 
 
 
 
 
 . Basically all distributions for which there exists an explicit analytical formula fall into this category (Binomial, Normal, Poisson, ...). The parameters in these cases have a concrete meaning in the underlying setup, which is a statistical model for the context of 
 
 
 
 .
The parameters are quite different in nature from 
 
 
 
  itself, because they do not describe 
 
 
 
 , but the observation context for 
 
 
 
 .
A parameterization of the form


 
  with


 
  and 
 
 
 
 , that mixes different distributions 
 
 
 
 , is called a mixture distribution, mixture or 
 
 
 
 -parameterization or mixture for short. All such parameterizations are related through an affine transformation

 
 
 . A parameterization with such a transformation rule is called flat.
A flat parameterization for 
 
 
 
  is an exponential or 
 
 
 
  parameterization, because the parameters are in the exponent of 
 
 
 
 . There are several important distributions, like Normal and Poisson, that fall into this category. These distributions are collectively referred to as exponential family or 
 
 
 
 -family. The 
 
 
 
 
 -manifold for such distributions is not affine, but the 
 
 
 
  manifold is. This is called 
 
 
 
 -affine. The parameterization 
 
 
 
  for the exponential family can be mapped to the one above by making 
 
 
 
  another parameter and extend 
 
 
 
 
 .
Differential geometry applied to probability
In information geometry, the methods of differential geometry are applied to describe the space of probability distributions for one variable 
 
 
 
 . This is done by using a coordinate or atlas

 
 . Furthermore, the probability 
 
 
 
  must be a differentiable and invertible function of 
 
 
 
 . In this case, the 
 
 
 
 
  are coordinates of the 
 
 
 
 -space, and the latter is a differential manifold

 
 .
Derivatives are defined as is usual for a differentiable manifold:



with 
 
 
 
 , for 
 
 
 
 
  a real-valued function on 
 
 
 
 .
Given a function 
 
 
 
  on 
 
 
 
 , one may "geometrize" it by taking it to define a new manifold. This is done by defining coordinate functions on this new manifold as


 
 . In this way one "geometricizes" a function 
 
 
 
 
 , by encoding it into the coordinates used to describe the system.
For 
 
 
 
  the inverse is 
 
 
 
  and the resulting manifold of 
 
 
 
  points is called the 
 
 
 
 -representation. The 
 
 
 
 
  manifold itself is called the 
 
 
 
 -representation. The 
 
 
 
 - or 
 
 
 
 -representations, in the sense used here, does not refer to the parameterization families of the distribution.
Tangent space
In standard differential geometry, the tangent space on a manifold 
 
 
 
  at a point 
 
 
 
 
  is given by:



In ordinary differential geometry, there is no canonical coordinate system on the manifold; thus, typically, all discussion must be with regard to an atlas, that is, with regard to functions on the manifold. As a result, tangent spaces and vectors are defined as operators acting on this space of functions. So, for example, in ordinary differential geometry, the basis vectors of the tangent space are the operators 
 
 
 
 .
However, with probability distributions 
 
 
 
 , one can calculate value-wise. So it is possible to express a tangent space vector directly as 
 
 
 
  ( 
 
 
 
 
 -representation ) or 
 
 
 
  ( 
 
 
 
 -representation ), and not as operators.
alpha representation
Important functions 
 
 
 
  of 
 
 
 
  are coded by a parameter 
 
 
 
 
  with the important values 
 
 
 
 , 
 
 
 
  and 
 
 
 
 :

mixed or 
 
 
 
 -representation ( 
 
 
 
 
  )
 
 

exponential or 
 
 
 
 -representation ( 
 
 
 
  )
 
 
 
  )


 
 
 -representation ( 
 
 
 
  )
 
 
 
  ( 
 
 
 
  )

Distributions that allow a flat parameterization 
 
 
 
  are called collectively 
 
 
 
 
 -family ( 
 
 
 
 -, 
 
 
 
 - or 
 
 
 
 -family ) of distributions and the according manifold is called 
 
 
 
 -affine.
The 
 
 
 
 
  tangent vector is 
 
 
 
 .
Inner product
One may introduce an inner product on the tangent space of manifold 
 
 
 
  at point 
 
 
 
  as a linear, symmetric and positive definite map


 
 .
This allows a Riemannian metric to be defined; the resulting manifold is a Riemannian manifold. All of the usual concepts of ordinary differential geometry carry over, including the norm


 
 , the line element

 
 , the volume element

 
 , and the cotangent space


 
  that is, the dual space to the tangent space 
 
 
 
 . From these, one may construct tensors, as usual.
Fisher metric as inner product
For probability manifolds such an inner product is given by the Fisher information metric.
Here are equivalent formulas of the Fisher information metric.





 
 , the 
 
 
 
  base vector in the 
 
 
 
 -representation, is also called the score.


 
 ,
 because 
 
 






 
 . This is the same for 
 
 
 
  and 
 
 
 
  families.




 
  with mimimum 
 
 
 
  for 
 
 
 
  entails 
 
 
 
  and 
 
 


 
  is applied only to the first parameter, and 
 
 
 
  only to the second.

 
  is the Kullback-Leibler divergence or relative entropy applicable to the 
 
 
 
 -families.


 For 
 
 
 
  one has 
 
 
 
 .

 
  is the Hellinger distance applicable to the 
 
 
 
 -family. 
 
 
 
 
  also evaluates to the Fisher metric.

This relation with a divergence 
 
 
 
  will be revisited further down.
The Fisher metric is motivated by

it satisfying the requirements for an inner product
its invariance for a sufficient statistic deterministic mapping from one variable to another and more general 
 
 
 
  for 
 
 
 
 , i.e. a broadened distribution has smaller 
 
 
 
 .
it being the Cramér–Rao bound.

 
 
 , therefore any 
 
 
 
  satisfying 
 
 
 
  belongs to 
 
 
 
 .
 For any 
 
 
 
  one has 
 
 
 
 
 , therefore 
 
 
 
 .

 
 .
 So 
 
 
 
  and therefore 
 
 
 
 .

 
 
  and with inefficient estimator one gets the Cramér–Rao bound 
 
 
 
 .

Affine connection
Like commonly done on Riemann manifolds, one may define an affine connection (or covariant derivative)



Given vector fields

 
  and 
 
 
 
  lying in the tangent bundle

 
 
 , the affine connection 
 
 
 
  describes how to differentiate the vector field 
 
 
 
  along the direction
 
 
 
 . It is itself a vector field; it is the sum of the infinitesimal change in the vector field 
 
 
 
 , as one moves along the direction 
 
 
 
 
 , plus the infinitessimal change of the vector 
 
 
 
  due to its parallel transport along the direction 
 
 
 
 . That is, it takes into account the changing nature of what it means to move a coordinate system in a "parallel" fashion, as one moves about in the manifold. In terms of the basis vectors 
 
 
 
 , one has the components:



The 
 
 
 
 
  are Christoffel symbols. The affine connection may be used for defining curvature and torsion, like is usual in Riemannian geometry.
Alpha connection
A non-metric connection is not determined by a metric tensor

 
 ; instead, it is and restricted by the requirement that the parallel transport

 
  between points 
 
 
 
  and 
 
 
 
  must be a linear combination of the base vectors in 
 
 
 
 
 . Here,


 
  expresses the parallel transport of 
 
 
 
  as linear combination of the base vectors in 
 
 
 
 , i.e. the new 
 
 
 
  minus the change. Note that it is not a tensor (does not transform as a tensor).
For such a metric, one can construct a dual connection 
 
 
 
 
  to make


 
 , for parallel transport using 
 
 
 
  and 
 
 
 
 .
For the mentioned 
 
 
 
 -families the affine connection is called the 
 
 
 
 
 -connection and can also be expressed in more ways.











For 
 
 
 
 :



 
 
  is a metric connection and 
 
 
 
  with 
 
 
 
 .


 
 ,


i.e. 
 
 
 
  is dual to 
 
 
 
 
  with respect to the Fisher metric.
 


If 
 
 
 
  this is called 
 
 
 
 -affine. Its dual is then 
 
 
 
 -affine.



 
 ,

i.e. 0-affine, and hence 
 
 
 
 
 , i.e. 1-affine.
 

Divergence
A function of two distributions (points) 
 
 
 
  with minimum 
 
 
 
  for 
 
 
 
  entails 
 
 
 
  and 
 
 
 
 
 . 
 
 
 
  is applied only to the first parameter, and 
 
 
 
  only to the second. 
 
 
 
  is the direction, which brought the two points to be equal, when applied to the first parameter, and to diverge again, when applied to the second parameter, i.e. 
 
 
 
 . The sign cancels in 
 
 
 
 
 , which we can define to be a metric 
 
 
 
 , if always positive.
The absolute derivative of 
 
 
 
  along 
 
 
 
  yields candidates for dual connections 
 
 
 
 . This metric and the connections relate to the Taylor series expansion 
 
 
 
 
  for the first parameter or second parameter. Here for the first parameter:



The term 
 
 
 
  is called the divergence or contrast function. A good choice is 
 
 
 
  with 
 
 
 
 
  convex for 
 
 
 
 . From Jensen's inequality it follows that 
 
 
 
  and, for 
 
 
 
 , we have


 
  which is the Kullback-Leibler divergence or relative entropy applicable to the 
 
 
 
 
 -families. In the above,


 
  is the Fisher metric. For 
 
 
 
  a different 
 
 
 
  yields


 
  The Hellinger distance applicable to the 
 
 
 
 -family is


 
  In this case, 
 
 
 
  also evaluates to the Fisher metric.
Canonical divergence
We now consider two manifolds 
 
 
 
  and 
 
 
 
 , represented by two sets of coordinate functions

 
  and 
 
 
 
 . The corresponding tangent space basis vectors will be denoted by 
 
 
 
  and 
 
 
 
 . The bilinear map 
 
 
 
  associates a quantity 
 
 
 
  to the dual base vectors. This defines an affine connection 
 
 
 
  for 
 
 
 
  and affine connection 
 
 
 
  for 
 
 
 
  that keep 
 
 
 
  constant for parallel transport of 
 
 
 
  and 
 
 
 
 , defined through 
 
 
 
  and 
 
 
 
 .
If 
 
 
 
  is flat, then there exists a coordinate system 
 
 
 
 , that does not change over 
 
 
 
 . In order to keep 
 
 
 
  constant, 
 
 
 
  must not change either, i.e.

 
  is also flat. Furthermore, in this case, we can choose coordinate systems such that



If 
 
 
 
  results as a function 
 
 
 
  on 
 
 
 
 , then making 
 
 
 
 , both coordinate system function sets describe 
 
 
 
 . The connections are such, though, that 
 
 
 
  makes 
 
 
 
  flat and 
 
 
 
  makes 
 
 
 
  flat. This dual space is denoted as 
 
 
 
 .

Because of the linear transform between the flat coordinate systems, we have 
 
 
 
  and 
 
 
 
 .
Because 
 
 
 
  and so for 
 
 
 
  it is possible to define two potentials 
 
 
 
  and 
 
 
 
  through 
 
 
 
  and 
 
 
 
  ( Legendre transform ).These are 
 
 
 
  and 
 
 
 
 .
Then
 
 

 
  and


 
 .







This naturally leads to the following definition of a canonical divergence:



Note the summation that is a representation of the metric due to 
 
 
 
 .
Properties of divergence
The meaning of the canonical divergence depends on the meaning of the metric 
 
 
 
  and vice versa ( 
 
 
 
  ). For the 
 
 
 
  metric (Fisher metric) with the dual connections this is the relative entropy. For the self-dual Euclidean space 
 
 
 
  leads to 
 
 

Similar to the Euclidean space the following holds:

Triangular relation
 
 
 
  (just substitute 
 
 
 
 )
 If 
 
 
 
  is not dually flat then this generalizes to:


 The last part drops in case of dual flatness. 
 
 
 
  is the exponential map.
Pythagorean Theorem: For 
 
 
 
  and 
 
 
 
  meeting on orthogonal lines at 
 
 
 
  ( 
 
 
 
  )


D(p| |r)=D(p| |q)+D(q| |r) 
 For 
 
 
 
  and 
 
 
 
  with 
 
 
 
  a 
 
 
 
 -autoparallel sub-manifold 
 
 
 
  implies that the 
 
 
 
 -geodesic connecting 
 
 
 
  and 
 
 
 
  is orthogonal to 
 
 
 
 .

By projecting 
 
 
 
  onto 
 
 
 
  of a curve 
 
 
 
  one can calculate
  the divergence of the curve 
 
 
 
  where 
 
 

  and 
 
 
 
  with 
 
 
 
 .
  With 
 
 
 
  this becomes 
 
 
 
 .

For an autoparallel sub-manifold parallel transport in it can be expressed with the sub-manifold's base vectors, i.e. 
 
 
 
 . A one-dimensional autoparallel sub-manifold is a geodesic.
Canonical divergence for the exponential family
For the exponential family 
 
 
 
  one has 
 
 
 
 . Applying 
 
 
 
  on both sides yields 
 
 
 
 . The other potential 
 
 
 
  ( 
 
 
 
  is entropy, 
 
 
 
  and 
 
 
 
  was used). 
 
 
 
  is the covariance of 
 
 
 
 , the Cramér–Rao bound, i.e. an efficient estimator must be exponential.
The canonical divergence is given by the Kullback-Leibler divergence 
 
 
 
  and the triangulation is 
 
 
 
 .
The minimal divergence to a sub-manifold given by a restriction like some constant 
 
 
 
  means maximizing 
 
 
 
 . With 
 
 
 
  this corresponds to the maximum entropy principle.
Canonical divergence for general alpha families
For general 
 
 
 
 -affine manifolds with 
 
 
 
  one has:



The connection induced by the divergence is not flat unless 
 
 
 
 . Then the Pythagorean theorem for two curves intersecting orthogonally at 
 
 
 
  is:



History
The history of information geometry is associated with the discoveries of at least the following people, and many others

Sir Ronald Aylmer Fisher
Harald Cramér
Calyampudi Radhakrishna Rao
Harold Jeffreys
Solomon Kullback
Jean-Louis Koszul
Richard Leibler
Claude Shannon
Imre Csiszár
Cencov
Bradley Efron
Paul Vos
Shun'ichi Amari
Hiroshi Nagaoka
Robert Kass
Shinto Eguchi
Ole Barndorff-Nielsen
Frank Nielsen
Giovanni Pistone
Bernard Hanzon
Damiano Brigo

Applications
Information geometry can be applied where parametrized distributions play a role.
Here an incomplete list:

statistical inference
time series and linear systems
quantum systems
neuronal networks
machine learning
statistical mechanics
biology
statistics
mathematical finance

See also

Ruppeiner geometry

References


Further reading

Shun'ichi Amari, Hiroshi Nagaoka - Methods of information geometry, Translations of mathematical monographs; v. 191, American Mathematical Society, 2000 (ISBN 978-0821805312)
Shun'ichi Amari - Differential-geometrical methods in statistics, Lecture notes in statistics, Springer-Verlag, Berlin, 1985.
M. Murray and J. Rice - Differential geometry and statistics, Monographs on Statistics and Applied Probability 48, Chapman and Hall, 1993.
R. E. Kass and P. W. Vos - Geometrical Foundations of Asymptotic Inference, Series in Probability and Statistics, Wiley, 1997.
N. N. Cencov - Statistical Decision Rules and Optimal Inference, Translations of Mathematical Monographs; v. 53, American Mathematical Society, 1982
Giovanni Pistone, and Sempi, C. (1995). "An infinitedimensional geometric structure on the space of all the probability measures equivalent to a given one", Annals of Statistics. 23 (5), 1543–1561.
Brigo, D, Hanzon, B, Le Gland, F, "Approximate nonlinear filtering by projection on exponential manifolds of densities", Bernoulli, 1999, Vol: 5, Pages: 495 - 534, ISSN: 1350-7265
Brigo, D, Diffusion Processes, "Manifolds of Exponential Densities, and Nonlinear Filtering", In: Ole E. Barndorff-Nielsen and Eva B. Vedel Jensen, editor, Geometry in Present Day Science, World Scientific, 1999
Arwini, Khadiga, Dodson, C. T. J. Information Geometry - Near Randomness and Near Independence, Lecture Notes in Mathematics Vol. 1953, Springer 2008 ISBN 978-3-540-69391-8
Th. Friedrich, "Die Fisher-Information und symplektische Strukturen", Math. Nachrichten 153 (1991), 273-296.

External links

Information Geometry overview by Cosma Rohilla Shalizi, July 2010
Information Geometry notes by John Baez, November 2012
blog Computational Information Geometry Wonderland by Frank Nielsen
pdf Information geometry for neural networks by Daniel Wagenaar

"
Category:Differential geometry Category:Information theory Category:Probability theory Category:Statistical theory Category:Category theory



Shun'ichi Amari, Hiroshi Nagaoka - Methods of information geometry, Translations of mathematical monographs; v. 191, American Mathematical Society, 2000 (ISBN 978-0821805312)↩





