   Leave-one-out error      Leave-one-out error    Leave-one-out cross-validation (CVloo) Stability An algorithm f has CVloo stability Œ≤ with respect to the loss function V if the following holds:       ‚àÄ  i  ‚àà   {  1  ,  ‚Ä¶  ,  m  }   ,   ‚Ñô  S    {   sup   z  ‚àà  Z    |  V   (   f  S   ,   z  i   )   -  V   (   f   S   |  i     ,   z  i   )   |  ‚â§   Œ≤   C  V    }   ‚â•  1  -   Œ¥   C  V       fragments  for-all  i    fragments  normal-{  1  normal-,  normal-‚Ä¶  normal-,  m  normal-}   normal-,   subscript  ‚Ñô  S    fragments  normal-{   subscript  supremum    z  Z    normal-|  V   fragments  normal-(   subscript  f  S   normal-,   subscript  z  i   normal-)    V   fragments  normal-(   subscript  f   superscript  S   fragments  normal-|  i     normal-,   subscript  z  i   normal-)   normal-|    subscript  Œ≤    C  V    normal-}    1    subscript  Œ¥    C  V      \forall i\in\{1,...,m\},\mathbb{P}_{S}\{\sup_{z\in Z}|V(f_{S},z_{i})-V(f_{S^{|%
 i}},z_{i})|\leq\beta_{CV}\}\geq 1-\delta_{CV}      Expected-to-leave-one-out error (    E  l  o   o   e  r  r        E  l  o   subscript  o    e  r  r      Eloo_{err}   ) Stability An algorithm f has    E  l  o   o   e  r  r        E  l  o   subscript  o    e  r  r      Eloo_{err}   stability if for each n there exists a    Œ≤   E  L   m     superscript   subscript  Œ≤    E  L    m    \beta_{EL}^{m}   and a    Œ¥   E  L   m     superscript   subscript  Œ¥    E  L    m    \delta_{EL}^{m}   such that:       ‚àÄ  i  ‚àà   {  1  ,  ‚Ä¶  ,  m  }   ,   ‚Ñô  S    {  |  I   [   f  S   ]   -   1  m    ‚àë   i  =  1   m   V   (   f   S   |  i     ,   z  i   )   |  ‚â§   Œ≤   E  L   m   }   ‚â•  1  -   Œ¥   E  L   m      fragments  for-all  i    fragments  normal-{  1  normal-,  normal-‚Ä¶  normal-,  m  normal-}   normal-,   subscript  ‚Ñô  S    fragments  normal-{  normal-|  I   fragments  normal-[   subscript  f  S   normal-]      1  m    superscript   subscript     i  1    m   V   fragments  normal-(   subscript  f   superscript  S   fragments  normal-|  i     normal-,   subscript  z  i   normal-)   normal-|    superscript   subscript  Œ≤    E  L    m   normal-}    1    superscript   subscript  Œ¥    E  L    m     \forall i\in\{1,...,m\},\mathbb{P}_{S}\{|I[f_{S}]-\frac{1}{m}\sum_{i=1}^{m}V(f%
 _{S^{|i}},z_{i})|\leq\beta_{EL}^{m}\}\geq 1-\delta_{EL}^{m}   , with    Œ≤   E  L   m     superscript   subscript  Œ≤    E  L    m    \beta_{EL}^{m}   and    Œ¥   E  L   m     superscript   subscript  Œ¥    E  L    m    \delta_{EL}^{m}   going to zero for    n  ‚Üí  inf     normal-‚Üí  n  infimum    n\rightarrow\inf     Preliminary Notations  X and Y ‚äÇ R being respectively an input and an output space, we consider a training set      S  =   {   z  1   =   (   x  1   ,   y  1   )   ,  .  .  ,   z  m   =   (   x  m   ,   y  m   )   }      fragments  S    fragments  normal-{   subscript  z  1     fragments  normal-(   subscript  x  1   normal-,   subscript  y  1   normal-)   normal-,  normal-.  normal-.  normal-,   subscript  z  m     fragments  normal-(   subscript  x  m   normal-,   subscript  y  m   normal-)   normal-}     S=\{z_{1}=(x_{1},\ y_{1})\ ,..,\ z_{m}=(x_{m},\ y_{m})\}   of size m in    Z  =   X  √ó  Y       Z    X  Y     Z=X\times Y   drawn i.i.d. from an unknown distribution D. A learning algorithm is a function   f   f   f   from    Z  m     subscript  Z  m    Z_{m}   into    F  ‚äÇ   Y  X       F    Y  X     F\subset YX   which maps a learning set S onto a function    f  S     subscript  f  S    f_{S}   from X to Y. To avoid complex notation, we consider only deterministic algorithms . It is also assumed that the algorithm   f   f   f   is symmetric with respect to S, i.e. it does not depend on the order of the elements in the training set. Furthermore, we assume that all functions are measurable and all sets are countable which does not limit the interest of the results presented here.  The loss of an hypothesis f with respect to an example    z  =   (  x  ,  y  )       z   x  y     z=(x,y)   is then defined as     V   (  f  ,  z  )    =   V   (   f   (  x  )    ,  y  )          V   f  z      V     f  x   y      V(f,z)=V(f(x),y)   . The empirical error of f is      I  S    [  f  ]    =    1  n    ‚àë   V   (  f  ,   z  i   )             subscript  I  S    delimited-[]  f        1  n       V   f   subscript  z  i         I_{S}[f]=\frac{1}{n}\sum V(f,z_{i})   .  The true error of f is     I   [  f  ]    =    ùîº  z   V   (  f  ,  z  )          I   delimited-[]  f       subscript  ùîº  z   V   f  z      I[f]=\mathbb{E}_{z}V(f,z)     Given a training set S of size m, we will build, for all i = 1....,m, modified training sets as follows:   By removing the i-th element        S   |  i    =   {   z  1   ,  ‚Ä¶  ,   z   i  -  1    ,   z   i  +  1    ,  ‚Ä¶  ,   z  m   }        superscript  S   fragments  normal-|  i      subscript  z  1   normal-‚Ä¶   subscript  z    i  1     subscript  z    i  1    normal-‚Ä¶   subscript  z  m      S^{|i}=\{z_{1},...,\ z_{i-1},\ z_{i+1},...,\ z_{m}\}      By replacing the i-th element        S  i   =   {   z  1   ,  ‚Ä¶  ,   z   i  -  1    ,   z  i  ‚Ä≤   ,   z   i  +  1    ,  ‚Ä¶  ,   z  m   }        superscript  S  i     subscript  z  1   normal-‚Ä¶   subscript  z    i  1     superscript   subscript  z  i   normal-‚Ä≤    subscript  z    i  1    normal-‚Ä¶   subscript  z  m      S^{i}=\{z_{1},...,\ z_{i-1},\ z_{i}^{\prime},\ z_{i+1},...,\ z_{m}\}     References  S. Mukherjee, P. Niyogi, T. Poggio, and R. M. Rifkin. Learning theory: stability is sufficient for generaliza- tion and necessary and sufficient for consistency of empirical risk minimization. Adv. Comput. Math., 25(1-3):161‚Äì193, 2006  "  Category:Machine learning   