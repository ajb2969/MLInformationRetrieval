   Stochastic programming      Stochastic programming  In the field of [[mathematical optimization]], '''stochastic programming''' is a framework for [[Mathematical model|modeling]] [[Optimization (mathematics)|optimization]] problems that involve [[uncertainty]]. Whereas deterministic optimization problems are formulated with known parameters, real world problems almost invariably include some unknown parameters. When the parameters are known only within certain bounds, one approach to tackling such problems is called [[robust optimization]]. Here the goal is to find a solution which is feasible for all such data and [[Optimization (mathematics)|optimal]] in some sense. [[Stochastic]] programming [[mathematical model|models]] are similar in style but take¬†advantage¬†of¬†the¬†fact¬†that probability  distributions governing¬†the¬†data¬†are¬†known¬†or¬†can¬†be¬†estimated.¬†The¬†goal¬†here¬†is¬†to¬†find¬†some¬†policy¬†that¬†is¬†feasible¬†for¬†all¬†(or¬†almost¬†all)¬†the¬†possible¬†data¬†instances¬†and¬†maximizes¬†the¬†expectation¬†of¬†some¬†function¬†of¬†the¬†decisions¬†and¬†the random  variables .¬†More¬†generally,¬†such¬†models¬†are¬†formulated,¬†solved¬†analytically¬†or¬†numerically,¬†and¬†analyzed¬†in¬†order¬†to¬†provide¬†useful¬†information¬†to¬†a¬†decision-maker. 1  As an example, consider two-stage linear programs . Here the decision maker takes some action in the first stage, after which a random event occurs affecting the outcome of the first-stage decision. A recourse decision can then be made in the second stage that compensates for any bad effects that might have been experienced as a result of the first-stage decision. The optimal policy from such a model is a single first-stage policy and a collection of recourse decisions (a decision rule) defining which second-stage action should be taken in response to each random outcome.  Stochastic programming has applications in a broad range of areas ranging from finance to transportation to energy optimization. 2 3 This article includes an example of optimizing an investment portfolio over time.  Two-Stage Problems  The basic idea of two-stage stochastic programming is that (optimal) decisions should be based on data available at the time the decisions are made and should not depend on future observations. Two-stage formulation is widely used in stochastic programming. The general formulation of a two-stage stochastic programming problem is given by:       min   x  ‚àà  X     {    g   (  x  )    =    f   (  x  )    +   E   [   Q   (  x  ,  Œæ  )    ]      }       subscript     x  X        g  x       f  x     E   delimited-[]    Q   x  Œæ          \min_{x\in X}\{g(x)=f(x)+E[Q(x,\xi)]\}     where    Q   (  x  ,  Œæ  )       Q   x  Œæ     Q(x,\xi)   is the optimal value of the second-stage problem       min  y    {   q   (  y  ,  Œæ  )    |     T   (  Œæ  )   x   +   W   (  Œæ  )   y    =   h   (  Œæ  )     }       subscript   y     q   y  Œæ          T  Œæ  x     W  Œæ  y      h  Œæ      \min_{y}\{q(y,\xi)|T(\xi)x+W(\xi)y=h(\xi)\}     The classical two-stage linear stochastic programming problems can be formulated as         min   x  ‚àà   ‚Ñù  n         g  ;   (  x  )    =     c  T   x   +   E   [   Q   (  x  ,  Œæ  )    ]           subject to      A  x   =  b          x  ‚â•  0           subscript     x   superscript  ‚Ñù  n        g  x        superscript  c  T   x     E   delimited-[]    Q   x  Œæ         missing-subexpression     subject to      A  x   b    missing-subexpression      missing-subexpression     x  0    missing-subexpression      \begin{array}[]{llr}\min\limits_{x\in\mathbb{R}^{n}}&g(x)=c^{T}x+E[Q(x,\xi)]&%
 \\
 \text{subject to}&Ax=b&\\
 &x\geq 0&\end{array}     where    Q   (  x  ,  Œæ  )       Q   x  Œæ     Q(x,\xi)   is the optimal value of the second-stage problem         min   y  ‚àà   ‚Ñù  m        q    (  Œæ  )   T   y        subject to       T   (  Œæ  )   x   +   W   (  Œæ  )   y    =   h   (  Œæ  )            y  ‚â•  0           subscript     y   superscript  ‚Ñù  m       q   superscript  Œæ  T   y    missing-subexpression     subject to        T  Œæ  x     W  Œæ  y      h  Œæ     missing-subexpression      missing-subexpression     y  0    missing-subexpression      \begin{array}[]{llr}\min\limits_{y\in\mathbb{R}^{m}}&q(\xi)^{T}y&\\
 \text{subject to}&T(\xi)x+W(\xi)y=h(\xi)&\\
 &y\geq 0&\end{array}     In such formulation    x  ‚àà   ‚Ñù  n       x   superscript  ‚Ñù  n     x\in\mathbb{R}^{n}   is the first-stage decision variable vector,    y  ‚àà   ‚Ñù  m       y   superscript  ‚Ñù  m     y\in\mathbb{R}^{m}   is the second-stage decision variable vector, and    Œæ   (  q  ,  T  ,  W  ,  h  )       Œæ   q  T  W  h     \xi(q,T,W,h)   contains the data of the second-stage problem. In this formulation, at the first stage we have to make a "here-and-now" decision   x   x   x   before the realization of the uncertain data   Œæ   Œæ   \xi   , viewed as a random vector, is known. At the second stage, after a realization of   Œæ   Œæ   \xi   becomes available, we optimize our behavior by solving an appropriate optimization problem.  At the first stage we optimize (minimize in the above formulation) the cost     c  T   x       superscript  c  T   x    c^{T}x   of the first-stage decision plus the expected cost of the (optimal) second-stage decision. We can view the second-stage problem simply as an optimization problem which describes our supposedly optimal behavior when the uncertain data is revealed, or we can consider its solution as a recourse action where the term    W  y      W  y    Wy   compensates for a possible inconsistency of the system     T  x   ‚â§  h        T  x   h    Tx\leq h   and     q  T   y       superscript  q  T   y    q^{T}y   is the cost of this recourse action.  The considered two-stage problem is linear because the objective functions and the constraints are linear. Conceptually this is not essential and one can consider more general two-stage stochastic programs. For example, if the first-stage problem is integer, one could add integrality constraints to the first-stage problem so that the feasible set is discrete. Non-linear objectives and constraints could also be incorporated if needed. 4  Distributional assumption  The formulation of the above two-stage problem assumes that the second-stage data   Œæ   Œæ   \xi   can be modeled as a random vector with a known probability distribution (not just uncertain). This would be justified in many situations. For example   Œæ   Œæ   \xi   could be information derived from historical data and the distribution does not significantly change over the considered period of time. In such situations one may reliably estimate the required probability distribution and the optimization on average could be justified by the Law of Large Numbers. Another example is that   Œæ   Œæ   \xi   could be realizations of a simulation model whose outputs are stochastic. The empirical distribution of the sample could be used as an approximation to the true but unknown output distribution.  Discretization  To solve the two-stage stochastic problem numerically, one often need to assume that the random vector   Œæ   Œæ   \xi   has a finite number of possible realizations, called scenarios , say     Œæ  1   ,  ‚Ä¶  ,   Œæ  K       subscript  Œæ  1   normal-‚Ä¶   subscript  Œæ  K     \xi_{1},\dots,\xi_{K}   , with respective probability masses     p  1   ,  ‚Ä¶  ,   p  K       subscript  p  1   normal-‚Ä¶   subscript  p  K     p_{1},\dots,p_{K}   . Then the expectation in the first-stage problem's objective function can be written as the summation:       E   [   Q   (  x  ,  Œæ  )    ]    =    ‚àë   k  =  1   K     p  k   Q   (  x  ,   Œæ  k   )           E   delimited-[]    Q   x  Œæ        superscript   subscript     k  1    K      subscript  p  k   Q   x   subscript  Œæ  k        E[Q(x,\xi)]=\sum\limits_{k=1}^{K}p_{k}Q(x,\xi_{k})     and, moreover, the two-stage problem can be formulated as one large linear programming problem (this is called the deterministic equivalent of the original problem, see section ).  When   Œæ   Œæ   \xi   has an infinite (or very large) number of possible realizations the standard approach is then to represent this distribution by scenarios. This approach raises three questions, namely:   How to construct scenarios, see ;  How to solve the deterministic equivalent. Optimizers such as CPLEX , GLPK and Gurobi can solve large linear/nonlinear problems. NEOS 5 server hosted at the Argonne National Laboratory allows free access to many modern solvers. The structure of a deterministic equivalent is particularly amenable to apply decomposition methods, 6 such as Benders' decomposition or scenario decomposition;  How to measure quality of the obtained solution with respect to the "true" optimum.   These questions are not independent. For example, the number of scenarios constructed will affect both the tractability of the deterministic equivalent and the quality of the obtained solutions.  Stochastic linear program  A stochastic linear program is a specific instance of the classical two-stage stochastic program. A stochastic LP is built from a collection of multi-period linear programs (LPs), each having the same structure but somewhat different data. The    k   t  h      superscript  k    t  h     k^{th}   two-period LP, representing the    k   t  h      superscript  k    t  h     k^{th}   scenario, may be regarded as having the following form:        Minimize      f  T   x     +      g  T   y     +      h  k  T    z  k          subject to     T  x     +     U  y       =    r           V  k   y     +      W  k    z  k      =     s  k        x      ,     y      ,      z  k     ‚â•    0        Minimize     superscript  f  T   x       superscript  g  T   y       superscript   subscript  h  k   T    subscript  z  k     missing-subexpression    missing-subexpression     subject to    T  x      U  y    missing-subexpression    missing-subexpression    r     missing-subexpression    missing-subexpression    missing-subexpression      subscript  V  k   y       subscript  W  k    subscript  z  k      subscript  s  k      missing-subexpression   x  absent  y  absent   subscript  z  k    0     \begin{array}[]{lccccccc}\text{Minimize}&f^{T}x&+&g^{T}y&+&h_{k}^{T}z_{k}&&\\
 \text{subject to}&Tx&+&Uy&&&=&r\\
 &&&V_{k}y&+&W_{k}z_{k}&=&s_{k}\\
 &x&,&y&,&z_{k}&\geq&0\end{array}     The vectors   x   x   x   and   y   y   y   contain the first-period variables, whose values must be chosen immediately. The vector    z  k     subscript  z  k    z_{k}   contains all of the variables for subsequent periods. The constraints      T  x   +   U  y    =  r          T  x     U  y    r    Tx+Uy=r   involve only first-period variables and are the same in every scenario. The other constraints involve variables of later periods and differ in some respects from scenario to scenario, reflecting uncertainty about the future.  Note that solving the    k   t  h      superscript  k    t  h     k^{th}   two-period LP is equivalent to assuming the    k   t  h      superscript  k    t  h     k^{th}   scenario in the second period with no uncertainty. In order to incorporate uncertainties in the second stage, one should assign probabilities to different scenarios and solve the corresponding deterministic equivalent.  Deterministic equivalent of a stochastic problem  With a finite number of scenarios, two-stage stochastic linear programs can be modelled as large linear programming problems. This formulation is often called the deterministic equivalent linear program, or abbreviated to deterministic equivalent. (Strictly speaking a deterministic equivalent is any mathematical program that can be used to compute the optimal first-stage decision, so these will exist for continuous probability distributions as well, when one can represent the second-stage cost in some closed form.) For example, to form the deterministic equivalent to the above stochastic linear program, we assign a probability    p  k     subscript  p  k    p_{k}   to each scenario    k  =   1  ,  ‚Ä¶  ,  K       k   1  normal-‚Ä¶  K     k=1,\dots,K   . Then we can minimize the expected value of the objective, subject to the constraints from all scenarios:        Minimize      f  T   x     +      g  T   y     +      p  1    h  1  T    z  1      +      p  2    h  2  T    z  2      +    ‚ãØ    +      p  K    h  K  T    z  K          subject to     T  x     +     U  y             =    r           V  1   y     +      W  1    z  1            =     s  1            V  2   y       +      W  2    z  2          =     s  2          ‚ãÆ         ‚ã±       ‚ãÆ           V  K   y           +      W  K    z  K      =     s  K        x      ,     y      ,      z  1       ,      z  2       ,     ‚Ä¶      ,      z  K     ‚â•    0        Minimize     superscript  f  T   x       superscript  g  T   y       subscript  p  1    superscript   subscript  h  1   T    subscript  z  1        subscript  p  2    superscript   subscript  h  2   T    subscript  z  2     normal-‚ãØ      subscript  p  K    superscript   subscript  h  K   T    subscript  z  K     missing-subexpression    missing-subexpression     subject to    T  x      U  y    missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression    r     missing-subexpression    missing-subexpression    missing-subexpression      subscript  V  1   y       subscript  W  1    subscript  z  1     missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression     subscript  s  1      missing-subexpression    missing-subexpression    missing-subexpression      subscript  V  2   y    missing-subexpression    missing-subexpression       subscript  W  2    subscript  z  2     missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression     subscript  s  2      missing-subexpression    missing-subexpression    missing-subexpression   normal-‚ãÆ   missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression   normal-‚ã±   missing-subexpression    missing-subexpression    missing-subexpression   normal-‚ãÆ     missing-subexpression    missing-subexpression    missing-subexpression      subscript  V  K   y    missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression    missing-subexpression       subscript  W  K    subscript  z  K      subscript  s  K      missing-subexpression   x  absent  y  absent   subscript  z  1   absent   subscript  z  2   absent  normal-‚Ä¶  absent   subscript  z  K    0     \begin{array}[]{lccccccccccccc}\text{Minimize}&f^{T}x&+&g^{T}y&+&p_{1}h_{1}^{T%
 }z_{1}&+&p_{2}h_{2}^{T}z_{2}&+&\cdots&+&p_{K}h_{K}^{T}z_{K}&&\\
 \text{subject to}&Tx&+&Uy&&&&&&&&&=&r\\
 &&&V_{1}y&+&W_{1}z_{1}&&&&&&&=&s_{1}\\
 &&&V_{2}y&&&+&W_{2}z_{2}&&&&&=&s_{2}\\
 &&&\vdots&&&&&&\ddots&&&&\vdots\\
 &&&V_{K}y&&&&&&&+&W_{K}z_{K}&=&s_{K}\\
 &x&,&y&,&z_{1}&,&z_{2}&,&\ldots&,&z_{K}&\geq&0\\
 \end{array}     We have a different vector    z  k     subscript  z  k    z_{k}   of later-period variables for each scenario   k   k   k   . The first-period variables   x   x   x   and   y   y   y   are the same in every scenario, however, because we must make a decision for the first period before we know which scenario will be realized. As a result, the constraints involving just   x   x   x   and   y   y   y   need only be specified once, while the remaining constraints must be given separately for each scenario.  Scenario construction  In practice it might be possible to construct scenarios by eliciting expert's opinions on the future. The number of constructed scenarios should be relatively modest so that the obtained deterministic equivalent can be solved with reasonable computational effort. It is often claimed that a solution that is optimal using only a few scenarios provides more adaptable plans than one that assumes a single scenario only. In some cases such a claim could be verified by a simulation. In theory some measures of guarantee that an obtained solution solves the original problem with reasonable accuracy. Typically in applications only the first stage optimal solution    x  *     superscript  x     x^{*}   has a practical value since almost always a "true" realization of the random data will be different from the set of constructed (generated) scenarios.  Suppose   Œæ   Œæ   \xi   contains   d   d   d   independent random components, each of which has three possible realizations (for example, future realizations of each random parameters are classified as low, medium and high), then the total number of scenarios is    K  =   3  d       K   superscript  3  d     K=3^{d}   . Such exponential growth of the number of scenarios makes model development using expert opinion very difficult even for reasonable size   d   d   d   . The situation becomes even worse if some random components of   Œæ   Œæ   \xi   have continuous distributions.  Monte Carlo sampling and Sample Average Approximation (SAA) Method  A common approach to reduce the scenario set to a manageable size is by using Monte Carlo simulation. Suppose the total number of scenarios is very large or even infinite. Suppose further that we can generate a sample     Œæ  1   ,   Œæ  2   ,  ‚Ä¶  ,   Œæ  N       superscript  Œæ  1    superscript  Œæ  2   normal-‚Ä¶   superscript  Œæ  N     \xi^{1},\xi^{2},\dots,\xi^{N}   of   N   N   N   replications of the random vector   Œæ   Œæ   \xi   . Usually the sample is assumed to be independent identically distributed (i.i.d sample). Given a sample, the expectation function     q   (  x  )    =   E   [   Q   (  x  ,  Œæ  )    ]          q  x     E   delimited-[]    Q   x  Œæ        q(x)=E[Q(x,\xi)]   is approximated by the sample average         q  ^   N    (  x  )    =    1  N     ‚àë   j  =  1   N    Q   (  x  ,   Œæ  j   )             subscript   normal-^  q   N   x       1  N     superscript   subscript     j  1    N     Q   x   superscript  Œæ  j         \hat{q}_{N}(x)=\frac{1}{N}\sum_{j=1}^{N}Q(x,\xi^{j})     and consequently the first-stage problem is given by            g  ^   N    (  x  )    =       min   x  ‚àà   ‚Ñù  n          c  T   x   +    1  N     ‚àë   j  =  1   N    Q   (  x  ,   Œæ  j   )              subject to     A  x     =    b        x    ‚â•    0             subscript   normal-^  g   N   x   absent    subscript     x   superscript  ‚Ñù  n          superscript  c  T   x       1  N     superscript   subscript     j  1    N     Q   x   superscript  Œæ  j         missing-subexpression    missing-subexpression      missing-subexpression   subject to    A  x    b     missing-subexpression    missing-subexpression   x   0     \begin{array}[]{rlrrr}\hat{g}_{N}(x)=&\min\limits_{x\in\mathbb{R}^{n}}&c^{T}x+%
 \frac{1}{N}\sum_{j=1}^{N}Q(x,\xi^{j})&\\
 &\text{subject to}&Ax&=&b\\
 &&x&\geq&0\end{array}     This formulation is known as the Sample Average Approximation method. The SAA problem is a function of the considered sample and in that sense is random. For a given sample     Œæ  1   ,   Œæ  2   ,  ‚Ä¶  ,   Œæ  N       superscript  Œæ  1    superscript  Œæ  2   normal-‚Ä¶   superscript  Œæ  N     \xi^{1},\xi^{2},\dots,\xi^{N}   the SAA problem is of the same form as a two-stage stochastic linear programming problem with the scenarios    Œæ  j     superscript  Œæ  j    \xi^{j}   .,    j  =   1  ,  ‚Ä¶  ,  N       j   1  normal-‚Ä¶  N     j=1,\dots,N   , each taken with the same probability     p  j   =   1  N        subscript  p  j     1  N     p_{j}=\frac{1}{N}   .  Statistical Inference  Consider the following stochastic programming problem        min   x  ‚àà  X     {    g   (  x  )    =    f   (  x  )    +   E   [   Q   (  x  ,  Œæ  )    ]      }       subscript     x  X        g  x       f  x     E   delimited-[]    Q   x  Œæ          \min\limits_{x\in X}\{g(x)=f(x)+E[Q(x,\xi)]\}      Here   X   X   X   is a nonempty closed subset of    ‚Ñù  n     superscript  ‚Ñù  n    \mathbb{R}^{n}   ,   Œæ   Œæ   \xi   is a random vector whose probability distribution   P   P   P   is supported on a set    Œû  ‚äÇ   ‚Ñù  d       normal-Œû   superscript  ‚Ñù  d     \Xi\subset\mathbb{R}^{d}   , and    Q  :    X  √ó  Œû   ‚Üí  ‚Ñù      normal-:  Q   normal-‚Üí    X  normal-Œû   ‚Ñù     Q:X\times\Xi\rightarrow\mathbb{R}   . In the framework of two-stage stochastic programming,    Q   (  x  ,  Œæ  )       Q   x  Œæ     Q(x,\xi)   is given by the optimal value of the corresponding second-stage problem.  Assume that    g   (  x  )       g  x    g(x)   is well defined and finite valued for all    x  ‚àà  X      x  X    x\in X   . This implies that for every    x  ‚àà  X      x  X    x\in X   the value    Q   (  x  ,  Œæ  )       Q   x  Œæ     Q(x,\xi)   is finite almost surely.  Suppose that we have a sample     Œæ  1   ,  ‚Ä¶  ,   Œæ  N       superscript  Œæ  1   normal-‚Ä¶   superscript  Œæ  N     \xi^{1},\dots,\xi^{N}   of   N   N   N   realizations of the random vector   Œæ   Œæ   \xi   . This random sample can be viewed as historical data of   N   N   N   observations of   Œæ   Œæ   \xi   , or it can be generated by Monte Carlo sampling techniques. Then we can formulate a corresponding sample average approximation        min   x  ‚àà  X     {      g  ^   N    (  x  )    =    f   (  x  )    +    1  N     ‚àë   j  =  1   N    Q   (  x  ,   Œæ  j   )        }       subscript     x  X         subscript   normal-^  g   N   x       f  x       1  N     superscript   subscript     j  1    N     Q   x   superscript  Œæ  j           \min\limits_{x\in X}\{\hat{g}_{N}(x)=f(x)+\frac{1}{N}\sum_{j=1}^{N}Q(x,\xi^{j})\}      By the Law of Large Numbers we have that, under some regularity conditions     1  N     ‚àë   j  =  1   N    Q   (  x  ,   Œæ  j   )           1  N     superscript   subscript     j  1    N     Q   x   superscript  Œæ  j        \frac{1}{N}\sum_{j=1}^{N}Q(x,\xi^{j})   converges pointwise with probability 1 to    E   [   Q   (  x  ,  Œæ  )    ]       E   delimited-[]    Q   x  Œæ       E[Q(x,\xi)]   as    N  ‚Üí  ‚àû     normal-‚Üí  N     N\rightarrow\infty   . Moreover, under mild additional conditions the convergence is uniform. We also have     E   [     g  ^   N    (  x  )    ]    =   g   (  x  )          E   delimited-[]     subscript   normal-^  g   N   x       g  x     E[\hat{g}_{N}(x)]=g(x)   , i.e.,      g  ^   N    (  x  )        subscript   normal-^  g   N   x    \hat{g}_{N}(x)   is an unbiased estimator of    g   (  x  )       g  x    g(x)   . Therefore it is natural to expect that the optimal value and optimal solutions of the SAA problem converge to their counterparts of the true problem as    N  ‚Üí  ‚àû     normal-‚Üí  N     N\rightarrow\infty   .  Consistency of SAA estimators  Suppose the feasible set   X   X   X   of the SAA problem is fixed, i.e., it is independent of the sample. Let    œë  *     superscript  œë     \vartheta^{*}   and    S  *     superscript  S     S^{*}   be the optimal value and the set of optimal solutions, respectively, of the true problem and let     œë  ^   N     subscript   normal-^  œë   N    \hat{\vartheta}_{N}   and     S  ^   N     subscript   normal-^  S   N    \hat{S}_{N}   be the optimal value and the set of optimal solutions, respectively, of the SAA problem.   Let    g  :   X  ‚Üí  ‚Ñù      normal-:  g   normal-‚Üí  X  ‚Ñù     g:X\rightarrow\mathbb{R}   and      g  ^   N   :   X  ‚Üí  ‚Ñù      normal-:   subscript   normal-^  g   N    normal-‚Üí  X  ‚Ñù     \hat{g}_{N}:X\rightarrow\mathbb{R}   be a sequence of (deterministic) real valued functions. The following two properties are equivalent:  for any     x  ¬Ø   ‚àà  X       normal-¬Ø  x   X    \overline{x}\in X   and any sequence     {   x  N   }   ‚äÇ  X        subscript  x  N    X    \{x_{N}\}\subset X   converging to    x  ¬Ø     normal-¬Ø  x    \overline{x}   it follows that      g  ^   N    (   x  N   )        subscript   normal-^  g   N    subscript  x  N     \hat{g}_{N}(x_{N})   converges to    g   (   x  ¬Ø   )       g   normal-¬Ø  x     g(\overline{x})     the function    f   (  ‚ãÖ  )       f  normal-‚ãÖ    f(\cdot)   is continuous on   X   X   X   and      g  ^   N    (  ‚ãÖ  )        subscript   normal-^  g   N   normal-‚ãÖ    \hat{g}_{N}(\cdot)   converges to    g   (  ‚ãÖ  )       g  normal-‚ãÖ    g(\cdot)   uniformly on any compact subset of   X   X   X      If the objective of the SAA problem      g  ^   N    (  x  )        subscript   normal-^  g   N   x    \hat{g}_{N}(x)   converges to the true problem's objective    g   (  x  )       g  x    g(x)   with probability 1, as    N  ‚Üí  ‚àû     normal-‚Üí  N     N\rightarrow\infty   , uniformly on the feasible set   X   X   X   . Then     œë  ^   N     subscript   normal-^  œë   N    \hat{\vartheta}_{N}   converges to    œë  *     superscript  œë     \vartheta^{*}   with probability 1 as    N  ‚Üí  ‚àû     normal-‚Üí  N     N\rightarrow\infty   .  Suppose that there exists a compact set    C  ‚äÇ   ‚Ñù  n       C   superscript  ‚Ñù  n     C\subset\mathbb{R}^{n}   such that  the set   S   S   S   of optimal solutions of the true problem is nonempty and is contained in   C   C   C     the function    g   (  x  )       g  x    g(x)   is finite valued and continuous on   C   C   C     the sequence of functions      g  ^   N    (  x  )        subscript   normal-^  g   N   x    \hat{g}_{N}(x)   converges to    g   (  x  )       g  x    g(x)   with probability 1, as    N  ‚Üí  ‚àû     normal-‚Üí  N     N\rightarrow\infty   , uniformly in    x  ‚àà  C      x  C    x\in C     for   N   N   N   large enough the set     S  ^   N     subscript   normal-^  S   N    \hat{S}_{N}   is nonempty and      S  ^   N   ‚äÇ  C       subscript   normal-^  S   N   C    \hat{S}_{N}\subset C   with probability 1      then      œë  ^   N   ‚Üí   œë  *      normal-‚Üí   subscript   normal-^  œë   N    superscript  œë      \hat{\vartheta}_{N}\rightarrow\vartheta^{*}   and     ùîª   (   S  *   ,    S  ^   N   )    ‚Üí  0     normal-‚Üí    ùîª    superscript  S     subscript   normal-^  S   N     0    \mathbb{D}(S^{*},\hat{S}_{N})\rightarrow 0   with probability 1 as    N  ‚Üí  ‚àû     normal-‚Üí  N     N\rightarrow\infty   . Note that    ùîª   (  A  ,  B  )       ùîª   A  B     \mathbb{D}(A,B)   denotes the deviation of set   A   A   A   from set   B   B   B    , defined as           ùîª   (  A  ,  B  )    :=    sup   x  ‚àà  A     {    inf    x  ‚Ä≤   ‚àà  B     ‚à•   x  -   x  ‚Ä≤    ‚à•    }       assign    ùîª   A  B      subscript  supremum    x  A       subscript  infimum     superscript  x  normal-‚Ä≤   B     norm    x   superscript  x  normal-‚Ä≤          \mathbb{D}(A,B):=\sup_{x\in A}\{\inf_{x^{\prime}\in B}\|x-x^{\prime}\|\}      In some situations the feasible set   X   X   X   of the SAA problem is estimated, then the corresponding SAA problem takes the form         min   x  ‚àà   X  N       g  ^   N     (  x  )         subscript     x   subscript  X  N      subscript   normal-^  g   N    x    \min_{x\in X_{N}}\hat{g}_{N}(x)      where    X  N     subscript  X  N    X_{N}   is a subset of    ‚Ñù  n     superscript  ‚Ñù  n    \mathbb{R}^{n}   depending on the sample and therefore is random. Nevertheless consistency results for SAA estimators can still be derived under some additional assumptions:   Suppose that there exists a compact set    C  ‚äÇ   ‚Ñù  n       C   superscript  ‚Ñù  n     C\subset\mathbb{R}^{n}   such that  the set   S   S   S   of optimal solutions of the true problem is nonempty and is contained in   C   C   C     the function    g   (  x  )       g  x    g(x)   is finite valued and continuous on   C   C   C     the sequence of functions      g  ^   N    (  x  )        subscript   normal-^  g   N   x    \hat{g}_{N}(x)   converges to    g   (  x  )       g  x    g(x)   with probability 1, as    N  ‚Üí  ‚àû     normal-‚Üí  N     N\rightarrow\infty   , uniformly in    x  ‚àà  C      x  C    x\in C     for   N   N   N   large enough the set     S  ^   N     subscript   normal-^  S   N    \hat{S}_{N}   is nonempty and      S  ^   N   ‚äÇ  C       subscript   normal-^  S   N   C    \hat{S}_{N}\subset C   with probability 1  if     x  N   ‚àà   X  N        subscript  x  N    subscript  X  N     x_{N}\in X_{N}   and    x  N     subscript  x  N    x_{N}   converges with probability 1 to a point   x   x   x   , then    x  ‚àà  X      x  X    x\in X     for some point    x  ‚àà   S  *       x   superscript  S      x\in S^{*}   there exists a sequence     x  N   ‚àà   X  N        subscript  x  N    subscript  X  N     x_{N}\in X_{N}   such that     x  N   ‚Üí  x     normal-‚Üí   subscript  x  N   x    x_{N}\rightarrow x   with probability 1.      then      œë  ^   N   ‚Üí   œë  *      normal-‚Üí   subscript   normal-^  œë   N    superscript  œë      \hat{\vartheta}_{N}\rightarrow\vartheta^{*}   and     ùîª   (   S  *   ,    S  ^   N   )    ‚Üí  0     normal-‚Üí    ùîª    superscript  S     subscript   normal-^  S   N     0    \mathbb{D}(S^{*},\hat{S}_{N})\rightarrow 0   with probability 1 as    N  ‚Üí  ‚àû     normal-‚Üí  N     N\rightarrow\infty   .     Asymptotics of the SAA optimal value  Suppose the sample     Œæ  1   ,  ‚Ä¶  ,   Œæ  N       superscript  Œæ  1   normal-‚Ä¶   superscript  Œæ  N     \xi^{1},\dots,\xi^{N}   is i.i.d. and fix a point    x  ‚àà  X      x  X    x\in X   . Then the sample average estimator      g  ^   N    (  x  )        subscript   normal-^  g   N   x    \hat{g}_{N}(x)   , of    g   (  x  )       g  x    g(x)   , is unbiased and have variance     1  N    œÉ  2    (  x  )         1  N    superscript  œÉ  2   x    \frac{1}{N}\sigma^{2}(x)   , where      œÉ  2    (  x  )    :=   V  a  r   [   Q   (  x  ,  Œæ  )    ]       assign     superscript  œÉ  2   x     V  a  r   delimited-[]    Q   x  Œæ        \sigma^{2}(x):=Var[Q(x,\xi)]   is supposed to be finite. Moreover, by the central limit theorem we have that         N    [     g  ^   N   -   g   (  x  )     ]     ‚Üí  ùíü    Y  x       ùíü  normal-‚Üí       N    delimited-[]     subscript   normal-^  g   N     g  x       subscript  Y  x     \sqrt{N}[\hat{g}_{N}-g(x)]\xrightarrow{\mathcal{D}}Y_{x}      where    ‚Üí  ùíü     ùíü  normal-‚Üí    \xrightarrow{\mathcal{D}}   denotes convergence in distribution and    Y  x     subscript  Y  x    Y_{x}   has a normal distribution with mean   0   0    and variance     œÉ  2    (  x  )        superscript  œÉ  2   x    \sigma^{2}(x)   , written as    ùí©   (  0  ,    œÉ  2    (  0  )    )       ùí©   0     superscript  œÉ  2   0      \mathcal{N}(0,\sigma^{2}(0))   .  In other words,      g  ^   N    (  x  )        subscript   normal-^  g   N   x    \hat{g}_{N}(x)   has asymptotically normal distribution, i.e., for large   N   N   N   ,      g  ^   N    (  x  )        subscript   normal-^  g   N   x    \hat{g}_{N}(x)   has approximately normal distribution with mean    g   (  x  )       g  x    g(x)   and variance     1  N    œÉ  2    (  x  )         1  N    superscript  œÉ  2   x    \frac{1}{N}\sigma^{2}(x)   . This leads to the following (approximate)    100   (   1  -  Œ±   )       100    1  Œ±     100(1-\alpha)   % confidence interval for    f   (  x  )       f  x    f(x)   :       [      g  ^   N    (  x  )    -    z   Œ±  /  2       œÉ  ^    (  x  )     N      ,      g  ^   N    (  x  )    +    z   Œ±  /  2       œÉ  ^    (  x  )     N      ]          subscript   normal-^  g   N   x      subscript  z    Œ±  2         normal-^  œÉ   x     N           subscript   normal-^  g   N   x      subscript  z    Œ±  2         normal-^  œÉ   x     N        \left[\hat{g}_{N}(x)-z_{\alpha/2}\frac{\hat{\sigma}(x)}{\sqrt{N}},\hat{g}_{N}(%
 x)+z_{\alpha/2}\frac{\hat{\sigma}(x)}{\sqrt{N}}\right]      where     z   Œ±  /  2    :=    Œ¶   -  1     (   1  -   Œ±  /  2    )       assign   subscript  z    Œ±  2       superscript  normal-Œ¶    1      1    Œ±  2       z_{\alpha/2}:=\Phi^{-1}(1-\alpha/2)   (here    Œ¶   (  ‚ãÖ  )       normal-Œ¶  normal-‚ãÖ    \Phi(\cdot)   denotes the cdf of the standard normal distribution) and          œÉ  ^   2    (  x  )    :=    1   N  -  1      ‚àë   j  =  1   N     [    Q   (  x  ,   Œæ  j   )    -    1  N     ‚àë   j  =  1   N    Q   (  x  ,   Œæ  j   )       ]   2        assign     superscript   normal-^  œÉ   2   x       1    N  1      superscript   subscript     j  1    N    superscript   delimited-[]      Q   x   superscript  Œæ  j         1  N     superscript   subscript     j  1    N     Q   x   superscript  Œæ  j         2       \hat{\sigma}^{2}(x):=\frac{1}{N-1}\sum_{j=1}^{N}\left[Q(x,\xi^{j})-\frac{1}{N}%
 \sum_{j=1}^{N}Q(x,\xi^{j})\right]^{2}      is the sample variance estimate of     œÉ  2    (  x  )        superscript  œÉ  2   x    \sigma^{2}(x)   . That is, the error of estimation of    g   (  x  )       g  x    g(x)   is (stochastically) of order    O   (   N   )       O    N     O(\sqrt{N})   .  Multistage portfolio optimization  The following is an example from finance of multi-stage stochastic programming. Suppose that at time    t  =  0      t  0    t=0   we have initial capital    W  0     subscript  W  0    W_{0}   to invest in   n   n   n   assets. Suppose further that we are allowed to rebalance our portfolio at times    t  =   1  ,  ‚Ä¶  ,   T  -  1        t   1  normal-‚Ä¶    T  1      t=1,\dots,T-1   but without injecting additional cash into it. At each period   t   t   t   we make a decision about redistributing the current wealth    W  t     subscript  W  t    W_{t}   among the   n   n   n   assets. Let     x  0   =   (   x  10   ,  ‚Ä¶  ,   x   n  0    )        subscript  x  0     subscript  x  10   normal-‚Ä¶   subscript  x    n  0       x_{0}=(x_{10},\dots,x_{n0})   be the initial amounts invested in the n assets. We require that each    x   i  0      subscript  x    i  0     x_{i0}   is nonnegative and that the balance equation      ‚àë   i  =  1   n    x   i  0     =   W  0         superscript   subscript     i  1    n    subscript  x    i  0      subscript  W  0     \sum_{i=1}^{n}x_{i0}=W_{0}   should hold.  Consider the total returns     Œæ  t   =   (   Œæ   1  t    ,  ‚Ä¶  ,   Œæ   n  t    )        subscript  Œæ  t     subscript  Œæ    1  t    normal-‚Ä¶   subscript  Œæ    n  t       \xi_{t}=(\xi_{1t},\dots,\xi_{nt})   for each period    t  =   1  ,  ‚Ä¶  ,  T       t   1  normal-‚Ä¶  T     t=1,\dots,T   . This forms a vector-valued random process     Œæ  1   ,  ‚Ä¶  ,   Œæ  T       subscript  Œæ  1   normal-‚Ä¶   subscript  Œæ  T     \xi_{1},\dots,\xi_{T}   . At time period    t  =  1      t  1    t=1   , we can rebalance the portfolio by specifying the amounts     x  1   =   (   x  11   ,  ‚Ä¶  ,   x   n  1    )        subscript  x  1     subscript  x  11   normal-‚Ä¶   subscript  x    n  1       x_{1}=(x_{11},\dots,x_{n1})   invested in the respective assets. At that time the returns in the first period have been realized so it is reasonable to use this information in the rebalancing decision. Thus, the second-stage decisions, at time    t  =  1      t  1    t=1   , are actually functions of realization of the random vector    Œæ  1     subscript  Œæ  1    \xi_{1}   , i.e.,     x  1   =    x  1    (   Œæ  1   )         subscript  x  1      subscript  x  1    subscript  Œæ  1      x_{1}=x_{1}(\xi_{1})   . Similarly, at time   t   t   t   the decision     x  t   =   (   x   1  t    ,  ‚Ä¶  ,   x   n  t    )        subscript  x  t     subscript  x    1  t    normal-‚Ä¶   subscript  x    n  t       x_{t}=(x_{1t},\dots,x_{nt})   is a function     x  t   =    x  t    (   Œæ   [  t  ]    )         subscript  x  t      subscript  x  t    subscript  Œæ   delimited-[]  t       x_{t}=x_{t}(\xi_{[t]})   of the available information given by     Œæ   [  t  ]    =   (   Œæ  1   ,  ‚Ä¶  ,   Œæ  t   )        subscript  Œæ   delimited-[]  t      subscript  Œæ  1   normal-‚Ä¶   subscript  Œæ  t      \xi_{[t]}=(\xi_{1},\dots,\xi_{t})   the history of the random process up to time   t   t   t   . A sequence of functions     x  t   =    x  t    (   Œæ   [  t  ]    )         subscript  x  t      subscript  x  t    subscript  Œæ   delimited-[]  t       x_{t}=x_{t}(\xi_{[t]})   ,    t  =   0  ,  ‚Ä¶  ,   T  -  1        t   0  normal-‚Ä¶    T  1      t=0,\dots,T-1   , with    x  0     subscript  x  0    x_{0}   being constant, defines an implementable policy of the decision process. It is said that such a policy is feasible if it satisfies the model constraints with probability 1, i.e., the nonnegativity constraints      x   i  t     (   Œæ   [  t  ]    )    ‚â•  0         subscript  x    i  t     subscript  Œæ   delimited-[]  t     0    x_{it}(\xi_{[t]})\geq 0   ,    i  =   1  ,  ‚Ä¶  ,  n       i   1  normal-‚Ä¶  n     i=1,\dots,n   ,    t  =   0  ,  ‚Ä¶  ,   T  -  1        t   0  normal-‚Ä¶    T  1      t=0,\dots,T-1   , and the balance of wealth constraints,         ‚àë   i  =  1   n     x   i  t     (   Œæ   [  t  ]    )     =   W  t    ,        superscript   subscript     i  1    n      subscript  x    i  t     subscript  Œæ   delimited-[]  t       subscript  W  t     \sum_{i=1}^{n}x_{it}(\xi_{[t]})=W_{t},     where in period    t  =   1  ,  ‚Ä¶  ,  T       t   1  normal-‚Ä¶  T     t=1,\dots,T   the wealth    W  t     subscript  W  t    W_{t}   is given by        W  t   =    ‚àë   i  =  1   n     Œæ   i  t     x   i  ,   t  -  1      (   Œæ   [   t  -  1   ]    )      ,       subscript  W  t     superscript   subscript     i  1    n      subscript  Œæ    i  t     subscript  x   i    t  1      subscript  Œæ   delimited-[]    t  1         W_{t}=\sum_{i=1}^{n}\xi_{it}x_{i,t-1}(\xi_{[t-1]}),     which depends on the realization of the random process and the decisions up to time   t   t   t   .  Suppose the objective is to maximize the expected utility of this wealth at the last period, that is, to consider the problem        max  E    [   U   (   W  T   )    ]    .        E    delimited-[]    U   subscript  W  T       \max E[U(W_{T})].     This is a multistage stochastic programming problem, where stages are numbered from    t  =  0      t  0    t=0   to    t  =   T  -  1       t    T  1     t=T-1   . Optimization is performed over all implementable and feasible policies. To complete the problem description one also needs to define the probability distribution of the random process     Œæ  1   ,  ‚Ä¶  ,   Œæ  T       subscript  Œæ  1   normal-‚Ä¶   subscript  Œæ  T     \xi_{1},\dots,\xi_{T}   . This can be done in various ways. For example, one can construct a particular scenario tree defining time evolution of the process. If at every stage the random return of each asset is allowed to have two continuations, independent of other assets, then the total number of scenarios is    2   n  T      superscript  2    n  T     2^{nT}   .  In order to write dynamic programming equations, consider the above multistage problem backward in time. At the last stage    t  =   T  -  1       t    T  1     t=T-1   , a realization     Œæ   [   T  -  1   ]    =   (   Œæ  1   ,  ‚Ä¶  ,   Œæ   T  -  1    )        subscript  Œæ   delimited-[]    T  1       subscript  Œæ  1   normal-‚Ä¶   subscript  Œæ    T  1       \xi_{[T-1]}=(\xi_{1},\dots,\xi_{T-1})   of the random process is known and    x   T  -  2      subscript  x    T  2     x_{T-2}   has been chosen. Therefore, one needs to solve the following problem         max   x   T  -  1        E   [  U   (   W  T   )   |   Œæ   [   T  -  1   ]    ]           subject to     W  T     =      ‚àë   i  =  1   n     Œæ   i  T     x   i  ,   T  -  1               ‚àë   i  =  1   n    x   i  ,   T  -  1        =     W   ;   T  -  1            x   T  -  1      ‚â•    0          subscript    subscript  x    T  1      fragments  E   fragments  normal-[  U   fragments  normal-(   subscript  W  T   normal-)   normal-|   subscript  Œæ   delimited-[]    T  1     normal-]     missing-subexpression    missing-subexpression    missing-subexpression     subject to   subscript  W  T      superscript   subscript     i  1    n      subscript  Œæ    i  T     subscript  x   i    T  1        missing-subexpression      missing-subexpression     superscript   subscript     i  1    n    subscript  x   i    T  1        fragments  W   subscript  normal-;    T  1      missing-subexpression      missing-subexpression    subscript  x    T  1     0   missing-subexpression      \begin{array}[]{lrclr}\max\limits_{x_{T-1}}&E[U(W_{T})|\xi_{[T-1]}]&\\
 \text{subject to}&W_{T}&=&\sum_{i=1}^{n}\xi_{iT}x_{i,T-1}\\
 &\sum_{i=1}^{n}x_{i,T-1}&=&W_{T-1}\\
 &x_{T-1}&\geq&0\end{array}     where    E   [  U   (   W  T   )   |   Œæ   [   T  -  1   ]    ]      fragments  E   fragments  normal-[  U   fragments  normal-(   subscript  W  T   normal-)   normal-|   subscript  Œæ   delimited-[]    T  1     normal-]     E[U(W_{T})|\xi_{[T-1]}]   denotes the conditional expectation of    U   (   W  T   )       U   subscript  W  T     U(W_{T})   given    Œæ   [   T  -  1   ]      subscript  Œæ   delimited-[]    T  1      \xi_{[T-1]}   . The optimal value of the above problem depends on    W   T  -  1      subscript  W    T  1     W_{T-1}   and    Œæ   [   T  -  1   ]      subscript  Œæ   delimited-[]    T  1      \xi_{[T-1]}   and is denoted     Q   T  -  1     (   W   T  -  1    ,   Œæ   [   T  -  1   ]    )        subscript  Q    T  1      subscript  W    T  1     subscript  Œæ   delimited-[]    T  1        Q_{T-1}(W_{T-1},\xi_{[T-1]})   .  Similarly, at stages    t  =    T  -  2   ,  ‚Ä¶  ,  1       t     T  2   normal-‚Ä¶  1     t=T-2,\dots,1   , one should solve the problem         max   x  t       E   [   Q   t  +  1     (   W   t  +  1    ,   Œæ   [   t  +  1   ]    )   |   Œæ   [  t  ]    ]           subject to     W   t  +  1      =      ‚àë   i  =  1   n     Œæ   i  ,   t  +  1      x   i  ,  t              ‚àë   i  =  1   n    x   i  ,  t       =     W   ;  t           x  t     ‚â•    0          subscript    subscript  x  t     fragments  E   fragments  normal-[   subscript  Q    t  1     fragments  normal-(   subscript  W    t  1    normal-,   subscript  Œæ   delimited-[]    t  1     normal-)   normal-|   subscript  Œæ   delimited-[]  t    normal-]     missing-subexpression    missing-subexpression    missing-subexpression     subject to   subscript  W    t  1       superscript   subscript     i  1    n      subscript  Œæ   i    t  1      subscript  x   i  t       missing-subexpression      missing-subexpression     superscript   subscript     i  1    n    subscript  x   i  t       fragments  W   subscript  normal-;  t     missing-subexpression      missing-subexpression    subscript  x  t    0   missing-subexpression      \begin{array}[]{lrclr}\max\limits_{x_{t}}&E[Q_{t+1}(W_{t+1},\xi_{[t+1]})|\xi_{%
 [t]}]&\\
 \text{subject to}&W_{t+1}&=&\sum_{i=1}^{n}\xi_{i,t+1}x_{i,t}\\
 &\sum_{i=1}^{n}x_{i,t}&=&W_{t}\\
 &x_{t}&\geq&0\end{array}     whose optimal value is denoted by     Q  t    (   W  t   ,   Œæ   [  t  ]    )        subscript  Q  t     subscript  W  t    subscript  Œæ   delimited-[]  t       Q_{t}(W_{t},\xi_{[t]})   . Finally, at stage    t  =  0      t  0    t=0   , one solves the problem         max   x  0       E   [    Q  1    (   W  1   ,   Œæ   [  1  ]    )    ]           subject to     W  1     =      ‚àë   i  =  1   n     Œæ   i  ,  1     x   i  0              ‚àë   i  =  1   n    x   i  0       =     W   ;  0           x  0     ‚â•    0          subscript    subscript  x  0      E   delimited-[]     subscript  Q  1     subscript  W  1    subscript  Œæ   delimited-[]  1         missing-subexpression    missing-subexpression    missing-subexpression     subject to   subscript  W  1      superscript   subscript     i  1    n      subscript  Œæ   i  1     subscript  x    i  0       missing-subexpression      missing-subexpression     superscript   subscript     i  1    n    subscript  x    i  0       fragments  W   subscript  normal-;  0     missing-subexpression      missing-subexpression    subscript  x  0    0   missing-subexpression      \begin{array}[]{lrclr}\max\limits_{x_{0}}&E[Q_{1}(W_{1},\xi_{[1]})]&\\
 \text{subject to}&W_{1}&=&\sum_{i=1}^{n}\xi_{i,1}x_{i0}\\
 &\sum_{i=1}^{n}x_{i0}&=&W_{0}\\
 &x_{0}&\geq&0\end{array}     Stagewise independent random process  For a general distribution of the process    Œæ  t     subscript  Œæ  t    \xi_{t}   , it may be hard to solve these dynamic programming equations. The situation simplifies dramatically if the process    Œæ  t     subscript  Œæ  t    \xi_{t}   is stagewise independent, i.e.,    Œæ  t     subscript  Œæ  t    \xi_{t}   is (stochastically) independent of     Œæ  1   ,  ‚Ä¶  ,   Œæ   t  -  1        subscript  Œæ  1   normal-‚Ä¶   subscript  Œæ    t  1      \xi_{1},\dots,\xi_{t-1}   for    t  =   2  ,  ‚Ä¶  ,  T       t   2  normal-‚Ä¶  T     t=2,\dots,T   . In this case, the corresponding conditional expectations become unconditional expectations, and the function     Q  t    (   W  t   )        subscript  Q  t    subscript  W  t     Q_{t}(W_{t})   ,    t  =   1  ,  ‚Ä¶  ,   T  -  1        t   1  normal-‚Ä¶    T  1      t=1,\dots,T-1   does not depend on    Œæ   [  t  ]      subscript  Œæ   delimited-[]  t     \xi_{[t]}   . That is,     Q   T  -  1     (   W   T  -  1    )        subscript  Q    T  1     subscript  W    T  1      Q_{T-1}(W_{T-1})   is the optimal value of the problem         max   x   T  -  1        E   [   U   (   W  T   )    ]           subject to     W  T     =      ‚àë   i  =  1   n     Œæ   i  T     x   i  ,   T  -  1               ‚àë   i  =  1   n    x   i  ,   T  -  1        =     W   ;   T  -  1            x   T  -  1      ‚â•    0          subscript    subscript  x    T  1       E   delimited-[]    U   subscript  W  T       missing-subexpression    missing-subexpression    missing-subexpression     subject to   subscript  W  T      superscript   subscript     i  1    n      subscript  Œæ    i  T     subscript  x   i    T  1        missing-subexpression      missing-subexpression     superscript   subscript     i  1    n    subscript  x   i    T  1        fragments  W   subscript  normal-;    T  1      missing-subexpression      missing-subexpression    subscript  x    T  1     0   missing-subexpression      \begin{array}[]{lrclr}\max\limits_{x_{T-1}}&E[U(W_{T})]&\\
 \text{subject to}&W_{T}&=&\sum_{i=1}^{n}\xi_{iT}x_{i,T-1}\\
 &\sum_{i=1}^{n}x_{i,T-1}&=&W_{T-1}\\
 &x_{T-1}&\geq&0\end{array}     and     Q  t    (   W  t   )        subscript  Q  t    subscript  W  t     Q_{t}(W_{t})   is the optimal value of         max   x  t       E   [    Q   t  +  1     (   W   t  +  1    )    ]           subject to     W   t  +  1      =      ‚àë   i  =  1   n     Œæ   i  ,   t  +  1      x   i  ,  t              ‚àë   i  =  1   n    x   i  ,  t       =     W   ;  t           x  t     ‚â•    0          subscript    subscript  x  t      E   delimited-[]     subscript  Q    t  1     subscript  W    t  1        missing-subexpression    missing-subexpression    missing-subexpression     subject to   subscript  W    t  1       superscript   subscript     i  1    n      subscript  Œæ   i    t  1      subscript  x   i  t       missing-subexpression      missing-subexpression     superscript   subscript     i  1    n    subscript  x   i  t       fragments  W   subscript  normal-;  t     missing-subexpression      missing-subexpression    subscript  x  t    0   missing-subexpression      \begin{array}[]{lrclr}\max\limits_{x_{t}}&E[Q_{t+1}(W_{t+1})]&\\
 \text{subject to}&W_{t+1}&=&\sum_{i=1}^{n}\xi_{i,t+1}x_{i,t}\\
 &\sum_{i=1}^{n}x_{i,t}&=&W_{t}\\
 &x_{t}&\geq&0\end{array}     for    t  =    T  -  2   ,  ‚Ä¶  ,  1       t     T  2   normal-‚Ä¶  1     t=T-2,\dots,1   .  Stochastic programming for nonlinear optimization  Many of the optimization problems in science and engineering involve nonlinear objective functions with uncertain model. In these cases, stochastic programming is applied to optimize the expected objective (sample average) over a set of realizations generated using Monte Carlo simulation.  For expensive function evaluations, model selection is used to reduce the number of realizations. Techniques such as out-of-sample validation is used to reduce the number of required realizations and the number of representative realizations. Recently, optimization with sample validation (OSV) (also referred to as "multilevel optimization with validation", MLOV) is proposed to significantly reduce the computational cost in stochastic programming for expensive function evaluations. Optimization with sample validation determines, in a systematic way, the number of realizations in optimization to adequately represent the entire set. Stochastic programming with OSV has been applied for optimization of oil field development planning (well placement and control optimization). 7  Biological applications  Stochastic dynamic programming is frequently used to model animal behaviour in such fields as behavioural ecology . 8 9 Empirical tests of models of optimal foraging , life-history transitions such as fledging in birds and egg laying in parasitoid wasps have shown the value of this modelling technique in explaining the evolution of behavioural decision making. These models are typically many-staged, rather than two-staged.  Economic applications  Stochastic dynamic programming is a useful tool in understanding decision making under uncertainty. The accumulation of capital stock under uncertainty is one example; often it is used by resource economists to analyze bioeconomic problems 10 where the uncertainty enters in such as weather, etc.  Software tools  Modelling languages  All discrete stochastic programming problems can be represented with any algebraic modeling language , manually implementing explicit or implicit non-anticipativity to make sure the resulting model respects the structure of the information made available at each stage. An instance of an SP problem generated by a general modelling language tends to grow quite large (linearly in the number of scenarios), and its matrix looses the structure that is intrinsic to this class of problems, which could otherwise be exploited at solution time by specific decomposition algorithms. Extensions to modelling languages specifically designed for SP are starting to appear, see:   AIMMS - supports the definition of SP problems  FuncDesigner - free software that includes stochastic programming and optimization by OpenOpt solvers; example1 , example2 , example3  SAMPL - a set of extensions to AMPL specifically designed to express stochastic programs (includes syntax for chance constraints, integrated chance constraints and Robust Optimization problems)   They both can generate SMPS instance level format, which conveys in a non-redundant form the structure of the problem to the solver.  Solvers   FortSP - solver for stochastic programming problems; it accepts SMPS input and implements various decomposition algorithms.  NEOS Solvers - Three solvers are available in the Neos Server : Bouncing Nested Benders Solvers (BNBS) for multi-stage stochastic linear programs, ddsip for two-stage stochastic programs with integer recourse, and Stochastic Decomposition (SD) for two-stage stochastic linear programs.  COIN-OR Stochastic Modeling Interface - An open source project within COIN-OR . It can read Stochastic MPS 11 input format as well as supports direct interfaces for scenario input, and generates the deterministic equivalent linear program for solution by COIN-OR solvers.   See also   Probabilistic-based design optimization  SAMPL algebraic modeling language  Scenario optimization  Stochastic optimization  Stochastic control   References  Further reading   John R. Birge and Fran√ßois V. Louveaux. Introduction to Stochastic Programming . Springer Verlag, New York, 1997.       G. Ch. Pflug: Optimization of Stochastic Models. The Interface between Simulation and Optimization . Kluwer, Dordrecht, 1996.    Andras Prekopa . Stochastic Programming. Kluwer Academic Publishers, Dordrecht, 1995.    Andrzej Ruszczynski and Alexander Shapiro (eds.) (2003) Stochastic Programming . Handbooks in Operations Research and Management Science, Vol. 10, Elsevier.       Stein W. Wallace and William T. Ziemba (eds.) (2005) Applications of Stochastic Programming . MPS-SIAM Book Series on Optimization 5      External links   Stochastic Programming Community Home Page   "  Category:Stochastic optimization  Category:Stochastic algorithms  Category:Mathematical optimization  Category:Operations research     ‚Ü©  Stein W. Wallace and William T. Ziemba (eds.). Applications of Stochastic Programming . MPS-SIAM Book Series on Optimization 5, 2005. ‚Ü©  Applications of stochastic programming are described at the following website, Stochastic Programming Community . ‚Ü©  ‚Ü©  http://www.neos-server.org/neos/ ‚Ü©  ‚Ü©  ‚Ü©  Mangel, M. & Clark, C. W. 1988. Dynamic modeling in behavioral ecology. Princeton University Press ISBN 0-691-08506-4 ‚Ü©  Houston, A. I & McNamara, J. M. 1999. Models of adaptive behaviour: an approach based on state . Cambridge University Press ISBN 0-521-65539-0 ‚Ü©  Howitt, R., Msangi, S., Reynaud, A and K. Knapp. 2002. "Using Polynomial Approximations to Solve Stochastic Dynamic Programming Problems: or A "Betty Crocker " Approach to SDP." University of California, Davis, Department of Agricultural and Resource Economics Working Paper. ‚Ü©  J.R. Birge, M.A.H. Dempster, H.I. Gassmann, E.A. Gunn, A.J. King and S.W. Wallace, A standard input format for multiperiod stochastic linear programs , COAL Newsletter #17 (1987) pp. 1-19. ‚Ü©     