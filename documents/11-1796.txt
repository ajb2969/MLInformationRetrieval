   Hat matrix      Hat matrix   In statistics , the hat matrix , H , sometimes also called influence matrix 1 and projection matrix , maps the vector of response values to the vector of fitted values (or predicted values). It describes the influence each response value has on each fitted value. 2 3 The diagonal elements of the hat matrix are the leverages , which describe the influence each response value has on the fitted value for that same observation.  If the vector of response values is denoted by y and the vector of fitted values by ŷ ,        𝐲  ^   =   H  𝐲    .       normal-^  𝐲     H  𝐲     \hat{\mathbf{y}}=H\mathbf{y}.   As ŷ is usually pronounced "y-hat", the hat matrix is so named as it "puts a hat on y ". The formula for the vector of residuals  r can also be expressed compactly using the hat matrix:       𝐫  =   𝐲  -   𝐲  ^    =   𝐲  -   H  𝐲    =    (   I  -  H   )   𝐲    .        𝐫    𝐲   normal-^  𝐲           𝐲    H  𝐲             I  H   𝐲      \mathbf{r}=\mathbf{y}-\mathbf{\hat{y}}=\mathbf{y}-H\mathbf{y}=(I-H)\mathbf{y}.   Moreover, the element in the i th row and j th column of H is equal to the covariance between the j th response value and the i th fitted value, divided by the variance of the former:       h   i  j    =    cov   [    y  ^   i   ,   y  j   ]    /   var   [   y  j   ]          subscript  h    i  j       cov   subscript   normal-^  y   i    subscript  y  j     var   subscript  y  j       \displaystyle h_{ij}=\operatorname{cov}[\hat{y}_{i},y_{j}]/\operatorname{var}[%
 y_{j}]   The covariance matrix of the residuals is therefore, by error propagation , equal to      (   I  -  H   )   ⊤   Σ   (   I  -  H   )        superscript    I  H   top   normal-Σ    I  H     \left(I-H\right)^{\top}\Sigma\left(I-H\right)   , where Σ is the covariance matrix of the error vector (and by extension, the response vector as well). For the case of linear models with independent and identically distributed errors in which Σ = σ 2 I , this reduces to ( I − H ) σ 2 . 4  Many types of models and techniques are subject to this formulation. A few examples are:   Linear model / linear least squares  Smoothing splines  Regression splines  Local regression  Kernel regression  Linear filtering   Linear model  Suppose that we wish to estimate a linear model using linear least squares. The model can be written as       𝐲  =    X  𝜷   +  𝜺    ,      𝐲      X  𝜷   𝜺     \mathbf{y}=X\boldsymbol{\beta}+\boldsymbol{\varepsilon},   where X is a matrix of explanatory variables (the design matrix ), β is a vector of unknown parameters to be estimated, and ε is the error vector.  Solution with unit weights and uncorrelated errors  When the weights for each observation are identical and the errors are uncorrelated, the estimated parameters are        𝜷  ^   =     (    X  ⊤   X   )    -  1     X  ⊤   𝐲    ,       normal-^  𝜷      superscript     superscript  X  top   X     1     superscript  X  top   𝐲     \hat{\boldsymbol{\beta}}=\left(X^{\top}X\right)^{-1}X^{\top}\mathbf{y},     so the fitted values are        𝐲  ^   =   X   𝜷  ^    =   X    (    X  ⊤   X   )    -  1     X  ⊤   𝐲    .         normal-^  𝐲     X   normal-^  𝜷           X   superscript     superscript  X  top   X     1     superscript  X  top   𝐲      \hat{\mathbf{y}}=X\hat{\boldsymbol{\beta}}=X\left(X^{\top}X\right)^{-1}X^{\top%
 }\mathbf{y}.     Therefore the hat matrix is given by       H  =   X    (    X  ⊤   X   )    -  1     X  ⊤     .      H    X   superscript     superscript  X  top   X     1     superscript  X  top      H=X\left(X^{\top}X\right)^{-1}X^{\top}.     In the language of linear algebra , the hat matrix is the orthogonal projection onto the column space of the design matrix X . 5 (Note that      (    X  ⊤   X   )    -  1     X  ⊤        superscript     superscript  X  top   X     1     superscript  X  top     \left(X^{\top}X\right)^{-1}X^{\top}   is the pseudoinverse of X .)  Some facts of the hat matrix in this setting are summarized as follows: 6        𝐫  =    (   I  -  H   )   𝐲    ,      𝐫      I  H   𝐲     \mathbf{r}=(I-H)\mathbf{y},   and     𝐫  =   𝐲  -   H  𝐲    ⟂  X   .        𝐫    𝐲    H  𝐲      perpendicular-to    X     \mathbf{r}=\mathbf{y}-H\mathbf{y}\perp X.     H is symmetric, and so is I - H .  H is idempotent     H  2   =  H       superscript  H  2   H    H^{2}=H   , and so is I - H .  X is invariant under H       H  X   =  X   ,        H  X   X    HX=X,   hence      (   I  -  H   )   X   =  0          I  H   X   0    (I-H)X=0   .        (   I  -  H   )   H   =   H   (   I  -  H   )    =  0.            I  H   H     H    I  H         0.     (I-H)H=H(I-H)=0.      The hat matrix corresponding to a linear model is symmetric and idempotent , that is,     H  2   =  H       superscript  H  2   H    H^{2}=H   . However, this is not always the case; in locally weighted scatterplot smoothing (LOESS) , for example, the hat matrix is in general neither symmetric nor idempotent.  For linear models , the trace of the hat matrix is equal to the rank of X , which is the number of independent parameters of the linear model. For other models such as LOESS that are still linear in the observations y , the hat matrix can be used to define the effective degrees of freedom of the model.  The hat matrix has a number of useful algebraic properties. 7 8 Practical applications of the hat matrix in regression analysis include leverage and Cook's distance , which are concerned with identifying observations which have a large effect on the results of a regression.  More generally  Non-identical weights and/or correlated errors  The above may be generalized to the cases where the weights are not identical and/or the errors are correlated. Suppose that the covariance matrix of the errors is Σ. Then since        𝜷  ^   =     (    X  ⊤    Σ   -  1    X   )    -  1     X  ⊤     Σ   -  1     𝐲    ,       normal-^  𝜷      superscript     superscript  X  top    superscript  normal-Σ    1    X     1     superscript  X  top    superscript  normal-Σ    1    𝐲     \hat{\boldsymbol{\beta}}=\left(X^{\top}\Sigma^{-1}X\right)^{-1}X^{\top}\Sigma^%
 {-1}\,\mathbf{y},     the hat matrix is thus       H  =   X    (    X  ⊤    Σ   -  1    X   )    -  1     X  ⊤    Σ   -  1      ,      H    X   superscript     superscript  X  top    superscript  normal-Σ    1    X     1     superscript  X  top    superscript  normal-Σ    1       H=X\left(X^{\top}\Sigma^{-1}X\right)^{-1}X^{\top}\Sigma^{-1},\,     and again it may be seen that H 2 = H , though now it is no longer symmetric.  Blockwise formula  Suppose the design matrix   C   C   C   can be decomposed by columns as    C  =   [  A  ,  B  ]       C   A  B     C=[A,B]   . Define the Hat operator as     H   (  X  )    =   X    (    X  ⊤   X   )    -  1     X  ⊤          H  X     X   superscript     superscript  X  top   X     1     superscript  X  top      H(X)=X\left(X^{\top}X\right)^{-1}X^{\top}   . Similarly, define the residual operator as     M   (  X  )    =   I  -   H   (  X  )           M  X     I    H  X      M(X)=I-H(X)   . Then the Hat matrix of   C   C   C   can be decomposed as follows:       H   (  C  )    =    H   (  A  )    +   H   (   M   (  A  )   B   )           H  C       H  A     H    M  A  B       H(C)=H(A)+H(M(A)B)    9  There are a number of applications of such a partitioning. The classical application has   A   A   A   a column of all ones, which allows one to analyze the effects of adding an intercept term to a regression. Another use is in the fixed effects model, where   A   A   A   is a large sparse matrix of the dummy variables for the fixed effect terms. One can use this partition to compute the hat matrix of   C   C   C   without explicitly forming the matrix   C   C   C   , which might be too large to fit into computer memory.  See also   Moore–Penrose pseudoinverse  Studentized residuals  Effective degrees of freedom  Idempotent matrix  Mean and predicted response   References  "  Category:Statistical terminology  Category:Regression analysis  Category:Matrices     Data Assimilation: Observation inﬂuence diagnostic of a data assimilation system ↩  ↩  ↩     Gans, P. (1992) Data Fitting in the Chemical Sciences, , Wiley. ISBN 978-0-471-93412-7 ↩  Draper, N.R., Smith, H. (1998) Applied Regression Analysis , Wiley. ISBN 0-471-17082-8 ↩  ↩     