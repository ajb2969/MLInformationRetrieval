   Hat matrix      Hat matrix   In statistics , the hat matrix , H , sometimes also called influence matrix 1 and projection matrix , maps the vector of response values to the vector of fitted values (or predicted values). It describes the influence each response value has on each fitted value. 2 3 The diagonal elements of the hat matrix are the leverages , which describe the influence each response value has on the fitted value for that same observation.  If the vector of response values is denoted by y and the vector of fitted values by Å· ,        ğ²  ^   =   H  ğ²    .       normal-^  ğ²     H  ğ²     \hat{\mathbf{y}}=H\mathbf{y}.   As Å· is usually pronounced "y-hat", the hat matrix is so named as it "puts a hat on y ". The formula for the vector of residuals  r can also be expressed compactly using the hat matrix:       ğ«  =   ğ²  -   ğ²  ^    =   ğ²  -   H  ğ²    =    (   I  -  H   )   ğ²    .        ğ«    ğ²   normal-^  ğ²           ğ²    H  ğ²             I  H   ğ²      \mathbf{r}=\mathbf{y}-\mathbf{\hat{y}}=\mathbf{y}-H\mathbf{y}=(I-H)\mathbf{y}.   Moreover, the element in the i th row and j th column of H is equal to the covariance between the j th response value and the i th fitted value, divided by the variance of the former:       h   i  j    =    cov   [    y  ^   i   ,   y  j   ]    /   var   [   y  j   ]          subscript  h    i  j       cov   subscript   normal-^  y   i    subscript  y  j     var   subscript  y  j       \displaystyle h_{ij}=\operatorname{cov}[\hat{y}_{i},y_{j}]/\operatorname{var}[%
 y_{j}]   The covariance matrix of the residuals is therefore, by error propagation , equal to      (   I  -  H   )   âŠ¤   Î£   (   I  -  H   )        superscript    I  H   top   normal-Î£    I  H     \left(I-H\right)^{\top}\Sigma\left(I-H\right)   , where Î£ is the covariance matrix of the error vector (and by extension, the response vector as well). For the case of linear models with independent and identically distributed errors in which Î£ = Ïƒ 2 I , this reduces to ( I âˆ’ H ) Ïƒ 2 . 4  Many types of models and techniques are subject to this formulation. A few examples are:   Linear model / linear least squares  Smoothing splines  Regression splines  Local regression  Kernel regression  Linear filtering   Linear model  Suppose that we wish to estimate a linear model using linear least squares. The model can be written as       ğ²  =    X  ğœ·   +  ğœº    ,      ğ²      X  ğœ·   ğœº     \mathbf{y}=X\boldsymbol{\beta}+\boldsymbol{\varepsilon},   where X is a matrix of explanatory variables (the design matrix ), Î² is a vector of unknown parameters to be estimated, and Îµ is the error vector.  Solution with unit weights and uncorrelated errors  When the weights for each observation are identical and the errors are uncorrelated, the estimated parameters are        ğœ·  ^   =     (    X  âŠ¤   X   )    -  1     X  âŠ¤   ğ²    ,       normal-^  ğœ·      superscript     superscript  X  top   X     1     superscript  X  top   ğ²     \hat{\boldsymbol{\beta}}=\left(X^{\top}X\right)^{-1}X^{\top}\mathbf{y},     so the fitted values are        ğ²  ^   =   X   ğœ·  ^    =   X    (    X  âŠ¤   X   )    -  1     X  âŠ¤   ğ²    .         normal-^  ğ²     X   normal-^  ğœ·           X   superscript     superscript  X  top   X     1     superscript  X  top   ğ²      \hat{\mathbf{y}}=X\hat{\boldsymbol{\beta}}=X\left(X^{\top}X\right)^{-1}X^{\top%
 }\mathbf{y}.     Therefore the hat matrix is given by       H  =   X    (    X  âŠ¤   X   )    -  1     X  âŠ¤     .      H    X   superscript     superscript  X  top   X     1     superscript  X  top      H=X\left(X^{\top}X\right)^{-1}X^{\top}.     In the language of linear algebra , the hat matrix is the orthogonal projection onto the column space of the design matrix X . 5 (Note that      (    X  âŠ¤   X   )    -  1     X  âŠ¤        superscript     superscript  X  top   X     1     superscript  X  top     \left(X^{\top}X\right)^{-1}X^{\top}   is the pseudoinverse of X .)  Some facts of the hat matrix in this setting are summarized as follows: 6        ğ«  =    (   I  -  H   )   ğ²    ,      ğ«      I  H   ğ²     \mathbf{r}=(I-H)\mathbf{y},   and     ğ«  =   ğ²  -   H  ğ²    âŸ‚  X   .        ğ«    ğ²    H  ğ²      perpendicular-to    X     \mathbf{r}=\mathbf{y}-H\mathbf{y}\perp X.     H is symmetric, and so is I - H .  H is idempotent     H  2   =  H       superscript  H  2   H    H^{2}=H   , and so is I - H .  X is invariant under H       H  X   =  X   ,        H  X   X    HX=X,   hence      (   I  -  H   )   X   =  0          I  H   X   0    (I-H)X=0   .        (   I  -  H   )   H   =   H   (   I  -  H   )    =  0.            I  H   H     H    I  H         0.     (I-H)H=H(I-H)=0.      The hat matrix corresponding to a linear model is symmetric and idempotent , that is,     H  2   =  H       superscript  H  2   H    H^{2}=H   . However, this is not always the case; in locally weighted scatterplot smoothing (LOESS) , for example, the hat matrix is in general neither symmetric nor idempotent.  For linear models , the trace of the hat matrix is equal to the rank of X , which is the number of independent parameters of the linear model. For other models such as LOESS that are still linear in the observations y , the hat matrix can be used to define the effective degrees of freedom of the model.  The hat matrix has a number of useful algebraic properties. 7 8 Practical applications of the hat matrix in regression analysis include leverage and Cook's distance , which are concerned with identifying observations which have a large effect on the results of a regression.  More generally  Non-identical weights and/or correlated errors  The above may be generalized to the cases where the weights are not identical and/or the errors are correlated. Suppose that the covariance matrix of the errors is Î£. Then since        ğœ·  ^   =     (    X  âŠ¤    Î£   -  1    X   )    -  1     X  âŠ¤     Î£   -  1     ğ²    ,       normal-^  ğœ·      superscript     superscript  X  top    superscript  normal-Î£    1    X     1     superscript  X  top    superscript  normal-Î£    1    ğ²     \hat{\boldsymbol{\beta}}=\left(X^{\top}\Sigma^{-1}X\right)^{-1}X^{\top}\Sigma^%
 {-1}\,\mathbf{y},     the hat matrix is thus       H  =   X    (    X  âŠ¤    Î£   -  1    X   )    -  1     X  âŠ¤    Î£   -  1      ,      H    X   superscript     superscript  X  top    superscript  normal-Î£    1    X     1     superscript  X  top    superscript  normal-Î£    1       H=X\left(X^{\top}\Sigma^{-1}X\right)^{-1}X^{\top}\Sigma^{-1},\,     and again it may be seen that H 2 = H , though now it is no longer symmetric.  Blockwise formula  Suppose the design matrix   C   C   C   can be decomposed by columns as    C  =   [  A  ,  B  ]       C   A  B     C=[A,B]   . Define the Hat operator as     H   (  X  )    =   X    (    X  âŠ¤   X   )    -  1     X  âŠ¤          H  X     X   superscript     superscript  X  top   X     1     superscript  X  top      H(X)=X\left(X^{\top}X\right)^{-1}X^{\top}   . Similarly, define the residual operator as     M   (  X  )    =   I  -   H   (  X  )           M  X     I    H  X      M(X)=I-H(X)   . Then the Hat matrix of   C   C   C   can be decomposed as follows:       H   (  C  )    =    H   (  A  )    +   H   (   M   (  A  )   B   )           H  C       H  A     H    M  A  B       H(C)=H(A)+H(M(A)B)    9  There are a number of applications of such a partitioning. The classical application has   A   A   A   a column of all ones, which allows one to analyze the effects of adding an intercept term to a regression. Another use is in the fixed effects model, where   A   A   A   is a large sparse matrix of the dummy variables for the fixed effect terms. One can use this partition to compute the hat matrix of   C   C   C   without explicitly forming the matrix   C   C   C   , which might be too large to fit into computer memory.  See also   Mooreâ€“Penrose pseudoinverse  Studentized residuals  Effective degrees of freedom  Idempotent matrix  Mean and predicted response   References  "  Category:Statistical terminology  Category:Regression analysis  Category:Matrices     Data Assimilation: Observation inï¬‚uence diagnostic of a data assimilation system â†©  â†©  â†©     Gans, P. (1992) Data Fitting in the Chemical Sciences, , Wiley. ISBN 978-0-471-93412-7 â†©  Draper, N.R., Smith, H. (1998) Applied Regression Analysis , Wiley. ISBN 0-471-17082-8 â†©  â†©     