<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1941">Yarowsky algorithm</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Yarowsky algorithm</h1>
<hr/>

<p>In <a href="computational_linguistics" title="wikilink">computational linguistics</a> the <strong>Yarowsky algorithm</strong> is an <a href="unsupervised_learning" title="wikilink">unsupervised learning</a> <a class="uri" href="algorithm" title="wikilink">algorithm</a> for <a href="word_sense_disambiguation" title="wikilink">word sense disambiguation</a> that uses the "one sense per <a class="uri" href="collocation" title="wikilink">collocation</a>" and the "one sense per discourse" properties of <a href="natural_language" title="wikilink">human languages</a> for word sense disambiguation. From observation, words tend to exhibit only one sense in most given discourse and in a given collocation.</p>
<h2 id="application">Application</h2>

<p>The algorithm starts with a large, untagged <a href="text_corpus" title="wikilink">corpus</a>, in which it identifies examples of the given <a href="polysemy" title="wikilink">polysemous</a> word, and stores all the relevant <a href="Sentence_(linguistics)" title="wikilink">sentences</a> as lines. For instance, Yarowsky uses the word "plant" in his 1995 paper to demonstrate the algorithm. If it is assumed that there are two possible senses of the word, the next step is to identify a small number of seed collocations representative of each sense, give each sense a label (i.e. sense A and B), then assign the appropriate label to all training examples containing the seed collocations. In this case, the words "life" and "manufacturing" are chosen as initial seed collocations for senses A and B respectively. The residual examples (85%–98% according to Yarowsky) remain untagged.</p>

<p>The algorithm should initially choose seed collocations representative that will distinguish sense A and B accurately and productively. This can be done by selecting seed words from a <a class="uri" href="dictionary" title="wikilink">dictionary</a>’s entry for that sense. The collocations tend to have stronger effect if they are adjacent to the target word, the effect weakens with distance. According to the criteria given in Yarowsky (1993), seed words that appear in the most reliable collocational relationships with the target word will be selected. The effect is much stronger for words in a <a href="wikt:predicate" title="wikilink">predicate</a>-argument relationship than for arbitrary associations at the same distance to the target word, and is much stronger for collocations with content words than with function words. Having said this, a collocation word can have several collocational relationships with the target word throughout the corpus. This could give the word different rankings or even different classifications. Alternatively, it can be done by identifying a single defining collocate for each class, and using for seeds only those contexts containing one of these defining words. A publicly available database <a class="uri" href="WordNet" title="wikilink">WordNet</a> can be used as an automatic source for such defining terms. In addition, words that occur near the target word in great frequency can be selected as seed collocations representative. This approach is not fully automatic, a human judge must decide which word will be selected for each target word’s sense, the outputs will be reliable indicators of the senses.</p>

<p>A <a href="decision_list" title="wikilink">decision list</a> algorithm is then used to identify other reliable collocations. This training algorithm calculates the probability Pr(Sense | Collocation), and the decision list is ranked by the log-likelihood ratio:</p>

<p>

<math display="block" id="Yarowsky_algorithm:0">
 <semantics>
  <mrow>
   <mi>log</mi>
   <mrow>
    <mo>(</mo>
    <mfrac>
     <mrow>
      <mi>Pr</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mtext>Sense</mtext>
        <mi>A</mi>
       </msub>
       <mo stretchy="false">|</mo>
       <msub>
        <mtext>Collocation</mtext>
        <mi>i</mi>
       </msub>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mrow>
      <mi>Pr</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mtext>Sense</mtext>
        <mi>B</mi>
       </msub>
       <mo stretchy="false">|</mo>
       <msub>
        <mtext>Collocation</mtext>
        <mi>i</mi>
       </msub>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mfrac>
    <mo>)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <log></log>
    <apply>
     <divide></divide>
     <apply>
      <ci>Pr</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <mtext>Sense</mtext>
       <ci>A</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <mtext>Collocation</mtext>
       <ci>i</ci>
      </apply>
     </apply>
     <apply>
      <ci>Pr</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <mtext>Sense</mtext>
       <ci>B</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <mtext>Collocation</mtext>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \log\left(\frac{\Pr(\text{Sense}_{A}|\text{Collocation}_{i})}{\Pr(\text{Sense}%
_{B}|\text{Collocation}_{i})}\right)
  </annotation>
 </semantics>
</math>

</p>

<p>A <a class="uri" href="smoothing" title="wikilink">smoothing</a> algorithm will then be used to avoid 0 values. The decision-list algorithm resolves many problems in a large set of non-independent evidence source by using only the most reliable piece of evidence rather than the whole matching collocation set.</p>

<p>The new resulting classifier will then be applied to the whole sample set. Add those examples in the <a href="errors_and_residuals_in_statistics" title="wikilink">residual</a> that are tagged as A or B with probability above a reasonable threshold to the seed sets. The decision-list algorithm and the above adding step are applied <a href="iteration" title="wikilink">iteratively</a>. As more newly-learned collocations are added to the seed sets, the sense A or sense B set will grow, and the original residual will shrink. However, these collocations stay in the seed sets only if their probability of classification remains above the threshold, otherwise they are returned to the residual for later classification. At the end of each iteration, the "one sense per discourse" property can be used to help preventing initially mistagged collocates and hence improving the purity of the seed sets.</p>

<p>In order to avoid strong collocates becoming indicators for the wrong class, the class-inclusion threshold needs to be randomly altered. For the same purpose, after intermediate convergence the algorithm will also need to increase the width of the context window.</p>

<p>The algorithm will continue to iterate until no more reliable collocations are found. The ‘One sense per discourse’ property can be used here for error correction. For a target word that has a binary sense partition, if the occurrences of the majority sense A exceed that of the minor sense B by a certain threshold, the minority ones will be relabeled as A. According to Yarowsky, for any sense to be clearly dominant, the occurrences of the target word should not be less than 4.</p>

<p>When the algorithm converges on a stable residual set, a final decision list of the target word is obtained. The most reliable collocations are at the top of the new list instead of the original seed words. The original untagged corpus is then tagged with sense labels and probabilities. The final decision list may now be applied to new data, the collocation with the highest rank in the list is used to classify the new data. For example, if the highest ranking collocation of the target word in the new data set is of sense A, then the target word is classified as sense A.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Semantic_net" title="wikilink">Semantic net</a></li>
<li><a href="Word_sense_disambiguation" title="wikilink">Word sense disambiguation</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li>Yarowsky, D. "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods". <em>Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics</em>. Cambridge, MA, pp. 189–196, 1995.</li>
</ul>

<p>"</p>

<p><a href="Category:Corpus_linguistics" title="wikilink">Category:Corpus linguistics</a> <a href="Category:Word-sense_disambiguation" title="wikilink">Category:Word-sense disambiguation</a></p>
</body>
</html>
