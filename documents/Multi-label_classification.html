<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1517">Multi-label classification</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Multi-label classification</h1>
<hr/>

<p>In <a href="machine_learning" title="wikilink">machine learning</a>, <strong>multi-label classification</strong> and the strongly related problem of <strong>multi-output classification</strong> are variants of the <a href="statistical_classification" title="wikilink">classification</a> problem where multiple target labels must be assigned to each instance. Multi-label classification should not be confused with <a href="multiclass_classification" title="wikilink">multiclass classification</a>, which is the problem of categorizing instances into more than two classes. Formally, multi-label learning can be phrased as the problem of finding a model that maps inputs <strong>x</strong> to binary vectors <strong>y</strong>, rather than scalar outputs as in the ordinary classification problem.</p>

<p>There are two main methods for tackling the multi-label classification problem:<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> problem transformation methods and algorithm adaptation methods. Problem transformation methods transform the multi-label problem into a set of <a href="binary_classification" title="wikilink">binary classification</a> problems, which can then be handled using single-class classifiers. Algorithm adaptation methods adapt the algorithms to directly perform multi-label classification. In other words, rather than trying to convert the problem to a simpler problem, they try to address the problem in its full form.</p>
<h2 id="problem-transformation-methods">Problem transformation methods</h2>

<p>Several problem transformation methods exist for multi-label classification; the baseline approach, called the <em>binary relevance</em> method,<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> amounts to independently training one binary classifier for each label. Given an unseen sample, the combined model then predicts all labels for this sample for which the respective classifiers predict a positive result. This method of dividing the task into multiple binary tasks has something in common with the one-vs.-all (OvA, or one-vs.-rest, OvR) method for <a href="multiclass_classification" title="wikilink">multiclass classification</a>. Note though that it is not the same method: in binary relevance we train one classifier for each label, not one classifier for each possible value for the label.</p>

<p>Various other transformations exist. Of these, the label powerset (LP) transformation creates one binary classifier for every label combination attested in the training set.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> The random 

<math display="inline" id="Multi-label_classification:0">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

-labelsets (RAKEL) algorithm uses multiple LP classifiers, each trained on a random subset of the actual labels; prediction using this <a href="ensemble_learning" title="wikilink">ensemble method</a> proceeds by a voting scheme.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>

<p><a href="Classifier_chains" title="wikilink">Classifier chains</a> are an alternative ensembling method <a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> that have been applied, for instance, in <a class="uri" href="HIV" title="wikilink">HIV</a> drug resistance prediction.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h2 id="adapted-algorithms-for-multi-label-classification">Adapted algorithms for multi-label classification</h2>

<p>Some classification algorithms/models have been adaptated to the multi-label task, without requiring problem transformations. Examples of these include:</p>
<ul>
<li><a class="uri" href="boosting" title="wikilink">boosting</a>: AdaBoost.MH and AdaBoost.MR are extended versions of <a class="uri" href="AdaBoost" title="wikilink">AdaBoost</a> for multi-label data.</li>
<li><a href="k-nearest_neighbors" title="wikilink">k-nearest neighbors</a>: the ML-kNN algorithm extends the k-NN classifier to multi-label data.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></li>
<li><a href="decision_trees" title="wikilink">decision trees</a>: "Clare" is an adapted C4.5 algorithm for multi-label classification; the modification involves the entropy calculations.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></li>
<li><a href="kernel_methods_for_vector_output" title="wikilink">kernel methods for vector output</a></li>
<li><a href="neural_networks" title="wikilink">neural networks</a>: BP-MLL is an adaptation of the popular back-propagation algorithm for multi-label learning.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></li>
</ul>
<h2 id="statistics-and-evaluation-metrics">Statistics and evaluation metrics</h2>

<p>The extent to which a dataset is multi-label can be captured in two statistics:<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></p>
<ul>
<li>Label cardinality is the average number of labels per example in the set

<math display="block" id="Multi-label_classification:1">
 <semantics>
  <mrow>
   <mfrac>
    <mn>1</mn>
    <mi>N</mi>
   </mfrac>
   <mrow>
    <munderover>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>N</mi>
    </munderover>
    <mrow>
     <mo stretchy="false">|</mo>
     <msub>
      <mi>Y</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">|</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <ci>N</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>N</ci>
     </apply>
     <apply>
      <abs></abs>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Y</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{1}{N}\sum_{i=1}^{N}|Y_{i}|
  </annotation>
 </semantics>
</math>

;</li>
<li>label density is the number of labels per sample divided by the total number of labels, averaged over the samples

<math display="block" id="Multi-label_classification:2">
 <semantics>
  <mrow>
   <mfrac>
    <mn>1</mn>
    <mi>N</mi>
   </mfrac>
   <mrow>
    <munderover>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>N</mi>
    </munderover>
    <mfrac>
     <mrow>
      <mo stretchy="false">|</mo>
      <msub>
       <mi>Y</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">|</mo>
     </mrow>
     <mrow>
      <mo stretchy="false">|</mo>
      <mi>L</mi>
      <mo stretchy="false">|</mo>
     </mrow>
    </mfrac>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <ci>N</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>N</ci>
     </apply>
     <apply>
      <divide></divide>
      <apply>
       <abs></abs>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Y</ci>
        <ci>i</ci>
       </apply>
      </apply>
      <apply>
       <abs></abs>
       <ci>L</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{1}{N}\sum_{i=1}^{N}\frac{|Y_{i}|}{|L|}
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Multi-label_classification:3">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mo largeop="true" mathsize="160%" stretchy="false" symmetric="true">⋃</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>N</mi>
    </msubsup>
    <msub>
     <mi>Y</mi>
     <mi>i</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>L</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <union></union>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>N</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L=\bigcup_{i=1}^{N}Y_{i}
  </annotation>
 </semantics>
</math>


.</li>
</ul>

<p>Evaluation metrics for multi-label classification performance are inherently different from those used in multi-class (or binary) classification, due to the inherent differences of the classification problem. If 

<math display="inline" id="Multi-label_classification:4">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>

 denotes the true set of labels for a given sample, and 

<math display="inline" id="Multi-label_classification:5">
 <semantics>
  <mi>P</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>P</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P
  </annotation>
 </semantics>
</math>

 the predicted set of labels, then the following metrics can be defined on that sample:</p>
<ul>
<li><a href="Hamming_distance" title="wikilink">Hamming</a> loss: the fraction of the wrong labels to the total number of labels. This is a <a href="loss_function" title="wikilink">loss function</a>, so the optimal value is zero. The closely related Hamming score, also called accuracy in the multi-label setting, is defined as the number of correct labels divided by the union of predicted and true labels, 

<math display="inline" id="Multi-label_classification:6">
 <semantics>
  <mfrac>
   <mrow>
    <mo stretchy="false">|</mo>
    <mrow>
     <mi>T</mi>
     <mo>∩</mo>
     <mi>P</mi>
    </mrow>
    <mo stretchy="false">|</mo>
   </mrow>
   <mrow>
    <mo stretchy="false">|</mo>
    <mrow>
     <mi>T</mi>
     <mo>∪</mo>
     <mi>P</mi>
    </mrow>
    <mo stretchy="false">|</mo>
   </mrow>
  </mfrac>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <abs></abs>
     <apply>
      <intersect></intersect>
      <ci>T</ci>
      <ci>P</ci>
     </apply>
    </apply>
    <apply>
     <abs></abs>
     <apply>
      <union></union>
      <ci>T</ci>
      <ci>P</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{|T\cap P|}{|T\cup P|}
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></li>
<li><a href="Precision_and_recall" title="wikilink">Precision, recall</a> and <a href="F1_score" title="wikilink">

<math display="inline" id="Multi-label_classification:7">
 <semantics>
  <msub>
   <mi>F</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>F</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F_{1}
  </annotation>
 </semantics>
</math>

 score</a>: precision is 

<math display="inline" id="Multi-label_classification:8">
 <semantics>
  <mfrac>
   <mrow>
    <mo stretchy="false">|</mo>
    <mrow>
     <mi>T</mi>
     <mo>∩</mo>
     <mi>P</mi>
    </mrow>
    <mo stretchy="false">|</mo>
   </mrow>
   <mrow>
    <mo stretchy="false">|</mo>
    <mi>P</mi>
    <mo stretchy="false">|</mo>
   </mrow>
  </mfrac>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <abs></abs>
     <apply>
      <intersect></intersect>
      <ci>T</ci>
      <ci>P</ci>
     </apply>
    </apply>
    <apply>
     <abs></abs>
     <ci>P</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{|T\cap P|}{|P|}
  </annotation>
 </semantics>
</math>


, recall is 

<math display="inline" id="Multi-label_classification:9">
 <semantics>
  <mfrac>
   <mrow>
    <mo stretchy="false">|</mo>
    <mrow>
     <mi>T</mi>
     <mo>∩</mo>
     <mi>P</mi>
    </mrow>
    <mo stretchy="false">|</mo>
   </mrow>
   <mrow>
    <mo stretchy="false">|</mo>
    <mi>T</mi>
    <mo stretchy="false">|</mo>
   </mrow>
  </mfrac>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <abs></abs>
     <apply>
      <intersect></intersect>
      <ci>T</ci>
      <ci>P</ci>
     </apply>
    </apply>
    <apply>
     <abs></abs>
     <ci>T</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{|T\cap P|}{|T|}
  </annotation>
 </semantics>
</math>

, and 

<math display="inline" id="Multi-label_classification:10">
 <semantics>
  <msub>
   <mi>F</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>F</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F_{1}
  </annotation>
 </semantics>
</math>

 is their <a href="harmonic_mean" title="wikilink">harmonic mean</a>.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></li>
<li>Exact match: is the most strict metric, indicating the percentage of samples that have all their labels classified correctly.</li>
</ul>

<p>Cross-validation in multi-label settings is complicated by the fact that the ordinary (binary/multiclass) way of <a href="stratified_sampling" title="wikilink">stratified sampling</a> will not work; alternative ways of approximate stratified sampling have been suggested.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></p>
<h2 id="implementations-and-datasets">Implementations and datasets</h2>

<p>Java implementations of multi-label algorithms are available in the <a href="http://mulan.sourceforge.net/">Mulan</a> and <a href="http://meka.sourceforge.net/">Meka</a> software packages, both based on <a href="Weka_(machine_learning)" title="wikilink">Weka</a>.</p>

<p>The <a class="uri" href="scikit-learn" title="wikilink">scikit-learn</a> python package implements some <a href="http://scikit-learn.org/stable/modules/multiclass.html">multi-labels algorithms and metrics</a>.</p>

<p>A list of commonly used multi-label data-sets is available at the <a href="http://mulan.sourceforge.net/datasets.html">Mulan website</a>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Multiclass_classification" title="wikilink">Multiclass classification</a></li>
<li><a href="Multiple-instance_learning" title="wikilink">Multiple-instance learning</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="further-reading">Further reading</h2>
<ul>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Classification_algorithms" title="wikilink">Category:Classification algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3"></li>
<li id="fn4"></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6">Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank. <a href="http://www.springerlink.com/content/ft3620x22m88p875/">Classifier Chains for Multi-label Classification</a>. Machine Learning Journal. Springer. Vol. 85(3), (2011).<a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"></li>
<li id="fn14"><a href="#fnref14">↩</a></li>
</ol>
</section>
</body>
</html>
