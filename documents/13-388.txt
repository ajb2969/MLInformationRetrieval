   Marcinkiewicz–Zygmund inequality      Marcinkiewicz–Zygmund inequality   In mathematics , the Marcinkiewicz–Zygmund inequality , named after Józef Marcinkiewicz and Antoni Zygmund , gives relations between moments of a collection of independent random variables . It is a generalization of the rule for the sum of variances of independent random variables to moments of arbitrary order.  Statement of the inequality  Theorem  J. Marcinkiewicz and A. Zygmund. Sur les foncions independantes. Fund. Math. , 28:60–90, 1937. Reprinted in Józef Marcinkiewicz, Collected papers , edited by Antoni Zygmund, Panstwowe Wydawnictwo Naukowe, Warsaw, 1964, pp. 233–259.  Yuan Shih Chow and Henry Teicher. Probability theory. Independence, interchangeability, martingales . Springer-Verlag, New York, second edition, 1988.  If    x  i     subscript  x  i    \textstyle x_{i}   ,    i  =   1  ,  …  ,  n       i   1  normal-…  n     \textstyle i=1,\ldots,n   , are independent random variables such that     E   (   x  i   )    =  0        E   subscript  x  i    0    \textstyle E\left(x_{i}\right)=0   and     E   (    |   x  i   |   p   )    <   +  ∞         E   superscript     subscript  x  i    p          \textstyle E\left(\left|x_{i}\right|^{p}\right)<+\infty   ,    1  ≤  p  <   +  ∞         1  p              \textstyle 1\leq p<+\infty   ,      A  p     subscript  A  p    \textstyle A_{p}     where    B  p     subscript  B  p    \textstyle B_{p}   and   p   p   \textstyle p   are positive constants, which depend only on    p  =  2      p  2    \textstyle p=2   .  The second-order case  In the case     A  2   =   B  2   =  1         subscript  A  2    subscript  B  2        1     \textstyle A_{2}=B_{2}=1   , the inequality holds with     E   (   x  i   )    =  0        E   subscript  x  i    0    \textstyle E\left(x_{i}\right)=0   , and it reduces to the rule for the sum of variances of independent random variables with zero mean, known from elementary statistics: If     E   (    |   x  i   |   2   )    <   +  ∞         E   superscript     subscript  x  i    2          \textstyle E\left(\left|x_{i}\right|^{2}\right)<+\infty   and      Var   (    ∑   i  =  1   n    x  i    )    =   E   (    |    ∑   i  =  1   n    x  i    |   2   )    =    ∑   i  =  1   n     ∑   j  =  1   n    E   (    x  i     x  ¯   j    )      =    ∑   i  =  1   n    E   (    |   x  i   |   2   )     =    ∑   i  =  1   n    Var   (   x  i   )      .          Var    superscript   subscript     i  1    n    subscript  x  i       E   superscript      superscript   subscript     i  1    n    subscript  x  i     2           superscript   subscript     i  1    n     superscript   subscript     j  1    n     E     subscript  x  i    subscript   normal-¯  x   j              superscript   subscript     i  1    n     E   superscript     subscript  x  i    2            superscript   subscript     i  1    n     Var   subscript  x  i        \mathrm{Var}\left(\sum_{i=1}^{n}x_{i}\right)=E\left(\left|\sum_{i=1}^{n}x_{i}%
 \right|^{2}\right)=\sum_{i=1}^{n}\sum_{j=1}^{n}E\left(x_{i}\overline{x}_{j}%
 \right)=\sum_{i=1}^{n}E\left(\left|x_{i}\right|^{2}\right)=\sum_{i=1}^{n}%
 \mathrm{Var}\left(x_{i}\right).   , then  $$\mathrm{Var}\left(\sum_{i=1}^{n}x_{i}\right)=E\left(  \left\vert \sum_{i=1}^{n}x_{i}\right\vert ^{2}\right)  =\sum_{i=1}^{n}\sum_{j=1}^{n}E\left( x_{i}\overline{x}_{j}\right)  =\sum_{i=1}^{n}E\left(  \left\vert x_{i}\right\vert ^{2}\right)  =\sum_{i=1}^{n}\mathrm{Var}\left(x_{i}\right).$$  See also  Several similar moment inequalities are known as Khintchine inequality and Rosenthal inequalities , and there are also extensions to more general symmetric statistics of independent random variables. 1  Notes   "  Category:Statistical inequalities  Category:Probabilistic inequalities  Category:Probability theorems  Category:Theorems in functional analysis     R. Ibragimov and Sh. Sharakhmetov. Analogues of Khintchine, Marcinkiewicz–Zygmund and Rosenthal inequalities for symmetric statistics. Scandinavian Journal of Statistics , 26(4):621–633, 1999. ↩     