   Point set registration      Point set registration   In computer vision and pattern recognition , point set registration , also known as point matching , is the process of finding a spatial transformation that aligns two point sets . The purpose of finding such a transformation includes merging multiple data sets into a globally consistent model, and mapping a new measurement to a known data set to identify features or to estimate its pose . A point set may be raw data from 3D scanning or an array of rangefinders . For use in image processing and feature-based image registration , a point set may be a set of features obtained by feature extraction from an image, for example corner detection . Point set registration is used in optical character recognition 1 2 and aligning data from magnetic resonance imaging with computer aided tomography scans. 3 4  Overview of problem  The problem may be summarized as follows: 5 Let    {  ℳ  ,  𝒮  }     ℳ  𝒮    \{\mathcal{M},\mathcal{S}\}   be two finite size point sets in a finite-dimensional real vector space    ℝ  d     superscript  ℝ  d    \mathbb{R}^{d}   , which contain   M   M   M   and   N   N   N   points respectively. The problem is to find a transformation to be applied to the moving "model" point set   ℳ   ℳ   \mathcal{M}   such that the difference between   ℳ   ℳ   \mathcal{M}   and the static "scene" set   𝒮   𝒮   \mathcal{S}   is minimized. In other words, a mapping from    ℝ  d     superscript  ℝ  d    \mathbb{R}^{d}   to    ℝ  d     superscript  ℝ  d    \mathbb{R}^{d}   is desired which yields the best alignment between the transformed "model" set and the "scene" set. The mapping may consist of a rigid or non-rigid transformation. The transformation model may be written as   T   T   T   where the transformed, registered model point set is:  It is useful to define an optimization parameter   θ   θ   \theta   :  such that it is clear that the optimizing algorithm adjusts   θ   θ   \theta   . Depending on the problem and number of dimensions, there may be more such parameters. The output of a point set registration algorithm is therefore the transformation parameter   θ   θ   \theta   of model   T   T   T   so that   ℳ   ℳ   \mathcal{M}   is optimally aligned to   𝒮   𝒮   \mathcal{S}   .  In convergence, it is desired for the distance between the two point sets to reach a global minimum. This is difficult without exhausting all possible transformations, so a local minimum suffices. The distance function between a transformed model point set    T   (  ℳ  )       T  ℳ    T(\mathcal{M})   and the scene point set   𝒮   𝒮   \mathcal{S}   is given by some function   dist   dist   \operatorname{dist}   . A simple approach is to take the square of the Euclidean distance for every pair of points:  (m - s)^2| 3 }}  Minimizing such a function in rigid registration is equivalent to solving a least squares problem. However, this function is sensitive to outlier data and consequently algorithms based on this function tend to be less robust against noisy data. A more robust formulation of the cost function uses some robust function    g   g   g   :  (T(\mathcal{M}), \mathcal{S}) = \sum_{m \in T(\mathcal{M})} \sum_{s \in \mathcal{S}} g((m - s)^2)| 4 }}  Such a formulation is known as an M-estimator . The robust function   g   g   g   is chosen such that the local configuration of the point set is insensitive to distant points, hence making it robust against outliers and noise. 6  Rigid registration  Given two point sets, rigid registration yields a rigid transformation which maps one point set to the other. A rigid transformation is defined as a transformation that does not change the distance between any two points. Typically such a transformation consists of translation and rotation . 7 In rare cases, the point set may also be mirrored.  Non-rigid registration  Given two point sets, non-rigid registration yields a non-rigid transformation which maps one point set to the other. Non-rigid transformations include affine transformations such as scaling and shear mapping . However, in the context of point set registration, non-rigid registration typically involves nonlinear transformation. If the eigenmodes of variation of the point set are known, the nonlinear transformation may be parametrized by the eigenvalues. 8 A nonlinear transformation may also be parametrized as a thin plate spline . 9 10  Point set registration algorithms  Some approaches to point set registration use algorithms that solve the more general graph matching problem. 11 However, the computational complexity of such methods tend to be high and they are limited to rigid registrations. Algorithms specific to the point set registration problem are described in the following sections.  Iterative closest point  The iterative closest point (ICP) algorithm was introduced by Besl and McKay. 12 The algorithm performs rigid registration in an iterative fashion by assuming that every point in   ℳ   ℳ   \mathcal{M}   corresponds with the closest point to it in   𝒮   𝒮   \mathcal{S}   , and then finding the least squares rigid transformation. As such, it works best if the initial pose of   ℳ   ℳ   \mathcal{M}   is sufficiently close to   𝒮   𝒮   \mathcal{S}   . In pseudocode , the basic algorithm is implemented as follows:   Algorithm  ICP     (  ℳ  ,  𝒮  )     ℳ  𝒮    (\mathcal{M},\mathcal{S})        θ   θ   \theta   :=    θ  0     subscript  θ  0    \theta_{0}     while not registered:      X   X   X    :=    ∅     \emptyset     for      m  i   ∈   T   (  ℳ  ,  θ  )         subscript  m  i     T   ℳ  θ      m_{i}\in T(\mathcal{M},\theta)    :        s  ^   j     subscript   normal-^  s   j    \hat{s}_{j}    := closest point in    𝒮   𝒮   \mathcal{S}    to     m  i     subscript  m  i    m_{i}        X   X   X   :=   X   X   X   +    ⟨   m  i   ,    s  ^   j   ⟩      subscript  m  i    subscript   normal-^  s   j     \langle m_{i},\hat{s}_{j}\rangle      θ   θ   \theta   := least squares     (  X  )    X   (X)     return    θ   θ   \theta     Here, the function '''least_squares''' performs least squares regression to minimize the distance in each of the    ⟨   m  i   ,    s  ^   j   ⟩      subscript  m  i    subscript   normal-^  s   j     \langle m_{i},\hat{s}_{j}\rangle   pairs, i.e. minimizing the distance function in Equation ().  Because the cost function of registration depends on finding the closest point in   𝒮   𝒮   \mathcal{S}   to every point in   ℳ   ℳ   \mathcal{M}   , it can change as the algorithm is running. As such, it is difficult to prove that ICP will in fact converge exactly to the local optimum. 13 In fact, empirically, ICP and EM-ICP do not converge to the local minimum of the cost function. 14 Nonetheless, because ICP is intuitive to understand and straightforward to implement, it remains the most commonly used point set registration algorithm. 15 Many variants of ICP have been proposed, affecting all phases of the algorithm from the selection and matching of points to the minimization strategy. 16 17 For example, the expectation maximization algorithm is applied to the ICP algorithm to form the EM-ICP method, and the Levenberg-Marquardt algorithm is applied to the ICP algorithm to form the LM-ICP method. 18  Robust point matching  Robust point matching (RPM) was introduced by Gold et al. 19 The method performs registration using deterministic annealing and soft assignment of correspondences between point sets. Whereas in ICP the correspondence generated by the nearest-neighbour heuristic is binary, RPM uses a soft correspondence where the correspondence between any two points can be anywhere from 0 to 1, although it ultimately converges to either 0 or 1. The correspondences found in RPM is always one-to-one, which is not always the case in ICP. 20 Let    m  i     subscript  m  i    m_{i}   be the   i   i   i   th point in   ℳ   ℳ   \mathcal{M}   and    s  j     subscript  s  j    s_{j}   be the   j   j   j   th point in   𝒮   𝒮   \mathcal{S}   . The match matrix    μ   μ   \mathbf{\mu}   is defined as such:  The problem is then defined as: Given two point sets   ℳ   ℳ   \mathcal{M}   and   𝒮   𝒮   \mathcal{S}   find the Affine transformation    T   T   T   and the match matrix   μ   μ   \mathbf{\mu}   that best relates them. 21 Knowing the optimal transformation makes it easy to determine the match matrix, and vice versa. However, the RPM algorithm determines both simultaneously. The transformation may be decomposed into a translation vector and a transformation matrix:       T   (  m  )    =    𝐀  m   +  𝐭         T  m       𝐀  m   𝐭     T(m)=\mathbf{A}m+\mathbf{t}     The matrix   𝐀   𝐀   \mathbf{A}   in 2D is composed of four separate parameters    {  a  ,  θ  ,  b  ,  c  }     a  θ  b  c    \{a,\theta,b,c\}   , which are scale, rotation, and the vertical and horizontal shear components respectively. The cost function is then:  subject to     ∀    j     ∑   i  =  1   M    μ   i  j       ≤  1       for-all    j    superscript   subscript     i  1    M    subscript  μ    i  j       1    \forall j~{}\sum_{i=1}^{M}\mu_{ij}\leq 1   ,     ∀    i     ∑   j  =  1   N    μ   i  j       ≤  1       for-all    i    superscript   subscript     j  1    N    subscript  μ    i  j       1    \forall i~{}\sum_{j=1}^{N}\mu_{ij}\leq 1   ,     ∀   i   j    μ   i  j      ∈   {  0  ,  1  }        for-all    i  j   subscript  μ    i  j       0  1     \forall ij~{}\mu_{ij}\in\{0,1\}   . The   α   α   \alpha   term biases the objective towards stronger correlation by decreasing the cost if the match matrix has more ones in it. The function    g   (  𝐀  )       g  𝐀    g(\mathbf{A})   serves to regularize the Affine transformation by penalizing large values of the scale and shear components:       g   (   𝐀   (  a  ,  θ  ,  b  ,  c  )    )    =   γ   (    a  2   +   b  2   +   c  2    )          g    𝐀   a  θ  b  c       γ     superscript  a  2    superscript  b  2    superscript  c  2       g(\mathbf{A}(a,\theta,b,c))=\gamma(a^{2}+b^{2}+c^{2})     for some regularization parameter   γ   γ   \gamma   .  The RPM method optimizes the cost function using the Softassign algorithm. The 1D case will be derived here. Given a set of variables    {   Q  j   }      subscript  Q  j     \{Q_{j}\}   where     Q  j   ∈   ℝ  1        subscript  Q  j    superscript  ℝ  1     Q_{j}\in\mathbb{R}^{1}   . A variable    μ  j     subscript  μ  j    \mu_{j}   is associated with each    Q  j     subscript  Q  j    Q_{j}   such that      ∑   j  =  1   J    μ  j    =  1        superscript   subscript     j  1    J    subscript  μ  j    1    \sum_{j=1}^{J}\mu_{j}=1   . The goal is to find   μ   μ   \mathbf{\mu}   that maximizes     ∑   j  =  1   J     μ  j    Q  j        superscript   subscript     j  1    J      subscript  μ  j    subscript  Q  j      \sum_{j=1}^{J}\mu_{j}Q_{j}   . This can be formulated as a continuous problem by introducing a control parameter    β  >  0      β  0    \beta>0   . In the deterministic annealing method, the control parameter   β   β   \beta   is slowly increased as the algorithm runs. Let   μ   μ   \mathbf{\mu}   be:  = \frac{\exp{(\beta Q_{\hat{j}})}}{\sum_{j=1}^J \exp{(\beta Q_j)}} | rpm.3 }}  this is known as the softmax function . As   β   β   \beta   increases, it approaches a binary value as desired in Equation (). The problem may now be generalized to the 2D case, where instead of maximizing     ∑   j  =  1   J     μ  j    Q  j        superscript   subscript     j  1    J      subscript  μ  j    subscript  Q  j      \sum_{j=1}^{J}\mu_{j}Q_{j}   , the following is maximized:  where       Q   i  j    =   -   (     ∥    s  j   -  𝐭  -   𝐀   m  i     ∥   2   -  α   )    =   -    ∂  cost    ∂   μ   i  j              subscript  Q    i  j         superscript   norm     subscript  s  j   𝐭    𝐀   subscript  m  i      2   α               cost      subscript  μ    i  j          Q_{ij}=-(\lVert s_{j}-\mathbf{t}-\mathbf{A}m_{i}\rVert^{2}-\alpha)=-\frac{%
 \partial\operatorname{cost}}{\partial\mu_{ij}}     This is straightforward, except that now the constraints on   μ   μ   \mu   are doubly stochastic matrix constraints     ∀    j     ∑   i  =  1   M    μ   i  j       =  1       for-all    j    superscript   subscript     i  1    M    subscript  μ    i  j       1    \forall j~{}\sum_{i=1}^{M}\mu_{ij}=1   and     ∀    i     ∑   j  =  1   N    μ   i  j       =  1       for-all    i    superscript   subscript     j  1    N    subscript  μ    i  j       1    \forall i~{}\sum_{j=1}^{N}\mu_{ij}=1   . As such the denominator from Equation () cannot be expressed for the 2D case simply. To satisfy the constraints, it is possible to use a result due to Sinkhorn, 22 which states that a doubly stochastic matrix is obtained from any square matrix with all positive entries by the iterative process of alternating row and column normalizations. Thus the algorithm is written as such: 23   Algorithm  RPM2D     (  ℳ  ,  𝒮  )     ℳ  𝒮    (\mathcal{M},\mathcal{S})        𝐭   𝐭   \mathbf{t}   := 0       a  ,  θ  ,  b  ,  c     a  θ  b  c    a,\theta,b,c    := 0      β   β   \beta    :=     β  0     subscript  β  0    \beta_{0}          μ  ^    i  j      subscript   normal-^  μ     i  j     \hat{\mu}_{ij}   :=    1  +  ϵ      1  ϵ    1+\epsilon     while     β  <   β  f       β   subscript  β  f     \beta<\beta_{f}    :  while    μ   μ   \mu    has not converged:   //  update  correspondence  parameters  by  softassign       Q   i  j      subscript  Q    i  j     Q_{ij}    :=     -    ∂  cost    ∂   μ   i  j              cost      subscript  μ    i  j        -\frac{\partial\operatorname{cost}}{\partial\mu_{ij}}         μ   i  j   0     subscript   superscript  μ  0     i  j     \mu^{0}_{ij}   :=    exp   (   β   Q   i  j     )         β   subscript  Q    i  j       \exp(\beta Q_{ij})      //  apply  Sinkhorn's  method  while     μ  ^     normal-^  μ    \hat{\mu}    has not converged:   //  update     μ  ^     normal-^  μ    \hat{\mu}    by  normalizing  across  all  rows:        μ  ^    i  j   1     subscript   superscript   normal-^  μ   1     i  j     \hat{\mu}^{1}_{ij}    :=       μ  ^    i  j   0      ∑   i  =  1    M  +  1       μ  ^    i  j   0         subscript   superscript   normal-^  μ   0     i  j      superscript   subscript     i  1      M  1     subscript   superscript   normal-^  μ   0     i  j       \frac{\hat{\mu}^{0}_{ij}}{\sum_{i=1}^{M+1}\hat{\mu}^{0}_{ij}}      //  update     μ  ^     normal-^  μ    \hat{\mu}    by  normalizing  across  all  columns:        μ  ^    i  j   0     subscript   superscript   normal-^  μ   0     i  j     \hat{\mu}^{0}_{ij}    :=       μ  ^    i  j   1      ∑   j  =  1    N  +  1       μ  ^    i  j   1         subscript   superscript   normal-^  μ   1     i  j      superscript   subscript     j  1      N  1     subscript   superscript   normal-^  μ   1     i  j       \frac{\hat{\mu}^{1}_{ij}}{\sum_{j=1}^{N+1}\hat{\mu}^{1}_{ij}}      //  update  pose  parameters  by  coordinate  descent  update    θ   θ   \theta    using analytical solution  update    𝐭   𝐭   \mathbf{t}    using analytical solution  update     a  ,  b  ,  c     a  b  c    a,b,c    using Newton's  method      β   β   \beta    :=      β  r   β       subscript  β  r   β    \beta_{r}\beta        γ   γ   \gamma   :=    γ   β  r       γ   subscript  β  r     \frac{\gamma}{\beta_{r}}     return     a  ,  b  ,  c  ,  θ     a  b  c  θ    a,b,c,\theta    and    𝐭   𝐭   \mathbf{t}     where the deterministic annealing control parameter   β   β   \beta   is initially set to    β  0     subscript  β  0    \beta_{0}   and increases by factor    β  r     subscript  β  r    \beta_{r}   until it reaches the maximum value    β  f     subscript  β  f    \beta_{f}   . The summations in the normalization steps sum to    M  +  1      M  1    M+1   and    N  +  1      N  1    N+1   instead of just   M   M   M   and   N   N   N   because the constraints on   μ   μ   \mu   are inequalities. As such the    M  +  1      M  1    M+1   th and    N  +  1      N  1    N+1   th elements are slack variables .  The algorithm can also be extended for point sets in 3D or higher dimensions. The constraints on the correspondence matrix   μ   μ   \mathbf{\mu}   are the same in the 3D case as in the 2D case. Hence the structure of the algorithm remains unchanged, with the main difference being how the rotation and translation matrices are solved. 24  Thin plate spline robust point matching  The thin plate spline robust point matching (TPS-RPM) algorithm by Chui and Rangarajan augments the RPM method to perform non-rigid registration by parametrizing the transformation as a thin plate spline . 25 However, because the thin plate spline parametrization only exists in three dimensions, the method cannot be extended to problems involving four or more dimensions.  Kernel correlation  The kernel correlation (KC) approach of point set registration was introduced by Tsin and Kanade. 26 Compared with ICP, the KC algorithm is more robust against noisy data. Unlike ICP, where, for every model point, only the closest scene point is considered, here every scene point affects every model point. 27 As such this is a multiply-linked registration algorithm. For some kernel function    K   K   K   , the kernel correlation    K  C      K  C    KC   of two points     x  i   ,   x  j       subscript  x  i    subscript  x  j     x_{i},x_{j}   is defined thus: 28   The kernel function    K   K   K   chosen for point set registration is typically symmetric and non-negative kernel, similar to the ones used in the Parzen window density estimation. The Gaussian kernel typically used for its simplicity, although other ones like the Epanechnikov kernel and the tricube kernel may be substituted. 29 The kernel correlation of an entire point set   χ   χ   \mathcal{\chi}   is defined as the sum of the kernel correlations of every point in the set to every other point in the set: 30  The KC of a point set is proportional, within a constant factor, to the logarithm of the information entropy . Observe that the KC is a measure of a "compactness" of the point set—trivially, if all points in the point set were at the same location, the KC would evaluate to zero. The cost function of the point set registration algorithm for some transformation parameter   θ   θ   \theta   is defined thus:  \sum_{s \in \mathcal{S}} KC(s, T(m, \theta))| kc.3 }}  Some algebraic manipulation yields:  The expression is simplified by observing that    K  C   (  𝒮  )       K  C  𝒮    KC(\mathcal{S})   is independent of   θ   θ   \theta   . Furthermore, assuming rigid registration,    K  C   (   T   (  ℳ  ,  θ  )    )       K  C    T   ℳ  θ      KC(T(\mathcal{M},\theta))   is invariant when   θ   θ   \theta   is changed because the Euclidean distance between every pair of points stays the same under rigid transformation . So the above equation may be rewritten as:  The kernel density estimates are defined as:        P  ℳ    (  x  ,  θ  )    =    1  N     ∑   m  ∈  ℳ     K   (  x  ,   T   (  m  ,  θ  )    )             subscript  P  ℳ    x  θ        1  N     subscript     m  ℳ      K   x    T   m  θ          P_{\mathcal{M}}(x,\theta)=\frac{1}{N}\sum_{m\in\mathcal{M}}K(x,T(m,\theta))           P  𝒮    (  x  )    =    1  N     ∑   s  ∈  𝒮     K   (  x  ,  s  )             subscript  P  𝒮   x       1  N     subscript     s  𝒮      K   x  s        P_{\mathcal{S}}(x)=\frac{1}{N}\sum_{s\in\mathcal{S}}K(x,s)     The cost function can then be shown to be the correlation of the two kernel density estimates:  \cdot P_{\mathcal{S}} ~ dx| kc.6 }}  Having established the cost function , the algorithm simply uses gradient descent to find the optimal transformation. It is computationally expensive to compute the cost function from scratch on every iteration, so a discrete version of the cost function Equation () is used. The kernel density estimates     P  ℳ   ,   P  𝒮       subscript  P  ℳ    subscript  P  𝒮     P_{\mathcal{M}},P_{\mathcal{S}}   can be evaluated at grid points and stored in a lookup table . Unlike the ICP and related methods, it is not necessary to fund the nearest neighbour, which allows the KC algorithm to be comparatively simple in implementation.  Compared to ICP and EM-ICP for noisy 2D and 3D point sets, the KC algorithm is less sensitive to noise and results in correct registration more often. 31  Gaussian mixture model  The kernel density estimates are sums of Gaussians and may therefore be represented as Gaussian mixture models (GMM). 32 Jian and Vemuri use the GMM version of the KC registration algorithm to perform non-rigid registration parametrized by thin plate splines .  Coherent point drift  Coherent point drift (CPD) was introduced by Myronenko and Song. 33 34 The algorithm takes a probabilistic approach to aligning point sets, similar to the GMM KC method. Unlike earlier approaches to non-rigid registration which assume a thin plate spline transformation model, CPD is agnostic with regard to the transformation model used. The point set   ℳ   ℳ   \mathcal{M}   represents the Gaussian mixture model (GMM) centroids. When the two point sets are optimally aligned, the correspondence is the maximum of the GMM posterior probability for a given data point. To preserve the topological structure of the point sets, the GMM centroids are forced to move coherently as a group. The expectation maximization algorithm is used to optimize the cost function. 35  Let there be   M   M   M   points in   ℳ   ℳ   \mathcal{M}   and   N   N   N   points in   𝒮   𝒮   \mathcal{S}   . The GMM probability density function for a point   s   s   s   is:  where, in   D   D   D   dimensions,    p   (  s  |  i  )      fragments  p   fragments  normal-(  s  normal-|  i  normal-)     p(s|i)   is the Gaussian distribution centered on point     m  i   ∈  ℳ       subscript  m  i   ℳ    m_{i}\in\mathcal{M}   .      p   (  s  |  i  )   =   1    (   2  π   σ  2    )    D  /  2  )     exp   (  -     ∥   s  -   m  i    ∥   2    2   σ  2     )      fragments  p   fragments  normal-(  s  normal-|  i  normal-)      1   superscript    2  π   superscript  σ  2     fragments  D   2  normal-)       fragments  normal-(      superscript   norm    s   subscript  m  i     2     2   superscript  σ  2     normal-)     p(s|i)=\frac{1}{(2\pi\sigma^{2})^{D/2)}}\exp{\left(-\frac{\lVert s-m_{i}\rVert%
 ^{2}}{2\sigma^{2}}\right)}     The membership probabilities     P   (  i  )    =   1  M         P  i     1  M     P(i)=\frac{1}{M}   is equal for all GMM components. The weight of the uniform distribution is denoted as    w  ∈   [  0  ,  1  ]       w   0  1     w\in[0,1]   . The mixture model is then:  The GMM centroids are re-parametrized by a set of parameters   θ   θ   \theta   estimated by maximizing the likelihood. This is equivalent to minimizing the negative log-likelihood function :  where it is assumed that the data is independent and identically distributed . The correspondence probability between two points    m  i     subscript  m  i    m_{i}   and    s  j     subscript  s  j    s_{j}   is defined as the posterior probability of the GMM centroid given the data point:      P   (  i  |   s  j   )   =    P   (  i  )   p   (   s  j   |  i  )     p   (   s  j   )        fragments  P   fragments  normal-(  i  normal-|   subscript  s  j   normal-)       fragments  P   fragments  normal-(  i  normal-)   p   fragments  normal-(   subscript  s  j   normal-|  i  normal-)      p   subscript  s  j       P(i|s_{j})=\frac{P(i)p(s_{j}|i)}{p(s_{j})}     The expectation maximization (EM) algorithm is used to find   θ   θ   \theta   and    σ  2     superscript  σ  2    \sigma^{2}   . The EM algorithm consists of two steps. First, in the E-step or estimation step, it guesses the values of parameters ("old" parameter values) and then uses Bayes' theorem to compute the posterior probability distributions     P  old    (  i  ,   s  j   )        superscript  P  old    i   subscript  s  j      P^{\text{old}}(i,s_{j})   of mixture components. Second, in the M-step or maximization step, the "new" parameter values are then found by minimizing the expectation of the complete negative log-likelihood function, i.e. the cost function:  (i|s_j) \log(P^{\text{new}}(i) p^{\text{new}}(s_j|i)) | cpd.4 }}  Ignoring constants independent of   θ   θ   \theta   and   σ   σ   \sigma   , Equation () can be expressed thus:  (i|s_j) \lVert s_j - T(m_i,\theta) \rVert^2  + \frac{N_\mathbf{P}D}{2}\log{\sigma^2}  | cpd.5 }}  where       N  𝐏   =   ∑   j  =  0   N    ∑   i  =  0   M    P  old    (  i  |   s  j   )   ≤  N     fragments   subscript  N  𝐏     superscript   subscript     j  0    N    superscript   subscript     i  0    M    superscript  P  old    fragments  normal-(  i  normal-|   subscript  s  j   normal-)    N    N_{\mathbf{P}}=\sum_{j=0}^{N}\sum_{i=0}^{M}P^{\text{old}}(i|s_{j})\leq N     with    N  =   N  𝐏       N   subscript  N  𝐏     N=N_{\mathbf{P}}   only if    w  =  0      w  0    w=0   . The posterior probabilities of GMM components computed using previous parameter values    P  old     superscript  P  old    P^{\text{old}}   is:  (i|s_j) =  \frac  {\exp  \left(  -\frac{1}{2\sigma^{\text{old}2}} \lVert s_j - T(m_i, \theta^{\text{old}})\rVert^2  \right) }  {\sum_{k=1}^{M} \exp  \left(  -\frac{1}{2\sigma^{\text{old}2}} \lVert s_j - T(m_k, \theta^{\text{old}})\rVert^2  \right) + (2\pi \sigma^2)^\frac{D}{2} \frac{w}{1-w} \frac{M}{N}}  | cpd.6 }}  Minimizing the cost function in Equation () necessarily decreases the negative log-likelihood function   E   E   E   in Equation () unless it is already at a local minimum. 36 Thus, the algorithm can be expressed using the following pseudocode, where the point sets   ℳ   ℳ   \mathcal{M}   and   𝒮   𝒮   \mathcal{S}   are represented as    M  ×  D      M  D    M\times D   and    N  ×  D      N  D    N\times D   matrices   𝐌   𝐌   \mathbf{M}   and   𝐒   𝐒   \mathbf{S}   respectively: 37   Algorithm  CPD     (  ℳ  ,  𝒮  )     ℳ  𝒮    (\mathcal{M},\mathcal{S})        θ   θ   \theta   :=    θ  0     subscript  θ  0    \theta_{0}     initialize     0  ≤  w  ≤  1        0  w       1     0\leq w\leq 1         σ  2     superscript  σ  2    \sigma^{2}   :=     1   D  N  M      ∑   j  =  1   N     ∑   i  =  1   M     ∥    s  j   -   m  i    ∥   2           1    D  N  M      superscript   subscript     j  1    N     superscript   subscript     i  1    M    superscript   norm     subscript  s  j    subscript  m  i     2       \frac{1}{DNM}\sum_{j=1}^{N}\sum_{i=1}^{M}\lVert s_{j}-m_{i}\rVert^{2}     while not registered:   //  E-step,  compute    𝐏   𝐏   \mathbf{P}     for     i  ∈   [  1  ,  M  ]       i   1  M     i\in[1,M]    and     j  ∈   [  1  ,  N  ]       j   1  N     j\in[1,N]    :       p   i  j      subscript  p    i  j     p_{ij}    :=      exp   (   -    1   2   σ  2       ∥    s  j   -   T   (   m  i   ,  θ  )     ∥   2     )        ∑   k  =  1   M     exp   (   -    1   2   σ  2       ∥    s  j   -   T   (   m  k   ,  θ  )     ∥   2     )     +     (   2  π   σ  2    )    D  2     w   1  -  w     M  N                 1    2   superscript  σ  2      superscript   norm     subscript  s  j     T    subscript  m  i   θ      2          superscript   subscript     k  1    M           1    2   superscript  σ  2      superscript   norm     subscript  s  j     T    subscript  m  k   θ      2          superscript    2  π   superscript  σ  2      D  2      w    1  w      M  N       \frac{\exp\left(-\frac{1}{2\sigma^{2}}\lVert s_{j}-T(m_{i},\theta)\rVert^{2}%
 \right)}{\sum_{k=1}^{M}\exp\left(-\frac{1}{2\sigma^{2}}\lVert s_{j}-T(m_{k},%
 \theta)\rVert^{2}\right)+(2\pi\sigma^{2})^{\frac{D}{2}}\frac{w}{1-w}\frac{M}{N}}      //  M-step,  solve  for  optimal  transformation       {  θ  ,   σ  2   }     θ   superscript  σ  2     \{\theta,\sigma^{2}\}    := solve     (  𝐒  ,  𝐌  ,  𝐏  )     𝐒  𝐌  𝐏    (\mathbf{S},\mathbf{M},\mathbf{P})     return    θ   θ   \theta     where the vector   𝟏   1   \mathbf{1}   is a column vector of ones. The '''solve''' function differs by the type of registration performed. For example, in rigid registration, the output is a scale   a   a   a   , a rotation matrix   𝐑   𝐑   \mathbf{R}   , and a translation vector   𝐭   𝐭   \mathbf{t}   . The parameter   θ   θ   \theta   can be written as a tuple of these:      θ  =   {  a  ,  𝐑  ,  𝐭  }       θ   a  𝐑  𝐭     \theta=\{a,\mathbf{R},\mathbf{t}\}     which is initialized to one, the identity matrix , and a column vector of zeroes:       θ  0   =   {  1  ,  𝐈  ,  𝟎  }        subscript  θ  0    1  𝐈  0     \theta_{0}=\{1,\mathbf{I},\mathbf{0}\}     The aligned point set is:       T   (  𝐌  )    =    a   𝐌𝐑  T    +   𝟏   𝐭  T           T  𝐌       a   superscript  𝐌𝐑  T      1   superscript  𝐭  T       T(\mathbf{M})=a\mathbf{M}\mathbf{R}^{T}+\mathbf{1}\mathbf{t}^{T}     The '''solve_rigid''' function for rigid registration can then be written as follows, with derivation of the algebra explained in Myronenko's 2010 paper. 38   solve_rigid     (  𝐒  ,  𝐌  ,  𝐏  )     𝐒  𝐌  𝐏    (\mathbf{S},\mathbf{M},\mathbf{P})         N  𝐏     subscript  N  𝐏    N_{\mathbf{P}}   :=     𝟏  T   𝐏𝟏       superscript  1  T   𝐏𝟏    \mathbf{1}^{T}\mathbf{P}\mathbf{1}       μ  s     subscript  μ  s    \mu_{s}   :=     1   N  𝐏     𝐒  T    𝐏  T   𝟏        1   subscript  N  𝐏     superscript  𝐒  T    superscript  𝐏  T   1    \frac{1}{N_{\mathbf{P}}}\mathbf{S}^{T}\mathbf{P}^{T}\mathbf{1}       μ  m     subscript  μ  m    \mu_{m}   :=     1   N  𝐏     𝐌  T   𝐏𝟏        1   subscript  N  𝐏     superscript  𝐌  T   𝐏𝟏    \frac{1}{N_{\mathbf{P}}}\mathbf{M}^{T}\mathbf{P}\mathbf{1}       𝐒  ^     normal-^  𝐒    \hat{\mathbf{S}}   :=    𝐒  -   𝟏   μ  s  T        𝐒    1   superscript   subscript  μ  s   T      \mathbf{S}-\mathbf{1}\mu_{s}^{T}       𝐌  ^     normal-^  𝐌    \hat{\mathbf{M}}   :=    𝐌  -   𝟏   μ  m  T        𝐌    1   superscript   subscript  μ  m   T      \mathbf{M}-\mathbf{1}\mu_{m}^{T}      𝐀   𝐀   \mathbf{A}   :=      𝐒  T   ^    𝐏  T    𝐌  ^        normal-^   superscript  𝐒  T     superscript  𝐏  T    normal-^  𝐌     \hat{\mathbf{S}^{T}}\mathbf{P}^{T}\hat{\mathbf{M}}       𝐔  ,  𝐕     𝐔  𝐕    \mathbf{U},\mathbf{V}   := svd     (  𝐀  )    𝐀   (\mathbf{A})    // the singular value decomposition of    𝐀  =   𝐔  Σ   𝐕  T        𝐀    𝐔  normal-Σ   superscript  𝐕  T      \mathbf{A}=\mathbf{U}\Sigma\mathbf{V}^{T}         𝐂   𝐂   \mathbf{C}    :=     diag   (  1  ,  …  ,  1  ,   det   (   𝐔𝐕  T   )    )      diag  1  normal-…  1     superscript  𝐔𝐕  T      \operatorname{diag}(1,...,1,\det(\mathbf{UV}^{T}))     //     diag   (  ξ  )      diag  ξ    \operatorname{diag}(\xi)    is  the  diagonal  matrix  formed  from  vector    ξ   ξ   \xi         𝐑   𝐑   \mathbf{R}    :=     𝐔𝐂𝐕  T     superscript  𝐔𝐂𝐕  T    \mathbf{UCV}^{T}        a   a   a   :=     tr   (    𝐀  T   𝐑   )     tr   (     𝐌  ^   𝐓     diag   (  𝐏𝟏  )     𝐌  ^     )         tr     superscript  𝐀  T   𝐑     tr     superscript   normal-^  𝐌   𝐓     diag  𝐏𝟏    normal-^  𝐌        \frac{\operatorname{tr}(\mathbf{A}^{T}\mathbf{R})}{\operatorname{tr}(\mathbf{%
 \hat{\mathbf{M}}^{T}\operatorname{diag}(\mathbf{P}\mathbf{1})\hat{\mathbf{M}}})}    //   tr   tr   \operatorname{tr}   is the trace of a matrix      𝐭   𝐭   \mathbf{t}    :=      μ  s   -   a  𝐑   μ  m         subscript  μ  s     a  𝐑   subscript  μ  m      \mu_{s}-a\mathbf{R}\mu_{m}         σ  2     superscript  σ  2    \sigma^{2}   :=      1    N  𝐏   D     (   tr   (     𝐒  ^   𝐓     diag   (    𝐏  𝐓   𝟏   )     𝐒  ^     )    )    -   a   tr   (    𝐀  T   𝐑   )             1     subscript  N  𝐏   D     tr     superscript   normal-^  𝐒   𝐓     diag     superscript  𝐏  𝐓   1     normal-^  𝐒         a   tr     superscript  𝐀  T   𝐑       \frac{1}{N_{\mathbf{P}}D}(\operatorname{tr}(\mathbf{\hat{\mathbf{S}}^{T}%
 \operatorname{diag}(\mathbf{P}^{T}\mathbf{1})\hat{\mathbf{S}}}))-a%
 \operatorname{tr}(\mathbf{A}^{T}\mathbf{R})     return      {  a  ,  𝐑  ,  𝐭  }   ,   σ  2       a  𝐑  𝐭    superscript  σ  2     \{a,\mathbf{R},\mathbf{t}\},\sigma^{2}     For affine registration, where the goal is to find an affine transformation instead of a rigid one, the output is an affine transformation matrix   𝐁   𝐁   \mathbf{B}   and a translation   𝐭   𝐭   \mathbf{t}   such that the aligned point set is:       T   (  𝐌  )    =    𝐌𝐁  T   +   𝟏   𝐭  T           T  𝐌      superscript  𝐌𝐁  T     1   superscript  𝐭  T       T(\mathbf{M})=\mathbf{M}\mathbf{B}^{T}+\mathbf{1}\mathbf{t}^{T}     The '''solve_affine''' function for rigid registration can then be written as follows, with derivation of the algebra explained in Myronenko's 2010 paper. 39   solve_affine     (  𝐒  ,  𝐌  ,  𝐏  )     𝐒  𝐌  𝐏    (\mathbf{S},\mathbf{M},\mathbf{P})         N  𝐏     subscript  N  𝐏    N_{\mathbf{P}}   :=     𝟏  T   𝐏𝟏       superscript  1  T   𝐏𝟏    \mathbf{1}^{T}\mathbf{P}\mathbf{1}       μ  s     subscript  μ  s    \mu_{s}   :=     1   N  𝐏     𝐒  T    𝐏  T   𝟏        1   subscript  N  𝐏     superscript  𝐒  T    superscript  𝐏  T   1    \frac{1}{N_{\mathbf{P}}}\mathbf{S}^{T}\mathbf{P}^{T}\mathbf{1}       μ  m     subscript  μ  m    \mu_{m}   :=     1   N  𝐏     𝐌  T   𝐏𝟏        1   subscript  N  𝐏     superscript  𝐌  T   𝐏𝟏    \frac{1}{N_{\mathbf{P}}}\mathbf{M}^{T}\mathbf{P}\mathbf{1}       𝐒  ^     normal-^  𝐒    \hat{\mathbf{S}}   :=    𝐒  -   𝟏   μ  s  T        𝐒    1   superscript   subscript  μ  s   T      \mathbf{S}-\mathbf{1}\mu_{s}^{T}       𝐌  ^     normal-^  𝐌    \hat{\mathbf{M}}   :=    𝐌  -   𝟏   μ  s  T        𝐌    1   superscript   subscript  μ  s   T      \mathbf{M}-\mathbf{1}\mu_{s}^{T}      𝐁   𝐁   \mathbf{B}   :=     (     𝐒  T   ^    𝐏  T    𝐌  ^    )     (     𝐌  T   ^     diag   (  𝐏𝟏  )     𝐌  ^     )    -  1           normal-^   superscript  𝐒  T     superscript  𝐏  T    normal-^  𝐌     superscript     normal-^   superscript  𝐌  T      diag  𝐏𝟏    normal-^  𝐌       1      (\hat{\mathbf{S}^{T}}\mathbf{P}^{T}\hat{\mathbf{M}})(\hat{\mathbf{M}^{T}}%
 \operatorname{diag}(\mathbf{P}\mathbf{1})\hat{\mathbf{M}})^{-1}      𝐭   𝐭   \mathbf{t}   :=     μ  s   -   𝐁   μ  m         subscript  μ  s     𝐁   subscript  μ  m      \mu_{s}-\mathbf{B}\mu_{m}       σ  2     superscript  σ  2    \sigma^{2}   :=      1    N  𝐏   D     (   tr   (    𝐒  ^     diag   (    𝐏  𝐓   𝟏   )     𝐒  ^     )    )    -   tr   (     𝐒  T   ^    𝐏  T    𝐌  ^    𝐁  T    )            1     subscript  N  𝐏   D     tr     normal-^  𝐒     diag     superscript  𝐏  𝐓   1     normal-^  𝐒        tr     normal-^   superscript  𝐒  T     superscript  𝐏  T    normal-^  𝐌    superscript  𝐁  T       \frac{1}{N_{\mathbf{P}}D}(\operatorname{tr}(\mathbf{\hat{\mathbf{S}}%
 \operatorname{diag}(\mathbf{P}^{T}\mathbf{1})\hat{\mathbf{S}}}))-\operatorname%
 {tr}(\hat{\mathbf{S}^{T}}\mathbf{P}^{T}\hat{\mathbf{M}}\mathbf{B}^{T})     return     𝐁  ,  𝐭  }  ,  σ    2      fragments  B  normal-,  t  normal-}  normal-,  σ   2     \mathbf{B},\mathbf{t}\},\sigma^{2}     It is also possible to use CPD with non-rigid registration using a parametrization derived using calculus of variation . 40  Sums of Gaussian distributions can be computed in linear time using the fast Gauss transform (FGT). 41 Consequently, the time complexity of CPD is    O   (   M  +  N   )       O    M  N     O(M+N)   , which is asymptotically much faster than    O   (   M  N   )       O    M  N     O(MN)   methods. 42  External links   Reference implementation of thin plate spline robust point matching  Reference implementation of kernel correlation point set registration  Reference implementation of coherent point drift  Reference implementation of ICP variants  Evaluation data sets for 3D rigid registration algorithms   References  "  Category:Computer vision  Category:Robotics  Category:Pattern matching      ↩  ↩  ↩  ↩        ↩      ↩   ↩       ↩  ↩       ↩  ↩  ↩             