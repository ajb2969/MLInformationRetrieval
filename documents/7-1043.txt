   Jacobi method      Jacobi method  table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
   margin: 0; padding: 0; vertical-align: baseline; border: none; }
 <style>
 table.sourceCode { width: 100%; line-height: 100%; }
 td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
 td.sourceCode { padding-left: 5px; }
 code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
 code > span.dt { color: #902000; } /* DataType */
 code > span.dv { color: #40a070; } /* DecVal */
 code > span.bn { color: #40a070; } /* BaseN */
 code > span.fl { color: #40a070; } /* Float */
 code > span.ch { color: #4070a0; } /* Char */
 code > span.st { color: #4070a0; } /* String */
 code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
 code > span.ot { color: #007020; } /* Other */
 code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
 code > span.fu { color: #06287e; } /* Function */
 code > span.er { color: #ff0000; font-weight: bold; } /* Error */
 code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 code > span.cn { color: #880000; } /* Constant */
 code > span.sc { color: #4070a0; } /* SpecialChar */
 code > span.vs { color: #4070a0; } /* VerbatimString */
 code > span.ss { color: #bb6688; } /* SpecialString */
 code > span.im { } /* Import */
 code > span.va { color: #19177c; } /* Variable */
 code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
 code > span.op { color: #666666; } /* Operator */
 code > span.bu { } /* BuiltIn */
 code > span.ex { } /* Extension */
 code > span.pp { color: #bc7a00; } /* Preprocessor */
 code > span.at { color: #7d9029; } /* Attribute */
 code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
 code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
 code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
 code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */     In numerical linear algebra , the Jacobi method (or Jacobi iterative method 1 ) is an algorithm for determining the solutions of a diagonally dominant  system of linear equations . Each diagonal element is solved for, and an approximate value is plugged in. The process is then iterated until it converges. This algorithm is a stripped-down version of the Jacobi transformation method of matrix diagonalization . The method is named after Carl Gustav Jacob Jacobi .  Description  Given a square system of n linear equations:       A  𝐱   =  𝐛        A  𝐱   𝐛    A\mathbf{x}=\mathbf{b}     where:        A  =   [      a  11      a  12     ⋯     a   1  n         a  21      a  22     ⋯     a   2  n        ⋮    ⋮    ⋱    ⋮       a   n  1       a   n  2      ⋯     a   n  n       ]    ,    𝐱  =   [      x  1        x  2       ⋮       x  n      ]    ,   𝐛  =   [      b  1        b  2       ⋮       b  n      ]      .     formulae-sequence    A     subscript  a  11    subscript  a  12   normal-⋯   subscript  a    1  n       subscript  a  21    subscript  a  22   normal-⋯   subscript  a    2  n      normal-⋮  normal-⋮  normal-⋱  normal-⋮     subscript  a    n  1     subscript  a    n  2    normal-⋯   subscript  a    n  n        formulae-sequence    𝐱     subscript  x  1      subscript  x  2     normal-⋮     subscript  x  n        𝐛     subscript  b  1      subscript  b  2     normal-⋮     subscript  b  n         A=\begin{bmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\
 a_{21}&a_{22}&\cdots&a_{2n}\\
 \vdots&\vdots&\ddots&\vdots\\
 a_{n1}&a_{n2}&\cdots&a_{nn}\end{bmatrix},\qquad\mathbf{x}=\begin{bmatrix}x_{1}%
 \\
 x_{2}\\
 \vdots\\
 x_{n}\end{bmatrix},\qquad\mathbf{b}=\begin{bmatrix}b_{1}\\
 b_{2}\\
 \vdots\\
 b_{n}\end{bmatrix}.     Then A can be decomposed into a diagonal component D , and the remainder R :        A  =    D  +  R   where     D  =    [      a  11     0    ⋯    0      0     a  22     ⋯    0      ⋮    ⋮    ⋱    ⋮      0    0    ⋯     a   n  n       ]   and  R   =   [     0     a  12     ⋯     a   1  n         a  21     0    ⋯     a   2  n        ⋮    ⋮    ⋱    ⋮       a   n  1       a   n  2      ⋯    0     ]     .     formulae-sequence    A     D  R   where        D       subscript  a  11   0  normal-⋯  0    0   subscript  a  22   normal-⋯  0    normal-⋮  normal-⋮  normal-⋱  normal-⋮    0  0  normal-⋯   subscript  a    n  n      and  R          0   subscript  a  12   normal-⋯   subscript  a    1  n       subscript  a  21   0  normal-⋯   subscript  a    2  n      normal-⋮  normal-⋮  normal-⋱  normal-⋮     subscript  a    n  1     subscript  a    n  2    normal-⋯  0        A=D+R\qquad\text{where}\qquad D=\begin{bmatrix}a_{11}&0&\cdots&0\\
 0&a_{22}&\cdots&0\\
 \vdots&\vdots&\ddots&\vdots\\
 0&0&\cdots&a_{nn}\end{bmatrix}\text{ and }R=\begin{bmatrix}0&a_{12}&\cdots&a_{%
 1n}\\
 a_{21}&0&\cdots&a_{2n}\\
 \vdots&\vdots&\ddots&\vdots\\
 a_{n1}&a_{n2}&\cdots&0\end{bmatrix}.     The solution is then obtained iteratively via        𝐱   (   k  +  1   )    =    D   -  1     (   𝐛  -   R   𝐱   (  k  )      )     ,       superscript  𝐱    k  1       superscript  D    1      𝐛    R   superscript  𝐱  k        \mathbf{x}^{(k+1)}=D^{-1}(\mathbf{b}-R\mathbf{x}^{(k)}),     where    𝐱   (  k  )      superscript  𝐱  k    \mathbf{x}^{(k)}   is the k th approximation or iteration of   𝐱   𝐱   \mathbf{x}   and    𝐱   (   k  +  1   )      superscript  𝐱    k  1     \mathbf{x}^{(k+1)}   is the next or k + 1 iteration of   𝐱   𝐱   \mathbf{x}   . The element-based formula is thus:         x  i   (   k  +  1   )    =    1   a   i  i      (    b  i   -    ∑   j  ≠  i      a   i  j     x  j   (  k  )       )     ,   i  =   1  ,  2  ,  …  ,  n     .     formulae-sequence     subscript   superscript  x    k  1    i       1   subscript  a    i  i        subscript  b  i     subscript     j  i       subscript  a    i  j     subscript   superscript  x  k   j          i   1  2  normal-…  n      x^{(k+1)}_{i}=\frac{1}{a_{ii}}\left(b_{i}-\sum_{j\neq i}a_{ij}x^{(k)}_{j}%
 \right),\quad i=1,2,\ldots,n.     The computation of x i ( k +1) requires each element in x ( k ) except itself. Unlike the Gauss–Seidel method , we can't overwrite x i ( k ) with x i ( k +1) , as that value will be needed by the rest of the computation. The minimum amount of storage is two vectors of size n .  Algorithm   Choose an initial guess    x   (  0  )      superscript  x  0    x^{(0)}   to the solution      k  =  0      k  0    k=0     while convergence not reached do  for i := 1 step until n do     σ  =  0      σ  0    \sigma=0      for j := 1 step until n do  if j ≠ i then     σ  =   σ  +    a   i  j     x  j   (  k  )          σ    σ     subscript  a    i  j     superscript   subscript  x  j   k       \sigma=\sigma+a_{ij}x_{j}^{(k)}      end if    end (j-loop)       x  i   (   k  +  1   )    =    (    b  i   -  σ   )    a   i  i          superscript   subscript  x  i     k  1         subscript  b  i   σ    subscript  a    i  i       x_{i}^{(k+1)}={{\left({b_{i}-\sigma}\right)}\over{a_{ii}}}       end (i-loop)  check if convergence is reached      k  =   k  +  1       k    k  1     k=k+1       loop (while convergence condition not reached)   Convergence  The standard convergence condition (for any iterative method) is when the spectral radius of the iteration matrix is less than 1:       ρ   (    D   -  1    R   )    <  1.        ρ     superscript  D    1    R    1.    \rho(D^{-1}R)<1.     The method is guaranteed to converge if the matrix A is strictly or irreducibly diagonally dominant . Strict row diagonal dominance means that for each row, the absolute value of the diagonal term is greater than the sum of absolute values of other terms:        |   a   i  i    |   >    ∑   j  ≠  i     |   a   i  j    |     .         subscript  a    i  i       subscript     j  i       subscript  a    i  j        \left|a_{ii}\right|>\sum_{j\neq i}{\left|a_{ij}\right|}.     The Jacobi method sometimes converges even if these conditions are not satisfied.  Example  A linear system of the form     A  x   =  b        A  x   b    Ax=b   with initial estimate    x   (  0  )      superscript  x  0    x^{(0)}   is given by        A  =   [     2    1      5    7     ]    ,    b  =    [     11      13     ]   and      x   (  0  )    =   [     1      1     ]      .     formulae-sequence    A    2  1    5  7      formulae-sequence    b     11    13    and       superscript  x  0     1    1        A=\begin{bmatrix}2&1\\
 5&7\\
 \end{bmatrix},\ b=\begin{bmatrix}11\\
 13\\
 \end{bmatrix}\quad\text{and}\quad x^{(0)}=\begin{bmatrix}1\\
 1\\
 \end{bmatrix}.   We use the equation     x   (   k  +  1   )    =    D   -  1     (   b  -   R   x   (  k  )      )         superscript  x    k  1       superscript  D    1      b    R   superscript  x  k        x^{(k+1)}=D^{-1}(b-Rx^{(k)})   , described above, to estimate   x   x   x   . First, we rewrite the equation in a more convenient form      D   -  1     (   b  -   R   x   (  k  )      )    =    T   x   (  k  )     +  C          superscript  D    1      b    R   superscript  x  k          T   superscript  x  k    C     D^{-1}(b-Rx^{(k)})=Tx^{(k)}+C   , where    T  =   -    D   -  1    R        T       superscript  D    1    R      T=-D^{-1}R   and    C  =    D   -  1    b       C     superscript  D    1    b     C=D^{-1}b   . Note that    R  =   L  +  U       R    L  U     R=L+U   where   L   L   L   and   U   U   U   are the strictly lower and upper parts of   A   A   A   . From the known values         D   -  1    =   [      1  /  2     0      0     1  /  7      ]    ,    L  =    [     0    0      5    0     ]   and     U  =   [     0    1      0    0     ]      .     formulae-sequence     superscript  D    1        1  2   0    0    1  7       formulae-sequence    L     0  0    5  0    and      U    0  1    0  0        D^{-1}=\begin{bmatrix}1/2&0\\
 0&1/7\\
 \end{bmatrix},\ L=\begin{bmatrix}0&0\\
 5&0\\
 \end{bmatrix}\quad\text{and}\quad U=\begin{bmatrix}0&1\\
 0&0\\
 \end{bmatrix}.   we determine    T  =   -    D   -  1     (   L  +  U   )         T       superscript  D    1      L  U       T=-D^{-1}(L+U)   as       T  =    [      1  /  2     0      0     1  /  7      ]    {    [     0    0       -  5     0     ]   +   [     0     -  1       0    0     ]    }    =   [     0     -   1  /  2         -   5  /  7      0     ]    .        T        1  2   0    0    1  7          0  0      5   0      0    1     0  0              0      1  2          5  7    0       T=\begin{bmatrix}1/2&0\\
 0&1/7\\
 \end{bmatrix}\left\{\begin{bmatrix}0&0\\
 -5&0\\
 \end{bmatrix}+\begin{bmatrix}0&-1\\
 0&0\\
 \end{bmatrix}\right\}=\begin{bmatrix}0&-1/2\\
 -5/7&0\\
 \end{bmatrix}.   Further, C is found as       C  =    [      1  /  2     0      0     1  /  7      ]    [     11      13     ]    =   [      11  /  2        13  /  7      ]    .        C        1  2   0    0    1  7       11    13              11  2       13  7        C=\begin{bmatrix}1/2&0\\
 0&1/7\\
 \end{bmatrix}\begin{bmatrix}11\\
 13\\
 \end{bmatrix}=\begin{bmatrix}11/2\\
 13/7\\
 \end{bmatrix}.   With T and C calculated, we estimate   x   x   x   as     x   (  1  )    =    T   x   (  0  )     +  C        superscript  x  1       T   superscript  x  0    C     x^{(1)}=Tx^{(0)}+C   :        x   (  1  )    =     [     0     -   1  /  2         -   5  /  7      0     ]    [     1      1     ]    +   [      11  /  2        13  /  7      ]    =   [     5.0       8  /  7      ]   ≈   [     5      1.143     ]    .         superscript  x  1         0      1  2          5  7    0      1    1         11  2       13  7             5.0      8  7            5    1.143       x^{(1)}=\begin{bmatrix}0&-1/2\\
 -5/7&0\\
 \end{bmatrix}\begin{bmatrix}1\\
 1\\
 \end{bmatrix}+\begin{bmatrix}11/2\\
 13/7\\
 \end{bmatrix}=\begin{bmatrix}5.0\\
 8/7\\
 \end{bmatrix}\approx\begin{bmatrix}5\\
 1.143\\
 \end{bmatrix}.   The next iteration yields        x   (  2  )    =     [     0     -   1  /  2         -   5  /  7      0     ]    [     5.0       8  /  7      ]    +   [      11  /  2        13  /  7      ]    =   [      69  /  14        -   12  /  7       ]   ≈   [     4.929       -  1.714      ]    .         superscript  x  2         0      1  2          5  7    0      5.0      8  7          11  2       13  7               69  14         12  7             4.929      1.714        x^{(2)}=\begin{bmatrix}0&-1/2\\
 -5/7&0\\
 \end{bmatrix}\par
 \begin{bmatrix}5.0\\
 8/7\\
 \end{bmatrix}+\begin{bmatrix}11/2\\
 13/7\\
 \end{bmatrix}=\begin{bmatrix}69/14\\
 -12/7\\
 \end{bmatrix}\approx\begin{bmatrix}4.929\\
 -1.714\\
 \end{bmatrix}.   This process is repeated until convergence (i.e., until    ∥    A   x   (  n  )     -  b   ∥     norm      A   superscript  x  n    b     \|Ax^{(n)}-b\|   is small). The solution after 25 iterations is       x  =   [     7.111       -  3.222      ]    .      x    7.111      3.222       x=\begin{bmatrix}7.111\\
 -3.222\end{bmatrix}.     Another example  Suppose we are given the following linear system:        10   x  1    -   x  2    +   2   x  3            10   subscript  x  1     subscript  x  2      2   subscript  x  3      \displaystyle 10x_{1}-x_{2}+2x_{3}     Suppose we choose (0, 0, 0, 0) as the initial approximation, then the first approximate solution is given by      x  1     subscript  x  1    \displaystyle x_{1}     Using the approximations obtained, the iterative procedure is repeated until the desired accuracy has been reached. The following are the approximated solutions after five iterations.          x  1     subscript  x  1    x_{1}          x  2     subscript  x  2    x_{2}          x  3     subscript  x  3    x_{3}          x  4     subscript  x  4    x_{4}             0.6   0.6   0.6         2.27272   2.27272   2.27272          -  1.1      1.1    -1.1         1.875   1.875   1.875           1.04727   1.04727   1.04727         1.7159   1.7159   1.7159          -  0.80522      0.80522    -0.80522         0.88522   0.88522   0.88522           0.93263   0.93263   0.93263         2.05330   2.05330   2.05330          -  1.0493      1.0493    -1.0493         1.13088   1.13088   1.13088           1.01519   1.01519   1.01519         1.95369   1.95369   1.95369          -  0.9681      0.9681    -0.9681         0.97384   0.97384   0.97384           0.98899   0.98899   0.98899         2.0114   2.0114   2.0114          -  1.0102      1.0102    -1.0102         1.02135   1.02135   1.02135        The exact solution of the system is (1, 2, −1, 1).  An example using Python 3 and Numpy  The following numerical procedure simply iterates to produce the solution vector.  import numpy as np
 
 ITERATION_LIMIT =  1000  # initialize the matrix A = np.array([[ 10 ., - 1 ., 2 ., 0 .],
               [ - 1 ., 11 ., - 1 ., 3 .],
               [ 2 ., - 1 ., 10 ., - 1 .],
               [ 0.0 , 3 ., - 1 ., 8 .]]) # initialize the RHS vector b = np.array([ 6 ., 25 ., - 11 ., 15 .]) # prints the system  print ( "System:" ) for i in  range (A.shape[ 0 ]):
     row = [ "{}*x{}" . format (A[i, j], j +  1 ) for j in  range (A.shape[ 1 ])] print ( " + " .join(row), "=" , b[i]) print ()
 
 x = np.zeros_like(b) for it_count in  range (ITERATION_LIMIT): print ( "Current solution:" , x)
     x_new = np.zeros_like(x) for i in  range (A.shape[ 0 ]):
         s1 = np.dot(A[i, :i], x[:i])
         s2 = np.dot(A[i, i +  1 :], x[i +  1 :])
         x_new[i] = (b[i] - s1 - s2) / A[i, i] if np.allclose(x, x_new, atol = 1e-10 ): break x = x_new print ( "Solution:" ) print (x)
 error = np.dot(A, x) - b print ( "Error:" ) print (error)  Produces the output:  System: 10.0 * x1 +  - 1.0 * x2 +  2.0 * x3 +  0.0 * x4 =  6.0  - 1.0 * x1 +  11.0 * x2 +  - 1.0 * x3 +  3.0 * x4 =  25.0  2.0 * x1 +  - 1.0 * x2 +  10.0 * x3 +  - 1.0 * x4 =  - 11.0  0.0 * x1 +  3.0 * x2 +  - 1.0 * x3 +  8.0 * x4 =  15.0 Current solution: [ 0 . 0 . 0 . 0 .]
 Current solution: [ 0.6  2.27272727  - 1.1  1.875 ]
 Current solution: [ 1.04727273  1.71590909  - 0.80522727  0.88522727 ]
 Current solution: [ 0.93263636  2.05330579  - 1.04934091  1.13088068 ]
 Current solution: [ 1.01519876  1.95369576  - 0.96810863  0.97384272 ]
 Current solution: [ 0.9889913  2.01141473  - 1.0102859  1.02135051 ]
 Current solution: [ 1.00319865  1.99224126  - 0.99452174  0.99443374 ]
 Current solution: [ 0.99812847  2.00230688  - 1.00197223  1.00359431 ]
 Current solution: [ 1.00062513  1.9986703  - 0.99903558  0.99888839 ]
 Current solution: [ 0.99967415  2.00044767  - 1.00036916  1.00061919 ]
 Current solution: [ 1.0001186  1.99976795  - 0.99982814  0.99978598 ]
 Current solution: [ 0.99994242  2.00008477  - 1.00006833  1.0001085 ]
 Current solution: [ 1.00002214  1.99995896  - 0.99996916  0.99995967 ]
 Current solution: [ 0.99998973  2.00001582  - 1.00001257  1.00001924 ]
 Current solution: [ 1.00000409  1.99999268  - 0.99999444  0.9999925 ]
 Current solution: [ 0.99999816  2.00000292  - 1.0000023  1.00000344 ]
 Current solution: [ 1.00000075  1.99999868  - 0.99999899  0.99999862 ]
 Current solution: [ 0.99999967  2.00000054  - 1.00000042  1.00000062 ]
 Current solution: [ 1.00000014  1.99999976  - 0.99999982  0.99999975 ]
 Current solution: [ 0.99999994  2.0000001  - 1.00000008  1.00000011 ]
 Current solution: [ 1.00000003  1.99999996  - 0.99999997  0.99999995 ]
 Current solution: [ 0.99999999  2.00000002  - 1.00000001  1.00000002 ]
 Current solution: [ 1 . 1.99999999  - 0.99999999  0.99999999 ]
 Current solution: [ 1 . 2 . - 1 . 1 .]
 Solution:
 [ 1 . 2 . - 1 . 1 .]
 Error:
 [ - 2.81440107e-08  5.15706873e-08  - 3.63466359e-08  4.17092547e-08 ]  Weighted Jacobi method  The weighted Jacobi iteration uses a parameter   ω   ω   \omega   to compute the iteration as       𝐱   (   k  +  1   )    =    ω   D   -  1     (   𝐛  -   R   𝐱   (  k  )      )    +    (   1  -  ω   )    𝐱   (  k  )           superscript  𝐱    k  1        ω   superscript  D    1      𝐛    R   superscript  𝐱  k          1  ω    superscript  𝐱  k       \mathbf{x}^{(k+1)}=\omega D^{-1}(\mathbf{b}-R\mathbf{x}^{(k)})+\left(1-\omega%
 \right)\mathbf{x}^{(k)}   with    ω  =   2  /  3       ω    2  3     \omega=2/3   being the usual choice. 2  Recent developments  In 2014, a refinement of the algorithm, called scheduled relaxation Jacobi method , was published. 3 4 The new method employs a schedule of over- and under-relaxations and provides a two-hundred fold performance improvement for solving elliptic equations discretized on large two- and three-dimensional Cartesian grids.  See also   Gauss–Seidel method  Successive over-relaxation  Iterative method. Linear systems  Gaussian Belief Propagation  Matrix splitting   References  External links      Jacobi Method from www.math-linux.com  Module for Jacobi and Gauss–Seidel Iteration  Numerical matrix inversion   "  Category:Numerical linear algebra  Category:Articles with example pseudocode  Category:Relaxation (iterative methods)  Category:Articles with example Python code     ↩  ↩   ↩    