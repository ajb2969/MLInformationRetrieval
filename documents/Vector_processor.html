<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1801">Vector processor</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Vector processor</h1>
<style>
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
<style>
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</body></html>
<body>
<hr/>

<p>In <a class="uri" href="computing" title="wikilink">computing</a>, a <strong>vector processor</strong> or <strong>array processor</strong> is a <a href="central_processing_unit" title="wikilink">central processing unit</a> (CPU) that implements an <a href="instruction_set" title="wikilink">instruction set</a> containing <a href="Instruction_(computer_science)" title="wikilink">instructions</a> that operate on <a href="Array_data_structure" title="wikilink">one-dimensional arrays</a> of data called <em>vectors</em>. (Compare <a href="scalar_processor" title="wikilink">scalar processors</a>, whose instructions operate on single data items.) Vector processors can greatly improve performance on certain workloads, notably numerical simulation and similar tasks. Vector machines appeared in the early 1970s and dominated <a class="uri" href="supercomputer" title="wikilink">supercomputer</a> design through the 1970s into the 1990s, notably the various <a class="uri" href="Cray" title="wikilink">Cray</a> platforms. The rapid fall in the <a href="price-to-performance_ratio" title="wikilink">price-to-performance ratio</a> of conventional <a class="uri" href="microprocessor" title="wikilink">microprocessor</a> designs led to the vector supercomputer's demise in the later 1990s.</p>

<p>most commodity CPUs implement architectures that feature instructions for a form of vector processing on multiple (vectorized) data sets, typically known as <a class="uri" href="SIMD" title="wikilink">SIMD</a> (<strong>S</strong>ingle <strong>I</strong>nstruction, <strong>M</strong>ultiple <strong>D</strong>ata). Common examples include <a href="Visual_Instruction_Set" title="wikilink">VIS</a>, <a href="MMX_(instruction_set)" title="wikilink">MMX</a>, <a href="Streaming_SIMD_Extensions" title="wikilink">SSE</a>, <a class="uri" href="AltiVec" title="wikilink">AltiVec</a> and <a href="Advanced_Vector_Extensions" title="wikilink">AVX</a>. Vector processing techniques also operate in <a href="video_game_console" title="wikilink"> video-game console</a> hardware and in <a href="graphics_accelerator" title="wikilink">graphics accelerators</a>. In 2000, <a class="uri" href="IBM" title="wikilink">IBM</a>, <a class="uri" href="Toshiba" title="wikilink">Toshiba</a> and <a class="uri" href="Sony" title="wikilink">Sony</a> collaborated to create the <a href="Cell_(microprocessor)" title="wikilink">Cell processor</a>, consisting of one scalar processor and eight vector processors, which found use in the Sony <a href="PlayStation_3" title="wikilink">PlayStation 3</a> among other applications.</p>

<p>Other CPU designs may include some multiple instructions for vector processing on multiple (vectorised) data sets, typically known as <a class="uri" href="MIMD" title="wikilink">MIMD</a> (<strong>M</strong>ultiple <strong>I</strong>nstruction, <strong>M</strong>ultiple <strong>D</strong>ata) and realized with <a class="uri" href="VLIW" title="wikilink">VLIW</a>. Such designs are usually dedicated to a particular application and not commonly marketed for general-purpose computing. The <a class="uri" href="Fujitsu" title="wikilink">Fujitsu</a> <a class="uri" href="FR-V" title="wikilink">FR-V</a> VLIW/<em>vector processor</em> combines both technologies.</p>
<h2 id="history">History</h2>
<h3 id="early-work">Early work</h3>

<p>Vector processing development began in the early 1960s at <a href="Westinghouse_Electric_Corporation" title="wikilink">Westinghouse</a> in their <strong>Solomon</strong> project. Solomon's goal was to dramatically increase math performance by using a large number of simple <a href="coprocessor" title="wikilink">math co-processors</a> under the control of a single master <a href="Central_processing_unit" title="wikilink">CPU</a>. The CPU fed a single common instruction to all of the <a href="arithmetic_logic_unit" title="wikilink">arithmetic logic units</a> (ALUs), one per "cycle", but with a different data point for each one to work on. This allowed the Solomon machine to apply a single <a class="uri" href="algorithm" title="wikilink">algorithm</a> to a large <a href="data_set" title="wikilink">data set</a>, fed in the form of an array.</p>

<p>In 1962, Westinghouse cancelled the project, but the effort was restarted at the <a href="University_of_Illinois_at_Urbana-Champaign" title="wikilink">University of Illinois</a> as the <a href="ILLIAC_IV" title="wikilink">ILLIAC IV</a>. Their version of the design originally called for a 1 <a class="uri" href="GFLOPS" title="wikilink">GFLOPS</a> machine with 256 ALUs, but, when it was finally delivered in 1972, it had only 64 ALUs and could reach only 100 to 150 MFLOPS. Nevertheless it showed that the basic concept was sound, and, when used on data-intensive applications, such as <a href="computational_fluid_dynamics" title="wikilink">computational fluid dynamics</a>, the "failed" ILLIAC was the fastest machine in the world. The ILLIAC approach of using separate ALUs for each data element is not common to later designs, and is often referred to under a separate category, <a href="massively_parallel" title="wikilink">massively parallel</a> computing.</p>

<p>A <a href="computer_for_operations_with_functions" title="wikilink">computer for operations with functions</a> was presented and developed by Kartsev in 1967.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h3 id="supercomputers">Supercomputers</h3>

<p>The first <em>successful</em> implementation of vector processing appears to be the <a href="Control_Data_Corporation" title="wikilink">Control Data Corporation</a> <a href="CDC_STAR-100" title="wikilink">STAR-100</a> and the <a href="Texas_Instruments" title="wikilink">Texas Instruments</a> <a href="Advanced_Scientific_Computer" title="wikilink">Advanced Scientific Computer</a> (ASC). The basic ASC (i.e., "one pipe") ALU used a pipeline architecture that supported both scalar and vector computations, with peak performance reaching approximately 20 MFLOPS, readily achieved when processing long vectors. Expanded ALU configurations supported "two pipes" or "four pipes" with a corresponding 2X or 4X performance gain. Memory bandwidth was sufficient to support these expanded modes. The STAR was otherwise slower than CDC's own supercomputers like the <a href="CDC_7600" title="wikilink">CDC 7600</a>, but at data related tasks they could keep up while being much smaller and less expensive. However the machine also took considerable time decoding the vector instructions and getting ready to run the process, so it required very specific data sets to work on before it actually sped anything up.</p>

<p>The vector technique was first fully exploited in 1976 by the famous <a class="uri" href="Cray-1" title="wikilink">Cray-1</a>. Instead of leaving the data in memory like the STAR and ASC, the Cray design had eight "vector registers," which held sixty-four 64-bit words each. The vector instructions were applied between registers, which is much faster than talking to main memory. The Cray design used pipeline parallelism to implement vector instructions rather than multiple ALUs. In addition the design had completely separate pipelines for different instructions, for example, addition/subtraction was implemented in different hardware than multiplication. This allowed a batch of vector instructions themselves to be pipelined, a technique they called <em>vector chaining</em>. The Cray-1 normally had a performance of about 80 MFLOPS, but with up to three chains running it could peak at 240 MFLOPS – a respectable number even as of 2002.</p>
<figure><b>(Figure)</b>
<figcaption><a href="Cray_J90" title="wikilink">Cray J90</a> processor module with four scalar/vector processors</figcaption>
</figure>

<p>Other examples followed. <a href="Control_Data_Corporation" title="wikilink">Control Data Corporation</a> tried to re-enter the high-end market again with its <a class="uri" href="ETA-10" title="wikilink">ETA-10</a> machine, but it sold poorly and they took that as an opportunity to leave the supercomputing field entirely. In the early and mid-1980s Japanese companies (<a class="uri" href="Fujitsu" title="wikilink">Fujitsu</a>, <a href="Hitachi,_Ltd." title="wikilink">Hitachi</a> and <a href="Nippon_Electric_Corporation" title="wikilink">Nippon Electric Corporation</a> (NEC) introduced register-based vector machines similar to the Cray-1, typically being slightly faster and much smaller. <a class="uri" href="Oregon" title="wikilink">Oregon</a>-based <a href="Floating_Point_Systems" title="wikilink">Floating Point Systems</a> (FPS) built add-on array processors for <a href="minicomputer" title="wikilink">minicomputers</a>, later building their own <a href="minisupercomputer" title="wikilink">minisupercomputers</a>. However Cray continued to be the performance leader, continually beating the competition with a series of machines that led to the <a class="uri" href="Cray-2" title="wikilink">Cray-2</a>, <a href="Cray_X-MP" title="wikilink">Cray X-MP</a> and <a href="Cray_Y-MP" title="wikilink">Cray Y-MP</a>. Since then, the supercomputer market has focused much more on <a href="massively_parallel" title="wikilink">massively parallel</a> processing rather than better implementations of vector processors. However, recognising the benefits of vector processing IBM developed <a href="IBM_ViVA" title="wikilink">Virtual Vector Architecture</a> for use in supercomputers coupling several scalar processors to act as a vector processor.</p>
<h3 id="simd">SIMD</h3>

<p>Vector processing techniques have since been added to almost all modern <a class="uri" href="CPU" title="wikilink">CPU</a> designs, although they are typically referred to as <a class="uri" href="SIMD" title="wikilink">SIMD</a>. In these implementations, the vector unit runs beside the main <a href="scalar_(computing)" title="wikilink">scalar</a> <a class="uri" href="CPU" title="wikilink">CPU</a>, and is fed data from vector instruction aware programs.</p>
<h2 id="description">Description</h2>

<p>In general terms, CPUs are able to manipulate one or two pieces of data at a time. For instance, most CPUs have an instruction that essentially says "add A to B and put the result in C". The data for A, B and C could be—in theory at least—encoded directly into the instruction. However, in efficient implementation things are rarely that simple. The data is rarely sent in raw form, and is instead "pointed to" by passing in an address to a memory location that holds the data. Decoding this address and getting the data out of the memory takes some time, during which the CPU traditionally would sit idle waiting for the requested data to show up. As CPU speeds have increased, this <em><a href="memory_latency" title="wikilink">memory latency</a></em> has historically become a large impediment to performance; see <a href="Random-access_memory#Memory_wall" title="wikilink">Memory wall</a>.</p>

<p>In order to reduce the amount of time consumed by these steps, most modern CPUs use a technique known as <a href="instruction_pipelining" title="wikilink">instruction pipelining</a> in which the instructions pass through several sub-units in turn. The first sub-unit reads the address and decodes it, the next "fetches" the values at those addresses, and the next does the math itself. With pipelining the "trick" is to start decoding the next instruction even before the first has left the CPU, in the fashion of an <a href="assembly_line" title="wikilink">assembly line</a>, so the <a href="address_decoder" title="wikilink">address decoder</a> is constantly in use. Any particular instruction takes the same amount of time to complete, a time known as the <em><a href="Latency_(engineering)" title="wikilink">latency</a></em>, but the CPU can process an entire batch of operations much faster and more efficiently than if it did so one at a time.</p>

<p>Vector processors take this concept one step further. Instead of pipelining just the instructions, they also pipeline the data itself. The processor is fed instructions that say not just to add A to B, but to add all of the numbers "from here to here" to all of the numbers "from there to there". Instead of constantly having to decode instructions and then fetch the data needed to complete them, the processor reads a single instruction from memory, and it is simply implied in the definition of the instruction <em>itself</em> that the instruction will operate again on another item of data, at an address one increment larger than the last. This allows for significant savings in decoding time.</p>

<p>To illustrate what a difference this can make, consider the simple task of adding two groups of 10 numbers together. In a normal programming language one would write a "loop" that picked up each of the pairs of numbers in turn, and then added them. To the CPU, this would look something like this:</p>

<p><code>execute this loop 10 times</code><br/>
<code>  read the next instruction and decode it</code><br/>
<code>  fetch this number</code><br/>
<code>  fetch that number</code><br/>
<code>  add them</code><br/>
<code>  put the result here</code><br/>
<code>end loop</code></p>

<p>But to a vector processor, this task looks considerably different:</p>

<p><code>read instruction and decode it</code><br/>
<code>fetch these 10 numbers</code><br/>
<code>fetch those 10 numbers</code><br/>
<code>add them</code><br/>
<code>put the results here</code></p>

<p>There are several savings inherent in this approach. For one, only two address translations are needed. Depending on the architecture, this can represent a significant savings by itself. Another saving is fetching and decoding the instruction itself, which has to be done only one time instead of ten. The code itself is also smaller, which can lead to more efficient memory use.</p>

<p>But more than that, a vector processor may have multiple <a href="functional_unit" title="wikilink">functional units</a> adding those numbers in parallel. The checking of dependencies between those numbers is not required as a vector instruction specifies multiple independent operations. This simplifies the control logic required, and can improve performance by avoiding stalls.</p>

<p>As mentioned earlier, the Cray implementations took this a step further, allowing several different types of operations to be carried out at the same time. Consider code that adds two numbers and then multiplies by a third; in the Cray, these would all be fetched at once, and both added and multiplied in a single operation. Using the pseudocode above, the Cray did:</p>

<p><code>read instruction and decode it</code><br/>
<code>fetch these 10 numbers</code><br/>
<code>fetch those 10 numbers</code><br/>
<code>fetch another 10 numbers</code><br/>
<code>add and multiply them</code><br/>
<code>put the results here</code></p>

<p>The math operations thus completed far faster overall, the limiting factor being the time required to fetch the data from memory.</p>

<p>Not all problems can be attacked with this sort of solution. Including these types of instructions necessarily adds complexity to the core CPU. That complexity typically makes <em>other</em> instructions run slower—i.e., whenever it is <strong>not</strong> adding up many numbers in a row. The more complex instructions also add to the complexity of the decoders, which might slow down the decoding of the more common instructions such as normal adding.</p>

<p>In fact, vector processors work best only when there are large amounts of data to be worked on. For this reason, these sorts of CPUs were found primarily in <a href="supercomputer" title="wikilink">supercomputers</a>, as the supercomputers themselves were, in general, found in places such as weather prediction centres and physics labs, where huge amounts of data are "crunched".</p>
<h2 id="performance-and-speed-up">Performance and Speed Up</h2>

<p>Let <strong><em>r</em></strong> be the vector speed ratio and <strong><em>f</em></strong> be the vectorization ratio. If the time taken for the vector unit to add an array of 64 numbers is 10 times faster than its equivalent scalar counterpart, r = 10. Also, if the total number of operations in a program is 100, out of which only 10 are scalar (after vectorization), then f = 90, i.e, 90% of the work is done by the vector unit. It follows the achievable speed up of:</p>

<p>

<math display="inline" id="Vector_processor:0">
 <semantics>
  <mrow>
   <mi>r</mi>
   <mo>/</mo>
   <mrow>
    <mo stretchy="false">[</mo>
    <mrow>
     <mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mn>1</mn>
        <mo>-</mo>
        <mi>f</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mo>*</mo>
      <mi>r</mi>
     </mrow>
     <mo>+</mo>
     <mi>f</mi>
    </mrow>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <ci>r</ci>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <apply>
      <plus></plus>
      <apply>
       <times></times>
       <apply>
        <minus></minus>
        <cn type="integer">1</cn>
        <ci>f</ci>
       </apply>
       <ci>r</ci>
      </apply>
      <ci>f</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r/[(1-f)*r+f]
  </annotation>
 </semantics>
</math>

</p>

<p>So, even if the performance of the vector unit is very high (

<math display="inline" id="Vector_processor:1">
 <semantics>
  <mrow>
   <mi>r</mi>
   <mo>=</mo>
   <mrow>
    <mi>i</mi>
    <mi>n</mi>
    <mi>f</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>y</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>r</ci>
    <apply>
     <times></times>
     <ci>i</ci>
     <ci>n</ci>
     <ci>f</ci>
     <ci>i</ci>
     <ci>n</ci>
     <ci>i</ci>
     <ci>t</ci>
     <ci>y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r=infinity
  </annotation>
 </semantics>
</math>

) we get a speedup less than 

<math display="inline" id="Vector_processor:2">
 <semantics>
  <mrow>
   <mn>1</mn>
   <mo>/</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mi>f</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <cn type="integer">1</cn>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <ci>f</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1/(1-f)
  </annotation>
 </semantics>
</math>

, which suggests that the ratio <strong><em>f</em></strong> is crucial to the performance. This ratio depends on the efficiency of the compilation like adjacency of the elements in memory.</p>
<h2 id="real-world-example-vector-instructions-usage-with-the-x86-architecture">Real world example: vector instructions usage with the x86 architecture</h2>

<p>Shown below is an actual <a class="uri" href="x86" title="wikilink">x86</a> architecture example for vector instruction usage with the <a href="Streaming_SIMD_Extensions" title="wikilink">SSE</a> instruction set. The example multiplies two arrays of single precision <a href="floating_point" title="wikilink">floating point</a> numbers. It's written in the C language with inline assembly code parts for compilation with <a href="GNU_Compiler_Collection" title="wikilink">GCC</a> (32bit).</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="co">//SSE simd function for vectorized multiplication of 2 arrays with single-precision floatingpoint numbers</span>
<span class="co">//1st param pointer on source/destination array, 2nd param 2. source array, 3rd param number of floats per array</span>
 <span class="dt">void</span> mul_asm(<span class="dt">float</span>* out, <span class="dt">float</span>* in, <span class="dt">unsigned</span> <span class="dt">int</span> leng)
 {    <span class="dt">unsigned</span> <span class="dt">int</span> count, rest;

      <span class="co">//compute if array is big enough for vector operation</span>
      rest  = (leng*<span class="dv">4</span>)%<span class="dv">16</span>;
      count = (leng*<span class="dv">4</span>)-rest;

     <span class="co">// vectorized part; 4 floats per loop iteration</span>
      <span class="kw">if</span> (count&gt;<span class="dv">0</span>){
      __asm __volatile__  (<span class="st">".intel_syntax noprefix</span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"loop:                 </span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"movups xmm0,[ebx+ecx] ;loads 4 floats in first register (xmm0)</span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"movups xmm1,[eax+ecx] ;loads 4 floats in second register (xmm1)</span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"mulps xmm0,xmm1       ;multiplies both vector registers</span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"movups [eax+ecx],xmm0 ;write back the result to memory</span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"sub ecx,16            ;increase address pointer by 4 floats</span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"jnz loop              </span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">".att_syntax prefix    </span><span class="ch">\n\t</span><span class="st">"</span>
        : : <span class="st">"a"</span> (out), <span class="st">"b"</span> (in), <span class="st">"c"</span>(count), <span class="st">"d"</span>(rest): <span class="st">"xmm0"</span>,<span class="st">"xmm1"</span>);
      }

      <span class="co">// scalar part; 1 float per loop iteration</span>
      <span class="kw">if</span> (rest!=<span class="dv">0</span>)
      {
       __asm __volatile__  (<span class="st">".intel_syntax noprefix</span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"add eax,ecx           </span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"add ebx,ecx           </span><span class="ch">\n\t</span><span class="st">"</span>

      <span class="st">"rest:                 </span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"movss xmm0,[ebx+edx]  ;load 1 float in first register (xmm0)</span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"movss xmm1,[eax+edx]  ;load 1 float in second register (xmm1)</span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"mulss xmm0,xmm1       ;multiplies both scalar parts of registers</span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"movss [eax+edx],xmm0  ;write back the result</span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"sub edx,4             </span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">"jnz rest              </span><span class="ch">\n\t</span><span class="st">"</span>
      <span class="st">".att_syntax prefix    </span><span class="ch">\n\t</span><span class="st">"</span>
        : : <span class="st">"a"</span> (out), <span class="st">"b"</span> (in), <span class="st">"c"</span>(count), <span class="st">"d"</span>(rest): <span class="st">"xmm0"</span>,<span class="st">"xmm1"</span>);
      }
      <span class="kw">return</span>;
 }</code></pre></div>
<h2 id="programming-heterogeneous-computing-architectures">Programming heterogeneous computing architectures</h2>

<p>Various machines were designed to include both traditional processors and vector processors, such as the <a class="uri" href="Fujitsu" title="wikilink">Fujitsu</a> AP1000 and AP3000. Programming such <a href="Heterogeneous_computing" title="wikilink">heterogeneous machines</a> can be difficult since developing programs that make best use of characteristics of different processors increases the programmer's burden. It increases code complexity and decreases portability of the code by requiring hardware specific code to be interleaved throughout application code.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Balancing the application workload across processors can be problematic, especially given that they typically have different performance characteristics. There are different conceptual models to deal with the problem, for example using a coordination language and program building blocks (programming libraries or higher order functions). Each block can have a different native implementation for each processor type. Users simply program using these abstractions and an intelligent compiler chooses the best implementation based on the context.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Stream_processing" title="wikilink">Stream processing</a></li>
<li><a class="uri" href="SIMD" title="wikilink">SIMD</a></li>
<li><a href="Automatic_vectorization" title="wikilink">Automatic vectorization</a></li>
<li><a href="Chaining_(vector_processing)" title="wikilink">Chaining (vector processing)</a></li>
<li><a href="Computer_for_operations_with_functions" title="wikilink">Computer for operations with functions</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://ei.cs.vt.edu/~history/Parallel.html">The History of the Development of Parallel Computing</a> (from 1955 to 1993)</li>
</ul>

<p>"</p>

<p><a href="Category:Parallel_computing" title="wikilink">Category:Parallel computing</a> <a href="Category:Central_processing_unit" title="wikilink">Category:Central processing unit</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
</ol>
</section>
</body>

