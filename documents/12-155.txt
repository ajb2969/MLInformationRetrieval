   Kernel smoother      Kernel smoother   A kernel smoother is a statistical technique for estimating a real valued function     f   (  X  )    (  X  ∈   ℝ  p   )      fragments  f   fragments  normal-(  X  normal-)    fragments  normal-(  X    superscript  ℝ  p   normal-)     f(X)\,\,\left(X\in\mathbb{R}^{p}\right)   by using its noisy observations, when no parametric model for this function is known. The estimated function is smooth, and the level of smoothness is set by a single parameter.  This technique is most appropriate for low-dimensional ( p K_{h_\lambda}(X_0 ,X) be a kernel defined by        K   h  λ     (   X  0   ,  X  )    =   D   (    ∥   X  -   X  0    ∥     h  λ    (   X  0   )     )           subscript  K   subscript  h  λ      subscript  X  0   X      D     norm    X   subscript  X  0        subscript  h  λ    subscript  X  0        K_{h_{\lambda}}(X_{0},X)=D\left(\frac{\left\|X-X_{0}\right\|}{h_{\lambda}(X_{0%
 })}\right)     where:        X  ,   X  0    ∈   ℝ  p        X   subscript  X  0     superscript  ℝ  p     X,X_{0}\in\mathbb{R}^{p}         ∥  ⋅  ∥     fragments  normal-∥  normal-⋅  normal-∥    \left\|\cdot\right\|   is the Euclidean norm       h  λ    (   X  0   )        subscript  h  λ    subscript  X  0     h_{\lambda}(X_{0})   is a parameter (kernel radius)  D ( t ) typically is a positive real valued function, which value is decreasing (or not increasing) for the increasing distance between the X and X 0 .   Popular kernels used for smoothing include   Epanechnikov  Tri-cube  Gaussian   Let      Y  ^    (  X  )    :    ℝ  p   →  ℝ      normal-:     normal-^  Y   X    normal-→   superscript  ℝ  p   ℝ     \hat{Y}(X):\mathbb{R}^{p}\to\mathbb{R}   be a continuous function of X . For each     X  0   ∈   ℝ  p        subscript  X  0    superscript  ℝ  p     X_{0}\in\mathbb{R}^{p}   , the Nadaraya-Watson kernel-weighted average (smooth Y ( X ) estimation) is defined by        Y  ^    (   X  0   )    =     ∑   i  =  1   N     K   h  λ     (   X  0   ,   X  i   )   Y   (   X  i   )       ∑   i  =  1   N     K   h  λ     (   X  0   ,   X  i   )             normal-^  Y    subscript  X  0        superscript   subscript     i  1    N      subscript  K   subscript  h  λ      subscript  X  0    subscript  X  i    Y   subscript  X  i       superscript   subscript     i  1    N      subscript  K   subscript  h  λ      subscript  X  0    subscript  X  i         \hat{Y}(X_{0})=\frac{\sum\limits_{i=1}^{N}{K_{h_{\lambda}}(X_{0},X_{i})Y(X_{i}%
 )}}{\sum\limits_{i=1}^{N}{K_{h_{\lambda}}(X_{0},X_{i})}}     where:   N is the number of observed points  Y ( X i ) are the observations at X i points.   In the following sections, we describe some particular cases of kernel smoothers.  Gaussian Kernel smoother  The Gaussian Kernel is one of the most common kernels. (It's also known as the radial basis function kernel ). The kernel is expressed with the equation below.       K   (   x  *   ,   x  i   )    =   exp   (   -     (    x  *   -   x  i    )   2    2   b  2      )          K    superscript  x     subscript  x  i            superscript     superscript  x     subscript  x  i    2     2   superscript  b  2         K(x^{*},x_{i})=\exp\left(-\frac{(x^{*}-x_{i})^{2}}{2b^{2}}\right)     Here, b is the length scale for the input space.  (Figure)  Gaussian kernel regression.png   Nearest neighbor smoother  The idea of the nearest neighbor smoother is the following. For each point X 0 , take m nearest neighbors and estimate the value of Y ( X 0 ) by averaging the values of these neighbors.  Formally,      h  m    (   X  0   )    =   ∥    X  0   -   X   [  m  ]     ∥          subscript  h  m    subscript  X  0     norm     subscript  X  0    subscript  X   delimited-[]  m        h_{m}(X_{0})=\left\|X_{0}-X_{[m]}\right\|   , where    X   [  m  ]      subscript  X   delimited-[]  m     X_{[m]}   is the m th closest to X 0 neighbor, and       D   (  t  )    =   {      1  /  m       if   |  t  |    ≤  1       0    otherwise            D  t    cases    1  m       if    t    1   0  otherwise     D(t)=\begin{cases}1/m&\text{if }|t|\leq 1\\
 0&\text{otherwise}\end{cases}     Example:  (Figure)  NNSmoother.svg   In this example, X is one-dimensional. For each X 0 , the     Y  ^    (   X  0   )        normal-^  Y    subscript  X  0     \hat{Y}(X_{0})   is an average value of 16 closest to X 0 points (denoted by red). The result is not smooth enough.  Kernel average smoother  The idea of the kernel average smoother is the following. For each data point X 0 , choose a constant distance size λ (kernel radius, or window width for p = 1 dimension), and compute a weighted average for all data points that are closer than   λ   λ   \lambda   to X 0 (the closer to X 0 points get higher weights).  Formally,       h  λ    (   X  0   )    =  λ  =  constant   ,           subscript  h  λ    subscript  X  0    λ       constant     h_{\lambda}(X_{0})=\lambda=\text{constant},   and D ( t ) is one of the popular kernels.  Example:  (Figure)  KernelSmoother.svg   For each X 0 the window width is constant, and the weight of each point in the window is schematically denoted by the yellow figure in the graph. It can be seen that the estimation is smooth, but the boundary points are biased. The reason for that is the non-equal number of points (from the right and from the left to the X 0 ) in the window, when the X 0 is close enough to the boundary.  Local linear regression  In the two previous sections we assumed that the underlying Y(X) function is locally constant, therefore we were able to use the weighted average for the estimation. The idea of local linear regression is to fit locally a straight line (or a hyperplane for higher dimensions), and not the constant (horizontal line). After fitting the line, the estimation     Y  ^    (   X  0   )        normal-^  Y    subscript  X  0     \hat{Y}(X_{0})   is provided by the value of this line at X 0 point. By repeating this procedure for each X 0 , one can get the estimation function     Y  ^    (  X  )        normal-^  Y   X    \hat{Y}(X)   . Like in previous section, the window width is constant       h  λ    (   X  0   )    =  λ  =  constant   .           subscript  h  λ    subscript  X  0    λ       constant     h_{\lambda}(X_{0})=\lambda=\text{constant}.   Formally, the local linear regression is computed by solving a weighted least square problem.  For one dimension ( p = 1):           min    α   (   X  0   )    ,   β   (   X  0   )         ∑   i  =  1   N      K   h  λ     (   X  0   ,   X  i   )     (    Y   (   X  i   )    -   α   (   X  0   )    -   β   (   X  0   )    X  i     )   2           ⇓           Y  ^     (   X  0   )    =    α   (   X  0   )    +   β   (   X  0   )    X  0             missing-subexpression      subscript      α   subscript  X  0      β   subscript  X  0        superscript   subscript     i  1    N      subscript  K   subscript  h  λ      subscript  X  0    subscript  X  i     superscript      Y   subscript  X  i      α   subscript  X  0      β   subscript  X  0    subscript  X  i     2         missing-subexpression   normal-⇓     missing-subexpression        normal-^  Y    subscript  X  0        α   subscript  X  0      β   subscript  X  0    subscript  X  0         \begin{aligned}&\displaystyle\min_{\alpha(X_{0}),\beta(X_{0})}\sum\limits_{i=1%
 }^{N}{K_{h_{\lambda}}(X_{0},X_{i})\left(Y(X_{i})-\alpha(X_{0})-\beta(X_{0})X_{%
 i}\right)^{2}}\\
 &\displaystyle\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,%
 \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\Downarrow\\
 &\displaystyle\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\hat{Y}(X_{0})%
 =\alpha(X_{0})+\beta(X_{0})X_{0}\\
 \end{aligned}     The closed form solution is given by:        Y  ^    (   X  0   )    =    (  1  ,   X  0   )     (    B  T   W   (   X  0   )   B   )    -  1     B  T   W   (   X  0   )   y          normal-^  Y    subscript  X  0       1   subscript  X  0     superscript     superscript  B  T   W   subscript  X  0   B     1     superscript  B  T   W   subscript  X  0   y     \hat{Y}(X_{0})=\left(1,X_{0}\right)\left(B^{T}W(X_{0})B\right)^{-1}B^{T}W(X_{0%
 })y     where:       y  =    (   Y   (   X  1   )    ,  …  ,   Y   (   X  N   )    )   T       y   superscript     Y   subscript  X  1    normal-…    Y   subscript  X  N     T     y=\left(Y(X_{1}),\dots,Y(X_{N})\right)^{T}         W   (   X  0   )   =  diag    (   K   h  λ     (   X  0   ,   X  i   )   )    N  ×  N       fragments  W   fragments  normal-(   subscript  X  0   normal-)    diag   subscript   fragments  normal-(   subscript  K   subscript  h  λ     fragments  normal-(   subscript  X  0   normal-,   subscript  X  i   normal-)   normal-)     N  N      W(X_{0})=\operatorname{diag}\left(K_{h_{\lambda}}(X_{0},X_{i})\right)_{N\times
 N}     B^{T}=\left( \begin{matrix}   1 & 1 & \dots & 1  \\  X_{1} & X_{2} & \dots & X_{N}  \\  \end{matrix} \right)  Example:  (Figure)  Localregressionsmoother.svg   The resulting function is smooth, and the problem with the biased boundary points is solved.  Local linear regression can be applied to any-dimensional space, though the question of what is a local neighborhood becomes more complicated. It is common to use k nearest training points to a test point to fit the local linear regression. This can lead to high variance of the fitted function. To bound the variance, the set of training points should contain the test point in their convex hull (see Gupta et al. reference).  Local polynomial regression  Instead of fitting locally linear functions, one can fit polynomial functions.  For p=1, one should minimize:        min      α   (   X  0   )    ,    β  j    (   X  0   )    ,  j   =  1   ,   …  ,  d        ∑   i  =  1   N     K   h  λ     (   X  0   ,   X  i   )     (    Y   (   X  i   )    -   α   (   X  0   )    -    ∑   j  =  1   d     β  j    (   X  0   )    X  i  j      )   2           formulae-sequence       α   subscript  X  0       subscript  β  j    subscript  X  0    j   1    normal-…  d        superscript   subscript     i  1    N      subscript  K   subscript  h  λ      subscript  X  0    subscript  X  i     superscript      Y   subscript  X  i      α   subscript  X  0      superscript   subscript     j  1    d      subscript  β  j    subscript  X  0    superscript   subscript  X  i   j      2       \underset{\alpha(X_{0}),\beta_{j}(X_{0}),j=1,...,d}{\mathop{\min}}\,\sum%
 \limits_{i=1}^{N}{K_{h_{\lambda}}(X_{0},X_{i})\left(Y(X_{i})-\alpha(X_{0})-%
 \sum\limits_{j=1}^{d}{\beta_{j}(X_{0})X_{i}^{j}}\right)^{2}}     with      Y  ^    (   X  0   )    =    α   (   X  0   )    +    ∑   j  =  1   d     β  j    (   X  0   )    X  0  j             normal-^  Y    subscript  X  0        α   subscript  X  0      superscript   subscript     j  1    d      subscript  β  j    subscript  X  0    superscript   subscript  X  0   j        \hat{Y}(X_{0})=\alpha(X_{0})+\sum\limits_{j=1}^{d}{\beta_{j}(X_{0})X_{0}^{j}}     In general case (p>1), one should minimize:            β  ^    (   X  0   )    =      arg  min    β   (   X  0   )         ∑   i  =  1   N      K   h  λ     (   X  0   ,   X  i   )     (    Y   (   X  i   )    -   b    (   X  i   )   T   β   (   X  0   )     )   2              b   (  X  )    =   (      1  ,       X  1   ,       X  2   ,  …       X  1  2   ,       X  2  2   ,  …       X  1     X  2    …      )            Y  ^    (   X  0   )    =   b    (   X  0   )   T    β  ^    (   X  0   )            missing-subexpression        normal-^  β    subscript  X  0         β   subscript  X  0           superscript   subscript     i  1    N      subscript  K   subscript  h  λ      subscript  X  0    subscript  X  i     superscript      Y   subscript  X  i      b   superscript   subscript  X  i   T   β   subscript  X  0     2          missing-subexpression       b  X     1   subscript  X  1     subscript  X  2   normal-…    superscript   subscript  X  1   2     superscript   subscript  X  2   2   normal-…      subscript  X  1    subscript  X  2   normal-…         missing-subexpression        normal-^  Y    subscript  X  0      b   superscript   subscript  X  0   T    normal-^  β    subscript  X  0        \begin{aligned}&\displaystyle\hat{\beta}(X_{0})=\underset{\beta(X_{0})}{%
 \mathop{\arg\min}}\,\sum\limits_{i=1}^{N}{K_{h_{\lambda}}(X_{0},X_{i})\left(Y(%
 X_{i})-b(X_{i})^{T}\beta(X_{0})\right)}^{2}\\
 &\displaystyle b(X)=\left(\begin{matrix}1,&X_{1},&X_{2},...&X_{1}^{2},&X_{2}^{%
 2},...&X_{1}X_{2}\,\,\,...\\
 \end{matrix}\right)\\
 &\displaystyle\hat{Y}(X_{0})=b(X_{0})^{T}\hat{\beta}(X_{0})\\
 \end{aligned}     See also   Savitzky–Golay filter  Kernel (statistics)  Kernel methods  Kernel density estimation  Local regression   References   Li, Q. and J.S. Racine. Nonparametric Econometrics: Theory and Practice . Princeton University Press, 2007, ISBN 0-691-12161-3.  T. Hastie, R. Tibshirani and J. Friedman, The Elements of Statistical Learning , Chapter 6, Springer, 2001. ISBN 0-387-95284-5 ( companion book site ).  M. Gupta, E. Garcia and E. Chin, "Adaptive Local Linear Regression with Application to Printer Color Management," IEEE Trans. Image Processing 2008.   "  Category:Non-parametric statistics   