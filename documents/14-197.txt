   Distance correlation      Distance correlation   In statistics and in probability theory , distance correlation is a measure of statistical dependence between two random variables or two random vectors of arbitrary, not necessarily equal dimension . An important property is that this measure of dependence is zero if and only if the random variables are statistically independent . This measure is derived from a number of other quantities that are used in its specification, specifically: distance variance , distance standard deviation and distance covariance . These take the same roles as the ordinary moments with corresponding names in the specification of the Pearson product-moment correlation coefficient .  These distance-based measures can be put into an indirect relationship to the ordinary moments by an alternative formulation (described below) using ideas related to Brownian motion , and this has led to the use of names such as Brownian covariance and Brownian distance covariance .  (Figure)  Several sets of ( x , y ) points, with the Distance correlation coefficient of x and y for each set. Compare to the graph on correlation   Background  The classical measure of dependence, the Pearson correlation coefficient , 1 is mainly sensitive to a linear relationship between two variables. Distance correlation was introduced in 2005 by Gabor J Szekely in several lectures to address this deficiency of Pearson’s correlation , namely that it can easily be zero for dependent variables. Correlation = 0 (uncorrelatedness) does not imply independence while distance correlation = 0 does imply independence. The first results on distance correlation were published in 2007 and 2009. 2 3 It was proved that distance covariance is the same as the Brownian covariance. 4 These measures are examples of energy distances .  Definitions  Distance covariance  Let us start with the definition of the sample distance covariance . Let ( X k , Y k ), k = 1, 2, ..., n be a statistical sample from a pair of real valued or vector valued random variables ( X , Y ). First, compute all pairwise distances      a   j  ,  k      subscript  a   j  k     \displaystyle a_{j,k}     where || ⋅ || denotes Euclidean norm . That is, compute the n by n distance matrices ( a j , k ) and ( b j , k ). Then take all doubly centered distances         A   j  ,  k    :=     a   j  ,  k    -    a  ¯    j  .    -    a  ¯    .  k     +    a  ¯    .  .      ,    B   j  ,  k    :=     b   j  ,  k    -    b  ¯    j  .    -    b  ¯    .  k     +    b  ¯    .  .       ,     formulae-sequence   assign   subscript  A   j  k         subscript  a   j  k     subscript   normal-¯  a    fragments  j  normal-.     subscript   normal-¯  a    fragments  normal-.  k      subscript   normal-¯  a    fragments  normal-.  normal-.       assign   subscript  B   j  k         subscript  b   j  k     subscript   normal-¯  b    fragments  j  normal-.     subscript   normal-¯  b    fragments  normal-.  k      subscript   normal-¯  b    fragments  normal-.  normal-.        A_{j,k}:=a_{j,k}-\overline{a}_{j.}-\overline{a}_{.k}+\overline{a}_{..},\qquad B%
 _{j,k}:=b_{j,k}-\overline{b}_{j.}-\overline{b}_{.k}+\overline{b}_{..},     where     a  ¯    j  .      subscript   normal-¯  a    fragments  j  normal-.     \textstyle\overline{a}_{j.}   is the   j   j   j   -th row mean,     a  ¯    .  k      subscript   normal-¯  a    fragments  normal-.  k     \textstyle\overline{a}_{.k}   is the   k   k   k   -th column mean, and     a  ¯    .  .      subscript   normal-¯  a    fragments  normal-.  normal-.     \textstyle\overline{a}_{..}   is the grand mean of the distance matrix of the X sample. The notation is similar for the b values. (In the matrices of centered distances ( A j , k ) and ( B j , k ) all rows and all columns sum to zero.) The squared sample distance covariance is simply the arithmetic average of the products A j , ''k B'' j , k :         dCov  n  2    (  X  ,  Y  )    :=    1   n  2      ∑    j  ,  k   =  1   n      A   j  ,  k      B   j  ,  k        .     assign    subscript   superscript  dCov  2   n   X  Y       1   superscript  n  2      superscript   subscript      j  k   1    n      subscript  A   j  k     subscript  B   j  k         \operatorname{dCov}^{2}_{n}(X,Y):=\frac{1}{n^{2}}\sum_{j,k=1}^{n}A_{j,k}\,B_{j%
 ,k}.   The statistic T n = n dCov 2 n ( X , Y ) determines a consistent multivariate test of independence of random vectors in arbitrary dimensions. For an implementation see dcov.test function in the energy package for R . 5  The population value of distance covariance can be defined along the same lines. Let X be a random variable that takes values in a p -dimensional Euclidean space with probability distribution   μ   μ   μ   and let Y be a random variable that takes values in a q -dimensional Euclidean space with probability distribution   ν   ν   ν   , and suppose that X and Y have finite expectations. Write          a  μ    (  x  )    :=   E   [   ∥   X  -  x   ∥   ]     ,     D   (  μ  )    :=   E   [    a  μ    (  X  )    ]     ,     d  μ    (  x  ,   x  ′   )    :=     ∥   x  -   x  ′    ∥   -    a  μ    (  x  )    -    a  μ    (   x  ′   )     +   D   (  μ  )        .     formulae-sequence   assign     subscript  a  μ   x    normal-E   norm    X  x       formulae-sequence   assign    D  μ    normal-E     subscript  a  μ   X      assign     subscript  d  μ    x   superscript  x  normal-′          norm    x   superscript  x  normal-′        subscript  a  μ   x      subscript  a  μ    superscript  x  normal-′       D  μ        a_{\mu}(x):=\operatorname{E}[\|X-x\|],\quad D(\mu):=\operatorname{E}[a_{\mu}(X%
 )],\quad d_{\mu}(x,x^{\prime}):=\|x-x^{\prime}\|-a_{\mu}(x)-a_{\mu}(x^{\prime}%
 )+D(\mu).     Finally, define the population value of squared distance covariance of X and Y as         dCov  2    (  X  ,  Y  )    :=   E   [    d  μ    (  X  ,   X  ′   )    d  ν    (  Y  ,   Y  ′   )    ]     .     assign    superscript  dCov  2   X  Y    normal-E     subscript  d  μ    X   superscript  X  normal-′     subscript  d  ν    Y   superscript  Y  normal-′        \operatorname{dCov}^{2}(X,Y):=\operatorname{E}\big[d_{\mu}(X,X^{\prime})d_{\nu%
 }(Y,Y^{\prime})\big].     One can show that this is equivalent to the following definition:       dCov  2    (  X  ,  Y  )       superscript  dCov  2   X  Y    \displaystyle\operatorname{dCov}^{2}(X,Y)     where E denotes expected value, and     (  X  ,  Y  )   ,     X  Y    \textstyle(X,Y),        (   X  ′   ,   Y  ′   )   ,      superscript  X  normal-′    superscript  Y  normal-′     \textstyle(X^{\prime},Y^{\prime}),   and    (   X  ′′   ,   Y  ′′   )      superscript  X  ′′    superscript  Y  ′′     \textstyle(X^{\prime\prime},Y^{\prime\prime})   are independent and identically distributed. Distance covariance can be expressed in terms of Pearson’s covariance, cov , as follows:         dCov  2    (  X  ,  Y  )    =    cov   (   ∥   X  -   X  ′    ∥   ,   ∥   Y  -   Y  ′    ∥   )    -   2   cov   (   ∥   X  -   X  ′    ∥   ,   ∥   Y  -   Y  ′′    ∥   )       .        superscript  dCov  2   X  Y      cov   norm    X   superscript  X  normal-′      norm    Y   superscript  Y  normal-′        2   cov   norm    X   superscript  X  normal-′      norm    Y   superscript  Y  ′′          \operatorname{dCov}^{2}(X,Y)=\operatorname{cov}(\|X-X^{\prime}\|,\|Y-Y^{\prime%
 }\|)-2\operatorname{cov}(\|X-X^{\prime}\|,\|Y-Y^{\prime\prime}\|).     This identity shows that the distance covariance is not the same as the covariance of distances, cov(|| X -''X' ||, || Y - Y' '' ||). This can be zero even if X and Y are not independent.  Alternately, the squared distance covariance can be defined as the weighted norm of the distance between the joint characteristic function of the random variables and the product of their marginal characteristic functions: 6       dCov  2    (  X  ,  Y  )    =    1    c  p    c  q       ∫   ℝ   p  +  q         |     ϕ   X  ,  Y     (  s  ,  t  )    -    ϕ  X    (  s  )    ϕ  Y    (  t  )     |   2      |  s  |   p   1  +  p      |  t  |   q   1  +  q      d   t   d  s           superscript  dCov  2   X  Y       1     subscript  c  p    subscript  c  q       subscript    superscript  ℝ    p  q          superscript         subscript  ϕ   X  Y     s  t       subscript  ϕ  X   s   subscript  ϕ  Y   t     2      superscript   subscript    s   p     1  p     superscript   subscript    t   q     1  q      d  t  d  s       \operatorname{dCov}^{2}(X,Y)=\frac{1}{c_{p}c_{q}}\int_{\mathbb{R}^{p+q}}\frac{%
 \left|\phi_{X,Y}(s,t)-\phi_{X}(s)\phi_{Y}(t)\right|^{2}}{|s|_{p}^{1+p}|t|_{q}^%
 {1+q}}dt\,ds     where ϕ X , Y ( s , t ), and are the characteristic functions of X , and Y , respectively, p , q denote the Euclidean dimension of X and Y , and thus of s and t , and c p , c q are constants. The weight function     (    c  p    c  q     |  s  |   p   1  +  p      |  t  |   q   1  +  q     )    -  1      superscript     subscript  c  p    subscript  c  q    superscript   subscript    s   p     1  p     superscript   subscript    t   q     1  q       1     ({c_{p}c_{q}}{|s|_{p}^{1+p}|t|_{q}^{1+q}})^{-1}   is chosen to produce a scale equivariant and rotation invariant measure that doesn't go to zero for dependent variables. 7 8 One interpretation 9 of the characteristic function definition is that the variables e isX and e itY are cyclic representations of X and Y with different periods given by s and t , and the expression in the numerator of the characteristic function definition of distance covariance is simply the classical covariance of e isX and e itY . The characteristic function definition clearly shows that dCov 2 ( X , Y ) = 0 if and only if X and Y are independent.  Distance variance  The distance variance is a special case of distance covariance when the two variables are identical. The population value of distance variance is the square root of         dVar  2    (  X  )    :=     E   [    ∥   X  -   X  ′    ∥   2   ]    +    E  2    [   ∥   X  -   X  ′    ∥   ]     -   2   E   [    ∥   X  -   X  ′    ∥    ∥   X  -   X  ′′    ∥    ]       ,     assign    superscript  dVar  2   X        normal-E   superscript   norm    X   superscript  X  normal-′     2      superscript  normal-E  2    norm    X   superscript  X  normal-′         2   normal-E     norm    X   superscript  X  normal-′      norm    X   superscript  X  ′′           \operatorname{dVar}^{2}(X):=\operatorname{E}[\|X-X^{\prime}\|^{2}]+%
 \operatorname{E}^{2}[\|X-X^{\prime}\|]-2\operatorname{E}[\|X-X^{\prime}\|\,\|X%
 -X^{\prime\prime}\|],   where   E   normal-E   \operatorname{E}   denotes the expected value,    X  ′     superscript  X  normal-′    X^{\prime}   is an independent and identically distributed copy of   X   X   X   and    X  ′′     superscript  X  ′′    X^{\prime\prime}   is independent of   X   X   X   and    X  ′     superscript  X  normal-′    X^{\prime}   and has the same distribution as   X   X   X   and    X  ′     superscript  X  normal-′    X^{\prime}   .  The sample distance variance is the square root of         dVar  n  2    (  X  )    :=    dCov  n  2    (  X  ,  X  )    =     1   n  2       ∑   k  ,  ℓ     A   k  ,  ℓ   2      ,       assign    subscript   superscript  dVar  2   n   X     subscript   superscript  dCov  2   n   X  X            1   superscript  n  2      subscript    k  normal-ℓ     superscript   subscript  A   k  normal-ℓ    2        \operatorname{dVar}^{2}_{n}(X):=\operatorname{dCov}^{2}_{n}(X,X)=\tfrac{1}{n^{%
 2}}\sum_{k,\ell}A_{k,\ell}^{2},   which is a relative of Corrado Gini ’s mean difference introduced in 1912 (but Gini did not work with centered distances).  Distance standard deviation  The distance standard deviation is the square root of the distance variance .  Distance correlation  The distance correlation  10 11 of two random variables is obtained by dividing their distance covariance by the product of their distance standard deviations . The distance correlation is        dCor   (  X  ,  Y  )    =    dCov   (  X  ,  Y  )       dVar   (  X  )     dVar   (  Y  )        ,       dCor  X  Y      dCov  X  Y        dVar  X    dVar  Y        \operatorname{dCor}(X,Y)=\frac{\operatorname{dCov}(X,Y)}{\sqrt{\operatorname{%
 dVar}(X)\,\operatorname{dVar}(Y)}},   and the sample distance correlation is defined by substituting the sample distance covariance and distance variances for the population coefficients above.  For easy computation of sample distance correlation see the dcor function in the energy package for R . 12  Properties  Distance correlation  (i)    0  ≤    dCor  n    (  X  ,  Y  )    ≤  1        0    subscript  dCor  n   X  Y        1     0\leq\operatorname{dCor}_{n}(X,Y)\leq 1   and    0  ≤   dCor   (  X  ,  Y  )    ≤  1        0   dCor  X  Y        1     0\leq\operatorname{dCor}(X,Y)\leq 1   .  (ii)     dCor   (  X  ,  Y  )    =  0       dCor  X  Y   0    \operatorname{dCor}(X,Y)=0   if and only if   X   X   X   and   Y   Y   Y   are independent.  (iii)      dCor  n    (  X  ,  Y  )    =  1        subscript  dCor  n   X  Y   1    \operatorname{dCor}_{n}(X,Y)=1   implies that dimensions of the linear subspaces spanned by   X   X   X   and   Y   Y   Y   samples respectively are almost surely equal and if we assume that these subspaces are equal, then in this subspace    Y  =   A  +    b   𝐂  X        Y    A    b  𝐂  X      Y=A+b\,\mathbf{C}X   for some vector   A   A   A   , scalar   b   b   b   , and orthonormal matrix    𝐂   𝐂   \mathbf{C}   .  Distance covariance  (i)     dCov   (  X  ,  Y  )    ≥  0       dCov  X  Y   0    \operatorname{dCov}(X,Y)\geq 0   and      dCov  n    (  X  ,  Y  )    ≥  0        subscript  dCov  n   X  Y   0    \operatorname{dCov}_{n}(X,Y)\geq 0   .  (ii)      dCov  2    (    a  1   +     b  1      𝐂  1    X    ,    a  2   +     b  2      𝐂  2    Y    )    =    |     b  1     b  2    |     dCov  2    (  X  ,  Y  )           superscript  dCov  2      subscript  a  1      subscript  b  1    subscript  𝐂  1   X       subscript  a  2      subscript  b  2    subscript  𝐂  2   Y            subscript  b  1    subscript  b  2       superscript  dCov  2   X  Y      \operatorname{dCov}^{2}(a_{1}+b_{1}\,\mathbf{C}_{1}\,X,a_{2}+b_{2}\,\mathbf{C}%
 _{2}\,Y)=|b_{1}\,b_{2}|\operatorname{dCov}^{2}(X,Y)   for all constant vectors     a  1   ,   a  2       subscript  a  1    subscript  a  2     a_{1},a_{2}   , scalars     b  1   ,   b  2       subscript  b  1    subscript  b  2     b_{1},b_{2}   , and orthonormal matrices     𝐂  1   ,   𝐂  2       subscript  𝐂  1    subscript  𝐂  2     \mathbf{C}_{1},\mathbf{C}_{2}   .  (iii) If the random vectors    (   X  1   ,   Y  1   )      subscript  X  1    subscript  Y  1     (X_{1},Y_{1})   and    (   X  2   ,   Y  2   )      subscript  X  2    subscript  Y  2     (X_{2},Y_{2})   are independent then        dCov   (    X  1   +   X  2    ,    Y  1   +   Y  2    )    ≤    dCov   (   X  1   ,   Y  1   )    +   dCov   (   X  2   ,   Y  2   )      .       dCov     subscript  X  1    subscript  X  2       subscript  Y  1    subscript  Y  2        dCov   subscript  X  1    subscript  Y  1     dCov   subscript  X  2    subscript  Y  2       \operatorname{dCov}(X_{1}+X_{2},Y_{1}+Y_{2})\leq\operatorname{dCov}(X_{1},Y_{1%
 })+\operatorname{dCov}(X_{2},Y_{2}).   Equality holds if and only if    X  1     subscript  X  1    X_{1}   and    Y  1     subscript  Y  1    Y_{1}   are both constants, or    X  2     subscript  X  2    X_{2}   and    Y  2     subscript  Y  2    Y_{2}   are both constants, or     X  1   ,   X  2   ,   Y  1   ,   Y  2       subscript  X  1    subscript  X  2    subscript  Y  1    subscript  Y  2     X_{1},X_{2},Y_{1},Y_{2}   are mutually independent.  (iv)     dCov   (  X  ,  Y  )    =  0       dCov  X  Y   0    \operatorname{dCov}(X,Y)=0   if and only if   X   X   X   and   Y   Y   Y   are independent.  This last property is the most important effect of working with centered distances.  The statistic     dCov  n  2    (  X  ,  Y  )       subscript   superscript  dCov  2   n   X  Y    \operatorname{dCov}^{2}_{n}(X,Y)   is a biased estimator of     dCov  2    (  X  ,  Y  )       superscript  dCov  2   X  Y    \operatorname{dCov}^{2}(X,Y)   . Under independence of X and Y 13        E   [    dCov  n  2    (  X  ,  Y  )    ]    =     n  -  1    n  2     {     (   n  -  2   )     dCov  2    (  X  ,  Y  )     +    E   [   ∥   X  -   X  ′    ∥   ]     E   [   ∥   Y  -   Y  ′    ∥   ]      }    =     n  -  1    n  2     E   [   ∥   X  -   X  ′    ∥   ]     E   [   ∥   Y  -   Y  ′    ∥   ]      .         normal-E    subscript   superscript  dCov  2   n   X  Y          n  1    superscript  n  2           n  2     superscript  dCov  2   X  Y       normal-E   norm    X   superscript  X  normal-′       normal-E   norm    Y   superscript  Y  normal-′                     n  1    superscript  n  2     normal-E   norm    X   superscript  X  normal-′       normal-E   norm    Y   superscript  Y  normal-′          \operatorname{E}[\operatorname{dCov}^{2}_{n}(X,Y)]=\frac{n-1}{n^{2}}\left\{(n-%
 2)\operatorname{dCov}^{2}(X,Y)+\operatorname{E}[\|X-X^{\prime}\|]\,%
 \operatorname{E}[\|Y-Y^{\prime}\|]\right\}=\frac{n-1}{n^{2}}\operatorname{E}[%
 \|X-X^{\prime}\|]\,\operatorname{E}[\|Y-Y^{\prime}\|].     An unbiased estimator of     dCov  2    (  X  ,  Y  )       superscript  dCov  2   X  Y    \operatorname{dCov}^{2}(X,Y)   is given by Székely and Rizzo. 14  Distance variance  (i)     dVar   (  X  )    =  0       dVar  X   0    \operatorname{dVar}(X)=0   if and only if    X  =   E   [  X  ]        X   normal-E  X     X=\operatorname{E}[X]   almost surely.  (ii)      dVar  n    (  X  )    =  0        subscript  dVar  n   X   0    \operatorname{dVar}_{n}(X)=0   if and only if every sample observation is identical.  (iii)     dVar   (   A  +    b    𝐂   X    )    =    |  b  |    dVar   (  X  )          dVar    A    b  𝐂  X         b    dVar  X      \operatorname{dVar}(A+b\,\mathbf{C}\,X)=|b|\operatorname{dVar}(X)   for all constant vectors   A   A   A   , scalars   b   b   b   , and orthonormal matrices   𝐂   𝐂   \mathbf{C}   .  (iv) If   X   X   X   and   Y   Y   Y   are independent then     dVar   (   X  +  Y   )    ≤    dVar   (  X  )    +   dVar   (  Y  )          dVar    X  Y       dVar  X    dVar  Y      \operatorname{dVar}(X+Y)\leq\operatorname{dVar}(X)+\operatorname{dVar}(Y)   .  Equality holds in (iv) if and only if one of the random variables   X   X   X   or   Y   Y   Y   is a constant.  Generalization  Distance covariance can be generalized to include powers of Euclidean distance. Define       dCov  2    (  X  ,  Y  ;  α  )       superscript  dCov  2   X  Y  α    \displaystyle\operatorname{dCov}^{2}(X,Y;\alpha)     Then for every    0  <  α  <  2        0  α       2     0<\alpha<2   ,   X   X   X   and   Y   Y   Y   are independent if and only if      dCov  2    (  X  ,  Y  ;  α  )    =  0        superscript  dCov  2   X  Y  α   0    \operatorname{dCov}^{2}(X,Y;\alpha)=0   . It is important to note that this characterization does not hold for exponent    α  =  2      α  2    \alpha=2   ; in this case for bivariate    (  X  ,  Y  )     X  Y    (X,Y)   ,    dCor   (  X  ,  Y  ;   α  =  2   )      dCor  X  Y    α  2     \operatorname{dCor}(X,Y;\alpha=2)   is a deterministic function of the Pearson correlation. 15 If    a   k  ,  ℓ      subscript  a   k  normal-ℓ     a_{k,\ell}   and    b   k  ,  ℓ      subscript  b   k  normal-ℓ     b_{k,\ell}   are   α   α   \alpha   powers of the corresponding distances,    0  <  α  ≤  2        0  α       2     0<\alpha\leq 2   , then   α   α   \alpha   sample distance covariance can be defined as the nonnegative number for which         dCov  n  2    (  X  ,  Y  ;  α  )    :=    1   n  2      ∑   k  ,  ℓ       A   k  ,  ℓ      B   k  ,  ℓ        .     assign    subscript   superscript  dCov  2   n   X  Y  α       1   superscript  n  2      subscript    k  normal-ℓ       subscript  A   k  normal-ℓ     subscript  B   k  normal-ℓ         \operatorname{dCov}^{2}_{n}(X,Y;\alpha):=\frac{1}{n^{2}}\sum_{k,\ell}A_{k,\ell%
 }\,B_{k,\ell}.     One can extend   dCov   dCov   \operatorname{dCov}   to metric-space -valued random variables    X   X   X   and   Y   Y   Y   : If   X   X   X   has law   μ   μ   \mu   in a metric space with metric   d   d   d   , then define      a  μ    (  x  )    :=   E   [   d   (  X  ,  x  )    ]       assign     subscript  a  μ   x    normal-E    d   X  x       a_{\mu}(x):=\operatorname{E}[d(X,x)]   ,     D   (  μ  )    :=   E   [    a  μ    (  X  )    ]       assign    D  μ    normal-E     subscript  a  μ   X      D(\mu):=\operatorname{E}[a_{\mu}(X)]   , and (provided    a  μ     subscript  a  μ    a_{\mu}   is finite, i.e.,   X   X   X   has finite first moment),      d  μ    (  x  ,   x  ′   )    :=     d   (  x  ,   x  ′   )    -    a  μ    (  x  )    -    a  μ    (   x  ′   )     +   D   (  μ  )        assign     subscript  d  μ    x   superscript  x  normal-′           d   x   superscript  x  normal-′        subscript  a  μ   x      subscript  a  μ    superscript  x  normal-′       D  μ      d_{\mu}(x,x^{\prime}):=d(x,x^{\prime})-a_{\mu}(x)-a_{\mu}(x^{\prime})+D(\mu)   . Then if   Y   Y   Y   has law   ν   ν   \nu   (in a possibly different metric space with finite first moment), define         dCov  2    (  X  ,  Y  )    :=   E   [    d  μ    (  X  ,   X  ′   )    d  ν    (  Y  ,   Y  ′   )    ]     .     assign    superscript  dCov  2   X  Y    normal-E     subscript  d  μ    X   superscript  X  normal-′     subscript  d  ν    Y   superscript  Y  normal-′        \operatorname{dCov}^{2}(X,Y):=\operatorname{E}\big[d_{\mu}(X,X^{\prime})d_{\nu%
 }(Y,Y^{\prime})\big].   This is non-negative for all such    X  ,  Y     X  Y    X,Y   iff both metric spaces have negative type. 16 Here, a metric space    (  M  ,  d  )     M  d    (M,d)   has negative type if    (  M  ,   d   1  /  2    )     M   superscript  d    1  2      (M,d^{1/2})   is isometric to a subset of a Hilbert space . 17 If both metric spaces have strong negative type, then      dCov  2    (  X  ,  Y  )    =  0        superscript  dCov  2   X  Y   0    \operatorname{dCov}^{2}(X,Y)=0   iff    X  ,  Y     X  Y    X,Y   are independent. 18  Alternative definition of distance covariance  The original distance covariance has been defined as the square root of     dCov  2    (  X  ,  Y  )       superscript  dCov  2   X  Y    \operatorname{dCov}^{2}(X,Y)   , rather than the squared coefficient itself.    dCov   (  X  ,  Y  )      dCov  X  Y    \operatorname{dCov}(X,Y)   has the property that it is the energy distance between the joint distribution of    X  ,  Y     normal-X  Y    \operatorname{X},Y   and the product of its marginals. Under this definition, however, the distance variance, rather than the distance standard deviation, is measured in the same units as the   X   normal-X   \operatorname{X}   distances.  Alternately, one could define distance covariance to be the square of the energy distance:      dCov  2    (  X  ,  Y  )    .      superscript  dCov  2   X  Y    \operatorname{dCov}^{2}(X,Y).   In this case, the distance standard deviation of   X   X   X   is measured in the same units as   X   X   X   distance, and there exists an unbiased estimator for the population distance covariance. 19  Under these alternate definitions, the distance correlation is also defined as the square     dCor  2    (  X  ,  Y  )       superscript  dCor  2   X  Y    \operatorname{dCor}^{2}(X,Y)   , rather than the square root.  Alternative formulation: Brownian covariance  Brownian covariance is motivated by generalization of the notion of covariance to stochastic processes. The square of the covariance of random variables X and Y can be written in the following form:      cov    (  X  ,  Y  )   2   =  E   [   (  X  -  E   (  X  )   )    (   X    ′    -  E   (   X    ′    )   )    (  Y  -  E   (  Y  )   )    (   Y    ′    -  E   (   Y    ′    )   )   ]      fragments  cov   superscript   fragments  normal-(  X  normal-,  Y  normal-)   2    normal-E   fragments  normal-[   fragments  normal-(  X   normal-E   fragments  normal-(  X  normal-)   normal-)    fragments  normal-(   superscript  X   normal-′     normal-E   fragments  normal-(   superscript  X   normal-′    normal-)   normal-)    fragments  normal-(  Y   normal-E   fragments  normal-(  Y  normal-)   normal-)    fragments  normal-(   superscript  Y   normal-′     normal-E   fragments  normal-(   superscript  Y   normal-′    normal-)   normal-)   normal-]     \operatorname{cov}(X,Y)^{2}=\operatorname{E}\left[\big(X-\operatorname{E}(X)%
 \big)\big(X^{\mathrm{{}^{\prime}}}-\operatorname{E}(X^{\mathrm{{}^{\prime}}})%
 \big)\big(Y-\operatorname{E}(Y)\big)\big(Y^{\mathrm{{}^{\prime}}}-%
 \operatorname{E}(Y^{\mathrm{{}^{\prime}}})\big)\right]     where E denotes the expected value and the prime denotes independent and identically distributed copies. We need the following generalization of this formula. If U(s), V(t) are arbitrary random processes defined for all real s and t then define the U-centered version of X by       X  U   :=    U   (  X  )    -    E  X    [   U   (  X  )    ∣   {   U   (  t  )    }   ]        assign   subscript  X  U       U  X     subscript  normal-E  X     U  X      U  t        X_{U}:=U(X)-\operatorname{E}_{X}\left[U(X)\mid\left\{U(t)\right\}\right]     whenever the subtracted conditional expected value exists and denote by Y V the V-centered version of Y. 20 21 22 The (U,V) covariance of (X,Y) is defined as the nonnegative number whose square is        cov   U  ,  V   2    (  X  ,  Y  )    :=   E   [    X  U    X  U    ′     Y  V    Y  V    ′     ]       assign    superscript   subscript  cov   U  V    2   X  Y    normal-E     subscript  X  U    superscript   subscript  X  U    normal-′     subscript  Y  V    superscript   subscript  Y  V    normal-′        \operatorname{cov}_{U,V}^{2}(X,Y):=\operatorname{E}\left[X_{U}X_{U}^{\mathrm{{%
 }^{\prime}}}Y_{V}Y_{V}^{\mathrm{{}^{\prime}}}\right]     whenever the right-hand side is nonnegative and finite. The most important example is when U and V are two-sided independent Brownian motions / Wiener processes with expectation zero and covariance |s| + |t| - |s-t| = 2 min(s,t). (This is twice the covariance of the standard Wiener process; here the factor 2 simplifies the computations.) In this case the (U,V) covariance is called Brownian covariance and is denoted by        cov  W    (  X  ,  Y  )    .      subscript  cov  W   X  Y    \operatorname{cov}_{W}(X,Y).     There is a surprising coincidence: The Brownian covariance is the same as the distance covariance:         cov  W    (  X  ,  Y  )    =   dCov   (  X  ,  Y  )     ,        subscript  cov  normal-W   X  Y    dCov  X  Y     \operatorname{cov}_{\mathrm{W}}(X,Y)=\operatorname{dCov}(X,Y),   and thus Brownian correlation is the same as distance correlation.  On the other hand, if we replace the Brownian motion with the deterministic identity function id then Cov id (X,Y) is simply the absolute value of the classical Pearson covariance ,         cov  id    (  X  ,  Y  )    =   |   cov   (  X  ,  Y  )    |    .        subscript  cov  id   X  Y      cov  X  Y      \operatorname{cov}_{\mathrm{id}}(X,Y)=\left|\operatorname{cov}(X,Y)\right|.     See also   RV coefficient  For a related third-order statistic, see Distance skewness .    Notes  References   Bickel, P.J. and Xu, Y. (2009) "Discussion of: Brownian distance covariance", Annals of Applied Statistics , 3 (4), 1266–1269. Free access to article  Gini, C. (1912). Variabilità e Mutabilità. Bologna: Tipografia di Paolo Cuppini.  Pearson, K. (1895). "Note on regression and inheritance in the case of two parents", Proceedings of the Royal Society , 58, 240–242  Pearson, K. (1920). "Notes on the history of correlation", Biometrika , 13, 25–45.  Székely, G. J. Rizzo, M. L. and Bakirov, N. K. (2007). "Measuring and testing independence by correlation of distances", The Annals of Statistics , 35/6, 2769–2794. Reprint  Székely, G. J. and Rizzo, M. L. (2009). "Brownian distance covariance", Annals of Applied Statistics , 3/4, 1233–1303. Reprint  Kosorok, M. R. (2009) "Discussion of: Brownian Distance Covariance", Annals of Applied Statistics , 3/4, 1270–1278. Free access to article  Székely, G.J. and Rizzo, M.L. (2014) Partial distance correlation with methods for dissimilarities, The Annals of Statistics, 42/6, 2382-2412.   External links   E-statistics (energy statistics)   "  Category:Statistical dependence  Category:Statistical distance measures  Category:Theory of probability distributions  Category:Multivariate statistics  Category:Covariance and correlation     Pearson (1895) ↩  Székely, Rizzo and Bakirov (2007) ↩  Székely & Rizzo (2009) ↩   energy package for R ↩  Székely & Rizzo (2009) Theorem 7, (3.7), p. 1249. ↩   ↩  ↩     Székely and Rizzo (2009), Rejoinder ↩  Székely & Rizzo (2014) ↩  Székely & Rizzo (2007) Theorem 7, p. 2785. ↩  Lyons, R. (2011) "Distance covariance in metric spaces". ↩  Klebanov, L. B. (2005) N-distances and their Applications , Karolinum Press, Charles University, Prague. ↩   Székely & Rizzo (2014) ↩   Bickel & Xu (2009) ↩  Kosorok (2009) ↩     