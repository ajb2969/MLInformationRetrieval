   V-statistic      V-statistic   V-statistics are a class of statistics named for Richard von Mises who developed their asymptotic distribution theory in a fundamental paper in 1947. 1 V-statistics are closely related to U-statistics 2 3 (U for “ unbiased ”) introduced by Wassily Hoeffding in 1948. 4 A V-statistic is a statistical function (of a sample) defined by a particular statistical functional of a probability distribution.  Statistical functions  Statistics that can be represented as functionals    T   (   F  n   )       T   subscript  F  n     T(F_{n})   of the empirical distribution function     (   F  n   )     subscript  F  n    (F_{n})   are called statistical functions . 5  Differentiability of the functional T plays a key role in the von Mises approach; thus von Mises considers differentiable statistical functionals . 6  Examples of statistical functions   The k -th central moment is the functional      T   (  F  )    =   ∫      (   x  -  μ   )   k    d  F   (  x  )           T  F        superscript    x  μ   k   d  F  x      T(F)=\int(x-\mu)^{k}\,dF(x)   , where    μ  =   E   [  X  ]        μ    E   delimited-[]  X      \mu=E[X]   is the expected value of X . The associated statistical function is the sample k -th central moment,       T  n   =   m  k   =   T   (   F  n   )    =    1  n     ∑   i  =  1   n     (    x  i   -   x  ¯    )   k      .         subscript  T  n    subscript  m  k          T   subscript  F  n             1  n     superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¯  x    k        T_{n}=m_{k}=T(F_{n})=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\overline{x})^{k}.     The chi-squared goodness-of-fit statistic is a statistical function T ( F n ), corresponding to the statistical functional       T   (  F  )    =    ∑   i  =  1   k      (      ∫   A  i      d  F    -   p  i    )   2    p  i      ,        T  F     superscript   subscript     i  1    k      superscript      subscript    subscript  A  i      d  F     subscript  p  i    2    subscript  p  i       T(F)=\sum_{i=1}^{k}\frac{(\int_{A_{i}}\,dF-p_{i})^{2}}{p_{i}},     where A i are the k cells and p i are the specified probabilities of the cells under the null hypothesis.  The Cramér–von-Mises and Anderson–Darling goodness-of-fit statistics are based on the functional       T   (  F  )    =   ∫      (    F   (  x  )    -    F  0    (  x  )     )   2    w   (  x  ;   F  0   )   d   F  0    (  x  )      ,        T  F        superscript      F  x      subscript  F  0   x    2   w   x   subscript  F  0    d   subscript  F  0   x      T(F)=\int(F(x)-F_{0}(x))^{2}\,w(x;F_{0})\,dF_{0}(x),   where w ( x ; F 0 ) is a specified weight function and F 0 is a specified null distribution. If w is the identity function then T ( F n ) is the well known Cramér–von-Mises goodness-of-fit statistic; if     w   (  x  ;   F  0   )    =    [    F  0    (  x  )    (   1  -    F  0    (  x  )     )    ]    -  1          w   x   subscript  F  0      superscript   delimited-[]     subscript  F  0   x    1     subscript  F  0   x        1      w(x;F_{0})=[F_{0}(x)(1-F_{0}(x))]^{-1}   then T ( F n ) is the Anderson–Darling statistic.   Representation as a V-statistic  Suppose x 1 , ..., x n is a sample. In typical applications the statistical function has a representation as the V-statistic        V   m  n    =    1   n  m      ∑    i  1   =  1   n    ⋯    ∑    i  m   =  1   n    h   (   x   i  1    ,   x   i  2    ,  …  ,   x   i  m    )         ,       subscript  V    m  n        1   superscript  n  m      superscript   subscript      subscript  i  1   1    n     normal-⋯    superscript   subscript      subscript  i  m   1    n     h    subscript  x   subscript  i  1     subscript  x   subscript  i  2    normal-…   subscript  x   subscript  i  m            V_{mn}=\frac{1}{n^{m}}\sum_{i_{1}=1}^{n}\cdots\sum_{i_{m}=1}^{n}h(x_{i_{1}},x_%
 {i_{2}},\dots,x_{i_{m}}),   where h is a symmetric kernel function. Serfling 7 discusses how to find the kernel in practice. V mn is called a V-statistic of degree m .  A symmetric kernel of degree 2 is a function h ( x , y ), such that h ( x , y ) = h ( y , x ) for all x and y in the domain of h. For samples x 1 , ..., x n , the corresponding V-statistic is defined        V   2  ,  n    =    1   n  2      ∑   i  =  1   n     ∑   j  =  1   n    h   (   x  i   ,   x  j   )        .       subscript  V   2  n        1   superscript  n  2      superscript   subscript     i  1    n     superscript   subscript     j  1    n     h    subscript  x  i    subscript  x  j          V_{2,n}=\frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}h(x_{i},x_{j}).     Example of a V-statistic   An example of a degree-2 V-statistic is the second central moment  m 2 . If h ( x , y ) = ( x − y ) 2 /2, the corresponding V-statistic is        V   2  ,  n    =    1   n  2      ∑   i  =  1   n     ∑   j  =  1   n     1  2     (    x  i   -   x  j    )   2       =    1  n     ∑   i  =  1   n     (    x  i   -   x  ¯    )   2      ,         subscript  V   2  n        1   superscript  n  2      superscript   subscript     i  1    n     superscript   subscript     j  1    n       1  2    superscript     subscript  x  i    subscript  x  j    2                1  n     superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¯  x    2        V_{2,n}=\frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{1}{2}(x_{i}-x_{j})^{2%
 }=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2},   which is the maximum likelihood estimator of variance . With the same kernel, the corresponding U-statistic is the (unbiased) sample variance:       s  2   =     (     n      2     )    -  1      ∑   i  <  j      1  2     (    x  i   -   x  j    )   2      =    1   n  -  1      ∑   i  =  1   n     (    x  i   -   x  ¯    )   2            superscript  s  2      superscript   binomial  n  2     1      subscript     i  j        1  2    superscript     subscript  x  i    subscript  x  j    2               1    n  1      superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¯  x    2        s^{2}={n\choose 2}^{-1}\sum_{i   .   Asymptotic distribution  In examples 1–3, the asymptotic distribution of the statistic is different: in (1) it is normal , in (2) it is chi-squared , and in (3) it is a weighted sum of chi-squared variables.  Von Mises' approach is a unifying theory that covers all of the cases above. 8 Informally, the type of asymptotic distribution of a statistical function depends on the order of "degeneracy," which is determined by which term is the first non-vanishing term in the Taylor expansion of the functional T . In case it is the linear term, the limit distribution is normal; otherwise higher order types of distributions arise (under suitable conditions such that a central limit theorem holds).  There are a hierarchy of cases parallel to asymptotic theory of U-statistics . 9 Let A ( m ) be the property defined by:   A ( m ):    Var( h ( X 1 , ..., X k )) = 0 for k 1, ..., X k )) > 0 for k = m ;  n m /2 R mn tends to zero (in probability). ( R mn is the remainder term in the Taylor series for T .)   Case m = 1 (Non-degenerate kernel):  If A (1) is true, the statistic is a sample mean and the Central Limit Theorem implies that T(F n ) is asymptotically normal .  In the variance example (4), m 2 is asymptotically normal with mean    σ  2     superscript  σ  2    \sigma^{2}   and variance     (    μ  4   -   σ  4    )   /  n         subscript  μ  4    superscript  σ  4    n    (\mu_{4}-\sigma^{4})/n   , where     μ  4   =   E    (   X  -   E   (  X  )     )   4         subscript  μ  4     E   superscript    X    E  X    4      \mu_{4}=E(X-E(X))^{4}   .  Case m = 2 (Degenerate kernel):  Suppose A (2) is true, and       E   [    h  2    (   X  1   ,   X  2   )    ]    <  ∞   ,    E   |   h   (   X  1   ,   X  1   )    |    <  ∞    ,     formulae-sequence      E   delimited-[]     superscript  h  2     subscript  X  1    subscript  X  2             E      h    subscript  X  1    subscript  X  1           E[h^{2}(X_{1},X_{2})]<\infty,\,E|h(X_{1},X_{1})|<\infty,   and     E   [   h   (  x  ,   X  1   )    ]    ≡  0        E   delimited-[]    h   x   subscript  X  1       0    E[h(x,X_{1})]\equiv 0   . Then nV 2,n converges in distribution to a weighted sum of independent chi-squared variables:        n   V   2  ,  n      ⟶  d     ∑   k  =  1   ∞     λ  k    Z  k  2      ,      superscript  normal-⟶  d     n   subscript  V   2  n       superscript   subscript     k  1         subscript  λ  k    subscript   superscript  Z  2   k       nV_{2,n}{\stackrel{d}{\longrightarrow}}\sum_{k=1}^{\infty}\lambda_{k}Z^{2}_{k},     where    Z  k     subscript  Z  k    Z_{k}   are independent standard normal variables and    λ  k     subscript  λ  k    \lambda_{k}   are constants that depend on the distribution F and the functional T . In this case the asymptotic distribution is called a quadratic form of centered Gaussian random variables . The statistic V 2, n is called a degenerate kernel V-statistic . The V-statistic associated with the Cramer–von Mises functional 10 (Example 3) is an example of a degenerate kernel V-statistic. 11  See also   U-statistic  Asymptotic distribution  Asymptotic theory (statistics)   Notes  References            "  Category:Estimation theory  Category:Asymptotic statistical theory     ↩  ↩  ↩  ↩  von Mises (1947), p. 309; Serfling (1980), p. 210. ↩   Serfling (1980, Section 6.5) ↩   Serfling (1980, Ch. 5–6); Lee (1990, Ch. 3) ↩   See Lee (1990, p. 160) for the kernel function. ↩     