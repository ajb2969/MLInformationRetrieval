   Efficient estimator      Efficient estimator   In statistics , an efficient estimator is an estimator that estimates the quantity of interest in some “best possible” manner. The notion of “best possible” relies upon the choice of a particular loss function — the function which quantifies the relative degree of undesirability of estimation errors of different magnitudes. The most common choice of the loss function is quadratic , resulting in the mean squared error criterion of optimality. 1  Finite-sample efficiency  Suppose } is a parametric model and are the data sampled from this model. Let  be an estimator for the parameter θ . If this estimator is unbiased (that is, ), then the Cramér–Rao inequality states the variance of this estimator is bounded from below:        Var   [   T   ]    ≥   ℐ  θ   -  1     ,       Var  T    superscript   subscript  ℐ  θ     1      \operatorname{Var}[\,T\,]\ \geq\ \mathcal{I}_{\theta}^{-1},   where    ℐ  θ     subscript  ℐ  θ    \scriptstyle\mathcal{I}_{\theta}   is the Fisher information matrix of the model at point θ . Generally, the variance measures the degree of dispersion of a random variable around its mean. Thus estimators with small variances are more concentrated, they estimate the parameters more precisely. We say that the estimator is finite-sample efficient estimator (in the class of unbiased estimators) if it reaches the lower bound in the Cramér–Rao inequality above, for all . Efficient estimators are always minimum variance unbiased estimators . However the converse is false: There exist point-estimation problems for which the minimum-variance mean-unbiased estimator is inefficient. 2  Historically, finite-sample efficiency was an early optimality criterion. However this criterion has some limitations:   Finite-sample efficient estimators are extremely rare. In fact, it was proved that efficient estimation is possible only in an exponential family , and only for the natural parameters of that family.  This notion of efficiency is restricted to the class of unbiased estimators. Since there are no good theoretical reasons to require that estimators are unbiased, this restriction is inconvenient. In fact, if we use mean squared error as a selection criterion, many biased estimators will slightly outperform the “best” unbiased ones. For example, in multivariate statistics for dimension three or more, the mean-unbiased estimator, sample mean , is inadmissible : Regardless of the outcome, its performance is worse than for example the James–Stein estimator .  Finite-sample efficiency is based on the variance, as a criterion according to which the estimators are judged. A more general approach is to use loss functions other than quadratic ones, in which case the finite-sample efficiency can no longer be formulated.   Example  Among the models encountered in practice, efficient estimators exist for: the mean μ of the normal distribution (but not the variance σ 2 ), parameter λ of the Poisson distribution , the probability p in the binomial or multinomial distribution .  Consider the model of a normal distribution with unknown mean but known variance:  The data consists of n  iid observations from this model: . We estimate the parameter θ using the sample mean of all observations:        T   (  X  )    =    1  n     ∑   i  =  1   n     x  i       .        T  X       1  n     superscript   subscript     i  1    n    subscript  x  i       T(X)=\frac{1}{n}\sum_{i=1}^{n}x_{i}\ .   This estimator has mean θ and variance of , which is equal to the reciprocal of the Fisher information from the sample. Thus, the sample mean is a finite-sample efficient estimator for the mean of the normal distribution.  Relative efficiency  If    T  1     subscript  T  1    T_{1}   and    T  2     subscript  T  2    T_{2}   are estimators for the parameter   θ   θ   \theta   , then    T  1     subscript  T  1    T_{1}   is said to dominate     T  2     subscript  T  2    T_{2}   if:   its mean squared error (MSE) is smaller for at least some value of   θ   θ   \theta     the MSE does not exceed that of    T  2     subscript  T  2    T_{2}   for any value of θ.   Formally,    T  1     subscript  T  1    T_{1}   dominates    T  2     subscript  T  2    T_{2}   if       E   [    (    T  1   -  θ   )   2   ]    ≤   E   [    (    T  2   -  θ   )   2   ]          normal-E   delimited-[]   superscript     subscript  T  1   θ   2       normal-E   delimited-[]   superscript     subscript  T  2   θ   2       \mathrm{E}\left[(T_{1}-\theta)^{2}\right]\leq\mathrm{E}\left[(T_{2}-\theta)^{2%
 }\right]     holds for all   θ   θ   \theta   , with strict inequality holding somewhere.  The relative efficiency is defined as       e   (   T  1   ,   T  2   )    =    E   [    (    T  2   -  θ   )   2   ]     E   [    (    T  1   -  θ   )   2   ]           e    subscript  T  1    subscript  T  2         normal-E   delimited-[]   superscript     subscript  T  2   θ   2       normal-E   delimited-[]   superscript     subscript  T  1   θ   2        e(T_{1},T_{2})=\frac{\mathrm{E}\left[(T_{2}-\theta)^{2}\right]}{\mathrm{E}%
 \left[(T_{1}-\theta)^{2}\right]}     Although   e   e   e   is in general a function of   θ   θ   \theta   , in many cases the dependence drops out; if this is so,   e   e   e   being greater than one would indicate that    T  1     subscript  T  1    T_{1}   is preferable, whatever the true value of   θ   θ   \theta   .  Asymptotic efficiency  For some estimators , they can attain efficiency asymptotically and are thus called asymptotically efficient estimators. This can be the case for some maximum likelihood estimators or for any estimators that attain equality of the Cramér-Rao bound asymptotically.  See also   Bayes estimator  Consistent estimator  Hodges’ estimator   Notes  References        Further reading      "  Category:Estimation theory  Category:Statistical theory  Category:Statistical terminology     ↩  ↩     