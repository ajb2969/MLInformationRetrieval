   Sliding window based part-of-speech tagging      Sliding window based part-of-speech tagging   Sliding window based part-of-speech tagging is used to part-of-speech tag a text.  A high percentage of words in a natural language are words which out of context can be assigned more than one part of speech. The percentage of these ambiguous words is typically around 30%, although it depends greatly on the language. Solving this problem is very important in many areas of natural language processing . For example in machine translation changing the part-of-speech of a word can dramatically change its translation.  Sliding window based part-of-speech taggers are programs which assign a single part-of-speech to a given lexical form of a word, by looking at a fixed sized "window" of words around the word to be disambiguated.  The two main advantages of this approach are:   It is possible to automatically train the tagger, getting rid of the need of manually tagging a corpus.  The tagger can be implemented as a finite state automaton ( Mealy machine )   Formal definition  Let      Γ  =   {   γ  1   ,   γ  2   ,  …  ,   γ   |  Γ  |    }       normal-Γ    subscript  γ  1    subscript  γ  2   normal-…   subscript  γ    normal-Γ       \Gamma=\{\gamma_{1},\gamma_{2},\ldots,\gamma_{|\Gamma|}\}     be the set of grammatical tags of the application, that is, the set of all possible tags which may be assigned to a word, and let      W  =   {   w  1   ,   w  2   ,  …  }       W     w  1     w  2   normal-…     W=\{w1,w2,\ldots\}     be the vocabulary of the application. Let      T  :   W  →   P   (  Γ  )        normal-:  T   normal-→  W    P  normal-Γ      T:W\rightarrow P(\Gamma)     be a function for morphological analysis which assigns each   w   w   w   its set of possible tags,     T   (  w  )    ⊆  Γ        T  w   normal-Γ    T(w)\subseteq\Gamma   , that can be implemented by a full-form lexicon, or a morphological analyser. Let      Σ  =   {   σ  1   ,   σ  2   ,  …  ,   σ   |  Σ  |    }       normal-Σ    subscript  σ  1    subscript  σ  2   normal-…   subscript  σ    normal-Σ       \Sigma=\{\sigma_{1},\sigma_{2},\ldots,\sigma_{|\Sigma|}\}     be the set of word classes, that in general will be a partition of   W   W   W   with the restriction that for each    σ  ∈  Σ      σ  normal-Σ    \sigma\in\Sigma   all of the words     w    Σ   σ      w  normal-Σ  σ    w\,\Sigma\,\sigma   will receive the same set of tags, that is, all of the words in each word class (   σ   σ   \sigma   ) belong to the same ambiguity class.  Normally,   Σ   normal-Σ   \Sigma   is constructed in a way that for high frequency words, each word class contains a single word, while for low frequency words, each word class corresponds to a single ambiguity class. This allows good performance for high frequency ambiguous words, and doesn't require too many parameters for the tagger.  With these definitions it is possible to state problem in the following way: Given a text     w   [  1  ]   w   [  2  ]   …  w   [  L  ]    ∈   W  *         w   delimited-[]  1   w   delimited-[]  2   normal-…  w   delimited-[]  L     superscript  W      w[1]w[2]\ldots w[L]\in W^{*}   each word    w   [  t  ]       w   delimited-[]  t     w[t]   is assigned a word class     T   (   w   [  t  ]    )    ∈  Σ        T    w   delimited-[]  t     normal-Σ    T(w[t])\in\Sigma   (either by using the lexicon or morphological analyser) in order to get an ambiguously tagged text     σ   [  1  ]   σ   [  2  ]   …  σ   [  L  ]    ∈   W  *         σ   delimited-[]  1   σ   delimited-[]  2   normal-…  σ   delimited-[]  L     superscript  W      \sigma[1]\sigma[2]\ldots\sigma[L]\in W^{*}   . The job of the tagger is to get a tagged text    γ   [  1  ]   γ   [  2  ]   …  γ   [  L  ]       γ   delimited-[]  1   γ   delimited-[]  2   normal-…  γ   delimited-[]  L     \gamma[1]\gamma[2]\ldots\gamma[L]   (with     γ   [  t  ]    ∈   T   (   σ   [  t  ]    )          γ   delimited-[]  t      T    σ   delimited-[]  t       \gamma[t]\in T(\sigma[t])   ) as correct as possible.  A statistical tagger looks for the most probable tag for an ambiguously tagged text    σ   [  1  ]   σ   [  2  ]   …  σ   [  L  ]       σ   delimited-[]  1   σ   delimited-[]  2   normal-…  σ   delimited-[]  L     \sigma[1]\sigma[2]\ldots\sigma[L]   :        γ  *    [  1  ]   …   γ  *    [  L  ]    =       arg   max     γ   [  t  ]    ∈   T   (   σ   [  t  ]    )      p    (   γ   [  1  ]   …  γ   [  L  ]   σ   [  1  ]   …  σ   [  L  ]    )           superscript  γ     delimited-[]  1   normal-…   superscript  γ     delimited-[]  L        subscript    arg  max       γ   delimited-[]  t      T    σ   delimited-[]  t       p     γ   delimited-[]  1   normal-…  γ   delimited-[]  L   σ   delimited-[]  1   normal-…  σ   delimited-[]  L       \gamma^{*}[1]\ldots\gamma^{*}[L]=\operatorname*{arg\,max}\limits_{\gamma[t]\in
 T%
 (\sigma[t])}p(\gamma[1]\ldots\gamma[L]\sigma[1]\ldots\sigma[L])     Using Bayes formula , this is converted into:        γ  *    [  1  ]   …   γ  *    [  L  ]    =       arg   max     γ   [  t  ]    ∈   T   (   σ   [  t  ]    )      p    (   γ   [  1  ]   …  γ   [  L  ]    )   p   (   σ   [  1  ]   …  σ   [  L  ]   γ   [  1  ]   …  γ   [  L  ]    )           superscript  γ     delimited-[]  1   normal-…   superscript  γ     delimited-[]  L        subscript    arg  max       γ   delimited-[]  t      T    σ   delimited-[]  t       p     γ   delimited-[]  1   normal-…  γ   delimited-[]  L    p    σ   delimited-[]  1   normal-…  σ   delimited-[]  L   γ   delimited-[]  1   normal-…  γ   delimited-[]  L       \gamma^{*}[1]\ldots\gamma^{*}[L]=\operatorname*{arg\,max}\limits_{\gamma[t]\in
 T%
 (\sigma[t])}p(\gamma[1]\ldots\gamma[L])p(\sigma[1]\ldots\sigma[L]\gamma[1]%
 \ldots\gamma[L])     where    p   (   γ   [  1  ]   γ   [  2  ]   …  γ   [  L  ]    )       p    γ   delimited-[]  1   γ   delimited-[]  2   normal-…  γ   delimited-[]  L      p(\gamma[1]\gamma[2]\ldots\gamma[L])   is the probability that a particular tag (syntactic probability) and    p   (   σ   [  1  ]   …  σ   [  L  ]   γ   [  1  ]   …  γ   [  L  ]    )       p    σ   delimited-[]  1   normal-…  σ   delimited-[]  L   γ   delimited-[]  1   normal-…  γ   delimited-[]  L      p(\sigma[1]\dots\sigma[L]\gamma[1]\ldots\gamma[L])   is the probability that this tag corresponds to the text    σ   [  1  ]   …  σ   [  L  ]       σ   delimited-[]  1   normal-…  σ   delimited-[]  L     \sigma[1]\ldots\sigma[L]   (lexical probability).  In a Markov model , these probabilities are approximated as products. The syntactic probabilities are modelled by a first order Markov process:       p   (   γ   [  1  ]   γ   [  2  ]   …  γ   [  L  ]    )    =    ∏   t  =  1    t  =  L     p   (   γ   [   t  +  1   ]   γ   [  t  ]    )           p    γ   delimited-[]  1   γ   delimited-[]  2   normal-…  γ   delimited-[]  L       superscript   subscript  product    t  1      t  L      p    γ   delimited-[]    t  1    γ   delimited-[]  t        p(\gamma[1]\gamma[2]\ldots\gamma[L])=\prod_{t=1}^{t=L}p(\gamma[t+1]\gamma[t])     where    γ   [  0  ]       γ   delimited-[]  0     \gamma[0]   and    γ   [   L  +  1   ]       γ   delimited-[]    L  1      \gamma[L+1]   are delimiter symbols.  Lexical probabilities are independent of context:       p   (   σ   [  1  ]   σ   [  2  ]   …  σ   [  L  ]   γ   [  1  ]   γ   [  2  ]   …  γ   [  L  ]    )    =    ∏   t  =  1    t  =  L     p   (   σ   [  t  ]   γ   [  t  ]    )           p    σ   delimited-[]  1   σ   delimited-[]  2   normal-…  σ   delimited-[]  L   γ   delimited-[]  1   γ   delimited-[]  2   normal-…  γ   delimited-[]  L       superscript   subscript  product    t  1      t  L      p    σ   delimited-[]  t   γ   delimited-[]  t        p(\sigma[1]\sigma[2]\ldots\sigma[L]\gamma[1]\gamma[2]\ldots\gamma[L])=\prod_{t%
 =1}^{t=L}p(\sigma[t]\gamma[t])     One form of tagging is to approximate the first probability formula:       p   (   σ   [  1  ]   σ   [  2  ]   …  σ   [  L  ]   γ   [  1  ]   γ   [  2  ]   …  γ   [  L  ]    )    =    ∏   t  =  1    t  =  L     p   (   γ   [  t  ]    C   (  -  )     [  t  ]   σ   [  t  ]    C   (  +  )     [  t  ]    )           p    σ   delimited-[]  1   σ   delimited-[]  2   normal-…  σ   delimited-[]  L   γ   delimited-[]  1   γ   delimited-[]  2   normal-…  γ   delimited-[]  L       superscript   subscript  product    t  1      t  L      p    γ   delimited-[]  t    subscript  C     delimited-[]  t   σ   delimited-[]  t    subscript  C     delimited-[]  t        p(\sigma[1]\sigma[2]\ldots\sigma[L]\gamma[1]\gamma[2]\ldots\gamma[L])=\prod_{t%
 =1}^{t=L}p(\gamma[t]C_{(-)}[t]\sigma[t]C_{(+)}[t])     where      C   (  -  )     [  t  ]    =   σ   [   t  -   N   (  -  )     ]   σ   [   t  -   N   (  -  )     ]   …  σ   [   t  -  1   ]           subscript  C     delimited-[]  t      σ   delimited-[]    t   subscript  N      σ   delimited-[]    t   subscript  N      normal-…  σ   delimited-[]    t  1       C_{(-)}[t]=\sigma[t-N_{(-)}]\sigma[t-N_{(-)}]\ldots\sigma[t-1]   is the right context of the size    N   (  +  )      subscript  N     N_{(+)}   .  In this way the sliding window algorithm only has to take into account a context of size     N   (  -  )    +   N   (  +  )    +  1       subscript  N     subscript  N    1    N_{(-)}+N_{(+)}+1   . For most applications     N   (  -  )    =   N   (  +  )    =  1         subscript  N     subscript  N         1     N_{(-)}=N_{(+)}=1   . For example to tag the ambiguous word "run" in the sentence "He runs from danger", only the tags of the words "He" and "from" are needed to be taken into account.  Further reading   Sanchez-Villamil, E., Forcada, M. L., and Carrasco, R. C. (2005). " Unsupervised training of a finite-state sliding-window part-of-speech tagger ". Lecture Notes in Computer Science / Lecture Notes in Artificial Intelligence , vol. 3230, p. 454-463   "  Category:Computational linguistics   