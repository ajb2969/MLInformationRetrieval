   Central limit theorem for directional statistics      Central limit theorem for directional statistics   In probability theory , the central limit theorem states conditions under which the average of a sufficiently large number of independent  random variables , each with finite mean and variance, will be approximately normally distributed . 1  Directional statistics is the subdiscipline of statistics that deals with directions ( unit vectors in R n ), axes (lines through the origin in R n ) or rotations in R n . The means and variances of directional quantities are all finite, so that the central limit theorem may be applied to the particular case of directional statistics. 2  This article will deal only with unit vectors in 2-dimensional space ( R 2 ) but the method described can be extended to the general case.  The central limit theorem  A sample of angles    Î¸  i     subscript  Î¸  i    \theta_{i}   are measured, and since they are indefinite to within a factor of    2  Ï€      2  Ï€    2\pi   , the complex definite quantity     z  i   =   e   i   Î¸  i     =    cos   (   Î¸  i   )    +   i   sin   (   Î¸  i   )             subscript  z  i    superscript  e    i   subscript  Î¸  i               subscript  Î¸  i      i     subscript  Î¸  i         z_{i}=e^{i\theta_{i}}=\cos(\theta_{i})+i\sin(\theta_{i})   is used as the random variate. The probability distribution from which the sample is drawn may be characterized by its moments, which may be expressed in Cartesian and polar form:       m  n   =   E   (   z  n   )    =    C  n   +   i   S  n     =    R  n     e   i   Î¸  n              subscript  m  n     E   superscript  z  n            subscript  C  n     i   subscript  S  n             subscript  R  n    superscript  e    i   subscript  Î¸  n         m_{n}=E(z^{n})=C_{n}+iS_{n}=R_{n}e^{i\theta_{n}}\,     It follows that:       C  n   =   E   (   cos   (   n  Î¸   )    )         subscript  C  n     E      n  Î¸       C_{n}=E(\cos(n\theta))\,          S  n   =   E   (   sin   (   n  Î¸   )    )         subscript  S  n     E      n  Î¸       S_{n}=E(\sin(n\theta))\,          R  n   =   |   E   (   z  n   )    |   =      C  n  2   +   S  n  2             subscript  R  n       E   superscript  z  n               superscript   subscript  C  n   2    superscript   subscript  S  n   2        R_{n}=|E(z^{n})|=\sqrt{C_{n}^{2}+S_{n}^{2}}\,          Î¸  n   =   arg   (   E   (   z  n   )    )         subscript  Î¸  n       E   superscript  z  n       \theta_{n}=\arg(E(z^{n}))\,     Sample moments for N trials are:        m  n   Â¯   =    1  N     âˆ‘   i  =  1   N    z  i  n     =     C  n   Â¯   +   i    S  n   Â¯     =     R  n   Â¯    e   i    Î¸  n   Â¯             normal-Â¯   subscript  m  n        1  N     superscript   subscript     i  1    N    superscript   subscript  z  i   n             normal-Â¯   subscript  C  n      i   normal-Â¯   subscript  S  n              normal-Â¯   subscript  R  n     superscript  e    i   normal-Â¯   subscript  Î¸  n          \overline{m_{n}}=\frac{1}{N}\sum_{i=1}^{N}z_{i}^{n}=\overline{C_{n}}+i%
 \overline{S_{n}}=\overline{R_{n}}e^{i\overline{\theta_{n}}}     where        C  n   Â¯   =    1  N     âˆ‘   i  =  1   N    cos   (   n   Î¸  i    )           normal-Â¯   subscript  C  n        1  N     superscript   subscript     i  1    N       n   subscript  Î¸  i         \overline{C_{n}}=\frac{1}{N}\sum_{i=1}^{N}\cos(n\theta_{i})           S  n   Â¯   =    1  N     âˆ‘   i  =  1   N    sin   (   n   Î¸  i    )           normal-Â¯   subscript  S  n        1  N     superscript   subscript     i  1    N       n   subscript  Î¸  i         \overline{S_{n}}=\frac{1}{N}\sum_{i=1}^{N}\sin(n\theta_{i})           R  n   Â¯   =    1  N     âˆ‘   i  =  1   N    |   z  i  n   |          normal-Â¯   subscript  R  n        1  N     superscript   subscript     i  1    N      superscript   subscript  z  i   n        \overline{R_{n}}=\frac{1}{N}\sum_{i=1}^{N}|z_{i}^{n}|           Î¸  n   Â¯   =    1  N     âˆ‘   i  =  1   N    arg   (   z  i  n   )           normal-Â¯   subscript  Î¸  n        1  N     superscript   subscript     i  1    N      superscript   subscript  z  i   n        \overline{\theta_{n}}=\frac{1}{N}\sum_{i=1}^{N}\arg(z_{i}^{n})     The vector [      C  1   Â¯   ,    S  1   Â¯       normal-Â¯   subscript  C  1     normal-Â¯   subscript  S  1      \overline{C_{1}},\overline{S_{1}}   ] may be used as a representation of the sample mean    (    m  1   Â¯   )     normal-Â¯   subscript  m  1     (\overline{m_{1}})   and may be taken as a 2-dimensional random variate. 3 The bivariate central limit theorem states that the joint probability distribution for     C  1   Â¯     normal-Â¯   subscript  C  1     \overline{C_{1}}   and     S  1   Â¯     normal-Â¯   subscript  S  1     \overline{S_{1}}   in the limit of a large number of samples is given by:       [    C  1   Â¯   ,    S  1   Â¯   ]    â†’  ğ‘‘    ğ’©   (   [   C  1   ,   S  1   ]   ,   Î£  /  N   )        d  normal-â†’     normal-Â¯   subscript  C  1     normal-Â¯   subscript  S  1       ğ’©     subscript  C  1    subscript  S  1      normal-Î£  N       [\overline{C_{1}},\overline{S_{1}}]\xrightarrow{d}\mathcal{N}([C_{1},S_{1}],%
 \Sigma/N)     where    ğ’©   (  )       ğ’©     \mathcal{N}()   is the bivariate normal distribution and   Î£   normal-Î£   \Sigma   is the covariance matrix for the circular distribution:       Î£  =   [      Ïƒ   C  C       Ïƒ   C  S         Ïƒ   S  C       Ïƒ   S  S       ]        normal-Î£     subscript  Ïƒ    C  C     subscript  Ïƒ    C  S       subscript  Ïƒ    S  C     subscript  Ïƒ    S  S        \Sigma=\begin{bmatrix}\sigma_{CC}&\sigma_{CS}\\
 \sigma_{SC}&\sigma_{SS}\end{bmatrix}\quad          Ïƒ   C  C    =    E   (    cos  2   Î¸   )    -   E     (   cos  Î¸   )   2           subscript  Ïƒ    C  C        E    superscript   2   Î¸      E   superscript    Î¸   2       \sigma_{CC}=E(\cos^{2}\theta)-E(\cos\theta)^{2}\,          Ïƒ   C  S    =   Ïƒ   S  C    =    E   (    cos  Î¸    sin  Î¸    )    -   E   (   cos  Î¸   )   E   (   sin  Î¸   )            subscript  Ïƒ    C  S     subscript  Ïƒ    S  C             E      Î¸     Î¸       E    Î¸   E    Î¸        \sigma_{CS}=\sigma_{SC}=E(\cos\theta\sin\theta)-E(\cos\theta)E(\sin\theta)\,          Ïƒ   S  S    =    E   (    sin  2   Î¸   )    -   E     (   sin  Î¸   )   2           subscript  Ïƒ    S  S        E    superscript   2   Î¸      E   superscript    Î¸   2       \sigma_{SS}=E(\sin^{2}\theta)-E(\sin\theta)^{2}\,     Note that the bivariate normal distribution is defined over the entire plane, while the mean is confined to be in the unit ball (on or inside the unit circle). This means that the integral of the limiting (bivariate normal) distribution over the unit ball will not be equal to unity, but rather approach unity as N approaches infinity.  It is desired to state the limiting bivariate distribution in terms of the moments of the distribution.  Covariance matrix in terms of moments  Using multiple angle trigonometric identities 4       C  2   =   E   (   cos   (   2  Î¸   )    )    =   E   (     cos  2   Î¸   -  1   )    =   E   (   1  -    sin  2   Î¸    )           subscript  C  2     E      2  Î¸            E      superscript   2   Î¸   1           E    1    superscript   2   Î¸        C_{2}=E(\cos(2\theta))=E(\cos^{2}\theta-1)=E(1-\sin^{2}\theta)\,          S  2   =   E   (   sin   (   2  Î¸   )    )    =   E   (   2   cos  Î¸    sin  Î¸    )           subscript  S  2     E      2  Î¸            E    2    Î¸     Î¸        S_{2}=E(\sin(2\theta))=E(2\cos\theta\sin\theta)\,     It follows that:       Ïƒ   C  C    =    E   (    cos  2   Î¸   )    -   E    (   cos  Î¸   )   2     =    1  2    (    1  +   C  2    -   2   C  1  2     )           subscript  Ïƒ    C  C        E    superscript   2   Î¸      E   superscript    Î¸   2              1  2       1   subscript  C  2      2   superscript   subscript  C  1   2         \sigma_{CC}=E(\cos^{2}\theta)-E(\cos\theta)^{2}=\frac{1}{2}\left(1+C_{2}-2C_{1%
 }^{2}\right)          Ïƒ   C  S    =    E   (    cos  Î¸    sin  Î¸    )    -   E   (   cos  Î¸   )   E   (   sin  Î¸   )     =    1  2    (    S  2   -   2   C  1    S  1     )           subscript  Ïƒ    C  S        E      Î¸     Î¸       E    Î¸   E    Î¸              1  2      subscript  S  2     2   subscript  C  1    subscript  S  1         \sigma_{CS}=E(\cos\theta\sin\theta)-E(\cos\theta)E(\sin\theta)=\frac{1}{2}%
 \left(S_{2}-2C_{1}S_{1}\right)          Ïƒ   S  S    =    E   (    sin  2   Î¸   )    -   E    (   sin  Î¸   )   2     =    1  2    (   1  -   C  2   -   2   S  1  2     )           subscript  Ïƒ    S  S        E    superscript   2   Î¸      E   superscript    Î¸   2              1  2     1   subscript  C  2     2   superscript   subscript  S  1   2         \sigma_{SS}=E(\sin^{2}\theta)-E(\sin\theta)^{2}=\frac{1}{2}\left(1-C_{2}-2S_{1%
 }^{2}\right)     The covariance matrix is now expressed in terms of the moments of the circular distribution.  The central limit theorem may also be expressed in terms of the polar components of the mean. If    P   (    C  1   Â¯   ,    S  1   Â¯   )   d    C  1   Â¯   d    S  1   Â¯       P    normal-Â¯   subscript  C  1     normal-Â¯   subscript  S  1     d   normal-Â¯   subscript  C  1    d   normal-Â¯   subscript  S  1      P(\overline{C_{1}},\overline{S_{1}})d\overline{C_{1}}d\overline{S_{1}}   is the probability of finding the mean in area element    d    C  1   Â¯   d    S  1   Â¯       d   normal-Â¯   subscript  C  1    d   normal-Â¯   subscript  S  1      d\overline{C_{1}}d\overline{S_{1}}   , then that probability may also be written    P   (     R  1   Â¯    cos   (    Î¸  1   Â¯   )     ,     R  1   Â¯    sin   (    Î¸  1   Â¯   )     )     R  1   Â¯   d    R  1   Â¯   d    Î¸  1   Â¯       P      normal-Â¯   subscript  R  1       normal-Â¯   subscript  Î¸  1         normal-Â¯   subscript  R  1       normal-Â¯   subscript  Î¸  1        normal-Â¯   subscript  R  1    d   normal-Â¯   subscript  R  1    d   normal-Â¯   subscript  Î¸  1      P(\overline{R_{1}}\cos(\overline{\theta_{1}}),\overline{R_{1}}\sin(\overline{%
 \theta_{1}}))\overline{R_{1}}d\overline{R_{1}}d\overline{\theta_{1}}   .  References    "  Category:Directional statistics    Category:Asymptotic statistical theory     â†©  â†©       