   Davies‚ÄìBouldin index      Davies‚ÄìBouldin index   The Davies‚ÄìBouldin index (DBI) (introduced by David L. Davies and Donald W. Bouldin in 1979) is a metric for evaluating clustering algorithms . 1 This is an internal evaluation scheme, where the validation of how well the clustering has been done is made using quantities and features inherent to the dataset. This has a drawback that a good value reported by this method does not imply the best information retrieval.  Preliminaries  Let C i be a cluster of vectors. Let X j be an n dimensional feature vector assigned to cluster C i .       S  i   =    1   T  i      ‚àë   j  =  1    T  i      ||    X  j   -   A  i    ||   p          subscript  S  i       1   subscript  T  i      superscript   subscript     j  1     subscript  T  i     subscript   norm     subscript  X  j    subscript  A  i     p       S_{i}=\frac{1}{T_{i}}\sum_{j=1}^{T_{i}}{\left|\left|X_{j}-A_{i}\right|\right|_%
 {p}}     Here    A  i     subscript  A  i    A_{i}   is the centroid of C i and T i is the size of the cluster i . S i is a measure of scatter within the cluster. Usually the value of p is 2, which makes this a Euclidean distance function between the centroid of the cluster, and the individual feature vectors. Many other distance metrics can be used, in the case of manifolds and higher dimensional data, where the euclidean distance may not be the best measure for determining the clusters. It is important to note that this distance metric has to match with the metric used in the clustering scheme itself for meaningful results.       M   i  ,  j    =    ||    A  i   -   A  j    ||   p   =    (    ‚àë   k  =  1   n     |    a   k  ,  i    -   a   k  ,  j     |   p    )    1  p           subscript  M   i  j     subscript   norm     subscript  A  i    subscript  A  j     p         superscript    superscript   subscript     k  1    n    superscript       subscript  a   k  i     subscript  a   k  j      p      1  p       M_{i,j}=\left|\left|A_{i}-A_{j}\right|\right|_{p}=\Bigl(\displaystyle\sum_{k=1%
 }^{n}\left|a_{k,i}-a_{k,j}\right|^{p}\Bigr)^{\frac{1}{p}}         M   i  ,  j      subscript  M   i  j     M_{i,j}   is a measure of separation between cluster    C  i     subscript  C  i    C_{i}   and cluster    C  j     subscript  C  j    C_{j}   .      a   k  ,  i      subscript  a   k  i     a_{k,i}   is the k th element of    A  i     subscript  A  i    A_{i}   , and there are n such elements in A for it is an n dimensional centroid.  Here k indexes the features of the data, and this is essentially the Euclidean distance between the centers of clusters i and j when p equals 2.  Definition  Let R i,j be a measure of how good the clustering scheme is. This measure, by definition has to account for M i,j the separation between the i th and the j th cluster, which ideally has to be as large as possible, and S i , the within cluster scatter for cluster i, which has to be as low as possible. Hence the Davies‚ÄìBouldin index is defined as the ratio of S i and M i,j such that these properties are conserved:        R   i  ,  j    ‚©æ  0       subscript  R   i  j    0    R_{i,j}\geqslant 0   .       R   i  ,  j    =   R   j  ,  i         subscript  R   i  j     subscript  R   j  i      R_{i,j}=R_{j,i}   .  When     S  j   ‚©æ   S  k        subscript  S  j    subscript  S  k     S_{j}\geqslant S_{k}   and     M   i  ,  j    =   M   i  ,  k         subscript  M   i  j     subscript  M   i  k      M_{i,j}=M_{i,k}   then     R   i  ,  j    >   R   i  ,  k         subscript  R   i  j     subscript  R   i  k      R_{i,j}>R_{i,k}   .  When     S  j   =   S  k        subscript  S  j    subscript  S  k     S_{j}=S_{k}   and     M   i  ,  j    ‚©Ω   M   i  ,  k         subscript  M   i  j     subscript  M   i  k      M_{i,j}\leqslant M_{i,k}   then     R   i  ,  j    >   R   i  ,  k         subscript  R   i  j     subscript  R   i  k      R_{i,j}>R_{i,k}   .   With this formulation, the lower the value, the better the separation of the clusters and the 'tightness' inside the clusters.  A solution that satisfies these properties is:       R   i  ,  j    =     S  i   +   S  j     M   i  ,  j          subscript  R   i  j         subscript  S  i    subscript  S  j     subscript  M   i  j       R_{i,j}=\frac{S_{i}+S_{j}}{M_{i,j}}     This is used to define D i :       D  i   ‚â°    max   j  ‚â†  i     R   i  ,  j          subscript  D  i     subscript     j  i     subscript  R   i  j       D_{i}\equiv\max_{j\neq i}R_{i,j}     If N is the number of clusters:      ùê∑ùêµ  ‚â°    1  N     ‚àë   i  =  1   N    D  i         ùê∑ùêµ      1  N     superscript   subscript     i  1    N    subscript  D  i       \mathit{DB}\equiv\frac{1}{N}\displaystyle\sum_{i=1}^{N}D_{i}     DB is called the Davies‚ÄìBouldin index. This is dependent both on the data as well as the algorithm. D i chooses the worst-case scenario, and this value is equal to R i,j for the most similar cluster to cluster i . There could be many variations to this formulation, like choosing the average of the cluster similarity, weighted average and so on.  Explanation  These conditions constrain the index so defined to be symmetric and non-negative. Due to the way it is defined, as a function of the ratio of the within cluster scatter, to the between cluster separation, a lower value will mean that the clustering is better. It happens to be the average similarity between each cluster and its most similar one, averaged over all the clusters, where the similarity is defined as S i above. This affirms the idea that no cluster has to be similar to another, and hence the best clustering scheme essentially minimizes the Davies‚ÄìBouldin index. This index thus defined is an average over all the i clusters, and hence a good measure of deciding how many clusters actually exists in the data is to plot it against the number of clusters it is calculated over. The number i for which this value is the lowest is a good measure of the number of clusters the data could be ideally classified into. This has applications in deciding the value of k in the kmeans algorithm, where the value of k is not known apriori. The SOM toolbox contains a MATLAB implementation. 2 A MATLAB implementation is also available via the MATLAB Statistics and Machine Learning Toolbox, using the "evalclusters" command. 3  External links   http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.2072  http://books.google.com/books?id=HY8gB2OIqSoC   Notes and references  "  Category:Clustering criteria     ‚Ü©  ‚Ü©  http://www.mathworks.com/help/stats/evalclusters.html ‚Ü©     