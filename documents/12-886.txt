   Sparse PCA      Sparse PCA   Sparse principal component analysis ( sparse PCA ) is a specialised technique used in statistical analysis and, in particular, in the analysis of multivariate data sets. It extends the classic method of principal component analysis (PCA) for the reduction of dimensionality of data by adding sparsity constraint on the input variables.  Ordinary principal component analysis (PCA) uses a vector space  transform to reduce multidimensional data sets to lower dimensions. It finds linear combinations of input variables, and transforms them into new variables (called principal components ) that correspond to directions of maximal variance in the data. The number of new variables created by these linear combinations is usually much lower than the number of input variables in the original dataset, while still explaining most of the variance present in the data.  A particular disadvantage of ordinary PCA is that the principal components are usually linear combinations of all input variables. Sparse PCA overcomes this disadvantage by finding linear combinations that contain just a few input variables.  Mathematical Formulation  Consider a data matrix , X , where each of the p columns represent an input variable, and each of the n rows represents an independent sample from data population. We assume each column of X has mean zero, otherwise we can subtract column-wise mean from each element of X . Let Σ=X T X be the empirical covariance matrix of X , which has dimension p×p . Given an integer k with 1≤k≤p , the sparse PCA problem can be formulated as maximizing the variance along a direction represented by vector    v  ∈   ℝ  p       v   superscript  ℝ  p     v\in\mathbb{R}^{p}   while constraining its cardinality:     max     \displaystyle\max    Eq. 1 The first constraint specifies that v is a unit vector. In the second constraint,     ∥  v  ∥   0     subscript   norm  v   0    \left\|v\right\|_{0}   represents the L0 norm of v , which is defined as the number of its non-zero components. So the second constraint specifies that the number of non-zero components in v is less than or equal to k , which is typically an integer that is much smaller than dimension p . The optimal value of  is known as the k -sparse largest eigenvalue .  If we take k=p , the problem reduces to the ordinary PCA , and the optimal value becomes the largest eigenvalue of covariance matrix Σ .  After finding the optimal solution v , we deflate Σ to obtain a new matrix        Σ  1   =   Σ  -    (    v  T   Σ  v   )   v   v  T      ,       subscript  normal-Σ  1     normal-Σ       superscript  v  T   normal-Σ  v   v   superscript  v  T       \Sigma_{1}=\Sigma-(v^{T}\Sigma v)vv^{T},   and iterate this process to obtain further principal components. However, unlike PCA, sparse PCA cannot guarantee that different principal components are orthogonal . In order to achieve orthogonality, additional constraints must be enforced.  Because of the cardinality constraint, the maximization problem is hard to solve exactly, especially when dimension p is high. In fact, the sparse PCA problem in  is NP-hard in the strong sense. 1 Several alternative approaches have been proposed, including   a regression framework, {{ cite journal   | author = H. Zou and T. Hastie and R. Tibshirani  | year = 2006  | title = Sparse principal component analysis  | journal = Jcgs 2006 15(2): 262-286  | url = http://www-stat.stanford.edu/~hastie/Papers/spc_jcgs.pdf  }}   a convex relaxation/semidefinite programming framework, {{ cite journal   | author = Alexandre d’Aspremont, Laurent El Ghaoui, Michael I. Jordan, Gert R. G. Lanckriet  | year = 2007  | title = A Direct Formulation for Sparse PCA Using Semidefinite Programming  | journal = SIAM Review 49(3):434–448  | url = http://www.cmap.polytechnique.fr/~aspremon/PDF/sparsesvd.pdf  }}   a generalized power method framework {{ cite journal   | author = Michel Journee, Yurii Nesterov, Peter Richtarik, Rodolphe Sepulchre  | year = 2008  | pages = 4724  | title = Generalized Power Method for Sparse Principal Component Analysis  | volume = 0811  | journal = CORE Discussion Paper 2008/70, Journal of Machine Learning Research 11 (2010) 517-553  | url = http://jmlr.csail.mit.edu/papers/volume11/journee10a/journee10a.pdf  | arxiv=0811.4724  }}   an alternating maximization framework {{ cite journal   | author = Peter Richtarik, Martin Takac and S. Damla Ahipasaoglu  | year = 2012  | title = Alternating Maximization: Unifying Framework for 8 Sparse PCA Formulations and Efficient Parallel Codes  | url = http://arxiv.org/abs/1212.4137  | arxiv=1212.4137  }}   forward/backward greedy search and exact methods using branch-and-bound techniques, {{ cite journal   | author = Baback Moghaddam, Yair Weiss, Shai Avidan  | year = 2005  | title = Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms  | journal = Advances in Neural Information Processing Systems (NIPS), MIT Press  | url = http://books.nips.cc/papers/files/nips18/NIPS2005_0643.pdf  }}   Bayesian formulation framework. {{ cite journal   | author = Yue Guan, Jennifer Dy  | year = 2009  | title = Sparse Probabilistic Principal Component Analysis  | journal = Journal of Machine Learning Research Workshop and Conference Proceedings| volume = 5: AISTATS 2009  | url = http://jmlr.csail.mit.edu/proceedings/papers/v5/guan09a/guan09a.pdf  }}  Semidefinite Programming Relaxation  It has been proposed that sparse PCA can be approximated by semidefinite programming (SDP). 2 Let   V   V   V   be a p×p symmetric matrix, we can rewrite the sparse PCA problem as     max     \displaystyle\max    Eq. 2  Tr is the matrix trace , and     ∥  V  ∥   0     subscript   norm  V   0    \|V\|_{0}   represents the non-zero elements in matrix V . The last line specifies that V has matrix rank one and is positive semidefinite . The last line means that we have    V  =   v   v  T        V    v   superscript  v  T      V=vv^{T}   , so  is equivalent to . If we drop the rank constraint and relax the cardinality contraint by a 1-norm convex constraint, we get a semidefinite programming relaxation, which can be solved efficiently in polynomial time:     max     \displaystyle\max    Eq. 3 In the second constraint,   𝟏   1   \mathbf{1}   is a p×1 vector of ones, and |V| is the matrix whose elements are the absolute values of the elements of V .  Unfortunately, the optimal solution   V   V   V   to the relaxed problem  is not guaranteed to have rank one. In that case,   V   V   V   can be truncated to retain only the dominant eigenvector.  Applications  Financial Data Analysis  Suppose ordinary PCA is applied to a dataset where each input variable represents a different asset, it may generate principal components that are weighted combination of all the assets. In contrast, sparse PCA would produce principal components that are weighted combination of only a few input assets, so one can easily interpret its meaning. Furthermore, if one uses a trading strategy based on these principal components, fewer assets imply less transaction costs.  Biology  Consider a dataset where each input variable corresponds to a specific gene. Sparse PCA can produce a principal component that involves only a few genes, so researchers can focus on these specific genes for further analysis.  Sparse PCA in High Dimension  Contemporary datasets often have the number of input variables ( p ) comparable with or even much larger than the number of samples ( n ). It has been shown that if p/n does not converge to zero, the classical PCA is not consistent . In other words, if we let k=p in , then the optimal value does not converge to the largest eigenvalue of data population when the sample size n→∞ , and the optimal solution does not converge to the direction of maximum variance. But sparse PCA can retain consistency even if     p  ≫  n   .     much-greater-than  p  n    \scriptstyle p\gg n.    {{ cite journal  | author =  Iain M Johnstone, Arthur Yu Lu  | year = 2009  | title = On Consistency and Sparsity for Principal Components Analysis in High Dimensions  | journal = Journal of the American Statistical Association, Volume 104, Issue 486, 2009  | url = http://amstat.tandfonline.com/doi/abs/10.1198/jasa.2009.0121#.VIPPDl2BtC0  | doi=10.1198/jasa.2009.0121  | pages=682–693  }}  An Example of Hypothesis Testing in High Dimension using Sparse PCA  More specifically, the k -sparse largest eigenvalue (the optimal value of ) can be used to discriminate an isometric model, where every direction has the same variance, from a spiked covariance model in high-dimensional setting. {{ cite journal  | author =  Quentin Berthet, Philippe Rigollet  | year = 2013  | title = Optimal Detection of Sparse Principal Components in High Dimension  | journal = The Annals of Statistics, 2013, Vol. 41, No. 1, 1780–1815.  | url = http://projecteuclid.org/euclid.aos/1378386239  }} Consider a hypothesis test where the null hypothesis specifies that data    X   X   X    are generated from multivariate normal distributuion with mean 0 and covariance equal to an identity matrix, and the alternative hypothesis specifies that data    X   X   X    is generated from a spiked model with signal strength    θ   θ   \theta    :        H  0   :   X  ∼    N   (  0  ,   I  p   )    ,   H  1     :   X  ∼   N   (  0  ,    I  p   +   θ  v   v  T     )      ,       normal-:   subscript  H  0    similar-to  X     N   0   subscript  I  p      subscript  H  1       normal-:     similar-to  X    N   0     subscript  I  p     θ  v   superscript  v  T           H_{0}:X\sim N(0,I_{p}),\quad H_{1}:X\sim N(0,I_{p}+\theta vv^{T}),   where    v  ∈   ℝ  p       v   superscript  ℝ  p     v\in\mathbb{R}^{p}   has only k non-zero coordinates. The largest k -sparse eigenvalue can discriminate the two hypothesis if and only if    θ  >   Θ   (     k   log   (  p  )     /  n    )        θ    normal-Θ        k    p    n       \theta>\Theta(\sqrt{k\log(p)/n})   .  Since computing k -sparse eigenvalue is NP-hard, one can approximate it by the optimal value of semidefinite programming relaxation (). If that case, we can discriminate the two hypotheses if    θ  >   Θ   (      k  2    log   (  p  )     /  n    )        θ    normal-Θ         superscript  k  2     p    n       \theta>\Theta(\sqrt{k^{2}\log(p)/n})   . The additional    k      k    \sqrt{k}   term cannot be improved by any other polynomical time algorithm if a conjecture for the hidden clique problem holds.  References    "  Category:Multivariate statistics  Category:Machine learning algorithms     ↩      