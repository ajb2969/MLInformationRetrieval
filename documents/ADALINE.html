<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1969">ADALINE</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>ADALINE</h1>
<hr/>
<figure><b>(Figure)</b>
<figcaption>Learning inside a single layer ADALINE</figcaption>
</figure>

<p><strong>ADALINE</strong> (<strong>Adaptive Linear Neuron</strong> or later <strong>Adaptive Linear Element</strong>) is an early single-layer <a href="artificial_neural_network" title="wikilink">artificial neural network</a> and the name of the physical device that implemented this network.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> The network uses <a href="memistor" title="wikilink">memistors</a>. It was developed by Professor <a href="Bernard_Widrow" title="wikilink">Bernard Widrow</a> and his graduate student <a href="Marcian_Hoff" title="wikilink">Ted Hoff</a> at <a href="Stanford_University" title="wikilink">Stanford University</a> in 1960. It is based on the <a href="McCulloch–Pitts_neuron" title="wikilink">McCulloch–Pitts neuron</a>. It consists of a weight, a bias and a summation function.</p>

<p>The difference between Adaline and the standard (<a href="McCulloch–Pitts_neuron" title="wikilink">McCulloch–Pitts</a>) <a class="uri" href="perceptron" title="wikilink">perceptron</a> is that in the learning phase the weights are adjusted according to the weighted sum of the inputs (the net). In the standard perceptron, the net is passed to the activation (<a href="transfer_function" title="wikilink">transfer</a>) function and the function's output is used for adjusting the weights.</p>

<p>There also exists an extension known as <a class="uri" href="Madaline" title="wikilink">Madaline</a>.</p>
<h2 id="definition">Definition</h2>

<p>Adaline is a multiple layer neural network with multiple nodes where each node accepts multiple inputs and generates one output. Given the following variables:as</p>
<ul>
<li>x is the input vector</li>
<li>w is the weight vector</li>
<li>n is the number of inputs</li>
<li>

<math display="inline" id="ADALINE:0">
 <semantics>
  <mi>θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta
  </annotation>
 </semantics>
</math>

 some constant</li>
<li>y is the output of the model</li>
</ul>

<p>then we find that the output is 

<math display="inline" id="ADALINE:1">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mi>j</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </msubsup>
     <mrow>
      <msub>
       <mi>x</mi>
       <mi>j</mi>
      </msub>
      <msub>
       <mi>w</mi>
       <mi>j</mi>
      </msub>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mi>θ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <apply>
     <plus></plus>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>j</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>j</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <ci>j</ci>
       </apply>
      </apply>
     </apply>
     <ci>θ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=\sum_{j=1}^{n}x_{j}w_{j}+\theta
  </annotation>
 </semantics>
</math>

. If we further assume that</p>
<ul>
<li>

<math display="inline" id="ADALINE:2">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mrow>
     <mi>n</mi>
     <mo>+</mo>
     <mn>1</mn>
    </mrow>
   </msub>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <apply>
      <plus></plus>
      <ci>n</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{n+1}=1
  </annotation>
 </semantics>
</math>

</li>
<li>

<math display="inline" id="ADALINE:3">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mrow>
     <mi>n</mi>
     <mo>+</mo>
     <mn>1</mn>
    </mrow>
   </msub>
   <mo>=</mo>
   <mi>θ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <apply>
      <plus></plus>
      <ci>n</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <ci>θ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{n+1}=\theta
  </annotation>
 </semantics>
</math>

</li>
</ul>

<p>then the output further reduces to the dot product of x and w

<math display="block" id="ADALINE:4">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mrow>
    <mi>x</mi>
    <mo>⋅</mo>
    <mi>w</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <apply>
     <ci>normal-⋅</ci>
     <ci>x</ci>
     <ci>w</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=x\cdot w
  </annotation>
 </semantics>
</math>

</p>
<h2 id="learning-algorithm">Learning algorithm</h2>

<p>Let us assume:</p>
<ul>
<li>

<math display="inline" id="ADALINE:5">
 <semantics>
  <mi>η</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>η</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \eta
  </annotation>
 </semantics>
</math>

 is the learning rate (some positive constant)</li>
<li>

<math display="inline" id="ADALINE:6">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 is the output of the model</li>
<li>

<math display="inline" id="ADALINE:7">
 <semantics>
  <mi>o</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>o</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   o
  </annotation>
 </semantics>
</math>

 is the target (desired) output</li>
</ul>

<p>then the weights are updated as follows 

<math display="inline" id="ADALINE:8">
 <semantics>
  <mrow>
   <mi>w</mi>
   <mo>←</mo>
   <mrow>
    <mi>w</mi>
    <mo>+</mo>
    <mrow>
     <mi>η</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>o</mi>
       <mo>-</mo>
       <mi>y</mi>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>x</mi>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-←</ci>
    <ci>w</ci>
    <apply>
     <plus></plus>
     <ci>w</ci>
     <apply>
      <times></times>
      <ci>η</ci>
      <apply>
       <minus></minus>
       <ci>o</ci>
       <ci>y</ci>
      </apply>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w\leftarrow w+\eta(o-y)x
  </annotation>
 </semantics>
</math>

. The ADALINE converges to the least squares error which is 

<math display="inline" id="ADALINE:9">
 <semantics>
  <mrow>
   <mi>E</mi>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>o</mi>
      <mo>-</mo>
      <mi>y</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>E</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <minus></minus>
      <ci>o</ci>
      <ci>y</ci>
     </apply>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   E=(o-y)^{2}
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> This update rule is in fact the <a href="stochastic_gradient_descent" title="wikilink">stochastic gradient descent</a> update for <a href="linear_regression" title="wikilink">linear regression</a>.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2">Youtube: <a href="http://www.youtube.com/watch?v=IEFRtz68m-8">widrowlms: Science in Action</a><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="http://www-isl.stanford.edu/~widrow/papers/t1960anadaptive.pdf">1960: An adaptive "ADALINE" neuron using chemical "memistors"</a><a href="#fnref3">↩</a></li>
<li id="fn4">Youtube: <a href="http://www.youtube.com/watch?v=hc2Zj55j1zU">widrowlms: The LMS algorithm and ADALINE. Part I - The LMS algorithm</a><a href="#fnref4">↩</a></li>
<li id="fn5">Youtube: <a href="http://www.youtube.com/watch?v=skfNlwEbqck">widrowlms: The LMS algorithm and ADALINE. Part II - ADALINE and memistor ADALINE</a><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
</ol>
</section>
</body>
</html>
