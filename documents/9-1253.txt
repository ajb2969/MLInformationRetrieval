   Broyden's method      Broyden's method   In numerical analysis, Broyden's method is a quasi-Newton method for finding roots in   k   k   k   variables. It was originally described by C. G. Broyden in 1965. 1  Newton's method for solving     𝐟   (  𝐱  )    =  𝟎        𝐟  𝐱   0    \mathbf{f}(\mathbf{x})=\mathbf{0}   uses the Jacobian matrix ,   𝐉   𝐉   \mathbf{J}   , at every iteration. However, computing this Jacobian is a difficult and expensive operation. The idea behind Broyden's method is to compute the whole Jacobian only at the first iteration, and to do a rank-one update at the other iterations.  In 1979 Gay proved that when Broyden's method is applied to a linear system of size    n  ×  n      n  normal-×  n    n×n   , it terminates in    2  n      2  n    2n   steps, 2 although like all quasi-Newton methods, it may not converge for nonlinear systems.  Description of the method  Solving single variable equation  In the secant method, we replace the first derivative    f  ′      f  normal-′    f′   at with the finite difference approximation:         f  ′    (   x  n   )    ≃     f   (   x  n   )    -   f   (   x   n  -  1    )       x  n   -   x   n  -  1       ,     similar-to-or-equals     superscript  f  normal-′    subscript  x  n          f   subscript  x  n      f   subscript  x    n  1         subscript  x  n    subscript  x    n  1        f^{\prime}(x_{n})\simeq\frac{f(x_{n})-f(x_{n-1})}{x_{n}-x_{n-1}},     and proceed similar to Newton's Method :       x   n  +  1    =    x  n   -    1    f  ′    (   x  n   )     f   (   x  n   )          subscript  x    n  1       subscript  x  n       1     superscript  f  normal-′    subscript  x  n     f   subscript  x  n       x_{n+1}=x_{n}-\frac{1}{f^{\prime}(x_{n})}f(x_{n})     where   n   n   n   is the iteration index.  Solving a system of nonlinear equations  To solve a system of   k   k   k   nonlinear equations        𝐟   (  𝐱  )    =  𝟎   ,        𝐟  𝐱   0    \mathbf{f}(\mathbf{x})=\mathbf{0},     where   𝐟   𝐟   \mathbf{f}   is a vector-valued function of vector   𝐱   𝐱   \mathbf{x}   :      𝐱  =   (   x  1   ,   x  2   ,   x  3   ,  …  ,   x  k   )       𝐱    subscript  x  1    subscript  x  2    subscript  x  3   normal-…   subscript  x  k      \mathbf{x}=(x_{1},x_{2},x_{3},\ldots,x_{k})          𝐟   (  𝐱  )    =   (    f  1    (   x  1   ,   x  2   ,  …  ,   x  k   )    ,    f  2    (   x  1   ,   x  2   ,  …  ,   x  k   )    ,  …  ,    f  k    (   x  1   ,   x  2   ,  …  ,   x  k   )    )         𝐟  𝐱       subscript  f  1     subscript  x  1    subscript  x  2   normal-…   subscript  x  k        subscript  f  2     subscript  x  1    subscript  x  2   normal-…   subscript  x  k     normal-…     subscript  f  k     subscript  x  1    subscript  x  2   normal-…   subscript  x  k        \mathbf{f}(\mathbf{x})=(f_{1}(x_{1},x_{2},\ldots,x_{k}),f_{2}(x_{1},x_{2},%
 \ldots,x_{k}),\ldots,f_{k}(x_{1},x_{2},\ldots,x_{k}))     For such problems, Broyden gives a generalization of the one-dimensional Newton's method, replacing the derivative with the Jacobian    𝐉   𝐉   \mathbf{J}   . The Jacobian matrix is determined iteratively based on the secant equation in the finite difference approximation:         𝐉  n    (    𝐱  n   -   𝐱   n  -  1     )    ≃    𝐟   (   𝐱  n   )    -   𝐟   (   𝐱   n  -  1    )      ,     similar-to-or-equals     subscript  𝐉  n      subscript  𝐱  n    subscript  𝐱    n  1          𝐟   subscript  𝐱  n      𝐟   subscript  𝐱    n  1        \mathbf{J}_{n}(\mathbf{x}_{n}-\mathbf{x}_{n-1})\simeq\mathbf{f}(\mathbf{x}_{n}%
 )-\mathbf{f}(\mathbf{x}_{n-1}),     where   n   n   n   is the iteration index. For clarity, let us define:        𝐟  n   =   𝐟   (   𝐱  n   )     ,       subscript  𝐟  n     𝐟   subscript  𝐱  n      \mathbf{f}_{n}=\mathbf{f}(\mathbf{x}_{n}),           Δ   𝐱  n    =    𝐱  n   -   𝐱   n  -  1      ,        normal-Δ   subscript  𝐱  n       subscript  𝐱  n    subscript  𝐱    n  1       \Delta\mathbf{x}_{n}=\mathbf{x}_{n}-\mathbf{x}_{n-1},           Δ   𝐟  n    =    𝐟  n   -   𝐟   n  -  1      ,        normal-Δ   subscript  𝐟  n       subscript  𝐟  n    subscript  𝐟    n  1       \Delta\mathbf{f}_{n}=\mathbf{f}_{n}-\mathbf{f}_{n-1},     so the above may be rewritten as:         𝐉  n   Δ   𝐱  n    ≃   Δ   𝐟  n     .     similar-to-or-equals     subscript  𝐉  n   normal-Δ   subscript  𝐱  n      normal-Δ   subscript  𝐟  n      \mathbf{J}_{n}\Delta\mathbf{x}_{n}\simeq\Delta\mathbf{f}_{n}.     Unfortunately, the above equation is underdetermined when   k   k   k   is greater than one. Broyden suggests using the current estimate of the Jacobian matrix and improving upon it by taking the solution to the secant equation that is a minimal modification to :       𝐉  n   =    𝐉   n  -  1    +      Δ   𝐟  n    -    𝐉   n  -  1    Δ   𝐱  n       ∥   Δ   𝐱  n    ∥   2    Δ   𝐱  n  T          subscript  𝐉  n      subscript  𝐉    n  1            normal-Δ   subscript  𝐟  n       subscript  𝐉    n  1    normal-Δ   subscript  𝐱  n      superscript   norm    normal-Δ   subscript  𝐱  n     2    normal-Δ   superscript   subscript  𝐱  n   normal-T       \mathbf{J}_{n}=\mathbf{J}_{n-1}+\frac{\Delta\mathbf{f}_{n}-\mathbf{J}_{n-1}%
 \Delta\mathbf{x}_{n}}{\|\Delta\mathbf{x}_{n}\|^{2}}\Delta\mathbf{x}_{n}^{%
 \mathrm{T}}     This minimizes the following Frobenius norm :        ∥    𝐉  n   -   𝐉   n  -  1     ∥   f   .     subscript   norm     subscript  𝐉  n    subscript  𝐉    n  1      normal-f    \|\mathbf{J}_{n}-\mathbf{J}_{n-1}\|_{\mathrm{f}}.     We may then proceed in the Newton direction:        𝐱   n  +  1    =    𝐱  n   -    𝐉  n   -  1    𝐟   (   𝐱  n   )      .       subscript  𝐱    n  1       subscript  𝐱  n      superscript   subscript  𝐉  n     1    𝐟   subscript  𝐱  n       \mathbf{x}_{n+1}=\mathbf{x}_{n}-\mathbf{J}_{n}^{-1}\mathbf{f}(\mathbf{x}_{n}).     Broyden also suggested using the Sherman-Morrison formula to update directly the inverse of the Jacobian matrix:       𝐉  n   -  1    =    𝐉   n  -  1    -  1    +      Δ   𝐱  n    -    𝐉   n  -  1    -  1    Δ   𝐟  n      Δ   𝐱  n  T    𝐉   n  -  1    -  1    Δ   𝐟  n     Δ   𝐱  n  T    𝐉   n  -  1    -  1           superscript   subscript  𝐉  n     1       superscript   subscript  𝐉    n  1      1            normal-Δ   subscript  𝐱  n       subscript   superscript  𝐉    1      n  1    normal-Δ   subscript  𝐟  n       normal-Δ   superscript   subscript  𝐱  n   normal-T    subscript   superscript  𝐉    1      n  1    normal-Δ   subscript  𝐟  n     normal-Δ   superscript   subscript  𝐱  n   normal-T    subscript   superscript  𝐉    1      n  1        \mathbf{J}_{n}^{-1}=\mathbf{J}_{n-1}^{-1}+\frac{\Delta\mathbf{x}_{n}-\mathbf{J%
 }^{-1}_{n-1}\Delta\mathbf{f}_{n}}{\Delta\mathbf{x}_{n}^{\mathrm{T}}\mathbf{J}^%
 {-1}_{n-1}\Delta\mathbf{f}_{n}}\Delta\mathbf{x}_{n}^{\mathrm{T}}\mathbf{J}^{-1%
 }_{n-1}     This first method is commonly known as the "good Broyden's method".  A similar technique can be derived by using a slightly different modification to . This yields a second method, the so-called "bad Broyden's method" (but see 3 ):       𝐉  n   -  1    =    𝐉   n  -  1    -  1    +      Δ   𝐱  n    -    𝐉   n  -  1    -  1    Δ   𝐟  n       ∥   Δ   𝐟  n    ∥   2    Δ   𝐟  n  T          superscript   subscript  𝐉  n     1       superscript   subscript  𝐉    n  1      1            normal-Δ   subscript  𝐱  n       subscript   superscript  𝐉    1      n  1    normal-Δ   subscript  𝐟  n      superscript   norm    normal-Δ   subscript  𝐟  n     2    normal-Δ   superscript   subscript  𝐟  n   normal-T       \mathbf{J}_{n}^{-1}=\mathbf{J}_{n-1}^{-1}+\frac{\Delta\mathbf{x}_{n}-\mathbf{J%
 }^{-1}_{n-1}\Delta\mathbf{f}_{n}}{\|\Delta\mathbf{f}_{n}\|^{2}}\Delta\mathbf{f%
 }_{n}^{\mathrm{T}}     This minimizes a different Frobenius norm:        ∥    𝐉  n   -  1    -   𝐉   n  -  1    -  1     ∥   f   .     subscript   norm     superscript   subscript  𝐉  n     1     superscript   subscript  𝐉    n  1      1      normal-f    \|\mathbf{J}_{n}^{-1}-\mathbf{J}_{n-1}^{-1}\|_{\mathrm{f}}.     Many other quasi-Newton schemes have been suggested in optimization , where one seeks a maximum or minimum by finding the root of the first derivative ( gradient in multi dimensions). The Jacobian of the gradient is called Hessian and is symmetric, adding further constraints to its update.  See also   Secant method  Newton's method  Quasi-Newton method  Newton's method in optimization  Davidon-Fletcher-Powell formula  Broyden-Fletcher-Goldfarb-Shanno (BFGS) method   References    External links   Module for Broyden's Method by John H. Mathews   "  Category:Root-finding algorithms     ↩  ↩  ↩     