


Broyden's method




Broyden's method

In numerical analysis, Broyden's method is a quasi-Newton method for finding roots in 
 
 
 
  variables. It was originally described by C. G. Broyden in 1965.1
Newton's method for solving 
 
 
 
  uses the Jacobian matrix, 
 
 
 
 , at every iteration. However, computing this Jacobian is a difficult and expensive operation. The idea behind Broyden's method is to compute the whole Jacobian only at the first iteration, and to do a rank-one update at the other iterations.
In 1979 Gay proved that when Broyden's method is applied to a linear system of size 
 
 
 
 , it terminates in 
 
 
 
  steps,2 although like all quasi-Newton methods, it may not converge for nonlinear systems.
Description of the method
Solving single variable equation
In the secant method, we replace the first derivative 
 
 
 
  at  with the finite difference approximation:



and proceed similar to Newton's Method:



where 
 
 
 
  is the iteration index.
 Solving a system of nonlinear equations
To solve a system of 
 
 
 
  nonlinear equations



where 
 
 
 
  is a vector-valued function of vector 
 
 
 
 :






For such problems, Broyden gives a generalization of the one-dimensional Newton's method, replacing the derivative with the Jacobian

 
 . The Jacobian matrix is determined iteratively based on the secant equation in the finite difference approximation:



where 
 
 
 
  is the iteration index. For clarity, let us define:









so the above may be rewritten as:



Unfortunately, the above equation is underdetermined when 
 
 
 
  is greater than one. Broyden suggests using the current estimate of the Jacobian matrix  and improving upon it by taking the solution to the secant equation that is a minimal modification to :



This minimizes the following Frobenius norm:



We may then proceed in the Newton direction:



Broyden also suggested using the Sherman-Morrison formula to update directly the inverse of the Jacobian matrix:



This first method is commonly known as the "good Broyden's method".
A similar technique can be derived by using a slightly different modification to . This yields a second method, the so-called "bad Broyden's method" (but see3):



This minimizes a different Frobenius norm:



Many other quasi-Newton schemes have been suggested in optimization, where one seeks a maximum or minimum by finding the root of the first derivative (gradient in multi dimensions). The Jacobian of the gradient is called Hessian and is symmetric, adding further constraints to its update.
See also

Secant method
Newton's method
Quasi-Newton method
Newton's method in optimization
Davidon-Fletcher-Powell formula
Broyden-Fletcher-Goldfarb-Shanno (BFGS) method

References


External links

Module for Broyden's Method by John H. Mathews

"
Category:Root-finding algorithms










