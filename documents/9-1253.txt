   Broyden's method      Broyden's method   In numerical analysis, Broyden's method is a quasi-Newton method for finding roots in   k   k   k   variables. It was originally described by C. G. Broyden in 1965. 1  Newton's method for solving     ğŸ   (  ğ±  )    =  ğŸ        ğŸ  ğ±   0    \mathbf{f}(\mathbf{x})=\mathbf{0}   uses the Jacobian matrix ,   ğ‰   ğ‰   \mathbf{J}   , at every iteration. However, computing this Jacobian is a difficult and expensive operation. The idea behind Broyden's method is to compute the whole Jacobian only at the first iteration, and to do a rank-one update at the other iterations.  In 1979 Gay proved that when Broyden's method is applied to a linear system of size    n  Ã—  n      n  normal-Ã—  n    nÃ—n   , it terminates in    2  n      2  n    2n   steps, 2 although like all quasi-Newton methods, it may not converge for nonlinear systems.  Description of the method  Solving single variable equation  In the secant method, we replace the first derivative    f  â€²      f  normal-â€²    fâ€²   at with the finite difference approximation:         f  â€²    (   x  n   )    â‰ƒ     f   (   x  n   )    -   f   (   x   n  -  1    )       x  n   -   x   n  -  1       ,     similar-to-or-equals     superscript  f  normal-â€²    subscript  x  n          f   subscript  x  n      f   subscript  x    n  1         subscript  x  n    subscript  x    n  1        f^{\prime}(x_{n})\simeq\frac{f(x_{n})-f(x_{n-1})}{x_{n}-x_{n-1}},     and proceed similar to Newton's Method :       x   n  +  1    =    x  n   -    1    f  â€²    (   x  n   )     f   (   x  n   )          subscript  x    n  1       subscript  x  n       1     superscript  f  normal-â€²    subscript  x  n     f   subscript  x  n       x_{n+1}=x_{n}-\frac{1}{f^{\prime}(x_{n})}f(x_{n})     where   n   n   n   is the iteration index.  Solving a system of nonlinear equations  To solve a system of   k   k   k   nonlinear equations        ğŸ   (  ğ±  )    =  ğŸ   ,        ğŸ  ğ±   0    \mathbf{f}(\mathbf{x})=\mathbf{0},     where   ğŸ   ğŸ   \mathbf{f}   is a vector-valued function of vector   ğ±   ğ±   \mathbf{x}   :      ğ±  =   (   x  1   ,   x  2   ,   x  3   ,  â€¦  ,   x  k   )       ğ±    subscript  x  1    subscript  x  2    subscript  x  3   normal-â€¦   subscript  x  k      \mathbf{x}=(x_{1},x_{2},x_{3},\ldots,x_{k})          ğŸ   (  ğ±  )    =   (    f  1    (   x  1   ,   x  2   ,  â€¦  ,   x  k   )    ,    f  2    (   x  1   ,   x  2   ,  â€¦  ,   x  k   )    ,  â€¦  ,    f  k    (   x  1   ,   x  2   ,  â€¦  ,   x  k   )    )         ğŸ  ğ±       subscript  f  1     subscript  x  1    subscript  x  2   normal-â€¦   subscript  x  k        subscript  f  2     subscript  x  1    subscript  x  2   normal-â€¦   subscript  x  k     normal-â€¦     subscript  f  k     subscript  x  1    subscript  x  2   normal-â€¦   subscript  x  k        \mathbf{f}(\mathbf{x})=(f_{1}(x_{1},x_{2},\ldots,x_{k}),f_{2}(x_{1},x_{2},%
 \ldots,x_{k}),\ldots,f_{k}(x_{1},x_{2},\ldots,x_{k}))     For such problems, Broyden gives a generalization of the one-dimensional Newton's method, replacing the derivative with the Jacobian    ğ‰   ğ‰   \mathbf{J}   . The Jacobian matrix is determined iteratively based on the secant equation in the finite difference approximation:         ğ‰  n    (    ğ±  n   -   ğ±   n  -  1     )    â‰ƒ    ğŸ   (   ğ±  n   )    -   ğŸ   (   ğ±   n  -  1    )      ,     similar-to-or-equals     subscript  ğ‰  n      subscript  ğ±  n    subscript  ğ±    n  1          ğŸ   subscript  ğ±  n      ğŸ   subscript  ğ±    n  1        \mathbf{J}_{n}(\mathbf{x}_{n}-\mathbf{x}_{n-1})\simeq\mathbf{f}(\mathbf{x}_{n}%
 )-\mathbf{f}(\mathbf{x}_{n-1}),     where   n   n   n   is the iteration index. For clarity, let us define:        ğŸ  n   =   ğŸ   (   ğ±  n   )     ,       subscript  ğŸ  n     ğŸ   subscript  ğ±  n      \mathbf{f}_{n}=\mathbf{f}(\mathbf{x}_{n}),           Î”   ğ±  n    =    ğ±  n   -   ğ±   n  -  1      ,        normal-Î”   subscript  ğ±  n       subscript  ğ±  n    subscript  ğ±    n  1       \Delta\mathbf{x}_{n}=\mathbf{x}_{n}-\mathbf{x}_{n-1},           Î”   ğŸ  n    =    ğŸ  n   -   ğŸ   n  -  1      ,        normal-Î”   subscript  ğŸ  n       subscript  ğŸ  n    subscript  ğŸ    n  1       \Delta\mathbf{f}_{n}=\mathbf{f}_{n}-\mathbf{f}_{n-1},     so the above may be rewritten as:         ğ‰  n   Î”   ğ±  n    â‰ƒ   Î”   ğŸ  n     .     similar-to-or-equals     subscript  ğ‰  n   normal-Î”   subscript  ğ±  n      normal-Î”   subscript  ğŸ  n      \mathbf{J}_{n}\Delta\mathbf{x}_{n}\simeq\Delta\mathbf{f}_{n}.     Unfortunately, the above equation is underdetermined when   k   k   k   is greater than one. Broyden suggests using the current estimate of the Jacobian matrix and improving upon it by taking the solution to the secant equation that is a minimal modification to :       ğ‰  n   =    ğ‰   n  -  1    +      Î”   ğŸ  n    -    ğ‰   n  -  1    Î”   ğ±  n       âˆ¥   Î”   ğ±  n    âˆ¥   2    Î”   ğ±  n  T          subscript  ğ‰  n      subscript  ğ‰    n  1            normal-Î”   subscript  ğŸ  n       subscript  ğ‰    n  1    normal-Î”   subscript  ğ±  n      superscript   norm    normal-Î”   subscript  ğ±  n     2    normal-Î”   superscript   subscript  ğ±  n   normal-T       \mathbf{J}_{n}=\mathbf{J}_{n-1}+\frac{\Delta\mathbf{f}_{n}-\mathbf{J}_{n-1}%
 \Delta\mathbf{x}_{n}}{\|\Delta\mathbf{x}_{n}\|^{2}}\Delta\mathbf{x}_{n}^{%
 \mathrm{T}}     This minimizes the following Frobenius norm :        âˆ¥    ğ‰  n   -   ğ‰   n  -  1     âˆ¥   f   .     subscript   norm     subscript  ğ‰  n    subscript  ğ‰    n  1      normal-f    \|\mathbf{J}_{n}-\mathbf{J}_{n-1}\|_{\mathrm{f}}.     We may then proceed in the Newton direction:        ğ±   n  +  1    =    ğ±  n   -    ğ‰  n   -  1    ğŸ   (   ğ±  n   )      .       subscript  ğ±    n  1       subscript  ğ±  n      superscript   subscript  ğ‰  n     1    ğŸ   subscript  ğ±  n       \mathbf{x}_{n+1}=\mathbf{x}_{n}-\mathbf{J}_{n}^{-1}\mathbf{f}(\mathbf{x}_{n}).     Broyden also suggested using the Sherman-Morrison formula to update directly the inverse of the Jacobian matrix:       ğ‰  n   -  1    =    ğ‰   n  -  1    -  1    +      Î”   ğ±  n    -    ğ‰   n  -  1    -  1    Î”   ğŸ  n      Î”   ğ±  n  T    ğ‰   n  -  1    -  1    Î”   ğŸ  n     Î”   ğ±  n  T    ğ‰   n  -  1    -  1           superscript   subscript  ğ‰  n     1       superscript   subscript  ğ‰    n  1      1            normal-Î”   subscript  ğ±  n       subscript   superscript  ğ‰    1      n  1    normal-Î”   subscript  ğŸ  n       normal-Î”   superscript   subscript  ğ±  n   normal-T    subscript   superscript  ğ‰    1      n  1    normal-Î”   subscript  ğŸ  n     normal-Î”   superscript   subscript  ğ±  n   normal-T    subscript   superscript  ğ‰    1      n  1        \mathbf{J}_{n}^{-1}=\mathbf{J}_{n-1}^{-1}+\frac{\Delta\mathbf{x}_{n}-\mathbf{J%
 }^{-1}_{n-1}\Delta\mathbf{f}_{n}}{\Delta\mathbf{x}_{n}^{\mathrm{T}}\mathbf{J}^%
 {-1}_{n-1}\Delta\mathbf{f}_{n}}\Delta\mathbf{x}_{n}^{\mathrm{T}}\mathbf{J}^{-1%
 }_{n-1}     This first method is commonly known as the "good Broyden's method".  A similar technique can be derived by using a slightly different modification to . This yields a second method, the so-called "bad Broyden's method" (but see 3 ):       ğ‰  n   -  1    =    ğ‰   n  -  1    -  1    +      Î”   ğ±  n    -    ğ‰   n  -  1    -  1    Î”   ğŸ  n       âˆ¥   Î”   ğŸ  n    âˆ¥   2    Î”   ğŸ  n  T          superscript   subscript  ğ‰  n     1       superscript   subscript  ğ‰    n  1      1            normal-Î”   subscript  ğ±  n       subscript   superscript  ğ‰    1      n  1    normal-Î”   subscript  ğŸ  n      superscript   norm    normal-Î”   subscript  ğŸ  n     2    normal-Î”   superscript   subscript  ğŸ  n   normal-T       \mathbf{J}_{n}^{-1}=\mathbf{J}_{n-1}^{-1}+\frac{\Delta\mathbf{x}_{n}-\mathbf{J%
 }^{-1}_{n-1}\Delta\mathbf{f}_{n}}{\|\Delta\mathbf{f}_{n}\|^{2}}\Delta\mathbf{f%
 }_{n}^{\mathrm{T}}     This minimizes a different Frobenius norm:        âˆ¥    ğ‰  n   -  1    -   ğ‰   n  -  1    -  1     âˆ¥   f   .     subscript   norm     superscript   subscript  ğ‰  n     1     superscript   subscript  ğ‰    n  1      1      normal-f    \|\mathbf{J}_{n}^{-1}-\mathbf{J}_{n-1}^{-1}\|_{\mathrm{f}}.     Many other quasi-Newton schemes have been suggested in optimization , where one seeks a maximum or minimum by finding the root of the first derivative ( gradient in multi dimensions). The Jacobian of the gradient is called Hessian and is symmetric, adding further constraints to its update.  See also   Secant method  Newton's method  Quasi-Newton method  Newton's method in optimization  Davidon-Fletcher-Powell formula  Broyden-Fletcher-Goldfarb-Shanno (BFGS) method   References    External links   Module for Broyden's Method by John H. Mathews   "  Category:Root-finding algorithms     â†©  â†©  â†©     