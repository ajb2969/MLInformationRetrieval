   Kruskal–Wallis one-way analysis of variance      Kruskal–Wallis one-way analysis of variance  The '''Kruskal–Wallis one-way analysis of variance''' by ranks (named after [[William Kruskal]] and [[W. Allen Wallis]]) is a [[non-parametric statistics|non-parametric]] method for testing whether samples originate from the same distribution. {{cite journal |last=Kruskal |last2=Wallis |year=1952 |title=Use of ranks in one-criterion variance analysis |journal=[[Journal of the American Statistical Association]] |volume=47 |issue=260 |pages=583–621 |doi=10.1080/01621459.1952.10483441 }} {{cite book |last=Corder |first=Gregory W. |first2=Dale I. |last2=Foreman |year=2009 |title=Nonparametric Statistics for Non-Statisticians |location=Hoboken |publisher=John Wiley & Sons |pages=99–105 |isbn=9780470454619 }} {{cite book |last=Siegel |last2=Castellan |y ear=1988 |title=Nonparametric Statistics for the Behavioral Sciences |edition=Second |location=New York |publisher=McGraw–Hill |isbn=0070573573 }} It is used for comparing two or more samples that are independent, and that may have different sample sizes, and extends the Mann–Whitney U test to more than two groups. The parametric equivalent of the Kruskal-Wallis test is the one-way analysis of variance (ANOVA). When rejecting the null hypothesis of the Kruskal-Wallis test, then at least one sample stochastically dominates at least one other sample. The test does not identify where this stochastic dominance occurs or for how many pairs of groups stochastic dominance obtains. Dunn's test 1 would help analyze the specific sample pairs for stochastic dominance.  Since it is a non-parametric method, the Kruskal–Wallis test does not assume a normal distribution of the residuals, unlike the analogous one-way analysis of variance. If the researcher can make the more stringent assumptions of an identically shaped and scaled distribution for all groups, except for any difference in medians, then the null hypothesis is that the medians of all groups are equal, and the alternative hypothesis is that at least one population median of one group is different from the population median of at least one other group.  Method   Rank all data from all groups together; i.e., rank the data from 1 to N ignoring group membership. Assign any tied values the average of the ranks they would have received had they not been tied.  The test statistic is given by:      K  =    (   N  -  1   )      ∑   i  =  1   g     n  i     (     r  ¯    i  ⋅    -   r  ¯    )   2       ∑   i  =  1   g     ∑   j  =  1    n  i      (    r   i  j    -   r  ¯    )   2        ,      K      N  1       superscript   subscript     i  1    g      subscript  n  i    superscript     subscript   normal-¯  r    i  normal-⋅     normal-¯  r    2       superscript   subscript     i  1    g     superscript   subscript     j  1     subscript  n  i     superscript     subscript  r    i  j     normal-¯  r    2         K=(N-1)\frac{\sum_{i=1}^{g}n_{i}(\bar{r}_{i\cdot}-\bar{r})^{2}}{\sum_{i=1}^{g}%
 \sum_{j=1}^{n_{i}}(r_{ij}-\bar{r})^{2}},   where:       n  i     subscript  n  i    n_{i}   is the number of observations in group   i   i   i         r   i  j      subscript  r    i  j     r_{ij}   is the rank (among all observations) of observation   j   j   j   from group   i   i   i        N   N   N   is the total number of observations across all groups        r  ¯    i  ⋅    =      ∑   j  =  1    n  i      r   i  j      n  i         subscript   normal-¯  r    i  normal-⋅        superscript   subscript     j  1     subscript  n  i     subscript  r    i  j      subscript  n  i      \bar{r}_{i\cdot}=\frac{\sum_{j=1}^{n_{i}}{r_{ij}}}{n_{i}}   ,       r  ¯   =    1  2    (   N  +  1   )         normal-¯  r       1  2     N  1      \bar{r}=\tfrac{1}{2}(N+1)   is the average of all the    r   i  j      subscript  r    i  j     r_{ij}   .   If the data contain no ties the denominator of the expression for   K   K   K   is exactly      (   N  -  1   )   N   (   N  +  1   )    /  12          N  1   N    N  1    12    (N-1)N(N+1)/12   and     r  ¯   =    N  +  1   2        normal-¯  r       N  1   2     \bar{r}=\tfrac{N+1}{2}   . Thus :   \begin{align} K & = \frac{12}{N(N+1)}\sum_{i=1}^g n_i \left(\bar{r}_{i\cdot} - \frac{N+1}{2}\right)^2 \\ & = \frac{12}{N(N+1)}\sum_{i=1}^g n_i \bar{r}_{i\cdot }^2 -\ 3(N+1). \end{align} The last formula only contains the squares of the average ranks.   A correction for ties if using the short-cut formula described in the previous point can be made by dividing   K   K   K   by    1  -      ∑   i  =  1   G     (    t  i  3   -   t  i    )      N  3   -  N        1      superscript   subscript     i  1    G      superscript   subscript  t  i   3    subscript  t  i        superscript  N  3   N      1-\frac{\sum_{i=1}^{G}(t_{i}^{3}-t_{i})}{N^{3}-N}   , where G is the number of groupings of different tied ranks, and t i is the number of tied values within group i that are tied at a particular value. This correction usually makes little difference in the value of K unless there are a large number of ties.  Finally, the p-value is approximated by    Pr   (    χ   g  -  1   2   ≥  K   )      Pr     subscript   superscript  χ  2     g  1    K     \Pr(\chi^{2}_{g-1}\geq K)   . If some    n  i     subscript  n  i    n_{i}   values are small (i.e., less than 5) the probability distribution of K can be quite different from this chi-squared distribution. If a table of the chi-squared probability distribution is available, the critical value of chi-squared,    χ   α  :   g  -  1    2     subscript   superscript  χ  2    normal-:  α    g  1      \chi^{2}_{\alpha:g-1}   , can be found by entering the table at g − 1 degrees of freedom and looking under the desired significance or alpha level.  If the statistic is not significant, then there is no evidence of stochastic dominance between the samples. However, if the test is significant then at least one sample stochastically dominates another sample. Therefore, a researcher might use sample contrasts between individual sample pairs, or post hoc tests using Dunn's test, which (1) properly employs the same rankings as the Kruskal-Wallis test, and (2) properly employs the pooled variance implied by the null hypothesis of the Kruskal-Wallis test in order to determine which of the sample pairs are significantly different. 2 When performing multiple sample contrasts or tests, the Type I error rate tends to become inflated, raising concerns about multiple comparisons .   Exact probability tables  A large amount of computing resources is required to compute exact probabilities for the Kruskal-Wallis test. Existing software only provides exact probabilities for sample sizes less than about 30 participants. These software programs rely on asymptotic approximation for larger sample sizes. Exact probability values for larger sample sizes are available. Spurrier (2003) published exact probability tables for samples as large as 45 participants. 3 Meyer and Seaman (2006) produced exact probability distributions for samples as large as 105 participants. 4  See also   one-way analysis of variance  Mann–Whitney U  Friedman test   References  External links   An online version of the test   "  Category:Statistical tests  Category:Analysis of variance  Category:Non-parametric statistics     ↩   ↩  Critical value tables and exact probabilities from Meyer and Seaman are available for download at http://faculty.virginia.edu/kruskal-wallis/ . A paper describing their work may also be found there. ↩     