


Learning vector quantization




Learning vector quantization

In computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm. LVQ is the supervised counterpart of vector quantization systems.
Overview
LVQ can be understood as a special case of an artificial neural network, more precisely, it applies a winner-take-all Hebbian learning-based approach. It is a precursor to Self-organizing maps (SOM) and related to Neural gas, and to the k-Nearest Neighbor algorithm (k-NN). LVQ was invented by Teuvo Kohonen.1
An LVQ system is represented by prototypes W=(w(i),...,w(n)) which are defined in the feature space of observed data. In winner-take-all training algorithms one determines, for each data point, the prototype which is closest to the input according to a given distance measure. The position of this so-called winner prototype is then adapted, i.e. the winner is moved closer if it correctly classifies the data point or moved away if it classifies the data point incorrectly.
An advantage of LVQ is that it creates prototypes that are easy to interpret for experts in the respective application domain.2 LVQ systems can be applied to multi-class classification problems in a natural way. It is used in a variety of practical applications, see http://liinwww.ira.uka.de/bibliography/Neural/SOM.LVQ.html for an extensive bibliography.
A key issue in LVQ is the choice of an appropriate measure of distance or similarity for training and classification. Recently, techniques have been developed which adapt a parameterized distance measure in the course of training the system, see e.g. (Schneider, Biehl, and Hammer, 2009) 3 and references therein.
LVQ can be a source of great help in classifying text documents.4
Algorithm
Below follows an informal description.
 The algorithm is consisted by 3 basic steps. The algorithm's input is:

how many neurons the system will have 
 
 

what weight each neuron has 
 
 
 
  for 
 
 

how fast the neurons are learning 
 
 
 
 .
and an input list containing vectors to train the neurons 
 
 


The algorithm's flow is:

For next input 
 
 
 
  in 
 
 
 
  find the neuron 
 
 
 
  at which 
 
 
 
  gets its minimum value, where 
 
 
 
  is the metric used ( Euclidean, etc. ).
Update 
 
 
 
 . A better explanation is get 
 
 
 
  closer to the input 
 
 
 
 .

 
 .
While there are vectors left in 
 
 
 
  go to step 1, else terminate.

Note
 
 
 
  and 
 
 
 
  are vectors in feature space.
 A more formal description can be found here: http://jsalatas.ictpro.gr/implementation-of-competitive-learning-networks-for-weka/
References



Self-Organizing Maps and Learning Vector Quantization for Feature Sequences, Somervuo and Kohonen. 2004 (pdf)

External links

LVQ for WEKA: Implementation of LVQ variants (LVQ1, OLVQ1, LVQ2.1, LVQ3, OLVQ3) for the WEKA Machine Learning Workbench.
lvq_pak official release (1996) by Kohonen and his team
LVQ for WEKA: Another implementation of LVQ in Java for the WEKA Machine Learning Workbench.

"
Category:Artificial neural networks Category:Classification algorithms



T. Kohonen. Self-Organizing Maps. Springer, Berlin, 1997.↩
T. Kohonen. Learning vector quantization. In: M.A. Arbib, editor, The Handbook of Brain Theory and Neural Networks., pages 537–540. MIT Press, Cambridge, MA, 1995.↩
P. Schneider, B. Hammer, and M. Biehl. Adaptive Relevance Matrices in Learning Vector Quantization.Neural Computation 21: 3532–3561, 2009. http://www.mitpressjournals.org/doi/abs/10.1162/neco.2009.10-08-892↩
Fahad and Sikander. Classification of textual documents using learning vector quantization. Information Technology Journal 6.1 (2007): 154-159. http://198.170.104.138/itj/2007/154-159.pdf↩




