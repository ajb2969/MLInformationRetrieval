   Aitken's delta-squared process      Aitken's delta-squared process  table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
   margin: 0; padding: 0; vertical-align: baseline; border: none; }
 <style>
 table.sourceCode { width: 100%; line-height: 100%; }
 td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
 td.sourceCode { padding-left: 5px; }
 code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
 code > span.dt { color: #902000; } /* DataType */
 code > span.dv { color: #40a070; } /* DecVal */
 code > span.bn { color: #40a070; } /* BaseN */
 code > span.fl { color: #40a070; } /* Float */
 code > span.ch { color: #4070a0; } /* Char */
 code > span.st { color: #4070a0; } /* String */
 code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
 code > span.ot { color: #007020; } /* Other */
 code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
 code > span.fu { color: #06287e; } /* Function */
 code > span.er { color: #ff0000; font-weight: bold; } /* Error */
 code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 code > span.cn { color: #880000; } /* Constant */
 code > span.sc { color: #4070a0; } /* SpecialChar */
 code > span.vs { color: #4070a0; } /* VerbatimString */
 code > span.ss { color: #bb6688; } /* SpecialString */
 code > span.im { } /* Import */
 code > span.va { color: #19177c; } /* Variable */
 code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
 code > span.op { color: #666666; } /* Operator */
 code > span.bu { } /* BuiltIn */
 code > span.ex { } /* Extension */
 code > span.pp { color: #bc7a00; } /* Preprocessor */
 code > span.at { color: #7d9029; } /* Attribute */
 code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
 code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
 code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
 code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */     In numerical analysis , Aitken's delta-squared process is a series acceleration method, used for accelerating the rate of convergence of a sequence. It is named after Alexander Aitken , who introduced this method in 1926. 1 Its early form was known to Seki KÅwa (end of 17th century) and was found for rectification of the circle, i.e. the calculation of Ï€. It is most useful for accelerating the convergence of a sequence that is converging linearly.  Definition  Given a sequence    x  =    (   x  n   )    n  âˆˆ  ð’©        x   subscript   subscript  x  n     n  ð’©      x={(x_{n})}_{n\in\mathcal{N}}   , one associates with this sequence the new sequence        A  x   =    (       x   n  +  2      x  n    -    (   x   n  +  1    )   2       x   n  +  2    -    2    x   n  +  1      +   x  n     )    n  âˆˆ    \Z   *      ,        A  x    subscript         subscript  x    n  2     subscript  x  n     superscript   subscript  x    n  1    2         subscript  x    n  2      2   subscript  x    n  1       subscript  x  n       n   superscript  \Z        Ax={\left(\frac{x_{n+2}\,x_{n}-(x_{n+1})^{2}}{x_{n+2}-2\,x_{n+1}+x_{n}}\right)%
 }_{n\in\Z^{*}},     which can, with improved numerical stability , also be written as         (   A  x   )   n   =    x  n   -     (   Î”   x  n    )   2     Î”  2    x  n       ,       subscript    A  x   n      subscript  x  n      superscript    normal-Î”   subscript  x  n    2      superscript  normal-Î”  2    subscript  x  n        (Ax)_{n}=x_{n}-\frac{(\Delta x_{n})^{2}}{\Delta^{2}x_{n}},   or equivalently     =    x   n  +  2    -     (   Î”   x   n  +  1     )   2     Î”  2    x  n          absent     subscript  x    n  2       superscript    normal-Î”   subscript  x    n  1     2      superscript  normal-Î”  2    subscript  x  n        =x_{n+2}-\frac{(\Delta x_{n+1})^{2}}{\Delta^{2}x_{n}}     where         Î”   x  n    =   (    x   n  +  1    -   x  n    )    ,    Î”   x   n  +  1     =   (    x   n  +  2    -   x   n  +  1     )     ,     formulae-sequence      normal-Î”   subscript  x  n       subscript  x    n  1     subscript  x  n         normal-Î”   subscript  x    n  1        subscript  x    n  2     subscript  x    n  1        \Delta x_{n}={(x_{n+1}-x_{n})},\ \Delta x_{n+1}={(x_{n+2}-x_{n+1})},     and         Î”  2    x  n    =     x  n   -   2   x   n  +  1      +   x   n  +  2     =    Î”   x   n  +  1     -   Î”   x  n      ,           superscript  normal-Î”  2    subscript  x  n         subscript  x  n     2   subscript  x    n  1       subscript  x    n  2              normal-Î”   subscript  x    n  1       normal-Î”   subscript  x  n        \Delta^{2}x_{n}=x_{n}-2x_{n+1}+x_{n+2}=\Delta x_{n+1}-\Delta x_{n},     for    n  =   0  ,  1  ,  2  ,  3  ,   â€¦        n   0  1  2  3  normal-â€¦     n=0,1,2,3,\dots\,     Obviously, A x is ill-defined if Î” 2 x contains a zero element, or equivalently, if the sequence of first differences has a repeating term. From a theoretical point of view, assuming that this occurs only for a finite number of indices, one could easily agree to consider the sequence A x restricted to indices n>n 0 with a sufficiently large n 0 . From a practical point of view, one does in general rather consider only the first few terms of the sequence, which usually provide the needed precision. Moreover, when numerically computing the sequence, one has to take care to stop the computation when rounding errors become too important in the denominator, where the Î”Â² operation may cancel too many significant digits . (It would be better for numerical calculation to use      Î”   x   n  +  1     -   Î”    x  n      =    (    x   n  +  2    -   x   n  +  1     )   -   (    x   n  +  1    -   x  n    )            normal-Î”   subscript  x    n  1       normal-Î”   subscript  x  n          subscript  x    n  2     subscript  x    n  1        subscript  x    n  1     subscript  x  n       \Delta x_{n+1}-\Delta x_{n}\ =(x_{n+2}-x_{n+1})-(x_{n+1}-x_{n})   rather than      x  n   -   2   x   n  +  1      +   x   n  +  2           subscript  x  n     2   subscript  x    n  1       subscript  x    n  2      x_{n}-2x_{n+1}+x_{n+2}   .)  Properties  Aitken's delta-squared process is a method of acceleration of convergence , and a particular case of a nonlinear sequence transformation .     x   x   x   will converge linearly to   â„“   normal-â„“   \ell   if there exists a number Î¼ âˆˆ (0, 1) such that         lim   n  â†’  âˆž      |    x   n  +  1    -  â„“   |    |    x  n   -  â„“   |     =  Î¼   .        subscript    normal-â†’  n            subscript  x    n  1    normal-â„“         subscript  x  n   normal-â„“      Î¼    \lim_{n\to\infty}\frac{|x_{n+1}-\ell|}{|x_{n}-\ell|}=\mu.     Aitken's method will accelerate the sequence    x  n     subscript  x  n    x_{n}   if      lim   n  â†’  âˆž        (   A  x   )   n   -  â„“     x  n   -  â„“     =  0.        subscript    normal-â†’  n          subscript    A  x   n   normal-â„“      subscript  x  n   normal-â„“     0.    \lim_{n\to\infty}\frac{(Ax)_{n}-\ell}{x_{n}-\ell}=0.      A   A   A   is not a linear operator, but a constant term drops out, viz     A   [   x  -  â„“   ]    =    A  x   -  â„“         A   delimited-[]    x  normal-â„“         A  x   normal-â„“     A[x-\ell]=Ax-\ell   , if   â„“   normal-â„“   \ell   is a constant. This is clear from the expression of    A  x      A  x    Ax   in terms of the finite difference operator   Î”   normal-Î”   \Delta   .  Although the new process does not in general converge quadratically, it can be shown that for a fixed point process, that is, for an iterated function sequence     x   n  +  1    =   f   (   x  n   )         subscript  x    n  1      f   subscript  x  n      x_{n+1}=f(x_{n})   for some function   f   f   f   , converging to a fixed point, the convergence is quadratic. In this case, the technique is known as Steffensen's method .  Empirically, the A -operation eliminates the "most important error term". One can check this by considering a sequence of the form     x  n   =   â„“  +   a  n   +   b  n         subscript  x  n     normal-â„“   superscript  a  n    superscript  b  n      x_{n}=\ell+a^{n}+b^{n}   , where    b  n     superscript  b  n    b^{n}   will then go to the limit like   x   x   x   goes to zero.  One can also show that if   â„“   normal-â„“   \ell   goes to its limit    A  x      A  x    Ax   at a rate strictly greater than 1,    A  x      A  x    Ax   does not have a better rate of convergence. (In practice, one rarely has e.g. quadratic convergence which would mean over 30 resp. 100 correct decimal places after 5 resp. 7 iterations (starting with 1 correct digit); usually no acceleration is needed in that case.)  In practice,   x   x   x   converges much faster to the limit than    A  x      A  x    Ax   does, as demonstrated by the example calculations below. Usually, it is much cheaper to calculate   x   x   x   (involving only calculation of differences, one multiplication and one division) than to calculate many more terms of the sequence     2   â‰ˆ  1.4142136        2   1.4142136    \sqrt{2}\approx 1.4142136   . Care must be taken, however, to avoid introducing errors due to insufficient precision when calculating the differences in the numerator and denominator of the expression.  Example calculations   The value of    a  n     subscript  a  n    a_{n}   can be approximated by assuming an initial value for      a   n  +  1    =     a  n   +   2   a  n     2    .       subscript  a    n  1         subscript  a  n     2   subscript  a  n     2     a_{n+1}=\frac{a_{n}+\frac{2}{a_{n}}}{2}.   and iterating the following:       a  0   =  1   :      normal-:     subscript  a  0   1   absent    a_{0}=1:       Starting with    Ï€  4      Ï€  4    \frac{\pi}{4}         n   x = iterated value   Ax     0   1   1.4285714     1   1.5   1.4141414     2   1.4166667   1.4142136     3   1.4142157   --     4   1.4142136   --      The value of     Ï€  4   =    âˆ‘   n  =  0   âˆž      (   -  1   )   n     2  n   +  1     â‰ˆ  0.785398          Ï€  4     superscript   subscript     n  0         superscript    1   n       2  n   1          0.785398     \frac{\pi}{4}=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{2n+1}\approx 0.785398   may be calculated as an infinite sum:          x   n  +  1    =   f   (   x  n   )         subscript  x    n  1      f   subscript  x  n      x_{n+1}=f(x_{n})            n   term   x = partial sum   Ax     0   1   1   0.79166667     1   âˆ’0.33333333   0.66666667   0.78333333     2   0.2   0.86666667   0.78630952     3   âˆ’0.14285714   0.72380952   0.78492063     4   0.11111111   0.83492063   0.78567821     5   âˆ’9.0909091Ã—10 âˆ’2   0.74401154   0.78522034     6   7.6923077Ã—10 âˆ’2   0.82093462   0.78551795     7   -6.6666667Ã—10 âˆ’2   0.75426795   --     8   5.8823529Ã—10 âˆ’2   0.81309148   --     Example pseudocode for Aitken extrapolation  The following is an example of using the Aitken extrapolation to help find the limit of the sequence    x  0     subscript  x  0    x_{0}   when given    Î±  =   f   (  Î±  )        Î±    f  Î±     \alpha=f(\alpha)   , which we assume to be the fixed point     x   n  +  1    =    1  2    (    x  n   +   2   x  n     )         subscript  x    n  1        1  2      subscript  x  n     2   subscript  x  n        x_{n+1}=\frac{1}{2}(x_{n}+\frac{2}{x_{n}})   . For instance, we could have     x  0   =  1       subscript  x  0   1    x_{0}=1   with    2      2    \sqrt{2}   which has the fixed point     f   (  x  )    =    1  2    (   x  +   2  x    )          f  x       1  2     x    2  x       f(x)=\frac{1}{2}(x+\frac{2}{x})   so that     f  â€²    (  Î±  )        superscript  f  normal-â€²   Î±    f^{\prime}(\alpha)   (see Methods of computing square roots ).  This pseudo code also computes the Aitken approximation to $f'(\alpha)$ . The Aitken extrapolates will be denoted by aitkenX . We must check if during the computation of the extrapolate the denominator becomes too small, which could happen if we already have a large amount of accuracy, since otherwise a large amount of error could be introduced. We denote this small number by epsilon .  %These choices depend on the problem being solved x0 = 1  %The initial value f(x) = ( 1 / 2 )*(x + 2 /x) %The function that find the next element in the sequence tolerance = 10 ^- 10  %10 digit accuracy is desired epsilon = 10 ^- 16  %Don't want to divide by a number smaller than this maxIterations = 20  %Don't allow the iterations to continue indefinitely haveWeFoundSolution = false %Were we able to find the solution to the desired tolerance? not yet. for i = 1 : maxIterations 
     x1 = f(x0)
     x2 = f(x1)
 
     lambda = absoluteValue((x2 - x1)/(x1 - x0)); %OPTIONAL: computes an approximation of |f'(fixedPoint)|, which is denoted by lambda denominator = x2 - 2 *x1 + x0
 
     if(absoluteValue(denominator) < epsilon) %Don't want to divide by too small of a number print( 'WARNING: denominator is too small' )
         break; %Leave the loop end
 
     aitkenX = x2 - ( (x2 - x1)^ 2 )/denominator
     
     if(absoluteValue(aitkenX - x2) < tolerance) %If the result is within tolerance print("The fixed point is", aitkenX)) %Display the result of the Aitken extrapolation haveWeFoundSolution = true
         break; %Done, so leave the loop end
 
     x0 = aitkenX %Update x0 to start again end
 
 if(haveWeFoundSolution == false) %If we weren't able to find a solution to within the desired tolerance print("Warning: Not able to find solution to within the desired tolerance of ", tolerance);
     print("The last computed extrapolate was ", aitkenX)
 end  See also   Rate of convergence  Limit of a sequence  Fixed point iteration  Richardson extrapolation  Sequence transformation  Shanks transformation  Steffensen's method   Notes    References   William H. Press, et al. , Numerical Recipes in C , (1987) Cambridge University Press, ISBN 0-521-43108-5 (See section 5.1 )  Abramowitz and Stegun, Handbook of Mathematical Functions , section 3.9.7  Kendall E. Atkinson, An Introduction to Numerical Analysis , (1989) John Wiley & Sons, Inc, ISBN 0-471-62489-6   "  Category:Numerical analysis     Alexander Aitken, "On Bernoulli's numerical solution of algebraic equations", Proceedings of the Royal Society of Edinburgh (1926) 46 pp. 289â€“305. â†©    