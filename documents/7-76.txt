   Characteristic function (probability theory)      Characteristic function (probability theory)   (Figure)  The characteristic function of a uniform U (‚Äì1,1) random variable. This function is real-valued because it corresponds to a random variable that is symmetric around the origin; however characteristic functions may generally be complex-valued.   In probability theory and statistics , the characteristic function of any real-valued  random variable completely defines its probability distribution . If a random variable admits a probability density function , then the characteristic function is the inverse Fourier transform of the probability density function. Thus it provides the basis of an alternative route to analytical results compared with working directly with probability density functions or cumulative distribution functions . There are particularly simple results for the characteristic functions of distributions defined by the weighted sums of random variables.  In addition to univariate distributions, characteristic functions can be defined for vector or matrix-valued random variables, and can also be extended to more generic cases.  The characteristic function always exists when treated as a function of a real-valued argument, unlike the moment-generating function . There are relations between the behavior of the characteristic function of a distribution and properties of the distribution, such as the existence of moments and the existence of a density function.  Introduction  The characteristic function provides an alternative way for describing a random variable . Similarly to the cumulative distribution function         F  X    (  x  )    =   E   [   ùüè   {  X  ‚â§  x  }    ]     ,         subscript  F  X   x    normal-E   subscript  1   fragments  normal-{  X   x  normal-}       F_{X}(x)=\operatorname{E}\left[\mathbf{1}_{\{X\leq x\}}\right],     ( where 1 { X ‚â§ x } is the indicator function ‚Äî it is equal to 1 when , and zero otherwise), which completely determines behavior and properties of the probability distribution of the random variable X , the characteristic function        œÜ  X    (  t  )    =   E   [   e   i  t  X    ]           subscript  œÜ  X   t    normal-E   superscript  e    i  t  X       \varphi_{X}(t)=\operatorname{E}\left[e^{itX}\right]     also completely determines behavior and properties of the probability distribution of the random variable X . The two approaches are equivalent in the sense that knowing one of the functions it is always possible to find the other, yet they both provide different insight for understanding the features of the random variable. However, in particular cases, there can be differences in whether these functions can be represented as expressions involving simple standard functions.  If a random variable admits a density function , then the characteristic function is its dual , in the sense that each of them is a Fourier transform of the other. If a random variable has a moment-generating function , then the domain of the characteristic function can be extended to the complex plane, and         œÜ  X    (   -   i  t    )    =    M  X    (  t  )     .         subscript  œÜ  X       i  t        subscript  M  X   t     \varphi_{X}(-it)=M_{X}(t).    1  Note however that the characteristic function of a distribution always exists, even when the probability density function or moment-generating function do not.  The characteristic function approach is particularly useful in analysis of linear combinations of independent random variables: a classical proof of the Central Limit Theorem uses characteristic functions and L√©vy's continuity theorem . Another important application is to the theory of the decomposability of random variables.  Definition  For a scalar random variable X the characteristic function is defined as the expected value of e itX , where i is the imaginary unit , and  is the argument of the characteristic function:      {        œÜ  X    :   ùêë  ‚Üí  ùêÇ            œÜ  X    (  t  )    =   E   [   e   i  t  X    ]    =     ‚à´  ùêë       e   i  t  x     d   F  X    (  x  )     =     ‚à´  ùêë      e   i  t  x     f  X    (  x  )   d  x    =     ‚à´  0  1       e   i  t   Q  X    (  p  )      d  p            cases   normal-:   subscript  œÜ  X    normal-‚Üí  ùêë  ùêÇ    otherwise         subscript  œÜ  X   t    normal-E   superscript  e    i  t  X            subscript   ùêë      superscript  e    i  t  x    d   subscript  F  X   x           subscript   ùêë      superscript  e    i  t  x     subscript  f  X   x  d  x           superscript   subscript   0   1      superscript  e    i  t   subscript  Q  X   p    d  p      otherwise    \begin{cases}\varphi_{X}\!:\mathbf{R}\to\mathbf{C}\\
 \varphi_{X}(t)=\operatorname{E}\left[e^{itX}\right]=\int_{\mathbf{R}}e^{itx}\,%
 dF_{X}(x)=\int_{\mathbf{R}}e^{itx}f_{X}(x)\,dx=\int_{0}^{1}e^{itQ_{X}(p)}\,dp%
 \end{cases}     Here F X is the cumulative distribution function of X , and the integral is of the Riemann‚ÄìStieltjes kind. If random variable X has a probability density function  f X , then the characteristic function is its Fourier transform with sign reversal in the complex exponential, 2 3 and the last formula in parentheses is valid. Q X ( p ) is the inverse cumulative distribution function of X also called the quantile function of X . 4  It should be noted though, that this convention for the constants appearing in the definition of the characteristic function differs from the usual convention for the Fourier transform. 5 For example some authors 6 define E e ‚àí2 œÄitX }} , which is essentially a change of parameter. Other notation may be encountered in the literature:    p  ^     normal-^  p    \scriptstyle\hat{p}   as the characteristic function for a probability measure p , or    f  ^     normal-^  f    \scriptstyle\hat{f}   as the characteristic function corresponding to a density f .  Generalizations  The notion of characteristic functions generalizes to multivariate random variables and more complicated random elements . The argument of the characteristic function will always belong to the continuous dual of the space where random variable X takes values. For common cases such definitions are listed below:   If X is a k -dimensional random vector , then for            œÜ  X    (  t  )    =   E   [   exp   (    i     t  T    X   )    ]     ,         subscript  œÜ  X   t    normal-E      i   superscript  t  T   X       \varphi_{X}(t)=\operatorname{E}\left[\exp({i\,t^{T}\!X})\right],         If X is a k√óp -dimensional random matrix , then for            œÜ  X    (  t  )    =   E   [   exp   (    i    tr   (     t  T    X   )     )    ]     ,         subscript  œÜ  X   t    normal-E      i   tr     superscript  t  T   X         \varphi_{X}(t)=\operatorname{E}\left[\exp\left({i\,\operatorname{tr}(t^{T}\!X)%
 }\right)\right],         If X is a complex  random variable , then for 7            œÜ  X    (  t  )    =   E   [   exp   (    i    Re   (    t  ¬Ø   X   )     )    ]     ,         subscript  œÜ  X   t    normal-E      i   Re     normal-¬Ø  t   X         \varphi_{X}(t)=\operatorname{E}\left[\exp({i\,\operatorname{Re}(\overline{t}X)%
 })\right],         If X is a k -dimensional complex  random vector , then for 8            œÜ  X    (  t  )    =   E   [   exp   (    i    Re   (     t  *    X   )     )    ]     ,         subscript  œÜ  X   t    normal-E      i   Re     superscript  t    X         \varphi_{X}(t)=\operatorname{E}\left[\exp({i\,\operatorname{Re}(t^{*}\!X)})%
 \right],         If X ( s ) is a stochastic process , then for all functions t ( s ) such that the integral ‚à´ R t ( s ) X ( s )d s converges for almost all realizations of X  9            œÜ  X    (  t  )    =   E   [   exp   (   i    ‚à´  ùêë    t   (  s  )   X   (  s  )   d  s     )    ]     .         subscript  œÜ  X   t    normal-E      i    subscript   ùêë     t  s  X  s  d  s         \varphi_{X}(t)=\operatorname{E}\left[\exp\left({i\int_{\mathbf{R}}t(s)X(s)ds}%
 \right)\right].        Here     T     T    {}^{T}   denotes matrix transpose , tr(¬∑) ‚Äî the matrix trace operator, Re(¬∑) is the real part of a complex number, z denotes complex conjugate , and * is conjugate transpose (that is  z T ''}} ).  Examples      Distribution   Characteristic function œÜ(t)       Degenerate Œ¥ a        e    i  t  a      superscript  e    i  t  a     \!e^{ita}        Bernoulli Bern( p )         1   -  p   +   p   e   i  t           1  p     p   superscript  e    i  t       \!1-p+pe^{it}        Binomial B( n, p )        (    1  -  p   +   p   e   i  t      )   n     superscript      1  p     p   superscript  e    i  t      n    \!(1-p+pe^{it})^{n}        Negative binomial NB( r, p )        (    1  -  p    1  -   p   e    i   t       )    r      superscript      1  p     1    p   superscript  e    i  t       r    \biggl(\frac{1-p}{1-pe^{i\,t}}\biggr)^{\!r}        Poisson Pois(Œª)        e    Œª   (    e   i  t    -  1   )       superscript  e    Œª     superscript  e    i  t    1      \!e^{\lambda(e^{it}-1)}        Uniform U( a, b )          e   i  t  b    -   e   i  t  a      i  t   (   b  -  a   )            superscript  e    i  t  b     superscript  e    i  t  a       i  t    b  a      \!\frac{e^{itb}-e^{ita}}{it(b-a)}        Laplace L( Œº, b )         e   i  t  Œº     1  +    b  2    t  2           superscript  e    i  t  Œº      1     superscript  b  2    superscript  t  2       \!\frac{e^{it\mu}}{1+b^{2}t^{2}}        Normal  N ( Œº, œÉ 2 )        e     i  t  Œº   -    1  2    œÉ  2    t  2        superscript  e      i  t  Œº       1  2    superscript  œÉ  2    superscript  t  2       \!e^{it\mu-\frac{1}{2}\sigma^{2}t^{2}}        Chi-squared œá 2 k        (   1  -   2  i  t    )    -   k  /  2       superscript    1    2  i  t        k  2      \!(1-2it)^{-k/2}        Cauchy C( Œº, Œ∏ )        e     i  t  Œº   -   Œ∏   |  t  |        superscript  e      i  t  Œº     Œ∏    t       \!e^{it\mu-\theta|t|}        Gamma Œì( k, Œ∏ )        (   1  -   i  t  Œ∏    )    -  k      superscript    1    i  t  Œ∏      k     \!(1-it\theta)^{-k}        Exponential Exp( Œª )        (   1  -   i  t   Œª   -  1      )    -  1      superscript    1    i  t   superscript  Œª    1        1     \!(1-it\lambda^{-1})^{-1}        Multivariate normal  N ( Œº , Œ£ )        e     i   t  T   Œº   -    1  2    t  T   Œ£  t       superscript  e      i   superscript  t  T   Œº       1  2    superscript  t  T   normal-Œ£  t      \!e^{it^{T}\mu-\frac{1}{2}t^{T}\Sigma t}        Multivariate Cauchy  MultiCauchy ( Œº , Œ£ ) 10        e     i   t  T   Œº   -     t  T   Œ£  t        superscript  e      i   superscript  t  T   Œº        superscript  t  T   normal-Œ£  t       \!e^{it^{T}\mu-\sqrt{t^{T}\Sigma t}}          Oberhettinger (1973) provides extensive tables of characteristic functions.  Properties   The characteristic function of a real-valued random variable always exists, since it is an integral of a bounded continuous function over a space whose measure is finite.  A characteristic function is uniformly continuous on the entire space  It is non-vanishing in a region around zero: œÜ(0) = 1.  It is bounded: |œÜ( t )| ‚â§ 1.  It is Hermitian :  œÜ( t ) }} . In particular, the characteristic function of a symmetric (around the origin) random variable is real-valued and even.  There is a bijection between probability distributions and characteristic functions. That is, for any two random variables X 1 , X 2 ,     X 1 , X 2 both have the same probability distribution if and only if     œÜ   X  1    =   œÜ   X  2         subscript  œÜ   subscript  X  1     subscript  œÜ   subscript  X  2      \varphi_{X_{1}}=\varphi_{X_{2}}         If a random variable X has moments up to k -th order, then the characteristic function œÜ X is k times continuously differentiable on the entire real line. In this case           E   [   X  k   ]    =     (   -  i   )   k    œÜ  X   (  k  )     (  0  )     .       normal-E   superscript  X  k       superscript    i   k    superscript   subscript  œÜ  X   k   0     \operatorname{E}[X^{k}]=(-i)^{k}\varphi_{X}^{(k)}(0).         If a characteristic function œÜ X has a k -th derivative at zero, then the random variable X has all moments up to k if k is even, but only up to  if k is odd. 11           œÜ  X   (  k  )     (  0  )    =    i  k    E   [   X  k   ]            superscript   subscript  œÜ  X   k   0      superscript  i  k    normal-E   superscript  X  k       \varphi_{X}^{(k)}(0)=i^{k}\operatorname{E}[X^{k}]         If X 1 , ‚Ä¶, X n are independent random variables, and a 1 , ‚Ä¶, a n are some constants, then the characteristic function of the linear combination of the X i 's is            œÜ     a  1    X  1    +  ‚ãØ  +    a  n    X  n       (  t  )    =    œÜ   X  1     (    a  1   t   )   ‚ãØ   œÜ   X  n     (    a  n   t   )     .         subscript  œÜ       subscript  a  1    subscript  X  1    normal-‚ãØ     subscript  a  n    subscript  X  n      t      subscript  œÜ   subscript  X  1       subscript  a  1   t   normal-‚ãØ   subscript  œÜ   subscript  X  n       subscript  a  n   t      \varphi_{a_{1}X_{1}+\cdots+a_{n}X_{n}}(t)=\varphi_{X_{1}}(a_{1}t)\cdots\varphi%
 _{X_{n}}(a_{n}t).          One specific case is the sum of two independent random variables X 1 and X 2 in which case one has         œÜ    X  1   +   X  2      (  t  )    =      œÜ   X  1     (  t  )    ‚ãÖ   œÜ   X  2      (  t  )     .         subscript  œÜ     subscript  X  1    subscript  X  2     t      normal-‚ãÖ     subscript  œÜ   subscript  X  1    t    subscript  œÜ   subscript  X  2     t     \varphi_{X_{1}+X_{2}}(t)=\varphi_{X_{1}}(t)\cdot\varphi_{X_{2}}(t).         The tail behavior of the characteristic function determines the smoothness of the corresponding density function.   Continuity  The bijection stated above between probability distributions and characteristic functions is continuous . That is, whenever a sequence of distribution functions F j ( x ) converges (weakly) to some distribution F ( x ), the corresponding sequence of characteristic functions œÜ j ( t ) will also converge, and the limit œÜ( t ) will correspond to the characteristic function of law F . More formally, this is stated as   L√©vy‚Äôs continuity theorem : A sequence X j of n -variate random variables converges in distribution to random variable X if and only if the sequence œÜ X j converges pointwise to a function œÜ which is continuous at the origin. Then œÜ is the characteristic function of X . 12    This theorem is frequently used to prove the law of large numbers , and the central limit theorem .  Inversion formulas  Since there is a one-to-one correspondence between cumulative distribution functions and characteristic functions, it is always possible to find one of these functions if we know the other one. The formula in definition of characteristic function allows us to compute œÜ when we know the distribution function F (or density f ). If, on the other hand, we know the characteristic function œÜ and want to find the corresponding distribution function, then one of the following inversion theorems can be used.  Theorem . If characteristic function œÜ X is integrable , then F X is absolutely continuous, and therefore X has the probability density function given by         f  X    (  x  )    =    F  X  ‚Ä≤    (  x  )    =    1   2  œÄ      ‚à´  ùêë     e   -   i  t  x      œÜ  X    (  t  )   d  t      ,           subscript  f  X   x      superscript   subscript  F  X   normal-‚Ä≤   x            1    2  œÄ      subscript   ùêë      superscript  e      i  t  x      subscript  œÜ  X   t  d  t        f_{X}(x)=F_{X}^{\prime}(x)=\frac{1}{2\pi}\int_{\mathbf{R}}e^{-itx}\varphi_{X}(%
 t)dt,   when X is scalar; in multivariate case the pdf is understood as the Radon‚ÄìNikodym derivative of the distribution Œº X with respect to the Lebesgue measure  Œª :         f  X    (  x  )    =     d   Œº  X     d  Œª     (  x  )    =    1    (   2  œÄ   )   n      ‚à´   ùêë  n      e   -   i   (   t  ‚ãÖ  x   )       œÜ  X    (  t  )   Œª   (   d  t   )       .           subscript  f  X   x         d   subscript  Œº  X      d  Œª    x            1   superscript    2  œÄ   n      subscript    superscript  ùêë  n       superscript  e      i   normal-‚ãÖ  t  x       subscript  œÜ  X   t  Œª    d  t         f_{X}(x)=\frac{d\mu_{X}}{d\lambda}(x)=\frac{1}{(2\pi)^{n}}\int_{\mathbf{R}^{n}%
 }e^{-i(t\cdot x)}\varphi_{X}(t)\lambda(dt).     Theorem (L√©vy) . 13 If œÜ X is characteristic function of distribution function F X , two points a are such that } is a continuity set of Œº X (in the univariate case this condition is equivalent to continuity of F X at points a and b ), then   If X is scalar:             F  X    (  b  )    -    F  X    (  a  )     =    1   2  œÄ      lim   T  ‚Üí  ‚àû      ‚à´   -  T    +  T         e   -   i  t  a     -   e   -   i  t  b       i  t      œÜ  X    (  t  )   d  t       .           subscript  F  X   b      subscript  F  X   a        1    2  œÄ      subscript    normal-‚Üí  T       superscript   subscript     T      T           superscript  e      i  t  a      superscript  e      i  t  b        i  t     subscript  œÜ  X   t  d  t        F_{X}(b)-F_{X}(a)=\frac{1}{2\pi}\lim_{T\to\infty}\int_{-T}^{+T}\frac{e^{-ita}-%
 e^{-itb}}{it}\,\varphi_{X}(t)\,dt.       This formula can be re-stated in a form more convenient for numerical computation as 14            F   (   x  +  h   )    -   F   (   x  -  h   )      2  h    =    1   2  œÄ      ‚à´   -  ‚àû   ‚àû      sin   h  t     h  t     e   -   i  t  x      œÜ  X    (  t  )   d  t      .            F    x  h      F    x  h       2  h        1    2  œÄ      superscript   subscript                  h  t      h  t     superscript  e      i  t  x      subscript  œÜ  X   t  d  t       \frac{F(x+h)-F(x-h)}{2h}=\frac{1}{2\pi}\int_{-\infty}^{\infty}\frac{\sin ht}{%
 ht}e^{-itx}\varphi_{X}(t)\,dt.         For a random variable bounded from below one can obtain    F   (  b  )       F  b    F(b)   by taking   a   a   a   such that     F   (  a  )    =  0.        F  a   0.    F(a)=0.   Otherwise, if a random variable is not bounded from below, the limit for    a  ‚Üí   -  ‚àû      normal-‚Üí  a        a\to-\infty   gives    F   (  b  )       F  b    F(b)   , but is numerically impractical. 15     If X is a vector random variable:     \mu_X\big(\{a     Theorem . If a is (possibly) an atom of X (in the univariate case this means a point of discontinuity of F X ) then   If X is scalar:            F  X    (  a  )    -    F  X    (   a  -  0   )     =    lim   T  ‚Üí  ‚àû      1   2  T      ‚à´   -  T    +  T      e   -   i  t  a      œÜ  X    (  t  )   d  t               subscript  F  X   a      subscript  F  X     a  0       subscript    normal-‚Üí  T         1    2  T      superscript   subscript     T      T       superscript  e      i  t  a      subscript  œÜ  X   t  d  t        F_{X}(a)-F_{X}(a-0)=\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{+T}e^{-ita}\varphi%
 _{X}(t)dt         If X is a vector random variable:           Œº  X    (   {  a  }   )    =    lim    T  1   ‚Üí  ‚àû     ‚ãØ    lim    T  n   ‚Üí  ‚àû      (    ‚àè   k  =  1   n    1   2   T  k      )     ‚à´   -  T   T     e   -   i   (   t  ‚ãÖ  a   )       œÜ  X    (  t  )   Œª   (   d  t   )                subscript  Œº  X    a      subscript    normal-‚Üí   subscript  T  1        normal-‚ãØ    subscript    normal-‚Üí   subscript  T  n          superscript   subscript  product    k  1    n     1    2   subscript  T  k        superscript   subscript     T    T      superscript  e      i   normal-‚ãÖ  t  a       subscript  œÜ  X   t  Œª    d  t           \mu_{X}(\{a\})=\lim_{T_{1}\to\infty}\cdots\lim_{T_{n}\to\infty}\left(\prod_{k=%
 1}^{n}\frac{1}{2T_{k}}\right)\int_{-T}^{T}e^{-i(t\cdot a)}\varphi_{X}(t)%
 \lambda(dt)        Theorem (Gil-Pelaez) . 16 For a univariate random variable X , if x is a continuity point of F X then         F  X    (  x  )    =    1  2   -    1  œÄ     ‚à´  0  ‚àû       Im   [    e   -   i  t  x      œÜ  X    (  t  )    ]    t    d  t       .         subscript  F  X   x       1  2       1  œÄ     superscript   subscript   0          Im     superscript  e      i  t  x      subscript  œÜ  X   t    t   d  t        F_{X}(x)=\frac{1}{2}-\frac{1}{\pi}\int_{0}^{\infty}\frac{\operatorname{Im}[e^{%
 -itx}\varphi_{X}(t)]}{t}\,dt.   where the imaginary part of a complex number   z   z   z   is given by     Im   (  z  )    =     (   z  -   z  *    )   /  2   i         Im  z         z   superscript  z     2   i     \mathrm{Im}(z)=(z-z^{*})/2i   .The integral may be not Lebesgue-integrable ; for example, when X is the discrete random variable that is always 0, it becomes the Dirichlet integral .  Inversion formulas for multivariate distributions are available. 17  Criteria for characteristic functions  First note that the set of all characteristic functions is closed under certain operations:   A convex linear combination      ‚àë  n     a  n    œÜ  n    (  t  )        subscript   n      subscript  a  n    subscript  œÜ  n   t     \scriptstyle\sum_{n}a_{n}\varphi_{n}(t)   (with      a  n   ‚â•  0   ,     ‚àë  n    a  n    =  1      formulae-sequence     subscript  a  n   0       subscript   n    subscript  a  n    1     \scriptstyle a_{n}\geq 0,\ \sum_{n}a_{n}=1   ) of a finite or a countable number of characteristic functions is also a characteristic function.  The product of a finite number of characteristic functions is also a characteristic function. The same holds for an infinite product provided that it converges to a function continuous at the origin.  If œÜ is a characteristic function and Œ± is a real number, then    œÜ  ¬Ø     normal-¬Ø  œÜ    \bar{\varphi}   , Re(œÜ), |œÜ| 2 , and œÜ(Œ± t ) are also characteristic functions.   It is well known that any non-decreasing c√†dl√†g function F with limits F (‚àí‚àû) = 0, F (+‚àû) = 1 corresponds to a cumulative distribution function of some random variable. There is also interest in finding similar simple criteria for when a given function œÜ could be the characteristic function of some random variable. The central result here is Bochner‚Äôs theorem , although its usefulness is limited because the main condition of the theorem, non-negative definiteness , is very hard to verify. Other theorems also exist, such as Khinchine‚Äôs, Mathias‚Äôs, or Cram√©r‚Äôs, although their application is just as difficult. P√≥lya‚Äôs theorem, on the other hand, provides a very simple convexity condition which is sufficient but not necessary. Characteristic functions which satisfy this condition are called P√≥lya-type. 18  Bochner‚Äôs theorem . An arbitrary function œÜ : R n ‚Üí C is the characteristic function of some random variable if and only if œÜ is positive definite , continuous at the origin, and if œÜ(0) = 1.  Khinchine‚Äôs criterion . A complex-valued, absolutely continuous function œÜ, with œÜ(0) = 1, is a characteristic function if and only if it admits the representation        œÜ   (  t  )    =    ‚à´  ùêë    g   (   t  +  Œ∏   )     g   (  Œ∏  )    ¬Ø   d  Œ∏     .        œÜ  t     subscript   ùêë     g    t  Œ∏    normal-¬Ø    g  Œ∏    d  Œ∏      \varphi(t)=\int_{\mathbf{R}}g(t+\theta)\overline{g(\theta)}d\theta.     Mathias‚Äô theorem . A real-valued, even, continuous, absolutely integrable function œÜ, with œÜ(0) = 1, is a characteristic function if and only if         (   -  1   )   n    (    ‚à´  ùêë    œÜ   (   p  t   )    e   -    t  2   2      H   2  n     (  t  )   d  t    )    ‚â•  0         superscript    1   n     subscript   ùêë     œÜ    p  t    superscript  e       superscript  t  2   2      subscript  H    2  n    t  d  t     0    (-1)^{n}\left(\int_{\mathbf{R}}\varphi(pt)e^{-\frac{t^{2}}{2}}H_{2n}(t)dt%
 \right)\geq 0   for n = 0,1,2,‚Ä¶, and all p > 0. Here H 2 n denotes the Hermite polynomial of degree 2 n .   P√≥lya‚Äôs theorem . If œÜ is a real-valued, even, continuous function which satisfies the conditions   œÜ(0) = 1,  œÜ is convex for t > 0,  œÜ(‚àû) = 0,   then œÜ( t ) is the characteristic function of an absolutely continuous symmetric distribution.  Uses  Because of the continuity theorem , characteristic functions are used in the most frequently seen proof of the central limit theorem . The main trick involved in making calculations with a characteristic function is recognizing the function as the characteristic function of a particular distribution.  Basic manipulations of distributions  Characteristic functions are particularly useful for dealing with linear functions of independent random variables. For example, if X 1 , X 2 , ..., X n is a sequence of independent (and not necessarily identically distributed) random variables, and        S  n   =    ‚àë   i  =  1   n     a  i    X  i      ,       subscript  S  n     superscript   subscript     i  1    n      subscript  a  i    subscript  X  i       S_{n}=\sum_{i=1}^{n}a_{i}X_{i},\,\!     where the a i are constants, then the characteristic function for S n is given by        œÜ   S  n     (  t  )    =    œÜ   X  1     (    a  1   t   )    œÜ   X  2     (    a  2   t   )   ‚ãØ   œÜ   X  n     (    a  n   t   )           subscript  œÜ   subscript  S  n    t      subscript  œÜ   subscript  X  1       subscript  a  1   t    subscript  œÜ   subscript  X  2       subscript  a  2   t   normal-‚ãØ   subscript  œÜ   subscript  X  n       subscript  a  n   t      \varphi_{S_{n}}(t)=\varphi_{X_{1}}(a_{1}t)\varphi_{X_{2}}(a_{2}t)\cdots\varphi%
 _{X_{n}}(a_{n}t)\,\!     In particular,  œÜ X ( t ) œÜ Y ( t )}} . To see this, write out the definition of characteristic function:        œÜ   X  +  Y     (  t  )    =   E   [   e   i  t   (   X  +  Y   )     ]    =   E   [    e   i  t  X     e   i  t  Y     ]    =     E   [   e   i  t  X    ]    E    [   e   i  t  Y    ]    =    œÜ  X    (  t  )    œÜ  Y    (  t  )             subscript  œÜ    X  Y    t    normal-E   superscript  e    i  t    X  Y            normal-E     superscript  e    i  t  X     superscript  e    i  t  Y               normal-E   superscript  e    i  t  X     E    delimited-[]   superscript  e    i  t  Y              subscript  œÜ  X   t   subscript  œÜ  Y   t      \varphi_{X+Y}(t)=\operatorname{E}\left[e^{it(X+Y)}\right]=\operatorname{E}%
 \left[e^{itX}e^{itY}\right]=\operatorname{E}\left[e^{itX}\right]E\left[e^{itY}%
 \right]=\varphi_{X}(t)\varphi_{Y}(t)     Observe that the independence of X and Y is required to establish the equality of the third and fourth expressions.  Another special case of interest is when 1/ n }} and then S n is the sample mean. In this case, writing X for the mean,        œÜ   X  ¬Ø     (  t  )    =     œÜ  X      (    t  n    )   n           subscript  œÜ   normal-¬Ø  X    t      subscript  œÜ  X    superscript    t  n   n      \varphi_{\overline{X}}(t)=\varphi_{X}\!\left(\tfrac{t}{n}\right)^{n}     Moments  Characteristic functions can also be used to find moments of a random variable. Provided that the n th moment exists, characteristic function can be differentiated n times and       E   [   X  n   ]    =     i   -  n      œÜ  X   (  n  )     (  0  )    =     i   -  n       [     d  n    d   t  n      œÜ  X    (  t  )    ]    t  =  0            normal-E   superscript  X  n       superscript  i    n     superscript   subscript  œÜ  X   n   0           superscript  i    n     subscript   delimited-[]       superscript  d  n     d   superscript  t  n      subscript  œÜ  X   t      t  0        \operatorname{E}\left[X^{n}\right]=i^{-n}\,\varphi_{X}^{(n)}(0)=i^{-n}\,\left[%
 \frac{d^{n}}{dt^{n}}\varphi_{X}(t)\right]_{t=0}\,\!     For example, suppose X has a standard Cauchy distribution . Then  e ‚àí{{!}} t {{!}} }} . See how this is not differentiable at t = 0, showing that the Cauchy distribution has no expectation . Also see that the characteristic function of the sample mean X of n  independent observations has characteristic function "text-decoration:overline;"'> X ( t ) {{=}} ( e ‚àí{{!}} t {{!}}/ n ) n {{=}} e ‚àí{{!}} t {{!}} }} , using the result from the previous section. This is the characteristic function of the standard Cauchy distribution: thus, the sample mean has the same distribution as the population itself.  The logarithm of a characteristic function is a cumulant generating function , which is useful for finding cumulants ; note that some instead define the cumulant generating function as the logarithm of the moment-generating function , and call the logarithm of the characteristic function the second cumulant generating function.  Data analysis  Characteristic functions can be used as part of procedures for fitting probability distributions to samples of data. Cases where this provides a practicable option compared to other possibilities include fitting the stable distribution since closed form expressions for the density are not available which makes implementation of maximum likelihood estimation difficult. Estimation procedures are available which match the theoretical characteristic function to the empirical characteristic function, calculated from the data. Paulson et al. (1975) and Heathcote (1977) provide some theoretical background for such an estimation procedure. In addition, Yu (2004) describes applications of empirical characteristic functions to fit time series models where likelihood procedures are impractical.  Example  The Gamma distribution with scale parameter Œ∏ and a shape parameter k has the characteristic function        (   1  -    Œ∏    i   t    )    -  k    .     superscript    1    Œ∏  i  t      k     (1-\theta\,i\,t)^{-k}.   Now suppose that we have       X   ‚àº   Œì   (   k  1   ,  Œ∏  )   and  Y   ‚àº   Œì   (   k  2   ,  Œ∏  )         similar-to  X    normal-Œì    subscript  k  1   Œ∏   and  Y     similar-to      normal-Œì    subscript  k  2   Œ∏       X~{}\sim\Gamma(k_{1},\theta)\mbox{ and }Y\sim\Gamma(k_{2},\theta)\,   with X and Y independent from each other, and we wish to know what the distribution of X + Y is. The characteristic functions are         œÜ  X    (  t  )    =    (   1  -    Œ∏    i   t    )    -   k  1      ,     œÜ  Y    (  t  )    =    (   1  -    Œ∏    i   t    )    -   k  2         formulae-sequence       subscript  œÜ  X   t    superscript    1    Œ∏  i  t       subscript  k  1           subscript  œÜ  Y   t    superscript    1    Œ∏  i  t       subscript  k  2        \varphi_{X}(t)=(1-\theta\,i\,t)^{-k_{1}},\,\qquad\varphi_{Y}(t)=(1-\theta\,i\,%
 t)^{-k_{2}}   which by independence and the basic properties of characteristic function leads to         œÜ   X  +  Y     (  t  )    =    œÜ  X    (  t  )    œÜ  Y    (  t  )    =     (   1  -    Œ∏    i   t    )    -   k  1       (   1  -    Œ∏    i   t    )    -   k  2      =    (   1  -    Œ∏    i   t    )    -   (    k  1   +   k  2    )      .           subscript  œÜ    X  Y    t      subscript  œÜ  X   t   subscript  œÜ  Y   t           superscript    1    Œ∏  i  t       subscript  k  1      superscript    1    Œ∏  i  t       subscript  k  2            superscript    1    Œ∏  i  t         subscript  k  1    subscript  k  2         \varphi_{X+Y}(t)=\varphi_{X}(t)\varphi_{Y}(t)=(1-\theta\,i\,t)^{-k_{1}}(1-%
 \theta\,i\,t)^{-k_{2}}=\left(1-\theta\,i\,t\right)^{-(k_{1}+k_{2})}.   This is the characteristic function of the gamma distribution scale parameter Œ∏ and shape parameter k 1 + k 2 , and we therefore conclude       X  +  Y   ‚àº   Œì   (    k  1   +   k  2    ,  Œ∏  )       similar-to    X  Y     normal-Œì      subscript  k  1    subscript  k  2    Œ∏      X+Y\sim\Gamma(k_{1}+k_{2},\theta)\,   The result can be expanded to n independent gamma distributed random variables with the same scale parameter and we get         ‚àÄ  i   ‚àà   {  1  ,  ‚Ä¶  ,  n  }    :     X  i   ‚àº    Œì   (   k  i   ,  Œ∏  )    ‚áí       ‚àë   i  =  1   n    X  i    ‚àº   Œì   (    ‚àë   i  =  1   n    k  i    ,  Œ∏  )       .     normal-:     for-all  i    1  normal-‚Ä¶  n     formulae-sequence   similar-to   subscript  X  i      normal-Œì    subscript  k  i   Œ∏    normal-‚áí     similar-to    superscript   subscript     i  1    n    subscript  X  i      normal-Œì     superscript   subscript     i  1    n    subscript  k  i    Œ∏        \forall i\in\{1,\ldots,n\}:X_{i}\sim\Gamma(k_{i},\theta)\qquad\Rightarrow%
 \qquad\sum_{i=1}^{n}X_{i}\sim\Gamma\left(\sum_{i=1}^{n}k_{i},\theta\right).     Entire characteristic functions  As defined above, the argument of the characteristic function is treated as a real number: however, certain aspects of the theory of characteristic functions are advanced by extending the definition into the complex plane by analytical continuation , in cases where this is possible. 19  Related concepts  Related concepts include the moment-generating function and the probability-generating function . The characteristic function exists for all probability distributions. This is not the case for the moment-generating function.  The characteristic function is closely related to the Fourier transform : the characteristic function of a probability density function p ( x ) is the complex conjugate of the continuous Fourier transform of p ( x ) (according to the usual convention; see continuous Fourier transform ‚Äì other conventions ).         œÜ  X    (  t  )    =   ‚ü®   e   i  t  X    ‚ü©   =    ‚à´  ùêë     e   i  t  x    p   (  x  )   d  x    =    (    ‚à´  ùêë     e   -   i  t  x     p   (  x  )   d  x    )   ¬Ø   =    P   (  t  )    ¬Ø    ,           subscript  œÜ  X   t    delimited-‚ü®‚ü©   superscript  e    i  t  X            subscript   ùêë      superscript  e    i  t  x    p  x  d  x          normal-¬Ø    subscript   ùêë      superscript  e      i  t  x     p  x  d  x           normal-¬Ø    P  t       \varphi_{X}(t)=\langle e^{itX}\rangle=\int_{\mathbf{R}}e^{itx}p(x)\,dx=%
 \overline{\left(\int_{\mathbf{R}}e^{-itx}p(x)\,dx\right)}=\overline{P(t)},     where P ( t ) denotes the continuous Fourier transform of the probability density function p ( x ). Likewise, p ( x ) may be recovered from œÜ X ( t ) through the inverse Fourier transform:        p   (  x  )    =    1   2  œÄ      ‚à´  ùêë     e   i  t  x    P   (  t  )   d  t     =    1   2  œÄ      ‚à´  ùêë     e   i  t  x        œÜ  X    (  t  )    ¬Ø    d  t      .          p  x       1    2  œÄ      subscript   ùêë      superscript  e    i  t  x    P  t  d  t              1    2  œÄ      subscript   ùêë      superscript  e    i  t  x     normal-¬Ø     subscript  œÜ  X   t    d  t        p(x)=\frac{1}{2\pi}\int_{\mathbf{R}}e^{itx}P(t)\,dt=\frac{1}{2\pi}\int_{%
 \mathbf{R}}e^{itx}\overline{\varphi_{X}(t)}\,dt.     Indeed, even when the random variable does not have a density, the characteristic function may be seen as the Fourier transform of the measure corresponding to the random variable.  Another related concept is the representation of probability distributions as elements of a reproducing kernel Hilbert space via the kernel embedding of distributions . This framework may be viewed as a generalization of the characteristic function under specific choices of the kernel function .  See also   Subindependence , a weaker condition than independence, that is defined in terms of characteristic functions.   References  Bibliography                    External links     {{-}}   "  Category:Probability theory  Category:Theory of probability distributions     Lukacs (1970) p. 196 ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  Kotz et al. p. 37 using 1 as the number of degree of freedom to recover the Cauchy distribution ‚Ü©  Lukacs (1970), Corollary 1 to Theorem 2.3.1 ‚Ü©  ‚Ü©  Named after the French mathematician Paul L√©vy ‚Ü©  Shepard, N.G. (1991a) ‚Ü©  Shepard, N.G. (1991a) ‚Ü©  Wendel, J.G. (1961) ‚Ü©  Shephard (1991a,b) ‚Ü©  Lukacs (1970), p.84 ‚Ü©  ‚Ü©     