<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1909">ABX test</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>ABX test</h1>
<hr/>

<p>An <strong>ABX test</strong> is a method of comparing two choices of sensory stimuli to identify detectable differences between them. A subject is presented with two known samples (sample <strong>A</strong>, the first reference, and sample <strong>B</strong>, the second reference) followed by one unknown sample <strong>X</strong> that is randomly selected from either A or B. The subject is then required to identify X as either A or B. If X cannot be identified reliably with a low <a class="uri" href="p-value" title="wikilink">p-value</a> in a predetermined number of trials, then the <a href="null_hypothesis" title="wikilink">null hypothesis</a> cannot be rejected and it cannot be proven that there is a perceptible difference between A and B.</p>

<p>ABX tests can easily be performed as <a href="double-blind_trial" title="wikilink">double-blind trials</a>, eliminating any possible unconscious influence from the researcher or the test supervisor. Because samples A and B are provided just prior to sample X, the difference does not have to be discerned from assumption based on long-term memory or past experience. Thus, the ABX test answers whether or not, under ideal circumstances, a perceptual difference can be found.</p>

<p>ABX tests are commonly used in evaluations of digital <a href="audio_data_compression" title="wikilink">audio data compression</a> methods; sample A is typically an uncompressed sample, and sample B is a compressed version of A. Audible <a href="compression_artifact" title="wikilink">compression artifacts</a> that indicate a shortcoming in the compression algorithm can be identified with subsequent testing. ABX tests can also be used to compare the different degrees of fidelity loss between two different audio formats at a given <a class="uri" href="bitrate" title="wikilink">bitrate</a>.</p>

<p>ABX tests can be used to audition input, processing, and output components as well as cabling: virtually any audio product or prototype design.</p>
<h2 id="history">History</h2>

<p>The history of ABX testing and naming dates back to 1950 in a paper published by two Bell Labs researchers, W. A. Munson and Mark B. Gardner, titled '' Standardizing Auditory Tests''.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<blockquote>

<p><em>"The purpose of the present paper is to describe a test procedure which has shown promise in this direction and to give descriptions of equipment which have been found helpful in minimizing the variability of the test results. The procedure, which we have called the “ABX” test, is a modification of the method of paired comparisons. An observer is presented with a time sequence of three signals for each judgment he is asked to make. During the first time interval he hears signal A, during the second, signal B, and finally signal X. His task is to indicate whether the sound heard during the X interval was more like that during the A interval or more like that during the B interval. For a threshold test, the A interval is quiet, the B interval is signal, and the X interval is either quiet or signal. "</em></p>
</blockquote>

<p>The test has evolved to other variations such as user control over duration and sequence of testing. One such example was the hardware ABX comparator in 1977, built by the ABX company in Troy, Michigan and documented by one of his founders, David Clark in his Audio Engineering Society Journal Paper, <em>High-Resolution Subjective Testing Using a Double-Blind Comparator</em><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<blockquote>

<p><strong><em>REFINEMENTS TO THE A/B TEST</em></strong></p>

<p>''The author's first experience with double-blind audibility testing was as a member of the SMWTMS Audio Club in early 1977. A button was provided which would select at random component A or B. Identifying one of these, the X component was greatly hampered by not having the known A and B available for reference.</p>

<p><em>This was corrected by using three interlocked pushbuttons, A, B, and X. Once an X was selected, it would remain that particular A or B until it was decided to move on to another random selection.</em></p>

<p><em>However, another problem quickly became obvious. There was always an audible relay transition time delay when switching from A to B. When switching from A to X, however, the time delay would be missing if X was really A and present if X was really B. This extraneous cue was removed by inserting a fixed length dropout time when any change was made. The dropout time was selected to be 50 ms which produces a slight consistent click while allowing subjectively instant comparison.</em></p>
</blockquote>

<p>The ABX company is now defunct and hardware comparators in general as commercial offerings extinct. Myriad of software tools exist such as Foobar ABX plug-in for performing file comparisons. But hardware equipment testing requires building custom implementations.</p>
<h2 id="hardware-tests">Hardware tests</h2>

<p> ABX test equipment utilizing relays to switch between two different hardware paths can help determine if there are perceptual differences in cables and components. Video, audio and digital transmission paths can be compared. If the switching is microprocessor controlled, double-blind tests are possible.</p>

<p>Loudspeaker level and line level audio comparisons could be performed on an ABX test device offered for sale as the <em>ABX Comparator</em> by <a href="QSC_Audio_Products" title="wikilink">QSC Audio Products</a> from 1998 to 2004. Other hardware solutions have been fabricated privately by individuals or organizations for internal testing.</p>
<h2 id="confidence">Confidence</h2>

<p>If only one ABX trial were performed, random guessing would incur a 50% chance of choosing the correct answer, the same as flipping a coin. In order to make a statement having some degree of <a href="Confidence_interval" title="wikilink">confidence</a>, many trials must be performed. By increasing the number of trials, the likelihood of statistically asserting a person's ability to distinguish A and B is enhanced for a given confidence level. A 95% confidence level is commonly considered <a href="Statistical_significance" title="wikilink">statistically significant</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> The company QSC, in the ABX Comparator user manual, recommended a minimum of ten listening trials in each round of tests.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p><strong>Results required for a 95% confidence level:</strong><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">
<p>Number of trials</p></th>
<th style="text-align: left;">
<p>10</p></th>
<th style="text-align: left;">
<p>11</p></th>
<th style="text-align: left;">
<p>12</p></th>
<th style="text-align: left;">
<p>13</p></th>
<th style="text-align: left;">
<p>14</p></th>
<th style="text-align: left;">
<p>15</p></th>
<th style="text-align: left;">
<p>16</p></th>
<th style="text-align: left;">
<p>17</p></th>
<th style="text-align: left;">
<p>18</p></th>
<th style="text-align: left;">
<p>19</p></th>
<th style="text-align: left;">
<p>20</p></th>
<th style="text-align: left;">
<p>21</p></th>
<th style="text-align: left;">
<p>22</p></th>
<th style="text-align: left;">
<p>23</p></th>
<th style="text-align: left;">
<p>24</p></th>
<th style="text-align: left;">
<p>25</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p><strong>Minimum number correct</strong></p></td>
<td style="text-align: left;">
<p>9</p></td>
<td style="text-align: left;">
<p>9</p></td>
<td style="text-align: left;">
<p>10</p></td>
<td style="text-align: left;">
<p>10</p></td>
<td style="text-align: left;">
<p>11</p></td>
<td style="text-align: left;">
<p>12</p></td>
<td style="text-align: left;">
<p>12</p></td>
<td style="text-align: left;">
<p>13</p></td>
<td style="text-align: left;">
<p>13</p></td>
<td style="text-align: left;">
<p>14</p></td>
<td style="text-align: left;">
<p>15</p></td>
<td style="text-align: left;">
<p>15</p></td>
<td style="text-align: left;">
<p>16</p></td>
<td style="text-align: left;">
<p>16</p></td>
<td style="text-align: left;">
<p>17</p></td>
<td style="text-align: left;">
<p>18</p></td>
</tr>
<tr class="even">
</tr>
</tbody>
</table>

<p>QSC recommended that no more than 25 trials be performed, as listener fatigue can set in, making the test less sensitive (less likely to reveal one's actual ability to discern the difference between A and B).<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> However a more sensitive test can be obtained by <a href="Pooled_variance" title="wikilink">pooling</a> the results from a number of such tests using separate individuals or tests from the same listener conducted in between rest breaks. For a large number of total trials N, a significant result (one with 95% confidence) can be claimed if the number of correct responses exceeds 

<math display="inline" id="ABX_test:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>N</mi>
    <mo>/</mo>
    <mn>2</mn>
   </mrow>
   <mo>+</mo>
   <msqrt>
    <mi>N</mi>
   </msqrt>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <divide></divide>
     <ci>N</ci>
     <cn type="integer">2</cn>
    </apply>
    <apply>
     <root></root>
     <ci>N</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N/2+\sqrt{N}
  </annotation>
 </semantics>
</math>

. Important decisions are normally based on a higher level of confidence, since an erroneous "significant result" would be claimed in one of 20 such tests simply by chance.</p>
<h2 id="software-tests">Software tests</h2>

<p>The <a class="uri" href="foobar2000" title="wikilink">foobar2000</a> and the <a href="Amarok_(audio)" title="wikilink">Amarok</a> audio players support software-based ABX testing, the latter using a third-party script. <a href="http://lacinato.com/cm/software/othersoft/abx">Lacinato ABX</a> is a cross-platform testing tool for Linux, Windows, and 64-bit Mac. <a href="http://sourceforge.net/p/avex/home/Home/">aveX</a> is an open-source software mainly developed for <a class="uri" href="Linux" title="wikilink">Linux</a> which also provides test-monitoring from a remote computer. <a href="http://www.surrey.ac.uk/msr/people/chris_hummersone/index.htm#additional">ABX patcher</a> is an ABX implementation for <a class="uri" href="Max/MSP" title="wikilink">Max/MSP</a>. More ABX software can be found at the archived <a href="http://web.archive.org/web/20070813001013/http://www.pcabx.com/">PCABX website</a>.</p>
<h2 id="potential-flaws">Potential flaws</h2>

<p>ABX is a type of <a href="forced_choice" title="wikilink">forced choice</a> testing. The listener at all times can vote whether "X" sounds the same as "A" or "B." Both answers are available to him. Such answers could be on merit, i.e. the listener indeed tried to identify whether X sounded closer to A or B. Or just voted randomly without even listening. Simply looking at the outcome of the test, i.e. X out of Y answers correct is not revealing of this problem. If not caught, incorrect tests will dilute the results of others who intently took the test and subjects the outcome to <a href="Simpson's_paradox" title="wikilink">Simpson's paradox</a>, resulting in false summary results.</p>

<p>This problem becomes more acute if the differences are small, or the content is selected that is not very revealing of the differences under test. The user may get frustrated and simply aim to finish the test by voting randomly. In this regard, forced choice tests such as ABX tend to favor negative outcome when differences are small if proper protocols are not used to guard against this problem.</p>

<p>Best practices as for example outlined in <a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> calls for 1) existence of controls and 2) screening of listeners:</p>
<blockquote>

<p><em>A major consideration is the inclusion of appropriate control conditions. Typically, control conditions include the presentation of unimpaired audio materials, introduced in ways that are unpredictable to the subjects. It is the differences between judgement of these control stimuli and the potentially impaired ones that allows one to conclude that the grades are actual assessments of the impairments.</em></p>
</blockquote>
<blockquote>

<p><strong><em>3.2.2 Post-screening of subjects</em></strong></p>

<p><em>Post-screening methods can be roughly separated into at least two classes; one is based on inconsistencies compared with the mean result and another relies on the ability of the subject to make correct identifications. The first class is never justifiable. Whenever a subjective listening test is performed with the test method recommended here, the required information for the second class of post-screening is automatically available. A suggested statistical method for doing this is described in Attachment 1.</em></p>

<p><em>The methods are primarily used to eliminate subjects who cannot make the appropriate discriminations. The application of a post-screening method may clarify the tendencies in a test result. However, bearing in mind the variability of subjects’ sensitivities to different artefacts,caution should be exercised.</em></p>
</blockquote>

<p>Other flaws include lack of listener training and familiarization with the test and content selected:</p>
<blockquote>

<p><strong><em>4.1 Familiarization or training phase</em></strong></p>

<p><em>Prior to formal grading, subjects must be allowed to become thoroughly familiar with the test facilities, the test environment, the grading process, the grading scales and the methods of their use. Subjects should also become thoroughly familiar with the artefacts under study. For the most sensitive tests they should be exposed to all the material they will be grading later in the formal grading sessions. During familiarization or training, subjects should be preferably together in groups (say, consisting of three subjects), so that they can interact freely and discuss the artefacts they detect with each other.</em></p>
</blockquote>

<p>Other problems might arise from the abx equipment itself, as outlined by the previous Clark reference where the equipment provides a <a href="Tell_(poker)" title="wikilink">tell</a>, allowing the listener to identify the source. Lack of transparency of the ABX fixture creates similar problems.</p>

<p>Since auditory tests such as ABX rely on <a href="short-term_memory" title="wikilink">short-term memory</a> which only lasts a few seconds, it is critical that the test fixture include mechanisms for the listener to locate short segments that can be compared quickly. Pops and glitches in switching apparatus likewise must be eliminated as otherwise they dominate what is stored in listener memory as opposed to the system under test.</p>
<h2 id="alternatives">Alternatives</h2>
<h3 id="algorithmic-audio-compression-evaluation">Algorithmic Audio Compression Evaluation</h3>

<p>Since ABX testing requires human beings for evaluation of lossy audio codecs, it is time-consuming and costly. Therefore, cheaper approaches have been developed, e.g. <a class="uri" href="PEAQ" title="wikilink">PEAQ</a>, which is an implementation of the <a href="Objective_Difference_Grade" title="wikilink">ODG</a>.</p>
<h3 id="mushra">MUSHRA</h3>

<p>In <a class="uri" href="MUSHRA" title="wikilink">MUSHRA</a>, the listener is presented with the reference (labeled as such), a certain number of test samples, a hidden version of the reference and one or more anchors. A 0-100 RATING scale makes it possible to rate very small differences.</p>
<h3 id="discrimination-testing">Discrimination testing</h3>

<p>Alternative general methods are used in <a href="discrimination_testing" title="wikilink">discrimination testing</a>, such as paired comparison, duo–trio, and <a href="triangle_testing" title="wikilink">triangle testing</a>. Of these, duo–trio and triangle testing are particularly close to ABX testing. Schematically:</p>
<dl>
<dt>Duo–trio: AXY – one known, two unknown (one equals A, other equals B), test is which unknown is the known: X = A (and Y = B), or Y = A (and X = B).<br/>
Triangle: XXY – three unknowns (two are A and one is B or one is A and two are B), test which is the odd one out: Y = 1, Y = 2, or Y = 3.</dt>
</dl>

<p>In this context, ABX testing is also known as "duo–trio" in "balanced reference" mode – both knowns are presented as references, rather than one alone.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Transparency_(data_compression)" title="wikilink">Transparency (data compression)</a></li>
<li><a class="uri" href="Psychophysics" title="wikilink">Psychophysics</a></li>
<li><a class="uri" href="Psychoacoustics" title="wikilink">Psychoacoustics</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Digital_audio" title="wikilink">Category:Digital audio</a> <a href="Category:Statistical_tests" title="wikilink">Category:Statistical tests</a> <a class="uri" href="Category:Psychophysics" title="wikilink">Category:Psychophysics</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4">QSC ABX Comparator user manual. (1998) p. 10<a href="#fnref4">↩</a></li>
<li id="fn5">] at<a href="#fnref5">↩</a></li>
<li id="fn6"><a class="uri" href="P-value" title="wikilink">P-value</a><a href="#fnref6">↩</a></li>
<li id="fn7"></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
</ol>
</section>
</body>
</html>
