<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="456">Limiting density of discrete points</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Limiting density of discrete points</h1>
<hr/>

<p>In <a href="information_theory" title="wikilink">information theory</a>, the <strong>limiting density of discrete points</strong> is an adjustment to the formula of <a href="Claude_Shannon" title="wikilink">Claude Shannon</a> for <a href="differential_entropy" title="wikilink">differential entropy</a>.</p>

<p>It was formulated by <a href="Edwin_Thompson_Jaynes" title="wikilink">Edwin Thompson Jaynes</a> to address defects in the initial definition of differential entropy.</p>
<h2 id="definition">Definition</h2>

<p>Shannon originally wrote down the following formula for the <a class="uri" href="entropy" title="wikilink">entropy</a> of a continuous distribution, known as <strong>differential entropy</strong>:</p>

<p>

<math display="block" id="Limiting_density_of_discrete_points:0">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>H</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>X</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mo>-</mo>
     <mrow>
      <mo largeop="true" symmetric="true">∫</mo>
      <mrow>
       <mi>p</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
       </mrow>
       <mrow>
        <mi>log</mi>
        <mi>p</mi>
       </mrow>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo rspace="4.2pt" stretchy="false">)</mo>
       </mrow>
       <mi>d</mi>
       <mi>x</mi>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>H</ci>
     <ci>X</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <int></int>
      <apply>
       <times></times>
       <ci>p</ci>
       <ci>x</ci>
       <apply>
        <log></log>
        <ci>p</ci>
       </apply>
       <ci>x</ci>
       <ci>d</ci>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(X)=-\int p(x)\log p(x)\,dx.
  </annotation>
 </semantics>
</math>

 Unlike Shannon's formula for the discrete entropy, however, this is not the result of any derivation (Shannon simply replaced the summation symbol in the discrete version with an integral) and it turns out to lack many of the properties that make the discrete entropy a useful measure of uncertainty. In particular, it is not invariant under a <a href="change_of_variables" title="wikilink">change of variables</a> and can even become negative.</p>

<p>Jaynes (1963, 1968) argued that the formula for the continuous entropy should be derived by taking the limit of increasingly dense discrete distributions.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Suppose that we have a set of 

<math display="inline" id="Limiting_density_of_discrete_points:1">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 discrete points 

<math display="inline" id="Limiting_density_of_discrete_points:2">
 <semantics>
  <mrow>
   <mo stretchy="false">{</mo>
   <msub>
    <mi>x</mi>
    <mi>i</mi>
   </msub>
   <mo stretchy="false">}</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <set>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>i</ci>
    </apply>
   </set>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{x_{i}\}
  </annotation>
 </semantics>
</math>

, such that in the limit 

<math display="inline" id="Limiting_density_of_discrete_points:3">
 <semantics>
  <mrow>
   <mi>n</mi>
   <mo>→</mo>
   <mi mathvariant="normal">∞</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>n</ci>
    <infinity></infinity>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n\to\infty
  </annotation>
 </semantics>
</math>

 their density approaches a function 

<math display="inline" id="Limiting_density_of_discrete_points:4">
 <semantics>
  <mrow>
   <mi>m</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>m</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m(x)
  </annotation>
 </semantics>
</math>

 called the "invariant measure".</p>

<p>

<math display="inline" id="Limiting_density_of_discrete_points:5">
 <semantics>
  <mrow>
   <mi>m</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>m</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m(x)
  </annotation>
 </semantics>
</math>

 It is similar to the (negative of the) <a href="Kullback–Leibler_divergence" title="wikilink">Kullback–Leibler divergence</a> or relative entropy, which is a comparison between two probability distributions, with one difference. In the Kullback-Leibler divergence, 

<math display="inline" id="Limiting_density_of_discrete_points:6">
 <semantics>
  <mrow>
   <mi>m</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>m</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m(x)
  </annotation>
 </semantics>
</math>

 must be a probability density, whereas in Jaynes' formula, 

<math display="inline" id="Limiting_density_of_discrete_points:7">
 <semantics>
  <mrow>
   <mi>m</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>m</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m(x)
  </annotation>
 </semantics>
</math>

 is simply a density, meaning that it does not have to integrate to 1.</p>

<p>Jaynes' continuous entropy formula has the property of being invariant under a change of variables, provided that 

<math display="inline" id="Limiting_density_of_discrete_points:8">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>p</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(x)
  </annotation>
 </semantics>
</math>

 and <span class="LaTeX">$p(x)$</span> are transformed in the same way. (This motivates the moniker "invariant measure" for <em>m</em>.) This solves many of the difficulties that come from applying Shannon's continuous entropy formula.</p>
<h2 id="references">References</h2>
<ul>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Probability_theory" title="wikilink">Category:Probability theory</a> <a href="Category:Theory_of_probability_distributions" title="wikilink">Category:Theory of probability distributions</a> <a href="Category:Information_theory" title="wikilink">Category:Information theory</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
</ol>
</section>
</body>
</html>
