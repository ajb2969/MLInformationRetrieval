   Random coordinate descent      Random coordinate descent   Randomized (Block) Coordinate Descent Method is an optimization algorithm popularized by Nesterov (2010) and Richtárik and Takáč (2011). The first analysis of this method, when applied to the problem of minimizing a smooth convex function, was performed by Nesterov (2010). 1 In Nesterov's analysis the method needs to be applied to a quadratic perturbation of the original function with an unknown scaling factor. Richtárik and Takáč (2011) give iteration complexity bounds which do not require this, i.e., the method is applied to the objective function directly. Furthermore, they generalize the setting to the problem of minimizing a composite function, i.e., sum of a smooth convex and a (possibly nonsmooth) convex block-separable function:        F   (  x  )    =    f   (  x  )    +   Ψ   (  x  )      ,        F  x       f  x     normal-Ψ  x      F(x)=f(x)+\Psi(x),     where      Ψ   (  x  )    =    ∑   i  =  1   n     Ψ  i    (   x   (  i  )    )      ,        normal-Ψ  x     superscript   subscript     i  1    n      subscript  normal-Ψ  i    superscript  x  i       \Psi(x)=\sum_{i=1}^{n}\Psi_{i}(x^{(i)}),       x  ∈   R  N       x   superscript  R  N     x\in R^{N}   is decomposed into   n   n   n   blocks of variables/coordinates    x  =   (   x   (  1  )    ,  …  ,   x   (  n  )    )       x    superscript  x  1   normal-…   superscript  x  n      x=(x^{(1)},\dots,x^{(n)})   and     Ψ  1   ,  …  ,   Ψ  n       subscript  normal-Ψ  1   normal-…   subscript  normal-Ψ  n     \Psi_{1},\dots,\Psi_{n}   are (simple) convex functions.  Example (block decomposition): If    x  =   (   x  1   ,   x  2   ,  …  ,   x  5   )   ∈   R  5         x    subscript  x  1    subscript  x  2   normal-…   subscript  x  5          superscript  R  5      x=(x_{1},x_{2},\dots,x_{5})\in R^{5}   and    n  =  3      n  3    n=3   , one may choose      x   (  1  )    =   (   x  1   ,   x  3   )    ,    x   (  2  )    =   (   x  2   ,   x  5   )       formulae-sequence     superscript  x  1     subscript  x  1    subscript  x  3        superscript  x  2     subscript  x  2    subscript  x  5       x^{(1)}=(x_{1},x_{3}),x^{(2)}=(x_{2},x_{5})   and     x   (  3  )    =   x  4        superscript  x  3    subscript  x  4     x^{(3)}=x_{4}   .  Example (block-separable regularizers):        n  =  N   ;    Ψ   (  x  )    =    ∥  x  ∥   1   =    ∑   i  =  1   n    |   x  i   |        formulae-sequence    n  N         normal-Ψ  x    subscript   norm  x   1          superscript   subscript     i  1    n      subscript  x  i         n=N;\Psi(x)=\|x\|_{1}=\sum_{i=1}^{n}|x_{i}|          N  =    N  1   +   N  2   +  …  +   N  n     ;    Ψ   (  x  )    =    ∑   i  =  1   n     ∥   x   (  i  )    ∥   2        formulae-sequence    N     subscript  N  1    subscript  N  2   normal-…   subscript  N  n         normal-Ψ  x     superscript   subscript     i  1    n    subscript   norm   superscript  x  i    2       N=N_{1}+N_{2}+\dots+N_{n};\Psi(x)=\sum_{i=1}^{n}\|x^{(i)}\|_{2}   , where     x   (  i  )    ∈   R   N  i         superscript  x  i    superscript  R   subscript  N  i      x^{(i)}\in R^{N_{i}}   and    ∥  ⋅   ∥  2      fragments  parallel-to  normal-⋅   subscript  parallel-to  2     \|\cdot\|_{2}   is the standard Euclidean norm.   Algorithm  Consider the optimization problem         min   x  ∈   R  n     f    (  x  )    ,        subscript     x   superscript  R  n     f   x    \min_{x\in R^{n}}f(x),     where   f   f   f   is a convex and smooth function.  Smoothness: By smoothness we mean the following: we assume the gradient of   f   f   f   is coordinate-wise Lipschitz continuous with constants     L  1   ,   L  2   ,  …  ,   L  n       subscript  L  1    subscript  L  2   normal-…   subscript  L  n     L_{1},L_{2},\dots,L_{n}   . That is, we assume that        |      ∇  i   f    (   x  +   h   e  i     )    -     ∇  i   f    (  x  )     |   ≤    L  i    |  h  |     ,              subscript  normal-∇  i   f     x    h   subscript  e  i          subscript  normal-∇  i   f   x        subscript  L  i     h      |\nabla_{i}f(x+he_{i})-\nabla_{i}f(x)|\leq L_{i}|h|,     for all    x  ∈   R  n       x   superscript  R  n     x\in R^{n}   and    h  ∈  R      h  R    h\in R   , where    ∇  i     subscript  normal-∇  i    \nabla_{i}   denotes the partial derivative with respect to variable    x   (  i  )      superscript  x  i    x^{(i)}   .  Nesterov, and Richtarik and Takac showed that the following algorithm converges to the optimal point:  Input      x  0   ∈   R  n        subscript  x  0    superscript  R  n     x_{0}\in R^{n}    //starting point  Output    x   x   x     set x=x_0  for k=1,... do  choose coordinate     i  ∈   {  1  ,  2  ,  …  ,  n  }       i   1  2  normal-…  n     i\in\{1,2,\dots,n\}    , uniformly at random  update      x   (  i  )    =    x   (  i  )    -    1   L  i     ∇   f  i     (  x  )          superscript  x  i      superscript  x  i       1   subscript  L  i     normal-∇   subscript  f  i    x      x^{(i)}=x^{(i)}-\frac{1}{L_{i}}\nabla f_{i}(x)      endfor;  Convergence rate  Since the iterates of this algorithm are random vectors, a complexity result would give a bound on the number of iterations needed for the method to output an approximate solution with high probability. It was shown in 2 that if    k  ≥     2  n   R  L    (   x  0   )    ϵ    log   (     f   (   x  0   )    -   f  *     ϵ  ρ    )         k        2  n   subscript  R  L    subscript  x  0    ϵ           f   subscript  x  0     superscript  f       ϵ  ρ        k\geq\frac{2nR_{L}(x_{0})}{\epsilon}\log\left(\frac{f(x_{0})-f^{*}}{\epsilon%
 \rho}\right)   , where      R  L    (  x  )    =    max  y     max    x  *   ∈   X  *      {     ∥   y  -   x  *    ∥   L   :    f   (  y  )    ≤   f   (  x  )      }            subscript  R  L   x     subscript   y     subscript      superscript  x     superscript  X       normal-:   subscript   norm    y   superscript  x      L       f  y     f  x         R_{L}(x)=\max_{y}\max_{x^{*}\in X^{*}}\{\|y-x^{*}\|_{L}:f(y)\leq f(x)\}   ,    f  *     superscript  f     f^{*}   is an optimal solution (     f  *   =    min   x  ∈   R  n      {   f   (  x  )    }         superscript  f      subscript     x   superscript  R  n       f  x      f^{*}=\min_{x\in R^{n}}\{f(x)\}   ),    ρ  ∈   (  0  ,  1  )       ρ   0  1     \rho\in(0,1)   is a confidence level and    ϵ  >  0      ϵ  0    \epsilon>0   is target accuracy, then    P  r  o  b   (  f   (   x  k   )   -   f  *   >  ϵ  )   ≤  ρ     fragments  P  r  o  b   fragments  normal-(  f   fragments  normal-(   subscript  x  k   normal-)     superscript  f     ϵ  normal-)    ρ    Prob(f(x_{k})-f^{*}>\epsilon)\leq\rho   .  Example on particular function  The following Figure shows how    x  k     subscript  x  k    x_{k}   develops during iterations, in principle. The problem is        f   (  x  )    =      1  2     x  T    (     1    0.5      0.5    1     )   x   -    (     1.5    1.5     )   x     ,    x  0   =    (     0    0     )   T       formulae-sequence      f  x         1  2    superscript  x  T     1  0.5    0.5  1    x       1.5  1.5    x        subscript  x  0    superscript    0  0    T      f(x)=\tfrac{1}{2}x^{T}\left(\begin{array}[]{cc}1&0.5\\
 0.5&1\end{array}\right)x-\left(\begin{array}[]{cc}1.5&1.5\end{array}\right)x,%
 \quad x_{0}=\left(\begin{array}[]{ cc}0&0\end{array}\right)^{T}     (Figure)  Convergence on small problem.jpg   Extension to Block Coordinate Setting  One can naturally extend this algorithm not only just to coordinates, but to blocks of coordinates. Assume that we have space    R  5     superscript  R  5    R^{5}   . This space has 5 coordinate directions, concretely      e  1   =    (  1  ,  0  ,  0  ,  0  ,  0  )   T    ,     e  2   =    (  0  ,  1  ,  0  ,  0  ,  0  )   T    ,     e  3   =    (  0  ,  0  ,  1  ,  0  ,  0  )   T    ,     e  4   =    (  0  ,  0  ,  0  ,  1  ,  0  )   T    ,    e  5   =    (  0  ,  0  ,  0  ,  0  ,  1  )   T          formulae-sequence     subscript  e  1    superscript   1  0  0  0  0   T     formulae-sequence     subscript  e  2    superscript   0  1  0  0  0   T     formulae-sequence     subscript  e  3    superscript   0  0  1  0  0   T     formulae-sequence     subscript  e  4    superscript   0  0  0  1  0   T       subscript  e  5    superscript   0  0  0  0  1   T         e_{1}=(1,0,0,0,0)^{T},e_{2}=(0,1,0,0,0)^{T},e_{3}=(0,0,1,0,0)^{T},e_{4}=(0,0,0%
 ,1,0)^{T},e_{5}=(0,0,0,0,1)^{T}   in which Random Coordinate Descent Method can move. However, one can group some coordinate directions into blocks and we can have instead of those 5 coordinate directions 3 block coordinate directions (see image).  See also   Coordinate descent  Gradient descent  Mathematical optimization   References  "  Category:Gradient methods     ↩  ↩     