


Spectral theorem




Spectral theorem
In [[mathematics]], particularly [[linear algebra]] and [[functional analysis]
 ], the spectral theorem is any of a number of results about linear operators or matrices. In broad terms, the spectral theorem provides conditions under which an operator or a matrix can be diagonalized (that is, represented as a diagonal matrix in some basis). Intuitively, diagonal matrices are computationally quite manageable, so it is of interest to see whether an arbitrary matrix can be diagonalized. The concept of diagonalization is relatively straightforward for operators on finite-dimensional vector spaces but requires some modification for operators on infinite-dimensional spaces. In general, the spectral theorem identifies a class of linear operators that can be modeled by multiplication operators, which are as simple as one can hope to find. In more abstract language, the spectral theorem is a statement about commutative C*-algebras. See also spectral theory for a historical perspective.
Examples of operators to which the spectral theorem applies are self-adjoint operators or more generally normal operators on Hilbert spaces.
The spectral theorem also provides a canonical decomposition, called the spectral decomposition, eigenvalue decomposition, or eigendecomposition, of the underlying vector space on which the operator acts.
Augustin Louis Cauchy proved the spectral theorem for self-adjoint matrices, i.e., that every real, symmetric matrix is diagonalizable. In addition, Cauchy was the first to be systematic about determinants.12 The spectral theorem as generalized by John von Neumann is today perhaps the most important result of operator theory.
This article mainly focuses on the simplest kind of spectral theorem, that for a self-adjoint operator on a Hilbert space. However, as noted above, the spectral theorem also holds for normal operators on a Hilbert space.
Finite-dimensional case
Hermitian maps and Hermitian matrices
We begin by considering a Hermitian matrix on  or . More generally we consider a Hermitian map

 
  on a finite-dimensional real or complex inner product space

 
  endowed with a positive definite Hermitian inner product. The Hermitian condition means that for all 
 
 
 
 ,



An equivalent condition is that  where  is the hermitian conjugate of 
 
 
 
 . In the case that 
 
 
 
  is identified with an Hermitian matrix, the matrix of  can be identified with its conjugate transpose. If 
 
 
 
  is a real matrix, this is equivalent to  (that is, 
 
 
 
  is a symmetric matrix).
This condition easily implies that all eigenvalues of a Hermitian map are real: it is enough to apply it to the case when 
 
 
 
 
  is an eigenvector. (Recall that an eigenvector of a linear map 
 
 
 
  is a (non-zero) vector 
 
 
 
  such that 
 
 
 
  for some scalar 
 
 
 
 . The value 
 
 
 
 
  is the corresponding eigenvalue. Moreover, the eigenvalues are solutions to the characteristic polynomial.)
Theorem. There exists an orthonormal basis of 
 
 
 
  consisting of eigenvectors of 
 
 
 
 . Each eigenvalue is real.
We provide a sketch of a proof for the case where the underlying field of scalars is the complex numbers.
By the fundamental theorem of algebra, applied to the characteristic polynomial of 
 
 
 
 , there is at least one eigenvalue  and eigenvector . Then since


 
  we find that  is real. Now consider the space , the orthogonal complement of . By Hermiticity, 
 
 
 
 
  is an invariant subspace of 
 
 
 
 . Applying the same argument to 
 
 
 
  shows that 
 
 
 
  has an eigenvector . Finite induction then finishes the proof.
The spectral theorem holds also for symmetric maps on finite-dimensional real inner product spaces, but the existence of an eigenvector does not follow immediately from the fundamental theorem of algebra. The easiest way to prove it is probably to consider 
 
 
 
  as a Hermitian matrix and use the fact that all eigenvalues of a Hermitian matrix are real.
If one chooses the eigenvectors of 
 
 
 
 
  as an orthonormal basis, the matrix representation of 
 
 
 
  in this basis is diagonal. Equivalently, 
 
 
 
  can be written as a linear combination of pairwise orthogonal projections, called its spectral decomposition. Let



be the eigenspace corresponding to an eigenvalue 
 
 
 
 . Note that the definition does not depend on any choice of specific eigenvectors. 
 
 
 
 
  is the orthogonal direct sum of the spaces  where the index ranges over eigenvalues. Let  be the orthogonal projection onto  and  the eigenvalues of 
 
 
 
 , one can write its spectral decomposition thus:



The spectral decomposition is a special case of both the Schur decomposition and the singular value decomposition.
Normal matrices
The spectral theorem extends to a more general class of matrices. Let 
 
 
 
  be an operator on a finite-dimensional inner product space. 
 
 
 
  is said to be normal if . One can show that 
 
 
 
 
  is normal if and only if it is unitarily diagonalizable. Proof: By the Schur decomposition, we can write any matrix as , where 
 
 
 
  is unitary and 
 
 
 
  is upper-triangular. If 
 
 
 
  is normal, one sees that . Therefore 
 
 
 
  must be diagonal since a normal upper triangular matrix is diagonal (see normal matrix). The converse is obvious.
In other words, 
 
 
 
 
  is normal if and only if there exists a unitary matrix

 
  such that



where 
 
 
 
  is a diagonal matrix. Then, the entries of the diagonal of 
 
 
 
  are the eigenvalues of 
 
 
 
 
 . The column vectors of 
 
 
 
  are the eigenvectors of 
 
 
 
  and they are orthonormal. Unlike the Hermitian case, the entries of 
 
 
 
  need not be real.
Compact self-adjoint operators
In Hilbert spaces in general, the statement of the spectral theorem for compact self-adjoint operators is virtually the same as in the finite-dimensional case.
Theorem. Suppose 
 
 
 
  is a compact self-adjoint operator on a Hilbert space 
 
 
 
 
 . There is an orthonormal basis of 
 
 
 
  consisting of eigenvectors of 
 
 
 
 . Each eigenvalue is real.
As for Hermitian matrices, the key point is to prove the existence of at least one nonzero eigenvector. To prove this, we cannot rely on determinants to show existence of eigenvalues, but instead one can use a maximization argument analogous to the variational characterization of eigenvalues. The above spectral theorem holds for real or complex Hilbert spaces.
If the compactness assumption is removed, it is not true that every self adjoint operator has eigenvectors.
Bounded self-adjoint operators
The next generalization we consider is that of bounded self-adjoint operators on a Hilbert space. Such operators may have no eigenvalues: for instance let 
 
 
 
  be the operator of multiplication by 
 
 
 
  on , that is,




Theorem:3
Let 
 
 
 
  be a bounded self-adjoint operator on a Hilbert space 
 
 
 
 . Then there is a measure space

 
  and a real-valued essentially bounded measurable function 
 
 
 
  on 
 
 
 
 
  and a unitary operator  such that
 










where 
 
 
 
  is the multiplication operator:
 










and 
 
 


This is the beginning of the vast research area of functional analysis called operator theory; see also the spectral measure.
There is also an analogous spectral theorem for bounded normal operators on Hilbert spaces. The only difference in the conclusion is that now 
 
 
 
 
  may be complex-valued.
An alternative formulation of the spectral theorem expresses the operator 
 
 
 
  as an integral of the coordinate function over the operator's spectrum with respect to a projection-valued measure.



When the normal operator in question is compact, this version of the spectral theorem reduces to something similar to the finite-dimensional spectral theorem above, except that the operator is expressed as a finite or countably infinite linear combination of projections, that is, the measure consists only of atoms.
General self-adjoint operators
Many important linear operators which occur in analysis, such as differential operators, are unbounded. There is also a spectral theorem for self-adjoint operators that applies in these cases. To give an example, any constant coefficient differential operator is unitarily equivalent to a multiplication operator. Indeed the unitary operator that implements this equivalence is the Fourier transform; the multiplication operator is a type of Fourier multiplier.
In general, spectral theorem for self-adjoint operators may take several equivalent forms.

Spectral theorem in the form of multiplication operator: For each self-adjoint operator 
 
 
 
  acting in a Hilbert space 
 
 
 
 , there exists a unitary operator, making an isometrically isomorphic mapping of the Hilbert space 
 
 
 
 
  onto the space , where the operator 
 
 
 
  is represented as a multiplication operator.

The Hilbert space 
 
 
 
  where a self-adjoint operator 
 
 
 
  acts may be decomposed into a direct sum of Hilbert spaces  in such a way that the operator 
 
 
 
 , narrowed to each space , has a simple spectrum. It is possible to construct a unique such decomposition (up to unitary equivalence), which is called an ordered spectral representation.
See also

Spectral theory
Matrix decomposition
Canonical form
Jordan decomposition, of which the spectral decomposition is a special case.
Singular value decomposition, a generalisation of spectral theorem to arbitrary matrices.
Eigendecomposition of a matrix

References

Sheldon Axler, Linear Algebra Done Right, Springer Verlag, 1997
Paul Halmos, "What Does the Spectral Theorem Say?", American Mathematical Monthly, volume 70, number 3 (1963), pages 241–247 Other link
M. Reed and B. Simon, Methods of Mathematical Physics, vols I–IV, Academic Press 1972.
G. Teschl, Mathematical Methods in Quantum Mechanics with Applications to Schrödinger Operators, http://www.mat.univie.ac.at/~gerald/ftp/book-schroe/, American Mathematical Society, 2009.


"
* Category:Linear algebra Category:Matrix theory Category:Singular value decomposition Category:Theorems in functional analysis



Cauchy and the spectral theory of matrices by Thomas Hawkins
A Short History of Operator Theory by Evans M. Harrell II





