   Autocovariance      Autocovariance   In probability and statistics , given a stochastic process     X  =   (   X  t   )       X   subscript  X  t     X=(X_{t})   , the autocovariance is a function that gives the covariance of the process with itself at pairs of time points. With the usual notation E for the expectation operator, if the process has the mean function     μ  t   =   E   [   X  t   ]         subscript  μ  t     E   delimited-[]   subscript  X  t       \mu_{t}=E[X_{t}]   , then the autocovariance is given by         C   X  X     (  t  ,  s  )    =   c  o  v   (   X  t   ,   X  s   )    =   E   [    (    X  t   -   μ  t    )    (    X  s   -   μ  s    )    ]    =    E   [    X  t    X  s    ]    -    μ  t    μ  s      .           subscript  C    X  X     t  s      c  o  v    subscript  X  t    subscript  X  s            E   delimited-[]       subscript  X  t    subscript  μ  t       subscript  X  s    subscript  μ  s                E   delimited-[]     subscript  X  t    subscript  X  s         subscript  μ  t    subscript  μ  s        C_{XX}(t,s)=cov(X_{t},X_{s})=E[(X_{t}-\mu_{t})(X_{s}-\mu_{s})]=E[X_{t}X_{s}]-%
 \mu_{t}\mu_{s}.\,     Autocovariance is related to the more commonly used autocorrelation of the process in question.  In the case of a random vector    X  =   (   X  1   ,   X  2   ,  …  ,   X  n   )       X    subscript  X  1    subscript  X  2   normal-…   subscript  X  n      X=(X_{1},X_{2},...,X_{n})   , the autocovariance would be a square n by n matrix    C   X  X      subscript  C    X  X     C_{XX}   with entries       C   X  X     (  j  ,  k  )    =   c  o  v   (   X  j   ,   X  k   )     .         subscript  C    X  X     j  k      c  o  v    subscript  X  j    subscript  X  k       C_{XX}(j,k)=cov(X_{j},X_{k}).\,   This is commonly known as the covariance matrix or matrix of covariances of the given random vector.  Stationarity  If X ( t ) is stationary process , then the following are true:       μ  t   =   μ  s   =   μ          subscript  μ  t    subscript  μ  s        μ     \mu_{t}=\mu_{s}=\mu\,   for all t , s  and        C   X  X     (  t  ,  s  )    =    C   X  X     (   s  -  t   )    =    C   X  X     (  τ  )             subscript  C    X  X     t  s       subscript  C    X  X      s  t            subscript  C    X  X    τ      C_{XX}(t,s)=C_{XX}(s-t)=C_{XX}(\tau)\,     where      τ  =   |   s  -  t   |       τ      s  t      \tau=|s-t|\,     is the lag time, or the amount of time by which the signal has been shifted.  As a result, the autocovariance becomes        C   X  X     (  τ  )    =   E   [    (    X   (  t  )    -  μ   )    (    X   (   t  +  τ   )    -  μ   )    ]           subscript  C    X  X    τ     E   delimited-[]        X  t   μ       X    t  τ    μ        C_{XX}(\tau)=E[(X(t)-\mu)(X(t+\tau)-\mu)]\,            =    E   [   X   (  t  )   X   (   t  +  τ   )    ]    -    μ  2         absent      E   delimited-[]    X  t  X    t  τ       superscript  μ  2      =E[X(t)X(t+\tau)]-\mu^{2}\,                =     σ  2    R   X  X     (  τ  )    -   μ  2     ,      absent       superscript  σ  2    subscript  R    X  X    τ    superscript  μ  2      =\sigma^{2}R_{XX}(\tau)-\mu^{2},\,        where     R   X  X     (  τ  )        subscript  R    X  X    τ    R_{XX}(\tau)   is the autocorrelation of the signal with variance    σ  2     superscript  σ  2    \sigma^{2}   . Some authors do not normalize the autocorrelation function. 1 In those literatures,       C   X  X     (  τ  )    =     R   X  X     (  τ  )    -   μ  2     ,         subscript  C    X  X    τ        subscript  R    X  X    τ    superscript  μ  2      C_{XX}(\tau)=R_{XX}(\tau)-\mu^{2},\,   .  Normalization  When normalized by dividing by the variance σ 2 , the autocovariance C becomes the autocorrelation  coefficient function c , 2         c   X  X     (  τ  )    =     C   X  X     (  τ  )     σ  2     .         subscript  c    X  X    τ        subscript  C    X  X    τ    superscript  σ  2      c_{XX}(\tau)=\frac{C_{XX}(\tau)}{\sigma^{2}}.\,     However, often the autocovariance is called autocorrelation even if this normalization has not been performed.  The autocovariance can be thought of as a measure of how similar a signal is to a time-shifted version of itself with an autocovariance of σ 2 indicating perfect correlation at that lag. The normalization with the variance will put this into the range [−1, 1].  Properties  The autocovariance of a linearly filtered process    Y  t     subscript  Y  t    Y_{t}          Y  t   =    ∑   k  =   -  ∞    ∞     a  k     X   t  +  k            subscript  Y  t     superscript   subscript     k             subscript  a  k    subscript  X    t  k        Y_{t}=\sum_{k=-\infty}^{\infty}a_{k}X_{t+k}\,      is       C   Y  Y     (  τ  )    =    ∑    k  ,  l   =   -  ∞    ∞     a  k    a  l  *    C   X  X     (    τ  +  k   -  l   )      .         subscript  C    Y  Y    τ     superscript   subscript      k  l              subscript  a  k    subscript   superscript  a    l    subscript  C    X  X        τ  k   l       C_{YY}(\tau)=\sum_{k,l=-\infty}^{\infty}a_{k}a^{*}_{l}C_{XX}(\tau+k-l).\,      See also   Autocorrelation  Covariance and Correlation  Covariance mapping  Cross-covariance  Cross-correlation  Noise covariance estimation (as an application example)   References    Lecture notes on autocovariance from WHOI   "  Category:Covariance and correlation  Category:Time series analysis  Category:Fourier analysis     http://ece-research.unm.edu/bsanthan/ece541/stat.pdf ↩  ↩     