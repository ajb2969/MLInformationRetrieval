   R√©nyi entropy      R√©nyi entropy   In information theory , the R√©nyi entropy generalizes the Hartley entropy , the Shannon entropy , the collision entropy and the min-entropy . Entropies quantify the diversity, uncertainty, or randomness of a system. The R√©nyi entropy is named after Alfr√©d R√©nyi . 1  The R√©nyi entropy is important in ecology and statistics as indices of diversity . The R√©nyi entropy is also important in quantum information , where it can be used as a measure of entanglement . In the Heisenberg XY spin chain model, the R√©nyi entropy as a function of Œ± can be calculated explicitly by virtue of the fact that it is an automorphic function with respect to a particular subgroup of the modular group . 2 3 In theoretical computer science , the min-entropy is used in the context of randomness extractors .  Definition  The R√©nyi entropy of order   Œ±   Œ±   \alpha   , where    Œ±  ‚â•  0      Œ±  0    \alpha\geq 0   and    Œ±  ‚â†  1      Œ±  1    \alpha\neq 1   , is defined as        H  Œ±    (  X  )    =    1   1  -  Œ±     log   (    ‚àë   i  =  1   n    p  i  Œ±    )            subscript  H  Œ±   X       1    1  Œ±        superscript   subscript     i  1    n    superscript   subscript  p  i   Œ±        H_{\alpha}(X)=\frac{1}{1-\alpha}\log\Bigg(\sum_{i=1}^{n}p_{i}^{\alpha}\Bigg)   . 4  Here,   X   X   X   is a discrete random variable with possible outcomes    1  ,  2  ,  ‚Ä¶  ,  n     1  2  normal-‚Ä¶  n    1,2,...,n   and corresponding probabilities     p  i   ‚âê   Pr   (   X  =  i   )       approaches-limit   subscript  p  i    Pr    X  i      p_{i}\doteq\Pr(X=i)   for    i  =   1  ,  ‚Ä¶  ,  n       i   1  normal-‚Ä¶  n     i=1,\dots,n   , and the logarithm is base 2. If the probabilities are     p  i   =   1  /  n        subscript  p  i     1  n     p_{i}=1/n   for all    i  =   1  ,  ‚Ä¶  ,  n       i   1  normal-‚Ä¶  n     i=1,\dots,n   , then all the R√©nyi entropies of the distribution are equal      H  Œ±    (  X  )    =   log  n          subscript  H  Œ±   X     n     H_{\alpha}(X)=\log n   . In general, for all discrete random variables   X   X   X   ,     H  Œ±    (  X  )        subscript  H  Œ±   X    H_{\alpha}(X)   is a non-increasing function in   Œ±   Œ±   \alpha   .  Applications often exploit the following relation between the R√©nyi entropy and the p -norm of the vector of probabilities:        H  Œ±    (  X  )    =    Œ±   1  -  Œ±     log   (    ‚à•  P  ‚à•   Œ±   )            subscript  H  Œ±   X       Œ±    1  Œ±       subscript   norm  P   Œ±       H_{\alpha}(X)=\frac{\alpha}{1-\alpha}\log\left(\|P\|_{\alpha}\right)   . Here, the discrete probability distribution    P  =   (   p  1   ,  ‚Ä¶  ,   p  n   )       P    subscript  p  1   normal-‚Ä¶   subscript  p  n      P=(p_{1},\dots,p_{n})   is interpreted as a vector in     \R   n     superscript  \R  n    \R^{n}   with     p  i   ‚â•  0       subscript  p  i   0    p_{i}\geq 0   and      ‚àë   i  =  1   n    p  i    =  1        superscript   subscript     i  1    n    subscript  p  i    1    \sum_{i=1}^{n}p_{i}=1   .  The R√©nyi entropy for any    Œ±  ‚â•  0      Œ±  0    \alpha\geq 0   is Schur concave .  Special cases of the R√©nyi entropy  As   Œ±   Œ±   \alpha   approaches zero, the R√©nyi entropy increasingly weighs all possible events more equally, regardless of their probabilities. In the limit for    Œ±  ‚Üí  0     normal-‚Üí  Œ±  0    \alpha\to 0   , the R√©nyi entropy is just the logarithm of the size of the support of   X   X   X   . The limit for    Œ±  ‚Üí  1     normal-‚Üí  Œ±  1    \alpha\to 1   is the Shannon entropy . As   Œ±   Œ±   \alpha   approaches infinity, the R√©nyi entropy is increasingly determined by the events of highest probability.  Hartley or max-entropy  Provided the probabilities are nonzero, 5     H  0     subscript  H  0    H_{0}   is the logarithm of the cardinality of X , sometimes called the Hartley entropy of X :         H  0    (  X  )    =   log  n   =   log   |  X  |     .           subscript  H  0   X     n            X       H_{0}(X)=\log n=\log|X|.\,     Shannon entropy  The limiting value of    H  Œ±     subscript  H  Œ±    H_{\alpha}   as    Œ±  ‚Üí  1     normal-‚Üí  Œ±  1    \alpha\rightarrow 1   is the Shannon entropy : 6         H  1    (  X  )    =   -    ‚àë   i  =  1   n     p  i    log   p  i        .         subscript  H  1   X       superscript   subscript     i  1    n      subscript  p  i      subscript  p  i         H_{1}(X)=-\sum_{i=1}^{n}p_{i}\log p_{i}.     Collision entropy  Collision entropy , sometimes just called "R√©nyi entropy," refers to the case    Œ±  =  2      Œ±  2    \alpha=2   ,       H  2    (  X  )   =  -  log   ‚àë   i  =  1   n    p  i  2   =  -  log  P   (  X  =  Y  )      fragments   subscript  H  2    fragments  normal-(  X  normal-)       superscript   subscript     i  1    n    superscript   subscript  p  i   2      P   fragments  normal-(  X   Y  normal-)     H_{2}(X)=-\log\sum_{i=1}^{n}p_{i}^{2}=-\log P(X=Y)     where X and Y are independent and identically distributed .  Min-entropy  In the limit as    Œ±  ‚Üí  ‚àû     normal-‚Üí  Œ±     \alpha\rightarrow\infty   , the R√©nyi entropy    H  Œ±     subscript  H  Œ±    H_{\alpha}   converges to the min-entropy     H  ‚àû     subscript  H     H_{\infty}   :         H  ‚àû    (  X  )    ‚âê    min  i    (   -   log   p  i     )    =   -   (    max  i    log   p  i     )    =   -   log    max  i     p  i        .       approaches-limit     subscript  H    X     subscript   i        subscript  p  i               subscript   i      subscript  p  i                 subscript   i    subscript  p  i         H_{\infty}(X)\doteq\min_{i}(-\log p_{i})=-(\max_{i}\log p_{i})=-\log\max_{i}p_%
 {i}\,.     Equivalently, the min-entropy     H  ‚àû    (  X  )        subscript  H    X    H_{\infty}(X)   is the largest real number   b   b   b   such that all events occur with probability at most    2   -  b      superscript  2    b     2^{-b}   .  The name min-entropy stems from the fact that it is the smallest entropy measure in the family of R√©nyi entropies. In this sense, it is the strongest way to measure the information content of a discrete random variable. In particular, the min-entropy is never larger than the Shannon entropy .  The min-entropy has important applications for randomness extractors in theoretical computer science : Extractors are able to extract randomness from random sources that have a large min-entropy; merely having a large Shannon entropy does not suffice for this task.  Inequalities between different values of Œ±  That    H  Œ±     subscript  H  Œ±    H_{\alpha}   is non-increasing in   Œ±   Œ±   \alpha   , which can be proven by differentiation, 7 as        -    d   H  Œ±     d  Œ±     =    1    (   1  -  Œ±   )   2      ‚àë   i  =  1   n     z  i    log   (    z  i   /   p  i    )        ,            d   subscript  H  Œ±      d  Œ±         1   superscript    1  Œ±   2      superscript   subscript     i  1    n      subscript  z  i        subscript  z  i    subscript  p  i          -\frac{dH_{\alpha}}{d\alpha}=\frac{1}{(1-\alpha)^{2}}\sum_{i=1}^{n}z_{i}\log(z%
 _{i}/p_{i}),   which is proportional to Kullback‚ÄìLeibler divergence (which is always non-negative), where     z  i   =    p  i  Œ±   /    ‚àë   j  =  1   n    p  j  Œ±          subscript  z  i      superscript   subscript  p  i   Œ±     superscript   subscript     j  1    n    superscript   subscript  p  j   Œ±       z_{i}=p_{i}^{\alpha}/\sum_{j=1}^{n}p_{j}^{\alpha}   .  In particular cases inequalities can be proven also by Jensen's inequality : 8 9        log  n   =   H  0   ‚â•   H  1   ‚â•   H  2   ‚â•   H  ‚àû    .          n    subscript  H  0         subscript  H  1         subscript  H  2         subscript  H       \log n=H_{0}\geq H_{1}\geq H_{2}\geq H_{\infty}.     For values of    Œ±  >  1      Œ±  1    \alpha>1   , inequalities in the other direction also hold. In particular, we have 10        H  2   ‚â§   2   H  ‚àû     .       subscript  H  2     2   subscript  H       H_{2}\leq 2H_{\infty}.     On the other hand, the Shannon entropy    H  1     subscript  H  1    H_{1}   can be arbitrarily high for a random variable   X   X   X   that has a given min-entropy.  R√©nyi divergence  As well as the absolute R√©nyi entropies, R√©nyi also defined a spectrum of divergence measures generalising the Kullback‚ÄìLeibler divergence . 11  The R√©nyi divergence of order Œ± , where , of a distribution P from a distribution Q is defined to be:       D  Œ±    (  P  ‚à•  Q  )   =   1   Œ±  -  1    log   (   ‚àë   i  =  1   n     p  i  Œ±    q  i   Œ±  -  1     )   =   1   Œ±  -  1    log   ‚àë   i  =  1   n    p  i  Œ±    q  i   1  -  Œ±    .     fragments   subscript  D  Œ±    fragments  normal-(  P  parallel-to  Q  normal-)      1    Œ±  1      fragments  normal-(   superscript   subscript     i  1    n      superscript   subscript  p  i   Œ±    superscript   subscript  q  i     Œ±  1     normal-)      1    Œ±  1      superscript   subscript     i  1    n    superscript   subscript  p  i   Œ±    superscript   subscript  q  i     1  Œ±    normal-.    D_{\alpha}(P\|Q)=\frac{1}{\alpha-1}\log\Bigg(\sum_{i=1}^{n}\frac{p_{i}^{\alpha%
 }}{q_{i}^{\alpha-1}}\Bigg)=\frac{1}{\alpha-1}\log\sum_{i=1}^{n}p_{i}^{\alpha}q%
 _{i}^{1-\alpha}.\,     Like the Kullback-Leibler divergence, the R√©nyi divergences are non-negative for . This divergence is also known as the alpha-divergence ( Œ± -divergence).  Some special cases:       D  0    (  P  ‚à•  Q  )   =  -  log  Q   (   {  i  :   p  i   >  0  }   )      fragments   subscript  D  0    fragments  normal-(  P  parallel-to  Q  normal-)      Q   fragments  normal-(   fragments  normal-{  i  normal-:   subscript  p  i    0  normal-}   normal-)     D_{0}(P\|Q)=-\log Q(\{i:p_{i}>0\})   : minus the log probability under Q that ;       D   1  /  2     (  P  ‚à•  Q  )   =  -  2  log   ‚àë   i  =  1   n      p  i    q  i        fragments   subscript  D    1  2     fragments  normal-(  P  parallel-to  Q  normal-)     2    superscript   subscript     i  1    n        subscript  p  i    subscript  q  i       D_{1/2}(P\|Q)=-2\log\sum_{i=1}^{n}\sqrt{p_{i}q_{i}}   : minus twice the logarithm of the Bhattacharyya coefficient ; ()       D  1    (  P  ‚à•  Q  )   =   ‚àë   i  =  1   n    p  i   log    p  i    q  i       fragments   subscript  D  1    fragments  normal-(  P  parallel-to  Q  normal-)     superscript   subscript     i  1    n    subscript  p  i       subscript  p  i    subscript  q  i      D_{1}(P\|Q)=\sum_{i=1}^{n}p_{i}\log\frac{p_{i}}{q_{i}}   : the Kullback-Leibler divergence ;       D  2    (  P  ‚à•  Q  )   =  log   ‚ü®    p  i    q  i    ‚ü©      fragments   subscript  D  2    fragments  normal-(  P  parallel-to  Q  normal-)      fragments  normal-‚ü®     subscript  p  i    subscript  q  i    normal-‚ü©     D_{2}(P\|Q)=\log\Big\langle\frac{p_{i}}{q_{i}}\Big\rangle   : the log of the expected ratio of the probabilities;       D  ‚àû    (  P  ‚à•  Q  )   =  log   sup  i     p  i    q  i       fragments   subscript  D     fragments  normal-(  P  parallel-to  Q  normal-)      subscript  supremum  i      subscript  p  i    subscript  q  i      D_{\infty}(P\|Q)=\log\sup_{i}\frac{p_{i}}{q_{i}}   : the log of the maximum ratio of the probabilities.  For any fixed distributions P and Q , R√©nyi divergence is nondecreasing as a function of its order Œ±, and it is continuous on the set of Œ± for which it is finite. 12  ==Why Œ±=1 is special == The value Œ± = 1, which gives the Shannon entropy and the Kullback‚ÄìLeibler divergence , is special because it is only at Œ±=1 that the chain rule of conditional probability holds exactly:      H   (  A  ,  X  )   =  H   (  A  )   +   ùîº   a  ‚àº  A     [  H   (  X  |  A  =  a  )   ]      fragments  H   fragments  normal-(  A  normal-,  X  normal-)    H   fragments  normal-(  A  normal-)     subscript  ùîº   similar-to  a  A     fragments  normal-[  H   fragments  normal-(  X  normal-|  A   a  normal-)   normal-]     H(A,X)=H(A)+\mathbb{E}_{a\sim A}\big[H(X|A=a)\big]     for the absolute entropies, and       D  KL    (  p   (  x  |  a  )   p   (  a  )   |  |  m   (  x  ,  a  )   )   =   D  KL    (  p   (  a  )   |  |  m   (  a  )   )   +   ùîº   p   (  a  )      {   D  KL    (  p   (  x  |  a  )   |  |  m   (  x  |  a  )   )   }   ,     fragments   subscript  D  KL    fragments  normal-(  p   fragments  normal-(  x  normal-|  a  normal-)   p   fragments  normal-(  a  normal-)   normal-|  normal-|  m   fragments  normal-(  x  normal-,  a  normal-)   normal-)     subscript  D  KL    fragments  normal-(  p   fragments  normal-(  a  normal-)   normal-|  normal-|  m   fragments  normal-(  a  normal-)   normal-)     subscript  ùîº    p  a     fragments  normal-{   subscript  D  KL    fragments  normal-(  p   fragments  normal-(  x  normal-|  a  normal-)   normal-|  normal-|  m   fragments  normal-(  x  normal-|  a  normal-)   normal-)   normal-}   normal-,    D_{\mathrm{KL}}(p(x|a)p(a)||m(x,a))=D_{\mathrm{KL}}(p(a)||m(a))+\mathbb{E}_{p(%
 a)}\{D_{\mathrm{KL}}(p(x|a)||m(x|a))\},     for the relative entropies.  The latter in particular means that if we seek a distribution p ( x , a ) which minimizes the divergence from some underlying prior measure m ( x , a ), and we acquire new information which only affects the distribution of a , then the distribution of p ( x | a ) remains m ( x | a ), unchanged.  The other R√©nyi divergences satisfy the criteria of being positive and continuous; being invariant under 1-to-1 co-ordinate transformations; and of combining additively when A and X are independent, so that if p ( A , X ) = p ( A ) p ( X ), then        H  Œ±    (  A  ,  X  )    =     H  Œ±    (  A  )    +    H  Œ±    (  X  )            subscript  H  Œ±    A  X         subscript  H  Œ±   A      subscript  H  Œ±   X      H_{\alpha}(A,X)=H_{\alpha}(A)+H_{\alpha}(X)\;     and       D  Œ±    (  P   (  A  )   P   (  X  )   ‚à•  Q   (  A  )   Q   (  X  )   )   =   D  Œ±    (  P   (  A  )   ‚à•  Q   (  A  )   )   +   D  Œ±    (  P   (  X  )   ‚à•  Q   (  X  )   )   .     fragments   subscript  D  Œ±    fragments  normal-(  P   fragments  normal-(  A  normal-)   P   fragments  normal-(  X  normal-)   parallel-to  Q   fragments  normal-(  A  normal-)   Q   fragments  normal-(  X  normal-)   normal-)     subscript  D  Œ±    fragments  normal-(  P   fragments  normal-(  A  normal-)   parallel-to  Q   fragments  normal-(  A  normal-)   normal-)     subscript  D  Œ±    fragments  normal-(  P   fragments  normal-(  X  normal-)   parallel-to  Q   fragments  normal-(  X  normal-)   normal-)   normal-.    D_{\alpha}(P(A)P(X)\|Q(A)Q(X))=D_{\alpha}(P(A)\|Q(A))+D_{\alpha}(P(X)\|Q(X)).     The stronger properties of the Œ± = 1 quantities, which allow the definition of conditional information and mutual information from communication theory, may be very important in other applications, or entirely unimportant, depending on those applications' requirements.  Exponential families  The R√©nyi entropies and divergences for an exponential family admit simple expressions 13        H  Œ±    (    p  F    (  x  ;  Œ∏  )    )    =    1   1  -  Œ±     (     F   (   Œ±  Œ∏   )    -   Œ±  F   (  Œ∏  )     +    log   E  p     [   e    (   Œ±  -  1   )   k   (  x  )     ]     )           subscript  H  Œ±      subscript  p  F    x  Œ∏         1    1  Œ±          F    Œ±  Œ∏      Œ±  F  Œ∏         subscript  E  p     delimited-[]   superscript  e      Œ±  1   k  x          H_{\alpha}(p_{F}(x;\theta))=\frac{1}{1-\alpha}\left(F(\alpha\theta)-\alpha F(%
 \theta)+\log E_{p}[e^{(\alpha-1)k(x)}]\right)     and       D  Œ±    (  p  :  q  )   =     J   F  ,  Œ±     (  Œ∏  :   Œ∏  ‚Ä≤   )     1  -  Œ±       fragments   subscript  D  Œ±    fragments  normal-(  p  normal-:  q  normal-)       fragments   subscript  J   F  Œ±     fragments  normal-(  Œ∏  normal-:   superscript  Œ∏  normal-‚Ä≤   normal-)      1  Œ±      D_{\alpha}(p:q)=\frac{J_{F,\alpha}(\theta:\theta^{\prime})}{1-\alpha}   where       J   F  ,  Œ±     (  Œ∏  :   Œ∏  ‚Ä≤   )   =  Œ±  F   (  Œ∏  )   +   (  1  -  Œ±  )   F   (   Œ∏  ‚Ä≤   )   -  F   (  Œ±  Œ∏  +   (  1  -  Œ±  )    Œ∏  ‚Ä≤   )      fragments   subscript  J   F  Œ±     fragments  normal-(  Œ∏  normal-:   superscript  Œ∏  normal-‚Ä≤   normal-)    Œ±  F   fragments  normal-(  Œ∏  normal-)     fragments  normal-(  1   Œ±  normal-)   F   fragments  normal-(   superscript  Œ∏  normal-‚Ä≤   normal-)    F   fragments  normal-(  Œ±  Œ∏    fragments  normal-(  1   Œ±  normal-)    superscript  Œ∏  normal-‚Ä≤   normal-)     J_{F,\alpha}(\theta:\theta^{\prime})=\alpha F(\theta)+(1-\alpha)F(\theta^{%
 \prime})-F(\alpha\theta+(1-\alpha)\theta^{\prime})   is a Jensen difference divergence.  Physical meaning  Renyi entropy in quantum physics is considered unphysical, or non-observable, due to its nonlinear dependence on density matrix. So is the Shannon entropy. Recently, Nazarov showed a correspondence that reveals the physical meaning of the Renyi entropy flow in time. His proposal is similar to the fluctuation-dissipation theorem in spirit and allows to measure quantum entropy using full counting statistics (FCS) of energy transfers. 14  See also   Diversity indices  Tsallis entropy  Generalized entropy index   Notes  References                                                "  Category:Information theory  Category:Entropy and information     ‚Ü©  ‚Ü©  ‚Ü©   RFC 4086, page 6 ‚Ü©  ‚Ü©  ‚Ü©       H  1   ‚â•   H  2        subscript  H  1    subscript  H  2     H_{1}\geq H_{2}   holds because      ‚àë   i  =  1   M     p  i    log   p  i      ‚â§   log    ‚àë   i  =  1   M    p  i  2           superscript   subscript     i  1    M      subscript  p  i      subscript  p  i           superscript   subscript     i  1    M    superscript   subscript  p  i   2       \sum\limits_{i=1}^{M}{p_{i}\log p_{i}}\leq\log\sum\limits_{i=1}^{M}{p_{i}^{2}}   . ‚Ü©       H  ‚àû   ‚â§   H  2        subscript  H     subscript  H  2     H_{\infty}\leq H_{2}   holds because     log    ‚àë   i  =  1   n    p  i  2     ‚â§   log    sup  i     p  i    (    ‚àë   i  =  1   n    p  i    )      =   log   sup   p  i                superscript   subscript     i  1    n    superscript   subscript  p  i   2          subscript  supremum  i      subscript  p  i     superscript   subscript     i  1    n    subscript  p  i                supremum   subscript  p  i        \log\sum\limits_{i=1}^{n}{p_{i}^{2}}\leq\log\sup_{i}p_{i}\left({\sum\limits_{i%
 =1}^{n}{p_{i}}}\right)=\log\sup p_{i}   . ‚Ü©       H  2   ‚â§   2   H  ‚àû         subscript  H  2     2   subscript  H       H_{2}\leq 2H_{\infty}   holds because     log    ‚àë   i  =  1   n    p  i  2     ‚â•   log    sup  i    p  i  2     =   2  log    sup  i    p  i                superscript   subscript     i  1    n    superscript   subscript  p  i   2          subscript  supremum  i    superscript   subscript  p  i   2            2     subscript  supremum  i    subscript  p  i        \log\sum\limits_{i=1}^{n}{p_{i}^{2}}\geq\log\sup_{i}p_{i}^{2}=2\log\sup_{i}p_{i}    ‚Ü©  ‚Ü©   ‚Ü©  , , ‚Ü©     