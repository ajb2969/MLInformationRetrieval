   Bernoulli distribution      Bernoulli distribution   |  kurtosis   =      1  -   6  p  q     p  q         1    6  p  q      p  q     \frac{1-6pq}{pq}    |  entropy    =      -   q   ln   (  q  )      -   p   ln   (  p  )             q    q       p    p      -q\ln(q)-p\ln(p)\,    |  mgf        =     q  +   p    e  t         q    p   superscript  e  t      q+pe^{t}\,    |  char       =     q  +   p    e   i  t          q    p   superscript  e    i  t       q+pe^{it}\,    |  pgf =     q  +   p   z        q    p  z     q+pz\,    |  fisher =     1   p   (   1  -  p   )        1    p    1  p      \frac{1}{p(1-p)}    |  }}  In probability theory and statistics , the Bernoulli distribution , named after Swiss scientist Jacob Bernoulli , is the probability distribution of a random variable which takes value 1 with success probability   p   p   p   and value 0 with failure probability    q  =   1  -  p       q    1  p     q=1-p   . It can be used, for example, to represent the toss of a (not necessarily fair) coin, where "1" is defined to mean "heads" and "0" is defined to mean "tails" (or vice versa).  The Bernoulli distribution is a special case of the two-point distribution , for which the two possible outcomes need not be 0 and 1.  Properties  If   X   X   X   is a random variable with this distribution, we have:      P  r   (  X  =  1  )   =  1  -  P  r   (  X  =  0  )   =  1  -  q  =  p  .     fragments  P  r   fragments  normal-(  X   1  normal-)    1   P  r   fragments  normal-(  X   0  normal-)    1   q   p  normal-.    Pr(X=1)=1-Pr(X=0)=1-q=p.\!     A classic example of a Bernoulli experiment is a single toss of a coin. The coin might come up heads with probability   p   p   p   and tails with probability    1  -  p      1  p    1-p   . The experiment is called fair if    p  =  0.5      p  0.5    p=0.5   , indicating the origin of the terminology in betting (the bet is fair if both possible outcomes have the same probability).  The probability mass function    f   f   f   of this distribution, over possible outcomes k , is       f   (  k  ;  p  )    =   {     p       if  k   =  1   ,        1  -  p       if  k   =  0.             f   k  p     cases  p      if  k   1     1  p       if  k   0.      f(k;p)=\begin{cases}p&\text{if }k=1,\\
 1-p&\text{if }k=0.\end{cases}     This can also be expressed as        f   (  k  ;  p  )    =    p  k      (   1  -  p   )    1  -  k     for  k   ∈   {  0  ,  1  }    .          f   k  p       superscript  p  k    superscript    1  p     1  k    for  k         0  1      f(k;p)=p^{k}(1-p)^{1-k}\!\quad\text{for }k\in\{0,1\}.     The expected value of a Bernoulli random variable   X   X   X   is       E   (  X  )    =  p        E  X   p    E\left(X\right)=p     and its variance is        Var   (  X  )    =   p   (   1  -  p   )     .        Var  X     p    1  p      \textrm{Var}\left(X\right)=p\left(1-p\right).     The Bernoulli distribution is a special case of the binomial distribution with    n  =  1      n  1    n=1   . 1  The kurtosis goes to infinity for high and low values of   p   p   p   , but for    p  =   1  /  2       p    1  2     p=1/2   the two-point distributions including the Bernoulli distribution have a lower excess kurtosis than any other probability distribution, namely −2.  The Bernoulli distributions for    0  ≤  p  ≤  1        0  p       1     0\leq p\leq 1   form an exponential family .  The maximum likelihood estimator of   p   p   p   based on a random sample is the sample mean .  Related distributions   If     X  1   ,  …  ,   X  n       subscript  X  1   normal-…   subscript  X  n     X_{1},\dots,X_{n}   are independent, identically distributed ( i.i.d. ) random variables, all Bernoulli distributed with success probability p , then       Y  =    ∑   k  =  1   n    X  k    ∼   B   (  n  ,  p  )          Y    superscript   subscript     k  1    n    subscript  X  k      similar-to      normal-B   n  p       Y=\sum_{k=1}^{n}X_{k}\sim\mathrm{B}(n,p)   ( binomial distribution ). The Bernoulli distribution is simply    B   (  1  ,  p  )       normal-B   1  p     \mathrm{B}(1,p)   .   The categorical distribution is the generalization of the Bernoulli distribution for variables with any constant number of discrete values.  The Beta distribution is the conjugate prior of the Bernoulli distribution.  The geometric distribution models the number of independent and identical Bernoulli trials needed to get one success.  If Y ~ Bernoulli(0.5), then (2 Y -1) has a Rademacher distribution .   See also   Bernoulli process  Bernoulli sampling  Bernoulli trial  Binary entropy function   Notes  References      Johnson, N.L., Kotz, S., Kemp A. (1993) Univariate Discrete Distributions (2nd Edition). Wiley. ISBN 0-471-54897-9   External links     Interactive graphic: Univariate Distribution Relationships   "   Category:Discrete distributions  Category:Distributions with conjugate priors  Category:Exponential family distributions  Category:Probability distributions     McCullagh and Nelder (1989) , Section 4.2.2. ↩     