   Kullback's inequality      Kullback's inequality   In information theory and statistics , Kullback's inequality is a lower bound on the Kullback–Leibler divergence expressed in terms of the large deviations  rate function . 1 If P and Q are probability distributions on the real line, such that P is absolutely continuous with respect to Q , i.e. P D_{KL}(P\|Q) \ge \Psi_Q^*(\mu'_1(P)), where    Ψ  Q  *     superscript   subscript  normal-Ψ  Q      \Psi_{Q}^{*}   is the rate function, i.e. the convex conjugate of the cumulant -generating function, of   Q   Q   Q   , and     μ  1  ′    (  P  )        subscript   superscript  μ  normal-′   1   P    \mu^{\prime}_{1}(P)   is the first moment of    P  .    P   P.     The Cramér–Rao bound is a corollary of this result.  Proof  Let P and Q be probability distributions (measures) on the real line, whose first moments exist, and such that [[Absolutely_continuous#Absolute_continuity_of_measures| P Q_\theta(A) = \frac{\int_A e^{\theta x}Q(dx)}{\int_{-\infty}^\infty e^{\theta x}Q(dx)}  = \frac{1}{M_Q(\theta)} \int_A e^{\theta x}Q(dx)  for every measurable set A , where    M  Q     subscript  M  Q    M_{Q}   is the moment-generating function of Q . (Note that Q 0 = Q .) Then       D   K  L     (  P  ∥  Q  )   =   D   K  L     (  P  ∥   Q  θ   )   +   ∫   supp  P     (  log    d   Q  θ     d  Q    )   d  P  .     fragments   subscript  D    K  L     fragments  normal-(  P  parallel-to  Q  normal-)     subscript  D    K  L     fragments  normal-(  P  parallel-to   subscript  Q  θ   normal-)     subscript     supp  P     fragments  normal-(       normal-d   subscript  Q  θ      normal-d  Q    normal-)   d  P  normal-.    D_{KL}(P\|Q)=D_{KL}(P\|Q_{\theta})+\int_{\mathrm{supp}P}\left(\log\frac{%
 \mathrm{d}Q_{\theta}}{\mathrm{d}Q}\right)\mathrm{d}P.   By Gibbs' inequality we have     D   K  L     (  P  ∥   Q  θ   )   ≥  0     fragments   subscript  D    K  L     fragments  normal-(  P  parallel-to   subscript  Q  θ   normal-)    0    D_{KL}(P\|Q_{\theta})\geq 0   so that       D   K  L     (  P  ∥  Q  )   ≥   ∫   supp  P     (  log    d   Q  θ     d  Q    )   d  P  =   ∫   supp  P     (  log    e   θ  x      M  Q    (  θ  )     )   P   (  d  x  )      fragments   subscript  D    K  L     fragments  normal-(  P  parallel-to  Q  normal-)     subscript     supp  P     fragments  normal-(       normal-d   subscript  Q  θ      normal-d  Q    normal-)   d  P    subscript     supp  P     fragments  normal-(      superscript  e    θ  x       subscript  M  Q   θ    normal-)   P   fragments  normal-(  d  x  normal-)     D_{KL}(P\|Q)\geq\int_{\mathrm{supp}P}\left(\log\frac{\mathrm{d}Q_{\theta}}{%
 \mathrm{d}Q}\right)\mathrm{d}P=\int_{\mathrm{supp}P}\left(\log\frac{e^{\theta x%
 }}{M_{Q}(\theta)}\right)P(dx)   Simplifying the right side, we have, for every real θ where       M  Q    (  θ  )    <  ∞   :      normal-:       subscript  M  Q   θ     absent    M_{Q}(\theta)<\infty:          D   K  L     (  P  ∥  Q  )   ≥   μ  1  ′    (  P  )   θ  -   Ψ  Q    (  θ  )   ,     fragments   subscript  D    K  L     fragments  normal-(  P  parallel-to  Q  normal-)     subscript   superscript  μ  normal-′   1    fragments  normal-(  P  normal-)   θ    subscript  normal-Ψ  Q    fragments  normal-(  θ  normal-)   normal-,    D_{KL}(P\|Q)\geq\mu^{\prime}_{1}(P)\theta-\Psi_{Q}(\theta),   where     μ  1  ′    (  P  )        subscript   superscript  μ  normal-′   1   P    \mu^{\prime}_{1}(P)   is the first moment, or mean, of P , and     Ψ  Q   =   log   M  Q         subscript  normal-Ψ  Q      subscript  M  Q      \Psi_{Q}=\log M_{Q}   is called the cumulant-generating function . Taking the supremum completes the process of convex conjugation and yields the rate function :       D   K  L     (  P  ∥  Q  )   ≥   sup  θ    {   μ  1  ′    (  P  )   θ  -   Ψ  Q    (  θ  )   }   =   Ψ  Q  *    (   μ  1  ′    (  P  )   )   .     fragments   subscript  D    K  L     fragments  normal-(  P  parallel-to  Q  normal-)     subscript  supremum  θ    fragments  normal-{   subscript   superscript  μ  normal-′   1    fragments  normal-(  P  normal-)   θ    subscript  normal-Ψ  Q    fragments  normal-(  θ  normal-)   normal-}     superscript   subscript  normal-Ψ  Q      fragments  normal-(   subscript   superscript  μ  normal-′   1    fragments  normal-(  P  normal-)   normal-)   normal-.    D_{KL}(P\|Q)\geq\sup_{\theta}\left\{\mu^{\prime}_{1}(P)\theta-\Psi_{Q}(\theta)%
 \right\}=\Psi_{Q}^{*}(\mu^{\prime}_{1}(P)).     Corollary: the Cramér–Rao bound  Start with Kullback's inequality  Let X θ be a family of probability distributions on the real line indexed by the real parameter θ, and satisfying certain regularity conditions . Then         lim   h  →  0       D   K  L     (   X   θ  +  h    ∥   X  θ   )     h  2     ≥    lim   h  →  0       Ψ  θ  *    (   μ   θ  +  h    )     h  2      ,        subscript    normal-→  h  0       fragments   subscript  D    K  L     fragments  normal-(   subscript  X    θ  h    parallel-to   subscript  X  θ   normal-)     superscript  h  2       subscript    normal-→  h  0         subscript   superscript  normal-Ψ    θ    subscript  μ    θ  h      superscript  h  2       \lim_{h\rightarrow 0}\frac{D_{KL}(X_{\theta+h}\|X_{\theta})}{h^{2}}\geq\lim_{h%
 \rightarrow 0}\frac{\Psi^{*}_{\theta}(\mu_{\theta+h})}{h^{2}},     where    Ψ  θ  *     subscript   superscript  normal-Ψ    θ    \Psi^{*}_{\theta}   is the convex conjugate of the cumulant-generating function of    X  θ     subscript  X  θ    X_{\theta}   and    μ   θ  +  h      subscript  μ    θ  h     \mu_{\theta+h}   is the first moment of     X   θ  +  h    .     subscript  X    θ  h     X_{\theta+h}.     Left side  The left side of this inequality can be simplified as follows:        lim   h  →  0       D   K  L     (   X   θ  +  h    ∥   X  θ   )     h  2     =    lim   h  →  0      1   h  2      ∫   -  ∞   ∞     (   log    d   X   θ  +  h      d   X  θ      )   d   X   θ  +  h              subscript    normal-→  h  0       fragments   subscript  D    K  L     fragments  normal-(   subscript  X    θ  h    parallel-to   subscript  X  θ   normal-)     superscript  h  2       subscript    normal-→  h  0        1   superscript  h  2      superscript   subscript                  normal-d   subscript  X    θ  h       normal-d   subscript  X  θ      normal-d   subscript  X    θ  h          \lim_{h\rightarrow 0}\frac{D_{KL}(X_{\theta+h}\|X_{\theta})}{h^{2}}=\lim_{h%
 \rightarrow 0}\frac{1}{h^{2}}\int_{-\infty}^{\infty}\left(\log\frac{\mathrm{d}%
 X_{\theta+h}}{\mathrm{d}X_{\theta}}\right)\mathrm{d}X_{\theta+h}           =    lim   h  →  0      1   h  2      ∫   -  ∞   ∞     [    (   1  -    d   X  θ     d   X   θ  +  h       )   +    1  2     (   1  -    d   X  θ     d   X   θ  +  h       )   2    +   o   (    (   1  -    d   X  θ     d   X   θ  +  h       )   2   )     ]   d   X   θ  +  h         ,      absent    subscript    normal-→  h  0        1   superscript  h  2      superscript   subscript             delimited-[]      1      normal-d   subscript  X  θ      normal-d   subscript  X    θ  h           1  2    superscript    1      normal-d   subscript  X  θ      normal-d   subscript  X    θ  h       2      o   superscript    1      normal-d   subscript  X  θ      normal-d   subscript  X    θ  h       2      normal-d   subscript  X    θ  h          =\lim_{h\rightarrow 0}\frac{1}{h^{2}}\int_{-\infty}^{\infty}\left[\left(1-%
 \frac{\mathrm{d}X_{\theta}}{\mathrm{d}X_{\theta+h}}\right)+\frac{1}{2}\left(1-%
 \frac{\mathrm{d}X_{\theta}}{\mathrm{d}X_{\theta+h}}\right)^{2}+o\left(\left(1-%
 \frac{\mathrm{d}X_{\theta}}{\mathrm{d}X_{\theta+h}}\right)^{2}\right)\right]%
 \mathrm{d}X_{\theta+h},       where we have expanded the logarithm    log  x      x    \log x   in a Taylor series in    1  -   1  /  x       1    1  x     1-1/x   ,    = \lim_{h\rightarrow 0} \frac 1 {h^2} \int_{-\infty}^\infty \left[   \frac 1 2 \left( 1 - \frac{\mathrm dX_\theta}{\mathrm dX_{\theta+h}} \right) ^ 2  \right]\mathrm dX_{\theta+h}         =    lim   h  →  0      1   h  2      ∫   -  ∞   ∞     [    1  2     (     d   X   θ  +  h     -   d   X  θ      d   X   θ  +  h      )   2    ]   d   X   θ  +  h        =    1  2    ℐ  X    (  θ  )     ,        absent    subscript    normal-→  h  0        1   superscript  h  2      superscript   subscript             delimited-[]      1  2    superscript        normal-d   subscript  X    θ  h       normal-d   subscript  X  θ       normal-d   subscript  X    θ  h      2     normal-d   subscript  X    θ  h                 1  2    subscript  ℐ  X   θ      =\lim_{h\rightarrow 0}\frac{1}{h^{2}}\int_{-\infty}^{\infty}\left[\frac{1}{2}%
 \left(\frac{\mathrm{d}X_{\theta+h}-\mathrm{d}X_{\theta}}{\mathrm{d}X_{\theta+h%
 }}\right)^{2}\right]\mathrm{d}X_{\theta+h}=\frac{1}{2}\mathcal{I}_{X}(\theta),   which is half the Fisher information of the parameter θ.  Right side  The right side of the inequality can be developed as follows:         lim   h  →  0       Ψ  θ  *    (   μ   θ  +  h    )     h  2     =    lim   h  →  0      1   h  2      sup  t    {     μ   θ  +  h    t   -    Ψ  θ    (  t  )     }       .        subscript    normal-→  h  0         subscript   superscript  normal-Ψ    θ    subscript  μ    θ  h      superscript  h  2       subscript    normal-→  h  0        1   superscript  h  2      subscript  supremum  t         subscript  μ    θ  h    t      subscript  normal-Ψ  θ   t          \lim_{h\rightarrow 0}\frac{\Psi^{*}_{\theta}(\mu_{\theta+h})}{h^{2}}=\lim_{h%
 \rightarrow 0}\frac{1}{h^{2}}{\sup_{t}\{\mu_{\theta+h}t-\Psi_{\theta}(t)\}}.   This supremum is attained at a value of t =τ where the first derivative of the cumulant-generating function is       Ψ  θ  ′    (  τ  )    =   μ   θ  +  h     ,         subscript   superscript  normal-Ψ  normal-′   θ   τ    subscript  μ    θ  h      \Psi^{\prime}_{\theta}(\tau)=\mu_{\theta+h},   but we have       Ψ  θ  ′    (  0  )    =   μ  θ    ,         subscript   superscript  normal-Ψ  normal-′   θ   0    subscript  μ  θ     \Psi^{\prime}_{\theta}(0)=\mu_{\theta},   so that         Ψ  θ  ′′    (  0  )    =     d   μ  θ     d  θ      lim   h  →  0     h  τ      .         subscript   superscript  normal-Ψ  ′′   θ   0         d   subscript  μ  θ      d  θ      subscript    normal-→  h  0      h  τ       \Psi^{\prime\prime}_{\theta}(0)=\frac{d\mu_{\theta}}{d\theta}\lim_{h%
 \rightarrow 0}\frac{h}{\tau}.   Moreover,         lim   h  →  0       Ψ  θ  *    (   μ   θ  +  h    )     h  2     =    1   2   Ψ  θ  ′′    (  0  )       (    d   μ  θ     d  θ    )   2    =    1   2  V  a  r   (   X  θ   )       (    d   μ  θ     d  θ    )   2     .          subscript    normal-→  h  0         subscript   superscript  normal-Ψ    θ    subscript  μ    θ  h      superscript  h  2         1    2   subscript   superscript  normal-Ψ  ′′   θ   0     superscript      d   subscript  μ  θ      d  θ    2             1    2  normal-V  normal-a  normal-r   subscript  X  θ      superscript      d   subscript  μ  θ      d  θ    2       \lim_{h\rightarrow 0}\frac{\Psi^{*}_{\theta}(\mu_{\theta+h})}{h^{2}}=\frac{1}{%
 2\Psi^{\prime\prime}_{\theta}(0)}\left(\frac{d\mu_{\theta}}{d\theta}\right)^{2%
 }=\frac{1}{2\mathrm{Var}(X_{\theta})}\left(\frac{d\mu_{\theta}}{d\theta}\right%
 )^{2}.     Putting both sides back together  We have:         1  2    ℐ  X    (  θ  )    ≥    1   2  V  a  r   (   X  θ   )       (    d   μ  θ     d  θ    )   2     ,          1  2    subscript  ℐ  X   θ       1    2  normal-V  normal-a  normal-r   subscript  X  θ      superscript      d   subscript  μ  θ      d  θ    2      \frac{1}{2}\mathcal{I}_{X}(\theta)\geq\frac{1}{2\mathrm{Var}(X_{\theta})}\left%
 (\frac{d\mu_{\theta}}{d\theta}\right)^{2},   which can be rearranged as:        Var   (   X  θ   )    ≥     (     d   μ  θ    /  d   θ   )   2     ℐ  X    (  θ  )      .        Var   subscript  X  θ       superscript        d   subscript  μ  θ    d   θ   2      subscript  ℐ  X   θ      \mathrm{Var}(X_{\theta})\geq\frac{(d\mu_{\theta}/d\theta)^{2}}{\mathcal{I}_{X}%
 (\theta)}.     See also   Kullback–Leibler divergence  Cramér–Rao bound  Fisher information  Large deviations theory  Convex conjugate  Rate function  Moment-generating function   Notes and references    "  Category:Information theory  Category:Statistical inequalities  Category:Estimation theory     ↩     