   Genetic algorithm      Genetic algorithm   (Figure)  The 2006 NASA ST5 spacecraft antenna. This complicated shape was found by an evolutionary computer design program to create the best radiation pattern. It is known as an Evolved antenna .   In the field of artificial intelligence , a genetic algorithm (GA) is a search  heuristic that mimics the process of natural selection . This heuristic (also sometimes called a metaheuristic ) is routinely used to generate useful solutions to optimization and search problems . Genetic algorithms belong to the larger class of evolutionary algorithms (EA), which generate solutions to optimization problems using techniques inspired by natural evolution, such as inheritance , mutation , selection , and crossover .  In computer science , engineering , computational physics,  molecular chemistry , statistics and applied probability , genetic algorithms are a class of interacting and nonlinear Monte Carlo methods to sample from complex high-dimensional probability distributions and to estimate their normalizing constants. Genetic particle algorithms approximate the target probability distributions by a large cloud of random samples termed particles or individuals. During the mutation transition, the particles evolve randomly around the space independently and to each particle is associated a fitness weight function. During the selection transitions, such an algorithm duplicates particles with high fitness at the expense of particles with low fitness which die. These genetic type particle samplers belong to the class of mean field particle methods .  In signal processing and Bayesian inference genetic algorithms allow to solve the nonlinear filtering problem . 1 2 These algorithms are called particle filters or sequential Monte Carlo methods . In computational physics and molecular simulation, genetic algorithms are also used to compute Feynman-Kac path integrals as the number of individuals tends to infinity. 3 4 These mutation-selection type algorithms are often called Resample or Reconfiguration Monte Carlo. They belong to the class of Quantum Monte Carlo methods and more specifically Diffusion Monte Carlo methodologies.  Genetic algorithms find application in bioinformatics , phylogenetics , computational science , signal and image processing,  Bayesian inference , machine learning , risk analysis and rare event sampling,  Engineering  and robotics , economics , manufacturing , mathematics , mathematical finance , molecular chemistry , computational physics , pharmacokinetic , pharmacometrics , and other fields.  Methodology  Optimization problems  In a genetic algorithm, a population of candidate solutions (called individuals, creatures, or phenotypes ) to an optimization problem is evolved toward better solutions. Each candidate solution has a set of properties (its chromosomes or genotype ) which can be mutated and altered; traditionally, solutions are represented in binary as strings of 0s and 1s, but other encodings are also possible.  The evolution usually starts from a population of randomly generated individuals, and is an iterative process , with the population in each iteration called a generation . In each generation, the fitness of every individual in the population is evaluated; the fitness is usually the value of the objective function in the optimization problem being solved. The more fit individuals are stochastically selected from the current population, and each individual's genome is modified ( recombined and possibly randomly mutated) to form a new generation. The new generation of candidate solutions is then used in the next iteration of the algorithm . Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population.  A typical genetic algorithm requires:   a genetic representation of the solution domain,  a fitness function to evaluate the solution domain.   A standard representation of each candidate solution is as an array of bits . Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size, which facilitates simple crossover operations. Variable length representations may also be used, but crossover implementation is more complex in this case. Tree-like representations are explored in genetic programming and graph-form representations are explored in evolutionary programming ; a mix of both linear chromosomes and trees is explored in gene expression programming .  Once the genetic representation and the fitness function are defined, a GA proceeds to initialize a population of solutions and then to improve it through repetitive application of the mutation, crossover, inversion and selection operators.  Initialization  The population size depends on the nature of the problem, but typically contains several hundreds or thousands of possible solutions. Often, the initial population is generated randomly, allowing the entire range of possible solutions (the search space ). Occasionally, the solutions may be "seeded" in areas where optimal solutions are likely to be found.  Selection  During each successive generation, a proportion of the existing population is selected to breed a new generation. Individual solutions are selected through a fitness-based process, where fitter solutions (as measured by a fitness function ) are typically more likely to be selected. Certain selection methods rate the fitness of each solution and preferentially select the best solutions. Other methods rate only a random sample of the population, as the former process may be very time-consuming.  The fitness function is defined over the genetic representation and measures the quality of the represented solution. The fitness function is always problem dependent. For instance, in the knapsack problem one wants to maximize the total value of objects that can be put in a knapsack of some fixed capacity. A representation of a solution might be an array of bits, where each bit represents a different object, and the value of the bit (0 or 1) represents whether or not the object is in the knapsack. Not every such representation is valid, as the size of objects may exceed the capacity of the knapsack. The fitness of the solution is the sum of values of all objects in the knapsack if the representation is valid, or 0 otherwise.  In some problems, it is hard or even impossible to define the fitness expression; in these cases, a simulation may be used to determine the fitness function value of a phenotype (e.g. computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype), or even interactive genetic algorithms are used.  Genetic operators  The next step is to generate a second generation population of solutions from those selected through a combination of genetic operators : crossover (also called recombination), and mutation .  For each new solution to be produced, a pair of "parent" solutions is selected for breeding from the pool selected previously. By producing a "child" solution using the above methods of crossover and mutation, a new solution is created which typically shares many of the characteristics of its "parents". New parents are selected for each new child, and the process continues until a new population of solutions of appropriate size is generated. Although reproduction methods that are based on the use of two parents are more "biology inspired", some research 5 6 suggests that more than two "parents" generate higher quality chromosomes.  These processes ultimately result in the next generation population of chromosomes that is different from the initial generation. Generally the average fitness will have increased by this procedure for the population, since only the best organisms from the first generation are selected for breeding, along with a small proportion of less fit solutions. These less fit solutions ensure genetic diversity within the genetic pool of the parents and therefore ensure the genetic diversity of the subsequent generation of children.  Opinion is divided over the importance of crossover versus mutation. There are many references in Fogel (2006) that support the importance of mutation-based search.  Although crossover and mutation are known as the main genetic operators, it is possible to use other operators such as regrouping, colonization-extinction, or migration in genetic algorithms. 7  It is worth tuning parameters such as the mutation probability, crossover probability and population size to find reasonable settings for the problem class being worked on. A very small mutation rate may lead to genetic drift (which is non- ergodic in nature). A recombination rate that is too high may lead to premature convergence of the genetic algorithm. A mutation rate that is too high may lead to loss of good solutions, unless elitist selection is employed.  Termination  This generational process is repeated until a termination condition has been reached. Common terminating conditions are:   A solution is found that satisfies minimum criteria  Fixed number of generations reached  Allocated budget (computation time/money) reached  The highest ranking solution's fitness is reaching or has reached a plateau such that successive iterations no longer produce better results  Manual inspection  Combinations of the above   Monte Carlo integration  A mutation-selection sampler  Simple genetic algorithms with mutation-selection transitions can also be used as to solve complex path integration problems. Suppose we are given some Markov process     X  n     subscript  X  n    X_{n}   evolving in some solution space   S   S   S   at every time step    n  ∈  ℕ      n  ℕ    n\in\mathbb{N}   .  We let      G  n    :   x  ∈  S  ↦    G  n    (  x  )    ∈   [  0  ,  ∞  ]       normal-:   subscript  G  n       x  S    maps-to       subscript  G  n   x         0        G_{n}~{}:~{}x\in S\mapsto G_{n}(x)\in\mathbb{[}0,\infty]   be some bounded fitness functions on the solution space   S   S   S   .  We consider a population of   N   N   N   individuals starting with   N   N   N   independent copies     (   ξ  0  i   )    1  ≤  i  ≤  N      subscript   subscript   superscript  ξ  i   0       1  i       N      \left(\xi^{i}_{0}\right)_{1\leq i\leq N}   of the random variable    X  0     subscript  X  0    X_{0}   . The genetic algorithm is defined the selection-mutation transitions given by the synthetic diagram       ξ  n   :=    (   ξ  n  i   )    1  ≤  i  ≤  N    ∈   S  N      -  -  -  -  -  -  -  -  -  -  ⟶   selection      ξ  ^   n   :=    (    ξ  ^   n  i   )    1  ≤  i  ≤  N    ∈   S  N      -  -  -  -  -  -  -  -  -  -  ⟶   mutation     ξ   n  +  1    :=    (   ξ   n  +  1   i   )    1  ≤  i  ≤  N    ∈   S  N        assign   subscript  ξ  n    subscript   subscript   superscript  ξ  i   n       1  i       N           superscript  S  N      superscript             normal-⟶   selection      subscript   normal-^  ξ   n     assign     subscript   subscript   superscript   normal-^  ξ   i   n       1  i       N           superscript  S  N      superscript             normal-⟶   mutation      subscript  ξ    n  1      assign     subscript   subscript   superscript  ξ  i     n  1        1  i       N           superscript  S  N      \xi_{n}:=\left(\xi^{i}_{n}\right)_{1\leq i\leq N}\in S^{N}\stackrel{\mbox{%
 selection}}{-\!\!\!\!-\!\!\!\!-\!\!\!\!-\!\!\!\!-\!\!\!\!-\!\!\!\!-\!\!\!\!-\!%
 \!\!\!-\!\!\!\!-\!\!\!\!\longrightarrow}~{}\widehat{\xi}_{n}:=\left(\widehat{%
 \xi}^{i}_{n}\right)_{1\leq i\leq N}\in S^{N}\stackrel{\mbox{mutation}}{-\!\!\!%
 \!-\!\!\!\!-\!\!\!\!-\!\!\!\!-\!\!\!\!-\!\!\!\!-\!\!\!\!-\!\!\!\!-\!\!\!\!-\!%
 \!\!\!\longrightarrow}~{}\xi_{n+1}:=\left(\xi^{i}_{n+1}\right)_{1\leq i\leq N}%
 \in S^{N}      During the selection transition we sample N (conditionally) independent random variables      ξ  ^   n   :=    (    ξ  ^   n  i   )    1  ≤  i  ≤  N       assign   subscript   normal-^  ξ   n    subscript   subscript   superscript   normal-^  ξ   i   n       1  i       N       \widehat{\xi}_{n}:=\left(\widehat{\xi}^{i}_{n}\right)_{1\leq i\leq N}   with common (conditional) distribution    P  r  o  b  a   (    ξ  ^   n  i   =    ξ  n  j    |   ξ  n   )   =     G  n    (   ξ  n  j   )       ∑   1  ≤  k  ≤  N       G  n    (   ξ  n  k   )      ,   for each   j  ∈   {  1  ,  …  ,  N  }      fragments  P  r  o  b  a   fragments  normal-(   subscript   superscript   normal-^  ξ   i   n     subscript   superscript  ξ  j   n   normal-|   subscript  ξ  n   normal-)         subscript  G  n    subscript   superscript  ξ  j   n      subscript       1  k       N        subscript  G  n    subscript   superscript  ξ  k   n      normal-,  for each  j    fragments  normal-{  1  normal-,  normal-…  normal-,  N  normal-}     Proba\left(\widehat{\xi}^{i}_{n}=\xi^{j}_{n}~{}|~{}\xi_{n}\right)=\frac{G_{n}(%
 \xi^{j}_{n})}{\sum_{1\leq k\leq N}G_{n}(\xi^{k}_{n})},~{}\mbox{for each}~{}j%
 \in\{1,\ldots,N\}   In other words, if     G  n    (   ξ  n  j   )        subscript  G  n    subscript   superscript  ξ  j   n     G_{n}(\xi^{j}_{n})   is the fitness of the   j   j   j   -th individual    ξ  n  j     subscript   superscript  ξ  j   n    \xi^{j}_{n}   in the population    ξ  n     subscript  ξ  n    \xi_{n}   , its probability of being selected in the next generation     ξ  ^   n     subscript   normal-^  ξ   n    \widehat{\xi}_{n}   is      G  n    (   ξ  n  j   )       ∑   1  ≤  k  ≤  N       G  n    (   ξ  n  k   )            subscript  G  n    subscript   superscript  ξ  j   n      subscript       1  k       N        subscript  G  n    subscript   superscript  ξ  k   n       \frac{G_{n}(\xi^{j}_{n})}{\sum_{1\leq k\leq N}G_{n}(\xi^{k}_{n})}   . This selection transition is also called the fitness-proportionate selection 8 , or the roulette-wheel selection. 9 For    [  0  ,  1  ]     0  1    [0,1]   -valued fitness functions    G  n     subscript  G  n    G_{n}   an alternative acceptance-rejection type selection transition can be underlined: For each    1  ≤  i  ≤  N        1  i       N     1\leq i\leq N   , with a probability     G  n    (   ξ  n  i   )        subscript  G  n    subscript   superscript  ξ  i   n     G_{n}(\xi^{i}_{n})   we accept the individual    ξ  n  i     subscript   superscript  ξ  i   n    \xi^{i}_{n}   ; that is, we set      ξ  ^   n  i   =   ξ  n  i        subscript   superscript   normal-^  ξ   i   n    subscript   superscript  ξ  i   n     \widehat{\xi}^{i}_{n}=\xi^{i}_{n}   . Otherwise, we sample a new indidual     ξ  ~   n  i     subscript   superscript   normal-~  ξ   i   n    \widetilde{\xi}^{i}_{n}   in the current population with the probability measure       P  r  o  b  a   (    ξ  ~   n  i   =    ξ  n  j    |   ξ  n   )   =     G  n    (   ξ  n  j   )       ∑   1  ≤  k  ≤  N       G  n    (   ξ  n  k   )      ,   for each   j  ∈   {  1  ,  …  ,  N  }      fragments  P  r  o  b  a   fragments  normal-(   subscript   superscript   normal-~  ξ   i   n     subscript   superscript  ξ  j   n   normal-|   subscript  ξ  n   normal-)         subscript  G  n    subscript   superscript  ξ  j   n      subscript       1  k       N        subscript  G  n    subscript   superscript  ξ  k   n      normal-,  for each  j    fragments  normal-{  1  normal-,  normal-…  normal-,  N  normal-}     Proba\left(\widetilde{\xi}^{i}_{n}=\xi^{j}_{n}~{}|~{}\xi_{n}\right)=\frac{G_{n%
 }(\xi^{j}_{n})}{\sum_{1\leq k\leq N}G_{n}(\xi^{k}_{n})},~{}\mbox{for each}~{}j%
 \in\{1,\ldots,N\}   and we set       ξ  ^   n  i   =    ξ  ~   n  i    .       subscript   superscript   normal-^  ξ   i   n    subscript   superscript   normal-~  ξ   i   n     \widehat{\xi}^{i}_{n}=\widetilde{\xi}^{i}_{n}.   This selection scheme can be interpreted as an acceptance-rejection sampler equipped with a recycling mechanism. 10   During the mutation transition , from each selected particle     ξ  ^   n  i     subscript   superscript   normal-^  ξ   i   n    \widehat{\xi}^{i}_{n}   we sample independently a transition       ξ  ^   n  i    ⟶   ξ   n  +  1   i      normal-⟶   subscript   superscript   normal-^  ξ   i   n    subscript   superscript  ξ  i     n  1      \widehat{\xi}^{i}_{n}~{}\longrightarrow\xi^{i}_{n+1}   of the Markov chain      X  n    →   X   n  +  1       normal-→   subscript  X  n    subscript  X    n  1      X_{n}~{}\rightarrow~{}X_{n+1}   starting at     X  n   =    ξ  ^   n  i        subscript  X  n    subscript   superscript   normal-^  ξ   i   n     X_{n}=\widehat{\xi}^{i}_{n}   , for    i  ∈   {  1  ,  …  ,  N  }       i   1  normal-…  N     i\in\{1,\ldots,N\}   .   Convergence results  When the number of individuals (and the computational power)    N  ↑  ∞     normal-↑  N     N\uparrow\infty   , for any function     f   :   x  ∈  S  ↦   f   (  x  )    ∈  ℝ      normal-:  f      x  S    maps-to      f  x        ℝ      f~{}:~{}x\in S\mapsto f(x)\in\mathbb{R}   , we have         1  N      ∑   1  ≤  i  ≤  N      f   (   ξ  n  i   )        ≈   N  ↑         E   (   f   (   X  n   )      ∏   0  ≤  k  <  n       G  k    (   X  k   )      )     E   (     ∏   0  ≤  k  <  n       G  k    (   X  k   )     )     and       1  N      ∑   1  ≤  i  ≤  N      f   (    ξ  ^   n  i   )        ≈   N  ↑        E   (   f   (   X  n   )      ∏   0  ≤  k  ≤  n       G  k    (   X  k   )      )     E   (     ∏   0  ≤  k  ≤  n       G  k    (   X  k   )     )         formulae-sequence    subscript    normal-↑  N  absent        1  N     subscript       1  i       N       f   subscript   superscript  ξ  i   n           E    f   subscript  X  n     subscript  product      0  k       n        subscript  G  k    subscript  X  k         E    subscript  product      0  k       n        subscript  G  k    subscript  X  k       and      subscript    normal-↑  N  absent        1  N     subscript       1  i       N       f   subscript   superscript   normal-^  ξ   i   n          E    f   subscript  X  n     subscript  product      0  k       n        subscript  G  k    subscript  X  k         E    subscript  product      0  k       n        subscript  G  k    subscript  X  k          \frac{1}{N}\sum_{1\leq i\leq N}~{}f\left(\xi^{i}_{n}\right)~{}\approx_{N%
 \uparrow}~{}\frac{E\left(f(X_{n})~{}\prod_{0\leq k     In addition, we have the unbiased estimate       ∏   0  ≤  k  ≤  n      1  N     ∑   1  ≤  i  ≤  N      G  k    (   ξ  k  i   )         ≈   N  ↑  ∞      E   (    ∏   0  ≤  k  ≤  n      G  k    (   X  k   )     )        subscript    normal-↑  N       subscript  product      0  k       n         1  N     subscript       1  i       N        subscript  G  k    subscript   superscript  ξ  i   k         E    subscript  product      0  k       n        subscript  G  k    subscript  X  k        \prod_{0\leq k\leq n}\frac{1}{N}\sum_{1\leq i\leq N}G_{k}(\xi^{i}_{k})~{}%
 \approx_{N\uparrow\infty}~{}E\left(\prod_{0\leq k\leq n}G_{k}(X_{k})\right)   In statistics and probability theory the limiting expectations are called Feynman-Kac formulae.  Genealogical tree algorithm   Tracing back in time the ancestral lines  of each individual      ξ  ^   n  i     subscript   superscript   normal-^  ξ   i   n    \widehat{\xi}^{i}_{n}   in the population after the n-th selection, we define the genealogical tree of the genetic population       ξ  ^    0  ,  n   i   ⟵    ξ  ^    1  ,  n   i   ⟵   …    ξ  ^     k  -  1   ,  n   i    ⟵    ξ  ^    k  ,  n   i   ⟵   …    ξ  ^     n  -  1   ,  n   i    ⟵    ξ  ^    n  ,  n   i   =    ξ  ^   n  i        normal-⟵   subscript   superscript   normal-^  ξ   i    0  n     subscript   superscript   normal-^  ξ   i    1  n      normal-⟵      normal-…   subscript   superscript   normal-^  ξ   i      k  1   n       normal-⟵     subscript   superscript   normal-^  ξ   i    k  n      normal-⟵      normal-…   subscript   superscript   normal-^  ξ   i      n  1   n       normal-⟵     subscript   superscript   normal-^  ξ   i    n  n          subscript   superscript   normal-^  ξ   i   n      \widehat{\xi}^{i}_{0,n}\longleftarrow~{}\widehat{\xi}^{i}_{1,n}\longleftarrow~%
 {}\ldots\widehat{\xi}^{i}_{k-1,n}\longleftarrow\widehat{\xi}^{i}_{k,n}%
 \longleftarrow\ldots\widehat{\xi}^{i}_{n-1,n}\longleftarrow\widehat{\xi}^{i}_{%
 n,n}=\widehat{\xi}^{i}_{n}   The lower indices     (  .  )    k  ,  n      subscript   fragments  normal-(  normal-.  normal-)    k  n     (.)_{k,n}   represent the level k of the ancestor of     ξ  ^   n  i     subscript   superscript   normal-^  ξ   i   n    \widehat{\xi}^{i}_{n}   .   When the number of individuals (and the computational power)    N  ↑  ∞     normal-↑  N     N\uparrow\infty   , for any function      f  n    :    (   x  0   ,  …  ,   x  n   )   ∈   S   n  +  1    ↦    f  n    (   x  0   ,  …  ,   x  n   )    ∈  ℝ      normal-:   subscript  f  n         subscript  x  0   normal-…   subscript  x  n     superscript  S    n  1      maps-to       subscript  f  n     subscript  x  0   normal-…   subscript  x  n          ℝ      f_{n}~{}:~{}(x_{0},\ldots,x_{n})\in S^{n+1}\mapsto f_{n}(x_{0},\ldots,x_{n})%
 \in\mathbb{R}   we have the Monte Carlo approximation       1  N      ∑   1  ≤  i  ≤  N       f  n    (    ξ  ^    0  ,  n   i   ,    ξ  ^    1  ,  n   i   ,  …  ,    ξ  ^     n  -  1   ,  n   i   ,    ξ  ^    n  ,  n   i   )        ≈   N  ↑  ∞       E   (    f  n    (   X  0   ,  …  ,   X  n   )      ∏   0  ≤  k  ≤  n       G  k    (   X  k   )      )     E   (     ∏   0  ≤  k  ≤  n       G  k    (   X  k   )     )         subscript    normal-↑  N         1  N     subscript       1  i       N        subscript  f  n     subscript   superscript   normal-^  ξ   i    0  n     subscript   superscript   normal-^  ξ   i    1  n    normal-…   subscript   superscript   normal-^  ξ   i      n  1   n     subscript   superscript   normal-^  ξ   i    n  n            E     subscript  f  n     subscript  X  0   normal-…   subscript  X  n      subscript  product      0  k       n        subscript  G  k    subscript  X  k         E    subscript  product      0  k       n        subscript  G  k    subscript  X  k         \frac{1}{N}\sum_{1\leq i\leq N}~{}f_{n}\left(\widehat{\xi}^{i}_{0,n},\widehat{%
 \xi}^{i}_{1,n},\ldots,\widehat{\xi}^{i}_{n-1,n},\widehat{\xi}^{i}_{n,n}\right)%
 ~{}\approx_{N\uparrow\infty}~{}\frac{E\left(f_{n}(X_{0},\ldots,X_{n})~{}\prod_%
 {0\leq k\leq n}G_{k}(X_{k})\right)}{E\left(\prod_{0\leq k\leq n}G_{k}(X_{k})%
 \right)}   The mathematical foundations and the first rigorous analysis of genetic algorithms for solving Monte Carlo integration problems are due to Pierre Del Moral 11 12 in 1996. The theory on Feynman-Kac genetic type particle algorithms has been developed in 2000 and 2004 in the books. 13 14  Illustrations   These Feynman-Kac path integration models arise a variety of scientific disciplines, including in computational physics, biology, information theory and computer sciences. 15 16 17 Their interpretations depend on the application domain. We illustrate these path integration models with some concrete examples:   If we choose the indicator function      G  n    (   x  n   )    =    1  A    (   x  n   )           subscript  G  n    subscript  x  n       subscript  1  A    subscript  x  n      G_{n}(x_{n})=1_{A}(x_{n})   of some subset of the state space, they represent the conditional distribution of a Markov chain given it stays in a given tube; that is, we have that E\left(f_n(X_0,\ldots,X_n)~|~X_0\in A,~\ldots, X_n\in A\right)   \displaystyle\frac{E\left(f_n(X_0,\ldots,X_n)\prod_{0\leq k\leq n}G_k(X_k)\right)}{E\left(\prod_{0\leq k\leq n}G_k(X_k)\right)} \quad\mbox{and}\quad E\left(\prod_{0\leq k\leq n}G_k(X_k)\right)  P\left(X_0\in A,~\ldots, X_n\in A\right)~ (as soon as the normalizing constant is strictly positive). In addition, we have the unbiased estimate      ∏   0  ≤  k  ≤  n     1  N    ∑   1  ≤  i  ≤  N     1  A    (   ξ  k  i   )     ≈   N  ↑  ∞     P   (   X  0   ∈  A  ,  …  ,   X  n   ∈  A  )      fragments   subscript  product      0  k       n       1  N    subscript       1  i       N      subscript  1  A    fragments  normal-(   subscript   superscript  ξ  i   k   normal-)    subscript    normal-↑  N     P   fragments  normal-(   subscript  X  0    A  normal-,  normal-…  normal-,   subscript  X  n    A  normal-)     \prod_{0\leq k\leq n}\frac{1}{N}\sum_{1\leq i\leq N}1_{A}(\xi^{i}_{k})~{}%
 \approx_{N\uparrow\infty}~{}P\left(X_{0}\in A,\ldots,X_{n}\in A\right)   where       1  A    x   ∈  S  ↦    1  A    (  x  )    ∈   {  0  ,  1  }            subscript  1  A   x   S    maps-to       subscript  1  A   x         0  1      1_{A}~{}x\in S\mapsto 1_{A}(x)\in\{0,1\}   stands for the indicator function of the set A.   In filtering problems the Markov chain    X  n     subscript  X  n    X_{n}   represents the signal process. This Markov chain can represent the evolution of some target (missile, plane, boat, and so on). Suppose the state    X  n     subscript  X  n    X_{n}   is partially observed by some sensor of the form     Y  n   =    h   (   X  n   )    +   V  n         subscript  Y  n       h   subscript  X  n     subscript  V  n      Y_{n}=h(X_{n})+V_{n}   for some function     h   :   x  ∈  S  ↦   h   (  x  )    ∈  ℝ      normal-:  h      x  S    maps-to      h  x        ℝ      h~{}:~{}x\in S\mapsto h(x)\in\mathbb{R}   and some sequence of random variables with a probability density    g   (  v  )       g  v    g(v)   . We fix the observations     Y  n   =   y  n        subscript  Y  n    subscript  y  n     Y_{n}=y_{n}   and we set      G  n    (  x  )    =   g   (    y  n   -   h   (  x  )     )           subscript  G  n   x     g     subscript  y  n     h  x       G_{n}(x)=g\left(y_{n}-h(x)\right)   . In this situation, the Feynman-Kac probability measures defined above coincide with the conditional distributions of the signal given the observation sequence; that is, we have that E\left(f_n(X_0,\ldots,X_n)~|~Y_0=y_0,~\ldots, Y_n=y_n\right)   =\displaystyle\frac{E\left(f_n(X_0,\ldots,X_n)\prod_{0\leq k\leq n}G_k(X_k)\right)}{E\left(\prod_{0\leq k\leq n}G_k(X_k)\right)} In addition, we have the unbiased estimate       ∏   0  ≤  k  ≤  n      1  N     ∑   1  ≤  i  ≤  N      G  k    (   ξ  k  i   )         ≈   N  ↑  ∞      p   (   y  0   ,  …  ,   y  n   )        subscript    normal-↑  N       subscript  product      0  k       n         1  N     subscript       1  i       N        subscript  G  k    subscript   superscript  ξ  i   k         p    subscript  y  0   normal-…   subscript  y  n       \prod_{0\leq k\leq n}\frac{1}{N}\sum_{1\leq i\leq N}G_{k}(\xi^{i}_{k})~{}%
 \approx_{N\uparrow\infty}~{}p(y_{0},\ldots,y_{n})   where    p   (   y  0   ,  …  ,   y  n   )       p    subscript  y  0   normal-…   subscript  y  n      p(y_{0},\ldots,y_{n})   stands for the probability density of the random variable    (   Y  0   ,  …  ,   Y  n   )      subscript  Y  0   normal-…   subscript  Y  n     (Y_{0},\ldots,Y_{n})   evaluated at    (   y  0   ,  …  ,   y  n   )      subscript  y  0   normal-…   subscript  y  n     (y_{0},\ldots,y_{n})   . In signal processing and Bayesian inference, the mutation-selection genetic algorithm is also called particle filter of Sequential Monte Carlo algorithm.   Consider a sequence of probability measures    π  β     subscript  π  β    \pi_{\beta}   on a finite solution space S indexed by some inverse temperature parameter    β  >  0      β  0    \beta>0   and defined by     π  β    (  x  )   =    e   -   β  V   (  x  )         ∑   y  ∈  S      e   -   β  V   (  y  )         for some function    V   :  x  ∈  S  ↦  V   (  x  )   ∈   [  0  ,  ∞  [      fragments   subscript  π  β    fragments  normal-(  x  normal-)       superscript  e      β  V  x       subscript     y  S     superscript  e      β  V  y        for some function  V  normal-:  x   S  maps-to  V   fragments  normal-(  x  normal-)     fragments  normal-[  0  normal-,   normal-[     \pi_{\beta}(x)=\frac{e^{-\beta V(x)}}{\sum_{y\in S}e^{-\beta V(y)}}\quad\mbox{%
 for some function}~{}V~{}:~{}x\in S\mapsto V(x)\in[0,\infty[   These probability measures concentrate to the set of global minima of the function V(.). More precisely, we have V_{\star}:=\inf_{x\in S}V(x)\quad\mbox{and}\quad\mathcal V:=\{x\in S~:~V(x)=V_{\star}\}\Rightarrow~\forall x\in\mathcal V\quad \pi_{\beta}(x)=\frac{   e^{-\beta \left(V(x)-V_{\star}\right)} }{\sum_{y\in S} e^{-\beta \left(V(y)-V_{\star}\right)}}= \frac{1}{\mbox{Card}(\mathcal V)+\sum_{y\in S-\mathcal V} e^{-\beta \left(V(y)-V_{\star}\right)}}\approx_{\beta\uparrow\infty}~\frac{1}{\mbox{Card}(\mathcal V)}and for    x  ∉  𝒱      x  𝒱    x\not\in\mathcal{V}   we have     π   β   (  x  )      ≈   β  ↑  ∞    0      subscript    normal-↑  β      subscript  π    β  x    0    \pi_{\beta(x)}\approx_{\beta\uparrow\infty}0   . This implies that      π  β    (  x  )      ≈   β  ↑  ∞        1   Card   (  𝒱  )       1  𝒱    (  x  )        subscript    normal-↑  β        subscript  π  β   x       1    Card  𝒱     subscript  1  𝒱   x     \pi_{\beta}(x)~{}\approx_{\beta\uparrow\infty}~{}\frac{1}{\mbox{Card}(\mathcal%
 {V})}~{}1_{\mathcal{V}}(x)   In other words, sampling a random variable with probability distribution     π  β    (  x  )        subscript  π  β   x    \pi_{\beta}(x)   is almost equivalent to that of sampling uniformly one of the global minima of the function V(.). Let     β  n   ≥   β   n  -  1         subscript  β  n    subscript  β    n  1      \beta_{n}\geq\beta_{n-1}   be a increasing sequence of parameters with     β  0   =  0       subscript  β  0   0    \beta_{0}=0   . Consider a Markov chain     X  n     subscript  X  n    X_{n}   starting with the uniform distribution      π  0    (  x  )    =    1  /  Card    (  S  )           subscript  π  0   x       1  Card   S     \pi_{0}(x)=1/\mbox{Card}(S)   on   S   S   S   and such that    π   β  n      subscript  π   subscript  β  n     \pi_{\beta_{n}}   is an invariant probability measure of the transition     X   n  -  1    →   X  n      normal-→   subscript  X    n  1     subscript  X  n     X_{n-1}\rightarrow X_{n}   at time n; that is, for any    n  ≥  1      n  1    n\geq 1   we have that     π   β  n     (  x  )   =    ∑   y  ∈  S     P  r  o  b  a   (   X  n   =   x   |   X   n  -  1    =  y  )    π   β  n     (  y  )      fragments   subscript  π   subscript  β  n     fragments  normal-(  x  normal-)     subscript     y  S    P  r  o  b  a   fragments  normal-(   subscript  X  n    x  normal-|   subscript  X    n  1     y  normal-)    subscript  π   subscript  β  n     fragments  normal-(  y  normal-)     \pi_{\beta_{n}}(x)=\sum_{y\in S}~{}Proba(X_{n}=x~{}|~{}X_{n-1}=y)~{}\pi_{\beta%
 _{n}}(y)   These Markov transitions can be designed using Markov Chain Monte Carlo (MCMC) methods such as the Metropolis-Hasting algorithm , or the Gibbs sampler. We consider the fitness function      G  n    (  x  )    =   exp   [   -    (    β   n  +  1    -   β  n    )   V   (  x  )     ]           subscript  G  n   x            subscript  β    n  1     subscript  β  n    V  x       G_{n}(x)=\exp{\left[-(\beta_{n+1}-\beta_{n})V(x)\right]}   In this situation, for any function     f   :   x  ∈  S  ↦   f   (  x  )    ∈  ℝ      normal-:  f      x  S    maps-to      f  x        ℝ      f~{}:~{}x\in S\mapsto f(x)\in\mathbb{R}   , we have   S   S   S   stands for the cardinality of the set $S$ . The corresponding genetic algorithm is defined in terms of MCMC mutations, the selection transitions consists of selecting the particles which are better adapted to the change of temperature. At every time step , we have the particle estimates $\frac{1}{N}\sum_{1\leq i\leq N}~f\left(\xi^i_n\right)~\approx_{N\uparrow\infty}~\sum_{x\in S}~f(x)~\pi_{\beta_n}(x)\quad\mbox{and the unbiased approximation}\quad
 \prod_{0\leq k In particular, this implies that \lim_{n\uparrow\infty}\beta_n=\infty~\Rightarrow~\lim_{n\uparrow\infty}\lim_{N\uparrow\infty}\frac{1}{N}\sum_{1\leq i\leq N}~f\left(\xi^i_n\right)=\frac{1}{\mbox{Card}(\mathcal V)}\sum_{x\in \mathcal V}~f(x)~$ Further details on these genetic type interacting MCMC methods (a.k.a. Sequential Monte Carlo - SMC) can be found in. 18 19 20    The building block hypothesis  Genetic algorithms are simple to implement, but their behavior is difficult to understand. In particular it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems. The building block hypothesis (BBH) consists of:   A description of a heuristic that performs adaptation by identifying and recombining "building blocks", i.e. low order, low defining-length schemata with above average fitness.  A hypothesis that a genetic algorithm performs adaptation by implicitly and efficiently implementing this heuristic.   Goldberg describes the heuristic as follows:   "Short, low order, and highly fit schemata are sampled, recombined [crossed over], and resampled to form strings of potentially higher fitness. In a way, by working with these particular schemata [the building blocks], we have reduced the complexity of our problem; instead of building high-performance strings by trying every conceivable combination, we construct better and better strings from the best partial solutions of past samplings.    "Because highly fit schemata of low defining length and low order play such an important role in the action of genetic algorithms, we have already given them a special name: building blocks. Just as a child creates magnificent fortresses through the arrangement of simple blocks of wood, so does a genetic algorithm seek near optimal performance through the juxtaposition of short, low-order, high-performance schemata, or building blocks."   Limitations  There are limitations of the use of a genetic algorithm compared to alternative optimization algorithms:   Repeated fitness function evaluation for complex problems is often the most prohibitive and limiting segment of artificial evolutionary algorithms. Finding the optimal solution to complex high-dimensional, multimodal problems often requires very expensive fitness function evaluations. In real world problems such as structural optimization problems, a single function evaluation may require several hours to several days of complete simulation. Typical optimization methods can not deal with such types of problem. In this case, it may be necessary to forgo an exact evaluation and use an approximated fitness that is computationally efficient. It is apparent that amalgamation of approximate models may be one of the most promising approaches to convincingly use GA to solve complex real life problems.    Genetic algorithms do not scale well with complexity. That is, where the number of elements which are exposed to mutation is large there is often an exponential increase in search space size. This makes it extremely difficult to use the technique on problems such as designing an engine, a house or plane. In order to make such problems tractable to evolutionary search, they must be broken down into the simplest representation possible. Hence we typically see evolutionary algorithms encoding designs for fan blades instead of engines, building shapes instead of detailed construction plans, airfoils instead of whole aircraft designs. The second problem of complexity is the issue of how to protect parts that have evolved to represent good solutions from further destructive mutation, particularly when their fitness assessment requires them to combine well with other parts.    The "better" solution is only in comparison to other solutions. As a result, the stop criterion is not clear in every problem.    In many problems, GAs may have a tendency to converge towards local optima or even arbitrary points rather than the global optimum of the problem. This means that it does not "know how" to sacrifice short-term fitness to gain longer-term fitness. The likelihood of this occurring depends on the shape of the fitness landscape : certain problems may provide an easy ascent towards a global optimum, others may make it easier for the function to find the local optima. This problem may be alleviated by using a different fitness function, increasing the rate of mutation, or by using selection techniques that maintain a diverse population of solutions, 21 although the No Free Lunch theorem 22 proves that there is no general solution to this problem. A common technique to maintain diversity is to impose a "niche penalty", wherein, any group of individuals of sufficient similarity (niche radius) have a penalty added, which will reduce the representation of that group in subsequent generations, permitting other (less similar) individuals to be maintained in the population. This trick, however, may not be effective, depending on the landscape of the problem. Another possible technique would be to simply replace part of the population with randomly generated individuals, when most of the population is too similar to each other. Diversity is important in genetic algorithms (and genetic programming ) because crossing over a homogeneous population does not yield new solutions. In evolution strategies and evolutionary programming , diversity is not essential because of a greater reliance on mutation.    Operating on dynamic data sets is difficult, as genomes begin to converge early on towards solutions which may no longer be valid for later data. Several methods have been proposed to remedy this by increasing genetic diversity somehow and preventing early convergence, either by increasing the probability of mutation when the solution quality drops (called triggered hypermutation ), or by occasionally introducing entirely new, randomly generated elements into the gene pool (called random immigrants ). Again, evolution strategies and evolutionary programming can be implemented with a so-called "comma strategy" in which parents are not maintained and new parents are selected only from offspring. This can be more effective on dynamic problems.    GAs cannot effectively solve problems in which the only fitness measure is a single right/wrong measure (like decision problems ), as there is no way to converge on the solution (no hill to climb). In these cases, a random search may find a solution as quickly as a GA. However, if the situation allows the success/failure trial to be repeated giving (possibly) different results, then the ratio of successes to failures provides a suitable fitness measure.    For specific optimization problems and problem instances, other optimization algorithms may be more efficient than genetic algorithms in terms of speed of convergence. Alternative and complementary algorithms include evolution strategies , evolutionary programming , simulated annealing , Gaussian adaptation , hill climbing , and swarm intelligence (e.g.: ant colony optimization , particle swarm optimization ) and methods based on integer linear programming . The suitability of genetic algorithms is dependent on the amount of knowledge of the problem; well known problems often have better, more specialized approaches.   Variants  Chromosome representation  The simplest algorithm represents each chromosome as a bit string . Typically, numeric parameters can be represented by integers , though it is possible to use floating point representations. The floating point representation is natural to evolution strategies and evolutionary programming . The notion of real-valued genetic algorithms has been offered but is really a misnomer because it does not really represent the building block theory that was proposed by John Henry Holland in the 1970s. This theory is not without support though, based on theoretical and experimental results (see below). The basic algorithm performs crossover and mutation at the bit level. Other variants treat the chromosome as a list of numbers which are indexes into an instruction table, nodes in a linked list , hashes , objects , or any other imaginable data structure . Crossover and mutation are performed so as to respect data element boundaries. For most data types, specific variation operators can be designed. Different chromosomal data types seem to work better or worse for different specific problem domains.  When bit-string representations of integers are used, Gray coding is often employed. In this way, small changes in the integer can be readily effected through mutations or crossovers. This has been found to help prevent premature convergence at so called Hamming walls , in which too many simultaneous mutations (or crossover events) must occur in order to change the chromosome to a better solution.  Other approaches involve using arrays of real-valued numbers instead of bit strings to represent chromosomes. Results from the theory of schemata suggest that in general the smaller the alphabet, the better the performance, but it was initially surprising to researchers that good results were obtained from using real-valued chromosomes. This was explained as the set of real values in a finite population of chromosomes as forming a virtual alphabet (when selection and recombination are dominant) with a much lower cardinality than would be expected from a floating point representation. 23 24  An expansion of the Genetic Algorithm accessible problem domain can be obtained through more complex encoding of the solution pools by concatenating several types of heterogenously encoded genes into one chromosome. 25 This particular approach allows for solving optimization problems that require vastly disparate definition domains for the problem parameters. For instance, in problems of cascaded controller tuning, the internal loop controller structure can belong to a conventional regulator of three parameters, whereas the external loop could implement a linguistic controller (such as a fuzzy system) which has an inherently different description. This particular form of encoding requires a specialized crossover mechanism that recombines the chromosome by section, and it is a useful tool for the modelling and simulation of complex adaptive systems, especially evolution processes.  Elitism  A practical variant of the general process of constructing a new population is to allow the best organism(s) from the current generation to carry over to the next, unaltered. This strategy is known as elitist selection and guarantees that the solution quality obtained by the GA will not decrease from one generation to the next. 26  Parallel implementations  Parallel implementations of genetic algorithms come in two flavours. Coarse-grained parallel genetic algorithms assume a population on each of the computer nodes and migration of individuals among the nodes. Fine-grained parallel genetic algorithms assume an individual on each processor node which acts with neighboring individuals for selection and reproduction. Other variants, like genetic algorithms for online optimization problems, introduce time-dependence or noise in the fitness function.  Adaptive GAs  Genetic algorithms with adaptive parameters (adaptive genetic algorithms, AGAs) is another significant and promising variant of genetic algorithms. The probabilities of crossover (pc) and mutation (pm) greatly determine the degree of solution accuracy and the convergence speed that genetic algorithms can obtain. Instead of using fixed values of pc and pm , AGAs utilize the population information in each generation and adaptively adjust the pc and pm in order to maintain the population diversity as well as to sustain the convergence capacity. In AGA (adaptive genetic algorithm), 27 the adjustment of pc and pm depends on the fitness values of the solutions. In CAGA (clustering-based adaptive genetic algorithm), 28 through the use of clustering analysis to judge the optimization states of the population, the adjustment of pc and pm depends on these optimization states. It can be quite effective to combine GA with other optimization methods. GA tends to be quite good at finding generally good global solutions, but quite inefficient at finding the last few mutations to find the absolute optimum. Other techniques (such as simple hill climbing) are quite efficient at finding absolute optimum in a limited region. Alternating GA and hill climbing can improve the efficiency of GA while overcoming the lack of robustness of hill climbing.  This means that the rules of genetic variation may have a different meaning in the natural case. For instance – provided that steps are stored in consecutive order – crossing over may sum a number of steps from maternal DNA adding a number of steps from paternal DNA and so on. This is like adding vectors that more probably may follow a ridge in the phenotypic landscape. Thus, the efficiency of the process may be increased by many orders of magnitude. Moreover, the inversion operator has the opportunity to place steps in consecutive order or any other suitable order in favour of survival or efficiency. (See for instance 29 or example in travelling salesman problem , in particular the use of an edge recombination operator .)  A variation, where the population as a whole is evolved rather than its individual members, is known as gene pool recombination.  A number of variations have been developed to attempt to improve performance of GAs on problems with a high degree of fitness epistasis, i.e. where the fitness of a solution consists of interacting subsets of its variables. Such algorithms aim to learn (before exploiting) these beneficial phenotypic interactions. As such, they are aligned with the Building Block Hypothesis in adaptively reducing disruptive recombination. Prominent examples of this approach include the mGA, 30 GEMGA 31 and LLGA. 32  Problem domains  Problems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems, and many scheduling software packages are based on GAs. GAs have also been applied to engineering . 33 Genetic algorithms are often applied as an approach to solve global optimization problems.  As a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing, i.e., mutation in combination with crossover , is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in. Observe that commonly used crossover operators cannot change any uniform population. Mutation alone can provide ergodicity of the overall genetic algorithm process (seen as a Markov chain ).  Examples of problems solved by genetic algorithms include: mirrors designed to funnel sunlight to a solar collector, 34 antennae designed to pick up radio signals in space, 35 and walking methods for computer figures. 36  In his Algorithm Design Manual , Skiena advises against genetic algorithms for any task:  History  In 1950, Alan Turing proposed a "learning machine" which would parallel the principles of evolution. 37 Computer simulation of evolution started as early as in 1954 with the work of Nils Aall Barricelli , who was using the computer at the Institute for Advanced Study in Princeton, New Jersey . 38 39 His 1954 publication was not widely noticed. Starting in 1957, 40 the Australian quantitative geneticist Alex Fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait. From these beginnings, computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970) 41 and Crosby (1973). 42 Fraser's simulations included all of the essential elements of modern genetic algorithms. In addition, Hans-Joachim Bremermann published a series of papers in the 1960s that also adopted a population of solution to optimization problems, undergoing recombination, mutation, and selection. Bremermann's research also included the elements of modern genetic algorithms. 43 Other noteworthy early pioneers include Richard Friedberg, George Friedman, and Michael Conrad. Many early papers are reprinted by Fogel (1998). 44  Although Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game, 45  artificial evolution became a widely recognized optimization method as a result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s – Rechenberg's group was able to solve complex engineering problems through evolution strategies . 46 47 48 49 Another approach was the evolutionary programming technique of Lawrence J. Fogel , which was proposed for generating artificial intelligence. Evolutionary programming originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of John Holland in the early 1970s, and particularly his book Adaptation in Natural and Artificial Systems (1975). His work originated with studies of cellular automata , conducted by Holland and his students at the University of Michigan . Holland introduced a formalized framework for predicting the quality of the next generation, known as Holland's Schema Theorem . Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in Pittsburgh, Pennsylvania .  As academic interest grew, the dramatic increase in desktop computational power allowed for practical application of the new technique. In the late 1980s, General Electric started selling the world's first genetic algorithm product, a mainframe-based toolkit designed for industrial processes. In 1989, Axcelis, Inc. released Evolver , the world's first commercial GA product for desktop computers. The New York Times technology writer John Markoff wrote 50 about Evolver in 1990, and it remained the only interactive commercial genetic algorithm until 1995. 51 Evolver was sold to Palisade in 1997, translated into several languages, and is currently in its 6th version. 52  Quantum Monte Carlo , and more specifically Diffusion Monte Carlo methods can also be interpreted as genetic type particle approximations of Feynman-Kac path integrals. 53 54 55 56 57 58 59 The origins of Quantum Monte Carlo methods are often attributed to Enrico Fermi and Robert Richtmyer who developed in 1948 a mean field particle interpretation of neutron-chain reactions, 60 but the first heuristic-like and genetic type particle algorithm (a.k.a. Resampled or Reconfiguration Monte Carlo methods) for estimating ground state energies of quantum systems (in reduced matrix models) is due to Jack H. Hetherington in 1984. 61 We also quote an earlier seminal works of Theodore E. Harris and Herman Kahn in particle physics, published in 1951, using mean field but heuristic-like genetic methods for estimating particle transmission energies. 62 In molecular chemistry, the use of genetic heuristic-like particle methodologies on path spaces (a.k.a. pruning and enrichment strategies) can be traced back to 1955 with the seminal work of Marshall. N. Rosenbluth and Arianna. W. Rosenbluth. 63  The use of genetic particle algorithms in advanced signal processing and Bayesian inference is more recent. It was in 1993, that Gordon et al., published in their seminal work 64 the first application of genetic type algorithm in Bayesian statistical inference. The authors named their algorithm 'the bootstrap filter', and demonstrated that compared to other filtering methods, their bootstrap algorithm does not require any assumption about that state-space or the noise of the system. We also quote another pioneering article in this field of Genshiro Kitagawa on a related "Monte Carlo filter", 65 and the ones by Pierre Del Moral 66 and Himilcon Carvalho, Pierre Del Moral, André Monin and Gérard Salut 67 on genetic type particle filters published in the mid-1990s. Genetic mutation-selection type particle filters were also developed in signal processing in the early 1989-1992 by P. Del Moral, J.C. Noyer, G. Rigal, and G. Salut in the LAAS-CNRS in a series of restricted and classified research reports with STCAN (Service Technique des Constructions et Armes Navales), the IT company DIGILOG, and the LAAS-CNRS (the Laboratory for Analysis and Architecture of Systems) on RADAR/SONAR and GPS signal processing problems. 68 69 70 71 72 73  From 1950 to 1996, all the publications on genetic algorithms, including the pruning and resample Monte Carlo methods introduced in computational physics and molecular chemistry, present natural and heuristic-like genetic algorithms applied to different situations without a single proof of their consistency, nor a discussion on the bias of the estimates and on genealogical and ancestral tree based algorithms. The mathematical foundations and the first rigorous analysis of these genetic particle algorithms are due to Pierre Del Moral 74 75 in 1996. The article 76 also contains a proof of the unbiased properties of a particle approximations of likelihood functions and unnormalized conditional probability measures. The unbiased particle estimator of the likelihood functions presented in this article is currently used today in Bayesian statistical inference. The theory on Feynman-Kac formulae and their genetic type particle interpretations has been developed in 2000 and 2004 in the books. 77 78 These abstract probabilistic models encapsulate genetic type algorithms, particle and bootstrap filters, interacting Kalman filters (a.k.a. Rao–Blackwellized particle filter 79 ), importance sampling and resampling style particle filter techniques, including genealogical tree based and particle backward methodologies for solving filtering and smoothing problems. Other classes of genetic type methodologies includes genealogical tree based models, 80 81 82 backward Markov particle models, 83 84 adaptive genetic algorithms, 85 island type methodologies, 86 87 and the more recent particle Markov chain Monte Carlo methodologies. 88 89  Related techniques  Parent fields  Genetic algorithms are a sub-field of:   Evolutionary algorithms  Evolutionary computing  Metaheuristics  Stochastic optimization  Optimization   Related fields  Particle filters  Particle filters or Sequential Monte Carlo methods (SMC) use a simple genetic algorithm with mutation-selection transitions to approximate the prediction-updating transitions of the nonlinear filtering equation . The number of individuals (a.k.a. particle or samples) represents the precision of the numerical algorithm. When the number of particles tends to infinity, the occupation measures of the population converge to the conditional distribution of the internal state of some dynamical system given some noisy and partial observations. In addition, the occupation measure of the ancestral lines of the genetic genealogical tree converge to the conditional distribution of the random trajectories of the system w.r.t. the sequence of observations. In this context, genetic algorithms provide a natural way to learn sequentially a series of observations to estimate the random states of some unknown dynamical process.  Evolutionary algorithms  Evolutionary algorithms is a sub-field of evolutionary computing .   Evolution strategies (ES, see Rechenberg, 1994) evolve individuals by means of mutation and intermediate or discrete recombination. ES algorithms are designed particularly to solve problems in the real-value domain. They use self-adaptation to adjust control parameters of the search. De-randomization of self-adaptation has led to the contemporary Covariance Matrix Adaptation Evolution Strategy ( CMA-ES ).    Evolutionary programming (EP) involves populations of solutions with primarily mutation and selection and arbitrary representations. They use self-adaptation to adjust parameters, and can include other variation operations such as combining information from multiple parents.    Gene expression programming (GEP) also uses populations of computer programs. These complex computer programs are encoded in simpler linear chromosomes of fixed length, which are afterwards expressed as expression trees. Expression trees or computer programs evolve because the chromosomes undergo mutation and recombination in a manner similar to the canonical GA. But thanks to the special organization of GEP chromosomes, these genetic modifications always result in valid computer programs. 90    Genetic programming (GP) is a related technique popularized by John Koza in which computer programs, rather than function parameters, are optimized. Genetic programming often uses tree-based internal data structures to represent the computer programs for adaptation instead of the list structures typical of genetic algorithms.    Grouping genetic algorithm (GGA) is an evolution of the GA where the focus is shifted from individual items, like in classical GAs, to groups or subset of items. 91 The idea behind this GA evolution proposed by Emanuel Falkenauer is that solving some complex problems, a.k.a. clustering or partitioning problems where a set of items must be split into disjoint group of items in an optimal way, would better be achieved by making characteristics of the groups of items equivalent to genes. These kind of problems include bin packing , line balancing, clustering with respect to a distance measure, equal piles, etc., on which classic GAs proved to perform poorly. Making genes equivalent to groups implies chromosomes that are in general of variable length, and special genetic operators that manipulate whole groups of items. For bin packing in particular, a GGA hybridized with the Dominance Criterion of Martello and Toth, is arguably the best technique to date.    Interactive evolutionary algorithms are evolutionary algorithms that use human evaluation. They are usually applied to domains where it is hard to design a computational fitness function, for example, evolving images, music, artistic designs and forms to fit users' aesthetic preference.   Swarm intelligence  Swarm intelligence is a sub-field of evolutionary computing .   Ant colony optimization ( ACO ) uses many ants (or agents) to traverse the solution space and find locally productive areas. While usually inferior to genetic algorithms and other forms of local search, it is able to produce results in problems where no global or up-to-date perspective can be obtained, and thus the other methods cannot be applied.    Particle swarm optimization (PSO) is a computational method for multi-parameter optimization which also uses population-based approach. A population (swarm) of candidate solutions (particles) moves in the search space, and the movement of the particles is influenced both by their own best known position and swarm's global best known position. Like genetic algorithms, the PSO method depends on information sharing among population members. In some problems the PSO is often more computationally efficient than the GAs, especially in unconstrained problems with continuous variables. Rania Hassan, Babak Cohanim, Olivier de Weck, Gerhard Vente   r (2005) A comparison of particle swarm optimization and the genetic algorithm   Intelligent Water Drops or the IWD algorithm 92 is a nature-inspired optimization algorithm inspired from natural water drops which change their environment to find the near optimal or optimal path to their destination. The memory is the river's bed and what is modified by the water drops is the amount of soil on the river's bed.   Other evolutionary computing algorithms  Evolutionary computation is a sub-field of the metaheuristic methods.   Harmony search (HS) is an algorithm mimicking the behaviour of musicians in the process of improvisation.    Memetic algorithm (MA), often called hybrid genetic algorithm among others, is a population-based method in which solutions are also subject to local improvement phases. The idea of memetic algorithms comes from memes , which unlike genes, can adapt themselves. In some problem areas they are shown to be more efficient than traditional evolutionary algorithms.    Bacteriologic algorithms (BA) inspired by evolutionary ecology and, more particularly, bacteriologic adaptation. Evolutionary ecology is the study of living organisms in the context of their environment, with the aim of discovering how they adapt. Its basic concept is that in a heterogeneous environment, you can't find one individual that fits the whole environment. So, you need to reason at the population level. It is also believed BAs could be successfully applied to complex positioning problems (antennas for cell phones, urban planning, and so on) or data mining. 93    Cultural algorithm (CA) consists of the population component almost identical to that of the genetic algorithm and, in addition, a knowledge component called the belief space.    Differential Search Algorithm (DS) inspired by migration of superorganisms. 94    Gaussian adaptation (normal or natural adaptation, abbreviated NA to avoid confusion with GA) is intended for the maximisation of manufacturing yield of signal processing systems. It may also be used for ordinary parametric optimisation. It relies on a certain theorem valid for all regions of acceptability and all Gaussian distributions. The efficiency of NA relies on information theory and a certain theorem of efficiency. Its efficiency is defined as information divided by the work needed to get the information. 95 Because NA maximises mean fitness rather than the fitness of the individual, the landscape is smoothed such that valleys between peaks may disappear. Therefore it has a certain “ambition” to avoid local peaks in the fitness landscape. NA is also good at climbing sharp crests by adaptation of the moment matrix, because NA may maximise the disorder ( average information ) of the Gaussian simultaneously keeping the mean fitness constant.   Other metaheuristic methods  Metaheuristic methods broadly fall within stochastic optimisation methods.   Simulated annealing (SA) is a related global optimization technique that traverses the search space by testing random mutations on an individual solution. A mutation that increases fitness is always accepted. A mutation that lowers fitness is accepted probabilistically based on the difference in fitness and a decreasing temperature parameter. In SA parlance, one speaks of seeking the lowest energy instead of the maximum fitness. SA can also be used within a standard GA algorithm by starting with a relatively high rate of mutation and decreasing it over time along a given schedule.    Tabu search (TS) is similar to simulated annealing in that both traverse the solution space by testing mutations of an individual solution. While simulated annealing generates only one mutated solution, tabu search generates many mutated solutions and moves to the solution with the lowest energy of those generated. In order to prevent cycling and encourage greater movement through the solution space, a tabu list is maintained of partial or complete solutions. It is forbidden to move to a solution that contains elements of the tabu list, which is updated as the solution traverses the solution space.    Extremal optimization (EO) Unlike GAs, which work with a population of candidate solutions, EO evolves a single solution and makes local modifications to the worst components. This requires that a suitable representation be selected which permits individual solution components to be assigned a quality measure ("fitness"). The governing principle behind this algorithm is that of emergent improvement through selectively removing low-quality components and replacing them with a randomly selected component. This is decidedly at odds with a GA that selects good solutions in an attempt to make better solutions.   Other stochastic optimisation methods   The cross-entropy (CE) method generates candidates solutions via a parameterized probability distribution. The parameters are updated via cross-entropy minimization, so as to generate better samples in the next iteration.    Reactive search optimization (RSO) advocates the integration of sub-symbolic machine learning techniques into search heuristics for solving complex optimization problems. The word reactive hints at a ready response to events during the search through an internal online feedback loop for the self-tuning of critical parameters. Methodologies of interest for Reactive Search include machine learning and statistics, in particular reinforcement learning, active or query learning, neural networks, and meta-heuristics.   See also   List of genetic algorithm applications  Genetic algorithms in signal processing (a.k.a. particle filters)  Propagation of schema  Universal Darwinism  Metaheuristics   References  Bibliography               Rechenberg, Ingo (1994): Evolutionsstrategie '94, Stuttgart: Fromman-Holzboog.  Schmitt, Lothar M; Nehaniv, Chrystopher L; Fujii, Robert H (1998), Linear analysis of genetic algorithms , Theoretical Computer Science 208: 111–148  Schmitt, Lothar M (2001), Theory of Genetic Algorithms , Theoretical Computer Science 259: 1–61  Schmitt, Lothar M (2004), Theory of Genetic Algorithms II: models for genetic operators over the string-tensor representation of populations and convergence to global optima for arbitrary fitness function under scaling , Theoretical Computer Science 310: 181–231  Schwefel, Hans-Paul (1974): Numerische Optimierung von Computer-Modellen (PhD thesis). Reprinted by Birkhäuser (1977).       External links  Resources   Genetic Algorithms Index Provides a list of resources in the genetic algorithms field   Tutorials   Genetic Algorithms Computer "evolve" in ways that resemble natural selection can solve complex problems even their creators do not fully understand An excellent introduction to GA by John Holland and with an application to the Prisoner's Dilemma  An online interactive GA tutorial for a reader to practise or learn how a GA works : Learn step by step or watch global convergence in batch, change the population size, crossover rates/bounds, mutation rates/bounds and selection mechanisms, and add constraints.  A Genetic Algorithm Tutorial by Darrell Whitley Computer Science Department Colorado State University An excellent tutorial with lots of theory  "Essentials of Metaheuristics" , 2009 (225 p). Free open text by Sean Luke.  Global Optimization Algorithms – Theory and Application   de:Genetische Algorithmen  sv:Genetisk programmering#Genetisk algoritm "    Category:Mathematical optimization  Category:Optimization algorithms and methods  Category:Search algorithms  Category:Cybernetics  Category:Digital organisms     ↩  ↩  ↩  ↩  Eiben, A. E. et al (1994). "Genetic algorithms with multi-parent recombination". PPSN III: Proceedings of the International Conference on Evolutionary Computation. The Third Conference on Parallel Problem Solving from Nature: 78–87. ISBN 3-540-58484-6. ↩  Ting, Chuan-Kang (2005). "On the Mean Convergence Time of Multi-parent Genetic Algorithms Without Selection". Advances in Artificial Life: 403–412. ISBN 978-3-540-28848-0. ↩  Akbari, Ziarati (2010). "A multilevel evolutionary algorithm for optimizing numerical functions" IJIEC 2 (2011): 419–430 1 ↩  ↩  ↩         ↩   ↩  ↩  ↩  Wolpert, D.H., Macready, W.G., 1995. No Free Lunch Theorems for Optimisation. Santa Fe Institute, SFI-TR-05-010, Santa Fe. ↩  ↩  ↩  ↩  ↩  Srinivas. M and Patnaik. L, "Adaptive probabilities of crossover and mutation in genetic algorithms," IEEE Transactions on System, Man and Cybernetics, vol.24, no.4, pp.656–667, 1994. ↩  ZHANG. J, Chung. H and Lo. W. L, “Clustering-Based Adaptive Crossover and Mutation Probabilities for Genetic Algorithms”, IEEE Transactions on Evolutionary Computation vol.11, no.3, pp. 326–335, 2007. ↩  Evolution-in-a-nutshell ↩  D.E. Goldberg, B. Korb, and K. Deb. "Messy genetic algorithms: Motivation, analysis, and first results". Complex Systems, 5(3):493–530, October 1989. ↩  Gene expression: The missing link in evolutionary computation ↩  G. Harik. Learning linkage to efficiently solve problems of bounded difficulty using genetic algorithms. PhD thesis, Dept. Computer Science, University of Michigan, Ann Arbour, 1997 ↩  Tomoiagă B, Chindriş M, Sumper A, Sudria-Andreu A, Villafafila-Robles R. Pareto Optimal Reconfiguration of Power Distribution Systems Using a Genetic Algorithm Based on NSGA-II. Energies. 2013; 6(3):1439-1455. ↩  ↩  ↩  http://goatstream.com/research/papers/SA2013/index.html ↩  ↩  ↩  ↩  ↩  ↩  ↩  02.27.96 - UC Berkeley's Hans Bremermann, professor emeritus and pioneer in mathematical biology, has died at 69 ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  Ruggiero, Murray A.. (2009-08-01) [ http://www.futuresmag.com/2009/08/01/fifteen-years-and-counting?t=technology&page; ;=2 Fifteen years and counting]. Futuresmag.com. Retrieved on 2013-08-07. ↩  Evolver: Sophisticated Optimization for Spreadsheets . Palisade. Retrieved on 2013-08-07. ↩   ↩  ↩  ↩  ↩  ↩  ↩  ↩   ↩  ↩  ↩  ↩   ↩             ↩    ↩   ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  Hamed Shah-Hosseini, The intelligent water drops algorithm: a nature-inspired swarm-based optimization algorithm, International Journal of Bio-Inspired Computation (IJBIC), vol. 1, no. ½, 2009, 2 ↩  ↩  ↩  ↩     