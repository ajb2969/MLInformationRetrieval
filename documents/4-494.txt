   Hamilton‚ÄìJacobi‚ÄìBellman equation      Hamilton‚ÄìJacobi‚ÄìBellman equation   The Hamilton‚ÄìJacobi‚ÄìBellman (HJB) equation is a partial differential equation which is central to optimal control theory. The solution of the HJB equation is the 'value function' which gives the minimum cost for a given dynamical system with an associated cost function.  When solved locally, the HJB is a necessary condition, but when solved over the whole of state space, the HJB equation is a necessary and sufficient condition for an optimum. The solution is open loop, but it also permits the solution of the closed loop problem. The HJB method can be generalized to stochastic systems as well.  Classical variational problems, for example the brachistochrone problem , can be solved using this method.  The equation is a result of the theory of dynamic programming which was pioneered in the 1950s by Richard Bellman and coworkers. 1 The corresponding discrete-time equation is usually referred to as the Bellman equation . In continuous time, the result can be seen as an extension of earlier work in classical physics on the Hamilton‚ÄìJacobi equation by William Rowan Hamilton and Carl Gustav Jacob Jacobi .  Optimal control problems  Consider the following problem in deterministic optimal control over the time period    [  0  ,  T  ]     0  T    [0,T]   :       V   (   x   (  0  )    ,  0  )    =    min  u    {     ‚à´  0  T    C   [   x   (  t  )    ,   u   (  t  )    ]   d  t    +   D   [   x   (  T  )    ]     }          V     x  0   0      subscript   u       superscript   subscript   0   T     C     x  t     u  t    d  t      D   delimited-[]    x  T         V(x(0),0)=\min_{u}\left\{\int_{0}^{T}C[x(t),u(t)]\,dt+D[x(T)]\right\}     where C[ ] is the scalar cost rate function and D [ ] is a function that gives the economic value or utility at the final state, x ( t ) is the system state vector, x (0) is assumed given, and u ( t ) for 0¬†‚â§ t ‚â§ T is the control vector that we are trying to find.  The system must also be subject to        x  Àô    (  t  )    =   F   [   x   (  t  )    ,   u   (  t  )    ]           normal-Àô  x   t     F     x  t     u  t       \dot{x}(t)=F[x(t),u(t)]\,     where F [ ] gives the vector determining physical evolution of the state vector over time.  The partial differential equation  For this simple system, the Hamilton‚ÄìJacobi‚ÄìBellman partial differential equation is         V  Àô    (  x  ,  t  )    +    min  u    {       ‚àá  V    (  x  ,  t  )    ‚ãÖ  F    (  x  ,  u  )    +   C   (  x  ,  u  )     }     =  0           normal-Àô  V    x  t      subscript   u        normal-‚ãÖ     normal-‚àá  V    x  t    F    x  u      C   x  u       0    \dot{V}(x,t)+\min_{u}\left\{\nabla V(x,t)\cdot F(x,u)+C(x,u)\right\}=0     subject to the terminal condition        V   (  x  ,  T  )    =   D   (  x  )     ,        V   x  T      D  x     V(x,T)=D(x),\,   where the    a  ‚ãÖ  b     normal-‚ãÖ  a  b    a\cdot b   means the dot product of the vectors a and b and   ‚àá   normal-‚àá   \nabla   is the gradient operator.  The unknown scalar    V   (  x  ,  t  )       V   x  t     V(x,t)   in the above PDE is the Bellman ' value function ', which represents the cost incurred from starting in state   x   x   x   at time   t   t   t   and controlling the system optimally from then until time   T   T   T   .  Deriving the equation  Intuitively HJB can be "derived" as follows. If    V   (   x   (  t  )    ,  t  )       V     x  t   t     V(x(t),t)   is the optimal cost-to-go function (also called the 'value function'), then by Richard Bellman's principle of optimality , going from time t to t + dt , we have        V   (   x   (  t  )    ,  t  )    =    min  u    {     ‚à´  t   t  +   d  t      C   (   x   (  t  )    ,   u   (  t  )    )   d  t    +   V   (   x   (   t  +   d  t    )    ,   t  +   d  t    )     }     .        V     x  t   t      subscript   u       superscript   subscript   t     t    d  t       C     x  t     u  t    d  t      V     x    t    d  t       t    d  t          V(x(t),t)=\min_{u}\left\{\int_{t}^{t+dt}C(x(t),u(t))\,dt+V(x(t+dt),t+dt)\right\}.     Note that the Taylor expansion of the last term is        V   (   x   (   t  +   d  t    )    ,   t  +   d  t    )    =    V   (   x   (  t  )    ,  t  )    +    V  Àô    (   x   (  t  )    ,  t  )   d  t   +      ‚àá  V    (   x   (  t  )    ,  t  )    ‚ãÖ   x  Àô     (  t  )   d  t   +   o   (   d  t   )      ,        V     x    t    d  t       t    d  t          V     x  t   t       normal-Àô  V      x  t   t   d  t      normal-‚ãÖ     normal-‚àá  V      x  t   t     normal-Àô  x    t  d  t     o    d  t       V(x(t+dt),t+dt)=V(x(t),t)+\dot{V}(x(t),t)\,dt+\nabla V(x(t),t)\cdot\dot{x}(t)%
 \,dt+o(dt),     where o( dt ) denotes the terms in the Taylor expansion of higher order than one. Then if we cancel V ( x ( t ), t ) on both sides, divide by dt , and take the limit as dt approaches zero, we obtain the HJB equation defined above.  Solving the equation  The HJB equation is usually solved backwards in time , starting from    t  =  T      t  T    t=T   and ending at    t  =  0      t  0    t=0   .  When solved over the whole of state space, the HJB equation is a necessary and sufficient condition for an optimum. 2 If we can solve for   V   V   V   then we can find from it a control   u   u   u   that achieves the minimum cost.  In general case, the HJB equation does not have a classical (smooth) solution. Several notions of generalized solutions have been developed to cover such situations, including viscosity solution ( Pierre-Louis Lions and Michael Crandall ), minimax solution ( Andrei Izmailovich Subbotin ), and others.  Extension to stochastic problems  The idea of solving a control problem by applying Bellman's principle of optimality and then working out backwards in time an optimizing strategy can be generalized to stochastic control problems. Consider similar as above      min   {     ‚à´  0  T    C   (  t  ,   X  t   ,   u  t   )   d  t    +   D   (   X  T   )     }           superscript   subscript   0   T     C   t   subscript  X  t    subscript  u  t    d  t      D   subscript  X  T       \min\left\{\int_{0}^{T}C(t,X_{t},u_{t})\,dt+D(X_{T})\right\}     now with     (   X  t   )    t  ‚àà   [  0  ,  T  ]       subscript   subscript  X  t     t   0  T      (X_{t})_{t\in[0,T]}\,\!   the stochastic process to optimize and     (   u  t   )    t  ‚àà   [  0  ,  T  ]       subscript   subscript  u  t     t   0  T      (u_{t})_{t\in[0,T]}\,\!   the steering. By first using Bellman and then expanding    V   (   X  t   ,  t  )       V    subscript  X  t   t     V(X_{t},t)   with It√¥'s rule , one finds the stochastic HJB equation         min  u    {    ùíú  V   (  x  ,  t  )    +   C   (  t  ,  x  ,  u  )     }    =  0   ,        subscript   u       ùíú  V   x  t      C   t  x  u      0    \min_{u}\left\{\mathcal{A}V(x,t)+C(t,x,u)\right\}=0,     where   ùíú   ùíú   \mathcal{A}   represents the stochastic differentiation operator, and subject to the terminal condition        V   (  x  ,  T  )    =   D   (  x  )     .        V   x  T      D  x     V(x,T)=D(x)\,\!.     Note that the randomness has disappeared. In this case a solution   V   V   V\,\!   of the latter does not necessarily solve the primal problem, it is a candidate only and a further verifying argument is required. This technique is widely used in Financial Mathematics to determine optimal investment strategies in the market (see for example Merton's portfolio problem ).  Application to LQG Control  As an example, we can look at a system with linear stochastic dynamics and quadratic cost. If the system dynamics is given by        d   x  t    =     (    a   x  t    +   b   u  t     )   d  t   +   œÉ  d   w  t      ,        d   subscript  x  t            a   subscript  x  t      b   subscript  u  t     d  t     œÉ  d   subscript  w  t       dx_{t}=(ax_{t}+bu_{t})dt+\sigma dw_{t},   and the cost accumulates at rate     C   (   x  t   ,   u  t   )    =     r   (  t  )    u  t  2    /  2   +    q   (  t  )    x  t  2    /  2          C    subscript  x  t    subscript  u  t           r  t   superscript   subscript  u  t   2    2       q  t   superscript   subscript  x  t   2    2      C(x_{t},u_{t})=r(t)u_{t}^{2}/2+q(t)x_{t}^{2}/2   , the HJB equation is given by        -     ‚àÇ  V    (  x  ,  t  )     ‚àÇ  t     =       1  2   q   (  t  )    x  2    +      ‚àÇ  V    (  x  ,  t  )     ‚àÇ  x    a  x    -     b  2    2  r   (  t  )       (     ‚àÇ  V    (  x  ,  t  )     ‚àÇ  x    )   2     +   œÉ      ‚àÇ  2   V    (  x  ,  t  )     ‚àÇ   x  2        .              V    x  t      t               1  2   q  t   superscript  x  2            V    x  t      x    a  x         superscript  b  2     2  r  t     superscript        V    x  t      x    2       œÉ        superscript   2   V    x  t       superscript  x  2         -\frac{\partial V(x,t)}{\partial t}=\frac{1}{2}q(t)x^{2}+\frac{\partial V(x,t)%
 }{\partial x}ax-\frac{b^{2}}{2r(t)}\left(\frac{\partial V(x,t)}{\partial x}%
 \right)^{2}+\sigma\frac{\partial^{2}V(x,t)}{\partial x^{2}}.   Assuming a quadratic form for the value function, we obtain the usual Riccati equation for the Hessian of the value function as is usual for Linear-quadratic-Gaussian control .  See also   Bellman equation , discrete-time counterpart of the Hamilton‚ÄìJacobi‚ÄìBellman equation  Pontryagin's minimum principle , necessary but not sufficient condition for optimum, by minimizing a Hamiltonian, but this has the advantage over HJB of only needing to be satisfied over the single trajectory being considered.   References       Further reading     "  Category:Partial differential equations  Category:Optimal control  Category:Dynamic programming  Category:Stochastic control     ‚Ü©  ‚Ü©     