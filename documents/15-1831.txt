


Computer Arimaa




Computer Arimaa

In 2002 Syed published the rules to the board game Arimaa and announced a $10,000 prize, available annually until 2020, for the first computer program (running on standard, off-the-shelf hardware) able to defeat each of three top-ranked human players in a three-game series.1 The game has been the subject of several research papers.
State space of Arimaa
Opening
The number of different ways that each player can set up their pieces at the beginning of the game is:



The player can put 8 rabbits on 16 possible squares, followed by 2 cats on the 8 remaining squares, 2 dogs on the 6 remaining squares, 2 horses on the four remaining squares, one camel on one of the two remaining squares, and the elephant on the final unused square.
Because each player can start the game with one of 64,864,800 opening setups, the total state space for the opening is:



As Christ-Jan Cox said in his Master's thesis, because the number of possible initial states is so large, "[i]t follows that it is very difficult to develop complete databases of opening moves."2
Artificial intelligence techniques
Material evaluation
It is important for the computer to be able to evaluate the value of the pieces on the board so it can assess whether or not a capture or exchange would be desirable. Assessing the relative value of pieces is an area of ongoing Arimaa research. Some currently-used systems are DAPE and FAME.
Techniques used in Arimaa bots
The following techniques are used by some or all of the artificial intelligence programs that play Arimaa:

Bitboards
Transposition tables
Zobrist hashing
Minimax and Alpha beta pruning
Killer moves and refutation tables
Static evaluation function
Quiescence search
Monte-Carlo Tree Search
UCT

Techniques rarely used In Arimaa bots

Opening book
Endgame tablebase

Computer performance
Several aspects of Arimaa make it difficult for computer programs to beat good human players. Because so much effort has gone into the development of strong chess-playing software, it is particularly relevant to understand why techniques applicable to chess are less effective for Arimaa.
Brute-force searching
Top chess programs use brute-force searching coupled with static position evaluation dominated by material considerations. Chess programs examine many, many possible moves, but they are not good (compared to humans) at determining who is winning at the end of a series of moves unless one side has more pieces than the other. The same is true for Arimaa programs, but their results are not as good in practice.
When brute-force searching is applied to Arimaa, the depth of the search is limited by the huge number of options each player has on each turn. Computationally, the number of options a player has available to them governs the number of different paths play can go down. This is known as the branching factor. The average branching factor in a game of Chess is about 35,3 whereas in Arimaa it is about 17,000.4
These differing branching factors imply that a computer which can search to a depth of eight turns for each player in chess, can only search about three turns deep for each player in Arimaa:



Alpha-beta pruning
Brute force search depth, for chess software, is nearly doubled by alpha-beta pruning, which allows the software to conclude that one move is better than another without examining every possible continuation of the weaker move. If the opponent can crush a certain move with one reply, it isn't necessary to examine other replies, which dramatically increases search speed. In Arimaa, however, the side to move switches only every four steps, which reduces the number of available cutoffs in a step-based search.
Furthermore, the usefulness of alpha-beta pruning is heavily dependent on the order in which moves are considered. Good moves must be considered before bad ones in order for the bad ones to be neglected. In particular, checking and capturing moves are key for pruning, because they are often much better than other moves. In Arimaa software the speedup provided by alpha-beta pruning is less, because captures are rarer. In rated games played on arimaa.com, only 3% of steps result in capture, compared to about 19% of chess moves that result in capture.
In most Arimaa positions, particularly toward the beginning of the game when the board is still crowded, a competent player can avoid losing any pieces within the next two turns. Compared to chess, Arimaa allows either player to delay captures for longer. Indeed, the median move number of the first capture in chess is turn 6, whereas in Arimaa it is turn 12. The struggle is initially more positional in Arimaa, and revolves around making captures unavoidable at some point in the future. This magnifies the importance of correctly judging who is gaining ground in non-material ways. Thus the strength of computer programs (examining millions of positions) is not as significant as their weakness (judging the position apart from who has more pieces).
The weakness of Arimaa programs in the opening phases is further magnified by the setup phase. In chess every game starts from the same position. By compiling before the game a list of stock replies to all standard opening moves, chess programs may often make a dozen or more excellent moves before starting to "think". Humans do the same, but have a smaller and less reliable memory of openings, which puts humans at a relative disadvantage in chess. Arimaa, in contrast, has millions of possible ways to set up the pieces even before the first piece moves. This prevents programs from having any meaningful opening book.
As the game progresses, exchanges and the advancement of rabbits tend to make the position more open and tactical. Arimaa programs typically play better in this sort of position, because they see tactical shots which humans overlook. However, it is usually possible for humans to avoid wide-open positions by conservative play, and to angle for strategic positions in which computers fare worse. Against a conservative opponent it is almost impossible to bust open the position in Arimaa, whereas in chess it is merely difficult. One must beat defensive play by the accumulation of small, long-term advantages, which programs do not do very well.
One additional technique from computer chess which does not apply to Arimaa is endgame tablebases. Master-level chess games sometimes trade down into unclear endgames with only a few pieces, for example king and knight vs. king and rook. It is possible to build, by retrograde analysis, an exhaustive table of the correct move in all such positions. Programs have only to consult a pre-generated table in such positions, rather than "thinking" afresh, which gives them a relative advantage over humans. Arimaa, in contrast, seldom comes to an endgame. Equal exchanges of pieces are less common than in chess, so it is rare for a game of Arimaa to "trade down" and still be unclear. An average game of Arimaa has only eight captures (compared to seventeen for chess), and top humans can often defeat top programs in Arimaa without losing a single piece, for example [http://arimaa.com/arimaa/games/jsShowGame.cgi?gid=298810&s;;=w the second game of the 2014 Challenge match]. Another example of low capture density is [http://arimaa.com/arimaa/games/jsShowGame.cgi?gid=218654&s;;=w this semifinal game of the 2012 World Championship], featuring only a single capture, a goal-forcing elephant sacrifice.
Omar Syed hopes that, because traditional artificial intelligence techniques are only moderately effective for Arimaa, programmers will be forced to use new artificial intelligence techniques to create a strong Arimaa-playing program. The successful quest to build a world-championship-caliber chess program has produced many techniques to successfully play games, but has contributed essentially nothing to more general reasoning; in fact, the techniques of chess playing programs have been excluded from some definitions of artificial intelligence; a goal for Arimaa is that the techniques involved in playing it will help the larger goals of artificial intelligence.
The structure of Syed's man-against-machine challenge is focused on rewarding advances in AI software and not advances in hardware. In the annual challenge, programs are run on machines chosen and provided by Syed himself, under the criterion that it be a typical, inexpensive, off-the-shelf home computer. The challenge would not be open to anyone requiring expensive multi-processor machines such as those used to challenge top-level chess players, much less something like the custom-built supercomputer Deep Blue, even though it was the success of this hardware-intensive approach which inspired Arimaa's invention. Syed believes that even the computer used in the 2004 challenge match (a Pentium 4 2.4 GHz system with 512 MB of RAM) had sufficient hardware to win the challenge prize if only it was running the proper software. Supercomputers might already have the power to conquer Arimaa by brute force using conventional AI software, and eventually personal computers will too, if hardware continues to advance at the current rate. This is why the Arimaa challenge prize is offered only until the year 2020.
Comparing the Arimaa Challenge to chess challenges
It has been argued that a computer has beaten the world chess champion but not beaten the human in the Arimaa Challenge because of six reasons:

Arimaa is a new game. Therefore, the number of programmers and amount of time devoted to computer Arimaa is much less than for computer chess. Computer chess had thousands more programmers and 40 more years than computer Arimaa. The later and smaller effort resulted in less and slower progress in computer Arimaa.
The rules for the Arimaa Challenge required the computer to show a higher playing ability than the rules for the chess matches. In the Arimaa Challenge, the computer must beat three human players in three matches. In the chess matches, the computer must win one match against one human player.
In the Arimaa Challenge, the computer needs to score 2/3 of the total points to win. In chess matches, the computer needs to score more than 1/2 of the total points to win.
In the Arimaa Challenge, the computer needs to win a qualification match. Humans are allowed to study the computer's previous games to find the computer’s weaknesses. In chess, there is no qualification match and no requirement that the computer's previous games be made publicly available.
In the Arimaa Challenge, the computer cannot be improved between games. In chess, the computer was improved between games.
In the Arimaa Challenge, the rules reject powerful or custom-made computers priced over $1,000. However, a powerful custom-made computer beat the world chess champion.

However, the Arimaa community disputes this argument point by point. To the first point, Arimaa is a new game, so the playing community is still small and even the best players are not professional players and have only been playing the game for a few years. Thus the human players in the Arimaa Challenge are much weaker than the human players in the chess challenge. The weakness of human players should make the Arimaa Challenge easier to conquer than chess, which compensates developers for having studied the problem for a shorter time.
The remaining five points compare the Arimaa Challenge only to Kasparov vs. Deep Blue, ignoring all other man vs. machine chess matches in which computers have prevailed. The chess match which can most closely be compared to the Arimaa Challenge match is the Man vs Machine World Team Championship. In 2004 and 2005 a team of humans played against a team of computer opponents. In both years the computers won by wide margin. In 2005 all three humans lost, the computers won 2/3 of the total points, the chess engines were commercially available for the humans to study, and the machine hardware used was not a supercomputer, but rather comparable to hardware used in the Arimaa Challenge.
Man-vs.-machine chess matches since 2005 have shown increasing computer dominance. For example, the 2006 Deep Fritz vs. Vladimir Kramnik and 2007 Rybka vs. Jaan Ehlvest matches gave additional advantages to the human player, but the computers (running on commodity hardware) prevailed anyway.
Computer competition in Arimaa
Arimaa Computer Championship
Arimaa Challenge
Resources for software developers
The Arimaa Engine Interface, developed by Brian Haskin, defines a protocol that allows an Arimaa engine to communicate with a controller.
According to the documentation: "An engine is a program capable of taking the state of an Arimaa game and selecting a legal move to make. A controller is anything that wants to communicate with and control an engine. This could be anything from a simple script to have the engine analyse a single position to a GUI program that allows games to be played with humans or other engines."5
The Arimaa Engine Interface includes an implementation of an engine and controller, documentation, and various scripts to control the engine and play games on any website which supports the protocol, including the official Arimaa website.67
Research papers

Arimaa challenge - comparission study of MCTS versus alpha-beta methods
 thesis by Thomas Jakl (Charles University, Prague) Oct 2011
Move Ranking and Evaluation in the Game of Arimaa
 thesis by David Jian Wu (Harvard College, Cambridge, Massachusetts, USA) May 2011
Arimaa, a New Challenge for Artificial Intelligence
 thesis by Stefano Carlini (University of Modena and Reggio Emilia, Italy) Apr 2010
Methods of MCTS and the game Arimaa
 thesis by Tomas Kozelek (Charles University of Prague, Czech Republic) Dec 2009
Modeling the game of Arimaa with linguistic geometry
 paper by Joséoberto Mercado Vega and Zvi Retchkiman Kösberg (from Instituto Politéico Nacional, presented at Proceedings of the 5th international conference on Computational Intelligence and Games, Milano, Italy) Sep 2009
Researching and Implementing a Computer Agent to Play Arimaa
 thesis by Sam Miller (University of Southampton, UK) May 2009
Plans, Patterns and Move Categories Guiding a Highly Selective Search
 paper by Gerhard Trippen (presented at the 2009 Advances in Computer Games 12 conference, Pamplona, Spain) May 2009
Arimaa, the Game of Real Intelligence?
 presentation by Nicolas A. Barriga (University Tecnica Federico Santa Maria, Chile) Aug 2006
Analysis and Implementation of the Game Arimaa and Appendix B
 thesis by Christ-Jan Cox (Universiteit Maastricht, Institute for Knowledge and Agent Technology), Mar 2006
Building a Strong Arimaa-playing Program
 thesis by Haizhi Zhong (University of Alberta, Dept. of Computing Science) Sep 2005
Building a World Champion Arimaa Program
 paper by David Fotland (www.Smart-Games.com) 2004
Arimaa - A New Game Designed to be Difficult for Computers
 paper by Omar Syed and Aamir Syed; Journal of the International Computer Games Association; Jun 2003

Footnotes


"
Category:Artificial intelligence Category:Computer science competitions Category:Game artificial intelligence




http://www.arimaa.com/arimaa/papers/CoxThesis/Cox_thesis1.pdf


http://arimaa.janzert.com/aei/aei-protocol.html
http://arimaa.janzert.com/aei/readme.txt
http://arimaa.com/arimaa/challenge/devBot.html




