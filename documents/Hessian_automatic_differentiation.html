<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="690">Hessian automatic differentiation</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Hessian automatic differentiation</h1>
<hr/>

<p>In <a href="applied_mathematics" title="wikilink">applied mathematics</a>, <strong>Hessian automatic differentiation</strong> are techniques based on <a href="automatic_differentiation" title="wikilink">automatic differentiation</a> (AD) that calculate the second derivative of a 

<math display="inline" id="Hessian_automatic_differentiation:0">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

-dimensional function, known as the <a href="Hessian_Matrix" title="wikilink">Hessian Matrix</a>.</p>

<p>When examining a function in a neighborhood of a point, one can discard many complicated global aspects of the function and accurately approximate it with simpler functions. The quadratic approximation is the best-fitting quadratic in the neighborhood of a point, and is frequently used in engineering and science. To calculate the quadratic approximation, one must first calculate its gradient and <a href="Hessian_matrix" title="wikilink">Hessian matrix</a>.</p>

<p>Let 

<math display="inline" id="Hessian_automatic_differentiation:1">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>:</mo>
   <mrow>
    <msup>
     <mi>ℝ</mi>
     <mi>n</mi>
    </msup>
    <mo>→</mo>
    <mi>ℝ</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>f</ci>
    <apply>
     <ci>normal-→</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>ℝ</ci>
      <ci>n</ci>
     </apply>
     <ci>ℝ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f:\mathbb{R}^{n}\rightarrow\mathbb{R}
  </annotation>
 </semantics>
</math>

, for each 

<math display="inline" id="Hessian_automatic_differentiation:2">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>∈</mo>
   <msup>
    <mi>ℝ</mi>
    <mi>n</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>x</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>ℝ</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x\in\mathbb{R}^{n}
  </annotation>
 </semantics>
</math>

 the Hessian matrix 

<math display="inline" id="Hessian_automatic_differentiation:3">
 <semantics>
  <mrow>
   <mrow>
    <mi>H</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>∈</mo>
   <msup>
    <mi>ℝ</mi>
    <mrow>
     <mi>n</mi>
     <mo>×</mo>
     <mi>n</mi>
    </mrow>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <times></times>
     <ci>H</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>ℝ</ci>
     <apply>
      <times></times>
      <ci>n</ci>
      <ci>n</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(x)\in\mathbb{R}^{n\times n}
  </annotation>
 </semantics>
</math>

 is the second order derivative and is a symmetric matrix. See the article on <a href="Hessian_matrix" title="wikilink">Hessian matrices</a> for more on the definition.</p>
<h2 id="reverse-hessian-vector-products">Reverse Hessian-vector products</h2>

<p>For a given 

<math display="inline" id="Hessian_automatic_differentiation:4">
 <semantics>
  <mrow>
   <mi>u</mi>
   <mo>∈</mo>
   <msup>
    <mi>ℝ</mi>
    <mi>n</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>u</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>ℝ</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   u\in\mathbb{R}^{n}
  </annotation>
 </semantics>
</math>

, this method efficiently calculates the Hessian-vector product 

<math display="inline" id="Hessian_automatic_differentiation:5">
 <semantics>
  <mrow>
   <mi>H</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>u</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>H</ci>
    <ci>x</ci>
    <ci>u</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(x)u
  </annotation>
 </semantics>
</math>

. Thus can be used to calculate the entire Hessian by calculating 

<math display="inline" id="Hessian_automatic_differentiation:6">
 <semantics>
  <mrow>
   <mi>H</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <msub>
    <mi>e</mi>
    <mi>i</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>H</ci>
    <ci>x</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>e</ci>
     <ci>i</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(x)e_{i}
  </annotation>
 </semantics>
</math>

, for 

<math display="inline" id="Hessian_automatic_differentiation:7">
 <semantics>
  <mrow>
   <mi>i</mi>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <mi>n</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>i</ci>
    <list>
     <cn type="integer">1</cn>
     <ci>normal-…</ci>
     <ci>n</ci>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i=1,\ldots,n
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>

<p>The method works by first using forward AD to perform 

<math display="inline" id="Hessian_automatic_differentiation:8">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>→</mo>
   <mrow>
    <msup>
     <mi>u</mi>
     <mi>T</mi>
    </msup>
    <mrow>
     <mo>∇</mo>
     <mi>f</mi>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>u</ci>
      <ci>T</ci>
     </apply>
     <apply>
      <ci>normal-∇</ci>
      <ci>f</ci>
     </apply>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)\rightarrow u^{T}\nabla f(x)
  </annotation>
 </semantics>
</math>

, subsequently the method then calculates the gradient of 

<math display="inline" id="Hessian_automatic_differentiation:9">
 <semantics>
  <mrow>
   <msup>
    <mi>u</mi>
    <mi>T</mi>
   </msup>
   <mrow>
    <mo>∇</mo>
    <mi>f</mi>
   </mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>u</ci>
     <ci>T</ci>
    </apply>
    <apply>
     <ci>normal-∇</ci>
     <ci>f</ci>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   u^{T}\nabla f(x)
  </annotation>
 </semantics>
</math>

 using Reverse AD to yield 

<math display="inline" id="Hessian_automatic_differentiation:10">
 <semantics>
  <mrow>
   <mrow>
    <mo>∇</mo>
    <mrow>
     <mo>(</mo>
     <mrow>
      <mrow>
       <mi>u</mi>
       <mo>⋅</mo>
       <mrow>
        <mo>∇</mo>
        <mi>f</mi>
       </mrow>
      </mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msup>
     <mi>u</mi>
     <mi>T</mi>
    </msup>
    <mi>H</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>H</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
      <mi>u</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mi>T</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <ci>normal-∇</ci>
      <apply>
       <times></times>
       <apply>
        <ci>normal-⋅</ci>
        <ci>u</ci>
        <apply>
         <ci>normal-∇</ci>
         <ci>f</ci>
        </apply>
       </apply>
       <ci>x</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>u</ci>
       <ci>T</ci>
      </apply>
      <ci>H</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <times></times>
       <ci>H</ci>
       <ci>x</ci>
       <ci>u</ci>
      </apply>
      <ci>T</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \nabla\left(u\cdot\nabla f(x)\right)=u^{T}H(x)=(H(x)u)^{T}
  </annotation>
 </semantics>
</math>

. Both of these two steps come at a time cost proportional to evaluating the function, thus the entire Hessian can be evaluated at a cost proportional to n evaluations of the function.</p>
<h2 id="reverse-hessian-edge_pushing">Reverse Hessian: Edge_Pushing</h2>

<p>An algorithm that calculates the entire Hessian with one forward and one reverse sweep of the computational graph is Edge_Pushing. Edge_Pushing is the result of applying the reverse gradient to the computational graph of the gradient. Naturally, this graph has <em>n</em> output nodes, thus in a sense one has to apply the reverse gradient method to each outgoing node. Edge_Pushing does this by taking into account overlapping calculations.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p><embed src="Edgepushingexample.pdf" title="fig:Example execution of Edge_Pushing"></embed> The algorithm's input is the computational graph of the function. After a preceding forward sweep where all intermediate values in the computational graph are calculated, the algorithm initiates a reverse sweep of the graph. Upon encountering a node that has a corresponding nonlinear elemental function, a new nonlinear edge is created between the node's predecessors indicating there is nonlinear interaction between them. See the example figure on the right. Appended to this nonlinear edge is an edge weight that is the second-order partial derivative of the nonlinear node in relation to its predecessors. This nonlinear edge is subsequently pushed down to further predecessors in such a way that when it reaches the independent nodes, its edge weight is the second-order partial derivative of the two independent nodes it connects.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="graph-colouring-techniques-for-hessians">Graph colouring techniques for Hessians</h2>

<p>The graph colouring techniques explore sparsity patterns of the Hessian matrix and cheap Hessian vector products to obtain the entire matrix. Thus these techniques are suited for large, sparse matrices. The general strategy of any such colouring technique is as follows.</p>
<ol>
<li>Obtain the global sparsity pattern of 

<math display="inline" id="Hessian_automatic_differentiation:11">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H
  </annotation>
 </semantics>
</math>

</li>
<li>Apply a graph colouring algorithm that allows us to compact the sparsity structure.</li>
<li>For each desired point 

<math display="inline" id="Hessian_automatic_differentiation:12">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>∈</mo>
   <msup>
    <mi>ℝ</mi>
    <mi>n</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>x</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>ℝ</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x\in\mathbb{R}^{n}
  </annotation>
 </semantics>
</math>

 calculate numeric entries of the compact matrix.</li>
<li>Recover the Hessian matrix from the compact matrix.</li>
</ol>

<p>Steps one and two need only be carried out once, and tend to be costly. When one wants to calculate the Hessian at numerous points (such as in an optimization routine), steps 3 and 4 are repeated.</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><figure><b>(Figure)</b>
<figcaption> <em>Coloured</em> Sparsity pattern of the Hessian Matrix</figcaption>
</figure></td>
<td style="text-align: left;"><figure><b>(Figure)</b>
<figcaption>The compact Hessian matrix</figcaption>
</figure></td>
</tr>
</tbody>
</table>

<p>As an example, the figure on the left shows the sparsity pattern of the Hessian matrix where the columns have been appropriately coloured in such a way to allow columns of the same colour to be merged without incurring in a collision between elements.</p>

<p>There are a number of colouring techniques, each with a specific recovery technique. For a comprehensive survey, see.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> There have been successful numerical results of such methods.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> {{-}}</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Hessian_matrix" title="wikilink">Hessian matrix</a></li>
<li><a href="Jacobian_matrix_and_determinant" title="wikilink">Jacobian matrix and determinant</a></li>
<li><a href="Automatic_differentiation" title="wikilink">Automatic differentiation</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.66.2603">What color is your Jacobian? Graph coloring for computing derivatives</a></li>
</ul>

<p>"</p>

<p><a href="Category:Differential_calculus" title="wikilink">Category:Differential calculus</a> <a class="uri" href="Category:Matrices" title="wikilink">Category:Matrices</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"></li>
<li id="fn2"></li>
<li id="fn3"></li>
<li id="fn4"></li>
<li id="fn5"></li>
</ol>
</section>
</body>
</html>
