   Hierarchical matrix      Hierarchical matrix   In numerical mathematics , hierarchical matrices (H-matrices)  1  2  3 are used as data-sparse approximations of non-sparse matrices. While a sparse matrix of dimension   n   n   n   can be represented efficiently in    O   (  n  )       O  n    O(n)   units of storage by storing only its non-zero entries, a non-sparse matrix would require    O   (   n  2   )       O   superscript  n  2     O(n^{2})   units of storage, and using this type of matrices for large problems would therefore be prohibitively expensive in terms of storage and computing time. Hierarchical matrices provide an approximation requiring only    O   (   n   k    log   (  n  )     )       O    n  k    n      O(nk\,\log(n))   units of storage, where   k   k   k   is a parameter controlling the accuracy of the approximation. In typical applications, e.g., when discretizing integral equations 4  5  6  7 or solving elliptic partial differential equations 8  9  10 a rank proportional to    log    (  1  /  œµ  )   Œ≥      fragments    superscript   fragments  normal-(  1   œµ  normal-)   Œ≥     \log(1/\epsilon)^{\gamma}   with a small constant   Œ≥   Œ≥   \gamma   is sufficient to ensure an accuracy of   œµ   œµ   \epsilon   . Compared to many other data-sparse representations of non-sparse matrices, hierarchical matrices offer a major advantage: the results of matrix arithmetic operations like matrix multiplication, factorization or inversion can be approximated in    O   (  n    k  Œ±    log    (  n  )   Œ≤   )      fragments  O   fragments  normal-(  n   superscript  k  Œ±     superscript   fragments  normal-(  n  normal-)   Œ≤   normal-)     O(nk^{\alpha}\,\log(n)^{\beta})   operations, where      Œ±  ,  Œ≤   ‚àà   {  1  ,  2  ,  3  }    .       Œ±  Œ≤    1  2  3     \alpha,\beta\in\{1,2,3\}.    11  Basic idea  Hierarchical matrices rely on local low-rank approximations: let    I  ,  J     I  J    I,J   be index sets, and let    G  ‚àà   ‚Ñù   I  √ó  J        G   superscript  ‚Ñù    I  J      G\in{\mathbb{R}}^{I\times J}   denote the matrix we have to approximate. In many applications (see above), we can find subsets     t  ‚äÜ  I   ,   s  ‚äÜ  J      formulae-sequence    t  I     s  J     t\subseteq I,s\subseteq J   such that     G  |    t  √ó  s      evaluated-at  G    t  s     G|_{t\times s}   can be approximated by a rank-   k   k   k   matrix. This approximation can be represented in factorized form      G  |    t  √ó  s    ‚âà   A   B  *         evaluated-at  G    t  s      A   superscript  B       G|_{t\times s}\approx AB^{*}   with factors     A  ‚àà   ‚Ñù   t  √ó  k     ,   B  ‚àà   ‚Ñù   s  √ó  k        formulae-sequence    A   superscript  ‚Ñù    t  k       B   superscript  ‚Ñù    s  k       A\in{\mathbb{R}}^{t\times k},B\in{\mathbb{R}}^{s\times k}   . While the standard representation of the matrix     G  |    t  √ó  s      evaluated-at  G    t  s     G|_{t\times s}   requires    O   (    (   #  t   )    (   #  s   )    )       O      normal-#  t     normal-#  s      O((\#t)(\#s))   units of storage, the factorized representation requires only    O   (   k   (    #  t   +   #  s    )    )       O    k      normal-#  t     normal-#  s       O(k(\#t+\#s))   units. If   k   k   k   is not too large, the storage requirements are reduced significantly.  In order to approximate the entire matrix   G   G   G   , it is split into a family of submatrices. Large submatrices are stored in factorized representation, while small submatrices are stored in standard representation in order to improve the efficiency.  Low-rank matrices are closely related to degenerate expansions used in panel clustering and the fast multipole method to approximate integral operators. In this sense, hierarchical matrices can be considered the algebraic counterparts of these techniques.  Application to integral operators  Hierarchical matrices are successfully used to treat integral equations, e.g., the single and double layer potential operators appearing in the boundary element method . A typical operator has the form        ùí¢   [  u  ]    (  x  )    =    ‚à´  Œ©    Œ∫   (  x  ,  y  )   u   (  y  )   d  y     .        ùí¢   delimited-[]  u   x     subscript   normal-Œ©     Œ∫   x  y   u  y  d  y      {\mathcal{G}}[u](x)=\int_{\Omega}\kappa(x,y)u(y)\,dy.     The Galerkin method leads to matrix entries of the form        g   i  j    =    ‚à´  Œ©     ‚à´  Œ©    Œ∫   (  x  ,  y  )    œÜ  i    (  x  )    œà  j    (  y  )   d   y   d  x      ,       subscript  g    i  j      subscript   normal-Œ©     subscript   normal-Œ©     Œ∫   x  y    subscript  œÜ  i   x   subscript  œà  j   y  d  y  d  x       g_{ij}=\int_{\Omega}\int_{\Omega}\kappa(x,y)\varphi_{i}(x)\psi_{j}(y)\,dy\,dx,     where     (   œÜ  i   )    i  ‚àà  I      subscript   subscript  œÜ  i     i  I     (\varphi_{i})_{i\in I}   and     (   œà  j   )    j  ‚àà  J      subscript   subscript  œà  j     j  J     (\psi_{j})_{j\in J}   are families of finite element basis functions. If the kernel function   Œ∫   Œ∫   \kappa   is sufficiently smooth, we can approximate it by polynomial interpolation to obtain         Œ∫  ~    (  x  ,  y  )    =    ‚àë   ŒΩ  =  1   k    Œ∫   (  x  ,   Œæ  ŒΩ   )    ‚Ñì  ŒΩ    (  y  )      ,         normal-~  Œ∫    x  y      superscript   subscript     ŒΩ  1    k     Œ∫   x   subscript  Œæ  ŒΩ     subscript  normal-‚Ñì  ŒΩ   y      \tilde{\kappa}(x,y)=\sum_{\nu=1}^{k}\kappa(x,\xi_{\nu})\ell_{\nu}(y),     where     (   Œæ  ŒΩ   )    ŒΩ  =  1   k     superscript   subscript   subscript  Œæ  ŒΩ     ŒΩ  1    k    (\xi_{\nu})_{\nu=1}^{k}   is the family of interpolation points and     (   ‚Ñì  ŒΩ   )    ŒΩ  =  1   k     superscript   subscript   subscript  normal-‚Ñì  ŒΩ     ŒΩ  1    k    (\ell_{\nu})_{\nu=1}^{k}   is the corresponding family of Lagrange polynomials . Replacing   Œ∫   Œ∫   \kappa   by    Œ∫  ~     normal-~  Œ∫    \tilde{\kappa}   yields an approximation        g  ~    i  j    =    ‚à´  Œ©     ‚à´  Œ©     Œ∫  ~    (  x  ,  y  )    œÜ  i    (  x  )    œà  j    (  y  )   d   y   d  x     =    ‚àë   ŒΩ  =  1   k     ‚à´  Œ©    Œ∫   (  x  ,   Œæ  ŒΩ   )    œÜ  i    (  x  )   d  x    ‚à´  Œ©     ‚Ñì  ŒΩ    (  y  )    œà  j    (  y  )   d  y       =    ‚àë   ŒΩ  =  1   k     a   i  ŒΩ     b   j  ŒΩ             subscript   normal-~  g     i  j      subscript   normal-Œ©     subscript   normal-Œ©      normal-~  Œ∫    x  y    subscript  œÜ  i   x   subscript  œà  j   y  d  y  d  x            superscript   subscript     ŒΩ  1    k     subscript   normal-Œ©     Œ∫   x   subscript  Œæ  ŒΩ     subscript  œÜ  i   x  d  x    subscript   normal-Œ©      subscript  normal-‚Ñì  ŒΩ   y   subscript  œà  j   y  d  y              superscript   subscript     ŒΩ  1    k      subscript  a    i  ŒΩ     subscript  b    j  ŒΩ         \tilde{g}_{ij}=\int_{\Omega}\int_{\Omega}\tilde{\kappa}(x,y)\varphi_{i}(x)\psi%
 _{j}(y)\,dy\,dx=\sum_{\nu=1}^{k}\int_{\Omega}\kappa(x,\xi_{\nu})\varphi_{i}(x)%
 \,dx\int_{\Omega}\ell_{\nu}(y)\psi_{j}(y)\,dy=\sum_{\nu=1}^{k}a_{i\nu}b_{j\nu}     with the coefficients        a   i  ŒΩ    =    ‚à´  Œ©    Œ∫   (  x  ,   Œæ  ŒΩ   )    œÜ  i    (  x  )   d  x     ,       subscript  a    i  ŒΩ      subscript   normal-Œ©     Œ∫   x   subscript  Œæ  ŒΩ     subscript  œÜ  i   x  d  x      a_{i\nu}=\int_{\Omega}\kappa(x,\xi_{\nu})\varphi_{i}(x)\,dx,           b   j  ŒΩ    =    ‚à´  Œ©     ‚Ñì  ŒΩ    (  y  )    œà  j    (  y  )   d  y     .       subscript  b    j  ŒΩ      subscript   normal-Œ©      subscript  normal-‚Ñì  ŒΩ   y   subscript  œà  j   y  d  y      b_{j\nu}=\int_{\Omega}\ell_{\nu}(y)\psi_{j}(y)\,dy.     If we choose     t  ‚äÜ  I   ,   s  ‚äÜ  J      formulae-sequence    t  I     s  J     t\subseteq I,s\subseteq J   and use the same interpolation points for all     i  ‚àà  t   ,   j  ‚àà  s      formulae-sequence    i  t     j  s     i\in t,j\in s   , we obtain      G  |    t  √ó  s    ‚âà   A   B  *         evaluated-at  G    t  s      A   superscript  B       G|_{t\times s}\approx AB^{*}   .  Obviously, any other approximation separating the variables   x   x   x   and   y   y   y   , e.g., the multipole expansion, would also allow us to split the double integral into two single integrals and thus arrive at a similar factorized low-rank matrix.  Of particular interest are cross approximation techniques 12  13  14 that use only the entries of the original matrix   G   G   G   to construct a low-rank approximation .  Application to elliptic partial differential equations  Since the solution operator of an elliptic partial differential equation can be expressed as an integral operator involving Green's function , it is not surprising that the inverse of the stiffness matrix arising from the finite element method can be approximated by a hierarchical matrix.  Green's function depends on the shape of the computational domain, therefore it is usually not known. Nevertheless, approximate arithmetic operations can be employed to compute an approximate inverse without knowing the function explicitly.  Surprisingly, it is possible to prove 15 16 that the inverse can be approximated even if the differential operator involves non-smooth coefficients and Green's function is therefore not smooth.  Arithmetic operations  The most important innovation of the hierarchical matrix method is the development of efficient algorithms for performing (approximate) matrix arithmetic operations on non-sparse matrices, e.g., to compute approximate inverses, LU decompositions and solutions to matrix equations.  The central algorithm is the efficient matrix-matrix multiplication, i.e., the computation of    Z  =   Z  +   Œ±  X  Y        Z    Z    Œ±  X  Y      Z=Z+\alpha XY   for hierarchical matrices    X  ,  Y  ,  Z     X  Y  Z    X,Y,Z   and a scalar factor   Œ±   Œ±   \alpha   . The algorithm requires the submatrices of the hierarchical matrices to be organized in a block tree structure and takes advantage of the properties of factorized low-rank matrices to compute the updated   Z   Z   Z   in    O   (  n    k  2    log    (  n  )   2   )      fragments  O   fragments  normal-(  n   superscript  k  2     superscript   fragments  normal-(  n  normal-)   2   normal-)     O(nk^{2}\,\log(n)^{2})   operations.  Taking advantage of the block structure, the inverse can be computed by using recursion to compute inverses and Schur complements of diagonal blocks and combining both using the matrix-matrix multiplication. In a similar way, the LU decomposition  17  18 can be constructed using only recursion and multiplication. Both operations also require    O   (  n    k  2    log    (  n  )   2   )      fragments  O   fragments  normal-(  n   superscript  k  2     superscript   fragments  normal-(  n  normal-)   2   normal-)     O(nk^{2}\,\log(n)^{2})   operations.  H 2 -matrices  In order to treat very large problems, the structure of hierarchical matrices can be improved: H 2 -matrices 19  20 replace the general low-rank structure of the blocks by a hierarchical representation closely related to the fast multipole method in order to reduce the storage complexity to    O   (   n  k   )       O    n  k     O(nk)   .  In the context of boundary integral operators, replacing the fixed rank   k   k   k   by block-dependent ranks leads to approximations that preserve the rate of convergence of the underlying boundary element method at a complexity of     O   (  n  )    .      O  n    O(n).    21 22  Literature    Software  HLib is a C software library implementing the most important algorithms for hierarchical and    ‚Ñã  2     superscript  ‚Ñã  2    {\mathcal{H}}^{2}   -matrices.  AHMED is a C++ software library that can be downloaded for educational purposes.  HLIBpro is an implementation of the core hierarchical matrix algorithms for commercial applications.  H2Lib is an open source implementation of hierarchical matrix algorithms intended for research and teaching.  awesome-hierarchical-matrices is a repository containing a list of other H-Matrices implementations.  "  Category:Matrices     W. Hackbusch, A sparse matrix arithmetic based on H-matrices. Part I: Introduction to H-matrices , Computing (1999), 62:89‚Äì108 ‚Ü©  M. Bebendorf, Hierarchical matrices: A means to efficiently solve elliptic boundary value problems , Springer (2008) ‚Ü©  W. Hackbusch, Hierarchische Matrizen. Algorithmen und Analysis , Springer (2009) ‚Ü©  W. Hackbusch and B. N. Khoromskij, A sparse H-Matrix Arithmetic. Part II: Application to Multi-Dimensional Problems , Computing (2000), 64:21‚Äì47 ‚Ü©  M. Bebendorf, Approximation of boundary element matrices , Num. Math. (2000), 86:565--589 ‚Ü©  M. Bebendorf and S. Rjasanow, Adaptive low-rank approximation of collocation matrices , Computing (2003), 70:1‚Äì24 ‚Ü©  S. B√∂rm and L. Grasedyck, Hybrid cross approximation of integral operators , Num. Math. (2005), 101:221‚Äì249 ‚Ü©  M. Bebendorf and W. Hackbusch, Existence of H-matrix approximants to the inverse FE-matrix of elliptic operators with    L  ‚àû     superscript  L     L^{\infty}   -coefficients , Num. Math. (2003), 95:1‚Äì28 ‚Ü©  S. B√∂rm, Approximation of solution operators of elliptic partial differential equations by H- and H 2 -matrices , Num. Math. (2010), 115:165‚Äì193 ‚Ü©  M. Faustmann, J.¬†M. Melenk and D. Praetorius, H-matrix approximability of the inverses of FEM matrices , to appear in Num. Math., preprint available at arXiv.org ‚Ü©  L. Grasedyck and W. Hackbusch, Construction and Arithmetics of H-Matrices , Computing (2003), 70:295‚Äì334 ‚Ü©    E. Tyrtyshnikov, Incomplete cross approximation in the mosaic-skeleton method , Computing (2000), 64:367‚Äì380 ‚Ü©    M. Bebendorf, Why finite element discretizations can be factored by triangular hierarchical matrices , SIAM J. Num. Anal. (2007), 45:1472‚Äì1494 ‚Ü©  L. Grasedyck, R. Kriemann and S. Le Borne, Domain decomposition based H-LU preconditioning , Num. Math. (2009), 112:565‚Äì600 ‚Ü©  W. Hackbusch, B. N. Khoromskij and S. A. Sauter, On H 2 -matrices , Lectures on Applied Mathematics (2002), 9‚Äì29 ‚Ü©  S. B√∂rm, Efficient Numerical Methods for Non-local Operators: H 2 -Matrix Compression, Algorithms and Analysis , EMS Tracts in Mathematics 14 (2010) ‚Ü©  S. A. Sauter, Variable order panel clustering , Computing (2000), 64:223‚Äì261 ‚Ü©  S. B√∂rm and S. A. Sauter, BEM with linear complexity for the classical boundary integral operators , Math. Comp. (2005), 74:1139‚Äì1177 ‚Ü©     