   Limited-memory BFGS      Limited-memory BFGS   Limited-memory BFGS ( L-BFGS or LM-BFGS ) is an optimization  algorithm in the family of quasi-Newton methods that approximates the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm using a limited amount of computer memory . It is a popular algorithm for parameter estimation in machine learning . 1 2  Like the original BFGS, L-BFGS uses an estimation to the inverse Hessian matrix to steer its search through variable space, but where BFGS stores a dense n × n approximation to the inverse Hessian ( n being the number of variables in the problem), L-BFGS stores only a few vectors that represent the approximation implicitly. Due to its resulting linear memory requirement, the L-BFGS method is particularly well suited for optimization problems with a large number of variables. Instead of the inverse Hessian H k , L-BFGS maintains a history of the past m updates of the position x and gradient ∇ f ( x ), where generally the history size m can be small (often m k''-vector product.  Algorithm  L-BFGS shares many features with other quasi-Newton algorithms, but is very different in how the matrix-vector multiplication for finding the search direction is carried out     d  k   =   -    H  k    g  k          subscript  d  k        subscript  H  k    subscript  g  k       d_{k}=-H_{k}g_{k}\,\!   . There are multiple published approaches using a history of updates to form this direction vector. Here, we give a common approach, the so-called "two loop recursion." 3 4  We'll take as given    x  k     subscript  x  k    x_{k}\,\!   , the position at the   k   k   k\,\!   -th iteration, and     g  k   ≡    ∇  f    (   x  k   )         subscript  g  k      normal-∇  f    subscript  x  k      g_{k}\equiv\nabla f(x_{k})   where   f   f   f\,\!   is the function being minimized, and all vectors are column vectors. We also assume that we have stored the last   m   m   m   updates of the form     s  k   =    x   k  +  1    -   x  k         subscript  s  k      subscript  x    k  1     subscript  x  k      s_{k}=x_{k+1}-x_{k}\,\!   and     y  k   =    g   k  +  1    -   g  k         subscript  y  k      subscript  g    k  1     subscript  g  k      y_{k}=g_{k+1}-g_{k}\,\!   . We'll define     ρ  k   =   1    y  k  T    s  k          subscript  ρ  k     1     subscript   superscript  y  normal-T   k    subscript  s  k       \rho_{k}=\frac{1}{y^{\rm T}_{k}s_{k}}   , and    H  k  0     subscript   superscript  H  0   k    H^{0}_{k}\,\!   will be the 'initial' approximate of the inverse Hessian that our estimate at iteration   k   k   k\,\!   begins with. Then we can compute the (uphill) direction as follows:      q  =   g  k       q   subscript  g  k     q=g_{k}\,\!     For     i  =    k  -  1   ,   k  -  2   ,  …  ,   k  -  m        i     k  1     k  2   normal-…    k  m      i=k-1,k-2,\ldots,k-m          α  i   =    ρ  i    s  i  T   q        subscript  α  i      subscript  ρ  i    subscript   superscript  s  normal-T   i   q     \alpha_{i}=\rho_{i}s^{\rm T}_{i}q\,\!       q  =   q  -    α  i    y  i         q    q     subscript  α  i    subscript  y  i       q=q-\alpha_{i}y_{i}\,\!        H  k   =      y   k  -  1   T    s   k  -  1     /   y   k  -  1   T     y   k  -  1          subscript  H  k          subscript   superscript  y  normal-T     k  1     subscript  s    k  1      subscript   superscript  y  normal-T     k  1      subscript  y    k  1       H_{k}=y^{\rm T}_{k-1}s_{k-1}/y^{\rm T}_{k-1}y_{k-1}       z  =    H  k   q       z     subscript  H  k   q     z=H_{k}q     For     i  =    k  -  m   ,    k  -  m   +  1   ,  …  ,   k  -  1        i     k  m       k  m   1   normal-…    k  1      i=k-m,k-m+1,\ldots,k-1          β  i   =    ρ  i    y  i  T   z        subscript  β  i      subscript  ρ  i    subscript   superscript  y  normal-T   i   z     \beta_{i}=\rho_{i}y^{\rm T}_{i}z\,\!       z  =   z  +    s  i    (    α  i   -   β  i    )         z    z     subscript  s  i      subscript  α  i    subscript  β  i        z=z+s_{i}(\alpha_{i}-\beta_{i})\,\!     Stop with       H  k    g  k    =  z         subscript  H  k    subscript  g  k    z    H_{k}g_{k}=z\,\!     This formulation is valid whether we are minimizing or maximizing. Note that if we are minimizing, the search direction would be the negative of z (since z is "uphill"), and if we are maximizing,    H  k  0     subscript   superscript  H  0   k    H^{0}_{k}   should be negative definite rather than positive definite. We would typically do a backtracking line search in the search direction (any line search would be valid, but L-BFGS does not require exact line searches in order to converge).  Commonly, the inverse Hessian    H  k  0     subscript   superscript  H  0   k    H^{0}_{k}\,\!   is represented as a diagonal matrix, so that initially setting   z   z   z\,\!   requires only an element-by-element multiplication.  This two loop update only works for the inverse Hessian. Approaches to implementing L-BFGS using the direct approximate Hessian    B  k     subscript  B  k    B_{k}\,\!   have also been developed, as have other means of approximating the inverse Hessian. 5  Applications  L-BFGS has been called "the algorithm of choice" for fitting log-linear (MaxEnt) models and conditional random fields with     ℓ  2     subscript  normal-ℓ  2    \ell_{2}   -regularization . 6  Variants  Since BFGS (and hence L-BFGS) is designed to minimize smooth functions without constraints , the L-BFGS algorithm must be modified to handle functions that include non- differentiable components or constraints. A popular class of modifications are called active-set methods, based on the concept of the active set . The idea is that when restricted to a small neighborhood of the current iterate, the function and constraints can be simplified.  L-BFGS-B  The L-BFGS-B algorithm extends L-BFGS to handle simple box constraints (aka bound constraints) on variables; that is, constraints of the form     l  i   ≤   x  i   ≤   u  i          subscript  l  i    subscript  x  i         subscript  u  i      l_{i}\leq x_{i}\leq u_{i}   where l i and u i are per-variable constant lower and upper bounds, respectively (for each x i , either or both bounds may be omitted). 7 8 The method works by identifying fixed and free variables at every step (using a simple gradient method), and then using the L-BFGS method on the free variables only to get higher accuracy, and then repeating the process.  OWL-QN  Orthant-wise limited-memory quasi-Newton ( OWL-QN ) is an L-BFGS variant for fitting     ℓ  1     subscript  normal-ℓ  1    \ell_{1}    - regularized models, exploiting the inherent sparsity of such models. 9 It minimizes functions of the form       f   (   x  →   )    =    g   (   x  →   )    +   C    ∥   x  →   ∥   1           f   normal-→  x        g   normal-→  x      C   subscript   norm   normal-→  x    1       f(\vec{x})=g(\vec{x})+C\|\vec{x}\|_{1}     where   g   g   g   is a differentiable  convex  loss function . The method is an active-set type method: at each iterate, it estimates the sign of each component of the variable, and restricts the subsequent step to have the same sign. Once the sign is fixed, the non-differentiable     ∥   x  →   ∥   1     subscript   norm   normal-→  x    1    \|\vec{x}\|_{1}   term becomes a smooth linear term which can be handled by L-BFGS. After a L-BFGS step, the method allows some variables to change sign, and repeats the process.  O-LBFGS  Schraudolph et al. present an online approximation to both BFGS and L-BFGS. 10 Similar to stochastic gradient descent , this can be used to reduce the computational complexity by evaluating the error function and gradient on a randomly drawn subset of the overall dataset in each iteration.  Implementations  An early, open source implementation of L-BFGS in Fortran exists in Netlib as a shar archive 1 . Multiple other open source implementations have been produced as translations of this Fortran code (e.g. java , and python via SciPy ). Other implementations exist:   fmincon (Matlab optimization toolbox)  FMINLBFGS (for Matlab, BSD license)  minFunc (also for Matlab)  LBFGS-D (in the D programming language) )  Frequently as part of generic optimization libraries (e.g. Mathematica , FuncLib C# library , and dlib C++ library )  The libLBFGS is a C implementation.   Implementations of variants  The L-BFGS-B variant also exists as ACM TOMS algorithm 778. 11 In February 2011, some of the authors of the original L-BFGS-B code posted a major update (version 3.0).  A reference implementation 12 is available in Fortran 77 (and with a Fortran 90 interface) at the author's website . This version, as well as older versions, has been converted to many other languages, including a Java wrapper for v3.0; Matlab interfaces for v3.0 , v2.4 , and v2.1 ; a C++ interface for v2.1; a Python interface for v3.0 as part of scipy.optimize.minimize ; an OCaml interface for v2.1 and v3.0; version 2.3 has been converted to C by f2c and is available at this website ; and R's  optim general-purpose optimizer routine includes L-BFGS-B by using method="L-BFGS-B" . 13  There exists a complete C++11  rewrite of the L-BFGS-B solver using Eigen3 .  OWL-QN implementations are available in:   C++ implementation by its designers , includes the original ICML paper on the algorithm 14  The CRF toolkit Wapiti includes a C implementation  libLBFGS   Works cited  Further reading      "  Category:Optimization algorithms and methods     ↩   ↩  ↩  ↩  ↩  ↩  ↩   ↩   ↩  ↩      