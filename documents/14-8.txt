   Integrated information theory      Integrated information theory   Integrated information theory ( IIT ) is a framework intended to understand and explain the nature of consciousness . It was developed by psychiatrist and neuroscientist Giulio Tononi of the University of Wisconsin–Madison . 1 For a popular account, see. 2 Tononi's initial ideas were further developed by Adam Barrett, who created similar measures of integrated information 3 such as "phi empirical".  Overview  The integrated information theory (IIT) of consciousness attempts to explain consciousness, or conscious experience, at the fundamental level using a principled, theoretical framework . The theory starts from two key postulations regarding the nature of consciousness: That consciousness has information regarding its experience, and that the experience is integrated to the extent that parts of an experience are informative of each other. 4  Here, IIT embraces the information theoretical sense of information ; that is, information is the reduction in uncertainty regarding the state of a variable, and conversely is what increases in specifying a variable with a growing number of possible states. When applied to conscious experience as we know it, since the number of different possible experiences generated by a human consciousness is considerably large, the amount of information this conscious system must hold should also be large. The list of a system's possible states is called its "repertoire" in IIT.  In a system composed of connected "mechanisms" (nodes containing information and causally influencing other nodes), the information among them is said to be integrated if and to the extent that there is a greater amount of information in the repertoire of a whole system regarding its previous state than there is in the sum of the all the mechanisms considered individually. In this way, integrated information does not increase by simply adding more mechanisms to a system if the mechanisms are independent of each other. Applied to consciousness, parts of an experience ( qualia ) such as color and shape are not experienced separately for the reason that they are integrated, unified in a single, whole experience; applied in another way, our digestive system is not considered part of our consciousness because the information generated in the body is not intrinsically integrated with the brain.  In IIT 3.0, the 2014 revision of IIT, five axioms were established in underpinning the theory: 5   Consciousness exists  Consciousness is compositional (structured)  Consciousness is informative  Consciousness is integrated  Consciousness is exclusive   See the original article for more information.  The suggestion is that the quantity of consciousness in a system is measured by the amount of integrated information it generates.  Qualia  The theory further proposes a way to characterize the quality of the experience, qualia itself, using a multidimensional space called qualia space (Q), wherein the ways mechanisms connect together form a shape within the space; the shape itself describes the qualia that is experienced. 6 (In the later IIT 3.0, qualia space—no longer referred to as Q -- takes into account the temporal aspects of a state.)  Q has an axis for every possible informational state of the system; any point in this space will have a component for each state. We can view the components as probabilities that the system is in that state, thus a point in Q represents a probability distribution, a repertoire, of what the system's state may be in. If all states in Q are equally probable (maximum entropy, all mechanisms are independent), then that is represented by a point with the same value at every dimension, summing to a magnitude of 1.  The quale of a properly connected system can be built up in Q by plotting the repertoires of the system being incrementally connected in all combinations. The line joining the base maximum entropy repertoire to a repertoire of a system with a single connection forms a "q-arrow". Multiple connections correspond to the concatenation of multiple q-arrows called "q-edges". The ultimate Q point corresponds to the actual, fully connected system, and is the point where all the q-arrows lead to. Finally, the multidimensional shape delimited by the q-edges can be called the quale produced by the system.  Calculating integrated information  Relative entropy/effective information      e  i   (  X   (  m  e  c  h  ,   x  1   )   )   =  H   [  p   (   X  0    (  m  e  c  h  ,   x  1   )   )   ∥  p   (   X  0    (  m  a  x  H  )   )   ]      fragments  e  i   fragments  normal-(  X   fragments  normal-(  m  e  c  h  normal-,   subscript  x  1   normal-)   normal-)    H   fragments  normal-[  p   fragments  normal-(   subscript  X  0    fragments  normal-(  m  e  c  h  normal-,   subscript  x  1   normal-)   normal-)   parallel-to  p   fragments  normal-(   subscript  X  0    fragments  normal-(  m  a  x  H  normal-)   normal-)   normal-]     ei(X(mech,x_{1}))=H[p(X_{0}(mech,x_{1}))\parallel p(X_{0}(maxH))]     where X is our system, mech is that system's mechanism,    x  1     subscript  x  1    x_{1}   is a state of the system, and    p   (    X  0    (   m  a  x  H   )    )       p     subscript  X  0     m  a  x  H      p(X_{0}(maxH))   is the uniform or potential distribution.  Effective Information is defined as the relative entropy H between the actual and potential repertoires, the Kullback–Leibler divergence . It is implicitly specified by mechanism and state, so it is an 'intrinsic' property of the system. The actual repertoire of states is calculated by perturbing the system in all possible ways to obtain the forward repertoire of output states. After that, Bayes' Rule is applied.  Example  Consider a system of two binary elements. It has four possible states (00, 01, 10, 11).  The first binary element operates randomly. The second binary element will be whatever the first element was in the previous state. Initially: (0, 0).  Its maximum entropy is given: p = (1/4, 1/4, 1/4, 1/4) Given state is 11 at time t Previous state must have been 11 or 10; hence, p = (0, 0, 1/2, 1/2) Applying the above equation, the effective information generated by the system is 1 bit.  Integration (Φ)  The measure of integrated information is denoted by the letter Phi (Φ).      Φ   (  X   (  m  e  c  h  ,   x  1   )   )   =  H   [  p   (   X  0    (  m  e  c  h  ,   x  1   )   )   ∥  Π  p    (  k    M  0    (  m  e  c  h  ,   μ  1   )   )   ]      fragments  Φ   fragments  normal-(  X   fragments  normal-(  m  e  c  h  normal-,   subscript  x  1   normal-)   normal-)    H   fragments  normal-[  p   fragments  normal-(   subscript  X  0    fragments  normal-(  m  e  c  h  normal-,   subscript  x  1   normal-)   normal-)   parallel-to  Π  p   fragments   superscript  normal-(  k    subscript  M  0    fragments  normal-(  m  e  c  h  normal-,   subscript  μ  1   normal-)   normal-)   normal-]     \Phi(X(mech,x_{1}))=H[p(X_{0}(mech,x_{1}))\parallel\Pi p(^{k}M_{0}(mech,\mu_{1%
 }))]   for     M  0     k   ∈   M  I  P        superscript   subscript  M  0   k     M  I  P     {}^{k}M_{0}\in MIP     where X is our system, mech is that system's mechanism,    x  1     subscript  x  1    x_{1}   is a state of the system, and    Π   (  p    (  k    M  0    (  m  e  c  h  ,   μ  1   )   )   )      fragments  Π   fragments  normal-(  p   fragments   superscript  normal-(  k    subscript  M  0    fragments  normal-(  m  e  c  h  normal-,   subscript  μ  1   normal-)   normal-)   normal-)     \Pi(p(^{k}M_{0}(mech,\mu_{1})))   is the product of all the probability distributions of each part of the system in the minimal information partition .  Φ will be high when there is a lot of information generated among the parts of a system as opposed to within them.  Qualia  Using relative entropy, the amount of information generated by a single connection c within the system is quantified by the equation:       Φ  c   =  H   [  p   (  X   (  m  e  c  h  ,  x  )   )   ∥  p   (  Y   (  m  e  c  h  ,  y  )   )   ]      fragments   subscript  normal-Φ  c    H   fragments  normal-[  p   fragments  normal-(  X   fragments  normal-(  m  e  c  h  normal-,  x  normal-)   normal-)   parallel-to  p   fragments  normal-(  Y   fragments  normal-(  m  e  c  h  normal-,  y  normal-)   normal-)   normal-]     \Phi_{c}=H[p(X(mech,x))\parallel p(Y(mech,y))]     where Y is the system with connection c removed.  Thus there are points Y and X in qualia space that correspond to the probability distributions of the system respectively with and without the connection c . The vector drawn from Y to X has length    Φ  c     subscript  normal-Φ  c    \Phi_{c}   , is associated with the connection c , and is called a q-arrow.  References  External links   Integrated Information Theory: A Provisional Manifesto  Integrated Information Theory: An Updated Account (2012)  From the Phenomenology to the Mechanisms of Consciousness: Integrated Information Theory 3.0 (2014)  http://arxiv.org/ftp/arxiv/papers/1405/1405.7089.pdf Consciousness: Here, There but NOT Everywhere (2014)  [ http://www.nytimes.com/2010/09/21/science/21consciousness.html?pagewanted=2&_r=1&sq; ;=integrated%20information%20theory&st;=nyt&scp;=1 Sizing Up Consciousness by Its Bits]  A Bit of Theory: Consciousness as Integrated Information Theory  Scientific American Article  Integrated Information in Discrete Dynamical Systems: Motivation and Theoretical Framework   Online videos   Consciousness and the Brain, Giulio Tononi  Christof Koch on the Neurobiology and Mathematics of Consciousness   "  Category:Consciousness studies  Category:Information theory     ↩  ↩  Barrett, A.B., & Seth, A.K. (2011). Practical measures of integrated information for time-series data. PLoS Comput. Biol., 7(1): e1001052 ↩   ↩  ↩     