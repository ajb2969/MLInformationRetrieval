   Kolmogorov's inequality      Kolmogorov's inequality   In probability theory , Kolmogorov's inequality is a so-called "maximal inequality " that gives a bound on the probability that the partial sums of a finite collection of independent random variables exceed some specified bound. The inequality is named after the Russian  mathematician  Andrey Kolmogorov .  Statement of the inequality  Let X 1 , ..., X n : Ω → R be independent  random variables defined on a common probability space (Ω, F , Pr), with expected value E[ X k ] = 0 and variance Var[ X k ]  0,        Pr   (     max   1  ≤  k  ≤  n     |   S  k   |    ≥  λ   )    ≤    1   λ  2     Var   [   S  n   ]     ≡    1   λ  2      ∑   k  =  1   n    Var   [   X  k   ]       ,         Pr      subscript       1  k       n        subscript  S  k     λ        1   superscript  λ  2     Var   subscript  S  n              1   superscript  λ  2      superscript   subscript     k  1    n    Var   subscript  X  k         \Pr\left(\max_{1\leq k\leq n}|S_{k}|\geq\lambda\right)\leq\frac{1}{\lambda^{2}%
 }\operatorname{Var}[S_{n}]\equiv\frac{1}{\lambda^{2}}\sum_{k=1}^{n}%
 \operatorname{Var}[X_{k}],     where S k = X 1 + ... + X k .  Proof  The following argument is due to Kareem Amin and employs discrete martingales . As argued in the discussion of Doob's martingale inequality , the sequence     S  1   ,   S  2   ,  …  ,   S  n       subscript  S  1    subscript  S  2   normal-…   subscript  S  n     S_{1},S_{2},\dots,S_{n}   is a martingale. Without loss of generality , we can assume that     S  0   =  0       subscript  S  0   0    S_{0}=0   and     S  i   ≥  0       subscript  S  i   0    S_{i}\geq 0   for all   i   i   i   . Define     (   Z  i   )    i  =  0   n     superscript   subscript   subscript  Z  i     i  0    n    (Z_{i})_{i=0}^{n}   as follows. Let     Z  0   =  0       subscript  Z  0   0    Z_{0}=0   , and       Z   i  +  1    =   {      S   i  +  1        if    max   1  ≤  j  ≤  i     S  j     <  λ        Z  i     otherwise            subscript  Z    i  1     cases     subscript  S    i  1        if    subscript       1  j       i      subscript  S  j     λ      subscript  Z  i   otherwise       Z_{i+1}=\left\{\begin{array}[]{ll}S_{i+1}&\text{ if }\displaystyle\max_{1\leq j%
 \leq i}S_{j}<\lambda\\
 Z_{i}&\text{ otherwise}\end{array}\right.   for all   i   i   i   . Then     (   Z  i   )    i  =  0   n     superscript   subscript   subscript  Z  i     i  0    n    (Z_{i})_{i=0}^{n}   is also a martingale. Since     S  i   -   S   i  -  1         subscript  S  i    subscript  S    i  1      S_{i}-S_{i-1}   is independent and mean zero,        ∑   i  =  1   n     E   [    (    S  i   -   S   i  -  1     )   2   ]        superscript   subscript     i  1    n     E   delimited-[]   superscript     subscript  S  i    subscript  S    i  1     2       \displaystyle\sum_{i=1}^{n}\text{E}[(S_{i}-S_{i-1})^{2}]   The same is true for     (   Z  i   )    i  =  0   n     superscript   subscript   subscript  Z  i     i  0    n    (Z_{i})_{i=0}^{n}   . Thus      Pr   (   max   1  ≤  i  ≤  n     S  i   ≥  λ  )      fragments  Pr   fragments  normal-(   subscript       1  i       n      subscript  S  i    λ  normal-)     \displaystyle\text{Pr}\left(\max_{1\leq i\leq n}S_{i}\geq\lambda\right)   by Chebyshev's inequality .  This inequality was generalized by Hájek and Rényi in 1955.  See also   Chebyshev's inequality  Etemadi's inequality  Landau–Kolmogorov inequality  Markov's inequality  Bernstein inequalities (probability theory)   References    (Theorem 22.4)    "  Category:Probability theory  Category:Stochastic processes  Category:Probabilistic inequalities  Category:Articles containing proofs   