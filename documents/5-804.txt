   Wald's equation      Wald's equation   In probability theory , Wald's equation , Wald's identity 1 or Wald's lemma 2 is an important identity that simplifies the calculation of the expected value of the sum of a random number of random quantities. In its simplest form, it relates the expectation of a sum of randomly many finite-mean, independent and identically distributed random variables to the expected number of terms in the sum and the random variables' common expectation under the condition that the number of terms in the sum is independent of the summands.  The equation is named after the mathematician  Abraham Wald . An identity for the second moment is given by the Blackwell–Girshick equation .  Basic version  Let be a sequence of real-valued, independent and identically distributed random variables and let   N   N   N   be a nonnegative integer-value random variable that is independent of the sequence . Suppose that   N   N   N   and the have finite expectations. Then        E   [    X  1   +  …  +   X  N    ]    =    E   [  N  ]     E   [   X  1   ]      .       normal-E     subscript  X  1   normal-…   subscript  X  N        normal-E  N    normal-E   subscript  X  1       \operatorname{E}[X_{1}+\dots+X_{N}]=\operatorname{E}[N]\operatorname{E}[X_{1}]\,.     Example  Roll a six-sided die . Take the number on the die (call it   N   N   N   ) and roll that number of six-sided dice to get the numbers , and add up their values. By Wald's equation, the resulting value on average is        E   [  N  ]     E   [  X  ]     =     1  +  2  +  3  +  4  +  5  +  6   6   ⋅    1  +  2  +  3  +  4  +  5  +  6   6    =   441  36   =  12.25 .           normal-E  N    normal-E  X     normal-⋅      1  2  3  4  5  6   6       1  2  3  4  5  6   6           441  36        12.25 .     \operatorname{E}[N]\operatorname{E}[X]=\frac{1+2+3+4+5+6}{6}\cdot\frac{1+2+3+4%
 +5+6}{6}=\frac{441}{36}=12.25\,.     General version  Let be an infinite sequence of real-valued random variables and let   N   N   N   be a nonnegative integer-valued random variable. Assume that   1 . are all integrable (finite-mean) random variables,  2 .  for every natural number    n   n   n   , and  3 . the infinite series satisfies            ∑   n  =  1   ∞    E   [    |   X  n   |    1   {  N  ≥  n  }     ]     <  ∞   .        superscript   subscript     n  1       normal-E       subscript  X  n     subscript  1   fragments  normal-{  N   n  normal-}          \sum_{n=1}^{\infty}\operatorname{E}\!\bigl[|X_{n}|1_{\{N\geq n\}}\bigr]<\infty.        Then the random sums        S  N   :=    ∑   n  =  1   N    X  n     ,    T  N   :=    ∑   n  =  1   N    E   [   X  n   ]         formulae-sequence   assign   subscript  S  N     superscript   subscript     n  1    N    subscript  X  n      assign   subscript  T  N     superscript   subscript     n  1    N    normal-E   subscript  X  n        S_{N}:=\sum_{n=1}^{N}X_{n},\qquad T_{N}:=\sum_{n=1}^{N}\operatorname{E}[X_{n}]   are integrable and        E   [   S  N   ]    =   E   [   T  N   ]     .       normal-E   subscript  S  N     normal-E   subscript  T  N      \operatorname{E}[S_{N}]=\operatorname{E}[T_{N}].   If, in addition,   4 . all have the same expectation, and  5 .   N   N   N   has finite expectation,   then        E   [   S  N   ]    =    E   [  N  ]     E   [   X  1   ]      .       normal-E   subscript  S  N       normal-E  N    normal-E   subscript  X  1       \operatorname{E}[S_{N}]=\operatorname{E}[N]\,\operatorname{E}[X_{1}].     Remark: Usually, the name Wald's equation refers to this last equality.  Discussion of assumptions  Clearly, assumption () is needed to formulate assumption () and Wald's equation. Assumption () controls the amount of dependence allowed between the sequence and the number   N   N   N   of terms; see the counterexample below for the necessity . Note that assumption () is satisfied when   N   N   N   is a stopping time for the sequence . Assumption () is of more technical nature, implying absolute convergence and therefore allowing arbitrary rearrangement of an infinite series in the proof.  If assumption () is satisfied, then assumption () can be strengthened to the simpler condition   6 . there exists a real constant   C   C   C   such that  for all natural numbers   n   n   n   .   Indeed, using assumption (),         ∑   n  =  1   ∞    E   [    |   X  n   |    1   {  N  ≥  n  }     ]     ≤   C    ∑   n  =  1   ∞    P   (   N  ≥  n   )       ,        superscript   subscript     n  1       normal-E       subscript  X  n     subscript  1   fragments  normal-{  N   n  normal-}         C    superscript   subscript     n  1       normal-P    N  n        \sum_{n=1}^{\infty}\operatorname{E}\!\bigl[|X_{n}|1_{\{N\geq n\}}\bigr]\leq C%
 \sum_{n=1}^{\infty}\operatorname{P}(N\geq n),     and the last series equals the expectation of   N   N   N    [ Proof ] , which is finite by assumption (). Therefore, () and () imply assumption ().  Assume in addition to () and () that   7 .   N   N   N   is independent of the sequence and  8 . there exists a constant   C   C   C   such that for all natural numbers   n   n   n   .   Then all the assumptions (), (), () and (), hence also () are satisfied. In particular, the conditions () and () are satisfied if   9 . the random variables all have the same distribution.   Note that the random variables of the sequence don't need to be independent.  The interesting point is to admit some dependence between the random number   N   N   N   of terms and the sequence . A standard version is to assume (), (), () and the existence of a filtration  such that   10 .   N   N   N   is a stopping time with respect to the filtration, and  11 . and are independent for every    n  ∈  ℕ      n  normal-∈  normal-ℕ    n∈ℕ   .   Then () implies that the event  is in , hence by () independent of . This implies (), and together with () it implies ().  For convenience (see the proof below using the optional stopping theorem) and to specify the relation of the sequence and the filtration , the following additional assumption is often imposed:   12 . the sequence is adapted to the filtration , meaning the is -measurable for every    n  ∈  ℕ      n  normal-∈  normal-ℕ    n∈ℕ   .   Note that () and () together imply that the random variables are independent.  Application  An application is in actuarial science when considering the total claim amount follows a compound Poisson process       S  N   =    ∑   n  =  1   N    X  n         subscript  S  N     superscript   subscript     n  1    N    subscript  X  n      S_{N}=\sum_{n=1}^{N}X_{n}     within a certain time period, say one year, arising from a random number   N   N   N   of individual insurance claims, whose sizes are described by the random variables . Under the above assumptions, Wald's equation can be used to calculate the expected total claim amount when information about the average claim number per year and the average claim size is available. Under stronger assumptions and with more information about the underlying distributions, Panjer's recursion can be used to calculate the distribution of .  Examples  Example with dependent terms  Let   N   N   N   be an integrable, -valued random variable, which is independent of the integrable, real-valued random variable   Z   Z   Z   with 0}} . Define (–1) n  Z }} for all    n  ∈  ℕ      n  normal-∈  normal-ℕ    n∈ℕ   . Then assumptions (), (), (), and () with E[| Z |]}} are satisfied, hence also () and (), and Wald's equation applies. If the distribution of   Z   Z   Z   is not symmetric, then () does not hold. Note that, when   Z   Z   Z   is not almost surely equal to the zero random variable, then () and () cannot hold simultaneously for any filtration , because   Z   Z   Z   cannot be independent of itself as (E[ Z ]) 2 {{=}} 0}} is impossible.  Example where the number of terms depends on the sequence  Let be a sequence of independent, symmetric, and }-valued random variables. For every    n  ∈  ℕ      n  normal-∈  normal-ℕ    n∈ℕ   let be the σ-algebra generated by and define    N  =  n      N  n    N=n   when is the first random variable taking the value    +  1      1    +1   . Note that  n ) {{=}} 1/2 n }} , hence by the ratio test . The assumptions (), () and (), hence () and () with    C  =  1      C  1    C=1   , (), (), and () hold, hence also (), and () and Wald's equation applies. However, () does not hold, because   N   N   N   is defined in terms of the sequence . Intuitively, one might expect to have in this example, because the summation stops right after a one, thereby apparently creating a positive bias. However, Wald's equation shows that this intuition is misleading.  Counterexamples  A counterexample illustrating the necessity of assumption ()  Consider a sequence of i.i.d. random variables, taking each of the two values 0 and 1 with probability ½ (actually, only is needed in the following). Define 1 – X 1 }} . Then is identically equal to zero, hence 0}} , but ½}} and ½}} and therefore Wald's equation does not hold. Indeed, the assumptions (), (), () and () are satisfied, however, the equation in assumption () holds for all    n  ∈  ℕ      n  normal-∈  normal-ℕ    n∈ℕ   except for    n  =  1      n  1    n=1   .  A counterexample illustrating the necessity of assumption ()  Very similar to the second example above, let be a sequence of independent, symmetric random variables, where takes each of the values and with probability ½. Let   N   N   N   be the first    n  ∈  ℕ      n  normal-∈  normal-ℕ    n∈ℕ   such that 2 n }} . Then, as above,   N   N   N   has finite expectation, hence assumption () holds. Since 0}} for all    n  ∈  ℕ      n  normal-∈  normal-ℕ    n∈ℕ   , assumptions () and () hold. However, since 1}} almost surely, Wald's equation cannot hold.  Since   N   N   N   is a stopping time with respect to the filtration generated by , assumption () holds, see above. Therefore, only assumption () can fail, and indeed, since       {  N  ≥  n  }   =   {   X  i   =  -   2  i   for  i  =  1  ,  …  ,  n  -  1  }      fragments   fragments  normal-{  N   n  normal-}     fragments  normal-{   subscript  X  i      superscript  2  i   for  i   1  normal-,  normal-…  normal-,  n   1  normal-}     \{N\geq n\}=\{X_{i}=-2^{i}\text{ for }i=1,\ldots,n-1\}     and therefore 1/2 n –1 }} for every    n  ∈  ℕ      n  normal-∈  normal-ℕ    n∈ℕ   , it follows that         ∑   n  =  1   ∞    E   [    |   X  n   |    1   {  N  ≥  n  }     ]     =    ∑   n  =  1   ∞      2  n     P   (   N  ≥  n   )      =    ∑   n  =  1   ∞   2   =  ∞   .          superscript   subscript     n  1       normal-E       subscript  X  n     subscript  1   fragments  normal-{  N   n  normal-}         superscript   subscript     n  1         superscript  2  n    normal-P    N  n             superscript   subscript     n  1      2            \sum_{n=1}^{\infty}\operatorname{E}\!\bigl[|X_{n}|1_{\{N\geq n\}}\bigr]=\sum_{%
 n=1}^{\infty}2^{n}\,\operatorname{P}(N\geq n)=\sum_{n=1}^{\infty}2=\infty.     A proof using the optional stopping theorem  Assume (), (), (), (), () and (). Using assumption (), define the sequence of random variables         M  n   =    ∑   i  =  1   n    (    X  i   -   E   [   X  i   ]     )     ,   n  ∈   ℕ  0     .     formulae-sequence     subscript  M  n     superscript   subscript     i  1    n      subscript  X  i    normal-E   subscript  X  i         n   subscript  ℕ  0      M_{n}=\sum_{i=1}^{n}(X_{i}-\operatorname{E}[X_{i}]),\quad n\in{\mathbb{N}}_{0}.     Assumption () implies that the conditional expectation of given equals almost surely for every    n  ∈  ℕ      n  normal-∈  normal-ℕ    n∈ℕ   , hence is a martingale with respect to the filtration by assumption (). Assumptions (), () and () make sure that we can apply the optional stopping theorem , hence  S N – T N }} is integrable and  Due to assumption (),        |   T  N   |   =   |    ∑   i  =  1   N    E   [   X  i   ]     |   ≤    ∑   i  =  1   N    E   [   |   X  i   |   ]     ≤   C  N    ,           subscript  T  N        superscript   subscript     i  1    N    normal-E   subscript  X  i             superscript   subscript     i  1    N    normal-E     subscript  X  i             C  N      |T_{N}|=\biggl|\sum_{i=1}^{N}\operatorname{E}[X_{i}]\biggr|\leq\sum_{i=1}^{N}%
 \operatorname{E}[|X_{i}|]\leq CN,     and due to assumption () this upper bound is integrable. Hence we can add the expectation of to both sides of Equation () and obtain by linearity        E   [   S  N   ]    =   E   [   T  N   ]     .       normal-E   subscript  S  N     normal-E   subscript  T  N      \operatorname{E}[S_{N}]=\operatorname{E}[T_{N}].     Remark: Note that this proof does not cover the above example with dependent terms .  General proof  This proof uses only Lebesgue's monotone and dominated convergence theorems . We prove the statement as given above in three steps.  Step 1: Integrability of the random sum  We first show that the random sum is integrable. Define the partial sums  Since   N   N   N   takes its values in and since 0}} , it follows that        |   S  N   |   =    ∑   i  =  1   ∞     |   S  i   |    1   {  N  =  i  }       .         subscript  S  N      superscript   subscript     i  1           subscript  S  i     subscript  1   fragments  normal-{  N   i  normal-}        |S_{N}|=\sum_{i=1}^{\infty}|S_{i}|\,1_{\{N=i\}}.     The Lebesgue monotone convergence theorem implies that        E   [   |   S  N   |   ]    =    ∑   i  =  1   ∞    E   [    |   S  i   |    1   {  N  =  i  }     ]      .       normal-E     subscript  S  N       superscript   subscript     i  1       normal-E       subscript  S  i     subscript  1   fragments  normal-{  N   i  normal-}         \operatorname{E}[|S_{N}|]=\sum_{i=1}^{\infty}\operatorname{E}[|S_{i}|\,1_{\{N=%
 i\}}].     By the triangle inequality,         |   S  i   |   ≤    ∑   n  =  1   i    |   X  n   |     ,   i  ∈  ℕ    .     formulae-sequence       subscript  S  i      superscript   subscript     n  1    i      subscript  X  n        i  ℕ     |S_{i}|\leq\sum_{n=1}^{i}|X_{n}|,\quad i\in{\mathbb{N}}.     Using this upper estimate and changing the order of summation (which is permitted because all terms are non-negative), we obtain  ]\le\sum_{n=1}^\infty\operatorname{E}[|X_n|\,1_{\{N\ge n\}}],| 15 }}  where the second inequality follows using the monotone convergence theorem. By assumption (), the infinite sequence on the right-hand side of () converges, hence is integrable.  Step 2: Integrability of the random sum  We now show that the random sum is integrable. Define the partial sums  of real numbers. Since   N   N   N   takes its values in and since 0}} , it follows that        |   T  N   |   =    ∑   i  =  1   ∞     |   T  i   |    1   {  N  =  i  }       .         subscript  T  N      superscript   subscript     i  1           subscript  T  i     subscript  1   fragments  normal-{  N   i  normal-}        |T_{N}|=\sum_{i=1}^{\infty}|T_{i}|\,1_{\{N=i\}}.     The Lebesgue monotone convergence theorem implies that        E   [   |   T  N   |   ]    =    ∑   i  =  1   ∞     |   T  i   |    P   (   N  =  i   )       .       normal-E     subscript  T  N       superscript   subscript     i  1           subscript  T  i     normal-P    N  i        \operatorname{E}[|T_{N}|]=\sum_{i=1}^{\infty}|T_{i}|\operatorname{P}(N=i).     By the triangle inequality,         |   T  i   |   ≤    ∑   n  =  1   i    |   E   [   X  n   ]    |     ,   i  ∈  ℕ    .     formulae-sequence       subscript  T  i      superscript   subscript     n  1    i      normal-E   subscript  X  n         i  ℕ     |T_{i}|\leq\sum_{n=1}^{i}\bigl|\!\operatorname{E}[X_{n}]\bigr|,\quad i\in{%
 \mathbb{N}}.     Using this upper estimate and changing the order of summation (which is permitted because all terms are non-negative), we obtain  By assumption (),          |   E   [   X  n   ]    |    P   (   N  ≥  n   )     =   |   E   [    X  n    1   {  N  ≥  n  }     ]    |   ≤   E   [    |   X  n   |    1   {  N  ≥  n  }     ]     ,   n  ∈  ℕ    .     formulae-sequence           normal-E   subscript  X  n      normal-P    N  n        normal-E     subscript  X  n    subscript  1   fragments  normal-{  N   n  normal-}             normal-E       subscript  X  n     subscript  1   fragments  normal-{  N   n  normal-}          n  ℕ     \bigl|\!\operatorname{E}[X_{n}]\bigr|\operatorname{P}(N\geq n)=\bigl|\!%
 \operatorname{E}[X_{n}1_{\{N\geq n\}}]\bigr|\leq\operatorname{E}[|X_{n}|1_{\{N%
 \geq n\}}],\quad n\in{\mathbb{N}}.     Substituting this into () yields        E   [   |   T  N   |   ]    ≤    ∑   n  =  1   ∞    E   [    |   X  n   |    1   {  N  ≥  n  }     ]      ,       normal-E     subscript  T  N       superscript   subscript     n  1       normal-E       subscript  X  n     subscript  1   fragments  normal-{  N   n  normal-}         \operatorname{E}[|T_{N}|]\leq\sum_{n=1}^{\infty}\operatorname{E}[|X_{n}|1_{\{N%
 \geq n\}}],     which is finite by assumption (), hence is integrable.  Step 3: Proof of the identity  To prove Wald's equation, we essentially go through the same steps again without the absolute value, making use of the integrability of the random sums and in order to show that they have the same expectation. Using the dominated convergence theorem with dominating random variable S N {{!}}}} and the definition of the partial sum given in (), it follows that        E   [   S  N   ]    =    ∑   i  =  1   ∞    E   [    S  i    1   {  N  =  i  }     ]     =    ∑   i  =  1   ∞     ∑   n  =  1   i    E   [    X  n    1   {  N  =  i  }     ]       .         normal-E   subscript  S  N      superscript   subscript     i  1       normal-E     subscript  S  i    subscript  1   fragments  normal-{  N   i  normal-}              superscript   subscript     i  1        superscript   subscript     n  1    i    normal-E     subscript  X  n    subscript  1   fragments  normal-{  N   i  normal-}           \operatorname{E}[S_{N}]=\sum_{i=1}^{\infty}\operatorname{E}[S_{i}1_{\{N=i\}}]=%
 \sum_{i=1}^{\infty}\sum_{n=1}^{i}\operatorname{E}[X_{n}1_{\{N=i\}}].     Due to the absolute convergence proved in () above using assumption (), we may rearrange the summation and obtain that        E   [   S  N   ]    =    ∑   n  =  1   ∞     ∑   i  =  n   ∞    E   [    X  n    1   {  N  =  i  }     ]      =    ∑   n  =  1   ∞    E   [    X  n    1   {  N  ≥  n  }     ]      ,         normal-E   subscript  S  N      superscript   subscript     n  1        superscript   subscript     i  n       normal-E     subscript  X  n    subscript  1   fragments  normal-{  N   i  normal-}               superscript   subscript     n  1       normal-E     subscript  X  n    subscript  1   fragments  normal-{  N   n  normal-}          \operatorname{E}[S_{N}]=\sum_{n=1}^{\infty}\sum_{i=n}^{\infty}\operatorname{E}%
 [X_{n}1_{\{N=i\}}]=\sum_{n=1}^{\infty}\operatorname{E}[X_{n}1_{\{N\geq n\}}],     where we used assumption () and the dominated convergence theorem with dominating random variable X n {{!}}}} for the second equality. Due to assumption () and the σ-additivity of the probability measure,      E   [    X  n    1   {  N  ≥  n  }     ]      normal-E     subscript  X  n    subscript  1   fragments  normal-{  N   n  normal-}       \displaystyle\operatorname{E}[X_{n}1_{\{N\geq n\}}]     Substituting this result into the previous equation, rearranging the summation (which is permitted due to absolute convergence, see () above), using linearity of expectation and the definition of the partial sum of expectations given in (),        E   [   S  N   ]    =    ∑   i  =  1   ∞     ∑   n  =  1   i    E   [    E   [   X  n   ]     1   {  N  =  i  }     ]      =    ∑   i  =  1   ∞    E   [      T  i    1   {  N  =  i  }     ⏟     =    T  N    1   {  N  =  i  }       ]      .         normal-E   subscript  S  N      superscript   subscript     i  1        superscript   subscript     n  1    i    normal-E    normal-E   subscript  X  n     subscript  1   fragments  normal-{  N   i  normal-}               superscript   subscript     i  1       normal-E   subscript   normal-⏟     subscript  T  i    subscript  1   fragments  normal-{  N   i  normal-}        absent     subscript  T  N    subscript  1   fragments  normal-{  N   i  normal-}            \operatorname{E}[S_{N}]=\sum_{i=1}^{\infty}\sum_{n=1}^{i}\operatorname{E}\!%
 \bigl[\operatorname{E}[X_{n}]1_{\{N=i\}}\bigr]=\sum_{i=1}^{\infty}%
 \operatorname{E}[\underbrace{T_{i}1_{\{N=i\}}}_{=\,T_{N}1_{\{N=i\}}}].     By using dominated convergence again with dominating random variable T N {{!}}}} ,        E   [   S  N   ]    =   E   [    T  N       ∑   i  =  1   ∞    1   {  N  =  i  }     ⏟     =   1   {  N  ≥  1  }       ]    =   E   [   T  N   ]     .         normal-E   subscript  S  N     normal-E     subscript  T  N    subscript   normal-⏟    superscript   subscript     i  1       subscript  1   fragments  normal-{  N   i  normal-}        absent   subscript  1   fragments  normal-{  N   1  normal-}              normal-E   subscript  T  N       \operatorname{E}[S_{N}]=\operatorname{E}\!\biggl[T_{N}\underbrace{\sum_{i=1}^{%
 \infty}1_{\{N=i\}}}_{=\,1_{\{N\geq 1\}}}\biggr]=\operatorname{E}[T_{N}].     If assumptions () and () are satisfied, then by linearity of expectation,        E   [   T  N   ]    =   E   [    ∑   n  =  1   N    E   [   X  n   ]     ]    =    E   [   X  1   ]     E   [      ∑   n  =  1   N   1   ⏟     =  N    ]     =    E   [  N  ]     E   [   X  1   ]      .         normal-E   subscript  T  N     normal-E    superscript   subscript     n  1    N    normal-E   subscript  X  n              normal-E   subscript  X  1     normal-E   subscript   normal-⏟    superscript   subscript     n  1    N   1      absent  N              normal-E  N    normal-E   subscript  X  1        \operatorname{E}[T_{N}]=\operatorname{E}\!\biggl[\sum_{n=1}^{N}\operatorname{E%
 }[X_{n}]\biggr]=\operatorname{E}[X_{1}]\operatorname{E}\!\biggl[\underbrace{%
 \sum_{n=1}^{N}1}_{=\,N}\biggr]=\operatorname{E}[N]\operatorname{E}[X_{1}].     This completes the proof.  Further generalizations   Wald's equation can be transferred to -valued random variables by applying the one-dimensional version to every component.  If are Bochner-integrable random variables taking values in a Banach space , then the general proof above can be adjusted accordingly.   See also   Lorden's inequality  Wald's martingale  Spitzer's formula   Notes  References       "  Category:Probability theory  Category:Articles containing proofs  Category:Actuarial science     ↩  ↩     