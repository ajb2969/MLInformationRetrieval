<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="304">Synaptic weight</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Synaptic weight</h1>
<hr/>

<p>In <a class="uri" href="neuroscience" title="wikilink">neuroscience</a> and <a href="computer_science" title="wikilink">computer science</a>, <strong>synaptic weight</strong> refers to the strength or amplitude of a connection between two nodes, corresponding in biology to the amount of influence the <a href="action_potential" title="wikilink">firing</a> of one <a class="uri" href="neuron" title="wikilink">neuron</a> has on another. The term is typically used in <a href="artificial_neural_network" title="wikilink">artificial</a> and <a href="biological_neural_network" title="wikilink">biological</a> <a href="neural_network" title="wikilink">neural network</a> research.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="computation">Computation</h2>

<p>In a computational neural network, a <a href="Euclidean_vector" title="wikilink">vector</a> or set of inputs 

<math display="inline" id="Synaptic_weight:2">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

 and outputs 

<math display="block" id="Synaptic_weight:3">
 <semantics>
  <mrow>
   <msub>
    <mi>y</mi>
    <mi>j</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <munder>
     <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
     <mi>i</mi>
    </munder>
    <mrow>
     <msub>
      <mi>w</mi>
      <mrow>
       <mi>i</mi>
       <mi>j</mi>
      </mrow>
     </msub>
     <mpadded width="+6.6pt">
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
     </mpadded>
     <mpadded width="+6.6pt">
      <mtext>or</mtext>
     </mpadded>
     <mtext>ùê≤</mtext>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>w</mi>
    <mtext>ùê±</mtext>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <ci>j</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <ci>i</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <apply>
         <times></times>
         <ci>i</ci>
         <ci>j</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
       <mtext>or</mtext>
       <mtext>y</mtext>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>w</ci>
      <mtext>x</mtext>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{j}=\sum_{i}w_{ij}x_{i}~{}~{}\textrm{or}~{}~{}\textbf{y}=w\textbf{x}
  </annotation>
 </semantics>
</math>

, or pre- and post-synaptic neurons respectively, are interconnected with synaptic weights represented by the matrix <span class="LaTeX">$w$</span>, where for a linear neuron</p>

<p><span class="LaTeX">$$y_j = \sum_i w_{ij} x_i ~~\textrm{or}~~ \textbf{y} = w\textbf{x}$$</span>.</p>

<p>The synaptic weight is changed by using a learning rule, the most basic of which is <a href="Hebb's_rule" title="wikilink">Hebb's rule</a>, which is usually stated in biological terms as</p>
<blockquote>

<p><em>Neurons that fire together, wire together.</em></p>
</blockquote>

<p>Computationally, this means that if a large signal from one of the input neurons results in a large signal from one of the output neurons, then the synaptic weight between those two neurons will increase. The rule is unstable, however, and is typically modified using such variations as <a href="Oja's_rule" title="wikilink">Oja's rule</a>, <a href="radial_basis_functions" title="wikilink">radial basis functions</a> or the <a class="uri" href="backpropagation" title="wikilink">backpropagation</a> algorithm.</p>
<h2 id="biology">Biology</h2>

<p>For biological networks, the effect of synaptic weights is not as simple as for linear neurons or <a href="Hebbian_learning" title="wikilink">Hebbian learning</a>. However, <a href="biophysics" title="wikilink">biophysical</a> models such as <a href="BCM_theory" title="wikilink">BCM theory</a> have seen some success in mathematically describing these networks.</p>

<p>In the mammalian <a href="central_nervous_system" title="wikilink">central nervous system</a>, signal transmission is carried out by interconnected networks of nerve cells, or neurons. For the basic <a href="pyramidal_neuron" title="wikilink">pyramidal neuron</a>, the input signal is carried by the <a class="uri" href="axon" title="wikilink">axon</a>, which releases neurotransmitter chemicals into the <a class="uri" href="synapse" title="wikilink">synapse</a> which is picked up by the <a class="uri" href="dendrites" title="wikilink">dendrites</a> of the next neuron, which can then generate an <a href="action_potential" title="wikilink">action potential</a> which is analogous to the output signal in the computational case.</p>

<p>The synaptic weight in this process is determined by several variable factors:</p>
<ul>
<li>How well the input signal propagates through the axon (see <a class="uri" href="myelination" title="wikilink">myelination</a>),</li>
<li>The amount of neurotransmitter released into the synapse and the amount that can be absorbed in the following cell (determined by the number of <a href="AMPA_receptor" title="wikilink">AMPA</a> and <a href="NMDA_receptor" title="wikilink">NMDA receptors</a> on the cell membrane and the amount of intracellular <a class="uri" href="calcium" title="wikilink">calcium</a> and other ions),</li>
<li>The number of such connections made by the axon to the dendrites,</li>
<li>How well the signal propagates and <a href="integrate-and-fire" title="wikilink">integrates</a> in the postsynaptic cell.</li>
</ul>

<p>The changes in synaptic weight that occur is known as <a href="synaptic_plasticity" title="wikilink">synaptic plasticity</a>, and the process behind long-term changes (<a href="long-term_potentiation" title="wikilink">long-term potentiation</a> and <a href="long-term_depression" title="wikilink">depression</a>) is still poorly understood. Hebb's original learning rule was originally applied to biological systems, but has had to undergo many modifications as a number of theoretical and experimental problems came to light.</p>
<h2 id="references">References</h2>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Neural_network" title="wikilink">Neural network</a></li>
<li><a href="Synaptic_plasticity" title="wikilink">Synaptic plasticity</a></li>
<li><a href="Hebbian_theory" title="wikilink">Hebbian theory</a></li>
</ul>

<p>"</p>

<p><a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a> <a href="Category:Neural_networks" title="wikilink">Category:Neural networks</a> <a class="uri" href="Category:Neuroscience" title="wikilink">Category:Neuroscience</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">‚Ü©</a></li>
</ol>
</section>
</body>
</html>
