   Transfer entropy      Transfer entropy  '''Transfer entropy''' is a [[non-parametric statistics|non-parametric statistic]] measuring the amount of directed (time-asymmetric) transfer of [[information]] between two [[random process]]es. {{cite journal|last=Schreiber|first=Thomas|title=Measuring Information Transfer|journal=Physical Review Letters|date=1 July 2000|volume=85|issue=2|pages=461–464|doi=10.1103/PhysRevLett.85.461|url=http://prl.aps.org/abstract/PRL/v85/i2/p461_1}} {{cite encyclopedia |year= 2007 |title = Grang er causality |last= Seth |first=Anil|encyclopedia= Scholarpedia |url= http://www.scholarpedia.org/article/Granger_causality|doi=10.4249/scholarpedia.1667 }} 1 Transfer entropy from a process X to another process Y is the amount of uncertainty reduced in future values of Y by knowing the past values of X given past values of Y . More specifically, if    X  t     subscript  X  t    X_{t}   and    Y  t     subscript  Y  t    Y_{t}   for    t  ∈  ℕ      t  ℕ    t\in\mathbb{N}   denote two random processes and the amount of information is measured using Shannon's entropy , the transfer entropy can be written as:       T   X  →  Y    =  H   (   Y  t   ∣   Y    t  -  1   :   t  -  L     )   -  H   (   Y  t   ∣   Y    t  -  1   :   t  -  L     ,   X    t  -  1   :   t  -  L     )   ,     fragments   subscript  T   normal-→  X  Y     H   fragments  normal-(   subscript  Y  t   normal-∣   subscript  Y   normal-:    t  1     t  L     normal-)    H   fragments  normal-(   subscript  Y  t   normal-∣   subscript  Y   normal-:    t  1     t  L     normal-,   subscript  X   normal-:    t  1     t  L     normal-)   normal-,    T_{X\rightarrow Y}=H\left(Y_{t}\mid Y_{t-1:t-L}\right)-H\left(Y_{t}\mid Y_{t-1%
 :t-L},X_{t-1:t-L}\right),     where H ( X ) is Shannon entropy of X . The above definition of transfer entropy has been extended by other types of entropy measures such as Rényi entropy . 2  Transfer entropy is conditional mutual information , 3 4 with the history of the influenced variable    Y    t  -  1   :   t  -  L       subscript  Y   normal-:    t  1     t  L      Y_{t-1:t-L}   in the condition. Transfer entropy reduces to Granger causality for vector auto-regressive processes . 5 Hence, it is advantageous when the model assumption of Granger causality doesn't hold, for example, analysis of non-linear signals . 6 7 However, it usually requires more samples for accurate estimation. 8 While it was originally defined for bivariate analysis , transfer entropy has been extended to multivariate forms, either conditioning on other potential source variables 9 or considering transfer from a collection of sources, 10 although these forms require more samples again.  Transfer entropy has been used for estimation of functional connectivity of neurons 11 12 and social influence in social networks . 13  See also   Conditional mutual information  Causality  Causality (physics)  Structural equation modeling  Rubin causal model  Mutual Information   References  External links    , a toolbox, developed in C++ and MATLAB , for computation of transfer entropy between spike trains.   , a toolbox, developed in Java and usable in MATLAB , GNU Octave and Python , for computation of transfer entropy and related information-theoretic measures in both discrete and continuous-valued data.   "  Category:Causality  Category:Nonlinear time series analysis  Category:Non-parametric statistics  Category:Entropy and information     ↩   ↩  ↩  ↩   ↩  ↩  ↩  ↩   ↩  ↩     