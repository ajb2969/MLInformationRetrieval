   Matrix Chernoff bound      Matrix Chernoff bound   For certain applications in linear algebra , it is useful to know properties of the probability distribution of the largest eigenvalue of a finite sum of random matrices . Suppose    {   ð—  k   }      subscript  ð—  k     \{\mathbf{X}_{k}\}   is a finite sequence of random matrices. Analogous to the well-known Chernoff bound for sums of scalars, a bound on the following is sought for a given parameter t :      Pr   {     Î»  max    (    âˆ‘  k    ð—  k    )    â‰¥  t   }      Pr       subscript  Î»      subscript   k    subscript  ð—  k     t     \Pr\left\{\lambda_{\max}\left(\sum_{k}\mathbf{X}_{k}\right)\geq t\right\}   The following theorems answer this general question under various assumptions; these assumptions are named below by analogy to their classical, scalar counterparts. All of these theorems can be found in , as the specific application of a general result which is derived below. A summary of related works is given.  Matrix Gaussian and Rademacher series  Self-adjoint matrices case  Consider a finite sequence    {   ð€  k   }      subscript  ð€  k     \{\mathbf{A}_{k}\}   of fixed, self-adjoint matrices with dimension   d   d   d   , and let    {   Î¾  k   }      subscript  Î¾  k     \{\xi_{k}\}   be a finite sequence of independent standard normal or independent Rademacher random variables.  Then, for all    t  â‰¥  0      t  0    t\geq 0   ,       Pr   {     Î»  max    (    âˆ‘  k     Î¾  k    ð€  k     )    â‰¥  t   }    â‰¤   d  â‹…   e   -     t  2   /  2    Ïƒ  2            Pr       subscript  Î»  max     subscript   k      subscript  Î¾  k    subscript  ð€  k      t     normal-â‹…  d   superscript  e         superscript  t  2   2    superscript  Ïƒ  2         \Pr\left\{\lambda_{\text{max}}\left(\sum_{k}\xi_{k}\mathbf{A}_{k}\right)\geq t%
 \right\}\leq d\cdot e^{-t^{2}/2\sigma^{2}}   where        Ïƒ  2   =   âˆ¥    âˆ‘  k    ð€  k  2    âˆ¥    .       superscript  Ïƒ  2    norm    subscript   k    subscript   superscript  ð€  2   k       \sigma^{2}=\bigg\|\sum_{k}\mathbf{A}^{2}_{k}\bigg\|.     Rectangular case  Consider a finite sequence    {   ð  k   }      subscript  ð  k     \{\mathbf{B}_{k}\}   of fixed, self-adjoint matrices with dimension     d  1   Ã—   d  2        subscript  d  1    subscript  d  2     d_{1}\times d_{2}   , and let    {   Î¾  k   }      subscript  Î¾  k     \{\xi_{k}\}   be a finite sequence of independent standard normal or independent Rademacher random variables. Define the variance parameter        Ïƒ  2   =   max   {   âˆ¥    âˆ‘  k     ð  k    ð  k  *     âˆ¥   ,   âˆ¥    âˆ‘  k     ð  k  *    ð  k     âˆ¥   }     .       superscript  Ïƒ  2      norm    subscript   k      subscript  ð  k    superscript   subscript  ð  k         norm    subscript   k      superscript   subscript  ð  k      subscript  ð  k         \sigma^{2}=\max\left\{\bigg\|\sum_{k}\mathbf{B}_{k}\mathbf{B}_{k}^{*}\bigg\|,%
 \bigg\|\sum_{k}\mathbf{B}_{k}^{*}\mathbf{B}_{k}\bigg\|\right\}.     Then, for all    t  â‰¥  0      t  0    t\geq 0   ,        Pr   {    âˆ¥    âˆ‘  k     Î¾  k    ð  k     âˆ¥   â‰¥  t   }    â‰¤    (    d  1   +   d  2    )   â‹…   e   -     t  2   /  2    Ïƒ  2        .       Pr     norm    subscript   k      subscript  Î¾  k    subscript  ð  k      t     normal-â‹…     subscript  d  1    subscript  d  2     superscript  e         superscript  t  2   2    superscript  Ïƒ  2         \Pr\left\{\bigg\|\sum_{k}\xi_{k}\mathbf{B}_{k}\bigg\|\geq t\right\}\leq(d_{1}+%
 d_{2})\cdot e^{-t^{2}/2\sigma^{2}}.     Matrix Chernoff inequalities  The classical Chernoff bounds concerns the sum of independent, nonnegative, and uniformly bounded random variables. In the matrix setting, the analogous theorem concerns a sum of positive-semidefinite random matrices subjected to a uniform eigenvalue bound.  Matrix Chernoff I  Consider a finite sequence    {   ð—  k   }      subscript  ð—  k     \{\mathbf{X}_{k}\}   of independent, random, self-adjoint matrices with dimension   d   d   d   . Assume that each random matrix satisfies        ð—  k   âª°   ðŸŽ  and       Î»  max    (   ð—  k   )    â‰¤  R      formulae-sequence   succeeds-or-equals   subscript  ð—  k    0  and         subscript  Î»  max    subscript  ð—  k    R     \mathbf{X}_{k}\succeq\mathbf{0}\quad\text{and}\quad\lambda_{\text{max}}(%
 \mathbf{X}_{k})\leq R   almost surely.  Define         Î¼  min   =     Î»  min    (    âˆ‘  k     ð”¼    ð—  k     )    and      Î¼  max   =    Î»  max    (    âˆ‘  k     ð”¼    ð—  k     )      .     formulae-sequence     subscript  Î¼  min       subscript  Î»  min     subscript   k     ð”¼   subscript  ð—  k      and       subscript  Î¼  max      subscript  Î»  max     subscript   k     ð”¼   subscript  ð—  k         \mu_{\text{min}}=\lambda_{\text{min}}\left(\sum_{k}\mathbb{E}\,\mathbf{X}_{k}%
 \right)\quad\text{and}\quad\mu_{\text{max}}=\lambda_{\text{max}}\left(\sum_{k}%
 \mathbb{E}\,\mathbf{X}_{k}\right).   Then        Pr   {     Î»  min    (    âˆ‘  k    ð—  k    )    â‰¤    (   1  -  Î´   )    Î¼  min     }    â‰¤   d  â‹…    [    e   -  Î´      (   1  -  Î´   )    1  -  Î´     ]     Î¼  min   /  R        for  Î´   âˆˆ    [  0  ,  1  ]   , and       formulae-sequence     Pr       subscript  Î»  min     subscript   k    subscript  ð—  k         1  Î´    subscript  Î¼  min       normal-â‹…  d   superscript   delimited-[]     superscript  e    Î´     superscript    1  Î´     1  Î´         subscript  Î¼  min   R          for  Î´      0  1   , and      \Pr\left\{\lambda_{\text{min}}\left(\sum_{k}\mathbf{X}_{k}\right)\leq(1-\delta%
 )\mu_{\text{min}}\right\}\leq d\cdot\left[\frac{e^{-\delta}}{(1-\delta)^{1-%
 \delta}}\right]^{\mu_{\text{min}}/R}\quad\text{for }\delta\in[0,1]\text{, and}           Pr   {     Î»  max    (    âˆ‘  k    ð—  k    )    â‰¥    (   1  +  Î´   )    Î¼  max     }    â‰¤   d  â‹…    [    e  Î´     (   1  +  Î´   )    1  +  Î´     ]     Î¼  max   /  R        for  Î´   â‰¥  0.      formulae-sequence     Pr       subscript  Î»  max     subscript   k    subscript  ð—  k         1  Î´    subscript  Î¼  max       normal-â‹…  d   superscript   delimited-[]     superscript  e  Î´    superscript    1  Î´     1  Î´         subscript  Î¼  max   R          for  Î´   0.     \Pr\left\{\lambda_{\text{max}}\left(\sum_{k}\mathbf{X}_{k}\right)\geq(1+\delta%
 )\mu_{\text{max}}\right\}\leq d\cdot\left[\frac{e^{\delta}}{(1+\delta)^{1+%
 \delta}}\right]^{\mu_{\text{max}}/R}\quad\text{for }\delta\geq 0.     Matrix Chernoff II  Consider a sequence    {   ð—  k   :   k  =   1  ,  2  ,  â€¦  ,  n    }     conditional-set   subscript  ð—  k     k   1  2  normal-â€¦  n      \{\mathbf{X}_{k}:k=1,2,\ldots,n\}   of independent, random, self-adjoint matrices that satisfy        ð—  k   âª°   ðŸŽ  and       Î»  max    (   ð—  k   )    â‰¤  1      formulae-sequence   succeeds-or-equals   subscript  ð—  k    0  and         subscript  Î»  max    subscript  ð—  k    1     \mathbf{X}_{k}\succeq\mathbf{0}\quad\text{and}\quad\lambda_{\text{max}}(%
 \mathbf{X}_{k})\leq 1   almost surely.  Compute the minimum and maximum eigenvalues of the average expectation,          Î¼  Â¯   min   =     Î»  min    (    1  n     âˆ‘   k  =  1   n     ð”¼    ð—  k      )    and       Î¼  Â¯   max   =    Î»  max    (    1  n     âˆ‘   k  =  1   n     ð”¼    ð—  k      )      .     formulae-sequence     subscript   normal-Â¯  Î¼   min       subscript  Î»  min       1  n     superscript   subscript     k  1    n     ð”¼   subscript  ð—  k       and       subscript   normal-Â¯  Î¼   max      subscript  Î»  max       1  n     superscript   subscript     k  1    n     ð”¼   subscript  ð—  k          \bar{\mu}_{\text{min}}=\lambda_{\text{min}}\left(\frac{1}{n}\sum_{k=1}^{n}%
 \mathbb{E}\,\mathbf{X}_{k}\right)\quad\text{and}\quad\bar{\mu}_{\text{max}}=%
 \lambda_{\text{max}}\left(\frac{1}{n}\sum_{k=1}^{n}\mathbb{E}\,\mathbf{X}_{k}%
 \right).   Then        Pr   {     Î»  min    (    1  n     âˆ‘   k  =  1   n    ð—  k     )    â‰¤  Î±   }    â‰¤   d  â‹…   e   -  n  D   (  Î±  âˆ¥    Î¼  Â¯   min   )         for  0   â‰¤  Î±  â‰¤     Î¼  Â¯   min   , and       formulae-sequence     Pr       subscript  Î»  min       1  n     superscript   subscript     k  1    n    subscript  ð—  k      Î±     normal-â‹…  d   superscript  e   fragments   n  D   fragments  normal-(  Î±  parallel-to   subscript   normal-Â¯  Î¼   min   normal-)             for  0   Î±          subscript   normal-Â¯  Î¼   min   , and       \Pr\left\{\lambda_{\text{min}}\left(\frac{1}{n}\sum_{k=1}^{n}\mathbf{X}_{k}%
 \right)\leq\alpha\right\}\leq d\cdot e^{-nD(\alpha\|\bar{\mu}_{\text{min}})}%
 \quad\text{for }0\leq\alpha\leq\bar{\mu}_{\text{min}}\text{, and}           Pr   {     Î»  max    (    1  n     âˆ‘   k  =  1   n    ð—  k     )    â‰¥  Î±   }    â‰¤   d  â‹…   e   -  n  D   (  Î±  âˆ¥    Î¼  Â¯   max   )         for    Î¼  Â¯   max    â‰¤  Î±  â‰¤  1.      formulae-sequence     Pr       subscript  Î»  max       1  n     superscript   subscript     k  1    n    subscript  ð—  k      Î±     normal-â‹…  d   superscript  e   fragments   n  D   fragments  normal-(  Î±  parallel-to   subscript   normal-Â¯  Î¼   max   normal-)             for   subscript   normal-Â¯  Î¼   max    Î±       1.      \Pr\left\{\lambda_{\text{max}}\left(\frac{1}{n}\sum_{k=1}^{n}\mathbf{X}_{k}%
 \right)\geq\alpha\right\}\leq d\cdot e^{-nD(\alpha\|\bar{\mu}_{\text{max}})}%
 \quad\text{for }\bar{\mu}_{\text{max}}\leq\alpha\leq 1.   The binary information divergence is defined as      D   (  a  âˆ¥  u  )   =  a   (  log  a  -  log  u  )   +   (  1  -  a  )    (  log   (  1  -  a  )   -  log   (  1  -  u  )   )      fragments  D   fragments  normal-(  a  parallel-to  u  normal-)    a   fragments  normal-(   a    u  normal-)     fragments  normal-(  1   a  normal-)    fragments  normal-(    fragments  normal-(  1   a  normal-)      fragments  normal-(  1   u  normal-)   normal-)     D(a\|u)=a\left(\log a-\log u\right)+(1-a)\left(\log(1-a)-\log(1-u)\right)   for     a  ,  u   âˆˆ   [  0  ,  1  ]        a  u    0  1     a,u\in[0,1]   .  Matrix Bennett and Bernstein inequalities  In the scalar setting, Bennett and Bernstein inequalities describe the upper tail of a sum of independent, zero-mean random variables that are either bounded or subexponential . In the matrix case, the analogous results concern a sum of zero-mean random matrices.  Bounded case  Consider a finite sequence    {   ð—  k   }      subscript  ð—  k     \{\mathbf{X}_{k}\}   of independent, random, self-adjoint matrices with dimension   d   d   d   . Assume that each random matrix satisfies        ð—  k   âª°   ðŸŽ  and       Î»  max    (   ð—  k   )    â‰¤  R      formulae-sequence   succeeds-or-equals   subscript  ð—  k    0  and         subscript  Î»  max    subscript  ð—  k    R     \mathbf{X}_{k}\succeq\mathbf{0}\quad\text{and}\quad\lambda_{\text{max}}(%
 \mathbf{X}_{k})\leq R   almost surely.  Compute the norm of the total variance,        Ïƒ  2   =   âˆ¥    âˆ‘  k     ð”¼    (   ð—  k  2   )     âˆ¥    .       superscript  Ïƒ  2    norm    subscript   k     ð”¼   subscript   superscript  ð—  2   k        \sigma^{2}=\bigg\|\sum_{k}\mathbb{E}\,(\mathbf{X}^{2}_{k})\bigg\|.     Then, the following chain of inequalities holds for all    t  â‰¥  0      t  0    t\geq 0   :      Pr   {     Î»  max    (     âˆ‘  k     ð—  k    )    â‰¥  t   }      Pr       subscript  Î»  max     subscript   k    subscript  ð—  k     t     \displaystyle\Pr\left\{\lambda_{\text{max}}\left(\sum_{k}\mathbf{X}_{k}\right)%
 \geq t\right\}   The function    h   (  u  )       h  u    h(u)   is defined as     h   (  u  )    =     (   1  +  u   )    log   (   1  +  u   )     -  u         h  u         1  u       1  u     u     h(u)=(1+u)\log(1+u)-u   for    u  â‰¥  0      u  0    u\geq 0   .  Subexponential case  Consider a finite sequence    {   ð—  k   }      subscript  ð—  k     \{\mathbf{X}_{k}\}   of independent, random, self-adjoint matrices with dimension   d   d   d   . Assume that         ð”¼    ð—  k    =   ðŸŽ  and       ð”¼    (   ð—  k  p   )    âª¯      p  !   2   â‹…   R   p  -  2      ð€  k  2        formulae-sequence      ð”¼   subscript  ð—  k     0  and     precedes-or-equals    ð”¼   superscript   subscript  ð—  k   p       normal-â‹…      p   2    superscript  R    p  2      superscript   subscript  ð€  k   2       \mathbb{E}\,\mathbf{X}_{k}=\mathbf{0}\quad\text{and}\quad\mathbb{E}\,(\mathbf{%
 X}_{k}^{p})\preceq\frac{p!}{2}\cdot R^{p-2}\mathbf{A}_{k}^{2}   for    p  =   2  ,  3  ,  4  ,  â€¦       p   2  3  4  normal-â€¦     p=2,3,4,\ldots   .  Compute the variance parameter,        Ïƒ  2   =   âˆ¥    âˆ‘  k    ð€  k  2    âˆ¥    .       superscript  Ïƒ  2    norm    subscript   k    subscript   superscript  ð€  2   k       \sigma^{2}=\bigg\|\sum_{k}\mathbf{A}^{2}_{k}\bigg\|.     Then, the following chain of inequalities holds for all    t  â‰¥  0      t  0    t\geq 0   :      Pr   {     Î»  max    (     âˆ‘  k     ð—  k    )    â‰¥  t   }      Pr       subscript  Î»  max     subscript   k    subscript  ð—  k     t     \displaystyle\Pr\left\{\lambda_{\text{max}}\left(\sum_{k}\mathbf{X}_{k}\right)%
 \geq t\right\}     Rectangular case  Consider a finite sequence    {   ð™  k   }      subscript  ð™  k     \{\mathbf{Z}_{k}\}   of independent, random, matrices with dimension     d  1   Ã—   d  2        subscript  d  1    subscript  d  2     d_{1}\times d_{2}   . Assume that each random matrix satisfies         ð”¼    ð™  k    =   ðŸŽ  and      âˆ¥   ð™  k   âˆ¥   â‰¤  R      formulae-sequence      ð”¼   subscript  ð™  k     0  and       norm   subscript  ð™  k    R     \mathbb{E}\,\mathbf{Z}_{k}=\mathbf{0}\quad\text{and}\quad\|\mathbf{Z}_{k}\|\leq
 R   almost surely. Define the variance parameter        Ïƒ  2   =   max   {   âˆ¥    âˆ‘  k     ð”¼    (    ð™  k    ð™  k  *    )     âˆ¥   ,   âˆ¥    âˆ‘  k     ð”¼    (    ð™  k  *    ð™  k    )     âˆ¥   }     .       superscript  Ïƒ  2      norm    subscript   k     ð”¼     subscript  ð™  k    superscript   subscript  ð™  k          norm    subscript   k     ð”¼     superscript   subscript  ð™  k      subscript  ð™  k          \sigma^{2}=\max\left\{\bigg\|\sum_{k}\mathbb{E}\,(\mathbf{Z}_{k}\mathbf{Z}_{k}%
 ^{*})\bigg\|,\bigg\|\sum_{k}\mathbb{E}\,(\mathbf{Z}_{k}^{*}\mathbf{Z}_{k})%
 \bigg\|\right\}.     Then, for all    t  â‰¥  0      t  0    t\geq 0          Pr   {    âˆ¥    âˆ‘  k    ð™  k    âˆ¥   â‰¥  t   }    â‰¤    (    d  1   +   d  2    )   â‹…   exp   (    -   t  2      Ïƒ  2   +    R  t   /  3     )          Pr     norm    subscript   k    subscript  ð™  k     t     normal-â‹…     subscript  d  1    subscript  d  2           superscript  t  2       superscript  Ïƒ  2       R  t   3         \Pr\left\{\bigg\|\sum_{k}\mathbf{Z}_{k}\bigg\|\geq t\right\}\leq(d_{1}+d_{2})%
 \cdot\exp\left(\frac{-t^{2}}{\sigma^{2}+Rt/3}\right)     Matrix Azuma, Hoeffding, and McDiarmid inequalities  Matrix Azuma  The scalar version of Azuma's inequality states that a scalar martingale exhibits normal concentration about its mean value, and the scale for deviations is controlled by the total maximum squared range of the difference sequence. The following is the extension in matrix setting.  Consider a finite adapted sequence    {   ð—  k   }      subscript  ð—  k     \{\mathbf{X}_{k}\}   of self-adjoint matrices with dimension   d   d   d   , and a fixed sequence    {   ð€  k   }      subscript  ð€  k     \{\mathbf{A}_{k}\}   of self-adjoint matrices that satisfy          ð”¼   k  -  1      ð—  k    =   ðŸŽ  and      ð—  k  2   âª¯   ð€  k  2       formulae-sequence       subscript  ð”¼    k  1     subscript  ð—  k     0  and     precedes-or-equals   superscript   subscript  ð—  k   2    superscript   subscript  ð€  k   2      \mathbb{E}_{k-1}\,\mathbf{X}_{k}=\mathbf{0}\quad\text{and}\quad\mathbf{X}_{k}^%
 {2}\preceq\mathbf{A}_{k}^{2}   almost surely.  Compute the variance parameter        Ïƒ  2   =   âˆ¥    âˆ‘  k    ð€  k  2    âˆ¥    .       superscript  Ïƒ  2    norm    subscript   k    subscript   superscript  ð€  2   k       \sigma^{2}=\bigg\|\sum_{k}\mathbf{A}^{2}_{k}\bigg\|.     Then, for all    t  â‰¥  0      t  0    t\geq 0          Pr   {     Î»  max    (    âˆ‘  k    ð—  k    )    â‰¥  t   }    â‰¤   d  â‹…   e   -     t  2   /  8    Ïƒ  2            Pr       subscript  Î»  max     subscript   k    subscript  ð—  k     t     normal-â‹…  d   superscript  e         superscript  t  2   8    superscript  Ïƒ  2         \Pr\left\{\lambda_{\text{max}}\left(\sum_{k}\mathbf{X}_{k}\right)\geq t\right%
 \}\leq d\cdot e^{-t^{2}/8\sigma^{2}}     The constant 1/8 can be improved to 1/2 when there is additional information available. One case occurs when each summand    ð—  k     subscript  ð—  k    \mathbf{X}_{k}   is conditionally symmetric. Another example requires the assumption that    ð—  k     subscript  ð—  k    \mathbf{X}_{k}   commutes almost surely with    ð€  k     subscript  ð€  k    \mathbf{A}_{k}   .  Matrix Hoeffding  Placing addition assumption that the summands in Matrix Azuma are independent gives a matrix extension of Hoeffding's inequalities .  Consider a finite sequence    {   ð—  k   }      subscript  ð—  k     \{\mathbf{X}_{k}\}   of independent, random, self-adjoint matrices with dimension   d   d   d   , and let    {   ð€  k   }      subscript  ð€  k     \{\mathbf{A}_{k}\}   be a sequence of fixed self-adjoint matrices. Assume that each random matrix satisfies         ð”¼    ð—  k    =   ðŸŽ  and      ð—  k  2   âª¯   ð€  k  2       formulae-sequence      ð”¼   subscript  ð—  k     0  and     precedes-or-equals   superscript   subscript  ð—  k   2    superscript   subscript  ð€  k   2      \mathbb{E}\,\mathbf{X}_{k}=\mathbf{0}\quad\text{and}\quad\mathbf{X}_{k}^{2}%
 \preceq\mathbf{A}_{k}^{2}   almost surely.  Then, for all    t  â‰¥  0      t  0    t\geq 0          Pr   {     Î»  max    (    âˆ‘  k    ð—  k    )    â‰¥  t   }    â‰¤   d  â‹…   e   -     t  2   /  8    Ïƒ  2            Pr       subscript  Î»  max     subscript   k    subscript  ð—  k     t     normal-â‹…  d   superscript  e         superscript  t  2   8    superscript  Ïƒ  2         \Pr\left\{\lambda_{\text{max}}\left(\sum_{k}\mathbf{X}_{k}\right)\geq t\right%
 \}\leq d\cdot e^{-t^{2}/8\sigma^{2}}   where        Ïƒ  2   =   âˆ¥    âˆ‘  k    ð€  k  2    âˆ¥    .       superscript  Ïƒ  2    norm    subscript   k    subscript   superscript  ð€  2   k       \sigma^{2}=\bigg\|\sum_{k}\mathbf{A}^{2}_{k}\bigg\|.     An improvement of this result was established in : for all    t  â‰¥  0      t  0    t\geq 0          Pr   {     Î»  max    (    âˆ‘  k    ð—  k    )    â‰¥  t   }    â‰¤   d  â‹…   e   -     t  2   /  2    Ïƒ  2            Pr       subscript  Î»  max     subscript   k    subscript  ð—  k     t     normal-â‹…  d   superscript  e         superscript  t  2   2    superscript  Ïƒ  2         \Pr\left\{\lambda_{\text{max}}\left(\sum_{k}\mathbf{X}_{k}\right)\geq t\right%
 \}\leq d\cdot e^{-t^{2}/2\sigma^{2}}   where        Ïƒ  2   =    1  2    âˆ¥     âˆ‘  k    ð€  k  2    +    ð”¼    ð—  k  2     âˆ¥    â‰¤   âˆ¥    âˆ‘  k    ð€  k  2    âˆ¥    .         superscript  Ïƒ  2       1  2    norm      subscript   k    subscript   superscript  ð€  2   k      ð”¼   subscript   superscript  ð—  2   k             norm    subscript   k    subscript   superscript  ð€  2   k        \sigma^{2}=\frac{1}{2}\bigg\|\sum_{k}\mathbf{A}^{2}_{k}+\mathbb{E}\,\mathbf{X}%
 ^{2}_{k}\bigg\|\leq\bigg\|\sum_{k}\mathbf{A}^{2}_{k}\bigg\|.     Matrix bounded difference (McDiarmid)  In scalar setting, McDiarmid's inequality provides one common way of bounding the differences by applying Azuma's inequality to a Doob martingale . A version of the bounded differences inequality holds in the matrix setting.  Let    {   ð™  k   :   k  =   1  ,  2  ,  â€¦  ,  n    }     conditional-set   subscript  ð™  k     k   1  2  normal-â€¦  n      \{\mathbf{Z}_{k}:k=1,2,\ldots,n\}   be an independent, family of random variables, and let   ð‡   ð‡   \mathbf{H}   be a function that maps   n   n   n   variables to a self-adjoint matrix of dimension   d   d   d   . Consider a sequence    {   ð€  k   }      subscript  ð€  k     \{\mathbf{A}_{k}\}   of fixed self-adjoint matrices that satisfy         (    ð‡   (   z  1   ,  â€¦  ,   z  k   ,  â€¦  ,   z  n   )    -   ð‡   (   z  1   ,  â€¦  ,   z  k  â€²   ,  â€¦  ,   z  n   )     )   2   âª¯   ð€  k  2    ,     precedes-or-equals   superscript      ð‡    subscript  z  1   normal-â€¦   subscript  z  k   normal-â€¦   subscript  z  n       ð‡    subscript  z  1   normal-â€¦   subscript   superscript  z  normal-â€²   k   normal-â€¦   subscript  z  n      2    superscript   subscript  ð€  k   2     \left(\mathbf{H}(z_{1},\ldots,z_{k},\ldots,z_{n})-\mathbf{H}(z_{1},\ldots,z^{%
 \prime}_{k},\ldots,z_{n})\right)^{2}\preceq\mathbf{A}_{k}^{2},   where    z  i     subscript  z  i    z_{i}   and    z  i  â€²     subscript   superscript  z  normal-â€²   i    z^{\prime}_{i}   range over all possible values of    Z  i     subscript  Z  i    Z_{i}   for each index   i   i   i   . Compute the variance parameter        Ïƒ  2   =   âˆ¥    âˆ‘  k    ð€  k  2    âˆ¥    .       superscript  Ïƒ  2    norm    subscript   k    subscript   superscript  ð€  2   k       \sigma^{2}=\bigg\|\sum_{k}\mathbf{A}^{2}_{k}\bigg\|.     Then, for all    t  â‰¥  0      t  0    t\geq 0           Pr   {     Î»  max    (    ð‡   (  ð³  )    -    ð”¼   ð‡   (  ð³  )     )    â‰¥  t   }    â‰¤   d  â‹…   e   -     t  2   /  8    Ïƒ  2        ,       Pr       subscript  Î»  max       ð‡  ð³     ð”¼  ð‡  ð³     t     normal-â‹…  d   superscript  e         superscript  t  2   8    superscript  Ïƒ  2         \Pr\left\{\lambda_{\text{max}}\left(\mathbf{H}(\mathbf{z})-\mathbb{E}\,\mathbf%
 {H}(\mathbf{z})\right)\geq t\right\}\leq d\cdot e^{-t^{2}/8\sigma^{2}},   where    ð³  =   (   Z  1   ,  â€¦  ,   Z  n   )       ð³    subscript  Z  1   normal-â€¦   subscript  Z  n      \mathbf{z}=(Z_{1},\ldots,Z_{n})   .  Survey of related theorems  The first bounds of this type were derived by . Recall the theorem above for self-adjoint matrix Gaussian and Rademacher bounds : For a finite sequence    {   ð€  k   }      subscript  ð€  k     \{\mathbf{A}_{k}\}   of fixed, self-adjoint matrices with dimension   d   d   d   and for    {   Î¾  k   }      subscript  Î¾  k     \{\xi_{k}\}   a finite sequence of independent standard normal or independent Rademacher random variables, then       Pr   {     Î»  max    (    âˆ‘  k     Î¾  k    ð€  k     )    â‰¥  t   }    â‰¤   d  â‹…   e   -     t  2   /  2    Ïƒ  2            Pr       subscript  Î»  max     subscript   k      subscript  Î¾  k    subscript  ð€  k      t     normal-â‹…  d   superscript  e         superscript  t  2   2    superscript  Ïƒ  2         \Pr\left\{\lambda_{\text{max}}\left(\sum_{k}\xi_{k}\mathbf{A}_{k}\right)\geq t%
 \right\}\leq d\cdot e^{-t^{2}/2\sigma^{2}}   where        Ïƒ  2   =   âˆ¥    âˆ‘  k    ð€  k  2    âˆ¥    .       superscript  Ïƒ  2    norm    subscript   k    subscript   superscript  ð€  2   k       \sigma^{2}=\bigg\|\sum_{k}\mathbf{A}^{2}_{k}\bigg\|.   Ahlswede and Winter would give the same result, except with       Ïƒ   A  W   2   =    âˆ‘  k     Î»  max    (   ð€  k  2   )          superscript   subscript  Ïƒ    A  W    2     subscript   k      subscript  Î»     superscript   subscript  ð€  k   2       \sigma_{AW}^{2}=\sum_{k}\lambda_{\max}\left(\mathbf{A}_{k}^{2}\right)   . By comparison, the    Ïƒ  2     superscript  Ïƒ  2    \sigma^{2}   in the theorem above commutes   Î£   normal-Î£   \Sigma   and    Î»  max     subscript  Î»     \lambda_{\max}   ; that is, it is the largest eigenvalue of the sum rather than the sum of the largest eigenvalues. It is never larger than the Ahlswedeâ€“Winter value (by the norm  triangle inequality ), but can be much smaller. Therefore, the theorem above gives a tighter bound than the Ahlswedeâ€“Winter result.  The chief contribution of  was the extension of the Laplace-transform method used to prove the scalar Chernoff bound (see Chernoff bound#Theorem for additive form (absolute error) ) to the case of self-adjoint matrices. The procedure given in the derivation below. All of the recent works on this topic follow this same procedure, and the chief differences follow from subsequent steps. Ahlswede & Winter use the Goldenâ€“Thompson inequality to proceed, whereas Tropp  uses Lieb's Theorem .  Suppose one wished to vary the length of the series ( n ) and the dimensions of the matrices ( d ) while keeping the right-hand side approximately constant. Then n must vary approximately as the log of d . Several papers have attempted to establish a bound without a dependence on dimensions. Rudelson and Vershynin  give a result for matrices which are the outer product of two vectors. provide a result without the dimensional dependence for low rank matrices . The original result was derived independently from the Ahlswedeâ€“Winter approach, but  proves a similar result using the Ahlswedeâ€“Winter approach.  Finally, Oliveira  proves a result for matrix martingales independently from the Ahlswedeâ€“Winter framework. Tropp  slightly improves on the result using the Ahlswedeâ€“Winter framework. Neither result is presented in this article.  Derivation and proof  Ahlswede and Winter  The Laplace transform argument found in  is a significant result in its own right: Let   ð˜   ð˜   \mathbf{Y}   be a random self-adjoint matrix. Then        Pr   {     Î»  max    (  Y  )    â‰¥  t   }    â‰¤    inf   Î¸  >  0     {    e   -   Î¸  t     â‹…   E   [   tr   e   Î¸  ð˜     ]     }     .       Pr       subscript  Î»    Y   t      subscript  infimum    Î¸  0      normal-â‹…   superscript  e      Î¸  t      normal-E   tr   superscript  e    Î¸  ð˜           \Pr\left\{\lambda_{\max}(Y)\geq t\right\}\leq\inf_{\theta>0}\left\{e^{-\theta t%
 }\cdot\operatorname{E}\left[\operatorname{tr}e^{\theta\mathbf{Y}}\right]\right\}.     To prove this, fix    Î¸  >  0      Î¸  0    \theta>0   . Then      Pr   {     Î»  max    (  ð˜  )    â‰¥  t   }      Pr       subscript  Î»    ð˜   t     \displaystyle\Pr\left\{\lambda_{\max}(\mathbf{Y})\geq t\right\}   The second-to-last inequality is Markov's inequality . The last inequality holds since     e    Î»  max   Î¸  ð˜    =    Î»  max    e   Î¸  ð˜     â‰¤   tr   e   Î¸  ð˜            superscript  e     subscript  Î»    Î¸  ð˜       subscript  Î»     superscript  e    Î¸  ð˜           tr   superscript  e    Î¸  ð˜        e^{\lambda_{\max}\theta\mathbf{Y}}=\lambda_{\max}e^{\theta\mathbf{Y}}\leq%
 \operatorname{tr}e^{\theta\mathbf{Y}}   . Since the left-most quantity is independent of   Î¸   Î¸   \theta   , the infimum over    Î¸  >  0      Î¸  0    \theta>0   remains an upper bound for it.  Thus, our task is to understand     E  tr    e   Î¸  ð˜        normal-E  tr    superscript  e    Î¸  ð˜      \operatorname{E}\operatorname{tr}e^{\theta\mathbf{Y}}   Nevertheless, since trace and expectation are both linear, we can commute them, so it is sufficient to consider     E   e   Î¸  ð˜     :=    ðŒ  ð˜    (  Î¸  )       assign   normal-E   superscript  e    Î¸  ð˜        subscript  ðŒ  ð˜   Î¸     \operatorname{E}e^{\theta\mathbf{Y}}:=\mathbf{M}_{\mathbf{Y}}(\theta)   , which we call the matrix generating function. This is where the methods of  and  diverge. The immediately following presentation follows .  The Goldenâ€“Thompson inequality implies that        tr   ðŒ    ð—  1   +   ð—  2       (  Î¸  )    â‰¤   tr   [    (   E   e   Î¸   ð—  1      )    (   E   e   Î¸   ð—  2      )    ]    =    tr   ðŒ   ð—  1      (  Î¸  )    ðŒ   ð—  2     (  Î¸  )             tr   subscript  ðŒ     subscript  ð—  1    subscript  ð—  2      Î¸    tr     normal-E   superscript  e    Î¸   subscript  ð—  1       normal-E   superscript  e    Î¸   subscript  ð—  2                tr   subscript  ðŒ   subscript  ð—  1     Î¸   subscript  ðŒ   subscript  ð—  2    Î¸      \operatorname{tr}\mathbf{M}_{\mathbf{X}_{1}+\mathbf{X}_{2}}(\theta)\leq%
 \operatorname{tr}\left[\left(\operatorname{E}e^{\theta\mathbf{X}_{1}}\right)%
 \left(\operatorname{E}e^{\theta\mathbf{X}_{2}}\right)\right]=\operatorname{tr}%
 \mathbf{M}_{\mathbf{X}_{1}}(\theta)\mathbf{M}_{\mathbf{X}_{2}}(\theta)   , where we used the linearity of expectation several times. Suppose    ð˜  =    âˆ‘  k    ð—  k        ð˜    subscript   k    subscript  ð—  k      \mathbf{Y}=\sum_{k}\mathbf{X}_{k}   . We can find an upper bound for     tr   ðŒ  ð˜     (  Î¸  )        tr   subscript  ðŒ  ð˜    Î¸    \operatorname{tr}\mathbf{M}_{\mathbf{Y}}(\theta)   by iterating this result. Noting that     tr   (  ð€ð  )    â‰¤     tr   (  ð€  )     Î»  max     (  ð  )         tr  ð€ð       tr  ð€    subscript  Î»     ð     \operatorname{tr}(\mathbf{AB})\leq\operatorname{tr}(\mathbf{A})\lambda_{\max}(%
 \mathbf{B})   , then         tr   ðŒ  ð˜     (  Î¸  )    â‰¤   tr   [    (   E   e     âˆ‘   k  =  1    n  -  1      Î¸   ð—  k       )    (   E   e   Î¸   ð—  n      )    ]    â‰¤     tr   (   E   e     âˆ‘   k  =  1    n  -  1      Î¸   ð—  k       )     Î»  max     (   E   e   Î¸   ð—  n      )     .           tr   subscript  ðŒ  ð˜    Î¸    tr     normal-E   superscript  e    superscript   subscript     k  1      n  1      Î¸   subscript  ð—  k        normal-E   superscript  e    Î¸   subscript  ð—  n                 tr   normal-E   superscript  e    superscript   subscript     k  1      n  1      Î¸   subscript  ð—  k         subscript  Î»      normal-E   superscript  e    Î¸   subscript  ð—  n          \operatorname{tr}\mathbf{M}_{\mathbf{Y}}(\theta)\leq\operatorname{tr}\left[%
 \left(\operatorname{E}e^{\sum_{k=1}^{n-1}\theta\mathbf{X}_{k}}\right)\left(%
 \operatorname{E}e^{\theta\mathbf{X}_{n}}\right)\right]\leq\operatorname{tr}%
 \left(\operatorname{E}e^{\sum_{k=1}^{n-1}\theta\mathbf{X}_{k}}\right)\lambda_{%
 \max}(\operatorname{E}e^{\theta\mathbf{X}_{n}}).   Iterating this, we get        tr   ðŒ  ð˜     (  Î¸  )    â‰¤    (   tr  ðˆ   )    [    Î   k    Î»  max    (   E   e   Î¸   ð—  k      )    ]    =   d   e     âˆ‘  k      Î»  max    (   log   E   e   Î¸   ð—  k       )                tr   subscript  ðŒ  ð˜    Î¸      tr  ðˆ    delimited-[]     subscript  normal-Î   k    subscript  Î»     normal-E   superscript  e    Î¸   subscript  ð—  k                d   superscript  e    subscript   k      subscript  Î»        normal-E   superscript  e    Î¸   subscript  ð—  k              \operatorname{tr}\mathbf{M}_{\mathbf{Y}}(\theta)\leq(\operatorname{tr}\mathbf{%
 I})\left[\Pi_{k}\lambda_{\max}(\operatorname{E}e^{\theta\mathbf{X}_{k}})\right%
 ]=de^{\sum_{k}\lambda_{\max}\left(\log\operatorname{E}e^{\theta\mathbf{X}_{k}}%
 \right)}     So far we have found a bound with an infimum over   Î¸   Î¸   \theta   . In turn, this can be bounded. At any rate, one can see how the Ahlswedeâ€“Winter bound arises as the sum of largest eigenvalues.  Tropp  The major contribution of  is the application of Lieb's theorem where  had applied the Goldenâ€“Thompson inequality . Tropp's corollary is the following: If   H   H   H   is a fixed self-adjoint matrix and   X   X   X   is a random self-adjoint matrix, then        E  tr    e   ð‡  +  ð—     â‰¤   tr   e   ð‡  +   log   (   E   e  ð—    )             normal-E  tr    superscript  e    ð‡  ð—      tr   superscript  e    ð‡     normal-E   superscript  e  ð—          \operatorname{E}\operatorname{tr}e^{\mathbf{H}+\mathbf{X}}\leq\operatorname{tr%
 }e^{\mathbf{H}+\log(\operatorname{E}e^{\mathbf{X}})}   Proof: Let    ð˜  =   e  ð—       ð˜   superscript  e  ð—     \mathbf{Y}=e^{\mathbf{X}}   . Then Lieb's theorem tells us that       f   (  ð˜  )    =   tr   e   ð‡  +   log   (  ð˜  )             f  ð˜    tr   superscript  e    ð‡    ð˜        f(\mathbf{Y})=\operatorname{tr}e^{\mathbf{H}+\log(\mathbf{Y})}   is concave. The final step is to use Jensen's inequality to move the expectation inside the function:         E  tr    e   ð‡  +   log   (  ð˜  )       â‰¤   tr   e   ð‡  +   log   (   E  ð˜   )        .        normal-E  tr    superscript  e    ð‡    ð˜       tr   superscript  e    ð‡     normal-E  ð˜         \operatorname{E}\operatorname{tr}e^{\mathbf{H}+\log(\mathbf{Y})}\leq%
 \operatorname{tr}e^{\mathbf{H}+\log(\operatorname{E}\mathbf{Y})}.     This gives us the major result of the paper: the subadditivity of the log of the matrix generating function.  Subadditivity of log mgf  Let    ð—  k     subscript  ð—  k    \mathbf{X}_{k}   be a finite sequence of independent, random self-adjoint matrices. Then for all    Î¸  âˆˆ  â„      Î¸  â„    \theta\in\mathbb{R}   ,        tr   ðŒ     âˆ‘  k     ð—  k       (  Î¸  )    â‰¤   tr   e     âˆ‘  k      log   ðŒ   ð—  k      (  Î¸  )              tr   subscript  ðŒ    subscript   k    subscript  ð—  k      Î¸    tr   superscript  e    subscript   k        subscript  ðŒ   subscript  ð—  k     Î¸        \operatorname{tr}\mathbf{M}_{\sum_{k}\mathbf{X}_{k}}(\theta)\leq\operatorname{%
 tr}e^{\sum_{k}\log\mathbf{M}_{\mathbf{X}_{k}}(\theta)}     Proof: It is sufficient to let    Î¸  =  1      Î¸  1    \theta=1   . Expanding the definitions, we need to show that         E  tr    e     âˆ‘  k     Î¸   ð—  k       â‰¤   tr   e     âˆ‘  k     log   E   e   Î¸   ð—  k           .        normal-E  tr    superscript  e    subscript   k     Î¸   subscript  ð—  k        tr   superscript  e    subscript   k       normal-E   superscript  e    Î¸   subscript  ð—  k            \operatorname{E}\operatorname{tr}e^{\sum_{k}\theta\mathbf{X}_{k}}\leq%
 \operatorname{tr}e^{\sum_{k}\log\operatorname{E}e^{\theta\mathbf{X}_{k}}}.     To complete the proof, we use the law of total expectation . Let    E  k     subscript  normal-E  k    \operatorname{E}_{k}   be the expectation conditioned on     ð—  1   ,  â€¦  ,   ð—  k       subscript  ð—  1   normal-â€¦   subscript  ð—  k     \mathbf{X}_{1},\ldots,\mathbf{X}_{k}   . Since we assume all the    ð—  i     subscript  ð—  i    \mathbf{X}_{i}   are independent,         E   k  -  1     e   ð—  k     =   E   e   ð—  k      .        subscript  normal-E    k  1     superscript  e   subscript  ð—  k      normal-E   superscript  e   subscript  ð—  k       \operatorname{E}_{k-1}e^{\mathbf{X}_{k}}=\operatorname{E}e^{\mathbf{X}_{k}}.   Define     ðšµ  k   =   log    E   k  -  1     e   ð—  k      =    log   ðŒ   ð—  k      (  Î¸  )           subscript  ðšµ  k        subscript  normal-E    k  1     superscript  e   subscript  ð—  k                subscript  ðŒ   subscript  ð—  k     Î¸      \mathbf{\Xi}_{k}=\log\operatorname{E}_{k-1}e^{\mathbf{X}_{k}}=\log\mathbf{M}_{%
 \mathbf{X}_{k}}(\theta)   .  Finally, we have       E  tr    e     âˆ‘   k  =  1   n     ð—  k         normal-E  tr    superscript  e    superscript   subscript     k  1    n    subscript  ð—  k       \displaystyle\operatorname{E}\operatorname{tr}e^{\sum_{k=1}^{n}\mathbf{X}_{k}}   where at every step m we use Tropp's corollary with       ð‡  m   =     âˆ‘   k  =  1    m  -  1     ð—  k    +    âˆ‘   k  =   m  +  1    n    ðšµ  k          subscript  ð‡  m       superscript   subscript     k  1      m  1     subscript  ð—  k      superscript   subscript     k    m  1     n    subscript  ðšµ  k       \mathbf{H}_{m}=\sum_{k=1}^{m-1}\mathbf{X}_{k}+\sum_{k=m+1}^{n}\mathbf{\Xi}_{k}     Master tail bound  The following is immediate from the previous result:       Pr   {     Î»  max    (    âˆ‘  k    ð—  k    )    â‰¥  t   }    â‰¤    inf   Î¸  >  0     {    e   -   Î¸  t      tr   e     âˆ‘  k      log   ðŒ   ð—  k      (  Î¸  )        }         Pr       subscript  Î»      subscript   k    subscript  ð—  k     t      subscript  infimum    Î¸  0        superscript  e      Î¸  t      tr   superscript  e    subscript   k        subscript  ðŒ   subscript  ð—  k     Î¸           \Pr\left\{\lambda_{\max}\left(\sum_{k}\mathbf{X}_{k}\right)\geq t\right\}\leq%
 \inf_{\theta>0}\left\{e^{-\theta t}\operatorname{tr}e^{\sum_{k}\log\mathbf{M}_%
 {\mathbf{X}_{k}}(\theta)}\right\}   All of the theorems given above are derived from this bound; the theorems consist in various ways to bound the infimum. These steps are significantly simpler than the proofs given.  References              "  Category:Linear algebra   