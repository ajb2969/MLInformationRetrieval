   Oja's rule      Oja's rule   Oja's learning rule , or simply Oja's rule , named after Finnish computer scientist Erkki Oja , is a model of how neurons in the brain or in artificial neural networks change connection strength, or learn, over time. It is a modification of the standard Hebb's Rule (see Hebbian learning ) that, through multiplicative normalization, solves all stability problems and generates an algorithm for principal components analysis . This is a computational form of an effect which is believed to happen in biological neurons.  Theory  Oja's rule requires a number of simplifications to derive, but in its final form it is demonstrably stable, unlike Hebb's rule. It is a single-neuron special case of the Generalized Hebbian Algorithm . However, Oja's rule can also be generalized in other ways to varying degrees of stability and success.  Formula  Oja's rule defines the change in presynaptic weights   𝐰   𝐰   \mathbf{w}   given the output response   y   y   y   of a neuron to its inputs   𝐱   𝐱   \mathbf{x}   to be         Δ    𝐰    =    𝐰   n  +  1    -    𝐰  n     =    η    y  n    (    𝐱  n   -    y  n    𝐰  n     )     ,          normal-Δ  𝐰      subscript  𝐰    n  1     subscript  𝐰  n           η   subscript  y  n      subscript  𝐱  n      subscript  y  n    subscript  𝐰  n         \,\Delta\mathbf{w}~{}=~{}\mathbf{w}_{n+1}-\mathbf{w}_{n}~{}=~{}\eta\,y_{n}(%
 \mathbf{x}_{n}-y_{n}\mathbf{w}_{n}),     where   η   η   η   is the learning rate which can also change with time. Note that the bold symbols are vectors and   n   n   n   defines a discrete time iteration. The rule can also be made for continuous iterations as          d  𝐰    d  t     =    η   y   (  t  )    (    𝐱   (  t  )    -   y   (  t  )   𝐰   (  t  )     )     .          d  𝐰     d  t      η  y  t      𝐱  t     y  t  𝐰  t       \,\frac{d\mathbf{w}}{dt}~{}=~{}\eta\,y(t)(\mathbf{x}(t)-y(t)\mathbf{w}(t)).     Derivation  The simplest learning rule known is Hebb's rule, which states in conceptual terms that neurons that fire together, wire together . In component form as a difference equation, it is written        Δ    𝐰    =    η   y   (   𝐱  n   )    𝐱  n          normal-Δ  𝐰     η  y   subscript  𝐱  n    subscript  𝐱  n      \,\Delta\mathbf{w}~{}=~{}\eta\,y(\mathbf{x}_{n})\mathbf{x}_{n}   , or in scalar form with implicit   n   n   n   -dependence,         w   i    (   n  +  1   )    =    w  i   +    η   y   (  𝐱  )    x  i            subscript  w  i     n  1       subscript  w  i     η  y  𝐱   subscript  x  i       \,w_{i}(n+1)~{}=~{}w_{i}+\eta\,y(\mathbf{x})x_{i}   ,  where is again the output, this time explicitly dependent on its input vector   𝐱   𝐱   \mathbf{x}   .  Hebb's rule has synaptic weights approaching infinity with a positive learning rate. We can stop this by normalizing the weights so that each weight's magnitude is restricted between 0, corresponding to no weight, and 1, corresponding to being the only input neuron with any weight. We do this by normalizing the weight vector to be of length one:         w   i    (   n  +  1   )    =     w  i   +    η   y   (  𝐱  )    x  i       (    ∑   j  =  1   m     [    w  j   +    η   y   (  𝐱  )    x  j     ]   p    )    1  /  p            subscript  w  i     n  1         subscript  w  i     η  y  𝐱   subscript  x  i      superscript    superscript   subscript     j  1    m    superscript   delimited-[]     subscript  w  j     η  y  𝐱   subscript  x  j      p      1  p       \,w_{i}(n+1)~{}=~{}\frac{w_{i}+\eta\,y(\mathbf{x})x_{i}}{\left(\sum_{j=1}^{m}[%
 w_{j}+\eta\,y(\mathbf{x})x_{j}]^{p}\right)^{1/p}}   .  Note that in Oja's original paper, 1     p  =  2      p  2    p=2   , corresponding to quadrature (root sum of squares), which is the familiar Cartesian normalization rule. However, any type of normalization, even linear, will give the same result without loss of generality .  Our next step is to expand this into a Taylor series for a small learning rate     |  η  |   ≪  1     much-less-than    η   1    |\eta|\ll 1   , giving         w   i    (   n  +  1   )    =      w  i     (    ∑  j    w  j  p    )    1  /  p      +   η   (     y   x  i      (    ∑  j    w  j  p    )    1  /  p     -     w  i     ∑  j    y   x  j    w  j        (    ∑  j    w  j  p    )    (   1  +   1  /  p    )      )    +   O   (   η  2   )            subscript  w  i     n  1         subscript  w  i    superscript    subscript   j    superscript   subscript  w  j   p      1  p       η        y   subscript  x  i     superscript    subscript   j    superscript   subscript  w  j   p      1  p          subscript  w  i     subscript   j     y   subscript  x  j    subscript  w  j       superscript    subscript   j    superscript   subscript  w  j   p      1    1  p          O   superscript  η  2       \,w_{i}(n+1)~{}=~{}\frac{w_{i}}{\left(\sum_{j}w_{j}^{p}\right)^{1/p}}~{}+~{}%
 \eta\left(\frac{yx_{i}}{\left(\sum_{j}w_{j}^{p}\right)^{1/p}}-\frac{w_{i}\sum_%
 {j}yx_{j}w_{j}}{\left(\sum_{j}w_{j}^{p}\right)^{(1+1/p)}}\right)~{}+~{}O(\eta^%
 {2})   .  For small   η   η   η   , our higher-order terms  go to zero. We again make the specification of a linear neuron, that is, the output of the neuron is equal to the sum of the product of each input and its synaptic weight, or        y    (  𝐱  )    =    ∑   j  =  1   m     x  j    w  j           y  𝐱     superscript   subscript     j  1    m      subscript  x  j    subscript  w  j       \,y(\mathbf{x})~{}=~{}\sum_{j=1}^{m}x_{j}w_{j}   .  We also specify that our weights normalize to   1   1   1   , which will be a necessary condition for stability, so       |  𝐰  |   =     (    ∑   j  =  1   m    w  j  p    )    1  /  p     =  1          𝐰    superscript    superscript   subscript     j  1    m    superscript   subscript  w  j   p      1  p         1     \,|\mathbf{w}|~{}=~{}\left(\sum_{j=1}^{m}w_{j}^{p}\right)^{1/p}~{}=~{}1   ,  which, when substituted into our expansion, gives Oja's rule, or         w   i    (   n  +  1   )    =    w  i   +    η   y   (    x  i   -    w  i   y    )            subscript  w  i     n  1       subscript  w  i     η  y     subscript  x  i      subscript  w  i   y        \,w_{i}(n+1)~{}=~{}w_{i}+\eta\,y(x_{i}-w_{i}y)   .  Stability and PCA  In analyzing the convergence of a single neuron evolving by Oja's rule, one extracts the first principal component , or feature, of a data set. Furthermore, with extensions using the Generalized Hebbian Algorithm , one can create a multi-Oja neural network that can extract as many features as desired, allowing for principal components analysis .  A principal component is extracted from a dataset   𝐱   𝐱   \mathbf{x}   through some associated vector , or  q j ⋅ x }} , and we can restore our original dataset by taking       𝐱   =    ∑  j     a  j    𝐪  j         𝐱    subscript   j      subscript  a  j    subscript  𝐪  j       \mathbf{x}~{}=~{}\sum_{j}a_{j}\mathbf{q}_{j}   .  In the case of a single neuron trained by Oja's rule, we find the weight vector converges to , or the first principal component, as time or number of iterations approaches infinity. We can also define, given a set of input vectors , that its correlation matrix  X i X j }} has an associated eigenvector given by with eigenvalue  . The variance of outputs of our Oja neuron ⟨y 2 ( n )⟩}} then converges with time iterations to the principal eigenvalue, or        lim   n  →  ∞      σ  2    (  n  )     =   λ  1         subscript    normal-→  n        superscript  σ  2   n     subscript  λ  1     \lim_{n\rightarrow\infty}\sigma^{2}(n)~{}=~{}\lambda_{1}   .  These results are derived using Lyapunov function analysis, and they show that Oja's neuron necessarily converges on strictly the first principal component if certain conditions are met in our original learning rule. Most importantly, our learning rate   η   η   η   is allowed to vary with time, but only such that its sum is divergent but its power sum is convergent , that is         ∑   n  =  1   ∞    η   (  n  )     =  ∞   ,      ∑   n  =  1   ∞    η    (  n  )   p     <  ∞   ,   p  >  1       formulae-sequence      superscript   subscript     n  1        η  n       formulae-sequence      superscript   subscript     n  1        η   superscript  n  p         p  1      \sum_{n=1}^{\infty}\eta(n)=\infty,~{}~{}~{}\sum_{n=1}^{\infty}\eta(n)^{p}<%
 \infty,~{}~{}~{}p>1   .  Our output activation function     y   (   𝐱   (  n  )    )       y    𝐱  n     y(\mathbf{x}(n))   is also allowed to be nonlinear and nonstatic, but it must be continuously differentiable in both   𝐱   𝐱   \mathbf{x}   and   𝐰   𝐰   \mathbf{w}   and have derivatives bounded in time. 2  Generalizations  Recently, in the context of associative learning, it has been shown that the Hebbian rule, which is similar to Oja's rule, can be generalized using an Ising-like model: 3 The main idea of the generalization is based on formulating energy function like in Ising model and then applying stochastic gradient descent algorithm to this energy function. The energy function and the update rule corresponding to following the derivative are given by:       E   (  𝐰  )    =    -   h  𝐰    -   b   𝐰  ⊤   𝐕𝐰   -   c   𝐰  ⊤   𝐱  y          E  𝐰         h  𝐰      b   superscript  𝐰  top   𝐕𝐰     c   superscript  𝐰  top   𝐱  y      E(\mathbf{w})=-h\mathbf{w}-b\mathbf{w}^{\top}\mathbf{V}\mathbf{w}-c\mathbf{w}^%
 {\top}\mathbf{x}y   ,       𝐰   n  +  1    =    𝐰  n   +   η   (   h  +   b   (   𝐕  +   𝐕  ⊤    )    𝐰  n    +   c   𝐱   n  +  1     y   n  +  1      )          subscript  𝐰    n  1       subscript  𝐰  n     η    h    b    𝐕   superscript  𝐕  top     subscript  𝐰  n      c   subscript  𝐱    n  1     subscript  y    n  1          \mathbf{w}_{n+1}=\mathbf{w}_{n}+\eta(h+b(\mathbf{V}+\mathbf{V}^{\top})\mathbf{%
 w}_{n}+c\mathbf{x}_{n+1}y_{n+1})   ,  where:    y  ∈   {   -  1   ,  1  }       y     1   1     y\in\{-1,1\}   ,    b  ∈  ℝ      b  ℝ    b\in\mathbb{R}   is the coupling among inputs,    c  >  0      c  0    c>0   is the correlation strength between the model and the output,    h  ∈  ℝ      h  ℝ    h\in\mathbb{R}   corresponds to the presence of an external magnetic field,    𝐕  ∈    {  0  ,  1  }    D  ×  D        𝐕   superscript   0  1     D  D      \mathbf{V}\in\{0,1\}^{D\times D}   determines the connections among inputs.  Then, for    h  =  0      h  0    h=0   ,    b  =  0      b  0    b=0   , and    c  =  1      c  1    c=1   we get the Hebbian rule, and for    h  =  0      h  0    h=0   ,    b  =   -  0.5       b    0.5     b=-0.5   ,    c  =  1      c  1    c=1   , and    𝐕  =  𝐈      𝐕  𝐈    \mathbf{V}=\mathbf{I}   , where   𝐈   𝐈   \mathbf{I}   is an identity matrix, introduce weight decay. The formula then reduces to:       𝐰   n  +  1    =    𝐰  n   +   η   (    2  b   𝐰  n    +    𝐱   n  +  1     y   n  +  1      )          subscript  𝐰    n  1       subscript  𝐰  n     η      2  b   subscript  𝐰  n       subscript  𝐱    n  1     subscript  y    n  1          \mathbf{w}_{n+1}=\mathbf{w}_{n}+\eta(2b\mathbf{w}_{n}+\mathbf{x}_{n+1}y_{n+1})   ,  Applications  Oja's rule was originally described in Oja's 1982 paper, 4 but the principle of self-organization to which it is applied is first attributed to Alan Turing in 1952. 5 PCA has also had a long history of use before Oja's rule formalized its use in network computation in 1989. The model can thus be applied to any problem of self-organizing mapping , in particular those in which feature extraction is of primary interest. Therefore, Oja's rule has an important place in image and speech processing. It is also useful as it expands easily to higher dimensions of processing, thus being able to integrate multiple outputs quickly. A canonical example is its use in binocular vision . 6  Biology and Oja's subspace rule  There is clear evidence for both long-term potentiation and long-term depression in biological neural networks, along with a normalization effect in both input weights and neuron outputs. However, while there is no direct experimental evidence yet of Oja's rule active in a biological neural network, a biophysical derivation of a generalization of the rule is possible. Such a derivation requires retrograde signalling from the postsynaptic neuron, which is biologically plausible (see neural backpropagation ), and takes the form of        Δ    w   i  j      ∝    ⟨    x  i    y  j    ⟩   -   ϵ   ⟨    (    c  pre   *    ∑  k     w   i  k     y  k      )   ⋅   (    c  post   *   y  j    )    ⟩      ,     proportional-to    normal-Δ   subscript  w    i  j        delimited-⟨⟩     subscript  x  i    subscript  y  j       ϵ   delimited-⟨⟩   normal-⋅     subscript  c  pre     subscript   k      subscript  w    i  k     subscript  y  k         subscript  c  post    subscript  y  j          \Delta w_{ij}~{}\propto~{}\langle x_{i}y_{j}\rangle-\epsilon\left\langle\left(%
 c_{\mathrm{pre}}*\sum_{k}w_{ik}y_{k}\right)\cdot\left(c_{\mathrm{post}}*y_{j}%
 \right)\right\rangle,     where as before is the synaptic weight between the   i   i   i   th input and   j   j   j   th output neurons,   x   x   x   is the input,   y   y   y   is the postsynaptic output, and we define   ε   ε   ε   to be a constant analogous the learning rate, and and are presynaptic and postsynaptic functions that model the weakening of signals over time. Note that the angle brackets denote the average and the  operator is a convolution . By taking the pre- and post-synaptic functions into frequency space and combining integration terms with the convolution, we find that this gives an arbitrary-dimensional generalization of Oja's rule known as Oja's Subspace , 7 namely        Δ   w    =     C  x   ⋅  w   -    w  ⋅  C   y     .        normal-Δ  w      normal-⋅    C  x   w      normal-⋅  w  C   y      \Delta w~{}=~{}Cx\cdot w-w\cdot Cy.    8  See also   BCM theory  Synaptic plasticity  Self-organizing map  Principal components analysis  Independent components analysis  Generalized Hebbian Algorithm   References  External links   Oja, Erkki: Oja learning rule in Scholarpedia  Oja, Erkki: Aalto University   "  Category:Neuroscience  Category:Artificial neural networks  Category:Neural networks  Category:Biophysics     ↩  ↩  Jakub M. Tomczak, Associative learning using Ising-like model , in Advances in Systems Science, (eds.) Jerzy Świątek, Adam Grzech, Paweł Świątek, Jakub M. Tomczak, Advances in Intelligent and Soft Computing, Vol. 240, Springer-Verlag, 2014, pp. 295-304, PDF ↩    ↩  ↩  ↩     