   Ranking SVM      Ranking SVM   In machine learning , a Ranking SVM is an application of Support vector machine , which is used to solve certain ranking problems. The algorithm of ranking SVM was published by Thorsten Joachims in 2003. 1 The original purpose of Ranking SVM is to improve the performance of an internet search engine . However, it was found that Ranking SVM also can be used to solve other problems such as Rank SIFT . 2  Description  Ranking SVM, one of the pair-wise ranking methods, which is used to adaptively sort the web-pages by their relationships (how relevant) to a specific query. A mapping function is required to define such relationship. The mapping function projects each data pair (inquire and clicked web-page) onto a feature space. These features combined with user’s click-through data (which implies page ranks for a specific query) can be considered as the training data for machine learning algorithms.  Generally, Ranking SVM includes three steps in the training period:   It maps the similarities between queries and the clicked pages onto certain feature space.  It calculates the distances between any two of the vectors obtained in step 1.  It forms optimization problem which is similar to SVM classification and solves such problem with the regular SVM solver.   Background  Ranking Method  Suppose   ℂ   ℂ   \mathbb{C}   is a data set containing   C   C   C   elements    c  i     subscript  c  i    c_{i}   .   r   r   r   is a ranking method applied to   ℂ   ℂ   \mathbb{C}   . Then the   r   r   r   in   ℂ   ℂ   \mathbb{C}   can be represented as a   C   C   C   by   C   C   C   asymmetric binary matrix. If the rank of    c  i     subscript  c  i    c_{i}   is higher than the rank of    c  j     subscript  c  j    c_{j}   , i.e.      r    c  i    <    r    c  j          r   subscript  c  i      r   subscript  c  j      r\ c_{i}   , the corresponding position of this matrix is set to value of "1". Otherwise the element in that position will be set as the value "0".  Kendall’s Tau 3 4  Kendall's Tau also refers to Kendall tau rank correlation coefficient , which is commonly used to compare two ranking methods for the same data set.  Suppose    r  1     subscript  r  1    r_{1}   and    r  2     subscript  r  2    r_{2}   are two ranking method applied to data set   ℂ   ℂ   \mathbb{C}   , the Kendall's Tau between    r  1     subscript  r  1    r_{1}   and    r  2     subscript  r  2    r_{2}   can be represented as follows:       τ   (   r  1   ,   r  2   )    =    P  -  Q    P  +  Q    =   1  -    2  Q    P  +  Q             τ    subscript  r  1    subscript  r  2         P  Q     P  Q           1      2  Q     P  Q        \tau(r_{1},r_{2})={P-Q\over P+Q}=1-{2Q\over P+Q}     where   P   P   P   is the number of the same elements in the upper triangular parts of matrices of    r  1     subscript  r  1    r_{1}   and    r  2     subscript  r  2    r_{2}   ,   Q   Q   Q   is the number of the different elements in the upper triangular parts of matrices of    r  1     subscript  r  1    r_{1}   and    r  2     subscript  r  2    r_{2}   . The diagonals of the matrices are not included in the upper triangular part stated above.  Information Retrieval Quality 5 6 7  Information retrieval quality is usually evaluated by the following three measurements:   Precision  Recall  Average Precision   For an specific query to a database, let    P  r  e  l  e  v  a  n  t      P  r  e  l  e  v  a  n  t    Prelevant   be the set of relevant information elements in the database and    P  r  e  t  r  i  e  v  e  d      P  r  e  t  r  i  e  v  e  d    Pretrieved   be the set of the retrieved information elements. Then the above three measurements can be represented as follows:           P  r  e  c  i  s  i  o  n   =    |    P  r  e  l  e  v  a  n  t   ∩   P  r  e  t  r  i  e  v  e  d    |    |   P  r  e  t  r  i  e  v  e  d   |     ;                 R  e  c  a  l  l   =    |    P  r  e  l  e  v  a  n  t   ∩   P  r  e  t  r  i  e  v  e  d    |    |   P  r  e  l  e  v  a  n  t   |     ;                 A  v  e  r  a  g  e  P  r  e  c  i  s  i  o  n   =    ∫  0  1    P  r  e  c   (   R   e  c  a  l  l    )   d   R   e  c  a  l  l       ,                      P  r  e  c  i  s  i  o  n           P  r  e  l  e  v  a  n  t     P  r  e  t  r  i  e  v  e  d         P  r  e  t  r  i  e  v  e  d       missing-subexpression    missing-subexpression      missing-subexpression    missing-subexpression    missing-subexpression         R  e  c  a  l  l           P  r  e  l  e  v  a  n  t     P  r  e  t  r  i  e  v  e  d         P  r  e  l  e  v  a  n  t       missing-subexpression    missing-subexpression      missing-subexpression    missing-subexpression    missing-subexpression         A  v  e  r  a  g  e  P  r  e  c  i  s  i  o  n     superscript   subscript   0   1     P  r  e  c   subscript  R    e  c  a  l  l    d   subscript  R    e  c  a  l  l        missing-subexpression    missing-subexpression     absent   missing-subexpression    missing-subexpression      \begin{array}[]{lcl}Precision={\left|Prelevant\cap Pretrieved\right|\over\left%
 |Pretrieved\right|};\\
 \\
 \par
  Recall={\left|Prelevant\cap Pretrieved\right|\over\left|Prelevant\right|%
 };\\
 \\
 AveragePrecision=\int_{0}^{1}{Prec(R_{ecall})}dR_{ecall},\\
 \par
 \end{array}     where    P  r  e  c   (   R   e  c  a  l  l    )       P  r  e  c   subscript  R    e  c  a  l  l      Prec(R_{ecall})   is the Precision function of    R  e  c  a  l  l      R  e  c  a  l  l    Recall   .  Let    r  *     superscript  r     r^{*}   and    r   f   (  q  )       subscript  r    f  q     r_{f(q)}   be the expected and proposed ranking methods of a database respectively, the lower bound of Average Precision of method    r   f   (  q  )       subscript  r    f  q     r_{f(q)}   can be represented as follows:       A  v  g  P  r  e  c   (   r   f   (  q  )     )    ≧    1  R     [   Q  +   (       R  +  1       2      )    ]    -  1      (    ∑   i  =  1   R    i    )   2          A  v  g  P  r  e  c   subscript  r    f  q         1  R    superscript   delimited-[]    Q   binomial    R  1   2       1     superscript    superscript   subscript     i  1    R     i    2      AvgPrec(r_{f(q)})\geqq{1\over R}\left[Q+{\left({{R+1}\atop{2}}\right)}\right]^%
 {-1}(\sum_{i=1}^{R}\sqrt{i})^{2}     where   Q   Q   Q   is the number of different elements in the upper triangular parts of matrices of    r  *     superscript  r     r^{*}   and    r   f   (  q  )       subscript  r    f  q     r_{f(q)}   and   R   R   R   is the number of relevant elements in the data set.  SVM Classifier 8  Suppose    (    x  →   i   ,   y  i   )      subscript   normal-→  x   i    subscript  y  i     (\vec{x}_{i},y_{i})   is the element of a training data set, where     x  →   i     subscript   normal-→  x   i    \vec{x}_{i}   is the feature vector (with information about features) and    y  i     subscript  y  i    y_{i}   is the label(which classifies the category of     x  →   i     subscript   normal-→  x   i    \vec{x}_{i}   ). An typical SVM classifier for such data set can be defined as the solution of the following optimization problem.          m  i  n  i  m  i  z  e   :    V   (   w  →   ,   ξ  →   )    =      1  2    w  →    ⋅   w  →    +   C  F   ∑   ξ  i  σ                s  .  t   .              σ  ≧  0   ;            ∀    y  i    (     w  →     x  →   i    +  b   )     ≧   1  -   ξ  i  σ     ;               w  h  e  r  e               b   i   s    a   s  c  a  l  a  r   ;            ∀   y  i    ∈   {   -  1   ,  1  }    ;            ∀   ξ  i    ≧  0   ;                 normal-:    m  i  n  i  m  i  z  e       V    normal-→  w    normal-→  ξ        normal-⋅      1  2    normal-→  w     normal-→  w      C  F     superscript   subscript  ξ  i   σ         missing-subexpression    missing-subexpression      formulae-sequence  s  t    missing-subexpression    missing-subexpression         σ  0    missing-subexpression    missing-subexpression        for-all     subscript  y  i        normal-→  w    subscript   normal-→  x   i    b       1   superscript   subscript  ξ  i   σ      missing-subexpression    missing-subexpression      missing-subexpression    missing-subexpression       w  h  e  r  e    missing-subexpression    missing-subexpression         b  i  s  a  s  c  a  l  a  r    missing-subexpression    missing-subexpression        for-all   subscript  y  i       1   1     missing-subexpression    missing-subexpression        for-all   subscript  ξ  i    0    missing-subexpression    missing-subexpression      missing-subexpression    missing-subexpression      \begin{array}[]{lcl}minimize:V(\vec{w},\vec{\xi})={1\over 2}\vec{w}\cdot\vec{w%
 }+CF\sum{\xi_{i}^{\sigma}}\\
 s.t.\\
 \begin{array}[]{lcl}\sigma\geqq 0;\\
 \par
 \forall y_{i}(\vec{w}\vec{x}_{i}+b)\geqq 1-\xi_{i}^{\sigma};\end{array}%
 \par
 \\
 \par
  where\\
 \begin{array}[]{lcl}b\ is\ a\ scalar;\\
 \forall y_{i}\in\left\{-1,1\right\};\\
 \forall\xi_{i}\geqq 0;\\
 \end{array}\end{array}     The solution of the above optimization problem can be represented as a linear combination of the feature vectors    x  i     subscript  x  i    x_{i}   s.        w  →   *   =    ∑  i     α  i    y  i    x  i          superscript   normal-→  w       subscript   i      subscript  α  i    subscript  y  i    subscript  x  i       \vec{w}^{*}=\sum_{i}{\alpha_{i}y_{i}x_{i}}     where    α  i     subscript  α  i    \alpha_{i}   is the coefficients to be determined.  Ranking SVM algorithm  Loss Function  Let    τ   P   (  f  )       subscript  τ    P  f     \tau_{P(f)}   be the Kendall's tau between expected ranking method    r  *     superscript  r     r^{*}   and proposed method    r   f   (  q  )       subscript  r    f  q     r_{f(q)}   , it can be proved that maximizing    τ   P   (  f  )       subscript  τ    P  f     \tau_{P(f)}   helps to minimize the lower bound of the Average Precision of    r   f   (  q  )       subscript  r    f  q     r_{f(q)}   .   Expected Loss Function  9   The negative    τ   P   (  f  )       subscript  τ    P  f     \tau_{P(f)}   can be selected as the loss function to minimize the lower bound of Average Precision of    r   f   (  q  )       subscript  r    f  q     r_{f(q)}        L   e  x  p  e  c  t  e  d    =   -   τ   P   (  f  )      =   -   ∫   τ   (   r   f   (  q  )     ,   r  *   )   d  P  r   (  q  ,   r  *   )             subscript  L    e  x  p  e  c  t  e  d       subscript  τ    P  f                τ    subscript  r    f  q     superscript  r     d  P  r   q   superscript  r           L_{expected}=-\tau_{P(f)}=-\int\tau(r_{f(q)},r^{*})dPr(q,r^{*})     where    P  r   (  q  ,   r  *   )       P  r   q   superscript  r       Pr(q,r^{*})   is the statistical distribution of    r  *     superscript  r     r^{*}   to certain query   q   q   q   .   Empirical Loss Function   Since the expected loss function is not applicable, the following empirical loss function is selected for the training data in practice.       L   e  m  p  i  r  i  c  a  l    =   -    τ  S    (  f  )     =   -    1  n     ∑   i  =  1   n    τ   (   r   f   (   q  i   )     ,   r  i  *   )              subscript  L    e  m  p  i  r  i  c  a  l         subscript  τ  S   f               1  n     superscript   subscript     i  1    n     τ    subscript  r    f   subscript  q  i      superscript   subscript  r  i             L_{empirical}=-\tau_{S}(f)=-{1\over n}\sum_{i=1}^{n}{\tau(r_{f(q_{i})},r_{i}^{%
 *})}     Collecting training data     n   n   n    i.i.d. queries are applied to a database and each query corresponds to a ranking method. So The training data set has   n   n   n   elements. Each elements containing a query and the corresponding ranking method.  Feature Space  A mapping function    Φ   (  q  ,  d  )       normal-Φ   q  d     \Phi(q,d)    10 11 is required to map each query and the element of database to a feature space. Then each point in the feature space is labelled with certain rank by ranking method.  Optimization problem  The points generated by the training data are in the feature space, which also carry the rank information (the labels). These labeled points can be used to find the boundary (classifier) that specifies the order of them. In the linear case, such boundary (classifier) is a vector.  Suppose    c  i     subscript  c  i    c_{i}   and    c  j     subscript  c  j    c_{j}   are two elements in the database and denote     (   c  i   ,   c  j   )   ∈  r        subscript  c  i    subscript  c  j    r    (c_{i},c_{j})\in r   if the rank of    c  i     subscript  c  i    c_{i}   is higher than    c  j     subscript  c  j    c_{j}   in certain ranking method   r   r   r   . Let vector    w  →     normal-→  w    \vec{w}   be the linear classifier candidate in the feature space. Then the ranking problem can be translated to the following SVM classification problem.Note that one ranking method corresponds to one query.          m  i  n  i  m  i  z  e   :    V   (   w  →   ,   ξ  →   )    =      1  2    w  →    ⋅   w  →    +    C   o  n  s  t  a  n  t     ∑   ξ   i  ,  j  ,  k                 s  .  t   .              ∀   ξ   i  ,  j  ,  k     ≧  0           ∀   (   c  i   ,   c  j   )    ∈   r  k  *              w  →    (    Φ   (   q  1   ,   c  i   )    -   Φ   (   q  1   ,   c  j   )     )    ≧   1  -   ξ   i  ,  j  ,  1      ;         …            w  →    (    Φ   (   q  n   ,   c  i   )    -   Φ   (   q  n   ,   c  j   )     )    ≧   1  -   ξ   i  ,  j  ,  n      ;             w  h  e  r   e   k   ∈    {  1  ,  2  ,   …  n   }   ,  i    ,   j  ∈   {  1  ,  2  ,  …  }     .                        normal-:    m  i  n  i  m  i  z  e       V    normal-→  w    normal-→  ξ        normal-⋅      1  2    normal-→  w     normal-→  w       subscript  C    o  n  s  t  a  n  t       subscript  ξ   i  j  k          missing-subexpression    missing-subexpression      formulae-sequence  s  t    missing-subexpression    missing-subexpression          for-all   subscript  ξ   i  j  k     0    missing-subexpression    missing-subexpression        for-all    subscript  c  i    subscript  c  j      superscript   subscript  r  k       missing-subexpression    missing-subexpression          normal-→  w       normal-Φ    subscript  q  1    subscript  c  i       normal-Φ    subscript  q  1    subscript  c  j         1   subscript  ξ   i  j  1       missing-subexpression    missing-subexpression     normal-…   missing-subexpression    missing-subexpression          normal-→  w       normal-Φ    subscript  q  n    subscript  c  i       normal-Φ    subscript  q  n    subscript  c  j         1   subscript  ξ   i  j  n       missing-subexpression    missing-subexpression      formulae-sequence      w  h  e  r  e  k     1  2    normal-…  n    i      j   1  2  normal-…      missing-subexpression    missing-subexpression     absent   missing-subexpression    missing-subexpression      missing-subexpression    missing-subexpression      \begin{array}[]{lcl}minimize:V(\vec{w},\vec{\xi})={1\over 2}\vec{w}\cdot\vec{w%
 }+C_{onstant}\sum{\xi_{i,j,k}}\\
 s.t.\\
 \begin{array}[]{lcl}\forall\xi_{i,j,k}\geqq 0\\
 \forall(c_{i},c_{j})\in r_{k}^{*}\\
 \vec{w}(\Phi(q_{1},c_{i})-\Phi(q_{1},c_{j}))\geqq 1-\xi_{i,j,1};\\
 ...\\
 \vec{w}(\Phi(q_{n},c_{i})-\Phi(q_{n},c_{j}))\geqq 1-\xi_{i,j,n};\\
 \par
  where\ k\in\left\{1,2,...n\right\},\ i,j\in\left\{1,2,...\right\}.\\
 \par
 \par
 \par
 \end{array}\end{array}     The above optimization problem is identical to the classical SVM classification problem, which is the reason why this algorithm is called Ranking-SVM.  Retrieval Function  The optimal vector     w  →   *     superscript   normal-→  w      \vec{w}^{*}   obtained by the training sample is        w  →   *   =   ∑    α   k  ,  l   *   Φ   (   q  k   ,   c  i   )          superscript   normal-→  w          superscript   subscript  α   k  l      normal-Φ    subscript  q  k    subscript  c  i        \vec{w}^{*}=\sum{\alpha_{k,l}^{*}\Phi(q_{k},c_{i})}     So the retrieval function could be formed based on such optimal classifier. For new query   q   q   q   , the retrieval function first projects all elements of the database to the feature space. Then it orders these feature points by the values of their inner products with the optimal vector. And the rank of each feature point is the rank of the corresponding element of database for the query   q   q   q   .  Application of Ranking SVM  Ranking SVM can be applied to rank the pages according to the query. The algorithm can be trained using click-through data, where consists of the following three parts:   Query.  Present ranking of search results  Search results clicked on by user   The combination of 2 and 3 cannot provide full training data order which is needed to apply the full SVM algorithm. Instead, it provides a part of the ranking information of the training data. So the algorithm can be slightly revised as follows.          m  i  n  i  m  i  z  e   :    V   (   w  →   ,   ξ  →   )    =      1  2    w  →    ⋅   w  →    +    C   o  n  t  a  n  t     ∑   ξ   i  ,  j  ,  k                 s  .  t   .              ∀   ξ   i  ,  j  ,  k     ≧  0           ∀   (   c  i   ,   c  j   )    ∈   r  k    ′               w  →    (    Φ   (   q  1   ,   c  i   )    -   Φ   (   q  1   ,   c  j   )     )    ≧   1  -   ξ   i  ,  j  ,  1      ;         …            w  →    (    Φ   (   q  n   ,   c  i   )    -   Φ   (   q  n   ,   c  j   )     )    ≧   1  -   ξ   i  ,  j  ,  n      ;             w  h  e  r   e   k   ∈    {  1  ,  2  ,   …  n   }   ,  i    ,   j  ∈   {  1  ,  2  ,  …  }     .                 normal-:    m  i  n  i  m  i  z  e       V    normal-→  w    normal-→  ξ        normal-⋅      1  2    normal-→  w     normal-→  w       subscript  C    o  n  t  a  n  t       subscript  ξ   i  j  k          missing-subexpression    missing-subexpression      formulae-sequence  s  t    missing-subexpression    missing-subexpression          for-all   subscript  ξ   i  j  k     0    missing-subexpression    missing-subexpression        for-all    subscript  c  i    subscript  c  j      superscript   subscript  r  k    normal-′      missing-subexpression    missing-subexpression          normal-→  w       normal-Φ    subscript  q  1    subscript  c  i       normal-Φ    subscript  q  1    subscript  c  j         1   subscript  ξ   i  j  1       missing-subexpression    missing-subexpression     normal-…   missing-subexpression    missing-subexpression          normal-→  w       normal-Φ    subscript  q  n    subscript  c  i       normal-Φ    subscript  q  n    subscript  c  j         1   subscript  ξ   i  j  n       missing-subexpression    missing-subexpression      formulae-sequence      w  h  e  r  e  k     1  2    normal-…  n    i      j   1  2  normal-…      missing-subexpression    missing-subexpression      missing-subexpression    missing-subexpression      \begin{array}[]{lcl}minimize:V(\vec{w},\vec{\xi})={1\over 2}\vec{w}\cdot\vec{w%
 }+C_{ontant}\sum{\xi_{i,j,k}}\\
 s.t.\\
 \begin{array}[]{lcl}\forall\xi_{i,j,k}\geqq 0\\
 \forall(c_{i},c_{j})\in r_{k}^{^{\prime}}\\
 \vec{w}(\Phi(q_{1},c_{i})-\Phi(q_{1},c_{j}))\geqq 1-\xi_{i,j,1};\\
 ...\\
 \vec{w}(\Phi(q_{n},c_{i})-\Phi(q_{n},c_{j}))\geqq 1-\xi_{i,j,n};\\
 where\ k\in\left\{1,2,...n\right\},\ i,j\in\left\{1,2,...\right\}.\\
 \end{array}\end{array}     The method    r  ′     superscript  r  normal-′    r^{\prime}   does not provide ranking information of the whole dataset, it's a subset of the full ranking method. So the condition of optimization problem becomes more relax compared with the original Ranking-SVM.  References  "  Category:Articles created via the Article Wizard  Category:Support vector machines     Joachims, T. (2003), "Optimizing Search Engines using Clickthrough Data", Proceedings of the ACM Conference on Knowledge Discovery and Data Mining ↩  Bing Li; Rong Xiao; Zhiwei Li; Rui Cai; Bao-Liang Lu; Lei Zhang; "Rank-SIFT: Learning to rank repeatable local interest points",Computer Vision and Pattern Recognition (CVPR), 2011 ↩  M.Kemeny . Rank Correlation Methods, Hafner, 1955 ↩  A.Mood, F. Graybill, and D. Boes. Introduction to the Theory of Statistics. McGraw-Hill, 3rd edition, 1974 ↩  J. Kemeny and L. Snell. Mathematical Models in THE Social Sciences. Ginn & Co. 1962 ↩  Y. Yao. Measuring retrieval effectiveness based on user preference of documents. Journal of the American Society for Information Science, 46(2): 133-145, 1995. ↩  R.Baeza- Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison- Wesley-Longman, Harlow, UK, May 1999 ↩  C. Cortes and V.N Vapnik. Support-vector networks. Machine Learning Journal, 20: 273-297,1995 ↩  V.Vapnik. Statistical Learning Theory. WILEY, Chichester,GB,1998 ↩  N.Fuhr. Optimum polynomial retrieval functions based on the probability ranking principle. ACM TRANSACTIONS on Information Systems, 7(3): 183-204 ↩  N.Fuhr, S.Hartmann, G.Lustig, M.Schwantner, K.Tzeras,and G.Knorz. Air/x - a rule-based multistage indexing system for large subject fields. In RIAO,1991 ↩     