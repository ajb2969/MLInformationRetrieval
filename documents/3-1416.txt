


Eigenvalue algorithm




Eigenvalue algorithm

 table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
   margin: 0; padding: 0; vertical-align: baseline; border: none; }
 <style>
 table.sourceCode { width: 100%; line-height: 100%; }
 td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
 td.sourceCode { padding-left: 5px; }
 code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
 code > span.dt { color: #902000; } /* DataType */
 code > span.dv { color: #40a070; } /* DecVal */
 code > span.bn { color: #40a070; } /* BaseN */
 code > span.fl { color: #40a070; } /* Float */
 code > span.ch { color: #4070a0; } /* Char */
 code > span.st { color: #4070a0; } /* String */
 code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
 code > span.ot { color: #007020; } /* Other */
 code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
 code > span.fu { color: #06287e; } /* Function */
 code > span.er { color: #ff0000; font-weight: bold; } /* Error */
 code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 code > span.cn { color: #880000; } /* Constant */
 code > span.sc { color: #4070a0; } /* SpecialChar */
 code > span.vs { color: #4070a0; } /* VerbatimString */
 code > span.ss { color: #bb6688; } /* SpecialString */
 code > span.im { } /* Import */
 code > span.va { color: #19177c; } /* Variable */
 code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
 code > span.op { color: #666666; } /* Operator */
 code > span.bu { } /* BuiltIn */
 code > span.ex { } /* Extension */
 code > span.pp { color: #bc7a00; } /* Preprocessor */
 code > span.at { color: #7d9029; } /* Attribute */
 code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
 code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
 code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
 code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
   



In numerical analysis, one of the most important problems is designing efficient and stable algorithms for finding the eigenvalues of a matrix. These eigenvalue algorithms may also find eigenvectors.
Eigenvalues and eigenvectors
Given an 
 
 
square matrix

 
  of real or complex numbers, an eigenvalue

 
  and its associated generalized eigenvector

 
  are a pair obeying the relation1



where 
 
 
 
  is a nonzero 
 
 
 
  column vector, 
 
 
 
  is the 
 
 
identity matrix, 
 
 
 
  is a positive integer, and both 
 
 
 
  and 
 
 
 
  are allowed to be complex even when 
 
 
 
  is real. When 
 
 
 
 , the vector is called simply an eigenvector, and the pair is called an eigenpair. In this case, 
 
 
 
 . Any eigenvalue 
 
 
 
  of 
 
 
 
  has ordinary2 eigenvectors associated to it, for if 
 
 
 
  is the smallest integer such that  for a generalized eigenvector 
 
 
 
 , then  is an ordinary eigenvector. The value 
 
 
 
  can always be taken as less than or equal to 
 
 
 
 . In particular,  for all generalized eigenvectors 
 
 
 
  associated with 
 
 

For each eigenvalue 
 
 
 
  of 
 
 
 
 , the kernel

 
  consists of all eigenvectors associated with 
 
 
 
  (along with 0), called the eigenspace of 
 
 
 
 , while the vector space  consists of all generalized eigenvectors, and is called the generalized eigenspace. The geometric multiplicity of 
 
 
 
  is the dimension of its eigenspace. The algebraic multiplicity of 
 
 
 
  is the dimension of its generalized eigenspace. The latter terminology is justified by the equation



where 
 
 
 
  is the determinant function, the  are all the distinct eigenvalues of 
 
 
 
  and the  are the corresponding algebraic multiplicities. The function  is the characteristic polynomial of 
 
 
 
 . So the algebraic multiplicity is the multiplicity of the eigenvalue as a zero of the characteristic polynomial. Since any eigenvector is also a generalized eigenvector, the geometric multiplicity is less than or equal to the algebraic multiplicity. The algebraic multiplicities sum up to 
 
 
 
 , the degree of the characteristic polynomial. The equation  is called the characteristic equation, as its roots are exactly the eigenvalues of 
 
 
 
 . By the Cayley-Hamilton theorem, 
 
 
 
  itself obeys the same equation: 3 As a consequence, the columns of the matrix 
 
 
 
  must be either 0 or generalized eigenvectors of the eigenvalue , since they are annihilated by 
 
 
 
  In fact, the column space is the generalized eigenspace of 
Any collection of generalized eigenvectors of distinct eigenvalues is linearly independent, so a basis for all of  can be chosen consisting of generalized eigenvectors. More particularly, this basis vi{{)}}1}}}} can be chosen and organized so that
:* if  and  have the same eigenvalue, then so does  for each 
 
 
 
  between 
 
 
 
  and 
 
 
 
 , and
:* if  is not an ordinary eigenvector, and if  is its eigenvalue, then  (in particular,  must be an ordinary eigenvector). If these basis vectors are placed as the column vectors of a matrix  [ v1 v2 ... vn ]}}, then 
 
 
 
  can be used to convert 
 
 
 
  to its Jordan normal form:



where the  are the eigenvalues,  if  and  otherwise.
More generally, if 
 
 
 
  is any invertible matrix, and 
 
 
 
  is an eigenvalue of 
 
 
 
  with generalized eigenvector 
 
 
 
 , then . Thus 
 
 
 
  is an eigenvalue of  with generalized eigenvector . That is, similar matrices have the same eigenvalues.
Normal, hermitian, and real-symmetric matrices
The adjoint  of a complex matrix 
 
 
 
  is the transpose of the conjugate of 
 
 
 
 : . A square matrix 
 
 
 
  is called normal if it commutes with its adjoint: . It is called hermitian if it is equal to its adjoint: . All hermitian matrices are normal. If 
 
 
 
  has only real elements, then the adjoint is just the transpose, and 
 
 
 
  is hermitian if and only if it is symmetric. When applied to column vectors, the adjoint can be used to define the canonical inner product on : .4 Normal, hermitian, and real-symmetric matrices have several useful properties:
:* Every generalized eigenvector of a normal matrix is an ordinary eigenvector.
:* Any normal matrix is similar to a diagonal matrix, since its Jordan normal form is diagonal.
:* Eigenvectors of distinct eigenvalues of a normal matrix are orthogonal.
:* For any normal matrix 
 
 
 
 ,  has an orthonormal basis consisting of eigenvectors of 
 
 
 
 . The corresponding matrix of eigenvectors is unitary.
:* The eigenvalues of a hermitian matrix are real, since  for a non-zero eigenvector 
 
 
 
 .
:* If 
 
 
 
  is real, there is an orthonormal basis for  consisting of eigenvectors of 
 
 
 
  if and only if 
 
 
 
  is symmetric.
It is possible for a real or complex matrix to have all real eigenvalues without being hermitian. For example, a real triangular matrix has its eigenvalues along its diagonal, but in general is not symmetric.
Condition number
Any problem of numeric calculation can be viewed as the evaluation of some function ƒ for some input 
 
 
 
 . The condition number

 
  of the problem is the ratio of the relative error in the function's output to the relative error in the input, and varies with both the function and the input. The condition number describes how error grows during the calculation. Its base-10 logarithm tells how many fewer digits of accuracy exist in the result than existed in the input. The condition number is a best-case scenario. It reflects the instability built into the problem, regardless of how it is solved. No algorithm can ever produce more accurate results than indicated by the condition number, except by chance. However, a poorly designed algorithm may produce significantly worse results. For example, as mentioned below, the problem of finding eigenvalues for normal matrices is always well-conditioned. However, the problem of finding the roots of a polynomial can be very ill-conditioned. Thus eigenvalue algorithms that work by finding the roots of the characteristic polynomial can be ill-conditioned even when the problem is not.
For the problem of solving the linear equation 
 
 
 
  where 
 
 
 
  is invertible, the condition number  is given by A{{!!}}op{{!!}}A−1{{!!}}op}}, where  {{!!}}op}} is the operator norm subordinate to the normal Euclidean norm on . Since this number is independent of 
 
 
 
  and is the same for 
 
 
 
  and , it is usually just called the condition number 
 
 
 
  of the matrix 
 
 
 
 . This value 
 
 
 
  is also the absolute value of the ratio of the largest eigenvalue of 
 
 
 
  to its smallest. If 
 
 
 
  is unitary, then A{{!!}}op = {{!!}}A−1{{!!}}op = 1}}, so 
 
 
 
 . For general matrices, the operator norm is often difficult to calculate. For this reason, other matrix norms are commonly used to estimate the condition number.
For the eigenvalue problem, Bauer and Fike proved that if 
 
 
 
  is an eigenvalue for a diagonalizable

 
  matrix 
 
 
 
  with eigenvector matrix 
 
 
 
 , then the absolute error in calculating 
 
 
 
  is bounded by the product of 
 
 
 
  and the absolute error in 
 
 
 
 .5 As a result, the condition number for finding 
 
 
 
  is V {{!!}}op {{!!}}V −1{{!!}}op}}. If 
 
 
 
  is normal, then 
 
 
 
  is unitary, and 
 
 
 
 . Thus the eigenvalue problem for all normal matrices is well-conditioned.
The condition number for the problem of finding the eigenspace of a normal matrix 
 
 
 
  corresponding to an eigenvalue 
 
 
 
  has been shown to be inversely proportional to the minimum distance between 
 
 
 
  and the other distinct eigenvalues of 
 
 
 
 .6 In particular, the eigenspace problem for normal matrices is well-conditioned for isolated eigenvalues. When eigenvalues are not isolated, the best that can be hoped for is to identify the span of all eigenvectors of nearby eigenvalues.
Algorithms
Any monic polynomial is the characteristic polynomial of its companion matrix. Therefore a general algorithm for finding eigenvalues could also be used to find the roots of polynomials. The Abel-Ruffini theorem shows that any such algorithm for dimensions greater than 4 must either be infinite, or involve functions of greater complexity than elementary arithmetic operations and fractional powers. For this reason algorithms that exactly calculate eigenvalues in a finite number of steps only exist for a few special classes of matrices. For general matrices, algorithms are iterative, producing better approximate solutions with each iteration.
Some algorithms produce every eigenvalue, others will produce a few, or only one. However, even the latter algorithms can be used to find all eigenvalues. Once an eigenvalue 
 
 
 
  of a matrix 
 
 
 
  has been identified, it can be used to either direct the algorithm towards a different solution next time, or to reduce the problem to one that no longer has 
 
 
 
  as a solution.
Redirection is usually accomplished by shifting: replacing 
 
 
 
  with 
 
 
 
  for some constant 
 
 
 
 . The eigenvalue found for 
 
 
 
  must have 
 
 
 
  added back in to get an eigenvalue for 
 
 
 
 . For example, for power iteration, 
 
 
 
 . Power iteration finds the largest eigenvalue in absolute value, so even when 
 
 
 
  is only an approximate eigenvalue, power iteration is unlikely to find it a second time. Conversely, inverse iteration based methods find the lowest eigenvalue, so 
 
 
 
  is chosen well away from 
 
 
 
  and hopefully closer to some other eigenvalue.
Reduction can be accomplished by restricting 
 
 
 
  to the column space of the matrix 
 
 
 
 , which 
 
 
 
  carries to itself. Since 
 
 
 
  is singular, the column space is of lesser dimension. The eigenvalue algorithm can then be applied to the restricted matrix. This process can be repeated until all eigenvalues are found.
If an eigenvalue algorithm does not produce eigenvectors, a common practice is to use an inverse iteration based algorithm with 
 
 
 
  set to a close approximation to the eigenvalue. This will quickly converge to the eigenvector of the closest eigenvalue to 
 
 
 
 . For small matrices, an alternative is to look at the column space of the product of 
 
 
 
  for each of the other eigenvalues 
 
 

Hessenberg and Tri-diagonal matrices
Because the eigenvalues of a triangular matrix are its diagonal elements, for general matrices there is no finite method like gaussian elimination to convert a matrix to triangular form while preserving eigenvalues. But it is possible to reach something close to triangular. An upper Hessenberg matrix is a square matrix for which all entries below the subdiagonal are zero. A lower Hessenberg matrix is one for which all entries above the superdiagonal are zero. Matrices that are both upper and lower Hessenberg are tridiagonal. Hessenberg and tridiagonal matrices are the starting points for many eigenvalue algorithms because the zero entries reduce the complexity of the problem. Several methods are commonly used to convert a general matrix into a Hessenberg matrix with the same eigenvalues. If the original matrix was symmetric or hermitian, then the resulting matrix will be tridiagonal.
When only eigenvalues are needed, there is no need to calculate the similarity matrix, as the transformed matrix has the same eigenvalues. If eigenvectors are needed as well, the similarity matrix may be needed to transform the eigenvectors of the Hessenberg matrix back into eigenvectors of the original matrix.




Method

Applies to

Produces

Cost without similarity matrix

Cost with similarity matrix

Description





Householder transformations

General

Hessenberg

{{cite book

last1 = Press

first1 = William H.



Givens rotations

General

Hessenberg

7


Apply planar rotations to zero out individual entries. Rotations are ordered so that later ones do not cause zero entries to become non-zero again.



Arnoldi iteration

General

Hessenberg



Perform Gram–Schmidt orthogonalization on Krylov subspaces.



Lanczos algorithm

Hermitian

Tridiagonal



Arnoldi iteration for hermitian matrices, with shortcuts.



Iterative algorithms
Iterative algorithms solve the eigenvalue problem by producing sequences that converge to the eigenvalues. Some algorithms also produce sequences of vectors that converge to the eigenvectors. Most commonly, the eigenvalue sequences are expressed as sequences of similar matrices which converge to a triangular or diagonal form, allowing the eigenvalues to be read easily. The eigenvector sequences are expressed as the corresponding similarity matrices.




Method

Applies to

Produces

Cost per step

Convergence

Description





Power iteration

General

eigenpair with largest value



Linear

Repeatedly applies the matrix to an arbitrary starting vector and renormalizes.



Inverse iteration

General



Linear

Power iteration for 



Rayleigh quotient iteration

Hermitian

eigenpair with value closest to μ


Cubic

Power iteration for  where  for each iteration is the Rayleigh quotient of the previous iteration.



Preconditioned Inverse iteration{{Citation

last=Neymeyr

first=K.

title=A geometric theory for preconditioned inverse iteration IV: On the fastest convergence cases.

journal=Linear Algebra Appl.

volume=415



Bisection method

Real Symmetric Tridiagonal

any eigenvalue


linear

Uses the bisection method to find roots of the characteristic polynomial, supported by the Sturm sequence.



Laguerre iteration

Real Symmetric Tridiagonal

any eigenvalue


cubic{{Citation

last1=Li



QR algorithm

Hessenberg

all eigenvalues



cubic

align="left" Factors A = QR, where Q is orthogonal and R is triangular, then applies the next iteration to RQ.



all eigenpairs





Jacobi eigenvalue algorithm

Real Symmetric

all eigenvalues



quadratic

Uses Givens rotations to attempt clearing all off-diagonal entries. This fails, but strengthens the diagonal.



Divide-and-conquer

Hermitian Tridiagonal

all eigenvalues




align="left" Divides the matrix into submatrices that are diagonalized then recombined.



all eigenpairs





Homotopy method

Real Symmetric Tridiagonal

all eigenpairs

{{math|O(n2){{Citation

last=Chu

first=Moody T.



Folded spectrum method

Real Symmetric

eigenpair with value closest to μ



Preconditioned inverse iteration applied to 



MRRR algorithm{{Citation

last1=Dhillon

first1=Inderjit S.

last2=Parlett

first2=Beresford N.

last3=Vömel



Direct calculation
While there is no simple algorithm to directly calculate eigenvalues for general matrices, there are numerous special classes of matrices where eigenvalues can be directly calculated. These include:
Triangular matrices
Since the determinant of a triangular matrix is the product of its diagonal entries, if T is triangular, then  Thus the eigenvalues of T are its diagonal entries.
Factorable polynomial equations
If 
 
 
 
  is any polynomial and 
 
 
 
  then the eigenvalues of 
 
 
 
  also satisfy the same equation. If 
 
 
 
  happens to have a known factorization, then the eigenvalues of 
 
 
 
  lie among its roots.
For example, a projection is a square matrix 
 
 
 
  satisfying . The roots of the corresponding scalar polynomial equation, , are 0 and 1. Thus any projection has 0 and 1 for its eigenvalues. The multiplicity of 0 as an eigenvalue is the nullity of 
 
 
 
 , while the multiplicity of 1 is the rank of 
 
 
 
 .
Another example is a matrix 
 
 
 
  that satisfies  for some scalar 
 
 
 
 . The eigenvalues must be 
 
 
 
 . The projection operators





 
  satisfy


 
  and



The column spaces of  and  are the eigenspaces of 
 
 
 
  corresponding to 
 
 
 
  and 
 
 
 
 , respectively.
2×2 matrices
For dimensions 2 through 4, formulas involving radicals exist that can be used to find the eigenvalues. While a common practice for 2×2 and 3×3 matrices, for 4×4 matrices the increasing complexity of the root formulas makes this approach less attractive.
For the 2×2 matrix



the characteristic polynomial is



Thus the eigenvalues can be found by using the quadratic formula:



Defining 
 
 
 
  to be the distance between the two eigenvalues, it is straightforward to calculate



with similar formulas for 
 
 
 
  and 
 
 
 
 . From this it follows that the calculation is well-conditioned if the eigenvalues are isolated.
Eigenvectors can be found by exploiting the Cayley-Hamilton theorem. If  are the eigenvalues, then , so the columns of  are annihilated by  and vice versa. Assuming neither matrix is zero, the columns of each must include eigenvectors for the other eigenvalue. (If either matrix is zero, then 
 
 
 
  is a multiple of the identity and any non-zero vector is an eigenvector.)
For example, suppose



then 
 
 
 
  and 
 
 
 
 , so the characteristic equation is



and the eigenvalues are 3 and -2. Now,



In both matrices, the columns are multiples of each other, so either column can be used. Thus, 
 
 
 
  can be taken as an eigenvector associated with the eigenvalue -2, and 
 
 
 
  as an eigenvector associated with the eigenvalue 3, as can be verified by multiplying them by 
 
 
 
 .
3×3 matrices
If 
 
 
 
  is a 3×3 matrix, then its characteristic equation can be expressed as:



This equation may be solved using the methods of Cardano or Lagrange, but an affine change to 
 
 
 
  will simplify the expression considerably, and lead directly to a trigonometric solution. If 
 
 
 
 , then 
 
 
 
  and 
 
 
 
  have the same eigenvectors, and 
 
 
 
  is an eigenvalue of 
 
 
 
  if and only if 
 
 
 
  is an eigenvalue of 
 
 
 
 . Letting 
 
 
 
  and 
 
 
 
 , gives



The substitution 
 
 
 
  and some simplification using the identity  reduces the equation to 
 
 
 
 . Thus



If 
 
 
 
  is complex or is greater than 2 in absolute value, the arccosine should be taken along the same branch for all three values of 
 
 
 
 . This issue doesn't arise when 
 
 
 
  is real and symmetric, resulting in a simple algorithm:8
% Given a real symmetric 3x3 matrix A, compute the eigenvalues
 
 p1 = A(1,2)^2 + A(1,3)^2 + A(2,3)^2
 if (p1 == 0) 
    % A is diagonal.
    eig1 = A(1,1)
    eig2 = A(2,2)
    eig3 = A(3,3)
 else
    q = trace(A)/3
    p2 = (A(1,1) - q)^2 + (A(2,2) - q)^2 + (A(3,3) - q)^2 + 2 * p1
    p = sqrt(p2 / 6)
    B = (1 / p) * (A - q * I)       % I is the identity matrix
    r = det(B) / 2
 
    % In exact arithmetic for a symmetric matrix  -1 <= r <= 1
    % but computation error can leave it slightly outside this range.
    if (r <= -1) 
       phi = pi / 3
    elseif (r >= 1)
       phi = 0
    else
       phi = acos(r) / 3
    end
 
    % the eigenvalues satisfy eig3 <= eig2 <= eig1
    eig1 = q + 2 * p * cos(phi)
    eig3 = q + 2 * p * cos(phi + (2*pi/3))
    eig2 = 3 * q - eig1 - eig3     % since trace(A) = eig1 + eig2 + eig3
 end
Once again, the eigenvectors of 
 
 
 
  can be obtained by recourse to the Cayley-Hamilton theorem. If  are distinct eigenvalues of 
 
 
 
 , then . Thus the columns of the product of any two of these matrices will contain an eigenvector for the third eigenvalue. However, if , then  and . Thus the generalized eigenspace of  is spanned by the columns of  while the ordinary eigenspace is spanned by the columns of . The ordinary eigenspace of  is spanned by the columns of .
For example, let



The characteristic equation is



with eigenvalues 1 (of multiplicity 2) and -1. Calculating,



and



Thus 
 
 
 
  is an eigenvector for -1, and 
 
 
 
  is an eigenvector for 1. 
 
 
 
  and 
 
 
 
  are both generalized eigenvectors associated with 1, either one of which could be combined with 
 
 
 
  and 
 
 
 
  to form a basis of generalized eigenvectors of 
 
 
 
 .In some routine eigenvectors are normalized to one, in such case make sure you normalized by column.
See also

List of eigenvalue algorithms

Notes
References
Further reading



"
Category:Numerical linear algebra



↩
The term "ordinary" is used here only to emphasize the distinction between "eigenvector" and "generalized eigenvector".↩
where the constant term is multiplied by the identity matrix 
 
 
 
 .↩
This ordering of the inner product (with the conjugate-linear position on the left), is preferred by physicists. Algebraists often place the conjugate-linear position on the right: .↩
↩
↩

↩



