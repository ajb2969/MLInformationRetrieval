<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1420">Online machine learning</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Online machine learning</h1>
<hr/>

<p>Online <a href="machine_learning" title="wikilink">machine learning</a> is used in the case where the data becomes available in a sequential fashion, in order to determine a mapping from the dataset to the corresponding labels. The key difference between online learning and batch learning (or <a href="Offline_learning" title="wikilink">"offline" learning</a>) techniques, is that in online learning the mapping is updated after the arrival of every new datapoint in a scalable fashion, whereas batch techniques are used when one has access to the entire training dataset at once. Online learning could be used in the case of a process occurring in time, for example the value of a stock given its history and other external factors, in which case the mapping updates as time goes on and we get more and more samples.</p>

<p>Ideally in online learning, the memory needed to store the function remains constant even with added datapoints, since the solution computed at one step is updated when a new datapoint becomes available, after which that datapoint can then be discarded. For many formulations, for example nonlinear <a href="kernel_methods" title="wikilink">kernel methods</a>, true online learning is not possible, though a form of hybrid online learning with recursive algorithms can be used. In this case, the space requirements are no longer guaranteed to be constant since it requires storing all previous datapoints, but the solution may take less time to compute with the addition of a new datapoint, as compared to batch learning techniques.</p>

<p>As in all machine learning problems, the goal of the algorithm is to minimize some performance criteria using a <a href="loss_function" title="wikilink">loss function</a>. For example, with stock market prediction the algorithm may attempt to minimize the <a href="mean_squared_error" title="wikilink">mean squared error</a> between the predicted and true value of a stock. Another popular performance criterion is to minimize the number of mistakes when dealing with classification problems. In addition to applications of a sequential nature, online learning algorithms are also relevant in applications with huge amounts of data such that traditional learning approaches that use the entire data set in aggregate are computationally infeasible.</p>
<h2 id="a-prototypical-online-supervised-learning-algorithm">A prototypical online supervised learning algorithm</h2>

<p>In the setting of <a href="supervised_learning" title="wikilink">supervised learning</a>, or learning from examples, we are interested in learning a function 

<math display="inline" id="Online_machine_learning:0">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>:</mo>
   <mrow>
    <mi>X</mi>
    <mo>‚Üí</mo>
    <mi>Y</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>f</ci>
    <apply>
     <ci>normal-‚Üí</ci>
     <ci>X</ci>
     <ci>Y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f:X\to Y
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Online_machine_learning:1">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 is thought of as a space of inputs and 

<math display="inline" id="Online_machine_learning:2">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

 as a space of outputs, that predicts well on instances that are drawn from a joint probability distribution 

<math display="inline" id="Online_machine_learning:3">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>p</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(x,y)
  </annotation>
 </semantics>
</math>

 on 

<math display="inline" id="Online_machine_learning:4">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>√ó</mo>
   <mi>Y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>X</ci>
    <ci>Y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X\times Y
  </annotation>
 </semantics>
</math>

. In this setting, we are given a <a href="loss_function" title="wikilink">loss function</a> 

<math display="inline" id="Online_machine_learning:5">
 <semantics>
  <mrow>
   <mi>V</mi>
   <mo>:</mo>
   <mrow>
    <mrow>
     <mi>Y</mi>
     <mo>√ó</mo>
     <mi>Y</mi>
    </mrow>
    <mo>‚Üí</mo>
    <mi>‚Ñù</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>V</ci>
    <apply>
     <ci>normal-‚Üí</ci>
     <apply>
      <times></times>
      <ci>Y</ci>
      <ci>Y</ci>
     </apply>
     <ci>‚Ñù</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V:Y\times Y\to\mathbb{R}
  </annotation>
 </semantics>
</math>

, such that 

<math display="inline" id="Online_machine_learning:6">
 <semantics>
  <mrow>
   <mi>V</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>V</ci>
    <interval closure="open">
     <apply>
      <times></times>
      <ci>f</ci>
      <ci>x</ci>
     </apply>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(f(x),y)
  </annotation>
 </semantics>
</math>

 measures the difference between the predicted value 

<math display="inline" id="Online_machine_learning:7">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)
  </annotation>
 </semantics>
</math>

 and the true value 

<math display="inline" id="Online_machine_learning:8">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

. The ideal goal is to select a function 

<math display="inline" id="Online_machine_learning:9">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>‚àà</mo>
   <mi class="ltx_font_mathcaligraphic">‚Ñã</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>f</ci>
    <ci>‚Ñã</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f\in\mathcal{H}
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Online_machine_learning:10">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">‚Ñã</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>‚Ñã</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{H}
  </annotation>
 </semantics>
</math>

 is a space of functions called a hypothesis space, so as to minimize the expected risk:</p>

<p>

<math display="block" id="Online_machine_learning:11">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>I</mi>
     <mrow>
      <mo stretchy="false">[</mo>
      <mi>f</mi>
      <mo stretchy="false">]</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mi>ùîº</mi>
     <mrow>
      <mo stretchy="false">[</mo>
      <mrow>
       <mi>V</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mi>f</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>x</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo stretchy="false">]</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mo largeop="true" symmetric="true">‚à´</mo>
     <mrow>
      <mi>V</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>f</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>,</mo>
       <mi>y</mi>
       <mo rspace="4.2pt" stretchy="false">)</mo>
      </mrow>
      <mi>d</mi>
      <mi>p</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo>,</mo>
       <mi>y</mi>
       <mo rspace="7.5pt" stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <ci>I</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <ci>f</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>ùîº</ci>
      <apply>
       <csymbol cd="latexml">delimited-[]</csymbol>
       <apply>
        <times></times>
        <ci>V</ci>
        <interval closure="open">
         <apply>
          <times></times>
          <ci>f</ci>
          <ci>x</ci>
         </apply>
         <ci>y</ci>
        </interval>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <int></int>
      <apply>
       <times></times>
       <ci>V</ci>
       <interval closure="open">
        <apply>
         <times></times>
         <ci>f</ci>
         <ci>x</ci>
        </apply>
        <ci>y</ci>
       </interval>
       <ci>d</ci>
       <ci>p</ci>
       <interval closure="open">
        <ci>x</ci>
        <ci>y</ci>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I[f]=\mathbb{E}[V(f(x),y)]=\int V(f(x),y)\,dp(x,y)\ .
  </annotation>
 </semantics>
</math>

 In reality, the learner never knows the true distribution 

<math display="inline" id="Online_machine_learning:12">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>p</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(x,y)
  </annotation>
 </semantics>
</math>

 over instances. Instead, the learner usually has access to a training set of examples 

<math display="inline" id="Online_machine_learning:13">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
   <mi mathvariant="normal">‚Ä¶</mi>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mi>n</mi>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <cn type="integer">1</cn>
     </apply>
    </interval>
    <ci>normal-‚Ä¶</ci>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>n</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <ci>n</ci>
     </apply>
    </interval>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x_{1},y_{1}),\ldots,(x_{n},y_{n})
  </annotation>
 </semantics>
</math>

 that are assumed to have been drawn <a class="uri" href="i.i.d." title="wikilink">i.i.d.</a> from the true distribution 

<math display="inline" id="Online_machine_learning:14">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>p</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(x,y)
  </annotation>
 </semantics>
</math>

. A common paradigm in this situation is to estimate a function 

<math display="inline" id="Online_machine_learning:15">
 <semantics>
  <mover accent="true">
   <mi>f</mi>
   <mo stretchy="false">^</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-^</ci>
    <ci>f</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{f}
  </annotation>
 </semantics>
</math>

 through <a href="empirical_risk_minimization" title="wikilink">empirical risk minimization</a> or regularized empirical risk minimization (usually <a href="Tikhonov_regularization" title="wikilink">Tikhonov regularization</a>). The choice of loss function here gives rise to several well-known learning algorithms such as regularized <a href="least_squares" title="wikilink">least squares</a> and <a href="support_vector_machines" title="wikilink">support vector machines</a>.</p>

<p>The above paradigm is not well-suited to the online learning setting though, as it requires complete a priori knowledge of the entire training set. In the pure online learning approach, the learning algorithm should update a sequence of functions 

<math display="inline" id="Online_machine_learning:16">
 <semantics>
  <mrow>
   <msub>
    <mi>f</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>f</mi>
    <mn>2</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">‚Ä¶</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <cn type="integer">2</cn>
    </apply>
    <ci>normal-‚Ä¶</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{1},f_{2},\ldots
  </annotation>
 </semantics>
</math>

 in a way such that the function 

<math display="inline" id="Online_machine_learning:17">
 <semantics>
  <msub>
   <mi>f</mi>
   <mrow>
    <mi>t</mi>
    <mo>+</mo>
    <mn>1</mn>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>f</ci>
    <apply>
     <plus></plus>
     <ci>t</ci>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{t+1}
  </annotation>
 </semantics>
</math>

 depends only on the previous function 

<math display="inline" id="Online_machine_learning:18">
 <semantics>
  <msub>
   <mi>f</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>f</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{t}
  </annotation>
 </semantics>
</math>

 and the next data point 

<math display="inline" id="Online_machine_learning:19">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <msub>
    <mi>x</mi>
    <mi>t</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>y</mi>
    <mi>t</mi>
   </msub>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="open">
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>t</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <ci>t</ci>
    </apply>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x_{t},y_{t})
  </annotation>
 </semantics>
</math>

. This approach has low memory requirements in the sense that it only requires storage of a representation of the current function 

<math display="inline" id="Online_machine_learning:20">
 <semantics>
  <msub>
   <mi>f</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>f</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{t}
  </annotation>
 </semantics>
</math>

 and the next data point 

<math display="inline" id="Online_machine_learning:21">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <msub>
    <mi>x</mi>
    <mi>t</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>y</mi>
    <mi>t</mi>
   </msub>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="open">
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>t</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <ci>t</ci>
    </apply>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x_{t},y_{t})
  </annotation>
 </semantics>
</math>

. A related approach that has larger memory requirements allows 

<math display="inline" id="Online_machine_learning:22">
 <semantics>
  <msub>
   <mi>f</mi>
   <mrow>
    <mi>t</mi>
    <mo>+</mo>
    <mn>1</mn>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>f</ci>
    <apply>
     <plus></plus>
     <ci>t</ci>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{t+1}
  </annotation>
 </semantics>
</math>

 to depend on 

<math display="inline" id="Online_machine_learning:23">
 <semantics>
  <msub>
   <mi>f</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>f</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{t}
  </annotation>
 </semantics>
</math>

 and all previous data points 

<math display="inline" id="Online_machine_learning:24">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
   <mi mathvariant="normal">‚Ä¶</mi>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mi>t</mi>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mi>t</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <cn type="integer">1</cn>
     </apply>
    </interval>
    <ci>normal-‚Ä¶</ci>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>t</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <ci>t</ci>
     </apply>
    </interval>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x_{1},y_{1}),\ldots,(x_{t},y_{t})
  </annotation>
 </semantics>
</math>

. We focus solely on the former approach here, and we consider both the case where the data is coming from an infinite stream 

<math display="inline" id="Online_machine_learning:25">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mn>2</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mn>2</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
   <mi mathvariant="normal">‚Ä¶</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <cn type="integer">1</cn>
     </apply>
    </interval>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">2</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <cn type="integer">2</cn>
     </apply>
    </interval>
    <ci>normal-‚Ä¶</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x_{1},y_{1}),(x_{2},y_{2}),\ldots
  </annotation>
 </semantics>
</math>

 and the case where the data is coming from a finite training set 

<math display="inline" id="Online_machine_learning:26">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
   <mi mathvariant="normal">‚Ä¶</mi>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mi>n</mi>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <cn type="integer">1</cn>
     </apply>
    </interval>
    <ci>normal-‚Ä¶</ci>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>n</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <ci>n</ci>
     </apply>
    </interval>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x_{1},y_{1}),\ldots,(x_{n},y_{n})
  </annotation>
 </semantics>
</math>

, in which case the online learning algorithm may make multiple passes through the data.</p>
<h3 id="the-algorithm-and-its-interpretations">The algorithm and its interpretations</h3>

<p>Here we outline a prototypical online learning algorithm in the supervised learning setting and we discuss several interpretations of this algorithm. For simplicity, consider the case where 

<math display="inline" id="Online_machine_learning:27">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>=</mo>
   <msup>
    <mi>‚Ñù</mi>
    <mi>d</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>X</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>‚Ñù</ci>
     <ci>d</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X=\mathbb{R}^{d}
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Online_machine_learning:28">
 <semantics>
  <mrow>
   <mi>Y</mi>
   <mo>‚äÜ</mo>
   <mi>‚Ñù</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <subset></subset>
    <ci>Y</ci>
    <ci>‚Ñù</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y\subseteq\mathbb{R}
  </annotation>
 </semantics>
</math>

, and 

<math display="inline" id="Online_machine_learning:29">
 <semantics>
  <mrow>
   <mi class="ltx_font_mathcaligraphic">‚Ñã</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mrow>
     <mo stretchy="false">‚ü®</mo>
     <mi>w</mi>
     <mo>,</mo>
     <mo>‚ãÖ</mo>
     <mo stretchy="false">‚ü©</mo>
    </mrow>
    <mo>:</mo>
    <mrow>
     <mi>w</mi>
     <mo>‚àà</mo>
     <msup>
      <mi>‚Ñù</mi>
      <mi>d</mi>
     </msup>
    </mrow>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>‚Ñã</ci>
    <apply>
     <csymbol cd="latexml">conditional-set</csymbol>
     <list>
      <ci>w</ci>
      <ci>normal-‚ãÖ</ci>
     </list>
     <apply>
      <in></in>
      <ci>w</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>‚Ñù</ci>
       <ci>d</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{H}=\{\langle w,\cdot\rangle:w\in\mathbb{R}^{d}\}
  </annotation>
 </semantics>
</math>

 is the set of all linear functionals from 

<math display="inline" id="Online_machine_learning:30">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 into 

<math display="inline" id="Online_machine_learning:31">
 <semantics>
  <mi>‚Ñù</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>‚Ñù</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbb{R}
  </annotation>
 </semantics>
</math>

, i.e. we are working with a linear kernel and functions 

<math display="inline" id="Online_machine_learning:32">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>‚àà</mo>
   <mi class="ltx_font_mathcaligraphic">‚Ñã</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>f</ci>
    <ci>‚Ñã</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f\in\mathcal{H}
  </annotation>
 </semantics>
</math>

 can be identified with vectors 

<math display="inline" id="Online_machine_learning:33">
 <semantics>
  <mrow>
   <mi>w</mi>
   <mo>‚àà</mo>
   <msup>
    <mi>‚Ñù</mi>
    <mi>d</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>w</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>‚Ñù</ci>
     <ci>d</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w\in\mathbb{R}^{d}
  </annotation>
 </semantics>
</math>

. Furthermore, assume that 

<math display="inline" id="Online_machine_learning:34">
 <semantics>
  <mrow>
   <mi>V</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mo>‚ãÖ</mo>
    <mo>,</mo>
    <mo>‚ãÖ</mo>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>V</ci>
    <interval closure="open">
     <ci>normal-‚ãÖ</ci>
     <ci>normal-‚ãÖ</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(\cdot,\cdot)
  </annotation>
 </semantics>
</math>

 is a convex, differentiable loss function. An online learning algorithm satisfying the low memory property discussed above consists of the following iteration:</p>

<p>

<math display="block" id="Online_machine_learning:35">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>t</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo>‚Üê</mo>
    <mrow>
     <msub>
      <mi>w</mi>
      <mi>t</mi>
     </msub>
     <mo>-</mo>
     <mrow>
      <msub>
       <mi>Œ≥</mi>
       <mi>t</mi>
      </msub>
      <mrow>
       <mo>‚àá</mo>
       <mi>V</mi>
      </mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mo stretchy="false">‚ü®</mo>
        <msub>
         <mi>w</mi>
         <mi>t</mi>
        </msub>
        <mo>,</mo>
        <msub>
         <mi>x</mi>
         <mi>t</mi>
        </msub>
        <mo stretchy="false">‚ü©</mo>
       </mrow>
       <mo>,</mo>
       <msub>
        <mi>y</mi>
        <mi>t</mi>
       </msub>
       <mo rspace="7.5pt" stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-‚Üê</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <apply>
      <plus></plus>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>t</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Œ≥</ci>
       <ci>t</ci>
      </apply>
      <apply>
       <ci>normal-‚àá</ci>
       <ci>V</ci>
      </apply>
      <interval closure="open">
       <list>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>w</ci>
         <ci>t</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>t</ci>
        </apply>
       </list>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <ci>t</ci>
       </apply>
      </interval>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{t+1}\leftarrow w_{t}-\gamma_{t}\nabla V(\langle w_{t},x_{t}\rangle,y_{t})\ ,
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Online_machine_learning:36">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mn>1</mn>
   </msub>
   <mo>‚Üê</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-‚Üê</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <cn type="integer">1</cn>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{1}\leftarrow 0
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Online_machine_learning:37">
 <semantics>
  <mrow>
   <mrow>
    <mo>‚àá</mo>
    <mi>V</mi>
   </mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mo stretchy="false">‚ü®</mo>
     <msub>
      <mi>w</mi>
      <mi>t</mi>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>x</mi>
      <mi>t</mi>
     </msub>
     <mo stretchy="false">‚ü©</mo>
    </mrow>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mi>t</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <ci>normal-‚àá</ci>
     <ci>V</ci>
    </apply>
    <interval closure="open">
     <list>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <ci>t</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>t</ci>
      </apply>
     </list>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <ci>t</ci>
     </apply>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \nabla V(\langle w_{t},x_{t}\rangle,y_{t})
  </annotation>
 </semantics>
</math>

 is the gradient of the loss for the next data point 

<math display="inline" id="Online_machine_learning:38">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <msub>
    <mi>x</mi>
    <mi>t</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>y</mi>
    <mi>t</mi>
   </msub>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="open">
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>t</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <ci>t</ci>
    </apply>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x_{t},y_{t})
  </annotation>
 </semantics>
</math>

 evaluated at the current linear functional 

<math display="inline" id="Online_machine_learning:39">
 <semantics>
  <msub>
   <mi>w</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{t}
  </annotation>
 </semantics>
</math>

, and 

<math display="inline" id="Online_machine_learning:40">
 <semantics>
  <mrow>
   <msub>
    <mi>Œ≥</mi>
    <mi>t</mi>
   </msub>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>Œ≥</ci>
     <ci>t</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma_{t}>0
  </annotation>
 </semantics>
</math>

 is a step-size parameter. In the case of an infinite stream of data, one can run this iteration, in principle, forever, and in the case of a finite but large set of data, one can consider a single pass or multiple passes (epochs) through the data.</p>

<p>Interestingly enough, the above simple iterative online learning algorithm has three distinct interpretations, each of which has distinct implications about the predictive quality of the sequence of functions 

<math display="inline" id="Online_machine_learning:41">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>w</mi>
    <mn>2</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">‚Ä¶</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <cn type="integer">2</cn>
    </apply>
    <ci>normal-‚Ä¶</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{1},w_{2},\ldots
  </annotation>
 </semantics>
</math>

. The first interpretation considers the above iteration as an instance of the <a href="stochastic_gradient_descent" title="wikilink">stochastic gradient descent</a> method applied to the problem of minimizing the expected risk 

<math display="inline" id="Online_machine_learning:42">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>w</mi>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>I</ci>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <ci>w</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I[w]
  </annotation>
 </semantics>
</math>

 defined above.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Indeed, in the case of an infinite stream of data, since the examples 

<math display="inline" id="Online_machine_learning:43">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>x</mi>
     <mn>2</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>y</mi>
     <mn>2</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
   <mi mathvariant="normal">‚Ä¶</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <cn type="integer">1</cn>
     </apply>
    </interval>
    <interval closure="open">
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">2</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>y</ci>
      <cn type="integer">2</cn>
     </apply>
    </interval>
    <ci>normal-‚Ä¶</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x_{1},y_{1}),(x_{2},y_{2}),\ldots
  </annotation>
 </semantics>
</math>

 are assumed to be drawn i.i.d. from the distribution 

<math display="inline" id="Online_machine_learning:44">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>p</ci>
    <interval closure="open">
     <ci>x</ci>
     <ci>y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(x,y)
  </annotation>
 </semantics>
</math>

, the sequence of gradients of 

<math display="inline" id="Online_machine_learning:45">
 <semantics>
  <mrow>
   <mi>V</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mo>‚ãÖ</mo>
    <mo>,</mo>
    <mo>‚ãÖ</mo>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>V</ci>
    <interval closure="open">
     <ci>normal-‚ãÖ</ci>
     <ci>normal-‚ãÖ</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(\cdot,\cdot)
  </annotation>
 </semantics>
</math>

 in the above iteration are an i.i.d. sample of stochastic estimates of the gradient of the expected risk 

<math display="inline" id="Online_machine_learning:46">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>w</mi>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>I</ci>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <ci>w</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I[w]
  </annotation>
 </semantics>
</math>

 and therefore one can apply complexity results for the stochastic gradient descent method to bound the deviation 

<math display="inline" id="Online_machine_learning:47">
 <semantics>
  <mrow>
   <mrow>
    <mi>I</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <msub>
      <mi>w</mi>
      <mi>t</mi>
     </msub>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>-</mo>
   <mrow>
    <mi>I</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <msup>
      <mi>w</mi>
      <mo>‚àó</mo>
     </msup>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <apply>
     <times></times>
     <ci>I</ci>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <ci>t</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>I</ci>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>w</ci>
       <ci>normal-‚àó</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I[w_{t}]-I[w^{\ast}]
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Online_machine_learning:48">
 <semantics>
  <msup>
   <mi>w</mi>
   <mo>‚àó</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>w</ci>
    <ci>normal-‚àó</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w^{\ast}
  </annotation>
 </semantics>
</math>

 is the minimizer of 

<math display="inline" id="Online_machine_learning:49">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>w</mi>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>I</ci>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <ci>w</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I[w]
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> This interpretation is also valid in the case of a finite training set; although with multiple passes through the data the gradients are no longer independent, still complexity results can be obtained in special cases.</p>

<p>The second interpretation applies to the case of a finite training set and considers the above recursion as an instance of the incremental gradient descent method<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> to minimize the empirical risk:</p>

<p>

<math display="block" id="Online_machine_learning:50">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mi>I</mi>
      <mi>n</mi>
     </msub>
     <mrow>
      <mo stretchy="false">[</mo>
      <mi>w</mi>
      <mo stretchy="false">]</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mfrac>
      <mn>1</mn>
      <mi>n</mi>
     </mfrac>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>n</mi>
      </munderover>
      <mrow>
       <mi>V</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mo stretchy="false">‚ü®</mo>
         <mi>w</mi>
         <mo>,</mo>
         <msub>
          <mi>x</mi>
          <mi>i</mi>
         </msub>
         <mo stretchy="false">‚ü©</mo>
        </mrow>
        <mo>,</mo>
        <msub>
         <mi>y</mi>
         <mi>i</mi>
        </msub>
        <mo rspace="7.5pt" stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>I</ci>
      <ci>n</ci>
     </apply>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <ci>w</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>n</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <times></times>
       <ci>V</ci>
       <interval closure="open">
        <list>
         <ci>w</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
        </list>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>i</ci>
        </apply>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I_{n}[w]=\frac{1}{n}\sum_{i=1}^{n}V(\langle w,x_{i}\rangle,y_{i})\ .
  </annotation>
 </semantics>
</math>

 Since the gradients of 

<math display="inline" id="Online_machine_learning:51">
 <semantics>
  <mrow>
   <mi>V</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mo>‚ãÖ</mo>
    <mo>,</mo>
    <mo>‚ãÖ</mo>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>V</ci>
    <interval closure="open">
     <ci>normal-‚ãÖ</ci>
     <ci>normal-‚ãÖ</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(\cdot,\cdot)
  </annotation>
 </semantics>
</math>

 in the above iteration are also stochastic estimates of the gradient of 

<math display="inline" id="Online_machine_learning:52">
 <semantics>
  <mrow>
   <msub>
    <mi>I</mi>
    <mi>n</mi>
   </msub>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>w</mi>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>I</ci>
     <ci>n</ci>
    </apply>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <ci>w</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I_{n}[w]
  </annotation>
 </semantics>
</math>

, this interpretation is also related to the stochastic gradient descent method, but applied to minimize the empirical risk as opposed to the expected risk. Since this interpretation concerns the empirical risk and not the expected risk, multiple passes through the data are readily allowed and actually lead to tighter bounds on the deviations 

<math display="inline" id="Online_machine_learning:53">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>I</mi>
     <mi>n</mi>
    </msub>
    <mrow>
     <mo stretchy="false">[</mo>
     <msub>
      <mi>w</mi>
      <mi>t</mi>
     </msub>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>-</mo>
   <mrow>
    <msub>
     <mi>I</mi>
     <mi>n</mi>
    </msub>
    <mrow>
     <mo stretchy="false">[</mo>
     <msubsup>
      <mi>w</mi>
      <mi>n</mi>
      <mo>‚àó</mo>
     </msubsup>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>I</ci>
      <ci>n</ci>
     </apply>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <ci>t</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>I</ci>
      <ci>n</ci>
     </apply>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>w</ci>
        <ci>normal-‚àó</ci>
       </apply>
       <ci>n</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I_{n}[w_{t}]-I_{n}[w^{\ast}_{n}]
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Online_machine_learning:54">
 <semantics>
  <msubsup>
   <mi>w</mi>
   <mi>n</mi>
   <mo>‚àó</mo>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>w</ci>
     <ci>normal-‚àó</ci>
    </apply>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w^{\ast}_{n}
  </annotation>
 </semantics>
</math>

 is the minimizer of 

<math display="inline" id="Online_machine_learning:55">
 <semantics>
  <mrow>
   <msub>
    <mi>I</mi>
    <mi>n</mi>
   </msub>
   <mrow>
    <mo stretchy="false">[</mo>
    <mi>w</mi>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>I</ci>
     <ci>n</ci>
    </apply>
    <apply>
     <csymbol cd="latexml">delimited-[]</csymbol>
     <ci>w</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I_{n}[w]
  </annotation>
 </semantics>
</math>

.</p>

<p>The third interpretation of the above recursion is distinctly different from the first two and concerns the case of sequential trials discussed above, where the data are potentially not i.i.d. and can perhaps be selected in an adversarial manner. At each step of this process, the learner is given an input 

<math display="inline" id="Online_machine_learning:56">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{t}
  </annotation>
 </semantics>
</math>

 and makes a prediction based on the current linear function 

<math display="inline" id="Online_machine_learning:57">
 <semantics>
  <msub>
   <mi>w</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{t}
  </annotation>
 </semantics>
</math>

. Only after making this prediction does the learner see the true label 

<math display="inline" id="Online_machine_learning:58">
 <semantics>
  <msub>
   <mi>y</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>y</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{t}
  </annotation>
 </semantics>
</math>

, at which point the learner is allowed to update 

<math display="inline" id="Online_machine_learning:59">
 <semantics>
  <msub>
   <mi>w</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{t}
  </annotation>
 </semantics>
</math>

 to 

<math display="inline" id="Online_machine_learning:60">
 <semantics>
  <msub>
   <mi>w</mi>
   <mrow>
    <mi>t</mi>
    <mo>+</mo>
    <mn>1</mn>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <apply>
     <plus></plus>
     <ci>t</ci>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{t+1}
  </annotation>
 </semantics>
</math>

. Since we are not making any distributional assumptions about the data, the goal here is to perform as well as if we could view the entire sequence of examples ahead of time; that is, we would like the sequence of functions 

<math display="inline" id="Online_machine_learning:61">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>w</mi>
    <mn>2</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">‚Ä¶</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <cn type="integer">2</cn>
    </apply>
    <ci>normal-‚Ä¶</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{1},w_{2},\ldots
  </annotation>
 </semantics>
</math>

 to have low regret relative to any vector 

<math display="inline" id="Online_machine_learning:62">
 <semantics>
  <msup>
   <mi>w</mi>
   <mo>‚àó</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>w</ci>
    <ci>normal-‚àó</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w^{\ast}
  </annotation>
 </semantics>
</math>

:</p>

<p>

<math display="block" id="Online_machine_learning:63">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mi>R</mi>
      <mi>T</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <msup>
       <mi>w</mi>
       <mo>‚àó</mo>
      </msup>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
       <mrow>
        <mi>t</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>T</mi>
      </munderover>
      <mrow>
       <mi>V</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mo stretchy="false">‚ü®</mo>
         <msub>
          <mi>w</mi>
          <mi>t</mi>
         </msub>
         <mo>,</mo>
         <msub>
          <mi>x</mi>
          <mi>t</mi>
         </msub>
         <mo stretchy="false">‚ü©</mo>
        </mrow>
        <mo>,</mo>
        <msub>
         <mi>y</mi>
         <mi>t</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo>-</mo>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
       <mrow>
        <mi>t</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>T</mi>
      </munderover>
      <mrow>
       <mi>V</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mo stretchy="false">‚ü®</mo>
         <msup>
          <mi>w</mi>
          <mo>‚àó</mo>
         </msup>
         <mo>,</mo>
         <msub>
          <mi>x</mi>
          <mi>t</mi>
         </msub>
         <mo stretchy="false">‚ü©</mo>
        </mrow>
        <mo>,</mo>
        <msub>
         <mi>y</mi>
         <mi>t</mi>
        </msub>
        <mo rspace="7.5pt" stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>R</ci>
      <ci>T</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>w</ci>
      <ci>normal-‚àó</ci>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>t</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>T</ci>
      </apply>
      <apply>
       <times></times>
       <ci>V</ci>
       <interval closure="open">
        <list>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>w</ci>
          <ci>t</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>t</ci>
         </apply>
        </list>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>t</ci>
        </apply>
       </interval>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>t</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>T</ci>
      </apply>
      <apply>
       <times></times>
       <ci>V</ci>
       <interval closure="open">
        <list>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <ci>w</ci>
          <ci>normal-‚àó</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>t</ci>
         </apply>
        </list>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>t</ci>
        </apply>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R_{T}(w^{\ast})=\sum_{t=1}^{T}V(\langle w_{t},x_{t}\rangle,y_{t})-\sum_{t=1}^{%
T}V(\langle w^{\ast},x_{t}\rangle,y_{t})\ .
  </annotation>
 </semantics>
</math>

 In this setting, the above recursion can be considered as an instance of the online gradient descent method for which there are complexity bounds that guarantee 

<math display="inline" id="Online_machine_learning:64">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msqrt>
     <mi>T</mi>
    </msqrt>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <root></root>
     <ci>T</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(\sqrt{T})
  </annotation>
 </semantics>
</math>

 regret.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>It should be noted that although the three interpretations of this algorithm yield complexity bounds in three distinct settings, each bound depends on the choice of step-size sequence 

<math display="inline" id="Online_machine_learning:65">
 <semantics>
  <mrow>
   <mo stretchy="false">{</mo>
   <msub>
    <mi>Œ≥</mi>
    <mi>t</mi>
   </msub>
   <mo stretchy="false">}</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <set>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>Œ≥</ci>
     <ci>t</ci>
    </apply>
   </set>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{\gamma_{t}\}
  </annotation>
 </semantics>
</math>

 in a different way, and thus we cannot simultaneously apply the consequences of all three interpretations; we must instead select the step-size sequence in a way that is tailored for the interpretation that is most relevant. Furthermore, the above algorithm and these interpretations can be extended to the case of a nonlinear kernel by simply considering 

<math display="inline" id="Online_machine_learning:66">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 to be the feature space associated with the kernel. Although in this case the memory requirements at each iteration are no longer 

<math display="inline" id="Online_machine_learning:67">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>d</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <ci>d</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(d)
  </annotation>
 </semantics>
</math>

, but are rather on the order of the number of data points considered so far.</p>
<h2 id="example-complexity-in-the-case-of-linear-least-squares">Example: Complexity in the Case of Linear Least Squares</h2>
<h3 id="batch-learning">Batch Learning</h3>

<p>Let us consider the setting of supervised learning with the square loss function 

<math display="inline" id="Online_machine_learning:68">
 <semantics>
  <mrow>
   <mrow>
    <mi>V</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mo stretchy="false">‚ü®</mo>
      <mi>w</mi>
      <mo>,</mo>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">‚ü©</mo>
     </mrow>
     <mo>,</mo>
     <msub>
      <mi>y</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mrow>
       <msubsup>
        <mi>x</mi>
        <mi>i</mi>
        <mi>T</mi>
       </msubsup>
       <mi>w</mi>
      </mrow>
      <mo>-</mo>
      <msub>
       <mi>y</mi>
       <mi>i</mi>
      </msub>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>V</ci>
     <interval closure="open">
      <list>
       <ci>w</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
      </list>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>i</ci>
      </apply>
     </interval>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
        <ci>T</ci>
       </apply>
       <ci>w</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>y</ci>
       <ci>i</ci>
      </apply>
     </apply>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V(\langle w,x_{i}\rangle,y_{i})=(x_{i}^{T}w-y_{i})^{2}
  </annotation>
 </semantics>
</math>

, (

<math display="inline" id="Online_machine_learning:69">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mi>i</mi>
   </msub>
   <mo>‚àà</mo>
   <msup>
    <mi>‚Ñù</mi>
    <mi>d</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>‚Ñù</ci>
     <ci>d</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i}\in\mathbb{R}^{d}
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Online_machine_learning:70">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mi>i</mi>
   </msub>
   <mo>‚àà</mo>
   <msup>
    <mi>‚Ñù</mi>
    <mi>d</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>‚Ñù</ci>
     <ci>d</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}\in\mathbb{R}^{d}
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Online_machine_learning:71">
 <semantics>
  <mrow>
   <msub>
    <mi>y</mi>
    <mi>i</mi>
   </msub>
   <mo>‚àà</mo>
   <mi>‚Ñù</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <ci>i</ci>
    </apply>
    <ci>‚Ñù</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{i}\in\mathbb{R}
  </annotation>
 </semantics>
</math>

). The solution after the arrival of every datapoint 

<math display="inline" id="Online_machine_learning:72">
 <semantics>
  <mrow>
   <mo stretchy="false">{</mo>
   <msub>
    <mi>x</mi>
    <mi>i</mi>
   </msub>
   <mo>,</mo>
   <msub>
    <mi>y</mi>
    <mi>i</mi>
   </msub>
   <mo stretchy="false">}</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <set>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <ci>i</ci>
    </apply>
   </set>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{x_{i},y_{i}\}
  </annotation>
 </semantics>
</math>

 is given by 

<math display="inline" id="Online_machine_learning:73">
 <semantics>
  <mrow>
   <msup>
    <mi>w</mi>
    <mo>*</mo>
   </msup>
   <mo>=</mo>
   <mrow>
    <msup>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <msup>
        <mi>X</mi>
        <mi>T</mi>
       </msup>
       <mi>X</mi>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mrow>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msup>
    <msup>
     <mi>X</mi>
     <mi>T</mi>
    </msup>
    <mi>Y</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>w</ci>
     <times></times>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>X</ci>
        <ci>T</ci>
       </apply>
       <ci>X</ci>
      </apply>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>X</ci>
      <ci>T</ci>
     </apply>
     <ci>Y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w^{*}=(X^{T}X)^{-1}X^{T}Y
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Online_machine_learning:74">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Online_machine_learning:75">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

 is built from the 

<math display="inline" id="Online_machine_learning:76">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

 data points, with 

<math display="inline" id="Online_machine_learning:77">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 being 

<math display="inline" id="Online_machine_learning:78">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

-by-

<math display="inline" id="Online_machine_learning:79">
 <semantics>
  <mi>d</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Online_machine_learning:80">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

 being 

<math display="inline" id="Online_machine_learning:81">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

-by-

<math display="inline" id="Online_machine_learning:82">
 <semantics>
  <mn>1</mn>
  <annotation-xml encoding="MathML-Content">
   <cn type="integer">1</cn>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1
  </annotation>
 </semantics>
</math>

. The solution of linear least squares problem is roughly 

<math display="inline" id="Online_machine_learning:83">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>i</mi>
     <msup>
      <mi>d</mi>
      <mn>2</mn>
     </msup>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <times></times>
     <ci>i</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>d</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(id^{2})
  </annotation>
 </semantics>
</math>

.</p>

<p>If we have 

<math display="inline" id="Online_machine_learning:84">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 total points in the dataset and we have to recompute the solution after the arrival of every datapoint 

<math display="inline" id="Online_machine_learning:85">
 <semantics>
  <mrow>
   <mi>i</mi>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>,</mo>
    <mi mathvariant="normal">‚Ä¶</mi>
    <mo>,</mo>
    <mi>n</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>i</ci>
    <list>
     <cn type="integer">1</cn>
     <ci>normal-‚Ä¶</ci>
     <ci>n</ci>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i=1,\ldots,n
  </annotation>
 </semantics>
</math>

, we have a total complexity 

<math display="inline" id="Online_machine_learning:86">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <msup>
      <mi>n</mi>
      <mn>2</mn>
     </msup>
     <msup>
      <mi>d</mi>
      <mn>2</mn>
     </msup>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>n</ci>
      <cn type="integer">2</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>d</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(n^{2}d^{2})
  </annotation>
 </semantics>
</math>

. Here we assume that the matrix 

<math display="inline" id="Online_machine_learning:87">
 <semantics>
  <mrow>
   <msup>
    <mi>X</mi>
    <mi>T</mi>
   </msup>
   <mi>X</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>X</ci>
     <ci>T</ci>
    </apply>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X^{T}X
  </annotation>
 </semantics>
</math>

 is invertible, otherwise we can proceed in a similar fashion with Tikhonov regularization.</p>
<h3 id="online-learning">Online Learning</h3>

<p>The recursive least squares algorithm considers an online approach to the least squares problem. It can be shown that for suitable initializations of 

<math display="inline" id="Online_machine_learning:88">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mn>0</mn>
   </msub>
   <mo>‚àà</mo>
   <msup>
    <mi>‚Ñù</mi>
    <mi>d</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <cn type="integer">0</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>‚Ñù</ci>
     <ci>d</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{0}\in\mathbb{R}^{d}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Online_machine_learning:89">
 <semantics>
  <mrow>
   <msub>
    <mi mathvariant="normal">Œì</mi>
    <mn>0</mn>
   </msub>
   <mo>‚àà</mo>
   <msup>
    <mi>‚Ñù</mi>
    <mrow>
     <mi>d</mi>
     <mi>x</mi>
     <mi>d</mi>
    </mrow>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>normal-Œì</ci>
     <cn type="integer">0</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>‚Ñù</ci>
     <apply>
      <times></times>
      <ci>d</ci>
      <ci>x</ci>
      <ci>d</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Gamma_{0}\in\mathbb{R}^{dxd}
  </annotation>
 </semantics>
</math>

, the solution of the linear least squares problem given in the previous section can be computed by the following iteration:</p>

<p>

<math display="block" id="Online_machine_learning:90">
 <semantics>
  <mrow>
   <msub>
    <mi mathvariant="normal">Œì</mi>
    <mi>i</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi mathvariant="normal">Œì</mi>
     <mrow>
      <mi>i</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo>-</mo>
    <mfrac>
     <mrow>
      <msub>
       <mi mathvariant="normal">Œì</mi>
       <mrow>
        <mi>i</mi>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
      </msub>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
      <msubsup>
       <mi>x</mi>
       <mi>i</mi>
       <mi>T</mi>
      </msubsup>
      <msub>
       <mi mathvariant="normal">Œì</mi>
       <mrow>
        <mi>i</mi>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
      </msub>
     </mrow>
     <mrow>
      <mn>1</mn>
      <mo>+</mo>
      <mrow>
       <msubsup>
        <mi>x</mi>
        <mi>i</mi>
        <mi>T</mi>
       </msubsup>
       <msub>
        <mi mathvariant="normal">Œì</mi>
        <mrow>
         <mi>i</mi>
         <mo>-</mo>
         <mn>1</mn>
        </mrow>
       </msub>
       <msub>
        <mi>x</mi>
        <mi>i</mi>
       </msub>
      </mrow>
     </mrow>
    </mfrac>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>normal-Œì</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>normal-Œì</ci>
      <apply>
       <minus></minus>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <divide></divide>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>normal-Œì</ci>
        <apply>
         <minus></minus>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
        <ci>T</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>normal-Œì</ci>
        <apply>
         <minus></minus>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
      </apply>
      <apply>
       <plus></plus>
       <cn type="integer">1</cn>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
         <ci>T</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>normal-Œì</ci>
         <apply>
          <minus></minus>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Gamma_{i}=\Gamma_{i-1}-\frac{\Gamma_{i-1}x_{i}x_{i}^{T}\Gamma_{i-1}}{1+x_{i}^%
{T}\Gamma_{i-1}x_{i}}
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="block" id="Online_machine_learning:91">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mi>i</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>i</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo>-</mo>
    <mrow>
     <msub>
      <mi mathvariant="normal">Œì</mi>
      <mi>i</mi>
     </msub>
     <msub>
      <mi>x</mi>
      <mi>i</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mrow>
        <msubsup>
         <mi>x</mi>
         <mi>i</mi>
         <mi>T</mi>
        </msubsup>
        <msub>
         <mi>w</mi>
         <mrow>
          <mi>i</mi>
          <mo>-</mo>
          <mn>1</mn>
         </mrow>
        </msub>
       </mrow>
       <mo>-</mo>
       <msub>
        <mi>y</mi>
        <mi>i</mi>
       </msub>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <minus></minus>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-Œì</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <minus></minus>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
         <ci>T</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>w</ci>
         <apply>
          <minus></minus>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}=w_{i-1}-\Gamma_{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})
  </annotation>
 </semantics>
</math>

</p>

<p>For the proof, see <a href="Recursive_least_squares" title="wikilink">RLS</a>.</p>

<p>The complexity for 

<math display="inline" id="Online_machine_learning:92">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 steps of this algorithm is 

<math display="inline" id="Online_machine_learning:93">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>n</mi>
     <msup>
      <mi>d</mi>
      <mn>2</mn>
     </msup>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <times></times>
     <ci>n</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>d</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(nd^{2})
  </annotation>
 </semantics>
</math>

, which is an order of magnitude faster than the corresponding batch learning complexity. The storage requirements at every step 

<math display="inline" id="Online_machine_learning:94">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

 here are constant at 

<math display="inline" id="Online_machine_learning:95">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msup>
     <mi>d</mi>
     <mn>2</mn>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>d</ci>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(d^{2})
  </annotation>
 </semantics>
</math>

, i.e. that of storing the matrix 

<math display="inline" id="Online_machine_learning:96">
 <semantics>
  <msub>
   <mi mathvariant="normal">Œì</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>normal-Œì</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Gamma_{i}
  </annotation>
 </semantics>
</math>

.</p>
<h4 id="stochastic-gradient-descent">Stochastic Gradient Descent</h4>

<p>If we now replace 

<math display="inline" id="Online_machine_learning:97">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mi>i</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>i</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo>-</mo>
    <mrow>
     <msub>
      <mi mathvariant="normal">Œì</mi>
      <mi>i</mi>
     </msub>
     <msub>
      <mi>x</mi>
      <mi>n</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mrow>
        <msubsup>
         <mi>x</mi>
         <mi>i</mi>
         <mi>T</mi>
        </msubsup>
        <msub>
         <mi>w</mi>
         <mrow>
          <mi>i</mi>
          <mo>-</mo>
          <mn>1</mn>
         </mrow>
        </msub>
       </mrow>
       <mo>-</mo>
       <msub>
        <mi>y</mi>
        <mi>i</mi>
       </msub>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <minus></minus>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-Œì</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>n</ci>
      </apply>
      <apply>
       <minus></minus>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
         <ci>T</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>w</ci>
         <apply>
          <minus></minus>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}=w_{i-1}-\Gamma_{i}x_{n}(x_{i}^{T}w_{i-1}-y_{i})
  </annotation>
 </semantics>
</math>

 by 

<math display="inline" id="Online_machine_learning:98">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mi>i</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>i</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo>-</mo>
    <mrow>
     <msub>
      <mi>Œ≥</mi>
      <mi>i</mi>
     </msub>
     <msub>
      <mi>x</mi>
      <mi>i</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mrow>
        <msubsup>
         <mi>x</mi>
         <mi>i</mi>
         <mi>T</mi>
        </msubsup>
        <msub>
         <mi>w</mi>
         <mrow>
          <mi>i</mi>
          <mo>-</mo>
          <mn>1</mn>
         </mrow>
        </msub>
       </mrow>
       <mo>-</mo>
       <msub>
        <mi>y</mi>
        <mi>i</mi>
       </msub>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <minus></minus>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Œ≥</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <minus></minus>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
         <ci>T</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>w</ci>
         <apply>
          <minus></minus>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>y</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}=w_{i-1}-\gamma_{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})
  </annotation>
 </semantics>
</math>

 (i.e. replacing 

<math display="inline" id="Online_machine_learning:99">
 <semantics>
  <mrow>
   <msub>
    <mi mathvariant="normal">Œì</mi>
    <mi>i</mi>
   </msub>
   <mo>‚àà</mo>
   <msup>
    <mi>‚Ñù</mi>
    <mrow>
     <mi>d</mi>
     <mo>√ó</mo>
     <mi>d</mi>
    </mrow>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>normal-Œì</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>‚Ñù</ci>
     <apply>
      <times></times>
      <ci>d</ci>
      <ci>d</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Gamma_{i}\in\mathbb{R}^{d\times d}
  </annotation>
 </semantics>
</math>

 by 

<math display="inline" id="Online_machine_learning:100">
 <semantics>
  <mrow>
   <msub>
    <mi>Œ≥</mi>
    <mi>i</mi>
   </msub>
   <mo>‚àà</mo>
   <mi>‚Ñù</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>Œ≥</ci>
     <ci>i</ci>
    </apply>
    <ci>‚Ñù</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma_{i}\in\mathbb{R}
  </annotation>
 </semantics>
</math>

), we have a stochastic gradient descent algorithm. In this case, the complexity for 

<math display="inline" id="Online_machine_learning:101">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 steps of this algorithm reduces to 

<math display="inline" id="Online_machine_learning:102">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>n</mi>
     <mi>d</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <times></times>
     <ci>n</ci>
     <ci>d</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(nd)
  </annotation>
 </semantics>
</math>

. The storage requirements at every step 

<math display="inline" id="Online_machine_learning:103">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

 are constant at 

<math display="inline" id="Online_machine_learning:104">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>d</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <ci>d</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(d)
  </annotation>
 </semantics>
</math>

.</p>

<p>However, the stepsize 

<math display="inline" id="Online_machine_learning:105">
 <semantics>
  <msub>
   <mi>Œ≥</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>Œ≥</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma_{i}
  </annotation>
 </semantics>
</math>

 needs to be chosen carefully to solve the expected risk minimization problem, as detailed above.</p>
<h2 id="books-with-substantial-treatment-of-online-machine-learning">Books with substantial treatment of online machine learning</h2>
<ul>
<li><em>Algorithmic Learning in a Random World</em> by Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Published by Springer Science+Business Media, Inc. 2005 ISBN 0-387-00152-2</li>
</ul>
<ul>
<li><em>Prediction, learning, and games</em> by <a href="Nicol√≤_Cesa-Bianchi" title="wikilink">Nicol√≤ Cesa-Bianchi</a> and G√°bor Lugosi. Cambridge University Press, 2006 ISBN 0-521-84108-9</li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Hierarchical_temporal_memory" title="wikilink">Hierarchical temporal memory</a></li>
<li><a href="k-nearest_neighbor_algorithm" title="wikilink">k-nearest neighbor algorithm</a></li>
<li><a href="Lazy_learning" title="wikilink">Lazy learning</a></li>
<li><a href="Learning_Vector_Quantization" title="wikilink">Learning Vector Quantization</a></li>
<li><a href="Offline_learning" title="wikilink">Offline learning</a>, the opposite model</li>
<li><a href="Online_algorithm" title="wikilink">Online algorithm</a></li>
<li><a href="Streaming_Algorithm" title="wikilink">Streaming Algorithm</a></li>
<li><a class="uri" href="Perceptron" title="wikilink">Perceptron</a></li>
<li><a href="Stochastic_gradient_descent" title="wikilink">Stochastic gradient descent</a></li>
<li><a href="Supervised_learning" title="wikilink">Supervised learning</a></li>
</ul>
<h2 id="references">References</h2>
<references>
</references>
<h2 id="external-links">External links</h2>
<ul>
<li><a class="uri" href="http://onlineprediction.net/">http://onlineprediction.net/</a>, Wiki for On-Line Prediction.</li>
</ul>

<p>"</p>

<p><a href="Category:Machine_learning_algorithms" title="wikilink">Category:Machine learning algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">‚Ü©</a></li>
<li id="fn2"><em>Stochastic Approximation Algorithms and Applications</em>, Harold J. Kushner and G. George Yin, New York: Springer-Verlag, 1997. ISBN 0-387-94916-X; 2nd ed., titled <em>Stochastic Approximation and Recursive Algorithms and Applications</em>, 2003, ISBN 0-387-00894-2.<a href="#fnref2">‚Ü©</a></li>
<li id="fn3">Bertsekas, D. P. (2011). Incremental gradient, subgradient, and proximal methods for convex optimization: a survey. Optimization for Machine Learning, 85.<a href="#fnref3">‚Ü©</a></li>
<li id="fn4">Shalev-Shwartz, S. (2011). Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2), 107-194.<a href="#fnref4">‚Ü©</a></li>
</ol>
</section>
</body>
</html>
