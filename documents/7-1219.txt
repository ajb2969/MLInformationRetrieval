   Cochrane–Orcutt estimation      Cochrane–Orcutt estimation   Cochrane–Orcutt estimation is a procedure in econometrics , which adjusts a linear model for serial correlation in the error term . It is named after statisticians  Donald Cochrane and Guy Orcutt . 1  Theory  Consider the model        y  t   =   α  +    X  t   β   +   ε  t     ,       subscript  y  t     α     subscript  X  t   β    subscript  ε  t      y_{t}=\alpha+X_{t}\beta+\varepsilon_{t},\,     where    y  t     subscript  y  t    y_{t}   is the value of the dependent variable of interest at time t ,   β   β   \beta   is a column vector of coefficients to be estimated,    X  t     subscript  X  t    X_{t}   is a row vector of explanatory variables at time t , and    ε  t     subscript  ε  t    \varepsilon_{t}   is the error term at time t .  If it is found via the Durbin–Watson statistic that the error term is serially correlated over time, then standard statistical inference as normally applied to regressions is invalid because standard errors are estimated with bias . To avoid this problem, the residuals must be modeled. If the process generating the residuals is found to be a stationary first-order autoregressive structure , 2       ε  t   =    ρ   ε   t  -  1     +   e  t     ,    |  ρ  |   <  1      formulae-sequence     subscript  ε  t       ρ   subscript  ε    t  1      subscript  e  t         ρ   1     \varepsilon_{t}=\rho\varepsilon_{t-1}+e_{t},\ |\rho|<1   , with the errors {    e  t     subscript  e  t    e_{t}   } being white noise , then the Cochrane–Orcutt procedure can be used to transform the model by taking a quasi-difference:         y  t   -   ρ   y   t  -  1      =    α   (   1  -  ρ   )    +   β   (    X  t   -   ρ   X   t  -  1      )    +   e  t     .         subscript  y  t     ρ   subscript  y    t  1          α    1  ρ      β     subscript  X  t     ρ   subscript  X    t  1        subscript  e  t      y_{t}-\rho y_{t-1}=\alpha(1-\rho)+\beta(X_{t}-\rho X_{t-1})+e_{t}.\,     In this specification the error terms are white noise, so statistical inference is valid. Then the sum of squared residuals (the sum of squared estimates of    e  t  2     superscript   subscript  e  t   2    e_{t}^{2}   ) is minimized with respect to    (  α  ,  β  )     α  β    (\alpha,\beta)   , conditional on   ρ   ρ   \rho   .  Estimating the autoregressive parameter  If   ρ   ρ   \rho   is not known, then it is estimated by first regressing the untransformed model and obtaining the residuals {     ε  ^   t     subscript   normal-^  ε   t    \hat{\varepsilon}_{t}   }, and regressing     ε  ^   t     subscript   normal-^  ε   t    \hat{\varepsilon}_{t}   on     ε  ^    t  -  1      subscript   normal-^  ε     t  1     \hat{\varepsilon}_{t-1}   , leading to an estimate of   ρ   ρ   \rho   and making the transformed regression sketched above feasible. (Note that one data point, the first, is lost in this regression.) This procedure of autoregressing estimated residuals can be done once and the resulting value of   ρ   ρ   \rho   can be used in the transformed y regression, or the residuals of the residuals autoregression can themselves be autoregressed in consecutive steps until no substantial change in the estimated value of   ρ   ρ   \rho   is observed.  It has to be noted, though, that the iterative Cochrane–Orcutt procedure might converge to a local but not global minimum of the residual sum of squares. 3 4  See also   Hildreth–Lu estimation  Prais–Winsten estimation  Newey–West estimator   References  Further reading        External links    by Mark Thoma .   "  Category:Econometrics  Category:Time series analysis  Category:Regression with time series structure     ↩  ↩  ↩  ↩     