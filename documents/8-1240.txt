   Interaction information      Interaction information   The interaction information (McGill 1954), or amounts of information (Hu Kuo Ting, 1962) or co-information (Bell 2003) is one of several generalizations of the mutual information , and expresses the amount information (redundancy or synergy) bound up in a set of variables, beyond that which is present in any subset of those variables. Unlike the mutual information, the interaction information can be either positive or negative. This confusing property has likely retarded its wider adoption as an information measure in machine learning and cognitive science. These functions, their negativity and minima have a direct interpretation in algebraic topology (Baudot & Bennequin, 2015).  The Three-Variable Case  For three variables    {  X  ,  Y  ,  Z  }     X  Y  Z    \{X,Y,Z\}   , the interaction information    I   (  X  ;  Y  ;  Z  )       I   X  Y  Z     I(X;Y;Z)   is given by         I   (  X  ;  Y  ;  Z  )      =     I   (  X  ;  Y  |  Z  )   -  I   (  X  ;  Y  )           =     I   (  X  ;  Z  |  Y  )   -  I   (  X  ;  Z  )           =     I   (  Y  ;  Z  |  X  )   -  I   (  Y  ;  Z  )            I   X  Y  Z      fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)    I   fragments  normal-(  X  normal-;  Y  normal-)      absent    fragments  I   fragments  normal-(  X  normal-;  Z  normal-|  Y  normal-)    I   fragments  normal-(  X  normal-;  Z  normal-)      absent    fragments  I   fragments  normal-(  Y  normal-;  Z  normal-|  X  normal-)    I   fragments  normal-(  Y  normal-;  Z  normal-)       \begin{matrix}I(X;Y;Z)&=&I;(X;Y|Z)-I(X;Y)\\
 &=&I;(X;Z|Y)-I(X;Z)\\
 &=&I;(Y;Z|X)-I(Y;Z)\end{matrix}     where, for example,    I   (  X  ;  Y  )       I   X  Y     I(X;Y)   is the mutual information between variables   X   X   X   and   Y   Y   Y   , and    I   (  X  ;  Y  |  Z  )      fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)     I(X;Y|Z)   is the conditional mutual information between variables   X   X   X   and   Y   Y   Y   given   Z   Z   Z   . Formally,      I   (  X  ;  Y  |  Z  )      fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)     \displaystyle I(X;Y|Z)     It thus follows that       I   (  X  ;  Y  ;  Z  )    =         I   X  Y  Z    absent    \displaystyle I(X;Y;Z)=     For the three-variable case, the interaction information    I   (  X  ;  Y  ;  Z  )       I   X  Y  Z     I(X;Y;Z)   is the difference between the information shared by    {  Y  ,  X  }     Y  X    \{Y,X\}   when   Z   Z   Z   has been fixed and when   Z   Z   Z   has not been fixed. (See also Fano's 1961 textbook.) Interaction information measures the influence of a variable   Z   Z   Z   on the amount of information shared between    {  Y  ,  X  }     Y  X    \{Y,X\}   . Because the term    I   (  X  ;  Y  |  Z  )      fragments  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)     I(X;Y|Z)   can be zero ‚Äî for example, when the dependency between    {  X  ,  Y  }     X  Y    \{X,Y\}   is due entirely to the influence of a common cause   Z   Z   Z   , the interaction information can be negative as well as positive. Negative interaction information indicates that variable   Z   Z   Z   inhibits (i.e., accounts for or explains some of) the correlation between    {  Y  ,  X  }     Y  X    \{Y,X\}   , whereas positive interaction information indicates that variable   Z   Z   Z   facilitates or enhances the correlation between    {  Y  ,  X  }     Y  X    \{Y,X\}   .  Interaction information is bounded. In the three variable case, it is bounded by      -  m  i   n    {  I   (  X  ;  Y  )   ,  I   (  Y  ;  Z  )   ,  I   (  X  ;  Z  )   }   ‚â§  I   (  X  ;  Y  ;  Z  )   ‚â§  m  i   n    {  I   (  X  ;  Y  |  Z  )   ,  I   (  Y  ;  Z  |  X  )   ,  I   (  X  ;  Z  |  Y  )   }      fragments   m  i  n   fragments  normal-{  I   fragments  normal-(  X  normal-;  Y  normal-)   normal-,  I   fragments  normal-(  Y  normal-;  Z  normal-)   normal-,  I   fragments  normal-(  X  normal-;  Z  normal-)   normal-}    I   fragments  normal-(  X  normal-;  Y  normal-;  Z  normal-)    m  i  n   fragments  normal-{  I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)   normal-,  I   fragments  normal-(  Y  normal-;  Z  normal-|  X  normal-)   normal-,  I   fragments  normal-(  X  normal-;  Z  normal-|  Y  normal-)   normal-}     -min\ \{I(X;Y),I(Y;Z),I(X;Z)\}\leq I(X;Y;Z)\leq min\ \{I(X;Y|Z),I(Y;Z|X),I(X;Z%
 |Y)\}     Example of Negative Interaction Information  Negative interaction information seems much more natural than positive interaction information in the sense that such explanatory effects are typical of common-cause structures. For example, clouds cause rain and also block the sun; therefore, the correlation between rain and darkness is partly accounted for by the presence of clouds,    I   (  r  a  i  n  ;  d  a  r  k  |  c  l  o  u  d  )   ‚â§  I   (  r  a  i  n  ;  d  a  r  k  )      fragments  I   fragments  normal-(  r  a  i  n  normal-;  d  a  r  k  normal-|  c  l  o  u  d  normal-)    I   fragments  normal-(  r  a  i  n  normal-;  d  a  r  k  normal-)     I(rain;dark|cloud)\leq I(rain;dark)   . The result is negative interaction information    I   (   r  a  i  n   ;   d  a  r  k   ;   c  l  o  u  d   )       I     r  a  i  n     d  a  r  k     c  l  o  u  d      I(rain;dark;cloud)   .  Example of Positive Interaction Information  The case of positive interaction information seems a bit less natural. A prototypical example of positive    I   (  X  ;  Y  ;  Z  )       I   X  Y  Z     I(X;Y;Z)   has   X   X   X   as the output of an XOR gate to which   Y   Y   Y   and   Z   Z   Z   are the independent random inputs. In this case    I   (  Y  ;  Z  )       I   Y  Z     I(Y;Z)   will be zero, but    I   (  Y  ;  Z  |  X  )      fragments  I   fragments  normal-(  Y  normal-;  Z  normal-|  X  normal-)     I(Y;Z|X)   will be positive (1 bit ) since once output   X   X   X   is known, the value on input   Y   Y   Y   completely determines the value on input   Z   Z   Z   . Since    I   (  Y  ;  Z  |  X  )   >  I   (  Y  ;  Z  )      fragments  I   fragments  normal-(  Y  normal-;  Z  normal-|  X  normal-)    I   fragments  normal-(  Y  normal-;  Z  normal-)     I(Y;Z|X)>I(Y;Z)   , the result is positive interaction information    I   (  X  ;  Y  ;  Z  )       I   X  Y  Z     I(X;Y;Z)   . It may seem that this example relies on a peculiar ordering of    X  ,  Y  ,  Z     X  Y  Z    X,Y,Z   to obtain the positive interaction, but the symmetry of the definition for    I   (  X  ;  Y  ;  Z  )       I   X  Y  Z     I(X;Y;Z)   indicates that the same positive interaction information results regardless of which variable we consider as the interloper or conditioning variable. For example, input   Y   Y   Y   and output   X   X   X   are also independent until input   Z   Z   Z   is fixed, at which time they are totally dependent (obviously), and we have the same positive interaction information as before,    I   (  X  ;  Y  ;  Z  )   =  I   (  X  ;  Y  |  Z  )   -  I   (  X  ;  Y  )      fragments  I   fragments  normal-(  X  normal-;  Y  normal-;  Z  normal-)    I   fragments  normal-(  X  normal-;  Y  normal-|  Z  normal-)    I   fragments  normal-(  X  normal-;  Y  normal-)     I(X;Y;Z)=I(X;Y|Z)-I(X;Y)   .  This situation is an instance where fixing the common effect    X   X   X   of causes   Y   Y   Y   and   Z   Z   Z   induces a dependency among the causes that did not formerly exist. This behavior is colloquially referred to as explaining away and is thoroughly discussed in the Bayesian Network literature (e.g., Pearl 1988). Pearl's example is auto diagnostics: A car's engine can fail to start    (  X  )    X   (X)   due either to a dead battery    (  Y  )    Y   (Y)   or due to a blocked fuel pump    (  Z  )    Z   (Z)   . Ordinarily, we assume that battery death and fuel pump blockage are independent events, because of the essential modularity of such automotive systems. Thus, in the absence of other information, knowing whether or not the battery is dead gives us no information about whether or not the fuel pump is blocked. However, if we happen to know that the car fails to start (i.e., we fix common effect   X   X   X   ), this information induces a dependency between the two causes battery death and fuel blockage . Thus, knowing that the car fails to start, if an inspection shows the battery to be in good health, we can conclude that the fuel pump must be blocked.  Battery death and fuel blockage are thus dependent, conditional on their common effect car starting . What the foregoing discussion indicates is that the obvious directionality in the common-effect graph belies a deep informational symmetry: If conditioning on a common effect increases the dependency between its two parent causes, then conditioning on one of the causes must create the same increase in dependency between the second cause and the common effect. In Pearl's automotive example, if conditioning on car starts induces    I   (  X  ;  Y  ;  Z  )       I   X  Y  Z     I(X;Y;Z)   bits of dependency between the two causes battery dead and fuel blocked , then conditioning on fuel blocked must induce    I   (  X  ;  Y  ;  Z  )       I   X  Y  Z     I(X;Y;Z)   bits of dependency between battery dead and car starts . This may seem odd because battery dead and car starts are already governed by the implication battery dead    ‚Üí   normal-‚Üí   \rightarrow    car doesn't start . However, these variables are still not totally correlated because the converse is not true. Conditioning on fuel blocked removes the major alternate cause of failure to start, and strengthens the converse relation and therefore the association between battery dead and car starts . A paper by Tsujishita (1995) focuses in greater depth on the third-order mutual information.  The Four-Variable Case  One can recursively define the n -dimensional interaction information in terms of the    (   n  -  1   )      n  1    (n-1)   -dimensional interaction information. For example, the four-dimensional interaction information can be defined as      I   (  W  ;  X  ;  Y  ;  Z  )       I   W  X  Y  Z     \displaystyle I(W;X;Y;Z)     or, equivalently,       I   (  W  ;  X  ;  Y  ;  Z  )    =         I   W  X  Y  Z    absent    \displaystyle I(W;X;Y;Z)=     The n -Variable Case  It is possible to extend all of these results to an arbitrary number of dimensions. The general expression for interaction information on variable set    ùí±  =   {   X  1   ,   X  2   ,  ‚Ä¶  ,   X  n   }       ùí±    subscript  X  1    subscript  X  2   normal-‚Ä¶   subscript  X  n      \mathcal{V}=\{X_{1},X_{2},\ldots,X_{n}\}   in terms of the marginal entropies is given by Hu Kuo Ting (1962), Jakulin & Bratko (2003).       I   (  ùí±  )    ‚â°   -    ‚àë   ùíØ  ‚äÜ  ùí±       (   -  1   )     |  ùí±  |   -   |  ùíØ  |     H   (  ùíØ  )            I  ùí±       subscript     ùíØ  ùí±       superscript    1       ùí±     ùíØ     H  ùíØ       I(\mathcal{V})\equiv-\sum_{\mathcal{T}\subseteq\mathcal{V}}(-1)^{\left|%
 \mathcal{V}\right|-\left|\mathcal{T}\right|}H(\mathcal{T})     which is an alternating (inclusion-exclusion) sum over all subsets    ùíØ  ‚äÜ  ùí±      ùíØ  ùí±    \mathcal{T}\subseteq\mathcal{V}   , where     |  ùí±  |   =  n        ùí±   n    \left|\mathcal{V}\right|=n   . Note that this is the information-theoretic analog to the Kirkwood approximation .  Difficulties Interpreting Interaction Information  The possible negativity of interaction information can be the source of some confusion (Bell 2003). As an example of this confusion, consider a set of eight independent binary variables    {   X  1   ,   X  2   ,   X  3   ,   X  4   ,   X  5   ,   X  6   ,   X  7   ,   X  8   }      subscript  X  1    subscript  X  2    subscript  X  3    subscript  X  4    subscript  X  5    subscript  X  6    subscript  X  7    subscript  X  8     \{X_{1},X_{2},X_{3},X_{4},X_{5},X_{6},X_{7},X_{8}\}   . Agglomerate these variables as follows:         Y  1     =     {   X  1   ,   X  2   ,   X  3   ,   X  4   ,   X  5   ,   X  6   ,   X  7   }        Y  2     =     {   X  4   ,   X  5   ,   X  6   ,   X  7   }        Y  3     =     {   X  5   ,   X  6   ,   X  7   ,   X  8   }          subscript  Y  1      subscript  X  1    subscript  X  2    subscript  X  3    subscript  X  4    subscript  X  5    subscript  X  6    subscript  X  7       subscript  Y  2      subscript  X  4    subscript  X  5    subscript  X  6    subscript  X  7       subscript  Y  3      subscript  X  5    subscript  X  6    subscript  X  7    subscript  X  8       \begin{matrix}Y_{1}&=&\{X_{1},X_{2},X_{3},X_{4},X_{5},X_{6},X_{7}\}\\
 Y_{2}&=&\{X_{4},X_{5},X_{6},X_{7}\}\\
 Y_{3}&=&\{X_{5},X_{6},X_{7},X_{8}\}\end{matrix}     Because the    Y  i     subscript  Y  i    Y_{i}   's overlap each other (are redundant) on the three binary variables    {   X  5   ,   X  6   ,   X  7   }      subscript  X  5    subscript  X  6    subscript  X  7     \{X_{5},X_{6},X_{7}\}   , we would expect the interaction information    I   (   Y  1   ;   Y  2   ;   Y  3   )       I    subscript  Y  1    subscript  Y  2    subscript  Y  3      I(Y_{1};Y_{2};Y_{3})   to equal    -  3      3    -3   bits, which it does. However, consider now the agglomerated variables         Y  1     =     {   X  1   ,   X  2   ,   X  3   ,   X  4   ,   X  5   ,   X  6   ,   X  7   }        Y  2     =     {   X  4   ,   X  5   ,   X  6   ,   X  7   }        Y  3     =     {   X  5   ,   X  6   ,   X  7   ,   X  8   }        Y  4     =     {   X  7   ,   X  8   }          subscript  Y  1      subscript  X  1    subscript  X  2    subscript  X  3    subscript  X  4    subscript  X  5    subscript  X  6    subscript  X  7       subscript  Y  2      subscript  X  4    subscript  X  5    subscript  X  6    subscript  X  7       subscript  Y  3      subscript  X  5    subscript  X  6    subscript  X  7    subscript  X  8       subscript  Y  4      subscript  X  7    subscript  X  8       \begin{matrix}Y_{1}&=&\{X_{1},X_{2},X_{3},X_{4},X_{5},X_{6},X_{7}\}\\
 Y_{2}&=&\{X_{4},X_{5},X_{6},X_{7}\}\\
 Y_{3}&=&\{X_{5},X_{6},X_{7},X_{8}\}\\
 Y_{4}&=&\{X_{7},X_{8}\}\end{matrix}     These are the same variables as before with the addition of     Y  4   =   {   X  7   ,   X  8   }        subscript  Y  4     subscript  X  7    subscript  X  8      Y_{4}=\{X_{7},X_{8}\}   . Because the    Y  i     subscript  Y  i    Y_{i}   's now overlap each other (are redundant) on only one binary variable    {   X  7   }      subscript  X  7     \{X_{7}\}   , we would expect the interaction information    I   (   Y  1   ;   Y  2   ;   Y  3   ;   Y  4   )       I    subscript  Y  1    subscript  Y  2    subscript  Y  3    subscript  Y  4      I(Y_{1};Y_{2};Y_{3};Y_{4})   to equal    -  1      1    -1   bit. However,    I   (   Y  1   ;   Y  2   ;   Y  3   ;   Y  4   )       I    subscript  Y  1    subscript  Y  2    subscript  Y  3    subscript  Y  4      I(Y_{1};Y_{2};Y_{3};Y_{4})   in this case is actually equal to    +  1      1    +1   bit, indicating a synergy rather than a redundancy. This is correct in the sense that         I   (   Y  1   ;   Y  2   ;   Y  3   ;   Y  4   )      =     I   (   Y  1   ;   Y  2   ;   Y  3   |   Y  4   )   -  I   (   Y  1   ;   Y  2   ;   Y  3   )           =      -  2   +  3          =    1          I    subscript  Y  1    subscript  Y  2    subscript  Y  3    subscript  Y  4       fragments  I   fragments  normal-(   subscript  Y  1   normal-;   subscript  Y  2   normal-;   subscript  Y  3   normal-|   subscript  Y  4   normal-)    I   fragments  normal-(   subscript  Y  1   normal-;   subscript  Y  2   normal-;   subscript  Y  3   normal-)      absent       2   3     absent   1     \begin{matrix}I(Y_{1};Y_{2};Y_{3};Y_{4})&=&I;(Y_{1};Y_{2};Y_{3}|Y_{4})-I(Y_{1};%
 Y_{2};Y_{3})\\
 &=&-2+3\\
 &=&1\end{matrix}     but it remains difficult to interpret.  Uses of Interaction Information   Jakulin and Bratko (2003b) provide a machine learning algorithm which uses interaction information.  Killian, Kravitz and Gilson (2007) use mutual information expansion to extract entropy estimates from molecular simulations.  Moore et al. (2006), Chanda P, Zhang A, Brazeau D, Sucheston L, Freudenheim JL, Ambrosone C, Ramanathan M. (2007) and Chanda P, Sucheston L, Zhang A, Brazeau D, Freudenheim JL, Ambrosone C, Ramanathan M. (2008) demonstrate the use of interaction information for analyzing gene-gene and gene-environmental interactions associated with complex diseases.   References   Baudot P., Bennequin D. The homological nature of entropy. Entropy, 2015, 17, 1-66. PDF    Bell, A J (2003), ¬ëThe co-information lattice¬í 1    Fano, R M (1961), Transmission of Information: A Statistical Theory of Communications , MIT Press, Cambridge, MA.    Garner W R (1962). Uncertainty and Structure as Psychological Concepts , JohnWiley & Sons, New York.          Hu Kuo Tin (1962), On the Amount of Information. Theory Probab. Appl.,7(4), 439-44. PDF    Jakulin A & Bratko I (2003a). Analyzing Attribute Dependencies, in N Lavra\quad{c}, D Gamberger, L Todorovski & H Blockeel, eds, Proceedings of the 7th European Conference on Principles and Practice of Knowledge Discovery in Databases , Springer, Cavtat-Dubrovnik, Croatia, pp.¬†229‚Äì240.    Jakulin A & Bratko I (2003b). Quantifying and visualizing attribute interactions 2 .          Moore JH, Gilbert JC, Tsai CT, Chiang FT, Holden T, Barney N, White BC (2006). A flexible computational framework for detecting, characterizing, and interpreting statistical patterns of epistasis in genetic studies of human disease susceptibility, Journal of Theoretical Biology  241 , 252-261. [ http://www.ncbi.nlm.nih.gov/pubmed/16457852?ordinalpos=19&itool; ;=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_DefaultReportPanel.Pubmed_RVDocSum]    Nemenman I (2004). Information theory, multivariate dependence, and genetic network inference 3 .    Pearl, J (1988), Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference , Morgan Kaufmann, San Mateo, CA.    Tsujishita, T (1995), ¬ëOn triple mutual information¬í, Advances in applied mathematics  16 , 269-¬ñ274.          "  Category:Information theory   