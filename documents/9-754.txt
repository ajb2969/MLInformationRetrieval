   Power transform      Power transform   In statistics , the power transform corresponds to a family of functions that are applied to create a monotonic transformation of data using power functions . This is a useful data transformation technique used to stabilize variance, make the data more normal distribution -like, improve the validity of measures of association such as the Pearson correlation between variables and for other data stabilization procedures.  Definition  The power transformation is defined as a continuously varying function, with respect to the power parameter λ , in a piece-wise function form that makes it continuous at the point of singularity ( λ = 0). For data vectors ( y 1 ,..., y n ) in which each y i > 0, the power transform is       y  i   (  λ  )    =   {         y  i  λ   -  1    λ    (   GM   (  y  )    )    λ  -  1      ,       if  λ   ≠  0          GM   (  y  )     ln   y  i     ,       if  λ   =  0            superscript   subscript  y  i   λ    cases       superscript   subscript  y  i   λ   1     λ   superscript   GM  y     λ  1          if  λ   0     GM  y      subscript  y  i         if  λ   0      y_{i}^{(\lambda)}=\begin{cases}\dfrac{y_{i}^{\lambda}-1}{\lambda(\operatorname%
 {GM}(y))^{\lambda-1}},&\text{if }\lambda\neq 0\\
 \operatorname{GM}(y)\ln{y_{i}},&\text{if }\lambda=0\end{cases}     where       GM   (  y  )    =     (    y  1   ⋯   y  n    )    1  /  n          GM  y    superscript     subscript  y  1   normal-⋯   subscript  y  n      1  n      \operatorname{GM}(y)=(y_{1}\cdots y_{n})^{1/n}\,     is the geometric mean of the observations y 1 , ..., y n .  The inclusion of the ( λ − 1)th power of the geometric mean in the denominator simplifies the scientific interpretation of any equation involving     y  i   (  λ  )      superscript   subscript  y  i   λ    y_{i}^{(\lambda)}   , because the units of measurement do not change as λ changes.  Box and Cox (1964) introduced the geometric mean into this transformation by first including the Jacobian of rescaled power transformation        y  λ   -  1   λ         superscript  y  λ   1   λ    \dfrac{y^{\lambda}-1}{\lambda}   .  with the likelihood. This Jacobian is as follows:      J   (  λ  ;   y  1   ,  …  ,   y  n   )   =   ∏   i  =  1   n   |  d   y  i   (  λ  )    /  d  y  |  =   ∏   i  =  1   n    y  i   λ  -  1    =  GM    (  y  )    n   (   λ  -  1   )        fragments  J   fragments  normal-(  λ  normal-;   subscript  y  1   normal-,  normal-…  normal-,   subscript  y  n   normal-)     superscript   subscript  product    i  1    n   normal-|  d   superscript   subscript  y  i   λ    d  y  normal-|    superscript   subscript  product    i  1    n    superscript   subscript  y  i     λ  1     GM   superscript   fragments  normal-(  y  normal-)     n    λ  1       J(\lambda;y_{1},...,y_{n})=\prod_{i=1}^{n}|dy_{i}^{(\lambda)}/dy|=\prod_{i=1}^%
 {n}y_{i}^{\lambda-1}=\operatorname{GM}(y)^{n(\lambda-1)}     This allows the normal log likelihood at its maximum to be written as follows:      log   (  ℒ   (   μ  ^   ,   σ  ^   )   )   =   (  -  n  /  2  )    (  log   (  2  π    σ  ^   2   )   +  1  )   +  n   (  λ  -  1  )   log   (  GM   (  y  )   )   =   (  -  n  /  2  )    (  log   (  2  π    σ  ^   2   /  GM    (  y  )    2   (   λ  -  1   )     )   +  1  )   .     fragments    fragments  normal-(  L   fragments  normal-(   normal-^  μ   normal-,   normal-^  σ   normal-)   normal-)     fragments  normal-(   n   2  normal-)    fragments  normal-(    fragments  normal-(  2  π   superscript   normal-^  σ   2   normal-)    1  normal-)    n   fragments  normal-(  λ   1  normal-)     fragments  normal-(  GM   fragments  normal-(  y  normal-)   normal-)     fragments  normal-(   n   2  normal-)    fragments  normal-(    fragments  normal-(  2  π   superscript   normal-^  σ   2    GM   superscript   fragments  normal-(  y  normal-)     2    λ  1     normal-)    1  normal-)   normal-.    \log(\mathcal{L}(\hat{\mu},\hat{\sigma}))=(-n/2)(\log(2\pi\hat{\sigma}^{2})+1)%
 +n(\lambda-1)\log(\operatorname{GM}(y))=(-n/2)(\log(2\pi\hat{\sigma}^{2}/%
 \operatorname{GM}(y)^{2(\lambda-1)})+1).     From here, absorbing    GM    (  y  )    2   (   λ  -  1   )        fragments  GM   superscript   fragments  normal-(  y  normal-)     2    λ  1       \operatorname{GM}(y)^{2(\lambda-1)}   into the expression for     σ  ^   2     superscript   normal-^  σ   2    \hat{\sigma}^{2}   produces an expression that establishes that minimizing the sum of squares of residuals from    y  i   (  λ  )      superscript   subscript  y  i   λ    y_{i}^{(\lambda)}   is equivalent to maximizing the sum of the normal log likelihood of deviations from     (    y  λ   -  1   )   /  λ         superscript  y  λ   1   λ    (y^{\lambda}-1)/\lambda   and the log of the Jacobian of the transformation.  The value at Y = 1 for any λ is 0, and the derivative with respect to Y there is 1 for any λ . Sometimes Y is a version of some other variable scaled to give Y = 1 at some sort of average value.  The transformation is a power transformation, but done in such a way as to make it continuous with the parameter λ at λ = 0. It has proved popular in regression analysis , including econometrics .  Box and Cox also proposed a more general form of the transformation that incorporates a shift parameter.       τ   (   y  i   ;  λ  ,  α  )    =   {         (    y  i   +  α   )   λ   -  1    λ    (   GM   (   y  +  α   )    )    λ  -  1           if  λ   ≠  0   ,               GM   (   y  +  α   )     ln   (    y  i   +  α   )          if  λ   =  0   ,             τ    subscript  y  i   λ  α     cases       superscript     subscript  y  i   α   λ   1     λ   superscript   GM    y  α      λ  1          if  λ   0   absent  otherwise    GM    y  α         subscript  y  i   α         if  λ   0      \tau(y_{i};\lambda,\alpha)=\begin{cases}\dfrac{(y_{i}+\alpha)^{\lambda}-1}{%
 \lambda(\operatorname{GM}(y+\alpha))^{\lambda-1}}&\text{if }\lambda\neq 0,\\
 \\
 \operatorname{GM}(y+\alpha)\ln(y_{i}+\alpha)&\text{if }\lambda=0,\end{cases}   which holds if y i + α > 0 for all i . If τ( Y , λ, α) follows a truncated normal distribution , then Y is said to follow a Box–Cox distribution .  Bickel and Doksum eliminated the need to use a truncated distribution by extending the range of the transformation to all y , as follows:       τ   (   y  i   ;  λ  ,  α  )    =   {         sgn   (    y  i   +  α   )      |    y  i   +  α   |   λ    -  1    λ    (   GM   (   y  +  α   )    )    λ  -  1           if  λ   ≠  0   ,               GM   (   y  +  α   )      sgn   (   y  +  α   )     ln   (    y  i   +  α   )           if  λ   =  0   ,             τ    subscript  y  i   λ  α     cases        sgn     subscript  y  i   α     superscript       subscript  y  i   α    λ    1     λ   superscript   GM    y  α      λ  1          if  λ   0   absent  otherwise     GM    y  α      sgn    y  α         subscript  y  i   α          if  λ   0      \tau(y_{i};\lambda,\alpha)=\begin{cases}\dfrac{\operatorname{sgn}(y_{i}+\alpha%
 )|y_{i}+\alpha|^{\lambda}-1}{\lambda(\operatorname{GM}(y+\alpha))^{\lambda-1}}%
 &\text{if }\lambda\neq 0,\\
 \\
 \operatorname{GM}(y+\alpha)\operatorname{sgn}(y+\alpha)\ln(y_{i}+\alpha)&\text%
 {if }\lambda=0,\end{cases}   ,  where sgn(.) is the Sign function . This change in definition has little practical import as long as   α   α   \alpha   is less than    min   (   y  i   )      min   subscript  y  i     \operatorname{min}(y_{i})   , which it usually is. 1  Bickel and Doksum also proved that the parameter estimates are consistent and asymptotically normal under appropriate regularity conditions, though the standard Cramér–Rao lower bound can substantially underestimate the variance when parameter values are small relative to the noise variance. 2 However, this problem of underestimating the variance may not be a substantive problem in many applications. 3 4  Box–Cox transformation  The one-parameter Box–Cox transformations are defined as:       y  i   (  λ  )    =   {         y  i  λ   -  1   λ         if  λ   ≠  0   ,        ln   (   y  i   )         if  λ   =  0   ,            superscript   subscript  y  i   λ    cases       superscript   subscript  y  i   λ   1   λ       if  λ   0      subscript  y  i        if  λ   0      y_{i}^{(\lambda)}=\begin{cases}\dfrac{y_{i}^{\lambda}-1}{\lambda}&\text{if }%
 \lambda\neq 0,\\
 \ln{(y_{i})}&\text{if }\lambda=0,\end{cases}     and the two-parameter Box-Cox transformations as:       y  i   (  𝝀  )    =   {          (    y  i   +   λ  2    )    λ  1    -  1    λ  1          if   λ  1    ≠  0   ,        ln   (    y  i   +   λ  2    )         if   λ  1    =  0   ,            superscript   subscript  y  i   𝝀    cases       superscript     subscript  y  i    subscript  λ  2     subscript  λ  1    1    subscript  λ  1        if   subscript  λ  1    0        subscript  y  i    subscript  λ  2         if   subscript  λ  1    0      y_{i}^{(\boldsymbol{\lambda})}=\begin{cases}\dfrac{(y_{i}+\lambda_{2})^{%
 \lambda_{1}}-1}{\lambda_{1}}&\text{if }\lambda_{1}\neq 0,\\
 \ln{(y_{i}+\lambda_{2})}&\text{if }\lambda_{1}=0,\end{cases}     as described in the original article. 5 6 Moreover, the first transformations hold for     y  i   >  0       subscript  y  i   0    y_{i}>0   and the second for     y  i   >   -   λ  2         subscript  y  i      subscript  λ  2      y_{i}>-\lambda_{2}   . 7  The parameter   λ   λ   \lambda   is estimated using the profile likelihood function.  Use of the power transform   Power transforms are ubiquitously used in various fields. For example, [ http://portal.acm.org/citation.cfm?id=1172964.1173292&coll; ;=&dl;=acm&CFID;=15151515&CFTOKEN;=6184618 multi-resolution and wavelet analysis], statistical data analysis, medical research , modeling of physical processes , geochemical data analysis , epidemiology 8 and many other clinical, environmental and social research areas.   Example  The BUPA liver data set 9 contains data on liver enzymes ALT and γGT . Suppose we are interested in using log(γGT) to predict ALT. A plot of the data appears in panel (a) of the figure. There appears to be non-constant variance, and a Box–Cox transformation might help.  image:BUPA BoxCox.JPG  The log-likelihood of the power parameter appears in panel (b). The horizontal reference line is at a distance of χ 1 2 /2 from the maximum and can be used to read off an approximate 95% confidence interval for λ. It appears as though a value close to zero would be good, so we take logs.  Possibly, the transformation could be improved by adding a shift parameter to the log transformation. Panel (c) of the figure shows the log-likelihood. In this case, the maximum of the likelihood is close to zero suggesting that a shift parameter is not needed. The final panel shows the transformed data with a superimposed regression line.  Note that although Box–Cox transformations can make big improvements in model fit, there are some issues that the transformation cannot help with. In the current example, the data are rather heavy-tailed so that the assumption of normality is not realistic and a robust regression approach leads to a more precise model.  Econometric application  Economists often characterize production relationships by some variant of the Box–Cox transformation.  Consider a common representation of production Q as dependent on services provided by a capital stock K and by labor hours N :        τ   (  Q  )    =    α  τ   (  K  )    +    (   1  -  α   )   τ   (  N  )      .        τ  Q       α  τ  K       1  α   τ  N      \tau(Q)=\alpha\tau(K)+(1-\alpha)\tau(N).\,     Solving for Q by inverting the Box–Cox transformation we find       Q  =    (    α   K  λ    +    (   1  -  α   )    N  λ     )    1  /  λ     ,      Q   superscript      α   superscript  K  λ        1  α    superscript  N  λ       1  λ      Q=\big(\alpha K^{\lambda}+(1-\alpha)N^{\lambda}\big)^{1/\lambda},\,     which is known as the constant elasticity of substitution (CES) production function.  The CES production function is a homogeneous function of degree one.  When λ = 1, this produces the linear production function:       Q  =    α  K   +    (   1  -  α   )   N     .      Q      α  K       1  α   N      Q=\alpha K+(1-\alpha)N.\,     When λ → 0 this produces the famous Cobb–Douglas production function:       Q  =    K  α    N   1  -  α      .      Q     superscript  K  α    superscript  N    1  α       Q=K^{\alpha}N^{1-\alpha}.\,     Activities and demonstrations  The SOCR resource pages contain a number of hands-on interactive activities 10 demonstrating the Box–Cox (Power) Transformation using Java applets and charts. These directly illustrate the effects of this transform on Q-Q plots , X-Y scatterplots , time-series plots and histograms .  Notes  References    Carroll, RJ and Ruppert, D. On prediction and the power transformation family . Biometrika 68: 609–615.   Handelsman, DJ. Optimal Power Transformations for Analysis of Sperm Concentration and Other Semen Variables. Journal of Andrology, Vol. 23, No. 5, September/October 2002.     External links    ( fixed link )   "  Category:Data analysis  Category:Transforms  Category:Statistical models     ↩   ↩  ↩  ↩  ↩   ↩  BUPA liver disorder dataset ↩  Power Transform Family Graphs , SOCR webpages ↩     