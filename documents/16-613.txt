   Matrix analysis      Matrix analysis   In mathematics , particularly in linear algebra and applications, matrix analysis is the study of matrices and their algebraic properties. 1 Some particular topics out of many include; operations defined on matrices (such as matrix addition , matrix multiplication and operations derived from these), functions of matrices (such as matrix exponentiation and matrix logarithm , and even sines and cosines etc. of matrices), 2 and the eigenvalues of matrices ( eigendecomposition of a matrix , eigenvalue perturbation theory).  Matrix spaces  The set of all m × n matrices over a number field  F denoted in this article M mn ( F ) form a vector space . Examples of F include the set of integers ℤ, the real numbers ℝ, and set of complex numbers ℂ. The spaces M mn ( F ) and M pq ( F ) are different spaces if m and p are unequal, and if n and q are unequal; for instance M 32 ( F ) ≠ M 23 ( F ). Two m × n matrices A and B in M mn ( F ) can be added together to form another matrix in the space M mn ( F ):        𝐀  ,  𝐁   ∈    M   m  n     (  F  )     ,    𝐀  +  𝐁   ∈    M   m  n     (  F  )        formulae-sequence     𝐀  𝐁      subscript  M    m  n    F        𝐀  𝐁      subscript  M    m  n    F      \mathbf{A},\mathbf{B}\in M_{mn}(F)\,,\quad\mathbf{A}+\mathbf{B}\in M_{mn}(F)     and multiplied by a α in F , to obtain another matrix in M mn ( F ):       α  ∈   F    ,    α  𝐀   ∈    M   m  n     (  F  )        formulae-sequence    α  F       α  𝐀      subscript  M    m  n    F      \alpha\in F\,,\quad\alpha\mathbf{A}\in M_{mn}(F)     Combining these two properties, a linear combination of matrices A and B are in M mn ( F ) is another matrix in M mn ( F ):        α  𝐀   +   β  𝐁    ∈    M   m  n     (  F  )            α  𝐀     β  𝐁       subscript  M    m  n    F     \alpha\mathbf{A}+\beta\mathbf{B}\in M_{mn}(F)     where α and β are numbers in F .  Any matrix can be expressed as a linear combination of basis matrices, which play the role of the basis vectors for the matrix space. For example, for the set of 2×2 matrices over the field of real numbers, M 22 (ℝ), one legitimate basis set of matrices is:         (     1    0      0    0     )    ,    (     0    1      0    0     )    ,    (     0    0      1    0     )    ,    (     0    0      0    1     )     ,       1  0    0  0      0  1    0  0      0  0    1  0      0  0    0  1      \begin{pmatrix}1&0\\
 0&0\end{pmatrix}\,,\quad\begin{pmatrix}0&1\\
 0&0\end{pmatrix}\,,\quad\begin{pmatrix}0&0\\
 1&0\end{pmatrix}\,,\quad\begin{pmatrix}0&0\\
 0&1\end{pmatrix}\,,     because any 2×2 matrix can be expressed as:        (     a     b  ;       c     d  ;      )   =    a   (     1    0      0    0     )    +   b   (     0    1      0    0     )    +   c   (     0    0      1    0     )    +   d    (     0    0      0    1     )       ,        a  b    c  d        a    1  0    0  0       b    0  1    0  0       c    0  0    1  0       d    0  0    0  1        \begin{pmatrix}a&b\\
 c&d\end{pmatrix}=a\begin{pmatrix}1&0\\
 0&0\end{pmatrix}+b\begin{pmatrix}0&1\\
 0&0\end{pmatrix}+c\begin{pmatrix}0&0\\
 1&0\end{pmatrix}+d\begin{pmatrix}0&0\\
 0&1\end{pmatrix}\,,     where a , b , c , d are all real numbers. This idea applies to other fields and matrices of higher dimensions.  Determinants  The determinant of a square matrix is an important property. The determinant indicates if a matrix is invertible (i.e. the inverse of a matrix exists when the determinant is nonzero). Determinants are used for finding eigenvalues of matrices (see below), and for solving a system of linear equations (see Cramer's rule ).  Eigenvalues and eigenvectors of matrices  Definitions  An n × n matrix A has eigenvectors  x and eigenvalues  λ defined by the relation:      𝐀𝐱  =   λ  𝐱       𝐀𝐱    λ  𝐱     \mathbf{A}\mathbf{x}=\lambda\mathbf{x}     In words, the matrix multiplication of A followed by an eigenvector x (here an n -dimensional column matrix ), is the same as multiplying the eigenvector by the eigenvalue. For an n × n matrix, there are n eigenvalues. The eigenvalues are the roots of the characteristic polynomial :        p  𝐀    (  λ  )    =   det   (   𝐀  -   λ  𝐈    )    =  0           subscript  p  𝐀   λ       𝐀    λ  𝐈          0     p_{\mathbf{A}}(\lambda)=\det(\mathbf{A}-\lambda\mathbf{I})=0     where I is the n × n  identity matrix .  Roots of polynomials , in this context the eigenvalues, can all be different, or some may be equal (in which case eigenvalue has multiplicity , the number of times an eigenvalue occurs). After solving for the eigenvalues, the eigenvectors corresponding to the eigenvalues can be found by the defining equation.  Perturbations of eigenvalues  Matrix similarity  Two n × n matrices A and B are similar if they are related by a similarity transformation :      𝐁  =   𝐏𝐀𝐏   -  1        𝐁   superscript  𝐏𝐀𝐏    1      \mathbf{B}=\mathbf{P}\mathbf{A}\mathbf{P}^{-1}     The matrix P is called a similarity matrix , and is necessarily invertible .  Unitary similarity  Canonical forms  Row echelon form  Jordan normal form  Weyr canonical form  Frobenius normal form  Triangular factorization  LU decomposition  LU decomposition splits a matrix into a matrix product of an upper triangular matrix and a lower triangle matrix.  Matrix norms  Since matrices form vector spaces, one can form axioms (analogous to those of vectors) to define a "size" of a particular matrix. The norm of a matrix is a positive real number.  Definition and axioms  For all matrices A and B in M mn ( F ), and all numbers α in F , a matrix norm, delimited by double vertical bars || ... ||, fulfills: 3   Nonnegative :          ∥  𝐀  ∥   ≥  0       norm  𝐀   0    \|\mathbf{A}\|\geq 0       with equality only for A = 0 , the zero matrix .    Scalar multiplication :          ∥   α  𝐀   ∥   =    |  α  |    ∥  𝐀  ∥         norm    α  𝐀        α    norm  𝐀      \|\alpha\mathbf{A}\|=|\alpha|\|\mathbf{A}\|         The triangular inequality :          ∥   𝐀  +  𝐁   ∥   ≤    ∥  𝐀  ∥   +   ∥  𝐁  ∥         norm    𝐀  𝐁       norm  𝐀    norm  𝐁      \|\mathbf{A}+\mathbf{B}\|\leq\|\mathbf{A}\|+\|\mathbf{B}\|        Frobenius norm  The Frobenius norm is analogous to the dot product of Euclidean vectors; multiply matrix elements entry-wise, add up the results, then take the positive square root:       ∥  𝐀  ∥   =    𝐀  :  𝐀    =     ∑   i  =  1   m     ∑   j  =  1   n     (   A   i  j    )   2             norm  𝐀      normal-:  𝐀  𝐀             superscript   subscript     i  1    m     superscript   subscript     j  1    n    superscript   subscript  A    i  j    2         \|\mathbf{A}\|=\sqrt{\mathbf{A}:\mathbf{A}}=\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}%
 (A_{ij})^{2}}     It is defined for matrices of any dimension (i.e. no restriction to square matrices).  Positive definite and semidefinite matrices  Functions  Matrix elements are not restricted to constant numbers, they can be mathematical variables .  Functions of matrices  A functions of a matrix takes in a matrix, and return something else (a number, vector, matrix, etc...).  Matrix-valued functions  A matrix valued function takes in something (a number, vector, matrix, etc...) and returns a matrix.  See also  Other branches of analysis   Mathematical analysis  Tensor analysis  Matrix calculus  Numerical analysis   Other concepts of linear algebra   Tensor product  Spectrum of an operator  Matrix geometrical series   Types of matrix   Orthogonal matrix , unitary matrix  Symmetric matrix , antisymmetric matrix  Stochastic matrix   Matrix functions   Matrix polynomial  Matrix exponential   Footnotes  References  Notes  Further reading              "  Category:Linear algebra  Category:Matrices  Category:Numerical analysis     ↩  ↩  Some authors, e.g. Horn and Johnson, use triple vertical bars instead of double: ||| A |||. ↩     