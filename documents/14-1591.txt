   Restricted Boltzmann machine      Restricted Boltzmann machine   A restricted Boltzmann machine ( RBM ) is a generative  stochastic  artificial neural network that can learn a probability distribution over its set of inputs. RBMs were initially invented under the name Harmonium by Paul Smolensky in 1986, 1 but only rose to prominence after Geoffrey Hinton and collaborators invented fast learning algorithms for them in the mid-2000s. RBMs have found applications in dimensionality reduction , 2  classification , 3  collaborative filtering , 4  feature learning 5 and topic modelling . 6 They can be trained in either supervised or unsupervised ways, depending on the task.  As their name implies, RBMs are a variant of Boltzmann machines , with the restriction that their neurons must form a bipartite graph : a pair of nodes from each of the two groups of units, commonly referred to as the "visible" and "hidden" units respectively, may have a symmetric connection between them, and there are no connections between nodes within a group. By contrast, "unrestricted" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based  contrastive divergence algorithm. 7  Restricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by "stacking" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation . 8  Structure  The standard type of RBM has binary-valued ( Boolean / Bernoulli ) hidden and visible units, and consists of a matrix of weights    W  =   (   w   i  ,  j    )       W   subscript  w   i  j      W=(w_{i,j})   (size m √ó n ) associated with the connection between hidden unit    h  j     subscript  h  j    h_{j}   and visible unit    v  i     subscript  v  i    v_{i}   , as well as bias weights (offsets)    a  i     subscript  a  i    a_{i}   for the visible units and    b  j     subscript  b  j    b_{j}   for the hidden units. Given these, the energy of a configuration (pair of boolean vectors)    (  v  ,  h  )     v  h    (v,h)   is defined as       E   (  v  ,  h  )    =    -    ‚àë  i     a  i    v  i      -    ‚àë  j     b  j    h  j     -    ‚àë  i     ‚àë  j     v  i    w   i  ,  j     h  j             E   v  h          subscript   i      subscript  a  i    subscript  v  i        subscript   j      subscript  b  j    subscript  h  j       subscript   i     subscript   j      subscript  v  i    subscript  w   i  j     subscript  h  j         E(v,h)=-\sum_{i}a_{i}v_{i}-\sum_{j}b_{j}h_{j}-\sum_{i}\sum_{j}v_{i}w_{i,j}h_{j}     or, in matrix notation,       E   (  v  ,  h  )    =    -    a  T   v    -    b  T   h   -    v  T   W  h          E   v  h           superscript  a  normal-T   v       superscript  b  normal-T   h      superscript  v  normal-T   W  h      E(v,h)=-a^{\mathrm{T}}v-b^{\mathrm{T}}h-v^{\mathrm{T}}Wh     This energy function is analogous to that of a Hopfield network . As in general Boltzmann machines, probability distributions over hidden and/or visible vectors are defined in terms of the energy function: 9       P   (  v  ,  h  )    =    1  Z    e   -   E   (  v  ,  h  )             P   v  h        1  Z    superscript  e      E   v  h         P(v,h)=\frac{1}{Z}e^{-E(v,h)}     where   Z   Z   Z   is a partition function defined as the sum of    e   -   E   (  v  ,  h  )        superscript  e      E   v  h       e^{-E(v,h)}   over all possible configurations (in other words, just a normalizing constant to ensure the probability distribution sums to 1). Similarly, the ( marginal ) probability of a visible (input) vector of booleans is the sum over all possible hidden layer configurations: 10       P   (  v  )    =    1  Z     ‚àë  h    e   -   E   (  v  ,  h  )              P  v       1  Z     subscript   h    superscript  e      E   v  h          P(v)=\frac{1}{Z}\sum_{h}e^{-E(v,h)}     Since the RBM has the shape of a bipartite graph, with no intra-layer connections, the hidden unit activations are mutually independent given the visible unit activations and conversely, the visible unit activations are mutually independent given the hidden unit activations. 11 That is, for   m   m   m   visible units and   n   n   n   hidden units, the conditional probability of a configuration of the visible units   v   v   v   , given a configuration of the hidden units   h   h   h   , is      P   (  v  |  h  )   =   ‚àè   i  =  1   m   P   (   v  i   |  h  )      fragments  P   fragments  normal-(  v  normal-|  h  normal-)     superscript   subscript  product    i  1    m   P   fragments  normal-(   subscript  v  i   normal-|  h  normal-)     P(v|h)=\prod_{i=1}^{m}P(v_{i}|h)   .  Conversely, the conditional probability of   h   h   h   given   v   v   v   is      P   (  h  |  v  )   =   ‚àè   j  =  1   n   P   (   h  j   |  v  )      fragments  P   fragments  normal-(  h  normal-|  v  normal-)     superscript   subscript  product    j  1    n   P   fragments  normal-(   subscript  h  j   normal-|  v  normal-)     P(h|v)=\prod_{j=1}^{n}P(h_{j}|v)   .  The individual activation probabilities are given by      P   (   h  j   =  1  |  v  )   =  œÉ   (   b  j   +   ‚àë   i  =  1   m    w   i  ,  j     v  i   )      fragments  P   fragments  normal-(   subscript  h  j    1  normal-|  v  normal-)    œÉ   fragments  normal-(   subscript  b  j     superscript   subscript     i  1    m    subscript  w   i  j     subscript  v  i   normal-)     P(h_{j}=1|v)=\sigma\left(b_{j}+\sum_{i=1}^{m}w_{i,j}v_{i}\right)\,   and     P    (   v  i   =  1  |  h  )   =  œÉ   (   a  i   +   ‚àë   j  =  1   n    w   i  ,  j     h  j   )      fragments  P   fragments  normal-(   subscript  v  i    1  normal-|  h  normal-)    œÉ   fragments  normal-(   subscript  a  i     superscript   subscript     j  1    n    subscript  w   i  j     subscript  h  j   normal-)     \,P(v_{i}=1|h)=\sigma\left(a_{i}+\sum_{j=1}^{n}w_{i,j}h_{j}\right)     where   œÉ   œÉ   \sigma   denotes the logistic sigmoid .  The visible units of RBM can be multinomial, although the hidden units are Bernoulli. In this case, the logistic function for visible units is replaced by the Softmax function      P   (   v  i  k   =  1  |  h  )   =    exp   (    a  i  k   +    Œ£  j    W   i  j   k    h  j     )      Œ£   k  =  1   K    exp   (    a  i  k   +    Œ£  j    W   i  j   k    h  j     )         fragments  P   fragments  normal-(   superscript   subscript  v  i   k    1  normal-|  h  normal-)           superscript   subscript  a  i   k      subscript  normal-Œ£  j    superscript   subscript  W    i  j    k    subscript  h  j         superscript   subscript  normal-Œ£    k  1    K        superscript   subscript  a  i   k      subscript  normal-Œ£  j    superscript   subscript  W    i  j    k    subscript  h  j          P(v_{i}^{k}=1|h)=\frac{\exp(a_{i}^{k}+\Sigma_{j}W_{ij}^{k}h_{j})}{\Sigma_{k=1}%
 ^{K}\exp(a_{i}^{k}+\Sigma_{j}W_{ij}^{k}h_{j})}     where K is the number of discrete values that the visible values have. They are applied in Topic Modeling, 12 and RecSys . 13  Relation to other models  Restricted Boltzmann machines are a special case of Boltzmann machines and Markov random fields . 14 15 Their graphical model corresponds to that of factor analysis . 16  Training algorithm  Restricted Boltzmann machines are trained to maximize the product of probabilities assigned to some training set   V   V   V   (a matrix, each row of which is treated as a visible vector   v   v   v   ),       arg   max  W      ‚àè   v  ‚àà  V     P   (  v  )            subscript   W      subscript  product    v  V      P  v      \arg\max_{W}\prod_{v\in V}P(v)     or equivalently, to maximize the expected  log probability of   V   V   V   : 17 18       arg    max  W   ùîº     [    ‚àë   v  ‚àà  V      log  P    (  v  )     ]           subscript   W   ùîº     delimited-[]    subscript     v  V        P   v       \arg\max_{W}\mathbb{E}\left[\sum_{v\in V}\log P(v)\right]     The algorithm most often used to train RBMs, that is, to optimize the weight vector   W   W   W   , is the contrastive divergence (CD) algorithm due to Hinton , originally developed to train PoE ( product of experts ) models. 19  20 The algorithm performs Gibbs sampling and is used inside a gradient descent procedure (similar to the way backpropagation is used inside such a procedure when training feedforward neural nets) to compute weight update.  The basic, single-step contrastive divergence (CD-1) procedure for a single sample can be summarized as follows:   Take a training sample   v   v   v   , compute the probabilities of the hidden units and sample a hidden activation vector   h   h   h   from this probability distribution.  Compute the outer product of   v   v   v   and   h   h   h   and call this the positive gradient .  From   h   h   h   , sample a reconstruction   v   v   v   of the visible units, then resample the hidden activations   h   h   h   from this. (Gibbs sampling step)  Compute the outer product of   v   v   v   and   h   h   h   and call this the negative gradient .  Let the weight update to    w   i  ,  j      subscript  w   i  j     w_{i,j}   be the positive gradient minus the negative gradient, times some learning rate     Œî   w   i  ,  j     =   œµ   (    v   h  ùñ≥    -    v  ‚Ä≤    h  ‚Ä≤   T    )          normal-Œî   subscript  w   i  j       œµ      v   superscript  h  ùñ≥       superscript  v  normal-‚Ä≤    superscript  h  normal-‚Ä≤   T       \Delta w_{i,j}=\epsilon(vh^{\mathsf{T}}-v^{\prime}h^{\prime\mathsf{}}{T})   .   The update rule for the biases   a   a   a   ,   b   b   b   is defined analogously.  A Practical Guide to Training RBMs written by Hinton can be found in his homepage. 21  A restricted/layered Boltzmann machine (RBM) has either bit or scalar node values, an array for each layer, and between those are scalar values potentially for every pair of nodes one from each layer and an adjacent layer. It is run and trained using "weighted coin flips" of a chance calculated at each individual node. Those chances are the logistic sigmoid of the sum of scalar weights of whichever pairs of nodes are on at the time, divided by temperature which decreases in each round of Simulated annealing as potentially all the data is trained in again. If either node in a pair is off, that weight is not counted. To run it, you go up and down the layers, updating the chances and weighted coin flips, until it converges to the coins in lowest layer (visible nodes) staying mostly a certain way. To train it, it is the same shape as running it except you observe the weights of the pairs that are on, the first time up you add the learning rate between those pairs, then go back down and up again and that time subtract the learning rate. As Geoffrey Hinton explained it, the first time up is to learn the data, and the second time up is to unlearn whatever its earlier reaction was to the data.  See also   Autoencoder  Deep learning  Helmholtz machine  Hopfield network   References  External links   Introduction to Restricted Boltzmann Machines . Edwin Chen's blog, July 18, 2011.  A Beginner's Guide to Restricted Boltzmann Machines . Deeplearning4j Documentation  Understanding RBMs . Deeplearning4j Documentation, August 4, 2015.   "  Category:Artificial neural networks  Category:Stochastic models     ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  Ruslan Salakhutdinov and Geoffrey Hinton (2010). Replicated softmax: an undirected topic model . Neural Information Processing Systems  23 . ‚Ü©  Miguel √Å. Carreira-Perpi√±√°n and Geoffrey Hinton (2005). On contrastive divergence learning. Artificial Intelligence and Statistics . ‚Ü©  ‚Ü©  Geoffrey Hinton (2010). A Practical Guide to Training Restricted Boltzmann Machines . UTML TR 2010‚Äì003, University of Toronto. ‚Ü©      ‚Ü©  Asja Fischer and Christian Igel. Training Restricted Boltzmann Machines: An Introduction . Pattern Recognition 47, pp. 25-39, 2014 ‚Ü©  ‚Ü©    Geoffrey Hinton (1999). Products of Experts . ICANN 1999 . ‚Ü©  ‚Ü©      