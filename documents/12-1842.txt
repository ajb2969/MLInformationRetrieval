   Vector space model      Vector space model   Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering , information retrieval , indexing and relevancy rankings. Its first use was in the SMART Information Retrieval System .  Definitions  Documents and queries are represented as vectors.       d  j   =   (   w   1  ,  j    ,   w   2  ,  j    ,  …  ,   w   t  ,  j    )        subscript  d  j     subscript  w   1  j     subscript  w   2  j    normal-…   subscript  w   t  j       d_{j}=(w_{1,j},w_{2,j},\ldots,w_{t,j})         q  =   (   w   1  ,  q    ,   w   2  ,  q    ,  …  ,   w   n  ,  q    )       q    subscript  w   1  q     subscript  w   2  q    normal-…   subscript  w   n  q       q=(w_{1,q},w_{2,q},\ldots,w_{n,q})     Each dimension corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is tf-idf weighting (see the example below).  The definition of term depends on the application. Typically terms are single words, keywords , or longer phrases. If words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus ).  Vector operations can be used to compare documents with queries.  Applications  (Figure)  vector space model.jpg   Relevance  rankings of documents in a keyword search can be calculated, using the assumptions of document similarities theory, by comparing the deviation of angles between each document vector and the original query vector where the query is represented as the same kind of vector as the documents.  In practice, it is easier to calculate the cosine of the angle between the vectors, instead of the angle itself:       cos  θ   =     𝐝  𝟐   ⋅  𝐪     ∥   𝐝  𝟐   ∥    ∥  𝐪  ∥           θ      normal-⋅   subscript  𝐝  2   𝐪      norm   subscript  𝐝  2     norm  𝐪       \cos{\theta}=\frac{\mathbf{d_{2}}\cdot\mathbf{q}}{\left\|\mathbf{d_{2}}\right%
 \|\left\|\mathbf{q}\right\|}     Where     𝐝  𝟐   ⋅  𝐪     normal-⋅   subscript  𝐝  2   𝐪    \mathbf{d_{2}}\cdot\mathbf{q}   is the intersection (i.e. the dot product ) of the document (d 2 in the figure to the right) and the query (q in the figure) vectors,    ∥   𝐝  𝟐   ∥     norm   subscript  𝐝  2     \left\|\mathbf{d_{2}}\right\|   is the norm of vector d 2 , and    ∥  𝐪  ∥     norm  𝐪    \left\|\mathbf{q}\right\|   is the norm of vector q. The norm of a vector is calculated as such:       ∥  𝐪  ∥   =     ∑   i  =  1   n    q  i  2          norm  𝐪       superscript   subscript     i  1    n    superscript   subscript  q  i   2       \left\|\mathbf{q}\right\|=\sqrt{\sum_{i=1}^{n}q_{i}^{2}}     As all vectors under consideration by this model are elementwise nonnegative, a cosine value of zero means that the query and document vector are orthogonal and have no match (i.e. the query term does not exist in the document being considered). See cosine similarity for further information.  Example: tf-idf weights  In the classic vector space model proposed by Salton , Wong and Yang 1 the term-specific weights in the document vectors are products of local and global parameters. The model is known as term frequency-inverse document frequency model. The weight vector for document d is     𝐯  d   =    [   w   1  ,  d    ,   w   2  ,  d    ,  …  ,   w   N  ,  d    ]   T        subscript  𝐯  d    superscript    subscript  w   1  d     subscript  w   2  d    normal-…   subscript  w   N  d     T     \mathbf{v}_{d}=[w_{1,d},w_{2,d},\ldots,w_{N,d}]^{T}   , where       w   t  ,  d    =    tf   t  ,  d    ⋅   log    |  D  |    |   {    d  ′   ∈   D    |   t  ∈   d  ′    }   |           subscript  w   t  d     normal-⋅   subscript  tf   t  d          D      conditional-set     superscript  d  normal-′   D     t   superscript  d  normal-′           w_{t,d}=\mathrm{tf}_{t,d}\cdot\log{\frac{|D|}{|\{d^{\prime}\in D\,|\,t\in d^{%
 \prime}\}|}}     and       tf   t  ,  d      subscript  tf   t  d     \mathrm{tf}_{t,d}   is term frequency of term t in document d (a local parameter)      log    |  D  |    |   {    d  ′   ∈   D    |   t  ∈   d  ′    }   |            D      conditional-set     superscript  d  normal-′   D     t   superscript  d  normal-′         \log{\frac{|D|}{|\{d^{\prime}\in D\,|\,t\in d^{\prime}\}|}}   is inverse document frequency (a global parameter).    |  D  |      D    |D|   is the total number of documents in the document set;    |   {    d  ′   ∈   D    |   t  ∈   d  ′    }   |       conditional-set     superscript  d  normal-′   D     t   superscript  d  normal-′       |\{d^{\prime}\in D\,|\,t\in d^{\prime}\}|   is the number of documents containing the term t .   Using the cosine the similarity between document d j and query q can be calculated as:       sim   (   d  j   ,  q  )    =     𝐝  𝐣   ⋅  𝐪     ∥   𝐝  𝐣   ∥    ∥  𝐪  ∥     =     ∑   i  =  1   N     w   i  ,  j     w   i  ,  q          ∑   i  =  1   N    w   i  ,  j   2        ∑   i  =  1   N    w   i  ,  q   2               sim    subscript  d  j   q       normal-⋅   subscript  𝐝  𝐣   𝐪      norm   subscript  𝐝  𝐣     norm  𝐪              superscript   subscript     i  1    N      subscript  w   i  j     subscript  w   i  q            superscript   subscript     i  1    N    superscript   subscript  w   i  j    2         superscript   subscript     i  1    N    superscript   subscript  w   i  q    2          \mathrm{sim}(d_{j},q)=\frac{\mathbf{d_{j}}\cdot\mathbf{q}}{\left\|\mathbf{d_{j%
 }}\right\|\left\|\mathbf{q}\right\|}=\frac{\sum_{i=1}^{N}w_{i,j}w_{i,q}}{\sqrt%
 {\sum_{i=1}^{N}w_{i,j}^{2}}\sqrt{\sum_{i=1}^{N}w_{i,q}^{2}}}     Advantages  The vector space model has the following advantages over the Standard Boolean model :   Simple model based on linear algebra  Term weights not binary  Allows computing a continuous degree of similarity between queries and documents  Allows ranking documents according to their possible relevance  Allows partial matching   Limitations  The vector space model has the following limitations:   Long documents are poorly represented because they have poor similarity values (a small scalar product and a large dimensionality )  Search keywords must precisely match document terms; word substrings might result in a " false positive match"  Semantic sensitivity; documents with similar context but different term vocabulary won't be associated, resulting in a " false negative match".  The order in which the terms appear in the document is lost in the vector space representation.  Theoretically assumes terms are statistically independent.  Weighting is intuitive but not very formal.   Many of these difficulties can, however, be overcome by the integration of various tools, including mathematical techniques such as singular value decomposition and lexical databases such as WordNet .  Models based on and extending the vector space model  Models based on and extending the vector space model include:   Generalized vector space model  Latent semantic analysis  Term Discrimination  Rocchio Classification  Random Indexing   Software that implements the vector space model  The following software packages may be of interest to those wishing to experiment with vector models and implement search services based upon them.  Free open source software   Apache Lucene . Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java.  Gensim is a Python+ NumPy framework for Vector Space modelling. It contains incremental (memory-efficient) algorithms for Tf–idf , Latent Semantic Indexing , Random Projections and Latent Dirichlet Allocation .  Weka . Weka is popular data mining package for Java including WordVectors and Bag Of Words models.   Further reading   G. Salton , A. Wong, and C. S. Yang (1975), " A Vector Space Model for Automatic Indexing ," Communications of the ACM , vol. 18, nr. 11, pages 613–620. (Article in which a vector space model was presented)  David Dubin (2004), The Most Influential Paper Gerard Salton Never Wrote  (Explains the history of the Vector Space Model and the non-existence of a frequently cited publication)  Description of the vector space model  Description of the classic vector space model by Dr E. Garcia  Relationship of vector space search to the "k-Nearest Neighbor" search   See also   Bag-of-words model  Nearest neighbor search  Compound term processing  Inverted index  w-shingling  Eigenvalues and eigenvectors  Conceptual Spaces  Sparse distributed memory   References    "  *     G. Salton , A. Wong , C. S. Yang, A vector space model for automatic indexing , Communications of the ACM, v.18 n.11, p.613-620, Nov. 1975 ↩     