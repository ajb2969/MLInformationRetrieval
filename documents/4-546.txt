   Stein's lemma      Stein's lemma   Stein's lemma , 1 named in honor of Charles Stein , is a theorem of probability theory that is of interest primarily because of its applications to statistical inference — in particular, to James–Stein estimation and empirical Bayes methods — and its applications to portfolio choice theory . The theorem gives a formula for the covariance of one random variable with the value of a function of another, when the two random variables are jointly normally distributed .  Statement of the lemma  Suppose X is a normally distributed  random variable with expectation μ and variance σ 2 . Further suppose g is a function for which the two expectations E( g ( X ) ( X − μ) ) and E( g ′( X ) ) both exist (the existence of the expectation of any random variable is equivalent to the finiteness of the expectation of its absolute value ). Then        E   (   g   (  X  )    (   X  -  μ   )    )    =    σ  2   E   (    g  ′    (  X  )    )     .        E    g  X    X  μ        superscript  σ  2   E     superscript  g  normal-′   X      E\bigl(g(X)(X-\mu)\bigr)=\sigma^{2}E\bigl(g^{\prime}(X)\bigr).     In general, suppose X and Y are jointly normally distributed. Then        Cov   (   g   (  X  )    ,  Y  )    =   E   (    g  ′    (  X  )    )    Cov   (  X  ,  Y  )      .       Cov    g  X   Y     E     superscript  g  normal-′   X    Cov  X  Y      \operatorname{Cov}(g(X),Y)=E(g^{\prime}(X))\operatorname{Cov}(X,Y).     Proof  In order to prove the univariate version of this lemma, recall that the probability density function for the normal distribution with expectation 0 and variance 1 is       φ   (  x  )    =    1    2  π      e   -    x  2   /  2            φ  x       1      2  π      superscript  e       superscript  x  2   2        \varphi(x)={1\over\sqrt{2\pi}}e^{-x^{2}/2}     and that for a normal distribution with expectation μ and variance σ 2 is        1  σ   φ   (    x  -  μ   σ   )    .        1  σ   φ      x  μ   σ     {1\over\sigma}\varphi\left({x-\mu\over\sigma}\right).     Then use integration by parts .  More general statement  Suppose X is in an exponential family , that is, X has the density         f  η    (  x  )    =    exp   (     η  ′   T   (  x  )    -   Ψ   (  η  )     )    h   (  x  )     .         subscript  f  η   x            superscript  η  normal-′   T  x     normal-Ψ  η     h  x     f_{\eta}(x)=\exp(\eta^{\prime}T(x)-\Psi(\eta))h(x).     Suppose this density has support    (  a  ,  b  )     a  b    (a,b)   where    a  ,  b     a  b    a,b   could be     -  ∞   ,  ∞            -\infty,\infty   and as    x  →   a  or  b      normal-→  x    a  or  b     x\rightarrow a\text{ or }b   ,      exp   (    η  ′   T   (  x  )    )    h   (  x  )   g   (  x  )    →  0     normal-→         superscript  η  normal-′   T  x    h  x  g  x   0    \exp(\eta^{\prime}T(x))h(x)g(x)\rightarrow 0   where   g   g   g   is any differentiable function such that     E   |    g  ′    (  X  )    |    <  ∞        E       superscript  g  normal-′   X        E|g^{\prime}(X)|<\infty   or      exp   (    η  ′   T   (  x  )    )    h   (  x  )    →  0     normal-→         superscript  η  normal-′   T  x    h  x   0    \exp(\eta^{\prime}T(x))h(x)\rightarrow 0   if    a  ,  b     a  b    a,b   finite. Then        E   (    (       h  ′    (  X  )    /  h    (  X  )    +   ∑    η  i    T  i  ′    (  X  )      )   g   (  X  )    )    =   -   E   g  ′    (  X  )      .        E             superscript  h  normal-′   X   h   X        subscript  η  i    superscript   subscript  T  i   normal-′   X     g  X        E   superscript  g  normal-′   X      E((h^{\prime}(X)/h(X)+\sum\eta_{i}T_{i}^{\prime}(X))g(X))=-Eg^{\prime}(X).     The derivation is same as the special case, namely, integration by parts.  If we only know   X   X   X   has support   ℝ   ℝ   \mathbb{R}   , then it could be the case that     E   |   g   (  X  )    |    <   ∞  and  E   |    g  ′    (  X  )    |    <  ∞          E      g  X        and  E       superscript  g  normal-′   X              E|g(X)|<\infty\text{ and }E|g^{\prime}(X)|<\infty   but      lim   x  →  ∞      f  η    (  x  )   g   (  x  )     ≠  0        subscript    normal-→  x        subscript  f  η   x  g  x    0    \lim_{x\rightarrow\infty}f_{\eta}(x)g(x)\not=0   . To see this, simply put     g   (  x  )    =  1        g  x   1    g(x)=1   and     f  η    (  x  )        subscript  f  η   x    f_{\eta}(x)   with infinitely spikes towards infinity but still integrable. One such example could be adapted from     f   (  x  )    =   {     1     x  ∈   [  n  ,   n  +   2   -  n     )        0    otherwise            f  x    cases  1    x   n    n   superscript  2    n       0  otherwise     f(x)=\begin{cases}1&x\in[n,n+2^{-n})\\
 0&\text{otherwise}\end{cases}   so that   f   f   f   is smooth.  Extensions to elliptically-contoured distributions also exist. 2 3  See also   Taylor expansions for the moments of functions of random variables   References    "  Category:Statistical theorems  Category:Probability theorems     Ingersoll, J., Theory of Financial Decision Making , Rowman and Littlefield, 1987: 13-14. ↩  ↩  ↩     