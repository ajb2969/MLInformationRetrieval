   Muller's method      Muller's method   Muller's method is a root-finding algorithm , a numerical method for solving equations of the form f ( x ) = 0. It was first presented by David E. Muller in 1956.  Muller's method is based on the secant method , which constructs at every iteration a line through two points on the graph of f . Instead, Muller's method uses three points, constructs the parabola through these three points, and takes the intersection of the x -axis with the parabola to be the next approximation.  Recurrence relation  Muller's method is a recursive method which generates an approximation of the root ξ of f at each iteration. Starting with the three initial values x 0 , x -1 and x -2 , the first iteration calculates the first approximation x 1 , the second iteration calculates the second approximation x 2 , the third iteration calculates the third approximation x 3 , etc. Hence the k th iteration generates approximation x k . Each iteration takes as input the last three generated approximations and the value of f at these approximations. Hence the k th iteration takes as input the values x k -1 , x k -2 and x k -3 and the function values f ( x k -1 ), f ( x k -2 ) and f ( x k -3 ). The approximation x k is calculated as follows.  A parabola y k ( x ) is constructed which goes through the three points ( x k -1 , f ( x k -1 )), ( x k -2 , f ( x k -2 )) and ( x k -3 , f ( x k -3 )). When written in the Newton form , y k ( x ) is         y  k    (  x  )    =    f   (   x   k  -  1    )    +    (   x  -   x   k  -  1     )   f   [   x   k  -  1    ,   x   k  -  2    ]    +    (   x  -   x   k  -  1     )    (   x  -   x   k  -  2     )   f   [   x   k  -  1    ,   x   k  -  2    ,   x   k  -  3    ]      ,         subscript  y  k   x       f   subscript  x    k  1         x   subscript  x    k  1     f    subscript  x    k  1     subscript  x    k  2          x   subscript  x    k  1       x   subscript  x    k  2     f    subscript  x    k  1     subscript  x    k  2     subscript  x    k  3         y_{k}(x)=f(x_{k-1})+(x-x_{k-1})f[x_{k-1},x_{k-2}]+(x-x_{k-1})(x-x_{k-2})f[x_{k%
 -1},x_{k-2},x_{k-3}],\,   where f [ x k -1 , x k -2 ] and f [ x k -1 , x k -2 , x k -3 ] denote divided differences . This can be rewritten as        y  k    (  x  )    =    f   (   x   k  -  1    )    +   w   (   x  -   x   k  -  1     )    +   f   [   x   k  -  1    ,   x   k  -  2    ,   x   k  -  3    ]      (   x  -   x   k  -  1     )   2             subscript  y  k   x       f   subscript  x    k  1       w    x   subscript  x    k  1        f    subscript  x    k  1     subscript  x    k  2     subscript  x    k  3      superscript    x   subscript  x    k  1     2       y_{k}(x)=f(x_{k-1})+w(x-x_{k-1})+f[x_{k-1},x_{k-2},x_{k-3}]\,(x-x_{k-1})^{2}\,   where       w  =     f   [   x   k  -  1    ,   x   k  -  2    ]    +   f   [   x   k  -  1    ,   x   k  -  3    ]     -   f   [   x   k  -  2    ,   x   k  -  3    ]      .      w        f    subscript  x    k  1     subscript  x    k  2        f    subscript  x    k  1     subscript  x    k  3         f    subscript  x    k  2     subscript  x    k  3         w=f[x_{k-1},x_{k-2}]+f[x_{k-1},x_{k-3}]-f[x_{k-2},x_{k-3}].\,   The next iterate x k is now given as the solution closest to x k -1 of the quadratic equation y k ( x ) = 0. This yields the recurrence relation        x  k   =    x   k  -  1    -    2  f   (   x   k  -  1    )     w  ±     w  2   -   4  f   (   x   k  -  1    )   f   [   x   k  -  1    ,   x   k  -  2    ,   x   k  -  3    ]          .       subscript  x  k      subscript  x    k  1        2  f   subscript  x    k  1      plus-or-minus  w       superscript  w  2     4  f   subscript  x    k  1    f    subscript  x    k  1     subscript  x    k  2     subscript  x    k  3             x_{k}=x_{k-1}-\frac{2f(x_{k-1})}{w\pm\sqrt{w^{2}-4f(x_{k-1})f[x_{k-1},x_{k-2},%
 x_{k-3}]}}.   In this formula, the sign should be chosen such that the denominator is as large as possible in magnitude. We do not use the standard formula for solving quadratic equations because that may lead to loss of significance .  Note that x k can be complex, even if the previous iterates were all real. This is in contrast with other root-finding algorithms like the secant method , Sidi's generalized secant method or Newton's method , whose iterates will remain real if one starts with real numbers. Having complex iterates can be an advantage (if one is looking for complex roots) or a disadvantage (if it is known that all roots are real), depending on the problem.  Speed of convergence  The order of convergence of Muller's method is approximately 1.84. This can be compared with 1.62 for the secant method and 2 for Newton's method . So, the secant method makes less progress per iteration than Muller's method and Newton's method makes more progress.  More precisely, if ξ denotes a single root of f (so f (ξ) = 0 and f '(ξ) ≠ 0), f is three times continuously differentiable, and the initial guesses x 0 , x 1 , and x 2 are taken sufficiently close to ξ, then the iterates satisfy         lim   k  →  ∞      |    x  k   -  ξ   |     |    x   k  -  1    -  ξ   |   μ     =    |     f  ′′′    (  ξ  )     6   f  ′    (  ξ  )     |     (   μ  -  1   )   /  2     ,        subscript    normal-→  k            subscript  x  k   ξ     superscript       subscript  x    k  1    ξ    μ      superscript         superscript  f  ′′′   ξ     6   superscript  f  normal-′   ξ         μ  1   2      \lim_{k\to\infty}\frac{|x_{k}-\xi|}{|x_{k-1}-\xi|^{\mu}}=\left|\frac{f^{\prime%
 \prime\prime}(\xi)}{6f^{\prime}(\xi)}\right|^{(\mu-1)/2},   where μ ≈ 1.84 is the positive solution of      x  3   -   x  2   -  x  -  1   =  0         superscript  x  3    superscript  x  2   x  1   0    x^{3}-x^{2}-x-1=0   .  Generalizations and related methods  Muller's method fits a parabola, i.e. a second-order polynomial , to the last three obtained points f ( x k -1 ), f ( x k -2 ) and f ( x k -3 ) in each iteration. One can generalize this and fit a polynomial p k,m ( x ) of degree  m to the last m +1 points in the k th iteration. Our parabola y k is written as p k ,2 in this notation. The degree m must be 1 or larger. The next approximation x k is now one of the roots of the p k,m , i.e. one of the solutions of p k,m ( x )=0. Taking m =1 we obtain the secant method whereas m =2 gives Muller's method.  Muller calculated that the sequence { x k } generated this way converges to the root ξ with an order μ m where μ m is the positive solution of      x   m  +  1    -   x  m   -   x   m  -  1    -  …  -  x  -  1   =  0         superscript  x    m  1     superscript  x  m    superscript  x    m  1    normal-…  x  1   0    x^{m+1}-x^{m}-x^{m-1}-\dots-x-1=0   .  The method is much more difficult though for m >2 than it is for m =1 or m =2 because it is much harder to determine the roots of a polynomial of degree 3 or higher. Another problem is that there seems no prescription of which of the roots of p k,m to pick as the next approximation x k for m >2.  These difficulties are overcome by Sidi's generalized secant method which also employs the polynomial p k,m . Instead of trying to solve p k,m ( x )=0, the next approximation x k is calculated with the aid of the derivative of p k,m at x k -1 in this method.  References   Muller, David E., "A Method for Solving Algebraic Equations Using an Automatic Computer," Mathematical Tables and Other Aids to Computation , 10 (1956), 208-215.  Atkinson, Kendall E. (1989). An Introduction to Numerical Analysis , 2nd edition, Section 2.4. John Wiley & Sons, New York. ISBN 0-471-50023-2.  Burden, R. L. and Faires, J. D. Numerical Analysis , 4th edition, pages 77ff.    External links   Module for Muller's Method by John H. Mathews   "  Category:Root-finding algorithms   