   Linear predictor function      Linear predictor function   In statistics and in machine learning , a linear predictor function is a linear function ( linear combination ) of a set of coefficients and explanatory variables ( independent variables ), whose value is used to predict the outcome of a dependent variable . Functions of this sort are standard in linear regression , where the coefficients are termed regression coefficients . However, they also occur in various types of linear classifiers (e.g. logistic regression , perceptrons , support vector machines , and linear discriminant analysis ), as well as in various other models, such as principal component analysis and factor analysis . In many of these models, the coefficients are referred to as "weights".  Basic form  The basic form of a linear predictor function    f   (  i  )       f  i    f(i)   for data point i (consisting of p explanatory variables), for i = 1, ..., n , is        f   (  i  )    =    Î²  0   +    Î²  1    x   i  1     +  â‹¯  +    Î²  p    x   i  p       ,        f  i      subscript  Î²  0      subscript  Î²  1    subscript  x    i  1     normal-â‹¯     subscript  Î²  p    subscript  x    i  p        f(i)=\beta_{0}+\beta_{1}x_{i1}+\cdots+\beta_{p}x_{ip},     where     Î²  0   ,  â€¦  ,   Î²  p       subscript  Î²  0   normal-â€¦   subscript  Î²  p     \beta_{0},\ldots,\beta_{p}   are the coefficients (regression coefficients, weights, etc.) indicating the relative effect of a particular explanatory variable on the outcome.  It is common to write the predictor function in a more compact form as follows:   The coefficients Î² 0 , Î² 1 , ..., Î² p are grouped into a single vector Î² of size p +Â 1.  For each data point i , an additional explanatory pseudo-variable x i 0 is added, with a fixed value of 1, corresponding to the intercept coefficient Î² 0 .  The resulting explanatory variables x i0 , x i 1 , ..., x ip are then grouped into a single vector x i of size p +Â 1.   This makes it possible to write the linear predictor function as follows:       f   (  i  )    =   ğœ·  â‹…   ğ±  i          f  i    normal-â‹…  ğœ·   subscript  ğ±  i      f(i)=\boldsymbol{\beta}\cdot\mathbf{x}_{i}     using the notation for a dot product between two vectors.  An equivalent form using matrix notation is as follows:       f   (  i  )    =    ğœ·  T    ğ±  i    =    ğ±  i  T   ğœ·           f  i      superscript  ğœ·  normal-T    subscript  ğ±  i            subscript   superscript  ğ±  normal-T   i   ğœ·      f(i)=\boldsymbol{\beta}^{\mathrm{T}}\mathbf{x}_{i}=\mathbf{x}^{\mathrm{T}}_{i}%
 \boldsymbol{\beta}     where   ğœ·   ğœ·   \boldsymbol{\beta}   and    ğ±  i     subscript  ğ±  i    \mathbf{x}_{i}   are assumed to be a p -by-1 column vectors (as is standard when representing vectors as matrices),    ğœ·  T     superscript  ğœ·  normal-T    \boldsymbol{\beta}^{\mathrm{T}}   indicates the matrix transpose of   ğœ·   ğœ·   \boldsymbol{\beta}   (which turns it into a 1-by- p  row vector ), and     ğœ·  T    ğ±  i        superscript  ğœ·  normal-T    subscript  ğ±  i     \boldsymbol{\beta}^{\mathrm{T}}\mathbf{x}_{i}   indicates matrix multiplication between the 1-by- p row vector and the p -by-1 column vector, producing a 1-by-1 matrix that is taken to be a scalar .  An example of the usage of such a linear predictor function is in linear regression , where each data point is associated with a continuous outcome y i , and the relationship written        y  i   =    f   (  i  )    +   Îµ  i    =     ğœ·  T     ğ±  i     +   Îµ  i     ,         subscript  y  i       f  i    subscript  Îµ  i              superscript  ğœ·  normal-T    subscript  ğ±  i     subscript  Îµ  i       y_{i}=f(i)+\varepsilon_{i}=\boldsymbol{\beta}^{\mathrm{T}}\mathbf{x}_{i}\ +%
 \varepsilon_{i},     where    Îµ  i     subscript  Îµ  i    \varepsilon_{i}   is a disturbance term or error variable â€” an unobserved random variable that adds noise to the linear relationship between the dependent variable and predictor function.  Stacking  In some models (standard linear regression in particular), the equations for each of the data points i = 1, ..., n are stacked together and written in vector form as       ğ²  =    ğ—  ğœ·   +  ğœº    ,      ğ²      ğ—  ğœ·   ğœº     \mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon},\,   where         ğ²  =   (      y  1        y  2       â‹®       y  n      )    ,   ğ—  =   (      ğ±  1  â€²        ğ±  2  â€²       â‹®       ğ±  n  â€²      )   =   (      x  11     â‹¯     x   1  p         x  21     â‹¯     x   2  p        â‹®    â‹±    â‹®       x   n  1      â‹¯     x   n  p       )     ,    ğœ·  =   (      Î²  1       â‹®       Î²  p      )    ,   ğœº  =   (      Îµ  1        Îµ  2       â‹®       Îµ  n      )      .     formulae-sequence   formulae-sequence    ğ²     subscript  y  1      subscript  y  2     normal-â‹®     subscript  y  n          ğ—     subscript   superscript  ğ±  normal-â€²   1      subscript   superscript  ğ±  normal-â€²   2     normal-â‹®     subscript   superscript  ğ±  normal-â€²   n             subscript  x  11   normal-â‹¯   subscript  x    1  p       subscript  x  21   normal-â‹¯   subscript  x    2  p      normal-â‹®  normal-â‹±  normal-â‹®     subscript  x    n  1    normal-â‹¯   subscript  x    n  p          formulae-sequence    ğœ·     subscript  Î²  1     normal-â‹®     subscript  Î²  p        ğœº     subscript  Îµ  1      subscript  Îµ  2     normal-â‹®     subscript  Îµ  n         \mathbf{y}=\begin{pmatrix}y_{1}\\
 y_{2}\\
 \vdots\\
 y_{n}\end{pmatrix},\quad\mathbf{X}=\begin{pmatrix}\mathbf{x}^{\prime}_{1}\\
 \mathbf{x}^{\prime}_{2}\\
 \vdots\\
 \mathbf{x}^{\prime}_{n}\end{pmatrix}=\begin{pmatrix}x_{11}&\cdots&x_{1p}\\
 x_{21}&\cdots&x_{2p}\\
 \vdots&\ddots&\vdots\\
 x_{n1}&\cdots&x_{np}\end{pmatrix},\quad\boldsymbol{\beta}=\begin{pmatrix}\beta%
 _{1}\\
 \vdots\\
 \beta_{p}\end{pmatrix},\quad\boldsymbol{\varepsilon}=\begin{pmatrix}%
 \varepsilon_{1}\\
 \varepsilon_{2}\\
 \vdots\\
 \varepsilon_{n}\end{pmatrix}.     The matrix X is known as the design matrix and encodes all known information about the independent variables . The variables    Îµ  i     subscript  Îµ  i    \varepsilon_{i}   are random variables , which in standard linear regression are distributed according to a standard normal distribution ; they express the influence of any unknown factors on the outcome.  This makes it possible to find optimal coefficients through the method of least squares using simple matrix operations. In particular, the optimal coefficients    ğœ·  ^     bold-^  ğœ·    \boldsymbol{\hat{\beta}}   as estimated by least squares can be written as follows:        ğœ·  ^   =     (    X  T   X   )    -  1     X  T   ğ²    .       bold-^  ğœ·      superscript     superscript  X  normal-T   X     1     superscript  X  normal-T   ğ²     \boldsymbol{\hat{\beta}}=(X^{\mathrm{T}}X)^{-1}X^{\mathrm{T}}\mathbf{y}.     The matrix      (    X  T   X   )    -  1     X  T        superscript     superscript  X  normal-T   X     1     superscript  X  normal-T     (X^{\mathrm{T}}X)^{-1}X^{\mathrm{T}}   is known as the Moore-Penrose pseudoinverse of X . Note that this formula assumes that X is of full rank , i.e. there is no multicollinearity among different explanatory variables (i.e. one variable can be perfectly, or almost perfectly, predicted from another). In such cases, the singular value decomposition can be used to compute the pseudoinverse.  The explanatory variables  Although the outcomes (dependent variables) to be predicted are assumed to be random variables , the explanatory variables themselves are usually not assumed to be random. Instead, they are assumed to be fixed values, and any random variables (e.g. the outcomes) are assumed to be conditional on them. As a result, the model user is free to transform the explanatory variables in arbitrary ways, including creating multiple copies of a given explanatory variable, each transformed using a different function. Other common techniques are to create new explanatory variables in the form of interaction variables by taking products of two (or sometimes more) existing explanatory variables.  When a fixed set of nonlinear functions are used to transform the value(s) of a data point, these functions are known as basis functions . An example is polynomial regression , which uses a linear predictor function to fit an arbitrary degree polynomial relationship (up to a given order) between two sets of data points (i.e. a single real-valued explanatory variable and a related real-valued dependent variable), by adding multiple explanatory variables corresponding to various powers of the existing explanatory variable. Mathematically, the form looks like this:        y  i   =    Î²  0   +    Î²  1    x  i    +    Î²  2    x  i  2    +  â‹¯  +    Î²  p    x  i  p      ,       subscript  y  i      subscript  Î²  0      subscript  Î²  1    subscript  x  i       subscript  Î²  2    superscript   subscript  x  i   2    normal-â‹¯     subscript  Î²  p    superscript   subscript  x  i   p       y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+\cdots+\beta_{p}x_{i}^{p},     In this case, for each data point, a set of explanatory variables is created as follows:      (     x   i  1    =   x  i    ,     x   i  2    =    x  i  2   ,  â€¦    ,    x   i  p    =   x  i  p      )     formulae-sequence     subscript  x    i  1     subscript  x  i     formulae-sequence     subscript  x    i  2      superscript   subscript  x  i   2   normal-â€¦       subscript  x    i  p     superscript   subscript  x  i   p       (x_{i1}=x_{i},x_{i2}=x_{i}^{2},\ldots,x_{ip}=x_{i}^{p})     and then standard linear regression is run. The basis functions in this example would be        Ï•   (  x  )    =   (    Ï•  1    (  x  )    ,    Ï•  2    (  x  )    ,  â€¦  ,    Ï•  p    (  x  )    )   =   (  x  ,   x  2   ,  â€¦  ,   x  p   )    .          bold-italic-Ï•  x       subscript  Ï•  1   x      subscript  Ï•  2   x   normal-â€¦     subscript  Ï•  p   x          x   superscript  x  2   normal-â€¦   superscript  x  p       \boldsymbol{\phi}(x)=(\phi_{1}(x),\phi_{2}(x),\ldots,\phi_{p}(x))=(x,x^{2},%
 \ldots,x^{p}).     This example shows that a linear predictor function can actually be much more powerful than it first appears: It only really needs to be linear in the coefficients . All sorts of non-linear functions of the explanatory variables can be fit by the model.  There is no particular need for the inputs to basis functions to be univariate or single-dimensional (or their outputs, for that matter, although in such a case, a K -dimensional output value is likely to be treated as K separate scalar-output basis functions). An example of this is radial basis functions (RBF's), which compute some transformed version of the distance to some fixed point:       Ï•   (  ğ±  ;  ğœ  )    =   Ï•   (   ||   ğ±  -  ğœ   ||   )    =   Ï•   (      (    x  1   -   c  1    )   2   +  â€¦  +    (    x  K   -   c  K    )   2     )            Ï•   ğ±  ğœ      Ï•   norm    ğ±  ğœ            Ï•       superscript     subscript  x  1    subscript  c  1    2   normal-â€¦   superscript     subscript  x  K    subscript  c  K    2         \phi(\mathbf{x};\mathbf{c})=\phi(||\mathbf{x}-\mathbf{c}||)=\phi(\sqrt{(x_{1}-%
 c_{1})^{2}+\ldots+(x_{K}-c_{K})^{2}})     An example is the Gaussian RBF, which has the same functional form as the normal distribution :       Ï•   (  ğ±  ;  ğœ  )    =   e   -   b    ||   ğ±  -  ğœ   ||   2            Ï•   ğ±  ğœ     superscript  e      b   superscript   norm    ğ±  ğœ    2        \phi(\mathbf{x};\mathbf{c})=e^{-b||\mathbf{x}-\mathbf{c}||^{2}}     which drops off rapidly as the distance from c increases.  A possible usage of RBF's is to create one for every observed data point. This means that the result of an RBF applied to a new data point will be close to 0 unless the new point is near to the point around which the RBF was applied. That is, the application of the radial basis functions will pick out the nearest point, and its regression coefficient will dominate. The result will be a form of nearest neighbor interpolation , where predictions are made by simply using the prediction of the nearest observed data point, possibly interpolating between multiple nearby data points when they are all similar distances away. This type of nearest neighbor method for prediction is often considered diametrically opposed to the type of prediction used in standard linear regression: But in fact, the transformations that can be applied to the explanatory variables in a linear predictor function are so powerful that even the nearest neighbor method can be implemented as a type of linear regression.  It is even possible to fit some functions that appear non-linear in the coefficients by transforming the coefficients into new coefficients that do appear linear. For example, a function of the form    a  +    b  2    x   i  1     +    c    x   i  2         a     superscript  b  2    subscript  x    i  1         c    subscript  x    i  2       a+b^{2}x_{i1}+\sqrt{c}x_{i2}   for coefficients    a  ,  b  ,  c     a  b  c    a,b,c   could be transformed into the appropriate linear function by applying the substitutions       b  â€²   =   b  2    ,    c  â€²   =   c     ,     formulae-sequence     superscript  b  normal-â€²    superscript  b  2       superscript  c  normal-â€²     c      b^{\prime}=b^{2},c^{\prime}=\sqrt{c},   leading to     a  +    b  â€²    x   i  1     +    c  â€²    x   i  2      ,      a     superscript  b  normal-â€²    subscript  x    i  1        superscript  c  normal-â€²    subscript  x    i  2       a+b^{\prime}x_{i1}+c^{\prime}x_{i2},   which is linear. Linear regression and similar techniques could be applied and will often still find the optimal coefficients, but their error estimates and such will be wrong.  The explanatory variables may be of any type : real-valued , binary , categorical , etc. The main distinction is between continuous variables (e.g. income, age, blood pressure , etc.) and discrete variables (e.g. sex, race, political party, etc.). Discrete variables referring to more than two possible choices are typically coded using dummy variables (or indicator variables ), i.e. separate explanatory variables taking the value 0 or 1 are created for each possible value of the discrete variable, with a 1 meaning "variable does have the given value" and a 0 meaning "variable does not have the given value". For example, a four-way discrete variable of blood type with the possible values "A, B, AB, O" would be converted to separate two-way dummy variables, "is-A, is-B, is-AB, is-O", where only one of them has the value 1 and all the rest have the value 0. This allows for separate regression coefficients to be matched for each possible value of the discrete variable.  Note that, for K categories, not all K dummy variables are independent of each other. For example, in the above blood type example, only three of the four dummy variables are independent, in the sense that once the values of three of the variables are known, the fourth is automatically determined. Thus, it's really only necessary to encode three of the four possibilities as dummy variables, and in fact if all four possibilities are encoded, the overall model becomes non- identifiable . This causes problems for a number of methods, such as the simple closed-form solution used in linear regression. The solution is either to avoid such cases by eliminating one of the dummy variables, and/or introduce a regularization constraint (which necessitates a more powerful, typically iterative, method for finding the optimal coefficients).  References  "  Category:Statistical models  Category:Machine learning   