   VEGAS algorithm      VEGAS algorithm   The VEGAS algorithm , due to G. P. Lepage, 1 is a method for reducing error in Monte Carlo simulations by using a known or approximate probability distribution function to concentrate the search in those areas of the integrand that make the greatest contribution to the final integral .  The VEGAS algorithm is based on importance sampling . It samples points from the probability distribution described by the function    |  f  |      f    |f|   , so that the points are concentrated in the regions that make the largest contribution to the integral.  In general, if the Monte Carlo integral of   f   f   f   is sampled with points distributed according to a probability distribution described by the function   g   g   g   , we obtain an estimate     E  g    (  f  ;  N  )        subscript  E  g    f  N     E_{g}(f;N)   ,        E  g    (  f  ;  N  )    =    1  N     ∑  i  N      f   (   x  i   )    /  g    (   x  i   )             subscript  E  g    f  N        1  N     superscript   subscript   i   N         f   subscript  x  i    g    subscript  x  i        E_{g}(f;N)={1\over N}\sum_{i}^{N}{f(x_{i})}/g(x_{i})   .  The variance of the new estimate is then        𝑉𝑎𝑟  g    (  f  ;  N  )    =   𝑉𝑎𝑟   (   f  /  g   ;  N  )           subscript  𝑉𝑎𝑟  g    f  N      𝑉𝑎𝑟     f  g   N      \mathit{Var}_{g}(f;N)=\mathit{Var}(f/g;N)     where    𝑉𝑎𝑟   (  f  ;  N  )       𝑉𝑎𝑟   f  N     \mathit{Var}(f;N)   is the variance of the original estimate,     𝑉𝑎𝑟   (  f  ;  N  )    =    E   (   f  2   ;  N  )    -    (   E   (  f  ;  N  )    )   2          𝑉𝑎𝑟   f  N        E    superscript  f  2   N     superscript    E   f  N    2      \mathit{Var}(f;N)=E(f^{2};N)-(E(f;N))^{2}   .  If the probability distribution is chosen as    g  =     |  f  |   /  I    (   |  f  |   )        g        f   I     f      g=|f|/I(|f|)   then it can be shown that the variance     𝑉𝑎𝑟  g    (  f  ;  N  )        subscript  𝑉𝑎𝑟  g    f  N     \mathit{Var}_{g}(f;N)   vanishes, and the error in the estimate will be zero. In practice it is not possible to sample from the exact distribution g for an arbitrary function, so importance sampling algorithms aim to produce efficient approximations to the desired distribution.  The VEGAS algorithm approximates the exact distribution by making a number of passes over the integration region while histogramming the function f. Each histogram is used to define a sampling distribution for the next pass. Asymptotically this procedure converges to the desired distribution. In order to avoid the number of histogram bins growing like    K  d     superscript  K  d    K^{d}   with dimension d the probability distribution is approximated by a separable function     g   (   x  1   ,   x  2   ,  …  )    =    g  1    (   x  1   )    g  2    (   x  2   )   ⋯         g    subscript  x  1    subscript  x  2   normal-…       subscript  g  1    subscript  x  1    subscript  g  2    subscript  x  2   normal-⋯     g(x_{1},x_{2},\ldots)=g_{1}(x_{1})g_{2}(x_{2})\cdots   so that the number of bins required is only Kd . This is equivalent to locating the peaks of the function from the projections of the integrand onto the coordinate axes. The efficiency of VEGAS depends on the validity of this assumption. It is most efficient when the peaks of the integrand are well-localized. If an integrand can be rewritten in a form which is approximately separable this will increase the efficiency of integration with VEGAS.  See also   Las Vegas algorithm  Monte Carlo integration   References   G.P. Lepage, A New Algorithm for Adaptive Multidimensional Integration, Journal of Computational Physics 27, 192-203, (1978)  G.P. Lepage, VEGAS: An Adaptive Multi-dimensional Integration Program, Cornell preprint CLNS 80-447, March 1980  The GNU Scientific Library provides VEGAS routines   "  Category:Monte Carlo methods  Category:Computational physics  Category:Statistical algorithms  Category:Variance reduction     ↩     