   Extended Kalman filter      Extended Kalman filter   In estimation theory , the extended Kalman filter (EKF) is the nonlinear version of the Kalman filter which linearizes about an estimate of the current mean and covariance . In the case of well defined transition models, the EKF has been considered 1 the de facto standard in the theory of nonlinear state estimation, navigation systems and GPS . 2  History  The papers establishing the mathematical foundations of Kalman type filters were published between 1959 and 1961. 3 4 5 The Kalman Filter is the optimal estimate for linear system models with additive independent white noise in both the transition and the measurement systems. Unfortunately, in engineering, most systems are nonlinear , so some attempt was immediately made to apply this filtering method to nonlinear systems. Most of this work was done at NASA Ames . 6 7 The EKF adapted techniques from calculus, namely multivariate Taylor Series expansions, to linearize a model about a working point. If the system model (as described below) is not well known or is inaccurate, then Monte Carlo methods , especially particle filters , are employed for estimation. Monte Carlo techniques predate the existence of the EKF but are more computationally expensive for any moderately dimensioned state-space .  Formulation  In the extended Kalman filter, the state transition and observation models need not be linear functions of the state but may instead be differentiable functions.       𝒙  k   =    f   (   𝒙   k  -  1    ,   𝒖   k  -  1    )    +   𝒘   k  -  1          subscript  𝒙  k       f    subscript  𝒙    k  1     subscript  𝒖    k  1       subscript  𝒘    k  1       \boldsymbol{x}_{k}=f(\boldsymbol{x}_{k-1},\boldsymbol{u}_{k-1})+\boldsymbol{w}%
 _{k-1}          𝒛  k   =    h   (   𝒙  k   )    +   𝒗  k         subscript  𝒛  k       h   subscript  𝒙  k     subscript  𝒗  k      \boldsymbol{z}_{k}=h(\boldsymbol{x}_{k})+\boldsymbol{v}_{k}     Where w k-1 and v k are the process and observation noises which are both assumed to be zero mean multivariate Gaussian noises with covariance  Q k and R k respectively. u k is the control vector.  The function f can be used to compute the predicted state from the previous estimate and similarly the function h can be used to compute the predicted measurement from the predicted state. However, f and h cannot be applied to the covariance directly. Instead a matrix of partial derivatives (the Jacobian ) is computed.  At each time step, the Jacobian is evaluated with current predicted states. These matrices can be used in the Kalman filter equations. This process essentially linearizes the non-linear function around the current estimate.  Discrete-time predict and update equations  Predict      Predicted state estimate         𝒙  ^    k  |  k  -  1    =   f   (    𝒙  ^    k  -  1  |  k  -  1    ,   𝒖   k  -  1    )         subscript   normal-^  𝒙    fragments  k  normal-|  k   1      f    subscript   normal-^  𝒙    fragments  k   1  normal-|  k   1     subscript  𝒖    k  1        \hat{\boldsymbol{x}}_{k|k-1}=f(\hat{\boldsymbol{x}}_{k-1|k-1},\boldsymbol{u}_{%
 k-1})        Predicted covariance estimate        𝑷   k  |  k  -  1    =     𝑭   k  -  1     𝑷   k  -  1  |  k  -  1     𝑭   k  -  1   ⊤    +   𝑸   k  -  1          subscript  𝑷   fragments  k  normal-|  k   1         subscript  𝑭    k  1     subscript  𝑷   fragments  k   1  normal-|  k   1     superscript   subscript  𝑭    k  1    top     subscript  𝑸    k  1       \boldsymbol{P}_{k|k-1}={{\boldsymbol{F}_{k-1}}}\boldsymbol{P}_{k-1|k-1}{{%
 \boldsymbol{F}_{k-1}^{\top}}}+\boldsymbol{Q}_{k-1}        Update      Innovation or measurement residual         𝒚  ~   k   =    𝒛  k   -   h   (    𝒙  ^    k  |  k  -  1    )          subscript   normal-~  𝒚   k      subscript  𝒛  k     h   subscript   normal-^  𝒙    fragments  k  normal-|  k   1        \tilde{\boldsymbol{y}}_{k}=\boldsymbol{z}_{k}-h(\hat{\boldsymbol{x}}_{k|k-1})        Innovation (or residual) covariance        𝑺  k   =     𝑯  k    𝑷   k  |  k  -  1     𝑯  k  ⊤    +   𝑹  k         subscript  𝑺  k        subscript  𝑯  k    subscript  𝑷   fragments  k  normal-|  k   1     superscript   subscript  𝑯  k   top     subscript  𝑹  k      \boldsymbol{S}_{k}={{\boldsymbol{H}_{k}}}\boldsymbol{P}_{k|k-1}{{\boldsymbol{H%
 }_{k}^{\top}}}+\boldsymbol{R}_{k}        Near-optimal Kalman gain        𝑲  k   =    𝑷   k  |  k  -  1     𝑯  k  ⊤    𝑺  k   -  1          subscript  𝑲  k      subscript  𝑷   fragments  k  normal-|  k   1     superscript   subscript  𝑯  k   top    superscript   subscript  𝑺  k     1       \boldsymbol{K}_{k}=\boldsymbol{P}_{k|k-1}{{\boldsymbol{H}_{k}^{\top}}}%
 \boldsymbol{S}_{k}^{-1}        Updated covariance estimate        𝑷   k  |  k    =    (   𝑰  -    𝑲  k    𝑯  k     )    𝑷   k  |  k  -  1          subscript  𝑷   fragments  k  normal-|  k        𝑰     subscript  𝑲  k    subscript  𝑯  k      subscript  𝑷   fragments  k  normal-|  k   1       \boldsymbol{P}_{k|k}=(\boldsymbol{I}-\boldsymbol{K}_{k}{{\boldsymbol{H}_{k}}})%
 \boldsymbol{P}_{k|k-1}        Updated state estimate         𝒙  ^    k  |  k    =     𝒙  ^    k  |  k  -  1    +    𝑲  k     𝒚  ~   k          subscript   normal-^  𝒙    fragments  k  normal-|  k       subscript   normal-^  𝒙    fragments  k  normal-|  k   1       subscript  𝑲  k    subscript   normal-~  𝒚   k       \hat{\boldsymbol{x}}_{k|k}=\hat{\boldsymbol{x}}_{k|k-1}+\boldsymbol{K}_{k}%
 \tilde{\boldsymbol{y}}_{k}        where the state transition and observation matrices are defined to be the following Jacobians       𝑭   k  -  1    =      ∂  f    ∂  𝒙    |      𝒙  ^    k  -  1  |  k  -  1    ,   𝒖   k  -  1           subscript  𝑭    k  1     evaluated-at      f     𝒙      subscript   normal-^  𝒙    fragments  k   1  normal-|  k   1     subscript  𝒖    k  1        {{\boldsymbol{F}_{k-1}}}=\left.\frac{\partial f}{\partial\boldsymbol{x}}\right%
 |_{\hat{\boldsymbol{x}}_{k-1|k-1},\boldsymbol{u}_{k-1}}          𝑯  k   =      ∂  h    ∂  𝒙    |     𝒙  ^    k  |  k  -  1          subscript  𝑯  k    evaluated-at      h     𝒙     subscript   normal-^  𝒙    fragments  k  normal-|  k   1       {{\boldsymbol{H}_{k}}}=\left.\frac{\partial h}{\partial\boldsymbol{x}}\right|_%
 {\hat{\boldsymbol{x}}_{k|k-1}}     Higher-order extended Kalman filters  The above recursion is a first-order extended Kalman filter (EKF). Higher order EKFs may be obtained by retaining more terms of the Taylor series expansions. For example, second and third order EKFs are described in . 8 However, higher order EKFs tend to only provide performance benefits when the measurement noise is small.  Non-additive noise formulation and equations  The typical formulation of the EKF involves the assumption of additive process and measurement noise. This assumption, however, is not necessary for EKF implementation. 9 Instead, consider a more general system of the form:       𝒙  k   =   f   (   𝒙   k  -  1    ,   𝒖   k  -  1    ,   𝒘   k  -  1    )         subscript  𝒙  k     f    subscript  𝒙    k  1     subscript  𝒖    k  1     subscript  𝒘    k  1        \boldsymbol{x}_{k}=f(\boldsymbol{x}_{k-1},\boldsymbol{u}_{k-1},\boldsymbol{w}_%
 {k-1})          𝒛  k   =   h   (   𝒙  k   ,   𝒗  k   )         subscript  𝒛  k     h    subscript  𝒙  k    subscript  𝒗  k       \boldsymbol{z}_{k}=h(\boldsymbol{x}_{k},\boldsymbol{v}_{k})     Where w k and v k are the process and observation noises which are both assumed to be zero mean multivariate Gaussian noises with covariance  Q k and R k respectively. Then the covariance prediction and innovation equations become       𝑷   k  |  k  -  1    =     𝑭   k  -  1     𝑷   k  -  1  |  k  -  1     𝑭   k  -  1   ⊤    +    𝑳   k  -  1     𝑸   k  -  1     𝑳   k  -  1   T          subscript  𝑷   fragments  k  normal-|  k   1         subscript  𝑭    k  1     subscript  𝑷   fragments  k   1  normal-|  k   1     superscript   subscript  𝑭    k  1    top       subscript  𝑳    k  1     subscript  𝑸    k  1     subscript   superscript  𝑳  T     k  1        \boldsymbol{P}_{k|k-1}={{{\boldsymbol{F}_{k-1}}}}{\boldsymbol{P}_{k-1|k-1}}{{{%
 \boldsymbol{F}_{k-1}^{\top}}}}{+}{\boldsymbol{L}_{k-1}}{\boldsymbol{Q}_{k-1}}{%
 \boldsymbol{L}^{T}_{k-1}}          𝑺  k   =     𝑯  k    𝑷   k  |  k  -  1     𝑯  k  ⊤    +    𝑴  k    𝑹  k    𝑴  k  T          subscript  𝑺  k        subscript  𝑯  k    subscript  𝑷   fragments  k  normal-|  k   1     superscript   subscript  𝑯  k   top       subscript  𝑴  k    subscript  𝑹  k    superscript   subscript  𝑴  k   T       \boldsymbol{S}_{k}={{\boldsymbol{H}_{k}}}{\boldsymbol{P}_{k|k-1}}{{\boldsymbol%
 {H}_{k}^{\top}}}{+}{\boldsymbol{M}_{k}}{\boldsymbol{R}_{k}}{\boldsymbol{M}_{k}%
 ^{T}}     where the matrices    𝑳   k  -  1      subscript  𝑳    k  1     \boldsymbol{L}_{k-1}   and    𝑴  k     subscript  𝑴  k    \boldsymbol{M}_{k}   are Jacobian matrices:       𝑳   k  -  1    =      ∂  f    ∂  𝒘    |      𝒙  ^    k  -  1  |  k  -  1    ,   𝒖   k  -  1           subscript  𝑳    k  1     evaluated-at      f     𝒘      subscript   normal-^  𝒙    fragments  k   1  normal-|  k   1     subscript  𝒖    k  1        {{\boldsymbol{L}_{k-1}}}=\left.\frac{\partial f}{\partial\boldsymbol{w}}\right%
 |_{\hat{\boldsymbol{x}}_{k-1|k-1},\boldsymbol{u}_{k-1}}          𝑴  k   =      ∂  h    ∂  𝒗    |     𝒙  ^    k  |  k  -  1          subscript  𝑴  k    evaluated-at      h     𝒗     subscript   normal-^  𝒙    fragments  k  normal-|  k   1       {{\boldsymbol{M}_{k}}}=\left.\frac{\partial h}{\partial\boldsymbol{v}}\right|_%
 {\hat{\boldsymbol{x}}_{k|k-1}}     The predicted state estimate and measurement residual are evaluated at the mean of the process and measurement noise terms, which is assumed to be zero. Otherwise, the non-additive noise formulation is implemented in the same manner as the additive noise EKF .  Continuous-time extended Kalman filter  Model       𝐱  ˙    (  t  )        normal-˙  𝐱   t    \displaystyle\dot{\mathbf{x}}(t)    Initialize        𝐱  ^    (   t  0   )    =   E   [   𝐱   (   t  0   )    ]   ,  𝐏   (   t  0   )    =   V  a  r   [   𝐱   (   t  0   )    ]             normal-^  𝐱    subscript  t  0      E   delimited-[]    𝐱   subscript  t  0     ,  𝐏   subscript  t  0           V  a  r   delimited-[]    𝐱   subscript  t  0         \hat{\mathbf{x}}(t_{0})=E\bigl[\mathbf{x}(t_{0})\bigr]\text{, }\mathbf{P}(t_{0%
 })=Var\bigl[\mathbf{x}(t_{0})\bigr]    Predict-Update        𝐱  ^   ˙    (  t  )        normal-˙   normal-^  𝐱    t    \displaystyle\dot{\hat{\mathbf{x}}}(t)   Unlike discrete-time extended Kalman filter, the prediction and update steps are coupled in continuous-time extended Kalman filter.  Discrete-time extended Kalman filter  Most physical systems are represented as continuous-time models while discrete-time measurements are frequently taken for state estimation via a digital processor. Therefore, the system model and measurement model are given by       𝐱  ˙    (  t  )        normal-˙  𝐱   t    \displaystyle\dot{\mathbf{x}}(t)   where     𝐱  k   =   𝐱   (   t  k   )         subscript  𝐱  k     𝐱   subscript  t  k      \mathbf{x}_{k}=\mathbf{x}(t_{k})   .  Initialize         𝐱  ^    0  |  0    =   E   [   𝐱   (   t  0   )    ]     ,    𝐏   0  |  0    =   V  a  r   [   𝐱   (   t  0   )    ]        formulae-sequence     subscript   normal-^  𝐱    fragments  0  normal-|  0      E   delimited-[]    𝐱   subscript  t  0          subscript  𝐏   fragments  0  normal-|  0      V  a  r   delimited-[]    𝐱   subscript  t  0         \hat{\mathbf{x}}_{0|0}=E\bigl[\mathbf{x}(t_{0})\bigr],\mathbf{P}_{0|0}=Var%
 \bigl[\mathbf{x}(t_{0})\bigr]    Predict       {          𝐱  ^   ˙    (  t  )    =   f   (    𝐱  ^    (  t  )    ,   𝐮   (  t  )    )     ,            𝐏  ˙    (  t  )    =    𝐅   (  t  )   𝐏   (  t  )    +   𝐏   (  t  )   𝐅    (  t  )   ⊤    +   𝐐   (  t  )      ,         with   {         𝐱  ^    (   t   k  -  1    )    =    𝐱  ^    k  -  1  |  k  -  1     ,           𝐏   (   t   k  -  1    )    =   𝐏   k  -  1  |  k  -  1     ,             cases       normal-˙   normal-^  𝐱    t     f      normal-^  𝐱   t     𝐮  t      otherwise       normal-˙  𝐏   t       𝐅  t  𝐏  t     𝐏  t  𝐅   superscript  t  top      𝐐  t     otherwise     with   cases       normal-^  𝐱    subscript  t    k  1      subscript   normal-^  𝐱    fragments  k   1  normal-|  k   1     otherwise      𝐏   subscript  t    k  1      subscript  𝐏   fragments  k   1  normal-|  k   1     otherwise      \displaystyle\begin{cases}\dot{\hat{\mathbf{x}}}(t)=f\bigl(\hat{\mathbf{x}}(t)%
 ,\mathbf{u}(t)\bigr),\\
 \dot{\mathbf{P}}(t)=\mathbf{F}(t)\mathbf{P}(t)+\mathbf{P}(t)\mathbf{F}(t)^{%
 \top}+\mathbf{Q}(t),\end{cases}\qquad\text{with }\begin{cases}\hat{\mathbf{x}}%
 (t_{k-1})=\hat{\mathbf{x}}_{k-1|k-1},\\
 \mathbf{P}(t_{k-1})=\mathbf{P}_{k-1|k-1},\end{cases}   where       𝐅   (  t  )    =      ∂  f    ∂  𝐱    |      𝐱  ^    (  t  )    ,   𝐮   (  t  )            𝐅  t    evaluated-at      f     𝐱        normal-^  𝐱   t     𝐮  t       \mathbf{F}(t)=\left.\frac{\partial f}{\partial\mathbf{x}}\right|_{\hat{\mathbf%
 {x}}(t),\mathbf{u}(t)}    Update       𝐊  k   =    𝐏   k  |  k  -  1     𝐇  k  ⊤     (     𝐇  k    𝐏   k  |  k  -  1     𝐇  k  ⊤    +   𝐑  k    )    -  1          subscript  𝐊  k      subscript  𝐏   fragments  k  normal-|  k   1     superscript   subscript  𝐇  k   top    superscript       subscript  𝐇  k    subscript  𝐏   fragments  k  normal-|  k   1     superscript   subscript  𝐇  k   top     subscript  𝐑  k      1       \mathbf{K}_{k}=\mathbf{P}_{k|k-1}\mathbf{H}_{k}^{\top}\bigl(\mathbf{H}_{k}%
 \mathbf{P}_{k|k-1}\mathbf{H}_{k}^{\top}+\mathbf{R}_{k}\bigr)^{-1}           𝐱  ^    k  |  k    =     𝐱  ^    k  |  k  -  1    +    𝐊  k    (    𝐳  k   -   h   (    𝐱  ^    k  |  k  -  1    )     )          subscript   normal-^  𝐱    fragments  k  normal-|  k       subscript   normal-^  𝐱    fragments  k  normal-|  k   1       subscript  𝐊  k      subscript  𝐳  k     h   subscript   normal-^  𝐱    fragments  k  normal-|  k   1          \hat{\mathbf{x}}_{k|k}=\hat{\mathbf{x}}_{k|k-1}+\mathbf{K}_{k}\bigl(\mathbf{z}%
 _{k}-h(\hat{\mathbf{x}}_{k|k-1})\bigr)          𝐏   k  |  k    =    (   𝐈  -    𝐊  k    𝐇  k     )    𝐏   k  |  k  -  1          subscript  𝐏   fragments  k  normal-|  k        𝐈     subscript  𝐊  k    subscript  𝐇  k      subscript  𝐏   fragments  k  normal-|  k   1       \mathbf{P}_{k|k}=(\mathbf{I}-\mathbf{K}_{k}\mathbf{H}_{k})\mathbf{P}_{k|k-1}   where       𝐇  k   =      ∂  h    ∂  𝐱    |     𝐱  ^    k  |  k  -  1          subscript  H  k    evaluated-at      h     x     subscript   normal-^  x    fragments  k  normal-|  k   1       \textbf{H}_{k}=\left.\frac{\partial h}{\partial\textbf{x}}\right|_{\hat{%
 \textbf{x}}_{k|k-1}}   The update equations are identical to those of discrete-time extended Kalman filter.  Disadvantages of the extended Kalman filter  Unlike its linear counterpart, the extended Kalman filter in general is not an optimal estimator (of course it is optimal if the measurement and the state transition model are both linear, as in that case the extended Kalman filter is identical to the regular one). In addition, if the initial estimate of the state is wrong, or if the process is modeled incorrectly, the filter may quickly diverge, owing to its linearization. Another problem with the extended Kalman filter is that the estimated covariance matrix tends to underestimate the true covariance matrix and therefore risks becoming inconsistent in the statistical sense without the addition of "stabilising noise".  Having stated this, the extended Kalman filter can give reasonable performance, and is arguably the de facto standard in navigation systems and GPS.  Robust extended Kalman filters  The extended Kalman filter arises by linearizing the signal model about the current state estimate and using the linear Kalman filter to predict the next estimate. This attempts to produce a locally optimal filter, however, it is not necessarily stable because the solutions of the underlying Riccati equation are not guaranteed to be positive definite. One way of improving performance is the faux algebraic Riccati technique 10 which trades off optimality for stability. The familiar structure of the extended Kalman filter is retained but stability is achieved by selecting a positive definite solution to a faux algebraic Riccati equation for the gain design.  Another way of improving extended Kalman filter performance is to employ the H-infinity results from robust control. Robust filters are obtained by adding a positive definite term to the design Riccati equation. 11 The additional term is parametrized by a scalar which the designer may tweak to achieve a trade-off between mean-square-error and peak error performance criteria.  Unscented Kalman filters  A nonlinear Kalman filter which shows promise as an improvement over the EKF is the unscented Kalman filter (UKF). In the UKF, the probability density is approximated by a deterministic sampling of points which represent the underlying distribution as a Gaussian . The nonlinear transformation of these points are intended to be an estimation of the posterior distribution, the moments of which can then be derived from the transformed samples. The transformation is known as the unscented transform . The UKF tends to be more robust and more accurate than the EKF in its estimation of error in all the directions.   "The extended Kalman filter (EKF) is probably the most widely used estimation algorithm for nonlinear systems. However, more than 35 years of experience in the estimation community has shown that is difficult to implement, difficult to tune, and only reliable for systems that are almost linear on the time scale of the updates. Many of these difficulties arise from its use of linearization." 12   A recent paper includes simulation results which suggest that some published variants of the UKF fail to be as accurate as the Second Order Extended Kalman Filter (SOEKF), called also the augmented Kalman filter. 13 The SOEKF predates the UKF by approximately 35 years with the moment dynamics first described by Bass et al. 14 The difficulty in implementing any Kalman-type filters for nonlinear state transitions stems from the numerical stability issues required for precision, 15 however the UKF does not escape this difficulty in that it uses linearization as well, namely linear regression. The stability issues for the UKF generally stem from the numerical approximation to the square root of the covariance matrix, whereas the stability issues for both the EKF and the SOEKF stem from possible issues in the Taylor Series approximation along the trajectory.  Invariant extended Kalman filter  The invariant extended Kalman filter (IEKF) is a modified version of the EKF for nonlinear systems possessing symmetries (or invariances ). It combines the advantages of both the EKF and the recently introduced symmetry-preserving filters . Indeed, instead of using a linear correction term based on a linear output error, it uses a geometrically adapted correction term based on an invariant output error; in the same way the gain matrix is not updated from a linear state error, but from an invariant state error. The main benefit is that the gain and covariance equations converge to constant values on a much bigger set of trajectories than equilibrium points as it is the case for the EKF, which results in a better convergence of the estimation.  See also   Kalman filter  Ensemble Kalman filter  Fast Kalman filter  Invariant extended Kalman filter  Moving horizon estimation  Particle filter  Unscented Kalman filter   References  Further reading              External links   Position estimation of a differential-wheel robot based on odometry and landmarks   "  Category:Estimation theory  Category:Nonlinear filters  Category:Robot control     ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩   Gustafsson, F.; Hendeby, G.; , "Some Relations Between Extended and Unscented Kalman Filters," Signal Processing, IEEE Transactions on , vol.60, no.2, pp.545-555, Feb. 2012 ↩  R. Bass, V. Norum, and L. Schwartz, “Optimal multichannel nonlinear filtering(optimal multichannel nonlinear filtering problem of minimum variance estimation of state of n- dimensional nonlinear system subject to stochastic disturbance),” J. Mathematical Analysis and Applications,vol. 16, pp. 152–164, 1966 ↩  M. Grewal and A. Andrews, Kalman Filtering : Theory and Practice Using MATLAB, 2nd ed. Wiley-Interscience, Jan. 2001. ↩     