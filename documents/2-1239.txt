   Channel capacity      Channel capacity   In electrical engineering , computer science and information theory , channel capacity is the tight upper bound on the rate at which information can be reliably transmitted over a communications channel . By the noisy-channel coding theorem , the channel capacity of a given channel is the limiting information rate (in units of information per unit time) that can be achieved with arbitrarily small error probability. 1 2  Information theory , developed by Claude E. Shannon during World War II , defines the notion of channel capacity and provides a mathematical model by which one can compute it. The key result states that the capacity of the channel, as defined above, is given by the maximum of the mutual information between the input and output of the channel, where the maximization is with respect to the input distribution. 3  Formal definition  (Figure)  Comm Channel.svg   Let   X   X   X   and   Y   Y   Y   be the random variables representing the input and output of the channel, respectively. Let     p   Y  |  X     (  y  |  x  )      fragments   subscript  p   fragments  Y  normal-|  X     fragments  normal-(  y  normal-|  x  normal-)     p_{Y|X}(y|x)   be the conditional distribution function of   Y   Y   Y   given   X   X   X   , which is an inherent fixed property of the communications channel. Then the choice of the marginal distribution      p  X    (  x  )        subscript  p  X   x    p_{X}(x)   completely determines the joint distribution      p   X  ,  Y     (  x  ,  y  )        subscript  p   X  Y     x  y     p_{X,Y}(x,y)   due to the identity        p    X  ,  Y     (  x  ,  y  )   =   p   Y  |  X     (  y  |  x  )    p  X    (  x  )      fragments   subscript  p   X  Y     fragments  normal-(  x  normal-,  y  normal-)     subscript  p   fragments  Y  normal-|  X     fragments  normal-(  y  normal-|  x  normal-)    subscript  p  X    fragments  normal-(  x  normal-)     \ p_{X,Y}(x,y)=p_{Y|X}(y|x)\,p_{X}(x)     which, in turn, induces a mutual information     I   (  X  ;  Y  )       I   X  Y     I(X;Y)   . The channel capacity is defined as       C   =    sup    p  X    (  x  )      I   (  X  ;  Y  )         C    subscript  supremum     subscript  p  X   x      I   X  Y       \ C=\sup_{p_{X}(x)}I(X;Y)\,     where the supremum is taken over all possible choices of     p  X    (  x  )        subscript  p  X   x    p_{X}(x)   .  Shannon capacity of a graph  If G is an undirected graph , it can be used to define a communications channel in which the symbols are the graph vertices, and two codewords may be confused with each other if their symbols in each position are equal or adjacent. The computational complexity of finding the Shannon capacity of such a channel remains open, but it can be upper bounded by another important graph invariant, the Lov√°sz number . 4  Noisy-channel coding theorem  The noisy-channel coding theorem states that for any Œµ > 0 and for any transmission rate  R less than the channel capacity C , there is an encoding and decoding scheme transmitting data at rate R whose error probability is less than Œµ, for a sufficiently large block length. Also, for any rate greater than the channel capacity, the probability of error at the receiver goes to one as the block length goes to infinity.  Example application  An application of the channel capacity concept to an additive white Gaussian noise (AWGN) channel with B Hz bandwidth and signal-to-noise ratio  S/N is the Shannon‚ÄìHartley theorem :      C  =   B    log  2    (   1  +   S  N    )         C    B    subscript   2     1    S  N        C=B\log_{2}\left(1+\frac{S}{N}\right)     C is measured in bits per second if the logarithm is taken in base 2, or nats per second if the natural logarithm is used, assuming B is in hertz ; the signal and noise powers S and N are measured in watts or volts 2 , so the signal-to-noise ratio here is expressed as a power ratio, not in decibels (dB); since figures are often cited in dB, a conversion may be needed. For example, 30 dB is a power ratio of     10   30  /  10    =   10  3   =  1000         superscript  10    30  10     superscript  10  3        1000     10^{30/10}=10^{3}=1000   . '''  Channel capacity in wireless communications  This section 5 focuses on the single-antenna, point-to-point scenario. For channel capacity in systems with multiple antennas, see the article on MIMO .  AWGN channel  If the average received power is    P  ¬Ø     normal-¬Ø  P    \bar{P}   [W] and the noise power spectral density is    N  0     subscript  N  0    N_{0}   [W/Hz], the AWGN channel capacity is       C   a  w  g  n    =   W    log  2    (   1  +    P  ¬Ø     N  0   W     )          subscript  C    a  w  g  n      W    subscript   2     1     normal-¬Ø  P      subscript  N  0   W         C_{awgn}=W\log_{2}\left(1+\frac{\bar{P}}{N_{0}W}\right)   [bits/s],  where     P  ¬Ø     N  0   W        normal-¬Ø  P      subscript  N  0   W     \frac{\bar{P}}{N_{0}W}   is the received signal-to-noise ratio (SNR). This result is known as the Shannon‚ÄìHartley theorem . 6  When the SNR is large (SNR >> 0 dB), the capacity    C  ‚âà   W    log  2     P  ¬Ø     N  0   W          C    W    subscript   2      normal-¬Ø  P      subscript  N  0   W        C\approx W\log_{2}\frac{\bar{P}}{N_{0}W}   is logarithmic in power and approximately linear in bandwidth. This is called the bandwidth-limited regime .  When the SNR is small (SNR C\approx \frac{\bar{P}}{N_0} \log_2 e is linear in power but insensitive to bandwidth. This is called the power-limited regime .  The bandwidth-limited regime and power-limited regime are illustrated in the figure.  (Figure)  AWGN channel capacity with the power-limited regime and bandwidth-limited regime indicated. Here,      P  ¬Ø    N  o    =   10  6          normal-¬Ø  P    subscript  N  o     superscript  10  6     \frac{\bar{P}}{N_{o}}=10^{6}   .   Frequency-selective channel  The capacity of the frequency-selective channel is given by so-called water filling power allocation,        C   N  c    =    ‚àë   n  =  0     N  c   -  1      log  2    (   1  +     P  n  *     |    h  ¬Ø   n   |   2     N  0     )      ,       subscript  C   subscript  N  c      superscript   subscript     n  0       subscript  N  c   1      subscript   2     1       superscript   subscript  P  n      superscript     subscript   normal-¬Ø  h   n    2     subscript  N  0         C_{N_{c}}=\sum_{n=0}^{N_{c}-1}\log_{2}\left(1+\frac{P_{n}^{*}|\bar{h}_{n}|^{2}%
 }{N_{0}}\right),     where     P  n  *   =   max   (   (    1  Œª   -    N  0     |    h  ¬Ø   n   |   2     )   ,  0  )         superscript   subscript  P  n           1  Œª      subscript  N  0    superscript     subscript   normal-¬Ø  h   n    2     0     P_{n}^{*}=\max\left(\left(\frac{1}{\lambda}-\frac{N_{0}}{|\bar{h}_{n}|^{2}}%
 \right),0\right)   and     |    h  ¬Ø   n   |   2     superscript     subscript   normal-¬Ø  h   n    2    |\bar{h}_{n}|^{2}   is the gain of subchannel   n   n   n   , with   Œª   Œª   \lambda   chosen to meet the power constraint.  Slow-fading channel  In a slow-fading channel , where the coherence time is greater than the latency requirement, there is no definite capacity as the maximum rate of reliable communications supported by the channel,     log  2    (   1  +     |  h  |   2   S  N  R    )       subscript   2     1     superscript    h   2   S  N  R      \log_{2}(1+|h|^{2}SNR)   , depends on the random channel gain     |  h  |   2     superscript    h   2    |h|^{2}   , which is unknown to the transmitter. If the transmitter encodes data at rate   R   R   R   [bits/s/Hz], there is a non-zero probability that the decoding error probability cannot be made arbitrarily small,      p   o  u  t      subscript  p    o  u  t     p_{out}   such that the outage probability   œµ   œµ   \epsilon   is less than   œµ   œµ   \epsilon   . This value is known as the    ùîº   (    log  2    (   1  +     |  h  |   2   S  N  R    )    )       ùîº    subscript   2     1     superscript    h   2   S  N  R       \mathbb{E}(\log_{2}(1+|h|^{2}SNR))   -outage capacity.  Fast-fading channel  In a fast-fading channel , where the latency requirement is greater than the coherence time and the codeword length spans many coherence periods, one can average over many independent channel fades by coding over a large number of coherence time intervals. Thus, it is possible to achieve a reliable rate of communication of $\mathbb{E}(\log_2 (1+|h|^2 SNR))$ [bits/s/Hz] and it is meaningful to speak of this value as the capacity of the fast-fading channel.  See also   Bandwidth (computing)  Bandwidth (signal processing)  Bit rate  Code rate  Error exponent  Nyquist rate  Negentropy  Redundancy  Sender , Encoder , Decoder , Receiver  Shannon‚ÄìHartley theorem  Spectral efficiency  Throughput   Advanced Communication Topics   MIMO  Cooperative diversity   External links    AWGN Channel Capacity with various constraints on the channel input (interactive demonstration)   References  "  Category:Information theory  Category:Telecommunication theory  Category:Television terminology     ‚Ü©  ‚Ü©  ‚Ü©  . ‚Ü©  ‚Ü©  ‚Ü©     