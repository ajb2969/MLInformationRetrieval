


Spectral clustering




Spectral clustering

In multivariate statistics and the clustering of data, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.
In application to image segmentation, spectral clustering is known as segmentation-based object categorization.
Algorithms
Given an enumerated set of data points, the similarity matrix may be defined as a symmetric matrix 
 
 
 
 , where 
 
 
 
  represents a measure of the similarity between data points with indexes 
 
 
 
  and 
 
 
 
 .
One spectral clustering technique is the normalized cuts algorithm or Shi–Malik algorithm introduced by Jianbo Shi and Jitendra Malik,1 commonly used for image segmentation. It partitions points into two sets 
 
 
 
  based on the eigenvector

 
  corresponding to the second-smallest eigenvalue of the symmetric normalized Laplacian defined as


 
 ,
where 
 
 
 
  is the diagonal matrix



A mathematically equivalent algorithm 2 takes the eigenvector corresponding to the largest eigenvalue of the random walk normalized Laplacian matrix 
 
 
 
 .
Another possibility is to use the Laplacian matrix defined as


 
  rather than the symmetric normalized Laplacian matrix.
Partitioning may be done in various ways, such as by computing the median 
 
 
 
  of the components of the second smallest eigenvector 
 
 
 
 , and placing all points whose component in 
 
 
 
  is greater than 
 
 
 
  in 
 
 
 
 , and the rest in 
 
 
 
 . The algorithm can be used for hierarchical clustering by repeatedly partitioning the subsets in this fashion.
Alternatively to computing just one eigenvector, k eigenvectors for some k, are computed, and then another algorithm (e.g. k-means clustering) is used to cluster points by their respective k components in these eigenvectors.
The efficiency of spectral clustering may be improved if the solution to the corresponding eigenvalue problem is performed in a matrix-free fashion, i.e., without explicitly manipulating or even computing the similarity matrix, as, e.g., in the Lanczos algorithm.
For large-sized graphs, the second eigenvalue of the (normalized) graph Laplacian matrix is often ill-conditioned, leading to slow convergence of iterative eigenvalue solvers. Preconditioning is a key technology accelerating the convergence, e.g., in the matrix-free LOBPCG method. Spectral clustering has been successfully applied on large graphs by first identifying their community structure, and then clustering communities.3
Spectral clustering is closely related to Nonlinear dimensionality reduction, and dimension reduction techniques such as locally-linear embedding can be used to reduce errors from noise or outliers.4
Relationship with k-means
The kernel k-means problem is an extension of the k-means problem where the input data points are mapped non-linearly into a higher-dimensional feature space via a kernel function 
 
 
 
 . The weighted kernel k-means problem further extends this problem by defining a weight 
 
 
 
  for each cluster as the reciprocal of the number of elements in the cluster,


 
  Suppose 
 
 
 
  is a matrix of the normalizing coefficients for each point for each cluster 
 
 
 
  if 
 
 
 
  and zero otherwise. Suppose 
 
 
 
  is the kernel matrix for all points. The weighted kernel k-means problem with n points and k clusters is given as,


 
  such that,





 
  such that 
 
 
 
 . In addition, there are identity constrains on 
 
 
 
  given by,


 
  where 
 
 
 
  represents a vector of ones.


 
  This problem can be recast as,


 
  This problem is equivalent to the spectral clustering problem when the identity constraints on 
 
 
 
  are relaxed. In particular, the weighted kernel k-means problem can be reformulated as a spectral clustering (graph partitioning) problem and vice versa. The output of the algorithms are eigenvectors which do not satisfy the identity requirements for indicator variables defined by 
 
 
 
 . Hence, post-processing of the eigenvectors is required for the equivalence between the problems.5 Transforming the spectral clustering problem into a weighted kernel k-means problem greatly reduces the computational burden.6
See also

Affinity propagation
Kernel principal component analysis
Cluster analysis
Spectral graph theory

References


"
Category:Data clustering algorithms Category:Algebraic graph theory



Jianbo Shi and Jitendra Malik, "Normalized Cuts and Image Segmentation", IEEE Transactions on PAMI, Vol. 22, No. 8, Aug 2000.
Marina Meilă & Jianbo Shi, "Learning Segmentation by Random Walks", Neural Information Processing Systems 13 (NIPS 2000), 2001, pp. 873–879.








