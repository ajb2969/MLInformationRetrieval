   Spectral clustering      Spectral clustering   In multivariate statistics and the clustering of data, spectral clustering techniques make use of the spectrum ( eigenvalues ) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.  In application to image segmentation, spectral clustering is known as segmentation-based object categorization .  Algorithms  Given an enumerated set of data points, the similarity matrix may be defined as a symmetric matrix   A   A   A   , where     A   i  j    ‚â•  0       subscript  A    i  j    0    A_{ij}\geq 0   represents a measure of the similarity between data points with indexes   i   i   i   and   j   j   j   .  One spectral clustering technique is the normalized cuts algorithm or Shi‚ÄìMalik algorithm introduced by Jianbo Shi and Jitendra Malik, 1 commonly used for image segmentation . It partitions points into two sets    (   B  1   ,   B  2   )      subscript  B  1    subscript  B  2     (B_{1},B_{2})   based on the eigenvector    v   v   v   corresponding to the second-smallest eigenvalue of the symmetric normalized Laplacian defined as       L   n  o  r  m    :=   I  -    D   -   1  /  2     A   D   -   1  /  2          assign   superscript  L    n  o  r  m      I     superscript  D      1  2     A   superscript  D      1  2         L^{norm}:=I-D^{-1/2}AD^{-1/2}   ,  where   D   D   D   is the diagonal matrix        D   i  i    =    ‚àë  j    A   i  j      .       subscript  D    i  i      subscript   j    subscript  A    i  j       D_{ii}=\sum_{j}A_{ij}.     A mathematically equivalent algorithm 2 takes the eigenvector corresponding to the largest eigenvalue of the random walk normalized Laplacian matrix    P  =    D   -  1    A       P     superscript  D    1    A     P=D^{-1}A   .  Another possibility is to use the Laplacian matrix defined as      L  :=   D  -  A      assign  L    D  A     L:=D-A   rather than the symmetric normalized Laplacian matrix.  Partitioning may be done in various ways, such as by computing the median   m   m   m   of the components of the second smallest eigenvector   v   v   v   , and placing all points whose component in   v   v   v   is greater than   m   m   m   in    B  1     subscript  B  1    B_{1}   , and the rest in    B  2     subscript  B  2    B_{2}   . The algorithm can be used for hierarchical clustering by repeatedly partitioning the subsets in this fashion.  Alternatively to computing just one eigenvector, k  eigenvectors for some k , are computed, and then another algorithm (e.g. k-means clustering ) is used to cluster points by their respective k components in these eigenvectors.  The efficiency of spectral clustering may be improved if the solution to the corresponding eigenvalue problem is performed in a matrix-free fashion , i.e., without explicitly manipulating or even computing the similarity matrix, as, e.g., in the Lanczos algorithm .  For large-sized graphs, the second eigenvalue of the (normalized) graph Laplacian matrix is often ill-conditioned , leading to slow convergence of iterative eigenvalue solvers. Preconditioning is a key technology accelerating the convergence, e.g., in the matrix-free LOBPCG method. Spectral clustering has been successfully applied on large graphs by first identifying their community structure , and then clustering communities. 3  Spectral clustering is closely related to Nonlinear dimensionality reduction , and dimension reduction techniques such as locally-linear embedding can be used to reduce errors from noise or outliers. 4  Relationship with k -means  The kernel k -means problem is an extension of the k -means problem where the input data points are mapped non-linearly into a higher-dimensional feature space via a kernel function     k   (   x  i   ,   x  j   )    =    œï  T    (   x  i   )   œï   (   x  j   )          k    subscript  x  i    subscript  x  j        superscript  œï  T    subscript  x  i   œï   subscript  x  j      k(x_{i},x_{j})=\phi^{T}(x_{i})\phi(x_{j})   . The weighted kernel k -means problem further extends this problem by defining a weight    w  r     subscript  w  r    w_{r}   for each cluster as the reciprocal of the number of elements in the cluster,        max   {   C  s   }      ‚àë   r  =  1   k     w  r     ‚àë     x  i   ,   x  j    ‚àà   C  r      k   (   x  i   ,   x  j   )        .       subscript     subscript  C  s       superscript   subscript     r  1    k      subscript  w  r     subscript       subscript  x  i    subscript  x  j     subscript  C  r       k    subscript  x  i    subscript  x  j          \max_{\{C_{s}\}}\sum_{r=1}^{k}w_{r}\sum_{x_{i},x_{j}\in C_{r}}k(x_{i},x_{j}).   Suppose   F   F   F   is a matrix of the normalizing coefficients for each point for each cluster     F   i  j    =   w  r        subscript  F    i  j     subscript  w  r     F_{ij}=w_{r}   if     i  ,  j   ‚àà   C  r        i  j    subscript  C  r     i,j\in C_{r}   and zero otherwise. Suppose   K   K   K   is the kernel matrix for all points. The weighted kernel k -means problem with n points and k clusters is given as,       max  F    trace   (   K  F   )         subscript   F    trace    K  F      \max_{F}\operatorname{trace}\left(KF\right)   such that,      F  =    G   n  √ó  k     G   n  √ó  k   T        F     subscript  G    n  k     superscript   subscript  G    n  k    T      F=G_{n\times k}G_{n\times k}^{T}           G  T   G   =  I         superscript  G  T   G   I    G^{T}G=I   such that     rank   (  G  )    =  k        rank  G   k    \text{rank}(G)=k   . In addition, there are identity constrains on   F   F   F   given by,       F  ‚ãÖ  ùïÄ   =  ùïÄ       normal-‚ãÖ  F  ùïÄ   ùïÄ    F\cdot\mathbb{I}=\mathbb{I}   where   ùïÄ   ùïÄ   \mathbb{I}   represents a vector of ones.        F  T   ùïÄ   =  ùïÄ         superscript  F  T   ùïÄ   ùïÄ    F^{T}\mathbb{I}=\mathbb{I}   This problem can be recast as,         max  G   trace    (    G  T   G   )    .        subscript   G   trace      superscript  G  T   G     \max_{G}\text{ trace }\left(G^{T}G\right).   This problem is equivalent to the spectral clustering problem when the identity constraints on   F   F   F   are relaxed. In particular, the weighted kernel k -means problem can be reformulated as a spectral clustering (graph partitioning) problem and vice versa. The output of the algorithms are eigenvectors which do not satisfy the identity requirements for indicator variables defined by   F   F   F   . Hence, post-processing of the eigenvectors is required for the equivalence between the problems. 5 Transforming the spectral clustering problem into a weighted kernel k -means problem greatly reduces the computational burden. 6  See also   Affinity propagation  Kernel principal component analysis  Cluster analysis  Spectral graph theory   References    "  Category:Data clustering algorithms  Category:Algebraic graph theory     Jianbo Shi and Jitendra Malik, "Normalized Cuts and Image Segmentation" , IEEE Transactions on PAMI, Vol. 22, No. 8, Aug 2000. ‚Ü©  Marina MeilƒÉ & Jianbo Shi, " Learning Segmentation by Random Walks ", Neural Information Processing Systems 13 (NIPS 2000), 2001, pp. 873‚Äì879. ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©  ‚Ü©     