   Maximally informative dimensions      Maximally informative dimensions  '''Maximally informative dimensions''' is a [[dimensionality reduction]] technique used in the statistical analyses of neural responses . Specifically, it is a way of projecting a stimulus onto a low-dimensional subspace so that as much information as possible about the stimulus is preserved in the neural response. It is motivated by the fact that natural stimuli are typically confined by their statistics to a lower-dimensional space than that spanned by white noise . 1 Within this subspace, however, stimulus-response functions may be either linear or nonlinear . The idea was originally developed by Tatyana Sharpee, Nicole Rust, and William Bialek in 2003. 2  Mathematical formulation  Neural stimulus-response functions are typically given as the probability of a neuron generating an action potential , or spike, in response to a stimulus   𝐬   𝐬   \mathbf{s}   . The goal of maximally informative dimensions is to find a small relevant subspace of the much larger stimulus space that accurately captures the salient features of   𝐬   𝐬   \mathbf{s}   . Let   D   D   D   denote the dimensionality of the entire stimulus space and   K   K   K   denote the dimensionality of the relevant subspace, such that    K  ≪  D     much-less-than  K  D    K\ll D   . We let    {   𝐯  K   }      superscript  𝐯  K     \{\mathbf{v}^{K}\}   denote the basis of the relevant subspace, and    𝐬  K     superscript  𝐬  K    \mathbf{s}^{K}   the projection of   𝐬   𝐬   \mathbf{s}   onto    {   𝐯  K   }      superscript  𝐯  K     \{\mathbf{v}^{K}\}   . Using Bayes' theorem we can write out the probability of a spike given a stimulus:      P   (  s  p  i  k  e  |   𝐬  K   )   =  P   (  s  p  i  k  e  )   f   (   𝐬  K   )      fragments  P   fragments  normal-(  s  p  i  k  e  normal-|   superscript  𝐬  K   normal-)    P   fragments  normal-(  s  p  i  k  e  normal-)   f   fragments  normal-(   superscript  𝐬  K   normal-)     P(spike|\mathbf{s}^{K})=P(spike)f(\mathbf{s}^{K})     where       f   (   𝐬  K   )    =    P   (   𝐬  K   |  s  p  i  k  e  )     P   (   𝐬  K   )           f   superscript  𝐬  K       fragments  P   fragments  normal-(   superscript  𝐬  K   normal-|  s  p  i  k  e  normal-)      P   superscript  𝐬  K       f(\mathbf{s}^{K})=\frac{P(\mathbf{s}^{K}|spike)}{P(\mathbf{s}^{K})}     is some nonlinear function of the projected stimulus.  In order to choose the optimal    {   𝐯  K   }      superscript  𝐯  K     \{\mathbf{v}^{K}\}   , we compare the prior stimulus distribution    P   (  𝐬  )       P  𝐬    P(\mathbf{s})   with the spike-triggered stimulus distribution    P   (  𝐬  |  s  p  i  k  e  )      fragments  P   fragments  normal-(  s  normal-|  s  p  i  k  e  normal-)     P(\mathbf{s}|spike)   using the Shannon information . The average information (averaged across all presented stimuli) per spike is given by       I   s  p  i  k  e    =   ∑  𝐬   P   (  𝐬  |  s  p  i  k  e  )   l  o   g  2    [  P   (  𝐬  |  s  p  i  k  e  )   /  P   (  𝐬  )   ]      fragments   subscript  I    s  p  i  k  e      subscript   𝐬   P   fragments  normal-(  s  normal-|  s  p  i  k  e  normal-)   l  o   subscript  g  2    fragments  normal-[  P   fragments  normal-(  s  normal-|  s  p  i  k  e  normal-)    P   fragments  normal-(  s  normal-)   normal-]     I_{spike}=\sum_{\mathbf{s}}P(\mathbf{s}|spike)log_{2}[P(\mathbf{s}|spike)/P(%
 \mathbf{s})]   . 3  Now consider a    K  =  1      K  1    K=1   dimensional subspace defined by a single direction   𝐯   𝐯   \mathbf{v}   . The average information conveyed by a single spike about the projection    x  =   𝐬  ⋅  𝐯       x   normal-⋅  𝐬  𝐯     x=\mathbf{s}\cdot\mathbf{v}   is      I   (  𝐯  )   =  ∫  d  x   P  𝐯    (  x  |  s  p  i  k  e  )   l  o  g  2   [   P  𝐯    (  x  |  s  p  i  k  e  )   /   P  𝐯    (  x  )   ]      fragments  I   fragments  normal-(  v  normal-)     d  x   subscript  P  𝐯    fragments  normal-(  x  normal-|  s  p  i  k  e  normal-)   l  o  g  2   fragments  normal-[   subscript  P  𝐯    fragments  normal-(  x  normal-|  s  p  i  k  e  normal-)     subscript  P  𝐯    fragments  normal-(  x  normal-)   normal-]     I(\mathbf{v})=\int dxP_{\mathbf{v}}(x|spike)log2[P_{\mathbf{v}}(x|spike)/P_{%
 \mathbf{v}}(x)]   ,  where the probability distributions are approximated by a measured data set via     P  𝐯    (  x  |  s  p  i  k  e  )   =    ⟨  δ   (  x  -  𝐬  ⋅  𝐯  )   |  s  p  i  k  e  ⟩   𝐬      fragments   subscript  P  𝐯    fragments  normal-(  x  normal-|  s  p  i  k  e  normal-)     subscript   fragments  normal-⟨  δ   fragments  normal-(  x   s  normal-⋅  v  normal-)   normal-|  s  p  i  k  e  normal-⟩   𝐬     P_{\mathbf{v}}(x|spike)=\langle\delta(x-\mathbf{s}\cdot\mathbf{v})|spike%
 \rangle_{\mathbf{s}}   and      P  𝐯    (  x  )    =    ⟨   δ   (   x  -   𝐬  ⋅  𝐯    )    ⟩   𝐬          subscript  P  𝐯   x    subscript   delimited-⟨⟩    δ    x   normal-⋅  𝐬  𝐯      𝐬     P_{\mathbf{v}}(x)=\langle\delta(x-\mathbf{s}\cdot\mathbf{v})\rangle_{\mathbf{s}}   , i.e., each presented stimulus is represented by a scaled Dirac delta function and the probability distributions are created by averaging over all spike-eliciting stimuli, in the former case, or the entire presented stimulus set, in the latter case. For a given dataset, the average information is a function only of the direction   𝐯   𝐯   \mathbf{v}   . Under this formulation, the relevant subspace of dimension    K  =  1      K  1    K=1   would be defined by the direction   𝐯   𝐯   \mathbf{v}   that maximizes the average information    I   (  𝐯  )       I  𝐯    I(\mathbf{v})   .  This procedure can readily be extended to a relevant subspace of dimension    K  >  1      K  1    K>1   by defining       P   𝐯  K     (  𝐱  |  s  p  i  k  e  )   =    ⟨   ∏   i  =  1   K   δ   (   x  i   -  𝐬  ⋅   𝐯  i   )   |  s  p  i  k  e  ⟩   𝐬      fragments   subscript  P   superscript  𝐯  K     fragments  normal-(  x  normal-|  s  p  i  k  e  normal-)     subscript   fragments  normal-⟨   superscript   subscript  product    i  1    K   δ   fragments  normal-(   subscript  x  i    s  normal-⋅   subscript  𝐯  i   normal-)   normal-|  s  p  i  k  e  normal-⟩   𝐬     P_{\mathbf{v}^{K}}(\mathbf{x}|spike)=\langle\prod_{i=1}^{K}\delta(x_{i}-%
 \mathbf{s}\cdot\mathbf{v}_{i})|spike\rangle_{\mathbf{s}}   and        P   𝐯  K     (  𝐱  )    =    ⟨    ∏   i  =  1   K    δ   (    x  i   -   𝐬  ⋅   𝐯  i     )     ⟩   𝐬          subscript  P   superscript  𝐯  K    𝐱    subscript   delimited-⟨⟩    superscript   subscript  product    i  1    K     δ     subscript  x  i    normal-⋅  𝐬   subscript  𝐯  i        𝐬     P_{\mathbf{v}^{K}}(\mathbf{x})=\langle\prod_{i=1}^{K}\delta(x_{i}-\mathbf{s}%
 \cdot\mathbf{v}_{i})\rangle_{\mathbf{s}}     and maximizing    I   (   𝐯  K   )       I   superscript  𝐯  K     I({\mathbf{v}^{K}})   .  Importance  Maximally informative dimensions does not make any assumptions about the Gaussianity of the stimulus set, which is important, because naturalistic stimuli tend to have non-Gaussian statistics. In this way the technique is more robust than other dimensionality reduction techniques such as spike-triggered covariance analyses.  References  "  Category:Neuroscience  Category:Computational neuroscience     D.J. Field. "Relations between the statistics of natural images and the response properties of cortical cells." J. Opt. Soc. am. A 4:2479-2394, 1987. ↩  Sharpee, Tatyana, Nicole C. Rust, and William Bialek. "Maximally informative dimensions: analyzing neural responses to natural signals." Advances in Neural Information Processing Systems (2003): 277-284. ↩  N. Brenner, S. P. Strong, R. Koberle, W. Bialek, and R. R. de Ruyter van Steveninck. "Synergy in a neural code. Neural Comp., 12:1531-1552, 2000. ↩     