   Mass estimation      Mass estimation   In data mining , mass estimation is a recently proposed data modelling mechanism, as an alternative to density estimation, to solve data mining problems. 1 2 Unlike density estimation , it models data distribution without using distance measure. Mass estimation has been successfully applied in solving different data mining tasks such as anomaly detection , 3 4 5 6 7 8 9  clustering , 10 11 12  information retrieval , 13 14 15  classification 16 17 and regression . 18 19 Mass-based data mining methods often perform better than or at least as well as the state-of-the-art methods, but they run orders of magnitude faster than their distance/density-based counterparts. It is because mass is more fundamental than density ( density = mass/volume ) which can be estimated more efficiently.  Definition  In the simplest form, mass (or data mass) is the number of instances in a bounded region. Given a function     \Tau   (     fragments  \Tau  normal-(    \Tau(    ·    )   normal-)   )   that defines local regions by partitioning the data space, the mass of an instance   x   x   x   that lies in the region     \Tau    (  x  )       \Tau  x    \Tau(x)   can be estimated as:       m   (  x  )    =   |    \Tau    (  x  )    |         m  x       \Tau  x      m(x)=|\Tau(x)|   Eqn.(1)  The mass base function    m  (     fragments  m  normal-(    m(    ·    )   normal-)   )   depends on the implementation of     \Tau   (     fragments  \Tau  normal-(    \Tau(    ·    )   normal-)   )   . Two regions can have the same mass regardless of the characteristics of regions such as shape, volume or density. All the instances in a region have the same mass, i.e., the cardinality of the region.  One-dimensional mass estimation  Let   D   D   D   be a set of   n   n   n   instances in   R   R   R   . The data is divided into two non-empty regions with a binary split at   s   s   s   on the real line. The mass distribution at   x   x   x   is estimated as the sum of     m  i    (  x  )        subscript  m  i   x    m_{i}(x)   weighted by    p   (   s  i   )       p   subscript  s  i     p(s_{i})   as a result of    n  -  1      n  1    n-1   possible    s  i     subscript  s  i    s_{i}   .     m  a  s  s   (  x  |  D  )   =   ∑   i  =  1    n  -  1     m  i    (  x  )   ×  p   (   s  i   )      fragments  m  a  s  s   fragments  normal-(  x  normal-|  D  normal-)     superscript   subscript     i  1      n  1     subscript  m  i    fragments  normal-(  x  normal-)    p   fragments  normal-(   subscript  s  i   normal-)     mass(x|D)=\sum_{i=1}^{n-1}m_{i}(x)\times p(s_{i})   Eqn.(2) where    p   (   s  i   )       p   subscript  s  i     p(s_{i})   is the probability of selecting    s  i     subscript  s  i    s_{i}   . If     x  1   <   x  2   <  …  <   x   n  -  1    <   x  n          subscript  x  1    subscript  x  2        normal-…        subscript  x    n  1          subscript  x  n      x_{1}   are instances in   D   D   D   , then     p   (   s  i   )    =     x   i  +  1    -   x  i      x  n   -   x  1     >  0          p   subscript  s  i         subscript  x    i  1     subscript  x  i       subscript  x  n    subscript  x  1          0     p(s_{i})=\frac{x_{i+1}-x_{i}}{x_{n}-x_{1}}>0   .  In the example shown in Figure 1:     m  a  s  s   (   x  1   |  D  )   =  1  p   (   s  1   )   +  2  p   (   s  2   )   +  3  p   (   s  3   )   +  4  p   (   s  4   )      fragments  m  a  s  s   fragments  normal-(   subscript  x  1   normal-|  D  normal-)    1  p   fragments  normal-(   subscript  s  1   normal-)    2  p   fragments  normal-(   subscript  s  2   normal-)    3  p   fragments  normal-(   subscript  s  3   normal-)    4  p   fragments  normal-(   subscript  s  4   normal-)     mass(x_{1}|D)=1p(s_{1})+2p(s_{2})+3p(s_{3})+4p(s_{4})        m  a  s  s   (   x  2   |  D  )   =  4  p   (   s  1   )   +  2  p   (   s  2   )   +  3  p   (   s  3   )   +  4  p   (   s  4   )      fragments  m  a  s  s   fragments  normal-(   subscript  x  2   normal-|  D  normal-)    4  p   fragments  normal-(   subscript  s  1   normal-)    2  p   fragments  normal-(   subscript  s  2   normal-)    3  p   fragments  normal-(   subscript  s  3   normal-)    4  p   fragments  normal-(   subscript  s  4   normal-)     mass(x_{2}|D)=4p(s_{1})+2p(s_{2})+3p(s_{3})+4p(s_{4})        m  a  s  s   (   x  3   |  D  )   =  4  p   (   s  1   )   +  3  p   (   s  2   )   +  3  p   (   s  3   )   +  4  p   (   s  4   )      fragments  m  a  s  s   fragments  normal-(   subscript  x  3   normal-|  D  normal-)    4  p   fragments  normal-(   subscript  s  1   normal-)    3  p   fragments  normal-(   subscript  s  2   normal-)    3  p   fragments  normal-(   subscript  s  3   normal-)    4  p   fragments  normal-(   subscript  s  4   normal-)     mass(x_{3}|D)=4p(s_{1})+3p(s_{2})+3p(s_{3})+4p(s_{4})        m  a  s  s   (   x  4   |  D  )   =  4  p   (   s  1   )   +  3  p   (   s  2   )   +  2  p   (   s  3   )   +  4  p   (   s  4   )      fragments  m  a  s  s   fragments  normal-(   subscript  x  4   normal-|  D  normal-)    4  p   fragments  normal-(   subscript  s  1   normal-)    3  p   fragments  normal-(   subscript  s  2   normal-)    2  p   fragments  normal-(   subscript  s  3   normal-)    4  p   fragments  normal-(   subscript  s  4   normal-)     mass(x_{4}|D)=4p(s_{1})+3p(s_{2})+2p(s_{3})+4p(s_{4})        m  a  s  s   (   x  5   |  D  )   =  4  p   (   s  1   )   +  3  p   (   s  2   )   +  2  p   (   s  3   )   +  1  p   (   s  4   )      fragments  m  a  s  s   fragments  normal-(   subscript  x  5   normal-|  D  normal-)    4  p   fragments  normal-(   subscript  s  1   normal-)    3  p   fragments  normal-(   subscript  s  2   normal-)    2  p   fragments  normal-(   subscript  s  3   normal-)    1  p   fragments  normal-(   subscript  s  4   normal-)     mass(x_{5}|D)=4p(s_{1})+3p(s_{2})+2p(s_{3})+1p(s_{4})     Mass estimation using Eqn. 2 is a level-1 mass estimation. Level-    h  >  1      h  1    h>1   mass can be estimated recursively as follows:     m  a  s  s   (  x  ,  h  |  D  )   =   ∑   i  =  1     |  D  |   -  1    m  a  s  s   (  x  ,  h  -  1  |   {  y  |  y  ∈    \Tau   i    (  x  )   }   )   ×  p   (   s  i   )      fragments  m  a  s  s   fragments  normal-(  x  normal-,  h  normal-|  D  normal-)     superscript   subscript     i  1        D   1    m  a  s  s   fragments  normal-(  x  normal-,  h   1  normal-|   fragments  normal-{  y  normal-|  y    subscript  \Tau  i    fragments  normal-(  x  normal-)   normal-}   normal-)    p   fragments  normal-(   subscript  s  i   normal-)     mass(x,h|D)=\sum_{i=1}^{|D|-1}mass(x,h-1|\{y|y\in\Tau_{i}(x)\})\times p(s_{i})   Eqn.(3) Examples of data modelling in terms of mass distribution are shown in Figure 2.   Properties  Mass estimation has the following characteristics: 20 21   A mass distribution stipulates an ordering from core points to fringe points in a data cloud. In addition, this ordering accentuates the fringe points with a concave function - fringe points have markedly smaller mass than points close to the core points.  Mass estimation is more efficient than density estimation because mass is computed by simple counting and it requires only a small sample through an ensemble approach.  Mass can be interpreted as a measure of relevance with respect to the concept underlying the data, i.e., core points indicate that they are highly relevant and fringe points indicates that they are less relevance.   Mass estimation has two advantages in relation to efficacy and efficiency. First, the concavity property mentioned above ensures that fringe points are ‘stretched’ to be farther from the core points in a mass space - making it easier to separate fringe points from those points close to core points. This property, otherwise hidden, can then be exploited by a data mining algorithm to achieve a better result for the intended task than the one without it. Second, mass estimation offers to solve a problem more efficiently using the ordering derived from data directly, without distance or related expensive calculation, when the problem demands ranking.  Multi-dimensional mass estimation  In multi-dimensional space,    R  d     superscript  R  d    R^{d}   , the mass distribution is estimated by constructing multiple random partitions of the data space      \Tau   i   (     fragments   subscript  \Tau  i   normal-(    \Tau_{i}(    ·     )  (  i  =  1  ,  2  ,  ⋯  ,  t  )     fragments  normal-)  normal-(  i   1  normal-,  2  normal-,  normal-⋯  normal-,  t  normal-)    )(i=1,2,\cdots,t)   where each      \Tau   i   (     fragments   subscript  \Tau  i   normal-(    \Tau_{i}(    ·    )   normal-)   )   partitions the space using a fixed sized sub-sample of data,    m  a  s  s  (     fragments  m  a  s  s  normal-(    mass(    ,   x   x   x       )   normal-)   )         m  i    (  x  )        subscript  m  i   x    m_{i}(x)   , is the aggregation of   t   t   t   from      \Tau   i    (  x  )        subscript  \Tau  i   x    \Tau_{i}(x)   different regions   x   x   x   where   t   t   t   lies in each of the      \Tau   i   (     fragments   subscript  \Tau  i   normal-(    \Tau_{i}(   partitions as shown in Figure 3.   )   normal-)   )    ·'''      \Tau   i   (     fragments   subscript  \Tau  i   normal-(    \Tau_{i}(   can be implemented in different ways.  1. Tree based implementation Each   )   normal-)   )    ·     S  i     subscript  S  i    S_{i}   is constructed from a random subsample   h   h   h   by recursively dividing the data space into two regions with a random axis parallel split on an attribute at each node of the tree. The height of the tree      \Tau   i    (  x  )        subscript  \Tau  i   x    \Tau_{i}(x)   determines the level of mass estimation. The mass is estimated by aggregating masses from the leaf nodes   x   x   x   where      \Tau   i   (     fragments   subscript  \Tau  i   normal-(    \Tau_{i}(   in each tree   )   normal-)   )    ·      m  a  s  s   (  x  )    =    1  t     ∑   i  =  1   t      m  i    (  x  )    ×   2    ℓ  i    (  x  )              m  a  s  s  x       1  t     superscript   subscript     i  1    t        subscript  m  i   x    superscript  2     subscript  normal-ℓ  i   x         mass(x)=\frac{1}{t}\sum_{i=1}^{t}m_{i}(x)\times 2^{\ell_{i}(x)}   as follows: 22 23      ℓ  i    (  x  )        subscript  normal-ℓ  i   x    \ell_{i}(x)   Eqn.(4) where   x   x   x   is the path length of the leaf node where      \Tau   i   (     fragments   subscript  \Tau  i   normal-(    \Tau_{i}(   traversed in   )   normal-)   )    ·      m  a  s  s   (  x  )    =    1  t     ∑   i  =  1   t     m  i    (  x  )            m  a  s  s  x       1  t     superscript   subscript     i  1    t      subscript  m  i   x       mass(x)=\frac{1}{t}\sum_{i=1}^{t}m_{i}(x)   .  If balanced binary trees 24 25 26 are constructed such that path lengths of all leaf nodes are the same, Eqn. 4 can be simplified by ignoring the constant path length term as:      m  a  s  s   (  x  )    =    1  t     ∑   i  =  1   t    2    ℓ  i    (  x  )             m  a  s  s  x       1  t     superscript   subscript     i  1    t    superscript  2     subscript  normal-ℓ  i   x        mass(x)=\frac{1}{t}\sum_{i=1}^{t}2^{\ell_{i}(x)}   Eqn.(5) An example of two-dimensional data modelling using Eqn. 5 is shown in Figure 4.   If trees are constructed in such a way so that the mass in all leaf nodes is 1, 27 Eqn. 4 can be simplified by ignoring the mass term as:      S  i   ⊂  D       subscript  S  i   D    S_{i}\subset D   Eqn.(6)  2. Nearest neighbour based implementation From a sample of data   ψ   ψ   \psi   , local regions are defined by a set of    (   Π  i   )     subscript  normal-Π  i    (\Pi_{i})   hypercubes    S  i     subscript  S  i    S_{i}   centered at each instance in    H  p     subscript  H  p    H_{p}   . A hypercube region    p  ∈  D      p  D    p\in D   is centered at     l  p   =     ||   p  -  q   ||   ∞   2        subscript  l  p      subscript   norm    p  q      2     l_{p}=\frac{||p-q||_{\infty}}{2}   with length    q  =   m  i   n   o  ∈   S  i      (    ||   q  -  o   ||   ∞   )        q    m  i   subscript  n    o   subscript  S  i      subscript   norm    q  o         q=min_{o\in S_{i}}(||q-o||_{\infty})   where   p   p   p   is the nearest neighbour of    S  i     subscript  S  i    S_{i}   in     Π  i   =    ∪   o  ∈   S  i      H  o         subscript  normal-Π  i     subscript     o   subscript  S  i      subscript  H  o      \Pi_{i}=\cup_{o\in S_{i}}H_{o}   and    Π  i     subscript  normal-Π  i    \Pi_{i}   . The mass in each hypercube in     M  i   ⊂  D   (  |   M  i   |  =  Ψ  ;  ψ  <  Ψ  <  n  )      fragments   subscript  M  i    D   fragments  normal-(  normal-|   subscript  M  i   normal-|   Ψ  normal-;  ψ   Ψ   n  normal-)     M_{i}\subset D(|M_{i}|=\Psi;\psi<\Psi   is estimated using another random sub-sample of data    O   (  n  )       O  n    O(n)   . LiNearN 28 is a density estimator based on mass that uses nearest neighbour based implementation of local regions. The distinguishing characteristic of LiNearN is that both the number of instances and the volume of regions are adaptive to local data distribution.  Applications  The significance of mass estimation in solving different data mining tasks is summarised in the table below. The last three columns in the table provide the comparison of mass-based methods with state-of-the-art distance or density based methods (DBSCAN 29 for clustering, LOF 30 for anomaly detection, Naive Bayes 31 for classification, InstRank 32 and Qsim 33 for information retrieval) in terms of time complexity, space complexity and task specific performance.      Task   Interpretation   Time complexity   Space complexity   Task specific performance       Clustering   High mass indicates core regions; Low mass indicates noise regions       O   (   n  2   )       O   superscript  n  2     O(n^{2})   vs    O   (  1  )       O  1    O(1)          O   (  n  )       O  n    O(n)   vs    O   (  n  )       O  n    O(n)      Comparable     Anomaly detection   High mass signifies normal instances; Low mass signifies anomalies       O   (   n  2   )       O   superscript  n  2     O(n^{2})   vs    O   (  1  )       O  1    O(1)          O   (  n  )       O  n    O(n)   vs    O   (  1  )       O  1    O(1)      Comparable     Classification   Use mass to estimate multidimensional likelihood in a Bayesian classifier       O   (  n  )       O  n    O(n)   vs    O   (  1  )       O  1    O(1)          O   (  n  )       O  n    O(n)   vs    x  ∈   R  d       x   superscript  R  d     x\in R^{d}      Comparable     Information retrieval   High (low) mass signifies that a database object is highly (less) relevant to the query   Comparable   Comparable   Better     Density can also be estimated using mass. The density function at       f  ¯   m    (  x  )    =    1  t     ∑   i  =  1   t      m  i    (  x  )      ψ  ×  v    (     \Tau   i    (  x  )    )              subscript   normal-¯  f   m   x       1  t     superscript   subscript     i  1    t        subscript  m  i   x       ψ  v      subscript  \Tau  i   x         \bar{f}_{m}(x)=\frac{1}{t}\sum_{i=1}^{t}\frac{m_{i}(x)}{\psi\times v(\Tau_{i}(%
 x))}   using mass base function is estimated as follows:     v  (     fragments  v  normal-(    v(   Eqn.(7)            [Using tree-based implementation 34 35 ] where   )   normal-)   )    ·       f  ¯   l    (  x  )   =   1  t    ∑   i  =  1   t   ρ   (  x  |   Π  i   ,   M  i   )      fragments   subscript   normal-¯  f   l    fragments  normal-(  x  normal-)      1  t    superscript   subscript     i  1    t   ρ   fragments  normal-(  x  normal-|   subscript  normal-Π  i   normal-,   subscript  M  i   normal-)     \bar{f}_{l}(x)=\frac{1}{t}\sum_{i=1}^{t}\rho(x|\Pi_{i},M_{i})   is the volume of the region.      ρ   (  x  |   Π  i   ,   M  i   )   =    m   (   H   (  x  |   Π  i   )    |   M  i   )       |   M  i   |   ×  l    (   H   (  x  |   Π  i   )    )        fragments  ρ   fragments  normal-(  x  normal-|   subscript  normal-Π  i   normal-,   subscript  M  i   normal-)       fragments  m   fragments  normal-(   subscript  H   fragments  normal-(  x  normal-|   subscript  normal-Π  i   normal-)    normal-|   subscript  M  i   normal-)           subscript  M  i    l    subscript  H   fragments  normal-(  x  normal-|   subscript  normal-Π  i   normal-)        \rho(x|\Pi_{i},M_{i})=\frac{m(H_{(x|\Pi_{i})}|M_{i})}{|M_{i}|\times l(H_{(x|%
 \Pi_{i})})}   Eqn.(8)            [Using LiNearN 36 ] where    H   (  x  |   Π  i   )      subscript  H   fragments  normal-(  x  normal-|   subscript  normal-Π  i   normal-)     H_{(x|\Pi_{i})}   ;    Π  i     subscript  normal-Π  i    \Pi_{i}   is the hypercube in   x   x   x   where    m   (  H  |  M  )      fragments  m   fragments  normal-(  H  normal-|  M  normal-)     m(H|M)   falls into;   M   M   M   is the number of instances from   H   H   H   that fall in the hubercube    l   (  H  )       l  H    l(H)   and   H   H   H   is the length of the hypercube    O   (   n  2   )       O   superscript  n  2     O(n^{2})   .  The density estimator based on mass ( DEMass ) 37 38 39 has been applied to replace existing density estimator in LOF and DBSCAN , improving the time complexity from    O   (  n  )       O  n    O(n)   to    O   (  n  )       O  n    O(n)   , and space complexity from    O   (  1  )       O  1    O(1)   to   t   t   t   . Mass estimation can also be used as a method for feature projection . 40 41 Instead of aggregating mass from    R  d     superscript  R  d    R^{d}   models, each mass model can be treated as a new feature. Instances in the original    R  t     superscript  R  t    R^{t}   space are projected to    x  ∈   R  d       x   superscript  R  d     x\in R^{d}   mass space. Each    y  ∈   R  t       y   superscript  R  t     y\in R^{t}   is projected to    y  =   ⟨    m  1    (  x  )    ,    m  2    (  x  )    ,  ⋯  ,    m  t    (  x  )    ⟩       y      subscript  m  1   x      subscript  m  2   x   normal-⋯     subscript  m  t   x      y=\langle m_{1}(x),m_{2}(x),\cdots,m_{t}(x)\rangle   where    h  =  1      h  1    h=1   . Now, the problem can be solved in the projected mass space using existing data mining methods.  So far, mass is used directly and indirectly (through density estimation and feature projection) to solve different data mining problems. A summary of algorithms based on 'Mass' and 'DEMass' is provided in the table below.      Task   DEMass   Mass       Clustering   DEMass-DBSCAN, 42 43 LiNearN-Cluster 44   MassTER 45     Anomaly detection   DEMass-LOF, 46 47 LiNearN 48   iForest, 49 SciForest, 50 HS-Trees, 51 ReMass-iForest 52     Classification   DEMass-Bayes 53   MassBayes 54     Information retrieval    ReFeat, 55 ReMass-ReFeat 56     Relationship with data depth  There is a close relationship between the proposed mass and data depth . 57 Mass estimation and data depth both delineate the centrality of a data cloud (as opposed to compactness in the case of the density measure). The properties common to both measures are: 58   The centre of a data cloud has the maximum value of the measure  An ordering from the centre (having the maximum value) to the fringe points (having the minimum values).   However, there are two key differences. 59   Not until recently (see 60 ) data depth always models a given data with one centre, regardless whether the data is unimodal or multi-modal; whereas mass can model both unimodal and multimodal data by setting    h  >  1      h  1    h>1   or    (  τ  )    τ   (\tau)   . Local data depth 61 has a parameter   τ   τ   \tau   which allows it to model multi-modal data as well as unimodal data. However, the performance of local data depth appears to be sensitive to the setting of   h   h   h   . In contrast, a single setting of     f  ^    (  x  )        normal-^  f   x    \hat{f}(x)   in mass estimation produce good task-specific performance.  Mass is a simple and straightforward measure, and has efficient estimation methods based on axis-parallel partitions only. Data depth has many different definitions, depending on the construct used to define depth. The constructs could be Mahalanobis, Convex Hull, simplicial, halfspace and so on, 62 all of which are expensive to compute 63 —this has been the main obstacle in applying data depth to real applications in multi-dimensional problems.   Existing density estimators versus DEMass  " Estimation of densities is a universal problem of statistics (knowing the densities one can solve various problems) " – Vapnik (1998). 64  Density estimators approximate a probability density function    D  =   {   x   (  1  )    ,   x   (  2  )    ,  ⋯  ,   x   (  n  )    }       D    superscript  x  1    superscript  x  2   normal-⋯   superscript  x  n      D=\{x^{(1)},x^{(2)},\cdots,x^{(n)}\}   from the observed data     x   (  i  )    ∈   R  d        superscript  x  i    superscript  R  d     x^{(i)}\in R^{d}   , where       f  ^    K  D  E     (  x  )    =    1   n  ×   b  d       ∑   i  =  1   n    K   (    d  i  s  t   (  x  ,   x   (  i  )    )    b   )             subscript   normal-^  f     K  D  E    x       1    n   superscript  b  d       superscript   subscript     i  1    n     K      d  i  s  t   x   superscript  x  i     b        \hat{f}_{KDE}(x)=\frac{1}{n\times b^{d}}\sum_{i=1}^{n}K\bigg(\frac{dist(x,x^{(%
 i)})}{b}\bigg)   . Kernel density estimator (KDE) and k-nearest neighbour density estimator (kNN) are the two widely used density estimators in data mining.  1. Kernel density estimator (KDE) 65     K   (  ⋅  )       K  normal-⋅    K(\cdot)   Eqn.(9) where   b   b   b   is a kernel function and       f  ^    k  N  N     (  x  )    =    |   N   (  x  ,  k  )    |    n  ×   V   N   (  x  ,  k  )              subscript   normal-^  f     k  N  N    x         N   x  k       n   subscript  V    N   x  k         \hat{f}_{kNN}(x)=\frac{|N(x,k)|}{n\times V_{N(x,k)}}   is a smoothing parameter called kernel bandwidth .  2. k-nearest neighbour density estimator (kNN) 66     N   (  x  ,  k  )       N   x  k     N(x,k)   Eqn.(10) where   k   k   k   is the set of   x   x   x   -Nearest Neighbours of   D   D   D   in    V   N   (  x  ,  k  )       subscript  V    N   x  k      V_{N(x,k)}   and    N   (  x  ,  k  )       N   x  k     N(x,k)   is the volume enclosed by       f  ^    k  N  N  D  i  s  t     (  x  )    =    |   N   (  x  ,  k  )    |    n  ×     ∑   y  ∈   N   (  x  ,  k  )        d  i  s  t   (  x  ,  y  )              subscript   normal-^  f     k  N  N  D  i  s  t    x         N   x  k       n    subscript     y    N   x  k        d  i  s  t   x  y         \hat{f}_{kNNDist}(x)=\frac{|N(x,k)|}{n\times\sum_{y\in N(x,k)}dist(x,y)}   .  3. Average distance k-nearest neighbour density estimator 67    ϵ   ϵ   \epsilon   Eqn.(11)  4.       f  ^   ϵ    (  x  )    =    |    N  ϵ    (  x  )    |    n  ×  ϵ           subscript   normal-^  f   ϵ   x          subscript  N  ϵ   x      n  ϵ      \hat{f}_{\epsilon}(x)=\frac{|N_{\epsilon}(x)|}{n\times\epsilon}   -neighbourhood density estimator 68       N  ϵ    (  x  )    =   {   y  ∈  D   |    d  i  s  t   (  x  ,  y  )    ≤  ϵ   }          subscript  N  ϵ   x    conditional-set    y  D       d  i  s  t   x  y    ϵ      N_{\epsilon}(x)=\{y\in D|dist(x,y)\leq\epsilon\}   Eqn.(12) where    (   O   (    n  2   d   )    )      O     superscript  n  2   d     \big(O(n^{2}d)\big)   .  All existing density estimators (Eqn. 9–12) require some form of distance function to measure pairwise distances between instances that leads to quadratic time complexity    (   O   (   n  d   )    )      O    n  d     \big(O(nd)\big)   and linear space complexity    (   O   (  n  )    )      O  n    \big(O(n)\big)   . Hence, the existing distance-based density estimator can not be applied to large data sets because of their high time and space complexities.  In contrast, density estimators based on mass (Eqn. 7 and 8) allow to estimate density without using pairwise distance measure. DEMass reduces the time complexity to linear    (   O   (  1  )    )      O  1    \big(O(1)\big)   and the space complexity to constant $\big(O(1)\big)$ .  Source codes   One-dimensional mass estimation (Mass_1.0.0.zip) [MATLAB]  Multi-dimensional mass estimation (Mass_1.1.0.zip) [MATLAB]  DEMass-DBSCAN [JAVA under WEKA platform]  DEMass-Bayes [JAVA under WEKA platform]  MassBayes [JAVA under WEKA platform]   References    "  Category:Data mining     ↩  ↩  ↩   ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩   ↩  ↩  ↩  ↩   ↩    ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩   ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩   ↩  ↩  ↩  ↩  ↩  ↩  ↩    ↩  ↩   ↩  ↩  ↩  ↩  ↩  ↩     