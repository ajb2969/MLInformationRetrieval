


Mass estimation




Mass estimation

In data mining, mass estimation is a recently proposed data modelling mechanism, as an alternative to density estimation, to solve data mining problems.12 Unlike density estimation, it models data distribution without using distance measure. Mass estimation has been successfully applied in solving different data mining tasks such as anomaly detection,3456789 clustering,101112 information retrieval,131415 classification1617 and regression.1819 Mass-based data mining methods often perform better than or at least as well as the state-of-the-art methods, but they run orders of magnitude faster than their distance/density-based counterparts. It is because mass is more fundamental than density (density = mass/volume) which can be estimated more efficiently.
Definition
In the simplest form, mass (or data mass) is the number of instances in a bounded region. Given a function 
 
 
·

 
  that defines local regions by partitioning the data space, the mass of an instance 
 
 
 
  that lies in the region 
 
 
 
 
  can be estimated as:


 
            Eqn.(1)
The mass base function 
 
 
·

 
  depends on the implementation of 
 
 
·

 
 
 . Two regions can have the same mass regardless of the characteristics of regions such as shape, volume or density. All the instances in a region have the same mass, i.e., the cardinality of the region.
One-dimensional mass estimation
Let 
 
 
 
  be a set of 
 
 
 
  instances in 
 
 
 
 . The data is divided into two non-empty regions with a binary split at 
 
 
 
  on the real line. The mass distribution at 
 
 
 
 
  is estimated as the sum of 
 
 
 
  weighted by 
 
 
 
  as a result of 
 
 
 
  possible 
 
 
 
 .

 
 
            Eqn.(2)
 where 
 
 
 
  is the probability of selecting 
 
 
 
 . If 
 
 
 
  are instances in 
 
 
 
 , then 
 
 
 
 
 .

 In the example shown in Figure 1:











 Mass estimation using Eqn. 2 is a level-1 mass estimation. Level-
 
 
 
  mass can be estimated recursively as follows:

 
            Eqn.(3)
 Examples of data modelling in terms of mass distribution are shown in Figure 2.

Properties
Mass estimation has the following characteristics:2021

A mass distribution stipulates an ordering from core points to fringe points in a data cloud. In addition, this ordering accentuates the fringe points with a concave function - fringe points have markedly smaller mass than points close to the core points.
Mass estimation is more efficient than density estimation because mass is computed by simple counting and it requires only a small sample through an ensemble approach.
Mass can be interpreted as a measure of relevance with respect to the concept underlying the data, i.e., core points indicate that they are highly relevant and fringe points indicates that they are less relevance.

Mass estimation has two advantages in relation to efficacy and efficiency. First, the concavity property mentioned above ensures that fringe points are ‘stretched’ to be farther from the core points in a mass space - making it easier to separate fringe points from those points close to core points. This property, otherwise hidden, can then be exploited by a data mining algorithm to achieve a better result for the intended task than the one without it. Second, mass estimation offers to solve a problem more efficiently using the ordering derived from data directly, without distance or related expensive calculation, when the problem demands ranking.
Multi-dimensional mass estimation
In multi-dimensional space, 
 
 
 
 , the mass distribution is estimated by constructing multiple random partitions of the data space 
 
 
·

 
 
  where each 
 
 
·

 
  partitions the space using a fixed sized sub-sample of data, 
 
 
, 
 
 




 
 , is the aggregation of 
 
 
 
  from 
 
 
 
  different regions 
 
 
 
  where 
 
 
 
 
  lies in each of the 
 
 
 
  partitions as shown in Figure 3. 
 
 
·'''
 
 
 
  can be implemented in different ways. 
1. Tree based implementation
 Each 
 
 
·

 
 
  is constructed from a random subsample 
 
 
 
  by recursively dividing the data space into two regions with a random axis parallel split on an attribute at each node of the tree. The height of the tree 
 
 
 
  determines the level of mass estimation. The mass is estimated by aggregating masses from the leaf nodes 
 
 
 
  where 
 
 
 
  in each tree 
 
 
·

 
  as follows:2223

 
            Eqn.(4)
 where 
 
 
 
  is the path length of the leaf node where 
 
 
 
  traversed in 
 
 
·

 
 .
If balanced binary trees242526 are constructed such that path lengths of all leaf nodes are the same, Eqn. 4 can be simplified by ignoring the constant path length term as:

 
            Eqn.(5)
 An example of two-dimensional data modelling using Eqn. 5 is shown in Figure 4.

If trees are constructed in such a way so that the mass in all leaf nodes is 1,27 Eqn. 4 can be simplified by ignoring the mass term as:

 
            Eqn.(6)
2. Nearest neighbour based implementation
 From a sample of data 
 
 
 
 , local regions are defined by a set of 
 
 
 
 
  hypercubes 
 
 
 
  centered at each instance in 
 
 
 
 . A hypercube region 
 
 
 
  is centered at 
 
 
 
  with length 
 
 
 
 
  where 
 
 
 
  is the nearest neighbour of 
 
 
 
  in 
 
 
 
  and 
 
 
 
 . The mass in each hypercube in 
 
 
 
 
  is estimated using another random sub-sample of data 
 
 
 
 .
 LiNearN28 is a density estimator based on mass that uses nearest neighbour based implementation of local regions. The distinguishing characteristic of LiNearN is that both the number of instances and the volume of regions are adaptive to local data distribution.
Applications
The significance of mass estimation in solving different data mining tasks is summarised in the table below. The last three columns in the table provide the comparison of mass-based methods with state-of-the-art distance or density based methods (DBSCAN29 for clustering, LOF30 for anomaly detection, Naive Bayes31 for classification, InstRank32 and Qsim33 for information retrieval) in terms of time complexity, space complexity and task specific performance.




Task

Interpretation

Time complexity

Space complexity

Task specific performance





Clustering

High mass indicates core regions; Low mass indicates noise regions



 
  vs 
 
 




 
  vs 
 
 


Comparable



Anomaly detection

High mass signifies normal instances; Low mass signifies anomalies



 
  vs 
 
 




 
  vs 
 
 


Comparable



Classification

Use mass to estimate multidimensional likelihood in a Bayesian classifier



 
 
  vs 
 
 




 
  vs 
 
 


Comparable



Information retrieval

High (low) mass signifies that a database object is highly (less) relevant to the query

Comparable

Comparable

Better



Density can also be estimated using mass. The density function at 
 
 
 
  using mass base function is estimated as follows:

 
 
            Eqn.(7)            [Using tree-based implementation3435]
 where 
 
 
·

 
  is the volume of the region.


 
            Eqn.(8)            [Using LiNearN36]
 where 
 
 
 
 ; 
 
 
 
 
  is the hypercube in 
 
 
 
  where 
 
 
 
  falls into; 
 
 
 
  is the number of instances from 
 
 
 
  that fall in the hubercube 
 
 
 
 
  and 
 
 
 
  is the length of the hypercube 
 
 
 
 .
The density estimator based on mass (DEMass)373839 has been applied to replace existing density estimator in LOF and DBSCAN, improving the time complexity from 
 
 
 
  to 
 
 
 
 , and space complexity from 
 
 
 
 
  to 
 
 
 
 .
  Mass estimation can also be used as a method for feature projection.4041 Instead of aggregating mass from 
 
 
 
  models, each mass model can be treated as a new feature. Instances in the original 
 
 
 
  space are projected to 
 
 
 
  mass space. Each 
 
 
 
 
  is projected to 
 
 
 
  where 
 
 
 
 . Now, the problem can be solved in the projected mass space using existing data mining methods.
So far, mass is used directly and indirectly (through density estimation and feature projection) to solve different data mining problems. A summary of algorithms based on 'Mass' and 'DEMass' is provided in the table below.




Task

DEMass

Mass





Clustering

DEMass-DBSCAN,4243 LiNearN-Cluster44

MassTER45



Anomaly detection

DEMass-LOF,4647 LiNearN48

iForest,49 SciForest,50 HS-Trees,51 ReMass-iForest 52



Classification

DEMass-Bayes53

MassBayes54



Information retrieval


ReFeat,55 ReMass-ReFeat56



Relationship with data depth
There is a close relationship between the proposed mass and data depth.57 Mass estimation and data depth both delineate the centrality of a data cloud (as opposed to compactness in the case of the density measure). The properties common to both measures are:58

The centre of a data cloud has the maximum value of the measure
An ordering from the centre (having the maximum value) to the fringe points (having the minimum values).

However, there are two key differences.59

Not until recently (see 60) data depth always models a given data with one centre, regardless whether the data is unimodal or multi-modal; whereas mass can model both unimodal and multimodal data by setting 
 
 
 
  or 
 
 
 
 . Local data depth61 has a parameter 
 
 
 
 
  which allows it to model multi-modal data as well as unimodal data. However, the performance of local data depth appears to be sensitive to the setting of 
 
 
 
 . In contrast, a single setting of 
 
 
 
  in mass estimation produce good task-specific performance.
Mass is a simple and straightforward measure, and has efficient estimation methods based on axis-parallel partitions only. Data depth has many different definitions, depending on the construct used to define depth. The constructs could be Mahalanobis, Convex Hull, simplicial, halfspace and so on,62 all of which are expensive to compute63—this has been the main obstacle in applying data depth to real applications in multi-dimensional problems.

Existing density estimators versus DEMass
"Estimation of densities is a universal problem of statistics (knowing the densities one can solve various problems)" – Vapnik (1998).64
Density estimators approximate a probability density function 
 
 
 
  from the observed data 
 
 
 
 , where 
 
 
 
 
 . Kernel density estimator (KDE) and k-nearest neighbour density estimator (kNN) are the two widely used density estimators in data mining.
1. Kernel density estimator (KDE)65

 
            Eqn.(9)
 where 
 
 
 
  is a kernel function and 
 
 
 
  is a smoothing parameter called kernel bandwidth.
2. k-nearest neighbour density estimator (kNN)66

 
            Eqn.(10)
 where 
 
 
 
 
  is the set of 
 
 
 
 -Nearest Neighbours of 
 
 
 
  in 
 
 
 
  and 
 
 
 
  is the volume enclosed by
 
 
 
 
 .
3. Average distance k-nearest neighbour density estimator67

 
            Eqn.(11)
4. 
 
 
 
 -neighbourhood density estimator68

 
            Eqn.(12)
 where 
 
 
 
 .
All existing density estimators (Eqn. 9–12) require some form of distance function to measure pairwise distances between instances that leads to quadratic time complexity 
 
 
 
 
  and linear space complexity 
 
 
 
 . Hence, the existing distance-based density estimator can not be applied to large data sets because of their high time and space complexities.
In contrast, density estimators based on mass (Eqn. 7 and 8) allow to estimate density without using pairwise distance measure. DEMass reduces the time complexity to linear 
 
 
 
  and the space complexity to constant $\big(O(1)\big)$.
Source codes

One-dimensional mass estimation(Mass_1.0.0.zip) [MATLAB]
Multi-dimensional mass estimation(Mass_1.1.0.zip) [MATLAB]
DEMass-DBSCAN [JAVA under WEKA platform]
DEMass-Bayes [JAVA under WEKA platform]
MassBayes [JAVA under WEKA platform]

References


"
Category:Data mining











































































