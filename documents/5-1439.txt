   Quadratic classifier      Quadratic classifier   A quadratic classifier is used in machine learning and statistical classification to separate measurements of two or more classes of objects or events by a quadric surface. It is a more general version of the linear classifier .  The classification problem  Statistical classification considers a set of vectors of observations x of an object or event, each of which has a known type y . This set is referred to as the training set . The problem is then to determine for a given new observation vector, what the best class should be. For a quadratic classifier, the correct solution is assumed to be quadratic in the measurements, so y will be decided based on        ğ±  ğ“   ğ€ğ±   +    ğ›  ğ“   ğ±   +  c         superscript  ğ±  ğ“   ğ€ğ±      superscript  ğ›  ğ“   ğ±   c    \mathbf{x^{T}Ax}+\mathbf{b^{T}x}+c     In the special case where each observation consists of two measurements, this means that the surfaces separating the classes will be conic sections ( i.e. either a line , a circle or ellipse , a parabola or a hyperbola ). In this sense we can state that a quadratic model is a generalization of the linear model, and its use is justified by the desire to extend the classifier's ability to represent more complex separating surfaces.  Quadratic discriminant analysis  Quadratic discriminant analysis (QDA) is closely related to linear discriminant analysis (LDA), where it is assumed that the measurements from each class are normally distributed . Unlike LDA however, in QDA there is no assumption that the covariance of each of the classes is identical. When the normality assumption is true, the best possible test for the hypothesis that a given measurement is from a given class is the likelihood ratio test . Suppose there are only two groups, (so    y  âˆˆ   {  0  ,  1  }       y   0  1     y\in\{0,1\}   ), and the means of each class are defined to be     Î¼   y  =  0    ,   Î¼   y  =  1        subscript  Î¼    y  0     subscript  Î¼    y  1      \mu_{y=0},\mu_{y=1}   and the covariances are defined as     Î£   y  =  0    ,   Î£   y  =  1        subscript  normal-Î£    y  0     subscript  normal-Î£    y  1      \Sigma_{y=0},\Sigma_{y=1}   . Then the likelihood ratio will be given by   Likelihood ratio =         2  Ï€   |   Î£   y  =  1    |      -  1     exp   (   -    1  2     (   x  -   Î¼   y  =  1     )   T    Î£   y  =  1    -  1     (   x  -   Î¼   y  =  1     )     )         2  Ï€   |   Î£   y  =  0    |      -  1     exp   (   -    1  2     (   x  -   Î¼   y  =  0     )   T    Î£   y  =  0    -  1     (   x  -   Î¼   y  =  0     )     )      <  t           superscript      2  Ï€     subscript  normal-Î£    y  1         1            1  2    superscript    x   subscript  Î¼    y  1     T    superscript   subscript  normal-Î£    y  1      1      x   subscript  Î¼    y  1            superscript      2  Ï€     subscript  normal-Î£    y  0         1            1  2    superscript    x   subscript  Î¼    y  0     T    superscript   subscript  normal-Î£    y  0      1      x   subscript  Î¼    y  0          t    \frac{\sqrt{2\pi|\Sigma_{y=1}|}^{-1}\exp\left(-\frac{1}{2}(x-\mu_{y=1})^{T}%
 \Sigma_{y=1}^{-1}(x-\mu_{y=1})\right)}{\sqrt{2\pi|\Sigma_{y=0}|}^{-1}\exp\left%
 (-\frac{1}{2}(x-\mu_{y=0})^{T}\Sigma_{y=0}^{-1}(x-\mu_{y=0})\right)}      for some threshold t. After some rearrangement, it can be shown that the resulting separating surface between the classes is a quadratic. The sample estimates of the mean vector and variance-covariance matrices will substitute the population quantities in this formula.  Other quadratic classifiers  While QDA is the most commonly used method for obtaining a classifier, other methods are also possible. One such method is to create a longer measurement vector from the old one by adding all pairwise products of individual measurements. For instance, the vector      [   x  1   ,   x  2   ,   x  3   ]      subscript  x  1    subscript  x  2    subscript  x  3     [x_{1},\;x_{2},\;x_{3}]     would become      [   x  1   ,   x  2   ,   x  3   ,   x  1  2   ,    x  1    x  2    ,    x  1    x  3    ,   x  2  2   ,    x  2    x  3    ,   x  3  2   ]      subscript  x  1    subscript  x  2    subscript  x  3    superscript   subscript  x  1   2      subscript  x  1    subscript  x  2       subscript  x  1    subscript  x  3     superscript   subscript  x  2   2      subscript  x  2    subscript  x  3     superscript   subscript  x  3   2     [x_{1},\;x_{2},\;x_{3},\;x_{1}^{2},\;x_{1}x_{2},\;x_{1}x_{3},\;x_{2}^{2},\;x_{%
 2}x_{3},\;x_{3}^{2}]   .  Finding a quadratic classifier for the original measurements would then become the same as finding a linear classifier based on the expanded measurement vector. This observation has been used in extending neural network models; 1 the "circular" case, which corresponds to introducing only the sum of pure quadratic terms      x   1  2   +   x  2  2   +     x  3  2     â€¦         superscript   subscript  x  1   2    superscript   subscript  x  2   2      superscript   subscript  x  3   2   normal-â€¦     \;x_{1}^{2}+x_{2}^{2}+x_{3}^{2}\;\ldots\;   with no mixed products (       x   1  2    x  2  2    ,    x  1  2     x  3  2     â€¦          superscript   subscript  x  1   2    superscript   subscript  x  2   2       superscript   subscript  x  1   2    superscript   subscript  x  3   2   normal-â€¦     \;x_{1}^{2}x_{2}^{2},\;x_{1}^{2}x_{3}^{2}\;\ldots\;   ), has been proven to be the optimal compromise between extending the classifier's representation power and controlling the risk of overfitting ( Vapnik-Chervonenkis dimension ). 2  For linear classifiers based only on dot products , these expanded measurements do not have to be actually computed, since the dot product in the higher-dimensional space is simply related to that in the original space. This is an example of the so-called kernel trick , which can be applied to linear discriminant analysis, as well as the support vector machine .  References    Sources:     "  Category:Classification algorithms  Category:Statistical classification     â†©  â†©     