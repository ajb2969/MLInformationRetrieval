<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1549">Sparse distributed memory</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Sparse distributed memory</h1>
<hr/>

<p><strong>Sparse distributed memory</strong> is a mathematical model of human <a href="long-term_memory" title="wikilink">long-term memory</a> introduced by <a href="Pentti_Kanerva" title="wikilink">Pentti Kanerva</a> in 1988 while he was at <a href="Ames_Research_Center" title="wikilink">NASA Ames Research Center</a>. It is a generalized <a href="random-access_memory" title="wikilink">random-access memory</a> (RAM) for long (e.g., 1,000 bit) binary words. These words serve as both addresses to and data for the memory. The main attribute of the memory is sensitivity to similarity, meaning that a word can be read back not only by giving the original write address but also by giving one close to it, as measured by the number of mismatched bits (i.e., the <a href="Hamming_distance" title="wikilink">Hamming distance</a> between <a href="memory_address" title="wikilink">memory addresses</a>).<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>

<p>The theory of the memory is mathematically complete <a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> and has been verified by computer simulation. It arose from the observation that the distances between points of a high-dimensional space resemble the proximity relations between <a class="uri" href="concepts" title="wikilink">concepts</a> in human memory. The theory is also practical in that memories based on it can be implemented with conventional <a href="Random-access_memory" title="wikilink">RAM-memory</a> elements.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="definition">Definition</h2>

<p>Sparse distributed memory is a mathematical representation of human memory, and uses <a href="Clustering_high-dimensional_data" title="wikilink">high-dimensional space</a> to help model the large amounts of memory that mimics that of the human neural network.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> It utilizes the <a href="Hamming_distance" title="wikilink">Hamming distance</a> to measure mismatched bits and read back data between the original write address and one near it.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> Human memory has a tendency to congregate memories based on similarities between them(although they may not be related), such as "firetrucks are red and apples are red".<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>

<p>Sparse distributed memory is based on pulling in patterns between different addresses. Imagine each line as a different memory address, an example from Kanerva's book:</p>
<dl>
<dd>"Why are fire engines painted red?
</dd>
<dd>Firemen's suspenders are red, too.
</dd>
<dd>Two and two are four.
</dd>
<dd>Four times three is twelve.
</dd>
<dd>Twelve inches in a foot.
</dd>
<dd>A foot is a ruler.
</dd>
<dd>Queen Mary is a ruler.
</dd>
<dd>Queen Mary sailed the sea.
</dd>
<dd>The sea has sharks.
</dd>
<dd>Sharks have fins.
</dd>
<dd>The Russians conquered the Finns.
</dd>
<dd>The Russians' color is red.
</dd>
<dd>Fire engines are always rushin'.
</dd>
<dd>So that's why they're painted red!"<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a>
</dd>
</dl>

<p>As a result, all of these addresses are <a href="Return_statement" title="wikilink">returned</a> to the user, although these may not be the only addresses in that list.</p>
<h3 id="neuron-model">Neuron model</h3>

<p>An idealized description of <a class="uri" href="neuron" title="wikilink">neuron</a> is as follows: a neuron has a cell body with two kinds of branches: <em><a class="uri" href="dendrites" title="wikilink">dendrites</a></em> and an <em><a class="uri" href="axon" title="wikilink">axon</a></em>. It receives input signals from other neurons via dendrites, integrates (sums) them and generates its own (electric) output signal which is sent to outside neurons via axon. The points of electric contact between neurons are called <em><a class="uri" href="synapses" title="wikilink">synapses</a></em>.</p>

<p>When a neuron generates signal it is <em>firing</em> and after firing it must <em>recover</em> before it fires again. The relative importance of a synapse to the firing of neuron is called <em>synaptic weight</em> (or <em>input coefficient</em>). There are two kinds of synapses: <a class="uri" href="excitatory" title="wikilink">excitatory</a> that trigger neuron to <em>fire</em> and <a class="uri" href="inhibitory" title="wikilink">inhibitory</a> that hinder firing. The neuron is either excitatory or inhibitory according to the kinds of synapses its axon makes.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>

<p>in addition to input the firing of neuron depends on <em>threshold</em>. The higher the threshold the more important it is that excitatory synapses have input while inhibitory ones do not.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> Whether a recovered neuron actually fires depends on whether it received sufficient excitatory input (beyond the threshold) and not too much of inhibitory input within a certain period.</p>

<p>The formal model of neuron makes further simplifying assumptions.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> An <em>n</em>-input neuron is modeled by a <em>linear threshold function</em> 

<math display="inline" id="Sparse_distributed_memory:0">
 <semantics>
  <mrow>
   <mi>F</mi>
   <mo>:</mo>
   <mrow>
    <mrow>
     <msup>
      <mrow>
       <mo stretchy="false">{</mo>
       <mn>0</mn>
       <mo>,</mo>
       <mn>1</mn>
       <mo stretchy="false">}</mo>
      </mrow>
      <mi>n</mi>
     </msup>
     <mo>-</mo>
    </mrow>
    <mo>></mo>
    <mrow>
     <mo stretchy="false">{</mo>
     <mn>0</mn>
     <mo>,</mo>
     <mn>1</mn>
     <mo stretchy="false">}</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>F</ci>
    <apply>
     <gt></gt>
     <apply>
      <csymbol cd="latexml">limit-from</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <set>
        <cn type="integer">0</cn>
        <cn type="integer">1</cn>
       </set>
       <ci>n</ci>
      </apply>
      <minus></minus>
     </apply>
     <set>
      <cn type="integer">0</cn>
      <cn type="integer">1</cn>
     </set>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F:\{0,1\}^{n}->\{0,1\}
  </annotation>
 </semantics>
</math>

 as follows :</p>

<p>For 

<math display="inline" id="Sparse_distributed_memory:1">
 <semantics>
  <mrow>
   <mi>i</mi>
   <mo>=</mo>
   <mrow>
    <mn>0</mn>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <mrow>
     <mi>n</mi>
     <mo>-</mo>
     <mn>1</mn>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>i</ci>
    <list>
     <cn type="integer">0</cn>
     <ci>normal-…</ci>
     <apply>
      <minus></minus>
      <ci>n</ci>
      <cn type="integer">1</cn>
     </apply>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i=0,...,n-1
  </annotation>
 </semantics>
</math>

 where n is the number of inputs, let 

<math display="inline" id="Sparse_distributed_memory:2">
 <semantics>
  <msub>
   <mi>F</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>F</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F_{t}
  </annotation>
 </semantics>
</math>

 be the output at time <em>t</em>

<math display="block" id="Sparse_distributed_memory:3">
 <semantics>
  <mrow>
   <msub>
    <mi>F</mi>
    <mi>t</mi>
   </msub>
   <mo>∈</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mn>0</mn>
    <mo>,</mo>
    <mn>1</mn>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>F</ci>
     <ci>t</ci>
    </apply>
    <set>
     <cn type="integer">0</cn>
     <cn type="integer">1</cn>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F_{t}\in\{0,1\}
  </annotation>
 </semantics>
</math>

, and let 

<math display="inline" id="Sparse_distributed_memory:4">
 <semantics>
  <msub>
   <mi>x</mi>
   <mrow>
    <mi>i</mi>
    <mo>,</mo>
    <mi>t</mi>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <list>
     <ci>i</ci>
     <ci>t</ci>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i,t}
  </annotation>
 </semantics>
</math>

 be the <em>i</em>-th input at time <em>t</em>

<math display="block" id="Sparse_distributed_memory:5">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mrow>
     <mi>i</mi>
     <mo>,</mo>
     <mi>t</mi>
    </mrow>
   </msub>
   <mo>∈</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mn>0</mn>
    <mo>,</mo>
    <mn>1</mn>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <list>
      <ci>i</ci>
      <ci>t</ci>
     </list>
    </apply>
    <set>
     <cn type="integer">0</cn>
     <cn type="integer">1</cn>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{i,t}\in\{0,1\}
  </annotation>
 </semantics>
</math>

. Let 

<math display="inline" id="Sparse_distributed_memory:6">
 <semantics>
  <msub>
   <mi>w</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}
  </annotation>
 </semantics>
</math>

 be the weight of the <em>i</em>-th input and let 

<math display="inline" id="Sparse_distributed_memory:7">
 <semantics>
  <mi>c</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>c</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   c
  </annotation>
 </semantics>
</math>

 be the threshold.</p>

<p>The <em>weighted sum</em> of the inputs at time <em>t</em> is defined by 

<math display="inline" id="Sparse_distributed_memory:8">
 <semantics>
  <mrow>
   <msub>
    <mi>S</mi>
    <mi>t</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>0</mn>
     </mrow>
     <mrow>
      <mi>n</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msubsup>
    <mrow>
     <msub>
      <mi>w</mi>
      <mi>i</mi>
     </msub>
     <msub>
      <mi>x</mi>
      <mrow>
       <mi>i</mi>
       <mo>,</mo>
       <mi>t</mi>
      </mrow>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>S</ci>
     <ci>t</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">0</cn>
       </apply>
      </apply>
      <apply>
       <minus></minus>
       <ci>n</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <list>
        <ci>i</ci>
        <ci>t</ci>
       </list>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S_{t}=\sum_{i=0}^{n-1}w_{i}x_{i,t}
  </annotation>
 </semantics>
</math>

</p>

<p>The <em>neuron output</em> at time <em>t</em> is then defined as a <a href="boolean_function" title="wikilink">boolean function</a>: 

<math display="inline" id="Sparse_distributed_memory:9">
 <semantics>
  <mrow>
   <msub>
    <mi>𝐒</mi>
    <mi>t</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <mo>{</mo>
    <mtable>
     <mtr>
      <mtd columnalign="left">
       <mn>1</mn>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mrow>
          <mtext>if</mtext>
          <msub>
           <mi>S</mi>
           <mi>t</mi>
          </msub>
         </mrow>
         <mo>≥</mo>
         <mi>c</mi>
        </mrow>
        <mo>,</mo>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mn>0</mn>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mtext>otherwise</mtext>
        <mo>.</mo>
       </mrow>
      </mtd>
     </mtr>
    </mtable>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>𝐒</ci>
     <ci>t</ci>
    </apply>
    <apply>
     <csymbol cd="latexml">cases</csymbol>
     <cn type="integer">1</cn>
     <apply>
      <geq></geq>
      <apply>
       <times></times>
       <mtext>if</mtext>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>S</ci>
        <ci>t</ci>
       </apply>
      </apply>
      <ci>c</ci>
     </apply>
     <cn type="integer">0</cn>
     <mtext>otherwise</mtext>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{S}_{t}=\begin{cases}1&\text{if }S_{t}>=c,\\
0&\text{otherwise }.\end{cases}
  </annotation>
 </semantics>
</math>

</p>

<p>Where F<sub>t</sub>=1 means that the neuron fires at time <em>t</em> and F<sub>t</sub>=0 that it doesn't, i.e. in order for neuron to fire the weighted sum must reach or exceed the threshold . Excitatory inputs increase the sum and inhibitory inputs decrease it.</p>
<h3 id="neuron-as-address-decoder">Neuron as address-decoder</h3>

<p>Kanerva's key thesis <a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> is that certain neurons could have their input coefficients and thresholds fixed over entire life of an organism and used as address decoders where <em>n</em>-tuple of input coefficients (the pattern to which neurons respond most readily) determines the <em>n</em>-bit memory address, and the threshold controls the size of the region of similar address patterns to which the neuron responds.</p>

<p>The address <em>a</em> of a neuron with input coefficients <em>w</em> where 

<math display="inline" id="Sparse_distributed_memory:10">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mn>0</mn>
   </msub>
   <mo>,</mo>
   <mo>.</mo>
   <mo>.</mo>
   <mo>,</mo>
   <msub>
    <mi>w</mi>
    <msub>
     <mi>n</mi>
     <mn>1</mn>
    </msub>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <cn type="integer">0</cn>
    </apply>
    <ci>normal-,</ci>
    <ci>normal-.</ci>
    <ci>normal-.</ci>
    <ci>normal-,</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>n</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{0},..,w_{n_{1}}
  </annotation>
 </semantics>
</math>


 is defined as an <em>n</em>-bit input pattern that maximizes the weighted sum. The maximum occurs when the inhibitory inputs are zeros and the excitatory inputs are ones. The <em>i</em>-th bit of address is: 

<math display="inline" id="Sparse_distributed_memory:11">
 <semantics>
  <mrow>
   <msub>
    <mi>𝐚</mi>
    <mi>i</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <mo>{</mo>
    <mtable>
     <mtr>
      <mtd columnalign="left">
       <mn>1</mn>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mrow>
          <mtext>if</mtext>
          <msub>
           <mi>w</mi>
           <mi>i</mi>
          </msub>
         </mrow>
         <mo>></mo>
         <mn>0</mn>
        </mrow>
        <mo>,</mo>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mn>0</mn>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mtext>if</mtext>
         <msub>
          <mi>w</mi>
          <mi>i</mi>
         </msub>
        </mrow>
        <mo><</mo>
        <mn>0.</mn>
       </mrow>
      </mtd>
     </mtr>
    </mtable>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>𝐚</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="latexml">cases</csymbol>
     <cn type="integer">1</cn>
     <apply>
      <gt></gt>
      <apply>
       <times></times>
       <mtext>if</mtext>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <ci>i</ci>
       </apply>
      </apply>
      <cn type="integer">0</cn>
     </apply>
     <cn type="integer">0</cn>
     <apply>
      <lt></lt>
      <apply>
       <times></times>
       <mtext>if</mtext>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <ci>i</ci>
       </apply>
      </apply>
      <cn type="float">0.</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{a}_{i}=\begin{cases}1&\text{if }w_{i}>0,\\
0&\text{if }w_{i}<0.\end{cases}
  </annotation>
 </semantics>
</math>

 (assuming weights are non-zero)</p>

<p>The <em>maximum weighted sum</em> 

<math display="inline" id="Sparse_distributed_memory:12">
 <semantics>
  <mrow>
   <mi>S</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>w</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>S</ci>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S(w)
  </annotation>
 </semantics>
</math>

 is then the sum of all positive coefficients: 

<math display="inline" id="Sparse_distributed_memory:13">
 <semantics>
  <mrow>
   <mrow>
    <mi>S</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>w</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <msub>
       <mi>w</mi>
       <mi>i</mi>
      </msub>
      <mo>></mo>
      <mn>0</mn>
     </mrow>
    </msub>
    <msub>
     <mi>w</mi>
     <mi>i</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>S</ci>
     <ci>w</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <gt></gt>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <ci>i</ci>
       </apply>
       <cn type="integer">0</cn>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S(w)=\sum_{w_{i}>0}w_{i}
  </annotation>
 </semantics>
</math>

</p>

<p>And the <em>minimum weighted sum</em> 

<math display="inline" id="Sparse_distributed_memory:14">
 <semantics>
  <mrow>
   <mi>s</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>w</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>s</ci>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s(w)
  </annotation>
 </semantics>
</math>

 would correspond to a point opposite the neuron address a`

<math display="block" id="Sparse_distributed_memory:15">
 <semantics>
  <mrow>
   <mrow>
    <mi>s</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>w</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <munder>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mrow>
      <msub>
       <mi>w</mi>
       <mi>i</mi>
      </msub>
      <mo><</mo>
      <mn>0</mn>
     </mrow>
    </munder>
    <msub>
     <mi>w</mi>
     <mi>i</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>s</ci>
     <ci>w</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <apply>
       <lt></lt>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <ci>i</ci>
       </apply>
       <cn type="integer">0</cn>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s(w)=\sum_{w_{i}<0}w_{i}
  </annotation>
 </semantics>
</math>

</p>

<p>When the threshold c is in range 

<math display="inline" id="Sparse_distributed_memory:16">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mo>∈</mo>
   <mi>N</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>d</ci>
    <ci>N</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d\in N
  </annotation>
 </semantics>
</math>

). SDM assumes that the address patterns actually describing physical situations of interest are <em>sparsely</em> scattered throughout the input space. It is impossible to reserve a separate physical location corresponding to each possible input; SDM implements only a limited number of physical or <em>hard</em> locations. The physical location is called a memory (or <em>hard</em>) location.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></p>

<p>Every hard location has associated with it two items:</p>
<ul>
<li>a fixed hard address, which is the N-bit address of the location</li>
<li>a contents portion that is M-bits wide and that can accumulate multiple M-bit data patterns written into the location. The contents' portion is not fixed; it is modified by data patterns written into the memory.</li>
</ul>

<p>In SDM a word could be stored in memory by writing it in a free storage location and at the same time providing the location with the appropriate address decoder. A neuron as an address decoder would select a location based on similarity of the location's address to the retrieval cue. Unlike conventional <a href="Turing_machines" title="wikilink">Turing machines</a> SDM is taking advantage of <em>parallel computing by the address decoders</em>. The mere <em>accessing the memory</em> is regarded as computing, the amount of which increases with memory size.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></p>
<h3 id="address-pattern">Address Pattern</h3>

<p>An N-bit vector used in writing to and reading from the memory. The address pattern is a coded description of an environmental state. (e.g. N = 256.)</p>
<h3 id="data-pattern">Data Pattern</h3>

<p>An M-bit vector that is the object of the writing and reading operations. Like the address pattern, it is a coded description of an environmental state. (e.g. M = 256.)</p>
<h3 id="pointer-chains">Pointer chains</h3>

<p>All of the items are linked in a single list (or array) of pointers to memory locations, and are stored in RAM. Each address in an array points to an individual line in the memory. That line is then returned if it is similar to other lines. Neurons are utilized as address decoders and encoders, similar to the way neurons work in the brain, and return items from the array that match or are similar.</p>
<h3 id="writing">Writing</h3>

<p>Writing is the operation of storing a data pattern into the memory using a particular address pattern. During a write, the input to the memory consists of an address pattern and a data pattern. The address pattern is used to select <em>hard</em> memory locations whose hard addresses are within a certain cutoff distance from the address pattern. The data pattern is stored into each of the selected locations.</p>
<h3 id="reading">Reading</h3>

<p>Reading is the operation of retrieving a data pattern from the memory using a particular address pattern. During a read, an address pattern is used to select a certain number of <em>hard</em> memory locations (just like during a write). The contents of the selected locations are bitwise summed and thresholded to derive an M-bit data pattern. This serves as the output read from the memory.</p>
<h3 id="critical-distance">Critical Distance</h3>

<p>The critical distance of a Sparse Distributed Memory can be approximately evaluated minimizing the following equation with the restriction 

<math display="inline" id="Sparse_distributed_memory:17">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mo>⩽</mo>
   <mi>n</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <ci>d</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d\leqslant n
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Sparse_distributed_memory:18">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>f</mi>
    <mo stretchy="false">~</mo>
   </mover>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>d</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo>{</mo>
     <mfrac>
      <mn>1</mn>
      <mn>2</mn>
     </mfrac>
     <mo>⋅</mo>
     <mrow>
      <mo>[</mo>
      <mn>1</mn>
      <mo>-</mo>
      <mi>N</mi>
      <mrow>
       <mo>(</mo>
       <mi>z</mi>
       <mo><</mo>
       <mfrac>
        <mrow>
         <mrow>
          <mi>w</mi>
          <mo>⋅</mo>
          <mi>s</mi>
         </mrow>
         <mi>h</mi>
         <mi>a</mi>
         <mi>r</mi>
         <mi>e</mi>
         <mi>d</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>d</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <msqrt>
         <mi>θ</mi>
        </msqrt>
       </mfrac>
       <mo>)</mo>
      </mrow>
      <mo>+</mo>
      <mi>N</mi>
      <mrow>
       <mo>(</mo>
       <mi>z</mi>
       <mo><</mo>
       <mfrac>
        <mrow>
         <mo>-</mo>
         <mrow>
          <mrow>
           <mi>w</mi>
           <mo>⋅</mo>
           <mi>s</mi>
          </mrow>
          <mi>h</mi>
          <mi>a</mi>
          <mi>r</mi>
          <mi>e</mi>
          <mi>d</mi>
          <mrow>
           <mo stretchy="false">(</mo>
           <mi>d</mi>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
        </mrow>
        <msqrt>
         <mi>θ</mi>
        </msqrt>
       </mfrac>
       <mo>)</mo>
      </mrow>
      <mo>]</mo>
     </mrow>
     <mo>-</mo>
     <mfrac>
      <mi>d</mi>
      <mi>n</mi>
     </mfrac>
     <mo>}</mo>
    </mrow>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <ci>normal-~</ci>
     <ci>f</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">d</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <ci>normal-{</ci>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <cn type="integer">2</cn>
      </apply>
      <ci>normal-⋅</ci>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-[</ci>
       <cn type="integer">1</cn>
       <minus></minus>
       <csymbol cd="unknown">N</csymbol>
       <cerror>
        <csymbol cd="ambiguous">fragments</csymbol>
        <ci>normal-(</ci>
        <csymbol cd="unknown">z</csymbol>
        <lt></lt>
        <apply>
         <divide></divide>
         <apply>
          <times></times>
          <apply>
           <ci>normal-⋅</ci>
           <ci>w</ci>
           <ci>s</ci>
          </apply>
          <ci>h</ci>
          <ci>a</ci>
          <ci>r</ci>
          <ci>e</ci>
          <ci>d</ci>
          <ci>d</ci>
         </apply>
         <apply>
          <root></root>
          <ci>θ</ci>
         </apply>
        </apply>
        <ci>normal-)</ci>
       </cerror>
       <plus></plus>
       <csymbol cd="unknown">N</csymbol>
       <cerror>
        <csymbol cd="ambiguous">fragments</csymbol>
        <ci>normal-(</ci>
        <csymbol cd="unknown">z</csymbol>
        <lt></lt>
        <apply>
         <divide></divide>
         <apply>
          <minus></minus>
          <apply>
           <times></times>
           <apply>
            <ci>normal-⋅</ci>
            <ci>w</ci>
            <ci>s</ci>
           </apply>
           <ci>h</ci>
           <ci>a</ci>
           <ci>r</ci>
           <ci>e</ci>
           <ci>d</ci>
           <ci>d</ci>
          </apply>
         </apply>
         <apply>
          <root></root>
          <ci>θ</ci>
         </apply>
        </apply>
        <ci>normal-)</ci>
       </cerror>
       <ci>normal-]</ci>
      </cerror>
      <minus></minus>
      <apply>
       <divide></divide>
       <ci>d</ci>
       <ci>n</ci>
      </apply>
      <ci>normal-}</ci>
     </cerror>
     <cn type="integer">2</cn>
    </apply>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \tilde{f}(d)=\left\{\frac{1}{2}\cdot\left[1-N\left(z<\frac{w\cdot shared(d)}{%
\sqrt{\theta}}\right)+N\left(z<\frac{-w\cdot shared(d)}{\sqrt{\theta}}\right)%
\right]-\frac{d}{n}\right\}^{2}
  </annotation>
 </semantics>
</math>

. The proof can be found in,<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a><a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> 

<math display="inline" id="Sparse_distributed_memory:19">
 <semantics>
  <mi>d</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d
  </annotation>
 </semantics>
</math>

</p>

<p>Where:</p>
<ul>
<li>

<math display="inline" id="Sparse_distributed_memory:20">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>

: is the distance to the target;</li>
<li>

<math display="inline" id="Sparse_distributed_memory:21">
 <semantics>
  <mi>s</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>s</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s
  </annotation>
 </semantics>
</math>

: is the number of hard-locations activated during read and write operations (this value depends on access radius values);</li>
<li>

<math display="inline" id="Sparse_distributed_memory:22">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H
  </annotation>
 </semantics>
</math>

: is the number of total stored bitstrings in memory;</li>
<li>

<math display="inline" id="Sparse_distributed_memory:23">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

: is the number of hard-locations in memory;</li>
<li>

<math display="inline" id="Sparse_distributed_memory:24">
 <semantics>
  <mi>θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta
  </annotation>
 </semantics>
</math>

: is the number of times the target bitstring was written in memory;</li>
<li>

<math display="inline" id="Sparse_distributed_memory:25">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>

: is the total of random bitstrings in all 

<math display="inline" id="Sparse_distributed_memory:26">
 <semantics>
  <mrow>
   <mi>s</mi>
   <mi>h</mi>
   <mi>a</mi>
   <mi>r</mi>
   <mi>e</mi>
   <mi>d</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>d</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>s</ci>
    <ci>h</ci>
    <ci>a</ci>
    <ci>r</ci>
    <ci>e</ci>
    <ci>d</ci>
    <ci>d</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   shared(d)
  </annotation>
 </semantics>
</math>

 hard-locations activated by a read operation;</li>
<li>

<math display="inline" id="Sparse_distributed_memory:27">
 <semantics>
  <mi>d</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d
  </annotation>
 </semantics>
</math>

: is the mean number of shared hard-locations activated by two bitstrings 

<math display="inline" id="Sparse_distributed_memory:28">
 <semantics>
  <mrow>
   <mi>N</mi>
   <mo>=</mo>
   <msup>
    <mn>2</mn>
    <mi>n</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>N</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cn type="integer">2</cn>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N=2^{n}
  </annotation>
 </semantics>
</math>

 bits away from each other. One can find some values for a 1000-dimensional SDM in Kanerva's book, Table 7.1, p. 63, or the equations to calculate to any SDM in Appendix B, p. 125 of the same book.</li>
</ul>
<h2 id="probabilistic-interpretation">Probabilistic Interpretation</h2>

<p>An <a href="associative_memory" title="wikilink">associative memory</a> system using sparse, distributed representations can be reinterpreted as an <a href="Importance_sampling" title="wikilink">importance sampler</a>, a <a href="Monte_Carlo_method" title="wikilink">Monte Carlo</a> method of approximating <a href="Bayesian_inference" title="wikilink">Bayesian inference</a>.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> The SDM can be considered a Monte Carlo approximation to a multidimensional <a href="conditional_probability" title="wikilink">conditional probability</a> integral. The SDM will produce acceptable responses from a training set when this approximation is valid, that is, when the training set contains sufficient data to provide good estimates of the underlying <a href="Joint_probability_distribution" title="wikilink">joint probabilities</a> and there are enough Monte Carlo samples to obtain an accurate estimate of the integral.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></p>
<h2 id="biological-plausibility">Biological plausibility</h2>

<p><a href="Sparse_coding" title="wikilink">Sparse coding</a> may be a general strategy of neural systems to augment memory capacity. To adapt to their environments, animals must learn which stimuli are associated with rewards or punishments and distinguish these reinforced stimuli from similar but irrelevant ones. Such task requires implementing stimulus-specific <a href="Associative_memory" title="wikilink">associative memories</a> in which only a few neurons out of a <a href="Neural_ensemble" title="wikilink">population</a> respond to any given stimulus and each neuron responds to only a few stimuli out of all possible stimuli.</p>

<p>Theoretical work on SDM by Kanerva has suggested that sparse coding increases the capacity of associative memory by reducing overlap between representations. Experimentally, sparse representations of sensory information have been observed in many systems, including vision,<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> audition,<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a> touch,<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> and olfaction.<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> However, despite the accumulating evidence for widespread sparse coding and theoretical arguments for its importance, a demonstration that sparse coding improves the stimulus-specificity of associative memory has been lacking until recently.</p>

<p>Some progress has been made in 2014 by <a href="Gero_Miesenböck" title="wikilink">Gero Miesenböck</a>'s lab at the <a href="University_of_Oxford" title="wikilink">University of Oxford</a> analyzing <a class="uri" href="Drosophila" title="wikilink">Drosophila</a> <a href="Olfactory_system" title="wikilink">Olfactory system</a>.<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a> In Drosophila, sparse odor coding by the <a href="Kenyon_cell" title="wikilink">Kenyon cells</a> of the <a href="Mushroom_bodies" title="wikilink">mushroom body</a> is thought to generate a large number of precisely addressable locations for the storage of odor-specific memories. Lin et al.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a> demonstrated that sparseness is controlled by a negative feedback circuit between Kenyon cells and the <a class="uri" href="GABAergic" title="wikilink">GABAergic</a> anterior paired lateral (APL) neuron. Systematic activation and blockade of each leg of this feedback circuit show that Kenyon cells activate APL and APL inhibits Kenyon cells. Disrupting the Kenyon cell-APL feedback loop decreases the sparseness of Kenyon cell odor responses, increases inter-odor correlations, and prevents flies from learning to discriminate similar, but not dissimilar, odors. These results suggest that feedback inhibition suppresses Kenyon cell activity to maintain sparse, decorrelated odor coding and thus the odor-specificity of memories.</p>
<h2 id="applications">Applications</h2>

<p>In applications of the memory, the words are patterns of features. Some features are produced by a sensory system, others control a motor system. There is a <em>current pattern</em> (of e.g. 1000 bits), which is the current contents of the system's <em>focus</em>. The sensors feed into the focus, the motors are driven from the focus, and the memory is accessed through the focus.</p>

<p>What goes on in the world-the system's "subjective" experience-is represented internally by a sequence of patterns in the focus. The memory stores this sequence and can recreate it later in the focus if addressed with a pattern similar to one to one encountered in the past. Thus, the memory learns to <em>predict</em> what is about to happen. Wide applications of the memory would be in systems that deal with real-world information in real time.</p>

<p>The applications include <a href="computer_vision" title="wikilink">vision</a> - detecting and identifying objects in a scene and anticipating subsequent scenes - <a class="uri" href="robotics" title="wikilink">robotics</a>, <a href="signal_processing" title="wikilink">signal detection and verification</a>, and adaptive <a href="machine_learning" title="wikilink">learning</a> and <a href="Adaptive_control" title="wikilink">control</a>. On the theoretical side, the working of the memory may help us understand <a class="uri" href="memory" title="wikilink">memory</a> and <a class="uri" href="learning" title="wikilink">learning</a> in humans and animals.<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a><a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a></p>
<h3 id="the-best-match-search">The Best Match Search</h3>

<p>SDM can be applied to the problem of finding the <em>best match</em> to a test word in a dataset of stored words.<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a><a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a> or, in other words, the <a href="Nearest_neighbor_search" title="wikilink">Nearest neighbor search</a> problem.</p>

<p>Consider a memory with N locations where 

<math display="inline" id="Sparse_distributed_memory:29">
 <semantics>
  <mrow>
   <mo stretchy="false">|</mo>
   <mi>x</mi>
   <mo stretchy="false">|</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <abs></abs>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   |x|
  </annotation>
 </semantics>
</math>

. Let each location have the capacity for one <em>n</em>-bit word (e.g. N= 2<sup>100</sup> 100-bit words), and let the address decoding be done by N address decoder neurons. Set the threshold of each neuron <em>x</em> to its maximum weighted sum 

<math display="inline" id="Sparse_distributed_memory:30">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>-</mo>
   <mrow>
    <mo stretchy="false">|</mo>
    <mi>d</mi>
    <mo stretchy="false">|</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <ci>x</ci>
    <apply>
     <abs></abs>
     <ci>d</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x-|d|
  </annotation>
 </semantics>
</math>

 and use a common parameter <em>d</em> to adjust all thresholds when accessing the memory. The effective threshold of neuron <em>x</em> will be then 

<math display="inline" id="Sparse_distributed_memory:31">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>d</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d=0
  </annotation>
 </semantics>
</math>

 which means that the location <em>x</em> is accessible every time the address <em>x</em> is within <em>d</em> bits of the address presented to memory (i.e. the address held by the address register). With 

<math display="inline" id="Sparse_distributed_memory:32">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mo>=</mo>
   <mi>n</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>d</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d=n
  </annotation>
 </semantics>
</math>

 we have a conventional <a href="random-access_memory" title="wikilink">random-access memory</a>. Assume further that each location has a special <em>location-occupied</em> bit that can be accessed in the same way as the regular datum bits. Writing a word to a location sets this <em>location-occupied</em> bit. Assume that only occupied location can be read.</p>

<p>To file the data in memory , start by setting 

<math display="inline" id="Sparse_distributed_memory:33">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>d</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d=0
  </annotation>
 </semantics>
</math>

 and issue a command to clear the <em>location-occupied</em> bit. This single operation marks all memory as unoccupied regardless of the values of the address register. Then set 

<math display="inline" id="Sparse_distributed_memory:34">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>d</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d=0
  </annotation>
 </semantics>
</math>

 and write each word <em>y</em> of the data set with <em>y</em> itself as the address. Notice that each write operation affects only one location: the location <em>y</em>. Filing time is thus proportional to the number of words in the dataset.</p>

<p>Finding the best match for a test word <em>z</em>, involves placing <em>z</em> in the address register and finding the least distance <em>d</em> for which there is an occupied location. We can start the search by setting 

<math display="inline" id="Sparse_distributed_memory:35">
 <semantics>
  <mrow>
   <mi>n</mi>
   <mo>/</mo>
   <mn>2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <ci>n</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n/2
  </annotation>
 </semantics>
</math>

 and incrementing <em>d</em> successively until an occupied location is found. This method gives average search times that are proportional to the number of address bits or slightly less than 

<math display="inline" id="Sparse_distributed_memory:36">
 <semantics>
  <mrow>
   <mi>n</mi>
   <mo>/</mo>
   <mn>2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <ci>n</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n/2
  </annotation>
 </semantics>
</math>

<a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a> because the nearest occupied location can be expected to be just under 

<math display="block" id="Sparse_distributed_memory:37">
 <semantics>
  <mrow>
   <mrow>
    <mpadded lspace="-1.7pt" width="-1.7pt">
     <mi>f</mi>
    </mpadded>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>+</mo>
    <msup>
     <mi>e</mi>
     <mrow>
      <mo>-</mo>
      <mrow>
       <mi>a</mi>
       <mi>x</mi>
      </mrow>
     </mrow>
    </msup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <plus></plus>
     <cn type="integer">1</cn>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>e</ci>
      <apply>
       <minus></minus>
       <apply>
        <times></times>
        <ci>a</ci>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \!f(x)=1+e^{-ax}
  </annotation>
 </semantics>
</math>

 bits from <em>z</em> (with <a href="binary_search" title="wikilink">binary search</a> on <em>d</em> this would be O(log(n)).</p>

<p>With 100-bit words 2<sup>100</sup> locations would be needed, i.e. an enormously large memory. However <em>if we construct the memory as we store the words of the dataset</em> we need only one location (and one address decoder) for each word of the data set. None of the unoccupied locations need be present. This represents the aspect of <em>sparseness</em> in SDM.</p>
<h3 id="speech-recognition"><a href="Speech_recognition" title="wikilink">Speech recognition</a></h3>

<p>SDM can be applied in transcribing <a class="uri" href="speech" title="wikilink">speech</a>, with the training consisting of "listening" to a large corpus of spoken <a class="uri" href="language" title="wikilink">language</a>. Two hard problems with natural speech are how to detect word boundaries and how to adjust to different speakers. The memory should be able to handle both. First, it stores sequences of patterns as pointer chains. In training—in listening to speech—it will build a probabilistic structure with the highest incidence of branching at word boundaries. In transcribing speech, these branching points are detected and tend to break the stream into segments that correspond to words. Second, the memory's sensitivity to similarity is its mechanism for adjusting to different speakers—and to the variations in the voice of the same speaker.<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a></p>
<h3 id="realizing-forgetting">"Realizing forgetting"</h3>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p><strong>Decay Functions</strong></p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><figure><b>(Figure)</b>
<figcaption>The exponential decay function</figcaption>
</figure></td>
</tr>
<tr class="even">
<td style="text-align: left;"><figure><b>(Figure)</b>
<figcaption>The negated-translated sigmoid function</figcaption>
</figure></td>
</tr>
</tbody>
</table></td>
</tr>
</tbody>
</table>

<p>At the University of Memphis, Uma Ramamurthy, Sidney K. D’Mello, and Stan Franklin created a modified version of the sparse distributed memory system that represents "realizing forgetting." It uses a decay equation to better show interference in data. The sparse distributed memory system distributes each pattern into approximately one hundredth of the locations, so interference can have detrimental results.<a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a></p>

<p>Two possible examples of decay from this modified sparse distributed memory are presented</p>

<p><strong>Exponential decay mechanism</strong>

<math display="block" id="Sparse_distributed_memory:38">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <mrow>
     <mo stretchy="false">[</mo>
     <mfrac>
      <mn>1</mn>
      <mrow>
       <mn>1</mn>
       <mo>+</mo>
       <msup>
        <mi>e</mi>
        <mrow>
         <mo>-</mo>
         <mrow>
          <mi>a</mi>
          <mrow>
           <mo stretchy="false">(</mo>
           <mrow>
            <mi>x</mi>
            <mo>-</mo>
            <mi>c</mi>
           </mrow>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
        </mrow>
       </msup>
      </mrow>
     </mfrac>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <apply>
        <plus></plus>
        <cn type="integer">1</cn>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>e</ci>
         <apply>
          <minus></minus>
          <apply>
           <times></times>
           <ci>a</ci>
           <apply>
            <minus></minus>
            <ci>x</ci>
            <ci>c</ci>
           </apply>
          </apply>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)=1-[\frac{1}{1+e^{-a(x-c)}}]
  </annotation>
 </semantics>
</math>

</p>

<p><strong>Negated-translated sigmoid decay mechanism</strong><span class="LaTeX">$$f(x)=1-[\frac{1}{1+e^{-a(x-c)}}]$$</span></p>

<p>In the exponential decay function, it approaches zero more quickly as <em>x</em> increases, and <em>a</em> is a constant(usually between 3-9) and <em>c</em> is a counter. For the negated-<a href="Translation_(geometry)" title="wikilink">translated</a> <a href="sigmoid_function" title="wikilink">sigmoid function</a>, the decay is similar to the exponential decay function when <em>a</em> is greater than 4.<a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a></p>

<p>As the graph approaches 0, it represents how the memory is being forgotten using decay mechanisms.</p>
<h3 id="genetic-memory">Genetic memory</h3>

<p><a href="Genetic_memory_(computer_science)" title="wikilink">Genetic memory</a> uses genetic algorithm and sparse distributed memory as an artificial neural network. It has been considered for use in creating artificial life.<a class="footnoteRef" href="#fn32" id="fnref32"><sup>32</sup></a></p>
<h3 id="statistical-prediction">Statistical Prediction</h3>

<p>SDM has been applied to statistical <a class="uri" href="prediction" title="wikilink">prediction</a>, the task of associating extremely large perceptual state vectors with future events. In conditions of near- or over- capacity, where the associative memory behavior of the model breaks down, the processing performed by the model can be interpreted as that of a statistical predictor and each data counter in an SDM can be viewed as an independent estimate of the conditional probability of a binary function f being equal to the activation set defined by the counter's memory location.<a class="footnoteRef" href="#fn33" id="fnref33"><sup>33</sup></a></p>
<h3 id="lida">LIDA</h3>

<p><a href="LIDA_(cognitive_architecture)" title="wikilink">LIDA</a> uses sparse distributed memory to help model <a class="uri" href="cognition" title="wikilink">cognition</a> in biological systems. The sparse distributed memory places space is recalling or recognizing the object that it has in relation to other objects. It was developed by Stan Franklin, the creator of the "realizing forgetting" modified sparse distributed memory system.<a class="footnoteRef" href="#fn34" id="fnref34"><sup>34</sup></a> Transient episodic and declarative memories have distributed representations in LIDA (based on modified version of SDM<a class="footnoteRef" href="#fn35" id="fnref35"><sup>35</sup></a>), there is evidence that this is also the case in the nervous system.<a class="footnoteRef" href="#fn36" id="fnref36"><sup>36</sup></a> (Also see <a href="Cognitive_architecture" title="wikilink">Cognitive architecture</a>)</p>
<h3 id="cmatie">CMatie</h3>

<p><a class="uri" href="CMatie" title="wikilink">CMatie</a> is a <a href="Artificial_consciousness" title="wikilink">‘conscious’</a> software agent developed to manage seminar announcements in the Mathematical Sciences Department at the <a href="University_of_Memphis" title="wikilink">University of Memphis</a>. It's based on SDM augmented with the use of <a href="genetic_algorithm" title="wikilink">genetic algorithms</a> as an <a href="associative_memory" title="wikilink">associative memory</a>.<a class="footnoteRef" href="#fn37" id="fnref37"><sup>37</sup></a></p>
<h3 id="htm">HTM</h3>

<p><a href="Hierarchical_temporal_memory" title="wikilink">Hierarchical temporal memory</a> utilizes SDM for storing sparse distributed representations of the data.</p>
<h2 id="extensions">Extensions</h2>

<p>Many extensions and improvements to SDM have been proposed, e.g.:</p>
<ul>
<li>Ternary memory space. This enables the memory to be used as a Transient Episodic Memory (TEM) in <a href="Cognitive_architecture" title="wikilink">cognitive software agents</a>. TEM is a memory with high specificity and low retention, used for events having features of a particular time and place.<a class="footnoteRef" href="#fn38" id="fnref38"><sup>38</sup></a><a class="footnoteRef" href="#fn39" id="fnref39"><sup>39</sup></a></li>
<li>Integer SDM that uses modular arithmetic integer vectors rather than binary vectors. This extension improves the representation capabilities of the memory and is more robust over normalization. It can also be extended to support forgetting and reliable sequence storage.<a class="footnoteRef" href="#fn40" id="fnref40"><sup>40</sup></a></li>
</ul>
<ul>
<li>Using word vectors of larger size than address vectors. This extension preserves many of the desirable properties of the original SDM: auto-associability, content addressability, distributed storage and robustness over noisy inputs. In addition, it adds new functionality, enabling an efficient auto-associative storage of sequences of vectors, as well as of other data structures such as trees.<a class="footnoteRef" href="#fn41" id="fnref41"><sup>41</sup></a></li>
</ul>
<h2 id="related-models">Related models</h2>
<ul>
<li><a href="Nearest_neighbor_search" title="wikilink">Approximate nearest neighbor search</a><a class="footnoteRef" href="#fn42" id="fnref42"><sup>42</sup></a></li>
<li><a href="Associative_Neural_Memories" title="wikilink">Associative Neural Memories</a><a class="footnoteRef" href="#fn43" id="fnref43"><sup>43</sup></a></li>
<li><a href="Autoassociative_memory" title="wikilink">Autoassociative memory</a></li>
<li><a href="Binary_Spatter_Codes" title="wikilink">Binary Spatter Codes</a><a class="footnoteRef" href="#fn44" id="fnref44"><sup>44</sup></a></li>
<li><a href="Cerebellar_model_articulation_controller" title="wikilink">Associative-memory models of the cerebellum</a></li>
<li><a href="Content-addressable_memory" title="wikilink">Content-addressable memory</a></li>
<li><a href="Correlation-matrix_memories" title="wikilink">Correlation-matrix memories</a> <a class="footnoteRef" href="#fn45" id="fnref45"><sup>45</sup></a></li>
<li><a href="Deep_Learning#Memory_networks" title="wikilink">Deep Learning#Memory networks</a></li>
<li><a href="Dynamic_Memory_Networks" title="wikilink">Dynamic Memory Networks</a> <a class="footnoteRef" href="#fn46" id="fnref46"><sup>46</sup></a></li>
<li><a href="Feedforward_neural_network" title="wikilink">Feedforward neural network</a></li>
<li><a href="Hierarchical_temporal_memory" title="wikilink">Hierarchical temporal memory</a></li>
<li><a href="Holographic_Reduced_Representation" title="wikilink">Holographic Reduced Representation</a><a class="footnoteRef" href="#fn47" id="fnref47"><sup>47</sup></a><a class="footnoteRef" href="#fn48" id="fnref48"><sup>48</sup></a></li>
<li><a href="Low-density_parity-check_code" title="wikilink">Low-density parity-check code</a></li>
<li><a href="Locality-sensitive_hashing" title="wikilink">Locality-sensitive hashing</a></li>
<li><a href="Memory_networks" title="wikilink">Memory networks</a> <a class="footnoteRef" href="#fn49" id="fnref49"><sup>49</sup></a></li>
<li><a href="Memory-prediction_framework" title="wikilink">Memory-prediction framework</a></li>
<li><a href="Random-access_memory" title="wikilink">Random-access memory</a> (as a special case of SDM)<a class="footnoteRef" href="#fn50" id="fnref50"><sup>50</sup></a></li>
<li><a href="Random_indexing" title="wikilink">Random indexing</a><a class="footnoteRef" href="#fn51" id="fnref51"><sup>51</sup></a></li>
<li><a href="Recursive_Auto-Associative_Memory_(RAAM)" title="wikilink">Recursive Auto-Associative Memory (RAAM)</a><a class="footnoteRef" href="#fn52" id="fnref52"><sup>52</sup></a></li>
<li><a href="Self-organizing_map" title="wikilink">Self-organizing map</a></li>
<li><a href="Semantic_hashing" title="wikilink">Semantic hashing</a> <a class="footnoteRef" href="#fn53" id="fnref53"><sup>53</sup></a></li>
<li><a href="Semantic_memory" title="wikilink">Semantic memory</a></li>
<li><a href="Semantic_network" title="wikilink">Semantic network</a></li>
<li><a href="Semantic_Pointer_Architecture" title="wikilink">Semantic Pointer Architecture</a><a class="footnoteRef" href="#fn54" id="fnref54"><sup>54</sup></a></li>
<li><a href="Sequence_Memory" title="wikilink">Sequence Memory</a><a class="footnoteRef" href="#fn55" id="fnref55"><sup>55</sup></a></li>
<li><a href="Neural_coding" title="wikilink">Sparse coding</a><a class="footnoteRef" href="#fn56" id="fnref56"><sup>56</sup></a></li>
<li><a href="Hierarchical_temporal_memory#Sparse_distributed_representations" title="wikilink">Sparse Distributed Representations</a></li>
<li><a href="Neural_Turing_machine" title="wikilink">Neural Turing machine</a><a class="footnoteRef" href="#fn57" id="fnref57"><sup>57</sup></a></li>
<li>Stacked <a href="autoencoder" title="wikilink">autoencoders</a><a class="footnoteRef" href="#fn58" id="fnref58"><sup>58</sup></a></li>
<li><a href="Vector_Symbolic_Architecture" title="wikilink">Vector Symbolic Architecture</a><a class="footnoteRef" href="#fn59" id="fnref59"><sup>59</sup></a></li>
<li><a href="Vector_space_model" title="wikilink">Vector space model</a></li>
<li><a href="Virtual_memory" title="wikilink">Virtual memory</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a class="uri" href="Category:Memory" title="wikilink">Category:Memory</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3">Flynn, Michael J., Pentti Kanerva, and Neil Bhadkamkar. "Sparse distributed memory prototype: principles and operation." (1989).<a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"></li>
<li id="fn8">Kandel, Eric R., James H. Schwartz, and Thomas M. Jessell, eds. Principles of neural science. Vol. 4. New York: McGraw-Hill, 2000.<a href="#fnref8">↩</a></li>
<li id="fn9">Eccles, John G. "Under the Spell of the Synapse." The Neurosciences: Paths of Discovery, I. Birkhäuser Boston, 1992. 159-179.<a href="#fnref9">↩</a></li>
<li id="fn10">McCulloch, Warren S., and Walter Pitts. "A logical calculus of the ideas immanent in nervous activity." The bulletin of mathematical biophysics 5.4 (1943): 115-133.<a href="#fnref10">↩</a></li>
<li id="fn11"></li>
<li id="fn12"></li>
<li id="fn13"></li>
<li id="fn14"><a href="#fnref14">↩</a></li>
<li id="fn15"><a href="#fnref15">↩</a></li>
<li id="fn16">Abbott, Joshua T., Jessica B. Hamrick, and Thomas L. Griffiths. "Approximating Bayesian inference with a sparse distributed memory system." Proceedings of the 35th annual conference of the cognitive science society. 2013.<a href="#fnref16">↩</a></li>
<li id="fn17">Anderson, Charles H. "A conditional probability interpretation of Kanerva's sparse distributed memory." Neural Networks, 1989. IJCNN., International Joint Conference on. IEEE, 1989.<a href="#fnref17">↩</a></li>
<li id="fn18">Vinje WE, Gallant JL. Sparse coding and decorrelation in primary visual cortex during natural vision. Science. 2000; 287:1273–1276. [PubMed: 10678835]<a href="#fnref18">↩</a></li>
<li id="fn19">Hromádka T, Deweese MR, Zador AM. Sparse representation of sounds in the unanesthetized auditory cortex. PLoS Biol. 2008; 6:e16. [PubMed: 18232737]<a href="#fnref19">↩</a></li>
<li id="fn20">Crochet S, Poulet JFA, Kremer Y, Petersen CCH. Synaptic mechanisms underlying sparse coding of active touch. Neuron. 2011; 69:1160–1175. [PubMed: 21435560]<a href="#fnref20">↩</a></li>
<li id="fn21">Ito I, Ong RCY, Raman B, Stopfer M. Sparse odor representation and olfactory learning. Nat Neurosci. 2008; 11:1177–1184. [PubMed: 18794840]<a href="#fnref21">↩</a></li>
<li id="fn22">A sparse memory is a precise memory. Oxford Science blog. 28 Feb 2014. <a class="uri" href="http://www.ox.ac.uk/news/science-blog/sparse-memory-precise-memory">http://www.ox.ac.uk/news/science-blog/sparse-memory-precise-memory</a><a href="#fnref22">↩</a></li>
<li id="fn23">Lin, Andrew C., et al. "Sparse, decorrelated odor coding in the mushroom body enhances learned odor discrimination." Nature neuroscience 17.4 (2014): 559-568.<a href="#fnref23">↩</a></li>
<li id="fn24"></li>
<li id="fn25">Denning, Peter J. Sparse distributed memory. Research Institute for Advanced Computer Science [NASA Ames Research Center], 1989.<a href="#fnref25">↩</a></li>
<li id="fn26"></li>
<li id="fn27">Minsky, Marvin, and Papert Seymour. "Perceptrons." (1969). "Time vs. memory for best matching - an open problem" p. 222-225<a href="#fnref27">↩</a></li>
<li id="fn28"></li>
<li id="fn29"></li>
<li id="fn30"><a href="#fnref30">↩</a></li>
<li id="fn31"></li>
<li id="fn32"><a href="#fnref32">↩</a></li>
<li id="fn33">Rogers, David. "Statistical prediction with Kanerva's sparse distributed memory." Advances in neural information processing systems. 1989.<a href="#fnref33">↩</a></li>
<li id="fn34">Rao, R. P. N., &amp; Fuentes, O. (1998). <a href="http://www.cs.utep.edu/ofuentes/raoML98.pdf">Hierarchical Learning of Navigational Behaviors in an Autonomous Robot using a Predictive Sparse Distributed Memory</a>. Machine Learning, 31, 87-113<a href="#fnref34">↩</a></li>
<li id="fn35">Franklin, Stan, et al. "The role of consciousness in memory." Brains, Minds and Media 1.1 (2005): 38.<a href="#fnref35">↩</a></li>
<li id="fn36">Shastri, L. 2002. Episodic memory and cortico-hippocampal interactions. Trends in Cognitive Sciences<a href="#fnref36">↩</a></li>
<li id="fn37">Anwar, Ashraf, and Stan Franklin. "Sparse distributed memory for ‘conscious’ software agents." Cognitive Systems Research 4.4 (2003): 339-354.<a href="#fnref37">↩</a></li>
<li id="fn38">D'Mello, Sidney K., Ramamurthy, U., &amp; Franklin, S. 2005. Encoding and Retrieval Efficiency of Episodic Data in a Modified Sparse Distributed Memory System. In Proceedings of the 27th Annual Meeting of the Cognitive Science Society. Stresa, Ital<a href="#fnref38">↩</a></li>
<li id="fn39">Ramamaurthy, U., Sidney K. D'Mello, and Stan Franklin. "Modified sparse distributed memory as transient episodic memory for cognitive software agents." Systems, Man and Cybernetics, 2004 IEEE International Conference on. Vol. 6. IEEE, 2004.<a href="#fnref39">↩</a></li>
<li id="fn40">Snaider, Javier, and Stan Franklin. "Integer sparse distributed memory." Twenty-fifth international flairs conference. 2012.<a href="#fnref40">↩</a></li>
<li id="fn41">Snaider, Javier, and Stan Franklin. "Extended sparse distributed memory and sequence storage." Cognitive Computation 4.2 (2012): 172-180.<a href="#fnref41">↩</a></li>
<li id="fn42">Muja, Marius. "Scalable nearest neighbour methods for high dimensional data." (2013).<a href="#fnref42">↩</a></li>
<li id="fn43">Hassoun, Mohamad H. Associative neural memories. Oxford University Press, Inc., 1993.<a href="#fnref43">↩</a></li>
<li id="fn44">Kanerva, Pentti. "Binary spatter-coding of ordered K-tuples." Artificial Neural Networks—ICANN 96. Springer Berlin Heidelberg, 1996. 869-873.<a href="#fnref44">↩</a></li>
<li id="fn45">Kohonen, Teuvo. "Correlation matrix memories." Computers, IEEE Transactions on 100.4 (1972): 353-359.<a href="#fnref45">↩</a></li>
<li id="fn46">Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury, Robert English, Brian Pierce, Peter Ondruska, Ishaan Gulrajani, Richard Socher. "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing." arXiv preprint arXiv:1506.07285 (2015).<a href="#fnref46">↩</a></li>
<li id="fn47">Plate, Tony A. "Holographic Reduced Representation: Distributed representation for cognitive structures." (2003).<a href="#fnref47">↩</a></li>
<li id="fn48">Kanerva, Pentti. "Computing with 10,000-bit words." Proc. 52nd Annual Allerton Conference on Communication, Control, and Computing. 2014.<a href="#fnref48">↩</a></li>
<li id="fn49">Weston, Jason, Sumit Chopra, and Antoine Bordes. "Memory networks." arXiv preprint arXiv:1410.3916 (2014).<a href="#fnref49">↩</a></li>
<li id="fn50"></li>
<li id="fn51">Joshi, Aditya, Johan Halseth, and Pentti Kanerva. "Language Recognition using Random Indexing." arXiv preprint arXiv:1412.7026 (2014). <a class="uri" href="http://arxiv.org/abs/1412.7026">http://arxiv.org/abs/1412.7026</a><a href="#fnref51">↩</a></li>
<li id="fn52">Pollack, Jordan B. "Recursive distributed representations." Artificial Intelligence 46.1 (1990): 77-105.<a href="#fnref52">↩</a></li>
<li id="fn53">Salakhutdinov, Ruslan, and Geoffrey Hinton. "Semantic hashing." RBM 500.3 (2007): 500.<a href="#fnref53">↩</a></li>
<li id="fn54">Eliasmith, Chris, et al. "A large-scale model of the functioning brain." science 338.6111 (2012): 1202-1205.<a href="#fnref54">↩</a></li>
<li id="fn55">Hawkins, Jeff, Dileep George, and Jamie Niemasik. "Sequence memory for prediction, inference and behaviour." Philosophical Transactions of the Royal Society B: Biological Sciences 364.1521 (2009): 1203-1209.<a href="#fnref55">↩</a></li>
<li id="fn56">Lee, Honglak, et al. "Efficient sparse coding algorithms." Advances in neural information processing systems. 2006.<a href="#fnref56">↩</a></li>
<li id="fn57">Graves, Alex, Greg Wayne, and Ivo Danihelka. "Neural Turing Machines." arXiv preprint arXiv:1410.5401 (2014).<a href="#fnref57">↩</a></li>
<li id="fn58">Vincent, Pascal, et al. "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion." The Journal of Machine Learning Research 11 (2010): 3371-3408.<a href="#fnref58">↩</a></li>
<li id="fn59">Rachkovskij, Dmitri A., and Ernst M. Kussul. "Binding and normalization of binary sparse distributed representations by context-dependent thinning." Neural Computation 13.2 (2001): 411-452.<a href="#fnref59">↩</a></li>
</ol>
</section>
</body>
</html>
