   Similarities between Wiener and LMS      Similarities between Wiener and LMS   The Least mean squares filter solution converges to the Wiener filter solution, assuming that the unknown system is LTI and the noise is stationary . Both filters can be used to identify the impulse response of an unknown system, knowing only the original input signal and the output of the unknown system. By relaxing the error criterion to reduce current sample error instead of minimizing the total error over all of n, the LMS algorithm can be derived from the Wiener filter.  Derivation of the Wiener filter for system identification  Given a known input signal    s   [  n  ]       s   delimited-[]  n     s[n]   , the output of an unknown LTI system    x   [  n  ]       x   delimited-[]  n     x[n]   can be expressed as:       x   [  n  ]    =     ∑   k  =  0    N  -  1      h  k   s   [   n  -  k   ]     +   w   [  n  ]           x   delimited-[]  n        superscript   subscript     k  0      N  1       subscript  h  k   s   delimited-[]    n  k        w   delimited-[]  n       x[n]=\sum_{k=0}^{N-1}h_{k}s[n-k]+w[n]     where    h  k     subscript  h  k    h_{k}   is an unknown filter tap coefficients and    w   [  n  ]       w   delimited-[]  n     w[n]   is noise.  The model system     x  ^    [  n  ]        normal-^  x    delimited-[]  n     \hat{x}[n]   , using a Wiener filter solution with an order N, can be expressed as:        x  ^    [  n  ]    =    ∑   k  =  0    N  -  1       h  ^   k   s   [   n  -  k   ]            normal-^  x    delimited-[]  n      superscript   subscript     k  0      N  1       subscript   normal-^  h   k   s   delimited-[]    n  k        \hat{x}[n]=\sum_{k=0}^{N-1}\hat{h}_{k}s[n-k]     where     h  ^   k     subscript   normal-^  h   k    \hat{h}_{k}   are the filter tap coefficients to be determined.  The error between the model and the unknown system can be expressed as:       e   [  n  ]    =    x   [  n  ]    -    x  ^    [  n  ]           e   delimited-[]  n        x   delimited-[]  n       normal-^  x    delimited-[]  n       e[n]=x[n]-\hat{x}[n]     The total squared error   E   E   E   can be expressed as:      E  =    ∑   n  =   -  ∞    ∞    e    [  n  ]   2         E    superscript   subscript     n            e   superscript   delimited-[]  n   2       E=\sum_{n=-\infty}^{\infty}e[n]^{2}       E  =    ∑   n  =   -  ∞    ∞     (    x   [  n  ]    -    x  ^    [  n  ]     )   2        E    superscript   subscript     n           superscript      x   delimited-[]  n       normal-^  x    delimited-[]  n     2      E=\sum_{n=-\infty}^{\infty}(x[n]-\hat{x}[n])^{2}       E  =    ∑   n  =   -  ∞    ∞    (     x    [  n  ]   2    -   2  x   [  n  ]    x  ^    [  n  ]     +    x  ^     [  n  ]   2     )        E    superscript   subscript     n                x   superscript   delimited-[]  n   2      2  x   delimited-[]  n    normal-^  x    delimited-[]  n        normal-^  x    superscript   delimited-[]  n   2        E=\sum_{n=-\infty}^{\infty}(x[n]^{2}-2x[n]\hat{x}[n]+\hat{x}[n]^{2})     Use the Minimum mean-square error criterion over all of   n   n   n   by setting its gradient to zero:       ∇  E   =  0       normal-∇  E   0    \nabla E=0   which is      ∂  E    ∂    h  ^   i     =  0          E      subscript   normal-^  h   i     0    \frac{\partial E}{\partial\hat{h}_{i}}=0   for all    i  =   0  ,  1  ,  2  ,  …  ,   N  -  1        i   0  1  2  normal-…    N  1      i=0,1,2,...,N-1         ∂  E    ∂    h  ^   i     =    ∂   ∂    h  ^   i       ∑   n  =   -  ∞    ∞    [     x    [  n  ]   2    -   2  x   [  n  ]    x  ^    [  n  ]     +    x  ^     [  n  ]   2     ]             E      subscript   normal-^  h   i             subscript   normal-^  h   i       superscript   subscript     n           delimited-[]        x   superscript   delimited-[]  n   2      2  x   delimited-[]  n    normal-^  x    delimited-[]  n        normal-^  x    superscript   delimited-[]  n   2          \frac{\partial E}{\partial\hat{h}_{i}}=\frac{\partial}{\partial\hat{h}_{i}}%
 \sum_{n=-\infty}^{\infty}[x[n]^{2}-2x[n]\hat{x}[n]+\hat{x}[n]^{2}]     Substitute the definition of     x  ^    [  n  ]        normal-^  x    delimited-[]  n     \hat{x}[n]   :        ∂  E    ∂    h  ^   i     =    ∂   ∂    h  ^   i       ∑   n  =   -  ∞    ∞    [     x    [  n  ]   2    -   2  x   [  n  ]     ∑   k  =  0    N  -  1       h  ^   k   s   [   n  -  k   ]       +    (    ∑   k  =  0    N  -  1       h  ^   k   s   [   n  -  k   ]     )   2    ]             E      subscript   normal-^  h   i             subscript   normal-^  h   i       superscript   subscript     n           delimited-[]        x   superscript   delimited-[]  n   2      2  x   delimited-[]  n     superscript   subscript     k  0      N  1       subscript   normal-^  h   k   s   delimited-[]    n  k         superscript    superscript   subscript     k  0      N  1       subscript   normal-^  h   k   s   delimited-[]    n  k      2         \frac{\partial E}{\partial\hat{h}_{i}}=\frac{\partial}{\partial\hat{h}_{i}}%
 \sum_{n=-\infty}^{\infty}[x[n]^{2}-2x[n]\sum_{k=0}^{N-1}\hat{h}_{k}s[n-k]+(%
 \sum_{k=0}^{N-1}\hat{h}_{k}s[n-k])^{2}]     Distribute the partial derivative:        ∂  E    ∂    h  ^   i     =    ∑   n  =   -  ∞    ∞    [    -   2  x   [  n  ]   s   [   n  -  i   ]     +   2   (    ∑   k  =  0    N  -  1       h  ^   k   s   [   n  -  k   ]     )   s   [   n  -  i   ]     ]            E      subscript   normal-^  h   i       superscript   subscript     n           delimited-[]        2  x   delimited-[]  n   s   delimited-[]    n  i        2    superscript   subscript     k  0      N  1       subscript   normal-^  h   k   s   delimited-[]    n  k      s   delimited-[]    n  i          \frac{\partial E}{\partial\hat{h}_{i}}=\sum_{n=-\infty}^{\infty}[-2x[n]s[n-i]+%
 2(\sum_{k=0}^{N-1}\hat{h}_{k}s[n-k])s[n-i]]     Using the definition of discrete cross-correlation :        R   x  y     (  i  )    =    ∑   n  =   -  ∞    ∞    x   [  n  ]   y   [   n  -  i   ]            subscript  R    x  y    i     superscript   subscript     n            x   delimited-[]  n   y   delimited-[]    n  i        R_{xy}(i)=\sum_{n=-\infty}^{\infty}x[n]y[n-i]         ∂  E    ∂    h  ^   i     =    -   2   R   x  s     [  i  ]     +   2    ∑   k  =  0    N  -  1       h  ^   k    R   s  s     [   i  -  k   ]       =  0            E      subscript   normal-^  h   i           2   subscript  R    x  s     delimited-[]  i       2    superscript   subscript     k  0      N  1       subscript   normal-^  h   k    subscript  R    s  s     delimited-[]    i  k             0     \frac{\partial E}{\partial\hat{h}_{i}}=-2R_{xs}[i]+2\sum_{k=0}^{N-1}\hat{h}_{k%
 }R_{ss}[i-k]=0     Rearrange the terms:        R   x  s     [  i  ]    =    ∑   k  =  0    N  -  1       h  ^   k    R   s  s     [   i  -  k   ]            subscript  R    x  s     delimited-[]  i      superscript   subscript     k  0      N  1       subscript   normal-^  h   k    subscript  R    s  s     delimited-[]    i  k        R_{xs}[i]=\sum_{k=0}^{N-1}\hat{h}_{k}R_{ss}[i-k]   for all    i  =   0  ,  1  ,  2  ,  …  ,   N  -  1        i   0  1  2  normal-…    N  1      i=0,1,2,...,N-1     This system of N equations with N unknowns can be determined.  Derivation of the LMS algorithm  By relaxing the infinite sum of the Wiener filter to just the error at time   n   n   n   , the LMS algorithm can be derived.  The squared error can be expressed as:      E  =    (    d   [  n  ]    -   y   [  n  ]     )   2       E   superscript      d   delimited-[]  n      y   delimited-[]  n     2     E=(d[n]-y[n])^{2}     Using the Minimum mean-square error criterion, take the gradient:        ∂  E    ∂  w    =    ∂   ∂  w      (    d   [  n  ]    -   y   [  n  ]     )   2            E     w           w     superscript      d   delimited-[]  n      y   delimited-[]  n     2      \frac{\partial E}{\partial w}=\frac{\partial}{\partial w}(d[n]-y[n])^{2}     Apply chain rule and substitute definition of y[n]        ∂  E    ∂  w    =   2   (    d   [  n  ]    -   y   [  n  ]     )    ∂   ∂  w     (    d   [  n  ]    -    ∑   k  =  0    N  -  1       w  ^   k   x   [   n  -  k   ]      )            E     w      2      d   delimited-[]  n      y   delimited-[]  n          w        d   delimited-[]  n      superscript   subscript     k  0      N  1       subscript   normal-^  w   k   x   delimited-[]    n  k          \frac{\partial E}{\partial w}=2(d[n]-y[n])\frac{\partial}{\partial w}(d[n]-%
 \sum_{k=0}^{N-1}\hat{w}_{k}x[n-k])         ∂  E    ∂  w    =   -   2   (   e   [  n  ]    )    (   x   [   n  -  i   ]    )             E     w        2    e   delimited-[]  n      x   delimited-[]    n  i         \frac{\partial E}{\partial w}=-2(e[n])(x[n-i])     Using gradient descent and a step size   μ   μ   \mu   :       w   [   n  +  1   ]    =    w   [  n  ]    -   μ    ∂  E    ∂  w            w   delimited-[]    n  1         w   delimited-[]  n      μ      E     w        w[n+1]=w[n]-\mu\frac{\partial E}{\partial w}     which becomes, for i = 0, 1, ..., N-1,       w   [   n  +  1   ]    =    w   [  n  ]    +   2  μ   (   e   [  n  ]    )    (   x   [   n  -  i   ]    )           w   delimited-[]    n  1         w   delimited-[]  n      2  μ    e   delimited-[]  n      x   delimited-[]    n  i         w[n+1]=w[n]+2\mu(e[n])(x[n-i])     This is the LMS update equation.  See also   Wiener filter  Least mean squares filter   References   J.G. Proakis and D.G. Manolakis, Digital Signal Processing: Principles, Algorithms, and Applications, Prentice-Hall, 4th ed., 2007.   "  Category:Digital signal processing  Category:Filter theory   