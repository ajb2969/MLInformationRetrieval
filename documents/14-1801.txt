   Hierarchical Dirichlet process      Hierarchical Dirichlet process   In statistics and machine learning , the hierarchical Dirichlet process (HDP) is a nonparametric  Bayesian approach to clustering grouped data . 1 2 It uses a Dirichlet process for each group of data, with the Dirichlet processes for all groups sharing a base distribution which is itself drawn from a Dirichlet process. This method allows groups to share statistical strength via sharing of clusters across groups. The base distribution being drawn from a Dirichlet process is important, because draws from a Dirichlet process are atomic probability measures, and the atoms will appear in all group-level Dirichlet processes. Since each atom corresponds to a cluster, clusters are shared across all groups. It was developed by Yee Whye Teh , Michael I. Jordan , Matthew J. Beal and David Blei and published in the Journal of the American Statistical Association in 2006. 3  Model  This model description is sourced from. 4 The HDP is a model for grouped data. What this means is that the data items come in multiple distinct groups. For example, in a topic model words are organized into documents, with each document formed by a bag (group) of words (data items). Indexing groups by    j  =   1  ,   …  J        j   1    normal-…  J      j=1,...J   , suppose each group consist of data items     x   j  1    ,   …   x   j  n         subscript  x    j  1      normal-…   subscript  x    j  n       x_{j1},...x_{jn}   .  The HDP is parameterized by a base distribution   H   H   H   which governs the a priori distribution over data items, and a number of concentration parameters which govern the a priori number of clusters and amount of sharing across groups. The   j   j   j   th group is associated with a random probability measure    G  j     subscript  G  j    G_{j}   which has distribution given by a Dirichlet process:          G  j   |   G  0        ∼   DP   (   α  j   ,   G  0   )            fragments   subscript  G  j   normal-|   subscript  G  0     similar-to  absent   DP   subscript  α  j    subscript  G  0        \begin{aligned}\displaystyle G_{j}|G_{0}&\displaystyle\sim\operatorname{DP}(%
 \alpha_{j},G_{0})\end{aligned}     where    α  j     subscript  α  j    \alpha_{j}   is the concentration parameter associated with the group, and    G  0     subscript  G  0    G_{0}   is the base distribution shared across all groups. In turn, the common base distribution is Dirichlet process distributed:         G  0       ∼   DP   (   α  0   ,  H  )            subscript  G  0    similar-to  absent   DP   subscript  α  0   H       \begin{aligned}\displaystyle G_{0}&\displaystyle\sim\operatorname{DP}(\alpha_{%
 0},H)\end{aligned}     with concentration parameter    α  0     subscript  α  0    \alpha_{0}   and base distribution   H   H   H   . Finally, to relate the Dirichlet processes back with the observed data, each data item    x   j  i      subscript  x    j  i     x_{ji}   is associated with a latent parameter    θ   j  i      subscript  θ    j  i     \theta_{ji}   :          θ   j  i    |   G  j        ∼   G  j          x   j  i    |   θ   j  i         ∼   F   (   θ   j  i    )            fragments   subscript  θ    j  i    normal-|   subscript  G  j     similar-to  absent   subscript  G  j       fragments   subscript  x    j  i    normal-|   subscript  θ    j  i      similar-to  absent    F   subscript  θ    j  i         \begin{aligned}\displaystyle\theta_{ji}|G_{j}&\displaystyle\sim G_{j}\\
 \displaystyle x_{ji}|\theta_{ji}&\displaystyle\sim F(\theta_{ji})\end{aligned}     The first line states that each parameter has a prior distribution given by    G  j     subscript  G  j    G_{j}   , while the second line states that each data item has a distribution    F   (   θ   j  i    )       F   subscript  θ    j  i      F(\theta_{ji})   parameterized by its associated parameter. The resulting model above is called a HDP mixture model, with the HDP referring to the hierarchically linked set of Dirichlet processes, and the mixture model referring to the way the Dirichlet processes are related to the data items.  To understand how the HDP implements a clustering model, and how clusters become shared across groups, recall that draws from a Dirichlet process are atomic probability measures with probability one. This means that the common base distribution    G  0     subscript  G  0    G_{0}   has a form which can be written as:         G  0       =     ∑   k  =  1   ∞      π   0  k     δ   θ  k  *              subscript  G  0     absent    superscript   subscript     k  1         subscript  π    0  k     subscript  δ   subscript   superscript  θ    k          \begin{aligned}\displaystyle G_{0}&\displaystyle=\sum_{k=1}^{\infty}\pi_{0k}%
 \delta_{\theta^{*}_{k}}\end{aligned}     where there are an infinite number of atoms,       θ  k  *   ,  k   =  1   ,   2  ,  …      formulae-sequence      subscript   superscript  θ    k   k   1    2  normal-…     \theta^{*}_{k},k=1,2,...   , assuming that the overall base distribution   H   H   H   has infinite support. Each atom is associated with a mass    π   0  k      subscript  π    0  k     \pi_{0k}   . The masses have to sum to one since    G  0     subscript  G  0    G_{0}   is a probability measure. Since    G  0     subscript  G  0    G_{0}   is itself the base distribution for the group specific Dirichlet processes, each    G  j     subscript  G  j    G_{j}   will have atoms given by the atoms of    G  0     subscript  G  0    G_{0}   , and can itself be written in the form:         G  j       =     ∑   k  =  1   ∞      π   j  k     δ   θ  k  *              subscript  G  j     absent    superscript   subscript     k  1         subscript  π    j  k     subscript  δ   subscript   superscript  θ    k          \begin{aligned}\displaystyle G_{j}&\displaystyle=\sum_{k=1}^{\infty}\pi_{jk}%
 \delta_{\theta^{*}_{k}}\end{aligned}     Thus the set of atoms is shared across all groups, with each group having its own group-specific atom masses. Relating this representation back to the observed data, we see that each data item is described by a mixture model:          x   j  i    |   G  j        ∼     ∑   k  =  1   ∞      π   j  k    F   (   θ  k  *   )             fragments   subscript  x    j  i    normal-|   subscript  G  j     similar-to  absent    superscript   subscript     k  1         subscript  π    j  k    F   subscript   superscript  θ    k         \begin{aligned}\displaystyle x_{ji}|G_{j}&\displaystyle\sim\sum_{k=1}^{\infty}%
 \pi_{jk}F(\theta^{*}_{k})\end{aligned}     where the atoms    θ  k  *     subscript   superscript  θ    k    \theta^{*}_{k}   play the role of the mixture component parameters, while the masses    π   j  k      subscript  π    j  k     \pi_{jk}   play the role of the mixing proportions. In conclusion, each group of data is modeled using a mixture model, with mixture components shared across all groups but mixing proportions being group-specific. In clustering terms, we can interpret each mixture component as modeling a cluster of data items, with clusters shared across all groups, and each group, having its own mixing proportions, composed of different combinations of clusters.  Applications  The HDP mixture model is a natural nonparametric generalization of Latent Dirichlet allocation , where the number of topics can be unbounded and learnt from data. 5 Here each group is a document consisting of a bag of words, each cluster is a topic, and each document is a mixture of topics. The HDP is also a core component of the infinite hidden Markov model , 6 which is a nonparametric generalization of the hidden Markov model allowing the number of states to be unbounded and learnt from data.  Generalizations  The HDP can be generalized in a number of directions. The Dirichlet processes can be replaced by Pitman-Yor processes , resulting in the Hierarchical Pitman-Yor process . The hierarchy can be deeper, with multiple levels of groups arranged in a hierarchy. Such an arrangement has been exploited in the sequence memoizer , a Bayesian nonparametric model for sequences which has a multi-level hierarchy of Pitman-Yor processes.  References    "  Category:Stochastic processes  Category:Non-parametric Bayesian methods     ↩  ↩         