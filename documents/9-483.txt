   Qualitative variation      Qualitative variation   An index of qualitative variation (IQV) is a measure of statistical dispersion in nominal distributions . There are a variety of these, but they have been relatively little-studied in the statistics literature. The simplest is the variation ratio , while more complex indices include the information entropy .  Properties  There several types of index used for the analysis of nominal data. Several are standard statistics that are used elsewhere - range , standard deviation , variance , mean deviation , coefficient of variation , median absolute deviation , interquartile range and quartile deviation .  In addition to these several statistics have been developed with nominal data in mind. A number have been summarized and devised by Wilcox , , who requires the following standardization properties to be satisfied:   Variation varies between 0 and 1.  Variation is 0 if and only if all cases belong to a single category.  Variation is 1 if and only if cases are evenly divided across all category. 1   In particular, the value of these standardized indices does not depend on the number of categories or number of samples.  For any index, the closer to uniform the distribution, the larger the variance, and the larger the differences in frequencies across categories, the smaller the variance.  Indices of qualitative variation are then analogous to information entropy , which is minimized when all cases belong to a single category and maximized in a uniform distribution. Indeed, information entropy can be used as an index of qualitative variation.  One characterization of a particular index of qualitative variation (IQV) is as a ratio of observed differences to maximum differences.  Wilcox's indexes  Wilcox gives a number of formulae for various indices of QV , the first, which he designates DM for "Deviation from the Mode", is a standardized form of the variation ratio , and is analogous to variance as deviation from the mean.  ModVR  The formula for the variation around the mode ( ModVR ) is derived as follows:      M  =    ∑   i  =  1   K    (    f  m   -   f  i    )        M    superscript   subscript     i  1    K      subscript  f  m    subscript  f  i       M=\sum_{i=1}^{K}(f_{m}-f_{i})     where f m is the modal frequency, K is the number of categories and f i is the frequency of the i th group.  This can be simplified to      M  =    K   f  m    -  N       M      K   subscript  f  m    N     M=Kf_{m}-N     where N is the total size of the sample.  Freeman's index (or variation ratio) is 2      v  =   1  -    f  m   N        v    1     subscript  f  m   N      v=1-\frac{f_{m}}{N}     This is related to M as follows:         (    f  m   N   )   -   1  K      N  K     (   K  -  1   )   N     =   M   N   (   K  -  1   )                subscript  f  m   N     1  K        N  K       K  1   N       M    N    K  1       \frac{(\frac{f_{m}}{N})-\frac{1}{K}}{\frac{N}{K}\frac{(K-1)}{N}}=\frac{M}{N(K-%
 1)}     The ModVR is defined as       M  o  d  V  R   =   1  -     K   f  m    -  N    N   (   K  -  1   )      =    K   (   N  -   f  m    )     N   (   K  -  1   )     =    K  v    K  -  1            M  o  d  V  R     1        K   subscript  f  m    N     N    K  1               K    N   subscript  f  m       N    K  1              K  v     K  1       ModVR=1-\frac{Kf_{m}-N}{N(K-1)}=\frac{K(N-f_{m})}{N(K-1)}=\frac{Kv}{K-1}     where v is Freeman's index.  Low values of ModVR correspond to small amount of variation and high values to larger amounts of variation.  When K is large, ModVR is approximately equal to Freeman's index v .  RanVR  This is based on the range around the mode. It is defined to be       R  a  n  V  R   =   1  -     f  m   -   f  l     f  m     =    f  l    f  m            R  a  n  V  R     1       subscript  f  m    subscript  f  l     subscript  f  m             subscript  f  l    subscript  f  m       RanVR=1-\frac{f_{m}-f_{l}}{f_{m}}=\frac{f_{l}}{f_{m}}     where f m is the modal frequency and f l is the lowest frequency.  AvDev  This is an analog of the mean deviation. It is defined as the arithmetic mean of the absolute differences of each value from the mean.       A  v  D  e  v   =   1  -    1   2  N     K   K  -  1      ∑   i  =  1   K    |    f  i   -   N  K    |            A  v  D  e  v     1      1    2  N      K    K  1      subscript   superscript   K     i  1         subscript  f  i     N  K          AvDev=1-\frac{1}{2N}\frac{K}{K-1}\sum^{K}_{i=1}|f_{i}-\frac{N}{K}|     MNDif  This is an analog of the mean difference - the average of the differences of all the possible pairs of variate values, taken regardless of sign. The mean difference differs from the mean and standard deviation because it is dependent on the spread of the variate values among themselves and not on the deviations from some central value. 3       M  N  D  i  f   =   1  -    1   N   (   K  -  1   )       ∑   i  =  1    K  -  1      ∑   j  =   i  +  1    K    |    f  i   -   f  j    |             M  N  D  i  f     1      1    N    K  1       superscript   subscript     i  1      K  1      superscript   subscript     j    i  1     K        subscript  f  i    subscript  f  j           MNDif=1-\frac{1}{N(K-1)}\sum_{i=1}^{K-1}\sum_{j=i+1}^{K}|f_{i}-f_{j}|     where f i and f j are the i th and j th frequencies respectively.  The MNDif is the Gini coefficient applied to qualitative data.  VarNC  This is an analog of the variance.       V  a  r  N  C   =   1  -    1   N  2     K   (   K  -  1   )     ∑    (    f  i   -   N  K    )   2            V  a  r  N  C     1      1   superscript  N  2      K    K  1       superscript     subscript  f  i     N  K    2        VarNC=1-\frac{1}{N^{2}}\frac{K}{(K-1)}\sum(f_{i}-\frac{N}{K})^{2}     It is the same index as Mueller and Schussler's Index of Qualitative Variation 4 and Gibbs' M2 index.  It is distributed as a chi square variable with K - 1 degrees of freedom . 5  StDev  Wilson has suggested two versions of this statistic.  The first is based on AvDev.       S  t  D  e   v  1    =   1  -      ∑   i  =  1   K     (    f  i   -   N  K    )   2       (   N  -   N  K    )   2   +    (   K  -  1   )     (   N  K   )   2              S  t  D  e   subscript  v  1      1        superscript   subscript     i  1    K    superscript     subscript  f  i     N  K    2       superscript    N    N  K    2       K  1    superscript    N  K   2          StDev_{1}=1-\sqrt{\frac{\sum_{i=1}^{K}(f_{i}-\frac{N}{K})^{2}}{(N-\frac{N}{K})%
 ^{2}+(K-1)(\frac{N}{K})^{2}}}     The second is based on MNDif       S  t  D  e   v  2    =   1  -      ∑   i  =  1    K  -  1      ∑   j  =   i  +  1    K    (    f  i   -   f  j    )       N  2    (   K  -  1   )             S  t  D  e   subscript  v  2      1        subscript   superscript     K  1      i  1      subscript   superscript   K     j    i  1        subscript  f  i    subscript  f  j         superscript  N  2     K  1         StDev_{2}=1-\sqrt{\frac{\sum^{K-1}_{i=1}\sum^{K}_{j=i+1}(f_{i}-f_{j})}{N^{2}(K%
 -1)}}     HRel  This index was originally developed by Claude Shannon for use in specifying the properties of comnmunication channels.       H  R  e  l   =    -   ∑    p  i   l  o   g  2    p  i        log  2   K          H  R  e  l            subscript  p  i   l  o   subscript  g  2    subscript  p  i        subscript   2   K      HRel=\frac{-\sum p_{i}log_{2}p_{i}}{\log_{2}K}     where p i = f i / N .  Gibb's indices and related formulae  Gibbs et al proposed six indexes. 6  M 1  The unstandardized index ( M 1)  is       M  1   =   1  -    ∑   i  =  1   K    p  i  2           M  1     1    superscript   subscript     i  1    K    superscript   subscript  p  i   2       M1=1-\sum_{i=1}^{K}p_{i}^{2}     where K is the number of categories and     p  i   =    f  i   /  N        subscript  p  i      subscript  f  i   N     p_{i}=f_{i}/N   is the proportion of observations that fall in a given category i .  M 1 can be interpreted as one minus the likelihood that a random pair of samples will belong to the same category , so this formula for IQV is a standardized likelihood of a random pair falling in the same category. This index has also referred to as the index of differentiation, the index of sustenance differentiation and the geographical differentiation index depending on the context it has been used in.  M 2  A second index is the M2 7 is:       M  2   =    K   K  -  1     (   1  -    ∑   i  =  1   K    p  i  2     )          M  2       K    K  1      1    superscript   subscript     i  1    K    superscript   subscript  p  i   2        M2=\frac{K}{K-1}\left(1-\sum_{i=1}^{K}p_{i}^{2}\right)     where K is the number of categories and     p  i   =    f  i   /  N        subscript  p  i      subscript  f  i   N     p_{i}=f_{i}/N   is the proportion of observations that fall in a given category i . The factor of    K   K  -  1       K    K  1     \frac{K}{K-1}   is for standardization.  M 1 and M 2 can be interpreted in terms of variance of a multinomial distribution (there called an "expanded binomial model"). M 1 is the variance of the multinomial distribution and M 2 is the ratio of the variance of the multinomial distribution to the variance of a binomial distribution .  M 4  The M 4 index is       M  4   =     ∑   i  =  1   K    |    X  i   -  m   |     2    ∑   i  =  1   K    X  i            M  4       superscript   subscript     i  1    K        subscript  X  i   m       2    superscript   subscript     i  1    K    subscript  X  i        M4=\frac{\sum_{i=1}^{K}|X_{i}-m|}{2\sum_{i=1}^{K}X_{i}}     where m is the mean.  M 6  The formula for M 6 is       M  6   =   K   [   1  -     ∑   i  =  1   K    |    X  i   -  m   |     2  N     ]          M  6     K   delimited-[]    1      superscript   subscript     i  1    K        subscript  X  i   m       2  N         M6=K\left[1-\frac{\sum_{i=1}^{K}|X_{i}-m|}{2N}\right]     where K is the number of categories, X i is the number of data points in the i th category, N is the total number of data points, || is the absolute value (modulus) and      m  =     ∑   i  =  1   K    X  i    N       m      superscript   subscript     i  1    K    subscript  X  i    N     m=\frac{\sum_{i=1}^{K}X_{i}}{N}     This formula can be simplified       M  6   =   K   [   1  -     ∑   i  =  1   K    |    p  i   -   1  N    |    2    ]          M  6     K   delimited-[]    1      superscript   subscript     i  1    K        subscript  p  i     1  N      2        M6=K\left[1-\frac{\sum_{i=1}^{K}|p_{i}-\frac{1}{N}|}{2}\right]     where p i is the proportion of the sample in the i th category.  In practice M 1 and M 6 tend to be highly correlated which militates against their combined used.  Related indices  The sum       ∑   i  =  1   K    p  i  2       superscript   subscript     i  1    K    superscript   subscript  p  i   2     \sum_{i=1}^{K}p_{i}^{2}     has also found application. This is known as the Simpson index in ecology and as the Herfindahl index or the Herfindahl-Hirschman index (HHI) in economics. A variant of this is known as the Hunter–Gaston index in microbiology 8  In linguistics and cryptanalysis this sum is known as the repeat rate. The incidence of coincidence ( IC ) is an unbiased estimator of this statistic 9       I  C   =   ∑     f  i    (    f  i   -  1   )     n   (   n  -  1   )            I  C          subscript  f  i      subscript  f  i   1      n    n  1        IC=\sum\frac{f_{i}(f_{i}-1)}{n(n-1)}     where f i is the count of the i th  grapheme in the text and n is the total number of graphemes in the text.   M 1   The M 1 statistic defined above has been proposed several times in a number of different settings under a variety of names. These include Gini's index of mutability, 10 Simpson's measure of diversity, 11 Bachi's index of linguistic homogeneity, 12 Mueller and Schuessler's index of qualitative variation, 13 Gibbs and Martin's index of industry diversification, 14 Lieberson's index. 15 and Blau's index in sociology, psychology and management studies. 16 The formulation of all these indices are identical.  Simpson's D is defined as      D  =   1  -    ∑   i  =  1   K      n  i    (    n  i   -  1   )     n   (   n  -  1   )           D    1    superscript   subscript     i  1    K        subscript  n  i      subscript  n  i   1      n    n  1         D=1-\sum_{i=1}^{K}{\frac{n_{i}(n_{i}-1)}{n(n-1)}}     where n is the total sample size and n i is the number of items in the i th category.  For large n we have      u  ∼   1  -    ∑   i  =  1   K    p  i  2        similar-to  u    1    superscript   subscript     i  1    K    superscript   subscript  p  i   2       u\sim 1-\sum_{i=1}^{K}p_{i}^{2}     Another statistic that has been proposed is the coefficient of unalikeability which ranges between 0 and 1. 17      u  =    c   (  x  ,  y  )      n  2   -  n        u      c   x  y       superscript  n  2   n      u=\frac{c(x,y)}{n^{2}-n}     where n is the sample size and c ( x , y ) = 1 if x and y are alike and 0 otherwise.  For large n we have      u  ∼   1  -    ∑   i  =  1   K    p  i  2        similar-to  u    1    superscript   subscript     i  1    K    superscript   subscript  p  i   2       u\sim 1-\sum_{i=1}^{K}p_{i}^{2}     where K is the number of categories.  Another related statistic is the quadratic entropy       H  2   =   2   (   1  -    ∑   i  =  1   K    p  i  2     )         superscript  H  2     2    1    superscript   subscript     i  1    K    superscript   subscript  p  i   2        H^{2}=2\left(1-\sum_{i=1}^{K}p_{i}^{2}\right)     which is itself related to the Gini index .   M 2   Greenberg's monolingual non weighted index of linguistic diversity 18 is the M 2 statistic defined above.   M 7   Another index – the M 7 – was created based on the M 4 index of Gibbs et al . 19       M  7   =     ∑   i  =  1   K     ∑   j  =  1   L    |    R  i   -  R   |      2   ∑   R  i            M  7       superscript   subscript     i  1    K     superscript   subscript     j  1    L        subscript  R  i   R        2     subscript  R  i        M7=\frac{\sum_{i=1}^{K}\sum_{j=1}^{L}|R_{i}-R|}{2\sum R_{i}}     where       R   i  j    =    O   i  j     E   i  j     =    O   i  j      n  i    p  j            subscript  R    i  j       subscript  O    i  j     subscript  E    i  j             subscript  O    i  j       subscript  n  i    subscript  p  j        R_{ij}=\frac{O_{ij}}{E_{ij}}=\frac{O_{ij}}{n_{i}p_{j}}     and      R  =     ∑   i  =  1   K     ∑   j  =  1   L    R   i  j        ∑   i  =  1   K    n  i         R      superscript   subscript     i  1    K     superscript   subscript     j  1    L    subscript  R    i  j        superscript   subscript     i  1    K    subscript  n  i       R=\frac{\sum_{i=1}^{K}\sum_{j=1}^{L}R_{ij}}{\sum_{i=1}^{K}n_{i}}     where K is the number of categories, L is the number of subtypes, O ij and E ij are the number observed and expected respectively of subtype j in the i th category, n i is the number in the i th category and p j is the proportion of subtype j in the complete sample.  Note: This index was designed to measure women's participation in the work place: the two subtypes it was developed for were male and female.  Other single sample indices  These indices are summary statistics of the variation within the sample.  Berger–Parker index  The Berger–Parker index equals the maximum    p  i     subscript  p  i    p_{i}   value in the dataset, i.e. the proportional abundance of the most abundant type. 20 This corresponds to the weighted generalized mean of the    p  i     subscript  p  i    p_{i}   values when q approaches infinity, and hence equals the inverse of true diversity of order infinity (1/ ∞ D ).  Brillouin index of diversity  This index is strictly applicable only to entire populations rather than to finite samples. It is defined as       I  B   =     log   (   N  !   )    -    ∑   i  =  1   K    (   log   (    n  i   !   )    )     N        subscript  I  B           N      superscript   subscript     i  1    K        subscript  n  i       N     I_{B}=\frac{\log(N!)-\sum_{i=1}^{K}(\log(n_{i}!))}{N}     where N is total number of individuals in the population, n i is the number of individuals in the i th category and N ! is the factorial of N . Brillouin's index of evenness is defined as       E  B   =    I  B   /   I   B   (  max  )           subscript  E  B      subscript  I  B    subscript  I    B        E_{B}=I_{B}/I_{B(\max)}     where I B (max) is the maximum value of I B .  Hill's diversity numbers  Hill suggested a family of diversity numbers 21       N  a   =   1    [    ∑   i  =  1   K    p  i  a    ]    a  -  1          subscript  N  a     1   superscript   delimited-[]    superscript   subscript     i  1    K    superscript   subscript  p  i   a       a  1       N_{a}=\frac{1}{\left[\sum_{i=1}^{K}p_{i}^{a}\right]^{a-1}}     For given values of a several of the other indices can be computed   a = 0: N a = species richness  a = 1: N a = Shannon's index  a = 2: N a = 1/Simpson's index (without the small sample correction)  a = 3: N a = 1/Berger–Parker index   Hill also suggested a family of evenness measures       E   a  ,  b    =    N  a    N  b         subscript  E   a  b       subscript  N  a    subscript  N  b      E_{a,b}=\frac{N_{a}}{N_{b}}     where a > b .  Hill's E 4 is       E  4   =    N  2    N  1         subscript  E  4      subscript  N  2    subscript  N  1      E_{4}=\frac{N_{2}}{N_{1}}     Hill's E 5 is       E  5   =     N  2   -  1     N  1   -  1         subscript  E  5        subscript  N  2   1      subscript  N  1   1      E_{5}=\frac{N_{2}-1}{N_{1}-1}     Margalef's index       I   M  a  r  g    =    S  -  1    l  o   g  e   N         subscript  I    M  a  r  g        S  1     l  o   subscript  g  e   N      I_{Marg}=\frac{S-1}{log_{e}N}     where S is the number of data types in the sample and N is the total size of the sample. 22  Menhinick's index       I  Men   =   S   N         subscript  I  Men     S    N      I_{\mathrm{Men}}=\frac{S}{\sqrt{N}}     where S is the number of data types in the sample and N is the total size of the sample. 23  In linguistics this index is the identical with the Kuraszkiewicz index (Guiard index) where S is the number of distinct words (types) and N is the total number of words (tokens) in the text being examined. 24 25 This index can be derived as a special case of the Generalised Torquist function. 26  Q statistic  This is a statistic invented by Kempton and Taylor. 27 and involves the quartiles of the sample. It is defined as      Q  =      1  2    (    n   R  1    +   n   R  2     )    +    ∑   j  =    R  1   +  1      R  2   -  1     n  j      l  o  g   (    R  2   /   R  1    )         Q          1  2      subscript  n    R  1     subscript  n    R  2        superscript   subscript     j     subscript  R  1   1        subscript  R  2   1     subscript  n  j       l  o  g     subscript  R  2    subscript  R  1        Q=\frac{\frac{1}{2}(n_{R1}+n_{R2})+\sum_{j=R_{1}+1}^{R_{2}-1}n_{j}}{log(R_{2}/%
 R_{1})}     where R 1 and R 1 are the 25% and 75% quartiles respectively on the cumulative species curve, n j is the number of species in the j th category, n Ri is the number of species in the class where R i falls ( i = 1 or 2).  Shannon–Wiener index  This is taken from information theory      H  =     log  e   N   -    1  N    ∑    n  i    p  i    log   (   p  i   )            H      subscript   e   N       1  N        subscript  n  i    subscript  p  i      subscript  p  i          H=\log_{e}N-\frac{1}{N}\sum n_{i}p_{i}\log(p_{i})     where N is the total number in the sample and p i is the proportion in the i th category.  In ecology where this index is commonly used, H usually lies between 1.5 and 3.5 and only rarely exceeds 4.0.  An approximate formula for the standard deviation ( SD ) of H is       S  D   (  H  )    =    1  N    [    ∑    p  i     [    log  e    (   p  i   )    ]   2     -   H  2    ]          S  D  H       1  N    delimited-[]         subscript  p  i    superscript   delimited-[]    subscript   e    subscript  p  i     2      superscript  H  2        SD(H)=\frac{1}{N}\left[\sum p_{i}[\log_{e}(p_{i})]^{2}-H^{2}\right]     where p i is the proportion made up by the i th category and N is the total in the sample.  A more accurate approximate value of the variance of H (var( H )) is given by 28       var   (  H  )    =      ∑    p  i     [   log   (   p  i   )    ]   2     -    [   ∑    p  i    log   (   p  i   )      ]   2    N   +    K  -  1    2   N  2     +       -  1   +   ∑   p  i  2     -   ∑    p  i   -  1     log   (   p  i   )       +   ∑    p  i   -  1     ∑    p  i    log   (   p  i   )          6   N  3           var  H              subscript  p  i    superscript   delimited-[]     subscript  p  i     2      superscript   delimited-[]       subscript  p  i      subscript  p  i       2    N       K  1     2   superscript  N  2               1      superscript   subscript  p  i   2          superscript   subscript  p  i     1       subscript  p  i            superscript   subscript  p  i     1         subscript  p  i      subscript  p  i           6   superscript  N  3        \operatorname{var}(H)=\frac{\sum p_{i}[\log(p_{i})]^{2}-\left[\sum p_{i}\log(p%
 _{i})\right]^{2}}{N}+\frac{K-1}{2N^{2}}+\frac{-1+\sum p_{i}^{2}-\sum p_{i}^{-1%
 }\log(p_{i})+\sum p_{i}^{-1}\sum p_{i}\log(p_{i})}{6N^{3}}     where N is the sample size and K is the number of categories.  A related index is the Pielou J defined as      J  =   H    log  e    (  S  )         J    H    subscript   e   S      J=\frac{H}{\log_{e}(S)}     One difficulty with this index is that S is unknown for a finite sample. In practice S is usually set to the maximum present in any category in the sample.  Rényi entropy  The Rényi entropy is a generalization of the Shannon entropy to other values of q than unity. It can be expressed:       H    q   =     1   1  -  q      ln   (    ∑   i  =  1   K    p  i  q    )          superscript  H  q       1    1  q        superscript   subscript     i  1    K    superscript   subscript  p  i   q        {}^{q}H=\frac{1}{1-q}\;\ln\left(\sum_{i=1}^{K}p_{i}^{q}\right)     which equals       H    q   =   ln   (   1     ∑   i  =  1   K     p  i    p  i   q  -  1       q  -  1     )    =   ln   (   D    q   )           superscript  H  q       1       q  1      superscript   subscript     i  1    K      subscript  p  i    superscript   subscript  p  i     q  1                 superscript  D  q       {}^{q}H=\ln\left({1\over\sqrt[q-1]{{\sum_{i=1}^{K}p_{i}p_{i}^{q-1}}}}\right)=%
 \ln({}^{q}\!D)     This means that taking the logarithm of true diversity based on any value of q gives the Rényi entropy corresponding to the same value of q .  The value of    D    q     superscript  D  q    {}^{q}\!D   is also known as the Hill number. 29  McIntosh's D and E      D  =    N  -     ∑   i  =  1   K    n  i       N  -   N         D      N      superscript   subscript     i  1    K    subscript  n  i        N    N       D=\frac{N-\sqrt{\sum_{i=1}^{K}n_{i}}}{N-\sqrt{N}}     where N is the total sample size and n i is the number in the i th category.      E  =    N  -     ∑   i  =  1   K    n  i       N  -   N   K          E      N      superscript   subscript     i  1    K    subscript  n  i        N    N    K        E=\frac{N-\sqrt{\sum_{i=1}^{K}n_{i}}}{N-\frac{N}{\sqrt{K}}}     where K is the number of categories.  Fisher's alpha  This was the first index to be derived for diversity. 30     K  =   α   ln   (   1  +   N  α    )         K    α      1    N  α        K=\alpha\ln(1+\frac{N}{\alpha})     where K is the number of categories and N is the number of data points in the sample. Fisher's α has to be estimated numerically from the data.  The expected number of individuals in the r th category where the categories have been placed in increasing size is       E   (   n  r   )    =   α    X  r   r          E   subscript  n  r      α     superscript  X  r   r      E(n_{r})=\alpha\frac{X^{r}}{r}     where X is an empirical parameter lying between 0 and 1. While X is best estimated numerically an approximate value can be obtained by solving the following two equations      N  =    α  X    1  -  X        N      α  X     1  X      N=\frac{\alpha X}{1-X}         K  =   -   α   ln   (   1  -  X   )          K      α      1  X        K=-\alpha\ln(1-X)     where K is the number of categories and N is the total sample size.  The variance of α is approximately 31       var   (  α  )    =   α    ln   (  X  )     (   1  -  X   )          var  α     α      X     1  X       \operatorname{var}(\alpha)=\frac{\alpha}{\ln(X)(1-X)}     Strong's index  This index ( D w ) is the distance between the Lorenz curve of species distribution and the 45 degree line. It is closely related to the Gini coefficient. 32  In symbols it is       D  w   =   m  a  x   [     c  i   K   -   i  N    ]         subscript  D  w     m  a  x   delimited-[]       subscript  c  i   K     i  N        D_{w}=max[\frac{c_{i}}{K}-\frac{i}{N}]     where max() is the maximum value taken over the N data points, K is the number of categories (or species) in the data set and c i is the cumulative total up and including the i th category.  Simpson's E  This is related to Simpson's D and is defined as      E  =    1  D   /  K       E      1  D   K     E=\frac{1}{D}/K     where D is Simpson's D and K is the number of categories in the sample.  Smith & Wilson's indices  Smith and Wilson suggested a number of indices based on Simpson's D .       E  1   =    1  -  D    1  -   1  K          subscript  E  1       1  D     1    1  K       E_{1}=\frac{1-D}{1-\frac{1}{K}}          E  2   =     log  e    (  D  )      log  e    (  K  )          subscript  E  2       subscript   e   D     subscript   e   K      E_{2}=\frac{\log_{e}(D)}{\log_{e}(K)}     where D is Simpson's D and K is the number of categories.  Heip's index      E  =     e  H   -  1    K  -  1        E       superscript  e  H   1     K  1      E=\frac{e^{H}-1}{K-1}     where H is the Shannon entropy and K is the number of categories.  This index is closely related to Sheldon's index which is      E  =    e  H   K       E     superscript  e  H   K     E=\frac{e^{H}}{K}     where H is the Shannon entropy and K is the number of categories.  Camargo's index  This index was created by Camargo in 1993. 33     E  =   1  -    ∑   i  =  1   K     ∑   j  =   i  +  1    K      p  i   -   p  j    K          E    1    superscript   subscript     i  1    K     superscript   subscript     j    i  1     K        subscript  p  i    subscript  p  j    K        E=1-\sum_{i=1}^{K}\sum_{j=i+1}^{K}\frac{p_{i}-p_{j}}{K}     where K is the number of categories and p i is the proportion in the i th category.  Smith & Wilson's B  This index was proposed by Smith and Wilson in 1996. 34      B  =   1  -    2  π   a  r  c  t  a  n   (  θ  )         B    1      2  π   a  r  c  t  a  n  θ      B=1-\frac{2}{\pi}arctan(\theta)     where θ is the slope of the log(abundance)-rank curve.  Nee, Harvey and Cotgreave's index  This is the slope of the log(abundance)-rank curve.  Bulla's E  There are two versions of this index - one for continuous distributions ( E c ) and the other for discrete ( E d ). 35       E  c   =    O  -   1  K     1  -   1  K          subscript  E  c       O    1  K      1    1  K       E_{c}=\frac{O-\frac{1}{K}}{1-\frac{1}{K}}          E  d   =    O  -   1  K   -    K  -  1   N     1  -   1  K   -    K  -  1   N          subscript  E  d       O    1  K       K  1   N      1    1  K       K  1   N       E_{d}=\frac{O-\frac{1}{K}-\frac{K-1}{N}}{1-\frac{1}{K}-\frac{K-1}{N}}     where      O  =   1  -    1  2    |    p  i   -   1  K    |         O    1      1  2        subscript  p  i     1  K         O=1-\frac{1}{2}|p_{i}-\frac{1}{K}|     is the Schoener-Czekanoski index, K is the number of categories and N is the sample size.  Horn's information theory index  This index ( R ik ) is based on Shannon's entropy. 36 It is defined as       R   i  k    =     H  max   -   H  obs      H  max   -   H  min          subscript  R    i  k         subscript  H     subscript  H  obs       subscript  H     subscript  H        R_{ik}=\frac{H_{\max}-H_{\mathrm{obs}}}{H_{\max}-H_{\min}}     where      X  =   ∑   x   i  j         X     subscript  x    i  j       X=\sum x_{ij}         X  =   ∑   x   k  j         X     subscript  x    k  j       X=\sum x_{kj}          H   (  X  )    =   ∑     x   i  j    X    log   X   x   i  j              H  X          subscript  x    i  j    X       X   subscript  x    i  j          H(X)=\sum\frac{x_{ij}}{X}\log\frac{X}{x_{ij}}          H   (  Y  )    =   ∑     x   k  j    Y    log   Y   x   k  j              H  Y          subscript  x    k  j    Y       Y   subscript  x    k  j          H(Y)=\sum\frac{x_{kj}}{Y}\log\frac{Y}{x_{kj}}          H  min   =     X   X  +  Y    H   (  X  )    +    Y   X  +  Y    H   (  Y  )          subscript  H          X    X  Y    H  X       Y    X  Y    H  Y      H_{\min}=\frac{X}{X+Y}H(X)+\frac{Y}{X+Y}H(Y)          H  max   =   ∑   (      x   i  j     X  +  Y     log    X  +  Y    x   i  j       +     x   k  j     X  +  Y     log    X  +  Y    x   k  j        )         subscript  H             subscript  x    i  j      X  Y          X  Y    subscript  x    i  j            subscript  x    k  j      X  Y          X  Y    subscript  x    k  j           H_{\max}=\sum\left(\frac{x_{ij}}{X+Y}\log\frac{X+Y}{x_{ij}}+\frac{x_{kj}}{X+Y}%
 \log\frac{X+Y}{x_{kj}}\right)          H  obs   =   ∑      x   i  j    +   x   k  j      X  +  Y     log    X  +  Y     x   i  j    +   x   k  j              subscript  H  obs            subscript  x    i  j     subscript  x    k  j       X  Y          X  Y      subscript  x    i  j     subscript  x    k  j           H_{\mathrm{obs}}=\sum\frac{x_{ij}+x_{kj}}{X+Y}\log\frac{X+Y}{x_{ij}+x_{kj}}     In these equations x ij and x kj are the number of times the j th data type appears in the i th or k th sample respectively.  Rarefaction index  In a rarefied sample a random subsample n in chosen from the total N items. In this sample some groups may be necessarily absent from this subsample. Let    X  n     subscript  X  n    X_{n}   be the number of groups still present in the subsample of n items.    X  n     subscript  X  n    X_{n}   is less than K the number of categories whenever at least one group is missing from this subsample.  The rarefaction curve ,    f  n     subscript  f  n    f_{n}   is defined as:       f  n   =   E   [   X  n   ]    =   K  -     (     N      n     )    -  1      ∑   i  =  1   K    (      N  -   N  i        n     )             subscript  f  n     E   delimited-[]   subscript  X  n            K     superscript   binomial  N  n     1      superscript   subscript     i  1    K    binomial    N   subscript  N  i    n         f_{n}=E[X_{n}]=K-{\left({{N}\atop{n}}\right)}^{-1}\sum_{i=1}^{K}{\left({{N-N_{%
 i}}\atop{n}}\right)}     Note that 0 ≤ f (n) ≤ K .  Furthermore,        f   (  0  )    =  0   ,     f   (  1  )    =  1   ,    f   (  N  )    =  K       formulae-sequence      f  0   0    formulae-sequence      f  1   1       f  N   K      f(0)=0,\ f(1)=1,\ f(N)=K   .  Despite being defined at discrete values of n , these curves are most frequently displayed as continuous functions. 37  This index is discussed further in Rarefaction (ecology) .  Caswell's V  This is a z type statistic based on Shannon's entropy. 38      V  =    H  -   E   (  H  )      S  D   (  H  )         V      H    E  H      S  D  H      V=\frac{H-E(H)}{SD(H)}     where H is the Shannon entropy, E ( H ) is the expected Shannon entropy for a neutral model of distribution and SD ( H ) is the standard deviation of the entropy. The standard deviation is estimated from the formula derived by Pielou       S  D   (  H  )    =    1  N    [    ∑    p  i     [    log  e    (   p  i   )    ]   2     -   H  2    ]          S  D  H       1  N    delimited-[]         subscript  p  i    superscript   delimited-[]    subscript   e    subscript  p  i     2      superscript  H  2        SD(H)=\frac{1}{N}\left[\sum p_{i}[\log_{e}(p_{i})]^{2}-H^{2}\right]     where p i is the proportion made up by the i th category and N is the total in the sample.  Lloyd & Ghelardi's index  This is       I   L  G    =   K   K  ′         subscript  I    L  G      K   superscript  K  normal-′      I_{LG}=\frac{K}{K^{\prime}}     where K is the number of categories and K ' is the number of categories according to MacArthur's broken stick model yielding the observed diversity.  Average taxonomic distinctness index  This index is used to compare the relationship between hosts and their parasites. 39 It incorporates information about the phylogenetic relationship amongst the host species.       S   T  D    =   2    ∑    ∑   i  <  j     ω   i  j       s   (   s  -  1   )           subscript  S    T  D      2        subscript     i  j     subscript  ω    i  j        s    s  1        S_{TD}=2\frac{\sum\sum_{i     where s is the number of host species used by a parasite and ω ij is the taxonomic distinctness between host species i and j .  Indices for comparison of two or more data types within a single sample  Several of these indexes have been developed to document the degree to which different data types of interest may coexist within a geographic area.  Index of dissimilarity  Let A and B be two types of data item. Then the index of dissimilarity is      D  =    1  2     ∑   i  =  1   K    |     A  i   A   -    B  i   B    |         D      1  2     superscript   subscript     i  1    K          subscript  A  i   A      subscript  B  i   B         D=\frac{1}{2}\sum_{i=1}^{K}\left|\frac{A_{i}}{A}-\frac{B_{i}}{B}\right|     where      A  =    ∑   i  =  1   K    A  i        A    superscript   subscript     i  1    K    subscript  A  i      A=\sum_{i=1}^{K}A_{i}         B  =    ∑   i  =  1   K    B  i        B    superscript   subscript     i  1    K    subscript  B  i      B=\sum_{i=1}^{K}B_{i}     A i is the number of data type A at sample site i , B i is the number of data type B at sample site i , K is the number of sites sampled and || is the absolute value.  This index is probably better known as the index of dissimilarity ( D ). 40 It is closely related to the Gini index.  This index is biased as its expectation under a uniform distribution is > 0.  A modification of this index has been proposed by Gorard and Taylor. 41 Their index (GT) is       G  T   =   D   (   1  -   A   A  +  B     )          G  T     D    1    A    A  B        GT=D\left(1-\frac{A}{A+B}\right)     Index of segregation  The index of segregation ( IS ) 42 is       S  I   =    1  2     ∑   i  =  1   K    |     A  i   A   -     t  i   -   A  i     T  -  A     |           S  I       1  2     superscript   subscript     i  1    K          subscript  A  i   A        subscript  t  i    subscript  A  i      T  A          SI=\frac{1}{2}\sum_{i=1}^{K}|\frac{A_{i}}{A}-\frac{t_{i}-A_{i}}{T-A}|     where      A  =    ∑   i  =  1   K    A  i        A    superscript   subscript     i  1    K    subscript  A  i      A=\sum_{i=1}^{K}A_{i}         T  =    ∑   i  =  1   K    t  i        T    superscript   subscript     i  1    K    subscript  t  i      T=\sum_{i=1}^{K}t_{i}     and K is the number of units, A i and t i is the number of data type A in unit i and the total number of all data types in unit i .  Hutchen's square root index  This index ( H ) is defined as 43      H  =   1  -    ∑   i  =  1   K     ∑   j  =  1   i      p  i    p  j            H    1    superscript   subscript     i  1    K     superscript   subscript     j  1    i        subscript  p  i    subscript  p  j          H=1-\sum_{i=1}^{K}\sum_{j=1}^{i}\sqrt{p_{i}p_{j}}     where p i is the proportion of the sample composed of the i th variate.  Lieberson's isolation index  This index ( L xy ) was invented by Lieberson in 1981. 44       L   x  y    =    1  N     ∑   i  =  1   K      X  i    Y  i     X  tot           subscript  L    x  y        1  N     superscript   subscript     i  1    K        subscript  X  i    subscript  Y  i     subscript  X  tot        L_{xy}=\frac{1}{N}\sum_{i=1}^{K}\frac{X_{i}Y_{i}}{X_{\mathrm{tot}}}     where X i and Y i are the variables of interest at the i th site, K is the number of sites examined and X tot is the total number of variate of type X in the study.  Bell's index  This index is defined as 45       I  R   =     p   x  x    -   p  x     1  -   p  x          subscript  I  R        subscript  p    x  x     subscript  p  x      1   subscript  p  x       I_{R}=\frac{p_{xx}-p_{x}}{1-p_{x}}     where p x is the proportion of the sample made up of variates of type X and       p   x  x    =     ∑   i  =  1   K     x  i    p  i      N  x         subscript  p    x  x        superscript   subscript     i  1    K      subscript  x  i    subscript  p  i      subscript  N  x      p_{xx}=\frac{\sum_{i=1}^{K}x_{i}p_{i}}{N_{x}}     where N x is the total number of variates of type X in the study, K is the number of samples in the study and x i and p i are the number of variates and the proportion of variates of type X respectively in the i th sample.  Index of isolation  The index of isolation is       I  I   =    ∑   i  =  1   K      A  i   A     A  i    t  i            I  I     superscript   subscript     i  1    K        subscript  A  i   A      subscript  A  i    subscript  t  i        II=\sum_{i=1}^{K}\frac{A_{i}}{A}\frac{A_{i}}{t_{i}}     where K is the number of units in the study, A i and t i is the number of units of type A and the number of all units in i th sample.  A modified index of isolation has also been proposed       M  I  I   =     I  I   -   A  T     1  -   A  T           M  I  I         I  I     A  T      1    A  T       MII=\frac{II-\frac{A}{T}}{1-\frac{A}{T}}     The MII lies between 0 and 1.  Gorard's index of segregation  This index (GS) is defined as       G  S   =    1  2     ∑   i  =  1   K    |     A  i   A   -    t  i   T    |           G  S       1  2     superscript   subscript     i  1    K          subscript  A  i   A      subscript  t  i   T         GS=\frac{1}{2}\sum_{i=1}^{K}|\frac{A_{i}}{A}-\frac{t_{i}}{T}|     where      A  =    ∑   i  =  1   K    A  i        A    superscript   subscript     i  1    K    subscript  A  i      A=\sum_{i=1}^{K}A_{i}         T  =    ∑   i  =  1   K    t  i        T    superscript   subscript     i  1    K    subscript  t  i      T=\sum_{i=1}^{K}t_{i}     and A i and t i are the number of data items of type A and the total number of items in the i th sample.  Index of exposure  This index is defined as       I  E   =    ∑   i  =  1   K      A  i   A     B  i    t  i            I  E     superscript   subscript     i  1    K        subscript  A  i   A      subscript  B  i    subscript  t  i        IE=\sum_{i=1}^{K}\frac{A_{i}}{A}\frac{B_{i}}{t_{i}}     where      A  =    ∑   i  =  1   K    A  i        A    superscript   subscript     i  1    K    subscript  A  i      A=\sum_{i=1}^{K}A_{i}     and A i and B i are the number of types A and B in the i th category and t i is the total number of data points in the i th category.  Ochai index  This is a binary form of the cosine index. 46 It is used to compare presence/absence data of two data types (here A and B ). It is defined as      O  =   a     (   a  +  b   )    (   a  +  c   )          O    a        a  b     a  c        O=\frac{a}{\sqrt{(a+b)(a+c)}}     where a is the number of sample units where both A and B are found, b is number of sample units where A but not B occurs and c is the number of sample units where type B is present but not type A .  Kulczynczi's coefficient  This coeficient was invented by Stanisław Kulczyński in 1927 47 and is an index of association between two types (here A and B ). It varies in value between 0 and 1. It is defined as      K  =    a  2    (    1   a  +  b    +   1   a  +  c     )        K      a  2       1    a  b      1    a  c        K=\frac{a}{2}(\frac{1}{a+b}+\frac{1}{a+c})     where a is the number of sample units where type A and type B are present, b is the number of sample units where type A but not type B is present and c is the number of sample units where type B is present but not type A .  Yule's Q  This index was invented by Yule in 1900. 48 It concerns the assoication of two different types (here A and B ). It is defined as      Q  =     a  d   -   b  c      a  d   +   b  c         Q        a  d     b  c        a  d     b  c       Q=\frac{ad-bc}{ad+bc}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present. Q varies in value between -1 and +1. In the ordinal case Q is known as the Goodman-Kruskal γ .  Because the denominator potentially may be zero, Leinhert and Sporer have recommened adding +1 to a , b , c and d . 49  Yule's Y  This index is defined as      Y  =      a  d    -    b  c        a  d    +    b  c          Y          a  d        b  c           a  d        b  c        Y=\frac{\sqrt{ad}-\sqrt{bc}}{\sqrt{ad}+\sqrt{bc}}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Baroni-Urbani-Buser coefficient  This index was invented by Baroni-Urbani and Buser in 1976. 50 It varies between 0 and 1 in value. It is defined as       B  U  B   =      a  d    +  a      a  d    +  a  +  b  +  c          B  U  B           a  d    a         a  d    a  b  c      BUB=\frac{\sqrt{ad}+a}{\sqrt{ad}+a+b+c}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present. When d = 0, this index is identical to the Jaccard index.  Hamman coefficient  This coefficient is defined as      H  =     (   a  +  d   )   -   (   b  +  c   )     a  +  b  +  c  +  d        H        a  d     b  c      a  b  c  d      H=\frac{(a+d)-(b+c)}{a+b+c+d}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Rogers-Tanimoto coefficient  This coefficient is defined as       R  T   =    (   a  +  d   )    a  +   2   (   b  +  c   )    +  d          R  T       a  d     a    2    b  c    d      RT=\frac{(a+d)}{a+2(b+c)+d}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Sokal-Sneath coefficient  This coefficient is defined as       S  S   =    2   (   a  +  d   )      2   (   a  +  d   )    +  b  +  c          S  S       2    a  d        2    a  d    b  c      SS=\frac{2(a+d)}{2(a+d)+b+c}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Sokal's binary distance  This coefficient is defined as       S  B  D   =     b  +  c    a  +  b  +  c  +  d           S  B  D         b  c     a  b  c  d       SBD=\sqrt{\frac{b+c}{a+b+c+d}}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Russel-Rao coeeficient  This coefficient is defined as       R  R   =   a   a  +  b  +  c  +  d          R  R     a    a  b  c  d      RR=\frac{a}{a+b+c+d}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Phi coefficient  This coefficient is defined as      ϕ  =     a  d   -   b  c       (   a  +  b   )    (   a  +  c   )    (   b  +  c   )    (   c  +  d   )          ϕ        a  d     b  c          a  b     a  c     b  c     c  d        \phi=\frac{ad-bc}{\sqrt{(a+b)(a+c)(b+c)(c+d)}}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Soergel's coefficient  This coefficient is defined as      S  =    b  +  c    b  +  c  +  d        S      b  c     b  c  d      S=\frac{b+c}{b+c+d}     where b is the number of samples where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Simpson's coefficient  This coefficient is defined as      S  =   a   a  +   m  i  n   (  b  ,  c  )          S    a    a    m  i  n   b  c        S=\frac{a}{a+min(b,c)}     where b is the number of samples where type A is present but not type B , c is the number of samples where type B is present but not type A .  Dennis' coefficient  This coefficient is defined as      D  =     a  d   -   b  c       (   a  +  b  +  c  +  d   )    (   a  +  b   )    (   a  +  c   )          D        a  d     b  c          a  b  c  d     a  b     a  c        D=\frac{ad-bc}{\sqrt{(a+b+c+d)(a+b)(a+c)}}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Forbes' coefficient  This coefficient is defined as      F  =    a   (   a  +  b  +  c  +  d   )      (   a  +  b   )    (   a  +  c   )         F      a    a  b  c  d        a  b     a  c       F=\frac{a(a+b+c+d)}{(a+b)(a+c)}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Simple match coefficient  This coefficient is defined as       S  M   =    a  +  d    (   a  +  b  +  c  +  d   )          S  M       a  d     a  b  c  d      SM=\frac{a+d}{(a+b+c+d)}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Fossum's coefficient  This coefficient is defined as      F  =     (   a  +  b  +  c  +  d   )     (   a  -  0.5   )   2      (   a  +  b   )    (   a  +  c   )         F        a  b  c  d    superscript    a  0.5   2        a  b     a  c       F=\frac{(a+b+c+d)(a-0.5)^{2}}{(a+b)(a+c)}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Stile's coefficient  This coefficient is defined as      S  =   l  o  g   [    n    (    |    a  d   -   b  c    |   -   n  2    )   2      (   a  +  b   )    (   a  +  c   )    (   b  +  d   )    (   c  +  d   )     ]        S    l  o  g   delimited-[]      n   superscript          a  d     b  c       n  2    2        a  b     a  c     b  d     c  d         S=log[\frac{n(|ad-bc|-\frac{n}{2})^{2}}{(a+b)(a+c)(b+d)(c+d)}]     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A , d is the sample count where neither type A nor type B are present, n equals a + b + c + d and || is the modulus (absolute value) of the difference.  Michael's coefficient  This coefficient is defined as      M  =    4   (    a  d   -   b  c    )       (   a  +  d   )   2   +    (   b  +  c   )   2         M      4      a  d     b  c        superscript    a  d   2    superscript    b  c   2       M=\frac{4(ad-bc)}{(a+d)^{2}+(b+c)^{2}}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Pierce's coefficient  In 1884 Pierce suggested the following coefficient      P  =     a  b   +   b  c      a  b   +   2  b  c   +   c  d         P        a  b     b  c        a  b     2  b  c     c  d       P=\frac{ab+bc}{ab+2bc+cd}     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present.  Hawkin-Dotson coefficent  In 1975 Hawkin and Dotson proposed the following coefficient       H  D   =    1  2    (    a   a  +  b  +  c    +   d   b  +  c  +  d     )          H  D       1  2       a    a  b  c      d    b  c  d        HD=\frac{1}{2}(\frac{a}{a+b+c}+\frac{d}{b+c+d})     where a is the number of samples where types A and B are both present, b is where type A is present but not type B , c is the number of samples where type B is present but not type A and d is the sample count where neither type A nor type B are present  Indices for comparison between two or more samples  Czekanowski's quantitative index  This is also known as the Bray–Curtis index , Schoener's index, least common percentage index, index of affinity or proportional similarity. It is related to the Sørensen similarity index .       C  Z  I   =    ∑   min   (   x  i   ,   x  j   )      ∑   (    x  i   +   x  j    )           C  Z  I          subscript  x  i    subscript  x  j          subscript  x  i    subscript  x  j        CZI=\frac{\sum\min(x_{i},x_{j})}{\sum(x_{i}+x_{j})}     where x i and x j are the number of species in sites i and j respectively and the minimum is taken over the number of species in common between the two sites.  Canberra metric  The Canberra distance is a weighted version of the L 1 metric. It was introduced by introduced in 1966 51 and refined in 1967 52 by G. N. Lance and W. T. Williams . It is used to defined a distance bwteen two vectors - here two sites with K categories within each site.  The Canberra distance d between vectors p and q in an K -dimensional real  vector space is       d   (  𝐩  ,  𝐪  )    =    ∑   i  =  1   n     |    p  i   -   q  i    |     |   p  i   |   +   |   q  i   |            d   𝐩  𝐪      superscript   subscript     i  1    n          subscript  p  i    subscript  q  i          subscript  p  i       subscript  q  i         d(\mathbf{p},\mathbf{q})=\sum_{i=1}^{n}\frac{|p_{i}-q_{i}|}{|p_{i}|+|q_{i}|}     where p i and q i are the values of the i th category of the two vectors.  Sorensen's coefficient of community  This is used to measure similarities between communities.       C  C   =    2  c     s  1   +   s  2           C  C       2  c      subscript  s  1    subscript  s  2       CC=\frac{2c}{s_{1}+s_{2}}     where s 1 and s 2 are the number of species in community 1 and 2 respectively and c is the number of species common to both areas.  Jaccard's index  This is a measure of the similarity between two samples:      J  =   A   A  +  B  +  C        J    A    A  B  C      J=\frac{A}{A+B+C}     where A is the number of data points shared between the two samples and B and C are the data points found only in the first and second samples respectively.  This index was invented in 1902 by the Swiss botanist Paul Jaccard . 53  Under a random distribution the expected value of J is 54     J  =    1  A    (   1   A  +  B  +  C    )        J      1  A     1    A  B  C       J=\frac{1}{A}(\frac{1}{A+B+C})     The standard error of this index with the assumption of a random distribution is       S  E   (  J  )    =     A   (   B  +  C   )     N    (   A  +  B  +  C   )   3            S  E  J         A    B  C      N   superscript    A  B  C   3        SE(J)=\sqrt{\frac{A(B+C)}{N(A+B+C)^{3}}}     where N is the total size of the sample.  Dice's index  This is a measure of the similarity between two samples:      D  =    2  A     2  A   +  B  +  C        D      2  A       2  A   B  C      D=\frac{2A}{2A+B+C}     where A is the number of data points shared between the two samples and B and C are the data points found only in the first and second samples respectively.  Match coefficient  This is a measure of the similarity between two samples:      M  =    N  -  B  -  C   N       M      N  B  C   N     M=\frac{N-B-C}{N}     where N is the number of data points in the two samples and B and C are the data points found only in the first and second samples respectively.  Morisita's index  Morisita’s index of dispersion ( I m ) is the scaled probability that two points chosen at random from the whole population are in the same sample. 55 Higher values indicate a more clumped distribution.       I  m   =    ∑   x   (   x  -  1   )      n  m   (   m  -  1   )          subscript  I  m         x    x  1       n  m    m  1       I_{m}=\frac{\sum x(x-1)}{nm(m-1)}     An alternative formulation is       I  m   =   n     ∑   x  2    -   ∑  x       (   ∑  x   )   2   -   ∑  x           subscript  I  m     n         superscript  x  2      x       superscript    x   2     x        I_{m}=n\frac{\sum x^{2}-\sum x}{\left(\sum x\right)^{2}-\sum x}     where n is the total sample size, m is the sample mean and x are the individual values with the sum taken over the whole sample. It is also equal to       I  m   =     n   I  M  C     n  m   -  1         subscript  I  m       n  I  M  C       n  m   1      I_{m}=\frac{n\ IMC}{nm-1}     where IMC is Lloyd's index of crowding. 56  This index is relatively independent of the population density but is affected by the sample size.  Morisita showed that the statistic 57         I  m    (    ∑  x   -  1   )    +  n   -   ∑  x            subscript  I  m       x   1    n     x     I_{m}\left(\sum x-1\right)+n-\sum x     is distributed as a chi-squared variable with n − 1 degrees of freedom.  A alternative significance test for this index has been developed for large samples. 58      z  =     I  m   -  1     2  /  n    m  2         z       subscript  I  m   1       2  n    superscript  m  2       z=\frac{I_{m}-1}{2/nm^{2}}     where m is the overall sample mean, n is the number of sample units and z is the normal distribution abscissa . Significance is tested by comparing the value of z against the values of the normal distribution .  Standardised Morisita’s index  Smith-Gill developed a statistic based on Morisita’s index which is independent of both sample size and population density and bounded by −1 and +1. This statistic is calculated as follows 59  First determine Morisita's index ( I d ) in the usual fashion. Then let k be the number of units the population was sampled from. Calculate the two critical values       M  u   =      χ  0.975  2   -  k   +   ∑  x      ∑  x   -  1         subscript  M  u          subscript   superscript  χ  2   0.975   k     x        x   1      M_{u}=\frac{\chi^{2}_{0.975}-k+\sum x}{\sum x-1}          M  c   =      χ  0.025  2   -  k   +   ∑  x      ∑  x   -  1         subscript  M  c          subscript   superscript  χ  2   0.025   k     x        x   1      M_{c}=\frac{\chi^{2}_{0.025}-k+\sum x}{\sum x-1}     where χ 2 is the chi square value for n − 1 degrees of freedom at the 97.5% and 2.5% levels of confidence.  The standardised index ( I p ) is then calculated from one of the formulae below  When I d ≥ M c > 1       I  p   =   0.5  +   0.5   (     I  d   -   M  c     k  -   M  c     )          subscript  I  p     0.5    0.5       subscript  I  d    subscript  M  c      k   subscript  M  c         I_{p}=0.5+0.5\left(\frac{I_{d}-M_{c}}{k-M_{c}}\right)     When M c > I d ≥ 1       I  p   =   0.5   (     I  d   -  1     M  u   -  1    )         subscript  I  p     0.5       subscript  I  d   1      subscript  M  u   1       I_{p}=0.5\left(\frac{I_{d}-1}{M_{u}-1}\right)     When 1 > I d ≥ M u       I  p   =   -   0.5   (     I  d   -  1     M  u   -  1    )          subscript  I  p       0.5       subscript  I  d   1      subscript  M  u   1        I_{p}=-0.5\left(\frac{I_{d}-1}{M_{u}-1}\right)     When 1 > M u > I d       I  p   =    -  0.5   +   0.5   (     I  d   -   M  u     M  u    )          subscript  I  p       0.5     0.5       subscript  I  d    subscript  M  u     subscript  M  u        I_{p}=-0.5+0.5\left(\frac{I_{d}-M_{u}}{M_{u}}\right)     I p ranges between +1 and −1 with 95% confidence intervals of ±0.5. I p has the value of 0 if the pattern is random; if the pattern is uniform, I p  p > 0.  Peet's evenness indices  These indices are a measure of evenness between samples. 60       E  1   =    I  -   I  min      I  max   -   I  min          subscript  E  1       I   subscript  I        subscript  I     subscript  I        E_{1}=\frac{I-I_{\min}}{I_{\max}-I_{\min}}          E  2   =   I   I  max         subscript  E  2     I   subscript  I       E_{2}=\frac{I}{I_{\max}}     where I is an index of diversity, I max and I min are the maximum and minimum values of I between the samples being compared.  Loevinger's coefficient  Loevinger has suggested a coefficient H defined as follows:      H  =      p   m  a  x     (   1  -   p   m  i  n     )      p   m  i  n     (   1  -   p   m  a  x     )          H         subscript  p    m  a  x      1   subscript  p    m  i  n         subscript  p    m  i  n      1   subscript  p    m  a  x          H=\sqrt{\frac{p_{max}(1-p_{min})}{p_{min}(1-p_{max})}}     where p max and p min are the maximum and minimum proportions in the sample.  Metrics used  A number of metrics (distances between samples) have been proposed.  Euclidean distance  While this is usually used in quantitative work it may also be used in qualitative work. This is defined as       d   j  k    =     ∑   i  =  1   N     (    x   i  j    -   x   i  k     )   2          subscript  d    j  k        superscript   subscript     i  1    N    superscript     subscript  x    i  j     subscript  x    i  k     2       d_{jk}=\sqrt{\sum_{i=1}^{N}(x_{ij}-x_{ik})^{2}}     where d jk is the distance between x ij and x ik .  Manhattan distance  While this is more commonly used in quantitative work it may also be used in qualitative work. This is defined as       d   j  k    =    ∑   i  =  1   N    |    x   i  j    -   x   i  k     |         subscript  d    j  k      superscript   subscript     i  1    N        subscript  x    i  j     subscript  x    i  k         d_{jk}=\sum_{i=1}^{N}|x_{ij}-x_{ik}|     where d jk is the distance between x ij and x ik and || is the absolute value of the difference between x ij and x ik .  Prevosti’s distance  This is related to the Manhattan distance. It was described by Prevosti et al and was used to compare differences between chromosomes . 61 Let P and Q be two collections of r finite probability distributions. Let these distributions have values that are divided into k categories. Then the distance D PQ is       D   P  Q    =    1  r     ∑   j  =  1   r     ∑   i  =  1   k    |    p   j  i    -   q   j  i     |           subscript  D    P  Q        1  r     superscript   subscript     j  1    r     superscript   subscript     i  1    k        subscript  p    j  i     subscript  q    j  i           D_{PQ}=\frac{1}{r}\sum_{j=1}^{r}\sum_{i=1}^{k}|p_{ji}-q_{ji}|     where r is the number of discrete probability distributions in each population, k j is the number of categories in distributions P j and Q j and p ji (respectively q ji ) is the theoretical probability of category i in distribution P j ( Q j ) in population P ( Q ).  Its statistical properties were examined by Sanchez et al 62 who recommended a bootstrap procedure to estimate confidence intervals when testing for differences between samples.  Other metrics  Let      A  =   ∑   x   i  j         A     subscript  x    i  j       A=\sum x_{ij}         B  =   ∑   x   i  k         B     subscript  x    i  k       B=\sum x_{ik}         J  =   ∑   min   (   x   i  j    ,   x   j  k    )         J       subscript  x    i  j     subscript  x    j  k        J=\sum\min(x_{ij},x_{jk})     where min( x , y ) is the lesser value of the pair x and y .  Then       d   j  k    =    A  +  B   -   2  J         subscript  d    j  k        A  B     2  J      d_{jk}=A+B-2J     is the Manhattan distance,       d   j  k    =     A  +  B   -   2  J     A  +  B         subscript  d    j  k          A  B     2  J      A  B      d_{jk}=\frac{A+B-2J}{A+B}     is the Bray−Curtis distance,       d   j  k    =     A  +  B   -   2  J      A  +  B   -  J         subscript  d    j  k          A  B     2  J        A  B   J      d_{jk}=\frac{A+B-2J}{A+B-J}     is the Jaccard (or Ruzicka) distance and       d   j  k    =   1  -    1  2    (    J  A   +   J  B    )          subscript  d    j  k      1      1  2       J  A     J  B        d_{jk}=1-\frac{1}{2}\left(\frac{J}{A}+\frac{J}{B}\right)     is the Kulczynski distance.  Ordinal data  If the categories are at least ordinal then a number of other indices may be computed.  Leik's D  Leik's measure of dispersion ( D ) is one such index. 63 Let there be K categories and let p i be f i / N where f i is the number in the i th category and let the categories be arranged in ascending order. Let       c  a   =    ∑   i  =  1   a    p  j         subscript  c  a     subscript   superscript   a     i  1     subscript  p  j      c_{a}=\sum^{a}_{i=1}p_{j}     where a ≤ K . Let d a = c a if c a ≤ 0.5 and 1 − c a ≤ 0.5 otherwise. Then      D  =   2    ∑   a  =  i   K     d  a    K  -  1          D    2    superscript   subscript     a  i    K      subscript  d  a     K  1        D=2\sum_{a=i}^{K}\frac{d_{a}}{K-1}     Normalised Herfindahl measure  This is the square of the coefficient of variation divided by N - 1 where N is the sample size.      H  =    1   N  -  1      s  2    m  2         H      1    N  1       superscript  s  2    superscript  m  2       H=\frac{1}{N-1}\frac{s^{2}}{m^{2}}     where m is the mean and s is the standard deviation.  Potential for Conflict Index  The Potential for Conﬂict Index (PCI) describes the ratio of scoring on either side of a rating scale’s centre point. 64 This index requires at least ordinal data. This ratio is often be displayed as a bubble graph .  The PCI uses an ordinal scale with an odd number of rating points (− n to + n ) centred at 0. It is calculated as follows       P  C  I   =     X  t   Z    [   1  -   |      ∑   i  =  1    r  +     X  +     X  t    -     ∑   i  =  1    r  -     X  -     X  t     |    ]          P  C  I        subscript  X  t   Z    delimited-[]    1          superscript   subscript     i  1     subscript  r      subscript  X      subscript  X  t        superscript   subscript     i  1     subscript  r      subscript  X      subscript  X  t           PCI=\frac{X_{t}}{Z}\left[1-\left|\frac{\sum_{i=1}^{r_{+}}X_{+}}{X_{t}}-\frac{%
 \sum_{i=1}^{r_{-}}X_{-}}{X_{t}}\right|\right]     where Z = 2 n , || is the absolute value (modulus), r + is the number of responses in the positive side of the scale, r - is the number of responses in the negative side of the scale, X + are the responses on the positive side of the scale, X - are the responses on the negative side of the scale and       X  t   =     ∑   i  =  1    r  +     |   X  +   |    +    ∑   i  =  1    r  -     |   X  -   |          subscript  X  t       superscript   subscript     i  1     subscript  r        subscript  X        superscript   subscript     i  1     subscript  r        subscript  X         X_{t}=\sum_{i=1}^{r_{+}}|X_{+}|+\sum_{i=1}^{r_{-}}|X_{-}|     Theoretical difficulties are known to exist with the PCI. The PCI can be computed only for scales with a neutral center point and an equal number of response options on either side of it. Also a uniform distribution of responses does not always yield the midpoint of the PCI statistic but rather varies with the number of possible responses or values in the scale. For example, ﬁve-, seven- and nine-point scales with a uniform distribution of responses give PCIs of 0.60, 0.57 and 0.50 respectively.  The first of these problems is relatively minor as most ordinal scales with an even number of response can be extended (or reduced) by a single value to give an odd number of possible responses. Scale can usually be recentred if this is required. The second problem is more difficult to resolve and may limit the PCI's applicability.  The PCI has been extended 65       P  C   I  2    =     ∑   i  =  1   K     ∑   j  =  1   i     k  i    k  j    d   i  j       δ         P  C   subscript  I  2        superscript   subscript     i  1    K     superscript   subscript     j  1    i      subscript  k  i    subscript  k  j    subscript  d    i  j       δ     PCI_{2}=\frac{\sum_{i=1}^{K}\sum_{j=1}^{i}k_{i}k_{j}d_{ij}}{\delta}     where K is the number of categories, k i is the number in the i th category, d ij is the distance between the i th and i th categories, and δ is the maximum distance on the scale multiplied by the number of times it can occur in the sample. For a sample with an even number of data points      δ  =     N  2   2    d  max        δ       superscript  N  2   2    subscript  d       \delta=\frac{N^{2}}{2}d_{\max}     and for a sample with an odd number of data points      δ  =      N  2   -  1   2    d  max        δ         superscript  N  2   1   2    subscript  d       \delta=\frac{N^{2}-1}{2}d_{\max}     where N is the number of data points in the sample and d max is the maximum distance between points on the scale.  Vaske et al suggest a number of possible distance measures for use with this index. 66       D  1   :    d   i  j    =    |    r  i   -   r  j    |   -  1       normal-:   subscript  D  1      subscript  d    i  j           subscript  r  i    subscript  r  j     1      D_{1}:d_{ij}=|r_{i}-r_{j}|-1     if the signs (+ or −) of r i and r j differ. If the signs are the same d ij = 0.       D  2   :    d   i  j    =   |    r  i   -   r  j    |       normal-:   subscript  D  2      subscript  d    i  j         subscript  r  i    subscript  r  j        D_{2}:d_{ij}=|r_{i}-r_{j}|          D  3   :    d   i  j    =    |    r  i   -   r  j    |   p       normal-:   subscript  D  3      subscript  d    i  j     superscript       subscript  r  i    subscript  r  j     p      D_{3}:d_{ij}=|r_{i}-r_{j}|^{p}     where p is an arbitrary real number > 0.       D   p   i  j     :    d   i  j    =    [    |    r  i   -   r  j    |   -   (   m  -  1   )    ]   p       normal-:    D   subscript  p    i  j        subscript  d    i  j     superscript   delimited-[]         subscript  r  i    subscript  r  j       m  1     p      Dp_{ij}:d_{ij}=[|r_{i}-r_{j}|-(m-1)]^{p}     if sign( r i ) ≠ sign( r i ) and p is a real number > 0. If the signs are the same then d ij = 0. m is D 1 , D 2 or D 3 .  The difference between D 1 and D 2 is that the first does not include neutrals in the distance while the latter does. For example respondents scoring −2 and +1 would have a distance of 2 under D 1 and 3 under D 2 .  The use of a power ( p ) in the distances allows for the rescaling of extreme responses. These differences can be highlighted with p > 1 or diminished with p 2 has a symmetric unimodal distribution. 67 The tails of its distribution are larger than those of a normal distribution.  Vaske et al suggest the use of a t test to compare the values of the PCI between samples if the PCIs are approximately normally distributed.  van der Eijk's A  This measure is a weighted average of the degree of agreement the frequency distribution. 68  A ranges from −1 (perfect bimodality ) to +1 (perfect unimodality ). It is defined as      A  =   U   (   1  -    S  -  1    K  -  1     )        A    U    1      S  1     K  1        A=U\left(1-\frac{S-1}{K-1}\right)     where U is the unimodality of the distribution, S the number of categories that have nonzero frequencies and K the total number of categories.  The value of U is 1 if the distribution has any of the three following characteristics:   all responses are in a single category  the responses are evenly distributed among all the categories  the responses are evenly distributed among two or more contiguous categories, with the other categories with zero responses   With distributions other than these the data must be divided into 'layers'. Within a layer the responses are either equal or zero. The categories do not have to be contiguous. A value for A for each layer ( A i ) is calculated and a weighted average for the distribution is determined. The weights ( w i ) for each layer are the number of responses in that layer. In symbols       A  overall   =   ∑    w  i    A  i          subscript  A  overall        subscript  w  i    subscript  A  i       A_{\mathrm{overall}}=\sum w_{i}A_{i}     A uniform distribution has A = 0: when all the responses fall into one category A = +1.  One theoretical problem with this index is that it assumes that the intervals are equally spaced. This may limit its applicability.  Related statistics  Birthday problem  If there are n units in the sample and they are randomly distributed into k categories ( n ≤ k ), this can be considerer a variant of the birthday problem. 69 The probability ( p ) of all the categories having only one unit is      p  =    ∏   i  =  1   n    (   1  -   i  k    )        p    superscript   subscript  product    i  1    n     1    i  k       p=\prod_{i=1}^{n}\left(1-\frac{i}{k}\right)     If c is large and n is small compared with c 2/3 then to a good approximation      p  =   exp   (    -   n  2     2  c    )        p         superscript  n  2      2  c       p=\exp\left(\frac{-n^{2}}{2c}\right)     This approximation follows from the exact formula as follows:        log  e    (   1  -   i  c    )    ≈   -   i  c          subscript   e     1    i  c         i  c      \log_{e}\left(1-\frac{i}{c}\right)\approx-\frac{i}{c}      Sample size estimates   For p = 0.5 and p = 0.05 respectively the following estimates of n may be useful      n  =   1.2   c        n    1.2    c      n=1.2\sqrt{c}         n  =   2.448   c    ≈   2.5   c          n    2.448    c           2.5    c       n=2.448\sqrt{c}\approx 2.5\sqrt{c}     This analysis can be extended to multiple categories. For p = 0.5 and p 0.05 we have respectively      n  =   1.2    1    ∑   i  =  1   k    1   c  i            n    1.2      1    superscript   subscript     i  1    k     1   subscript  c  i          n=1.2\sqrt{\frac{1}{\sum_{i=1}^{k}\frac{1}{c_{i}}}}         n  ≈   2.5    1    ∑   i  =  1   k    1   c  i            n    2.5      1    superscript   subscript     i  1    k     1   subscript  c  i          n\approx 2.5\sqrt{\frac{1}{\sum_{i=1}^{k}\frac{1}{c_{i}}}}     where c i is the size of the i th category. This analysis assumes that the categories are independent.  If the data is ordered in some fashion then for at least one event occurring in two categories lying within j categories of each other than a probability of 0.5 or 0.05 requires a sample size ( n ) respectively of 70      n  =   1.2    c    2  j   +  1          n    1.2      c      2  j   1        n=1.2\sqrt{\frac{c}{2j+1}}         n  ≈   2.5    c    2  j   +  1          n    2.5      c      2  j   1        n\approx 2.5\sqrt{\frac{c}{2j+1}}     where c is the number of categories.  Birthday-death day problem  Whether or not there is a relation between birthdays and death days has been investigated with the following statistic 71      -    log  10    (    1  +   2  d    365   )          subscript   10       1    2  d    365      -\log_{10}\left(\frac{1+2d}{365}\right)     where d is the number of days in the year between the birthday and the death day.  Evaluation of indices  Different indices give different values of variation, and may be used for different purposes: several are used and critiqued in the sociology literature especially.  If one wishes to simply make ordinal comparisons between samples (is one sample more or less varied than another), the choice of IQV is relatively less important, as they will often give the same ordering.  Where the data is ordinal a method that may be of use in comparing samples is ORDANOVA .  In some cases it is useful to not standardize an index to run from 0 to 1, regardless of number of categories or samples , but one generally so standardizes it.  See also   Categorical data  Diversity index  Information entropy  Logarithmic distribution  Statistical dispersion  Variation ratio  Whipple's index   Notes  References                 "  Category:Statistical deviation and dispersion  Category:Summary statistics for categorical data  Category:Categorical data     This can only happen if the number of cases is a multiple of the number of categories. ↩  Freemen LC (1965) Elementary applied statistics. New York: John Wiley and Sons pp 40–43 ↩  Kendal MC, Stuart A (1958) The advanced theory of statistics. Hafner Publishing Company p46 ↩  Mueller JE, Schuessler KP (1961) Statistical reasoning in sociology. Boston: Houghton Mifflin Company. pp 177–179 ↩  Wilcox AR (1967) Indices of qualitative variation ↩  Gibbs JP, Poston Jr, Dudley L (1975) The division of labor: Conceptualization and related measures. Social Forces 53 (3) 468–476 doi:10.2307/2576589 ↩  IQV at xycoon ↩  Hunter PR, Gaston MA (1988) Numerical index of the discriminatory ability of typing systems: an application of Simpson's index of diversity. J Clin Microbiol 26(11): 2465–2466 ↩  Friedman WF (1925) The incidence of coincidence and its applications in cryptanalysis. Technical Paper. Office of the Chief Signal Officer. United States Government Printing Office. ↩  Gini CW (1912) Variability and mutability, contribution to the study of statistical distributions and relations. Studi Economico-Giuricici della R. Universita de Cagliari ↩  Simpson EH (1949) Measurement of diversity. Nature 163:688 ↩  Bachi R (1956) A statistical analysis of the revival of Hebrew in Israel. In: Bachi R (ed) Scripta Hierosolymitana, Vol III, Jerusalem: Magnus press pp 179–247 ↩  Mueller JH, Schuessler KF (1961) Statistical reasoning in sociology. Boston: Houghton Mifflin ↩  Gibbs JP, Martin, WT (1962) Urbanization, technology and division of labor: International patterns. American Sociological Review 27: 667–677 ↩  Lieberson S (1969) Measuring population diversity. American Sociological Review 34(6) 850–862 ↩  Blau P (1977) Inequality and Heterogeneity. Free Press, New York ↩  Perry M, Kader G (2005) Variation as unalikeability. Teaching Stats 27 (2) 58–60 ↩  Greenberg JH (1956) The measurement of linguistic diversity. Language 32: 109–115 ↩  Lautard EH (1978) PhD thesis ↩  Berger WH, Parker FL (1970) Diversity of planktonic Foramenifera in deep sea sediments. Science 168:1345–1347 ↩  Hill, M O. 1973. Diversity and evenness: a unifying notation and its consequences. Ecology 54:427–431 ↩  Margalef R (1958) Temporal succession and spatial heterogeneity in phytoplankton. In: Perspectives in marine biology. Buzzati-Traverso (ed) Univ Calif Press, Berkeley pp 323–347 ↩  Menhinick EF (1964) A comparison of some species-individuals diversity indices applied to samples of field insects. Ecology 45 (4) 859–861 ↩  Kuraszkiewicz W (1951) Nakladen Wroclawskiego Towarzystwa Naukowego ↩  Guiraud P (1954) Les caractères statistiques du vocabulaire. Presses Universitaires de France, Paris ↩  Panas E (2001) The Generalized Torquist: Specification and estimation of a new vocabulary-text size function. J Quant Ling 8(3) 233–252 ↩  Kempton RA, Taylor LR (1976) Models and statistics for species diversity. Nature 262: 818–820 ↩  Hutcheson K (1970) A test for comparing diversities based on the Shannon formula. J Theo Biol 29: 151–154 ↩  Hill MO (1973) Ecology 54 (2) 427–432 ↩  Fisher RA, Corbet A, Williams CB (1943) The relation between the number of species and the number of individuals in a random sample of an animal population. Animal Ecol 12: 42–58 ↩  Anscombe (1950) Sampling theory of the negative binomial and logarithmic series distributions. Biometrika 37: 358–382 ↩  Strong WL (2002) Assessing species abundance uneveness within and between plant communities. Community Ecology 3: 237–246 ↩  Camargo JA (1993) Must dominance increase with the number of subordinate species in competitive interactions? J. Theor Biol 161 537–542 ↩  Smith, Wilson (1996) ↩  Bulla L (1994) An index of evenness and its associated diversity measure. Oikos 70:167–171 ↩  Horn HS (1966) Measurement of 'overlap' in comparative ecological studies. Am Nat 100 (914): 419–423 ↩  Siegel, Andrew F (2006) Rarefaction curves. Encyclopedia of Statistical Sciences 10.1002/0471667196.ess2195.pub2. ↩  Caswell H (1976) Community structure: a neutral model analysis. Ecol Monogr 46: 327–354 ↩  Poulin R, Mouillot D (2003) Parasite specialization from a phylogenetic perspective: a new index of host speciﬁcity. Parasitology 126: 473–480 ↩  Duncan OD, Duncan B (1955) A methodological analysis of segregation indexes. Am Sociol Review, 20: 210–217 ↩  Gorard S, Taylor C (2002b) What is segregation? A comparison of measures in terms of 'strong' and 'weak' compositional invariance. Sociology, 36(4), 875–895 ↩  Massey DS, Denton NA (1988) The dimensions of residential segregation. Social Forces 67: 281–315 ↩  Hutchens RM (2004) One measure of segregation. International Economic Review 45: 555–578 ↩  Lieberson S (1981) An asymmetrical approach to segregation. In: Peach C, Robinson V, Smith S (ed.s) Ethnic segregation in cities. London: Croom Helmp. 61–82 ↩  Bell W (1954) A probability model for the measurement of ecological segregation. Social Forces 32:357–364 ↩  Ochiai A (1957) Zoogeographic studies on the soleoid fishes found in Japan and its neighbouring regions. Bull Jpn Soc Sci Fish 22: 526–530 ↩  Kulczynski S (1927) Die Pflanzenassoziationen der Pieninen. Bulletin International de l'Academie Polonaise des Sciences et des Lettres, Classe des Sciences ↩  Yule GU (1900) On the association of attributes in statistics. Philos Trans Roy Soc ↩  Lienert GA and Sporer SL (1982) Interkorrelationen seltner Symptome mittels Nullfeldkorrigierter YuleKoeffizienten. Psychologische Beitrage 24: 411–418 ↩  Baroni-Urbani C & Buser MW (1976) similarity of binary Data. Systematic Biology 25: 251-259 ↩  ↩  ↩  Jaccard P (1902) Lois de distribution florale. Bulletin de la Socíeté Vaudoise des Sciences Naturelles 38:67-130 ↩  Archer AW and Maples CG (1989) Response of selected binomial coefficients to varying degrees of matrix sparseness and to matrices with known data interrelationships. Mathematical Geology 21: 741-753 ↩  Morisita M (1959) Measuring the dispersion and the analysis of distribution patterns. Memoires of the Faculty of Science, Kyushu University Series E. Biol 2:215–235 ↩  Lloyd M (1967) Mean crowding. J Anim Ecol 36: 1–30 ↩  Morisita M (1959) Measuring the dispersion and the analysis of distribution patterns. Memoires of the Faculty of Science, Kyushu University Series E. Biol 2:215–235 ↩  Pedigo LP & Buntin GD (1994) Handbook of sampling methods for arthropods in agriculture. CRC Boca Raton FL ↩  Smith-Gill S J (1975) Cytophysiological basis of disruptive pigmentary patterns in the leopard frog Rana pipiens . II. Wild type and mutant cell specific patterns. J Morphol 146, 35–54 ↩  Peet (1974) The measurements of species diversity. Ann Rev Ecol System 5: 285–307 ↩  Prevosti A, Ribo, G, Serra L, Aguade M, Balanya J, Monclus M, Mestres F (1988) Colonization of America by Drosophila subobscura : experiment in natural populations that supports the adaptive role of chromosomal inversion polymorphism. Proc Natl Acad Sci USA 85: 5597–5600 ↩  Sanchez A, Ocana J, Utzetb F, Serrac L (2003) Comparison of Prevosti genetic distances. Journal of Statistical Planning and Inference 109 (2003) 43–65 ↩  Leik R (1966) A measure of ordinal consensus. Pacific sociological review 9 (2): 85–90 ↩  Manfredo M, Vaske, JJ, Teel TL (2003) The potential for conflict index: A graphic approach tp practical significance of human dimensions research. Human Dimensions of Wildlife 8: 219–228 ↩  Vaske JJ, Beaman J, Barreto H, Shelby LB (2010) An extension and further validation of the potential for conﬂict index. Leisure Sciences 32: 240–254 ↩  Vaske JJ, Beaman J, Barreto H, Shelby LB (2010) An extension and further validation of the potential for conﬂict index. Leisure Sciences 32: 240–254 ↩  Vaske JJ, Beaman J, Barreto H, Shelby LB (2010) An extension and further validation of the potential for conﬂict index. Leisure Sciences 32: 240–254 ↩  Van der Eijk C (2001) Measuring agreement in ordered rating scales. Quality and quantity 35(3): 325–341 ↩  Von Mises R (1939) Uber Aufteilungs-und Besetzungs-Wahrcheinlichkeiten. Revue de la Facultd des Sciences de de I'Universite d'lstanbul NS 4: 145−163 ↩  Sevast'yanov BA (1972) Poisson limit law for a scheme of sums of dependent random variables. (trans. S. M. Rudolfer) Theory of probability and its applications, 17: 695−699 ↩  Hoaglin DC, Mosteller, F and Tukey, JW (1985) Exploring data tables, trends, and shapes, New York: John Wiley ↩     