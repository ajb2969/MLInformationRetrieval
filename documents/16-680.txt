   Functional principal component analysis      Functional principal component analysis   Functional principal component analysis ( FPCA ) is a statistical method for investigating the dominant modes of variation of functional data . Using this method, a random function is represented in the eigenbasis, which is an orthonormal basis of the Hilbert space  L 2 that consists of the eigenfunctions of the autocovariance operator . FPCA represents functional data in the most parsimonious way, in the sense that when using a fixed number of basis functions , the eigenfunction basis explains more variation than any other basis expansion. FPCA can be applied for representing random functions, 1 or functional regression 2 and classification.  Formulation  For a square-integrable  stochastic process  X ( t ), t âˆˆ ğ’¯, let       Î¼   (  t  )    =   E   (   X   (  t  )    )          Î¼  t     E    X  t      \mu(t)=\text{E}(X(t))   and        G   (  s  ,  t  )    =   Cov   (   X   (  s  )    ,   X   (  t  )    )    =    âˆ‘   k  =  1   âˆ     Î»  k    Ï†  k    (  s  )    Ï†  k    (  t  )      ,          G   s  t      Cov     X  s     X  t            superscript   subscript     k  1         subscript  Î»  k    subscript  Ï†  k   s   subscript  Ï†  k   t       G(s,t)=\text{Cov}(X(s),X(t))=\sum_{k=1}^{\infty}\lambda_{k}\varphi_{k}(s)%
 \varphi_{k}(t),   where Î» 1 â‰¥ Î» 2 â‰¥ Â·Â·Â· â‰¥ 0 are the eigenvalues and Ï† 1 , Ï† 2 , ... are the orthonormal eigenfunctions of the linear Hilbertâ€“Schmidt operator       G  :      L  2    (  ğ’¯  )    â†’    L  2    (  ğ’¯  )     ,    G   (  f  )    =    âˆ«  ğ’¯    G   (  s  ,  t  )   f   (  s  )   d  s       .     normal-:  G   formulae-sequence   normal-â†’     superscript  L  2   ğ’¯      superscript  L  2   ğ’¯        G  f     subscript   ğ’¯     G   s  t   f  s  d  s        G:L^{2}(\mathcal{T})\rightarrow L^{2}(\mathcal{T}),\,G(f)=\int_{\mathcal{T}}G(%
 s,t)f(s)ds.     By the Karhunenâ€“LoÃ¨ve theorem , one can express the centered process in the eigenbasis,         X   (  t  )    -   Î¼   (  t  )     =    âˆ‘   k  =  1   âˆ     Î¾  k    Ï†  k    (  t  )      ,          X  t     Î¼  t      superscript   subscript     k  1         subscript  Î¾  k    subscript  Ï†  k   t      X(t)-\mu(t)=\sum_{k=1}^{\infty}\xi_{k}\varphi_{k}(t),   where       Î¾  k   =    âˆ«  ğ’¯     (    X   (  t  )    -   Î¼   (  t  )     )    Ï†  k    (  t  )   d  t         subscript  Î¾  k     subscript   ğ’¯         X  t     Î¼  t     subscript  Ï†  k   t  d  t      \xi_{k}=\int_{\mathcal{T}}(X(t)-\mu(t))\varphi_{k}(t)dt   is the principal component associated with the k -th eigenfunction Ï† k , with the properties         E   (   Î¾  k   )    =  0   ,    Var   (   Î¾  k   )    =    Î»  k   and E   (    Î¾  k    Î¾  l    )    =   0  for  k   â‰   l    .     formulae-sequence      E   subscript  Î¾  k    0         Var   subscript  Î¾  k       subscript  Î»  k   and E     subscript  Î¾  k    subscript  Î¾  l            0  for  k        l      \text{E}(\xi_{k})=0,\text{Var}(\xi_{k})=\lambda_{k}\text{ and }\text{E}(\xi_{k%
 }\xi_{l})=0\text{ for }k\neq l.     The centered process is then equivalent to Î¾ 1 , Î¾ 2 , .... A common assumption is that X can be represented by only the first few eigenfunctions (after subtracting the mean function), i.e.        X   (  t  )    â‰ˆ    X  m    (  t  )    =    Î¼   (  t  )    +    âˆ‘   k  =  1   m     Î¾  k    Ï†  k    (  t  )       ,          X  t      subscript  X  m   t            Î¼  t     superscript   subscript     k  1    m      subscript  Î¾  k    subscript  Ï†  k   t        X(t)\approx X_{m}(t)=\mu(t)+\sum_{k=1}^{m}\xi_{k}\varphi_{k}(t),   where        E   (    âˆ«  ğ’¯      (    X   (  t  )    -    X  m    (  t  )     )   2   d  t    )    =    âˆ‘   j  >  m     Î»  j    â†’   0  as  m   â†’  âˆ   .          normal-E    subscript   ğ’¯      superscript      X  t      subscript  X  m   t    2   d  t       subscript     j  m     subscript  Î»  j      normal-â†’      0  as  m     normal-â†’        \mathrm{E}\left(\int_{\mathcal{T}}\left(X(t)-X_{m}(t)\right)^{2}dt\right)=\sum%
 _{j>m}\lambda_{j}\rightarrow 0\text{ as }m\rightarrow\infty.     Interpretation of eigenfunctions  The first eigenfunction Ï† 1 depicts the dominant mode of variation of X .        Ï†  1   =      arg   max     âˆ¥  Ï†  âˆ¥   =  1     {   Var   (    âˆ«  ğ’¯     (    X   (  t  )    -   Î¼   (  t  )     )   Ï†   (  t  )   d  t    )    }     ,       subscript  Ï†  1         norm  Ï†   1     arg  max      Var    subscript   ğ’¯         X  t     Î¼  t    Ï†  t  d  t         \varphi_{1}=\underset{\|\mathbf{\varphi}\|=1}{\operatorname{arg\,max}}\left\{%
 \operatorname{Var}(\int_{\mathcal{T}}(X(t)-\mu(t))\varphi(t)dt)\right\},   where        âˆ¥  Ï†  âˆ¥   =    (    âˆ«  ğ’¯    Ï†    (  t  )   2   d  t    )    1  2     .       norm  Ï†    superscript    subscript   ğ’¯     Ï†   superscript  t  2   d  t      1  2      \|\mathbf{\varphi}\|=\left(\int_{\mathcal{T}}\varphi(t)^{2}dt\right)^{\frac{1}%
 {2}}.     The k -th eigenfunction Ï† k is the dominant mode of variation orthogonal to Ï† 1 , Ï† 2 , ... , Ï† k -1 ,        Ï†  k   =      arg   max       âˆ¥  Ï†  âˆ¥   =  1   ,    âŸ¨  Ï†  ,   Ï†  j   âŸ©   =   0  for  j   =  1    ,   â€¦  ,   k  -  1       {   Var   (    âˆ«  ğ’¯     (    X   (  t  )    -   Î¼   (  t  )     )   Ï†   (  t  )   d  t    )    }     ,       subscript  Ï†  k       formulae-sequence   formulae-sequence     norm  Ï†   1        Ï†   subscript  Ï†  j      0  for  j        1      normal-â€¦    k  1       arg  max      Var    subscript   ğ’¯         X  t     Î¼  t    Ï†  t  d  t         \varphi_{k}=\underset{\|\mathbf{\varphi}\|=1,\langle\varphi,\varphi_{j}\rangle%
 =0\text{ for }j=1,\dots,k-1}{\operatorname{arg\,max}}\left\{\operatorname{Var}%
 (\int_{\mathcal{T}}(X(t)-\mu(t))\varphi(t)dt)\right\},   where        âŸ¨  Ï†  ,   Ï†  j   âŸ©   =    âˆ«  ğ’¯    Ï†   (  t  )    Ï†  j    (  t  )   d  t     ,    for  j   =   1  ,  â€¦  ,   k  -  1.        formulae-sequence     Ï†   subscript  Ï†  j      subscript   ğ’¯     Ï†  t   subscript  Ï†  j   t  d  t         for  j    1  normal-â€¦    k  1.       \langle\varphi,\varphi_{j}\rangle=\int_{\mathcal{T}}\varphi(t)\varphi_{j}(t)dt%
 ,\text{ for }j=1,\dots,k-1.     Estimation  Let Y ij = X i ( t ij ) + Îµ ij be the observations made at locations (usually time points) t ij , where X i is the i -th realization of the smooth stochastic process that generates the data, and Îµ ij are identically and independently distributed normal random variable with mean 0 and variance Ïƒ 2 , j = 1, 2, ..., m i . To obtain an estimate of the mean function Î¼ ( t ij ), if a dense sample on a regular grid is available, one may take the average at each location t ij :         Î¼  ^    (   t   i  j    )    =    1  n     âˆ‘   i  =  1   n    Y   i  j       .         normal-^  Î¼    subscript  t    i  j         1  n     superscript   subscript     i  1    n    subscript  Y    i  j        \hat{\mu}(t_{ij})=\frac{1}{n}\sum_{i=1}^{n}Y_{ij}.   If the observations are sparse, one needs to smooth the data pooled from all observations to obtain the mean estimate, 3 using smoothing methods like local linear smoothing or spline smoothing .  Then the estimate of the covariance function     G  ^    (  s  ,  t  )        normal-^  G    s  t     \hat{G}(s,t)   is obtained by averaging (in the dense case) or smoothing (in the sparse case) the raw covariances          G  i    (   t   i  j    ,   t   i  l    )    =    (    Y   i  j    -    Î¼  ^    (   t   i  j    )     )    (    Y   i  l    -    Î¼  ^    (   t   i  l    )     )     ,    j  â‰   l   ,   i  =   1  ,  â€¦  ,  n      .     formulae-sequence       subscript  G  i     subscript  t    i  j     subscript  t    i  l           subscript  Y    i  j       normal-^  Î¼    subscript  t    i  j         subscript  Y    i  l       normal-^  Î¼    subscript  t    i  l         formulae-sequence    j  l     i   1  normal-â€¦  n       G_{i}(t_{ij},t_{il})=(Y_{ij}-\hat{\mu}(t_{ij}))(Y_{il}-\hat{\mu}(t_{il})),j%
 \neq l,i=1,\dots,n.     Note that the diagonal elements of G i should be removed because they contain measurement error. 4  In practice,     G  ^    (  s  ,  t  )        normal-^  G    s  t     \hat{G}(s,t)   is discretized to an equal-spaced dense grid, and the estimation of eigenvalues Î» k and eigenvectors v k is carried out by numerical linear algebra. 5 The eigenfunction estimates     Ï†  ^   k     subscript   normal-^  Ï†   k    \hat{\varphi}_{k}   can then be obtained by interpolating the eigenvectors      v  k   ^   .     normal-^   subscript  v  k     \hat{v_{k}}.     The fitted covariance should be positive definite and symmetric and is then obtained as         G  ~    (  s  ,  t  )    =    âˆ‘    Î»  k   >  0       Î»  ^   k     Ï†  ^   k    (  s  )     Ï†  ^   k    (  t  )      .         normal-~  G    s  t      subscript      subscript  Î»  k   0       subscript   normal-^  Î»   k    subscript   normal-^  Ï†   k   s   subscript   normal-^  Ï†   k   t      \tilde{G}(s,t)=\sum_{\lambda_{k}>0}\hat{\lambda}_{k}\hat{\varphi}_{k}(s)\hat{%
 \varphi}_{k}(t).     Let     V  ^    (  t  )        normal-^  V   t    \hat{V}(t)   be a smoothed version of the diagonal elements G i ( t ij , t ij ) of the raw covariance matrices. Then     V  ^    (  t  )        normal-^  V   t    \hat{V}(t)   is an estimate of ( G ( t , t ) + Ïƒ 2 ). An estimate of Ïƒ 2 is obtained by         Ïƒ  ^   2   =    2   |  ğ’¯  |      âˆ«  ğ’¯     (     V  ^    (  t  )    -    G  ~    (  t  ,  t  )     )   d  t      ,       superscript   normal-^  Ïƒ   2       2    ğ’¯      subscript   ğ’¯          normal-^  V   t      normal-~  G    t  t     d  t       \hat{\sigma}^{2}=\frac{2}{|\mathcal{T}|}\int_{\mathcal{T}}(\hat{V}(t)-\tilde{G%
 }(t,t))dt,   if       Ïƒ  ^   2   >  0   ;       superscript   normal-^  Ïƒ   2   0    \hat{\sigma}^{2}>0;   otherwise      Ïƒ  ^   2   =  0.       superscript   normal-^  Ïƒ   2   0.    \hat{\sigma}^{2}=0.     If the observations X ij , j =1, 2, ..., m i are dense in ğ’¯, then the k -th FPC Î¾ k can be estimated by numerical integration , implementing         Î¾  ^   k   =   âŸ¨   X  -   Î¼  ^    ,    Ï†  ^   k   âŸ©    .       subscript   normal-^  Î¾   k      X   normal-^  Î¼     subscript   normal-^  Ï†   k      \hat{\xi}_{k}=\langle X-\hat{\mu},\hat{\varphi}_{k}\rangle.     However, if the observations are sparse, this method will not work. Instead, one can use best linear unbiased predictors , 6 yielding         Î¾  ^   k   =     Î»  ^   k     Ï†  ^   k  T     Î£  ^    Y  i    -  1     (    Y  i   -   Î¼  ^    )     ,       subscript   normal-^  Î¾   k      subscript   normal-^  Î»   k    superscript   subscript   normal-^  Ï†   k   T    superscript   subscript   normal-^  normal-Î£    subscript  Y  i      1       subscript  Y  i    normal-^  Î¼       \hat{\xi}_{k}=\hat{\lambda}_{k}\hat{\varphi}_{k}^{T}\hat{\Sigma}_{Y_{i}}^{-1}(%
 Y_{i}-\hat{\mu}),   where        Î£  ^    Y  i    =    G  ~   +     Ïƒ  ^   2    ğˆ   m  i           subscript   normal-^  normal-Î£    subscript  Y  i       normal-~  G      superscript   normal-^  Ïƒ   2    subscript  ğˆ   subscript  m  i        \hat{\Sigma}_{Y_{i}}=\tilde{G}+\hat{\sigma}^{2}\mathbf{I}_{m_{i}}   , and    G  ~     normal-~  G    \tilde{G}   is evaluated at the grid points generated by t ij , j = 1, 2, ..., m i . The algorithm, PACE, has an available Matlab package. 7  Asymptotic convergence properties of these estimates have been investigated. 8 9 10  Applications  FPCA can be applied for displaying the modes of functional variation, 11 12 in scatterplots of FPCs against each other or of responses against FPCs, for modeling sparse longitudinal data , 13 or for functional regression and classification, e.g., functional linear regression. 14  Scree plots and other methods can be used to determine the number of included components.  Connection with principal component analysis  The following table shows a comparison of various elements of principal component analysis (PCA) and FPCA. The two methods are both used for dimensionality reduction . In implementations, FPCA uses a PCA step.  However, PCA and FPCA differ in some critical aspects. First, the order of multivariate data in PCA can be permuted , which has no effect on the analysis, but the order of functional data carries time or space information and cannot be reordered. Second, the spacing of observations in FPCA matters, while there is no spacing issue in PCA. Third, regular PCA does not work for high-dimensional data without regularization , while FPCA has a built-in regularization due to the smoothness of the functional data and the truncation to a finite number of included components.      Element   In PCA   In FPCA       Data       X  âˆˆ   â„  p       X   superscript  â„  p     X\in\mathbb{R}^{p}          X  âˆˆ    L  2    (  ğ’¯  )        X     superscript  L  2   ğ’¯     X\in L^{2}(\mathcal{T})        Dimension       p  <  âˆ      p     p<\infty         âˆ     \infty        Mean       Î¼  =   E   (  X  )        Î¼    E  X     \mu=\text{E}(X)           Î¼   (  t  )    =   E   (   X   (  t  )    )          Î¼  t     E    X  t      \mu(t)=\text{E}(X(t))        Covariance        Cov   (  X  )    =   Î£   p  Ã—  p          Cov  X    subscript  normal-Î£    p  p      \text{Cov}(X)=\Sigma_{p\times p}           Cov   (   X   (  s  )    ,   X   (  t  )    )    =   G   (  s  ,  t  )          Cov     X  s     X  t       G   s  t      \text{Cov}(X(s),X(t))=G(s,t)        Eigenvalues        Î»  1   ,   Î»  2   ,  â€¦  ,   Î»  p       subscript  Î»  1    subscript  Î»  2   normal-â€¦   subscript  Î»  p     \lambda_{1},\lambda_{2},\dots,\lambda_{p}           Î»  1   ,   Î»  2   ,  â€¦      subscript  Î»  1    subscript  Î»  2   normal-â€¦    \lambda_{1},\lambda_{2},\dots        Eigenvectors/Eigenfunctions        ğ¯  1   ,   ğ¯  2   ,  â€¦  ,   ğ¯  p       subscript  ğ¯  1    subscript  ğ¯  2   normal-â€¦   subscript  ğ¯  p     \mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{p}            Ï†  1    (  t  )    ,    Ï†  2    (  t  )    ,  â€¦        subscript  Ï†  1   t      subscript  Ï†  2   t   normal-â€¦    \varphi_{1}(t),\varphi_{2}(t),\dots        Inner Product        âŸ¨  ğ—  ,  ğ˜  âŸ©   =    âˆ‘   k  =  1   p     X  k    Y  k          ğ—  ğ˜     superscript   subscript     k  1    p      subscript  X  k    subscript  Y  k       \langle\mathbf{X},\mathbf{Y}\rangle=\sum_{k=1}^{p}X_{k}Y_{k}           âŸ¨  X  ,  Y  âŸ©   =    âˆ«  ğ’¯    X   (  t  )   Y   (  t  )   d  t         X  Y     subscript   ğ’¯     X  t  Y  t  d  t      \langle X,Y\rangle=\int_{\mathcal{T}}X(t)Y(t)dt        Principal Components         z  k   =   âŸ¨   X  -  Î¼   ,   ğ¯  ğ¤   âŸ©    ,   k  =   1  ,  2  ,  â€¦  ,  p       formulae-sequence     subscript  z  k      X  Î¼    subscript  ğ¯  ğ¤       k   1  2  normal-â€¦  p      z_{k}=\langle X-\mu,\mathbf{v_{k}}\rangle,k=1,2,\dots,p            Î¾  k   =   âŸ¨   X  -  Î¼   ,   Ï†  k   âŸ©    ,   k  =   1  ,  2  ,  â€¦       formulae-sequence     subscript  Î¾  k      X  Î¼    subscript  Ï†  k       k   1  2  normal-â€¦      \xi_{k}=\langle X-\mu,\varphi_{k}\rangle,k=1,2,\dots        See also   Principal component analysis   Notes  References     "  Category:Statistical methods  Category:Non-parametric statistics     â†©  â†©  â†©  â†©  â†©   â†©   â†©  â†©   â†©       