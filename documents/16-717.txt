   Kernel embedding of distributions      Kernel embedding of distributions  In machine learning , the kernel embedding of distributions (also called the kernel mean or mean map ) comprises a class of nonparametric methods in which a probability distribution is represented as an element of a reproducing kernel Hilbert space (RKHS). 1 A generalization of the individual data-point feature mapping done in classical kernel methods , the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, while allowing one to compare and manipulate distributions using Hilbert space operations such as inner products , distances, projections , linear transformations , and spectral analysis . 2 This learning framework is very general and can be applied to distributions over any space   Î©   normal-Î©   \Omega   on which a sensible kernel function (measuring similarity between elements of   Î©   normal-Î©   \Omega   ) may be defined. For example, various kernels have been proposed for learning from data which are: vectors in    â„  d     superscript  â„  d    \mathbb{R}^{d}   , discrete classes/categories, strings , graphs / networks , images, time series , manifolds , dynamical systems , and other structured objects. 3 4 The theory behind kernel embeddings of distributions has been primarily developed by Alex Smola , Le Song , Arthur Gretton , and Bernhard SchÃ¶lkopf .  The analysis of distributions is fundamental in machine learning and statistics , and many algorithms in these fields rely on information theoretic approaches such as entropy , mutual information , or Kullbackâ€“Leibler divergence . However, to estimate these quantities, one must first either perform density estimation, or employ sophisticated space-partitioning/bias-correction strategies which are typically infeasible for high-dimensional data. 5 Commonly, methods for modeling complex distributions rely on parametric assumptions that may be unfounded or computationally challenging (e.g. Gaussian mixture models ), while nonparametric methods like kernel density estimation (Note: the smoothing kernels in this context have a different interpretation than the kernels discussed here) or characteristic function representation (via the Fourier transform of the distribution) break down in high-dimensional settings. 6  Methods based on the kernel embedding of distributions sidestep these problems and also possess the following advantages: 7   Data may be modeled without restrictive assumptions about the form of the distributions and relationships between variables  Intermediate density estimation is not needed  Practitioners may specify the properties of a distribution most relevant for their problem (incorporating prior knowledge via choice of the kernel)  If a characteristic kernel is used, then the embedding can uniquely preserve all information about a distribution, while thanks to the kernel trick , computations on the potentially infinite-dimensional RKHS can be implemented in practice as simple Gram matrix operations  Dimensionality-independent rates of convergence for the empirical kernel mean (estimated using samples from the distribution) to the kernel embedding of the true underlying distribution can be proven.  Learning algorithms based on this framework exhibit good generalization ability and finite sample convergence, while often being simpler and more effective than information theoretic methods   Thus, learning via the kernel embedding of distributions offers a principled drop-in replacement for information theoretic approaches and is a framework which not only subsumes many popular methods in machine learning and statistics as special cases, but also can lead to entirely new learning algorithms.  Definitions  Let   X   X   X   denote a random variable with domain   Î©   normal-Î©   \Omega   and distribution    P   (  X  )       P  X    P(X)   . Given a kernel   k   k   k   on    Î©  Ã—  Î©      normal-Î©  normal-Î©    \Omega\times\Omega   , the Moore-Aronszajn Theorem asserts the existence of a RKHS   â„‹   â„‹   \mathcal{H}   (a Hilbert space of functions    f  :   Î©  â†¦  â„      normal-:  f   maps-to  normal-Î©  â„     f:\Omega\mapsto\mathbb{R}   equipped with inner products     âŸ¨  â‹…  ,  â‹…  âŸ©   â„‹     subscript   normal-â‹…  normal-â‹…   â„‹    \langle\cdot,\cdot\rangle_{\mathcal{H}}   and norms    |  |  â‹…  |   |  â„‹      fragments  normal-|  normal-|  normal-â‹…  normal-|   subscript  normal-|  â„‹     ||\cdot||_{\mathcal{H}}   ) in which the element     k    (  x  ,  â‹…  )       k   x  normal-â‹…     \ k(x,\cdot)   satisfies the reproducing property       âŸ¨  f  ,   k   (  x  ,  â‹…  )    âŸ©   â„‹   =   f   (  x  )    âˆ€  f    âˆˆ  â„‹   ,    âˆ€  x   âˆˆ  Î©      formulae-sequence       subscript   f    k   x  normal-â‹…     â„‹     f  x   for-all  f         â„‹       for-all  x   normal-Î©     \langle f,k(x,\cdot)\rangle_{\mathcal{H}}=f(x)\ \forall f\in\mathcal{H},%
 \forall x\in\Omega   . One may alternatively consider     k    (  x  ,  â‹…  )       k   x  normal-â‹…     \ k(x,\cdot)   an implicit feature mapping    Ï•   (  x  )       Ï•  x    \phi(x)   from   Î©   normal-Î©   \Omega   to   â„‹   â„‹   \mathcal{H}   (which is therefore also called the feature space), so that      k    (  x  ,   x  â€²   )    =    âŸ¨   Ï•   (  x  )    ,   Ï•   (   x  â€²   )    âŸ©   â„‹         k   x   superscript  x  normal-â€²      subscript     Ï•  x     Ï•   superscript  x  normal-â€²     â„‹     \ k(x,x^{\prime})=\langle\phi(x),\phi(x^{\prime})\rangle_{\mathcal{H}}   can be viewed as a measure of similarity between points     x  ,   x  â€²    âˆˆ  Î©       x   superscript  x  normal-â€²    normal-Î©    x,x^{\prime}\in\Omega   . While the similarity measure is linear in the feature space, it may be highly nonlinear in the original space depending on the choice of kernel.  Kernel embedding  The kernel embedding of the distribution    P   (  X  )       P  X    P(X)   in   â„‹   â„‹   \mathcal{H}   (also called the kernel mean or mean map ) is given by: 8         Î¼  X   :=    ğ”¼  X    [   k   (  X  ,  â‹…  )    ]    =    ğ”¼  X    [   Ï•   (  X  )    ]    =    âˆ«  Î©    Ï•   (  x  )   d  P   (  x  )          assign   subscript  Î¼  X      subscript  ğ”¼  X    delimited-[]    k   X  normal-â‹…              subscript  ğ”¼  X    delimited-[]    Ï•  X            subscript   normal-Î©     Ï•  x  normal-d  P  x       \mu_{X}:=\mathbb{E}_{X}[k(X,\cdot)]=\mathbb{E}_{X}[\phi(X)]=\int_{\Omega}\phi(%
 x)\ \mathrm{d}P(x)        A kernel is characteristic if the mean embedding    Î¼  :    {   family of distributions over  Î©   }   â†¦  â„‹      normal-:  Î¼   maps-to     family of distributions over  normal-Î©    â„‹     \mu:\{\text{family of distributions over }\Omega\}\mapsto\mathcal{H}   is injective. 9 Each distribution can thus be uniquely represented in the RKHS and all statistical features of distributions are preserved by the kernel embedding if a characteristic kernel is used.  Empirical kernel embedding  Given   n   n   n   training examples    {   x  1   ,  â€¦  ,   x  n   }      subscript  x  1   normal-â€¦   subscript  x  n     \{x_{1},\dots,x_{n}\}   drawn independently and identically distributed (i.i.d.) from   P   P   P   , the kernel embedding of   P   P   P   can be empirically estimated as          Î¼  ^   X   =    1  n     âˆ‘   i  =  1   n    Ï•   (   x  i   )           subscript   normal-^  Î¼   X       1  n     superscript   subscript     i  1    n     Ï•   subscript  x  i        \widehat{\mu}_{X}=\frac{1}{n}\sum_{i=1}^{n}\phi(x_{i})        Joint distribution embedding  If   Y   Y   Y   denotes another random variable (for simplicity, assume the domain of   Y   Y   Y   is also   Î©   normal-Î©   \Omega   with the same kernel   k   k   k   which satisfies     âŸ¨     Ï•   (  x  )    âŠ—  Ï•    (  y  )    ,     Ï•   (   x  â€²   )    âŠ—  Ï•    (   y  â€²   )    âŸ©   =     k   (  x  ,   x  â€²   )    âŠ—  k    (  y  ,   y  â€²   )            tensor-product    Ï•  x   Ï•   y      tensor-product    Ï•   superscript  x  normal-â€²    Ï•    superscript  y  normal-â€²        tensor-product    k   x   superscript  x  normal-â€²     k    y   superscript  y  normal-â€²       \langle\phi(x)\otimes\phi(y),\phi(x^{\prime})\otimes\phi(y^{\prime})\rangle=k(%
 x,x^{\prime})\otimes k(y,y^{\prime})   ), then the joint distribution     P   (  X  ,  Y  )       P   X  Y     P(X,Y)   can be mapped into a tensor product feature space    â„‹  âŠ—  â„‹     tensor-product  â„‹  â„‹    \mathcal{H}\otimes\mathcal{H}   via 10         ğ’   X  Y    =    ğ”¼   X  Y     [     Ï•   (  X  )    âŠ—  Ï•    (  Y  )    ]    =    âˆ«   Î©  Ã—  Î©       Ï•   (  x  )    âŠ—  Ï•    (  y  )   d  P   (  x  ,  y  )            subscript  ğ’    X  Y       subscript  ğ”¼    X  Y     delimited-[]     tensor-product    Ï•  X   Ï•   Y            subscript     normal-Î©  normal-Î©       tensor-product    Ï•  x   Ï•   y  normal-d  P   x  y        \mathcal{C}_{XY}=\mathbb{E}_{XY}[\phi(X)\otimes\phi(Y)]=\int_{\Omega\times%
 \Omega}\phi(x)\otimes\phi(y)\ \mathrm{d}P(x,y)        By the equivalence between a tensor and a linear map , this joint embedding may be interpreted as an uncentered cross-covariance operator     ğ’   X  Y    :   â„‹  â†¦  â„‹      normal-:   subscript  ğ’    X  Y     maps-to  â„‹  â„‹     \mathcal{C}_{XY}:\mathcal{H}\mapsto\mathcal{H}   from which the cross-covariance of mean-zero functions     f  ,  g   âˆˆ  â„‹       f  g   â„‹    f,g\in\mathcal{H}   can be computed as 11          Cov   X  Y     (   f   (  X  )    ,   g   (  Y  )    )    :=    ğ”¼   X  Y     [   f   (  X  )   g   (  Y  )    ]    =    âŸ¨  f  ,    ğ’   X  Y    g   âŸ©   â„‹   =    âŸ¨   f  âŠ—  g   ,   ğ’   X  Y    âŸ©    â„‹  âŠ—  â„‹         assign     subscript  Cov    X  Y       f  X     g  Y        subscript  ğ”¼    X  Y     delimited-[]    f  X  g  Y           subscript   f     subscript  ğ’    X  Y    g    â„‹         subscript    tensor-product  f  g    subscript  ğ’    X  Y      tensor-product  â„‹  â„‹       \text{Cov}_{XY}(f(X),g(Y)):=\mathbb{E}_{XY}[f(X)g(Y)]=\langle f,\mathcal{C}_{%
 XY}g\rangle_{\mathcal{H}}=\langle f\otimes g,\mathcal{C}_{XY}\rangle_{\mathcal%
 {H}\otimes\mathcal{H}}        Given   n   n   n   pairs of training examples    {   (   x  1   ,   y  1   )   ,  â€¦  ,   (   x  n   ,   y  n   )   }       subscript  x  1    subscript  y  1    normal-â€¦    subscript  x  n    subscript  y  n      \{(x_{1},y_{1}),\dots,(x_{n},y_{n})\}   drawn i.i.d. from   P   P   P   , we can also empirically estimate the joint distribution kernel embedding via          ğ’  ^    X  Y    =    1  n     âˆ‘   i  =  1   n      Ï•   (   x  i   )    âŠ—  Ï•    (   y  i   )           subscript   normal-^  ğ’     X  Y        1  n     superscript   subscript     i  1    n      tensor-product    Ï•   subscript  x  i    Ï•    subscript  y  i        \widehat{\mathcal{C}}_{XY}=\frac{1}{n}\sum_{i=1}^{n}\phi(x_{i})\otimes\phi(y_{%
 i})        Conditional distribution embedding  Given a conditional distribution     P   (  Y  âˆ£  X  )      fragments  P   fragments  normal-(  Y  normal-âˆ£  X  normal-)     P(Y\mid X)   , one can define the corresponding RKHS embedding as 12         Î¼   Y  âˆ£  x    =   ğ”¼   Y  âˆ£  x     [  Ï•   (  Y  )   ]   =   âˆ«  Î©   Ï•   (  y  )   d  P   (  y  âˆ£  x  )      fragments   subscript  Î¼   fragments  Y  normal-âˆ£  x      subscript  ğ”¼   fragments  Y  normal-âˆ£  x     fragments  normal-[  Ï•   fragments  normal-(  Y  normal-)   normal-]     subscript   normal-Î©   Ï•   fragments  normal-(  y  normal-)   d  P   fragments  normal-(  y  normal-âˆ£  x  normal-)     \mu_{Y\mid x}=\mathbb{E}_{Y\mid x}[\phi(Y)]=\int_{\Omega}\phi(y)\ \mathrm{d}P(%
 y\mid x)        Note that the embedding of    P   (  Y  âˆ£  X  )      fragments  P   fragments  normal-(  Y  normal-âˆ£  X  normal-)     P(Y\mid X)   thus defines a family of points in the RKHS indexed by the values   x   x   x   taken by conditioning variable   X   X   X   . By fixing   X   X   X   to a particular value, we obtain a single element in   â„‹   â„‹   \mathcal{H}   , and thus it is natural to define the operator         ğ’   Y  âˆ£  X    :   â„‹  â†¦  â„‹      normal-:   subscript  ğ’   fragments  Y  normal-âˆ£  X     maps-to  â„‹  â„‹     \mathcal{C}_{Y\mid X}:\mathcal{H}\mapsto\mathcal{H}   as     ğ’   Y  âˆ£  X    =    ğ’   Y  X     ğ’   X  X    -  1          subscript  ğ’   fragments  Y  normal-âˆ£  X       subscript  ğ’    Y  X     superscript   subscript  ğ’    X  X      1       \mathcal{C}_{Y\mid X}=\mathcal{C}_{YX}\mathcal{C}_{XX}^{-1}        which given the feature mapping of   x   x   x   outputs the conditional embedding of   Y   Y   Y   given    X  =  x      X  x    X=x   . Assuming that for all     g  âˆˆ  â„‹   :     ğ”¼   Y  âˆ£  X     [   g   (  Y  )    ]    âˆˆ  â„‹      normal-:    g  â„‹        subscript  ğ”¼   fragments  Y  normal-âˆ£  X     delimited-[]    g  Y     â„‹     g\in\mathcal{H}:\ \mathbb{E}_{Y\mid X}[g(Y)]\in\mathcal{H}   , it can be shown that 13         Î¼   Y  âˆ£  x    =    ğ’   Y  âˆ£  X    Ï•   (  x  )         subscript  Î¼   fragments  Y  normal-âˆ£  x       subscript  ğ’   fragments  Y  normal-âˆ£  X    Ï•  x     \mu_{Y\mid x}=\mathcal{C}_{Y\mid X}\phi(x)        This assumption is always true for finite domains with characteristic kernels, but may not necessarily hold for continuous domains. 14 Nevertheless, even in cases where the assumption fails,     ğ’   Y  âˆ£  X    Ï•   (  x  )        subscript  ğ’   fragments  Y  normal-âˆ£  X    Ï•  x    \mathcal{C}_{Y\mid X}\phi(x)   may still be used to approximate the conditional kernel embedding    Î¼   Y  âˆ£  x      subscript  Î¼   fragments  Y  normal-âˆ£  x     \mu_{Y\mid x}   , and in practice, the inversion operator is replaced with a regularized version of itself     (    ğ’   X  X    +   Î»  ğˆ    )    -  1      superscript     subscript  ğ’    X  X      Î»  ğˆ      1     (\mathcal{C}_{XX}+\lambda\mathbf{I})^{-1}   (where   ğˆ   ğˆ   \mathbf{I}   denotes the identity matrix ).  Given training examples    {   (   x  1   ,   y  1   )   ,  â€¦  ,   (   x  n   ,   y  n   )   }       subscript  x  1    subscript  y  1    normal-â€¦    subscript  x  n    subscript  y  n      \{(x_{1},y_{1}),\dots,(x_{n},y_{n})\}   , the empirical kernel conditional embedding operator may be estimated as 15          C  ^    Y  âˆ£  X    =   ğš½    (   ğŠ  +   Î»  ğˆ    )    -  1     ğš¼  T         subscript   normal-^  C    fragments  Y  normal-âˆ£  X      ğš½   superscript    ğŠ    Î»  ğˆ      1     superscript  ğš¼  T      \widehat{C}_{Y\mid X}=\boldsymbol{\Phi}(\mathbf{K}+\lambda\mathbf{I})^{-1}%
 \boldsymbol{\Upsilon}^{T}        where     ğš½  =   (   Ï•   (   y  i   )    ,  â€¦  ,   (   y  n   )   )    ,   ğš¼  =   (   Ï•   (   x  i   )    ,  â€¦  ,   (   x  n   )   )       formulae-sequence    ğš½     Ï•   subscript  y  i    normal-â€¦   subscript  y  n       ğš¼     Ï•   subscript  x  i    normal-â€¦   subscript  x  n       \boldsymbol{\Phi}=\left(\phi(y_{i}),\dots,(y_{n})\right),\boldsymbol{\Upsilon}%
 =\left(\phi(x_{i}),\dots,(x_{n})\right)   are implicitly formed feature matrices,    ğŠ  =    ğš¼  T   ğš¼       ğŠ     superscript  ğš¼  T   ğš¼     \mathbf{K}=\boldsymbol{\Upsilon}^{T}\boldsymbol{\Upsilon}   is the Gram matrix for samples of   X   X   X   , and   Î»   Î»   \lambda   is a regularization parameter needed to avoid overfitting .  Thus, the empirical estimate of the kernel conditional embedding is given by a weighted sum of samples of   Y   Y   Y   in the feature space:          Î¼  ^    Y  âˆ£  x    =    âˆ‘   i  =  1   n     Î²  i    (  x  )   Ï•   (   y  i   )     =   ğš½  ğœ·   (  x  )           subscript   normal-^  Î¼    fragments  Y  normal-âˆ£  x      superscript   subscript     i  1    n      subscript  Î²  i   x  Ï•   subscript  y  i            ğš½  ğœ·  x      \widehat{\mu}_{Y\mid x}=\sum_{i=1}^{n}\beta_{i}(x)\phi(y_{i})=\boldsymbol{\Phi%
 }\boldsymbol{\beta}(x)   where     ğœ·   (  x  )    =     (   ğŠ  +   Î»  ğˆ    )    -  1     ğŠ  x          ğœ·  x      superscript    ğŠ    Î»  ğˆ      1     subscript  ğŠ  x      \boldsymbol{\beta}(x)=(\mathbf{K}+\lambda\mathbf{I})^{-1}\mathbf{K}_{x}   and     ğŠ  x   =    (   k   (   x  1   ,  x  )    ,  â€¦  ,   k   (   x  n   ,  x  )    )   T        subscript  ğŠ  x    superscript     k    subscript  x  1   x    normal-â€¦    k    subscript  x  n   x     T     \mathbf{K}_{x}=\left(k(x_{1},x),\dots,k(x_{n},x)\right)^{T}        Properties   The expectation of any function   f   f   f   in the RKHS can be computed as an inner product with the kernel embedding:           ğ”¼  X    [   f   (  X  )    ]    =    âŸ¨  f  ,   Î¼  X   âŸ©   â„‹          subscript  ğ”¼  X    delimited-[]    f  X      subscript   f   subscript  Î¼  X    â„‹     \mathbb{E}_{X}[f(X)]=\langle f,\mu_{X}\rangle_{\mathcal{H}}         In the presence of large sample sizes, manipulations of the    n  Ã—  n      n  n    n\times n   Gram matrix may be computationally demanding. Through use of a low-rank approximation of the Gram matrix (such as the incomplete Cholesky factorization ), running time and memory requirements of kernel-embedding-based learning algorithms can be drastically reduced without suffering much loss in approximation accuracy. 16   Convergence of empirical kernel mean to the true distribution embedding   If   k   k   k   is defined such that    f  âˆˆ   [  0  ,  1  ]       f   0  1     f\in[0,1]   for all    f  âˆˆ  â„‹      f  â„‹    f\in\mathcal{H}   with      ||  f  ||   â„‹   â‰¤  1       subscript   norm  f   â„‹   1    ||f||_{\mathcal{H}}\leq 1   (as is the case for the widely used radial basis function kernels), then with probability at least    1  -  Î´      1  Î´    \ 1-\delta   : 17       ||    Î¼  X   -    Î¼  ^   X    ||   â„‹   =    sup   f  âˆˆ   â„¬   (  0  ,  1  )       |     ğ”¼  X    [   f   (  X  )    ]    -    1  n     âˆ‘   i  =  1   n    f   (   x  i   )       |    â‰¤     2  n    ğ”¼  X    [    tr  K    ]    +     log   (   2  /  Î´   )     2  n             subscript   norm     subscript  Î¼  X    subscript   normal-^  Î¼   X     â„‹     subscript  supremum    f    â„¬   0  1             subscript  ğ”¼  X    delimited-[]    f  X         1  n     superscript   subscript     i  1    n     f   subscript  x  i                    2  n    subscript  ğ”¼  X    delimited-[]      tr  K              2  Î´      2  n         ||\mu_{X}-\widehat{\mu}_{X}||_{\mathcal{H}}=\sup_{f\in\mathcal{B}(0,1)}\left|%
 \mathbb{E}_{X}[f(X)]-\frac{1}{n}\sum_{i=1}^{n}f(x_{i})\right|\leq\frac{2}{n}%
 \mathbb{E}_{X}\left[\sqrt{\text{tr}K}\right]+\sqrt{\frac{\log(2/\delta)}{2n}}    where    â„¬   (  0  ,  1  )       â„¬   0  1     \mathcal{B}(0,1)   denotes the unit ball in   â„‹   â„‹   \mathcal{H}   and   ğŠ   ğŠ   \mathbf{K}   is the Gram matrix whose    i  ,  j     i  j    i,j   th entry is    k   (   x  i   ,   x  j   )       k    subscript  x  i    subscript  x  j      k(x_{i},x_{j})   .  The rate of convergence (in RKHS norm) of the empirical kernel embedding to its distribution counterpart is    O   (   n   -   1  /  2     )       O   superscript  n      1  2       O(n^{-1/2})   and does not depend on the dimension of   X   X   X   .  Statistics based on kernel embeddings thus avoid the curse of dimensionality , and though the true underlying distribution is unknown in practice, one can (with high probability) obtain an approximation within    O   (   n   -   1  /  2     )       O   superscript  n      1  2       O(n^{-1/2})   of the true kernel embedding based on a finite sample of size   n   n   n   .  For the embedding of conditional distributions, the empirical estimate can be seen as a weighted average of feature mappings (where the weights     Î²  i    (  x  )        subscript  Î²  i   x    \beta_{i}(x)   depend on the value of the conditioning variable and capture the effect of the conditioning on the kernel embedding). In this case, the empirical estimate converges to the conditional distribution RKHS embedding with rate    O   (   n   -   1  /  4     )       O   superscript  n      1  4       O\left(n^{-1/4}\right)   if the regularization parameter   Î»   Î»   \lambda   is decreased as    O   (   n   -   1  /  2     )       O   superscript  n      1  2       O\left(n^{-1/2}\right)   , though faster rates of convergence may be achieved by placing additional assumptions on the joint distribution. 18   Universal kernels   Letting    C   (  ğ’³  )       C  ğ’³    C(\mathcal{X})   denote the space of continuous  bounded functions on compact domain   ğ’³   ğ’³   \mathcal{X}   , we call a kernel   k   k   k    universal if    k   (  x  ,  â‹…  )       k   x  normal-â‹…     k(x,\cdot)   is continuous for all   x   x   x   and the RKHS induced by   k   k   k   is dense in    C   (  ğ’³  )       C  ğ’³    C(\mathcal{X})   .    If   k   k   k   induces a strictly positive definite kernel matrix for any set of distinct points, then it is a universal kernel. 19 For example, the widely used Gaussian RBF kernel          k   (  x  ,   x  â€²   )    =   exp   (   -    1   2   Ïƒ  2       ||   x  -   x  â€²    ||   2     )          k   x   superscript  x  normal-â€²             1    2   superscript  Ïƒ  2      superscript   norm    x   superscript  x  normal-â€²     2        k(x,x^{\prime})=\exp\left(-\frac{1}{2\sigma^{2}}||x-x^{\prime}||^{2}\right)        on compact subsets of    â„  d     superscript  â„  d    \mathbb{R}^{d}   is universal.   If   k   k   k   is universal, then it is characteristic , i.e. the kernel embedding is one-to-one. 20   Parameter selection for conditional distribution kernel embeddings   The empirical kernel conditional distribution embedding operator     ğ’  ^    Y  |  X      subscript   normal-^  ğ’    fragments  Y  normal-|  X     \widehat{\mathcal{C}}_{Y|X}   can alternatively be viewed as the solution of the following regularized least squares (function-valued) regression problem 21           min   ğ’  :   â„‹  â†¦  â„‹       âˆ‘   i  =  1   n     ||    Ï•   (   y  i   )    -   ğ’  Ï•   (   x  i   )     ||   â„‹  2     +   Î»    ||  ğ’  ||    H  S   2           subscript    normal-:  ğ’   maps-to  â„‹  â„‹       superscript   subscript     i  1    n    superscript   subscript   norm      Ï•   subscript  y  i      ğ’  Ï•   subscript  x  i      â„‹   2       Î»   superscript   subscript   norm  ğ’     H  S    2      \min_{\mathcal{C}:\mathcal{H}\mapsto\mathcal{H}}\sum_{i=1}^{n}||\phi(y_{i})-%
 \mathcal{C}\phi(x_{i})||_{\mathcal{H}}^{2}+\lambda||\mathcal{C}||_{HS}^{2}   where    |  |  â‹…  |   |   H  S       fragments  normal-|  normal-|  normal-â‹…  normal-|   subscript  normal-|    H  S      ||\cdot||_{HS}   is the Hilbert-Schmidt norm .      One can thus select the regularization parameter   Î»   Î»   \lambda   by performing cross-validation based on the squared loss function of the regression problem.   Rules of probability as operations in the RKHS  This section illustrates how basic probabilistic rules may be reformulated as (multi)linear algebraic operations in the kernel embedding framework and is primarily based on the work of Song et al. 22 23 The following notation is adopted:        P   (  X  ,  Y  )    =         P   X  Y    absent    P(X,Y)=   joint distribution over random variables    X  ,  Y     X  Y    X,Y          P   (  X  )    =    âˆ«  Î©    P   (  X  ,   d  y   )     =           P  X     subscript   normal-Î©     P   X    normal-d  y           absent     P(X)=\int_{\Omega}P(X,\mathrm{d}y)=   marginal distribution of   X   X   X   ;     P   (  Y  )    =         P  Y   absent    P(Y)=   marginal distribution of   Y   Y   Y         P   (  Y  âˆ£  X  )   =    P   (  X  ,  Y  )     P   (  X  )     =     fragments  P   fragments  normal-(  Y  normal-âˆ£  X  normal-)        P   X  Y      P  X       P(Y\mid X)=\frac{P(X,Y)}{P(X)}=   conditional distribution of   Y   Y   Y   given   X   X   X   with corresponding conditional embedding operator    ğ’   Y  âˆ£  X      subscript  ğ’   fragments  Y  normal-âˆ£  X     \mathcal{C}_{Y\mid X}          Ï€   (  Y  )    =         Ï€  Y   absent    \pi(Y)=   prior distribution over   Y   Y   Y        Q   Q   Q   is used to distinguish distributions which incorporate the prior from distributions   P   P   P   which do not rely on the prior   In practice, all embeddings are empirically estimated from data    {   (   x  1   ,   y  1   )   ,  â€¦  ,   (   x  n   ,   y  n   )   }       subscript  x  1    subscript  y  1    normal-â€¦    subscript  x  n    subscript  y  n      \{(x_{1},y_{1}),\dots,(x_{n},y_{n})\}   and it assumed that a set of samples    {    y  ~   1   ,  â€¦  ,    y  ~    n  ~    }      subscript   normal-~  y   1   normal-â€¦   subscript   normal-~  y    normal-~  n      \{\widetilde{y}_{1},\dots,\widetilde{y}_{\widetilde{n}}\}   may be used to estimate the kernel embedding of the prior distribution    Ï€   (  Y  )       Ï€  Y    \pi(Y)   .  Kernel sum rule  In probability theory, the marginal distribution of   X   X   X   can be computed by integrating out   Y   Y   Y   from the joint density (including the prior distribution on   Y   Y   Y   )        Q   (  X  )   =   âˆ«  Î©   P   (  X  âˆ£  Y  )   d  Ï€   (  Y  )      fragments  Q   fragments  normal-(  X  normal-)     subscript   normal-Î©   P   fragments  normal-(  X  normal-âˆ£  Y  normal-)   d  Ï€   fragments  normal-(  Y  normal-)     Q(X)=\int_{\Omega}P(X\mid Y)\mathrm{d}\pi(Y)        The analog of this rule in the kernel embedding framework states that    Î¼  X  Ï€     superscript   subscript  Î¼  X   Ï€    \mu_{X}^{\pi}   , the RKHS embedding of    Q   (  X  )       Q  X    Q(X)   , can be computed via         Î¼  X  Ï€   =    ğ”¼  Y    [    ğ’   X  âˆ£  Y    Ï•   (  Y  )    ]    =    ğ’   X  âˆ£  Y     ğ”¼  Y    [   Ï•   (  Y  )    ]    =    ğ’   X  âˆ£  Y     Î¼  Y  Ï€           superscript   subscript  Î¼  X   Ï€      subscript  ğ”¼  Y    delimited-[]     subscript  ğ’   fragments  X  normal-âˆ£  Y    Ï•  Y             subscript  ğ’   fragments  X  normal-âˆ£  Y     subscript  ğ”¼  Y    delimited-[]    Ï•  Y             subscript  ğ’   fragments  X  normal-âˆ£  Y     superscript   subscript  Î¼  Y   Ï€       \mu_{X}^{\pi}=\mathbb{E}_{Y}[\mathcal{C}_{X\mid Y}\phi(Y)]=\mathcal{C}_{X\mid Y%
 }\mathbb{E}_{Y}[\phi(Y)]=\mathcal{C}_{X\mid Y}\mu_{Y}^{\pi}   where    Î¼  Y  Ï€     superscript   subscript  Î¼  Y   Ï€    \mu_{Y}^{\pi}   is the kernel embedding of    Ï€   (  Y  )       Ï€  Y    \pi(Y)        In practical implementations, the kernel sum rule takes the following form          Î¼  ^   X  Ï€   =     ğ’  ^    X  âˆ£  Y      Î¼  ^   Y  Ï€    =   ğš¼    (   G  +   Î»  ğˆ    )    -  1     ğ‘®  ~   ğœ¶          superscript   subscript   normal-^  Î¼   X   Ï€      subscript   normal-^  ğ’    fragments  X  normal-âˆ£  Y     superscript   subscript   normal-^  Î¼   Y   Ï€           ğš¼   superscript    G    Î»  ğˆ      1     normal-~  ğ‘®   ğœ¶      \widehat{\mu}_{X}^{\pi}=\widehat{\mathcal{C}}_{X\mid Y}\widehat{\mu}_{Y}^{\pi}%
 =\boldsymbol{\Upsilon}(G+\lambda\mathbf{I})^{-1}\widetilde{\boldsymbol{G}}%
 \boldsymbol{\alpha}        where     Î¼  Y  Ï€   =    âˆ‘   i  =  1    n  ~      Î±  i   Ï•   (    y  ~   i   )          superscript   subscript  Î¼  Y   Ï€     superscript   subscript     i  1     normal-~  n       subscript  Î±  i   Ï•   subscript   normal-~  y   i       \mu_{Y}^{\pi}=\sum_{i=1}^{\widetilde{n}}\alpha_{i}\phi(\widetilde{y}_{i})   is the empirical kernel embedding of the prior distribution,    ğœ¶  =    (   Î±  1   ,  â€¦  ,   Î±   n  ~    )   T       ğœ¶   superscript    subscript  Î±  1   normal-â€¦   subscript  Î±   normal-~  n     T     \boldsymbol{\alpha}=(\alpha_{1},\dots,\alpha_{\widetilde{n}})^{T}   ,    ğš¼  =   (   Ï•   (   x  1   )    ,  â€¦  ,   Ï•   (   x  n   )    )       ğš¼     Ï•   subscript  x  1    normal-â€¦    Ï•   subscript  x  n       \boldsymbol{\Upsilon}=\left(\phi(x_{1}),\dots,\phi(x_{n})\right)   , and    ğ†  ,   ğ†  ~      ğ†   normal-~  ğ†     \mathbf{G},\widetilde{\mathbf{G}}   are Gram matrices with entries      ğ†   i  j    =   k   (   y  i   ,   y  j   )     ,     ğ†  ~    i  j    =   k   (   y  i   ,    y  ~   j   )        formulae-sequence     subscript  ğ†    i  j      k    subscript  y  i    subscript  y  j         subscript   normal-~  ğ†     i  j      k    subscript  y  i    subscript   normal-~  y   j        \mathbf{G}_{ij}=k(y_{i},y_{j}),\widetilde{\mathbf{G}}_{ij}=k(y_{i},\widetilde{%
 y}_{j})   respectively.  Kernel chain rule  In probability theory, a joint distribution can be factorized into a product between conditional and marginal distributions        Q   (  X  ,  Y  )   =  P   (  X  âˆ£  Y  )   Ï€   (  Y  )      fragments  Q   fragments  normal-(  X  normal-,  Y  normal-)    P   fragments  normal-(  X  normal-âˆ£  Y  normal-)   Ï€   fragments  normal-(  Y  normal-)     Q(X,Y)=P(X\mid Y)\pi(Y)        The analog of this rule in the kernel embedding framework states that    ğ’   X  Y   Ï€     superscript   subscript  ğ’    X  Y    Ï€    \mathcal{C}_{XY}^{\pi}   , the joint embedding of    Q   (  X  ,  Y  )       Q   X  Y     Q(X,Y)   , can be factorized as a composition of conditional embedding operator with the auto-covariance operator associated with    Ï€   (  Y  )       Ï€  Y    \pi(Y)            ğ’   X  Y   Ï€   =    ğ’   X  âˆ£  Y     ğ’   Y  Y   Ï€         superscript   subscript  ğ’    X  Y    Ï€      subscript  ğ’   fragments  X  normal-âˆ£  Y     superscript   subscript  ğ’    Y  Y    Ï€      \mathcal{C}_{XY}^{\pi}=\mathcal{C}_{X\mid Y}\mathcal{C}_{YY}^{\pi}   where     ğ’   X  Y   Ï€   =    ğ”¼   X  Y     [     Ï•   (  X  )    âŠ—  Ï•    (  Y  )    ]         superscript   subscript  ğ’    X  Y    Ï€      subscript  ğ”¼    X  Y     delimited-[]     tensor-product    Ï•  X   Ï•   Y       \mathcal{C}_{XY}^{\pi}=\mathbb{E}_{XY}[\phi(X)\otimes\phi(Y)]   and     ğ’   Y  Y   Ï€   =    ğ”¼  Y    [     Ï•   (  Y  )    âŠ—  Ï•    (  Y  )    ]         superscript   subscript  ğ’    Y  Y    Ï€      subscript  ğ”¼  Y    delimited-[]     tensor-product    Ï•  Y   Ï•   Y       \mathcal{C}_{YY}^{\pi}=\mathbb{E}_{Y}[\phi(Y)\otimes\phi(Y)]        In practical implementations, the kernel chain rule takes the following form          ğ’  ^    X  Y   Ï€   =     ğ’  ^    X  âˆ£  Y      ğ’  ^    Y  Y   Ï€    =   ğš¼    (   ğ†  +   Î»  ğˆ    )    -  1     ğ†  ~   diag   (  ğœ¶  )    ğš½  T           superscript   subscript   normal-^  ğ’     X  Y    Ï€      subscript   normal-^  ğ’    fragments  X  normal-âˆ£  Y     superscript   subscript   normal-^  ğ’     Y  Y    Ï€           ğš¼   superscript    ğ†    Î»  ğˆ      1     normal-~  ğ†   diag  ğœ¶   superscript  ğš½  T       \widehat{\mathcal{C}}_{XY}^{\pi}=\widehat{\mathcal{C}}_{X\mid Y}\widehat{%
 \mathcal{C}}_{YY}^{\pi}=\boldsymbol{\Upsilon}(\mathbf{G}+\lambda\mathbf{I})^{-%
 1}\widetilde{\mathbf{G}}\text{diag}(\boldsymbol{\alpha})\boldsymbol{\Phi}^{T}        Kernel Bayes' rule  In probability theory, a posterior distribution can be expressed in terms of a prior distribution and a likelihood function as        Q   (  Y  âˆ£  x  )   =    P   (  x  âˆ£  Y  )   Ï€   (  Y  )     Q   (  x  )        fragments  Q   fragments  normal-(  Y  normal-âˆ£  x  normal-)       fragments  P   fragments  normal-(  x  normal-âˆ£  Y  normal-)   Ï€   fragments  normal-(  Y  normal-)      Q  x      Q(Y\mid x)=\frac{P(x\mid Y)\pi(Y)}{Q(x)}   where    Q   (  x  )   =   âˆ«  Î©   P   (  x  âˆ£  y  )   d  Ï€   (  y  )      fragments  Q   fragments  normal-(  x  normal-)     subscript   normal-Î©   P   fragments  normal-(  x  normal-âˆ£  y  normal-)   d  Ï€   fragments  normal-(  y  normal-)     Q(x)=\int_{\Omega}P(x\mid y)\mathrm{d}\pi(y)        The analog of this rule in the kernel embedding framework expresses the kernel embedding of the conditional distribution in terms of conditional embedding operators which are modified by the prior distribution         Î¼   Y  âˆ£  x   Ï€   =    ğ’   Y  âˆ£  X   Ï€   Ï•   (  x  )    =    ğ’   Y  X   Ï€     (   ğ’   X  X   Ï€   )    -  1    Ï•   (  x  )           superscript   subscript  Î¼   fragments  Y  normal-âˆ£  x    Ï€      superscript   subscript  ğ’   fragments  Y  normal-âˆ£  X    Ï€   Ï•  x           superscript   subscript  ğ’    Y  X    Ï€    superscript   superscript   subscript  ğ’    X  X    Ï€     1    Ï•  x      \mu_{Y\mid x}^{\pi}=\mathcal{C}_{Y\mid X}^{\pi}\phi(x)=\mathcal{C}_{YX}^{\pi}(%
 \mathcal{C}_{XX}^{\pi})^{-1}\phi(x)   where from the chain rule     ğ’   Y  X   Ï€   =    (    ğ’   X  âˆ£  Y     ğ’   Y  Y   Ï€    )   T        superscript   subscript  ğ’    Y  X    Ï€    superscript     subscript  ğ’   fragments  X  normal-âˆ£  Y     superscript   subscript  ğ’    Y  Y    Ï€    T     \mathcal{C}_{YX}^{\pi}=\left(\mathcal{C}_{X\mid Y}\mathcal{C}_{YY}^{\pi}\right%
 )^{T}   .     In practical implementations, the kernel Bayes' rule takes the following form          Î¼  ^    Y  âˆ£  x   Ï€   =     ğ’  ^    Y  X   Ï€     (     (    ğ’  ^    X  X    )   2   +    Î»  ~   ğˆ    )    -  1      ğ’  ^    X  X   Ï€   Ï•   (  x  )    =    ğš½  ~    ğš²  T     (     (  ğƒğŠ  )   2   +    Î»  ~   ğˆ    )    -  1     ğŠğƒğŠ  x           superscript   subscript   normal-^  Î¼    fragments  Y  normal-âˆ£  x    Ï€      superscript   subscript   normal-^  ğ’     Y  X    Ï€    superscript     superscript   subscript   normal-^  ğ’     X  X    2      normal-~  Î»   ğˆ      1     superscript   subscript   normal-^  ğ’     X  X    Ï€   Ï•  x           normal-~  ğš½    superscript  ğš²  T    superscript     superscript  ğƒğŠ  2      normal-~  Î»   ğˆ      1     subscript  ğŠğƒğŠ  x       \widehat{\mu}_{Y\mid x}^{\pi}=\widehat{\mathcal{C}}_{YX}^{\pi}\left((\widehat{%
 \mathcal{C}}_{XX})^{2}+\widetilde{\lambda}\mathbf{I}\right)^{-1}\widehat{%
 \mathcal{C}}_{XX}^{\pi}\phi(x)=\widetilde{\boldsymbol{\Phi}}\boldsymbol{%
 \Lambda}^{T}\left((\mathbf{D}\mathbf{K})^{2}+\widetilde{\lambda}\mathbf{I}%
 \right)^{-1}\mathbf{K}\mathbf{D}\mathbf{K}_{x}        where     ğš²  =     (   ğ†  +    Î»  ~   ğˆ    )    -  1     ğ†  ~   diag   (  ğœ¶  )     ,   ğƒ  =   diag   (     (   ğ†  +    Î»  ~   ğˆ    )    -  1     ğ†  ~   ğœ¶   )        formulae-sequence    ğš²     superscript    ğ†     normal-~  Î»   ğˆ      1     normal-~  ğ†   diag  ğœ¶      ğƒ    diag     superscript    ğ†     normal-~  Î»   ğˆ      1     normal-~  ğ†   ğœ¶       \boldsymbol{\Lambda}=\left(\mathbf{G}+\widetilde{\lambda}\mathbf{I}\right)^{-1%
 }\widetilde{\mathbf{G}}\text{diag}(\boldsymbol{\alpha}),\mathbf{D}=\text{diag}%
 \left(\left(\mathbf{G}+\widetilde{\lambda}\mathbf{I}\right)^{-1}\widetilde{%
 \mathbf{G}}\boldsymbol{\alpha}\right)   . Two regularization parameters are used in this framework   Î»   Î»   \lambda   for the estimation of       ğ’  ^    Y  X   Ï€   ,    ğ’  ^    X  X   Ï€    =   ğš¼  ğƒ   ğš¼  T          superscript   subscript   normal-^  ğ’     Y  X    Ï€    superscript   subscript   normal-^  ğ’     X  X    Ï€      ğš¼  ğƒ   superscript  ğš¼  T      \widehat{\mathcal{C}}_{YX}^{\pi},\widehat{\mathcal{C}}_{XX}^{\pi}=\boldsymbol{%
 \Upsilon}\mathbf{D}\boldsymbol{\Upsilon}^{T}   and    Î»  ~     normal-~  Î»    \widetilde{\lambda}   for the estimation of the final conditional embedding operator      ğ’  ^    Y  âˆ£  X   Ï€   =     ğ’  ^    Y  X   Ï€     (     (    ğ’  ^    X  X   Ï€   )   2   +    Î»  ~   ğˆ    )    -  1      ğ’  ^    X  X   Ï€         superscript   subscript   normal-^  ğ’    fragments  Y  normal-âˆ£  X    Ï€      superscript   subscript   normal-^  ğ’     Y  X    Ï€    superscript     superscript   superscript   subscript   normal-^  ğ’     X  X    Ï€   2      normal-~  Î»   ğˆ      1     superscript   subscript   normal-^  ğ’     X  X    Ï€      \widehat{\mathcal{C}}_{Y\mid X}^{\pi}=\widehat{\mathcal{C}}_{YX}^{\pi}\left((%
 \widehat{\mathcal{C}}_{XX}^{\pi})^{2}+\widetilde{\lambda}\mathbf{I}\right)^{-1%
 }\widehat{\mathcal{C}}_{XX}^{\pi}   . The latter regularization is done on square of     ğ’  ^    X  X   Ï€     superscript   subscript   normal-^  ğ’     X  X    Ï€    \widehat{\mathcal{C}}_{XX}^{\pi}   because   D   D   D   may not be positive definite .  Applications  Measuring distance between distributions  The maximum mean discrepancy (MMD) is a distance-measure between distributions    P   (  X  )       P  X    P(X)   and    Q   (  Y  )       Q  Y    Q(Y)   which is defined as the squared distance between their embeddings in the RKHS 24         MMD   (  P  ,  Q  )    =    ||    Î¼  X   -   Î¼  Y    ||   â„‹  2         MMD   P  Q     superscript   subscript   norm     subscript  Î¼  X    subscript  Î¼  Y     â„‹   2     \text{MMD}(P,Q)=\left|\left|\mu_{X}-\mu_{Y}\right|\right|_{\mathcal{H}}^{2}        While most distance-measures between distributions such as the widely used Kullbackâ€“Leibler divergence either require density estimation (either parametrically or nonparametrically) or space partitioning/bias correction strategies, 25 the MMD is easily estimated as an empirical mean which is concentrated around the true value of the MMD. The characterization of this distance as the maximum mean discrepancy refers to the fact that computing the MMD is equivalent to finding the RKHS function that maximizes the difference in expectations between the two probability distributions         MMD   (  P  ,  Q  )    =    sup     ||  f  ||   â„‹   â‰¤  1     (     ğ”¼  X    [   f   (  X  )    ]    -    ğ”¼  Y    [   f   (  Y  )    ]     )          MMD   P  Q      subscript  supremum     subscript   norm  f   â„‹   1         subscript  ğ”¼  X    delimited-[]    f  X        subscript  ğ”¼  Y    delimited-[]    f  Y         \text{MMD}(P,Q)=\sup_{||f||_{\mathcal{H}}\leq 1}\left(\mathbb{E}_{X}[f(X)]-%
 \mathbb{E}_{Y}[f(Y)]\right)        Kernel two sample test  Given n training examples from    P   (  X  )       P  X    P(X)   and m samples from    Q   (  Y  )       Q  Y    Q(Y)   , one can formulate a test statistic based on the empirical estimate of the MMD          MMD  ^    (  P  ,  Q  )    =    ||     1  n     âˆ‘   i  =  1   n    Ï•   (   x  i   )      -    1  m     âˆ‘   i  =  1   m    Ï•   (   y  i   )       ||   â„‹  2   =    1   n  m      âˆ‘   i  =  1   n     âˆ‘   j  =  1   m    [     k   (   x  i   ,   x  j   )    +   k   (   y  i   ,   y  j   )     -   2  k   (   x  i   ,   y  j   )     ]               normal-^  MMD    P  Q     superscript   subscript   norm        1  n     superscript   subscript     i  1    n     Ï•   subscript  x  i          1  m     superscript   subscript     i  1    m     Ï•   subscript  y  i        â„‹   2            1    n  m      superscript   subscript     i  1    n     superscript   subscript     j  1    m    delimited-[]        k    subscript  x  i    subscript  x  j       k    subscript  y  i    subscript  y  j        2  k    subscript  x  i    subscript  y  j             \widehat{\text{MMD}}(P,Q)=\left|\left|\frac{1}{n}\sum_{i=1}^{n}\phi(x_{i})-%
 \frac{1}{m}\sum_{i=1}^{m}\phi(y_{i})\right|\right|_{\mathcal{H}}^{2}=\frac{1}{%
 nm}\sum_{i=1}^{n}\sum_{j=1}^{m}\left[k(x_{i},x_{j})+k(y_{i},y_{j})-2k(x_{i},y_%
 {j})\right]        to obtain a two-sample test  26 of the null hypothesis that both samples stem from the same distribution (i.e.    P  =  Q      P  Q    P=Q   ) against the broad alternative    P  â‰   Q      P  Q    P\neq Q   .  Density estimation via kernel embeddings  Although learning algorithms in the kernel embedding framework circumvent the need for intermediate density estimation, one may nonetheless use the empirical embedding to perform density estimation based on n samples drawn from an underlying distribution    P  X  *     superscript   subscript  P  X      P_{X}^{*}   . This can be done by solving the following optimization problem 27 28          max   P  X    H    (   P  X   )         subscript    subscript  P  X    H    subscript  P  X     \max_{P_{X}}H(P_{X})   subject to      ||     Î¼  ^   X   -    Î¼  X    [   P  X   ]     ||   â„‹   â‰¤  Ïµ       subscript   norm     subscript   normal-^  Î¼   X      subscript  Î¼  X    delimited-[]   subscript  P  X       â„‹   Ïµ    ||\widehat{\mu}_{X}-\mu_{X}[P_{X}]||_{\mathcal{H}}\leq\epsilon        where the maximization is done over the entire space of distributions on   Î©   normal-Î©   \Omega   . Here,     Î¼  X    [   P  X   ]        subscript  Î¼  X    delimited-[]   subscript  P  X      \mu_{X}[P_{X}]   is the kernel embedding of the proposed density    P  X     subscript  P  X    P_{X}   and   H   H   H   is an entropy-like quantity (e.g. Entropy , KL divergence , Bregman divergence ). The distribution which solves this optimization may be interpreted as a compromise between fitting the empirical kernel means of the samples well, while still allocating a substantial portion of the probability mass to all regions of the probability space (much of which may not be represented in the training examples). In practice, a good approximate solution of the difficult optimization may be found by restricting the space of candidate densities to a mixture of M candidate distributions with regularized mixing proportions. Connections between the ideas underlying Gaussian processes and conditional random fields may be drawn with the estimation of conditional probability distributions in this fashion, if one views the feature mappings associated with the kernel as sufficient statistics in generalized (possibly infinite-dimensional) exponential families . 29  Measuring dependence of random variables  A measure of the statistical dependence between random variables   X   X   X   and   Y   Y   Y   (from any domains on which sensible kernels can be defined) can be formulated based on the Hilbertâ€“Schmidt Independence Criterion 30         HSIC   (  X  ,  Y  )    =    ||    ğ’   X  Y    -    Î¼  X   âŠ—   Î¼  Y     ||    â„‹  âŠ—  â„‹   2         HSIC   X  Y     superscript   subscript   norm     subscript  ğ’    X  Y     tensor-product   subscript  Î¼  X    subscript  Î¼  Y       tensor-product  â„‹  â„‹    2     \text{HSIC}(X,Y)=\left|\left|\mathcal{C}_{XY}-\mu_{X}\otimes\mu_{Y}\right|%
 \right|_{\mathcal{H}\otimes\mathcal{H}}^{2}        and can be used as a principled replacement for mutual information , Pearson correlation or any other dependence measure used in learning algorithms. Most notably, HSIC can detect arbitrary dependencies (when a characteristic kernel is used in the embeddings, HSIC is zero if and only if the variables are independent ), and can be used to measure dependence between different types of data (e.g. images and text captions). Given n i.i.d. samples of each random variable, a simple parameter-free unbiased estimator of HSIC which exhibits concentration about the true value can be computed in    O   (   n   (    d  f  2   +   d  g  2    )    )       O    n     superscript   subscript  d  f   2    superscript   subscript  d  g   2       O(n(d_{f}^{2}+d_{g}^{2}))   time, 31 where the Gram matrices of the two datasets are approximated using     ğ€ğ€  T   ,   ğğ  T       superscript  ğ€ğ€  T    superscript  ğğ  T     \mathbf{A}\mathbf{A}^{T},\mathbf{B}\mathbf{B}^{T}   with     ğ€  âˆˆ   â„   n  Ã—   d  f      ,   ğ  âˆˆ   â„   n  Ã—   d  g         formulae-sequence    ğ€   superscript  â„    n   subscript  d  f        ğ   superscript  â„    n   subscript  d  g        \mathbf{A}\in\mathbb{R}^{n\times d_{f}},\mathbf{B}\in\mathbb{R}^{n\times d_{g}}   . The desirable properties of HSIC have led to the formulation of numerous algorithms which utilize this dependence measure for a variety of common machine learning tasks such as: feature selection (BAHSIC 32 ), clustering (CLUHSIC 33 ), and dimensionality reduction (MUHSIC 34 ).  Kernel belief propagation  Belief propagation is a fundamental algorithm for inference in graphical models in which nodes repeatedly pass and receive messages corresponding to the evaluation of conditional expectations. In the kernel embedding framework, the messages may be represented as RKHS functions and the conditional distribution embeddings can be applied to efficiently compute message updates. Given n samples of random variables represented by nodes in a Markov Random Field , the incoming message to node t from node u can be expressed as      m   u  t     (  â‹…  )    =    âˆ‘   i  =  1   n     Î²   u  t   i   Ï•   (   x  t  i   )            subscript  m    u  t    normal-â‹…     superscript   subscript     i  1    n      superscript   subscript  Î²    u  t    i   Ï•   superscript   subscript  x  t   i       m_{ut}(\cdot)=\sum_{i=1}^{n}\beta_{ut}^{i}\phi(x_{t}^{i})   if it assumed to lie in the RKHS. The kernel belief propagation update message from t to node s is then given by 35          m  ^    t  s    =    (   âŠ™   u  âˆˆ    N   (  t  )    \  s      ğŠ  t    ğœ·   u  t    )   T     (   ğŠ  s   +  Î»  ğˆ  )    -  1     ğš¼  s  T   Ï•   (   x  s   )      fragments   subscript   normal-^  m     t  s      superscript   fragments  normal-(   subscript  direct-product    u   normal-\    N  t   s      subscript  ğŠ  t    subscript  ğœ·    u  t    normal-)   T    superscript   fragments  normal-(   subscript  ğŠ  s    Î»  I  normal-)     1     superscript   subscript  ğš¼  s   T   Ï•   fragments  normal-(   subscript  x  s   normal-)     \widehat{m}_{ts}=\left(\odot_{u\in N(t)\backslash s}\mathbf{K}_{t}\boldsymbol{%
 \beta}_{ut}\right)^{T}(\mathbf{K}_{s}+\lambda\mathbf{I})^{-1}\boldsymbol{%
 \Upsilon}_{s}^{T}\phi(x_{s})        where   âŠ™   direct-product   \odot   denotes the element-wise vector product,     N   (  t  )    \  s     normal-\    N  t   s    N(t)\backslash s   is the set of nodes connected to t excluding node s ,     ğœ·   u  t    =   (   Î²   u  t   1   ,  â€¦  ,   Î²   u  t   n   )        subscript  ğœ·    u  t      superscript   subscript  Î²    u  t    1   normal-â€¦   superscript   subscript  Î²    u  t    n      \boldsymbol{\beta}_{ut}=\left(\beta_{ut}^{1},\dots,\beta_{ut}^{n}\right)   ,     ğŠ  t   ,   ğŠ  s       subscript  ğŠ  t    subscript  ğŠ  s     \mathbf{K}_{t},\mathbf{K}_{s}   are the Gram matrices of the samples from variables     X  t   ,   X  s       subscript  X  t    subscript  X  s     X_{t},X_{s}   , respectively, and     ğš¼  s   =   (   Ï•   (   x  s  1   )    ,  â€¦  ,   Ï•   (   x  s  n   )    )        subscript  ğš¼  s      Ï•   superscript   subscript  x  s   1    normal-â€¦    Ï•   superscript   subscript  x  s   n       \boldsymbol{\Upsilon}_{s}=\left(\phi(x_{s}^{1}),\dots,\phi(x_{s}^{n})\right)   is the feature matrix for the samples from    X  s     subscript  X  s    X_{s}   .  Thus, if the incoming messages to node t are linear combinations of feature mapped samples from    X  t     subscript  X  t    X_{t}   , then the outgoing message from this node is also a linear combination of feature mapped samples from    X  s     subscript  X  s    X_{s}   . This RKHS function representation of message-passing updates therefore produces an efficient belief propagation algorithm in which the potentials are nonparametric functions inferred from the data so that arbitrary statistical relationships may be modeled. 36  Nonparametric filtering in hidden Markov models  In the hidden Markov model (HMM), two key quantities of interest are the transition probabilities between hidden states    P   (   S  t   âˆ£   S   t  -  1    )      fragments  P   fragments  normal-(   superscript  S  t   normal-âˆ£   superscript  S    t  1    normal-)     P(S^{t}\mid S^{t-1})   and the emission probabilities    P   (   O  t   âˆ£   S  t   )      fragments  P   fragments  normal-(   superscript  O  t   normal-âˆ£   superscript  S  t   normal-)     P(O^{t}\mid S^{t})   for observations. Using the kernel conditional distribution embedding framework, these quantities may be expressed in terms of samples from the HMM. A serious limitation of the embedding methods in this domain is the need for training samples containing hidden states, as otherwise inference with arbitrary distributions in the HMM is not possible.  One common use of HMMs is filtering in which the goal is to estimate posterior distribution over the hidden state    s  t     superscript  s  t    s^{t}   at time step t given a history of previous observations     h  t   =   (   o  1   ,  â€¦  ,   o  t   )        superscript  h  t     superscript  o  1   normal-â€¦   superscript  o  t      h^{t}=(o^{1},\dots,o^{t})   from the system. In filtering, a belief state     P   (   S   t  +  1    âˆ£   h   t  +  1    )      fragments  P   fragments  normal-(   superscript  S    t  1    normal-âˆ£   superscript  h    t  1    normal-)     P(S^{t+1}\mid h^{t+1})   is recursively maintained via a prediction step (where updates    P   (   S   t  +  1    âˆ£   h  t   )   =   ğ”¼    S  t   âˆ£   h  t      [  P   (   S   t  +  1    âˆ£   S  t   )   ]      fragments  P   fragments  normal-(   superscript  S    t  1    normal-âˆ£   superscript  h  t   normal-)     subscript  ğ”¼   fragments   superscript  S  t   normal-âˆ£   superscript  h  t      fragments  normal-[  P   fragments  normal-(   superscript  S    t  1    normal-âˆ£   superscript  S  t   normal-)   normal-]     P(S^{t+1}\mid h^{t})=\mathbb{E}_{S^{t}\mid h^{t}}[P(S^{t+1}\mid S^{t})]   are computed by marginalizing out the previous hidden state) followed by a conditioning step (where updates    P   (   S   t  +  1    âˆ£   h  t   ,   o   t  +  1    )   âˆ  P   (   o   t  +  1    âˆ£   S   t  +  1    )   P   (   S   t  +  1    âˆ£   h  t   )      fragments  P   fragments  normal-(   superscript  S    t  1    normal-âˆ£   superscript  h  t   normal-,   superscript  o    t  1    normal-)   proportional-to  P   fragments  normal-(   superscript  o    t  1    normal-âˆ£   superscript  S    t  1    normal-)   P   fragments  normal-(   superscript  S    t  1    normal-âˆ£   superscript  h  t   normal-)     P(S^{t+1}\mid h^{t},o^{t+1})\propto P(o^{t+1}\mid S^{t+1})P(S^{t+1}\mid h^{t})   are computed by applying Bayes' rule to condition on a new observation). 37 The RKHS embedding of the belief state at time t+1 can be recursively expressed as         Î¼    S   t  +  1    âˆ£   h   t  +  1      =    ğ’    S   t  +  1     O   t  +  1     Ï€     (   ğ’    O   t  +  1     O   t  +  1     Ï€   )    -  1    Ï•   (   o   t  +  1    )         subscript  Î¼   fragments   superscript  S    t  1    normal-âˆ£   superscript  h    t  1         superscript   subscript  ğ’     superscript  S    t  1     superscript  O    t  1      Ï€    superscript   superscript   subscript  ğ’     superscript  O    t  1     superscript  O    t  1      Ï€     1    Ï•   superscript  o    t  1       \mu_{S^{t+1}\mid h^{t+1}}=\mathcal{C}_{S^{t+1}O^{t+1}}^{\pi}\left(\mathcal{C}_%
 {O^{t+1}O^{t+1}}^{\pi}\right)^{-1}\phi(o^{t+1})        by computing the embeddings of the prediction step via the kernel sum rule and the embedding of the conditioning step via kernel Bayes' rule . Assuming a training sample    (    s  ~   1   ,  â€¦  ,    s  ~   T   ,    o  ~   1   ,  â€¦  ,    o  ~   T   )      superscript   normal-~  s   1   normal-â€¦   superscript   normal-~  s   T    superscript   normal-~  o   1   normal-â€¦   superscript   normal-~  o   T     (\widetilde{s}^{1},\dots,\widetilde{s}^{T},\widetilde{o}^{1},\dots,\widetilde{%
 o}^{T})   is given, one can in practice estimate      Î¼  ^     S   t  +  1    âˆ£   h   t  +  1      =    âˆ‘   i  =  1   T     Î±  i  t   Ï•   (    s  ~   t   )          subscript   normal-^  Î¼    fragments   superscript  S    t  1    normal-âˆ£   superscript  h    t  1        superscript   subscript     i  1    T      superscript   subscript  Î±  i   t   Ï•   superscript   normal-~  s   t       \widehat{\mu}_{S^{t+1}\mid h^{t+1}}=\sum_{i=1}^{T}\alpha_{i}^{t}\phi(%
 \widetilde{s}^{t})   and filtering with kernel embeddings is thus implemented recursively using the following updates for the weights    ğœ¶  =   (   Î±  1   ,  â€¦  ,   Î±  T   )       ğœ¶    subscript  Î±  1   normal-â€¦   subscript  Î±  T      \boldsymbol{\alpha}=(\alpha_{1},\dots,\alpha_{T})    38         ğƒ   t  +  1    =   diag   (     (   G  +   Î»  ğˆ    )    -  1     G  ~    ğœ¶  t    )         superscript  ğƒ    t  1      diag     superscript    G    Î»  ğˆ      1     normal-~  G    superscript  ğœ¶  t       \mathbf{D}^{t+1}=\text{diag}\left((G+\lambda\mathbf{I})^{-1}\widetilde{G}%
 \boldsymbol{\alpha}^{t}\right)          ğœ¶   t  +  1    =    ğƒ   t  +  1    ğŠ    (     (    ğƒ   t  +  1    K   )   2   +    Î»  ~   ğˆ    )    -  1     ğƒ   t  +  1     ğŠ   o   t  +  1           superscript  ğœ¶    t  1       superscript  ğƒ    t  1    ğŠ   superscript     superscript     superscript  ğƒ    t  1    K   2      normal-~  Î»   ğˆ      1     superscript  ğƒ    t  1     subscript  ğŠ   superscript  o    t  1        \boldsymbol{\alpha}^{t+1}=\mathbf{D}^{t+1}\mathbf{K}\left((\mathbf{D}^{t+1}K)^%
 {2}+\widetilde{\lambda}\mathbf{I}\right)^{-1}\mathbf{D}^{t+1}\mathbf{K}_{o^{t+%
 1}}        where    ğ†  ,  ğŠ     ğ†  ğŠ    \mathbf{G},\mathbf{K}   denote the Gram matrices of      s  ~   1   ,  â€¦  ,    s  ~   T       superscript   normal-~  s   1   normal-â€¦   superscript   normal-~  s   T     \widetilde{s}^{1},\dots,\widetilde{s}^{T}   and      o  ~   1   ,  â€¦  ,    o  ~   T       superscript   normal-~  o   1   normal-â€¦   superscript   normal-~  o   T     \widetilde{o}^{1},\dots,\widetilde{o}^{T}   respectively,    ğ†  ~     normal-~  ğ†    \widetilde{\mathbf{G}}   is a transfer Gram matrix defined as      ğ†  ~    i  j    =   k   (    s  ~   i   ,    s  ~    j  +  1    )         subscript   normal-~  ğ†     i  j      k    subscript   normal-~  s   i    subscript   normal-~  s     j  1        \widetilde{\mathbf{G}}_{ij}=k(\widetilde{s}_{i},\widetilde{s}_{j+1})   , and     ğŠ   o   t  +  1     =    (   k   (    o  ~   1   ,   o   t  +  1    )    ,  â€¦  ,   k   (    o  ~   T   ,   o   t  +  1    )    )   T        subscript  ğŠ   superscript  o    t  1      superscript     k    superscript   normal-~  o   1    superscript  o    t  1      normal-â€¦    k    superscript   normal-~  o   T    superscript  o    t  1       T     \mathbf{K}_{o^{t+1}}=(k(\widetilde{o}^{1},o^{t+1}),\dots,k(\widetilde{o}^{T},o%
 ^{t+1}))^{T}   .  Support measure machines  The support measure machine (SMM) is a generalization of the support vector machine (SVM) in which the training examples are probability distributions paired with labels       {   P  i   ,   y  i   }    i  =  1   n   ,   y  i    âˆˆ   {   +  1   ,   -  1   }         superscript   subscript    subscript  P  i    subscript  y  i      i  1    n    subscript  y  i       1     1      \{P_{i},y_{i}\}_{i=1}^{n},\ y_{i}\in\{+1,-1\}   . 39 SMMs solve the standard SVM dual optimization problem using the following expected kernel         K   (   P   (  X  )    ,   Q   (  Z  )    )    =    âŸ¨   Î¼  X   ,   Î¼  Z   âŸ©   â„‹   =    ğ”¼   X  Z     [   k   (  x  ,  z  )    ]            K     P  X     Q  Z      subscript    subscript  Î¼  X    subscript  Î¼  Z    â„‹           subscript  ğ”¼    X  Z     delimited-[]    k   x  z         K\left(P(X),Q(Z)\right)=\langle\mu_{X},\mu_{Z}\rangle_{\mathcal{H}}=\mathbb{E}%
 _{XZ}[k(x,z)]        which is computable in closed form for many common specific distributions    P  i     subscript  P  i    P_{i}   (such as the Gaussian distribution) combined with popular embedding kernels   k   k   k   (e.g. the Gaussian kernel or polynomial kernel), or can be accurately empirically estimated from i.i.d. samples       {   x  i   }    i  =  1   n   âˆ¼   P   (  X  )     ,     {   z  j   }    j  =  1   m   âˆ¼   Q   (  Z  )        formulae-sequence   similar-to   superscript   subscript    subscript  x  i      i  1    n     P  X     similar-to   superscript   subscript    subscript  z  j      j  1    m     Q  Z      \{x_{i}\}_{i=1}^{n}\sim P(X),\{z_{j}\}_{j=1}^{m}\sim Q(Z)   via          K  ^    (  X  ,  Z  )    =    1   n  m      âˆ‘   i  =  1   n     âˆ‘   j  =  1   m    k   (   x  i   ,   z  j   )              normal-^  K    X  Z        1    n  m      superscript   subscript     i  1    n     superscript   subscript     j  1    m     k    subscript  x  i    subscript  z  j          \widehat{K}\left(X,Z\right)=\frac{1}{nm}\sum_{i=1}^{n}\sum_{j=1}^{m}k(x_{i},z_%
 {j})        Under certain choices of the embedding kernel   k   k   k   , the SMM applied to training examples     {   P  i   ,   y  i   }    i  =  1   n     superscript   subscript    subscript  P  i    subscript  y  i      i  1    n    \{P_{i},y_{i}\}_{i=1}^{n}   is equivalent to a SVM trained on samples     {   x  i   ,   y  i   }    i  =  1   n     superscript   subscript    subscript  x  i    subscript  y  i      i  1    n    \{x_{i},y_{i}\}_{i=1}^{n}   , and thus the SMM can be viewed as a flexible SVM in which a different data-dependent kernel (specified by the assumed form of the distribution    P  i     subscript  P  i    P_{i}   ) may be placed on each training point. 40  Domain adaptation under covariate, target, and conditional shift  The goal of domain adaptation is the formulation of learning algorithms which generalize well when the training and test data have different distributions. Given training examples     {   (   x  i   t  r    ,   y  i   t  r    )   }    i  =  1   n     superscript   subscript     superscript   subscript  x  i     t  r     superscript   subscript  y  i     t  r        i  1    n    \{(x_{i}^{tr},y_{i}^{tr})\}_{i=1}^{n}   and a test set     {   (   x  j   t  e    ,   y  j   t  e    )   }    j  =  1   m     superscript   subscript     superscript   subscript  x  j     t  e     superscript   subscript  y  j     t  e        j  1    m    \{(x_{j}^{te},y_{j}^{te})\}_{j=1}^{m}   where the    y  j   t  e      superscript   subscript  y  j     t  e     y_{j}^{te}   are unknown, three types of differences are commonly assumed between the distribution of the training examples     P   t  r     (  X  ,  Y  )        superscript  P    t  r     X  Y     P^{tr}(X,Y)   and the test distribution     P   t  e     (  X  ,  Y  )        superscript  P    t  e     X  Y     P^{te}(X,Y)   : 41 42   Covariate Shift in which the marginal distribution of the covariates changes across domains      P   t  r     (  X  )    â‰     P   t  e     (  X  )           superscript  P    t  r    X      superscript  P    t  e    X     P^{tr}(X)\neq P^{te}(X)     Target Shift in which the marginal distribution of the outputs changes across domains      P   t  r     (  Y  )    â‰     P   t  e     (  Y  )           superscript  P    t  r    Y      superscript  P    t  e    Y     P^{tr}(Y)\neq P^{te}(Y)     Conditional Shift in which    P   (  Y  )       P  Y    P(Y)   remains the same across domains, but the conditional distributions differ     P   t  r     (  X  âˆ£  Y  )   â‰    P   t  e     (  X  âˆ£  Y  )      fragments   superscript  P    t  r     fragments  normal-(  X  normal-âˆ£  Y  normal-)     superscript  P    t  e     fragments  normal-(  X  normal-âˆ£  Y  normal-)     P^{tr}(X\mid Y)\neq P^{te}(X\mid Y)   . In general, the presence of conditional shift leads to an ill-posed problem, and the additional assumption that    P   (  X  âˆ£  Y  )      fragments  P   fragments  normal-(  X  normal-âˆ£  Y  normal-)     P(X\mid Y)   changes only under location - scale (LS) transformations on   X   X   X   is commonly imposed to make the problem tractable.   By utilizing the kernel embedding of marginal and conditional distributions, practical approaches to deal with the presence of these types of differences between training and test domains can be formulated. Covariate shift may be accounted for by reweighting examples via estimates of the ratio       P   t  e     (  X  )    /   P   t  r      (  X  )            superscript  P    t  e    X    superscript  P    t  r     X    P^{te}(X)/P^{tr}(X)   obtained directly from the kernel embeddings of the marginal distributions of   X   X   X   in each domain without any need for explicit estimation of the distributions. 43 Target shift, which cannot be similarly dealt with since no samples from   Y   Y   Y   are available in the test domain, is accounted for by weighting training examples using the vector     ğœ·  *    (   ğ²   t  r    )        superscript  ğœ·     superscript  ğ²    t  r      \boldsymbol{\beta}^{*}(\mathbf{y}^{tr})   which solves the following optimization problem (where in practice, empirical approximations must be used) 44         min   ğœ·   (  y  )       ||     ğ’    (  X  âˆ£  Y  )    t  r      ğ”¼   Y   t  r      [   ğœ·   (  y  )   Ï•   (  y  )    ]    -   Î¼   X   t  e      ||   â„‹  2        subscript     ğœ·  y     superscript   subscript   norm       subscript  ğ’   superscript   fragments  normal-(  X  normal-âˆ£  Y  normal-)     t  r      subscript  ğ”¼   superscript  Y    t  r      delimited-[]    ğœ·  y  Ï•  y      subscript  Î¼   superscript  X    t  e       â„‹   2     \min_{\boldsymbol{\beta}(y)}\left|\left|\mathcal{C}_{{(X\mid Y)}^{tr}}\mathbb{%
 E}_{Y^{tr}}[\boldsymbol{\beta}(y)\phi(y)]-\mu_{X^{te}}\right|\right|_{\mathcal%
 {H}}^{2}   subject to      ğœ·   (  y  )    â‰¥  0   ,     ğ”¼   Y   t  r      [   ğœ·   (  y  )    ]    =  1      formulae-sequence      ğœ·  y   0        subscript  ğ”¼   superscript  Y    t  r      delimited-[]    ğœ·  y     1     \boldsymbol{\beta}(y)\geq 0,\mathbb{E}_{Y^{tr}}[\boldsymbol{\beta}(y)]=1        To deal with location scale conditional shift, one can perform a LS transformation of the training points to obtain new transformed training data     ğ—   n  e  w    =     ğ—   t  r    âŠ™  ğ–   +  ğ        superscript  ğ—    n  e  w       direct-product   superscript  ğ—    t  r    ğ–   ğ     \mathbf{X}^{new}=\mathbf{X}^{tr}\odot\mathbf{W}+\mathbf{B}   (where   âŠ™   direct-product   \odot   denotes the element-wise vector product). To ensure similar distributions between the new transformed training samples and the test data,    ğ–  ,  ğ     ğ–  ğ    \mathbf{W},\mathbf{B}   are estimated by minimizing the following empirical kernel embedding distance 45          ||     Î¼  ^    X   n  e  w     -    Î¼  ^    X   t  e      ||   â„‹  2   =    ||      ğ’  ^     (  X  âˆ£  Y  )    n  e  w       Î¼  ^    Y   t  r      -    Î¼  ^    X   t  e      ||   â„‹  2        superscript   subscript   norm     subscript   normal-^  Î¼    superscript  X    n  e  w      subscript   normal-^  Î¼    superscript  X    t  e       â„‹   2    superscript   subscript   norm       subscript   normal-^  ğ’    superscript   fragments  normal-(  X  normal-âˆ£  Y  normal-)     n  e  w      subscript   normal-^  Î¼    superscript  Y    t  r       subscript   normal-^  Î¼    superscript  X    t  e       â„‹   2     \left|\left|\widehat{\mu}_{X^{new}}-\widehat{\mu}_{X^{te}}\right|\right|_{%
 \mathcal{H}}^{2}=\left|\left|\widehat{\mathcal{C}}_{(X\mid Y)^{new}}\widehat{%
 \mu}_{Y^{tr}}-\widehat{\mu}_{X^{te}}\right|\right|_{\mathcal{H}}^{2}        In general, the kernel embedding methods for dealing with LS conditional shift and target shift may be combined to find a reweighted transformation of the training data which mimics the test distribution, and these methods may perform well even in the presence of conditional shifts other than location-scale changes. 46  Domain generalization via invariant feature representation  Given N sets of training examples sampled i.i.d. from distributions      P   (  1  )     (  X  ,  Y  )    ,    P   (  2  )     (  X  ,  Y  )    ,  â€¦  ,    P   (  N  )     (  X  ,  Y  )          superscript  P  1    X  Y       superscript  P  2    X  Y    normal-â€¦     superscript  P  N    X  Y      P^{(1)}(X,Y),P^{(2)}(X,Y),\dots,P^{(N)}(X,Y)   , the goal of domain generalization is to formulate learning algorithms which perform well on test examples sampled from a previously unseen domain     P  *    (  X  ,  Y  )        superscript  P     X  Y     P^{*}(X,Y)   where no data from the test domain is available at training time. If conditional distributions    P   (  Y  âˆ£  X  )      fragments  P   fragments  normal-(  Y  normal-âˆ£  X  normal-)     P(Y\mid X)   are assumed to be relatively similar across all domains, then a learner capable of domain generalization must estimate a functional relationship between the variables which is robust to changes in the marginals    P   (  X  )       P  X    P(X)   . Based on kernel embeddings of these distributions, Domain Invariant Component Analysis (DICA) is a method which determines the transformation of the training data that minimizes the difference between marginal distributions while preserving a common conditional distribution shared between all training domains. 47 DICA thus extracts invariants , features that transfer across domains, and may be viewed as a generalization of many popular dimension-reduction methods such as kernel principal component analysis , transfer component analysis, and covariance operator inverse regression. 48  Defining a probability distribution   ğ’«   ğ’«   \mathcal{P}   on the RKHS   â„‹   â„‹   \mathcal{H}   with      ğ’«   (   Î¼    X   (  i  )     Y   (  i  )      )    =    1  /  N   for  i   =  1   ,   â€¦  ,  N      formulae-sequence        ğ’«   subscript  Î¼     superscript  X  i    superscript  Y  i          1  N   for  i        1     normal-â€¦  N     \mathcal{P}(\mu_{X^{(i)}Y^{(i)}})=1/N\text{ for }i=1,\dots,N   , DICA measures dissimilarity between domains via distributional variance which is computed as          V  â„‹    (  ğ’«  )    =     1  N   tr   (  ğ†  )    -    1   N  2      âˆ‘    i  ,  j   =  1   N    ğ†   i  j              subscript  V  â„‹   ğ’«         1  N   tr  ğ†       1   superscript  N  2      superscript   subscript      i  j   1    N    subscript  ğ†    i  j         V_{\mathcal{H}}(\mathcal{P})=\frac{1}{N}\text{tr}(\mathbf{G})-\frac{1}{N^{2}}%
 \sum_{i,j=1}^{N}\mathbf{G}_{ij}   where     ğ†   i  j    =    âŸ¨   Î¼   X   (  i  )     ,   Î¼   X   (  j  )     âŸ©   â„‹        subscript  ğ†    i  j     subscript    subscript  Î¼   superscript  X  i     subscript  Î¼   superscript  X  j     â„‹     \mathbf{G}_{ij}=\langle\mu_{X^{(i)}},\mu_{X^{(j)}}\rangle_{\mathcal{H}}        so   ğ†   ğ†   \mathbf{G}   is a    N  Ã—  N      N  N    N\times N   Gram matrix over the distributions from which the training data are sampled. Finding an orthogonal transform onto a low-dimensional subspace  B (in the feature space) which minimizes the distributional variance, DICA simultaneously ensures that B aligns with the bases of a central subspace  C for which   Y   Y   Y   becomes independent of   X   X   X   given     C  T   X       superscript  C  T   X    C^{T}X   across all domains. In the absence of target values   Y   Y   Y   , an unsupervised version of DICA may be formulated which finds a low-dimensional subspace that minimizes distributional variance while simultaneously maximizing the variance of   X   X   X   (in the feature space) across all domains (rather than preserving a central subspace). 49  Example  In this simple example, which is taken from Song et al., 50     X  ,  Y     X  Y    X,Y   are assumed to be discrete random variables which take values in the set    {  1  ,  â€¦  ,  K  }     1  normal-â€¦  K    \{1,\dots,K\}   and the kernel is chosen to be the Kronecker delta function, so     k   (  x  ,   x  â€²   )    =   Î´   (  x  ,   x  â€²   )          k   x   superscript  x  normal-â€²       Î´   x   superscript  x  normal-â€²       k(x,x^{\prime})=\delta(x,x^{\prime})   . The feature map corresponding to this kernel is the standard basis vector     Ï•   (  x  )    =   ğ  x         Ï•  x    subscript  ğ  x     \phi(x)=\mathbf{e}_{x}   . The kernel embeddings of such a distributions are thus vectors of marginal probabilities while the embeddings of joint distributions in this setting are    K  Ã—  K      K  K    K\times K   matrices specifying joint probability tables, and the explicit form of these embeddings is    \mu_X = \mathbb{E}_X [\mathbf{e}_x] = \left(     \begin{array}{c} P(X=1) \\ \vdots \\ P(X=K) \\ \end{array} \right)         ğ’   X  Y    =   ğ”¼   X  Y     [   ğ  X   âŠ—   e  Y   ]   =    (  P   (  X  =  s  ,  Y  =  t  )   )     s  ,  t   âˆˆ   {  1  ,  â€¦  ,  K  }        fragments   subscript  ğ’    X  Y      subscript  ğ”¼    X  Y     fragments  normal-[   subscript  ğ  X   tensor-product   subscript  e  Y   normal-]     subscript   fragments  normal-(  P   fragments  normal-(  X   s  normal-,  Y   t  normal-)   normal-)      s  t    1  normal-â€¦  K       \mathcal{C}_{XY}=\mathbb{E}_{XY}[\mathbf{e}_{X}\otimes e_{Y}]=\bigg(P(X=s,Y=t)%
 \bigg)_{s,t\in\{1,\dots,K\}}        The conditional distribution embedding operator     ğ’   Y  âˆ£  X    =    ğ’   Y  X     ğ’   X  X    -  1          subscript  ğ’   fragments  Y  normal-âˆ£  X       subscript  ğ’    Y  X     superscript   subscript  ğ’    X  X      1       \mathcal{C}_{Y\mid X}=\mathcal{C}_{YX}\mathcal{C}_{XX}^{-1}   is in this setting a conditional probability table         ğ’   Y  âˆ£  X    =    (  P   (  Y  =  s  âˆ£  X  =  t  )   )     s  ,  t   âˆˆ   {  1  ,  â€¦  ,  K  }        fragments   subscript  ğ’   fragments  Y  normal-âˆ£  X      subscript   fragments  normal-(  P   fragments  normal-(  Y   s  normal-âˆ£  X   t  normal-)   normal-)      s  t    1  normal-â€¦  K       \mathcal{C}_{Y\mid X}=\bigg(P(Y=s\mid X=t)\bigg)_{s,t\in\{1,\dots,K\}}       and \mathcal{C}_{XX} =\left(   \begin{array}{c c c} P(X=1) & \dots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \dots & P(X=K) \\ \end{array} \right)  Thus, the embeddings of the conditional distribution under a fixed value of   X   X   X   may be computed as    \mu_{Y \mid x} = \mathcal{C}_{Y \mid X} \phi(x) = \left(     \begin{array}{c} P(Y=1 \mid X = x) \\ \vdots \\ P(Y=K \mid X = x) \\ \end{array} \right)  In this discrete-valued setting with the Kronecker delta kernel, the kernel sum rule becomes    \underbrace{ \left(     \begin{array}{c} Q(X=1) \\ \vdots \\ P(X = N) \\ \end{array} \right) }_{\mu_Y^\pi} = \underbrace{ \left( \begin{array}{c} \\ P(X=s \mid Y=t) \\ \\ \end{array} \right) }_{ \mathcal{C}_{X\mid Y} } \underbrace{ \left( \begin{array}{c} \pi(Y=1) \\ \vdots \\ pi(Y = N) \\ \end{array} \right) }_{ \mu_Y^\pi}  The kernel chain rule in this case is given by    \underbrace{ \left( \begin{array}{c} \\ Q(X=s,Y=t) \\ \\ \end{array} \right) }_{\mathcal{C}_{XY}^\pi} =     \underbrace{Â \left(Â \begin{array}{c}Â \\Â P(X=sÂ \midÂ Y=t)Â \\Â \\Â \end{array}Â \right)Â }_{\mathcal{C}_{XÂ \midÂ Y}}  \underbrace{ \left( \begin{array}{c c c} \pi(Y=1) & \dots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \dots & \pi(Y=K) \\ \end{array} \right) }_{\mathcal{C}_{YY}^\pi}  References  "  Category:Machine learning  Category:Statistics     A. Smola, A. Gretton, L. Song, B. SchÃ¶lkopf. (2007). A Hilbert Space Embedding for Distributions . Algorithmic Learning Theory: 18th International Conference . Springer: 13â€“31. â†©  L. Song, K. Fukumizu, F. Dinuzzo, A. Gretton (2013). Kernel Embeddings of Conditional Distributions: A unified kernel framework for nonparametric inference in graphical models . IEEE Signal Processing Magazine  30 : 98â€“111. â†©  J. Shawe-Taylor, N. Christianini. (2004). Kernel Methods for Pattern Analysis . Cambridge University Press, Cambridge, UK. â†©  T. Hofmann, B. SchÃ¶lkopf, A. Smola. (2008). [ http://projecteuclid.org/DPubS?service=UI&version; ;=1.0&verb;=Display&handle;=euclid.aos/1211819561 Kernel Methods in Machine Learning]. The Annals of Statistics  36 (3):1171â€“1220. â†©  L. Song. (2008) Learning via Hilbert Space Embedding of Distributions . PhD Thesis, University of Sidney. â†©     K. Fukumizu, A. Gretton, X. Sun, and B. SchÃ¶lkopf (2008). Kernel measures of conditional independence . Advances in Neural Information Processing Systems  20 , MIT Press, Cambridge, MA. â†©   L. Song, J. Huang, A. J. Smola, K. Fukumizu. (2009). Hilbert space embeddings of conditional distributions . Proc. Int. Conf. Machine Learning . Montreal, Canada: 961-968. â†©          A. Gretton, K. Borgwardt, M. Rasch, B. SchÃ¶lkopf, A. Smola. (2007). A kernel method for the two-sample-problem . Advances in Neural Information Processing Systems  19 , MIT Press, Cambridge, MA. â†©  S. Grunewalder, G. Lever, L. Baldassarre, S. Patterson, A. Gretton, M. Pontil. (2012). Conditional mean embeddings as regressors . Proc. Int. Conf. Machine Learning : 1823â€“1830. â†©      A. Gretton, K. Borgwardt, M. Rasch, B. SchÃ¶lkopf, A. Smola. (2012). A kernel two-sample test . Journal of Machine Learning Research , 13 : 723-773. â†©   M. DudÃ­k, S. J. Phillips, R. E. Schapire. (2007). Maximum Entropy Distribution Estimation with Generalized Regularization and an Application to Species Distribution Modeling . Journal of Machine Learning Research , 8 : 1217-1260. â†©   A. Gretton, O. Bousquet, A. Smola, B. SchÃ¶lkopf. (2005). Measuring statistical dependence with Hilbertâ€“Schmidt norms . Proc. Intl. Conf. on Algorithmic Learning Theory : 63â€“78. â†©   L. Song, A. Smola , A. Gretton, K. Borgwardt, J. Bedo. (2007). Supervised feature selection via dependence estimation . Proc. Intl. Conf. Machine Learning , Omnipress: 823â€“830. â†©  L. Song, A. Smola, A. Gretton, K. Borgwardt. (2007). A dependence maximization view of clustering . Proc. Intl. Conf. Machine Learning . Omnipress: 815â€“822. â†©  L. Song, A. Smola, K. Borgwardt, A. Gretton. (2007). Colored maximum variance unfolding . Neural Information Processing Systems . â†©      K. Muandet, K. Fukumizu, F. Dinuzzo, B. SchÃ¶lkopf. (2012). Learning from Distributions via Support Measure Machines . Advances in Neural Information Processing Systems : 10â€“18. â†©   K. Zhang, B. SchÃ¶lkopf, K. Muandet, Z. Wang. (2013). Domain adaptation under target and conditional shift . ''Journal of Machine Learning Research, 28 (3): 819â€“827. â†©  A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, B. SchÃ¶lkopf. (2008). Covariate shift and local learning by distribution matching. In J. Quinonero-Candela, M. Sugiyama, A. Schwaighofer, N. Lawrence (eds.). Dataset shift in machine learning , MIT Press, Cambridge, MA: 131â€“160. â†©      K. Muandet, D. Balduzzi, B. SchÃ¶lkopf. (2013). Domain Generalization Via Invariant Feature Representation . 30th International Conference on Machine Learning . â†©        