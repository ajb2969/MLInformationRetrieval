   Poisson regression      Poisson regression   In statistics , Poisson regression is a form of regression analysis used to model count data and contingency tables . Poisson regression assumes the response variable Y has a Poisson distribution , and assumes the logarithm of its expected value can be modeled by a linear combination of unknown parameters . A Poisson regression model is sometimes known as a log-linear model , especially when used to model contingency tables.  Poisson regression models are generalized linear models with the logarithm as the (canonical) link function , and the Poisson distribution function as the assumed probability distribution of the response.  Regression models  If    ùê±  ‚àà   ‚Ñù  n       ùê±   superscript  ‚Ñù  n     \mathbf{x}\in\mathbb{R}^{n}   is a vector of independent variables , then the model takes the form        log   (   E   (  Y  ‚à£  ùê±  )    )    =   Œ±  +    Œ≤  ‚Ä≤   ùê±     ,         normal-E  Y  ùê±      Œ±     superscript  Œ≤  normal-‚Ä≤   ùê±      \log(\operatorname{E}(Y\mid\mathbf{x}))=\alpha+\mathbf{\beta}^{\prime}\mathbf{%
 x},     where    Œ±  ‚àà  ‚Ñù      Œ±  ‚Ñù    \alpha\in\mathbb{R}   and    Œ≤  ‚àà   ‚Ñù  n       Œ≤   superscript  ‚Ñù  n     \mathbf{\beta}\in\mathbb{R}^{n}   . Sometimes this is written more compactly as        log   (   E   (  Y  ‚à£  ùê±  )    )    =    ùúΩ  ‚Ä≤   ùê±    ,         normal-E  Y  ùê±       superscript  ùúΩ  normal-‚Ä≤   ùê±     \log(\operatorname{E}(Y\mid\mathbf{x}))=\boldsymbol{\theta}^{\prime}\mathbf{x},\,     where x is now an ( n +¬†1)-dimensional vector consisting of n independent variables concatenated to a vector of ones. Here Œ∏ is simply Œ± concatenated to Œ≤ .  Thus, when given a Poisson regression model Œ∏ and an input vector x , the predicted mean of the associated Poisson distribution is given by        E   (  Y  ‚à£  ùê±  )    =   e    ùúΩ  ‚Ä≤   ùê±     .       normal-E  Y  ùê±    superscript  e     superscript  ùúΩ  normal-‚Ä≤   ùê±      \operatorname{E}(Y\mid\mathbf{x})=e^{\boldsymbol{\theta}^{\prime}\mathbf{x}}.\,     If Y i are independent observations with corresponding values x i of the predictor variables, then Œ∏ can be estimated by maximum likelihood . The maximum-likelihood estimates lack a closed-form expression and must be found by numerical methods. The probability surface for maximum-likelihood Poisson regression is always concave, making Newton‚ÄìRaphson or other gradient-based methods appropriate estimation techniques.  Maximum likelihood-based parameter estimation  Given a set of parameters Œ∏ and an input vector x , the mean of the predicted Poisson distribution , as stated above, is given by       E   (  Y  ‚à£  x  )    =    e    Œ∏  ‚Ä≤   x          normal-E  Y  x    superscript  e     superscript  Œ∏  normal-‚Ä≤   x      \operatorname{E}(Y\mid x)=e^{\theta^{\prime}x}\,   ,  and thus, the Poisson distribution's probability mass function is given by      p   (  y  ‚à£  x  ;  Œ∏  )   =      [   E   (  Y  ‚à£  x  )    ]   y   √ó   e   -   E   (  Y  ‚à£  x  )        y  !    =     e   y   Œ∏  ‚Ä≤   x     e   -   e    Œ∏  ‚Ä≤   x        y  !       fragments  p   fragments  normal-(  y  normal-‚à£  x  normal-;  Œ∏  normal-)         superscript   delimited-[]   normal-E  Y  x    y    superscript  e     normal-E  Y  x        y          superscript  e    y   superscript  Œ∏  normal-‚Ä≤   x     superscript  e     superscript  e     superscript  Œ∏  normal-‚Ä≤   x         y      p(y\mid x;\theta)=\frac{[\operatorname{E}(Y\mid x)]^{y}\times e^{-%
 \operatorname{E}(Y\mid x)}}{y!}=\frac{e^{y\theta^{\prime}x}e^{-e^{\theta^{%
 \prime}x}}}{y!}     Now suppose we are given a data set consisting of m vectors      x  i   ‚àà   ‚Ñù   n  +  1     ,   i  =   1  ,  ‚Ä¶  ,  m       formulae-sequence     subscript  x  i    superscript  ‚Ñù    n  1       i   1  normal-‚Ä¶  m      x_{i}\in\mathbb{R}^{n+1},\,i=1,\ldots,m   , along with a set of m values      y  1   ,  ‚Ä¶  ,   y  m    ‚àà  ‚Ñù        subscript  y  1   normal-‚Ä¶   subscript  y  m    ‚Ñù    y_{1},\ldots,y_{m}\in\mathbb{R}   . Then, for a given set of parameters Œ∏ , the probability of attaining this particular set of data is given by      p   (   y  1   ,  ‚Ä¶  ,   y  m   ‚à£   x  1   ,  ‚Ä¶  ,   x  m   ;  Œ∏  )   =   ‚àè   i  =  1   m      e    y  i    Œ∏  ‚Ä≤    x  i      e   -   e    Œ∏  ‚Ä≤    x  i          y  i   !    .     fragments  p   fragments  normal-(   subscript  y  1   normal-,  normal-‚Ä¶  normal-,   subscript  y  m   normal-‚à£   subscript  x  1   normal-,  normal-‚Ä¶  normal-,   subscript  x  m   normal-;  Œ∏  normal-)     superscript   subscript  product    i  1    m        superscript  e     subscript  y  i    superscript  Œ∏  normal-‚Ä≤    subscript  x  i      superscript  e     superscript  e     superscript  Œ∏  normal-‚Ä≤    subscript  x  i           subscript  y  i     normal-.    p(y_{1},\ldots,y_{m}\mid x_{1},\ldots,x_{m};\theta)=\prod_{i=1}^{m}\frac{e^{y_%
 {i}\theta^{\prime}x_{i}}e^{-e^{\theta^{\prime}x_{i}}}}{y_{i}!}.     By the method of maximum likelihood , we wish to find the set of parameters Œ∏ that makes this probability as large as possible. To do this, the equation is first rewritten as a likelihood function in terms of Œ∏:      L   (  Œ∏  ‚à£  X  ,  Y  )   =   ‚àè   i  =  1   m      e    y  i    Œ∏  ‚Ä≤    x  i      e   -   e    Œ∏  ‚Ä≤    x  i          y  i   !       fragments  L   fragments  normal-(  Œ∏  normal-‚à£  X  normal-,  Y  normal-)     superscript   subscript  product    i  1    m        superscript  e     subscript  y  i    superscript  Œ∏  normal-‚Ä≤    subscript  x  i      superscript  e     superscript  e     superscript  Œ∏  normal-‚Ä≤    subscript  x  i           subscript  y  i       L(\theta\mid X,Y)=\prod_{i=1}^{m}\frac{e^{y_{i}\theta^{\prime}x_{i}}e^{-e^{%
 \theta^{\prime}x_{i}}}}{y_{i}!}   .  Note that the expression on the right hand side has not actually changed. A formula in this form is typically difficult to work with; instead, one uses the log-likelihood :      ‚Ñì   (  Œ∏  ‚à£  X  ,  Y  )   =  log  L   (  Œ∏  ‚à£  X  ,  Y  )   =   ‚àë   i  =  1   m    (   y  i    Œ∏  ‚Ä≤    x  i   -   e    Œ∏  ‚Ä≤    x  i     -  log   (   y  i   !  )   )      fragments  ‚Ñì   fragments  normal-(  Œ∏  normal-‚à£  X  normal-,  Y  normal-)     L   fragments  normal-(  Œ∏  normal-‚à£  X  normal-,  Y  normal-)     superscript   subscript     i  1    m    fragments  normal-(   subscript  y  i    superscript  Œ∏  normal-‚Ä≤    subscript  x  i     superscript  e     superscript  Œ∏  normal-‚Ä≤    subscript  x  i        fragments  normal-(   subscript  y  i    normal-)   normal-)     \ell(\theta\mid X,Y)=\log L(\theta\mid X,Y)=\sum_{i=1}^{m}\left(y_{i}\theta^{%
 \prime}x_{i}-e^{\theta^{\prime}x_{i}}-\log(y_{i}!)\right)   .  Notice that the parameters Œ∏ only appear in the first two terms of each term in the summation. Therefore, given that we are only interested in finding the best value for Œ∏ we may drop the y i ! and simply write      ‚Ñì   (  Œ∏  ‚à£  X  ,  Y  )   =   ‚àë   i  =  1   m    (   y  i    Œ∏  ‚Ä≤    x  i   -   e    Œ∏  ‚Ä≤    x  i     )      fragments  ‚Ñì   fragments  normal-(  Œ∏  normal-‚à£  X  normal-,  Y  normal-)     superscript   subscript     i  1    m    fragments  normal-(   subscript  y  i    superscript  Œ∏  normal-‚Ä≤    subscript  x  i     superscript  e     superscript  Œ∏  normal-‚Ä≤    subscript  x  i     normal-)     \ell(\theta\mid X,Y)=\sum_{i=1}^{m}\left(y_{i}\theta^{\prime}x_{i}-e^{\theta^{%
 \prime}x_{i}}\right)   .  To find a maximum, we need to solve an equation      ‚àÇ  ‚Ñì   (  Œ∏  ‚à£  X  ,  Y  )     ‚àÇ  Œ∏    =  0         fragments   ‚Ñì   fragments  normal-(  Œ∏  normal-‚à£  X  normal-,  Y  normal-)      Œ∏    0    \frac{\partial\ell(\theta\mid X,Y)}{\partial\theta}=0   which has no closed-form solution. However, the negative log-likelihood,    -  ‚Ñì   (  Œ∏  ‚à£  X  ,  Y  )      fragments   ‚Ñì   fragments  normal-(  Œ∏  normal-‚à£  X  normal-,  Y  normal-)     -\ell(\theta\mid X,Y)   , is a convex function, and so standard convex optimization techniques such as gradient descent can be applied to find the optimal value of Œ∏ .  Poisson regression in practice  Poisson regression may be appropriate when the dependent variable is a count, for instance of events such as the arrival of a telephone call at a call centre. 1 The events must be independent in the sense that the arrival of one call will not make another more or less likely, but the probability per unit time of events is understood to be related to covariates such as time of day.  "Exposure" and offset  Poisson regression may also be appropriate for rate data, where the rate is a count of events occurring to a particular unit of observation, divided by some measure of that unit's exposure . For example, biologists may count the number of tree species in a forest, and the rate would be the number of species per square kilometre. Demographers may model death rates in geographic areas as the count of deaths divided by person‚àíyears. More generally, event rates can be calculated as events per unit time, which allows the observation window to vary for each unit. In these examples, exposure is respectively unit area, person‚àíyears and unit time. In Poisson regression this is handled as an offset , where the exposure variable enters on the right-hand side of the equation, but with a parameter estimate (for log(exposure)) constrained to 1.       log   (   E   (  Y  ‚à£  x  )    )    =    log   (  exposure  )    +    Œ∏  ‚Ä≤   x           normal-E  Y  x        exposure      superscript  Œ∏  normal-‚Ä≤   x      \log{(\operatorname{E}(Y\mid x))}=\log{(\text{exposure})}+\theta^{\prime}x   which implies        log   (   E   (  Y  ‚à£  x  )    )    -   log   (  exposure  )     =   log   (    E   (  Y  ‚à£  x  )    exposure   )    =    Œ∏  ‚Ä≤   x              normal-E  Y  x      exposure         normal-E  Y  x   exposure            superscript  Œ∏  normal-‚Ä≤   x      \log{(\operatorname{E}(Y\mid x))}-\log{(\text{exposure})}=\log{\left(\frac{%
 \operatorname{E}(Y\mid x)}{\text{exposure}}\right)}=\theta^{\prime}x     Offset in the case of a GLM in R can be achieved using the offset() function:  glm(y ~ offset(log(exposure)) + x, family=poisson(link=log) )  Overdispersion and zero inflation  A characteristic of the Poisson distribution is that its mean is equal to its variance. In certain circumstances, it will be found that the observed variance is greater than the mean; this is known as overdispersion and indicates that the model is not appropriate. A common reason is the omission of relevant explanatory variables, or dependent observations. Under some circumstances, the problem of overdispersion can be solved by using quasi-likelihood estimation or a negative binomial distribution instead. 2 3  Another common problem with Poisson regression is excess zeros: if there are two processes at work, one determining whether there are zero events or any events, and a Poisson process determining how many events there are, there will be more zeros than a Poisson regression would predict. An example would be the distribution of cigarettes smoked in an hour by members of a group where some individuals are non-smokers.  Other generalized linear models such as the negative binomial model or zero-inflated model may function better in these cases.  Use in survival analysis  Poisson regression creates proportional hazards models, one class of survival analysis : see proportional hazards models for descriptions of Cox models.  Extensions  Regularized Poisson regression  When estimating the parameters for Poisson regression, one typically tries to find values for Œ∏ that maximize the likelihood of an expression of the form        ‚àë   i  =  1   m    log   (   p   (   y  i   ;   e    Œ∏  ‚Ä≤   x    )    )     ,      superscript   subscript     i  1    m       p    subscript  y  i    superscript  e     superscript  Œ∏  normal-‚Ä≤   x         \sum_{i=1}^{m}\log(p(y_{i};e^{\theta^{\prime}x})),     where m is the number of examples in the data set, and    p   (   y  i   ;   e    Œ∏  ‚Ä≤   x    )       p    subscript  y  i    superscript  e     superscript  Œ∏  normal-‚Ä≤   x       p(y_{i};e^{\theta^{\prime}x})   is the probability mass function of the Poisson distribution with the mean set to    e    Œ∏  ‚Ä≤   x      superscript  e     superscript  Œ∏  normal-‚Ä≤   x     e^{\theta^{\prime}x}   . Regularization can be added to this optimization problem by instead maximizing         ‚àë   i  =  1   m    log   (   p   (   y  i   ;   e    Œ∏  ‚Ä≤   x    )    )     -   Œª    ‚à•  Œ∏  ‚à•   2  2     ,        superscript   subscript     i  1    m       p    subscript  y  i    superscript  e     superscript  Œ∏  normal-‚Ä≤   x          Œª   superscript   subscript   norm  Œ∏   2   2      \sum_{i=1}^{m}\log(p(y_{i};e^{\theta^{\prime}x}))-\lambda\left\|\theta\right\|%
 _{2}^{2},     for some positive constant   Œª   Œª   \lambda   . This technique, similar to ridge regression , can reduce overfitting .  Implementations  Some statistics packages include implementations of Poisson regression.   GenStat : Poisson regression is a standard option of the regression section, using the "MODEL", "FIT" and associated commands; it is also available in the "Stats > Regression Analysis > Generalized Linear Models" menu.  MATLAB Statistics Toolbox: Poisson regression can be performed using the "glmfit" and "glmval" functions. 4  Microsoft Excel : Excel is not capable of doing Poisson regression by default. One of the Excel Add-ins for Poisson regression is XPost  mPlus: mPlus allows for Poisson regression using the command COUNT IS when specifying the data  R : The function for fitting a generalized linear model in R is glm(), and can be used for Poisson Regression  SAS : Poisson regression in SAS is done by using GENMOD, HPGENSELECT, COUNTREG, GLIMMIX, and NLMIXED  SPSS : In SPSS, Poisson regression is done by using the GENLIN command  Stata : Stata has a procedure for Poisson regression named poisson , 5 and for panel data xtpoisson . 6  CrimeStat : CrimeStat has Poisson, Poisson NB1, Poisson-Gamma(negative binomial), and Poisson-Lognormal regression models.   See also   Zero-inflated model  Poisson distribution   References  Further reading         "  Category:Regression analysis  Category:Generalized linear models  Category:Categorical data  Category:Econometrics     ‚Ü©  ‚Ü©  ‚Ü©  http://www.mathworks.com/help/toolbox/stats/glmfit.html ‚Ü©  Poisson regression ‚Ü©  Fixed-effects, random-effects, and population-averaged Poisson models ‚Ü©     