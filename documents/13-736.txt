   Proofs involving ordinary least squares      Proofs involving ordinary least squares   The purpose of this page is to provide supplementary materials for the Ordinary least squares article, reducing the load of the main article with mathematics and improving its accessibility, while at the same time retaining the completeness of exposition.  Least squares estimator for Œ≤  Using matrix notation, the sum of squared residuals is given by       S   (  b  )    =     (   y  -   X  b    )   ‚Ä≤    (   y  -   X  b    )          S  b      superscript    y    X  b    normal-‚Ä≤     y    X  b       S(b)=(y-Xb)^{\prime}(y-Xb)\,     Where     ‚Ä≤     normal-‚Ä≤    {}^{\prime}   denotes the matrix transpose.  Since this is a quadratic expression and S ( b )¬†‚â•¬†0, the global minimum will be found by differentiating it with respect to b :      0  =     d  S    d   b  ‚Ä≤      (   Œ≤  ^   )    =      d   d   b  ‚Ä≤      (      y  ‚Ä≤   y   -    b  ‚Ä≤    X  ‚Ä≤   y   -    y  ‚Ä≤   X  b    +    b  ‚Ä≤    X  ‚Ä≤   X  b    )    |    b  =   Œ≤  ^     =    -   2   X  ‚Ä≤   y    +   2   X  ‚Ä≤   X   Œ≤  ^           0        d  S     d   superscript  b  normal-‚Ä≤      normal-^  Œ≤          evaluated-at      d    d   superscript  b  normal-‚Ä≤            superscript  y  normal-‚Ä≤   y      superscript  b  normal-‚Ä≤    superscript  X  normal-‚Ä≤   y      superscript  y  normal-‚Ä≤   X  b       superscript  b  normal-‚Ä≤    superscript  X  normal-‚Ä≤   X  b       b   normal-^  Œ≤                2   superscript  X  normal-‚Ä≤   y      2   superscript  X  normal-‚Ä≤   X   normal-^  Œ≤        0=\frac{dS}{db^{\prime}}(\hat{\beta})=\frac{d}{db^{\prime}}\bigg(y^{\prime}y-b%
 ^{\prime}X^{\prime}y-y^{\prime}Xb+b^{\prime}X^{\prime}Xb\bigg)\bigg|_{b=\hat{%
 \beta}}=-2X^{\prime}y+2X^{\prime}X\hat{\beta}     By assumption matrix X has full column rank, and therefore X'X is invertible and the least squares estimator for Œ≤ is given by       Œ≤  ^   =     (    X  ‚Ä≤   X   )    -  1     X  ‚Ä≤    y         normal-^  Œ≤      superscript     superscript  X  normal-‚Ä≤   X     1     superscript  X  normal-‚Ä≤   y     \hat{\beta}=(X^{\prime}X)^{-1}X^{\prime}y\,     Unbiasedness and Variance of    Œ≤  ^     normal-^  Œ≤    \hat{\beta}     Plug y = XŒ≤ + Œµ into the formula for    Œ≤  ^     normal-^  Œ≤    \hat{\beta}   and then use the Law of iterated expectation :         E   [   Œ≤  ^   ]        =   E   [     (    X  ‚Ä≤   X   )    -  1     X  ‚Ä≤    (    X  Œ≤   +  Œµ   )    ]            =   Œ≤  +   E   [     (    X  ‚Ä≤   X   )    -  1     X  ‚Ä≤   Œµ   ]             =   Œ≤  +   E   [   E   [     (    X  ‚Ä≤   X   )    -  1     X  ‚Ä≤   Œµ   |  X  ]    ]             =   Œ≤  +   E   [     (    X  ‚Ä≤   X   )    -  1     X  ‚Ä≤    E   [  Œµ  |  X  ]     ]           =  Œ≤   ,          normal-E   normal-^  Œ≤      absent   normal-E     superscript     superscript  X  normal-‚Ä≤   X     1     superscript  X  normal-‚Ä≤       X  Œ≤   Œµ         missing-subexpression     absent    Œ≤   normal-E     superscript     superscript  X  normal-‚Ä≤   X     1     superscript  X  normal-‚Ä≤   Œµ         missing-subexpression     absent    Œ≤   normal-E   normal-E     superscript     superscript  X  normal-‚Ä≤   X     1     superscript  X  normal-‚Ä≤   Œµ   X         missing-subexpression     absent    Œ≤   normal-E     superscript     superscript  X  normal-‚Ä≤   X     1     superscript  X  normal-‚Ä≤    normal-E  Œµ  X         absent  Œ≤      \begin{aligned}\displaystyle\operatorname{E}[\,\hat{\beta}]&\displaystyle=%
 \operatorname{E}\Big[(X^{\prime}X)^{-1}X^{\prime}(X\beta+\varepsilon)\Big]\\
 &\displaystyle=\beta+\operatorname{E}\Big[(X^{\prime}X)^{-1}X^{\prime}%
 \varepsilon\Big]\\
 &\displaystyle=\beta+\operatorname{E}\Big[\operatorname{E}\Big[(X^{\prime}X)^{%
 -1}X^{\prime}\varepsilon|X\Big]\Big]\\
 &\displaystyle=\beta+\operatorname{E}\Big[(X^{\prime}X)^{-1}X^{\prime}%
 \operatorname{E}[\varepsilon|X]\Big]&\displaystyle=\beta,\\
 \end{aligned}     where E[ Œµ | X ]¬†=¬†0 by assumptions of the model.  For the variance, let     œÉ  2   I       superscript  œÉ  2   I    \sigma^{2}I   denote the covariance matrix of   Œµ   Œµ   \varepsilon   . Then,         E   [    (    Œ≤  ^   -  Œ≤   )     (    Œ≤  ^   -  Œ≤   )   T    ]        =   E   [    (     (    X  ‚Ä≤   X   )    -  1     X  ‚Ä≤   Œµ   )     (     (    X  ‚Ä≤   X   )    -  1     X  ‚Ä≤   Œµ   )   T    ]             =    œÉ  2     (    X  ‚Ä≤   X   )    -  1      ,               normal-E       normal-^  Œ≤   Œ≤    superscript     normal-^  Œ≤   Œ≤   T       absent   normal-E       superscript     superscript  X  normal-‚Ä≤   X     1     superscript  X  normal-‚Ä≤   Œµ    superscript     superscript     superscript  X  normal-‚Ä≤   X     1     superscript  X  normal-‚Ä≤   Œµ   T         missing-subexpression     absent     superscript  œÉ  2    superscript     superscript  X  normal-‚Ä≤   X     1        absent     \begin{aligned}\displaystyle\operatorname{E}[\,(\hat{\beta}-\beta)(\hat{\beta}%
 -\beta)^{T}]&\displaystyle=\operatorname{E}\Big[((X^{\prime}X)^{-1}X^{\prime}%
 \varepsilon)((X^{\prime}X)^{-1}X^{\prime}\varepsilon)^{T}\Big]\\
 &\displaystyle=\sigma^{2}(X^{\prime}X)^{-1},\\
 \displaystyle\par
 \par
 \end{aligned}     where we used the fact that     Œ≤  ^   -  Œ≤       normal-^  Œ≤   Œ≤    \hat{\beta}-\beta   is just an affine transformation of   Œµ   Œµ   \varepsilon   by the matrix      (    X  ‚Ä≤   X   )    -  1     X  ‚Ä≤        superscript     superscript  X  normal-‚Ä≤   X     1     superscript  X  normal-‚Ä≤     (X^{\prime}X)^{-1}X^{\prime}   ( see article on the multivariate normal distribution under the affine transformation section).  For a simple linear regression model, where    Œ≤  =    [   Œ≤  0   ,   Œ≤  1   ]   T       Œ≤   superscript    subscript  Œ≤  0    subscript  Œ≤  1    T     \beta=[\beta_{0},\beta_{1}]^{T}   (    Œ≤  0     subscript  Œ≤  0    \beta_{0}   is the y-intercept and    Œ≤  1     subscript  Œ≤  1    \beta_{1}   is the slope), one obtains          œÉ  2     (    X  ‚Ä≤   X   )    -  1         =    œÉ  2     (    ‚àë    x  i    x  i  ‚Ä≤      )    -  1             =    œÉ  2     (    ‚àë     (  1  ,   x  i   )   ‚Ä≤    (  1  ,   x  i   )      )    -  1             =    œÉ  2     (    ‚àë   (     1     x  i        x  i      x  i  2      )     )    -  1             =    œÉ  2     (     n     ‚àë   x  i         ‚àë   x  i       ‚àë   x  i  2       )    -  1             =     œÉ  2   ‚ãÖ    1    n   ‚àë   x  i  2     -    (   ‚àë   x  i    )   2        (      ‚àë   x  i  2       -   ‚àë   x  i          -   ‚àë   x  i       n     )            =     œÉ  2   ‚ãÖ    1   n    ‚àë   i  =  1   n     (    x  i   -   x  ¬Ø    )   2         (      ‚àë   x  i  2       -   ‚àë   x  i          -   ‚àë   x  i       n     )              superscript  œÉ  2    superscript     superscript  X  normal-‚Ä≤   X     1       absent     superscript  œÉ  2    superscript       subscript  x  i    superscript   subscript  x  i   normal-‚Ä≤       1         missing-subexpression     absent     superscript  œÉ  2    superscript       superscript   1   subscript  x  i    normal-‚Ä≤    1   subscript  x  i        1         missing-subexpression     absent     superscript  œÉ  2    superscript      1   subscript  x  i      subscript  x  i    superscript   subscript  x  i   2        1         missing-subexpression     absent     superscript  œÉ  2    superscript    n     subscript  x  i         subscript  x  i       superscript   subscript  x  i   2        1         missing-subexpression     absent     normal-‚ãÖ   superscript  œÉ  2     1      n     superscript   subscript  x  i   2      superscript     subscript  x  i    2           superscript   subscript  x  i   2         subscript  x  i            subscript  x  i     n         missing-subexpression     absent     normal-‚ãÖ   superscript  œÉ  2     1    n    superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¬Ø  x    2            superscript   subscript  x  i   2         subscript  x  i            subscript  x  i     n         \begin{aligned}\displaystyle\sigma^{2}(X^{\prime}X)^{-1}&\displaystyle=\sigma^%
 {2}\left(\sum x_{i}x_{i}^{\prime}\right)^{-1}\\
 &\displaystyle=\sigma^{2}\left(\sum(1,x_{i})^{\prime}(1,x_{i})\right)^{-1}\\
 &\displaystyle=\sigma^{2}\left(\sum\begin{pmatrix}1&x_{i}\\
 x_{i}&x_{i}^{2}\end{pmatrix}\right)^{-1}\\
 &\displaystyle=\sigma^{2}\begin{pmatrix}n&\sum x_{i}\\
 \sum x_{i}&\sum x_{i}^{2}\end{pmatrix}^{-1}\\
 &\displaystyle=\sigma^{2}\cdot\frac{1}{n\sum x_{i}^{2}-(\sum x_{i})^{2}}\begin%
 {pmatrix}\sum x_{i}^{2}&-\sum x_{i}\\
 -\sum x_{i}&n\end{pmatrix}\\
 &\displaystyle=\sigma^{2}\cdot\frac{1}{n\sum_{i=1}^{n}{(x_{i}-\bar{x})^{2}}}%
 \begin{pmatrix}\sum x_{i}^{2}&-\sum x_{i}\\
 -\sum x_{i}&n\end{pmatrix}\\
 \end{aligned}          V  a  r   (   Œ≤  1   )         =     œÉ  2     ‚àë   i  =  1   n     (    x  i   -   x  ¬Ø    )   2       .           V  a  r   subscript  Œ≤  1      absent     superscript  œÉ  2     superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¬Ø  x    2         \begin{aligned}\displaystyle Var(\beta_{1})&\displaystyle=\frac{\sigma^{2}}{%
 \sum_{i=1}^{n}{(x_{i}-\bar{x})^{2}}}.\end{aligned}     Expected value of     œÉ  ^   2     superscript   normal-^  œÉ   2    \hat{\sigma}^{2}     First we will plug in the expression for y into the estimator, and use the fact that X'M = MX =¬†0 (matrix M projects onto the space orthogonal to X ):        œÉ  ^   2   =     1  n     y  ‚Ä≤   M  y   =     1  n      (    X  Œ≤   +  Œµ   )   ‚Ä≤   M   (    X  Œ≤   +  Œµ   )    =     1  n     Œµ  ‚Ä≤   M  Œµ          superscript   normal-^  œÉ   2       1  n    superscript  y  normal-‚Ä≤   M  y            1  n    superscript      X  Œ≤   Œµ   normal-‚Ä≤   M      X  Œ≤   Œµ             1  n    superscript  Œµ  normal-‚Ä≤   M  Œµ      \hat{\sigma}^{2}=\tfrac{1}{n}y^{\prime}My=\tfrac{1}{n}(X\beta+\varepsilon)^{%
 \prime}M(X\beta+\varepsilon)=\tfrac{1}{n}\varepsilon^{\prime}M\varepsilon     Now we can recognize Œµ'MŒµ as a 1√ó1 matrix, such matrix is equal to its own trace . This is useful because by properties of trace operator, tr ( AB )= tr ( BA ), and we can use this to separate disturbance Œµ from matrix M which is a function of regressors X :       E    œÉ  ^   2    =     1  n     E   [   tr   (    Œµ  ‚Ä≤   M  Œµ   )    ]     =     1  n     tr   (   E   [   M  Œµ   Œµ  ‚Ä≤    ]    )            normal-E   superscript   normal-^  œÉ   2        1  n    normal-E   tr     superscript  Œµ  normal-‚Ä≤   M  Œµ               1  n    tr   normal-E    M  Œµ   superscript  Œµ  normal-‚Ä≤          \operatorname{E}\,\hat{\sigma}^{2}=\tfrac{1}{n}\operatorname{E}\big[%
 \operatorname{tr}(\varepsilon^{\prime}M\varepsilon)\big]=\tfrac{1}{n}%
 \operatorname{tr}\big(\operatorname{E}[M\varepsilon\varepsilon^{\prime}]\big)     Using the Law of iterated expectation this can be written as       E    œÉ  ^   2    =     1  n     tr   (   E   [    M    E   [   Œµ   Œµ  ‚Ä≤    |  X  ]     ]    )     =     1  n     tr   (   E   [    œÉ  2   M  I   ]    )     =     1  n     œÉ  2    E   [   tr  M   ]            normal-E   superscript   normal-^  œÉ   2        1  n    tr   normal-E    M   normal-E    Œµ   superscript  Œµ  normal-‚Ä≤    X                1  n    tr   normal-E     superscript  œÉ  2   M  I               1  n    superscript  œÉ  2    normal-E   tr  M        \operatorname{E}\,\hat{\sigma}^{2}=\tfrac{1}{n}\operatorname{tr}\Big(%
 \operatorname{E}\big[M\,\operatorname{E}[\varepsilon\varepsilon^{\prime}|X]%
 \big]\Big)=\tfrac{1}{n}\operatorname{tr}\big(\operatorname{E}[\sigma^{2}MI]%
 \big)=\tfrac{1}{n}\sigma^{2}\operatorname{E}\big[\operatorname{tr}\,M\big]     Recall that M = I ‚àí P where P is the projection onto linear space spanned by columns of matrix X . By properties of a projection matrix , it has p =¬†rank( X ) eigenvalues equal to 1, and all other eigenvalues are equal to 0. Trace of a matrix is equal to the sum of its characteristic values, thus tr( P )= p , and tr( M )¬†= n ‚àí p . Therefore       E    œÉ  ^   2    =     n  -  p   n    œÉ  2         normal-E   superscript   normal-^  œÉ   2          n  p   n    superscript  œÉ  2      \operatorname{E}\,\hat{\sigma}^{2}=\frac{n-p}{n}\sigma^{2}     Note: in the later section ‚ÄúMaximum likelihood‚Äù we show that under the additional assumption that errors are distributed normally, the estimator     œÉ  ^   2     superscript   normal-^  œÉ   2    \hat{\sigma}^{2}   is proportional to a chi-squared distribution with n ‚Äì p degrees of freedom, from which the formula for expected value would immediately follow. However the result we have shown in this section is valid regardless of the distribution of the errors, and thus has importance on its own.  Consistency and asymptotic normality of    Œ≤  ^     normal-^  Œ≤    \hat{\beta}     Estimator    Œ≤  ^     normal-^  Œ≤    \hat{\beta}   can be written as       Œ≤  ^   =     (     1  n     X  ‚Ä≤   X   )    -  1      1  n     X  ‚Ä≤   y   =   Œ≤  +     (     1  n     X  ‚Ä≤   X   )    -  1      1  n     X  ‚Ä≤   Œµ    =    Œ≤   +     (    1  n     ‚àë   i  =  1   n     x  i    x  i  ‚Ä≤      )    -  1     (    1  n     ‚àë   i  =  1   n     x  i    Œµ  i      )            normal-^  Œ≤      superscript      1  n    superscript  X  normal-‚Ä≤   X     1      1  n    superscript  X  normal-‚Ä≤   y          Œ≤     superscript      1  n    superscript  X  normal-‚Ä≤   X     1      1  n    superscript  X  normal-‚Ä≤   Œµ           Œ≤     superscript      1  n     superscript   subscript     i  1    n      subscript  x  i    subscript   superscript  x  normal-‚Ä≤   i        1        1  n     superscript   subscript     i  1    n      subscript  x  i    subscript  Œµ  i           \hat{\beta}=\big(\tfrac{1}{n}X^{\prime}X\big)^{-1}\tfrac{1}{n}X^{\prime}y=%
 \beta+\big(\tfrac{1}{n}X^{\prime}X\big)^{-1}\tfrac{1}{n}X^{\prime}\varepsilon=%
 \beta\;+\;\bigg(\frac{1}{n}\sum_{i=1}^{n}x_{i}x^{\prime}_{i}\bigg)^{\!\!-1}%
 \bigg(\frac{1}{n}\sum_{i=1}^{n}x_{i}\varepsilon_{i}\bigg)   We can use the law of large numbers to establish that         1  n     ‚àë   i  =  1   n     x  i     x  i  ‚Ä≤         ‚Üí  ùëù     E   [    x  i    x  i  ‚Ä≤    ]    =    Q   x  x    n    ,     1  n     ‚àë   i  =  1   n     x  i     Œµ  i         ‚Üí  ùëù     E   [    x  i    Œµ  i    ]    =  0      formulae-sequence      p  normal-‚Üí       1  n     superscript   subscript     i  1    n      subscript  x  i    subscript   superscript  x  normal-‚Ä≤   i       normal-E     subscript  x  i    superscript   subscript  x  i   normal-‚Ä≤             subscript  Q    x  x    n         p  normal-‚Üí       1  n     superscript   subscript     i  1    n      subscript  x  i    subscript  Œµ  i       normal-E     subscript  x  i    subscript  Œµ  i          0      \frac{1}{n}\sum_{i=1}^{n}x_{i}x^{\prime}_{i}\ \xrightarrow{p}\ \operatorname{E%
 }[x_{i}x_{i}^{\prime}]=\frac{Q_{xx}}{n},\qquad\frac{1}{n}\sum_{i=1}^{n}x_{i}%
 \varepsilon_{i}\ \xrightarrow{p}\ \operatorname{E}[x_{i}\varepsilon_{i}]=0   By Slutsky's theorem and continuous mapping theorem these results can be combined to establish consistency of estimator    Œ≤  ^     normal-^  Œ≤    \hat{\beta}   :        Œ≤  ^      ‚Üí  ùëù     Œ≤  +    Q   x  x    -  1    ‚ãÖ  0    =  Œ≤        p  normal-‚Üí    normal-^  Œ≤     Œ≤   normal-‚ãÖ   superscript   subscript  Q    x  x      1    0         Œ≤     \hat{\beta}\ \xrightarrow{p}\ \beta+Q_{xx}^{-1}\cdot 0=\beta     The central limit theorem tells us that         1   n      ‚àë   i  =  1   n     x  i     Œµ  i         ‚Üí  ùëë     ùí©   (  0  ,  V  )     ,      d  normal-‚Üí       1    n      superscript   subscript     i  1    n      subscript  x  i    subscript  Œµ  i        ùí©   0  V      \frac{1}{\sqrt{n}}\sum_{i=1}^{n}x_{i}\varepsilon_{i}\ \xrightarrow{d}\ %
 \mathcal{N}\big(0,\,V\big),   where    V  =   Var   [    x  i    Œµ  i    ]    =   E   [    Œµ  i  2    x  i     x  i  ‚Ä≤     ]    =   E   [    E   [   Œµ  i  2   |   x  i   ]     x  i     x  i  ‚Ä≤     ]    =    œÉ  2     Q   x  x    n          V   Var     subscript  x  i    subscript  Œµ  i           normal-E     superscript   subscript  Œµ  i   2    subscript  x  i    subscript   superscript  x  normal-‚Ä≤   i           normal-E     normal-E   superscript   subscript  Œµ  i   2    subscript  x  i     subscript  x  i    subscript   superscript  x  normal-‚Ä≤   i             superscript  œÉ  2      subscript  Q    x  x    n       V=\operatorname{Var}[x_{i}\varepsilon_{i}]=\operatorname{E}[\,\varepsilon_{i}^%
 {2}x_{i}x^{\prime}_{i}\,]=\operatorname{E}\big[\,\operatorname{E}[\varepsilon_%
 {i}^{2}|x_{i}]\;x_{i}x^{\prime}_{i}\,\big]=\sigma^{2}\frac{Q_{xx}}{n}     Applying Slutsky's theorem again we'll have        n    (    Œ≤  ^   -  Œ≤   )    =     (    1  n     ‚àë   i  =  1   n     x  i    x  i  ‚Ä≤      )    -  1     (    1   n      ‚àë   i  =  1   n     x  i    Œµ  i      )      ‚Üí  ùëë        Q   x  x    -  1    n   ‚ãÖ  ùí©    (  0  ,    œÉ  2     Q   x  x    n    )    =   ùí©   (  0  ,    œÉ  2    Q   x  x    -  1    n   )              n      normal-^  Œ≤   Œ≤       superscript      1  n     superscript   subscript     i  1    n      subscript  x  i    subscript   superscript  x  normal-‚Ä≤   i        1        1    n      superscript   subscript     i  1    n      subscript  x  i    subscript  Œµ  i          d  normal-‚Üí        normal-‚ãÖ     superscript   subscript  Q    x  x      1    n   ùí©    0     superscript  œÉ  2      subscript  Q    x  x    n             ùí©   0     superscript  œÉ  2    superscript   subscript  Q    x  x      1    n        \sqrt{n}(\hat{\beta}-\beta)=\bigg(\frac{1}{n}\sum_{i=1}^{n}x_{i}x^{\prime}_{i}%
 \bigg)^{\!\!-1}\bigg(\frac{1}{\sqrt{n}}\sum_{i=1}^{n}x_{i}\varepsilon_{i}\bigg%
 )\ \xrightarrow{d}\ Q_{xx}^{-1}n\cdot\mathcal{N}\big(0,\sigma^{2}\frac{Q_{xx}}%
 {n}\big)=\mathcal{N}\big(0,\sigma^{2}Q_{xx}^{-1}n\big)     Maximum likelihood approach  Maximum likelihood estimation is a generic technique for estimating the unknown parameters in a statistical model by constructing a log-likelihood function corresponding to the joint distribution of the data, then maximizing this function over all possible parameter values. In order to apply this method, we have to make an assumption about the distribution of y given X so that the log-likelihood function can be constructed. The connection of maximum likelihood estimation to OLS arises when this distribution is modeled as a multivariate normal .  Specifically, assume that the errors Œµ have multivariate normal distribution with mean 0 and variance matrix œÉ 2 I . Then the distribution of y conditionally on X is      y  |   X   ‚àº  ùí©   (  X  Œ≤  ,   œÉ  2   I  )      fragments  y  normal-|  X  similar-to  N   fragments  normal-(  X  Œ≤  normal-,   superscript  œÉ  2   I  normal-)     y|X\ \sim\ \mathcal{N}(X\beta,\,\sigma^{2}I)   and the log-likelihood function of the data will be      ‚Ñí   (  Œ≤  ,   œÉ  2   |  X  )      fragments  L   fragments  normal-(  Œ≤  normal-,   superscript  œÉ  2   normal-|  X  normal-)     \displaystyle\mathcal{L}(\beta,\sigma^{2}|X)   Differentiating this expression with respect to Œ≤ and œÉ 2 we'll find the ML estimates of these parameters:          ‚àÇ  ‚Ñí    ‚àÇ   Œ≤  ‚Ä≤      =   -     1   2   œÉ  2       (    -   2   X  ‚Ä≤   y    +   2   X  ‚Ä≤   X  Œ≤    )     =  0     ‚áí   Œ≤  ^    =     (    X  ‚Ä≤   X   )    -  1     X  ‚Ä≤   y       formulae-sequence          ‚Ñí      superscript  Œ≤  normal-‚Ä≤           1    2   superscript  œÉ  2           2   superscript  X  normal-‚Ä≤   y      2   superscript  X  normal-‚Ä≤   X  Œ≤           0       normal-‚áí   normal-^  Œ≤       superscript     superscript  X  normal-‚Ä≤   X     1     superscript  X  normal-‚Ä≤   y      \displaystyle\frac{\partial\mathcal{L}}{\partial\beta^{\prime}}=-\frac{1}{2%
 \sigma^{2}}\Big(-2X^{\prime}y+2X^{\prime}X\beta\Big)=0\quad\Rightarrow\quad%
 \hat{\beta}=(X^{\prime}X)^{-1}X^{\prime}y   We can check that this is indeed a maximum by looking at the Hessian matrix of the log-likelihood function.  Finite sample distribution  Since we have assumed in this section that the distribution of error terms is known to be normal, it becomes possible to derive the explicit expressions for the distributions of estimators    Œ≤  ^     normal-^  Œ≤    \hat{\beta}   and     œÉ  ^   2     superscript   normal-^  œÉ   2    \hat{\sigma}^{2}   :       Œ≤  ^   =     (    X  ‚Ä≤   X   )    -  1     X  ‚Ä≤   y   =     (    X  ‚Ä≤   X   )    -  1     X  ‚Ä≤    (    X  Œ≤   +  Œµ   )    =   Œ≤  +     (    X  ‚Ä≤   X   )    -  1     X  ‚Ä≤   ùí©   (  0  ,    œÉ  2   I   )            normal-^  Œ≤      superscript     superscript  X  normal-‚Ä≤   X     1     superscript  X  normal-‚Ä≤   y           superscript     superscript  X  normal-‚Ä≤   X     1     superscript  X  normal-‚Ä≤       X  Œ≤   Œµ           Œ≤     superscript     superscript  X  normal-‚Ä≤   X     1     superscript  X  normal-‚Ä≤   ùí©   0     superscript  œÉ  2   I         \hat{\beta}=(X^{\prime}X)^{-1}X^{\prime}y=(X^{\prime}X)^{-1}X^{\prime}(X\beta+%
 \varepsilon)=\beta+(X^{\prime}X)^{-1}X^{\prime}\mathcal{N}(0,\sigma^{2}I)   so that by the affine transformation properties of multivariate normal distribution       Œ≤  ^   |   X   ‚àº  ùí©   (  Œ≤  ,   œÉ  2     (   X  ‚Ä≤   X  )    -  1    )   .     fragments   normal-^  Œ≤   normal-|  X  similar-to  N   fragments  normal-(  Œ≤  normal-,   superscript  œÉ  2    superscript   fragments  normal-(   superscript  X  normal-‚Ä≤   X  normal-)     1    normal-)   normal-.    \hat{\beta}|X\ \sim\ \mathcal{N}(\beta,\,\sigma^{2}(X^{\prime}X)^{-1}).     Similarly the distribution of     œÉ  ^   2     superscript   normal-^  œÉ   2    \hat{\sigma}^{2}   follows from       œÉ  ^   2     superscript   normal-^  œÉ   2    \displaystyle\hat{\sigma}^{2}   where    M  =   I  -   X    (    X  ‚Ä≤   X   )    -  1     X  ‚Ä≤         M    I    X   superscript     superscript  X  normal-‚Ä≤   X     1     superscript  X  normal-‚Ä≤       M=I-X(X^{\prime}X)^{-1}X^{\prime}   is the symmetric projection matrix onto subspace orthogonal to X , and thus ''MX = X'M = ''0. We have argued before that this matrix has rank of n‚Äìp , and thus by properties of chi-squared distribution ,        n   œÉ  2       œÉ  ^   2   |  X  =    (  Œµ  /  œÉ  )   ‚Ä≤   M   (  Œµ  /  œÉ  )   ‚àº   œá   n  -  p   2      fragments    n   superscript  œÉ  2     superscript   normal-^  œÉ   2   normal-|  X    superscript   fragments  normal-(  Œµ   œÉ  normal-)   normal-‚Ä≤   M   fragments  normal-(  Œµ   œÉ  normal-)   similar-to   subscript   superscript  œá  2     n  p      \tfrac{n}{\sigma^{2}}\hat{\sigma}^{2}|X=(\varepsilon/\sigma)^{\prime}M(%
 \varepsilon/\sigma)\ \sim\ \chi^{2}_{n-p}     Moreover, the estimators    Œ≤  ^     normal-^  Œ≤    \hat{\beta}   and     œÉ  ^   2     superscript   normal-^  œÉ   2    \hat{\sigma}^{2}   turn out to be independent (conditional on X ), a fact which is fundamental for construction of the classical t- and F-tests. The independence can be easily seen from following: the estimator    Œ≤  ^     normal-^  Œ≤    \hat{\beta}   represents coefficients of vector decomposition of     y  ^   =   X   Œ≤  ^    =   P  y   =    X  Œ≤   +   P  Œµ           normal-^  y     X   normal-^  Œ≤           P  y            X  Œ≤     P  Œµ       \hat{y}=X\hat{\beta}=Py=X\beta+P\varepsilon   by the basis of columns of X , as such    Œ≤  ^     normal-^  Œ≤    \hat{\beta}   is a function of PŒµ . At the same time, the estimator     œÉ  ^   2     superscript   normal-^  œÉ   2    \hat{\sigma}^{2}   is a norm of vector MŒµ divided by n , and thus this estimator is a function of MŒµ . Now, random variables (PŒµ, MŒµ) are jointly normal as a linear transformation of Œµ , and they are also uncorrelated because ''PM = ''0. By properties of multivariate normal distribution, this means that PŒµ and MŒµ are independent, and therefore estimators    Œ≤  ^     normal-^  Œ≤    \hat{\beta}   and     œÉ  ^   2     superscript   normal-^  œÉ   2    \hat{\sigma}^{2}   will be independent as well.  "  Category:Article proofs  Category:Regression analysis  Category:Least squares   