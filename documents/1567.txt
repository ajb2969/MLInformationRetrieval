   Sequential minimal optimization      Sequential minimal optimization   Sequential minimal optimization ( SMO ) is an algorithm for solving the quadratic programming (QP) problem that arises during the training of support vector machines . It was invented by John Platt in 1998 at Microsoft Research . 1 SMO is widely used for training support vector machines and is implemented by the popular LIBSVM tool. 2 3 The publication of the SMO algorithm in 1998 has generated a lot of excitement in the SVM community, as previously available methods for SVM training were much more complex and required expensive third-party QP solvers. 4  Optimization problem  Consider a binary classification problem with a dataset ( x 1 , y 1 ), ..., ( x n , y n ), where x i is an input vector and  is a binary label corresponding to it. A soft-margin support vector machine is trained by solving a quadratic programming problem, which is expressed in the dual form as follows:         max  α     ∑   i  =  1   n    α  i     -    1  2     ∑   i  =  1   n     ∑   j  =  1   n     y  i    y  j   K   (   x  i   ,   x  j   )    α  i    α  j        ,         subscript   α     superscript   subscript     i  1    n    subscript  α  i         1  2     superscript   subscript     i  1    n     superscript   subscript     j  1    n      subscript  y  i    subscript  y  j   K    subscript  x  i    subscript  x  j     subscript  α  i    subscript  α  j         \max_{\alpha}\sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}y%
 _{i}y_{j}K(x_{i},x_{j})\alpha_{i}\alpha_{j},      subject to:        0  ≤   α  i   ≤  C   ,    for  i   =   1  ,  2  ,  …  ,  n     ,     formulae-sequence      0   subscript  α  i        C        for  i    1  2  normal-…  n      0\leq\alpha_{i}\leq C,\quad\mbox{ for }i=1,2,\ldots,n,           ∑   i  =  1   n     y  i    α  i     =  0        superscript   subscript     i  1    n      subscript  y  i    subscript  α  i     0    \sum_{i=1}^{n}y_{i}\alpha_{i}=0      where C is an SVM hyperparameter and K ( x i , x j ) is the kernel function , both supplied by the user; and the variables    α  i     subscript  α  i    \alpha_{i}   are Lagrange multipliers .  Algorithm  SMO is an iterative algorithm for solving the optimization problem described above. SMO breaks this problem into a series of smallest possible sub-problems, which are then solved analytically. Because of the linear equality constraint involving the Lagrange multipliers    α  i     subscript  α  i    \alpha_{i}   , the smallest possible problem involves two such multipliers. Then, for any two multipliers    α  1     subscript  α  1    \alpha_{1}   and    α  2     subscript  α  2    \alpha_{2}   , the constraints are reduced to:        0  ≤   α  1    ,    α  2   ≤  C    ,     formulae-sequence    0   subscript  α  1       subscript  α  2   C     0\leq\alpha_{1},\alpha_{2}\leq C,             y  1    α  1    +    y  2    α  2     =  k   ,           subscript  y  1    subscript  α  1       subscript  y  2    subscript  α  2     k    y_{1}\alpha_{1}+y_{2}\alpha_{2}=k,     and this reduced problem can be solved analytically: one needs to find a minimum of a one-dimensional quadratic function.   k   k   k   is the negative of the sum over the rest of terms in the equality constraint, which is fixed in each iteration.  The algorithm proceeds as follows:   Find a Lagrange multiplier    α  1     subscript  α  1    \alpha_{1}   that violates the Karush–Kuhn–Tucker (KKT) conditions for the optimization problem.  Pick a second multiplier    α  2     subscript  α  2    \alpha_{2}   and optimize the pair    (   α  1   ,   α  2   )      subscript  α  1    subscript  α  2     (\alpha_{1},\alpha_{2})   .  Repeat steps 1 and 2 until convergence.   When all the Lagrange multipliers satisfy the KKT conditions (within a user-defined tolerance), the problem has been solved. Although this algorithm is guaranteed to converge, heuristics are used to choose the pair of multipliers so as to accelerate the rate of convergence.  See also   Kernel perceptron   References  "  Category:Optimization algorithms and methods  Category:Support vector machines     ↩  ↩  Luca Zanni (2006). Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems . ↩  ↩     