   Competitive learning      Competitive learning   Competitive learning is a form of unsupervised learning in artificial neural networks , in which nodes compete for the right to respond to a subset of the input data. 1 A variant of Hebbian learning , competitive learning works by increasing the specialization of each node in the network. It is well suited to finding clusters within data.  Models and algorithms based on the principle of competitive learning include vector quantization and self-organising maps (Kohonen maps).  Architecture and implementation  (Figure)  Competitive neural network architecture   Competitive Learning is usually implemented with Neural Networks that contain a hidden layer which is commonly known as “competitive layer”. 2 Every competitive neuron i is described by a vector of weights     𝐰  i   =    (   w   i  1    ,  .  .  ,   w   i  d    )   T   ,  i  =  1  ,  .  .  ,  M     fragments   subscript  𝐰  i     superscript   fragments  normal-(   subscript  w    i  1    normal-,  normal-.  normal-.  normal-,   subscript  w    i  d    normal-)   T   normal-,  i   1  normal-,  normal-.  normal-.  normal-,  M    {\mathbf{w}}_{i}=\left({w_{i1},..,w_{id}}\right)^{T},i=1,..,M   and calculates the similarity measure between the input data     𝐱  n   =    (   x   n  1    ,  .  .  ,   x   n  d    )   T   ∈   ℝ  d      fragments   superscript  𝐱  n     superscript   fragments  normal-(   subscript  x    n  1    normal-,  normal-.  normal-.  normal-,   subscript  x    n  d    normal-)   T     superscript  ℝ  d     {\mathbf{x}}^{n}=\left({x_{n1},..,x_{nd}}\right)^{T}\in\mathbb{R}^{d}   and the weight vector    𝐰  i     subscript  𝐰  i    {\mathbf{w}}_{i}   .  For every input vector, the competitive neurons “compete” with each other to see which one of them is the most similar to that particular input vector. The winner neuron m sets its output     o  i   =  1       subscript  o  i   1    o_{i}=1   and all the other competitive neurons set their output     o  i   =  0  ,  i  =  1  ,  .  .  ,  M  ,  i  ≠  m     fragments   subscript  o  i    0  normal-,  i   1  normal-,  normal-.  normal-.  normal-,  M  normal-,  i   m    o_{i}=0,i=1,..,M,i\neq m   .  Usually, in order to measure similarity the inverse of the Euclidean distance is used    ∥   𝐱  -   𝐰  i    ∥     norm    𝐱   subscript  𝐰  i      \left\|{{\mathbf{x}}-{\mathbf{w}}_{i}}\right\|   between the input vector    𝐱  n     superscript  𝐱  n    {\mathbf{x}}^{n}   and the weight vector    𝐰  i     subscript  𝐰  i    {\mathbf{w}}_{i}   .  Example algorithm  Here is a simple competitive learning algorithm to find three clusters within some input data.  1. (Set-up.) Let a set of sensors all feed into three different nodes, so that every node is connected to every sensor. Let the weights that each node gives to its sensors be set randomly between 0.0 and 1.0. Let the output of each node be the sum of all its sensors, each sensor's signal strength being multiplied by its weight.  2. When the net is shown an input, the node with the highest output is deemed the winner. The input is classified as being within the cluster corresponding to that node.  3. The winner updates each of its weights, moving weight from the connections that gave it weaker signals to the connections that gave it stronger signals.  Thus, as more data are received, each node converges on the centre of the cluster that it has come to represent and activates more strongly for inputs in this cluster and more weakly for inputs in other clusters.  See also   Ensemble learning  Pandemonium architecture   References  Further Information and Software   Draft Report "Some Competitive Learning Methods" (contains descriptions of several related algos)  DemoGNG - Java simulator for competitive learning methods   "  Category:Artificial neural networks     ↩  ↩     