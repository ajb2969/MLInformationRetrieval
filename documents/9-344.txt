   Nonlinear conjugate gradient method      Nonlinear conjugate gradient method   In numerical optimization , the nonlinear conjugate gradient method generalizes the conjugate gradient method to nonlinear optimization . For a quadratic function    f   (  x  )       f  x    \displaystyle f(x)   :         f   (  x  )    =    ∥    A  x   -  b   ∥   2         f  x    superscript   norm      A  x   b    2     \displaystyle f(x)=\|Ax-b\|^{2}        The minimum of   f   f   f   is obtained when the gradient is 0:          ∇  x   f   =   2   A  ⊤    (    A  x   -  b   )    =  0          subscript  normal-∇  x   f     2   superscript  A  top       A  x   b         0     \nabla_{x}f=2A^{\top}(Ax-b)=0   .     Whereas linear conjugate gradient seeks a solution to the linear equation      A  ⊤   A  x   =    A  ⊤   b          superscript  A  top   A  x      superscript  A  top   b     \displaystyle A^{\top}Ax=A^{\top}b   , the nonlinear conjugate gradient method is generally used to find the local minimum of a nonlinear function using its gradient      ∇  x   f      subscript  normal-∇  x   f    \nabla_{x}f   alone. It works when the function is approximately quadratic near the minimum, which is the case when the function is twice differentiable at the minimum.  Given a function    f   (  x  )       f  x    \displaystyle f(x)   of   N   N   N   variables to minimize, its gradient     ∇  x   f      subscript  normal-∇  x   f    \nabla_{x}f   indicates the direction of maximum increase. One simply starts in the opposite ( steepest descent ) direction:         Δ   x  0    =   -     ∇  x   f    (   x  0   )           normal-Δ   subscript  x  0          subscript  normal-∇  x   f    subscript  x  0       \Delta x_{0}=-\nabla_{x}f(x_{0})        with an adjustable step length   α   α   \displaystyle\alpha   and performs a line search in this direction until it reaches the minimum of   f   f   \displaystyle f   :         α  0   :=    arg    min  α   f     (    x  0   +   α  Δ   x  0     )       assign   subscript  α  0         subscript   α   f       subscript  x  0     α  normal-Δ   subscript  x  0        \displaystyle\alpha_{0}:=\arg\min_{\alpha}f(x_{0}+\alpha\Delta x_{0})   ,       x  1   =    x  0   +    α  0   Δ   x  0          subscript  x  1      subscript  x  0      subscript  α  0   normal-Δ   subscript  x  0       \displaystyle x_{1}=x_{0}+\alpha_{0}\Delta x_{0}        After this first iteration in the steepest direction    Δ   x  0       normal-Δ   subscript  x  0     \displaystyle\Delta x_{0}   , the following steps constitute one iteration of moving along a subsequent conjugate direction    s  n     subscript  s  n    \displaystyle s_{n}   , where     s  0   =   Δ   x  0         subscript  s  0     normal-Δ   subscript  x  0      \displaystyle s_{0}=\Delta x_{0}   :   Calculate the steepest direction     Δ   x  n    =   -     ∇  x   f    (   x  n   )           normal-Δ   subscript  x  n          subscript  normal-∇  x   f    subscript  x  n       \Delta x_{n}=-\nabla_{x}f(x_{n})   ,  Compute    β  n     subscript  β  n    \displaystyle\beta_{n}   according to one of the formulas below,  Update the conjugate direction     s  n   =    Δ   x  n    +    β  n    s   n  -  1           subscript  s  n       normal-Δ   subscript  x  n       subscript  β  n    subscript  s    n  1        \displaystyle s_{n}=\Delta x_{n}+\beta_{n}s_{n-1}     Perform a line search: optimize     α  n   =    arg    min  α   f     (    x  n   +   α   s  n     )         subscript  α  n         subscript   α   f       subscript  x  n     α   subscript  s  n        \displaystyle\alpha_{n}=\arg\min_{\alpha}f(x_{n}+\alpha s_{n})   ,  Update the position     x   n  +  1    =    x  n   +    α  n    s  n          subscript  x    n  1       subscript  x  n      subscript  α  n    subscript  s  n       \displaystyle x_{n+1}=x_{n}+\alpha_{n}s_{n}   ,   With a pure quadratic function the minimum is reached within N iterations (excepting roundoff error), but a non-quadratic function will make slower progress. Subsequent search directions lose conjugacy requiring the search direction to be reset to the steepest descent direction at least every N iterations, or sooner if progress stops. However, resetting every iteration turns the method into steepest descent . The algorithm stops when it finds the minimum, determined when no progress is made after a direction reset (i.e. in the steepest descent direction), or when some tolerance criterion is reached.  Within a linear approximation, the parameters   α   α   \displaystyle\alpha   and   β   β   \displaystyle\beta   are the same as in the linear conjugate gradient method but have been obtained with line searches. The conjugate gradient method can follow narrow ( ill-conditioned ) valleys where the steepest descent method slows down and follows a criss-cross pattern.  Four of the best known formulas for    β  n     subscript  β  n    \displaystyle\beta_{n}   are named after their developers and are given by the following formulas:   Fletcher–Reeves:     \beta_{n}^{FR} = \frac{\Delta x_n^\top \Delta x_n}     {\Delta x_{n-1}^\top \Delta x_{n-1}}   Polak–Ribière:     \beta_{n}^{PR} = \frac{\Delta x_n^\top (\Delta x_n-\Delta x_{n-1})}     {\Delta x_{n-1}^\top \Delta x_{n-1}}   Hestenes-Stiefel:     \beta_n^{HS} = -\frac{\Delta x_n^\top (\Delta x_n-\Delta x_{n-1})}     {s_{n-1}^\top (\Delta x_n-\Delta x_{n-1})}   Dai–Yuan:     \beta_{n}^{DY} = -\frac{\Delta x_n^\top \Delta x_n}     {s_{n-1}^\top (\Delta x_n-\Delta x_{n-1})} .  These formulas are equivalent for a quadratic function, but for nonlinear optimization the preferred formula is a matter of heuristics or taste. A popular choice is    β  =   max   {  0  ,   β   P  R    }        β    0   superscript  β    P  R       \displaystyle\beta=\max\{0,\,\beta^{PR}\}   which provides a direction reset automatically.  Newton based methods - Newton-Raphson Algorithm , Quasi-Newton methods (e.g., BFGS method ) - tend to converge in fewer iterations, although each iteration typically requires more computation than a conjugate gradient iteration as Newton-like methods require computing the Hessian (matrix of second derivatives) in addition to the gradient. Quasi-Newton methods also require more memory to operate (see also the limited memory L-BFGS method).  See also   Nelder–Mead method  conjugate gradient method   External links   An Introduction to the Conjugate Gradient Method Without the Agonizing Pain by Jonathan Richard Shewchuk.  [ http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.3325&rep; ;=rep1&type;=pdf A NONLINEAR CONJUGATE GRADIENT METHOD WITH A STRONG GLOBAL CONVERGENCE PROPERTY] by Y. H. DAI and Y. YUAN.   ru:Метод сопряжённых градиентов  "  Category:Optimization algorithms and methods  Category:Gradient methods   