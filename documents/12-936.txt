   Summed area table      Summed area table   A summed area table is a data structure and algorithm for quickly and efficiently generating the sum of values in a rectangular subset of a grid. In the image processing domain, it is also known as an integral image . It was first introduced to computer graphics in 1984 by Frank Crow for use with mipmaps . In computer vision it was first prominently used within the Viola–Jones object detection framework in 2001. However, historically, this principle is very well known in the study of multi-dimensional probability distribution functions, namely in computing 2D (or ND) probabilities (area under the probability distribution) from the respective cumulative distribution functions . 1  The algorithm  As the name suggests, the value at any point ( x , y ) in the summed area table is just the sum of all the pixels above and to the left of ( x , y ), inclusive: 2 3       I   (  x  ,  y  )    =    ∑       x  ′   ≤  x         y  ′   ≤  y        i   (   x  ′   ,   y  ′   )           I   x  y      subscript        superscript  x  normal-′   x        superscript  y  normal-′   y        i    superscript  x  normal-′    superscript  y  normal-′        I(x,y)=\sum_{\begin{smallmatrix}x^{\prime}\leq x\\
 y^{\prime}\leq y\end{smallmatrix}}i(x^{\prime},y^{\prime})     Moreover, the summed area table can be computed efficiently in a single pass over the image, using the fact that the value in the summed area table at ( x , y ) is just:       I   (  x  ,  y  )    =     i   (  x  ,  y  )    +   I   (   x  -  1   ,  y  )    +   I   (  x  ,   y  -  1   )     -   I   (   x  -  1   ,   y  -  1   )           I   x  y          i   x  y      I     x  1   y      I   x    y  1        I     x  1     y  1        I(x,y)=i(x,y)+I(x-1,y)+I(x,y-1)-I(x-1,y-1)\,     Once the summed area table has been computed, the task of evaluating any rectangle can be accomplished in constant time with just four array references. Specifically, using the notation in the figure at right, having A=(x0, y0), B=(x1, y0), C=(x0, y1) and D=(x1, y1), the sum of    i   (  x  ,  y  )       i   x  y     i(x,y)   over the rectangle spanned by A, B,C and D is just         ∑       x  0   <  x  ≤   x  1          y  0   <  y  ≤   y  1         i   (  x  ,  y  )     =     I   (  D  )    +   I   (  A  )     -   I   (  B  )    -   I   (  C  )      .        subscript           x  0   x         x  1             y  0   y         y  1          i   x  y           I  D     I  A      I  B     I  C      \sum_{\begin{smallmatrix}x0     Extensions   This method is naturally extended to continuous domains. 4    The method can be also extended to high-dimensional images. 5 If the corners of the rectangle are    x  p     superscript  x  p    x^{p}   with   p   p   p   in     {  0  ,  1  }   d     superscript   0  1   d    \{0,1\}^{d}   , then the sum of image values contained in the rectangle are computed with the formula        ∑   p  ∈    {  0  ,  1  }   d        (   -  1   )    d  -    ∥  p  ∥   1     I   (   x  p   )        subscript     p   superscript   0  1   d        superscript    1     d   subscript   norm  p   1     I   superscript  x  p      \sum_{p\in\{0,1\}^{d}}(-1)^{d-\|p\|_{1}}I(x^{p})\,     where    I   (  x  )       I  x    I(x)   is the integral image at   x   x   x   and   d   d   d   the image dimension. The notation    x  p     superscript  x  p    x^{p}   correspond in the example to    d  =  2      d  2    d=2   ,    A  =   x   (  0  ,  0  )        A   superscript  x   0  0      A=x^{(0,0)}   ,    B  =   x   (  1  ,  0  )        B   superscript  x   1  0      B=x^{(1,0)}   ,    C  =   x   (  1  ,  1  )        C   superscript  x   1  1      C=x^{(1,1)}   and    D  =   x   (  0  ,  1  )        D   superscript  x   0  1      D=x^{(0,1)}   . In neuroimaging , for example, the images have dimension    d  =  3      d  3    d=3   or    d  =  4      d  4    d=4   , when using voxels or voxels with a time-stamp.   This method has been extended to high-order integral image as in the work of Phan et al. 6 who provided two, three, or four integral images for quickly and efficiently calculating the standard deviation (variance), skewness, and kurtosis of local block in the image. This is detailed below:   To compute variance or standard deviation of a block, we need two integral images:       I   (  x  ,  y  )    =    ∑       x  ′   ≤  x         y  ′   ≤  y        i   (   x  ′   ,   y  ′   )           I   x  y      subscript        superscript  x  normal-′   x        superscript  y  normal-′   y        i    superscript  x  normal-′    superscript  y  normal-′        I(x,y)=\sum_{\begin{smallmatrix}x^{\prime}\leq x\\
 y^{\prime}\leq y\end{smallmatrix}}i(x^{\prime},y^{\prime})           I  2    (  x  ,  y  )    =    ∑       x  ′   ≤  x         y  ′   ≤  y         i  2    (   x  ′   ,   y  ′   )            superscript  I  2    x  y      subscript        superscript  x  normal-′   x        superscript  y  normal-′   y         superscript  i  2     superscript  x  normal-′    superscript  y  normal-′        I^{2}(x,y)=\sum_{\begin{smallmatrix}x^{\prime}\leq x\\
 y^{\prime}\leq y\end{smallmatrix}}i^{2}(x^{\prime},y^{\prime})   The variance is given by:        Var   (  X  )    =    1  n     ∑   i  =  1   n     (    x  i   -  μ   )   2      .       Var  X       1  n     superscript   subscript     i  1    n    superscript     subscript  x  i   μ   2       \operatorname{Var}(X)=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\mu)^{2}.   Let    S  1     subscript  S  1    S_{1}   and    S  2     subscript  S  2    S_{2}   denote the summations of block    A  B  C  D      A  B  C  D    ABCD   of   I   I   I   and    I  2     superscript  I  2    I^{2}   , respectively.    S  1     subscript  S  1    S_{1}   and    S  2     subscript  S  2    S_{2}   are computed quickly by integral image. Now, we manipulate the variance equation as:       Var   (  X  )    =    1  n     ∑   i  =  1   n    (     x  i  2   -   2  ⋅  μ  ⋅   x  i     +   μ  2    )     =    1  n    (      ∑   i  =  1   n     (   x  i   )   2    -   2  ⋅    ∑   i  =  1   n    (   μ  ⋅   x  i    )      +    ∑   i  =  1   n    (   μ  2   )     )    =    1  n    (      ∑   i  =  1   n     (   x  i   )   2    -   2  ⋅    ∑   i  =  1   n    (   μ  ⋅   x  i    )      +   n  ⋅   (   μ  2   )     )           Var  X       1  n     superscript   subscript     i  1    n        superscript   subscript  x  i   2    normal-⋅  2  μ   subscript  x  i      superscript  μ  2               1  n         superscript   subscript     i  1    n    superscript   subscript  x  i   2     normal-⋅  2    superscript   subscript     i  1    n    normal-⋅  μ   subscript  x  i         superscript   subscript     i  1    n    superscript  μ  2               1  n         superscript   subscript     i  1    n    superscript   subscript  x  i   2     normal-⋅  2    superscript   subscript     i  1    n    normal-⋅  μ   subscript  x  i        normal-⋅  n   superscript  μ  2         \operatorname{Var}(X)=\frac{1}{n}\sum_{i=1}^{n}(x_{i}^{2}-2\cdot\mu\cdot x_{i}%
 +\mu^{2})=\frac{1}{n}(\sum_{i=1}^{n}(x_{i})^{2}-2\cdot\sum_{i=1}^{n}(\mu\cdot x%
 _{i})+\sum_{i=1}^{n}(\mu^{2}))=\frac{1}{n}(\sum_{i=1}^{n}(x_{i})^{2}-2\cdot%
 \sum_{i=1}^{n}(\mu\cdot x_{i})+n\cdot(\mu^{2}))          =    1  n    (      ∑   i  =  1   n     (   x  i   )   2    -   2  ⋅  μ  ⋅    ∑   i  =  1   n    (   x  i   )      +   n  ⋅   (   μ  2   )     )    =    1  n    (     S  2   -     2  ⋅   S  1    /  n   ⋅   S  1     +   n  ⋅   (    (    S  1   /  n   )   2   )     )    =    1  n    (    S  2   -     (   S  1   )   2   /  n    )          absent      1  n         superscript   subscript     i  1    n    superscript   subscript  x  i   2     normal-⋅  2  μ    superscript   subscript     i  1    n    subscript  x  i       normal-⋅  n   superscript  μ  2               1  n        subscript  S  2    normal-⋅     normal-⋅  2   subscript  S  1    n    subscript  S  1      normal-⋅  n   superscript     subscript  S  1   n   2               1  n      subscript  S  2      superscript   subscript  S  1   2   n        =\frac{1}{n}(\sum_{i=1}^{n}(x_{i})^{2}-2\cdot\mu\cdot\sum_{i=1}^{n}(x_{i})+n%
 \cdot(\mu^{2}))=\frac{1}{n}(S_{2}-2\cdot S_{1}/n\cdot S_{1}+n\cdot((S_{1}/n)^{%
 2}))=\frac{1}{n}(S_{2}-(S_{1})^{2}/n)   Where    μ  =    S  1   /  n       μ     subscript  S  1   n     \mu=S_{1}/n   and     S  2   =    ∑   i  =  1   n    (   x  i  2   )         subscript  S  2     superscript   subscript     i  1    n    superscript   subscript  x  i   2      S_{2}=\sum_{i=1}^{n}(x_{i}^{2})   .  Similar to the estimation of the mean (   μ   μ   \mu   ) and variance (    V  a  r      V  a  r    Var   ), which requires the integral images of the first and second power of the image respectively (i.e.    I  ,   I  2      I   superscript  I  2     I,I^{2}   ); manipulations similar to the ones mentioned above can be made to the third and fourth powers of the images (i.e      I  3    (  x  ,  y  )    ,    I  4    (  x  ,  y  )          superscript  I  3    x  y       superscript  I  4    x  y      I^{3}(x,y),I^{4}(x,y)   .) for obtaining the skewness and kurtosis. 7 But one important implementation detail that must be kept in mind for the above methods, as mentioned by F Shafait et al. 8 is that of integer overflow occurring for the higher order integral images in case 32-bit integers are used.  References    External links   Summed table implementation in object detection    Lecture videos:    An introduction to the theory behind the integral image algorithm  A demonstration to a continuous version of the integral image algorithm, from the Wolfram Demonstrations Project   "  Category:Digital geometry  Category:Computer graphics data structures     ↩  ↩  ↩   ↩  ↩   ↩     