   Kernel random forest      Kernel random forest   In machine learning, kernel random forests establish the connection between random forests and kernel methods . By slightly modifying their definition, random forests can be rewritten as kernel methods , which are more interpretable and easier to analyze. 1  History  Leo Breiman 2 was the first person to notice the link between random forest and kernel methods . He pointed out that random forests which are grown using i.i.d random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon 3 established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani 4 proposed Random Forest Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet 5 first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest . He also gave explicit expressions for kernels based on centred random forest 6 and uniform random forest, 7 two simplified models of random forest . He named these two KeRFs by Centred KeRF and Uniform KeRF ,and proved upper bounds on their rates of consistency.  Notations and definitions  Preliminaries: Centred forests  Centred forest 8 is a simplified model for Breiman's original random forest , which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level   k   k   k   is built, where    k  ∈  ℕ      k  ℕ    k\in\mathbb{N}   is a parameter of the algorithm.  Uniform forest  Uniform forest 9 is another simplified model for Breiman's original random forest , which uniformly selects an attribute among all attributes and performs splits at a point uniformly drawn on the side of the cell, along the preselected attribute.  From random forest to KeRF  Given a training sample     𝒟  n   =    {   (   𝐗  i   ,   Y  i   )   }    i  =  1   n        subscript  𝒟  n    superscript   subscript     subscript  𝐗  i    subscript  Y  i       i  1    n     \mathcal{D}_{n}=\{(\mathbf{X}_{i},Y_{i})\}_{i=1}^{n}   of      [  0  ,  1  ]   p   ×  ℝ       superscript   0  1   p   ℝ    [0,1]^{p}\times\mathbb{R}   -valued independent random variables distributed as the independent prototype pair    (  𝐗  ,  Y  )     𝐗  Y    (\mathbf{X},Y)   , where     𝔼   [   Y  2   ]    <  ∞        𝔼   delimited-[]   superscript  Y  2        \mathbb{E}[Y^{2}]<\infty   . We aim at predicting the response   Y   Y   Y   ,associated with the random variable   𝐗   𝐗   \mathbf{X}   , by estimating the regression function    m   (  𝐱  )   =  𝔼   [  Y  |  𝐗  =  𝐱  ]      fragments  m   fragments  normal-(  x  normal-)    E   fragments  normal-[  Y  normal-|  X   x  normal-]     m(\mathbf{x})=\mathbb{E}[Y|\mathbf{X}=\mathbf{x}]   . A random regression forest is an ensemble of   M   M   M   randomized regression trees. Denote     m  n    (  𝐱  ,   𝚯  j   )        subscript  m  n    𝐱   subscript  𝚯  j      m_{n}(\mathbf{x},\mathbf{\Theta}_{j})   the predicted value at point   𝐱   𝐱   \mathbf{x}   by the   j   j   j   -th tree, where     𝚯  1   ,  ⋯  ,   𝚯  M       subscript  𝚯  1   normal-⋯   subscript  𝚯  M     \mathbf{\Theta}_{1},\cdots,\mathbf{\Theta}_{M}   are independent random variables, distributed as a generic random variable   𝚯   𝚯   \mathbf{\Theta}   , independent of the sample    𝒟  n     subscript  𝒟  n    \mathcal{D}_{n}   . This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate      m   M  ,  n     (  𝐱  ,   Θ  1   ,  ⋯  ,   Θ  M   )    =    1  M     ∑   j  =  1   M     m  n    (  𝐱  ,   Θ  j   )             subscript  m   M  n     𝐱   subscript  normal-Θ  1   normal-⋯   subscript  normal-Θ  M         1  M     superscript   subscript     j  1    M      subscript  m  n    𝐱   subscript  normal-Θ  j         m_{M,n}(\mathbf{x},\Theta_{1},\cdots,\Theta_{M})=\frac{1}{M}\sum_{j=1}^{M}m_{n%
 }(\mathbf{x},\Theta_{j})   . For regression trees, we have     m  n   =    ∑   i  =  1   n      Y  i    𝟏    𝐗  i   ∈    A  n    (  𝐱  ,   Θ  j   )         N  n    (  𝐱  ,   Θ  j   )           subscript  m  n     superscript   subscript     i  1    n        subscript  Y  i    subscript  1     subscript  𝐗  i      subscript  A  n    𝐱   subscript  normal-Θ  j           subscript  N  n    𝐱   subscript  normal-Θ  j         m_{n}=\sum_{i=1}^{n}\frac{Y_{i}\mathbf{1}_{\mathbf{X}_{i}\in A_{n}(\mathbf{x},%
 \Theta_{j})}}{N_{n}(\mathbf{x},\Theta_{j})}   , where     A  n    (  𝐱  ,   Θ  j   )        subscript  A  n    𝐱   subscript  normal-Θ  j      A_{n}(\mathbf{x},\Theta_{j})   is the cell containing   𝐱   𝐱   \mathbf{x}   , designed with randomness    Θ  j     subscript  normal-Θ  j    \Theta_{j}   and dataset    𝒟  n     subscript  𝒟  n    \mathcal{D}_{n}   , and      N  n    (  𝐱  ,   Θ  j   )    =    ∑   i  =  1   n    𝟏    𝐗  i   ∈    A  n    (  𝐱  ,   Θ  j   )              subscript  N  n    𝐱   subscript  normal-Θ  j       superscript   subscript     i  1    n    subscript  1     subscript  𝐗  i      subscript  A  n    𝐱   subscript  normal-Θ  j          N_{n}(\mathbf{x},\Theta_{j})=\sum_{i=1}^{n}\mathbf{1}_{\mathbf{X}_{i}\in A_{n}%
 (\mathbf{x},\Theta_{j})}   .  Thus random forest estimates satisfy, for all    𝐱  ∈    [  0  ,  1  ]   d       𝐱   superscript   0  1   d     \mathbf{x}\in[0,1]^{d}   ,      m   M  ,  n     (  𝐱  ,   Θ  1   ,  …  ,   Θ  M   )    =    1  M     ∑   j  =  1   M    (    ∑   i  =  1   n      Y  i    𝟏    𝐗  i   ∈    A  n    (  𝐱  ,   Θ  j   )         N  n    (  𝐱  ,   Θ  j   )      )            subscript  m   M  n     𝐱   subscript  normal-Θ  1   normal-…   subscript  normal-Θ  M         1  M     superscript   subscript     j  1    M     superscript   subscript     i  1    n        subscript  Y  i    subscript  1     subscript  𝐗  i      subscript  A  n    𝐱   subscript  normal-Θ  j           subscript  N  n    𝐱   subscript  normal-Θ  j           m_{M,n}(\mathbf{x},\Theta_{1},\ldots,\Theta_{M})=\frac{1}{M}\sum_{j=1}^{M}%
 \left(\sum_{i=1}^{n}\frac{Y_{i}\mathbf{1}_{\mathbf{X}_{i}\in A_{n}(\mathbf{x},%
 \Theta_{j})}}{N_{n}(\mathbf{x},\Theta_{j})}\right)   . Random regression forest has two level of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet 10 defined KeRF by         m  ~    M  ,  n     (  𝐱  ,   Θ  1   ,  …  ,   Θ  M   )    =    1    ∑   j  =  1   M     N  n    (  𝐱  ,   Θ  j   )        ∑   j  =  1   M     ∑   i  =  1   n     Y  i    𝟏    𝐗  i   ∈    A  n    (  𝐱  ,   Θ  j   )                 subscript   normal-~  m    M  n     𝐱   subscript  normal-Θ  1   normal-…   subscript  normal-Θ  M         1    superscript   subscript     j  1    M      subscript  N  n    𝐱   subscript  normal-Θ  j         superscript   subscript     j  1    M     superscript   subscript     i  1    n      subscript  Y  i    subscript  1     subscript  𝐗  i      subscript  A  n    𝐱   subscript  normal-Θ  j             \tilde{m}_{M,n}(\mathbf{x},\Theta_{1},\ldots,\Theta_{M})=\frac{1}{\sum_{j=1}^{%
 M}N_{n}(\mathbf{x},\Theta_{j})}\sum_{j=1}^{M}\sum_{i=1}^{n}Y_{i}\mathbf{1}_{%
 \mathbf{X}_{i}\in A_{n}(\mathbf{x},\Theta_{j})}   ,which is equal to the mean of the    Y  i     subscript  Y  i    Y_{i}   's falling in the cells containing   𝐱   𝐱   \mathbf{x}   in the forest. If we define      K   M  ,  n     (  𝐱  ,  𝐳  )    =    1  M     ∑   j  =  1   M    𝟏    𝐗  i   ∈    A  n    (  𝐱  ,   Θ  j   )               subscript  K   M  n     𝐱  𝐳        1  M     superscript   subscript     j  1    M    subscript  1     subscript  𝐗  i      subscript  A  n    𝐱   subscript  normal-Θ  j           K_{M,n}(\mathbf{x},\mathbf{z})=\frac{1}{M}\sum_{j=1}^{M}\mathbf{1}_{\mathbf{X}%
 _{i}\in A_{n}(\mathbf{x},\Theta_{j})}   the connection function of the   M   M   M   finite forest,then almost surely, we have       m  ~    M  ,  n     (  𝐱  ,   Θ  1   ,  …  ,   Θ  M   )    =      ∑   i  =  1   n      Y  i    K   M  ,  n     (  𝐱  ,   𝐗  i   )        ∑   ℓ  =  1   n      K   M  ,  n     (  𝐱  ,   𝐗  ℓ   )             subscript   normal-~  m    M  n     𝐱   subscript  normal-Θ  1   normal-…   subscript  normal-Θ  M         superscript   subscript     i  1    n      subscript  Y  i    subscript  K   M  n     𝐱   subscript  𝐗  i        superscript   subscript     normal-ℓ  1    n      subscript  K   M  n     𝐱   subscript  𝐗  normal-ℓ         \tilde{m}_{M,n}(\mathbf{x},\Theta_{1},\ldots,\Theta_{M})=\frac{\sum_{i=1}^{n}Y%
 _{i}K_{M,n}(\mathbf{x},\mathbf{X}_{i})}{\sum_{\ell=1}^{n}K_{M,n}(\mathbf{x},%
 \mathbf{X}_{\ell})}   , which defines the KeRF .  Centred KeRF  The construction of Centred KeRF of level   k   k   k   is the same as for centred forest , except that predictions are made by      m  ~    M  ,  n     (  𝐱  ,   Θ  1   ,  …  ,   Θ  M   )        subscript   normal-~  m    M  n     𝐱   subscript  normal-Θ  1   normal-…   subscript  normal-Θ  M      \tilde{m}_{M,n}(\mathbf{x},\Theta_{1},\ldots,\Theta_{M})   , the corresponding kernel function, or connection function is        K  k   c  c     (  𝐱  ,  𝐳  )    =    ∑     k  1   ,  …  ,   k  d   ,     ∑   j  =  1   d     k  j     =  k       k  !      k  1   !   …    k  d   !       (   1  d   )   k     ∏   j  =  1   d    𝟏    ⌈    2   k  j     x  j    ⌉   =   ⌈    2   k  j     z  j    ⌉               superscript   subscript  K  k     c  c     𝐱  𝐳      subscript       subscript  k  1   normal-…   subscript  k  d     superscript   subscript     j  1    d    subscript  k  j     k          k        subscript  k  1    normal-…     subscript  k  d       superscript    1  d   k     superscript   subscript  product    j  1    d    subscript  1         superscript  2   subscript  k  j     subscript  x  j          superscript  2   subscript  k  j     subscript  z  j            K_{k}^{cc}(\mathbf{x},\mathbf{z})=\sum_{k_{1},\ldots,k_{d},\sum_{j=1}^{d}k_{j}%
 =k}\frac{k!}{k_{1}!\ldots k_{d}!}\left(\frac{1}{d}\right)^{k}\prod_{j=1}^{d}%
 \mathbf{1}_{\lceil 2^{k_{j}}x_{j}\rceil=\lceil 2^{k_{j}}z_{j}\rceil}   , for all     𝐱  ,  𝐳   ∈    [  0  ,  1  ]   d        𝐱  𝐳    superscript   0  1   d     \mathbf{x},\mathbf{z}\in[0,1]^{d}   .  Uniform KeRF  Uniform KeRF is built in the same way as uniform forest , except that predictions are made by      m  ~    M  ,  n     (  𝐱  ,   Θ  1   ,  …  ,   Θ  M   )        subscript   normal-~  m    M  n     𝐱   subscript  normal-Θ  1   normal-…   subscript  normal-Θ  M      \tilde{m}_{M,n}(\mathbf{x},\Theta_{1},\ldots,\Theta_{M})   , the corresponding kernel function, or connection function is        K  k   u  f     (  𝟎  ,  𝐱  )    =    ∑     k  1   ,  …  ,   k  d   ,     ∑   j  =  1   d     k  j     =  k       k  !      k  1   !   …    k  d   !       (   1  d   )   k     ∏   m  =  1   d    (   1  -    |   x  m   |     ∑   j  =  0     k  m   -  1       (   -   ln   |   x  m   |     )   j    j  !       )             superscript   subscript  K  k     u  f     0  𝐱      subscript       subscript  k  1   normal-…   subscript  k  d     superscript   subscript     j  1    d    subscript  k  j     k          k        subscript  k  1    normal-…     subscript  k  d       superscript    1  d   k     superscript   subscript  product    m  1    d     1       subscript  x  m      superscript   subscript     j  0       subscript  k  m   1       superscript         subscript  x  m      j     j            K_{k}^{uf}(\mathbf{0},\mathbf{x})=\sum_{k_{1},\ldots,k_{d},\sum_{j=1}^{d}k_{j}%
 =k}\frac{k!}{k_{1}!\ldots k_{d}!}\left(\frac{1}{d}\right)^{k}\prod_{m=1}^{d}%
 \left(1-|x_{m}|\sum_{j=0}^{k_{m}-1}\frac{(-\ln|x_{m}|)^{j}}{j!}\right)   , for all    𝐱  ∈    [  0  ,  1  ]   d       𝐱   superscript   0  1   d     \mathbf{x}\in[0,1]^{d}   .  Properties  Relation between KeRF and random forest  Predictions given by KeRF and random forests are close if the number of points in each cell is controlled:          Assume that there exist sequences   (   a  n   )    ,    (   b  n   )   such that, a.s.   ,   a  n    ≤    N  n    (  𝐱  ,  Θ  )    ≤    b  n   and   a  n    ≤    1  M     ∑   m  =  1   M     N  n   𝐱      ,    Θ  m   ≤    b  n   ,  then almost surely     ,     formulae-sequence         Assume that there exist sequences   subscript  a  n       subscript  b  n   such that, a.s.    subscript  a  n       subscript  N  n    𝐱  normal-Θ            subscript  b  n   and   subscript  a  n             1  M     superscript   subscript     m  1    M      subscript  N  n   𝐱          subscript  normal-Θ  m     subscript  b  n   then almost surely      \text{Assume that there exist sequences }(a_{n}),(b_{n})\text{ such that, a.s.%
 },a_{n}\leq N_{n}(\mathbf{x},\Theta)\leq b_{n}\text{ and }a_{n}\leq\frac{1}{M}%
 \sum_{m=1}^{M}N_{n}{\mathbf{x},\Theta_{m}}\leq b_{n},\text{ then almost surely},          |     m   M  ,  n     (  𝐱  )    -     m  ~    M  ,  n     (  𝐱  )     |   ≤      b  n   -   a  n     a  n      m  ~    M  ,  n     (  𝐱  )               subscript  m   M  n    𝐱      subscript   normal-~  m    M  n    𝐱            subscript  b  n    subscript  a  n     subscript  a  n     subscript   normal-~  m    M  n    𝐱     |m_{M,n}(\mathbf{x})-\tilde{m}_{M,n}(\mathbf{x})|\leq\frac{b_{n}-a_{n}}{a_{n}}%
 \tilde{m}_{M,n}(\mathbf{x})   .  Relation between infinite KeRF and infinite random forest  When the number of trees   M   M   M   goes to infinity, then we have infinite random forest and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:       Assume that there exist sequences   (   ε  n   )    ,   (   a  n   )   ,    (   b  n   )   such that, a.s.        Assume that there exist sequences   subscript  ε  n     subscript  a  n      subscript  b  n   such that, a.s.     \text{Assume that there exist sequences }(\varepsilon_{n}),(a_{n}),(b_{n})%
 \text{ such that, a.s.}           𝔼   [    N  n    (  𝐱  ,  Θ  )    ]    ≥  1        𝔼   delimited-[]     subscript  N  n    𝐱  normal-Θ      1    \mathbb{E}[N_{n}(\mathbf{x},\Theta)]\geq 1   ,      ℙ   [   a  n   ≤   N  n    (  𝐱  ,  Θ  )   ≤   b  n   ∣   𝒟  n   ]   ≥  1  -   ε  n   /  2     fragments  P   fragments  normal-[   subscript  a  n     subscript  N  n    fragments  normal-(  x  normal-,  Θ  normal-)     subscript  b  n   normal-∣   subscript  𝒟  n   normal-]    1    subscript  ε  n    2    \mathbb{P}[a_{n}\leq N_{n}(\mathbf{x},\Theta)\leq b_{n}\mid\mathcal{D}_{n}]%
 \geq 1-\varepsilon_{n}/2   ,      ℙ   [   a  n   ≤   𝔼  Θ    [   N  n    (  𝐱  ,  Θ  )   ]   ≤   b  n   ∣   𝒟  n   ]   ≥  1  -   ε  n   /  2     fragments  P   fragments  normal-[   subscript  a  n     subscript  𝔼  normal-Θ    fragments  normal-[   subscript  N  n    fragments  normal-(  x  normal-,  Θ  normal-)   normal-]     subscript  b  n   normal-∣   subscript  𝒟  n   normal-]    1    subscript  ε  n    2    \mathbb{P}[a_{n}\leq\mathbb{E}_{\Theta}[N_{n}(\mathbf{x},\Theta)]\leq b_{n}%
 \mid\mathcal{D}_{n}]\geq 1-\varepsilon_{n}/2   ,       Then almost surely,  |   m   ∞  ,  n     (  𝐱  -    m  ~    ∞  ,  n     (  𝐱  )   |  ≤     b  n   -   a  n     a  n      m  ~    ∞  ,  n     (  𝐱  )   +  n   ε  n    (   max   1  ≤  i  ≤  n     Y  i   )       fragments  Then almost surely,  normal-|   subscript  m    n     fragments  normal-(  x    subscript   normal-~  m     n     fragments  normal-(  x  normal-)   normal-|        subscript  b  n    subscript  a  n     subscript  a  n     subscript   normal-~  m     n     fragments  normal-(  x  normal-)    n   subscript  ε  n    fragments  normal-(   subscript       1  i       n      subscript  Y  i   normal-)      \text{Then almost surely,}|m_{\infty,n}(\mathbf{x}-\tilde{m}_{\infty,n}(%
 \mathbf{x})|\leq\frac{b_{n}-a_{n}}{a_{n}}\tilde{m}_{\infty,n}(\mathbf{x})+n%
 \varepsilon_{n}\left(\max_{1\leq i\leq n}Y_{i}\right)   .  Consistency results  Assume that    Y  =    m   (  𝐗  )    +  ε       Y      m  𝐗   ε     Y=m(\mathbf{X})+\varepsilon   , where   ε   ε   \varepsilon   is a centred Gaussian noise, independent of   𝐗   𝐗   \mathbf{X}   , with finite variance     σ  2   <  ∞       superscript  σ  2      \sigma^{2}<\infty   . Moreover,   𝐗   𝐗   \mathbf{X}   is uniformly distributed on     [  0  ,  1  ]   d     superscript   0  1   d    [0,1]^{d}   and   m   m   m   is Lipschitz . Scornet 11 proved upper bounds on the rates of consistency for centred KeRF and uniform KeRF .  Consistency of centred KeRF  Providing    k  →  ∞     normal-→  k     k\rightarrow\infty   and     n  /   2  k    →  ∞     normal-→    n   superscript  2  k       n/2^{k}\rightarrow\infty   , there exists a constant     C  1   >  0       subscript  C  1   0    C_{1}>0   such that, for all   n   n   n   ,     𝔼    [      m  ~   n   c  c     (  𝐗  )    -   m   (  𝐗  )     ]   2    ≤    C  1    n   -   1  /   (   3  +   d   log  2     )        (   log  n   )   2          𝔼   superscript   delimited-[]       superscript   subscript   normal-~  m   n     c  c    𝐗     m  𝐗     2       subscript  C  1    superscript  n      1    3    d    2         superscript    n   2      \mathbb{E}[\tilde{m}_{n}^{cc}(\mathbf{X})-m(\mathbf{X})]^{2}\leq C_{1}n^{-1/(3%
 +d\log 2)}(\log n)^{2}   .  Consistency of uniform KeRF  Providing    k  →  ∞     normal-→  k     k\rightarrow\infty   and     n  /   2  k    →  ∞     normal-→    n   superscript  2  k       n/2^{k}\rightarrow\infty   , there exists a constant    C  >  0      C  0    C>0   such that,     𝔼    [      m  ~   n   u  f     (  𝐗  )    -   m   (  𝐗  )     ]   2    ≤   C   n   -   2  /   (   6  +   3  d   log  2     )        (   log  n   )   2          𝔼   superscript   delimited-[]       superscript   subscript   normal-~  m   n     u  f    𝐗     m  𝐗     2      C   superscript  n      2    6    3  d    2         superscript    n   2      \mathbb{E}[\tilde{m}_{n}^{uf}(\mathbf{X})-m(\mathbf{X})]^{2}\leq Cn^{-2/(6+3d%
 \log 2)}(\log n)^{2}   .  References        "  Category:Statistics  Category:Machine learning     ↩  ↩  ↩  ↩   ↩  ↩         