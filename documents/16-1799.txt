


Kernel random forest




Kernel random forest

In machine learning, kernel random forests establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.1
History
Leo Breiman2 was the first person to notice the link between random forest and kernel methods. He pointed out that random forests which are grown using i.i.d random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon 3 established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani4 proposed Random Forest Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet5 first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest. He also gave explicit expressions for kernels based on centred random forest6 and uniform random forest,7 two simplified models of random forest. He named these two KeRFs by Centred KeRF and Uniform KeRF,and proved upper bounds on their rates of consistency.
Notations and definitions
Preliminaries: Centred forests
Centred forest8 is a simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level 
 
 
 
 
  is built, where 
 
 
 
  is a parameter of the algorithm.
Uniform forest
Uniform forest9 is another simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at a point uniformly drawn on the side of the cell, along the preselected attribute.
From random forest to KeRF
Given a training sample 
 
 
 
  of 
 
 
 
 -valued independent random variables distributed as the independent prototype pair 
 
 
 
 , where 
 
 
 
 
 . We aim at predicting the response 
 
 
 
 ,associated with the random variable 
 
 
 
 , by estimating the regression function 
 
 
 
 . A random regression forest is an ensemble of 
 
 
 
  randomized regression trees. Denote 
 
 
 
 
  the predicted value at point 
 
 
 
  by the 
 
 
 
 -th tree, where 
 
 
 
  are independent random variables, distributed as a generic random variable 
 
 
 
 , independent of the sample 
 
 
 
 
 . This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate 
 
 
 
 . For regression trees, we have 
 
 
 
 , where 
 
 
 
  is the cell containing 
 
 
 
 , designed with randomness 
 
 
 
 
  and dataset 
 
 
 
 , and 
 
 
 
 .
Thus random forest estimates satisfy, for all 
 
 
 
 , 
 
 
 
 . Random regression forest has two level of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet10 defined KeRF by


 
 ,which is equal to the mean of the 
 
 
 
 's falling in the cells containing 
 
 
 
  in the forest. If we define 
 
 
 
  the connection function of the 
 
 
 
  finite forest,then almost surely, we have 
 
 
 
 , which defines the KeRF.
Centred KeRF
The construction of Centred KeRF of level 
 
 
 
  is the same as for centred forest, except that predictions are made by 
 
 
 
 , the corresponding kernel function, or connection function is


 
 , for all 
 
 
 
 .
Uniform KeRF
Uniform KeRF is built in the same way as uniform forest, except that predictions are made by 
 
 
 
 , the corresponding kernel function, or connection function is


 
 , for all 
 
 
 
 .
Properties
Relation between KeRF and random forest
Predictions given by KeRF and random forests are close if the number of points in each cell is controlled:





 
 .
Relation between infinite KeRF and infinite random forest
When the number of trees 
 
 
 
  goes to infinity, then we have infinite random forest and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:






 
 ,


 
 ,


 
 ,



 
 .
Consistency results
Assume that 
 
 
 
 , where 
 
 
 
  is a centred Gaussian noise, independent of 
 
 
 
 , with finite variance 
 
 
 
 . Moreover, 
 
 
 
  is uniformly distributed on 
 
 
 
  and 
 
 
 
  is Lipschitz. Scornet11 proved upper bounds on the rates of consistency for centred KeRF and uniform KeRF.
Consistency of centred KeRF
Providing 
 
 
 
  and 
 
 
 
 , there exists a constant 
 
 
 
  such that, for all 
 
 
 
 , 
 
 
 
 .
Consistency of uniform KeRF
Providing 
 
 
 
  and 
 
 
 
 , there exists a constant 
 
 
 
  such that, 
 
 
 
 .
References






"
Category:Statistics Category:Machine learning


















