   Ratio estimator      Ratio estimator   The ratio estimator is a statistical parameter and is defined to be the ratio of means of two variates . Ratio estimates are biased and corrections must be made when they are used in experimental or survey work. The ratio estimates are asymmetrical and symmetrical tests such as the t test should not be used to generate confidence intervals.  The bias is of the order O (1/ n ) (see big O notation ) so as the sample size ( n ) increases, the bias will asymptotically approach 0. Therefore, the estimator is approximately unbiased for large sample sizes.  Definition  Assume there are two characteristics – x and y – that can be observed for each sampled element in the data set. The ratio R is      R  =     μ  ¯   y   /     μ  ¯   x         R     subscript   normal-¯  μ   y    subscript   normal-¯  μ   x      R=\bar{\mu}_{y}/\bar{\mu}_{x}\,     The ratio estimate of a value of the y variate ( θ y ) is       θ  y   =   R    θ  x          subscript  θ  y     R   subscript  θ  x      \theta_{y}=R\theta_{x}\,     where θ x is the corresponding value of the x variate. θ y is known to be asymptotically normally distributed. 1  Statistical properties  The sample ratio ( r ) is estimated from the sample      r  =    y  ¯    x  ¯    =     ∑   i  =  1   n   y     ∑   i  =  1   n   x          r     normal-¯  y    normal-¯  x             superscript   subscript     i  1    n   y     superscript   subscript     i  1    n   x       r=\frac{\bar{y}}{\bar{x}}=\frac{\sum_{i=1}^{n}y}{\sum_{i=1}^{n}x}     That the ratio is biased can be shown with Jensen's inequality as follows:       E   (   x  y   )    =   E   (   x   1  y    )    =   E   (  x  )   E   (   1  y   )    ≥   E   (  x  )    1   E   (  y  )      =    E   (  x  )     E   (  y  )             E    x  y      E    x    1  y            E  x  E    1  y           E  x    1    E  y              E  x     E  y       E\left(\frac{x}{y}\right)=E\left(x\frac{1}{y}\right)=E(x)E\left(\frac{1}{y}%
 \right)\geq E(x)\frac{1}{E(y)}=\frac{E(x)}{E(y)}     Under simple random sampling the bias is of the order O ( n −1 ). An upper bound on the relative bias of the estimate is provided by the coefficient of variation (the ratio of the standard deviation to the mean ). 2 Under simple random sampling the relative bias is O ( n −1/2 ).  Correction of the mean's bias  The correction methods, depending on the distributions of the x and y variates, differ in their efficiency making it difficult to recommend an overall best method. Because the estimates of r are biased a corrected version should be used in all subsequent calculations.  A correction of the bias accurate to the first order is 3       r  corr   =   r  -    s    [   y  /  x   ]   x     m  x          subscript  r  corr     r     subscript  s     delimited-[]    y  x    x     subscript  m  x       r_{\mathrm{corr}}=r-\frac{s_{[y/x]x}}{m_{x}}     where m x is the mean of the variate x and s ab is the covariance between a and b .  To simplify the notation s ab will be used subsequently to denote the covariance between the variates a and b .  Another estimator based on the Taylor expansion is       r  corr   =   r  +    1  n    (   1  -    n  -  1    N  -  1     )      r   s  x  2    -   ρ   s  x    s  y      m  x  2           subscript  r  corr     r      1  n     1      n  1     N  1           r   superscript   subscript  s  x   2      ρ   subscript  s  x    subscript  s  y      superscript   subscript  m  x   2        r_{\mathrm{corr}}=r+\frac{1}{n}(1-\frac{n-1}{N-1})\frac{rs_{x}^{2}-\rho s_{x}s%
 _{y}}{m_{x}^{2}}     where n is the sample size, N is the population size, m x is the mean of the variate x , s x 2 and s y 2 are the sample variances of the x and y variates respectively and ρ is the sample correlation between the x and y variates.  A computationally simpler but slightly less accurate version of this estimator is       r  corr   =   r  -     N  -  n   N     (    r   s  x  2    -   ρ   s  x    s  y     )    n   m  x  2            subscript  r  corr     r        N  n   N         r   superscript   subscript  s  x   2      ρ   subscript  s  x    subscript  s  y       n   superscript   subscript  m  x   2         r_{\mathrm{corr}}=r-\frac{N-n}{N}\frac{(rs_{x}^{2}-\rho s_{x}s_{y})}{nm_{x}^{2}}     where N is the population size, n is the sample size, m x is the mean of the x variate, s x 2 and s y 2 are the sample variances of the x and y variates respectively and ρ is the sample correlation between the x and y variates. These versions differ only in the factor in the denominator ( N - 1 ). For a large N the difference is negligible.  A second-order correction is 4       r  corr   =   r   [   1  +    1  n    (    1   m  x    -    s   x  y      m  x    m  y      )    +    1   n  2     (     2   m  x  2    -     s   x  y      m  x    m  y      [   2  +   3   m  x     ]     +    s    x  2   y      m  x  2    m  y      )     ]         subscript  r  corr     r   delimited-[]    1      1  n       1   subscript  m  x       subscript  s    x  y       subscript  m  x    subscript  m  y           1   superscript  n  2          2   superscript   subscript  m  x   2         subscript  s    x  y       subscript  m  x    subscript  m  y      delimited-[]    2    3   subscript  m  x           subscript  s     superscript  x  2   y       superscript   subscript  m  x   2    subscript  m  y            r_{\mathrm{corr}}=r\left[1+\frac{1}{n}\left(\frac{1}{m_{x}}-\frac{s_{xy}}{m_{x%
 }m_{y}}\right)+\frac{1}{n^{2}}\left(\frac{2}{m_{x}^{2}}-\frac{s_{xy}}{m_{x}m_{%
 y}}\left[2+\frac{3}{m_{x}}\right]+\frac{s_{x^{2}y}}{m_{x}^{2}m_{y}}\right)\right]     Other methods of bias correction have also been proposed. To simplify the notation the following variables will be used      θ  =    1  n   -   1  N        θ      1  n     1  N      \theta=\frac{1}{n}-\frac{1}{N}          c  x  2   =    s  x  2    m  x  2         superscript   subscript  c  x   2      superscript   subscript  s  x   2    superscript   subscript  m  x   2      c_{x}^{2}=\frac{s_{x}^{2}}{m_{x}^{2}}          c   x  y    =    s   x  y      m  x    m  y          subscript  c    x  y       subscript  s    x  y       subscript  m  x    subscript  m  y       c_{xy}=\frac{s_{xy}}{m_{x}m_{y}}     Pascual's estimator: 5       r  corr   =   r  +     N  -  1   N      m  y   -   r   m  x      n  -  1           subscript  r  corr     r        N  1   N        subscript  m  y     r   subscript  m  x       n  1        r_{\mathrm{corr}}=r+\frac{N-1}{N}\frac{m_{y}-rm_{x}}{n-1}     Beale's estimator: 6       r  corr   =   r    1  +   θ   c   x  y       1  +   θ   c  x  2            subscript  r  corr     r      1    θ   subscript  c    x  y        1    θ   superscript   subscript  c  x   2         r_{\mathrm{corr}}=r\frac{1+\theta c_{xy}}{1+\theta c_{x}^{2}}     Tin's estimator: 7       r  corr   =   r   (   1  +   θ   (    c   x  y    -   c  x  2    )     )         subscript  r  corr     r    1    θ     subscript  c    x  y     superscript   subscript  c  x   2         r_{\mathrm{corr}}=r\left(1+\theta\left(c_{xy}-c_{x}^{2}\right)\right)     Sahoo's estimator: 8       r  corr   =   r   1  +   θ   (    c  x  2   -   c   x  y     )           subscript  r  corr     r    1    θ     superscript   subscript  c  x   2    subscript  c    x  y          r_{\mathrm{corr}}=\frac{r}{1+\theta(c_{x}^{2}-c_{xy})}     Sahoo has also proposed a number of additional estimators: 9       r  corr   =   r   (   1  +   θ   c   x  y      )    (   1  -   θ   c  x  2     )         subscript  r  corr     r    1    θ   subscript  c    x  y        1    θ   superscript   subscript  c  x   2        r_{\mathrm{corr}}=r(1+\theta c_{xy})(1-\theta c_{x}^{2})          r  corr   =    r   (   1  -   θ   c  x  2     )     1  -   θ   c   x  y            subscript  r  corr       r    1    θ   superscript   subscript  c  x   2        1    θ   subscript  c    x  y         r_{\mathrm{corr}}=\frac{r(1-\theta c_{x}^{2})}{1-\theta c_{xy}}          r  corr   =   r    (   1  +   θ   c   x  y      )    (   1  +   θ   c  x  2     )          subscript  r  corr     r      1    θ   subscript  c    x  y        1    θ   superscript   subscript  c  x   2         r_{\mathrm{corr}}=\frac{r}{(1+\theta c_{xy})(1+\theta c_{x}^{2})}     If m x and m y are both greater than 10, then the following approximation is correct to order O( n −3 ). 10       r  corr   =   r   [   1  -    2    n  2    m  x      (    1   m  x    -    s   x  y      m  x    m  y      )    (   1  +   13   2  n    +   8   n   m  x      )     ]         subscript  r  corr     r   delimited-[]    1      2     superscript  n  2    subscript  m  x         1   subscript  m  x       subscript  s    x  y       subscript  m  x    subscript  m  y        1    13    2  n      8    n   subscript  m  x            r_{\mathrm{corr}}=r\left[1-\frac{2}{n^{2}m_{x}}\left(\frac{1}{m_{x}}-\frac{s_{%
 xy}}{m_{x}m_{y}}\right)\left(1+\frac{13}{2n}+\frac{8}{nm_{x}}\right)\right]     An asymptotically correct estimator is 11       r  corr   =    r  +    c  x  2     m  y    m  x      -    s   x  y     m  x  2          subscript  r  corr       r     superscript   subscript  c  x   2      subscript  m  y    subscript  m  x         subscript  s    x  y     superscript   subscript  m  x   2       r_{\mathrm{corr}}=r+c_{x}^{2}\frac{m_{y}}{m_{x}}-\frac{s_{xy}}{m_{x}^{2}}     Jackknife estimation  A jackknife estimate of the ratio is less biased than the naive form. A jackknife estimator of the ratio is       r  corr   =    n  r   -     n  -  1   n     ∑   i  ≠  j  =  1   n    r  i           subscript  r  corr       n  r         n  1   n     superscript   subscript       i  j       1     n    subscript  r  i        r_{\mathrm{corr}}=nr-\frac{n-1}{n}\sum_{i\neq j=1}^{n}r_{i}     where n is the size of the sample and the r i are estimated with the omission of one pair of variates at a time. 12  An alternative method is to divide the sample into g groups each of size p with n = pg . 13 Let r i be the estimate of the i th group. Then the estimator       r  corr   =    g  r   -     g  -  1   g     ∑   i  =  1   g    r  i           subscript  r  corr       g  r         g  1   g     superscript   subscript     i  1    g    subscript  r  i        r_{\mathrm{corr}}=gr-\frac{g-1}{g}\sum_{i=1}^{g}r_{i}     has a bias of at most O ( n −2 ).  Other estimators based on the division of the sample into g groups are: 14       r  corr   =     g   g  +  1    r   -    1   g   (   g  -  1   )       ∑   i  =  1   g    r  i           subscript  r  corr         g    g  1    r       1    g    g  1       superscript   subscript     i  1    g    subscript  r  i        r_{\mathrm{corr}}=\frac{g}{g+1}r-\frac{1}{g(g-1)}\sum_{i=1}^{g}r_{i}          r  corr   =    r  ¯   +    n   n  -  1       m  y   -    r  ¯    m  x      m  x           subscript  r  corr      normal-¯  r       n    n  1         subscript  m  y      normal-¯  r    subscript  m  x      subscript  m  x        r_{\mathrm{corr}}=\bar{r}+\frac{n}{n-1}\frac{m_{y}-\bar{r}m_{x}}{m_{x}}          r  corr   =     r  g   ¯   +    g   (    m  y   -     r  g   ¯    m  x     )     m  x          subscript  r  corr      normal-¯   subscript  r  g        g     subscript  m  y      normal-¯   subscript  r  g     subscript  m  x       subscript  m  x       r_{\mathrm{corr}}=\bar{r_{g}}+\frac{g(m_{y}-\bar{r_{g}}m_{x})}{m_{x}}     where    r  ¯     normal-¯  r    \bar{r}   is the mean of the ratios r g of the g groups and        r  g   ¯   =   ∑    r  i    ′    g         normal-¯   subscript  r  g         superscript   subscript  r  i    normal-′    g      \bar{r_{g}}=\sum\frac{r_{i}^{{}^{\prime}}}{g}     where r i ' is the value of the sample ratio with the i th group omitted.  Other methods of estimation  Other methods of estimating a ratio estimator include maximum likelihood and bootstrapping . 15  Estimate of total  The estimated total of the y variate ( τ y ) is       τ  y   =   r   τ  x         subscript  τ  y     r   subscript  τ  x      \tau_{y}=r\tau_{x}     where ( τ x ) is the total of the x variate.  Variance estimates  The variance of the sample ratio is approximately:       var   (  r  )    =    1    s  x  2   +   m  x  2      [      (    s  y  2   -   s    x  2    [    y  2   /   x  2    ]      )   -    (   s   x   [   y  /  x   ]     )   2    +   2   m  y    s   x   [   y  /  x   ]       -     s  x  2    m  x  2     (    m  y   -   s   x   [   y  /  x   ]    2    )     ]         var  r       1     superscript   subscript  s  x   2    superscript   subscript  m  x   2      delimited-[]           superscript   subscript  s  y   2    subscript  s     superscript  x  2    delimited-[]     superscript  y  2    superscript  x  2         superscript   subscript  s    x   delimited-[]    y  x      2      2   subscript  m  y    subscript  s    x   delimited-[]    y  x             superscript   subscript  s  x   2    superscript   subscript  m  x   2       subscript  m  y    superscript   subscript  s    x   delimited-[]    y  x      2          \operatorname{var}(r)=\frac{1}{s_{x}^{2}+m_{x}^{2}}\left[(s_{y}^{2}-s_{x^{2}[y%
 ^{2}/x^{2}]})-(s_{x[y/x]})^{2}+2m_{y}s_{x[y/x]}-\frac{s_{x}^{2}}{m_{x}^{2}}(m_%
 {y}-s_{x[y/x]}^{2})\right]     where s x 2 and s y 2 are the variances of the x and y variates respectively, m x and m y are the means of the x and y variates respectively and s ab is the covariance of a and b .  Although the approximate variance estimator of the ratio given below is biased, if the sample size is large, the bias in this estimator is negligible.       var   (  r  )    =     N  -  n   N    1   m  x  2       ∑   i  =  1   n    (    y  i   -   r   x  i     )     n  -  1          var  r         N  n   N     1   superscript   subscript  m  x   2        superscript   subscript     i  1    n      subscript  y  i     r   subscript  x  i        n  1       \operatorname{var}(r)=\frac{N-n}{N}\frac{1}{m_{x}^{2}}\frac{\sum_{i=1}^{n}(y_{%
 i}-rx_{i})}{n-1}     where N is the population size, n is the sample size and m x is the mean of the x variate.  Another estimator of the variance based on the Taylor expansion is       var   (  r  )    =    1  n    (   1  -    n  -  1    N  -  1     )        r  2    s  x  2    +   s  y  2    -   2  r  ρ   s  x    s  y      m  x  2          var  r       1  n     1      n  1     N  1              superscript  r  2    superscript   subscript  s  x   2     superscript   subscript  s  y   2      2  r  ρ   subscript  s  x    subscript  s  y      superscript   subscript  m  x   2       \operatorname{var}(r)=\frac{1}{n}(1-\frac{n-1}{N-1})\frac{r^{2}s_{x}^{2}+s_{y}%
 ^{2}-2r\rho s_{x}s_{y}}{m_{x}^{2}}     where n is the sample size, N is the population size and ρ is the correlation coefficient between the x and y variates.  An estimate accurate to O( n −2 ) is 16       var   (  r  )    =    1  n    [      s  y  2    m  x  2    +     m  y  2    s  x  2     m  x  4     -    2   m  y    s   x  y      m  x  3     ]         var  r       1  n    delimited-[]         superscript   subscript  s  y   2    superscript   subscript  m  x   2         superscript   subscript  m  y   2    superscript   subscript  s  x   2     superscript   subscript  m  x   4         2   subscript  m  y    subscript  s    x  y      superscript   subscript  m  x   3         \operatorname{var}(r)=\frac{1}{n}\left[\frac{s_{y}^{2}}{m_{x}^{2}}+\frac{m_{y}%
 ^{2}s_{x}^{2}}{m_{x}^{4}}-\frac{2m_{y}s_{xy}}{m_{x}^{3}}\right]     An estimator accurate to O( n −3 ) is 17       var   (  r  )    =    r  2    [     1  n    (     1   m  x    +   1   m  y     -    2   s   x  y       m  x    m  y      )    +    1   n  2     (     6   m  x  2    +   3    m  x    m  y     +    s   x  y     [     4   m  y  2    -   8    m  x    m  y     -   16    m  x  2    m  y      +    5   s   x  y       m  x  2    m  y  2      ]    +    4   s    x  2   y       m  x  2    m  y      -    2   s   x   y  2        m  x    m  y  2      )     ]         var  r      superscript  r  2    delimited-[]        1  n         1   subscript  m  x      1   subscript  m  y         2   subscript  s    x  y        subscript  m  x    subscript  m  y           1   superscript  n  2          6   superscript   subscript  m  x   2      3     subscript  m  x    subscript  m  y        subscript  s    x  y     delimited-[]        4   superscript   subscript  m  y   2      8     subscript  m  x    subscript  m  y       16     superscript   subscript  m  x   2    subscript  m  y          5   subscript  s    x  y        superscript   subscript  m  x   2    superscript   subscript  m  y   2            4   subscript  s     superscript  x  2   y        superscript   subscript  m  x   2    subscript  m  y          2   subscript  s    x   superscript  y  2         subscript  m  x    superscript   subscript  m  y   2            \operatorname{var}(r)=r^{2}\left[\frac{1}{n}\left(\frac{1}{m_{x}}+\frac{1}{m_{%
 y}}-\frac{2s_{xy}}{m_{x}m_{y}}\right)+\frac{1}{n^{2}}\left(\frac{6}{m_{x}^{2}}%
 +\frac{3}{m_{x}m_{y}}+s_{xy}\left[\frac{4}{m_{y}^{2}}-\frac{8}{m_{x}m_{y}}-%
 \frac{16}{m_{x}^{2}m_{y}}+\frac{5s_{xy}}{m_{x}^{2}m_{y}^{2}}\right]+\frac{4s_{%
 x^{2}y}}{m_{x}^{2}m_{y}}-\frac{2s_{xy^{2}}}{m_{x}m_{y}^{2}}\right)\right]     A jackknife estimator of the variance is       var   (  r  )    =    1   n   (   n  -  1   )       ∑   i  ≠  j   n     (    r  i   -   r  J    )   2          var  r       1    n    n  1       superscript   subscript     i  j    n    superscript     subscript  r  i    subscript  r  J    2       \operatorname{var}(r)=\frac{1}{n(n-1)}\sum_{i\neq j}^{n}(r_{i}-r_{J})^{2}     where r i is the ratio with the i th pair of variates omitted and r J is the jackknife estimate of the ratio. 18  Variance of total  The variance of the estimated total is       var   (   τ  y   )    =    τ  y  2    var   (  r  )          var   subscript  τ  y       superscript   subscript  τ  y   2    var  r      \operatorname{var}(\tau_{y})=\tau_{y}^{2}\operatorname{var}(r)     Variance of mean  The variance of the estimated mean of the y variate is       var   (   y  ¯   )    =    m  x  2    var   (  r  )     =     N  -  n   N    1   m  x  2       ∑   i  =  1   n    (    y  i   -   r   x  i     )     n  -  1     =     N  -  n   N     (     s  y  2   +    r  2    s  x  2     -   2  r  ρ   s  x    s  y     )   n           var   normal-¯  y       superscript   subscript  m  x   2    var  r               N  n   N     1   superscript   subscript  m  x   2        superscript   subscript     i  1    n      subscript  y  i     r   subscript  x  i        n  1                N  n   N          superscript   subscript  s  y   2      superscript  r  2    superscript   subscript  s  x   2       2  r  ρ   subscript  s  x    subscript  s  y     n       \operatorname{var}(\bar{y})=m_{x}^{2}\operatorname{var}(r)=\frac{N-n}{N}\frac{%
 1}{m_{x}^{2}}\frac{\sum_{i=1}^{n}(y_{i}-rx_{i})}{n-1}\par
 =\frac{N-n}{N}\frac{%
 (s_{y}^{2}+r^{2}s_{x}^{2}-2r\rho s_{x}s_{y})}{n}     where m x is the mean of the x variate, s x 2 and s y 2 are the sample variances of the x and y variates respectively and ρ is the sample correlation between the x and y variates.  Skewness  The skewness and the kurtosis of the ratio depend on the distributions of the x and y variates. Estimates have been made of these parameters for normally distributed  x and y variates but for other distributions no expressions have yet been derived. It has been found that in general ratio variables are skewed to the right, are leptokurtic and their nonnormality is increased when magnitude of the denominator's coefficient of variation is increased.  For normally distributed x and y variates the skewness of the ratio is approximately 19      γ  =    (     m  y   ω      n   m  x    m  y    ω  2    +    m  x  2    m  y       )    (   6  +    1   n   m  x      [   44  +   1   1  +     ω  2    m  y    /   m  x       ]     )        γ         subscript  m  y   ω         n   subscript  m  x    subscript  m  y    superscript  ω  2       superscript   subscript  m  x   2    subscript  m  y         6      1    n   subscript  m  x      delimited-[]    44    1    1       superscript  ω  2    subscript  m  y     subscript  m  x             \gamma=\left(\frac{m_{y}\omega}{\sqrt{nm_{x}m_{y}\omega^{2}+m_{x}^{2}m_{y}}}%
 \right)\left(6+\frac{1}{nm_{x}}\left[44+\frac{1}{1+\omega^{2}m_{y}/m_{x}}%
 \right]\right)     where      ω  =   1  -    m  x    cov   (  x  ,  y  )          ω    1     subscript  m  x    cov  x  y       \omega=1-m_{x}\operatorname{cov}(x,y)\,     Effect on confidence intervals  Because the ratio estimate is generally skewed confidence intervals created with the variance and symmetrical tests such as the t test are incorrect. 20 These confidence intervals tend to overestimate the size of the left confidence interval and underestimate the size of the right.  If the ratio estimator is unimodal (which is frequently the case) then a conservative estimate of the 95% confidence intervals can be made with the Vysochanskiï–Petunin inequality .  Alternative methods of bias reduction  An alternative method of reducing or eliminating the bias in the ratio estimator is to alter the method of sampling. The variance of the ratio using these methods differs from the estimates given previously.  Lahiri's method  Lahiri introduced the first of these sampling schemes in 1951. 21  Choose a number M ≥ max( x 1 , ..., x N ) where N is the population size. Chose one of these elements ( x i ). Chose u at random from a uniform distribution  U (0, 1). If uM ≤ x i , then x i is retained in the sample. If not then it is rejected and a new element is chosen. Repeat this process N times. The same process is carried out with the y variate. Then the ratio of the sum of the y variates and the sum of the x variates chosen in this fashion is an unbiased estimate of the ratio estimator.  In symbols we have      r  =    ∑   y  i     ∑   x  i         r       subscript  y  i       subscript  x  i       r=\frac{\sum y_{i}}{\sum x_{i}}     where x i and y i are chosen according to the scheme described above.  Midzuno-Sen's method  In 1952 Midzuno and Sen independently described a sampling scheme that provides an unbiased estimator of the ratio. 22 23  The first sample is chosen with probability proportional to the size of the x variate. The remaining n - 1 samples are chosen at random without replacement from the remaining N - 1 members in the population. The probability of selection under this scheme is      P  =    ∑   x  i      (       N  -  1        n  -  1       )   X        P       subscript  x  i       binomial    N  1     n  1    X      P=\frac{\sum x_{i}}{{N-1\choose n-1}X}     where X is the sum of the N  x variates and the x i are the n members of the sample.  The ratio estimator given by this scheme is unbiased.  Ordinary least squares regression  If a linear relationship between the x and y variates exists and the regression equation passes through the origin then the estimated variance of the regression equation is always less than that of the ratio estimator. The precise relationship between the variances depends on the linearity of the relationship between the x and y variates: when the relationship is other than linear the ratio estimate may have a lower variance than that estimated by regression.  Uses  Although the ratio estimator may be of use in a number of settings it is of particular use in two cases:   when the variates x and y are highly correlated through the origin  when the total population size is unknown   History  The first known use of the ratio estimator was by John Graunt in England who in 1662 was the first to estimate the ratio y / x where y represented the total population and x the known total number of registered births in the same areas during the preceding year.  Later Messance (~1765) and Moheau (1778) published very carefully prepared estimates for France based on enumeration of population in certain districts and on the count of births, deaths and marriages as reported for the whole country. The districts from which the ratio of inhabitants to birth was determined only constituted a sample.  In 1802, Laplace wished to estimate the population of France. No population census had been carried out and Laplace lacked the resources to count every individual. Instead he sampled 30 parishes whose total number of inhabitants was 2,037,615. The parish baptismal registrations were considered to be reliable estimates of the number of live births so he used the total number of births over a three-year period. The sample estimate was 71,866.333 baptisms per year over this period giving a ratio of one registered baptism for every 28.35 persons. The total number of baptismal registrations for France was also available to him and he assumed that the ratio of live births to population was constant. He then used the ratio from his sample to estimate the population of France.  Karl Pearson said in 1897 that the ratio estimates are biased and cautioned against their use. 24  See also   Ratio distribution   References  "  Category:Statistical deviation and dispersion  Category:Articles containing proofs  Category:Data analysis  Category:Statistical ratios     Scott AJ, Wu CFJ (1981) On the asymptotic distribution of ratio and regression estimators. JASA 76: 98–102 ↩  Cochran WG (1977) Sampling techniques. New York: John Wiley & Sons ↩  Hartley HO, Ross A (1954) Unbiased ratio estimators. Nature 174: 270–271 ↩  Ogliore RC, Huss GR, Nagashima K (2011) Ratio estimation in SIMS analysis. Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms 269 (17) 1910–1918 ↩  Pascual JN (1961) Unbiased ratio estimators in stratified sampling. JASA 56(293):70–87 ↩  Beale EML (1962) Some use of computers in operational research. Industrielle Organization 31: 27-28 ↩  Tin M (1965) Comparison of some ratio estimators. JASA 60: 294–307 ↩  Sahoo LN (1983). On a method of bias reduction in ratio estimation. J Statist Res 17:1—6 ↩  Sahoo LN (1987) On a class of almost unbiased estimators for population ratio. Statistics 18: 119-121 ↩  Ogliore RC, Huss GR, Nagashima K (2011) Ratio estimation in SIMS analysis. Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms 269 (17) 1910–1918 ↩  van Kempen GMP, van Vliet LJ (2000) Mean and variance of ratio estimators used in fluorescence ratio imaging. Cytometry 39:300–305 ↩  Choquet D, L'ecuyer P, Léger C (1999) Bootstrap confidence intervals for ratios of expectations. ACM Transactions on Modeling and Computer Simulation - TOMACS 9 (4) 326-348 DOI: 10.1145/352222.352224 ↩  Durbin J (1959) A note on the application of Quenouille's method of bias reduction to estimation of ratios. Biometrika 46: 477-480 ↩  Mickey MR (1959) Some finite population unbiased ratio and regression estimators. JASA 54: 596–612 ↩  Choquet D, l'Ecuyer P, Leger C(1999) Bootstrap confidence intervals for ratios of expectations. ↩  van Kempen GMP, van Vliet LJ (2000) Mean and variance of ratio estimators used in fluorescence ratio imaging. Cytometry 39:300–305 ↩  Ogliore RC, Huss GR, Nagashima K (2011) Ratio estimation in SIMS analysis. Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms 269 (17) 1910–1918 ↩  Choquet D, L'ecuyer P, Léger C (1999) Bootstrap confidence intervals for ratios of expectations. ACM Transactions on Modeling and Computer Simulation - TOMACS 9 (4) 326-348 DOI: 10.1145/352222.352224 ↩  Tin M (1965) Comparison of some ratio estimators. JASA 60 (309) 294–307 ↩  Choquet D, l'Ecuyer P, Leger C(1999) Bootstrap confidence intervals for ratios of expectations. ↩  Lahiri DB (1951) A method of sample selection providing unbiased ratio estimates. Bull Int Stat Inst 33: 133–140 ↩  Midzuno H (1952) On the sampling system with probability proportional to the sum of the sizes. Ann Inst Stat Math 3: 99-107 ↩  Sen AR (1952) Present status of probability sampling and its use in the estimation of a characteristic. Econometrika 20-103 ↩  Pearson K (1897) On a form of spurious correlation that may arise when indices are used for the measurement of organs. Proc Roy Soc Lond 60: 498 ↩     