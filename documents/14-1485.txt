   EigenMoments      EigenMoments    EigenMoments 1 is a set of orthogonal , noise robust, invariant to rotation, scaling and translation and distribution sensitive moments . Their application can be found in signal processing and computer vision as descriptors of the signal or image. The descriptors can later be used for classification purposes.  It is obtained by performing orthogonalization , via eigen analysis on geometric moments . 2  Framework summary  EigenMoments are computed by performing eigen analysis on the moment space of an image by maximizing Signal to Noise Ratio in the feature space in form of Rayleigh quotient .  This approach has several benefits in Image processing applications:   Dependency of moments in the moment space on the distribution of the images being transformed, ensures decorrelation of the final feature space after eigen analysis on the moment space.  The ability of EigenMoments to take into account distribution of the image makes it more versatile and adaptable for different genres.  Generated moment kernels are orthogonal and therefore analysis on the moment space becomes easier. Transformation with orthogonal moment kernels into moment space is analogous to projection of the image onto a number of orthogonal axes.  Nosiy components can be removed. This makes EigenMoments robust for classification applications.  Optimal information compaction can be obtained and therefore a few number of moments are needed to characterize the images.   Problem formulation  Assume that a signal vector    s  âˆˆ   â„›  n       s   superscript  â„›  n     s\in\mathcal{R}^{n}   is taken from a certain distribution having coorelation    C  âˆˆ   â„›   n  Ã—  n        C   superscript  â„›    n  n      C\in\mathcal{R}^{n\times n}   ,i.e.    C  =   E   [   s   s  T    ]        C    E   delimited-[]    s   superscript  s  T        C=E[ss^{T}]   where E[.] denotes expected value.  Dimension of signal space, n, is often too large to be useful for practical application such as pattern classification, we need to transform the signal space into a space with lower dimensionality.  This is performed by a two-step linear transformation:       q  =    W  T    X  T   s    ,      q     superscript  W  T    superscript  X  T   s     q=W^{T}X^{T}s,     where    q  =    [   q  1   ,  â€¦  ,   q  n   ]   T   âˆˆ   â„›  k         q   superscript    subscript  q  1   normal-â€¦   subscript  q  n    T         superscript  â„›  k      q=[q_{1},...,q_{n}]^{T}\in\mathcal{R}^{k}   is the transformed signal,    X  =    [   x  1   ,  â€¦  ,   x  n   ]   T   âˆˆ   â„›   n  Ã—  m          X   superscript    subscript  x  1   normal-â€¦   subscript  x  n    T         superscript  â„›    n  m       X=[x_{1},...,x_{n}]^{T}\in\mathcal{R}^{n\times m}   a fixed transformation matrix which transforms the signal into the moment space, and    W  =    [   w  1   ,  â€¦  ,   w  n   ]   T   âˆˆ   â„›   m  Ã—  k          W   superscript    subscript  w  1   normal-â€¦   subscript  w  n    T         superscript  â„›    m  k       W=[w_{1},...,w_{n}]^{T}\in\mathcal{R}^{m\times k}   the transformation matrix which we are going to determine by maximizing the SNR of the feature space resided by   q   q   q   . For the case of Geometric Moments, X would be the monomials. If    m  =  k  =  n        m  k       n     m=k=n   , a full rank transformation would result, however usually we have    m  â‰¤  n      m  n    m\leq n   and    k  â‰¤  m      k  m    k\leq m   . This is specially the case when   n   n   n   is of high dimensions.  Finding   W   W   W   that maximizes the SNR of the feature space:        S  N   R   t  r  a  n  s  f  o  r  m     =     w  T    X  T   C  X  w     w  T    X  T   N  X  w     ,        S  N   subscript  R    t  r  a  n  s  f  o  r  m          superscript  w  T    superscript  X  T   C  X  w      superscript  w  T    superscript  X  T   N  X  w      SNR_{transform}=\frac{w^{T}X^{T}CXw}{w^{T}X^{T}NXw},     where N is the correlation matrix of the noise signal. The problem can thus be formulated as        w  1   ,  â€¦  ,   w  k    =   a  r  g  m  a   x  w      w  T    X  T   C  X  w     w  T    X  T   N  X  w           subscript  w  1   normal-â€¦   subscript  w  k      a  r  g  m  a   subscript  x  w        superscript  w  T    superscript  X  T   C  X  w      superscript  w  T    superscript  X  T   N  X  w       {w_{1},...,w_{k}}=argmax_{w}\frac{w^{T}X^{T}CXw}{w^{T}X^{T}NXw}     subject to constraints:         w  i  T    X  T   N  X   w  j    =   Î´   i  j     ,         superscript   subscript  w  i   T    superscript  X  T   N  X   subscript  w  j     subscript  Î´    i  j      w_{i}^{T}X^{T}NXw_{j}=\delta_{ij},   where    Î´   i  j      subscript  Î´    i  j     \delta_{ij}   is the Kronecker delta .  It can be observed that this maximization is Rayleigh quotient by letting    A  =    X  T   C  X       A     superscript  X  T   C  X     A=X^{T}CX   and    B  =    X  T   N  X       B     superscript  X  T   N  X     B=X^{T}NX   and therefore can be written as:        w  1   ,  â€¦  ,   w  k    =      arg   max   ð‘¥      w  T   A  w     w  T   B  w           subscript  w  1   normal-â€¦   subscript  w  k       x    arg  max         superscript  w  T   A  w      superscript  w  T   B  w       {w_{1},...,w_{k}}=\underset{x}{\operatorname{arg\,max}}\frac{w^{T}Aw}{w^{T}Bw}   ,      w  i  T   B   w  j    =   Î´   i  j           superscript   subscript  w  i   T   B   subscript  w  j     subscript  Î´    i  j      w_{i}^{T}Bw_{j}=\delta_{ij}     Rayleigh quotient  Optimization of Rayleigh quotient 3 4 has the form:         max  w   R    (  w  )    =    max  w      w  T   A  w     w  T   B  w             subscript   w   R   w     subscript   w        superscript  w  T   A  w      superscript  w  T   B  w       \max_{w}R(w)=\max_{w}\frac{w^{T}Aw}{w^{T}Bw}     and   A   A   A   and   B   B   B   , both are symmetric and   B   B   B   is positive definite and therefore invertibale . scaling   w   w   w   does not change the value of the object function and hence and additional scalar constraint      w  T   B  w   =  1         superscript  w  T   B  w   1    w^{T}Bw=1   can be imposed on   w   w   w   and no solution would be lost when the objective function is optimized.  This constraint optimization problem can be solved using Lagrangian multiplier :       max  w     w  T   A  w       subscript   w      superscript  w  T   A  w     \max_{w}{w^{T}Aw}   subject to      w  T   B  w   =  1         superscript  w  T   B  w   1    {w^{T}Bw}=1          max  w   â„’    (  w  )    =    max  w    (    w  T  A  w   -   Î»   w  T   B  w    )            subscript   w   â„’   w     subscript   w       w  T  A  w     Î»   superscript  w  T   B  w       \max_{w}\mathcal{L}(w)=\max_{w}(w{T}Aw-\lambda w^{T}Bw)     equating first derivative to zero and we will have:       A  w   =   Î»  B  w         A  w     Î»  B  w     Aw=\lambda Bw     which is an instance of Generelized Eigenvalue Problem (GEP). The GEP has the form:       A  w   =   Î»  B  w         A  w     Î»  B  w     Aw=\lambda Bw     for any pair    (  w  ,  Î»  )     w  Î»    (w,\lambda)   that is a solution to above equation,   w   w   w   is called a generalized eigenvector and   Î»   Î»   \lambda   is called a generalized eigenvalue .  Finding   w   w   w   and   Î»   Î»   \lambda   that satisfies this equations would produce the result which optimizes Rayleigh quotient .  One way of maximizing Rayleigh quotient is through solving the Generalized Eigen Problem . Dimension reduction can be performed by simply choosing the first components    w  i     subscript  w  i    w_{i}   ,    i  =   1  ,  â€¦  ,  k       i   1  normal-â€¦  k     i=1,...,k   , with the highest values for    R   (  w  )       R  w    R(w)   out of the   m   m   m   components, and discard the rest. Interpretation of this transformation is rotating and scaling the moment space, transforming it into a feature space with maximized SNR and therefore, the first   k   k   k   components are the components with highest   k   k   k    SNR values.  The other method to look at this solution is to use the concept of simultaneous diagonalization instead of Generalized Eigen Problem .  Simultaneous diagonalization   Let    A  =    X  T   C  X       A     superscript  X  T   C  X     A=X^{T}CX   and    B  =    X  T   N  X       B     superscript  X  T   N  X     B=X^{T}NX   as mentioned earlier. We can write   W   W   W   as two separate transformation matrices:        W  =    W  1    W  2     .      W     subscript  W  1    subscript  W  2      W=W_{1}W_{2}.          W  1     subscript  W  1    W_{1}   can be found by first diagonalize B:         P  T   B  P   =   D  B          superscript  P  T   B  P    subscript  D  B     P^{T}BP=D_{B}   .  Where    D  B     subscript  D  B    D_{B}   is a diagonal matrix sorted in increasing order. Since   B   B   B   is positive definite, thus     D  B   >  0       subscript  D  B   0    D_{B}>0   . We can discard those eigenvalues that large and retain those close to 0, since this means the energy of the noise is close to 0 in this space, at this stage it is also possible to discard those eigenvectors that have large eigenvalues .  Let    P  ^     normal-^  P    \hat{P}   be the first   k   k   k   columns of   P   P   P   , now       P  T   ^   B   P  ^    =    D  B   ^          normal-^   superscript  P  T    B   normal-^  P     normal-^   subscript  D  B      \hat{P^{T}}B\hat{P}=\hat{D_{B}}   where     D  B   ^     normal-^   subscript  D  B     \hat{D_{B}}   is the    k  Ã—  k      k  k    k\times k   principal submatrix of    D  B     subscript  D  B    D_{B}   .   Let        W  1   =    P  ^      D  B   ^    -   1  /  2           subscript  W  1      normal-^  P    superscript   normal-^   subscript  D  B        1  2        W_{1}=\hat{P}\hat{D_{B}}^{-1/2}     and hence:        W  1  T   B   W  1    =     (    P  ^      D  B   ^    -   1  /  2      )   T   B   (    P  ^      D  B   ^    -   1  /  2      )    =  I           superscript   subscript  W  1   T   B   subscript  W  1       superscript     normal-^  P    superscript   normal-^   subscript  D  B        1  2      T   B     normal-^  P    superscript   normal-^   subscript  D  B        1  2            I     W_{1}^{T}BW_{1}=(\hat{P}\hat{D_{B}}^{-1/2})^{T}B(\hat{P}\hat{D_{B}}^{-1/2})=I   .      W  1     subscript  W  1    W_{1}   whiten   B   B   B   and reduces the dimensionality from   m   m   m   to   k   k   k   . The transformed space resided by     q  â€²   =    W  1  T    X  T   s        superscript  q  normal-â€²      superscript   subscript  W  1   T    superscript  X  T   s     q^{\prime}=W_{1}^{T}X^{T}s   is called the noise space.   Then, we diagonalize      W  1  T   A   W  1        superscript   subscript  W  1   T   A   subscript  W  1     W_{1}^{T}AW_{1}   :         W  2  T    W  1  T   A   W  1    W  2    =   D  A          superscript   subscript  W  2   T    superscript   subscript  W  1   T   A   subscript  W  1    subscript  W  2     subscript  D  A     W_{2}^{T}W_{1}^{T}AW_{1}W_{2}=D_{A}   ,  where      W  2  T    W  2    =  I         superscript   subscript  W  2   T    subscript  W  2    I    W_{2}^{T}W_{2}=I   .    D  A     subscript  D  A    D_{A}   is the matrix with eigenvalues of     W  1  T   A   W  1        superscript   subscript  W  1   T   A   subscript  W  1     W_{1}^{T}AW_{1}   on its diagonal. We may retain all the eigenvalues and their corresponding eigenvectors since the most of the noise are already discarded in previous step.   Finally the transformation is given by:       W  =    W  1    W  2        W     subscript  W  1    subscript  W  2      W=W_{1}W_{2}     where   W   W   W    diagonalizes both the numerator and denominator of the SNR ,        W  T   A  W   =   D  A          superscript  W  T   A  W    subscript  D  A     W^{T}AW=D_{A}   ,      W  T   B  W   =  I         superscript  W  T   B  W   I    W^{T}BW=I   and the transformation of signal   s   s   s   is defined as    q  =    W  T    X  T   s   =    W  2  T    W  1  T    X  T   s         q     superscript  W  T    superscript  X  T   s           superscript   subscript  W  2   T    superscript   subscript  W  1   T    superscript  X  T   s      q=W^{T}X^{T}s=W_{2}^{T}W_{1}^{T}X^{T}s   .  Information loss  To find the information loss when we discard some of the eigenvalues and eigenvectors we can perform following analysis:        Î·    =     1  -    t  r  a  c  e   (    W  1  T   A   W  1    )     t  r  a  c  e   (    D  B   -   1  /  2      P  T   A  P   D  B   -   1  /  2      )           =     1  -    t  r  a  c  e   (      D  B   ^    -   1  /  2       P  ^   T   A   P  ^      D  B   ^    -   1  /  2      )     t  r  a  c  e   (    D  B   -   1  /  2      P  T   A  P   D  B   -   1  /  2      )            Î·     1      t  r  a  c  e     superscript   subscript  W  1   T   A   subscript  W  1       t  r  a  c  e     superscript   subscript  D  B       1  2      superscript  P  T   A  P   superscript   subscript  D  B       1  2            missing-subexpression      1      t  r  a  c  e     superscript   normal-^   subscript  D  B        1  2      superscript   normal-^  P   T   A   normal-^  P    superscript   normal-^   subscript  D  B        1  2         t  r  a  c  e     superscript   subscript  D  B       1  2      superscript  P  T   A  P   superscript   subscript  D  B       1  2            \begin{array}[]{lll}\eta&=&1-\frac{trace(W_{1}^{T}AW_{1})}{trace(D_{B}^{-1/2}P%
 ^{T}APD_{B}^{-1/2})}\\
 &=&1-\frac{trace(\hat{D_{B}}^{-1/2}\hat{P}^{T}A\hat{P}\hat{D_{B}}^{-1/2})}{%
 trace(D_{B}^{-1/2}P^{T}APD_{B}^{-1/2})}\par
 \end{array}     Eigenmoments  Eigenmoments are derived by applying the above framework on Geometric Moments. They can be derived for both 1D and 2D signals.  1D signal  If we let    X  =   [  1  ,  x  ,   x  2   ,  â€¦  ,   x   m  -  1    ]       X   1  x   superscript  x  2   normal-â€¦   superscript  x    m  1       X=[1,x,x^{2},...,x^{m-1}]   , i.e. the monomials , after the transformation    X  T     superscript  X  T    X^{T}   we obtain Geometric Moments, denoted by vector   M   M   M   , of signal    s  =   [   s   (  x  )    ]       s   delimited-[]    s  x      s=[s(x)]   ,i.e.    M  =    X  T   s       M     superscript  X  T   s     M=X^{T}s   .  In practice it is difficult to estimate the correlation signal due to insufficient number of samples, therefore parametric approaches are utilized.  One such model can be defined as:       r   (   x  1   ,   x  2   )    =   r   (  0  ,  0  )    e   -   c    (    x  1   -   x  2    )   2             r    subscript  x  1    subscript  x  2       r   0  0    superscript  e      c   superscript     subscript  x  1    subscript  x  2    2         r(x_{1},x_{2})=r(0,0)e^{-c(x_{1}-x_{2})^{2}}   ,  (Figure)  Plot of the parametric model which predicts correlations in the input signal.     r   (   x  1   ,   x  2   )    =   r   (  0  ,  0  )    e   -   c    (    x  1   -   x  2    )   2             r    subscript  x  1    subscript  x  2       r   0  0    superscript  e      c   superscript     subscript  x  1    subscript  x  2    2         r(x_{1},x_{2})=r(0,0)e^{-c(x_{1}-x_{2})^{2}}      where     r   (  0  ,  0  )    =   E   [   t  r   (   s   s  T    )    ]          r   0  0      E   delimited-[]    t  r    s   superscript  s  T         r(0,0)=E[tr(ss^{T})]   . This model of correlation can be replaced by other models however this model covers general natural images.  Since    r   (  0  ,  0  )       r   0  0     r(0,0)   does not affect the maximization it can be dropped.      A  =    X  T   C  X   =    âˆ«   -  1   1     âˆ«   -  1   1      [    x  1  j    x  2  i    e   -   c    (    x  1   -   x  2    )   2       ]     i  ,  j   =  0     i  ,  j   =   m  -  1     d   x  1   d   x  2            A     superscript  X  T   C  X          superscript   subscript     1    1     superscript   subscript     1    1      superscript   subscript   delimited-[]     superscript   subscript  x  1   j    superscript   subscript  x  2   i    superscript  e      c   superscript     subscript  x  1    subscript  x  2    2           i  j   0       i  j     m  1     d   subscript  x  1   d   subscript  x  2         A=X^{T}CX=\int_{-1}^{1}\int_{-1}^{1}[x_{1}^{j}x_{2}^{i}e^{-c(x_{1}-x_{2})^{2}}%
 ]_{i,j=0}^{i,j=m-1}dx_{1}dx_{2}     The correlation of noise can be modelled as     Ïƒ  n  2   Î´   (   x  1   ,   x  2   )        superscript   subscript  Ïƒ  n   2   Î´    subscript  x  1    subscript  x  2      \sigma_{n}^{2}\delta(x_{1},x_{2})   , where    Ïƒ  n  2     superscript   subscript  Ïƒ  n   2    \sigma_{n}^{2}   is the energy of noise.Again    Ïƒ  n  2     superscript   subscript  Ïƒ  n   2    \sigma_{n}^{2}   can be dropped because the constant does not have any effect on the maximization problem.      B  =    X  T   N  X   =    âˆ«   -  1   1     âˆ«   -  1   1      [    x  1  j    x  2  i   Î´   (   x  1   ,   x  2   )    ]     i  ,  j   =  0     i  ,  j   =   m  -  1     d   x  1   d   x  2            B     superscript  X  T   N  X          superscript   subscript     1    1     superscript   subscript     1    1      superscript   subscript   delimited-[]     superscript   subscript  x  1   j    superscript   subscript  x  2   i   Î´    subscript  x  1    subscript  x  2         i  j   0       i  j     m  1     d   subscript  x  1   d   subscript  x  2         B=X^{T}NX=\int_{-1}^{1}\int_{-1}^{1}[x_{1}^{j}x_{2}^{i}\delta(x_{1},x_{2})]_{i%
 ,j=0}^{i,j=m-1}dx_{1}dx_{2}       B  =    X  T   N  X   =    âˆ«   -  1   1      [   x  1   j  +  i    ]     i  ,  j   =  0     i  ,  j   =   m  -  1     d   x  1     =    X  T   X         B     superscript  X  T   N  X          superscript   subscript     1    1      superscript   subscript   delimited-[]   superscript   subscript  x  1     j  i        i  j   0       i  j     m  1     d   subscript  x  1             superscript  X  T   X      B=X^{T}NX=\int_{-1}^{1}[x_{1}^{j+i}]_{i,j=0}^{i,j=m-1}dx_{1}=X^{T}X     Using the computed A and B and applying the algorithm discussed in previous section we find   W   W   W   and set of transformed monomials     Î¦  =   [   Ï•  1   ,  â€¦  ,   Ï•  k   ]   =   X  W         normal-Î¦    subscript  Ï•  1   normal-â€¦   subscript  Ï•  k           X  W      \Phi=[\phi_{1},...,\phi_{k}]=XW   which produces the moment kernels of EM. The moment kernels of EM decorrelate the correlation in the image.        Î¦  T   C  Î¦   =     (   X  W   )   T   C   (   X  W   )    =   D  C            superscript  normal-Î¦  T   C  normal-Î¦      superscript    X  W   T   C    X  W          subscript  D  C      \Phi^{T}C\Phi=(XW)^{T}C(XW)=D_{C}   ,  and are orthogonal:          Î¦  T   Î¦     =       (   X  W   )   T    (   X  W   )         =      W  T    X  T   X        =      W  T    X  T   N  X  W        =      W  T   B  W        =    I           superscript  normal-Î¦  T   normal-Î¦       superscript    X  W   T     X  W       missing-subexpression       superscript  W  T    superscript  X  T   X      missing-subexpression       superscript  W  T    superscript  X  T   N  X  W      missing-subexpression       superscript  W  T   B  W      missing-subexpression    I     \begin{array}[]{lll}\Phi^{T}\Phi&=&(XW)^{T}(XW)\\
 &=&W^{T}X^{T}X\\
 &=&W^{T}X^{T}NXW\\
 &=&W^{T}BW\\
 &=&I\\
 \end{array}     Example computation  Taking    c  =  0.5      c  0.5    c=0.5   , the dimension of moment space as    m  =  6      m  6    m=6   and the dimension of feature space as    k  =  4      k  4    k=4   , we will have:      W  =   (     0.0    0     -  0.7745      -  0.8960       2.8669     -  4.4622     0.0    0.0      0.0    0.0    7.9272    2.4523       -  4.0225     20.6505    0.0    0.0      0.0    0.0     -  9.2789      -  0.1239        -  0.5092      -  18.4582     0.0    0.0     )       W    0.0  0    0.7745     0.8960     2.8669    4.4622   0.0  0.0    0.0  0.0  7.9272  2.4523      4.0225   20.6505  0.0  0.0    0.0  0.0    9.2789     0.1239       0.5092     18.4582   0.0  0.0      W=\left(\begin{array}[]{cccc}0.0&0&-0.7745&-0.8960\\
 2.8669&-4.4622&0.0&0.0\\
 0.0&0.0&7.9272&2.4523\\
 -4.0225&20.6505&0.0&0.0\\
 0.0&0.0&-9.2789&-0.1239\\
 -0.5092&-18.4582&0.0&0.0\end{array}\right)     and         Ï•  1     =      2.8669  x   -   4.0225   x  3    -   0.5092   x  5          Ï•  2     =       -   4.4622  x    +   20.6505   x  3     -   18.4582   x  5          Ï•  3     =       -  0.7745   +   7.9272   x  2     -   9.2789   x  4          Ï•  4     =       -  0.8960   +   2.4523   x  2     -   0.1239   x  4            subscript  Ï•  1        2.8669  x     4.0225   superscript  x  3      0.5092   superscript  x  5        subscript  Ï•  2            4.4622  x      20.6505   superscript  x  3       18.4582   superscript  x  5        subscript  Ï•  3          0.7745     7.9272   superscript  x  2       9.2789   superscript  x  4        subscript  Ï•  4          0.8960     2.4523   superscript  x  2       0.1239   superscript  x  4        \begin{array}[]{lll}\phi_{1}&=&2.8669x-4.0225x^{3}-0.5092x^{5}\\
 \phi_{2}&=&-4.4622x+20.6505x^{3}-18.4582x^{5}\\
 \phi_{3}&=&-0.7745+7.9272x^{2}-9.2789x^{4}\\
 \phi_{4}&=&-0.8960+2.4523x^{2}-0.1239x^{4}\\
 \end{array}     2D signal  The derivation for 2D signal is the same as 1D signal except that conventional Geometric Moments are directly employed to obtain the set of 2D EigenMoments.  The definition of Geometric Moments of order    (   p  +  q   )      p  q    (p+q)   for 2D image signal is:       m   p  q    =    âˆ«   -  1   1     âˆ«   -  1   1     x  p    y  q   f   (  x  ,  y  )   d  x  d  y          subscript  m    p  q      superscript   subscript     1    1     superscript   subscript     1    1      superscript  x  p    superscript  y  q   f   x  y   d  x  d  y       m_{pq}=\int_{-1}^{1}\int_{-1}^{1}x^{p}y^{q}f(x,y)dxdy   .  which can be denoted as    M  =    {   m   j  ,  i    }     i  ,  j   =  0     i  ,  j   =   m  -  1         M   superscript   subscript    subscript  m   j  i        i  j   0       i  j     m  1       M=\{m_{j,i}\}_{i,j=0}^{i,j=m-1}   . Then the set of 2D EigenMoments are:      Î©  =    W  T   M  W       normal-Î©     superscript  W  T   M  W     \Omega=W^{T}MW   ,  where    Î©  =    {   Î©   j  ,  i    }     i  ,  j   =  0     i  ,  j   =   k  -  1         normal-Î©   superscript   subscript    subscript  normal-Î©   j  i        i  j   0       i  j     k  1       \Omega=\{\Omega_{j,i}\}_{i,j=0}^{i,j=k-1}   is a matrix that contains the set of EigenMoments.       Î©   j  ,  i    =    Î£   r  =  0    m  -  1     Î£   s  =  0    m  -  1     w   r  ,  j     w   s  ,  i     m   r  ,  s          subscript  normal-Î©   j  i       superscript   subscript  normal-Î£    r  0      m  1     superscript   subscript  normal-Î£    s  0      m  1     subscript  w   r  j     subscript  w   s  i     subscript  m   r  s       \Omega_{j,i}=\Sigma_{r=0}^{m-1}\Sigma_{s=0}^{m-1}w_{r,j}w_{s,i}m_{r,s}   .  EigenMoment invariants (EMI)  In order to obtain a set of moment invariants we can use normalized Geometric Moments     M  ^     normal-^  M    \hat{M}   instead of   M   M   M   .  Normalized Geometric Moments are invariant to Rotation,Scaling and Transformation and defined by:          m  ^    p  q      =      Î±  p   +  q  +   2    âˆ«   -  1   1     âˆ«   -  1   1     [     (   x  -   x  c    )   c  o  s   (  Î¸  )    +    (   y  -   y  c    )   s  i  n   (  Î¸  )     ]   p            =      Ã—    [    -    (   x  -   x  c    )   s  i  n   (  Î¸  )     +    (   y  -   y  c    )   c  o  s   (  Î¸  )     ]   q         =       Ã—   f   (  x  ,  y  )   d  x  d  y    ,          subscript   normal-^  m     p  q        superscript  Î±  p   q    2    superscript   subscript     1    1     superscript   subscript     1    1    superscript   delimited-[]        x   superscript  x  c    c  o  s  Î¸       y   superscript  y  c    s  i  n  Î¸     p          missing-subexpression      absent   superscript   delimited-[]          x   superscript  x  c    s  i  n  Î¸        y   superscript  y  c    c  o  s  Î¸     q       missing-subexpression      absent    f   x  y   d  x  d  y       \begin{array}[]{lll}\hat{m}_{pq}&=&\alpha^{p}+q+2\int_{-1}^{1}\int_{-1}^{1}[(x%
 -x^{c})cos(\theta)+(y-y^{c})sin(\theta)]^{p}\\
 &=&\times[-(x-x^{c})sin(\theta)+(y-y^{c})cos(\theta)]^{q}\\
 &=&\times f(x,y)dxdy,\\
 \end{array}     where     (   x  c   ,   y  c   )   =   (    m  10   /   m  00    ,    m  01   /   m  00    )         superscript  x  c    superscript  y  c        subscript  m  10    subscript  m  00       subscript  m  01    subscript  m  00       (x^{c},y^{c})=(m_{10}/m_{00},m_{01}/m_{00})   is the centroid of the image    f   (  x  ,  y  )       f   x  y     f(x,y)   and        Î±    =      [    m  00  S   /   m  00    ]    1  /  2        Î¸    =      1  2   t  a   n   -  1      2   m  11      m  20   -   m  02            Î±    superscript   delimited-[]     superscript   subscript  m  00   S    subscript  m  00       1  2      Î¸       1  2   t  a   superscript  n    1        2   subscript  m  11       subscript  m  20    subscript  m  02         \begin{array}[]{lll}\alpha&=&[m_{00}^{S}/m_{00}]^{1/2}\\
 \theta&=&\frac{1}{2}tan^{-1}\frac{2m_{11}}{m_{20}-m_{02}}\end{array}   .      m  00  S     superscript   subscript  m  00   S    m_{00}^{S}   in this equation is a scaling factor depending on the image.    m  00  S     superscript   subscript  m  00   S    m_{00}^{S}   is usually set to 1 for binary images.  See also   Computer Vision  Signal Processing  Image moment   References  External links   implementation of EigenMoments in Matlab   "  Category:Signal processing  Category:Computer vision  Category:Articles created via the Article Wizard     Pew-Thian Yap, Raveendran Paramesran, Eigenmoments, Pattern Recognition, Volume 40, Issue 4, April 2007, Pages 1234-1244, ISSN 0031-3203, 10.1016/j.patcog.2006.07.003. â†©  M. K. Hu, "Visual Pattern Recognition by Moment Invariants", IRE Trans. Info. Theory, vol. IT-8, pp.179â€“187, 1962 â†©  T. De Bie, N. Cristianini, R. Rosipal, Eigenproblems in pattern recognition, in: E. Bayro-Corrochano (Ed.), Handbook of Computational Geometry for Pattern Recognition, Computer Vision, Neurocomputing and Robotics, Springer, Heidelberg, 2004G. â†©  Strang, Linear Algebra and Its Applications, second ed., Academic Press, New York, 1980. â†©     