   Generalized Hebbian Algorithm      Generalized Hebbian Algorithm  The '''Generalized Hebbian Algorithm''' ('''GHA'''), also known in the literature as '''Sanger's rule''', is a linear [[Feedforward neural network|feedforward]] [[neural network model]] for [[unsupervised learning]] with applications primarily in [[principal components analysis]]. First defined in 1989, {{cite journal |last=Sanger |first=Terence D. |authorlink=Terence Sanger |year=1989 |title= Optimal unsupervised learning in a single-layer linear feedforward neural network |journal=Neural Networks |volume=2 |issue=6 |pages=459–473 |id= |url=http://courses.cs.washington.edu/courses/cse528/09sp/sanger_pca_nn.pdf |accessdate= 2007-11-24 |quot e=|doi= 10.1016/0893-6080(89)90044-0 }} it is similar to Oja's rule in its formulation and stability, except it can be applied to networks with multiple outputs. The name originates because of the similarity between the algorithm and a hypothesis made by Donald Hebb 1 about the way in which synaptic strengths in the brain are modified in response to experience, i.e., that changes are proportional to the correlation between the firing of pre- and post-synaptic neurons . 2  Theory  GHA combines Oja's rule with the Gram-Schmidt process to produce a learning rule of the form        Δ     w   i  j      =   η   (     y  j    x  i    -    y  j     ∑   k  =  1   j     w   i  k     y  k       )          normal-Δ   subscript  w    i  j       η       subscript  y  j    subscript  x  i       subscript  y  j     superscript   subscript     k  1    j      subscript  w    i  k     subscript  y  k          \,\Delta w_{ij}~{}=~{}\eta\left(y_{j}x_{i}-y_{j}\sum_{k=1}^{j}w_{ik}y_{k}\right)   , 3  where defines the synaptic weight or connection strength between the   i   i   i   th input and   j   j   j   th output neurons,   x   x   x   and   y   y   y   are the input and output vectors, respectively, and   η   η   η   is the learning rate parameter.  Derivation  In matrix form, Oja's rule can be written         d  w   (  t  )     d  t     =    w   (  t  )   Q   -   diag   [   w   (  t  )   Q  w    (  t  )   T    ]   w   (  t  )             d  w  t     d  t        w  t  Q     diag   delimited-[]    w  t  Q  w   superscript  t  normal-T     w  t      \,\frac{dw(t)}{dt}~{}=~{}w(t)Q-\mathrm{diag}[w(t)Qw(t)^{\mathrm{T}}]w(t)   ,  and the Gram-Schmidt algorithm is        Δ   w   (  t  )    =   -   lower   [   w   (  t  )   w    (  t  )   T    ]   w   (  t  )           normal-Δ  w  t       lower   delimited-[]    w  t  w   superscript  t  normal-T     w  t      \,\Delta w(t)~{}=~{}-\mathrm{lower}[w(t)w(t)^{\mathrm{T}}]w(t)   ,  where    w   (  t  )       w  t    w(t)   is any matrix, in this case representing synaptic weights,  η  x  x T }} is the autocorrelation matrix, simply the outer product of inputs,    d  i  a  g      d  i  a  g    diag   is the function that diagonalizes a matrix, and    l  o  w  e  r      l  o  w  e  r    lower   is the function that sets all matrix elements on or above the diagonal equal to 0. We can combine these equations to get our original rule in matrix form,        Δ   w   (  t  )    =   η   (  t  )    (    𝐲   (  t  )   𝐱    (  t  )   T    -   LT   [   𝐲   (  t  )   𝐲    (  t  )   T    ]   w   (  t  )     )          normal-Δ  w  t     η  t      𝐲  t  𝐱   superscript  t  normal-T      LT   delimited-[]    𝐲  t  𝐲   superscript  t  normal-T     w  t       \,\Delta w(t)~{}=~{}\eta(t)\left(\mathbf{y}(t)\mathbf{x}(t)^{\mathrm{T}}-%
 \mathrm{LT}[\mathbf{y}(t)\mathbf{y}(t)^{\mathrm{T}}]w(t)\right)   ,  where the function    L  T      L  T    LT   sets all matrix elements above the diagonal equal to 0, and note that our output     𝐲   (  t  )    =   w   (  t  )   𝐱   (  t  )          𝐲  t     w  t  𝐱  t     \mathbf{y}(t)=w(t)\mathbf{x}(t)   is a linear neuron. 4  Stability and PCA  5  6  Applications  GHA is used in applications where a self-organizing map is necessary, or where a feature or principal components analysis can be used. Examples of such cases include artificial intelligence and speech and image processing.  Its importance comes from the fact that learning is a single-layer process—that is, a synaptic weight changes only depending on the response of the inputs and outputs of that layer, thus avoiding the multi-layer dependence associated with the backpropagation algorithm. It also has a simple and predictable trade-off between learning speed and accuracy of convergence as set by the learning rate parameter   η   η   η   . 7  See also   Hebbian learning  Oja's rule  Factor analysis  PCA network   References  "  Category:Artificial neural networks     ↩  ↩  ↩   ↩  ↩      