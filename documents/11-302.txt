   Generalized Hebbian Algorithm      Generalized Hebbian Algorithm  The '''Generalized Hebbian Algorithm''' ('''GHA'''), also known in the literature as '''Sanger's rule''', is a linear [[Feedforward neural network|feedforward]] [[neural network model]] for [[unsupervised learning]] with applications primarily in [[principal components analysis]]. First defined in 1989, {{cite journal |last=Sanger |first=Terence D. |authorlink=Terence Sanger |year=1989 |title= Optimal unsupervised learning in a single-layer linear feedforward neural network |journal=Neural Networks |volume=2 |issue=6 |pages=459â€“473 |id= |url=http://courses.cs.washington.edu/courses/cse528/09sp/sanger_pca_nn.pdf |accessdate= 2007-11-24 |quot e=|doi= 10.1016/0893-6080(89)90044-0 }} it is similar to Oja's rule in its formulation and stability, except it can be applied to networks with multiple outputs. The name originates because of the similarity between the algorithm and a hypothesis made by Donald Hebb 1 about the way in which synaptic strengths in the brain are modified in response to experience, i.e., that changes are proportional to the correlation between the firing of pre- and post-synaptic neurons . 2  Theory  GHA combines Oja's rule with the Gram-Schmidt process to produce a learning rule of the form        Î”     w   i  j      =   Î·   (     y  j    x  i    -    y  j     âˆ‘   k  =  1   j     w   i  k     y  k       )          normal-Î”   subscript  w    i  j       Î·       subscript  y  j    subscript  x  i       subscript  y  j     superscript   subscript     k  1    j      subscript  w    i  k     subscript  y  k          \,\Delta w_{ij}~{}=~{}\eta\left(y_{j}x_{i}-y_{j}\sum_{k=1}^{j}w_{ik}y_{k}\right)   , 3  where defines the synaptic weight or connection strength between the   i   i   i   th input and   j   j   j   th output neurons,   x   x   x   and   y   y   y   are the input and output vectors, respectively, and   Î·   Î·   Î·   is the learning rate parameter.  Derivation  In matrix form, Oja's rule can be written         d  w   (  t  )     d  t     =    w   (  t  )   Q   -   diag   [   w   (  t  )   Q  w    (  t  )   T    ]   w   (  t  )             d  w  t     d  t        w  t  Q     diag   delimited-[]    w  t  Q  w   superscript  t  normal-T     w  t      \,\frac{dw(t)}{dt}~{}=~{}w(t)Q-\mathrm{diag}[w(t)Qw(t)^{\mathrm{T}}]w(t)   ,  and the Gram-Schmidt algorithm is        Î”   w   (  t  )    =   -   lower   [   w   (  t  )   w    (  t  )   T    ]   w   (  t  )           normal-Î”  w  t       lower   delimited-[]    w  t  w   superscript  t  normal-T     w  t      \,\Delta w(t)~{}=~{}-\mathrm{lower}[w(t)w(t)^{\mathrm{T}}]w(t)   ,  where    w   (  t  )       w  t    w(t)   is any matrix, in this case representing synaptic weights,  Î·  x  x T }} is the autocorrelation matrix, simply the outer product of inputs,    d  i  a  g      d  i  a  g    diag   is the function that diagonalizes a matrix, and    l  o  w  e  r      l  o  w  e  r    lower   is the function that sets all matrix elements on or above the diagonal equal to 0. We can combine these equations to get our original rule in matrix form,        Î”   w   (  t  )    =   Î·   (  t  )    (    ğ²   (  t  )   ğ±    (  t  )   T    -   LT   [   ğ²   (  t  )   ğ²    (  t  )   T    ]   w   (  t  )     )          normal-Î”  w  t     Î·  t      ğ²  t  ğ±   superscript  t  normal-T      LT   delimited-[]    ğ²  t  ğ²   superscript  t  normal-T     w  t       \,\Delta w(t)~{}=~{}\eta(t)\left(\mathbf{y}(t)\mathbf{x}(t)^{\mathrm{T}}-%
 \mathrm{LT}[\mathbf{y}(t)\mathbf{y}(t)^{\mathrm{T}}]w(t)\right)   ,  where the function    L  T      L  T    LT   sets all matrix elements above the diagonal equal to 0, and note that our output     ğ²   (  t  )    =   w   (  t  )   ğ±   (  t  )          ğ²  t     w  t  ğ±  t     \mathbf{y}(t)=w(t)\mathbf{x}(t)   is a linear neuron. 4  Stability and PCA  5  6  Applications  GHA is used in applications where a self-organizing map is necessary, or where a feature or principal components analysis can be used. Examples of such cases include artificial intelligence and speech and image processing.  Its importance comes from the fact that learning is a single-layer processâ€”that is, a synaptic weight changes only depending on the response of the inputs and outputs of that layer, thus avoiding the multi-layer dependence associated with the backpropagation algorithm. It also has a simple and predictable trade-off between learning speed and accuracy of convergence as set by the learning rate parameter   Î·   Î·   Î·   . 7  See also   Hebbian learning  Oja's rule  Factor analysis  PCA network   References  "  Category:Artificial neural networks     â†©  â†©  â†©   â†©  â†©      