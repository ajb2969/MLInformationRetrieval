   Linear model      Linear model   In statistics , the term linear model is used in different ways according to the context. The most common occurrence is in connection with regression models and the term is often taken as synonymous with linear regression model. However, the term is also used in time series analysis with a different meaning. In each case, the designation "linear" is used to identify a subclass of models for which substantial reduction in the complexity of the related statistical theory is possible.  Linear regression models  For the regression case, the statistical model is as follows. Given a (random) sample       (   Y  i   ,   X   i  1    ,  …  ,   X   i  p    )   ,  i   =  1   ,   …  ,  n      formulae-sequence       subscript  Y  i    subscript  X    i  1    normal-…   subscript  X    i  p     i   1    normal-…  n     (Y_{i},X_{i1},\ldots,X_{ip}),\,i=1,\ldots,n   the relation between the observations Y i and the independent variables  X ij is formulated as        Y  i   =    β  0   +    β  1    ϕ  1    (   X   i  1    )    +  ⋯  +    β  p    ϕ  p    (   X   i  p    )    +   ε  i      i  =   1  ,  …  ,  n       formulae-sequence     subscript  Y  i      subscript  β  0      subscript  β  1    subscript  ϕ  1    subscript  X    i  1     normal-⋯     subscript  β  p    subscript  ϕ  p    subscript  X    i  p      subscript  ε  i       i   1  normal-…  n      Y_{i}=\beta_{0}+\beta_{1}\phi_{1}(X_{i1})+\cdots+\beta_{p}\phi_{p}(X_{ip})+%
 \varepsilon_{i}\qquad i=1,\ldots,n     where     ϕ  1   ,  …  ,   ϕ  p       subscript  ϕ  1   normal-…   subscript  ϕ  p     \phi_{1},\ldots,\phi_{p}   may be nonlinear functions. In the above, the quantities ε i are random variables representing errors in the relationship. The "linear" part of the designation relates to the appearance of the regression coefficients , β j in a linear way in the above relationship. Alternatively, one may say that the predicted values corresponding to the above model, namely        Y  ^   i   =   β  0   +   β  1    ϕ  1    (   X   i  1    )   +  ⋯  +   β  p    ϕ  p    (   X   i  p    )    (  i  =  1  ,  …  ,  n  )   ,     fragments   subscript   normal-^  Y   i     subscript  β  0     subscript  β  1    subscript  ϕ  1    fragments  normal-(   subscript  X    i  1    normal-)    normal-⋯    subscript  β  p    subscript  ϕ  p    fragments  normal-(   subscript  X    i  p    normal-)   italic-   fragments  normal-(  i   1  normal-,  normal-…  normal-,  n  normal-)   normal-,    \hat{Y}_{i}=\beta_{0}+\beta_{1}\phi_{1}(X_{i1})+\cdots+\beta_{p}\phi_{p}(X_{ip%
 })\qquad(i=1,\ldots,n),   are linear functions of the β j .  Given that estimation is undertaken on the basis of a least squares analysis, estimates of the unknown parameters β j are determined by minimising a sum of squares function       S  =    ∑   i  =  1   n     (    Y  i   -   β  0   -    β  1    ϕ  1    (   X   i  1    )    -  ⋯  -    β  p    ϕ  p    (   X   i  p    )     )   2     .      S    superscript   subscript     i  1    n    superscript     subscript  Y  i    subscript  β  0      subscript  β  1    subscript  ϕ  1    subscript  X    i  1     normal-⋯     subscript  β  p    subscript  ϕ  p    subscript  X    i  p      2      S=\sum_{i=1}^{n}\left(Y_{i}-\beta_{0}-\beta_{1}\phi_{1}(X_{i1})-\cdots-\beta_{%
 p}\phi_{p}(X_{ip})\right)^{2}.   From this, it can readily be seen that the "linear" aspect of the model means the following:  :*the function to be minimised is a quadratic function of the β j for which minimisation is a relatively simple problem;  :*the derivatives of the function are linear functions of the β j making it easy to find the minimising values;  :*the minimising values β j are linear functions of the observations Y i ;  :*the minimising values β j are linear functions of the random errors ε i which makes it relatively easy to determine the statistical properties of the estimated values of β j .  Time series models  An example of a linear time series model is an autoregressive moving average model . Here the model for values { X t } in a time series can be written in the form        X  t   =   c  +   ε  t   +    ∑   i  =  1   p     ϕ  i    X   t  -  i      +    ∑   i  =  1   q     θ  i    ε   t  -  i        .       subscript  X  t     c   subscript  ε  t     superscript   subscript     i  1    p      subscript  ϕ  i    subscript  X    t  i        superscript   subscript     i  1    q      subscript  θ  i    subscript  ε    t  i         X_{t}=c+\varepsilon_{t}+\sum_{i=1}^{p}\phi_{i}X_{t-i}+\sum_{i=1}^{q}\theta_{i}%
 \varepsilon_{t-i}.\,     where again the quantities ε t are random variables representing innovations which are new random effects that appear at a certain time but also affect values of X at later times. In this instance the use of the term "linear model" refers to the structure of the above relationship in representing X t as a linear function of past values of the same time series and of current and past values of the innovations. 1 This particular aspect of the structure means that it is relatively simple to derive relations for the mean and covariance properties of the time series. Note that here the "linear" part of the term "linear model" is not referring to the coefficients φ i and θ i , as it would be in the case of a regression model, which looks structurally similar.  Other uses in statistics  There are some other instances where "nonlinear model" is used to contrast with a linearly structured model, although the term "linear model" is not usually applied. One example of this is nonlinear dimensionality reduction .  See also   General linear model  Generalized linear model  Linear system  Statistical model   References    ar:نموذج الانحدار الخطي  de:Lineares Modell  fr:Modèle linéaire "  Category:Statistical models  Category:Time series models  Category:Regression analysis  Category:Statistical terminology     Priestley, M.B. (1988) Non-linear and Non-stationary time series analysis , Academic Press. ISBN 0-12-564911-8 ↩     