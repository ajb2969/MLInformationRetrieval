   Descent direction      Descent direction   In optimization , a descent direction is a vector    ğ©  âˆˆ   â„  n       ğ©   superscript  â„  n     \mathbf{p}\in\mathbb{R}^{n}   that, in the sense below, moves us closer towards a local minimum    ğ±  *     superscript  ğ±     \mathbf{x}^{*}   of our objective function    f  :    â„  n   â†’  â„      normal-:  f   normal-â†’   superscript  â„  n   â„     f:\mathbb{R}^{n}\to\mathbb{R}   .  Suppose we are computing    ğ±  *     superscript  ğ±     \mathbf{x}^{*}   by an iterative method, such as line search . We define a descent direction     ğ©  k   âˆˆ   â„  n        subscript  ğ©  k    superscript  â„  n     \mathbf{p}_{k}\in\mathbb{R}^{n}   at the   k   k   k   th iterate to be any    ğ©  k     subscript  ğ©  k    \mathbf{p}_{k}   such that     âŸ¨   ğ©  k   ,    âˆ‡  f    (   ğ±  k   )    âŸ©   <  0        subscript  ğ©  k      normal-âˆ‡  f    subscript  ğ±  k     0    \langle\mathbf{p}_{k},\nabla f(\mathbf{x}_{k})\rangle<0   , where    âŸ¨  ,  âŸ©     fragments  normal-âŸ¨  normal-,  normal-âŸ©    \langle,\rangle   denotes the inner product . The motivation for such an approach is that small steps along    ğ©  k     subscript  ğ©  k    \mathbf{p}_{k}   guarantee that   f   f   \displaystyle f   is reduced, by Taylor's theorem .  Using this definition, the negative of a non-zero gradient is always a descent direction, as     âŸ¨   -    âˆ‡  f    (   ğ±  k   )     ,    âˆ‡  f    (   ğ±  k   )    âŸ©   =   -   âŸ¨    âˆ‡  f    (   ğ±  k   )    ,    âˆ‡  f    (   ğ±  k   )    âŸ©    <  0              normal-âˆ‡  f    subscript  ğ±  k        normal-âˆ‡  f    subscript  ğ±  k           normal-âˆ‡  f    subscript  ğ±  k       normal-âˆ‡  f    subscript  ğ±  k           0     \langle-\nabla f(\mathbf{x}_{k}),\nabla f(\mathbf{x}_{k})\rangle=-\langle%
 \nabla f(\mathbf{x}_{k}),\nabla f(\mathbf{x}_{k})\rangle<0   .  Numerous methods exist to compute descent directions, all with differing merits. For example, one could use gradient descent or the conjugate gradient method .  More generally, if   P   P   P   is a positive definite matrix, then    d  =   -   P   âˆ‡  f    (  x  )         d      P   normal-âˆ‡  f   x      d=-P\nabla f(x)   is a descent direction 1 at   x   x   x   . This generality is used in preconditioned gradient descent methods.  "  Category:Mathematical optimization     â†©     