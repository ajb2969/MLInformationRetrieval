   Descent direction      Descent direction   In optimization , a descent direction is a vector    𝐩  ∈   ℝ  n       𝐩   superscript  ℝ  n     \mathbf{p}\in\mathbb{R}^{n}   that, in the sense below, moves us closer towards a local minimum    𝐱  *     superscript  𝐱     \mathbf{x}^{*}   of our objective function    f  :    ℝ  n   →  ℝ      normal-:  f   normal-→   superscript  ℝ  n   ℝ     f:\mathbb{R}^{n}\to\mathbb{R}   .  Suppose we are computing    𝐱  *     superscript  𝐱     \mathbf{x}^{*}   by an iterative method, such as line search . We define a descent direction     𝐩  k   ∈   ℝ  n        subscript  𝐩  k    superscript  ℝ  n     \mathbf{p}_{k}\in\mathbb{R}^{n}   at the   k   k   k   th iterate to be any    𝐩  k     subscript  𝐩  k    \mathbf{p}_{k}   such that     ⟨   𝐩  k   ,    ∇  f    (   𝐱  k   )    ⟩   <  0        subscript  𝐩  k      normal-∇  f    subscript  𝐱  k     0    \langle\mathbf{p}_{k},\nabla f(\mathbf{x}_{k})\rangle<0   , where    ⟨  ,  ⟩     fragments  normal-⟨  normal-,  normal-⟩    \langle,\rangle   denotes the inner product . The motivation for such an approach is that small steps along    𝐩  k     subscript  𝐩  k    \mathbf{p}_{k}   guarantee that   f   f   \displaystyle f   is reduced, by Taylor's theorem .  Using this definition, the negative of a non-zero gradient is always a descent direction, as     ⟨   -    ∇  f    (   𝐱  k   )     ,    ∇  f    (   𝐱  k   )    ⟩   =   -   ⟨    ∇  f    (   𝐱  k   )    ,    ∇  f    (   𝐱  k   )    ⟩    <  0              normal-∇  f    subscript  𝐱  k        normal-∇  f    subscript  𝐱  k           normal-∇  f    subscript  𝐱  k       normal-∇  f    subscript  𝐱  k           0     \langle-\nabla f(\mathbf{x}_{k}),\nabla f(\mathbf{x}_{k})\rangle=-\langle%
 \nabla f(\mathbf{x}_{k}),\nabla f(\mathbf{x}_{k})\rangle<0   .  Numerous methods exist to compute descent directions, all with differing merits. For example, one could use gradient descent or the conjugate gradient method .  More generally, if   P   P   P   is a positive definite matrix, then    d  =   -   P   ∇  f    (  x  )         d      P   normal-∇  f   x      d=-P\nabla f(x)   is a descent direction 1 at   x   x   x   . This generality is used in preconditioned gradient descent methods.  "  Category:Mathematical optimization     ↩     