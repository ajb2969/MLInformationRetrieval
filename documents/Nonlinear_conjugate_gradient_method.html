<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="344">Nonlinear conjugate gradient method</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Nonlinear conjugate gradient method</h1>
<hr/>

<p>In <a href="numerical_optimization" title="wikilink">numerical optimization</a>, the <strong>nonlinear conjugate gradient method</strong> generalizes the <a href="conjugate_gradient_method" title="wikilink">conjugate gradient method</a> to <a href="nonlinear_optimization" title="wikilink">nonlinear optimization</a>. For a quadratic function 

<math display="inline" id="Nonlinear_conjugate_gradient_method:0">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle f(x)
  </annotation>
 </semantics>
</math>

:</p>
<dl>
<dd><dl>
<dd>

<math display="inline" id="Nonlinear_conjugate_gradient_method:1">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo>∥</mo>
     <mrow>
      <mrow>
       <mi>A</mi>
       <mi>x</mi>
      </mrow>
      <mo>-</mo>
      <mi>b</mi>
     </mrow>
     <mo>∥</mo>
    </mrow>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <apply>
       <minus></minus>
       <apply>
        <times></times>
        <ci>A</ci>
        <ci>x</ci>
       </apply>
       <ci>b</ci>
      </apply>
     </apply>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle f(x)=\|Ax-b\|^{2}
  </annotation>
 </semantics>
</math>


</dd>
</dl>
</dd>
</dl>

<p>The minimum of 

<math display="inline" id="Nonlinear_conjugate_gradient_method:2">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 is obtained when the <a class="uri" href="gradient" title="wikilink">gradient</a> is 0:</p>
<dl>
<dd><dl>
<dd>

<math display="inline" id="Nonlinear_conjugate_gradient_method:3">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mo>∇</mo>
     <mi>x</mi>
    </msub>
    <mi>f</mi>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mn>2</mn>
    <msup>
     <mi>A</mi>
     <mo>⊤</mo>
    </msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mrow>
       <mi>A</mi>
       <mi>x</mi>
      </mrow>
      <mo>-</mo>
      <mi>b</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>normal-∇</ci>
       <ci>x</ci>
      </apply>
      <ci>f</ci>
     </apply>
     <apply>
      <times></times>
      <cn type="integer">2</cn>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>A</ci>
       <csymbol cd="latexml">top</csymbol>
      </apply>
      <apply>
       <minus></minus>
       <apply>
        <times></times>
        <ci>A</ci>
        <ci>x</ci>
       </apply>
       <ci>b</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <cn type="integer">0</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \nabla_{x}f=2A^{\top}(Ax-b)=0
  </annotation>
 </semantics>
</math>

.
</dd>
</dl>
</dd>
</dl>

<p>Whereas linear conjugate gradient seeks a solution to the linear equation 

<math display="inline" id="Nonlinear_conjugate_gradient_method:4">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>A</mi>
     <mo>⊤</mo>
    </msup>
    <mi>A</mi>
    <mi>x</mi>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msup>
     <mi>A</mi>
     <mo>⊤</mo>
    </msup>
    <mi>b</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>A</ci>
      <csymbol cd="latexml">top</csymbol>
     </apply>
     <ci>A</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>A</ci>
      <csymbol cd="latexml">top</csymbol>
     </apply>
     <ci>b</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle A^{\top}Ax=A^{\top}b
  </annotation>
 </semantics>
</math>

, the nonlinear conjugate gradient method is generally used to find the <a href="maxima_and_minima" title="wikilink">local minimum</a> of a nonlinear function using its <a class="uri" href="gradient" title="wikilink">gradient</a> 

<math display="inline" id="Nonlinear_conjugate_gradient_method:5">
 <semantics>
  <mrow>
   <msub>
    <mo>∇</mo>
    <mi>x</mi>
   </msub>
   <mi>f</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>normal-∇</ci>
     <ci>x</ci>
    </apply>
    <ci>f</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \nabla_{x}f
  </annotation>
 </semantics>
</math>

 alone. It works when the function is approximately quadratic near the minimum, which is the case when the function is twice differentiable at the minimum.</p>

<p>Given a function 

<math display="inline" id="Nonlinear_conjugate_gradient_method:6">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle f(x)
  </annotation>
 </semantics>
</math>

 of 

<math display="inline" id="Nonlinear_conjugate_gradient_method:7">
 <semantics>
  <mi>N</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>N</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N
  </annotation>
 </semantics>
</math>

 variables to minimize, its gradient 

<math display="inline" id="Nonlinear_conjugate_gradient_method:8">
 <semantics>
  <mrow>
   <msub>
    <mo>∇</mo>
    <mi>x</mi>
   </msub>
   <mi>f</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>normal-∇</ci>
     <ci>x</ci>
    </apply>
    <ci>f</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \nabla_{x}f
  </annotation>
 </semantics>
</math>

 indicates the direction of maximum increase. One simply starts in the opposite (<a href="steepest_descent" title="wikilink">steepest descent</a>) direction:</p>
<dl>
<dd><dl>
<dd>

<math display="inline" id="Nonlinear_conjugate_gradient_method:9">
 <semantics>
  <mrow>
   <mrow>
    <mi mathvariant="normal">Δ</mi>
    <msub>
     <mi>x</mi>
     <mn>0</mn>
    </msub>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <mrow>
      <msub>
       <mo>∇</mo>
       <mi>x</mi>
      </msub>
      <mi>f</mi>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mn>0</mn>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>normal-Δ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">0</cn>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>normal-∇</ci>
        <ci>x</ci>
       </apply>
       <ci>f</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <cn type="integer">0</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Delta x_{0}=-\nabla_{x}f(x_{0})
  </annotation>
 </semantics>
</math>


</dd>
</dl>
</dd>
</dl>

<p>with an adjustable step length 

<math display="inline" id="Nonlinear_conjugate_gradient_method:10">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\alpha
  </annotation>
 </semantics>
</math>

 and performs a <a href="line_search" title="wikilink">line search</a> in this direction until it reaches the minimum of 

<math display="inline" id="Nonlinear_conjugate_gradient_method:11">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle f
  </annotation>
 </semantics>
</math>

:</p>
<dl>
<dd><dl>
<dd>

<math display="inline" id="Nonlinear_conjugate_gradient_method:12">
 <semantics>
  <mrow>
   <msub>
    <mi>α</mi>
    <mn>0</mn>
   </msub>
   <mo>:=</mo>
   <mrow>
    <mrow>
     <mi>arg</mi>
     <mrow>
      <munder>
       <mi>min</mi>
       <mi>α</mi>
      </munder>
      <mi>f</mi>
     </mrow>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>x</mi>
       <mn>0</mn>
      </msub>
      <mo>+</mo>
      <mrow>
       <mi>α</mi>
       <mi mathvariant="normal">Δ</mi>
       <msub>
        <mi>x</mi>
        <mn>0</mn>
       </msub>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">assign</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>α</ci>
     <cn type="integer">0</cn>
    </apply>
    <apply>
     <times></times>
     <apply>
      <arg></arg>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <min></min>
        <ci>α</ci>
       </apply>
       <ci>f</ci>
      </apply>
     </apply>
     <apply>
      <plus></plus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <cn type="integer">0</cn>
      </apply>
      <apply>
       <times></times>
       <ci>α</ci>
       <ci>normal-Δ</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <cn type="integer">0</cn>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\alpha_{0}:=\arg\min_{\alpha}f(x_{0}+\alpha\Delta x_{0})
  </annotation>
 </semantics>
</math>

,
</dd>
<dd>

<math display="inline" id="Nonlinear_conjugate_gradient_method:13">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mn>1</mn>
   </msub>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>x</mi>
     <mn>0</mn>
    </msub>
    <mo>+</mo>
    <mrow>
     <msub>
      <mi>α</mi>
      <mn>0</mn>
     </msub>
     <mi mathvariant="normal">Δ</mi>
     <msub>
      <mi>x</mi>
      <mn>0</mn>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">0</cn>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>α</ci>
       <cn type="integer">0</cn>
      </apply>
      <ci>normal-Δ</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <cn type="integer">0</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle x_{1}=x_{0}+\alpha_{0}\Delta x_{0}
  </annotation>
 </semantics>
</math>


</dd>
</dl>
</dd>
</dl>

<p>After this first iteration in the steepest direction 

<math display="inline" id="Nonlinear_conjugate_gradient_method:14">
 <semantics>
  <mrow>
   <mi mathvariant="normal">Δ</mi>
   <msub>
    <mi>x</mi>
    <mn>0</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-Δ</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">0</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\Delta x_{0}
  </annotation>
 </semantics>
</math>

, the following steps constitute one iteration of moving along a subsequent conjugate direction 

<math display="inline" id="Nonlinear_conjugate_gradient_method:15">
 <semantics>
  <msub>
   <mi>s</mi>
   <mi>n</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>s</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle s_{n}
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Nonlinear_conjugate_gradient_method:16">
 <semantics>
  <mrow>
   <msub>
    <mi>s</mi>
    <mn>0</mn>
   </msub>
   <mo>=</mo>
   <mrow>
    <mi mathvariant="normal">Δ</mi>
    <msub>
     <mi>x</mi>
     <mn>0</mn>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>s</ci>
     <cn type="integer">0</cn>
    </apply>
    <apply>
     <times></times>
     <ci>normal-Δ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">0</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle s_{0}=\Delta x_{0}
  </annotation>
 </semantics>
</math>

:</p>
<ol>
<li>Calculate the steepest direction

<math display="block" id="Nonlinear_conjugate_gradient_method:17">
 <semantics>
  <mrow>
   <mrow>
    <mi mathvariant="normal">Δ</mi>
    <msub>
     <mi>x</mi>
     <mi>n</mi>
    </msub>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <mrow>
      <msub>
       <mo>∇</mo>
       <mi>x</mi>
      </msub>
      <mi>f</mi>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mi>n</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>normal-Δ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>n</ci>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>normal-∇</ci>
        <ci>x</ci>
       </apply>
       <ci>f</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>n</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Delta x_{n}=-\nabla_{x}f(x_{n})
  </annotation>
 </semantics>
</math>

,</li>
<li>Compute 

<math display="inline" id="Nonlinear_conjugate_gradient_method:18">
 <semantics>
  <msub>
   <mi>β</mi>
   <mi>n</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>β</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\beta_{n}
  </annotation>
 </semantics>
</math>

 according to one of the formulas below,</li>
<li>Update the conjugate direction

<math display="block" id="Nonlinear_conjugate_gradient_method:19">
 <semantics>
  <mrow>
   <msub>
    <mi>s</mi>
    <mi>n</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi mathvariant="normal">Δ</mi>
     <msub>
      <mi>x</mi>
      <mi>n</mi>
     </msub>
    </mrow>
    <mo>+</mo>
    <mrow>
     <msub>
      <mi>β</mi>
      <mi>n</mi>
     </msub>
     <msub>
      <mi>s</mi>
      <mrow>
       <mi>n</mi>
       <mo>-</mo>
       <mn>1</mn>
      </mrow>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>s</ci>
     <ci>n</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <ci>normal-Δ</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>n</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>β</ci>
       <ci>n</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>s</ci>
       <apply>
        <minus></minus>
        <ci>n</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle s_{n}=\Delta x_{n}+\beta_{n}s_{n-1}
  </annotation>
 </semantics>
</math>

</li>
<li>Perform a line search: optimize 

<math display="inline" id="Nonlinear_conjugate_gradient_method:20">
 <semantics>
  <mrow>
   <msub>
    <mi>α</mi>
    <mi>n</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi>arg</mi>
     <mrow>
      <munder>
       <mi>min</mi>
       <mi>α</mi>
      </munder>
      <mi>f</mi>
     </mrow>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>x</mi>
       <mi>n</mi>
      </msub>
      <mo>+</mo>
      <mrow>
       <mi>α</mi>
       <msub>
        <mi>s</mi>
        <mi>n</mi>
       </msub>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>α</ci>
     <ci>n</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <arg></arg>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <min></min>
        <ci>α</ci>
       </apply>
       <ci>f</ci>
      </apply>
     </apply>
     <apply>
      <plus></plus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>n</ci>
      </apply>
      <apply>
       <times></times>
       <ci>α</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>s</ci>
        <ci>n</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\alpha_{n}=\arg\min_{\alpha}f(x_{n}+\alpha s_{n})
  </annotation>
 </semantics>
</math>

,</li>
<li>Update the position

<math display="block" id="Nonlinear_conjugate_gradient_method:21">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mrow>
     <mi>n</mi>
     <mo>+</mo>
     <mn>1</mn>
    </mrow>
   </msub>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>x</mi>
     <mi>n</mi>
    </msub>
    <mo>+</mo>
    <mrow>
     <msub>
      <mi>α</mi>
      <mi>n</mi>
     </msub>
     <msub>
      <mi>s</mi>
      <mi>n</mi>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <apply>
      <plus></plus>
      <ci>n</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>n</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>α</ci>
       <ci>n</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>s</ci>
       <ci>n</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle x_{n+1}=x_{n}+\alpha_{n}s_{n}
  </annotation>
 </semantics>
</math>

,</li>
</ol>

<p>With a pure quadratic function the minimum is reached within N iterations (excepting roundoff error), but a non-quadratic function will make slower progress. Subsequent search directions lose conjugacy requiring the search direction to be reset to the steepest descent direction at least every N iterations, or sooner if progress stops. However, resetting every iteration turns the method into <a href="steepest_descent" title="wikilink">steepest descent</a>. The algorithm stops when it finds the minimum, determined when no progress is made after a direction reset (i.e. in the steepest descent direction), or when some tolerance criterion is reached.</p>

<p>Within a linear approximation, the parameters 

<math display="inline" id="Nonlinear_conjugate_gradient_method:22">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\alpha
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Nonlinear_conjugate_gradient_method:23">
 <semantics>
  <mi>β</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>β</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\beta
  </annotation>
 </semantics>
</math>

 are the same as in the linear conjugate gradient method but have been obtained with line searches. The conjugate gradient method can follow narrow (<a class="uri" href="ill-conditioned" title="wikilink">ill-conditioned</a>) valleys where the <a href="steepest_descent" title="wikilink">steepest descent</a> method slows down and follows a criss-cross pattern.</p>

<p>Four of the best known formulas for 

<math display="inline" id="Nonlinear_conjugate_gradient_method:24">
 <semantics>
  <msub>
   <mi>β</mi>
   <mi>n</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>β</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\beta_{n}
  </annotation>
 </semantics>
</math>

 are named after their developers and are given by the following formulas:</p>
<ul>
<li>Fletcher–Reeves:</li>
</ul>
<dl>
<dd><dl>
<dd><math>\beta_{n}^{FR} = \frac{\Delta x_n^\top \Delta x_n}
</math></dd>
</dl>
</dd>
</dl>

<p>{\Delta x_{n-1}^\top \Delta x_{n-1}} </p>
<ul>
<li>Polak–Ribière:</li>
</ul>
<dl>
<dd><dl>
<dd><math>\beta_{n}^{PR} = \frac{\Delta x_n^\top (\Delta x_n-\Delta x_{n-1})}
</math></dd>
</dl>
</dd>
</dl>

<p>{\Delta x_{n-1}^\top \Delta x_{n-1}} </p>
<ul>
<li>Hestenes-Stiefel:</li>
</ul>
<dl>
<dd><dl>
<dd><math>\beta_n^{HS} = -\frac{\Delta x_n^\top (\Delta x_n-\Delta x_{n-1})}
</math></dd>
</dl>
</dd>
</dl>

<p>{s_{n-1}^\top (\Delta x_n-\Delta x_{n-1})} </p>
<ul>
<li>Dai–Yuan:</li>
</ul>
<dl>
<dd><dl>
<dd><math>\beta_{n}^{DY} = -\frac{\Delta x_n^\top \Delta x_n}
</math></dd>
</dl>
</dd>
</dl>

<p>{s_{n-1}^\top (\Delta x_n-\Delta x_{n-1})} .</p>

<p>These formulas are equivalent for a quadratic function, but for nonlinear optimization the preferred formula is a matter of heuristics or taste. A popular choice is 

<math display="inline" id="Nonlinear_conjugate_gradient_method:25">
 <semantics>
  <mrow>
   <mi>β</mi>
   <mo>=</mo>
   <mrow>
    <mi>max</mi>
    <mrow>
     <mo stretchy="false">{</mo>
     <mn>0</mn>
     <mo rspace="4.2pt">,</mo>
     <msup>
      <mi>β</mi>
      <mrow>
       <mi>P</mi>
       <mi>R</mi>
      </mrow>
     </msup>
     <mo stretchy="false">}</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>β</ci>
    <apply>
     <max></max>
     <cn type="integer">0</cn>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>β</ci>
      <apply>
       <times></times>
       <ci>P</ci>
       <ci>R</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\beta=\max\{0,\,\beta^{PR}\}
  </annotation>
 </semantics>
</math>

 which provides a direction reset automatically.</p>

<p>Newton based methods - <a href="Newton-Raphson_Algorithm" title="wikilink">Newton-Raphson Algorithm</a>, <a href="Quasi-Newton_methods" title="wikilink">Quasi-Newton methods</a> (e.g., <a href="BFGS_method" title="wikilink">BFGS method</a>) - tend to converge in fewer iterations, although each iteration typically requires more computation than a conjugate gradient iteration as Newton-like methods require computing the <a href="Hessian_matrix" title="wikilink">Hessian</a> (matrix of second derivatives) in addition to the gradient. Quasi-Newton methods also require more memory to operate (see also the limited memory <a class="uri" href="L-BFGS" title="wikilink">L-BFGS</a> method).</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Nelder–Mead_method" title="wikilink">Nelder–Mead method</a></li>
<li><a href="conjugate_gradient_method" title="wikilink">conjugate gradient method</a></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a> by Jonathan Richard Shewchuk.</li>
<li>[<a class="uri" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.3325&amp;rep">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.3325&amp;rep;</a>;=rep1&amp;type;=pdf A NONLINEAR CONJUGATE GRADIENT METHOD WITH A STRONG GLOBAL CONVERGENCE PROPERTY] by Y. H. DAI and Y. YUAN.</li>
</ul>

<p><a href="ru:Метод_сопряжённых_градиентов" title="wikilink">ru:Метод сопряжённых градиентов</a></p>

<p>"</p>

<p><a href="Category:Optimization_algorithms_and_methods" title="wikilink">Category:Optimization algorithms and methods</a> <a href="Category:Gradient_methods" title="wikilink">Category:Gradient methods</a></p>
</body>
</html>
