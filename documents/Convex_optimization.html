<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1212">Convex optimization</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Convex optimization</h1>
<hr/>

<p><strong>Convex minimization</strong>, a subfield of <a href="mathematical_optimization" title="wikilink">optimization</a>, studies the problem of minimizing <a href="convex_function" title="wikilink">convex functions</a> over <a href="convex_set" title="wikilink">convex sets</a>. The convexity property can make optimization in some sense "easier" than the general case - for example, any <a href="local_optimum" title="wikilink">local minimum</a> must be a <a href="global_optimum" title="wikilink">global minimum</a>.</p>

<p>Given a <a href="real_number" title="wikilink">real</a> <a href="vector_space" title="wikilink">vector space</a> 

<math display="inline" id="Convex_optimization:0">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 together with a <a href="convex_function" title="wikilink">convex</a>, real-valued <a href="function_(mathematics)" title="wikilink">function</a></p>

<p>

<math display="block" id="Convex_optimization:1">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mo>:</mo>
   <mrow>
    <mi class="ltx_font_mathcaligraphic">ùí≥</mi>
    <mo>‚Üí</mo>
    <mi>‚Ñù</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>f</ci>
    <apply>
     <ci>normal-‚Üí</ci>
     <ci>ùí≥</ci>
     <ci>‚Ñù</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f:\mathcal{X}\to\mathbb{R}
  </annotation>
 </semantics>
</math>

</p>

<p>defined on a <a href="convex_set" title="wikilink">convex subset</a> 

<math display="inline" id="Convex_optimization:2">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ùí≥</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ùí≥</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{X}
  </annotation>
 </semantics>
</math>

 of 

<math display="inline" id="Convex_optimization:3">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

, the problem is to find any point 

<math display="inline" id="Convex_optimization:4">
 <semantics>
  <msup>
   <mi>x</mi>
   <mo>‚àó</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>x</ci>
    <ci>normal-‚àó</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x^{\ast}
  </annotation>
 </semantics>
</math>

 in 

<math display="inline" id="Convex_optimization:5">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ùí≥</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ùí≥</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{X}
  </annotation>
 </semantics>
</math>

 for which the number 

<math display="inline" id="Convex_optimization:6">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)
  </annotation>
 </semantics>
</math>


 is smallest, i.e., a point 

<math display="inline" id="Convex_optimization:7">
 <semantics>
  <msup>
   <mi>x</mi>
   <mo>‚àó</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>x</ci>
    <ci>normal-‚àó</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x^{\ast}
  </annotation>
 </semantics>
</math>

 such that</p>

<p>

<math display="block" id="Convex_optimization:8">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msup>
      <mi>x</mi>
      <mo>‚àó</mo>
     </msup>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>‚â§</mo>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <apply>
     <times></times>
     <ci>f</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>x</ci>
      <ci>normal-‚àó</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x^{\ast})\leq f(x)
  </annotation>
 </semantics>
</math>

 for all 

<math display="inline" id="Convex_optimization:9">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>‚àà</mo>
   <mi class="ltx_font_mathcaligraphic">ùí≥</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>x</ci>
    <ci>ùí≥</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x\in\mathcal{X}
  </annotation>
 </semantics>
</math>

.</p>

<p>The convexity of 

<math display="inline" id="Convex_optimization:10">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 makes the powerful tools of <a href="convex_analysis" title="wikilink">convex analysis</a> applicable. In finite-dimensional <a href="normed_space" title="wikilink">normed spaces</a>, the <a href="Hahn‚ÄìBanach_theorem" title="wikilink">Hahn‚ÄìBanach theorem</a> and the existence of <a href="subgradient" title="wikilink">subgradients</a> lead to a particularly satisfying theory of <a href="necessary_and_sufficient_conditions" title="wikilink">necessary and sufficient conditions</a> for optimality, a <a href="dual_problem" title="wikilink">duality theory</a> generalizing that for <a href="linear_programming" title="wikilink">linear programming</a>, and effective computational methods.</p>

<p>Convex minimization has applications in a wide range of disciplines, such as automatic <a href="control_systems" title="wikilink">control systems</a>, estimation and <a href="signal_processing" title="wikilink">signal processing</a>, communications and networks, electronic <a href="circuit_design" title="wikilink">circuit design</a>, data analysis and modeling, <a class="uri" href="statistics" title="wikilink">statistics</a> (<a href="optimal_design" title="wikilink">optimal design</a>), and <a class="uri" href="finance" title="wikilink">finance</a>. With recent improvements in computing and in optimization theory, convex minimization is nearly as straightforward as <a href="linear_programming" title="wikilink">linear programming</a>. Many optimization problems can be reformulated as convex minimization problems. For example, the problem of <em>maximizing</em> a <em>concave</em> function <em>f</em> can be re-formulated equivalently as a problem of <em>minimizing</em> the function -<em>f</em>, which is <em>convex</em>.</p>
<h2 id="convex-optimization-problem">Convex optimization problem</h2>

<p>An <em>optimization problem</em> (also referred to as a <em>mathematical programming problem</em> or <em>minimization problem</em>) of finding some 

<math display="inline" id="Convex_optimization:11">
 <semantics>
  <mrow>
   <msup>
    <mi>x</mi>
    <mo>‚àó</mo>
   </msup>
   <mo>‚àà</mo>
   <mi class="ltx_font_mathcaligraphic">ùí≥</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>x</ci>
     <ci>normal-‚àó</ci>
    </apply>
    <ci>ùí≥</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x^{\ast}\in\mathcal{X}
  </annotation>
 </semantics>
</math>


 such that</p>

<p>

<math display="block" id="Convex_optimization:12">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msup>
       <mi>x</mi>
       <mo>‚àó</mo>
      </msup>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mi>min</mi>
     <mrow>
      <mo stretchy="false">{</mo>
      <mrow>
       <mrow>
        <mi>f</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>:</mo>
       <mrow>
        <mi>x</mi>
        <mo>‚àà</mo>
        <mi class="ltx_font_mathcaligraphic">ùí≥</mi>
       </mrow>
      </mrow>
      <mo stretchy="false">}</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>x</ci>
      <ci>normal-‚àó</ci>
     </apply>
    </apply>
    <apply>
     <min></min>
     <apply>
      <ci>normal-:</ci>
      <apply>
       <times></times>
       <ci>f</ci>
       <ci>x</ci>
      </apply>
      <apply>
       <in></in>
       <ci>x</ci>
       <ci>ùí≥</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x^{\ast})=\min\{f(x):x\in\mathcal{X}\},
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Convex_optimization:13">
 <semantics>
  <mrow>
   <mi class="ltx_font_mathcaligraphic">ùí≥</mi>
   <mo>‚äÇ</mo>
   <msup>
    <mi>‚Ñù</mi>
    <mi>n</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <subset></subset>
    <ci>ùí≥</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>‚Ñù</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{X}\subset\mathbb{R}^{n}
  </annotation>
 </semantics>
</math>

 is the <em>feasible set</em> and 

<math display="inline" id="Convex_optimization:14">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>:</mo>
   <mrow>
    <msup>
     <mi>‚Ñù</mi>
     <mi>n</mi>
    </msup>
    <mo>‚Üí</mo>
    <mi>‚Ñù</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <ci>normal-‚Üí</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>‚Ñù</ci>
      <ci>n</ci>
     </apply>
     <ci>‚Ñù</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x):\mathbb{R}^{n}\rightarrow\mathbb{R}
  </annotation>
 </semantics>
</math>

 is the <em>objective</em>, is called <em>convex</em> if 

<math display="inline" id="Convex_optimization:15">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ùí≥</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ùí≥</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{X}
  </annotation>
 </semantics>
</math>

 is a closed convex set and 

<math display="inline" id="Convex_optimization:16">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)
  </annotation>
 </semantics>
</math>


 is convex on 

<math display="inline" id="Convex_optimization:17">
 <semantics>
  <msup>
   <mi>‚Ñù</mi>
   <mi>n</mi>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>‚Ñù</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbb{R}^{n}
  </annotation>
 </semantics>
</math>

. <a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> <a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>Alternatively, an optimization problem of the form</p>

<p>

<math display="inline" id="Convex_optimization:18">
 <semantics>
  <mo>minimize</mo>
  <annotation-xml encoding="MathML-Content">
   <ci>minimize</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\operatorname{minimize}
  </annotation>
 </semantics>
</math>


 is called convex if the functions 

<math display="inline" id="Convex_optimization:19">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mo>,</mo>
    <mrow>
     <msub>
      <mi>g</mi>
      <mn>1</mn>
     </msub>
     <mi mathvariant="normal">‚Ä¶</mi>
     <msub>
      <mi>g</mi>
      <mi>m</mi>
     </msub>
    </mrow>
   </mrow>
   <mo>:</mo>
   <mrow>
    <msup>
     <mi>‚Ñù</mi>
     <mi>n</mi>
    </msup>
    <mo>‚Üí</mo>
    <mi>‚Ñù</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <list>
     <ci>f</ci>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>g</ci>
       <cn type="integer">1</cn>
      </apply>
      <ci>normal-‚Ä¶</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>g</ci>
       <ci>m</ci>
      </apply>
     </apply>
    </list>
    <apply>
     <ci>normal-‚Üí</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>‚Ñù</ci>
      <ci>n</ci>
     </apply>
     <ci>‚Ñù</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f,g_{1}\ldots g_{m}:\mathbb{R}^{n}\rightarrow\mathbb{R}
  </annotation>
 </semantics>
</math>

 are convex.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="theory">Theory</h2>

<p>The following statements are true about the convex minimization problem:</p>
<ul>
<li>if a <a href="local_minimum" title="wikilink">local minimum</a> exists, then it is a <a href="global_minimum" title="wikilink">global minimum</a>.</li>
<li>the set of all (global) minima is convex.</li>
<li>for each <em>strictly</em> convex function, if the function has a minimum, then the minimum is unique.</li>
</ul>

<p>These results are used by the theory of convex minimization along with geometric notions from <a href="functional_analysis" title="wikilink">functional analysis</a> (in Hilbert spaces) such as the <a href="Hilbert_projection_theorem" title="wikilink">Hilbert projection theorem</a>, the <a href="separating_hyperplane_theorem" title="wikilink">separating hyperplane theorem</a>, and <a href="Farkas'_lemma" title="wikilink">Farkas' lemma</a>.</p>
<h2 id="standard-form">Standard form</h2>

<p><em>Standard form</em> is the usual and most intuitive form of describing a convex minimization problem. It consists of the following three parts:</p>
<ul>
<li>A <strong>convex function</strong> 

<math display="inline" id="Convex_optimization:20">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>:</mo>
   <mrow>
    <msup>
     <mi>‚Ñù</mi>
     <mi>n</mi>
    </msup>
    <mo>‚Üí</mo>
    <mi>‚Ñù</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <ci>normal-‚Üí</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>‚Ñù</ci>
      <ci>n</ci>
     </apply>
     <ci>‚Ñù</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x):\mathbb{R}^{n}\to\mathbb{R}
  </annotation>
 </semantics>
</math>

 to be minimized over the variable 

<math display="inline" id="Convex_optimization:21">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

</li>
<li><strong>Inequality constraints</strong> of the form 

<math display="inline" id="Convex_optimization:22">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>g</mi>
     <mi>i</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>‚â§</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>g</ci>
      <ci>i</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g_{i}(x)\leq 0
  </annotation>
 </semantics>
</math>

, where the functions 

<math display="inline" id="Convex_optimization:23">
 <semantics>
  <msub>
   <mi>g</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>g</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g_{i}
  </annotation>
 </semantics>
</math>

 are convex</li>
<li><strong>Equality constraints</strong> of the form 

<math display="inline" id="Convex_optimization:24">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>h</mi>
     <mi>i</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>h</ci>
      <ci>i</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h_{i}(x)=0
  </annotation>
 </semantics>
</math>

, where the functions 

<math display="inline" id="Convex_optimization:25">
 <semantics>
  <msub>
   <mi>h</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>h</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h_{i}
  </annotation>
 </semantics>
</math>

 are <a href="affine_transformation" title="wikilink">affine</a>. In practice, the terms "linear" and "affine" are often used interchangeably. Such constraints can be expressed in the form 

<math display="inline" id="Convex_optimization:26">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>h</mi>
     <mi>i</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <msubsup>
      <mi>a</mi>
      <mi>i</mi>
      <mi>T</mi>
     </msubsup>
     <mi>x</mi>
    </mrow>
    <mo>+</mo>
    <msub>
     <mi>b</mi>
     <mi>i</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>h</ci>
      <ci>i</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>a</ci>
        <ci>i</ci>
       </apply>
       <ci>T</ci>
      </apply>
      <ci>x</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>b</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h_{i}(x)=a_{i}^{T}x+b_{i}
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Convex_optimization:27">
 <semantics>
  <msub>
   <mi>a</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>a</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a_{i}
  </annotation>
 </semantics>
</math>

 is a column-vector and 

<math display="inline" id="Convex_optimization:28">
 <semantics>
  <msub>
   <mi>b</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>b</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b_{i}
  </annotation>
 </semantics>
</math>

 a real number.</li>
</ul>

<p>A convex minimization problem is thus written as</p>

<p>

<math display="inline" id="Convex_optimization:29">
 <semantics>
  <munder accentunder="true">
   <mo>minimize</mo>
   <mo>ùë•</mo>
  </munder>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>x</ci>
    <ci>minimize</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\underset{x}{\operatorname{minimize}}
  </annotation>
 </semantics>
</math>


</p>

<p>Note that every equality constraint 

<math display="inline" id="Convex_optimization:30">
 <semantics>
  <mrow>
   <mrow>
    <mi>h</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>h</ci>
     <ci>x</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h(x)=0
  </annotation>
 </semantics>
</math>

 can be equivalently replaced by a pair of inequality constraints 

<math display="inline" id="Convex_optimization:31">
 <semantics>
  <mrow>
   <mrow>
    <mi>h</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>‚â§</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <apply>
     <times></times>
     <ci>h</ci>
     <ci>x</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h(x)\leq 0
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Convex_optimization:32">
 <semantics>
  <mrow>
   <mrow>
    <mo>-</mo>
    <mrow>
     <mi>h</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>‚â§</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <ci>h</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   -h(x)\leq 0
  </annotation>
 </semantics>
</math>

. Therefore, for theoretical purposes, equality constraints are redundant; however, it can be beneficial to treat them specially in practice.</p>

<p>Following from this fact, it is easy to understand why 

<math display="inline" id="Convex_optimization:33">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>h</mi>
     <mi>i</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>h</ci>
      <ci>i</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h_{i}(x)=0
  </annotation>
 </semantics>
</math>

 has to be affine as opposed to merely being convex. If 

<math display="inline" id="Convex_optimization:34">
 <semantics>
  <mrow>
   <msub>
    <mi>h</mi>
    <mi>i</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>h</ci>
     <ci>i</ci>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h_{i}(x)
  </annotation>
 </semantics>
</math>

 is convex, 

<math display="inline" id="Convex_optimization:35">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>h</mi>
     <mi>i</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>‚â§</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>h</ci>
      <ci>i</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h_{i}(x)\leq 0
  </annotation>
 </semantics>
</math>

 is convex, but 

<math display="inline" id="Convex_optimization:36">
 <semantics>
  <mrow>
   <mrow>
    <mo>-</mo>
    <mrow>
     <msub>
      <mi>h</mi>
      <mi>i</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>‚â§</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>h</ci>
       <ci>i</ci>
      </apply>
      <ci>x</ci>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   -h_{i}(x)\leq 0
  </annotation>
 </semantics>
</math>

 is <em>concave</em>. Therefore, the only way for 

<math display="inline" id="Convex_optimization:37">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>h</mi>
     <mi>i</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>h</ci>
      <ci>i</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h_{i}(x)=0
  </annotation>
 </semantics>
</math>

 to be convex is for 

<math display="inline" id="Convex_optimization:38">
 <semantics>
  <mrow>
   <msub>
    <mi>h</mi>
    <mi>i</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>h</ci>
     <ci>i</ci>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h_{i}(x)
  </annotation>
 </semantics>
</math>

 to be affine.</p>
<h2 id="examples">Examples</h2>

<p>The following problems are all convex minimization problems, or can be transformed into convex minimizations problems via a change of variables:</p>
<ul>
<li><a href="Least_squares" title="wikilink">Least squares</a></li>
<li><a href="Linear_programming" title="wikilink">Linear programming</a></li>
<li>Convex <a href="quadratic_programming" title="wikilink">quadratic minimization</a> with linear constraints</li>
<li><a href="Quadratically_constrained_quadratic_programming" title="wikilink">quadratic minimization with convex quadratic constraints</a></li>
<li><a href="Conic_optimization" title="wikilink">Conic optimization</a></li>
<li><a href="Geometric_programming" title="wikilink">Geometric programming</a></li>
<li><a href="Second_order_cone_programming" title="wikilink">Second order cone programming</a></li>
<li><a href="Semidefinite_programming" title="wikilink">Semidefinite programming</a></li>
<li><a href="Entropy_maximization" title="wikilink">Entropy maximization</a> with appropriate constraints</li>
</ul>
<h2 id="lagrange-multipliers">Lagrange multipliers</h2>

<p>Consider a convex minimization problem given in standard form by a cost function 

<math display="inline" id="Convex_optimization:39">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x)
  </annotation>
 </semantics>
</math>

 and inequality constraints 

<math display="inline" id="Convex_optimization:40">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>g</mi>
     <mi>i</mi>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>‚â§</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>g</ci>
      <ci>i</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g_{i}(x)\leq 0
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Convex_optimization:41">
 <semantics>
  <mrow>
   <mi>i</mi>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mi mathvariant="normal">‚Ä¶</mi>
    <mi>m</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>i</ci>
    <apply>
     <times></times>
     <cn type="integer">1</cn>
     <ci>normal-‚Ä¶</ci>
     <ci>m</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i=1\ldots m
  </annotation>
 </semantics>
</math>

. Then the domain 

<math display="inline" id="Convex_optimization:42">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ùí≥</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ùí≥</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{X}
  </annotation>
 </semantics>
</math>

 is:</p>

<p>

<math display="block" id="Convex_optimization:43">
 <semantics>
  <mrow>
   <mrow>
    <mi class="ltx_font_mathcaligraphic">ùí≥</mi>
    <mo>=</mo>
    <mrow>
     <mo>{</mo>
     <mrow>
      <mi>x</mi>
      <mo>‚àà</mo>
      <mi>X</mi>
     </mrow>
     <mo stretchy="false">|</mo>
     <mrow>
      <mrow>
       <mrow>
        <msub>
         <mi>g</mi>
         <mn>1</mn>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>‚â§</mo>
       <mrow>
        <mn>0</mn>
        <mo>,</mo>
        <mi mathvariant="normal">‚Ä¶</mi>
       </mrow>
      </mrow>
      <mo>,</mo>
      <mrow>
       <mrow>
        <msub>
         <mi>g</mi>
         <mi>m</mi>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>‚â§</mo>
       <mn>0</mn>
      </mrow>
     </mrow>
     <mo>}</mo>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>ùí≥</ci>
    <apply>
     <csymbol cd="latexml">conditional-set</csymbol>
     <apply>
      <in></in>
      <ci>x</ci>
      <ci>X</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">formulae-sequence</csymbol>
      <apply>
       <leq></leq>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>g</ci>
         <cn type="integer">1</cn>
        </apply>
        <ci>x</ci>
       </apply>
       <list>
        <cn type="integer">0</cn>
        <ci>normal-‚Ä¶</ci>
       </list>
      </apply>
      <apply>
       <leq></leq>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>g</ci>
         <ci>m</ci>
        </apply>
        <ci>x</ci>
       </apply>
       <cn type="integer">0</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{X}=\left\{{x\in X|g_{1}(x)\leq 0,\ldots,g_{m}(x)\leq 0}\right\}.
  </annotation>
 </semantics>
</math>

</p>

<p>The <a href="Lagrange_multipliers" title="wikilink">Lagrangian function</a> for the problem is</p>
<dl>
<dd><em>L</em>(<em>x</em>,<em>Œª</em><sub>0</sub>,...,<em>Œª</em><sub><em>m</em></sub>) = <em>Œª</em><sub>0</sub><em>f</em>(<em>x</em>) + <em>Œª</em><sub>1</sub><em>g</em><sub>1</sub>(<em>x</em>) + ... + <em>Œª</em><sub><em>m</em></sub><em>g</em><sub><em>m</em></sub>(<em>x</em>).
</dd>
</dl>

<p>For each point <em>x</em> in <em>X</em> that minimizes <em>f</em> over <em>X</em>, there exist real numbers <em>Œª</em><sub>0</sub>, ..., <em>Œª</em><sub>m</sub>, called <a href="Lagrange_multipliers" title="wikilink">Lagrange multipliers</a>, that satisfy these conditions simultaneously:</p>
<ol>
<li><em>x</em> minimizes <em>L</em>(<em>y</em>, Œª<sub>0</sub>, Œª<sub>1</sub>, ..., Œª<sub>m</sub>) over all <em>y</em> in <em>X</em>,</li>
<li>Œª<sub>0</sub> ‚â• 0, Œª<sub>1</sub> ‚â• 0, ..., Œª<sub><em>m</em></sub> ‚â• 0, with at least one Œª<sub><em>k</em></sub>&gt;0,</li>
<li><em>Œª</em><sub>1</sub><em>g</em><sub>1</sub>(<em>x</em>) = 0, ..., <em>Œª</em><sub><em>m</em></sub><em>g</em><sub><em>m</em></sub>(<em>x</em>) = 0 (complementary slackness).</li>
</ol>

<p>If there exists a "strictly feasible point", i.e., a point <em>z</em> satisfying</p>
<dl>
<dd><em>g</em><sub>1</sub>(<em>z</em>) <em>m</em>(<em>z</em>) &lt; 0,
</dd>
</dl>

<p>then the statement above can be upgraded to assert that Œª<sub>0</sub>=1.</p>

<p>Conversely, if some <em>x</em> in <em>X</em> satisfies 1-3 for <a href="scalar_(mathematics)" title="wikilink">scalars</a> Œª<sub>0</sub>, ..., Œª<sub><em>m</em></sub> with Œª<sub>0</sub> = 1, then <em>x</em> is certain to minimize <em>f</em> over <em>X</em>.</p>
<h2 id="methods">Methods</h2>

<p>Convex minimization problems can be solved by the following contemporary methods:<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<ul>
<li>"Bundle methods" (Wolfe, Lemar√©chal, Kiwiel), and</li>
<li><a href="Subgradient_method#Subgradient-projection_&amp;_bundle_methods" title="wikilink">Subgradient projection</a> methods (Polyak),</li>
<li><a href="Interior-point_methods" title="wikilink">Interior-point methods</a> (Nemirovskii and Nesterov).</li>
</ul>

<p>Other methods of interest:</p>
<ul>
<li><a href="Cutting-plane_methods" title="wikilink">Cutting-plane methods</a></li>
<li><a href="Ellipsoid_method" title="wikilink">Ellipsoid method</a></li>
<li><a href="Subgradient_method" title="wikilink">Subgradient method</a></li>
<li><a href="Drift_plus_penalty" title="wikilink">Dual subgradients and the drift-plus-penalty method</a></li>
</ul>

<p>Subgradient methods can be implemented simply and so are widely used.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> Dual subgradient methods are subgradient methods applied to a <a href="Duality_(optimization)" title="wikilink">dual problem</a>. The <a href="Drift_plus_penalty" title="wikilink">drift-plus-penalty</a> method is similar to the dual subgradient method, but takes a time average of the primal variables.</p>
<h2 id="convex-minimization-with-good-complexity-self-concordant-barriers">Convex minimization with good complexity: Self-concordant barriers</h2>

<p>The efficiency of iterative methods is poor for the class of convex problems, because this class includes "bad guys" whose minimum cannot be approximated without a large number of function and subgradient evaluations;<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> thus, to have practically appealing efficiency results, it is necessary to make additional restrictions on the class of problems. Two such classes are problems special <a href="barrier_function" title="wikilink">barrier functions</a>, first <em>self-concordant</em> barrier functions, according to the theory of Nesterov and Nemirovskii, and second <em>self-regular</em> barrier functions according to the theory of Terlaky and coauthors.</p>
<h2 id="quasiconvex-minimization">Quasiconvex minimization</h2>

<p>Problems with convex level sets can be efficiently minimized, in theory. <a href="Yurii_Nesterov" title="wikilink">Yurii Nesterov</a> proved that quasi-convex minimization problems could be solved efficiently, and his results were extended by Kiwiel.<ref>In <a href="Analysis_of_algorithms" title="wikilink">theory</a>, quasiconvex programming and convex programming problems can be solved in reasonable amount of time, where the number of iterations grows like a polynomial in the dimension of the problem (and in the reciprocal of the approximation error tolerated):</ref></p>

<p>Kiwiel acknowledges that <a href="Yurii_Nesterov" title="wikilink">Yurii Nesterov</a> first established that quasiconvex minimization problems can be solved efficiently.</p>

<p> However, such theoretically "efficient" methods use "divergent-series" <a href="gradient_descent#Stepsize_rules" title="wikilink">stepsize rules</a>, which were first developed for classical <a href="subgradient_method" title="wikilink">subgradient methods</a>. Classical subgradient methods using divergent-series rules are much slower than modern methods of convex minimization, such as subgradient projection methods, <a href="bundle_method" title="wikilink">bundle methods</a> of descent, and nonsmooth <a href="filter_method" title="wikilink">filter methods</a>.</p>

<p>Solving even close-to-convex but non-convex problems can be computationally intractable. Minimizing a unimodal function is intractable, regardless of the smoothness of the function, according to results of Ivanov.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h2 id="convex-maximization">Convex maximization</h2>

<p>Conventionally, the definition of the convex optimization problem (we recall) requires that the objective function <em>f</em> to be <em>minimized</em> and the feasible set be convex. In the special case of linear programming (LP), the objective function is both concave and convex, and so LP can also consider the problem of maximizing an objective function without confusion. However, for most convex minimization problems, the objective function is not concave, and therefore a problem and then such problems are formulated in the standard form of convex optimization problems, that is, minimizing the convex objective function.</p>

<p>For nonlinear convex minimization, the associated maximization problem obtained by substituting the <a class="uri" href="supremum" title="wikilink">supremum</a> operator for the <a class="uri" href="infimum" title="wikilink">infimum</a> operator is not a problem of convex optimization, as conventionally defined. However, it is studied in the larger field of convex optimization as a problem of convex maximization.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>

<p>The convex maximization problem is especially important for studying the existence of maxima. Consider the restriction of a convex function to a <a href="compact_set" title="wikilink">compact</a> convex set: Then, on that set, the function attains its constrained <em>maximum</em> only on the boundary.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> Such results, called "<a href="maximum_principle" title="wikilink">maximum principles</a>", are useful in the theory of <a href="harmonic_functions" title="wikilink">harmonic functions</a>, <a href="potential_theory" title="wikilink">potential theory</a>, and <a href="partial_differential_equation" title="wikilink">partial differential equations</a>.</p>

<p>The problem of minimizing a <a href="quadratic_polynomial" title="wikilink">quadratic</a> <a href="multivariate_polynomial" title="wikilink">multivariate polynomial</a> on a <a class="uri" href="cube" title="wikilink">cube</a> is <a class="uri" href="NP-hard" title="wikilink">NP-hard</a>.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> In fact, in the <a href="quadratic_programming" title="wikilink">quadratic minimization</a> problem, if the matrix has only one negative <a class="uri" href="eigenvalue" title="wikilink">eigenvalue</a>, is <a class="uri" href="NP-hard" title="wikilink">NP-hard</a>.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></p>
<h2 id="extensions">Extensions</h2>

<p>Advanced treatments consider convex functions that can attain positive infinity, also; the <a href="characteristic_function_(convex_analysis)" title="wikilink">indicator function</a> of convex analysis is zero for every 

<math display="inline" id="Convex_optimization:44">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>‚àà</mo>
   <mi class="ltx_font_mathcaligraphic">ùí≥</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>x</ci>
    <ci>ùí≥</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x\in\mathcal{X}
  </annotation>
 </semantics>
</math>

 and positive infinity otherwise.</p>

<p>Extensions of convex functions include <a href="Biconvex_optimization" title="wikilink">biconvex</a>, <a href="pseudo-convex_function" title="wikilink">pseudo-convex</a>, and <a href="quasi-convex_function" title="wikilink">quasi-convex functions</a>. Partial extensions of the theory of convex analysis and iterative methods for approximately solving non-convex minimization problems occur in the field of <a href="Convexity_(mathematics)#Generalizations_and_extensions_for_convexity" title="wikilink">generalized convexity</a> ("abstract convex analysis").</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Duality_(optimization)" title="wikilink">Duality</a></li>
<li><a href="Karush‚ÄìKuhn‚ÄìTucker_conditions" title="wikilink">Karush‚ÄìKuhn‚ÄìTucker conditions</a></li>
<li><a href="Optimization_problem" title="wikilink">Optimization problem</a></li>
<li><a href="Proximal_gradient_method" title="wikilink">Proximal gradient method</a></li>
</ul>
<h2 id="notes">Notes</h2>
<references>
</references>
<h2 id="references">References</h2>
<ul>
<li></li>
<li></li>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li><a href="Jonathan_M._Borwein" title="wikilink">Borwein, Jonathan</a>, and Lewis, Adrian. (2000). <em>Convex Analysis and Nonlinear Optimization</em>. Springer.</li>
</ul>
<ul>
<li>Hiriart-Urruty, Jean-Baptiste, and <a href="Claude_Lemar√©chal" title="wikilink">Lemar√©chal, Claude</a>. (2004). <em>Fundamentals of Convex analysis</em>. Berlin: Springer.</li>
</ul>
<ul>
<li></li>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li>Nesterov, Y. and Nemirovsky, A. (1994). 'Interior Point Polynomial Methods in Convex Programming.'' SIAM</li>
</ul>
<ul>
<li>Nesterov, Yurii. (2004). <em>Introductory Lectures on Convex Optimization</em>, Kluwer Academic Publishers</li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li>Stephen Boyd and Lieven Vandenberghe, <a href="http://www.stanford.edu/~boyd/cvxbook/"><em>Convex optimization</em></a> (book in pdf)</li>
<li><a href="http://www.stanford.edu/class/ee364a/">EE364a: Convex Optimization I</a> and <a href="http://www.stanford.edu/class/ee364b/">EE364b: Convex Optimization II</a>, Stanford course homepages</li>
<li><a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-253-convex-analysis-and-optimization-spring-2010/">6.253: Convex Analysis and Optimization</a>, an MIT OCW course homepage</li>
<li>Brian Borchers, <a href="http://infohost.nmt.edu/~borchers/presentation.pdf">An overview of software for convex optimization</a></li>
</ul>

<p>"</p>

<p><a href="Category:Mathematical_optimization" title="wikilink">Category:Mathematical optimization</a> <a href="Category:Convex_analysis" title="wikilink">Category:Convex analysis</a> <a href="Category:Convex_optimization" title="wikilink"> </a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">‚Ü©</a></li>
<li id="fn2"><a href="#fnref2">‚Ü©</a></li>
<li id="fn3">Boyd/Vandenberghe, p. 7<a href="#fnref3">‚Ü©</a></li>
<li id="fn4">For methods for convex minimization, see the volumes by Hiriart-Urruty and Lemar√©chal (bundle) and the textbooks by <a href="Andrzej_Piotr_Ruszczy≈Ñski" title="wikilink">Ruszczy≈Ñski</a>, <a href="Dimitri_Bertsekas" title="wikilink">Bertsekas</a>, and Boyd and Vandenberghe (interior point).<a href="#fnref4">‚Ü©</a></li>
<li id="fn5">Bertsekas<a href="#fnref5">‚Ü©</a></li>
<li id="fn6"> discuss a "bad guy" constructed by Arkadi Nemirovskii.<a href="#fnref6">‚Ü©</a></li>
<li id="fn7">Nemirovskii and Judin<a href="#fnref7">‚Ü©</a></li>
<li id="fn8">Convex maximization is mentioned in the subsection on convex optimization in this textbook: [<a class="uri" href="http://books.google.se/books?id=9sbsMkuFzhYC&amp;pg">http://books.google.se/books?id=9sbsMkuFzhYC&amp;pg;</a>;=PA206&amp;dq;=%22convex+maximization%22,+%22convex+minimization%22+OR+%22convex+optimization%22&amp;hl;=sv&amp;sa;=X&amp;ei;=YswrT8-kGqfV4QTKs8CwDg&amp;ved;=0CF8Q6AEwCA#v=onepage&amp;q;=%22convex%20maximization%22%2C%20%22convex%20minimization%22%20OR%20%22convex%20optimization%22&amp;f;=false Ulrich Faigle, Walter Kern, and George Still. <em>Algorithmic principles of mathematical programming</em>. Springer-Verlag. Texts in Mathematics. Chapter 10.2, Subsection "Convex optimization", pages 205-206.]<a href="#fnref8">‚Ü©</a></li>
<li id="fn9">Theorem 32.1 in Rockafellar's <em>Convex Analysis</em> states this <a href="maximum_principle" title="wikilink">maximum principle</a> for extended real-valued functions.<a href="#fnref9">‚Ü©</a></li>
<li id="fn10">Sahni, S. "Computationally related problems," in SIAM Journal on Computing, 3, 262--279, 1974.<a href="#fnref10">‚Ü©</a></li>
<li id="fn11">Quadratic programming with one negative eigenvalue is NP-hard, Panos M. Pardalos and Stephen A. Vavasis in <em>Journal of Global Optimization</em>, Volume 1, Number 1, 1991, pg.15-22.<a href="#fnref11">‚Ü©</a></li>
</ol>
</section>
</body>
</html>
