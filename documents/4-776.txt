   Bhattacharyya distance      Bhattacharyya distance   In statistics , the Bhattacharyya distance measures the similarity of two discrete or continuous probability distributions . It is closely related to the Bhattacharyya coefficient which is a measure of the amount of overlap between two statistical samples or populations. Both measures are named after Anil Kumar Bhattacharya , a statistician who worked in the 1930s at the Indian Statistical Institute . 1  The coefficient can be used to determine the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the Mahalanobis distance , as the Mahalanobis distance is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same. Therefore, when two classes have similar means but different standard deviations, the Mahalanobis distance would tend to zero, however, the Bhattacharyya distance would grow depending on the difference between the standard deviations. (this paragraph is possibly wrong information.)  Definition  For discrete probability distributions  p and q over the same domain  X , the Bhattacharyya distance is defined as:        D  B    (  p  ,  q  )    =   -   ln   (   B  C   (  p  ,  q  )    )            subscript  D  B    p  q          B  C   p  q        D_{B}(p,q)=-\ln\left(BC(p,q)\right)   where:       B  C   (  p  ,  q  )    =    ∑   x  ∈  X      p   (  x  )   q   (  x  )            B  C   p  q      subscript     x  X        p  x  q  x       BC(p,q)=\sum_{x\in X}\sqrt{p(x)q(x)}   is the Bhattacharyya coefficient .  For continuous probability distributions , the Bhattacharyya coefficient is defined as:       B  C   (  p  ,  q  )    =   ∫      p   (  x  )   q   (  x  )      d  x          B  C   p  q            p  x  q  x    d  x      BC(p,q)=\int\sqrt{p(x)q(x)}\,dx     In either case,    0  ≤   B  C   ≤  1        0    B  C        1     0\leq BC\leq 1   and    0  ≤   D  B   ≤  ∞        0   subscript  D  B            0\leq D_{B}\leq\infty   .    D  B     subscript  D  B    D_{B}   does not obey the triangle inequality , but the Hellinger distance      1  -   B  C          1    B  C      \sqrt{1-BC}   does obey the triangle inequality.  In its simplest formulation, the Bhattacharyya distance between two classes under the normal distribution can be calculated 2 by extracting the mean and variances of two separate distributions or classes:        D  B    (  p  ,  q  )    =     1  4    ln   (    1  4    (     σ  p  2    σ  q  2    +    σ  q  2    σ  p  2    +  2   )    )     +    1  4    (     (    μ  p   -   μ  q    )   2     σ  p  2   +   σ  q  2     )            subscript  D  B    p  q          1  4         1  4        superscript   subscript  σ  p   2    superscript   subscript  σ  q   2       superscript   subscript  σ  q   2    superscript   subscript  σ  p   2    2          1  4      superscript     subscript  μ  p    subscript  μ  q    2      superscript   subscript  σ  p   2    superscript   subscript  σ  q   2         D_{B}(p,q)=\frac{1}{4}\ln\left(\frac{1}{4}\left(\frac{\sigma_{p}^{2}}{\sigma_{%
 q}^{2}}+\frac{\sigma_{q}^{2}}{\sigma_{p}^{2}}+2\right)\right)+\frac{1}{4}\left%
 (\frac{(\mu_{p}-\mu_{q})^{2}}{\sigma_{p}^{2}+\sigma_{q}^{2}}\right)     where:             D  B    (  p  ,  q  )        subscript  D  B    p  q     D_{B}(p,q)      is the Bhattacharyya distance between p and q distributions or classes,         σ  p     subscript  σ  p    \sigma_{p}      is the variance of the p -th distribution,         μ  p     subscript  μ  p    \mu_{p}      is the mean of the p -th distribution, and         p  ,  q     p  q    p,q      are two different distributions.       The Mahalanobis distance used in Fisher's linear discriminant analysis is a particular case of the Bhattacharyya Distance. When the variances of the two distributions are the same the first term of the distance is zero as this term depends solely on the variances of the distributions (left case of the figure). The first term will grow as the variances differ (right case of the figure). The second term, on the other hand, will be zero if the means are equal and is inversely proportional to the variances.  For multivariate normal distributions     p  i   =   𝒩   (   𝝁  i   ,   𝚺  i   )         subscript  p  i     𝒩    subscript  𝝁  i    subscript  𝚺  i       p_{i}=\mathcal{N}(\boldsymbol{\mu}_{i},\,\boldsymbol{\Sigma}_{i})   ,        D  B   =     1  8     (    𝝁  1   -   𝝁  2    )   T    𝚺   -  1     (    𝝁  1   -   𝝁  2    )    +    1  2     ln    (    det  𝚺     det     𝚺  1     det   𝚺  2        )       ,       subscript  D  B         1  8    superscript     subscript  𝝁  1    subscript  𝝁  2    T    superscript  𝚺    1       subscript  𝝁  1    subscript  𝝁  2         1  2         𝚺          subscript  𝚺  1      subscript  𝚺  2             D_{B}={1\over 8}(\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2})^{T}\boldsymbol{%
 \Sigma}^{-1}(\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2})+{1\over 2}\ln\,\left({%
 \det\boldsymbol{\Sigma}\over\sqrt{\det\boldsymbol{\Sigma}_{1}\,\det\boldsymbol%
 {\Sigma}_{2}}}\right),     where    𝝁  i     subscript  𝝁  i    \boldsymbol{\mu}_{i}   and    𝚺  i     subscript  𝚺  i    \boldsymbol{\Sigma}_{i}   are the means and covariances of the distributions, and       𝚺  =     𝚺  1   +   𝚺  2    2    .      𝚺       subscript  𝚺  1    subscript  𝚺  2    2     \boldsymbol{\Sigma}={\boldsymbol{\Sigma}_{1}+\boldsymbol{\Sigma}_{2}\over 2}.     Note that, in this case, the first term in the Bhattacharyya distance is related to the Mahalanobis distance .  Bhattacharyya coefficient  The Bhattacharyya coefficient is an approximate measurement of the amount of overlap between two statistical samples. The coefficient can be used to determine the relative closeness of the two samples being considered.  Calculating the Bhattacharyya coefficient involves a rudimentary form of integration of the overlap of the two samples. The interval of the values of the two samples is split into a chosen number of partitions , and the number of members of each sample in each partition is used in the following formula,        B  C   (  𝐩  ,  𝐪  )    =    ∑   i  =  1   n      p  i    q  i       ,        B  C   𝐩  𝐪      superscript   subscript     i  1    n        subscript  p  i    subscript  q  i        BC(\mathbf{p},\mathbf{q})=\sum_{i=1}^{n}\sqrt{p_{i}q_{i}},    3  where, considering the samples p and q , n is the number of partitions, and    p  i     subscript  p  i    p_{i}   ,    q  i     subscript  q  i    q_{i}   are the numbers of members of samples p and q in the i -th partition.  This formula hence is larger with each partition that has members from both sample, and larger with each partition that has a large overlap of the two sample's members within it. The choice of number of partitions depends on the number of members in each sample; too few partitions will lose accuracy by overestimating the overlap region, and too many partitions will lose accuracy by creating individual partitions with no members despite being in a densely populated sample space.  The Bhattacharyya coefficient will be 0 if there is no overlap at all due to the multiplication by zero in every partition. This means the distance between fully separated samples will not be exposed by this coefficient alone.  Applications  The Bhattacharyya distance is widely used in research of feature extraction and selection, 4 image processing, 5 speaker recognition, 6 phone clustering. 7  A "Bhattacharyya space" has been proposed as a feature selection technique that can be applied to texture segmentation. 8  A "Bhattacharyya coefficient" has also been proposed as a feature selection technique that can be used to estimate the given distance between the Bhattacharyya number and any given Nesli coordinate. 9  See also   Bhattacharyya angle  Kullback–Leibler divergence  Hellinger distance  Mahalanobis distance  Chernoff bound  Rényi entropy   References               For a short list of properties, see: http://www.mtm.ufsc.br/~taneja/book/node20.html   External links     "  Category:Statistical distance measures  Category:Statistical deviation and dispersion     ↩  Guy B. Coleman, Harry C. Andrews, "Image Segmentation by Clustering", Proc IEEE , Vol. 67, No. 5, pp. 773–785, 1979 ↩  D. Comaniciu, V. Ramesh, P. Meer, Real-Time Tracking of Non-Rigid Objects using Mean Shift , BEST PAPER AWARD, IEEE Conf. Computer Vision and Pattern Recognition (CVPR'00), Hilton Head Island, South Carolina, Vol. 2, 142–149, 2000 ↩  Euisun Choi, Chulhee Lee, "Feature extraction based on the Bhattacharyya distance", Pattern Recognition , Volume 36, Issue 8, August 2003, Pages 1703–1709 ↩  François Goudail, Philippe Réfrégier, Guillaume Delyon, "Bhattacharyya distance as a contrast parameter for statistical processing of noisy optical images", JOSA A , Vol. 21, Issue 7, pp. 1231−1240 (2004) ↩  Chang Huai You, "An SVM Kernel With GMM-Supervector Based on the Bhattacharyya Distance for Speaker Recognition", Signal Processing Letters , IEEE, Vol 16, Is 1, pp. 49-52 ↩  Mak, B., "Phone clustering using the Bhattacharyya distance", Spoken Language , 1996. ICSLP 96. Proceedings., Fourth International Conference on, Vol 4, pp. 2005–2008 vol.4, 3−6 Oct 1996 ↩  Reyes-Aldasoro, C.C., and A. Bhalerao, "The Bhattacharyya space for feature selection and its application to texture segmentation", Pattern Recognition , (2006) Vol. 39, Issue 5, May 2006, pp. 812–826 ↩      