   Law of the unconscious statistician      Law of the unconscious statistician   In probability theory and statistics , the law of the unconscious statistician (sometimes abbreviated LOTUS) is a theorem used to calculate the expected value of a function  g ( X ) of a random variable  X when one knows the probability distribution of X but one does not explicitly know the distribution of g ( X ).  The form of the law can depend on the form in which one states the probability distribution of the random variable X . If it is a discrete distribution and one knows its probability mass function  ƒ X (but not ƒ g(X) ), then the expected value of g ( X ) is        E   [   g   (  X  )    ]    =    ∑  x    g   (  x  )    f  X    (  x  )      ,       normal-E    g  X      subscript   x     g  x   subscript  f  X   x      \operatorname{E}[g(X)]=\sum_{x}g(x)f_{X}(x),\,     where the sum is over all possible values x of X . If it is a continuous distribution and one knows its probability density function  ƒ X (but not ƒ g(X) ), then the expected value of g ( X ) is       E   [   g   (  X  )    ]    =    ∫   -  ∞   ∞    g   (  x  )    f  X    (  x  )   d  x         normal-E    g  X      superscript   subscript            g  x   subscript  f  X   x  d  x      \operatorname{E}[g(X)]=\int_{-\infty}^{\infty}g(x)f_{X}(x)\,dx     (provided the values of X are real numbers as opposed to vectors, complex numbers, etc.).  Regardless of continuity-versus-discreteness and related issues, if one knows the cumulative probability distribution function  F X (but not F g(X) ), then the expected value of g ( X ) is given by a Riemann–Stieltjes integral       E   [   g   (  X  )    ]    =    ∫   -  ∞   ∞    g   (  x  )   d   F  X    (  x  )          normal-E    g  X      superscript   subscript            g  x  d   subscript  F  X   x      \operatorname{E}[g(X)]=\int_{-\infty}^{\infty}g(x)\,dF_{X}(x)     (again assuming X is real-valued). 1 2  However, the result is so well known that it is usually used without stating a name for it: the name is not extensively used. For justifications of the result for discrete and continuous random variables see. 3  From the perspective of measure  A technically complete derivation of the result is available using arguments in measure theory , in which the probability space of a transformed random variable  g ( X ) is related to that of the original random variable X . The steps here involve defining a pushforward measure for the transformed space, and the result is then an example of a change of variables formula . 4        ∫  Ω     g  ∘  X   d  P    =    ∫  ℝ    g  d   (    X  *   P   )           subscript   normal-Ω       g  X   d  P      subscript   ℝ     g  d     subscript  X    P       \int_{\Omega}g\circ XdP=\int_{\mathbb{R}}gd(X_{*}P)     We say   X   X   X   has a density if    d   (    X  *   P   )       d     subscript  X    P     d(X_{*}P)   is absolutely continuous with respect to the Lebesgue measure   μ   μ   \mu   . In that case       d   (    X  *   P   )    =   f   d  μ          d     subscript  X    P      f   subscript  d  μ      d(X_{*}P)=fd_{\mu}     where    f  :   ℝ  →  ℝ      normal-:  f   normal-→  ℝ  ℝ     f:\mathbb{R}\to\mathbb{R}   is the density (see Radon-Nikodym derivative ). So the above can be rewritten as the more familiar       E   [   g   (  X  )    ]    =    ∫  Ω     g  ∘  X   d  P    =    ∫  ℝ    g   (  x  )   f   (  x  )   d  x           normal-E    g  X      subscript   normal-Ω       g  X   d  P           subscript   ℝ     g  x  f  x  d  x       \operatorname{E}[g(X)]=\int_{\Omega}g\circ XdP=\int_{\mathbb{R}}g(x)f(x)dx     References  "  Category:Theory of probability distributions  Category:Statistical laws     Eric Key (1998) Lecture 6: Random variables , Lecture notes, University of Leeds ↩  Bengt Ringner (2009) "Law of the unconscious statistician" , unpublished note, Centre for Mathematical Sciences, Lund University ↩  Virtual Laboratories in Probability and Statistics , Sect. 3.1 "Expected Value: Definition and Properties", item "Basic Results: Change of Variables Theorem". ↩  S.R.Srinivasa Varadhan (2002) Lecture notes on Limit Theorems , NYU (Section 1.4) ↩     