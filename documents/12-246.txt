   Stein's unbiased risk estimate      Stein's unbiased risk estimate   In statistics , Stein's unbiased risk estimate (SURE) is an unbiased  estimator of the mean-squared error of "a nearly arbitrary, nonlinear biased estimator." 1 In other words, it provides an indication of the accuracy of a given estimator. This is important since the true mean-squared error of an estimator is a function of the unknown parameter to be estimated, and thus cannot be determined exactly.  The technique is named after its discoverer, Charles Stein . 2  Formal statement  Let    μ  ∈   ℝ  d       μ   superscript  ℝ  d     \mu\in{\mathbb{R}}^{d}   be an unknown parameter and let    x  ∈   ℝ  d       x   superscript  ℝ  d     x\in{\mathbb{R}}^{d}   be a measurement vector whose components are independent and distributed normally with mean   μ   μ   \mu   and variance    σ  2     superscript  σ  2    \sigma^{2}   . Suppose    h   (  x  )       h  x    h(x)   is an estimator of   μ   μ   \mu   from   x   x   x   , and can be written     h   (  x  )    =   x  +   g   (  x  )           h  x     x    g  x      h(x)=x+g(x)   , where   g   g   g   is weakly differentiable . Then, Stein's unbiased risk estimate is given by 3        SURE   (  h  )    =    d   σ  2    +    ∥   g   (  x  )    ∥   2   +   2   σ  2     ∑   i  =  1   d     ∂   ∂   x  i      g  i    (  x  )        ,        SURE  h       d   superscript  σ  2     superscript   norm    g  x    2     2   superscript  σ  2     superscript   subscript     i  1    d           subscript  x  i      subscript  g  i   x        \mathrm{SURE}(h)=d\sigma^{2}+\|g(x)\|^{2}+2\sigma^{2}\sum_{i=1}^{d}\frac{%
 \partial}{\partial x_{i}}g_{i}(x),   where     g  i    (  x  )        subscript  g  i   x    g_{i}(x)   is the   i   i   i   th component of the function    g   (  x  )       g  x    g(x)   , and    ∥  ⋅  ∥     fragments  parallel-to  normal-⋅  parallel-to    \|\cdot\|   is the Euclidean norm .  The importance of SURE is that it is an unbiased estimate of the mean-squared error (or squared error risk) of    h   (  x  )       h  x    h(x)   , i.e.         E  μ    {   SURE   (  h  )    }    =   MSE   (  h  )     ,         subscript  E  μ      SURE  h       MSE  h     E_{\mu}\{\mathrm{SURE}(h)\}=\mathrm{MSE}(h),\,\!   with        MSE   (  h  )    =    E  μ     ∥    h   (  x  )    -  μ   ∥   2     .        MSE  h      subscript  E  μ    superscript   norm      h  x   μ    2      \mathrm{MSE}(h)=E_{\mu}\|h(x)-\mu\|^{2}.     Thus, minimizing SURE can act as a surrogate for minimizing the MSE. Note that there is no dependence on the unknown parameter   μ   μ   \mu   in the expression for SURE above. Thus, it can be manipulated (e.g., to determine optimal estimation settings) without knowledge of   μ   μ   \mu   .  Proof  We wish to show that        E  μ     ∥    h   (  x  )    -  μ   ∥   2    =    E  μ    {   SURE   (  h  )    }           subscript  E  μ    superscript   norm      h  x   μ    2       subscript  E  μ      SURE  h       E_{\mu}\|h(x)-\mu\|^{2}=E_{\mu}\{\mathrm{SURE}(h)\}   . We start by expanding the MSE as       E  μ     ∥    h   (  x  )    -  μ   ∥   2        subscript  E  μ    superscript   norm      h  x   μ    2     \displaystyle E_{\mu}\|h(x)-\mu\|^{2}   Now we use integration by parts to rewrite the last term:       E  μ   g    (  x  )   T    (   x  -  μ   )        subscript  E  μ   g   superscript  x  T     x  μ     \displaystyle E_{\mu}g(x)^{T}(x-\mu)   Substituting this into the expression for the MSE, we arrive at         E  μ     ∥    h   (  x  )    -  μ   ∥   2    =    E  μ    (    d   σ  2    +    ∥   g   (  x  )    ∥   2   +   2   σ  2     ∑   i  =  1   d     d   g  i     d   x  i        )     .         subscript  E  μ    superscript   norm      h  x   μ    2       subscript  E  μ       d   superscript  σ  2     superscript   norm    g  x    2     2   superscript  σ  2     superscript   subscript     i  1    d       d   subscript  g  i      d   subscript  x  i           E_{\mu}\|h(x)-\mu\|^{2}=E_{\mu}\left(d\sigma^{2}+\|g(x)\|^{2}+2\sigma^{2}\sum_%
 {i=1}^{d}\frac{dg_{i}}{dx_{i}}\right).     Applications  A standard application of SURE is to choose a parametric form for an estimator, and then optimize the values of the parameters to minimize the risk estimate. This technique has been applied in several settings. For example, a variant of the James–Stein estimator can be derived by finding the optimal shrinkage estimator . 4 The technique has also been used by Donoho and Johnstone to determine the optimal shrinkage factor in a wavelet  denoising setting. 5  References  "  Category:Error  Category:Estimation theory  Category:Risk      ↩  ↩   ↩     