


Hoeffding's inequality




Hoeffding's inequality

In probability theory, Hoeffding's inequality provides an upper bound on the probability that the sum of random variables deviates from its expected value. Hoeffding's inequality was proved by Wassily Hoeffding in 1963.1
Hoeffding's inequality is a special case of the Azuma–Hoeffding inequality, and it is more general than the Bernstein inequality, proved by Sergei Bernstein in 1923. They are also special cases of McDiarmid's inequality.
Special case of Bernoulli random variables
Hoeffding's inequality can be applied to the important special case of identically distributed Bernoulli random variables, and this is how the inequality is often used in combinatorics and computer science. We consider a coin that shows heads with probability 
 
 
 
  and tails with probability 
 
 
 
 . We toss the coin 
 
 
 
  times. The expected number of times the coin comes up heads is 
 
 
 
 . Furthermore, the probability that the coin comes up heads at most 
 
 
 
  times can be exactly quantified by the following expression:



where 
 
 
 
  is the number of heads in 
 
 
 
  coin tosses.
When 
 
 
 
  for some 
 
 
 
 , Hoeffding's inequality bounds this probability by a term that is exponentially small in :



Similarly, when 
 
 
 
  for some 
 
 
 
 , Hoeffding's inequality bounds the probability that we see at least 
 
 
 
  more tosses that show heads than we would expect:



Hence Hoeffding's inequality implies that the number of heads that we see is concentrated around its mean, with exponentially small tail.



General case
Let  be independent random variables. Assume that the  are almost surely bounded; that is:



We define the empirical mean of these variables



One of the inequalities in Theorem 1 of :



Theorem 2 of  is a generalization of the above inequality when it is known that  are strictly bounded by the intervals :



which are valid for positive values of 
 
 
 
 . Here 
 
 
 
  is the expected value of 
 
 
 
 . The inequalities can be also stated in terms of the sum



of the random variables:






Note that the inequalities also hold when the  have been obtained using sampling without replacement; in this case the random variables are not independent anymore. A proof of this statement can be found in Hoeffding's paper. For slightly better bounds in the case of sampling without replacement, see for instance the paper by .
Proof
In this section, we give a proof of Hoeffding's inequality.2 For the proof, we require the following:

Hoeffding's Lemma. Suppose 
 
 
 
  is a real random variable with mean zero such that 
 
 
 
 . Then
 
 




Proof of Hoeffding's Lemma
First note that if one of 
 
 
 
  or 
 
 
 
  is zero, then 
 
 
 
  and the inequality follows. If both are nonzero, then 
 
 
 
  must be negative and 
 
 
 
  must be positive.
Next, recall that  is a convex function on the real line:



Applying 
 
 
 
  to both sides of the above inequality gives us:



Let 
 
 
 
  and define:



 
  is well defined on 
 
 
 
 , to see this we calculate:



The definition of 
 
 
 
  implies



By Taylor's theorem, for every real 
 
 
 
  there exists a 
 
 
 
  between 
 
 
 
  and 
 
 
 
  such that



Note that:



Therefore,



This implies



Proof of Inequality
Using this lemma, we can prove Hoeffding's inequality. Suppose  are 
 
 
 
  independent random variables such that



Let



Then for 
 
 
 
 , Markov's inequality and the independence of  implies:



To get the best possible upper bound, we find the minimum of the right hand side of the last inequality as a function of 
 
 
 
 . Define



Note that 
 
 
 
  is a quadratic function and achieves its minimum at



Thus we get



Usage
Confidence Intervals
Hoeffding's inequality is useful to analyse the number of required samples needed to obtain a confidence interval by solving the inequality in Theorem 1:



The inequality states that the probability that the estimated and true values differ by more than 
 
 
 
  is bounded by . Symmetrically, the inequality is also valid for another side of the difference:



By adding them both up, we can obtain two-sided variant of this inequality:



This probability can be interpreted as the level of significance 
 
 
 
  (probability of making an error) for a confidence interval around 
 
 
 
  of size 
 
 
 
 :



Solving the above for 
 
 
 
  gives us the following:



Therefore, we require at least 
 
 
 
  samples to acquire 
 
 
 
 -confidence interval 
 
 
 
 .
Hence, the cost of acquiring the confidence interval is sublinear in terms of confidence level and quadratic in terms of precision.
Note that this inequality is the most conservative of the three in Theorem 1, and there are more efficient methods of estimating a confidence interval.
See also

Bennett's inequality
Chebyshev's inequality
Dvoretzky–Kiefer–Wolfowitz inequality
Markov's inequality
Chernoff bounds
Hoeffding's lemma
Concentration inequality

Notes
References





 
"
Category:Probabilistic inequalities



↩
; for a more intuitive proof, see this note↩




