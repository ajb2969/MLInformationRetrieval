   Hoeffding's inequality      Hoeffding's inequality   In probability theory , Hoeffding's inequality provides an upper bound on the probability that the sum of random variables deviates from its expected value . Hoeffding's inequality was proved by Wassily Hoeffding in 1963. 1  Hoeffding's inequality is a special case of the Azuma‚ÄìHoeffding inequality , and it is more general than the Bernstein inequality , proved by Sergei Bernstein in 1923. They are also special cases of McDiarmid's inequality .  Special case of Bernoulli random variables  Hoeffding's inequality can be applied to the important special case of identically distributed Bernoulli random variables , and this is how the inequality is often used in combinatorics and computer science . We consider a coin that shows heads with probability   p   p   p   and tails with probability    1  ‚àí  p      1  normal-‚àí  p    1‚àíp   . We toss the coin   n   n   n   times. The expected number of times the coin comes up heads is    p  n      p  n    pn   . Furthermore, the probability that the coin comes up heads at most   k   k   k   times can be exactly quantified by the following expression:      ‚Ñô   (  H   (  n  )   ‚â§  k  )   =   ‚àë   i  =  0   k    (     n      i     )    p  i     (  1  -  p  )    n  -  i    ,     fragments  P   fragments  normal-(  H   fragments  normal-(  n  normal-)    k  normal-)     superscript   subscript     i  0    k    binomial  n  i    superscript  p  i    superscript   fragments  normal-(  1   p  normal-)     n  i    normal-,    \mathbb{P}(H(n)\leq k)=\sum_{i=0}^{k}{\left({{n}\atop{i}}\right)}p^{i}(1-p)^{n%
 -i},     where    H   (  n  )       H  n    H(n)   is the number of heads in   n   n   n   coin tosses.  When    k  =    (   p  ‚àí  Œµ   )   n       k      p  normal-‚àí  Œµ   n     k=(p‚àíŒµ)n   for some    Œµ  >  0      Œµ  0    Œµ>0   , Hoeffding's inequality bounds this probability by a term that is exponentially small in :      ‚Ñô   (  H   (  n  )   ‚â§   (  p  -  Œµ  )   n  )   ‚â§  exp   (  -  2   Œµ  2   n  )   .     fragments  P   fragments  normal-(  H   fragments  normal-(  n  normal-)     fragments  normal-(  p   Œµ  normal-)   n  normal-)      fragments  normal-(   2   superscript  Œµ  2   n  normal-)   normal-.    \mathbb{P}(H(n)\leq(p-\varepsilon)n)\leq\exp\left(-2\varepsilon^{2}n\right).     Similarly, when    k  =    (   p  +  Œµ   )   n       k      p  Œµ   n     k=(p+Œµ)n   for some    Œµ  >  0      Œµ  0    Œµ>0   , Hoeffding's inequality bounds the probability that we see at least    Œµ  n      Œµ  n    Œµn   more tosses that show heads than we would expect:      ‚Ñô   (  H   (  n  )   ‚â•   (  p  +  Œµ  )   n  )   ‚â§  exp   (  -  2   Œµ  2   n  )   .     fragments  P   fragments  normal-(  H   fragments  normal-(  n  normal-)     fragments  normal-(  p   Œµ  normal-)   n  normal-)      fragments  normal-(   2   superscript  Œµ  2   n  normal-)   normal-.    \mathbb{P}(H(n)\geq(p+\varepsilon)n)\leq\exp\left(-2\varepsilon^{2}n\right).     Hence Hoeffding's inequality implies that the number of heads that we see is concentrated around its mean, with exponentially small tail.      ‚Ñô   (   (  p  -  œµ  )   n  ‚â§  H   (  n  )   ‚â§   (  p  +  Œµ  )   n  )   ‚â•  1  -  2  exp   (  -  2   Œµ  2   n  )   .     fragments  P   fragments  normal-(   fragments  normal-(  p   œµ  normal-)   n   H   fragments  normal-(  n  normal-)     fragments  normal-(  p   Œµ  normal-)   n  normal-)    1   2    fragments  normal-(   2   superscript  Œµ  2   n  normal-)   normal-.    \mathbb{P}((p-\epsilon)n\leq H(n)\leq(p+\varepsilon)n)\geq 1-2\exp\left(-2%
 \varepsilon^{2}n\right).     General case  Let be independent random variables . Assume that the are almost surely bounded; that is:      ‚Ñô   (   X  i   ‚àà   [   a  i   ,   b  i   ]   )   =  1  ,  1  ‚â§  i  ‚â§  n  .     fragments  P   fragments  normal-(   subscript  X  i     fragments  normal-[   subscript  a  i   normal-,   subscript  b  i   normal-]   normal-)    1  normal-,  1   i   n  normal-.    \mathbb{P}(X_{i}\in[a_{i},b_{i}])=1,\qquad 1\leq i\leq n.     We define the empirical mean of these variables        X  ¬Ø   =    1  n    (    X  1   +  ‚ãØ  +   X  n    )     .       normal-¬Ø  X       1  n      subscript  X  1   normal-‚ãØ   subscript  X  n       \overline{X}=\frac{1}{n}(X_{1}+\cdots+X_{n}).     One of the inequalities in Theorem 1 of :      ‚Ñô   (   X  ¬Ø   -  E   [   X  ¬Ø   ]   ‚â•  t  )   ‚â§   e   -   2  n   t  2         fragments  P   fragments  normal-(   normal-¬Ø  X    E   fragments  normal-[   normal-¬Ø  X   normal-]    t  normal-)     superscript  e      2  n   superscript  t  2        \displaystyle\mathbb{P}(\overline{X}-\mathrm{E}[\overline{X}]\geq t)\leq e^{-2%
 nt^{2}}     Theorem 2 of  is a generalization of the above inequality when it is known that are strictly bounded by the intervals :      ‚Ñô   (   X  ¬Ø   -  E   [   X  ¬Ø   ]   ‚â•  t  )      fragments  P   fragments  normal-(   normal-¬Ø  X    E   fragments  normal-[   normal-¬Ø  X   normal-]    t  normal-)     \displaystyle\mathbb{P}\left(\overline{X}-\mathrm{E}\left[\overline{X}\right]%
 \geq t\right)     which are valid for positive values of   t   t   t   . Here    E  o  v  e  r  l  i  n  e  i  n  e  X      E  o  v  e  r  l  i  n  e  i  n  e  X    EoverlineineX   is the expected value of    X  ¬Ø     normal-¬Ø  X    \overline{X}   . The inequalities can be also stated in terms of the sum       S  n   =    X  1   +  ‚ãØ  +   X  n         subscript  S  n      subscript  X  1   normal-‚ãØ   subscript  X  n      S_{n}=X_{1}+\cdots+X_{n}     of the random variables:      ‚Ñô   (   S  n   -  E   [   S  n   ]   ‚â•  t  )   ‚â§  exp   (  -    2   t  2      ‚àë   i  =  1   n     (    b  i   -   a  i    )   2     )   ,     fragments  P   fragments  normal-(   subscript  S  n    E   fragments  normal-[   subscript  S  n   normal-]    t  normal-)      fragments  normal-(       2   superscript  t  2      superscript   subscript     i  1    n    superscript     subscript  b  i    subscript  a  i    2     normal-)   normal-,    \mathbb{P}(S_{n}-\mathrm{E}[S_{n}]\geq t)\leq\exp\left(-\frac{2t^{2}}{\sum_{i=%
 1}^{n}(b_{i}-a_{i})^{2}}\right),         ‚Ñô   (  |   S  n   -  E   [   S  n   ]   |  ‚â•  t  )   ‚â§  2  exp   (  -    2   t  2      ‚àë   i  =  1   n     (    b  i   -   a  i    )   2     )   .     fragments  P   fragments  normal-(  normal-|   subscript  S  n    E   fragments  normal-[   subscript  S  n   normal-]   normal-|   t  normal-)    2    fragments  normal-(       2   superscript  t  2      superscript   subscript     i  1    n    superscript     subscript  b  i    subscript  a  i    2     normal-)   normal-.    \mathbb{P}(|S_{n}-\mathrm{E}[S_{n}]|\geq t)\leq 2\exp\left(-\frac{2t^{2}}{\sum%
 _{i=1}^{n}(b_{i}-a_{i})^{2}}\right).     Note that the inequalities also hold when the have been obtained using sampling without replacement; in this case the random variables are not independent anymore. A proof of this statement can be found in Hoeffding's paper. For slightly better bounds in the case of sampling without replacement, see for instance the paper by .  Proof  In this section, we give a proof of Hoeffding's inequality. 2 For the proof, we require the following:   Hoeffding's Lemma. Suppose   X   X   X   is a real random variable with mean zero such that    ‚Ñô   (  X  ‚àà   [  a  ,  b  ]   )   =  1     fragments  P   fragments  normal-(  X    fragments  normal-[  a  normal-,  b  normal-]   normal-)    1    \textstyle\mathbb{P}\left(X\in\left[a,b\right]\right)=1   . Then       E   [   e   s  X    ]    ‚â§   exp   (     1  8     s  2     (   b  -  a   )   2    )     .        normal-E   delimited-[]   superscript  e    s  X            1  8    superscript  s  2    superscript    b  a   2       \mathrm{E}\left[e^{sX}\right]\leq\exp\left(\tfrac{1}{8}s^{2}(b-a)^{2}\right).       Proof of Hoeffding's Lemma  First note that if one of   a   a   a   or   b   b   b   is zero, then    ‚Ñô   (  X  =  0  )   =  1     fragments  P   fragments  normal-(  X   0  normal-)    1    \textstyle\mathbb{P}\left(X=0\right)=1   and the inequality follows. If both are nonzero, then   a   a   a   must be negative and   b   b   b   must be positive.  Next, recall that is a convex function on the real line:      ‚àÄ  x  ‚àà   [  a  ,  b  ]   :   e   s  x    ‚â§    b  -  x    b  -  a     e   s  a    +    x  -  a    b  -  a     e   s  b    .     fragments  for-all  x    fragments  normal-[  a  normal-,  b  normal-]   normal-:  italic-   superscript  e    s  x         b  x     b  a     superscript  e    s  a         x  a     b  a     superscript  e    s  b    normal-.    \forall x\in[a,b]:\qquad e^{sx}\leq\frac{b-x}{b-a}e^{sa}+\frac{x-a}{b-a}e^{sb}.     Applying    E  ‚ãÖ      E  normal-‚ãÖ    E‚ãÖ   to both sides of the above inequality gives us:      E   [   e   s  X    ]       normal-E   delimited-[]   superscript  e    s  X       \displaystyle\mathrm{E}\left[e^{sX}\right]     Let    u  =   s   (   b  ‚àí  a   )        u    s    b  normal-‚àí  a      u=s(b‚àía)   and define:      {      œÜ  :   ùêë  ‚Üí  ùêë           œÜ   (  u  )    =    -   Œ∏  u    +   log   (    1  -  Œ∏   +   Œ∏   e  u     )             cases   normal-:  œÜ   normal-‚Üí  ùêë  ùêë    otherwise      œÜ  u         Œ∏  u          1  Œ∏     Œ∏   superscript  e  u        otherwise    \begin{cases}\varphi:\mathbf{R}\to\mathbf{R}\\
 \varphi(u)=-\theta u+\log\left(1-\theta+\theta e^{u}\right)\end{cases}      œÜ   œÜ   œÜ   is well defined on   ùêë   ùêë   \mathbf{R}   , to see this we calculate:       1  -  Œ∏   +   Œ∏   e  u          1  Œ∏     Œ∏   superscript  e  u      \displaystyle 1-\theta+\theta e^{u}     The definition of   œÜ   œÜ   œÜ   implies        E   [   e   s  X    ]    ‚â§   e   œÜ   (  u  )      .        normal-E   delimited-[]   superscript  e    s  X       superscript  e    œÜ  u      \mathrm{E}\left[e^{sX}\right]\leq e^{\varphi(u)}.     By Taylor's theorem , for every real   u   u   u   there exists a   v   v   v   between   0   0    and   u   u   u   such that        œÜ   (  u  )    =    œÜ   (  0  )    +   u   œÜ  ‚Ä≤    (  0  )    +     1  2     u  2    œÜ  ‚Ä≤‚Ä≤    (  v  )      .        œÜ  u       œÜ  0     u   superscript  œÜ  normal-‚Ä≤   0       1  2    superscript  u  2    superscript  œÜ  ‚Ä≤‚Ä≤   v      \varphi(u)=\varphi(0)+u\varphi^{\prime}(0)+\tfrac{1}{2}u^{2}\varphi^{\prime%
 \prime}(v).     Note that:      œÜ   (  0  )       œÜ  0    \displaystyle\varphi(0)     Therefore,        œÜ   (  u  )    ‚â§   0  +   u  ‚ãÖ  0   +      1  2     u  2    ‚ãÖ    1  4      =     1  8     u  2    =     1  8     s  2     (   b  -  a   )   2     .          œÜ  u     0   normal-‚ãÖ  u  0    normal-‚ãÖ      1  2    superscript  u  2      1  4              1  8    superscript  u  2             1  8    superscript  s  2    superscript    b  a   2       \varphi(u)\leq 0+u\cdot 0+\tfrac{1}{2}u^{2}\cdot\tfrac{1}{4}=\tfrac{1}{8}u^{2}%
 =\tfrac{1}{8}s^{2}(b-a)^{2}.     This implies        E   [   e   s  X    ]    ‚â§   exp   (     1  8     s  2     (   b  -  a   )   2    )     .        normal-E   delimited-[]   superscript  e    s  X            1  8    superscript  s  2    superscript    b  a   2       \mathrm{E}\left[e^{sX}\right]\leq\exp\left(\tfrac{1}{8}s^{2}(b-a)^{2}\right).     Proof of Inequality  Using this lemma, we can prove Hoeffding's inequality. Suppose are   n   n   n   independent random variables such that      ‚Ñô   (   X  i   ‚àà   [   a  i   ,   b  i   ]   )   =  1  ,  1  ‚â§  i  ‚â§  n  .     fragments  P   fragments  normal-(   subscript  X  i     fragments  normal-[   subscript  a  i   normal-,   subscript  b  i   normal-]   normal-)    1  normal-,  1   i   n  normal-.    \mathbb{P}\left(X_{i}\in[a_{i},b_{i}]\right)=1,\qquad 1\leq i\leq n.     Let        S  n   =    X  1   +  ‚ãØ  +   X  n     .       subscript  S  n      subscript  X  1   normal-‚ãØ   subscript  X  n      S_{n}=X_{1}+\cdots+X_{n}.     Then for    s  ,   t  ‚â•  0      s    t  normal-‚â•  0     s,t‚â•0   , Markov's inequality and the independence of implies:      ‚Ñô   (   S  n   -  E   [   S  n   ]   ‚â•  t  )      fragments  P   fragments  normal-(   subscript  S  n    E   fragments  normal-[   subscript  S  n   normal-]    t  normal-)     \displaystyle\mathbb{P}\left(S_{n}-\mathrm{E}\left[S_{n}\right]\geq t\right)     To get the best possible upper bound, we find the minimum of the right hand side of the last inequality as a function of   s   s   s   . Define      {      g  :    ùêë  +   ‚Üí  ùêë           g   (  s  )    =    -   s  t    +      s  2   8       ‚àë   i  =  1   n      (    b  i   -   a  i    )   2              cases   normal-:  g   normal-‚Üí   subscript  ùêë    ùêë    otherwise      g  s         s  t         superscript  s  2   8     superscript   subscript     i  1    n    superscript     subscript  b  i    subscript  a  i    2       otherwise    \begin{cases}g:\mathbf{R_{+}}\to\mathbf{R}\\
 g(s)=-st+\frac{s^{2}}{8}\sum_{i=1}^{n}(b_{i}-a_{i})^{2}\end{cases}     Note that   g   g   g   is a quadratic function and achieves its minimum at       s  =    4  t     ‚àë   i  =  1   n     (    b  i   -   a  i    )   2      .      s      4  t     superscript   subscript     i  1    n    superscript     subscript  b  i    subscript  a  i    2       s=\frac{4t}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}.     Thus we get      ‚Ñô   (   S  n   -  E   [   S  n   ]   ‚â•  t  )   ‚â§  exp   (  -    2   t  2      ‚àë   i  =  1   n     (    b  i   -   a  i    )   2     )   .     fragments  P   fragments  normal-(   subscript  S  n    E   fragments  normal-[   subscript  S  n   normal-]    t  normal-)      fragments  normal-(       2   superscript  t  2      superscript   subscript     i  1    n    superscript     subscript  b  i    subscript  a  i    2     normal-)   normal-.    \mathbb{P}\left(S_{n}-\mathrm{E}\left[S_{n}\right]\geq t\right)\leq\exp\left(-%
 \frac{2t^{2}}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}\right).     Usage  Confidence Intervals  Hoeffding's inequality is useful to analyse the number of required samples needed to obtain a confidence interval by solving the inequality in Theorem 1:      ‚Ñô   (   X  ¬Ø   -  E   [   X  ¬Ø   ]   ‚â•  t  )   ‚â§   e   -   2  n   t  2         fragments  P   fragments  normal-(   normal-¬Ø  X    E   fragments  normal-[   normal-¬Ø  X   normal-]    t  normal-)     superscript  e      2  n   superscript  t  2        \mathbb{P}(\overline{X}-\mathrm{E}[\overline{X}]\geq t)\leq e^{-2nt^{2}}     The inequality states that the probability that the estimated and true values differ by more than   t   t   t   is bounded by . Symmetrically, the inequality is also valid for another side of the difference:      ‚Ñô   (  -   X  ¬Ø   +  E   [   X  ¬Ø   ]   ‚â•  t  )   ‚â§   e   -   2  n   t  2         fragments  P   fragments  normal-(    normal-¬Ø  X    E   fragments  normal-[   normal-¬Ø  X   normal-]    t  normal-)     superscript  e      2  n   superscript  t  2        \mathbb{P}(-\overline{X}+\mathrm{E}[\overline{X}]\geq t)\leq e^{-2nt^{2}}     By adding them both up, we can obtain two-sided variant of this inequality:      ‚Ñô   (  |   X  ¬Ø   -  E   [   X  ¬Ø   ]   |  ‚â•  t  )   ‚â§  2   e   -   2  n   t  2         fragments  P   fragments  normal-(  normal-|   normal-¬Ø  X    E   fragments  normal-[   normal-¬Ø  X   normal-]   normal-|   t  normal-)    2   superscript  e      2  n   superscript  t  2        \mathbb{P}(|\overline{X}-\mathrm{E}[\overline{X}]|\geq t)\leq 2e^{-2nt^{2}}     This probability can be interpreted as the level of significance   Œ±   Œ±   \alpha   (probability of making an error) for a confidence interval around    E   [   X  ¬Ø   ]       normal-E   delimited-[]   normal-¬Ø  X      \mathrm{E}[\overline{X}]   of size    2  t      2  t    2t   :      Œ±  =  ‚Ñô   (   X  ¬Ø   ‚àâ   [  E   [   X  ¬Ø   ]   -  t  ,  E   [   X  ¬Ø   ]   +  t  ]   )   ‚â§  2   e   -   2  n   t  2         fragments  Œ±   P   fragments  normal-(   normal-¬Ø  X     fragments  normal-[  E   fragments  normal-[   normal-¬Ø  X   normal-]    t  normal-,  E   fragments  normal-[   normal-¬Ø  X   normal-]    t  normal-]   normal-)    2   superscript  e      2  n   superscript  t  2        \alpha=\mathbb{P}(\overline{X}\notin[\mathrm{E}[\overline{X}]-t,\mathrm{E}[%
 \overline{X}]+t])\leq 2e^{-2nt^{2}}     Solving the above for   n   n   n   gives us the following:      n  ‚â•   -    log   (   Œ±  /  2   )     2   t  2          n          Œ±  2      2   superscript  t  2        n\geq-\frac{\log(\alpha/2)}{2t^{2}}     Therefore, we require at least    -    log   (   Œ±  /  2   )     2   t  2               Œ±  2      2   superscript  t  2       \textstyle-\frac{\log(\alpha/2)}{2t^{2}}   samples to acquire    (   1  -  Œ±   )      1  Œ±    \textstyle(1-\alpha)   -confidence interval     E   [   X  ¬Ø   ]    ¬±  t     plus-or-minus    normal-E   delimited-[]   normal-¬Ø  X     t    \textstyle\mathrm{E}[\overline{X}]\pm t   .  Hence, the cost of acquiring the confidence interval is sublinear in terms of confidence level and quadratic in terms of precision.  Note that this inequality is the most conservative of the three in Theorem 1, and there are more efficient methods of estimating a confidence interval .  See also   Bennett's inequality  Chebyshev's inequality  Dvoretzky‚ÄìKiefer‚ÄìWolfowitz inequality  Markov's inequality  Chernoff bounds  Hoeffding's lemma  Concentration inequality   Notes  References         "  Category:Probabilistic inequalities     ‚Ü©  ; for a more intuitive proof, see this note ‚Ü©     