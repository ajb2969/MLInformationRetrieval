   Radial basis function kernel      Radial basis function kernel   In machine learning , the ( Gaussian ) radial basis function kernel , or RBF kernel , is a popular kernel function used in various kernelized learning algorithms. In particular, it is commonly used in support vector machine  classification . 1  The RBF kernel on two samples x and x ', represented as feature vectors in some input space , is defined as 2       K   (  𝐱  ,   𝐱  ′   )    =   exp   (   -     ||   𝐱  -   𝐱  ′    ||   2    2   σ  2      )          K   𝐱   superscript  𝐱  normal-′            superscript   norm    𝐱   superscript  𝐱  normal-′     2     2   superscript  σ  2         K(\mathbf{x},\mathbf{x^{\prime}})=\exp\left(-\frac{||\mathbf{x}-\mathbf{x^{%
 \prime}}||^{2}}{2\sigma^{2}}\right)        ||   𝐱  -   𝐱  ′    ||   2     superscript   norm    𝐱   superscript  𝐱  normal-′     2    \textstyle||\mathbf{x}-\mathbf{x^{\prime}}||^{2}   may be recognized as the squared Euclidean distance between the two feature vectors.   σ   σ   \sigma   is a free parameter. An equivalent, but simpler, definition involves a parameter    γ  =   1   2   σ  2         γ    1    2   superscript  σ  2       \textstyle\gamma=\tfrac{1}{2\sigma^{2}}   :       K   (  𝐱  ,   𝐱  ′   )    =   exp   (   -   γ    ||   𝐱  -   𝐱  ′    ||   2     )          K   𝐱   superscript  𝐱  normal-′           γ   superscript   norm    𝐱   superscript  𝐱  normal-′     2        K(\mathbf{x},\mathbf{x^{\prime}})=\exp(-\gamma||\mathbf{x}-\mathbf{x^{\prime}}%
 ||^{2})     Since the value of the RBF kernel decreases with distance and ranges between zero (in the limit) and one (when    𝐱  =  𝐱      𝐱  𝐱    \mathbf{x}=\mathbf{x}   ), it has a ready interpretation as a similarity measure . 3 The feature space of the kernel has an infinite number of dimensions; for    σ  =  1      σ  1    \sigma=1   , its expansion is: 4       exp   (   -    1  2     ||   𝐱  -   𝐱  ′    ||   2     )    =    ∑   j  =  0   ∞       (    𝐱  ⊤    𝐱  ′    )   j    j  !     exp   (   -    1  2     ||  𝐱  ||   2     )     exp   (   -    1  2     ||   𝐱  ′   ||   2     )                  1  2    superscript   norm    𝐱   superscript  𝐱  normal-′     2        superscript   subscript     j  0           superscript     superscript  𝐱  top    superscript  𝐱  normal-′    j     j            1  2    superscript   norm  𝐱   2              1  2    superscript   norm   superscript  𝐱  normal-′    2          \exp\left(-\frac{1}{2}||\mathbf{x}-\mathbf{x^{\prime}}||^{2}\right)=\sum_{j=0}%
 ^{\infty}\frac{(\mathbf{x}^{\top}\mathbf{x^{\prime}})^{j}}{j!}\exp\left(-\frac%
 {1}{2}||\mathbf{x}||^{2}\right)\exp\left(-\frac{1}{2}||\mathbf{x^{\prime}}||^{%
 2}\right)     Approximations  Because support vector machines and other models employing the kernel trick do not scale well to large numbers of training samples or large numbers of features in the input space, several approximations to the RBF kernel (and similar kernels) have been devised. 5 Typically, these take the form of a function z that maps a single vector to a vector of higher dimensionality, approximating the kernel:       z   (  𝐱  )   z   (   𝐱  ′   )    ≈   φ   (  𝐱  )   φ   (   𝐱  ′   )    =   K   (  𝐱  ,   𝐱  ′   )            z  𝐱  z   superscript  𝐱  normal-′      φ  𝐱  φ   superscript  𝐱  normal-′           K   𝐱   superscript  𝐱  normal-′        z(\mathbf{x})z(\mathbf{x^{\prime}})\approx\varphi(\mathbf{x})\varphi(\mathbf{x%
 ^{\prime}})=K(\mathbf{x},\mathbf{x^{\prime}})     where   φ   φ   \textstyle\varphi   is the implicit mapping embedded in the RBF kernel.  One way to construct such a z is to randomly sample from the Fourier transformation of the kernel. 6 Another approach uses the Nyström method to approximate the eigendecomposition of the Gram matrix  K , using only a random sample of the training set. 7  External links   Kernels Part 1: What is an RBF Kernel? Really?   See also   Gaussian function  Kernel (statistics)  Polynomial kernel  Radial basis function  Radial basis function network   References  "  Category:Kernel methods for machine learning     Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard and Chih-Jen Lin (2010). "Training and testing low-degree polynomial data mappings via linear SVM" . J. Machine Learning Research  11 :1471–1490. ↩  Vert, Jean-Philippe, Koji Tsuda, and Bernhard Schölkopf (2004). "A primer on kernel methods".  Kernel Methods in Computational Biology . ↩   ↩  Andreas Müller (2012). Kernel Approximations for Efficient SVMs (and other feature extraction methods) . ↩  Ali Rahimi and Benjamin Recht (2007). "Random features for large-scale kernel machines" . Neural Information Processing Systems . ↩  ↩     