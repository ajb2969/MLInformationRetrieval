   Explained sum of squares      Explained sum of squares   In statistics , the explained sum of squares (ESS), alternatively known as the model sum of squares or sum of squares due to regression ( "SSR" – not to be confused with the residual sum of squares  RSS ), is a quantity used in describing how well a model, often a regression model , represents the data being modelled. In particular, the explained sum of squares measures how much variation there is in the modelled values and this is compared to the total sum of squares , which measures how much variation there is in the observed data, and to the residual sum of squares , which measures the variation in the modelling errors.  Definition  The explained sum of squares (ESS) is the sum of the squares of the deviations of the predicted values from the mean value of a response variable, in a standard regression model — for example, , where y i is the i  th observation of the response variable , x ji is the i  th observation of the j  th  explanatory variable , a and b i are coefficients , i indexes the observations from 1 to n , and ε i is the i  th value of the error term . In general, the greater the ESS, the better the estimated model performs.  If    a  ^     normal-^  a    \hat{a}   and     b  ^   i     subscript   normal-^  b   i    \hat{b}_{i}   are the estimated coefficients , then        y  ^   i   =    a  ^   +     b  1   ^    x   1  i     +     b  2   ^    x   2  i     +   ⋯         subscript   normal-^  y   i      normal-^  a      normal-^   subscript  b  1     subscript  x    1  i        normal-^   subscript  b  2     subscript  x    2  i     normal-⋯     \hat{y}_{i}=\hat{a}+\hat{b_{1}}x_{1i}+\hat{b_{2}}x_{2i}+\cdots\,     is the i th predicted value of the response variable. The ESS is the sum of the squares of the differences of the predicted values and the mean value of the response variable:       ESS  =    ∑   i  =  1   n     (     y  ^   i   -   y  ¯    )   2     .      ESS    superscript   subscript     i  1    n    superscript     subscript   normal-^  y   i    normal-¯  y    2      \text{ESS}=\sum_{i=1}^{n}\left(\hat{y}_{i}-\bar{y}\right)^{2}.     In general: total sum of squares = explained sum of squares + residual sum of squares .  Partitioning in simple linear regression  The following equality, stating that the total sum of squares equals the residual sum of squares plus the explained sum of squares, is generally true in simple linear regression:         ∑   i  =  1   n     (    y  i   -   y  ¯    )   2    =     ∑   i  =  1   n     (    y  i   -    y  ^   i    )   2    +    ∑   i  =  1   n     (     y  ^   i   -   y  ¯    )   2      .        superscript   subscript     i  1    n    superscript     subscript  y  i    normal-¯  y    2        superscript   subscript     i  1    n    superscript     subscript  y  i    subscript   normal-^  y   i    2      superscript   subscript     i  1    n    superscript     subscript   normal-^  y   i    normal-¯  y    2       \sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-\hat{y}%
 _{i}\right)^{2}+\sum_{i=1}^{n}\left(\hat{y}_{i}-\bar{y}\right)^{2}.     Simple derivation        (    y  i   -   y  ¯    )   =    (    y  i   -    y  ^   i    )   +   (     y  ^   i   -   y  ¯    )     .         subscript  y  i    normal-¯  y         subscript  y  i    subscript   normal-^  y   i       subscript   normal-^  y   i    normal-¯  y       \displaystyle(y_{i}-\bar{y})=(y_{i}-\hat{y}_{i})+(\hat{y}_{i}-\bar{y}).     Square both sides and sum over all i :         ∑   i  =  1   n     (    y  i   -   y  ¯    )   2    =     ∑   i  =  1   n     (    y  i   -    y  ^   i    )   2    +    ∑   i  =  1   n     (     y  ^   i   -   y  ¯    )   2    +    ∑   i  =  1   n    2   (     y  ^   i   -   y  ¯    )    (    y  i   -    y  ^   i    )       .        superscript   subscript     i  1    n    superscript     subscript  y  i    normal-¯  y    2        superscript   subscript     i  1    n    superscript     subscript  y  i    subscript   normal-^  y   i    2      superscript   subscript     i  1    n    superscript     subscript   normal-^  y   i    normal-¯  y    2      superscript   subscript     i  1    n     2     subscript   normal-^  y   i    normal-¯  y       subscript  y  i    subscript   normal-^  y   i         \sum_{i=1}^{n}(y_{i}-\bar{y})^{2}=\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}+\sum_{%
 i=1}^{n}(\hat{y}_{i}-\bar{y})^{2}+\sum_{i=1}^{n}2(\hat{y}_{i}-\bar{y})(y_{i}-%
 \hat{y}_{i}).     Simple linear regression gives     a  ^   =    y  ¯   -    b  ^    x  ¯          normal-^  a      normal-¯  y      normal-^  b    normal-¯  x       \hat{a}=\bar{y}-\hat{b}\bar{x}   . 1 What follows depends on this.        ∑   i  =  1   n     2   (     y  ^   i   -   y  ¯    )    (    y  i   -    y  ^   i    )        superscript   subscript     i  1    n     2     subscript   normal-^  y   i    normal-¯  y       subscript  y  i    subscript   normal-^  y   i       \displaystyle\sum_{i=1}^{n}2(\hat{y}_{i}-\bar{y})(y_{i}-\hat{y}_{i})     Again simple linear regression gives 2        b  ^   =     ∑   i  =  1   n     (    x  i   -   x  ¯    )    (    y  i   -   y  ¯    )       ∑   i  =  1   n     (    x  i   -   x  ¯    )   2      ,       normal-^  b       superscript   subscript     i  1    n        subscript  x  i    normal-¯  x       subscript  y  i    normal-¯  y        superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¯  x    2       \hat{b}=\frac{\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum_{i=1}^{n}(x_{%
 i}-\bar{x})^{2}},           ∑   i  =  1   n     2   (     y  ^   i   -   y  ¯    )    (    y  i   -    y  ^   i    )        superscript   subscript     i  1    n     2     subscript   normal-^  y   i    normal-¯  y       subscript  y  i    subscript   normal-^  y   i       \displaystyle\sum_{i=1}^{n}2(\hat{y}_{i}-\bar{y})(y_{i}-\hat{y}_{i})     Partitioning in the general ordinary least squares model  The general regression model with n observations and k explanators, the first of which is a constant unit vector whose coefficient is the regression intercept, is      y  =    X  β   +  e       y      X  β   e     y=X\beta+e     where y is an n × 1 vector of dependent variable observations, each column of the n × k matrix X is a vector of observations on one of the k explanators,   β   β   \beta   is a k × 1 vector of true coefficients, and e is an n × 1 vector of the true underlying errors. The ordinary least squares estimator for   β   β   \beta   is        β  ^   =     (    X  T   X   )    -  1     X  T   y    .       normal-^  β      superscript     superscript  X  T   X     1     superscript  X  T   y     \hat{\beta}=(X^{T}X)^{-1}X^{T}y.     The residual vector    e  ^     normal-^  e    \hat{e}   is     y  -   X   β  ^     =   y  -   X    (    X  T   X   )    -  1     X  T   y          y    X   normal-^  β       y    X   superscript     superscript  X  T   X     1     superscript  X  T   y      y-X\hat{\beta}=y-X(X^{T}X)^{-1}X^{T}y   , so the residual sum of squares      e  ^   T    e  ^        superscript   normal-^  e   T    normal-^  e     \hat{e}^{T}\hat{e}   is, after simplification,        R  S  S   =     y  T   y   -    y  T   X    (    X  T   X   )    -  1     X  T   y     .        R  S  S        superscript  y  T   y      superscript  y  T   X   superscript     superscript  X  T   X     1     superscript  X  T   y      RSS=y^{T}y-y^{T}X(X^{T}X)^{-1}X^{T}y.     Denote as    y  ¯     normal-¯  y    \bar{y}   the constant vector all of whose elements are the sample mean    y  m     subscript  y  m    y_{m}   of the dependent variable values in the vector y . Then the total sum of squares is        T  S  S   =     (   y  -   y  ¯    )   T    (   y  -   y  ¯    )    =      y  T   y   -   2   y  T    y  ¯     +     y  ¯   T    y  ¯      .          T  S  S      superscript    y   normal-¯  y    T     y   normal-¯  y                 superscript  y  T   y     2   superscript  y  T    normal-¯  y        superscript   normal-¯  y   T    normal-¯  y        TSS=(y-\bar{y})^{T}(y-\bar{y})=y^{T}y-2y^{T}\bar{y}+\bar{y}^{T}\bar{y}.     The explained sum of squares, defined as the sum of squared deviations of the predicted values from the observed mean of y , is        E  S  S   =     (    y  ^   -   y  ¯    )   T    (    y  ^   -   y  ¯    )    =       y  ^   T    y  ^    -   2    y  ^   T    y  ¯     +     y  ¯   T    y  ¯      .          E  S  S      superscript     normal-^  y    normal-¯  y    T      normal-^  y    normal-¯  y                 superscript   normal-^  y   T    normal-^  y      2   superscript   normal-^  y   T    normal-¯  y        superscript   normal-¯  y   T    normal-¯  y        ESS=(\hat{y}-\bar{y})^{T}(\hat{y}-\bar{y})=\hat{y}^{T}\hat{y}-2\hat{y}^{T}\bar%
 {y}+\bar{y}^{T}\bar{y}.     Using     y  ^   =   X   β  ^         normal-^  y     X   normal-^  β      \hat{y}=X\hat{\beta}   in this, and simplifying to obtain       y  ^   T    y  ^    =    y  T   X    (    X  T   X   )    -  1     X  T   y          superscript   normal-^  y   T    normal-^  y       superscript  y  T   X   superscript     superscript  X  T   X     1     superscript  X  T   y     \hat{y}^{T}\hat{y}=y^{T}X(X^{T}X)^{-1}X^{T}y   , gives the result that TSS = ESS + RSS if and only if      y  T    y  ¯    =     y  ^   T    y  ¯           superscript  y  T    normal-¯  y       superscript   normal-^  y   T    normal-¯  y      y^{T}\bar{y}=\hat{y}^{T}\bar{y}   . The left side of this is    y  m     subscript  y  m    y_{m}   times the sum of the elements of y , and the right side is    y  m     subscript  y  m    y_{m}   times the sum of the elements of    y  ^     normal-^  y    \hat{y}   , so the condition is that the sum of the elements of y equals the sum of the elements of    y  ^     normal-^  y    \hat{y}   , or equivalently that the sum of the prediction errors (residuals)     y  i   -    y  ^   i        subscript  y  i    subscript   normal-^  y   i     y_{i}-\hat{y}_{i}   is zero. This can be seen to be true by noting the well-known OLS property that the k × 1 vector      X  T    e  ^    =    X  T    [   I  -   X    (    X  T   X   )    -  1     X  T     ]   y   =  0           superscript  X  T    normal-^  e       superscript  X  T    delimited-[]    I    X   superscript     superscript  X  T   X     1     superscript  X  T      y        0     X^{T}\hat{e}=X^{T}[I-X(X^{T}X)^{-1}X^{T}]y=0   : since the first column of X is a vector of ones, the first element of this vector     X  T    e  ^        superscript  X  T    normal-^  e     X^{T}\hat{e}   is the sum of the residuals and is equal to zero. This proves that the condition holds for the result that TSS = ESS + RSS .  In linear algebra terms, we have     R  S  S   =    ∥   y  -   y  ^    ∥   2  2         R  S  S    superscript   subscript   norm    y   normal-^  y     2   2     RSS=\|y-{\hat{y}}\|_{2}^{2}   ,     T  S  S   =    ∥   y  -   y  ¯    ∥   2  2         T  S  S    superscript   subscript   norm    y   normal-¯  y     2   2     TSS=\|y-\bar{y}\|_{2}^{2}   ,     E  S  S   =    ∥    y  ^   -   y  ¯    ∥   2  2         E  S  S    superscript   subscript   norm     normal-^  y    normal-¯  y     2   2     ESS=\|{\hat{y}}-\bar{y}\|_{2}^{2}   . The proof can be simplified by noting that      y  T    y  ^    =     y  ^   T    y  ^           superscript  y  T    normal-^  y       superscript   normal-^  y   T    normal-^  y      y^{T}{\hat{y}}={\hat{y}}^{T}{\hat{y}}   . The proof is as follows:          y  ^   T    y  ^    =    y  T   X    (    X  T   X   )    -  1     X  T   X    (    X  T   X   )    -  1     X  T   y   =    y  T   X    (    X  T   X   )    -  1     X  T   y   =    y  T    y  ^     ,           superscript   normal-^  y   T    normal-^  y       superscript  y  T   X   superscript     superscript  X  T   X     1     superscript  X  T   X   superscript     superscript  X  T   X     1     superscript  X  T   y           superscript  y  T   X   superscript     superscript  X  T   X     1     superscript  X  T   y           superscript  y  T    normal-^  y       {\hat{y}}^{T}{\hat{y}}=y^{T}X(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1}X^{T}y=y^{T}X(X^{%
 T}X)^{-1}X^{T}y=y^{T}{\hat{y}},     Thus,       T  S  S   =    ∥   y  -   y  ¯    ∥   2  2   =    ∥     y  -   y  ^    +   y  ^    -   y  ¯    ∥   2  2           T  S  S    superscript   subscript   norm    y   normal-¯  y     2   2         superscript   subscript   norm        y   normal-^  y     normal-^  y     normal-¯  y     2   2      TSS=\|y-\bar{y}\|_{2}^{2}=\|y-{\hat{y}}+{\hat{y}}-\bar{y}\|_{2}^{2}          T  S  S   =      R  S  S   +   E  S  S   +   2   y  T    y  ^     -   2    y  ^   T    y  ^    -   2   y  T    y  ¯     +   2    y  ^   T    y  ¯           T  S  S           R  S  S     E  S  S     2   superscript  y  T    normal-^  y       2   superscript   normal-^  y   T    normal-^  y      2   superscript  y  T    normal-¯  y       2   superscript   normal-^  y   T    normal-¯  y       TSS=RSS+ESS+2y^{T}{\hat{y}}-2{\hat{y}}^{T}{\hat{y}}-2y^{T}{\bar{y}}+2{\hat{y}}%
 ^{T}{\bar{y}}          T  S  S   =      R  S  S   +   E  S  S    -   2   y  T    y  ¯     +   2    y  ^   T    y  ¯           T  S  S           R  S  S     E  S  S      2   superscript  y  T    normal-¯  y       2   superscript   normal-^  y   T    normal-¯  y       TSS=RSS+ESS-2y^{T}{\bar{y}}+2{\hat{y}}^{T}{\bar{y}}           y  T    y  ¯    =     y  ^   T    y  ¯           superscript  y  T    normal-¯  y       superscript   normal-^  y   T    normal-¯  y      y^{T}\bar{y}=\hat{y}^{T}\bar{y}   which again gives the result that TSS = ESS + RSS if and only if $y^T \bar y = \hat y^T \bar y$ .  See also   Sum of squares (statistics)  Lack-of-fit sum of squares   Notes  References   S. E. Maxwell and H. D. Delaney (1990), "Designing experiments and analyzing data: A model comparison perspective". Wadsworth. pp. 289–290.  G. A. Milliken and D. E. Johnson (1984), "Analysis of messy data", Vol. I: Designed experiments. Van Nostrand Reinhold. pp. 146–151.  B. G. Tabachnick and L. S. Fidell (2007), "Experimental design using ANOVA". Duxbury. p. 220.  B. G. Tabachnick and L. S. Fidell (2007), "Using multivariate statistics", 5th ed. Pearson Education. pp. 217–218.   "  Category:Regression analysis  Category:Least squares     ↩      