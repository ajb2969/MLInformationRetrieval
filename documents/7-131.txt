   Quicksort      Quicksort  table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
   margin: 0; padding: 0; vertical-align: baseline; border: none; }
 <style>
 table.sourceCode { width: 100%; line-height: 100%; }
 td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
 td.sourceCode { padding-left: 5px; }
 code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
 code > span.dt { color: #902000; } /* DataType */
 code > span.dv { color: #40a070; } /* DecVal */
 code > span.bn { color: #40a070; } /* BaseN */
 code > span.fl { color: #40a070; } /* Float */
 code > span.ch { color: #4070a0; } /* Char */
 code > span.st { color: #4070a0; } /* String */
 code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
 code > span.ot { color: #007020; } /* Other */
 code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
 code > span.fu { color: #06287e; } /* Function */
 code > span.er { color: #ff0000; font-weight: bold; } /* Error */
 code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 code > span.cn { color: #880000; } /* Constant */
 code > span.sc { color: #4070a0; } /* SpecialChar */
 code > span.vs { color: #4070a0; } /* VerbatimString */
 code > span.ss { color: #bb6688; } /* SpecialString */
 code > span.im { } /* Import */
 code > span.va { color: #19177c; } /* Variable */
 code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
 code > span.op { color: #666666; } /* Operator */
 code > span.bu { } /* BuiltIn */
 code > span.ex { } /* Extension */
 code > span.pp { color: #bc7a00; } /* Preprocessor */
 code > span.at { color: #7d9029; } /* Attribute */
 code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
 code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
 code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
 code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */     Quicksort (sometimes called partition-exchange sort ) is an efficient sorting algorithm , serving as a systematic method for placing the elements of an array in order. Developed by Tony Hoare in 1959, 1 with his work published in 1961, 2 it is still a commonly used algorithm for sorting. When implemented well, it can be about two or three times faster than its main competitors, merge sort and heapsort . 3  Quicksort is a comparison sort , meaning that it can sort items of any type for which a "less-than" relation (formally, a total order ) is defined. In efficient implementations it is not a stable sort , meaning that the relative order of equal sort items is not preserved. Quicksort can operate in-place on an array, requiring small additional amounts of memory to perform the sorting.  Mathematical analysis of quicksort shows that, on average , the algorithm takes O ( n log n ) comparisons to sort n items. In the worst case , it makes O( n 2 ) comparisons, though this behavior is rare.  History  The quicksort algorithm was developed in 1960 by Tony Hoare while in the Soviet Union , as a visiting student at Moscow State University . At that time, Hoare worked in a project on machine translation for the National Physical Laboratory . As a part of translation process, he needed to sort the words of Russian sentence prior to looking them up in a Russian-English dictionary which was already sorted in alphabetic order on magnetic tape . 4 After recognizing that his first idea, insertion sort , would be bit slow, he quickly came up with new idea that was Quicksort. He wrote a program in Mercury Autocode for the partition but couldn't write the program to account for the list of unsorted segments. On return to England, he was asked to write code for Shellsort as part of his new job. Hoare mentioned to his boss that he knew of faster algorithm and his boss bet sixpence that he didn't. His boss ultimately accepted that he had lost the bet. Later, Hoare learned about ALGOL and its ability to do recursion which enabled him to publish the code in ACM . 5  Quicksort gained widespread adoption, appearing, for example, in Unix as the default library sort function, hence it lent its name to the C standard library function [[qsort]] 6 and in the reference implementation of Java .  Robert Sedgewick 's Ph.D. thesis in 1975 is considered a milestone in the study of Quicksort where he resolved many open problems related to the analysis of various pivot selection schemes including Samplesort , adaptive partitioning by Van Emden 7 as well as derivation of expected number of comparisons and swaps. 8 Bentley and McIlroy incorporated various improvements for use in programming libraries including technique to deal with equal elements and a pivot scheme known as pseudomedian of nine where sample of 9 elements are divided in groups of 3 and then median of the 3 medians from 3 groups is chosen. 9  Jon Bentley described another simpler and compact partitioning scheme in his book Programming Pearls that he attributed to Nico Lomuto. Later Bentley wrote that he used Hoare's version for years but never really understood it but Lomuto's version was simple enough to prove correct. 10 Bentley described Quicksort as the "most beautiful code I had ever written" in the same essay. Lomuto's partition scheme was also popularized by the textbook Introduction to Algorithms although it is inferior to Hoare's scheme because it does 3 times more swaps on average and degrades to    O   (   n  2   )       O   superscript  n  2     O(n^{2})   runtime when all elements are equal. 11   In 2009, Yaroslavskiy proposed the new dual pivot Quicksort implementation. 12 In the Java core library mailing lists, he initiated a discussion claiming his new algorithm to be superior to the runtime libraryâ€™s sorting method at that time based on the widely used and carefully tuned variant of classic Quicksort by Bently and McIlroy. 13 Yaroslavskiyâ€™s Quicksort has been chosen as the new default sorting algorithm in Oracleâ€™s Java 7 runtime library after extensive empirical performance tests. 14  Algorithm  Quicksort is a divide and conquer algorithm . Quicksort first divides a large array into two smaller sub-arrays: the low elements and the high elements. Quicksort can then recursively sort the sub-arrays.  The steps are:   Pick an element, called a pivot , from the array.  Reorder the array so that all elements with values less than the pivot come before the pivot, while all elements with values greater than the pivot come after it (equal values can go either way). After this partitioning, the pivot is in its final position. This is called the partition operation.  Recursively apply the above steps to the sub-array of elements with smaller values and separately to the sub-array of elements with greater values.   The base case of the recursion is arrays of size zero or one, which never need to be sorted. In pseudocode , a quicksort that sorts elements    l  o      l  o    lo   through    h  i      h  i    hi   (inclusive) of an array   A   A   A   can be expressed compactly as 15  quicksort(A, lo, hi) if lo < hi
     p = partition(A, lo, hi)
     quicksort(A, lo, p -  1 )
     quicksort(A, p +  1 , hi)  (Figure)  In-place partition in action on a small list. The boxed element is the pivot element, blue elements are less or equal, and red elements are larger. Note: Appears to show partition using less-than-or-equal instead of strictly less-than as indicated in algorithm discussion.   Sorting the entire array is accomplished by calling quicksort(A, 1, length(A)) .  The algorithm for partition has several variants. Some popular variants are described below.  Lomuto partition scheme  This scheme is attributed to Nico Lomuto and popularized by Bentley in his book Programming Pearls 16 and Cormen et al. in their book Introduction to Algorithms . 17 This scheme chooses a pivot which is typically the last element in the array. The algorithm maintains the index to put pivot in variable i and each time when it finds an element less than or equal to pivot, this index is incremented and that element would be placed before the pivot. As this scheme is more compact and easy to understand, it is frequently used in introductory material, although it is less efficient than Hoare's original scheme. 18 This scheme degrade to    O   (   n  2   )       O   superscript  n  2     O(n^{2})   when array is already sorted as well as when the array has all equal elements. 19 There have been various variants proposed to boost performance including various ways to select pivot, deal with equal elements, use other sorting algorithms such as Insertion sort for small arrays and so on.  partition(A, lo, hi)
     pivot = A[hi]
     i = lo -  1  // place for pivot for j = lo to hi -  1  if A[j] <= pivot
             i = i +  1 swap A[i] with A[j]
     i = i +  1 swap A[i] with A[hi] return i  Hoare partition scheme  This is the original partition scheme described by C.A.R. Hoare using two indices that moves in opposite direction until an inversion is found in which case the elements are swapped to bring them in relative sort order. 20 When the indices crosses each other, algorithm stops and returns the index value. There are many variants of this algorithm, for example, selecting pivot from A[hi] instead of A[lo] . Hoare scheme is more efficient than Lomuto's partition scheme because it does 3 times less swaps on average and it creates efficient partitions even when all values are equal. 21 Like Lomuto's partition scheme, Hoare partitioning also causes Quicksort to degrade to    O   (   n  2   )       O   superscript  n  2     O(n^{2})   when array is already sorted and also doesn't produce stable sort.  partition(A, lo, hi)
     pivot = A[hi]
     i = lo -  1 j = hi while  True do
             i = i +  1  while A[i] < pivot
         do
             j = j -  1  while A[j] > pivot if (i < j)
             swap A[i] with A[j] else swap A[i] with A[hi] return i  Implementation issues  Choice of pivot  In the very early versions of quicksort, the leftmost element of the partition would often be chosen as the pivot element. Unfortunately, this causes worst-case behavior on already sorted arrays, which is a rather common use-case. The problem was easily solved by choosing either a random index for the pivot, choosing the middle index of the partition or (especially for longer partitions) choosing the median of the first, middle and last element of the partition for the pivot (as recommended by Sedgewick ). 22 This "median-of-three" rule counters the case of sorted (or reverse-sorted) input, and gives a better estimate of the optimal pivot (the true median) than selecting any single element, when no information about the ordering of the input is known.  Specifically, the expected number of comparisons needed to sort   n   n   n   elements (see ) with random pivot selection is    1.386  n  l  o  g  n      1.386  n  l  o  g  n    1.386nlogn   . Median-of-three pivoting brings this down to , at the expense of a three-percent increase in the expected number of swaps. An even stronger pivoting rule, for larger arrays, is to pick the ninther , a recursive median-of-three, defined as        n  i  n  t  h  e  r   (  a  )    =   m  e  d  i  a  n   (    m  e  d  i  a  n   -   o  f   -   t  h  r  e  e   (   f  i  r  s  t  â…“  o  f  a   )     ,    m  e  d  i  a  n   -   o  f   -   t  h  r  e  e   (   m  i  d  d  l  e  â…“  o  f  a   )     ,    m  e  d  i  a  n   -   o  f   -   t  h  r  e  e   (   f  i  n  a  l  â…“  o  f  a   )     )          n  i  n  t  h  e  r  a     m  e  d  i  a  n       m  e  d  i  a  n     o  f     t  h  r  e  e    f  i  r  s  t  normal-â…“  o  f  a         m  e  d  i  a  n     o  f     t  h  r  e  e    m  i  d  d  l  e  normal-â…“  o  f  a         m  e  d  i  a  n     o  f     t  h  r  e  e    f  i  n  a  l  normal-â…“  o  f  a         ninther(a)=median(median-of-three(firstâ…“ofa),median-of-three(middleâ…“ofa),%
 median-of-three(finalâ…“ofa))      Selecting a pivot element is also complicated by the existence of integer overflow . If the boundary indices of the subarray being sorted are sufficiently large, the naÃ¯ve expression for the middle index,     (    l  o   +   h  i    )   /  2          l  o     h  i    2    (lo+hi)/2   , will cause overflow and provide an invalid pivot index. This can be overcome by using, for example,     l  o   +    (   h  i  âˆ’  l  o   )   /  2         l  o       h  i  normal-âˆ’  l  o   2     lo+(hiâˆ’lo)/2   to index the middle element, at the cost of more complex arithmetic. Similar issues arise in some other methods of selecting the pivot element.  Repeated elements  With a partitioning algorithm such as the one described above (even with one that chooses good pivot values), quicksort exhibits poor performance for inputs that contain many repeated elements. The problem is clearly apparent when all the input elements are equal: at each recursion, the left partition is empty (no input values are less than the pivot), and the right partition has only decreased by one element (the pivot is removed). Consequently, the algorithm takes quadratic time to sort an array of equal values.  To solve this problem (sometimes called the Dutch national flag problem 23 ), an alternative linear-time partition routine can be used that separates the values into three groups: values less than the pivot, values equal to the pivot, and values greater than the pivot. (Bentley and McIlroy call this a "fat partition" and note that it was already implemented in the  of Version 7 Unix . 24 ) The values equal to the pivot are already sorted, so only the less-than and greater-than partitions need to be recursively sorted. In pseudocode, the quicksort algorithm becomes  quicksort(A,Â lo,Â hi)   if lo $kÂ â‰ªÂ n$ elements).Â InÂ theÂ caseÂ ofÂ allÂ equalÂ elements,Â theÂ modifiedÂ quicksortÂ willÂ performÂ atÂ mostÂ twoÂ recursiveÂ callsÂ onÂ emptyÂ subarraysÂ andÂ thusÂ finishÂ inÂ linearÂ time.  Optimizations  Two other important optimizations, also suggested by Sedgewick and widely used in practice are: 25 26   To make sure at most    O   (   l  o  g  n   )       O    l  o  g  n     O(logn)   space is used, recurse first into the smaller side of the partition, then use a tail call to recurse into the other.  Use insertion sort , which has a smaller constant factor and is thus faster on small arrays, for invocations on small arrays (i.e. where the length is less than a threshold   k   k   k   determined experimentally). This can be implemented by simply stopping the recursion when less than   k   k   k   elements are left, leaving the entire array   k   k   k   -sorted: each element will be at most   k   k   k   positions away from its final position. Then, a single insertion sort pass 27 finishes the sort in    O   (   k  n   )       O    k  n     O(kn)   time. A separate insertion sort of each small segment as they are identified adds the overhead of starting and stopping many small sorts, but avoids wasting effort comparing keys across the many segment boundaries, where keys will be in order due to the workings of the quicksort process.   Parallelization  Quicksort's divide-and-conquer formulation makes it amenable to parallelization using task parallelism . The partitioning step is accomplished through the use of a parallel prefix sum algorithm to compute an index for each array element in its section of the partitioned array. 28 29 Given an array of size   n   n   n   , the partitioning step performs    O   (  n  )       O  n    O(n)   work in    O   (   l  o  g  n   )       O    l  o  g  n     O(logn)   time and requires    O   (  n  )       O  n    O(n)   additional scratch space. After the array has been partitioned, the two partitions can be sorted recursively in parallel. Assuming an ideal choice of pivots, parallel quicksort sorts an array of size   n   n   n   in    O   (   n  l  o  g  n   )       O    n  l  o  g  n     O(nlogn)   work in    O   (   l  o  g  Â²  n   )       O    l  o  g  normal-Â²  n     O(logÂ²n)   time using    O   (  n  )       O  n    O(n)   additional space.  Quicksort has some disadvantages when compared to alternative sorting algorithms, like merge sort , which complicate its efficient parallelization. The depth of quicksort's divide-and-conquer tree directly impacts the algorithm's scalability, and this depth is highly dependent on the algorithm's choice of pivot. Additionally, it is difficult to parallelize the partitioning step efficiently in-place. The use of scratch space simplifies the partitioning step, but increases the algorithm's memory footprint and constant overheads.  Other more sophisticated parallel sorting algorithms can achieve even better time bounds. 30 For example, in 1991 David Powers described a parallelized quicksort (and a related radix sort ) that can operate in    O   (   l  o  g  n   )       O    l  o  g  n     O(logn)   time on a CRCW PRAM with   n   n   n   processors by performing partitioning implicitly. 31  Formal analysis  Average-case analysis using discrete probability  To sort an array of   n   n   n   distinct elements, quicksort takes    O   (   n  l  o  g  n   )       O    n  l  o  g  n     O(nlogn)   time in expectation, averaged over all    n  !      n    n!   permutations of   n   n   n   elements with equal probability . Why? For a start, it is not hard to see that the partition operation takes    O   (  n  )       O  n    O(n)   time.  In the most unbalanced case, each time the partitioning is performed the list is divided into two sublists of   0   0    and    n  âˆ’  1      n  normal-âˆ’  1    nâˆ’1   sizes (for example, if all elements of the array are equal). This means each recursive call processes a list of size one less than the previous list. Consequently, we can make    n  âˆ’  1      n  normal-âˆ’  1    nâˆ’1   nested calls before we reach a list of size 1. This means that the call tree is a linear chain of    n  âˆ’  1      n  normal-âˆ’  1    nâˆ’1   nested calls. The   i   i   i   th call does    O   (   n  âˆ’  i   )       O    n  normal-âˆ’  i     O(nâˆ’i)   work to do the partition, and      âˆ‘   i  =  0   n    (   n  -  i   )    =   O   (   n  2   )          superscript   subscript     i  0    n     n  i      O   superscript  n  2      \textstyle\sum_{i=0}^{n}(n-i)=O(n^{2})   , so in that case Quicksort takes    O   (   n  Â²   )       O    n  normal-Â²     O(nÂ²)   time. That is the worst case: given knowledge of which comparisons are performed by the sort, there are adaptive algorithms that are effective at generating worst-case input for quicksort on-the-fly, regardless of the pivot selection strategy. 32  In the most balanced case, each time we perform a partition we divide the list into two nearly equal pieces. This means each recursive call processes a list of half the size. Consequently, we can make only nested calls before we reach a list of size 1. This means that the depth of the call tree is . But no two calls at the same level of the call tree process the same part of the original list; thus, each level of calls needs only    O   (  n  )       O  n    O(n)   time all together (each call has some constant overhead, but since there are only    O   (  n  )       O  n    O(n)   calls at each level, this is subsumed in the    O   (  n  )       O  n    O(n)   factor). The result is that the algorithm uses only    O   (   n  l  o  g  n   )       O    n  l  o  g  n     O(nlogn)   time.  In fact, it's not necessary to be perfectly balanced; even if each pivot splits the elements with 75% on one side and 25% on the other side (or any other fixed fraction), the call depth is still limited to     log   4  /  3    n      subscript     4  3    n    \log_{4/3}n   , so the total running time is still    O   (   n  l  o  g  n   )       O    n  l  o  g  n     O(nlogn)   .  On average, if the pivot has rank somewhere in the middle 50 percent, that is, between the 25th percentile and the 75th percentile, then it splits the elements with at least 25% and at most 75% on each side. If we could consistently choose a pivot from the two middle 50 percent, we would only have to split the list at most     log   4  /  3    n      subscript     4  3    n    \log_{4/3}n   times before reaching lists of size 1, yielding an    O   (   n  l  o  g  n   )       O    n  l  o  g  n     O(nlogn)   algorithm.  When the input is a random permutation, the pivot has a random rank, and so it is not guaranteed to be in the middle 50 percent. However, when we start from a random permutation, in each recursive call the pivot has a random rank in its list, and so it is in the middle 50 percent about half the time. That is good enough. Imagine that you flip a coin: heads means that the rank of the pivot is in the middle 50 percent, tail means that it isn't. Imagine that you are flipping a coin over and over until you get   k   k   k   heads. Although this could take a long time, on average only    2  k      2  k    2k   flips are required, and the chance that you won't get   k   k   k   heads after    100  k      100  k    100k   flips is highly improbable (this can be made rigorous using Chernoff bounds). By the same argument, Quicksort's recursion will terminate on average at a call depth of only    2    log   4  /  3    n       2    subscript     4  3    n     2\log_{4/3}n   . But if its average call depth is    O   (   l  o  g  n   )       O    l  o  g  n     O(logn)   , and each level of the call tree processes at most   n   n   n   elements, the total amount of work done on average is the product,    O   (   n  l  o  g  n   )       O    n  l  o  g  n     O(nlogn)   . Note that the algorithm does not have to verify that the pivot is in the middle halfâ€”if we hit it any constant fraction of the times, that is enough for the desired complexity.  Average-case analysis using recurrences  An alternative approach is to set up a recurrence relation for the    T   (  n  )       T  n    T(n)   factor, the time needed to sort a list of size   n   n   n   . In the most unbalanced case, a single quicksort call involves    O   (  n  )       O  n    O(n)   work plus two recursive calls on lists of size   0   0    and    n  âˆ’  1      n  normal-âˆ’  1    nâˆ’1   , so the recurrence relation is        T   (  n  )    =    O   (  n  )    +   T   (  0  )    +   T   (   n  -  1   )     =    O   (  n  )    +   T   (   n  -  1   )      .          T  n       O  n     T  0     T    n  1              O  n     T    n  1        T(n)=O(n)+T(0)+T(n-1)=O(n)+T(n-1).     This is the same relation as for insertion sort and selection sort , and it solves to worst case     T   (  n  )    =   O   (   n  Â²   )          T  n     O    n  normal-Â²      T(n)=O(nÂ²)   .  In the most balanced case, a single quicksort call involves    O   (  n  )       O  n    O(n)   work plus two recursive calls on lists of size    n  /  2      n  2    n/2   , so the recurrence relation is        T   (  n  )    =    O   (  n  )    +   2  T   (   n  2   )      .        T  n       O  n     2  T    n  2       T(n)=O(n)+2T\left(\frac{n}{2}\right).     The master theorem tells us that     T   (  n  )    =   O   (   n  l  o  g  n   )          T  n     O    n  l  o  g  n      T(n)=O(nlogn)   .  The outline of a formal proof of the    O   (   n  l  o  g  n   )       O    n  l  o  g  n     O(nlogn)   expected time complexity follows. Assume that there are no duplicates as duplicates could be handled with linear time pre- and post-processing, or considered cases easier than the analyzed. When the input is a random permutation, the rank of the pivot is uniform random from 0 to    n  âˆ’  1      n  normal-âˆ’  1    nâˆ’1   . Then the resulting parts of the partition have sizes   i   i   i   and    n  âˆ’  i  âˆ’  1      n  normal-âˆ’  i  normal-âˆ’  1    nâˆ’iâˆ’1   , and i is uniform random from 0 to    n  âˆ’  1      n  normal-âˆ’  1    nâˆ’1   . So, averaging over all possible splits and noting that the number of comparisons for the partition is    n  âˆ’  1      n  normal-âˆ’  1    nâˆ’1   , the average number of comparisons over all permutations of the input sequence can be estimated accurately by solving the recurrence relation:       C   (  n  )    =    n  -  1   +    1  n     âˆ‘   i  =  0    n  -  1     (    C   (  i  )    +   C   (   n  -  i  -  1   )     )            C  n       n  1       1  n     superscript   subscript     i  0      n  1        C  i     C    n  i  1          C(n)=n-1+\frac{1}{n}\sum_{i=0}^{n-1}(C(i)+C(n-i-1))     Solving the recurrence gives     C   (  n  )    =   2  n  l  n  n  â‰ˆ  1.39  n  l  o  g  â‚‚  n         C  n     2  n  l  n  n  normal-â‰ˆ  1.39  n  l  o  g  normal-â‚‚  n     C(n)=2nlnnâ‰ˆ1.39nlogâ‚‚n   .  This means that, on average, quicksort performs only about 39% worse than in its best case. In this sense it is closer to the best case than the worst case. Also note that a comparison sort cannot use less than    l  o  g  â‚‚   (   n  !   )       l  o  g  normal-â‚‚    n     logâ‚‚(n!)   comparisons on average to sort   n   n   n   items (as explained in the article Comparison sort ) and in case of large   n   n   n   , Stirling's approximation yields    l  o  g  â‚‚   (   n  !   )   â‰ˆ  n   (   l  o  g  â‚‚  n  âˆ’  l  o  g  â‚‚  e   )       l  o  g  normal-â‚‚    n   normal-â‰ˆ  n    l  o  g  normal-â‚‚  n  normal-âˆ’  l  o  g  normal-â‚‚  e     logâ‚‚(n!)â‰ˆn(logâ‚‚nâˆ’logâ‚‚e)   , so quicksort is not much worse than an ideal comparison sort. This fast average runtime is another reason for quicksort's practical dominance over other sorting algorithms.  Analysis of randomized quicksort  Using the same analysis, one can show that randomized quicksort has the desirable property that, for any input, it requires only    O   (   n  l  o  g  n   )       O    n  l  o  g  n     O(nlogn)    expected time (averaged over all choices of pivots). However, there also exists a combinatorial proof.  To each execution of quicksort corresponds the following binary search tree (BST): the initial pivot is the root node; the pivot of the left half is the root of the left subtree, the pivot of the right half is the root of the right subtree, and so on. The number of comparisons of the execution of quicksort equals the number of comparisons during the construction of the BST by a sequence of insertions. So, the average number of comparisons for randomized quicksort equals the average cost of constructing a BST when the values inserted    (   x  1   ,   x  2   ,  â€¦  ,   x  n   )      subscript  x  1    subscript  x  2   normal-â€¦   subscript  x  n     (x_{1},x_{2},...,x_{n})   form a random permutation.  Consider a BST created by insertion of a sequence    (   x  1   ,   x  2   ,  â€¦  ,   x  n   )      subscript  x  1    subscript  x  2   normal-â€¦   subscript  x  n     (x_{1},x_{2},...,x_{n})   of values forming a random permutation. Let   C   C   C   denote the cost of creation of the BST. We have    x  i     subscript  x  i    x_{i}   is an binary random variable expressing whether during the insertion of    x  j     subscript  x  j    x_{j}   there was a comparison to    ð”¼   [  C  ]       ð”¼   delimited-[]  C     \mathbb{E}[C]   .  By linearity of expectation , the expected value   C   C   C   of   i   i   i   is    j  <  i      j  i    j   , once sorted, define    x  j     subscript  x  j    x_{j}   intervals. The core structural observation is that    x  i     subscript  x  i    x_{i}   is compared to    x  j     subscript  x  j    x_{j}   in the algorithm if and only if    (   x  1   ,   x  2   ,  â€¦  ,   x  n   )      subscript  x  1    subscript  x  2   normal-â€¦   subscript  x  n     (x_{1},x_{2},...,x_{n})   falls inside one of the two intervals adjacent to    (   x  1   ,   x  2   ,  â€¦  ,   x  j   ,   x  i   )      subscript  x  1    subscript  x  2   normal-â€¦   subscript  x  j    subscript  x  i     (x_{1},x_{2},...,x_{j},x_{i})   .  Observe that since    x  i     subscript  x  i    x_{i}   is a random permutation,    x  j     subscript  x  j    x_{j}   is also a random permutation, so the probability that    2   j  +  1       2    j  1     \frac{2}{j+1}   is adjacent to    O   (   l  o  g  n   )       O    l  o  g  n     O(logn)   is exactly    O   (  1  )       O  1    O(1)   .  We end with a short calculation:    O   (   l  o  g  n   )       O    l  o  g  n     O(logn)   comparisons (close to the information theoretic lower bound) and ${\Theta}(n\log n)$ operations; at worst they perform ${\Theta}(n\log^2 n)$ comparisons (and also operations); these are in-place, requiring only additional ${O}(\log n)$ space. Practical efficiency and smaller variance in performance were demonstrated against optimised quicksorts (of Sedgewick and Bentley - McIlroy ). 33  See also   Introsort  Flashsort   Notes  References        (Reprinted in Hoare and Jones: Essays in computing science , 1989.)   Donald Knuth . The Art of Computer Programming , Volume 3: Sorting and Searching , Third Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Pages 113â€“122 of section 5.2.2: Sorting by Exchanging.  Thomas H. Cormen , Charles E. Leiserson , Ronald L. Rivest , and Clifford Stein . Introduction to Algorithms , Second Edition. MIT Press and McGraw-Hill , 2001. ISBN 0-262-03293-7. Chapter 7: Quicksort, pp.Â 145â€“164.  A. LaMarca and R. E. Ladner. "The Influence of Caches on the Performance of Sorting." Proceedings of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, 1997. pp.Â 370â€“379.  Faron Moller . Analysis of Quicksort . CS 332: Designing Algorithms. Department of Computer Science, Swansea University .     External links   Animated Sorting Algorithms: Quick Sort â€“ graphical demonstration and discussion of quick sort  Open Data Structures - Section 11.1.2 - Quicksort  Literate implementations of Quicksort in various languages on LiteratePrograms   no:Sorteringsalgoritme#Quick sort "  Category:Sorting algorithms  Category:Comparison sorts  Category:Articles with example pseudocode  Category:1961 in science     â†©  â†©  â†©  â†©  â†©   â†©    â†©  â†©  â†©  â†©  â†©  â†©    â†©   â†©     â†©  qsort.c in GNU libc : 1 , 2 â†©  http://www.ugrad.cs.ubc.ca/~cs260/chnotes/ch6/Ch6CovCompiled.html â†©  â†©  Umut A. Acar, Guy E Blelloch, Margaret Reid-Miller, and Kanat Tangwongsan, Quicksort and Sorting Lower Bounds , Parallel and Sequential Data Structures and Algorithms . 2013. â†©  â†©  â†©  â†©  â†©  Richard Cole, David C. Kandathil: "The average case analysis of Partition sorts" , European Symposium on Algorithms, 14â€“17 September 2004, Bergen, Norway. Published: Lecture Notes in Computer Science 3221, Springer Verlag, pp. 240-251. â†©    