   Kurtosis      Kurtosis   In probability theory and statistics , kurtosis (from , kyrtos or kurtos , meaning "curved, arching") is any measure of the "peakedness" of the probability distribution of a real -valued random variable . 1 In a similar way to the concept of skewness , kurtosis is a descriptor of the shape of a probability distribution and, just as for skewness, there are different ways of quantifying it for a theoretical distribution and corresponding ways of estimating it from a sample from a population. There are various interpretations of kurtosis, and of how particular measures should be interpreted; these are primarily peakedness (width of peak), tail weight, and lack of shoulders (distribution primarily peak and tails, not in between).  One common measure of kurtosis, originating with Karl Pearson , is based on a scaled version of the fourth moment of the data or population, but it has been argued that this really measures heavy tails , and not peakedness. 2 For this measure, higher kurtosis means more of the variance is the result of infrequent extreme deviations , as opposed to frequent modestly sized deviations. It is common practice to use an adjusted version of Pearson's kurtosis, the excess kurtosis , to provide a comparison of the shape of a given distribution to that of the normal distribution . Distributions with negative or positive excess kurtosis are called platykurtic distributions or leptokurtic distributions respectively.  Alternative measures of kurtosis are: the L-kurtosis , which is a scaled version of the fourth L-moment ; measures based on 4 population or sample quantiles . 3 These correspond to the alternative measures of skewness that are not based on ordinary moments. 4  (Figure)  The "Darkness" data is platykurtic (−0.194), while "Far Red Light" shows leptokurtosis (0.055)   Pearson moments  The fourth standardized moment is defined as        β  2   =    E   [    (   X  -  μ   )   4   ]      (   E   [    (   X  -  μ   )   2   ]    )   2    =    μ  4    σ  4     ,         subscript  β  2      normal-E   superscript    X  μ   4     superscript   normal-E   superscript    X  μ   2    2            subscript  μ  4    superscript  σ  4       {\beta_{2}=}\frac{\operatorname{E}[(X-{\mu})^{4}]}{(\operatorname{E}[(X-{\mu})%
 ^{2}])^{2}}{=}\frac{\mu_{4}}{\sigma^{4}},   where μ 4 is the fourth moment about the mean and σ is the standard deviation . The fourth standardized moment is bounded below by the squared skewness plus 1: 5        μ  4    σ  4    ≥     (    μ  3    σ  3    )   2   +  1.          subscript  μ  4    superscript  σ  4       superscript     subscript  μ  3    superscript  σ  3    2   1.     \frac{\mu_{4}}{\sigma^{4}}\geq\left(\frac{\mu_{3}}{\sigma^{3}}\right)^{2}+1.   where μ 3 is the third moment about the mean . A complementary upper bound for the fourth standardized moment of n (n>2) real numbers is 6         μ  4    σ  4    ≤     1  2     n  -  3    n  -  2      (    μ  3    σ  3    )   2    +   n  2     .         subscript  μ  4    superscript  σ  4          1  2       n  3     n  2     superscript     subscript  μ  3    superscript  σ  3    2      n  2      \frac{\mu_{4}}{\sigma^{4}}\leq\frac{1}{2}\frac{n-3}{n-2}\left(\frac{\mu_{3}}{%
 \sigma^{3}}\right)^{2}+\frac{n}{2}.     The fourth standardized moment is sometimes used as the definition of kurtosis in older works, but is not the definition used here.  Kurtosis is more commonly defined as the fourth cumulant divided by the square of the second cumulant, which is equal to the fourth moment around the mean divided by the square of the variance of the probability distribution minus 3,        γ  2   =    κ  4    κ  2  2    =     μ  4    σ  4    -  3    ,         subscript  γ  2      subscript  κ  4    superscript   subscript  κ  2   2              subscript  μ  4    superscript  σ  4    3      \gamma_{2}=\frac{\kappa_{4}}{\kappa_{2}^{2}}=\frac{\mu_{4}}{\sigma^{4}}-3,     which is also known as . The "minus 3" at the end of this formula is often explained as a correction to make the kurtosis of the normal distribution equal to zero. Another reason can be seen by looking at the formula for the kurtosis of the sum of random variables. Suppose that Y is the sum of n identically distributed independent random variables all with the same distribution as X . Then        Kurt   [  Y  ]    =     κ  4    (  Y  )      κ  2     (  Y  )   2     =    n   κ  4    (  X  )      (   n   κ  2    (  X  )    )   2    =    1  n      κ  4    (  X  )      κ  2     (  X  )   2      =    1  n    Kurt   [  X  ]      .         Kurt  Y        subscript  κ  4   Y      subscript  κ  2    superscript  Y  2              n   subscript  κ  4   X    superscript    n   subscript  κ  2   X   2             1  n        subscript  κ  4   X      subscript  κ  2    superscript  X  2               1  n    Kurt  X       \operatorname{Kurt}[Y]=\frac{\kappa_{4}(Y)}{\kappa_{2}(Y)^{2}}=\frac{n\kappa_{%
 4}(X)}{(n\kappa_{2}(X))^{2}}=\frac{1}{n}\frac{\kappa_{4}(X)}{\kappa_{2}(X)^{2}%
 }=\frac{1}{n}\operatorname{Kurt}[X].     This formula would be much more complicated if kurtosis were defined just as μ 4 / σ 4 (without the minus 3).  More generally, if X 1 , ..., X n are independent random variables, not necessarily identically distributed, but all having the same variance, then        Kurt   (    ∑   i  =  1   n    X  i    )    =    1   n  2      ∑   i  =  1   n    Kurt   (   X  i   )       ,       Kurt    superscript   subscript     i  1    n    subscript  X  i         1   superscript  n  2      superscript   subscript     i  1    n    Kurt   subscript  X  i        \operatorname{Kurt}\left(\sum_{i=1}^{n}X_{i}\right)={1\over n^{2}}\sum_{i=1}^{%
 n}\operatorname{Kurt}(X_{i}),     whereas this identity would not hold if the definition did not include the subtraction of 3.  Without the assumption about    X  i     subscript  X  i    X_{i}   having the same variance, we have        Kurt   (    ∑   i  =  1   n    X  i    )    =    ∑   i  =  1   n      σ  i  4   ⋅   Kurt   (   X  i   )       (    ∑   j  =  1   n    σ  j  2    )   2      ,       Kurt    superscript   subscript     i  1    n    subscript  X  i       superscript   subscript     i  1    n      normal-⋅   superscript   subscript  σ  i   4    Kurt   subscript  X  i      superscript    superscript   subscript     j  1    n    superscript   subscript  σ  j   2    2       \operatorname{Kurt}\left(\sum_{i=1}^{n}X_{i}\right)=\sum_{i=1}^{n}\frac{\sigma%
 _{i}^{\,4}\cdot\operatorname{Kurt}(X_{i})}{\left(\sum_{j=1}^{n}\sigma_{j}^{\,2%
 }\right)^{2}},   where    σ  i     subscript  σ  i    \sigma_{i}   is the standard deviation of    X  i     subscript  X  i    X_{i}   .  The fourth standardized moment must be at least 1, so the excess kurtosis must be −2 or more. This lower bound is realized by the Bernoulli distribution with p = ½, or "coin toss". There is no upper limit to the excess kurtosis and it may be infinite.  Interpretation  The exact interpretation of the Pearson measure of kurtosis (or excess kurtosis) is disputed. The "classical" interpretation, which applies only to symmetric and unimodal distributions (those whose skewness is 0), is that kurtosis measures both the "peakedness" of the distribution and the heaviness of its tail . 7 Various statisticians have proposed other interpretations, such as "lack of shoulders" (where the "shoulder" is defined vaguely as the area between the peak and the tail, or more specifically as the area about one standard deviation from the mean) or "bimodality". 8 Balanda and MacGillivray assert that the standard definition of kurtosis "is a poor measure of the kurtosis, peakedness, or tail weight of a distribution" 9 and instead propose to "define kurtosis vaguely as the location- and scale-free movement of probability mass from the shoulders of a distribution into its center and tails". 10  Moors' interpretation  Moors has given a more intuitive explanation of the meaning of kurtosis. 11 Let      Z  =    X  -  μ   σ       Z      X  μ   σ     Z=\frac{X-\mu}{\sigma}     where X is a random variable, μ is the mean and σ is the standard deviation. Then      κ  =   E   (   Z  4   )    =    v  a  r   (   Z  2   )    +    [   E   (   Z  2   )    ]   2    =    v  a  r   (   Z  2   )    +  1         κ    E   superscript  Z  4             v  a  r   superscript  Z  2     superscript   delimited-[]    E   superscript  Z  2     2             v  a  r   superscript  Z  2    1      \kappa=E(Z^{4})=var(Z^{2})+[E(Z^{2})]^{2}=var(Z^{2})+1     where κ is the kurtosis, var is the variance and E () is the expectation operator. The kurtosis can now be seen to be a measure of the dispersion of Z 2 around its expectation. Alternatively it can be seen to be a measure of the dispersion of Z around +1 and -1. κ attains its minimum value in a symmetric two point distribution. In terms of the original variable X , the kurtosis is a measure of the dispersion of X around the two values μ ± σ .  High values of κ arise in two circumstances   where the probability mass is concentrated around the mean    where the probability mass is concentrated in the tails of the distribution.   Terminology and examples  A high kurtosis distribution has a sharper peak and fatter tails , while a low kurtosis distribution has a more rounded peak and thinner tails.  Distributions with zero excess kurtosis are called mesokurtic , or mesokurtotic. The most prominent example of a mesokurtic distribution is the normal distribution family, regardless of the values of its parameters . A few other well-known distributions can be mesokurtic, depending on parameter values: for example the binomial distribution is mesokurtic for    p  =    1  /  2   ±    1  /  12         p   plus-or-minus    1  2       1  12       p=1/2\pm\sqrt{1/12}   .  A distribution with positive excess kurtosis is called leptokurtic , or leptokurtotic. "Lepto-" means "slender". 12 In terms of shape, a leptokurtic distribution has a more acute peak around the mean and fatter tails . Examples of leptokurtic distributions include the Student's t-distribution , Rayleigh distribution , Laplace distribution , exponential distribution , Poisson distribution and the logistic distribution . Such distributions are sometimes termed super-Gaussian .  A distribution with negative excess kurtosis is called platykurtic , or platykurtotic. "Platy-" means "broad". 13 In terms of shape, a platykurtic distribution has a lower, wider peak around the mean and thinner tails . Examples of platykurtic distributions include the continuous or discrete uniform distributions , and the raised cosine distribution . The most platykurtic distribution of all is the Bernoulli distribution with p = ½ (for example the number of times one obtains "heads" when flipping a coin once, a coin toss ), for which the excess kurtosis is −2. Such distributions are sometimes termed sub-Gaussian . 14   Graphical examples  The Pearson type VII family    The effects of kurtosis are illustrated using a parametric family of distributions whose kurtosis can be adjusted while their lower-order moments and cumulants remain constant. Consider the Pearson type VII family , which is a special case of the Pearson type IV family restricted to symmetric densities. The probability density function is given by        f   (  x  ;  a  ,  m  )    =     Γ   (  m  )      a     π    Γ   (   m  -   1  /  2    )       [   1  +    (   x  a   )   2    ]    -  m      ,        f   x  a  m          normal-Γ  m     a    π   normal-Γ    m    1  2       superscript   delimited-[]    1   superscript    x  a   2       m       f(x;a,m)=\frac{\Gamma(m)}{a\,\sqrt{\pi}\,\Gamma(m-1/2)}\left[1+\left(\frac{x}{%
 a}\right)^{2}\right]^{-m},\!     where a is a scale parameter and m is a shape parameter .  All densities in this family are symmetric. The k th moment exists provided m > ( k + 1)/2. For the kurtosis to exist, we require m > 5/2. Then the mean and skewness exist and are both identically zero. Setting a 2 = 2 m − 3 makes the variance equal to unity. Then the only free parameter is m , which controls the fourth moment (and cumulant) and hence the kurtosis. One can reparameterize with    m  =    5  /  2   +   3  /   γ  2         m      5  2     3   subscript  γ  2       m=5/2+3/\gamma_{2}   , where    γ  2     subscript  γ  2    \gamma_{2}   is the excess kurtosis as defined above. This yields a one-parameter leptokurtic family with zero mean, unit variance, zero skewness, and arbitrary positive kurtosis. The reparameterized density is      g   (  x  ;   γ  2   )   =  f   (  x  ;  a  =    2  +   6  /   γ  2      ,  m  =  5  /  2  +  3  /   γ  2   )   .     fragments  g   fragments  normal-(  x  normal-;   subscript  γ  2   normal-)    f   fragments  normal-(  x  normal-;  a       2    6   subscript  γ  2      normal-,  m   5   2   3    subscript  γ  2   normal-)   normal-.    g(x;\gamma_{2})=f(x;\;a=\sqrt{2+6/\gamma_{2}},\;m=5/2+3/\gamma_{2}).\!     In the limit as     γ  2   →  ∞     normal-→   subscript  γ  2      \gamma_{2}\to\infty   one obtains the density        g   (  x  )    =   3    (   2  +   x  2    )    -   5  /  2       ,        g  x     3   superscript    2   superscript  x  2        5  2        g(x)=3\left(2+x^{2}\right)^{-5/2},\!     which is shown as the red curve in the images on the right.  In the other direction as     γ  2   →  0     normal-→   subscript  γ  2   0    \gamma_{2}\to 0   one obtains the standard normal density as the limiting distribution, shown as the black curve.  In the images on the right, the blue curve represents the density    x  ↦   g   (  x  ;  2  )       maps-to  x    g   x  2      x\mapsto g(x;2)   with kurtosis of 2. The top image shows that leptokurtic densities in this family have a higher peak than the mesokurtic normal density. The comparatively fatter tails of the leptokurtic densities are illustrated in the second image, which plots the natural logarithm of the Pearson type VII densities: the black curve is the logarithm of the standard normal density, which is a parabola . One can see that the normal density allocates little probability mass to the regions far from the mean ("has thin tails"), compared with the blue curve of the leptokurtic Pearson type VII density with kurtosis of 2. Between the blue curve and the black are other Pearson type VII densities with γ 2 = 1, 1/2, 1/4, 1/8, and 1/16. The red curve again shows the upper limit of the Pearson type VII family, with     γ  2   =  ∞       subscript  γ  2      \gamma_{2}=\infty   (which, strictly speaking, means that the fourth moment does not exist). The red curve decreases the slowest as one moves outward from the origin ("has fat tails").  Kurtosis of well-known distributions    Several well-known, unimodal and symmetric distributions from different parametric families are compared here. Each has a mean and skewness of zero. The parameters have been chosen to result in a variance equal to 1 in each case. The images on the right show curves for the following seven densities, on a linear scale and logarithmic scale:   D: Laplace distribution , also known as the double exponential distribution, red curve (two straight lines in the log-scale plot), excess kurtosis = 3    S: hyperbolic secant distribution , orange curve, excess kurtosis = 2    L: logistic distribution , green curve, excess kurtosis = 1.2    N: normal distribution , black curve (inverted parabola in the log-scale plot), excess kurtosis = 0    C: raised cosine distribution , cyan curve, excess kurtosis = −0.593762...    W: Wigner semicircle distribution , blue curve, excess kurtosis = −1    U: uniform distribution , magenta curve (shown for clarity as a rectangle in both images), excess kurtosis = −1.2.   Note that in these cases the platykurtic densities have bounded support , whereas the densities with positive or zero excess kurtosis are supported on the whole real line .  There exist platykurtic densities with infinite support,   e.g., exponential power distributions with sufficiently large shape parameter b   and there exist leptokurtic densities with finite support.   e.g., a distribution that is uniform between −3 and −0.3, between −0.3 and 0.3, and between 0.3 and 3, with the same density in the (−3, −0.3) and (0.3, 3) intervals, but with 20 times more density in the (−0.3, 0.3) interval   Sample kurtosis  For a sample of n values the sample excess kurtosis is       g  2   =     m  4    m  2  2    -  3   =      1  n     ∑   i  =  1   n     (    x  i   -   x  ¯    )   4       (    1  n     ∑   i  =  1   n     (    x  i   -   x  ¯    )   2     )   2    -  3          subscript  g  2        subscript  m  4    superscript   subscript  m  2   2    3                1  n     superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¯  x    4      superscript      1  n     superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¯  x    2     2    3      g_{2}=\frac{m_{4}}{m_{2}^{2}}-3=\frac{\tfrac{1}{n}\sum_{i=1}^{n}(x_{i}-%
 \overline{x})^{4}}{\left(\tfrac{1}{n}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}%
 \right)^{2}}-3     where m 4 is the fourth sample moment about the mean , m 2 is the second sample moment about the mean (that is, the sample variance ), x i is the i th value, and    x  ¯     normal-¯  x    \overline{x}   is the sample mean .  The variance of the sample kurtosis of a sample of size n from the normal distribution is 15       24  n    (   n  -  1   )   2      (   n  -  3   )    (   n  -  2   )    (   n  +  3   )    (   n  +  5   )          24  n   superscript    n  1   2        n  3     n  2     n  3     n  5      \frac{24n(n-1)^{2}}{(n-3)(n-2)(n+3)(n+5)}     An approximate alternative is 24/ n but this is inaccurate for small samples.  Estimators of population kurtosis  Given a sub-set of samples from a population, the sample excess kurtosis above is a biased estimator of the population excess kurtosis. The usual estimator of the population excess kurtosis defined as follows:       G  2   =    k  4    k  2  2         subscript  G  2      subscript  k  4    superscript   subscript  k  2   2      G_{2}=\frac{k_{4}}{k_{2}^{2}}        =        n  2     (     (   n  +  1   )    m  4    -    3    (   n  -  1   )    m  2  2     )      (   n  -  1   )    (   n  -  2   )    (   n  -  3   )         (   n  -  1   )   2      n  2     m  2  2          absent         superscript  n  2         n  1    subscript  m  4      3    n  1    superscript   subscript  m  2   2          n  1     n  2     n  3        superscript    n  1   2      superscript  n  2    superscript   subscript  m  2   2        =\frac{n^{2}\,((n+1)\,m_{4}-3\,(n-1)\,m_{2}^{2})}{(n-1)\,(n-2)\,(n-3)}\;\frac{%
 (n-1)^{2}}{n^{2}\,m_{2}^{2}}         =     n  -  1     (   n  -  2   )    (   n  -  3   )      (     (   n  +  1   )     m  4    m  2  2     -    3    (   n  -  1   )     )        absent        n  1       n  2     n  3           n  1      subscript  m  4    superscript   subscript  m  2   2       3    n  1        =\frac{n-1}{(n-2)\,(n-3)}\left((n+1)\,\frac{m_{4}}{m_{2}^{2}}-3\,(n-1)\right)         =     n  -  1     (   n  -  2   )    (   n  -  3   )      (     (   n  +  1   )    g  2    +  6   )        absent        n  1       n  2     n  3           n  1    subscript  g  2    6      =\frac{n-1}{(n-2)\,(n-3)}\left((n+1)\,g_{2}+6\right)         =        (   n  +  1   )    n    (   n  -  1   )      (   n  -  2   )    (   n  -  3   )          ∑   i  =  1   n      (    x  i   -   x  ¯    )   4      (     ∑   i  =  1   n      (    x  i   -   x  ¯    )   2    )   2     -    3      (   n  -  1   )   2     (   n  -  2   )    (   n  -  3   )           absent            n  1   n    n  1        n  2     n  3         superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¯  x    4     superscript    superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¯  x    2    2       3     superscript    n  1   2       n  2     n  3         =\frac{(n+1)\,n\,(n-1)}{(n-2)\,(n-3)}\;\frac{\sum_{i=1}^{n}(x_{i}-\bar{x})^{4}%
 }{\left(\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\right)^{2}}-3\,\frac{(n-1)^{2}}{(n-2%
 )\,(n-3)}         =        (   n  +  1   )   n     (   n  -  1   )    (   n  -  2   )    (   n  -  3   )          ∑   i  =  1   n      (    x  i   -   x  ¯    )   4     k  2  2     -    3      (   n  -  1   )   2     (   n  -  2   )    (   n  -  3   )           absent            n  1   n       n  1     n  2     n  3         superscript   subscript     i  1    n    superscript     subscript  x  i    normal-¯  x    4     superscript   subscript  k  2   2       3     superscript    n  1   2       n  2     n  3         =\frac{(n+1)\,n}{(n-1)\,(n-2)\,(n-3)}\;\frac{\sum_{i=1}^{n}(x_{i}-\bar{x})^{4}%
 }{k_{2}^{2}}-3\,\frac{(n-1)^{2}}{(n-2)(n-3)}     where k 4 is the unique symmetric unbiased estimator of the fourth cumulant , k 2 is the unbiased estimate of the second cumulant (identical to the unbiased estimate of the sample variance), m 4 is the fourth sample moment about the mean, m 2 is the second sample moment about the mean, x i is the i th value, and    x  ¯     normal-¯  x    \bar{x}   is the sample mean. Unfortunately,    G  2     subscript  G  2    G_{2}   is itself generally biased. For the normal distribution it is unbiased. 16  Applications  D'Agostino's K-squared test is a goodness-of-fit  normality test based on a combination of the sample skewness and sample kurtosis, as is the Jarque–Bera test for normality.  For non-normal samples, the variance of the sample variance depends on the kurtosis; for details, please see variance .  Pearson's definition of kurtosis is used as an indicator of intermittency in turbulence . 17  Applying band-pass filters to digital images, kurtosis values tend to be uniform, independent of the range of the filter. This behavior, termed kurtosis convergence , can be used to detect image splicing in forensic analysis. 18  Other measures of kurtosis  A different measure of "kurtosis", that is of the "peakedness" of a distribution, is provided by using L-moments instead of the ordinary moments. 19 20  See also   Kurtosis risk   References  Further reading    Alternative source (Comparison of kurtosis estimators)      External links    [ http://www.fxsolver.com/solve/share/RMqwaVp85T_5rbacksPD4g ==/ Kurtosis calculator]  Free Online Software (Calculator) computes various types of skewness and kurtosis statistics for any dataset (includes small and large sample tests)..  Kurtosis on the Earliest known uses of some of the words of mathematics  Celebrating 100 years of Kurtosis a history of the topic, with different measures of kurtosis.   "  Category:Theory of probability distributions  Category:Statistical deviation and dispersion     Dodge, Y. (2003) The Oxford Dictionary of Statistical Terms , OUP. ISBN 0-19-920613-9 ↩  SAS Elementary Statistics Procedures , SAS Institute (section on Kurtosis) ↩  ↩   ↩  ↩  ↩  ↩  Balanda and MacGillivray, p. 114. ↩   Moors JJA (1986b) The meaning of kurtosis: Darlington reexamined. The American Statistician 40 (4) 283-284 ↩  http://medical-dictionary.thefreedictionary.com/lepto- ↩  http://www.yourdictionary.com/platy-prefix ↩  The original paper presenting sub-Gaussians  See also ↩  ↩   ↩  1 ↩  ↩  ↩     