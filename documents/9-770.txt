   Squared deviations      Squared deviations   In probability theory and statistics , the definition of variance is either the expected value (when considering a theoretical distribution ), or average value (for actual experimental data), of squared deviations from the mean. Computations for analysis of variance involve the partitioning of a sum of squared deviations . An understanding of the complex computations involved is greatly enhanced by a detailed study of the statistical value:       E   (   X  2   )    .     normal-E   superscript  X  2     \operatorname{E}(X^{2}).     It is well known that for a random variable    X   X   X   with mean   μ   μ   \mu   and variance    σ  2     superscript  σ  2    \sigma^{2}   :       σ  2   =    E   (   X  2   )    -   μ  2         superscript  σ  2      normal-E   superscript  X  2     superscript  μ  2      \sigma^{2}=\operatorname{E}(X^{2})-\mu^{2}    1  Therefore        E   (   X  2   )    =    σ  2   +   μ  2     .       normal-E   superscript  X  2       superscript  σ  2    superscript  μ  2      \operatorname{E}(X^{2})=\sigma^{2}+\mu^{2}.     From the above, the following are easily derived:       E   (   ∑   (   X  2   )    )    =    n   σ  2    +   n   μ  2          normal-E     superscript  X  2         n   superscript  σ  2      n   superscript  μ  2       \operatorname{E}\left(\sum\left(X^{2}\right)\right)=n\sigma^{2}+n\mu^{2}          E   (    (   ∑  X   )   2   )    =    n   σ  2    +    n  2    μ  2          normal-E   superscript    X   2        n   superscript  σ  2       superscript  n  2    superscript  μ  2       \operatorname{E}\left(\left(\sum X\right)^{2}\right)=n\sigma^{2}+n^{2}\mu^{2}     If    Y  ^     normal-^  Y    \hat{Y}   is a vector of n predictions, and   Y   Y   Y   is the vector of the true values, then the SSE of the predictor is:     S  S  E   =    1  2     ∑   i  =  1   n     (     Y  i   ^   -   Y  i    )   2           S  S  E       1  2     superscript   subscript     i  1    n    superscript     normal-^   subscript  Y  i     subscript  Y  i    2       SSE=\frac{1}{2}\sum_{i=1}^{n}(\hat{Y_{i}}-Y_{i})^{2}     Sample variance  The sum of squared deviations needed to calculate sample variance (before deciding whether to divide by n or n − 1) is most easily calculated as      S  =    ∑   x  2    -     (   ∑  x   )   2   n        S       superscript  x  2       superscript    x   2   n      S=\sum x^{2}-\frac{\left(\sum x\right)^{2}}{n}     From the two derived expectations above the expected value of this sum is       E   (  S  )    =     n   σ  2    +   n   μ  2     -     n   σ  2    +    n  2    μ  2     n         normal-E  S         n   superscript  σ  2      n   superscript  μ  2           n   superscript  σ  2       superscript  n  2    superscript  μ  2     n      \operatorname{E}(S)=n\sigma^{2}+n\mu^{2}-\frac{n\sigma^{2}+n^{2}\mu^{2}}{n}     which implies        E   (  S  )    =    (   n  -  1   )    σ  2     .       normal-E  S       n  1    superscript  σ  2      \operatorname{E}(S)=(n-1)\sigma^{2}.     This effectively proves the use of the divisor n − 1 in the calculation of an unbiased sample estimate of σ 2 .  Partition — analysis of variance  In the situation where data is available for k different treatment groups having size n i where i varies from 1 to k , then it is assumed that the expected mean of each group is       E   (   μ  i   )    =   μ  +   T  i         normal-E   subscript  μ  i      μ   subscript  T  i      \operatorname{E}(\mu_{i})=\mu+T_{i}     and the variance of each treatment group is unchanged from the population variance    σ  2     superscript  σ  2    \sigma^{2}   .  Under the Null Hypothesis that the treatments have no effect, then each of the    T  i     subscript  T  i    T_{i}   will be zero.  It is now possible to calculate three sums of squares:   Individual       I  =   ∑   x  2        I     superscript  x  2      I=\sum x^{2}          E   (  I  )    =    n   σ  2    +   n   μ  2          normal-E  I       n   superscript  σ  2      n   superscript  μ  2       \operatorname{E}(I)=n\sigma^{2}+n\mu^{2}      Treatments       T  =    ∑   i  =  1   k    (     (   ∑  x   )   2   /   n  i    )        T    superscript   subscript     i  1    k      superscript    x   2    subscript  n  i       T=\sum_{i=1}^{k}\left(\left(\sum x\right)^{2}/n_{i}\right)          E   (  T  )    =    k   σ  2    +    ∑   i  =  1   k     n  i     (   μ  +   T  i    )   2           normal-E  T       k   superscript  σ  2      superscript   subscript     i  1    k      subscript  n  i    superscript    μ   subscript  T  i    2        \operatorname{E}(T)=k\sigma^{2}+\sum_{i=1}^{k}n_{i}(\mu+T_{i})^{2}          E   (  T  )    =    k   σ  2    +   n   μ  2    +   2  μ    ∑   i  =  1   k    (    n  i    T  i    )     +    ∑   i  =  1   k     n  i     (   T  i   )   2           normal-E  T       k   superscript  σ  2      n   superscript  μ  2      2  μ    superscript   subscript     i  1    k      subscript  n  i    subscript  T  i        superscript   subscript     i  1    k      subscript  n  i    superscript   subscript  T  i   2        \operatorname{E}(T)=k\sigma^{2}+n\mu^{2}+2\mu\sum_{i=1}^{k}(n_{i}T_{i})+\sum_{%
 i=1}^{k}n_{i}(T_{i})^{2}     Under the null hypothesis that the treatments cause no differences and all the    T  i     subscript  T  i    T_{i}   are zero, the expectation simplifies to        E   (  T  )    =    k   σ  2    +   n   μ  2      .       normal-E  T       k   superscript  σ  2      n   superscript  μ  2       \operatorname{E}(T)=k\sigma^{2}+n\mu^{2}.      Combination       C  =     (   ∑  x   )   2   /  n       C     superscript    x   2   n     C=\left(\sum x\right)^{2}/n          E   (  C  )    =    σ  2   +   n   μ  2          normal-E  C      superscript  σ  2     n   superscript  μ  2       \operatorname{E}(C)=\sigma^{2}+n\mu^{2}     Sums of squared deviations  Under the null hypothesis, the difference of any pair of I , T , and C does not contain any dependency on   μ   μ   \mu   , only    σ  2     superscript  σ  2    \sigma^{2}   .       E   (   I  -  C   )    =    (   n  -  1   )    σ  2         normal-E    I  C        n  1    superscript  σ  2      \operatorname{E}(I-C)=(n-1)\sigma^{2}   total squared deviations aka total sum of squares       E   (   T  -  C   )    =    (   k  -  1   )    σ  2         normal-E    T  C        k  1    superscript  σ  2      \operatorname{E}(T-C)=(k-1)\sigma^{2}   treatment squared deviations aka explained sum of squares       E   (   I  -  T   )    =    (   n  -  k   )    σ  2         normal-E    I  T        n  k    superscript  σ  2      \operatorname{E}(I-T)=(n-k)\sigma^{2}   residual squared deviations aka residual sum of squares  The constants ( n − 1), ( k − 1), and ( n − k ) are normally referred to as the number of degrees of freedom .  Example  In a very simple example, 5 observations arise from two treatments. The first treatment gives three values 1, 2, and 3, and the second treatment gives two values 4, and 6.      I  =     1  2   1   +    2  2   1   +    3  2   1   +    4  2   1   +    6  2   1    =  66        I       superscript  1  2   1      superscript  2  2   1      superscript  3  2   1      superscript  4  2   1      superscript  6  2   1         66     I=\frac{1^{2}}{1}+\frac{2^{2}}{1}+\frac{3^{2}}{1}+\frac{4^{2}}{1}+\frac{6^{2}}%
 {1}=66         T  =      (   1  +  2  +  3   )   2   3   +     (   4  +  6   )   2   2    =   12  +  50   =  62        T       superscript    1  2  3   2   3      superscript    4  6   2   2           12  50        62     T=\frac{(1+2+3)^{2}}{3}+\frac{(4+6)^{2}}{2}=12+50=62         C  =     (   1  +  2  +  3  +  4  +  6   )   2   5   =   256  /  5   =  51.2        C     superscript    1  2  3  4  6   2   5          256  5        51.2     C=\frac{(1+2+3+4+6)^{2}}{5}=256/5=51.2     Giving   Total squared deviations = 66 − 51.2 = 14.8 with 4 degrees of freedom.  Treatment squared deviations = 62 − 51.2 = 10.8 with 1 degree of freedom.  Residual squared deviations = 66 − 62 = 4 with 3 degrees of freedom.   Two-way analysis of variance  The following hypothetical example gives the yields of 15 plants subject to two different environmental variations, and three different fertilisers.       Extra CO 2   Extra humidity       No fertiliser   7, 2, 1   7, 6     Nitrate   11, 6   10, 7, 3     Phosphate   5, 3, 4   11, 4     Five sums of squares are calculated:      Factor   Calculation   Sum       σ  2     superscript  σ  2    \sigma^{2}          Individual        7  2   +   2  2   +   1  2   +   7  2   +   6  2   +   11  2   +   6  2   +   10  2   +   7  2   +   3  2   +   5  2   +   3  2   +   4  2   +   11  2   +   4  2        superscript  7  2    superscript  2  2    superscript  1  2    superscript  7  2    superscript  6  2    superscript  11  2    superscript  6  2    superscript  10  2    superscript  7  2    superscript  3  2    superscript  5  2    superscript  3  2    superscript  4  2    superscript  11  2    superscript  4  2     7^{2}+2^{2}+1^{2}+7^{2}+6^{2}+11^{2}+6^{2}+10^{2}+7^{2}+3^{2}+5^{2}+3^{2}+4^{2%
 }+11^{2}+4^{2}      641   15     Fertiliser × Environment          (   7  +  2  +  1   )   2   3   +     (   7  +  6   )   2   2   +     (   11  +  6   )   2   2   +     (   10  +  7  +  3   )   2   3   +     (   5  +  3  +  4   )   2   3   +     (   11  +  4   )   2   2          superscript    7  2  1   2   3      superscript    7  6   2   2      superscript    11  6   2   2      superscript    10  7  3   2   3      superscript    5  3  4   2   3      superscript    11  4   2   2     \frac{(7+2+1)^{2}}{3}+\frac{(7+6)^{2}}{2}+\frac{(11+6)^{2}}{2}+\frac{(10+7+3)^%
 {2}}{3}+\frac{(5+3+4)^{2}}{3}+\frac{(11+4)^{2}}{2}      556.1667   6     Fertiliser          (   7  +  2  +  1  +  7  +  6   )   2   5   +     (   11  +  6  +  10  +  7  +  3   )   2   5   +     (   5  +  3  +  4  +  11  +  4   )   2   5          superscript    7  2  1  7  6   2   5      superscript    11  6  10  7  3   2   5      superscript    5  3  4  11  4   2   5     \frac{(7+2+1+7+6)^{2}}{5}+\frac{(11+6+10+7+3)^{2}}{5}+\frac{(5+3+4+11+4)^{2}}{5}      525.4   3     Environment          (   7  +  2  +  1  +  11  +  6  +  5  +  3  +  4   )   2   8   +     (   7  +  6  +  10  +  7  +  3  +  11  +  4   )   2   7          superscript    7  2  1  11  6  5  3  4   2   8      superscript    7  6  10  7  3  11  4   2   7     \frac{(7+2+1+11+6+5+3+4)^{2}}{8}+\frac{(7+6+10+7+3+11+4)^{2}}{7}      519.2679   2     Composite         (   7  +  2  +  1  +  11  +  6  +  5  +  3  +  4  +  7  +  6  +  10  +  7  +  3  +  11  +  4   )   2   15       superscript    7  2  1  11  6  5  3  4  7  6  10  7  3  11  4   2   15    \frac{(7+2+1+11+6+5+3+4+7+6+10+7+3+11+4)^{2}}{15}      504.6   1     Finally, the sums of squared deviations required for the analysis of variance can be calculated.      Factor   Sum       σ  2     superscript  σ  2    \sigma^{2}      Total   Environment   Fertiliser   Fertiliser × Environment   Residual       Individual   641   15   1      1     Fertiliser × Environment   556.1667   6      1   −1     Fertiliser   525.4   3     1   −1      Environment   519.2679   2    1    −1      Composite   504.6   1   −1   −1   −1   1                Squared deviations     136.4   14.668   20.8   16.099   84.833     Degrees of freedom     14   1   2   2   9     See also   Variance decomposition  Errors and residuals in statistics  Absolute deviation   References    "  Category:Statistical deviation and dispersion  Category:Analysis of variance     Mood & Graybill: An introduction to the Theory of Statistics (McGraw Hill) ↩     