   Huber loss      Huber loss   In statistics , the Huber loss is a loss function used in robust regression , that is less sensitive to outliers in data than the squared error loss . A variant for classification is also sometimes used.  Definition  The Huber loss function describes the penalty incurred by an estimation procedure    f   f   f   . Huber (1964) defines the loss function piecewise by 1        L  δ    (  a  )    =   {        1  2     a  2         for   |  a  |    ≤  δ   ,         δ   (    |  a  |   -     1  2    δ    )    ,     otherwise.             subscript  L  δ   a    cases      1  2    superscript  a  2        for    a    δ     δ      a       1  2   δ     otherwise.     L_{\delta}(a)=\begin{cases}\frac{1}{2}{a^{2}}&\text{for }|a|\leq\delta,\\
 \delta(|a|-\frac{1}{2}\delta),&\text{otherwise.}\end{cases}     This function is quadratic for small values of   a   a   a   , and linear for large values, with equal values and slopes of the different sections at the two points where     |  a  |   =  δ        a   δ    |a|=\delta   . The variable   a   a   a   often refers to the residuals, that is to the difference between the observed and predicted values    a  =   y  -   f   (  x  )         a    y    f  x      a=y-f(x)   , so the former can be expanded to 2        L  δ    (  y  ,   f   (  x  )    )    =   {        1  2      (   y  -   f   (  x  )     )   2         for   |   y  -   f   (  x  )     |    ≤  δ   ,          δ    |   y  -   f   (  x  )     |    -     1  2     δ  2       otherwise.             subscript  L  δ    y    f  x      cases      1  2    superscript    y    f  x    2        for      y    f  x      δ       δ      y    f  x          1  2    superscript  δ  2     otherwise.     L_{\delta}(y,f(x))=\begin{cases}\frac{1}{2}(y-f(x))^{2}&\textrm{for }|y-f(x)|%
 \leq\delta,\\
 \delta\,|y-f(x)|-\frac{1}{2}\delta^{2}&\textrm{otherwise.}\end{cases}     Motivation  Two very commonly used loss functions are the squared loss ,     L   (  a  )    =   a  2         L  a    superscript  a  2     L(a)=a^{2}   , and the absolute loss ,     L   (  a  )    =   |  a  |         L  a     a     L(a)=|a|   . While the absolute loss is not differentiable at exactly one point,    a  =  0      a  0    a=0   , where it is subdifferentiable with its convex  subdifferential equal to the interval    [    -  1   +  1   ]     delimited-[]      1   1     [-1+1]   ; the absolute-value loss function results in a median-unbiased estimator, which can be evaluated for particular data sets by linear programming . The squared loss has the disadvantage that it has the tendency to be dominated by outliers—when summing over a set of   a   a   a   's (as in     ∑   i  =  1   n    L   (   a  i   )        superscript   subscript     i  1    n     L   subscript  a  i      \sum_{i=1}^{n}L(a_{i})   ), the sample mean is influenced too much by a few particularly large a-values when the distribution is heavy tailed: in terms of estimation theory , the asymptotic relative efficiency of the mean is poor for heavy-tailed distributions  As defined above, the Huber loss function is convex in a uniform neighborhood of its minimum    a  =  0      a  0    a=0   , at the boundary of this uniform neighborhood, the Huber loss function has a differentiable extension to an affine function at points    a  =   -  δ       a    δ     a=-\delta   and    a  =  δ      a  δ    a=\delta   . These properties allow it to combine much of the sensitivity of the mean-unbiased, minimum-variance estimator of the mean (using the quadratic loss function) and the robustness of the median-unbiased estimor (using the absolute value function).  Pseudo-Huber loss function  The Pseudo-Huber loss function can be used as a smooth approximation of the Huber loss function, and ensures that derivatives are continuous for all degrees. It is defined as 3 4         L  δ    (  a  )    =    δ  2    (     1  +    (   a  /  δ   )   2     -  1   )     .         subscript  L  δ   a      superscript  δ  2         1   superscript    a  δ   2     1      L_{\delta}(a)=\delta^{2}(\sqrt{1+(a/\delta)^{2}}-1).     As such, this function approximates     a  2   /  2       superscript  a  2   2    a^{2}/2   for small values of   a   a   a   , and approximates a straight line with slope   δ   δ   \delta   for large values of   a   a   a   .  While the above is the most common form, other smooth approximations of the Huber loss function also exist. 5  Variant for classification  For classification purposes, a variant of the Huber loss called modified Huber is sometimes used. Given a prediction    f   (  x  )       f  x    f(x)   (a real-valued classifier score) and a true binary class label    y  ∈   {   +  1   ,   -  1   }       y     1     1      y\in\{+1,-1\}   , the modified Huber loss is defined as 6       L   (  y  ,   f   (  x  )    )    =   {      max    (  0  ,  1  -   y   f   (  x  )   )   2          for    y   f   (  x  )    ≥   -  1    ,        -   4   y   f   (  x  )       otherwise.            L   y    f  x      cases   fragments    superscript   fragments  normal-(  0  normal-,  1   y  f   fragments  normal-(  x  normal-)   normal-)   2        for  y  f  x     1        4  y  f  x    otherwise.     L(y,f(x))=\begin{cases}\max(0,1-y\,f(x))^{2}&\textrm{for }\,\,y\,f(x)\geq-1,\\
 -4y\,f(x)&\textrm{otherwise.}\end{cases}     The term    max   (  0  ,   1  -    y   f   (  x  )     )       0    1    y  f  x      \max(0,1-y\,f(x))   is the hinge loss used by support vector machines ; the quadratically smoothed hinge loss is a generalization of   L   L   L   . 7  Applications  The Huber loss function is used in robust statistics , M-estimation and additive modelling . 8  See also   Robust regression  M-estimator  Visual comparison of different M-estimators   References  "  Category:Robust statistics  Category:M-estimators  Category:Loss functions     ↩  Compared to Hastie et al. , the loss is scaled by a factor of ½, to be consistent with Huber's original definition given earlier. ↩  ↩  ↩  ↩  ↩   ↩     