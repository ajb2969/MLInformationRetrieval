   Uniformly most powerful test      Uniformly most powerful test   In statistical hypothesis testing , a uniformly most powerful ( UMP ) test is a hypothesis test which has the greatest power 1 − β among all possible tests of a given size  α . For example, according to the Neyman–Pearson lemma , the likelihood-ratio test is UMP for testing simple (point) hypotheses.  Setting  Let   X   X   X   denote a random vector (corresponding to the measurements), taken from a parametrized family of probability density functions or probability mass functions      f  θ    (  x  )        subscript  f  θ   x    f_{\theta}(x)   , which depends on the unknown deterministic parameter    θ  ∈  Θ      θ  normal-Θ    \theta\in\Theta   . The parameter space   Θ   normal-Θ   \Theta   is partitioned into two disjoint sets    Θ  0     subscript  normal-Θ  0    \Theta_{0}   and    Θ  1     subscript  normal-Θ  1    \Theta_{1}   . Let    H  0     subscript  H  0    H_{0}   denote the hypothesis that    θ  ∈   Θ  0       θ   subscript  normal-Θ  0     \theta\in\Theta_{0}   , and let    H  1     subscript  H  1    H_{1}   denote the hypothesis that    θ  ∈   Θ  1       θ   subscript  normal-Θ  1     \theta\in\Theta_{1}   . The binary test of hypotheses is performed using a test function    ϕ   (  x  )       ϕ  x    \phi(x)   .       ϕ   (  x  )    =   {     1      if  x   ∈  R       0      if  x   ∈  A             ϕ  x    cases  1      if  x   R   0      if  x   A      \phi(x)=\begin{cases}1&\text{if }x\in R\\
 0&\text{if }x\in A\end{cases}   meaning that    H  1     subscript  H  1    H_{1}   is in force if the measurement    X  ∈  R      X  R    X\in R   and that    H  0     subscript  H  0    H_{0}   is in force if the measurement    X  ∈  A      X  A    X\in A   . Note that    A  ∪  R      A  R    A\cup R   is a disjoint covering of the measurement space.  Formal definition  A test function    ϕ   (  x  )       ϕ  x    \phi(x)   is UMP of size   α   α   \alpha   if for any other test function     ϕ  ′    (  x  )        superscript  ϕ  normal-′   x    \phi^{\prime}(x)   satisfying         sup   θ  ∈   Θ  0         E  θ    ϕ  ′     (  X  )     =   α  ′   ≤  α  =     sup   θ  ∈   Θ  0         E  θ   ϕ    (  X  )             subscript  supremum    θ   subscript  normal-Θ  0         subscript  normal-E  θ    superscript  ϕ  normal-′    X     superscript  α  normal-′        α         subscript  supremum    θ   subscript  normal-Θ  0         subscript  normal-E  θ   ϕ   X       \sup_{\theta\in\Theta_{0}}\;\operatorname{E}_{\theta}\phi^{\prime}(X)=\alpha^{%
 \prime}\leq\alpha=\sup_{\theta\in\Theta_{0}}\;\operatorname{E}_{\theta}\phi(X)\,   we have           E  θ    ϕ  ′     (  X  )    =   1  -   β  ′    ≤   1  -  β   =     E  θ   ϕ    (  X  )       ∀  θ   ∈   Θ  1     .     formulae-sequence          subscript  normal-E  θ    superscript  ϕ  normal-′    X     1   superscript  β  normal-′           1  β            subscript  normal-E  θ   ϕ   X        for-all  θ    subscript  normal-Θ  1      \operatorname{E}_{\theta}\phi^{\prime}(X)=1-\beta^{\prime}\leq 1-\beta=%
 \operatorname{E}_{\theta}\phi(X)\quad\forall\theta\in\Theta_{1}.     The Karlin-Rubin theorem  The Karlin-Rubin theorem 1 can be regarded as an extension of the Neyman-Pearson lemma for composite hypotheses. Consider a scalar measurement having a probability density function parameterized by a scalar parameter θ , and define the likelihood ratio     l   (  x  )    =      f   θ  1     (  x  )    /   f   θ  0      (  x  )          l  x          subscript  f   subscript  θ  1    x    subscript  f   subscript  θ  0     x     l(x)=f_{\theta_{1}}(x)/f_{\theta_{0}}(x)   . If    l   (  x  )       l  x    l(x)   is monotone non-decreasing, in   x   x   x   , for any pair     θ  1   ≥   θ  0        subscript  θ  1    subscript  θ  0     \theta_{1}\geq\theta_{0}   (meaning that the greater   x   x   x   is, the more likely    H  1     subscript  H  1    H_{1}   is), then the threshold test:       ϕ   (  x  )    =   {     1      if  x   >   x  0        0      if  x   <   x  0              ϕ  x    cases  1      if  x    subscript  x  0    0      if  x    subscript  x  0       \phi(x)=\begin{cases}1&\text{if }x>x_{0}\\
 0&\text{if }x      where    x  0     subscript  x  0    x_{0}   is chosen such that       E   θ  0    ϕ    (  X  )    =  α          subscript  normal-E   subscript  θ  0    ϕ   X   α    \operatorname{E}_{\theta_{0}}\phi(X)=\alpha      is the UMP test of size α for testing      H  0   :   θ  ≤    θ  0   vs.   H  1     :   θ  >   θ  0     .       normal-:   subscript  H  0     θ     subscript  θ  0   vs.   subscript  H  1       normal-:      θ   subscript  θ  0       H_{0}:\theta\leq\theta_{0}\text{ vs. }H_{1}:\theta>\theta_{0}.     Note that exactly the same test is also UMP for testing      H  0   :   θ  =    θ  0   vs.   H  1     :   θ  >   θ  0     .       normal-:   subscript  H  0     θ     subscript  θ  0   vs.   subscript  H  1       normal-:      θ   subscript  θ  0       H_{0}:\theta=\theta_{0}\text{ vs. }H_{1}:\theta>\theta_{0}.     Important case: The exponential family  Although the Karlin-Rubin theorem may seem weak because of its restriction to scalar parameter and scalar measurement, it turns out that there exist a host of problems for which the theorem holds. In particular, the one-dimensional exponential family of probability density functions or probability mass functions with        f  θ    (  x  )    =   c   (  θ  )   h   (  x  )    exp   (   π   (  θ  )   T   (  x  )    )            subscript  f  θ   x     c  θ  h  x      π  θ  T  x       f_{\theta}(x)=c(\theta)h(x)\exp(\pi(\theta)T(x))   has a monotone non-decreasing likelihood ratio in the sufficient statistic  T ( x ), provided that    π   (  θ  )       π  θ    \pi(\theta)   is non-decreasing.  Example  Let    X  =   (   X  0   ,   X  1   ,  …  ,   X   M  -  1    )       X    subscript  X  0    subscript  X  1   normal-…   subscript  X    M  1       X=(X_{0},X_{1},\dots,X_{M-1})   denote i.i.d. normally distributed   N   N   N   -dimensional random vectors with mean    θ  m      θ  m    \theta m   and covariance matrix   R   R   R   . We then have        f  θ    (  X  )    =     (   2  π   )    -    M  N   /  2       |  R  |    -   M  /  2      exp   {   -    1  2     ∑   n  =  0    M  -  1       (    X  n   -   θ  m    )   T    R   -  1     (    X  n   -   θ  m    )       }     =            subscript  f  θ   X      superscript    2  π         M  N   2      superscript    R       M  2             1  2     superscript   subscript     n  0      M  1       superscript     subscript  X  n     θ  m    T    superscript  R    1       subscript  X  n     θ  m               absent     f_{\theta}(X)=(2\pi)^{-MN/2}|R|^{-M/2}\exp\left\{-\frac{1}{2}\sum_{n=0}^{M-1}(%
 X_{n}-\theta m)^{T}R^{-1}(X_{n}-\theta m)\right\}=          =     (   2  π   )    -    M  N   /  2       |  R  |    -   M  /  2      exp   {   -    1  2     ∑   n  =  0    M  -  1     (    θ  2    m  T    R   -  1    m   )      }         absent     superscript    2  π         M  N   2      superscript    R       M  2             1  2     superscript   subscript     n  0      M  1       superscript  θ  2    superscript  m  T    superscript  R    1    m          =(2\pi)^{-MN/2}|R|^{-M/2}\exp\left\{-\frac{1}{2}\sum_{n=0}^{M-1}(\theta^{2}m^{%
 T}R^{-1}m)\right\}        exp   {   -    1  2     ∑   n  =  0    M  -  1      X  n  T    R   -  1     X  n       }     exp   {   θ   m  T    R   -  1      ∑   n  =  0    M  -  1     X  n     }                1  2     superscript   subscript     n  0      M  1       superscript   subscript  X  n   T    superscript  R    1     subscript  X  n            θ   superscript  m  T    superscript  R    1      superscript   subscript     n  0      M  1     subscript  X  n        \exp\left\{-\frac{1}{2}\sum_{n=0}^{M-1}X_{n}^{T}R^{-1}X_{n}\right\}\exp\left\{%
 \theta m^{T}R^{-1}\sum_{n=0}^{M-1}X_{n}\right\}   which is exactly in the form of the exponential family shown in the previous section, with the sufficient statistic being        T   (  X  )    =    m  T    R   -  1      ∑   n  =  0    M  -  1     X  n      .        T  X      superscript  m  T    superscript  R    1      superscript   subscript     n  0      M  1     subscript  X  n       T(X)=m^{T}R^{-1}\sum_{n=0}^{M-1}X_{n}.     Thus, we conclude that the test       ϕ   (  T  )    =   {     1      if  T   >   t  0        0      if  T   <   t  0              ϕ  T    cases  1      if  T    subscript  t  0    0      if  T    subscript  t  0       \phi(T)=\begin{cases}1&\text{if }T>t_{0}\\
 0&\text{if }T            E   θ  0    ϕ    (  T  )    =  α          subscript  normal-E   subscript  θ  0    ϕ   T   α    \operatorname{E}_{\theta_{0}}\phi(T)=\alpha     is the UMP test of size   α   α   \alpha   for testing     H  0   :   θ  ≤   θ  0       normal-:   subscript  H  0     θ   subscript  θ  0      H_{0}:\theta\leq\theta_{0}   vs.     H  1   :   θ  >   θ  0       normal-:   subscript  H  1     θ   subscript  θ  0      H_{1}:\theta>\theta_{0}     Further discussion  Finally, we note that in general, UMP tests do not exist for vector parameters or for two-sided tests (a test in which one hypothesis lies on both sides of the alternative). Why is it so?  The reason is that in these situations, the most powerful test of a given size for one possible value of the parameter (e.g. for    θ  1     subscript  θ  1    \theta_{1}   where     θ  1   >   θ  0        subscript  θ  1    subscript  θ  0     \theta_{1}>\theta_{0}   ) is different from the most powerful test of the same size for a different value of the parameter (e.g. for    θ  2     subscript  θ  2    \theta_{2}   where     θ  2   <   θ  0        subscript  θ  2    subscript  θ  0     \theta_{2}<\theta_{0}   ). As a result, no test is uniformly most powerful.  References  Further reading   L. L. Scharf, Statistical Signal Processing , Addison-Wesley, 1991, section 4.7.   "  Category:Hypothesis testing  Category:Statistical terminology     Casella, G.; Berger, R.L. (2008), Statistical Inference , Brooks/Cole. ISBN 0-495-39187-5 (Theorem 8.3.17) ↩     