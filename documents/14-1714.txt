   Proto-value functions      Proto-value functions   In applied mathematics , proto-value functions (PVFs) are automatically learned basis functions that are useful in approximating task-specific value functions, providing a compact representation of the powers of transition matrices. They provide a novel framework for solving the credit assignment problem . The framework introduces a novel approach to solving Markov decision processes (MDP) and reinforcement learning problems, using multiscale spectral and manifold learning methods. Proto-value functions are generated by spectral analysis of a graph, using the graph Laplacian .  Proto-value functions were first introduced in the context of reinforcement learning by Sridhar Mahadevan in his paper, Proto-Value Functions: Developmental Reinforcement Learning at ICML 2005. 1  Motivation  Value function approximation is a critical component to solving MDPs defined over a continuous state space. A good function approximator allows an RL agent to accurately represent the value of any state it has experienced, without explicitly storing its value. Linear function approximation using basis functions is a common way of constructing a value function approximation, like Radial basis functions , polynomial state encodings, and CMACs . However, parameters associated with these basis functions often require significant domain-specific hand-engineering. 2 Proto-value functions attempts to solve this required hand-engineering by accounting for the underlying manifold structure of the problem domain. 3  Overview  Proto-value functions are task-independent global basis functions that collectively span the entire space of possible value functions for a given state space. 4 They incorporate geometric constraints intrinsic to the environment. For example, states close in Euclidean distance (such as states on opposite sides of a wall) may be far apart in manifold space. Previous approaches to this nonlinearity problem lacked a broad theoretical framework, and consequently have only been explored in the context of discrete MDPs .  Proto-value functions arise from reformulating the problem of value function approximation as real-valued function approximation on a graph or manifold. This results in broader applicability of the learned bases and enables a new class of learning algorithms, which learn representations and policies at the same time. 5  Basis functions from graph Laplacian  In this approach, we will construct the basis functions by spectral analysis of the graph Laplacian, a self-adjoint (or symmetric) operator on the space of functions on the graph, closely related to the random walk operator.  For the sake of simplicity, assume that the underlying state space can be represented as an undirected unweighted graph    G  =   (  V  ,  E  )       G   V  E     G=\left(V,E\right)   The combinatorial Laplacian    L   L   L   is defined as the operator    L  =   D  -  A       L    D  A     L=D-A   , where   D   D   D   is a diagonal matrix called the degree matrix and   A   A   A   is the adjacency matrix . 6  The spectral analysis of the Laplace operator on a graph consists of finding the eigenvalues and eigenfunctions which solve the equation       L   ϕ  λ    =   λ   ϕ  λ          L   subscript  ϕ  λ      λ   subscript  ϕ  λ      L\phi_{\lambda}=\lambda\phi_{\lambda}   , where   L   L   L   is the combinatorial Laplacian,    ϕ  λ     subscript  ϕ  λ    \phi_{\lambda}   is an eigenfunction associated with the eigenvalue   λ   λ   \lambda   . Here the term "eigenfunction" is used to denote what is traditionally referred to as eigenvector in linear algebra, because the Laplacian eigenvectors can naturally be viewed as functions that map each vertex to a real number. 7  The combinatorial Laplacian is not the only operator on graphs to select from. Other possible graph operators include:   Normalized Laplacian     L  normalized   =   I  -    D   -   1  /  2     A   D   -   1  /  2            subscript  L  normalized     I     superscript  D      1  2     A   superscript  D      1  2         L_{\text{normalized}}=I-D^{-1/2}AD^{-1/2}    8  Random Walk    P  =    D   -  1    A       P     superscript  D    1    A     P=D^{-1}A    9   Graph construction on discrete state space  For a finite state space the graph   G   G   G   mentioned above can be simply constructed by examining the connections between states. Let    S  i     subscript  S  i    S_{i}   and    S  j     subscript  S  j    S_{j}   be any two states. Then       G   i  ,  j    =   {     1      if   S  i    ↔   S  j        0    otherwise           subscript  G   i  j     cases  1   normal-↔    if   subscript  S  i     subscript  S  j    0  otherwise     G_{i,j}=\begin{cases}1&\text{if }S_{i}\leftrightarrow S_{j}\\
 0&\text{otherwise}\end{cases}   It is important to note that this can only be done when the state space is finite and of reasonable size.  Graph construction on continuous or large state space  For a continuous state space or simply a very large discrete state space, it is necessary to sample from the manifold in state space. Then constructing the Graph   G   G   G   based on the samples. There are a few issues to consider here: 10   How to sample the manifold  Random walk or guided exploration   How to determine if two sample should be connected   Application  Once the PVFs are generated, they can be plugged into a traditional function approximation framework. One such method is least-squares approximation.  Least-squares approximation using proto-value functions  Let     Φ  G   =   {   V  1  G   ,  …  ,   V  k  G   }        subscript  normal-Φ  G     superscript   subscript  V  1   G   normal-…   superscript   subscript  V  k   G      \Phi_{G}=\left\{V_{1}^{G},\dots,V_{k}^{G}\right\}   be the basis set of PVFs, where each    V  i  G     superscript   subscript  V  i   G    V_{i}^{G}   is the eigenfunction defined over all states in the graph   G   G   G   . Let     V  ^   π     superscript   normal-^  V   π    \hat{V}^{\pi}   be the target value function that is only known for a subset of states     S  M  G   =   {   s  1   ,  …  ,   s  m   }        superscript   subscript  S  M   G     subscript  s  1   normal-…   subscript  s  m      S_{M}^{G}=\left\{s_{1},\dots,s_{m}\right\}   .  Define the gram matrix        K  G   =     (   Φ  m  G   )   T    Φ  m  G     .       subscript  K  G      superscript   superscript   subscript  normal-Φ  m   G   T    superscript   subscript  normal-Φ  m   G      K_{G}=\left(\Phi_{m}^{G}\right)^{T}\Phi_{m}^{G}.     here    S  m  G     superscript   subscript  S  m   G    S_{m}^{G}   is the component wise projection of the PVFs onto the states in    S  G  m     superscript   subscript  S  G   m    S_{G}^{m}   . Hence, each entry of the gram matrix is        K  G    (  i  ,  j  )    =    ∑  k     V  i  G    (  k  )    V  j  G    (  k  )            subscript  K  G    i  j      subscript   k      superscript   subscript  V  i   G   k   superscript   subscript  V  j   G   k      K_{G}\left(i,j\right)=\sum_{k}V_{i}^{G}(k)V_{j}^{G}(k)     Now the we can solve for the coefficients that minimize the least squares error with the equation       α  =    K  G   -  1      (   Φ  M  G   )   T     V  ^   π     .      α     superscript   subscript  K  G     1     superscript   superscript   subscript  normal-Φ  M   G   T    superscript   normal-^  V   π      \alpha=K_{G}^{-1}\left(\Phi_{M}^{G}\right)^{T}\hat{V}^{\pi}.     A nonlinear least-squares approach is possible by using the k PVFs with the largest absolute coefficients to compute the approximation. 11  See also   Reinforcement learning  Markov decision process  Basis function  Eigenfunction  Laplacian matrix   References  "  Category:Spectral theory  Category:Types of functions     Mahadevan, S. Proto-Value Functions: Developmental Reinforcement Learning . Proceedings of the International Conference on Machine Learning ICML 2005 ↩  Johns, J. and Mahadevan, S., Constructing Basis Functions from Directed Graphs for Value Function Approximation , International Conference on Machine Learning (ICML), 2007 ↩    Mahadevan, S. and Maggiono, M., Proto-Value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes , University of Massachusetts, Department of Computer Science Technical Report TR-2006-35, 2006 ↩    Mahadevan, S. and Maggiono, M., ICML 2006 tutorial . ↩        