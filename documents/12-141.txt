   Mean and predicted response      Mean and predicted response   In linear regression  mean response and predicted response are values of the dependent variable calculated from the regression parameters and a given value of the independent variable. The values of these two responses are the same, but their calculated variances are different.  Straight line regression  In straight line fitting, the model is       y  i   =   Î±  +   Î²   x  i    +    Ïµ  i          subscript  y  i     Î±    Î²   subscript  x  i     subscript  Ïµ  i      y_{i}=\alpha+\beta x_{i}+\epsilon_{i}\,   where    y  i     subscript  y  i    y_{i}   is the response variable ,    x  i     subscript  x  i    x_{i}   is the explanatory variable , Îµ i is the random error, and   Î±   Î±   \alpha   and   Î²   Î²   \beta   are parameters. The predicted response value for a given explanatory value, x d , is given by         y  ^   d   =    Î±  ^   +    Î²  ^    x  d      ,       subscript   normal-^  y   d      normal-^  Î±      normal-^  Î²    subscript  x  d       \hat{y}_{d}=\hat{\alpha}+\hat{\beta}x_{d},   while the actual response would be       y  d   =   Î±  +   Î²   x  d    +    Ïµ  d          subscript  y  d     Î±    Î²   subscript  x  d     subscript  Ïµ  d      y_{d}=\alpha+\beta x_{d}+\epsilon_{d}\,     Expressions for the values and variances of    Î±  ^     normal-^  Î±    \hat{\alpha}   and    Î²  ^     normal-^  Î²    \hat{\beta}   are given in linear regression .  Mean response is an estimate of the mean of the y population associated with x d , that is    E   (  y  |   x  d   )   =     y  ^   d       fragments  E   fragments  normal-(  y  normal-|   subscript  x  d   normal-)     subscript   normal-^  y   d     E(y|x_{d})=\hat{y}_{d}\!   . The variance of the mean response is given by        Var   (    Î±  ^   +    Î²  ^    x  d     )    =    Var   (   Î±  ^   )    +    (   Var   Î²  ^    )    x  d  2    +   2   x  d   Cov   (   Î±  ^   ,   Î²  ^   )      .        Var     normal-^  Î±      normal-^  Î²    subscript  x  d          Var   normal-^  Î±        Var   normal-^  Î²     superscript   subscript  x  d   2      2   subscript  x  d   Cov    normal-^  Î±    normal-^  Î²        \text{Var}\left(\hat{\alpha}+\hat{\beta}x_{d}\right)=\text{Var}\left(\hat{%
 \alpha}\right)+\left(\text{Var}\hat{\beta}\right)x_{d}^{2}+2x_{d}\text{Cov}%
 \left(\hat{\alpha},\hat{\beta}\right).   This expression can be simplified to        Var   (    Î±  ^   +    Î²  ^    x  d     )    =    Ïƒ  2    (    1  m   +     (    x  d   -   x  Â¯    )   2    âˆ‘    (    x  i   -   x  Â¯    )   2      )     .        Var     normal-^  Î±      normal-^  Î²    subscript  x  d         superscript  Ïƒ  2       1  m      superscript     subscript  x  d    normal-Â¯  x    2      superscript     subscript  x  i    normal-Â¯  x    2         \text{Var}\left(\hat{\alpha}+\hat{\beta}x_{d}\right)=\sigma^{2}\left(\frac{1}{%
 m}+\frac{\left(x_{d}-\bar{x}\right)^{2}}{\sum(x_{i}-\bar{x})^{2}}\right).     To demonstrate this simplification, one can make use of the identity        âˆ‘    (    x  i   -   x  Â¯    )   2    =    âˆ‘   x  i  2    -    1  m     (   âˆ‘   x  i    )   2      .         superscript     subscript  x  i    normal-Â¯  x    2         superscript   subscript  x  i   2        1  m    superscript     subscript  x  i    2       \sum(x_{i}-\bar{x})^{2}=\sum x_{i}^{2}-\frac{1}{m}\left(\sum x_{i}\right)^{2}.     The predicted response distribution is the predicted distribution of the residuals at the given point x d . So the variance is given by        Var   (    y  d   -   [    Î±  ^   +    Î²  ^    x  d     ]    )    =    Var   (   y  d   )    +   Var   (    Î±  ^   +    Î²  ^    x  d     )      .        Var     subscript  y  d    delimited-[]     normal-^  Î±      normal-^  Î²    subscript  x  d            Var   subscript  y  d      Var     normal-^  Î±      normal-^  Î²    subscript  x  d         \text{Var}\left(y_{d}-\left[\hat{\alpha}+\hat{\beta}x_{d}\right]\right)=\text{%
 Var}\left(y_{d}\right)+\text{Var}\left(\hat{\alpha}+\hat{\beta}x_{d}\right).     The second part of this expression was already calculated for the mean response. Since     Var   (   y  d   )    =   Ïƒ  2         Var   subscript  y  d     superscript  Ïƒ  2     \text{Var}\left(y_{d}\right)=\sigma^{2}   (a fixed but unknown parameter that can be estimated), the variance of the predicted response is given by        Var   (    y  d   -   [    Î±  ^   +    Î²  ^    x  d     ]    )    =    Ïƒ  2   +    Ïƒ  2    (    1  m   +     (    x  d   -   x  Â¯    )   2    âˆ‘    (    x  i   -   x  Â¯    )   2      )     =    Ïƒ  2    (   1  +   1  m   +     (    x  d   -   x  Â¯    )   2    âˆ‘    (    x  i   -   x  Â¯    )   2      )     .          Var     subscript  y  d    delimited-[]     normal-^  Î±      normal-^  Î²    subscript  x  d           superscript  Ïƒ  2      superscript  Ïƒ  2       1  m      superscript     subscript  x  d    normal-Â¯  x    2      superscript     subscript  x  i    normal-Â¯  x    2                superscript  Ïƒ  2     1    1  m      superscript     subscript  x  d    normal-Â¯  x    2      superscript     subscript  x  i    normal-Â¯  x    2          \text{Var}\left(y_{d}-\left[\hat{\alpha}+\hat{\beta}x_{d}\right]\right)=\sigma%
 ^{2}+\sigma^{2}\left(\frac{1}{m}+\frac{\left(x_{d}-\bar{x}\right)^{2}}{\sum(x_%
 {i}-\bar{x})^{2}}\right)=\sigma^{2}\left(1+\frac{1}{m}+\frac{\left(x_{d}-\bar{%
 x}\right)^{2}}{\sum(x_{i}-\bar{x})^{2}}\right).     Confidence intervals  The    100    (   1  -  Î±   )   %       100   percent    1  Î±      100(1-\alpha)\%    confidence intervals are computed as     y  d   Â±    t    Î±  2   ,   m  -  n  -  1      Var       plus-or-minus   subscript  y  d      subscript  t     Î±  2     m  n  1       Var      y_{d}\pm t_{\frac{\alpha}{2},m-n-1}\sqrt{\text{Var}}   . Thus, the confidence interval for predicted response is wider than the interval for mean response. This is expected intuitively â€“ the variance of the population of   y   y   y   values does not shrink when one samples from it, because the random variable Îµ i does not decrease, but the variance of the mean of the   y   y   y   does shrink with increased sampling, because the variance in    Î±  ^     normal-^  Î±    \hat{\alpha}   and    Î²  ^     normal-^  Î²    \hat{\beta}   decrease, so the mean response (predicted response value) becomes closer to    Î±  +   Î²   x  d        Î±    Î²   subscript  x  d      \alpha+\beta x_{d}   .  This is analogous to the difference between the variance of a population and the variance of the sample mean of a population: the variance of a population is a parameter and does not change, but the variance of the sample mean decreases with increased samples.  General linear regression  The general linear model can be written as       y  i   =     âˆ‘   j  =  1    j  =  n      X   i  j     Î²  j     +    Ïµ  i          subscript  y  i       superscript   subscript     j  1      j  n       subscript  X    i  j     subscript  Î²  j      subscript  Ïµ  i      y_{i}=\sum_{j=1}^{j=n}X_{ij}\beta_{j}+\epsilon_{i}\,     Therefore since     y  d   =    âˆ‘   j  =  1    j  =  n      X   d  j      Î²  ^   j          subscript  y  d     superscript   subscript     j  1      j  n       subscript  X    d  j     subscript   normal-^  Î²   j       y_{d}=\sum_{j=1}^{j=n}X_{dj}\hat{\beta}_{j}   the general expression for the variance of the mean response is        Var   (    âˆ‘   j  =  1    j  =  n      X   d  j      Î²  ^   j     )    =    âˆ‘   i  =  1    i  =  n      âˆ‘   j  =  1    j  =  n      X   d  i     M   i  j     X   d  j        ,        Var    superscript   subscript     j  1      j  n       subscript  X    d  j     subscript   normal-^  Î²   j        superscript   subscript     i  1      i  n      superscript   subscript     j  1      j  n       subscript  X    d  i     subscript  M    i  j     subscript  X    d  j         \text{Var}\left(\sum_{j=1}^{j=n}X_{dj}\hat{\beta}_{j}\right)=\sum_{i=1}^{i=n}%
 \sum_{j=1}^{j=n}X_{di}M_{ij}X_{dj},   where M is the covariance matrix of the parameters, given by      ğŒ  =    Ïƒ  2     (    ğ—  ğ“   ğ—   )    -  1         ğŒ     superscript  Ïƒ  2    superscript     superscript  ğ—  ğ“   ğ—     1       \mathbf{M}=\sigma^{2}\left(\mathbf{X^{T}X}\right)^{-1}   .  See also   Prediction interval   References     "  Category:Regression analysis  Category:Estimation theory   