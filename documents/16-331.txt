   T-distributed stochastic neighbor embedding      T-distributed stochastic neighbor embedding   t-distributed stochastic neighbor embedding (t-SNE) is a machine learning algorithm for dimensionality reduction developed by Laurens van der Maaten and Geoffrey Hinton . 1 It is a nonlinear dimensionality reduction technique that is particularly well suited for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in a scatter plot. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points.  The t-SNE algorithms comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an infinitesimal probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullbackâ€“Leibler divergence between the two distributions with respect to the locations of the points in the map. Note that whilst the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate.  t-SNE has been used in a wide range of applications, including computer security research, 2  music analysis , 3  cancer research , 4 and bioinformatics . 5  Details  Given a set of   N   N   N   high-dimensional objects     ğ±  1   ,  â€¦  ,   ğ±  N       subscript  ğ±  1   normal-â€¦   subscript  ğ±  N     \mathbf{x}_{1},\dots,\mathbf{x}_{N}   , t-SNE first computes probabilities    p   i  j      subscript  p    i  j     p_{ij}   that are proportional to the similarity of objects    ğ±  i     subscript  ğ±  i    \mathbf{x}_{i}   and    ğ±  j     subscript  ğ±  j    \mathbf{x}_{j}   , as follows:        p   j  |  i    =    exp   (   -      âˆ¥    ğ±  i   -   ğ±  j    âˆ¥   2   /  2    Ïƒ  i  2     )       âˆ‘   k  â‰   i      exp   (   -      âˆ¥    ğ±  i   -   ğ±  k    âˆ¥   2   /  2    Ïƒ  i  2     )       ,       subscript  p   fragments  j  normal-|  i               superscript   norm     subscript  ğ±  i    subscript  ğ±  j     2   2    superscript   subscript  Ïƒ  i   2        subscript     k  i             superscript   norm     subscript  ğ±  i    subscript  ğ±  k     2   2    superscript   subscript  Ïƒ  i   2          p_{j|i}=\frac{\exp(-\lVert\mathbf{x}_{i}-\mathbf{x}_{j}\rVert^{2}/2\sigma_{i}^%
 {2})}{\sum_{k\neq i}\exp(-\lVert\mathbf{x}_{i}-\mathbf{x}_{k}\rVert^{2}/2%
 \sigma_{i}^{2})},        p   i  j    =     p   j  |  i    +   p   i  |  j      2  N         subscript  p    i  j         subscript  p   fragments  j  normal-|  i     subscript  p   fragments  i  normal-|  j       2  N      p_{ij}=\frac{p_{j|i}+p_{i|j}}{2N}     The bandwidth of the Gaussian kernels     Ïƒ  i     subscript  Ïƒ  i    \sigma_{i}   , is set in such a way that the perplexity of the conditional distribution equals a predefined perplexity using a binary search . As a result, the bandwidth is adapted to the density of the data: smaller values of    Ïƒ  i     subscript  Ïƒ  i    \sigma_{i}   are used in denser parts of the data space.  t-SNE aims to learn a   d   d   d   -dimensional map     ğ²  1   ,  â€¦  ,   ğ²  N       subscript  ğ²  1   normal-â€¦   subscript  ğ²  N     \mathbf{y}_{1},\dots,\mathbf{y}_{N}   (with     ğ²  i   âˆˆ   â„  d        subscript  ğ²  i    superscript  â„  d     \mathbf{y}_{i}\in\mathbb{R}^{d}   ) that reflects the similarities    p   i  j      subscript  p    i  j     p_{ij}   as well as possible. To this end, it measures similarities    q   i  j      subscript  q    i  j     q_{ij}   between two points in the map    ğ²  i     subscript  ğ²  i    \mathbf{y}_{i}   and    ğ²  j     subscript  ğ²  j    \mathbf{y}_{j}   , using a very similar approach. Specifically,    q   i  j      subscript  q    i  j     q_{ij}   is defined as:       q   i  j    =     (   1  +    âˆ¥    ğ²  i   -   ğ²  j    âˆ¥   2    )    -  1       âˆ‘   k  â‰   l       (   1  +    âˆ¥    ğ²  k   -   ğ²  l    âˆ¥   2    )    -  1           subscript  q    i  j       superscript    1   superscript   norm     subscript  ğ²  i    subscript  ğ²  j     2      1      subscript     k  l     superscript    1   superscript   norm     subscript  ğ²  k    subscript  ğ²  l     2      1        q_{ij}=\frac{(1+\lVert\mathbf{y}_{i}-\mathbf{y}_{j}\rVert^{2})^{-1}}{\sum_{k%
 \neq l}(1+\lVert\mathbf{y}_{k}-\mathbf{y}_{l}\rVert^{2})^{-1}}     Herein a heavy-tailed Student-t distribution is used to measure similarities between low-dimensional points in order to allow dissimilar objects to be modeled far apart in the map CITATION.  The locations of the points    ğ²  i     subscript  ğ²  i    \mathbf{y}_{i}   in the map are determined by minimizing the (non-symmetric) Kullbackâ€“Leibler divergence of the distribution   Q   Q   Q   from the distribution   P   P   P   , that is:      K  L   (  P  |  |  Q  )   =   âˆ‘   i  â‰   j      p   i  j     log    p   i  j     q   i  j        fragments  K  L   fragments  normal-(  P  normal-|  normal-|  Q  normal-)     subscript     i  j     subscript  p    i  j        subscript  p    i  j     subscript  q    i  j       KL(P||Q)=\sum_{i\neq j}p_{ij}\,\log\frac{p_{ij}}{q_{ij}}     The minimization of the Kullbackâ€“Leibler divergence with respect to the points    ğ²  i     subscript  ğ²  i    \mathbf{y}_{i}   is performed using gradient descent . The result of this optimization is a map that reflects the similarities between the high-dimensional inputs well.  References  Software   t-Distributed Stochastic Neighbor Embedding http://homepage.tudelft.nl/19j49/t-SNE.html   "  Category:Machine learning algorithms     â†©  â†©  â†©  â†©  â†©     