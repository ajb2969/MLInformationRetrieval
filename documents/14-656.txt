   AIXI      AIXI   AIXI is a mathematical formalism for artificial general intelligence . It combines Solomonoff induction with sequential decision theory . AIXI was first proposed by Marcus Hutter in 2000 1 and the results below are proved in Hutter's 2005 book Universal Artificial Intelligence . 2  AIXI is a reinforcement learning agent ; it maximizes the expected total rewards received from the environment. Intuitively, it simultaneously considers every computable hypothesis. In each time step, it looks at every possible program and evaluates how many rewards that program generates depending on the next action taken. The promised rewards are then weighted by the subjective belief that this program constitutes the true environment. This belief is computed from the length of the program: longer programs are considered less likely, in line with Occam's razor . AIXI then selects the action that has the highest expected total reward in the weighted sum of all these programs.  Definition  The AIXI agent interacts sequentially with some (stochastic and unknown to AIXI) environment   μ   μ   \mu   . In step t , the agent outputs an action    a  t     subscript  a  t    a_{t}   and the environment responds with an observation    o  t     subscript  o  t    o_{t}   and a reward    r  t     subscript  r  t    r_{t}   distributed according to the conditional probability    μ   (   o  t    r  t   |   a  1    o  1    r  1   …   a   t  -  1     o   t  -  1     r   t  -  1     a  t   )      fragments  μ   fragments  normal-(   subscript  o  t    subscript  r  t   normal-|   subscript  a  1    subscript  o  1    subscript  r  1   normal-…   subscript  a    t  1     subscript  o    t  1     subscript  r    t  1     subscript  a  t   normal-)     \mu(o_{t}r_{t}|a_{1}o_{1}r_{1}...a_{t-1}o_{t-1}r_{t-1}a_{t})   . Then this cycle repeats for t + 1 . The agent tries to maximize cumulative future reward     r  t   +  …  +   r  m        subscript  r  t   normal-…   subscript  r  m     r_{t}+\ldots+r_{m}   for a fixed lifetime m .  Given a current time t and history     a  1    o  1    r  1   …   a   t  -  1     o   t  -  1     r   t  -  1         subscript  a  1    subscript  o  1    subscript  r  1   normal-…   subscript  a    t  1     subscript  o    t  1     subscript  r    t  1      a_{1}o_{1}r_{1}...a_{t-1}o_{t-1}r_{t-1}   , the action AIXI outputs is defined as 3        arg   max   a  t       ∑    o  t    r  t      …   max   a  m      ∑    o  m    r  m       [    r  t   +  …  +   r  m    ]     ∑   q  :    U   (  q  ,    a  1   …   a  m    )    =    o  1    r  1   …   o  m    r  m        2   -   length   (  q  )            ,         subscript    subscript  a  t       subscript      subscript  o  t    subscript  r  t       normal-…   subscript    subscript  a  m      subscript      subscript  o  m    subscript  r  m        delimited-[]     subscript  r  t   normal-…   subscript  r  m       subscript    normal-:  q      U   q     subscript  a  1   normal-…   subscript  a  m         subscript  o  1    subscript  r  1   normal-…   subscript  o  m    subscript  r  m        superscript  2      length  q            \arg\max_{a_{t}}\sum_{o_{t}r_{t}}\ldots\max_{a_{m}}\sum_{o_{m}r_{m}}[r_{t}+%
 \ldots+r_{m}]\sum_{q:\;U(q,a_{1}\ldots a_{m})=o_{1}r_{1}\ldots o_{m}r_{m}}2^{-%
 \textrm{length}(q)},     where U denotes a monotone  universal Turing machine , and q ranges over all programs on the universal machine U .  The parameters to AIXI are the universal Turing machine and the agent's lifetime m . The latter dependence can be removed by the use of discounting.  Optimality  AIXI's performance is measured by the expected total number of rewards it receives. AIXI has been proven to be optimal in the following ways. 4   Pareto optimality : there is no other agent that performs at least as well as AIXI in all environments while performing strictly better in at least one environment.  Balanced Pareto optimality: Like Pareto optimality, but considering a weighted sum of environments.  Self-optimizing: a policy p is called self-optimizing for an environment   μ   μ   \mu   if the performance of p approaches the theoretical maximum for   μ   μ   \mu   when the length of the agent's lifetime (not time) goes to infinity. For environment classes where self-optimizing policies exist, AIXI is self-optimizing.   However, AIXI does have limitations. It is restricted to maximizing rewards based off of percepts as opposed to external states. It also assumes it interacts with the environment solely through action and percept channels, preventing it from considering the possibility of being damaged or modified. It also assumes the environment is computable. 5 Since AIXI is incomputable, it assigns zero probability to its own existence.  Computational aspects  Like Solomonoff induction , AIXI is incomputable. However, there are computable approximations of it. One such approximation is AIXI tl , which performs as least as well as the provably best time t and space l limited agent. 6 Another approximation to AIXI with a restricted environment class is MC-AIXI(FAC-CTW), which has had some success playing simple games such as partially observable  Pac-Man . 7 8  References  "  Category:Decision theory  Category:Statistical inference  Category:Machine learning     ↩  ↩  http://hutter1.net/ai/uaibook.htm ↩   ↩   ↩  http://www.youtube.com/watch?v=yfsMHtmGDKE ↩     