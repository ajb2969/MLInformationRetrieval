   Correction for attenuation      Correction for attenuation   Correction for attenuation is a statistical procedure, due to Spearman (1904), to "rid a correlation coefficient from the weakening effect of measurement error" (Jensen, 1998), a phenomenon also known as regression dilution . In measurement and statistics , it is also called disattenuation . The correlation between two sets of parameters or measurements is estimated in a manner that accounts for measurement error contained within the estimates of those parameters.  Background  Correlations between parameters are diluted or weakened by measurement error. Disattenuation provides for a more accurate estimate of the correlation between the parameters by accounting for this effect.  Definition  The disattenuated estimate of the correlation between two sets of parameters or measures is therefore       ρ  =    corr   (   β  ^   ,   θ  ^   )       R  β    R  θ       .      ρ      corr    normal-^  β    normal-^  θ          subscript  R  β    subscript  R  θ        \rho=\frac{\mbox{corr}(\hat{\beta},\hat{\theta})}{\sqrt{R_{\beta}R_{\theta}}}.     That is, the disattenuated correlation is obtained by dividing the correlation between the estimates by the geometric mean of the separation indices of the two sets of estimates. Expressed in terms of Classical test theory, the correlation is divided by the geometric mean of the reliability coefficients of two tests.  Given two random variables    X   X   X   and   Y   Y   Y   , with correlation     r   x  y      subscript  r    x  y     r_{xy}   , and a known reliability for each variable,    r   x  x      subscript  r    x  x     r_{xx}   and    r   y  y      subscript  r    y  y     r_{yy}   , the correlation between   X   X   X   and   Y   Y   Y   corrected for attenuation is     r    x  ′    y  ′     =    r   x  y       r   x  x     r   y  y            subscript  r     superscript  x  normal-′    superscript  y  normal-′        subscript  r    x  y         subscript  r    x  x     subscript  r    y  y         r_{x^{\prime}y^{\prime}}=\frac{r_{xy}}{\sqrt{r_{xx}r_{yy}}}   .  How well the variables are measured affects the correlation of X and Y . The correction for attenuation tells you what the correlation would be if you could measure X and Y with perfect reliability.  If   X   X   X   and   Y   Y   Y   are taken to be imperfect measurements of underlying variables    X  ′     superscript  X  normal-′    X^{\prime}   and    Y  ′     superscript  Y  normal-′    Y^{\prime}   with independent errors, then    r    x  ′    y  ′       subscript  r     superscript  x  normal-′    superscript  y  normal-′      r_{x^{\prime}y^{\prime}}   measures the true correlation between    X  ′     superscript  X  normal-′    X^{\prime}   and    Y  ′     superscript  Y  normal-′    Y^{\prime}   .  Derivation of the formula  Let   β   β   \beta   and   θ   θ   \theta   be the true values of two attributes of some person or statistical unit . These values are regarded as random variables by virtue of the statistical unit being selected randomly from some population . Let    β  ^     normal-^  β    \hat{\beta}   and    θ  ^     normal-^  θ    \hat{\theta}   be estimates of   β   β   \beta   and   θ   θ   \theta   derived either directly by observation-with-error or from application of a measurement model, such as the Rasch model . Also, let         \hat{\beta} = \beta + \epsilon_{\beta} , \quad\quad \hat{\theta} = \theta + \epsilon_\theta,  where    ϵ  β     subscript  ϵ  β    \epsilon_{\beta}   and    ϵ  θ     subscript  ϵ  θ    \epsilon_{\theta}   are the measurement errors associated with the estimates    β  ^     normal-^  β    \hat{\beta}   and    θ  ^     normal-^  θ    \hat{\theta}   .  The correlation between two sets of estimates is       corr   (   β  ^   ,   θ  ^   )    =    cov   (   β  ^   ,   θ  ^   )       var   [   β  ^   ]   var   [   θ  ^      ]         corr   normal-^  β    normal-^  θ       cov   normal-^  β    normal-^  θ     fragments     fragments  var   fragments  normal-[   normal-^  β   normal-]   var   fragments  normal-[   normal-^  θ      normal-]      \operatorname{corr}(\hat{\beta},\hat{\theta})=\frac{\operatorname{cov}(\hat{%
 \beta},\hat{\theta})}{\sqrt{\operatorname{var}[\hat{\beta}]\operatorname{var}[%
 \hat{\theta}}]}            =\frac{\operatorname{cov}(\beta+\epsilon_{\beta}, \theta+\epsilon_\theta)}{\sqrt{\operatorname{var}[\beta+\epsilon_{\beta}]\operatorname{var}[\theta+\epsilon_\theta]}},  which, assuming the errors are uncorrelated with each other and with the estimates, gives       corr   (   β  ^   ,   θ  ^   )    =    cov   (  β  ,  θ  )       (    var   [  β  ]    +   var   [   ϵ  β   ]     )    (    var   [  θ  ]    +   var   [   ϵ  θ   ]     )           corr   normal-^  β    normal-^  θ       cov  β  θ          var  β    var   subscript  ϵ  β        var  θ    var   subscript  ϵ  θ          \operatorname{corr}(\hat{\beta},\hat{\theta})=\frac{\operatorname{cov}(\beta,%
 \theta)}{\sqrt{(\operatorname{var}[\beta]+\operatorname{var}[\epsilon_{\beta}]%
 )(\operatorname{var}[\theta]+\operatorname{var}[\epsilon_{\theta}])}}            =\frac{\operatorname{cov}(\beta,\theta)}{\sqrt{(\operatorname{var}[\beta]\operatorname{var}[\theta])}}.\frac{\sqrt{\operatorname{var}[\beta]\operatorname{var}[\theta]}}{\sqrt{(\operatorname{var}[\beta]+\operatorname{var}[\epsilon_\beta])(\operatorname{var}[\theta]+\operatorname{var}[\epsilon_\theta])}}         =\rho \sqrt{R_\beta R_\theta},  where    R  β     subscript  R  β    R_{\beta}   is the separation index of the set of estimates of   β   β   \beta   , which is analogous to Cronbach's alpha ; that is, in terms of Classical test theory ,    R  β     subscript  R  β    R_{\beta}   is analogous to a reliability coefficient. Specifically, the separation index is given as follows:        R  β   =    var   [  β  ]      var   [  β  ]    +   var   [   ϵ  β   ]      =     var   [   β  ^   ]    -   var   [   ϵ  β   ]      var   [   β  ^   ]      ,         subscript  R  β      var  β      var  β    var   subscript  ϵ  β                var   normal-^  β     var   subscript  ϵ  β      var   normal-^  β        R_{\beta}=\frac{\operatorname{var}[\beta]}{\operatorname{var}[\beta]+%
 \operatorname{var}[\epsilon_{\beta}]}=\frac{\operatorname{var}[\hat{\beta}]-%
 \operatorname{var}[\epsilon_{\beta}]}{\operatorname{var}[\hat{\beta}]},     where the mean squared standard error of person estimate gives an estimate of the variance of the errors,    ϵ  β     subscript  ϵ  β    \epsilon_{\beta}   . The standard errors are normally produced as a by-product of the estimation process (see Rasch model estimation ).  See also   Regression dilution  Errors-in-variables model   References   Jensen, A.R. (1998). ''The g Factor: The Science of Mental Ability'' Praeger, Connecticut, USA. ISBN 0-275-96103-6  Spearman, C. (1904) "The Proof and Measurement of Association between Two Things". The American Journal of Psychology , 15 (1), 72–101   External links   Disattenuating correlations  [ http://pareonline.net/getvn.asp?v=8&n; ;=11 Disattenuation of correlation and regression coefficients: Jason W. Osborne]   "  Category:Measurement  Category:Covariance and correlation  Category:Psychometrics   