   Karush–Kuhn–Tucker conditions      Karush–Kuhn–Tucker conditions  In [[mathematical optimization]], the '''Karush–Kuhn–Tucker (KKT) conditions''' (also known as the '''Kuhn –Tucker conditions''') are first order necessary conditions for a solution in nonlinear programming to be optimal , provided that some regularity conditions are satisfied. Allowing inequality constraints, the KKT approach to nonlinear programming generalizes the method of Lagrange multipliers , which allows only equality constraints. The system of equations corresponding to the KKT conditions is usually not solved directly, except in the few special cases where a closed-form solution can be derived analytically. In general, many optimization algorithms can be interpreted as methods for numerically solving the KKT system of equations. 1  The KKT conditions were originally named after Harold W. Kuhn , and Albert W. Tucker , who first published the conditions in 1951. 2 Later scholars discovered that the necessary conditions for this problem had been stated by William Karush in his master's thesis in 1939. 3 4  Nonlinear optimization problem  Consider the following nonlinear optimization problem :   Maximize    f   (  x  )       f  x    f(x)       subject to            g  i    (  x  )    ≤  0   ,     h  j    (  x  )    =  0      formulae-sequence       subscript  g  i   x   0        subscript  h  j   x   0     g_{i}(x)\leq 0,h_{j}(x)=0        where x is the optimization variable,   f   f   f   is the objective or cost function,      g  i     (  i  =  1  ,  …  ,  m  )      fragments   subscript  g  i    fragments  normal-(  i   1  normal-,  normal-…  normal-,  m  normal-)     g_{i}\ (i=1,\ldots,m)   are the inequality constraint functions, and      h  j     (  j  =  1  ,  …  ,  l  )      fragments   subscript  h  j    fragments  normal-(  j   1  normal-,  normal-…  normal-,  l  normal-)     h_{j}\ (j=1,\ldots,l)   are the equality constraint functions. The numbers of inequality and equality constraints are denoted m and l , respectively.  Necessary conditions  Suppose that the objective function     f  :    ℝ  n   →  ℝ      normal-:  f   normal-→   superscript  ℝ  n   ℝ     f:\mathbb{R}^{n}\rightarrow\mathbb{R}   and the constraint functions     g  i   :    ℝ  n   →  ℝ      normal-:   subscript  g  i    normal-→   superscript  ℝ  n   ℝ     g_{i}:\,\!\mathbb{R}^{n}\rightarrow\mathbb{R}   and     h  j   :    ℝ  n   →  ℝ      normal-:   subscript  h  j    normal-→   superscript  ℝ  n   ℝ     h_{j}:\,\!\mathbb{R}^{n}\rightarrow\mathbb{R}   are continuously differentiable at a point    x  *     superscript  x     x^{*}   . If    x  *     superscript  x     x^{*}   is a local minimum that satisfies some regularity conditions (see below), then there exist constants      μ  i     (  i  =  1  ,  …  ,  m  )      fragments   subscript  μ  i    fragments  normal-(  i   1  normal-,  normal-…  normal-,  m  normal-)     \mu_{i}\ (i=1,\ldots,m)   and      λ  j     (  j  =  1  ,  …  ,  l  )      fragments   subscript  λ  j    fragments  normal-(  j   1  normal-,  normal-…  normal-,  l  normal-)     \lambda_{j}\ (j=1,\ldots,l)   , called KKT multipliers, such that  thumb|upright=2|Inequality constraint diagram for optimization problems   Stationarity  For maximizing f(x)       ∇  f    (   x  *   )    =     ∑   i  =  1   m     μ  i    ∇   g  i     (   x  *   )     +    ∑   j  =  1   l     λ  j    ∇   h  j     (   x  *   )       ,         normal-∇  f    superscript  x         superscript   subscript     i  1    m      subscript  μ  i    normal-∇   subscript  g  i     superscript  x        superscript   subscript     j  1    l      subscript  λ  j    normal-∇   subscript  h  j     superscript  x         \nabla f(x^{*})=\sum_{i=1}^{m}\mu_{i}\nabla g_{i}(x^{*})+\sum_{j=1}^{l}\lambda%
 _{j}\nabla h_{j}(x^{*}),     For minimizing f(x)      -    ∇  f    (   x  *   )     =     ∑   i  =  1   m     μ  i    ∇   g  i     (   x  *   )     +    ∑   j  =  1   l     λ  j    ∇   h  j     (   x  *   )       ,           normal-∇  f    superscript  x          superscript   subscript     i  1    m      subscript  μ  i    normal-∇   subscript  g  i     superscript  x        superscript   subscript     j  1    l      subscript  λ  j    normal-∇   subscript  h  j     superscript  x         -\nabla f(x^{*})=\sum_{i=1}^{m}\mu_{i}\nabla g_{i}(x^{*})+\sum_{j=1}^{l}%
 \lambda_{j}\nabla h_{j}(x^{*}),       Primal feasibility         g  i    (   x  *   )    ≤  0   ,    for all  i   =   1  ,  …  ,  m       formulae-sequence       subscript  g  i    superscript  x     0       for all  i    1  normal-…  m      g_{i}(x^{*})\leq 0,\mbox{ for all }i=1,\ldots,m            h  j    (   x  *   )    =  0   ,    for all  j   =   1  ,  …  ,  l       formulae-sequence       subscript  h  j    superscript  x     0       for all  j    1  normal-…  l      h_{j}(x^{*})=0,\mbox{ for all }j=1,\ldots,l\,\!       Dual feasibility        μ  i   ≥  0   ,    for all  i   =   1  ,  …  ,  m       formulae-sequence     subscript  μ  i   0       for all  i    1  normal-…  m      \mu_{i}\geq 0,\mbox{ for all }i=1,\ldots,m       Complementary slackness          μ  i    g  i    (   x  *   )    =  0   ,     for all   i   =   1  ,  …  ,  m     .     formulae-sequence       subscript  μ  i    subscript  g  i    superscript  x     0       for all  i    1  normal-…  m      \mu_{i}g_{i}(x^{*})=0,\mbox{for all}\;i=1,\ldots,m.      In the particular case    m  =  0      m  0    m=0   , i.e., when there are no inequality constraints, the KKT conditions turn into the Lagrange conditions, and the KKT multipliers are called Lagrange multipliers .  If some of the functions are non-differentiable, subdifferential versions of Karush–Kuhn–Tucker (KKT) conditions are available. 5  Regularity conditions (or constraint qualifications)  In order for a minimum point    x  *     superscript  x     x^{*}   to satisfy the above KKT conditions, the problem should satisfy some regularity conditions; the most used ones are listed below:   Linearity constraint qualification : If    g  i     subscript  g  i    g_{i}   and    h  j     subscript  h  j    h_{j}   are affine functions , then no other condition is needed.  Linear independence constraint qualification (LICQ): the gradients of the active inequality constraints and the gradients of the equality constraints are linearly independent at    x  *     superscript  x     x^{*}   .  Mangasarian–Fromovitz constraint qualification (MFCQ): the gradients of the active inequality constraints and the gradients of the equality constraints are positive-linearly independent at    x  *     superscript  x     x^{*}   .  Constant rank constraint qualification (CRCQ): for each subset of the gradients of the active inequality constraints and the gradients of the equality constraints the rank at a vicinity of    x  *     superscript  x     x^{*}   is constant.  Constant positive linear dependence constraint qualification (CPLD): for each subset of the gradients of the active inequality constraints and the gradients of the equality constraints, if it is positive-linear dependent at    x  *     superscript  x     x^{*}   then it is positive-linear dependent at a vicinity of    x  *     superscript  x     x^{*}   .  Quasi-normality constraint qualification (QNCQ): if the gradients of the active inequality constraints and the gradients of the equality constraints are positive-linearly dependent at    x  *     superscript  x     x^{*}   with associated multipliers    λ  i     subscript  λ  i    \lambda_{i}   for equalities and    μ  j     subscript  μ  j    \mu_{j}   for inequalities, then there is no sequence     x  k   →   x  *      normal-→   subscript  x  k    superscript  x      x_{k}\to x^{*}   such that     λ  i   ≠  0  ⇒    λ  i    h  i    (   x  k   )    >  0         subscript  λ  i   0    normal-⇒       subscript  λ  i    subscript  h  i    subscript  x  k         0     \lambda_{i}\neq 0\Rightarrow\lambda_{i}h_{i}(x_{k})>0   and     μ  j   ≠  0  ⇒    μ  j    g  j    (   x  k   )    >  0         subscript  μ  j   0    normal-⇒       subscript  μ  j    subscript  g  j    subscript  x  k         0     \mu_{j}\neq 0\Rightarrow\mu_{j}g_{j}(x_{k})>0   .  Slater condition : for a convex problem , there exists a point   x   x   x   such that     h   (  x  )    =  0        h  x   0    h(x)=0   and      g  i    (  x  )    <  0         subscript  g  i   x   0    g_{i}(x)<0   .   (     v  1   ,  …  ,   v  n       subscript  v  1   normal-…   subscript  v  n     v_{1},\ldots,v_{n}   ) is positive-linear dependent if there exists      a  1   ≥   0  ,  …    ,    a  n   ≥  0      formulae-sequence     subscript  a  1    0  normal-…       subscript  a  n   0     a_{1}\geq 0,\ldots,a_{n}\geq 0   not all zero such that       a  1    v  1    +  ⋯  +    a  n    v  n     =  0           subscript  a  1    subscript  v  1    normal-⋯     subscript  a  n    subscript  v  n     0    a_{1}v_{1}+\cdots+a_{n}v_{n}=0   .  It can be shown that LICQ⇒MFCQ⇒CPLD⇒QNCQ, LICQ⇒CRCQ⇒CPLD⇒QNCQ (and the converses are not true), although MFCQ is not equivalent to CRCQ 6 . In practice weaker constraint qualifications are preferred since they provide stronger optimality conditions.  Sufficient conditions  In some cases, the necessary conditions are also sufficient for optimality. In general, the necessary conditions are not sufficient for optimality and additional information is necessary, such as the Second Order Sufficient Conditions (SOSC). For smooth functions, SOSC involve the second derivatives, which explains its name.  The necessary conditions are sufficient for optimality if the objective function   f   f   f   is a concave function , the inequality constraints    g  j     subscript  g  j    g_{j}   are continuously differentiable convex functions and the equality constraints    h  i     subscript  h  i    h_{i}   are affine functions .  It was shown by Martin in 1985 that the broader class of functions in which KKT conditions guarantees global optimality are the so-called Type 1 invex functions . 7 8  Second Order Sufficient Conditions  For smooth, non-linear optimisation problems, a second order sufficient condition is given as follows. Consider     x  *   ,   λ  *   ,   ρ  *       superscript  x     superscript  λ     superscript  ρ      x^{*},\lambda^{*},\rho^{*}   that find a local minimum using the Karush-Kuhn-Tucker conditions above. With    ρ  *     superscript  ρ     \rho^{*}   such that strict complementarity is held at    x  *     superscript  x     x^{*}   ( i.e. all     ρ  i   >  0       subscript  ρ  i   0    \mathbb{\rho}_{i}>0    ), then for all    s  ≠  0      s  0    s\neq 0   such that         [     ∂  g    (   x  *   )     ∂  x    ,     ∂  h    (   x  *   )     ∂  x    ]   T   s   =  0         superscript         g    superscript  x       x          h    superscript  x       x     T   s   0    \left[\frac{\partial g(x^{*})}{\partial x},\frac{\partial h(x^{*})}{\partial x%
 }\right]^{T}s=0     the following equation must hold;        s  ′     ∇   x  x   2   L    (   x  *   ,   λ  *   ,   ρ  *   )   s   >  0         superscript  s  normal-′     subscript   superscript  normal-∇  2     x  x    L     superscript  x     superscript  λ     superscript  ρ     s   0    s^{\prime}\nabla^{2}_{xx}L(x^{*},\lambda^{*},\rho^{*})s>0     If the above condition is strictly met, the function is a strict constrained local minimum.  Economics  Often in mathematical economics the KKT approach is used in theoretical models in order to obtain qualitative results. For example, consider a firm that maximizes its sales revenue subject to a minimum profit constraint. Letting Q be the quantity of output produced (to be chosen), R ( Q ) be sales revenue with a positive first derivative and with a zero value at zero output, C ( Q ) be production costs with a positive first derivative and with a non-negative value at zero output, and    G   m  i  n      subscript  G    m  i  n     G_{min}   be the positive minimal acceptable level of profit , then the problem is a meaningful one if the revenue function levels off so it eventually is less steep than the cost function. The problem expressed in the previously given minimization form is   Minimize    -   R   (  Q  )          R  Q     -R(Q)     subject to       G   m  i  n    ≤    R   (  Q  )    -   C   (  Q  )          subscript  G    m  i  n        R  Q     C  Q      G_{min}\leq R(Q)-C(Q)          Q  ≥  0   ,      Q  0    Q\geq 0,      and the KKT conditions are          (     d  R   /  d   Q   )    (   1  +  μ   )    -   μ   (     d  C   /  d   Q   )     ≤  0   ,                d  R   d   Q     1  μ      μ        d  C   d   Q     0    (\text{d}R/\text{d}Q)(1+\mu)-\mu(\text{d}C/\text{d}Q)\leq 0,          Q  ≥  0   ,      Q  0    Q\geq 0,           Q   [     (     d  R   /  d   Q   )    (   1  +  μ   )    -   μ   (     d  C   /  d   Q   )     ]    =  0   ,        Q   delimited-[]            d  R   d   Q     1  μ      μ        d  C   d   Q       0    Q[(\text{d}R/\text{d}Q)(1+\mu)-\mu(\text{d}C/\text{d}Q)]=0,            R   (  Q  )    -   C   (  Q  )    -   G   m  i  n     ≥  0   ,          R  Q     C  Q    subscript  G    m  i  n     0    R(Q)-C(Q)-G_{min}\geq 0,          μ  ≥  0   ,      μ  0    \mu\geq 0,          μ   [    R   (  Q  )    -   C   (  Q  )    -   G   m  i  n     ]    =  0.        μ   delimited-[]      R  Q     C  Q    subscript  G    m  i  n       0.    \mu[R(Q)-C(Q)-G_{min}]=0.     Since Q =0 would violate the minimum profit constraint, we have Q >0 and hence the third condition implies that the first condition holds with equality. Solving that equality gives          d  R   /  d   Q   =    μ   1  +  μ     (     d  C   /  d   Q   )     .            d  R   d   Q       μ    1  μ          d  C   d   Q      \text{d}R/\text{d}Q=\frac{\mu}{1+\mu}(\text{d}C/\text{d}Q).     Because it was given that      d  R   /  d   Q          d  R   d   Q    \text{d}R/\text{d}Q   and      d  C   /  d   Q          d  C   d   Q    \text{d}C/\text{d}Q   are strictly positive, this inequality along with the non-negativity condition on   μ   μ   \mu   guarantees that   μ   μ   \mu   is positive and so the revenue-maximizing firm operates at a level of output at which marginal revenue       d  R   /  d   Q          d  R   d   Q    \text{d}R/\text{d}Q   is less than marginal cost       d  C   /  d   Q          d  C   d   Q    \text{d}C/\text{d}Q   — a result that is of interest because it contrasts with the behavior of a profit maximizing firm, which operates at a level at which they are equal.  Value function  If we reconsider the optimization problem as a maximization problem with constant inequality constraints,       Maximize   f   (  x  )       Maximize  f  x    \text{Maximize }\;f(x)            g  i    (  x  )    ≤   a  i    ,     h  j    (  x  )    =  0.      formulae-sequence       subscript  g  i   x    subscript  a  i         subscript  h  j   x   0.     g_{i}(x)\leq a_{i},h_{j}(x)=0.            V   (   a  1   ,  …  ,   a  n   )    =    sup  x    f   (  x  )           V    subscript  a  1   normal-…   subscript  a  n       subscript  supremum  x     f  x      V(a_{1},\ldots,a_{n})=\sup\limits_{x}f(x)        The value function is defined as         g  i    (  x  )    ≤   a  i    ,     h  j    (  x  )    =  0      formulae-sequence       subscript  g  i   x    subscript  a  i         subscript  h  j   x   0     g_{i}(x)\leq a_{i},h_{j}(x)=0           j  ∈   {  1  ,  …  ,  l  }    ,   i  ∈   {  1  ,  …  ,  m  }     .     formulae-sequence    j   1  normal-…  l      i   1  normal-…  m      j\in\{1,\ldots,l\},i\in\{1,\ldots,m\}.           {  a  ∈   ℝ  m   |  for some  x  ∈  X  ,   g  i    (  x  )   ≤   a  i   ,  i  ∈   {  1  ,  …  ,  m  }   .     fragments  normal-{  a    superscript  ℝ  m   normal-|  for some  x   X  normal-,   subscript  g  i    fragments  normal-(  x  normal-)     subscript  a  i   normal-,  i    fragments  normal-{  1  normal-,  normal-…  normal-,  m  normal-}   normal-.    \{a\in\mathbb{R}^{m}|\text{for some }x\in X,g_{i}(x)\leq a_{i},i\in\{1,\ldots,%
 m\}.              μ  i     subscript  μ  i    \mu_{i}        (So the domain of V is    a  i     subscript  a  i    a_{i}   )  Given this definition, each coefficient,    a  i     subscript  a  i    a_{i}   , is the rate at which the value function increases as    μ  0     subscript  μ  0    \mu_{0}   increases. Thus if each     ∇  f    (   x  *   )        normal-∇  f    superscript  x      \nabla f(x^{*})   is interpreted as a resource constraint, the coefficients tell you how much increasing a resource will increase the optimum value of our function f . This interpretation is especially important in economics and is used, for instance, in utility maximization problems .  Generalizations  With an extra constant multiplier        μ  0    ∇  f    (   x  *   )    +    ∑   i  =  1   m     μ  i    ∇   g  i     (   x  *   )     +    ∑   j  =  1   l     λ  j    ∇   h  j     (   x  *   )      =  0   ,           subscript  μ  0    normal-∇  f    superscript  x       superscript   subscript     i  1    m      subscript  μ  i    normal-∇   subscript  g  i     superscript  x        superscript   subscript     j  1    l      subscript  λ  j    normal-∇   subscript  h  j     superscript  x       0    \mu_{0}\nabla f(x^{*})+\sum_{i=1}^{m}\mu_{i}\nabla g_{i}(x^{*})+\sum_{j=1}^{l}%
 \lambda_{j}\nabla h_{j}(x^{*})=0,   , which may be zero, in front of $\nabla f(x^*)$ the KKT stationarity conditions turn into  $$\mu_0 \nabla f(x^*) + \sum_{i=1}^m \mu_i \nabla g_i(x^*) + \sum_{j=1}^l \lambda_j \nabla h_j(x^*) = 0,$$  which are called the Fritz John conditions .  The KKT conditions belong to a wider class of the First Order Necessary Conditions (FONC), which allow for non-smooth functions using subderivatives .  See also   Farkas' lemma  Big M method - corresponding technique for linear optimization problems   References    Further reading        External links   Karush–Kuhn–Tucker conditions with derivation and examples  Examples and Tutorials on the KKT Conditions   de:Konvexe Optimierung#Karush-Kuhn-Tucker-Bedingungen "  Category:Mathematical optimization  Category:Mathematical and quantitative methods (economics)     ↩   ↩  ↩  ↩  ↩  ↩  ↩  ↩     