   Reed–Solomon error correction      Reed–Solomon error correction   Reed–Solomon codes are an important group of error-correcting codes that were introduced by Irving S. Reed and Gustave Solomon in 1960. 1 They have many important applications, the most prominent of which include consumer technologies such as CDs , DVDs , Blu-ray Discs , QR Codes , data transmission technologies such as DSL and WiMAX , broadcast systems such as DVB and ATSC , and storage systems such as RAID 6 . They are also used in satellite communication.  In coding theory , the Reed–Solomon code belongs to the class of non- binary  cyclic error-correcting codes. The Reed–Solomon code is based on univariate  polynomials over finite fields .  It is able to detect and correct multiple symbol errors. By adding   t   t   t   check symbols to the data, a Reed–Solomon code can detect any combination of up to   t   t   t   erroneous symbols, or correct up to      ⌊  t   /  2   ⌋          normal-⌊  t   2   normal-⌋    ⌊t/2⌋   symbols. As an erasure code , it can correct up to   t   t   t   known erasures, or it can detect and correct combinations of errors and erasures. Furthermore, Reed–Solomon codes are suitable as multiple- burst bit-error correcting codes, since a sequence of    b  +    1       b    normal-  1     b+ 1   consecutive bit errors can affect at most two symbols of size   b   b   b   . The choice of   t   t   t   is up to the designer of the code, and may be selected within wide limits.  History  Reed–Solomon codes were developed in 1960 by Irving S. Reed and Gustave Solomon , who were then staff members of MIT  Lincoln Laboratory . Their seminal article was entitled "Polynomial Codes over Certain Finite Fields." . When the article was written, an efficient decoding algorithm was not known. A solution for the latter was found in 1969 by Elwyn Berlekamp and James Massey , and is since known as the Berlekamp–Massey decoding algorithm . In 1977, Reed–Solomon codes were notably implemented in the Voyager program in the form of concatenated codes . The first commercial application in mass-produced consumer products appeared in 1982 with the compact disc , where two interleaved Reed–Solomon codes are used. Today, Reed–Solomon codes are widely implemented in digital storage devices and digital communication standards, though they are being slowly replaced by more modern low-density parity-check (LDPC) codes or turbo codes . For example, Reed–Solomon codes are used in the digital video broadcasting (DVB) standard DVB-S , but LDPC codes are used in its successor DVB-S2 .  Basis  The Reed–Solomon code is based on univariate polynomials over finite fields; in particular, for some parameters   k   k   k   and   n   n   n   , the codewords of the Reed–Solomon code consists of all function tables of polynomials of degree less than   k   k   k   over the finite field with   n   n   n   elements - for this to work,   n   n   n   has to be prime power . The encoding scheme of the Reed–Solomon code turns   k   k   k   symbols into such a function table, which is essentially a list of   n   n   n   symbols. One way to perform this encoding is by interpreting the   k   k   k   given symbols as the first segment of the function table of a polynomial of degree less than   k   k   k   . A simple argument shows that there is exactly one such polynomial, and the remaining    n  −  k      n  normal-−  k    n−k   symbols can thus be generated by evaluating the polynomial at those points. Since the   n   n   n   transmitted symbols form an overdetermined system that specifies a polynomial of degree less than   k   k   k   , we can use interpolation techniques at the receiver to recover the original message in case not too many errors happened.  In order to achieve the most efficient decoding procedures, the encoding procedure of the Reed–Solomon code is often constructed a bit differently, namely as cyclic BCH codes : the   k   k   k   source symbols are interpreted as the coefficients of a polynomial of degree less than   k   k   k   , and the additional    n  −  k      n  normal-−  k    n−k   symbols are derived from the coefficients of a polynomial constructed by multiplying    p   (  x  )       p  x    p(x)   with a cyclic generator polynomial .  Applications  Data storage  Reed–Solomon coding is very widely used in mass storage systems to correct the burst errors associated with media defects.  Reed–Solomon coding is a key component of the compact disc . It was the first use of strong error correction coding in a mass-produced consumer product, and DAT and DVD use similar schemes. In the CD, two layers of Reed–Solomon coding separated by a 28-way convolutional  interleaver yields a scheme called Cross-Interleaved Reed–Solomon Coding ( CIRC ). The first element of a CIRC decoder is a relatively weak inner (32,28) Reed–Solomon code, shortened from a (255,251) code with 8-bit symbols. This code can correct up to 2 byte errors per 32-byte block. More importantly, it flags as erasures any uncorrectable blocks, i.e., blocks with more than 2 byte errors. The decoded 28-byte blocks, with erasure indications, are then spread by the deinterleaver to different blocks of the (28,24) outer code. Thanks to the deinterleaving, an erased 28-byte block from the inner code becomes a single erased byte in each of 28 outer code blocks. The outer code easily corrects this, since it can handle up to 4 such erasures per block.  The result is a CIRC that can completely correct error bursts up to 4000 bits, or about 2.5 mm on the disc surface. This code is so strong that most CD playback errors are almost certainly caused by tracking errors that cause the laser to jump track, not by uncorrectable error bursts. 2  DVDs use a similar scheme, but with much larger blocks, a (208,192) inner code, and a (182,172) outer code.  Reed–Solomon error correction is also used in parchive files which are commonly posted accompanying multimedia files on USENET . The Distributed online storage service Wuala also makes use of Reed–Solomon when breaking up files.  Bar code  Almost all two-dimensional bar codes such as PDF-417 , MaxiCode , Datamatrix , QR Code , and Aztec Code use Reed–Solomon error correction to allow correct reading even if a portion of the bar code is damaged. When the bar code scanner cannot recognize a bar code symbol, it will treat it as an erasure.  Reed–Solomon coding is less common in one-dimensional bar codes, but is used by the PostBar symbology.  Data transmission  Specialized forms of Reed–Solomon codes, specifically Cauchy -RS and Vandermonde -RS, can be used to overcome the unreliable nature of data transmission over erasure channels . The encoding process assumes a code of RS( N , K ) which results in N codewords of length N symbols each storing K symbols of data, being generated, that are then sent over an erasure channel.  Any combination of K codewords received at the other end is enough to reconstruct all of the N codewords. The code rate is generally set to 1/2 unless the channel's erasure likelihood can be adequately modelled and is seen to be less. In conclusion, N is usually 2 K , meaning that at least half of all the codewords sent must be received in order to reconstruct all of the codewords sent.  Reed–Solomon codes are also used in xDSL systems and CCSDS 's Space Communications Protocol Specifications as a form of forward error correction .  Space transmission  One significant application of Reed–Solomon coding was to encode the digital pictures sent back by the Voyager space probe.  Voyager introduced Reed–Solomon coding concatenated with convolutional codes , a practice that has since become very widespread in deep space and satellite (e.g., direct digital broadcasting) communications.  Viterbi decoders tend to produce errors in short bursts. Correcting these burst errors is a job best done by short or simplified Reed–Solomon codes.  Modern versions of concatenated Reed–Solomon/Viterbi-decoded convolutional coding were and are used on the Mars Pathfinder , Galileo , Mars Exploration Rover and Cassini missions, where they perform within about 1–1.5 dB of the ultimate limit imposed by the Shannon capacity .  These concatenated codes are now being replaced by more powerful turbo codes .  Constructions  The Reed–Solomon code is actually a family of codes: For every choice of the three parameters k a_1,\dots,a_n of the field F , and the sequence of values is the corresponding codeword. Formally, the set   𝐂   𝐂   \mathbf{C}   of codewords of the Reed–Solomon code is defined as follows:      k  -  1      k  1    k-1   agree in at most     n  -   (   k  -  1   )    =    n  -  k   +  1         n    k  1        n  k   1     n-(k-1)=n-k+1   points, this means that any two codewords of the Reed–Solomon code disagree in at least    k  -  1      k  1    k-1   positions. Furthermore, there are two polynomials that do agree in    d  =    n  -  k   +  1       d      n  k   1     d=n-k+1   points but are not equal, and thus, the distance of the Reed–Solomon code is exactly    δ  =   d  /  n   =    1  -   k  /  n    +   1  /  n    ∼   1  -  R         δ    d  n            1    k  n      1  n      similar-to      1  R      \delta=d/n=1-k/n+1/n\sim 1-R   . Then the relative distance is    R  =   k  /  n       R    k  n     R=k/n   , where     δ  +  R   ≤  1        δ  R   1    \delta+R\leq 1   is the rate. This trade-off between the relative distance and the rate is asymptotically optimal since, by the Singleton bound , every code satisfies    q  k     superscript  q  k    q^{k}   . Being a code that achieves this optimal trade-off, the Reed–Solomon code belongs to the class of maximum distance separable codes .  While the number of different polynomials of degree less than k and the number of different messages are both equal to     a  1   ,  …  ,   a  k       subscript  a  1   normal-…   subscript  a  k     a_{1},\dots,a_{k}   , and thus every message can be uniquely mapped to such a polynomial, there are different ways of doing this encoding. The original construction of  interprets the message x as the coefficients of the polynomial p , whereas subsequent constructions interpret the message as the values of the polynomial at the first k points     a  1   ,  …  ,   a  n       subscript  a  1   normal-…   subscript  a  n     a_{1},\dots,a_{n}   and obtain the polynomial p by interpolating these values with a polynomial of degree less than k . The latter encoding procedure, while being slightly less efficient, has the advantage that it gives rise to a systematic code , that is, the original message is always contained as a subsequence of the codeword.  In many contexts it is convenient to choose the sequence   α   α   \alpha   of evaluation points so that they exhibit some additional structure. In particular, it is useful to choose the sequence of successive powers of a primitive root    F   F   F   of the field   α   α   \alpha   , that is,     a  i   =   α  i        subscript  a  i    superscript  α  i     a_{i}=\alpha^{i}   is generator of the finite field's multiplicative group and the sequence is defined as    i  =   1  ,  …  ,   q  -  1        i   1  normal-…    q  1      i=1,\dots,q-1   for all   F   F   F   . This sequence contains all elements of   0   0    except for    n  =   q  -  1       n    q  1     n=q-1   , so in this setting, the block length is    p   (  a  )       p  a    p(a)   . Then it follows that, whenever   F   F   F   is a polynomial over    p   (   α  a   )       p    α  a     p(\alpha a)   , then the function    p   (  a  )       p  a    p(a)   is also a polynomial of the same degree, which gives rise to a codeword that is a cyclic left-shift of the codeword derived from    x  =   (   x  1   ,  …  ,   x  k   )   ∈   F  k         x    subscript  x  1   normal-…   subscript  x  k          superscript  F  k      x=(x_{1},\dots,x_{k})\in F^{k}   ; thus, this construction of Reed–Solomon codes gives rise to a cyclic code .  Simple encoding procedure: The message as a sequence of coefficients  In the original construction of , the message    p  x     subscript  p  x    p_{x}   is mapped to the polynomial       p  x    (  a  )    =    ∑   i  =  1   k     x  i     a   i  -  1        .         subscript  p  x   a     superscript   subscript     i  1    k      subscript  x  i    superscript  a    i  1        p_{x}(a)=\sum_{i=1}^{k}x_{i}a^{i-1}\,.   with     p   p   p   As described above, the codeword is then obtained by evaluating   n   n   n   at     a  1   ,  …  ,   a  n       subscript  a  1   normal-…   subscript  a  n     a_{1},\dots,a_{n}   different points   F   F   F   of the field    C  :    F  k   →   F  n       normal-:  C   normal-→   superscript  F  k    superscript  F  n      C:F^{k}\to F^{n}   . Thus the classical encoding function      C   (  x  )    =   (    p  x    (   a  1   )    ,  …  ,    p  x    (   a  n   )    )    .        C  x       subscript  p  x    subscript  a  1    normal-…     subscript  p  x    subscript  a  n       C(x)=\big(p_{x}(a_{1}),\dots,p_{x}(a_{n})\big)\,.   for the Reed–Solomon code is defined as follows:     C   C   C   This function     C   (  x  )    =   x  ⋅  A         C  x    normal-⋅  x  A     C(x)=x\cdot A   is a linear mapping , that is, it satisfies    (   k  ×  n   )      k  n    (k\times n)   for the following   A   A   A   -matrix   F   F   F   with elements from    A  =   [     1    …    1       a  1     …     a  n        a  1  2     …     a  n  2       ⋮    ⋱    ⋮       a  1   k  -  1      …     a  n   k  -  1       ]       A    1  normal-…  1     subscript  a  1   normal-…   subscript  a  n      superscript   subscript  a  1   2   normal-…   superscript   subscript  a  n   2     normal-⋮  normal-⋱  normal-⋮     superscript   subscript  a  1     k  1    normal-…   superscript   subscript  a  n     k  1        A=\begin{bmatrix}1&\dots&1\\
 a_{1}&\dots&a_{n}\\
 a_{1}^{2}&\dots&a_{n}^{2}\\
 \vdots&\ddots&\vdots\\
 a_{1}^{k-1}&\dots&a_{n}^{k-1}\end{bmatrix}   :     F   F   F   This matrix is the transpose of a Vandermonde matrix over   A   A   A   . In other words, the Reed–Solomon code is a linear code , and in the classical encoding procedure, its generator matrix is   x   x   x   .  Systematic encoding procedure: The message as an initial sequence of values  As mentioned above, there is an alternative way to map codewords    p  x     subscript  p  x    p_{x}   to polynomials    p  x     subscript  p  x    p_{x}   . In this alternative encoding procedure, the polynomial   k   k   k   is the unique polynomial of degree less than      p  x    (   a  i   )    =   x  i          subscript  p  x    subscript  a  i     subscript  x  i     p_{x}(a_{i})=x_{i}   such that      i  =   1  ,  …  ,  k       i   1  normal-…  k     i=1,\dots,k   holds for all    p  x     subscript  p  x    p_{x}   . To compute this polynomial   x   x   x   from     a   k  +  1    ,  …  ,   a  n       subscript  a    k  1    normal-…   subscript  a  n     a_{k+1},\dots,a_{n}   , one can use Lagrange interpolation . Once it has been found, it is evaluated at the other points    C  :    F  k   →   F  n       normal-:  C   normal-→   superscript  F  k    superscript  F  n      C:F^{k}\to F^{n}   of the field. The alternative encoding function      C   (  x  )    =   (    p  x    (   a  1   )    ,  …  ,    p  x    (   a  n   )    )    .        C  x       subscript  p  x    subscript  a  1    normal-…     subscript  p  x    subscript  a  n       C(x)=\big(p_{x}(a_{1}),\dots,p_{x}(a_{n})\big)\,.   for the Reed–Solomon code is then again just the sequence of values:     k   k   k   This time, however, the first   x   x   x   entries of the codeword are exactly equal to   x   x   x   , so this encoding procedure gives rise to a systematic code . It can be checked that the alternative encoding function is a linear mapping as well.  The BCH view: The codeword as a sequence of coefficients  In this view, the sender again maps the message    p  x     subscript  p  x    p_{x}   to a polynomial    p  x     subscript  p  x    p_{x}   , and for this, any of the two mappings above can be used (where the message is either interpreted as the coefficients of    p  x     subscript  p  x    p_{x}   or as the initial sequence of values of    p  x     subscript  p  x    p_{x}   ). Once the sender has constructed the polynomial    p  x     subscript  p  x    p_{x}   in some way, however, instead of sending the values of   s   s   s   at all points, the sender computes some related polynomial    n  -  1      n  1    n-1   of degree at most    n  =   q  -  1       n    q  1     n=q-1   for   n   n   n   and sends the    s   (  a  )       s  a    s(a)    coefficients of that polynomial. The polynomial     p  x    (  a  )        subscript  p  x   a    p_{x}(a)   is constructed by multiplying the message polynomial    k  -  1      k  1    k-1   , which has degree at most    g   (  a  )       g  a    g(a)   , with a generator polynomial     n  -  k      n  k    n-k   of degree    g   (  x  )       g  x    g(x)   that is known to both the sender and the receiver. 3 The generator polynomial    α  ,   α  2   ,  …  ,   α   n  -  k       α   superscript  α  2   normal-…   superscript  α    n  k      \alpha,\alpha^{2},\dots,\alpha^{n-k}   is defined as the polynomial whose roots are exactly      g   (  x  )    =    (   x  -  α   )    (   x  -   α  2    )   ⋯   (   x  -   α   n  -  k     )    =    g  0   +    g  1   x   +  ⋯  +    g   n  -  k  -  1     x   n  -  k  -  1     +    x   n  -  k       .          g  x       x  α     x   superscript  α  2    normal-⋯    x   superscript  α    n  k              subscript  g  0      subscript  g  1   x   normal-⋯     subscript  g    n  k  1     superscript  x    n  k  1      superscript  x    n  k        g(x)=(x-\alpha)(x-\alpha^{2})\cdots(x-\alpha^{n-k})=g_{0}+g_{1}x+\cdots+g_{n-k%
 -1}x^{n-k-1}+x^{n-k}\,.   , i.e.,      n  =   q  -  1       n    q  1     n=q-1     The transmitter sends the     s   (  a  )    =      p  x    (  a  )    ⋅  g    (  a  )          s  a      normal-⋅     subscript  p  x   a   g   a     s(a)=p_{x}(a)\cdot g(a)   coefficients of    𝐂  ′     superscript  𝐂  normal-′    \mathbf{C^{\prime}}   . Thus, in the BCH view of Reed Solomon codes, the set    n  =   q  -  1       n    q  1     n=q-1   of codewords is defined for      𝐂  ′   =   {   (   s  1   ,   s  2   ,  …  ,   s  n   )   |    s   (  a  )    =     ∑   i  =  1   n     s  i    a   i  -  1    is a polynomial that has at least the roots   α  1     ,   α  2   ,  …  ,    α   n  -  k       }    .       superscript  𝐂  normal-′    conditional-set    subscript  s  1    subscript  s  2   normal-…   subscript  s  n        s  a      superscript   subscript     i  1    n      subscript  s  i    superscript  a    i  1    is a polynomial that has at least the roots   superscript  α  1      superscript  α  2   normal-…   superscript  α    n  k         \mathbf{C^{\prime}}=\Big\{\;\big(s_{1},s_{2},\dots,s_{n}\big)\;\Big|\;s(a)=%
 \sum_{i=1}^{n}s_{i}a^{i-1}\text{ is a polynomial that has at least the roots }%
 \alpha^{1},\alpha^{2},\dots,\alpha^{n-k}\;\Big\}\,.   as follows: 4      g   (  a  )       g  a    g(a)   With this definition of the codewords, it can be immediately seen that a Reed–Solomon code is a polynomial code , and in particular a BCH code . The generator polynomial    α  ,   α  2   ,  …  ,   α   n  -  k       α   superscript  α  2   normal-…   superscript  α    n  k      \alpha,\alpha^{2},\ldots,\alpha^{n-k}   is the minimal polynomial with roots    g   (  a  )       g  a    g(a)   as defined above, and the codewords are exactly the polynomials that are divisible by    r   (  a  )       r  a    r(a)   .  Since Reed–Solomon codes are a special case of BCH codes and the Berlekamp–Massey algorithm has been designed for the decoding of such codes, it is applicable to Reed–Solomon codes: The receiver interprets the received word as the coefficients of a polynomial     r   (  a  )    =   s   (  a  )          r  a     s  a     r(a)=s(a)   . If no error has occurred during the transmission, that is, if      p  x    (  a  )    =     r   (  a  )    /  g    (  a  )           subscript  p  x   a         r  a   g   a     p_{x}(a)=r(a)/g(a)   , then the receiver can use polynomial division to determine the message polynomial    p   (  a  )       p  a    p(a)   . In general, the receiver can use polynomial division to construct the unique polynomials    e   (  a  )       e  a    e(a)   and    e   (  a  )       e  a    e(a)   , such that    g   (  a  )       g  a    g(a)   has degree less than the degree of      r   (  a  )    =      p   (  a  )    ⋅  g    (  a  )    +   e   (  a  )      .        r  a        normal-⋅    p  a   g   a     e  a      r(a)=p(a)\cdot g(a)+e(a)\,.   and      e   (  a  )       e  a    e(a)   If the remainder polynomial    r   (  a  )       r  a    r(a)   is not identically zero, then an error has occurred during the transmission. The receiver can evaluate    g   (  a  )       g  a    g(a)   at the roots of    s   (  a  )       s  a    s(a)   and build a system of equations that eliminates    r   (  a  )       r  a    r(a)   and identifies which coefficients of    r   (  a  )       r  a    r(a)   are in error, and the magnitude of each coefficient's error ( and ). If the system of equations can be solved, then the receiver knows how to modify the received word    s   (  a  )       s  a    s(a)   to get the most likely codeword     s   (  x  )    =   p   (  x  )   g   (  x  )          s  x     p  x  g  x     s(x)=p(x)g(x)   that was sent.  Systematic encoding procedure  The above encoding procedure for the BCH view of Reed–Solomon codes is classical, but does not give rise to a systematic encoding procedure , i.e., the codewords do not necessarily contain the message as a subsequence. To remedy this fact, instead of sending    s   (  x  )       s  x    s(x)   , the encoder constructs the transmitted polynomial   k   k   k   such that the coefficients of the    p   (  x  )       p  x    p(x)   largest monomials are equal to the corresponding coefficients of    s   (  x  )       s  x    s(x)   , and the lower-order coefficients of    s   (  x  )       s  x    s(x)   are chosen exactly in such a way that    g   (  x  )       g  x    g(x)   becomes evenly divisible by    p   (  x  )       p  x    p(x)   . Then the coefficients of    s   (  x  )       s  x    s(x)   are a subsequence of the coefficients of    p   (  x  )       p  x    p(x)   . To get a code that is overall systematic, we construct the message polynomial    p   (  x  )       p  x    p(x)   by interpreting the message as the sequence of its coefficients.  Formally, the construction is done by multiplying    x  t     superscript  x  t    x^{t}   by    t  =   n  -  k       t    n  k     t=n-k   to make room for the    g   (  x  )       g  x    g(x)   check symbols, dividing that product by   t   t   t   to find the remainder, and then compensating for that remainder by subtracting it. The     s  r    (  x  )        subscript  s  r   x    s_{r}(x)   check symbols are created by computing the remainder       s  r    (  x  )    =     p   (  x  )    ⋅    x  t     mod   g   (  x  )      .         subscript  s  r   x    modulo   normal-⋅    p  x    superscript  x  t      g  x      s_{r}(x)=p(x)\cdot x^{t}\ \bmod\ g(x)\,.   :      t  -  1      t  1    t-1   Note that the remainder has degree at most     x   t  -  1    ,   x   t  -  2    ,  …  ,   x  1   ,   x  0       superscript  x    t  1     superscript  x    t  2    normal-…   superscript  x  1    superscript  x  0     x^{t-1},x^{t-2},\dots,x^{1},x^{0}   , whereas the coefficients of     p   (  x  )    ⋅   x  t      normal-⋅    p  x    superscript  x  t     p(x)\cdot x^{t}   in the polynomial    s   (  x  )       s  x    s(x)   are zero. Therefore, the following definition of the codeword   k   k   k   has the property that the first    p   (  x  )       p  x    p(x)   coefficients are identical to the coefficients of      s   (  x  )    =     p   (  x  )    ⋅   x  t    -    s  r    (  x  )      .        s  x      normal-⋅    p  x    superscript  x  t       subscript  s  r   x      s(x)=p(x)\cdot x^{t}-s_{r}(x)\,.   :      s   (  x  )       s  x    s(x)   As a result, the codewords    𝐂  ′     superscript  𝐂  normal-′    \mathbf{C^{\prime}}   are indeed elements of    g   (  x  )       g  x    g(x)   , that is, they are evenly divisible by the generator polynomial      s   (  x  )    ≡     p   (  x  )    ⋅   x  t    -    s  r    (  x  )     ≡     s  r    (  x  )    -    s  r    (  x  )     ≡   0  mod   g   (  x  )      .          s  x      normal-⋅    p  x    superscript  x  t       subscript  s  r   x              subscript  s  r   x      subscript  s  r   x          modulo  0    g  x       s(x)\equiv p(x)\cdot x^{t}-s_{r}(x)\equiv s_{r}(x)-s_{r}(x)\equiv 0\mod g(x)\,.   : 5     𝐂   𝐂   \mathbf{C}     Equivalence of the two views  At first sight, the two views of Reed–Solomon codes above seem very different. In the first definition, codewords in the set    𝐂  ′     superscript  𝐂  normal-′    \mathbf{C^{\prime}}   are values of polynomials, whereas in the second set    𝐂  =   𝐂  ′       𝐂   superscript  𝐂  normal-′     \mathbf{C}=\mathbf{C^{\prime}}   , they are coefficients . Moreover, the polynomials in the first definition are required to be of small degree, whereas those in the second definition are required to have specific roots. Yet, it can be shown that the two sets are actually equal, that is,     a  1   ,  …  ,   a  n       subscript  a  1   normal-…   subscript  a  n     a_{1},\dots,a_{n}   holds (for an appropriate choice of    p   (  x  )       p  x    p(x)   ).  The equivalence of the two definitions is proved using the discrete Fourier transform . This transform, which exists in all finite fields as well as the complex numbers, establishes a duality between the coefficients of polynomials and their values. This duality can be approximately summarized as follows: Let    q   (  x  )       q  x    q(x)   and   n   n   n   be two polynomials of degree less than    p   (  x  )       p  x    p(x)   . If the values of    q   (  x  )       q  x    q(x)   are the coefficients of    q   (  x  )       q  x    q(x)   , then (up to a scalar factor and reordering), the values of    p   (  x  )       p  x    p(x)   are the coefficients of    x  =   α  i       x   superscript  α  i     x=\alpha^{i}   . For this to make sense, the values must be taken at locations    i  =   0  ,  …  ,   n  -  1        i   0  normal-…    n  1      i=0,\dots,n-1   , for   α   α   \alpha   , where   n   n   n   is a primitive      p   (  x  )    =    v  0   +    v  1   x   +    v  2    x  2    +  ⋯  +    v   n  -  1     x   n  -  1       ,        p  x      subscript  v  0      subscript  v  1   x      subscript  v  2    superscript  x  2    normal-⋯     subscript  v    n  1     superscript  x    n  1        p(x)=v_{0}+v_{1}x+v_{2}x^{2}+\cdots+v_{n-1}x^{n-1},   th root of unity .  To be more precise, let       q   (  x  )    =    f  0   +    f  1   x   +    f  2    x  2    +  ⋯  +    f   n  -  1     x   n  -  1            q  x      subscript  f  0      subscript  f  1   x      subscript  f  2    superscript  x  2    normal-⋯     subscript  f    n  1     superscript  x    n  1        q(x)=f_{0}+f_{1}x+f_{2}x^{2}+\cdots+f_{n-1}x^{n-1}         p   (  x  )       p  x    p(x)   and assume    q   (  x  )       q  x    q(x)   and    p   (  x  )       p  x    p(x)   are related by the discrete Fourier transform. Then the coefficients and values of    q   (  x  )       q  x    q(x)   and    i  =   0  ,  …  ,   n  -  1        i   0  normal-…    n  1      i=0,\dots,n-1   are related as follows: for all     f  i   =   p   (   α  i   )         subscript  f  i     p   superscript  α  i      f_{i}=p(\alpha^{i})   ,     v  i   =    1  n   q   (   α   n  -  i    )         subscript  v  i       1  n   q   superscript  α    n  i       \textstyle v_{i}=\frac{1}{n}q(\alpha^{n-i})   and    (   f  0   ,  …  ,   f   n  -  1    )      subscript  f  0   normal-…   subscript  f    n  1      (f_{0},\ldots,f_{n-1})   .  Using these facts, we have    p   (  x  )       p  x    p(x)   is a code word of the Reed–Solomon code according to the first definition   if and only if   k   k   k   is of degree less than     f  0   ,  …  ,   f   n  -  1        subscript  f  0   normal-…   subscript  f    n  1      f_{0},\ldots,f_{n-1}   (because    p   (  x  )       p  x    p(x)   are the values of     v  i   =  0       subscript  v  i   0    v_{i}=0   ),  if and only if    i  =   k  ,  …  ,   n  -  1        i   k  normal-…    n  1      i=k,\ldots,n-1   for     q   (   α  i   )    =  0        q   superscript  α  i    0    q(\alpha^{i})=0   ,  if and only if    i  =   1  ,  …  ,   n  -  k        i   1  normal-…    n  k      i=1,\ldots,n-k   for     q   (   α  i   )    =   n   v   n  -  i           q   superscript  α  i      n   subscript  v    n  i       q(\alpha^{i})=nv_{n-i}   (because    (   f  0   ,  …  ,   f   n  -  1    )      subscript  f  0   normal-…   subscript  f    n  1      (f_{0},\ldots,f_{n-1})   ),  if and only if    n  -  k      n  k    n-k   is a code word of the Reed–Solomon code according to the second definition.   This shows that the two definitions are equivalent.  Remarks  Designers are not required to use the "natural" sizes of Reed–Solomon code blocks. A technique known as "shortening" can produce a smaller code of any desired size from a larger code. For example, the widely used (255,223) code can be converted to a (160,128) code by padding the unused portion of the source block with 95 binary zeroes and not transmitting them. At the decoder, the same portion of the block is loaded locally with binary zeroes. The Delsarte-Goethals-Seidel 6 theorem illustrates an example of an application of shortened Reed–Solomon codes. In parallel to shortening, a technique known as puncturing allows omitting some of the encoded parity symbols.  Properties  The Reed–Solomon code is a [ n , k , n − k + 1] code; in other words, it is a linear block code of length n (over F ) with dimension  k and minimum Hamming distance  n − k + 1. The Reed–Solomon code is optimal in the sense that the minimum distance has the maximum value possible for a linear code of size ( n , k ); this is known as the Singleton bound . Such a code is also called a maximum distance separable (MDS) code .  The error-correcting ability of a Reed–Solomon code is determined by its minimum distance, or equivalently, by     (   n  -  k   )   /  2        n  k   2    (n-k)/2   , the measure of redundancy in the block. If the locations of the error symbols are not known in advance, then a Reed–Solomon code can correct up to   E   E   E   erroneous symbols, i.e., it can correct half as many errors as there are redundant symbols added to the block. Sometimes error locations are known in advance (e.g., "side information" in demodulator  signal-to-noise ratios )—these are called erasures . A Reed–Solomon code (like any MDS code ) is able to correct twice as many erasures as errors, and any combination of errors and erasures can be corrected as long as the relation 2 E + S ≤ n − k is satisfied, where   S   S   S   is the number of errors and   F   F   F   is the number of erasures in the block.  For practical uses of Reed–Solomon codes, it is common to use a finite field    2  m     superscript  2  m    2^{m}   with   m   m   m   elements. In this case, each symbol can be represented as an    n  =    2  m   -  1       n     superscript  2  m   1     n=2^{m}-1   -bit value. The sender sends the data points as encoded blocks, and the number of symbols in the encoded block is    n  =    2  8   -  1   =  255        n     superscript  2  8   1        255     n=2^{8}-1=255   . Thus a Reed–Solomon code operating on 8-bit symbols has   k   k   k   symbols per block. (This is a very popular value because of the prevalence of byte-oriented computer systems.) The number    k  <  n      k  n    k   , with    k  =  223      k  223    k=223   , of data symbols in the block is a design parameter. A commonly used code encodes    n  =  255      n  255    n=255   eight-bit data symbols plus 32 eight-bit parity symbols in an     (  n  ,  k  )   =   (  255  ,  223  )        n  k    255  223     (n,k)=(255,223)   -symbol block; this is denoted as a    (  n  ,  k  )     n  k    (n,k)   code, and is capable of correcting up to 16 symbol errors per block.  The Reed–Solomon code properties discussed above make them especially well-suited to applications where errors occur in bursts . This is because it does not matter to the code how many bits in a symbol are in error — if multiple bits in a symbol are corrupted it only counts as a single error. Conversely, if a data stream is not characterized by error bursts or drop-outs but by random single bit errors, a Reed–Solomon code is usually a poor choice compared to a binary code.  The Reed–Solomon code, like the convolutional code , is a transparent code. This means that if the channel symbols have been inverted somewhere along the line, the decoders will still operate. The result will be the inversion of the original data. However, the Reed–Solomon code loses its transparency when the code is shortened. The "missing" bits in a shortened code need to be filled by either zeros or ones, depending on whether the data is complemented or not. (To put it another way, if the symbols are inverted, then the zero-fill needs to be inverted to a one-fill.) For this reason it is mandatory that the sense of the data (i.e., true or complemented) be resolved before Reed–Solomon decoding.  Error correction algorithms  Theoretical decoder  described a theoretical decoder that corrected errors by finding the most popular message polynomial. The decoder for a RS   k   k   k   code would look at all possible subsets of   n   n   n   symbols from the set of   k   k   k   symbols that were received. For the code to be correctable in general, at least   k   k   k   symbols had to be received correctly, and     (      n      k      )   =    n  !      (   n  -  k   )   !    k  !          binomial  n  k       n         n  k      k       \textstyle{\left({{n}\atop{k}}\right)}={n!\over(n-k)!k!}   symbols are needed to interpolate the message polynomial. The decoder would interpolate a message polynomial for each subset, and it would keep track of the resulting polynomial candidates. The most popular message is the corrected result. Unfortunately, there are a lot of subsets, so the algorithm is impractical. The number of subsets is the binomial coefficient ,    (  255  ,  249  )     255  249    (255,249)   , and the number of subsets is infeasible for even modest codes. For a     s   (  x  )    =    ∑   i  =  0    n  -  1      c  i    x  i           s  x     superscript   subscript     i  0      n  1       subscript  c  i    superscript  x  i       s(x)=\sum_{i=0}^{n-1}c_{i}x^{i}   code that can correct 3 errors, the naive theoretical decoder would examine 359 billion subsets. The Reed–Solomon code needed a practical decoder.  Peterson decoder  developed a practical decoder based on syndrome decoding.  Berlekamp (below) would improve on that decoder.  Syndrome decoding  The transmitted message is viewed as the coefficients of a polynomial s ( x ) that is divisible by a generator polynomial g ( x ).        g   (  x  )    =    ∏   j  =  1    n  -  k     (   x  -   α  j    )     ,        g  x     superscript   subscript  product    j  1      n  k      x   superscript  α  j       g(x)=\prod_{j=1}^{n-k}(x-\alpha^{j}),           s   (   α  i   )    =  0   ,   i  =   1  ,  2  ,  …  ,   n  -  k        formulae-sequence      s   superscript  α  i    0     i   1  2  normal-…    n  k       s(\alpha^{i})=0,\ i=1,2,\ldots,n-k     where α is a primitive root.  Since s ( x ) is divisible by generator g ( x ), it follows that       r   (  x  )    =    s   (  x  )    +   e   (  x  )           r  x       s  x     e  x      r(x)=s(x)+e(x)     The transmitted polynomial is corrupted in transit by an error polynomial e ( x ) to produce the received polynomial r ( x ).       e   (  x  )    =    ∑   i  =  0    n  -  1      e  i    x  i           e  x     superscript   subscript     i  0      n  1       subscript  e  i    superscript  x  i       e(x)=\sum_{i=0}^{n-1}e_{i}x^{i}          e   (  x  )    =    ∑   k  =  1   ν     e   i  k     x   i  k            e  x     superscript   subscript     k  1    ν      subscript  e   subscript  i  k     superscript  x   subscript  i  k        e(x)=\sum_{k=1}^{\nu}e_{i_{k}}x^{i_{k}}     where e i is the coefficient for the i -th power of x . Coefficient e i will be zero if there is no error at that power of x and nonzero if there is an error. If there are ν errors at distinct powers i k of x , then      S  j     subscript  S  j    \displaystyle S_{j}     The goal of the decoder is to find the number of errors ( ν ), the positions of the errors ( i k ), and the error values at those positions ( e i k ). From those, e(x) can be calculated and subtracted from r(x) to get the original message s(x).  The syndromes S j are defined as      (   α  j   ,   S  j   )      superscript  α  j    subscript  S  j     (\alpha^{j},S_{j})     The advantage of looking at the syndromes is that the message polynomial drops out. Another possible way of calculating e(x) is using polynomial interpolation to find the only polynomial that passes through the points     S  j   =   e   (   α  j   )         subscript  S  j     e   superscript  α  j      S_{j}=e(\alpha^{j})   (Because      X  k   =   α   i  k     ,    Y  k   =   e   i  k        formulae-sequence     subscript  X  k    superscript  α   subscript  i  k        subscript  Y  k    subscript  e   subscript  i  k       X_{k}=\alpha^{i_{k}},\ Y_{k}=e_{i_{k}}   ), however, this is not used widely because polynomial interpolation is not always feasible in the fields used in Reed–Solomon error correction. For example, it is feasible over the integers (of course), but it is infeasible over the integers modulo a prime.  Error locators and error values  For convenience, define the error locators  X k and error values  Y k as:       S  j   =    ∑   k  =  1   ν     Y  k    X  k  j          subscript  S  j     superscript   subscript     k  1    ν      subscript  Y  k    superscript   subscript  X  k   j       S_{j}=\sum_{k=1}^{\nu}Y_{k}X_{k}^{j}     Then the syndromes can be written in terms of the error locators and error values as        [      X  1  1      X  2  1     ⋯     X  ν  1        X  1  2      X  2  2     ⋯     X  ν  2       ⋮    ⋮       ⋮       X  1   n  -  k       X  2   n  -  k      ⋯     X  ν   n  -  k       ]    [      Y  1        Y  2       ⋮       Y  ν      ]    =   [      S  1        S  2       ⋮       S   n  -  k       ]            superscript   subscript  X  1   1    superscript   subscript  X  2   1   normal-⋯   superscript   subscript  X  ν   1      superscript   subscript  X  1   2    superscript   subscript  X  2   2   normal-⋯   superscript   subscript  X  ν   2     normal-⋮  normal-⋮  absent  normal-⋮     superscript   subscript  X  1     n  k     superscript   subscript  X  2     n  k    normal-⋯   superscript   subscript  X  ν     n  k         subscript  Y  1      subscript  Y  2     normal-⋮     subscript  Y  ν         subscript  S  1      subscript  S  2     normal-⋮     subscript  S    n  k        \begin{bmatrix}X_{1}^{1}&X_{2}^{1}&\cdots&X_{\nu}^{1}\\
 X_{1}^{2}&X_{2}^{2}&\cdots&X_{\nu}^{2}\\
 \vdots&\vdots&&\vdots\\
 X_{1}^{n-k}&X_{2}^{n-k}&\cdots&X_{\nu}^{n-k}\\
 \end{bmatrix}\begin{bmatrix}Y_{1}\\
 Y_{2}\\
 \vdots\\
 Y_{\nu}\end{bmatrix}=\begin{bmatrix}S_{1}\\
 S_{2}\\
 \vdots\\
 S_{n-k}\end{bmatrix}     The syndromes give a system of n − k ≥ 2 ν equations in 2 ν unknowns, but that system of equations is nonlinear in the X k and does not have an obvious solution. However, if the X k were known (see below), then the syndrome equations provide a linear system of equations that can easily be solved for the Y k error values.       Λ   (  x  )    =    ∏   k  =  1   ν    (   1  -   x   X  k     )    =   1  +    Λ  1    x  1    +    Λ  2    x  2    +  ⋯  +    Λ  ν    x  ν             normal-Λ  x     superscript   subscript  product    k  1    ν     1    x   subscript  X  k             1     subscript  normal-Λ  1    superscript  x  1       subscript  normal-Λ  2    superscript  x  2    normal-⋯     subscript  normal-Λ  ν    superscript  x  ν        \Lambda(x)=\prod_{k=1}^{\nu}(1-xX_{k})=1+\Lambda_{1}x^{1}+\Lambda_{2}x^{2}+%
 \cdots+\Lambda_{\nu}x^{\nu}     Consequently, the problem is finding the X k , because then the leftmost matrix would be known, and both sides of the equation could be multiplied by its inverse, yielding Y k  Error locator polynomial  Peterson found a linear recurrence relation that gave rise to a system of linear equations.  Solving those equations identifies the error locations.  Define the error locator polynomial Λ( x ) as      X  k   -  1      superscript   subscript  X  k     1     X_{k}^{-1}     The zeros of Λ( x ) are the reciprocals     Λ   (   X  k   -  1    )    =  0        normal-Λ   superscript   subscript  X  k     1     0    \Lambda(X_{k}^{-1})=0   :       Λ   (   X  k   -  1    )    =   1  +    Λ  1    X  k   -  1     +    Λ  2    X  k   -  2     +  ⋯  +    Λ  ν    X  k   -  ν      =  0          normal-Λ   superscript   subscript  X  k     1       1     subscript  normal-Λ  1    superscript   subscript  X  k     1        subscript  normal-Λ  2    superscript   subscript  X  k     2     normal-⋯     subscript  normal-Λ  ν    superscript   subscript  X  k     ν           0     \Lambda(X_{k}^{-1})=1+\Lambda_{1}X_{k}^{-1}+\Lambda_{2}X_{k}^{-2}+\cdots+%
 \Lambda_{\nu}X_{k}^{-\nu}=0          Y  k    X  k   j  +  ν         subscript  Y  k    superscript   subscript  X  k     j  ν      Y_{k}X_{k}^{j+\nu}     Multiply both sides by      Y  k    X  k   j  +  ν    Λ   (   X  k   -  1    )    =  0.         subscript  Y  k    superscript   subscript  X  k     j  ν    normal-Λ   superscript   subscript  X  k     1     0.    \displaystyle Y_{k}X_{k}^{j+\nu}\Lambda(X_{k}^{-1})=0.   and it will still be zero. j is any number such that 1≤j≤v.         ∑   k  =  1   ν     (     Y  k    X  k   j  +  ν     +    Λ  1    Y  k    X  k    j  +  ν   -  1     +    Λ  2    Y  k    X  k    j  +  ν   -  2     +  ⋯  +    Λ  ν    Y  k    X  k  j     )    =  0        superscript   subscript     k  1    ν        subscript  Y  k    superscript   subscript  X  k     j  ν        subscript  normal-Λ  1    subscript  Y  k    superscript   subscript  X  k       j  ν   1        subscript  normal-Λ  2    subscript  Y  k    superscript   subscript  X  k       j  ν   2     normal-⋯     subscript  normal-Λ  ν    subscript  Y  k    superscript   subscript  X  k   j      0    \displaystyle\sum_{k=1}^{\nu}(Y_{k}X_{k}^{j+\nu}+\Lambda_{1}Y_{k}X_{k}^{j+\nu-%
 1}+\Lambda_{2}Y_{k}X_{k}^{j+\nu-2}+\cdots+\Lambda_{\nu}Y_{k}X_{k}^{j})=0     Sum for k = 1 to ν        S   j  +  ν    +    Λ  1    S    j  +  ν   -  1     +  ⋯  +    Λ   ν  -  1     S   j  +  1     +    Λ  ν    S  j     =   0          subscript  S    j  ν       subscript  normal-Λ  1    subscript  S      j  ν   1     normal-⋯     subscript  normal-Λ    ν  1     subscript  S    j  1        subscript  normal-Λ  ν    subscript  S  j     0    S_{j+\nu}+\Lambda_{1}S_{j+\nu-1}+\cdots+\Lambda_{\nu-1}S_{j+1}+\Lambda_{\nu}S_%
 {j}=0\,     This reduces to         S  j    Λ  ν    +    S   j  +  1     Λ   ν  -  1     +  ⋯  +    S    j  +  ν   -  1     Λ  1     =   -   S   j  +  ν              subscript  S  j    subscript  normal-Λ  ν       subscript  S    j  1     subscript  normal-Λ    ν  1     normal-⋯     subscript  S      j  ν   1     subscript  normal-Λ  1        subscript  S    j  ν       S_{j}\Lambda_{\nu}+S_{j+1}\Lambda_{\nu-1}+\cdots+S_{j+\nu-1}\Lambda_{1}=-S_{j+\nu}           [      S  1      S  2     ⋯     S  ν        S  2      S  3     ⋯     S   ν  +  1        ⋮    ⋮       ⋮       S  ν      S   ν  +  1      ⋯     S    2  ν   -  1       ]    [      Λ  ν        Λ   ν  -  1        ⋮       Λ  1      ]    =   [      -   S   ν  +  1          -   S   ν  +  2         ⋮       -   S   ν  +  ν        ]            subscript  S  1    subscript  S  2   normal-⋯   subscript  S  ν      subscript  S  2    subscript  S  3   normal-⋯   subscript  S    ν  1      normal-⋮  normal-⋮  absent  normal-⋮     subscript  S  ν    subscript  S    ν  1    normal-⋯   subscript  S      2  ν   1         subscript  normal-Λ  ν      subscript  normal-Λ    ν  1      normal-⋮     subscript  normal-Λ  1           subscript  S    ν  1          subscript  S    ν  2       normal-⋮       subscript  S    ν  ν         \begin{bmatrix}S_{1}&S_{2}&\cdots&S_{\nu}\\
 S_{2}&S_{3}&\cdots&S_{\nu+1}\\
 \vdots&\vdots&&\vdots\\
 S_{\nu}&S_{\nu+1}&\cdots&S_{2\nu-1}\end{bmatrix}\begin{bmatrix}\Lambda_{\nu}\\
 \Lambda_{\nu-1}\\
 \vdots\\
 \Lambda_{1}\end{bmatrix}=\begin{bmatrix}-S_{\nu+1}\\
 -S_{\nu+2}\\
 \vdots\\
 -S_{\nu+\nu}\end{bmatrix}     This yields a system of linear equations that can be solved for the coefficients Λ i of the error location polynomial:      Δ  =    S  i   +     Λ  1     S   i  -  1     +  ⋯  +     Λ  e     S   i  -  e          normal-Δ     subscript  S  i      subscript  normal-Λ  1    subscript  S    i  1     normal-⋯     subscript  normal-Λ  e    subscript  S    i  e        \Delta=S_{i}+\Lambda_{1}\ S_{i-1}+\cdots+\Lambda_{e}\ S_{i-e}   The above assumes the decoder knows the number of errors ν , but that number has not been determined yet. The PGZ decoder does not determine ν directly but rather searches for it by trying successive values. The decoder first assumes the largest value for a trial ν and sets up the linear system for that value. If the equations can be solved (i.e., the matrix determinant is nonzero), then that trial value is the number of errors. If the linear system cannot be solved, then the trial ν is reduced by one and the next smaller system is examined.  Obtain the error locators from the error locator polynomial  Use the coefficients Λ i found in the last step to build the error location polynomial. The roots of the error location polynomial can be found by exhaustive search. The error locators are the reciprocals of those roots. Chien search is an efficient implementation of this step.  Calculate the error locations  Calculate i k by taking the log base a of X k . This is generally done using a precomputed lookup table.  Calculate the error values  Once the error locators are known, the error values can be determined. This can be done by direct solution for Y k in the error equations given above, or using the Forney algorithm .  Fix the errors  Finally, e(x) is generated from i k and e i k and then is subtracted from r(x) to get the sent message s(x).  Berlekamp–Massey decoder  The Berlekamp–Massey algorithm is an alternate iterative procedure for finding the error locator polynomial. During each iteration, it calculates a discrepancy based on a current instance of Λ(x) with an assumed number of errors e :      G  F   (  929  )       G  F  929    GF(929)     and then adjusts Λ(x) and e so that a recalculated Δ would be zero. The article Berlekamp–Massey algorithm has a detailed description of the procedure. In the following example, C(x) is used to represent Λ(x).  Example  Consider the Reed–Solomon code defined in    α  =  3      α  3    α=3   with    t  =  4      t  4    t=4   and     g   (  x  )    =    (   x  -  3   )    (   x  -   3  2    )    (   x  -   3  3    )    (   x  -   3  4    )    =    x  4   +   809   x  3    +   723   x  2    +   568  x   +  522           g  x       x  3     x   superscript  3  2      x   superscript  3  3      x   superscript  3  4             superscript  x  4     809   superscript  x  3      723   superscript  x  2      568  x   522      g(x)=(x-3)(x-3^{2})(x-3^{3})(x-3^{4})=x^{4}+809x^{3}+723x^{2}+568x+522   (this is used in PDF417 barcodes). The generator polynomial is        s  r    (  x  )    =    p   (  x  )    x  t    mod   g   (  x  )     =    547   x  3    +   738   x  2    +   442  x   +  455            subscript  s  r   x    modulo    p  x   superscript  x  t      g  x             547   superscript  x  3      738   superscript  x  2      442  x   455      s_{r}(x)=p(x)\,x^{t}\mod g(x)=547x^{3}+738x^{2}+442x+455   If the message polynomial is 3 x 2 + 2 x + 1}} , then the codeword is calculated as follows.       s   (  x  )    =    p   (  x  )    x  t    -    s  r    (  x  )     =    3   x  6    +   2   x  5    +   1   x  4    +   382   x  3    +   191   x  2    +   487  x   +  474           s  x       p  x   superscript  x  t       subscript  s  r   x             3   superscript  x  6      2   superscript  x  5      1   superscript  x  4      382   superscript  x  3      191   superscript  x  2      487  x   474      s(x)=p(x)\,x^{t}-s_{r}(x)=3x^{6}+2x^{5}+1x^{4}+382x^{3}+191x^{2}+487x+474          r   (  x  )    =    s   (  x  )    +   e   (  x  )     =    3   x  6    +   2   x  5    +   123   x  4    +   456   x  3    +   191   x  2    +   487  x   +  474           r  x       s  x     e  x             3   superscript  x  6      2   superscript  x  5      123   superscript  x  4      456   superscript  x  3      191   superscript  x  2      487  x   474      r(x)=s(x)+e(x)=3x^{6}+2x^{5}+123x^{4}+456x^{3}+191x^{2}+487x+474   Errors in transmission might cause this to be received instead.       S  1   =   r   (   3  1   )    =    3  ⋅   3  6    +   2  ⋅   3  5    +   123  ⋅   3  4    +   456  ⋅   3  3    +   191  ⋅   3  2    +   487  ⋅  3   +  474   =  732         subscript  S  1     r   superscript  3  1            normal-⋅  3   superscript  3  6     normal-⋅  2   superscript  3  5     normal-⋅  123   superscript  3  4     normal-⋅  456   superscript  3  3     normal-⋅  191   superscript  3  2     normal-⋅  487  3   474        732     S_{1}=r(3^{1})=3\cdot 3^{6}+2\cdot 3^{5}+123\cdot 3^{4}+456\cdot 3^{3}+191%
 \cdot 3^{2}+487\cdot 3+474=732   The syndromes are calculated by evaluating r at powers of α .        S  2   =   r   (   3  2   )    =  637   ,    S  3   =   r   (   3  3   )    =  762   ,    S  4   =   r   (   3  4   )    =  925      formulae-sequence       subscript  S  2     r   superscript  3  2         637         subscript  S  3     r   superscript  3  3         762         subscript  S  4     r   superscript  3  4         925      S_{2}=r(3^{2})=637,\;S_{3}=r(3^{3})=762,\;S_{4}=r(3^{4})=925          Ω   (  x  )    =    S   (  x  )   Λ   (  x  )    mod   x  4    =    546  x   +   732            normal-Ω  x    modulo    S  x  normal-Λ  x    superscript  x  4             546  x   732      \Omega(x)=S(x)\Lambda(x)\mod x^{4}=546x+732\,   To correct the errors, first use the Berlekamp–Massey algorithm to calculate the error locator polynomial.      n   S n +1   d   C   B   b   m       0   732   732   197 x + 1   1   732   1     1   637   846   173 x + 1   1   732   2     2   762   412   634 x 2 + 173 x + 1   173 x + 1   412   1     3   925   576   329 x 2 + 821 x + 1   173 x + 1   412   2     The final value of C is the error locator polynomial, Λ( x ). The zeros can be found by trial substitution. They are x 1 = 757 = 3 −3 and x 2 = 562 = 3 −4 , corresponding to the error locations. To calculate the error values, apply the Forney algorithm .        Λ  ′    (  x  )    =    658  x   +   821           superscript  normal-Λ  normal-′   x       658  x   821     \Lambda^{\prime}(x)=658x+821\,          e  1   =   -     Ω   (   x  1   )    /   Λ  ′     (   x  1   )     =   -   649  /  54    =   280  ×  843   =   74          subscript  e  1           normal-Ω   subscript  x  1     superscript  normal-Λ  normal-′     subscript  x  1              649  54           280  843        74     e_{1}=-\Omega(x_{1})/\Lambda^{\prime}(x_{1})=-649/54=280\times 843=74\,          e  2   =   -     Ω   (   x  2   )    /   Λ  ′     (   x  2   )     =   122          subscript  e  2           normal-Ω   subscript  x  2     superscript  normal-Λ  normal-′     subscript  x  2          122     e_{2}=-\Omega(x_{2})/\Lambda^{\prime}(x_{2})=122\,        k   k   k   Subtracting e 1 x 3 and e 2 x 4 from the received polynomial r reproduces the original codeword s .  Euclidean decoder  Another iterative method for calculating the error locator polynomial is based on the Euclidean algorithm   t = number of parities  R 0 = x t   S 0 = syndrome polynomial  A 0 = 1  B 0 = 0  i = 0  while degree of S i ≥ ( t /2)  Q = R i / S i   S i +1 = R i – Q S i = R i modulo S i   A i +1 = Q A i + B i   R i +1 = S i   B i +1 = A i   i = i + 1    Λ( x ) = A i / A i (0)  Ω( x ) = (–1) deg A i  S i / A i (0)   A i (0) is the constant (least significant) term of A i .  Here is an example of the Euclidean method, using the same data as the Berlekamp Massey example above. In the table below, R and S are forward, A and B are reversed.      i   R i   A i   S i   B i       0   001 x 4 + 000 x 3 + 000 x 2 + 000 x + 000   001   925 x 3 + 762 x 2 + 637 x + 732   000     1   925 x 3 + 762 x 2 + 637 x + 732   533 + 232 x   683 x 2 + 676 x + 024   001     2   683 x 2 + 676 x + 024   544 + 704 x + 608 x 2   673 x + 596   533 + 232 x        Λ( x ) = A 2 / 544 = 001 + 821 x + 329 x 2   Ω( x ) = (–1) 2  S 2 / 544 = 546 x + 732   Decoding in frequency domain (sketch)  The above algorithms are presented in the time domain . Decoding in the frequency domain , using Fourier transform techniques, can offer computational and implementation advantages.  The following is a sketch of the main idea behind this error correction technique.  By definition, a code word of a Reed–Solomon code is given by the sequence of values of a low-degree polynomial over a finite field . A key fact for the error correction algorithm is that the values and the coefficients of a polynomial are related by the discrete Fourier transform .  The purpose of a Fourier transform is to convert a signal from a time domain to a frequency domain or vice versa. In case of the Fourier transform over a finite field , the frequency domain signal corresponds to the coefficients of a polynomial, and the time domain signal correspond to the values of the same polynomial.  As shown in Figures 1 and 2, an isolated value in the frequency domain corresponds to a smooth wave in the time domain. The wavelength depends on the location of the isolated value.  Conversely, as shown in Figures 3 and 4, an isolated value in the time domain corresponds to a smooth wave in the frequency domain.  In a Reed–Solomon code, the frequency domain is divided into two regions as shown in Figure 5: a left (low-frequency) region of length    n  -  k      n  k    n-k   , and a right (high-frequency) region of length   k   k   k   . A data word is then embedded into the left region (corresponding to the    k  -  1      k  1    k-1   coefficients of a polynomial of degree at most    n  -  k      n  k    n-k   ), while the right region is filled with zeros. The result is Fourier transformed into the time domain, yielding a code word that is composed only of low frequencies. In the absence of errors, a code word can be decoded by reverse Fourier transforming it back into the frequency domain.  Now consider a code word containing a single error, as shown in red in Figure 6. The effect of this error in the frequency domain is a smooth, single-frequency wave in the right region, called the syndrome of the error. The error location can be determined by determining the frequency of the syndrome signal.  Similarly, if two or more errors are introduced in the code word, the syndrome will be a signal composed of two or more frequencies, as shown in Figure 7. As long as it is possible to determine the frequencies of which the syndrome is composed, it is possible to determine the error locations. Notice that the error locations depend only on the frequencies of these waves, whereas the error magnitudes depend on their amplitudes and phase.  The problem of determining the error locations has therefore been reduced to the problem of finding, given a sequence of    G  F   (   2  m   )       G  F   superscript  2  m     GF(2^{m})   values, the smallest set of elementary waves into which these values can be decomposed. It is known from digital signal processing that this problem is equivalent to finding the roots of the minimal polynomial of the sequence, or equivalently, of finding the shortest linear feedback shift register (LFSR) for the sequence. The latter problem can either be solved inefficiently by solving a system of linear equations, or more efficiently by the Berlekamp–Massey algorithm .  Decoding beyond the error-correction bound  The Singleton bound states that the minimum distance d of a linear block code of size ( n , k ) is upper-bounded by n − k + 1. The distance d was usually understood to limit the error-correction capability to ⌊ d /2⌋. The Reed–Solomon code achieves this bound with equality, and can thus correct up to ⌊( n − k + 1)/2⌋ errors. However, this error-correction bound is not exact.  In 1999, Madhu Sudan and Venkatesan Guruswami at MIT published "Improved Decoding of Reed–Solomon and Algebraic-Geometry Codes" introducing an algorithm that allowed for the correction of errors beyond half the minimum distance of the code. 7 It applies to Reed–Solomon codes and more generally to algebraic geometric codes . This algorithm produces a list of codewords (it is a list-decoding algorithm) and is based on interpolation and factorization of polynomials over $GF(2^m)$ and its extensions.  Soft-decoding  The algebraic decoding methods described above are hard-decision methods, which means that for every symbol a hard decision is made about its value. For example, a decoder could associate with each symbol an additional value corresponding to the channel demodulator 's confidence in the correctness of the symbol. The advent of LDPC and turbo codes , which employ iterated soft-decision belief propagation decoding methods to achieve error-correction performance close to the theoretical limit , has spurred interest in applying soft-decision decoding to conventional algebraic codes. In 2003, Ralf Koetter and Alexander Vardy presented a polynomial-time soft-decision algebraic list-decoding algorithm for Reed–Solomon codes, which was based upon the work by Sudan and Guruswami. 8  See also   BCH code  Cyclic code  Chien search  Berlekamp–Massey algorithm  Forward error correction  Berlekamp–Welch algorithm  Folded Reed–Solomon code   Notes  References                  External links  Information and Tutorials   Introduction to Reed-Solomon codes: principles, architecture and implementation (CMU)  A Tutorial on Reed–Solomon Coding for Fault-Tolerance in RAID-like Systems  Algebraic soft-decoding of Reed–Solomon codes  Wikiversity: Reed–Solomon codes for coders  BBC R&D; White Paper WHP031    Code   Schifra Open Source C++ Reed–Solomon Codec  Henry Minsky's RSCode library, Reed–Solomon encoder/decoder  Open Source C++ Reed-Solomon Soft Decoding library  Matlab implementation of errors and-erasures Reed–Solomon decoding  Pure-Python implementation of a Reed–Solomon codec   "  Category:Error detection and correction  Category:Coding theory     ↩  ↩  Not quite true. See remarks below. ↩  ↩  See , for example. ↩  . Explains the Delsarte-Goethals-Seidel theorem as used in the context of the error correcting code for compact disc . ↩  ↩  ↩     