   Binary entropy function      Binary entropy function   (Figure)  Entropy of a Bernoulli trial as a function of success probability, called the binary entropy function .   In information theory , the binary entropy function , denoted    H   (  p  )       H  p    H(p)\,   or     H  b    (  p  )        subscript  H  normal-b   p    H_{\mathrm{b}}(p)\,   , is defined as the entropy of a Bernoulli process with probability of success p . Mathematically, the Bernoulli trial is modelled as a random variable  X that can take on only two values: 0 and 1. The event    X  =  1      X  1    X=1   is considered a success and the event    X  =  0      X  0    X=0   is considered a failure. (These two events are mutually exclusive and exhaustive.)  If    Pr   (  X  =  1  )   =  p  ,     fragments  Pr   fragments  normal-(  X   1  normal-)    p  normal-,    \mathrm{Pr}(X=1)=p,   then    Pr   (  X  =  0  )   =  1  -  p     fragments  Pr   fragments  normal-(  X   0  normal-)    1   p    \mathrm{Pr}(X=0)=1-p   and the entropy of X (in shannons ) is given by        H   (  X  )    =    H  b    (  p  )    =    -   p    log  2   p     -    (   1  -  p   )     log  2    (   1  -  p   )       .          H  X      subscript  H  normal-b   p              p    subscript   2   p         1  p     subscript   2     1  p         H(X)=H_{\mathrm{b}}(p)=-p\log_{2}p-(1-p)\log_{2}(1-p).\,     where    0    log  2   0       0    subscript   2   0     0\log_{2}0   is taken to be 0. The logarithms in this formula are usually taken (as shown in the graph) to the base 2. See binary logarithm .  When     p  =   1  2    ,      p    1  2     p=\frac{1}{2},   the binary entropy function attains its maximum value. This is the case of the unbiased bit , the most common unit of information entropy .      H   (  p  )       H  p    H(p)   is distinguished from the entropy function     H   (  X  )       H  X    H(X)   in that the former takes a single real number as a parameter whereas the latter takes a distribution or random variables as a parameter. Sometimes the binary entropy function is also written as     H  2    (  p  )        subscript  H  2   p    H_{2}(p)   . However, it is different from and should not be confused with the Rényi entropy , which is denoted as     H  2    (  X  )        subscript  H  2   X    H_{2}(X)   .  Explanation  In terms of information theory, entropy is considered to be a measure of the uncertainty in a message. To put it intuitively, suppose    p  =  0      p  0    p=0   . At this probability, the event is certain never to occur, and so there is no uncertainty at all, leading to an entropy of 0. If    p  =  1      p  1    p=1   , the result is again certain, so the entropy is 0 here as well. When    p  =   1  /  2       p    1  2     p=1/2   , the uncertainty is at a maximum; if one were to place a fair bet on the outcome in this case, there is no advantage to be gained with prior knowledge of the probabilities. In this case, the entropy is maximum at a value of 1 bit. Intermediate values fall between these cases; for instance, if    p  =   1  /  4       p    1  4     p=1/4   , there is still a measure of uncertainty on the outcome, but one can still predict the outcome correctly more often than not, so the uncertainty measure, or entropy, is less than 1 full bit.  Derivative  The derivative of the binary entropy function may be expressed as the negative of the logit function:         d   d  p     H  b    (  p  )    =   -    logit  2    (  p  )     =   -    log  2    (   p   1  -  p    )      .            d    d  p     subscript  H  normal-b   p       subscript  logit  2   p             subscript   2     p    1  p         {d\over dp}H_{\mathrm{b}}(p)=-\operatorname{logit}_{2}(p)=-\log_{2}\left(\frac%
 {p}{1-p}\right).\,     Taylor series  The Taylor series of the binary entropy function in a neighborhood of 1/2 is        H  b    (  p  )    =   1  -    1   2   ln  2       ∑   n  =  1    \infin       (   1  -   2  p    )    2  n     n   (    2  n   -  1   )               subscript  H  normal-b   p     1      1    2    2       subscript   superscript   \infin     n  1       superscript    1    2  p      2  n      n      2  n   1          H_{\mathrm{b}}(p)=1-\frac{1}{2\ln 2}\sum^{\infin}_{n=1}\frac{(1-2p)^{2n}}{n(2n%
 -1)}   for    0  ≤  p  ≤  1        0  p       1     0\leq p\leq 1   .  See also   Metric entropy  Information theory  Information entropy   References   David J. C. MacKay. Information Theory, Inference, and Learning Algorithms Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1   External links  "  Category:Entropy and information   