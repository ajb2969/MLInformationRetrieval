   Mean field particle methods      Mean field particle methods   Mean field particle methods are a broad class of interacting type  Monte Carlo algorithms for simulating from a sequence of probability distributions satisfying a nonlinear evolution equation 1 2 3 4 These flows of probability measures can always be interpreted as the distributions of the random states of a Markov process whose transition probabilities depends on the distributions of the current random states. 5 6 A natural way to simulate these sophisticated nonlinear Markov processes is to sample a large number of copies of the process, replacing in the evolution equation the unknown distributions of the random states by the sampled empirical measures . In contrast with traditional Monte Carlo and Markov chain Monte Carlo methodologies these mean field particle techniques rely on sequential interacting samples . The terminology mean field reflects the fact that each of the samples (a.k.a. particles, individuals, walkers, agents, creatures, or phenotypes) interacts with the empirical measures of the process. When the size of the system tends to infinity, these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain, so that the statistical interaction between particles vanishes. In other words, starting with a chaotic configuration based on independent copies of initial state of the nonlinear Markov chain model, the chaos propagates at any time horizon as the size the system tends to infinity; that is, finite blocks of particles reduces to independent copies of the nonlinear Markov process. This result is called the propagation of chaos property. 7 8 9 The terminology "propagation of chaos" originated with the work of Mark Kac in 1976 on a colliding mean field kinetic gas model 10  History  The theory of mean field interacting particle models had certainly started by the mid-1960s, with the work of Henry P. McKean Jr. on Markov interpretations of a class of nonlinear parabolic partial differential equations arising in fluid mechanics. 11 12 The mathematical foundations of these classes of models were developed from the mid-1980s to the mid-1990s by several mathematicians, including Werner Braun, Klaus Hepp, 13 Karl Oelschläger, 14 15 16 Gérard Ben Arous and Marc Brunaud, 17 Donald Dawson, Jean Vaillancourt 18 and Jürgen Gärtner, 19 20 Christian Léonard, 21 Sylvie Méléard, Sylvie Roelly, 22  Alain-Sol Sznitman 23 24 and Hiroshi Tanaka 25 for diffusion type models; F. Alberto Grünbaum, 26 Tokuzo Shiga, Hiroshi Tanaka, 27 Sylvie Méléard and Carl Graham 28 29 30 for general classes of interacting jump-diffusion processes.  We also quote an earlier pioneering article by Theodore E. Harris and Herman Kahn, published in 1951, using mean field but heuristic-like genetic methods for estimating particle transmission energies. 31 Mean field genetic type particle methodologies are also used as heuristic natural search algorithms (a.k.a. metaheuristic ) in evolutionary computing. The origins of these mean field computational techniques can be traced to 1950 and 1954 with the work of Alan Turing on genetic type mutation-selection learning machines 32 and the articles by Nils Aall Barricelli at the Institute for Advanced Study in Princeton, New Jersey . 33 34 The Australian geneticist Alex Fraser also published in 1957 a series of papers on the genetic type simulation of artificial selection of organisms. 35  Quantum Monte Carlo , and more specifically Diffusion Monte Carlo methods can also be interpreted as a mean field particle approximation of Feynman-Kac path integrals. 36 37 38 39 40 41 42 The origins of Quantum Monte Carlo methods are often attributed to Enrico Fermi and Robert Richtmyer who developed in 1948 a mean field particle interpretation of neutron-chain reactions, 43 but the first heuristic-like and genetic type particle algorithm (a.k.a. Resampled or Reconfiguration Monte Carlo methods) for estimating ground state energies of quantum systems (in reduced matrix models) is due to Jack H. Hetherington in 1984 44 In molecular chemistry, the use of genetic heuristic-like particle methodologies (a.k.a. pruning and enrichment strategies) can be traced back to 1955 with the seminal work of Marshall. N. Rosenbluth and Arianna. W. Rosenbluth. 45  The first pioneering articles on the applications of these heuristic-like particle methodologies in nonlinear filtering problems were the independent studies of Neil Gordon, David Salmon and Adrian Smith (bootstrap filter), 46 Genshiro Kitagawa (Monte Carlo filter) , 47 and the one by Himilcon Carvalho, Pierre Del Moral, André Monin and Gérard Salut 48 published in the 1990s. The term interacting "particle filters" was first coined in 1996 by Del Moral. 49 Particle filters were also developed in signal processing in the early 1989-1992 by P. Del Moral, J.C. Noyer, G. Rigal, and G. Salut in the LAAS-CNRS in a series of restricted and classified research reports with STCAN (Service Technique des Constructions et Armes Navales), the IT company DIGILOG, and the LAAS-CNRS (the Laboratory for Analysis and Architecture of Systems) on RADAR/SONAR and GPS signal processing problems. 50 51 52 53 54 55  The foundations and the first rigorous analysis on the convergence of genetic type models and mean field Feynman-Kac particle methodologies are due to Pierre Del Moral 56 57 in 1996. Branching type particle methodologies with varying population sizes were also developed in the end of the 1990s by Dan Crisan, Jessica Gaines and Terry Lyons, 58 59 60 and by Dan Crisan, Pierre Del Moral and Terry Lyons. 61 The first uniform convergence results with respect to the time parameter for mean field particle models were developed in the end of the 1990s by Pierre Del Moral and Alice Guionnet 62 63 for interacting jump type processes, and by Florent Malrieu for nonlinear diffusion type processes. 64  New classes of mean field particle simulation techniques for Feynman-Kac path-integration problems includes genealogical tree based models, 65 66 67 backward particle models, 68 69 adaptive mean field particle models, 70 island type particle models, 71 72 and particle Markov chain Monte Carlo methodologies 73 74  Applications  In physics , and more particularly in statistical mechanics , these nonlinear evolution equations are often used to describe the statistical behavior of microscopic interacting particles in a fluid or in some condensed matter. In this context, the random evolution of a virtual fluid or a gas particle is represented by McKean-Vlasov diffusion processes , reaction–diffusion systems , or Boltzmann type collision processes . 75 76 77 78 79 As its name indicates, the mean field particle model represents the collective behavior of microscopic particles weakly interacting with their occupation measures. The macroscopic behavior of these many-body particle systems is encapsulated in the limiting model obtained when the size of the population tends to infinity. Boltzmann equations represent the macroscopic evolution of colliding particles in rarefied gases, while McKean Vlasov diffusions represent the macroscopic behavior of fluid particles and granular gases.  In computational physics and more specifically in quantum mechanics , the ground state energies of quantum systems is associated with the top of the spectrum of Schrödinger's operators. The Schrödinger equation is the quantum mechanics version of the Newton's second law of motion of classical mechanics (the mass times the acceleration is the sum of the forces). This equation represents the wave function (a.k.a. the quantum state) evolution of some physical system, including molecular, atomic of subatomic systems, as well as macroscopic systems like the universe. 80 The solution of the imaginary time Schrödinger equation (a.k.a. the heat equation) is given by a Feynman-Kac distribution associated with a free evolution Markov process (often represented by Brownian motions) in the set of electronic or macromolecular configurations and some potential energy function. The long time behavior of these nonlinear semigroups is related to top eigenvalues and ground state energies of Schrödinger's operators. 81 82 83 84 85 86 The genetic type mean field interpretation of these Feynman-Kac models are termed Resample Monte Carlo, or Diffusion Monte Carlo methods. These branching type evolutionary algorithms are based on mutation and selection transitions. During the mutation transition, the walkers evolve randomly and independently in a potential energy landscape on particle configurations. The mean field selection process (a.k.a. quantum teleportation, population reconfiguration, resampled transition) is associated with a fitness function that reflects the particle absorption in an energy well. Configurations with low relative energy are more likely to duplicate. In molecular chemistry, and statistical physics Mean field particle methodologies are also used to sample Boltzmann-Gibbs measures associated with some cooling schedule, and to compute their normalizing constants (a.k.a. free energies, or partition functions). 87 88 89 90  In computational biology , and more specifically in population genetics , spatial branching processes with competitive selection and migration mechanisms can also represented by mean field genetic type population dynamics models . 91 92 The first moments of the occupation measures of a spatial branching process are given by Feynman-Kac distribution flows. 93 94 The mean field genetic type approximation of these flows offers a fixed population size interpretation of these branching processes. 95 96 97 Extinction probabilities can be interpreted as absorption probabilities of some Markov process evolving in some absorbing environment. These absorption models are represented by Feynman-Kac models. 98 99 100 101 The long time behavior of these processes conditioned on non-extinction can be expressed in an equivalent way by quasi-invariant measures , Yaglom limits, 102 or invariant measures of nonlinear normalized Feynman-Kac flows. 103 104 105 106 107 108  In computer sciences , and more particularly in artificial intelligence these mean field type genetic algorithms are used as random search heuristics that mimic the process of evolution to generate useful solutions to complex optimization problems. 109 110 111 These stochastic search algorithms belongs to the class of Evolutionary models . The idea is to propagate a population of feasible candidate solutions using mutation and selection mechanisms. The mean field interaction between the individuals is encapsulated in the selection and the cross-over mechanisms.  In mean field games and multi-agent interacting systems theories, mean field particle processes are used to represent the collective behavior of complex systems with interacting individuals. 112 113 114 115 116 117 118 119 In this context, the mean field interaction is encapsulated in the decision process of interacting agents. The limiting model as the number of agents tends to infinity is sometimes called the continuum model of agents 120  In information theory , and more specifically in statistical machine learning and signal processing , mean field particle methods are used to sample sequentially from the conditional distributions of some random process with respect to a sequence of observations or a cascade of rare events . 121 122 123 124 In discrete time nonlinear filtering problems , the conditional distributions of the random states of a signal given partial and noisy observations satisfy a nonlinear updating-prediction evolution equation. The updating step is given by Bayes' rule , and the prediction step is a Chapman-Kolmogorov transport equation . The mean field particle interpretation of these nonlinear filtering equations is a genetic type selection-mutation particle algorithm 125 During the mutation step, the particles evolve independently of one another according to the Markov transitions of the signal . During the selection stage, particles with small relative likelihood values are killed, while the ones with high relative values are multiplied. 126 127 These mean field particle techniques are also used to solve multiple-object tracking problems, and more specifically to estimate association measures 128 129 130 131  The continuous time version of these particle models are mean field Moran type particle interpretations of the robust optimal filter evolution equations or the Kushner-Stratonotich stochastic partial differential equation. 132 133 134 These genetic type mean field particle algorithms also termed Particle Filters and Sequential Monte Carlo methods are extensively and routinely used in operation research and statistical inference . 135  136 The term "particle filters" was first coined in 1996 by Del Moral, 137 and the term "sequential Monte Carlo" by Liu and Chen in 1998. Subset  simulation techniques are particular instances of genetic particle schemes and Feynman-Kac particle models equipped with Markov  chain  Monte  Carlo  mutation transitions 138 139  Illustrations of the Mean field simulation method  Countable state space models  To motivate the mean field simulation algorithm let's look at a sequence of probability distributions      (   η  n   )    n  =   0  ,  1  ,  …       subscript   subscript  η  n     n   0  1  normal-…      \textstyle\left(\eta_{n}\right)_{n=0,1,\ldots}   on some finite or countable state space   S   S   \textstyle S   satisfying an evolution equation of the form    for some possibly nonlinear mapping   Φ   normal-Φ   \textstyle\Phi   from the set    P   (  S  )       P  S    \textstyle P(S)   of probability measures on   S   S   \textstyle S   into itself. These distributions are given by vectors     η  n   =    (    η  n    (  x  )    )    x  ∈  S         subscript  η  n    subscript     subscript  η  n   x     x  S      \textstyle\eta_{n}=(\eta_{n}(x))_{x\in S}   of numbers      η  n    (  x  )    ∈   [  0  ,  1  ]          subscript  η  n   x    0  1     \textstyle\eta_{n}(x)\in[0,1]   , with    x  ∈  S      x  S    \textstyle x\in S   , such that      ∑   x  ∈  S      η  n    (  x  )     =  1        subscript     x  S       subscript  η  n   x    1    \textstyle\sum_{x\in S}\eta_{n}(x)=1   , so that   Φ   normal-Φ   \textstyle\Phi   is a mapping from the    (   s  -  1   )      s  1    \textstyle(s-1)   - unit simplex into itself, where   s   s   \textstyle s   stands for the cardinality of the set   S   S   \textstyle S   . When   s   s   \textstyle s   is too large, the solving of the equation () is intractable or computationally very costly. One natural way to approximate these evolution equations is to reduce sequentially the state space using a mean field particle model. One of the simplest mean field simulation scheme is defined by the Markov chain     ξ  n   (  N  )    =    (   ξ  n   (  N  ,  i  )    )    1  ≤  i  ≤  N         subscript   superscript  ξ  N   n    subscript   subscript   superscript  ξ   N  i    n       1  i       N       \textstyle\xi^{(N)}_{n}=\left(\xi^{(N,i)}_{n}\right)_{1\leq i\leq N}   on the product space    S  N     superscript  S  N    \textstyle S^{N}   , starting with   N   N   \textstyle N   independent random variables with probability distribution    η  0     subscript  η  0    \textstyle\eta_{0}   and elementary transitions       P  r   (   ξ   n  +  1    (  N  ,  1  )    =   y  1   ,   ξ   n  +  1    (  N  ,  2  )    =   y  2   ,  …  ,   ξ   n  +  1    (  N  ,  N  )    =   y  N   ∣   ξ  n   (  N  )    )   =   ∏   1  ≤  i  ≤  N    Φ   (   η  n  N   )    (   y  i   )      fragments  P  r   fragments  normal-(   subscript   superscript  ξ   N  1      n  1      superscript  y  1   normal-,   subscript   superscript  ξ   N  2      n  1      superscript  y  2   normal-,  normal-…  normal-,   subscript   superscript  ξ   N  N      n  1      superscript  y  N   normal-∣   subscript   superscript  ξ  N   n   normal-)     subscript  product      1  i       N     Φ   fragments  normal-(   superscript   subscript  η  n   N   normal-)    fragments  normal-(   superscript  y  i   normal-)      Pr\left(\xi^{(N,1)}_{n+1}=y^{1},\xi^{(N,2)}_{n+1}=y^{2},\ldots,\xi^{(N,N)}_{n+%
 1}=y^{N}\mid\xi^{(N)}_{n}\right)=\prod_{1\leq i\leq N}\Phi\left(\eta_{n}^{N}%
 \right)\left(y^{i}\right)\quad   , with the empirical measure       η   n  N   =    1  N     ∑   1  ≤  j  ≤  N     1   ξ  n   (  N  ,  j  )            subscript   superscript  η  N   n       1  N     subscript       1  j       N      subscript  1   subscript   superscript  ξ   N  j    n        \quad\eta^{N}_{n}=\frac{1}{N}\sum_{1\leq j\leq N}1_{\xi^{(N,j)}_{n}}   and the indicator function      y   ↦    1  x    (  y  )       maps-to  y     subscript  1  x   y     \textstyle\quad y\mapsto 1_{x}(y)   of the state   x   x   x   .   In other words, given    ξ  n   (  N  )      subscript   superscript  ξ  N   n    \textstyle\xi^{(N)}_{n}   the samples    ξ   n  +  1    (  N  )      subscript   superscript  ξ  N     n  1     \textstyle\xi^{(N)}_{n+1}   are independent random variables with probability distribution    Φ   (   η  n  N   )       normal-Φ   superscript   subscript  η  n   N     \textstyle\Phi\left(\eta_{n}^{N}\right)   . The rationale behind this mean field simulation technique is the following: We expect that when    η  n  N     superscript   subscript  η  n   N    \textstyle\eta_{n}^{N}   is a good approximation of    η  n     subscript  η  n    \textstyle\eta_{n}   , then    Φ   (   η  n  N   )       normal-Φ   superscript   subscript  η  n   N     \textstyle\Phi\left(\eta_{n}^{N}\right)   is an approximation of     Φ   (   η  n   )    =   η   n  +  1          normal-Φ   subscript  η  n     subscript  η    n  1      \textstyle\Phi\left(\eta_{n}\right)=\eta_{n+1}   Thus, since    η   n  +  1   N     superscript   subscript  η    n  1    N    \textstyle\eta_{n+1}^{N}   is the empirical measure of   N   N   \textstyle N   conditionally independent random variables with common probability distribution    Φ   (   η  n  N   )       normal-Φ   superscript   subscript  η  n   N     \textstyle\Phi\left(\eta_{n}^{N}\right)   , we expect    η   n  +  1   N     superscript   subscript  η    n  1    N    \textstyle\eta_{n+1}^{N}   to be a good approximation of    η   n  +  1      subscript  η    n  1     \textstyle\eta_{n+1}   .  Another strategy is to find a collection     K   η  n    =    (    K   η  n     (  x  ,  y  )    )     x  ,  y   ∈  S         subscript  K   subscript  η  n     subscript     subscript  K   subscript  η  n     x  y       x  y   S      \textstyle K_{\eta_{n}}=\left(K_{\eta_{n}}(x,y)\right)_{x,y\in S}   of stochastic matrices indexed by     η  n   ∈   P   (  S  )         subscript  η  n     P  S     \eta_{n}\in\textstyle P(S)   such that    This formula allows us to interpret the sequence     (   η  n   )    n  ≥  0      subscript   subscript  η  n     n  0     \textstyle\left(\eta_{n}\right)_{n\geq 0}   as the probability distributions of the random states     (    X  ¯   n   )    n  ≥  0      subscript   subscript   normal-¯  X   n     n  0     \textstyle\left(\overline{X}_{n}\right)_{n\geq 0}   of the nonlinear Markov chain model with elementary transitions       P  r   (    X  ¯    n  +  1    =  y  ∣    X  ¯   n   =  x  )   =   K   η  n     (  x  ,  y  )      fragments  P  r   fragments  normal-(   subscript   normal-¯  X     n  1     y  normal-∣   subscript   normal-¯  X   n    x  normal-)     subscript  K   subscript  η  n     fragments  normal-(  x  normal-,  y  normal-)     \textstyle Pr(\overline{X}_{n+1}=y\mid\overline{X}_{n}=x)=K_{\eta_{n}}(x,y)   with     L  a  w   (    X  ¯   n   )    =   η  n         L  a  w   subscript   normal-¯  X   n     subscript  η  n     \textstyle Law(\overline{X}_{n})=\eta_{n}      A collection of Markov transitions    K   η  n      subscript  K   subscript  η  n     K_{\eta_{n}}   satisfying the equation () is called a McKean interpretation of the sequence of measures    η  n     subscript  η  n    \textstyle\eta_{n}   . The mean field particle interpretation of () is now defined by the Markov chain     ξ  n   (  N  )    =    (   ξ  n   (  N  ,  i  )    )    1  ≤  i  ≤  N         subscript   superscript  ξ  N   n    subscript   subscript   superscript  ξ   N  i    n       1  i       N       \textstyle\xi^{(N)}_{n}=\left(\xi^{(N,i)}_{n}\right)_{1\leq i\leq N}   on the product space    S  N     superscript  S  N    \textstyle S^{N}   , starting with   N   N   \textstyle N   independent random copies of    X  0     subscript  X  0    \textstyle X_{0}   and elementary transitions       P  r   (   ξ   n  +  1    (  N  ,  1  )    =   y  1   ,   ξ   n  +  1    (  N  ,  2  )    =   y  2   ,  …  ,   ξ   n  +  1    (  N  ,  N  )    =   y  N   ∣   ξ  n   (  N  )    )   =   ∏   1  ≤  i  ≤  N     K    n  +  1   ,   η  n  N      (   ξ  n   (  N  ,  i  )    ,   y  i   )      fragments  P  r   fragments  normal-(   subscript   superscript  ξ   N  1      n  1      superscript  y  1   normal-,   subscript   superscript  ξ   N  2      n  1      superscript  y  2   normal-,  normal-…  normal-,   subscript   superscript  ξ   N  N      n  1      superscript  y  N   normal-∣   subscript   superscript  ξ  N   n   normal-)     subscript  product      1  i       N      subscript  K     n  1    superscript   subscript  η  n   N      fragments  normal-(   subscript   superscript  ξ   N  i    n   normal-,   superscript  y  i   normal-)      Pr\left(\xi^{(N,1)}_{n+1}=y^{1},\xi^{(N,2)}_{n+1}=y^{2},\ldots,\xi^{(N,N)}_{n+%
 1}=y^{N}\mid\xi^{(N)}_{n}\right)=\prod_{1\leq i\leq N}K_{n+1,\eta_{n}^{N}}%
 \left(\xi^{(N,i)}_{n},y^{i}\right)\quad   , with the empirical measure      η   n  N   =    1  N     ∑   1  ≤  j  ≤  N     1   ξ  n   (  N  ,  j  )            subscript   superscript  η  N   n       1  N     subscript       1  j       N      subscript  1   subscript   superscript  ξ   N  j    n        \quad\eta^{N}_{n}=\frac{1}{N}\sum_{1\leq j\leq N}1_{\xi^{(N,j)}_{n}}      Under some weak regularity conditions 140 on the mapping   Φ   normal-Φ   \textstyle\Phi   for any function    f  :   x  ∈  S  ↦   f   (  x  )    ∈  ℝ      normal-:  f      x  S    maps-to      f  x        ℝ      \textstyle f:x\in S\mapsto f(x)\in\mathbb{R}   , we have the almost sure convergence        1  N    ∑   1  ≤  j  ≤  N    f   (   ξ  n   (  N  ,  j  )    )    ⟶   N  ↑  ∞    E   (  f   (    X  ¯   n   )   )   =   ∑   x  ∈  S     η  n    (  x  )   f   (  x  )      fragments    1  N    subscript       1  j       N     f   fragments  normal-(   subscript   superscript  ξ   N  j    n   normal-)    subscript  normal-⟶   normal-↑  N      E   fragments  normal-(  f   fragments  normal-(   subscript   normal-¯  X   n   normal-)   normal-)     subscript     x  S     subscript  η  n    fragments  normal-(  x  normal-)   f   fragments  normal-(  x  normal-)     \frac{1}{N}\sum_{1\leq j\leq N}f\left(\xi^{(N,j)}_{n}\right)\longrightarrow_{N%
 \uparrow\infty}\quad E\left(f(\overline{X}_{n})\right)=\sum_{x\in S}\eta_{n}(x%
 )f(x)      These nonlinear Markov processes and their mean field particle interpretation can be extended to time non homogeneous models on general measurable state spaces. 141  Feynman-Kac models  To illustrate the abstract models presented above, we consider a stochastic matrix    M  =    (   M   (  x  ,  y  )    )     x  ,  y   ∈  S        M   subscript    M   x  y       x  y   S      \textstyle M=(M(x,y))_{x,y\in S}   and some function     G  :  x  ∈   S   ↦  G   (  x  )   ∈  ]   0  ,  1  ]     fragments   fragments  G  normal-:  x   S  maps-to  G   fragments  normal-(  x  normal-)    normal-]   0  normal-,  1  normal-]    \textstyle G:x\in S~{}\mapsto~{}G(x)\in]0,1]   . We associate with these two objects the mapping        Φ   :    η  n   =    (    η  n    (  x  )    )    x  ∈  S    ∈   P   (  S  )    ↦   Φ   (   η  n   )    =    (   Φ   (   η  n   )    (  y  )    )    y  ∈  S    ∈   P   (  S  )        normal-:  normal-Φ       subscript  η  n    subscript     subscript  η  n   x     x  S           P  S     maps-to      normal-Φ   subscript  η  n          subscript    normal-Φ   subscript  η  n   y     y  S           P  S       \Phi~{}:~{}\eta_{n}=(\eta_{n}(x))_{x\in S}\in P(S)~{}\mapsto~{}\Phi(\eta_{n})=%
 \left(\Phi(\eta_{n})(y)\right)_{y\in S}\in P(S)      with the entries    Φ   (   η  n   )    (  y  )       normal-Φ   subscript  η  n   y    \Phi(\eta_{n})(y)   defined for any    y  ∈  S      y  S    y\in S   by    and the Boltzmann-Gibbs measures     Ψ  G    (   η  n   )    (  x  )        subscript  normal-Ψ  G    subscript  η  n   x    \textstyle\Psi_{G}(\eta_{n})(x)   defined by         Ψ  G    (   η  n   )    (  x  )    =      η  n    (  x  )   G   (  x  )      ∑   z  ∈  S      η  n    (  z  )   G   (  z  )              subscript  normal-Ψ  G    subscript  η  n   x        subscript  η  n   x  G  x     subscript     z  S       subscript  η  n   z  G  z       \displaystyle\Psi_{G}(\eta_{n})(x)=\frac{\eta_{n}(x)G(x)}{\sum_{z\in S}\eta_{n%
 }(z)G(z)}      We denote by     K   η  n    =    (    K   η  n     (  x  ,  y  )    )     x  ,  y   ∈  S         subscript  K   subscript  η  n     subscript     subscript  K   subscript  η  n     x  y       x  y   S      \textstyle K_{\eta_{n}}=\left(K_{\eta_{n}}(x,y)\right)_{x,y\in S}   the collection of stochastic matrices indexed by     η  n   ∈   P   (  S  )         subscript  η  n     P  S     \eta_{n}\in\textstyle P(S)   given by         K   η  n     (  x  ,  y  )    =    ϵ  G   (  x  )   M   (  x  ,  y  )    +    (   1  -   ϵ  G   (  x  )     )   Φ   (   η  n   )    (  y  )            subscript  K   subscript  η  n     x  y        ϵ  G  x  M   x  y        1    ϵ  G  x    normal-Φ   subscript  η  n   y      K_{\eta_{n}}(x,y)=\epsilon G(x)~{}M(x,y)+(1-\epsilon G(x))~{}\Phi(\eta_{n})(y)      for some parameter    ϵ  ∈   [  0  ,  1  ]       ϵ   0  1     \textstyle\epsilon\in[0,1]   . It is readily checked that the equation () is satisfied. In addition, we can also show (cf. for instance 142 ) that the solution of () is given by the Feynman-Kac formula       η  0     subscript  η  0    \textstyle\eta_{0}   with initial distribution   M   M   \textstyle M   and Markov transition     f   :   x  ∈  S  ↦  ℝ      normal-:  f      x  S    maps-to    ℝ      \textstyle f~{}:~{}x\in S\mapsto\mathbb{R}   .   gFor any function     G   (  x  )    =  1        G  x   1    \textstyle G(x)=1   we have       ϵ  =  1      ϵ  1    \epsilon=1      If     K   η  n     (  x  ,  y  )   =  M   (  x  ,  y  )   =  P  r   (   X   n  +  1    =  y  |   X  n   =  x  )      fragments   subscript  K   subscript  η  n     fragments  normal-(  x  normal-,  y  normal-)    M   fragments  normal-(  x  normal-,  y  normal-)    P  r   fragments  normal-(   subscript  X    n  1     y  normal-|   subscript  X  n    x  normal-)     K_{\eta_{n}}(x,y)=M(x,y)=Pr\left(X_{n+1}=y|X_{n}=x\right)   is the unit function and     η  n    (  x  )   =  E   (   1  x    (   X  n   )   )   =  P  r   (   X  n   =  x  )      fragments   subscript  η  n    fragments  normal-(  x  normal-)    E   fragments  normal-(   subscript  1  x    fragments  normal-(   subscript  X  n   normal-)   normal-)    P  r   fragments  normal-(   subscript  X  n    x  normal-)     \textstyle\displaystyle\eta_{n}(x)=E\left(1_{x}(X_{n})\right)=Pr(X_{n}=x)   , then we have     η   n  +  1     (  y  )   =    ∑   x  ∈  S      η  n    (  x  )   M   (  x  ,  y  )   ⇔  P  r   (   X   n  +  1    =  y  )   =     ∑   x  ∈  S      P  r   (   X   n  +  1    =  y  |   X  n   =  x  )   P  r   (   X  n   =  x  )      fragments   subscript  η    n  1     fragments  normal-(  y  normal-)     subscript     x  S     subscript  η  n    fragments  normal-(  x  normal-)   M   fragments  normal-(  x  normal-,  y  normal-)    normal-⇔   P  r   fragments  normal-(   subscript  X    n  1     y  normal-)     subscript     x  S    P  r   fragments  normal-(   subscript  X    n  1     y  normal-|   subscript  X  n    x  normal-)   P  r   fragments  normal-(   subscript  X  n    x  normal-)     \displaystyle\eta_{n+1}(y)=\sum_{x\in S}\eta_{n}(x)M(x,y)\quad\Leftrightarrow%
 \quad Pr\left(X_{n+1}=y\right)=\sum_{x\in S}~{}Pr\left(X_{n+1}=y|X_{n}=x\right%
 )Pr\left(X_{n}=x\right)   and   N   N   \textstyle N   . In this situation, the equation () reduces to the Chapman-Kolmogorov equation       ξ   n  +  1    (  N  ,  i  )      subscript   superscript  ξ   N  i      n  1     \textstyle\xi^{(N,i)}_{n+1}      The mean field particle interpretation of this Feynman-Kac model is defined by sampling sequentially      K    n  +  1   ,   η  n  N      (   ξ  n   (  N  ,  i  )    ,  y  )    =    ϵ  G   (   ξ  n   (  N  ,  i  )    )   M   (   ξ  n   (  N  ,  i  )    ,  y  )    +    (   1  -   ϵ  G   (   ξ  n   (  N  ,  i  )    )     )     ∑   1  ≤  j  ≤  N        G   (   ξ  n   (  N  ,  j  )    )       ∑   1  ≤  k  ≤  N      G   (   ξ  n   (  N  ,  k  )    )       M   (   ξ  n   (  N  ,  j  )    ,  y  )              subscript  K     n  1    superscript   subscript  η  n   N       subscript   superscript  ξ   N  i    n   y        ϵ  G   subscript   superscript  ξ   N  i    n   M    subscript   superscript  ξ   N  i    n   y        1    ϵ  G   subscript   superscript  ξ   N  i    n       subscript       1  j       N           G   subscript   superscript  ξ   N  j    n      subscript       1  k       N       G   subscript   superscript  ξ   N  k    n      M    subscript   superscript  ξ   N  j    n   y         K_{n+1,\eta_{n}^{N}}\left(\xi^{(N,i)}_{n},y\right)=\epsilon G\left(\xi^{(N,i)}%
 _{n}\right)M\left(\xi^{(N,i)}_{n},y\right)+\left(1-\epsilon G\left(\xi^{(N,i)}%
 _{n}\right)\right)~{}\sum_{1\leq j\leq N}\frac{G\left(\xi^{(N,j)}_{n}\right)}{%
 \sum_{1\leq k\leq N}G\left(\xi^{(N,k)}_{n}\right)}~{}M\left(\xi^{(N,j)}_{n},y\right)   conditionally independent random variables    ϵ  G   (   ξ  n   (  N  ,  i  )    )       ϵ  G   subscript   superscript  ξ   N  i    n     \textstyle\epsilon G\left(\xi^{(N,i)}_{n}\right)   with probability distribution       ξ  n   (  N  ,  i  )      subscript   superscript  ξ   N  i    n    \textstyle\xi^{(N,i)}_{n}      In other words, with a probability     ξ   n  +  1    (  N  ,  i  )    =  y       subscript   superscript  ξ   N  i      n  1    y    \textstyle\xi^{(N,i)}_{n+1}=y   the particle    M   (   ξ  n   (  N  ,  i  )    ,  y  )       M    subscript   superscript  ξ   N  i    n   y     \textstyle M\left(\xi^{(N,i)}_{n},y\right)   evolve to a new random state    ξ  n   (  N  ,  i  )      subscript   superscript  ξ   N  i    n    \textstyle\xi^{(N,i)}_{n}   randomly chosen with the probability distribution    ξ  n   (  N  ,  j  )      subscript   superscript  ξ   N  j    n    \textstyle\xi^{(N,j)}_{n}   ; otherwise,    G   (   ξ  n   (  N  ,  j  )    )       G   subscript   superscript  ξ   N  j    n     \textstyle G\left(\xi^{(N,j)}_{n}\right)   jumps to a new location     ξ   n  +  1    (  N  ,  i  )    =  y       subscript   superscript  ξ   N  i      n  1    y    \textstyle\xi^{(N,i)}_{n+1}=y   randomly chosen with a probability proportional to    M   (   ξ  n   (  N  ,  j  )    ,  y  )       M    subscript   superscript  ξ   N  j    n   y     \textstyle M\left(\textstyle\xi^{(N,j)}_{n},y\right)   , and evolves to a new random state     G   (  x  )    =  1        G  x   1    \textstyle G(x)=1   randomly chosen with the probability distribution    ϵ  =  1      ϵ  1    \epsilon=1   . If    X  n     subscript  X  n    \textstyle X_{n}   is the unit function and    ϵ  =  0      ϵ  0    \textstyle\epsilon=0   , the interaction between the particle vanishes and the particle model reduces to a sequence of independent copies of the Markov chain   G   G   \textstyle G   . When   M   M   \textstyle M   the mean field particle model described above reduces to a simple mutation-selection genetic algorithm with fitness function     X  ¯   n     subscript   normal-¯  X   n    \textstyle\overline{X}_{n}   and mutation transition    n  =   0  ,  1  ,  …       n   0  1  normal-…     \textstyle n=0,1,\ldots   . These nonlinear Markov chain models and their mean field particle interpretation can be extended to time non homogeneous models on general measurable state spaces (including transition states, path spaces and random excursion spaces) and continuous time models. 143 144 145  Gaussian nonlinear state space models  We consider a sequence of real valued random variables    W  n     subscript  W  n    \textstyle W_{n}   indexed by the discrete time index    σ  >  0      σ  0    \textstyle\sigma>0   and defined sequentially by the equations    with a collection    a  ,  b  ,  c     a  b  c    \textstyle a,b,c   of independent standard Gaussian random variables, a parameter   ℝ   ℝ   \mathbb{R}   and some functions     X  ¯   0     subscript   normal-¯  X   0    \textstyle\overline{X}_{0}   from the set of real numbers    η  n     subscript  η  n    \textstyle\eta_{n}   into itself, and some standard Gaussian initial random state     X  ¯   n     subscript   normal-¯  X   n    \textstyle\overline{X}_{n}   . We let   f   f   f   be the probability distribution of the random state      E   (   f   (    X  ¯   n   )    )    =     ∫   -  ∞    +  ∞      f   (  x  )    η  n    (   d  x   )            E    f   subscript   normal-¯  X   n       superscript   subscript               f  x   subscript  η  n     d  x       \textstyle E\left(f(\overline{X}_{n})\right)=\displaystyle\int_{-\infty}^{+%
 \infty}f(x)~{}\eta_{n}(dx)\quad   ; that is, for any bounded measurable functions      P   r   (    X  ¯   n   ∈  d  x  )   =   η  n    (  d  x  )      fragments  P  r   fragments  normal-(   subscript   normal-¯  X   n    d  x  normal-)     subscript  η  n    fragments  normal-(  d  x  normal-)     \quad\textstyle Pr(\overline{X}_{n}\in dx)=\eta_{n}(dx)   we have       d  x      d  x    \textstyle dx   , with   x   x   \textstyle x      The l.h.s. formula is expressed in terms of the Lebesgue integral , and   f   f   \textstyle f   stands for an infinitesimal neighborhood of the state    E   (  f   (    X  ¯    n  +  1    )   |    X  ¯   n   =  x  )   =   ∫   -  ∞    +  ∞     K   η  n     (  x  ,  d  y  )   f   (  y  )      fragments  E   fragments  normal-(  f   fragments  normal-(   subscript   normal-¯  X     n  1    normal-)   normal-|   subscript   normal-¯  X   n    x  normal-)     superscript   subscript              subscript  K   subscript  η  n     fragments  normal-(  x  normal-,  d  y  normal-)   f   fragments  normal-(  y  normal-)      E\left(f(\overline{X}_{n+1})~{}|~{}\overline{X}_{n}=x\right)=\int_{-\infty}^{+%
 \infty}K_{\eta_{n}}(x,dy)~{}f(y)\quad   . The Markov transition of the chain is given for any bounded measurable functions      K    η  n     (  x  ,  d  y  )   =  P  r   (    X  ¯    n  +  1    ∈  d  y  ∣    X  ¯   n   =  x  )   =    1     2  π    σ     exp   {  -   1   2   σ  2       (  y  -   [  b   (  x  )    ∫   -  ∞    +  ∞    a   (  z  )    η  n    (  d  z  )   +  c   (  x  )   ]   )   2   }   d  y     fragments   subscript  K   subscript  η  n     fragments  normal-(  x  normal-,  d  y  normal-)    P  r   fragments  normal-(   subscript   normal-¯  X     n  1     d  y  normal-∣   subscript   normal-¯  X   n    x  normal-)      1        2  π    σ      fragments  normal-{     1    2   superscript  σ  2      superscript   fragments  normal-(  y    fragments  normal-[  b   fragments  normal-(  x  normal-)    superscript   subscript             a   fragments  normal-(  z  normal-)    subscript  η  n    fragments  normal-(  d  z  normal-)    c   fragments  normal-(  x  normal-)   normal-]   normal-)   2   normal-}   d  y    \quad K_{\eta_{n}}(x,dy)=Pr(\overline{X}_{n+1}\in dy\mid\overline{X}_{n}=x)=%
 \frac{1}{\sqrt{2\pi}\sigma}~{}\exp{\left\{-\frac{1}{2\sigma^{2}}\left(y-\left[%
 b(x)\int_{-\infty}^{+\infty}a(z)~{}\eta_{n}(dz)+c(x)\right]\right)^{2}\right\}%
 }~{}dy   by the formula       η  n     subscript  η  n    \textstyle\eta_{n}   , with       ∫   -  ∞    +  ∞      η   n  +  1     (   d  y   )   f   (  y  )     =    ∫   -  ∞    +  ∞      [    ∫   -  ∞    +  ∞      η  n    (   d  x   )    K   η  n     (  x  ,   d  y   )     ]   f   (  y  )            superscript   subscript                subscript  η    n  1      d  y   f  y      superscript   subscript                delimited-[]    superscript   subscript                subscript  η  n     d  x    subscript  K   subscript  η  n     x    d  y       f  y      \int_{-\infty}^{+\infty}\eta_{n+1}(dy)~{}f(y)=\int_{-\infty}^{+\infty}\left[%
 \int_{-\infty}^{+\infty}\eta_{n}(dx)K_{\eta_{n}}(x,dy)\right]~{}f(y)\quad      Using the tower property of conditional expectations we prove that the probability distributions   f   f   f   satisfy the nonlinear equation          η   n  +  1     =   Φ   (   η  n   )    =    η  n    K   η  n        ⇔    η   n  +  1     (   d  y   )     =    (    η  n    K   η  n     )    (   d  y   )    =    ∫   x  ∈  ℝ      η  n    (   d  x   )    K   η  n     (  x  ,   d  y   )         formulae-sequence       subscript  η    n  1      normal-Φ   subscript  η  n            subscript  η  n    subscript  K   subscript  η  n            normal-⇔     subscript  η    n  1      d  y          subscript  η  n    subscript  K   subscript  η  n       d  y           subscript     x  ℝ       subscript  η  n     d  x    subscript  K   subscript  η  n     x    d  y          \eta_{n+1}~{}=\Phi\left(\eta_{n}\right)=~{}\eta_{n}K_{\eta_{n}}\quad%
 \Leftrightarrow\quad\eta_{n+1}(dy)~{}=~{}\left(\eta_{n}K_{\eta_{n}}\right)(dy)%
 ~{}=~{}\int_{x\in\mathbb{R}}\eta_{n}(dx)K_{\eta_{n}}(x,dy)      for any bounded measurable functions     ξ  n   (  N  )    =    (   ξ  n   (  N  ,  i  )    )    1  ≤  i  ≤  N         subscript   superscript  ξ  N   n    subscript   subscript   superscript  ξ   N  i    n       1  i       N       \textstyle\xi^{(N)}_{n}=\left(\xi^{(N,i)}_{n}\right)_{1\leq i\leq N}   . This equation is sometimes written in the more synthetic form       ℝ  N     superscript  ℝ  N    \textstyle\mathbb{R}^{N}      The mean field particle interpretation of this model is defined by the Markov chain      ξ   n  +  1    (  N  ,  i  )    =     (     1  N       ∑   1  ≤  j  ≤  N      a   (   ξ  n   (  N  ,  i  )    )      )   b   (   ξ  n   (  N  ,  i  )    )    +   c   (   ξ  n   (  N  ,  i  )    )    +    σ    W  n  i       1  ≤  i  ≤  N      formulae-sequence     subscript   superscript  ξ   N  i      n  1            1  N     subscript       1  j       N       a   subscript   superscript  ξ   N  i    n      b   subscript   superscript  ξ   N  i    n      c   subscript   superscript  ξ   N  i    n      σ   subscript   superscript  W  i   n          1  i       N      \xi^{(N,i)}_{n+1}=\left(\displaystyle\frac{1}{N}\sum_{1\leq j\leq N}a\left(\xi%
 ^{(N,i)}_{n}\right)\right)~{}b\left(\xi^{(N,i)}_{n}\right)+c\left(\xi^{(N,i)}_%
 {n}\right)+\sigma~{}W^{i}_{n}\quad 1\leq i\leq N   on the product space     ξ  0   (  N  )    =    (   ξ  0   (  N  ,  i  )    )    1  ≤  i  ≤  N         subscript   superscript  ξ  N   0    subscript   subscript   superscript  ξ   N  i    0       1  i       N       \textstyle\xi^{(N)}_{0}=\left(\xi^{(N,i)}_{0}\right)_{1\leq i\leq N}   defined by        (   W  n  i   )    1  ≤  i  ≤  N      subscript   subscript   superscript  W  i   n       1  i       N      \textstyle\left(W^{i}_{n}\right)_{1\leq i\leq N}      In the above display,   N   N   \textstyle N   and     X  ¯   0     subscript   normal-¯  X   0    \textstyle\overline{X}_{0}   stands for    W  n     subscript  W  n    \textstyle W_{n}   independent copies of    n  =   1  ,  2  ,  …       n   1  2  normal-…     \textstyle n=1,2,\ldots   and    a  ,  b  ,  c     a  b  c    \textstyle a,b,c   , with     1  N    ∑   1  ≤  j  ≤  N    f   (   ξ  n   (  N  ,  i  )    )   =    ∫  ℝ    f   (  y  )    η  n  N    (  d  y  )    ⟶   N  ↑  ∞    E   (  f   (    X  ¯   n   )   )   =    ∫  ℝ    f   (  y  )    η  n    (  d  y  )      fragments    1  N    subscript       1  j       N     f   fragments  normal-(   subscript   superscript  ξ   N  i    n   normal-)     subscript   ℝ   f   fragments  normal-(  y  normal-)    subscript   superscript  η  N   n    fragments  normal-(  d  y  normal-)    subscript  normal-⟶   normal-↑  N      E   fragments  normal-(  f   fragments  normal-(   subscript   normal-¯  X   n   normal-)   normal-)     subscript   ℝ   f   fragments  normal-(  y  normal-)    subscript  η  n    fragments  normal-(  d  y  normal-)     \frac{1}{N}\sum_{1\leq j\leq N}f\left(\xi^{(N,i)}_{n}\right)=\int_{\mathbb{R}}%
 ~{}f(y)~{}\eta^{N}_{n}(dy)\longrightarrow_{N\uparrow\infty}\quad E\left(f(%
 \overline{X}_{n})\right)=\int_{\mathbb{R}}~{}f(y)~{}\eta_{n}(dy)   . For regular models (for instance for bounded Lipschitz functions     η  n  N   =    1  N     ∑   1  ≤  j  ≤  N     δ   ξ  n   (  N  ,  i  )            subscript   superscript  η  N   n       1  N     subscript       1  j       N      subscript  δ   subscript   superscript  ξ   N  i    n        \eta^{N}_{n}=\frac{1}{N}\sum_{1\leq j\leq N}\delta_{\xi^{(N,i)}_{n}}   ) we have the almost sure convergence      f   f   \textstyle f   , with the empirical measure    δ  x     subscript  δ  x    \textstyle\delta_{x}      for any bounded measurable functions   x   x   \textstyle x   (cf. for instance 146 ). In the above display,     W  ¯    t  n      subscript   normal-¯  W    subscript  t  n     \textstyle\overline{W}_{t_{n}}   stands for the Dirac measure at the state     c   (  x  )    =  x        c  x   x    \textstyle c(x)=x   .  Continuous time mean field models  We consider a standard Brownian motion     b   (  x  )       b  x    \textstyle b(x)   (a.k.a. Wiener Process ) evaluated on a time mesh sequence   σ   σ   \textstyle\sigma   . We choose     b   (  x  )    ×  h        b  x   h    \textstyle b(x)\times h   in the equation (), we replace    σ  ×   h       σ    h     \textstyle\sigma\times\sqrt{h}   and     X  ¯    t  n      subscript   normal-¯  X    subscript  t  n     \textstyle\overline{X}_{t_{n}}   by     X  ¯   n     subscript   normal-¯  X   n    \textstyle\overline{X}_{n}   and    t  n     subscript  t  n    \textstyle t_{n}   , and we write    (     W  ¯    t   n  +  1     -    W  ¯    t  n     )       subscript   normal-¯  W    subscript  t    n  1      subscript   normal-¯  W    subscript  t  n      \textstyle\left(\overline{W}_{t_{n+1}}-\overline{W}_{t_{n}}\right)   instead of      t  n   -   t   n  -  1     =  h         subscript  t  n    subscript  t    n  1     h    \textstyle t_{n}-t_{n-1}=h   the values of the random states evaluated at the time step   h   h   \textstyle h   . Recalling that   0   0   \textstyle 0   are independent centered Gaussian random variables with variance     d    X  ¯   t    =    E   (   a   (    X  ¯   t   )    )   b   (    X  ¯   t   )   d  t   +    σ   d    W  ¯   t           d   subscript   normal-¯  X   t        E    a   subscript   normal-¯  X   t    b   subscript   normal-¯  X   t   d  t     σ  d   subscript   normal-¯  W   t       d\overline{X}_{t}=E\left(a\left(\overline{X}_{t}\right)\right)~{}b(\overline{X%
 }_{t})~{}dt+\sigma~{}d\overline{W}_{t}   , the resulting equation can be rewritten in the following form   -\overline{X}_{t_n}=E\left(a\left(\overline{X}_{t_n}\right)\right)~b(\overline{X}_{t_n})~h+\sigma~\left(\overline{W}_{t_{n+1}}-\overline{W}_{t_n}\right)| 4 }}   When time step     ξ  t   (  N  )    =    (   ξ  t   (  N  ,  i  )    )    1  ≤  i  ≤  N         subscript   superscript  ξ  N   t    subscript   subscript   superscript  ξ   N  i    t       1  i       N       \textstyle\xi^{(N)}_{t}=\left(\xi^{(N,i)}_{t}\right)_{1\leq i\leq N}   tends to    ℝ  N     superscript  ℝ  N    \textstyle\mathbb{R}^{N}   , the above equation converge to the nonlinear diffusion process         d   ξ  t   (  N  ,  i  )     =     (     1  N       ∑   1  ≤  j  ≤  N      a   (   ξ  t   (  N  ,  i  )    )      )   b   (   ξ  t   (  N  ,  i  )    )    +    σ   d    W  ¯   t  i       1  ≤  i  ≤  N      formulae-sequence      d   subscript   superscript  ξ   N  i    t            1  N     subscript       1  j       N       a   subscript   superscript  ξ   N  i    t      b   subscript   superscript  ξ   N  i    t      σ  d   superscript   subscript   normal-¯  W   t   i          1  i       N      d\xi^{(N,i)}_{t}=\left(\displaystyle\frac{1}{N}\sum_{1\leq j\leq N}a\left(\xi^%
 {(N,i)}_{t}\right)\right)~{}b\left(\xi^{(N,i)}_{t}\right)+\sigma~{}d\overline{%
 W}_{t}^{i}\quad 1\leq i\leq N      The mean field continuous time model associated with these nonlinear diffusions is the (interacting) diffusion process     ξ  0   (  N  )    =    (   ξ  0   (  N  ,  i  )    )    1  ≤  i  ≤  N         subscript   superscript  ξ  N   0    subscript   subscript   superscript  ξ   N  i    0       1  i       N       \textstyle\xi^{(N)}_{0}=\left(\xi^{(N,i)}_{0}\right)_{1\leq i\leq N}   on the product space     (    W  ¯   t  i   )    1  ≤  i  ≤  N      subscript   superscript   subscript   normal-¯  W   t   i       1  i       N      \textstyle\left(\overline{W}_{t}^{i}\right)_{1\leq i\leq N}   defined by      N   N   \textstyle N      In the above display,     X  ¯   0     subscript   normal-¯  X   0    \textstyle\overline{X}_{0}   and     W  ¯   t     subscript   normal-¯  W   t    \textstyle\overline{W}_{t}   stands for    a  ,  b     a  b    \textstyle a,b   independent copies of     1  N    ∑   1  ≤  j  ≤  N    f   (   ξ  t   (  N  ,  i  )    )   =    ∫  ℝ    f   (  y  )    η  t  N    (  d  y  )    ⟶   N  ↑  ∞    E   (  f   (    X  ¯   t   )   )   =    ∫  ℝ    f   (  y  )    η  t    (  d  y  )      fragments    1  N    subscript       1  j       N     f   fragments  normal-(   subscript   superscript  ξ   N  i    t   normal-)     subscript   ℝ   f   fragments  normal-(  y  normal-)    subscript   superscript  η  N   t    fragments  normal-(  d  y  normal-)    subscript  normal-⟶   normal-↑  N      E   fragments  normal-(  f   fragments  normal-(   subscript   normal-¯  X   t   normal-)   normal-)     subscript   ℝ   f   fragments  normal-(  y  normal-)    subscript  η  t    fragments  normal-(  d  y  normal-)     \frac{1}{N}\sum_{1\leq j\leq N}f\left(\xi^{(N,i)}_{t}\right)=\int_{\mathbb{R}}%
 ~{}f(y)~{}\eta^{N}_{t}(dy)\longrightarrow_{N\uparrow\infty}\quad E\left(f(%
 \overline{X}_{t})\right)=\int_{\mathbb{R}}~{}f(y)~{}\eta_{t}(dy)   and     η  t   =   L  a  w   (    X  ¯   t   )         subscript  η  t     L  a  w   subscript   normal-¯  X   t      \textstyle\eta_{t}=Law\left(\overline{X}_{t}\right)   . For regular models (for instance for bounded Lipschitz functions     η  t  N   =    1  N     ∑   1  ≤  j  ≤  N     δ   ξ  t   (  N  ,  i  )            subscript   superscript  η  N   t       1  N     subscript       1  j       N      subscript  δ   subscript   superscript  ξ   N  i    t        \eta^{N}_{t}=\frac{1}{N}\sum_{1\leq j\leq N}\delta_{\xi^{(N,i)}_{t}}   ) we have the almost sure convergence      f   f   \textstyle f   , with $\textstyle \eta_t=Law\left(\overline{X}_{t}\right)$ , and the empirical measure $\eta^N_t=\frac{1}{N}\sum_{1\leq j\leq N}\delta_{\xi^{(N,i)}_t}$   for any bounded measurable functions $\textstyle f$ (cf. for instance. 147 ). These nonlinear Markov processes and their mean field particle interpretation can be extended to interacting jump-diffusion processes 148 149 150 151  References  External links   Feynman-Kac models and interacting particle systems Theoretical aspects and a list of application domains of Feynman-Kac particle methodologies.  Sequential Monte Carlo method and particle filters resources  Interacting Particle Systems resources  QMC in Cambridge and around the world General information about Quantum Monte Carlo.  EVOLVER Software package for stochastic optimisation using genetic algorithms  CASINO Quantum Monte Carlo program developed by the Theory of Condensed Matter group at the Cavendish Laboratory in Cambridge.  Biips is a probabilistic programming software for Bayesian inference with interacting particle systems.   "  Category:Telecommunication theory  Category:Statistical data types  Category:Numerical analysis  Category:Statistical mechanics  Category:Sampling techniques  Category:Stochastic simulation  Category:Probabilistic complexity theory  Category:Risk management      ↩    ↩      ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩     ↩  ↩  ↩  ↩  ↩   ↩  ↩  ↩  ↩  ↩  P. Del Moral, G. Rigal, and G. Salut. Estimation and nonlinear optimal control : An unified framework for particle solutions LAAS-CNRS, Toulouse, Research Report no. 91137, DRET-DIGILOG- LAAS/CNRS contract, April (1991). ↩  P. Del Moral, G. Rigal, and G. Salut. Nonlinear and non Gaussian particle filters applied to inertial platform repositioning. LAAS-CNRS, Toulouse, Research Report no. 92207, STCAN/DIGILOG-LAAS/CNRS Convention STCAN no. A.91.77.013, (94p.) September (1991). ↩  P. Del Moral, G. Rigal, and G. Salut. Estimation and nonlinear optimal control : Particle resolution in filtering and estimation. Experimental results. Convention DRET no. 89.34.553.00.470.75.01, Research report no.2 (54p.), January (1992). ↩  P. Del Moral, G. Rigal, and G. Salut. Estimation and nonlinear optimal control : Particle resolution in filtering and estimation. Theoretical results Convention DRET no. 89.34.553.00.470.75.01, Research report no.3 (123p.), October (1992). ↩  P. Del Moral, J.-Ch. Noyer, G. Rigal, and G. Salut. Particle filters in radar signal processing : detection, estimation and air targets recognition. LAAS-CNRS, Toulouse, Research report no. 92495, December (1992). ↩  P. Del Moral, G. Rigal, and G. Salut. Estimation and nonlinear optimal control : Particle resolution in filtering and estimation. Studies on: Filtering, optimal control, and maximum likelihood estimation. Convention DRET no. 89.34.553.00.470.75.01. Research report no.4 (210p.), January (1993). ↩   ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩    ↩   ↩  ↩  ↩  ↩  ↩  ↩      ↩  ↩          ↩  ↩  ↩  ↩  ↩  ↩     ↩  ↩  ↩  ↩  ↩      ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩  ↩   ↩   ↩  ↩  ↩  ↩   ↩   ↩   ↩   ↩  ↩  ↩  ↩  ↩                 