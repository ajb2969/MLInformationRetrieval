   Segmentation-based object categorization      Segmentation-based object categorization   The image segmentation problem is concerned with partitioning an image into multiple regions according to some homogeneity criterion. This article is primarily concerned with graph theoretic approaches to image segmentation. Segmentation-based object categorization can be viewed as a specific case of spectral clustering applied to image segmentation.  Applications of image segmentation   Image compression   Segment the image into homogeneous components, and use the most suitable compression algorithm for each component to improve compression.   Medical diagnosis   Automatic segmentation of MRI images for identification of cancerous regions.   Mapping and measurement   Automatic analysis of remote sensing data from satellites to identify and measure regions of interest.    Segmentation using normalized cuts  Graph theoretic formulation  The set of points in an arbitrary feature space can be represented as a weighted undirected complete graph G = (V, E), where the nodes of the graph are the points in the feature space. The weight    w   i  j      subscript  w    i  j     w_{ij}   of an edge     (  i  ,  j  )   ∈  E       i  j   E    (i,j)\in E   is a function of the similarity between the nodes   i   i   i   and   j   j   j   . In this context, we can formulate the image segmentation problem as a graph partitioning problem that asks for a partition     V  1   ,  ⋯  ,   V  k       subscript  V  1   normal-⋯   subscript  V  k     V_{1},\cdots,V_{k}   of the vertex set   V   V   V   , where, according to some measure, the vertices in any set    V  i     subscript  V  i    V_{i}   have high similarity, and the vertices in two different sets     V  i   ,   V  j       subscript  V  i    subscript  V  j     V_{i},V_{j}   have low similarity.  Normalized cuts  Let G = ( V , E , w ) be a weighted graph. Let   A   A   A   and   B   B   B   be two subsets of vertices.  Let:       w   (  A  ,  B  )    =    ∑    i  ∈  A   ,   j  ∈  B      w   i  j           w   A  B      subscript    formulae-sequence    i  A     j  B      subscript  w    i  j       w(A,B)=\sum\limits_{i\in A,j\in B}w_{ij}          ncut   (  A  ,  B  )    =     w   (  A  ,  B  )     w   (  A  ,  V  )     +    w   (  A  ,  B  )     w   (  B  ,  V  )           ncut  A  B         w   A  B      w   A  V         w   A  B      w   B  V        \operatorname{ncut}(A,B)=\frac{w(A,B)}{w(A,V)}+\frac{w(A,B)}{w(B,V)}          nassoc   (  A  ,  B  )    =     w   (  A  ,  A  )     w   (  A  ,  V  )     +    w   (  B  ,  B  )     w   (  B  ,  V  )           nassoc  A  B         w   A  A      w   A  V         w   B  B      w   B  V        \operatorname{nassoc}(A,B)=\frac{w(A,A)}{w(A,V)}+\frac{w(B,B)}{w(B,V)}     In the normalized cuts approach, 1 for any cut    (  S  ,   S  ¯   )     S   normal-¯  S     (S,\overline{S})   in   G   G   G   ,    ncut   (  S  ,   S  ¯   )      ncut  S   normal-¯  S     \operatorname{ncut}(S,\overline{S})   measures the similarity between different parts, and    nassoc   (  S  ,   S  ¯   )      nassoc  S   normal-¯  S     \operatorname{nassoc}(S,\overline{S})   measures the total similarity of vertices in the same part.  Since     ncut   (  S  ,   S  ¯   )    =   2  -   nassoc   (  S  ,   S  ¯   )          ncut  S   normal-¯  S      2   nassoc  S   normal-¯  S       \operatorname{ncut}(S,\overline{S})=2-\operatorname{nassoc}(S,\overline{S})   , a cut    (   S  *   ,    S  ¯   *   )      superscript  S     superscript   normal-¯  S       (S^{*},{\overline{S}}^{*})   that minimizes    ncut   (  S  ,   S  ¯   )      ncut  S   normal-¯  S     \operatorname{ncut}(S,\overline{S})   also maximizes    nassoc   (  S  ,   S  ¯   )      nassoc  S   normal-¯  S     \operatorname{nassoc}(S,\overline{S})   .  Computing a cut    (   S  *   ,    S  ¯   *   )      superscript  S     superscript   normal-¯  S       (S^{*},{\overline{S}}^{*})   that minimizes    ncut   (  S  ,   S  ¯   )      ncut  S   normal-¯  S     \operatorname{ncut}(S,\overline{S})   is an NP-hard problem. However, we can find in polynomial time a cut    (  S  ,   S  ¯   )     S   normal-¯  S     (S,\overline{S})   of small normalized weight    ncut   (  S  ,   S  ¯   )      ncut  S   normal-¯  S     \operatorname{ncut}(S,\overline{S})   using spectral techniques .  The ncut algorithm  Let:       d   (  i  )    =    ∑  j    w   i  j           d  i     subscript   j    subscript  w    i  j       d(i)=\sum\limits_{j}w_{ij}     Also, let D be an    n  ×  n      n  n    n\times n   diagonal matrix with   d   d   d   on the diagonal, and let   W   W   W   be an    n  ×  n      n  n    n\times n   symmetric matrix with     W   i  j    =   w   i  j         subscript  W    i  j     subscript  w    i  j      W_{ij}=w_{ij}   .  After some algebraic manipulations, we get:        min   (  S  ,   S  ¯   )     ncut   (  S  ,   S  ¯   )     =    min  y      y  T    (   D  -  W   )   y     y  T   D  y            subscript    S   normal-¯  S      ncut  S   normal-¯  S       subscript   y        superscript  y  T     D  W   y      superscript  y  T   D  y       \min\limits_{(S,\overline{S})}\operatorname{ncut}(S,\overline{S})=\min\limits_%
 {y}\frac{y^{T}(D-W)y}{y^{T}Dy}     subject to the constraints:        y  i   ∈   {  1  ,   -  b   }        subscript  y  i    1    b      y_{i}\in\{1,-b\}   , for some constant    -  b      b    -b           y  t   D  1   =  0         superscript  y  t   D  1   0    y^{t}D1=0      Minimizing      y  T    (   D  -  W   )   y     y  T   D  y          superscript  y  T     D  W   y      superscript  y  T   D  y     \frac{y^{T}(D-W)y}{y^{T}Dy}   subject to the constraints above is NP-hard . To make the problem tractable, we relax the constraints on   y   y   y   , and allow it to take real values. The relaxed problem can be solved by solving the generalized eigenvalue problem      (   D  -  W   )   y   =   λ  D  y           D  W   y     λ  D  y     (D-W)y=\lambda Dy   for the second smallest generalized eigenvalue.  The partitioning algorithm:   Given a set of features, set up a weighted graph    G  =   (  V  ,  E  )       G   V  E     G=(V,E)   , compute the weight of each edge, and summarize the information in   D   D   D   and   W   W   W   .  Solve      (   D  -  W   )   y   =   λ  D  y           D  W   y     λ  D  y     (D-W)y=\lambda Dy   for eigenvectors with the smallest eigenvalues.  Use the eigenvector with the second smallest eigenvalue to bipartition the graph (e.g. grouping according to sign).  Decide if the current partition should be subdivided.  Recursively partition the segmented parts, if necessary.   Computational Complexity  Solving a standard eigenvalue problem for all eigenvectors (using the QR algorithm , for instance) takes    O   (   n  3   )       O   superscript  n  3     O(n^{3})   time. This is impractical for image segmentation applications where   n   n   n   is the number of pixels in the image.  Since only one eigenvector, corresponding to the second smallest generalized eigenvalue, is used by the ncut algorithm, efficiency can be dramatically improved if the solve of the corresponding eigenvalue problem is performed in a matrix-free fashion , i.e., without explicitly manipulating with or even computing the matrix W, as, e.g., in the Lanczos algorithm . Matrix-free methods require only a function that performs a matrix-vector product for a given vector, on every iteration. For image segmentaion, the matrix W is typically sparse, with a number of nonzero entries    O   (  n  )       O  n    O(n)   , so such a matrix-vector product takes    O   (  n  )       O  n    O(n)   time.  For high-resolution images, the second eigenvalue is often ill-conditioned , leading to slow convergence of iterative eigenvalue solvers, such as the Lanczos algorithm . Preconditioning is a key technology accelerating the convergence, e.g., in the matrix-free LOBPCG method. Computing the eigenvector using an optimally preconditioned matrix-free method takes    O   (  n  )       O  n    O(n)   time, which is the optimal complexity, since the eigenvector has   n   n   n   components.  OBJ CUT  OBJ CUT 2 is an efficient method that automatically segments an object. The OBJ CUT method is a generic method, and therefore it is applicable to any object category model. Given an image D containing an instance of a known object category, e.g. cows, the OBJ CUT algorithm computes a segmentation of the object, that is, it infers a set of labels m .  Let m be a set of binary labels, and let   Θ   normal-Θ   \Theta   be a shape parameter(   Θ   normal-Θ   \Theta   is a shape prior on the labels from a layered pictorial structure (LPS) model). An energy function    E   (  m  ,  Θ  )       E   m  normal-Θ     E(m,\Theta)   is defined as follows.      E   (  m  ,  Θ  )   =  ∑   ϕ  x    (  D  |   m  x   )   +   ϕ  x    (   m  x   |  Θ  )   +  ∑   Ψ   x  y     (   m  x   ,   m  y   )   +  ϕ   (  D  |   m  x   ,   m  y   )      fragments  E   fragments  normal-(  m  normal-,  Θ  normal-)      subscript  ϕ  x    fragments  normal-(  D  normal-|   subscript  m  x   normal-)     subscript  ϕ  x    fragments  normal-(   subscript  m  x   normal-|  Θ  normal-)      subscript  normal-Ψ    x  y     fragments  normal-(   subscript  m  x   normal-,   subscript  m  y   normal-)    ϕ   fragments  normal-(  D  normal-|   subscript  m  x   normal-,   subscript  m  y   normal-)     E(m,\Theta)=\sum\phi_{x}(D|m_{x})+\phi_{x}(m_{x}|\Theta)+\sum\Psi_{xy}(m_{x},m%
 _{y})+\phi(D|m_{x},m_{y})   (1)  The term     ϕ  x    (  D  |   m  x   )   +   ϕ  x    (   m  x   |  Θ  )      fragments   subscript  ϕ  x    fragments  normal-(  D  normal-|   subscript  m  x   normal-)     subscript  ϕ  x    fragments  normal-(   subscript  m  x   normal-|  Θ  normal-)     \phi_{x}(D|m_{x})+\phi_{x}(m_{x}|\Theta)   is called a unary term, and the term     Ψ   x  y     (   m  x   ,   m  y   )   +  ϕ   (  D  |   m  x   ,   m  y   )      fragments   subscript  normal-Ψ    x  y     fragments  normal-(   subscript  m  x   normal-,   subscript  m  y   normal-)    ϕ   fragments  normal-(  D  normal-|   subscript  m  x   normal-,   subscript  m  y   normal-)     \Psi_{xy}(m_{x},m_{y})+\phi(D|m_{x},m_{y})   is called a pairwise term. A unary term consists of the likelihood     ϕ  x    (  D  |   m  x   )      fragments   subscript  ϕ  x    fragments  normal-(  D  normal-|   subscript  m  x   normal-)     \phi_{x}(D|m_{x})   based on color, and the unary potential     ϕ  x    (   m  x   |  Θ  )      fragments   subscript  ϕ  x    fragments  normal-(   subscript  m  x   normal-|  Θ  normal-)     \phi_{x}(m_{x}|\Theta)   based on the distance from   Θ   normal-Θ   \Theta   . A pairwise term consists of a prior     Ψ   x  y     (   m  x   ,   m  y   )        subscript  normal-Ψ    x  y      subscript  m  x    subscript  m  y      \Psi_{xy}(m_{x},m_{y})   and a contrast term    ϕ   (  D  |   m  x   ,   m  y   )      fragments  ϕ   fragments  normal-(  D  normal-|   subscript  m  x   normal-,   subscript  m  y   normal-)     \phi(D|m_{x},m_{y})   .  The best labeling    m  *     superscript  m     m^{*}   minimizes     ∑  i     w  i   E   (  m  ,   Θ  i   )        subscript   i      subscript  w  i   E   m   subscript  normal-Θ  i       \sum\limits_{i}w_{i}E(m,\Theta_{i})   , where    w  i     subscript  w  i    w_{i}   is the weight of the parameter    Θ  i     subscript  normal-Θ  i    \Theta_{i}   .       m  *   =    arg   min  m      ∑  i     w  i   E   (  m  ,   Θ  i   )           superscript  m         subscript   m      subscript   i      subscript  w  i   E   m   subscript  normal-Θ  i         m^{*}=\arg\min\limits_{m}\sum\limits_{i}w_{i}E(m,\Theta_{i})   (2)  Algorithm   Given an image D, an object category is chosen, e.g. cows or horses.  The corresponding LPS model is matched to D to obtain the samples     Θ  1   ,  ⋯  ,   Θ  s       subscript  normal-Θ  1   normal-⋯   subscript  normal-Θ  s     \Theta_{1},\cdots,\Theta_{s}     The objective function given by equation (2) is determined by computing    E   (  m  ,   Θ  i   )       E   m   subscript  normal-Θ  i      E(m,\Theta_{i})   and using     w  i   =  g   (   Θ  i   |  Z  )      fragments   subscript  w  i    g   fragments  normal-(   subscript  normal-Θ  i   normal-|  Z  normal-)     w_{i}=g(\Theta_{i}|Z)     The objective function is minimized using a single MINCUT operation to obtain the segmentation m .   Other approaches   Jigsaw approach 3  Image parsing 4  Interleaved segmentation 5  LOCUS 6  LayoutCRF 7  Minimum spanning tree-based segmentation   References  "  Category:Object recognition and categorization     Jianbo Shi and Jitendra Malik (1997): "Normalized Cuts and Image Segmentation", IEEE Conference on Computer Vision and Pattern Recognition, pp 731–737 ↩  M. P. Kumar, P. H. S. Torr, and A. Zisserman. Obj cut. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition , San Diego, pages 18–25, 2005. ↩  E. Borenstein, S. Ullman: Class-specic, top-down segmentation. In Proceedings of the 7th European Conference on Computer Vision, Copenhagen, Denmark, pages 109–124, 2002. ↩  Z. Tu, X. Chen, A. L. Yuille, S. C. Zhu: Image Parsing: Unifying Segmentation, Detection, and Recognition. Toward Category-Level Object Recognition 2006: 545–576 ↩  B. Leibe, A. Leonardis, B. Schiele: An Implicit Shape Model for Combined Object Categorization and Segmentation. Toward Category-Level Object Recognition 2006: 508–524 ↩  J. Winn, N. Joijic. Locus: Learning object classes with unsupervised segmentation. In Proceedings of the IEEE International Conference on Computer Vision, Beijing, 2005. ↩  J. M. Winn, J. Shotton: The Layout Consistent Random Field for Recognizing and Segmenting Partially Occluded Objects. CVPR (1) 2006: 37–44 ↩     