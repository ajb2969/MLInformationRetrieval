   Stationary process      Stationary process   In mathematics and statistics , a stationary process (or strict(ly) stationary process or strong(ly) stationary process ) is a stochastic process whose joint probability distribution does not change when shifted in time. Consequently, parameters such as the mean and variance , if they are present, also do not change over time and do not follow any trends.  Stationarity is used as a tool in time series analysis , where the raw data is often transformed to become stationary; for example, economic data are often seasonal and/or dependent on a non-stationary price level. An important type of non-stationary process that does not include a trend-like behavior is the cyclostationary process .  Note that a "stationary process" is not the same thing as a "process with a stationary distribution ". Indeed there are further possibilities for confusion with the use of "stationary" in the context of stochastic processes; for example a "time-homogeneous" Markov chain is sometimes said to have "stationary transition probabilities". Besides, all stationary Markov random processes are time-homogeneous.  Definition  Formally, let    {   X  t   }      subscript  X  t     \left\{X_{t}\right\}   be a stochastic process and let     F  X    (   x    t  1   +  œÑ    ,  ‚Ä¶  ,   x    t  k   +  œÑ    )        subscript  F  X     subscript  x     subscript  t  1   œÑ    normal-‚Ä¶   subscript  x     subscript  t  k   œÑ       F_{X}(x_{t_{1}+\tau},\ldots,x_{t_{k}+\tau})   represent the cumulative distribution function of the joint distribution of    {   X  t   }      subscript  X  t     \left\{X_{t}\right\}   at times      t  1   +  œÑ   ,  ‚Ä¶  ,    t  k   +  œÑ         subscript  t  1   œÑ   normal-‚Ä¶     subscript  t  k   œÑ     t_{1}+\tau,\ldots,t_{k}+\tau   . Then,    {   X  t   }      subscript  X  t     \left\{X_{t}\right\}   is said to be strictly(or strongly) stationary if, for all   k   k   k   , for all   œÑ   œÑ   \tau   , and for all     t  1   ,  ‚Ä¶  ,   t  k       subscript  t  1   normal-‚Ä¶   subscript  t  k     t_{1},\ldots,t_{k}   ,         F  X    (   x    t  1   +  œÑ    ,  ‚Ä¶  ,   x    t  k   +  œÑ    )    =    F  X    (   x   t  1    ,  ‚Ä¶  ,   x   t  k    )     .         subscript  F  X     subscript  x     subscript  t  1   œÑ    normal-‚Ä¶   subscript  x     subscript  t  k   œÑ         subscript  F  X     subscript  x   subscript  t  1    normal-‚Ä¶   subscript  x   subscript  t  k        F_{X}(x_{t_{1}+\tau},\ldots,x_{t_{k}+\tau})=F_{X}(x_{t_{1}},\ldots,x_{t_{k}}).     Since   œÑ   œÑ   \tau   does not affect     F  X    (  ‚ãÖ  )        subscript  F  X   normal-‚ãÖ    F_{X}(\cdot)   ,    F  X     subscript  F  X    F_{X}   is not a function of time.  Examples  As an example, white noise is stationary. The sound of a cymbal clashing, if hit only once, is not stationary because the acoustic power of the clash (and hence its variance) diminishes with time. However, it would be possible to invent a stochastic process describing when the cymbal is hit, such that the overall response would form a stationary process. For example, if the cymbal were hit at moments in time corresponding to a homogeneous Poisson Process , the overall response would be stationary.  An example of a discrete-time stationary process where the sample space is also discrete (so that the random variable may take one of N possible values) is a Bernoulli scheme . Other examples of a discrete-time stationary process with continuous sample space include some autoregressive and moving average processes which are both subsets of the autoregressive moving average model . Models with a non-trivial autoregressive component may be either stationary or non-stationary, depending on the parameter values, and important non-stationary special cases are where unit roots exist in the model.  Let Y be any scalar random variable , and define a time-series { X t }, by        X  t   =   Y   for all  t     .       subscript  X  t    Y    for all  t      X_{t}=Y\qquad\text{ for all }t.   . Then { X t } is a stationary time series, for which realisations consist of a series of constant values, with a different constant value for each realisation. A law of large numbers does not apply on this case, as the limiting value of an average from a single realisation takes the random value determined by Y , rather than taking the expected value of Y.  As a further example of a stationary process for which any single realisation has an apparently noise-free structure, let Y have a uniform distribution on (0,2œÄ] and define the time series { X t } by         X  t   =   cos   (   t  +  Y   )       for  t   ‚àà  ‚Ñù    .     formulae-sequence     subscript  X  t       t  Y         for  t   ‚Ñù     X_{t}=\cos(t+Y)\quad\text{ for }t\in\mathbb{R}.   Then { X t } is strictly stationary.  Weaker forms of stationarity  Weak or wide-sense stationarity  A weaker form of stationarity commonly employed in signal processing is known as weak-sense stationarity , wide-sense stationarity (WSS), covariance stationarity , or second-order stationarity . WSS random processes only require that 1st moment and autocovariance do not vary with respect to time. Any strictly stationary process which has a mean and a covariance is also WSS.  So, a continuous -time random process  x ( t ) which is WSS has the following restrictions on its mean function       ùîº   [   x   (  t  )    ]    =    m  x    (  t  )    =    m  x    (   t  +  œÑ   )    for all   œÑ   ‚àà  ‚Ñù          ùîº   delimited-[]    x  t        subscript  m  x   t           subscript  m  x     t  œÑ   for all  œÑ        ‚Ñù     \mathbb{E}[x(t)]=m_{x}(t)=m_{x}(t+\tau)\,\,\text{ for all }\,\tau\in\mathbb{R}     and autocovariance function        ùîº   [    (    x   (   t  1   )    -    m  x    (   t  1   )     )    (    x   (   t  2   )    -    m  x    (   t  2   )     )    ]    =    C  x    (   t  1   ,   t  2   )    =    C  x    (    t  1   +   (   -   t  2    )    ,    t  2   +   (   -   t  2    )    )    =    C  x    (    t  1   -   t  2    ,  0  )     .          ùîº   delimited-[]        x   subscript  t  1       subscript  m  x    subscript  t  1         x   subscript  t  2       subscript  m  x    subscript  t  2           subscript  C  x     subscript  t  1    subscript  t  2             subscript  C  x       subscript  t  1      subscript  t  2        subscript  t  2      subscript  t  2               subscript  C  x       subscript  t  1    subscript  t  2    0       \mathbb{E}[(x(t_{1})-m_{x}(t_{1}))(x(t_{2})-m_{x}(t_{2}))]=C_{x}(t_{1},t_{2})=%
 C_{x}(t_{1}+(-t_{2}),t_{2}+(-t_{2}))=C_{x}(t_{1}-t_{2},0).     The first property implies that the mean function m x ( t ) must be constant. The second property implies that the covariance function depends only on the difference between    t  1     subscript  t  1    t_{1}   and    t  2     subscript  t  2    t_{2}   and only needs to be indexed by one variable rather than two variables. Thus, instead of writing,       C  x    (    t  1   -   t  2    ,  0  )        subscript  C  x       subscript  t  1    subscript  t  2    0     \,\!C_{x}(t_{1}-t_{2},0)\,     the notation is often abbreviated and written as:         C  x    (  œÑ  )   where  œÑ   =    t  1   -   t  2     .         subscript  C  x   œÑ  where  œÑ      subscript  t  1    subscript  t  2      C_{x}(\tau)\,\!\mbox{ where }\tau=t_{1}-t_{2}.     This also implies that the autocorrelation depends only on    œÑ  =    t  1   -   t  2        œÑ     subscript  t  1    subscript  t  2      \tau=t_{1}-t_{2}   , since         R  x    (   t  1   ,   t  2   )    =    R  x    (    t  1   -   t  2    )     .         subscript  R  x     subscript  t  1    subscript  t  2        subscript  R  x      subscript  t  1    subscript  t  2       \,\!R_{x}(t_{1},t_{2})=R_{x}(t_{1}-t_{2}).     The main advantage of wide-sense stationarity is that it places the time-series in the context of Hilbert spaces . Let H be the Hilbert space generated by { x ( t )} (that is, the closure of the set of all linear combinations of these random variables in the Hilbert space of all square-integrable random variables on the given probability space). By the positive definiteness of the autocovariance function, it follows from Bochner's theorem that there exists a positive measure Œº on the real line such that H is isomorphic to the Hilbert subspace of L 2 ( Œº ) generated by { e ‚àí2œÄiŒæ‚ãÖt }. This then gives the following Fourier-type decomposition for continuous time stationary stochastic process: there exists a stochastic process œâ Œæ with orthogonal increments such that, for all t        x   (  t  )    =   ‚à´    e   -    2  œÄ  i  Œª   ‚ãÖ  t     d   œâ  Œª      ,        x  t        superscript  e     normal-‚ãÖ    2  œÄ  i  Œª   t     d   subscript  œâ  Œª       x(t)=\int e^{-2\pi i\lambda\cdot t}d\omega_{\lambda},     where the integral on the right hand side is interpreted in a suitable (Riemann) sense. Same result holds for a discrete-time stationary process, with the spectral measure now defined on the unit circle.  When processing WSS random signals with linear , time-invariant ( LTI ) filters , it is helpful to think of the correlation function as a linear operator . Since it is a circulant operator (depends only on the difference between the two arguments), its eigenfunctions are the Fourier complex exponentials. Additionally, since the eigenfunctions of LTI operators are also complex exponentials , LTI processing of WSS random signals is highly tractable‚Äîall computations can be performed in the frequency domain . Thus, the WSS assumption is widely employed in signal processing algorithms .  Other terminology  The terminology used for types of stationarity other than strict stationarity can be rather mixed. Some examples follow.  :* Priestley uses stationary up to order  m if conditions similar to those given here for wide sense stationarity apply relating to moments up to order m . 1 2 Thus wide sense stationarity would be equivalent to "stationary to order 2", which is different from the definition of second-order stationarity given here.  :* Honarkhah and Caers also use the assumption of stationarity in the context of multiple-point geostatistics, where higher n-point statistics are assumed to be stationary in the spatial domain. 3  See also   L√©vy process  Stationary ergodic process  Wiener‚ÄìKhinchin theorem  Ergodicity  Statistical regularity   References  Further reading     External links   Spectral decomposition of a random function (Springer)   "  Category:Stochastic processes  Category:Signal processing     ‚Ü©  ‚Ü©  ‚Ü©     