   Second moment method      Second moment method   In mathematics, the second moment method is a technique used in probability theory and analysis to show that a random variable has positive probability of being positive. More generally, the "moment method" consists of bounding the probability that a random variable fluctuates far from its mean, by using its moments. 1  The method is often quantitative, in that one can often deduce a lower bound on the probability that the random variable is larger than some constant times its expectation. The method involves comparing the second moment of random variables to the square of the first moment.  First moment method  The first moment method is a simple application of Markov's inequality for integer-valued variables. For a non-negative , integer-valued random variable X , we may want to prove that X = 0 with high probability. To obtain an upper bound for P( X > 0), and thus a lower bound for P( X = 0), we first note that since X takes only integer values, P( X > 0) = P( X ≥ 1). Since X is non-negative we can now apply Markov's inequality to obtain P( X ≥ 1) ≤ E[ X ]. Combining these we have P( X > 0) ≤ E[ X ]; the first moment method is simply the use of this inequality.  Second moment method  In the other direction, E[ X ] being "large" does not directly imply that P( X = 0) is small. However, we can often use the second moment to derive such a conclusion, using Cauchy–Schwarz inequality .  Theorem : If X ≥ 0 is a random variable with finite variance, then        P   (   X  >  0   )    ≥     (   E   [  X  ]    )   2    E   [   X  2   ]      .       normal-P    X  0       superscript   normal-E  X   2    normal-E   superscript  X  2       \operatorname{P}(X>0)\geq\frac{(\operatorname{E}[X])^{2}}{\operatorname{E}[X^{%
 2}]}.     Proof : Using the Cauchy-Schwarz inequality , we have      E   [  X  ]   =  E   [  X   1   {  X  >  0  }    ]   ≤  E    [   X  2   ]    1  /  2    P    (  X  >  0  )    1  /  2    .     fragments  normal-E   fragments  normal-[  X  normal-]    normal-E   fragments  normal-[  X   subscript  1   fragments  normal-{  X   0  normal-}    normal-]    normal-E   superscript   fragments  normal-[   superscript  X  2   normal-]     1  2    normal-P   superscript   fragments  normal-(  X   0  normal-)     1  2    normal-.    \operatorname{E}[X]=\operatorname{E}[X\,\mathbf{1}_{\{X>0\}}]\leq\operatorname%
 {E}[X^{2}]^{1/2}\operatorname{P}(X>0)^{1/2}.   Solving for    P   (   X  >  0   )      normal-P    X  0     \operatorname{P}(X>0)   , the desired inequality then follows. ∎  The method can also be used on distributional limits of random variables. Furthermore, the estimate of the previous theorem can be refined by means of the so-called Paley–Zygmund inequality . Suppose that X n is a sequence of non-negative real-valued random variables which converge in law to a random variable X . If there are finite positive constants c 1 , c 2 such that       E   [   X  n  2   ]    ≤    c  1   E    [   X  n   ]   2          E   delimited-[]   superscript   subscript  X  n   2        subscript  c  1   E   superscript   delimited-[]   subscript  X  n    2      E\left[X_{n}^{2}\right]\leq c_{1}E[X_{n}]^{2}          E   [   X  n   ]    ≥   c  2         E   delimited-[]   subscript  X  n      subscript  c  2     E\left[X_{n}\right]\geq c_{2}     hold for every n , then it follows from the Paley–Zygmund inequality that for every n and θ in (0, 1)      P   (   X  n   ≥   c  2   θ  )   ≥     (   1  -  θ   )   2    c  1    .     fragments  P   fragments  normal-(   subscript  X  n     subscript  c  2   θ  normal-)       superscript    1  θ   2    subscript  c  1    normal-.    P(X_{n}\geq c_{2}\theta)\geq\frac{(1-\theta)^{2}}{c_{1}}.     Consequently, the same inequality is satisfied by X .  Example application of method  Setup of problem  The Bernoulli bond percolation  subgraph of a graph G at parameter p is a random subgraph obtained from G by deleting every edge of G with probability 1− p , independently. The infinite complete binary tree  T is an infinite tree where one vertex (called the root) has two neighbors and every other vertex has three neighbors. The second moment method can be used to show that at every parameter p ∈ (1/2, 1] with positive probability the connected component of the root in the percolation subgraph of T is infinite.  Application of method  Let K be the percolation component of the root, and let T n be the set of vertices of T that are at distance n from the root. Let X n be the number of vertices in T n ∩ K . To prove that K is infinite with positive probability, it is enough to show that      lim sup   n  →  ∞     1    X  n   >  0     >  0        subscript  limit-supremum   normal-→  n      subscript  1     subscript  X  n   0     0    \limsup_{n\to\infty}1_{X_{n}>0}>0   with positive probability. By the reverse Fatou lemma , it suffices to show that     inf   n  →  ∞    P   (   X  n   >  0  )   >  0     fragments   subscript  infimum   normal-→  n     P   fragments  normal-(   subscript  X  n    0  normal-)    0    \inf_{n\to\infty}P(X_{n}>0)>0   . The Cauchy–Schwarz inequality gives      E    [   X  n   ]   2   ≤  E   [   X  n  2   ]   E   [    (   1    X  n   >  0    )   2   ]   =  E   [   X  n  2   ]   P   (   X  n   >  0  )   .     fragments  E   superscript   fragments  normal-[   subscript  X  n   normal-]   2    E   fragments  normal-[   superscript   subscript  X  n   2   normal-]   E   fragments  normal-[   superscript   fragments  normal-(   subscript  1     subscript  X  n   0    normal-)   2   normal-]    E   fragments  normal-[   superscript   subscript  X  n   2   normal-]   P   fragments  normal-(   subscript  X  n    0  normal-)   normal-.    E[X_{n}]^{2}\leq E[X_{n}^{2}]\,E\left[(1_{X_{n}>0})^{2}\right]=E[X_{n}^{2}]\,P%
 (X_{n}>0).   Therefore, it is sufficient to show that         inf  n     E    [   X  n   ]   2     E   [   X  n  2   ]      >   0    ,        subscript  infimum  n       E   superscript   delimited-[]   subscript  X  n    2      E   delimited-[]   superscript   subscript  X  n   2       0    \inf_{n}\frac{E\left[X_{n}\right]^{2}}{E\left[X_{n}^{2}\right]}>0\,,   that is, that the second moment is bounded from above by a constant times the first moment squared (and both are nonzero). In many applications of the second moment method, one is not able to calculate the moments precisely, but can nevertheless establish this inequality.  In this particular application, these moments can be calculated. For every specific v in T n ,      P   (  v  ∈  K  )   =   p  n   .     fragments  P   fragments  normal-(  v   K  normal-)     superscript  p  n   normal-.    P(v\in K)=p^{n}.\,     Since     |   T  n   |   =   2  n          subscript  T  n     superscript  2  n     |T_{n}|=2^{n}   , it follows that       E   [   X  n   ]    =     2  n     p  n          E   delimited-[]   subscript  X  n        superscript  2  n    superscript  p  n      E[X_{n}]=2^{n}\,p^{n}     which is the first moment. Now comes the second moment calculation.       E    [   X  n  2   ]   =   E    [   ∑   v  ∈   T  n      ∑   u  ∈   T  n      1   v  ∈  K     1   u  ∈  K    ]   =   ∑   v  ∈   T  n      ∑   u  ∈   T  n     P   (  v  ,  u  ∈  K  )   .     fragments  E   fragments  normal-[   superscript   subscript  X  n   2   normal-]    E   fragments  normal-[   subscript     v   subscript  T  n      subscript     u   subscript  T  n      subscript  1    v  K     subscript  1    u  K    normal-]     subscript     v   subscript  T  n      subscript     u   subscript  T  n     P   fragments  normal-(  v  normal-,  u   K  normal-)   normal-.    E\!\left[X_{n}^{2}\right]=E\!\left[\sum_{v\in T_{n}}\sum_{u\in T_{n}}1_{v\in K%
 }\,1_{u\in K}\right]=\sum_{v\in T_{n}}\sum_{u\in T_{n}}P(v,u\in K).     For each pair v , u in T n let w(v, u) denote the vertex in T that is farthest away from the root and lies on the simple path in T to each of the two vertices v and u , and let k(v, u) denote the distance from w to the root. In order for v , u to both be in K , it is necessary and sufficient for the three simple paths from w(v, u) to v , u and the root to be in K . Since the number of edges contained in the union of these three paths is 2 n − k(v, u) , we obtain      P   (  v  ,  u  ∈  K  )   =   p    2  n   -   k   (  v  ,  u  )      .     fragments  P   fragments  normal-(  v  normal-,  u   K  normal-)     superscript  p      2  n     k   v  u      normal-.    P(v,u\in K)=p^{2n-k(v,u)}.     The number of pairs (v, u) such that k(v, u) = s is equal to      2  s    2   n  -  s     2   n  -  s  -  1     =   2    2  n   -  s  -  1           superscript  2  s    superscript  2    n  s     superscript  2    n  s  1      superscript  2      2  n   s  1      2^{s}\,2^{n-s}\,2^{n-s-1}=2^{2n-s-1}   , for s = 0, 1, ..., n . Hence,         E    [   X  n  2   ]    =    ∑   s  =  0   n     2    2  n   -  s  -  1     p    2  n   -  s      =     1  2      (   2  p   )   n     ∑   s  =  0   n     (   2  p   )   s     =     1  2       (   2  p   )   n        (   2  p   )    n  +  1    -  1     2  p   -  1     ≤     p    2  p   -  1     E    [   X  n   ]   2     ,          E   delimited-[]   superscript   subscript  X  n   2       superscript   subscript     s  0    n      superscript  2      2  n   s  1     superscript  p      2  n   s               1  2    superscript    2  p   n     superscript   subscript     s  0    n    superscript    2  p   s              1  2    superscript    2  p   n        superscript    2  p     n  1    1       2  p   1              p      2  p   1    E   superscript   delimited-[]   subscript  X  n    2       E\!\left[X_{n}^{2}\right]=\sum_{s=0}^{n}2^{2n-s-1}p^{2n-s}=\frac{1}{2}\,(2p)^{%
 n}\sum_{s=0}^{n}(2p)^{s}=\frac{1}{2}\,(2p)^{n}\,\frac{(2p)^{n+1}-1}{2p-1}\leq%
 \frac{p}{2p-1}\,E[X_{n}]^{2},     which completes the proof.  Discussion   The choice of the random variables X n was rather natural in this setup. In some more difficult applications of the method, some ingenuity might be required in order to choose the random variables X n for which the argument can be carried through.  The Paley–Zygmund inequality is sometimes used instead of the Cauchy–Schwarz inequality and may occasionally give more refined results.  Under the (incorrect) assumption that the events v , u in K are always independent, one has    P   (  v  ,  u  ∈  K  )   =  P   (  v  ∈  K  )   P   (  u  ∈  K  )      fragments  P   fragments  normal-(  v  normal-,  u   K  normal-)    P   fragments  normal-(  v   K  normal-)   P   fragments  normal-(  u   K  normal-)     P(v,u\in K)=P(v\in K)\,P(u\in K)   , and the second moment is equal to the first moment squared. The second moment method typically works in situations in which the corresponding events or random variables are “nearly independent".  In this application, the random variables X n are given as sums           X  n   =    ∑   v  ∈   T  n      1   v  ∈  K      .       subscript  X  n     subscript     v   subscript  T  n      subscript  1    v  K       X_{n}=\sum_{v\in T_{n}}1_{v\in K}.       In other applications, the corresponding useful random variables are integrals        X  n   =   ∫    f  n    (  t  )   d  μ   (  t  )      ,       subscript  X  n        subscript  f  n   t  d  μ  t      X_{n}=\int f_{n}(t)\,d\mu(t),      where the functions f n are random. In such a situation, one considers the product measure μ × μ and calculates : \begin{align}    E \left[X_n^2 \right ] & = E\left[\int\int f_n(x)\,f_n(y)\,d\mu(x)\,d\mu(y)\right ] \\ & = E\left[ \int\int E\left[f_n(x)\,f_n(y)\right]\,d\mu(x)\,d\mu(y)\right ], \end{align}   where the last step is typically justified using Fubini's theorem .   References         "  Category:Probabilistic inequalities  Category:Articles containing proofs     ↩     