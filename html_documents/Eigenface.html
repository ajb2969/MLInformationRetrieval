<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="249">Eigenface</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Eigenface</h1>
<style>
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
<style>
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</body></html>
<body>
<hr/>

<p> </p>

<p><strong>Eigenfaces</strong> is the name given to a set of <a href="eigenvector" title="wikilink">eigenvectors</a> when they are used in the <a href="computer_vision" title="wikilink">computer vision</a> problem of human <a href="facial_recognition_system" title="wikilink">face recognition</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> The approach of using eigenfaces for <a href="Facial_recognition_system" title="wikilink">recognition</a> was developed by Sirovich and Kirby (1987) and used by <a href="Matthew_Turk" title="wikilink">Matthew Turk</a> and <a href="Alex_Pentland" title="wikilink">Alex Pentland</a> in face classification.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> The eigenvectors are derived from the <a href="covariance_matrix" title="wikilink">covariance matrix</a> of the <a href="probability_distribution" title="wikilink">probability distribution</a> over the high-<a href="dimension" title="wikilink">dimensional</a> <a href="vector_space" title="wikilink">vector space</a> of face images. The eigenfaces themselves form a basis set of all images used to construct the covariance matrix. This produces dimension reduction by allowing the smaller set of basis images to represent the original training images. Classification can be achieved by comparing how faces are represented by the basis set.</p>
<h2 id="history">History</h2>

<p>The Eigenface approach began with a search for a low-dimensional representation of face images. Sirovich and Kirby (1987) showed that <a href="Principal_Component_Analysis" title="wikilink">Principal Component Analysis</a> could be used on a collection of face images to form a set of basis features. These basis images, known as Eigenpictures, could be linearly combined to reconstruct images in the original training set. If the training set consists of <em>M</em> images, principal component analysis could form a basis set of <em>N</em> images, where ''N Turk, Matthew A and Pentland, Alex P. <em>Face recognition using eigenfaces</em>. Computer Vision and Pattern Recognition, 1991. Proceedings {CVPR'91.}, {IEEE} Computer Society Conference on 1991 In addition to designing a system for automated face recognition using eigenfaces, they showed a way of calculating the <a class="uri" href="eigenvectors" title="wikilink">eigenvectors</a> of a <a href="covariance_matrix" title="wikilink">covariance matrix</a> in such a way as to make it possible for computers at that time to perform eigen-decomposition on a large number of face images. Face images usually occupy a high-dimensional space and conventional principal component analysis was intractable on such data sets. Turk and Pentland's paper demonstrated ways to extract the eigenvectors based on matrices sized by the number of images rather than the number of pixels.</p>

<p>Once established, the eigenface method was expanded to include methods of preprocessing to improve accuracy.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> Multiple manifold approaches were also used to build sets of eigenfaces for different subjects<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> and different features, such as the eyes.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h2 id="eigenface-generation">Eigenface generation</h2>

<p>A <strong>set of eigenfaces</strong> can be generated by performing a mathematical process called <a href="principal_component_analysis" title="wikilink">principal component analysis</a> (PCA) on a large set of images depicting different human faces. Informally, eigenfaces can be considered a set of "standardized face ingredients", derived from <a href="statistical_analysis" title="wikilink">statistical analysis</a> of many pictures of faces. Any human face can be considered to be a combination of these standard faces. For example, one's face might be composed of the average face plus 10% from eigenface 1, 55% from eigenface 2, and even -3% from eigenface 3. Remarkably, it does not take many eigenfaces combined together to achieve a fair approximation of most faces. Also, because a person's face is not recorded by a <a href="digital_photograph" title="wikilink">digital photograph</a>, but instead as just a list of values (one value for each eigenface in the database used), much less space is taken for each person's face.</p>

<p>The eigenfaces that are created will appear as light and dark areas that are arranged in a specific pattern. This pattern is how different features of a face are singled out to be evaluated and scored. There will be a pattern to evaluate <a class="uri" href="symmetry" title="wikilink">symmetry</a>, if there is any style of facial hair, where the hairline is, or evaluate the size of the nose or mouth. Other eigenfaces have patterns that are less simple to identify, and the image of the eigenface may look very little like a face.</p>

<p>The technique used in creating eigenfaces and using them for recognition is also used outside of facial recognition. This technique is also used for <a href="Graphology" title="wikilink">handwriting analysis</a>, <a href="lip_reading" title="wikilink">lip reading</a>, <a href="Speaker_recognition" title="wikilink">voice recognition</a>, <a href="sign_language" title="wikilink">sign language</a>/hand <a class="uri" href="gestures" title="wikilink">gestures</a> interpretation and <a href="medical_imaging" title="wikilink">medical imaging</a> analysis. Therefore, some do not use the term eigenface, but prefer to use 'eigenimage'.</p>
<h3 id="practical-implementation">Practical implementation</h3>

<p>To create a set of eigenfaces, one must:</p>
<ol>
<li>Prepare a training set of face images. The pictures constituting the training set should have been taken under the same lighting conditions, and must be normalized to have the eyes and mouths aligned across all images. They must also be all resampled to a common <a class="uri" href="pixel" title="wikilink">pixel</a> resolution (<em>r</em> × <em>c</em>). Each image is treated as one vector, simply by <a href="concatenation" title="wikilink">concatenating</a> the rows of pixels in the original image, resulting in a single row with <em>r</em> × <em>c</em> elements. For this implementation, it is assumed that all images of the training set are stored in a single <a href="Matrix_(mathematics)" title="wikilink">matrix</a> <strong>T</strong>, where each column of the matrix is an image.</li>
<li>Subtract the <a class="uri" href="mean" title="wikilink">mean</a>. The average image <strong>a</strong> has to be calculated and then subtracted from each original image in <strong>T</strong>.</li>
<li>Calculate the <a href="eigenvectors_and_eigenvalues" title="wikilink">eigenvectors and eigenvalues</a> of the <a href="covariance_matrix" title="wikilink">covariance matrix</a> <strong>S</strong>. Each eigenvector has the same dimensionality (number of components) as the original images, and thus can itself be seen as an image. The eigenvectors of this covariance matrix are therefore called eigenfaces. They are the directions in which the images differ from the mean image. Usually this will be a computationally expensive step (if at all possible), but the practical applicability of eigenfaces stems from the possibility to compute the eigenvectors of <strong>S</strong> efficiently, without ever computing <strong>S</strong> explicitly, as detailed below.</li>
<li>Choose the principal components. Sort the eigenvalues in descending order and arrange eigenvectors accordingly. The number of principal components k is determined arbitrarily by setting a threshold ε on the total variance. Total variance v = n*(λ1+ λ2+…+ λn), n= number of data images by</li>
<li>k is the smallest number satisfies 

<math display="block" id="Eigenface:0">
 <semantics>
  <mrow>
   <mfrac>
    <mrow>
     <mi>n</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <msub>
        <mi>λ</mi>
        <mn>1</mn>
       </msub>
       <mo>+</mo>
       <msub>
        <mi>λ</mi>
        <mn>2</mn>
       </msub>
       <mo>+</mo>
       <mi mathvariant="normal">…</mi>
       <mo>+</mo>
       <msub>
        <mi>λ</mi>
        <mi>k</mi>
       </msub>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mi>v</mi>
   </mfrac>
   <mo>></mo>
   <mi>ϵ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <ci>n</ci>
      <apply>
       <plus></plus>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>λ</ci>
        <cn type="integer">1</cn>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>λ</ci>
        <cn type="integer">2</cn>
       </apply>
       <ci>normal-…</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>λ</ci>
        <ci>k</ci>
       </apply>
      </apply>
     </apply>
     <ci>v</ci>
    </apply>
    <ci>ϵ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{n(\lambda_{1}+\lambda_{2}+...+\lambda_{k})}{v}>\epsilon
  </annotation>
 </semantics>
</math>

</li>
</ol>

<p>These eigenfaces can now be used to represent both existing and new faces: we can project a new (mean-subtracted) image on the eigenfaces and thereby record how that new face differs from the mean face. The eigenvalues associated with each eigenface represent how much the images in the training set vary from the mean image in that direction. We lose information by projecting the image on a subset of the eigenvectors, but we minimize this loss by keeping those eigenfaces with the largest eigenvalues. For instance, if we are working with a 100 x 100 image, then we will obtain 10,000 eigenvectors. In practical applications, most faces can typically be identified using a projection on between 100 and 150 eigenfaces, so that most of the 10,000 eigenvectors can be discarded.</p>
<h3 id="matlab-example-code">Matlab example code</h3>

<p>Here is an example of calculating eigenfaces with Extended Yale Face Database B. To evade computational and storage bottleneck, the face images are sampled down by a factor 4x4=16.</p>
<div class="sourceCode"><pre class="sourceCode matlab"><code class="sourceCode matlab">clear all ;
close all; 
load yalefaces 
[h,w,n] = size(yalefaces); 
d = h*w; 
<span class="co">% vectorize images </span>
x = reshape(yalefaces,[d n]); 
x = double(x); 
<span class="co">%subtract mean </span>
x=bsxfun(@minus, x’, mean(x’))’; 
<span class="co">% calculate covariance </span>
s = cov(x’); 
<span class="co">% obtain eigenvalue &amp; eigenvector </span>
[V,D] = eig(s);
eigval = diag(D); 
<span class="co">% sort eigenvalues in descending order </span>
eigval = eigval(end:-<span class="fl">1</span>:<span class="fl">1</span>); 
V = fliplr(V); 
<span class="co">% show 0th through 15th principal eigenvectors </span>
eig0 = reshape(mean(x,<span class="fl">2</span>), [h,w]); 
figure,subplot(<span class="fl">4</span>,<span class="fl">4</span>,<span class="fl">1</span>) 
imagesc(eig0) 
colormap gray 
for i = <span class="fl">1</span>:<span class="fl">15</span> 
subplot(<span class="fl">4</span>,<span class="fl">4</span>,i+<span class="fl">1</span>) 
imagesc(reshape(V(:,i),h,w)) 
end </code></pre></div>

<p>Note that although the covariance matrix S generates many eigenfaces, only a fraction of those are needed to represent the majority of the faces. For example, to represent 95% of the total variation of all face images, only the first 43 eigenfaces are needed. To calculate this result, implement the following code:</p>
<div class="sourceCode"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span class="co">% evaluate the number of principal components needed to represent 95% Total variance. </span>
eigsum = sum(eigval); 
csum = <span class="fl">0</span>; 
for i = <span class="fl">1</span>:d 
csum = csum + eigval(i); 
tv = csum/eigsum; 
if tv &gt; <span class="fl">0.95</span> 
k95 = i; 
break 
end ;
end;</code></pre></div>
<h3 id="computing-the-eigenvectors">Computing the eigenvectors</h3>

<p>Performing PCA directly on the covariance matrix of the images is often computationally infeasible. If small, say 100 × 100, greyscale images are used, each image is a point in a 10,000-dimensional space and the covariance matrix <strong>S</strong> is a matrix of 10,000 × 10,000 = 10<sup>8</sup> elements. However the <a href="Rank_(linear_algebra)" title="wikilink">rank</a> of the covariance matrix is limited by the number of training examples: if there are <em>N</em> training examples, there will be at most <em>N</em> − 1 eigenvectors with non-zero eigenvalues. If the number of training examples is smaller than the dimensionality of the images, the principal components can be computed more easily as follows.</p>

<p>Let <strong>T</strong> be the matrix of preprocessed training examples, where each column contains one mean-subtracted image. The covariance matrix can then be computed as <strong>S</strong> = <strong>TT</strong><sup>T</sup> and the eigenvector decomposition of <strong>S</strong> is given by</p>

<p>

<math display="block" id="Eigenface:1">
 <semantics>
  <mrow>
   <msub>
    <mi>𝐒𝐯</mi>
    <mi>i</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msup>
     <mi>𝐓𝐓</mi>
     <mi>T</mi>
    </msup>
    <msub>
     <mi>𝐯</mi>
     <mi>i</mi>
    </msub>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>λ</mi>
     <mi>i</mi>
    </msub>
    <msub>
     <mi>𝐯</mi>
     <mi>i</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐒𝐯</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>𝐓𝐓</ci>
       <ci>T</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>𝐯</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>𝐯</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{Sv}_{i}=\mathbf{T}\mathbf{T}^{T}\mathbf{v}_{i}=\lambda_{i}\mathbf{v}_{i}
  </annotation>
 </semantics>
</math>

 However <strong>TT</strong><sup>T</sup> is a large matrix, and if instead we take the eigenvalue decomposition of</p>

<p>

<math display="block" id="Eigenface:2">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>𝐓</mi>
     <mi>T</mi>
    </msup>
    <msub>
     <mi>𝐓𝐮</mi>
     <mi>i</mi>
    </msub>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>λ</mi>
     <mi>i</mi>
    </msub>
    <msub>
     <mi>𝐮</mi>
     <mi>i</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>𝐓</ci>
      <ci>T</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐓𝐮</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>λ</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐮</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{T}^{T}\mathbf{T}\mathbf{u}_{i}=\lambda_{i}\mathbf{u}_{i}
  </annotation>
 </semantics>
</math>

 then we notice that by pre-multiplying both sides of the equation with <strong>T</strong>, we obtain</p>

<p>

<math display="block" id="Eigenface:3">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>𝐓𝐓</mi>
     <mi>T</mi>
    </msup>
    <msub>
     <mi>𝐓𝐮</mi>
     <mi>i</mi>
    </msub>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>λ</mi>
     <mi>i</mi>
    </msub>
    <msub>
     <mi>𝐓𝐮</mi>
     <mi>i</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>𝐓𝐓</ci>
      <ci>T</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐓𝐮</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>λ</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐓𝐮</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{T}\mathbf{T}^{T}\mathbf{T}\mathbf{u}_{i}=\lambda_{i}\mathbf{T}\mathbf{%
u}_{i}
  </annotation>
 </semantics>
</math>

 Meaning that, if <strong>u</strong><sub>i</sub> is an eigenvector of <strong>T</strong><sup>T</sup><strong>T</strong>, then <strong>v</strong><sub>i</sub> = <strong>Tu</strong><sub>i</sub> is an eigenvector of <strong>S</strong>. If we have a training set of 300 images of 100 × 100 pixels, the matrix <strong>T</strong><sup>T</sup><strong>T</strong> is a 300 × 300 matrix, which is much more manageable than the 10,000 × 10,000 covariance matrix. Notice however that the resulting vectors <strong>v</strong><sub>i</sub> are not normalised; if normalisation is required it should be applied as an extra step.</p>
<h3 id="connection-with-svd">Connection with SVD</h3>

<p>Let X denote the d x n data matrix with column xi as the image vector with mean subtracted. Then,</p>

<p><code>  </code>

<math display="inline" id="Eigenface:4">
 <semantics>
  <mrow>
   <mrow>
    <mi>c</mi>
    <mi>o</mi>
    <mi>v</mi>
    <mi>a</mi>
    <mi>r</mi>
    <mi>i</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>c</mi>
    <mi>e</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>X</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>X</mi>
     <msup>
      <mi>X</mi>
      <mi>T</mi>
     </msup>
    </mrow>
    <mi>n</mi>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>c</ci>
     <ci>o</ci>
     <ci>v</ci>
     <ci>a</ci>
     <ci>r</ci>
     <ci>i</ci>
     <ci>a</ci>
     <ci>n</ci>
     <ci>c</ci>
     <ci>e</ci>
     <ci>X</ci>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <ci>X</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>X</ci>
       <ci>T</ci>
      </apply>
     </apply>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   covariance(X)=\frac{XX^{T}}{n}
  </annotation>
 </semantics>
</math>

</p>

<p>Let the <a href="singular_value_decomposition" title="wikilink">singular value decomposition</a> (SVD) of X be:</p>

<p><code> </code>

<math display="inline" id="Eigenface:5">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>=</mo>
   <mrow>
    <mi>U</mi>
    <mi mathvariant="normal">Σ</mi>
    <msup>
     <mi>V</mi>
     <mi>T</mi>
    </msup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>X</ci>
    <apply>
     <times></times>
     <ci>U</ci>
     <ci>normal-Σ</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>V</ci>
      <ci>T</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X=U{\Sigma}V^{T}
  </annotation>
 </semantics>
</math>

</p>

<p>Then the eigenvalue decomposition for 

<math display="inline" id="Eigenface:6">
 <semantics>
  <mrow>
   <mi>X</mi>
   <msup>
    <mi>X</mi>
    <mi>T</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>X</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>X</ci>
     <ci>T</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   XX^{T}
  </annotation>
 </semantics>
</math>

 is:</p>

<p><code> </code>

<math display="inline" id="Eigenface:7">
 <semantics>
  <mrow>
   <mrow>
    <mi>X</mi>
    <msup>
     <mi>X</mi>
     <mi>T</mi>
    </msup>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>U</mi>
    <mi mathvariant="normal">Σ</mi>
    <msup>
     <mi mathvariant="normal">Σ</mi>
     <mi>T</mi>
    </msup>
    <msup>
     <mi>U</mi>
     <mi>T</mi>
    </msup>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>U</mi>
    <mi mathvariant="normal">Λ</mi>
    <msup>
     <mi>U</mi>
     <mi>T</mi>
    </msup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <ci>X</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>X</ci>
       <ci>T</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>U</ci>
      <ci>normal-Σ</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>normal-Σ</ci>
       <ci>T</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>U</ci>
       <ci>T</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>U</ci>
      <ci>normal-Λ</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>U</ci>
       <ci>T</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   XX^{T}=U{\Sigma}{{\Sigma}^{T}}U^{T}=U{\Lambda}U^{T}
  </annotation>
 </semantics>
</math>

<code>, where Λ=diag (eigenvalues of </code>

<math display="inline" id="Eigenface:8">
 <semantics>
  <mrow>
   <mi>X</mi>
   <msup>
    <mi>X</mi>
    <mi>T</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>X</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>X</ci>
     <ci>T</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   XX^{T}
  </annotation>
 </semantics>
</math>

<code>)</code></p>

<p>Thus we can see easily that:</p>

<p><code> The eigenfaces = </code>

<math display="inline" id="Eigenface:9">
 <semantics>
  <mi>U</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>U</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   U
  </annotation>
 </semantics>
</math>

<code>, the left singular vectors of </code>

<math display="inline" id="Eigenface:10">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

<br/>
<code> The ith eigenvalue of </code>

<math display="inline" id="Eigenface:11">
 <semantics>
  <mrow>
   <mi>X</mi>
   <msup>
    <mi>X</mi>
    <mi>T</mi>
   </msup>
   <mo>=</mo>
   <mfrac>
    <mn>1</mn>
    <mi>n</mi>
   </mfrac>
   <mo stretchy="false">(</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">X</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>X</ci>
     <ci>T</ci>
    </apply>
    <eq></eq>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <ci>n</ci>
    </apply>
    <ci>normal-(</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   XX^{T}=\frac{1}{n}(
  </annotation>
 </semantics>
</math>

<code> ith singular value of </code>

<math display="inline" id="Eigenface:12">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo stretchy="false">)</mo>
   <msup>
    <mi></mi>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">X</csymbol>
    <ci>normal-)</ci>
    <apply>
     <cn type="integer">2</cn>
    </apply>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X)^{2}
  </annotation>
 </semantics>
</math>

</p>

<p>Using SVD on data matrix X, we don’t need to calculate the actual covariance matrix to get eigenfaces.</p>
<h2 id="use-in-facial-recognition">Use in facial recognition</h2>

<p>Facial recognition was the source of motivation behind the creation of eigenfaces. For this use, eigenfaces have advantages over other techniques available, such as the system's speed and efficiency. As eigenface is primarily a dimension reduction method, a system can represent many subjects with a relatively small set of data. As a face recognition system it is also fairly invariant to large reductions in image sizing, however it begins to fail considerably when the variation between the seen images and probe image is large.</p>

<p>To recognise faces, gallery images, those seen by the system, are saved as collections of weights describing the contribution each eigenface has to that image. When a new face is presented to the system for classification, its own weights are found by projecting the image onto the collection of eigenfaces. This provides a set of weights describing the probe face. These weights are then classified against all weights in the gallery set to find the closest match. A nearest neighbour method is a simple approach for finding the <a href="Euclidean_Distance" title="wikilink">Euclidean Distance</a> between two vectors, where the minimum can be classified as the closest subject.</p>

<p>Intuitively, recognition process with eigenface method is to project query images into the face-space spanned by eigenfaces we have calculated and in that face-space find the closest match to a face class.</p>

<p><code> </code><strong><code>Pseudo</code> <code>code</code></strong><code>:</code><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a><br/>
<code> </code><br/>
<code> *  Given input image vector </code>

<math display="inline" id="Eigenface:13">
 <semantics>
  <mrow>
   <mi>U</mi>
   <mo>∈</mo>
   <msup>
    <mi mathvariant="normal">ℜ</mi>
    <mi>n</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>U</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <real></real>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   U\in\Re^{n}
  </annotation>
 </semantics>
</math>

<code>, the mean image vector from the database </code>

<math display="inline" id="Eigenface:14">
 <semantics>
  <mi>M</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>M</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   M
  </annotation>
 </semantics>
</math>

<code>, calculate the weight of the kth eigenface as:</code><br/>
<code>      </code>

<math display="inline" id="Eigenface:15">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mi>k</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mi>V</mi>
     <mi>k</mi>
     <mi>T</mi>
    </msubsup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>U</mi>
      <mo>-</mo>
      <mi>M</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <ci>k</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>V</ci>
       <ci>k</ci>
      </apply>
      <ci>T</ci>
     </apply>
     <apply>
      <minus></minus>
      <ci>U</ci>
      <ci>M</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{k}=V_{k}^{T}(U-M)
  </annotation>
 </semantics>
</math>

<br/>
<code>      Then form a weight vector </code>

<math display="inline" id="Eigenface:16">
 <semantics>
  <mrow>
   <mi>W</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">[</mo>
    <msub>
     <mi>w</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mn>2</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mi>k</mi>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">]</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>W</ci>
    <list>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>normal-…</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>k</ci>
     </apply>
     <ci>normal-…</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>n</ci>
     </apply>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W=[w_{1},w_{2},...,w_{k},...,w_{n}]
  </annotation>
 </semantics>
</math>

<br/>
<code> *  Compare W with weight vectors </code>

<math display="inline" id="Eigenface:17">
 <semantics>
  <msub>
   <mi>W</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>W</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W_{m}
  </annotation>
 </semantics>
</math>

<code> of images in the database. Find the Euclidean distance.</code><br/>
<code>      </code>

<math display="inline" id="Eigenface:18">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo fence="true">||</mo>
     <mrow>
      <mi>W</mi>
      <mo>-</mo>
      <msub>
       <mi>W</mi>
       <mi>m</mi>
      </msub>
     </mrow>
     <mo fence="true">||</mo>
    </mrow>
    <mn>2</mn>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>d</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <apply>
       <minus></minus>
       <ci>W</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>W</ci>
        <ci>m</ci>
       </apply>
      </apply>
     </apply>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d=||W-W_{m}||^{2}
  </annotation>
 </semantics>
</math>

<br/>
<code> *  If </code>

<math display="inline" id="Eigenface:19">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mo><</mo>
   <msub>
    <mi>ϵ</mi>
    <mn>1</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <lt></lt>
    <ci>d</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>ϵ</ci>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d<\epsilon_{1}
  </annotation>
 </semantics>
</math>

<code>, then the mth entry in the database is a candidate of recognition.</code><br/>
<code> *  If </code>

<math display="inline" id="Eigenface:20">
 <semantics>
  <mrow>
   <msub>
    <mi>ϵ</mi>
    <mn>1</mn>
   </msub>
   <mo><</mo>
   <mi>d</mi>
   <mo><</mo>
   <msub>
    <mi>ϵ</mi>
    <mn>2</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <lt></lt>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>ϵ</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>d</ci>
    </apply>
    <apply>
     <lt></lt>
     <share href="#.cmml">
     </share>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>ϵ</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \epsilon_{1}<d<\epsilon_{2}
  </annotation>
 </semantics>
</math>

<code>, then </code>

<math display="inline" id="Eigenface:21">
 <semantics>
  <mi>U</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>U</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   U
  </annotation>
 </semantics>
</math>

<code> may be an unknown face and can be added to the database.</code><br/>
<code> *  If </code>

<math display="inline" id="Eigenface:22">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mo>></mo>
   <mrow>
    <msub>
     <mi>ϵ</mi>
     <mn>2</mn>
    </msub>
    <mo>,</mo>
    <mi>U</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>d</ci>
    <list>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>ϵ</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>U</ci>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d>\epsilon_{2},U
  </annotation>
 </semantics>
</math>

<code> is not a face image.</code><br/>
</p>

<p>The weights of each gallery image only convey information describing that image, not that subject. An image of one subject under frontal lighting may have very different weights to those of the same subject under strong left lighting. This limits the application of such a system. Experiments in the original Eigenface paper presented the following results: an average of 96% with light variation, 85% with orientation variation, and 64% with size variation. </p>

<p>Various extensions have been made to the eigenface method such <a class="uri" href="eigenfeatures" title="wikilink">eigenfeatures</a>. This method combines <a href="facial_metrics" title="wikilink">facial metrics</a> (measuring distance between facial features) with the eigenface representation. Another method similar to the eigenface technique is '<a href="fisherface" title="wikilink">fisherfaces</a>' which uses <a href="Linear_discriminant_analysis" title="wikilink">Linear discriminant analysis</a>.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> This method for facial recognition is less sensitive to variation in lighting and pose of the face than using eigenfaces. Fisherface utilises labelled data to retain more of the class specific information during the dimension reduction stage.</p>

<p>A further alternative to eigenfaces and fisherfaces is the <a href="active_appearance_model" title="wikilink">active appearance model</a>. This approach uses an <a href="Active_Shape_Model" title="wikilink">Active Shape Model</a> to describe the outline of a face. By collecting many face outlines, <a href="Principal_Component_Analysis" title="wikilink">Principal Component Analysis</a> can be used to form a basis set of models which, encapsulate the variation of different faces.</p>

<p>Many modern approaches still use <a href="Principal_component_analysis" title="wikilink">Principal component analysis</a> as a means of dimension reduction or to form basis images for different modes of variation.</p>
<h2 id="review-on-eigenface">Review on Eigenface</h2>

<p>Eigenface provides an easy and cheap way to realize face recognition in that:</p>
<ul>
<li>Its training process is completely automatic and easy to code.</li>
<li>Eigenface adequately reduces statistical complexity in face image representation.</li>
<li>Once eigenfaces of a database are calculated, face recognition can be achieved in real time.</li>
<li>Eigenface can handle large databases.</li>
</ul>

<p>However, the deficiencies of the eigenface method are also obvious:</p>
<ul>
<li>Very sensitive to lighting, scale and translation; requires a highly controlled environment.</li>
<li>Eigenface has difficulty capturing expression changes.</li>
<li>The most significant eigenfaces are mainly about illumination encoding and don't provide useful information regarding the actual face.</li>
</ul>

<p>To cope with illumination distraction in practice, the eigenface method usually discards the first three eigenfaces from the dataset. Since illumination is usually the cause behind the largest variations in face images, the first three eigenfaces will mainly capture the information of 3-dimensional lighting changes, which has little contribution to face recognition. By discarding those three eigenfaces, there will be a decent amount of boost in accuracy of face recognition, but other methods such as Fisherface and Linear space still have the advantage.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Craniofacial_anthropometry" title="wikilink">Craniofacial anthropometry</a></li>
<li><a href="Human_appearance" title="wikilink">Human appearance</a></li>
<li><a href="Pattern_recognition" title="wikilink">Pattern recognition</a></li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li>A. Pentland, B. Moghaddam, T. Starner, O. Oliyide, and M. Turk. (1993). "<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.3791">View-based and modular Eigenspaces for face recognition</a>". Technical Report 245, M.I.T Media Lab.</li>
<li></li>
<li></li>
<li></li>
<li>T. Heseltine, N. Pears, J. Austin, Z. Chen (2003). "<a href="http://www-users.cs.york.ac.uk/~nep/research/3Dface/tomh/0059.pdf">Face Recognition: A Comparison of Appearance-Based Approaches</a>". <em>Proc. VIIth Digital Image Computing: Techniques and Applications</em>, vol 1. 59-68.</li>
<li></li>
<li></li>
<li>Delac, K., Grgic, M., Liatsis, P. (2005). "<a href="http://www.vcl.fer.hr/papers_pdf/Appearance-based%20Statistical%20Methods%20for%20Face%20Recognition.pdf">Appearance-based Statistical Methods for Face Recognition</a>". <em>Proceedings of the 47th International Symposium ELMAR-2005 focused on Multimedia Systems and Applications</em>, Zadar, Croatia, 08-10 June 2005, pp. 151–158</li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.face-rec.org">Face Recognition Homepage</a></li>
<li><a href="http://www.face-rec.org/source-codes/">PCA on the FERET Dataset</a></li>
<li><a href="http://develintel.blogspot.com/2005/12/eigenfaces.html">Developing Intelligence</a> Eigenfaces and the Fusiform Face Area</li>
<li><a href="http://onionesquereality.wordpress.com/2009/02/11/face-recognition-using-eigenfaces-and-distance-classifiers-a-tutorial/">A Tutorial on Face Recognition Using Eigenfaces and Distance Classifiers</a></li>
<li><a href="http://www.cs.ait.ac.th/~mdailey/matlab/">Matlab example code for eigenfaces</a></li>
<li>[<a class="uri" href="http://www.compvision.ru/forum/index.php?showtopic=74&amp;view">http://www.compvision.ru/forum/index.php?showtopic=74&amp;view;</a>;=getnewpost OpenCV + C++Builder6 implementation of PCA]</li>
<li><a href="http://cognitrn.psych.indiana.edu/nsfgrant/FaceMachine/faceMachine.html">Java applet demonstration of eigenfaces</a></li>
<li><a href="http://jeremykun.com/2011/07/27/eigenfaces/">Introduction to eigenfaces</a></li>
<li><a href="http://docs.opencv.org/modules/contrib/doc/facerec/facerec_tutorial.html/">Face Recognition Function in OpenCV</a></li>
<li><a href="http://www.mathworks.com/matlabcentral/fileexchange/33325-eigenface-based-facial-expression-classification/">Eigenface-based Facial Expression Recognition in Matlab</a></li>
</ul>

<p>"</p>

<p><a href="Category:Face_recognition" title="wikilink">Category:Face recognition</a> <a href="Category:Articles_with_example_MATLAB/Octave_code" title="wikilink">Category:Articles with example MATLAB/Octave code</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Ruiz-del-Solar, J and Navarrete, P.<em>Eigenspace-based face recognition: a comparative study of different approaches</em>, 2005<a href="#fnref1">↩</a></li>
<li id="fn2">Turk, Matthew A and Pentland, Alex P. '' Face recognition using eigenfaces''. Computer Vision and Pattern Recognition, 1991. Proceedings {CVPR'91.}, {IEEE} Computer Society Conference on 1991<a href="#fnref2">↩</a></li>
<li id="fn3">Draper, Bruce A. and Yambor, Wendy S and Beveridge, J Ross. <em>Analyzing pca-based face recognition algorithms: Eigenvector selection and distance measures</em>. 2002.<a href="#fnref3">↩</a></li>
<li id="fn4">Belhumeur, {P,N} and Kriegman, D. <em>What is the set of images of an object under all possible lighting conditions?</em>. Proceedings {CVPR} '96, 1996 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition, 1996<a href="#fnref4">↩</a></li>
<li id="fn5">Burnstone J and Yin H. <em>Eigenlights: Recovering Illumination From Face Images</em>. Conf Proceedings on IDEAL 2011.<a href="#fnref5">↩</a></li>
<li id="fn6">Moghaddam, B and Wahid, W and Pentland, A. <em>Beyond eigenfaces: probabilistic matching for face recognition</em>. Third {IEEE} International Conference on Automatic Face and Gesture Recognition, 1998. Proceedings<a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8">Belhumeur,P N, and Hespanha, {J,P}, and Kriegman, D.<em>Eigenfaces vs. Fisherfaces: recognition using class specific linear projection</em>, 1997.<a href="#fnref8">↩</a></li>
</ol>
</section>
</body>

