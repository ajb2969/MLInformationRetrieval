<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="1616">Information gain ratio</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Information gain ratio</h1>
<hr/>
<p>In <a href="decision_tree_learning" title="wikilink">decision tree learning</a>, <strong>Information gain ratio</strong> is a ratio of <a href="Information_gain_in_decision_trees" title="wikilink">information gain</a> to the intrinsic information. It is used to reduce a bias towards multi-valued attributes by taking the number and size of branches into account when choosing an attribute.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="information-gain-calculation">Information Gain Calculation</h2>
<p>Let <span class="LaTeX">$Attr$</span> be the set of all attributes and <span class="LaTeX">$Ex$</span> the set of all training examples, <span class="LaTeX">$value(x,a)$</span> with <span class="LaTeX">$x\in Ex$</span> defines the value of a specific example <span class="LaTeX">$x$</span> for attribute <span class="LaTeX">$a\in Attr$</span>, <span class="LaTeX">$H$</span> specifies the <a href="Entropy_(information_theory)#Definition" title="wikilink">entropy</a>. The <span class="LaTeX">$values(a)$</span> function denotes set of all possible values of attribute <span class="LaTeX">$a \in Attr$</span>. The information gain for an attribute <span class="LaTeX">$a\in Attr$</span> is defined as follows:</p>
<p><span class="LaTeX">$IG(Ex,a)=H(Ex) -\sum_{v\in values(a)} \left(\frac{|\{x\in Ex|value(x,a)=v\}|}{|Ex|} \cdot H(\{x\in Ex|value(x,a)=v\})\right)$</span></p>
<p>The information gain is equal to the total entropy for an attribute if for each of the attribute values a unique classification can be made for the result attribute. In this case the relative entropies subtracted from the total entropy are 0.</p>
<h2 id="intrinsic-value-calculation">Intrinsic Value Calculation</h2>
<p>The intrinsic value for a test is defined as follows:</p>
<p><span class="LaTeX">$IV(Ex,a)= -\sum_{v\in values(a)} \frac{|\{x\in Ex|value(x,a)=v\}|}{|Ex|} * \log_2\left(\frac{|\{x\in Ex|value(x,a)=v\}|}{|Ex|}\right)$</span></p>
<h2 id="information-gain-ratio-calculation">Information Gain Ratio Calculation</h2>
<p>The information gain ratio is just the ratio between the information gain and the intrinsic value: <span class="LaTeX">$IGR(Ex,a)=IG / IV$</span></p>
<h2 id="advantages">Advantages</h2>
<p>Information gain ratio biases the <a href="Decision_tree_learning" title="wikilink">decision tree</a> against considering attributes with a large number of distinct values. So it solves the drawback of information gain—namely, information gain applied to attributes that can take on a large number of distinct values might learn the <a href="training_set" title="wikilink">training set</a> too well. For example, suppose that we are building a decision tree for some data describing a business's customers. Information gain is often used to decide which of the attributes are the most relevant, so they can be tested near the root of the tree. One of the input attributes might be the customer's <a href="credit_card_number" title="wikilink">credit card number</a>. This attribute has a high information gain, because it uniquely identifies each customer, but we do <em>not</em> want to include it in the decision tree: deciding how to treat a customer based on their credit card number is unlikely to generalize to customers we haven't seen before.</p>
<h2 id="references">References</h2>
<p>"</p>
<p><a href="Category:Decision_trees" title="wikilink">Category:Decision trees</a> <a href="Category:Classification_algorithms" title="wikilink">Category:Classification algorithms</a> <a href="Category:Entropy_and_information" title="wikilink">Category:Entropy and information</a> <a href="Category:Statistical_ratios" title="wikilink">Category:Statistical ratios</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a class="uri" href="http://www.ke.tu-darmstadt.de/lehre/archiv/ws0809/mldm/dt.pdf">http://www.ke.tu-darmstadt.de/lehre/archiv/ws0809/mldm/dt.pdf</a><a href="#fnref1">↩</a></li>
</ol>
</section>
</body>
</html>
