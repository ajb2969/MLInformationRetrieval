<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="758">Consistent estimator</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Consistent estimator</h1>
<hr/>
<figure><b>(Figure)</b>
<figcaption>{<em>T</em><sub>1</sub>, <em>T</em><sub>2</sub>, <em>T</em><sub>3</sub>, …} is a sequence of estimators for parameter <em>θ</em><sub>0</sub>, the true value of which is 4. This sequence is consistent: the estimators are getting more and more concentrated near the true value <em>θ</em><sub>0</sub>; at the same time, these estimators are biased. The limiting distribution of the sequence is a degenerate random variable which equals <em>θ</em><sub>0</sub> with probability 1.</figcaption>
</figure>

<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, a <strong>consistent estimator</strong> or <strong>asymptotically consistent estimator</strong> is an <a class="uri" href="estimator" title="wikilink">estimator</a>—a rule for computing estimates of a parameter <em>θ</em><sub>0</sub>—having the property that as the number of data points used increases indefinitely, the resulting sequence of estimates <a href="convergence_in_probability" title="wikilink">converges in probability</a> to <em>θ</em><sub>0</sub>. This means that the distributions of the estimates become more and more concentrated near the true value of the parameter being estimated, so that the probability of the estimator being arbitrarily close to <em>θ</em><sub>0</sub> converges to one.</p>

<p>In practice one constructs an estimator as a function of an available sample of <a href="sample_size" title="wikilink">size</a> <em>n</em>, and then imagines being able to keep collecting data and expanding the sample <em>ad infinitum</em>. In this way one would obtain a sequence of estimates indexed by <em>n</em>, and consistency is a property of what occurs as the sample size “grows to infinity”. If the sequence of estimates can be mathematically shown to converge in probability to the true value <em>θ</em><sub>0</sub>, it is called a consistent estimator; otherwise the estimator is said to be <strong>inconsistent</strong>.</p>

<p>Consistency as defined here is sometimes referred to as <em>weak consistency</em>. When we replace convergence in probability with <a href="almost_sure_convergence" title="wikilink">almost sure convergence</a>, then the estimator is said to be <em>strongly consistent</em>. Consistency is related to <a href="bias_of_an_estimator" title="wikilink">bias</a>; see <a href="#Bias_versus_consistency" title="wikilink">bias versus consistency</a>.</p>
<h2 id="definition">Definition</h2>

<p>Loosely speaking, an <a class="uri" href="estimator" title="wikilink">estimator</a> <em>T<sub>n</sub></em> of parameter <em>θ</em> is said to be <strong>consistent</strong>, if it <a href="convergence_in_probability" title="wikilink">converges in probability</a> to the true value of the parameter:</p>

<p>

<math display="block" id="Consistent_estimator:0">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mpadded width="+2.8pt">
      <munder accentunder="true">
       <mo>plim</mo>
       <mrow>
        <mi>n</mi>
        <mo>→</mo>
        <mi mathvariant="normal">∞</mi>
       </mrow>
      </munder>
     </mpadded>
     <msub>
      <mi>T</mi>
      <mi>n</mi>
     </msub>
    </mrow>
    <mo>=</mo>
    <mi>θ</mi>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <apply>
       <ci>normal-→</ci>
       <ci>n</ci>
       <infinity></infinity>
      </apply>
      <ci>plim</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>T</ci>
      <ci>n</ci>
     </apply>
    </apply>
    <ci>θ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \underset{n\to\infty}{\operatorname{plim}}\;T_{n}=\theta.
  </annotation>
 </semantics>
</math>

</p>

<p>A more rigorous definition takes into account the fact that <em>θ</em> is actually unknown, and thus the convergence in probability must take place for every possible value of this parameter. Suppose } is a family of distributions (the <a href="parametric_model" title="wikilink">parametric model</a>), and } is an infinite <a href="statistical_sample" title="wikilink">sample</a> from the distribution <em>p<sub>θ</sub></em>. Let { <em>T<sub>n</sub></em>(<em>X<sup>θ</sup></em>) } be a sequence of estimators for some parameter <em>g</em>(<em>θ</em>). Usually <em>T<sub>n</sub></em> will be based on the first <em>n</em> observations of a sample. Then this sequence {<em>T<sub>n</sub></em>} is said to be (weakly) <strong>consistent</strong> if </p>

<p>

<math display="block" id="Consistent_estimator:1">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <mpadded width="+2.8pt">
       <munder accentunder="true">
        <mo>plim</mo>
        <mrow>
         <mi>n</mi>
         <mo>→</mo>
         <mi mathvariant="normal">∞</mi>
        </mrow>
       </munder>
      </mpadded>
      <msub>
       <mi>T</mi>
       <mi>n</mi>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <msup>
        <mi>X</mi>
        <mi>θ</mi>
       </msup>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>=</mo>
     <mrow>
      <mi>g</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>θ</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mo rspace="12.5pt">,</mo>
    <mrow>
     <mrow>
      <mpadded width="+5pt">
       <mtext>for all</mtext>
      </mpadded>
      <mi>θ</mi>
     </mrow>
     <mo>∈</mo>
     <mi mathvariant="normal">Θ</mi>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <apply>
       <apply>
        <ci>normal-→</ci>
        <ci>n</ci>
        <infinity></infinity>
       </apply>
       <ci>plim</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>T</ci>
       <ci>n</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>X</ci>
       <ci>θ</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>g</ci>
      <ci>θ</ci>
     </apply>
    </apply>
    <apply>
     <in></in>
     <apply>
      <times></times>
      <mtext>for all</mtext>
      <ci>θ</ci>
     </apply>
     <ci>normal-Θ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \underset{n\to\infty}{\operatorname{plim}}\;T_{n}(X^{\theta})=g(\theta),\ \ %
\text{for all}\ \theta\in\Theta.
  </annotation>
 </semantics>
</math>

</p>

<p>This definition uses <em>g</em>(<em>θ</em>) instead of simply <em>θ</em>, because often one is interested in estimating a certain function or a sub-vector of the underlying parameter. In the next example we estimate the location parameter of the model, but not the scale:</p>
<h2 id="examples">Examples</h2>
<h3 id="sample-mean-of-a-normal-random-variable">Sample mean of a normal random variable</h3>

<p>Suppose one has a sequence of observations {<em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, …} from a <a href="Normal_distribution" title="wikilink">normal <em>N</em>(<em>μ</em>, <em>σ</em><sup>2</sup>)</a> distribution. To estimate <em>μ</em> based on the first <em>n</em> observations, one can use the <a href="sample_mean" title="wikilink">sample mean</a>: <em>T<sub>n</sub></em> = (<em>X</em><sub>1</sub> + … + <em>X<sub>n</sub></em>)/<em>n</em>. This defines a sequence of estimators, indexed by the sample size <em>n</em>.</p>

<p>From the properties of the normal distribution, we know the <a href="sampling_distribution" title="wikilink">sampling distribution</a> of this statistic: <em>T</em><sub><em>n</em></sub> is itself normally distributed, with mean <em>μ</em> and variance <em>σ</em><sup>2</sup>/<em>n</em>. Equivalently, 

<math display="inline" id="Consistent_estimator:2">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <msub>
      <mi>T</mi>
      <mi>n</mi>
     </msub>
     <mo>-</mo>
     <mi>μ</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>/</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>σ</mi>
     <mo>/</mo>
     <msqrt>
      <mi>n</mi>
     </msqrt>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>T</ci>
      <ci>n</ci>
     </apply>
     <ci>μ</ci>
    </apply>
    <apply>
     <divide></divide>
     <ci>σ</ci>
     <apply>
      <root></root>
      <ci>n</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \scriptstyle(T_{n}-\mu)/(\sigma/\sqrt{n})
  </annotation>
 </semantics>
</math>

 has a standard normal distribution:</p>

<p>

<math display="block" id="Consistent_estimator:3">
 <semantics>
  <mrow>
   <mrow>
    <mpadded width="-1.7pt">
     <mi>Pr</mi>
    </mpadded>
    <mrow>
     <mo rspace="4.2pt">[</mo>
     <mrow>
      <mrow>
       <mo stretchy="false">|</mo>
       <mrow>
        <msub>
         <mi>T</mi>
         <mi>n</mi>
        </msub>
        <mo>-</mo>
        <mi>μ</mi>
       </mrow>
       <mo stretchy="false">|</mo>
      </mrow>
      <mo>≥</mo>
      <mpadded width="+1.7pt">
       <mi>ε</mi>
      </mpadded>
     </mrow>
     <mo>]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mpadded width="-1.7pt">
     <mi>Pr</mi>
    </mpadded>
    <mrow>
     <mo>[</mo>
     <mrow>
      <mfrac>
       <mrow>
        <mpadded width="+1.7pt">
         <msqrt>
          <mi>n</mi>
         </msqrt>
        </mpadded>
        <mrow>
         <mo mathsize="120%" stretchy="false">|</mo>
         <mrow>
          <msub>
           <mi>T</mi>
           <mi>n</mi>
          </msub>
          <mo>-</mo>
          <mi>μ</mi>
         </mrow>
         <mo mathsize="120%" stretchy="false">|</mo>
        </mrow>
       </mrow>
       <mi>σ</mi>
      </mfrac>
      <mo>≥</mo>
      <mrow>
       <mrow>
        <msqrt>
         <mi>n</mi>
        </msqrt>
        <mi>ε</mi>
       </mrow>
       <mo>/</mo>
       <mi>σ</mi>
      </mrow>
     </mrow>
     <mo>]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mn>2</mn>
    <mrow>
     <mo>(</mo>
     <mrow>
      <mn>1</mn>
      <mo>-</mo>
      <mrow>
       <mi mathvariant="normal">Φ</mi>
       <mrow>
        <mo>(</mo>
        <mfrac>
         <mrow>
          <mpadded width="+1.7pt">
           <msqrt>
            <mi>n</mi>
           </msqrt>
          </mpadded>
          <mi>ε</mi>
         </mrow>
         <mi>σ</mi>
        </mfrac>
        <mo>)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
   <mo>→</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <ci>Pr</ci>
      <apply>
       <geq></geq>
       <apply>
        <abs></abs>
        <apply>
         <minus></minus>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>T</ci>
          <ci>n</ci>
         </apply>
         <ci>μ</ci>
        </apply>
       </apply>
       <ci>ε</ci>
      </apply>
     </apply>
     <apply>
      <ci>Pr</ci>
      <apply>
       <geq></geq>
       <apply>
        <divide></divide>
        <apply>
         <times></times>
         <apply>
          <root></root>
          <ci>n</ci>
         </apply>
         <apply>
          <abs></abs>
          <apply>
           <minus></minus>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>T</ci>
            <ci>n</ci>
           </apply>
           <ci>μ</ci>
          </apply>
         </apply>
        </apply>
        <ci>σ</ci>
       </apply>
       <apply>
        <divide></divide>
        <apply>
         <times></times>
         <apply>
          <root></root>
          <ci>n</ci>
         </apply>
         <ci>ε</ci>
        </apply>
        <ci>σ</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <cn type="integer">2</cn>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <apply>
        <times></times>
        <ci>normal-Φ</ci>
        <apply>
         <divide></divide>
         <apply>
          <times></times>
          <apply>
           <root></root>
           <ci>n</ci>
          </apply>
          <ci>ε</ci>
         </apply>
         <ci>σ</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <ci>normal-→</ci>
     <share href="#.cmml">
     </share>
     <cn type="integer">0</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Pr\!\left[\,|T_{n}-\mu|\geq\varepsilon\,\right]=\Pr\!\left[\frac{\sqrt{n}\,%
\big|T_{n}-\mu\big|}{\sigma}\geq\sqrt{n}\varepsilon/\sigma\right]=2\left(1-%
\Phi\left(\frac{\sqrt{n}\,\varepsilon}{\sigma}\right)\right)\to 0
  </annotation>
 </semantics>
</math>

 as <em>n</em> tends to infinity, for any fixed . Therefore, the sequence <em>T<sub>n</sub></em> of sample means is consistent for the population mean <em>μ</em>.</p>
<h2 id="establishing-consistency">Establishing consistency</h2>

<p>The notion of asymptotic consistency is very close, almost synonymous to the notion of convergence in probability. As such, any theorem, lemma, or property which establishes convergence in probability may be used to prove the consistency. Many such tools exist:</p>
<ul>
<li>In order to demonstrate consistency directly from the definition one can use the inequality </li>
</ul>
<dl>
<dd><dl>
<dd><math>
</math></dd>
</dl>
</dd>
</dl>

<p><code>   \Pr\!\big[h(T_n-\theta)\geq\varepsilon\big] \leq \frac{\operatorname{E}\big[h(T_n-\theta)\big]}{\varepsilon},</code><br/>
<code> </code></p>

<p>the most common choice for function <em>h</em> being either the absolute value (in which case it is known as <a href="Markov_inequality" title="wikilink">Markov inequality</a>), or the quadratic function (respectively <a href="Chebychev's_inequality" title="wikilink">Chebychev's inequality</a>).</p>
<ul>
<li>Another useful result is the <a href="continuous_mapping_theorem" title="wikilink">continuous mapping theorem</a>: if <em>T<sub>n</sub></em> is consistent for <em>θ</em> and <em>g</em>(·) is a real-valued function continuous at point <em>θ</em>, then <em>g</em>(<em>T<sub>n</sub></em>) will be consistent for <em>g</em>(<em>θ</em>):</li>
</ul>
<dl>
<dd><dl>
<dd><math>
</math></dd>
</dl>
</dd>
</dl>

<p><code>   T_n\ \xrightarrow{p}\ \theta\ \quad\Rightarrow\quad g(T_n)\ \xrightarrow{p}\ g(\theta)</code><br/>
<code> </code></p>
<ul>
<li><a href="Slutsky’s_theorem" title="wikilink">Slutsky’s theorem</a> can be used to combine several different estimators, or an estimator with a non-random convergent sequence. If <em>T<sub>n</sub></em> →<sup><em>p</em></sup><em>α</em>, and <em>S<sub>n</sub></em> →<sup><em>p</em></sup><em>β</em>, then </li>
</ul>
<dl>
<dd><dl>
<dd><math>\begin{align}
</math></dd>
</dl>
</dd>
</dl>

<p><code> &amp; T_n + S_n \ \xrightarrow{p}\ \alpha+\beta, \\</code><br/>
<code> &amp; T_n   S_n \ \xrightarrow{p}\ \alpha \beta, \\</code><br/>
<code> &amp; T_n / S_n \ \xrightarrow{p}\ \alpha/\beta, \text{ provided that }\beta\neq0</code><br/>
<code> \end{align}</code></p>
<ul>
<li>If estimator <em>T<sub>n</sub></em> is given by an explicit formula, then most likely the formula will employ sums of random variables, and then the <a href="law_of_large_numbers" title="wikilink">law of large numbers</a> can be used: for a sequence {<em>X<sub>n</sub></em>} of random variables and under suitable conditions,</li>
</ul>
<dl>
<dd><dl>
<dd>

<math display="inline" id="Consistent_estimator:4">
 <semantics>
  <mrow>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mi>n</mi>
    </mfrac>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </msubsup>
     <mrow>
      <mi>g</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>X</mi>
        <mi>i</mi>
       </msub>
       <mo rspace="7.5pt" stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mpadded width="+5pt">
    <mover accent="true">
     <mo>→</mo>
     <mo>𝑝</mo>
    </mover>
   </mpadded>
   <mrow>
    <mo>E</mo>
    <mrow>
     <mo rspace="4.2pt" stretchy="false">[</mo>
     <mrow>
      <mi>g</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>X</mi>
       <mo rspace="4.2pt" stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <apply>
     <ci>p</ci>
     <ci>normal-→</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>n</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <times></times>
       <ci>g</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>X</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <ci>normal-E</ci>
     <apply>
      <times></times>
      <ci>g</ci>
      <ci>X</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{1}{n}\sum_{i=1}^{n}g(X_{i})\ \xrightarrow{p}\ \operatorname{E}[\,g(X)\,]
  </annotation>
 </semantics>
</math>


</dd>
</dl>
</dd>
</dl>
<ul>
<li>If estimator <em>T<sub>n</sub></em> is defined implicitly, for example as a value that maximizes certain objective function (see <a href="extremum_estimator" title="wikilink">extremum estimator</a>), then a more complicated argument involving <a href="stochastic_equicontinuity" title="wikilink">stochastic equicontinuity</a> has to be used.</li>
</ul>
<h2 id="bias-versus-consistency">Bias versus consistency</h2>

<p><a href="Bias_of_an_estimator" title="wikilink">Bias</a> is related to consistency as follows: a sequence of estimators is consistent <a href="if_and_only_if" title="wikilink">if and only if</a> it converges to a value and the bias converges to zero. Consistent estimators are convergent and <em>asymptotically</em> unbiased (hence converge to the correct value): individual estimators in the sequence may be biased, but the overall sequence still consistent, if the bias converges to zero. Conversely, if the sequence does not converge to a value, then it is not consistent, regardless of whether the estimators in the sequence are biased or not.</p>
<h3 id="unbiased-but-not-consistent">Unbiased but not consistent</h3>

<p>An estimator can be <a href="biased_estimator" title="wikilink">unbiased</a> but not consistent. For example, for an <a class="uri" href="iid" title="wikilink">iid</a> sample {<em>x</em>,..., <em>x</em>} one can use <em>T</em>(<em>X</em>) = <em>x</em> as the estimator of the mean E[<em>x</em>]. Note that here the sampling distribution of <em>T</em> is the same as the underlying distribution (for any <em>n,</em> as it ignores all points but the first), so E[<em>T</em>(<em>X</em>)] = E[<em>x</em>] and it is unbiased, but it does not converge to any value.</p>

<p>However, if a sequence of estimators is unbiased <em>and</em> converges to a value, then it is consistent, as it must converge to the correct value.</p>
<h3 id="biased-but-consistent">Biased but consistent</h3>

<p>Alternatively, an estimator can be biased but consistent. For example if the mean is estimated by 

<math display="inline" id="Consistent_estimator:5">
 <semantics>
  <mrow>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mi>n</mi>
    </mfrac>
    <mrow>
     <mo largeop="true" symmetric="true">∑</mo>
     <msub>
      <mi>x</mi>
      <mi>i</mi>
     </msub>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mfrac>
    <mn>1</mn>
    <mi>n</mi>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>n</ci>
     </apply>
     <apply>
      <sum></sum>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   {1\over n}\sum x_{i}+{1\over n}
  </annotation>
 </semantics>
</math>

 it is biased, but as 

<math display="inline" id="Consistent_estimator:6">
 <semantics>
  <mrow>
   <mi>n</mi>
   <mo>→</mo>
   <mi mathvariant="normal">∞</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>n</ci>
    <infinity></infinity>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n\rightarrow\infty
  </annotation>
 </semantics>
</math>

, it approaches the correct value, and so it is consistent.</p>

<p>Important examples include the <a href="sample_variance" title="wikilink">sample variance</a> and <a href="sample_standard_deviation" title="wikilink">sample standard deviation</a>. Without <a href="Bessel's_correction" title="wikilink">Bessel's correction</a> (using the sample size <em>n</em> instead of the <a href="Degrees_of_freedom_(statistics)" title="wikilink">degrees of freedom</a> <em>n</em> − 1), these are both negatively biased but consistent estimators. With the correction, the unbiased sample variance is unbiased, while the corrected sample standard deviation is still biased, but less so, and both are still consistent: the correction factor converges to 1 as sample size grows.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Efficient_estimator" title="wikilink">Efficient estimator</a></li>
<li><a href="Fisher_consistency" title="wikilink">Fisher consistency</a> — alternative, although rarely used concept of consistency for the estimators</li>
<li><a href="Regression_dilution" title="wikilink">Regression dilution</a></li>
<li><a href="Statistical_hypothesis_testing" title="wikilink">Statistical hypothesis testing</a></li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li>

<p>by <a href="Mark_Thoma" title="wikilink">Mark Thoma</a></p></li>
</ul>

<p>"</p>

<p><a href="Category:Statistical_theory" title="wikilink">Category:Statistical theory</a> <a href="Category:Statistical_inference" title="wikilink">Category:Statistical inference</a> <a href="Category:Estimation_theory" title="wikilink">Category:Estimation theory</a> <a href="Category:Asymptotic_statistical_theory" title="wikilink">Category:Asymptotic statistical theory</a></p>
</body>
</html>
