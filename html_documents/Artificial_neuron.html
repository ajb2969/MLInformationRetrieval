<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="265">Artificial neuron</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Artificial neuron</h1>
<hr>An '''artificial neuron''' is a mathematical [[Function (mathematics)|function]] conceived as a [[Mathematica
<p>l model|model]] of biological <a href="neuron" title="wikilink">neurons</a>. Artificial neurons are the constitutive units in an <a href="artificial_neural_network" title="wikilink">artificial neural network</a>. Depending on the specific model used they may be called a <strong>semi-linear unit</strong>, <strong>Nv neuron</strong>, <strong>binary neuron</strong>, <strong>linear threshold function</strong>, or <strong>McCulloch–Pitts (MCP) neuron</strong>. The artificial neuron receives one or more inputs (representing <a href="dendrite" title="wikilink">dendrites</a>) and sums them to produce an output (representing a neuron's <a class="uri" href="axon" title="wikilink">axon</a>). Usually the sums of each node are weighted, and the sum is passed through a <a class="uri" href="non-linear" title="wikilink">non-linear</a> function known as an <a href="activation_function" title="wikilink">activation function</a> or <a href="transfer_function" title="wikilink">transfer function</a>. The transfer functions usually have a <a href="sigmoid_function" title="wikilink">sigmoid shape</a>, but they may also take the form of other non-linear functions, <a class="uri" href="piecewise" title="wikilink">piecewise</a> linear functions, or <a href="#Step_function" title="wikilink">step functions</a>. They are also often <a href="Monotonic_function" title="wikilink">monotonically increasing</a>, <a href="Continuous_function" title="wikilink">continuous</a>, <a href="Differentiable_function" title="wikilink">differentiable</a> and <a href="Bounded_function" title="wikilink">bounded</a>.</p>

<p>The artificial neuron transfer function should not be confused with a linear system's <a href="transfer_function" title="wikilink">transfer function</a>.</p>
<h2 id="basic-structure">Basic structure</h2>

<p>For a given artificial neuron, let there be <em>m</em> + 1 inputs with signals <em>x</em><sub>0</sub> through <em>x</em><sub><em>m</em></sub> and weights <em>w</em><sub>0</sub> through <em>w</em><sub><em>m</em></sub>. Usually, the <em>x</em><sub>0</sub> input is assigned the value +1, which makes it a <em>bias</em> input with <em>w</em><sub><em>k</em>0</sub> = <em>b</em><sub><em>k</em></sub>. This leaves only <em>m</em> actual inputs to the neuron: from <em>x</em><sub>1</sub> to <em>x</em><sub><em>m</em></sub>.</p>

<p>The output of the <em>k</em>th neuron is:</p>

<p>

<math display="block" id="Artificial_neuron:0">
 <semantics>
  <mrow>
   <msub>
    <mi>y</mi>
    <mi>k</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <mi>φ</mi>
    <mrow>
     <mo>(</mo>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>j</mi>
        <mo>=</mo>
        <mn>0</mn>
       </mrow>
       <mi>m</mi>
      </munderover>
      <mrow>
       <msub>
        <mi>w</mi>
        <mrow>
         <mi>k</mi>
         <mi>j</mi>
        </mrow>
       </msub>
       <msub>
        <mi>x</mi>
        <mi>j</mi>
       </msub>
      </mrow>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <ci>k</ci>
    </apply>
    <apply>
     <times></times>
     <ci>φ</ci>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>j</ci>
         <cn type="integer">0</cn>
        </apply>
       </apply>
       <ci>m</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <apply>
         <times></times>
         <ci>k</ci>
         <ci>j</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>j</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y_{k}=\varphi\left(\sum_{j=0}^{m}w_{kj}x_{j}\right)
  </annotation>
 </semantics>
</math>

</p>

<p>Where 

<math display="inline" id="Artificial_neuron:1">
 <semantics>
  <mi>φ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>φ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \varphi
  </annotation>
 </semantics>
</math>

 (phi) is the transfer function.</p>
<figure><b>(Figure)</b>
<figcaption>artificial neuron.png</figcaption>
</figure>

<p>The output is analogous to the <a class="uri" href="axon" title="wikilink">axon</a> of a biological neuron, and its value propagates to the input of the next layer, through a synapse. It may also exit the system, possibly as part of an output vector.</p>

<p>It has no learning process as such. Its transfer function weights are calculated and threshold value are predetermined.</p>
<h2 id="comparison-to-biological-neurons">Comparison to biological neurons</h2>

<p>Artificial neurons are designed to mimic aspects of their biological counterparts.</p>
<ul>
<li><em><a class="uri" href="Dendrites" title="wikilink">Dendrites</a></em> – In a biological neuron, the dendrites act as the input vector. These dendrites allow the cell to receive signals from a large (&gt;1000) number of neighboring neurons. As in the above mathematical treatment, each dendrite is able to perform "multiplication" by that dendrite's "weight value." The multiplication is accomplished by increasing or decreasing the ratio of synaptic neurotransmitters to signal chemicals introduced into the dendrite in response to the synaptic neurotransmitter. A negative multiplication effect can be achieved by transmitting signal inhibitors (i.e. oppositely charged ions) along the dendrite in response to the reception of synaptic neurotransmitters.</li>
</ul>
<ul>
<li><em><a href="Soma_(biology)" title="wikilink">Soma</a></em> – In a biological neuron, the soma acts as the summation function, seen in the above mathematical description. As positive and negative signals (exciting and inhibiting, respectively) arrive in the soma from the dendrites, the positive and negative ions are effectively added in summation, by simple virtue of being mixed together in the solution inside the cell's body.</li>
</ul>
<ul>
<li><em><a class="uri" href="Axon" title="wikilink">Axon</a></em> – The axon gets its signal from the summation behavior which occurs inside the soma. The opening to the axon essentially samples the electrical potential of the solution inside the soma. Once the soma reaches a certain potential, the axon will transmit an all-in signal pulse down its length. In this regard, the axon behaves as the ability for us to connect our artificial neuron to other artificial neurons.</li>
</ul>

<p>Unlike most artificial neurons, however, biological neurons fire in discrete pulses. Each time the electrical potential inside the soma reaches a certain threshold, a pulse is transmitted down the axon. This pulsing can be translated into continuous values. The rate (activations per second, etc.) at which an axon fires converts directly into the rate at which neighboring cells get signal ions introduced into them. The faster a biological neuron fires, the faster nearby neurons accumulate electrical potential (or lose electrical potential, depending on the "weighting" of the dendrite that connects to the neuron that fired). It is this conversion that allows computer scientists and mathematicians to simulate biological neural networks using artificial neurons which can output distinct values (often from −1 to 1).</p>
<h2 id="history">History</h2>

<p>The first artificial neuron was the Threshold Logic Unit (TLU), or Linear Threshold Unit,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> first proposed by <a href="Warren_McCulloch" title="wikilink">Warren McCulloch</a> and <a href="Walter_Pitts" title="wikilink">Walter Pitts</a> in 1943. The model was specifically targeted as a computational model of the "nerve net" in the brain.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> As a transfer function, it employed a threshold, equivalent to using the <a href="Heaviside_step_function" title="wikilink">Heaviside step function</a>. Initially, only a simple model was considered, with binary inputs and outputs, some restrictions on the possible weights, and a more flexible threshold value. Since the beginning it was already noticed that any boolean function could be implemented by networks of such devices, what is easily seen from the fact that one can implement the AND and OR functions, and use them in the <a href="disjunctive_normal_form" title="wikilink">disjunctive</a> or the <a href="conjunctive_normal_form" title="wikilink">conjunctive normal form</a>. Researchers also soon realized that cyclic networks, with <a href="feedback" title="wikilink">feedbacks</a> through neurons, could define dynamical systems with memory, but most of the research concentrated (and still does) on strictly feed-forward networks because of the smaller difficulty they present.</p>

<p>One important and pioneering artificial neural network that used the linear threshold function was the <a class="uri" href="perceptron" title="wikilink">perceptron</a>, developed by <a href="Frank_Rosenblatt" title="wikilink">Frank Rosenblatt</a>. This model already considered more flexible weight values in the neurons, and was used in machines with adaptive capabilities. The representation of the threshold values as a bias term was introduced by <a href="Bernard_Widrow" title="wikilink">Bernard Widrow</a> in 1960 – see <a class="uri" href="ADALINE" title="wikilink">ADALINE</a>.</p>

<p>In the late 1980s, when research on neural networks regained strength, neurons with more continuous shapes started to be considered. The possibility of differentiating the activation function allows the direct use of the <a href="gradient_descent" title="wikilink">gradient descent</a> and other optimization algorithms for the adjustment of the weights. Neural networks also started to be used as a general function approximation model. The best known training algorithm called <a class="uri" href="backpropagation" title="wikilink">backpropagation</a> has been rediscovered several times but its first development goes back to the work of <a href="Paul_Werbos" title="wikilink">Paul Werbos</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<h2 id="types-of-transfer-functions">Types of transfer functions</h2>

<p>The transfer function of a neuron is chosen to have a number of properties which either enhance or simplify the network containing the neuron. Crucially, for instance, any <a href="multilayer_perceptron" title="wikilink">multilayer perceptron</a> using a <em>linear</em> transfer function has an equivalent single-layer network; a non-linear function is therefore necessary to gain the advantages of a multi-layer network.</p>

<p>Below, <em>u</em> refers in all cases to the weighted sum of all the inputs to the neuron, i.e. for <em>n</em> inputs,</p>

<p>

<math display="block" id="Artificial_neuron:2">
 <semantics>
  <mrow>
   <mi>u</mi>
   <mo>=</mo>
   <mrow>
    <munderover>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>n</mi>
    </munderover>
    <mrow>
     <msub>
      <mi>w</mi>
      <mi>i</mi>
     </msub>
     <msub>
      <mi>x</mi>
      <mi>i</mi>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>u</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>n</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   u=\sum_{i=1}^{n}w_{i}x_{i}
  </annotation>
 </semantics>
</math>

</p>

<p>where <strong>w</strong> is a vector of <em>synaptic weights</em> and <strong>x</strong> is a vector of inputs.</p>
<h3 id="step-function">Step function</h3>

<p>The output <em>y</em> of this transfer function is binary, depending on whether the input meets a specified threshold, <em>θ</em>. The "signal" is sent, i.e. the output is set to one, if the activation meets the threshold.</p>

<p>

<math display="block" id="Artificial_neuron:3">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>=</mo>
   <mrow>
    <mo>{</mo>
    <mtable displaystyle="true">
     <mtr>
      <mtd columnalign="left">
       <mn>1</mn>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mtext>if</mtext>
         <mi>u</mi>
        </mrow>
        <mo>≥</mo>
        <mi>θ</mi>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mn>0</mn>
      </mtd>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mtext>if</mtext>
         <mi>u</mi>
        </mrow>
        <mo><</mo>
        <mi>θ</mi>
       </mrow>
      </mtd>
     </mtr>
    </mtable>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>y</ci>
    <apply>
     <csymbol cd="latexml">cases</csymbol>
     <cn type="integer">1</cn>
     <apply>
      <geq></geq>
      <apply>
       <times></times>
       <mtext>if</mtext>
       <ci>u</ci>
      </apply>
      <ci>θ</ci>
     </apply>
     <cn type="integer">0</cn>
     <apply>
      <lt></lt>
      <apply>
       <times></times>
       <mtext>if</mtext>
       <ci>u</ci>
      </apply>
      <ci>θ</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=\begin{cases}1&\text{if }u\geq\theta\\
0&\text{if }u<\theta\end{cases}
  </annotation>
 </semantics>
</math>

</p>

<p>This function is used in <a href="perceptron" title="wikilink">perceptrons</a> and often shows up in many other models. It performs a division of the <a href="Vector_space" title="wikilink">space</a> of inputs by a <a class="uri" href="hyperplane" title="wikilink">hyperplane</a>. It is specially useful in the last layer of a network intended to perform binary classification of the inputs. It can be approximated from other sigmoidal functions by assigning large values to the weights.</p>
<h3 id="linear-combination">Linear combination</h3>

<p>In this case, the output unit is simply the weighted sum of its inputs plus a <em>bias</em> term. A number of such linear neurons perform a linear transformation of the input vector. This is usually more useful in the first layers of a network. A number of analysis tools exist based on linear models, such as <a href="harmonic_analysis" title="wikilink">harmonic analysis</a>, and they can all be used in neural networks with this linear neuron. The bias term allows us to make <a href="homogeneous_coordinates" title="wikilink">affine transformations</a> to the data.</p>

<p>See: <a href="Linear_transformation" title="wikilink">Linear transformation</a>, <a href="Harmonic_analysis" title="wikilink">Harmonic analysis</a>, <a href="Linear_filter" title="wikilink">Linear filter</a>, <a class="uri" href="Wavelet" title="wikilink">Wavelet</a>, <a href="Principal_component_analysis" title="wikilink">Principal component analysis</a>, <a href="Independent_component_analysis" title="wikilink">Independent component analysis</a>, <a class="uri" href="Deconvolution" title="wikilink">Deconvolution</a>.</p>
<h3 id="sigmoid">Sigmoid</h3>

<p>A fairly simple non-linear function, a <a href="sigmoid_function" title="wikilink">sigmoid function</a> such as the logistic function also has an easily calculated derivative, which can be important when calculating the weight updates in the network. It thus makes the network more easily manipulable mathematically, and was attractive to early computer scientists who needed to minimize the computational load of their simulations. It is commonly seen in <a href="multilayer_perceptron" title="wikilink">multilayer perceptrons</a> using a <a class="uri" href="backpropagation" title="wikilink">backpropagation</a> algorithm.</p>
<h2 id="pseudocode-algorithm">Pseudocode algorithm</h2>

<p>The following is a simple <a class="uri" href="pseudocode" title="wikilink">pseudocode</a> implementation of a single TLU which takes <a href="Boolean_data_type" title="wikilink">boolean</a> inputs (true or false), and returns a single boolean output when activated. An <a href="object_oriented" title="wikilink">object-oriented</a> model is used. No method of training is defined, since several exist. If a purely functional model were used, the class TLU below would be replaced with a function TLU with input parameters threshold, weights, and inputs that returned a boolean value.</p>

<p><code> </code><strong><code>class</code></strong><code> TLU </code><strong><code>defined</code> <code>as:</code></strong><br/>
<code>  </code><strong><code>data</code> <code>member</code></strong><code> threshold </code><strong><code>:</code></strong><code> number</code><br/>
<code>  </code><strong><code>data</code> <code>member</code></strong><code> weights </code><strong><code>:</code> <code>list</code> <code>of</code></strong><code> numbers </code><strong><code>of</code> <code>size</code></strong><code> X</code><br/>
<code>  </code><strong><code>function</code> <code>member</code></strong><code> fire( inputs </code><strong><code>:</code> <code>list</code> <code>of</code></strong><code> booleans </code><strong><code>of</code> <code>size</code></strong><code> X ) </code><strong><code>:</code></strong><code> boolean </code><strong><code>defined</code> <code>as:</code></strong><br/>
<code>   </code><strong><code>variable</code></strong><code> T </code><strong><code>:</code></strong><code> number</code><br/>
<code>   T </code><strong><code>←</code></strong><code> 0</code><br/>
<code>   </code><strong><code>for</code> <code>each</code></strong><code> i </code><strong><code>in</code></strong><code> 1 </code><strong><code>to</code></strong><code> X </code><strong><code>:</code></strong><br/>
<code>    </code><strong><code>if</code></strong><code> inputs(i) </code><strong><code>is</code></strong><code> true </code><strong><code>:</code></strong><br/>
<code>     T </code><strong><code>←</code></strong><code> T + weights(i)</code><br/>
<code>    </code><strong><code>end</code> <code>if</code></strong><br/>
<code>   </code><strong><code>end</code> <code>for</code> <code>each</code></strong><br/>
<code>   </code><strong><code>if</code></strong><code> T &gt; threshold </code><strong><code>:</code></strong><br/>
<code>    </code><strong><code>return</code></strong><code> true</code><br/>
<code>   </code><strong><code>else:</code></strong><br/>
<code>    </code><strong><code>return</code></strong><code> false</code><br/>
<code>   </code><strong><code>end</code> <code>if</code></strong><br/>
<code>  </code><strong><code>end</code> <code>function</code></strong><br/>
<code> </code><strong><code>end</code> <code>class</code></strong></p>
<h2 id="limitations">Limitations</h2>

<p>Simple artificial neurons, such as the McCulloch–Pitts model, are sometimes described as "caricature models", since they are intended to reflect one or more neurophysiological observations, but without regard to realism.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h2 id="comparison-with-biological-neuron-coding">Comparison with biological neuron coding</h2>

<p>New research has shown that <a href="unary_coding" title="wikilink">unary coding</a> is used in the neural circuits responsible for <a class="uri" href="birdsong" title="wikilink">birdsong</a> production. <a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> <a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> The use of unary n biological networks is presumably due to the inherent simplicity of the coding. Another contributing factor could be the fact that unary coding provides a certain degree of error correction.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a class="uri" href="ADALINE" title="wikilink">ADALINE</a></li>
<li><a href="Biological_neuron_models" title="wikilink">Biological neuron models</a></li>
<li><a href="Binding_neuron" title="wikilink">Binding neuron</a></li>
<li><a class="uri" href="Connectionism" title="wikilink">Connectionism</a></li>
<li><a href="Neural_network" title="wikilink">Neural network</a></li>
<li><a href="Nv_network" title="wikilink">Nv network</a></li>
<li><a class="uri" href="Perceptron" title="wikilink">Perceptron</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="further-reading">Further reading</h2>
<ul>
<li><a href="Warren_McCulloch" title="wikilink">McCulloch, W</a>. and <a href="Walter_Pitts" title="wikilink">Pitts, W</a>. (1943). <em>A logical calculus of the ideas immanent in nervous activity.</em> Bulletin of Mathematical Biophysics, 5:115–133. <a href="http://link.springer.com/article/10.1007%2FBF02478259">2</a></li>
<li>A.S. Samardak, A. Nogaret, N. B. Janson, A. G. Balanov, I. Farrer and D. A. Ritchie. "Noise-Controlled Signal Transmission in a Multithread Semiconductor Neuron" // Phys.Rev.Lett. 102 (2009) 226802, <a href="http://prl.aps.org/abstract/PRL/v102/i22/e226802">3</a></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=NhTZnnJJP64">Artifical neuron mimicks function of human cells</a></li>
<li><a href="http://www.mind.ilstu.edu/curriculum/modOverview.php?modGUI=212">4</a> A good general overview</li>
</ul>

<p>"</p>

<p><a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a> <a href="Category:American_inventions" title="wikilink">Category:American inventions</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="Paul_Werbos" title="wikilink">Paul Werbos</a>, Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University, 1974<a href="#fnref3">↩</a></li>
<li id="fn4"><a href="Paul_Werbos" title="wikilink">Paul Werbos</a>, Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, Volume 78, Issue 10, 1550–1560, Oct 1990, doi10.1109/5.58337<a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7">Moore J.M. et al., Motor pathway convergence predicts syllable repertoire size in oscine birds. Proc. Nat. Acad. Sc. USA 108: 16440–16445, 2011.<a href="#fnref7">↩</a></li>
<li id="fn8">Potluri, P., Error correction capacity of unary coding. 2014. <a href="http://arxiv.org/abs/1411.7406">1</a><a href="#fnref8">↩</a></li>
</ol>
</section>
</hr></body>
</html>
