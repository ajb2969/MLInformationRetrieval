<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="803">Canopy clustering algorithm</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Canopy clustering algorithm</h1>
<hr/>

<p>The <strong>canopy clustering algorithm</strong> is an unsupervised pre-<a href="Data_clustering" title="wikilink">clustering</a> algorithm introduced by <a href="Andrew_McCallum" title="wikilink">Andrew McCallum</a>, Kamal Nigam and Lyle Ungar in 2000.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> It is often used as preprocessing step for the <a href="K-means_algorithm" title="wikilink">K-means algorithm</a> or the <a href="Hierarchical_clustering" title="wikilink">Hierarchical clustering</a> algorithm. It is intended to speed up <a href="Computer_cluster" title="wikilink">clustering</a> operations on large <a href="data_set" title="wikilink">data sets</a>, where using another algorithm directly may be impractical due to the size of the data set.</p>

<p>The algorithm proceeds as follows, using two thresholds 

<math display="inline" id="Canopy_clustering_algorithm:0">
 <semantics>
  <msub>
   <mi>T</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>T</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T_{1}
  </annotation>
 </semantics>
</math>

 (the loose distance) and 

<math display="inline" id="Canopy_clustering_algorithm:1">
 <semantics>
  <msub>
   <mi>T</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>T</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T_{2}
  </annotation>
 </semantics>
</math>

 (the tight distance), where 

<math display="inline" id="Canopy_clustering_algorithm:2">
 <semantics>
  <mrow>
   <msub>
    <mi>T</mi>
    <mn>1</mn>
   </msub>
   <mo>></mo>
   <msub>
    <mi>T</mi>
    <mn>2</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>T</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>T</ci>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T_{1}>T_{2}
  </annotation>
 </semantics>
</math>

 .<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<ol>
<li>Begin with the set of data points to be clustered.</li>
<li>Remove a point from the set, beginning a new 'canopy'.</li>
<li>For each point left in the set, assign it to the new canopy if the distance less than the loose distance 

<math display="inline" id="Canopy_clustering_algorithm:3">
 <semantics>
  <msub>
   <mi>T</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>T</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T_{1}
  </annotation>
 </semantics>
</math>

.</li>
<li>If the distance of the point is additionally less than the tight distance 

<math display="inline" id="Canopy_clustering_algorithm:4">
 <semantics>
  <msub>
   <mi>T</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>T</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T_{2}
  </annotation>
 </semantics>
</math>

, remove it from the original set.</li>
<li>Repeat from step 2 until there are no more data points in the set to cluster.</li>
<li>These relatively cheaply clustered canopies can be sub-clustered using a more expensive but accurate algorithm.</li>
</ol>

<p>An important note is that individual data points may be part of several canopies. As an additional speed-up, an approximate and fast distance metric can be used for 3, where a more accurate and slow distance metric can be used for step 4.</p>

<p>Since the algorithm uses distance functions and requires the specification of distance thresholds, its applicability for high-dimensional data is limited by the <a href="curse_of_dimensionality" title="wikilink">curse of dimensionality</a>. Only when a cheap and approximative – low-dimensional – distance function is available, the produced canopies will preserve the clusters produced by K-means.</p>
<h2 id="benefits">Benefits</h2>
<ul>
<li>The number of instances of training data that must be compared at each step is reduced</li>
<li>There is some evidence that the resulting clusters are improved<ref><a href="https://cwiki.apache.org/confluence/display/MAHOUT/Canopy+Clustering">Mahout description of Canopy-Clustering</a></ref></li>
</ul>

<p><code>Retrieved 2011-04-02.</code></p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Data_clustering_algorithms" title="wikilink">Category:Data clustering algorithms</a> <a href="Category:Statistical_algorithms" title="wikilink">Category:Statistical algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"></li>
<li id="fn2">McCallum, A.; Nigam, K.; and Ungar L.H. (2000) <a href="http://www.kamalnigam.com/papers/canopy-kdd00.pdf">"Efficient Clustering of High Dimensional Data Sets with Application to Reference Matching"</a>, Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, 169-178 <a href="#fnref2">↩</a></li>
<li id="fn3"><a class="uri" href="http://courses.cs.washington.edu/courses/cse590q/04au/slides/DannyMcCallumKDD00.ppt">http://courses.cs.washington.edu/courses/cse590q/04au/slides/DannyMcCallumKDD00.ppt</a> Retrieved 2014-09-06.<a href="#fnref3">↩</a></li>
</ol>
</section>
</body>
</html>
