<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="208">Linear classifier</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Linear classifier</h1>
<hr/>

<p>In the field of <a href="machine_learning" title="wikilink">machine learning</a>, the goal of <a href="statistical_classification" title="wikilink">statistical classification</a> is to use an object's characteristics to identify which class (or group) it belongs to. A <strong>linear classifier</strong> achieves this by making a classification decision based on the value of a <a href="linear_combination" title="wikilink">linear combination</a> of the characteristics. An object's characteristics are also known as <a href="Features_(pattern_recognition)" title="wikilink">feature values</a> and are typically presented to the machine in a vector called a <a href="feature_vector" title="wikilink">feature vector</a>. Such classifiers work well for practical problems such as <a href="document_classification" title="wikilink">document classification</a>, and more generally for problems with many variables (<a href="feature_vector" title="wikilink">features</a>), reaching accuracy levels comparable to non-linear classifiers while taking less time to train and use.</p>
<h2 id="definition">Definition</h2>
<figure><b>(Figure)</b>
<figcaption>In this case, the solid and empty dots can be correctly classified by any number of linear classifiers. H1 (blue) classifies them correctly, as does H2 (red). H2 could be considered "better" in the sense that it is also furthest from both groups. H3 (green) fails to correctly classify the dots.</figcaption>
</figure>

<p>If the input feature vector to the classifier is a <a href="real_number" title="wikilink">real</a> vector 

<math display="inline" id="Linear_classifier:0">
 <semantics>
  <mover accent="true">
   <mi>x</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{x}
  </annotation>
 </semantics>
</math>

, then the output score is</p>

<p>

<math display="block" id="Linear_classifier:1">
 <semantics>
  <mrow>
   <mrow>
    <mi>y</mi>
    <mo>=</mo>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mover accent="true">
        <mi>w</mi>
        <mo stretchy="false">→</mo>
       </mover>
       <mo>⋅</mo>
       <mover accent="true">
        <mi>x</mi>
        <mo stretchy="false">→</mo>
       </mover>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo>(</mo>
      <mrow>
       <munder>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mi>j</mi>
       </munder>
       <mrow>
        <msub>
         <mi>w</mi>
         <mi>j</mi>
        </msub>
        <msub>
         <mi>x</mi>
         <mi>j</mi>
        </msub>
       </mrow>
      </mrow>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>y</ci>
     <apply>
      <times></times>
      <ci>f</ci>
      <apply>
       <ci>normal-⋅</ci>
       <apply>
        <ci>normal-→</ci>
        <ci>w</ci>
       </apply>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <ci>f</ci>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <ci>j</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>w</ci>
         <ci>j</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>j</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y=f(\vec{w}\cdot\vec{x})=f\left(\sum_{j}w_{j}x_{j}\right),
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Linear_classifier:2">
 <semantics>
  <mover accent="true">
   <mi>w</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{w}
  </annotation>
 </semantics>
</math>

 is a real vector of weights and <em>f</em> is a function that converts the <a href="dot_product" title="wikilink">dot product</a> of the two vectors into the desired output. (In other words, 

<math display="inline" id="Linear_classifier:3">
 <semantics>
  <mover accent="true">
   <mi>w</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{w}
  </annotation>
 </semantics>
</math>

 is a <a class="uri" href="one-form" title="wikilink">one-form</a> or <a href="linear_functional" title="wikilink">linear functional</a> mapping 

<math display="inline" id="Linear_classifier:4">
 <semantics>
  <mover accent="true">
   <mi>x</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{x}
  </annotation>
 </semantics>
</math>

 onto <strong>R</strong>.) The weight vector 

<math display="inline" id="Linear_classifier:5">
 <semantics>
  <mover accent="true">
   <mi>w</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{w}
  </annotation>
 </semantics>
</math>

 is learned from a set of labeled training samples. Often <em>f</em> is a simple function that maps all values above a certain threshold to the first class and all other values to the second class. A more complex <em>f</em> might give the probability that an item belongs to a certain class.</p>

<p>For a two-class classification problem, one can visualize the operation of a linear classifier as splitting a high-dimensional input space with a <a class="uri" href="hyperplane" title="wikilink">hyperplane</a>: all points on one side of the hyperplane are classified as "yes", while the others are classified as "no".</p>

<p>A linear classifier is often used in situations where the speed of classification is an issue, since it is often the fastest classifier, especially when 

<math display="inline" id="Linear_classifier:6">
 <semantics>
  <mover accent="true">
   <mi>x</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{x}
  </annotation>
 </semantics>
</math>

 is sparse. Also, linear classifiers often work very well when the number of dimensions in 

<math display="inline" id="Linear_classifier:7">
 <semantics>
  <mover accent="true">
   <mi>x</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{x}
  </annotation>
 </semantics>
</math>

 is large, as in <a href="document_classification" title="wikilink">document classification</a>, where each element in 

<math display="inline" id="Linear_classifier:8">
 <semantics>
  <mover accent="true">
   <mi>x</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{x}
  </annotation>
 </semantics>
</math>

 is typically the number of occurrences of a word in a document (see <a href="document-term_matrix" title="wikilink">document-term matrix</a>). In such cases, the classifier should be well-<a href="regularization_(machine_learning)" title="wikilink">regularized</a>.</p>
<h2 id="generative-models-vs.-discriminative-models">Generative models vs. discriminative models</h2>

<p>There are two broad classes of methods for determining the parameters of a linear classifier 

<math display="inline" id="Linear_classifier:9">
 <semantics>
  <mover accent="true">
   <mi>w</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{w}
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Methods of the first class model <a href="conditional_probability" title="wikilink">conditional density functions</a> 

<math display="inline" id="Linear_classifier:10">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">|</mo>
    <mi>class</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <ci>normal-→</ci>
      <ci>x</ci>
     </apply>
     <ci>normal-|</ci>
     <csymbol cd="unknown">class</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(\vec{x}|{\rm class})
  </annotation>
 </semantics>
</math>

. Examples of such algorithms include:</p>
<ul>
<li><a href="linear_discriminant_analysis" title="wikilink">Linear Discriminant Analysis (or Fisher's linear discriminant)</a> (LDA)—assumes <a href="normal_distribution" title="wikilink">Gaussian</a> conditional density models</li>
<li><a href="Naive_Bayes_classifier" title="wikilink">Naive Bayes classifier</a> with multinomial or multivariate Bernoulli event models.</li>
</ul>

<p>The second set of methods includes <a href="discriminative_model" title="wikilink">discriminative models</a>, which attempt to maximize the quality of the output on a <a href="training_set" title="wikilink">training set</a>. Additional terms in the training cost function can easily perform <a href="regularization_(machine_learning)" title="wikilink">regularization</a> of the final model. Examples of discriminative training of linear classifiers include</p>
<ul>
<li><a href="Logistic_regression" title="wikilink">Logistic regression</a>—maximum likelihood estimation of 

<math display="inline" id="Linear_classifier:11">
 <semantics>
  <mover accent="true">
   <mi>w</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>w</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{w}
  </annotation>
 </semantics>
</math>

 assuming that the observed training set was generated by a binomial model that depends on the output of the classifier.</li>
<li><a class="uri" href="Perceptron" title="wikilink">Perceptron</a>—an algorithm that attempts to fix all errors encountered in the training set</li>
<li><a href="Support_vector_machine" title="wikilink">Support vector machine</a>—an algorithm that maximizes the <a href="Margin_(machine_learning)" title="wikilink">margin</a> between the decision hyperplane and the examples in the training set.</li>
</ul>

<p><strong>Note:</strong> Despite its name, LDA does not belong to the class of discriminative models in this taxonomy. However, its name makes sense when we compare LDA to the other main linear <a href="dimensionality_reduction" title="wikilink">dimensionality reduction</a> algorithm: <a href="principal_components_analysis" title="wikilink">principal components analysis</a> (PCA). LDA is a <a href="supervised_learning" title="wikilink">supervised learning</a> algorithm that utilizes the labels of the data, while PCA is an <a href="unsupervised_learning" title="wikilink">unsupervised learning</a> algorithm that ignores the labels. To summarize, the name is a historical artifact.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>Discriminative training often yields higher accuracy than modeling the conditional density functions. However, handling missing data is often easier with conditional density models.</p>

<p>All of the linear classifier algorithms listed above can be converted into non-linear algorithms operating on a different input space 

<math display="inline" id="Linear_classifier:12">
 <semantics>
  <mrow>
   <mi>φ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mover accent="true">
     <mi>x</mi>
     <mo stretchy="false">→</mo>
    </mover>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>φ</ci>
    <apply>
     <ci>normal-→</ci>
     <ci>x</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \varphi(\vec{x})
  </annotation>
 </semantics>
</math>

, using the <a href="kernel_trick" title="wikilink">kernel trick</a>.</p>
<h3 id="discriminative-training">Discriminative training</h3>

<p>Discriminative training of linear classifiers usually proceeds in a <a href="supervised_learning" title="wikilink">supervised</a> way, by means of an <a href="optimization_algorithm" title="wikilink">optimization algorithm</a> that is given a training set with desired outputs and a <a href="loss_function" title="wikilink">loss function</a> that measures the discrepancy between the classifier's outputs and the desired outputs. Thus, the learning algorithm solves an optimization problem of the form<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>

<math display="block" id="Linear_classifier:13">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>arg</mi>
     <mrow>
      <munder>
       <mi>min</mi>
       <mi>𝐰</mi>
      </munder>
      <mi>R</mi>
     </mrow>
    </mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>𝐰</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mrow>
    <mi>C</mi>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>N</mi>
     </munderover>
     <mrow>
      <mi>L</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>y</mi>
        <mi>i</mi>
       </msub>
       <mo>,</mo>
       <mrow>
        <msup>
         <mi>𝐰</mi>
         <mi>𝖳</mi>
        </msup>
        <msub>
         <mi>𝐱</mi>
         <mi>i</mi>
        </msub>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <apply>
      <arg></arg>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <min></min>
        <ci>𝐰</ci>
       </apply>
       <ci>R</ci>
      </apply>
     </apply>
     <ci>𝐰</ci>
    </apply>
    <apply>
     <times></times>
     <ci>C</ci>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>N</ci>
      </apply>
      <apply>
       <times></times>
       <ci>L</ci>
       <interval closure="open">
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <ci>𝐰</ci>
          <ci>𝖳</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>𝐱</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \arg\min_{\mathbf{w}}R(\mathbf{w})+C\sum_{i=1}^{N}L(y_{i},\mathbf{w}^{\mathsf{%
T}}\mathbf{x}_{i})
  </annotation>
 </semantics>
</math>

</p>

<p>where</p>
<ul>
<li>

<math display="inline" id="Linear_classifier:14">
 <semantics>
  <mi>𝐰</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐰</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{w}
  </annotation>
 </semantics>
</math>

 are the classifier's parameters,</li>
<li><mtpl></mtpl> is the loss of the prediction given the desired output <mtpl></mtpl> for the 

<math display="inline" id="Linear_classifier:15">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

'th training example,</li>
<li>

<math display="inline" id="Linear_classifier:16">
 <semantics>
  <mrow>
   <mi>R</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>𝐰</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>R</ci>
    <ci>𝐰</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R(\mathbf{w})
  </annotation>
 </semantics>
</math>

 is a <a href="Regularization_(mathematics)" title="wikilink">regularization</a> term that prevents the parameters from getting too large (causing <a class="uri" href="overfitting" title="wikilink">overfitting</a>), and</li>
<li>

<math display="inline" id="Linear_classifier:17">
 <semantics>
  <mi>C</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>C</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   C
  </annotation>
 </semantics>
</math>

 is some constant (set by the user of the learning algorithm) that weighs the regularization against the loss.</li>
</ul>

<p>Popular loss functions include the <a href="hinge_loss" title="wikilink">hinge loss</a> (for linear SVMs) and the <a href="log_loss" title="wikilink">log loss</a> (for linear logistic regression). If the regularization function 

<math display="inline" id="Linear_classifier:18">
 <semantics>
  <mi>R</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>R</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R
  </annotation>
 </semantics>
</math>

 is <a href="convex_function" title="wikilink">convex</a>, then the above is a <a href="convex_optimization" title="wikilink">convex problem</a>. Many algorithms exist for solving such problems; popular ones for linear classification include (<a href="Stochastic_gradient_descent" title="wikilink">stochastic</a>) <a href="gradient_descent" title="wikilink">gradient descent</a>, <a class="uri" href="L-BFGS" title="wikilink">L-BFGS</a>, <a href="coordinate_descent" title="wikilink">coordinate descent</a> and <a href="Newton_method" title="wikilink">Newton methods</a>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Linear_regression" title="wikilink">Linear regression</a></li>
<li><a href="Winnow_(algorithm)" title="wikilink">Winnow (algorithm)</a></li>
<li><a href="Quadratic_classifier" title="wikilink">Quadratic classifier</a></li>
<li><a href="Support_vector_machines" title="wikilink">Support vector machines</a></li>
</ul>
<h2 id="notes">Notes</h2>
<references>
</references>

<p>See also:</p>
<ol>
<li>Y. Yang, X. Liu, "A re-examination of text categorization", Proc. ACM SIGIR Conference, pp. 42–49, (1999). <a href="http://citeseer.ist.psu.edu/yang99reexamination.html">paper @ citeseer</a></li>
<li>R. Herbrich, "Learning Kernel Classifiers: Theory and Algorithms," MIT Press, (2001). ISBN 0-262-08306-X</li>
</ol>

<p>"</p>

<p><a href="Category:Classification_algorithms" title="wikilink">Category:Classification algorithms</a> <a href="Category:Statistical_classification" title="wikilink">Category:Statistical classification</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">T. Mitchell, Generative and Discriminative Classifiers: Naive Bayes and Logistic Regression. Draft Version, 2005 <a href="http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf">download</a><a href="#fnref1">↩</a></li>
<li id="fn2">A. Y. Ng and M. I. Jordan. On Discriminative vs. Generative Classifiers: A comparison of logistic regression and Naive Bayes. in NIPS 14, 2002. <a href="http://www.cs.berkeley.edu/~jordan/papers/ng-jordan-nips01.ps">download</a><a href="#fnref2">↩</a></li>
<li id="fn3">R.O. Duda, P.E. Hart, D.G. Stork, "Pattern Classification", Wiley, (2001). ISBN 0-471-05669-3<a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
</ol>
</section>
</body>
</html>
