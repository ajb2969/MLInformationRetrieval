<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1131">Evaluation of binary classifiers</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Evaluation of binary classifiers</h1>
<hr/>

<p>{| class="wikitable" align="right" width=35% style="font-size:98%; margin-left:0.5em; padding:0.25em; background:#f1f5fc;" |+ Terminology and derivations<br/>
from a confusion matrix |- valign=top |</p>
<dl>
<dt>true positive (TP)</dt>
<dd>eqv. with hit
</dd>
<dt>true negative (TN)<br/>
false positive (FP)</dt>
<dd>eqv. with <a href="false_alarm" title="wikilink">false alarm</a>, <a href="Type_I_error" title="wikilink">Type I error</a>
</dd>
<dt>false negative (FN)</dt>
<dd>eqv. with miss, <a href="Type_II_error" title="wikilink">Type II error</a>
</dd>
</dl>
<hr/>
<dl>
<dt><a href="sensitivity_(test)" title="wikilink">sensitivity</a> or true positive rate (TPR)</dt>
<dd>eqv. with <a href="hit_rate" title="wikilink">hit rate</a>, <a href="Information_retrieval#Recall" title="wikilink">recall</a>
</dd>
<dd>

<math display="inline" id="Evaluation_of_binary_classifiers:0">
 <semantics>
  <mrow>
   <mi>𝑇𝑃𝑅</mi>
   <mo>=</mo>
   <mrow>
    <mi>𝑇𝑃</mi>
    <mo>/</mo>
    <mi>P</mi>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>𝑇𝑃</mi>
    <mo>/</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>𝑇𝑃</mi>
      <mo>+</mo>
      <mi>𝐹𝑁</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>𝑇𝑃𝑅</ci>
     <apply>
      <divide></divide>
      <ci>𝑇𝑃</ci>
      <ci>P</ci>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <divide></divide>
      <ci>𝑇𝑃</ci>
      <apply>
       <plus></plus>
       <ci>𝑇𝑃</ci>
       <ci>𝐹𝑁</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathit{TPR}=\mathit{TP}/P=\mathit{TP}/(\mathit{TP}+\mathit{FN})
  </annotation>
 </semantics>
</math>


</dd>
<dt><a href="Specificity_(tests)" title="wikilink">specificity</a> (SPC) or True Negative Rate</dt>
<dd>

<math display="inline" id="Evaluation_of_binary_classifiers:1">
 <semantics>
  <mrow>
   <mi>𝑆𝑃𝐶</mi>
   <mo>=</mo>
   <mrow>
    <mi>𝑇𝑁</mi>
    <mo>/</mo>
    <mi>N</mi>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>𝑇𝑁</mi>
    <mo>/</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>𝐹𝑃</mi>
      <mo>+</mo>
      <mi>𝑇𝑁</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>𝑆𝑃𝐶</ci>
     <apply>
      <divide></divide>
      <ci>𝑇𝑁</ci>
      <ci>N</ci>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <divide></divide>
      <ci>𝑇𝑁</ci>
      <apply>
       <plus></plus>
       <ci>𝐹𝑃</ci>
       <ci>𝑇𝑁</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathit{SPC}=\mathit{TN}/N=\mathit{TN}/(\mathit{FP}+\mathit{TN})
  </annotation>
 </semantics>
</math>


</dd>
<dt><a href="Information_retrieval#Precision" title="wikilink">precision</a> or <a href="positive_predictive_value" title="wikilink">positive predictive value</a> (PPV)</dt>
<dd>

<math display="inline" id="Evaluation_of_binary_classifiers:2">
 <semantics>
  <mrow>
   <mi>𝑃𝑃𝑉</mi>
   <mo>=</mo>
   <mrow>
    <mi>𝑇𝑃</mi>
    <mo>/</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>𝑇𝑃</mi>
      <mo>+</mo>
      <mi>𝐹𝑃</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>𝑃𝑃𝑉</ci>
    <apply>
     <divide></divide>
     <ci>𝑇𝑃</ci>
     <apply>
      <plus></plus>
      <ci>𝑇𝑃</ci>
      <ci>𝐹𝑃</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathit{PPV}=\mathit{TP}/(\mathit{TP}+\mathit{FP})
  </annotation>
 </semantics>
</math>


</dd>
<dt><a href="negative_predictive_value" title="wikilink">negative predictive value</a> (NPV)</dt>
<dd>

<math display="inline" id="Evaluation_of_binary_classifiers:3">
 <semantics>
  <mrow>
   <mi>𝑁𝑃𝑉</mi>
   <mo>=</mo>
   <mrow>
    <mi>𝑇𝑁</mi>
    <mo>/</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>𝑇𝑁</mi>
      <mo>+</mo>
      <mi>𝐹𝑁</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>𝑁𝑃𝑉</ci>
    <apply>
     <divide></divide>
     <ci>𝑇𝑁</ci>
     <apply>
      <plus></plus>
      <ci>𝑇𝑁</ci>
      <ci>𝐹𝑁</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathit{NPV}=\mathit{TN}/(\mathit{TN}+\mathit{FN})
  </annotation>
 </semantics>
</math>


</dd>
<dt><a href="Information_retrieval#Fall-out" title="wikilink">fall-out</a> or false positive rate (FPR)</dt>
<dd>

<math display="inline" id="Evaluation_of_binary_classifiers:4">
 <semantics>
  <mrow>
   <mi>𝐹𝑃𝑅</mi>
   <mo>=</mo>
   <mrow>
    <mi>𝐹𝑃</mi>
    <mo>/</mo>
    <mi>N</mi>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>𝐹𝑃</mi>
    <mo>/</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>𝐹𝑃</mi>
      <mo>+</mo>
      <mi>𝑇𝑁</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>𝐹𝑃𝑅</ci>
     <apply>
      <divide></divide>
      <ci>𝐹𝑃</ci>
      <ci>N</ci>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <divide></divide>
      <ci>𝐹𝑃</ci>
      <apply>
       <plus></plus>
       <ci>𝐹𝑃</ci>
       <ci>𝑇𝑁</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathit{FPR}=\mathit{FP}/N=\mathit{FP}/(\mathit{FP}+\mathit{TN})
  </annotation>
 </semantics>
</math>


</dd>
<dt><a href="false_discovery_rate" title="wikilink">false discovery rate</a> (FDR)</dt>
<dd>

<math display="inline" id="Evaluation_of_binary_classifiers:5">
 <semantics>
  <mrow>
   <mi>𝐹𝐷𝑅</mi>
   <mo>=</mo>
   <mrow>
    <mi>𝐹𝑃</mi>
    <mo>/</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>𝐹𝑃</mi>
      <mo>+</mo>
      <mi>𝑇𝑃</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <mi>𝑃𝑃𝑉</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>𝐹𝐷𝑅</ci>
     <apply>
      <divide></divide>
      <ci>𝐹𝑃</ci>
      <apply>
       <plus></plus>
       <ci>𝐹𝑃</ci>
       <ci>𝑇𝑃</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
      <ci>𝑃𝑃𝑉</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathit{FDR}=\mathit{FP}/(\mathit{FP}+\mathit{TP})=1-\mathit{PPV}
  </annotation>
 </semantics>
</math>


</dd>
<dt>Miss Rate or <a href="Type_I_and_type_II_errors#False_positive_and_false_negative_rates" title="wikilink">False Negative Rate</a> (FNR)</dt>
<dd>

<math display="inline" id="Evaluation_of_binary_classifiers:6">
 <semantics>
  <mrow>
   <mi>𝐹𝑁𝑅</mi>
   <mo>=</mo>
   <mrow>
    <mi>𝐹𝑁</mi>
    <mo>/</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>𝐹𝑁</mi>
      <mo>+</mo>
      <mi>𝑇𝑃</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>𝐹𝑁𝑅</ci>
    <apply>
     <divide></divide>
     <ci>𝐹𝑁</ci>
     <apply>
      <plus></plus>
      <ci>𝐹𝑁</ci>
      <ci>𝑇𝑃</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathit{FNR}=\mathit{FN}/(\mathit{FN}+\mathit{TP})
  </annotation>
 </semantics>
</math>


</dd>
</dl>
<hr/>
<dl>
<dt><a class="uri" href="accuracy" title="wikilink">accuracy</a> (ACC)</dt>
<dd>

<math display="inline" id="Evaluation_of_binary_classifiers:7">
 <semantics>
  <mrow>
   <mi>𝐴𝐶𝐶</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>𝑇𝑃</mi>
      <mo>+</mo>
      <mi>𝑇𝑁</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>/</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>P</mi>
      <mo>+</mo>
      <mi>N</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>𝐴𝐶𝐶</ci>
    <apply>
     <divide></divide>
     <apply>
      <plus></plus>
      <ci>𝑇𝑃</ci>
      <ci>𝑇𝑁</ci>
     </apply>
     <apply>
      <plus></plus>
      <ci>P</ci>
      <ci>N</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathit{ACC}=(\mathit{TP}+\mathit{TN})/(P+N)
  </annotation>
 </semantics>
</math>


</dd>
<dt><a href="F1_score" title="wikilink">F1 score</a></dt>
<dd>is the <a href="Harmonic_mean#Harmonic_mean_of_two_numbers" title="wikilink">harmonic mean</a> of <a href="Information_retrieval#Precision" title="wikilink">precision</a> and <a href="sensitivity_(test)" title="wikilink">sensitivity</a>
</dd>
<dd>

<math display="inline" id="Evaluation_of_binary_classifiers:8">
 <semantics>
  <mrow>
   <mi mathvariant="italic">F1</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mn>2</mn>
     <mi>𝑇𝑃</mi>
    </mrow>
    <mo>/</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mrow>
       <mn>2</mn>
       <mi>𝑇𝑃</mi>
      </mrow>
      <mo>+</mo>
      <mi>𝐹𝑃</mi>
      <mo>+</mo>
      <mi>𝐹𝑁</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>italic-F1</ci>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <cn type="integer">2</cn>
      <ci>𝑇𝑃</ci>
     </apply>
     <apply>
      <plus></plus>
      <apply>
       <times></times>
       <cn type="integer">2</cn>
       <ci>𝑇𝑃</ci>
      </apply>
      <ci>𝐹𝑃</ci>
      <ci>𝐹𝑁</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathit{F1}=2\mathit{TP}/(2\mathit{TP}+\mathit{FP}+\mathit{FN})
  </annotation>
 </semantics>
</math>


</dd>
<dt><a href="Matthews_correlation_coefficient" title="wikilink">Matthews correlation coefficient</a> (MCC)</dt>
<dd><math> \frac{ TP \times TN - FP \times FN } {\sqrt{ (TP+FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }
</math></dd>
</dl>

<p></p>
<dl>
<dt><a href="Uncertainty_coefficient" title="wikilink">Uncertainty coefficient</a>, aka Proficiency</dt>
<dd><math>\begin{align} L &amp;= (P+N)\times \log(P+N) \\
</math></dd>
</dl>

<p>LTP &amp;= TP \times \log\frac{TP}{(TP+FP)(TP+FN)} \\ LFP &amp;= FP \times \log\frac{FP}{(FP+TP)(FP+TN)} \\ LFN &amp;= FN \times \log\frac{FN}{(FN+TP)(FN+TN)} \\ LTN &amp;= TN \times \log\frac{TN}{(TN+FP)(TN+FN)} \\ LP &amp;= P \times \log \frac{P}{P+N} \\ LN &amp;= N \times \log\frac{N}{P+N} \\ UC &amp;= \frac{L + LTP + LFP + LFN + LTN}{L + LP + LN} \end{align}</p>
<dl>
<dt>Informedness = Sensitivity + Specificity − 1<br/>
Markedness = PPV + NPV − 1<br/>
</dt>
</dl>

<p><span style="font-size:90%;"><em>Sources: Fawcett (2006) and Powers (2011).</em><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></span> |}</p>
<figure><b>(Figure)</b>
<figcaption>From the <a href="confusion_matrix" title="wikilink">confusion matrix</a> you can derive four basic measures</figcaption>
</figure>

<p>There are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine <a href="sensitivity_and_specificity" title="wikilink">sensitivity and specificity</a> are often used, while in computer science <a href="precision_and_recall" title="wikilink">precision and recall</a> are preferred. An important distinction is between metrics that are independent on the <a class="uri" href="prevalence" title="wikilink">prevalence</a> (how often each category occurs in the population), and metrics that depend on the prevalence – both types are useful, but they have very different properties.</p>
<h2 id="contingency-table">Contingency table</h2>

<p>Given a data set, a classification (the output of a classifier on that set) gives two numbers: the number of positives and the number of negatives, which add up to the total size of the set. To evaluate a classifier, one compares its output to another reference classification – ideally a perfect classification, but in practice the output of another <a href="gold_standard_(test)" title="wikilink">gold standard</a> test – and <a href="cross_tabulation" title="wikilink">cross tabulates</a> the data into a 2×2 <a href="contingency_table" title="wikilink">contingency table</a>, comparing the two classifications. One then evaluates the classifier <em>relative</em> to the gold standard by computing <a href="summary_statistic" title="wikilink">summary statistics</a> of these 4 numbers. Generally these statistics will be <a href="scale_invariant" title="wikilink">scale invariant</a> (scaling all the numbers by the same factor does not change the output), to make them independent of population size, which is achieved by using ratios of <a href="homogeneous_function" title="wikilink">homogeneous functions</a>, most simply <a href="homogeneous_linear" title="wikilink">homogeneous linear</a> or <a href="homogeneous_quadratic" title="wikilink">homogeneous quadratic</a> functions.</p>

<p>Say we test some people for the presence of a disease. Some of these people have the disease, and our test correctly says they are positive. They are called <em><a href="true_positive" title="wikilink">true positives</a></em> (TP). Some have the disease, but the test incorrectly claims they don't. They are called <em><a href="false_negative" title="wikilink">false negatives</a></em> (FN). Some don't have the disease, and the test says they don't – <em><a href="true_negative" title="wikilink">true negatives</a></em> (TN). Finally, there might be healthy people who have a positive test result – <em><a href="false_positive" title="wikilink">false positives</a></em> (FP). These can be arranged into a 2×2 contingency table (<a href="confusion_matrix" title="wikilink">confusion matrix</a>), conventionally with the test result on the vertical axis and the actual condition on the horizontal axis.</p>

<p>These numbers can then be totaled, yielding both a <a href="grand_total" title="wikilink">grand total</a> and <a href="marginal_total" title="wikilink">marginal totals</a>. Totaling the entire table, the number of true positives, false negatives, true negatives, and false positives add up to 100% of the set. Totaling the rows (adding horizontally) the number of true positives and false positives add up to 100% of the test positives, and likewise for negatives. Totaling the columns (adding vertically), the number of true positives and false negatives add up to 100% of the condition positives (conversely for negatives). The basic marginal ratio statistics are obtained by dividing the 2×2=4 values in the table by the marginal totals (either rows or columns), yielding 2 auxiliary 2×2 tables, for a total of 8 ratios. These ratios come in 4 complementary pairs, each pair summing to 1, and so each of these derived 2×2 tables can be summarized as a pair of 2 numbers, together with their complements. Further statistics can be obtained by taking ratios of these ratios, ratios of ratios of ratios, or more complicated functions.</p>

<p>The contingency table and the most common derived ratios are summarized below; see sequel for details.</p>

<p>Note that the columns correspond to the condition actually being positive or negative (or classified as such by the gold standard), as indicated by the color-coding, and the associated statistics are prevalence-independent, while the rows correspond to the <em>test</em> being positive or negative, and the associated statistics are prevalence-dependent. There are analogous likelihood ratios for prediction values, but these are less commonly used, and not depicted above.</p>
<h2 id="sensitivity-and-specificity">Sensitivity and specificity</h2>

<p>The fundamental prevalence-independent statistics are <a href="sensitivity_and_specificity" title="wikilink">sensitivity and specificity</a>.</p>

<p><strong><a href="Sensitivity_(tests)" title="wikilink">Sensitivity</a></strong> or <a href="True_Positive_Rate" title="wikilink">True Positive Rate</a> (TPR), also known as <a href="Recall_(information_retrieval)" title="wikilink">recall</a>, is the proportion of people that tested positive and are positive (True Positive, TP) of all the people that actually are positive (Condition Positive, CP = TP + FN). It can be seen as <em>the probability that the test is positive given that the patient is sick</em>. With higher sensitivity, fewer actual cases of disease go undetected (or, in the case of the factory quality control, fewer faulty products go to the market).</p>

<p><strong><a href="Specificity_(tests)" title="wikilink">Specificity</a></strong> (SPC) or <a href="True_Negative_Rate" title="wikilink">True Negative Rate</a> (TNR) is the proportion of people that tested negative and are negative (True Negative, TN) of all the people that actually are negative (Condition Negative, CN = TN + FP). As with sensitivity, it can be looked at as <em>the probability that the test result is negative given that the patient is not sick</em>. With higher specificity, fewer healthy people are labeled as sick (or, in the factory case, fewer good products are discarding).</p>

<p>The relationship between sensitivity and specificity, as well as the performance of the classifier, can be visualized and studied using the <a href="Receiver_Operating_Characteristic" title="wikilink">Receiver Operating Characteristic</a> (ROC) curve.</p>

<p>In theory, sensitivity and specificity are independent in the sense that it is possible to achieve 100% in both (such as in the red/blue ball example given above). In more practical, less contrived instances, however, there is usually a trade-off, such that they are inversely proportional to one another to some extent. This is because we rarely measure the actual thing we would like to classify; rather, we generally measure an indicator of the thing we would like to classify, referred to as a <a href="surrogate_endpoint" title="wikilink">surrogate marker</a>. The reason why 100% is achievable in the ball example is because redness and blueness is determined by directly detecting redness and blueness. However, indicators are sometimes compromised, such as when non-indicators mimic indicators or when indicators are time-dependent, only becoming evident after a certain lag time. The following example of a pregnancy test will make use of such an indicator.</p>

<p>Modern pregnancy tests <em>do not</em> use the pregnancy itself to determine pregnancy status; rather, <a href="human_chorionic_gonadotropin" title="wikilink">human chorionic gonadotropin</a> is used, or hCG, present in the urine of <a class="uri" href="gravid" title="wikilink">gravid</a> females, as a <em>surrogate marker to indicate</em> that a woman is pregnant. Because hCG can also be produced by a <a href="neoplasm" title="wikilink">tumor</a>, the specificity of modern pregnancy tests cannot be 100% (in that false positives are possible). Also, because hCG is present in the urine in such small concentrations after fertilization and early <a class="uri" href="embryogenesis" title="wikilink">embryogenesis</a>, the sensitivity of modern pregnancy tests cannot be 100% (in that false negatives are possible).</p>
<h3 id="likelihood-ratios">Likelihood ratios</h3>
<h2 id="positive-and-negative-predictive-values">Positive and negative predictive values</h2>

<p>In addition to sensitivity and specificity, the performance of a binary classification test can be measured with <a href="positive_predictive_value" title="wikilink">positive predictive value</a> (PPV), also known as <a href="Precision_(information_retrieval)" title="wikilink">precision</a>, and <a href="negative_predictive_value" title="wikilink">negative predictive value</a> (NPV). The positive prediction value answers the question "If the test result is <em>positive</em>, how well does that <em>predict</em> an actual presence of disease?". It is calculated as TP/(TP + FP); that is, it is the proportion of true positives out of all positive results. The negative prediction value is the same, but for negatives, naturally.</p>
<h3 id="impact-of-prevalence-on-prediction-values">Impact of prevalence on prediction values</h3>

<p>Prevalence has a significant impact on prediction values. As an example, suppose there is a test for a disease with 99% sensitivity and 99% specificity. If 2000 people are tested and the prevalence (in the sample) is 50%, 1000 of them are sick and 1000 of them are healthy. Thus about 990 true positives and 990 true negatives are likely, with 10 false positives and 10 false negatives. The positive and negative prediction values would be 99%, so there can be high confidence in the result.</p>

<p>However, if the prevalence is only 5%, so of the 2000 people only 100 are really sick, then the prediction values change significantly. The likely result is 99 true positives, 1 false negative, 1881 true negatives and 19 false positives. Of the 19+99 people tested positive, only 99 really have the disease – that means, intuitively, that given that a patient's test result is positive, there is only 84% chance that they really have the disease. On the other hand, given that the patient's test result is negative, there is only 1 chance in 1882, or 0.05% probability, that the patient has the disease despite the test result.</p>
<h3 id="likelihood-ratios-1">Likelihood ratios</h3>
<h2 id="precision-and-recall">Precision and recall</h2>
<h3 id="relationships">Relationships</h3>

<p>There are various relationships between these ratios.</p>

<p>If the prevalence, sensitivity, and specificity are known, the positive predictive value can be obtained from the following identity:</p>
<dl>
<dd><dl>
<dd>

<math display="inline" id="Evaluation_of_binary_classifiers:9">
 <semantics>
  <mrow>
   <mtext>PPV</mtext>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mtext>sensitivity</mtext>
      <mo stretchy="false">)</mo>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mtext>prevalence</mtext>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mtext>sensitivity</mtext>
       <mo stretchy="false">)</mo>
      </mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mtext>prevalence</mtext>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>+</mo>
     <mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mn>1</mn>
        <mo>-</mo>
        <mtext>specificity</mtext>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mn>1</mn>
        <mo>-</mo>
        <mtext>prevalence</mtext>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <mtext>PPV</mtext>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <mtext>sensitivity</mtext>
      <mtext>prevalence</mtext>
     </apply>
     <apply>
      <plus></plus>
      <apply>
       <times></times>
       <mtext>sensitivity</mtext>
       <mtext>prevalence</mtext>
      </apply>
      <apply>
       <times></times>
       <apply>
        <minus></minus>
        <cn type="integer">1</cn>
        <mtext>specificity</mtext>
       </apply>
       <apply>
        <minus></minus>
        <cn type="integer">1</cn>
        <mtext>prevalence</mtext>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \text{PPV}=\frac{(\text{sensitivity})(\text{prevalence})}{(\text{sensitivity})%
(\text{prevalence})+(1-\text{specificity})(1-\text{prevalence})}
  </annotation>
 </semantics>
</math>


</dd>
</dl>
</dd>
</dl>

<p>If the prevalence, sensitivity, and specificity are known, the negative predictive value can be obtained from the following identity:</p>
<dl>
<dd><dl>
<dd><math>
</math></dd>
</dl>
</dd>
</dl>

<p>\text{NPV} = \frac{(\text{specificity}) (1 - \text{prevalence})}{(\text{specificity}) (1 - \text{prevalence}) + (1 - \text{sensitivity}) (\text{prevalence})}. </p>
<h2 id="single-metrics">Single metrics</h2>

<p>In addition to the paired metrics, there are also single metrics that give a single number to evaluate the test.</p>

<p>Perhaps the simplest statistic is <a href="Accuracy_and_precision#In_binary_classification" title="wikilink">accuracy</a> or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications: (TP + TN)/Total Population = (TP + TN)/(TP + TN + FP + FN). This is often not very useful, compared to the marginal ratios, as it does not yield useful marginal interpretations, due to mixing true positives (test positive, condition positive) and true negatives (test negative, condition negative) – in terms of the condition table, it sums the diagonal; further, it is prevalence-dependent. The complement is the Fraction Incorrect (FiC): FC + FiC = 1, or (FP + FN)/(TP + TN + FP + FN) – this is the sum of the <a class="uri" href="antidiagonal" title="wikilink">antidiagonal</a>, divided by the total population.</p>

<p>The <a href="diagnostic_odds_ratio" title="wikilink">diagnostic odds ratio</a> (DOR) is a more useful overall metric, which can be defined directly as (TP×TN)/(FP×FN) = (TP/FN)/(FP/TN), or indirectly as a ratio of ratio of ratios (ratio of likelihood ratios, which are themselves ratios of True Rates or Prediction Values). This has a useful interpretation – as an <a href="odds_ratio" title="wikilink">odds ratio</a> – and is prevalence-independent.</p>

<p>An <a class="uri" href="F-score" title="wikilink">F-score</a> is a combination of the <a href="Precision_(information_retrieval)" title="wikilink">precision</a> and the <a href="Recall_(information_retrieval)" title="wikilink">recall</a>, providing a single score. There is a one-parameter family of statistics, with parameter <em>β,</em> which determines the relative weights of precision and recall. The traditional or balanced F-score (<a href="F1_score" title="wikilink">F1 score</a>) is the <a href="Harmonic_mean#Harmonic_mean_of_two_numbers" title="wikilink">harmonic mean</a> of precision and recall:</p>

<p>

<math display="block" id="Evaluation_of_binary_classifiers:10">
 <semantics>
  <mrow>
   <msub>
    <mi>F</mi>
    <mn>1</mn>
   </msub>
   <mo>=</mo>
   <mrow>
    <mn>2</mn>
    <mo>⋅</mo>
    <mfrac>
     <mrow>
      <mi>precision</mi>
      <mo>⋅</mo>
      <mi>recall</mi>
     </mrow>
     <mrow>
      <mi>precision</mi>
      <mo>+</mo>
      <mi>recall</mi>
     </mrow>
    </mfrac>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>F</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <ci>normal-⋅</ci>
     <cn type="integer">2</cn>
     <apply>
      <divide></divide>
      <apply>
       <ci>normal-⋅</ci>
       <ci>precision</ci>
       <ci>recall</ci>
      </apply>
      <apply>
       <plus></plus>
       <ci>precision</ci>
       <ci>recall</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F_{1}=2\cdot\frac{\mathrm{precision}\cdot\mathrm{recall}}{\mathrm{precision}+%
\mathrm{recall}}
  </annotation>
 </semantics>
</math>

.</p>
<h3 id="alternative-metrics">Alternative metrics</h3>

<p>Note, however, that the F-scores do not take the true negative rate into account, and that measures such as the Phi coefficient, <a href="Matthews_correlation_coefficient" title="wikilink">Matthews correlation coefficient</a>, Informedness or Cohen's kappa may be preferable to assess the performance of a binary classifier.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> As a <a href="Correlation_and_dependence" title="wikilink">correlation coefficient</a>, the Matthews correlation coefficient is the <a href="geometric_mean" title="wikilink">geometric mean</a> of the <a href="regression_coefficient" title="wikilink">regression coefficients</a> of the problem and its <a href="Dual_(mathematics)" title="wikilink">dual</a>. The component regression coefficients of the Matthews correlation coefficient are <a class="uri" href="markedness" title="wikilink">markedness</a> (deltap) and informedness (deltap').<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>Other metrics include <a href="Youden's_J_statistic" title="wikilink">Youden's J statistic</a>.</p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Statistical_classification" title="wikilink">Category:Statistical classification</a> <a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
</ol>
</section>
</body>
</html>
