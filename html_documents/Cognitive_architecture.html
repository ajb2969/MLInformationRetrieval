<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1265">Cognitive architecture</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Cognitive architecture</h1>
<hr/>

<p>A <strong>cognitive architecture</strong> can refer to a theory about the structure of the <a href="human_mind" title="wikilink">human mind</a>. One of the main goals of a cognitive architecture is to summarize the various results of cognitive psychology in a comprehensive computer model. However, the results need to be in a formalized form so far that they can be the basis of a computer program. The formalized models can be used to further refine a comprehensive theory of cognition, and more immediately, as a commercially usable model. Successful cognitive architectures include <a class="uri" href="ACT-R" title="wikilink">ACT-R</a> (Adaptive Control of Thought, ACT), <a href="Soar_(cognitive_architecture)" title="wikilink">SOAR</a> and <a class="uri" href="OpenCog" title="wikilink">OpenCog</a>.</p>
<h2 id="history">History</h2>

<p><a href="Herbert_A._Simon" title="wikilink">Herbert A. Simon</a>, one of the founders of the field of artificial intelligence, stated that the 1960 thesis by his student <a href="Ed_Feigenbaum" title="wikilink">Ed Feigenbaum</a>, <a class="uri" href="EPAM" title="wikilink">EPAM</a> provided a possible "architecture for cognition"<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> because it included some commitments for how more than one fundamental aspect of the human mind worked. In EPAM's case, human memory and human learning.</p>

<p><a href="John_Robert_Anderson_(psychologist)" title="wikilink">John R. Anderson</a> started research on human memory in the early 1970s and his 1973 thesis with <a href="Gordon_H._Bower" title="wikilink">Gordon H. Bower</a> provided a theory of human associative memory.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> He included more aspects of his research on long-term memory and thinking processes into this research and eventually designed a cognitive architecture he eventually called <a href="ACT-R" title="wikilink">ACT</a>. He and his student used the term "cognitive architecture" in his lab to refer to the ACT theory as embodied in the collection of papers and designs since they didn't yet have any sort of complete implementation at the time.</p>

<p>In 1983 John R. Anderson published the seminal work in this area, entitled <em>The Architecture of Cognition.</em><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> One can distinguish between the theory of cognition and the implementation of the theory. The theory of cognition outlined the structure of the various parts of the mind and made commitments to the use of rules, associative networks, and other aspects. The cognitive architecture implements the theory on computers. The software used to implement the cognitive architectures were also "cognitive architectures". Thus, a cognitive architecture can also refer to a blueprint for <a href="intelligent_agent" title="wikilink">intelligent agents</a>. It proposes (artificial) <a href="computation" title="wikilink">computational</a> processes that act like certain cognitive systems, most often, like a person, or acts <a href="intelligence_(trait)" title="wikilink">intelligent</a> under some definition. Cognitive architectures form a subset of general <a href="agent_architecture" title="wikilink">agent architectures</a>. The term 'architecture' implies an approach that attempts to model not only behavior, but also structural properties of the modelled system.</p>
<h2 id="distinctions">Distinctions</h2>

<p>Cognitive architectures can be <a href="symbol" title="wikilink">symbolic</a>, <a href="connectionism" title="wikilink">connectionist</a>, or <a href="hybrid_intelligent_system" title="wikilink">hybrid</a>. Some cognitive architectures or models are based on a set of <a href="Cognitivism_(psychology)" title="wikilink">generic rules</a>, as, e.g., the <a href="Information_Processing_Language" title="wikilink">Information Processing Language</a> (e.g., <a href="Soar_(cognitive_architecture)" title="wikilink">Soar</a> based on the <a href="unified_theory_of_cognition" title="wikilink">unified theory of cognition</a>, or similarly <a class="uri" href="ACT-R" title="wikilink">ACT-R</a>). Many of these architectures are based on the-mind-is-like-a-computer analogy. In contrast subsymbolic processing specifies no such rules a priori and relies on emergent properties of processing units (e.g. nodes). Hybrid architectures combine both types of processing (such as <a href="CLARION_(cognitive_architecture)" title="wikilink">CLARION</a>). A further distinction is whether the architecture is <a class="uri" href="centralized" title="wikilink">centralized</a> with a neural correlate of a <a href="computer_processor" title="wikilink">processor</a> at its core, or <a class="uri" href="decentralized" title="wikilink">decentralized</a> (distributed). The decentralized flavor, has become popular under the name of <a href="parallel_distributed_processing" title="wikilink">parallel distributed processing</a> in mid-1980s and <a class="uri" href="connectionism" title="wikilink">connectionism</a>, a prime example being <a href="neural_network" title="wikilink">neural networks</a>. A further design issue is additionally a decision between <a href="holism" title="wikilink">holistic</a> and <a href="atomism" title="wikilink">atomistic</a>, or (more concrete) <a href="modularity_(programming)" title="wikilink">modular</a> structure. By analogy, this extends to issues of <a href="knowledge_representation" title="wikilink">knowledge representation</a>.</p>

<p>In traditional <a href="artificial_intelligence" title="wikilink">AI</a>, <a href="intelligence_(trait)" title="wikilink">intelligence</a> is often programmed from above: the programmer is the <a href="creationist" title="wikilink">creator</a>, and makes something and imbues it with its intelligence, though many traditional AI systems were also designed to learn (e.g. improving their game-playing or problem-solving competence). <a href="Biologically_inspired_computing" title="wikilink">Biologically inspired computing</a>, on the other hand, takes sometimes a more <a href="top-down_and_bottom-up_design" title="wikilink">bottom-up</a>, <a href="decentralisation" title="wikilink">decentralised</a> approach; bio-inspired techniques often involve the method of specifying a set of simple generic rules or a set of simple nodes, from the interaction of which <a href="emergence" title="wikilink">emerges</a> the overall behavior. It is hoped to build up <a class="uri" href="complexity" title="wikilink">complexity</a> until the end result is something markedly complex (see <a href="complex_system" title="wikilink">complex systems</a>). However, it is also arguable that systems designed <a href="top-down_and_bottom-up_design" title="wikilink">top-down</a> on the basis of observations of what humans and other animals can do rather than on observations of brain mechanisms, are also biologically inspired, though in a different way.</p>
<h2 id="some-well-known-cognitive-architectures">Some well-known cognitive architectures</h2>

<p>A comprehensive review of implemented cognitive architectures has been undertaken in 2010 by Samsonovish et. al.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> and is available as an online repository.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> Some well-known cognitive architectures, in alphabetical order:</p>
<ul>
<li><a class="uri" href="4CAPS" title="wikilink">4CAPS</a>, developed at <a href="Carnegie_Mellon_University" title="wikilink">Carnegie Mellon University</a> under <a href="Marcel_Just" title="wikilink">Marcel A. Just</a></li>
<li><a class="uri" href="ACT-R" title="wikilink">ACT-R</a>, developed at <a href="Carnegie_Mellon_University" title="wikilink">Carnegie Mellon University</a> under <a href="John_Robert_Anderson_(psychologist)" title="wikilink">John R. Anderson</a>.</li>
<li><a class="uri" href="ALifeE" title="wikilink">ALifeE</a>, developed under <a href="Toni_Conde" title="wikilink">Toni Conde</a> at the <a href="Ecole_Polytechnique_Fédérale_de_Lausanne" title="wikilink">Ecole Polytechnique Fédérale de Lausanne</a>.</li>
<li><a href="Apex_(cognitive_architecture)" title="wikilink">Apex</a> developed under <a href="Michael_Freed" title="wikilink">Michael Freed</a> at <a href="NASA_Ames_Research_Center" title="wikilink">NASA Ames Research Center</a>.</li>
<li><a class="uri" href="ASMO" title="wikilink">ASMO</a>, developed under <a href="Rony_Novianto" title="wikilink">Rony Novianto</a> at <a href="University_of_Technology,_Sydney" title="wikilink">University of Technology, Sydney</a>.</li>
<li><a href="CHREST_(cognitive_architecture)" title="wikilink">CHREST</a>, developed under <a href="Fernand_Gobet" title="wikilink">Fernand Gobet</a> at <a href="Brunel_University" title="wikilink">Brunel University</a> and Peter C. Lane at the <a href="University_of_Hertfordshire" title="wikilink">University of Hertfordshire</a>.</li>
<li><a href="CLARION_(cognitive_architecture)" title="wikilink">CLARION</a> the cognitive architecture, developed under <a href="Ron_Sun" title="wikilink">Ron Sun</a> at <a href="Rensselaer_Polytechnic_Institute" title="wikilink">Rensselaer Polytechnic Institute</a> and University of Missouri.</li>
<li><a href="Cerebellar_model_articulation_controller" title="wikilink">CMAC</a> - The Cerebellar Model Articulation Controller (CMAC) is a type of neural network based on a model of the mammalian <a class="uri" href="cerebellum" title="wikilink">cerebellum</a>. It is a type of <a href="Association_(psychology)" title="wikilink">associative</a> <a class="uri" href="memory" title="wikilink">memory</a>.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> The CMAC was first proposed as a function modeler for <a href="Robot_control" title="wikilink">robotic controllers</a> by <a href="James_Albus" title="wikilink">James Albus</a> in 1975 and has been extensively used in <a href="reinforcement_learning" title="wikilink">reinforcement learning</a> and also as for automated <a class="uri" href="classification" title="wikilink">classification</a> in the <a href="machine_learning" title="wikilink">machine learning</a> community.</li>
<li><a class="uri" href="CMatie" title="wikilink">CMatie</a> is a ‘conscious’ software agent developed to manage seminar announcements in the Mathematical Sciences Department at the <a href="University_of_Memphis" title="wikilink">University of Memphis</a>. It's based on <a href="Sparse_distributed_memory" title="wikilink">Sparse distributed memory</a> augmented with the use of <a href="Genetic_algorithm" title="wikilink">genetic algorithms</a> as an <a href="associative_memory" title="wikilink">associative memory</a>.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></li>
<li><a href="Copycat_(software)" title="wikilink">Copycat</a>, by <a href="Douglas_Hofstadter" title="wikilink">Douglas Hofstadter</a> and <a href="Melanie_Mitchell" title="wikilink">Melanie Mitchell</a> at the <a href="Indiana_University_(Bloomington)" title="wikilink">Indiana University</a>.</li>
<li><a href="DUAL_Cognitive_Architecture" title="wikilink">DUAL</a>, developed at the <a href="New_Bulgarian_University" title="wikilink">New Bulgarian University</a> under <a href="Boicho_Kokinov" title="wikilink">Boicho Kokinov</a>.</li>
<li>EPIC, developed under David E. Kieras and David E. Meyer at the <a href="University_of_Michigan" title="wikilink">University of Michigan</a>.</li>
<li><a class="uri" href="FORR" title="wikilink">FORR</a> developed by Susan L. Epstein at <a href="The_City_University_of_New_York" title="wikilink">The City University of New York</a>.</li>
<li>GAIuS developed by Sevak Avakians.</li>
<li><a href="Google_DeepMind" title="wikilink">Google DeepMind</a> - The company has created a <a href="neural_network" title="wikilink">neural network</a> that learns how to play <a href="video_games" title="wikilink">video games</a> in a similar fashion to humans<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> and a neural network that may be able to access an external memory like a conventional <a href="Turing_machine" title="wikilink">Turing machine</a>,<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> resulting in a computer that appears to possibly mimic the <a href="short-term_memory" title="wikilink">short-term memory</a> of the human brain. The underlying algorithm is based on a combination of <a class="uri" href="Q-learning" title="wikilink">Q-learning</a> with multilayer <a href="recurrent_neural_network" title="wikilink">recurrent neural network</a>.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> (Also see an overview by <a href="Jürgen_Schmidhuber" title="wikilink">Jürgen Schmidhuber</a> on earlier related work in <a href="Deep_learning" title="wikilink">Deep learning</a><a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a>)</li>
<li>The <a class="uri" href="H-Cogaff" title="wikilink">H-Cogaff</a> architecture, which is a special case of the <a class="uri" href="CogAff" title="wikilink">CogAff</a> schema.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><ref></ref></li>
</ul>

<p><a href="http://www.cs.bham.ac.uk/research/projects/cogaff/00-02.html#89">A Framework for comparing agent architectures</a>, Aaron Sloman and Matthias Scheutz, in Proceedings of the UK Workshop on Computational Intelligence, Birmingham, UK, September 2002.</p>
<ul>
<li><a href="Hierarchical_temporal_memory" title="wikilink">Hierarchical temporal memory</a> is an <a href="online_machine_learning" title="wikilink">online machine learning</a> model developed by <a href="Jeff_Hawkins" title="wikilink">Jeff Hawkins</a> and <a href="Dileep_George" title="wikilink">Dileep George</a> of <a href="Numenta" title="wikilink">Numenta, Inc.</a> that models some of the structural and <a href="algorithm" title="wikilink">algorithmic</a> properties of the <a class="uri" href="neocortex" title="wikilink">neocortex</a>. HTM is a <a href="bionics" title="wikilink">biomimetic</a> model based on the <a href="memory-prediction_framework" title="wikilink">memory-prediction</a> theory of brain function described by <a href="Jeff_Hawkins" title="wikilink">Jeff Hawkins</a> in his book <em><a href="On_Intelligence" title="wikilink">On Intelligence</a></em>. HTM is a method for discovering and inferring the high-level causes of observed input patterns and sequences, thus building an increasingly complex model of the world.</li>
<li><a href="JACK_Intelligent_Agents" title="wikilink">CoJACK</a> An <a class="uri" href="ACT-R" title="wikilink">ACT-R</a> inspired extension to the <a href="JACK_Intelligent_Agents" title="wikilink">JACK</a> multi-agent system that adds a cognitive architecture to the agents for eliciting more realistic (human-like) behaviors in virtual environments.</li>
<li><a href="LIDA_(cognitive_architecture)" title="wikilink">IDA and LIDA</a>, implementing <a href="Global_Workspace_Theory" title="wikilink">Global Workspace Theory</a>, developed under <a href="Stan_Franklin" title="wikilink">Stan Franklin</a> at the <a href="University_of_Memphis" title="wikilink">University of Memphis</a>.</li>
<li><a href="Memory_Networks" title="wikilink">Memory Networks</a> - created by <a class="uri" href="Facebook" title="wikilink">Facebook</a> AI research group in 2014 this architecture presents a new class of <a href="Machine_learning" title="wikilink">learning</a> models called memory networks. Memory networks reason with <a class="uri" href="inference" title="wikilink">inference</a> components combined with a <a href="long-term_memory" title="wikilink">long-term memory</a> component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></li>
<li><a class="uri" href="OpenCog" title="wikilink">OpenCog</a>, an open-source implementation of reasoning, natural language processing, psi-theory and robotic control.</li>
<li><a href="MANIC_(Cognitive_Architecture)" title="wikilink">MANIC (Cognitive Architecture)</a>, Michael S. Gashler, University of Arkansas.</li>
<li><a class="uri" href="PreAct" title="wikilink">PreAct</a>, developed under Dr. Norm Geddes at ASI.</li>
<li><a class="uri" href="PRODIGY" title="wikilink">PRODIGY</a>, by Veloso et al.</li>
<li><a href="Procedural_Reasoning_System" title="wikilink">PRS</a> 'Procedural Reasoning System', developed by <a href="Michael_Georgeff" title="wikilink">Michael Georgeff</a> and <a href="Amy_L._Lansky" title="wikilink">Amy Lansky</a> at <a href="SRI_International" title="wikilink">SRI International</a>.</li>
<li><a class="uri" href="Psi-Theory" title="wikilink">Psi-Theory</a> developed under <a href="Dietrich_Dörner" title="wikilink">Dietrich Dörner</a> at the <a href="Otto-Friedrich_University" title="wikilink">Otto-Friedrich University</a> in <a class="uri" href="Bamberg" title="wikilink">Bamberg</a>, <a class="uri" href="Germany" title="wikilink">Germany</a>.</li>
<li><a class="uri" href="R-CAST" title="wikilink">R-CAST</a>, developed at the <a href="Pennsylvania_State_University" title="wikilink">Pennsylvania State University</a>.</li>
<li><a href="Spaun_(Semantic_Pointer_Architecture_Unified_Network)" title="wikilink">Spaun (Semantic Pointer Architecture Unified Network)</a> - by Chris Eliasmith at the Centre for Theoretical Neuroscience at the <a href="University_of_Waterloo" title="wikilink">University of Waterloo</a> - Spaun is a network of 2,500,000 artificial spiking neurons, which uses groups of these neurons to complete cognitive tasks via flexibile coordination. Components of the model communicate using spiking neurons that implement neural representations called “semantic pointers” using various firing patterns. Semantic pointers can be understood as being elements of a compressed neural vector space.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></li>
<li><a href="Soar_(cognitive_architecture)" title="wikilink">Soar</a>, developed under <a href="Allen_Newell" title="wikilink">Allen Newell</a> and <a href="John_E._Laird" title="wikilink">John Laird</a> at <a href="Carnegie_Mellon_University" title="wikilink">Carnegie Mellon University</a> and the <a href="University_of_Michigan" title="wikilink">University of Michigan</a>.</li>
<li><a href="Society_of_mind" title="wikilink">Society of mind</a> and its successor the <a href="Emotion_machine" title="wikilink">Emotion machine</a> proposed by <a href="Marvin_Minsky" title="wikilink">Marvin Minsky</a>.</li>
<li><a href="Sparse_distributed_memory" title="wikilink">Sparse distributed memory</a> was proposed by <a href="Pentti_Kanerva" title="wikilink">Pentti Kanerva</a> at <a href="Ames_Research_Center" title="wikilink">NASA Ames Research Center</a> as a realizable architecture that could store large patterns and retrieve them based on partial matches with patterns representing current sensory inputs.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> This memory exhibits behaviors, both in theory and in experiment, that resemble those previously unapproached by machines - e.g., rapid recognition of faces or odors, discovery of new connections between seemingly unrelated ideas, etc. Sparse distributed memory is used for storing and retrieving large amounts (

<math display="inline" id="Cognitive_architecture:0">
 <semantics>
  <msup>
   <mn>2</mn>
   <mn>1000</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <cn type="integer">2</cn>
    <cn type="integer">1000</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   2^{1000}
  </annotation>
 </semantics>
</math>

 <a href="bit" title="wikilink">bits</a>) of information without focusing on the accuracy but on similarity of information.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> There are some recent applications in robot navigation<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> and experience-based robot manipulation.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a></li>
<li><a class="uri" href="Sparsey" title="wikilink">Sparsey</a> by Neurithmic Systems is an event recognition framework via deep hierarchical sparse distributed codes<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a></li>
<li><a href="Subsumption_architecture" title="wikilink">Subsumption architectures</a>, developed e.g. by <a href="Rodney_Brooks" title="wikilink">Rodney Brooks</a> (though it could be argued whether they are <em>cognitive</em>).</li>
<li><a href="QuBIC:_Quantum_and_Bio-inspired_Cognitive_Architecture_for_Machine_Consciousness" title="wikilink">QuBIC: Quantum and Bio-inspired Cognitive Architecture for Machine Consciousness</a> developed by Wajahat M. Qazi and Khalil Ahmad at Department of Computer Science, GC University Lahore Pakistan and School of Computer Science, NCBA&amp;E; Lahore, Pakistan</li>
<li><a href="http://tinycog.sourceforge.net">TinyCog</a> a minimalist open-source implementation of a cognitive architecture based on the ideas of Scene Based Reasoning</li>
<li><a href="Vector_LIDA" title="wikilink">Vector LIDA</a> is a variation of the <a href="LIDA_(cognitive_architecture)" title="wikilink">LIDA</a> cognitive architecture that employs high-dimensional <a href="Modular_Composite_Representation_(MCR)" title="wikilink">Modular Composite Representation (MCR)</a> vectors as its main representation model and Integer <a href="Sparse_distributed_memory" title="wikilink">Sparse Distributed Memory</a><a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> as its main memory implementation technology. The advantages of this new model include a more realistic and biologically plausible model, better integration with its <a href="episodic_memory" title="wikilink">episodic memory</a>, better integration with other low level perceptual processing (such as <a href="deep_learning" title="wikilink">deep learning</a> systems), better scalability, and easier learning mechanisms.<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a></li>
<li><a class="uri" href="VisNet" title="wikilink">VisNet</a> by <a href="Edmund_Rolls" title="wikilink">Edmund Rolls</a> at the <a href="University_of_Oxford" title="wikilink">Oxford</a> Centre for Computational Neuroscience - A feature hierarchy model in which invariant representations can be built by self-organizing learning based on the temporal and spatial statistics of the visual input produced by objects as they transform in the world.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a></li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Artificial_brain" title="wikilink">Artificial brain</a></li>
<li><a href="Artificial_consciousness" title="wikilink">Artificial consciousness</a></li>
<li><a href="Autonomous_agent" title="wikilink">Autonomous agent</a></li>
<li><a href="Biologically_inspired_cognitive_architectures" title="wikilink">Biologically inspired cognitive architectures</a></li>
<li><a href="Blue_Brain_Project" title="wikilink">Blue Brain Project</a></li>
<li><a href="BRAIN_Initiative" title="wikilink">BRAIN Initiative</a></li>
<li><a href="Cognitive_architecture_comparison" title="wikilink">Cognitive architecture comparison</a></li>
<li><a href="Cognitive_science" title="wikilink">Cognitive science</a></li>
<li><a href="Commonsense_reasoning" title="wikilink">Commonsense reasoning</a></li>
<li><a href="Conceptual_Spaces" title="wikilink">Conceptual Spaces</a></li>
<li><a href="Deep_learning" title="wikilink">Deep learning</a></li>
<li><a href="Google_Brain" title="wikilink">Google Brain</a></li>
<li><a href="Image_schema" title="wikilink">Image schema</a></li>
<li><a class="uri" href="Neocognitron" title="wikilink">Neocognitron</a></li>
<li><a href="Neural_correlates_of_consciousness" title="wikilink">Neural correlates of consciousness</a></li>
<li><a href="Simulated_reality" title="wikilink">Simulated reality</a></li>
<li><a href="Social_simulation" title="wikilink">Social simulation</a></li>
<li><a href="Unified_theory_of_cognition" title="wikilink">Unified theory of cognition</a></li>
<li><a href="Never-Ending_Language_Learning" title="wikilink">Never-Ending Language Learning</a></li>
<li><a href="Bayesian_approaches_to_brain_function" title="wikilink">Bayesian Brain</a></li>
<li><a href="Open_Mind_Common_Sense" title="wikilink">Open Mind Common Sense</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>

<p><a href="de:Kognitionswissenschaft#Kognitive_Architekturen" title="wikilink">de:Kognitionswissenschaft#Kognitive Architekturen</a>"</p>

<p><a href="Category:Cognitive_architecture" title="wikilink"> </a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a class="uri" href="https://saltworks.stanford.edu/catalog/druid:st035tk1755">https://saltworks.stanford.edu/catalog/druid:st035tk1755</a><a href="#fnref1">↩</a></li>
<li id="fn2">"<a href="http://garfield.library.upenn.edu/classics1979/A1979HX09600001.pdf">This Week’s Citation Classic: Anderson J R &amp; Bower G H. Human associative memory. Washington</a>," in: CC. Nr. 52 Dec 24-31, 1979.<a href="#fnref2">↩</a></li>
<li id="fn3"><a href="John_Robert_Anderson_(psychologist)" title="wikilink">John R. Anderson</a>. <em><a href="https://books.google.nl/books?id=zL0eAgAAQBAJ">The Architecture of Cognition</a>,</em> 1983/2013.<a href="#fnref3">↩</a></li>
<li id="fn4">Samsonovich, Alexei V. "Toward a Unified Catalog of Implemented Cognitive Architectures." BICA 221 (2010): 195-244.<a href="#fnref4">↩</a></li>
<li id="fn5"><a class="uri" href="http://bicasociety.org/cogarch/">http://bicasociety.org/cogarch/</a><a href="#fnref5">↩</a></li>
<li id="fn6">J.S. Albus (1979). "Mechanisms of Planning and Problem Solving in the Brain". In: <em>Mathematical Biosciences</em>. Vol. 45, pp. 247293, 1979.<a href="#fnref6">↩</a></li>
<li id="fn7">Anwar, Ashraf, and Stan Franklin. "Sparse distributed memory for ‘conscious’ software agents." Cognitive Systems Research 4.4 (2003): 339-354.<a href="#fnref7">↩</a></li>
<li id="fn8">Mnih, Volodymyr, et al. "Playing atari with deep reinforcement learning." arXiv preprint arXiv:1312.5602 (2013).<a href="#fnref8">↩</a></li>
<li id="fn9">Graves, Alex, Greg Wayne, and Ivo Danihelka. "Neural Turing Machines." arXiv preprint arXiv:1410.5401 (2014).<a href="#fnref9">↩</a></li>
<li id="fn10">Mnih, Volodymyr, et al. "Human-level control through deep reinforcement learning." Nature 518.7540 (2015): 529-533.<a href="#fnref10">↩</a></li>
<li id="fn11"><a class="uri" href="http://people.idsia.ch/~juergen/naturedeepmind.html">http://people.idsia.ch/~juergen/naturedeepmind.html</a><a href="#fnref11">↩</a></li>
<li id="fn12">Schmidhuber, Jürgen. "Deep learning in neural networks: An overview." Neural Networks 61 (2015): 85-117.<a href="#fnref12">↩</a></li>
<li id="fn13"><a href="http://ieeexplore.ieee.org/iel5/9900/31471/01467219.pdf">An Intelligent Architecture for Integrated Control and Asset Management for Industrial Processes</a> Taylor, J.H. Sayda, A.F. in <em>Intelligent Control</em>, 2005. Proceedings of the 2005 IEEE International Symposium on, Mediterrean Conference on Control and Automation. pp 1397–1404<a href="#fnref13">↩</a></li>
<li id="fn14">Weston, Jason, Sumit Chopra, and Antoine Bordes. "Memory networks." arXiv preprint arXiv:1410.3916 (2014).<a href="#fnref14">↩</a></li>
<li id="fn15">Eliasmith, Chris, et al. "A large-scale model of the functioning brain." science 338.6111 (2012): 1202-1205.<a href="#fnref15">↩</a></li>
<li id="fn16">Denning, Peter J. "Sparse distributed memory." (1989).Url: <a class="uri" href="http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19920002425.pdf">http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19920002425.pdf</a><a href="#fnref16">↩</a></li>
<li id="fn17"><a href="#fnref17">↩</a></li>
<li id="fn18">Mendes, Mateus, Manuel Crisóstomo, and A. Paulo Coimbra. "Robot navigation using a sparse distributed memory." Robotics and automation, 2008. ICRA 2008. IEEE international conference on. IEEE, 2008.<a href="#fnref18">↩</a></li>
<li id="fn19">Jockel, Sascha, Felix Lindner, and Jianwei Zhang. "Sparse distributed memory for experience-based robot manipulation." Robotics and Biomimetics, 2008. ROBIO 2008. IEEE International Conference on. IEEE, 2009.<a href="#fnref19">↩</a></li>
<li id="fn20">Rinkus, Gerard J. "Sparsey™: event recognition via deep hierarchical sparse distributed codes." Frontiers in computational neuroscience 8 (2014).<a href="#fnref20">↩</a></li>
<li id="fn21">Snaider, Javier, and Stan Franklin. "Integer sparse distributed memory." Twenty-fifth international FLAIRS conference. 2012.<a href="#fnref21">↩</a></li>
<li id="fn22">Snaider, Javier, and Stan Franklin. "Vector LIDA." Procedia Computer Science 41 (2014): 188-203.<a href="#fnref22">↩</a></li>
<li id="fn23">Rolls, Edmund T. "Invariant visual object and face recognition: neural and computational bases, and a model, VisNet." Frontiers in computational neuroscience 6 (2012).<a href="#fnref23">↩</a></li>
</ol>
</section>
</body>
</html>
