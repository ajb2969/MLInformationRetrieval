<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1943">Regularization (mathematics)</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Regularization (mathematics)</h1>
<hr/>

<p><strong>Regularization</strong>, in <a class="uri" href="mathematics" title="wikilink">mathematics</a> and <a class="uri" href="statistics" title="wikilink">statistics</a> and particularly in the fields of <a href="machine_learning" title="wikilink">machine learning</a> and <a href="inverse_problem" title="wikilink">inverse problems</a>, refers to a process of introducing additional information in order to solve an <a href="ill-posed_problem" title="wikilink">ill-posed problem</a> or to prevent <a class="uri" href="overfitting" title="wikilink">overfitting</a>. This information is usually of the form of a penalty for complexity, such as restrictions for <a href="smooth_function" title="wikilink">smoothness</a> or bounds on the <a href="normed_vector_space" title="wikilink">vector space norm</a>.</p>

<p>A theoretical justification for regularization is that it attempts to impose <a href="Occam's_razor" title="wikilink">Occam's razor</a> on the solution. From a <a href="Bayesian_inference" title="wikilink">Bayesian</a> point of view, many regularization techniques correspond to imposing certain <a href="prior_probability" title="wikilink">prior</a> distributions on model parameters.</p>

<p>The same idea arose in many fields of <a class="uri" href="science" title="wikilink">science</a>. For example, the <a href="least-squares_method" title="wikilink">least-squares method</a> can be viewed as a very simple form of regularization. A simple form of regularization applied to <a href="integral_equation" title="wikilink">integral equations</a>, generally termed <a href="Tikhonov_regularization" title="wikilink">Tikhonov regularization</a> after <a href="Andrey_Nikolayevich_Tikhonov" title="wikilink">Andrey Nikolayevich Tikhonov</a>, is essentially a trade-off between fitting the data and reducing a norm of the solution. More recently, <a href="non-linear_regularization" title="wikilink">non-linear regularization</a> methods, including <a href="total_variation_regularization" title="wikilink">total variation regularization</a> have become popular.</p>
<h2 id="regularization-in-statistics-and-machine-learning">Regularization in statistics and machine learning</h2>

<p>In statistics and machine learning, regularization methods are used for model selection, in particular to prevent <a class="uri" href="overfitting" title="wikilink">overfitting</a> by penalizing models with extreme parameter values. The most common variants in machine learning are 

<math display="inline" id="Regularization_(mathematics):0">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mi mathvariant="normal">₁</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>L</ci>
    <ci>normal-₁</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L₁
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Regularization_(mathematics):1">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mi mathvariant="normal">₂</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>L</ci>
    <ci>normal-₂</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L₂
  </annotation>
 </semantics>
</math>

 regularization, which can be added to learning algorithms that minimize a <a href="loss_function" title="wikilink">loss function</a> 

<math display="inline" id="Regularization_(mathematics):2">
 <semantics>
  <mrow>
   <mi>E</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>X</mi>
    <mo>,</mo>
    <mi>Y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>E</ci>
    <interval closure="open">
     <ci>X</ci>
     <ci>Y</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   E(X,Y)
  </annotation>
 </semantics>
</math>

 by instead minimizing 

<math display="inline" id="Regularization_(mathematics):3">
 <semantics>
  <mrow>
   <mrow>
    <mi>E</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>X</mi>
     <mo>,</mo>
     <mi>Y</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mrow>
    <mi>α</mi>
    <mi mathvariant="normal">‖</mi>
    <mi>w</mi>
    <mi mathvariant="normal">‖</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <ci>E</ci>
     <interval closure="open">
      <ci>X</ci>
      <ci>Y</ci>
     </interval>
    </apply>
    <apply>
     <times></times>
     <ci>α</ci>
     <ci>normal-‖</ci>
     <ci>w</ci>
     <ci>normal-‖</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   E(X,Y)+α‖w‖
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Regularization_(mathematics):4">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

 is the model's weight vector, ‖·‖ is either the 

<math display="inline" id="Regularization_(mathematics):5">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mi mathvariant="normal">₁</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>L</ci>
    <ci>normal-₁</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L₁
  </annotation>
 </semantics>
</math>

 norm or the squared 

<math display="inline" id="Regularization_(mathematics):6">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mi mathvariant="normal">₂</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>L</ci>
    <ci>normal-₂</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L₂
  </annotation>
 </semantics>
</math>

 norm, and α is a free parameter that needs to be tuned empirically (typically by <a href="Cross-validation_(statistics)" title="wikilink">cross-validation</a>; see <a href="hyperparameter_optimization" title="wikilink">hyperparameter optimization</a>). This method applies to many models. When applied in <a href="linear_regression" title="wikilink">linear regression</a>, the resulting models are termed <a href="Least_squares#Lasso_method" title="wikilink">lasso</a> or <a href="ridge_regression" title="wikilink">ridge regression</a>, but regularization is also employed in (binary and <a href="multinomial_logistic_regression" title="wikilink">multiclass</a>) <a href="logistic_regression" title="wikilink">logistic regression</a>, <a href="artificial_neural_network" title="wikilink">neural nets</a>, <a href="support_vector_machine" title="wikilink">support vector machines</a>, <a href="conditional_random_field" title="wikilink">conditional random fields</a> and some <a href="matrix_decomposition" title="wikilink">matrix decomposition</a> methods. 

<math display="inline" id="Regularization_(mathematics):7">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mi mathvariant="normal">₂</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>L</ci>
    <ci>normal-₂</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L₂
  </annotation>
 </semantics>
</math>

 regularization may also be called "weight decay", in particular in the setting of neural nets.</p>

<p>

<math display="inline" id="Regularization_(mathematics):8">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mi mathvariant="normal">₁</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>L</ci>
    <ci>normal-₁</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L₁
  </annotation>
 </semantics>
</math>

 regularization is often preferred because it produces sparse models and thus performs <a href="feature_selection" title="wikilink">feature selection</a> within the learning algorithm, but since the 

<math display="inline" id="Regularization_(mathematics):9">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mi mathvariant="normal">₁</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>L</ci>
    <ci>normal-₁</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L₁
  </annotation>
 </semantics>
</math>

 norm is not differentiable, it may require changes to learning algorithms, in particular gradient-based learners.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p><a href="Bayesian_model_comparison" title="wikilink">Bayesian learning</a> methods make use of a <a href="prior_probability" title="wikilink">prior probability</a> that (usually) gives lower probability to more complex models. Well-known model selection techniques include the <a href="Akaike_information_criterion" title="wikilink">Akaike information criterion</a> (AIC), <a href="minimum_description_length" title="wikilink">minimum description length</a> (MDL), and the <a href="Bayesian_information_criterion" title="wikilink">Bayesian information criterion</a> (BIC). Alternative methods of controlling overfitting not involving regularization include <a href="cross-validation_(statistics)" title="wikilink">cross-validation</a>.</p>

<p>Regularization can be used to fine-tune model complexity using an augmented error function with cross-validation. The data sets used in complex models can produce a levelling-off of validation as complexity of the models increases. Training data sets errors decrease while the validation data set error remains constant. Regularization introduces a second factor which weights the penalty against more complex models with an increasing variance in the data errors. This gives an increasing penalty as model complexity increases.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>Examples of applications of different methods of regularization to the <a href="linear_model" title="wikilink">linear model</a> are:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">
<p>Model</p></th>
<th style="text-align: left;">
<p>Fit measure</p></th>
<th style="text-align: left;">
<p>Entropy measure<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p><a href="Akaike_information_criterion" title="wikilink">AIC</a>/<a href="Bayesian_information_criterion" title="wikilink">BIC</a></p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Regularization_(mathematics):10">
 <semantics>
  <msub>
   <mrow>
    <mo>∥</mo>
    <mrow>
     <mi>Y</mi>
     <mo>-</mo>
     <mrow>
      <mi>X</mi>
      <mi>β</mi>
     </mrow>
    </mrow>
    <mo>∥</mo>
   </mrow>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <apply>
      <minus></minus>
      <ci>Y</ci>
      <apply>
       <times></times>
       <ci>X</ci>
       <ci>β</ci>
      </apply>
     </apply>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|Y-X\beta\|_{2}
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Regularization_(mathematics):11">
 <semantics>
  <msub>
   <mrow>
    <mo>∥</mo>
    <mi>β</mi>
    <mo>∥</mo>
   </mrow>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <ci>β</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|\beta\|_{0}
  </annotation>
 </semantics>
</math>

</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p><a href="Ridge_regression" title="wikilink">Ridge regression</a></p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Regularization_(mathematics):12">
 <semantics>
  <msub>
   <mrow>
    <mo>∥</mo>
    <mrow>
     <mi>Y</mi>
     <mo>-</mo>
     <mrow>
      <mi>X</mi>
      <mi>β</mi>
     </mrow>
    </mrow>
    <mo>∥</mo>
   </mrow>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <apply>
      <minus></minus>
      <ci>Y</ci>
      <apply>
       <times></times>
       <ci>X</ci>
       <ci>β</ci>
      </apply>
     </apply>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|Y-X\beta\|_{2}
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Regularization_(mathematics):13">
 <semantics>
  <msub>
   <mrow>
    <mo>∥</mo>
    <mi>β</mi>
    <mo>∥</mo>
   </mrow>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <ci>β</ci>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|\beta\|_{2}
  </annotation>
 </semantics>
</math>

</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p><a href="Least_squares#Lasso_method" title="wikilink">Lasso</a><ref>{{Cite journal</ref></p></td>
<td style="text-align: left;">
<p>last = Tibshirani</p></td>
<td style="text-align: left;">
<p>first = Robert</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p><a href="Basis_pursuit_denoising" title="wikilink">Basis pursuit denoising</a></p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Regularization_(mathematics):14">
 <semantics>
  <msub>
   <mrow>
    <mo>∥</mo>
    <mrow>
     <mi>Y</mi>
     <mo>-</mo>
     <mrow>
      <mi>X</mi>
      <mi>β</mi>
     </mrow>
    </mrow>
    <mo>∥</mo>
   </mrow>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <apply>
      <minus></minus>
      <ci>Y</ci>
      <apply>
       <times></times>
       <ci>X</ci>
       <ci>β</ci>
      </apply>
     </apply>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|Y-X\beta\|_{2}
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Regularization_(mathematics):15">
 <semantics>
  <mrow>
   <mi>λ</mi>
   <msub>
    <mrow>
     <mo>∥</mo>
     <mi>β</mi>
     <mo>∥</mo>
    </mrow>
    <mn>1</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>λ</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <ci>β</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda\|\beta\|_{1}
  </annotation>
 </semantics>
</math>

</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>Rudin-Osher-Fatemi model (TV)</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Regularization_(mathematics):16">
 <semantics>
  <msub>
   <mrow>
    <mo>∥</mo>
    <mrow>
     <mi>Y</mi>
     <mo>-</mo>
     <mrow>
      <mi>X</mi>
      <mi>β</mi>
     </mrow>
    </mrow>
    <mo>∥</mo>
   </mrow>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <apply>
      <minus></minus>
      <ci>Y</ci>
      <apply>
       <times></times>
       <ci>X</ci>
       <ci>β</ci>
      </apply>
     </apply>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|Y-X\beta\|_{2}
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Regularization_(mathematics):17">
 <semantics>
  <mrow>
   <mi>λ</mi>
   <msub>
    <mrow>
     <mo>∥</mo>
     <mrow>
      <mo>∇</mo>
      <mi>β</mi>
     </mrow>
     <mo>∥</mo>
    </mrow>
    <mn>1</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>λ</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <apply>
       <ci>normal-∇</ci>
       <ci>β</ci>
      </apply>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda\|\nabla\beta\|_{1}
  </annotation>
 </semantics>
</math>

</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p><a href="Potts_model" title="wikilink">Potts model</a></p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Regularization_(mathematics):18">
 <semantics>
  <msub>
   <mrow>
    <mo>∥</mo>
    <mrow>
     <mi>Y</mi>
     <mo>-</mo>
     <mrow>
      <mi>X</mi>
      <mi>β</mi>
     </mrow>
    </mrow>
    <mo>∥</mo>
   </mrow>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <apply>
      <minus></minus>
      <ci>Y</ci>
      <apply>
       <times></times>
       <ci>X</ci>
       <ci>β</ci>
      </apply>
     </apply>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|Y-X\beta\|_{2}
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Regularization_(mathematics):19">
 <semantics>
  <mrow>
   <mi>λ</mi>
   <msub>
    <mrow>
     <mo>∥</mo>
     <mrow>
      <mo>∇</mo>
      <mi>β</mi>
     </mrow>
     <mo>∥</mo>
    </mrow>
    <mn>0</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>λ</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <apply>
       <ci>normal-∇</ci>
       <ci>β</ci>
      </apply>
     </apply>
     <cn type="integer">0</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda\|\nabla\beta\|_{0}
  </annotation>
 </semantics>
</math>

</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>RLAD<ref>{{Cite conference</ref></p></td>
<td style="text-align: left;">
<p>author = Li Wang, Michael D. Gordon &amp; Ji Zhu</p></td>
<td style="text-align: left;">
<p>title = Regularized Least Absolute Deviations Regression and an Efficient Algorithm for Parameter Tuning</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>Dantzig Selector<ref>{{Cite journal</ref></p></td>
<td style="text-align: left;">
<p>last = Candes</p></td>
<td style="text-align: left;">
<p>first = Emmanuel | authorlink = Emmanuel Candès</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>SLOPE<ref>{{Cite journal</ref></p></td>
<td style="text-align: left;">
<p>author = Małgorzata Bogdan, Ewout van den Berg, Weijie Su &amp; Emmanuel J. Candes</p></td>
<td style="text-align: left;">
<p>title = Statistical estimation and testing via the ordered L1 norm</p></td>
</tr>
</tbody>
</table>

<p>A linear combination of the LASSO and ridge regression methods is <a href="elastic_net_regularization" title="wikilink">elastic net regularization</a>.</p>
<h2 id="ensemble-based-regularization">Ensemble-based regularization</h2>

<p>In <a href="inverse_problem" title="wikilink">inverse problem</a> theory, an optimization problem is usually solved to generate a model that provides a good match to observed data. In this context, a regularization term is used to preserve prior information about the model and prevent over-fitting and convergence to a model that matches the data but does not predict well. Ensemble-based regularization is based on utilizing an ensemble (i.e., a set) of realizations from the prior probability distribution function (pdf) to construct a regularization term .<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> This regularization is flexible as it is based on representing the prior pdf using a set of realizations, instead of using, say a mean and covariance matrix for Gaussian distribution.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Bayesian_interpretation_of_regularization" title="wikilink">Bayesian interpretation of regularization</a></li>
<li><a href="Regularization_by_spectral_filtering" title="wikilink">Regularization by spectral filtering</a></li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<ul>
<li>A. Neumaier, Solving ill-conditioned and singular linear systems: A tutorial on regularization, SIAM Review 40 (1998), 636-666. Available in <a href="http://www.mat.univie.ac.at/~neum/ms/regtutorial.pdf">pdf</a> from <a href="http://www.mat.univie.ac.at/~neum/">author's website</a>.</li>
</ul>

<p><a class="uri" href="de:Regularisierung" title="wikilink">de:Regularisierung</a>"</p>

<p><a href="Category:Mathematical_analysis" title="wikilink">Category:Mathematical analysis</a> <a href="Category:Inverse_problems" title="wikilink">Category:Inverse problems</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
</ol>
</section>
</body>
</html>
