<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1719">Association rule learning</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Association rule learning</h1>
<hr/>

<p><strong>Association rule learning</strong> is a popular and well researched method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using different measures of interestingness.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Based on the concept of strong rules, Rakesh Agrawal et al.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> introduced association rules for discovering regularities between products in large-scale transaction data recorded by <a class="uri" href="point-of-sale" title="wikilink">point-of-sale</a> (POS) systems in supermarkets. For example, the rule 

<math display="inline" id="Association_rule_learning:0">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>onions</mi>
    <mo>,</mo>
    <mi>potatoes</mi>
    <mo stretchy="false">}</mo>
   </mrow>
   <mo>⇒</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>burger</mi>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-⇒</ci>
    <set>
     <ci>onions</ci>
     <ci>potatoes</ci>
    </set>
    <set>
     <ci>burger</ci>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{\mathrm{onions,potatoes}\}\Rightarrow\{\mathrm{burger}\}
  </annotation>
 </semantics>
</math>

 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional <a class="uri" href="pricing" title="wikilink">pricing</a> or <a href="product_placement" title="wikilink">product placements</a>. In addition to the above example from <a href="market_basket_analysis" title="wikilink">market basket analysis</a> association rules are employed today in many application areas including <a href="Web_usage_mining" title="wikilink">Web usage mining</a>, <a href="intrusion_detection" title="wikilink">intrusion detection</a>, <a href="Continuous_production" title="wikilink">Continuous production</a>, and <a class="uri" href="bioinformatics" title="wikilink">bioinformatics</a>. In contrast with <a href="sequence_mining" title="wikilink">sequence mining</a>, association rule learning typically does not consider the order of items either within a transaction or across transactions.</p>
<h2 id="definition">Definition</h2>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p>Example database with 5 transactions and 5 items</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>transaction ID</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>2</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>3</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>4</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>5</p></td>
</tr>
<tr class="even">
</tr>
</tbody>
</table>

<p>Following the original definition by Agrawal et al.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> the problem of association rule mining is defined as:</p>

<p>Let 

<math display="inline" id="Association_rule_learning:1">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <msub>
     <mi>i</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>i</mi>
     <mn>2</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>i</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>I</ci>
    <set>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>i</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>i</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>normal-…</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>i</ci>
      <ci>n</ci>
     </apply>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I=\{i_{1},i_{2},\ldots,i_{n}\}
  </annotation>
 </semantics>
</math>

 be a set of 

<math display="inline" id="Association_rule_learning:2">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 binary attributes called <em>items</em>.</p>

<p>Let 

<math display="inline" id="Association_rule_learning:3">
 <semantics>
  <mrow>
   <mi>D</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <msub>
     <mi>t</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>t</mi>
     <mn>2</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>t</mi>
     <mi>m</mi>
    </msub>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>D</ci>
    <set>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>normal-…</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <ci>m</ci>
     </apply>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D=\{t_{1},t_{2},\ldots,t_{m}\}
  </annotation>
 </semantics>
</math>

 be a set of transactions called the <em>database</em>.</p>

<p>Each <em>transaction</em> in 

<math display="inline" id="Association_rule_learning:4">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D
  </annotation>
 </semantics>
</math>


 has a unique transaction ID and contains a subset of the items in 

<math display="inline" id="Association_rule_learning:5">
 <semantics>
  <mi>I</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>I</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I
  </annotation>
 </semantics>
</math>

.</p>

<p>A <em>rule</em> is defined as an implication of the form:</p>

<p>

<math display="inline" id="Association_rule_learning:6">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>⇒</mo>
   <mi>Y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-⇒</ci>
    <ci>X</ci>
    <ci>Y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X\Rightarrow Y
  </annotation>
 </semantics>
</math>

</p>

<p>Where 

<math display="inline" id="Association_rule_learning:7">
 <semantics>
  <mrow>
   <mrow>
    <mi>X</mi>
    <mo>,</mo>
    <mi>Y</mi>
   </mrow>
   <mo>⊆</mo>
   <mi>I</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <subset></subset>
    <list>
     <ci>X</ci>
     <ci>Y</ci>
    </list>
    <ci>I</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X,Y\subseteq I
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Association_rule_learning:8">
 <semantics>
  <mrow>
   <mrow>
    <mi>X</mi>
    <mo>∩</mo>
    <mi>Y</mi>
   </mrow>
   <mo>=</mo>
   <mi mathvariant="normal">∅</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <intersect></intersect>
     <ci>X</ci>
     <ci>Y</ci>
    </apply>
    <emptyset></emptyset>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X\cap Y=\emptyset
  </annotation>
 </semantics>
</math>

.</p>

<p>Every rule is composed by two different set of items, also known as <em>itemsets</em>, 

<math display="inline" id="Association_rule_learning:9">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>


 an 

<math display="inline" id="Association_rule_learning:10">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Association_rule_learning:11">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 is called <em>antecedent</em> or left-hand-side (LHS) and 

<math display="inline" id="Association_rule_learning:12">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

 <em>consequent</em> or right-hand-side (RHS).</p>

<p>To illustrate the concepts, we use a small example from the supermarket domain. The set of items is 

<math display="inline" id="Association_rule_learning:13">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>milk</mi>
    <mo>,</mo>
    <mi>bread</mi>
    <mo>,</mo>
    <mi>butter</mi>
    <mo>,</mo>
    <mi>beer</mi>
    <mo>,</mo>
    <mi>diapers</mi>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>I</ci>
    <set>
     <ci>milk</ci>
     <ci>bread</ci>
     <ci>butter</ci>
     <ci>beer</ci>
     <ci>diapers</ci>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I=\{\mathrm{milk,bread,butter,beer,diapers}\}
  </annotation>
 </semantics>
</math>

 and in the table is shown a small database containing the items, where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represent the absence of an item in a that transaction.</p>

<p>An example rule for the supermarket could be 

<math display="inline" id="Association_rule_learning:14">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>butter</mi>
    <mo>,</mo>
    <mi>bread</mi>
    <mo stretchy="false">}</mo>
   </mrow>
   <mo>⇒</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>milk</mi>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-⇒</ci>
    <set>
     <ci>butter</ci>
     <ci>bread</ci>
    </set>
    <set>
     <ci>milk</ci>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{\mathrm{butter,bread}\}\Rightarrow\{\mathrm{milk}\}
  </annotation>
 </semantics>
</math>


 meaning that if butter and bread are bought, customers also buy milk.</p>

<p>Note: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and data-sets often contain thousands or millions of transactions.</p>
<h2 id="useful-concepts">Useful Concepts</h2>

<p>In order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.</p>

<p>Let 

<math display="inline" id="Association_rule_learning:15">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 an item-set, 

<math display="inline" id="Association_rule_learning:16">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>⇒</mo>
   <mi>Y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-⇒</ci>
    <ci>X</ci>
    <ci>Y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X\Rightarrow Y
  </annotation>
 </semantics>
</math>

 an association rule and 

<math display="inline" id="Association_rule_learning:17">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>

 a set of transactions of a given database.</p>
<h3 id="support">Support</h3>

<p>The support value of 

<math display="inline" id="Association_rule_learning:18">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 with respect to 

<math display="inline" id="Association_rule_learning:19">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>


 is defined as the proportion of transactions in the database which contains the item-set 

<math display="inline" id="Association_rule_learning:20">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

. In formula

<math display="block" id="Association_rule_learning:21">
 <semantics>
  <mrow>
   <mi>supp</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>X</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>supp</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{supp}(X)
  </annotation>
 </semantics>
</math>

 In the example database, the item-set 

<math display="inline" id="Association_rule_learning:22">
 <semantics>
  <mrow>
   <mo stretchy="false">{</mo>
   <mi>milk</mi>
   <mo>,</mo>
   <mi>bread</mi>
   <mo>,</mo>
   <mi>butter</mi>
   <mo stretchy="false">}</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <set>
    <ci>milk</ci>
    <ci>bread</ci>
    <ci>butter</ci>
   </set>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{\mathrm{milk,bread,butter}\}
  </annotation>
 </semantics>
</math>

 has a support of 

<math display="inline" id="Association_rule_learning:23">
 <semantics>
  <mrow>
   <mrow>
    <mn>1</mn>
    <mo>/</mo>
    <mn>5</mn>
   </mrow>
   <mo>=</mo>
   <mn>0.2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <cn type="integer">5</cn>
    </apply>
    <cn type="float">0.2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1/5=0.2
  </annotation>
 </semantics>
</math>

 since it occurs in 20% of all transactions (1 out of 5 transactions). The argument of 

<math display="inline" id="Association_rule_learning:24">
 <semantics>
  <mrow>
   <mi>supp</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>supp</ci>
    <list></list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{supp}()
  </annotation>
 </semantics>
</math>


 is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).</p>
<h3 id="confidence">Confidence</h3>

<p>The <em>confidence</em> value of a rule, 

<math display="inline" id="Association_rule_learning:25">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>⇒</mo>
   <mi>Y</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-⇒</ci>
    <ci>X</ci>
    <ci>Y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X\Rightarrow Y
  </annotation>
 </semantics>
</math>

 , with respect to a set of transactions 

<math display="inline" id="Association_rule_learning:26">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>

, is the proportion the transactions that contains 

<math display="inline" id="Association_rule_learning:27">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 which also contains 

<math display="inline" id="Association_rule_learning:28">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

.</p>

<p>Let define it as:</p>

<p>

<math display="inline" id="Association_rule_learning:29">
 <semantics>
  <mrow>
   <mi>conf</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>X</mi>
    <mo>⇒</mo>
    <mi>Y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>supp</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>X</mi>
    <mo>∪</mo>
    <mi>Y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>/</mo>
   <mi>supp</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>X</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">conf</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-⇒</ci>
     <csymbol cd="unknown">Y</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">supp</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">X</csymbol>
     <union></union>
     <csymbol cd="unknown">Y</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <divide></divide>
    <csymbol cd="unknown">supp</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{conf}(X\Rightarrow Y)=\mathrm{supp}(X\cup Y)/\mathrm{supp}(X)
  </annotation>
 </semantics>
</math>


.</p>

<p>For example, the rule 

<math display="inline" id="Association_rule_learning:30">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>butter</mi>
    <mo>,</mo>
    <mi>bread</mi>
    <mo stretchy="false">}</mo>
   </mrow>
   <mo>⇒</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>milk</mi>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-⇒</ci>
    <set>
     <ci>butter</ci>
     <ci>bread</ci>
    </set>
    <set>
     <ci>milk</ci>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{\mathrm{butter,bread}\}\Rightarrow\{\mathrm{milk}\}
  </annotation>
 </semantics>
</math>

 has a confidence of 

<math display="inline" id="Association_rule_learning:31">
 <semantics>
  <mrow>
   <mrow>
    <mn>0.2</mn>
    <mo>/</mo>
    <mn>0.2</mn>
   </mrow>
   <mo>=</mo>
   <mn>1.0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <divide></divide>
     <cn type="float">0.2</cn>
     <cn type="float">0.2</cn>
    </apply>
    <cn type="float">1.0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   0.2/0.2=1.0
  </annotation>
 </semantics>
</math>

 in the database, which means that for 100% of the transactions containing butter and bread the rule is correct (100% of the times a customer buys butter and bread, milk is bought as well).</p>

<p>Note that 

<math display="inline" id="Association_rule_learning:32">
 <semantics>
  <mrow>
   <mi>s</mi>
   <mi>u</mi>
   <mi>p</mi>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>X</mi>
     <mo>∪</mo>
     <mi>Y</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>s</ci>
    <ci>u</ci>
    <ci>p</ci>
    <ci>p</ci>
    <apply>
     <union></union>
     <ci>X</ci>
     <ci>Y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   supp(X\cup Y)
  </annotation>
 </semantics>
</math>

 means the support of the union of the items in X and Y. This is somewhat confusing since we normally think in terms of probabilities of <a href="Event_(probability_theory)" title="wikilink"> events</a> and not sets of items. We can rewrite 

<math display="inline" id="Association_rule_learning:33">
 <semantics>
  <mrow>
   <mi>s</mi>
   <mi>u</mi>
   <mi>p</mi>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>X</mi>
     <mo>∪</mo>
     <mi>Y</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>s</ci>
    <ci>u</ci>
    <ci>p</ci>
    <ci>p</ci>
    <apply>
     <union></union>
     <ci>X</ci>
     <ci>Y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   supp(X\cup Y)
  </annotation>
 </semantics>
</math>

 as the joint probability 

<math display="inline" id="Association_rule_learning:34">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <msub>
      <mi>E</mi>
      <mi>X</mi>
     </msub>
     <mo>∩</mo>
     <msub>
      <mi>E</mi>
      <mi>Y</mi>
     </msub>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <apply>
     <intersect></intersect>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>E</ci>
      <ci>X</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>E</ci>
      <ci>Y</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(E_{X}\cap E_{Y})
  </annotation>
 </semantics>
</math>


, where 

<math display="inline" id="Association_rule_learning:35">
 <semantics>
  <msub>
   <mi>E</mi>
   <mi>X</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>E</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   E_{X}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Association_rule_learning:36">
 <semantics>
  <msub>
   <mi>E</mi>
   <mi>Y</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>E</ci>
    <ci>Y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   E_{Y}
  </annotation>
 </semantics>
</math>

 are the events that a transaction contains itemset 

<math display="inline" id="Association_rule_learning:37">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 or 

<math display="inline" id="Association_rule_learning:38">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

, respectively.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> Thus confidence can be interpreted as an estimate of the conditional probability 

<math display="inline" id="Association_rule_learning:39">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>E</mi>
     <mi>Y</mi>
    </msub>
    <mo stretchy="false">|</mo>
    <msub>
     <mi>E</mi>
     <mi>X</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>E</ci>
      <ci>Y</ci>
     </apply>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>E</ci>
      <ci>X</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(E_{Y}|E_{X})
  </annotation>
 </semantics>
</math>


, the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h3 id="lift">Lift</h3>

<p>The <em><a href="lift_(data_mining)" title="wikilink">lift</a></em> of a rule is defined as:</p>

<p>

<math display="inline" id="Association_rule_learning:40">
 <semantics>
  <mrow>
   <mi>lift</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>X</mi>
    <mo>⇒</mo>
    <mi>Y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>supp</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>X</mi>
       <mo>∪</mo>
       <mi>Y</mi>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mrow>
      <mrow>
       <mi>supp</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>×</mo>
      <mi>supp</mi>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>Y</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">lift</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-⇒</ci>
     <csymbol cd="unknown">Y</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <ci>supp</ci>
      <apply>
       <union></union>
       <ci>X</ci>
       <ci>Y</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <times></times>
       <apply>
        <times></times>
        <ci>supp</ci>
        <ci>X</ci>
       </apply>
       <ci>supp</ci>
      </apply>
      <ci>Y</ci>
     </apply>
    </apply>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{lift}(X\Rightarrow Y)=\frac{\mathrm{supp}(X\cup Y)}{\mathrm{supp}(X)%
\times\mathrm{supp}(Y)}
  </annotation>
 </semantics>
</math>

 or the ratio of the observed support to that expected if X and Y were <a href="Independence_(probability_theory)" title="wikilink">independent</a>.</p>

<p>For Example, the rule 

<math display="inline" id="Association_rule_learning:41">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>milk</mi>
    <mo>,</mo>
    <mi>bread</mi>
    <mo stretchy="false">}</mo>
   </mrow>
   <mo>⇒</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>butter</mi>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-⇒</ci>
    <set>
     <ci>milk</ci>
     <ci>bread</ci>
    </set>
    <set>
     <ci>butter</ci>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{\mathrm{milk,bread}\}\Rightarrow\{\mathrm{butter}\}
  </annotation>
 </semantics>
</math>

 has a lift of 

<math display="inline" id="Association_rule_learning:42">
 <semantics>
  <mrow>
   <mfrac>
    <mn>0.2</mn>
    <mrow>
     <mn>0.4</mn>
     <mo>×</mo>
     <mn>0.4</mn>
    </mrow>
   </mfrac>
   <mo>=</mo>
   <mn>1.25</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <divide></divide>
     <cn type="float">0.2</cn>
     <apply>
      <times></times>
      <cn type="float">0.4</cn>
      <cn type="float">0.4</cn>
     </apply>
    </apply>
    <cn type="float">1.25</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{0.2}{0.4\times 0.4}=1.25
  </annotation>
 </semantics>
</math>

.</p>
<h3 id="conviction">Conviction</h3>

<p>The <em>conviction</em> of a rule is defined as 

<math display="inline" id="Association_rule_learning:43">
 <semantics>
  <mrow>
   <mi>conv</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>X</mi>
    <mo>⇒</mo>
    <mi>Y</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mrow>
      <mi>supp</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>Y</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mi>conf</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>X</mi>
      <mo>⇒</mo>
      <mi>Y</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">conv</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">X</csymbol>
     <ci>normal-⇒</ci>
     <csymbol cd="unknown">Y</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
      <apply>
       <times></times>
       <ci>supp</ci>
       <ci>Y</ci>
      </apply>
     </apply>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <cn type="integer">1</cn>
      <minus></minus>
      <csymbol cd="unknown">conf</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">X</csymbol>
       <ci>normal-⇒</ci>
       <csymbol cd="unknown">Y</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
    </apply>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{conv}(X\Rightarrow Y)=\frac{1-\mathrm{supp}(Y)}{1-\mathrm{conf}(X%
\Rightarrow Y)}
  </annotation>
 </semantics>
</math>

.</p>

<p>For Example, the rule 

<math display="inline" id="Association_rule_learning:44">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>milk</mi>
    <mo>,</mo>
    <mi>bread</mi>
    <mo stretchy="false">}</mo>
   </mrow>
   <mo>⇒</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>butter</mi>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-⇒</ci>
    <set>
     <ci>milk</ci>
     <ci>bread</ci>
    </set>
    <set>
     <ci>butter</ci>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{\mathrm{milk,bread}\}\Rightarrow\{\mathrm{butter}\}
  </annotation>
 </semantics>
</math>


 has a conviction of 

<math display="inline" id="Association_rule_learning:45">
 <semantics>
  <mrow>
   <mfrac>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mn>0.4</mn>
    </mrow>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mn>.5</mn>
    </mrow>
   </mfrac>
   <mo>=</mo>
   <mn>1.2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <divide></divide>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
      <cn type="float">0.4</cn>
     </apply>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
      <cn type="float">.5</cn>
     </apply>
    </apply>
    <cn type="float">1.2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{1-0.4}{1-.5}=1.2
  </annotation>
 </semantics>
</math>

, and can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction value of 1.2 shows that the rule 

<math display="inline" id="Association_rule_learning:46">
 <semantics>
  <mrow>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>milk</mi>
    <mo>,</mo>
    <mi>bread</mi>
    <mo stretchy="false">}</mo>
   </mrow>
   <mo>⇒</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mi>butter</mi>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-⇒</ci>
    <set>
     <ci>milk</ci>
     <ci>bread</ci>
    </set>
    <set>
     <ci>butter</ci>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \{\mathrm{milk,bread}\}\Rightarrow\{\mathrm{butter}\}
  </annotation>
 </semantics>
</math>

 would be incorrect 20% more often (1.2 times as often) if the association between X and Y was purely random chance.</p>
<h2 id="process">Process</h2>

<p> Association rules are usually required to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Association rule generation is usually split up into two separate steps:</p>
<ol>
<li>A minimum support threshold is applied to find all <em>frequent item-sets</em> in a database.</li>
<li>A minimum confidence constraint is applied to these frequent item-sets in order to form rules.</li>
</ol>

<p>While the second step is straightforward, the first step needs more attention.</p>

<p>Finding all frequent item-sets in a database is difficult since it involves searching all possible item-sets (item combinations). The set of possible item-sets is the <a href="power_set" title="wikilink">power set</a> over 

<math display="inline" id="Association_rule_learning:47">
 <semantics>
  <mi>I</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>I</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I
  </annotation>
 </semantics>
</math>

 and has size 

<math display="inline" id="Association_rule_learning:48">
 <semantics>
  <mrow>
   <msup>
    <mn>2</mn>
    <mi>n</mi>
   </msup>
   <mo>-</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cn type="integer">2</cn>
     <ci>n</ci>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   2^{n}-1
  </annotation>
 </semantics>
</math>

 (excluding the empty set which is not a valid item-set). Although the size of the power-set grows exponentially in the number of items 

<math display="inline" id="Association_rule_learning:49">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>


 in 

<math display="inline" id="Association_rule_learning:50">
 <semantics>
  <mi>I</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>I</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I
  </annotation>
 </semantics>
</math>

, efficient search is possible using the <strong><em>downward-closure property</em></strong> of support<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> (also called <em>anti-monotonicity</em><a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a>) which guarantees that for a frequent itemset, all its subsets are also frequent and thus for an infrequent item-set, all its super-sets must also be infrequent. Exploiting this property, efficient algorithms (e.g., Apriori<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> and Eclat<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a>) can find all frequent item-sets.</p>
<h2 id="history">History</h2>

<p>The concept of association rules was popularised particularly due to the 1993 article of Agrawal et al.,<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> which has acquired more than 6000 citations according to Google Scholar, as of March 2008, and is thus one of the most cited papers in the Data Mining field. However, it is possible that what is now called "association rules" is similar to what appears in the 1966 paper<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> on GUHA, a general data mining method developed by <a href="Petr_Hájek" title="wikilink">Petr Hájek</a> et al.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></p>
<h2 id="alternative-measures-of-interestingness">Alternative measures of interestingness</h2>

<p>In addition to confidence, other measures of <em>interestingness</em> for rules have been proposed. Some popular measures are:</p>
<ul>
<li>All-confidence<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></li>
</ul>
<ul>
<li>Collective strength<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></li>
</ul>
<ul>
<li>Conviction<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a></li>
</ul>
<ul>
<li>Leverage<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></li>
</ul>
<ul>
<li>Lift (originally called interest)<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a></li>
</ul>

<p>A definition of these measures can be found <a href="http://michael.hahsler.net/research/association_rules/measures.html">here</a>. Several more measures are presented and compared by Tan et al.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a> Looking for techniques that can model what the user has known (and using these models as interestingness measures) is currently an active research trend under the name of "Subjective Interestingness."</p>
<h2 id="statistically-sound-associations">Statistically sound associations</h2>

<p>One limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations. These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance. For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side. There are approximately 1,000,000,000,000 such rules. If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association. If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules. Statistically sound association discovery<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a><a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> controls this risk, in most cases reducing the risk of finding <em>any</em> spurious associations to a user-specified significance level.</p>
<h2 id="algorithms">Algorithms</h2>

<p>Many algorithms for generating association rules were presented over time.</p>

<p>Some well known algorithms are <a href="Apriori_algorithm" title="wikilink">Apriori</a>, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.</p>
<h3 id="apriori-algorithm">Apriori algorithm</h3>

<p>Apriori<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a> is the best-known algorithm to mine association rules. It uses a breadth-first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support.</p>
<h3 id="eclat-algorithm">Eclat algorithm</h3>

<p>Eclat<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a> (alt. ECLAT, stands for Equivalence Class Transformation) is a depth-first search algorithm using set intersection. It is a naturally elegant algorithm suitable for both sequential as well as parallel execution with locality enhancing properties. It was first introduced by Zaki, Parthasarathy, Li and Ogihara in a series of papers written in 1997.</p>

<p>Mohammed Javeed Zaki, Srinivasan Parthasarathy, Wei Li: A Localized Algorithm for Parallel Association Mining. SPAA 1997: 321-330</p>

<p>Mohammed Javeed Zaki, Srinivasan Parthasarathy, Mitsunori Ogihara, Wei Li: Parallel Algorithms for Discovery of Association Rules. Data Min. Knowl. Discov. 1(4): 343-373 (1997)</p>
<h3 id="fp-growth-algorithm">FP-growth algorithm</h3>

<p>FP stands for frequent pattern.</p>

<p>In the first pass, the algorithm counts occurrence of items (attribute-value pairs) in the dataset, and stores them to 'header table'. In the second pass, it builds the FP-tree structure by inserting instances. Items in each instance have to be sorted by descending order of their frequency in the dataset, so that the tree can be processed quickly. Items in each instance that do not meet minimum coverage threshold are discarded. If many instances share most frequent items, FP-tree provides high compression close to tree root.</p>

<p>Recursive processing of this compressed version of main dataset grows large item sets directly, instead of generating candidate items and testing them against the entire database. Growth starts from the bottom of the header table (having longest branches), by finding all instances matching given condition. New tree is created, with counts projected from the original tree corresponding to the set of instances that are conditional on the attribute, with each node getting sum of its children counts. Recursive growth ends when no individual items conditional on the attribute meet minimum support threshold, and processing continues on the remaining header items of the original FP-tree.</p>

<p>Once the recursive process has completed, all large item sets with minimum coverage have been found, and association rule creation begins.<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a></p>
<h3 id="others">Others</h3>
<h4 id="aprioridp">AprioriDP</h4>

<p>AprioriDP<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a> utilizes <a href="Dynamic_Programming" title="wikilink">Dynamic Programming</a> in Frequent itemset mining. The working principle is to eliminate the candidate generation like FP-tree, but it stores support count in specialized data structure instead of tree.</p>
<h4 id="context-based-association-rule-mining-algorithm">Context Based Association Rule Mining Algorithm</h4>

<p><a href="http://www.sciencedirect.com/science/article/pii/S0950705112002237">CBPNARM</a> is the newly developed algorithm which is developed in 2013 to mine association rules on the basis of context. It uses context variable on the basis of which the support of an itemset is changed on the basis of which the rules are finally populated to the rule set.</p>
<h4 id="node-set-based-algorithms">Node-set-based algorithms</h4>

<p>FIN,<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> PrePost <a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a> and PPV <a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a> are three algorithms based on node sets. They use nodes in a coding FP-tree to represent itemsets, and employ a depth-first search strategy to discovery frequent itemsets using "intersection" of node sets.</p>
<h4 id="guha-procedure-assoc">GUHA procedure ASSOC</h4>

<p><a class="uri" href="GUHA" title="wikilink">GUHA</a> is a general method for exploratory data analysis that has theoretical foundations in <a href="observational_calculi" title="wikilink">observational calculi</a>.<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a></p>

<p>The ASSOC procedure<a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a> is a GUHA method which mines for generalized association rules using fast <a href="bitstring" title="wikilink">bitstrings</a> operations. The association rules mined by this method are more general than those output by apriori, for example "items" can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori: an arbitrary combination of supported interest measures can be used.</p>
<h4 id="opus-search">OPUS search</h4>

<p>OPUS is an efficient algorithm for rule discovery that, in contrast to most alternatives, does not require either monotone or anti-monotone constraints such as minimum support.<a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a> Initially used to find rules for a fixed consequent<a class="footnoteRef" href="#fn32" id="fnref32"><sup>32</sup></a><a class="footnoteRef" href="#fn33" id="fnref33"><sup>33</sup></a> it has subsequently been extended to find rules with any item as a consequent.<a class="footnoteRef" href="#fn34" id="fnref34"><sup>34</sup></a> OPUS search is the core technology in the popular <a href="http://www.giwebb.com">Magnum Opus</a> association discovery system.</p>
<h2 id="lore">Lore</h2>

<p>A famous story about association rule mining is the "beer and diaper" story. A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true.<a class="footnoteRef" href="#fn35" id="fnref35"><sup>35</sup></a> Daniel Powers says:<a class="footnoteRef" href="#fn36" id="fnref36"><sup>36</sup></a></p>
<blockquote>

<p>In 1992, Thomas Blischok, manager of a retail consulting group at Teradata, and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis "did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves.</p>
</blockquote>
<h2 id="other-types-of-association-mining">Other types of association mining</h2>

<p><strong>Multi-Relation Association Rules</strong>: Multi-Relation Association Rules (MRAR) is a new class of association rules which in contrast to primitive, simple and even multi-relational association rules (that are usually extracted from multi-relational databases), each rule item consists of one entity but several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations <em>live in</em>, <em>nearby</em> and <em>humid</em>: “Those who <em>live in</em> a place which is <em>near by</em> a city with <em>humid</em> climate type and also are <em>younger</em> than 20 -&gt; their <em>health condition</em> is good”. Such association rules are extractable from RDBMS data or semantic web data.<a class="footnoteRef" href="#fn37" id="fnref37"><sup>37</sup></a></p>

<p><strong><a href="Context_Based_Association_Rules" title="wikilink">Context Based Association Rules</a></strong> is a form of association rule. <strong>Context Based Association Rules</strong> claims more accuracy in association rule mining by considering a hidden variable named context variable which changes the final set of association rules depending upon the value of context variables. For example the baskets orientation in market basket analysis reflects an odd pattern in the early days of month.This might be because of abnormal context i.e. salary is drawn at the start of the month <a class="footnoteRef" href="#fn38" id="fnref38"><sup>38</sup></a></p>

<p><strong><a href="Contrast_set_learning" title="wikilink">Contrast set learning</a></strong> is a form of associative learning. <strong>Contrast set learners</strong> use rules that differ meaningfully in their distribution across subsets.<a class="footnoteRef" href="#fn39" id="fnref39"><sup>39</sup></a><a class="footnoteRef" href="#fn40" id="fnref40"><sup>40</sup></a></p>

<p><strong>Weighted class learning</strong> is another form of associative learning in which weight may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.</p>

<p><strong>High-order pattern discovery</strong> facilitate the capture of high-order (polythetic) patterns or event associations that are intrinsic to complex real-world data. <a class="footnoteRef" href="#fn41" id="fnref41"><sup>41</sup></a></p>

<p><strong><a href="K-optimal_pattern_discovery" title="wikilink">K-optimal pattern discovery</a></strong> provides an alternative to the standard approach to association rule learning that requires that each pattern appear frequently in the data.</p>

<p><strong>Approximate Frequent Itemset</strong> mining is a relaxed version of Frequent Itemset mining that allows some of the items in some of the rows to be 0.<a class="footnoteRef" href="#fn42" id="fnref42"><sup>42</sup></a></p>

<p><strong>Generalized Association Rules</strong> hierarchical taxonomy (concept hierarchy)</p>

<p><strong>Quantitative Association Rules</strong> categorical and quantitative data <a class="footnoteRef" href="#fn43" id="fnref43"><sup>43</sup></a></p>

<p><strong>Interval Data Association Rules</strong> e.g. partition the age into 5-year-increment ranged</p>

<p><strong>Maximal Association Rules</strong></p>

<p>'''Sequential pattern mining ''' discovers subsequences that are common to more than minsup sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.<a class="footnoteRef" href="#fn44" id="fnref44"><sup>44</sup></a></p>

<p><strong>Sequential Rules</strong> discovering relationships between items while considering the time ordering. It is generally applied on a sequence database. For example, a sequential rule found in database of sequences of customer transactions can be that customers who bought a computer and CD-Roms, later bought a webcam, with a given confidence and support.</p>

<p>'''Warmr '''is shipped as part of the ACE data mining suite. It allows association rule learning for first order relational rules.<a class="footnoteRef" href="#fn45" id="fnref45"><sup>45</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Sequence_mining" title="wikilink">Sequence mining</a></li>
<li><a href="Production_system_(computer_science)" title="wikilink">Production system (computer science)</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<h3 id="bibliographies">Bibliographies</h3>
<ul>
<li><a href="http://www.uco.es/grupos/kdis/ARMBibliography">Extensive Bibliography on Association Rules</a> by J.M. Luna</li>
<li><a href="http://michael.hahsler.net/research/bib/association_rules/">Annotated Bibliography on Association Rules</a> by M. Hahsler</li>
<li><a href="http://www.statsoft.com/textbook/association-rules/">Statsoft Electronic Statistics Textbook: Association Rules</a> by <a class="uri" href="Dell" title="wikilink">Dell</a> Software</li>
</ul>
<h3 id="implementations">Implementations</h3>
<dl>
<dt>Open-Source data-mining suites</dt>
</dl>
<ul>
<li><a href="http://www.borgelt.net/fpm.html">Christian Borgelt's implementations of Apriori, FP-Growth and Eclat</a> written in C with Python bindings.</li>
<li><a class="uri" href="ELKI" title="wikilink">ELKI</a> includes Java implementations of Apriori, Eclat and FPGrowth.</li>
<li><a href="Orange_(software)" title="wikilink">Orange</a> module <a href="http://www.ailab.si/orange/doc/modules/orngAssoc.htm">orngAssoc</a>.</li>
<li><a href="R_(programming_language)" title="wikilink">R</a> package <a href="http://cran.r-project.org/package=arules">arules</a> for mining association rules and frequent itemsets.</li>
<li><a href="http://www.philippe-fournier-viger.com/spmf/">SPMF</a> offers many open-source implementations for association rule mining, itemset mining and sequential pattern mining.</li>
<li><a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a>, a collection of machine learning algorithms for data mining tasks written in <a href="Java_(programming_language)" title="wikilink">Java</a></li>
</ul>
<dl>
<dt>Academic example code</dt>
</dl>
<ul>
<li><a href="http://www.cs.umb.edu/~laur/ARtool/">ARtool</a>, GPL Java association rule mining application with GUI, offering implementations of multiple algorithms for discovery of frequent patterns and extraction of association rules (includes Apriori and FPgrowth, last updated 2002)</li>
<li><a href="http://adrem.ua.ac.be/~goethals/software/">Bart Goethals' frequent pattern mining implementations</a></li>
<li><a href="http://ferda.sourceforge.net">Ferda Dataminer</a>, an extensible visual data mining platform, implements GUHA procedures ASSOC and features multirelational data mining</li>
<li><a href="http://fimi.ua.ac.be/src/">Frequent Itemset Mining Implementations Repository (FIMI)</a></li>
<li><a href="http://www.uco.es/grupos/kdis/ARMBibliography/algorithms.html">Java implementations of association rule mining algorithms</a> by KDIS</li>
<li><a href="http://ai4r.org">Ruby implementation (AI4R)</a></li>
<li>Zaki, Mohammed J.; <a href="http://www.cs.rpi.edu/~zaki/software/">Data Mining Software</a></li>
</ul>
<dl>
<dt>Commercial offers</dt>
</dl>
<ul>
<li><a class="uri" href="KNIME" title="wikilink">KNIME</a>, an open source workflow oriented data preprocessing and analysis platform</li>
<li><a href="KXEN_Inc." title="wikilink">KXEN</a>, a commercial Data Mining software</li>
<li><a href="http://lispminer.vse.cz">LISp Miner</a>, mines for generalized (GUHA) association rules (uses bitstrings, not apriori algorithm)</li>
<li><a href="http://www.giwebb.com">Magnum Opus</a>, a system for statistically sound association discovery</li>
<li><a class="uri" href="RapidMiner" title="wikilink">RapidMiner</a>, a Java data mining software suite</li>
<li><a class="uri" href="STATISTICA" title="wikilink">STATISTICA</a>, commercial statistics software with an Association Rules module</li>
</ul>

<p>"</p>

<p><a href="Category:Data_management" title="wikilink">Category:Data management</a> <a href="Category:Data_mining" title="wikilink">Category:Data mining</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Piatetsky-Shapiro, Gregory (1991), <em>Discovery, analysis, and presentation of strong rules</em>, in Piatetsky-Shapiro, Gregory; and Frawley, William J.; eds., <em>Knowledge Discovery in Databases</em>, AAAI/MIT Press, Cambridge, MA.<a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4">Michael Hahsler (2015). A Probabilistic Comparison of Commonly Used Interest Measures for Association Rules. <a class="uri" href="http://michael.hahsler.net/research/association_rules/measures.html">http://michael.hahsler.net/research/association_rules/measures.html</a><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8">Pei, Jian; Han, Jiawei; and Lakshmanan, Laks V. S.; <em>Mining frequent itemsets with convertible constraints</em>, in <em>Proceedings of the 17th International Conference on Data Engineering, April 2–6, 2001, Heidelberg, Germany</em>, 2001, pages 433-442<a href="#fnref8">↩</a></li>
<li id="fn9">Agrawal, Rakesh; and Srikant, Ramakrishnan; <a href="http://rakesh.agrawal-family.com/papers/vldb94apriori.pdf"><em>Fast algorithms for mining association rules in large databases</em></a>, in Bocca, Jorge B.; Jarke, Matthias; and Zaniolo, Carlo; editors, <em>Proceedings of the 20th International Conference on Very Large Data Bases (VLDB), Santiago, Chile, September 1994</em>, pages 487-499<a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"></li>
<li id="fn12">Hájek, Petr; Havel, Ivan; Chytil, Metoděj; <em>The GUHA method of automatic hypotheses determination</em>, Computing 1 (1966) 293-308<a href="#fnref12">↩</a></li>
<li id="fn13">Hájek, Petr; Feglar, Tomas; Rauch, Jan; and Coufal, David; <em>The GUHA method, data preprocessing and mining</em>, Database Support for Data Mining Applications, Springer, 2004, ISBN 978-3-540-22479-2<a href="#fnref13">↩</a></li>
<li id="fn14">Omiecinski, Edward R.; <em>Alternative interest measures for mining associations in databases</em>, IEEE Transactions on Knowledge and Data Engineering, 15(1):57-69, Jan/Feb 2003<a href="#fnref14">↩</a></li>
<li id="fn15">Aggarwal, Charu C.; and Yu, Philip S.; <em>A new framework for itemset generation</em>, in <em>PODS 98, Symposium on Principles of Database Systems, Seattle, WA, USA, 1998</em>, pages 18-24<a href="#fnref15">↩</a></li>
<li id="fn16">Brin, Sergey; Motwani, Rajeev; Ullman, Jeffrey D.; and Tsur, Shalom; <em>Dynamic itemset counting and implication rules for market basket data</em>, in <em>SIGMOD 1997, Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD 1997), Tucson, Arizona, USA, May 1997</em>, pp. 255-264<a href="#fnref16">↩</a></li>
<li id="fn17">Piatetsky-Shapiro, Gregory; <em>Discovery, analysis, and presentation of strong rules</em>, Knowledge Discovery in Databases, 1991, pp. 229-248<a href="#fnref17">↩</a></li>
<li id="fn18">Brin, Sergey; Motwani, Rajeev; Ullman, Jeffrey D.; and Tsur, Shalom; <em>Dynamic itemset counting and implication rules for market basket data</em>, in <em>SIGMOD 1997, Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD 1997), Tucson, Arizona, USA, May 1997</em>, pp. 265-276<a href="#fnref18">↩</a></li>
<li id="fn19">Tan, Pang-Ning; Kumar, Vipin; and Srivastava, Jaideep; <em>Selecting the right objective measure for association analysis</em>, Information Systems, 29(4):293-313, 2004<a href="#fnref19">↩</a></li>
<li id="fn20">Webb, Geoffrey I. (2007); <em>Discovering Significant Patterns</em>, Machine Learning 68(1), Netherlands: Springer, pp. 1-33 <a href="http://springerlink.metapress.com/content/4r35537x6vxg0523/?p=9291269dbfed4750a6e1d6e9bf6f3c13π=0">online access</a><a href="#fnref20">↩</a></li>
<li id="fn21">Gionis, Aristides; <a href="Heikki_Mannila" title="wikilink">Mannila, Heikki</a>; Mielikäinen, Taneli; and Tsaparas, Panayiotis; <em>Assessing Data Mining Results via Swap Randomization</em>, ACM Transactions on Knowledge Discovery from Data (TKDD), Volume 1, Issue 3 (December 2007), Article No. 14<a href="#fnref21">↩</a></li>
<li id="fn22"></li>
<li id="fn23"></li>
<li id="fn24">Witten, Frank, Hall: Data mining practical machine learning tools and techniques, 3rd edition<a href="#fnref24">↩</a></li>
<li id="fn25"></li>
<li id="fn26"></li>
<li id="fn27"></li>
<li id="fn28"></li>
<li id="fn29">Rauch, Jan; <em>Logical calculi for knowledge discovery in databases</em>, in <em>Proceedings of the First European Symposium on Principles of Data Mining and Knowledge Discovery</em>, Springer, 1997, pp. 47-57<a href="#fnref29">↩</a></li>
<li id="fn30"><a href="#fnref30">↩</a></li>
<li id="fn31">Webb, Geoffrey I. (1995); <em>OPUS: An Efficient Admissible Algorithm for Unordered Search</em>, Journal of Artificial Intelligence Research 3, Menlo Park, CA: AAAI Press, pp. 431-465 <a href="http://www.cs.washington.edu/research/jair/abstracts/webb95a.html">online access</a><a href="#fnref31">↩</a></li>
<li id="fn32"></li>
<li id="fn33"><a href="#fnref33">↩</a></li>
<li id="fn34">Webb, Geoffrey I. (2000); <em>Efficient Search for Association Rules</em>, in Ramakrishnan, Raghu; and Stolfo, Sal; eds.; <em>Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2000), Boston, MA</em>, New York, NY: The Association for Computing Machinery, pp. 99-107 <a href="http://www.csse.monash.edu/~webb/Files/Webb00b.pdf">online access</a><a href="#fnref34">↩</a></li>
<li id="fn35"><a class="uri" href="http://www.dssresources.com/newsletters/66.php">http://www.dssresources.com/newsletters/66.php</a><a href="#fnref35">↩</a></li>
<li id="fn36"></li>
<li id="fn37">Ramezani, Reza, Mohamad Saraee, and Mohammad Ali Nematbakhsh; <em>MRAR: Mining Multi-Relation Association Rules</em>, Journal of Computing and Security, 1, no. 2 (2014)<a href="#fnref37">↩</a></li>
<li id="fn38">Shaheen, M; Shahbaz, M; and Guergachi, A; <em>Context Based Positive and Negative Spatio Temporal Association Rule Mining</em>, Elsevier Knowledge-Based Systems, Jan 2013, pp. 261-273<a href="#fnref38">↩</a></li>
<li id="fn39"><a href="#fnref39">↩</a></li>
<li id="fn40">Menzies, Tim; and Hu, Ying; <em>Data Mining for Very Busy People</em>, IEEE Computer, October 2003, pp. 18-25<a href="#fnref40">↩</a></li>
<li id="fn41"><a href="#fnref41">↩</a></li>
<li id="fn42">Jinze Liu, Susan Paulsen, Xing Sun, Wei Wang, Andrew Nobel, J. P. (2006). Mining approximate frequent itemsets in the presence of noise: Algorithm and analysis. Retrieved from <a class="uri" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.62.3805">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.62.3805</a><a href="#fnref42">↩</a></li>
<li id="fn43"><a href="#fnref43">↩</a></li>
<li id="fn44">Zaki, Mohammed J. (2001); <em>SPADE: An Efficient Algorithm for Mining Frequent Sequences</em>, Machine Learning Journal, 42, pp. 31–60<a href="#fnref44">↩</a></li>
<li id="fn45"><a href="#fnref45">↩</a></li>
</ol>
</section>
</body>
</html>
