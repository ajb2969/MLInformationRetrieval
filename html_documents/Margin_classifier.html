<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="562">Margin classifier</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Margin classifier</h1>
<hr/>

<p>In <a href="machine_learning" title="wikilink">machine learning</a>, a <strong>margin classifer</strong> is a <a href="Statistical_classification" title="wikilink">classifier</a> which is able to give an associated distance from the decision boundary for each example. For instance, if a <a href="linear_classifier" title="wikilink">linear classifier</a> (e.g. <a class="uri" href="perceptron" title="wikilink">perceptron</a> or <a href="linear_discriminant_analysis" title="wikilink">linear discriminant analysis</a>) is used, the distance (typically <a href="euclidean_distance" title="wikilink">euclidean distance</a>, though others may be used) of an example from the separating hyperplane is the margin of that example.</p>

<p>The notion of margin is important in several machine learning classification algorithms, as it can be used to bound the <a href="generalization_error" title="wikilink">generalization error</a> of the classifier. These bounds are frequently shown using the <a href="VC_dimension" title="wikilink">VC dimension</a>. Of particular prominence is the generalization <a href="error_bound" title="wikilink">error bound</a> on <a href="Boosting_(meta-algorithm)" title="wikilink">boosting</a> algorithms and <a href="support_vector_machine" title="wikilink">support vector machines</a>.</p>
<h2 id="support-vector-machine-definition-of-margin">Support vector machine definition of margin</h2>

<p>See <a href="support_vector_machine" title="wikilink">support vector machines</a> and <a href="maximum-margin_hyperplane" title="wikilink">maximum-margin hyperplane</a> for details.</p>
<h2 id="margin-for-boosting-algorithms">Margin for boosting algorithms</h2>

<p>The margin for an iterative <a href="Boosting_(machine_learning)" title="wikilink">boosting</a> algorithm given a set of examples with two classes can be defined as follows. The classifier is given an example pair 

<math display="inline" id="Margin_classifier:0">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mi>x</mi>
   <mo>,</mo>
   <mi>y</mi>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="open">
    <ci>x</ci>
    <ci>y</ci>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x,y)
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Margin_classifier:1">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>∈</mo>
   <mi>X</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>x</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x\in X
  </annotation>
 </semantics>
</math>

 is a domain space and 

<math display="inline" id="Margin_classifier:2">
 <semantics>
  <mrow>
   <mi>y</mi>
   <mo>∈</mo>
   <mi>Y</mi>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mrow>
     <mo>-</mo>
     <mn>1</mn>
    </mrow>
    <mo>,</mo>
    <mrow>
     <mo>+</mo>
     <mn>1</mn>
    </mrow>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <in></in>
     <ci>y</ci>
     <ci>Y</ci>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <set>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <plus></plus>
       <cn type="integer">1</cn>
      </apply>
     </set>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y\in Y=\{-1,+1\}
  </annotation>
 </semantics>
</math>

 is the label of the example. The iterative boosting algorithm then selects a classifier 

<math display="inline" id="Margin_classifier:3">
 <semantics>
  <mrow>
   <msub>
    <mi>h</mi>
    <mi>j</mi>
   </msub>
   <mo>∈</mo>
   <mi>C</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>h</ci>
     <ci>j</ci>
    </apply>
    <ci>C</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h_{j}\in C
  </annotation>
 </semantics>
</math>

 at each iteration 

<math display="inline" id="Margin_classifier:4">
 <semantics>
  <mi>j</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>j</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   j
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Margin_classifier:5">
 <semantics>
  <mi>C</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>C</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   C
  </annotation>
 </semantics>
</math>

 is a space of possible classifiers that predict real values. This hypothesis is then weighted by 

<math display="inline" id="Margin_classifier:6">
 <semantics>
  <mrow>
   <msub>
    <mi>α</mi>
    <mi>j</mi>
   </msub>
   <mo>∈</mo>
   <mi>R</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>α</ci>
     <ci>j</ci>
    </apply>
    <ci>R</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha_{j}\in R
  </annotation>
 </semantics>
</math>

 as selected by the boosting algorithm. At iteration 

<math display="inline" id="Margin_classifier:7">
 <semantics>
  <mi>t</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>t</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   t
  </annotation>
 </semantics>
</math>

, The margin of an example 

<math display="inline" id="Margin_classifier:8">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 can thus be defined as</p>

<p>

<math display="block" id="Margin_classifier:9">
 <semantics>
  <mfrac>
   <mrow>
    <mi>y</mi>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mi>j</mi>
      <mi>t</mi>
     </msubsup>
     <mrow>
      <msub>
       <mi>α</mi>
       <mi>j</mi>
      </msub>
      <msub>
       <mi>h</mi>
       <mi>j</mi>
      </msub>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mrow>
    <mo largeop="true" symmetric="true">∑</mo>
    <mrow>
     <mo stretchy="false">|</mo>
     <msub>
      <mi>α</mi>
      <mi>j</mi>
     </msub>
     <mo stretchy="false">|</mo>
    </mrow>
   </mrow>
  </mfrac>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <times></times>
     <ci>y</ci>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <ci>j</ci>
       </apply>
       <ci>t</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>α</ci>
        <ci>j</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>h</ci>
        <ci>j</ci>
       </apply>
       <ci>x</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <sum></sum>
     <apply>
      <abs></abs>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>α</ci>
       <ci>j</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{y\sum_{j}^{t}\alpha_{j}h_{j}(x)}{\sum|\alpha_{j}|}
  </annotation>
 </semantics>
</math>

</p>

<p>By this definition, the margin is positive if the example is labeled correctly and negative if the example is labeled incorrectly.</p>

<p>This definition may be modified and is not the only way to define margin for boosting algorithms. However, there are reasons why this definition may be appealing.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="examples-of-margin-based-algorithms">Examples of margin-based algorithms</h2>

<p>Many classifiers can give an associated margin for each example. However, only some classifiers utilize information of the margin while learning from a data set.</p>

<p>Many boosting algorithms rely on the notion of a margin to give weights to examples. If a convex loss is utilized (as in <a class="uri" href="AdaBoost" title="wikilink">AdaBoost</a>, <a class="uri" href="LogitBoost" title="wikilink">LogitBoost</a>, and all members of the <a class="uri" href="AnyBoost" title="wikilink">AnyBoost</a> family of algorithms) then an example with higher margin will receive less (or equal) weight than an example with lower margin. This leads the boosting algorithm to focus weight on low margin examples. In nonconvex algorithms (e.g. <a class="uri" href="BrownBoost" title="wikilink">BrownBoost</a>), the margin still dictates the weighting of an example, though the weighting is non-monotone with respect to margin. There exists boosting algorithms that provably maximize the minimum margin (e.g. see <a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a>).</p>

<p><a href="Support_vector_machine" title="wikilink">Support vector machines</a> provably maximize the margin of the separating hyperplane. Support vector machines that are trained using noisy data (there exists no perfect separation of the data in the given space) maximize the soft margin. More discussion of this can be found in the <a href="support_vector_machine" title="wikilink">support vector machine</a> article.</p>

<p>The <a class="uri" href="voted-perceptron" title="wikilink">voted-perceptron</a> algorithm is a margin maximizing algorithm based on an iterative application of the classic <a class="uri" href="perceptron" title="wikilink">perceptron</a> algorithm.</p>
<h2 id="generalization-error-bounds">Generalization error bounds</h2>

<p>One theoretical motivation behind margin classifiers is that their <a href="generalization_error" title="wikilink">generalization error</a> may be bound by parameters of the algorithm and a margin term. An example of such a bound is for the AdaBoost algorithm.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> Let 

<math display="inline" id="Margin_classifier:10">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

 be a set of 

<math display="inline" id="Margin_classifier:11">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

 examples sampled independently at random from a distribution 

<math display="inline" id="Margin_classifier:12">
 <semantics>
  <mi>D</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>D</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D
  </annotation>
 </semantics>
</math>

. Assume the VC-dimension of the underlying base classifier is 

<math display="inline" id="Margin_classifier:13">
 <semantics>
  <mi>d</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Margin_classifier:14">
 <semantics>
  <mrow>
   <mi>m</mi>
   <mo>≥</mo>
   <mi>d</mi>
   <mo>≥</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <geq></geq>
     <ci>m</ci>
     <ci>d</ci>
    </apply>
    <apply>
     <geq></geq>
     <share href="#.cmml">
     </share>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m\geq d\geq 1
  </annotation>
 </semantics>
</math>

. Then with probability 

<math display="inline" id="Margin_classifier:15">
 <semantics>
  <mrow>
   <mn>1</mn>
   <mo>-</mo>
   <mi>δ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <cn type="integer">1</cn>
    <ci>δ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1-\delta
  </annotation>
 </semantics>
</math>

 we have the bound</p>

<p>

<math display="block" id="Margin_classifier:16">
 <semantics>
  <mrow>
   <msub>
    <mi>P</mi>
    <mi>D</mi>
   </msub>
   <mrow>
    <mo>(</mo>
    <mfrac>
     <mrow>
      <mi>y</mi>
      <mrow>
       <msubsup>
        <mo largeop="true" symmetric="true">∑</mo>
        <mi>j</mi>
        <mi>t</mi>
       </msubsup>
       <mrow>
        <msub>
         <mi>α</mi>
         <mi>j</mi>
        </msub>
        <msub>
         <mi>h</mi>
         <mi>j</mi>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
     <mrow>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mo stretchy="false">|</mo>
       <msub>
        <mi>α</mi>
        <mi>j</mi>
       </msub>
       <mo stretchy="false">|</mo>
      </mrow>
     </mrow>
    </mfrac>
    <mo>≤</mo>
    <mn>0</mn>
    <mo>)</mo>
   </mrow>
   <mo>≤</mo>
   <msub>
    <mi>P</mi>
    <mi>S</mi>
   </msub>
   <mrow>
    <mo>(</mo>
    <mfrac>
     <mrow>
      <mi>y</mi>
      <mrow>
       <msubsup>
        <mo largeop="true" symmetric="true">∑</mo>
        <mi>j</mi>
        <mi>t</mi>
       </msubsup>
       <mrow>
        <msub>
         <mi>α</mi>
         <mi>j</mi>
        </msub>
        <msub>
         <mi>h</mi>
         <mi>j</mi>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
     <mrow>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mo stretchy="false">|</mo>
       <msub>
        <mi>α</mi>
        <mi>j</mi>
       </msub>
       <mo stretchy="false">|</mo>
      </mrow>
     </mrow>
    </mfrac>
    <mo>≤</mo>
    <mi>θ</mi>
    <mo>)</mo>
   </mrow>
   <mo>+</mo>
   <mi>O</mi>
   <mrow>
    <mo>(</mo>
    <mfrac>
     <mn>1</mn>
     <msqrt>
      <mi>m</mi>
     </msqrt>
    </mfrac>
    <msqrt>
     <mrow>
      <mrow>
       <mrow>
        <mi>d</mi>
        <mrow>
         <msup>
          <mi>log</mi>
          <mn>2</mn>
         </msup>
         <mrow>
          <mo stretchy="false">(</mo>
          <mrow>
           <mi>m</mi>
           <mo>/</mo>
           <mi>d</mi>
          </mrow>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
       </mrow>
       <mo>/</mo>
       <msup>
        <mi>θ</mi>
        <mn>2</mn>
       </msup>
      </mrow>
      <mo>+</mo>
      <mrow>
       <mi>log</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mn>1</mn>
         <mo>/</mo>
         <mi>δ</mi>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </msqrt>
    <mo>)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>P</ci>
     <ci>D</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <divide></divide>
      <apply>
       <times></times>
       <ci>y</ci>
       <apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <sum></sum>
          <ci>j</ci>
         </apply>
         <ci>t</ci>
        </apply>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>α</ci>
          <ci>j</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>h</ci>
          <ci>j</ci>
         </apply>
         <ci>x</ci>
        </apply>
       </apply>
      </apply>
      <apply>
       <sum></sum>
       <apply>
        <abs></abs>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>α</ci>
         <ci>j</ci>
        </apply>
       </apply>
      </apply>
     </apply>
     <leq></leq>
     <cn type="integer">0</cn>
     <ci>normal-)</ci>
    </cerror>
    <leq></leq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>P</ci>
     <ci>S</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <divide></divide>
      <apply>
       <times></times>
       <ci>y</ci>
       <apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <sum></sum>
          <ci>j</ci>
         </apply>
         <ci>t</ci>
        </apply>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>α</ci>
          <ci>j</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>h</ci>
          <ci>j</ci>
         </apply>
         <ci>x</ci>
        </apply>
       </apply>
      </apply>
      <apply>
       <sum></sum>
       <apply>
        <abs></abs>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>α</ci>
         <ci>j</ci>
        </apply>
       </apply>
      </apply>
     </apply>
     <leq></leq>
     <csymbol cd="unknown">θ</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <plus></plus>
    <csymbol cd="unknown">O</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <apply>
       <root></root>
       <ci>m</ci>
      </apply>
     </apply>
     <apply>
      <root></root>
      <apply>
       <plus></plus>
       <apply>
        <divide></divide>
        <apply>
         <times></times>
         <ci>d</ci>
         <apply>
          <apply>
           <csymbol cd="ambiguous">superscript</csymbol>
           <log></log>
           <cn type="integer">2</cn>
          </apply>
          <apply>
           <divide></divide>
           <ci>m</ci>
           <ci>d</ci>
          </apply>
         </apply>
        </apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>θ</ci>
         <cn type="integer">2</cn>
        </apply>
       </apply>
       <apply>
        <log></log>
        <apply>
         <divide></divide>
         <cn type="integer">1</cn>
         <ci>δ</ci>
        </apply>
       </apply>
      </apply>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P_{D}\left(\frac{y\sum_{j}^{t}\alpha_{j}h_{j}(x)}{\sum|\alpha_{j}|}\leq 0%
\right)\leq P_{S}\left(\frac{y\sum_{j}^{t}\alpha_{j}h_{j}(x)}{\sum|\alpha_{j}|%
}\leq\theta\right)+O\left(\frac{1}{\sqrt{m}}\sqrt{d\log^{2}(m/d)/\theta^{2}+%
\log(1/\delta)}\right)
  </annotation>
 </semantics>
</math>

</p>

<p>for all 

<math display="inline" id="Margin_classifier:17">
 <semantics>
  <mrow>
   <mi>θ</mi>
   <mo>></mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>θ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta>0
  </annotation>
 </semantics>
</math>

.</p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Classification_algorithms" title="wikilink">Category:Classification algorithms</a> <a href="Category:Statistical_classification" title="wikilink">Category:Statistical classification</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Robert E. Schapire, Yoav Freund, Peter Bartlett and Wee Sun Lee.(1998) "Boosting the margin: A new explanation for the effectiveness of voting methods", <em>The Annals of Statistics</em>, 26(5):1651–1686<a href="#fnref1">↩</a></li>
<li id="fn2">Manfred Warmuth and Karen Glocer and Gunnar Rätsch. Boosting Algorithms for Maximizing the Soft Margin. In the Proceedings of Advances in Neural Information Processing Systems 20, 2007, pp 1585–1592.<a href="#fnref2">↩</a></li>
<li id="fn3"></li>
</ol>
</section>
</body>
</html>
