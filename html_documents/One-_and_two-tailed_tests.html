<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="409">One- and two-tailed tests</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>One- and two-tailed tests</h1>
<hr/>

<p>  In statistical <a href="significance_testing" title="wikilink">significance testing</a>, a <strong>one-tailed test</strong> and a <strong>two-tailed test</strong> are alternative ways of computing the <a href="statistical_significance" title="wikilink">statistical significance</a> of a <a class="uri" href="parameter" title="wikilink">parameter</a> inferred from a data set, in terms of a <a href="test_statistic" title="wikilink">test statistic</a>. A two-tailed test is used if deviations of the estimated parameter in either direction from some benchmark value are considered theoretically possible; in contrast, a one-tailed test is used if only deviations in one direction are considered possible. Alternative names are <strong>one-sided</strong> and <strong>two-sided</strong> tests; the terminology "tail" is used because the extreme portions of distributions, where observations lead to rejection of the <a href="null_hypothesis" title="wikilink">null hypothesis</a>, are small and often "tail off" toward zero as in the <a href="normal_distribution" title="wikilink">normal distribution</a> or "bell curve", pictured above right.</p>
<h2 id="applications">Applications</h2>

<p>One-tailed tests are used for asymmetric distributions that have a single tail, such as the <a href="chi-squared_distribution" title="wikilink">chi-squared distribution</a>, which are common in measuring <a class="uri" href="goodness-of-fit" title="wikilink">goodness-of-fit</a>, or for one side of a distribution that has two tails, such as the <a href="normal_distribution" title="wikilink">normal distribution</a>, which is common in estimating location; this corresponds to specifying a direction. Two-tailed tests are only applicable when there are two tails, such as in the normal distribution, and correspond to considering either direction significant.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>In the approach of <a href="Ronald_Fisher" title="wikilink">Ronald Fisher</a>, the <a href="null_hypothesis" title="wikilink">null hypothesis</a> H<sub>0</sub> will be rejected when the <a href="p-value" title="wikilink"><em>p</em>-value</a> of the <a href="test_statistic" title="wikilink">test statistic</a> is sufficiently extreme (vis-a-vis the test statistic's <a href="sampling_distribution" title="wikilink">sampling distribution</a>) and thus judged unlikely to be the result of chance. In a one-tailed test, "extreme" is decided beforehand as either meaning "sufficiently small" <em>or</em> meaning "sufficiently large" – values in the other direction are considered not significant. In a two-tailed test, "extreme" means "either sufficiently small or sufficiently large", and values in either direction are considered significant.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> For a given test statistic there is a single two-tailed test, and two one-tailed tests, one each for either direction. Given data of a given significance level in a two-tailed test for a test statistic, in the corresponding one-tailed tests for the same test statistic it will be considered either twice as significant (half the <em>p</em>-value), if the data is in the direction specified by the test, or not significant at all (<em>p</em>-value above 0.5), if the data is in the direction opposite that specified by the test.</p>

<p>For example, if <a href="#Coin_flipping_example" title="wikilink">flipping a coin</a>, testing whether it is biased <em>towards</em> heads is a one-tailed test, and getting data of "all heads" would be seen as highly significant, while getting data of "all tails" would be not significant at all (<em>p</em> = 1). By contrast, testing whether it is biased in <em>either</em> direction is a two-tailed test, and either "all heads" or "all tails" would both be seen as highly significant data. In medical testing, while one is generally interested in whether a treatment results in outcomes that are <em>better</em> than chance, thus suggesting a one-tailed test; a <em>worse</em> outcome is also interesting for the scientific field, therefore one should use a two-tailed test that corresponds instead to testing whether the treatment results in outcomes that are <em>different</em> from chance, either better or worse.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> In the archetypal <a href="lady_tasting_tea" title="wikilink">lady tasting tea</a> experiment, Fisher tested whether the lady in question was <em>better</em> than chance at distinguishing two types of tea preparation, not whether her ability was <em>different</em> from chance, and thus he used a one-tailed test.</p>
<h2 id="coin-flipping-example">Coin flipping example</h2>

<p>In coin flipping, the <a href="null_hypothesis" title="wikilink">null hypothesis</a> is a sequence of <a href="Bernoulli_trial" title="wikilink">Bernoulli trials</a> with probability 0.5, yielding a random variable <em>X</em> which is 1 for heads and 0 for tails, and a common test statistic is the <a href="sample_mean" title="wikilink">sample mean</a> (of the number of heads) 

<math display="inline" id="One-_and_two-tailed_tests:0">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>X</mi>
    <mo stretchy="false">¯</mo>
   </mover>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-¯</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{X}.
  </annotation>
 </semantics>
</math>

 If testing for whether the coin is biased towards heads, a one-tailed test would be used – only large numbers of heads would be significant. In that case a data set of five heads (HHHHH), with sample mean of 1, has a 

<math display="inline" id="One-_and_two-tailed_tests:1">
 <semantics>
  <mrow>
   <mrow>
    <mn>1</mn>
    <mo>/</mo>
    <mn>32</mn>
   </mrow>
   <mo>=</mo>
   <mn>0.03125</mn>
   <mo>≈</mo>
   <mn>0.03</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <cn type="integer">32</cn>
     </apply>
     <cn type="float">0.03125</cn>
    </apply>
    <apply>
     <approx></approx>
     <share href="#.cmml">
     </share>
     <cn type="float">0.03</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1/32=0.03125\approx 0.03
  </annotation>
 </semantics>
</math>

 chance of occurring, (5 consecutive flips with 2 outcomes - ((1/2)^5 =32), and thus would have 

<math display="inline" id="One-_and_two-tailed_tests:2">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mo>≈</mo>
   <mn>0.03</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <approx></approx>
    <ci>p</ci>
    <cn type="float">0.03</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p\approx 0.03
  </annotation>
 </semantics>
</math>

 and would be significant (rejecting the null hypothesis) if using 0.05 as the cutoff. However, if testing for whether the coin is biased towards heads or tails, a two-tailed test would be used, and a data set of five heads (sample mean 1) is as extreme as a data set of five tails (sample mean 0), so the <em>p</em>-value would be 

<math display="inline" id="One-_and_two-tailed_tests:3">
 <semantics>
  <mrow>
   <mrow>
    <mn>2</mn>
    <mo>/</mo>
    <mn>32</mn>
   </mrow>
   <mo>=</mo>
   <mn>0.0625</mn>
   <mo>≈</mo>
   <mn>0.06</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <divide></divide>
      <cn type="integer">2</cn>
      <cn type="integer">32</cn>
     </apply>
     <cn type="float">0.0625</cn>
    </apply>
    <apply>
     <approx></approx>
     <share href="#.cmml">
     </share>
     <cn type="float">0.06</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   2/32=0.0625\approx 0.06
  </annotation>
 </semantics>
</math>

 and this would not be significant (not rejecting the null hypothesis) if using 0.05 as the cutoff.</p>
<h2 id="history">History</h2>

<p> The <em>p</em>-value was introduced by <a href="Karl_Pearson" title="wikilink">Karl Pearson</a> in  in the <a href="Pearson's_chi-squared_test" title="wikilink">Pearson's chi-squared test</a>, where he defined P (original notation) as the probability that the statistic would be at or above a given level. This is a one-tailed definition, and the chi-squared distribution is asymmetric, only assuming positive or zero values, and has only one tail, the upper one. It measures <a href="goodness_of_fit" title="wikilink">goodness of fit</a> of data with a theoretical distribution, with zero corresponding to exact agreement with the theoretical distribution; the <em>p</em>-value thus measures how likely the fit would be this bad or worse.</p>

<p> The distinction between one-tailed and two-tailed tests was popularized by <a href="Ronald_Fisher" title="wikilink">Ronald Fisher</a> in the influential book <a href="Statistical_Methods_for_Research_Workers" title="wikilink">Statistical Methods for Research Workers</a> , where he applied it especially to the <a href="normal_distribution" title="wikilink">normal distribution</a>, which is a symmetric distribution with two equal tails. The normal distribution is a common measure of location, rather than goodness-of-fit, and has two tails, corresponding to the estimate of location being above or below the theoretical location (e.g., sample mean compared with theoretical mean). In the case of a symmetric distribution such as the normal distribution, the one-tailed <em>p</em>-value is exactly half the two-tailed <em>p</em>-value: </p>

<p>Fisher emphasized the importance of measuring the tail – the observed value of the test statistic and all more extreme – rather than simply the probability of specific outcome itself, in his <em><a href="The_Design_of_Experiments" title="wikilink">The Design of Experiments</a></em> (1935). He explains this as because a <em>specific</em> set of data may be unlikely (in the null hypothesis), but more extreme outcomes likely, so seen in this light, the specific but not extreme unlikely data should not be considered significant.</p>
<h2 id="relation-to-hypothesis-testing">Relation to hypothesis testing</h2>

<p><em>p</em>-values and one-tailed/two-tailed tests are a concept in the significance testing of Fisher, which only uses a null hypothesis, and either rejects it or not. <em>p</em>-values are not used in the <a href="hypothesis_testing" title="wikilink">hypothesis testing</a> of <a href="Jerzy_Neyman" title="wikilink">Jerzy Neyman</a> and <a href="Egon_Pearson" title="wikilink">Egon Pearson</a>, which instead compares the null hypothesis to an <a href="alternative_hypothesis" title="wikilink">alternative hypothesis</a>, and chooses between them. However, these approaches are frequently confused and conflated – see <a href="statistical_hypothesis_testing" title="wikilink">statistical hypothesis testing</a> – and thus <em>p</em>-values and one-tailed or two-tailed tests of significance may be incorrectly used in Neyman–Pearson-style hypothesis testing.</p>

<p>This is a mistaken interpretation, but it is a common mistake; this results in a confusing mixture of terminology, as follows – note that "significance level" is used in different senses in Fisher and in Neyman–Pearson, while "alternative hypothesis" is used only in Neyman–Pearson. In this context a one-tailed test is interpreted as using an "alternative hypothesis" that some parameter is <em>greater</em> than it is in the null hypothesis (or <em>less</em>), while a two-tailed test is interpreted as using as "alternative hypothesis" that the parameter is <em>different</em> from what it is in the null hypothesis. For example, if the null hypothesis is that the mean 

<math display="inline" id="One-_and_two-tailed_tests:4">
 <semantics>
  <mi>μ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>μ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu
  </annotation>
 </semantics>
</math>

 is some value 

<math display="inline" id="One-_and_two-tailed_tests:5">
 <semantics>
  <mrow>
   <msub>
    <mi>μ</mi>
    <mn>0</mn>
   </msub>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>μ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu_{0},
  </annotation>
 </semantics>
</math>

 then the one-tailed test "corresponds to" the alternative hypothesis 

<math display="inline" id="One-_and_two-tailed_tests:6">
 <semantics>
  <mrow>
   <mi>μ</mi>
   <mo>></mo>
   <msub>
    <mi>μ</mi>
    <mn>0</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>μ</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>μ</ci>
     <cn type="integer">0</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu>\mu_{0}
  </annotation>
 </semantics>
</math>

 (or 

<math display="inline" id="One-_and_two-tailed_tests:7">
 <semantics>
  <mrow>
   <mi>μ</mi>
   <mo><</mo>
   <msub>
    <mi>μ</mi>
    <mn>0</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <lt></lt>
    <ci>μ</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>μ</ci>
     <cn type="integer">0</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu<\mu_{0}
  </annotation>
 </semantics>
</math>

), while the two-tailed test "corresponds to" the alternative hypothesis 

<math display="inline" id="One-_and_two-tailed_tests:8">
 <semantics>
  <mrow>
   <mrow>
    <mi>μ</mi>
    <mo>≠</mo>
    <msub>
     <mi>μ</mi>
     <mn>0</mn>
    </msub>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <neq></neq>
    <ci>μ</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>μ</ci>
     <cn type="integer">0</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu\neq\mu_{0}.
  </annotation>
 </semantics>
</math>

 While Fisher rejected the notion of an alternative hypothesis, Neyman accused him of subconsciously harboring an alternative hypothesis when choosing how to evaluate the null hypothesis, of which this one-tailed/two-tailed choice is one example.</p>

<p>Further, since in the Neyman–Pearson approach "significance levels" (in the sense of <a href="false_positive" title="wikilink">false positive</a>/<a href="type_I_error" title="wikilink">type I error</a> rate, rather than in the Fisher sense of <em>p</em>-value of the test statistic), which are denoted by <em>α,</em> share the "significance level" name and are also conventionally 0.05, these two concepts may be confused. In this case the cut-offs in the tails are denoted by <em>α,</em> and then compared with the <em>p</em>-value of the data, using <em>α</em>/2 at each end in the two-tailed test. This is incorrect – <em>p</em>-values are not simply related to false positives and cannot be compared with <em>α,</em> as discussed at <a href="p-value" title="wikilink"><em>p</em>-value</a> – but this notation is very common.</p>
<h2 id="specific-tests">Specific tests</h2>

<p>If the test statistic follows a <a href="Student's_t-distribution" title="wikilink">Student's <em>t</em> distribution</a> in the null hypothesis – which is common where the underlying variable follows a <a href="normal_distribution" title="wikilink">normal distribution</a> with unknown scaling factor, then the test is referred to as a one-tailed or two-tailed <a href="Student's_t-test" title="wikilink"><em>t</em>-test</a>. If the test is performed using the actual population mean and variance, rather than an estimate from a sample, it would be called a one-tailed or two-tailed <a href="Z-test" title="wikilink"><em>Z</em>-test</a>.</p>

<p>The <a href="Quantile_function#Applications" title="wikilink">statistical tables</a> for <em>t</em> and for <em>Z</em> provide <a href="critical_values" title="wikilink">critical values</a> for both one- and two-tailed tests. That is, they provide the critical values that cut off an entire region at one or the other end of the sampling distribution as well as the critical values that cut off the regions (of half the size) at both ends of the sampling distribution.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Paired_difference_test" title="wikilink">Paired difference test</a>, when two samples are being compared</li>
</ul>
<h2 id="references">References</h2>
<ul>
<li></li>
<li></li>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Statistical_terminology" title="wikilink">Category:Statistical terminology</a> <a href="Category:Statistical_tests" title="wikilink">Category:Statistical tests</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Kock, N. (2015). <a href="https://drive.google.com/file/d/0B76EXfrQqs3ha255TkliQk1ONEE/view">One-tailed or two-tailed P values in PLS-SEM?</a> International Journal of e-Collaboration, 11(2), 1-7.<a href="#fnref1">↩</a></li>
<li id="fn2">Mundry, R., &amp; Fischer, J. (1998). <a href="http://www.sciencedirect.com/science/article/pii/S0003347298907564">Use of statistical programs for nonparametric tests of small samples often leads to incorrect P values: Examples from Animal Behaviour.</a> Animal behaviour, 56(1), 256-259.<a href="#fnref2">↩</a></li>
<li id="fn3">Pillemer, D. B. (1991). <a href="http://edr.sagepub.com/content/20/9/13.short">One-versus two-tailed hypothesis tests in contemporary educational research.</a> Educational Researcher, 20(9), 13-17.<a href="#fnref3">↩</a></li>
<li id="fn4"><a href="John_E._Freund" title="wikilink">John E. Freund</a>, (1984) <em>Modern Elementary Statistics</em>, sixth edition. Prentice hall. ISBN 0-13-593525-3 (Section "Inferences about Means", chapter "Significance Tests", page 289.)<a href="#fnref4">↩</a></li>
<li id="fn5">J M Bland, D G Bland (BMJ, 1994) <em>Statistics Notes: One and two sided tests of significance</em><a href="#fnref5">↩</a></li>
</ol>
</section>
</body>
</html>
