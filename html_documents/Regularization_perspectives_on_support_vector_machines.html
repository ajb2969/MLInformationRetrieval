<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="190">Regularization perspectives on support vector machines</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Regularization perspectives on support vector machines</h1>
<hr/>
<p><strong>Regularization perspectives on support vector machines</strong> provide a way of interpreting <a href="support_vector_machine" title="wikilink">support vector machines</a> (SVMs) in the context of other machine learning algorithms. SVM algorithms categorize <a class="uri" href="multidimensional" title="wikilink">multidimensional</a> data, with the goal of fitting the <a href="training_set" title="wikilink">training set</a> data well, but also avoiding <a class="uri" href="overfitting" title="wikilink">overfitting</a>, so that the solution <a href="generalize" title="wikilink">generalizes</a> to new data points. <a href="Regularization_(mathematics)" title="wikilink">Regularization</a> algorithms also aim to fit training set data and avoid overfitting. They do this by choosing a fitting function that has low error on the training set, but also is not too complicated, where complicated functions are functions with high <a href="norm_(mathematics)" title="wikilink">norms</a> in some <a href="function_space" title="wikilink">function space</a>. Specifically, <a href="Tikhonov_regularization" title="wikilink">Tikhonov regularization</a> algorithms choose a function that minimize the sum of training set error plus the function's norm. The training set error can be calculated with different <a href="loss_function" title="wikilink">loss functions</a>. For example, <a href="regularized_least_squares" title="wikilink">regularized least squares</a> is a special case of Tikhonov regularization using the <a href="squared_error_loss" title="wikilink">squared error loss</a> as the loss function.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<p>Regularization perspectives on support vector machines interpret SVM as a special case Tikhonov regularization, specifically Tikhonov regularization with the <a href="hinge_loss" title="wikilink">hinge loss</a> for a loss function. This provides a theoretical framework with which to analyze SVM algorithms and compare them to other algorithms with the same goals: to <a class="uri" href="generalize" title="wikilink">generalize</a> without <a class="uri" href="overfitting" title="wikilink">overfitting</a>. SVM was first proposed in 1995 by <a href="Corinna_Cortes" title="wikilink">Corinna Cortes</a> and <a href="Vladimir_Vapnik" title="wikilink">Vladimir Vapnik</a>, and framed geometrically as a method for finding <a href="hyperplane" title="wikilink">hyperplanes</a> that can separate <a class="uri" href="multidimensional" title="wikilink">multidimensional</a> data into two categories.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> This traditional geometric interpretation of SVMs provides useful intuition about how SVMs work, but is difficult to relate to other <a href="machine_learning" title="wikilink">machine learning</a> techniques for avoiding overfitting like <a href="regularization_(mathematics)" title="wikilink">regularization</a>, <a href="early_stopping" title="wikilink">early stopping</a>, <a class="uri" href="sparsity" title="wikilink">sparsity</a> and <a href="Bayesian_inference" title="wikilink">Bayesian inference</a>. However, once it was discovered that SVM is also a <a href="special_case" title="wikilink">special case</a> of Tikhonov regularization, regularization perspectives on SVM provided the theory necessary to fit SVM within a broader class of algorithms.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> This has enabled detailed comparisons between SVM and other forms of Tikhonov regularization, and theoretical grounding for why it is beneficial to use SVM's loss function, the hinge loss.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h2 id="theoretical-background">Theoretical background</h2>
<p>In the <a href="statistical_learning_theory" title="wikilink">statistical learning theory</a> framework, an <a class="uri" href="algorithm" title="wikilink">algorithm</a> is a strategy for choosing a <a href="function_(mathematics)" title="wikilink">function</a> <span class="LaTeX">$f:\mathbf X \to \mathbf Y$</span> given a training set <span class="LaTeX">$S = \{(x_1,y_1),\ldots, (x_n,y_n)\}$</span> of inputs, <span class="LaTeX">$x_i$</span>, and their labels, <span class="LaTeX">$y_i$</span> (the labels are usually <span class="LaTeX">$\pm1$</span>). <a href="Regularization_(mathematics)" title="wikilink">Regularization</a> strategies avoid <a class="uri" href="overfitting" title="wikilink">overfitting</a> by choosing a function that fits the data, but is not too complex. Specifically:</p>
<p><span class="LaTeX">$f = \text{arg}\min_{f\in\mathcal{H}}\left\{\frac{1}{n}\sum_{i=1}^n V(y_i,f(x_i))+\lambda| |f| |^2_\mathcal{H}\right\}$</span>,</p>
<p>where <span class="LaTeX">$\mathcal{H}$</span> is a <a href="hypothesis_space" title="wikilink">hypothesis space</a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> of functions, <span class="LaTeX">$V:\mathbf Y \times \mathbf Y \to \mathbb R$</span> is the loss function, <span class="LaTeX">$| |\cdot| |_\mathcal H$</span> is a <a href="norm_(mathematics)" title="wikilink">norm</a> on the hypothesis space of functions, and <span class="LaTeX">$\lambda\in\mathbb R$</span> is the <a href="regularization_parameter" title="wikilink">regularization parameter</a>.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>
<p>When <span class="LaTeX">$\mathcal{H}$</span> is a <a href="reproducing_kernel_Hilbert_space" title="wikilink">reproducing kernel Hilbert space</a>, there exists a <a href="kernel_function" title="wikilink">kernel function</a> <span class="LaTeX">$K: \mathbf X \times \mathbf X \to \mathbb R$</span> that can be written as an <span class="LaTeX">$n\times n$</span> <a class="uri" href="symmetric" title="wikilink">symmetric</a> <a href="Positive-definite_kernel" title="wikilink">positive definite</a> <a href="matrix_(mathematics)" title="wikilink">matrix</a> <span class="LaTeX">$\mathbf K$</span>. By the <a href="representer_theorem" title="wikilink">representer theorem</a>,<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> <span class="LaTeX">$f(x_i) = \sum_{f=1}^n c_j \mathbf K_{ij}$</span>, and <span class="LaTeX">$| |f| |^2_{\mathcal H} = \langle f,f\rangle_\mathcal H = \sum_{i=1}^n\sum_{j=1}^n c_ic_jK(x_i,x_j) = c^T\mathbf K c$</span></p>
<h2 id="special-properties-of-the-hinge-loss">Special properties of the hinge loss</h2>
<figure><b>(Figure)</b>
<figcaption>Hinge and misclassification loss functions</figcaption>
</figure>
<p>The simplest and most intuitive loss function for categorization is the misclassification loss, or 0-1 loss, which is 0 if <span class="LaTeX">$f(x_i)=y_i$</span> and 1 if <span class="LaTeX">$f(x_i) \neq y_i$</span>, i.e. the <a href="Heaviside_step_function" title="wikilink">Heaviside step function</a> on <span class="LaTeX">$-y_if(x_i)$</span>. However, this loss function is not <a href="convex_function" title="wikilink">convex</a>, which makes the regularization problem very difficult to minimize computationally. Therefore, we look for convex substitutes for the 0-1 loss. The hinge loss, <span class="LaTeX">$V(y_i,f(x_i)) = (1-yf(x))_+$</span> where <span class="LaTeX">$(s)_+ = max(s,0)$</span>, provides such a <a href="convex_relaxation" title="wikilink">convex relaxation</a>. In fact, the hinge loss is the tightest convex <a href="upper_bound" title="wikilink">upper bound</a> to the 0-1 misclassification loss function,<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> and with infinite data returns the <a href="Bayes'_theorem" title="wikilink">Bayes</a> optimal solution:<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> <span class="LaTeX">$f_b(x) = \left\{\begin{matrix}1&p;(1|x)>p(-1|x)\\-1&p;(1|x)<p(-1|x)\end{matrix}\right.< math="">

==Derivation==

The Tikhonov regularization problem can be shown to be equivalent to traditional formulations of SVM by expressing it in terms of the hinge loss.<ref>For a detailed derivation, see {{cite book|last=Rifkin|first=Ryan|title=Everything Old is New Again: A Fresh Look at Historical Approaches in Machine Learning|year=2002|publisher=MIT (PhD thesis)|url=http://web.mit.edu/~9.520/www/Papers/thesis-rifkin.pdf}}</ref> With the hinge loss,

<math> V(y_i,f(x_i)) = (1-yf(x))_+$</span></p>
<p>where <span class="LaTeX">$(s)_+ = max(s,0)$</span>, the regularization problem becomes</p>
<p><span class="LaTeX">$f = \text{arg}\min_{f\in\mathcal{H}}\left\{\frac{1}{n}\sum_{i=1}^n  (1-yf(x))_+ +\lambda| |f| |^2_\mathcal{H}\right\}$</span>.</p>
<p>Multiplying by <span class="LaTeX">$1/(2\lambda)$</span> yields</p>
<p><span class="LaTeX">$f = \text{arg}\min_{f\in\mathcal{H}}\left\{C\sum_{i=1}^n  (1-yf(x))_+ +\frac{1}{2}| |f| |^2_\mathcal{H}\right\}$</span>,</p>
<p>with <span class="LaTeX">$C = 1/(2\lambda n)$</span>, which is equivalent to the standard SVM minimization problem.</p>
<h2 id="notes-and-references">Notes and references</h2>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>"</p>
<p><a href="Category:Support_vector_machines" title="wikilink">Category:Support vector machines</a> <a href="Category:Estimation_theory" title="wikilink">Category:Estimation theory</a> <a href="Category:Mathematical_analysis" title="wikilink">Category:Mathematical analysis</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">â†©</a></li>
<li id="fn2"><a href="#fnref2">â†©</a></li>
<li id="fn3"></li>
<li id="fn4"><a href="#fnref4">â†©</a></li>
<li id="fn5"><a href="#fnref5">â†©</a></li>
<li id="fn6"><a href="#fnref6">â†©</a></li>
<li id="fn7">A hypothesis space is the set of functions used to model the data in a machine learning problem. Each function corresponds to a hypothesis about the structure of the data. Typically the functions in a hypothesis space form a <a href="Hilbert_space" title="wikilink">Hilbert space</a> of functions with norm formed from the loss function.<a href="#fnref7">â†©</a></li>
<li id="fn8">For insight on choosing the parameter, see, e.g., <a href="#fnref8">â†©</a></li>
<li id="fn9">See <a href="#fnref9">â†©</a></li>
<li id="fn10"></li>
<li id="fn11"></li>
<li id="fn12"><a href="#fnref12">â†©</a></li>
</ol>
</section>
</body>
</html>
