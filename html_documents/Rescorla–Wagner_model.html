<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="71">Rescorla–Wagner model</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Rescorla–Wagner model</h1>
<hr/>

<p>The <strong>Rescorla–Wagner model</strong> is a model of <a href="classical_conditioning" title="wikilink">classical conditioning</a> in which the animal is said to learn from the discrepancy between what is expected to happen and what actually happens. This is a trial-level model in which each stimulus is either present or not present at some point in the trial. The prediction of the <a href="unconditioned_stimulus" title="wikilink">unconditioned stimulus</a> for a trial can be represented as the sum of all the associative strengths for the <a href="conditioned_stimuli" title="wikilink">conditioned stimuli</a> present during the trial. This is the feature of the model that represents a major advance over previous models, and allowed a straightforward explanation of important experimental phenomena such as <a href="Blocking_effect" title="wikilink">blocking</a>. For this reason, the Rescorla–Wagner model has become one of the most influential models of learning, though it has been frequently criticized since its publication. It has attracted considerable attention in recent years, as many studies have suggested that the phasic activity of dopamine neurons in mesostriatal DA projections in the midbrain encodes for the type of prediction error detailed in the model.</p>

<p>The Rescorla–Wagner model was created by <a href="Robert_A._Rescorla" title="wikilink">Robert A. Rescorla</a> of the <a href="University_of_Pennsylvania" title="wikilink">University of Pennsylvania</a> and <a href="Allan_R._Wagner" title="wikilink">Allan R. Wagner</a> of <a href="Yale_University" title="wikilink">Yale University</a> in 1972.</p>
<h2 id="success-and-popularity">Success and popularity</h2>

<p>The Rescorla–Wagner model has been successful and popular because:<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<ol>
<li>it can generate clear and ordinal predictions</li>
<li>it has a number of successful predictions</li>
<li>processing event representation by intensity and unexpectedness has an intuitive appeal</li>
<li>it provides considerable heuristic value</li>
<li>it has relatively few free parameters and independent variables</li>
<li>it has had little competition from other theories</li>
</ol>
<h2 id="basic-assumptions-of-the-model">Basic assumptions of the model</h2>
<ol>
<li>The amount of surprise an organism is assumed to experience when encountering an <a href="unconditioned_stimulus" title="wikilink">unconditioned stimulus</a> (US) is assumed to be dependent on the summed associative value of all cues present during that trial. This assumption differs from previous models which considered only the associative value of a particular <a href="conditioned_stimulus" title="wikilink">conditioned stimulus</a> (CS) to be the determining aspect of surprise.</li>
<li><a class="uri" href="Excitation" title="wikilink">Excitation</a> and <a href=":wikt:inhibition" title="wikilink">inhibition</a> are opposite features. One stimulus can only have a positive associative strength (being a conditioned excitor) or a negative associative strength (being a conditioned inhibitor); it cannot have both.</li>
<li>The associative strength of a <a href="Stimulus_(psychology)" title="wikilink">stimulus</a> is expressed directly in the behavior it elicits/inhibits. There is no way of <a class="uri" href="learning" title="wikilink">learning</a> about a stimulus without showing what was learned in the organism's reactions.</li>
<li>The <a class="uri" href="salience" title="wikilink">salience</a> of a CS is a constant. The salience of a CS (alpha) is not supposed to undergo any changes during training and can thus be represented by a constant.</li>
<li>The history of a cue does not have any effects on its current state. It is only the current associative value of a cue which determines the amount of learning. It does not matter whether the CS may have undergone several conditioning-extinction sessions or the like.</li>
</ol>

<p>The first two assumptions are unique to the Rescorla–Wagner model. The last three assumptions were present in antecedents of the model and are less central to the theory but still important to the structure of the model.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h2 id="equation">Equation</h2>

<p>

<math display="block" id="Rescorla–Wagner_model:0">
 <semantics>
  <mrow>
   <mrow>
    <mi mathvariant="normal">Δ</mi>
    <msubsup>
     <mi>V</mi>
     <mi>X</mi>
     <mrow>
      <mi>n</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
    </msubsup>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>α</mi>
     <mi>X</mi>
    </msub>
    <mi>β</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>λ</mi>
      <mo>-</mo>
      <msub>
       <mi>V</mi>
       <mrow>
        <mi>t</mi>
        <mi>o</mi>
        <mi>t</mi>
       </mrow>
      </msub>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>normal-Δ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>V</ci>
       <apply>
        <plus></plus>
        <ci>n</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>X</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>α</ci>
      <ci>X</ci>
     </apply>
     <ci>β</ci>
     <apply>
      <minus></minus>
      <ci>λ</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>V</ci>
       <apply>
        <times></times>
        <ci>t</ci>
        <ci>o</ci>
        <ci>t</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Delta V^{n+1}_{X}=\alpha_{X}\beta(\lambda-V_{tot})
  </annotation>
 </semantics>
</math>

</p>

<p>and</p>

<p>

<math display="block" id="Rescorla–Wagner_model:1">
 <semantics>
  <mrow>
   <msub>
    <mi>V</mi>
    <mrow>
     <mi>t</mi>
     <mi>o</mi>
     <mi>t</mi>
    </mrow>
   </msub>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mi>V</mi>
     <mi>X</mi>
     <mi>n</mi>
    </msubsup>
    <mo>+</mo>
    <mrow>
     <mi mathvariant="normal">Δ</mi>
     <msubsup>
      <mi>V</mi>
      <mi>X</mi>
      <mrow>
       <mi>n</mi>
       <mo>+</mo>
       <mn>1</mn>
      </mrow>
     </msubsup>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>V</ci>
     <apply>
      <times></times>
      <ci>t</ci>
      <ci>o</ci>
      <ci>t</ci>
     </apply>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>V</ci>
       <ci>n</ci>
      </apply>
      <ci>X</ci>
     </apply>
     <apply>
      <times></times>
      <ci>normal-Δ</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>V</ci>
        <apply>
         <plus></plus>
         <ci>n</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>X</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V_{tot}=V^{n}_{X}+\Delta V^{n+1}_{X}
  </annotation>
 </semantics>
</math>

</p>

<p>where</p>
<ul>
<li>

<math display="inline" id="Rescorla–Wagner_model:2">
 <semantics>
  <mrow>
   <mi mathvariant="normal">Δ</mi>
   <msub>
    <mi>V</mi>
    <mi>X</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>normal-Δ</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>V</ci>
     <ci>X</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Delta V_{X}
  </annotation>
 </semantics>
</math>

 is the change in the strength of association of X</li>
<li>

<math display="inline" id="Rescorla–Wagner_model:3">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

 is the salience of the CS (bounded by 0 and 1)</li>
<li>

<math display="inline" id="Rescorla–Wagner_model:4">
 <semantics>
  <mi>β</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>β</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \beta
  </annotation>
 </semantics>
</math>

 is the rate parameter for the US (bounded by 0 and 1), sometimes called its <a href="association_value" title="wikilink">association value</a></li>
<li>

<math display="inline" id="Rescorla–Wagner_model:5">
 <semantics>
  <mi>λ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>λ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda
  </annotation>
 </semantics>
</math>

 is the maximum conditioning possible for the US</li>
<li>

<math display="inline" id="Rescorla–Wagner_model:6">
 <semantics>
  <msub>
   <mi>V</mi>
   <mi>X</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>V</ci>
    <ci>X</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V_{X}
  </annotation>
 </semantics>
</math>

 is the current associative strength</li>
<li>

<math display="inline" id="Rescorla–Wagner_model:7">
 <semantics>
  <msub>
   <mi>V</mi>
   <mrow>
    <mi>t</mi>
    <mi>o</mi>
    <mi>t</mi>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>V</ci>
    <apply>
     <times></times>
     <ci>t</ci>
     <ci>o</ci>
     <ci>t</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V_{tot}
  </annotation>
 </semantics>
</math>

 is the total associative strength of all CS</li>
</ul>
<h2 id="the-revised-rw-model-by-van-hamme-and-wasserman-1994">The revised RW model by Van Hamme and Wasserman (1994)</h2>

<p>Van Hamme and Wasserman have extended the original Rescorla–Wagner (RW) model and introduced a new factor in their revised RW model in 1994:<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> They suggested that not only conditioned stimuli physically present on a given trial can undergo changes in their associative strength, the associative value of a CS can also be altered by a within-compound-association with a CS present on that trial. A within-compound-association is established if two CSs are presented together during training (compound stimulus). If one of the two component CSs is subsequently presented alone, then it is assumed to activate a representation of the other (previously paired) CS as well. Van Hamme and Wasserman propose that stimuli indirectly activated through within-compound-associations have a negative learning parameter—thus phenomena of retrospective reevaluation can be explained.</p>

<p>Let's consider the following example, an experimental paradigm called "backward blocking," indicative of retrospective revaluation, where AB is the compound stimulus A+B:</p>
<ul>
<li>Phase 1: AB–US</li>
<li>Phase 2: A–US</li>
</ul>

<p>Test trials: Group 1, which received both Phase 1- and 2-trials, elicits a weaker conditioned response (CR) to B compared to the Control group, which only received Phase 1-trials.</p>

<p>The original RW model cannot account for this effect. But the revised model can: In Phase 2, stimulus B is indirectly activated through within-compound-association with A. But instead of a positive learning parameter (usually called alpha) when physically present, during Phase 2, B has a negative learning parameter. Thus during the second phase, B's associative strength declines whereas A's value increases because of its positive learning parameter.</p>

<p>Thus, the revised RW model can explain why the CR elicited by B after backward blocking training is weaker compared with AB-only conditioning.</p>
<h2 id="some-failures-of-the-rw-model">Some failures of the RW model</h2>
<dl>
<dt>Spontaneous recovery from <a class="uri" href="extinction" title="wikilink">extinction</a> and recovery from extinction caused by reminder treatments (reinstatement)</dt>
<dd>It is a well-established observation that a time-out interval after completion of extinction results in partial recovery from extinction, i.e., the previously extinguished reaction or response recurs—but usually at a lower level than before extinction training. Reinstatement refers to the phenomenon that exposure to the US from training alone after completion of extinction results in partial recovery from extinction. The RW model can't account for those phenomena.
</dd>
</dl>
<dl>
<dt>Extinction of a previously conditioned inhibitor</dt>
<dd>The RW model predicts that repeated presentation of a conditioned inhibitor alone (a CS with negative associative strength) results in extinction of this stimulus (a decline of its negative associative value). This is a false prediction. Contrarily, experiments show the repeated presentation of a conditioned inhibitor alone even increases its inhibitory potential.
</dd>
</dl>
<dl>
<dt>Facilitated <a class="uri" href="reacquisition" title="wikilink">reacquisition</a> after extinction</dt>
<dd>One of the assumptions of the model is that the history of conditioning of a CS does not have any influences on its present status—only its current associative value is important. Contrary to this assumption, many experiments show that stimuli that were first conditioned and then extinguished are more easily reconditioned (i.e., fewer trials are necessary for conditioning).
</dd>
</dl>
<dl>
<dt>The exclusiveness of excitation and inhibition</dt>
<dd>The RW model also assumes that <a class="uri" href="excitation" title="wikilink">excitation</a> and <a href=":wikt:inhibition" title="wikilink">inhibition</a> are opponent features. A stimulus can either have excitatory potential (a positive associative strength) or inhibitory potential (a negative associative strength), but not both. By contrast it is sometimes observed, that stimuli can have both qualities. One example is <a href="backward_conditioning" title="wikilink">backward excitatory conditioning</a> in which a CS is backwardly paired with a US (US–CS instead of CS–US). This usually makes the CS become a conditioned excitor. But interestingly, the stimulus also has inhibitory features which can be proven by the retardation of acquisition test. This test is used to assess the inhibitory potential of a stimulus since it is observed that excitatory conditioning with a previously conditioned inhibitor is retarded. The backwardly conditioned stimulus passes this test and thus seems to have both excitatory and inhibitory features.
</dd>
</dl>
<dl>
<dt>Pairing a novel stimulus with a conditioned inhibitor</dt>
<dd>A conditioned inhibitor is assumed to have a negative associative value. By presenting an inhibitor with a novel stimulus (i.e., its associative strength is zero), the model predicts that the novel cue should become a conditioned excitor. This is not the case in experimental situations. The predictions of the model stem from its basic term (lambda-V). Since the summed associative strength of all stimuli (V) present on the trial is negative (zero + inhibitory potential) and lambda is zero (no US present), the resulting change in the associative strength is positive, thus making the novel cue a conditioned excitor.
</dd>
</dl>
<dl>
<dt>CS-preexposure effect</dt>
<dd>The CS-<a class="uri" href="preexposure" title="wikilink">preexposure</a> effect (also called <a href="latent_inhibition" title="wikilink">latent inhibition</a>) is the well-established observation that conditioning after exposure to the stimulus later used as the CS in conditioning is retarded. The RW model doesn't predict any effect of presenting a novel stimulus without a US.
</dd>
</dl>
<dl>
<dt>Higher-order conditioning</dt>
<dd>In <a href="higher-order_conditioning" title="wikilink">higher-order conditioning</a> a previously conditioned CS is paired with a novel cue (i.e., first CS1–US then CS2–CS1). This usually makes the novel cue CS2 elicit similar reactions to the CS1. The model cannot account for this phenomenon since during CS2–CS1 trials, no US is present. But by allowing CS1 to act similarly to a US, one can reconcile the model with this effect.
</dd>
</dl>
<dl>
<dt>Sensory preconditioning</dt>
<dd><a href="Sensory_preconditioning" title="wikilink">Sensory preconditioning</a> refers to first pairing two novel cues (CS1–CS2) and then pairing one of them with a US (CS2–US). This turns both CS1 and CS2 into conditioned excitors. The RW model cannot explain this, since during the CS1–CS2-phase both stimuli have an associative value of zero and lambda is also zero (no US present) which results in no change in the associative strength of the stimuli.
</dd>
</dl>
<h2 id="references">References</h2>
<ul>
<li>Rescorla, R.A. &amp; Wagner, A.R. (1972) <a href="http://www.ualberta.ca/~egray/teaching/Rescorla%20&amp;%20Wagner%201972.pdf"><em>A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</em></a>, Classical Conditioning II, A.H. Black &amp; W.F. Prokasy, Eds., pp. 64–99. Appleton-Century-Crofts.</li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.scholarpedia.org/article/Rescorla-Wagner_Model">Scholarpedia Rescorla–Wagner model</a></li>
</ul>

<p>"</p>

<p><a href="Category:Behavioral_concepts" title="wikilink">Category:Behavioral concepts</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
</ol>
</section>
</body>
</html>
