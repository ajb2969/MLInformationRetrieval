<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="706">Power iteration</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Power iteration</h1>
<style>
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
<style>
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</body></html>
<body>
<hr/>

<p>In <a class="uri" href="mathematics" title="wikilink">mathematics</a>, the <strong>power iteration</strong> is an <a href="eigenvalue_algorithm" title="wikilink">eigenvalue algorithm</a>: given a <a href="matrix_(mathematics)" title="wikilink">matrix</a> <em>A</em>, the algorithm will produce a number λ (the <a class="uri" href="eigenvalue" title="wikilink">eigenvalue</a>) and a nonzero vector <em>v</em> (the eigenvector), such that <em>Av</em> = λ<em>v</em>. The algorithm is also known as the Von Mises iteration.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>

<p>The power iteration is a very simple algorithm. It does not compute a <a href="matrix_decomposition" title="wikilink">matrix decomposition</a>, and hence it can be used when <em>A</em> is a very large <a href="sparse_matrix" title="wikilink">sparse matrix</a>. However, it will find only one eigenvalue (the one with the greatest <a href="absolute_value" title="wikilink">absolute value</a>) and it may converge only slowly.</p>
<h2 id="the-method">The method</h2>

<p>The power iteration algorithm starts with a vector <em>b</em><sub>0</sub>, which may be an approximation to the dominant eigenvector or a random vector. The method is described by the iteration</p>

<p>

<math display="block" id="Power_iteration:0">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>b</mi>
     <mrow>
      <mi>k</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo>=</mo>
    <mfrac>
     <mrow>
      <mi>A</mi>
      <msub>
       <mi>b</mi>
       <mi>k</mi>
      </msub>
     </mrow>
     <mrow>
      <mo>∥</mo>
      <mrow>
       <mi>A</mi>
       <msub>
        <mi>b</mi>
        <mi>k</mi>
       </msub>
      </mrow>
      <mo>∥</mo>
     </mrow>
    </mfrac>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>b</ci>
     <apply>
      <plus></plus>
      <ci>k</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <ci>A</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>b</ci>
       <ci>k</ci>
      </apply>
     </apply>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <apply>
       <times></times>
       <ci>A</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>b</ci>
        <ci>k</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b_{k+1}=\frac{Ab_{k}}{\|Ab_{k}\|}.
  </annotation>
 </semantics>
</math>

 So, at every iteration, the vector <em>b</em><sub><em>k</em></sub> is multiplied by the matrix <em>A</em> and normalized.</p>

<p>Under the assumptions:</p>
<ul>
<li><em>A</em> has an eigenvalue that is strictly greater in magnitude than its other eigenvalues</li>
<li>The starting vector 

<math display="inline" id="Power_iteration:1">
 <semantics>
  <msub>
   <mi>b</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>b</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b_{0}
  </annotation>
 </semantics>
</math>

 has a nonzero component in the direction of an eigenvector associated with the dominant eigenvalue.</li>
</ul>

<p>then:</p>
<ul>
<li>A subsequence of 

<math display="inline" id="Power_iteration:2">
 <semantics>
  <mrow>
   <mo>(</mo>
   <msub>
    <mi>b</mi>
    <mi>k</mi>
   </msub>
   <mo>)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>b</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \left(b_{k}\right)
  </annotation>
 </semantics>
</math>

 converges to an eigenvector associated with the dominant eigenvalue</li>
</ul>

<p>Note that the sequence 

<math display="inline" id="Power_iteration:3">
 <semantics>
  <mrow>
   <mo>(</mo>
   <msub>
    <mi>b</mi>
    <mi>k</mi>
   </msub>
   <mo>)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>b</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \left(b_{k}\right)
  </annotation>
 </semantics>
</math>


 does not necessarily converge. It can be shown that:<br/>


<math display="inline" id="Power_iteration:4">
 <semantics>
  <mrow>
   <msub>
    <mi>b</mi>
    <mi>k</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <mrow>
     <msup>
      <mi>e</mi>
      <mrow>
       <mi>i</mi>
       <msub>
        <mi>ϕ</mi>
        <mi>k</mi>
       </msub>
      </mrow>
     </msup>
     <msub>
      <mi>v</mi>
      <mn>1</mn>
     </msub>
    </mrow>
    <mo>+</mo>
    <msub>
     <mi>r</mi>
     <mi>k</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>b</ci>
     <ci>k</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>e</ci>
       <apply>
        <times></times>
        <ci>i</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>ϕ</ci>
         <ci>k</ci>
        </apply>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>v</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>r</ci>
      <ci>k</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   b_{k}=e^{i\phi_{k}}v_{1}+r_{k}
  </annotation>
 </semantics>
</math>

 where

<math display="block" id="Power_iteration:5">
 <semantics>
  <msub>
   <mi>v</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>v</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   v_{1}
  </annotation>
 </semantics>
</math>

 is an eigenvector associated with the dominant eigenvalue, and 

<math display="inline" id="Power_iteration:6">
 <semantics>
  <mrow>
   <mrow>
    <mo>∥</mo>
    <msub>
     <mi>r</mi>
     <mi>k</mi>
    </msub>
    <mo>∥</mo>
   </mrow>
   <mo>→</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>r</ci>
      <ci>k</ci>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|r_{k}\|\rightarrow 0
  </annotation>
 </semantics>
</math>

. The presence of the term 

<math display="inline" id="Power_iteration:7">
 <semantics>
  <msup>
   <mi>e</mi>
   <mrow>
    <mi>i</mi>
    <msub>
     <mi>ϕ</mi>
     <mi>k</mi>
    </msub>
   </mrow>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>e</ci>
    <apply>
     <times></times>
     <ci>i</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>ϕ</ci>
      <ci>k</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   e^{i\phi_{k}}
  </annotation>
 </semantics>
</math>

 implies that 

<math display="inline" id="Power_iteration:8">
 <semantics>
  <mrow>
   <mo>(</mo>
   <msub>
    <mi>b</mi>
    <mi>k</mi>
   </msub>
   <mo>)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>b</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \left(b_{k}\right)
  </annotation>
 </semantics>
</math>


 does not converge unless 

<math display="inline" id="Power_iteration:9">
 <semantics>
  <mrow>
   <msup>
    <mi>e</mi>
    <mrow>
     <mi>i</mi>
     <msub>
      <mi>ϕ</mi>
      <mi>k</mi>
     </msub>
    </mrow>
   </msup>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>e</ci>
     <apply>
      <times></times>
      <ci>i</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ϕ</ci>
       <ci>k</ci>
      </apply>
     </apply>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   e^{i\phi_{k}}=1
  </annotation>
 </semantics>
</math>

 Under the two assumptions listed above, the sequence 

<math display="inline" id="Power_iteration:10">
 <semantics>
  <mrow>
   <mo>(</mo>
   <msub>
    <mi>μ</mi>
    <mi>k</mi>
   </msub>
   <mo>)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>μ</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \left(\mu_{k}\right)
  </annotation>
 </semantics>
</math>

 defined by: 

<math display="inline" id="Power_iteration:11">
 <semantics>
  <mrow>
   <msub>
    <mi>μ</mi>
    <mi>k</mi>
   </msub>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <msubsup>
      <mi>b</mi>
      <mi>k</mi>
      <mo>*</mo>
     </msubsup>
     <mi>A</mi>
     <msub>
      <mi>b</mi>
      <mi>k</mi>
     </msub>
    </mrow>
    <mrow>
     <msubsup>
      <mi>b</mi>
      <mi>k</mi>
      <mo>*</mo>
     </msubsup>
     <msub>
      <mi>b</mi>
      <mi>k</mi>
     </msub>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>μ</ci>
     <ci>k</ci>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>b</ci>
        <ci>k</ci>
       </apply>
       <times></times>
      </apply>
      <ci>A</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>b</ci>
       <ci>k</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>b</ci>
        <ci>k</ci>
       </apply>
       <times></times>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>b</ci>
       <ci>k</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu_{k}=\frac{b_{k}^{*}Ab_{k}}{b_{k}^{*}b_{k}}
  </annotation>
 </semantics>
</math>

 converges to the dominant eigenvalue.</p>

<p>This can be run as a simulation program with the following simple algorithm:</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">for</span> each(<span class="st">''</span>simulation<span class="st">''</span>) {
    <span class="co">// calculate the matrix-by-vector product Ab</span>
    <span class="kw">for</span>(i=<span class="dv">0</span>; i<n; (j="&lt;span" class="st">"0;"</n;></code></pre></div></body> (k=<span class="st">"0;"</span> *=<span class="st">""</span> +=<span class="st">"tmp[k]*tmp[k];"</span> &lt;=<span class="st">""</span> a=<span class="st">""</span> b=<span class="st">"tmp/norm;"</span> b[j];=<span class="st">""</span> calculate=<span class="st">""</span> column=<span class="st">""</span> dot=<span class="st">""</span> <span class="kw">for</span>=<span class="st">""</span> i++)=<span class="st">""</span> i-th=<span class="st">""</span> in=<span class="st">""</span> iteration=<span class="st">""</span> j++)=<span class="st">""</span> j<n;=<span class="st">""</n;=<span> k++)=<span class="st">""</span> k<n;=<span class="st">""</n;=<span> length=<span class="st">""</span> next=<span class="st">""</span> norm=<span class="st">"sqrt(norm_sq);"</span> norm_sq=<span class="st">""</span> normalize=<span class="st">""</span> of=<span class="st">""</span> product=<span class="st">""</span> resultant=<span class="st">""</span> row=<span class="st">""</span> source=<span class="st">""</span> the=<span class="st">""</span> tmp[i]=<span class="st">""</span> to=<span class="st">""</span> unit=<span class="st">""</span> vector=<span class="st">""</span> with=<span class="st">""</span> {=<span class="st">""</span> }=<span class="st">""</span>&gt;
The value of <span class="st">''</span>norm<span class="st">''</span> converges to the absolute value of the dominant eigenvalue, <span class="kw">and</span> the vector <span class="st">''</span>b<span class="st">''</span> to an associated eigenvector.

<span class="st">''</span>Note:<span class="st">''</span> The above code assumes real A,b. To handle complex; A[i][j] becomes conj(A[i][j]), <span class="kw">and</span> tmp[k]*tmp[k] becomes conj(tmp[k])*tmp[k]

This algorithm is the one used to calculate such things as the Google [[<span class="kw">PageRank</span>]].

The method can also be used to calculate the [[<span class="kw">spectral</span> <span class="kw">radius</span>]] of a matrix by computing the [[<span class="kw">Rayleigh</span> <span class="kw">quotient</span>]]
:<math> \frac{b_k^\top A b_k}{b_k^\top b_k} = \frac{b_{k<span class="dv">+1</span>}^\top b_k}{b_k^\top b_k}. </math>

==Analysis==
Let <math>A</math> be decomposed into its [[<span class="kw">Jordan</span> <span class="kw">canonical</span> <span class="kw">form</span>]]:<math>A=VJV^{<span class="dv">-1</span>}</math>,
where the first column of <math>V</math> is an eigenvector of <math>A</math> corresponding to the dominant eigenvalue <math>\lambda_{<span class="dv">1</span>}</math>.
Since the dominant eigenvalue of <math>A</math> is unique,
the first Jordan block of <math>J</math> is the <math><span class="dv">1</span> \times <span class="dv">1</span></math> matrix
<math>\begin{bmatrix} \lambda_{<span class="dv">1</span>} \end{bmatrix} </math>, where
<math>\lambda_{<span class="dv">1</span>}</math> is the largest eigenvalue of <span class="st">''</span>A<span class="st">''</span> in magnitude.
The starting vector <math>b_{<span class="dv">0</span>}</math>
can be written as a linear combination of the columns of V:
<math>b_{<span class="dv">0</span>} = c_{<span class="dv">1</span>}v_{<span class="dv">1</span>} + c_{<span class="dv">2</span>}v_{<span class="dv">2</span>} + \cdots + c_{n}v_{n}</math>.
By assumption, <math>b_{<span class="dv">0</span>}</math> has a nonzero component in the direction of the dominant eigenvalue, so
<math>c_{<span class="dv">1</span>} \ne <span class="dv">0</span></math>.

The computationally useful [[<span class="kw">recurrence</span> <span class="kw">relation</span>]] <span class="kw">for</span> <math>b_{k<span class="dv">+1</span>}</math>
can be rewritten as:
<math>b_{k<span class="dv">+1</span>}=\frac{Ab_{k}}{\|Ab_{k}\|}=\frac{A^{k<span class="dv">+1</span>}b_{<span class="dv">0</span>}}{\|A^{k<span class="dv">+1</span>}b_{<span class="dv">0</span>}\|}</math>,
where the expression:<math>\frac{A^{k<span class="dv">+1</span>}b_{<span class="dv">0</span>}}{\|A^{k<span class="dv">+1</span>}b_{<span class="dv">0</span>}\|}</math> is more amenable to the following analysis.
<br>
<math>\displaystyle
\begin{array}{lcl}
b_{k} &amp;=&amp; \frac{A^{k}b_{<span class="dv">0</span>}}{\| A^{k} b_{<span class="dv">0</span>} \|} \\
      &amp;=&amp; \frac{\left( VJV^{<span class="dv">-1</span>} \right)^{k} b_{<span class="dv">0</span>}}{\|\left( VJV^{<span class="dv">-1</span>} \right)^{k}b_{<span class="dv">0</span>}\|} \\
      &amp;=&amp; \frac{ VJ^{k}V^{<span class="dv">-1</span>} b_{<span class="dv">0</span>}}{\| V J^{k} V^{<span class="dv">-1</span>} b_{<span class="dv">0</span>}\|} \\
      &amp;=&amp; \frac{ VJ^{k}V^{<span class="dv">-1</span>} \left( c_{<span class="dv">1</span>}v_{<span class="dv">1</span>} + c_{<span class="dv">2</span>}v_{<span class="dv">2</span>} + \cdots + c_{n}v_{n} \right)}
               {\| V J^{k} V^{<span class="dv">-1</span>} \left( c_{<span class="dv">1</span>}v_{<span class="dv">1</span>} + c_{<span class="dv">2</span>}v_{<span class="dv">2</span>} + \cdots + c_{n}v_{n} \right)\|} \\
      &amp;=&amp; \frac{ VJ^{k}\left( c_{<span class="dv">1</span>}e_{<span class="dv">1</span>} + c_{<span class="dv">2</span>}e_{<span class="dv">2</span>} + \cdots + c_{n}e_{n} \right)}
                {\| V J^{k} \left( c_{<span class="dv">1</span>}e_{<span class="dv">1</span>} + c_{<span class="dv">2</span>}e_{<span class="dv">2</span>} + \cdots + c_{n}e_{n} \right) \|} \\
      &amp;=&amp; \left( \frac{\lambda_{<span class="dv">1</span>}}{|\lambda_{<span class="dv">1</span>}|} \right)^{k} \frac{c_{<span class="dv">1</span>}}{|c_{<span class="dv">1</span>}|}
          \frac{ v_{<span class="dv">1</span>} + \frac{<span class="dv">1</span>}{c_{<span class="dv">1</span>}} V \left( \frac{<span class="dv">1</span>}{\lambda_1} J \right)^{k} 
                      \left( c_{<span class="dv">2</span>}e_{<span class="dv">2</span>} +  \cdots + c_{n}e_{n} \right)}
               {\| v_{<span class="dv">1</span>} + \frac{<span class="dv">1</span>}{c_{<span class="dv">1</span>}} V \left( \frac{<span class="dv">1</span>}{\lambda_1} J \right)^{k} 
                      \left( c_{<span class="dv">2</span>}e_{<span class="dv">2</span>} +  \cdots + c_{n}e_{n} \right) \| }
           
\end{array}
</math>
<br>
The expression above simplifies as <math>k \rightarrow \infty </math>
<br>
<math>
\left( \frac{<span class="dv">1</span>}{\lambda_{<span class="dv">1</span>}} J \right)^{k} = 
\begin{bmatrix}
[<span class="dv">1</span>] &amp; &amp; &amp; &amp; \\
&amp; \left( \frac{<span class="dv">1</span>}{\lambda_{<span class="dv">1</span>}} J_{<span class="dv">2</span>} \right)^{k}&amp; &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; \left( \frac{<span class="dv">1</span>}{\lambda_{<span class="dv">1</span>}} J_{m} \right)^{k} \\
\end{bmatrix}
\rightarrow
\begin{bmatrix}
<span class="dv">1</span> &amp; &amp; &amp; &amp; \\
&amp; <span class="dv">0</span> &amp; &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; <span class="dv">0</span> \\
\end{bmatrix}
</math>
as
<math> k \rightarrow \infty </math>.
<br>
The limit follows from the fact that the eigenvalue of
<math> \frac{<span class="dv">1</span>}{\lambda_{<span class="dv">1</span>}} J_{i} </math>
is less than <span class="dv">1</span> in magnitude, so
<math>
\left( \frac{<span class="dv">1</span>}{\lambda_{<span class="dv">1</span>}} J_{i} \right)^{k} \rightarrow <span class="dv">0</span>
</math>
as
<math> k \rightarrow \infty </math>
<br>
It follows that:
<br>
<math>
\frac{<span class="dv">1</span>}{c_{<span class="dv">1</span>}} V \left( \frac{<span class="dv">1</span>}{\lambda_1} J \right)^{k} 
\left( c_{<span class="dv">2</span>}e_{<span class="dv">2</span>} +  \cdots + c_{n}e_{n} \right)
\rightarrow <span class="dv">0</span>
</math>
as
<math>
k \rightarrow \infty
</math>
<br>
Using <span class="kw">this</span> fact,
<math>b_{k}</math>
can be written in a form that emphasizes its relationship with <math>v_{<span class="dv">1</span>}</math> when k is large:
<br>
<math>
\begin{matrix}
b_{k} &amp;=&amp; \left( \frac{\lambda_{<span class="dv">1</span>}}{|\lambda_{<span class="dv">1</span>}|} \right)^{k} \frac{c_{<span class="dv">1</span>}}{|c_{<span class="dv">1</span>}|}
          \frac{ v_{<span class="dv">1</span>} + \frac{<span class="dv">1</span>}{c_{<span class="dv">1</span>}} V \left( \frac{<span class="dv">1</span>}{\lambda_1} J \right)^{k} 
                      \left( c_{<span class="dv">2</span>}e_{<span class="dv">2</span>} +  \cdots + c_{n}e_{n} \right)}
               {\| v_{<span class="dv">1</span>} + \frac{<span class="dv">1</span>}{c_{<span class="dv">1</span>}} V \left( \frac{<span class="dv">1</span>}{\lambda_1} J \right)^{k} 
                      \left( c_{<span class="dv">2</span>}e_{<span class="dv">2</span>} +  \cdots + c_{n}e_{n} \right) \| }
      &amp;=&amp; e^{i \phi_{k}} \frac{c_{<span class="dv">1</span>}}{|c_{<span class="dv">1</span>}|} v_{<span class="dv">1</span>} + r_{k}
\end{matrix}
</math>
where <math> e^{i \phi_{k}} = \left( \lambda_{<span class="dv">1</span>} / |\lambda_{<span class="dv">1</span>}| \right)^{k} </math>
<span class="kw">and</span>
<math> \| r_{k} \| \rightarrow <span class="dv">0</span> </math>
as
<math>k \rightarrow \infty </math>
<br>
The sequence
<math> \left( b_{k} \right)</math> is bounded, so it contains a convergent subsequence. Note that the eigenvector corresponding to the dominant eigenvalue is only unique up to a scalar, so although the sequence <math>\left(b_{k}\right)</math> may <span class="kw">not</span> converge,
<math>b_{k}</math> is nearly an eigenvector of <span class="st">''</span>A<span class="st">''</span> <span class="kw">for</span> large k.

Alternatively, <span class="kw">if</span> <span class="st">''</span>A<span class="st">''</span> is [[<span class="kw">diagonalizable</span>]], then the following proof yields the same result
<br>
Let λ<sub><span class="dv">1</span></sub>, λ<sub><span class="dv">2</span></sub>, …, λ<sub><span class="st">''</span>m<span class="st">''</span></sub> be the <var>m</var> eigenvalues (counted with multiplicity) of <var>A</var> <span class="kw">and</span> let <span class="st">''</span>v<span class="st">''</span><sub><span class="dv">1</span></sub>, <span class="st">''</span>v<span class="st">''</span><sub><span class="dv">2</span></sub>, …, <span class="st">''</span>v<span class="st">''</span><sub><span class="st">''</span>m<span class="st">''</span></sub> be the corresponding eigenvectors.  Suppose that <math>\lambda_1</math> is the dominant eigenvalue, so that <math>|\lambda_1| &gt; |\lambda_j|</math> <span class="kw">for</span> <math>j&gt;<span class="dv">1</span></math>.

The initial vector <math>b_0</math> can be written:
:<math>b_0 = c_{<span class="dv">1</span>}v_{<span class="dv">1</span>} + c_{<span class="dv">2</span>}v_{<span class="dv">2</span>} + \cdots + c_{m}v_{m}.</math>
If <math>b_0</math> is chosen randomly (with uniform probability), then <span class="st">''</span>c<span class="st">''</span><sub><span class="dv">1</span></sub> ≠ <span class="dv">0</span> with [[<span class="kw">Almost</span> <span class="kw">surely</span>|<span class="kw">probability</span> <span class="kw">1</span>]].  Now,
:<math>\begin{array}{lcl}A^{k}b_0 &amp; = &amp; c_{<span class="dv">1</span>}A^{k}v_{<span class="dv">1</span>} + c_{<span class="dv">2</span>}A^{k}v_{<span class="dv">2</span>} + \cdots + c_{m}A^{k}v_{m} \\
&amp; = &amp; c_{<span class="dv">1</span>}\lambda_{<span class="dv">1</span>}^{k}v_{<span class="dv">1</span>} + c_{<span class="dv">2</span>}\lambda_{<span class="dv">2</span>}^{k}v_{<span class="dv">2</span>} + \cdots + c_{m}\lambda_{m}^{k}v_{m} \\
&amp; = &amp; c_{<span class="dv">1</span>}\lambda_{<span class="dv">1</span>}^{k} \left( v_{<span class="dv">1</span>} + \frac{c_{<span class="dv">2</span>}}{c_{<span class="dv">1</span>}}\left(\frac{\lambda_{<span class="dv">2</span>}}{\lambda_{<span class="dv">1</span>}}\right)^{k}v_{<span class="dv">2</span>} + \cdots + \frac{c_{m}}{c_{<span class="dv">1</span>}}\left(\frac{\lambda_{m}}{\lambda_{<span class="dv">1</span>}}\right)^{k}v_{m}\right). \end{array}</math>

The expression within parentheses converges to <math>v_1</math> because <math>|\lambda_j/\lambda_1| &lt; <span class="dv">1</span></math> <span class="kw">for</span> <math>j&gt;<span class="dv">1</span></math>. On the other hand, we have
:<math> b_k = \frac{A^kb_0}{\|A^kb_0\|}. </math>
Therefore, <math>b_k</math> converges to (a multiple of) the eigenvector <math>v_1</math>. The convergence is [[<span class="kw">geometric</span> <span class="kw">sequence</span>|<span class="kw">geometric</span>]], with ratio
:<math> \left| \frac{\lambda_2}{\lambda_1} \right|, </math>
where <math>\lambda_2</math> denotes the second dominant eigenvalue. Thus, the method converges slowly <span class="kw">if</span> there is an eigenvalue close in magnitude to the dominant eigenvalue.

==Applications==
Although the power iteration method approximates only one eigenvalue of a matrix, it remains useful <span class="kw">for</span> certain [[<span class="kw">computational</span> <span class="kw">problem</span>]]s. For instance, [[<span class="kw">Google</span>]] uses it to calculate the [[<span class="kw">PageRank</span>]] of documents in their search engine,<ref>{{cite news|author=Ipsen, Ilse, <span class="kw">and</span> Rebecca M. Wills|url=http:<span class="co">//www4.ncsu.edu/~ipsen/ps/slides_imacs.pdf|title=7th IMACS International Symposium on Iterative Methods in Scientific Computing|location=Fields Institute, Toronto, Canada|date=5–8 May 2005}}</span></ref> and [[Twitter]] uses it to show users recommendations of who to follow.<ref name="twitterwtf">Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Bosagh Zadeh [http://dl.acm.org/citation.cfm?id=2488433 WTF: The who-to-follow system at Twitter], Proceedings of the 22nd international conference on World Wide Web</ref> For matrices that are well-conditioned and as sparse as the Web matrix, the power iteration method can be more efficient than other methods of finding the dominant eigenvector.</br></br></br></br></br></br></br></br></br></br>

Some of the more advanced eigenvalue algorithms can be understood as variations of the power iteration. For instance, the [[<span class="kw">inverse</span> <span class="kw">iteration</span>]] method applies power iteration to the matrix <math>A^{<span class="dv">-1</span>}</math>. Other algorithms look at the whole subspace generated by the vectors <math>b_k</math>. This subspace is known as the [[<span class="kw">Krylov</span> <span class="kw">subspace</span>]]. It can be computed by [[<span class="kw">Arnoldi</span> <span class="kw">iteration</span>]] <span class="kw">or</span> [[<span class="kw">Lanczos</span> <span class="kw">iteration</span>]].
Another variation of the power method that simultaneously gives n eigenvalues <span class="kw">and</span> eigenfunctions,
as well as accelerated convergence as <math> \left| \lambda_{n<span class="dv">+1</span>} / \lambda_1\right|, </math> is
<span class="st">"Multiple extremal eigenpairs by the power method"</span>
in the Journal of Computational Physics
Volume <span class="dv">227</span> Issue <span class="dv">19</span>, October, <span class="dv">2008</span>,
Pages <span class="dv">8508-8522</span> (Also see pdf below <span class="kw">for</span> Los Alamos National Laboratory report LA-UR<span class="bn">-07</span><span class="dv">-4046</span>)

==See also==
* [[<span class="kw">Rayleigh</span> <span class="kw">quotient</span> <span class="kw">iteration</span>]]
* [[<span class="kw">Inverse</span> <span class="kw">iteration</span>]]

==References==
{{Reflist}}

==External links==
* [http:<span class="co">//www.math.buffalo.edu/~pitman/courses/mth437/na2/node17.html Power method], part of lecture notes on numerical linear algebra by E. Bruce Pitman, State University of New York.</span>
* [http:<span class="co">//math.fullerton.edu/mathews/n2003/PowerMethodMod.html Module for the Power Method]</span>
* [http:<span class="co">//arxiv.org/pdf/0807.1261.pdf] Los Alamos report LA-UR-07-4046 ""Multiple extremal eigenpairs by the power method"</span>

{{Numerical linear algebra}}
{{Use dmy dates|date=September <span class="dv">2010</span>}}
{{DEFAULTSORT:Power Iteration}}
[[<span class="kw">Category:Numerical</span> <span class="kw">linear</span> <span class="kw">algebra</span>]]<span class="st">"</span>

<p></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="Richard_von_Mises" title="wikilink">Richard von Mises</a> and H. Pollaczek-Geiringer, <em>Praktische Verfahren der Gleichungsauflösung</em>, ZAMM - Zeitschrift für Angewandte Mathematik und Mechanik 9, 152-164 (1929).<a href="#fnref1">↩</a></li>
</ol>
</section>


