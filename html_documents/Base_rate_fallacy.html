<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="504">Base rate fallacy</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Base rate fallacy</h1>
<hr/>

<p><strong>Base rate fallacy</strong>, also called <strong>base rate neglect</strong> or <strong>base rate bias</strong>, is a <a href="formal_fallacy" title="wikilink">formal fallacy</a>. If presented with related <a href="base_rate" title="wikilink">base rate</a> information (i.e. generic, general information) and specific information (information only pertaining to a certain case), the mind tends to ignore the former and focus on the latter. <a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="example-1">Example 1</h2>
<dl>
<dd>John is a man who wears <a href="Gothic_fashion" title="wikilink">gothic</a> inspired clothing, has long black hair, and listens to <a href="death_metal" title="wikilink">death metal</a>. How likely is it that he is a <a class="uri" href="Christian" title="wikilink">Christian</a> and how likely is it that he is a <a class="uri" href="Satanist" title="wikilink">Satanist</a>?
</dd>
</dl>

<p>If people were asked this question, they would likely underestimate the probability of him being a Christian, and overestimate the probability of him being a Satanist. This is because they would ignore that the base rate of being a Christian (there are about 2 billion in the world) is vastly higher than that of being a Satanist (estimated to be in the thousands).<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Therefore, even if such clothing choices indicated an order of magnitude jump in probability of being a Satanist, the probability of being a Christian is still much larger.</p>
<h2 id="example-2">Example 2</h2>
<dl>
<dd>A group of policemen have <a href="breathalyzer" title="wikilink">breathalyzers</a> displaying false drunkenness in 5% of the cases in which the driver is sober. However, the breathalyzers never fail to detect a truly drunk person. One in a thousand drivers are driving drunk. Suppose the policemen then stop a driver at random, and force the driver to take a breathalyzer test. It indicates that the driver is drunk. We assume you don't know anything else about him or her. How high is the probability he or she really is drunk?
</dd>
</dl>

<p>Many would answer as high as 0.95, but the correct probability is about 0.02.</p>

<p>To find the correct answer, one should use <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a>. The goal is to find the probability that the driver is drunk given that the breathalyzer indicated he/she is drunk, which can be represented as</p>

<p>

<math display="block" id="Base_rate_fallacy:0">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>d</mi>
    <mi>r</mi>
    <mi>u</mi>
    <mi>n</mi>
    <mi>k</mi>
    <mo stretchy="false">|</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">d</csymbol>
     <csymbol cd="unknown">r</csymbol>
     <csymbol cd="unknown">u</csymbol>
     <csymbol cd="unknown">n</csymbol>
     <csymbol cd="unknown">k</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(drunk|D)
  </annotation>
 </semantics>
</math>

 where "D" means that the breathalyzer indicates that the driver is drunk. Bayes' Theorem tells us that</p>

<p>

<math display="block" id="Base_rate_fallacy:1">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>d</mi>
    <mi>r</mi>
    <mi>u</mi>
    <mi>n</mi>
    <mi>k</mi>
    <mo stretchy="false">|</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>p</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>D</mi>
      <mo stretchy="false">|</mo>
      <mi>d</mi>
      <mi>r</mi>
      <mi>u</mi>
      <mi>n</mi>
      <mi>k</mi>
      <mo rspace="4.2pt" stretchy="false">)</mo>
     </mrow>
     <mi>p</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>d</mi>
      <mi>r</mi>
      <mi>u</mi>
      <mi>n</mi>
      <mi>k</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mi>p</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>D</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">d</csymbol>
     <csymbol cd="unknown">r</csymbol>
     <csymbol cd="unknown">u</csymbol>
     <csymbol cd="unknown">n</csymbol>
     <csymbol cd="unknown">k</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <cerror>
      <csymbol cd="ambiguous">fragments</csymbol>
      <csymbol cd="unknown">p</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">D</csymbol>
       <ci>normal-|</ci>
       <csymbol cd="unknown">d</csymbol>
       <csymbol cd="unknown">r</csymbol>
       <csymbol cd="unknown">u</csymbol>
       <csymbol cd="unknown">n</csymbol>
       <csymbol cd="unknown">k</csymbol>
       <ci>normal-)</ci>
      </cerror>
      <csymbol cd="unknown">p</csymbol>
      <cerror>
       <csymbol cd="ambiguous">fragments</csymbol>
       <ci>normal-(</ci>
       <csymbol cd="unknown">d</csymbol>
       <csymbol cd="unknown">r</csymbol>
       <csymbol cd="unknown">u</csymbol>
       <csymbol cd="unknown">n</csymbol>
       <csymbol cd="unknown">k</csymbol>
       <ci>normal-)</ci>
      </cerror>
     </cerror>
     <apply>
      <times></times>
      <ci>p</ci>
      <ci>D</ci>
     </apply>
    </apply>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(drunk|D)=\frac{p(D|drunk)\,p(drunk)}{p(D)}
  </annotation>
 </semantics>
</math>

 We were told the following in the first paragraph:</p>

<p>

<math display="block" id="Base_rate_fallacy:2">
 <semantics>
  <mrow>
   <mrow>
    <mi>p</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>d</mi>
      <mi>r</mi>
      <mi>u</mi>
      <mi>n</mi>
      <mi>k</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0.001</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>p</ci>
     <apply>
      <times></times>
      <ci>d</ci>
      <ci>r</ci>
      <ci>u</ci>
      <ci>n</ci>
      <ci>k</ci>
     </apply>
    </apply>
    <cn type="float">0.001</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(drunk)=0.001
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="block" id="Base_rate_fallacy:3">
 <semantics>
  <mrow>
   <mrow>
    <mi>p</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>s</mi>
      <mi>o</mi>
      <mi>b</mi>
      <mi>e</mi>
      <mi>r</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0.999</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>p</ci>
     <apply>
      <times></times>
      <ci>s</ci>
      <ci>o</ci>
      <ci>b</ci>
      <ci>e</ci>
      <ci>r</ci>
     </apply>
    </apply>
    <cn type="float">0.999</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(sober)=0.999
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="block" id="Base_rate_fallacy:4">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">|</mo>
    <mi>d</mi>
    <mi>r</mi>
    <mi>u</mi>
    <mi>n</mi>
    <mi>k</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>1.00</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">d</csymbol>
     <csymbol cd="unknown">r</csymbol>
     <csymbol cd="unknown">u</csymbol>
     <csymbol cd="unknown">n</csymbol>
     <csymbol cd="unknown">k</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="float">1.00</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(D|drunk)=1.00
  </annotation>
 </semantics>
</math>

</p>

<p>

<math display="block" id="Base_rate_fallacy:5">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">|</mo>
    <mi>s</mi>
    <mi>o</mi>
    <mi>b</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>0.05</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">s</csymbol>
     <csymbol cd="unknown">o</csymbol>
     <csymbol cd="unknown">b</csymbol>
     <csymbol cd="unknown">e</csymbol>
     <csymbol cd="unknown">r</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="float">0.05</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(D|sober)=0.05
  </annotation>
 </semantics>
</math>

 As you can see from the formula, one needs p(D) for Bayes' Theorem, which one can compute from the preceding values using</p>

<p>

<math display="block" id="Base_rate_fallacy:6">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">|</mo>
    <mi>d</mi>
    <mi>r</mi>
    <mi>u</mi>
    <mi>n</mi>
    <mi>k</mi>
    <mo rspace="4.2pt" stretchy="false">)</mo>
   </mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>d</mi>
    <mi>r</mi>
    <mi>u</mi>
    <mi>n</mi>
    <mi>k</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>+</mo>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>D</mi>
    <mo stretchy="false">|</mo>
    <mi>s</mi>
    <mi>o</mi>
    <mi>b</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mo rspace="4.2pt" stretchy="false">)</mo>
   </mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>s</mi>
    <mi>o</mi>
    <mi>b</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">d</csymbol>
     <csymbol cd="unknown">r</csymbol>
     <csymbol cd="unknown">u</csymbol>
     <csymbol cd="unknown">n</csymbol>
     <csymbol cd="unknown">k</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">d</csymbol>
     <csymbol cd="unknown">r</csymbol>
     <csymbol cd="unknown">u</csymbol>
     <csymbol cd="unknown">n</csymbol>
     <csymbol cd="unknown">k</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <plus></plus>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">s</csymbol>
     <csymbol cd="unknown">o</csymbol>
     <csymbol cd="unknown">b</csymbol>
     <csymbol cd="unknown">e</csymbol>
     <csymbol cd="unknown">r</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">s</csymbol>
     <csymbol cd="unknown">o</csymbol>
     <csymbol cd="unknown">b</csymbol>
     <csymbol cd="unknown">e</csymbol>
     <csymbol cd="unknown">r</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(D)=p(D|drunk)\,p(drunk)+p(D|sober)\,p(sober)
  </annotation>
 </semantics>
</math>

 which gives</p>

<p>

<math display="block" id="Base_rate_fallacy:7">
 <semantics>
  <mrow>
   <mrow>
    <mi>p</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>D</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>0.05095</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>p</ci>
     <ci>D</ci>
    </apply>
    <cn type="float">0.05095</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(D)=0.05095
  </annotation>
 </semantics>
</math>

 Plugging these numbers into Bayes' Theorem, one finds that</p>

<p>

<math display="block" id="Base_rate_fallacy:8">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>d</mi>
    <mi>r</mi>
    <mi>u</mi>
    <mi>n</mi>
    <mi>k</mi>
    <mo stretchy="false">|</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>0.019627</mn>
   <mo>⋅</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">d</csymbol>
     <csymbol cd="unknown">r</csymbol>
     <csymbol cd="unknown">u</csymbol>
     <csymbol cd="unknown">n</csymbol>
     <csymbol cd="unknown">k</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="float">0.019627</cn>
    <ci>normal-⋅</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(drunk|D)=0.019627\cdot
  </annotation>
 </semantics>
</math>

</p>

<p>A more intuitive explanation: in average, for every 1,000 drivers tested,</p>
<ul>
<li>1 driver is drunk, and it is 100% certain that for that driver there is a <em>true</em> positive test result, so there is 1 <em>true</em> positive test result</li>
<li>999 drivers are not drunk, and among those drivers there are 5% <em>false</em> positive test results, so there are 49.95 <em>false</em> positive test results</li>
</ul>

<p>therefore the probability that one of the drivers among the 1 + 49.95 = 50.95 positive test results really is drunk is 

<math display="inline" id="Base_rate_fallacy:9">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>d</mi>
    <mi>r</mi>
    <mi>u</mi>
    <mi>n</mi>
    <mi>k</mi>
    <mo stretchy="false">|</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
   <mo>/</mo>
   <mn>50.95</mn>
   <mo>≈</mo>
   <mn>0.019627</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">d</csymbol>
     <csymbol cd="unknown">r</csymbol>
     <csymbol cd="unknown">u</csymbol>
     <csymbol cd="unknown">n</csymbol>
     <csymbol cd="unknown">k</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <cn type="integer">1</cn>
    <divide></divide>
    <cn type="float">50.95</cn>
    <approx></approx>
    <cn type="float">0.019627</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(drunk|D)=1/50.95\approx 0.019627
  </annotation>
 </semantics>
</math>

.</p>

<p>The validity of this result does, however, hinge on the validity of the initial assumption that the policemen stopped the driver truly at random, and not because of bad driving. If that or another non-arbitrary reason for stopping the driver was present, then the calculation also involves the probability of a drunk driver driving competently and a non-drunk driver driving competently.</p>
<h2 id="example-3">Example 3</h2>

<p>In a city of 1 million inhabitants let there be 100 terrorists and 999,900 non-terrorists. To simplify the example, it is assumed that all people present in the city are inhabitants. Thus, the base rate probability of a randomly selected inhabitant of the city being a terrorist is 0.0001, and the base rate probability of that same inhabitant being a non-terrorist is 0.9999. In an attempt to catch the terrorists, the city installs an alarm system with a surveillance camera and automatic facial recognition software.</p>

<p>The software has two failure rates of 1%:</p>
<ul>
<li>The false negative rate: If the camera scans a terrorist, a bell will ring 99% of the time, and it will fail to ring 1% of the time.</li>
<li>The false positive rate: If the camera scans a non-terrorist, a bell will not ring 99% of the time, but it will ring 1% of the time.</li>
</ul>

<p>Suppose now that an inhabitant triggers the alarm. What is the chance that the person is a terrorist? In other words, what is P(T | B), the probability that a terrorist has been detected given the ringing of the bell? Someone making the 'base rate fallacy' would infer that there is a 99% chance that the detected person is a terrorist. Although the inference seems to make sense, it is actually bad reasoning, and a calculation below will show that the chances they are a terrorist are actually near 1%, not near 99%.</p>

<p>The fallacy arises from confusing the natures of two different failure rates. The 'number of non-bells per 100 terrorists' and the 'number of non-terrorists per 100 bells' are unrelated quantities. One does not necessarily equal the other, and they don't even have to be almost equal. To show this, consider what happens if an identical alarm system were set up in a second city with no terrorists at all. As in the first city, the alarm sounds for 1 out of every 100 non-terrorist inhabitants detected, but unlike in the first city, the alarm never sounds for a terrorist. Therefore 100% of all occasions of the alarm sounding are for non-terrorists, but a false negative rate cannot even be calculated. The 'number of non-terrorists per 100 bells' in that city is 100, yet P(T | B) = 0%. There is zero chance that a terrorist has been detected given the ringing of the bell.</p>

<p>Imagine that the city's entire population of one million people pass in front of the camera. About 99 of the 100 terrorists will trigger the alarm—and so will about 9,999 of the 999,900 non-terrorists. Therefore, about 10,098 people will trigger the alarm, among which about 99 will be terrorists. So, the probability that a person triggering the alarm actually is a terrorist, is only about 99 in 10,098, which is less than 1%, and very, very far below our initial guess of 99%.</p>

<p>The base rate fallacy is so misleading in this example because there are many more non-terrorists than terrorists.</p>
<h2 id="findings-in-psychology">Findings in psychology</h2>

<p>In experiments, people have been found to prefer individuating information over general information when the former is available.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>

<p>In some experiments, students were asked to estimate the <a href="grade_point_average" title="wikilink">grade point averages</a> (GPAs) of hypothetical students. When given relevant statistics about GPA distribution, students tended to ignore them if given descriptive information about the particular student, even if the new descriptive information was obviously of little or no relevance to school performance.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> This finding has been used to argue that interviews are an unnecessary part of the <a href="college_admissions" title="wikilink">college admissions</a> process because interviewers are unable to pick successful candidates better than basic statistics.</p>

<p><a href="Psychologist" title="wikilink">Psychologists</a> <a href="Daniel_Kahneman" title="wikilink">Daniel Kahneman</a> and <a href="Amos_Tversky" title="wikilink">Amos Tversky</a> attempted to explain this finding in terms of a <a href="heuristics_in_judgment_and_decision_making" title="wikilink">simple rule or "heuristic"</a> called <a href="representativeness_heuristic" title="wikilink">representativeness</a>. They argued that many judgements relating to likelihood, or to cause and effect, are based on how representative one thing is of another, or of a category.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> Kahneman considers base rate neglect to be a specific form of <a href="extension_neglect" title="wikilink">extension neglect</a>.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> <a href="Richard_Nisbett" title="wikilink">Richard Nisbett</a> has argued that some <a href="attributional_bias" title="wikilink">attributional biases</a> like the <a href="fundamental_attribution_error" title="wikilink">fundamental attribution error</a> are instances of the base rate fallacy: people underutilize "consensus information" (the "base rate") about how others behaved in similar situations and instead prefer simpler <a href="dispositional_attribution" title="wikilink">dispositional attributions</a>.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>

<p>There is considerable debate in psychology on the conditions under which people do or do not appreciate base rate information.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a><a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> Researchers in the heuristics-and-biases program have stressed empirical findings showing that people tend to ignore base rates and make inferences that violate certain norms of probabilistic reasoning, such as <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a>. The conclusion drawn from this line of research was that human probabilistic thinking is fundamentally flawed and error-prone.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> Other researchers have emphasized the link between cognitive processes and information formats, arguing that such conclusions are not generally warranted.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></p>

<p>Consider again Example 2 from above. The required inference is to estimate the (posterior) probability that a (randomly picked) driver is drunk, given that the breathalyzer test is positive. Formally, this probability can be calculated using <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a>, as shown above. However, there are different ways of presenting the relevant information. Consider the following, formally equivalent variant of the problem:</p>
<dl>
<dd> 1 out of 1000 drivers are driving drunk. The breathalyzers never fail to detect a truly drunk person. For 50 out of the 999 drivers who are not drunk the breathalyzer falsely displays drunkness. Suppose the policemen then stop a driver at random, and force them to take a breathalyzer test. It indicates that he or she is drunk. We assume you don't know anything else about him or her. How high is the probability he or she really is drunk?
</dd>
</dl>

<p>In this case, the relevant numerical information—<em>p</em>(drunk), <em>p</em>(D | drunk), <em>p</em>(D | sober)—is presented in terms of natural frequencies with respect to a certain reference class (see <a href="reference_class_problem" title="wikilink">reference class problem</a>). Empirical studies show that people's inferences correspond more closely to Bayes' rule when information is presented this way, helping to overcome base-rate neglect in laypeople<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> and experts.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> As a consequence, organizations like the <a href="Cochrane_Collaboration" title="wikilink">Cochrane Collaboration</a> recommend using this kind of format for communicating health statistics.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> Teaching people to translate these kinds of Bayesian reasoning problems into natural frequency formats is more effective than merely teaching them to plug probabilities (or percentages) into Bayes' theorem.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> It has also been shown that graphical representations of natural frequencies (e.g., icon arrays) help people to make better inferences.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a><a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a><a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a></p>

<p>Why are natural frequency formats helpful? One important reason is that this information format facilitates the required inference because it simplifies the necessary calculations. This can be seen when using an alternative way of computing the required probability <em>p</em>(drunk|D):</p>

<p>

<math display="block" id="Base_rate_fallacy:10">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>d</mi>
    <mi>r</mi>
    <mi>u</mi>
    <mi>n</mi>
    <mi>k</mi>
    <mo stretchy="false">|</mo>
    <mi>D</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>N</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mrow>
        <mi>d</mi>
        <mi>r</mi>
        <mi>u</mi>
        <mi>n</mi>
        <mi>k</mi>
       </mrow>
       <mo>∩</mo>
       <mi>D</mi>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mi>N</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>D</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
   <mo>=</mo>
   <mfrac>
    <mn>1</mn>
    <mn>51</mn>
   </mfrac>
   <mo>=</mo>
   <mn>0.0196</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">p</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">d</csymbol>
     <csymbol cd="unknown">r</csymbol>
     <csymbol cd="unknown">u</csymbol>
     <csymbol cd="unknown">n</csymbol>
     <csymbol cd="unknown">k</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">D</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <ci>N</ci>
      <apply>
       <intersect></intersect>
       <apply>
        <times></times>
        <ci>d</ci>
        <ci>r</ci>
        <ci>u</ci>
        <ci>n</ci>
        <ci>k</ci>
       </apply>
       <ci>D</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>N</ci>
      <ci>D</ci>
     </apply>
    </apply>
    <eq></eq>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <cn type="integer">51</cn>
    </apply>
    <eq></eq>
    <cn type="float">0.0196</cn>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(drunk|D)=\frac{N(drunk\cap D)}{N(D)}=\frac{1}{51}=0.0196
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>N</em>(drunk ∩ D) denotes the number of drivers that are drunk and get a positive breathalyzer result, and <em>N</em>(D) denotes the total number of cases with a positive breathalyzer result. The equivalence of this equation to the above one follows from the axioms of probability theory, according to which <em>N</em>(drunk ∩ D) = <em>N</em> × <em>p</em> (D | drunk) × <em>p</em> (drunk). Importantly, although this equation is formally equivalent to Bayes’ rule, it is not psychologically equivalent. Using natural frequencies simplifies the inference because the required mathematical operation can be performed on natural numbers, instead of normalized fractions (i.e., probabilities), because it makes the high number of false positives more transparent, and because natural frequencies exhibit a "nested-set structure".<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a><a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a></p>

<p>It is important to note that not any kind of frequency format facilitates Bayesian reasoning.<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a><a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a> Natural frequencies refer to frequency information that results from <em>natural sampling</em>,<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> which preserves base rate information (e.g., number of drunken drivers when taking a random sample of drivers). This is different from <em>systematic sampling</em>, in which base rates are fixed a priori (e.g., in scientific experiments). In the latter case it is not possible to infer the posterior probability <em>p</em> (drunk | positive test) from comparing the number of drivers who are drunk and test positive compared to the total number of people who get a positive breathalyzer result, because base rate information is not preserved and must be explicitly re-introduced using Bayes' theorem.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Bayesian_probability" title="wikilink">Bayesian probability</a></li>
<li><a href="Data_dredging" title="wikilink">Data dredging</a></li>
<li><a href="False_positive_paradox" title="wikilink">False positive paradox</a></li>
<li><a href="Inductive_argument" title="wikilink">Inductive argument</a></li>
<li><a href="List_of_cognitive_biases" title="wikilink">List of cognitive biases</a></li>
<li><a href="Misleading_vividness" title="wikilink">Misleading vividness</a></li>
<li><a href="Prosecutor's_fallacy" title="wikilink">Prosecutor's fallacy</a></li>
<li><a class="uri" href="Stereotype" title="wikilink">Stereotype</a></li>
</ul>
<h2 id="references-list">References list</h2>
<references>
</references>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.fallacyfiles.org/baserate.html">The Base Rate Fallacy</a> The Fallacy Files</li>
<li><a href="https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art15.html#ft145">Psychology of Intelligence Analysis: Base Rate Fallacy</a></li>
<li><a href="http://www.youtube.com/watch?v=D8VZqxcu0I0">The base rate fallacy explained visually</a> (Video)</li>
<li><a href="http://www.eeps.com/riskicon/">Interactive page for visualizing statistical information and Bayesian inference problems</a></li>
<li><a href="http://ipdas.ohri.ca/IPDAS-Chapter-C.pdf">Current ‘best practice’ for communicating probabilities in health according to the International Patient Decision Aid Standards (IPDAS) Collaboration</a></li>
</ul>

<p>"</p>

<p><a href="Category:Relevance_fallacies" title="wikilink">Category:Relevance fallacies</a> <a href="Category:Cognitive_biases" title="wikilink">Category:Cognitive biases</a> <a href="Category:Behavioral_finance" title="wikilink">Category:Behavioral finance</a> <a href="Category:Probability_fallacies" title="wikilink">Category:Probability fallacies</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"><a href="#fnref11">↩</a></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"><a href="#fnref14">↩</a></li>
<li id="fn15"></li>
<li id="fn16"><a href="#fnref16">↩</a></li>
<li id="fn17"><a href="#fnref17">↩</a></li>
<li id="fn18"><a href="#fnref18">↩</a></li>
<li id="fn19"></li>
<li id="fn20"><a href="#fnref20">↩</a></li>
<li id="fn21"><a href="#fnref21">↩</a></li>
<li id="fn22"><a href="#fnref22">↩</a></li>
<li id="fn23"></li>
<li id="fn24"><a href="#fnref24">↩</a></li>
<li id="fn25"><a href="#fnref25">↩</a></li>
<li id="fn26"><a href="#fnref26">↩</a></li>
</ol>
</section>
</body>
</html>
