<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="1538">Minor (linear algebra)</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Minor (linear algebra)</h1>
<hr/>
<p>In <a href="linear_algebra" title="wikilink">linear algebra</a>, a <strong>minor</strong> of a <a href="matrix_(mathematics)" title="wikilink">matrix</a> <strong>A</strong> is the <a class="uri" href="determinant" title="wikilink">determinant</a> of some smaller <a href="square_matrix" title="wikilink">square matrix</a>, cut down from <strong>A</strong> by removing one or more of its rows or columns. Minors obtained by removing just one row and one column from square matrices (<strong>first minors</strong>) are required for calculating matrix <strong>cofactors</strong>, which in turn are useful for computing both the determinant and <a href="Inverse_matrix" title="wikilink">inverse</a> of square matrices.</p>
<h2 id="definition-and-illustration">Definition and illustration</h2>
<h3 id="first-minors">First minors</h3>
<p>If A is a <a href="square_matrix" title="wikilink">square matrix</a>, then the <strong>minor</strong> of the entry in the <em>i</em>-th row and <em>j</em>-th column (also called the <strong>(<em>i</em>,<em>j</em>) minor</strong>, or a <strong>first minor</strong><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a>) is the <a class="uri" href="determinant" title="wikilink">determinant</a> of the <a class="uri" href="submatrix" title="wikilink">submatrix</a> formed by deleting the i-th row and j-th column. This number is often denoted M<sub><em>i,j</em></sub>. The (i,j) <strong>cofactor</strong> is obtained by multiplying the minor by <span class="LaTeX">$(-1)^{i+j}$</span>.</p>
<p>To illustrate these definitions, consider the following 3 by 3 matrix,</p>
<p><span class="LaTeX">$$\begin{bmatrix}
\,\,\,1 & 4 & 7 \\
\,\,\,3 & 0 & 5 \\
-1 & 9 & \!11 \\
\end{bmatrix}$$</span></p>
<p>To compute the minor <em>M</em><sub>23</sub> and the cofactor <em>C</em><sub>23</sub>, we find the determinant of the above matrix with row 2 and column 3 removed.</p>
<p><span class="LaTeX">$$M_{23} = \det \begin{bmatrix}
\,\,1 & 4 & \Box\, \\
\,\Box & \Box & \Box\, \\
-1 & 9 & \Box\, \\
\end{bmatrix}= \det \begin{bmatrix}
\,\,\,1 & 4\, \\
-1 & 9\, \\
\end{bmatrix} = (9-(-4)) = 13$$</span></p>
<p>So the cofactor of the (2,3) entry is</p>
<p><span class="LaTeX">$$\ C_{23} = (-1)^{2+3}(M_{23}) = -13.$$</span></p>
<h3 id="general-definition">General definition</h3>
<p>Let <strong>A</strong> be an <em>m</em> × <em>n</em> matrix and <em>k</em> an integer with 0 ''' × <em>k</em> minor''' of <strong>A</strong>, also called <strong>minor determinant of order k</strong> of <strong>A</strong> or, if <span class="LaTeX">$m = n$</span>, <strong>(n-k):th minor determinant</strong> of <strong>A</strong>, with the word "determinant" often omitted and the word "order" sometimes replaced by "degree", is the determinant of a <em>k</em> × <em>k</em> matrix obtained from <strong>A</strong> by deleting <em>m</em> − <em>k</em> rows and <em>n</em> − <em>k</em> columns. Sometimes the term is used to refer to the <em>k</em> × <em>k</em> matrix obtained from <strong>A</strong> as above (by deleting <em>m</em> − <em>k</em> rows and <em>n</em> − <em>k</em> columns), but this matrix should be referred to as a <strong>(square) submatrix</strong> of <strong>A</strong>, leaving the term "minor" to refer to the determinant of this matrix. For a matrix <strong>A</strong> as above, there are a total of <span class="LaTeX">${m \choose k} \cdot {n \choose k}$</span> minors of size <em>k</em> × <em>k</em>. <strong>Minor of order zero</strong> is often defined to be 1. For a square matrix, <strong>zeroth minor</strong> is just the determinant of the matrix.,.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<p>Let <span class="LaTeX">$1 \leq i_1 < i_2 < \ldots < i_k \leq m$</span>, <span class="LaTeX">$1 \leq j_1 < j_2 < \ldots < j_k \leq n$</span> be ordered sequences (in natural order, as it is always assumed when talking about minors unless otherwise stated) of indexes, call them <span class="LaTeX">$I$</span> and <span class="LaTeX">$J$</span>, respectively. The minor <span class="LaTeX">$\det \left( (A_{i_p, j_q})_{p,q = 1, \ldots, k} \right)$</span> corresponding to these choices of indexes is denoted <span class="LaTeX">$\det_{I,J} A$</span> or <span class="LaTeX">$[A]_{I,J}$</span> or <span class="LaTeX">$M_{I,J}$</span> or <span class="LaTeX">$M_{i_1, i_2, \ldots, i_k, j_1, j_2, \ldots, j_k}$</span> or <span class="LaTeX">$M_{(i),(j)}$</span> (where the <span class="LaTeX">$(i)$</span> denotes the sequence of indexes <span class="LaTeX">$I$</span>, etc.), depending on the source. Also, there are two types of denotations in use in literature: by the minor associated to ordered sequences of indexes <strong>I</strong> and <strong>J</strong>, some authors<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> mean the determinant of the matrix that is formed as above, by taking the elements of the original matrix from the rows whose indexes are in <strong>I</strong> and columns whose indexes are in <strong>J</strong>, whereas some other authors mean by a minor associated to <strong>I</strong> and <strong>J</strong> the determinant of the matrix formed from the original matrix by deleting the rows in <strong>I</strong> and columns in <strong>J</strong>.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> Which notation is used should always be checked from the source in question. In this article, we use the inclusive definition of choosing the elements from rows of <strong>I</strong> and columns of <strong>J</strong>. The exceptional case is the case of the first minor or the (i,j)-minor described above; in that case, the exclusive notation <span class="LaTeX">$M_{i,j} = \det \left( \left( A_{p,q} \right)_{p \neq i, q \neq j} \right)$</span> is standard everywhere in the literature and is used in this article also.</p>
<h3 id="complement">Complement</h3>
<p>The complement, <em>B<sub>ijk...,pqr...</sub></em>, of a minor, <em>M<sub>ijk...,pqr...</sub></em>, of a square matrix, <strong>A</strong>, is formed by the determinant of the matrix <strong>A</strong> from which all the rows (<em>ijk...</em>) and columns (<em>pqr...</em>) associated with <em>M<sub>ijk...,pqr...</sub></em> have been removed. The complement of the first minor of an element <em>a<sub>ij</sub></em> is merely that element.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h2 id="applications-of-minors-and-cofactors">Applications of minors and cofactors</h2>
<h3 id="cofactor-expansion-of-the-determinant">Cofactor expansion of the determinant</h3>
<p>The cofactors feature prominently in <a href="Laplace_expansion" title="wikilink">Laplace's formula</a> for the expansion of determinants, which is a method of computing larger determinants in terms of smaller ones. Given the <span class="LaTeX">$n\times n$</span> matrix <span class="LaTeX">$(a_{ij})$</span>, the determinant of <em>A</em> (denoted det(<em>A</em>)) can be written as the sum of the cofactors of any row or column of the matrix multiplied by the entries that generated them. In other words, the cofactor expansion along the <em>j</em>th column gives:</p>
<p><span class="LaTeX">$$\ \det(\mathbf A) = a_{1j}C_{1j} + a_{2j}C_{2j} + a_{3j}C_{3j} + ... + a_{nj}C_{nj} = \sum_{i=1}^{n} a_{ij} C_{ij}$$</span></p>
<p>The cofactor expansion along the <em>i</em>th row gives:</p>
<p><span class="LaTeX">$$\ \det(\mathbf A) = a_{i1}C_{i1} + a_{i2}C_{i2} + a_{i3}C_{i3} + ... + a_{in}C_{in} = \sum_{j=1}^{n} a_{ij} C_{ij}$$</span></p>
<h3 id="inverse-of-a-matrix">Inverse of a matrix</h3>
<p>One can write down the inverse of an invertible matrix by computing its cofactors by using <a href="Cramer's_rule" title="wikilink">Cramer's rule</a>, as follows. The matrix formed by all of the cofactors of a square matrix <strong>A</strong> is called the <strong>cofactor matrix</strong> (also called the <strong>matrix of cofactors</strong>):</p>
<p><span class="LaTeX">$$\mathbf C=\begin{bmatrix}
    C_{11}  & C_{12} & \cdots &   C_{1n}   \\
    C_{21}  & C_{22} & \cdots &   C_{2n}   \\
  \vdots & \vdots & \ddots & \vdots \\ 
    C_{n1}  & C_{n2} & \cdots &  C_{nn}
\end{bmatrix}$$</span></p>
<p>Then the inverse of <strong>A</strong> is the transpose of the cofactor matrix times the reciprocal of the determinant of <em>A</em>:</p>
<p><span class="LaTeX">$$\mathbf A^{-1} = \frac{1}{\operatorname{det}(\mathbf A)} \mathbf C^\mathsf{T}.$$</span></p>
<p>The transpose of the cofactor matrix is called the <a class="uri" href="adjugate" title="wikilink">adjugate</a> matrix (also called the <strong>classical adjoint</strong>) of <strong>A</strong>.</p>
<p>The above formula can be generalized as follows: Let <span class="LaTeX">$1 \leq i_1 < i_2 < \ldots < i_k \leq n$</span>, <span class="LaTeX">$1 \leq j_1 < j_2 < \ldots < j_k \leq n$</span> be ordered sequences (in natural order) of indexes (here <strong>A</strong> is an <span class="LaTeX">$n \times n$</span>-matrix). Then</p>
<p><span class="LaTeX">$$[\mathbf A^{-1}]_{I,J} = \pm\frac{[\mathbf A]_{J',I'}}{\det \mathbf A}$$</span>,</p>
<p>where <span class="LaTeX">$I', J'$</span> denote the ordered sequences of indices (the indices are in natural order of magnitude, as above) complementary to <span class="LaTeX">$I, J$</span>, so that every index <span class="LaTeX">$1,\ldots,n$</span> appears exactly one time in either <span class="LaTeX">$I$</span> or <span class="LaTeX">$I'$</span>, but not in both (similarly for the <span class="LaTeX">$J$</span> and <span class="LaTeX">$J'$</span>) and <span class="LaTeX">$[\mathbf A]_{I,J}$</span> denotes the determinant of the submatrix of <strong>A</strong> formed by choosing the rows of the index set <span class="LaTeX">$I$</span> and columns of index set <span class="LaTeX">$J$</span>. Also, <span class="LaTeX">$[\mathbf A]_{I,J} = \det \left( (A_{i_p, j_q})_{p,q = 1, \ldots, k} \right)$</span> . A simple proof can be given using wedge product. Indeed,</p>
<p><span class="LaTeX">$$[\mathbf A^{-1}]_{I,J}(e_1\wedge\ldots \wedge e_n) = \pm(\mathbf A^{-1}e_{j_1})\wedge \ldots \wedge(\mathbf A^{-1}e_{j_k})\wedge e_{i'_1}\wedge\ldots \wedge e_{i'_{n-k}},$$</span></p>
<p>where <span class="LaTeX">$e_1,\ldots,e_n$</span> are the basis vectors. Acting by <span class="LaTeX">$\mathbf A$</span> on both sides, one gets</p>
<p><span class="LaTeX">$$[\mathbf A^{-1}]_{I,J}\det \mathbf A (e_1\wedge\ldots \wedge e_n) = \pm (e_{j_1})\wedge \ldots \wedge(e_{j_k})\wedge (\mathbf A e_{i'_1})\wedge\ldots \wedge (\mathbf A e_{i'_{n-k}})=\pm [\mathbf A]_{J',I'}(e_1\wedge\ldots \wedge e_n).$$</span></p>
<p>The sign can be worked out to be <span class="LaTeX">$(-1)^{ \sum_{s=1}^{k} i_s - \sum_{s=1}^{k} j_s}$</span>, also the sign is determined by the sums of elements in <span class="LaTeX">$I,J$</span>.</p>
<h3 id="other-applications">Other applications</h3>
<p>Given an <em>m</em> × <em>n</em> matrix with <a href="real_number" title="wikilink">real</a> entries (or entries from any other <a href="field_(mathematics)" title="wikilink">field</a>) and <a href="rank_(matrix_theory)" title="wikilink">rank</a> <em>r</em>, then there exists at least one non-zero <em>r</em> × <em>r</em> minor, while all larger minors are zero.</p>
<p>We will use the following notation for minors: if <strong>A</strong> is an <em>m</em> × <em>n</em> matrix, <em>I</em> is a <a class="uri" href="subset" title="wikilink">subset</a> of {1,...,<em>m</em>} with <em>k</em> elements and <em>J</em> is a subset of {1,...,<em>n</em>} with <em>k</em> elements, then we write [<strong>A</strong>]<sub><em>I</em>,<em>J</em></sub> for the <em>k</em> × <em>k</em> minor of <strong>A</strong> that corresponds to the rows with index in <em>I</em> and the columns with index in <em>J</em>.</p>
<ul>
<li>If <em>I</em> = <em>J</em>, then [<strong>A</strong>]<sub><em>I</em>,<em>J</em></sub> is called a <strong>principal minor</strong>.</li>
<li>If the matrix that corresponds to a principal minor is a quadratic upper-left part of the larger matrix (i.e., it consists of matrix elements in rows and columns from 1 to k), then the principal minor is called a <strong>leading principal minor (of order k)</strong> or <strong>corner (principal) minor (of order k)</strong>.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> For an <em>n</em> × <em>n</em> square matrix, there are <em>n</em> leading principal minors.</li>
<li>A <strong>basic minor</strong> of a matrix is the determinant of a square submatrix that is of maximal size and has nonzero determinant.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></li>
<li>For <a href="Hermitian_matrix" title="wikilink">Hermitian matrices</a>, the leading principal minors can be used to test for <a href="positive-definite_matrix" title="wikilink">positive definiteness</a> and the principal minors can be used to test for <a href="positive-semidefinite_matrix" title="wikilink">positive semidefiniteness</a>. See <a href="Sylvester's_criterion" title="wikilink">Sylvester's criterion</a> for more details.</li>
</ul>
<p>Both the formula for ordinary <a href="matrix_multiplication" title="wikilink">matrix multiplication</a> and the <a href="Cauchy-Binet_formula" title="wikilink">Cauchy-Binet formula</a> for the determinant of the product of two matrices are special cases of the following general statement about the minors of a product of two matrices. Suppose that <strong>A</strong> is an <em>m</em> × <em>n</em> matrix, <strong>B</strong> is an <em>n</em> × <em>p</em> matrix, <em>I</em> is a <a class="uri" href="subset" title="wikilink">subset</a> of {1,...,<em>m</em>} with <em>k</em> elements and <em>J</em> is a subset of {1,...,<em>p</em>} with <em>k</em> elements. Then</p>
<p><span class="LaTeX">$$[\mathbf{AB}]_{I,J} = \sum_{K} [\mathbf{A}]_{I,K} [\mathbf{B}]_{K,J}\,$$</span> where the sum extends over all subsets <em>K</em> of {1,...,<em>n</em>} with <em>k</em> elements. This formula is a straightforward extension of the Cauchy-Binet formula.</p>
<h2 id="multilinear-algebra-approach">Multilinear algebra approach</h2>
<p>A more systematic, algebraic treatment of the minor concept is given in <a href="multilinear_algebra" title="wikilink">multilinear algebra</a>, using the <a href="wedge_product" title="wikilink">wedge product</a>: the <em>k</em>-minors of a matrix are the entries in the <em>k</em>th <a href="exterior_power" title="wikilink">exterior power</a> map.</p>
<p>If the columns of a matrix are wedged together <em>k</em> at a time, the <em>k</em> × <em>k</em> minors appear as the components of the resulting <em>k</em>-vectors. For example, the 2 × 2 minors of the matrix</p>
<p><span class="LaTeX">$$\begin{pmatrix}
1 & 4 \\
3 & \!\!-1 \\
2 & 1 \\
\end{pmatrix}$$</span> are −13 (from the first two rows), −7 (from the first and last row), and 5 (from the last two rows). Now consider the wedge product</p>
<p><span class="LaTeX">$$(\mathbf{e}_1 + 3\mathbf{e}_2 +2\mathbf{e}_3)\wedge(4\mathbf{e}_1-\mathbf{e}_2+\mathbf{e}_3)$$</span> where the two expressions correspond to the two columns of our matrix. Using the properties of the wedge product, namely that it is <a class="uri" href="bilinear" title="wikilink">bilinear</a> and</p>
<p><span class="LaTeX">$$\mathbf{e}_i\wedge \mathbf{e}_i = 0$$</span> and</p>
<p><span class="LaTeX">$$\mathbf{e}_i\wedge \mathbf{e}_j = - \mathbf{e}_j\wedge \mathbf{e}_i,$$</span> we can simplify this expression to</p>
<p><span class="LaTeX">$$-13 \mathbf{e}_1\wedge \mathbf{e}_2 -7 \mathbf{e}_1\wedge \mathbf{e}_3 +5 \mathbf{e}_2\wedge \mathbf{e}_3$$</span> where the coefficients agree with the minors computed earlier.</p>
<h2 id="a-remark-about-different-notations">A remark about different notations</h2>
<p>In some books <a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> instead of <em>cofactor</em> the term <em>adjunct</em> is used. Moreover, it is denoted as <strong>A</strong><sub>ij</sub> and defined in the same way as cofactor:</p>
<dl>
<dd><dl>
<dd><span class="LaTeX">$\mathbf{A}_{ij} = (-1)^{i+j} \mathbf{M}_{ij}$</span>
</dd>
</dl>
</dd>
</dl>
<p>Using this notation the inverse matrix is written this way:</p>
<p><span class="LaTeX">$$\mathbf{A}^{-1} = \frac{1}{\det(A)}\begin{bmatrix}
    A_{11}  & A_{21} & \cdots &   A_{n1}   \\
    A_{12}  & A_{22} & \cdots &   A_{n2}   \\
  \vdots & \vdots & \ddots & \vdots \\ 
    A_{1n}  & A_{2n} & \cdots &  A_{nn}
\end{bmatrix}$$</span></p>
<p>Keep in mind that <em>adjunct</em> is not <a class="uri" href="adjugate" title="wikilink">adjugate</a> or <a class="uri" href="adjoint" title="wikilink">adjoint</a>. In modern terminology, the "adjoint" of a matrix most often refers to the corresponding <a href="adjoint_operator" title="wikilink">adjoint operator</a>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a class="uri" href="Submatrix" title="wikilink">Submatrix</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-19-determinant-formulas-and-cofactors/">MIT Linear Algebra Lecture on Cofactors</a> at Google Video, from MIT OpenCourseWare</li>
<li><a href="http://planetmath.org/encyclopedia/Cofactor.html">PlanetMath entry of <em>Cofactors</em></a></li>
<li><a href="http://www.encyclopediaofmath.org/index.php/Minor">Springer Encyclopedia of Mathematics entry for <em>Minor</em></a></li>
</ul>
<p>"</p>
<p><a href="Category:Matrix_theory" title="wikilink">Category:Matrix theory</a> <a class="uri" href="Category:Determinants" title="wikilink">Category:Determinants</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Burnside, William Snow & Panton, Arthur William (1886) <em>[<a class="uri" href="http://books.google.com/books?id=BhgPAAAAIAAJ&pg">http://books.google.com/books?id=BhgPAAAAIAAJ&pg</a>;=PA239&lpg;=PA239&dq;=first+minor+determinant&source;=web&ots;=BqWTlFMGIB&sig;=aeCdnU1sARW9tshE_zhirJZ5dRU&hl;=en Theory of Equations: with an Introduction to the Theory of Binary Algebraic Form]</em>.<a href="#fnref1">↩</a></li>
<li id="fn2">Elementary Matrix Algebra (Third edition), Franz E. Hohn, The Macmillan Company, 1973, ISBN 978-0-02-355950-1<a href="#fnref2">↩</a></li>
<li id="fn3">Minor. Encyclopedia of Mathematics. <a class="uri" href="http://www.encyclopediaofmath.org/index.php?title=Minor&oldid">http://www.encyclopediaofmath.org/index.php?title=Minor&oldid</a>;=30176<a href="#fnref3">↩</a></li>
<li id="fn4">Linear Algebra and Geometry, Igor R. Shafarevich, Alexey O. Remizov, Springer-Verlag Berlin Heidelberg, 2013, ISBN 978-3-642-30993-9<a href="#fnref4">↩</a></li>
<li id="fn5">Elementary Matrix Algebra (Third edition), Franz E. Hohn, The Macmillan Company, 1973, ISBN 978-0-02-355950-1<a href="#fnref5">↩</a></li>
<li id="fn6">Bertha Jeffreys, [<a class="uri" href="http://books.google.co.uk/books?id=Qs-xdYBQ_5wC&pg">http://books.google.co.uk/books?id=Qs-xdYBQ_5wC&pg</a>;=PA135 <em>Methods of Mathematical Physics</em>], p.135, Cambridge University Press, 1999 ISBN 0-521-66402-0.<a href="#fnref6">↩</a></li>
<li id="fn7">Minor. Encyclopedia of Mathematics. <a class="uri" href="http://www.encyclopediaofmath.org/index.php?title=Minor&oldid">http://www.encyclopediaofmath.org/index.php?title=Minor&oldid</a>;=30176<a href="#fnref7">↩</a></li>
<li id="fn8">Minor. Encyclopedia of Mathematics. <a class="uri" href="http://www.encyclopediaofmath.org/index.php?title=Minor&oldid">http://www.encyclopediaofmath.org/index.php?title=Minor&oldid</a>;=30176<a href="#fnref8">↩</a></li>
<li id="fn9"><a href="Felix_Gantmacher" title="wikilink">Felix Gantmacher</a>, <em>Theory of matrices</em> (1st ed., original language is Russian), Moscow: State Publishing House of technical and theoretical literature, 1953, p.491,<a href="#fnref9">↩</a></li>
</ol>
</section>
</body>
</html>
