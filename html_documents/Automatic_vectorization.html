<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1358">Automatic vectorization</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Automatic vectorization</h1>
<style>
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
<style>
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</body></html>
<body>
<hr/>

<p><strong>Automatic vectorization</strong>, in <a href="parallel_computing" title="wikilink">parallel computing</a>, is a special case of automatic <a class="uri" href="parallelization" title="wikilink">parallelization</a>, where a <a href="computer_program" title="wikilink">computer program</a> is converted from a <a href="scalar_(computing)" title="wikilink">scalar</a> implementation, which processes a single pair of <a href="operand" title="wikilink">operands</a> at a time, to a <a href="Array_data_structure" title="wikilink">vector</a> implementation, which processes one operation on multiple pairs of operands at once. For example, modern conventional computers, including specialized <a class="uri" href="supercomputers" title="wikilink">supercomputers</a>, typically have <a href="vector_processing" title="wikilink">vector operations</a> that simultaneously perform operations such as the following four additions:</p>

<p>

<math display="inline" id="Automatic_vectorization:0">
 <semantics>
  <msub>
   <mi>c</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>c</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle c_{1}
  </annotation>
 </semantics>
</math>


</p>

<p>However, in most <a href="programming_language" title="wikilink">programming languages</a> one typically writes loops that sequentially perform additions of many numbers. Here is an example of such a loop, written in <a href="C_(programming_language)" title="wikilink">C</a>:</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="kw">for</span> (i=<span class="dv">0</span>; i<n; +="&lt;span" class="st">""</n;></code></pre></div></body> &lt;=<span class="st">""</span> b[i];=<span class="st">""</span> c[i]=<span class="st">"a[i]"</span> i++)=<span class="st">""</span> source=<span class="st">""</span>&gt;

A vectorizing [[compiler]] transforms such loops into sequences of vector operations. These vector operations perform additions on length-four (in our example) blocks of elements from the arrays <code>a</code>, <code>b</code> and <code>c</code>. Automatic vectorization is a major research topic in computer science.

==Background==
Early computers generally had one logic unit that sequentially executed one instruction on one operand pair at a time. Computer programs and [[programming language]]s were accordingly designed to execute sequentially. Modern computers can <span class="kw">do</span> many things at once. Many optimizing compilers feature <span class="dt">auto</span>-vectorization, a compiler feature where particular parts of sequential programs are transformed into equivalent parallel ones, to produce code which will well utilize a vector processor. For a compiler to produce such efficient code <span class="kw">for</span> a programming language intended <span class="kw">for</span> use on a vector-processor would be much simpler, but, as much real-world code is sequential, the optimization is of great utility.

'''Loop vectorization''' converts procedural loops that iterate over multiple pairs of data items and assigns a separate processing unit to each pair. Most programs spend most of their execution times within such loops. Vectorizing loops can lead to significant performance gains without programmer intervention, especially on large data sets. Vectorization can sometimes instead slow execution because of [[Pipeline (computing)|pipeline]] synchronization, data movement timing and other issues.

[[Intel]]'s [[MMX (instruction set)|MMX]], [[Streaming SIMD Extensions|SSE]], [[Advanced Vector Extensions|AVX]] and [[Power Architecture]]'s [[Altivec|AltiVec]] and [[ARM Holdings|ARM]]'s [[ARM NEON|NEON]] instruction sets support such vectorized loops.

Many constraints prevent or hinder vectorization. [[Loop dependence analysis]] identifies loops that can be vectorized, relying on the [[data dependence]] of the instructions inside loops.

==Guarantees==
Automatic vectorization, like any [[loop optimization]] or other compile-time optimization, must exactly preserve program behavior.

===Data dependencies===
All dependencies must be respected during execution to prevent incorrect results.

In general, loop invariant dependencies and [[Loop_dependence_analysis#Classification|lexically forward dependencies]] can be easily vectorized, and lexically backward dependencies can be transformed into lexically forward dependencies. However, these transformations must be done safely, in order to ensure that the dependence between '''all statements''' remain true to the original.

Cyclic dependencies must be processed independently of the vectorized instructions.

===Data precision===
[[Integer (computer science)|Integer]] [[Precision (computer science)|precision]] (bit-size) must be kept during vector instruction execution. The correct vector instruction must be chosen based on the size and behavior of the internal integers. Also, with mixed integer types, extra care must be taken to promote/demote them correctly without losing precision. Special care must be taken with [[sign extension]] (because multiple integers are packed inside the same <span class="dt">register</span>) and during shift operations, or operations with [[carry bit]]s that would otherwise be taken into account.

[[Floating-point]] precision must be kept as well, unless [[IEEE<span class="dv">-754</span>]] compliance is turned off, in which <span class="kw">case</span> operations will be faster but the results may vary slightly. Big variations, even ignoring IEEE<span class="dv">-754</span> usually means programmer error. The programmer can also force constants and loop variables to single precision (<span class="kw">default</span> is normally <span class="dt">double</span>) to execute twice as many operations per instruction.

==Theory==
To vectorize a program, the compiler's optimizer must first understand the dependencies between statements and re-align them, <span class="kw">if</span> necessary. Once the dependencies are mapped, the optimizer must properly arrange the implementing instructions changing appropriate candidates to vector instructions, which operate on multiple data items.

===Building the dependency graph===
The first step is to build the [[dependency graph]], identifying which statements depend on which other statements. This involves examining each statement and identifying every  data item that the statement accesses, mapping array access modifiers to functions and checking every access' dependency to all others in all statements. [[Alias analysis]] can be used to certify that the different variables access (or intersects) the same region in memory.

The dependency graph contains all local dependencies with distance not greater than the vector size. So, <span class="kw">if</span> the vector <span class="dt">register</span> is <span class="dv">128</span> bits, and the array type is <span class="dv">32</span> bits, the vector size is <span class="dv">128</span>/<span class="dv">32</span> = <span class="dv">4</span>. All other non-cyclic dependencies should not invalidate vectorization, since there won't be any concurrent access in the same vector instruction.

Suppose the vector size is the same as <span class="dv">4</span> ints:

<source class="st" lang="&lt;span">"C"</source>&gt;
<span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">128</span>; i++) {
  a[i] = a[i<span class="dv">-16</span>]; <span class="co">// 16 &gt; 4, safe to ignore</span>
  a[i] = a[i<span class="dv">-1</span>]; <span class="co">// 1 &lt; 4, stays on dependency graph</span>
}
<h3 id="clustering">Clustering</h3>

<p>Using the graph, the optimizer can then cluster the <a href="strongly_connected_components" title="wikilink">strongly connected components</a> (SCC) and separate vectorizable statements from the rest.</p>

<p>For example, consider a program fragment containing three statement groups inside a loop: (SCC1+SCC2), SCC3 and SCC4, in that order, in which only the second group (SCC3) can be vectorized. The final program will then contain three loops, one for each group, with only the middle one vectorized. The optimizer cannot join the first with the last without violating statement execution order, which would invalidate the necessary guarantees.</p>
<h3 id="detecting-idioms">Detecting idioms</h3>

<p>Some non-obvious dependencies can be further optimized based on specific idioms.</p>

<p>For instance, the following self-data-dependencies can be vectorized because the value of the right-hand values (<a href="Left-hand_side_and_right-hand_side_of_an_equation" title="wikilink">RHS</a>) are fetched and then stored on the left-hand value, so there is no way the data will change within the assignment.</p>
<div class="sourceCode"><pre class="sourceCode C"><code class="sourceCode c">a[i] = a[i] + a[i<span class="dv">+1</span>];</code></pre></div>

<p>Self-dependence by scalars can be vectorized by <a href="variable_elimination" title="wikilink">variable elimination</a>.</p>
<h2 id="general-framework">General framework</h2>

<p>The general framework for loop vectorization is split into four stages:</p>
<ul>
<li><strong>Prelude</strong>: Where the loop-independent variables are prepared to be used inside the loop. This normally involves moving them to vector registers with specific patterns that will be used in vector instructions. This is also the place to insert the run-time dependence check. If the check decides vectorization is not possible, branch to <strong>Cleanup</strong>.</li>
<li><strong>Loop(s)</strong>: All vectorized (or not) loops, separated by SCCs clusters in order of appearance in the original code.</li>
<li><strong>Postlude</strong>: Return all loop-independent variables, inductions and reductions.</li>
<li><strong>Cleanup</strong>: Implement plain (non-vectorized) loops for iterations at the end of a loop that are not a multiple of the vector size or for when run-time checks prohibit vector processing.</li>
</ul>
<h2 id="run-time-vs.-compile-time">Run-time vs. compile-time</h2>

<p>Some vectorizations cannot be fully checked at compile time. Compile-time optimization requires an explicit array index. Library functions can also defeat optimization if the data they process is supplied by the caller. Even in these cases, run-time optimization can still vectorize loops on-the-fly.</p>

<p>This run-time check is made in the <strong>prelude</strong> stage and directs the flow to vectorized instructions if possible, otherwise reverting to standard processing, depending on the variables that are being passed on the registers or scalar variables.</p>

<p>The following code can easily be vectorized on compile time, as it doesn't have any dependence on external parameters. Also, the language guarantees that neither will occupy the same region in memory as any other variable, as they are local variables and live only in the execution <a href="stack_(data_structure)" title="wikilink">stack</a>.</p>
<div class="sourceCode"><pre class="sourceCode C"><code class="sourceCode c"><span class="dt">int</span> a[<span class="dv">128</span>];
<span class="dt">int</span> b[<span class="dv">128</span>];
<span class="co">// initialize b</span>

<span class="kw">for</span> (i = <span class="dv">0</span>; i&lt;<span class="dv">128</span>; i++)
  a[i] = b[i] + <span class="dv">5</span>;</code></pre></div>

<p>On the other hand, the code below has no information on memory positions, because the references are <a href="pointer_(computer_programming)" title="wikilink">pointers</a> and the memory they point to lives in the <a href="Dynamic_memory_allocation" title="wikilink">heap</a>.</p>
<div class="sourceCode"><pre class="sourceCode C"><code class="sourceCode c"><span class="dt">int</span> *a = malloc(<span class="dv">128</span>*<span class="kw">sizeof</span>(<span class="dt">int</span>));
<span class="dt">int</span> *b = malloc(<span class="dv">128</span>*<span class="kw">sizeof</span>(<span class="dt">int</span>));
<span class="co">// initialize b</span>

<span class="kw">for</span> (i = <span class="dv">0</span>; i&lt;<span class="dv">128</span>; i++, a++, b++)
  *a = *b + <span class="dv">5</span>;
<span class="co">// ... </span>
<span class="co">// ...</span>
<span class="co">// ...</span>
free(b);
free(a);</code></pre></div>

<p>A quick run-time check on the <a href="memory_address" title="wikilink">address</a> of both <em>a</em> and <em>b</em>, plus the loop iteration space (128) is enough to tell if the arrays overlap or not, thus revealing any dependencies.</p>

<p>There exist some tools to dynamically analyze existing applications to assess the inherent latent potential for SIMD parallelism, exploitable through further compiler advances and/or via manual code changes. <a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="techniques">Techniques</h2>

<p>An example would be a program to multiply two vectors of numeric data. A scalar approach would be something like:</p>
<div class="sourceCode"><pre class="sourceCode C"><code class="sourceCode c"> <span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i++)
    C[i] = A[i]*B[i];</code></pre></div>

<p>This could be vectorized to look something like:</p>
<div class="sourceCode"><pre class="sourceCode C"><code class="sourceCode c">  <span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i+=<span class="dv">4</span>)
     C[i:i<span class="dv">+3</span>] = A[i:i<span class="dv">+3</span>]*B[i:i<span class="dv">+3</span>];</code></pre></div>

<p>Here, C[i:i+3] represents the four array elements from C[i] to C[i+3] and the vector processor can perform four operations for a single vector instruction. Since the four vector operations complete in roughly the same time as one scalar instruction, the vector approach can run up to four times faster than the original code.</p>

<p>There are two distinct compiler approaches: one based on the conventional vectorization technique and the other based on <a href="loop_unwinding" title="wikilink">loop unrolling</a>.</p>
<h3 id="loop-level-automatic-vectorization">Loop-level automatic vectorization</h3>

<p>This technique, used for conventional vector machines, tries to find and exploit <a class="uri" href="SIMD" title="wikilink">SIMD</a> parallelism at the loop level. It consists of two major steps as follows.</p>
<ol>
<li>Find an innermost loop that can be vectorized</li>
<li>Transform the loop and generate vector codes</li>
</ol>

<p>In the first step, the compiler looks for obstacles that can prevent vectorization. A major obstacle for vectorization is <a href="Instruction_level_parallelism" title="wikilink">true data dependency</a> shorter than the vector length. Other obstacles include function calls and short iteration counts.</p>

<p>Once the loop is determined to be vectorizable, the loop is stripmined by the vector length and each scalar instruction within the loop body is replaced with the corresponding vector instruction. Below, the component transformations for this step are shown using the above example.</p>
<ul>
<li>After stripmining</li>
</ul>
<div class="sourceCode"><pre class="sourceCode C"><code class="sourceCode c"><span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i+=<span class="dv">4</span>)
    <span class="kw">for</span> (ii = <span class="dv">0</span>; ii &lt; <span class="dv">4</span>; ii++)
       C[i+ii] = A[i+ii]*B[i+ii];</code></pre></div>
<ul>
<li>After loop distribution using temporary arrays</li>
</ul>
<div class="sourceCode"><pre class="sourceCode C"><code class="sourceCode c">  <span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i+=<span class="dv">4</span>)
  {
    <span class="kw">for</span> (ii = <span class="dv">0</span>; ii &lt; <span class="dv">4</span>; ii++) tA[ii] = A[i+ii];
    <span class="kw">for</span> (ii = <span class="dv">0</span>; ii &lt; <span class="dv">4</span>; ii++) tB[ii] = B[i+ii];
    <span class="kw">for</span> (ii = <span class="dv">0</span>; ii &lt; <span class="dv">4</span>; ii++) tC[ii] = tA[ii]*tB[ii];
    <span class="kw">for</span> (ii = <span class="dv">0</span>; ii &lt; <span class="dv">4</span>; ii++) C[i+ii] = tC[ii];
  }</code></pre></div>
<ul>
<li>After replacing with vector codes</li>
</ul>
<div class="sourceCode"><pre class="sourceCode C"><code class="sourceCode c"><span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i+=<span class="dv">4</span>)
  {
    vA = vec_ld( &amp;A;[i] );
    vB = vec_ld( &amp;B;[i] );
    vC = vec_mul( vA, vB );
    vec_st( vC, &amp;C;[i] );
  }</code></pre></div>
<h3 id="basic-block-level-automatic-vectorization">Basic block level automatic vectorization</h3>

<p>This relatively new technique specifically targets modern SIMD architectures with short vector lengths.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Although loops can be unrolled to increase the amount of SIMD parallelism in basic blocks, this technique exploits SIMD parallelism within basic blocks rather than loops. The two major steps are as follows.</p>
<ol>
<li>The innermost loop is unrolled by a factor of the vector length to form a large loop body.</li>
<li>Isomorphic scalar instructions (that perform the same operation) are packed into a vector instruction if dependencies do not prevent doing so.</li>
</ol>

<p>To show step-by-step transformations for this approach, the same example is used again.</p>
<ul>
<li>After loop unrolling (by the vector length, assumed to be 4 in this case)</li>
</ul>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i+=<span class="dv">4</span>)
  {
     sA0 = ld( &amp;A;[i<span class="dv">+0</span>] );
     sB0 = ld( &amp;B;[i<span class="dv">+0</span>] );
     sC0 = sA0 * sB0;
     st( sC0, &amp;C;[i<span class="dv">+0</span>] );
           ...
     sA3 = ld( &amp;A;[i<span class="dv">+3</span>] );
     sB3 = ld( &amp;B;[i<span class="dv">+3</span>] );
     sC3 = sA3 * sB3;
     st( sC3, &amp;C;[i<span class="dv">+3</span>] );
  }</code></pre></div>
<ul>
<li>After packing</li>
</ul>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i+=<span class="dv">4</span>)
  {
     (sA0,sA1,sA2,sA3) = ld( &amp;A;[i<span class="dv">+0</span>:i<span class="dv">+3</span>] );
     (sB0,sB1,sB2,sB3) = ld( &amp;B;[i<span class="dv">+0</span>:i<span class="dv">+3</span>] );
     (sC0,sC1,sC2,sC3) = (sA0,sA1,sA2,sA3) * (sB0,sB1,sB2,sB3);
     st( (sC0,sC1,sC2,sC3), &amp;C;[i<span class="dv">+0</span>:i<span class="dv">+3</span>] );
  }</code></pre></div>
<ul>
<li>After code generation</li>
</ul>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i+=<span class="dv">4</span>)
  {
    vA = vec_ld( &amp;A;[i] );
    vB = vec_ld( &amp;B;[i] );
    vC = vec_mul( vA, vB );
    vec_st( vC, &amp;C;[i] );
  }</code></pre></div>

<p>Here, sA1, sB1, ... represent scalar variables and vA, vB, and vC represent vector variables.</p>

<p>Most automatically vectorizing commercial compilers use the conventional loop-level approach except the IBM XL Compiler,<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> which uses both.</p>
<h3 id="in-the-presence-of-control-flow">In the presence of control flow</h3>

<p>The presence of if-statements in the loop body requires the execution of instructions in all control paths to merge the multiple values of a variable. One general approach is to go through a sequence of code transformations: predication → vectorization(using one of the above methods) → remove vector predicates → remove scalar predicates.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> If the following code is used as an example to show these transformations;</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i++)
     <span class="kw">if</span> (A[i] &gt; <span class="dv">0</span>)
       C[i] = B[i];
     <span class="kw">else</span>
       D[i] = D[i<span class="dv">-1</span>];</code></pre></div>
<ul>
<li>After predication</li>
</ul>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i++)
  {
     P = A[i] &gt; <span class="dv">0</span>;
     NP = !P;
     C[i] = B[i];     (P)
     D[i] = D[i<span class="dv">-1</span>];   (NP)
  }</code></pre></div>

<p>where (P) denotes a predicate guarding the statement.</p>
<ul>
<li>After vectorization</li>
</ul>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i+=<span class="dv">4</span>)
  {
     vP  = A[i:i<span class="dv">+3</span>] &gt; (<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>);
     vNP = vec_not(vP);
     C[i:i<span class="dv">+3</span>] = B[i:i<span class="dv">+3</span>];     (vP)
     (NP1,NP2,NP3,NP4) = vNP;
     D[i<span class="dv">+3</span>] = D[i<span class="dv">+2</span>];         (NP4)
     D[i<span class="dv">+2</span>] = D[i<span class="dv">+1</span>];         (NP3)
     D[i<span class="dv">+1</span>] = D[i];           (NP2)
     D[i]     = D[i<span class="dv">-1</span>];         (NP1)
  }</code></pre></div>
<ul>
<li>After removing vector predicates</li>
</ul>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i+=<span class="dv">4</span>)
  {
     vP  = A[i:i<span class="dv">+3</span>] &gt; (<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>);
     vNP = vec_not(vP);
     C[i:i<span class="dv">+3</span>] = vec_sel(C[i:i<span class="dv">+3</span>],B[i:i<span class="dv">+3</span>],vP);
     (NP1,NP2,NP3,NP4) = vNP;
     D[i<span class="dv">+3</span>] = D[i<span class="dv">+2</span>];         (NP4)
     D[i<span class="dv">+2</span>] = D[i<span class="dv">+1</span>];         (NP3)
     D[i<span class="dv">+1</span>] = D[i];           (NP2)
     D[i]     = D[i<span class="dv">-1</span>];         (NP1)
  }</code></pre></div>
<ul>
<li>After removing scalar predicates</li>
</ul>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i+=<span class="dv">4</span>)
  {
     vP  = A[i:i<span class="dv">+3</span>] &gt; (<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>);
     vNP = vec_not(vP);
     C[i:i<span class="dv">+3</span>] = vec_sel(C[i:i<span class="dv">+3</span>],B[i:i<span class="dv">+3</span>],vP);
     (NP1,NP2,NP3,NP4) = vNP;
     <span class="kw">if</span> (NP4) D[i<span class="dv">+3</span>] = D[i<span class="dv">+2</span>];
     <span class="kw">if</span> (NP3) D[i<span class="dv">+2</span>] = D[i<span class="dv">+1</span>];
     <span class="kw">if</span> (NP2) D[i<span class="dv">+1</span>] = D[i];
     <span class="kw">if</span> (NP1) D[i]   = D[i<span class="dv">-1</span>];
  }</code></pre></div>
<h3 id="reducing-vectorization-overhead-in-the-presence-of-control-flow">Reducing vectorization overhead in the presence of control flow</h3>

<p>Having to execute the instructions in all control paths in vector code has been one of the major factors that slow down the vector code with respect to the scalar baseline. The more complex the control flow becomes and the more instructions are bypassed in the scalar code the larger the vectorization overhead grows. To reduce this vectorization overhead, vector branches can be inserted to bypass vector instructions similar to the way scalar branches bypass scalar instructions.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> Below, AltiVec predicates are used to show how this can be achieved.</p>
<ul>
<li>Scalar baseline (original code)</li>
</ul>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i++)
  {
     <span class="kw">if</span> (A[i] &gt; <span class="dv">0</span>)
     {
       C[i] = B[i];
       <span class="kw">if</span> (B[i] &lt; <span class="dv">0</span>)
         D[i] = E[i];
     }
  }</code></pre></div>
<ul>
<li>After vectorization in the presence of control flow</li>
</ul>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i+=<span class="dv">4</span>)
  {
     vPA = A[i:i<span class="dv">+3</span>] &gt; (<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>);
     C[i:i<span class="dv">+3</span>] = vec_sel(C[i:i<span class="dv">+3</span>],B[i:i<span class="dv">+3</span>],vPA);
     vT = B[i:i<span class="dv">+3</span>] &lt; (<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>);
     vPB = vec_sel((<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), vT, vPA);
     D[i:i<span class="dv">+3</span>] = vec_sel(D[i:i<span class="dv">+3</span>],E[i:i<span class="dv">+3</span>],vPB);
  }</code></pre></div>
<ul>
<li>After inserting vector branches</li>
</ul>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="kw">for</span> (i = <span class="dv">0</span>; i &lt; <span class="dv">1024</span>; i+=<span class="dv">4</span>)
     <span class="kw">if</span> (vec_any_gt(A[i:i<span class="dv">+3</span>],(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)))
     {
        vPA = A[i:i<span class="dv">+3</span>] &gt; (<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>);
        C[i:i<span class="dv">+3</span>] = vec_sel(C[i:i<span class="dv">+3</span>],B[i:i<span class="dv">+3</span>],vPA);
        vT = B[i:i<span class="dv">+3</span>] &lt; (<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>);
        vPB = vec_sel((<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), vT, vPA);
        <span class="kw">if</span> (vec_any_ne(vPB,(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)))
           D[i:i<span class="dv">+3</span>] = vec_sel(D[i:i<span class="dv">+3</span>],E[i:i<span class="dv">+3</span>],vPB);
     }</code></pre></div>

<p>There are two things to note in the final code with vector branches; First, the predicate defining instruction for vPA is also included within the body of the outer vector branch by using vec_any_gt. Second, the profitability of the inner vector branch for vPB depends on the conditional probability of vPB having false values in all fields given vPA has false values in all fields.</p>

<p>Consider an example where the outer branch in the scalar baseline is always taken, bypassing most instructions in the loop body. The intermediate case above, without vector branches, executes all vector instructions. The final code, with vector branches, executes both the comparison and the branch in vector mode, potentially gaining performance over the scalar baseline.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Chaining_(vector_processing)" title="wikilink">Chaining (vector processing)</a></li>
</ul>
<h2 id="references">References</h2>

<p><a class="uri" href="de:Vektorisierung" title="wikilink">de:Vektorisierung</a> <a class="uri" href="lt:Vektorizacija" title="wikilink">lt:Vektorizacija</a> <a class="uri" href="ja:ベクトル化" title="wikilink">ja:ベクトル化</a>"</p>

<p><a href="Category:Compiler_optimizations" title="wikilink">Category:Compiler optimizations</a> <a href="Category:Distributed_computing_problems" title="wikilink">Category:Distributed computing problems</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">[<a class="uri" href="http://dl.acm.org/citation.cfm?id=2254108&amp;CFID">http://dl.acm.org/citation.cfm?id=2254108&amp;CFID;</a>;=305005555&amp;CFTOKEN;=26320981]<a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
</ol>
</section>


