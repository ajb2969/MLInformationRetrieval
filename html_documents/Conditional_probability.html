<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="939">Conditional probability</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Conditional probability</h1>
<hr/>
<p>In <a href="probability_theory" title="wikilink">probability theory</a>, a <strong>conditional probability</strong> measures the <a class="uri" href="probability" title="wikilink">probability</a> of an <a href="Event_(probability_theory)" title="wikilink">event</a> given that (by assumption, presumption, assertion or evidence) another event has occurred.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> If the event of interest is <em>A</em> and the event <em>B</em> is known or assumed to have occurred, "the conditional probability of <em>A</em> given <em>B</em>", or "the probability of <em>A</em> under the condition <em>B</em>", is usually written as <em>P</em>(<em>A</em>|<em>B</em>), or sometimes <em>P</em><span class="LaTeX">$_{ B }$</span>(<em>A</em>). For example, the probability that any given person has a cough on any given day may be only 5%. But if we know or assume that the person has a <a href="Common_cold" title="wikilink">cold</a>, then they are much more likely to be coughing. The conditional probability of coughing given that you have a cold might be a much higher 75%.</p>
<p>The concept of conditional probability is one of the most fundamental and one of the most important concepts in probability theory.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> But conditional probabilities can be quite slippery and require careful interpretation.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> For example, there need not be a causal or temporal relationship between <em>A</em> and <em>B</em>.</p>
<p><em>P</em>(<em>A</em>|<em>B</em>) may or may not be equal to <em>P</em>(<em>A</em>) (the unconditional probability of <em>A</em>). If <em>P</em>(<em>A</em>|<em>B</em>) = <em>P</em>(<em>A</em>), then <em>A</em> and <em>B</em> are said to be independent. Also, in general, <em>P</em>(<em>A</em>|<em>B</em>) (the conditional probability of A given B) is not equal to <em>P</em>(<em>B</em>|<em>A</em>). For example, if you have cancer you might have a 90% chance of testing positive for cancer, but if you test positive for cancer you might have only a 10% chance of actually having cancer because cancer is very rare. Falsely equating the two probabilities causes various errors of reasoning such as the <a href="base_rate_fallacy" title="wikilink">base rate fallacy</a>. Conditional probabilities can be correctly reversed using <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a>.</p>
<h2 id="definition">Definition</h2>
<figure><b>(Figure)</b>
<figcaption>Illustration of conditional probabilities with an <a href="Euler_diagram" title="wikilink">Euler diagram</a>. The unconditional <a class="uri" href="probability" title="wikilink">probability</a> <em>P</em>(<em>A</em>) = 0.52. However, the conditional probability <em>P</em>(<em>A</em>|<em>B</em><span class="LaTeX">$_{ 1 }$</span>) = 1, <em>P</em>(<em>A</em>|<em>B</em><span class="LaTeX">$_{ 2 }$</span>) ≈ 0.75, and <em>P</em>(<em>A</em>|<em>B</em><span class="LaTeX">$_{ 3 }$</span>) = 0.</figcaption>
</figure>
<figure><b>(Figure)</b>
<figcaption>On a <a href="Tree_diagram_(probability_theory)" title="wikilink">tree diagram</a>, branch probabilities are conditional on the event associated with the parent node.</figcaption>
</figure>
<figure><b>(Figure)</b>
<figcaption>Venn Pie Chart describing conditional probabilities</figcaption>
</figure>
<h3 id="conditioning-on-an-event">Conditioning on an event</h3>
<h4 id="kolmogorov-definition">Kolmogorov definition</h4>
<p>Given two <a href="event_(probability_theory)" title="wikilink">events</a> <em>A</em> and <em>B</em> from the sigma-field of a probability space with <em>P</em>(<em>B</em>) > 0, the conditional probability of <em>A</em> given <em>B</em> is defined as the <a class="uri" href="quotient" title="wikilink">quotient</a> of the probability of the joint of events <em>A</em> and <em>B</em>, and the <a class="uri" href="probability" title="wikilink">probability</a> of <em>B</em>:</p>
<p><span class="LaTeX">$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$</span></p>
<p>This may be visualized as restricting the sample space to <em>B</em>. The logic behind this equation is that if the outcomes are restricted to <em>B</em>, this set serves as the new sample space.</p>
<p>Note that this is a definition but not a theoretical result. We just denote the quantity <em><span class="LaTeX">$P(A\cap B)/P(B)$</span></em> as <em><span class="LaTeX">$P(A|B)$</span></em> and call it the conditional probability of <em>A</em> given <em>B</em>.</p>
<h4 id="as-an-axiom-of-probability">As an axiom of probability</h4>
<p>Some authors, such as <a href="Bruno_de_Finetti" title="wikilink">De Finetti</a>, prefer to introduce conditional probability as an <a href="Probability_axioms" title="wikilink">axiom of probability</a>:</p>
<p><span class="LaTeX">$$P(A \cap B) = P(A|B)P(B)$$</span></p>
<p>Although mathematically equivalent, this may be preferred philosophically; under major <a href="probability_interpretations" title="wikilink">probability interpretations</a> such as the <a href="Subjective_probability" title="wikilink">subjective theory</a>, conditional probability is considered a primitive entity. Further, this "multiplication axiom" introduces a symmetry with the summation axiom for <a href="mutually_exclusive_events" title="wikilink">mutually exclusive events</a>:<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<p><span class="LaTeX">$$P(A \cup B) = P(A) + P(B) - \cancelto0{P(A \cap B)}$$</span></p>
<h3 id="definition-with-σ-algebra">Definition with σ-algebra</h3>
<p>If <em>P</em>(<em>B</em>) = 0, then the simple definition of <em>P</em>(<em>A</em>|<em>B</em>) is <a href="defined_and_undefined" title="wikilink">undefined</a>. However, it is possible to define a conditional probability with respect to a <a href="sigma_algebra" title="wikilink">σ-algebra</a> of such events (such as those arising from a <a href="continuous_random_variable" title="wikilink">continuous random variable</a>).</p>
<p>For example, if <em>X</em> and <em>Y</em> are non-degenerate and jointly continuous random variables with density <em>ƒ</em><span class="LaTeX">$_{ X , Y }$</span>(<em>x</em>, <em>y</em>) then, if <em>B</em> has positive <a href="Measure_(mathematics)" title="wikilink">measure</a>,</p>
<p><span class="LaTeX">$$P(X \in A \mid Y \in B) =
\frac{\int_{y\in B}\int_{x\in A} f_{X,Y}(x,y)\,dx\,dy}{\int_{y\in B}\int_{x\in\Omega} f_{X,Y}(x,y)\,dx\,dy} .$$</span></p>
<p>The case where <em>B</em> has zero measure can only be dealt with directly in the case that <em>B</em> = {<em>y</em><span class="LaTeX">$_{ 0 }$</span>}, representing a single point, in which case</p>
<p><span class="LaTeX">$$P(X \in A \mid Y = y_0) = \frac{\int_{x\in A} f_{X,Y}(x,y_0)\,dx}{\int_{x\in\Omega} f_{X,Y}(x,y_0)\,dx} .$$</span></p>
<p>If <em>A</em> has measure zero then the conditional probability is zero. An indication of why the more general case of zero measure cannot be dealt with in a similar way can be seen by noting that the limit, as all <em>δy</em><span class="LaTeX">$_{ i }$</span> approach zero, of</p>
<p><span class="LaTeX">$$P(X \in A \mid Y \in \cup_i[y_i,y_i+\delta y_i]) \approxeq
\frac{\sum_{i} \int_{x\in A} f_{X,Y}(x,y_i)\,dx\,\delta y_i}{\sum_{i}\int_{x\in\Omega} f_{X,Y}(x,y_i) \,dx\, \delta y_i} ,$$</span> depends on their relationship as they approach zero. See <a href="conditional_expectation" title="wikilink">conditional expectation</a> for more information.</p>
<h3 id="conditioning-on-a-random-variable">Conditioning on a random variable</h3>
<p>Conditioning on an event may be generalized to conditioning on a random variable. Let <em>X</em> be a random variable; we assume for the sake of presentation that <em>X</em> is discrete, that is, <em>X</em> takes on only finitely many values <em>x</em>. Let <em>A</em> be an event. The conditional probability of <em>A</em> given <em>X</em> is defined as the random variable, written <em>P(A|X)</em>, that takes on the value</p>
<p><span class="LaTeX">$$P(A\mid X=x)$$</span></p>
<p>whenever</p>
<p><span class="LaTeX">$$X=x.$$</span></p>
<p>More formally:</p>
<p><span class="LaTeX">$$P(A|X)(\omega)=P(A\mid X=X(\omega)) .$$</span></p>
<p>The conditional probability <em>P</em>(<em>A</em>|<em>X</em>) is a function of <em>X</em>, e.g., if the function <em>g</em> is defined as</p>
<p><span class="LaTeX">$$g(x)= P(A\mid X=x)$$</span>,</p>
<p>then</p>
<p><span class="LaTeX">$$P(A|X) =g\circ X$$</span></p>
<p>Note that <em>P</em>(<em>A</em>|<em>X</em>) and <em>X</em> are now both <a href="random_variable" title="wikilink">random variables</a>. From the <a href="law_of_total_probability" title="wikilink">law of total probability</a>, the <a href="expected_value" title="wikilink">expected value</a> of <em>P</em>(<em>A</em>|<em>X</em>) is equal to the unconditional <a class="uri" href="probability" title="wikilink">probability</a> of <em>A</em>.</p>
<h2 id="example">Example</h2>
<p>Suppose that somebody secretly rolls two fair six-sided <a class="uri" href="dice" title="wikilink">dice</a>, and we must predict the outcome.</p>
<ul>
<li>Let <em>A</em> be the value rolled on <a href="dice" title="wikilink">die</a> 1</li>
<li>Let <em>B</em> be the value rolled on <a href="dice" title="wikilink">die</a> 2</li>
</ul>
<p><strong>''What is the probability that <em>A</em> = 2?</strong>''</p>
<p>Table 1 shows the <a href="sample_space" title="wikilink">sample space</a> of 36 outcomes</p>
<p>Clearly, <em>A</em> = 2 in exactly 6 of the 36 outcomes, thus <em>P</em>(<em>A</em>=2) = <span class="LaTeX">${6}/{36}$</span> = <span class="LaTeX">${1}/{6}$</span>.</p>
<dl>
<dd>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><p>Table 1</p></td>
<td style="text-align: left;"><p>+</p></td>
<td style="text-align: left;"><p>B=1</p></td>
<td style="text-align: left;"><p>2</p></td>
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>A=1</p></td>
<td style="text-align: left;"><p>2</p></td>
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>2</p></td>
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
<td style="text-align: left;"><p>9</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
<td style="text-align: left;"><p>9</p></td>
<td style="text-align: left;"><p>10</p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
<td style="text-align: left;"><p>9</p></td>
<td style="text-align: left;"><p>10</p></td>
<td style="text-align: left;"><p>11</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
<td style="text-align: left;"><p>9</p></td>
<td style="text-align: left;"><p>10</p></td>
<td style="text-align: left;"><p>11</p></td>
<td style="text-align: left;"><p>12</p></td>
</tr>
</tbody>
</table>
</dd>
</dl>
<p><strong>''Suppose it is revealed that <em>A</em>+<em>B</em> ≤ 5</strong>''</p>
<p>What is the probability <em>A</em>+<em>B</em> ≤ 5 ?</p>
<p>Table 2 shows that <em>A</em>+<em>B</em> ≤ 5 for exactly 10 of the same 36 outcomes, thus <em>P</em>(<em>A</em>+<em>B</em> ≤ 5) = <span class="LaTeX">${10}/{36}$</span></p>
<dl>
<dd>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><p>Table 2</p></td>
<td style="text-align: left;"><p>+</p></td>
<td style="text-align: left;"><p>B=1</p></td>
<td style="text-align: left;"><p>2</p></td>
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>A=1</p></td>
<td style="text-align: left;"><p>2</p></td>
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>2</p></td>
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
<td style="text-align: left;"><p>9</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
<td style="text-align: left;"><p>9</p></td>
<td style="text-align: left;"><p>10</p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
<td style="text-align: left;"><p>9</p></td>
<td style="text-align: left;"><p>10</p></td>
<td style="text-align: left;"><p>11</p></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
<td style="text-align: left;"><p>9</p></td>
<td style="text-align: left;"><p>10</p></td>
<td style="text-align: left;"><p>11</p></td>
<td style="text-align: left;"><p>12</p></td>
</tr>
<tr class="odd">
</tr>
</tbody>
</table>
</dd>
</dl>
<p><strong>''What is the probability that <em>A</em> = 2 <em>given that</em> <em>A</em>+<em>B</em> ≤ 5 ?</strong>''</p>
<p>Table 3 shows that for 3 of these 10 outcomes, <em>A</em> = 2</p>
<p>Thus, the conditional probability <em>P</em>(<em>A</em>=2 | <em>A</em>+<em>B</em> ≤ 5) = <span class="LaTeX">${3}/{10}$</span> = 0.3.</p>
<dl>
<dd>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><p>Table 3</p></td>
<td style="text-align: left;"><p>+</p></td>
<td style="text-align: left;"><p>B=1</p></td>
<td style="text-align: left;"><p>2</p></td>
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>A=1</p></td>
<td style="text-align: left;"><p>2</p></td>
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>2</p></td>
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>3</p></td>
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
<td style="text-align: left;"><p>9</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>4</p></td>
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
<td style="text-align: left;"><p>9</p></td>
<td style="text-align: left;"><p>10</p></td>
</tr>
<tr class="even">
<td style="text-align: left;"><p>5</p></td>
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
<td style="text-align: left;"><p>9</p></td>
<td style="text-align: left;"><p>10</p></td>
<td style="text-align: left;"><p>11</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><p>6</p></td>
<td style="text-align: left;"><p>7</p></td>
<td style="text-align: left;"><p>8</p></td>
<td style="text-align: left;"><p>9</p></td>
<td style="text-align: left;"><p>10</p></td>
<td style="text-align: left;"><p>11</p></td>
<td style="text-align: left;"><p>12</p></td>
</tr>
</tbody>
</table>
</dd>
</dl>
<h2 id="use-in-inference">Use in inference</h2>
<p>In statistical inference, the conditional probability is an update of the probability of an <a href="Event_(probability_theory)" title="wikilink">event</a> based on new information.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> Incorporating the new information can be done as follows <a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<ul>
<li>Let <em>A</em> the event of interest be in the <a href="sample_space" title="wikilink">sample space</a>, say (<em>X</em>,<em>P</em>).</li>
<li>The occurrence of the event <em>A</em> knowing that event <em>B</em> has or will have occurred, means the occurrence of <em>A</em> as it is restricted to <em>B</em>, i.e. <span class="LaTeX">$A \cap B$</span>.</li>
<li>Without the knowledge of the occurrence of <em>B</em>, the information about the occurrence of <em>A</em> would simply be <em>P</em>(<em>A</em>)</li>
<li>The probability of <em>A</em> knowing that event <em>B</em> has or will have occurred, will be the probability of <span class="LaTeX">$A \cap B$</span> compared with <em>P</em>(<em>B</em>), the probability <em>B</em> has occurred.</li>
<li>This results in <em>P</em>(<em>A</em>|<em>B</em>) = <em>P</em>(<em>A</em> <span class="LaTeX">$\cap$</span> <em>B</em>)/<em>P</em>(<em>B</em>) whenever <em>P</em>(<em>B</em>)>0 and 0 otherwise.</li>
</ul>
<p>Note: This approach results in a probability measure that is consistent with the original probability measure and satisfies all the <a href="Probability_axioms" title="wikilink">Kolmogorov Axioms</a>. This conditional probability measure also could have resulted by assuming that the relative magnitude of the probability of A with respect to X will be preserved with respect to B (cf. <a href="#Formal_derivation" title="wikilink"> a Formal Derivation</a> below).</p>
<p>Note: The phraseology "evidence" or "information" is generally used in the <a href="Bayesian_probability" title="wikilink">Bayesian interpretation of probability</a>. The conditioning event is interpreted as evidence for the conditioned event. That is, <em>P</em>(<em>A</em>) is the probability of <em>A</em> before accounting for evidence <em>E</em>, and <em>P</em>(<em>A</em>|<em>E</em>) is the probability of <em>A</em> after having accounted for evidence <em>E</em> or after having updated <em>P</em>(<em>A</em>). This is consistent with the frequentist interpretation, which presumably is the first definition given above.</p>
<h2 id="statistical-independence">Statistical independence</h2>
<p>Events <em>A</em> and <em>B</em> are defined to be <a href="Independence_(probability_theory)" title="wikilink">statistically independent</a> if:</p>
<p><span class="LaTeX">$$\begin{align}
             P(A \cap B) &= P(A) P(B) \\
  \Leftrightarrow P(A|B) &= P(A) \\
  \Leftrightarrow P(B|A) &= P(B)
\end{align}$$</span></p>
<p>That is, the occurrence of <em>A</em> does not affect the probability of <em>B</em>, and vice versa. Although the derived forms may seem more intuitive, they are not the preferred definition as the conditional probabilities may be undefined if <em>P</em>(<em>A</em>) or <em>P</em>(<em>B</em>) are 0, and the preferred definition is symmetrical in <em>A</em> and <em>B</em>.</p>
<h2 id="common-fallacies">Common fallacies</h2>
<dl>
<dd><em>These fallacies should not be confused with Robert K. Shope's 1978 <a href="http://lesswrong.com/r/discussion/lw/9om/the_conditional_fallacy_in_contemporary_philosophy/">"conditional fallacy"</a>, which deals with counterfactual examples that <a href="beg_the_question" title="wikilink">beg the question</a>.</em>
</dd>
</dl>
<h3 id="assuming-conditional-probability-is-of-similar-size-to-its-inverse">Assuming conditional probability is of similar size to its inverse</h3>
<p> In general, it cannot be assumed that <em>P</em>(<em>A</em>|<em>B</em>) ≈ <em>P</em>(<em>B</em>|<em>A</em>). This can be an insidious error, even for those who are highly conversant with statistics.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> The relationship between <em>P</em>(<em>A</em>|<em>B</em>) and <em>P</em>(<em>B</em>|<em>A</em>) is given by <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a>:</p>
<p><span class="LaTeX">$$P(B|A) = \frac{P(A|B) P(B)}{P(A)}.$$</span></p>
<p>That is, <em>P</em>(<em>A</em>|<em>B</em>) ≈ <em>P</em>(<em>B</em>|<em>A</em>) only if <em>P</em>(<em>B</em>)/<em>P</em>(<em>A</em>) ≈ 1, or equivalently, <em>P</em>(<em>A</em>) ≈ <em>P</em>(<em>B</em>).</p>
<p>Alternatively, noting that <em>A</em> ∩ <em>B</em> = <em>B</em> ∩ <em>A</em>, and applying conditional probability:</p>
<p><span class="LaTeX">$$P(A|B)P(B) = P(A \cap B) = P(B \cap A) = P(B|A)P(A)$$</span></p>
<p>Rearranging gives the result.</p>
<h3 id="assuming-marginal-and-conditional-probabilities-are-of-similar-size">Assuming marginal and conditional probabilities are of similar size</h3>
<p>In general, it cannot be assumed that <em>P</em>(<em>A</em>) ≈ <em>P</em>(<em>A</em>|<em>B</em>). These probabilities are linked through the <a href="law_of_total_probability" title="wikilink">law of total probability</a>:</p>
<p><span class="LaTeX">$$P(A) = \sum_n P(A \cap B_n) = \sum_n P(A|B_n)P(B_n)$$</span>.</p>
<p>where the events <span class="LaTeX">$(B_n)$</span> form a countable <a href="Partition_of_a_set" title="wikilink">partition</a> of <span class="LaTeX">$A$</span>.</p>
<p>This fallacy may arise through <a href="selection_bias" title="wikilink">selection bias</a>.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> For example, in the context of a medical claim, let <em>S</em><span class="LaTeX">$_{ C }$</span> be the event that a <a href="sequelae" title="wikilink">sequela</a> (chronic disease) <em>S</em> occurs as a consequence of circumstance (acute condition) <em>C</em>. Let <em>H</em> be the event that an individual seeks medical help. Suppose that in most cases, <em>C</em> does not cause <em>S</em> so <em>P</em>(<em>S</em><span class="LaTeX">$_{ C }$</span>) is low. Suppose also that medical attention is only sought if <em>S</em> has occurred due to <em>C</em>. From experience of patients, a doctor may therefore erroneously conclude that <em>P</em>(<em>S</em><span class="LaTeX">$_{ C }$</span>) is high. The actual probability observed by the doctor is <em>P</em>(<em>S</em><span class="LaTeX">$_{ C }$</span>|<em>H</em>).</p>
<h3 id="over--or-under-weighting-priors">Over- or under-weighting priors</h3>
<p>Not taking prior probability into account partially or completely is called <em><a href="base_rate_neglect" title="wikilink">base rate neglect</a></em>. The reverse, insufficient adjustment from the prior probability is ''<a href="conservatism_(Bayesian)" title="wikilink">conservatism</a>.</p>
<h2 id="formal-derivation">Formal derivation</h2>
<p>Formally, <em>P</em>(<em>A</em>|<em>B</em>) is defined as the probability of <em>A</em> according to a new probability function on the sample space, such that outcomes not in <em>B</em> have probability 0 and that it is consistent with all original <a href="probability_measure" title="wikilink">probability measures</a>.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a><a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<p>Let Ω be a <a href="sample_space" title="wikilink">sample space</a> with <a href="elementary_event" title="wikilink">elementary events</a> {ω}. Suppose we are told the event <em>B</em> ⊆ Ω has occurred. A new probability distribution (denoted by the conditional notation) is to be assigned on {ω} to reflect this. For events in <em>B</em>, it is reasonable to assume that the relative magnitudes of the probabilities will be preserved. For some constant scale factor α, the new distribution will therefore satisfy:</p>
<p><span class="LaTeX">$$\begin{align}
  &\text{1. }\omega \in B : P(\omega|B) = \alpha P(\omega) \\
  &\text{2. }\omega \notin B : P(\omega|B) = 0 \\
  &\text{3. }\sum_{\omega \in \Omega} {P(\omega|B)} = 1.
\end{align}$$</span></p>
<p>Substituting 1 and 2 into 3 to select α:</p>
<p><span class="LaTeX">$$\begin{align}
  \sum_{\omega \in \Omega} {P(\omega | B)} &= \sum_{\omega \in B} {\alpha P(\omega)} + \cancelto{0}{\sum_{\omega \notin B} 0} \\
                                           &= \alpha \sum_{\omega \in B} {P(\omega)} \\
                                           &= \alpha \cdot P(B) \\
                        \Rightarrow \alpha &= \frac{1}{P(B)}
\end{align}$$</span></p>
<p>So the new probability distribution is</p>
<p><span class="LaTeX">$$\begin{align}
     \text{1. }\omega \in B&: P(\omega|B) = \frac{P(\omega)}{P(B)} \\
  \text{2. }\omega \notin B&: P(\omega| B) = 0
\end{align}$$</span></p>
<p>Now for a general event <em>A</em>,</p>
<p><span class="LaTeX">$$\begin{align}
  P(A|B) &= \sum_{\omega \in A \cap B} {P(\omega | B)} + \cancelto{0}{\sum_{\omega \in A \cap B^c} P(\omega|B)} \\
         &= \sum_{\omega \in A \cap B} {\frac{P(\omega)}{P(B)}} \\
         &= \frac{P(A \cap B)}{P(B)}
\end{align}$$</span></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Borel–Kolmogorov_paradox" title="wikilink">Borel–Kolmogorov paradox</a></li>
<li><a href="Chain_rule_(probability)" title="wikilink">Chain rule (probability)</a></li>
<li><a href="Class_membership_probabilities" title="wikilink">Class membership probabilities</a></li>
<li><a href="Conditional_probability_distribution" title="wikilink">Conditional probability distribution</a></li>
<li><a href="Conditioning_(probability)" title="wikilink">Conditioning (probability)</a></li>
<li><a href="Joint_probability_distribution" title="wikilink">Joint probability distribution</a></li>
<li><a href="Monty_Hall_problem" title="wikilink">Monty Hall problem</a></li>
<li><a href="Posterior_probability" title="wikilink">Posterior probability</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li></li>
<li><a href="F._Thomas_Bruss" title="wikilink">F. Thomas Bruss</a> Der Wyatt-Earp-Effekt oder die betörende Macht kleiner Wahrscheinlichkeiten (in German), Spektrum der Wissenschaft (German Edition of Scientific American), Vol 2, 110–113, (2007).</li>
<li><a href="http://setosa.io/conditional/">Visual explanation of conditional probability</a></li>
</ul>
<p>"</p>
<p><a href="Category:Probability_theory" title="wikilink">Category:Probability theory</a> <a href="Category:Logical_fallacies" title="wikilink">Category:Logical fallacies</a> <a class="uri" href="Category:Conditionals" title="wikilink">Category:Conditionals</a> <a href="Category:Statistical_ratios" title="wikilink">Category:Statistical ratios</a> <a href="Category:Statistical_terminology" title="wikilink">Category:Statistical terminology</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4">Gillies, Donald (2000); "Philosophical Theories of Probability"; Routledge; Chapter 4 "The subjective theory"<a href="#fnref4">↩</a></li>
<li id="fn5"></li>
<li id="fn6"></li>
<li id="fn7">Paulos, J.A. (1988) <em>Innumeracy: Mathematical Illiteracy and its Consequences</em>, Hill and Wang. ISBN 0-8090-7447-8 (p. 63 <em>et seq.</em>)<a href="#fnref7">↩</a></li>
<li id="fn8">Thomas Bruss, F; Der Wyatt Earp Effekt; Spektrum der Wissenschaft; March 2007<a href="#fnref8">↩</a></li>
<li id="fn9">George Casella and Roger L. Berger (1990), <em>Statistical Inference</em>, Duxbury Press, ISBN 0-534-11958-1 (p. 18 <em>et seq.</em>)<a href="#fnref9">↩</a></li>
<li id="fn10"><a href="http://math.dartmouth.edu/~prob/prob/prob.pdf">Grinstead and Snell's Introduction to Probability</a>, p. 134<a href="#fnref10">↩</a></li>
</ol>
</section>
</body>
</html>
