<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="886">Sparse PCA</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Sparse PCA</h1>
<hr/>

<p><strong>Sparse principal component analysis</strong> (<strong>sparse PCA</strong>) is a specialised technique used in statistical analysis and, in particular, in the analysis of <a href="multivariate_analysis" title="wikilink">multivariate</a> data sets. It extends the classic method of <a href="principal_component_analysis" title="wikilink">principal component analysis</a> (PCA) for the reduction of dimensionality of data by adding sparsity constraint on the input variables.</p>

<p>Ordinary principal component analysis (PCA) uses a <a href="vector_space" title="wikilink">vector space</a> <a class="uri" href="transform" title="wikilink">transform</a> to reduce multidimensional data sets to lower dimensions. It finds <a href="linear_combinations" title="wikilink">linear combinations</a> of input variables, and transforms them into new variables (called <strong>principal components</strong>) that correspond to directions of maximal <a class="uri" href="variance" title="wikilink">variance</a> in the data. The number of new variables created by these linear combinations is usually much lower than the number of input variables in the original dataset, while still explaining most of the variance present in the data.</p>

<p>A particular disadvantage of ordinary PCA is that the principal components are usually linear combinations of all input variables. Sparse PCA overcomes this disadvantage by finding linear combinations that contain just a few input variables.</p>
<h2 id="mathematical-formulation">Mathematical Formulation</h2>

<p>Consider a data <a href="Matrix_(mathematics)" title="wikilink">matrix</a>, <em>X</em>, where each of the <em>p</em> columns represent an input variable, and each of the <em>n</em> rows represents an independent sample from data population. We assume each column of <em>X</em> has mean zero, otherwise we can subtract column-wise mean from each element of <em>X</em>. Let <em>Î£=X<sup>T</sup>X</em> be the empirical <a href="covariance_matrix" title="wikilink">covariance matrix</a> of <em>X</em>, which has dimension <em>pÃ—p</em>. Given an integer <em>k</em> with <em>1â‰¤kâ‰¤p</em>, the sparse PCA problem can be formulated as maximizing the variance along a direction represented by vector 

<math display="inline" id="Sparse_PCA:0">
 <semantics>
  <mrow>
   <mi>v</mi>
   <mo>âˆˆ</mo>
   <msup>
    <mi>â„</mi>
    <mi>p</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>v</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>â„</ci>
     <ci>p</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   v\in\mathbb{R}^{p}
  </annotation>
 </semantics>
</math>

 while constraining its cardinality:</p>

<p>

<math display="inline" id="Sparse_PCA:1">
 <semantics>
  <mi>max</mi>
  <annotation-xml encoding="MathML-Content">
   <max></max>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\max
  </annotation>
 </semantics>
</math>


<mtpl><eqref>Eq. 1<eqref></eqref></eqref></mtpl> The first constraint specifies that <em>v</em> is a unit vector. In the second constraint, 

<math display="inline" id="Sparse_PCA:2">
 <semantics>
  <msub>
   <mrow>
    <mo>âˆ¥</mo>
    <mi>v</mi>
    <mo>âˆ¥</mo>
   </mrow>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <ci>v</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \left\|v\right\|_{0}
  </annotation>
 </semantics>
</math>

 represents the <a href="L0_norm" title="wikilink">L0 norm</a> of <em>v</em>, which is defined as the number of its non-zero components. So the second constraint specifies that the number of non-zero components in <em>v</em> is less than or equal to <em>k</em>, which is typically an integer that is much smaller than dimension <em>p</em>. The optimal value of  is known as the <em>k</em>-sparse largest <a class="uri" href="eigenvalue" title="wikilink">eigenvalue</a>.</p>

<p>If we take <em>k=p</em>, the problem reduces to the ordinary <a href="Principal_component_analysis" title="wikilink">PCA</a>, and the optimal value becomes the largest eigenvalue of covariance matrix <em>Î£</em>.</p>

<p>After finding the optimal solution <em>v</em>, we deflate <em>Î£</em> to obtain a new matrix</p>

<p>

<math display="block" id="Sparse_PCA:3">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi mathvariant="normal">Î£</mi>
     <mn>1</mn>
    </msub>
    <mo>=</mo>
    <mrow>
     <mi mathvariant="normal">Î£</mi>
     <mo>-</mo>
     <mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <msup>
         <mi>v</mi>
         <mi>T</mi>
        </msup>
        <mi mathvariant="normal">Î£</mi>
        <mi>v</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mi>v</mi>
      <msup>
       <mi>v</mi>
       <mi>T</mi>
      </msup>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>normal-Î£</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <minus></minus>
     <ci>normal-Î£</ci>
     <apply>
      <times></times>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>v</ci>
        <ci>T</ci>
       </apply>
       <ci>normal-Î£</ci>
       <ci>v</ci>
      </apply>
      <ci>v</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>v</ci>
       <ci>T</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Sigma_{1}=\Sigma-(v^{T}\Sigma v)vv^{T},
  </annotation>
 </semantics>
</math>

 and iterate this process to obtain further principal components. However, unlike PCA, sparse PCA cannot guarantee that different principal components are <a class="uri" href="orthogonal" title="wikilink">orthogonal</a>. In order to achieve orthogonality, additional constraints must be enforced.</p>

<p>Because of the cardinality constraint, the maximization problem is hard to solve exactly, especially when dimension <em>p</em> is high. In fact, the sparse PCA problem in  is <a class="uri" href="NP-hard" title="wikilink">NP-hard</a> in the strong sense.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> Several alternative approaches have been proposed, including</p>
<ul>
<li>a regression framework,<ref>{{ cite journal</ref></li>
</ul>

<p><code>|Â authorÂ =Â H.Â ZouÂ andÂ T.Â HastieÂ andÂ R.Â Tibshirani</code><br/>
<code>|Â yearÂ =Â 2006</code><br/>
<code>|Â titleÂ =Â SparseÂ principalÂ componentÂ analysis</code><br/>
<code>|Â journalÂ =Â JcgsÂ 2006Â 15(2):Â 262-286</code><br/>
<code>|Â urlÂ =Â </code><a href="http://www-stat.stanford.edu/~hastie/Papers/spc_jcgs.pdf"><code>http://www-stat.stanford.edu/~hastie/Papers/spc_jcgs.pdf</code></a><br/>
<code>}}</code></p>
<ul>
<li>a convex relaxation/semidefinite programming framework,<ref name="SDP">{{ cite journal</ref></li>
</ul>

<p><code>|Â authorÂ =Â AlexandreÂ dâ€™Aspremont,Â LaurentÂ ElÂ Ghaoui,Â MichaelÂ I.Â Jordan,Â GertÂ R.Â G.Â Lanckriet</code><br/>
<code>|Â yearÂ =Â 2007</code><br/>
<code>|Â titleÂ =Â AÂ DirectÂ FormulationÂ forÂ SparseÂ PCAÂ UsingÂ SemidefiniteÂ Programming</code><br/>
<code>|Â journalÂ =Â SIAMÂ ReviewÂ 49(3):434â€“448</code><br/>
<code>|Â urlÂ =Â </code><a href="http://www.cmap.polytechnique.fr/~aspremon/PDF/sparsesvd.pdf"><code>http://www.cmap.polytechnique.fr/~aspremon/PDF/sparsesvd.pdf</code></a><br/>
<code>}}</code></p>
<ul>
<li>a generalized power method framework<ref>{{ cite journal</ref></li>
</ul>

<p><code>|Â authorÂ =Â MichelÂ Journee,Â YuriiÂ Nesterov,Â PeterÂ Richtarik,Â RodolpheÂ Sepulchre</code><br/>
<code>|Â yearÂ =Â 2008</code><br/>
<code>|Â pagesÂ =Â 4724</code><br/>
<code>|Â titleÂ =Â GeneralizedÂ PowerÂ MethodÂ forÂ SparseÂ PrincipalÂ ComponentÂ Analysis</code><br/>
<code>|Â volumeÂ =Â 0811</code><br/>
<code>|Â journalÂ =Â COREÂ DiscussionÂ PaperÂ 2008/70,Â JournalÂ ofÂ MachineÂ LearningÂ ResearchÂ 11Â (2010)Â 517-553</code><br/>
<code>|Â urlÂ =Â </code><a href="http://jmlr.csail.mit.edu/papers/volume11/journee10a/journee10a.pdf"><code>http://jmlr.csail.mit.edu/papers/volume11/journee10a/journee10a.pdf</code></a><br/>
<code>|Â arxiv=0811.4724</code></p>

<p>}}</p>
<ul>
<li>an alternating maximization framework<ref>{{ cite journal</ref></li>
</ul>

<p><code>|Â authorÂ =Â PeterÂ Richtarik,Â MartinÂ TakacÂ andÂ S.Â DamlaÂ AhipasaogluÂ </code><br/>
<code>|Â yearÂ =Â 2012</code><br/>
<code>|Â titleÂ =Â AlternatingÂ Maximization:Â UnifyingÂ FrameworkÂ forÂ 8Â SparseÂ PCAÂ FormulationsÂ andÂ EfficientÂ ParallelÂ Codes</code><br/>
<code>|Â urlÂ =Â </code><a href="http://arxiv.org/abs/1212.4137"><code>http://arxiv.org/abs/1212.4137</code></a><br/>
<code>|Â arxiv=1212.4137</code><br/>
<code>}}</code></p>
<ul>
<li>forward/backward greedy search and exact methods using branch-and-bound techniques,<ref>{{ cite journal</ref></li>
</ul>

<p><code>|Â authorÂ =Â BabackÂ Moghaddam,Â YairÂ Weiss,Â ShaiÂ AvidanÂ </code><br/>
<code>|Â yearÂ =Â 2005Â </code><br/>
<code>|Â titleÂ =Â SpectralÂ BoundsÂ forÂ SparseÂ PCA:Â ExactÂ andÂ GreedyÂ AlgorithmsÂ </code><br/>
<code>|Â journalÂ =Â AdvancesÂ inÂ NeuralÂ InformationÂ ProcessingÂ SystemsÂ (NIPS),Â MITÂ Press</code><br/>
<code>|Â urlÂ =Â </code><a href="http://books.nips.cc/papers/files/nips18/NIPS2005_0643.pdf"><code>http://books.nips.cc/papers/files/nips18/NIPS2005_0643.pdf</code></a><br/>
<code>}}</code></p>
<ul>
<li>Bayesian formulation framework.<ref>{{ cite journal</ref></li>
</ul>

<p><code>|Â authorÂ =Â YueÂ Guan,Â JenniferÂ DyÂ </code><br/>
<code>|Â yearÂ =Â 2009Â </code><br/>
<code>|Â titleÂ =Â SparseÂ ProbabilisticÂ PrincipalÂ ComponentÂ AnalysisÂ </code><br/>
<code>|Â journalÂ =Â JournalÂ ofÂ MachineÂ LearningÂ ResearchÂ WorkshopÂ andÂ ConferenceÂ Proceedings|Â volumeÂ =Â 5:Â AISTATSÂ 2009</code><br/>
<code>|Â urlÂ =Â </code><a href="http://jmlr.csail.mit.edu/proceedings/papers/v5/guan09a/guan09a.pdf"><code>http://jmlr.csail.mit.edu/proceedings/papers/v5/guan09a/guan09a.pdf</code></a><br/>
<code>}}</code></p>
<h3 id="semidefinite-programming-relaxation">Semidefinite Programming Relaxation</h3>

<p>It has been proposed that sparse PCA can be approximated by <a href="semidefinite_programming" title="wikilink">semidefinite programming</a> (SDP).<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Let 

<math display="inline" id="Sparse_PCA:4">
 <semantics>
  <mi>V</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>V</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V
  </annotation>
 </semantics>
</math>

 be a <em>pÃ—p</em> symmetric matrix, we can rewrite the sparse PCA problem as</p>

<p>

<math display="inline" id="Sparse_PCA:5">
 <semantics>
  <mi>max</mi>
  <annotation-xml encoding="MathML-Content">
   <max></max>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\max
  </annotation>
 </semantics>
</math>


<mtpl><eqref>Eq. 2<eqref></eqref></eqref></mtpl> <em>Tr</em> is the <a href="matrix_trace" title="wikilink">matrix trace</a>, and 

<math display="inline" id="Sparse_PCA:6">
 <semantics>
  <msub>
   <mrow>
    <mo>âˆ¥</mo>
    <mi>V</mi>
    <mo>âˆ¥</mo>
   </mrow>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <ci>V</ci>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|V\|_{0}
  </annotation>
 </semantics>
</math>

 represents the non-zero elements in matrix <em>V</em>. The last line specifies that <em>V</em> has <a href="matrix_rank" title="wikilink">matrix rank</a> one and is <a href="positive_semidefinite_matrix" title="wikilink">positive semidefinite</a>. The last line means that we have 

<math display="inline" id="Sparse_PCA:7">
 <semantics>
  <mrow>
   <mi>V</mi>
   <mo>=</mo>
   <mrow>
    <mi>v</mi>
    <msup>
     <mi>v</mi>
     <mi>T</mi>
    </msup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>V</ci>
    <apply>
     <times></times>
     <ci>v</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>v</ci>
      <ci>T</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V=vv^{T}
  </annotation>
 </semantics>
</math>

, so  is equivalent to . If we drop the rank constraint and relax the cardinality contraint by a 1-norm <a class="uri" href="convex" title="wikilink">convex</a> constraint, we get a semidefinite programming relaxation, which can be solved efficiently in polynomial time:</p>

<p>

<math display="inline" id="Sparse_PCA:8">
 <semantics>
  <mi>max</mi>
  <annotation-xml encoding="MathML-Content">
   <max></max>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\max
  </annotation>
 </semantics>
</math>


<mtpl><eqref>Eq. 3<eqref></eqref></eqref></mtpl> In the second constraint, 

<math display="inline" id="Sparse_PCA:9">
 <semantics>
  <mn>ğŸ</mn>
  <annotation-xml encoding="MathML-Content">
   <cn type="integer">1</cn>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{1}
  </annotation>
 </semantics>
</math>

 is a <em>pÃ—1</em> vector of ones, and <em>|V|</em> is the matrix whose elements are the absolute values of the elements of <em>V</em>.</p>

<p>Unfortunately, the optimal solution 

<math display="inline" id="Sparse_PCA:10">
 <semantics>
  <mi>V</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>V</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V
  </annotation>
 </semantics>
</math>

 to the relaxed problem  is not guaranteed to have rank one. In that case, 

<math display="inline" id="Sparse_PCA:11">
 <semantics>
  <mi>V</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>V</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V
  </annotation>
 </semantics>
</math>

 can be truncated to retain only the dominant eigenvector.</p>
<h2 id="applications">Applications</h2>
<h3 id="financial-data-analysis">Financial Data Analysis</h3>

<p>Suppose ordinary PCA is applied to a dataset where each input variable represents a different asset, it may generate principal components that are weighted combination of all the assets. In contrast, sparse PCA would produce principal components that are weighted combination of only a few input assets, so one can easily interpret its meaning. Furthermore, if one uses a trading strategy based on these principal components, fewer assets imply less transaction costs.</p>
<h3 id="biology">Biology</h3>

<p>Consider a dataset where each input variable corresponds to a specific gene. Sparse PCA can produce a principal component that involves only a few genes, so researchers can focus on these specific genes for further analysis.</p>
<h2 id="sparse-pca-in-high-dimension">Sparse PCA in High Dimension</h2>

<p>Contemporary datasets often have the number of input variables (<em>p</em>) comparable with or even much larger than the number of samples (<em>n</em>). It has been shown that if <em>p/n</em> does not converge to zero, the classical PCA is not <a href="Consistency_(statistics)" title="wikilink">consistent</a>. In other words, if we let <em>k=p</em> in , then the optimal value does not converge to the largest eigenvalue of data population when the sample size <em>nâ†’âˆ</em>, and the optimal solution does not converge to the direction of maximum variance. But sparse PCA can retain consistency even if 

<math display="inline" id="Sparse_PCA:12">
 <semantics>
  <mrow>
   <mrow>
    <mi>p</mi>
    <mo>â‰«</mo>
    <mi>n</mi>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">much-greater-than</csymbol>
    <ci>p</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \scriptstyle p\gg n.
  </annotation>
 </semantics>
</math>

<ref name="consistency">{{ cite journal</ref></p>

<p><code>|Â authorÂ =Â Â IainÂ MÂ Johnstone,Â ArthurÂ YuÂ Lu</code><br/>
<code>|Â yearÂ =Â 2009</code><br/>
<code>|Â titleÂ =Â OnÂ ConsistencyÂ andÂ SparsityÂ forÂ PrincipalÂ ComponentsÂ AnalysisÂ inÂ HighÂ Dimensions</code><br/>
<code>|Â journalÂ =Â JournalÂ ofÂ theÂ AmericanÂ StatisticalÂ Association,Â VolumeÂ 104,Â IssueÂ 486,Â 2009</code><br/>
<code>|Â urlÂ =Â </code><a href="http://amstat.tandfonline.com/doi/abs/10.1198/jasa.2009.0121#.VIPPDl2BtC0"><code>http://amstat.tandfonline.com/doi/abs/10.1198/jasa.2009.0121#.VIPPDl2BtC0</code></a><br/>
<code>|Â doi=10.1198/jasa.2009.0121</code><br/>
<code>|Â pages=682â€“693</code><br/>
<code>}}</code></p>
<h3 id="an-example-of-hypothesis-testing-in-high-dimension-using-sparse-pca">An Example of Hypothesis Testing in High Dimension using Sparse PCA</h3>

<p>More specifically, the <em>k</em>-sparse largest eigenvalue (the optimal value of ) can be used to discriminate an isometric model, where every direction has the same variance, from a spiked covariance model in high-dimensional setting.<ref name="optimal">{{ cite journal</ref></p>

<p><code>|Â authorÂ =Â Â QuentinÂ Berthet,Â PhilippeÂ Rigollet</code><br/>
<code>|Â yearÂ =Â 2013</code><br/>
<code>|Â titleÂ =Â OptimalÂ DetectionÂ ofÂ SparseÂ PrincipalÂ ComponentsÂ inÂ HighÂ Dimension</code><br/>
<code>|Â journalÂ =Â TheÂ AnnalsÂ ofÂ Statistics,Â 2013,Â Vol.Â 41,Â No.Â 1,Â 1780â€“1815.</code><br/>
<code>|Â urlÂ =Â </code><a href="http://projecteuclid.org/euclid.aos/1378386239"><code>http://projecteuclid.org/euclid.aos/1378386239</code></a><br/>
<code>}}</code><code>Â ConsiderÂ aÂ hypothesisÂ testÂ whereÂ theÂ nullÂ hypothesisÂ specifiesÂ thatÂ dataÂ </code>

<math display="inline" id="Sparse_PCA:13">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

<code>Â areÂ generatedÂ fromÂ multivariateÂ normalÂ distributuionÂ withÂ meanÂ 0Â andÂ covarianceÂ equalÂ toÂ anÂ identityÂ matrix,Â andÂ theÂ alternativeÂ hypothesisÂ specifiesÂ thatÂ dataÂ </code>

<math display="inline" id="Sparse_PCA:14">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

<code>Â isÂ generatedÂ fromÂ aÂ spikedÂ modelÂ withÂ signalÂ strengthÂ </code>

<math display="inline" id="Sparse_PCA:15">
 <semantics>
  <mi>Î¸</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Î¸</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta
  </annotation>
 </semantics>
</math>

<code>:</code></p>

<p>

<math display="block" id="Sparse_PCA:16">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>H</mi>
     <mn>0</mn>
    </msub>
    <mo>:</mo>
    <mrow>
     <mi>X</mi>
     <mo>âˆ¼</mo>
     <mrow>
      <mrow>
       <mi>N</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mn>0</mn>
        <mo>,</mo>
        <msub>
         <mi>I</mi>
         <mi>p</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo rspace="12.5pt">,</mo>
      <msub>
       <mi>H</mi>
       <mn>1</mn>
      </msub>
     </mrow>
    </mrow>
    <mo>:</mo>
    <mrow>
     <mi>X</mi>
     <mo>âˆ¼</mo>
     <mrow>
      <mi>N</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mn>0</mn>
       <mo>,</mo>
       <mrow>
        <msub>
         <mi>I</mi>
         <mi>p</mi>
        </msub>
        <mo>+</mo>
        <mrow>
         <mi>Î¸</mi>
         <mi>v</mi>
         <msup>
          <mi>v</mi>
          <mi>T</mi>
         </msup>
        </mrow>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <ci>normal-:</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>H</ci>
      <cn type="integer">0</cn>
     </apply>
     <apply>
      <csymbol cd="latexml">similar-to</csymbol>
      <ci>X</ci>
      <list>
       <apply>
        <times></times>
        <ci>N</ci>
        <interval closure="open">
         <cn type="integer">0</cn>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>I</ci>
          <ci>p</ci>
         </apply>
        </interval>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>H</ci>
        <cn type="integer">1</cn>
       </apply>
      </list>
     </apply>
    </apply>
    <apply>
     <ci>normal-:</ci>
     <share href="#.cmml">
     </share>
     <apply>
      <csymbol cd="latexml">similar-to</csymbol>
      <ci>X</ci>
      <apply>
       <times></times>
       <ci>N</ci>
       <interval closure="open">
        <cn type="integer">0</cn>
        <apply>
         <plus></plus>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>I</ci>
          <ci>p</ci>
         </apply>
         <apply>
          <times></times>
          <ci>Î¸</ci>
          <ci>v</ci>
          <apply>
           <csymbol cd="ambiguous">superscript</csymbol>
           <ci>v</ci>
           <ci>T</ci>
          </apply>
         </apply>
        </apply>
       </interval>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H_{0}:X\sim N(0,I_{p}),\quad H_{1}:X\sim N(0,I_{p}+\theta vv^{T}),
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Sparse_PCA:17">
 <semantics>
  <mrow>
   <mi>v</mi>
   <mo>âˆˆ</mo>
   <msup>
    <mi>â„</mi>
    <mi>p</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>v</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>â„</ci>
     <ci>p</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   v\in\mathbb{R}^{p}
  </annotation>
 </semantics>
</math>

 has only <em>k</em> non-zero coordinates. The largest <em>k</em>-sparse eigenvalue can discriminate the two hypothesis if and only if 

<math display="inline" id="Sparse_PCA:18">
 <semantics>
  <mrow>
   <mi>Î¸</mi>
   <mo>></mo>
   <mrow>
    <mi mathvariant="normal">Î˜</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msqrt>
      <mrow>
       <mrow>
        <mi>k</mi>
        <mrow>
         <mi>log</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>p</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
       </mrow>
       <mo>/</mo>
       <mi>n</mi>
      </mrow>
     </msqrt>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>Î¸</ci>
    <apply>
     <times></times>
     <ci>normal-Î˜</ci>
     <apply>
      <root></root>
      <apply>
       <divide></divide>
       <apply>
        <times></times>
        <ci>k</ci>
        <apply>
         <log></log>
         <ci>p</ci>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta>\Theta(\sqrt{k\log(p)/n})
  </annotation>
 </semantics>
</math>

.</p>

<p>Since computing <em>k</em>-sparse eigenvalue is NP-hard, one can approximate it by the optimal value of semidefinite programming relaxation (). If that case, we can discriminate the two hypotheses if 

<math display="inline" id="Sparse_PCA:19">
 <semantics>
  <mrow>
   <mi>Î¸</mi>
   <mo>></mo>
   <mrow>
    <mi mathvariant="normal">Î˜</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msqrt>
      <mrow>
       <mrow>
        <msup>
         <mi>k</mi>
         <mn>2</mn>
        </msup>
        <mrow>
         <mi>log</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>p</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
       </mrow>
       <mo>/</mo>
       <mi>n</mi>
      </mrow>
     </msqrt>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>Î¸</ci>
    <apply>
     <times></times>
     <ci>normal-Î˜</ci>
     <apply>
      <root></root>
      <apply>
       <divide></divide>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>k</ci>
         <cn type="integer">2</cn>
        </apply>
        <apply>
         <log></log>
         <ci>p</ci>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta>\Theta(\sqrt{k^{2}\log(p)/n})
  </annotation>
 </semantics>
</math>

. The additional 

<math display="inline" id="Sparse_PCA:20">
 <semantics>
  <msqrt>
   <mi>k</mi>
  </msqrt>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <root></root>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sqrt{k}
  </annotation>
 </semantics>
</math>

 term cannot be improved by any other polynomical time algorithm if a conjecture for the hidden <a href="clique_problem" title="wikilink">clique problem</a> holds.</p>
<h2 id="references">References</h2>
<references>
</references>

<p>"</p>

<p><a href="Category:Multivariate_statistics" title="wikilink">Category:Multivariate statistics</a> <a href="Category:Machine_learning_algorithms" title="wikilink">Category:Machine learning algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">â†©</a></li>
<li id="fn2"></li>
</ol>
</section>
</body>
</html>
