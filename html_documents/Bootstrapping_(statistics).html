<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1202">Bootstrapping (statistics)</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Bootstrapping (statistics)</h1>
<hr/>
<figure><b>(Figure)</b>
<figcaption><a href="Statistic" title="wikilink">Statistics</a> distributions obtained from <a href="Simon_Newcomb" title="wikilink">Simon Newcomb</a> <a href="Simon_Newcomb#Speed_of_light" title="wikilink">speed of light dataset</a> obtained through bootstrapping: the final result differs between the <a href="standard_deviation" title="wikilink">standard deviation</a> and the <a href="median_absolute_deviation" title="wikilink">median absolute deviation</a> (both measures of dispersion) distributions.</figcaption>
</figure>

<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, <strong>bootstrapping</strong> can refer to any test or metric that relies on <a href="random_sampling_with_replacement" title="wikilink">random sampling with replacement</a>. Bootstrapping allows assigning measures of accuracy (defined in terms of bias, variance, <a href="confidence_interval" title="wikilink">confidence intervals</a>, prediction error or some other such measure) to sample estimates.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> Generally, it falls in the broader class of <a href="Resampling_(statistics)" title="wikilink">resampling</a> methods.</p>

<p>Bootstrapping is the practice of estimating properties of an <a class="uri" href="estimator" title="wikilink">estimator</a> (such as its <a class="uri" href="variance" title="wikilink">variance</a>) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the <a href="empirical_distribution_function" title="wikilink">empirical distribution function</a> of the observed data. In the case where a set of observations can be assumed to be from an <a href="independent_and_identically_distributed" title="wikilink">independent and identically distributed</a> population, this can be implemented by constructing a number of <a href="Resampling_(statistics)" title="wikilink">resamples</a> with replacement, of the observed dataset (and of equal size to the observed dataset).</p>

<p>It may also be used for constructing <a href="statistical_hypothesis_testing" title="wikilink">hypothesis tests</a>. It is often used as an alternative to <a href="statistical_inference" title="wikilink">statistical inference</a> based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of <a href="standard_error" title="wikilink">standard errors</a>.</p>
<h2 id="history">History</h2>

<p>The bootstrap was published by <a href="Bradley_Efron" title="wikilink">Bradley Efron</a> in "Bootstrap methods: another look at the jackknife" (1979).<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> It was inspired by earlier work on the <a href="Jackknife_method" title="wikilink">jackknife</a>.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a><a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> Improved estimates of the variance were developed later.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> A Bayesian extension was developed in 1981.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> The bias-corrected and accelerated (BCa) bootstrap was developed by Efron in 1987,<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> and the ABC procedure in 1992.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>
<h2 id="approach">Approach</h2>

<p>The basic idea of bootstrapping is that inference about a population from sample data (sample → population) can be modeled by <em>resampling</em> the sample data and performing inference on (resample → sample). As the population is unknown, the true error in a sample statistic against its population value is unknowable. In bootstrap-resamples, the 'population' is in fact the sample, and this is known; hence the quality of inference from resample data → 'true' sample is measurable.</p>

<p>More formally, the bootstrap works by treating inference of the true probability distribution J, given the original data, as being analogous to inference of the empirical distribution of Ĵ, given the resampled data. The accuracy of inferences regarding Ĵ using the resampled data can be assessed because we know Ĵ. If Ĵ is a reasonable approximation to J, then the quality of inference on J can in turn be inferred.</p>

<p>As an example, assume we are interested in the average (or <a class="uri" href="mean" title="wikilink">mean</a>) height of people worldwide. We cannot measure all the people in the global population, so instead we sample only a tiny part of it, and measure that. Assume the sample is of size N; that is, we measure the heights of N individuals. From that single sample, only one estimate of the mean can be obtained. In order to reason about the population, we need some sense of the <a href="Statistical_dispersion" title="wikilink">variability</a> of the mean that we have computed.</p>

<p>The simplest bootstrap method involves taking the original data set of N heights, and, using a computer, sampling from it to form a new sample (called a 'resample' or bootstrap sample) that is also of size N. The bootstrap sample is taken from the original using <a href="Simple_random_sample" title="wikilink">sampling with replacement</a> so, assuming N is sufficiently large, for all practical purposes there is virtually zero probability that it will be identical to the original "real" sample. This process is repeated a large number of times (typically 1,000 or 10,000 times), and for each of these bootstrap samples we compute its mean (each of these are called bootstrap estimates). We now have a histogram of bootstrap means. This provides an estimate of the shape of the distribution of the mean from which we can answer questions about how much the mean varies. (The method here, described for the mean, can be applied to almost any other <a class="uri" href="statistic" title="wikilink">statistic</a> or <a class="uri" href="estimator" title="wikilink">estimator</a>.)</p>
<h2 id="situations-where-bootstrapping-is-useful">Situations where bootstrapping is useful</h2>

<p>Adèr et al.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> recommend the bootstrap procedure for the following situations:</p>

<p>:*<strong>When the theoretical distribution of a statistic of interest is complicated or unknown.</strong> Since the bootstrapping procedure is distribution-independent it provides an indirect method to assess the properties of the distribution underlying the sample and the parameters of interest that are derived from this distribution.</p>

<p>:*<strong>When the <a href="sample_size" title="wikilink">sample size</a> is insufficient for straightforward statistical inference.</strong> If the underlying distribution is well-known, bootstrapping provides a way to account for the distortions caused by the specific sample that may not be fully representative of the population.</p>

<p>:* <strong>When <a href="statistical_power" title="wikilink">power calculations</a> have to be performed, and a small pilot sample is available.</strong> Most power and sample size calculations are heavily dependent on the standard deviation of the statistic of interest. If the estimate used is incorrect, the required sample size will also be wrong. One method to get an impression of the variation of the statistic is to use a small pilot sample and perform bootstrapping on it to get impression of the variance.</p>
<h2 id="discussion">Discussion</h2>
<h3 id="advantages">Advantages</h3>

<p>A great advantage of bootstrap is its simplicity. It is a straightforward way to derive estimates of <a href="standard_error_(statistics)" title="wikilink">standard errors</a> and <a href="confidence_intervals" title="wikilink">confidence intervals</a> for complex estimators of complex parameters of the distribution, such as percentile points, proportions, odds ratio, and correlation coefficients. Bootstrap is also an appropriate way to control and check the stability of the results. Although for most problems it is impossible to know the true confidence interval, bootstrap is asymptotically more accurate than the standard intervals obtained using sample variance and assumptions of normality.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></p>
<h3 id="disadvantages">Disadvantages</h3>

<p>Although bootstrapping is (under some conditions) asymptotically consistent, it does not provide general finite-sample guarantees. The apparent simplicity may conceal the fact that important assumptions are being made when undertaking the bootstrap analysis (e.g. independence of samples) where these would be more formally stated in other approaches.</p>
<h3 id="recommendations">Recommendations</h3>

<p>The number of bootstrap samples recommended in literature has increased as available computing power has increased. If the results may have substantial real-world consequences, then one should use as many samples as is reasonable, given available computing power and time. Increasing the number of samples cannot increase the amount of information in the original data, it can only reduce the effects of random sampling errors which can arise from a bootstrap procedure itself.</p>
<h2 id="types-of-bootstrap-scheme">Types of bootstrap scheme</h2>

<p>In univariate problems, it is usually acceptable to resample the individual observations with replacement ("case resampling" below). In small samples, a parametric bootstrap approach might be preferred. For other problems, a <em>smooth bootstrap</em> will likely be preferred.</p>

<p>For regression problems, various other alternatives are available.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a></p>
<h3 id="case-resampling">Case resampling</h3>

<p>Bootstrap is generally useful for estimating the distribution of a statistic (e.g. mean, variance) without using normal theory (e.g. z-statistic, t-statistic). Bootstrap comes in handy when there is no analytical form or normal theory to help estimate the distribution of the statistics of interest, since bootstrap method can apply to most random quantities, e.g., the ratio of variance and mean. There are at least two ways of performing case resampling.</p>
<ol>
<li>The Monte Carlo algorithm for case resampling is quite simple. First, we resample the data with replacement, and the size of the resample must be equal to the size of the original data set. Then the statistic of interest is computed from the resample from the first step. We repeat this routine many times to get a more precise estimate of the Bootstrap distribution of the statistic.</li>
<li>The 'exact' version for case resampling is similar, but we exhaustively enumerate every possible resample of the data set. This can be computationally expensive as there are a total of 

<math display="inline" id="Bootstrapping_(statistics):0">
 <semantics>
  <mrow>
   <mo>(</mo>
   <mstyle scriptlevel="+1">
    <mtable columnspacing="0.4em" rowspacing="0.2ex">
     <mtr>
      <mtd>
       <mrow>
        <mrow>
         <mn>2</mn>
         <mi>n</mi>
        </mrow>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
      </mtd>
     </mtr>
     <mtr>
      <mtd>
       <mi>n</mi>
      </mtd>
     </mtr>
    </mtable>
   </mstyle>
   <mo>)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">binomial</csymbol>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <cn type="integer">2</cn>
      <ci>n</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   {\left({{2n-1}\atop{n}}\right)}
  </annotation>
 </semantics>
</math>

 different resamples, where <em>n</em> is the size of the data set.</li>
</ol>
<h4 id="estimating-the-distribution-of-sample-mean">Estimating the distribution of sample mean</h4>

<p>Consider a coin-flipping experiment. We flip the coin and record whether it lands heads or tails. (Assume for simplicity that there are only two outcomes) Let <mtpl> x''<sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub>10</sub>}}</mtpl> be 10 observations from the experiment. <mtpl> 1}}</mtpl> if the i th flip lands heads, and 0 otherwise. From normal theory, we can use <a href="Student's_t-statistic" title="wikilink">t-statistic</a> to estimate the distribution of the sample mean, 

<math display="inline" id="Bootstrapping_(statistics):1">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>x</mi>
    <mo stretchy="false">¯</mo>
   </mover>
   <mo>=</mo>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mn>10</mn>
    </mfrac>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>x</mi>
       <mn>1</mn>
      </msub>
      <mo>+</mo>
      <msub>
       <mi>x</mi>
       <mn>2</mn>
      </msub>
      <mo>+</mo>
      <mi mathvariant="normal">…</mi>
      <mo>+</mo>
      <msub>
       <mi>x</mi>
       <mn>10</mn>
      </msub>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>normal-¯</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <cn type="integer">10</cn>
     </apply>
     <apply>
      <plus></plus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <cn type="integer">2</cn>
      </apply>
      <ci>normal-…</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <cn type="integer">10</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{x}=\frac{1}{10}(x_{1}+x_{2}+\ldots+x_{10})
  </annotation>
 </semantics>
</math>

.</p>

<p>Instead, we use bootstrap, specifically case resampling, to derive the distribution of 

<math display="inline" id="Bootstrapping_(statistics):2">
 <semantics>
  <mover accent="true">
   <mi>x</mi>
   <mo stretchy="false">¯</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-¯</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{x}
  </annotation>
 </semantics>
</math>

. We first resample the data to obtain a <em>bootstrap resample</em>. An example of the first resample might look like this <mtpl> <em>x</em><sub>2</sub>, <em>x</em><sub>1</sub>, <em>x</em><sub>10</sub>, <em>x</em><sub>10</sub>, <em>x</em><sub>3</sub>, <em>x</em><sub>4</sub>, <em>x</em><sub>6</sub>, <em>x</em><sub>7</sub>, <em>x</em><sub>1</sub>, <em>x</em><sub>9</sub>}}</mtpl>. Note that there are some duplicates since a bootstrap resample comes from sampling with replacement from the data. Note also that the number of data points in a bootstrap resample is equal to the number of data points in our original observations. Then we compute the mean of this resample and obtain the first <em>bootstrap mean</em>: <em>μ</em><sub>1</sub>*. We repeat this process to obtain the second resample <em>X</em><sub>2</sub>* and compute the second bootstrap mean <em>μ</em><sub>2</sub>*. If we repeat this 100 times, then we have <em>μ</em><sub>1</sub>*, <em>μ</em><sub>2</sub>*, …, <em>μ</em><sub>100</sub>*. This represents an <em>empirical bootstrap distribution</em> of sample mean. From this empirical distribution, one can derive a <em>bootstrap confidence interval</em> for the purpose of hypothesis testing.</p>
<h4 id="regression">Regression</h4>

<p>In regression problems, <em>case resampling</em> refers to the simple scheme of resampling individual cases - often rows of a <a href="data_set" title="wikilink">data set</a>. For regression problems, so long as the data set is fairly large, this simple scheme is often acceptable. However, the method is open to criticism.</p>

<p>In regression problems, the <a href="explanatory_variable" title="wikilink">explanatory variables</a> are often fixed, or at least observed with more control than the response variable. Also, the range of the explanatory variables defines the information available from them. Therefore, to resample cases means that each bootstrap sample will lose some information. As such, alternative bootstrap procedures should be considered.</p>
<h3 id="bayesian-bootstrap">Bayesian bootstrap</h3>

<p>Bootstrapping can be interpreted in a <a href="Bayesian_probability" title="wikilink">Bayesian</a> framework using a scheme that creates new datasets through reweighting the initial data. Given a set of 

<math display="inline" id="Bootstrapping_(statistics):3">
 <semantics>
  <mi>N</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>N</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N
  </annotation>
 </semantics>
</math>

 data points, the weighting assigned to data point 

<math display="inline" id="Bootstrapping_(statistics):4">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

 in a new dataset 

<math display="inline" id="Bootstrapping_(statistics):5">
 <semantics>
  <msup>
   <mi class="ltx_font_mathcaligraphic">𝒟</mi>
   <mi>J</mi>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>𝒟</ci>
    <ci>J</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{D}^{J}
  </annotation>
 </semantics>
</math>

 is 

<math display="inline" id="Bootstrapping_(statistics):6">
 <semantics>
  <mrow>
   <msubsup>
    <mi>w</mi>
    <mi>i</mi>
    <mi>J</mi>
   </msubsup>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mi>x</mi>
     <mi>i</mi>
     <mi>J</mi>
    </msubsup>
    <mo>-</mo>
    <msubsup>
     <mi>x</mi>
     <mrow>
      <mi>i</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
     <mi>J</mi>
    </msubsup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>w</ci>
      <ci>J</ci>
     </apply>
     <ci>i</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>x</ci>
       <ci>J</ci>
      </apply>
      <ci>i</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>x</ci>
       <ci>J</ci>
      </apply>
      <apply>
       <minus></minus>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w^{J}_{i}=x^{J}_{i}-x^{J}_{i-1}
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Bootstrapping_(statistics):7">
 <semantics>
  <msup>
   <mi>𝐱</mi>
   <mi>J</mi>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>𝐱</ci>
    <ci>J</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}^{J}
  </annotation>
 </semantics>
</math>

 is a low-to-high ordered list of 

<math display="inline" id="Bootstrapping_(statistics):8">
 <semantics>
  <mrow>
   <mi>N</mi>
   <mo>-</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <ci>N</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N-1
  </annotation>
 </semantics>
</math>

 uniformly distributed random numbers on 

<math display="inline" id="Bootstrapping_(statistics):9">
 <semantics>
  <mrow>
   <mo stretchy="false">[</mo>
   <mn>0</mn>
   <mo>,</mo>
   <mn>1</mn>
   <mo stretchy="false">]</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="closed">
    <cn type="integer">0</cn>
    <cn type="integer">1</cn>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   [0,1]
  </annotation>
 </semantics>
</math>

, preceded by 0 and succeeded by 1. The distributions of a parameter inferred from considering many such datasets 

<math display="inline" id="Bootstrapping_(statistics):10">
 <semantics>
  <msup>
   <mi class="ltx_font_mathcaligraphic">𝒟</mi>
   <mi>J</mi>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>𝒟</ci>
    <ci>J</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{D}^{J}
  </annotation>
 </semantics>
</math>

 are then interpretable as <a href="posterior_distribution" title="wikilink">posterior distributions</a> on that parameter.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a></p>
<h3 id="smooth-bootstrap">Smooth bootstrap</h3>

<p>Under this scheme, a small amount of (usually normally distributed) zero-centered random noise is added onto each resampled observation. This is equivalent to sampling from a <a href="kernel_density" title="wikilink">kernel density</a> estimate of the data.</p>
<h3 id="parametric-bootstrap">Parametric bootstrap</h3>

<p>In this case a parametric model is fitted to the data, often by <a href="maximum_likelihood" title="wikilink">maximum likelihood</a>, and samples of <a href="random_number_generation" title="wikilink">random numbers</a> are drawn from this fitted model. Usually the sample drawn has the same sample size as the original data. Then the quantity, or estimate, of interest is calculated from these data. This sampling process is repeated many times as for other bootstrap methods. The use of a parametric model at the sampling stage of the bootstrap methodology leads to procedures which are different from those obtained by applying basic statistical theory to inference for the same model.</p>
<h3 id="resampling-residuals">Resampling residuals</h3>

<p>Another approach to bootstrapping in regression problems is to resample <a href="Errors_and_residuals_in_statistics" title="wikilink">residuals</a>. The method proceeds as follows.</p>
<ol>
<li>Fit the model and retain the fitted values 

<math display="inline" id="Bootstrapping_(statistics):11">
 <semantics>
  <msub>
   <mover accent="true">
    <mi>y</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <ci>normal-^</ci>
     <ci>y</ci>
    </apply>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{y}_{i}
  </annotation>
 </semantics>
</math>

 and the residuals 

<math display="inline" id="Bootstrapping_(statistics):12">
 <semantics>
  <mrow>
   <msub>
    <mover accent="true">
     <mi>ϵ</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mi>i</mi>
   </msub>
   <mo>=</mo>
   <msub>
    <mi>y</mi>
    <mi>i</mi>
   </msub>
   <mo>-</mo>
   <msub>
    <mover accent="true">
     <mi>y</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mi>i</mi>
   </msub>
   <mo>,</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>i</mi>
    <mo>=</mo>
    <mn>1</mn>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <mi>n</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <ci>normal-^</ci>
      <ci>ϵ</ci>
     </apply>
     <ci>i</ci>
    </apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>y</ci>
     <ci>i</ci>
    </apply>
    <minus></minus>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <ci>normal-^</ci>
      <ci>y</ci>
     </apply>
     <ci>i</ci>
    </apply>
    <ci>normal-,</ci>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">i</csymbol>
     <eq></eq>
     <cn type="integer">1</cn>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <csymbol cd="unknown">n</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\epsilon}_{i}=y_{i}-\hat{y}_{i},(i=1,\dots,n)
  </annotation>
 </semantics>
</math>

.</li>
<li>For each pair, (<em>x<sub>i</sub></em>, <em>y<sub>i</sub></em>), in which <em>x<sub>i</sub></em> is the (possibly multivariate) explanatory variable, add a randomly resampled residual, 

<math display="inline" id="Bootstrapping_(statistics):13">
 <semantics>
  <msub>
   <mover accent="true">
    <mi>ϵ</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mi>j</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <ci>normal-^</ci>
     <ci>ϵ</ci>
    </apply>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\epsilon}_{j}
  </annotation>
 </semantics>
</math>

, to the response variable <em>y<sub>i</sub></em>. In other words create synthetic response variables 

<math display="inline" id="Bootstrapping_(statistics):14">
 <semantics>
  <mrow>
   <msubsup>
    <mi>y</mi>
    <mi>i</mi>
    <mo>*</mo>
   </msubsup>
   <mo>=</mo>
   <mrow>
    <msub>
     <mover accent="true">
      <mi>y</mi>
      <mo stretchy="false">^</mo>
     </mover>
     <mi>i</mi>
    </msub>
    <mo>+</mo>
    <msub>
     <mover accent="true">
      <mi>ϵ</mi>
      <mo stretchy="false">^</mo>
     </mover>
     <mi>j</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>y</ci>
      <times></times>
     </apply>
     <ci>i</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <ci>normal-^</ci>
       <ci>y</ci>
      </apply>
      <ci>i</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <ci>normal-^</ci>
       <ci>ϵ</ci>
      </apply>
      <ci>j</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y^{*}_{i}=\hat{y}_{i}+\hat{\epsilon}_{j}
  </annotation>
 </semantics>
</math>

 where <em>j</em> is selected randomly from the list (1, …, <em>n</em>) for every <em>i</em>.</li>
<li>Refit the model using the fictitious response variables 

<math display="inline" id="Bootstrapping_(statistics):15">
 <semantics>
  <msubsup>
   <mi>y</mi>
   <mi>i</mi>
   <mo>*</mo>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>y</ci>
     <times></times>
    </apply>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y^{*}_{i}
  </annotation>
 </semantics>
</math>

, and retain the quantities of interest (often the parameters, 

<math display="inline" id="Bootstrapping_(statistics):16">
 <semantics>
  <msubsup>
   <mover accent="true">
    <mi>μ</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mi>i</mi>
   <mo>*</mo>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <ci>normal-^</ci>
      <ci>μ</ci>
     </apply>
     <times></times>
    </apply>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\mu}^{*}_{i}
  </annotation>
 </semantics>
</math>

, estimated from the synthetic 

<math display="inline" id="Bootstrapping_(statistics):17">
 <semantics>
  <msubsup>
   <mi>y</mi>
   <mi>i</mi>
   <mo>*</mo>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>y</ci>
     <times></times>
    </apply>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y^{*}_{i}
  </annotation>
 </semantics>
</math>

).</li>
<li>Repeat steps 2 and 3 a large number of times.</li>
</ol>

<p>This scheme has the advantage that it retains the information in the explanatory variables. However, a question arises as to which residuals to resample. Raw residuals are one option; another is <a href="studentized_residuals" title="wikilink">studentized residuals</a> (in linear regression). Whilst there are arguments in favour of using studentized residuals; in practice, it often makes little difference and it is easy to run both schemes and compare the results against each other.</p>
<h3 id="gaussian-process-regression-bootstrap">Gaussian process regression bootstrap</h3>

<p>When data are temporally correlated, straightforward bootstrapping destroys the inherent correlations. This method uses Gaussian process regression to fit a probabilistic model from which replicates may then be drawn. <a href="Gaussian_processes" title="wikilink">Gaussian processes</a> are methods from Bayesian <a href="non-parametric_statistics" title="wikilink">non-parametric statistics</a> but are here used to construct a parametric bootstrap approach, which implicitly allows the time-dependence of the data to be taken into account.</p>
<h3 id="wild-bootstrap">Wild bootstrap</h3>

<p>The Wild bootstrap, proposed originally by Wu (1986),<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> is suited when the model exhibits <a class="uri" href="heteroskedasticity" title="wikilink">heteroskedasticity</a>. The idea is, like the residual bootstrap, to leave the regressors at their sample value, but to resample the response variable based on the residuals values. That is, for each replicate, one computes a new 

<math display="inline" id="Bootstrapping_(statistics):18">
 <semantics>
  <mi>y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y
  </annotation>
 </semantics>
</math>

 based on</p>

<p>

<math display="block" id="Bootstrapping_(statistics):19">
 <semantics>
  <mrow>
   <msubsup>
    <mi>y</mi>
    <mi>i</mi>
    <mo>*</mo>
   </msubsup>
   <mo>=</mo>
   <mrow>
    <msub>
     <mover accent="true">
      <mi>y</mi>
      <mo stretchy="false">^</mo>
     </mover>
     <mi>i</mi>
    </msub>
    <mo>+</mo>
    <mrow>
     <msub>
      <mover accent="true">
       <mi>ϵ</mi>
       <mo stretchy="false">^</mo>
      </mover>
      <mi>i</mi>
     </msub>
     <msub>
      <mi>v</mi>
      <mi>i</mi>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>y</ci>
      <times></times>
     </apply>
     <ci>i</ci>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <ci>normal-^</ci>
       <ci>y</ci>
      </apply>
      <ci>i</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-^</ci>
        <ci>ϵ</ci>
       </apply>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>v</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   y^{*}_{i}=\hat{y}_{i}+\hat{\epsilon}_{i}v_{i}
  </annotation>
 </semantics>
</math>

</p>

<p>so the residuals are randomly multiplied by a random variable 

<math display="inline" id="Bootstrapping_(statistics):20">
 <semantics>
  <msub>
   <mi>v</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>v</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   v_{i}
  </annotation>
 </semantics>
</math>

 with mean 0 and variance 1. This method assumes that the 'true' residual distribution is symmetric and can offer advantages over simple residual sampling for smaller sample sizes. Different forms are used for the random variable 

<math display="inline" id="Bootstrapping_(statistics):21">
 <semantics>
  <msub>
   <mi>v</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>v</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   v_{i}
  </annotation>
 </semantics>
</math>

, such as</p>

<p>:*The <a href="standard_normal_distribution" title="wikilink">standard normal distribution</a></p>

<p>:*A distribution suggested by Mammen (1993).<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a></p>
<dl>
<dd><dl>
<dd><math> v_i = \left\{\begin{matrix}
</math></dd>
</dl>
</dd>
</dl>

<p>-(\sqrt{5} -1)/2 &amp; \mbox {with prob. } (\sqrt{5} +1)/(2\sqrt{5}) \\ (\sqrt{5} +1)/2 &amp; \mbox {with prob. } (\sqrt{5} -1)/(2\sqrt{5}) \end{matrix}\right.</p>

<p>:*Or the simpler distribution, linked to the <a href="Rademacher_distribution" title="wikilink">Rademacher distribution</a>:</p>
<dl>
<dd><dl>
<dd><math> v_i = \left\{\begin{matrix}
</math></dd>
</dl>
</dd>
</dl>

<p>-1 &amp; \mbox {with prob. } 1/2 \\ 1 &amp; \mbox {with prob. } 1/2 \end{matrix}\right.</p>
<h3 id="block-bootstrap">Block bootstrap</h3>

<p>The block bootstrap is used when the data, or the errors in a model, are correlated. In this case, a simple case or residual resampling will fail, as it is not able to replicate the correlation in the data. The block bootstrap tries to replicate the correlation by resampling instead blocks of data. The block bootstrap has been used mainly with data correlated in time (i.e. time series) but can also be used with data correlated in space, or among groups (so-called cluster data).</p>
<h4 id="time-series-simple-block-bootstrap">Time series: Simple block bootstrap</h4>

<p>In the (simple) block bootstrap, the variable of interest is split into non-overlapping blocks.</p>
<h4 id="time-series-moving-block-bootstrap">Time series: Moving block bootstrap</h4>

<p>In the moving block bootstrap, introduced by Künsch (1989),<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a> data is split into n-b+1 overlapping blocks of length b: Observation 1 to b will be block 1, observation 2 to b+1 will be block 2 etc. Then from these n-b+1 blocks, n/b blocks will be drawn at random with replacement. Then aligning these n/b blocks in the order they were picked, will give the bootstrap observations.</p>

<p>This bootstrap works with dependent data, however, the bootstrapped observations will not be stationary anymore by construction. But, it was shown that varying randomly the block length can avoid this problem.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a> This method is known as the <em>stationary bootstrap.</em> Other related modifications of the moving block bootstrap are the <em>Markovian bootstrap</em> and a stationary bootstrap method that matches subsequent blocks based on standard deviation matching.</p>
<h4 id="cluster-data-block-bootstrap">Cluster data: block bootstrap</h4>

<p>Cluster data describes data where many observations per unit are observed. This could be observing many firms in many states, or observing students in many classes. In such cases, the correlation structure is simplified, and one does usually make the assumption that data is correlated with a group/cluster, but independent between groups/clusters. The structure of the block bootstrap is easily obtained (where the block just corresponds to the group), and usually only the groups are resampled, while the observations within the groups are left unchanged. Cameron et al. (2008) <a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a> discusses this for clustered errors in linear regression.</p>
<h2 id="choice-of-statistic">Choice of statistic</h2>

<p>The bootstrap distribution of a point estimator of a population parameter has been used to produce a bootstrapped <a href="confidence_interval" title="wikilink">confidence interval</a> for the parameter's true value, if the parameter can be written as a <a href="U_statistic#statistical_functional" title="wikilink">function of the population's distribution</a>.</p>

<p><a href="Population_parameter" title="wikilink">Population parameters</a> are estimated with many <a href="point_estimator" title="wikilink">point estimators</a>. Popular families of point-estimators include <a href="UMVU" title="wikilink">mean-unbiased minimum-variance estimators</a>, <a href="median-unbiased_estimator" title="wikilink">median-unbiased estimators</a>, <a href="Bayesian_estimator" title="wikilink">Bayesian estimators</a> (for example, the <a href="posterior_distribution" title="wikilink">posterior distribution</a>'s <a href="Bayesian_estimator#Posterior_mode" title="wikilink">mode</a>, <a href="Bayesian_estimator#Posterior_median" title="wikilink">median</a>, <a href="Bayesian_estimator#Posterior_mean" title="wikilink">mean</a>), and <a href="maximum_likelihood" title="wikilink">maximum-likelihood estimators</a>.</p>

<p>A Bayesian point estimator and a maximum-likelihood estimator have good performance when the sample size is infinite, according to <a href="asymptotic_theory" title="wikilink">asymptotic theory</a>. For practical problems with finite samples, other estimators may be preferable. Asymptotic theory suggests techniques that often improve the performance of bootstrapped estimators; the bootstrapping of a maximum-likelihood estimator may often be improved using transformations related to <a href="pivotal_quantity" title="wikilink">pivotal quantities</a>.<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a></p>
<h2 id="deriving-confidence-intervals-from-the-bootstrap-distribution">Deriving confidence intervals from the bootstrap distribution</h2>

<p>The bootstrap distribution of a parameter-estimator has been used to calculate <a href="confidence_intervals" title="wikilink">confidence intervals</a> for its population-parameter.</p>
<h3 id="bias-asymmetry-and-confidence-intervals">Bias, asymmetry, and confidence intervals</h3>
<ul>
<li><strong>Bias</strong>: The bootstrap distribution and the sample may disagree systematically, in which case <a href="Bias_(statistics)" title="wikilink">bias</a> may occur.
<dl>
<dd>If the bootstrap distribution of an estimator is symmetric, then percentile confidence-interval are often used; such intervals are appropriate especially for median-unbiased estimators of minimum risk (with respect to an <a href="absolute_value" title="wikilink">absolute</a> <a href="loss_function" title="wikilink">loss function</a>). Bias in the bootstrap distribution will lead to bias in the confidence-interval.
</dd>
<dd>Otherwise, if the bootstrap distribution is non-symmetric, then percentile confidence-intervals are often inappropriate.
</dd>
</dl></li>
</ul>
<h3 id="methods-for-bootstrap-confidence-intervals">Methods for bootstrap confidence intervals</h3>

<p>There are several methods for constructing confidence intervals from the bootstrap distribution of a <a href="real_number" title="wikilink">real</a> parameter:</p>
<ul>
<li><strong>Basic Bootstrap</strong>. The basic bootstrap is the simplest scheme to construct the confidence interval: one simply takes the empirical quantiles from the bootstrap distribution of the parameter (see Davison and Hinkley 1997, equ. 5.6 p. 194):</li>
</ul>

<p>

<math display="block" id="Bootstrapping_(statistics):22">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mrow>
    <mrow>
     <mn>2</mn>
     <mi>θ</mi>
    </mrow>
    <mo>-</mo>
    <msubsup>
     <mi>θ</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mn>1</mn>
       <mo>-</mo>
       <mrow>
        <mi>α</mi>
        <mo>/</mo>
        <mn>2</mn>
       </mrow>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo>*</mo>
    </msubsup>
   </mrow>
   <mo>;</mo>
   <mrow>
    <mrow>
     <mn>2</mn>
     <mi>θ</mi>
    </mrow>
    <mo>-</mo>
    <msubsup>
     <mi>θ</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>α</mi>
       <mo>/</mo>
       <mn>2</mn>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo>*</mo>
    </msubsup>
   </mrow>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <cn type="integer">2</cn>
      <ci>θ</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>θ</ci>
       <times></times>
      </apply>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <apply>
        <divide></divide>
        <ci>α</ci>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <cn type="integer">2</cn>
      <ci>θ</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>θ</ci>
       <times></times>
      </apply>
      <apply>
       <divide></divide>
       <ci>α</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (2\theta-\theta^{*}_{(1-\alpha/2)};2\theta-\theta^{*}_{(\alpha/2)})
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Bootstrapping_(statistics):23">
 <semantics>
  <msubsup>
   <mi>θ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mrow>
      <mi>α</mi>
      <mo>/</mo>
      <mn>2</mn>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>*</mo>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>θ</ci>
     <times></times>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <divide></divide>
      <ci>α</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta^{*}_{(1-\alpha/2)}
  </annotation>
 </semantics>
</math>

 denotes the 

<math display="inline" id="Bootstrapping_(statistics):24">
 <semantics>
  <mrow>
   <mn>1</mn>
   <mo>-</mo>
   <mrow>
    <mi>α</mi>
    <mo>/</mo>
    <mn>2</mn>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <cn type="integer">1</cn>
    <apply>
     <divide></divide>
     <ci>α</ci>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1-\alpha/2
  </annotation>
 </semantics>
</math>

 <a class="uri" href="percentile" title="wikilink">percentile</a> of the bootstrapped coefficients 

<math display="inline" id="Bootstrapping_(statistics):25">
 <semantics>
  <msup>
   <mi>θ</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>θ</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta^{*}
  </annotation>
 </semantics>
</math>

.</p>
<ul>
<li><strong>Percentile Bootstrap</strong>. The percentile bootstrap proceeds in a similar way to the basic bootstrap, using percentiles of the bootstrap distribution, but with a different formula (note the inversion of the left and right quantiles!):</li>
</ul>

<p>

<math display="block" id="Bootstrapping_(statistics):26">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <msubsup>
    <mi>θ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>α</mi>
      <mo>/</mo>
      <mn>2</mn>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>*</mo>
   </msubsup>
   <mo>;</mo>
   <msubsup>
    <mi>θ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mn>1</mn>
      <mo>-</mo>
      <mrow>
       <mi>α</mi>
       <mo>/</mo>
       <mn>2</mn>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>*</mo>
   </msubsup>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>θ</ci>
      <times></times>
     </apply>
     <apply>
      <divide></divide>
      <ci>α</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>θ</ci>
      <times></times>
     </apply>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
      <apply>
       <divide></divide>
       <ci>α</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (\theta^{*}_{(\alpha/2)};\theta^{*}_{(1-\alpha/2)})
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Bootstrapping_(statistics):27">
 <semantics>
  <msubsup>
   <mi>θ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mrow>
      <mi>α</mi>
      <mo>/</mo>
      <mn>2</mn>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>*</mo>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>θ</ci>
     <times></times>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <divide></divide>
      <ci>α</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta^{*}_{(1-\alpha/2)}
  </annotation>
 </semantics>
</math>

 denotes the 

<math display="inline" id="Bootstrapping_(statistics):28">
 <semantics>
  <mrow>
   <mn>1</mn>
   <mo>-</mo>
   <mrow>
    <mi>α</mi>
    <mo>/</mo>
    <mn>2</mn>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <cn type="integer">1</cn>
    <apply>
     <divide></divide>
     <ci>α</ci>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1-\alpha/2
  </annotation>
 </semantics>
</math>

 <a class="uri" href="percentile" title="wikilink">percentile</a> of the bootstrapped coefficients 

<math display="inline" id="Bootstrapping_(statistics):29">
 <semantics>
  <msup>
   <mi>θ</mi>
   <mo>*</mo>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>θ</ci>
    <times></times>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta^{*}
  </annotation>
 </semantics>
</math>

. See Davison and Hinkley (1997, equ. 5.18 p. 203) and Efron and Tibshirani (1993, equ 13.5 p. 171). This method can be applied to any statistic. It will work well in cases where the bootstrap distribution is symmetrical and centered on the observed statistic<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> and where the sample statistic is median-unbiased and has maximum concentration (or minimum risk with respect to an absolute value loss function). In other cases, the percentile bootstrap can be too narrow. When working with small sample sizes (i.e., less than 50), the percentile confidence intervals for (for example) the <a class="uri" href="variance" title="wikilink">variance</a> statistic will be too narrow. So that with a sample of 20 points, 90% confidence interval will include the true variance only 78% of the time<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a></p>
<ul>
<li><strong><a href="Studentization" title="wikilink">Studentized</a> Bootstrap</strong>. The studentized bootstrap, also called bootstrap-t, works similarly as the usual confidence interval, but replaces the quantiles from the normal or student approximation by the quantiles from the bootstrap distribution of the <a href="Student's_t-test" title="wikilink">Student's t-test</a> (see Davison and Hinkley 1997, equ. 5.7 p. 194 and Efron and Tibshirani 1993 equ 12.22, p. 160):</li>
</ul>

<p>

<math display="block" id="Bootstrapping_(statistics):30">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mrow>
    <mi>θ</mi>
    <mo>-</mo>
    <mrow>
     <msubsup>
      <mi>t</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mn>1</mn>
        <mo>-</mo>
        <mrow>
         <mi>α</mi>
         <mo>/</mo>
         <mn>2</mn>
        </mrow>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mo>*</mo>
     </msubsup>
     <mo>⋅</mo>
     <msub>
      <mover accent="true">
       <mrow>
        <mi>s</mi>
        <mi>e</mi>
       </mrow>
       <mo stretchy="false">^</mo>
      </mover>
      <mi>θ</mi>
     </msub>
    </mrow>
   </mrow>
   <mo>;</mo>
   <mrow>
    <mi>θ</mi>
    <mo>-</mo>
    <mrow>
     <msubsup>
      <mi>t</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>α</mi>
        <mo>/</mo>
        <mn>2</mn>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mo>*</mo>
     </msubsup>
     <mo>⋅</mo>
     <msub>
      <mover accent="true">
       <mrow>
        <mi>s</mi>
        <mi>e</mi>
       </mrow>
       <mo stretchy="false">^</mo>
      </mover>
      <mi>θ</mi>
     </msub>
    </mrow>
   </mrow>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <minus></minus>
     <ci>θ</ci>
     <apply>
      <ci>normal-⋅</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>t</ci>
        <times></times>
       </apply>
       <apply>
        <minus></minus>
        <cn type="integer">1</cn>
        <apply>
         <divide></divide>
         <ci>α</ci>
         <cn type="integer">2</cn>
        </apply>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-^</ci>
        <apply>
         <times></times>
         <ci>s</ci>
         <ci>e</ci>
        </apply>
       </apply>
       <ci>θ</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <ci>θ</ci>
     <apply>
      <ci>normal-⋅</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>t</ci>
        <times></times>
       </apply>
       <apply>
        <divide></divide>
        <ci>α</ci>
        <cn type="integer">2</cn>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-^</ci>
        <apply>
         <times></times>
         <ci>s</ci>
         <ci>e</ci>
        </apply>
       </apply>
       <ci>θ</ci>
      </apply>
     </apply>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (\theta-t^{*}_{(1-\alpha/2)}\cdot\hat{se}_{\theta};\theta-t^{*}_{(\alpha/2)}%
\cdot\hat{se}_{\theta})
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Bootstrapping_(statistics):31">
 <semantics>
  <msubsup>
   <mi>t</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mrow>
      <mi>α</mi>
      <mo>/</mo>
      <mn>2</mn>
     </mrow>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>*</mo>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>t</ci>
     <times></times>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <divide></divide>
      <ci>α</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   t^{*}_{(1-\alpha/2)}
  </annotation>
 </semantics>
</math>

 denotes the 

<math display="inline" id="Bootstrapping_(statistics):32">
 <semantics>
  <mrow>
   <mn>1</mn>
   <mo>-</mo>
   <mrow>
    <mi>α</mi>
    <mo>/</mo>
    <mn>2</mn>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <cn type="integer">1</cn>
    <apply>
     <divide></divide>
     <ci>α</ci>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1-\alpha/2
  </annotation>
 </semantics>
</math>

 <a class="uri" href="percentile" title="wikilink">percentile</a> of the bootstrapped <a href="Student's_t-test" title="wikilink">Student's t-test</a> 

<math display="inline" id="Bootstrapping_(statistics):33">
 <semantics>
  <mrow>
   <msup>
    <mi>t</mi>
    <mo>*</mo>
   </msup>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msup>
       <mover accent="true">
        <mi>θ</mi>
        <mo stretchy="false">^</mo>
       </mover>
       <mo>*</mo>
      </msup>
      <mo>-</mo>
      <mover accent="true">
       <mi>θ</mi>
       <mo stretchy="false">^</mo>
      </mover>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>/</mo>
    <msub>
     <mover accent="true">
      <mrow>
       <mi>s</mi>
       <mi>e</mi>
      </mrow>
      <mo stretchy="false">^</mo>
     </mover>
     <msup>
      <mover accent="true">
       <mi>θ</mi>
       <mo stretchy="false">^</mo>
      </mover>
      <mo>*</mo>
     </msup>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>t</ci>
     <times></times>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <ci>normal-^</ci>
        <ci>θ</ci>
       </apply>
       <times></times>
      </apply>
      <apply>
       <ci>normal-^</ci>
       <ci>θ</ci>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <apply>
       <ci>normal-^</ci>
       <apply>
        <times></times>
        <ci>s</ci>
        <ci>e</ci>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <ci>normal-^</ci>
        <ci>θ</ci>
       </apply>
       <times></times>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   t^{*}=(\hat{\theta}^{*}-\hat{\theta})/\hat{se}_{\hat{\theta}^{*}}
  </annotation>
 </semantics>
</math>

, while 

<math display="inline" id="Bootstrapping_(statistics):34">
 <semantics>
  <msub>
   <mover accent="true">
    <mrow>
     <mi>s</mi>
     <mi>e</mi>
    </mrow>
    <mo stretchy="false">^</mo>
   </mover>
   <mi>θ</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <ci>normal-^</ci>
     <apply>
      <times></times>
      <ci>s</ci>
      <ci>e</ci>
     </apply>
    </apply>
    <ci>θ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{se}_{\theta}
  </annotation>
 </semantics>
</math>

 is the estimated standard error of the coefficient in the original model.</p>

<p>The studentized test enjoys optimal properties as the statistic that is bootstrapped is <a href="Pivotal_quantity" title="wikilink">pivotal</a> (i.e. it does not depend on <a href="nuisance_parameter" title="wikilink">nuisance parameters</a> as the t-test follows asymptotically a N(0,1) distribution), unlike the percentile bootstrap.</p>
<ul>
<li><strong>Bias-Corrected Bootstrap</strong> - adjusts for <a href="Bias_(statistics)" title="wikilink">bias</a> in the bootstrap distribution.</li>
<li><strong>Accelerated Bootstrap</strong> - The bias-corrected and accelerated (BCa) bootstrap, by Efron (1987),<a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a> adjusts for both bias and <a class="uri" href="skewness" title="wikilink">skewness</a> in the bootstrap distribution. This approach is accurate in a wide variety of settings, has reasonable computation requirements, and produces reasonably narrow intervals.</li>
</ul>
<h2 id="example-applications">Example applications</h2>
<h3 id="smoothed-bootstrap">Smoothed bootstrap</h3>

<p>In 1878, <a href="Simon_Newcomb" title="wikilink">Simon Newcomb</a> took observations on the <a href="speed_of_light" title="wikilink">speed of light</a>.<a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a> The data set contains two <a href="outlier" title="wikilink">outliers</a>, which greatly <a href="influence_(statistics)" title="wikilink">influence</a> the <a href="sample_mean" title="wikilink">sample mean</a>. (Note that the sample mean need not be a <a href="consistent_estimator" title="wikilink">consistent estimator</a> for any <a href="population_mean" title="wikilink">population mean</a>, because no mean need exist for a <a href="heavy-tailed_distribution" title="wikilink">heavy-tailed distribution</a>.) A well-defined and <a href="robust_statistics" title="wikilink">robust statistic</a> for central tendency is the sample median, which is consistent and <a href="median-unbiased_estimator" title="wikilink">median-unbiased</a> for the population median.</p>

<p>The bootstrap distribution for Newcomb's data appears below. A convolution-method of regularization reduces the discreteness of the bootstrap distribution, by adding a small amount of <em>N</em>(0, <em>σ</em><sup>2</sup>) random noise to each bootstrap sample. A conventional choice is 

<math display="inline" id="Bootstrapping_(statistics):35">
 <semantics>
  <mrow>
   <mi>σ</mi>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>/</mo>
    <msqrt>
     <mi>n</mi>
    </msqrt>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>σ</ci>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <apply>
      <root></root>
      <ci>n</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sigma=1/\sqrt{n}
  </annotation>
 </semantics>
</math>

 for sample size <em>n</em>.</p>

<p>Histograms of the bootstrap distribution and the smooth bootstrap distribution appear below. The bootstrap distribution of the sample-median has only a small number of values. The smoothed bootstrap distribution has a richer <a href="support_(measure_theory)" title="wikilink">support</a>.</p>
<figure><b>(Figure)</b>
<figcaption>MedianHists.png</figcaption>
</figure>

<p>In this example, the bootstrapped 95% (percentile) confidence-interval for the population median is (26, 28.5), which is close to the interval for (25.98, 28.46) for the smoothed bootstrap.</p>
<h2 id="relation-to-other-approaches-to-inference">Relation to other approaches to inference</h2>
<h3 id="relationship-to-other-resampling-methods">Relationship to other resampling methods</h3>

<p>The bootstrap is distinguished from :</p>
<ul>
<li>the <a href="Jackknife_resampling" title="wikilink">jackknife</a> procedure, used to estimate biases of sample statistics and to estimate variances, and</li>
<li><a href="Cross-validation_(statistics)" title="wikilink">cross-validation</a>, in which the parameters (e.g., regression weights, factor loadings) that are estimated in one subsample are applied to another subsample.</li>
</ul>

<p>For more details see <a href="resampling_(statistics)#Bootstrap" title="wikilink">bootstrap resampling</a>.</p>

<p><a href="Bootstrap_aggregating" title="wikilink">Bootstrap aggregating</a> (bagging) is a <a class="uri" href="meta-algorithm" title="wikilink">meta-algorithm</a> based on averaging the results of multiple bootstrap samples.</p>
<h3 id="u-statistics">U-statistics</h3>

<p>In situations where an obvious statistic can be devised to measure a required characteristic using only a small number, <em>r</em>, of data items, a corresponding statistic based on the entire sample can be formulated. Given an <em>r</em>-sample statistic, one can create an <em>n</em>-sample statistic by something similar to bootstrapping (taking the average of the statistic over all subsamples of size <em>r</em>). This procedure is known to have certain good properties and the result is a <a class="uri" href="U-statistic" title="wikilink">U-statistic</a>. The <a href="sample_mean" title="wikilink">sample mean</a> and <a href="sample_variance" title="wikilink">sample variance</a> are of this form, for <em>r</em>=1 and <em>r</em>=2.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Accuracy_and_precision" title="wikilink">Accuracy and precision</a></li>
<li><a href="Bootstrap_aggregating" title="wikilink">Bootstrap aggregating</a></li>
<li><a href="Empirical_likelihood" title="wikilink">Empirical likelihood</a></li>
<li><a href="Imputation_(statistics)" title="wikilink">Imputation (statistics)</a></li>
<li><a href="Reliability_(statistics)" title="wikilink">Reliability (statistics)</a></li>
<li><a class="uri" href="Reproducibility" title="wikilink">Reproducibility</a></li>
<li><a href="Engelbart's_Law" title="wikilink">Engelbart's Law</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="further-reading">Further reading</h2>
<ul>
<li>

<p>popular-science</p></li>
<li></li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://people.revoledu.com/kardi/tutorial/Bootstrap/index.html">Bootstrap sampling tutorial using MS Excel</a></li>
<li><a href="http://excelandfinance.com/simulation-of-stock-prices/bootstrap/">Bootstrap example to simulate stock prices using MS Excel</a></li>
<li><a href="http://www.burns-stat.com/pages/Tutor/bootstrap_resampling.html">bootstrapping tutorial</a></li>
<li><a href="http://cran.r-project.org/package=animation">package animation</a></li>
</ul>
<h3 id="software">Software</h3>
<ul>
<li><a href="http://www.statistics101.net/">Statistics101: Resampling, Bootstrap, Monte Carlo Simulation program.</a> Free program written in Java to run on any operating system.</li>
</ul>

<p>"</p>

<p><a href="Category:Computational_statistics" title="wikilink">Category:Computational statistics</a> <a href="Category:Data_analysis" title="wikilink">Category:Data analysis</a> <a href="Category:Statistical_inference" title="wikilink">Category:Statistical inference</a> <a href="Category:Resampling_(statistics)" title="wikilink">Category:Resampling (statistics)</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"> <a href="http://lib.stat.cmu.edu/S/bootstrap.funs">software</a><a href="#fnref1">↩</a></li>
<li id="fn2">Second Thoughts on the Bootstrap - Bradley Efron, 2003<a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4">Weisstein, Eric W. "Bootstrap Methods." From MathWorld--A Wolfram Web Resource. <a class="uri" href="http://mathworld.wolfram.com/BootstrapMethods.html">http://mathworld.wolfram.com/BootstrapMethods.html</a><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="http://www.economics.soton.ac.uk/staff/aldrich/Mathematical%20Words.htm#boots">Notes for Earliest Known Uses of Some of the Words of Mathematics: Bootstrap</a> (John Aldrich)<a href="#fnref5">↩</a></li>
<li id="fn6"><a href="http://jeff560.tripod.com/b.html">Earliest Known Uses of Some of the Words of Mathematics (B)</a> (Jeff Miller)<a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8">Quenouille M (1949) Approximate tests of correlation in time-series. J Roy Statist Soc Ser B 11 68–84<a href="#fnref8">↩</a></li>
<li id="fn9">Tukey J (1958) Bias and confidence in not-quite large samples (abstract). Ann Math Statist 29 614<a href="#fnref9">↩</a></li>
<li id="fn10">Jaeckel L (1972) The infinitesimal jackknife. Memorandum MM72-1215-11, Bell Lab<a href="#fnref10">↩</a></li>
<li id="fn11">Bickel P, Freeman D (1981) Some asymptotic theory for the bootstrap. Ann Statist 9 1196–1217<a href="#fnref11">↩</a></li>
<li id="fn12">Singh K (1981) On the asymptotic accuracy of Efron’s bootstrap. Ann Statist 9 1187–1195<a href="#fnref12">↩</a></li>
<li id="fn13">Rubin D (1981). The Bayesian bootstrap. Ann Statist 9 130–134<a href="#fnref13">↩</a></li>
<li id="fn14"></li>
<li id="fn15">Diciccio T, Efron B (1992) More accurate confidence intervals in exponential families. Biometrika 79 231–245<a href="#fnref15">↩</a></li>
<li id="fn16"></li>
<li id="fn17">DiCiccio TJ, Efron B (1996) Bootstrap confidence intervals (with Discussion). Statistical Science 11: 189-228<a href="#fnref17">↩</a></li>
<li id="fn18">Efron B., R. J. Tibshirani, An introduction to the bootstrap, Chapman &amp; Hall/CRC 1998<a href="#fnref18">↩</a></li>
<li id="fn19">Rubin, D. B. (1981). "The Bayesian bootstrap". <em><a href="Annals_of_Statistics" title="wikilink">Annals of Statistics</a></em>, 9, 130.<a href="#fnref19">↩</a></li>
<li id="fn20"><a href="#fnref20">↩</a></li>
<li id="fn21"><a href="#fnref21">↩</a></li>
<li id="fn22">Künsch, H. R. (1989). “The jackknife and the bootstrap for general stationary observations,” Annals of Statistics, 17, 1217–1241.<a href="#fnref22">↩</a></li>
<li id="fn23">Politis, D.N. and Romano, J.P. (1994). The stationary bootstrap. Journal of American Statistical Association, 89, 1303-1313.<a href="#fnref23">↩</a></li>
<li id="fn24">Cameron, A. C., J. B. Gelbach, and D. L. Miller (2008): “Bootstrap-based im- provements for inference with clustered errors,” Review of Economics and Statistics, 90, 414–427<a href="#fnref24">↩</a></li>
<li id="fn25"><a href="#fnref25">↩</a></li>
<li id="fn26"><a href="#fnref26">↩</a></li>
<li id="fn27"><a href="#fnref27">↩</a></li>
<li id="fn28"><a href="#fnref28">↩</a></li>
<li id="fn29"><a href="http://www.stat.columbia.edu/~gelman/book/data/">Data from examples in <em>Bayesian Data Analysis</em></a><a href="#fnref29">↩</a></li>
</ol>
</section>
</body>
</html>
