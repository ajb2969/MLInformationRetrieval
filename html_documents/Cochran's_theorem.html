<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="1708">Cochran's theorem</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Cochran's theorem</h1>
<hr/>
<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, <strong>Cochran's theorem</strong>, devised by <a href="William_G._Cochran" title="wikilink">William G. Cochran</a>,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> is a <a class="uri" href="theorem" title="wikilink">theorem</a> used to justify results relating to the <a href="probability_distribution" title="wikilink">probability distributions</a> of statistics that are used in the <a href="analysis_of_variance" title="wikilink">analysis of variance</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h2 id="statement">Statement</h2>
<p>Suppose <em>U</em><sub>1</sub>, ..., <em>U</em><sub><em>n</em></sub> are <a href="statistical_independence" title="wikilink">independent</a> standard <a href="normal_distribution" title="wikilink">normally distributed</a> <a href="random_variable" title="wikilink">random variables</a>, and an identity of the form</p>
<p><span class="LaTeX">$$\sum_{i=1}^n U_i^2=Q_1+\cdots + Q_k$$</span></p>
<p>can be written, where each <em>Q</em><sub><em>i</em></sub> is a sum of squares of linear combinations of the <em>U</em>s. Further suppose that</p>
<p><span class="LaTeX">$$r_1+\cdots +r_k=n$$</span></p>
<p>where <em>r</em><sub><em>i</em></sub> is the <a href="rank_(linear_algebra)" title="wikilink">rank</a> of <em>Q</em><sub><em>i</em></sub>. Cochran's theorem states that the <em>Q</em><sub><em>i</em></sub> are independent, and each <em>Q</em><sub><em>i</em></sub> has a <a href="chi-squared_distribution" title="wikilink">chi-squared distribution</a> with <em>r</em><sub><em>i</em></sub> <a href="degrees_of_freedom_(statistics)" title="wikilink">degrees of freedom</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> Here the rank of <em>Q</em><sub><em>i</em></sub> should be interpreted as meaning the rank of the matrix <em>B</em><sup>(<em>i</em>)</sup>, with elements <em>B</em><sub><em>j,k</em></sub><sup>(<em>i</em>)</sup>, in the representation of <em>Q</em><sub><em>i</em></sub> as a <a href="quadratic_form" title="wikilink">quadratic form</a>:</p>
<p><span class="LaTeX">$$Q_i=\sum_{j=1}^n\sum_{k=1}^n U_j B_{j,k}^{(i)} U_k .$$</span></p>
<p>Less formally, it is the number of linear combinations included in the sum of squares defining <em>Q</em><sub><em>i</em></sub>, provided that these linear combinations are linearly independent.</p>
<h3 id="proof">Proof</h3>
<p>We first show that the matrices <em>B</em><sup>(<em>i</em>)</sup> can be <a href="Matrix_diagonalization#Simultaneous_diagonalization" title="wikilink">simultaneously diagonalized</a> and that their non-zero <a href="eigenvalue" title="wikilink">eigenvalues</a> are all equal to +1. We then use the <a href="Basis_(linear_algebra)" title="wikilink">vector basis</a> that diagonalize them to simplify their <a href="Characteristic_function_(probability_theory)" title="wikilink">characteristic function</a> and show their independence and distribution.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<p>Each of the matrices <em>B</em><sup>(<em>i</em>)</sup> has <a href="rank_(linear_algebra)" title="wikilink">rank</a> <em>r</em><sub><em>i</em></sub> and so has exactly <em>r</em><sub><em>i</em></sub> non-zero <a href="eigenvalue" title="wikilink">eigenvalues</a>. For each <em>i</em>, the sum <span class="LaTeX">$C^{(i)} \equiv \sum_{j\ne i}B^{(j)}$</span> has at most rank <span class="LaTeX">$\sum_{j\ne i}r_j = N-r_i$</span>. Since <span class="LaTeX">$B^{(i)}+C^{(i)} = I_{N \times N}$</span>, it follows that <em>C</em><sup>(<em>i</em>)</sup> has exactly rank N-<em>r</em><sub><em>i</em></sub>.</p>
<p>Therefore <em>B</em><sup>(<em>i</em>)</sup> and <em>C</em><sup>(<em>i</em>)</sup> can be <a href="Matrix_diagonalization#Simultaneous_diagonalization" title="wikilink">simultaneously diagonalized</a>. This can be shown by first diagonalizing <em>B</em><sup>(<em>i</em>)</sup>. In this basis, it is of the form:</p>
<p><span class="LaTeX">$$\begin{bmatrix}
\lambda_1      & 0          & ... & ...           & ... &0 \\
0              & \lambda_2  & 0   & ...           & ... & 0 \\
0              &  ...       & ... & ...           & ... & 0\\
0              &  ...       &0    & \lambda_{r_i} & 0 &... \\
 0 & ...      &    & 0             & 0...&0\\
 0 & ...      &    & 0             & ...&...\\
 0 & ...      &    & 0             & 0...&0
 \end{bmatrix}.$$</span></p>
<p>Thus the lower <span class="LaTeX">$(N-r_i)$</span> rows are zero. Since <span class="LaTeX">$C^{(i)} = I - B^{(i)}$</span>, it follows these rows in <em>C</em><sup>(<em>i</em>)</sup> in this basis contain a right block which is a <span class="LaTeX">$(N-r_i)\times(N-r_i)$</span> unit matrix, with zeros in the rest of these rows. But since <em>C</em><sup>(<em>i</em>)</sup> has rank N-<em>r</em><sub><em>i</em></sub>, it must be zero elsewhere. Thus it is diagonal in this basis as well. Moreover, it follows that all the non-zero <a href="eigenvalue" title="wikilink">eigenvalues</a> of both <em>B</em><sup>(<em>i</em>)</sup> and <em>C</em><sup>(<em>i</em>)</sup> are +1.</p>
<p>It follows that the non-zero eigenvalues of all the <em>B</em>-s are equal to +1. Moreover, the above analysis can be repeated in the diagonal basis for <span class="LaTeX">$C^{(1)} = B^{(2)} + \sum_{j>2}B^{(j)}$</span>. In this basis <span class="LaTeX">$C^{(1)}$</span> is the identity of an <span class="LaTeX">$(N-r_i)\times(N-r_i)$</span> vector space, so it follows that both <em>B</em><sup>(<em>2</em>)</sup> and <span class="LaTeX">$\sum_{j>2}B^{(j)}$</span> are simultaneously diagonalizable in this vector space (and hence also together <em>B</em><sup>(<em>1</em>)</sup>). By repeating this over and over it follows that all the <em>B</em>-s are simultaneously diagonalizable.</p>
<p>Thus there exists an <a href="orthogonal_matrix" title="wikilink">orthogonal matrix</a> S such that for all i between 1 and <em>k</em><span class="LaTeX">$$S^\mathrm{T}B^{(i)} S$$</span> is diagonal with the diagonal having 1-s at the places between <span class="LaTeX">$r_1 + ... + r_{i-1} +1$</span> and <span class="LaTeX">$r_1 + ... + r_i$</span>.</p>
<p>Let <span class="LaTeX">$U_i^\prime$</span> be the independent variables <span class="LaTeX">$U_i$</span> after transformation by S.</p>
<p>The characteristic function of <em>Q</em><sub><em>i</em></sub> is:</p>
<p><span class="LaTeX">$$\begin{align}
\varphi_i(t) =& (2\pi)^{-N/2} \int dU_1 \int dU_2 ... \int dU_N e^{i t Q_i} \cdot e^{-\frac{U_1^2}{2}}\cdot e^{-\frac{U_2^2}{2}}\cdot ...e^{-\frac{U_N^2}{2}} = (2\pi)^{-N/2} \left(\prod_{j=1}^N \int dU_j\right) e^{i t Q_i} \cdot e^{-\sum_{j=1}^N \frac{U_j^2}{2}} \\
=& (2\pi)^{-N/2} \left(\prod_{j=1}^N \int dU_j^\prime\right) e^{i t\cdot \sum_{m = r_1+...+r_{i-1}+1}^{r_1+...+r_i} (U_m^\prime)^2} \cdot e^{-\sum_{j=1}^N \frac{{U_j^\prime}^2}{2}}  \\
=& (1 - 2 i t)^{-r_i/2} 
\end{align}$$</span></p>
<p>This is the <a href="Fourier_transform" title="wikilink">Fourier transform</a> of the <a href="chi-squared_distribution" title="wikilink">chi-squared distribution</a> with <em>r</em><sub><em>i</em></sub> degrees of freedom. Therefore this is the distribution of <em>Q</em><sub><em>i</em></sub>.</p>
<p>Moreover, the characteristic function of the joint distribution of all the <em>Q</em><sub><em>i</em></sub>-s is:</p>
<p><span class="LaTeX">$$\begin{align}
\varphi(t_1, t_2... t_k) =& (2\pi)^{-N/2} \left(\prod_{j=1}^N \int dU_j\right) e^{i \sum_{i=1}^k t_i \cdot Q_i} \cdot e^{-\sum_{j=1}^N \frac{U_j^2}{2}} \\
=& (2\pi)^{-N/2} \left(\prod_{j=1}^N \int dU_j^\prime\right) e^{i \cdot \sum_{i=1}^k t_i \sum_{k = r_1+...+r_{i-1}+1}^{r_1+...+r_i}  (U_k^\prime)^2} \cdot e^{-\sum_{j=1}^N \frac{{U_j^\prime}^2}{2}}  \\
=& \prod_{i=1}^k (1 - 2 i t_i)^{-r_i/2} = \prod_{i=1}^k \varphi_i(t_i)
\end{align}$$</span></p>
<p>From which it follows that all the <em>Q</em><sub><em>i</em></sub>-s are statistically independent.</p>
<h2 id="examples">Examples</h2>
<h3 id="sample-mean-and-sample-variance">Sample mean and sample variance</h3>
<p>If <em>X</em><sub>1</sub>, ..., <em>X</em><sub><em>n</em></sub> are independent normally distributed random variables with mean μ and standard deviation σ then</p>
<p><span class="LaTeX">$$U_i = \frac{X_i-\mu}{\sigma}$$</span></p>
<p>is <a href="standard_normal" title="wikilink">standard normal</a> for each <em>i</em>. It is possible to write</p>
<p><span class="LaTeX">$$\sum_{i=1}^n U_i^2=\sum_{i=1}^n\left(\frac{X_i-\overline{X}}{\sigma}\right)^2
+ n\left(\frac{\overline{X}-\mu}{\sigma}\right)^2$$</span></p>
<p>(here <span class="LaTeX">$\overline{X}$</span> is the <a href="Arithmetic_mean" title="wikilink">sample mean</a>). To see this identity, multiply throughout by <span class="LaTeX">$\sigma^2$</span> and note that</p>
<p><span class="LaTeX">$$\sum(X_i-\mu)^2=
\sum(X_i-\overline{X}+\overline{X}-\mu)^2$$</span></p>
<p>and expand to give</p>
<p><span class="LaTeX">$$\sum(X_i-\mu)^2=
\sum(X_i-\overline{X})^2+\sum(\overline{X}-\mu)^2+
2\sum(X_i-\overline{X})(\overline{X}-\mu).$$</span></p>
<p>The third term is zero because it is equal to a constant times</p>
<p><span class="LaTeX">$$\sum(\overline{X}-X_i)=0,$$</span></p>
<p>and the second term has just <em>n</em> identical terms added together. Thus</p>
<p><span class="LaTeX">$$\sum(X_i-\mu)^2=
\sum(X_i-\overline{X})^2+n(\overline{X}-\mu)^2 ,$$</span></p>
<p>and hence</p>
<p><span class="LaTeX">$$\sum\left(\frac{X_i-\mu}{\sigma}\right)^2=
\sum\left(\frac{X_i-\overline{X}}{\sigma}\right)^2
+n\left(\frac{\overline{X}-\mu}{\sigma}\right)^2
=Q_1+Q_2.$$</span></p>
<p>Now the rank of <em>Q</em><sub>2</sub> is just 1 (it is the square of just one linear combination of the standard normal variables). The rank of <em>Q</em><sub>1</sub> can be shown to be <em>n</em> − 1, and thus the conditions for Cochran's theorem are met.</p>
<p>Cochran's theorem then states that <em>Q</em><sub>1</sub> and <em>Q</em><sub>2</sub> are independent, with chi-squared distributions with <em>n</em> − 1 and 1 degree of freedom respectively. This shows that the sample mean and <a href="sample_variance" title="wikilink">sample variance</a> are independent. This can also be shown by <a href="Basu's_theorem" title="wikilink">Basu's theorem</a>, and in fact this property <em>characterizes</em> the normal distribution – for no other distribution are the sample mean and sample variance independent.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h3 id="distributions">Distributions</h3>
<p>The result for the distributions is written symbolically as</p>
<p><span class="LaTeX">$$\sum\left(X_i-\overline{X}\right)^2  \sim \sigma^2 \chi^2_{n-1}.$$</span></p>
<p><span class="LaTeX">$$n(\overline{X}-\mu)^2\sim \sigma^2 \chi^2_1,$$</span></p>
<p>Both these random variables are proportional to the true but unknown variance σ<sup>2</sup>. Thus their ratio does not depend on σ<sup>2</sup> and, because they are statistically independent. The distribution of their ratio is given by</p>
<p><span class="LaTeX">$$\frac{n\left(\overline{X}-\mu\right)^2}
{\frac{1}{n-1}\sum\left(X_i-\overline{X}\right)^2}\sim \frac{\chi^2_1}{\frac{1}{n-1}\chi^2_{n-1}}
   \sim F_{1,n-1}$$</span></p>
<p>where <em>F</em><sub>1,<em>n</em> − 1</sub> is the <a class="uri" href="F-distribution" title="wikilink">F-distribution</a> with 1 and <em>n</em> − 1 degrees of freedom (see also <a href="Student's_t-distribution" title="wikilink">Student's t-distribution</a>). The final step here is effectively the definition of a random variable having the F-distribution.</p>
<h3 id="estimation-of-variance">Estimation of variance</h3>
<p>To estimate the variance σ<sup>2</sup>, one estimator that is sometimes used is the <a href="maximum_likelihood" title="wikilink">maximum likelihood</a> estimator of the variance of a normal distribution</p>
<p><span class="LaTeX">$$\widehat{\sigma}^2=
\frac{1}{n}\sum\left(
X_i-\overline{X}\right)^2.$$</span></p>
<p>Cochran's theorem shows that</p>
<p><span class="LaTeX">$$\frac{n\widehat{\sigma}^2}{\sigma^2}\sim\chi^2_{n-1}$$</span></p>
<p>and the properties of the chi-squared distribution show that the expected value of <span class="LaTeX">$\widehat{\sigma}^2$</span> is σ<sup>2</sup>(<em>n</em> − 1)/<em>n</em>.</p>
<h2 id="alternative-formulation">Alternative formulation</h2>
<p>The following version is often seen when considering linear regression. Suppose that <span class="LaTeX">$Y\sim N_n(0,\sigma^2I_n)$</span> is a standard <a href="Multivariate_normal_distribution" title="wikilink">multivariate normal</a> <a href="random_vector" title="wikilink">random vector</a> (here <span class="LaTeX">$I_n$</span> denotes the n-by-n <a href="identity_matrix" title="wikilink">identity matrix</a>), and if <span class="LaTeX">$A_1,\ldots,A_k$</span> are all n-by-n <a href="symmetric_matrices" title="wikilink">symmetric matrices</a> with <span class="LaTeX">$\sum_{i=1}^kA_i=I_n$</span>. Then, on defining <span class="LaTeX">$r_i=Rank(A_i)$</span>, any one of the following conditions implies the other two:</p>
<ul>
<li><span class="LaTeX">$\sum_{i=1}^kr_i=n ,$</span></li>
<li><span class="LaTeX">$Y^TA_iY\sim\sigma^2\chi^2_{r_i}$</span> (thus the <span class="LaTeX">$A_i$</span> are <a href="positive_semidefinite" title="wikilink">positive semidefinite</a>)</li>
<li><span class="LaTeX">$Y^TA_iY$</span> is independent of <span class="LaTeX">$Y^TA_jY$</span> for <span class="LaTeX">$i\neq j .$</span></li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Cramér's_theorem" title="wikilink">Cramér's theorem</a>, on decomposing normal distribution</li>
<li><a href="Infinite_divisibility_(probability)" title="wikilink">Infinite divisibility (probability)</a></li>
</ul>
<h2 id="references">References</h2>
<references>
</references>
<p>"</p>
<p><a href="Category:Statistical_theorems" title="wikilink">Category:Statistical theorems</a> <a href="Category:Characterization_of_probability_distributions" title="wikilink">Category:Characterization of probability distributions</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4">Craig A.T. (1938) On The Independence of Certain Estimates of Variances. Ann. Math. Statist. 9, pp. 48-55<a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
</ol>
</section>
</body>
</html>
