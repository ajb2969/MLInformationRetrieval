<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1726">Language model</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Language model</h1>
<hr/>

<p>A statistical <strong>language model</strong> is a <a href="probability_distribution" title="wikilink">probability distribution</a> over sequences of words. Given such a sequence, say of length 

<math display="inline" id="Language_model:0">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

, it assigns a probability 

<math display="inline" id="Language_model:1">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>w</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mi>m</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <vector>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-…</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>m</ci>
     </apply>
    </vector>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(w_{1},\ldots,w_{m})
  </annotation>
 </semantics>
</math>

 to the whole sequence. Having a way to estimate the relative likelihood of different phrases is useful in many <a href="natural_language_processing" title="wikilink">natural language processing</a> applications. Language modeling is used in <a href="speech_recognition" title="wikilink">speech recognition</a>, <a href="machine_translation" title="wikilink">machine translation</a>, <a href="part-of-speech_tagging" title="wikilink">part-of-speech tagging</a>, <a class="uri" href="parsing" title="wikilink">parsing</a>, <a href="handwriting_recognition" title="wikilink">handwriting recognition</a>, <a href="information_retrieval" title="wikilink">information retrieval</a> and other applications.</p>

<p>In speech recognition, the computer tries to match sounds with word sequences. The language model provides context to distinguish between words and phrases that sound similar. For example, in <a href="American_English" title="wikilink">American English</a>, the phrases "recognize speech" and "wreck a nice beach" are pronounced almost the same but mean very different things. These ambiguities are easier to resolve when evidence from the language model is incorporated with the pronunciation model and the <a href="acoustic_model" title="wikilink">acoustic model</a>.</p>

<p>Language models are used in information retrieval in the <a href="query_likelihood_model" title="wikilink">query likelihood model</a>. Here a separate language model is associated with each <a class="uri" href="document" title="wikilink">document</a> in a collection. Documents are ranked based on the probability of the query <em>Q</em> in the document's language model 

<math display="inline" id="Language_model:2">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>Q</mi>
    <mo>∣</mo>
    <msub>
     <mi>M</mi>
     <mi>d</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">Q</csymbol>
     <ci>normal-∣</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>M</ci>
      <ci>d</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(Q\mid M_{d})
  </annotation>
 </semantics>
</math>

. Commonly, the <a class="uri" href="unigram" title="wikilink">unigram</a> language model is used for this purpose—otherwise known as the <a href="bag_of_words_model" title="wikilink">bag of words model</a>.</p>

<p>Data sparsity is a major problem in building language models. Most possible word sequences will not be observed in training. One solution is to make the assumption that the probability of a word only depends on the previous <em>n</em> words. This is known as an <a href="n-gram" title="wikilink"><em>n</em>-gram</a> model or unigram model when <em>n</em> = 1.</p>
<h2 id="unigram-models">Unigram models</h2>

<p>A unigram model used in information retrieval can be treated as the combination of several one-state <a href="Finite-state_machine" title="wikilink">finite automata</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> It splits the probabilities of different terms in a context, e.g. from 

<math display="inline" id="Language_model:3">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>t</mi>
     <mn>1</mn>
    </msub>
    <msub>
     <mi>t</mi>
     <mn>2</mn>
    </msub>
    <msub>
     <mi>t</mi>
     <mn>3</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>t</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>t</mi>
     <mn>2</mn>
    </msub>
    <mo>∣</mo>
    <msub>
     <mi>t</mi>
     <mn>1</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>t</mi>
     <mn>3</mn>
    </msub>
    <mo>∣</mo>
    <msub>
     <mi>t</mi>
     <mn>1</mn>
    </msub>
    <msub>
     <mi>t</mi>
     <mn>2</mn>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">2</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">3</cn>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>normal-∣</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">3</cn>
     </apply>
     <ci>normal-∣</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(t_{1}t_{2}t_{3})=P(t_{1})P(t_{2}\mid t_{1})P(t_{3}\mid t_{1}t_{2})
  </annotation>
 </semantics>
</math>

 to 

<math display="inline" id="Language_model:4">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>P</mi>
     <mtext>uni</mtext>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>t</mi>
       <mn>1</mn>
      </msub>
      <msub>
       <mi>t</mi>
       <mn>2</mn>
      </msub>
      <msub>
       <mi>t</mi>
       <mn>3</mn>
      </msub>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>P</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>t</mi>
      <mn>1</mn>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mi>P</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>t</mi>
      <mn>2</mn>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mi>P</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>t</mi>
      <mn>3</mn>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>P</ci>
      <mtext>uni</mtext>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>t</ci>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>t</ci>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>t</ci>
       <cn type="integer">3</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>P</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>P</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>P</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>t</ci>
      <cn type="integer">3</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P_{\text{uni}}(t_{1}t_{2}t_{3})=P(t_{1})P(t_{2})P(t_{3})
  </annotation>
 </semantics>
</math>

.</p>

<p>In this model, the probability to hit each word all depends on its own, so we only have one-state finite automata as units. For each automaton, we only have one way to hit its only state, assigned with one probability. Viewing from the whole model, the sum of all the one-state-hitting probabilities should be 1. Followed is an illustration of a unigram model of a document.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">
<p>Terms</p></th>
<th style="text-align: left;">
<p>Probability in doc</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p>a</p></td>
<td style="text-align: left;">
<p>0.1</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>world</p></td>
<td style="text-align: left;">
<p>0.2</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>likes</p></td>
<td style="text-align: left;">
<p>0.05</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>we</p></td>
<td style="text-align: left;">
<p>0.05</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>share</p></td>
<td style="text-align: left;">
<p>0.3</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>...</p></td>
<td style="text-align: left;">
<p>...</p></td>
</tr>
</tbody>
</table>

<p>

<math display="block" id="Language_model:5">
 <semantics>
  <mrow>
   <mrow>
    <munder>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mtext>term in doc</mtext>
    </munder>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mtext>term</mtext>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mpadded width="+1.7pt">
    <mn>1</mn>
   </mpadded>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <sum></sum>
      <mtext>term in doc</mtext>
     </apply>
     <apply>
      <times></times>
      <ci>P</ci>
      <mtext>term</mtext>
     </apply>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{\text{term in doc}}P(\text{term})=1\,
  </annotation>
 </semantics>
</math>

</p>

<p>The probability generated for a specific query is calculated as</p>

<p>

<math display="block" id="Language_model:6">
 <semantics>
  <mrow>
   <mrow>
    <mi>P</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mtext>query</mtext>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <munder>
     <mo largeop="true" movablelimits="false" symmetric="true">∏</mo>
     <mtext>term in query</mtext>
    </munder>
    <mrow>
     <mi>P</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mtext>term</mtext>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>P</ci>
     <mtext>query</mtext>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <csymbol cd="latexml">product</csymbol>
      <mtext>term in query</mtext>
     </apply>
     <apply>
      <times></times>
      <ci>P</ci>
      <mtext>term</mtext>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(\text{query})=\prod_{\text{term in query}}P(\text{term})
  </annotation>
 </semantics>
</math>

</p>

<p>For different documents, we can build their own unigram models, with different hitting probabilities of words in it. And we use probabilities from different documents to generate different hitting probabilities for a query. Then we can rank documents for a query according to the generating probabilities. Next is an example of two unigram models of two documents.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">
<p>Terms</p></th>
<th style="text-align: left;">
<p>Probability in Doc1</p></th>
<th style="text-align: left;">
<p>Probability in Doc2</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p>a</p></td>
<td style="text-align: left;">
<p>0.1</p></td>
<td style="text-align: left;">
<p>0.3</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>world</p></td>
<td style="text-align: left;">
<p>0.2</p></td>
<td style="text-align: left;">
<p>0.1</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>likes</p></td>
<td style="text-align: left;">
<p>0.05</p></td>
<td style="text-align: left;">
<p>0.03</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>we</p></td>
<td style="text-align: left;">
<p>0.05</p></td>
<td style="text-align: left;">
<p>0.02</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>share</p></td>
<td style="text-align: left;">
<p>0.3</p></td>
<td style="text-align: left;">
<p>0.2</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>...</p></td>
<td style="text-align: left;">
<p>...</p></td>
<td style="text-align: left;">
<p>...</p></td>
</tr>
</tbody>
</table>

<p>In information retrieval contexts, unigram language models are often smoothed to avoid instances where <em>P</em>(term) = 0. A common approach is to generate a maximum-likelihood model for the entire collection and <a href="Linear_interpolation" title="wikilink">linearly interpolate</a> the collection model with a maximum-likelihood model for each document to create a smoothed document model.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h2 id="n-gram-models"><em>n</em>-gram models</h2>

<p>In an <em>n</em>-gram model, the probability 

<math display="inline" id="Language_model:7">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>w</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mi>m</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <vector>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-…</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>m</ci>
     </apply>
    </vector>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(w_{1},\ldots,w_{m})
  </annotation>
 </semantics>
</math>

 of observing the sentence 

<math display="inline" id="Language_model:8">
 <semantics>
  <mrow>
   <msub>
    <mi>w</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">…</mi>
   <mo>,</mo>
   <msub>
    <mi>w</mi>
    <mi>m</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-…</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>w</ci>
     <ci>m</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{1},\ldots,w_{m}
  </annotation>
 </semantics>
</math>

 is approximated as</p>

<p>

<math display="block" id="Language_model:9">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>w</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mi>m</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <munderover>
    <mo largeop="true" movablelimits="false" symmetric="true">∏</mo>
    <mrow>
     <mi>i</mi>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mi>m</mi>
   </munderover>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>w</mi>
     <mi>i</mi>
    </msub>
    <mo>∣</mo>
    <msub>
     <mi>w</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>i</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>≈</mo>
   <munderover>
    <mo largeop="true" movablelimits="false" symmetric="true">∏</mo>
    <mrow>
     <mi>i</mi>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mi>m</mi>
   </munderover>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>w</mi>
     <mi>i</mi>
    </msub>
    <mo>∣</mo>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>i</mi>
      <mo>-</mo>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>n</mi>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>i</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>m</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <csymbol cd="latexml">product</csymbol>
      <ci>m</ci>
     </apply>
     <apply>
      <eq></eq>
      <ci>i</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>i</ci>
     </apply>
     <ci>normal-∣</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <minus></minus>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <approx></approx>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <csymbol cd="latexml">product</csymbol>
      <ci>m</ci>
     </apply>
     <apply>
      <eq></eq>
      <ci>i</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>i</ci>
     </apply>
     <ci>normal-∣</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <minus></minus>
       <ci>i</ci>
       <apply>
        <minus></minus>
        <ci>n</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
     </apply>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <minus></minus>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(w_{1},\ldots,w_{m})=\prod^{m}_{i=1}P(w_{i}\mid w_{1},\ldots,w_{i-1})\approx%
\prod^{m}_{i=1}P(w_{i}\mid w_{i-(n-1)},\ldots,w_{i-1})
  </annotation>
 </semantics>
</math>

</p>

<p>Here, it is assumed that the probability of observing the <em>i<sup>th</sup></em> word <em>w<sub>i</sub></em> in the context history of the preceding <em>i</em> − 1 words can be approximated by the probability of observing it in the shortened context history of the preceding <em>n</em> − 1 words (<em>n</em><sup>th</sup> order <a href="Markov_property" title="wikilink">Markov property</a>).</p>

<p>The conditional probability can be calculated from <em>n</em>-gram model frequency counts:</p>

<p>

<math display="block" id="Language_model:10">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>w</mi>
     <mi>i</mi>
    </msub>
    <mo>∣</mo>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>i</mi>
      <mo>-</mo>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>n</mi>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>i</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>count</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>w</mi>
       <mrow>
        <mi>i</mi>
        <mo>-</mo>
        <mrow>
         <mo stretchy="false">(</mo>
         <mrow>
          <mi>n</mi>
          <mo>-</mo>
          <mn>1</mn>
         </mrow>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </msub>
      <mo>,</mo>
      <mi mathvariant="normal">…</mi>
      <mo>,</mo>
      <msub>
       <mi>w</mi>
       <mrow>
        <mi>i</mi>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
      </msub>
      <mo>,</mo>
      <msub>
       <mi>w</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mrow>
     <mi>count</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>w</mi>
       <mrow>
        <mi>i</mi>
        <mo>-</mo>
        <mrow>
         <mo stretchy="false">(</mo>
         <mrow>
          <mi>n</mi>
          <mo>-</mo>
          <mn>1</mn>
         </mrow>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </msub>
      <mo>,</mo>
      <mi mathvariant="normal">…</mi>
      <mo>,</mo>
      <msub>
       <mi>w</mi>
       <mrow>
        <mi>i</mi>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>i</ci>
     </apply>
     <ci>normal-∣</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <minus></minus>
       <ci>i</ci>
       <apply>
        <minus></minus>
        <ci>n</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
     </apply>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <minus></minus>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <ci>count</ci>
      <vector>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <apply>
         <minus></minus>
         <ci>i</ci>
         <apply>
          <minus></minus>
          <ci>n</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
       </apply>
       <ci>normal-…</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <apply>
         <minus></minus>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <ci>i</ci>
       </apply>
      </vector>
     </apply>
     <apply>
      <times></times>
      <ci>count</ci>
      <vector>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <apply>
         <minus></minus>
         <ci>i</ci>
         <apply>
          <minus></minus>
          <ci>n</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
       </apply>
       <ci>normal-…</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <apply>
         <minus></minus>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
      </vector>
     </apply>
    </apply>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(w_{i}\mid w_{i-(n-1)},\ldots,w_{i-1})=\frac{\mathrm{count}(w_{i-(n-1)},%
\ldots,w_{i-1},w_{i})}{\mathrm{count}(w_{i-(n-1)},\ldots,w_{i-1})}
  </annotation>
 </semantics>
</math>

</p>

<p>The words <strong>bigram</strong> and <strong>trigram</strong> language model denote <em>n</em>-gram model language models with <em>n</em> = 2 and <em>n</em> = 3, respectively.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>Typically, however, the <em>n</em>-gram model probabilities are not derived directly from the frequency counts, because models derived this way have severe problems when confronted with any <em>n</em>-grams that have not explicitly been seen before. Instead, some form of <em>smoothing</em> is necessary, assigning some of the total probability mass to unseen words or <em>n</em>-grams. Various methods are used, from simple "add-one" smoothing (assign a count of 1 to unseen <em>n</em>-grams) to more sophisticated models, such as <a href="Good-Turing_discounting" title="wikilink">Good-Turing discounting</a> or <a href="Katz's_back-off_model" title="wikilink">back-off models</a>.</p>
<h3 id="example">Example</h3>

<p>In a bigram (<em>n</em> = 2) language model, the probability of the sentence <em>I saw the red house</em> is approximated as</p>

<p>

<math display="inline" id="Language_model:11">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mtext>I, saw, the, red, house</mtext>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <mtext>I, saw, the, red, house</mtext>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle P(\text{I, saw, the, red, house})
  </annotation>
 </semantics>
</math>


</p>

<p>whereas in a trigram (<em>n</em> = 3) language model, the approximation is</p>

<p>

<math display="inline" id="Language_model:12">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mtext>I, saw, the, red, house</mtext>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <mtext>I, saw, the, red, house</mtext>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle P(\text{I, saw, the, red, house})
  </annotation>
 </semantics>
</math>


</p>

<p>Note that the context of the first <em>n</em> – 1 <em>n</em>-grams is filled with start-of-sentence markers, typically denoted <s></s>.</p>

<p>Additionally, without an end-of-sentence marker, the probability of an ungrammatical sequence <em>*I saw the</em> would always be higher than that of the longer sentence <em>I saw the red house.</em></p>
<h2 id="neural-net-language-models">Neural net language models</h2>

<p>A <em>neural net language model</em> is a <a href="Artificial_neural_network" title="wikilink">neural network</a> trained to predict word probabilities. Such networks alleviate the <a href="curse_of_dimensionality" title="wikilink">curse of dimensionality</a> in language modeling: as language models are trained on larger and larger texts, the number of unique words (the vocabulary) increases and the number of possible sequences of words increases <a href="Exponential_growth" title="wikilink">exponentially</a> with the size of the vocabulary, causing a data sparsity problem because for each of the exponentially many sequences, statistics are needed to properly estimate probabilities. Neural networks avoid this problem by representing words in a <a href="distributed_representation" title="wikilink">distributed</a> way, as non-linear combinations of weights in a neural net.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> The neural net architecture might be feed-forward or <a href="recurrent_neural_network" title="wikilink">recurrent</a>.</p>

<p>Typically, neural net language models are constructed and trained as <a href="probabilistic_classifier" title="wikilink">probabilistic classifiers</a> that learn to predict a probability distribution</p>

<p>

<math display="block" id="Language_model:13">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>w</mi>
     <mi>t</mi>
    </msub>
    <mo stretchy="false">|</mo>
    <mi>context</mi>
    <mo rspace="4.2pt" stretchy="false">)</mo>
   </mrow>
   <mo>∀</mo>
   <mi>t</mi>
   <mo>∈</mo>
   <mi>V</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>t</ci>
     </apply>
     <ci>normal-|</ci>
     <csymbol cd="unknown">context</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="latexml">for-all</csymbol>
    <csymbol cd="unknown">t</csymbol>
    <in></in>
    <csymbol cd="unknown">V</csymbol>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(w_{t}|\mathrm{context})\,\forall t\in V
  </annotation>
 </semantics>
</math>

.</p>

<p>I.e., the network is trained to predict a probability distribution over the vocabulary, given some linguistic context. This is done using standard neural net training algorithms such as <a href="stochastic_gradient_descent" title="wikilink">stochastic gradient descent</a> with <a class="uri" href="backpropagation" title="wikilink">backpropagation</a>. The context might be a fixed-size window of previous words, so that the network predicts</p>

<p>

<math display="block" id="Language_model:14">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>w</mi>
     <mi>t</mi>
    </msub>
    <mo stretchy="false">|</mo>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>t</mi>
      <mo>-</mo>
      <mi>k</mi>
     </mrow>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>t</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>t</ci>
     </apply>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <minus></minus>
       <ci>t</ci>
       <ci>k</ci>
      </apply>
     </apply>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <minus></minus>
       <ci>t</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(w_{t}|w_{t-k},\dots,w_{t-1})
  </annotation>
 </semantics>
</math>

</p>

<p>from a <a href="feature_vector" title="wikilink">feature vector</a> representing the previous 

<math display="inline" id="Language_model:15">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

 words. Another option is to use "future" words as well as "past" words as features, so that the estimated probability is<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>

<p>

<math display="block" id="Language_model:16">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>w</mi>
     <mi>t</mi>
    </msub>
    <mo stretchy="false">|</mo>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>t</mi>
      <mo>-</mo>
      <mi>k</mi>
     </mrow>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>t</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>t</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>t</mi>
      <mo>+</mo>
      <mi>k</mi>
     </mrow>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>t</ci>
     </apply>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <minus></minus>
       <ci>t</ci>
       <ci>k</ci>
      </apply>
     </apply>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <minus></minus>
       <ci>t</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <plus></plus>
       <ci>t</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <plus></plus>
       <ci>t</ci>
       <ci>k</ci>
      </apply>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(w_{t}|w_{t-k},\dots,w_{t-1},w_{t+1},\dots,w_{t+k})
  </annotation>
 </semantics>
</math>

.</p>

<p>A third option, that allows faster training, is to invert the previous problem and make a neural network learn the context, given a word. One then maximizes the log-probability<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>

<p>

<math display="block" id="Language_model:17">
 <semantics>
  <mrow>
   <munder>
    <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
    <mrow>
     <mrow>
      <mrow>
       <mo>-</mo>
       <mi>k</mi>
      </mrow>
      <mo>≤</mo>
      <mrow>
       <mi>j</mi>
       <mo>-</mo>
       <mn>1</mn>
      </mrow>
     </mrow>
     <mo rspace="4.2pt">,</mo>
     <mrow>
      <mi>j</mi>
      <mo>≤</mo>
      <mi>k</mi>
     </mrow>
    </mrow>
   </munder>
   <mi>log</mi>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>t</mi>
      <mo>+</mo>
      <mi>j</mi>
     </mrow>
    </msub>
    <mo stretchy="false">|</mo>
    <msub>
     <mi>w</mi>
     <mi>t</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <sum></sum>
     <apply>
      <csymbol cd="ambiguous">formulae-sequence</csymbol>
      <apply>
       <leq></leq>
       <apply>
        <minus></minus>
        <ci>k</ci>
       </apply>
       <apply>
        <minus></minus>
        <ci>j</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <apply>
       <leq></leq>
       <ci>j</ci>
       <ci>k</ci>
      </apply>
     </apply>
    </apply>
    <log></log>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <plus></plus>
       <ci>t</ci>
       <ci>j</ci>
      </apply>
     </apply>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>t</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{-k\leq j-1,\,j\leq k}\log P(w_{t+j}|w_{t})
  </annotation>
 </semantics>
</math>

</p>

<p>This is called a <a class="uri" href="skip-gram" title="wikilink">skip-gram</a> language model, and is the basis of the popular<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> word2vec program.</p>

<p>Instead of using neural net language models to produce actual probabilities, it is common to instead use the distributed representation encoded in the networks' "hidden" layers as representations of words; each word is then mapped onto an 

<math display="inline" id="Language_model:18">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

-dimensional real vector called the <a href="word_embedding" title="wikilink">word embedding</a>, where 

<math display="inline" id="Language_model:19">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 is the size of the layer just before the output layer. The representations in skip-gram models have the distinct characteristic that they model semantic relations between words as <a href="linear_combination" title="wikilink">linear combinations</a>, capturing a form of <a class="uri" href="compositionality" title="wikilink">compositionality</a>. For example, in some such models, if 

<math display="inline" id="Language_model:20">
 <semantics>
  <mi>v</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>v</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   v
  </annotation>
 </semantics>
</math>

 is the function that maps a word 

<math display="inline" id="Language_model:21">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

 to its 

<math display="inline" id="Language_model:22">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

-d vector representation, then</p>

<p>

<math display="block" id="Language_model:23">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <mi>v</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>king</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>-</mo>
     <mrow>
      <mi>v</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>male</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>v</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>female</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>≈</mo>
   <mrow>
    <mi>v</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>queen</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <approx></approx>
    <apply>
     <plus></plus>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>v</ci>
       <ci>king</ci>
      </apply>
      <apply>
       <times></times>
       <ci>v</ci>
       <ci>male</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>v</ci>
      <ci>female</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>v</ci>
     <ci>queen</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   v(\mathrm{king})-v(\mathrm{male})+v(\mathrm{female})\approx v(\mathrm{queen})
  </annotation>
 </semantics>
</math>

</p>

<p>where ≈ is made precise by stipulating that its right-hand side must be the <a href="Nearest_neighbor_search" title="wikilink">nearest neighbor</a> of the value of the left-hand side.</p>
<h2 id="other-models">Other models</h2>

<p>A <a href="positional_language_model" title="wikilink">positional language model</a><a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> is one that describes the probability of given words occurring close to one another in a text, not necessarily immediately adjacent. Similarly, bag-of-concepts models<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> leverage on the semantics associated with multi-word expressions such as <em>buy_christmas_present</em>, even when they are used in information-rich sentences like "today I bought a lot of very nice Christmas presents".</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Factored_language_model" title="wikilink">Factored language model</a></li>
<li><a href="Cache_language_model" title="wikilink">Cache language model</a></li>
<li><a href="Katz's_back-off_model" title="wikilink">Katz's back-off model</a></li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<h3 id="further-reading">Further reading</h3>
<ul>
<li></li>
<li></li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.cs.columbia.edu/~mcollins/">Lecture notes on language models, parsing and machine translation with PCFG, CRF, MaxEnt, MEMM, EM, GLM, HMM by Michael Collins(Columbia University)</a></li>
<li><a href="http://www-lium.univ-lemans.fr/cslm">CSLM</a> – Free toolkit for <a href="feedforward_neural_network" title="wikilink">feedforward neural</a> language models</li>
<li><a href="https://github.com/jnory/DALM">DALM</a> – Fast, Free software for language model queries</li>
<li><a href="http://sourceforge.net/projects/irstlm">IRSTLM</a> – Free software for language modeling</li>
<li><a href="http://www.phontron.com/kylm">Kylm</a> (Kyoto Language Modeling Toolkit) – Free language modeling toolkit in Java</li>
<li><a href="http://kheafield.com/code/kenlm">KenLM</a> – Fast, Free software for language modeling</li>
<li><a href="https://lmsharp.codeplex.com/">LMSharp</a> – Free language model toolkit for <a href="Kneser–Ney_smoothing" title="wikilink">Kneser–Ney-smoothed</a> <em>n</em>-gram models and <a href="recurrent_neural_network" title="wikilink">recurrent neural network</a> models</li>
<li><a href="https://code.google.com/p/mitlm">MITLM</a> – MIT Language Modeling toolkit. Free software</li>
<li><a href="http://nlg.isi.edu/software/nplm">NPLM</a> – Free toolkit for <a href="feedforward_neural_network" title="wikilink">feedforward neural</a> language models</li>
<li><a href="http://openfst.cs.nyu.edu/twiki/bin/view/GRM/NGramLibrary">OpenGrm NGram</a> library – Free software for language modeling. Built on <a class="uri" href="OpenFst" title="wikilink">OpenFst</a>.</li>
<li><a href="https://github.com/pauldb89/OxLM">OxLM</a> – Free toolkit for <a href="feedforward_neural_network" title="wikilink">feedforward neural</a> language models</li>
<li><a href="http://sifaka.cs.uiuc.edu/~ylv2/pub/plm/plm.htm">Positional Language Model</a></li>
<li><a href="http://sourceforge.net/projects/randlm">RandLM</a> – Free software for <a href="randomised_language_modeling" title="wikilink">randomised language modeling</a></li>
<li><a href="http://rnnlm.org">RNNLM</a> – Free <a href="recurrent_neural_network" title="wikilink">recurrent neural network</a> language model toolkit</li>
<li><a href="http://www-speech.sri.com/projects/srilm">SRILM</a> – Proprietary software for language modeling</li>
<li><a href="http://vsiivola.github.io/variKN">VariKN</a> – Free software for creating, growing and pruning Kneser-Ney smoothed <em>n</em>-gram models.</li>
<li><a href="http://www.keithv.com/software/csr/">Language models trained on newswire data</a></li>
</ul>

<p>"</p>

<p><a href="Category:Language_modeling" title="wikilink">*</a> <a href="Category:Statistical_natural_language_processing" title="wikilink">Category:Statistical natural language processing</a> <a href="Category:Markov_models" title="wikilink">Category:Markov models</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: An Introduction to Information Retrieval, pages 237–240. Cambridge University Press, 2009<a href="#fnref1">↩</a></li>
<li id="fn2">Buttcher, Clarke, and Cormack. Information Retrieval: Implementing and Evaluating Search Engines. pg. 289–291. MIT Press.<a href="#fnref2">↩</a></li>
<li id="fn3">Craig Trim, <a href="http://trimc-nlp.blogspot.com/2013/04/language-modeling.html"><em>What is Language Modeling?</em></a>, April 26th, 2013.<a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8">Yuanhua Lv and ChengXiang Zhai, <a href="http://times.cs.uiuc.edu/czhai/pub/sigir09-PLM.pdf"><em>Positional Language Models for Information Retrieval</em></a>, in Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 2009.<a href="#fnref8">↩</a></li>
<li id="fn9">E. Cambria and A. Hussain. Sentic Computing: Techniques, Tools, and Applications. Dordrecht, Netherlands: Springer, ISBN 978-94-007-5069-2 (2012)<a href="#fnref9">↩</a></li>
</ol>
</section>
</body>
</html>
