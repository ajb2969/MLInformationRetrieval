<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1553">Self-information</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Self-information</h1>
<hr/>

<p>In <a href="information_theory" title="wikilink">information theory</a>, <strong>self-information</strong> is a measure of the information content associated with an <a href="event_(probability_theory)" title="wikilink">event</a> in a <a href="probability_space" title="wikilink">probability space</a> or with the value of a discrete <a href="random_variable" title="wikilink">random variable</a>. It is expressed in a <a href="Units_of_information" title="wikilink">unit of information</a>, for example <a href="bit" title="wikilink">bits</a>, <a href="Nat_(unit)" title="wikilink">nats</a>, or <a href="Hartley_(unit)" title="wikilink">hartleys</a>, depending on the base of the logarithm used in its calculation.</p>

<p>The term <strong>self-information</strong> is also sometimes used as a synonym of the related information-theoretic concept of <a href="Entropy_(information_theory)" title="wikilink">entropy</a>. These two meanings are not equivalent, and this article covers the first sense only.</p>
<h2 id="definition">Definition</h2>

<p>By definition, the amount of self-information contained in a probabilistic <a href="event_(probability_theory)" title="wikilink">event</a> depends only on the <a class="uri" href="probability" title="wikilink">probability</a> of that event: the smaller its probability, the larger the self-information associated with receiving the information that the event indeed occurred.</p>

<p>Further, by definition, the <a href="Measurement" title="wikilink">measure</a> of self-information is positive and additive. If an event <em>C</em> is the <strong>intersection</strong> of two <a href="statistical_independence" title="wikilink">independent</a> events <em>A</em> and <em>B</em>, then the amount of information at the proclamation that <em>C</em> has happened, equals the <strong>sum</strong> of the amounts of information at proclamations of event <em>A</em> and event <em>B</em> respectively: <em>I(A ∩ B)=I(A)+I(B)</em>.</p>

<p>Taking into account these properties, the self-information 

<math display="inline" id="Self-information:0">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>ω</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>I</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>ω</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I(\omega_{n})
  </annotation>
 </semantics>
</math>

 associated with outcome 

<math display="inline" id="Self-information:1">
 <semantics>
  <msub>
   <mi>ω</mi>
   <mi>n</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>ω</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \omega_{n}
  </annotation>
 </semantics>
</math>

 with probability 

<math display="inline" id="Self-information:2">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>ω</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>ω</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(\omega_{n})
  </annotation>
 </semantics>
</math>

 is:</p>

<p>

<math display="block" id="Self-information:3">
 <semantics>
  <mrow>
   <mrow>
    <mi>I</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>ω</mi>
      <mi>n</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>log</mi>
    <mrow>
     <mo>(</mo>
     <mfrac>
      <mn>1</mn>
      <mrow>
       <mi>P</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <msub>
         <mi>ω</mi>
         <mi>n</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mfrac>
     <mo>)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <mi>log</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>P</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <msub>
         <mi>ω</mi>
         <mi>n</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <ci>I</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ω</ci>
       <ci>n</ci>
      </apply>
     </apply>
     <apply>
      <log></log>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <apply>
        <times></times>
        <ci>P</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>ω</ci>
         <ci>n</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <minus></minus>
      <apply>
       <log></log>
       <apply>
        <times></times>
        <ci>P</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>ω</ci>
         <ci>n</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I(\omega_{n})=\log\left(\frac{1}{P(\omega_{n})}\right)=-\log(P(\omega_{n}))
  </annotation>
 </semantics>
</math>

</p>

<p>This definition complies with the above conditions. In the above definition, the base of the logarithm is not specified: if using base 2, the unit of 

<math display="inline" id="Self-information:4">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>ω</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>I</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>ω</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle I(\omega_{n})
  </annotation>
 </semantics>
</math>

 is <a href="bit" title="wikilink">bits</a>. When using the logarithm of base 

<math display="inline" id="Self-information:5">
 <semantics>
  <mi>e</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>e</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle e
  </annotation>
 </semantics>
</math>

, the unit will be the <a href="Nat_(unit)" title="wikilink">nat</a>. For the log of base 10, the unit will be <a href="Hartley_(unit)" title="wikilink">hartley</a>.</p>

<p>As a quick illustration, the information content associated with an outcome of 4 heads (or any specific outcome) in 4 consecutive tosses of a coin would be 4 bits (probability 1/16), and the information content associated with getting a result other than the one specified would be 0.09 bits (probability 15/16). See below for detailed examples.</p>

<p>This measure has also been called <strong>surprisal</strong>, as it represents the "<a href="surprise_(emotion)" title="wikilink">surprise</a>" of seeing the outcome (a highly improbable outcome is very surprising). This term was coined by <a href="Myron_Tribus" title="wikilink">Myron Tribus</a> in his 1961 book <em>Thermostatics and Thermodynamics</em>.</p>

<p>The <a href="information_entropy" title="wikilink">information entropy</a> of a random event is the <a href="expected_value" title="wikilink">expected value</a> of its self-information.</p>

<p><strong>Self-information</strong> is an example of a <a href="Scoring_rule" title="wikilink">proper scoring rule</a>.</p>
<h2 id="examples">Examples</h2>
<ul>
<li>On <a href="coin_flipping" title="wikilink">tossing a coin</a>, the chance of 'tail' is 0.5. When it is proclaimed that indeed 'tail' occurred, this amounts to</li>
</ul>
<dl>
<dd><em>I</em>('tail') = log<sub>2</sub> (1/0.5) = log<sub>2</sub> 2 = 1 bits of information.
</dd>
</dl>
<ul>
<li>When throwing a fair <a class="uri" href="dice" title="wikilink">dice</a>, the probability of 'four' is 1/6. When it is proclaimed that 'four' has been thrown, the amount of self-information is</li>
</ul>
<dl>
<dd><em>I</em>('four') = log<sub>2</sub> (1/(1/6)) = log<sub>2</sub> (6) = 2.585 bits.
</dd>
</dl>
<ul>
<li>When, independently, two dice are thrown, the amount of information associated with {throw 1 = 'two' &amp; throw 2 = 'four'} equals</li>
</ul>
<dl>
<dd><em>I</em>('throw 1 is two &amp; throw 2 is four') = log<sub>2</sub> (1/P(throw 1 = 'two' &amp; throw 2 = 'four')) = log<sub>2</sub> (1/(1/36)) = log<sub>2</sub> (36) = 5.170 bits.<br/>
 This outcome equals the sum of the individual amounts of self-information associated with {throw 1 = 'two'} and {throw 2 = 'four'}; namely 2.585 + 2.585 = 5.170 bits.
</dd>
</dl>
<ul>
<li>In the same two dice situation we can also consider the information present in the statement "The sum of the two dice is five"</li>
</ul>
<dl>
<dd><em>I</em>('The sum of throws 1 and 2 is five') = log<sub>2</sub> (1/P('throw 1 and 2 sum to five')) = log<sub>2</sub> (1/(4/36)) = 3.17 bits. The (4/36) is because there are four ways out of 36 possible to sum two dice to 5. This shows how more complex or ambiguous events can still carry information.
</dd>
</dl>
<h2 id="self-information-of-a-partitioning">Self-information of a partitioning</h2>

<p>The self-information of a <a href="Partition_of_a_set" title="wikilink">partitioning of elements within a set</a> (or clustering) is the <a href="Expected_value" title="wikilink">expectation</a> of the information of a test object; if we select an element at random and observe in which partition/cluster it exists, what quantity of information do we expect to obtain? The information of a partitioning 

<math display="inline" id="Self-information:6">
 <semantics>
  <mi>C</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>C</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   C
  </annotation>
 </semantics>
</math>

 with 

<math display="inline" id="Self-information:7">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>k</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>P</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(k)
  </annotation>
 </semantics>
</math>

 denoting the fraction of elements within partition 

<math display="inline" id="Self-information:8">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

 is <a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> 

<math display="inline" id="Self-information:9">
 <semantics>
  <mrow>
   <mrow>
    <mi>I</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>C</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>E</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mo>-</mo>
      <mrow>
       <mi>log</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mi>P</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>C</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mi>k</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </msubsup>
     <mrow>
      <mi>P</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>k</mi>
       <mo stretchy="false">)</mo>
      </mrow>
      <mrow>
       <mi>log</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mi>P</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>k</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <ci>I</ci>
      <ci>C</ci>
     </apply>
     <apply>
      <times></times>
      <ci>E</ci>
      <apply>
       <minus></minus>
       <apply>
        <log></log>
        <apply>
         <times></times>
         <ci>P</ci>
         <ci>C</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <minus></minus>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>k</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>n</ci>
       </apply>
       <apply>
        <times></times>
        <ci>P</ci>
        <ci>k</ci>
        <apply>
         <log></log>
         <apply>
          <times></times>
          <ci>P</ci>
          <ci>k</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I(C)=E(-\log(P(C)))=-\sum_{k=1}^{n}P(k)\log(P(k))
  </annotation>
 </semantics>
</math>

</p>
<h2 id="relationship-to-entropy">Relationship to entropy</h2>

<p>The <a href="Entropy_(information_theory)" title="wikilink">entropy</a> is the expected value of the self-information of the values of a discrete random variable. Sometimes, the entropy itself is called the "self-information" of the random variable, possibly because the entropy satisfies 

<math display="inline" id="Self-information:10">
 <semantics>
  <mrow>
   <mrow>
    <mi>H</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>X</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>I</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>X</mi>
     <mo>;</mo>
     <mi>X</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>H</ci>
     <ci>X</ci>
    </apply>
    <apply>
     <times></times>
     <ci>I</ci>
     <list>
      <ci>X</ci>
      <ci>X</ci>
     </list>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(X)=I(X;X)
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Self-information:11">
 <semantics>
  <mrow>
   <mi>I</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>X</mi>
    <mo>;</mo>
    <mi>X</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>I</ci>
    <list>
     <ci>X</ci>
     <ci>X</ci>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   I(X;X)
  </annotation>
 </semantics>
</math>

 is the <a href="mutual_information" title="wikilink">mutual information</a> of X with itself.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h2 id="references">References</h2>
<references>
</references>
<ul>
<li>C.E. Shannon, A Mathematical Theory of Communication, Bell Syst. Techn. J., Vol. 27, pp 379–423, (Part I), 1948.</li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.umsl.edu/~fraundor/egsurpri.html">Examples of surprisal measures</a></li>
<li><a href="http://www.lecb.ncifcrf.gov/~toms/glossary.html#surprisal">"Surprisal" entry in a glossary of molecular information theory</a></li>
<li><a href="http://ilab.usc.edu/surprise/">Bayesian Theory of Surprise</a></li>
</ul>

<p>"</p>

<p><a href="Category:Information_theory" title="wikilink">Category:Information theory</a> <a href="Category:Entropy_and_information" title="wikilink">Category:Entropy and information</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Marina Meilă; Comparing clusterings—an information based distance; Journal of Multivariate Analysis, Volume 98, Issue 5, May 2007<a href="#fnref1">↩</a></li>
<li id="fn2">Thomas M. Cover, Joy A. Thomas; Elements of Information Theory; p. 20; 1991.<a href="#fnref2">↩</a></li>
</ol>
</section>
</body>
</html>
