<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="1234">Projection pursuit regression</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Projection pursuit regression</h1>
<hr/>
<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, <strong>projection pursuit regression (PPR)</strong> is a <a href="statistical_model" title="wikilink">statistical model</a> developed by <a href="Jerome_H._Friedman" title="wikilink">Jerome H. Friedman</a> and <a href="Werner_Stuetzle" title="wikilink">Werner Stuetzle</a> which is an extension of <a href="additive_model" title="wikilink">additive models</a>. This model adapts the additive models in that it first projects the <a href="data_matrix_(multivariate_statistics)" title="wikilink">data matrix</a> of <a href="explanatory_variable" title="wikilink">explanatory variables</a> in the optimal direction before applying smoothing functions to these explanatory variables.</p>
<h2 id="model-overview">Model overview</h2>
<p>The model consists of <a href="linear_combination" title="wikilink">linear combinations</a> of non-linear transformations of linear combinations of explanatory variables. The basic model takes the form</p>
<p><span class="LaTeX">$$Y=\beta_0 + \sum_{j=1}^r f_j (\beta_j'x) + \varepsilon ,$$</span></p>
<p>where <em>x</em> is a column vector containing a particular row of the <a href="design_matrix" title="wikilink">design matrix</a> <em>X</em> which contains <em>p</em> explanatory variables (columns) and <em>n</em> observations (row). Here <em>Y</em> is a particular observation variable (identifying the row being considered) to be predicted, {<em>β<sub>j</sub></em>} is a collection of <em>r</em> vectors (each a unit vector of length <em>p</em>) which contain the unknown parameters. Finally <em>r</em> is the number of modelled smoothed non-parametric functions to be used as constructed explanatory variables. The value of <em>r</em> is found through <a href="cross-validation_(statistics)" title="wikilink">cross-validation</a> or a forward stage-wise strategy which stops when the model fit cannot be significantly improved. For large values of <em>r</em> and an appropriate set of functions <em>f<sub>j</sub></em>, the PPR model is considered a universal estimator as it can estimate any continuous function in <strong>R</strong><sup><em>p</em></sup>.</p>
<p>Thus this model takes the form of the basic additive model but with the additional <em>β<sub>j</sub></em> component; making it fit <span class="LaTeX">$\beta_j 'x$</span> rather than the actual inputs <em>x</em>. The vector <span class="LaTeX">$\beta_j 'X$</span> is the projection of <em>X</em> onto the unit vector <em>β<sub>j</sub></em>, where the directions <em>β<sub>j</sub></em> are chosen to optimize model fit. The functions <em>f<sub>j</sub></em> are unspecified by the model and estimated using some flexible smoothing method; preferably one with well defined second derivatives to simplify computation. This allows the PPR to be very general as it fits non-linear functions <em>f<sub>j</sub></em> of any class of linear combinations in <em>X</em>. Due to the flexibility and generality of this model, it is difficult to interpret the fitted model because each input variable has been entered into the model in a complex and multifaceted way. Thus the model is far more useful for prediction than creating a model to understand the data.</p>
<h2 id="model-estimation">Model estimation</h2>
<p>For a given set of data <span class="LaTeX">$(y_i ,x_i )$</span>, the goal is to minimize the error function</p>
<p><span class="LaTeX">$$S=\sum_{i=1}^n \left[ y_i - \sum_{j=1}^r f_j (\beta_j 'x_i) \right]^2 ,$$</span></p>
<p>over the functions <span class="LaTeX">$f_j$</span> and vectors <span class="LaTeX">$\beta_j$</span>. After estimating the smoothing functions <span class="LaTeX">$f_j$</span>, one generally uses the <a class="uri" href="Gauss–Newton" title="wikilink">Gauss–Newton</a> iterated convergence technique to solve for <span class="LaTeX">$\beta_j$</span>; provided that the functions <span class="LaTeX">$f_j$</span> are twice differentiable.</p>
<p>It has been shown that the convergence rate, the bias and the variance are affected by the estimation of <span class="LaTeX">$\beta_j$</span> and <span class="LaTeX">$f_j$</span>. It has also been shown that <span class="LaTeX">$\beta_j$</span> converges at an order of <span class="LaTeX">$n^\frac{1}{2}$</span>, while <span class="LaTeX">$\beta_j$</span> converges at a slightly worse order.</p>
<h2 id="advantages-of-ppr-estimation">Advantages of PPR estimation</h2>
<ul>
<li>It uses univariate regression functions instead of their multivariate form, thus effectively dealing with the <a href="curse_of_dimensionality" title="wikilink">curse of dimensionality</a></li>
<li>Univariate regression allows for simple and efficient estimation</li>
<li>Relative to <a href="generalized_additive_model" title="wikilink">generalized additive models</a>, PPR can estimate a much richer class of functions</li>
<li>Unlike local averaging methods (such as <a href="k-nearest_neighbors" title="wikilink">k-nearest neighbors</a>), PPR can ignore variables with low explanatory power.</li>
</ul>
<h2 id="disadvantages-of-ppr-estimation">Disadvantages of PPR estimation</h2>
<ul>
<li>PPR requires examining an M-dimensional parameter space in order to estimate <span class="LaTeX">$\beta_j$</span>.</li>
<li>One must select the smoothing parameter for <span class="LaTeX">$f_j$</span>.</li>
<li>The model is often difficult to interpret</li>
</ul>
<h2 id="extensions-of-ppr">Extensions of PPR</h2>
<ul>
<li>Alternate smoothers, such as the radial function, harmonic function and additive function, have been suggested and their performances vary depending on the data sets used.</li>
<li>Alternate optimization criteria have been used as well, such as standard absolute deviations and <a href="mean_absolute_deviation" title="wikilink">mean absolute deviations</a>.</li>
<li><a href="Ordinary_least_squares" title="wikilink">Ordinary least squares</a> can be used to simplify calculations as often the data does not have strong non-linearities.</li>
<li>Sliced Inverse Regression (SIR) has been used to choose the direction vectors for PPR.</li>
<li>Generalized PPR combines regular PPR with iteratively reweighted least squares (IRLS) and a <a href="link_function" title="wikilink">link function</a> to estimate binary data.</li>
</ul>
<h2 id="ppr-vs-neural-networks-nn">PPR vs neural networks (NN)</h2>
<p>Both projection pursuit regression and <a href="neural_networks" title="wikilink">neural networks</a> models project the input vector onto a one-dimensional hyperplane and then apply a nonlinear transformation of the input variables that are then added in a linear fashion. Thus both follow the same steps to overcome the curse of dimensionality. The main difference is that the functions <span class="LaTeX">$f_j$</span> being fitted in PPR can be different for each combination of input variables and are estimated one at a time and then updated with the weights, whereas is NN these are all specified upfront and estimated simultaneously.</p>
<p>Thus, PPR estimation is more straightforward than NN and the transformations of variables in PPR is data driven whereas in NN, these transformations are fixed.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Projection_pursuit" title="wikilink">Projection pursuit</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li>Friedman, J.H. and Stuetzle, W. (1981) Projection Pursuit Regression. Journal of the American Statistical Association, 76, 817–823.</li>
<li>Hand, D., <a href="Heikki_Mannila" title="wikilink">Mannila, H.</a> and Smyth, P, (2001) Principles of Data Mining. MIT Press. ISBN 0-262-08290-X</li>
<li>Hall, P. (1988) Estimating the direction in which a data set is the most interesting, Probab. Teory Related Fields, 80, 51–77.</li>
<li>Hastie, T. J., Tibshirani, R. J. and Friedman, J.H. (2009). <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning: Data Mining, Inference and Prediction</a>. Springer. ISBN 978-0-387-84857-0</li>
<li>Klinke, S. and Grassmann, J. (2000) ‘Projection Pursuit Regression’ in Smoothing and Regression: Approaches, Computation and Application. Ed. Schimek, M.G.. Wiley Interscience.</li>
</ul>
<p>"</p>
<p><a href="Category:Regression_analysis" title="wikilink">Category:Regression analysis</a></p>
</body>
</html>
