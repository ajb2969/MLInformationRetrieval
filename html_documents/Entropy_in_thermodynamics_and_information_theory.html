<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="192">Entropy in thermodynamics and information theory</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Entropy in thermodynamics and information theory</h1>
<hr/>
<p>There are close parallels between the mathematical expressions for the thermodynamic <a class="uri" href="entropy" title="wikilink">entropy</a>, usually denoted by <em>S</em>, of a physical system in the <a href="statistical_thermodynamics" title="wikilink">statistical thermodynamics</a> established by <a href="Ludwig_Boltzmann" title="wikilink">Ludwig Boltzmann</a> and <a href="J._Willard_Gibbs" title="wikilink">J. Willard Gibbs</a> in the 1870s, and the <a href="information_entropy" title="wikilink">information-theoretic entropy</a>, usually expressed as <em>H</em>, of <a href="Claude_Elwood_Shannon" title="wikilink">Claude Shannon</a> and <a href="Ralph_Hartley" title="wikilink">Ralph Hartley</a> developed in the 1940s. Shannon, although not initially aware of this similarity, commented on it upon publicizing information theory in <em><a href="A_Mathematical_Theory_of_Communication" title="wikilink">A Mathematical Theory of Communication</a></em>.</p>
<p>This article explores what links there are between the two concepts, and how far they can be regarded as connected.</p>
<h2 id="equivalence-of-form-of-the-defining-expressions">Equivalence of form of the defining expressions</h2>
<p> The defining expression for <a class="uri" href="entropy" title="wikilink">entropy</a> in the theory of <a href="statistical_mechanics" title="wikilink">statistical mechanics</a> established by <a href="Ludwig_Boltzmann" title="wikilink">Ludwig Boltzmann</a> and <a href="J._Willard_Gibbs" title="wikilink">J. Willard Gibbs</a> in the 1870s, is of the form:</p>
<p><span class="LaTeX">$$S = - k_B \sum_i p_i \ln p_i,\,$$</span> where <span class="LaTeX">$p_i$</span> is the probability of the <a href="Microstate_(statistical_mechanics)" title="wikilink">microstate</a> <em>i</em> taken from an equilibrium ensemble.</p>
<p>The defining expression for <a href="information_entropy" title="wikilink">entropy</a> in the theory of <a href="information_theory" title="wikilink">information</a> established by <a href="Claude_E._Shannon" title="wikilink">Claude E. Shannon</a> in 1948 is of the form:</p>
<p><span class="LaTeX">$$H = - \sum_i p_i \log_b p_i,\,$$</span> where <span class="LaTeX">$p_i$</span> is the probability of the message <span class="LaTeX">$m_i$</span> taken from the message space <em>M</em> and <em>b</em> is the <a href="base_(exponentiation)" title="wikilink">base</a> of the <a class="uri" href="logarithm" title="wikilink">logarithm</a> used. Common values of <em>b</em> are 2, <a href="e_(mathematical_constant)" title="wikilink">Euler's number <span class="LaTeX">$e$</span></a>, and 10, and the unit of entropy is <a class="uri" href="bit" title="wikilink">bit</a> for <em>b</em> = 2, <a href="Nat_(unit)" title="wikilink">nat</a> for <em>b</em> = <span class="LaTeX">$e$</span>, and <a href="dit_(information)" title="wikilink">dit</a> (or digit) for <em>b</em> = 10.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<p>Mathematically <em>H</em> may also be seen as an <a href="average_information" title="wikilink">average information</a>, taken over the message space, because when a certain message occurs with probability <em>p</em><sub><em>i</em></sub>, the information −log(<em>p</em><sub><em>i</em></sub>) will be obtained.</p>
<p>If all the microstates are equiprobable (a <a href="microcanonical_ensemble" title="wikilink">microcanonical ensemble</a>), the statistical thermodynamic entropy reduces to the form, as given by Boltzmann,</p>
<p><span class="LaTeX">$$S = k_B \ln W \,$$</span> where <em>W</em> is the number of microstates.</p>
<p>If all the messages are equiprobable, the information entropy reduces to the <a href="Hartley_entropy" title="wikilink">Hartley entropy</a></p>
<p><span class="LaTeX">$$H = \log_b |M|\,$$</span> where <span class="LaTeX">$|M|$</span> is the <a class="uri" href="cardinality" title="wikilink">cardinality</a> of the message space <em>M</em>.</p>
<p>The logarithm in the thermodynamic definition is the <a href="natural_logarithm" title="wikilink">natural logarithm</a>. It can be shown that the <a href="Entropy_(statistical_thermodynamics)" title="wikilink">Gibbs entropy</a> formula, with the natural logarithm, reproduces all of the properties of the macroscopic <a href="classical_thermodynamics" title="wikilink">classical thermodynamics</a> of <a class="uri" href="Clausius" title="wikilink">Clausius</a>. (See article: <a href="Entropy_(statistical_views)" title="wikilink">Entropy (statistical views)</a>).</p>
<p>The <a class="uri" href="logarithm" title="wikilink">logarithm</a> can also be taken to the natural base in the case of information entropy. This is equivalent to choosing to measure information in nats instead of the usual <a href="bit" title="wikilink">bits</a>. In practice, information entropy is almost always calculated using base 2 logarithms, but this distinction amounts to nothing other than a change in units. One nat is about 1.44 bits.</p>
<p>The presence of <a href="Boltzmann's_constant" title="wikilink">Boltzmann's constant</a> <em>k</em> in the thermodynamic definitions is a historical accident, reflecting the conventional units of temperature. It is there to make sure that the statistical definition of thermodynamic entropy matches the classical entropy of Clausius, thermodynamically conjugate to <a class="uri" href="temperature" title="wikilink">temperature</a>. For a simple compressible system that can only perform volume work, the <a href="first_law_of_thermodynamics" title="wikilink">first law of thermodynamics</a> becomes</p>
<p><span class="LaTeX">$$dE = -p dV + T dS \,$$</span> But one can equally well write this equation in terms of what physicists and chemists sometimes call the 'reduced' or dimensionless entropy, σ = <em>S</em>/<em>k</em>, so that</p>
<p><span class="LaTeX">$$dE = -p dV + k_B T d\sigma \,$$</span> Just as <em>S</em> is conjugate to <em>T</em>, so σ is conjugate to <em>k<sub>B</sub>T</em> (the energy that is characteristic of <em>T</em> on a molecular scale).</p>
<h2 id="theoretical-relationship">Theoretical relationship</h2>
<p>Despite the foregoing, there is a difference between the two quantities. The <a href="information_entropy" title="wikilink">information entropy</a> <em>H</em> can be calculated for <em>any</em> <a href="probability_distribution" title="wikilink">probability distribution</a> (if the "message" is taken to be that the event <em>i</em> which had probability <em>p<sub>i</sub></em> occurred, out of the space of the events possible), while the <a href="entropy" title="wikilink">thermodynamic entropy</a> <em>S</em> refers to thermodynamic probabilities <em>p<sub>i</sub></em> specifically. The difference is more theoretical than actual, however, because any probability distribution can be approximated arbitrarily closely by some thermodynamic system.</p>
<p>Moreover, a direct connection can be made between the two. If the probabilities in question are the thermodynamic probabilities <em>p<sub>i</sub></em>: the (reduced) <a href="Gibbs_entropy" title="wikilink">Gibbs entropy</a> σ can then be seen as simply the amount of Shannon information needed to define the detailed microscopic state of the system, given its macroscopic description. Or, in the words of <a href="G._N._Lewis" title="wikilink">G. N. Lewis</a> writing about chemical entropy in 1930, "Gain in entropy always means loss of information, and nothing more". To be more concrete, in the discrete case using base two logarithms, the reduced Gibbs entropy is equal to the minimum number of yes–no questions needed to be answered in order to fully specify the <a href="Microstate_(statistical_mechanics)" title="wikilink">microstate</a>, given that we know the macrostate.</p>
<p>Furthermore, the prescription to find the equilibrium distributions of statistical mechanics—such as the Boltzmann distribution—by maximising the Gibbs entropy subject to appropriate constraints (the <a href="Gibbs_algorithm" title="wikilink">Gibbs algorithm</a>) can be seen as something not unique to thermodynamics, but as a principle of general relevance in statistical inference, if it is desired to find a <a href="principle_of_maximum_entropy" title="wikilink">maximally uninformative probability distribution</a>, subject to certain constraints on its averages. (These perspectives are explored further in the article <a href="Maximum_entropy_thermodynamics" title="wikilink">Maximum entropy thermodynamics</a>.)</p>
<h2 id="information-is-physical">Information is physical</h2>
<h3 id="szilards-engine">Szilard's engine</h3>
<figure><b>(Figure)</b>
<figcaption>N-atom engine schematic</figcaption>
</figure>
<p>A physical <a href="thought_experiment" title="wikilink">thought experiment</a> demonstrating how just the possession of information might in principle have thermodynamic consequences was established in 1929 by <a href="Leó_Szilárd" title="wikilink">Leó Szilárd</a>, in a refinement of the famous <a href="Maxwell's_demon" title="wikilink">Maxwell's demon</a> scenario.</p>
<p>Consider Maxwell's set-up, but with only a single gas particle in a box. If the supernatural demon knows which half of the box the particle is in (equivalent to a single bit of information), it can close a shutter between the two halves of the box, close a piston unopposed into the empty half of the box, and then extract <span class="LaTeX">$k_B T \ln 2$</span> joules of useful work if the shutter is opened again. The particle can then be left to isothermally expand back to its original equilibrium occupied volume. In just the right circumstances therefore, the possession of a single bit of Shannon information (a single bit of <a class="uri" href="negentropy" title="wikilink">negentropy</a> in Brillouin's term) really does correspond to a reduction in the entropy of the physical system. The global entropy is not decreased, but information to energy conversion is possible.</p>
<p>Using a phase-contrast microscope equipped with a high speed camera connected to a computer, as <em>demon</em>, the principle has been actually demonstrated.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> In this experiment, information to energy conversion is performed on a <a href="Brownian_motion" title="wikilink">Brownian</a> particle by means of <em>feedback control</em>; that is, synchronizing the work given to the particle with the information obtained on its position. Computing energy balances for different feedback protocols, has confirmed that the <a href="Jarzynski_equality" title="wikilink">Jarzynski equality</a> requires a generalization that accounts for the amount of information involved in the feedback.</p>
<h3 id="landauers-principle">Landauer's principle</h3>
<p>In fact one can generalise: any information that has a physical representation must somehow be embedded in the statistical mechanical degrees of freedom of a physical system.</p>
<p>Thus, <a href="Rolf_Landauer" title="wikilink">Rolf Landauer</a> argued in 1961, if one were to imagine starting with those degrees of freedom in a thermalised state, there would be a real reduction in thermodynamic entropy if they were then re-set to a known state. This can only be achieved under information-preserving microscopically deterministic dynamics if the uncertainty is somehow dumped somewhere else – i.e. if the entropy of the environment (or the non information-bearing degrees of freedom) is increased by at least an equivalent amount, as required by the Second Law, by gaining an appropriate quantity of heat: specifically <em>kT</em> ln 2 of heat for every 1 bit of randomness erased.</p>
<p>On the other hand, Landauer argued, there is no thermodynamic objection to a logically reversible operation potentially being achieved in a physically reversible way in the system. It is only logically irreversible operations – for example, the erasing of a bit to a known state, or the merging of two computation paths – which must be accompanied by a corresponding entropy increase. When information is physical, all processing of its representations, i.e. generation, encoding, transmission, decoding and interpretation, are natural processes where entropy increases by consumption of free energy.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<p>Applied to the Maxwell's demon/Szilard engine scenario, this suggests that it might be possible to "read" the state of the particle into a computing apparatus with no entropy cost; but <em>only</em> if the apparatus has already been <small>SET</small> into a known state, rather than being in a thermalised state of uncertainty. To <small>SET</small> (or <small>RESET</small>) the apparatus into this state will cost all the entropy that can be saved by knowing the state of Szilard's particle.</p>
<h2 id="negentropy">Negentropy</h2>
<p>Shannon entropy has been related by physicist <a href="Léon_Brillouin" title="wikilink">Léon Brillouin</a> to a concept sometimes called <a class="uri" href="negentropy" title="wikilink">negentropy</a>. In 1953, Brillouin derived a general equation<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> stating that the changing of an information bit value requires at least kT ln(2) energy. This is the same energy as the work <a href="Leo_Szilard" title="wikilink">Leo Szilard</a>'s engine produces in the idealistic case. In his book,<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> he further explored this problem concluding that any cause of a bit value change (measurement, decision about a yes/no question, erasure, display, etc.) will require the same amount, kT ln(2), of energy. Consequently, acquiring information about a system’s microstates is associated with an entropy production, while erasure yields entropy production only when the bit value is changing. Setting up a bit of information in a sub-system originally in thermal equilibrium results in a local entropy reduction however there is no violation of the second law of thermodynamics, according to Brillouin, since a reduction in any local system’s thermodynamic entropy results in an increase in thermodynamic entropy elsewhere. In this way, Brillouin clarified the meaning of negentropy which was considered as controversial because its earlier understanding can yield Carnot efficiency higher than one.</p>
<p>In 2009, Mahulikar & Herwig redefined thermodynamic negentropy as the specific entropy deficit of the dynamically ordered sub-system relative to its surroundings.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> This definition enabled the formulation of the <em>Negentropy Principle</em>, which is mathematically shown to follow from the 2nd Law of Thermodynamics, during order existence.</p>
<h2 id="black-holes">Black holes</h2>
<p><a href="Stephen_Hawking" title="wikilink">Stephen Hawking</a> often speaks of the thermodynamic entropy of <a href="black_hole" title="wikilink">black holes</a> in terms of their information content.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> Do black holes destroy information? It appears that there are deep relations between the <a href="Black_hole_thermodynamics#Black_hole_entropy" title="wikilink">entropy of a black hole</a> and information loss<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> See <em><a href="Black_hole_thermodynamics" title="wikilink">Black hole thermodynamics</a></em> and <em><a href="Black_hole_information_paradox" title="wikilink">Black hole information paradox</a></em>.</p>
<h2 id="quantum-theory">Quantum theory</h2>
<p>Hirschman showed,<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> cf. <a href="Hirschman_uncertainty" title="wikilink">Hirschman uncertainty</a>, that <a href="Heisenberg's_uncertainty_principle" title="wikilink">Heisenberg's uncertainty principle</a> can be expressed as a particular lower bound on the sum of the classical distribution entropies of the <em>quantum observable</em> probability distributions of a quantum mechanical state, the square of the wave-function, in coordinate, and also momentum space, when expressed in <a href="Planck_units" title="wikilink">Planck units</a>. The resulting inequalities provide a tighter bound on the uncertainty relations of Heisenberg.</p>
<p>One could speak of the "<a href="joint_entropy" title="wikilink">joint entropy</a>" of the position and momentum distributions in this quantity by considering them independent, but since they are not jointly observable, they cannot be considered as a <a href="joint_distribution" title="wikilink">joint distribution</a>. Note that this entropy is not the accepted entropy of a quantum system, the <a href="Von_Neumann_entropy" title="wikilink">Von Neumann entropy</a>, −Tr <em>ρ</em> ln<em>ρ</em> = −⟨ln<em>ρ</em>⟩. In phase-space, the Von Neumann entropy can nevertheless be represented equivalently to Hilbert space, even though positions and momenta are quantum conjugate variables; and thus leads to a properly bounded entropy distinctly <em>different</em> (more detailed) than Hirschman's; this one accounts for the <em>full information content of a mixture of quantum states</em>.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<p>(Dissatisfaction with the Von Neumann entropy from quantum information points of view has been expressed by Stotland, Pomeransky, Bachmat and Cohen, who have introduced a yet different definition of entropy that reflects the inherent uncertainty of quantum mechanical states. This definition allows distinction between the minimum uncertainty entropy of pure states, and the excess statistical entropy of mixtures.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a>)</p>
<h2 id="the-fluctuation-theorem">The fluctuation theorem</h2>
<p>The <a href="fluctuation_theorem" title="wikilink">fluctuation theorem</a> provides a mathematical justification of the <a href="second_law_of_thermodynamics" title="wikilink">second law of thermodynamics</a> under these principles, and precisely defines the limitations of the applicability of that law for systems away from thermodynamic equilibrium.</p>
<h2 id="topics-of-recent-research">Topics of recent research</h2>
<h3 id="is-information-quantized">Is information quantized?</h3>
<p>In 1995, <a href="Tim_Palmer_(meteorologist)" title="wikilink">Tim Palmer</a> signalled two unwritten assumptions about Shannon's definition of information that may make it inapplicable as such to <a href="quantum_mechanics" title="wikilink">quantum mechanics</a>:</p>
<ul>
<li>The supposition that there is such a thing as an observable state (for instance the upper face of a dice or a coin) <em>before</em> the observation begins</li>
<li>The fact that knowing this state does not depend on the order in which observations are made (<a class="uri" href="commutativity" title="wikilink">commutativity</a>)</li>
</ul>
<p><a href="Anton_Zeilinger" title="wikilink">Anton Zeilinger</a>'s and <a href="Caslav_Brukner" title="wikilink">Caslav Brukner</a>'s article<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> synthesized and developed these remarks. The so-called <a href="Zeilinger's_principle" title="wikilink">Zeilinger's principle</a> suggests that the quantization observed in QM could be bound to <em>information</em> quantization (one cannot observe less than one bit, and what is not observed is by definition "random"). Nevertheless, these claims remain quite controversial. Detailed discussions of the applicability of the Shannon information in quantum mechanics and an argument that Zeilinger's principle cannot explain quantization have been published,<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a><a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> that show that Brukner and Zeilinger change, in the middle of the calculation in their article, the numerical values of the probabilities needed to compute the Shannon entropy, so that the calculation makes little sense.</p>
<h3 id="extracting-work-from-quantum-information-in-a-szilárd-engine">Extracting work from quantum information in a Szilárd engine</h3>
<p>In 2013, a description was published<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> of a two atom version of a Szilárd engine using <a href="Quantum_discord" title="wikilink">Quantum discord</a> to generate work from purely quantum information.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> Refinements in the lower temperature limit were suggested.<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a></p>
<h2 id="see-also">See also</h2>
<h2 id="references">References</h2>
<h2 id="additional-references">Additional references</h2>
<ul>
<li></li>
<li>
<p>. [Republication of 1962 original.]</p></li>
<li></li>
<li>
<p>(A highly technical collection of writings giving an overview of the concept of entropy as it appears in various disciplines.)</p></li>
<li>
<p>.</p></li>
<li>
<p>.</p></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li>
<p>(<a href="http://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-3-379.pdf">as PDF</a>)</p></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://plato.stanford.edu/entries/information-entropy/">Information Processing and Thermodynamic Entropy</a> Stanford Encyclopedia of Philosophy.</li>
<li><a href="http://en.wikibooks.org/wiki/An_Intuitive_Guide_to_the_Concept_of_Entropy_Arising_in_Various_Sectors_of_Science"><em>An Intuitive Guide to the Concept of Entropy Arising in Various Sectors of Science</em></a> — a wikibook on the interpretation of the concept of entropy.</li>
</ul>
<p>"</p>
<p><a href="Category:Thermodynamic_entropy" title="wikilink">Category:Thermodynamic entropy</a> <a href="Category:Entropy_and_information" title="wikilink">Category:Entropy and information</a> <a href="Category:Philosophy_of_thermal_and_statistical_physics" title="wikilink">Category:Philosophy of thermal and statistical physics</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Schneider, T.D, <a href="http://alum.mit.edu/www/toms/paper/primer/primer.pdf">Information theory primer with an appendix on logarithms</a>, National Cancer Institute, 14 April 2007.<a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4">Leon Brillouin (1953), "The negentropy principle of information", <em>J. Applied Physics</em> <strong>24</strong>, 1152-1163.<a href="#fnref4">↩</a></li>
<li id="fn5">Leon Brillouin, <em>Science and Information theory</em>, Dover, 1956<a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"> <a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"><a href="#fnref11">↩</a></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="http://arxiv.org/abs/quant-ph/0112178">Timpson, 2003</a><a href="#fnref13">↩</a></li>
<li id="fn14"><a href="http://arxiv.org/abs/quant-ph/0007116">Hall, 2000</a><a href="#fnref14">↩</a></li>
<li id="fn15"><a href="http://arxiv.org/abs/quant-ph/0302049">Mana, 2004</a><a href="#fnref15">↩</a></li>
<li id="fn16"><a href="#fnref16">↩</a></li>
<li id="fn17"><a href="#fnref17">↩</a></li>
<li id="fn18"><a href="#fnref18">↩</a></li>
</ol>
</section>
</body>
</html>
