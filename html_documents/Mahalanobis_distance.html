<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="709">Mahalanobis distance</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Mahalanobis distance</h1>
<hr/>

<p>The <strong>Mahalanobis distance</strong> is a measure of the distance between a point P and a <a href="Probability_distribution" title="wikilink">distribution</a> D, introduced by <a href="Prasanta_Chandra_Mahalanobis" title="wikilink">P. C. Mahalanobis</a> in 1936.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> It is a multi-dimensional generalization of the idea of measuring how many <a href="standard_deviations" title="wikilink">standard deviations</a> away P is from the <a class="uri" href="mean" title="wikilink">mean</a> of D. This distance is zero if P is at the mean of D, and grows as P moves away from the mean: along each <a href="principal_component" title="wikilink">principal component</a> axis, it measures the number of standard deviations from P to the mean of D. If each of these axes is rescaled to have unit variance, then Mahalanobis distance corresponds to standard <a href="Euclidean_distance" title="wikilink">Euclidean distance</a> in the transformed space. Mahalanobis distance is thus <a class="uri" href="unitless" title="wikilink">unitless</a> and <a href="Scale_invariance" title="wikilink">scale-invariant</a>, and takes into account the <a class="uri" href="correlations" title="wikilink">correlations</a> of the <a href="data_set" title="wikilink">data set</a>.</p>
<h2 id="definition-and-properties">Definition and properties</h2>

<p>The Mahalanobis distance of an observation 

<math display="inline" id="Mahalanobis_distance:0">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>x</mi>
      <mn>1</mn>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>x</mi>
      <mn>2</mn>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>x</mi>
      <mn>3</mn>
     </msub>
     <mo>,</mo>
     <mi mathvariant="normal">…</mi>
     <mo>,</mo>
     <msub>
      <mi>x</mi>
      <mi>N</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mi>T</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>x</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <vector>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <cn type="integer">3</cn>
      </apply>
      <ci>normal-…</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>N</ci>
      </apply>
     </vector>
     <ci>T</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x=(x_{1},x_{2},x_{3},\dots,x_{N})^{T}
  </annotation>
 </semantics>
</math>

 from a set of observations with mean 

<math display="inline" id="Mahalanobis_distance:1">
 <semantics>
  <mrow>
   <mi>μ</mi>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>μ</mi>
      <mn>1</mn>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>μ</mi>
      <mn>2</mn>
     </msub>
     <mo>,</mo>
     <msub>
      <mi>μ</mi>
      <mn>3</mn>
     </msub>
     <mo>,</mo>
     <mi mathvariant="normal">…</mi>
     <mo>,</mo>
     <msub>
      <mi>μ</mi>
      <mi>N</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
    <mi>T</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>μ</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <vector>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>μ</ci>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>μ</ci>
       <cn type="integer">2</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>μ</ci>
       <cn type="integer">3</cn>
      </apply>
      <ci>normal-…</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>μ</ci>
       <ci>N</ci>
      </apply>
     </vector>
     <ci>T</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu=(\mu_{1},\mu_{2},\mu_{3},\dots,\mu_{N})^{T}
  </annotation>
 </semantics>
</math>

 and <a href="covariance_matrix" title="wikilink">covariance matrix</a> <em>S</em> is defined as:</p>

<p>

<math display="block" id="Mahalanobis_distance:2">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mi>D</mi>
      <mi>M</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <msqrt>
     <mrow>
      <msup>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mi>x</mi>
         <mo>-</mo>
         <mi>μ</mi>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
       <mi>T</mi>
      </msup>
      <msup>
       <mi>S</mi>
       <mrow>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
      </msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>x</mi>
        <mo>-</mo>
        <mi>μ</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </msqrt>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>D</ci>
      <ci>M</ci>
     </apply>
     <ci>x</ci>
    </apply>
    <apply>
     <root></root>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <ci>x</ci>
        <ci>μ</ci>
       </apply>
       <ci>T</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>S</ci>
       <apply>
        <minus></minus>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <apply>
       <minus></minus>
       <ci>x</ci>
       <ci>μ</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D_{M}(x)=\sqrt{(x-\mu)^{T}S^{-1}(x-\mu)}.\,
  </annotation>
 </semantics>
</math>

<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>Mahalanobis distance (or "generalized squared interpoint distance" for its squared value<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a>) can also be defined as a dissimilarity measure between two <a href="random_vector" title="wikilink">random vectors</a> 

<math display="inline" id="Mahalanobis_distance:3">
 <semantics>
  <mover accent="true">
   <mi>x</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{x}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Mahalanobis_distance:4">
 <semantics>
  <mover accent="true">
   <mi>y</mi>
   <mo stretchy="false">→</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \vec{y}
  </annotation>
 </semantics>
</math>

 of the same <a href="probability_distribution" title="wikilink">distribution</a> with the <a href="covariance_matrix" title="wikilink">covariance matrix</a> <em>S</em>:</p>

<p>

<math display="block" id="Mahalanobis_distance:5">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>d</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mo>,</mo>
      <mover accent="true">
       <mi>y</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <msqrt>
     <mrow>
      <msup>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mover accent="true">
          <mi>x</mi>
          <mo stretchy="false">→</mo>
         </mover>
         <mo>-</mo>
         <mover accent="true">
          <mi>y</mi>
          <mo stretchy="false">→</mo>
         </mover>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
       <mi>T</mi>
      </msup>
      <msup>
       <mi>S</mi>
       <mrow>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
      </msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mover accent="true">
         <mi>x</mi>
         <mo stretchy="false">→</mo>
        </mover>
        <mo>-</mo>
        <mover accent="true">
         <mi>y</mi>
         <mo stretchy="false">→</mo>
        </mover>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </msqrt>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>d</ci>
     <interval closure="open">
      <apply>
       <ci>normal-→</ci>
       <ci>x</ci>
      </apply>
      <apply>
       <ci>normal-→</ci>
       <ci>y</ci>
      </apply>
     </interval>
    </apply>
    <apply>
     <root></root>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <apply>
         <ci>normal-→</ci>
         <ci>x</ci>
        </apply>
        <apply>
         <ci>normal-→</ci>
         <ci>y</ci>
        </apply>
       </apply>
       <ci>T</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>S</ci>
       <apply>
        <minus></minus>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <apply>
       <minus></minus>
       <apply>
        <ci>normal-→</ci>
        <ci>x</ci>
       </apply>
       <apply>
        <ci>normal-→</ci>
        <ci>y</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d(\vec{x},\vec{y})=\sqrt{(\vec{x}-\vec{y})^{T}S^{-1}(\vec{x}-\vec{y})}.\,
  </annotation>
 </semantics>
</math>

</p>

<p>If the covariance matrix is the identity matrix, the Mahalanobis distance reduces to the <a href="Euclidean_distance" title="wikilink">Euclidean distance</a>. If the covariance matrix is <a href="Diagonal_matrix" title="wikilink">diagonal</a>, then the resulting distance measure is called a <em>normalized Euclidean distance</em>:</p>

<p>

<math display="block" id="Mahalanobis_distance:6">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>d</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mover accent="true">
       <mi>x</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mo>,</mo>
      <mover accent="true">
       <mi>y</mi>
       <mo stretchy="false">→</mo>
      </mover>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <msqrt>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>N</mi>
      </munderover>
      <mfrac>
       <msup>
        <mrow>
         <mo stretchy="false">(</mo>
         <mrow>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo>-</mo>
          <msub>
           <mi>y</mi>
           <mi>i</mi>
          </msub>
         </mrow>
         <mo stretchy="false">)</mo>
        </mrow>
        <mn>2</mn>
       </msup>
       <msubsup>
        <mi>s</mi>
        <mi>i</mi>
        <mn>2</mn>
       </msubsup>
      </mfrac>
     </mrow>
    </msqrt>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>d</ci>
     <interval closure="open">
      <apply>
       <ci>normal-→</ci>
       <ci>x</ci>
      </apply>
      <apply>
       <ci>normal-→</ci>
       <ci>y</ci>
      </apply>
     </interval>
    </apply>
    <apply>
     <root></root>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>N</ci>
      </apply>
      <apply>
       <divide></divide>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <minus></minus>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>y</ci>
          <ci>i</ci>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>s</ci>
         <ci>i</ci>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d(\vec{x},\vec{y})=\sqrt{\sum_{i=1}^{N}{(x_{i}-y_{i})^{2}\over s_{i}^{2}}},
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>s<sub>i</sub></em> is the <a href="standard_deviation" title="wikilink">standard deviation</a> of the <em>x<sub>i</sub></em> and <em>y<sub>i</sub></em> over the sample set.</p>

<p>Mahalanobis distance is preserved under full-rank linear transformations of the space <a href="linear_span" title="wikilink">spanned</a> by the data. This means that if the data has a nontrivial nullspace, Mahalanobis distance can be computed after projecting the data (non-degenerately) down onto any space of the appropriate dimension for the data.</p>
<h2 id="intuitive-explanation">Intuitive explanation</h2>

<p>Consider the problem of estimating the probability that a test point in <em>N</em>-dimensional <a href="Euclidean_space" title="wikilink">Euclidean space</a> belongs to a set, where we are given sample points that definitely belong to that set. Our first step would be to find the average or center of mass of the sample points. Intuitively, the closer the point in question is to this center of mass, the more likely it is to belong to the set.</p>

<p>However, we also need to know if the set is spread out over a large range or a small range, so that we can decide whether a given distance from the center is noteworthy or not. The simplistic approach is to estimate the <a href="standard_deviation" title="wikilink">standard deviation</a> of the distances of the sample points from the center of mass. If the distance between the test point and the center of mass is less than one standard deviation, then we might conclude that it is highly probable that the test point belongs to the set. The further away it is, the more likely that the test point should not be classified as belonging to the set.</p>

<p>This intuitive approach can be made quantitative by defining the normalized distance between the test point and the set to be 

<math display="inline" id="Mahalanobis_distance:7">
 <semantics>
  <mfrac>
   <mrow>
    <mi>x</mi>
    <mo>-</mo>
    <mi>μ</mi>
   </mrow>
   <mi>σ</mi>
  </mfrac>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <minus></minus>
     <ci>x</ci>
     <ci>μ</ci>
    </apply>
    <ci>σ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   {x-\mu}\over\sigma
  </annotation>
 </semantics>
</math>

. By plugging this into the normal distribution we can derive the probability of the test point belonging to the set.</p>

<p>The drawback of the above approach was that we assumed that the sample points are distributed about the center of mass in a spherical manner. Were the distribution to be decidedly non-spherical, for instance ellipsoidal, then we would expect the probability of the test point belonging to the set to depend not only on the distance from the center of mass, but also on the direction. In those directions where the ellipsoid has a short axis the test point must be closer, while in those where the axis is long the test point can be further away from the center.</p>

<p>Putting this on a mathematical basis, the ellipsoid that best represents the set's probability distribution can be estimated by building the covariance matrix of the samples. The Mahalanobis distance is simply the distance of the test point from the center of mass divided by the width of the ellipsoid in the direction of the test point.</p>
<h2 id="normal-distributions">Normal distributions</h2>

<p>For a <a href="multivariate_normal_distribution" title="wikilink">normal distribution</a> in any number of dimensions, the probability of an observation is uniquely determined by the Mahalanobis distance d. Specifically, 

<math display="inline" id="Mahalanobis_distance:8">
 <semantics>
  <msup>
   <mi>d</mi>
   <mn>2</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <ci>d</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d^{2}
  </annotation>
 </semantics>
</math>

 is <a href="Chi-squared_distribution" title="wikilink">chi-squared distributed</a>. If the number of dimensions is 2, for example, the probability of a particular calculated d being inside of dth is 

<math display="inline" id="Mahalanobis_distance:9">
 <semantics>
  <mrow>
   <mn>1</mn>
   <mo>-</mo>
   <msup>
    <mi>e</mi>
    <mrow>
     <mo>-</mo>
     <mrow>
      <mrow>
       <mi>d</mi>
       <mi>t</mi>
       <msup>
        <mi>h</mi>
        <mn>2</mn>
       </msup>
      </mrow>
      <mo>/</mo>
      <mn>2</mn>
     </mrow>
    </mrow>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <cn type="integer">1</cn>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>e</ci>
     <apply>
      <minus></minus>
      <apply>
       <divide></divide>
       <apply>
        <times></times>
        <ci>d</ci>
        <ci>t</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>h</ci>
         <cn type="integer">2</cn>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   1-e^{-dth^{2}/2}
  </annotation>
 </semantics>
</math>

. To determine a threshold to achieve a particular probability, p, use 

<math display="inline" id="Mahalanobis_distance:10">
 <semantics>
  <mrow>
   <mrow>
    <mi>d</mi>
    <mi>t</mi>
    <msup>
     <mi>h</mi>
     <mn>2</mn>
    </msup>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <mn>2</mn>
     <mrow>
      <mi>ln</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mn>1</mn>
        <mo>-</mo>
        <mi>p</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>d</ci>
     <ci>t</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>h</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <cn type="integer">2</cn>
      <apply>
       <ln></ln>
       <apply>
        <minus></minus>
        <cn type="integer">1</cn>
        <ci>p</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   dth^{2}=-2\ln(1-p)
  </annotation>
 </semantics>
</math>

, for 2 dimensions. For number of dimensions other than 2, the cumulative chi-squared distribution should be consulted.</p>

<p>In a normal distribution, the region where the Mahalanobis distance is less than one (i.e. the region inside the ellipsoid at distance one) is exactly the region where the probability distribution is <a href="concave_function" title="wikilink">concave</a>.</p>

<p>Mahalanobis distance is proportional, for a normal distribution, to the square root of the negative log likelihood (after adding a constant so the minimum is at zero).</p>
<h2 id="relationship-to-normal-random-variables">Relationship to normal random variables</h2>

<p>In general, given a normal (<a class="uri" href="Gaussian" title="wikilink">Gaussian</a>) random variable 

<math display="inline" id="Mahalanobis_distance:11">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 with variance 

<math display="inline" id="Mahalanobis_distance:12">
 <semantics>
  <mrow>
   <mi>S</mi>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>S</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S=1
  </annotation>
 </semantics>
</math>

 and mean 

<math display="inline" id="Mahalanobis_distance:13">
 <semantics>
  <mrow>
   <mi>μ</mi>
   <mo>=</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>μ</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu=0
  </annotation>
 </semantics>
</math>

, any other normal random variable 

<math display="inline" id="Mahalanobis_distance:14">
 <semantics>
  <mi>R</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>R</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R
  </annotation>
 </semantics>
</math>

 (with mean 

<math display="inline" id="Mahalanobis_distance:15">
 <semantics>
  <msub>
   <mi>μ</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>μ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mu_{1}
  </annotation>
 </semantics>
</math>

 and variance 

<math display="inline" id="Mahalanobis_distance:16">
 <semantics>
  <msub>
   <mi>S</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>S</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S_{1}
  </annotation>
 </semantics>
</math>

) can be defined in terms of 

<math display="inline" id="Mahalanobis_distance:17">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

 by the equation 

<math display="inline" id="Mahalanobis_distance:18">
 <semantics>
  <mrow>
   <mrow>
    <mi>R</mi>
    <mo>=</mo>
    <mrow>
     <msub>
      <mi>μ</mi>
      <mn>1</mn>
     </msub>
     <mo>+</mo>
     <mrow>
      <msqrt>
       <msub>
        <mi>S</mi>
        <mn>1</mn>
       </msub>
      </msqrt>
      <mi>X</mi>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>R</ci>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>μ</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <times></times>
      <apply>
       <root></root>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>S</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>X</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R=\mu_{1}+\sqrt{S_{1}}X.
  </annotation>
 </semantics>
</math>

 Conversely, to recover a normalized random variable from any normal random variable, one can typically solve for 

<math display="inline" id="Mahalanobis_distance:19">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>R</mi>
      <mo>-</mo>
      <msub>
       <mi>μ</mi>
       <mn>1</mn>
      </msub>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mo>/</mo>
    <msqrt>
     <msub>
      <mi>S</mi>
      <mn>1</mn>
     </msub>
    </msqrt>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>X</ci>
    <apply>
     <divide></divide>
     <apply>
      <minus></minus>
      <ci>R</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>μ</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <root></root>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>S</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X=(R-\mu_{1})/\sqrt{S_{1}}
  </annotation>
 </semantics>
</math>

. If we square both sides, and take the square-root, we will get an equation for a metric that looks a lot like the Mahalanobis distance:</p>

<p>

<math display="block" id="Mahalanobis_distance:20">
 <semantics>
  <mrow>
   <mrow>
    <mi>D</mi>
    <mo>=</mo>
    <msqrt>
     <msup>
      <mi>X</mi>
      <mn>2</mn>
     </msup>
    </msqrt>
    <mo>=</mo>
    <msqrt>
     <mrow>
      <msup>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <mi>R</mi>
         <mo>-</mo>
         <msub>
          <mi>μ</mi>
          <mn>1</mn>
         </msub>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
       <mn>2</mn>
      </msup>
      <mo>/</mo>
      <msub>
       <mi>S</mi>
       <mn>1</mn>
      </msub>
     </mrow>
    </msqrt>
    <mo>=</mo>
    <msqrt>
     <mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>R</mi>
        <mo>-</mo>
        <msub>
         <mi>μ</mi>
         <mn>1</mn>
        </msub>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <msubsup>
       <mi>S</mi>
       <mn>1</mn>
       <mrow>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
      </msubsup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>R</mi>
        <mo>-</mo>
        <msub>
         <mi>μ</mi>
         <mn>1</mn>
        </msub>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </msqrt>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>D</ci>
     <apply>
      <root></root>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>X</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <root></root>
      <apply>
       <divide></divide>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <minus></minus>
         <ci>R</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>μ</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <cn type="integer">2</cn>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>S</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <root></root>
      <apply>
       <times></times>
       <apply>
        <minus></minus>
        <ci>R</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>μ</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>S</ci>
         <cn type="integer">1</cn>
        </apply>
        <apply>
         <minus></minus>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <apply>
        <minus></minus>
        <ci>R</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>μ</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D=\sqrt{X^{2}}=\sqrt{(R-\mu_{1})^{2}/S_{1}}=\sqrt{(R-\mu_{1})S_{1}^{-1}(R-\mu_%
{1})}.
  </annotation>
 </semantics>
</math>

</p>

<p>The resulting magnitude is always non-negative and varies with the distance of the data from the mean, attributes that are convenient when trying to define a model for the data.</p>
<h2 id="relationship-to-leverage">Relationship to leverage</h2>

<p>Mahalanobis distance is closely related to the <a href="Partial_leverage" title="wikilink">leverage statistic</a>, 

<math display="inline" id="Mahalanobis_distance:21">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>

, but has a different scale:<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>

<math display="block" id="Mahalanobis_distance:22">
 <semantics>
  <mrow>
   <mrow>
    <msup>
     <mi>D</mi>
     <mn>2</mn>
    </msup>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>N</mi>
       <mo>-</mo>
       <mn>1</mn>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>h</mi>
       <mo>-</mo>
       <mrow>
        <mn>1</mn>
        <mo>/</mo>
        <mi>N</mi>
       </mrow>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>D</ci>
     <cn type="integer">2</cn>
    </apply>
    <apply>
     <times></times>
     <apply>
      <minus></minus>
      <ci>N</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <minus></minus>
      <ci>h</ci>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>N</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D^{2}=(N-1)(h-1/N).
  </annotation>
 </semantics>
</math>

</p>
<h2 id="applications">Applications</h2>

<p>Mahalanobis's definition was prompted by the problem of identifying the similarities of skulls based on measurements in 1927.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>

<p>Mahalanobis distance is widely used in <a href="Data_clustering" title="wikilink">cluster analysis</a> and <a href="Statistical_classification" title="wikilink">classification</a> techniques. It is closely related to <a href="Hotelling's_T-square_distribution" title="wikilink">Hotelling's T-square distribution</a> used for multivariate statistical testing and Fisher's <a href="Linear_Discriminant_Analysis" title="wikilink">Linear Discriminant Analysis</a> that is used for <a href="supervised_classification" title="wikilink">supervised classification</a>.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>

<p>In order to use the Mahalanobis distance to classify a test point as belonging to one of N classes, one first <a href="Estimation_of_covariance_matrices" title="wikilink">estimates the covariance matrix</a> of each class, usually based on samples known to belong to each class. Then, given a test sample, one computes the Mahalanobis distance to each class, and classifies the test point as belonging to that class for which the Mahalanobis distance is minimal.</p>

<p>Mahalanobis distance and leverage are often used to detect <a href="outlier" title="wikilink">outliers</a>, especially in the development of <a href="linear_regression" title="wikilink">linear regression</a> models. A point that has a greater Mahalanobis distance from the rest of the sample population of points is said to have higher leverage since it has a greater influence on the slope or coefficients of the regression equation. Mahalanobis distance is also used to determine multivariate outliers. Regression techniques can be used to determine if a specific case within a sample population is an outlier via the combination of two or more variable scores. Even for normal distributions, a point can be a multivariate outlier even if it is not a univariate outlier for any variable (consider a probability density concentrated along the line 

<math display="inline" id="Mahalanobis_distance:23">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mn>1</mn>
   </msub>
   <mo>=</mo>
   <msub>
    <mi>x</mi>
    <mn>2</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{1}=x_{2}
  </annotation>
 </semantics>
</math>

, for example), making Mahalanobis distance a more sensitive measure than checking dimensions individually.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Bregman_divergence" title="wikilink">Bregman divergence</a> (the Mahalanobis distance is an example of a Bregman divergence)</li>
<li><a href="Bhattacharyya_distance" title="wikilink">Bhattacharyya distance</a> related, for measuring similarity between data sets (and not between a point and a data set)</li>
<li><a href="Hamming_distance" title="wikilink">Hamming distance</a> identifies the difference bit by bit of two strings</li>
<li><a href="Hellinger_distance" title="wikilink">Hellinger distance</a>, also a measure of distance between data sets</li>
<li><a href="Similarity_learning" title="wikilink">Similarity learning</a>, for other approaches to learn a distance metric from examples.</li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li></li>
<li><a href="http://people.revoledu.com/kardi/tutorial/Similarity/MahalanobisDistance.html">Mahalanobis distance tutorial</a> – interactive online program and spreadsheet computation</li>
<li><a href="http://matlabdatamining.blogspot.com/2006/11/mahalanobis-distance.html">Mahalanobis distance (Nov-17-2006)</a> – overview of Mahalanobis distance, including MATLAB code</li>
<li><a href="http://blogs.sas.com/content/iml/2012/02/15/what-is-mahalanobis-distance/">What is Mahalanobis distance?</a> – intuitive, illustrated explanation, from Rick Wicklin on blogs.sas.com</li>
</ul>

<p>"</p>

<p><a href="Category:Statistical_distance_measures" title="wikilink">Category:Statistical distance measures</a> <a href="Category:Statistical_terminology" title="wikilink">Category:Statistical terminology</a> <a href="Category:Multivariate_statistics" title="wikilink">Category:Multivariate statistics</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2">De Maesschalck, Roy; Jouan-Rimbaud, Delphine; and Massart, Désiré L. (2000); <em>The Mahalanobis distance</em>, Chemometrics and Intelligent Laboratory Systems 50:1–18<a href="#fnref2">↩</a></li>
<li id="fn3">Gnanadesikan, Ramanathan; and Kettenring, John R. (1972); <em>Robust estimates, residuals, and outlier detection with multiresponse data</em>, Biometrics 28:81–124<a href="#fnref3">↩</a></li>
<li id="fn4">Schinka, John A.; Velicer, Wayne F.; and Weiner, Irving B. (2003); <em>Handbook of psychology: Research methods in psychology</em>, John Wiley and Sons<a href="#fnref4">↩</a></li>
<li id="fn5">Mahalanobis, Prasanta Chandra (1927); <em>Analysis of race mixture in Bengal</em>, Journal and Proceedings of the Asiatic Society of Bengal, 23:301–333<a href="#fnref5">↩</a></li>
<li id="fn6">McLachlan, Geoffrey J. (1992); <em>Discriminant Analysis and Statistical Pattern Recognition</em>, Wiley Interscience, p. 12. ISBN 0-471-69115-1<a href="#fnref6">↩</a></li>
</ol>
</section>
</body>
</html>
