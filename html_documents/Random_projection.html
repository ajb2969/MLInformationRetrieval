<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="1269">Random projection</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Random projection</h1>
<hr/>
<p>In theoretical computer science, <strong>random projection</strong> is a graphical technique used to <a href="dimensionality_reduction" title="wikilink">reduce the dimensionality</a> of a set of points which lie in <a href="Euclidean_space" title="wikilink">Euclidean space</a>. Random projection methods are powerful methods known for their simplicity and less erroneous output compared with other methods. According to experimental results, random projection preserve distances well, but empirical results are sparse.</p>
<h2 id="dimensionality-reduction">Dimensionality reduction</h2>
<p>Dimensionality reduction, as the name suggests, is reducing number of random variables using various machine learning methods and techniques. Dimensionality reduction is used mainly to reduce of the problem of managing and manipulation of large data sets. When we have large data sets it is too difficult for us to perform various operations like pattern recognition. Dimensionality reduction techniques generally uses linear transformations in determining the intrinsic dimensionality of the manifold as well as extracting its principal directions. There are various techniques like <a href="Principal_Component_Analysis" title="wikilink">Principal Component Analysis</a>, <a href="linear_discriminant_analysis" title="wikilink">linear discriminant analysis</a>, <a href="canonical_correlation_analysis" title="wikilink">canonical correlation analysis</a>, Discrete Cosine transform Method, Gauss Method, random Projection etc.</p>
<p>The random projection module implements a simple and computationally efficient way to reduce the dimensionality of data by trading a controlled amount of accuracy for faster processing times and smaller model sizes. This module construct two types of random matrix: Gaussian random matrix and sparse random matrix.</p>
<p>The dimensions and distribution of random projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset. It is a general data set reduction method in which a higher dimensional data a projected on lower dimensional subspace so that we can get a lower size subspace maintaining the relative distances of the elements. The main idea behind random projection is <a href="Johnson-Lindenstrauss_lemma" title="wikilink">Johnson-Lindenstrauss lemma</a><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> in which it is stated that if points in a vector space are in sufficient high dimension and we project them on a suitably high dimensional space then the distances between points remains almost preserved. In random projection, the original d-dimensional data is projected to a k-dimensional (k X_{kXN}^{RP}=R_{kXd}X_{dXN} is the projection of the data onto a lower k-dimensional subspace.Random projection is computationally very simple: forming the random matrix "R" and projecting the <span class="LaTeX">$dXN$</span> data matrix X into K dimensions of order <span class="LaTeX">$O(dkN)$</span>, and if the data matrix X is sparse with about c nonzero entries per column the complexity is of order <span class="LaTeX">$O(ckN)$</span>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<p>The random matrix R can be generated using Gaussian distribution like this:<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> The first row is a random unit vector uniformly chosen from <span class="LaTeX">$S^{N-1}$</span>. The second row is a random unit vector from the space orthogonal to the first row, the third row is a random unit vector from the space orthogonal to the first two rows, and so on. In this way of chooing R the following properties can be satisfied:</p>
<ul>
<li>Spherical symmetry: For any orthogonal matrix <span class="LaTeX">$A \in O(N)$</span>, RA and R have the same distribution.</li>
<li>Orthogonality: The rows of R are orthogonal to each other.</li>
<li>Normality: The rows of R are unit-length vectors.</li>
</ul>
<p>Achlioptas<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> shows that the Gaussian distribution can be replaced by a much simpler distribution such as</p>
<p><span class="LaTeX">$$R_{i,j} = \sqrt{3} \begin{cases}
+1 & with\ probability \frac{1}{6}\\
0 & with\ probability \frac{2}{3}\\
-1 & with\ probability \frac{1}{6} \end{cases}$$</span> This is efficient for database applications because the computations can be performed using integer arithmetic.</p>
<h2 id="references">References</h2>
<ul>
<li>Fodor,I. (2002) <a href="http://citeseerx.ist.psu.edu/viewdoc/versions?doi=10.1.1.8.5098">"A survey of dimension reduction techniques"</a>. Center for Applied Scientific Computing, Lawrence Livermore National, Technical Report UCRL-ID-148494</li>
<li>ADITYA KRISHNA MENON (2007) <a href="http://cseweb.ucsd.edu/~akmenon/HonoursThesis.pdf">"Random projections and applications to dimensionality reduction"</a>. School of Information Technologies, The University of Sydney, Australia</li>
<li>ADITYA Ramdas <a href="http://www.cs.cmu.edu/~aramdas/reports/MLTreport.pdf">"A Random Introduction To Random Projections"</a>. Carnegie Mellon University</li>
</ul>
<p>"</p>
<p><a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a> <a href="Category:Dimension_reduction" title="wikilink">Category:Dimension reduction</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">.<a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
</ol>
</section>
</body>
</html>
