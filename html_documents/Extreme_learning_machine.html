<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1930">Extreme learning machine</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Extreme learning machine</h1>
<hr/>

<p><strong>Extreme learning machines</strong> are <a href="feedforward_neural_network" title="wikilink">feedforward neural network</a> for <a href="statistical_classification" title="wikilink">classification</a> or <a href="regression_analysis" title="wikilink">regression</a> with a single layer of hidden nodes, where the weights connecting inputs to hidden nodes are randomly assigned and never updated. These weights between hidden nodes and outputs are learned in a single step, which essentially amounts to learning a linear model. The name "extreme learning machine" (ELM) was given to such models by Guang-Bin Huang.</p>

<p>These models can produce good generalization performance and learn thousands of times faster than networks trained using <a class="uri" href="backpropagation" title="wikilink">backpropagation</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="algorithm">Algorithm</h2>

<p>The simplest ELM training algorithm learns a model of the form</p>

<p>

<math display="block" id="Extreme_learning_machine:0">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>𝐘</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>𝐖</mi>
     <mn>2</mn>
    </msub>
    <mi>σ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>𝐖</mi>
       <mn>1</mn>
      </msub>
      <mi>x</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>normal-^</ci>
     <ci>𝐘</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>𝐖</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>σ</ci>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>𝐖</ci>
       <cn type="integer">1</cn>
      </apply>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{\hat{Y}}=\mathbf{W}_{2}\sigma(\mathbf{W}_{1}x)
  </annotation>
 </semantics>
</math>

</p>

<p>where <mtpl></mtpl> is the matrix of input-to-hidden-layer weights, 

<math display="inline" id="Extreme_learning_machine:1">
 <semantics>
  <mi>σ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>σ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   σ
  </annotation>
 </semantics>
</math>

 is some <a href="activation_function" title="wikilink">activation function</a>, and <mtpl></mtpl> is the matrix of hidden-to-output-layer weights. The algorithm proceeds as follows:</p>
<ol>
<li>Fill <mtpl></mtpl> with Gaussian random noise;</li>
<li>estimate <mtpl></mtpl> by <a href="least-squares_fit" title="wikilink">least-squares fit</a> to a matrix of response variables 

<math display="inline" id="Extreme_learning_machine:2">
 <semantics>
  <mi>𝐘</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐘</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{Y}
  </annotation>
 </semantics>
</math>

, computed using the <a href="Moore–Penrose_pseudoinverse" title="wikilink">pseudoinverse</a> <mtpl></mtpl>, given a <a href="design_matrix" title="wikilink">design matrix</a> 

<math display="inline" id="Extreme_learning_machine:3">
 <semantics>
  <mi>𝐗</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝐗</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{X}
  </annotation>
 </semantics>
</math>

:

<p>

<math display="block" id="Extreme_learning_machine:4">
 <semantics>
  <mrow>
   <msub>
    <mi>𝐖</mi>
    <mn>2</mn>
   </msub>
   <mo>=</mo>
   <mrow>
    <mi>σ</mi>
    <msup>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <msub>
        <mi>𝐖</mi>
        <mn>1</mn>
       </msub>
       <mi>𝐗</mi>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo>+</mo>
    </msup>
    <mi>𝐘</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>𝐖</ci>
     <cn type="integer">2</cn>
    </apply>
    <apply>
     <times></times>
     <ci>σ</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>𝐖</ci>
        <cn type="integer">1</cn>
       </apply>
       <ci>𝐗</ci>
      </apply>
      <plus></plus>
     </apply>
     <ci>𝐘</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{W}_{2}=\sigma(\mathbf{W}_{1}\mathbf{X})^{+}\mathbf{Y}
  </annotation>
 </semantics>
</math>

</p></li>
</ol>
<h2 id="controversy">Controversy</h2>

<p>The purported invention of the ELM, in 2006, provoked some debate. In particular, it was pointed out in <em>IEEE Transactions on Neural Networks</em> that the idea of using a hidden layer connected to the inputs by random untrained weights was already suggested in the original papers on <a href="RBF_network" title="wikilink">RBF networks</a> in the late 1980s, and experiments with <a href="multi-layer_perceptrons" title="wikilink">multi-layer perceptrons</a> with similar randomness had appeared in about the same timeframe; Guang-Bin Huang replied by pointing out subtle differences.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> In a 2015 paper, Huang responded to complaints about his invention of the name ELM for already-existing methods, complaining of "very negative and unhelpful comments on ELM in neither academic nor professional manner due to various reasons and intentions" and an "irresponsible anonymous attack which intends to destroy harmony research environment", arguing that his work "provides a unifying learning platform" for various types of neural nets.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Liquid_state_machine" title="wikilink">Liquid state machine</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Neural_networks" title="wikilink">Category:Neural networks</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
</ol>
</section>
</body>
</html>
