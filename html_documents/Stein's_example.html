<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1678">Stein's example</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Stein's example</h1>
<hr/>

<p><strong><a href="Charles_Stein_(statistician)" title="wikilink">Stein</a>'s example</strong> (or <strong>phenomenon</strong> or <strong>paradox</strong>), in <a href="decision_theory" title="wikilink">decision theory</a> and <a href="estimation_theory" title="wikilink">estimation theory</a>, is the phenomenon that when three or more parameters are estimated simultaneously, there exist combined <a href="estimator" title="wikilink">estimators</a> more accurate on average (that is, having lower expected <a href="mean_squared_error" title="wikilink">mean squared error</a>) than any method that handles the parameters separately. It is named after <a href="Charles_Stein_(statistician)" title="wikilink">Charles Stein</a> of <a href="Stanford_University" title="wikilink">Stanford University</a>, who discovered the phenomenon in 1955.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>

<p><a href="#An_intuitive_explanation" title="wikilink">An intuitive explanation</a> is that optimizing for the mean-squared error of a <em>combined</em> estimator is not the same as optimizing for the errors of separate estimators of the individual parameters. In practical terms, if the combined error is in fact of interest, then a combined estimator should be used, even if the underlying parameters are independent; this occurs in <a href="channel_estimation" title="wikilink">channel estimation</a> in telecommunications, for instance (different factors affect overall channel performance). On the other hand, if one is instead interested in estimating an individual parameter, then using a combined estimator does not help and is in fact worse.</p>
<h2 id="formal-statement">Formal statement</h2>

<p>The following is perhaps the simplest form of the paradox, the special case in which the number of observations is equal to (rather than greater than) the number of parameters to be estimated. Let <strong><em>θ</em></strong> be a vector consisting of <em>n</em> ≥ 3 unknown parameters. To estimate these parameters, a single measurement <em>X</em><sub><em>i</em></sub> is performed for each parameter <em>θ</em><sub><em>i</em></sub>, resulting in a vector <strong>X</strong> of length <em>n</em>. Suppose the measurements are <a href="statistical_independence" title="wikilink">independent</a>, <a href="Normal_distribution" title="wikilink">Gaussian</a> <a href="random_variable" title="wikilink">random variables</a>, with mean <strong><em>θ</em></strong> and variance 1, i.e.,</p>

<p>

<math display="block" id="Stein's_example:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>𝐗</mi>
    <mo>∼</mo>
    <mrow>
     <mi>N</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>𝜽</mi>
      <mo>,</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <ci>𝐗</ci>
    <apply>
     <times></times>
     <ci>N</ci>
     <interval closure="open">
      <ci>𝜽</ci>
      <cn type="integer">1</cn>
     </interval>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   {\mathbf{X}}\sim N({\boldsymbol{\theta}},1).\,
  </annotation>
 </semantics>
</math>

</p>

<p>Thus, each parameter is estimated using a single noisy measurement, and each measurement is equally inaccurate.</p>

<p>Under such conditions, it is most intuitive (and most common) to use each measurement as an estimate of its corresponding parameter. This so-called "ordinary" decision rule can be written as</p>

<p>

<math display="block" id="Stein's_example:1">
 <semantics>
  <mrow>
   <mrow>
    <mover accent="true">
     <mi>𝜽</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mo>=</mo>
    <mi>𝐗</mi>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>normal-^</ci>
     <ci>𝜽</ci>
    </apply>
    <ci>𝐗</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\boldsymbol{\theta}}={\mathbf{X}}.\,
  </annotation>
 </semantics>
</math>

</p>

<p>The quality of such an estimator is measured by its <a href="risk_function" title="wikilink">risk function</a>. A commonly used risk function is the <a href="mean_squared_error" title="wikilink">mean squared error</a>, defined as</p>

<p>

<math display="block" id="Stein's_example:2">
 <semantics>
  <mrow>
   <mrow>
    <mo>E</mo>
    <mrow>
     <mo>[</mo>
     <msup>
      <mrow>
       <mo>∥</mo>
       <mrow>
        <mi>𝜽</mi>
        <mo>-</mo>
        <mover accent="true">
         <mi>𝜽</mi>
         <mo stretchy="false">^</mo>
        </mover>
       </mrow>
       <mo>∥</mo>
      </mrow>
      <mn>2</mn>
     </msup>
     <mo>]</mo>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-E</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <csymbol cd="latexml">norm</csymbol>
      <apply>
       <minus></minus>
       <ci>𝜽</ci>
       <apply>
        <ci>normal-^</ci>
        <ci>𝜽</ci>
       </apply>
      </apply>
     </apply>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{E}\left[\|{\boldsymbol{\theta}}-\hat{\boldsymbol{\theta}}\|^{2}%
\right].
  </annotation>
 </semantics>
</math>

 Surprisingly, it turns out that the "ordinary" estimator proposed above is suboptimal in terms of mean squared error when <em>n</em> ≥ 3. In other words, in the setting discussed here, there exist alternative estimators which <em>always</em> achieve lower <strong><em>mean</em></strong> squared error, no matter what the value of 

<math display="inline" id="Stein's_example:3">
 <semantics>
  <mi>𝜽</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝜽</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   {\boldsymbol{\theta}}
  </annotation>
 </semantics>
</math>

 is.</p>

<p>For a given θ one could obviously define a perfect "estimator" which is always just θ, but this estimator would be bad for other values of θ. The estimators of Stein's paradox are, for a given θ, better than <em>X</em> for some values of <em>X</em> but necessarily worse for others (except perhaps for one particular θ vector, for which the new estimate is always better than <em>X</em>). It is only on average that they are better.</p>

<p>More accurately, an estimator 

<math display="inline" id="Stein's_example:4">
 <semantics>
  <msub>
   <mover accent="true">
    <mi>𝜽</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <ci>normal-^</ci>
     <ci>𝜽</ci>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\boldsymbol{\theta}}_{1}
  </annotation>
 </semantics>
</math>

 is said to <a href="dominating_decision_rule" title="wikilink">dominate</a> another estimator 

<math display="inline" id="Stein's_example:5">
 <semantics>
  <msub>
   <mover accent="true">
    <mi>𝜽</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <ci>normal-^</ci>
     <ci>𝜽</ci>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\boldsymbol{\theta}}_{2}
  </annotation>
 </semantics>
</math>

 if, for all values of 

<math display="inline" id="Stein's_example:6">
 <semantics>
  <mi>𝜽</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝜽</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   {\boldsymbol{\theta}}
  </annotation>
 </semantics>
</math>

, the risk of 

<math display="inline" id="Stein's_example:7">
 <semantics>
  <msub>
   <mover accent="true">
    <mi>𝜽</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <ci>normal-^</ci>
     <ci>𝜽</ci>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\boldsymbol{\theta}}_{1}
  </annotation>
 </semantics>
</math>

 is lower than, or equal to, the risk of 

<math display="inline" id="Stein's_example:8">
 <semantics>
  <msub>
   <mover accent="true">
    <mi>𝜽</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <ci>normal-^</ci>
     <ci>𝜽</ci>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\boldsymbol{\theta}}_{2}
  </annotation>
 </semantics>
</math>

, <em>and</em> if the inequality is <a class="uri" href="strict" title="wikilink">strict</a> for some 

<math display="inline" id="Stein's_example:9">
 <semantics>
  <mi>𝜽</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>𝜽</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   {\boldsymbol{\theta}}
  </annotation>
 </semantics>
</math>

. An estimator is said to be <a href="admissible_decision_rule" title="wikilink">admissible</a> if no other estimator dominates it, otherwise it is <em>inadmissible</em>. Thus, Stein's example can be simply stated as follows: <em>The ordinary decision rule for estimating the mean of a multivariate Gaussian distribution is inadmissible under mean squared error risk.</em></p>

<p>Many simple, practical estimators achieve better performance than the ordinary estimator. The best-known example is the <a href="James–Stein_estimator" title="wikilink">James–Stein estimator</a>, which works by starting at <em>X</em> and moving towards a particular point (such as the origin) by an amount inversely proportional to the distance of <em>X</em> from that point.</p>

<p>For a sketch of the proof of this result, see <a href="Proof_of_Stein's_example" title="wikilink">Proof of Stein's example</a>.</p>
<h2 id="implications">Implications</h2>

<p>Stein's example is surprising, since the "ordinary" decision rule is intuitive and commonly used. In fact, numerous methods for estimator construction, including <a href="maximum_likelihood_estimation" title="wikilink">maximum likelihood estimation</a>, <a href="BLUE" title="wikilink">best linear unbiased estimation</a>, <a href="least_squares" title="wikilink">least squares</a> estimation and optimal <a href="equivariant_estimation" title="wikilink">equivariant estimation</a>, all result in the "ordinary" estimator. Yet, as discussed above, this estimator is suboptimal.</p>

<p>To demonstrate the unintuitive nature of Stein's example, consider the following real-world example. Suppose we are to estimate three unrelated parameters, such as the US wheat yield for 1993, the number of spectators at the Wimbledon tennis tournament in 2001, and the weight of a randomly chosen candy bar from the supermarket. Suppose we have independent Gaussian measurements of each of these quantities. Stein's example now tells us that we can get a better estimate (on average) for the vector of three parameters by simultaneously using the three unrelated measurements.</p>

<p>At first sight it appears that somehow we get a better estimator for US wheat yield by measuring some other unrelated statistics such as the number of spectators at Wimbledon and the weight of a candy bar. This is of course absurd; we have not obtained a better estimator for US wheat yield by itself, but we have produced an estimator for the vector of the means of all three random variables, which has a reduced <em>total</em> risk. This occurs because the cost of a bad estimate in one component of the vector is compensated by a better estimate in another component. Also, a specific set of the three estimated mean values obtained with the new estimator will not necessarily be better than the ordinary set (the measured values). It is only on average that the new estimator is better.</p>
<h2 id="an-intuitive-explanation">An intuitive explanation</h2>

<p>For any particular value of <strong>θ</strong> the new estimator will improve at least one of the individual mean square errors 

<math display="inline" id="Stein's_example:10">
 <semantics>
  <mrow>
   <mrow>
    <mo>E</mo>
    <mrow>
     <mo>[</mo>
     <msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <msub>
         <mi>θ</mi>
         <mi>i</mi>
        </msub>
        <mo>-</mo>
        <msub>
         <mover accent="true">
          <mi>θ</mi>
          <mo stretchy="false">^</mo>
         </mover>
         <mi>i</mi>
        </msub>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mn>2</mn>
     </msup>
     <mo>]</mo>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-E</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>θ</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <apply>
        <ci>normal-^</ci>
        <ci>θ</ci>
       </apply>
       <ci>i</ci>
      </apply>
     </apply>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{E}\left[({\theta_{i}}-{\hat{\theta}_{i}})^{2}\right].
  </annotation>
 </semantics>
</math>

 This is not hard − for instance, if 

<math display="inline" id="Stein's_example:11">
 <semantics>
  <msub>
   <mi>θ</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>θ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta_{1}
  </annotation>
 </semantics>
</math>

 is between −1 and 1, and σ = 1, then an estimator that moves 

<math display="inline" id="Stein's_example:12">
 <semantics>
  <msub>
   <mi>X</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>X</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X_{1}
  </annotation>
 </semantics>
</math>

 towards 0 by 0.5 (or sets it to zero if its absolute value was less than 0.5) will have a lower mean square error than 

<math display="inline" id="Stein's_example:13">
 <semantics>
  <msub>
   <mi>X</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>X</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X_{1}
  </annotation>
 </semantics>
</math>

 itself. But there are other values of 

<math display="inline" id="Stein's_example:14">
 <semantics>
  <msub>
   <mi>θ</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>θ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta_{1}
  </annotation>
 </semantics>
</math>

 for which this estimator is worse than 

<math display="inline" id="Stein's_example:15">
 <semantics>
  <msub>
   <mi>X</mi>
   <mn>1</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>X</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X_{1}
  </annotation>
 </semantics>
</math>

 itself. The trick of the Stein estimator, and others that yield the Stein paradox, is that they adjust the shift in such a way that there is always (for any <strong>θ</strong> vector) at least one 

<math display="inline" id="Stein's_example:16">
 <semantics>
  <msub>
   <mi>X</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>X</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X_{i}
  </annotation>
 </semantics>
</math>

 whose mean square error is improved, and its improvement more than compensates for any degradation in mean square error that might occur for another 

<math display="inline" id="Stein's_example:17">
 <semantics>
  <msub>
   <mover accent="true">
    <mi>θ</mi>
    <mo stretchy="false">^</mo>
   </mover>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <ci>normal-^</ci>
     <ci>θ</ci>
    </apply>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\theta}_{i}
  </annotation>
 </semantics>
</math>

. The trouble is that, without knowing <strong>θ</strong>, you don't know which of the <em>n</em> mean square errors are improved, so you can't use the Stein estimator only for those parameters.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="James–Stein_estimator" title="wikilink">James–Stein estimator</a></li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<ul>
<li></li>
<li></li>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Decision_theory" title="wikilink">Category:Decision theory</a> <a href="Category:Mathematical_examples" title="wikilink">Category:Mathematical examples</a> <a href="Category:Statistical_paradoxes" title="wikilink">Category:Statistical paradoxes</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
</ol>
</section>
</body>
</html>
