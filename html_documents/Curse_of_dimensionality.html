<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="682">Curse of dimensionality</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Curse of dimensionality</h1>
<hr/>

<p>The <strong>curse of dimensionality</strong> refers to various phenomena that arise when analyzing and organizing data in <a href="high-dimensional_space" title="wikilink">high-dimensional spaces</a> (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the <a href="three-dimensional_space" title="wikilink">three-dimensional</a> <a href="physical_space" title="wikilink">physical space</a> of everyday experience.</p>

<p>There are multiple phenomena referred to by this name in domains such as <a href="numerical_analysis" title="wikilink">numerical analysis</a>, <a href="Sampling_(statistics)" title="wikilink">sampling</a>, <a class="uri" href="combinatorics" title="wikilink">combinatorics</a>, <a href="machine_learning" title="wikilink">machine learning</a>, <a href="data_mining" title="wikilink">data mining</a> and <a href="database" title="wikilink">databases</a>. The common theme of these problems is that when the dimensionality increases, the <a class="uri" href="volume" title="wikilink">volume</a> of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data however all objects appear to be sparse and dissimilar in many ways which prevents common data organization strategies from being efficient.</p>

<p>The term <em>curse of dimensionality</em> was coined by <a href="Richard_E._Bellman" title="wikilink">Richard E. Bellman</a> when considering problems in dynamic optimization.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h2 id="the-curse-of-dimensionality-depends-on-the-algorithm">The "curse of dimensionality" depends on the algorithm</h2>

<p>The "curse of dimensionality" is not a problem of high-dimensional data, but a joint problem of the data and the algorithm being applied. It arises when the algorithm does not scale well to high-dimensional data, typically due to needing an amount of time or memory that is <a href="exponential_growth" title="wikilink">exponential</a> in the number of dimensions of the data.</p>

<p>When facing the curse of dimensionality, a good solution can often be found by changing the algorithm, or by pre-processing the data into a lower-dimensional form. For example, the notion of <a href="intrinsic_dimension" title="wikilink">intrinsic dimension</a> refers to the fact that any low-dimensional data space can trivially be turned into a higher-dimensional space by adding redundant (e.g. duplicate) or randomized dimensions, and in turn many high-dimensional data sets can be reduced to lower-dimensional data without significant information loss. This is also reflected by the effectiveness of <a href="dimension_reduction" title="wikilink">dimension reduction</a> methods such as <a href="principal_component_analysis" title="wikilink">principal component analysis</a> in many situations. Algorithms that are based on distance functions or nearest neighbor search can also work robustly on data having many spurious dimensions, depending on the statistics of those dimensions.</p>
<h2 id="curse-of-dimensionality-in-different-domains">Curse of dimensionality in different domains</h2>
<h3 id="combinatorics">Combinatorics</h3>

<p>In some problems, each variable can take one of several discrete values, or the range of possible values is divided to give a finite number of possibilities. Taking the variables together, a huge number of combinations of values must be considered. This effect is also known as the <a href="combinatorial_explosion" title="wikilink">combinatorial explosion</a>. Even in the simplest case of d binary variables, the number of possible combinations already is 

<math display="inline" id="Curse_of_dimensionality:0">
 <semantics>
  <mrow>
   <mi>O</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msup>
     <mn>2</mn>
     <mi>d</mi>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>O</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <cn type="integer">2</cn>
     <ci>d</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   O(2^{d})
  </annotation>
 </semantics>
</math>

, exponential in the dimensionality. Naively, each additional dimension doubles the effort needed to try all combinations.</p>
<h3 id="sampling">Sampling</h3>

<p>There is an exponential increase in <a class="uri" href="volume" title="wikilink">volume</a> associated with adding extra dimensions to a <a href="Space_(mathematics)" title="wikilink">mathematical space</a>. For example, 10<sup>2</sup>=100 evenly-spaced sample points suffice to sample a <a href="unit_interval" title="wikilink">unit interval</a> (a "1-dimensional cube") with no more than 10<sup>−2</sup>=0.01 distance between points; an equivalent sampling of a 10-dimensional <a href="unit_hypercube" title="wikilink">unit hypercube</a> with a lattice that has a spacing of 10<sup>−2</sup>=0.01 between adjacent points would require 10<sup>20</sup>[=(10<sup>2</sup>)<sup>10</sup>] sample points. In general, with a spacing distance of 10<sup>-n</sup> the 10-dimensional hypercube appears to be a factor of 10<sup>n(10-1)</sup>[=(10<sup>n</sup>)<sup>10</sup>/(10<sup>n</sup>)] "larger" than the 1-dimensional hypercube, which is the unit interval. In the above example n=2: when using a sampling distance of 0.01 the 10-dimensional hypercube appears to be 10<sup>18</sup> "larger" than the unit interval. This effect is a combination of the combinatorics problems above and the distance function problems explained below.</p>
<h3 id="optimization">Optimization</h3>

<p>When solving dynamic <a href="optimization_(mathematics)" title="wikilink">optimization</a> problems by numerical <a href="backward_induction" title="wikilink">backward induction</a>, the objective function must be computed for each combination of values. This is a significant obstacle when the dimension of the "state variable" is large.</p>
<h3 id="machine-learning">Machine learning</h3>

<p>In <a href="machine_learning" title="wikilink">machine learning</a> problems that involve learning a "state-of-nature" (maybe an infinite distribution) from a finite number of data samples in a high-dimensional <a href="feature_space" title="wikilink">feature space</a> with each feature having a number of possible values, an enormous amount of training data are required to ensure that there are several samples with each combination of values. With a fixed number of training samples, the predictive power reduces as the dimensionality increases, and this is known as the <em>Hughes effect</em><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> or <em>Hughes phenomenon</em> (named after Gordon F. Hughes).<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h3 id="bayesian-statistics">Bayesian statistics</h3>

<p>The curse of dimensionality has often been a difficulty with <a href="Bayesian_statistics" title="wikilink">Bayesian statistics</a>, for which the <a href="posterior_distribution" title="wikilink">posterior distributions</a> often have many parameters.</p>

<p>However, this problem has been largely overcome by the advent of simulation-based Bayesian inference, especially using <a href="Markov_chain_Monte_Carlo" title="wikilink">Markov chain Monte Carlo</a> methods, which suffices for many practical problems. Of course, simulation-based methods converge slowly and therefore are not a panacea for high-dimensional problems.</p>
<h3 id="distance-functions">Distance functions</h3>

<p>When a measure such as a <a href="Euclidean_distance" title="wikilink">Euclidean distance</a> is defined using many coordinates, there is little difference in the distances between different pairs of samples.</p>

<p>One way to illustrate the "vastness" of high-dimensional Euclidean space is to compare the proportion of an inscribed <a class="uri" href="hypersphere" title="wikilink">hypersphere</a> with radius 

<math display="inline" id="Curse_of_dimensionality:1">
 <semantics>
  <mi>r</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>r</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r
  </annotation>
 </semantics>
</math>

 and dimension 

<math display="inline" id="Curse_of_dimensionality:2">
 <semantics>
  <mi>d</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d
  </annotation>
 </semantics>
</math>

, to that of a <a class="uri" href="hypercube" title="wikilink">hypercube</a> with edges of length 

<math display="inline" id="Curse_of_dimensionality:3">
 <semantics>
  <mrow>
   <mn>2</mn>
   <mi>r</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <cn type="integer">2</cn>
    <ci>r</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   2r
  </annotation>
 </semantics>
</math>

. The volume of such a sphere is

<math display="block" id="Curse_of_dimensionality:4">
 <semantics>
  <mfrac>
   <mrow>
    <mn>2</mn>
    <msup>
     <mi>r</mi>
     <mi>d</mi>
    </msup>
    <msup>
     <mi>π</mi>
     <mrow>
      <mi>d</mi>
      <mo>/</mo>
      <mn>2</mn>
     </mrow>
    </msup>
   </mrow>
   <mrow>
    <mpadded width="+2.8pt">
     <mi>d</mi>
    </mpadded>
    <mi mathvariant="normal">Γ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>d</mi>
      <mo>/</mo>
      <mn>2</mn>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mfrac>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <times></times>
     <cn type="integer">2</cn>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>r</ci>
      <ci>d</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>π</ci>
      <apply>
       <divide></divide>
       <ci>d</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>d</ci>
     <ci>normal-Γ</ci>
     <apply>
      <divide></divide>
      <ci>d</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{2r^{d}\pi^{d/2}}{d\;\Gamma(d/2)}
  </annotation>
 </semantics>
</math>

. The volume of the cube would be

<math display="block" id="Curse_of_dimensionality:5">
 <semantics>
  <msup>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mn>2</mn>
     <mi>r</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>d</mi>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <apply>
     <times></times>
     <cn type="integer">2</cn>
     <ci>r</ci>
    </apply>
    <ci>d</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (2r)^{d}
  </annotation>
 </semantics>
</math>

. As the dimension 

<math display="inline" id="Curse_of_dimensionality:6">
 <semantics>
  <mi>d</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d
  </annotation>
 </semantics>
</math>

 of the space increases, the hypersphere becomes an insignificant volume relative to that of the hypercube. This can clearly be seen by comparing the proportions as the dimension 

<math display="inline" id="Curse_of_dimensionality:7">
 <semantics>
  <mi>d</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d
  </annotation>
 </semantics>
</math>

 goes to infinity:</p>

<p>

<math display="block" id="Curse_of_dimensionality:8">
 <semantics>
  <mrow>
   <mfrac>
    <msup>
     <mi>π</mi>
     <mrow>
      <mi>d</mi>
      <mo>/</mo>
      <mn>2</mn>
     </mrow>
    </msup>
    <mrow>
     <mi>d</mi>
     <msup>
      <mn>2</mn>
      <mrow>
       <mi>d</mi>
       <mo>-</mo>
       <mn>1</mn>
      </mrow>
     </msup>
     <mi mathvariant="normal">Γ</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>d</mi>
       <mo>/</mo>
       <mn>2</mn>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mfrac>
   <mo>→</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <apply>
     <divide></divide>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>π</ci>
      <apply>
       <divide></divide>
       <ci>d</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>d</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <cn type="integer">2</cn>
       <apply>
        <minus></minus>
        <ci>d</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>normal-Γ</ci>
      <apply>
       <divide></divide>
       <ci>d</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{\pi^{d/2}}{d2^{d-1}\Gamma(d/2)}\rightarrow 0
  </annotation>
 </semantics>
</math>

 as 

<math display="inline" id="Curse_of_dimensionality:9">
 <semantics>
  <mrow>
   <mi>d</mi>
   <mo>→</mo>
   <mi mathvariant="normal">∞</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>d</ci>
    <infinity></infinity>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d\rightarrow\infty
  </annotation>
 </semantics>
</math>

. Furthermore, the distance between the center and the corners is 

<math display="inline" id="Curse_of_dimensionality:10">
 <semantics>
  <mrow>
   <mi>r</mi>
   <msqrt>
    <mi>d</mi>
   </msqrt>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>r</ci>
    <apply>
     <root></root>
     <ci>d</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r\sqrt{d}
  </annotation>
 </semantics>
</math>

, which increases without bound for fixed r. In this sense, nearly all of the high-dimensional space is "far away" from the centre. To put it another way, the high-dimensional unit hypercube can be said to consist almost entirely of the "corners" of the hypercube, with almost no "middle".</p>

<p>This also helps to understand the <a href="chi-squared_distribution" title="wikilink">chi-squared distribution</a>. Indeed, the (non-central) chi-squared distribution associated to a random point in the interval <em>[-1,1]</em> is the same as the distribution of the length-squared of a random point in the <em>d</em>-cube. By the law of large numbers, this distribution concentrates itself in a narrow band around <em>d</em> times the standard deviation squared (σ<sup>2</sup>) of the original derivation. This illuminates the chi-squared distribution and also illustrates that most of the volume of the <em>d</em>-cube concentrates near the surface of a sphere of radius 

<math display="inline" id="Curse_of_dimensionality:11">
 <semantics>
  <msqrt>
   <mi>d</mi>
  </msqrt>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <root></root>
    <ci>d</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sqrt{d}
  </annotation>
 </semantics>
</math>

<em>σ</em>.</p>

<p>A further development of this phenomenon is as follows. Any fixed distribution on <em>R</em> induces a product distribution on points in <em>R<sup>d</sup></em>. For any fixed <em>n</em>, it turns out that the minimum and the maximum distance between a random reference point <em>Q</em> and a list of <em>n</em> random data points <em>P<sub>1</sub>,...,P<sub>n</sub></em> become indiscernible compared to the minimum distance:<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>

<p>

<math display="block" id="Curse_of_dimensionality:12">
 <semantics>
  <mrow>
   <mrow>
    <munder>
     <mo movablelimits="false">lim</mo>
     <mrow>
      <mi>d</mi>
      <mo>→</mo>
      <mi mathvariant="normal">∞</mi>
     </mrow>
    </munder>
    <mrow>
     <mi>E</mi>
     <mrow>
      <mo>(</mo>
      <mfrac>
       <mrow>
        <mrow>
         <msub>
          <mo>dist</mo>
          <mi>max</mi>
         </msub>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>d</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>-</mo>
        <mrow>
         <msub>
          <mo>dist</mo>
          <mi>min</mi>
         </msub>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>d</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
       </mrow>
       <mrow>
        <msub>
         <mo>dist</mo>
         <mi>min</mi>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>d</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mfrac>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>→</mo>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <limit></limit>
      <apply>
       <ci>normal-→</ci>
       <ci>d</ci>
       <infinity></infinity>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>E</ci>
      <apply>
       <divide></divide>
       <apply>
        <minus></minus>
        <apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>dist</ci>
          <max></max>
         </apply>
         <ci>d</ci>
        </apply>
        <apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>dist</ci>
          <min></min>
         </apply>
         <ci>d</ci>
        </apply>
       </apply>
       <apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>dist</ci>
         <min></min>
        </apply>
        <ci>d</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lim_{d\to\infty}E\left(\frac{\operatorname{dist}_{\max}(d)-\operatorname{dist%
}_{\min}(d)}{\operatorname{dist}_{\min}(d)}\right)\to 0
  </annotation>
 </semantics>
</math>

. This is often cited as distance functions losing their usefulness (for the nearest-neighbor criterion in feature-comparison algorithms, for example) in high dimensions. However, recent research has shown this to only hold in the artificial scenario when the one-dimensional distributions <em>R</em> are independent and identically distributed.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> When attributes are correlated, data can become easier and provide higher distance contrast and the <a href="signal-to-noise_ratio" title="wikilink">signal-to-noise ratio</a> was found to play an important role, thus <a href="feature_selection" title="wikilink">feature selection</a> should be used.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>
<h3 id="nearest-neighbor-search">Nearest neighbor search</h3>

<p>The effect complicates <a href="nearest_neighbor_search" title="wikilink">nearest neighbor search</a> in high dimensional space. It is not possible to quickly reject candidates by using the difference in one coordinate as a lower bound for a distance based on all the dimensions.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a><a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>

<p>However, it has recently been observed that the mere number of dimensions does not necessarily result in difficulties,<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> since <em>relevant</em> additional dimensions can also increase the contrast. In addition, for the resulting ranking it remains useful to discern close and far neighbors. Irrelevant ("noise") dimensions, however, reduce the contrast in the manner described above. In <a href="time_series_analysis" title="wikilink">time series analysis</a>, where the data are inherently high-dimensional, distance functions also work reliably as long as the <a href="signal-to-noise_ratio" title="wikilink">signal-to-noise ratio</a> is high enough.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></p>
<h4 id="k-nearest-neighbor-classification"><em>k</em>-nearest neighbor classification</h4>

<p>Another effect of high dimensionality on distance functions concerns <em>k</em>-nearest neighbor (<em>k</em>-NN) <a href="Graph_(mathematics)" title="wikilink">graphs</a> constructed from a <a href="data_set" title="wikilink">data set</a> using a distance function. As the dimension increases, the <a class="uri" href="indegree" title="wikilink">indegree</a> distribution of the <em>k</em>-NN <a href="directed_graph" title="wikilink">digraph</a> becomes <a href="Skewness" title="wikilink">skewed</a> with a peak on the right because of the emergence of a disproportionate number of <strong>hubs</strong>, that is, data-points that appear in many more <em>k</em>-NN lists of other data-points than the average. This phenomenon can have a considerable impact on various techniques for <a href="Classification_(machine_learning)" title="wikilink">classification</a> (including the <a href="K-nearest_neighbor_algorithm" title="wikilink"><em>k</em>-NN classifier</a>), <a href="semi-supervised_learning" title="wikilink">semi-supervised learning</a>, and <a href="Cluster_analysis" title="wikilink">clustering</a>,<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> and it also affects <a href="information_retrieval" title="wikilink">information retrieval</a>.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></p>
<h3 id="anomaly-detection">Anomaly detection</h3>

<p>In a recent survey, Zimek et al. identified the following problems when searching for <a href="anomaly_detection" title="wikilink">anomalies</a> in high-dimensional data:<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>
<ol>
<li>Concentration of scores and distances: derived values such as distances become numerically similar</li>
<li>Irrelevant attributes: in high dimensional data, a significant number of attributes may be irrelevant</li>
<li>Definition of reference sets: for local methods, reference sets are often nearest-neighbor based</li>
<li>Incomparable scores for different dimensionalities: different subspaces produce incomparable scores</li>
<li>Interpretability of scores: the scores often no longer convey a semantic meaning</li>
<li>Exponential search space: the search space can no longer be systematically scanned</li>
<li><a href="Data_snooping" title="wikilink">Data snooping</a> bias: given the large search space, for every desired significance an hypothesis can be found</li>
<li>Hubness: certain objects occur more frequently in neighbor lists than others.</li>
</ol>

<p>Many of the analyzed specialized methods tackle one or another of these problems, but there remain many open research questions.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Bellman_equation" title="wikilink">Bellman equation</a></li>
<li><a href="Backwards_induction" title="wikilink">Backwards induction</a></li>
<li><a href="Cluster_analysis" title="wikilink">Cluster analysis</a></li>
<li><a href="Clustering_high-dimensional_data" title="wikilink">Clustering high-dimensional data</a></li>
<li><a href="Combinatorial_explosion" title="wikilink">Combinatorial explosion</a></li>
<li><a href="Concentration_of_measure" title="wikilink">Concentration of measure</a></li>
<li><a href="Dimension_reduction" title="wikilink">Dimension reduction</a></li>
<li><a href="Dynamic_programming" title="wikilink">Dynamic programming</a></li>
<li><a href="Fourier-related_transforms" title="wikilink">Fourier-related transforms</a></li>
<li><a href="High-dimensional_space" title="wikilink">High-dimensional space</a></li>
<li><a href="Linear_least_squares_(mathematics)" title="wikilink">Linear least squares</a></li>
<li><a href="Multilinear_principal_component_analysis" title="wikilink">Multilinear PCA</a></li>
<li><a href="Multilinear_subspace_learning" title="wikilink">Multilinear subspace learning</a></li>
<li><a href="Principal_component_analysis" title="wikilink">Principal component analysis</a></li>
<li><a class="uri" href="Quasi-random" title="wikilink">Quasi-random</a></li>
<li><a href="Singular_value_decomposition" title="wikilink">Singular value decomposition</a></li>
<li><a class="uri" href="Stereology" title="wikilink">Stereology</a></li>
<li><a href="Time_series" title="wikilink">Time series</a></li>
<li><a class="uri" href="Wavelet" title="wikilink">Wavelet</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Numerical_analysis" title="wikilink">Category:Numerical analysis</a> <a href="Category:Dynamic_programming" title="wikilink">Category:Dynamic programming</a> <a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">,<br/>
Republished: <a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5">Not to be confused with the unrelated, but similarly named, <em>Hughes effect in <a class="uri" href="electromagnetism" title="wikilink">electromagnetism</a></em> (named after [<a class="uri" href="http://spiedl.aip.org/vsearch/servlet/VerityServlet?KEY=SPIEDL&amp;possible1">http://spiedl.aip.org/vsearch/servlet/VerityServlet?KEY=SPIEDL&amp;possible1;</a>;=Hughes%2C+Declan+C.&amp;possible1zone;=author&amp;maxdisp;=25&amp;smode;=strresults&amp;pjournals;=OPEGAR%2CJBOPFO%2CPSISDG%2CJEIME5%2CJMMMGF%2CJARSC4%2CJNOACQ&amp;deliveryType;=spiedl&amp;aqs;=true Declan C. Hughes]) which refers to an asymmetry in the <a class="uri" href="hysteresis" title="wikilink">hysteresis</a> curves of <a href="Magnetic_core" title="wikilink">laminated cores</a> made of certain <a href="magnetic_materials" title="wikilink">magnetic materials</a>, such as <a class="uri" href="permalloy" title="wikilink">permalloy</a> or <a class="uri" href="mu-metal" title="wikilink">mu-metal</a>, in alternating magnetic fields.<a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"></li>
<li id="fn8"></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"><a href="#fnref11">↩</a></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"><a href="#fnref14">↩</a></li>
<li id="fn15"><a href="#fnref15">↩</a></li>
</ol>
</section>
</body>
</html>
