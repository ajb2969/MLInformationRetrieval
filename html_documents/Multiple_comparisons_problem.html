<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="570">Multiple comparisons problem</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Multiple comparisons problem</h1>
<hr/>

<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, the <strong>multiple comparisons</strong>, <strong>multiplicity</strong> or <strong>multiple testing problem</strong> occurs when one considers a set of <a href="statistical_inference" title="wikilink">statistical inferences</a> simultaneously<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> or infers a subset of parameters selected based on the observed values.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> It is also known as the <a href="look-elsewhere_effect" title="wikilink">look-elsewhere effect</a>.</p>

<p>Errors in inference, including <a href="confidence_interval" title="wikilink">confidence intervals</a> that fail to include their corresponding population parameters or <a href="hypothesis_test" title="wikilink">hypothesis tests</a> that incorrectly reject the <a href="null_hypothesis" title="wikilink">null hypothesis</a>, are more likely to occur when one considers the set as a whole. Several statistical techniques have been developed to prevent this from happening, allowing significance levels for single and multiple comparisons to be directly compared. These techniques generally require a higher significance threshold for individual comparisons, so as to compensate for the number of inferences being made.</p>
<h2 id="history">History</h2>

<p>The interest in the problem of multiple comparisons began in the 1950s with the work of <a class="uri" href="Tukey" title="wikilink">Tukey</a> and <a class="uri" href="Scheffé" title="wikilink">Scheffé</a>. New methods and procedures came out: <a href="Closed_testing_procedure" title="wikilink">Closed testing procedure</a> (Marcus et al., 1976), <a href="Holm–Bonferroni_method" title="wikilink">Holm–Bonferroni method</a> (1979). Later, in the 1980s, the issue of multiple comparisons came back (Hochberg and Tamhane (1987), Westfall and Young (1993), and Hsu (1996)). In 1995 the work on <a href="False_discovery_rate" title="wikilink">False discovery rate</a> and other new ideas began. In 1996 the first conference on multiple comparisons took place in <a class="uri" href="Israel" title="wikilink">Israel</a>. This was followed by conferences around the world: <a class="uri" href="Berlin" title="wikilink">Berlin</a> (2000), <a href="Bethesda,_Maryland" title="wikilink">Bethesda</a> (2002), <a class="uri" href="Shanghai" title="wikilink">Shanghai</a> (2005), <a class="uri" href="Vienna" title="wikilink">Vienna</a> (2007), and <a class="uri" href="Tokyo" title="wikilink">Tokyo</a> (2009). All these reflect an acceleration of increase of interest in multiple comparisons.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="the-problem">The problem</h2>

<p>In this context the term "comparisons" refers to comparisons of two groups, such as a treatment group and a control group. "Multiple comparisons" arise when a statistical analysis encompasses a number of formal comparisons, with the presumption that attention will focus on the strongest differences among all comparisons that are made. Failure to compensate for multiple comparisons can have important real-world consequences, as illustrated by the following examples.</p>
<ul>
<li>Suppose the treatment is a new way of teaching writing to students, and the control is the standard way of teaching writing. Students in the two groups can be compared in terms of grammar, spelling, organization, content, and so on. As more attributes are compared, it becomes more likely that the treatment and control groups will appear to differ on at least one attribute <em>by random chance alone.</em></li>
</ul>
<ul>
<li>Suppose we consider the efficacy of a <a href="Pharmacology" title="wikilink">drug</a> in terms of the reduction of any one of a number of disease symptoms. As more symptoms are considered, it becomes more likely that the drug will appear to be an improvement over existing drugs in terms of at least one symptom.</li>
</ul>
<ul>
<li>Suppose we consider the safety of a drug in terms of the occurrences of different types of side effects. As more types of side effects are considered, it becomes more likely that the new drug will appear to be less safe than existing drugs in terms of at least one side effect.</li>
</ul>

<p>In all three examples, as the number of comparisons increases, it becomes more likely that the groups being compared will appear to differ in terms of at least one attribute. Our confidence that a result will generalize to independent data should generally be weaker if it is observed as part of an analysis that involves multiple comparisons, rather than an analysis that involves only a single comparison.</p>

<p>For example, if one test is performed at the 5% level, there is only a 5% chance of incorrectly rejecting the null hypothesis if the null hypothesis is true. However, for 100 tests where all null hypotheses are true, the expected number of incorrect rejections is 5. If the tests are independent, the probability of at least one incorrect rejection is 99.4%. These errors are called <a href="false_positive" title="wikilink">false positives</a> or <a href="Type_I_error" title="wikilink">Type I errors</a>.</p>

<p>The problem also occurs for <a href="confidence_intervals" title="wikilink">confidence intervals</a>, note that a single confidence interval with 95% <a href="coverage_probability" title="wikilink">coverage probability</a> level will likely contain the population parameter it is meant to contain, i.e. in the long run 95% of confidence intervals built in that way will contain the true population parameter. However, if one considers 100 confidence intervals simultaneously, with coverage probability 0.95 each, it is highly likely that at least one interval will not contain its population parameter. The expected number of such non-covering intervals is 5, and if the intervals are independent, the probability that at least one interval does not contain the population parameter is 99.4%.</p>

<p>Techniques have been developed to control the false positive error rate associated with performing multiple statistical tests. Similarly, techniques have been developed to adjust confidence intervals so that the probability of at least one of the intervals not covering its target value is controlled.</p>
<h3 id="classification-of-m-hypothesis-tests">Classification of <em>m</em> hypothesis tests</h3>

<p>The following table gives a number of errors committed when testing 

<math display="inline" id="Multiple_comparisons_problem:0">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

 null hypotheses. It defines some random variables that are related to the 

<math display="inline" id="Multiple_comparisons_problem:1">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

 hypothesis tests.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">
<p>|</p></th>
<th style="text-align: left;">
<p>Null hypothesis is True (H<sub>0</sub>)</p></th>
<th style="text-align: left;">
<p>Alternative hypothesis is True (H<sub>1</sub>)</p></th>
<th style="text-align: left;">
<p>| Total</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p>| Declared significant</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Multiple_comparisons_problem:2">
 <semantics>
  <mi>V</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>V</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Multiple_comparisons_problem:3">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>


</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Multiple_comparisons_problem:4">
 <semantics>
  <mi>R</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>R</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R
  </annotation>
 </semantics>
</math>

</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>| Declared non-significant</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Multiple_comparisons_problem:5">
 <semantics>
  <mi>U</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>U</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   U
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Multiple_comparisons_problem:6">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Multiple_comparisons_problem:7">
 <semantics>
  <mrow>
   <mi>m</mi>
   <mo>-</mo>
   <mi>R</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <ci>m</ci>
    <ci>R</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m-R
  </annotation>
 </semantics>
</math>

</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>Total</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Multiple_comparisons_problem:8">
 <semantics>
  <msub>
   <mi>m</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>m</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m_{0}
  </annotation>
 </semantics>
</math>


</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Multiple_comparisons_problem:9">
 <semantics>
  <mrow>
   <mi>m</mi>
   <mo>-</mo>
   <msub>
    <mi>m</mi>
    <mn>0</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <ci>m</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>m</ci>
     <cn type="integer">0</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m-m_{0}
  </annotation>
 </semantics>
</math>

</p></td>
<td style="text-align: left;">
<p>

<math display="inline" id="Multiple_comparisons_problem:10">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

</p></td>
</tr>
</tbody>
</table>
<ul>
<li>

<math display="inline" id="Multiple_comparisons_problem:11">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

 is the total number hypotheses tested</li>
<li>

<math display="inline" id="Multiple_comparisons_problem:12">
 <semantics>
  <msub>
   <mi>m</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>m</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m_{0}
  </annotation>
 </semantics>
</math>

 is the number of true <a href="null_hypothesis" title="wikilink">null hypotheses</a></li>
<li>

<math display="inline" id="Multiple_comparisons_problem:13">
 <semantics>
  <mrow>
   <mi>m</mi>
   <mo>-</mo>
   <msub>
    <mi>m</mi>
    <mn>0</mn>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <ci>m</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>m</ci>
     <cn type="integer">0</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m-m_{0}
  </annotation>
 </semantics>
</math>


 is the number of true <a href="alternative_hypothesis" title="wikilink">alternative hypotheses</a></li>
<li>

<math display="inline" id="Multiple_comparisons_problem:14">
 <semantics>
  <mi>V</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>V</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V
  </annotation>
 </semantics>
</math>

 is the number of <a href="Type_I_and_type_II_errors" title="wikilink">false positives (Type I error)</a> (also called "false discoveries")</li>
<li>

<math display="inline" id="Multiple_comparisons_problem:15">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

 is the number of <a href="Type_I_and_type_II_errors" title="wikilink">true positives</a> (also called "true discoveries")</li>
<li>

<math display="inline" id="Multiple_comparisons_problem:16">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>

 is the number of <a href="Type_I_and_type_II_errors" title="wikilink">false negatives (Type II error)</a></li>
<li>

<math display="inline" id="Multiple_comparisons_problem:17">
 <semantics>
  <mi>U</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>U</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   U
  </annotation>
 </semantics>
</math>

 is the number of <a href="Type_I_and_type_II_errors" title="wikilink">true negatives</a></li>
<li>

<math display="inline" id="Multiple_comparisons_problem:18">
 <semantics>
  <mi>R</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>R</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R
  </annotation>
 </semantics>
</math>


 is the number of rejected null hypotheses (also called "discoveries")</li>
<li>In 

<math display="inline" id="Multiple_comparisons_problem:19">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

 hypothesis tests of which 

<math display="inline" id="Multiple_comparisons_problem:20">
 <semantics>
  <msub>
   <mi>m</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>m</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m_{0}
  </annotation>
 </semantics>
</math>

 are true null hypotheses, 

<math display="inline" id="Multiple_comparisons_problem:21">
 <semantics>
  <mi>R</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>R</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R
  </annotation>
 </semantics>
</math>

 is an observable random variable, and 

<math display="inline" id="Multiple_comparisons_problem:22">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Multiple_comparisons_problem:23">
 <semantics>
  <mi>T</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>T</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   T
  </annotation>
 </semantics>
</math>


, 

<math display="inline" id="Multiple_comparisons_problem:24">
 <semantics>
  <mi>U</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>U</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   U
  </annotation>
 </semantics>
</math>

, and 

<math display="inline" id="Multiple_comparisons_problem:25">
 <semantics>
  <mi>V</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>V</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   V
  </annotation>
 </semantics>
</math>

 are unobservable <a href="random_variable" title="wikilink">random variables</a>.</li>
</ul>
<h2 id="example-flipping-coins">Example: Flipping coins</h2>

<p>For example, one might declare that a coin was biased if in 10 flips it landed heads at least 9 times. Indeed, if one assumes as a <a href="null_hypothesis" title="wikilink">null hypothesis</a> that the coin is fair, then the probability that a fair coin would come up heads at least 9 out of 10 times is (10 + 1) × (1/2)<sup>10</sup> = 0.0107. This is relatively unlikely, and under <a href="statistical_significance" title="wikilink">statistical criteria</a> such as <a class="uri" href="p-value" title="wikilink">p-value</a> 100 ≈ 0.34. Therefore the application of our single-test coin-fairness criterion to multiple comparisons would be more likely to falsely identify at least one fair coin as unfair.</p>
<h2 id="what-can-be-done">What can be done</h2>

<p>For hypothesis testing, the problem of multiple comparisons (also known as the <em>multiple testing problem</em>) results from the increase in <a href="type_I_error" title="wikilink">type I error</a> that occurs when statistical tests are used repeatedly. If <em>k</em> independent comparisons are performed, the experiment-wide <a href="statistical_significance" title="wikilink">significance level</a> 

<math display="inline" id="Multiple_comparisons_problem:26">
 <semantics>
  <mover accent="true">
   <mi>α</mi>
   <mo stretchy="false">¯</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-¯</ci>
    <ci>α</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{\alpha}
  </annotation>
 </semantics>
</math>

, also termed FWER for <a href="family-wise_error_rate" title="wikilink">family-wise error rate</a>, is given by</p>

<p>

<math display="block" id="Multiple_comparisons_problem:27">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>α</mi>
    <mo stretchy="false">¯</mo>
   </mover>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <msup>
     <mrow>
      <mo>(</mo>
      <mrow>
       <mn>1</mn>
       <mo>-</mo>
       <msub>
        <mi>α</mi>
        <mrow>
         <mo stretchy="false">{</mo>
         <mrow>
          <mpadded width="+5pt">
           <mi>per</mi>
          </mpadded>
          <mi>comparison</mi>
         </mrow>
         <mo stretchy="false">}</mo>
        </mrow>
       </msub>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mi>k</mi>
    </msup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>normal-¯</ci>
     <ci>α</ci>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>α</ci>
        <set>
         <apply>
          <times></times>
          <ci>per</ci>
          <ci>comparison</ci>
         </apply>
        </set>
       </apply>
      </apply>
      <ci>k</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{\alpha}=1-\left(1-\alpha_{\mathrm{\{per\ comparison\}}}\right)^{k}
  </annotation>
 </semantics>
</math>

. Hence, unless the tests are perfectly dependent, 

<math display="inline" id="Multiple_comparisons_problem:28">
 <semantics>
  <mover accent="true">
   <mi>α</mi>
   <mo stretchy="false">¯</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-¯</ci>
    <ci>α</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{\alpha}
  </annotation>
 </semantics>
</math>


 increases as the number of comparisons increases. If we do not assume that the comparisons are independent, then we can still say:</p>

<p>

<math display="block" id="Multiple_comparisons_problem:29">
 <semantics>
  <mrow>
   <mrow>
    <mover accent="true">
     <mi>α</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mo>≤</mo>
    <mrow>
     <mi>k</mi>
     <mo>⋅</mo>
     <msub>
      <mi>α</mi>
      <mrow>
       <mo stretchy="false">{</mo>
       <mrow>
        <mpadded width="+5pt">
         <mi>per</mi>
        </mpadded>
        <mi>comparison</mi>
       </mrow>
       <mo stretchy="false">}</mo>
      </mrow>
     </msub>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <apply>
     <ci>normal-¯</ci>
     <ci>α</ci>
    </apply>
    <apply>
     <ci>normal-⋅</ci>
     <ci>k</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>α</ci>
      <set>
       <apply>
        <times></times>
        <ci>per</ci>
        <ci>comparison</ci>
       </apply>
      </set>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{\alpha}\leq k\cdot\alpha_{\mathrm{\{per\ comparison\}}},
  </annotation>
 </semantics>
</math>

</p>

<p>which follows from <a href="Boole's_inequality" title="wikilink">Boole's inequality</a>. Example

<math display="block" id="Multiple_comparisons_problem:30">
 <semantics>
  <mrow>
   <mn>0.2649</mn>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <msup>
     <mrow>
      <mo>(</mo>
      <mrow>
       <mn>1</mn>
       <mo>-</mo>
       <mn>.05</mn>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mn>6</mn>
    </msup>
   </mrow>
   <mo>≤</mo>
   <mrow>
    <mn>.05</mn>
    <mo>×</mo>
    <mn>6</mn>
   </mrow>
   <mo>=</mo>
   <mn>0.3</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <cn type="float">0.2649</cn>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <cn type="integer">1</cn>
        <cn type="float">.05</cn>
       </apply>
       <cn type="integer">6</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <leq></leq>
     <share href="#.cmml">
     </share>
     <apply>
      <times></times>
      <cn type="float">.05</cn>
      <cn type="integer">6</cn>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <cn type="float">0.3</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   0.2649=1-\left(1-.05\right)^{6}\leq.05\times 6=0.3
  </annotation>
 </semantics>
</math>

</p>

<p>There are different ways to assure that the family-wise error rate is at most 

<math display="inline" id="Multiple_comparisons_problem:31">
 <semantics>
  <mover accent="true">
   <mi>α</mi>
   <mo stretchy="false">¯</mo>
  </mover>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-¯</ci>
    <ci>α</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \bar{\alpha}
  </annotation>
 </semantics>
</math>

. The most conservative method, but which is free of dependence and distributional assumptions, is the <a href="Bonferroni_correction" title="wikilink">Bonferroni correction</a> 

<math display="inline" id="Multiple_comparisons_problem:32">
 <semantics>
  <mrow>
   <msub>
    <mi>α</mi>
    <mrow>
     <mo stretchy="false">{</mo>
     <mrow>
      <mpadded width="+5pt">
       <mi>per</mi>
      </mpadded>
      <mi>comparison</mi>
     </mrow>
     <mo stretchy="false">}</mo>
    </mrow>
   </msub>
   <mo>=</mo>
   <mrow>
    <mover accent="true">
     <mi>α</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mo>/</mo>
    <mi>k</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>α</ci>
     <set>
      <apply>
       <times></times>
       <ci>per</ci>
       <ci>comparison</ci>
      </apply>
     </set>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <ci>normal-¯</ci>
      <ci>α</ci>
     </apply>
     <ci>k</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha_{\mathrm{\{per\ comparison\}}}=\bar{\alpha}/k
  </annotation>
 </semantics>
</math>

.</p>

<p>A more accurate correction can be obtained by solving the equation for the family-wise error rate of 

<math display="inline" id="Multiple_comparisons_problem:33">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>


 independent comparisons for 

<math display="inline" id="Multiple_comparisons_problem:34">
 <semantics>
  <msub>
   <mi>α</mi>
   <mrow>
    <mo stretchy="false">{</mo>
    <mrow>
     <mpadded width="+5pt">
      <mi>per</mi>
     </mpadded>
     <mi>comparison</mi>
    </mrow>
    <mo stretchy="false">}</mo>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>α</ci>
    <set>
     <apply>
      <times></times>
      <ci>per</ci>
      <ci>comparison</ci>
     </apply>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha_{\mathrm{\{per\ comparison\}}}
  </annotation>
 </semantics>
</math>

. This yields 

<math display="inline" id="Multiple_comparisons_problem:35">
 <semantics>
  <mrow>
   <msub>
    <mi>α</mi>
    <mrow>
     <mo stretchy="false">{</mo>
     <mrow>
      <mpadded width="+5pt">
       <mi>per</mi>
      </mpadded>
      <mi>comparison</mi>
     </mrow>
     <mo stretchy="false">}</mo>
    </mrow>
   </msub>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <msup>
     <mrow>
      <mo>(</mo>
      <mrow>
       <mn>1</mn>
       <mo>-</mo>
       <mover accent="true">
        <mi>α</mi>
        <mo stretchy="false">¯</mo>
       </mover>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mfrac>
      <mn>1</mn>
      <mi>k</mi>
     </mfrac>
    </msup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>α</ci>
     <set>
      <apply>
       <times></times>
       <ci>per</ci>
       <ci>comparison</ci>
      </apply>
     </set>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <apply>
        <ci>normal-¯</ci>
        <ci>α</ci>
       </apply>
      </apply>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>k</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha_{\mathrm{\{per\ comparison\}}}=1-{\left(1-\bar{\alpha}\right)}^{\frac{1%
}{k}}
  </annotation>
 </semantics>
</math>

, which is known as the <a href="Šidák_correction" title="wikilink">Šidák correction</a>. Another procedure is the <a href="Holm–Bonferroni_method" title="wikilink">Holm–Bonferroni method</a>, which uniformly delivers more power than the simple Bonferroni correction, by testing only the most extreme p-value (

<math display="inline" id="Multiple_comparisons_problem:36">
 <semantics>
  <mrow>
   <mi>i</mi>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>i</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i=1
  </annotation>
 </semantics>
</math>

) against the strictest criterion, and the others (

<math display="inline" id="Multiple_comparisons_problem:37">
 <semantics>
  <mrow>
   <mi>i</mi>
   <mo>></mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>i</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i>1
  </annotation>
 </semantics>
</math>

) against progressively less strict criteria.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> 

<math display="inline" id="Multiple_comparisons_problem:38">
 <semantics>
  <mrow>
   <msub>
    <mi>α</mi>
    <mrow>
     <mo stretchy="false">{</mo>
     <mrow>
      <mpadded width="+5pt">
       <mi>per</mi>
      </mpadded>
      <mi>comparison</mi>
     </mrow>
     <mo stretchy="false">}</mo>
    </mrow>
   </msub>
   <mo>=</mo>
   <mrow>
    <mover accent="true">
     <mi>α</mi>
     <mo stretchy="false">¯</mo>
    </mover>
    <mo>/</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mrow>
       <mi>k</mi>
       <mo>-</mo>
       <mi>i</mi>
      </mrow>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>α</ci>
     <set>
      <apply>
       <times></times>
       <ci>per</ci>
       <ci>comparison</ci>
      </apply>
     </set>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <ci>normal-¯</ci>
      <ci>α</ci>
     </apply>
     <apply>
      <plus></plus>
      <apply>
       <minus></minus>
       <ci>k</ci>
       <ci>i</ci>
      </apply>
      <cn type="integer">1</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha_{\mathrm{\{per\ comparison\}}}=\bar{\alpha}/(k-i+1)
  </annotation>
 </semantics>
</math>


.</p>
<h2 id="methods">Methods</h2>

<p><strong>Multiple testing correction</strong> refers to re-calculating probabilities obtained from a statistical test which was repeated multiple times. In order to retain a prescribed family-wise error rate α in an analysis involving more than one comparison, the error rate for each comparison must be more stringent than α. Boole's inequality implies that if each of <em>k</em> tests is performed to have type I error rate α/<em>k</em>, the total error rate will not exceed α. This is called the <a href="Bonferroni_correction" title="wikilink">Bonferroni correction</a>, and is one of the most commonly used approaches for multiple comparisons.</p>

<p>In some situations, the Bonferroni correction is substantially conservative, i.e., the actual family-wise error rate is much less than the prescribed level α. This occurs when the test statistics are highly dependent (in the extreme case where the tests are perfectly dependent, the family-wise error rate with no multiple comparisons adjustment and the per-test error rates are identical). For example, in fMRI analysis,<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> tests are done on over 100,000 <a href="voxel" title="wikilink">voxels</a> in the brain. The Bonferroni method would require p-values to be smaller than .05/100000 to declare significance. Since adjacent voxels tend to be highly correlated, this threshold is generally too stringent.</p>

<p>Because simple techniques such as the Bonferroni method can be too conservative, there has been a great deal of attention paid to developing better techniques, such that the overall rate of false positives can be maintained without inflating the rate of false negatives unnecessarily. Such methods can be divided into general categories:</p>
<ul>
<li>Methods where total alpha can be proved to never exceed 0.05 (or some other chosen value) under any conditions. These methods provide "strong" control against Type I error, in all conditions including a partially correct null hypothesis.</li>
<li>Methods where total alpha can be proved not to exceed 0.05 except under certain defined conditions.</li>
<li>Methods which rely on an <a href="omnibus_test" title="wikilink">omnibus test</a> before proceeding to multiple comparisons. Typically these methods require a significant <a class="uri" href="ANOVA" title="wikilink">ANOVA</a>/<a href="Tukey's_range_test" title="wikilink">Tukey's range test</a> before proceeding to multiple comparisons. These methods have "weak" control of Type I error.</li>
<li>Empirical methods, which control the proportion of Type I errors adaptively, utilizing correlation and distribution characteristics of the observed data.</li>
</ul>

<p>The advent of computerized <a href="resampling_(statistics)" title="wikilink">resampling</a> methods, such as <a href="bootstrapping_(statistics)" title="wikilink">bootstrapping</a> and <a href="Monte_Carlo_simulation" title="wikilink">Monte Carlo simulations</a>, has given rise to many techniques in the latter category. In some cases where exhaustive permutation resampling is performed, these tests provide exact, strong control of Type I error rates; in other cases, such as bootstrap sampling, they provide only approximate control.</p>
<h2 id="post-hoc-testing-of-anovas">Post-hoc testing of ANOVAs</h2>

<p>Multiple comparison procedures are commonly used in an analysis of variance after obtaining a significant <a href="omnibus_test" title="wikilink">omnibus test</a> result, like the <a class="uri" href="ANOVA" title="wikilink">ANOVA</a> <a class="uri" href="F-test" title="wikilink">F-test</a>. The significant ANOVA result suggests rejecting the global null hypothesis H<sub>0</sub> that the means are the same across the groups being compared. Multiple comparison procedures are then used to determine which means differ. In a one-way ANOVA involving <em>K</em> group means, there are <em>K</em>(<em>K</em> − 1)/2 pairwise comparisons.</p>

<p>A number of methods have been proposed for this problem, some of which are:</p>
<dl>
<dt>Single-step procedures</dt>
</dl>
<ul>
<li><a href="Tukey's_range_test" title="wikilink">Tukey–Kramer method</a> (Tukey's HSD) (1951)</li>
<li><a href="Scheffé's_method" title="wikilink">Scheffé's method</a> (1953)</li>
<li><a href="Rodger's_method" title="wikilink">Rodger's method</a> (precludes type 1 error rate inflation, using a decision-based error rate)</li>
</ul>
<dl>
<dt>Multi-step procedures based on <a href="Studentized_range" title="wikilink">Studentized range</a> statistic</dt>
</dl>
<ul>
<li><a href="Duncan's_new_multiple_range_test" title="wikilink">Duncan's new multiple range test</a> (1955)</li>
<li>The <a href="Nemenyi_test" title="wikilink">Nemenyi test</a> is similar to <a href="Tukey's_range_test" title="wikilink">Tukey's range test</a> in ANOVA.</li>
</ul>
<ul>
<li>The <a href="Bonferroni–Dunn_test" title="wikilink">Bonferroni–Dunn test</a> allows comparisons, controlling the familywise error rate.</li>
<li><a href="Newman–Keuls_method" title="wikilink">Student Newman-Keuls</a> <a href="post-hoc_analysis" title="wikilink">post-hoc analysis</a></li>
<li><a href="Dunnett's_test" title="wikilink">Dunnett's test</a> (1955) for comparison of number of treatments to a single control group.</li>
</ul>

<p>Choosing the most appropriate multiple-comparison procedure for your specific situation is not easy. Many tests are available, and they differ in a number of ways.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>

<p>For example,if the variances of the groups being compared are similar, the Tukey–Kramer method is generally viewed as performing optimally or near-optimally in a broad variety of circumstances.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> The situation where the variance of the groups being compared differ is more complex, and different methods perform well in different circumstances.</p>

<p>The <a href="Kruskal–Wallis_test" title="wikilink">Kruskal–Wallis test</a> is the <a class="uri" href="non-parametric" title="wikilink">non-parametric</a> alternative to ANOVA. Multiple comparisons can be done using pairwise comparisons (for example using <a href="Wilcoxon_rank_sum" title="wikilink">Wilcoxon rank sum</a> tests) and using a correction to determine if the post-hoc tests are significant (for example a <a href="Bonferroni_correction" title="wikilink">Bonferroni correction</a>).</p>
<h2 id="large-scale-multiple-testing">Large-scale multiple testing</h2>

<p>Traditional methods for multiple comparisons adjustments focus on correcting for modest numbers of comparisons, often in an <a href="analysis_of_variance" title="wikilink">analysis of variance</a>. A different set of techniques have been developed for "large-scale multiple testing", in which thousands or even greater numbers of tests are performed. For example, in <a class="uri" href="genomics" title="wikilink">genomics</a>, when using technologies such as <a href="DNA_microarray" title="wikilink">microarrays</a>, expression levels of tens of thousands of genes can be measured, and genotypes for millions of genetic markers can be measured. Particularly in the field of <a href="genetic_association" title="wikilink">genetic association</a> studies, there has been a serious problem with non-replication — a result being strongly statistically significant in one study but failing to be replicated in a follow-up study. Such non-replication can have many causes, but it is widely considered that failure to fully account for the consequences of making multiple comparisons is one of the causes.</p>

<p>In different branches of science, multiple testing is handled in different ways. It has been argued that if statistical tests are only performed when there is a strong basis for expecting the result to be true, multiple comparisons adjustments are not necessary.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> It has also been argued that use of multiple testing corrections is an inefficient way to perform <a href="empirical_research" title="wikilink">empirical research</a>, since multiple testing adjustments control false positives at the potential expense of many more <a href="Type_I_and_type_II_errors" title="wikilink">false negatives</a>. On the other hand, it has been argued that advances in <a class="uri" href="measurement" title="wikilink">measurement</a> and <a href="information_technology" title="wikilink">information technology</a> have made it far easier to generate large datasets for <a href="exploratory_data_analysis" title="wikilink">exploratory analysis</a>, often leading to the testing of large numbers of hypotheses with no prior basis for expecting many of the hypotheses to be true. In this situation, very high <a href="false_positive_rate" title="wikilink">false positive rates</a> are expected unless multiple comparisons adjustments are made.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>

<p>For large-scale testing problems where the goal is to provide definitive results, the <a href="familywise_error_rate" title="wikilink">familywise error rate</a> remains the most accepted parameter for ascribing significance levels to statistical tests. Alternatively, if a study is viewed as exploratory, or if significant results can be easily re-tested in an independent study, control of the <a href="false_discovery_rate" title="wikilink">false discovery rate</a> (FDR)<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a><a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> is often preferred. The FDR, defined as the expected proportion of false positives among all significant tests, allows researchers to identify a set of "candidate positives", of which a high proportion are likely to be true. The false positives within the candidate set can then be identified in a follow-up study.</p>
<h3 id="assessing-whether-any-alternative-hypotheses-are-true">Assessing whether any alternative hypotheses are true</h3>
<figure><b>(Figure)</b>
<figcaption>A <a href="Q-Q_plot" title="wikilink">normal quantile plot</a> for a simulated set of test statistics that have been standardized to be <a href="standard_score" title="wikilink">Z-scores</a> under the null hypothesis. The departure of the upper tail of the distribution from the expected trend along the diagonal is due to the presence of substantially more large test statistic values than would be expected if all null hypotheses were true. The red point corresponds to the fourth largest observed test statistic, which is 3.13, versus an expected value of 2.06. The blue point corresponds to the fifth smallest test statistic, which is -1.75, versus an expected value of -1.96. The graph suggests that it is unlikely that all the null hypotheses are true, and that most or all instances of a true alternative hypothesis result from deviations in the positive direction.</figcaption>
</figure>

<p>A basic question faced at the outset of analyzing a large set of testing results is whether there is evidence that any of the alternative hypotheses are true. One simple meta-test that can be applied when it is assumed that the tests are independent of each other is to use the <a href="Poisson_distribution" title="wikilink">Poisson distribution</a> as a model for the number of significant results at a given level α that would be found when all null hypotheses are true. If the observed number of positives is substantially greater than what should be expected, this suggests that there are likely to be some true positives among the significant results. For example, if 1000 independent tests are performed, each at level α = 0.05, we expect 50 significant tests to occur when all null hypotheses are true. Based on the Poisson distribution with mean 50, the probability of observing more than 61 significant tests is less than 0.05, so if we observe more than 61 significant results, it is very likely that some of them correspond to situations where the alternative hypothesis holds. A drawback of this approach is that it over-states the evidence that some of the alternative hypotheses are true when the <a href="test_statistic" title="wikilink">test statistics</a> are positively correlated, which commonly occurs in practice. </p>

<p>Another common approach that can be used in situations where the <a href="test_statistics" title="wikilink">test statistics</a> can be standardized to <a href="standard_score" title="wikilink">Z-scores</a> is to make a <a href="Q-Q_plot" title="wikilink">normal quantile plot</a> of the test statistics. If the observed quantiles are markedly more <a href="statistical_dispersion" title="wikilink">dispersed</a> than the normal quantiles, this suggests that some of the significant results may be true positives.</p>
<h2 id="see-also">See also</h2>
<dl>
<dt>Key concepts</dt>
</dl>
<ul>
<li><a href="Familywise_error_rate" title="wikilink">Familywise error rate</a></li>
<li><a href="False_positive_rate" title="wikilink">False positive rate</a></li>
<li><a href="False_discovery_rate" title="wikilink">False discovery rate</a> (FDR)</li>
<li><a href="False_coverage_rate" title="wikilink">False coverage rate</a> (FCR)</li>
<li><a href="Interval_estimation" title="wikilink">Interval estimation</a></li>
<li><a href="Post-hoc_analysis" title="wikilink">Post-hoc analysis</a></li>
<li><a href="Experimentwise_error_rate" title="wikilink">Experimentwise error rate</a></li>
</ul>
<dl>
<dt>General methods of alpha adjustment for multiple comparisons</dt>
</dl>
<ul>
<li><a href="Closed_testing_procedure" title="wikilink">Closed testing procedure</a></li>
<li><a href="Bonferroni_correction" title="wikilink">Bonferroni correction</a></li>
<li>Boole–<a href="Bonferroni_bound" title="wikilink">Bonferroni bound</a></li>
<li><a href="Holm–Bonferroni_method" title="wikilink">Holm–Bonferroni method</a></li>
</ul>
<dl>
<dt>Related concepts</dt>
</dl>
<ul>
<li><a href="Testing_hypotheses_suggested_by_the_data" title="wikilink">Testing hypotheses suggested by the data</a></li>
<li><a href="Texas_sharpshooter_fallacy" title="wikilink">Texas sharpshooter fallacy</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="further-reading">Further reading</h2>
<ul>
<li>F. Betz, T. Hothorn, P. Westfall (2010), Multiple Comparisons Using R, CRC Press</li>
<li>S. Dudoit and M. J. van der Laan (2008), Multiple Testing Procedures with Application to Genomics, Springer</li>
<li>B. Phipson and G. K. Smyth (2010), Permutation P-values Should Never Be Zero: Calculating Exact P-values when Permutations are Randomly Drawn, Statistical Applications in Genetics and Molecular Biology Vol.. 9 Iss. 1, Article 39, </li>
<li>P. H. Westfall and S. S. Young (1993), Resampling-based Multiple Testing: Examples and Methods for p-Value Adjustment, Wiley</li>
<li>P. Westfall, R. Tobias, R. Wolfinger (2011) Multiple comparisons and multiple testing using SAS, 2nd edn, SAS Institute</li>
</ul>

<p>"</p>

<p><a href="Category:Hypothesis_testing" title="wikilink">Category:Hypothesis testing</a> <a href="Category:Multiple_comparisons" title="wikilink"> </a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7">Howell (2002, Chapter 12: Multiple comparisons among treatment means)<a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"><a href="#fnref11">↩</a></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
</ol>
</section>
</body>
</html>
