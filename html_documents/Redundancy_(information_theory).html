<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1822">Redundancy (information theory)</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Redundancy (information theory)</h1>
<hr/>

<p><strong>Redundancy</strong> in <a href="information_theory" title="wikilink">information theory</a> is the number of bits used to transmit a message minus the number of bits of actual information in the message. Informally, it is the amount of wasted "space" used to transmit certain data. <a href="Data_compression" title="wikilink">Data compression</a> is a way to reduce or eliminate unwanted redundancy, while <a href="checksum" title="wikilink">checksums</a> are a way of adding desired redundancy for purposes of <a href="error_detection" title="wikilink">error detection</a> when communicating over a noisy channel of limited <a href="channel_capacity" title="wikilink">capacity</a>.</p>
<h2 id="quantitative-definition">Quantitative definition</h2>

<p>In describing the redundancy of raw data, the <strong><a href="Entropy_rate" title="wikilink">rate</a></strong> of a source of information is the average <a href="Information_entropy" title="wikilink">entropy</a> per symbol. For memoryless sources, this is merely the entropy of each symbol, while, in the most general case of a <a href="stochastic_process" title="wikilink">stochastic process</a>, it is</p>

<p>

<math display="block" id="Redundancy_(information_theory):0">
 <semantics>
  <mrow>
   <mrow>
    <mi>r</mi>
    <mo>=</mo>
    <mrow>
     <munder>
      <mo movablelimits="false">lim</mo>
      <mrow>
       <mi>n</mi>
       <mo>‚Üí</mo>
       <mi mathvariant="normal">‚àû</mi>
      </mrow>
     </munder>
     <mrow>
      <mfrac>
       <mn>1</mn>
       <mi>n</mi>
      </mfrac>
      <mi>H</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>M</mi>
        <mn>1</mn>
       </msub>
       <mo>,</mo>
       <msub>
        <mi>M</mi>
        <mn>2</mn>
       </msub>
       <mo>,</mo>
       <mrow>
        <mi mathvariant="normal">‚Ä¶</mi>
        <msub>
         <mi>M</mi>
         <mi>n</mi>
        </msub>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>r</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <limit></limit>
      <apply>
       <ci>normal-‚Üí</ci>
       <ci>n</ci>
       <infinity></infinity>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>n</ci>
      </apply>
      <ci>H</ci>
      <vector>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>M</ci>
        <cn type="integer">1</cn>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>M</ci>
        <cn type="integer">2</cn>
       </apply>
       <apply>
        <times></times>
        <ci>normal-‚Ä¶</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>M</ci>
         <ci>n</ci>
        </apply>
       </apply>
      </vector>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r=\lim_{n\to\infty}\frac{1}{n}H(M_{1},M_{2},\dots M_{n}),
  </annotation>
 </semantics>
</math>

</p>

<p>the limit, as <em>n</em> goes to infinity, of the <a href="joint_entropy" title="wikilink">joint entropy</a> of the first <em>n</em> symbols divided by <em>n</em>. It is common in information theory to speak of the "rate" or "<a href="Information_entropy" title="wikilink">entropy</a>" of a language. This is appropriate, for example, when the source of information is English prose. The rate of a memoryless source is simply 

<math display="inline" id="Redundancy_(information_theory):1">
 <semantics>
  <mrow>
   <mi>H</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>M</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>H</ci>
    <ci>M</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H(M)
  </annotation>
 </semantics>
</math>

, since by definition there is no interdependence of the successive messages of a memoryless source.</p>

<p>The <strong>absolute rate</strong> of a language or source is simply</p>

<p>

<math display="block" id="Redundancy_(information_theory):2">
 <semantics>
  <mrow>
   <mrow>
    <mi>R</mi>
    <mo>=</mo>
    <mrow>
     <mi>log</mi>
     <mrow>
      <mo stretchy="false">|</mo>
      <mi>ùïÑ</mi>
      <mo stretchy="false">|</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>R</ci>
    <apply>
     <log></log>
     <apply>
      <abs></abs>
      <ci>ùïÑ</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R=\log|\mathbb{M}|,\,
  </annotation>
 </semantics>
</math>

</p>

<p>the <a class="uri" href="logarithm" title="wikilink">logarithm</a> of the <a class="uri" href="cardinality" title="wikilink">cardinality</a> of the message space, or alphabet. (This formula is sometimes called the <a href="Hartley_function" title="wikilink">Hartley function</a>.) This is the maximum possible rate of information that can be transmitted with that alphabet. (The logarithm should be taken to a base appropriate for the unit of measurement in use.) The absolute rate is equal to the actual rate if the source is memoryless and has a <a href="Uniform_distribution_(discrete)" title="wikilink">uniform distribution</a>.</p>

<p>The <strong>absolute redundancy</strong> can then be defined as</p>

<p>

<math display="block" id="Redundancy_(information_theory):3">
 <semantics>
  <mrow>
   <mrow>
    <mi>D</mi>
    <mo>=</mo>
    <mrow>
     <mi>R</mi>
     <mo>-</mo>
     <mi>r</mi>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>D</ci>
    <apply>
     <minus></minus>
     <ci>R</ci>
     <ci>r</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D=R-r,\,
  </annotation>
 </semantics>
</math>

</p>

<p>the difference between the absolute rate and the rate.</p>

<p>The quantity 

<math display="inline" id="Redundancy_(information_theory):4">
 <semantics>
  <mfrac>
   <mi>D</mi>
   <mi>R</mi>
  </mfrac>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <ci>D</ci>
    <ci>R</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{D}{R}
  </annotation>
 </semantics>
</math>

 is called the <strong>relative redundancy</strong> and gives the maximum possible <a href="data_compression_ratio" title="wikilink">data compression ratio</a>, when expressed as the percentage by which a file size can be decreased. (When expressed as a ratio of original file size to compressed file size, the quantity 

<math display="inline" id="Redundancy_(information_theory):5">
 <semantics>
  <mrow>
   <mi>R</mi>
   <mo>:</mo>
   <mi>r</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-:</ci>
    <ci>R</ci>
    <ci>r</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   R:r
  </annotation>
 </semantics>
</math>

 gives the maximum compression ratio that can be achieved.) Complementary to the concept of relative redundancy is <strong>efficiency</strong>, defined as 

<math display="inline" id="Redundancy_(information_theory):6">
 <semantics>
  <mrow>
   <mfrac>
    <mi>r</mi>
    <mi>R</mi>
   </mfrac>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <ci>r</ci>
    <ci>R</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{r}{R},
  </annotation>
 </semantics>
</math>

 so that 

<math display="inline" id="Redundancy_(information_theory):7">
 <semantics>
  <mrow>
   <mrow>
    <mfrac>
     <mi>r</mi>
     <mi>R</mi>
    </mfrac>
    <mo>+</mo>
    <mfrac>
     <mi>D</mi>
     <mi>R</mi>
    </mfrac>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <plus></plus>
     <apply>
      <divide></divide>
      <ci>r</ci>
      <ci>R</ci>
     </apply>
     <apply>
      <divide></divide>
      <ci>D</ci>
      <ci>R</ci>
     </apply>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \frac{r}{R}+\frac{D}{R}=1
  </annotation>
 </semantics>
</math>

. A memoryless source with a uniform distribution has zero redundancy (and thus 100% efficiency), and cannot be compressed.</p>
<h2 id="other-notions-of-redundancy">Other notions of redundancy</h2>

<p>A measure of <em>redundancy</em> between two variables is the <a href="mutual_information" title="wikilink">mutual information</a> or a normalized variant. A measure of redundancy among many variables is given by the <a href="total_correlation" title="wikilink">total correlation</a>.</p>

<p>Redundancy of compressed data refers to the difference between the <a href="expected_value" title="wikilink">expected</a> compressed data length of 

<math display="inline" id="Redundancy_(information_theory):8">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 messages 

<math display="inline" id="Redundancy_(information_theory):9">
 <semantics>
  <mrow>
   <mi>L</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msup>
     <mi>M</mi>
     <mi>n</mi>
    </msup>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>L</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>M</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L(M^{n})\,\!
  </annotation>
 </semantics>
</math>

 (or expected data rate 

<math display="inline" id="Redundancy_(information_theory):10">
 <semantics>
  <mrow>
   <mrow>
    <mi>L</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msup>
      <mi>M</mi>
      <mi>n</mi>
     </msup>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>/</mo>
   <mi>n</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <times></times>
     <ci>L</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>M</ci>
      <ci>n</ci>
     </apply>
    </apply>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L(M^{n})/n\,\!
  </annotation>
 </semantics>
</math>

) and the entropy 

<math display="inline" id="Redundancy_(information_theory):11">
 <semantics>
  <mrow>
   <mi>n</mi>
   <mi>r</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>n</ci>
    <ci>r</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   nr\,\!
  </annotation>
 </semantics>
</math>

 (or entropy rate 

<math display="inline" id="Redundancy_(information_theory):12">
 <semantics>
  <mi>r</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>r</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r\,\!
  </annotation>
 </semantics>
</math>

). (Here we assume the data is <a href="ergodicity" title="wikilink">ergodic</a> and <a href="Stationary_process" title="wikilink">stationary</a>, e.g., a memoryless source.) Although the rate difference 

<math display="inline" id="Redundancy_(information_theory):13">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>L</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msup>
       <mi>M</mi>
       <mi>n</mi>
      </msup>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>/</mo>
    <mi>n</mi>
   </mrow>
   <mo>-</mo>
   <mi>r</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <apply>
     <divide></divide>
     <apply>
      <times></times>
      <ci>L</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>M</ci>
       <ci>n</ci>
      </apply>
     </apply>
     <ci>n</ci>
    </apply>
    <ci>r</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L(M^{n})/n-r\,\!
  </annotation>
 </semantics>
</math>

 can be arbitrarily small as 

<math display="inline" id="Redundancy_(information_theory):14">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n\,\!
  </annotation>
 </semantics>
</math>

 increased, the actual difference 

<math display="inline" id="Redundancy_(information_theory):15">
 <semantics>
  <mrow>
   <mrow>
    <mi>L</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msup>
      <mi>M</mi>
      <mi>n</mi>
     </msup>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>-</mo>
   <mrow>
    <mi>n</mi>
    <mi>r</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <apply>
     <times></times>
     <ci>L</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>M</ci>
      <ci>n</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>n</ci>
     <ci>r</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L(M^{n})-nr\,\!
  </annotation>
 </semantics>
</math>

, cannot, although it can be theoretically upper-bounded by 1 in the case of finite-entropy memoryless sources.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Data_compression" title="wikilink">Data compression</a></li>
<li><a href="Hartley_function" title="wikilink">Hartley function</a></li>
<li><a class="uri" href="Negentropy" title="wikilink">Negentropy</a></li>
<li><a href="Source_coding_theorem" title="wikilink">Source coding theorem</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li></li>
<li></li>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Information_theory" title="wikilink">Category:Information theory</a></p>
</body>
</html>
