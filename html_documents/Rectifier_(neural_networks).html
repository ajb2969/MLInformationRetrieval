<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="721">Rectifier (neural networks)</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Rectifier (neural networks)</h1>
<hr/>

<p> In the context of <a href="artificial_neural_network" title="wikilink">artificial neural networks</a>, the <strong>rectifier</strong> is an <a href="activation_function" title="wikilink">activation function</a> defined as</p>

<p>
<math display="block" id="Rectifier_(neural_networks):0">
<semantics>
<mrow>
<mrow>
<mi>f</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mi>max</mi>
<mrow>
<mo stretchy="false">(</mo>
<mn>0</mn>
<mo>,</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<times></times>
<ci>f</ci>
<ci>x</ci>
</apply>
<apply>
<max></max>
<cn type="integer">0</cn>
<ci>x</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   f(x)=\max(0,x)
  </annotation>
</semantics>
</math>
</p>

<p>where <em>x</em> is the input to a neuron. This activation function has been argued to be more biologically plausible<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> than the widely used <a href="Logistic_function" title="wikilink">logistic sigmoid</a> (which is inspired by <a href="probability_theory" title="wikilink">probability theory</a>; see <a href="logistic_regression" title="wikilink">logistic regression</a>) and its more practical<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> counterpart, the <a href="Hyperbolic_function" title="wikilink">hyperbolic tangent</a>. The rectifier is, , the most popular activation function for <a href="deep_learning" title="wikilink">deep neural networks</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>A unit employing the rectifier is also called a <strong>rectified linear unit</strong> (<strong>ReLU</strong>).<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>A smooth approximation to the rectifier is the <a href="analytic_function" title="wikilink">analytic function</a></p>

<p>
<math display="block" id="Rectifier_(neural_networks):1">
<semantics>
<mrow>
<mrow>
<mi>f</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mi>ln</mi>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mi>x</mi>
</msup>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<times></times>
<ci>f</ci>
<ci>x</ci>
</apply>
<apply>
<ln></ln>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<ci>x</ci>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   f(x)=\ln(1+e^{x})
  </annotation>
</semantics>
</math>
</p>

<p>which is called the <strong>softplus</strong> function.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> The derivative of softplus is 

<math display="inline" id="Rectifier_(neural_networks):2">
<semantics>
<mrow>
<mrow>
<msup>
<mi>f</mi>
<mo>‚Ä≤</mo>
</msup>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<msup>
<mi>e</mi>
<mi>x</mi>
</msup>
<mo>/</mo>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<msup>
<mi>e</mi>
<mi>x</mi>
</msup>
<mo>+</mo>
<mn>1</mn>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mn>1</mn>
<mo>/</mo>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mn>1</mn>
<mo>+</mo>
<msup>
<mi>e</mi>
<mrow>
<mo>-</mo>
<mi>x</mi>
</mrow>
</msup>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<and></and>
<apply>
<eq></eq>
<apply>
<times></times>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>f</ci>
<ci>normal-‚Ä≤</ci>
</apply>
<ci>x</ci>
</apply>
<apply>
<divide></divide>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<ci>x</ci>
</apply>
<apply>
<plus></plus>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<ci>x</ci>
</apply>
<cn type="integer">1</cn>
</apply>
</apply>
</apply>
<apply>
<eq></eq>
<share href="#.cmml">
</share>
<apply>
<divide></divide>
<cn type="integer">1</cn>
<apply>
<plus></plus>
<cn type="integer">1</cn>
<apply>
<csymbol cd="ambiguous">superscript</csymbol>
<ci>e</ci>
<apply>
<minus></minus>
<ci>x</ci>
</apply>
</apply>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   f^{\prime}(x)=e^{x}/(e^{x}+1)=1/(1+e^{-x})
  </annotation>
</semantics>
</math>

, i.e. the logistic function.</p>

<p>Rectified linear units find applications in <a href="computer_vision" title="wikilink">computer vision</a> using <a href="Deep_learning" title="wikilink">deep neural nets</a>.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h2 id="variants">Variants</h2>
<h3 id="noisy-relus">Noisy ReLUs</h3>

<p>Rectified linear units can be extended to include <a href="Gaussian_noise" title="wikilink">Gaussian noise</a>, making them noisy ReLUs, giving<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>

<p>
<math display="block" id="Rectifier_(neural_networks):3">
<semantics>
<mrow>
<mrow>
<mi>f</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mi>max</mi>
<mrow>
<mo stretchy="false">(</mo>
<mn>0</mn>
<mo>,</mo>
<mrow>
<mi>x</mi>
<mo>+</mo>
<mrow>
<mi class="ltx_font_mathcaligraphic">ùí©</mi>
<mrow>
<mo stretchy="false">(</mo>
<mn>0</mn>
<mo>,</mo>
<mrow>
<mi>œÉ</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<times></times>
<ci>f</ci>
<ci>x</ci>
</apply>
<apply>
<max></max>
<cn type="integer">0</cn>
<apply>
<plus></plus>
<ci>x</ci>
<apply>
<times></times>
<ci>ùí©</ci>
<interval closure="open">
<cn type="integer">0</cn>
<apply>
<times></times>
<ci>œÉ</ci>
<ci>x</ci>
</apply>
</interval>
</apply>
</apply>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   f(x)=\max(0,x+\mathcal{N}(0,\sigma(x)))
  </annotation>
</semantics>
</math>
</p>

<p>Noisy ReLUs have been used with some success in <a href="restricted_Boltzmann_machine" title="wikilink">restricted Boltzmann machines</a> for computer vision tasks.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>
<h3 id="leaky-relus">Leaky ReLUs</h3>

<p>Leaky ReLUs allow a small, non-zero gradient when the unit is not active.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>

<p>
<math display="block" id="Rectifier_(neural_networks):4">
<semantics>
<mrow>
<mrow>
<mi>f</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow>
<mo>{</mo>
<mtable displaystyle="true">
<mtr>
<mtd columnalign="left">
<mi>x</mi>
</mtd>
<mtd columnalign="left">
<mrow>
<mrow>
<mtext>if</mtext>
<mi>x</mi>
</mrow>
<mo>&gt;</mo>
<mn>0</mn>
</mrow>
</mtd>
</mtr>
<mtr>
<mtd columnalign="left">
<mrow>
<mn>0.01</mn>
<mi>x</mi>
</mrow>
</mtd>
<mtd columnalign="left">
<mtext>otherwise</mtext>
</mtd>
</mtr>
</mtable>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<times></times>
<ci>f</ci>
<ci>x</ci>
</apply>
<apply>
<csymbol cd="latexml">cases</csymbol>
<ci>x</ci>
<apply>
<gt></gt>
<apply>
<times></times>
<mtext>if</mtext>
<ci>x</ci>
</apply>
<cn type="integer">0</cn>
</apply>
<apply>
<times></times>
<cn type="float">0.01</cn>
<ci>x</ci>
</apply>
<mtext>otherwise</mtext>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   f(x)=\begin{cases}x&amp;\mbox{if }x&gt;0\\
0.01x&amp;\mbox{otherwise}\end{cases}
  </annotation>
</semantics>
</math>
</p>
<h2 id="advantages">Advantages</h2>
<ul>
<li>Biological plausibility: One-sided, compared to the antisymmetry of <a class="uri" href="tanh" title="wikilink">tanh</a>.</li>
<li>Sparse activation: For example, in a randomly initialized networks, only about 50% of hidden units are activated (having a non-zero output).</li>
<li>Efficient gradient propagation: No <a href="vanishing_gradient_problem" title="wikilink">vanishing gradient problem</a> or exploding effect.</li>
<li>Efficient computation: Only comparison, addition and multiplication.</li>
</ul>

<p>For the first time in 2011,<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units, compared to <a href="sigmoid_function" title="wikilink">sigmoid function</a> or similar activation functions, allow for faster and effective training of deep neural architectures on large and complex datasets.</p>
<h2 id="potential-problems">Potential problems</h2>
<ul>
<li>Non-differentiable at zero: however it is differentiable at any point arbitrarily close to 0.</li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Softmax_function" title="wikilink">Softmax function</a></li>
<li><a href="Sigmoid_function" title="wikilink">Sigmoid function</a></li>
<li><a href="Tobit_model" title="wikilink">Tobit model</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">‚Ü©</a></li>
<li id="fn2"><a href="#fnref2">‚Ü©</a></li>
<li id="fn3"><a href="#fnref3">‚Ü©</a></li>
<li id="fn4"></li>
<li id="fn5">C. Dugas, Y. Bengio, F. B√©lisle, C. Nadeau, R. Garcia, NIPS'2000, (2001),<a href="http://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf">Incorporating Second-Order Functional Knowledge for Better Option Pricing</a><a href="#fnref5">‚Ü©</a></li>
<li id="fn6"></li>
<li id="fn7"><a href="#fnref7">‚Ü©</a></li>
<li id="fn8"></li>
<li id="fn9">Andrew L. Maas, Awni Y. Hannun, Andrew Y. Ng (2014). <a href="http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf">Rectifier Nonlinearities Improve Neural Network Acoustic Models</a><a href="#fnref9">‚Ü©</a></li>
<li id="fn10"></li>
</ol>
</section>
</body>
</html>
