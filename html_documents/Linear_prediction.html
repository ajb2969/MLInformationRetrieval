<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="625">Linear prediction</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Linear prediction</h1>
<hr/>

<p><strong>Linear prediction</strong> is a mathematical operation where future values of a <a href="discrete_time" title="wikilink">discrete-time</a> <a href="Signal_processing" title="wikilink">signal</a> are estimated as a <a href="linear_transformation" title="wikilink">linear function</a> of previous samples.</p>

<p>In <a href="digital_signal_processing" title="wikilink">digital signal processing</a>, linear prediction is often called <a href="linear_predictive_coding" title="wikilink">linear predictive coding</a> (LPC) and can thus be viewed as a subset of <a href="filter_theory" title="wikilink">filter theory</a>. In <a href="system_analysis" title="wikilink">system analysis</a> (a subfield of <a class="uri" href="mathematics" title="wikilink">mathematics</a>), linear prediction can be viewed as a part of <a href="mathematical_model" title="wikilink">mathematical modelling</a> or <a href="Optimization_(mathematics)" title="wikilink">optimization</a>.</p>
<h2 id="the-prediction-model">The prediction model</h2>

<p>The most common representation is</p>

<p>

<math display="block" id="Linear_prediction:0">
 <semantics>
  <mrow>
   <mrow>
    <mover accent="true">
     <mi>x</mi>
     <mo>^</mo>
    </mover>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>n</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <munderover>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>p</mi>
    </munderover>
    <mrow>
     <msub>
      <mi>a</mi>
      <mi>i</mi>
     </msub>
     <mi>x</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mi>n</mi>
       <mo>-</mo>
       <mi>i</mi>
      </mrow>
      <mo rspace="4.2pt" stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <ci>normal-^</ci>
      <ci>x</ci>
     </apply>
     <ci>n</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>p</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>a</ci>
       <ci>i</ci>
      </apply>
      <ci>x</ci>
      <apply>
       <minus></minus>
       <ci>n</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \widehat{x}(n)=\sum_{i=1}^{p}a_{i}x(n-i)\,
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Linear_prediction:1">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>x</mi>
    <mo>^</mo>
   </mover>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>n</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <ci>normal-^</ci>
     <ci>x</ci>
    </apply>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \widehat{x}(n)
  </annotation>
 </semantics>
</math>

 is the predicted signal value, 

<math display="inline" id="Linear_prediction:2">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mi>n</mi>
     <mo>-</mo>
     <mi>i</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>x</ci>
    <apply>
     <minus></minus>
     <ci>n</ci>
     <ci>i</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x(n-i)
  </annotation>
 </semantics>
</math>

 the previous observed values, and 

<math display="inline" id="Linear_prediction:3">
 <semantics>
  <msub>
   <mi>a</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>a</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a_{i}
  </annotation>
 </semantics>
</math>

 the predictor coefficients. The error generated by this estimate is</p>

<p>

<math display="block" id="Linear_prediction:4">
 <semantics>
  <mrow>
   <mrow>
    <mi>e</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>n</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi>x</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>n</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>-</mo>
    <mrow>
     <mover accent="true">
      <mi>x</mi>
      <mo>^</mo>
     </mover>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>n</mi>
      <mo rspace="4.2pt" stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>e</ci>
     <ci>n</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <ci>x</ci>
      <ci>n</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <ci>normal-^</ci>
       <ci>x</ci>
      </apply>
      <ci>n</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   e(n)=x(n)-\widehat{x}(n)\,
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Linear_prediction:5">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>n</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>x</ci>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x(n)
  </annotation>
 </semantics>
</math>

 is the true signal value.</p>

<p>These equations are valid for all types of (one-dimensional) linear prediction. The differences are found in the way the parameters 

<math display="inline" id="Linear_prediction:6">
 <semantics>
  <msub>
   <mi>a</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>a</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a_{i}
  </annotation>
 </semantics>
</math>

 are chosen.</p>

<p>For multi-dimensional signals the error metric is often defined as</p>

<p>

<math display="block" id="Linear_prediction:7">
 <semantics>
  <mrow>
   <mrow>
    <mi>e</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>n</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>∥</mo>
    <mrow>
     <mrow>
      <mi>x</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>n</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>-</mo>
     <mrow>
      <mover accent="true">
       <mi>x</mi>
       <mo>^</mo>
      </mover>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>n</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mo rspace="4.2pt">∥</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>e</ci>
     <ci>n</ci>
    </apply>
    <apply>
     <csymbol cd="latexml">norm</csymbol>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <ci>x</ci>
       <ci>n</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <ci>normal-^</ci>
        <ci>x</ci>
       </apply>
       <ci>n</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   e(n)=\|x(n)-\widehat{x}(n)\|\,
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Linear_prediction:8">
 <semantics>
  <mrow>
   <mo>∥</mo>
   <mo>⋅</mo>
   <mo>∥</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="latexml">parallel-to</csymbol>
    <ci>normal-⋅</ci>
    <csymbol cd="latexml">parallel-to</csymbol>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \|\cdot\|
  </annotation>
 </semantics>
</math>

 is a suitable chosen vector <a href="norm_(mathematics)" title="wikilink">norm</a>. Predictions such as 

<math display="inline" id="Linear_prediction:9">
 <semantics>
  <mrow>
   <mover accent="true">
    <mi>x</mi>
    <mo>^</mo>
   </mover>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>n</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <ci>normal-^</ci>
     <ci>x</ci>
    </apply>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \widehat{x}(n)
  </annotation>
 </semantics>
</math>

 are routinely used within <a href="Kalman_filter" title="wikilink">Kalman filters</a> and smoothers <a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> to estimate current and past signal values, respectively.</p>
<h3 id="estimating-the-parameters">Estimating the parameters</h3>

<p>The most common choice in optimization of parameters 

<math display="inline" id="Linear_prediction:10">
 <semantics>
  <msub>
   <mi>a</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>a</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a_{i}
  </annotation>
 </semantics>
</math>

 is the <a href="root_mean_square" title="wikilink">root mean square</a> criterion which is also called the <a class="uri" href="autocorrelation" title="wikilink">autocorrelation</a> criterion. In this method we minimize the expected value of the squared error E[e<sup>2</sup>(n)], which yields the equation</p>

<p>

<math display="block" id="Linear_prediction:11">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>p</mi>
     </munderover>
     <mrow>
      <msub>
       <mi>a</mi>
       <mi>i</mi>
      </msub>
      <mi>R</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>j</mi>
        <mo>-</mo>
        <mi>i</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mo>-</mo>
     <mrow>
      <mi>R</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>j</mi>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>p</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>a</ci>
       <ci>i</ci>
      </apply>
      <ci>R</ci>
      <apply>
       <minus></minus>
       <ci>j</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <ci>R</ci>
      <ci>j</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{i=1}^{p}a_{i}R(j-i)=-R(j),
  </annotation>
 </semantics>
</math>

</p>

<p>for 1 ≤ <em>j</em> ≤ <em>p</em>, where <em>R</em> is the <a class="uri" href="autocorrelation" title="wikilink">autocorrelation</a> of signal <em>x</em><sub><em>n</em></sub>, defined as</p>

<p>

<math display="block" id="Linear_prediction:12">
 <semantics>
  <mrow>
   <mrow>
    <mpadded lspace="5pt" width="+5pt">
     <mi>R</mi>
    </mpadded>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>i</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>E</mi>
    <mrow>
     <mo stretchy="false">{</mo>
     <mrow>
      <mi>x</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>n</mi>
       <mo stretchy="false">)</mo>
      </mrow>
      <mi>x</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>n</mi>
        <mo>-</mo>
        <mi>i</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo rspace="4.2pt" stretchy="false">}</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>R</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <times></times>
     <ci>E</ci>
     <set>
      <apply>
       <times></times>
       <ci>x</ci>
       <ci>n</ci>
       <ci>x</ci>
       <apply>
        <minus></minus>
        <ci>n</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </set>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ R(i)=E\{x(n)x(n-i)\}\,
  </annotation>
 </semantics>
</math>

,</p>

<p>and <em>E</em> is the <a href="expected_value" title="wikilink">expected value</a>. In the multi-dimensional case this corresponds to minimizing the <a href="Lp_space" title="wikilink">L<sub>2</sub> norm</a>.</p>

<p>The above equations are called the <a href="normal_equations" title="wikilink">normal equations</a> or <a href="Autoregressive_model#Yule-Walker_equations" title="wikilink">Yule-Walker equations</a>. In matrix form the equations can be equivalently written as</p>

<p>

<math display="block" id="Linear_prediction:13">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>R</mi>
     <mi>a</mi>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mo>-</mo>
     <mi>r</mi>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>R</ci>
     <ci>a</ci>
    </apply>
    <apply>
     <minus></minus>
     <ci>r</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Ra=-r,\,
  </annotation>
 </semantics>
</math>

</p>

<p>where the autocorrelation matrix <em>R</em> is a symmetric, p×p <a href="Toeplitz_matrix" title="wikilink">Toeplitz matrix</a> with elements <em>r</em><sub><em>i</em>,<em>j</em></sub> = <em>R</em>(<em>i</em> − <em>j</em>), 0≤i,j<p, <em="">r</p,></p></body></html>="" <em>r</em><sub autocorrelation="" is="" the="" vector=""><em>j</em></sub>

<p>= <em>R</em>(<em>j</em>), 0<j≤p, <em="">a</j≤p,></p>="" :<math and="" another="" approach="" defined="" errors="" form="" general="" in="" is="" minimize="" more="" of="" parameter="" squares="" sum="" the="" to="" vector="" vector.="">e(n) = x(n) - \widehat{x}(n) = x(n) - \sum_{i=1}^p a_i x(n-i) = - \sum_{i=0}^p a_i x(n-i)</math>

<p>where the optimisation problem searching over all 

<math display="inline" id="Linear_prediction:14">
 <semantics>
  <msub>
   <mi>a</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>a</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a_{i}
  </annotation>
 </semantics>
</math>

 must now be constrained with 

<math display="inline" id="Linear_prediction:15">
 <semantics>
  <mrow>
   <msub>
    <mi>a</mi>
    <mn>0</mn>
   </msub>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mn>1</mn>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>a</ci>
     <cn type="integer">0</cn>
    </apply>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a_{0}=-1
  </annotation>
 </semantics>
</math>

.</p>

<p>On the other hand, if the mean square prediction error is constrained to be unity and the prediction error equation is included on top of the normal equations, the augmented set of equations is obtained as</p>

<p>

<math display="block" id="Linear_prediction:16">
 <semantics>
  <mrow>
   <mrow>
    <mpadded lspace="5pt" width="+5pt">
     <mi>R</mi>
    </mpadded>
    <mi>a</mi>
   </mrow>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">[</mo>
     <mn>1</mn>
     <mo>,</mo>
     <mn>0</mn>
     <mo>,</mo>
     <mi mathvariant="normal">…</mi>
     <mo>,</mo>
     <mn>0</mn>
     <mo stretchy="false">]</mo>
    </mrow>
    <mi mathvariant="normal">T</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>R</ci>
     <ci>a</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <list>
      <cn type="integer">1</cn>
      <cn type="integer">0</cn>
      <ci>normal-…</ci>
      <cn type="integer">0</cn>
     </list>
     <ci>normal-T</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \ Ra=[1,0,...,0]^{\mathrm{T}}
  </annotation>
 </semantics>
</math>

</p>

<p>where the index <em>i</em> ranges from 0 to <em>p</em>, and <em>R</em> is a (<em>p</em> + 1) × (<em>p</em> + 1) matrix.</p>

<p>Specification of the parameters of the linear predictor is a wide topic and a large number of other approaches have been proposed. In fact, the autocorrelation method is the most common and it is used, for example, for <a href="speech_coding" title="wikilink">speech coding</a> in the <a href="Global_System_for_Mobile_Communications" title="wikilink">GSM</a> standard.</p>

<p>Solution of the matrix equation <em>Ra</em> = <em>r</em> is computationally a relatively expensive process. The <a href="Gauss_algorithm" title="wikilink">Gauss algorithm</a> for matrix inversion is probably the oldest solution but this approach does not efficiently use the symmetry of <em>R</em> and <em>r</em>. A faster algorithm is the <a href="Levinson_recursion" title="wikilink">Levinson recursion</a> proposed by <a href="Norman_Levinson" title="wikilink">Norman Levinson</a> in 1947, which recursively calculates the solution. In particular, the autocorrelation equations above may be more efficiently solved by the Durbin algorithm.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>Later, <a href="Philippe_Delsarte" title="wikilink">Delsarte</a> et al. proposed an improvement to this algorithm called the <a href="split_Levinson_recursion" title="wikilink">split Levinson recursion</a> which requires about half the number of multiplications and divisions. It uses a special symmetrical property of parameter vectors on subsequent recursion levels. That is, calculations for the optimal predictor containing <em>p</em> terms make use of similar calculations for the optimal predictor containing <em>p</em> − 1 terms.</p>

<p>Another way of identifying model parameters is to iteratively calculate state estimates using <a href="Kalman_filter" title="wikilink">Kalman filters</a> and obtaining <a href="maximum_likelihood" title="wikilink">maximum likelihood</a> estimates within <a href="Expectation–maximization_algorithm" title="wikilink">Expectation–maximization algorithms</a>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Autoregressive_model" title="wikilink">Autoregressive model</a></li>
<li><a href="Prediction_interval" title="wikilink">Prediction interval</a></li>
<li><a href="Rasta_filtering" title="wikilink">Rasta filtering</a></li>
<li><a href="Minimum_mean_square_error" title="wikilink">Minimum mean square error</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="further-reading">Further reading</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://labrosa.ee.columbia.edu/matlab/rastamat/">PLP and RASTA (and MFCC, and inversion) in Matlab</a></li>
</ul>

<p>"</p>

<p><a href="Category:Time_series_analysis" title="wikilink">Category:Time series analysis</a> <a href="Category:Signal_processing" title="wikilink">Category:Signal processing</a> <a href="Category:Estimation_theory" title="wikilink">Category:Estimation theory</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2">M. A. Ramirez (2008) "A Levinson Algorithm Based on an Isometric Transformation of Durbin's," IEEE Signal Processing Lett., vol. 15, pp. 99-102. | url = <a class="uri" href="http://dx.doi.org/10.1109/LSP.2007.910319">http://dx.doi.org/10.1109/LSP.2007.910319</a><a href="#fnref2">↩</a></li>
</ol>
</section>


