<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1258">Infineta Systems</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Infineta Systems</h1>
<hr/>

<p><strong>Infineta Systems</strong> was a company that made <a href="WAN_optimization" title="wikilink">WAN optimization</a> products for high performance, latency-sensitive network applications. The company advertised that its Data Mobility Switch (DMS) allowed application <a href="Bit_rate" title="wikilink">data rate</a> to exceed the nominal data rate of the link. Infineta Systems ceased operations by February 2013, a <a href="Liquidator_(law)" title="wikilink">liquidator</a> was appointed, and its products will no longer be manufactured, sold or distributed. <a href="Riverbed_Technology" title="wikilink">Riverbed Technology</a> purchased some of Infineta's assets from the liquidator.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="company">Company</h2>

<p>Infineta was founded in 2008 by Raj Kanaya, the CEO, and K.V.S. Ramarao, the CTO. The term "Big Data" was coined by CPO Haseeb Budhani, who later went on to found BubblewrApp. <a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Ramarao concluded the computational resources, especially I/O operations and CPU cycles, associated with <a href="data_compression" title="wikilink">data compression</a> technologies would ultimately limit their scalability.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> He and Kanaya determined founded Infineta to develop algorithms and hardware. The company had six patents pending.</p>

<p>Infineta was headquartered in <a href="San_Jose,_California" title="wikilink">San Jose, California</a> and attracted $30 million in two rounds of venture funding from <a href="Alloy_Ventures" title="wikilink">Alloy Ventures</a>, North Bridge Venture Partners, and Rembrandt Venture Partners.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h2 id="products">Products</h2>

<p>Infineta launched its Data Mobility Switch in June 2011 after more than two years of development and field trials. The DMS was the first WAN optimization technology to work at throughput rates of 10 Gbit/s.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> Infineta designed the product in <a class="uri" href="FPGA" title="wikilink">FPGA</a> hardware around a multi-Gigabit switch fabric to minimize latency. As a result, accelerated packets average no more than 50 microseconds port-to-port latency. Unaccelerated packets are bridged through the system at wire speed.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> The company decided against designs based on software, large dictionaries, and existing algorithms because it claimed the operational overhead and latency introduced by these technologies did not permit scaling required for data center applications, which include replication, data migrations, and <a class="uri" href="virtualization" title="wikilink">virtualization</a>.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>

<p>The DMS works by removing redundant data from network flows, which allows the same information to be transferred across a link using only 10%-15% of the bytes otherwise required. This kind of compression is similar to <a href="data_deduplication" title="wikilink">data deduplication</a>. The effect is that either the applications generating the data will respond by increasing performance, or, there will be a net decrease in the amount of WAN bandwidth those applications consume.</p>

<p>The product was designed to addresses the long-standing issue of TCP performance<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> on <a href="long_fat_network" title="wikilink">long fat networks</a>, so even unreduced data can achieve throughputs equivalent to the WAN bandwidth. To illustrate what this means, take the example of transferring a 2.5 GBytes (20 billion bits) file from New York to Chicago (15 ms latency, 30 ms <a href="Round-trip_delay_time" title="wikilink">round-trip time</a> ) over a 1 Gbit/s link. With standard TCP, which uses a 64 KB window size, the file transfer would take about 20 minutes. The theoretical maximum throughput is 1 Gbit/s, or about 20 seconds. The DMS performs the transfer in 19.5 to 21 seconds.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h2 id="competitors">Competitors</h2>

<p>Other vendors in the area of WAN optimization included <a href="FatPipe_Networks" title="wikilink">FatPipe Networks</a> which provides WAN optimization on multiple lines, <a class="uri" href="Aryaka" title="wikilink">Aryaka</a>, <a href="Blue_Coat_Systems" title="wikilink">Blue Coat Systems</a>, <a href="Cisco_Systems" title="wikilink">Cisco Systems</a> WAAS, <a class="uri" href="Exinda" title="wikilink">Exinda</a>, <a href="Riverbed_Technology" title="wikilink">Riverbed Technology</a>, and <a href="Silver_Peak_Systems" title="wikilink">Silver Peak Systems</a>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Data_migration" title="wikilink">Data migration</a></li>
<li><a href="WAN_optimization" title="wikilink">WAN optimization</a></li>
<li><a href="Network_latency" title="wikilink">Network latency</a></li>
<li><a href="Network_congestion" title="wikilink">Network congestion</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:WAN_optimization" title="wikilink">Category:WAN optimization</a> <a href="Category:Computer_storage_companies" title="wikilink">Category:Computer storage companies</a> <a href="Category:Computer_companies_of_the_United_States" title="wikilink">Category:Computer companies of the United States</a> <a href="Category:Defunct_networking_companies" title="wikilink">Category:Defunct networking companies</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7">"Typically, the entire latency budget between two servers participating in long distance live migration is around 5-6 milliseconds. Enterprises will need to find ways to optimize this latency-sensitive workflow while remaining within the necessary latency budget." - Jim Metzler, Vice President, Ashton, Metzler &amp; Associates<a href="#fnref7">↩</a></li>
<li id="fn8">“Highly-scalable, multi-gigabit WAN optimization will play a critical role in next-generation data centers as more applications, data, and services become centralized and delivered to remote sites over a WAN.... Achieving the highest degree of performance while simplifying data center architecture around space, cooling, and power will be crucial.” Joe Skorupa, research vice president, data center convergence, Gartner.<a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10">Throughput can be calculated as

<math display="block" id="Infineta_Systems:0">
 <semantics>
  <mrow>
   <mi>Throughput</mi>
   <mo>≤</mo>
   <mfrac>
    <mi>RWIN</mi>
    <mi>RTT</mi>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <leq></leq>
    <ci>Throughput</ci>
    <apply>
     <divide></divide>
     <ci>RWIN</ci>
     <ci>RTT</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathrm{Throughput}\leq\frac{\mathrm{RWIN}}{\mathrm{RTT}}\,\!
  </annotation>
 </semantics>
</math>

 where RWIN is the TCP Receive Window and RTT is the latency to and from the target. The default TCP window size in the absence of <a href="TCP_window_scale_option" title="wikilink">window scaling</a> is 65,536 <a href="byte" title="wikilink">bytes</a>, or 524,228 bits. So for this example, Throughput = 524,228 bits / 0.03 seconds = 17,476,267 bits/second or about 17.5 <a href="megabit" title="wikilink">Mbit</a>/s. Divide the bits to be transferred by the rate of transfer: 20,000,000,000 bits / 17,476,267 = 1,176.5 seconds, or 19.6 minutes.<a href="#fnref10">↩</a></li>
</ol>
</section>
</body>
</html>
