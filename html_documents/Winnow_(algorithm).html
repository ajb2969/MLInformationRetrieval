<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="21">Winnow (algorithm)</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Winnow (algorithm)</h1>
<hr/>

<p>The <strong>winnow algorithm</strong><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> is a technique from <a href="machine_learning" title="wikilink">machine learning</a> for learning a <a href="linear_classifier" title="wikilink">linear classifier</a> from labeled examples. It is very similar to the <a href="perceptron" title="wikilink">perceptron algorithm</a>. However, the perceptron algorithm uses an additive weight-update scheme, while Winnow uses a multiplicative scheme that allows it to perform much better when many dimensions are irrelevant (hence its name). It is a simple algorithm that scales well to high-dimensional data. During training, Winnow is shown a sequence of positive and negative examples. From these it learns a decision <a class="uri" href="hyperplane" title="wikilink">hyperplane</a> that can then be used to label novel examples as positive or negative. The algorithm can also be used in the <a href="Online_machine_learning" title="wikilink">online learning</a> setting, where the learning and the classification phase are not clearly separated.</p>
<h2 id="algorithm">Algorithm</h2>

<p>The basic algorithm, Winnow1, is as follows. The instance space is 

<math display="inline" id="Winnow_(algorithm):0">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>=</mo>
   <msup>
    <mrow>
     <mo stretchy="false">{</mo>
     <mn>0</mn>
     <mo>,</mo>
     <mn>1</mn>
     <mo stretchy="false">}</mo>
    </mrow>
    <mi>n</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>X</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <set>
      <cn type="integer">0</cn>
      <cn type="integer">1</cn>
     </set>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X=\{0,1\}^{n}
  </annotation>
 </semantics>
</math>

, that is, each instance is described as a set of <a class="uri" href="Boolean-valued" title="wikilink">Boolean-valued</a> <a href="features_(pattern_recognition)" title="wikilink">features</a>. The algorithm maintains non-negative weights 

<math display="inline" id="Winnow_(algorithm):1">
 <semantics>
  <msub>
   <mi>w</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}
  </annotation>
 </semantics>
</math>

 for 

<math display="inline" id="Winnow_(algorithm):2">
 <semantics>
  <mrow>
   <mi>i</mi>
   <mo>∈</mo>
   <mrow>
    <mo stretchy="false">{</mo>
    <mrow>
     <mn>1...</mn>
     <mi>n</mi>
    </mrow>
    <mo stretchy="false">}</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>i</ci>
    <set>
     <apply>
      <times></times>
      <cn type="float">1...</cn>
      <ci>n</ci>
     </apply>
    </set>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i\in\{1...n\}
  </annotation>
 </semantics>
</math>

, which are initially set to 1, one weight for each feature. When the learner is given an example 

<math display="inline" id="Winnow_(algorithm):3">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <msub>
    <mi>x</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mrow>
    <mi mathvariant="normal">…</mi>
    <msub>
     <mi>x</mi>
     <mi>n</mi>
    </msub>
   </mrow>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="open">
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">1</cn>
    </apply>
    <apply>
     <times></times>
     <ci>normal-…</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>n</ci>
     </apply>
    </apply>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (x_{1},...x_{n})
  </annotation>
 </semantics>
</math>


, it applies the typical prediction rule for linear classifiers:</p>
<ul>
<li><strong>If</strong> 

<math display="inline" id="Winnow_(algorithm):4">
 <semantics>
  <mrow>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>n</mi>
    </msubsup>
    <mrow>
     <msub>
      <mi>w</mi>
      <mi>i</mi>
     </msub>
     <msub>
      <mi>x</mi>
      <mi>i</mi>
     </msub>
    </mrow>
   </mrow>
   <mo>></mo>
   <mi mathvariant="normal">Θ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>n</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
    <ci>normal-Θ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{i=1}^{n}w_{i}x_{i}>\Theta
  </annotation>
 </semantics>
</math>

, <strong>then</strong> predict 1</li>
<li><strong>Otherwise</strong> predict 0</li>
</ul>

<p>Here 

<math display="inline" id="Winnow_(algorithm):5">
 <semantics>
  <mi mathvariant="normal">Θ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>normal-Θ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Theta
  </annotation>
 </semantics>
</math>

 is a real number that is called the <em>threshold</em>. Together with the weights, the threshold defines a dividing hyperplane in the instance space. Good bounds are obtained if 

<math display="inline" id="Winnow_(algorithm):6">
 <semantics>
  <mrow>
   <mi mathvariant="normal">Θ</mi>
   <mo>=</mo>
   <mrow>
    <mi>n</mi>
    <mo>/</mo>
    <mn>2</mn>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>normal-Θ</ci>
    <apply>
     <divide></divide>
     <ci>n</ci>
     <cn type="integer">2</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Theta=n/2
  </annotation>
 </semantics>
</math>

 (see below).</p>

<p>For each example with which it is presented, the learner applies the following update rule:</p>
<ul>
<li>If an example is correctly classified, do nothing.</li>
<li>If an example is predicted to be 1 but the correct result was 0, all of the weights implicated in the mistake are set to 0 (demotion step).</li>
<li>If an example is predicted to be 0 but the correct result was 1, all of the weights implicated in the mistake are multiplied by 

<math display="inline" id="Winnow_(algorithm):7">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

 (promotion step).</li>
</ul>

<p>Here, "implicated" means weights on features of the instance that have value 1. A typical value for 

<math display="inline" id="Winnow_(algorithm):8">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>


 is 2.</p>

<p>There are many variations to this basic approach. <em>Winnow2</em><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> is similar except that in the demotion step the weights are divided by 

<math display="inline" id="Winnow_(algorithm):9">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

 instead of being set to 0. <em>Balanced Winnow</em> maintains two sets of weights, and thus two hyperplanes. This can then be generalized for <a href="multi-label_classification" title="wikilink">multi-label classification</a>.</p>
<h2 id="mistake-bounds">Mistake bounds</h2>

<p>In certain circumstances, it can be shown that the number of mistakes Winnow makes as it learns has an <a href="Upper_and_lower_bounds" title="wikilink">upper bound</a> that is independent of the number of instances with which it is presented. If the Winnow1 algorithm uses 

<math display="inline" id="Winnow_(algorithm):10">
 <semantics>
  <mrow>
   <mi>α</mi>
   <mo>></mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <gt></gt>
    <ci>α</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha>1
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Winnow_(algorithm):11">
 <semantics>
  <mrow>
   <mi mathvariant="normal">Θ</mi>
   <mo>≥</mo>
   <mrow>
    <mn>1</mn>
    <mo>/</mo>
    <mi>α</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <geq></geq>
    <ci>normal-Θ</ci>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <ci>α</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Theta\geq 1/\alpha
  </annotation>
 </semantics>
</math>

 on a target function that is a 

<math display="inline" id="Winnow_(algorithm):12">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

-literal monotone disjunction given by 

<math display="inline" id="Winnow_(algorithm):13">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>x</mi>
      <mn>1</mn>
     </msub>
     <mo>,</mo>
     <mrow>
      <mi mathvariant="normal">…</mi>
      <msub>
       <mi>x</mi>
       <mi>n</mi>
      </msub>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>x</mi>
     <msub>
      <mi>i</mi>
      <mn>1</mn>
     </msub>
    </msub>
    <mo>∪</mo>
    <mi mathvariant="normal">…</mi>
    <mo>∪</mo>
    <msub>
     <mi>x</mi>
     <msub>
      <mi>i</mi>
      <mi>k</mi>
     </msub>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <interval closure="open">
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <times></times>
       <ci>normal-…</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>n</ci>
       </apply>
      </apply>
     </interval>
    </apply>
    <apply>
     <union></union>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>normal-…</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>i</ci>
       <ci>k</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(x_{1},...x_{n})=x_{i_{1}}\cup...\cup x_{i_{k}}
  </annotation>
 </semantics>
</math>


, then for any sequence of instances the total number of mistakes is bounded by: 

<math display="inline" id="Winnow_(algorithm):14">
 <semantics>
  <mrow>
   <mrow>
    <mi>α</mi>
    <mi>k</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mrow>
       <msub>
        <mi>log</mi>
        <mi>α</mi>
       </msub>
       <mi mathvariant="normal">Θ</mi>
      </mrow>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mfrac>
    <mi>n</mi>
    <mi mathvariant="normal">Θ</mi>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <ci>α</ci>
     <ci>k</ci>
     <apply>
      <plus></plus>
      <apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <log></log>
        <ci>α</ci>
       </apply>
       <ci>normal-Θ</ci>
      </apply>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <divide></divide>
     <ci>n</ci>
     <ci>normal-Θ</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha k(\log_{\alpha}\Theta+1)+\frac{n}{\Theta}
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="references">References</h2>
<references>
</references>

<p>"</p>

<p><a href="Category:Classification_algorithms" title="wikilink">Category:Classification algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Nick Littlestone (1988). "Learning Quickly When Irrelevant Attributes Abound: A New Linear-threshold Algorithm", ''<a href="http://www.springerlink.com/content/j0k7t38567325716/">Machine Learning 285–318(2)</a>.<a href="#fnref1">↩</a></li>
<li id="fn2"></li>
<li id="fn3">Nick Littlestone (1989). "Mistake bounds and logarithmic linear-threshold learning algorithms". Technical report UCSC-CRL-89-11, University of California, Santa Cruz.<a href="#fnref3">↩</a></li>
</ol>
</section>
</body>
</html>
