<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="286">Feature hashing</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Feature hashing</h1>
<style>
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
<style>
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</body></html>
<body>
<hr/>

<p>In <a href="machine_learning" title="wikilink">machine learning</a>, <strong>feature hashing</strong>, also known as the <strong>hashing trick</strong><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> (by analogy to the <a href="kernel_trick" title="wikilink">kernel trick</a>), is a fast and space-efficient way of vectorizing <a href="Feature_(machine_learning)" title="wikilink">features</a>, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a <a href="hash_function" title="wikilink">hash function</a> to the features and using their hash values as indices directly, rather than looking the indices up in an <a href="associative_array" title="wikilink">associative array</a>.</p>
<h2 id="motivating-example">Motivating example</h2>

<p>In a typical <a href="document_classification" title="wikilink">document classification</a> task, the input to the machine learning algorithm (both during learning and classification) is free text. From this, a <a href="bag_of_words" title="wikilink">bag of words</a> (BOW) representation is constructed: the individual <a href="Type‚Äìtoken_distinction" title="wikilink">tokens</a> are extracted and counted, and each distinct token in the training set defines a <a href="Feature_(machine_learning)" title="wikilink">feature</a> (independent variable) of each of the documents in both the training and test sets.</p>

<p>Machine learning algorithms, however, are typically defined in terms of numerical vectors. Therefore, the bags of words for a set of documents is regarded as a <a href="term-document_matrix" title="wikilink">term-document matrix</a> where each row is a single document, and each column is a single feature/word; the entry 

<math display="inline" id="Feature_hashing:0">
 <semantics>
  <mrow>
   <mi>i</mi>
   <mo>,</mo>
   <mi>j</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <ci>i</ci>
    <ci>j</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i,j
  </annotation>
 </semantics>
</math>

 in such a matrix captures the frequency (or weight) of the 

<math display="inline" id="Feature_hashing:1">
 <semantics>
  <mi>j</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>j</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   j
  </annotation>
 </semantics>
</math>

'th term of the <em>vocabulary</em> in document 

<math display="inline" id="Feature_hashing:2">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

. (An alternative convention swaps the rows and columns of the matrix, but this difference is immaterial.) Typically, these vectors are extremely <a href="sparse_matrix" title="wikilink">sparse</a>.</p>

<p>The common approach is to construct, at learning time or prior to that, a <em>dictionary</em> representation of the vocabulary of the training set, and use that to map words to indices. <a href="Hash_table" title="wikilink">Hash tables</a> and <a href="trie" title="wikilink">tries</a> are common candidates for dictionary implementation. E.g., the three documents</p>
<ul>
<li>''John likes to watch movies. ''</li>
<li><em>Mary likes movies too.</em></li>
<li><em>John also likes football.</em></li>
</ul>

<p>can be converted, using the dictionary</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">
<p>Term</p></th>
<th style="text-align: left;">
<p>Index</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p>John</p></td>
<td style="text-align: left;">
<p>1</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>likes</p></td>
<td style="text-align: left;">
<p>2</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>to</p></td>
<td style="text-align: left;">
<p>3</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>watch</p></td>
<td style="text-align: left;">
<p>4</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>movies</p></td>
<td style="text-align: left;">
<p>5</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>Mary</p></td>
<td style="text-align: left;">
<p>6</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>too</p></td>
<td style="text-align: left;">
<p>7</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>also</p></td>
<td style="text-align: left;">
<p>8</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>football</p></td>
<td style="text-align: left;">
<p>9</p></td>
</tr>
</tbody>
</table>

<p>to the term-document matrix</p>

<p>

<math display="block" id="Feature_hashing:3">
 <semantics>
  <mrow>
   <mo>(</mo>
   <mtable displaystyle="true">
    <mtr>
     <mtd columnalign="center">
      <mtext>John</mtext>
     </mtd>
     <mtd columnalign="center">
      <mtext>likes</mtext>
     </mtd>
     <mtd columnalign="center">
      <mtext>to</mtext>
     </mtd>
     <mtd columnalign="center">
      <mtext>watch</mtext>
     </mtd>
     <mtd columnalign="center">
      <mtext>movies</mtext>
     </mtd>
     <mtd columnalign="center">
      <mtext>Mary</mtext>
     </mtd>
     <mtd columnalign="center">
      <mtext>too</mtext>
     </mtd>
     <mtd columnalign="center">
      <mtext>also</mtext>
     </mtd>
     <mtd columnalign="center">
      <mtext>football</mtext>
     </mtd>
    </mtr>
    <mtr>
     <mtd columnalign="center">
      <mn>1</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>1</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>1</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>1</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>1</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
    </mtr>
    <mtr>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>1</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>1</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>1</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>1</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
    </mtr>
    <mtr>
     <mtd columnalign="center">
      <mn>1</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>1</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>0</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>1</mn>
     </mtd>
     <mtd columnalign="center">
      <mn>1</mn>
     </mtd>
    </mtr>
   </mtable>
   <mo>)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <matrix>
    <matrixrow>
     <mtext>John</mtext>
     <mtext>likes</mtext>
     <mtext>to</mtext>
     <mtext>watch</mtext>
     <mtext>movies</mtext>
     <mtext>Mary</mtext>
     <mtext>too</mtext>
     <mtext>also</mtext>
     <mtext>football</mtext>
    </matrixrow>
    <matrixrow>
     <cn type="integer">1</cn>
     <cn type="integer">1</cn>
     <cn type="integer">1</cn>
     <cn type="integer">1</cn>
     <cn type="integer">1</cn>
     <cn type="integer">0</cn>
     <cn type="integer">0</cn>
     <cn type="integer">0</cn>
     <cn type="integer">0</cn>
    </matrixrow>
    <matrixrow>
     <cn type="integer">0</cn>
     <cn type="integer">1</cn>
     <cn type="integer">0</cn>
     <cn type="integer">0</cn>
     <cn type="integer">1</cn>
     <cn type="integer">1</cn>
     <cn type="integer">1</cn>
     <cn type="integer">0</cn>
     <cn type="integer">0</cn>
    </matrixrow>
    <matrixrow>
     <cn type="integer">1</cn>
     <cn type="integer">1</cn>
     <cn type="integer">0</cn>
     <cn type="integer">0</cn>
     <cn type="integer">0</cn>
     <cn type="integer">0</cn>
     <cn type="integer">0</cn>
     <cn type="integer">1</cn>
     <cn type="integer">1</cn>
    </matrixrow>
   </matrix>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \begin{pmatrix}\textrm{John}&\textrm{likes}&\textrm{to}&\textrm{watch}&\textrm%
{movies}&\textrm{Mary}&\textrm{too}&\textrm{also}&\textrm{football}\\
1&1&1&1&1&0&0&0&0\\
0&1&0&0&1&1&1&0&0\\
1&1&0&0&0&0&0&1&1\end{pmatrix}
  </annotation>
 </semantics>
</math>

</p>

<p>(Punctuation was removed, as is usual in document classification and clustering.)</p>

<p>The problem with this process is that such dictionaries take up a large amount of storage space and grow in size as the training set grows in accordance with <a href="Heaps'_law" title="wikilink">Heaps' law</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> Moreover, when the vocabulary is kept fixed, an adversary may try to invent new words or misspellings that are not in the stored vocabulary so as to circumvent a machine learned filter; this is why feature hashing has been tried for <a href="spam_filtering" title="wikilink">spam filtering</a> at <a href="Yahoo!_Research" title="wikilink">Yahoo! Research</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>Note that the hashing trick isn't limited to text classification and similar tasks at the document level, but can be applied to any problem that involves large (perhaps unbounded) numbers of features.</p>
<h2 id="feature-vectorization-using-the-hashing-trick">Feature vectorization using the hashing trick</h2>

<p>Instead of maintaining a dictionary, a feature vectorizer that uses the hashing trick can build a vector of a pre-defined length by applying a hash function 

<math display="inline" id="Feature_hashing:4">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>

 to the features (e.g., words) in the items under consideration, then using the hash values directly as feature indices and updating the resulting vector at those indices:</p>
<div class="sourceCode"><pre class="sourceCode pascal"><code class="sourceCode pascal"> <span class="kw">function</span> hashing_vectorizer(features : <span class="kw">array</span> <span class="kw">of</span> <span class="dt">string</span>, N : <span class="dt">integer</span>):
     x := <span class="kw">new</span> vector[N]
     <span class="kw">for</span> f <span class="kw">in</span> features:
         h := hash(f)
         x[h <span class="kw">mod</span> N] += <span class="dv">1</span>
     return x</code></pre></div>

<p>It has been suggested that a second, single-bit output hash function 

<math display="inline" id="Feature_hashing:5">
 <semantics>
  <mi>Œæ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Œæ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Œæ
  </annotation>
 </semantics>
</math>

 be used to determine the sign of the update value, to counter the effect of <a href="Hash_table#Collision_resolution" title="wikilink">hash collisions</a>.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> If such a hash function is used, the algorithm becomes</p>
<div class="sourceCode"><pre class="sourceCode pascal"><code class="sourceCode pascal"> <span class="kw">function</span> hashing_vectorizer(features : <span class="kw">array</span> <span class="kw">of</span> <span class="dt">string</span>, N : <span class="dt">integer</span>):
     x := <span class="kw">new</span> vector[N]
     <span class="kw">for</span> f <span class="kw">in</span> features:
         h := hash(f)
         idx := h <span class="kw">mod</span> N
         <span class="kw">if</span> Œæ(f) == <span class="dv">1</span>:
             x[idx] += <span class="dv">1</span>
         <span class="kw">else</span>:
             x[idx] -= <span class="dv">1</span>
     return x</code></pre></div>

<p>The above pseudocode actually converts each sample into a vector. An optimized version would instead only generate a stream of (

<math display="inline" id="Feature_hashing:6">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>

,

<math display="inline" id="Feature_hashing:7">
 <semantics>
  <mi>Œæ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Œæ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Œæ
  </annotation>
 </semantics>
</math>


) pairs and let the learning and prediction algorithms consume such streams; a <a href="linear_model" title="wikilink">linear model</a> can then be implemented as a single hash table representing the coefficient vector.</p>
<h3 id="properties">Properties</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">
<p><em>Œæ</em>(<em>f</em>‚ÇÅ)</p></th>
<th style="text-align: left;">
<p><em>Œæ</em>(<em>f</em>‚ÇÇ)</p></th>
<th style="text-align: left;">
<p>Final value, <em>Œæ</em>(<em>f</em>‚ÇÅ) + <em>Œæ</em>(<em>f</em>‚ÇÇ)</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p>-1</p></td>
<td style="text-align: left;">
<p>-1</p></td>
<td style="text-align: left;">
<p>-2</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>-1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>0</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>-1</p></td>
<td style="text-align: left;">
<p>0</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>1</p></td>
<td style="text-align: left;">
<p>2</p></td>
</tr>
</tbody>
</table>

<p>When a second hash function <em>Œæ</em> is used to determine the sign of a feature's value, the <a href="Expected_value" title="wikilink">expected</a> <a class="uri" href="mean" title="wikilink">mean</a> of each column in the output array becomes zero because <em>Œæ</em> causes some collisions to cancel out.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> E.g., suppose an input contains two symbolic features <em>f</em>‚ÇÅ and <em>f</em>‚ÇÇ that collide with each other, but not with any other features in the same input; then there are four possibilities which, if we make no assumptions about <em>Œæ</em>, have equal probability, as listed in the table on the right.</p>

<p>In this example, there is a 50% probability that the hash collision cancels out. Multiple hash functions can be used to further reduce the risk of collisions.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>

<p>Furthermore, if <em>œÜ</em> is the transformation implemented by a hashing trick with a sign hash <em>Œæ</em> (i.e. <em>œÜ</em>(<em>x</em>) is the feature vector produced for a sample <em>x</em>), then <a href="inner_product" title="wikilink">inner products</a> in the hashed space are unbiased:</p>

<p>

<math display="block" id="Feature_hashing:8">
 <semantics>
  <mrow>
   <mrow>
    <mi>ùîº</mi>
    <mrow>
     <mo stretchy="false">[</mo>
     <mrow>
      <mo stretchy="false">‚ü®</mo>
      <mrow>
       <mi>œÜ</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>,</mo>
      <mrow>
       <mi>œÜ</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <msup>
         <mi>x</mi>
         <mo>‚Ä≤</mo>
        </msup>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo stretchy="false">‚ü©</mo>
     </mrow>
     <mo stretchy="false">]</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">‚ü®</mo>
    <mi>x</mi>
    <mo>,</mo>
    <msup>
     <mi>x</mi>
     <mo>‚Ä≤</mo>
    </msup>
    <mo stretchy="false">‚ü©</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>ùîº</ci>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <list>
       <apply>
        <times></times>
        <ci>œÜ</ci>
        <ci>x</ci>
       </apply>
       <apply>
        <times></times>
        <ci>œÜ</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <ci>x</ci>
         <ci>normal-‚Ä≤</ci>
        </apply>
       </apply>
      </list>
     </apply>
    </apply>
    <list>
     <ci>x</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>x</ci>
      <ci>normal-‚Ä≤</ci>
     </apply>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbb{E}[\langle\varphi(x),\varphi(x^{\prime})\rangle]=\langle x,x^{\prime}\rangle
  </annotation>
 </semantics>
</math>

</p>

<p>where the expectation is taken over the hashing function <em>œÜ</em>.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> It can be verified that

<math display="inline" id="Feature_hashing:9">
 <semantics>
  <mrow>
   <mo stretchy="false">‚ü®</mo>
   <mrow>
    <mi>œÜ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>,</mo>
   <mrow>
    <mi>œÜ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msup>
      <mi>x</mi>
      <mo>‚Ä≤</mo>
     </msup>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo stretchy="false">‚ü©</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <times></times>
     <ci>œÜ</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <times></times>
     <ci>œÜ</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>x</ci>
      <ci>normal-‚Ä≤</ci>
     </apply>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \langle\varphi(x),\varphi(x^{\prime})\rangle
  </annotation>
 </semantics>
</math>

 is a <a href="Positive-definite_matrix" title="wikilink">positive semi-definite</a> <a href="Kernel_trick" title="wikilink">kernel</a>.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<h3 id="extensions-and-variations">Extensions and variations</h3>

<p>Recent work extends the hashing trick to supervised mappings from words to indices,<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> which are explicitly learned to avoid collisions of important terms.</p>
<h3 id="applications-and-practical-performance">Applications and practical performance</h3>

<p>Ganchev and Dredze showed that in text classification applications with random hash functions and several tens of thousands of columns in the output vectors, feature hashing need not have an adverse effect on classification performance, even without the signed hash function.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> Weinberger et al. applied their variant of hashing to the problem of <a href="spam_filter" title="wikilink">spam filtering</a>, formulating this as a <a href="multi-task_learning" title="wikilink">multi-task learning</a> problem where the input features are pairs (user, feature) so that a single parameter vector captured per-user spam filters as well as a global filter for several hundred thousand users, and found that the accuracy of the filter went up.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></p>
<h2 id="implementations">Implementations</h2>

<p>Implementations of the hashing trick are present in:</p>
<ul>
<li><a href="Apache_Mahout" title="wikilink">Apache Mahout</a><a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></li>
<li><a class="uri" href="Gensim" title="wikilink">Gensim</a><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></li>
<li><a class="uri" href="scikit-learn" title="wikilink">scikit-learn</a><a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></li>
<li>sofia-ml<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a></li>
<li><a href="Vowpal_Wabbit" title="wikilink">Vowpal Wabbit</a></li>
<li><a href="Apache_Spark" title="wikilink">Apache Spark</a><a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></li>
<li><a href="R_(programming_language)" title="wikilink">R</a><a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a></li>
</ul>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Bloom_filter" title="wikilink">Bloom filter</a></li>
<li><a href="Count‚Äìmin_sketch" title="wikilink">Count‚Äìmin sketch</a></li>
<li><a href="Locality-sensitive_hashing" title="wikilink">Locality-sensitive hashing</a></li>
<li><a class="uri" href="MinHash" title="wikilink">MinHash</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://hunch.net/~jl/projects/hash_reps/index.html">Hashing Representations for Machine Learning</a> on John Langford's website</li>
<li><a href="http://metaoptimize.com/qa/questions/6943/what-is-the-hashing-trick">What is the "hashing trick"? - MetaOptimize Q+A</a></li>
</ul>

<p>"</p>

<p><a class="uri" href="Category:Hashing" title="wikilink">Category:Hashing</a> <a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"></li>
<li id="fn2"><a href="#fnref2">‚Ü©</a></li>
<li id="fn3"><a href="#fnref3">‚Ü©</a></li>
<li id="fn4"><a href="#fnref4">‚Ü©</a></li>
<li id="fn5"></li>
<li id="fn6"><a href="#fnref6">‚Ü©</a></li>
<li id="fn7"></li>
<li id="fn8"></li>
<li id="fn9"><a href="#fnref9">‚Ü©</a></li>
<li id="fn10"><a href="#fnref10">‚Ü©</a></li>
<li id="fn11"></li>
<li id="fn12"></li>
<li id="fn13"></li>
<li id="fn14"><a href="#fnref14">‚Ü©</a></li>
<li id="fn15"><a href="#fnref15">‚Ü©</a></li>
<li id="fn16"><a href="#fnref16">‚Ü©</a></li>
<li id="fn17"><a href="#fnref17">‚Ü©</a></li>
<li id="fn18"><a href="#fnref18">‚Ü©</a></li>
</ol>
</section>
</body>

