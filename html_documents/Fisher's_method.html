<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1363">Fisher's method</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Fisher's method</h1>
<hr>[[File:Kequals2.jpg|thumb|right|right|400px|Under Fisher's method, two small [[p-value]]s ''P''<sub>1</sub> and ''P''<sub>2</sub> combine to form a smaller p-value.  The yellow-green boundary defines the region where the meta-analysis p-value is below 0.05.  For example, if both  p-values are around 0.10, or i
<p>f one is around 0.04 and one is around 0.25, the meta-analysis p-value is around 0.05.]] In <a class="uri" href="statistics" title="wikilink">statistics</a>, <strong>Fisher's method</strong>,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> also known as <strong>Fisher's combined probability test</strong>, is a technique for <a href="data_fusion" title="wikilink">data fusion</a> or "<a class="uri" href="meta-analysis" title="wikilink">meta-analysis</a>" (analysis of analyses). It was developed by and named for <a href="Ronald_Fisher" title="wikilink">Ronald Fisher</a>. In its basic form, it is used to combine the results from several <a href="Statistical_independence" title="wikilink">independent</a> <a href="Statistical_hypothesis_testing" title="wikilink">tests</a> bearing upon the same overall <a href="statistical_hypothesis_testing" title="wikilink">hypothesis</a> (<em>H</em><sub>0</sub>).</p>
<h2 id="application-to-independent-test-statistics">Application to independent test statistics</h2>

<p>Fisher's method combines extreme value <a class="uri" href="probabilities" title="wikilink">probabilities</a> from each test, commonly known as "<a href="p-value" title="wikilink">p-values</a>", into one <a href="test_statistic" title="wikilink">test statistic</a> (<em>X</em><sup>2</sup>) using the formula</p>

<p>

<math display="block" id="Fisher's_method:0">
 <semantics>
  <mrow>
   <mrow>
    <msubsup>
     <mi>X</mi>
     <mrow>
      <mn>2</mn>
      <mi>k</mi>
     </mrow>
     <mn>2</mn>
    </msubsup>
    <mo>∼</mo>
    <mrow>
     <mo>-</mo>
     <mrow>
      <mn>2</mn>
      <mrow>
       <munderover>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mrow>
         <mi>i</mi>
         <mo>=</mo>
         <mn>1</mn>
        </mrow>
        <mi>k</mi>
       </munderover>
       <mrow>
        <mi>ln</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <msub>
          <mi>p</mi>
          <mi>i</mi>
         </msub>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>X</ci>
      <cn type="integer">2</cn>
     </apply>
     <apply>
      <times></times>
      <cn type="integer">2</cn>
      <ci>k</ci>
     </apply>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <cn type="integer">2</cn>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>k</ci>
       </apply>
       <apply>
        <ln></ln>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>p</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X^{2}_{2k}\sim-2\sum_{i=1}^{k}\ln(p_{i}),
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>p</em><sub><em>i</em></sub> is the p-value for the <em>i</em><sup>th</sup> hypothesis test. When the p-values tend to be small, the test statistic <em>X</em><sup>2</sup> will be large, which suggests that the null hypotheses are not true for every test.</p>

<p>When all the null hypotheses are true, and the <em>p</em><sub><em>i</em></sub> (or their corresponding test statistics) are independent, <em>X</em><sup>2</sup> has a <a href="chi-squared_distribution" title="wikilink">chi-squared distribution</a> with 2<em>k</em> <a href="Degrees_of_freedom_(statistics)" title="wikilink">degrees of freedom</a>, where <em>k</em> is the number of <a href="Statistical_hypothesis_testing" title="wikilink">tests</a> being combined. This fact can be used to determine the <a class="uri" href="p-value" title="wikilink">p-value</a> for <em>X</em><sup>2</sup>.</p>

<p>The distribution of <em>X</em><sup>2</sup> is a <a href="chi-squared_distribution" title="wikilink">chi-squared distribution</a> for the following reason. Under the null hypothesis for test <em>i</em>, the p-value <em>p</em><sub><em>i</em></sub> follows a <a href="uniform_distribution_(continuous)" title="wikilink">uniform distribution</a> on the interval [0,1]. The negative natural logarithm of a uniformly distributed value follows an <a href="exponential_distribution" title="wikilink">exponential distribution</a>. Scaling a value that follows an exponential distribution by a factor of two yields a quantity that follows a <a href="chi-squared_distribution" title="wikilink">chi-squared distribution</a> with two degrees of freedom. Finally, the sum of <em>k</em> independent chi-squared values, each with two degrees of freedom, follows a chi-squared distribution with 2<em>k</em> degrees of freedom.</p>
<h2 id="limitations-of-independent-assumption">Limitations of independent assumption</h2>

<p>Dependence among statistical tests is generally positive, which means that the p-value of <em>X</em><sup>2</sup> is too small (anti-conservative) if the dependency is not taken into account. Thus, if Fisher's method for independent tests is applied in a dependent setting, and the p-value is not small enough to reject the null hypothesis, then that conclusion will continue to hold even if the dependence is not properly accounted for. However, if positive dependence is not accounted for, and the meta-analysis p-value is found to be small, the evidence against the null hypothesis is generally overstated. The <a href="False_discovery_rate#Controlling_procedures" title="wikilink">mean false discovery rate</a>, 

<math display="inline" id="Fisher's_method:1">
 <semantics>
  <mrow>
   <mrow>
    <mi>α</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>k</mi>
      <mo>+</mo>
      <mn>1</mn>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>/</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <mn>2</mn>
     <mi>k</mi>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <times></times>
     <ci>α</ci>
     <apply>
      <plus></plus>
      <ci>k</ci>
      <cn type="integer">1</cn>
     </apply>
    </apply>
    <apply>
     <times></times>
     <cn type="integer">2</cn>
     <ci>k</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha(k+1)/(2k)
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Fisher's_method:2">
 <semantics>
  <mi>α</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>α</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \alpha
  </annotation>
 </semantics>
</math>

 reduced for 

<math display="inline" id="Fisher's_method:3">
 <semantics>
  <mi>k</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>k</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k
  </annotation>
 </semantics>
</math>

 independent or positively correlated tests, may suffice to control <a href="Type_I_and_type_II_errors#Type_I_error" title="wikilink">alpha</a> for useful comparison to an over-small p-value from Fisher's <em>X</em><sup>2</sup>.</p>
<h2 id="extension-to-dependent-test-statistics">Extension to dependent test statistics</h2>

<p>In cases where the tests are not independent, the null distribution of <em>X</em><sup>2</sup> is more complicated. A common strategy is to approximate the null distribution with a scaled <mtpl></mtpl> random variable. Different approaches may be used depending on whether or not the covariance between the different p-values is known.</p>

<p><a href="Extensions_of_Fisher%27s_method#Brown.27s_method" title="wikilink">Brown's method</a> <a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> can be used to combine dependent p-values whose underlying test statistics have a multivariate normal distribution with a known covariance matrix. <a href="Extensions_of_Fisher%27s_method#Kost.27s_method:_t_approximation" title="wikilink">Kost's method</a> <a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> extends Brown's to allow one to combine p-values when the covariance matrix is known only up to a scalar multiplicative factor.</p>
<h2 id="interpretation">Interpretation</h2>

<p>Fisher's method is typically applied to a collection of independent test statistics, usually from separate studies having the same null hypothesis. The meta-analysis null hypothesis is that all of the separate null hypotheses are true. The meta-analysis alternative hypothesis is that at least one of the separate <em>alternative</em> hypotheses is true.</p>

<p>In some settings, it makes sense to consider the possibility of "heterogeneity," in which the null hypothesis holds in some studies but not in others, or where different alternative hypotheses may hold in different studies. A common reason for the latter form of heterogeneity is that <a href="effect_size" title="wikilink">effect sizes</a> may differ among populations. For example, consider a collection of medical studies looking at the risk of a high glucose diet for developing type II <a class="uri" href="diabetes" title="wikilink">diabetes</a>. Due to genetic or environmental factors, the true risk associated with a given level of glucose consumption may be greater in some human populations than in others.</p>

<p>In other settings, the alternative hypothesis is either universally false, or universally true – there is no possibility of it holding in some settings but not in others. For example, consider several experiments designed to test a particular physical law. Any discrepancies among the results from separate studies or experiments must be due to chance, possibly driven by differences in <a href="statistical_power" title="wikilink">power</a>.</p>

<p>In the case of a meta-analysis using two-sided tests, it is possible to reject the meta-analysis null hypothesis even when the individual studies show strong effects in differing directions. In this case, we are rejecting the hypothesis that the null hypothesis is true in every study, but this does not imply that there is a uniform alternative hypothesis that holds across all studies. Thus, two-sided meta-analysis is particularly sensitive to heterogeneity in the alternative hypotheses. One sided meta-analysis can detect heterogeneity in the effect magnitudes, but focuses on a single, pre-specified effect direction.</p>
<h2 id="relation-to-stouffers-z-score-method">Relation to Stouffer's Z-score method</h2>
<figure><b>(Figure)</b>
<embed src="zlogp.pdf" title="The relationship between Fisher's method and Stouffer's method can be understood from the relationship between z and −log(p)"></embed><figcaption>The relationship between Fisher's method and Stouffer's method can be understood from the relationship between <em>z</em> and −log(<em>p</em>)</figcaption>
</figure>

<p>A closely related approach to Fisher's method is based on Z-scores rather than p-values.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> If we let <em>Z</em><sub><em>i</em></sub>  =  <em>Φ</em><sup> − 1</sup>(1−<em>p</em><sub><em>i</em></sub>), where <em>Φ</em> is the standard normal <a href="cumulative_distribution_function" title="wikilink">cumulative distribution function</a>, then</p>

<p>

<math display="block" id="Fisher's_method:4">
 <semantics>
  <mrow>
   <mrow>
    <mi>Z</mi>
    <mo>∼</mo>
    <mfrac>
     <mrow>
      <msubsup>
       <mo largeop="true" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>k</mi>
      </msubsup>
      <msub>
       <mi>Z</mi>
       <mi>i</mi>
      </msub>
     </mrow>
     <msqrt>
      <mi>k</mi>
     </msqrt>
    </mfrac>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <ci>Z</ci>
    <apply>
     <divide></divide>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>k</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Z</ci>
       <ci>i</ci>
      </apply>
     </apply>
     <apply>
      <root></root>
      <ci>k</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Z\sim\frac{\sum_{i=1}^{k}Z_{i}}{\sqrt{k}},
  </annotation>
 </semantics>
</math>

</p>

<p>is a Z-score for the overall meta-analysis. This Z-score is appropriate for one-sided right-tailed p-values; minor modifications can be made if two-sided or left-tailed p-values are being analyzed. This method is named for the sociologist <a href="Samuel_A._Stouffer" title="wikilink">Samuel A. Stouffer</a>.</p>

<p>Since Fisher's method is based on the average of −log(<em>p</em><sub><em>i</em></sub>) values, and the Z-score method is based on the average of the <em>Z</em><sub><em>i</em></sub> values, the relationship between these two approaches follows from the relationship between <em>z</em> and −log(<em>p</em>) = −log(1−<em>Φ</em>(<em>z</em>)). For the normal distribution, these two values are not perfectly linearly related, but they follow a highly linear relationship over the range of Z-values most often observed, from 1 to 5. As a result, the power of the Z-score method is nearly identical to the power of Fisher's method.</p>

<p>One advantage of the Z-score approach is that it is straightforward to introduce weights. <a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> If the <em>i</em><sup><em>th</em></sup> Z-score is weighted by <em>w</em><sub><em>i</em></sub>, then the meta-analysis Z-score is</p>

<p>

<math display="block" id="Fisher's_method:5">
 <semantics>
  <mrow>
   <mrow>
    <mi>Z</mi>
    <mo>∼</mo>
    <mfrac>
     <mrow>
      <msubsup>
       <mo largeop="true" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>k</mi>
      </msubsup>
      <mrow>
       <msub>
        <mi>w</mi>
        <mi>i</mi>
       </msub>
       <msub>
        <mi>Z</mi>
        <mi>i</mi>
       </msub>
      </mrow>
     </mrow>
     <msqrt>
      <mrow>
       <msubsup>
        <mo largeop="true" symmetric="true">∑</mo>
        <mrow>
         <mi>i</mi>
         <mo>=</mo>
         <mn>1</mn>
        </mrow>
        <mi>k</mi>
       </msubsup>
       <msubsup>
        <mi>w</mi>
        <mi>i</mi>
        <mn>2</mn>
       </msubsup>
      </mrow>
     </msqrt>
    </mfrac>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <ci>Z</ci>
    <apply>
     <divide></divide>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>k</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>w</ci>
        <ci>i</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Z</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <root></root>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>k</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>w</ci>
         <ci>i</ci>
        </apply>
        <cn type="integer">2</cn>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Z\sim\frac{\sum_{i=1}^{k}w_{i}Z_{i}}{\sqrt{\sum_{i=1}^{k}w_{i}^{2}}},
  </annotation>
 </semantics>
</math>

</p>

<p>which follows a standard normal distribution under the null hypothesis. While weighted versions of Fisher's statistic can be derived, the null distribution becomes a weighted sum of independent chi-squared statistics, which is less convenient to work with.</p>

<p>Implemented in <a href="R_(programming_language)" title="wikilink">R</a>, a functions to compute Fisher's <em>X</em><sup>2</sup> and Stouffer's (weighted) Z and their <em>p</em>-values is:</p>
<pre class="rsplus"><code>
Stouffer.test &lt;- function(p, w) { # p is a vector of p-values
  if (missing(w)) {
    w &lt;- rep(1, length(p))/length(p)
  } else {
    if (length(w) != length(p))
      stop("Length of p and w must equal!")
  }
  Zi &lt;- qnorm(1-p) 
  Z  &lt;- sum(w*Zi)/sqrt(sum(w^2))
  p.val &lt;- 1-pnorm(Z)
  return(c(Z = Z, p.value = p.val))
}

Fisher.test &lt;- function(p) {
  Xsq &lt;- -2*sum(log(p))
  p.val &lt;- pchisq(Xsq, df = 2*length(p), lower.tail = FALSE)
  return(c(Xsq = Xsq, p.value = p.val))
}

p &lt;- c(.01, .2, .3)
Stouffer.test(p = p)  # p-value = 0.017
Fisher.test(p = p)  # p-value = 0.022</code></pre>
<h2 id="references">References</h2>
<references>
</references>
<h2 id="see-also">See also</h2>
<ul>
<li>An alternative source for Fisher's 1948 note: <a href="http://digital.library.adelaide.edu.au/dspace/bitstream/2440/15258/1/224A.pdf">1</a></li>
</ul>

<p>"</p>

<p><a href="Category:Statistical_tests" title="wikilink">Category:Statistical tests</a> <a class="uri" href="Category:Meta-analysis" title="wikilink">Category:Meta-analysis</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
</ol>
</section>
</hr></body>
</html>
