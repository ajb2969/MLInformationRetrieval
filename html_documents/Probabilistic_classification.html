<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="933">Probabilistic classification</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Probabilistic classification</h1>
<hr/>

<p>In <a href="machine_learning" title="wikilink">machine learning</a>, a <strong>probabilistic classifier</strong> is a <a href="statistical_classification" title="wikilink">classifier</a> that is able to predict, given a sample input, a <a href="probability_distribution" title="wikilink">probability distribution</a> over a set of classes, rather than only outputting the most likely class that the sample should belong to. Probabilistic classifiers provide classification with a degree of certainty, which can be useful in its own right,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> or when combining classifiers into <a href="ensemble_classifier" title="wikilink">ensembles</a>.</p>

<p>Formally, an "ordinary" classifier is some rule, or <a href="function_(mathematics)" title="wikilink">function</a>, that assigns to a sample 

<math display="inline" id="Probabilistic_classification:0">
<semantics>
<mi>x</mi>
<annotation-xml encoding="MathML-Content">
<ci>x</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   x
  </annotation>
</semantics>
</math>

 a class label 

<math display="inline" id="Probabilistic_classification:1">
<semantics>
<mi>≈∑</mi>
<annotation-xml encoding="MathML-Content">
<ci>≈∑</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   ≈∑
  </annotation>
</semantics>
</math>

:</p>

<p>
<math display="block" id="Probabilistic_classification:2">
<semantics>
<mrow>
<mover accent="true">
<mi>y</mi>
<mo stretchy="false">^</mo>
</mover>
<mo>=</mo>
<mrow>
<mi>f</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<ci>normal-^</ci>
<ci>y</ci>
</apply>
<apply>
<times></times>
<ci>f</ci>
<ci>x</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \hat{y}=f(x)
  </annotation>
</semantics>
</math>
</p>

<p>The samples come from some set 

<math display="inline" id="Probabilistic_classification:3">
<semantics>
<mi>X</mi>
<annotation-xml encoding="MathML-Content">
<ci>X</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   X
  </annotation>
</semantics>
</math>

 (e.g., the set of all <a href="document_classification" title="wikilink">documents</a>, or the set of all <a href="Computer_vision#Recognition" title="wikilink">images</a>), while the class labels form a finite set 

<math display="inline" id="Probabilistic_classification:4">
<semantics>
<mi>Y</mi>
<annotation-xml encoding="MathML-Content">
<ci>Y</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   Y
  </annotation>
</semantics>
</math>

 defined prior to training.</p>

<p>Probabilistic classifiers generalize this notion of classifiers: instead of functions, they are <a href="conditional_probability" title="wikilink">conditional</a> distributions 

<math display="inline" id="Probabilistic_classification:5">
<semantics>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>Y</mi>
<mo stretchy="false">|</mo>
<mi>X</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>Pr</ci>
<ci>Y</ci>
<ci>X</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Pr(Y|X)
  </annotation>
</semantics>
</math>

, meaning that for a given 

<math display="inline" id="Probabilistic_classification:6">
<semantics>
<mrow>
<mi>x</mi>
<mo>‚àà</mo>
<mi>X</mi>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<in></in>
<ci>x</ci>
<ci>X</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   x\in X
  </annotation>
</semantics>
</math>

, they assign probabilities to all 

<math display="inline" id="Probabilistic_classification:7">
<semantics>
<mrow>
<mi>y</mi>
<mo>‚àà</mo>
<mi>Y</mi>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<in></in>
<ci>y</ci>
<ci>Y</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   y\in Y
  </annotation>
</semantics>
</math>

 (and these probabilities sum to one). "Hard" classification can then be done using the <a href="Bayes_estimator" title="wikilink">optimal decision rule</a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>
<math display="block" id="Probabilistic_classification:8">
<semantics>
<mrow>
<mover accent="true">
<mi>y</mi>
<mo stretchy="false">^</mo>
</mover>
<mo>=</mo>
<mrow>
<mrow>
<msub>
<mrow>
<mi>arg</mi>
<mi>max</mi>
</mrow>
<mi>y</mi>
</msub>
<mi>Pr</mi>
</mrow>
<mrow>
<mo stretchy="false">(</mo>
<mrow>
<mi>Y</mi>
<mo>=</mo>
<mi>y</mi>
</mrow>
<mo stretchy="false">|</mo>
<mi>X</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<eq></eq>
<apply>
<ci>normal-^</ci>
<ci>y</ci>
</apply>
<apply>
<apply>
<apply>
<csymbol cd="ambiguous">subscript</csymbol>
<apply>
<arg></arg>
<max></max>
</apply>
<ci>y</ci>
</apply>
<ci>Pr</ci>
</apply>
<apply>
<eq></eq>
<ci>Y</ci>
<ci>y</ci>
</apply>
<ci>X</ci>
</apply>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \hat{y}=\operatorname{\arg\max}_{y}\Pr(Y=y|X)
  </annotation>
</semantics>
</math>
</p>

<p>or, in English, the predicted class is that which has the highest probability.</p>

<p>Binary probabilistic classifiers are also called <a href="binomial_regression" title="wikilink">binomial regression</a> models in <a class="uri" href="statistics" title="wikilink">statistics</a>. In <a class="uri" href="econometrics" title="wikilink">econometrics</a>, probabilistic classification in general is called <a href="discrete_choice" title="wikilink">discrete choice</a>.</p>

<p>Some classification models, such as <a href="naive_Bayes_classifier" title="wikilink">naive Bayes</a>, <a href="logistic_regression" title="wikilink">logistic regression</a> and <a href="multilayer_perceptron" title="wikilink">multilayer perceptrons</a> (when trained under an appropriate <a href="loss_function" title="wikilink">loss function</a>) are naturally probabilistic. Other models such as <a href="support_vector_machine" title="wikilink">support vector machines</a> are not, but <a href="#Probability_calibration" title="wikilink">methods exist</a> to turn them into probabilistic classifiers.</p>
<h2 id="generative-and-conditional-training">Generative and conditional training</h2>

<p>Some models, such as logistic regression, are conditionally trained: they optimize the conditional probability 

<math display="inline" id="Probabilistic_classification:9">
<semantics>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>Y</mi>
<mo stretchy="false">|</mo>
<mi>X</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>Pr</ci>
<ci>Y</ci>
<ci>X</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Pr(Y|X)
  </annotation>
</semantics>
</math>

 directly on a training set (see <a href="empirical_risk_minimization" title="wikilink">empirical risk minimization</a>). Other classifiers, such as naive Bayes, are trained <a href="Generative_model" title="wikilink">generatively</a>: at training time, the class-conditional distribution 

<math display="inline" id="Probabilistic_classification:10">
<semantics>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>X</mi>
<mo stretchy="false">|</mo>
<mi>Y</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>Pr</ci>
<ci>X</ci>
<ci>Y</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Pr(X|Y)
  </annotation>
</semantics>
</math>

 and the class <a href="Prior_probability" title="wikilink">prior</a>
<math display="inline" id="Probabilistic_classification:11">
<semantics>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>Y</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>Pr</ci>
<ci>Y</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Pr(Y)
  </annotation>
</semantics>
</math>

 are found, and the conditional distribution 

<math display="inline" id="Probabilistic_classification:12">
<semantics>
<mrow>
<mi>Pr</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>Y</mi>
<mo stretchy="false">|</mo>
<mi>X</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<apply>
<ci>Pr</ci>
<ci>Y</ci>
<ci>X</ci>
</apply>
</annotation-xml>
<annotation encoding="application/x-tex">
   \Pr(Y|X)
  </annotation>
</semantics>
</math>

 is derived using <a href="Bayes'_theorem" title="wikilink">Bayes' rule</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="probability-calibration">Probability calibration</h2>

<p>Not all classification models are naturally probabilistic, and some that are, notably naive Bayes classifiers, <a href="decision_tree_learning" title="wikilink">decision trees</a> and <a href="Boosting_(machine_learning)" title="wikilink">boosting</a> methods, produce distorted class probability distributions.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> In the case of decision trees, where 

<math display="inline" id="Probabilistic_classification:13">
<semantics>
<mrow>
<mi>P</mi>
<mi>r</mi>
<mrow>
<mo stretchy="false">(</mo>
<mi>y</mi>
<mo stretchy="false">|</mo>
<mi>ùê±</mi>
<mo stretchy="false">)</mo>
</mrow>
</mrow>
<annotation-xml encoding="MathML-Content">
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<csymbol cd="unknown">P</csymbol>
<csymbol cd="unknown">r</csymbol>
<cerror>
<csymbol cd="ambiguous">fragments</csymbol>
<ci>normal-(</ci>
<csymbol cd="unknown">y</csymbol>
<ci>normal-|</ci>
<csymbol cd="unknown">x</csymbol>
<ci>normal-)</ci>
</cerror>
</cerror>
</annotation-xml>
<annotation encoding="application/x-tex">
   Pr(y|\mathbf{x})
  </annotation>
</semantics>
</math>

 is the proportion of training samples with label 

<math display="inline" id="Probabilistic_classification:14">
<semantics>
<mi>y</mi>
<annotation-xml encoding="MathML-Content">
<ci>y</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   y
  </annotation>
</semantics>
</math>

 in the leaf where 

<math display="inline" id="Probabilistic_classification:15">
<semantics>
<mi>ùê±</mi>
<annotation-xml encoding="MathML-Content">
<ci>ùê±</ci>
</annotation-xml>
<annotation encoding="application/x-tex">
   \mathbf{x}
  </annotation>
</semantics>
</math>

 ends up, these distortions come about because learning algorithms such as <a class="uri" href="C4.5" title="wikilink">C4.5</a> or <a href="Predictive_analytics#Classification_and_regression_trees" title="wikilink">CART</a> explicitly aim to produce homogeneous leaves (giving probabilities close to zero or one, and thus high <a href="Bias_of_an_estimator" title="wikilink">bias</a>) while using few samples to estimate the relevant proportion (high <a href="Bias‚Äìvariance_tradeoff" title="wikilink">variance</a>).<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>

<p>For classification models that produce some kind of "score" on their outputs (such as a distorted probability distribution or the "signed distance to the hyperplane" in a support vector machine), there are several methods that turn these scores into properly <a href="Calibration_(statistics)" title="wikilink">calibrated</a> class membership probabilities.</p>

<p>For the <a href="binary_classification" title="wikilink">binary</a> case, a common approach is to apply <a href="Platt_scaling" title="wikilink">Platt scaling</a>, which learns a <a href="logistic_regression" title="wikilink">logistic regression</a> model on the scores.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> An alternative method using <a href="isotonic_regression" title="wikilink">isotonic regression</a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> is generally superior to Platt's method when sufficient training data is available.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>

<p>In the <a href="multiclass_classification" title="wikilink">multiclass</a> case, one can use a reduction to binary tasks, followed by univariate calibration with an algorithm as described above and further application of the pairwise coupling algorithm by Hastie and Tibshirani.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>
<h2 id="evaluating-probabilistic-classification">Evaluating probabilistic classification</h2>

<p>Commonly used loss functions for probabilistic classification include <a href="log_loss" title="wikilink">log loss</a> and the <a href="mean_squared_error" title="wikilink">mean squared error</a> between the predicted and the true probability distributions. The former of these is commonly used to train logistic models.</p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Probabilistic_models" title="wikilink">Category:Probabilistic models</a> <a href="Category:Statistical_classification" title="wikilink">Category:Statistical classification</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><mtpl></mtpl><a href="#fnref1">‚Ü©</a></li>
<li id="fn2"><a href="#fnref2">‚Ü©</a></li>
<li id="fn3"></li>
<li id="fn4"><a href="#fnref4">‚Ü©</a></li>
<li id="fn5"><a href="#fnref5">‚Ü©</a></li>
<li id="fn6"><a href="#fnref6">‚Ü©</a></li>
<li id="fn7"><a href="#fnref7">‚Ü©</a></li>
<li id="fn8"></li>
<li id="fn9"><a href="#fnref9">‚Ü©</a></li>
</ol>
</section>
</body>
</html>
