<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="876">Criss-cross algorithm</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Criss-cross algorithm</h1>
<hr/>

<p> In <a href="optimization_(mathematics)" title="wikilink">mathematical optimization</a>, the <strong>criss-cross algorithm</strong> denotes a family of <a href="algorithm" title="wikilink">algorithms</a> for <a href="linear_programming" title="wikilink">linear programming</a>. Variants of the criss-cross algorithm also solve more general problems with <a href="linear_programming" title="wikilink">linear inequality constraints</a> and <a href="nonlinear_programming" title="wikilink">nonlinear</a> <a href="optimization_(mathematics)" title="wikilink">objective functions</a>; there are criss-cross algorithms for <a href="linear-fractional_programming" title="wikilink">linear-fractional programming</a> problems,<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> <a href="quadratic_programming" title="wikilink">quadratic-programming</a> problems, and <a href="linear_complementarity_problem" title="wikilink">linear complementarity problems</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>Like the <a href="simplex_algorithm" title="wikilink">simplex algorithm</a> of <a href="George_Dantzig" title="wikilink">George B. Dantzig</a>, the criss-cross algorithm is not a <a href="time_complexity" title="wikilink">polynomial-time algorithm</a> for linear programming. Both algorithms visit all 2<sup><em>D</em></sup> corners of a (perturbed) <a href="unit_cube" title="wikilink">cube</a> in dimension <em>D</em>, the <a href="Klee–Minty_cube" title="wikilink">Klee–Minty cube</a> (after <a href="Victor_Klee" title="wikilink">Victor Klee</a> and <a href="George_J._Minty" title="wikilink">George J. Minty</a>), in the <a href="worst-case_complexity" title="wikilink">worst case</a>.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> However, when it is started at a random corner, the criss-cross algorithm <a href="Average-case_complexity" title="wikilink">on average</a> visits only <em>D</em> additional corners.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a><a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> Thus, for the three-dimensional cube, the algorithm visits all 8 corners in the worst case and exactly 3 additional corners on average.</p>
<h2 id="history">History</h2>

<p>The criss-cross algorithm was published independently by <a href="Tamás_Terlaky" title="wikilink">Tamás Terlaky</a><a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> and by Zhe-Min Wang;<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> related algorithms appeared in unpublished reports by other authors.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></p>
<h2 id="comparison-with-the-simplex-algorithm-for-linear-optimization">Comparison with the simplex algorithm for linear optimization</h2>

<p> In linear programming, the criss-cross algorithm pivots between a sequence of bases but differs from the <a href="simplex_algorithm" title="wikilink">simplex algorithm</a> of <a href="George_Dantzig" title="wikilink">George Dantzig</a>. The simplex algorithm first finds a (primal-) feasible basis by solving a "<em>phase-one</em> problem"; in "phase two", the simplex algorithm pivots between a sequence of basic ''feasible ''solutions so that the objective function is non-decreasing with each pivot, terminating when with an optimal solution (also finally finding a "dual feasible" solution).<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a><a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></p>

<p>The criss-cross algorithm is simpler than the simplex algorithm, because the criss-cross algorithm only has one-phase. Its pivoting rules are similar to the <a href="Bland's_rule" title="wikilink">least-index pivoting rule of Bland</a>.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> Bland's rule uses only <a href="sign_function" title="wikilink">signs</a> of coefficients rather than their <a href="real_number#Axiomatic_approach" title="wikilink">(real-number) order</a> when deciding eligible pivots. Bland's rule selects an entering variables by comparing values of reduced costs, using the real-number ordering of the eligible pivots.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a><a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> Unlike Bland's rule, the criss-cross algorithm is "purely combinatorial", selecting an entering variable and a leaving variable by considering only the signs of coefficients rather than their real-number ordering.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a><a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> The criss-cross algorithm has been applied to furnish constructive proofs of basic results in <a href="real_number" title="wikilink">real</a> <a href="linear_algebra" title="wikilink">linear algebra</a>, such as the <a href="Farkas_lemma" title="wikilink">lemma of Farkas</a>.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a></p>

<p>While most simplex variants are monotonic in the objective (strictly in the non-degenerate case), most variants of the criss-cross algorithm lack a monotone merit function which can be a disadvantage in practice.</p>
<h2 id="description">Description</h2>

<p>The criss-cross algorithm works on a standard pivot tableau (or on-the-fly calculated parts of a tableau, if implemented like the revised simplex method). In a general step, if the tableau is primal or dual infeasible, it selects one of the infeasible rows / columns as the pivot row / column using an index selection rule. An important property is that the selection is made on the union of the infeasible indices and the standard version of the algorithm does not distinguish column and row indices (that is, the column indices basic in the rows). If a row is selected then the algorithm uses the index selection rule to identify a position to a dual type pivot, while if a column is selected then it uses the index selection rule to find a row position and carries out a primal type pivot.</p>
<h2 id="computational-complexity-worst-and-average-cases">Computational complexity: Worst and average cases</h2>

<p> The <a href="time_complexity" title="wikilink">time complexity</a> of an <a class="uri" href="algorithm" title="wikilink">algorithm</a> counts the number of <a href="arithmetic_operation" title="wikilink">arithmetic operations</a> sufficient for the algorithm to solve the problem. For example, <a href="Gaussian_elimination" title="wikilink">Gaussian elimination</a> requires on the <a href="Big_oh" title="wikilink">order of</a><em> D</em><sup>3</sup> operations, and so it is said to have polynomial time-complexity, because its complexity is bounded by a <a href="cubic_polynomial" title="wikilink">cubic polynomial</a>. There are examples of algorithms that do not have polynomial-time complexity. For example, a generalization of Gaussian elimination called <a href="Buchberger's_algorithm" title="wikilink">Buchberger's algorithm</a> has for its complexity an exponential function of the problem data (the <a href="degree_of_a_polynomial" title="wikilink">degree of the polynomials</a> and the number of variables of the <a href="multivariate_polynomial" title="wikilink">multivariate polynomials</a>). Because exponential functions eventually grow much faster than polynomial functions, an exponential complexity implies that an algorithm has slow performance on large problems.</p>

<p>Several algorithms for linear programming—<a class="uri" href="Khachiyan" title="wikilink">Khachiyan</a>'s <a href="ellipsoidal_algorithm" title="wikilink">ellipsoidal algorithm</a>, <a class="uri" href="Karmarkar" title="wikilink">Karmarkar</a>'s <a href="Karmarkar's_algorithm" title="wikilink">projective algorithm</a>, and <a href="interior-point_method" title="wikilink">central-path algorithms</a>—have polynomial time-complexity (in the <a href="worst_case_complexity" title="wikilink">worst case</a> and thus <a href="average_case_complexity" title="wikilink">on average</a>). The ellipsoidal and projective algorithms were published before the criss-cross algorithm.</p>

<p>However, like the simplex algorithm of Dantzig, the criss-cross algorithm is <em>not</em> a polynomial-time algorithm for linear programming. Terlaky's criss-cross algorithm visits all the 2<sup><em>D</em></sup> corners of a (perturbed) cube in dimension <em>D</em>, according to a paper of Roos; Roos's paper modifies the <a href="Victor_Klee" title="wikilink">Klee</a>–Minty construction of a <a href="unit_cube" title="wikilink">cube</a> on which the simplex algorithm takes 2<sup><em>D</em></sup> steps.<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a><a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a><a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a> Like the simplex algorithm, the criss-cross algorithm visits all 8 corners of the three-dimensional cube in the worst case.</p>

<p>When it is initialized at a random corner of the cube, the criss-cross algorithm visits only <em>D</em> additional corners, however, according to a 1994 paper by Fukuda and Namiki.<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a><a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a> Trivially, the simplex algorithm takes on average <em>D</em> steps for a cube.<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a><a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a> Like the simplex algorithm, the criss-cross algorithm visits exactly 3 additional corners of the three-dimensional cube on average.</p>
<h2 id="variants">Variants</h2>

<p>The criss-cross algorithm has been extended to solve more general problems than linear programming problems.</p>
<h3 id="other-optimization-problems-with-linear-constraints">Other optimization problems with linear constraints</h3>

<p>There are variants of the criss-cross algorithm for linear programming, for <a href="quadratic_programming" title="wikilink">quadratic programming</a>, and for the <a href="linear_complementarity_problem" title="wikilink">linear-complementarity problem</a> with "sufficient matrices";<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a><a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a><a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a><a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a><a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a><a class="footnoteRef" href="#fn32" id="fnref32"><sup>32</sup></a> conversely, for linear complementarity problems, the criss-cross algorithm terminates finitely only if the matrix is a sufficient matrix.<a class="footnoteRef" href="#fn33" id="fnref33"><sup>33</sup></a><a class="footnoteRef" href="#fn34" id="fnref34"><sup>34</sup></a> A <a href="sufficient_matrix" title="wikilink">sufficient matrix</a> is a generalization both of a <a href="positive-definite_matrix" title="wikilink">positive-definite matrix</a> and of a <a class="uri" href="P-matrix" title="wikilink">P-matrix</a>, whose <a href="principal_minor" title="wikilink">principal minors</a> are each positive.<a class="footnoteRef" href="#fn35" id="fnref35"><sup>35</sup></a><a class="footnoteRef" href="#fn36" id="fnref36"><sup>36</sup></a><a class="footnoteRef" href="#fn37" id="fnref37"><sup>37</sup></a> The criss-cross algorithm has been adapted also for <a href="linear-fractional_programming" title="wikilink">linear-fractional programming</a>.<a class="footnoteRef" href="#fn38" id="fnref38"><sup>38</sup></a><a class="footnoteRef" href="#fn39" id="fnref39"><sup>39</sup></a></p>
<h3 id="vertex-enumeration">Vertex enumeration</h3>

<p>The criss-cross algorithm was used in an algorithm for <a href="Vertex_enumeration_problem" title="wikilink">enumerating all the vertices of a polytope</a>, which was published by <a href="David_Avis" title="wikilink">David Avis</a> and Komei Fukuda in 1992.<a class="footnoteRef" href="#fn40" id="fnref40"><sup>40</sup></a> Avis and Fukuda presented an algorithm which finds the <em>v</em> vertices of a <a class="uri" href="polyhedron" title="wikilink">polyhedron</a> defined by a nondegenerate system of <em>n</em> <a href="linear_inequality" title="wikilink">linear inequalities</a> in <em>D</em> <a href="dimension_(vector_space)" title="wikilink">dimensions</a> (or, dually, the <em>v</em> <a href="facet" title="wikilink">facets</a> of the <a href="convex_hull" title="wikilink">convex hull</a> of <em>n</em> points in <em>D</em> dimensions, where each facet contains exactly <em>D</em> given points) in time <a href="Big_Oh_notation" title="wikilink">O</a>(<em>nDv</em>) and O(<em>nD</em>) <a href="space_complexity" title="wikilink">space</a>.<a class="footnoteRef" href="#fn41" id="fnref41"><sup>41</sup></a></p>
<h3 id="oriented-matroids">Oriented matroids</h3>

<p> The criss-cross algorithm is often studied using the theory of <a href="oriented_matroid" title="wikilink">oriented matroids</a> (OMs), which is a <a href="combinatorics" title="wikilink">combinatorial</a> abstraction of linear-optimization theory.<a class="footnoteRef" href="#fn42" id="fnref42"><sup>42</sup></a><ref>The theory of <a href="oriented_matroid" title="wikilink">oriented matroids</a> was initiated by <a href="R._Tyrrell_Rockafellar" title="wikilink">R. Tyrrell Rockafellar</a>. :</ref></p>

<p>Rockafellar was influenced by the earlier studies of <a href="Albert_W._Tucker" title="wikilink">Albert W. Tucker</a> and <a href="George_J._Minty" title="wikilink">George J. Minty</a>. Tucker and Minty had studied the sign patterns of the matrices arising through the pivoting operations of Dantzig's simplex algorithm.</p>

<p> Indeed, Bland's pivoting rule was based on his previous papers on oriented-matroid theory. However, Bland's rule exhibits cycling on some oriented-matroid linear-programming problems.<a class="footnoteRef" href="#fn43" id="fnref43"><sup>43</sup></a> The first purely combinatorial algorithm for linear programming was devised by <a href="Michael_J._Todd_(mathematician)" title="wikilink">Michael J. Todd</a>.<a class="footnoteRef" href="#fn44" id="fnref44"><sup>44</sup></a><a class="footnoteRef" href="#fn45" id="fnref45"><sup>45</sup></a> Todd's algorithm was developed not only for linear-programming in the setting of oriented matroids, but also for <a href="quadratic_programming" title="wikilink">quadratic-programming problems</a> and <a href="linear_complementarity_problem" title="wikilink">linear-complementarity problems</a>.<a class="footnoteRef" href="#fn46" id="fnref46"><sup>46</sup></a><a class="footnoteRef" href="#fn47" id="fnref47"><sup>47</sup></a> Todd's algorithm is complicated even to state, unfortunately, and its finite-convergence proofs are somewhat complicated.<a class="footnoteRef" href="#fn48" id="fnref48"><sup>48</sup></a></p>

<p>The criss-cross algorithm and its proof of finite termination can be simply stated and readily extend the setting of oriented matroids. The algorithm can be further simplified for <em>linear feasibility problems</em>, that is for <a href="linear_system" title="wikilink">linear systems</a> with <a href="linear_inequality" title="wikilink">nonnegative variables</a>; these problems can be formulated for oriented matroids.<a class="footnoteRef" href="#fn49" id="fnref49"><sup>49</sup></a> The criss-cross algorithm has been adapted for problems that are more complicated than linear programming: There are oriented-matroid variants also for the quadratic-programming problem and for the linear-complementarity problem.<a class="footnoteRef" href="#fn50" id="fnref50"><sup>50</sup></a><a class="footnoteRef" href="#fn51" id="fnref51"><sup>51</sup></a><a class="footnoteRef" href="#fn52" id="fnref52"><sup>52</sup></a></p>
<h2 id="summary">Summary</h2>

<p>The criss-cross algorithm is a simply stated algorithm for linear programming. It was the second fully combinatorial algorithm for linear programming. The partially combinatorial simplex algorithm of Bland cycles on some (nonrealizable) oriented matroids. The first fully combinatorial algorithm was published by Todd, and it is also like the simplex algorithm in that it preserves feasibility after the first feasible basis is generated; however, Todd's rule is complicated. The criss-cross algorithm is not a simplex-like algorithm, because it need not maintain feasibility. The criss-cross algorithm does not have polynomial time-complexity, however.</p>

<p>Researchers have extended the criss-cross algorithm for many optimization-problems, including linear-fractional programming. The criss-cross algorithm can solve quadratic programming problems and linear complementarity problems, even in the setting of oriented matroids. Even when generalized, the criss-cross algorithm remains simply stated.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Jack_Edmonds" title="wikilink">Jack Edmonds</a> (pioneer of combinatorial optimization and oriented-matroid theorist; doctoral advisor of Komei Fukuda)</li>
</ul>
<h2 id="notes">Notes</h2>
<references>
</references>
<h2 id="references">References</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.ifor.math.ethz.ch/~fukuda/">Komei Fukuda (ETH Zentrum, Zurich)</a> with <a href="http://www.ifor.math.ethz.ch/~fukuda/publ/publ.html">publications</a></li>
<li><a href="http://coral.ie.lehigh.edu/~terlaky/">Tamás Terlaky (Lehigh University)</a> with <a href="http://coral.ie.lehigh.edu/~terlaky/publications">publications</a></li>
</ul>

<p>"</p>

<p><a href="Category:Linear_programming" title="wikilink">Category:Linear programming</a> <a href="Category:Oriented_matroids" title="wikilink">Category:Oriented matroids</a> <a href="Category:Combinatorial_optimization" title="wikilink">Category:Combinatorial optimization</a> <a href="Category:Optimization_algorithms_and_methods" title="wikilink">Category:Optimization algorithms and methods</a> <a href="Category:Combinatorial_algorithms" title="wikilink">Category:Combinatorial algorithms</a> <a href="Category:Geometric_algorithms" title="wikilink">Category:Geometric algorithms</a> <a href="Category:Exchange_algorithms" title="wikilink">Category:Exchange algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"></li>
<li id="fn6"></li>
<li id="fn7"></li>
<li id="fn8">The simplex algorithm takes on average <em>D</em> steps for a cube. : <a href="#fnref8">↩</a></li>
<li id="fn9"> and <a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"></li>
<li id="fn12"></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"><a href="#fnref14">↩</a></li>
<li id="fn15"></li>
<li id="fn16">Bland's rule is also related to an earlier least-index rule, which was proposed by Katta G. Murty for the <a href="linear_complementarity_problem" title="wikilink">linear complementarity problem</a>, according to .<a href="#fnref16">↩</a></li>
<li id="fn17"></li>
<li id="fn18"></li>
<li id="fn19"><a href="#fnref19">↩</a></li>
<li id="fn20"></li>
<li id="fn21"></li>
<li id="fn22"><a href="#fnref22">↩</a></li>
<li id="fn23"><a href="#fnref23">↩</a></li>
<li id="fn24"><a href="#fnref24">↩</a></li>
<li id="fn25"></li>
<li id="fn26">More generally, for the simplex algorithm, the expected number of steps is proportional to <em>D</em> for linear-programming problems that are randomly drawn from the <a href="Euclidean_metric" title="wikilink">Euclidean</a> <a href="unit_sphere" title="wikilink">unit sphere</a>, as proved by Borgwardt and by <a href="Stephen_Smale" title="wikilink">Smale</a>.<a href="#fnref26">↩</a></li>
<li id="fn27"></li>
<li id="fn28"></li>
<li id="fn29"><a href="#fnref29">↩</a></li>
<li id="fn30"><a href="#fnref30">↩</a></li>
<li id="fn31"><a href="#fnref31">↩</a></li>
<li id="fn32"><a href="#fnref32">↩</a></li>
<li id="fn33"></li>
<li id="fn34"></li>
<li id="fn35"></li>
<li id="fn36"></li>
<li id="fn37"><a href="#fnref37">↩</a></li>
<li id="fn38"></li>
<li id="fn39"></li>
<li id="fn40"><a href="#fnref40">↩</a></li>
<li id="fn41">The <em>v</em> vertices in a simple arrangement of <em>n</em> <a href="hyperplane" title="wikilink">hyperplanes</a> in <em>D</em> dimensions can be found in O(<em>n</em><sup>2</sup><em>Dv</em>) time and O(<em>nD</em>) <a href="space_complexity" title="wikilink">space complexity</a>.<a href="#fnref41">↩</a></li>
<li id="fn42"></li>
<li id="fn43"></li>
<li id="fn44"></li>
<li id="fn45"></li>
<li id="fn46"></li>
<li id="fn47"><a href="#fnref47">↩</a></li>
<li id="fn48"></li>
<li id="fn49"></li>
<li id="fn50"></li>
<li id="fn51"></li>
<li id="fn52"></li>
</ol>
</section>
</body>
</html>
