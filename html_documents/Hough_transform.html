<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="880">Hough transform</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Hough transform</h1>
<hr/>

<p>The <strong>Hough transform</strong> is a <a href="feature_extraction" title="wikilink">feature extraction</a> technique used in <a href="image_analysis" title="wikilink">image analysis</a>, <a href="computer_vision" title="wikilink">computer vision</a>, and <a href="digital_image_processing" title="wikilink">digital image processing</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> The purpose of the technique is to find imperfect instances of objects within a certain class of shapes by a voting procedure. This voting procedure is carried out in a <a href="parameter_space" title="wikilink">parameter space</a>, from which object candidates are obtained as local maxima in a so-called accumulator space that is explicitly constructed by the algorithm for computing the Hough transform.</p>

<p>The classical Hough transform was concerned with the identification of <a href="Line_(mathematics)" title="wikilink">lines</a> in the image, but later the Hough transform has been extended to identifying positions of arbitrary shapes, most commonly circles or ellipses. The Hough transform as it is universally used today was invented by <a href="Richard_Duda" title="wikilink">Richard Duda</a> and <a href="Peter_E._Hart" title="wikilink">Peter Hart</a> in 1972, who called it a "generalized Hough transform"<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> after the related 1962 patent of Paul Hough.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> The transform was popularized in the <a href="computer_vision" title="wikilink">computer vision</a> community by <a href="Dana_H._Ballard" title="wikilink">Dana H. Ballard</a> through a 1981 journal article titled "<a href="Generalised_Hough_Transform" title="wikilink">Generalizing the Hough transform to detect arbitrary shapes</a>".</p>
<h2 id="history">History</h2>

<p>It was initially invented for machine analysis of <a href="bubble_chamber" title="wikilink">bubble chamber</a> photographs (Hough, 1959).</p>

<p>The Hough transform was patented as  in 1962 and assigned to the U.S. Atomic Energy Commission with the name "Method and Means for Recognizing Complex Patterns". This patent uses a slope-intercept parametrization for straight lines, which awkwardly leads to an unbounded transform space since the slope can go to infinity.</p>

<p>The rho-theta parametrization universally used today was first described in</p>
<dl>
<dd>Duda, R. O. and P. E. Hart, "Use of the Hough Transformation to Detect Lines and Curves in Pictures," <em>Comm. ACM, Vol. 15</em>, pp. 11‚Äì15 (January, 1972),
</dd>
</dl>

<p>although it was already standard for the <a href="Radon_transform" title="wikilink">Radon transform</a> since at least the 1930s.</p>

<p>O'Gorman and Clowes' variation is described in</p>
<dl>
<dd>
</dd>
</dl>

<p>The story of how the modern form of the Hough transform was invented is given in</p>
<dl>
<dd>Hart, P. E., "<a href="http://www.iro.umontreal.ca/~mignotte/IFT6150/ComplementCours/HoughTransform.pdf">How the Hough Transform was Invented</a>" (PDF, 268 kB), <em>IEEE Signal Processing Magazine, Vol 26, Issue 6</em>, pp 18 - 22 (November, 2009).
</dd>
</dl>
<h2 id="theory">Theory</h2>

<p>In automated analysis of digital images, a subproblem often arises of detecting simple shapes, such as straight lines, circles or ellipses. In many cases an <a href="edge_detection" title="wikilink">edge detector</a> can be used as a pre-processing stage to obtain image points or image pixels that are on the desired curve in the image space. Due to imperfections in either the image data or the edge detector, however, there may be missing points or pixels on the desired curves as well as spatial deviations between the ideal line/circle/ellipse and the noisy edge points as they are obtained from the edge detector. For these reasons, it is often non-trivial to group the extracted edge features to an appropriate set of lines, circles or ellipses. The purpose of the Hough transform is to address this problem by making it possible to perform groupings of edge points into object candidates by performing an explicit voting procedure over a set of parameterized image objects (Shapiro and Stockman, 304).</p>

<p>The simplest case of Hough transform is detecting straight lines. In general, the straight line <em>y¬†=¬†mx¬†+¬†b</em> can be represented as a point (<em>b</em>, <em>m</em>) in the parameter space. However, vertical lines pose a problem. They would give rise to unbounded values of the slope parameter <em>m</em>. Thus, for computational reasons, Duda and Hart<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> proposed the use of the <a href="Hesse_normal_form" title="wikilink">Hesse normal form</a></p>

<p>

<math display="block" id="Hough_transform:0">
 <semantics>
  <mrow>
   <mi>r</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi>x</mi>
     <mrow>
      <mi>cos</mi>
      <mi>Œ∏</mi>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>y</mi>
     <mrow>
      <mi>sin</mi>
      <mi>Œ∏</mi>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>r</ci>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <ci>x</ci>
      <apply>
       <cos></cos>
       <ci>Œ∏</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>y</ci>
      <apply>
       <sin></sin>
       <ci>Œ∏</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r=x\cos\theta+y\sin\theta
  </annotation>
 </semantics>
</math>

 ,</p>

<p>where 

<math display="inline" id="Hough_transform:1">
 <semantics>
  <mi>r</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>r</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r
  </annotation>
 </semantics>
</math>

 is the distance from the <a href="Origin_(mathematics)" title="wikilink">origin</a> to the closest point on the straight line, and 

<math display="inline" id="Hough_transform:2">
 <semantics>
  <mi>Œ∏</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Œ∏</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta
  </annotation>
 </semantics>
</math>

 (<em>theta</em>) is the angle between the 

<math display="inline" id="Hough_transform:3">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 axis and the line connecting the origin with that closest point.</p>

<p> It is therefore possible to associate with each line of the image a pair 

<math display="inline" id="Hough_transform:4">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mi>r</mi>
   <mo>,</mo>
   <mi>Œ∏</mi>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="open">
    <ci>r</ci>
    <ci>Œ∏</ci>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (r,\theta)
  </annotation>
 </semantics>
</math>

. The 

<math display="inline" id="Hough_transform:5">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mi>r</mi>
   <mo>,</mo>
   <mi>Œ∏</mi>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="open">
    <ci>r</ci>
    <ci>Œ∏</ci>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (r,\theta)
  </annotation>
 </semantics>
</math>

 plane is sometimes referred to as <em>Hough space</em> for the set of straight lines in two dimensions. This representation makes the Hough transform conceptually very close to the two-dimensional <a href="Radon_transform" title="wikilink">Radon transform</a>. (They can be seen as different ways of looking at the same transform.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a>)</p>

<p>Given a <em>single point</em> in the plane, then the set of <em>all</em> straight lines going through that point corresponds to a <a href="Sine_wave" title="wikilink">sinusoidal</a> curve in the (<em>r,Œ∏</em>) plane, which is unique to that point. A set of two or more points that form a straight line will produce sinusoids which cross at the (<em>r,Œ∏</em>) for that line. Thus, the problem of detecting <a href="Line_(geometry)#Collinear_points" title="wikilink">collinear points</a> can be converted to the problem of finding <a href="Concurrent_lines" title="wikilink">concurrent</a> curves.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h2 id="implementation">Implementation</h2>

<p>The linear Hough transform <a class="uri" href="algorithm" title="wikilink">algorithm</a> uses a two-dimensional array, called an accumulator, to detect the existence of a line described by 

<math display="inline" id="Hough_transform:6">
 <semantics>
  <mrow>
   <mi>r</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mi>x</mi>
     <mrow>
      <mi>cos</mi>
      <mi>Œ∏</mi>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>y</mi>
     <mrow>
      <mi>sin</mi>
      <mi>Œ∏</mi>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>r</ci>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <ci>x</ci>
      <apply>
       <cos></cos>
       <ci>Œ∏</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>y</ci>
      <apply>
       <sin></sin>
       <ci>Œ∏</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   r=x\cos\theta+y\sin\theta
  </annotation>
 </semantics>
</math>

. The <a class="uri" href="dimension" title="wikilink">dimension</a> of the accumulator equals the number of unknown parameters, i.e., two, considering quantized values of r and Œ∏ in the pair (r,Œ∏). For each pixel at <em>(x,y)</em> and its neighborhood, the Hough transform algorithm determines if there is enough evidence of a straight line at that pixel. If so, it will calculate the parameters (r,Œ∏) of that line, and then look for the accumulator's bin that the parameters fall into, and increment the value of that bin. By finding the bins with the highest values, typically by looking for local maxima in the accumulator space, the most likely lines can be extracted, and their (approximate) geometric definitions read off. (Shapiro and Stockman, 304) The simplest way of finding these <em>peaks</em> is by applying some form of threshold, but other techniques may yield better results in different circumstances - determining which lines are found as well as how many. Since the lines returned do not contain any length information, it is often necessary, in the next step, to find which parts of the image match up with which lines. Moreover, due to imperfection errors in the edge detection step, there will usually be errors in the accumulator space, which may make it non-trivial to find the appropriate peaks, and thus the appropriate lines.</p>

<p>The final result of the linear Hough transform is a two-dimensional array (matrix) similar to the accumulator‚Äîone dimension of this matrix is the quantized angle Œ∏ and the other dimension is the quantized distance r. Each element of the matrix has a value equal to the number of points or pixels that are positioned on the line represented by quantized parameters (r, Œ∏). So the element with the highest value indicates the straight line that is most represented in the input image.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>
<h2 id="example">Example</h2>

<p>Consider three data points, shown here as black dots.</p>
<figure><b>(Figure)</b>
<figcaption>Hough transform diagram.png</figcaption>
</figure>
<ul>
<li>For each data point, a number of lines are plotted going through it, all at different angles. These are shown here as solid lines.</li>
<li>For each solid line a line is plotted which is <a class="uri" href="perpendicular" title="wikilink">perpendicular</a> to it and which intersects the <a href="Origin_(mathematics)" title="wikilink">origin</a>. These are shown as dashed lines.</li>
<li>The length (i.e. perpendicular distance to the origin) and angle of each dashed line is measured. In the diagram above, the results are shown in tables.</li>
<li>This is repeated for each data point.</li>
<li>A graph of the line lengths for each angle, known as a Hough space graph, is then created.</li>
</ul>
<figure><b>(Figure)</b>
<figcaption>Hough space plot example.png</figcaption>
</figure>

<p>The point where the curves intersect gives a distance and angle. This distance and angle indicate the line which intersects the points being tested. In the graph shown the lines intersect at the pink point; this corresponds to the solid pink line in the diagrams above, which passes through all three points.</p>

<p>The following is a different example showing the results of a Hough transform on a raster image containing two thick lines.</p>
<figure><b>(Figure)</b>
<figcaption>Hough-example-result-en.png</figcaption>
</figure>

<p>The results of this transform were stored in a matrix. Cell value represents the number of curves through any point. Higher cell values are rendered brighter. The two distinctly bright spots are the Hough parameters of the two lines. From these spots' positions, angle and distance from image center of the two lines in the input image can be determined.</p>
<h2 id="variations-and-extensions">Variations and extensions</h2>
<h3 id="using-the-gradient-direction-to-reduce-the-number-of-votes">Using the gradient direction to reduce the number of votes</h3>

<p>An improvement suggested by O'Gorman and Clowes can be used to detect lines if one takes into account that the local <a class="uri" href="gradient" title="wikilink">gradient</a> of the image intensity will necessarily be orthogonal to the edge. Since <a href="edge_detection" title="wikilink">edge detection</a> generally involves computing the intensity <a class="uri" href="gradient" title="wikilink">gradient</a> magnitude, the gradient direction is often found as a side effect. If a given point of coordinates (<em>x,y</em>) happens to indeed be on a line, then the local direction of the gradient gives the <em>Œ∏</em> parameter corresponding to said line, and the <em>r</em> parameter is then immediately obtained. (Shapiro and Stockman, 305) The gradient direction can be estimated to within 20¬∞, which shortens the sinusoid trace from the full 180¬∞ to roughly 45¬∞. This reduces the computation time and has the interesting effect of reducing the number of useless votes, thus enhancing the visibility of the spikes corresponding to real lines in the image.</p>
<h3 id="kernel-based-hough-transform-kht">Kernel-based Hough transform (KHT)</h3>

<p>Fernandes and Oliveira <a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> suggested an improved voting scheme for the Hough transform that allows a software implementation to achieve real-time performance even on relatively large images (e.g., 1280√ó960). The Kernel-based Hough transform uses the same 

<math display="inline" id="Hough_transform:7">
 <semantics>
  <mrow>
   <mo stretchy="false">(</mo>
   <mi>r</mi>
   <mo>,</mo>
   <mi>Œ∏</mi>
   <mo stretchy="false">)</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <interval closure="open">
    <ci>r</ci>
    <ci>Œ∏</ci>
   </interval>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   (r,\theta)
  </annotation>
 </semantics>
</math>

 parameterization proposed by Duda and Hart but operates on clusters of approximately collinear pixels. For each cluster, votes are cast using an oriented elliptical-Gaussian kernel that models the uncertainty associated with the best-fitting line with respect to the corresponding cluster. The approach not only significantly improves the performance of the voting scheme, but also produces a much cleaner accumulator and makes the transform more robust to the detection of spurious lines.</p>
<h3 id="d-kernel-based-hough-transform-for-plane-detection-3dkht">3-D Kernel-based Hough transform for plane detection (3DKHT)</h3>

<p>Limberger and Oliveira <a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> suggested a deterministic technique for plane detection in unorganized point clouds whose cost is 

<math display="inline" id="Hough_transform:8">
 <semantics>
  <mrow>
   <mi>n</mi>
   <mrow>
    <mi>log</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>n</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>n</ci>
    <apply>
     <log></log>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n\log(n)
  </annotation>
 </semantics>
</math>

 in the number of samples, achieving real-time performance for relatively large datasets (up to 

<math display="inline" id="Hough_transform:9">
 <semantics>
  <msup>
   <mn>10</mn>
   <mn>5</mn>
  </msup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <cn type="integer">10</cn>
    <cn type="integer">5</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   10^{5}
  </annotation>
 </semantics>
</math>

 points on a 3.4 GHz CPU). It is based on a fast Hough-transform voting strategy for planar regions, inspired by the Kernel-based Hough transform (KHT). This 3D Kernel-based Hough transform (3DKHT) uses a fast and robust algorithm to segment clusters of approximately co-planar samples, and casts votes for individual clusters (instead of for individual samples) on a (

<math display="inline" id="Hough_transform:10">
 <semantics>
  <mrow>
   <mi>Œ∏</mi>
   <mo>,</mo>
   <mi>œï</mi>
   <mo>,</mo>
   <mi>œÅ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <ci>Œ∏</ci>
    <ci>œï</ci>
    <ci>œÅ</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \theta,\phi,\rho
  </annotation>
 </semantics>
</math>

) spherical accumulator using a trivariate Gaussian kernel. The approach is several orders of magnitude faster than existing (non-deterministic) techniques for plane detection in point clouds, such as <a href="Randomized_Hough_Transform" title="wikilink">RHT</a> and <a class="uri" href="RANSAC" title="wikilink">RANSAC</a>, and scales better with the size of the datasets. It can be used with any application that requires fast detection of planar features on large datasets.</p>
<h3 id="hough-transform-of-curves-and-its-generalization-for-analytical-and-non-analytical-shapes">Hough transform of curves, and its generalization for analytical and non-analytical shapes</h3>

<p>Although the version of the transform described above applies only to finding straight lines, a similar transform can be used for finding any shape which can be represented by a set of parameters. A circle, for instance, can be transformed into a set of three parameters, representing its center and radius, so that the Hough space becomes three dimensional. Arbitrary ellipses and curves can also be found this way, as can any shape easily expressed as a set of parameters.</p>

<p>The generalization of the Hough transform for detecting analytical shapes in spaces having any dimensionality was proposed by Fernandes and Oliveira.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> In contrast to other Hough transform-based approaches for analytical shapes, Fernandes' technique does not depend on the shape one wants to detect nor on the input data type. The detection can be driven to a type of analytical shape by changing the assumed model of geometry where data have been encoded (e.g., <a href="euclidean_space" title="wikilink">euclidean space</a>, <a href="projective_space" title="wikilink">projective space</a>, <a href="conformal_geometry" title="wikilink">conformal geometry</a>, and so on), while the proposed formulation remains unchanged. Also, it guarantees that the intended shapes are represented with the smallest possible number of parameters, and it allows the concurrent detection of different kinds of shapes that best fit an input set of entries with different dimensionalities and different geometric definitions (e.g., the concurrent detection of planes and spheres that best fit a set of points, straight lines and circles).</p>

<p>For more complicated shapes in the plane (i.e., shapes that cannot be represented analytically in some 2D space), the <a href="Generalised_Hough_transform" title="wikilink">Generalised Hough transform</a> <a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> is used, which allows a feature to vote for a particular position, orientation and/or scaling of the shape using a predefined look-up table.</p>
<h3 id="circle-detection-process">Circle detection process</h3>

<p>The process of identifying possible <a href="Circle_Hough_Transform" title="wikilink">circular objects in Hough space</a> is relatively simple,</p>
<ul>
<li>First we create our accumulator space which is made up of a cell for each pixel, initially each of these will be set to 0.</li>
<li>For each(edge point in image(i, j)): Increment all cells which according to the equation of a circle( (i-a)¬≤ + (j-b)¬≤ = r¬≤ ) could be the center of a circle, these cells are represented by the letter 'a' in the equation.</li>
<li>For all possible value of a found in the previous step, find all possible values of b which satisfy the equation.</li>
<li>Search for the local maxima cells, these are any cells whose value is greater than every other cell in its neighbourhood. These cells are the one with the highest probability of being the location of the circle(s) we are trying to locate.</li>
</ul>

<p>Note that in most problems we will know the radius of the circle we are trying to locate beforehand, however if this is not the case we can use a three-dimensional accumulator space, this is much more computationally expensive. This method can also detect circles that are partially outside of the accumulator space if enough of its area is still present within it.</p>
<h3 id="detection-of-3d-objects-planes-and-cylinders">Detection of 3D objects (Planes and cylinders)</h3>

<p>Hough transform can also be used for the detection of 3D objects in range data or 3D <a href="point_cloud" title="wikilink">point clouds</a>. The extension of classical Hough transform for plane detection is quite straightforward. A plane is represented by its explicit equation 

<math display="inline" id="Hough_transform:11">
 <semantics>
  <mrow>
   <mi>z</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <msub>
      <mi>a</mi>
      <mi>x</mi>
     </msub>
     <mi>x</mi>
    </mrow>
    <mo>+</mo>
    <mrow>
     <msub>
      <mi>a</mi>
      <mi>y</mi>
     </msub>
     <mi>y</mi>
    </mrow>
    <mo>+</mo>
    <mi>d</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>z</ci>
    <apply>
     <plus></plus>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>a</ci>
       <ci>x</ci>
      </apply>
      <ci>x</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>a</ci>
       <ci>y</ci>
      </apply>
      <ci>y</ci>
     </apply>
     <ci>d</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   z=a_{x}x+a_{y}y+d
  </annotation>
 </semantics>
</math>

 for which we can use a 3D Hough space corresponding to 

<math display="inline" id="Hough_transform:12">
 <semantics>
  <msub>
   <mi>a</mi>
   <mi>x</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>a</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a_{x}
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Hough_transform:13">
 <semantics>
  <msub>
   <mi>a</mi>
   <mi>y</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>a</ci>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a_{y}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Hough_transform:14">
 <semantics>
  <mi>d</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>d</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   d
  </annotation>
 </semantics>
</math>

. This extension suffers from the same problems as its 2D counterpart i.e., near horizontal planes can be reliably detected, while the performance deteriorates as planar direction becomes vertical (big values of 

<math display="inline" id="Hough_transform:15">
 <semantics>
  <msub>
   <mi>a</mi>
   <mi>x</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>a</ci>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a_{x}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Hough_transform:16">
 <semantics>
  <msub>
   <mi>a</mi>
   <mi>y</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>a</ci>
    <ci>y</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a_{y}
  </annotation>
 </semantics>
</math>

 amplify the noise in the data). This formulation of the plane has been used for the detection of planes in the <a href="point_cloud" title="wikilink">point clouds</a> acquired from airborne laser scanning <a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> and works very well because in that domain all planes are nearly horizontal.</p>

<p>For generalized plane detection using Hough transform, the plane can be parametrized by its normal vector 

<math display="inline" id="Hough_transform:17">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

 (using spherical coordinates) and its distance from the origin 

<math display="inline" id="Hough_transform:18">
 <semantics>
  <mi>œÅ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>œÅ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \rho
  </annotation>
 </semantics>
</math>

 resulting in a three dimensional Hough space. This results in each point in the input data voting for a sinusoidal surface in the Hough space. The intersection of these sinusoidal surfaces indicates presence of a plane.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> A more general approach for more than 3 dimensions requires search heuristics to remain feasible.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>

<p>Hough transform has also been used to find cylindrical objects in <a href="point_cloud" title="wikilink">point clouds</a> using a two step approach. The first step finds the orientation of the cylinder and the second step finds the position and radius.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a></p>
<h3 id="using-weighted-features">Using weighted features</h3>

<p>One common variation detail. That is, finding the bins with the highest count in one stage can be used to constrain the range of values searched in the next.</p>
<h3 id="carefully-chosen-parameter-space">Carefully chosen parameter space</h3>

<p>A high-dimensional parameter space for the Hough transform is not only slow, but if implemented without forethought can easily overrun the available memory. Even if the programming environment allows the allocation of an array larger than the available memory space through virtual memory, the number of page swaps required for this will be very demanding because the accumulator array is used in a randomly accessed fashion, rarely stopping in contiguous memory as it skips from index to index.</p>

<p>Consider the task of finding ellipses in an 800x600 image. Assuming that the radii of the ellipses are oriented along principal axes, the parameter space is four-dimensional. (x,y) defines the center of the ellipse, and a and b denote the two radii. Allowing the center to be anywhere in the image, adds the constraint 0<x< p="">

<p>As discussed in the algorithm (on page 2 of the paper), this approach uses only a one-dimensional accumulator (for the minor axis) in order to detect ellipses in the image. The complexity is O(N<sup>3</sup>) in the number of non-zero points in the image.</p>
<h2 id="limitations">Limitations</h2>

<p>The Hough transform is only efficient if a high number of votes fall in the right bin, so that the bin can be easily detected amid the background noise. This means that the bin must not be too small, or else some votes will fall in the neighboring bins, thus reducing the visibility of the main bin.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></p>

<p>Also, when the number of parameters is large (that is, when we are using the Hough transform with typically more than three parameters), the average number of votes cast in a single bin is very low, and those bins corresponding to a real figure in the image do not necessarily appear to have a much higher number of votes than their neighbors. The complexity increases at a rate of 

<math display="inline" id="Hough_transform:19">
 <semantics>
  <mrow>
   <mi class="ltx_font_mathcaligraphic">ùí™</mi>
   <mrow>
    <mo>(</mo>
    <msup>
     <mi>A</mi>
     <mrow>
      <mi>m</mi>
      <mo>-</mo>
      <mn>2</mn>
     </mrow>
    </msup>
    <mo>)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>ùí™</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>A</ci>
     <apply>
      <minus></minus>
      <ci>m</ci>
      <cn type="integer">2</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{O}\left({A^{m-2}}\right)
  </annotation>
 </semantics>
</math>

 with each additional parameter, where 

<math display="inline" id="Hough_transform:20">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A
  </annotation>
 </semantics>
</math>

 is the size of the image space and 

<math display="inline" id="Hough_transform:21">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

 is the number of parameters. (Shapiro and Stockman, 310) Thus, the Hough transform must be used with great care to detect anything other than lines or circles.</p>

<p>Finally, much of the efficiency of the Hough transform is dependent on the quality of the input data: the edges must be detected well for the Hough transform to be efficient. Use of the Hough transform on noisy images is a very delicate matter and generally, a denoising stage must be used before. In the case where the image is corrupted by speckle, as is the case in radar images, the <a href="Radon_transform" title="wikilink">Radon transform</a> is sometimes preferred to detect lines, because it attenuates the noise through summation.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Generalised_Hough_transform" title="wikilink">Generalised Hough transform</a></li>
<li><a href="Randomized_Hough_transform" title="wikilink">Randomized Hough transform</a></li>
<li><a href="Radon_transform" title="wikilink">Radon transform</a></li>
<li><a href="Fourier_transform" title="wikilink">Fourier transform</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://cimg.sourceforge.net/screenshots.shtml">hough_transform.cpp</a> - C++ code - example of CImg library (<a href="Open_source_software" title="wikilink">open source</a> library, <a class="uri" href="C++" title="wikilink">C++</a> source code, <a class="uri" href="Grayscale" title="wikilink">Grayscale</a> images)</li>
<li><a href="http://matlabtricks.com/post-39/understanding-the-hough-transform">Interactive Demonstration on the Basics of the Hough Transform</a></li>
<li><a class="uri" href="http://www.rob.cs.tu-bs.de/content/04-teaching/06-interactive/Hough.html">http://www.rob.cs.tu-bs.de/content/04-teaching/06-interactive/Hough.html</a> - <a href="Java_Applet" title="wikilink">Java Applet</a> + Source for learning the Hough transformation in slope-intercept form</li>
<li><a class="uri" href="http://www.rob.cs.tu-bs.de/content/04-teaching/06-interactive/HNF.html">http://www.rob.cs.tu-bs.de/content/04-teaching/06-interactive/HNF.html</a> - <a href="Java_Applet" title="wikilink">Java Applet</a> + Source for learning the Hough-Transformation in normal form</li>
<li><a class="uri" href="http://www.sydlogan.com/deskew.html">http://www.sydlogan.com/deskew.html</a> - Deskew images using Hough transform (<a class="uri" href="Grayscale" title="wikilink">Grayscale</a> images, <a class="uri" href="C++" title="wikilink">C++</a> source code)</li>
<li><a class="uri" href="http://imaging.gmse.net/articledeskew.html">http://imaging.gmse.net/articledeskew.html</a> - Deskew images using Hough transform (<a href="Visual_Basic" title="wikilink">Visual Basic</a> source code)</li>
<li><a class="uri" href="http://www.mitov.com/products/visionlab">http://www.mitov.com/products/visionlab</a> - <a href="Embarcadero_Delphi" title="wikilink">Delphi</a>, <a class="uri" href="C++" title="wikilink">C++</a> and <a href=".NET_Framework" title="wikilink">.NET</a> free for educational purposes library containing Line, Circle and Line segment Hough transform components.</li>
<li><a href="http://www.isprs.org/proceedings/XXXVI/3-W52/final_papers/Tarsha-Kurdi_2007.pdf">Tarsha-Kurdi, F., Landes, T., Grussenmeyer, P., 2007a. Hough-transform and extended RANSAC algorithms for automatic detection of 3d building roof planes from Lidar data.</a> ISPRS Proceedings. Workshop Laser scanning. Espoo, Finland, September 12‚Äì14, 2007.</li>
<li><a href="http://intopii.com/into/">Into</a> contains open source implementations of linear and circular Hough transform in C++</li>
<li><a class="uri" href="http://www.vision.ime.usp.br/~edelgado/defesa/code/hough.html">http://www.vision.ime.usp.br/~edelgado/defesa/code/hough.html</a> Hough-transform for Ellipse detection, implemented in C.</li>
<li><a href="http://scikit-image.org/docs/dev/api/skimage.transform.html">scikit-image</a> Hough-transform for line, circle and ellipse, implemented in Python.</li>
<li><a href="http://www.mathworks.com/matlabcentral/fileexchange/40537-wavelet-based-circular-hough-transform">1</a> Hough transform based on wavelet filtering, to detect a circle of a particular radius. (Matlab code.)</li>
<li><a href="http://www.mathworks.com/help/images/detect-lines-in-images.html">Hough transform for lines using MATLAB</a></li>
<li><a href="http://www.mathworks.com/help/images/ref/imfindcircles.html">Hough transform for circles and ellipses in MATLAB</a></li>
<li><a href="http://www2.ic.uff.br/~laffernandes/projects/kht/">KHT</a> - C++ source code.</li>
<li><a href="http://www.inf.ufrgs.br/~oliveira/pubs_files/HT3D/HT3D_page.html">3DKHT</a> - C++ source code and datasets.</li>
</ul>

<p>"</p>

<p><a href="Category:Feature_detection_(computer_vision)" title="wikilink">Category:Feature detection (computer vision)</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Shapiro, Linda and Stockman, George. "Computer Vision", Prentice-Hall, Inc. 2001<a href="#fnref1">‚Ü©</a></li>
<li id="fn2">Duda, R. O. and P. E. Hart, "Use of the Hough Transformation to Detect Lines and Curves in Pictures," <em>Comm. ACM, Vol. 15</em>, pp. 11‚Äì15 (January, 1972)<a href="#fnref2">‚Ü©</a></li>
<li id="fn3">Hough, P.V.C. <em>Method and means for recognizing complex patterns,</em> U.S. Patent 3,069,654, Dec. 18, 1962<a href="#fnref3">‚Ü©</a></li>
<li id="fn4">P.V.C. Hough, <em>Machine Analysis of Bubble Chamber Pictures,</em> Proc. Int. Conf. High Energy Accelerators and Instrumentation, 1959<a href="#fnref4">‚Ü©</a></li>
<li id="fn5"><a href="#fnref5">‚Ü©</a></li>
<li id="fn6"><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.2.9419">CiteSeerX ‚Äî A short introduction to the Radon and Hough transforms and how they relate to each other</a><a href="#fnref6">‚Ü©</a></li>
<li id="fn7"><a href="#fnref7">‚Ü©</a></li>
<li id="fn8"><a href="#fnref8">‚Ü©</a></li>
<li id="fn9"><a href="#fnref9">‚Ü©</a></li>
<li id="fn10"><a href="#fnref10">‚Ü©</a></li>
<li id="fn11"><a href="#fnref11">‚Ü©</a></li>
<li id="fn12"><a href="#fnref12">‚Ü©</a></li>
<li id="fn13">Vosselman, G., Dijkman, S: "3D Building Model Reconstruction from Point Clouds and Ground Plans", International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, vol 34, part 3/W4, October 22‚Äì24, 2001, Annapolis, MA, USA, pp.37- 44.<a href="#fnref13">‚Ü©</a></li>
<li id="fn14">Tahir Rabbani: "Automatic reconstruction of industrial installations - Using point clouds and images", page 43-44, Publications on Geodesy 62, Delft, 2006. ISBN 978-90-6132-297-9 <a class="uri" href="http://www.ncg.knaw.nl/Publicaties/Geodesy/62Rabbani.html">http://www.ncg.knaw.nl/Publicaties/Geodesy/62Rabbani.html</a><a href="#fnref14">‚Ü©</a></li>
<li id="fn15"><a href="#fnref15">‚Ü©</a></li>
<li id="fn16">Tahir Rabbani and Frank van den Heuvel, "Efficient hough transform for automatic detection of cylinders in point clouds" in Proceedings of the 11th Annual Conference of the Advanced School for Computing and Imaging (ASCI '05), The Netherlands, June 2005.<a href="#fnref16">‚Ü©</a></li>
<li id="fn17"><a href="#fnref17">‚Ü©</a></li>
</ol>
</section>
</x<></p></body>
</html>
