<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1231">Recursive neural network</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Recursive neural network</h1>
<hr/>

<p>A <strong>recursive neural network</strong> (RNN) is a kind of <a href="deep_learning#Deep_neural_networks" title="wikilink">deep neural network</a> created by applying the same set of weights <a href="recursion" title="wikilink">recursively</a> over a structure, to produce a <a href="structured_prediction" title="wikilink">structured prediction</a> over variable-length input, or a scalar prediction on it, by traversing a given structure in <a href="topological_sort" title="wikilink">topological order</a>. RNNs have been successful in learning sequence and tree structures in <a href="natural_language_processing" title="wikilink">natural language processing</a>, mainly phrase and sentence continuous representations based on <a href="word_embedding" title="wikilink">word embedding</a>. RNNs have first been introduced to learn <a href="distributed_representation" title="wikilink">distributed representations</a> of structure, such as <a href="mathematical_logic" title="wikilink">logical terms</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="architectures">Architectures</h2>
<h3 id="basic-architecture">Basic Architecture</h3>
<figure><b>(Figure)</b>
<figcaption>A simple recursive neural network architecture</figcaption>
</figure>

<p>In the most simple architecture, nodes are combined into parents using a weight matrix that is shared across the whole network, and a non-linearity such as <em><a class="uri" href="tanh" title="wikilink">tanh</a></em>. If <em>c</em><sub>1</sub> and <em>c</em><sub>2</sub> are <em>n</em>-dimensional vector representation of nodes, their parent will also be an <em>n</em>-dimensional vector, calculated as</p>

<p>

<math display="inline" id="Recursive_neural_network:0">
 <semantics>
  <mrow>
   <msub>
    <mi>p</mi>
    <mrow>
     <mn>1</mn>
     <mo>,</mo>
     <mn>2</mn>
    </mrow>
   </msub>
   <mo>=</mo>
   <mrow>
    <mi>tanh</mi>
    <mrow>
     <mo>(</mo>
     <mrow>
      <mi>W</mi>
      <mrow>
       <mo stretchy="false">[</mo>
       <msub>
        <mi>c</mi>
        <mn>1</mn>
       </msub>
       <mo>;</mo>
       <msub>
        <mi>c</mi>
        <mn>2</mn>
       </msub>
       <mo stretchy="false">]</mo>
      </mrow>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>p</ci>
     <list>
      <cn type="integer">1</cn>
      <cn type="integer">2</cn>
     </list>
    </apply>
    <apply>
     <tanh></tanh>
     <apply>
      <times></times>
      <ci>W</ci>
      <list>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>c</ci>
        <cn type="integer">1</cn>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>c</ci>
        <cn type="integer">2</cn>
       </apply>
      </list>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p_{1,2}=\tanh\left(W[c_{1};c_{2}]\right)
  </annotation>
 </semantics>
</math>

</p>

<p>Where <em>W</em> is a learned 

<math display="inline" id="Recursive_neural_network:1">
 <semantics>
  <mrow>
   <mrow>
    <mi>n</mi>
    <mo>×</mo>
    <mn>2</mn>
   </mrow>
   <mi>n</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <times></times>
     <ci>n</ci>
     <cn type="integer">2</cn>
    </apply>
    <ci>n</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n\times 2n
  </annotation>
 </semantics>
</math>

 weight matrix.</p>

<p>This architecture, with a few improvements, has been used for successfully parsing natural scenes and for syntactic parsing of natural language sentences.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<h3 id="recursive-neural-tensor-network">Recursive Neural Tensor Network</h3>

<p>These networks use a single, tensor-based composition function for all nodes in the tree.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h2 id="training">Training</h2>
<h3 id="stochastic-gradient-descent">Stochastic gradient descent</h3>

<p>Typically, <a href="stochastic_gradient_descent" title="wikilink">stochastic gradient descent</a> (SGD) is used to train the network. The gradient is computed using <a href="backpropagation_through_structure" title="wikilink">backpropagation through structure</a> (BPTS), a variant of <a href="backpropagation_through_time" title="wikilink">backpropagation through time</a> used for <a href="recurrent_neural_networks" title="wikilink">recurrent neural networks</a>.</p>
<h2 id="related-models">Related models</h2>

<p><a href="Recurrent_neural_network" title="wikilink">Recurrent neural networks</a> are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.</p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Artificial_intelligence" title="wikilink">Category:Artificial intelligence</a> <a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
</ol>
</section>
</body>
</html>
