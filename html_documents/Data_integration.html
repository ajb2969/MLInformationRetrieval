<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1761">Data integration</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Data integration</h1>
<hr/>

<p><strong>Data integration</strong> involves combining <a class="uri" href="data" title="wikilink">data</a> residing in different sources and providing users with a unified view of these data.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> This process becomes significant in a variety of situations, which include both commercial (when two similar companies need to merge their <a href="database" title="wikilink">databases</a>) and scientific (combining research results from different <a class="uri" href="bioinformatics" title="wikilink">bioinformatics</a> repositories, for example) domains. Data integration appears with increasing frequency as the volume and the need to share existing data <a href="Information_explosion" title="wikilink">explodes</a>.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> It has become the focus of extensive theoretical work, and numerous open problems remain unsolved.</p>
<h2 id="history">History</h2>
<figure><b>(Figure)</b>
<figcaption>Figure 1: Simple schematic for a data warehouse. The <a href="Extract,_transform,_load" title="wikilink">ETL</a> process extracts information from the source databases, transforms it and then loads it into the data warehouse.</figcaption>
</figure>
<figure><b>(Figure)</b>
<figcaption>Figure 2: Simple schematic for a data-integration solution. A system designer constructs a mediated schema against which users can run queries. The <a href="virtual_database" title="wikilink">virtual database</a> interfaces with the source databases via <a href="Wrapper_pattern" title="wikilink">wrapper</a> code if required.</figcaption>
</figure>

<p>Issues with combining <a class="uri" href="heterogeneous" title="wikilink">heterogeneous</a> data sources, often referred to as <a href="information_silo" title="wikilink">information silos</a>, under a single query interface have existed for some time. In the early 1980s, computer scientists began designing systems for interoperability of heterogeneous databases.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> The first data integration system driven by structured metadata was designed at the <a href="University_of_Minnesota" title="wikilink">University of Minnesota</a> in 1991, for the <a href="IPUMS" title="wikilink">Integrated Public Use Microdata Series (IPUMS)</a>. IPUMS used a <a href="data_warehousing" title="wikilink">data warehousing</a> approach, which <a href="Extract,_transform,_load" title="wikilink">extracts, transforms, and loads</a> data from heterogeneous sources into a single view <a href="logical_schema" title="wikilink">schema</a> so data from different sources become compatible.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> By making thousands of population databases interoperable, IPUMS demonstrated the feasibility of large-scale data integration. The data warehouse approach offers a <a href="Coupling_(computer_science)" title="wikilink">tightly coupled</a> architecture because the data are already physically reconciled in a single queryable repository, so it usually takes little time to resolve queries.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>

<p>The data warehouse approach is less feasible for datasets that are frequently updated, requiring the <a href="Extract,_transform,_load" title="wikilink">ETL</a> process to be continuously re-executed for synchronization. Difficulties also arise in constructing data warehouses when one has only a query interface to summary data sources and no access to the full data. This problem frequently emerges when integrating several commercial query services like travel or classified advertisement web applications.</p>

<p>the trend in data integration has favored loosening the coupling between data and providing a unified query-interface to access real time data over a <a href="data_mediation" title="wikilink">mediated</a> schema (see figure 2), which allows information to be retrieved directly from original databases. This approach relies on mappings between the mediated schema and the schema of original sources, and transform a query into specialized queries to match the schema of the original databases. Such mappings can be specified in 2 ways : as a mapping from entities in the mediated schema to entities in the original sources (the "<a href="Global_As_View" title="wikilink">Global As View</a>" (GAV) approach), or as a mapping from entities in the original sources to the mediated schema (the "<a href="Local_As_View" title="wikilink">Local As View</a>" (LAV) approach). The latter approach requires more sophisticated inferences to resolve a query on the mediated schema, but makes it easier to add new data sources to a (stable) mediated schema.</p>

<p>some of the work in data integration research concerns the <a href="semantic_integration" title="wikilink">semantic integration</a> problem. This problem addresses not the structuring of the architecture of the integration, but how to resolve <a class="uri" href="semantic" title="wikilink">semantic</a> conflicts between heterogeneous data sources. For example, if two companies merge their databases, certain concepts and definitions in their respective schemas like "earnings" inevitably have different meanings. In one database it may mean profits in dollars (a floating-point number), while in the other it might represent the number of sales (an integer). A common strategy for the resolution of such problems involves the use of <a href="ontology_(computer_science)" title="wikilink">ontologies</a> which explicitly define schema terms and thus help to resolve semantic conflicts. This approach represents <a href="ontology_based_data_integration" title="wikilink">ontology-based data integration</a>. On the other hand, the problem of combining research results from different bioinformatics repositories requires bench-marking of the similarities, computed from different data sources, on a single criterion such as positive predictive value. This enables the data sources to be directly comparable and can be integrated even when the natures of experiments are distinct.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a>  it was determined that current <a href="data_modeling" title="wikilink">data modeling</a> methods were imparting data isolation into every <a href="data_architecture" title="wikilink">data architecture</a> in the form of islands of disparate data and <a href="information_silo" title="wikilink">information silos</a>. This data isolation is an unintended artifact of the data modeling methodology that results in the development of disparate data models. Disparate data models, when instantiated as databases, form disparate databases. Enhanced data model methodologies have been developed to eliminate the data isolation artifact and to promote the development of integrated data models.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a><a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> One enhanced data modeling method recasts data models by augmenting them with structural <a class="uri" href="metadata" title="wikilink">metadata</a> in the form of standardized data entities. As a result of recasting multiple data models, the set of recast data models will now share one or more commonality relationships that relate the structural metadata now common to these data models. Commonality relationships are a peer-to-peer type of entity relationships that relate the standardized data entities of multiple data models. Multiple data models that contain the same standard data entity may participate in the same commonality relationship. When integrated data models are instantiated as databases and are properly populated from a common set of master data, then these databases are integrated.</p>
<h2 id="example">Example</h2>

<p>Consider a <a href="web_application" title="wikilink">web application</a> where a user can query a variety of information about cities (such as crime statistics, weather, hotels, demographics, etc.). Traditionally, the information must be stored in a single database with a single schema. But any single enterprise would find information of this breadth somewhat difficult and expensive to collect. Even if the resources exist to gather the data, it would likely duplicate data in existing crime databases, weather websites, and census data.</p>

<p>A data-integration solution may address this problem by considering these external resources as <a href="materialized_view" title="wikilink">materialized views</a> over a <a href="Virtual_database" title="wikilink">virtual mediated schema</a>, resulting in "virtual data integration". This means application-developers construct a virtual schema — the <em>mediated schema</em> — to best model the kinds of answers their users want. Next, they design "wrappers" or adapters for each data source, such as the crime database and weather website. These adapters simply transform the local query results (those returned by the respective websites or databases) into an easily processed form for the data integration solution (see figure 2). When an application-user queries the mediated schema, the data-integration solution transforms this query into appropriate queries over the respective data sources. Finally, the virtual database combines the results of these queries into the answer to the user's query.</p>

<p>This solution offers the convenience of adding new sources by simply constructing an adapter or an application software blade for them. It contrasts with <a href="Extract,_transform,_load" title="wikilink">ETL</a> systems or with a single database solution, which require manual integration of entire new dataset into the system. The virtual ETL solutions leverage <a href="Virtual_database" title="wikilink">virtual mediated schema</a> to implement data harmonization; whereby the data are copied from the designated "master" source to the defined targets, field by field. Advanced <a href="Data_virtualization" title="wikilink">Data virtualization</a> is also built on the concept of object-oriented modeling in order to construct virtual mediated schema or virtual metadata repository, using <a href="hub_and_spoke" title="wikilink">hub and spoke</a> architecture.</p>

<p>Each data source is disparate and as such is not designed to support reliable joins between data sources. Therefore, data virtualization as well as data federation depends upon accidental data commonality to support combining data and information from disparate data sets. Because of this lack of data value commonality across data sources, the return set may be inaccurate, incomplete, and impossible to validate.</p>

<p>One solution is to recast disparate databases to integrate these databases without the need for <a href="Extract,_transform,_load" title="wikilink">ETL</a>. The recast databases support commonality constraints where referential integrity may be enforced between databases. The recast databases provide designed data access paths with data value commonality across databases.</p>
<h2 id="theory-of-data-integration">Theory of data integration</h2>

<p>The theory of data integration<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> forms a subset of database theory and formalizes the underlying concepts of the problem in <a href="first-order_logic" title="wikilink">first-order logic</a>. Applying the theories gives indications as to the feasibility and difficulty of data integration. While its definitions may appear abstract, they have sufficient generality to accommodate all manner of integration systems.</p>
<h3 id="definitions">Definitions</h3>

<p>Data integration systems are formally defined as a <a href="Triple_(mathematics)" title="wikilink">triple</a> 

<math display="inline" id="Data_integration:0">
 <semantics>
  <mrow>
   <mo>⟨</mo>
   <mi>G</mi>
   <mo>,</mo>
   <mi>S</mi>
   <mo>,</mo>
   <mi>M</mi>
   <mo>⟩</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <ci>G</ci>
    <ci>S</ci>
    <ci>M</ci>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \left\langle G,S,M\right\rangle
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Data_integration:1">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>

 is the global (or mediated) schema, 

<math display="inline" id="Data_integration:2">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>


 is the heterogeneous set of source schemas, and 

<math display="inline" id="Data_integration:3">
 <semantics>
  <mi>M</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>M</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   M
  </annotation>
 </semantics>
</math>

 is the mapping that maps queries between the source and the global schemas. Both 

<math display="inline" id="Data_integration:4">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Data_integration:5">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

 are expressed in <a href="formal_language" title="wikilink">languages</a> over <a href="alphabet_(computer_science)" title="wikilink">alphabets</a> composed of symbols for each of their respective <a href="Relational_database" title="wikilink">relations</a>. The <a href="Functional_predicate" title="wikilink">mapping</a> 

<math display="inline" id="Data_integration:6">
 <semantics>
  <mi>M</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>M</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   M
  </annotation>
 </semantics>
</math>

 consists of assertions between queries over 

<math display="inline" id="Data_integration:7">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>


 and queries over 

<math display="inline" id="Data_integration:8">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

. When users pose queries over the data integration system, they pose queries over 

<math display="inline" id="Data_integration:9">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>

 and the mapping then asserts connections between the elements in the global schema and the source schemas.</p>

<p>A database over a schema is defined as a set of sets, one for each relation (in a relational database). The database corresponding to the source schema 

<math display="inline" id="Data_integration:10">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

 would comprise the set of sets of tuples for each of the heterogeneous data sources and is called the <em>source database</em>. Note that this single source database may actually represent a collection of disconnected databases. The database corresponding to the virtual mediated schema 

<math display="inline" id="Data_integration:11">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>

 is called the <em>global database</em>. The global database must satisfy the mapping 

<math display="inline" id="Data_integration:12">
 <semantics>
  <mi>M</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>M</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   M
  </annotation>
 </semantics>
</math>


 with respect to the source database. The legality of this mapping depends on the nature of the correspondence between 

<math display="inline" id="Data_integration:13">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Data_integration:14">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

. Two popular ways to model this correspondence exist: <em>Global as View</em> or GAV and <em>Local as View</em> or LAV.</p>
<figure><b>(Figure)</b>
<figcaption>Figure 3: Illustration of tuple space of the GAV and LAV mappings.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> In GAV, the system is constrained to the set of tuples mapped by the mediators while the set of tuples expressible over the sources may be much larger and richer. In LAV, the system is constrained to the set of tuples in the sources while the set of tuples expressible over the global schema can be much larger. Therefore, LAV systems must often deal with incomplete answers.</figcaption>
</figure>

<p>GAV systems model the global database as a set of <a href="view_(database)" title="wikilink">views</a> over 

<math display="inline" id="Data_integration:15">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

. In this case 

<math display="inline" id="Data_integration:16">
 <semantics>
  <mi>M</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>M</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   M
  </annotation>
 </semantics>
</math>

 associates to each element of 

<math display="inline" id="Data_integration:17">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>


 as a query over 

<math display="inline" id="Data_integration:18">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

. <a href="Query_optimizer" title="wikilink">Query processing</a> becomes a straightforward operation due to the well-defined associations between 

<math display="inline" id="Data_integration:19">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Data_integration:20">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

. The burden of complexity falls on implementing mediator code instructing the data integration system exactly how to retrieve elements from the source databases. If any new sources join the system, considerable effort may be necessary to update the mediator, thus the GAV approach appears preferable when the sources seem unlikely to change.</p>

<p>In a GAV approach to the example data integration system above, the system designer would first develop mediators for each of the city information sources and then design the global schema around these mediators. For example, consider if one of the sources served a weather website. The designer would likely then add a corresponding element for weather to the global schema. Then the bulk of effort concentrates on writing the proper mediator code that will transform predicates on weather into a query over the weather website. This effort can become complex if some other source also relates to weather, because the designer may need to write code to properly combine the results from the two sources.</p>

<p>On the other hand, in LAV, the source database is modeled as a set of <a href="view_(database)" title="wikilink">views</a> over 

<math display="inline" id="Data_integration:21">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>

. In this case 

<math display="inline" id="Data_integration:22">
 <semantics>
  <mi>M</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>M</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   M
  </annotation>
 </semantics>
</math>


 associates to each element of 

<math display="inline" id="Data_integration:23">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

 a query over 

<math display="inline" id="Data_integration:24">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>

. Here the exact associations between 

<math display="inline" id="Data_integration:25">
 <semantics>
  <mi>G</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>G</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   G
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Data_integration:26">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

 are no longer well-defined. As is illustrated in the next section, the burden of determining how to retrieve elements from the sources is placed on the query processor. The benefit of an LAV modeling is that new sources can be added with far less work than in a GAV system, thus the LAV approach should be favored in cases where the mediated schema is less stable or likely to change.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></p>

<p>In an LAV approach to the example data integration system above, the system designer designs the global schema first and then simply inputs the schemas of the respective city information sources. Consider again if one of the sources serves a weather website. The designer would add corresponding elements for weather to the global schema only if none existed already. Then programmers write an adapter or wrapper for the website and add a schema description of the website's results to the source schemas. The complexity of adding the new source moves from the designer to the query processor.</p>
<h3 id="query-processing">Query processing</h3>

<p>The theory of query processing in data integration systems is commonly expressed using conjunctive <a href="Database_query_language" title="wikilink">queries</a> and <a class="uri" href="Datalog" title="wikilink">Datalog</a>, a purely declarative <a href="logic_programming" title="wikilink">logic programming</a> language.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> One can loosely think of a <a href="conjunctive_query" title="wikilink">conjunctive query</a> as a logical function applied to the relations of a database such as "

<math display="inline" id="Data_integration:27">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>A</mi>
    <mo>,</mo>
    <mi>B</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <interval closure="open">
     <ci>A</ci>
     <ci>B</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(A,B)
  </annotation>
 </semantics>
</math>


 where 

<math display="inline" id="Data_integration:28">
 <semantics>
  <mrow>
   <mi>A</mi>
   <mo><</mo>
   <mi>B</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <lt></lt>
    <ci>A</ci>
    <ci>B</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A<B
  </annotation>
 </semantics>
</math>

". If a tuple or set of tuples is substituted into the rule and satisfies it (makes it true), then we consider that tuple as part of the set of answers in the query. While formal languages like <a class="uri" href="Datalog" title="wikilink">Datalog</a> express these queries concisely and without ambiguity, common <a class="uri" href="SQL" title="wikilink">SQL</a> queries count as conjunctive queries as well.</p>

<p>In terms of data integration, "query containment" represents an important property of conjunctive queries. A query 

<math display="inline" id="Data_integration:29">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A
  </annotation>
 </semantics>
</math>

 contains another query 

<math display="inline" id="Data_integration:30">
 <semantics>
  <mi>B</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>B</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B
  </annotation>
 </semantics>
</math>

 (denoted 

<math display="inline" id="Data_integration:31">
 <semantics>
  <mrow>
   <mi>A</mi>
   <mo>⊃</mo>
   <mi>B</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <subset></subset>
    <ci>B</ci>
    <ci>A</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A\supset B
  </annotation>
 </semantics>
</math>

) if the results of applying 

<math display="inline" id="Data_integration:32">
 <semantics>
  <mi>B</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>B</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   B
  </annotation>
 </semantics>
</math>


 are a subset of the results of applying 

<math display="inline" id="Data_integration:33">
 <semantics>
  <mi>A</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>A</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   A
  </annotation>
 </semantics>
</math>

 for any database. The two queries are said to be equivalent if the resulting sets are equal for any database. This is important because in both GAV and LAV systems, a user poses conjunctive queries over a <em>virtual</em> schema represented by a set of <a href="view_(database)" title="wikilink">views</a>, or "materialized" conjunctive queries. Integration seeks to rewrite the queries represented by the views to make their results equivalent or maximally contained by our user's query. This corresponds to the problem of answering queries using views (<a class="uri" href="AQUV" title="wikilink">AQUV</a>).<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></p>

<p>In GAV systems, a system designer writes mediator code to define the query-rewriting. Each element in the user's query corresponds to a substitution rule just as each element in the global schema corresponds to a query over the source. Query processing simply expands the subgoals of the user's query according to the rule specified in the mediator and thus the resulting query is likely to be equivalent. While the designer does the majority of the work beforehand, some GAV systems such as <a href="http://www-db.stanford.edu/tsimmis/">Tsimmis</a> involve simplifying the mediator description process.</p>

<p>In LAV systems, queries undergo a more radical process of rewriting because no mediator exists to align the user's query with a simple expansion strategy. The integration system must execute a search over the space of possible queries in order to find the best rewrite. The resulting rewrite may not be an equivalent query but maximally contained, and the resulting tuples may be incomplete.  the MiniCon algorithm<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> is the leading query rewriting algorithm for LAV data integration systems.</p>

<p>In general, the complexity of query rewriting is <a class="uri" href="NP-complete" title="wikilink">NP-complete</a>.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a> If the space of rewrites is relatively small this does not pose a problem — even for integration systems with hundreds of sources.</p>
<h2 id="data-integration-in-the-life-sciences">Data integration in the life sciences</h2>

<p>Large-scale questions in science, such as <a href="global_warming" title="wikilink">global warming</a>, <a href="invasive_species" title="wikilink">invasive species</a> spread, and <a href="resource_depletion" title="wikilink">resource depletion</a>, are increasingly requiring the collection of disparate data sets for <a class="uri" href="meta-analysis" title="wikilink">meta-analysis</a>. This type of data integration is especially challenging for ecological and environmental data because <a href="metadata_standards" title="wikilink">metadata standards</a> are not agreed upon and there are many different data types produced in these fields. <a href="National_Science_Foundation" title="wikilink">National Science Foundation</a> initiatives such as <a class="uri" href="Datanet" title="wikilink">Datanet</a> are intended to make data integration easier for scientists by providing <a class="uri" href="cyberinfrastructure" title="wikilink">cyberinfrastructure</a> and setting standards. The five funded <a class="uri" href="Datanet" title="wikilink">Datanet</a> initiatives are <a class="uri" href="DataONE" title="wikilink">DataONE</a>,<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> led by William Michener at the <a href="University_of_New_Mexico" title="wikilink">University of New Mexico</a>; The Data Conservancy,<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> led by Sayeed Choudhury of <a href="Johns_Hopkins_University" title="wikilink">Johns Hopkins University</a>; SEAD: Sustainable Environment through Actionable Data,<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> led by <a href="Margaret_Hedstrom" title="wikilink">Margaret Hedstrom</a> of the <a href="University_of_Michigan" title="wikilink">University of Michigan</a>; the DataNet Federation Consortium,<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a> led by Reagan Moore of the <a href="University_of_North_Carolina" title="wikilink">University of North Carolina</a>; and <em>Terra Populus</em>,<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> led by <a href="Steven_Ruggles" title="wikilink">Steven Ruggles</a> of the <a href="University_of_Minnesota" title="wikilink">University of Minnesota</a>. The Research Data Alliance,<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> has more recently explored creating global data integration frameworks.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Business_semantics_management" title="wikilink">Business semantics management</a></li>
<li><a href="Core_data_integration" title="wikilink">Core data integration</a></li>
<li><a href="Customer_data_integration" title="wikilink">Customer data integration</a></li>
<li><a href="Data_curation" title="wikilink">Data curation</a></li>
<li><a href="Data_fusion" title="wikilink">Data fusion</a></li>
<li><a href="Data_mapping" title="wikilink">Data mapping</a></li>
<li><a href="Data_virtualization" title="wikilink">Data virtualization</a></li>
<li><a href="Data_Warehousing" title="wikilink">Data Warehousing</a></li>
<li><a href="Data_wrangling" title="wikilink">Data wrangling</a></li>
<li><a href="Database_model" title="wikilink">Database model</a></li>
<li><a class="uri" href="Datalog" title="wikilink">Datalog</a></li>
<li><a class="uri" href="Dataspaces" title="wikilink">Dataspaces</a></li>
<li><a href="Edge_data_integration" title="wikilink">Edge data integration</a></li>
<li><a href="Enterprise_application_integration" title="wikilink">Enterprise application integration</a></li>
<li><a href="Enterprise_Architecture_framework" title="wikilink">Enterprise Architecture framework</a></li>
<li><a href="Enterprise_Information_Integration" title="wikilink">Enterprise Information Integration</a> (EII)</li>
<li><a href="Enterprise_integration" title="wikilink">Enterprise integration</a></li>
<li><a href="Extract,_transform,_load" title="wikilink">Extract, transform, load</a></li>
<li><a class="uri" href="Geodi" title="wikilink">Geodi</a>: Geoscientific Data Integration</li>
<li><a href="Information_integration" title="wikilink">Information integration</a></li>
<li><a href="Information_Server" title="wikilink">Information Server</a></li>
<li><a href="Information_silo" title="wikilink">Information silo</a></li>
<li><a href="Integration_Competency_Center" title="wikilink">Integration Competency Center</a></li>
<li><a href="Integration_Consortium" title="wikilink">Integration Consortium</a></li>
<li><a class="uri" href="JXTA" title="wikilink">JXTA</a></li>
<li><a href="Master_data_management" title="wikilink">Master data management</a></li>
<li><a href="Object-relational_mapping" title="wikilink">Object-relational mapping</a></li>
<li><a href="Ontology_based_data_integration" title="wikilink">Ontology based data integration</a></li>
<li><a href="Open_Text" title="wikilink">Open Text</a></li>
<li><a href="Schema_Matching" title="wikilink">Schema Matching</a></li>
<li><a href="Semantic_Integration" title="wikilink">Semantic Integration</a></li>
<li><a class="uri" href="SQL" title="wikilink">SQL</a></li>
<li><a href="Three_schema_approach" title="wikilink">Three schema approach</a></li>
<li><a class="uri" href="UDEF" title="wikilink">UDEF</a></li>
<li><a href="Web_service" title="wikilink">Web service</a></li>
</ul>
<h2 id="references">References</h2>
<references>
</references>
<h2 id="further-reading">Further reading</h2>
<ul>
<li></li>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Data_management" title="wikilink">Category:Data management</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"></li>
<li id="fn15"></li>
<li id="fn16"><a href="#fnref16">↩</a></li>
<li id="fn17"><a href="#fnref17">↩</a></li>
<li id="fn18"><a href="#fnref18">↩</a></li>
<li id="fn19"><a href="#fnref19">↩</a></li>
<li id="fn20"><a href="#fnref20">↩</a></li>
<li id="fn21"><a href="#fnref21">↩</a></li>
</ol>
</section>
</body>
</html>
