<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1059">Early stopping</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Early stopping</h1>
<hr/>

<p>In <a href="machine_learning" title="wikilink">machine learning</a>, <strong>early stopping</strong> is a form of <a href="regularization_(mathematics)" title="wikilink">regularization</a> used to avoid <a class="uri" href="overfitting" title="wikilink">overfitting</a> when training a learner with an iterative method, such as <a href="gradient_descent" title="wikilink">gradient descent</a>. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set. Past that point, however, improving the learner's fit to the training data comes at the expense of increased <a href="generalization_error" title="wikilink">generalization error</a>. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation.</p>
<h2 id="background">Background</h2>

<p>This section presents some of the basic machine-learning concepts required for a description of early stopping methods.</p>
<h3 id="overfitting">Overfitting</h3>
<figure><b>(Figure)</b>
<embed src="Overfitting on Training Set Data.pdf" title="This image represents the problem of overfitting in machine learning. The red dots represent training set data. The green line represents the true functional relationship, while the blue line shows the learned function, which has fallen victim to overfitting."></embed><figcaption>This image represents the problem of overfitting in machine learning. The red dots represent training set data. The green line represents the true functional relationship, while the blue line shows the learned function, which has fallen victim to overfitting.</figcaption>
</figure>

<p><a href="Machine_learning" title="wikilink">Machine learning</a> algorithms train a model based on a finite set of training data. During this training, the model is evaluated based on how well it predicts the observations contained in the training set. In general, however, the goal of a machine learning scheme is to produce a model that generalizes, that is, that predicts previously unseen observations. Overfitting occurs when a model fits the data in the training set well, while incurring larger <a href="generalization_error" title="wikilink">generalization error</a>.</p>
<h3 id="regularization">Regularization</h3>

<p>Regularization, in the context of machine learning, refers to the process of modifying a learning algorithm so as to prevent overfitting. This generally involves imposing some sort of smoothness constraint on the learned model.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> This smoothness may be enforced explicitly, by fixing the number of parameters in the model, or by augmenting the cost function as in <a href="Tikhonov_regularization" title="wikilink">Tikhonov regularization</a>. Tikhonov regularization, along with <a href="principal_component_regression" title="wikilink">principal component regression</a> and many other regularization schemes, fall under the umbrella of <a href="spectral_regularization" title="wikilink">spectral regularization</a>, regularization characterized by the application of a filter. Early stopping also belongs to this class of methods.</p>
<h3 id="gradient-descent-methods">Gradient Descent Methods</h3>

<p>Gradient descent methods are first-order, iterative, optimization methods. Each iteration updates an approximate solution to the optimization problem by taking a step in the direction of the negative of the gradient of the objective function. By choosing the step-size appropriately, such a method can be made to converge to a local minimum of the objective function. Gradient descent is used in machine-learning by defining a <em>loss function</em> that reflects the error of the learner on the training set and then minimizing that function.</p>
<h2 id="definition">Definition</h2>

<p><strong>Early stopping</strong> refers to any <a href="regularization_(mathematics)" title="wikilink">regularization (machine-learning)</a> technique wherein an iterative <a href="machine_learning" title="wikilink">machine-learning</a> scheme is stopped prior to convergence so as to prevent <a class="uri" href="overfitting" title="wikilink">overfitting</a>.</p>
<h2 id="early-stopping-based-on-analytical-results">Early stopping based on analytical results</h2>
<h3 id="early-stopping-in-statistical-learning-theory">Early stopping in <a href="statistical_learning_theory" title="wikilink">statistical learning theory</a></h3>

<p>Early-stopping can be used to regularize <a href="non-parametric_regression" title="wikilink">non-parametric regression</a> problems encountered in <a href="machine_learning" title="wikilink">machine learning</a>. For a given input space, 

<math display="inline" id="Early_stopping:0">
 <semantics>
  <mi>X</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>X</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X
  </annotation>
 </semantics>
</math>

, output space, 

<math display="inline" id="Early_stopping:1">
 <semantics>
  <mi>Y</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>Y</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y
  </annotation>
 </semantics>
</math>

, and samples drawn from an unknown probability measure, 

<math display="inline" id="Early_stopping:2">
 <semantics>
  <mi>ρ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ρ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \rho
  </annotation>
 </semantics>
</math>

, on 

<math display="inline" id="Early_stopping:3">
 <semantics>
  <mrow>
   <mi>Z</mi>
   <mo>=</mo>
   <mrow>
    <mi>X</mi>
    <mo>×</mo>
    <mi>Y</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>Z</ci>
    <apply>
     <times></times>
     <ci>X</ci>
     <ci>Y</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Z=X\times Y
  </annotation>
 </semantics>
</math>

, the goal of such problems is to approximate a <em>regression function</em>, 

<math display="inline" id="Early_stopping:4">
 <semantics>
  <msub>
   <mi>f</mi>
   <mi>ρ</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>f</ci>
    <ci>ρ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{\rho}
  </annotation>
 </semantics>
</math>

, given by</p>

<p>

<math display="block" id="Early_stopping:5">
 <semantics>
  <mrow>
   <msub>
    <mi>f</mi>
    <mi>ρ</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <msub>
    <mo largeop="true" symmetric="true">∫</mo>
    <mi>Y</mi>
   </msub>
   <mi>y</mi>
   <mi>d</mi>
   <mi>ρ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo stretchy="false">|</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
   <mi>x</mi>
   <mo>∈</mo>
   <mi>X</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <ci>ρ</ci>
    </apply>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <int></int>
     <ci>Y</ci>
    </apply>
    <csymbol cd="unknown">y</csymbol>
    <csymbol cd="unknown">d</csymbol>
    <csymbol cd="unknown">ρ</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-,</ci>
    <csymbol cd="unknown">x</csymbol>
    <in></in>
    <csymbol cd="unknown">X</csymbol>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{\rho}(x)=\int_{Y}yd\rho(y|x),x\in X
  </annotation>
 </semantics>
</math>

,</p>

<p>where 

<math display="inline" id="Early_stopping:6">
 <semantics>
  <mrow>
   <mi>ρ</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>y</mi>
    <mo stretchy="false">|</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">ρ</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">y</csymbol>
     <ci>normal-|</ci>
     <csymbol cd="unknown">x</csymbol>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \rho(y|x)
  </annotation>
 </semantics>
</math>

 is the conditional distribution at 

<math display="inline" id="Early_stopping:7">
 <semantics>
  <mi>x</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>x</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x
  </annotation>
 </semantics>
</math>

 induced by 

<math display="inline" id="Early_stopping:8">
 <semantics>
  <mi>ρ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ρ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \rho
  </annotation>
 </semantics>
</math>

.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> One common choice for approximating the regression function is to use functions from a <a href="reproducing_kernel_Hilbert_space" title="wikilink">reproducing kernel Hilbert space</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> These spaces can be infinite dimensional, in which they can supply solutions that overfit training sets of arbitrary size. Regularization is, therefore, especially important for these methods. One way to regularize non-parametric regression problems is to apply an early stopping rule to an iterative procedure such as gradient descent.</p>

<p>The early stopping rules proposed for these problems are based on analysis of upper bounds on the generalization error as a function of the iteration number. They yield prescriptions for the number of iterations to run that can be computed prior to starting the solution process.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> <a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<h4 id="example-least-squares-loss">Example: Least-squares loss</h4>

<p>(Adapted from Yao, Rosasco and Caponnetto, 2007<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a>)</p>

<p>Let 

<math display="inline" id="Early_stopping:9">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>⊆</mo>
   <msup>
    <mi>ℝ</mi>
    <mi>n</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <subset></subset>
    <ci>X</ci>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>ℝ</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X\subseteq\mathbb{R}^{n}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Early_stopping:10">
 <semantics>
  <mrow>
   <mi>Y</mi>
   <mo>=</mo>
   <mi>ℝ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>Y</ci>
    <ci>ℝ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y=\mathbb{R}
  </annotation>
 </semantics>
</math>

. Given a set of samples</p>

<p>

<math display="block" id="Early_stopping:11">
 <semantics>
  <mrow>
   <mi>𝐳</mi>
   <mo>=</mo>
   <mrow>
    <mo>{</mo>
    <mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
      <mo>,</mo>
      <msub>
       <mi>y</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
     <mo>∈</mo>
     <mrow>
      <mi>X</mi>
      <mo>×</mo>
      <mi>Y</mi>
     </mrow>
    </mrow>
    <mo>:</mo>
    <mrow>
     <mi>i</mi>
     <mo>=</mo>
     <mrow>
      <mn>1</mn>
      <mo>,</mo>
      <mi mathvariant="normal">…</mi>
      <mo>,</mo>
      <mi>m</mi>
     </mrow>
    </mrow>
    <mo>}</mo>
   </mrow>
   <mo>∈</mo>
   <msup>
    <mi>Z</mi>
    <mi>m</mi>
   </msup>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>𝐳</ci>
     <apply>
      <csymbol cd="latexml">conditional-set</csymbol>
      <apply>
       <in></in>
       <interval closure="open">
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>i</ci>
        </apply>
       </interval>
       <apply>
        <times></times>
        <ci>X</ci>
        <ci>Y</ci>
       </apply>
      </apply>
      <apply>
       <eq></eq>
       <ci>i</ci>
       <list>
        <cn type="integer">1</cn>
        <ci>normal-…</ci>
        <ci>m</ci>
       </list>
      </apply>
     </apply>
    </apply>
    <apply>
     <in></in>
     <share href="#.cmml">
     </share>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>Z</ci>
      <ci>m</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{z}=\left\{(x_{i},y_{i})\in X\times Y:i=1,\dots,m\right\}\in Z^{m}
  </annotation>
 </semantics>
</math>

,</p>

<p>drawn independently from 

<math display="inline" id="Early_stopping:12">
 <semantics>
  <mi>ρ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ρ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \rho
  </annotation>
 </semantics>
</math>

, minimize the functional</p>

<p>

<math display="block" id="Early_stopping:13">
 <semantics>
  <mrow>
   <mrow>
    <mi class="ltx_font_mathcaligraphic">ℰ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>f</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msub>
     <mo largeop="true" symmetric="true">∫</mo>
     <mrow>
      <mi>X</mi>
      <mo>×</mo>
      <mi>Y</mi>
     </mrow>
    </msub>
    <mrow>
     <msup>
      <mrow>
       <mo>(</mo>
       <mrow>
        <mrow>
         <mi>f</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mi>x</mi>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>-</mo>
        <mi>y</mi>
       </mrow>
       <mo>)</mo>
      </mrow>
      <mn>2</mn>
     </msup>
     <mi>d</mi>
     <mi>ρ</mi>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>ℰ</ci>
     <ci>f</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <int></int>
      <apply>
       <times></times>
       <ci>X</ci>
       <ci>Y</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <apply>
         <times></times>
         <ci>f</ci>
         <ci>x</ci>
        </apply>
        <ci>y</ci>
       </apply>
       <cn type="integer">2</cn>
      </apply>
      <ci>d</ci>
      <ci>ρ</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{E}(f)=\int_{X\times Y}\left(f(x)-y\right)^{2}d\rho
  </annotation>
 </semantics>
</math>

</p>

<p>where, 

<math display="inline" id="Early_stopping:14">
 <semantics>
  <mi>f</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>f</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f
  </annotation>
 </semantics>
</math>

 is a member of the reproducing kernel Hilbert space 

<math display="inline" id="Early_stopping:15">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ℋ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ℋ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{H}
  </annotation>
 </semantics>
</math>

. That is, minimize the expected risk for a Least-squares loss function. Since 

<math display="inline" id="Early_stopping:16">
 <semantics>
  <mi class="ltx_font_mathcaligraphic">ℰ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ℰ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{E}
  </annotation>
 </semantics>
</math>

 depends on the unknown probability measure 

<math display="inline" id="Early_stopping:17">
 <semantics>
  <mi>ρ</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ρ</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \rho
  </annotation>
 </semantics>
</math>

, it cannot be used for computation. Instead, consider the following empirical risk</p>

<p>

<math display="block" id="Early_stopping:18">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mi class="ltx_font_mathcaligraphic">ℰ</mi>
      <mi>𝐳</mi>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>f</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mfrac>
      <mn>1</mn>
      <mi>m</mi>
     </mfrac>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>m</mi>
      </munderover>
      <msup>
       <mrow>
        <mo>(</mo>
        <mrow>
         <mrow>
          <mi>f</mi>
          <mrow>
           <mo stretchy="false">(</mo>
           <msub>
            <mi>x</mi>
            <mi>i</mi>
           </msub>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
         <mo>-</mo>
         <msub>
          <mi>y</mi>
          <mi>i</mi>
         </msub>
        </mrow>
        <mo>)</mo>
       </mrow>
       <mn>2</mn>
      </msup>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>ℰ</ci>
      <ci>𝐳</ci>
     </apply>
     <ci>f</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>m</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>m</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <apply>
         <times></times>
         <ci>f</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>x</ci>
          <ci>i</ci>
         </apply>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>y</ci>
         <ci>i</ci>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{E}_{\mathbf{z}}(f)=\frac{1}{m}\sum_{i=1}^{m}\left(f(x_{i})-y_{i}%
\right)^{2}.
  </annotation>
 </semantics>
</math>

</p>

<p>Let 

<math display="inline" id="Early_stopping:19">
 <semantics>
  <msub>
   <mi>f</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>f</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{t}
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Early_stopping:20">
 <semantics>
  <msubsup>
   <mi>f</mi>
   <mi>t</mi>
   <mi>𝐳</mi>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <ci>t</ci>
    </apply>
    <ci>𝐳</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{t}^{\mathbf{z}}
  </annotation>
 </semantics>
</math>

 be the <em>t</em>-th iterates of gradient descent applied to the expected and empirical risks, respectively, where both iterations are initialized at the origin, and both use the step size 

<math display="inline" id="Early_stopping:21">
 <semantics>
  <msub>
   <mi>γ</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>γ</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \gamma_{t}
  </annotation>
 </semantics>
</math>

. The 

<math display="inline" id="Early_stopping:22">
 <semantics>
  <msub>
   <mi>f</mi>
   <mi>t</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>f</ci>
    <ci>t</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{t}
  </annotation>
 </semantics>
</math>

 form the <em>population iteration</em>, which converges to 

<math display="inline" id="Early_stopping:23">
 <semantics>
  <msub>
   <mi>f</mi>
   <mi>ρ</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>f</ci>
    <ci>ρ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{\rho}
  </annotation>
 </semantics>
</math>

, but cannot be used in computation, while the 

<math display="inline" id="Early_stopping:24">
 <semantics>
  <msubsup>
   <mi>f</mi>
   <mi>t</mi>
   <mi>𝐳</mi>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <ci>t</ci>
    </apply>
    <ci>𝐳</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{t}^{\mathbf{z}}
  </annotation>
 </semantics>
</math>

 form the <em>sample iteration</em> which usually converges to an overfitting solution.</p>

<p>We want to control the difference between the expected risk of the sample iteration and the minimum expected risk, that is, the expected risk of the regression function:</p>

<p>

<math display="block" id="Early_stopping:25">
 <semantics>
  <mrow>
   <mrow>
    <mi class="ltx_font_mathcaligraphic">ℰ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msubsup>
      <mi>f</mi>
      <mi>t</mi>
      <mi>𝐳</mi>
     </msubsup>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>-</mo>
   <mrow>
    <mi class="ltx_font_mathcaligraphic">ℰ</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>f</mi>
      <mi>ρ</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <apply>
     <times></times>
     <ci>ℰ</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>f</ci>
       <ci>t</ci>
      </apply>
      <ci>𝐳</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>ℰ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>f</ci>
      <ci>ρ</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{E}(f_{t}^{\mathbf{z}})-\mathcal{E}(f_{\rho})
  </annotation>
 </semantics>
</math>

</p>

<p>This difference can be rewritten as the sum of two terms: the difference in expected risk between the sample and population iterations and that between the population iteration and the regression function:</p>

<p>

<math display="block" id="Early_stopping:26">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi class="ltx_font_mathcaligraphic">ℰ</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msubsup>
       <mi>f</mi>
       <mi>t</mi>
       <mi>𝐳</mi>
      </msubsup>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>-</mo>
    <mrow>
     <mi class="ltx_font_mathcaligraphic">ℰ</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>f</mi>
       <mi>ρ</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <mo>[</mo>
     <mrow>
      <mrow>
       <mi class="ltx_font_mathcaligraphic">ℰ</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <msubsup>
         <mi>f</mi>
         <mi>t</mi>
         <mi>𝐳</mi>
        </msubsup>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>-</mo>
      <mrow>
       <mi class="ltx_font_mathcaligraphic">ℰ</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <msub>
         <mi>f</mi>
         <mi>t</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo>]</mo>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mo>[</mo>
     <mrow>
      <mrow>
       <mi class="ltx_font_mathcaligraphic">ℰ</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <msub>
         <mi>f</mi>
         <mi>t</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>-</mo>
      <mrow>
       <mi class="ltx_font_mathcaligraphic">ℰ</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <msub>
         <mi>f</mi>
         <mi>ρ</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo>]</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <ci>ℰ</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>f</ci>
        <ci>t</ci>
       </apply>
       <ci>𝐳</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>ℰ</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>f</ci>
       <ci>ρ</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <apply>
       <minus></minus>
       <apply>
        <times></times>
        <ci>ℰ</ci>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>f</ci>
          <ci>t</ci>
         </apply>
         <ci>𝐳</ci>
        </apply>
       </apply>
       <apply>
        <times></times>
        <ci>ℰ</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>f</ci>
         <ci>t</ci>
        </apply>
       </apply>
      </apply>
     </apply>
     <apply>
      <csymbol cd="latexml">delimited-[]</csymbol>
      <apply>
       <minus></minus>
       <apply>
        <times></times>
        <ci>ℰ</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>f</ci>
         <ci>t</ci>
        </apply>
       </apply>
       <apply>
        <times></times>
        <ci>ℰ</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>f</ci>
         <ci>ρ</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathcal{E}(f_{t}^{\mathbf{z}})-\mathcal{E}(f_{\rho})=\left[\mathcal{E}(f_{t}^%
{\mathbf{z}})-\mathcal{E}(f_{t})\right]+\left[\mathcal{E}(f_{t})-\mathcal{E}(f%
_{\rho})\right]
  </annotation>
 </semantics>
</math>

</p>

<p>This equation presents a <a href="Bias-variance_dilemma" title="wikilink">bias-variance tradeoff</a>, which is then solved to give an optimal stopping rule that may depend on the unknown probability distribution. That rule has associated probabilistic bounds on the generalization error. For the analysis leading to the early stopping rule and bounds, the reader is referred to the original article.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> In practice, data-driven methods, e.g. cross-validation can be used to obtain an adaptive stopping rule.</p>
<h3 id="early-stopping-in-boosting">Early stopping in Boosting</h3>

<p><a href="Boosting_(machine_learning)" title="wikilink">Boosting</a> refers to a family of algorithms in which a set of <strong>weak learners</strong> (learners that are only slightly correlated with the true process) are combined to produce a <strong>strong learner</strong>. It has been shown, for several boosting algorithms (including <a class="uri" href="AdaBoost" title="wikilink">AdaBoost</a>), that regularization via early stopping can provide guarantees of <a href="consistency_(statistics)" title="wikilink">consistency</a>, that is, that the result of the algorithm approaches the true solution as the number of samples goes to infinity.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> <a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> <a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h4 id="l_-2--boosting">L

<math display="inline" id="Early_stopping:27">
 <semantics>
  <msub>
   <mi></mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   {}_{2}
  </annotation>
 </semantics>
</math>

-Boosting</h4>

<p>Boosting methods have close ties to the gradient descent methods described <a href="#Early_stopping_in_non-parametric_regression" title="wikilink">above</a> can be regarded as a boosting method based on the 

<math display="inline" id="Early_stopping:28">
 <semantics>
  <msub>
   <mi>L</mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>L</ci>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L_{2}
  </annotation>
 </semantics>
</math>

 loss: <em>L

<math display="inline" id="Early_stopping:29">
 <semantics>
  <msub>
   <mi></mi>
   <mn>2</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   {}_{2}
  </annotation>
 </semantics>
</math>

Boost</em>.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></p>
<h2 id="early-stopping-based-on-cross-validation">Early stopping based on cross-validation</h2>

<p>These early stopping rules work by splitting the original training set into a new training set and a validation set. The error on the validation set is used as a proxy for the <a href="generalization_error" title="wikilink">generalization error</a> in determining when overfitting has begun. These methods are most commonly employed in the training of <a href="neural_networks" title="wikilink">neural networks</a>. Prechelt gives the following summary of a naive implementation of cross-validation based early stopping as follows:<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></p>

<p>This simple procedure is complicated in practice by the fact that the validation error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when overfitting has truly begun.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a class="uri" href="Overfitting" title="wikilink">Overfitting</a>, early stopping is one of methods used to prevent overfitting</li>
<li><a href="Generalization_error" title="wikilink">Generalization error</a></li>
<li><a href="Regularization_(mathematics)" title="wikilink">Regularization (mathematics)</a></li>
<li><a href="Statistical_Learning_Theory" title="wikilink">Statistical Learning Theory</a></li>
<li><a href="Boosting_(machine_learning)" title="wikilink">Boosting (machine learning)</a></li>
<li><a href="Cross-validation_(statistics)" title="wikilink">Cross-validation</a>, in particular using a "Validation Set"</li>
<li><a href="Neural_Networks" title="wikilink">Neural Networks</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a> <a href="Category:Artificial_neural_networks" title="wikilink">Category:Artificial neural networks</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"></li>
<li id="fn7"></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"></li>
</ol>
</section>
</body>
</html>
