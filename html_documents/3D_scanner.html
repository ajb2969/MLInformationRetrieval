<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1169">3D scanner</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>3D scanner</h1>
<hr/>

<p> <embed src="3D scanned interior of St Joseph's Church, Subiaco.ogv" title="fig:3D scanned interior of St Joseph's Church, Subiaco"></embed></p>

<p>A <strong>3D scanner</strong> is a device that analyses a real-world object or environment to collect data on its shape and possibly its appearance (e.g. colour). The collected data can then be used to construct digital <a href="3D_modelling" title="wikilink">three-dimensional models</a>.</p>

<p>Many different technologies can be used to build these 3D-scanning devices; each technology comes with its own limitations, advantages and costs. Many limitations in the kind of objects that can be digitised are still present, for example, optical technologies encounter many difficulties with shiny, mirroring or transparent objects. For example, <a href="industrial_computed_tomography_scanning" title="wikilink">industrial computed tomography scanning</a> can be used to construct digital 3D models, applying <a href="non-destructive_testing" title="wikilink">non-destructive testing</a>.</p>

<p>Collected 3D data is useful for a wide variety of applications. These devices are used extensively by the entertainment industry in the production of movies and video games. Other common applications of this technology include <a href="industrial_design" title="wikilink">industrial design</a>, <a class="uri" href="orthotics" title="wikilink">orthotics</a> and <a class="uri" href="prosthetics" title="wikilink">prosthetics</a>, <a href="reverse_engineering" title="wikilink">reverse engineering</a> and <a class="uri" href="prototyping" title="wikilink">prototyping</a>, <a href="quality_control" title="wikilink">quality control</a>/inspection and documentation of cultural artifacts.</p>
<h2 id="functionality">Functionality</h2>

<p> The purpose of a 3D scanner is usually to create a <a href="point_cloud" title="wikilink">point cloud</a> of geometric samples on the surface of the subject. These points can then be used to extrapolate the shape of the subject (a process called <a href="3D_reconstruction" title="wikilink">reconstruction</a>). If colour information is collected at each point, then the colours on the surface of the subject can also be determined.</p>

<p>3D scanners share several traits with cameras. Like cameras, they have a cone-like <a href="field_of_view" title="wikilink">field of view</a>, and like cameras, they can only collect information about surfaces that are not obscured. While a camera collects colour information about surfaces within its field of view, a 3D scanner collects distance information about surfaces within its field of view. The "picture" produced by a 3D scanner describes the distance to a surface at each point in the picture. This allows the three dimensional position of each point in the picture to be identified.</p>

<p>For most situations, a single scan will not produce a complete model of the subject. Multiple scans, even hundreds, from many different directions are usually required to obtain information about all sides of the subject. These scans have to be brought into a common <a href="coordinate_system" title="wikilink">reference system</a>, a process that is usually called <em>alignment</em> or <em><a href="image_registration" title="wikilink">registration</a></em>, and then merged to create a complete model. This whole process, going from the single range map to the whole model, is usually known as the 3D scanning pipeline.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<h2 id="technology">Technology</h2>

<p>There are a variety of technologies for digitally acquiring the shape of a 3D object. A well established classification<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> divides them into two types: contact and non-contact. Non-contact solutions can be further divided into two main categories, active and passive. There are a variety of technologies that fall under each of these categories.</p>
<h3 id="contact">Contact</h3>

<p> Contact 3D scanners probe the subject through physical touch, while the object is in contact with or resting on a <a href="Flatness_(manufacturing)" title="wikilink">precision flat</a> <a href="surface_plate" title="wikilink">surface plate</a>, ground and polished to a specific maximum of surface roughness. Where the object to be scanned is not flat or can not rest stably on a flat surface, it is supported and held firmly in place by a <a href="Fixture_(tool)" title="wikilink">fixture</a>.</p>

<p>The scanner mechanism may have three different forms:</p>
<ul>
<li>A carriage system with rigid arms held tightly in perpendicular relationship and each axis gliding along a track. Such systems work best with flat profile shapes or simple convex curved surfaces.</li>
<li>An articulated arm with rigid bones and high precision angular sensors. The location of the end of the arm involves complex math calculating the wrist rotation angle and hinge angle of each joint. This is ideal for probing into crevasses and interior spaces with a small mouth opening.</li>
<li>A combination of both methods may be used, such as an articulated arm suspended from a traveling carriage, for mapping large objects with interior cavities or overlapping surfaces.</li>
</ul>

<p>A <strong>CMM</strong> (<a href="coordinate_measuring_machine" title="wikilink">coordinate measuring machine</a>) is an example of a contact 3D scanner. It is used mostly in manufacturing and can be very precise. The disadvantage of CMMs though, is that it requires contact with the object being scanned. Thus, the act of scanning the object might modify or damage it. This fact is very significant when scanning delicate or valuable objects such as historical artifacts. The other disadvantage of CMMs is that they are relatively slow compared to the other scanning methods. Physically moving the arm that the probe is mounted on can be very slow and the fastest CMMs can only operate on a few hundred hertz. In contrast, an optical system like a laser scanner can operate from 10 to 500 kHz.</p>

<p>Other examples are the hand driven touch probes used to digitise clay models in computer animation industry.</p>
<h3 id="non-contact-active">Non-contact active</h3>

<p>Active scanners emit some kind of radiation or light and detect its reflection or radiation passing through object in order to probe an object or environment. Possible types of emissions used include light, <a href="Non-Contact_Ultrasound" title="wikilink">ultrasound</a> or x-ray.</p>
<h4 id="time-of-flight">Time-of-flight</h4>
<figure><b>(Figure)</b>
<figcaption>This <a class="uri" href="lidar" title="wikilink">lidar</a> scanner may be used to scan buildings, rock formations, etc., to produce a 3D model. The lidar can aim its laser beam in a wide range: its head rotates horizontally, a mirror flips vertically. The laser beam is used to measure the distance to the first object on its path.</figcaption>
</figure>

<p>The time-of-flight 3D laser scanner is an active scanner that uses laser light to probe the subject. At the heart of this type of scanner is a time-of-flight <a href="laser_range_finder" title="wikilink">laser range finder</a>. The laser range finder finds the distance of a surface by timing the round-trip time of a pulse of light. A laser is used to emit a pulse of light and the amount of time before the reflected light is seen by a detector is measured. Since the <a href="speed_of_light" title="wikilink">speed of light</a> 

<math display="inline" id="3D_scanner:0">
 <semantics>
  <mi>c</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>c</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   c
  </annotation>
 </semantics>
</math>

 is known, the round-trip time determines the travel distance of the light, which is twice the distance between the scanner and the surface. If 

<math display="inline" id="3D_scanner:1">
 <semantics>
  <mi>t</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>t</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   t
  </annotation>
 </semantics>
</math>

 is the round-trip time, then distance is equal to 

<math display="inline" id="3D_scanner:2">
 <semantics>
  <mrow>
   <mrow>
    <mpadded width="-1.7pt">
     <mi>c</mi>
    </mpadded>
    <mo rspace="0.8pt">⋅</mo>
    <mi>t</mi>
   </mrow>
   <mo>/</mo>
   <mn>2</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <ci>normal-⋅</ci>
     <ci>c</ci>
     <ci>t</ci>
    </apply>
    <cn type="integer">2</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \textstyle c\!\cdot\!t/2
  </annotation>
 </semantics>
</math>

. The accuracy of a time-of-flight 3D laser scanner depends on how precisely we can measure the 

<math display="inline" id="3D_scanner:3">
 <semantics>
  <mi>t</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>t</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   t
  </annotation>
 </semantics>
</math>

 time: 3.3 <a href="picosecond" title="wikilink">picoseconds</a> (approx.) is the time taken for light to travel 1 millimetre.</p>

<p>The laser range finder only detects the distance of one point in its direction of view. Thus, the scanner scans its entire field of view one point at a time by changing the range finder's direction of view to scan different points. The view direction of the laser range finder can be changed either by rotating the range finder itself, or by using a system of rotating mirrors. The latter method is commonly used because mirrors are much lighter and can thus be rotated much faster and with greater accuracy. Typical time-of-flight 3D laser scanners can measure the distance of 10,000~100,000 points every second.</p>

<p>Time-of-flight devices are also available in a 2D configuration. This is referred to as a <a href="time-of-flight_camera" title="wikilink">time-of-flight camera</a>.</p>
<h4 id="triangulation">Triangulation</h4>
<figure><b>(Figure)</b>
<figcaption>Principle of a laser triangulation sensor. Two object positions are shown.</figcaption>
</figure>

<p>Triangulation based 3D laser scanners are also active scanners that use laser light to probe the environment. With respect to time-of-flight 3D laser scanner the triangulation laser shines a laser on the subject and exploits a camera to look for the location of the laser dot. Depending on how far away the laser strikes a surface, the laser dot appears at different places in the camera's field of view. This technique is called triangulation because the laser dot, the camera and the laser emitter form a triangle. The length of one side of the triangle, the distance between the camera and the laser emitter is known. The angle of the laser emitter corner is also known. The angle of the camera corner can be determined by looking at the location of the laser dot in the camera's field of view. These three pieces of information fully determine the shape and size of the triangle and give the location of the laser dot corner of the triangle. In most cases a laser stripe, instead of a single laser dot, is swept across the object to speed up the acquisition process. The <a href="National_Research_Council_of_Canada" title="wikilink">National Research Council of Canada</a> was among the first institutes to develop the triangulation based laser scanning technology in 1978.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<h4 id="strengths-and-weaknesses">Strengths and weaknesses</h4>

<p><em>Time-of-flight</em> and <em>triangulation</em> range finders each have strengths and weaknesses that make them suitable for different situations. The advantage of <strong>time-of-flight</strong> range finders is that they are capable of operating over very long distances, on the order of kilometres. These scanners are thus suitable for scanning large structures like buildings or geographic features. The disadvantage of <em>time-of-flight</em> range finders is their accuracy. Due to the high speed of light, timing the round-trip time is difficult and the accuracy of the distance measurement is relatively low, on the order of millimetres.<br/>
<strong>Triangulation</strong> range finders are exactly the opposite. They have a limited range of some meters, but their accuracy is relatively high. The accuracy of <em>triangulation</em> range finders is on the order of tens of micrometers.</p>

<p><strong>Time-of-flight</strong> scanners' accuracy can be lost when the laser hits the edge of an object because the information that is sent back to the scanner is from two different locations for one laser pulse. The coordinate relative to the scanner's position for a point that has hit the edge of an object will be calculated based on an average and therefore will put the point in the wrong place. When using a high resolution scan on an object the chances of the beam hitting an edge are increased and the resulting data will show noise just behind the edges of the object. Scanners with a smaller beam width will help to solve this problem but will be limited by range as the beam width will increase over distance. Software can also help by determining that the first object to be hit by the laser beam should cancel out the second.</p>

<p>At a rate of 10,000 sample points per second, low resolution scans can take less than a second, but high resolution scans, requiring millions of samples, can take minutes for some <em>time-of-flight</em> scanners. The problem this creates is distortion from motion. Since each point is sampled at a different time, any motion in the subject or the scanner will distort the collected data. Thus, it is usually necessary to mount both the subject and the scanner on stable platforms and minimise vibration. Using these scanners to scan objects in motion is very difficult.</p>

<p>Recently, there has been research on compensating for distortion from small amounts of vibration <a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> and distortions due to motion and/or rotation.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>

<p>When scanning in one position for any length of time slight movement can occur in the scanner position due to changes in temperature. If the scanner is set on a tripod and there is strong sunlight on one side of the scanner then that side of the tripod will expand and slowly distort the scan data from one side to another. Some laser scanners have level compensators built into them to counteract any movement of the scanner during the scan process.</p>
<h4 id="conoscopic-holography">Conoscopic holography</h4>

<p>Conoscopic holography measures distances by using the polarization properties of a converging light cone that reflect from an object. At the core of the technology stands an anisotropic crystal: a ray that traverses it splits into two components that share the same path but have orthogonal polarizations. The crystal’s anisotropic structure forces each of the polarized light rays to propagate at a different velocity, thus creating a phase difference between them. This phase difference enables the formation of an interference pattern that varies with the distance from the object under measurement.</p>

<p>In classical holography, a hologram is created by recording an <a href="Conoscopic_interference_pattern" title="wikilink">interference pattern</a> formed between an object beam and a reference beam using a coherent light source. The two beams propagate at the same velocity (same refractive index), but follow different geometric paths. This means that when overlapped, the phase difference between the two beams depends only on the geometric path difference. This phase difference is responsible for the creation of a measurable interference pattern that can later be used to reconstruct the original light field.</p>

<p>In conoscopic holography, however, a light beam that traverses an optically anisotropic crystal is split into two beams that share the same geometric path but have orthogonal polarization modes. The refractive indices of these two beams generally differ from each other. Therefore, after the two beams exit the crystal an interference pattern is generated. The features of this pattern depend on the distance from the light's source.</p>

<p>Since both beams propagate through the same geometric path, conoscopic holography is highly stable in comparison to interferometry-based measurement techniques. Moreover, it is also possible to perform measurements using incoherent light. Conoscopic Holography was first introduced by <a href="http://www.optimet.com/">Optimet</a> (Optical Metrology Ltd) and the company has several technological patents in this field. It was founded in 1995 by <a href="Ophir_Optronics" title="wikilink">Ophir Optronics Solutions</a> a part of Newport Corporation. The sensor sometimes called Laser displacement sensor, or distance sensor, eventually provide a cloud of points which a 3D model can be extract of it.</p>

<p>The sensor emits an eye-safe laser beam, which is focused by an objective lens, and hits the specimen being measured. Part of the scattered light travels back from the specimen into the sensor, and enters the conoscopic unit that contains the optically anisotropic crystal. The resulting interference pattern is detected, and signal processing algorithms are then used to retrieve the distance information from the measured data.</p>

<p>Conoscopic Holography sensor allows both diffusive and reflective material measurement, and its unique advantage is wide angle coverage of up to 170 deg, high accuracy of down to 1 micron, and the ability to go into small holes to measure bottom and side walls.</p>
<figure><b>(Figure)</b>
<figcaption>Using a periscope allows to into small diameter holes and measure bottom and side walls.</figcaption>
</figure>
<h3 id="hand-held-laser-scanners">Hand-held laser scanners</h3>

<p>Hand-held laser scanners create a 3D image through the triangulation mechanism described above: a laser dot or line is projected onto an object from a hand-held device and a sensor (typically a <a href="charge-coupled_device" title="wikilink">charge-coupled device</a> or <a href="position_sensitive_device" title="wikilink">position sensitive device</a>) measures the distance to the surface. Data is collected in relation to an internal coordinate system and therefore to collect data where the scanner is in motion the position of the scanner must be determined. The position can be determined by the scanner using reference features on the surface being scanned (typically adhesive reflective tabs, but natural features have been also used in research work <a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a>) or by using an external tracking method. External tracking often takes the form of a <a href="laser_tracker" title="wikilink">laser tracker</a> (to provide the sensor position) with integrated camera (to determine the orientation of the scanner) or a <a class="uri" href="photogrammetric" title="wikilink">photogrammetric</a> solution using 3 or more cameras providing the complete <a href="Six_degrees_of_freedom" title="wikilink">Six degrees of freedom</a> of the scanner. Both techniques tend to use <a href="infra_red" title="wikilink">infra red</a> <a href="Light-emitting_diode" title="wikilink">Light-emitting diodes</a> attached to the scanner which are seen by the camera(s) through filters providing resilience to ambient lighting.</p>

<p>Data is collected by a computer and recorded as data points within <a href="Three-dimensional_space" title="wikilink">Three-dimensional space</a>, with processing this can be converted into a triangulated mesh and then a <a href="Computer-aided_design" title="wikilink">Computer-aided design</a> model, often as <a href="Non_uniform_rational_B-spline" title="wikilink">Non uniform rational B-spline</a> surfaces. Hand-held laser scanners can combine this data with passive, visible-light sensors — which capture surface textures and colours — to build (or "<a href="reverse_engineer" title="wikilink">reverse engineer</a>") a full 3D model.</p>
<h3 id="structured-light">Structured light</h3>

<p>Structured-light 3D scanners project a pattern of light on the subject and look at the deformation of the pattern on the subject. The pattern is projected onto the subject using either an <a href="LCD_projector" title="wikilink">LCD projector</a> or other stable light source. A camera, offset slightly from the pattern projector, looks at the shape of the pattern and calculates the distance of every point in the field of view.</p>

<p>Structured-light scanning is still a very active area of research with many research papers published each year. Perfect maps have also been proven useful as structured light patterns that solve the correspondence problem and allow for error detection and error correction.[24] [See [<a class="uri" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp">http://ieeexplore.ieee.org/xpl/login.jsp?tp</a>=&amp;arnumber;=667888&amp;url;=http%3A%2F%2Fieeexplore.ieee.org%2Fiel4%2F34%2F14695%2F00667888.pdf%3Farnumber%3D667888 Morano, R., et al. "Structured Light Using Pseudorandom Codes,"] <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.</p>

<p>The advantage of structured-light 3D scanners is speed and precision. Instead of scanning one point at a time, structured light scanners scan multiple points or the entire field of view at once. Scanning an entire field of view in a fraction of a second reduces or eliminates the problem of distortion from motion. Some existing systems are capable of scanning moving objects in real-time. VisionMaster creates a 3D scanning system with a 5-megapixel camera – 5 million data points are acquired in every frame.</p>

<p>A real-time scanner using digital fringe projection and phase-shifting technique (certain kinds of structured light methods) was developed, to capture, reconstruct, and render high-density details of dynamically deformable objects (such as facial expressions) at 40 frames per second.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> Recently, another scanner has been developed. Different patterns can be applied to this system, and the frame rate for capturing and data processing achieves 120 frames per second. It can also scan isolated surfaces, for example two moving hands.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> By utilising the binary defocusing technique, speed breakthroughs have been made that could reach hundreds of <a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> to thousands of frames per second.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a></p>
<h3 id="modulated-light">Modulated light</h3>

<p>Modulated light 3D scanners shine a continually changing light at the subject. Usually the light source simply cycles its amplitude in a <a class="uri" href="sinusoidal" title="wikilink">sinusoidal</a> pattern. A camera detects the reflected light and the amount the pattern is shifted by determines the distance the light travelled. Modulated light also allows the scanner to ignore light from sources other than a laser, so there is no interference.</p>
<h3 id="volumetric-techniques">Volumetric techniques</h3>
<h4 id="medical">Medical</h4>

<p><a href="Computed_tomography" title="wikilink">Computed tomography</a> (CT) is a medical imaging method which generates a three-dimensional image of the inside of an object from a large series of two-dimensional X-ray images, similarly <a href="Magnetic_resonance_imaging" title="wikilink">Magnetic resonance imaging</a> is another medical imaging technique that provides much greater contrast between the different soft tissues of the body than computed tomography (CT) does, making it especially useful in neurological (brain), musculoskeletal, cardiovascular, and oncological (cancer) imaging. These techniques produce a <a href="voxel" title="wikilink">discrete 3D volumetric representation</a> that can be directly <a href="Volume_rendering" title="wikilink">visualised</a>, manipulated or converted to traditional 3D surface by mean of <a href="marching_cubes" title="wikilink">isosurface extraction algorithms</a>.</p>
<h4 id="industrial">Industrial</h4>

<p>Although most common in medicine, <a href="Industrial_computed_tomography" title="wikilink">Industrial computed tomography</a>, <a href="Industrial_CT_Scanning" title="wikilink">Microtomography</a> and MRI are also used in other fields for acquiring a digital representation of an object and its interior, such as non destructive materials testing, <a href="reverse_engineering" title="wikilink">reverse engineering</a>, or studying biological and paleontological specimens.</p>
<h3 id="non-contact-passive">Non-contact passive</h3>

<p>Passive 3D imaging solutions do not emit any kind of radiation themselves, but instead rely on detecting reflected ambient radiation. Most solutions of this type detect visible light because it is a readily available ambient radiation. Other types of radiation, such as infra red could also be used. Passive methods can be very cheap, because in most cases they do not need particular hardware but simple digital cameras.</p>
<ul>
<li><em>Stereoscopic</em> systems usually employ two video cameras, slightly apart, looking at the same scene. By analysing the slight differences between the images seen by each camera, it is possible to determine the distance at each point in the images. This method is based on the same principles driving human <a href="stereoscopic_vision" title="wikilink">stereoscopic vision</a><a href="http://www.cogs.susx.ac.uk/users/davidy/teachvision/vision5.html">1</a>.</li>
<li><em><a href="Photometric_Stereo" title="wikilink">Photometric</a></em> systems usually use a single camera, but take multiple images under varying lighting conditions. These techniques attempt to invert the image formation model in order to recover the surface orientation at each pixel.</li>
<li><em>Silhouette</em> techniques use outlines created from a sequence of photographs around a three-dimensional object against a well contrasted background. These <a href="silhouette" title="wikilink">silhouettes</a> are extruded and intersected to form the <a href="visual_hull" title="wikilink">visual hull</a> approximation of the object. With these approaches some concavities of an object (like the interior of a bowl) cannot be detected.</li>
</ul>
<h4 id="user-assisted-image-based-modelling">User assisted (image-based modelling)</h4>

<p>There are other methods that, based on the user assisted detection and identification of some features and shapes on a set of different pictures of an object are able to build an approximation of the object itself. This kind of techniques are useful to build fast approximation of simple shaped objects like buildings. Various commercial packages are available like <a class="uri" href="D-Sculptor" title="wikilink">D-Sculptor</a>, <a class="uri" href="iModeller" title="wikilink">iModeller</a>, <a href="Autodesk_ImageModeler" title="wikilink">Autodesk ImageModeler</a>, <a class="uri" href="123DCatch" title="wikilink">123DCatch</a> or <a class="uri" href="PhotoModeler" title="wikilink">PhotoModeler</a>.</p>

<p>This sort of 3D imaging solution is based on the principles of <a class="uri" href="photogrammetry" title="wikilink">photogrammetry</a>. It is also somewhat similar in methodology to <a href="panoramic_photography" title="wikilink">panoramic photography</a>, except that the photos are taken of one object on a three-dimensional space in order to replicate it instead of taking a series of photos from one point in a three-dimensional space in order to replicate the surrounding environment.</p>
<h2 id="reconstruction">Reconstruction</h2>
<h3 id="from-point-clouds">From point clouds</h3>

<p>The <a href="point_cloud" title="wikilink">point clouds</a> produced by 3D scanners and 3D imaging can be used directly for measurement and visualisation in the architecture and construction world.</p>
<h3 id="from-models">From models</h3>

<p>Most applications, however, use instead polygonal 3D models, <a class="uri" href="NURBS" title="wikilink">NURBS</a> surface models, or editable feature-based CAD models (aka <a href="Solid_modeling" title="wikilink">Solid models</a>).</p>
<ul>
<li><a href="Polygon_mesh" title="wikilink">Polygon mesh</a> models: In a polygonal representation of a shape, a curved surface is modeled as many small faceted flat surfaces (think of a sphere modeled as a disco ball). Polygon models—also called Mesh models, are useful for visualisation, for some <a href="Computer-aided_manufacturing" title="wikilink">CAM</a> (i.e., machining), but are generally "heavy" ( i.e., very large data sets), and are relatively un-editable in this form. Reconstruction to polygonal model involves finding and connecting adjacent points with straight lines in order to create a continuous surface. Many applications, both free and nonfree, are available for this purpose (e.g. <a class="uri" href="MeshLab" title="wikilink">MeshLab</a>, PointCab, kubit PointCloud for AutoCAD, JRC 3D Reconstructor, imagemodel, PolyWorks, Rapidform, <a class="uri" href="Geomagic" title="wikilink">Geomagic</a>, Imageware, <a href="Rhino_3D" title="wikilink">Rhino 3D</a> etc.).</li>
<li><a href="Freeform_surface_modelling" title="wikilink">Surface models</a>: The next level of sophistication in modeling involves using a quilt of <em>curved</em> surface patches to model our shape. These might be NURBS, TSplines or other curved representations of curved topology. Using NURBS, our sphere is a true mathematical sphere. Some applications offer patch layout by hand but the best in class offer both automated patch layout and manual layout. These patches have the advantage of being lighter and more manipulable when exported to CAD. Surface models are somewhat editable, but only in a sculptural sense of pushing and pulling to deform the surface. This representation lends itself well to modelling organic and artistic shapes. Providers of surface modellers include Rapidform, <a class="uri" href="Geomagic" title="wikilink">Geomagic</a>, <a href="Rhino_3D" title="wikilink">Rhino 3D</a>, Maya, T Splines etc.</li>
<li><a href="Solid_modelling" title="wikilink">Solid CAD models</a>: From an engineering/manufacturing perspective, the ultimate representation of a digitised shape is the editable, parametric CAD model. After all, CAD is the common "language" of industry to describe, edit and maintain the shape of the enterprise's assets. In CAD, our sphere is described by parametric features which are easily edited by changing a value (e.g., centre point and radius).</li>
</ul>

<p>These CAD models describe not simply the envelope or shape of the object, but CAD models also embody the "design intent" (i.e., critical features and their relationship to other features). An example of design intent not evident in the shape alone might be a brake drum's lug bolts, which must be concentric with the hole in the centre of the drum. This knowledge would drive the sequence and method of creating the CAD model; a designer with an awareness of this relationship would not design the lug bolts referenced to the outside diameter, but instead, to the center. A modeler creating a CAD model will want to include both Shape and design intent in the complete CAD model.</p>

<p>Vendors offer different approaches to getting to the parametric CAD model. Some export the NURBS surfaces and leave it to the CAD designer to complete the model in CAD (e.g., <a class="uri" href="Geomagic" title="wikilink">Geomagic</a>, Imageware, <a href="Rhino_3D" title="wikilink">Rhino 3D</a>). Others use the scan data to create an editable and verifiable feature based model that is imported into CAD with full feature tree intact, yielding a complete, native CAD model, capturing both shape and design intent (e.g. <a class="uri" href="Geomagic" title="wikilink">Geomagic</a>, Rapidform). Still other CAD applications are robust enough to manipulate limited points or polygon models within the CAD environment (e.g., <a class="uri" href="CATIA" title="wikilink">CATIA</a>, <a class="uri" href="AutoCAD" title="wikilink">AutoCAD</a>, <a class="uri" href="Revit" title="wikilink">Revit</a>).</p>
<h3 id="from-a-set-of-2d-slices">From a set of 2D slices</h3>
<figure><b>(Figure)</b>
<figcaption>3D reconstruction of the brain and eyeballs from CT scanned DICOM images. In this image, areas with the density of bone or air were made transparent, and the slices stacked up in an approximate free-space alignment. The outer ring of material around the brain are the soft tissues of skin and muscle on the outside of the skull. A black box encloses the slices to provide the black background. Since these are simply 2D images stacked up, when viewed on edge the slices disappear since they have effectively zero thickness. Each DICOM scan represents about 5mm of material averaged into a thin slice.</figcaption>
</figure>

<p><a href="X-ray_computed_tomography" title="wikilink">CT</a>, <a href="industrial_CT_scanning" title="wikilink">industrial CT</a>, <a class="uri" href="MRI" title="wikilink">MRI</a>, or <a href="x-ray_microtomography" title="wikilink">Micro-CT</a> scanners do not produce point clouds but a set of 2D slices (each termed a "tomogram") which are then 'stacked together' to produce a 3D representation. There are several ways to do this depending on the output required:</p>
<ul>
<li><a href="Volume_rendering" title="wikilink">Volume rendering</a>: Different parts of an object usually have different threshold values or greyscale densities. From this, a 3-dimensional model can be constructed and displayed on screen. Multiple models can be constructed from various thresholds, allowing different colours to represent each component of the object. Volume rendering is usually only used for visualisation of the scanned object.</li>
<li><a href="Segmentation_(image_processing)" title="wikilink">Image segmentation</a>: Where different structures have similar threshold/greyscale values, it can become impossible to separate them simply by adjusting volume rendering parameters. The solution is called segmentation, a manual or automatic procedure that can remove the unwanted structures from the image. Image segmentation software usually allows export of the segmented structures in CAD or STL format for further manipulation.</li>
<li><a href="Image-based_meshing" title="wikilink">Image-based meshing</a>: When using 3D image data for computational analysis (e.g. CFD and FEA), simply segmenting the data and meshing from CAD can become time consuming, and virtually intractable for the complex topologies typical of image data. The solution is called image-based meshing, an automated process of generating an accurate and realistic geometrical description of the scan data.</li>
</ul>
<h3 id="from-laser-scans">From laser scans</h3>

<p><em><a href="Laser_scanning" title="wikilink">Laser scanning</a></em> describes the general method to sample or scan a surface using <a class="uri" href="laser" title="wikilink">laser</a> technology. Several areas of application exist that mainly differ in the power of the lasers that are used, and in the results of the scanning process. Low laser power is used when the scanned surface doesn't have to be influenced, e.g. when it only has to be digitised. <a class="uri" href="Confocal" title="wikilink">Confocal</a> or <a href="Three-dimensional_space" title="wikilink">3D</a> laser scanning are methods to get information about the scanned surface. Another low-power application uses structured light projection systems for solar cell flatness metrology, enabling stress calculation throughout in excess of 2000 wafers per hour.<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></p>

<p>The laser power used for laser scanning equipment in industrial applications is typically less than 1W. The power level is usually on the order of 200 mW or less.</p>
<h2 id="applications">Applications</h2>
<h3 id="construction-industry-and-civil-engineering">Construction industry and civil engineering</h3>
<ul>
<li><a href="Robotic_control" title="wikilink">Robotic control</a>: e.g. a laser scanner may function as the "eye" of a robot.<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></li>
<li>As-built drawings of bridges, industrial plants, and monuments</li>
<li>Documentation of historical sites</li>
<li>Site modelling and lay outing</li>
<li>Quality control</li>
<li>Quantity surveys</li>
<li>Freeway redesign</li>
<li>Establishing a bench mark of pre-existing shape/state in order to detect structural changes resulting from exposure to extreme loadings such as earthquake, vessel/truck impact or fire.</li>
<li>Create GIS (geographic information system) maps and <a class="uri" href="geomatics" title="wikilink">geomatics</a>.</li>
<li>Subsurface laser scanning in mines and Karst voids.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></li>
<li>Forensic documentation <a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a></li>
</ul>
<h3 id="design-process">Design process</h3>
<ul>
<li>Increasing accuracy working with complex parts and shapes,</li>
<li>Coordinating product design using parts from multiple sources,</li>
<li>Updating old CD scans with those from more current technology,</li>
<li>Replacing missing or older parts,</li>
<li>Creating cost savings by allowing as-built design services, for example in automotive manufacturing plants,</li>
<li>"Bringing the plant to the engineers" with web shared scans, and</li>
<li>Saving travel costs.</li>
</ul>
<h3 id="entertainment">Entertainment</h3>

<p>3D scanners are used by the <a href="entertainment_industry" title="wikilink">entertainment industry</a> to create digital 3D models for <a href="movie" title="wikilink">movies</a>, <a href="video_game" title="wikilink">video games</a> and leisure purposes. They are heavily utilised in <a href="virtual_cinematography" title="wikilink">virtual cinematography</a>. In cases where a real-world equivalent of a model exists, it is much faster to scan the real-world object than to manually create a model using 3D modeling software. Frequently, artists sculpt physical models of what they want and scan them into digital form rather than directly creating digital models on a computer.</p>
<h3 id="reverse-engineering">Reverse engineering</h3>

<p><a href="Reverse_engineering" title="wikilink">Reverse engineering</a> of a mechanical component requires a precise digital model of the objects to be reproduced. Rather than a set of points a precise digital model can be represented by a <a href="polygon_mesh" title="wikilink">polygon mesh</a>, a set of flat or curved <a class="uri" href="NURBS" title="wikilink">NURBS</a> surfaces, or ideally for mechanical components, a CAD solid model. A 3D scanner can be used to digitise free-form or gradually changing shaped components as well as prismatic geometries whereas a <a href="coordinate_measuring_machine" title="wikilink">coordinate measuring machine</a> is usually used only to determine simple dimensions of a highly prismatic model. These data points are then processed to create a usable digital model, usually using specialized reverse engineering software.</p>
<h3 id="cultural-heritage">Cultural heritage</h3>

<p>There have been many research projects undertaken via the scanning of historical sites and artifacts both for documentation and analysis purposes.</p>

<p>The combined use of <a href="3D_scanning" title="wikilink">3D scanning</a> and <a href="3D_printing" title="wikilink">3D printing</a> technologies allows the replication of real objects without the use of traditional <a href="plaster_cast" title="wikilink">plaster casting</a> techniques, that in many cases can be too <a href="wikt:invasive" title="wikilink">invasive</a> for being performed on precious or delicate cultural heritage artifacts.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> In an example of a typical application scenario, a <a class="uri" href="gargoyle" title="wikilink">gargoyle</a> model was digitally acquired using a 3D scanner and the produced 3D data was processed using <a class="uri" href="MeshLab" title="wikilink">MeshLab</a>. The resulting digital <a href="3D_model" title="wikilink">3D model</a> was fed to a <a href="rapid_prototyping" title="wikilink">rapid prototyping</a> machine to create a real resin replica of the original object.</p>
<h4 id="michelangelo">Michelangelo</h4>

<p>In 1999, two different research groups started scanning Michelangelo's statues. <a href="Stanford_University" title="wikilink">Stanford University</a> with a group led by <a href="Marc_Levoy" title="wikilink">Marc Levoy</a><a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> used a custom laser triangulation scanner built by <a href="Cyberware_(company)" title="wikilink">Cyberware</a> to scan Michelangelo's statues in Florence, notably the <a href="David_(Michelangelo)" title="wikilink">David</a>, the Prigioni and the four statues in The Medici Chapel. The scans produced a data point density of one sample per 0.25 mm, detailed enough to see Michelangelo's chisel marks. These detailed scans produced a large amount of data (up to 32 gigabytes) and processing the data from his scans took 5 months. Approximately in the same period a research group from <a class="uri" href="IBM" title="wikilink">IBM</a>, led by <a href="Holly_Rushmeier" title="wikilink">H. Rushmeier</a> and F. Bernardini scanned the <a href="The_Deposition_(Michelangelo)" title="wikilink">Pietà of Florence</a> acquiring both geometric and colour details. The digital model, result of the Stanford scanning campaign, was thoroughly used in the 2004 subsequent restoration of the statue.<a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a></p>
<h4 id="monticello">Monticello</h4>

<p>In 2002, David Luebke, et al. scanned Thomas Jefferson's Monticello.<a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a> A commercial time of flight laser scanner, the DeltaSphere 3000, was used. The scanner data was later combined with colour data from digital photographs to create the Virtual Monticello, and the Jefferson's Cabinet exhibits in the New Orleans Museum of Art in 2003. The Virtual Monticello exhibit simulated a window looking into Jefferson's Library. The exhibit consisted of a rear projection display on a wall and a pair of stereo glasses for the viewer. The glasses, combined with polarised projectors, provided a 3D effect. Position tracking hardware on the glasses allowed the display to adapt as the viewer moves around, creating the illusion that the display is actually a hole in the wall looking into Jefferson's Library. The Jefferson's Cabinet exhibit was a barrier stereogram (essentially a non-active hologram that appears different from different angles) of Jefferson's Cabinet.</p>
<h4 id="cuneiform-tablets">Cuneiform tablets</h4>

<p>In 2003, Subodh Kumar, et al. undertook the 3D scanning of ancient cuneiform tablets.<a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a> Again, a laser triangulation scanner was used. The tablets were scanned on a regular grid pattern at a resolution of .</p>
<h4 id="kasubi-tombs">Kasubi Tombs</h4>

<p>A 2009 <a class="uri" href="CyArk" title="wikilink">CyArk</a> 3D scanning project at Uganda's historic <a href="Kasubi_Tombs" title="wikilink">Kasubi Tombs</a>, a <a href="UNESCO_World_Heritage_Site" title="wikilink">UNESCO World Heritage Site</a>, using a Leica HDS 4500, produced detailed architectural models of Muzibu Azaala Mpanga, the main building at the complex and tomb of the <a href="Kabaka_of_Buganda" title="wikilink">Kabakas</a> (Kings) of Uganda. A fire on March 16, 2010, burned down much of the Muzibu Azaala Mpanga structure, and reconstruction work is likely to lean heavily upon the dataset produced by the 3D scan mission.<a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a></p>
<h4 id="plastico-di-roma-antica">"Plastico di Roma antica"</h4>

<p>In 2005, Gabriele Guidi, et al. scanned the "Plastico di Roma antica",<a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a> a model of Rome created in the last century. Neither the triangulation method, nor the time of flight method satisfied the requirements of this project because the item to be scanned was both large and contained small details. They found though, that a modulated light scanner was able to provide both the ability to scan an object the size of the model and the accuracy that was needed. The modulated light scanner was supplemented by a triangulation scanner which was used to scan some parts of the model.</p>
<h4 id="other-projects">Other projects</h4>

<p>The 3D Encounters Project at the <a href="Petrie_Museum_of_Egyptian_Archaeology" title="wikilink">Petrie Museum of Egyptian Archaeology</a> aims to use 3D laser scanning to create a high quality 3D image library of artefacts and enable digital travelling exhibitions of fragile Egyptian artefacts, <a href="English_Heritage" title="wikilink">English Heritage</a> has investigated the use of 3D laser scanning for a wide range of applications to gain archaeological and condition data, and the <a href="National_Conservation_Centre" title="wikilink">National Conservation Centre</a> in Liverpool has also produced 3D laser scans on commission, including portable object and in situ scans of archaeological sites.<a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a> The <a href="Smithsonian_Institution" title="wikilink">Smithsonian Institution</a> has a project called <a href="http://3d.si.edu">Smithsonian X 3D</a> notable for the breadth of types of 3D objects they are attempting to scan. These include small objects such as insects and flowers, to human sized objects such as <a href="Amelia_Earhart" title="wikilink">Amelia Earhart</a>'s Flight Suit to room sized objects such as the <a href="USS_Philadelphia_(1776)" title="wikilink">Gunboat Philadelphia</a> to historic sites such as <a href="Liang_Bua" title="wikilink">Liang Bua</a> in Indonesia. Also of note the data from these scans is being made available to the public for free and downloadable in several data formats.</p>
<h3 id="medical-cadcam">Medical CAD/CAM</h3>

<p>3D scanners are used to capture the 3D shape of a patient in <a class="uri" href="orthotics" title="wikilink">orthotics</a> and <a class="uri" href="dentistry" title="wikilink">dentistry</a>. It gradually supplants tedious plaster cast. CAD/CAM software are then used to design and manufacture the <a class="uri" href="orthosis" title="wikilink">orthosis</a>, <a class="uri" href="prosthesis" title="wikilink">prosthesis</a> or <a href="dental_implants" title="wikilink">dental implants</a>.</p>

<p>Many Chairside dental CAD/CAM systems and Dental Laboratory CAD/CAM systems use 3D Scanner technologies to capture the 3D surface of a dental preparation (either <em>in vivo</em> or <em>in vitro</em>), in order to produce a restoration digitally using CAD software and ultimately produce the final restoration using a CAM technology (such as a CNC milling machine, or 3D printer). The chairside systems are designed to facilitate the 3D scanning of a preparation <em>in vivo</em> and produce the restoration (such as a Crown, Onlay, Inlay or Veneer).</p>
<h3 id="quality-assurance-and-industrial-metrology">Quality assurance and industrial metrology</h3>

<p>The digitalisation of real-world objects is of vital importance in various application domains. This method is especially applied in industrial quality assurance to measure the geometric dimension accuracy. Industrial processes such as assembly are complex, highly automated and typically based on CAD (Computer Aided Design) data. The problem is that the same degree of automation is also required for quality assurance. It is, for example, a very complex task to assemble a modern car, since it consists of many parts that must fit together at the very end of the production line. The optimal performance of this process is guaranteed by quality assurance systems. Especially the geometry of the metal parts must be checked in order to assure that they have the correct dimensions, fit together and finally work reliably.</p>

<p>Within highly automated processes, the resulting geometric measures are transferred to machines that manufacture the desired objects. Due to mechanical uncertainties and abrasions, the result may differ from its digital nominal. In order to automatically capture and evaluate these deviations, the manufactured part must be digitised as well. For this purpose, 3D scanners are applied to generate point samples from the object's surface which are finally compared against the nominal data.<a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a></p>

<p>The process of comparing 3D data against a CAD model is referred to as CAD-Compare, and can be a useful technique for applications such as determining wear patterns on moulds and tooling, determining accuracy of final build, analysing gap and flush, or analysing highly complex sculpted surfaces. At present, laser triangulation scanners, structured light and contact scanning are the predominant technologies employed for industrial purposes, with contact scanning remaining the slowest, but overall most accurate option.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="3D_printing" title="wikilink">3D printing</a></li>
<li><a href="3D_reconstruction" title="wikilink">3D reconstruction</a></li>
<li><a href="3D_computer_graphics_software" title="wikilink">3D computer graphics software</a></li>
<li><a href="Angle-sensitive_pixel" title="wikilink">Angle-sensitive pixel</a></li>
<li><a href="Depth_map" title="wikilink">Depth map</a></li>
<li><a href="Epipolar_geometry" title="wikilink">Epipolar geometry</a></li>
<li><a href="Light-field_camera" title="wikilink">Light-field camera</a></li>
<li><a class="uri" href="Photogrammetry" title="wikilink">Photogrammetry</a></li>
<li><a href="Range_imaging" title="wikilink">Range imaging</a></li>
<li><a href="Structured-light_3D_scanner" title="wikilink">Structured-light 3D scanner</a></li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<ul>
<li>Changsoo Je, Sang Wook Lee, and Rae-Hong Park. <a href="http://dx.doi.org/10.1007/978-3-540-24670-1_8">High-Contrast Color-Stripe Pattern for Rapid Structured-Light Range Imaging</a>. Computer Vision – ECCV 2004, LNCS 3021, pp. 95–107, Springer-Verlag Berlin Heidelberg, May 10, 2004.</li>
<li></li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www.cs.cmu.edu/~seitz/course/Sigg00/notes.html">3D Photography Course Notes</a></li>
<li><a href="http://www.vision.caltech.edu/bouguetj/ICCV98/">3D Photography on your desk</a>: development of a simple and inexpensive method for extracting the three-dimensional shape of objects</li>
<li><a href="http://rangevision.com/en/gallery/3d-gallery">3D Interactive models</a></li>
<li><a href="http://home.lagoa.com/2014/04/whats-the-right-3d-scanner-for-you/">What’s the Right 3D Scanner for You?.</a></li>
<li></li>
<li><a class="uri" href="http://www.optimet.com/">http://www.optimet.com/</a></li>
<li><a class="uri" href="http://www.techbriefs.com/component/content/article/296">http://www.techbriefs.com/component/content/article/296</a></li>
<li><a class="uri" href="http://opticalengineering.spiedigitallibrary.org/article.aspx?articleid=1076763">http://opticalengineering.spiedigitallibrary.org/article.aspx?articleid=1076763</a></li>
<li><a class="uri" href="http://www.mdpi.com/1424-8220/9/9/7021">http://www.mdpi.com/1424-8220/9/9/7021</a></li>
<li><a class="uri" href="http://www.optimet.com/truemap.php">http://www.optimet.com/truemap.php</a></li>
<li></li>
<li></li>
<li></li>
</ul>

<p>"</p>

<p><a href="Category:Computing_input_devices" title="wikilink">Category:Computing input devices</a> <a href="Category:3D_imaging" title="wikilink">Scanner</a> <a href="Category:Industrial_design" title="wikilink">Category:Industrial design</a> <a href="Category:Laser_image_acquisition" title="wikilink">Category:Laser image acquisition</a> <a href="Category:Image_scanners" title="wikilink">Category:Image scanners</a> <a href="Category:Articles_containing_video_clips" title="wikilink">Category:Articles containing video clips</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"><a href="#fnref11">↩</a></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"><a href="http://ceit.aut.ac.ir/~shiry/publications/Matthias-icmtpaper_fin.pdf">Landmark detection by a rotary laser scanner for autonomous robot navigation in sewer pipes</a>, Matthias Dorn et al., Proceedings of the ICMIT 2003, the second International Conference on Mechatronics and Information Technology, pp. 600- 604, Jecheon, Korea, Dec. 2003<a href="#fnref14">↩</a></li>
<li id="fn15"><a href="#fnref15">↩</a></li>
<li id="fn16"><a class="uri" href="http://www.leica-geosystems.us/forensic/">http://www.leica-geosystems.us/forensic/</a><a href="#fnref16">↩</a></li>
<li id="fn17"><a href="#fnref17">↩</a></li>
<li id="fn18"><a href="#fnref18">↩</a></li>
<li id="fn19"><a href="#fnref19">↩</a></li>
<li id="fn20"><a href="#fnref20">↩</a></li>
<li id="fn21"><a href="#fnref21">↩</a></li>
<li id="fn22"><a href="#fnref22">↩</a></li>
<li id="fn23"><a href="#fnref23">↩</a></li>
<li id="fn24"><a href="#fnref24">↩</a></li>
<li id="fn25"><a href="#fnref25">↩</a></li>
</ol>
</section>
</body>
</html>
