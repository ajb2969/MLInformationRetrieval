<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="857">Second partial derivative test</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Second partial derivative test</h1>
<hr/>
<p>In <a class="uri" href="mathematics" title="wikilink">mathematics</a>, the <strong>second partial derivative test</strong> is a method in <a href="multivariable_calculus" title="wikilink">multivariable calculus</a> used to determine if a <a href="Critical_point_(mathematics)" title="wikilink">critical point</a> of a function is a <a href="maxima_and_minima" title="wikilink">local minimum</a>, maximum or <a href="saddle_point" title="wikilink">saddle point</a>.</p>
<h2 id="the-test">The test</h2>
<h3 id="functions-of-two-variables">Functions of two variables</h3>
<p>Suppose that <em>f</em>(<em>x</em>, <em>y</em>) is a differentiable <a href="real_function" title="wikilink">real function</a> of two variables whose second <a href="partial_derivative" title="wikilink">partial derivatives</a> exist. The <a href="Hessian_matrix" title="wikilink">Hessian matrix</a> <em>H</em> of <em>f</em> is the 2 × 2 matrix of partial derivatives of <em>f</em>:</p>
<p><span class="LaTeX">$$H(x,y) = \begin{pmatrix}f_{xx}(x,y) &f;_{xy}(x,y)\\f_{yx}(x,y) &f;_{yy}(x,y)\end{pmatrix}$$</span>.</p>
<p>Define <em>D</em>(<em>x</em>, <em>y</em>) to be the <a class="uri" href="determinant" title="wikilink">determinant</a></p>
<p><span class="LaTeX">$$D(x,y)=\det(H(x,y)) = f_{xx}(x,y)f_{yy}(x,y) - \left( f_{xy}(x,y) \right)^2$$</span>,</p>
<p>of <em>H</em>. Finally, suppose that (<em>a</em>, <em>b</em>) is a critical point of <em>f</em> (that is, <em>f</em><sub><em>x</em></sub>(<em>a</em>, <em>b</em>) = <em>f</em><sub><em>y</em></sub>(<em>a</em>, <em>b</em>) = 0). Then the second partial derivative test asserts the following:<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>
<ol>
<li>If <span class="LaTeX">$D(a,b)>0$</span> and <span class="LaTeX">$f_{xx}(a,b)>0$</span> then <span class="LaTeX">$(a,b)$</span> is a local minimum of <em>f</em>.</li>
<li>If <span class="LaTeX">$D(a,b)>0$</span> and <span class="LaTeX">$f_{xx}(a,b)<0$</span> then <span class="LaTeX">$(a,b)$</span> is a local maximum of <em>f</em>.</li>
<li>If <span class="LaTeX">$D(a,b)<0$</span> then <span class="LaTeX">$(a,b)$</span> is a <a href="saddle_point" title="wikilink">saddle point</a> of <em>f</em>.</li>
<li>If <span class="LaTeX">$D(a,b)=0$</span> then the second derivative test is inconclusive, and the point (<em>a</em>, <em>b</em>) could be any of a minimum, maximum or saddle point.</li>
</ol>
<p>Note that other equivalent versions of the test are possible. For example, some texts may use the <a href="trace_(linear_algebra)" title="wikilink">trace</a> <em>f</em><sub><em>xx</em></sub> + <em>f</em><sub><em>yy</em></sub> in place of the value <em>f</em><sub><em>xx</em></sub> in the first two cases above. Such variations in the procedure applied do not alter the outcome of the test.</p>
<h3 id="functions-of-many-variables">Functions of many variables</h3>
<p>For a function <em>f</em> of more than two variables, there is a generalization of the rule above. In this context, instead of examining the determinant of the Hessian matrix, one must look at the <a href="eigenvalues_and_eigenvectors" title="wikilink">eigenvalues</a> of the Hessian matrix at the critical point. The following test can be applied at any critical point (<em>a</em>, <em>b</em>, ...) for which the Hessian matrix is <a href="invertible_matrix" title="wikilink">invertible</a>:</p>
<ol>
<li>If the Hessian is positive definite (equivalently, has all eigenvalues positive) at (<em>a</em>, <em>b</em>, ...), then <em>f</em> attains a local minimum at (<em>a</em>, <em>b</em>, ...).</li>
<li>If the Hessian is negative definite (equivalently, has all eigenvalues negative) at (<em>a</em>, <em>b</em>, ...), then <em>f</em> attains a local maximum at (<em>a</em>, <em>b</em>, ...).</li>
<li>If the Hessian has both positive and negative eigenvalues then (<em>a</em>, <em>b</em>, ...) is a saddle point for <em>f</em> (and in fact this is true even if (<em>a</em>, <em>b</em>, ...) is degenerate).</li>
</ol>
<p>In those cases not listed above, the test is inconclusive.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>
<p>Note that for functions of three or more variables, the <em>determinant</em> of the Hessian does not provide enough information to classify the critical point, because the number of jointly sufficient second-order conditions is equal to the number of variables, and the sign condition on the determinant of the Hessian is only one of the conditions. Note also that this statement of the second derivative test for many variables also applies in the two-variable and one-variable case. In the latter case, we recover the usual <a href="second_derivative_test" title="wikilink">second derivative test</a>.</p>
<p>In the two variable case, <span class="LaTeX">$D(a, b)$</span> and <span class="LaTeX">$f_{xx}(a,b)$</span> are the principal <a href="Minor_(linear_algebra)" title="wikilink">minors</a> of the Hessian. The first two conditions listed above on the signs of these minors are the conditions for the positive or negative definiteness of the Hessian. For the general case of an arbitrary number <em>n</em> of variables, there are <em>n</em> sign conditions on the <em>n</em> principal minors of the Hessian matrix that together are equivalent to positive or negative definiteness of the Hessian: for a local minimum, all the principal minors need to be positive, while for a local maximum, the minors with an odd number of rows and columns need to be negative and the minors with an even number of rows and columns need to be positive. See <a href="Hessian_matrix#Bordered_Hessian" title="wikilink">Hessian matrix#Bordered Hessian</a> for a discussion that generalizes these rules to the case of equality-constrained optimization.</p>
<h2 id="geometric-interpretation-in-the-two-variable-case">Geometric interpretation in the two-variable case</h2>
<p>Assume that all derivatives of <em>f</em> are evaluated at (<em>a</em>, <em>b</em>), and that the values of the first derivatives vanish there.</p>
<p>If <span class="LaTeX">$D<0$</span> then <span class="LaTeX">$f_{xx}f_{yy} < f_{xy}^2$</span>. If <span class="LaTeX">$f_{xx}$</span> and <span class="LaTeX">$f_{yy}$</span> have different signs, then one must be positive and the other must be negative. Thus the concavities of the <em>x</em> cross section (the <em>yz</em> trace) and the <em>y</em> cross section (the <em>xz</em> trace) are in opposite direction. This is clearly a saddle point.</p>
<p>If <span class="LaTeX">$D>0$</span> then <span class="LaTeX">$f_{xx}f_{yy} > f_{xy}^2$</span>, which implies that <span class="LaTeX">$f_{xx}$</span> and <span class="LaTeX">$f_{yy}$</span> are the same sign and sufficiently large. For this case the concavities of the <em>x</em> and <em>y</em> cross sections are either both up if positive, or both down if negative. This is clearly a local minimum or a local maximum, respectively.</p>
<p>This leaves the last case of <em>D</em> f_{xx}f_{yy}  — and <span class="LaTeX">$f_{xx}$</span> and <span class="LaTeX">$f_{yy}$</span> having the same sign. The geometric interpretation of what is happening here is that since <span class="LaTeX">$f_{xy}$</span> is large it means the slope of the graph in one direction is changing rapidly as we move in the orthogonal direction and overcoming the concavity of the orthogonal direction. So for example, let's take the case of all second derivatives are positive and (<em>a</em>,<em>b</em>) = (0,0). In the case of <em>D</em> > 0 it would mean that any direction in the <em>xy</em> plane we move from the origin, the value of the function increases — a local minimum. In the <em>D</em> f_{xy} sufficiently large), however, if we move at some direction between the <em>x</em> and <em>y</em> axis into the second quadrant, for example, of the <em>xy</em> plane, then despite the fact that the positive concavity would cause us to expect the value of the function to increase, the slope in the <em>x</em> direction is increasing even faster, which means that as we go left (negative <em>x</em>-direction) into the second quadrant, the value of the function ends up decreasing. Additionally, since the origin is a stationary point by hypothesis, we have a saddle point.</p>
<h2 id="examples">Examples</h2>
<p><a href="File:Second_partial_derivative_test.png" title="wikilink">thumb|upright=1.75|critical points of <span class="LaTeX">$f(x, y) = (x+y)(xy + xy^2)$</span><br/>
 maxima (red) and saddle points (blue)</a> To find and classify the critical points of the function</p>
<p><span class="LaTeX">$$z = f(x, y) = (x+y)(xy + xy^2)$$</span>,</p>
<p>we first set the partial derivatives</p>
<p><span class="LaTeX">$$\frac{\partial z}{\partial x} = y(2x +y)(y+1)$$</span> and <span class="LaTeX">$\frac{\partial z}{\partial y} = x \left( 3y^2 +2y(x+1) + x \right)$</span></p>
<p>equal to zero and solve the resulting equations simultaneously to find the four critical points</p>
<p><span class="LaTeX">$$(0,0), (0, -1), (1,-1)$$</span> and <span class="LaTeX">$\left(\frac{3}{8}, -\frac{3}{4}\right)$</span>.</p>
<p>In order to classify the critical points, we examine the value of the determinant <em>D</em>(<em>x</em>, <em>y</em>) of the Hessian of <em>f</em> at each of the four critical points. We have</p>
<p><span class="LaTeX">$$\begin{align}
 D(a, b) &= f_{xx}(a,b)f_{yy}(a,b) - \left( f_{xy}(a,b) \right)^2 \\ 
         &= 2b(b+1) \cdot 2a(a + 3b + 1) - (2a + 2b + 4ab + 3b^2)^2.
\end{align}$$</span> Now we plug in all the different critical values we found to label them; we have</p>
<p><span class="LaTeX">$$D(0, 0) = 0; ~~ D(0, -1) = -1; ~~ D(1, -1) = -1; ~~ D\left(\frac{3}{8}, -\frac{3}{4}\right) = \frac{27}{128}.$$</span></p>
<p>Thus, the second partial derivative test indicates that <em>f</em>(<em>x</em>, <em>y</em>) has saddle points at (0, −1) and (1, −1) and has a local maximum at <span class="LaTeX">$\left(\frac{3}{8}, -\frac{3}{4}\right)$</span> since <span class="LaTeX">$f_{xx} = -\frac{3}{8} < 0$</span>. At the remaining critical point (0, 0) the second derivative test is insufficient, and one must use higher order tests or other tools to determine the behavior of the function at this point. (In fact, one can show that <em>f</em> takes both positive and negative values in small neighborhoods around (0, 0) and so this point is a saddle point of <em>f</em>.)</p>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<ul>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://tutorial.math.lamar.edu/Classes/CalcIII/RelativeExtrema.aspx"><em>Relative Minimums and Maximums</em></a> - Paul's Online Math Notes - Calc III Notes (Lamar University)</li>
<li></li>
</ul>
<p>"</p>
<p><a href="Category:Multivariable_calculus" title="wikilink">Category:Multivariable calculus</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">, [<a class="uri" href="http://books.google.de/books?id=eNHhKxXCJaEC&pg">http://books.google.de/books?id=eNHhKxXCJaEC&pg</a>;=PA803 p. 803].<a href="#fnref1">↩</a></li>
<li id="fn2">Kurt Endl/Wolfgang Luh: <em>Analysis II</em>. Aula-Verlag 1972, 7th edition 1989, ISBN 3-89104-455-0, pp. 248-258 (German)<a href="#fnref2">↩</a></li>
</ol>
</section>
</body>
</html>
