<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title offset="763">Similarity learning</title>
   <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js">
    </script>
</head>
<body>
<h1>Similarity learning</h1>
<hr/>
<p><strong>Similarity learning</strong> is an area of supervised <a href="machine_learning" title="wikilink">machine learning</a> in <a href="artificial_intelligence" title="wikilink">artificial intelligence</a>. It is closely related to <a href="regression_(machine_learning)" title="wikilink">regression</a> and <a href="classification_in_machine_learning" title="wikilink">classification</a>, but the goal is to learn from examples a similarity function that measures how similar or related two objects are. It has applications in <a class="uri" href="ranking" title="wikilink">ranking</a>, in <a href="recommendation_systems" title="wikilink">recommendation systems</a> and face verification.</p>
<h2 id="learning-setup">Learning setup</h2>
<p>There are three common setups for similarity and metric distance learning.</p>
<ul>
<li><em><a href="Regression_(machine_learning)" title="wikilink">Regression</a> similarity learning</em>. In this setup, pairs of objects are given <span class="LaTeX">$(x_i^1, x_i^2)$</span> together with a measure of their similarity <span class="LaTeX">$y_i \in R$</span>. The goal is to learn a function that approximates <span class="LaTeX">$f(x_i^1, x_i^2) \sim y_i$</span> for every new labeled triplet example <span class="LaTeX">$(x_i^1, x_i^2, y_i)$</span>. This is typically achieved by minimizing a regularized loss <span class="LaTeX">$min_W \sum_i loss(w;x_i^1, x_i^2,y_i) + reg(w)$</span>.</li>
<li><em><a href="Classification_in_machine_learning" title="wikilink">Classification</a> similarity learning</em>. Given are pairs of similar objects <span class="LaTeX">$(x_i, x_i^+)$</span> and non similar objects <span class="LaTeX">$(x_i, x_i^-)$</span>. An equivalent formulation is that every pair <span class="LaTeX">$(x_i^1, x_i^2)$</span> is given together with a binary label <span class="LaTeX">$y_i \in \{0,1\}$</span> that determines if the two objects are similar or not. The goal is again to learn a classifier that can decide if a new pair of objects is similar or not.</li>
<li><em>Ranking similarity learning</em>. Given are triplets of objects <span class="LaTeX">$(x_i, x_i^+, x_i^-)$</span> whose relative similarity obey a predefined order<span class="LaTeX">$$x_i$$</span> is known to be more similar to <span class="LaTeX">$x_i^+$</span> than to <span class="LaTeX">$x_i^-$</span>. The goal is to learn a function <span class="LaTeX">$f$</span> such that for any new triplet of objects <span class="LaTeX">$(x, x^+, x^-)$</span>, it obeys <span class="LaTeX">$f(x, x^+) > f(x, x^-)$</span>. This setup assumes a weaker form of supervision than in regression, because instead of providing an exact measure of similarity, one only has to provide the relative order of similarity. For this reason, ranking-based similarity learning is easier to apply in real large scale applications.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></li>
</ul>
<p>A common approach for learning similarity, is to model the similarity function as a <a href="bilinear_form" title="wikilink">bilinear form</a>. For example, in the case of ranking similarity learning, one aims to learn a matrix W that parametrizes the similarity function <span class="LaTeX">$f_W(x, z)  = x^T W z$</span>.</p>
<h2 id="metric-learning">Metric learning</h2>
<p>Similarity learning is closely related to <em>distance metric learning</em>. Metric learning is the task of learning a distance function over objects. A <a href="Metric_(mathematics)" title="wikilink">metric</a> or <a href="distance_function" title="wikilink">distance function</a> has to obey four axioms: <a href="non-negative" title="wikilink">non-negativity</a>, <a href="Identity_of_indiscernibles" title="wikilink">Identity of indiscernibles</a>, <a class="uri" href="symmetry" title="wikilink">symmetry</a> and <a class="uri" href="subadditivity" title="wikilink">subadditivity</a> / triangle inequality. In practice, metric learning algorithms ignore the condition of identity of indiscernibles and learn a pseudo-metric.</p>
<p>When the objects <span class="LaTeX">$x_i$</span> are vectors in <span class="LaTeX">$R^d$</span>, then any matrix <span class="LaTeX">$W$</span> in the symmetric positive semi-definite cone <span class="LaTeX">$S_+^d$</span> defines a distance pseudo-metric of the space of x through the form <span class="LaTeX">$D_W(x_1, x_2)^2 = (x_1-x_2)^{\top} W (x_1-x_2)$</span>. When <span class="LaTeX">$W$</span> is a symmetric positive definite matrix, <span class="LaTeX">$D_W$</span> is a metric. Moreover, as any symmetric positive semi-definite matrix <span class="LaTeX">$W \in S_+^d$</span> can be decomposed as <span class="LaTeX">$W = L^{\top}L$</span> where <span class="LaTeX">$L \in R^{e \times d}$</span> and <span class="LaTeX">$e \geq rank(W)$</span>, the distance function <span class="LaTeX">$D_W$</span> can be rewritten equivalently <span class="LaTeX">$D_W(x_1, x_2)^2 = (x_1-x_2)^{\top} L^{\top}L (x_1-x_2) = \| L (x_1-x_2) \|_2^2$</span>. The distance <span class="LaTeX">$D_W(x_1, x_2)^2=\| x_1' - x_2' \|_2^2$</span> corresponds to the Euclidean distance between the projected feature vectors <span class="LaTeX">$x_1'= Lx_1$</span> and <span class="LaTeX">$x_2'= Lx_2$</span>. Some well-known approaches for metric learning include <a href="Large_margin_nearest_neighbor" title="wikilink">Large margin nearest neighbor</a> <a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> , Information theoretic metric learning (ITML).<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>
<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, the <a class="uri" href="covariance" title="wikilink">covariance</a> matrix of the data is sometimes used to define a distance metric called <a href="Mahalanobis_distance" title="wikilink">Mahalanobis distance</a>.</p>
<h2 id="applications">Applications</h2>
<p>Similarity learning is used in information retrieval for learning to rank, in face verification or face identification,<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> and in <a href="recommendation_systems" title="wikilink">recommendation systems</a>. Also, many machine learning approaches rely on some metric. This includes unsupervised learning such as <a href="clustering_(machine_learning)" title="wikilink">clustering</a>, which groups together close or similar objects. It also includes supervised approaches like <a href="K-nearest_neighbor_algorithm" title="wikilink">K-nearest neighbor algorithm</a> which rely on labels of nearby objects to decide on the label of a new object. Metric learning has been proposed as a preprocessing step for many of these approaches .<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h2 id="further-reading">Further reading</h2>
<p>For further information on this topic, see the survey on metric and similarity learning by Kulis.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></p>
<h2 id="references">References</h2>
<p>"</p>
<p><a href="Category:Machine_learning" title="wikilink">Category:Machine learning</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
</ol>
</section>
</body>
</html>
