<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="424">Additive smoothing</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Additive smoothing</h1>
<hr/>

<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, <strong>additive smoothing</strong>, also called <strong><a href="Pierre-Simon_Laplace" title="wikilink">Laplace</a> smoothing</strong><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> (not to be confused with <a href="Laplacian_smoothing" title="wikilink">Laplacian smoothing</a>), or <strong><a href="George_James_Lidstone" title="wikilink">Lidstone</a> smoothing</strong>, is a technique used to <a href="smoothing" title="wikilink">smooth</a> <a href="Level_of_measurement#Nominal_scale" title="wikilink">categorical data</a>. Given an observation <strong>x</strong> = (<em>x</em><sub>1</sub>, …, <em>x</em><sub><em>d</em></sub>) from a <a href="multinomial_distribution" title="wikilink">multinomial distribution</a> with <em>N</em> trials and parameter vector <strong>θ</strong> = (<em>θ</em><sub>1</sub>, …, <em>θ</em><sub><em>d</em></sub>), a "smoothed" version of the data gives the <a class="uri" href="estimator" title="wikilink">estimator</a>:</p>

<p>

<math display="block" id="Additive_smoothing:0">
 <semantics>
  <mrow>
   <msub>
    <mover accent="true">
     <mi>θ</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mi>i</mi>
   </msub>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <msub>
      <mi>x</mi>
      <mi>i</mi>
     </msub>
     <mo>+</mo>
     <mi>α</mi>
    </mrow>
    <mrow>
     <mi>N</mi>
     <mo>+</mo>
     <mrow>
      <mi>α</mi>
      <mi>d</mi>
     </mrow>
    </mrow>
   </mfrac>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>i</mi>
    <mo>=</mo>
    <mn>1</mn>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <mi>d</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <ci>normal-^</ci>
      <ci>θ</ci>
     </apply>
     <ci>i</ci>
    </apply>
    <eq></eq>
    <apply>
     <divide></divide>
     <apply>
      <plus></plus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>i</ci>
      </apply>
      <ci>α</ci>
     </apply>
     <apply>
      <plus></plus>
      <ci>N</ci>
      <apply>
       <times></times>
       <ci>α</ci>
       <ci>d</ci>
      </apply>
     </apply>
    </apply>
    <ci>italic-</ci>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">i</csymbol>
     <eq></eq>
     <cn type="integer">1</cn>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <csymbol cd="unknown">d</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-,</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\theta}_{i}=\frac{x_{i}+\alpha}{N+\alpha d}\qquad(i=1,\ldots,d),
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>α</em> &gt; 0 is the smoothing parameter (<em>α</em> = 0 corresponds to no smoothing). Additive smoothing is a type of <a href="shrinkage_estimator" title="wikilink">shrinkage estimator</a>, as the resulting estimate will be between the empirical estimate <em>x<sub>i</sub></em> / <em>N</em>, and the uniform probability 1/<em>d</em>. Using Laplace's <a href="rule_of_succession" title="wikilink">rule of succession</a>, some authors have argued that <em>α</em> should be 1 (in which case the term <strong>add-one smoothing</strong><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> is also used), though in practice a smaller value is typically chosen.</p>

<p>From a <a href="Bayesian_inference" title="wikilink">Bayesian</a> point of view, this corresponds to the <a href="expected_value" title="wikilink">expected value</a> of the <a href="posterior_distribution" title="wikilink">posterior distribution</a>, using a symmetric <a href="Dirichlet_distribution" title="wikilink">Dirichlet distribution</a> with parameter <em>α</em> as a <a href="prior_distribution" title="wikilink">prior</a>.</p>
<h2 id="history">History</h2>

<p>Laplace came up with this smoothing technique when he tried to estimate the chance that the sun will rise tomorrow. His rationale was that even given a large sample of days with the rising sun, we still can not be completely sure that the sun will still rise tomorrow (known as the <a href="sunrise_problem" title="wikilink">sunrise problem</a>).<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<h2 id="generalized-to-the-case-of-known-incidence-rates">Generalized to the case of known incidence rates</h2>

<p>Often you are testing the bias of an unknown trial population against a control population with known parameters (incidence rates) <strong>μ</strong> = (<em>μ</em><sub>1</sub>, …, <em>μ</em><sub><em>d</em></sub>). In this case the uniform probability 1/<em>d</em> should be replaced by the known incidence rate of the control population <em>μ</em><sub><em>i</em></sub> to calculate the smoothed estimator :</p>

<p>

<math display="block" id="Additive_smoothing:1">
 <semantics>
  <mrow>
   <msub>
    <mover accent="true">
     <mi>θ</mi>
     <mo stretchy="false">^</mo>
    </mover>
    <mi>i</mi>
   </msub>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <msub>
      <mi>x</mi>
      <mi>i</mi>
     </msub>
     <mo>+</mo>
     <mrow>
      <msub>
       <mi>μ</mi>
       <mi>i</mi>
      </msub>
      <mi>α</mi>
      <mi>d</mi>
     </mrow>
    </mrow>
    <mrow>
     <mi>N</mi>
     <mo>+</mo>
     <mrow>
      <mi>α</mi>
      <mi>d</mi>
     </mrow>
    </mrow>
   </mfrac>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>i</mi>
    <mo>=</mo>
    <mn>1</mn>
    <mo>,</mo>
    <mi mathvariant="normal">…</mi>
    <mo>,</mo>
    <mi>d</mi>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <apply>
      <ci>normal-^</ci>
      <ci>θ</ci>
     </apply>
     <ci>i</ci>
    </apply>
    <eq></eq>
    <apply>
     <divide></divide>
     <apply>
      <plus></plus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>i</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>μ</ci>
        <ci>i</ci>
       </apply>
       <ci>α</ci>
       <ci>d</ci>
      </apply>
     </apply>
     <apply>
      <plus></plus>
      <ci>N</ci>
      <apply>
       <times></times>
       <ci>α</ci>
       <ci>d</ci>
      </apply>
     </apply>
    </apply>
    <ci>italic-</ci>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <csymbol cd="unknown">i</csymbol>
     <eq></eq>
     <cn type="integer">1</cn>
     <ci>normal-,</ci>
     <ci>normal-…</ci>
     <ci>normal-,</ci>
     <csymbol cd="unknown">d</csymbol>
     <ci>normal-)</ci>
    </cerror>
    <ci>normal-,</ci>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \hat{\theta}_{i}=\frac{x_{i}+\mu_{i}\alpha d}{N+\alpha d}\qquad(i=1,\ldots,d),
  </annotation>
 </semantics>
</math>

</p>

<p>As a consistency check, if the empirical estimator happens to equal the incidence rate, i.e. <em>μ</em><sub>i</sub> = <em>x</em><sub>i</sub> / <em>N</em>, the smoothed estimator is independent of <em>α</em> and also equals the incidence rate.</p>
<h2 id="applications">Applications</h2>
<h3 id="classification">Classification</h3>

<p>Additive smoothing is commonly a component of <a href="naive_Bayes_classifier" title="wikilink">naive Bayes classifiers</a>.</p>
<h3 id="statistical-language-modelling">Statistical language modelling</h3>

<p>In a <a href="bag_of_words_model" title="wikilink">bag of words model</a> of natural language processing and information retrieval, the data consists of the number of occurrences of each word in a document. Additive smoothing allows the assignment of non-zero probabilities to words which do not occur in the sample.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Bayesian_average" title="wikilink">Bayesian average</a></li>
<li><a class="uri" href="Pseudocount" title="wikilink">Pseudocount</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li>SF Chen, J Goodman (1996). "<a href="http://www.aclweb.org/anthology/P/P96/P96-1041.pdf">An empirical study of smoothing techniques for language modeling</a>". <em>Proceedings of the 34th annual meeting on Association for Computational Linguistics</em>.</li>
</ul>

<p>"</p>

<p><a href="Category:Statistical_natural_language_processing" title="wikilink">Category:Statistical natural language processing</a> <a href="Category:Categorical_data" title="wikilink">Category:Categorical data</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">C.D. Manning, P. Raghavan and M. Schütze (2008). <em>Introduction to Information Retrieval</em>. Cambridge University Press, p. 260.<a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="http://www.youtube.com/watch?v=qRJ3GKMOFrE#t=4201">Lecture 5 | Machine Learning (Stanford)</a> at 1h10m into the lecture<a href="#fnref4">↩</a></li>
</ol>
</section>
</body>
</html>
