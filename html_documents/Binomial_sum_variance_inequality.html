<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1326">Binomial sum variance inequality</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Binomial sum variance inequality</h1>
<hr/>

<p>The <strong>binomial sum variance inequality</strong> states that the variance of the sum of <a href="Binomial_distribution" title="wikilink">binomially distributed</a> <a href="random_variable" title="wikilink">random variables</a> will always be less than or equal to the variance of a binomial variable with the same <a href="Binomial_distribution#Mean_and_variance" title="wikilink"><em>n</em> and <em>p</em></a> parameters. In <a href="probability_theory" title="wikilink">probability theory</a> and <a class="uri" href="statistics" title="wikilink">statistics</a>, the <a href="Binomial_distribution#Sums_of_binomials" title="wikilink">sum</a> of independent binomial random variables is itself a binomial random variable if all the component variables share the same <a href="Binomial_distribution#Mean_and_variance" title="wikilink">success probability</a>. If success probabilities differ, the probability distribution of the sum is not binomial.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> The lack of uniformity in success probabilities across independent trials leads to a smaller variance.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a><a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> and is a special case of a more general theorem involving the <a href="expected_value" title="wikilink">expected value</a> of convex functions.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> In some statistical applications, the standard binomial variance estimator can be used even if the component probabilities differ, though with a variance estimate that has an upward <a href="Bias_(statistics)" title="wikilink">bias</a>.</p>
<h2 id="inequality-statement">Inequality statement</h2>

<p>Consider the sum, <em>Z</em>, of two independent binomial random variables, <em>X</em> ~ B(<em>m</em><sub>0</sub>, <em>p</em><sub>0</sub>) and <em>Y</em> ~ B(<em>m</em><sub>1</sub>, <em>p</em><sub>1</sub>), where <em>Z</em> = <em>X</em> + <em>Y</em>. Then, the variance of <em>Z</em> is less than or equal to its variance under the assumption that <em>p</em><sub>0</sub> = <em>p</em><sub>1</sub>, that is, if <em>Z</em> had a binomial distribution.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> Symbolically, Var(<em>Z</em>) ≤ E[<em>Z</em>] (1 – E[<em>Z</em>] / (<em>m</em><sub>0</sub> + <em>m</em><sub>1</sub>) ).</p>

<p><strong>Proof.</strong> If <em>Z</em> has a binomial distribution with parameters <em>n</em> and <em>p</em>, then the <a href="expected_value" title="wikilink">expected value</a> of <em>Z</em>, [[Binomial distribution#Mean and variance| E[<em>Z</em>] = <em>np</em>]] and the variance of <em>Z</em> is equal to <a href="Binomial_distribution#Mean_and_variance" title="wikilink"><em>np</em>(1 – <em>p</em>)</a> or equivalently, Var(<em>Z</em>) = E[<em>Z</em>] (1 – E[<em>Z</em>] / <em>n</em>), where in this case, <em>n</em> = <em>m</em><sub>0</sub> + <em>m</em><sub>1</sub>. The random variables <em>X</em> and <em>Y</em> are independent, therefore the <a href="Variance#Sum_of_uncorrelated_variables" title="wikilink">variance of the sum is equal to the sum of the variances</a>, that is, Var(<em>Z</em>) = E[<em>X</em>] (1 – E[<em>X</em>] / <em>m</em><sub>0</sub>) + E[<em>Y</em>] (1 – E[<em>Y</em>] / <em>m</em><sub>1</sub>). Thus, if we can show that, E[<em>X</em>] (1 – E[<em>X</em>] / <em>m</em><sub>0</sub>) + E[<em>Y</em>] (1 – E[<em>Y</em>] / <em>m</em><sub>1</sub>) ≤ E[<em>Z</em>] (1 – E[<em>Z</em>] / (<em>m</em><sub>1</sub>+<em>m</em><sub>0</sub>)), then we have proved the theorem. Simplifying this inequality yields, E[<em>X</em>](1 – E[<em>X</em>]/<em>m</em><sub>0</sub>) + E[<em>Y</em>](1 – E[<em>Y</em>]/<em>m</em><sub>1</sub>) ≤ (E[<em>X</em>] + E[<em>Y</em>])(1 – (E[<em>X</em>] + E[<em>Y</em>])/(<em>m</em><sub>0</sub> + <em>m</em><sub>1</sub>)), which leads to the relation (<em>m</em><sub>1</sub>E[<em>X</em>])<sup>2</sup> – 2<em>m</em><sub>0</sub><em>m</em><sub>1</sub>E[<em>X</em>]E[<em>Y</em>] + (<em>m</em><sub>0</sub> E[<em>Y</em>])<sup>2</sup> ≥ 0, or equivalently, (<em>m</em><sub>1</sub>E[<em>X</em>] – <em>m</em><sub>0</sub>E[<em>Y</em>])<sup>2</sup> ≥ 0, which is true for all independent binomial distributions that <em>X</em> and <em>Y</em> could take, because a square of a <a href="real_number" title="wikilink">real number</a> is always greater than or equal to zero.</p>

<p>Although this proof was developed for the sum of two variables, it is easily generalized to greater than two. Additionally, if the individual success probabilities are known, then the variance is known to take the form<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>

<p>

<math display="block" id="Binomial_sum_variance_inequality:0">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mo>Var</mo>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>Z</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mi>n</mi>
      <mover accent="true">
       <mi>p</mi>
       <mo stretchy="false">¯</mo>
      </mover>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mn>1</mn>
        <mo>-</mo>
        <mover accent="true">
         <mi>p</mi>
         <mo stretchy="false">¯</mo>
        </mover>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mo>-</mo>
     <mrow>
      <mi>n</mi>
      <msup>
       <mi>s</mi>
       <mn>2</mn>
      </msup>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>Var</ci>
     <ci>Z</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <times></times>
      <ci>n</ci>
      <apply>
       <ci>normal-¯</ci>
       <ci>p</ci>
      </apply>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <apply>
        <ci>normal-¯</ci>
        <ci>p</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>n</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>s</ci>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{Var}(Z)=n\bar{p}(1-\bar{p})-ns^{2},
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Binomial_sum_variance_inequality:1">
 <semantics>
  <mrow>
   <msup>
    <mi>s</mi>
    <mn>2</mn>
   </msup>
   <mo>=</mo>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mi>n</mi>
    </mfrac>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </msubsup>
     <msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <msub>
         <mi>p</mi>
         <mi>i</mi>
        </msub>
        <mo>-</mo>
        <mover accent="true">
         <mi>p</mi>
         <mo stretchy="false">¯</mo>
        </mover>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mn>2</mn>
     </msup>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>s</ci>
     <cn type="integer">2</cn>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>n</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <minus></minus>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>p</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <ci>normal-¯</ci>
         <ci>p</ci>
        </apply>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   s^{2}=\frac{1}{n}\sum_{i=1}^{n}(p_{i}-\bar{p})^{2}
  </annotation>
 </semantics>
</math>

. This expression also implies that the variance is always less than that of the binomial distribution with 

<math display="inline" id="Binomial_sum_variance_inequality:2">
 <semantics>
  <mrow>
   <mi>p</mi>
   <mo>=</mo>
   <mover accent="true">
    <mi>p</mi>
    <mo stretchy="false">¯</mo>
   </mover>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>p</ci>
    <apply>
     <ci>normal-¯</ci>
     <ci>p</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p=\bar{p}
  </annotation>
 </semantics>
</math>

, because the standard expression for the variance is decreased by <em>ns</em><sup>2</sup>, a positive number.</p>
<h2 id="applications">Applications</h2>

<p>The inequality can be useful in the context of <a href="Multiple_comparisons_problem" title="wikilink">multiple testing</a>, where many <a href="statistical_hypothesis_testing" title="wikilink">statistical hypothesis tests</a> are conducted within a particular study. Each test can be treated as a <a href="Bernoulli_distribution" title="wikilink">Bernoulli variable</a> with a success probability <em>p</em>. Consider the total number of positive tests as a random variable denoted by <em>S</em>. This quantity is important in the estimation of <a href="false_discovery_rate" title="wikilink"> false discovery rates (FDR)</a>, which quantify uncertainty in the test results. If the <a href="statistical_hypothesis_testing#the_testing_process" title="wikilink">null hypothesis</a> is true for some tests and the <a href="statistical_hypothesis_testing#the_testing_process" title="wikilink">alternative hypothesis</a> is true for other tests, then success probabilities are likely to differ between these two groups. However, the variance inequality theorem states that if the tests are independent, the variance of <em>S</em> will be no greater than it would be under a binomial distribution.</p>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Probability_theorems" title="wikilink">Category:Probability theorems</a> <a href="Category:Statistical_theorems" title="wikilink">Category:Statistical theorems</a> <a href="Category:Statistical_inequalities" title="wikilink">Category:Statistical inequalities</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Butler K, Stephens M. 1993. The distribution of a sum of binomial random variables. [<a class="uri" href="http://www.dtic.mil/cgi-bin/GetTRDoc?Location=U2&amp;doc">http://www.dtic.mil/cgi-bin/GetTRDoc?Location=U2&amp;doc;</a>;=GetTRDoc.pdf&amp;AD;=ADA266969 Technical Report No. 467.] Department of Statistics, Stanford University<a href="#fnref1">↩</a></li>
<li id="fn2">Nedelman, J and Wallenius, T., 1986. Bernoulli trials, Poisson trials, surprising variances, and Jensen’s Inequality. The American Statistician, 40(4):286–289.<a href="#fnref2">↩</a></li>
<li id="fn3">Feller, W. 1968. An introduction to probability theory and its applications (Vol. 1, 3rd ed.). New York: John Wiley.<a href="#fnref3">↩</a></li>
<li id="fn4">Johnson, N. L. and Kotz, S. 1969. Discrete distributions. New York: John Wiley<a href="#fnref4">↩</a></li>
<li id="fn5">Kendall, M. and Stuart, A. 1977. The advanced theory of statistics. New York: Macmillan.<a href="#fnref5">↩</a></li>
<li id="fn6">Drezner, Z. and Farnum, N. 2007. A generalized binomial distribution. Communications in Statistics – Theory and Methods, 22(11):3051–3063.<a href="#fnref6">↩</a></li>
<li id="fn7">Hoeffding, W. 1956. On the distribution of the number of successes in indepdendent trials. Annals of Mathematical Statistics (27):713–721.<a href="#fnref7">↩</a></li>
<li id="fn8">Millstein J, Volfson D. 2013. <a href="http://journal.frontiersin.org/Journal/10.3389/fgene.2013.00179/abstract">Computationally efficient permutation-based confidence interval estimation for tail-area FDR.</a> Frontiers in Genetics | Statistical Genetics and Methodology 4(179):1–11. PMCID: PMC3775454<a href="#fnref8">↩</a></li>
<li id="fn9"></li>
</ol>
</section>
</body>
</html>
