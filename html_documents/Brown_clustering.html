<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="842">Brown clustering</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Brown clustering</h1>
<hr>In [[natural language processing]], '''Brown clustering'''<ref name="liang"></ref><ref name="turian"></ref> or '''IBM clustering'''<ref name="jurafsky"></ref> is a form of [[hierarchical clustering]] of words based on the contexts in which they occur, proposed by Peter F. Brown of [[IBM]] in the context of [[language model]]ing.<ref>{{cite journal |author1=Peter F. Brown |author2=Peter V. deSouza |author3=Robert L. Mercer |author4=Vincent J. Della Pietra |author5=Jenifer C. Lai |title=Class-based {{mvar|n}}-gram models of natural languag
<p>e |journal=Computational Linguistics |volume=18 |issue=4 |year=1992 |url=<a class="uri" href="http://aclweb.org/anthology/J/J92/J92-4003.pdf">http://aclweb.org/anthology/J/J92/J92-4003.pdf</a>}} The intuition behind the method is that a <strong>class-based language model</strong> (also called <strong>cluster 

<math display="inline" id="Brown_clustering:0">
 <semantics>
  <mi>n</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>n</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   n
  </annotation>
 </semantics>
</math>

-gram model</strong><a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a>), i.e. one where probabilities of words are based on the classes (clusters) of previous words, can overcome the <a href="data_sparsity" title="wikilink">data sparsity</a> problem inherent in language modeling. Jurafsky and Martin give the example of a flight reservation system that needs to estimate the <a class="uri" href="likelihood" title="wikilink">likelihood</a> of the bigram "to Shanghai", without having seen this in a training set. The system can obtain a good estimate if it can cluster "Shanghai" with other city names, then make its estimate based on the likelihood of phrases such as "to London", "to Beijing" and "to Denver".<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>Brown clustering is an agglomerative, bottom-up form of clustering that groups words (i.e., <a href="Type–token_distinction" title="wikilink">types</a>) into a binary tree of classes, using a merging criterion based on the <a class="uri" href="log-probability" title="wikilink">log-probability</a> of a text under a class-based language model, i.e. a probability model that takes the clustering into account. This model has the same general form as a <a href="hidden_Markov_model" title="wikilink">hidden Markov model</a>.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> That is, given cluster membership indicators 

<math display="inline" id="Brown_clustering:1">
 <semantics>
  <mrow>
   <mi>c</mi>
   <mi>ᵢ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>c</ci>
    <ci>ᵢ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   cᵢ
  </annotation>
 </semantics>
</math>

 for the tokens 

<math display="inline" id="Brown_clustering:2">
 <semantics>
  <mrow>
   <mi>w</mi>
   <mi>ᵢ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>w</ci>
    <ci>ᵢ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   wᵢ
  </annotation>
 </semantics>
</math>

 in a text, the probability of the 

<math display="inline" id="Brown_clustering:3">
 <semantics>
  <mrow>
   <mi>w</mi>
   <mi>ᵢ</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>w</ci>
    <ci>ᵢ</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   wᵢ
  </annotation>
 </semantics>
</math>

 given 

<math display="inline" id="Brown_clustering:4">
 <semantics>
  <mrow>
   <mi>w</mi>
   <mi>ᵢ</mi>
   <mi mathvariant="normal">₋</mi>
   <mi mathvariant="normal">₁</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>w</ci>
    <ci>ᵢ</ci>
    <ci>normal-₋</ci>
    <ci>normal-₁</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   wᵢ₋₁
  </annotation>
 </semantics>
</math>

 is given by<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>

<p>

<math display="block" id="Brown_clustering:5">
 <semantics>
  <mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>w</mi>
     <mi>i</mi>
    </msub>
    <mo stretchy="false">|</mo>
    <msub>
     <mi>w</mi>
     <mrow>
      <mi>i</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mo>=</mo>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>w</mi>
     <mi>i</mi>
    </msub>
    <mo stretchy="false">|</mo>
    <msub>
     <mi>c</mi>
     <mi>i</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
   <mi>P</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>c</mi>
     <mi>i</mi>
    </msub>
    <mo stretchy="false">|</mo>
    <msub>
     <mi>c</mi>
     <mrow>
      <mi>i</mi>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <cerror>
    <csymbol cd="ambiguous">fragments</csymbol>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>i</ci>
     </apply>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <apply>
       <minus></minus>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <eq></eq>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>i</ci>
     </apply>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>c</ci>
      <ci>i</ci>
     </apply>
     <ci>normal-)</ci>
    </cerror>
    <csymbol cd="unknown">P</csymbol>
    <cerror>
     <csymbol cd="ambiguous">fragments</csymbol>
     <ci>normal-(</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>c</ci>
      <ci>i</ci>
     </apply>
     <ci>normal-|</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>c</ci>
      <apply>
       <minus></minus>
       <ci>i</ci>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <ci>normal-)</ci>
    </cerror>
   </cerror>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P(w_{i}|w_{i-1})=P(w_{i}|c_{i})P(c_{i}|c_{i-1})
  </annotation>
 </semantics>
</math>

</p>

<p>Finding the clustering which maximizes the likelihood of the data is computationally infeasible. The approach proposed by Brown is a <a href="greedy_algorithm" title="wikilink">greedy heuristic</a>.</p>

<p>Brown clustering generates a fixed number of output classes. It is important to choose the correct number of classes, though this is task-dependent.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> The cluster memberships of words resulting from Brown clustering can be used as features in a variety of <a href="machine_learning" title="wikilink">machine-learned</a> natural language processing tasks.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Feature_learning" title="wikilink">Feature learning</a></li>
<li><a href="http://www.derczynski.com/sheffield/brown-tuning/">How to tune Brown clustering</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Cluster_analysis" title="wikilink">Category:Cluster analysis</a> <a href="Category:Hidden_Markov_models" title="wikilink">Category:Hidden Markov models</a> <a href="Category:Language_modeling" title="wikilink">Category:Language modeling</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
</ol>
</section>
</ref></hr></body>
</html>
