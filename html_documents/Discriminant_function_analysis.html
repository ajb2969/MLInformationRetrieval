<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1736">Discriminant function analysis</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Discriminant function analysis</h1>
<hr>'''Discriminant function analysis''' is a statistical analysis to predict a [[categorical variable|cat
<p>egorical]] <a href="dependent_variable" title="wikilink">dependent</a> <a href="Variable_(mathematics)#Applied_statistics" title="wikilink">variable</a> (called a grouping variable) by one or more <a href="continuous_variable" title="wikilink">continuous</a> or <a href="Binary_variable" title="wikilink">binary</a> <a href="independent_variable" title="wikilink">independent</a> variables (called predictor variables). The original dichotomous discriminant analysis was developed by Sir <a href="Ronald_Fisher" title="wikilink">Ronald Fisher</a> in 1936.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> It is different from an <a class="uri" href="ANOVA" title="wikilink">ANOVA</a> or <a class="uri" href="MANOVA" title="wikilink">MANOVA</a>, which is used to predict one (ANOVA) or multiple (MANOVA) continuous dependent variables by one or more independent categorical variables. Discriminant function analysis is useful in determining whether a set of variables is effective in predicting category membership.<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>Discriminant analysis is used when groups are known a priori (unlike in <a href="cluster_analysis" title="wikilink">cluster analysis</a>). Each case must have a score on one or more quantitative predictor measures, and a score on a group measure.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> In simple terms, discriminant function analysis is classification - the act of distributing things into groups, classes or categories of the same type.</p>

<p>Moreover, it is a useful follow-up procedure to a MANOVA instead of doing a series of one-way ANOVAs, for ascertaining how the groups differ on the composite of dependent variables. In this case, a significant F test allows classification based on a linear combination of predictor variables. Terminology can get confusing here, as in MANOVA, the dependent variables are the predictor variables, and the independent variables are the grouping variables.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<h2 id="assumptions">Assumptions</h2>

<p>The assumptions of discriminant analysis are the same as those for MANOVA. The analysis is quite sensitive to outliers and the size of the smallest group must be larger than the number of predictor variables.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>
<ul>
<li><a href="Multivariate_normal_distribution" title="wikilink">Multivariate normality</a>: Independent variables are normal for each level of the grouping variable.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a><a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a></li>
</ul>
<ul>
<li>Homogeneity of variance/covariance (<a class="uri" href="homoscedasticity" title="wikilink">homoscedasticity</a>): Variances among group variables are the same across levels of predictors. Can be tested with Box's M statistic.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> It has been suggested, however, that <a href="linear_discriminant_analysis" title="wikilink">linear discriminant analysis</a> be used when covariances are equal, and that <a href="quadratic_classifier#Quadratic_discriminant_analysis" title="wikilink">quadratic discriminant analysis</a> may be used when covariances are not equal.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></li>
</ul>
<ul>
<li><a class="uri" href="Multicollinearity" title="wikilink">Multicollinearity</a>: Predictive power can decrease with an increased correlation between predictor variables.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></li>
</ul>
<ul>
<li><a href="statistical_independence" title="wikilink">Independence</a>: Participants are assumed to be randomly sampled, and a participant’s score on one variable is assumed to be independent of scores on that variable for all other participants.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a><a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a></li>
</ul>

<p><code>   It has been suggested that discriminant analysis is relatively robust to slight violations of these assumptions,</code><a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><code> and it has also been shown that discriminant analysis may still be reliable when using dichotomous variables (where multivariate normality is often violated).</code><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a></p>
<h2 id="discriminant-functions">Discriminant functions</h2>

<p>Discriminant analysis works by creating one or more linear combinations of predictors, creating a new <a href="latent_variable" title="wikilink">latent variable</a> for each function. These functions are called discriminant functions. The number of functions possible is either <em>Ng</em>-1 where <em>Ng</em> = number of groups, or <em>p</em> (the number of predictors), whichever is smaller. The first function created maximizes the differences between groups on that function. The second function maximizes differences on that function, but also must not be correlated with the previous function. This continues with subsequent functions with the requirement that the new function not be correlated with any of the previous functions.</p>

<p>Given group 

<math display="inline" id="Discriminant_function_analysis:0">
 <semantics>
  <mi>j</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>j</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   j
  </annotation>
 </semantics>
</math>

, with 

<math display="inline" id="Discriminant_function_analysis:1">
 <semantics>
  <msub>
   <mi>ℝ</mi>
   <mi>j</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>ℝ</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbb{R}_{j}
  </annotation>
 </semantics>
</math>

 sets of sample space, there is a discriminant rule such that if 

<math display="inline" id="Discriminant_function_analysis:2">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>∈</mo>
   <msub>
    <mi>ℝ</mi>
    <mi>j</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>x</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>ℝ</ci>
     <ci>j</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x\in\mathbb{R}_{j}
  </annotation>
 </semantics>
</math>

, then 

<math display="inline" id="Discriminant_function_analysis:3">
 <semantics>
  <mrow>
   <mi>x</mi>
   <mo>∈</mo>
   <mi>j</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>x</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x\in j
  </annotation>
 </semantics>
</math>

. Discriminant analysis then, finds “good” regions of 

<math display="inline" id="Discriminant_function_analysis:4">
 <semantics>
  <msub>
   <mi>ℝ</mi>
   <mi>j</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>ℝ</ci>
    <ci>j</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbb{R}_{j}
  </annotation>
 </semantics>
</math>

 to minimize classification error, therefore leading to a high percent correct classified in the classification table.Hardle, W., Simar, L. (2007). <em>Applied Multivariate Statistical Analysis</em>. Springer Berlin Heidelberg. pp. 289–303.</p>

<p>Each function is given a discriminant score to determine how well it predicts group placement.</p>
<ul>
<li>Structure Correlation Coefficients: The correlation between each predictor and the discriminant score of each function. This is a whole correlation.Garson, G. D. (2008). Discriminant function analysis. <a class="uri" href="http://www2.chass.ncsu.edu/garson/pa765/discrim.htm">http://www2.chass.ncsu.edu/garson/pa765/discrim.htm</a> .</li>
<li>Standardized Coefficients: Each predictor’s unique contribution to each function, therefore this is a <a href="partial_correlation" title="wikilink">partial correlation</a>. Indicates the relative importance of each predictor in predicting group assignment from each function.</li>
<li>Functions at Group Centroids: Mean discriminant scores for each grouping variable are given for each function. The farther apart the means are, the less error there will be in classification.</li>
</ul>
<h2 id="discrimination-rules">Discrimination rules</h2>
<ul>
<li><a href="Maximum_likelihood" title="wikilink">Maximum likelihood</a>: Assigns x to the group that maximizes population (group) density.<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></li>
<li>Bayes Discriminant Rule: Assigns x to the group that maximizes 

<math display="inline" id="Discriminant_function_analysis:5">
 <semantics>
  <mrow>
   <msub>
    <mi>π</mi>
    <mi>i</mi>
   </msub>
   <msub>
    <mi>f</mi>
    <mi>i</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>π</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <ci>i</ci>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \pi_{i}f_{i}(x)
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Discriminant_function_analysis:6">
 <semantics>
  <mrow>
   <msub>
    <mi>f</mi>
    <mi>i</mi>
   </msub>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>f</ci>
     <ci>i</ci>
    </apply>
    <ci>x</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f_{i}(x)
  </annotation>
 </semantics>
</math>

 represents the <a href="prior_probability" title="wikilink">prior probability</a> of that classification, and <em>π<sub>i</sub></em> represents the population density.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a></li>
<li><a href="Linear_Discriminant_Analysis" title="wikilink">Fisher’s linear discriminant rule</a>: Maximizes the ratio between <em>SS</em><sub>between</sub> and <em>SS</em><sub>within</sub>, and finds a linear combination of the predictors to predict group.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a></li>
</ul>
<h2 id="eigenvalues">Eigenvalues</h2>

<p><code>   An </code><a href="eigenvalues_and_eigenvectors" title="wikilink"><code>eigenvalue</code></a><code> in discriminant analysis is the characteristic root of each function.</code><code> It is an indication of how well that function differentiates the groups, where the larger the eigenvalue, the better the function differentiates.</code><a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a><code> This however, should be interpreted with caution, as eigenvalues have no upper limit.</code><a class="footnoteRef" href="#fn19" id="fnref19"><sup>19</sup></a><a class="footnoteRef" href="#fn20" id="fnref20"><sup>20</sup></a><br/>
<code>   The eigenvalue can be viewed as a ratio of </code><em><code>SS</code></em><sub><code>between</code></sub><code> and </code><em><code>SS</code></em><sub><code>within</code></sub><code> as in ANOVA when the dependent variable is the discriminant function, and the groups are the levels of the </code><a href="Instrumental_variable" title="wikilink"><code>IV</code></a><code>.</code><a class="footnoteRef" href="#fn21" id="fnref21"><sup>21</sup></a><code> This means that the largest eigenvalue is associated with the first function, the second largest with the second, etc..</code></p>
<h2 id="effect-size">Effect size</h2>

<p><code>   Some suggest the use of eigenvalues as </code><a href="effect_size" title="wikilink"><code>effect</code> <code>size</code></a><code> measures, however, this is generally not supported.</code><a class="footnoteRef" href="#fn22" id="fnref22"><sup>22</sup></a><code> Instead, the </code><a href="canonical_correlation" title="wikilink"><code>canonical</code> <code>correlation</code></a><code> is the preferred measure of effect size. It is similar to the eigenvalue, but is the square root of the ratio of </code><em><code>SS</code></em><sub><code>between</code></sub><code> and </code><em><code>SS</code></em><sub><code>total</code></sub><code>. It is the correlation between groups and the function.</code><a class="footnoteRef" href="#fn23" id="fnref23"><sup>23</sup></a><code> </code><br/>
<code>   Another popular measure of effect size is the percent of variance</code><code> for each function.  This is calculated by: (</code><em><code>λ</code><sub><code>x</code></sub><code>/Σλ</code><sub><code>i</code></sub></em><code>) X 100 where </code><em><code>λ</code><sub><code>x</code></sub></em><code> is the eigenvalue for the function and Σ</code><em><code>λ</code><sub><code>i</code></sub></em><code> is the sum of all eigenvalues. This tells us how strong the prediction is for that particular function compared to the others.</code><a class="footnoteRef" href="#fn24" id="fnref24"><sup>24</sup></a><code> </code><br/>
<code>   Percent correctly classified can also be analyzed as an effect size. The kappa value</code><code> can describe this while correcting for chance agreement.</code><a class="footnoteRef" href="#fn25" id="fnref25"><sup>25</sup></a></p>
<h2 id="variations">Variations</h2>
<ul>
<li><a href="Linear_Discriminant_Analysis#Multiclass_LDA" title="wikilink">Multiple discriminant analysis (MDA)</a>: related to MANOVA. Has more than two groups, and uses multiple dummy variables.<a class="footnoteRef" href="#fn26" id="fnref26"><sup>26</sup></a></li>
<li>Sequential discriminant analysis: assesses the importance of a set of IVs over and above a set of controls. In this case, the controls are entered first, and then the IVs.<a class="footnoteRef" href="#fn27" id="fnref27"><sup>27</sup></a></li>
<li>Stepwise discriminant analysis: Selects the most correlated predictor first, removes that variance in the grouping variable then adds the next most correlated and continues until the change in canonical correlation is not significant. Of course, both forward and backward stepwise procedures may be performed.<a class="footnoteRef" href="#fn28" id="fnref28"><sup>28</sup></a></li>
</ul>
<h2 id="comparison-to-logistic-regression">Comparison to logistic regression</h2>

<p><code>   Discriminant function analysis is very similar to </code><a href="logistic_regression" title="wikilink"><code>logistic</code> <code>regression</code></a><code>, and both can be used to answer the same research questions.</code><a class="footnoteRef" href="#fn29" id="fnref29"><sup>29</sup></a><code> Logistic regression does not have as many assumptions and restrictions as discriminant analysis. However, when discriminant analysis’ assumptions are met, it is more powerful than logistic regression.</code><code> Unlike logistic regression, discriminant analysis can be used with small sample sizes. It has been shown that when sample sizes are equal, and homogeneity of variance/covariance holds, discriminant analysis is more accurate.</code><a class="footnoteRef" href="#fn30" id="fnref30"><sup>30</sup></a><code> With all this being considered, logistic regression is the common choice nowadays, since the assumptions of discriminant analysis are rarely met.</code><a class="footnoteRef" href="#fn31" id="fnref31"><sup>31</sup></a><a class="footnoteRef" href="#fn32" id="fnref32"><sup>32</sup></a></p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Statistical_classification" title="wikilink">Statistical classification</a></li>
<li><a href="Linear_discriminant_analysis" title="wikilink">Linear discriminant analysis</a></li>
<li><a href="Multiple_discriminant_analysis" title="wikilink">Multiple discriminant analysis</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://www2.chass.ncsu.edu/garson/pa765/discrim.htm">Course notes, Discriminant function analysis by G. David Garson, NC State University</a></li>
<li><a href="http://people.revoledu.com/kardi/tutorial/LDA/">Discriminant analysis tutorial in Microsoft Excel by Kardi Teknomo</a></li>
<li><a href="http://www.psychstat.missouristate.edu/multibook/mlt03m.html">Course notes, Discriminant function analysis by David W. Stockburger, Missouri State University</a></li>
<li><a href="http://userwww.sfsu.edu/~efc/classes/biol710/discrim/discrim.pdf">Discriminant function analysis (DA) by John Poulsen and Aaron French, San Francisco State University</a></li>
</ul>

<p>"</p>

<p><a href="Category:Multivariate_statistics" title="wikilink">Category:Multivariate statistics</a> <a href="Category:Statistical_classification" title="wikilink">Category:Statistical classification</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Cohen et al. Applied Multiple Regression/Correlation Analysis for the Behavioural Sciences 3rd ed. (2003). Taylor &amp; Francis Group.<a href="#fnref1">↩</a></li>
<li id="fn2">Green, S.B. Salkind, N. J. &amp; Akey, T. M. (2008). Using SPSS for Windows and Macintosh: Analyzing and understanding data. New Jersey: Prentice Hall.<a href="#fnref2">↩</a></li>
<li id="fn3">BÖKEOĞLU ÇOKLUK, Ö, &amp; BÜYÜKÖZTÜRK, Ş. (2008). Discriminant function analysis: Concept and application. Eğitim araştırmaları dergisi, (33), 73-92.<a href="#fnref3">↩</a></li>
<li id="fn4"></li>
<li id="fn5"></li>
<li id="fn6"></li>
<li id="fn7"></li>
<li id="fn8"></li>
<li id="fn9"></li>
<li id="fn10"></li>
<li id="fn11"></li>
<li id="fn12"></li>
<li id="fn13"><code>Lachenbruch,</code> <code>P.</code> <code>A.</code> <code>(1975).</code> <em><code>Discriminant</code> <code>analysis</code></em><code>.</code> <code>NY:</code> <code>Hafner</code><a href="#fnref13">↩</a></li>
<li id="fn14"><code>Klecka,</code> <code>William</code> <code>R.</code> <code>(1980).</code> <em><code>Discriminant</code> <code>analysis</code></em><code>.</code> <code>Quantitative</code> <code>Applications</code> <code>in</code> <code>the</code> <code>Social</code> <code>Sciences</code> <code>Series,</code> <code>No.</code> <code>19.</code> <code>Thousand</code> <code>Oaks,</code> <code>CA:</code> <code>Sage</code> <code>Publications.</code><a href="#fnref14">↩</a></li>
<li id="fn15">Hardle, W., Simar, L. (2007). <em>Applied Multivariate Statistical Analysis</em>. Springer Berlin Heidelberg. pp. 289-303.<a href="#fnref15">↩</a></li>
<li id="fn16"></li>
<li id="fn17"></li>
<li id="fn18"></li>
<li id="fn19"></li>
<li id="fn20"></li>
<li id="fn21"></li>
<li id="fn22"></li>
<li id="fn23"></li>
<li id="fn24"></li>
<li id="fn25"></li>
<li id="fn26">Garson, G. D. (2008). Discriminant function analysis. <a class="uri" href="http://www2.chass.ncsu.edu/garson/pa765/discrim.htm">http://www2.chass.ncsu.edu/garson/pa765/discrim.htm</a> .<a href="#fnref26">↩</a></li>
<li id="fn27"></li>
<li id="fn28"></li>
<li id="fn29"></li>
<li id="fn30"></li>
<li id="fn31"></li>
<li id="fn32"></li>
</ol>
</section>
</hr></body>
</html>
