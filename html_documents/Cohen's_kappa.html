<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1268">Cohen's kappa</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Cohen's kappa</h1>
<hr/>

<p><strong>Cohen's kappa coefficient</strong> is a <a class="uri" href="statistic" title="wikilink">statistic</a> which measures <a href="inter-rater_agreement" title="wikilink">inter-rater agreement</a> for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, since κ takes into account the agreement occurring by chance.</p>
<h2 id="calculation">Calculation</h2>

<p>Cohen's kappa measures the agreement between two raters who each classify <em>N</em> items into <em>C</em> mutually exclusive categories. The first mention of a kappa-like statistic is attributed to Galton (1892),<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> see Smeeton (1985).<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a></p>

<p>The equation for κ is:</p>

<p>

<math display="block" id="Cohen's_kappa:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>κ</mi>
    <mo>=</mo>
    <mfrac>
     <mrow>
      <msub>
       <mi>p</mi>
       <mi>o</mi>
      </msub>
      <mo>-</mo>
      <msub>
       <mi>p</mi>
       <mi>e</mi>
      </msub>
     </mrow>
     <mrow>
      <mn>1</mn>
      <mo>-</mo>
      <msub>
       <mi>p</mi>
       <mi>e</mi>
      </msub>
     </mrow>
    </mfrac>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>κ</ci>
    <apply>
     <divide></divide>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>p</ci>
       <ci>o</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>p</ci>
       <ci>e</ci>
      </apply>
     </apply>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>p</ci>
       <ci>e</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \kappa=\frac{p_{o}-p_{e}}{1-p_{e}},\!
  </annotation>
 </semantics>
</math>

</p>

<p>where <mtpl></mtpl> is the relative observed agreement among raters, and <mtpl></mtpl> is the hypothetical probability of chance agreement, using the observed data to calculate the probabilities of each observer randomly saying each category. If the raters are in complete agreement then 

<math display="inline" id="Cohen's_kappa:1">
 <semantics>
  <mrow>
   <mi>κ</mi>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>κ</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   κ=1
  </annotation>
 </semantics>
</math>

. If there is no agreement among the raters other than what would be expected by chance (as given by <mtpl></mtpl>), 

<math display="inline" id="Cohen's_kappa:2">
 <semantics>
  <mrow>
   <mi>κ</mi>
   <mi mathvariant="normal">≤</mi>
   <mn>0</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>κ</ci>
    <ci>normal-≤</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   κ≤0
  </annotation>
 </semantics>
</math>

.</p>

<p>The seminal paper introducing kappa as a new technique was published by <a href="Jacob_Cohen_(statistician)" title="wikilink">Jacob Cohen</a> in the journal <em>Educational and Psychological Measurement</em> in 1960.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a></p>

<p>A similar statistic, called <a href="Scott's_Pi" title="wikilink">pi</a>, was proposed by Scott (1955). Cohen's kappa and <a href="Scott's_Pi" title="wikilink">Scott's pi</a> differ in terms of how <mtpl></mtpl> is calculated.</p>

<p>Note that Cohen's kappa measures agreement between <strong>two</strong> raters only. For a similar measure of agreement (<a href="Fleiss'_kappa" title="wikilink">Fleiss' kappa</a>) used when there are more than two raters, see <a href="Joseph_L._Fleiss" title="wikilink">Fleiss</a> (1971). The Fleiss kappa, however, is a multi-rater generalization of <a href="Scott's_Pi" title="wikilink">Scott's pi</a> statistic, not Cohen's kappa. Kappa is also used to compare performance in <a href="Machine_Learning" title="wikilink">Machine Learning</a> but the directional version known as Informedness or <a href="Youden's_J_statistic" title="wikilink">Youden's J statistic</a> is argued to be more appropriate for supervised learning.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<h2 id="example">Example</h2>

<p>Suppose that you were analyzing data related to a group of 50 people applying for a grant. Each grant proposal was read by two readers and each reader either said "Yes" or "No" to the proposal. Suppose the dis/agreement count data were as follows, where A and B are readers, data on the diagonal slanting left shows the count of agreements and the data on the diagonal slanting right, disagreements:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">
<p>B</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p>Yes</p></td>
<td style="text-align: left;">
<p>No</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>A</p></td>
<td style="text-align: left;">
<p>Yes</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>No</p></td>
<td style="text-align: left;">
<p>10</p></td>
</tr>
</tbody>
</table>

<p>Note that there were 20 proposals that were granted by both reader A and reader B, and 15 proposals that were rejected by both readers. Thus, the observed proportionate agreement is <mtpl> (20 + 15) / 50 {{=}} 0.70}}</mtpl></p>

<p>To calculate <mtpl></mtpl> (the probability of random agreement) we note that:</p>
<ul>
<li>Reader A said "Yes" to 25 applicants and "No" to 25 applicants. Thus reader A said "Yes" 50% of the time.</li>
<li>Reader B said "Yes" to 30 applicants and "No" to 20 applicants. Thus reader B said "Yes" 60% of the time.</li>
</ul>

<p>Therefore the probability that both of them would say "Yes" randomly is  and the probability that both of them would say "No" is  Thus the overall probability of random agreement is </p>

<p>So now applying our formula for Cohen's Kappa we get:</p>

<p>

<math display="block" id="Cohen's_kappa:3">
 <semantics>
  <mrow>
   <mi>κ</mi>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <msub>
      <mi>p</mi>
      <mi>o</mi>
     </msub>
     <mo>-</mo>
     <msub>
      <mi>p</mi>
      <mi>e</mi>
     </msub>
    </mrow>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <msub>
      <mi>p</mi>
      <mi>e</mi>
     </msub>
    </mrow>
   </mfrac>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mn>0.70</mn>
     <mo>-</mo>
     <mn>0.50</mn>
    </mrow>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mn>0.50</mn>
    </mrow>
   </mfrac>
   <mo>=</mo>
   <mpadded width="-1.7pt">
    <mn>0.40</mn>
   </mpadded>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>κ</ci>
     <apply>
      <divide></divide>
      <apply>
       <minus></minus>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>p</ci>
        <ci>o</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>p</ci>
        <ci>e</ci>
       </apply>
      </apply>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>p</ci>
        <ci>e</ci>
       </apply>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <apply>
      <divide></divide>
      <apply>
       <minus></minus>
       <cn type="float">0.70</cn>
       <cn type="float">0.50</cn>
      </apply>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <cn type="float">0.50</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <cn type="float">0.40</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \kappa=\frac{p_{o}-p_{e}}{1-p_{e}}=\frac{0.70-0.50}{1-0.50}=0.40\!
  </annotation>
 </semantics>
</math>

</p>
<h2 id="same-percentages-but-different-numbers">Same percentages but different numbers</h2>

<p>A case sometimes considered to be a problem with Cohen's Kappa occurs when comparing the Kappa calculated for two pairs of raters with the two raters in each pair having the same percentage agreement but one pair give a similar number of ratings while the other pair give a very different number of ratings.<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> For instance, in the following two cases there is equal agreement between A and B (60 out of 100 in both cases) so we would expect the relative values of Cohen's Kappa to reflect this. However, calculating Cohen's Kappa for each:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">
<p>B</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p>Yes</p></td>
<td style="text-align: left;">
<p>No</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>A</p></td>
<td style="text-align: left;">
<p>Yes</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>No</p></td>
<td style="text-align: left;">
<p>25</p></td>
</tr>
</tbody>
</table>

<p>

<math display="block" id="Cohen's_kappa:4">
 <semantics>
  <mrow>
   <mi>κ</mi>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mn>0.60</mn>
     <mo>-</mo>
     <mn>0.54</mn>
    </mrow>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mn>0.54</mn>
    </mrow>
   </mfrac>
   <mo>=</mo>
   <mn>0.1304</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>κ</ci>
     <apply>
      <divide></divide>
      <apply>
       <minus></minus>
       <cn type="float">0.60</cn>
       <cn type="float">0.54</cn>
      </apply>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <cn type="float">0.54</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <cn type="float">0.1304</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \kappa=\frac{0.60-0.54}{1-0.54}=0.1304
  </annotation>
 </semantics>
</math>

</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">
<p>B</p></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">
<p>Yes</p></td>
<td style="text-align: left;">
<p>No</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">
<p>A</p></td>
<td style="text-align: left;">
<p>Yes</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;">
<p>No</p></td>
<td style="text-align: left;">
<p>5</p></td>
</tr>
</tbody>
</table>

<p>

<math display="block" id="Cohen's_kappa:5">
 <semantics>
  <mrow>
   <mi>κ</mi>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mn>0.60</mn>
     <mo>-</mo>
     <mn>0.46</mn>
    </mrow>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <mn>0.46</mn>
    </mrow>
   </mfrac>
   <mo>=</mo>
   <mn>0.2593</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <ci>κ</ci>
     <apply>
      <divide></divide>
      <apply>
       <minus></minus>
       <cn type="float">0.60</cn>
       <cn type="float">0.46</cn>
      </apply>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <cn type="float">0.46</cn>
      </apply>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <share href="#.cmml">
     </share>
     <cn type="float">0.2593</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \kappa=\frac{0.60-0.46}{1-0.46}=0.2593
  </annotation>
 </semantics>
</math>

</p>

<p>we find that it shows greater similarity between A and B in the second case, compared to the first. This is because while the percentage agreement is the same, the percentage agreement that would occur 'by chance' is significantly higher in the first case (0.54 compared to 0.46).</p>
<h2 id="significance-and-magnitude">Significance and magnitude</h2>
<figure><b>(Figure)</b>
<figcaption>Kappa (vertical axis) and <a href="Accuracy_and_precision#In_binary_classification" title="wikilink">Accuracy</a> (horizontal axis) calculated from the same simulated binary data. Each point on the graph is calculated from a pairs of judges randomly rating 10 subjects for having a diagnosis of X or not. Note in this example a Kappa=0 is approximately equivalent to an accuracy=0.5</figcaption>
</figure>

<p><em><a href="Statistical_significance" title="wikilink">Statistical significance</a></em> makes no claim on how important is the magnitude in a given application or what is considered as high or low agreement.</p>

<p>Statistical significance for kappa is rarely reported, probably because even relatively low values of kappa can nonetheless be significantly different from zero but not of sufficient magnitude to satisfy investigators.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> Still, its standard error has been described<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> and is computed by various computer programs.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>

<p>If statistical significance is not a useful guide, what magnitude of kappa reflects adequate agreement? Guidelines would be helpful, but factors other than agreement can influence its magnitude, which makes interpretation of a given magnitude problematic. As Sim and Wright noted, two important factors are prevalence (are the codes equiprobable or do their probabilities vary) and bias (are the marginal probabilities for the two observers similar or different). Other things being equal, kappas are higher when codes are equiprobable. On the other hand Kappas are higher when codes are distributed asymmetrically by the two observers. In contrast to probability variations, the effect of bias is greater when Kappa is small than when it is large.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>

<p>Another factor is the number of codes. As number of codes increases, kappas become higher. Based on a simulation study, Bakeman and colleagues concluded that for fallible observers, values for kappa were lower when codes were fewer. And, in agreement with Sim &amp; Wrights's statement concerning prevalence, kappas were higher when codes were roughly equiprobable. Thus Bakeman et al. concluded that "no one value of kappa can be regarded as universally acceptable."<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> They also provide a computer program that lets users compute values for kappa specifying number of codes, their probability, and observer accuracy. For example, given equiprobable codes and observers who are 85% accurate, value of kappa are 0.49, 0.60, 0.66, and 0.69 when number of codes is 2, 3, 5, and 10, respectively.</p>

<p>Nonetheless, magnitude guidelines have appeared in the literature. Perhaps the first was Landis and Koch,<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> who characterized values Gwet, K. (2010). "<a href="http://www.agreestat.com/">Handbook of Inter-Rater Reliability (Second Edition)</a>" ISBN 978-0-9708062-2-2  Fleiss's<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> equally arbitrary guidelines characterize kappas over 0.75 as excellent, 0.40 to 0.75 as fair to good, and below 0.40 as poor.</p>
<h2 id="weighted-kappa">Weighted kappa</h2>

<p>Weighted kappa lets you count disagreements differently<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a> and is especially useful when codes are ordered.<a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> Three matrices are involved, the matrix of observed scores, the matrix of expected scores based on chance agreement, and the weight matrix. Weight matrix cells located on the diagonal (upper-left to bottom-right) represent agreement and thus contain zeros. Off-diagonal cells contain weights indicating the seriousness of that disagreement. Often, cells one off the diagonal are weighted 1, those two off 2, etc.</p>

<p>The equation for weighted κ is:</p>

<p>

<math display="block" id="Cohen's_kappa:6">
 <semantics>
  <mrow>
   <mi>κ</mi>
   <mo>=</mo>
   <mrow>
    <mn>1</mn>
    <mo>-</mo>
    <mfrac>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mn>1</mn>
       <mo>-</mo>
       <mrow>
        <msubsup>
         <mo largeop="true" symmetric="true">∑</mo>
         <mrow>
          <mi>i</mi>
          <mo>=</mo>
          <mn>1</mn>
         </mrow>
         <mi>k</mi>
        </msubsup>
        <mrow>
         <msubsup>
          <mo largeop="true" symmetric="true">∑</mo>
          <mrow>
           <mi>j</mi>
           <mo>=</mo>
           <mn>1</mn>
          </mrow>
          <mi>k</mi>
         </msubsup>
         <mrow>
          <msub>
           <mi>w</mi>
           <mrow>
            <mi>i</mi>
            <mi>j</mi>
           </mrow>
          </msub>
          <msub>
           <mi>x</mi>
           <mrow>
            <mi>i</mi>
            <mi>j</mi>
           </mrow>
          </msub>
         </mrow>
        </mrow>
       </mrow>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <mn>1</mn>
       <mo>-</mo>
       <mrow>
        <msubsup>
         <mo largeop="true" symmetric="true">∑</mo>
         <mrow>
          <mi>i</mi>
          <mo>=</mo>
          <mn>1</mn>
         </mrow>
         <mi>k</mi>
        </msubsup>
        <mrow>
         <msubsup>
          <mo largeop="true" symmetric="true">∑</mo>
          <mrow>
           <mi>j</mi>
           <mo>=</mo>
           <mn>1</mn>
          </mrow>
          <mi>k</mi>
         </msubsup>
         <mrow>
          <msub>
           <mi>w</mi>
           <mrow>
            <mi>i</mi>
            <mi>j</mi>
           </mrow>
          </msub>
          <msub>
           <mi>m</mi>
           <mrow>
            <mi>i</mi>
            <mi>j</mi>
           </mrow>
          </msub>
         </mrow>
        </mrow>
       </mrow>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mfrac>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>κ</ci>
    <apply>
     <minus></minus>
     <cn type="integer">1</cn>
     <apply>
      <divide></divide>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <sum></sum>
          <apply>
           <eq></eq>
           <ci>i</ci>
           <cn type="integer">1</cn>
          </apply>
         </apply>
         <ci>k</ci>
        </apply>
        <apply>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <sum></sum>
           <apply>
            <eq></eq>
            <ci>j</ci>
            <cn type="integer">1</cn>
           </apply>
          </apply>
          <ci>k</ci>
         </apply>
         <apply>
          <times></times>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>w</ci>
           <apply>
            <times></times>
            <ci>i</ci>
            <ci>j</ci>
           </apply>
          </apply>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>x</ci>
           <apply>
            <times></times>
            <ci>i</ci>
            <ci>j</ci>
           </apply>
          </apply>
         </apply>
        </apply>
       </apply>
      </apply>
      <apply>
       <minus></minus>
       <cn type="integer">1</cn>
       <apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <sum></sum>
          <apply>
           <eq></eq>
           <ci>i</ci>
           <cn type="integer">1</cn>
          </apply>
         </apply>
         <ci>k</ci>
        </apply>
        <apply>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <sum></sum>
           <apply>
            <eq></eq>
            <ci>j</ci>
            <cn type="integer">1</cn>
           </apply>
          </apply>
          <ci>k</ci>
         </apply>
         <apply>
          <times></times>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>w</ci>
           <apply>
            <times></times>
            <ci>i</ci>
            <ci>j</ci>
           </apply>
          </apply>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>m</ci>
           <apply>
            <times></times>
            <ci>i</ci>
            <ci>j</ci>
           </apply>
          </apply>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \kappa=1-\frac{(1-\sum_{i=1}^{k}\sum_{j=1}^{k}w_{ij}x_{ij})}{(1-\sum_{i=1}^{k}%
\sum_{j=1}^{k}w_{ij}m_{ij})}
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>k</em>=number of codes and 

<math display="inline" id="Cohen's_kappa:7">
 <semantics>
  <msub>
   <mi>w</mi>
   <mrow>
    <mi>i</mi>
    <mi>j</mi>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <apply>
     <times></times>
     <ci>i</ci>
     <ci>j</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{ij}
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Cohen's_kappa:8">
 <semantics>
  <msub>
   <mi>x</mi>
   <mrow>
    <mi>i</mi>
    <mi>j</mi>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <apply>
     <times></times>
     <ci>i</ci>
     <ci>j</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{ij}
  </annotation>
 </semantics>
</math>

, and 

<math display="inline" id="Cohen's_kappa:9">
 <semantics>
  <msub>
   <mi>m</mi>
   <mrow>
    <mi>i</mi>
    <mi>j</mi>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>m</ci>
    <apply>
     <times></times>
     <ci>i</ci>
     <ci>j</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m_{ij}
  </annotation>
 </semantics>
</math>

 are elements in the weight, observed, and expected matrices, respectively. When diagonal cells contain weights of 0 and all off-diagonal cells weights of 1, this formula produces the same value of kappa as the calculation given above.</p>
<h2 id="kappa-maximum">Kappa maximum</h2>

<p>Kappa assumes its theoretical maximum value of 1 only when both observers distribute codes the same, that is, when corresponding row and column sums are identical. Anything less is less than perfect agreement. Still, the maximum value kappa could achieve given unequal distributions helps interpret the value of kappa actually obtained. The equation for κ maximum is:<a class="footnoteRef" href="#fn15" id="fnref15"><sup>15</sup></a></p>

<p>

<math display="block" id="Cohen's_kappa:10">
 <semantics>
  <mrow>
   <msub>
    <mi>κ</mi>
    <mi>max</mi>
   </msub>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <msub>
      <mi>P</mi>
      <mi>max</mi>
     </msub>
     <mo>-</mo>
     <msub>
      <mi>P</mi>
      <mi>exp</mi>
     </msub>
    </mrow>
    <mrow>
     <mn>1</mn>
     <mo>-</mo>
     <msub>
      <mi>P</mi>
      <mi>exp</mi>
     </msub>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>κ</ci>
     <max></max>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <minus></minus>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>P</ci>
       <max></max>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>P</ci>
       <exp></exp>
      </apply>
     </apply>
     <apply>
      <minus></minus>
      <cn type="integer">1</cn>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>P</ci>
       <exp></exp>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \kappa_{\max}=\frac{P_{\max}-P_{\exp}}{1-P_{\exp}}
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Cohen's_kappa:11">
 <semantics>
  <mrow>
   <msub>
    <mi>P</mi>
    <mi>exp</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>k</mi>
    </msubsup>
    <mrow>
     <msub>
      <mi>P</mi>
      <mrow>
       <mi>i</mi>
       <mo>+</mo>
      </mrow>
     </msub>
     <msub>
      <mi>P</mi>
      <mrow>
       <mo>+</mo>
       <mi>i</mi>
      </mrow>
     </msub>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>P</ci>
     <exp></exp>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>k</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>P</ci>
       <apply>
        <csymbol cd="latexml">limit-from</csymbol>
        <ci>i</ci>
        <plus></plus>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>P</ci>
       <apply>
        <plus></plus>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P_{\exp}=\sum_{i=1}^{k}P_{i+}P_{+i}
  </annotation>
 </semantics>
</math>

, as usual, 

<math display="inline" id="Cohen's_kappa:12">
 <semantics>
  <mrow>
   <msub>
    <mi>P</mi>
    <mi>max</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>k</mi>
    </msubsup>
    <mrow>
     <mi>min</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>P</mi>
       <mrow>
        <mi>i</mi>
        <mo>+</mo>
       </mrow>
      </msub>
      <mo>,</mo>
      <msub>
       <mi>P</mi>
       <mrow>
        <mo>+</mo>
        <mi>i</mi>
       </mrow>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>P</ci>
     <max></max>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>k</ci>
     </apply>
     <apply>
      <min></min>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>P</ci>
       <apply>
        <csymbol cd="latexml">limit-from</csymbol>
        <ci>i</ci>
        <plus></plus>
       </apply>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>P</ci>
       <apply>
        <plus></plus>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P_{\max}=\sum_{i=1}^{k}\min(P_{i+},P_{+i})
  </annotation>
 </semantics>
</math>

,</p>

<p><em>k</em> = number of codes, 

<math display="inline" id="Cohen's_kappa:13">
 <semantics>
  <msub>
   <mi>P</mi>
   <mrow>
    <mi>i</mi>
    <mo>+</mo>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>P</ci>
    <apply>
     <csymbol cd="latexml">limit-from</csymbol>
     <ci>i</ci>
     <plus></plus>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P_{i+}
  </annotation>
 </semantics>
</math>

 are the row probabilities, and 

<math display="inline" id="Cohen's_kappa:14">
 <semantics>
  <msub>
   <mi>P</mi>
   <mrow>
    <mo>+</mo>
    <mi>i</mi>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>P</ci>
    <apply>
     <plus></plus>
     <ci>i</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   P_{+i}
  </annotation>
 </semantics>
</math>

 are the column probabilities.</p>
<h2 id="limitations">Limitations</h2>

<p>Some researchers have expressed concern over κ's tendency to take the observed categories' frequencies as givens, which can make it unreliable for measuring agreement in situations such as the diagnosis of rare diseases. In these situations, κ tends to underestimate the agreement on the rare category.<a class="footnoteRef" href="#fn16" id="fnref16"><sup>16</sup></a> For this reason, κ is considered an overly conservative measure of agreement.<a class="footnoteRef" href="#fn17" id="fnref17"><sup>17</sup></a> Others<a class="footnoteRef" href="#fn18" id="fnref18"><sup>18</sup></a> contest the assertion that kappa "takes into account" chance agreement. To do this effectively would require an explicit model of how chance affects rater decisions. The so-called chance adjustment of kappa statistics supposes that, when not completely certain, raters simply guess—a very unrealistic scenario.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Intraclass_correlation" title="wikilink">Intraclass correlation</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="further-reading">Further reading</h2>
<ul>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li>Fleiss, J. L. (1981) <em>Statistical methods for rates and proportions</em>. 2nd ed. (New York: John Wiley) pp. 38–46</li>
<li></li>
<li>Gwet, Kilem L. (2014) <em><a href="http://www.agreestat.com/book4/">Handbook of Inter-Rater Reliability, Fourth Edition</a></em>, (Gaithersburg : Advanced Analytics, LLC) ISBN 978-0970806284</li>
<li></li>
<li></li>
<li>Gwet, K. (2008). "<a href="http://www.agreestat.com/research_papers/wiley_encyclopedia2008_eoct631.pdf">Intrarater Reliability</a>." <em>Wiley Encyclopedia of Clinical Trials, Copyright 2008 John Wiley &amp; Sons, Inc.</em></li>
<li></li>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://dl.dropbox.com/u/27743223/201209-eacl2012-Kappa.pdf">The Problem with Kappa</a></li>
<li><a href="http://www.agreestat.com/research_papers.html">Kappa, its meaning, problems, and several alternatives</a></li>
<li><a href="http://www.john-uebersax.com/stat/kappa.htm#procon">Kappa Statistics: Pros and Cons</a></li>
<li><a href="http://www.gsu.edu/~psyrab/ComKappa2.zip">Windows program for kappa, weighted kappa, and kappa maximum</a></li>
<li><a href="http://akcora.wordpress.com/2011/05/30/weighted-kappa-example-in-php/">Java and PHP implementation of weighted Kappa</a></li>
</ul>
<h3 id="online-calculators">Online calculators</h3>
<ul>
<li><a href="http://www.glue.umd.edu/~dchoy/thesis/Kappa/">Cohen's Kappa for Maps</a></li>
<li><a href="http://justus.randolph.name/kappa">Online (Multirater) Kappa Calculator</a></li>
<li><a href="https://mlnl.net/jg/software/ira/">Online Kappa Calculator (multiple raters and variables)</a></li>
</ul>

<p>"</p>

<p><a href="Category:Categorical_data" title="wikilink">Category:Categorical data</a> <a href="Category:Non-parametric_statistics" title="wikilink">Category:Non-parametric statistics</a> <a href="Category:Inter-rater_reliability" title="wikilink">Category:Inter-rater reliability</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Galton, F. (1892). <em>Finger Prints</em> Macmillan, London.<a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3">Cohen, Jacob (1960). "A coefficient of agreement for nominal scales". Educational and Psychological Measurement 20 (1): 37–46. <a class="uri" href="doi:10.1177/001316446002000104">doi:10.1177/001316446002000104</a><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"></li>
<li id="fn7"><a href="#fnref7">↩</a></li>
<li id="fn8"><a href="#fnref8">↩</a></li>
<li id="fn9"><a href="#fnref9">↩</a></li>
<li id="fn10"><a href="#fnref10">↩</a></li>
<li id="fn11"><a href="#fnref11">↩</a></li>
<li id="fn12"><a href="#fnref12">↩</a></li>
<li id="fn13"><a href="#fnref13">↩</a></li>
<li id="fn14"><a href="#fnref14">↩</a></li>
<li id="fn15"><a href="#fnref15">↩</a></li>
<li id="fn16"><a href="#fnref16">↩</a></li>
<li id="fn17"><a href="#fnref17">↩</a></li>
<li id="fn18"><a href="#fnref18">↩</a></li>
</ol>
</section>
</body>
</html>
