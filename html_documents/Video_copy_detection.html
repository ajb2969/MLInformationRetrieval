<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="544">Video copy detection</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Video copy detection</h1>
<hr/>

<p><strong>Video copy detection</strong> is the process of detecting <a href="Copyright_infringement" title="wikilink">illegally copied videos</a> by analyzing them and comparing them to original content.</p>

<p>The goal of this process is to protect a video creator's <a href="intellectual_property" title="wikilink">intellectual property</a>.</p>
<h2 id="history">History</h2>

<p>Indyk et al. produced a video copy detection theory based on the length of the film; however, it worked only for whole films without modifications. When applied to short clips of a video, Idynk et al.'s technique does not detect that the clip is a copy.</p>

<p>Later, Oostveen et al. introduced the concept of a <a href="Fingerprint_(computing)" title="wikilink"><em>fingerprint</em></a>, or <em><a href="hash_function" title="wikilink">hash function</a></em>, that creates a unique signature of the video based on its contents. This fingerprint is based on the length of the video and the brightness, as determined by splitting it into a grid. The fingerprint cannot be used to recreate the original video because it describes only certain features of its respective video.</p>

<p>Some time ago, B.Coskun et al. presented two robust algorithms based on <a href="discrete_cosine_transform" title="wikilink">discrete cosine transform</a>.</p>

<p>Hampapur and Balle created an algorithm creating a global description of a piece of video based on the video's motion, color, space, and length.</p>

<p>To look at the color levels of the image was thought, and for this reason, Li et al. created an algorithm that examines the colors of a clip by creating a binary signature get from the histogram of every frame. This algorithm, however, returns inconsistent results in cases in which a <a class="uri" href="logo" title="wikilink">logo</a> is added to the video, because the insertion of the logo's color elements adds false information that can confuse the system.</p>
<h2 id="techniques">Techniques</h2>
<figure><b>(Figure)</b>
<figcaption>Watermarked image</figcaption>
</figure>
<h3 id="watermarks">Watermarks</h3>

<p><a href="Watermark" title="wikilink">Watermarks</a> are used to introduce an invisible signal into a video to ease the detection of illegal copies. This technique is widely used by <a href="Photography" title="wikilink">photographers</a>. Placing a watermark on a video such that it is easily seen by an audience allows the content creator to detect easily whether the image has been copied.</p>

<p>The limitation of watermarks is that if the original image is not watermarked, then it is not possible to know whether other images are copies.</p>
<h3 id="content-based-signature">Content-based signature</h3>
<figure><b>(Figure)</b>
<figcaption>Video copy detection.</figcaption>
</figure>

<p>In this technique, a unique signature is created for the video on the basis of the video's content. Various video copy detection <a class="uri" href="algorithms" title="wikilink">algorithms</a> exist that use features of the video's content to assign the video a unique <a class="uri" href="fingerprint" title="wikilink">fingerprint</a>. The fingerprint can be compared with other videos' fingerprints stored in a <a class="uri" href="database" title="wikilink">database</a>.</p>

<p>This type of algorithm has a significant problem: if various aspects of the videos' contents are similar, it is difficult for an algorithm to determine whether the video in question is a copy of the original or merely similar to it. In such a case (e.g., two distinct <a href="news_broadcast" title="wikilink">news broadcasts</a>), the algorithm can return that the video in question is a copy.</p>
<h2 id="algorithms">Algorithms</h2>

<p>The following are some algorithms and techniques proposed for video copy detection.</p>
<h3 id="global-descriptors">Global Descriptors</h3>
<h4 id="global-temporal-descriptor">Global temporal descriptor</h4>

<p>In this algorithm, a <em>global intensity</em> is defined as the sum of all intensities of all pixels weighted along all the video. Thus, an identity for a video sample can be constructed on the basis of the length of the video and the pixel intensities throughout.</p>

<p>The global intensity <em>a(t)</em> is defined as:</p>

<p>

<math display="inline" id="Video_copy_detection:0">
 <semantics>
  <mrow>
   <mrow>
    <mi>a</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>t</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <msubsup>
     <mo largeop="true" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>N</mi>
    </msubsup>
    <mrow>
     <mi>K</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <msup>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <mi>I</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>i</mi>
         <mo>,</mo>
         <mrow>
          <mi>t</mi>
          <mo>-</mo>
          <mn>1</mn>
         </mrow>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mn>2</mn>
     </msup>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>a</ci>
     <ci>t</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>N</ci>
     </apply>
     <apply>
      <times></times>
      <ci>K</ci>
      <ci>i</ci>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <times></times>
        <ci>I</ci>
        <interval closure="open">
         <ci>i</ci>
         <apply>
          <minus></minus>
          <ci>t</ci>
          <cn type="integer">1</cn>
         </apply>
        </interval>
       </apply>
       <cn type="integer">2</cn>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   a(t)=\sum_{i=1}^{N}K(i)(I(i,t-1))^{2}
  </annotation>
 </semantics>
</math>

</p>

<p>Where <em>k</em> is the weighting of the image, <em>I</em> is the image, and <em>N</em> is the number of pixels in the image.</p>
<h4 id="global-ordinal-measurement-descriptor">Global ordinal measurement descriptor</h4>

<p>In this algorithm, the video is divided in <em>N</em> blocks, sorted by <a href="gray_level" title="wikilink">gray level</a>. Then it's possible to create a <a href="Vector_(mathematics_and_physics)" title="wikilink">vector</a> describing the average gray level of each block.</p>

<p>With these average levels it is possible to create a new vector <em>S(t)</em>, the video's signature:</p>

<p>

<math display="inline" id="Video_copy_detection:1">
 <semantics>
  <mrow>
   <mrow>
    <mi>S</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>t</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <msub>
     <mi>r</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <msub>
     <mi>r</mi>
     <mn>2</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">⋯</mi>
    <mo>,</mo>
    <msub>
     <mi>r</mi>
     <mi>N</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>S</ci>
     <ci>t</ci>
    </apply>
    <vector>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>r</ci>
      <cn type="integer">1</cn>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>r</ci>
      <cn type="integer">2</cn>
     </apply>
     <ci>normal-⋯</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>r</ci>
      <ci>N</ci>
     </apply>
    </vector>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S(t)=(r_{1},r_{2},\cdots,r_{N})
  </annotation>
 </semantics>
</math>

</p>

<p>To compare two videos, the algorithm defines a <em>D(t)</em> representing the similarity between both.</p>

<p>

<math display="inline" id="Video_copy_detection:2">
 <semantics>
  <mrow>
   <mrow>
    <mi>D</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>t</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mi>T</mi>
    </mfrac>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mn>1</mn>
       <mo>=</mo>
       <mrow>
        <mi>t</mi>
        <mo>-</mo>
        <mfrac>
         <mi>T</mi>
         <mn>2</mn>
        </mfrac>
       </mrow>
      </mrow>
      <mrow>
       <mi>t</mi>
       <mo>+</mo>
       <mfrac>
        <mi>T</mi>
        <mn>2</mn>
       </mfrac>
      </mrow>
     </msubsup>
     <mrow>
      <mo>|</mo>
      <mtable>
       <mtr>
        <mtd columnalign="center">
         <mrow>
          <mrow>
           <mi>R</mi>
           <mrow>
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
           </mrow>
          </mrow>
          <mo>-</mo>
          <mrow>
           <mi>C</mi>
           <mrow>
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
           </mrow>
          </mrow>
         </mrow>
        </mtd>
       </mtr>
      </mtable>
      <mo>|</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>D</ci>
     <ci>t</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <ci>T</ci>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <cn type="integer">1</cn>
         <apply>
          <minus></minus>
          <ci>t</ci>
          <apply>
           <divide></divide>
           <ci>T</ci>
           <cn type="integer">2</cn>
          </apply>
         </apply>
        </apply>
       </apply>
       <apply>
        <plus></plus>
        <ci>t</ci>
        <apply>
         <divide></divide>
         <ci>T</ci>
         <cn type="integer">2</cn>
        </apply>
       </apply>
      </apply>
      <apply>
       <determinant></determinant>
       <matrix>
        <matrixrow>
         <apply>
          <minus></minus>
          <apply>
           <times></times>
           <ci>R</ci>
           <ci>i</ci>
          </apply>
          <apply>
           <times></times>
           <ci>C</ci>
           <ci>i</ci>
          </apply>
         </apply>
        </matrixrow>
       </matrix>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   D(t)=\frac{1}{T}\sum_{1=t-\frac{T}{2}}^{t+\frac{T}{2}}\begin{vmatrix}R(i)-C(i)%
\end{vmatrix}
  </annotation>
 </semantics>
</math>

</p>

<p>The value returned by <em>D(t)</em> helps determine whether the video in question is a copy.</p>
<h4 id="ordinal-and-temporal-descriptors">Ordinal and Temporal Descriptors</h4>

<p>This technique was proposed by L.Chen and F. Stentiford. A measurement of dissimilarity is made by combining the two aforementioned algorithms, <a href="#Global_temporal_descriptor" title="wikilink">Global temporal descriptors</a> and <a href="#Global_ordinal_measurement_descriptor" title="wikilink">Global ordinal measurement descriptors</a>, in <a href="Space-time" title="wikilink">time and space</a>.</p>
<h3 id="local-descriptors">Local Descriptors</h3>
<h4 id="aj">AJ</h4>

<p>Described by A. Joly et al., this algorithm is an improvement of Harris' Interest Points detector. This technique suggests that in many videos a significant number of frames are almost identical, so it is more efficient to test not every frame but just those depicting a significant amount of motion.</p>
<h4 id="vicopt">ViCopT</h4>

<p><em>ViCopT</em> uses the interest points from each image to define a signature of the whole video. In every image, the algorithms identifies and defines two parts: the <em>background</em>, a set of static elements along a temporal sequence, and the <em>motion</em>, persistent points changing positions throughout the video.</p>
<h4 id="space-time-interest-points-stip">Space Time Interest Points (STIP)</h4>

<p>This algorithm was developed by I. Laptev and T.Lindeberg. It uses the interest points technique along the space and time to define the video signature, and creates a 34th-<a href="Dimension_(mathematics)" title="wikilink">dimension</a> vector that stores this signature.</p>
<h3 id="algorithm-showcasing">Algorithm showcasing</h3>

<p>There exist algorithms for video copy detection that are in use today. In 2007, there was an evaluation showcase known as the <a href="Multimedia_Understanding_Through_Semantics,_Computation_and_Learning" title="wikilink">Multimedia Understanding Through Semantics, Computation and Learning (MUSLE)</a>, which tested video copy detection algorithms on various video samples ranging from home video recordings to TV show segments ranging from one minute to one hour in length.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="http://www-rocq.inria.fr/imedia/civr-bench/">MUSCLE (Multimedia Understanding through Semantics, Computation and Learning)</a> </li>
<li><a href="http://www.research.ibm.com/ecvg/dtv/replay.html">IBM - Exploring Computer vision group</a> </li>
<li>

<p></p></li>
</ul>

<p>"</p>

<p><a class="uri" href="Category:Multimedia" title="wikilink">Category:Multimedia</a> <a class="uri" href="Category:Technology" title="wikilink">Category:Technology</a></p>
</body>
</html>
