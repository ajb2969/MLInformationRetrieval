<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="377">Hill climbing</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Hill climbing</h1>
<hr/>

<p>In <a href="computer_science" title="wikilink">computer science</a>, <strong>hill climbing</strong> is a <a href="Optimization_(mathematics)" title="wikilink">mathematical optimization</a> technique which belongs to the family of <a href="Local_search_(optimization)" title="wikilink">local search</a>. It is an iterative algorithm that starts with an arbitrary solution to a problem, then attempts to find a better solution by <a href="incremental_heuristic_search" title="wikilink">incrementally</a> changing a single element of the solution. If the change produces a better solution, an incremental change is made to the new solution, repeating until no further improvements can be found.</p>

<p>For example, hill climbing can be applied to the <a href="travelling_salesman_problem" title="wikilink">travelling salesman problem</a>. It is easy to find an initial solution that visits all the cities but will be very poor compared to the optimal solution. The algorithm starts with such a solution and makes small improvements to it, such as switching the order in which two cities are visited. Eventually, a much shorter route is likely to be obtained.</p>

<p>Hill climbing is good for finding a <a href="local_optimum" title="wikilink">local optimum</a> (a solution that cannot be improved by considering a neighbouring configuration) but it is not necessarily guaranteed to find the best possible solution (the <a href="global_optimum" title="wikilink">global optimum</a>) out of all possible solutions (the <a href="Candidate_solution" title="wikilink">search space</a>). In <a href="convex_optimization" title="wikilink">convex</a> problems, hill-climbing <em>is</em> optimal. Examples of algorithms that solve convex problems by hill-climbing include the <a href="simplex_algorithm" title="wikilink">simplex algorithm</a> for <a href="linear_programming" title="wikilink">linear programming</a> and <a href="binary_search" title="wikilink">binary search</a>.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a></p>

<p>The characteristic that only local optima are guaranteed can be cured by using restarts (repeated local search), or more complex schemes based on iterations, like <a href="iterated_local_search" title="wikilink">iterated local search</a>, on memory, like <a href="reactive_search_optimization" title="wikilink">reactive search optimization</a> and <a href="tabu_search" title="wikilink">tabu search</a>, or memory-less stochastic modifications, like <a href="simulated_annealing" title="wikilink">simulated annealing</a>.</p>

<p>The relative simplicity of the algorithm makes it a popular first choice amongst optimizing algorithms. It is used widely in <a href="artificial_intelligence" title="wikilink">artificial intelligence</a>, for reaching a goal state from a starting node. Choice of next node and starting node can be varied to give a list of related algorithms. Although more advanced algorithms such as <a href="simulated_annealing" title="wikilink">simulated annealing</a> or <a href="tabu_search" title="wikilink">tabu search</a> may give better results, in some situations hill climbing works just as well. Hill climbing can often produce a better result than other algorithms when the amount of time available to perform a search is limited, such as with real-time systems. It is an <a href="anytime_algorithm" title="wikilink">anytime algorithm</a>: it can return a valid solution even if it's interrupted at any time before it ends.</p>
<h2 id="mathematical-description">Mathematical description</h2>

<p>Hill climbing attempts to maximize (or minimize) a target <a href="function_(mathematics)" title="wikilink">function</a> 

<math display="inline" id="Hill_climbing:0">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>ùê±</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>ùê±</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(\mathbf{x})
  </annotation>
 </semantics>
</math>

, where 

<math display="inline" id="Hill_climbing:1">
 <semantics>
  <mi>ùê±</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ùê±</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}
  </annotation>
 </semantics>
</math>


 is a vector of continuous and/or discrete values. At each iteration, hill climbing will adjust a single element in 

<math display="inline" id="Hill_climbing:2">
 <semantics>
  <mi>ùê±</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ùê±</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}
  </annotation>
 </semantics>
</math>

 and determine whether the change improves the value of 

<math display="inline" id="Hill_climbing:3">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>ùê±</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>ùê±</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(\mathbf{x})
  </annotation>
 </semantics>
</math>

. (Note that this differs from <a href="gradient_descent" title="wikilink">gradient descent</a> methods, which adjust all of the values in 

<math display="inline" id="Hill_climbing:4">
 <semantics>
  <mi>ùê±</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ùê±</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}
  </annotation>
 </semantics>
</math>

 at each iteration according to the gradient of the hill.) With hill climbing, any change that improves 

<math display="inline" id="Hill_climbing:5">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>ùê±</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>ùê±</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(\mathbf{x})
  </annotation>
 </semantics>
</math>

 is accepted, and the process continues until no change can be found to improve the value of 

<math display="inline" id="Hill_climbing:6">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>ùê±</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>ùê±</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(\mathbf{x})
  </annotation>
 </semantics>
</math>


. Then 

<math display="inline" id="Hill_climbing:7">
 <semantics>
  <mi>ùê±</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ùê±</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}
  </annotation>
 </semantics>
</math>

 is said to be "locally optimal".</p>

<p>In discrete vector spaces, each possible value for 

<math display="inline" id="Hill_climbing:8">
 <semantics>
  <mi>ùê±</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>ùê±</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}
  </annotation>
 </semantics>
</math>

 may be visualized as a <a href="vertex_(graph_theory)" title="wikilink">vertex</a> in a <a href="graph_(mathematics)" title="wikilink">graph</a>. Hill climbing will follow the graph from vertex to vertex, always locally increasing (or decreasing) the value of 

<math display="inline" id="Hill_climbing:9">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>ùê±</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <ci>ùê±</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(\mathbf{x})
  </annotation>
 </semantics>
</math>

, until a <a href="local_maximum" title="wikilink">local maximum</a> (or <a href="local_minimum" title="wikilink">local minimum</a>) 

<math display="inline" id="Hill_climbing:10">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{m}
  </annotation>
 </semantics>
</math>

 is reached.</p>
<figure><b>(Figure)</b>
<figcaption><em>A convex surface. Hill-climbers are well-suited for optimizing over such surfaces, and will converge to the global maximum.</em></figcaption>
</figure>
<h2 id="variants">Variants</h2>

<p>In <strong>simple hill climbing</strong>, the first closer node is chosen, whereas in <strong>steepest ascent hill climbing</strong> all successors are compared and the closest to the solution is chosen. Both forms fail if there is no closer node, which may happen if there are local maxima in the search space which are not solutions. Steepest ascent hill climbing is similar to <a href="best-first_search" title="wikilink">best-first search</a>, which tries all possible extensions of the current path instead of only one.</p>

<p><strong><a href="Stochastic_hill_climbing" title="wikilink">Stochastic hill climbing</a></strong> does not examine all neighbors before deciding how to move. Rather, it selects a neighbor at random, and decides (based on the amount of improvement in that neighbor) whether to move to that neighbor or to examine another.</p>

<p><a href="Coordinate_descent" title="wikilink">Coordinate descent</a> does a <a href="line_search" title="wikilink">line search</a> along one coordinate direction at the current point in each iteration. Some versions of coordinate descent randomly pick a different coordinate direction each iteration.</p>

<p><strong>Random-restart hill climbing</strong> is a <a class="uri" href="meta-algorithm" title="wikilink">meta-algorithm</a> built on top of the hill climbing algorithm. It is also known as <strong>Shotgun hill climbing</strong>. It iteratively does hill-climbing, each time with a random initial condition 

<math display="inline" id="Hill_climbing:11">
 <semantics>
  <msub>
   <mi>x</mi>
   <mn>0</mn>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <cn type="integer">0</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{0}
  </annotation>
 </semantics>
</math>


. The best 

<math display="inline" id="Hill_climbing:12">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{m}
  </annotation>
 </semantics>
</math>

 is kept: if a new run of hill climbing produces a better 

<math display="inline" id="Hill_climbing:13">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>m</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>m</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{m}
  </annotation>
 </semantics>
</math>

 than the stored state, it replaces the stored state.</p>

<p>Random-restart hill climbing is a surprisingly effective algorithm in many cases. It turns out that it is often better to spend CPU time exploring the space, than carefully optimizing from an initial condition. </p>
<h2 id="problems">Problems</h2>
<h3 id="local-maxima">Local maxima</h3>
<figure><b>(Figure)</b>
<figcaption><em>A surface with two local maxima. (Only one of them is the global maximum.) If a hill-climber begins in a poor location, it may converge to the lower maximum.</em></figcaption>
</figure>

<p>Hill climbing will not necessarily find the global maximum, but may instead converge on a <a href="maxima_and_minima" title="wikilink">local maximum</a>. This problem does not occur if the heuristic is convex. However, as many functions are not convex hill climbing may often fail to reach a global maximum. Other local search algorithms try to overcome this problem such as <a href="stochastic_hill_climbing" title="wikilink">stochastic hill climbing</a>, <a href="random_walk" title="wikilink">random walks</a> and <a href="simulated_annealing" title="wikilink">simulated annealing</a>.</p>

<p> </p>
<h3 id="ridges-and-alleys">Ridges and alleys</h3>
<figure><b>(Figure)</b>
<figcaption><em>A ridge</em></figcaption>
</figure>

<p>Ridges are a challenging problem for hill climbers that optimize in continuous spaces. Because hill climbers only adjust one element in the vector at a time, each step will move in an axis-aligned direction. If the target function creates a narrow ridge that ascends in a non-axis-aligned direction (or if the goal is to minimize, a narrow alley that descends in a non-axis-aligned direction), then the hill climber can only ascend the ridge (or descend the alley) by zig-zagging. If the sides of the ridge (or alley) are very steep, then the hill climber may be forced to take very tiny steps as it zig-zags toward a better position. Thus, it may take an unreasonable length of time for it to ascend the ridge (or descend the alley).</p>

<p>By contrast, gradient descent methods can move in any direction that the ridge or alley may ascend or descend. Hence, gradient descent or <a href="conjugate_gradient_method" title="wikilink">conjugate gradient method</a> is generally preferred over hill climbing when the target function is differentiable. Hill climbers, however, have the advantage of not requiring the target function to be differentiable, so hill climbers may be preferred when the target function is complex.</p>
<h3 id="plateau">Plateau</h3>

<p>Another problem that sometimes occurs with hill climbing is that of a plateau. A plateau is encountered when the search space is flat, or sufficiently flat that the value returned by the target function is indistinguishable from the value returned for nearby regions due to the precision used by the machine to represent its value. In such cases, the hill climber may not be able to determine in which direction it should step, and may wander in a direction that never leads to improvement.</p>
<h2 id="pseudocode">Pseudocode</h2>
<pre><code>
Discrete Space Hill Climbing Algorithm
   currentNode = startNode;
   loop do
      L = NEIGHBORS(currentNode);
      nextEval = -INF;
      nextNode = NULL;
      for all x in L 
         if (EVAL(x) &gt; nextEval)
              nextNode = x;
              nextEval = EVAL(x);
      if nextEval &lt;= EVAL(currentNode)
         //Return current node since no better neighbors exist
         return currentNode;
      currentNode = nextNode;</code></pre>
<pre><code>Continuous Space Hill Climbing Algorithm
   currentPoint = initialPoint;    // the zero-magnitude vector is common
   stepSize = initialStepSizes;    // a vector of all 1's is common
   acceleration = someAcceleration; // a value such as 1.2 is common
   candidate[0] = -acceleration;
   candidate[1] = -1 / acceleration;
   candidate[2] = 0;
   candidate[3] = 1 / acceleration;
   candidate[4] = acceleration;
   loop do
      before = EVAL(currentPoint);
      for each element i in currentPoint do
         best = -1;
         bestScore = -INF;
         for j from 0 to 4         // try each of 5 candidate locations
            currentPoint[i] = currentPoint[i] + stepSize[i] * candidate[j];
            temp = EVAL(currentPoint);
            currentPoint[i] = currentPoint[i] - stepSize[i] * candidate[j];
            if(temp &gt; bestScore)
                 bestScore = temp;
                 best = j;
         if candidate[best] is not 0
            currentPoint[i] = currentPoint[i] + stepSize[i] * candidate[best];
            stepSize[i] = stepSize[i] * candidate[best]; // accelerate
      if (EVAL(currentPoint) - before) &lt; epsilon 
         return currentPoint;</code></pre>

<p>Contrast <a href="genetic_algorithm" title="wikilink">genetic algorithm</a>; <a href="random_optimization" title="wikilink">random optimization</a>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Gradient_descent" title="wikilink">Gradient descent</a></li>
<li><a href="Greedy_algorithm" title="wikilink">Greedy algorithm</a></li>
<li><a href="Walrasian_auction" title="wikilink">T√¢tonnement</a></li>
<li><a class="uri" href="Mean-shift" title="wikilink">Mean-shift</a></li>
</ul>
<h2 id="references">References</h2>
<ul>
<li></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://yuval.bar-or.org/index.php?item=9">Hill Climbing visualization</a> A visualization of a hill-climbing (greedy) solution to the N-Queens puzzle by Yuval Baror.</li>
</ul>

<p>"</p>

<p><a href="Category:Optimization_algorithms_and_methods" title="wikilink">Category:Optimization algorithms and methods</a> <a href="Category:Search_algorithms" title="wikilink">Category:Search algorithms</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">‚Ü©</a></li>
</ol>
</section>
</body>
</html>
