<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1691">Bulk synchronous parallel</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Bulk synchronous parallel</h1>
<hr/>

<p>The <strong>Bulk Synchronous Parallel</strong> (BSP) <a href="Abstract_machine" title="wikilink">abstract computer</a> is a <a href="bridging_model" title="wikilink">bridging model</a> for designing <a href="parallel_algorithm" title="wikilink">parallel algorithms</a>. It serves a purpose similar to the <a href="Parallel_Random_Access_Machine" title="wikilink">Parallel Random Access Machine</a> (PRAM) model. BSP differs from PRAM by not taking communication and synchronization for granted. An important part of analyzing a BSP algorithm rests on quantifying the synchronization and communication needed.</p>
<h2 id="history">History</h2>

<p>The BSP model was developed by <a href="Leslie_Valiant" title="wikilink">Leslie Valiant</a> of <a href="Harvard_University" title="wikilink">Harvard University</a> during the 1980s. The definitive article<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> was published in 1990.</p>

<p>Between 1990 and 1992, Leslie Valiant and Bill McColl of <a href="Oxford_University" title="wikilink">Oxford University</a> worked on ideas for a distributed memory BSP programming model, in Princeton and at Harvard. Between 1992 and 1997, McColl led a large research team at Oxford that developed various BSP programming libraries, languages and tools, and also numerous massively parallel BSP algorithms. With interest and momentum growing, McColl then led a group from Oxford, Harvard, Florida, Princeton, Bell Labs, Columbia and Utrecht that developed and published the BSPlib Standard for BSP programming in 1996.</p>

<p>Valiant developed an extension to the BSP model in the 2000s, leading to the publication of the Multi-BSP model<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> in 2011.</p>
<h2 id="the-model">The model</h2>

<p>A BSP computer consists of</p>
<ol>
<li>components capable of processing and/or local memory transactions (i.e., processors),</li>
<li>a network that routes messages between pairs of such components, and</li>
<li>a hardware facility that allows for the synchronisation of all or a subset of components.</li>
</ol>

<p>This is commonly interpreted as a set of processors which may follow different <a href="thread_(computer_science)" title="wikilink">threads</a> of computation, with each processor equipped with fast local memory and interconnected by a communication network. A BSP algorithm relies heavily on the third feature; a computation proceeds in a series of global <em>supersteps</em>, which consists of three components:</p>
<ul>
<li><em>Concurrent computation</em>: every participating processor may perform local computations, i.e., each process can only make use of values stored in the local fast memory of the processor. The computations occur asynchronously of all the others but may overlap with communication.</li>
<li><em>Communication</em>: The processes exchange data between themselves to facilitate remote data storage capabilities.</li>
<li><em>Barrier synchronisation</em>: When a process reaches this point (the <em>barrier</em>), it waits until all other processes have reached the same barrier.</li>
</ul>

<p>The computation and communication actions do not have to be ordered in time. Communication typically takes the form of the one-sided <em>put</em> and <em>get</em> Direct Remote Memory Access (DRMA) calls, rather than paired two-sided <em>send</em> and <em>receive</em> message passing calls. The barrier synchronization concludes the superstep: it ensures that all one-sided communications are properly concluded. Systems based on two-sided communication include this synchronisation cost implicitly for every message sent. The method for barrier synchronisation relies on the hardware facility of the BSP computer. In Valiant's original paper,<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> this facility periodically checks if the end of the current superstep is reached globally. The period of this check is denoted by 

<math display="inline" id="Bulk_synchronous_parallel:0">
 <semantics>
  <mi>L</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>L</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   L
  </annotation>
 </semantics>
</math>

.</p>

<p>The figure below shows this in a diagrammatic form. The processes are not regarded as having a particular linear order (from left to right or otherwise), and may be mapped to processors in any way.</p>
<figure><b>(Figure)</b>
<figcaption>bsp.wiki.fig1.svg</figcaption>
</figure>

<p>The BSP model is also well-suited to enable automatic memory management for distributed-memory computing through overdecomposition of the problem and oversubscription of the processors. The computation is divided into more logical processes than there are physical processors, and processes are randomly assigned to processors. This strategy can be shown statistically to lead to almost perfect load balancing, both of work and communication.</p>
<h2 id="communication">Communication</h2>

<p>In many parallel programming systems, communications are considered at the level of individual actions: sending and receiving a message, memory to memory transfer, etc. This is difficult to work with, since there are many simultaneous communication actions in a parallel program, and their interactions are typically complex. In particular, it is difficult to say much about the time any single communication action will take to complete.</p>

<p>The BSP model considers communication actions <em>en masse</em>. This has the effect that an upper bound on the time taken to communicate a set of data can be given. BSP considers all communication actions of a superstep as one unit, and assumes all individual messages sent as part of this unit have a fixed size.</p>

<p>The maximum number of incoming or outgoing messages for a superstep is denoted by 

<math display="inline" id="Bulk_synchronous_parallel:1">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>

. The ability of a communication network to deliver data is captured by a parameter 

<math display="inline" id="Bulk_synchronous_parallel:2">
 <semantics>
  <mi>g</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>g</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g
  </annotation>
 </semantics>
</math>

, defined such that it takes time 

<math display="inline" id="Bulk_synchronous_parallel:3">
 <semantics>
  <mrow>
   <mi>h</mi>
   <mi>g</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>h</ci>
    <ci>g</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   hg
  </annotation>
 </semantics>
</math>

 for a processor to deliver 

<math display="inline" id="Bulk_synchronous_parallel:4">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>


 messages of size 1.</p>

<p>A message of length 

<math display="inline" id="Bulk_synchronous_parallel:5">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

 obviously takes longer to send than a message of size 1. However, the BSP model does not make a distinction between a message length of 

<math display="inline" id="Bulk_synchronous_parallel:6">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

 or 

<math display="inline" id="Bulk_synchronous_parallel:7">
 <semantics>
  <mi>m</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>m</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   m
  </annotation>
 </semantics>
</math>

 messages of length 1. In either case the cost is said to be 

<math display="inline" id="Bulk_synchronous_parallel:8">
 <semantics>
  <mrow>
   <mi>m</mi>
   <mi>g</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>m</ci>
    <ci>g</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   mg
  </annotation>
 </semantics>
</math>

.</p>

<p>The parameter 

<math display="inline" id="Bulk_synchronous_parallel:9">
 <semantics>
  <mi>g</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>g</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g
  </annotation>
 </semantics>
</math>


 is dependent on the following factors:</p>
<ul>
<li>The protocols used to interact within the communication network.</li>
<li>Buffer management by both the processors and the communication network.</li>
<li>The routing strategy used in the network.</li>
<li>The BSP runtime system.</li>
</ul>

<p>In practice, 

<math display="inline" id="Bulk_synchronous_parallel:10">
 <semantics>
  <mi>g</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>g</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g
  </annotation>
 </semantics>
</math>

 is determined empirically for each parallel computer. Note that 

<math display="inline" id="Bulk_synchronous_parallel:11">
 <semantics>
  <mi>g</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>g</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   g
  </annotation>
 </semantics>
</math>

 is not the normalised single-word delivery time, but the single-word delivery time under continuous traffic conditions.</p>
<h2 id="barriers">Barriers</h2>

<p>The one-sided communication of the BSP model requires <a href="Barrier_(computer_science)" title="wikilink">barrier synchronization</a>. <a href="Barrier_(computer_science)" title="wikilink">Barriers</a> are potentially costly, but avoid the possibility of <a class="uri" href="deadlock" title="wikilink">deadlock</a> or <a href="Deadlock" title="wikilink">livelock</a>, since barriers cannot create circular data dependencies. Tools to detect them and deal with them are unnecessary. Barriers also permit novel forms of <a href="Fault-tolerant_system" title="wikilink">fault tolerance</a>.</p>

<p>The cost of barrier synchronization is influenced by a couple of issues:</p>
<ol>
<li>The cost imposed by the variation in the completion time of the participating concurrent computations. Take the example where all but one of the processes have completed their work for this superstep, and are waiting for the last process, which still has a lot of work to complete. The best that an implementation can do is ensure that each process works on roughly the same problem size.</li>
<li>The cost of reaching a globally consistent state in all of the processors. This depends on the communication network, but also on whether there is special-purpose hardware available for synchronizing, and on the way in which interrupts are handled by processors.</li>
</ol>

<p>The cost of a barrier synchronization is denoted by 

<math display="inline" id="Bulk_synchronous_parallel:12">
 <semantics>
  <mi>l</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>l</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   l
  </annotation>
 </semantics>
</math>

. Note that 

<math display="inline" id="Bulk_synchronous_parallel:13">
 <semantics>
  <mi>p</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>p</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p
  </annotation>
 </semantics>
</math>

 is determined empirically.</p>

<p>On large computers barriers are expensive, and this is increasingly so on large scales. There is a large body of literature on removing synchronization points from existing algorithms, both in the context of BSP computing and beyond. For example, many algorithms allow for the local detection of the global end of a superstep simply by comparing local information to the number of messages already received. This drives the cost of a global synchronisation, compared to the minimally required latency of communication, to zero.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a> Yet also this minimal latency is expected to increase further for future supercomputer architectures and network interconnects; the BSP model, along with other models for parallel computation, require adaptation to cope with this trend. Multi-BSP<a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a> is one BSP-based solution.</p>
<h2 id="the-cost-of-a-bsp-algorithm">The Cost of a BSP algorithm</h2>

<p>The cost of a superstep is determined as the sum of three terms; the cost of the longest running local computation, the cost of global communication between the processors, and the cost of the barrier synchronisation at the end of the superstep. The cost of one superstep for 

<math display="inline" id="Bulk_synchronous_parallel:14">
 <semantics>
  <mrow>
   <mrow>
    <mi>m</mi>
    <mi>a</mi>
    <msubsup>
     <mi>x</mi>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>p</mi>
    </msubsup>
    <mrow>
     <mo stretchy="false">(</mo>
     <msub>
      <mi>w</mi>
      <mi>i</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mrow>
    <mi>m</mi>
    <mi>a</mi>
    <msubsup>
     <mi>x</mi>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>p</mi>
    </msubsup>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>h</mi>
       <mi>i</mi>
      </msub>
      <mi>g</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>+</mo>
   <mi>l</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <apply>
     <times></times>
     <ci>m</ci>
     <ci>a</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>p</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>w</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <ci>m</ci>
     <ci>a</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>p</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>h</ci>
       <ci>i</ci>
      </apply>
      <ci>g</ci>
     </apply>
    </apply>
    <ci>l</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   max_{i=1}^{p}(w_{i})+max_{i=1}^{p}(h_{i}g)+l
  </annotation>
 </semantics>
</math>


 processors:</p>

<p>

<math display="inline" id="Bulk_synchronous_parallel:15">
 <semantics>
  <msub>
   <mi>w</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>w</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w_{i}
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Bulk_synchronous_parallel:16">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

 is the cost for the local computation in process 

<math display="inline" id="Bulk_synchronous_parallel:17">
 <semantics>
  <msub>
   <mi>h</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>h</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h_{i}
  </annotation>
 </semantics>
</math>

, and 

<math display="inline" id="Bulk_synchronous_parallel:18">
 <semantics>
  <mi>i</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>i</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   i
  </annotation>
 </semantics>
</math>

 is the number of messages sent or received by process 

<math display="inline" id="Bulk_synchronous_parallel:19">
 <semantics>
  <mrow>
   <mi>w</mi>
   <mo>+</mo>
   <mrow>
    <mi>h</mi>
    <mi>g</mi>
   </mrow>
   <mo>+</mo>
   <mi>l</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <ci>w</ci>
    <apply>
     <times></times>
     <ci>h</ci>
     <ci>g</ci>
    </apply>
    <ci>l</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w+hg+l
  </annotation>
 </semantics>
</math>


. Note that homogeneous processors are assumed here. It is more common for the expression to be written as 

<math display="inline" id="Bulk_synchronous_parallel:20">
 <semantics>
  <mi>w</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>w</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   w
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Bulk_synchronous_parallel:21">
 <semantics>
  <mi>h</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>h</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   h
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Bulk_synchronous_parallel:22">
 <semantics>
  <mrow>
   <mrow>
    <mi>W</mi>
    <mo>+</mo>
    <mrow>
     <mi>H</mi>
     <mi>g</mi>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>S</mi>
     <mi>l</mi>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">∑</mo>
      <mrow>
       <mi>s</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>S</mi>
     </msubsup>
     <msub>
      <mi>w</mi>
      <mi>s</mi>
     </msub>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>g</mi>
     <mrow>
      <msubsup>
       <mo largeop="true" symmetric="true">∑</mo>
       <mrow>
        <mi>s</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>S</mi>
      </msubsup>
      <msub>
       <mi>h</mi>
       <mi>s</mi>
      </msub>
     </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
     <mi>S</mi>
     <mi>l</mi>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <plus></plus>
     <ci>W</ci>
     <apply>
      <times></times>
      <ci>H</ci>
      <ci>g</ci>
     </apply>
     <apply>
      <times></times>
      <ci>S</ci>
      <ci>l</ci>
     </apply>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>s</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>S</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>w</ci>
       <ci>s</ci>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>g</ci>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>s</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>S</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>h</ci>
        <ci>s</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <times></times>
      <ci>S</ci>
      <ci>l</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W+Hg+Sl=\sum_{s=1}^{S}w_{s}+g\sum_{s=1}^{S}h_{s}+Sl
  </annotation>
 </semantics>
</math>

 are maxima. The cost of the algorithm then, is the sum of the costs of each superstep.</p>

<p>

<math display="inline" id="Bulk_synchronous_parallel:23">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

 where 

<math display="inline" id="Bulk_synchronous_parallel:24">
 <semantics>
  <mi>W</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>W</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W
  </annotation>
 </semantics>
</math>


 is the number of supersteps.</p>

<p>

<math display="inline" id="Bulk_synchronous_parallel:25">
 <semantics>
  <mi>H</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>H</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H
  </annotation>
 </semantics>
</math>

, 

<math display="inline" id="Bulk_synchronous_parallel:26">
 <semantics>
  <mi>S</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>S</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   S
  </annotation>
 </semantics>
</math>

, and 

<math display="inline" id="Bulk_synchronous_parallel:27">
 <semantics>
  <mrow>
   <mi>H</mi>
   <mo>∈</mo>
   <mrow>
    <mi>O</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <mi>n</mi>
      <mo>/</mo>
      <mi>p</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <in></in>
    <ci>H</ci>
    <apply>
     <times></times>
     <ci>O</ci>
     <apply>
      <divide></divide>
      <ci>n</ci>
      <ci>p</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H\in O(n/p)
  </annotation>
 </semantics>
</math>

 are usually modelled as functions, that vary with problem size. These three characteristics of a BSP algorithm are usually described in terms of <a href="asymptotic_notation" title="wikilink">asymptotic notation</a>, e.g. <span class="LaTeX">$H \in O(n/p)$</span>.</p>
<h2 id="extensions-and-uses">Extensions and uses</h2>

<p>Interest in BSP has soared in recent years, with Google adopting it as a major technology for graph analytics at massive scale via technologies like Pregel and MapReduce. Also, with the next generation of Hadoop decoupling the MapReduce model from the rest of the Hadoop infrastructure, there are now active open source projects to add explicit BSP programming, as well as other high performance parallel programming models, on top of Hadoop. Examples are <a href="Apache_Hama" title="wikilink">Apache Hama</a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a> and <a href="Apache_Giraph" title="wikilink">Apache Giraph</a>.</p>

<p>BSP has been extended by many authors to address concerns about BSP's unsuitability for modelling specific architectures or computational paradigms. One example of this is the decomposable BSP model. The model has also been used in the creation of a number of new programming languages and interfaces, such as Bulk Synchronous Parallel ML (BSML), BSPLib,<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> <a href="Apache_Hama" title="wikilink">Apache Hama</a>,<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> and Pregel.<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a></p>

<p>Notable implementations of the BSPLib standard are the Paderborn University BSP library<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a> and the Oxford BSP Toolset by Jonathan Hill.<a class="footnoteRef" href="#fn11" id="fnref11"><sup>11</sup></a> Modern implementations include BSPonMPI<a class="footnoteRef" href="#fn12" id="fnref12"><sup>12</sup></a> (which simulates BSP on top of the <a href="Message_Passing_Interface" title="wikilink">Message Passing Interface</a>), and MulticoreBSP<a class="footnoteRef" href="#fn13" id="fnref13"><sup>13</sup></a><a class="footnoteRef" href="#fn14" id="fnref14"><sup>14</sup></a> (a novel implementation targeting modern shared-memory architectures). MulticoreBSP for C is especially notable for its capability of starting nested BSP runs, thus allowing for explicit Multi-BSP programming.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Computer_cluster" title="wikilink">Computer cluster</a></li>
<li><a href="Concurrent_computing" title="wikilink">Concurrent computing</a></li>
<li><a href="Concurrency_(computer_science)" title="wikilink">Concurrency</a></li>
<li><a href="Dataflow_programming" title="wikilink">Dataflow programming</a></li>
<li><a href="Grid_computing" title="wikilink">Grid computing</a></li>
<li><a href="Parallel_computing" title="wikilink">Parallel computing</a></li>
<li><a class="uri" href="ScientificPython" title="wikilink">ScientificPython</a></li>
<li><a href="LogP_machine" title="wikilink">LogP machine</a></li>
<li><a href="Automatic_mutual_exclusion" title="wikilink">Automatic mutual exclusion</a></li>
<li><a href="Apache_Hama" title="wikilink">Apache Hama</a></li>
<li><a href="Apache_Giraph" title="wikilink">Apache Giraph</a></li>
</ul>
<h2 id="references">References</h2>
<h2 id="external-links">External links</h2>
<ul>
<li>Collection of papers on BSP by <a href="http://www.linkedin.com/in/billmccoll">Bill McColl</a> and others. [<a class="uri" href="http://paloaltodata.com/index.php?option=com_content&amp;view">http://paloaltodata.com/index.php?option=com_content&amp;view;</a>;=article&amp;id;=22 BSP Papers]</li>
<li>BSP topic on Quora. <a href="http://www.quora.com/Bulk-Synchronous-Parallel-Computing">Bulk Synchronous Parallel Computing</a></li>
<li>D.B. Skillicorn, Jonathan Hill, W. F. McColl, <a href="ftp://ftp.comlab.ox.ac.uk/pub/Documents/techpapers/Jonathan.Hill/SkillHillMcColl_QA.ps.gz">Questions and answers about BSP</a> (1996)</li>
<li><a href="http://www.bsp-worldwide.org/">BSP Worldwide</a></li>
<li><a href="http://www.bsp-worldwide.org/implmnts/oxtool/papers.html">BSP related papers</a></li>
<li>

<p><a href=":fr:BSML" title="wikilink">Bulk Synchronous Parallel ML</a> ( <a href="http://frederic.loulergue.eu/research/bsml/index.html">official website</a>)</p></li>
<li><a href="http://hama.apache.org/">Apache Hama</a></li>
<li><a href="http://giraph.apache.org/">Apache Giraph</a></li>
<li><a href="http://www2.cs.uni-paderborn.de/~pub/">Paderborn University BSP library</a></li>
<li><a href="http://bsponmpi.sourceforge.net">BSPonMPI</a></li>
<li><a href="http://www.multicorebsp.com">MulticoreBSP</a></li>
</ul>

<p>"</p>

<p><a href="Category:Models_of_computation" title="wikilink">Category:Models of computation</a> <a href="Category:Parallel_computing" title="wikilink">Category:Parallel computing</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1">Leslie G. Valiant, A bridging model for parallel computation, Communications of the ACM, Volume 33 Issue 8, Aug. 1990 <a href="http://portal.acm.org/citation.cfm?id=79173.79181">1</a><a href="#fnref1">↩</a></li>
<li id="fn2">Valiant, L. G. (2011). A bridging model for multi-core computing. Journal of Computer and System Sciences, 77(1), 154-166 <a href="http://dx.doi.org/10.1016/j.jcss.2010.06.012">2</a><a href="#fnref2">↩</a></li>
<li id="fn3"></li>
<li id="fn4">Alpert, R., &amp; Philbin, J. (1997). cBSP: Zero-cost synchronization in a modified BSP model. NEC Research Institute, 4 Independence Way, Princeton NJ, 8540, [<a class="uri" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.7784&amp;rep">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.7784&amp;rep;</a>;=rep1&amp;type;=pdf].<a href="#fnref4">↩</a></li>
<li id="fn5"></li>
<li id="fn6"><a href="http://hama.apache.org/">Apache Hama</a><a href="#fnref6">↩</a></li>
<li id="fn7"><a href="http://www.bsp-worldwide.org/implmnts/oxtool/bsplib.html">BSPlib</a><a href="#fnref7">↩</a></li>
<li id="fn8"></li>
<li id="fn9"><a href="http://dl.acm.org/citation.cfm?id=1582723">Pregel</a><a href="#fnref9">↩</a></li>
<li id="fn10">The Paderborn University BSP (PUB) Library - Design, Implementation and Performance Heinz Nixdorf Institute, Departement of Computer Science, University of Paderborn, Germany, <a href="http://www.uni-paderborn.de/fachbereich/AG/agmadh/PapersPostscript/inri.98.tr-rsfb-98-063.ps.gz">technical report</a>.<a href="#fnref10">↩</a></li>
<li id="fn11">Jonathan Hill: <a href="http://www.bsp-worldwide.org/implmnts/oxtool/">The Oxford BSP Toolset</a>, 1998.<a href="#fnref11">↩</a></li>
<li id="fn12">Wijnand J. Suijlen: <a href="http://bsponmpi.sourceforge.net">BSPonMPI</a>, 2006.<a href="#fnref12">↩</a></li>
<li id="fn13">MulticoreBSP for C: a high-performance library for shared-memory parallel programming by A. N. Yzelman, R. H. Bisseling, D. Roose, and K. Meerbergen in International Journal of Parallel Programming, in press (2013), <a href="http://dx.doi.org/10.1109/TPDS.2013.31"></a><a class="uri" href="doi:10.1109/TPDS.2013.31">doi:10.1109/TPDS.2013.31</a>.<a href="#fnref13">↩</a></li>
<li id="fn14">An Object-Oriented Bulk Synchronous Parallel Library for Multicore Programming by A. N. Yzelman &amp; Rob H. Bisseling in Concurrency and Computation: Practice and Experience 24(5), pp. 533-553 (2012), <a href="http://dx.doi.org/10.1002/cpe.1843"></a><a class="uri" href="doi:10.1002/cpe.1843">doi:10.1002/cpe.1843</a>.<a href="#fnref14">↩</a></li>
</ol>
</section>
</body>
</html>
