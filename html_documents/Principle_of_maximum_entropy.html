<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="935">Principle of maximum entropy</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Principle of maximum entropy</h1>
<hr/>

<p>The <strong>principle of maximum entropy</strong> states that, subject to precisely stated prior data (such as a <a class="uri" href="proposition" title="wikilink">proposition</a> that expresses <a href="#Testable_information" title="wikilink">testable information</a>), the <a href="probability_distribution" title="wikilink">probability distribution</a> which best represents the current state of knowledge is the one with largest <a href="Entropy_(information_theory)" title="wikilink">entropy</a>.</p>

<p>Another way of stating this: Take precisely stated prior data or testable information about a probability distribution function. Consider the set of all trial probability distributions that would encode the prior data. Of those, the one with maximal <a href="information_entropy" title="wikilink">information entropy</a> is the proper distribution, according to this principle.</p>
<h2 id="history">History</h2>

<p>The principle was first expounded by <a href="E._T._Jaynes" title="wikilink">E. T. Jaynes</a> in two papers in 1957<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a><a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a> where he emphasized a natural correspondence between <a href="statistical_mechanics" title="wikilink">statistical mechanics</a> and <a href="information_theory" title="wikilink">information theory</a>. In particular, Jaynes offered a new and very general rationale why the Gibbsian method of statistical mechanics works. He argued that the <a class="uri" href="entropy" title="wikilink">entropy</a> of statistical mechanics and the <a href="information_entropy" title="wikilink">information entropy</a> of <a href="information_theory" title="wikilink">information theory</a> are principally the same thing. Consequently, <a href="statistical_mechanics" title="wikilink">statistical mechanics</a> should be seen just as a particular application of a general tool of logical <a class="uri" href="inference" title="wikilink">inference</a> and information theory.</p>
<h2 id="overview">Overview</h2>

<p>In most practical cases, the stated prior data or testable information is given by a set of <a href="conserved_quantities" title="wikilink">conserved quantities</a> (average values of some moment functions), associated with the <a href="probability_distribution" title="wikilink">probability distribution</a> in question. This is the way the maximum entropy principle is most often used in <a href="statistical_thermodynamics" title="wikilink">statistical thermodynamics</a>. Another possibility is to prescribe some <a class="uri" href="symmetries" title="wikilink">symmetries</a> of the probability distribution. The equivalence between <a href="conserved_quantities" title="wikilink">conserved quantities</a> and corresponding <a href="symmetry_groups" title="wikilink">symmetry groups</a> implies a similar equivalence for these two ways of specifying the testable information in the maximum entropy method.</p>

<p>The maximum entropy principle is also needed to guarantee the uniqueness and consistency of probability assignments obtained by different methods, <a href="statistical_mechanics" title="wikilink">statistical mechanics</a> and <a href="logical_inference" title="wikilink">logical inference</a> in particular.</p>

<p>The maximum entropy principle makes explicit our freedom in using different forms of <a href="prior_information" title="wikilink">prior data</a>. As a special case, a uniform <a href="prior_probability" title="wikilink">prior probability</a> density (Laplace's <a href="principle_of_indifference" title="wikilink">principle of indifference</a>, sometimes called the principle of insufficient reason), may be adopted. Thus, the maximum entropy principle is not merely an alternative way to view the usual methods of inference of classical statistics, but represents a significant conceptual generalization of those methods. It means that thermodynamics systems need not be shown to be <a class="uri" href="ergodic" title="wikilink">ergodic</a> to justify treatment as a <a href="statistical_ensemble" title="wikilink">statistical ensemble</a>.</p>

<p>In ordinary language, the principle of maximum entropy can be said to express a claim of epistemic modesty, or of maximum ignorance. The selected distribution is the one that makes the least claim to being informed beyond the stated prior data, that is to say the one that admits the most ignorance beyond the stated prior data.</p>
<h2 id="testable-information">Testable information</h2>

<p>The principle of maximum entropy is useful explicitly only when applied to <em>testable information</em>. Testable information is a statement about a probability distribution whose truth or falsity is well-defined. For example, the statements</p>
<dl>
<dd>the <a href="expected_value" title="wikilink">expectation</a> of the variable <em>x</em> is 2.87
</dd>
</dl>

<p>and</p>
<dl>
<dd><em>p</em><sub>2</sub> + <em>p</em><sub>3</sub> &gt; 0.6
</dd>
</dl>

<p>(where <em>p</em><sub>2</sub> + <em>p</em><sub>3</sub> are probabilities of events) are statements of testable information.</p>

<p>Given testable information, the maximum entropy procedure consists of seeking the <a href="probability_distribution" title="wikilink">probability distribution</a> which maximizes <a href="information_entropy" title="wikilink">information entropy</a>, subject to the constraints of the information. This constrained optimization problem is typically solved using the method of <a href="Lagrange_multiplier" title="wikilink">Lagrange multipliers</a>.</p>

<p>Entropy maximization with no testable information respects the universal "constraint" that the sum of the probabilities is one. Under this constraint, the maximum entropy discrete probability distribution is the <a href="uniform_distribution_(discrete)" title="wikilink">uniform distribution</a>,</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:0">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>p</mi>
     <mi>i</mi>
    </msub>
    <mo>=</mo>
    <mrow>
     <mpadded width="+5pt">
      <mfrac>
       <mn>1</mn>
       <mi>n</mi>
      </mfrac>
     </mpadded>
     <mpadded width="+5pt">
      <mi>for</mi>
     </mpadded>
     <mpadded width="+5pt">
      <mi>all</mi>
     </mpadded>
     <mi>i</mi>
    </mrow>
    <mo>∈</mo>
    <mrow>
     <mo stretchy="false">{</mo>
     <mn>1</mn>
     <mo>,</mo>
     <mi mathvariant="normal">…</mi>
     <mo>,</mo>
     <mpadded width="+1.7pt">
      <mi>n</mi>
     </mpadded>
     <mo stretchy="false">}</mo>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <and></and>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>p</ci>
      <ci>i</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>n</ci>
      </apply>
      <ci>for</ci>
      <ci>all</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <in></in>
     <share href="#.cmml">
     </share>
     <set>
      <cn type="float">1</cn>
      <ci>normal-…</ci>
      <ci>n</ci>
     </set>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p_{i}=\frac{1}{n}\ {\rm for\ all}\ i\in\{\,1,\dots,n\,\}.
  </annotation>
 </semantics>
</math>

</p>
<h2 id="applications">Applications</h2>

<p>The principle of maximum entropy is commonly applied in two ways to inferential problems:</p>
<h3 id="prior-probabilities">Prior probabilities</h3>

<p>The principle of maximum entropy is often used to obtain <a href="prior_probability" title="wikilink">prior probability distributions</a> for <a href="Bayesian_inference" title="wikilink">Bayesian inference</a>. Jaynes was a strong advocate of this approach, claiming the maximum entropy distribution represented the least informative distribution.<a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> A large amount of literature is now dedicated to the elicitation of maximum entropy priors and links with <a href="channel_coding" title="wikilink">channel coding</a>.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a><a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a><a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h3 id="maximum-entropy-models">Maximum entropy models</h3>

<p>Alternatively, the principle is often invoked for model specification: in this case the observed data itself is assumed to be the testable information. Such models are widely used in <a href="natural_language_processing" title="wikilink">natural language processing</a>. An example of such a model is <a href="logistic_regression" title="wikilink">logistic regression</a>, which corresponds to the maximum entropy classifier for independent observations.</p>
<h2 id="general-solution-for-the-maximum-entropy-distribution-with-linear-constraints">General solution for the maximum entropy distribution with linear constraints</h2>
<h3 id="discrete-case">Discrete case</h3>

<p>We have some testable information <em>I</em> about a quantity <em>x</em> taking values in {<em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>,..., <em>x<sub>n</sub></em>}. We assume this information has the form of <em>m</em> constraints on the expectations of the functions <em>f<sub>k</sub></em>; that is, we require our probability distribution to satisfy</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:1">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <munderover>
       <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
       <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>n</mi>
      </munderover>
      <mrow>
       <mrow>
        <mi>Pr</mi>
        <mrow>
         <mo stretchy="false">(</mo>
         <msub>
          <mi>x</mi>
          <mi>i</mi>
         </msub>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <msub>
        <mi>f</mi>
        <mi>k</mi>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <msub>
         <mi>x</mi>
         <mi>i</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mrow>
     <mo>=</mo>
     <msub>
      <mi>F</mi>
      <mi>k</mi>
     </msub>
    </mrow>
    <mrow>
     <mi>k</mi>
     <mo>=</mo>
     <mrow>
      <mn>1</mn>
      <mo>,</mo>
      <mi mathvariant="normal">…</mi>
      <mo>,</mo>
      <mi>m</mi>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <eq></eq>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <ci>Pr</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>f</ci>
        <ci>k</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>F</ci>
      <ci>k</ci>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <ci>k</ci>
     <list>
      <cn type="integer">1</cn>
      <ci>normal-…</ci>
      <ci>m</ci>
     </list>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{i=1}^{n}\Pr(x_{i})f_{k}(x_{i})=F_{k}\qquad k=1,\ldots,m.
  </annotation>
 </semantics>
</math>

</p>

<p>Furthermore, the probabilities must sum to one, giving the constraint</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:2">
 <semantics>
  <mrow>
   <mrow>
    <munderover>
     <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
     <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>n</mi>
    </munderover>
    <mrow>
     <mi>Pr</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>1.</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>n</ci>
     </apply>
     <apply>
      <ci>Pr</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
    <cn type="float">1.</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{i=1}^{n}\Pr(x_{i})=1.
  </annotation>
 </semantics>
</math>

</p>

<p>The probability distribution with maximum information entropy subject to these constraints is</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:3">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>Pr</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mfrac>
      <mn>1</mn>
      <mrow>
       <mi>Z</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <msub>
         <mi>λ</mi>
         <mn>1</mn>
        </msub>
        <mo>,</mo>
        <mi mathvariant="normal">…</mi>
        <mo>,</mo>
        <msub>
         <mi>λ</mi>
         <mi>m</mi>
        </msub>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
     </mfrac>
     <mrow>
      <mi>exp</mi>
      <mrow>
       <mo>[</mo>
       <mrow>
        <mrow>
         <msub>
          <mi>λ</mi>
          <mn>1</mn>
         </msub>
         <msub>
          <mi>f</mi>
          <mn>1</mn>
         </msub>
         <mrow>
          <mo stretchy="false">(</mo>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>+</mo>
        <mi mathvariant="normal">⋯</mi>
        <mo>+</mo>
        <mrow>
         <msub>
          <mi>λ</mi>
          <mi>m</mi>
         </msub>
         <msub>
          <mi>f</mi>
          <mi>m</mi>
         </msub>
         <mrow>
          <mo stretchy="false">(</mo>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
       </mrow>
       <mo>]</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>Pr</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>i</ci>
     </apply>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <apply>
       <times></times>
       <ci>Z</ci>
       <vector>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>λ</ci>
         <cn type="integer">1</cn>
        </apply>
        <ci>normal-…</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>λ</ci>
         <ci>m</ci>
        </apply>
       </vector>
      </apply>
     </apply>
     <apply>
      <exp></exp>
      <apply>
       <plus></plus>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>λ</ci>
         <cn type="integer">1</cn>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>f</ci>
         <cn type="integer">1</cn>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
       </apply>
       <ci>normal-⋯</ci>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>λ</ci>
         <ci>m</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>f</ci>
         <ci>m</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Pr(x_{i})=\frac{1}{Z(\lambda_{1},\ldots,\lambda_{m})}\exp\left[\lambda_{1}f_{%
1}(x_{i})+\cdots+\lambda_{m}f_{m}(x_{i})\right].
  </annotation>
 </semantics>
</math>

</p>

<p>It is sometimes called the <a href="Gibbs_distribution" title="wikilink">Gibbs distribution</a>. The normalization constant is determined by</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:4">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>Z</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>λ</mi>
       <mn>1</mn>
      </msub>
      <mo>,</mo>
      <mi mathvariant="normal">…</mi>
      <mo>,</mo>
      <msub>
       <mi>λ</mi>
       <mi>m</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </munderover>
     <mrow>
      <mi>exp</mi>
      <mrow>
       <mo>[</mo>
       <mrow>
        <mrow>
         <msub>
          <mi>λ</mi>
          <mn>1</mn>
         </msub>
         <msub>
          <mi>f</mi>
          <mn>1</mn>
         </msub>
         <mrow>
          <mo stretchy="false">(</mo>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
        <mo>+</mo>
        <mi mathvariant="normal">⋯</mi>
        <mo>+</mo>
        <mrow>
         <msub>
          <mi>λ</mi>
          <mi>m</mi>
         </msub>
         <msub>
          <mi>f</mi>
          <mi>m</mi>
         </msub>
         <mrow>
          <mo stretchy="false">(</mo>
          <msub>
           <mi>x</mi>
           <mi>i</mi>
          </msub>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
       </mrow>
       <mo>]</mo>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>Z</ci>
     <vector>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <cn type="integer">1</cn>
      </apply>
      <ci>normal-…</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>m</ci>
      </apply>
     </vector>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>i</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>n</ci>
     </apply>
     <apply>
      <exp></exp>
      <apply>
       <plus></plus>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>λ</ci>
         <cn type="integer">1</cn>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>f</ci>
         <cn type="integer">1</cn>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
       </apply>
       <ci>normal-⋯</ci>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>λ</ci>
         <ci>m</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>f</ci>
         <ci>m</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>x</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Z(\lambda_{1},\ldots,\lambda_{m})=\sum_{i=1}^{n}\exp\left[\lambda_{1}f_{1}(x_{%
i})+\cdots+\lambda_{m}f_{m}(x_{i})\right],
  </annotation>
 </semantics>
</math>

</p>

<p>and is conventionally called the <a href="partition_function_(mathematics)" title="wikilink">partition function</a>. (Interestingly, the <a href="Pitman–Koopman_theorem" title="wikilink">Pitman–Koopman theorem</a> states that the necessary and sufficient condition for a sampling distribution to admit <a href="sufficiency_(statistics)" title="wikilink">sufficient statistics</a> of bounded dimension is that it have the general form of a maximum entropy distribution.)</p>

<p>The λ<sub>k</sub> parameters are Lagrange multipliers whose particular values are determined by the constraints according to</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:5">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>F</mi>
     <mi>k</mi>
    </msub>
    <mo>=</mo>
    <mrow>
     <mfrac>
      <mo>∂</mo>
      <mrow>
       <mo>∂</mo>
       <msub>
        <mi>λ</mi>
        <mi>k</mi>
       </msub>
      </mrow>
     </mfrac>
     <mrow>
      <mi>log</mi>
      <mi>Z</mi>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>λ</mi>
       <mn>1</mn>
      </msub>
      <mo>,</mo>
      <mi mathvariant="normal">…</mi>
      <mo>,</mo>
      <msub>
       <mi>λ</mi>
       <mi>m</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>F</ci>
     <ci>k</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <partialdiff></partialdiff>
      <apply>
       <partialdiff></partialdiff>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>λ</ci>
        <ci>k</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <log></log>
      <ci>Z</ci>
     </apply>
     <vector>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <cn type="integer">1</cn>
      </apply>
      <ci>normal-…</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>m</ci>
      </apply>
     </vector>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F_{k}=\frac{\partial}{\partial\lambda_{k}}\log Z(\lambda_{1},\ldots,\lambda_{m%
}).
  </annotation>
 </semantics>
</math>

</p>

<p>These <em>m</em> simultaneous equations do not generally possess a <a href="closed_form_solution" title="wikilink">closed form solution</a>, and are usually solved by <a href="Numerical_analysis" title="wikilink">numerical methods</a>.</p>
<h3 id="continuous-case">Continuous case</h3>

<p>For <a href="continuous_distribution" title="wikilink">continuous distributions</a>, the Shannon entropy cannot be used, as it is only defined for discrete probability spaces. Instead <a href="E._T._Jaynes" title="wikilink">Edwin Jaynes</a> (1963, 1968, 2003) gave the following formula, which is closely related to the <a href="relative_entropy" title="wikilink">relative entropy</a> (see also <a href="differential_entropy" title="wikilink">differential entropy</a>).</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:6">
 <semantics>
  <mrow>
   <msub>
    <mi>H</mi>
    <mi>c</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <mrow>
     <mo largeop="true" symmetric="true">∫</mo>
     <mrow>
      <mi>p</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
      <mrow>
       <mi>log</mi>
       <mrow>
        <mpadded width="+1.7pt">
         <mfrac>
          <mrow>
           <mi>p</mi>
           <mrow>
            <mo stretchy="false">(</mo>
            <mi>x</mi>
            <mo stretchy="false">)</mo>
           </mrow>
          </mrow>
          <mrow>
           <mi>m</mi>
           <mrow>
            <mo stretchy="false">(</mo>
            <mi>x</mi>
            <mo stretchy="false">)</mo>
           </mrow>
          </mrow>
         </mfrac>
        </mpadded>
        <mi>d</mi>
        <mi>x</mi>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>H</ci>
     <ci>c</ci>
    </apply>
    <apply>
     <minus></minus>
     <apply>
      <int></int>
      <apply>
       <times></times>
       <ci>p</ci>
       <ci>x</ci>
       <apply>
        <log></log>
        <apply>
         <times></times>
         <apply>
          <divide></divide>
          <apply>
           <times></times>
           <ci>p</ci>
           <ci>x</ci>
          </apply>
          <apply>
           <times></times>
           <ci>m</ci>
           <ci>x</ci>
          </apply>
         </apply>
         <ci>d</ci>
         <ci>x</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   H_{c}=-\int p(x)\log\frac{p(x)}{m(x)}\,dx
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>m</em>(<em>x</em>), which Jaynes called the "invariant measure", is proportional to the <a href="limiting_density_of_discrete_points" title="wikilink">limiting density of discrete points</a>. For now, we shall assume that <em>m</em> is known; we will discuss it further after the solution equations are given.</p>

<p>A closely related quantity, the relative entropy, is usually defined as the <a href="Kullback–Leibler_divergence" title="wikilink">Kullback–Leibler divergence</a> of <em>m</em> from <em>p</em> (although it is sometimes, confusingly, defined as the negative of this). The inference principle of minimizing this, due to Kullback, is known as the <a href="Kullback–Leibler_divergence#Principle_of_minimum_discrimination_information" title="wikilink">Principle of Minimum Discrimination Information</a>.</p>

<p>We have some testable information <em>I</em> about a quantity <em>x</em> which takes values in some <a href="interval_(mathematics)" title="wikilink">interval</a> of the <a href="real_numbers" title="wikilink">real numbers</a> (all integrals below are over this interval). We assume this information has the form of <em>m</em> constraints on the expectations of the functions <em>f<sub>k</sub></em>, i.e. we require our probability density function to satisfy</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:7">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mrow>
      <mo largeop="true" symmetric="true">∫</mo>
      <mrow>
       <mi>p</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
       </mrow>
       <msub>
        <mi>f</mi>
        <mi>k</mi>
       </msub>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
       </mrow>
       <mi>d</mi>
       <mi>x</mi>
      </mrow>
     </mrow>
     <mo>=</mo>
     <msub>
      <mi>F</mi>
      <mi>k</mi>
     </msub>
    </mrow>
    <mrow>
     <mi>k</mi>
     <mo>=</mo>
     <mrow>
      <mn>1</mn>
      <mo>,</mo>
      <mi mathvariant="normal">…</mi>
      <mo>,</mo>
      <mi>m</mi>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <eq></eq>
     <apply>
      <int></int>
      <apply>
       <times></times>
       <ci>p</ci>
       <ci>x</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>f</ci>
        <ci>k</ci>
       </apply>
       <ci>x</ci>
       <ci>d</ci>
       <ci>x</ci>
      </apply>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>F</ci>
      <ci>k</ci>
     </apply>
    </apply>
    <apply>
     <eq></eq>
     <ci>k</ci>
     <list>
      <cn type="integer">1</cn>
      <ci>normal-…</ci>
      <ci>m</ci>
     </list>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \int p(x)f_{k}(x)dx=F_{k}\qquad k=1,\ldots,m.
  </annotation>
 </semantics>
</math>

</p>

<p>And of course, the probability density must integrate to one, giving the constraint</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:8">
 <semantics>
  <mrow>
   <mrow>
    <mo largeop="true" symmetric="true">∫</mo>
    <mrow>
     <mi>p</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
     <mi>d</mi>
     <mi>x</mi>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>1.</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <int></int>
     <apply>
      <times></times>
      <ci>p</ci>
      <ci>x</ci>
      <ci>d</ci>
      <ci>x</ci>
     </apply>
    </apply>
    <cn type="float">1.</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \int p(x)dx=1.
  </annotation>
 </semantics>
</math>

</p>

<p>The probability density function with maximum <em>H<sub>c</sub></em> subject to these constraints is</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:9">
 <semantics>
  <mrow>
   <mrow>
    <mi>p</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mfrac>
     <mn>1</mn>
     <mrow>
      <mi>Z</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <msub>
        <mi>λ</mi>
        <mn>1</mn>
       </msub>
       <mo>,</mo>
       <mi mathvariant="normal">…</mi>
       <mo>,</mo>
       <msub>
        <mi>λ</mi>
        <mi>m</mi>
       </msub>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mfrac>
    <mi>m</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>x</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mi>exp</mi>
     <mrow>
      <mo>[</mo>
      <mrow>
       <mrow>
        <msub>
         <mi>λ</mi>
         <mn>1</mn>
        </msub>
        <msub>
         <mi>f</mi>
         <mn>1</mn>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
       <mo>+</mo>
       <mi mathvariant="normal">⋯</mi>
       <mo>+</mo>
       <mrow>
        <msub>
         <mi>λ</mi>
         <mi>m</mi>
        </msub>
        <msub>
         <mi>f</mi>
         <mi>m</mi>
        </msub>
        <mrow>
         <mo stretchy="false">(</mo>
         <mi>x</mi>
         <mo stretchy="false">)</mo>
        </mrow>
       </mrow>
      </mrow>
      <mo>]</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>p</ci>
     <ci>x</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <cn type="integer">1</cn>
      <apply>
       <times></times>
       <ci>Z</ci>
       <vector>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>λ</ci>
         <cn type="integer">1</cn>
        </apply>
        <ci>normal-…</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>λ</ci>
         <ci>m</ci>
        </apply>
       </vector>
      </apply>
     </apply>
     <ci>m</ci>
     <ci>x</ci>
     <apply>
      <exp></exp>
      <apply>
       <plus></plus>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>λ</ci>
         <cn type="integer">1</cn>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>f</ci>
         <cn type="integer">1</cn>
        </apply>
        <ci>x</ci>
       </apply>
       <ci>normal-⋯</ci>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>λ</ci>
         <ci>m</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>f</ci>
         <ci>m</ci>
        </apply>
        <ci>x</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(x)=\frac{1}{Z(\lambda_{1},\ldots,\lambda_{m})}m(x)\exp\left[\lambda_{1}f_{1}%
(x)+\cdots+\lambda_{m}f_{m}(x)\right]
  </annotation>
 </semantics>
</math>

</p>

<p>with the <a href="partition_function_(mathematics)" title="wikilink">partition function</a> determined by</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:10">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>Z</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>λ</mi>
       <mn>1</mn>
      </msub>
      <mo>,</mo>
      <mi mathvariant="normal">…</mi>
      <mo>,</mo>
      <msub>
       <mi>λ</mi>
       <mi>m</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mo largeop="true" symmetric="true">∫</mo>
     <mrow>
      <mi>m</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mi>x</mi>
       <mo stretchy="false">)</mo>
      </mrow>
      <mrow>
       <mi>exp</mi>
       <mrow>
        <mo>[</mo>
        <mrow>
         <mrow>
          <msub>
           <mi>λ</mi>
           <mn>1</mn>
          </msub>
          <msub>
           <mi>f</mi>
           <mn>1</mn>
          </msub>
          <mrow>
           <mo stretchy="false">(</mo>
           <mi>x</mi>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
         <mo>+</mo>
         <mi mathvariant="normal">⋯</mi>
         <mo>+</mo>
         <mrow>
          <msub>
           <mi>λ</mi>
           <mi>m</mi>
          </msub>
          <msub>
           <mi>f</mi>
           <mi>m</mi>
          </msub>
          <mrow>
           <mo stretchy="false">(</mo>
           <mi>x</mi>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
        </mrow>
        <mo>]</mo>
       </mrow>
      </mrow>
      <mi>d</mi>
      <mi>x</mi>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>Z</ci>
     <vector>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <cn type="integer">1</cn>
      </apply>
      <ci>normal-…</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>m</ci>
      </apply>
     </vector>
    </apply>
    <apply>
     <int></int>
     <apply>
      <times></times>
      <ci>m</ci>
      <ci>x</ci>
      <apply>
       <exp></exp>
       <apply>
        <plus></plus>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>λ</ci>
          <cn type="integer">1</cn>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>f</ci>
          <cn type="integer">1</cn>
         </apply>
         <ci>x</ci>
        </apply>
        <ci>normal-⋯</ci>
        <apply>
         <times></times>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>λ</ci>
          <ci>m</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>f</ci>
          <ci>m</ci>
         </apply>
         <ci>x</ci>
        </apply>
       </apply>
      </apply>
      <ci>d</ci>
      <ci>x</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Z(\lambda_{1},\ldots,\lambda_{m})=\int m(x)\exp\left[\lambda_{1}f_{1}(x)+%
\cdots+\lambda_{m}f_{m}(x)\right]dx.
  </annotation>
 </semantics>
</math>

</p>

<p>As in the discrete case, the values of the 

<math display="inline" id="Principle_of_maximum_entropy:11">
 <semantics>
  <msub>
   <mi>λ</mi>
   <mi>k</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>λ</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \lambda_{k}
  </annotation>
 </semantics>
</math>

 parameters are determined by the constraints according to</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:12">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>F</mi>
     <mi>k</mi>
    </msub>
    <mo>=</mo>
    <mrow>
     <mfrac>
      <mo>∂</mo>
      <mrow>
       <mo>∂</mo>
       <msub>
        <mi>λ</mi>
        <mi>k</mi>
       </msub>
      </mrow>
     </mfrac>
     <mrow>
      <mi>log</mi>
      <mi>Z</mi>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>λ</mi>
       <mn>1</mn>
      </msub>
      <mo>,</mo>
      <mi mathvariant="normal">…</mi>
      <mo>,</mo>
      <msub>
       <mi>λ</mi>
       <mi>m</mi>
      </msub>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>F</ci>
     <ci>k</ci>
    </apply>
    <apply>
     <times></times>
     <apply>
      <divide></divide>
      <partialdiff></partialdiff>
      <apply>
       <partialdiff></partialdiff>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>λ</ci>
        <ci>k</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <log></log>
      <ci>Z</ci>
     </apply>
     <vector>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <cn type="integer">1</cn>
      </apply>
      <ci>normal-…</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>λ</ci>
       <ci>m</ci>
      </apply>
     </vector>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   F_{k}=\frac{\partial}{\partial\lambda_{k}}\log Z(\lambda_{1},\ldots,\lambda_{m%
}).
  </annotation>
 </semantics>
</math>

</p>

<p>The invariant measure function <em>m</em>(<em>x</em>) can be best understood by supposing that <em>x</em> is known to take values only in the <a href="bounded_interval" title="wikilink">bounded interval</a> (<em>a</em>, <em>b</em>), and that no other information is given. Then the maximum entropy probability density function is</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:13">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>p</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mi>A</mi>
      <mo>⋅</mo>
      <mi>m</mi>
     </mrow>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo rspace="22.5pt">,</mo>
   <mrow>
    <mi>a</mi>
    <mo><</mo>
    <mi>x</mi>
    <mo><</mo>
    <mi>b</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">formulae-sequence</csymbol>
    <apply>
     <eq></eq>
     <apply>
      <times></times>
      <ci>p</ci>
      <ci>x</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <ci>normal-⋅</ci>
       <ci>A</ci>
       <ci>m</ci>
      </apply>
      <ci>x</ci>
     </apply>
    </apply>
    <apply>
     <and></and>
     <apply>
      <lt></lt>
      <ci>a</ci>
      <ci>x</ci>
     </apply>
     <apply>
      <lt></lt>
      <share href="#.cmml">
      </share>
      <ci>b</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p(x)=A\cdot m(x),\qquad a<x<b
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>A</em> is a normalization constant. The invariant measure function is actually the prior density function encoding 'lack of relevant information'. It cannot be determined by the principle of maximum entropy, and must be determined by some other logical method, such as the <a href="principle_of_transformation_groups" title="wikilink">principle of transformation groups</a> or <a href="Marginalization_(probability)" title="wikilink">marginalization theory</a>.</p>
<h3 id="examples">Examples</h3>

<p>For several examples of maximum entropy distributions, see the article on <a href="maximum_entropy_probability_distribution" title="wikilink">maximum entropy probability distributions</a>.</p>
<h2 id="justifications-for-the-principle-of-maximum-entropy">Justifications for the principle of maximum entropy</h2>

<p>Proponents of the principle of maximum entropy justify its use in assigning probabilities in several ways, including the following two arguments. These arguments take the use of <a href="Bayesian_probability" title="wikilink">Bayesian probability</a> as given, and are thus subject to the same postulates.</p>
<h3 id="information-entropy-as-a-measure-of-uninformativeness">Information entropy as a measure of 'uninformativeness'</h3>

<p>Consider a <strong>discrete probability distribution</strong> among <em>m</em> mutually exclusive <a href="proposition" title="wikilink">propositions</a>. The most informative distribution would occur when one of the propositions was known to be true. In that case, the information entropy would be equal to zero. The least informative distribution would occur when there is no reason to favor any one of the propositions over the others. In that case, the only reasonable probability distribution would be uniform, and then the information entropy would be equal to its maximum possible value, log <em>m</em>. The information entropy can therefore be seen as a numerical measure which describes how uninformative a particular probability distribution is, ranging from zero (completely informative) to log <em>m</em> (completely uninformative).</p>

<p>By choosing to use the distribution with the maximum entropy allowed by our information, the argument goes, we are choosing the most uninformative distribution possible. To choose a distribution with lower entropy would be to assume information we do not possess. Thus the maximum entropy distribution is the only reasonable distribution.</p>
<h3 id="the-wallis-derivation">The Wallis derivation</h3>

<p>The following argument is the result of a suggestion made by <a href="Graham_Wallis" title="wikilink">Graham Wallis</a> to E. T. Jaynes in 1962.<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> It is essentially the same mathematical argument used for the <a href="Maxwell–Boltzmann_statistics" title="wikilink">Maxwell–Boltzmann statistics</a> in <a href="statistical_mechanics" title="wikilink">statistical mechanics</a>, although the conceptual emphasis is quite different. It has the advantage of being strictly combinatorial in nature, making no reference to information entropy as a measure of 'uncertainty', 'uninformativeness', or any other imprecisely defined concept. The information entropy function is not assumed <em>a priori</em>, but rather is found in the course of the argument; and the argument leads naturally to the procedure of maximizing the information entropy, rather than treating it in some other way.</p>

<p>Suppose an individual wishes to make a probability assignment among <em>m</em> <a href="mutually_exclusive" title="wikilink">mutually exclusive</a> propositions. She has some testable information, but is not sure how to go about including this information in her probability assessment. She therefore conceives of the following random experiment. She will distribute <em>N</em> quanta of probability (each worth 1/<em>N</em>) at random among the <em>m</em> possibilities. (One might imagine that she will throw <em>N</em> balls into <em>m</em> buckets while blindfolded. In order to be as fair as possible, each throw is to be independent of any other, and every bucket is to be the same size.) Once the experiment is done, she will check if the probability assignment thus obtained is consistent with her information. (For this step to be successful, the information must be a constraint given by an open set in the space of probability measures). If it is inconsistent, she will reject it and try again. If it is consistent, her assessment will be</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:14">
 <semantics>
  <mrow>
   <msub>
    <mi>p</mi>
    <mi>i</mi>
   </msub>
   <mo>=</mo>
   <mfrac>
    <msub>
     <mi>n</mi>
     <mi>i</mi>
    </msub>
    <mi>N</mi>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>p</ci>
     <ci>i</ci>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>n</ci>
      <ci>i</ci>
     </apply>
     <ci>N</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   p_{i}=\frac{n_{i}}{N}
  </annotation>
 </semantics>
</math>

</p>

<p>where <em>p<sub>i</sub></em> is the probability of the <em>i</em><sup>th</sup> proposition, while <em>n<sub>i</sub></em> is the number of quanta that were assigned to the <em>i</em><sup>th</sup> proposition (i.e. the number of balls that ended up in bucket <em>i</em>).</p>

<p>Now, in order to reduce the 'graininess' of the probability assignment, it will be necessary to use quite a large number of quanta of probability. Rather than actually carry out, and possibly have to repeat, the rather long random experiment, the protagonist decides to simply calculate and use the most probable result. The probability of any particular result is the <a href="multinomial_distribution" title="wikilink">multinomial distribution</a>,</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:15">
 <semantics>
  <mrow>
   <mrow>
    <mi>P</mi>
    <mi>r</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>𝐩</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mi>W</mi>
    <mo>⋅</mo>
    <msup>
     <mi>m</mi>
     <mrow>
      <mo>-</mo>
      <mi>N</mi>
     </mrow>
    </msup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>P</ci>
     <ci>r</ci>
     <ci>𝐩</ci>
    </apply>
    <apply>
     <ci>normal-⋅</ci>
     <ci>W</ci>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>m</ci>
      <apply>
       <minus></minus>
       <ci>N</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Pr(\mathbf{p})=W\cdot m^{-N}
  </annotation>
 </semantics>
</math>

</p>

<p>where</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:16">
 <semantics>
  <mrow>
   <mi>W</mi>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>N</mi>
     <mo lspace="0pt" rspace="3.5pt">!</mo>
    </mrow>
    <mrow>
     <mrow>
      <msub>
       <mi>n</mi>
       <mn>1</mn>
      </msub>
      <mo lspace="0pt" rspace="3.5pt">!</mo>
     </mrow>
     <mrow>
      <msub>
       <mi>n</mi>
       <mn>2</mn>
      </msub>
      <mo lspace="0pt" rspace="3.5pt">!</mo>
     </mrow>
     <mpadded width="+1.7pt">
      <mi mathvariant="normal">⋯</mi>
     </mpadded>
     <mrow>
      <msub>
       <mi>n</mi>
       <mi>m</mi>
      </msub>
      <mo lspace="0pt" rspace="3.5pt">!</mo>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>W</ci>
    <apply>
     <divide></divide>
     <apply>
      <factorial></factorial>
      <ci>N</ci>
     </apply>
     <apply>
      <times></times>
      <apply>
       <factorial></factorial>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>n</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <apply>
       <factorial></factorial>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>n</ci>
        <cn type="integer">2</cn>
       </apply>
      </apply>
      <ci>normal-⋯</ci>
      <apply>
       <factorial></factorial>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>n</ci>
        <ci>m</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   W=\frac{N!}{n_{1}!\,n_{2}!\,\cdots\,n_{m}!}
  </annotation>
 </semantics>
</math>

</p>

<p>is sometimes known as the multiplicity of the outcome.</p>

<p>The most probable result is the one which maximizes the multiplicity <em>W</em>. Rather than maximizing <em>W</em> directly, the protagonist could equivalently maximize any monotonic increasing function of <em>W</em>. She decides to maximize</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:17">
 <semantics>
  <mtable displaystyle="true">
   <mtr>
    <mtd columnalign="right">
     <mrow>
      <mfrac>
       <mn>1</mn>
       <mi>N</mi>
      </mfrac>
      <mrow>
       <mi>log</mi>
       <mi>W</mi>
      </mrow>
     </mrow>
    </mtd>
    <mtd columnalign="center">
     <mo>=</mo>
    </mtd>
    <mtd columnalign="left">
     <mrow>
      <mfrac>
       <mn>1</mn>
       <mi>N</mi>
      </mfrac>
      <mrow>
       <mi>log</mi>
       <mfrac>
        <mrow>
         <mi>N</mi>
         <mo lspace="0pt" rspace="3.5pt">!</mo>
        </mrow>
        <mrow>
         <mrow>
          <msub>
           <mi>n</mi>
           <mn>1</mn>
          </msub>
          <mo lspace="0pt" rspace="3.5pt">!</mo>
         </mrow>
         <mrow>
          <msub>
           <mi>n</mi>
           <mn>2</mn>
          </msub>
          <mo lspace="0pt" rspace="3.5pt">!</mo>
         </mrow>
         <mpadded width="+1.7pt">
          <mi mathvariant="normal">⋯</mi>
         </mpadded>
         <mrow>
          <msub>
           <mi>n</mi>
           <mi>m</mi>
          </msub>
          <mo lspace="0pt" rspace="3.5pt">!</mo>
         </mrow>
        </mrow>
       </mfrac>
      </mrow>
     </mrow>
    </mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd></mtd>
    <mtd></mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd columnalign="center">
     <mo>=</mo>
    </mtd>
    <mtd columnalign="left">
     <mrow>
      <mfrac>
       <mn>1</mn>
       <mi>N</mi>
      </mfrac>
      <mrow>
       <mi>log</mi>
       <mfrac>
        <mrow>
         <mi>N</mi>
         <mo lspace="0pt" rspace="3.5pt">!</mo>
        </mrow>
        <mrow>
         <mrow>
          <mrow>
           <mo stretchy="false">(</mo>
           <mrow>
            <mi>N</mi>
            <msub>
             <mi>p</mi>
             <mn>1</mn>
            </msub>
           </mrow>
           <mo stretchy="false">)</mo>
          </mrow>
          <mo lspace="0pt" rspace="3.5pt">!</mo>
         </mrow>
         <mrow>
          <mrow>
           <mo stretchy="false">(</mo>
           <mrow>
            <mi>N</mi>
            <msub>
             <mi>p</mi>
             <mn>2</mn>
            </msub>
           </mrow>
           <mo stretchy="false">)</mo>
          </mrow>
          <mo lspace="0pt" rspace="3.5pt">!</mo>
         </mrow>
         <mpadded width="+1.7pt">
          <mi mathvariant="normal">⋯</mi>
         </mpadded>
         <mrow>
          <mrow>
           <mo stretchy="false">(</mo>
           <mrow>
            <mi>N</mi>
            <msub>
             <mi>p</mi>
             <mi>m</mi>
            </msub>
           </mrow>
           <mo stretchy="false">)</mo>
          </mrow>
          <mo lspace="0pt" rspace="3.5pt">!</mo>
         </mrow>
        </mrow>
       </mfrac>
      </mrow>
     </mrow>
    </mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd></mtd>
    <mtd></mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd columnalign="center">
     <mo>=</mo>
    </mtd>
    <mtd columnalign="left">
     <mrow>
      <mrow>
       <mfrac>
        <mn>1</mn>
        <mi>N</mi>
       </mfrac>
       <mrow>
        <mo>(</mo>
        <mrow>
         <mrow>
          <mi>log</mi>
          <mrow>
           <mi>N</mi>
           <mo lspace="0pt" rspace="3.5pt">!</mo>
          </mrow>
         </mrow>
         <mo>-</mo>
         <mrow>
          <munderover>
           <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
           <mrow>
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
           </mrow>
           <mi>m</mi>
          </munderover>
          <mrow>
           <mi>log</mi>
           <mrow>
            <mo stretchy="false">(</mo>
            <mrow>
             <mrow>
              <mo stretchy="false">(</mo>
              <mrow>
               <mi>N</mi>
               <msub>
                <mi>p</mi>
                <mi>i</mi>
               </msub>
              </mrow>
              <mo stretchy="false">)</mo>
             </mrow>
             <mo lspace="0pt" rspace="3.5pt">!</mo>
            </mrow>
            <mo stretchy="false">)</mo>
           </mrow>
          </mrow>
         </mrow>
        </mrow>
        <mo>)</mo>
       </mrow>
      </mrow>
      <mo>.</mo>
     </mrow>
    </mtd>
   </mtr>
  </mtable>
  <annotation-xml encoding="MathML-Content">
   <matrix>
    <matrixrow>
     <apply>
      <times></times>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>N</ci>
      </apply>
      <apply>
       <log></log>
       <ci>W</ci>
      </apply>
     </apply>
     <eq></eq>
     <apply>
      <times></times>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>N</ci>
      </apply>
      <apply>
       <log></log>
       <apply>
        <divide></divide>
        <apply>
         <factorial></factorial>
         <ci>N</ci>
        </apply>
        <apply>
         <times></times>
         <apply>
          <factorial></factorial>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>n</ci>
           <cn type="integer">1</cn>
          </apply>
         </apply>
         <apply>
          <factorial></factorial>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>n</ci>
           <cn type="integer">2</cn>
          </apply>
         </apply>
         <ci>normal-⋯</ci>
         <apply>
          <factorial></factorial>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>n</ci>
           <ci>m</ci>
          </apply>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <eq></eq>
     <apply>
      <times></times>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>N</ci>
      </apply>
      <apply>
       <log></log>
       <apply>
        <divide></divide>
        <apply>
         <factorial></factorial>
         <ci>N</ci>
        </apply>
        <apply>
         <times></times>
         <apply>
          <factorial></factorial>
          <apply>
           <times></times>
           <ci>N</ci>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>p</ci>
            <cn type="integer">1</cn>
           </apply>
          </apply>
         </apply>
         <apply>
          <factorial></factorial>
          <apply>
           <times></times>
           <ci>N</ci>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>p</ci>
            <cn type="integer">2</cn>
           </apply>
          </apply>
         </apply>
         <ci>normal-⋯</ci>
         <apply>
          <factorial></factorial>
          <apply>
           <times></times>
           <ci>N</ci>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>p</ci>
            <ci>m</ci>
           </apply>
          </apply>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <eq></eq>
     <apply>
      <times></times>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>N</ci>
      </apply>
      <apply>
       <minus></minus>
       <apply>
        <log></log>
        <apply>
         <factorial></factorial>
         <ci>N</ci>
        </apply>
       </apply>
       <apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <sum></sum>
          <apply>
           <eq></eq>
           <ci>i</ci>
           <cn type="integer">1</cn>
          </apply>
         </apply>
         <ci>m</ci>
        </apply>
        <apply>
         <log></log>
         <apply>
          <factorial></factorial>
          <apply>
           <times></times>
           <ci>N</ci>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>p</ci>
            <ci>i</ci>
           </apply>
          </apply>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </matrixrow>
   </matrix>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \begin{array}[]{rcl}\frac{1}{N}\log W&=&\frac{1}{N}\log\frac{N!}{n_{1}!\,n_{2}%
!\,\cdots\,n_{m}!}\\
\\
&=&\frac{1}{N}\log\frac{N!}{(Np_{1})!\,(Np_{2})!\,\cdots\,(Np_{m})!}\\
\\
&=&\frac{1}{N}\left(\log N!-\sum_{i=1}^{m}\log((Np_{i})!)\right).\end{array}
  </annotation>
 </semantics>
</math>

</p>

<p>At this point, in order to simplify the expression, the protagonist takes the limit as 

<math display="inline" id="Principle_of_maximum_entropy:18">
 <semantics>
  <mrow>
   <mi>N</mi>
   <mo>→</mo>
   <mi mathvariant="normal">∞</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>normal-→</ci>
    <ci>N</ci>
    <infinity></infinity>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N\to\infty
  </annotation>
 </semantics>
</math>

, i.e. as the probability levels go from grainy discrete values to smooth continuous values. Using <a href="Stirling's_approximation" title="wikilink">Stirling's approximation</a>, she finds</p>

<p>

<math display="block" id="Principle_of_maximum_entropy:19">
 <semantics>
  <mtable displaystyle="true">
   <mtr>
    <mtd columnalign="right">
     <mrow>
      <munder>
       <mo movablelimits="false">lim</mo>
       <mrow>
        <mi>N</mi>
        <mo>→</mo>
        <mi mathvariant="normal">∞</mi>
       </mrow>
      </munder>
      <mrow>
       <mo>(</mo>
       <mrow>
        <mfrac>
         <mn>1</mn>
         <mi>N</mi>
        </mfrac>
        <mrow>
         <mi>log</mi>
         <mi>W</mi>
        </mrow>
       </mrow>
       <mo>)</mo>
      </mrow>
     </mrow>
    </mtd>
    <mtd columnalign="center">
     <mo>=</mo>
    </mtd>
    <mtd columnalign="left">
     <mrow>
      <mfrac>
       <mn>1</mn>
       <mi>N</mi>
      </mfrac>
      <mrow>
       <mo>(</mo>
       <mrow>
        <mrow>
         <mi>N</mi>
         <mrow>
          <mi>log</mi>
          <mi>N</mi>
         </mrow>
        </mrow>
        <mo>-</mo>
        <mrow>
         <munderover>
          <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
          <mrow>
           <mi>i</mi>
           <mo>=</mo>
           <mn>1</mn>
          </mrow>
          <mi>m</mi>
         </munderover>
         <mrow>
          <mi>N</mi>
          <msub>
           <mi>p</mi>
           <mi>i</mi>
          </msub>
          <mrow>
           <mi>log</mi>
           <mrow>
            <mo stretchy="false">(</mo>
            <mrow>
             <mi>N</mi>
             <msub>
              <mi>p</mi>
              <mi>i</mi>
             </msub>
            </mrow>
            <mo stretchy="false">)</mo>
           </mrow>
          </mrow>
         </mrow>
        </mrow>
       </mrow>
       <mo>)</mo>
      </mrow>
     </mrow>
    </mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd></mtd>
    <mtd></mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd columnalign="center">
     <mo>=</mo>
    </mtd>
    <mtd columnalign="left">
     <mrow>
      <mrow>
       <mi>log</mi>
       <mi>N</mi>
      </mrow>
      <mo>-</mo>
      <mrow>
       <munderover>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mrow>
         <mi>i</mi>
         <mo>=</mo>
         <mn>1</mn>
        </mrow>
        <mi>m</mi>
       </munderover>
       <mrow>
        <msub>
         <mi>p</mi>
         <mi>i</mi>
        </msub>
        <mrow>
         <mi>log</mi>
         <mrow>
          <mo stretchy="false">(</mo>
          <mrow>
           <mi>N</mi>
           <msub>
            <mi>p</mi>
            <mi>i</mi>
           </msub>
          </mrow>
          <mo stretchy="false">)</mo>
         </mrow>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
    </mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd></mtd>
    <mtd></mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd columnalign="center">
     <mo>=</mo>
    </mtd>
    <mtd columnalign="left">
     <mrow>
      <mrow>
       <mi>log</mi>
       <mi>N</mi>
      </mrow>
      <mo>-</mo>
      <mrow>
       <mrow>
        <mi>log</mi>
        <mi>N</mi>
       </mrow>
       <mrow>
        <munderover>
         <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
         <mrow>
          <mi>i</mi>
          <mo>=</mo>
          <mn>1</mn>
         </mrow>
         <mi>m</mi>
        </munderover>
        <msub>
         <mi>p</mi>
         <mi>i</mi>
        </msub>
       </mrow>
      </mrow>
      <mo>-</mo>
      <mrow>
       <munderover>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mrow>
         <mi>i</mi>
         <mo>=</mo>
         <mn>1</mn>
        </mrow>
        <mi>m</mi>
       </munderover>
       <mrow>
        <msub>
         <mi>p</mi>
         <mi>i</mi>
        </msub>
        <mrow>
         <mi>log</mi>
         <msub>
          <mi>p</mi>
          <mi>i</mi>
         </msub>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
    </mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd></mtd>
    <mtd></mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd columnalign="center">
     <mo>=</mo>
    </mtd>
    <mtd columnalign="left">
     <mrow>
      <mrow>
       <mrow>
        <mo>(</mo>
        <mrow>
         <mn>1</mn>
         <mo>-</mo>
         <mrow>
          <munderover>
           <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
           <mrow>
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
           </mrow>
           <mi>m</mi>
          </munderover>
          <msub>
           <mi>p</mi>
           <mi>i</mi>
          </msub>
         </mrow>
        </mrow>
        <mo>)</mo>
       </mrow>
       <mrow>
        <mi>log</mi>
        <mi>N</mi>
       </mrow>
      </mrow>
      <mo>-</mo>
      <mrow>
       <munderover>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mrow>
         <mi>i</mi>
         <mo>=</mo>
         <mn>1</mn>
        </mrow>
        <mi>m</mi>
       </munderover>
       <mrow>
        <msub>
         <mi>p</mi>
         <mi>i</mi>
        </msub>
        <mrow>
         <mi>log</mi>
         <msub>
          <mi>p</mi>
          <mi>i</mi>
         </msub>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
    </mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd></mtd>
    <mtd></mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd columnalign="center">
     <mo>=</mo>
    </mtd>
    <mtd columnalign="left">
     <mrow>
      <mo>-</mo>
      <mrow>
       <munderover>
        <mo largeop="true" movablelimits="false" symmetric="true">∑</mo>
        <mrow>
         <mi>i</mi>
         <mo>=</mo>
         <mn>1</mn>
        </mrow>
        <mi>m</mi>
       </munderover>
       <mrow>
        <msub>
         <mi>p</mi>
         <mi>i</mi>
        </msub>
        <mrow>
         <mi>log</mi>
         <msub>
          <mi>p</mi>
          <mi>i</mi>
         </msub>
        </mrow>
       </mrow>
      </mrow>
     </mrow>
    </mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd></mtd>
    <mtd></mtd>
   </mtr>
   <mtr>
    <mtd></mtd>
    <mtd columnalign="center">
     <mo>=</mo>
    </mtd>
    <mtd columnalign="left">
     <mrow>
      <mrow>
       <mi>H</mi>
       <mrow>
        <mo stretchy="false">(</mo>
        <mi>𝐩</mi>
        <mo stretchy="false">)</mo>
       </mrow>
      </mrow>
      <mo>.</mo>
     </mrow>
    </mtd>
   </mtr>
  </mtable>
  <annotation-xml encoding="MathML-Content">
   <matrix>
    <matrixrow>
     <apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <limit></limit>
       <apply>
        <ci>normal-→</ci>
        <ci>N</ci>
        <infinity></infinity>
       </apply>
      </apply>
      <apply>
       <times></times>
       <apply>
        <divide></divide>
        <cn type="integer">1</cn>
        <ci>N</ci>
       </apply>
       <apply>
        <log></log>
        <ci>W</ci>
       </apply>
      </apply>
     </apply>
     <eq></eq>
     <apply>
      <times></times>
      <apply>
       <divide></divide>
       <cn type="integer">1</cn>
       <ci>N</ci>
      </apply>
      <apply>
       <minus></minus>
       <apply>
        <times></times>
        <ci>N</ci>
        <apply>
         <log></log>
         <ci>N</ci>
        </apply>
       </apply>
       <apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <sum></sum>
          <apply>
           <eq></eq>
           <ci>i</ci>
           <cn type="integer">1</cn>
          </apply>
         </apply>
         <ci>m</ci>
        </apply>
        <apply>
         <times></times>
         <ci>N</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>p</ci>
          <ci>i</ci>
         </apply>
         <apply>
          <log></log>
          <apply>
           <times></times>
           <ci>N</ci>
           <apply>
            <csymbol cd="ambiguous">subscript</csymbol>
            <ci>p</ci>
            <ci>i</ci>
           </apply>
          </apply>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <eq></eq>
     <apply>
      <minus></minus>
      <apply>
       <log></log>
       <ci>N</ci>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>m</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>p</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <log></log>
         <apply>
          <times></times>
          <ci>N</ci>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <ci>p</ci>
           <ci>i</ci>
          </apply>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <eq></eq>
     <apply>
      <minus></minus>
      <apply>
       <log></log>
       <ci>N</ci>
      </apply>
      <apply>
       <times></times>
       <apply>
        <log></log>
        <ci>N</ci>
       </apply>
       <apply>
        <apply>
         <csymbol cd="ambiguous">superscript</csymbol>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <sum></sum>
          <apply>
           <eq></eq>
           <ci>i</ci>
           <cn type="integer">1</cn>
          </apply>
         </apply>
         <ci>m</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>p</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>m</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>p</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <log></log>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>p</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <eq></eq>
     <apply>
      <minus></minus>
      <apply>
       <times></times>
       <apply>
        <minus></minus>
        <cn type="integer">1</cn>
        <apply>
         <apply>
          <csymbol cd="ambiguous">superscript</csymbol>
          <apply>
           <csymbol cd="ambiguous">subscript</csymbol>
           <sum></sum>
           <apply>
            <eq></eq>
            <ci>i</ci>
            <cn type="integer">1</cn>
           </apply>
          </apply>
          <ci>m</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>p</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
       <apply>
        <log></log>
        <ci>N</ci>
       </apply>
      </apply>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>m</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>p</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <log></log>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>p</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <eq></eq>
     <apply>
      <minus></minus>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>i</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <ci>m</ci>
       </apply>
       <apply>
        <times></times>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>p</ci>
         <ci>i</ci>
        </apply>
        <apply>
         <log></log>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>p</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
    </matrixrow>
    <matrixrow>
     <cerror>
      <csymbol cd="ambiguous">missing-subexpression</csymbol>
     </cerror>
     <eq></eq>
     <apply>
      <times></times>
      <ci>H</ci>
      <ci>𝐩</ci>
     </apply>
    </matrixrow>
   </matrix>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \begin{array}[]{rcl}\lim_{N\to\infty}\left(\frac{1}{N}\log W\right)&=&\frac{1}%
{N}\left(N\log N-\sum_{i=1}^{m}Np_{i}\log(Np_{i})\right)\\
\\
&=&\log N-\sum_{i=1}^{m}p_{i}\log(Np_{i})\\
\\
&=&\log N-\log N\sum_{i=1}^{m}p_{i}-\sum_{i=1}^{m}p_{i}\log p_{i}\\
\\
&=&\left(1-\sum_{i=1}^{m}p_{i}\right)\log N-\sum_{i=1}^{m}p_{i}\log p_{i}\\
\\
&=&-\sum_{i=1}^{m}p_{i}\log p_{i}\\
\\
&=&H(\mathbf{p}).\end{array}
  </annotation>
 </semantics>
</math>

</p>

<p>All that remains for the protagonist to do is to maximize entropy under the constraints of her testable information. She has found that the maximum entropy distribution is the most probable of all "fair" random distributions, in the limit as the probability levels go from discrete to continuous.</p>
<h3 id="compatibility-with-bayes-theorem">Compatibility with Bayes' theorem</h3>

<p>Giffin et al. (2007) state that <a href="Bayes'_theorem" title="wikilink">Bayes' theorem</a> and the principle of maximum entropy are completely compatible and can be seen as special cases of the "method of maximum relative entropy". They state that this method reproduces every aspect of orthodox Bayesian inference methods. In addition this new method opens the door to tackling problems that could not be addressed by either the maximal entropy principle or orthodox Bayesian methods individually. Moreover, recent contributions (Lazar 2003, and Schennach 2005) show that frequentist relative-entropy-based inference approaches (such as <a href="empirical_likelihood" title="wikilink">empirical likelihood</a> and <a href="exponentially_tilted_empirical_likelihood" title="wikilink">exponentially tilted empirical likelihood</a> - see e.g. Owen 2001 and Kitamura 2006) can be combined with prior information to perform Bayesian posterior analysis.</p>

<p>Jaynes stated Bayes' theorem was a way to calculate a probability, while maximum entropy was a way to assign a prior probability distribution.<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a></p>

<p>It is however, possible in concept to solve for a posterior distribution directly from a stated prior distribution using the <a href="Cross_entropy" title="wikilink">Principle of Minimum Cross Entropy</a> (or the Principle of Maximum Entropy being a special case of using a <a href="uniform_distribution_(discrete)" title="wikilink">uniform distribution</a> as the given prior), independently of any Bayesian considerations by treating the problem formally as a constrained optimisation problem, the Entropy functional being the objective function. For the case of given average values as testable information (averaged over the sought after probability distribution), the sought after distribution is formally the <a href="Gibbs_measure" title="wikilink">Gibbs (or Boltzmann) distribution</a> the parameters of which must be solved for in order to achieve minimum cross entropy and satisfy the given testable information.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Akaike_information_criterion" title="wikilink">Akaike information criterion</a></li>
<li><a class="uri" href="Dissipation" title="wikilink">Dissipation</a></li>
<li><a href="Entropy_maximization" title="wikilink">Entropy maximization</a></li>
<li><a href="Maximum_entropy_classifier" title="wikilink">Maximum entropy classifier</a></li>
<li><a href="Maximum_entropy_probability_distribution" title="wikilink">Maximum entropy probability distribution</a></li>
<li><a href="Maximum_entropy_spectral_estimation" title="wikilink">Maximum entropy spectral estimation</a></li>
<li><a href="Maximum_entropy_thermodynamics" title="wikilink">Maximum entropy thermodynamics</a></li>
</ul>
<h2 id="notes">Notes</h2>
<h2 id="references">References</h2>
<ul>
<li></li>
<li>Jaynes, E. T., 1986 (new version online 1996), <a href="http://bayes.wustl.edu/etj/articles/cmonkeys.pdf">'Monkeys, kangaroos and 

<math display="inline" id="Principle_of_maximum_entropy:20">
 <semantics>
  <mi>N</mi>
  <annotation-xml encoding="MathML-Content">
   <ci>N</ci>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   N
  </annotation>
 </semantics>
</math>

'</a>, in <em>Maximum-Entropy and Bayesian Methods in Applied Statistics</em>, J. H. Justice (ed.), Cambridge University Press, Cambridge, p. 26.</li>
<li>Bajkova, A. T., 1992, <em>The generalization of maximum entropy method for reconstruction of complex functions</em>. Astronomical and Astrophysical Transactions, V.1, issue 4, p. 313-320.</li>
<li>Giffin, A. and Caticha, A., 2007, <a href="http://arxiv.org/abs/0708.1593"><em>Updating Probabilities with Data and Moments</em></a></li>
<li>Guiasu, S. and Shenitzer, A., 1985, 'The principle of maximum entropy', The Mathematical Intelligencer, <strong>7</strong>(1), 42-48.</li>
<li>Harremoës P. and Topsøe F., 2001, <em>Maximum Entropy Fundamentals</em>, Entropy, 3(3), 191-226.</li>
<li>Kapur, J. N.; and <a href="H._K._Kesavan" title="wikilink"> Kesavan, H. K.</a>, 1992, <em>Entropy optimization principles with applications</em>, Boston: Academic Press. ISBN 0-12-397670-7</li>
<li>Kitamura, Y., 2006, <a href="http://cowles.econ.yale.edu/P/cd/d15b/d1569.pdf"><em>Empirical Likelihood Methods in Econometrics: Theory and Practice</em></a>, Cowles Foundation Discussion Papers 1569, Cowles Foundation, Yale University.</li>
<li>Lazar, N., 2003, "Bayesian Empirical Likelihood", Biometrika, 90, 319-326.</li>
<li>Owen, A. B., <em>Empirical Likelihood</em>, Chapman and Hall.</li>
<li>Schennach, S. M., 2005, "Bayesian Exponentially Tilted Empirical Likelihood", Biometrika, 92(1), 31-46.</li>
<li>Uffink, Jos, 1995, <a href="http://www.phys.uu.nl/~wwwgrnsl/jos/mepabst/mep.pdf">'Can the Maximum Entropy Principle be explained as a consistency requirement?'</a>, Studies in History and Philosophy of Modern Physics <strong>26B</strong>, 223-261.</li>
</ul>
<h2 id="further-reading">Further reading</h2>
<ul>
<li>Ratnaparkhi A. (1997) [<a class="uri" href="http://repository.upenn.edu/cgi/viewcontent.cgi?article=1083&amp;context">http://repository.upenn.edu/cgi/viewcontent.cgi?article=1083&amp;context;</a>;=ircs_reports "A simple introduction to maximum entropy models for natural language processing"] Technical Report 97-08, Institute for Research in Cognitive Science, University of Pennsylvania. An easy-to-read introduction to maximum entropy methods in the context of natural language processing.</li>
</ul>
<ul>
<li>

<p>Open access article containing pointers to various papers and software implementations of Maximum Entropy Model on the net.</p></li>
</ul>
<h2 id="external-links">External links</h2>
<ul>
<li><a href="http://homepages.inf.ed.ac.uk/s0450736/maxent.html">Maximum Entropy Modeling</a> Links to publications, software and resources</li>
<li>[<a class="uri" href="http://www.cs.cmu.edu/">http://www.cs.cmu.edu/</a>~./aberger/maxent.html MaxEnt and Exponential Models] Links to pedagogically-oriented material on maximum entropy and exponential models</li>
</ul>

<p>"</p>

<p><a href="Category:Entropy_and_information" title="wikilink">Category:Entropy and information</a> <a href="Category:Statistical_theory" title="wikilink">Category:Statistical theory</a> <a href="Category:Bayesian_statistics" title="wikilink">Category:Bayesian statistics</a> <a href="Category:Statistical_principles" title="wikilink">Category:Statistical principles</a> <a href="Category:Probability_assessment" title="wikilink">Category:Probability assessment</a> <a href="Category:Mathematical_principles" title="wikilink">Category:Mathematical principles</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">↩</a></li>
<li id="fn2"><a href="#fnref2">↩</a></li>
<li id="fn3"><a href="#fnref3">↩</a></li>
<li id="fn4"><a href="#fnref4">↩</a></li>
<li id="fn5"><a href="#fnref5">↩</a></li>
<li id="fn6"><a href="#fnref6">↩</a></li>
<li id="fn7"></li>
<li id="fn8"></li>
</ol>
</section>
</body>
</html>
