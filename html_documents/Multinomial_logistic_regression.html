<html lang="en">
<head>
<meta charset="utf-8"/>
<title offset="1440">Multinomial logistic regression</title>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript">
</script>
</head>
<body>
<h1>Multinomial logistic regression</h1>
<hr/>
<dl>
<dd><em>"Multinomial regression" redirects here. See also <a href="Multinomial_probit" title="wikilink">Multinomial probit</a>.</em>
</dd>
</dl>

<p>In <a class="uri" href="statistics" title="wikilink">statistics</a>, <strong>multinomial logistic regression</strong> is a <a href="statistical_classification" title="wikilink">classification</a> method that generalizes <a href="logistic_regression" title="wikilink">logistic regression</a> to <a href="multiclass_classification" title="wikilink">multiclass problems</a>, i.e. with more than two possible discrete outcomes.<a class="footnoteRef" href="#fn1" id="fnref1"><sup>1</sup></a> That is, it is a model that is used to predict the probabilities of the different possible outcomes of a <a href="categorical_distribution" title="wikilink">categorically distributed</a> <a href="dependent_variable" title="wikilink">dependent variable</a>, given a set of <a href="independent_variable" title="wikilink">independent variables</a> (which may be real-valued, binary-valued, categorical-valued, etc.).</p>

<p>Multinomial logistic regression is known by a variety of other names, including <strong>polytomous LR</strong>,<a class="footnoteRef" href="#fn2" id="fnref2"><sup>2</sup></a><a class="footnoteRef" href="#fn3" id="fnref3"><sup>3</sup></a> <strong>multiclass LR</strong>, <strong><a href="Softmax_activation_function" title="wikilink">softmax</a> regression</strong>, <strong>multinomial logit</strong>, <strong>maximum entropy</strong> (<strong>MaxEnt</strong>) classifier, <strong>conditional maximum entropy model</strong>.<a class="footnoteRef" href="#fn4" id="fnref4"><sup>4</sup></a></p>
<h2 id="introduction">Introduction</h2>

<p>Multinomial logistic regression is used when the <a href="dependent_variable" title="wikilink">dependent variable</a> in question is <a href="Level_of_measurement#Nominal_measurement" title="wikilink">nominal</a> (equivalently <em>categorical</em>, meaning that it falls into any one of a set of categories which cannot be ordered in any meaningful way) and for which there are more than two categories. Some examples would be:</p>
<ul>
<li>Which major will a college student choose, given their grades, stated likes and dislikes, etc.?</li>
<li>Which blood type does a person have, given the results of various diagnostic tests?</li>
<li>In a hands-free mobile phone dialing application, which person's name was spoken, given various properties of the speech signal?</li>
<li>Which candidate will a person vote for, given particular demographic characteristics?</li>
<li>Which country will a firm locate an office in, given the characteristics of the firm and of the various candidate countries?</li>
</ul>

<p>These are all <a href="statistical_classification" title="wikilink">statistical classification</a> problems. They all have in common a <a href="dependent_variable" title="wikilink">dependent variable</a> to be predicted that comes from one of a limited set of items which cannot be meaningfully ordered, as well as a set of <a href="independent_variable" title="wikilink">independent variables</a> (also known as features, explanators, etc.), which are used to predict the dependent variable. Multinomial logit regression is a particular solution to the classification problem that assumes that a linear combination of the observed features and some problem-specific parameters can be used to determine the probability of each particular outcome of the dependent variable. The best values of the parameters for a given problem are usually determined from some training data (e.g. some people for whom both the diagnostic test results and blood types are known, or some examples of known words being spoken).</p>
<h2 id="assumptions">Assumptions</h2>

<p>The multinomial logit model assumes that data are case specific; that is, each independent variable has a single value for each case. The multinomial logit model also assumes that the dependent variable cannot be perfectly predicted from the independent variables for any case. As with other types of regression, there is no need for the independent variables to be <a href="statistically_independent" title="wikilink">statistically independent</a> from each other (unlike, for example, in a <a href="naive_Bayes_classifier" title="wikilink">naive Bayes classifier</a>); however, <a href="multicollinearity" title="wikilink">collinearity</a> is assumed to be relatively low, as it becomes difficult to differentiate between the impact of several variables if this is not the case. <a class="footnoteRef" href="#fn5" id="fnref5"><sup>5</sup></a></p>

<p>If the multinomial logit is used to model choices, it relies on the assumption of <a href="independence_of_irrelevant_alternatives" title="wikilink">independence of irrelevant alternatives</a> (IIA), which is not always desirable. This assumption states that the odds of preferring one class over another do not depend on the presence or absence of other "irrelevant" alternatives. For example, the relative probabilities of taking a car or bus to work do not change if a bicycle is added as an additional possibility. This allows the choice of <em>K</em> alternatives to be modeled as a set of <em>K</em>-1 independent binary choices, in which one alternative is chosen as a "pivot" and the other <em>K</em>-1 compared against it, one at a time. The IIA hypothesis is a core hypothesis in rational choice theory; however numerous studies in psychology show that individuals often violate this assumption when making choices. An example of a problem case arises if choices include a car and a blue bus. Suppose the odds ratio between the two is 1 : 1. Now if the option of a red bus is introduced, a person may be indifferent between a red and a blue bus, and hence may exhibit a car : blue bus : red bus odds ratio of 1 : 0.5 : 0.5, thus maintaining a 1 : 1 ratio of car : any bus while adopting a changed car : blue bus ratio of 1 : 0.5. Here the red bus option was not in fact irrelevant, because a red bus was a <a href="perfect_substitute" title="wikilink">perfect substitute</a> for a blue bus.</p>

<p>If the multinomial logit is used to model choices, it may in some situations impose too much constraint on the relative preferences between the different alternatives. This point is especially important to take into account if the analysis aims to predict how choices would change if one alternative was to disappear (for instance if one political candidate withdraws from a three candidate race). Other models like the <a href="nested_logit" title="wikilink">nested logit</a> or the <a href="multinomial_probit" title="wikilink">multinomial probit</a> may be used in such cases as they need not violate the IIA.<a class="footnoteRef" href="#fn6" id="fnref6"><sup>6</sup></a></p>
<h2 id="model">Model</h2>
<h3 id="introduction-1">Introduction</h3>

<p>There are multiple ways to describe the mathematical model underlying multinomial logistic regression, all of which are equivalent. This can make it difficult to compare different treatments of the subject in different texts. The article on <a href="logistic_regression" title="wikilink">logistic regression</a> presents a number of equivalent formulations of simple logistic regression, and many of these have equivalents in the multinomial logit model.</p>

<p>The idea behind all of them, as in many other <a href="statistical_classification" title="wikilink">statistical classification</a> techniques, is to construct a <a href="linear_predictor_function" title="wikilink">linear predictor function</a> that constructs a score from a set of weights that are <a href="linear_combination" title="wikilink">linearly combined</a> with the explanatory variables (features) of a given observation using a <a href="dot_product" title="wikilink">dot product</a>:</p>

<p>

<math display="block" id="Multinomial_logistic_regression:0">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mo>score</mo>
     <mrow>
      <mo stretchy="false">(</mo>
      <msub>
       <mi>ùêó</mi>
       <mi>i</mi>
      </msub>
      <mo>,</mo>
      <mi>k</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <msub>
      <mi>ùú∑</mi>
      <mi>k</mi>
     </msub>
     <mo>‚ãÖ</mo>
     <msub>
      <mi>ùêó</mi>
      <mi>i</mi>
     </msub>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>score</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>ùêó</ci>
      <ci>i</ci>
     </apply>
     <ci>k</ci>
    </apply>
    <apply>
     <ci>normal-‚ãÖ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>ùú∑</ci>
      <ci>k</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>ùêó</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{score}(\mathbf{X}_{i},k)=\boldsymbol{\beta}_{k}\cdot\mathbf{X}_{%
i},
  </annotation>
 </semantics>
</math>

</p>

<p>where <strong>X</strong><sub><em>i</em></sub> is the vector of explanatory variables describing observation <em>i</em>, <strong>Œ≤</strong><sub><em>k</em></sub> is a vector of weights (or <a href="regression_coefficient" title="wikilink">regression coefficients</a>) corresponding to outcome <em>k</em>, and score(<strong>X</strong><sub><em>i</em></sub>, <em>k</em>) is the score associated with assigning observation <em>i</em> to category <em>k</em>. In <a href="discrete_choice" title="wikilink">discrete choice</a> theory, where observations represent people and outcomes represent choices, the score is considered the <a class="uri" href="utility" title="wikilink">utility</a> associated with person <em>i</em> choosing outcome <em>k</em>. The predicted outcome is the one with the highest score.</p>

<p>The difference between the multinomial logit model and numerous other methods, models, algorithms, etc. with the same basic setup (the <a class="uri" href="perceptron" title="wikilink">perceptron</a> algorithm, <a href="support_vector_machine" title="wikilink">support vector machines</a>, <a href="linear_discriminant_analysis" title="wikilink">linear discriminant analysis</a>, etc.) is the procedure for determining (training) the optimal weights/coefficients and the way that the score is interpreted. In particular, in the multinomial logit model, the score can directly be converted to a probability value, indicating the <a class="uri" href="probability" title="wikilink">probability</a> of observation <em>i</em> choosing outcome <em>k</em> given the measured characteristics of the observation. This provides a principled way of incorporating the prediction of a particular multinomial logit model into a larger procedure that may involve multiple such predictions, each with a possibility of error. Without such means of combining predictions, errors tend to multiply. For example, imagine a large predictive model that is broken down into a series of submodels where the prediction of a given submodel is used as the input of another submodel, and that prediction is in turn used as the input into a third submodel, etc. If each submodel has 90% accuracy in its predictions, and there are five submodels in series, then the overall model has only .9<sup>5</sup> = 59% accuracy. If each submodel has 80% accuracy, then overall accuracy drops to .8<sup>5</sup> = 33% accuracy. This issue is known as <a href="error_propagation" title="wikilink">error propagation</a> and is a serious problem in real-world predictive models, which are usually composed of numerous parts. Predicting probabilities of each possible outcome, rather than simply making a single optimal prediction, is one means of alleviating this issue.</p>
<h3 id="setup">Setup</h3>

<p>The basic setup is the same as in <a href="logistic_regression" title="wikilink">logistic regression</a>, the only difference being that the <a href="dependent_variable" title="wikilink">dependent variables</a> are <a href="categorical_variable" title="wikilink">categorical</a> rather than <a href="binary_variable" title="wikilink">binary</a>, i.e. there are <em>K</em> possible outcomes rather than just two. The following description is somewhat shortened; for more details, consult the <a href="logistic_regression" title="wikilink">logistic regression</a> article.</p>
<h4 id="data-points">Data points</h4>

<p>Specifically, it is assumed that we have a series of <em>N</em> observed data points. Each data point <em>i</em> (ranging from <em>1</em> to <em>N</em>) consists of a set of <em>M</em> explanatory variables <em>x</em><sub><em>1,i</em></sub> ... <em>x</em><sub><em>M,i</em></sub> (aka <a href="independent_variable" title="wikilink">independent variables</a>, predictor variables, features, etc.), and an associated <a href="categorical_variable" title="wikilink">categorical</a> outcome <em>Y</em><sub><em>i</em></sub> (aka <a href="dependent_variable" title="wikilink">dependent variable</a>, response variable), which can take on one of <em>K</em> possible values. These possible values represent logically separate categories (e.g. different political parties, blood types, etc.), and are often described mathematically by arbitrarily assigning each a number from 1 to <em>K</em>. The explanatory variables and outcome represent observed properties of the data points, and are often thought of as originating in the observations of <em>N</em> "experiments" ‚Äî although an "experiment" may consist in nothing more than gathering data. The goal of multinomial logistic regression is to construct a model that explains the relationship between the explanatory variables and the outcome, so that the outcome of a new "experiment" can be correctly predicted for a new data point for which the explanatory variables, but not the outcome, are available. In the process, the model attempts to explain the relative effect of differing explanatory variables on the outcome.</p>

<p>Some examples:</p>
<ul>
<li>The observed outcomes are different variants of a disease such as <a class="uri" href="hepatitis" title="wikilink">hepatitis</a> (possibly including "no disease" and/or other related diseases) in a set of patients, and the explanatory variables might be characteristics of the patients thought to be pertinent (sex, race, age, <a href="blood_pressure" title="wikilink">blood pressure</a>, outcomes of various liver-function tests, etc.). The goal is then to predict which disease is causing the observed liver-related symptoms in a new patient.</li>
<li>The observed outcomes are the party chosen by a set of people in an election, and the explanatory variables are the demographic characteristics of each person (e.g. sex, race, age, income, etc.). The goal is then to predict the likely vote of a new voter with given characteristics.</li>
</ul>
<h4 id="linear-predictor">Linear predictor</h4>

<p>As in other forms of linear regression, multinomial logistic regression uses a <a href="linear_predictor_function" title="wikilink">linear predictor function</a> 

<math display="inline" id="Multinomial_logistic_regression:1">
 <semantics>
  <mrow>
   <mi>f</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>k</mi>
    <mo>,</mo>
    <mi>i</mi>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <times></times>
    <ci>f</ci>
    <interval closure="open">
     <ci>k</ci>
     <ci>i</ci>
    </interval>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(k,i)
  </annotation>
 </semantics>
</math>

 to predict the probability that observation <em>i</em> has outcome <em>k</em>, of the following form:</p>

<p>

<math display="block" id="Multinomial_logistic_regression:2">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>k</mi>
      <mo>,</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <msub>
      <mi>Œ≤</mi>
      <mrow>
       <mn>0</mn>
       <mo>,</mo>
       <mi>k</mi>
      </mrow>
     </msub>
     <mo>+</mo>
     <mrow>
      <msub>
       <mi>Œ≤</mi>
       <mrow>
        <mn>1</mn>
        <mo>,</mo>
        <mi>k</mi>
       </mrow>
      </msub>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>1</mn>
        <mo>,</mo>
        <mi>i</mi>
       </mrow>
      </msub>
     </mrow>
     <mo>+</mo>
     <mrow>
      <msub>
       <mi>Œ≤</mi>
       <mrow>
        <mn>2</mn>
        <mo>,</mo>
        <mi>k</mi>
       </mrow>
      </msub>
      <msub>
       <mi>x</mi>
       <mrow>
        <mn>2</mn>
        <mo>,</mo>
        <mi>i</mi>
       </mrow>
      </msub>
     </mrow>
     <mo>+</mo>
     <mi mathvariant="normal">‚ãØ</mi>
     <mo>+</mo>
     <mrow>
      <msub>
       <mi>Œ≤</mi>
       <mrow>
        <mi>M</mi>
        <mo>,</mo>
        <mi>k</mi>
       </mrow>
      </msub>
      <msub>
       <mi>x</mi>
       <mrow>
        <mi>M</mi>
        <mo>,</mo>
        <mi>i</mi>
       </mrow>
      </msub>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <interval closure="open">
      <ci>k</ci>
      <ci>i</ci>
     </interval>
    </apply>
    <apply>
     <plus></plus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Œ≤</ci>
      <list>
       <cn type="integer">0</cn>
       <ci>k</ci>
      </list>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Œ≤</ci>
       <list>
        <cn type="integer">1</cn>
        <ci>k</ci>
       </list>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <list>
        <cn type="integer">1</cn>
        <ci>i</ci>
       </list>
      </apply>
     </apply>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Œ≤</ci>
       <list>
        <cn type="integer">2</cn>
        <ci>k</ci>
       </list>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <list>
        <cn type="integer">2</cn>
        <ci>i</ci>
       </list>
      </apply>
     </apply>
     <ci>normal-‚ãØ</ci>
     <apply>
      <times></times>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Œ≤</ci>
       <list>
        <ci>M</ci>
        <ci>k</ci>
       </list>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <list>
        <ci>M</ci>
        <ci>i</ci>
       </list>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(k,i)=\beta_{0,k}+\beta_{1,k}x_{1,i}+\beta_{2,k}x_{2,i}+\cdots+\beta_{M,k}x_{%
M,i},
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Multinomial_logistic_regression:3">
 <semantics>
  <msub>
   <mi>Œ≤</mi>
   <mrow>
    <mi>m</mi>
    <mo>,</mo>
    <mi>k</mi>
   </mrow>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>Œ≤</ci>
    <list>
     <ci>m</ci>
     <ci>k</ci>
    </list>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \beta_{m,k}
  </annotation>
 </semantics>
</math>


 is a <a href="regression_coefficient" title="wikilink">regression coefficient</a> associated with the <em>m</em>th explanatory variable and the <em>k</em>th outcome. As explained in the <a href="logistic_regression" title="wikilink">logistic regression</a> article, the regression coefficients and explanatory variables are normally grouped into vectors of size <em>M+1</em>, so that the predictor function can be written more compactly:</p>

<p>

<math display="block" id="Multinomial_logistic_regression:4">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>f</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mi>k</mi>
      <mo>,</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <msub>
      <mi>ùú∑</mi>
      <mi>k</mi>
     </msub>
     <mo>‚ãÖ</mo>
     <msub>
      <mi>ùê±</mi>
      <mi>i</mi>
     </msub>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <interval closure="open">
      <ci>k</ci>
      <ci>i</ci>
     </interval>
    </apply>
    <apply>
     <ci>normal-‚ãÖ</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>ùú∑</ci>
      <ci>k</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>ùê±</ci>
      <ci>i</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(k,i)=\boldsymbol{\beta}_{k}\cdot\mathbf{x}_{i},
  </annotation>
 </semantics>
</math>

</p>

<p>where 

<math display="inline" id="Multinomial_logistic_regression:5">
 <semantics>
  <msub>
   <mi>ùú∑</mi>
   <mi>k</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>ùú∑</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \boldsymbol{\beta}_{k}
  </annotation>
 </semantics>
</math>

 is the set of regression coefficients associated with outcome <em>k</em>, and 

<math display="inline" id="Multinomial_logistic_regression:6">
 <semantics>
  <msub>
   <mi>ùê±</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>ùê±</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \mathbf{x}_{i}
  </annotation>
 </semantics>
</math>

 (a row vector) is the set of explanatory variables associated with observation <em>i</em>.</p>
<h3 id="as-a-set-of-independent-binary-regressions">As a set of independent binary regressions</h3>

<p>One fairly simple way to arrive at the multinomial logit model is to imagine, for <em>K</em> possible outcomes, running <em>K</em>-1 independent binary logistic regression models, in which one outcome is chosen as a "pivot" and then the other <em>K</em>-1 outcomes are separately regressed against the pivot outcome. This would proceed as follows, if outcome <em>K</em> (the last outcome) is chosen as the pivot:</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:7">
 <semantics>
  <mrow>
   <mi>ln</mi>
   <mstyle displaystyle="true">
    <mfrac>
     <mrow>
      <mi>Pr</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <msub>
         <mi>Y</mi>
         <mi>i</mi>
        </msub>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
     <mrow>
      <mi>Pr</mi>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <msub>
         <mi>Y</mi>
         <mi>i</mi>
        </msub>
        <mo>=</mo>
        <mi>K</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
     </mrow>
    </mfrac>
   </mstyle>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ln></ln>
    <apply>
     <divide></divide>
     <apply>
      <ci>Pr</ci>
      <apply>
       <eq></eq>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Y</ci>
        <ci>i</ci>
       </apply>
       <cn type="integer">1</cn>
      </apply>
     </apply>
     <apply>
      <ci>Pr</ci>
      <apply>
       <eq></eq>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Y</ci>
        <ci>i</ci>
       </apply>
       <ci>K</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\ln\frac{\Pr(Y_{i}=1)}{\Pr(Y_{i}=K)}
  </annotation>
 </semantics>
</math>


</p>

<p>Note that we have introduced separate sets of regression coefficients, one for each possible outcome.</p>

<p>If we exponentiate both sides, and solve for the probabilities, we get:</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:8">
 <semantics>
  <mrow>
   <mi>Pr</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <msub>
      <mi>Y</mi>
      <mi>i</mi>
     </msub>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>Pr</ci>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <ci>i</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\Pr(Y_{i}=1)
  </annotation>
 </semantics>
</math>


</p>

<p>Using the fact that all <em>K</em> of the probabilities must sum to one, we find:</p>

<p>

<math display="block" id="Multinomial_logistic_regression:9">
 <semantics>
  <mrow>
   <mrow>
    <mi>Pr</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>Y</mi>
       <mi>i</mi>
      </msub>
      <mo>=</mo>
      <mi>K</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <mn>1</mn>
    <mrow>
     <mn>1</mn>
     <mo>+</mo>
     <mrow>
      <msubsup>
       <mo largeop="true" symmetric="true">‚àë</mo>
       <mrow>
        <mi>k</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mrow>
        <mi>K</mi>
        <mo>-</mo>
        <mn>1</mn>
       </mrow>
      </msubsup>
      <msup>
       <mi>e</mi>
       <mrow>
        <msub>
         <mi>ùú∑</mi>
         <mi>k</mi>
        </msub>
        <mo>‚ãÖ</mo>
        <msub>
         <mi>ùêó</mi>
         <mi>i</mi>
        </msub>
       </mrow>
      </msup>
     </mrow>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>Pr</ci>
     <apply>
      <eq></eq>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Y</ci>
       <ci>i</ci>
      </apply>
      <ci>K</ci>
     </apply>
    </apply>
    <apply>
     <divide></divide>
     <cn type="integer">1</cn>
     <apply>
      <plus></plus>
      <cn type="integer">1</cn>
      <apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <sum></sum>
         <apply>
          <eq></eq>
          <ci>k</ci>
          <cn type="integer">1</cn>
         </apply>
        </apply>
        <apply>
         <minus></minus>
         <ci>K</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <apply>
        <csymbol cd="ambiguous">superscript</csymbol>
        <ci>e</ci>
        <apply>
         <ci>normal-‚ãÖ</ci>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>ùú∑</ci>
          <ci>k</ci>
         </apply>
         <apply>
          <csymbol cd="ambiguous">subscript</csymbol>
          <ci>ùêó</ci>
          <ci>i</ci>
         </apply>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Pr(Y_{i}=K)=\frac{1}{1+\sum_{k=1}^{K-1}e^{\boldsymbol{\beta}_{k}\cdot\mathbf{%
X}_{i}}}
  </annotation>
 </semantics>
</math>

</p>

<p>We can use this to find the other probabilities:</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:10">
 <semantics>
  <mrow>
   <mi>Pr</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <msub>
      <mi>Y</mi>
      <mi>i</mi>
     </msub>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>Pr</ci>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <ci>i</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\Pr(Y_{i}=1)
  </annotation>
 </semantics>
</math>


</p>

<p>The fact that we run multiple regressions reveals why the model relies on the assumption of <a href="independence_of_irrelevant_alternatives" title="wikilink">independence of irrelevant alternatives</a> described above.</p>
<h3 id="estimating-the-coefficients">Estimating the coefficients</h3>

<p>The unknown parameters in each vector <em>Œ≤<sub>k</sub></em> are typically jointly estimated by <a href="maximum_a_posteriori" title="wikilink">maximum a posteriori</a> (MAP) estimation, which is an extension of <a href="maximum_likelihood" title="wikilink">maximum likelihood</a> using <a href="regularization_(mathematics)" title="wikilink">regularization</a> of the weights to prevent pathological solutions (usually a squared regularizing function, which is equivalent to placing a zero-mean <a href="Gaussian_distribution" title="wikilink">Gaussian</a> <a href="prior_distribution" title="wikilink">prior distribution</a> on the weights, but other distributions are also possible). The solution is typically found using an iterative procedure such as <a href="generalized_iterative_scaling" title="wikilink">generalized iterative scaling</a>,<a class="footnoteRef" href="#fn7" id="fnref7"><sup>7</sup></a> <a href="iteratively_reweighted_least_squares" title="wikilink">iteratively reweighted least squares</a> (IRLS),<a class="footnoteRef" href="#fn8" id="fnref8"><sup>8</sup></a> by means of <a href="gradient-based_optimization" title="wikilink">gradient-based optimization</a> algorithms such as <a class="uri" href="L-BFGS" title="wikilink">L-BFGS</a>,<a class="footnoteRef" href="#fn9" id="fnref9"><sup>9</sup></a> or by specialized <a href="coordinate_descent" title="wikilink">coordinate descent</a> algorithms.<a class="footnoteRef" href="#fn10" id="fnref10"><sup>10</sup></a></p>
<h3 id="as-a-log-linear-model">As a log-linear model</h3>

<p>The formulation of binary logistic regression as a <a href="Logistic_regression#log-linear_model" title="wikilink">log-linear model</a> can be directly extended to multi-way regression. That is, we model the <a class="uri" href="logarithm" title="wikilink">logarithm</a> of the probability of seeing a given output using the linear predictor as well as an additional <a href="normalization_factor" title="wikilink">normalization factor</a>:</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:11">
 <semantics>
  <mrow>
   <mi>ln</mi>
   <mrow>
    <mi>Pr</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>Y</mi>
       <mi>i</mi>
      </msub>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ln></ln>
    <apply>
     <ci>Pr</ci>
     <apply>
      <eq></eq>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Y</ci>
       <ci>i</ci>
      </apply>
      <cn type="integer">1</cn>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\ln\Pr(Y_{i}=1)
  </annotation>
 </semantics>
</math>


</p>

<p>As in the binary case, we need an extra term 

<math display="inline" id="Multinomial_logistic_regression:12">
 <semantics>
  <mrow>
   <mo>-</mo>
   <mrow>
    <mi>ln</mi>
    <mi>Z</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <apply>
     <ln></ln>
     <ci>Z</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   -\ln Z
  </annotation>
 </semantics>
</math>

 to ensure that the whole set of probabilities forms a <a href="probability_distribution" title="wikilink">probability distribution</a>, i.e. so that they all sum to one:</p>

<p>

<math display="block" id="Multinomial_logistic_regression:13">
 <semantics>
  <mrow>
   <mrow>
    <munderover>
     <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
     <mrow>
      <mi>k</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>K</mi>
    </munderover>
    <mrow>
     <mi>Pr</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <msub>
        <mi>Y</mi>
        <mi>i</mi>
       </msub>
       <mo>=</mo>
       <mi>k</mi>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>k</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>K</ci>
     </apply>
     <apply>
      <ci>Pr</ci>
      <apply>
       <eq></eq>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Y</ci>
        <ci>i</ci>
       </apply>
       <ci>k</ci>
      </apply>
     </apply>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \sum_{k=1}^{K}\Pr(Y_{i}=k)=1
  </annotation>
 </semantics>
</math>

</p>

<p>The reason why we need to add a term to ensure normalization, rather than multiply as is usual, is because we have taken the logarithm of the probabilities. Exponentiating both sides turns the additive term into a multiplicative factor, and in the process shows why we wrote the term in the form 

<math display="inline" id="Multinomial_logistic_regression:14">
 <semantics>
  <mrow>
   <mo>-</mo>
   <mrow>
    <mi>ln</mi>
    <mi>Z</mi>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <apply>
     <ln></ln>
     <ci>Z</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   -\ln Z
  </annotation>
 </semantics>
</math>


 rather than simply 

<math display="inline" id="Multinomial_logistic_regression:15">
 <semantics>
  <mrow>
   <mo>+</mo>
   <mi>Z</mi>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <plus></plus>
    <ci>Z</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   +Z
  </annotation>
 </semantics>
</math>

:</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:16">
 <semantics>
  <mrow>
   <mi>Pr</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <msub>
      <mi>Y</mi>
      <mi>i</mi>
     </msub>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>Pr</ci>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <ci>i</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\Pr(Y_{i}=1)
  </annotation>
 </semantics>
</math>


</p>

<p>We can compute the value of <em>Z</em> by applying the above constraint that requires all probabilities to sum to 1:</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:17">
 <semantics>
  <mrow>
   <mn>1</mn>
   <mo>=</mo>
   <mrow>
    <mstyle displaystyle="true">
     <munderover>
      <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
      <mrow>
       <mi>k</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>K</mi>
     </munderover>
    </mstyle>
    <mrow>
     <mi>Pr</mi>
     <mrow>
      <mo stretchy="false">(</mo>
      <mrow>
       <msub>
        <mi>Y</mi>
        <mi>i</mi>
       </msub>
       <mo>=</mo>
       <mi>k</mi>
      </mrow>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <cn type="integer">1</cn>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>k</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>K</ci>
     </apply>
     <apply>
      <ci>Pr</ci>
      <apply>
       <eq></eq>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>Y</ci>
        <ci>i</ci>
       </apply>
       <ci>k</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle 1=\sum_{k=1}^{K}\Pr(Y_{i}=k)
  </annotation>
 </semantics>
</math>


</p>

<p>Therefore:</p>

<p>

<math display="block" id="Multinomial_logistic_regression:18">
 <semantics>
  <mrow>
   <mi>Z</mi>
   <mo>=</mo>
   <mrow>
    <munderover>
     <mo largeop="true" movablelimits="false" symmetric="true">‚àë</mo>
     <mrow>
      <mi>k</mi>
      <mo>=</mo>
      <mn>1</mn>
     </mrow>
     <mi>K</mi>
    </munderover>
    <msup>
     <mi>e</mi>
     <mrow>
      <msub>
       <mi>ùú∑</mi>
       <mi>k</mi>
      </msub>
      <mo>‚ãÖ</mo>
      <msub>
       <mi>ùêó</mi>
       <mi>i</mi>
      </msub>
     </mrow>
    </msup>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>Z</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>k</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>K</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>e</ci>
      <apply>
       <ci>normal-‚ãÖ</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>ùú∑</ci>
        <ci>k</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>ùêó</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Z=\sum_{k=1}^{K}e^{\boldsymbol{\beta}_{k}\cdot\mathbf{X}_{i}}
  </annotation>
 </semantics>
</math>

</p>

<p>Note that this factor is "constant" in the sense that it is not a function of <em>Y</em><sub><em>i</em></sub>, which is the variable over which the probability distribution is defined. However, it is definitely not constant with respect to the explanatory variables, or crucially, with respect to the unknown regression coefficients <strong><em>Œ≤</em></strong><sub><em>k</em></sub>, which we will need to determine through some sort of <a href="mathematical_optimization" title="wikilink">optimization</a> procedure.</p>

<p>The resulting equations for the probabilities are</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:19">
 <semantics>
  <mrow>
   <mi>Pr</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <msub>
      <mi>Y</mi>
      <mi>i</mi>
     </msub>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>Pr</ci>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <ci>i</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\Pr(Y_{i}=1)
  </annotation>
 </semantics>
</math>


</p>

<p>Or generally:</p>

<p>

<math display="block" id="Multinomial_logistic_regression:20">
 <semantics>
  <mrow>
   <mrow>
    <mi>Pr</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>Y</mi>
       <mi>i</mi>
      </msub>
      <mo>=</mo>
      <mi>c</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <msup>
     <mi>e</mi>
     <mrow>
      <msub>
       <mi>ùú∑</mi>
       <mi>c</mi>
      </msub>
      <mo>‚ãÖ</mo>
      <msub>
       <mi>ùêó</mi>
       <mi>i</mi>
      </msub>
     </mrow>
    </msup>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">‚àë</mo>
      <mrow>
       <mi>k</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>K</mi>
     </msubsup>
     <msup>
      <mi>e</mi>
      <mrow>
       <msub>
        <mi>ùú∑</mi>
        <mi>k</mi>
       </msub>
       <mo>‚ãÖ</mo>
       <msub>
        <mi>ùêó</mi>
        <mi>i</mi>
       </msub>
      </mrow>
     </msup>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>Pr</ci>
     <apply>
      <eq></eq>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Y</ci>
       <ci>i</ci>
      </apply>
      <ci>c</ci>
     </apply>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>e</ci>
      <apply>
       <ci>normal-‚ãÖ</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>ùú∑</ci>
        <ci>c</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>ùêó</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>k</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>K</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>e</ci>
       <apply>
        <ci>normal-‚ãÖ</ci>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>ùú∑</ci>
         <ci>k</ci>
        </apply>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>ùêó</ci>
         <ci>i</ci>
        </apply>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Pr(Y_{i}=c)=\frac{e^{\boldsymbol{\beta}_{c}\cdot\mathbf{X}_{i}}}{\sum_{k=1}^{%
K}e^{\boldsymbol{\beta}_{k}\cdot\mathbf{X}_{i}}}
  </annotation>
 </semantics>
</math>

</p>

<p>The following function:</p>

<p>

<math display="block" id="Multinomial_logistic_regression:21">
 <semantics>
  <mrow>
   <mrow>
    <mo>softmax</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>k</mi>
     <mo>,</mo>
     <msub>
      <mi>x</mi>
      <mn>1</mn>
     </msub>
     <mo>,</mo>
     <mi mathvariant="normal">‚Ä¶</mi>
     <mo>,</mo>
     <msub>
      <mi>x</mi>
      <mi>n</mi>
     </msub>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mfrac>
    <msup>
     <mi>e</mi>
     <msub>
      <mi>x</mi>
      <mi>k</mi>
     </msub>
    </msup>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">‚àë</mo>
      <mrow>
       <mi>i</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>n</mi>
     </msubsup>
     <msup>
      <mi>e</mi>
      <msub>
       <mi>x</mi>
       <mi>i</mi>
      </msub>
     </msup>
    </mrow>
   </mfrac>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>softmax</ci>
     <ci>k</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>normal-‚Ä¶</ci>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>x</ci>
      <ci>n</ci>
     </apply>
    </apply>
    <apply>
     <divide></divide>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>e</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>x</ci>
       <ci>k</ci>
      </apply>
     </apply>
     <apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <sum></sum>
        <apply>
         <eq></eq>
         <ci>i</ci>
         <cn type="integer">1</cn>
        </apply>
       </apply>
       <ci>n</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">superscript</csymbol>
       <ci>e</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{softmax}(k,x_{1},\ldots,x_{n})=\frac{e^{x_{k}}}{\sum_{i=1}^{n}e^%
{x_{i}}}
  </annotation>
 </semantics>
</math>

</p>

<p>is referred to as the <a href="softmax_function" title="wikilink">softmax function</a>. The reason is that the effect of exponentiating the values 

<math display="inline" id="Multinomial_logistic_regression:22">
 <semantics>
  <mrow>
   <msub>
    <mi>x</mi>
    <mn>1</mn>
   </msub>
   <mo>,</mo>
   <mi mathvariant="normal">‚Ä¶</mi>
   <mo>,</mo>
   <msub>
    <mi>x</mi>
    <mi>n</mi>
   </msub>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <list>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-‚Ä¶</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>n</ci>
    </apply>
   </list>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{1},\ldots,x_{n}
  </annotation>
 </semantics>
</math>

 is to exaggerate the differences between them. As a result, 

<math display="inline" id="Multinomial_logistic_regression:23">
 <semantics>
  <mrow>
   <mo>softmax</mo>
   <mrow>
    <mo stretchy="false">(</mo>
    <mi>k</mi>
    <mo>,</mo>
    <msub>
     <mi>x</mi>
     <mn>1</mn>
    </msub>
    <mo>,</mo>
    <mi mathvariant="normal">‚Ä¶</mi>
    <mo>,</mo>
    <msub>
     <mi>x</mi>
     <mi>n</mi>
    </msub>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>softmax</ci>
    <ci>k</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <cn type="integer">1</cn>
    </apply>
    <ci>normal-‚Ä¶</ci>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>x</ci>
     <ci>n</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \operatorname{softmax}(k,x_{1},\ldots,x_{n})
  </annotation>
 </semantics>
</math>

 will return a value close to 0 whenever <em>

<math display="inline" id="Multinomial_logistic_regression:24">
 <semantics>
  <msub>
   <mi>x</mi>
   <mi>k</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>x</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   x_{k}
  </annotation>
 </semantics>
</math>

</em> is significantly less than the maximum of all the values, and will return a value close to 1 when applied to the maximum value, unless it is extremely close to the next-largest value. Thus, the softmax function can be used to construct a <a href="weighted_average" title="wikilink">weighted average</a> that behaves as a <a href="smooth_function" title="wikilink">smooth function</a> (which can be conveniently <a href="differentiation_(mathematics)" title="wikilink">differentiated</a>, etc.) and which approximates the <a href="indicator_function" title="wikilink">indicator function</a></p>

<p>

<math display="block" id="Multinomial_logistic_regression:25">
 <semantics>
  <mrow>
   <mrow>
    <mi>f</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>k</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>{</mo>
    <mtable displaystyle="true">
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mrow>
          <mpadded width="+2.8pt">
           <mn>1</mn>
          </mpadded>
          <mpadded width="+2.8pt">
           <mtext>if</mtext>
          </mpadded>
          <mi>k</mi>
         </mrow>
         <mo>=</mo>
         <mrow>
          <mrow>
           <mi>arg</mi>
           <mi>max</mi>
          </mrow>
          <mrow>
           <mo stretchy="false">(</mo>
           <msub>
            <mi>x</mi>
            <mn>1</mn>
           </msub>
           <mo>,</mo>
           <mi mathvariant="normal">‚Ä¶</mi>
           <mo>,</mo>
           <msub>
            <mi>x</mi>
            <mi>n</mi>
           </msub>
           <mo stretchy="false">)</mo>
          </mrow>
         </mrow>
        </mrow>
        <mo>,</mo>
       </mrow>
      </mtd>
      <mtd></mtd>
     </mtr>
     <mtr>
      <mtd columnalign="left">
       <mrow>
        <mrow>
         <mpadded width="+2.8pt">
          <mn>0</mn>
         </mpadded>
         <mtext>otherwise</mtext>
        </mrow>
        <mo>.</mo>
       </mrow>
      </mtd>
      <mtd></mtd>
     </mtr>
    </mtable>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <times></times>
     <ci>f</ci>
     <ci>k</ci>
    </apply>
    <apply>
     <csymbol cd="latexml">cases</csymbol>
     <apply>
      <eq></eq>
      <apply>
       <times></times>
       <cn type="integer">1</cn>
       <mtext>if</mtext>
       <ci>k</ci>
      </apply>
      <apply>
       <apply>
        <arg></arg>
        <max></max>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <cn type="integer">1</cn>
       </apply>
       <ci>normal-‚Ä¶</ci>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>x</ci>
        <ci>n</ci>
       </apply>
      </apply>
     </apply>
     <mtext>otherwise</mtext>
     <apply>
      <times></times>
      <cn type="integer">0</cn>
      <mtext>otherwise</mtext>
     </apply>
     <mtext>otherwise</mtext>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   f(k)=\begin{cases}1\;\textrm{ if }\;k=\operatorname{\arg\max}(x_{1},\ldots,x_{%
n}),\\
0\;\textrm{ otherwise}.\end{cases}
  </annotation>
 </semantics>
</math>

</p>

<p>Thus, we can write the probability equations as</p>

<p>

<math display="block" id="Multinomial_logistic_regression:26">
 <semantics>
  <mrow>
   <mrow>
    <mi>Pr</mi>
    <mrow>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>Y</mi>
       <mi>i</mi>
      </msub>
      <mo>=</mo>
      <mi>c</mi>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
   <mo>=</mo>
   <mrow>
    <mo>softmax</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>c</mi>
     <mo>,</mo>
     <mrow>
      <msub>
       <mi>ùú∑</mi>
       <mn>1</mn>
      </msub>
      <mo>‚ãÖ</mo>
      <msub>
       <mi>ùêó</mi>
       <mi>i</mi>
      </msub>
     </mrow>
     <mo>,</mo>
     <mi mathvariant="normal">‚Ä¶</mi>
     <mo>,</mo>
     <mrow>
      <msub>
       <mi>ùú∑</mi>
       <mi>K</mi>
      </msub>
      <mo>‚ãÖ</mo>
      <msub>
       <mi>ùêó</mi>
       <mi>i</mi>
      </msub>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <apply>
     <ci>Pr</ci>
     <apply>
      <eq></eq>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>Y</ci>
       <ci>i</ci>
      </apply>
      <ci>c</ci>
     </apply>
    </apply>
    <apply>
     <ci>softmax</ci>
     <ci>c</ci>
     <apply>
      <ci>normal-‚ãÖ</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ùú∑</ci>
       <cn type="integer">1</cn>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ùêó</ci>
       <ci>i</ci>
      </apply>
     </apply>
     <ci>normal-‚Ä¶</ci>
     <apply>
      <ci>normal-‚ãÖ</ci>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ùú∑</ci>
       <ci>K</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ùêó</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \Pr(Y_{i}=c)=\operatorname{softmax}(c,\boldsymbol{\beta}_{1}\cdot\mathbf{X}_{i%
},\ldots,\boldsymbol{\beta}_{K}\cdot\mathbf{X}_{i})
  </annotation>
 </semantics>
</math>

</p>

<p>The softmax function thus serves as the equivalent of the <a href="logistic_function" title="wikilink">logistic function</a> in binary logistic regression.</p>

<p>Note that not all of the 

<math display="inline" id="Multinomial_logistic_regression:27">
 <semantics>
  <msub>
   <mi>Œ≤</mi>
   <mi>k</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>Œ≤</ci>
    <ci>k</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \beta_{k}
  </annotation>
 </semantics>
</math>

 vectors of coefficients are uniquely <a href="identifiability" title="wikilink">identifiable</a>. This is due to the fact that all probabilities must sum to 1, making one of them completely determined once all the rest are known. As a result there are only 

<math display="inline" id="Multinomial_logistic_regression:28">
 <semantics>
  <mrow>
   <mi>k</mi>
   <mo>-</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <ci>k</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k-1
  </annotation>
 </semantics>
</math>

 separately specifiable probabilities, and hence 

<math display="inline" id="Multinomial_logistic_regression:29">
 <semantics>
  <mrow>
   <mi>k</mi>
   <mo>-</mo>
   <mn>1</mn>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <minus></minus>
    <ci>k</ci>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   k-1
  </annotation>
 </semantics>
</math>

 separately identifiable vectors of coefficients. One way to see this is to note that if we add a constant vector to all of the coefficient vectors, the equations are identical:</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:30">
 <semantics>
  <mstyle displaystyle="true">
   <mfrac>
    <msup>
     <mi>e</mi>
     <mrow>
      <mrow>
       <mo stretchy="false">(</mo>
       <mrow>
        <msub>
         <mi>ùú∑</mi>
         <mi>c</mi>
        </msub>
        <mo>+</mo>
        <mi>C</mi>
       </mrow>
       <mo stretchy="false">)</mo>
      </mrow>
      <mo>‚ãÖ</mo>
      <msub>
       <mi>ùêó</mi>
       <mi>i</mi>
      </msub>
     </mrow>
    </msup>
    <mrow>
     <msubsup>
      <mo largeop="true" symmetric="true">‚àë</mo>
      <mrow>
       <mi>k</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <mi>K</mi>
     </msubsup>
     <msup>
      <mi>e</mi>
      <mrow>
       <mrow>
        <mo stretchy="false">(</mo>
        <mrow>
         <msub>
          <mi>ùú∑</mi>
          <mi>k</mi>
         </msub>
         <mo>+</mo>
         <mi>C</mi>
        </mrow>
        <mo stretchy="false">)</mo>
       </mrow>
       <mo>‚ãÖ</mo>
       <msub>
        <mi>ùêó</mi>
        <mi>i</mi>
       </msub>
      </mrow>
     </msup>
    </mrow>
   </mfrac>
  </mstyle>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <divide></divide>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>e</ci>
     <apply>
      <ci>normal-‚ãÖ</ci>
      <apply>
       <plus></plus>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>ùú∑</ci>
        <ci>c</ci>
       </apply>
       <ci>C</ci>
      </apply>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <ci>ùêó</ci>
       <ci>i</ci>
      </apply>
     </apply>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <apply>
       <csymbol cd="ambiguous">subscript</csymbol>
       <sum></sum>
       <apply>
        <eq></eq>
        <ci>k</ci>
        <cn type="integer">1</cn>
       </apply>
      </apply>
      <ci>K</ci>
     </apply>
     <apply>
      <csymbol cd="ambiguous">superscript</csymbol>
      <ci>e</ci>
      <apply>
       <ci>normal-‚ãÖ</ci>
       <apply>
        <plus></plus>
        <apply>
         <csymbol cd="ambiguous">subscript</csymbol>
         <ci>ùú∑</ci>
         <ci>k</ci>
        </apply>
        <ci>C</ci>
       </apply>
       <apply>
        <csymbol cd="ambiguous">subscript</csymbol>
        <ci>ùêó</ci>
        <ci>i</ci>
       </apply>
      </apply>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\frac{e^{(\boldsymbol{\beta}_{c}+C)\cdot\mathbf{X}_{i}}}{\sum_{k=%
1}^{K}e^{(\boldsymbol{\beta}_{k}+C)\cdot\mathbf{X}_{i}}}
  </annotation>
 </semantics>
</math>


</p>

<p>As a result, it is conventional to set 

<math display="inline" id="Multinomial_logistic_regression:31">
 <semantics>
  <mrow>
   <mi>C</mi>
   <mo>=</mo>
   <mrow>
    <mo>-</mo>
    <msub>
     <mi>ùú∑</mi>
     <mi>K</mi>
    </msub>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <eq></eq>
    <ci>C</ci>
    <apply>
     <minus></minus>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>ùú∑</ci>
      <ci>K</ci>
     </apply>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   C=-\boldsymbol{\beta}_{K}
  </annotation>
 </semantics>
</math>

 (or alternatively, one of the other coefficient vectors). Essentially, we set the constant so that one of the vectors becomes 0, and all of the other vectors get transformed into the difference between those vectors and the vector we chose. This is equivalent to "pivoting" around one of the <em>K</em> choices, and examining how much better or worse all of the other <em>K</em>-1 choices are, relative to the choice are pivoting around. Mathematically, we transform the coefficients as follows:</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:32">
 <semantics>
  <msubsup>
   <mi>ùú∑</mi>
   <mn>1</mn>
   <mo>‚Ä≤</mo>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">superscript</csymbol>
     <ci>ùú∑</ci>
     <ci>normal-‚Ä≤</ci>
    </apply>
    <cn type="integer">1</cn>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\boldsymbol{\beta}^{\prime}_{1}
  </annotation>
 </semantics>
</math>


</p>

<p>This leads to the following equations:</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:33">
 <semantics>
  <mrow>
   <mi>Pr</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <msub>
      <mi>Y</mi>
      <mi>i</mi>
     </msub>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>Pr</ci>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <ci>i</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\Pr(Y_{i}=1)
  </annotation>
 </semantics>
</math>


</p>

<p>Other than the prime symbols on the regression coefficients, this is exactly the same as the form of the model described above, in terms of <em>K</em>-1 independent two-way regressions.</p>
<h3 id="as-a-latent-variable-model">As a latent-variable model</h3>

<p>It is also possible to formulate multinomial logistic regression as a latent variable model, following the <a href="Logistic_regression#As_a_two-way_latent-variable_model" title="wikilink">two-way latent variable model</a> described for binary logistic regression. This formulation is common in the theory of <a href="discrete_choice" title="wikilink">discrete choice</a> models, and makes it easier to compare multinomial logistic regression to the related <a href="multinomial_probit" title="wikilink">multinomial probit</a> model, as well as to extend it to more complex models.</p>

<p>Imagine that, for each data point <em>i</em> and possible outcome <em>k</em>, there is a continuous <a href="latent_variable" title="wikilink">latent variable</a> <em>Y</em><sub><em>i,k</em></sub><sup><em>*</em></sup> (i.e. an unobserved <a href="random_variable" title="wikilink">random variable</a>) that is distributed as follows:</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:34">
 <semantics>
  <msubsup>
   <mi>Y</mi>
   <mrow>
    <mi>i</mi>
    <mo>,</mo>
    <mn>1</mn>
   </mrow>
   <mo>‚àó</mo>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>Y</ci>
     <list>
      <ci>i</ci>
      <cn type="integer">1</cn>
     </list>
    </apply>
    <ci>normal-‚àó</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle Y_{i,1}^{\ast}
  </annotation>
 </semantics>
</math>


</p>

<p>where 

<math display="inline" id="Multinomial_logistic_regression:35">
 <semantics>
  <mrow>
   <mrow>
    <msub>
     <mi>Œµ</mi>
     <mi>k</mi>
    </msub>
    <mo>‚àº</mo>
    <mrow>
     <msub>
      <mo>EV</mo>
      <mn>1</mn>
     </msub>
     <mrow>
      <mo stretchy="false">(</mo>
      <mn>0</mn>
      <mo>,</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>,</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>Œµ</ci>
     <ci>k</ci>
    </apply>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>EV</ci>
      <cn type="integer">1</cn>
     </apply>
     <cn type="integer">0</cn>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \varepsilon_{k}\sim\operatorname{EV}_{1}(0,1),
  </annotation>
 </semantics>
</math>

 i.e. a standard type-1 <a href="extreme_value_distribution" title="wikilink">extreme value distribution</a>.</p>

<p>This latent variable can be thought of as the <a class="uri" href="utility" title="wikilink">utility</a> associated with data point <em>i</em> choosing outcome <em>k</em>, where there is some randomness in the actual amount of utility obtained, which accounts for other unmodeled factors that go into the choice. The value of the actual variable 

<math display="inline" id="Multinomial_logistic_regression:36">
 <semantics>
  <msub>
   <mi>Y</mi>
   <mi>i</mi>
  </msub>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">subscript</csymbol>
    <ci>Y</ci>
    <ci>i</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y_{i}
  </annotation>
 </semantics>
</math>

 is then determined in a non-random fashion from these latent variables (i.e. the randomness has been moved from the observed outcomes into the latent variables), where outcome <em>k</em> is chosen if and only if the associated utility (the value of 

<math display="inline" id="Multinomial_logistic_regression:37">
 <semantics>
  <msubsup>
   <mi>Y</mi>
   <mrow>
    <mi>i</mi>
    <mo>,</mo>
    <mi>k</mi>
   </mrow>
   <mo>‚àó</mo>
  </msubsup>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="ambiguous">superscript</csymbol>
    <apply>
     <csymbol cd="ambiguous">subscript</csymbol>
     <ci>Y</ci>
     <list>
      <ci>i</ci>
      <ci>k</ci>
     </list>
    </apply>
    <ci>normal-‚àó</ci>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y_{i,k}^{\ast}
  </annotation>
 </semantics>
</math>

) is greater than the utilities of all the other choices, i.e. if the utility associated with outcome <em>k</em> is the maximum of all the utilities. (Since the latent variables are <a href="continuous_variable" title="wikilink">continuous</a>, the probability of two having exactly the same value is 0, so we basically don't have to worry about that situation.) That is:</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:38">
 <semantics>
  <mrow>
   <mi>Pr</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <msub>
      <mi>Y</mi>
      <mi>i</mi>
     </msub>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>Pr</ci>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <ci>i</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\Pr(Y_{i}=1)
  </annotation>
 </semantics>
</math>


</p>

<p>Or equivalently:</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:39">
 <semantics>
  <mrow>
   <mi>Pr</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <msub>
      <mi>Y</mi>
      <mi>i</mi>
     </msub>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>Pr</ci>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <ci>i</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\Pr(Y_{i}=1)
  </annotation>
 </semantics>
</math>


</p>

<p>Let's look more closely at the first equation, which we can write as follows:</p>

<p>

<math display="inline" id="Multinomial_logistic_regression:40">
 <semantics>
  <mrow>
   <mi>Pr</mi>
   <mrow>
    <mo stretchy="false">(</mo>
    <mrow>
     <msub>
      <mi>Y</mi>
      <mi>i</mi>
     </msub>
     <mo>=</mo>
     <mn>1</mn>
    </mrow>
    <mo stretchy="false">)</mo>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <ci>Pr</ci>
    <apply>
     <eq></eq>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>Y</ci>
      <ci>i</ci>
     </apply>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   \displaystyle\Pr(Y_{i}=1)
  </annotation>
 </semantics>
</math>


</p>

<p>There are a few things to realize here:</p>
<ol>
<li>In general, if 

<math display="inline" id="Multinomial_logistic_regression:41">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>‚àº</mo>
   <mrow>
    <msub>
     <mo>EV</mo>
     <mn>1</mn>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>a</mi>
     <mo>,</mo>
     <mi>b</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <ci>X</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>EV</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>a</ci>
     <ci>b</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X\sim\operatorname{EV}_{1}(a,b)
  </annotation>
 </semantics>
</math>

 and 

<math display="inline" id="Multinomial_logistic_regression:42">
 <semantics>
  <mrow>
   <mi>Y</mi>
   <mo>‚àº</mo>
   <mrow>
    <msub>
     <mo>EV</mo>
     <mn>1</mn>
    </msub>
    <mrow>
     <mo stretchy="false">(</mo>
     <mi>a</mi>
     <mo>,</mo>
     <mi>b</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <ci>Y</ci>
    <apply>
     <apply>
      <csymbol cd="ambiguous">subscript</csymbol>
      <ci>EV</ci>
      <cn type="integer">1</cn>
     </apply>
     <ci>a</ci>
     <ci>b</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   Y\sim\operatorname{EV}_{1}(a,b)
  </annotation>
 </semantics>
</math>

 then 

<math display="inline" id="Multinomial_logistic_regression:43">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>X</mi>
     <mo>-</mo>
     <mi>Y</mi>
    </mrow>
    <mo>‚àº</mo>
    <mrow>
     <mo>Logistic</mo>
     <mrow>
      <mo stretchy="false">(</mo>
      <mn>0</mn>
      <mo>,</mo>
      <mi>b</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <apply>
     <minus></minus>
     <ci>X</ci>
     <ci>Y</ci>
    </apply>
    <apply>
     <ci>Logistic</ci>
     <cn type="integer">0</cn>
     <ci>b</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X-Y\sim\operatorname{Logistic}(0,b).
  </annotation>
 </semantics>
</math>

 That is, the difference of two <a href="independent_identically_distributed" title="wikilink">independent identically distributed</a> extreme-value-distributed variables follows the <a href="logistic_distribution" title="wikilink">logistic distribution</a>, where the first parameter is unimportant. This is understandable since the first parameter is a <a href="location_parameter" title="wikilink">location parameter</a>, i.e. it shifts the mean by a fixed amount, and if two values are both shifted by the same amount, their difference remains the same. This means that all of the relational statements underlying the probability of a given choice involve the logistic distribution, which makes the initial choice of the extreme-value distribution, which seemed rather arbitrary, somewhat more understandable.</li>
<li>The second parameter in an extreme-value or logistic distribution is a <a href="scale_parameter" title="wikilink">scale parameter</a>, such that if 

<math display="inline" id="Multinomial_logistic_regression:44">
 <semantics>
  <mrow>
   <mi>X</mi>
   <mo>‚àº</mo>
   <mrow>
    <mo>Logistic</mo>
    <mrow>
     <mo stretchy="false">(</mo>
     <mn>0</mn>
     <mo>,</mo>
     <mn>1</mn>
     <mo stretchy="false">)</mo>
    </mrow>
   </mrow>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <ci>X</ci>
    <apply>
     <ci>Logistic</ci>
     <cn type="integer">0</cn>
     <cn type="integer">1</cn>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   X\sim\operatorname{Logistic}(0,1)
  </annotation>
 </semantics>
</math>

 then 

<math display="inline" id="Multinomial_logistic_regression:45">
 <semantics>
  <mrow>
   <mrow>
    <mrow>
     <mi>b</mi>
     <mi>X</mi>
    </mrow>
    <mo>‚àº</mo>
    <mrow>
     <mo>Logistic</mo>
     <mrow>
      <mo stretchy="false">(</mo>
      <mn>0</mn>
      <mo>,</mo>
      <mi>b</mi>
      <mo stretchy="false">)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mo>.</mo>
  </mrow>
  <annotation-xml encoding="MathML-Content">
   <apply>
    <csymbol cd="latexml">similar-to</csymbol>
    <apply>
     <times></times>
     <ci>b</ci>
     <ci>X</ci>
    </apply>
    <apply>
     <ci>Logistic</ci>
     <cn type="integer">0</cn>
     <ci>b</ci>
    </apply>
   </apply>
  </annotation-xml>
  <annotation encoding="application/x-tex">
   bX\sim\operatorname{Logistic}(0,b).
  </annotation>
 </semantics>
</math>

 This means that the effect of using an error variable with an arbitrary scale parameter in place of scale 1 can be compensated simply by multiplying all regression vectors by the same scale. Together with the previous point, this shows that the use of a standard extreme-value distribution (location 0, scale 1) for the error variables entails no loss of generality over using an arbitrary extreme-value distribution. In fact, the model is <a class="uri" href="nonidentifiable" title="wikilink">nonidentifiable</a> (no single set of optimal coefficients) if the more general distribution is used.</li>
<li>Because only differences of vectors of regression coefficients are used, adding an arbitrary constant to all coefficient vectors has no effect on the model. This means that, just as in the log-linear model, only <em>K</em>-1 of the coefficient vectors are identifiable, and the last one can be set to an arbitrary value (e.g. 0).</li>
</ol>

<p>Actually finding the values of the above probabilities is somewhat difficult, and is a problem of computing a particular <a href="order_statistic" title="wikilink">order statistic</a> (the first, i.e. maximum) of a set of values. However, it can be shown that the resulting expressions are the same as in above formulations, i.e. the two are equivalent.</p>
<h2 id="estimation-of-intercept">Estimation of intercept</h2>

<p>When using multinomial logistic regression, one category of the dependent variable is chosen as the reference category. Separate <a href="odds_ratio" title="wikilink">odds ratios</a> are determined for all independent variables for each category of the dependent variable with the exception of the reference category, which is omitted from the analysis. The exponential beta coefficient represents the change in the odds of the dependent variable being in a particular category vis-a-vis the reference category, associated with a one unit change of the corresponding independent variable.</p>
<h2 id="application-in-natural-language-processing">Application in natural language processing</h2>

<p>In <a href="natural_language_processing" title="wikilink">natural language processing</a>, multinomial LR classifiers are commonly used as an alternative to <a href="naive_Bayes_classifier" title="wikilink">naive Bayes classifiers</a> because they do not assume <a href="statistical_independence" title="wikilink">statistical independence</a> of the random variables (commonly known as <em>features</em>) that serve as predictors. However, learning in such a model is slower than for a naive Bayes classifier, and thus may not be appropriate given a very large number of classes to learn. In particular, learning in a Naive Bayes classifier is a simple matter of counting up the number of co-occurrences of features and classes, while in a maximum entropy classifier the weights, which are typically maximized using <a href="maximum_a_posteriori" title="wikilink">maximum a posteriori</a> (MAP) estimation, must be learned using an iterative procedure; see <a href="#Estimating_the_coefficients" title="wikilink">#Estimating the coefficients</a>.</p>
<h2 id="see-also">See also</h2>
<ul>
<li><a href="Logistic_regression" title="wikilink">Logistic regression</a></li>
<li><a href="Multinomial_probit" title="wikilink">Multinomial probit</a></li>
</ul>
<h2 id="references">References</h2>

<p>"</p>

<p><a href="Category:Mathematical_and_quantitative_methods_(economics)" title="wikilink">Category:Mathematical and quantitative methods (economics)</a> <a class="uri" href="Category:Econometrics" title="wikilink">Category:Econometrics</a> <a href="Category:Regression_analysis" title="wikilink">Category:Regression analysis</a> <a href="Category:Classification_algorithms" title="wikilink">Category:Classification algorithms</a> <a href="Category:Log-linear_models" title="wikilink">Category:Log-linear models</a></p>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><a href="#fnref1">‚Ü©</a></li>
<li id="fn2"><a href="#fnref2">‚Ü©</a></li>
<li id="fn3"><a href="#fnref3">‚Ü©</a></li>
<li id="fn4"><a href="#fnref4">‚Ü©</a></li>
<li id="fn5"><a href="#fnref5">‚Ü©</a></li>
<li id="fn6">Baltas, G.; Doyle, P. (2001). Random Utility Models in Marketing Research: A Survey. <em>Journal of Business Research</em> <strong>51</strong>: 115-125.<a href="#fnref6">‚Ü©</a></li>
<li id="fn7"><a href="#fnref7">‚Ü©</a></li>
<li id="fn8"><a href="#fnref8">‚Ü©</a></li>
<li id="fn9"></li>
<li id="fn10"><a href="#fnref10">‚Ü©</a></li>
</ol>
</section>
</body>
</html>
